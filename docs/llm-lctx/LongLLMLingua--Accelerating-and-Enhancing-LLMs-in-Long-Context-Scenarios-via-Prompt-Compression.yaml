- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:04:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:04:42'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LongLLMLingua: 通过提示压缩加速和增强LLMs在长上下文场景中的性能'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
- en: Huiqiang Jiang, Qianhui Wu, Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 慧强 蒋, 乾辉 吴, 旭芳 罗,
- en: Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 东升 李, Chin-Yew Lin, 玉清 杨, 丽丽 邱
- en: Microsoft Corporation {hjiang,qianhuiwu,xufang.luo}@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司 {hjiang,qianhuiwu,xufang.luo}@microsoft.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In long context scenarios, large language models (LLMs) face three main challenges:
    higher computational/financial cost, longer latency, and inferior performance.
    Some studies reveal that the performance of LLMs depends on both the density and
    the position of the key information (question relevant) in the input prompt. Inspired
    by these findings, we propose LongLLMLingua for prompt compression towards improving
    LLMs’ perception of the key information to simultaneously address the three challenges.
    We conduct evaluation on a wide range of long context scenarios including single-/multi-document
    QA, few-shot learning, summarization, synthetic tasks, and code completion. The
    experimental results show that LongLLMLingua compressed prompt can derive higher
    performance with much less cost. The latency of the end-to-end system is also
    reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance
    boost of up to 17.1% over the original prompt with $\sim$10k tokens at a compression
    rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.
    ¹¹1Our code is available at [https://aka.ms/LLMLingua](https://aka.ms/LLMLingua).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在长上下文场景中，大型语言模型（LLMs）面临三个主要挑战：更高的计算/经济成本、更长的延迟和较差的性能。一些研究表明，LLMs的性能依赖于输入提示中关键信息（与问题相关）的密度和位置。受这些发现的启发，我们提出了LongLLMLingua用于提示压缩，以提高LLMs对关键信息的感知，从而同时解决这三个挑战。我们在包括单文档/多文档问答、少样本学习、摘要生成、合成任务和代码补全在内的广泛长上下文场景中进行了评估。实验结果表明，LongLLMLingua压缩的提示能够在成本大幅降低的情况下获得更高的性能。端到端系统的延迟也有所减少。例如，在NaturalQuestions基准测试中，LongLLMLingua在压缩率为2x-10x的情况下，比原始提示提高了高达17.1%的性能，同时可以将端到端延迟提高1.4x-3.8x。¹¹1我们的代码可在[https://aka.ms/LLMLingua](https://aka.ms/LLMLingua)获取。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: ChatGPT and other large language models (LLMs) have revolutionized user-oriented
    language technologies and are serving as crucial components in more and more applications.
    Carefully designing prompts is necessary to achieve better performance in specific
    downstream tasks. The commonly used technologies such as In-Context Learning (ICL) (Dong
    et al., [2023](#bib.bib8)), Retrieval Augment Generation (RAG) (Lewis et al.,
    [2020](#bib.bib17)), and Agent (Park et al., [2023](#bib.bib24)) are driving prompts
    to be increasingly longer, even reaching thousands of tokens. Scenarios such as
    multi-document question answering, code completion, and document summarization
    also necessitate the processing of long contexts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT和其他大型语言模型（LLMs）已经彻底改变了面向用户的语言技术，并且在越来越多的应用中扮演着重要角色。精心设计提示是实现特定下游任务更好性能的必要条件。常用技术如上下文学习（ICL）（Dong
    et al., [2023](#bib.bib8)）、检索增强生成（RAG）（Lewis et al., [2020](#bib.bib17)）和智能体（Park
    et al., [2023](#bib.bib24)）正在推动提示变得越来越长，甚至达到数千个标记。多文档问答、代码补全和文档摘要等场景也需要处理长上下文。
- en: 'There are three main challenges when LLMs are used in long context scenarios:
    (1) The higher computational and financial cost required to run these models or
    to call APIs from companies providing LLM services. This can be a significant
    barrier for individuals or smaller organizations with limited resources. (2) The
    longer latency associated with LLMs, which can cause delays in generating responses
    or predictions and is particularly problematic in real-time scenarios where users
    expect quick and accurate responses. (3) The inferior performance caused by the
    extended window size of LLMs (Xiong et al., [2023](#bib.bib33)), and the low density
    as well as the less sensitive position of the question-relevant key information
    in the prompt. Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    shows that LLMs’ performance in downstream tasks may decrease as the noisy information
    in the prompt increases (Shi et al., [2023](#bib.bib29)). Moreover, the purple
    curve in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    indicates that LLMs’ ability to capture the relevant information depends on their
    positions in the prompt (Liu et al., [2023](#bib.bib20)): they achieve the highest
    performance when relevant information occurs at the beginning or end of the input
    context, and significantly degrades if relevant information is located in the
    middle of long contexts.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '当 LLM 被应用于长上下文场景时，主要面临三个挑战：(1) 运行这些模型或调用提供 LLM 服务的公司 API 所需的更高计算和财务成本。这对于资源有限的个人或较小的组织来说可能是一个重要障碍。
    (2) 与 LLM 相关的更长延迟，这可能导致生成响应或预测的延迟，并在需要快速准确响应的实时场景中特别成问题。 (3) 由于 LLM 的扩展窗口大小（Xiong
    等，[2023](#bib.bib33)）以及提示中与问题相关的关键信息的低密度和较低的敏感位置导致的性能下降。图 [1a](#S1.F1.sf1 "图 1
    ‣ 1 介绍 ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的 LLM") 显示，LLM 在下游任务中的表现可能会随着提示中的噪声信息增加而下降（Shi
    等，[2023](#bib.bib29)）。此外，图 [1b](#S1.F1.sf2 "图 1 ‣ 1 介绍 ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的
    LLM") 中的紫色曲线表明，LLM 捕捉相关信息的能力取决于其在提示中的位置（Liu 等，[2023](#bib.bib20)）：当相关信息出现在输入上下文的开始或结束时，它们的表现最佳，如果相关信息位于长上下文的中间则显著下降。'
- en: '![Refer to caption](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
- en: (a) Performance v.s. Document Number
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 性能 vs. 文档数量
- en: '![Refer to caption](img/808a8cc98f1873ee2c344faffaffa60d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/808a8cc98f1873ee2c344faffaffa60d.png)'
- en: (b) Performance v.s. Key Information Position
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 性能 vs. 关键信息位置
- en: 'Figure 1: (a) LLMs’ performance in downstream tasks may decrease as the noisy
    information in the prompt increases. In this case, we keep $k$ implies more noise
    introduced into the prompt. To improve the key information density in the prompt,
    we present question-aware coarse-to-fine compression. (b) LLMs’ ability to capture
    the relevant information depends on their positions in the prompt. To reduce information
    loss in the middle, we introduce a document reordering mechanism.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: (a) 当提示中的噪声信息增加时，LLM 在下游任务中的表现可能会下降。在这种情况下，我们保持$k$意味着提示中引入了更多噪声。为了提高提示中的关键信息密度，我们提出了基于问题感知的粗到细压缩方法。
    (b) LLM 捕捉相关信息的能力取决于其在提示中的位置。为了减少中间的信息丢失，我们引入了文档重排序机制。'
- en: 'Inspired by these observations, we propose LongLLMLingua to address the three
    challenges. Specifically, we use the advanced while efficient LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) as our backbone framework for prompt compression
    to address the first two challenges, i.e., reduce cost and latency. However, in
    the case of long contexts, the distribution of question-relevant key information
    in the prompt is generally sparse. Existing prompt compression methods like LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) and Selective-Context (Li, [2023](#bib.bib19)) that
    do not consider the content of the question during compression may retain too
    much noisy information in the compressed results, leading to inferior performance.
    In this paper, LongLLMLingua is designed to enhance LLM’s perception of key information
    (relevant to the question) in the prompt, so that the third challenge of inferior
    performance in long context scenarios could be addressed. Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression") is an example. The underlying
    principle of LongLLMLingua is that small language models are inherently capable
    of capturing the distribution of key information relevant to a given question.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这些观察结果的启发，我们提出了 LongLLMLingua 来解决这三个挑战。具体而言，我们使用先进且高效的 LLMLingua （Jiang 等，[2023a](#bib.bib13)）作为我们的主框架进行提示压缩，以解决前两个挑战，即降低成本和延迟。然而，在长上下文的情况下，提示中与问题相关的关键信息的分布通常是稀疏的。现有的提示压缩方法如
    LLMLingua （Jiang 等，[2023a](#bib.bib13)）和 Selective-Context （Li，[2023](#bib.bib19)）在压缩过程中不考虑问题的内容，可能会在压缩结果中保留过多噪声信息，导致性能下降。在本文中，LongLLMLingua
    旨在增强 LLM 对提示中与问题相关的关键信息的感知，从而解决长上下文场景中性能下降的第三个挑战。图 [1b](#S1.F1.sf2 "图1 ‣ 1 引言
    ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的 LLM") 是一个例子。LongLLMLingua 的基本原理是小型语言模型本质上能够捕捉与给定问题相关的关键信息的分布。
- en: 'Our main contributions are five-fold: (1) We propose a question-aware coarse-to-fine
    compression method to improve the key information density in the prompt (Sec.
    [4.1](#S4.SS1 "4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")); (2) We introduce a document reordering mechanism to reduce
    information loss in the middle. (Sec. [4.2](#S4.SS2 "4.2 How to reduce information
    loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression")); (3) We present dynamic
    compression ratios to bridge the coarse-grained compression and fine-grained compression
    for adaptive granular control (Sec. [4.3](#S4.SS3 "4.3 How to achieve adaptive
    granular control during compression? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression")); (4) We
    propose a post-compression subsequence recovery strategy to improve the integrity
    of the key information ([4.4](#S4.SS4 "4.4 How to improve the integrity of key
    information? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression")). (5) We evaluate LongLLMLingua
    on three benchmarks, i.e., NaturalQuestions (Liu et al., [2023](#bib.bib20)),
    LongBench (Bai et al., [2023](#bib.bib1)), and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)).
    Experimental results demonstrate that compared with original prompts, LongLLMLingua
    compressed prompts can achieve higher performance with much lower costs. The latency
    of the end-to-end system is also reduced.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的主要贡献有五点：（1）我们提出了一种基于问题的粗到细压缩方法，以提高提示中的关键信息密度（见 [4.1](#S4.SS1 "4.1 如何提高提示中的关键信息密度？
    ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（2）我们引入了一种文档重排机制，以减少中间的信息丢失（见
    [4.2](#S4.SS2 "4.2 如何减少中间的信息丢失？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（3）我们提出了动态压缩比，以在粗粒度压缩和细粒度压缩之间建立桥梁，实现自适应粒度控制（见
    [4.3](#S4.SS3 "4.3 如何在压缩过程中实现自适应粒度控制？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（4）我们提出了一种后压缩子序列恢复策略，以提高关键信息的完整性（见
    [4.4](#S4.SS4 "4.4 如何提高关键信息的完整性？ ‣ 4 LongLLMLingua ‣ LongLLMLingua: 通过提示压缩加速和增强长上下文场景中的LLMs")）；（5）我们在三个基准上评估了LongLLMLingua，即NaturalQuestions（刘等，[2023](#bib.bib20)），LongBench（白等，[2023](#bib.bib1)），和ZeroSCROLLS（Shaham等，[2023](#bib.bib28)）。实验结果表明，与原始提示相比，LongLLMLingua压缩提示可以实现更高的性能，成本却低得多。端到端系统的延迟也有所减少。'
- en: 2 Problem Formulation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题定义
- en: 'Following LLMLingua (Jiang et al., [2023a](#bib.bib13)), we use $\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$
    can be any additional materials that users append to the prompt to get a better
    response from LLMs for $\mathbf{x}^{\text{que}}$. The objective of a prompt compression
    system can be formulated as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 参考LLMLingua（蒋等，[2023a](#bib.bib13)），我们使用$\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$，这些可以是用户附加到提示中的任何额外材料，以获得更好的LLMs响应。提示压缩系统的目标可以表述为：
- en: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
- en: where $\widetilde{\mathbf{x}}$ and $\widetilde{\mathbf{y}}$ for joint optimization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\widetilde{\mathbf{x}}$和$\widetilde{\mathbf{y}}$用于联合优化。
- en: '3 Preliminary: LLMLingua'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步研究：LLMLingua
- en: LLMLingua (Jiang et al., [2023a](#bib.bib13)) uses a small language model $\mathcal{M}_{S}$
    used for prompt compression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua（蒋等，[2023a](#bib.bib13)）使用一个小型语言模型$\mathcal{M}_{S}$用于提示压缩。
- en: 4 LongLLMLingua
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LongLLMLingua
- en: '![Refer to caption](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
- en: 'Figure 2: Framework of LongLLMLingua. Gray Italic content: As in LLMLingua.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LongLLMLingua的框架。灰色斜体内容：如LLMLingua所示。
- en: LongLLMLingua is developed upon the framework of LLMLingua towards prompt compression
    in long context scenarios. The primary challenge in long context scenarios is
    how to enhance LLM’s perception of key information relevant to the question in
    the prompt. LongLLMLingua addresses this challenge from three perspectives, and
    further applies a subsequence recovery strategy to improve the accuracy and reliability
    of the information provided to users. We elaborate on each component in this section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LongLLMLingua是在LLMLingua框架的基础上开发的，旨在长上下文场景中的提示压缩。长上下文场景中的主要挑战是如何增强LLM对提示中与问题相关的关键信息的感知。LongLLMLingua从三个方面解决这一挑战，并进一步应用子序列恢复策略，以提高提供给用户的信息的准确性和可靠性。本节将详细阐述每个组成部分。
- en: 4.1 How to improve key information density in the prompt?
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 如何提高提示中的关键信息密度？
- en: Question-Aware Coarse-Grained Compression
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题感知粗粒度压缩
- en: In coarse-grained compression, we aim to figure out a metric $r_{k}$ as the
    intermediate compressed results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在粗粒度压缩中，我们旨在找出一个指标 $r_{k}$ 作为中间压缩结果。
- en: 'LLMLingua uses document-level perplexity to represent the importance of documents:
    $r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$
    and instead become noise, reducing key information density in the compressed results
    and bringing difficulties for LLM to output correct answers. As shown in Figure [3a](#S4.F3.sf1
    "In Figure 3 ‣ Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve
    key information density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"), the recall@16
    of LLMLingua only reaches 50%, indicating its incompetence in retaining key information
    during compression.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua使用文档级困惑度来表示文档的重要性：$r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log
    p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$，这会变成噪声，降低压缩结果中的关键信息密度，并使LLM输出正确答案变得困难。如图[3a](#S4.F3.sf1
    "在图3中 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣ LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")所示，LLMLingua的recall@16仅达到50%，这表明其在压缩过程中保留关键信息的能力不足。
- en: Retrieval-based methods are also feasible here. We can use $\mathbf{x}^{\text{que}}$75%
    accuracy in recall@5, which implies that the final accuracy upper bound of LLMs
    with 4x compression is only 75%.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的方法在这里也是可行的。我们可以在 recall@5 中达到 $\mathbf{x}^{\text{que}}$ 的75%准确率，这意味着具有4倍压缩的LLMs的最终准确率上限仅为75%。
- en: '![Refer to caption](img/2e6955b6f2f232c285024fef939bddac.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e6955b6f2f232c285024fef939bddac.png)'
- en: (a) Recall Distribution
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 召回分布
- en: '![Refer to caption](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
- en: (b) Perplexity Distribution
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 困惑度分布
- en: 'Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset.
    (b) Comparison between perplexities and contrastive perplexities of tokens in
    the prompt from Multi-documemnt QA dataset. The document with the ground truth
    is located on the left side of the dashed line.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3： (a) 在NaturalQuestions多文档问答数据集上的召回对比。(b) Multi-documemnt QA数据集中提示中令牌的困惑度和对比困惑度的比较。地面真实值的文档位于虚线的左侧。
- en: 'One approach to improve key information density in the compressed results is
    to calculate document-level perplexity conditioned on the question $\mathbf{x}^{\text{que}}$.
    It can be regarded as a regularization term that mitigates the impact of hallucinations.
    This can be formulated as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提高压缩结果中关键信息密度的一种方法是计算以问题 $\mathbf{x}^{\text{que}}$ 为条件的文档级困惑度。这可以视为一种正则化项，用以缓解幻觉的影响。这可以表示为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $x^{\text{que},\text{restrict}}_{i}$ in the number of tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x^{\text{que},\text{restrict}}_{i}$ 为令牌的数量。
- en: 'Figure [3a](#S4.F3.sf1 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") demonstrates that our coarse-level compression approach achieves
    the highest recall with different numbers of retained documents, suggesting that
    it preserves the most key information from the documents $(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$
    in the compressed results.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3a](#S4.F3.sf1 "在图3 ‣ 问题感知粗粒度压缩 ‣ 4.1 如何提高提示中的关键信息密度？ ‣ 4 LongLLMLingua ‣
    LongLLMLingua：通过提示压缩加速和增强长上下文场景中的LLMs")展示了我们的粗粒度压缩方法在保留不同数量的文档时达到了最高的召回率，表明它在压缩结果中保留了来自文档$(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$的最关键信息。
- en: Question-Aware Fine-Grained Compression
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题感知细粒度压缩
- en: In fine-grained compression, we assess the importance of each token in the instruction
    $\mathbf{x}^{\text{ins}}$, so that the compressed results could contain more question-relevant
    key information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度压缩中，我们评估指令$\mathbf{x}^{\text{ins}}$中每个标记的重要性，以便压缩结果能够包含更多与问题相关的关键信息。
- en: 'A straightforward solution for the awareness of $\mathbf{x}^{\text{que}}$ can
    be formulated as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于$\mathbf{x}^{\text{que}}$的意识，可以制定一个直接的解决方案：
- en: '|  | $s_{i}=\text{perplexity}(x_{i}&#124;x_{Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.
    Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to , of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.
    Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to , of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen'
- en: 'Figure 6: Comparing the compressed prompt and LLMs’ response before and after
    using Question-aware Fine-grained Compression and Subsequence Recovery($1/\tau$=30x,
    high compression ratio setting) from NaturalQuestions Multi-document QA (Liu et al.,
    [2023](#bib.bib20)) using GPT-3.5-Turbo.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 比较使用问题感知细粒度压缩和子序列恢复($1/\tau$=30x，高压缩比设置)前后的压缩提示和 LLM 的响应，来自 NaturalQuestions
    多文档 QA (Liu et al., [2023](#bib.bib20))，使用 GPT-3.5-Turbo。'
- en: Appendix D Economic Cost
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 经济成本
- en: '|  | Multi-document QA | LongBench | ZeroScolls |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 多文档 QA | LongBench | ZeroScrolls |'
- en: '| --- | --- | --- | --- |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Original | 4.6 | 31.5 | 30.6 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 4.6 | 31.5 | 30.6 |'
- en: '| Ours | 1.3 | 3.0 | 3.2 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 1.3 | 3.0 | 3.2 |'
- en: 'Figure 7: The inference costs(per 1,000 samples $) for various datasets using
    GPT-3.5-Turbo.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 使用 GPT-3.5-Turbo 的各种数据集的推理成本（每 1,000 个样本 $）。'
- en: 'Table [7](#A4.F7 "Figure 7 ‣ Appendix D Economic Cost ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") presents
    the estimated per 1,000 samples inference costs for various datasets, encompassing
    input prompts and generated output text, based on GPT-3.5-Turbo pricing^(14)^(14)14https://openai.com/pricing.
    Our approach demonstrates substantial savings in computational resources and monetary
    expenses, particularly in long context situations. Cost reductions of $3.3, $28.5,
    and $27.4 per 1,000 samples are observed for Multi-document QA, LongBench, and
    ZeroScrolls, respectively.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [7](#A4.F7 "图 7 ‣ 附录 D 经济成本 ‣ LongLLMLingua: 在长上下文场景中通过提示压缩加速和增强 LLM") 显示了各种数据集每
    1,000 个样本的推理成本，包括输入提示和生成的输出文本，基于 GPT-3.5-Turbo 定价^(14)^(14)14https://openai.com/pricing。我们的方法在计算资源和货币开支方面表现出显著的节省，特别是在长上下文情况下。对于
    Multi-document QA、LongBench 和 ZeroScrolls，每 1,000 个样本的成本分别减少了 $3.3、$28.5 和 $27.4。'
- en: Appendix E Cases Study
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 案例研究
- en: 'Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in . Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 'Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in . Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019'
- en: 'Figure 8: Cases study on NaturalQuestions Multi-document QA dataset (Liu et al.,
    [2023](#bib.bib20)) in 4x constraint using GPT-3.5-Turbo.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 在 4x 约束下使用 GPT-3.5-Turbo 对 NaturalQuestions 多文档 QA 数据集 (Liu et al., [2023](#bib.bib20))
    进行的案例研究。'
- en: 'Compressed
    Prompt: Please complete the code given below.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩提示：请完成下面给出的代码。
- en: '[PRE0]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next line of code: LLMs’ Response:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行代码：LLMs 的响应：
- en: '[PRE1]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Ground Truth:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 真实情况：
- en: '[PRE2]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Zero-shot LLMs’ Response:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本 LLMs 的响应：
- en: '[PRE3]'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE3]'
- en: 'Figure 9: Cases study on lcc code completion task in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在 LongBench 基准测试中对 lcc 代码补全任务的案例研究 (Bai et al., [2023](#bib.bib1))，使用 GPT-3.5-Turbo
    和 2,000 个约束条件。
- en: 'Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something'
- en: 'Figure 10: Cases study on trec few-show learning in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在 LongBench 基准测试中对 trec 少样本学习的案例研究 (Bai et al., [2023](#bib.bib1))，使用 GPT-3.5-Turbo
    和 2,000 个约束条件。
