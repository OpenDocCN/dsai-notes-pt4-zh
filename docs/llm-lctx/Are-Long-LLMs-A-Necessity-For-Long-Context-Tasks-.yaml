- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:03:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:22
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Are Long-LLMs A Necessity For Long-Context Tasks?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期大语言模型（Long-LLMs）是否是处理长期上下文任务的必要条件？
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.15318](https://ar5iv.labs.arxiv.org/html/2405.15318)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.15318](https://ar5iv.labs.arxiv.org/html/2405.15318)
- en: Hongjin Qian^(1,2), Zheng Liu², Peitian Zhang¹, Kelong Mao¹, Yujia Zhou¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hongjin Qian^(1,2)，Zheng Liu²，Peitian Zhang¹，Kelong Mao¹，Yujia Zhou¹
- en: Xu Chen¹, Zhicheng Dou¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xu Chen¹，Zhicheng Dou¹
- en: ¹ Gaoling School of Artificial Intelligence, Renmin University of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 中国人民大学高岭人工智能学院
- en: ² Beijing Academy of Artificial Intelligence
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 北京人工智能研究院
- en: '{chienqhj,zhengliu1026}@gmail.com Corresponding author.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{chienqhj,zhengliu1026}@gmail.com 通讯作者。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The learning and deployment of long-LLMs remains a challenging problem despite
    recent progresses. In this work, we argue that the long-LLMs are not a necessity
    to solve long-context tasks, as common long-context tasks are short-context solvable,
    i.e. they can be solved by purely working with oracle short-contexts within the
    long-context tasks’ inputs. On top of this argument, we propose a framework called
    LC-Boost (Long-Context Bootstrapper), which enables a short-LLM to address the
    long-context tasks in a bootstrapping manner. In our framework, the short-LLM
    prompts itself to reason for two critical decisions: 1) how to access to the appropriate
    part of context within the input, 2) how to make effective use of the accessed
    context. By adaptively accessing and utilizing the context based on the presented
    tasks, LC-Boost can serve as a general framework to handle diversified long-context
    processing problems. We comprehensively evaluate different types of tasks from
    popular long-context benchmarks, where LC-Boost is able to achieve a substantially
    improved performance with a much smaller consumption of resource.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近有所进展，但学习和部署长期大语言模型（long-LLMs）仍然是一个具有挑战性的问题。在这项工作中，我们认为长期大语言模型并不是解决长期上下文任务的必要条件，因为常见的长期上下文任务可以通过短期上下文解决，即它们可以通过在长期上下文任务的输入中仅处理短期上下文来解决。在这一论点的基础上，我们提出了一个叫做LC-Boost（Long-Context
    Bootstrapper）的框架，该框架使短期大语言模型能够以引导的方式处理长期上下文任务。在我们的框架中，短期大语言模型通过自我提示来推理两个关键决策：1）如何访问输入中的适当上下文部分，2）如何有效利用已访问的上下文。通过根据呈现的任务自适应地访问和利用上下文，LC-Boost可以作为一个通用框架来处理多样化的长期上下文处理问题。我们从流行的长期上下文基准中综合评估了不同类型的任务，其中LC-Boost能够以更小的资源消耗实现显著改进的性能。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) are widely adopted for real-world applications.
    Many of the applications are associated with long-sequence inputs, such as long-document
    question answering and summarization. As such, the LLMs are commonly expected
    to have a long working context (a.k.a. long-LLMs) in order to confront such demanding
    scenarios (Bai et al., [2023](#bib.bib1); Zhang et al., [2024a](#bib.bib2)). Unfortunately,
    the learning and deployment of long-LLMs are still challenging in multiple perspectives.
    Particularly, many existing LLMs are initially introduced with a limited size
    of context (e.g., 2K for Llama-1 Touvron et al. ([2023a](#bib.bib3)), 4K for Llama-2
    Touvron et al. ([2023b](#bib.bib4)), 8K for Llama-3 ¹¹1[https://llama.meta.com/llama3/](https://llama.meta.com/llama3/)).
    Although the initial short-LLM can be fine-tuned to establish a much longer context,
    it is likely to take substantial costs; and more seriously, it is extremely resource-consuming
    to deploy the long-LLMs (Kaplan et al., [2020](#bib.bib5)). The continually training
    may also compromise the LLMs’ general capability over short contexts (Liu et al.,
    [2023](#bib.bib6); Li et al., [2023a](#bib.bib7)). In fact, it remains an open
    problem to explore new solutions which may tackle long-context tasks both effectively
    and efficiently.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在实际应用中被广泛采用。许多应用与长序列输入相关，例如长文档问答和摘要。因此，LLMs通常期望具有较长的工作上下文（即长期大语言模型）以应对这种要求高的场景（Bai等，[2023](#bib.bib1)；Zhang等，[2024a](#bib.bib2)）。不幸的是，学习和部署长期大语言模型在多个方面仍然具有挑战性。特别是，许多现有的LLMs最初引入时上下文大小有限（例如，Llama-1
    Touvron等（[2023a](#bib.bib3)）为2K，Llama-2 Touvron等（[2023b](#bib.bib4)）为4K，Llama-3 ¹¹1[https://llama.meta.com/llama3/](https://llama.meta.com/llama3/)）为8K）。尽管可以对初始的短期大语言模型进行微调以建立更长的上下文，但这可能需要大量成本；更严重的是，部署长期大语言模型的资源消耗极大（Kaplan等，[2020](#bib.bib5)）。持续的训练也可能损害LLMs在短期上下文中的总体能力（Liu等，[2023](#bib.bib6)；Li等，[2023a](#bib.bib7)）。实际上，探索新的解决方案，以有效且高效地处理长期上下文任务仍然是一个开放的问题。
- en: In this paper, we argue that most long-context tasks are short-context solvable.
    That is to say, the long-context tasks, despite associated with long-sequence
    inputs, can be addressed by merely working with short-contexts in a strategic
    way. For example, the reading comprehension or summarization of a book can be
    solved based on the extraction of necessary key facts from the book. The above
    argument is akin to the working patterns of human beings and modern computers,
    where arbitrary long-form problems can always be decomposed and solved on top
    of a limited memory capacity (Adolphs, [1999](#bib.bib8); Bryant and O’Hallaron,
    [2011](#bib.bib9)). However, even if the above argument holds, it is still non-trivial
    to solve the long-context tasks purely based on short contexts. This is because
    different tasks call for distinct ways of accessing and utilizing information
    from the long context; therefore, there can hardly be any fixed rules to handle
    all possible situations. To address this challenge, we propose a method, called
    LC-Boost, where short-LLMs are employed to solve general long-context tasks in
    a bootstrapping manner. LC-Boost operates with two critical reasoning steps. One
    is the reasoning of Access, where the LLM prompts itself to plan for how to access
    the appropriate part of context within the input. The other one is the reasoning
    of Utilize, where the LLM figures out how to make effective use of the accessed
    context. Thanks to the above design, LC-Boost is able to adaptively handle diversified
    long-context tasks according to their unique nature. For example, given a knowledge-grounded
    QA problem, the LLM may directly access to the knowledgable context through retrieval,
    and generate the answer in the form of RAG. Besides, it may sequentially scan
    the long context chunk-by-chunk if the task calls for the aggregation of specific
    information from the entire input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们认为大多数长上下文任务实际上可以通过短上下文来解决。也就是说，尽管长上下文任务涉及到长序列输入，但通过策略性地处理短上下文也能解决这些任务。例如，书籍的阅读理解或总结可以基于从书籍中提取必要的关键事实来解决。上述观点类似于人类和现代计算机的工作模式，其中任意长形式的问题总是可以在有限的记忆容量上进行分解和解决（Adolphs，[1999](#bib.bib8)；Bryant
    和 O’Hallaron，[2011](#bib.bib9)）。然而，即使上述观点成立，基于短上下文纯粹解决长上下文任务仍然是一个非平凡的挑战。这是因为不同的任务需要不同的方式来访问和利用长上下文中的信息；因此，几乎不存在能够处理所有可能情况的固定规则。为了解决这个挑战，我们提出了一种方法，称为
    LC-Boost，其中短期 LLMs 被用于以引导方式解决一般的长上下文任务。LC-Boost 通过两个关键的推理步骤进行操作。一个是访问推理，即 LLM
    自我提示规划如何访问输入中的适当上下文部分。另一个是利用推理，即 LLM 解决如何有效利用已访问的上下文。由于以上设计，LC-Boost 能够根据长上下文任务的独特性质自适应地处理多样化的长上下文任务。例如，给定一个知识驱动的问答问题，LLM
    可以通过检索直接访问知识性上下文，并以 RAG 形式生成答案。此外，如果任务需要从整个输入中聚合特定信息，LLM 可能会按块顺序扫描长上下文。
- en: '![Refer to caption](img/8a039b3710c4272eefd31b277dcc9bd2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a039b3710c4272eefd31b277dcc9bd2.png)'
- en: 'Figure 1: Illustration for LC-Boost. The LLM is prompted to reason for how
    to access to proper context and how to utilize the accessed context to solve the
    task. Toy Examples. (A) Brute-force solution. Despite correctness, it is unnecessarily
    expensive due to the processing of the entire context simultaneously. (B) Naive
    RAG. It is hard to handle problems like information aggregation, which leads to
    the incomplete answer. (C) LC-Boost leverages RAG to tackle the problem, which
    produces the correct answer in a small cost. (D) LC-Boost processes the long-context
    via sequential scan, which correctly solves the problem based on the comprehensively
    collected information.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LC-Boost 的说明。LLM 被提示推理如何访问适当的上下文以及如何利用已访问的上下文来解决任务。示例。（A）暴力解决方案。尽管正确，但由于同时处理整个上下文而显得不必要地昂贵。（B）天真
    RAG。处理信息聚合等问题较为困难，导致答案不完整。（C）LC-Boost 利用 RAG 来解决问题，从而以较小的成本产生正确答案。（D）LC-Boost
    通过顺序扫描处理长上下文，根据全面收集的信息正确解决问题。
- en: 'The following toy examples are presented to better illustrate the mechanism
    of LC-Boost (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?")). Particular, there are two common approaches to tackle
    long-context problems: (A) the brute-force method based on long-LLMs, (B) the
    surrogate methods, like RAG Xu et al. ([2023a](#bib.bib10)). Despite being straightforward,
    the brute-force method is likely to incur huge unnecessary costs as the problem
    could be directly solved by simple surrogate methods, like RAG. On the other hand,
    although the surrogate methods may help in certain cases, they are likely to become
    useless in other situations. For instance, the RAG-based methods are inappropriate
    to handle information aggregation problems, as showcased in (B). In contrast,
    LC-Boost is able to handle general long-context tasks thanks to the proper reasoning
    of how to access and utilize the long-context information based on each specific
    task. As shown in (C), it can directly access to the needed information via retrieval
    and generate the answer based on RAG. Meanwhile, it can also process the entire
    context in a divide-and-conquer manner, which will fully collect the information
    and solve the problem presented in (D).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例展示了LC-Boost的机制（图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Are Long-LLMs
    A Necessity For Long-Context Tasks?")）。特别地，有两种常见的方法来解决长上下文问题：（A）基于长上下文语言模型的暴力破解方法，（B）替代方法，如RAG
    Xu等人（[2023a](#bib.bib10)）。尽管直观，暴力破解方法可能会产生巨大的不必要成本，因为该问题可以通过简单的替代方法如RAG直接解决。另一方面，尽管替代方法在某些情况下可能有效，但在其他情况下可能变得无用。例如，RAG方法不适用于处理信息聚合问题，如在（B）中展示的那样。相比之下，由于基于每个特定任务的长上下文信息访问和利用的适当推理，LC-Boost能够处理一般的长上下文任务。如（C）所示，它可以通过检索直接访问所需的信息，并基于RAG生成答案。同时，它还可以以分而治之的方式处理整个上下文，这将充分收集信息并解决（D）中提出的问题。
- en: We perform comprehensive experiments for LC-Boost, including both popular real-world
    long-context problems, like question-answering and summarization of long documents,
    and a wide variety of synthetic tasks. In our experiments, LC-Boost is able to
    achieve equivalent performances as the brute-force methods based on strong long-LLMs,
    e.g., GPT-4-128K. In many cases, its performances can even notably surpass the
    brute-force methods, probably due to the elimination of distracting context. Besides,
    our experiments also underscore the importance of reasoning and adaptability,
    as LC-Boost outperforms all short-LLM surrogates with predefined access and utilization
    of context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对LC-Boost进行了全面的实验，包括流行的实际长上下文问题，如问答和长文档的摘要，以及各种合成任务。在我们的实验中，LC-Boost能够实现与基于强大长上下文语言模型（如GPT-4-128K）的方法相当的性能。在许多情况下，它的性能甚至显著超过了这些方法，这可能是由于消除了干扰上下文。此外，我们的实验还强调了推理和适应性的的重要性，因为LC-Boost在预定义上下文访问和利用的所有短上下文语言模型替代品中表现优越。
- en: To summarize, our paper makes the following contributions. (1) We identify the
    research problem of solving long-context problems with short-LLMs. To the best
    of our knowledge, it is the first study of its kind, which is important to not
    only address the problem itself but also meaningful to the sustainability and
    energy-efficient running of AI industry in a broader sense. (2) We propose a novel
    framework LC-Boost, which is able to adaptively handle general long-context tasks
    based on the reasoning of how to access and utilize the long context. (3) We empirically
    verify the effectiveness of LC-Boost based on its superior performances achieved
    from low resource-consumption.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的论文做出了以下贡献：（1）我们识别了用短上下文语言模型解决长上下文问题的研究问题。据我们所知，这是首个此类研究，它不仅对解决问题本身重要，而且对AI行业的可持续性和节能运行具有广泛的意义。（2）我们提出了一个新颖的框架LC-Boost，该框架能够基于如何访问和利用长上下文的推理自适应地处理一般长上下文任务。（3）我们通过低资源消耗获得的优越性能经验性地验证了LC-Boost的有效性。
- en: 2 LC-Boost
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LC-Boost
- en: 2.1 Preliminaries
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基础知识
- en: 'LLMs can be succinctly defined as ${\mathcal{Y}}=\gamma(q)$ refers to the answer
    produced by the LLMs. As highlighted in many previous studies, e.g., (Ji et al.,
    [2023](#bib.bib11); Lewis et al., [2020](#bib.bib12); Shuster et al., [2021](#bib.bib13)),
    the knowledge embedded in an LLM’s parameters is static and, consequently, often
    fails to adequately address user queries requiring up-to-date or in-depth knowledge.
    To address this limitation, we can introduce external knowledge (refer to as context
    ${\mathcal{X}}$ as: ${\mathcal{Y}}=\gamma(q,{\mathcal{X}}).$'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 可以简洁地定义为 ${\mathcal{Y}}=\gamma(q)$，指的是LLMs产生的答案。正如许多先前研究所强调的，例如（Ji 等，[2023](#bib.bib11)；Lewis
    等，[2020](#bib.bib12)；Shuster 等，[2021](#bib.bib13)），LLM参数中嵌入的知识是静态的，因此，通常无法充分解决需要最新或深入知识的用户查询。为了解决这一局限性，我们可以引入外部知识（称为上下文
    ${\mathcal{X}}$），如下所示： ${\mathcal{Y}}=\gamma(q,{\mathcal{X}})$。
- en: 'As discussed in Section [1](#S1 "1 Introduction ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?"), in many scenarios, the context ${\mathcal{X}}$ is a
    long sequence, necessitating that LLMs manage long contexts. However, most existing
    LLMs were originally introduced with limited context sizes (e.g., 4K). Consequently,
    these models are unable to process inputs that exceed their capacity without truncation.
    In this paper, we characterize such scenarios as long-context problem. It involves
    LLMs processing inputs that notably surpass their inherent context limitations,
    which can be formally described by:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[1](#S1 "1 Introduction ‣ Are Long-LLMs A Necessity For Long-Context Tasks?")节所讨论，在许多场景中，上下文
    ${\mathcal{X}}$ 是一个长序列，要求LLMs处理长上下文。然而，大多数现有的LLMs最初都是以有限的上下文大小引入的（例如，4K）。因此，这些模型在没有截断的情况下无法处理超出其容量的输入。在本文中，我们将这种情况描述为长上下文问题。它涉及到LLMs处理明显超出其固有上下文限制的输入，这可以通过以下方式正式描述：
- en: '|  | ${\mathcal{Y}}=\gamma(q,{\mathcal{X}})\quad\text{s.t.}&#124;{\mathcal{X}}&#124;\gg
    L,$ |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\mathcal{Y}}=\gamma(q,{\mathcal{X}})\quad\text{s.t.}|{\mathcal{X}}|
    \gg L,$ |  | (1) |'
- en: where $L$.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$。
- en: '2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable?'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 初步研究：大多数长上下文任务是否可以用短上下文解决？
- en: 'Despite the potential for fine-tuning LLMs to handle much longer contexts,
    this approach incurs substantial costs. Additionally, directly processing long
    contexts during the inference stage exponentially increases computing resource
    consumption, which is not environmentally friendly. In the following, we conduct
    a pilot study from both theoretical and empirical perspectives to explore the
    question: Are most long-context tasks solvable with short contexts?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有潜力通过微调大型语言模型（LLMs）以处理更长的上下文，但这种方法会产生巨大的成本。此外，在推理阶段直接处理长上下文会指数级增加计算资源的消耗，这对环境并不友好。接下来，我们将从理论和实证两个角度进行初步研究，探讨这样一个问题：大多数长上下文任务是否可以用短上下文解决？
- en: Theoretical Analysis
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 理论分析
- en: Suppose we have an input variable ${\mathcal{X}}$ is denoted by $\tilde{{\mathcal{X}}}$.
    In other words, the optimal $\tilde{{\mathcal{X}}}$. According to the data processing
    inequality (DPI), we have $I({\mathcal{X}};\tilde{{\mathcal{X}}})\geq I({\mathcal{X}};{\mathcal{Y}})$
    as the full context $\mathcal{X}$.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个输入变量 ${\mathcal{X}}$，用 $\tilde{{\mathcal{X}}}$ 表示。换句话说，最优的 $\tilde{{\mathcal{X}}}$。根据数据处理不等式（DPI），我们有
    $I({\mathcal{X}};\tilde{{\mathcal{X}}})\geq I({\mathcal{X}};{\mathcal{Y}})$，作为完整的上下文
    $\mathcal{X}$。
- en: 'In practical scenarios, obtaining the optimal $\tilde{{\mathcal{X}}}$ might
    be challenging if ${\mathcal{X}}$ and process each subset variable separately.
    Thus, according to the chain rule for mutual information Cover ([1999](#bib.bib14)),
    we have:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，如果 ${\mathcal{X}}$ 是由 $\tilde{{\mathcal{X}}}$ 表示的，获取最优的 $\tilde{{\mathcal{X}}}$
    可能是具有挑战性的，并且需要分别处理每个子集变量。因此，根据互信息的链式法则 Cover ([1999](#bib.bib14))，我们有：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: which indicates that the mutual information $I({\mathcal{X}},\tilde{{\mathcal{X}}})$
    given all previous subsets.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，给定所有先前子集的情况下，互信息 $I({\mathcal{X}},\tilde{{\mathcal{X}}})$。
- en: 'In the scenario of Eq. [1](#S2.E1 "In 2.1 Preliminaries ‣ 2 LC-Boost ‣ Are
    Long-LLMs A Necessity For Long-Context Tasks?"), the variable $\mathcal{X}$ given
    the output answer ${\mathcal{Y}}$ can be computed by processing each subset $\mathcal{X}_{i}$.
    Consequently, Eq. [2](#S2.E2 "In Theoretical Analysis ‣ 2.2 Pilot Study: Are Most
    Long-Context Tasks Short-Context Solvable? ‣ 2 LC-Boost ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?") can be reformulated as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程[1](#S2.E1 "在2.1 初步研究 ‣ 2 LC-Boost ‣ 长LLMs是否对长上下文任务是必需的？")的情境下，给定输出答案${\mathcal{Y}}$的变量$\mathcal{X}$可以通过处理每个子集$\mathcal{X}_{i}$来计算。因此，方程[2](#S2.E2
    "在理论分析 ‣ 2.2 初步研究：大多数长上下文任务是否可通过短上下文解决？ ‣ 2 LC-Boost ‣ 长LLMs是否对长上下文任务是必需的？")可以被重新表述如下：
- en: '|  | $I({\mathcal{X}},\tilde{{\mathcal{X}}})=I({\mathcal{X}}_{1},\cdots,{\mathcal{X}}_{n};\tilde{{\mathcal{X}}})\simeq
    I({\mathcal{X}}_{1};\tilde{{\mathcal{X}}})+\sum\limits_{i=2}^{n}I({\mathcal{X}}_{i};\tilde{{\mathcal{X}}}&#124;\hat{{\mathcal{X}}}_{i})).$
    |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $I({\mathcal{X}},\tilde{{\mathcal{X}}})=I({\mathcal{X}}_{1},\cdots,{\mathcal{X}}_{n};\tilde{{\mathcal{X}}})\simeq
    I({\mathcal{X}}_{1};\tilde{{\mathcal{X}}})+\sum\limits_{i=2}^{n}I({\mathcal{X}}_{i};\tilde{{\mathcal{X}}}&#124;\hat{{\mathcal{X}}}_{i})).$
    |  | (3) |'
- en: 'The equality can be upheld under two specific conditions: (1) the decomposed
    variables $\{\mathcal{X}_{1},\ldots,\mathcal{X}_{n}\}$. Otherwise, $I({\mathcal{X}},\tilde{{\mathcal{X}}})$
    can only be approximately estimated.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该等式在两个特定条件下可以保持： (1) 被分解的变量$\{\mathcal{X}_{1},\ldots,\mathcal{X}_{n}\}$。否则，$I({\mathcal{X}},\tilde{{\mathcal{X}}})$只能被近似估算。
- en: Empirical Analysis
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实证分析
- en: '![Refer to caption](img/1052a9c17a311d7ebad06e64ad167f0d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1052a9c17a311d7ebad06e64ad167f0d.png)'
- en: 'Figure 2: Pilot Study Across Various Tasks: In the Brute-force setting, the
    entire context is processed by GPT-4-128K. In the LC-Boost setting, the maximum
    context length is restricted to 4K, and LC-Boost is utilized to solve the long-context
    problem with short context.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同任务的初步研究：在暴力破解设置下，整个上下文由GPT-4-128K处理。在LC-Boost设置下，最大上下文长度限制为4K，LC-Boost被用来解决短上下文中的长上下文问题。
- en: To empirically assess the accuracy of estimating the minimal necessary context
    $\tilde{\mathcal{X}}$, which then guides the model to produce the final output
    (the LC-Boost setting).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实证评估最小必要上下文$\tilde{\mathcal{X}}$的估算准确性，进而指导模型生成最终输出（LC-Boost设置）。
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ Empirical Analysis ‣ 2.2 Pilot Study: Are Most
    Long-Context Tasks Short-Context Solvable? ‣ 2 LC-Boost ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?") presents the experiment results, which generally indicate
    that LC-Boost consistently performs as well as or better than the brute-force
    setting. In particular, for tasks such as QA, few-shot learning, and synthetic
    tasks, LC-Boost outperforms the brute-force setting. This is because the decomposed
    short contexts for these tasks are more likely to be mutually independent given
    the input query which can be adequately supported by a few extracted contexts
    from the long context. By precisely locating these supported context, it can filter
    out irrelevant context of ${\mathcal{X}}$ is well-estimated. Consequently, in
    these tasks, LC-Boost achieves performance that is equal to or slightly better
    than the brute-force setting.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S2.F2 "图2 ‣ 实证分析 ‣ 2.2 初步研究：大多数长上下文任务是否可通过短上下文解决？ ‣ 2 LC-Boost ‣ 长LLMs是否对长上下文任务是必需的？")展示了实验结果，通常表明LC-Boost的表现始终与暴力破解设置一样好或更好。特别是在QA、少样本学习和合成任务等任务中，LC-Boost的表现优于暴力破解设置。这是因为这些任务的分解短上下文在给定输入查询时更可能是相互独立的，这可以通过从长上下文中提取的少量上下文得到充分支持。通过精确定位这些支持的上下文，可以有效过滤掉${\mathcal{X}}$中无关的上下文。因此，在这些任务中，LC-Boost的表现等于或略优于暴力破解设置。
- en: 'Through theoretical analysis, we can posit that long-context tasks are short-context
    solvable if we can estimate a better minimal necessary context $\tilde{\mathcal{X}}$
    from the long context $\mathcal{X}$. This indicates that using short contexts
    can be comparatively more advantageous than using the full context. Therefore,
    we can validate our argument in Section [1](#S1 "1 Introduction ‣ Are Long-LLMs
    A Necessity For Long-Context Tasks?"): most long-context tasks, if not all, are
    short-context solvable.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理论分析，我们可以假设如果我们能够从长上下文$\mathcal{X}$中估算出更好的最小必要上下文$\tilde{\mathcal{X}}$，则长上下文任务是可通过短上下文解决的。这表明，使用短上下文可能比使用完整上下文更具优势。因此，我们可以在第[1](#S1
    "1 引言 ‣ 长LLMs是否对长上下文任务是必需的？")节中验证我们的论点：大多数长上下文任务（如果不是全部的话）是可以通过短上下文解决的。
- en: '2.3 The Proposed Method: LC-Boost'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 提议的方法：LC-Boost
- en: We propose a method called LC-Boost, which utilizes short LLMs to solve general
    long-context tasks. LC-Boost begins with an input query $q$ is infeasible for
    long-context tasks. To address this, we propose solving long-context tasks by
    strategically understanding the decomposed short contexts $\mathcal{X}=\{\mathcal{X}_{1},\cdots,\mathcal{X}_{n}\}$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种名为 LC-Boost 的方法，它利用短 LLMs 来解决一般的长上下文任务。LC-Boost 从输入查询 $q$ 开始，这对于长上下文任务是不可行的。为了解决这个问题，我们建议通过战略性地理解分解的短上下文
    $\mathcal{X}=\{\mathcal{X}_{1},\cdots,\mathcal{X}_{n}\}$ 来解决长上下文任务。
- en: 'LC-Boost achieves this goal through a decision-making process involving iterative
    interactions between LC-Boost and the decomposed short contexts $\{\mathcal{X}_{1},\cdots,\mathcal{X}_{n}\}$,
    employing two types of actions: information access and information utilization.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: LC-Boost 通过涉及 LC-Boost 和分解的短上下文 $\{\mathcal{X}_{1},\cdots,\mathcal{X}_{n}\}$
    之间的迭代交互的决策过程实现这一目标，采用两种类型的动作：信息访问和信息利用。
- en: We denote an action at time step $i$ by $\tilde{{\mathcal{X}}}_{i}$, as well
    as all previous extracted relevant information $\tilde{{\mathcal{X}}}_{1:i-1}$
    denotes LC-Boost’s underlying LLM.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用时间步 $i$ 的动作表示为 $\tilde{{\mathcal{X}}}_{i}$，以及所有之前提取的相关信息 $\tilde{{\mathcal{X}}}_{1:i-1}$
    表示 LC-Boost 的基础 LLM。
- en: 'Predicting the action $a_{i}$; (5) [Merge]: generating relevant context $\tilde{{\mathcal{X}}}_{i}$;
    (6) [Answer]: answering the user query and returning; (7) [Aggregation]: aggregating
    all relevant information and returning. We define our LC-Boost frame in Algorithm [1](#alg1
    "Algorithm 1 ‣ 2.3 The Proposed Method: LC-Boost ‣ 2 LC-Boost ‣ Are Long-LLMs
    A Necessity For Long-Context Tasks?").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 预测动作 $a_{i}$；（5）[Merge]：生成相关上下文 $\tilde{{\mathcal{X}}}_{i}$；（6）[Answer]：回答用户查询并返回；（7）[Aggregation]：聚合所有相关信息并返回。我们在算法[1](#alg1
    "算法 1 ‣ 2.3 提议的方法：LC-Boost ‣ 2 LC-Boost ‣ 长上下文任务是否需要长-LLMs？") 中定义了我们的 LC-Boost
    框架。
- en: Algorithm 1 LC-Boost Framework
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 LC-Boost 框架
- en: '1:  Input: Input query $q$4:Initialize $\text{extracted relevant context}~{}\tilde{{\mathcal{X}}}_{0}\leftarrow\text{None}$
    is [Move]  then  $i\leftarrow i+1$ is [Append]  then generate relevant context
    by $\tilde{{\mathcal{X}}}_{i}=a_{i}({\mathcal{X}}_{i})$  $\in\{$14:  end while15:  return
    answer ${\mathcal{Y}}$'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：输入查询 $q$ 4: 初始化 $\text{提取的相关上下文}~{}\tilde{{\mathcal{X}}}_{0}\leftarrow\text{无}$
    是 [Move] 然后 $i\leftarrow i+1$ 是 [Append] 然后通过 $\tilde{{\mathcal{X}}}_{i}=a_{i}({\mathcal{X}}_{i})$
    生成相关上下文 $\in\{$ 14: 结束 while 15: 返回答案 ${\mathcal{Y}}$'
- en: Though the pre-defined action space $\mathcal{A}$ in a flexible trajectory,
    avoiding the need to browse the entire long context. This makes the information
    accessing process more efficient. (2) Accurate information acquisition: Through
    the [Append] and [Merge] actions, LC-Boost can either independently extract relevant
    information from the current short context, appending it to previously extracted
    information, or merge the current relevant information into the previous relevant
    information. This capability allows LC-Boost to acquire relevant information in
    a compatible manner, making it adaptable to many knowledge-intensive tasks. and
    (3) Dynamic answering: Using the [Answer] and [Aggregate] actions, LC-Boost can
    dynamically utilize the acquired relevant information to produce the target form
    of the answer (e.g., a short answer for QA tasks via the [Answer] action, or a
    long answer for summarization tasks via the [Aggregate] action).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预定义的动作空间 $\mathcal{A}$ 在灵活的轨迹中，避免了浏览整个长上下文的需要。这使得信息访问过程更加高效。（2）准确的信息获取：通过[Append]和[Merge]动作，LC-Boost
    可以独立从当前短上下文中提取相关信息，将其附加到之前提取的信息中，或者将当前相关信息合并到之前的相关信息中。这种能力使 LC-Boost 能够以兼容的方式获取相关信息，使其适应许多知识密集型任务。（3）动态回答：使用[Answer]和[Aggregate]动作，LC-Boost
    可以动态利用获取的相关信息生成目标形式的答案（例如，通过[Answer]动作生成短答案用于 QA 任务，或通过[Aggregate]动作生成长答案用于总结任务）。
- en: 'In our pilot study depicted in Figure [2](#S2.F2 "Figure 2 ‣ Empirical Analysis
    ‣ 2.2 Pilot Study: Are Most Long-Context Tasks Short-Context Solvable? ‣ 2 LC-Boost
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?"), we observe that while GPT-3.5
    serves as an inferior foundation model compared to GPT-4, it still demonstrates
    significant effectiveness when incorporated with LC-Boost. Given considerations
    of efficiency and cost-effectiveness, we employ GPT-3.5 as the foundation model
    for LC-Boost in the subsequent experiments. Besides, we show the prompts used
    in LC-Boost in Appendix [B](#A2 "Appendix B Implementation Details ‣ Are Long-LLMs
    A Necessity For Long-Context Tasks?").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们展示于图 [2](#S2.F2 "Figure 2 ‣ Empirical Analysis ‣ 2.2 Pilot Study: Are Most
    Long-Context Tasks Short-Context Solvable? ‣ 2 LC-Boost ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?") 的初步研究中，我们观察到虽然GPT-3.5作为基础模型不如GPT-4，但在与LC-Boost结合时仍显示出显著的效果。考虑到效率和性价比，我们在后续实验中采用GPT-3.5作为LC-Boost的基础模型。此外，我们在附录 [B](#A2
    "Appendix B Implementation Details ‣ Are Long-LLMs A Necessity For Long-Context
    Tasks?")中展示了LC-Boost使用的提示。'
- en: 3 Experiments
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Experiment Settings
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'We evaluate LC-Boost and baseline models on 12 datasets, including: (1) Single-Doc
    QA: NarrativeQA (Kočiský et al., [2017](#bib.bib17)), Qasper (Dasigi et al., [2021](#bib.bib18)),
    and MultiFieldQA (Bai et al., [2023](#bib.bib1)). (2) Multi-Doc QA: HotpotQA (Yang
    et al., [2018](#bib.bib19)), 2WikiMQA (Ho et al., [2020](#bib.bib20)), and MuSiQue (Trivedi
    et al., [2022](#bib.bib21)). (3) Summarization: GovReport (Huang et al., [2021](#bib.bib22))
    and MultiNews (Fabbri et al., [2019](#bib.bib23)). (4) Few-shot Learning: SAMSum (Gliwa
    et al., [2019](#bib.bib24)). (5) Synthetic Task: Passage Count (Bai et al., [2023](#bib.bib1))
    and Self-Constructed Dataset. (6) Code Completion: LCC (Guo et al., [2023](#bib.bib25)).
    More details about the evaluation datasets and metrics are introduced in Appendix [A](#A1
    "Appendix A More details of the Datasets ‣ Are Long-LLMs A Necessity For Long-Context
    Tasks?").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在12个数据集上评估了LC-Boost和基线模型，包括：（1）单文档问答：NarrativeQA (Kočiský et al., [2017](#bib.bib17))，Qasper (Dasigi
    et al., [2021](#bib.bib18))，和MultiFieldQA (Bai et al., [2023](#bib.bib1))。（2）多文档问答：HotpotQA (Yang
    et al., [2018](#bib.bib19))，2WikiMQA (Ho et al., [2020](#bib.bib20))，和MuSiQue (Trivedi
    et al., [2022](#bib.bib21))。（3）总结：GovReport (Huang et al., [2021](#bib.bib22))和MultiNews (Fabbri
    et al., [2019](#bib.bib23))。（4）少量样本学习：SAMSum (Gliwa et al., [2019](#bib.bib24))。（5）合成任务：Passage
    Count (Bai et al., [2023](#bib.bib1))和自构建数据集。（6）代码补全：LCC (Guo et al., [2023](#bib.bib25))。关于评估数据集和指标的更多细节见附录 [A](#A1
    "Appendix A More details of the Datasets ‣ Are Long-LLMs A Necessity For Long-Context
    Tasks?")。
- en: 'We compare our LC-Boost with three types of models: (1) Short LLMs (defined
    as with context length $<$ 32K): LongChat-v1.5-7B-32K (Li et al., [2023b](#bib.bib27)),
    Mistral-7B-Instruct-v0.2-32K (Jiang et al., [2023a](#bib.bib28)), Llama3-8B-80K Zhang
    et al. ([2024b](#bib.bib29)), Phi-3-mini-128K (Abdin et al., [2024](#bib.bib30))
    and Yi-9B-200K (AI et al., [2024](#bib.bib31)); (3) Closed-Source LLMs: DeepSeek-v2 (236B
    MoE model, ranks top-tier in MT-Bench) (DeepSeek-AI, [2024](#bib.bib32)), Claude-3-Haiku²²2[https://www.anthropic.com/claude](https://www.anthropic.com/claude)
    and GPT-3.5-turbo-16K³³3[https://platform.openai.com/docs/models](https://platform.openai.com/docs/models).
    In the experiments, if the context length exceed the model’s length limit, following Bai
    et al. ([2023](#bib.bib1)), we truncate the context from the middle since the
    front and end of the context may contain crucial information. We provide further
    implementation details in Appendix [B](#A2 "Appendix B Implementation Details
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LC-Boost与三种类型的模型进行了比较：（1）短文档LLMs（定义为上下文长度$<$ 32K）：LongChat-v1.5-7B-32K (Li
    et al., [2023b](#bib.bib27))，Mistral-7B-Instruct-v0.2-32K (Jiang et al., [2023a](#bib.bib28))，Llama3-8B-80K (Zhang
    et al., [2024b](#bib.bib29))，Phi-3-mini-128K (Abdin et al., [2024](#bib.bib30))和Yi-9B-200K (AI
    et al., [2024](#bib.bib31))；（3）封闭源LLMs：DeepSeek-v2 (236B MoE模型，在MT-Bench中排名顶级) (DeepSeek-AI,
    [2024](#bib.bib32))，Claude-3-Haiku²²2[https://www.anthropic.com/claude](https://www.anthropic.com/claude)和GPT-3.5-turbo-16K³³3[https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)。在实验中，如果上下文长度超过模型的长度限制，我们按照Bai
    et al. ([2023](#bib.bib1))的方法，从中间截断上下文，因为上下文的前后可能包含关键信息。我们在附录 [B](#A2 "Appendix
    B Implementation Details ‣ Are Long-LLMs A Necessity For Long-Context Tasks?")中提供了进一步的实施细节。
- en: 3.2 Main Results
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 主要结果
- en: 'Table 1: Main experiment results. The best results are in bold and the secondary
    results are marked with underline. We report the average scores (%) on the main
    tasks. The detailed scores over all dataset are shown in Table [3](#A0.T3 "Table
    3 ‣ Are Long-LLMs A Necessity For Long-Context Tasks?").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：主要实验结果。最佳结果以**粗体**显示，次要结果以*下划线*标记。我们报告了主要任务上的平均分数（%）。所有数据集上的详细分数见表 [3](#A0.T3
    "Table 3 ‣ Are Long-LLMs A Necessity For Long-Context Tasks?")。
- en: '| Models | Single-Doc | Multi-Doc | Summ. | Few-shot | Synthetic | Code |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 单文档 | 多文档 | 摘要 | 少样本 | 合成 | 代码 |'
- en: '| Short LLMs (Context Length $<$ 32K) |  |  |  |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 短 LLMs（上下文长度 $<$ 32K） |  |  |  |  |'
- en: '| Llama2-7B-Chat-4K | 24.9 | 22.5 | 26.6 | 40.7 | 6.3 | 52.4 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-Chat-4K | 24.9 | 22.5 | 26.6 | 40.7 | 6.3 | 52.4 |'
- en: '| Llama3-8B-Instruct-8K | 37.3 | 36.0 | 26.5 | 42.7 | 15.0 | 57.5 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Instruct-8K | 37.3 | 36.0 | 26.5 | 42.7 | 15.0 | 57.5 |'
- en: '| Vicuna-v1.5-7B-16K | 28.0 | 18.6 | 27.5 | 40.8 | 8.9 | 51.0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-v1.5-7B-16K | 28.0 | 18.6 | 27.5 | 40.8 | 8.9 | 51.0 |'
- en: '| Long LLMs (Context Length $\geq$ 32K) |  |  |  |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 长 LLMs（上下文长度 $\geq$ 32K） |  |  |  |  |'
- en: '| LongChat-v1.5-7B-32K | 28.7 | 20.6 | 28.6 | 34.2 | 6.8 | 53.0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-v1.5-7B-32K | 28.7 | 20.6 | 28.6 | 34.2 | 6.8 | 53.0 |'
- en: '| Mistral-7B-Instruct-v0.2-32K | 31.9 | 26.0 | 29.3 | 43.0 | 14.0 | 55.4 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct-v0.2-32K | 31.9 | 26.0 | 29.3 | 43.0 | 14.0 | 55.4 |'
- en: '| Llama3-8B-80K | 43.6 | 43.1 | 30.2 | 42.9 | 19.6 | 53.6 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-80K | 43.6 | 43.1 | 30.2 | 42.9 | 19.6 | 53.6 |'
- en: '| Phi-3-mini-128K | 33.5 | 38.2 | 28.8 | 36.0 | 19.9 | 60.1 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3-mini-128K | 33.5 | 38.2 | 28.8 | 36.0 | 19.9 | 60.1 |'
- en: '| Yi-9B-200K | 29.6 | 38.7 | 28.4 | 14.6 | 6.5 | 72.1 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 29.6 | 38.7 | 28.4 | 14.6 | 6.5 | 72.1 |'
- en: '| Closed-Source LLMs |  |  |  |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 封闭源 LLMs |  |  |  |  |'
- en: '| DeepSeek-v2 (32K) | 37.6 | 49.1 | 30.8 | 39.3 | 14.5 | 37.0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-v2 (32K) | 37.6 | 49.1 | 30.8 | 39.3 | 14.5 | 37.0 |'
- en: '| Claude-3-Haiku (200K) | 41.9 | 45.4 | 30.1 | 7.2 | 25.5 | 16.9 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Haiku (200K) | 41.9 | 45.4 | 30.1 | 7.2 | 25.5 | 16.9 |'
- en: '| GPT-3.5-turbo-16K | 39.8 | 38.7 | 28.1 | 41.7 | 18.7 | 54.7 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-16K | 39.8 | 38.7 | 28.1 | 41.7 | 18.7 | 54.7 |'
- en: '| LC-Boost (4K) | 47.8 | 56.4 | 31.8 | 44.1 | 27.5 | 59.0 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost (4K) | 47.8 | 56.4 | 31.8 | 44.1 | 27.5 | 59.0 |'
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Are Long-LLMs
    A Necessity For Long-Context Tasks?") shows the overall experimental results for
    all models across all tasks. From the table, we derive several key findings: First,
    LC-Boost, with a context length of 4K, outperforms all baseline models in all
    tasks except for the Code Completion task. This result verifies LC-Boost’s capability
    to effectively solve long-context tasks by strategically processing decomposed
    short contexts. Second, long LLMs generally perform better than short LLMs, indicating
    the effectiveness of fine-tuning LLMs to adapt to long contexts. However, the
    performance of long LLMs is not consistently stable across different tasks. For
    example, Yi-9B-200K excels in the Code Completion task but does not show consistent
    performance in other tasks such as single-doc QA, few-shot learning, and synthetic
    tasks. This inconsistency suggests that adapting LLMs to long contexts may compromise
    their general abilities. Last, LC-Boost consistently surpasses its underlying
    LLM, GPT-3.5-turbo-16K, across all tasks by a notable margin. This demonstrates
    that LC-Boost can achieve improved performance while simultaneously reducing resource
    costs, making LC-Boost an environmentally friendly method.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S3.T1 "Table 1 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?") 显示了所有模型在所有任务上的总体实验结果。从表中，我们得出几个关键发现：首先，LC-Boost 在4K上下文长度下，在所有任务中表现优于所有基线模型，除了代码补全任务。这一结果验证了
    LC-Boost 在通过策略性处理分解的短上下文来有效解决长上下文任务的能力。其次，长 LLMs 通常比短 LLMs 表现更好，这表明微调 LLMs 以适应长上下文的有效性。然而，长
    LLMs 在不同任务上的表现并不总是稳定。例如，Yi-9B-200K 在代码补全任务中表现出色，但在单文档 QA、少样本学习和合成任务等其他任务中表现不一致。这种不一致性表明，将
    LLMs 适应长上下文可能会损害其通用能力。最后，LC-Boost 在所有任务中都显著超越其基础 LLM，GPT-3.5-turbo-16K。这表明 LC-Boost
    能在提高性能的同时减少资源成本，使其成为一种环保的方法。
- en: '3.3 Ablation Study: Dynamic is Important'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 消融研究：动态性的重要性
- en: 'To investigate the necessity of LC-Boost’s design, we conduct ablation studies
    by changing LC-Boost’s action space $\mathcal{A}$ relevant short contexts and
    selectively process a few of them. (8): Brute-force: Directly produce the answer
    based on the entire long context. (9) Random: For each short context, randomly
    select an action. Based on the acquired information from each strategy, LC-Boost
    then selects either the [Answer] or [Aggregation] action to produce the final
    answer.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究 LC-Boost 设计的必要性，我们通过改变 LC-Boost 的动作空间 $\mathcal{A}$ 相关短上下文并有选择地处理其中一些来进行消融研究。(8):
    粗暴法: 基于整个长上下文直接生成答案。(9) 随机: 对于每个短上下文，随机选择一个动作。根据从每种策略获得的信息，LC-Boost 然后选择 [答案]
    或 [聚合] 动作来生成最终答案。'
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Ablation Study: Dynamic is Important ‣ 3
    Experiments ‣ Are Long-LLMs A Necessity For Long-Context Tasks?") illustrates
    the results, from which we find that: (1) Compared to fixed processing strategies,
    LC-Boost customizes the action trajectory for each query, resulting in notable
    performance improvements. This finding emphasizes the importance of the dynamic
    capabilities of LC-Boost. (2) LC-Boost is particularly effective in single-doc
    QA and multi-doc QA tasks, as it can accurately select the minimal necessary context
    required to answer the input query, filtering out irrelevant information from
    the long context. (3) In the few-shot learning task, LC-Boost does not significantly
    outperform the fixed strategies. This is attributed to the numerous in-context
    examples provided within the task, which offer substantial guidance, thus diminishing
    the impact of the number of in-context examples on the final performance.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S3.F3 "图 3 ‣ 3.3 消融研究: 动态性的重要性 ‣ 3 实验 ‣ 长上下文任务是否需要长-LLMs?") 说明了结果，从中我们发现：（1）与固定处理策略相比，LC-Boost
    为每个查询定制了行动轨迹，带来了显著的性能提升。这一发现强调了 LC-Boost 动态能力的重要性。（2）LC-Boost 在单文档问答和多文档问答任务中特别有效，因为它能够准确选择回答输入查询所需的最小必要上下文，从长上下文中过滤出无关信息。（3）在少量示例学习任务中，LC-Boost
    的表现并未显著优于固定策略。这归因于任务中提供的众多上下文示例，这些示例提供了大量指导，从而减小了上下文示例数量对最终性能的影响。'
- en: '![Refer to caption](img/d8609ead10150d897e2e9093769ddb4d.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d8609ead10150d897e2e9093769ddb4d.png)'
- en: 'Figure 3: Performance comparison on different context processing strategies
    in the ablation study. NarrativeQA (left) is a single-doc QA task. HotpotQA (middle)
    is a multi-doc QA task. SamSUM (right) is a few-shot learning task.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 在消融研究中对不同上下文处理策略的性能比较。NarrativeQA（左侧）是单文档问答任务。HotpotQA（中间）是多文档问答任务。SamSUM（右侧）是少量示例学习任务。'
- en: '3.4 Case Study: Model Behavior Analysis on Self-Construct Dataset'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 案例研究：自构数据集上的模型行为分析
- en: 'Table 2: Case study on the self-constructed dataset. Correct answers are marked
    in teal, incorrect answers in red, and ambiguous answers in orange.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 自构数据集上的案例研究。正确答案用青绿色标记，错误答案用红色标记，模糊答案用橙色标记。'
- en: '| Query: How many papers in ACL 2023 only have one author? |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 查询: ACL 2023 中有多少篇论文只有一个作者？ |'
- en: '| Context: Full accepted paper list in ACL 2023 main conference. (Context length:
    45K) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 上下文: ACL 2023 主会议的所有接受论文列表。（上下文长度: 45K） |'
- en: '| Ground-truth target: 8 papers |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 真实目标: 8 篇 |'
- en: '| Phi-3-mini-128K: 11 papers  GPT-3.5-turbo-16K: 0 papers  Claude-3-Haiku-200K:
    1 papers (Acc. Score: 0) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3-mini-128K: 11 篇  GPT-3.5-turbo-16K: 0 篇  Claude-3-Haiku-200K: 1 篇（准确率得分:
    0） |'
- en: '| LC-Boost’s action trajectory: [Task Reasoning]  $\rightarrow$  [Aggregation]
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost 的行动轨迹: [任务推理]  $\rightarrow$  [聚合] |'
- en: '| LC-Boost: 8 papers (Acc. Score: 1) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost: 8 篇（准确率得分: 1） |'
- en: '| Query: List all people names that are petrified, separated by comma. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 查询: 列出所有被石化的人名，用逗号分隔。 |'
- en: '| Context: Full content of Harry Potter and the Chamber of Secrets. (Context
    length: 122.6K) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 上下文: 《哈利·波特与密室》的完整内容。（上下文长度: 122.6K） |'
- en: '| Ground-truth target: Colin Creevey, Justin Finch-Fletchley, Penelope Clearwater,
    Hermione Granger |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 真实目标: 科林·克里维, 贾斯廷·芬奇-弗莱奇利, 佩内洛普·克里亚沃, 赫敏·格兰杰 |'
- en: '| Phi-3-mini-128K: Hermione Granger, Ginny Weasley, Mrs Norris (F1-Score: 0.29)
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3-mini-128K: 赫敏·格兰杰, 金妮·韦斯莱, 诺里斯夫人（F1-得分: 0.29） |'
- en: '| GPT-3.5-turbo-16K: Colin Creevey, Mrs Norris (F1-Score: 0.33) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-16K: 科林·克里维, 诺里斯夫人（F1-得分: 0.33） |'
- en: '| Claude-3-Haiku-200K: Nick, Hermione, Ron (F1-Score: 0.18) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Haiku-200K: 尼克, 赫敏, 罗恩（F1-得分: 0.18） |'
- en: '| LC-Boost’s action trajectory: [Task Reasoning]  $\rightarrow$  [Aggregation]
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost 的行动轨迹: [任务推理]  $\rightarrow$  [聚合] |'
- en: '| LC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs
    Norris (F1-Score: 0.71) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost: Colin Creevey, Penelope Clearwater, Hermione Granger, Nick, Mrs
    Norris (F1-Score: 0.71) |'
- en: 'In Table [2](#S3.T2 "Table 2 ‣ 3.4 Case Study: Model Behavior Analysis on Self-Construct
    Dataset ‣ 3 Experiments ‣ Are Long-LLMs A Necessity For Long-Context Tasks?"),
    we present two case studies from the self-constructed dataset. These cases are
    particularly challenging as they require reasoning across the entire long context.
    Despite having sufficient context size, LLMs struggle to generate correct responses.
    In contrast, LC-Boost dynamically customizes solutions for each case, thereby
    effectively solving the problems using a shorter context length.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[2](#S3.T2 "表 2 ‣ 3.4 案例研究：自构数据集上的模型行为分析 ‣ 3 实验 ‣ 长-LLMs 是否是长上下文任务的必要条件？")中，我们展示了来自自构数据集的两个案例研究。这些案例特别具有挑战性，因为它们需要跨越整个长上下文进行推理。尽管上下文大小足够，LLMs
    仍然难以生成正确的响应。相比之下，LC-Boost 为每个案例动态定制解决方案，从而有效地使用较短的上下文长度解决问题。
- en: For the first query, LC-Boost performs [Append] or [Move] actions across all
    short context along with a rewritten query, "Extract paper information in the
    following list that have only one author," derived via [Task Reasoning]. After
    processing all short contexts, LC-Boost employs the [Aggregation] action to compile
    the final answer. This approach simplifies the task compared to directly extracting
    a numeric answer from the entire long context, mimicking the human process of
    reading comprehension and thereby producing accurate results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个查询，LC-Boost 执行[附加]或[移动]操作来处理所有短上下文，并结合重写后的查询，“提取以下列表中仅有一个作者的论文信息”，该查询通过[任务推理]得出。在处理所有短上下文后，LC-Boost
    使用[聚合]操作来汇总最终答案。这种方法相比于直接从整个长上下文中提取数值答案，简化了任务，模拟了人类阅读理解的过程，从而产生准确的结果。
- en: In the second case, the query necessitates conditional reasoning on each short
    context. As highlighted in previous research (Liu et al., [2023](#bib.bib6)),
    reasoning directly from the entire context risks losing crucial information, particularly
    in the middle of the long context. Thus LLMs tend to miss key details such as
    people’s names. LC-Boost addresses this issue by processing only one short context
    at a step where it extracts information from arbitrary position of the long text
    with equal accuracy. Additionally, answers marked in orange include non-human
    names (e.g., cat, ghost) that are misconstrued as people, illustrating a common
    challenge where models fail to differentiate in-depth entity properties.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，查询需要对每个短上下文进行条件推理。正如之前的研究（Liu et al., [2023](#bib.bib6)）所强调的，直接从整个上下文推理有丢失关键信息的风险，特别是在长上下文的中间部分。因此，LLMs
    倾向于遗漏关键细节，例如人名。LC-Boost 通过逐步处理一个短上下文，提取长文本中任意位置的信息，同时保持相同的准确性，从而解决了这一问题。此外，标记为橙色的答案包括非人名（例如猫、鬼魂），这些被误解为人名，显示出模型在区分实体属性上的常见挑战。
- en: 3.5 Context be Short, Energy be Saved!
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 上下文短，节能！
- en: Recently, we have witnessed the remarkable success of LLMs, which are becoming
    an indispensable part of our daily lives. We believe that in the near future,
    LLMs will become as ubiquitous as electricity or gas supply, serving as fundamental
    infrastructure in human society. At that point, the energy consumption of LLMs
    will emerge as a significant environmental concern. Therefore, it is imperative
    for the research community to focus on reducing the energy consumption associated
    with these models. Figure [4](#S3.F4 "Figure 4 ‣ 3.5 Context be Short, Energy
    be Saved! ‣ 3 Experiments ‣ Are Long-LLMs A Necessity For Long-Context Tasks?")
    presents an analysis of energy consumption, comparing the brute-force approach
    with our LC-Boost method. The $y$, assuming the use of an A100 GPU with a compute
    capability of 312 TFLOPS for BFLOAT16 operations and a maximum TDP of 400W⁴⁴4The
    calculation of total float operations is based on the method outlined in [https://www.harmdevries.com/post/context-length/](https://www.harmdevries.com/post/context-length/).
    The practical energy consumption is estimated by recording the GPU time and GPU
    power during inference with different context lengths. We use a Llama2-7B-128K (Peng
    et al., [2023](#bib.bib33)) and a Llama2-7B-chat-4K (Touvron et al., [2023a](#bib.bib3))
    for the brute-force setting and LC-Boost, respectively.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们目睹了大型语言模型（LLMs）的显著成功，这些模型正成为我们日常生活中不可或缺的一部分。我们相信，在不久的将来，LLMs将像电力或天然气供应一样普及，成为人类社会的基础设施。到那时，LLMs的能源消耗将成为一个重要的环境问题。因此，研究界必须重点关注减少这些模型的能源消耗。图[4](#S3.F4
    "图 4 ‣ 3.5 缩短上下文，节省能源！ ‣ 3 实验 ‣ 长上下文任务是否需要长LLMs？")展示了能源消耗的分析，对比了暴力破解方法和我们的LC-Boost方法。假设使用具有312
    TFLOPS计算能力的A100 GPU进行BFLOAT16操作，最大TDP为400W⁴⁴4总浮点运算的计算基于[https://www.harmdevries.com/post/context-length/](https://www.harmdevries.com/post/context-length/)中概述的方法。实际能源消耗通过记录在不同上下文长度下推理的GPU时间和GPU功率来估算。我们分别使用Llama2-7B-128K（Peng等，[2023](#bib.bib33)）和Llama2-7B-chat-4K（Touvron等，[2023a](#bib.bib3)）进行暴力破解设置和LC-Boost。
- en: '![Refer to caption](img/1035cf4d1582181cfc660b65efb2cc39.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1035cf4d1582181cfc660b65efb2cc39.png)'
- en: 'Figure 4: Energy consumption analysis.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：能源消耗分析。
- en: Figure [4](#S3.F4 "Figure 4 ‣ 3.5 Context be Short, Energy be Saved! ‣ 3 Experiments
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?") clearly indicates that longer
    context lengths significantly increase energy consumption with the brute-force
    method, especially evident in practical measurements. This difference is primarily
    due to the need to distribute sequence activation tensors across multiple GPUs
    in practical experiment, with tensor I/O exacerbating inference latency and thereby
    inflating energy costs. In contrast, our LC-Boost method, working with 4K context
    lengths, shows only a mild increase in energy consumption across contexts, thereby
    confirming its energy efficiency while maintaining comparable or superior performance
    on long-context tasks. We also provide an analysis on token consumption in Appendix [C](#A3
    "Appendix C Token Consumption Analysis ‣ Are Long-LLMs A Necessity For Long-Context
    Tasks?").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S3.F4 "图 4 ‣ 3.5 缩短上下文，节省能源！ ‣ 3 实验 ‣ 长上下文任务是否需要长LLMs？")清楚地表明，较长的上下文长度显著增加了暴力破解方法的能源消耗，尤其在实际测量中表现得尤为明显。这种差异主要是由于在实际实验中需要将序列激活张量分布到多个GPU上，张量I/O加剧了推理延迟，从而增加了能源成本。相比之下，我们的LC-Boost方法在4K上下文长度下，仅在上下文中显示出轻微的能源消耗增加，从而确认了其在保持长上下文任务中表现相当或更优的同时，具有较高的能源效率。我们还在附录[C](#A3
    "附录 C 令牌消耗分析 ‣ 长上下文任务是否需要长LLMs？")中提供了令牌消耗的分析。
- en: 4 Related Works
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Dealing with long contexts is a fundamental research problem for LLMs, as many
    real-world applications involve long-context inputs (Li et al., [2023a](#bib.bib7);
    Fu et al., [2024](#bib.bib34)). The most direct approach to address long-context
    tasks is to increase the working context size of LLMs (Abdin et al., [2024](#bib.bib30);
    AI et al., [2024](#bib.bib31); Li et al., [2023a](#bib.bib7); Cai et al., [2024](#bib.bib35)).
    A year ago, significant research efforts focused on extending the working context
    size of LLMs from 4K to 32K (Jiang et al., [2023a](#bib.bib28); Li et al., [2023b](#bib.bib27);
    Chen et al., [2023a](#bib.bib36); Du et al., [2022](#bib.bib37)). Currently, many
    popular open-source and close-source LLMs still operate with a context size under
    32K (Touvron et al., [2023a](#bib.bib3); OpenAI, [2023](#bib.bib38)), such as
    GPT-3.5-turbo, which has a 16K context length. Recently, research has shifted
    towards extending LLMs’ working context to the million-level. Notably, GPT-4 was
    updated to a 128K context length not long ago, and the newly released GPT-4o also
    operates with a 128K context. Moreover, several recent open-source LLMs have been
    introduced with context lengths exceeding 100K, for example, the Yi series model
    supports up to 200K (AI et al., [2024](#bib.bib31)), and the Phi-3 model operates
    with 128K (Abdin et al., [2024](#bib.bib30)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 处理长上下文是大型语言模型（LLMs）的一个基本研究问题，因为许多实际应用涉及长上下文输入（Li et al., [2023a](#bib.bib7);
    Fu et al., [2024](#bib.bib34)）。解决长上下文任务的最直接方法是增加LLMs的工作上下文大小（Abdin et al., [2024](#bib.bib30);
    AI et al., [2024](#bib.bib31); Li et al., [2023a](#bib.bib7); Cai et al., [2024](#bib.bib35)）。一年前，重要的研究努力集中在将LLMs的工作上下文大小从4K扩展到32K（Jiang
    et al., [2023a](#bib.bib28); Li et al., [2023b](#bib.bib27); Chen et al., [2023a](#bib.bib36);
    Du et al., [2022](#bib.bib37)）。目前，许多流行的开源和闭源LLMs仍然在32K以下的上下文大小下运行（Touvron et al.,
    [2023a](#bib.bib3); OpenAI, [2023](#bib.bib38)），例如GPT-3.5-turbo，其上下文长度为16K。最近，研究已转向将LLMs的工作上下文扩展到百万级。值得注意的是，GPT-4不久前已更新至128K的上下文长度，新发布的GPT-4o也以128K上下文运行。此外，一些近期的开源LLMs被推出，支持超过100K的上下文长度，例如，Yi系列模型支持高达200K（AI
    et al., [2024](#bib.bib31)），Phi-3模型则以128K运行（Abdin et al., [2024](#bib.bib30)）。
- en: Instead of merely increasing the context length, another approach to address
    long-context tasks involves extracting a short surrogate context from the full
    context. This includes techniques like retrieval-augmented generation (RAG) and
    context refinement methods (Izacard and Grave, [2021a](#bib.bib39); Gao et al.,
    [2024](#bib.bib40); Wang et al., [2023](#bib.bib41); Qian et al., [2024](#bib.bib42)).
    However, many of these methods utilize task-specific strategies to manage the
    long context. For instance, RAG methods often deploy retrievers to select relevant
    context chunks as supporting evidence (Izacard and Grave, [2021b](#bib.bib43);
    Xu et al., [2023b](#bib.bib44); Jiang et al., [2023b](#bib.bib45)). Recent studies
    have criticized the chunking process in RAG for undermining the semantic coherence
    of the long context and have proposed chunking-free methods to refine the long
    context into a concise surrogate context (Qian et al., [2024](#bib.bib42); Luo
    et al., [2024](#bib.bib46)). Furthermore, some studies have also explored sequential
    processing strategies, such as Ratner et al. ([2022](#bib.bib47)) and Xu et al.
    ([2023a](#bib.bib10)), to sequentially process the context in a manner that preserves
    its integrity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单纯增加上下文长度外，另一个处理长上下文任务的方法是从完整上下文中提取短的替代上下文。这包括像检索增强生成（RAG）和上下文细化方法（Izacard
    and Grave, [2021a](#bib.bib39); Gao et al., [2024](#bib.bib40); Wang et al., [2023](#bib.bib41);
    Qian et al., [2024](#bib.bib42)）这样的技术。然而，许多这些方法利用特定任务的策略来管理长上下文。例如，RAG方法通常使用检索器选择相关的上下文块作为支持证据（Izacard
    and Grave, [2021b](#bib.bib43); Xu et al., [2023b](#bib.bib44); Jiang et al.,
    [2023b](#bib.bib45)）。近期研究批评了RAG中的块处理过程，认为其削弱了长上下文的语义一致性，并提出了无块处理的方法来将长上下文细化为简洁的替代上下文（Qian
    et al., [2024](#bib.bib42); Luo et al., [2024](#bib.bib46)）。此外，一些研究还探讨了顺序处理策略，例如Ratner
    et al. ([2022](#bib.bib47))和Xu et al. ([2023a](#bib.bib10))，以顺序方式处理上下文，保持其完整性。
- en: Lastly, reasoning-based methods also show significant potential for addressing
    long context tasks (Nakano et al., [2022](#bib.bib48); Yang et al., [2023](#bib.bib49);
    Driess et al., [2023](#bib.bib50)). These methods predominantly employ a decision-making
    process to navigate through the long context sequentially, utilizing reasoning
    techniques such as in-context learning (Dong et al., [2022](#bib.bib51)), chain-of-thought
    (Wei et al., [2022](#bib.bib52)), and self-reflection (Shinn et al., [2023](#bib.bib53)).
    In this paper, LC-Boost incorporates a decision-making process that dynamically
    customizes the action trajectory for each query, thereby offering considerable
    flexibility in accessing and leveraging information to produce the final output
    answer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，基于推理的方法也显示出在处理长上下文任务中的显著潜力（Nakano et al., [2022](#bib.bib48); Yang et al.,
    [2023](#bib.bib49); Driess et al., [2023](#bib.bib50)）。这些方法主要采用决策过程来顺序地导航长上下文，利用诸如上下文学习（Dong
    et al., [2022](#bib.bib51)）、思维链（Wei et al., [2022](#bib.bib52)）和自我反思（Shinn et
    al., [2023](#bib.bib53)）等推理技术。在本文中，LC-Boost 结合了一种动态定制每个查询的动作轨迹的决策过程，从而在访问和利用信息以生成最终输出答案方面提供了相当大的灵活性。
- en: 5 Conclusion
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we argue that most long-context tasks are short-context solvable,
    and we validate this claim through both theoretical and empirical analysis. We
    propose a method called LC-Boost to solve long-context tasks by decomposing the
    long context into short contexts and processing them using a decision-making process.
    We conduct experiments on 12 datasets to compare LC-Boost with long LLMs and other
    baseline models. Empirical results verify LC-Boost’s effectiveness in solving
    long-context tasks. Additionally, we discuss the energy consumption of LC-Boost
    versus long LLMs, demonstrating that LC-Boost can achieve comparable performance
    with significantly less energy consumption. In Appendix [D](#A4 "Appendix D Limitations
    and Broad Impact ‣ Are Long-LLMs A Necessity For Long-Context Tasks?"), we also
    discuss the limitations and broader impact of this paper.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们认为大多数长上下文任务可以通过短上下文解决，我们通过理论和实证分析验证了这一观点。我们提出了一种名为 LC-Boost 的方法，通过将长上下文分解为短上下文并使用决策过程进行处理，来解决长上下文任务。我们在
    12 个数据集上进行了实验，将 LC-Boost 与长 LLMs 和其他基线模型进行比较。实证结果验证了 LC-Boost 在解决长上下文任务中的有效性。此外，我们讨论了
    LC-Boost 与长 LLMs 的能源消耗，展示了 LC-Boost 可以在显著降低能源消耗的情况下实现相当的性能。在附录 [D](#A4 "附录 D 限制和广泛影响
    ‣ 长 LLMs 是否对长上下文任务是必要的？") 中，我们还讨论了本文的局限性和更广泛的影响。
- en: References
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*, 2023.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou 等人。Longbench：一个双语、多任务的长上下文理解基准。*arXiv
    预印本 arXiv:2308.14508*，2023。
- en: 'Zhang et al. (2024a) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. $\infty$bench: Extending long context evaluation beyond 100k tokens, 2024a.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024a) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, 和 Maosong
    Sun。$\infty$bench：扩展超过 100k tokens 的长上下文评估，2024a。
- en: 'Touvron et al. (2023a) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023a.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023a) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等人。Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023a。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等人。Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models, 2020.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu 和 Dario Amodei。神经语言模型的规模定律，2020。
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts, 2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 中间迷失：语言模型如何使用长上下文，2023。
- en: Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length
    of open-source llms truly promise? In *NeurIPS 2023 Workshop on Instruction Tuning
    and Instruction Following*, 2023a.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 开源大语言模型的上下文长度究竟能承诺多长时间？发表于*NeurIPS
    2023 Workshop on Instruction Tuning and Instruction Following*，2023a。
- en: Adolphs (1999) Ralph Adolphs. Social cognition and the human brain. *Trends
    in cognitive sciences*, 3(12):469–479, 1999.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adolphs (1999) Ralph Adolphs. 社会认知与人脑。*趋势在认知科学*，3(12):469–479，1999。
- en: 'Bryant and O’Hallaron (2011) Randal E Bryant and David Richard O’Hallaron.
    *Computer systems: a programmer’s perspective*. Prentice Hall, 2011.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bryant and O’Hallaron (2011) Randal E Bryant 和 David Richard O’Hallaron. *计算机系统：程序员的视角*。Prentice
    Hall，2011。
- en: 'Xu et al. (2023a) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. Retrieval meets Long Context Large Language Models. *arXiv*, 2023a.
    doi: 10.48550/arxiv.2310.03025. Experimental.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023a) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, 和 Bryan
    Catanzaro. 检索遇见长上下文大语言模型。*arXiv*，2023a。doi: 10.48550/arxiv.2310.03025。实验性。'
- en: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination
    in natural language generation. *ACM Computing Surveys*, 55(12):1–38, 2023.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, 和 Pascale Fung. 自然语言生成中的幻觉调查。*ACM
    Computing Surveys*，55(12):1–38，2023。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-Augmented Generation
    for knowledge-intensive NLP tasks. In *Advances in Neural Information Processing
    Systems*, volume 33, pages 9459–9474, 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, Sebastian Riedel, 和 Douwe Kiela. 用于知识密集型NLP任务的检索增强生成。发表于*Advances
    in Neural Information Processing Systems*，第33卷，第9459–9474页，2020年。网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)。
- en: 'Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. Retrieval augmentation reduces hallucination in conversation. In
    *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages
    3784–3803, Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.findings-emnlp.320. URL [https://aclanthology.org/2021.findings-emnlp.320](https://aclanthology.org/2021.findings-emnlp.320).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, 和
    Jason Weston. 检索增强减少对话中的幻觉。发表于*Findings of the Association for Computational Linguistics:
    EMNLP 2021*，第3784–3803页，多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。doi: 10.18653/v1/2021.findings-emnlp.320。网址
    [https://aclanthology.org/2021.findings-emnlp.320](https://aclanthology.org/2021.findings-emnlp.320)。'
- en: Cover (1999) Thomas M Cover. *Elements of information theory*. John Wiley &
    Sons, 1999.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cover (1999) Thomas M Cover. *信息理论要素*。John Wiley & Sons，1999。
- en: Tishby and Zaslavsky (2015) Naftali Tishby and Noga Zaslavsky. Deep learning
    and the information bottleneck principle, 2015.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tishby and Zaslavsky (2015) Naftali Tishby 和 Noga Zaslavsky. 深度学习与信息瓶颈原理，2015。
- en: Mohri et al. (2018) Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
    *Foundations of machine learning*. MIT press, 2018.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohri et al. (2018) Mehryar Mohri, Afshin Rostamizadeh, 和 Ameet Talwalkar. *机器学习基础*。MIT出版社，2018。
- en: Kočiský et al. (2017) Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading
    comprehension challenge, 2017.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kočiský et al. (2017) Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, 和 Edward Grefenstette. NarrativeQA 阅读理解挑战，2017。
- en: 'Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A
    Smith, and Matt Gardner. A dataset of information-seeking questions and answers
    anchored in research papers. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pages 4599–4610, 2021.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah
    A Smith, 和 Matt Gardner. 基于研究论文的信息检索问题和答案数据集。在 *2021年北美计算语言学协会年会：人类语言技术会议论文集*
    中，页面 4599–4610, 2021。
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W.
    Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for
    diverse, explainable multi-hop question answering, 2018.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    W. Cohen, Ruslan Salakhutdinov, 和 Christopher D. Manning. Hotpotqa: 多样化、可解释的多跳问答数据集，2018。'
- en: 'Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.
    Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning
    steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, *Proceedings of
    the 28th International Conference on Computational Linguistics*, pages 6609–6625,
    Barcelona, Spain (Online), December 2020\. International Committee on Computational
    Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL [https://aclanthology.org/2020.coling-main.580](https://aclanthology.org/2020.coling-main.580).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, 和 Akiko Aizawa.
    构建多跳QA数据集以全面评估推理步骤。在 Donia Scott, Nuria Bel, 和 Chengqing Zong 编辑的 *第28届国际计算语言学会议论文集*
    中，页面 6609–6625, 西班牙巴萨罗那（在线），2020年12月。国际计算语言学委员会。doi: 10.18653/v1/2020.coling-main.580。网址
    [https://aclanthology.org/2020.coling-main.580](https://aclanthology.org/2020.coling-main.580)。'
- en: 'Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
    and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition.
    *Transactions of the Association for Computational Linguistics*, 10:539–554, 2022.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
    和 Ashish Sabharwal. Musique: 通过单跳问题组合实现多跳问题。*计算语言学协会会刊*，10:539–554, 2022。'
- en: 'Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji,
    and Lu Wang. Efficient attentions for long document summarization. In Kristina
    Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven
    Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 1419–1436, Online, June 2021\.
    Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112.
    URL [https://aclanthology.org/2021.naacl-main.112](https://aclanthology.org/2021.naacl-main.112).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji,
    和 Lu Wang. 长文档摘要的高效注意力机制。在 Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,
    Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
    和 Yichao Zhou 编辑的 *2021年北美计算语言学协会年会：人类语言技术会议论文集* 中，页面 1419–1436, 在线，2021年6月。计算语言学协会。doi:
    10.18653/v1/2021.naacl-main.112。网址 [https://aclanthology.org/2021.naacl-main.112](https://aclanthology.org/2021.naacl-main.112)。'
- en: 'Fabbri et al. (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and
    Dragomir R. Radev. Multi-news: a large-scale multi-document summarization dataset
    and abstractive hierarchical model, 2019.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri et al. (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, 和
    Dragomir R. Radev. Multi-news: 大规模多文档摘要数据集和抽象层次模型，2019。'
- en: 'Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander
    Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.
    In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu, editors, *Proceedings
    of the 2nd Workshop on New Frontiers in Summarization*, pages 70–79, Hong Kong,
    China, November 2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409.
    URL [https://aclanthology.org/D19-5409](https://aclanthology.org/D19-5409).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, 和 Aleksander
    Wawer. SAMSum 语料库：用于抽象摘要的人类标注对话数据集。在 Lu Wang, Jackie Chi Kit Cheung, Giuseppe
    Carenini, 和 Fei Liu 编辑的 *第2届摘要新前沿研讨会论文集* 中，页面 70–79, 中国香港，2019年11月。计算语言学协会。doi:
    10.18653/v1/D19-5409。网址 [https://aclanthology.org/D19-5409](https://aclanthology.org/D19-5409)。'
- en: 'Guo et al. (2023) Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley.
    Longcoder: A long-range pre-trained language model for code completion, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. (2023) Daya Guo, Canwen Xu, Nan Duan, Jian Yin, 和 Julian McAuley.
    Longcoder: 一种用于代码完成的长范围预训练语言模型，2023。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing。Vicuna: 一个以 90%* chatgpt 质量打动 GPT-4 的开源聊天机器人，2023年3月。网址
    [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: Li et al. (2023b) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length?, June 2023b. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang。开源 llms 在上下文长度上的实际承诺有多长？，2023年6月。网址
    [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat)。
- en: Jiang et al. (2023a) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023a.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023a) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, 等。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Zhang et al. (2024b) Peitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin
    Qian, Qiwei Ye, and Zhicheng Dou. Extending llama-3’s context ten-fold overnight,
    2024b.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024b) Peitian Zhang, Ninglu Shao, Zheng Liu, Shitao Xiao, Hongjin
    Qian, Qiwei Ye, 和 Zhicheng Dou。将 llama-3 的上下文扩展十倍的技术，2024年。
- en: 'Abdin et al. (2024) Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja,
    Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del
    Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek
    Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie
    Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo
    Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen
    Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi,
    Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid
    Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase,
    Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning
    Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua
    Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang,
    Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna
    Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report:
    A highly capable language model locally on your phone, 2024.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdin et al. (2024) Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja,
    Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat
    Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai,
    Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie
    Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg,
    Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett,
    Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis,
    Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi
    Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra,
    Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas
    Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy,
    Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital
    Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel
    Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav,
    Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang,
    Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, 和 Xiren Zhou。Phi-3 技术报告：一款在手机上表现卓越的语言模型，2024年。
- en: 'AI et al. (2024) 01\. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,
    Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open
    foundation models by 01.ai, 2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AI et al. (2024) 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge
    Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, 和 Zonghong Dai。Yi: 01.ai 的开放基础模型，2024年。'
- en: 'DeepSeek-AI (2024) DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient
    mixture-of-experts language model, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepSeek-AI (2024) DeepSeek-AI。Deepseek-v2：一种强大、经济且高效的专家混合语言模型，2024年。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models. In *The Twelfth
    International Conference on Learning Representations*, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan 和 Enrico Shippole.
    Yarn: 大型语言模型的高效上下文窗口扩展。发表于 *第十二届国际学习表示会议*，2023年。'
- en: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context,
    2024.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim 和 Hao Peng。数据工程以扩展语言模型至 128k 上下文，2024年。
- en: Cai et al. (2024) Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen,
    Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical
    report. *arXiv preprint arXiv:2403.17297*, 2024.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人 (2024) Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin
    Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu 等人。Internlm2 技术报告。*arXiv 预印本 arXiv:2403.17297*，2024年。
- en: 'Chen et al. (2023a) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2023a.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2023a) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han 和 Jiaya Jia。Longlora: 高效微调长上下文大型语言模型。发表于 *第十二届国际学习表示会议*，2023a。'
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 320–335, 2022.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人 (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin
    Yang 和 Jie Tang。Glm: 使用自回归空白填充的通用语言模型预训练。发表于 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，页码 320–335，2022年。'
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. [https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf),
    2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。Gpt-4 技术报告。 [https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf)，2023年。
- en: Izacard and Grave (2021a) Gautier Izacard and Edouard Grave. Leveraging passage
    retrieval with generative models for open domain question answering, 2021a.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 和 Grave (2021a) Gautier Izacard 和 Edouard Grave. 利用生成模型进行开放域问答的段落检索，2021a。
- en: 'Gao et al. (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented
    generation for large language models: A survey, 2024.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi
    Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang 和 Haofen Wang。大语言模型的检索增强生成：综述，2024年。
- en: Wang et al. (2023) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez,
    and Graham Neubig. Learning to filter context for retrieval-augmented generation,
    2023.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez 和 Graham
    Neubig。学习过滤上下文以进行检索增强生成，2023年。
- en: Qian et al. (2024) Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, and Zhicheng
    Dou. Grounding language model with chunking-free in-context retrieval, 2024.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 (2024) Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou 和 Zhicheng Dou。通过无块上下文检索进行语言模型的基础化，2024年。
- en: Izacard and Grave (2021b) Gautier Izacard and Edouard Grave. Distilling knowledge
    from reader to retriever for question answering. In *International Conference
    on Learning Representations*, 2021b. URL [https://openreview.net/forum?id=NTEz-6wysdb](https://openreview.net/forum?id=NTEz-6wysdb).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 和 Grave (2021b) Gautier Izacard 和 Edouard Grave。将知识从阅读器提炼至检索器以进行问答。发表于
    *国际学习表示会议*，2021b。URL [https://openreview.net/forum?id=NTEz-6wysdb](https://openreview.net/forum?id=NTEz-6wysdb)。
- en: Xu et al. (2023b) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. Retrieval meets long context large language models. In *The Twelfth
    International Conference on Learning Representations*, 2023b.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023b) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan
    Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi 和 Bryan Catanzaro。检索与长上下文大型语言模型相遇。发表于
    *第十二届国际学习表示会议*，2023b。
- en: Jiang et al. (2023b) Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian
    Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval
    augmented generation. *arXiv preprint arXiv:2305.06983*, 2023b. URL [https://arxiv.org/pdf/2305.06983](https://arxiv.org/pdf/2305.06983).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023b) Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu,
    Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, 和 Graham Neubig。主动检索增强生成。*arXiv 预印本
    arXiv:2305.06983*，2023b年。网址 [https://arxiv.org/pdf/2305.06983](https://arxiv.org/pdf/2305.06983)。
- en: 'Luo et al. (2024) Kun Luo, Zheng Liu, Shitao Xiao, and Kang Liu. Bge landmark
    embedding: A chunking-free embedding method for retrieval augmented long-context
    large language models, 2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等 (2024) Kun Luo, Zheng Liu, Shitao Xiao, 和 Kang Liu。Bge landmark embedding：一种无块的嵌入方法，用于检索增强的长上下文大型语言模型，2024年。
- en: 'Ratner et al. (2022) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    Parallel Context Windows Improve In-Context Learning of Large Language Models.
    *arXiv*, 2022. doi: 10.48550/arxiv.2212.10947. Window.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ratner 等 (2022) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar,
    Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, 和 Yoav Shoham。并行上下文窗口改善大型语言模型的上下文学习。*arXiv*，2022年。doi:
    10.48550/arxiv.2212.10947。窗口。'
- en: 'Nakano et al. (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted
    question-answering with human feedback, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano 等 (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button,
    Matthew Knight, Benjamin Chess, 和 John Schulman。Webgpt：具有人工反馈的浏览器辅助问答，2022年。
- en: 'Yang et al. (2023) Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online
    decision making: Benchmarks and additional opinions. *arXiv preprint arXiv:2306.02224*,
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2023) Hui Yang, Sifu Yue, 和 Yunzhong He。Auto-gpt 用于在线决策：基准测试和附加意见。*arXiv
    预印本 arXiv:2306.02224*，2023年。
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. Palm-e: An embodied multimodal language model. *arXiv preprint
    arXiv:2303.03378*, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess 等 (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,
    等人。Palm-e：一种具身的多模态语言模型。*arXiv 预印本 arXiv:2303.03378*，2023年。
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning.
    *arXiv preprint arXiv:2301.00234*, 2022.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, 和 Zhifang Sui。关于上下文学习的调查。*arXiv 预印本 arXiv:2301.00234*，2022年。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting
    elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle
    Belgrave, and Kyunghyun Cho, editors, *Advances in Neural Information Processing
    Systems*, 2022. URL [https://openreview.net/forum?id=_VjQlMeSB_J](https://openreview.net/forum?id=_VjQlMeSB_J).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed H. Chi, Quoc V Le, 和 Denny Zhou。链式思维提示引发大型语言模型的推理。在 Alice H. Oh, Alekh
    Agarwal, Danielle Belgrave, 和 Kyunghyun Cho 主编的*《神经信息处理系统进展》*中，2022年。网址 [https://openreview.net/forum?id=_VjQlMeSB_J](https://openreview.net/forum?id=_VjQlMeSB_J)。
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等 (2023) Noah Shinn, Beck Labash, 和 Ashwin Gopinath。Reflexion：一种具有动态记忆和自我反思的自主代理。*arXiv
    预印本 arXiv:2303.11366*，2023年。
- en: 'Chen et al. (2023b) Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu
    Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity
    text embeddings through self-knowledge distillation, 2023b.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023b) Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian,
    和 Zheng Liu。Bge m3-embedding：通过自我知识蒸馏实现的多语言、多功能、多粒度文本嵌入，2023b年。
- en: 'Table 3: Main experiment results. The best results are in bold and the secondary
    results are marked with underline. We report the average scores (%) on all tasks.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：主要实验结果。最佳结果用粗体显示，次要结果用下划线标记。我们报告了所有任务的平均分数（%）。
- en: '| Model | Narrative | Qasper | MultiField | Hotpot | MuSiQue | 2Wiki |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 叙述 | Qasper | MultiField | Hotpot | MuSiQue | 2Wiki |'
- en: '| Short LLMs (Context Length $<$ 32K) |  |  |  |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 短期 LLMs（上下文长度 $<$ 32K） |  |  |  |  |'
- en: '| Llama2-7B-Chat-4K | 18.7 | 19.2 | 36.8 | 25.4 | 9.4 | 32.8 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-Chat-4K | 18.7 | 19.2 | 36.8 | 25.4 | 9.4 | 32.8 |'
- en: '| Llama3-8B-Instruct-8K | 21.5 | 43.0 | 47.5 | 47.3 | 23.3 | 37.5 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Instruct-8K | 21.5 | 43.0 | 47.5 | 47.3 | 23.3 | 37.5 |'
- en: '| Vicuna-v1.5-7B-16K | 19.4 | 26.1 | 38.5 | 25.3 | 9.8 | 20.8 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-v1.5-7B-16K | 19.4 | 26.1 | 38.5 | 25.3 | 9.8 | 20.8 |'
- en: '| Long LLMs (Context Length $\geq$ 32K) |  |  |  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 长期 LLM（上下文长度 $\geq$ 32K） |  |  |  |'
- en: '| LongChat-v1.5-7B-32K | 16.9 | 27.7 | 41.4 | 31.5 | 9.7 | 20.6 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-v1.5-7B-32K | 16.9 | 27.7 | 41.4 | 31.5 | 9.7 | 20.6 |'
- en: '| Mistral-7B-Instruct-v0.2-32K | 21.6 | 29.2 | 47.9 | 37.7 | 18.6 | 21.8 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct-v0.2-32K | 21.6 | 29.2 | 47.9 | 37.7 | 18.6 | 21.8 |'
- en: '| Llama3-8B-80K | 28.8 | 47.4 | 54.5 | 55.8 | 27.4 | 46.0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-80K | 28.8 | 47.4 | 54.5 | 55.8 | 27.4 | 46.0 |'
- en: '| Phi-3-mini-128K | 21.0 | 39.4 | 51.5 | 48.1 | 28.2 | 38.1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3-mini-128K | 21.0 | 39.4 | 51.5 | 48.1 | 28.2 | 38.1 |'
- en: '| Yi-9B-200K | 15.6 | 39.3 | 33.8 | 51.4 | 26.6 | 38.2 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 15.6 | 39.3 | 33.8 | 51.4 | 26.6 | 38.2 |'
- en: '| Closed-Source LLMs |  |  |  |  |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 闭源 LLMs |  |  |  |  |'
- en: '| DeepSeek-v2 (32K) | 18.3 | 45.7 | 48.9 | 57.7 | 22.6 | 66.9 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-v2 (32K) | 18.3 | 45.7 | 48.9 | 57.7 | 22.6 | 66.9 |'
- en: '| Claude-3-Haiku (200K) | 30.2 | 44.0 | 51.5 | 51.5 | 32.5 | 52.1 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Haiku (200K) | 30.2 | 44.0 | 51.5 | 51.5 | 32.5 | 52.1 |'
- en: '| GPT-3.5-turbo-16K | 23.6 | 43.3 | 52.3 | 51.6 | 26.9 | 37.7 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-16K | 23.6 | 43.3 | 52.3 | 51.6 | 26.9 | 37.7 |'
- en: '| LC-Boost (4K) | 30.6 | 50.6 | 62.1 | 63.5 | 42.5 | 63.1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost (4K) | 30.6 | 50.6 | 62.1 | 63.5 | 42.5 | 63.1 |'
- en: '| Model | GovReport | MultiNews | SAMSum | LCC | PCount | Self |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GovReport | MultiNews | SAMSum | LCC | PCount | Self |'
- en: '| Short LLMs (Context Length $<$ 32K) |  |  |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 短期 LLM（上下文长度 $<$ 32K） |  |  |  |  |'
- en: '| Llama2-7B-Chat-4K | 27.3 | 25.8 | 40.7 | 52.4 | 2.1 | 10.5 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-Chat-4K | 27.3 | 25.8 | 40.7 | 52.4 | 2.1 | 10.5 |'
- en: '| Llama3-8B-Instruct-8K | 30.1 | 27.6 | 42.7 | 57.5 | 8.0 | 21.9 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Instruct-8K | 30.1 | 27.6 | 42.7 | 57.5 | 8.0 | 21.9 |'
- en: '| Vicuna-v1.5-7B-16K | 27.9 | 27.2 | 40.8 | 51.0 | 6.5 | 11.3 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-v1.5-7B-16K | 27.9 | 27.2 | 40.8 | 51.0 | 6.5 | 11.3 |'
- en: '| Long LLMs (Context Length $\geq$ 32K) |  |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 长期 LLM（上下文长度 $\geq$ 32K） |  |  |  |'
- en: '| LongChat-v1.5-7B-32K | 30.8 | 26.4 | 34.2 | 53.0 | 1.0 | 12.5 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-v1.5-7B-32K | 30.8 | 26.4 | 34.2 | 53.0 | 1.0 | 12.5 |'
- en: '| Mistral-7B-Instruct-v0.2-32K | 31.7 | 26.9 | 43.0 | 55.4 | 2.6 | 25.4 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct-v0.2-32K | 31.7 | 26.9 | 43.0 | 55.4 | 2.6 | 25.4 |'
- en: '| Llama3-8B-80K | 32.3 | 28.1 | 42.9 | 53.6 | 3.5 | 35.7 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-80K | 32.3 | 28.1 | 42.9 | 53.6 | 3.5 | 35.7 |'
- en: '| Phi-3-mini-128K | 32.6 | 24.9 | 36.0 | 60.1 | 3.2 | 36.5 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Phi-3-mini-128K | 32.6 | 24.9 | 36.0 | 60.1 | 3.2 | 36.5 |'
- en: '| Yi-9B-200K | 30.3 | 26.5 | 14.6 | 72.0 | 4.2 | 8.7 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Yi-9B-200K | 30.3 | 26.5 | 14.6 | 72.0 | 4.2 | 8.7 |'
- en: '| Closed-Source LLMs |  |  |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 闭源 LLMs |  |  |  |  |'
- en: '| DeepSeek-v2 (32K) | 35.2 | 26.3 | 39.3 | 37.0 | 12.7 | 16.2 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-v2 (32K) | 35.2 | 26.3 | 39.3 | 37.0 | 12.7 | 16.2 |'
- en: '| Claude-3-Haiku (200K) | 34.1 | 26.1 | 7.2 | 16.9 | 5.0 | 46.0 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Haiku (200K) | 34.1 | 26.1 | 7.2 | 16.9 | 5.0 | 46.0 |'
- en: '| GPT-3.5-turbo-16K | 29.5 | 26.7 | 41.7 | 54.7 | 4.5 | 32.9 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-16K | 29.5 | 26.7 | 41.7 | 54.7 | 4.5 | 32.9 |'
- en: '| LC-Boost (4K) | 34.4 | 29.2 | 44.1 | 59.0 | 7.2 | 47.7 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| LC-Boost (4K) | 34.4 | 29.2 | 44.1 | 59.0 | 7.2 | 47.7 |'
- en: Appendix A More details of the Datasets
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集的更多细节
- en: 'Table 4: Statistical information of the datasets utilized in this paper.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 本文使用的数据集的统计信息。'
- en: '| Dataset | Narrative | Qasper | MultiField | Hotpot | MuSiQue | 2Wiki |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | Narrative | Qasper | MultiField | Hotpot | MuSiQue | 2Wiki |'
- en: '| Num of Samples | 200 | 200 | 150 | 200 | 200 | 200 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 样本数量 | 200 | 200 | 150 | 200 | 200 | 200 |'
- en: '| Ave. Length | 18,409 | 3,619 | 4,559 | 9,151 | 11,214 | 4,887 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 平均长度 | 18,409 | 3,619 | 4,559 | 9,151 | 11,214 | 4,887 |'
- en: '| Metric | F1 | F1 | F1 | F1 | F1 | F1 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | F1 | F1 | F1 | F1 | F1 | F1 |'
- en: '| Dataset | GovReport | MultiNews | SAMSum | PCount | LCC | Self |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | GovReport | MultiNews | SAMSum | PCount | LCC | Self |'
- en: '| Num of Samples | 200 | 200 | 200 | 200 | 500 | 32 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 样本数量 | 200 | 200 | 200 | 200 | 500 | 32 |'
- en: '| Ave. Length | 8,734 | 2,113 | 6,258 | 11,141 | 1,235 | 39,420 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 平均长度 | 8,734 | 2,113 | 6,258 | 11,141 | 1,235 | 39,420 |'
- en: '| Metric | Rouge-L | Rouge-L | Rouge-L | Accuracy | Edit Sim | F1&Accuracy
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | Rouge-L | Rouge-L | Rouge-L | 准确率 | 编辑相似度 | F1&准确率 |'
- en: We evaluated all models on 12 datasets, as shown in Table [4](#A1.T4 "Table
    4 ‣ Appendix A More details of the Datasets ‣ Are Long-LLMs A Necessity For Long-Context
    Tasks?"). Most of these datasets are provided by the LongBench benchmark [Bai
    et al., [2023](#bib.bib1)]. Following LongBench, we used F1-score, accuracy, and
    edit similarity as the evaluation metrics. Additionally, we manually annotated
    a self-constructed dataset comprising long contexts from practical scenarios,
    such as the full schedule of the Olympic Games and the complete list of accepted
    papers at ACL. The queries in the self-constructed dataset involve reasoning over
    the entire long context. For example, “Who has the most accepted papers at ACL
    2023?” These queries require the model to accurately understand the long context
    and perform reasoning, making them highly challenging. The details of the self-constructed
    dataset are in Table [5](#A1.T5 "Table 5 ‣ Appendix A More details of the Datasets
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?").
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 12 个数据集上评估了所有模型，如表[4](#A1.T4 "表 4 ‣ 附录 A 数据集更多细节 ‣ 长文本 LLM 是否对长上下文任务必需？")所示。这些数据集大多数由
    LongBench 基准测试 [Bai et al., [2023](#bib.bib1)] 提供。按照 LongBench 的方法，我们使用了 F1 分数、准确率和编辑相似性作为评估指标。此外，我们手动标注了一个自建数据集，其中包含了实际场景中的长上下文，例如奥运会的完整日程和
    ACL 会议的录用论文完整列表。自建数据集中的查询涉及对整个长上下文进行推理。例如，“谁在 ACL 2023 中有最多的录用论文？”这些查询要求模型准确理解长上下文并进行推理，因此非常具有挑战性。自建数据集的详细信息见表
    [5](#A1.T5 "表 5 ‣ 附录 A 数据集更多细节 ‣ 长文本 LLM 是否对长上下文任务必需？")。
- en: 'Table 5: Data details of the self-constructed dataset.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：自建数据集的数据详情。
- en: '| Source | Length | # Queries | Example Query |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 来源 | 长度 | # 查询 | 示例查询 |'
- en: '| Accepted paper list of ACL 2023 Main Conference | 44,490 | 7 | Who has the
    most accepted paper in ACL 2023? |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ACL 2023 主会议的录用论文列表 | 44,490 | 7 | 谁在 ACL 2023 中有最多的录用论文？ |'
- en: '| The Diamond Sutra | 19,993 | 3 | How many chapters of the Sutra? |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 《金刚经》 | 19,993 | 3 | 经文有多少章？ |'
- en: '| Schedule of The 2024 Olympic Games | 15,844 | 9 | Which day has the most
    gold medal events? |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 2024 年奥运会日程 | 15,844 | 9 | 哪一天的金牌赛事最多？ |'
- en: '| Subtitle of The Big Bang Theory S3E14 | 11,136 | 6 | How long does this episode?
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 《生活大爆炸》第三季第十四集的字幕 | 11,136 | 6 | 这一集的时长是多少？ |'
- en: '| The Little Prince | 22,471 | 4 | How many planets does the little prince
    visit? |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 《小王子》 | 22,471 | 4 | 小王子访问了多少个星球？ |'
- en: '| Harry Potter and the Chamber of Secrets | 122,591 | 3 | How many times has
    the chamber of secret been opened? |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 《哈利·波特与密室》 | 122,591 | 3 | 密室被打开过多少次？ |'
- en: Appendix B Implementation Details
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实现细节
- en: LC-Boost begins with the [Task Understanding] action after receiving the input
    query and context, using the prompt shown in Table [6](#A3.T6 "Table 6 ‣ Appendix
    C Token Consumption Analysis ‣ Are Long-LLMs A Necessity For Long-Context Tasks?").
    If the task does not include an input query, the two columns "Below is the query"
    and "{input_query}" are omitted. Besides, for the synthetic task, we use the prompt
    shown in Table [7](#A3.T7 "Table 7 ‣ Appendix C Token Consumption Analysis ‣ Are
    Long-LLMs A Necessity For Long-Context Tasks?") to reformulate the query for better
    adaptation to LC-Boost. Based on the output of the [Task Understanding] action,
    LC-Boost adopts different strategies to perform the task. Specifically, “option
    [1]” directs LC-Boost to utilize a retriever to rank all chunks of the long context.
    In this paper, we employ BGE-Reranker-Large as the retriever Chen et al. [[2023b](#bib.bib54)].
    For “option [2]” and “option [3]”, LC-Boost uses the prompts shown in Table [10](#A3.T10
    "Table 10 ‣ Appendix C Token Consumption Analysis ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?") and Table [8](#A3.T8 "Table 8 ‣ Appendix C Token Consumption
    Analysis ‣ Are Long-LLMs A Necessity For Long-Context Tasks?") to sequentially
    process each short context, respectively. After processing each short context,
    if the output is not "null", the newly summarized context is added to the "previous
    summarization".
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LC-Boost 在接收到输入查询和上下文后，开始执行 [Task Understanding] 操作，使用表格 [6](#A3.T6 "Table 6
    ‣ Appendix C Token Consumption Analysis ‣ Are Long-LLMs A Necessity For Long-Context
    Tasks?") 中显示的提示。如果任务不包含输入查询，则省略“以下是查询”和“{input_query}”两列。此外，对于合成任务，我们使用表格 [7](#A3.T7
    "Table 7 ‣ Appendix C Token Consumption Analysis ‣ Are Long-LLMs A Necessity For
    Long-Context Tasks?") 中显示的提示来重新表述查询，以更好地适应 LC-Boost。根据 [Task Understanding] 操作的输出，LC-Boost
    采用不同策略来执行任务。具体而言，“选项 [1]”指示 LC-Boost 利用检索器对所有长上下文块进行排序。在本文中，我们采用 BGE-Reranker-Large
    作为检索器 Chen et al. [[2023b](#bib.bib54)]。对于“选项 [2]”和“选项 [3]”，LC-Boost 使用表格 [10](#A3.T10
    "Table 10 ‣ Appendix C Token Consumption Analysis ‣ Are Long-LLMs A Necessity
    For Long-Context Tasks?") 和表格 [8](#A3.T8 "Table 8 ‣ Appendix C Token Consumption
    Analysis ‣ Are Long-LLMs A Necessity For Long-Context Tasks?") 中显示的提示，分别顺序处理每个短上下文。处理完每个短上下文后，如果输出不是
    "null"，则将新总结的上下文添加到“先前总结”中。
- en: Once all short contexts are processed, LC-Boost aggregates all relevant information
    to produce the final answer. At this stage, we use the prompt provided by LongBench,
    replacing the full context with the surrogate context produced by LC-Boost. For
    “option [4]”, LC-Boost utilizes the prompts provided by LongBench to process each
    short context and produces the answer as soon as the proper information is found.
    Table [9](#A3.T9 "Table 9 ‣ Appendix C Token Consumption Analysis ‣ Are Long-LLMs
    A Necessity For Long-Context Tasks?") presents an example prompt from LongBench,
    designed for MultiFieldQA tasks. We modified the prompt by adding the instruction
    “If no answer can be found in the text, please output "null"”. This allows LC-Boost
    to skip irrelevant short contexts, performing the [Move] action. Specifically,
    for the Code Completion task, LC-Boost reversely browses the context code from
    near to far as the near context are more useful to predict the code completion.
    We evaluate all baseline models following the settings provided in LongBench ⁵⁵5[https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench).
    We use a node with 8 A100 80G GPUs to conduct all experiments.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有短上下文被处理完毕，LC-Boost 会汇总所有相关信息以生成最终答案。在此阶段，我们使用 LongBench 提供的提示，将完整上下文替换为
    LC-Boost 生成的替代上下文。对于“选项 [4]”，LC-Boost 利用 LongBench 提供的提示来处理每个短上下文，并在找到适当信息后立即生成答案。表格
    [9](#A3.T9 "Table 9 ‣ Appendix C Token Consumption Analysis ‣ Are Long-LLMs A
    Necessity For Long-Context Tasks?") 展示了一个来自 LongBench 的示例提示，旨在用于 MultiFieldQA
    任务。我们通过添加指令“如果在文本中找不到答案，请输出 'null'”来修改提示。这使得 LC-Boost 能够跳过不相关的短上下文，执行 [Move] 操作。具体而言，对于代码补全任务，LC-Boost
    会从近到远逆向浏览上下文代码，因为近的上下文对于预测代码补全更为有用。我们依据 LongBench 提供的设置评估所有基线模型 ⁵⁵5[https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench)。我们使用一个配备
    8 个 A100 80G GPU 的节点进行所有实验。
- en: Appendix C Token Consumption Analysis
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 标记消耗分析
- en: In Section [3.5](#S3.SS5 "3.5 Context be Short, Energy be Saved! ‣ 3 Experiments
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?"), our analysis confirms that
    LC-Boost significantly reduces energy consumption compared to long LLMs. However,
    most closed-source LLMs, such as the underlying model of LC-Boost, GPT-3.5-turbo,
    charge based on token consumption, e.g., US$0.50 per 1M tokens for input and US$1.50
    per 1M tokens for output⁶⁶6[https://openai.com/api/pricing/](https://openai.com/api/pricing/).
    Consequently, it is crucial to examine whether the decision-making process of
    LC-Boost increases token consumption compared to the brute-force method.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [3.5](#S3.SS5 "3.5 上下文简短，节能！ ‣ 3 实验 ‣ 长上下文任务是否需要长 LLMs？") 节中，我们的分析确认 LC-Boost
    相较于长 LLMs 显著减少了能耗。然而，大多数闭源 LLMs，如 LC-Boost 的基础模型 GPT-3.5-turbo，按照令牌消耗收费，例如，每 1M
    个输入令牌收费 US$0.50，每 1M 个输出令牌收费 US$1.50⁶⁶6[https://openai.com/api/pricing/](https://openai.com/api/pricing/)。因此，必须检查
    LC-Boost 的决策过程是否增加了与蛮力方法相比的令牌消耗。
- en: 'To address this issue, we recorded the end-to-end token consumption for three
    datasets: NarrativeQA, GovReport, and LCC. After token counting, we conclude that
    LC-Boost’s token consumption was 34.1% of the brute-force method’s consumption
    in NarrativeQA, 112% in GovReport, and 29.5% in LCC. These results indicate that
    LC-Boost’s token consumption varies significantly across different tasks. For
    tasks requiring precise context location, such as QA and code completion, LC-Boost
    can respond as soon as the relevant context is identified, thereby avoiding the
    need to process the full context. However, for tasks that necessitate information
    aggregation, such as summarization, LC-Boost may require more tokens for prompts
    in each iteration. In practice, for token-consumption-sensitive LLMs, there might
    be a trade-off between performance and cost-efficiency, which also varies considerably
    across different tasks.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们记录了三组数据集的端到端令牌消耗：NarrativeQA、GovReport 和 LCC。经过令牌计数后，我们得出结论，LC-Boost
    在 NarrativeQA 中的令牌消耗为蛮力方法的 34.1%，在 GovReport 中为 112%，在 LCC 中为 29.5%。这些结果表明，LC-Boost
    的令牌消耗在不同任务中差异显著。对于需要精确上下文定位的任务，如 QA 和代码补全，LC-Boost 能够在相关上下文被识别后立即响应，从而避免处理整个上下文。然而，对于需要信息聚合的任务，如摘要，LC-Boost
    可能需要在每次迭代中使用更多的提示令牌。在实践中，对于对令牌消耗敏感的 LLMs，可能会存在性能与成本效率之间的权衡，这也在不同任务中有很大差异。
- en: 'Table 6: Prompt Template for the [Task Understanding] action.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：用于 [任务理解] 操作的提示模板。
- en: '| You need to process a task with a long context that greatly exceeds your
    context limit. The only feasible way to handle this is by processing the long
    context chunk by chunk. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 你需要处理一个上下文极其长的任务，这远远超出了你的上下文限制。处理这种任务的唯一可行方法是将长上下文分块处理。'
- en: '| Below is the original task prompt: {task_prompt} |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 以下是原始任务提示：{task_prompt} |'
- en: '| Below is the query: {input_query} |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 以下是查询：{input_query} |'
- en: '| You have the following options to process the long context. Choose one of
    them: |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 你有以下选项来处理长上下文。选择其中一个： |'
- en: '| [1]. Retrieve the chunk most relevant to the input query to support answer
    generation. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '[1]. 检索与输入查询最相关的块以支持答案生成。'
- en: '| [2]. Summarize each chunk and then aggregate the summaries after processing
    all chunks. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '[2]. 总结每个块，然后在处理完所有块后汇总总结。'
- en: '| [3]. Extract key sentences from each chunk and then aggregate them after
    processing all chunks. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '[3]. 从每个块中提取关键句子，然后在处理完所有块后汇总它们。'
- en: '| [4]. Sequentially scan chunks and produce the answer as soon as the query
    can be answered. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '[4]. 顺序扫描各个块，并在查询能够回答时立即生成答案。'
- en: '| Below are some examples for reference: |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 以下是一些参考示例： |'
- en: '| The examples begin as follows: {examples} The examples conclude here. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 示例开始如下：{examples} 示例到此结束。 |'
- en: '| Please learn the examples and select one of the options by only outputting
    the corresponding index number. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 请学习这些示例，并仅输出相应的索引号码来选择其中一个选项。 |'
- en: 'Table 7: Query Rewritten Prompt Template for the [Task Understanding] action.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：用于 [任务理解] 操作的查询重写提示模板。
- en: '| You need to process a task with a long context that greatly exceeds your
    context limit. The only feasible way to handle this is by processing the long
    context chunk by chunk. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: 你需要处理一个上下文极其长的任务，这远远超出了你的上下文限制。处理这种任务的唯一可行方法是将长上下文分块处理。
- en: '| Below is the original task prompt: {task_prompt} |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 以下是原始任务提示：{task_prompt} |'
- en: '| Below is the query: {input_query} |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 以下是查询：{input_query} |'
- en: '| You will process the long context with the following strategy: {strategy}
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 你将使用以下策略处理长上下文：{strategy} |'
- en: '| Do you think the the query is proper for processing context chunk? If not,
    rewrite the query. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 你认为这个查询适合处理上下文片段吗？如果不适合，请重写查询。 |'
- en: '| Below are some examples for reference: |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 以下是一些参考示例： |'
- en: '| The examples begin as follows: {examples} The examples conclude here. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 示例开始如下：{examples} 示例到此为止。 |'
- en: '| Please study the examples carefully. If the query needs to be rewritten,
    directly output the revised query. If no revision is necessary, output “null”.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 请仔细研究示例。如果查询需要重写，直接输出修订后的查询。如果不需要修订，则输出“null”。 |'
- en: 'Table 8: Prompt Template for the [Append] action.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: [Append] 动作的提示模板。'
- en: '| You are given an article and a question. Read the article carefully and follow
    my instructions to process it. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 你会得到一篇文章和一个问题。仔细阅读文章，并按照我的指示进行处理。 |'
- en: '| Article: |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 文章： |'
- en: '| The article begins as follows: {article} The article concludes here. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 文章开始如下：{article} 文章到此为止。 |'
- en: '| Question: |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 问题： |'
- en: '| {question} |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| {question} |'
- en: '| Instructions: |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 指示： |'
- en: '| Each sentence in the article is marked with a sentence identifier [si], for
    example [s1]. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 文章中的每个句子都标有句子标识符 [si]，例如 [s1]。 |'
- en: '| Select up to ten key sentences from the article that are most likely to answer
    the question. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 从文章中选择最多十个最可能回答问题的关键句子。 |'
- en: '| Only output the selected sentence identifiers, separated by commas. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 仅输出选定句子的标识符，用逗号分隔。 |'
- en: '| Example: [s39],[s54] |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 示例: [s39],[s54] |'
- en: '| If no sentences are relevant, please output "null". |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 如果没有相关句子，请输出“null”。 |'
- en: 'Table 9: Prompt Template for the MultiFieldQA Task from the LongBench Benchmark.
    Additions made by us are highlighted in blue.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '表9: 来自 LongBench 基准的 MultiFieldQA 任务提示模板。我们做的添加部分以蓝色突出显示。'
- en: '| Read the following text and answer briefly. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 阅读以下文本并简要回答。 |'
- en: '| {context} |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| {context} |'
- en: '| Now, answer the following question based on the above text, only give me
    the answer and do not output any |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 现在，根据上述文本回答以下问题，只给出答案，不要输出任何其他内容。 |'
- en: '| other words. If no answer can be found in the text, please output "null".
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 其他词。如果文本中找不到答案，请输出“null”。 |'
- en: '| Question:{question} |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 问题：{question} |'
- en: '| Answer: |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 回答： |'
- en: 'Table 10: Prompt Template for the [Merge] action.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '表10: [Merge] 动作的提示模板。'
- en: '| You are provided with a portion of an article, a question, and summarization
    of the article’s previous portions. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 你会得到一部分文章、一道问题以及文章之前部分的总结。 |'
- en: '| Read the article portion and follow my instructions to process it. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 阅读文章部分，并按照我的指示处理。 |'
- en: '| Article: |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 文章： |'
- en: '| The article begins as follows: {article} The article concludes here. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 文章开始如下：{article} 文章到此为止。 |'
- en: '| Previous summarization: |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 之前的总结： |'
- en: '| The previous summarization is as follows: {previous_sum} The previous summarization
    concludes here. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 之前的总结如下：{previous_sum} 之前的总结到此为止。 |'
- en: '| Question: |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 问题： |'
- en: '| {question} |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| {question} |'
- en: '| Instruction: |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 指示： |'
- en: '| Summarize the partial article to supplement the previous summarization, which
    can better support the task. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 总结部分文章以补充之前的总结，以更好地支持任务。 |'
- en: '| If no content needs to be supplemented, please output "null". |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 如果没有内容需要补充，请输出“null”。 |'
- en: Appendix D Limitations and Broad Impact
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 限制与广泛影响
- en: 'In this paper, we propose LC-Boost, a method dedicated to solving long-context
    tasks using short contexts. However, there are several limitations we would like
    to address in the future work: (1) Although we conduct comprehensive experiments
    on many tasks and provide theoretical analysis to support our major claim that
    most long-context tasks are short-context solvable, there may be more complicated
    scenarios that require understanding the full context in a brute-force setting.
    LC-Boost might not be able to process such tasks effectively. (2) As mentioned
    in Section [2.3](#S2.SS3 "2.3 The Proposed Method: LC-Boost ‣ 2 LC-Boost ‣ Are
    Long-LLMs A Necessity For Long-Context Tasks?"), LC-Boost selects actions from
    a discrete action space. While we argue that the pre-defined action space is versatile
    enough to handle most scenarios, a more elegant solution would be to predict actions
    in a continuous space. We conducted preliminary experiments to explore allowing
    LC-Boost to prompt itself to predict actions without a predefined action space,
    such as writing prompts or code autonomously. These experiments resulted in highly
    unstable performance, particularly for models like GPT-3.5, as such requirements
    are still challenging. We believe that with a much stronger foundation model,
    LC-Boost could be expected to predict actions in a continuous space. (3) We choose
    GPT-3.5 as the foundation model for LC-Boost, instead of open-source LLMs. The
    reason is that GPT-3.5 is a strong, yet efficient model that can generally understand
    most instructions. However, we found that most open-source LLMs lack these properties
    in a zero-shot setting. Fine-tuning these open-source LLMs might be helpful, but
    constructing such instruction data is infeasible and expensive.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们提出了 LC-Boost，这是一种专门解决使用短上下文进行长上下文任务的方法。然而，我们希望在未来的工作中解决以下几个限制：（1）虽然我们在许多任务上进行了全面的实验，并提供了理论分析来支持我们的主要主张，即大多数长上下文任务是可以通过短上下文解决的，但仍可能存在需要在强行设置中理解完整上下文的更复杂场景。LC-Boost
    可能无法有效处理这些任务。（2）正如第 [2.3](#S2.SS3 "2.3 The Proposed Method: LC-Boost ‣ 2 LC-Boost
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?") 节中提到的，LC-Boost 从离散的动作空间中选择动作。虽然我们认为预定义的动作空间足够灵活以处理大多数场景，但更优雅的解决方案是预测连续空间中的动作。我们进行了初步实验，探索允许
    LC-Boost 自我提示以预测没有预定义动作空间的动作，例如自主编写提示或代码。这些实验的结果表现出高度不稳定，特别是对于像 GPT-3.5 这样的模型，因为这些要求仍然具有挑战性。我们相信，随着基础模型的显著增强，LC-Boost
    可能会期望在连续空间中预测动作。（3）我们选择了 GPT-3.5 作为 LC-Boost 的基础模型，而不是开源 LLM。原因是 GPT-3.5 是一个强大但高效的模型，通常能够理解大多数指令。然而，我们发现大多数开源
    LLM 在零样本设置中缺乏这些特性。微调这些开源 LLM 可能会有所帮助，但构建这样的指令数据是不可行且昂贵的。'
- en: As discussed in Section [3.5](#S3.SS5 "3.5 Context be Short, Energy be Saved!
    ‣ 3 Experiments ‣ Are Long-LLMs A Necessity For Long-Context Tasks?"), LLMs are
    likely to become a fundamental infrastructure in the near future. At that scale,
    their energy consumption will pose significant environmental challenges. As shown
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.5 Context be Short, Energy be Saved! ‣ 3 Experiments
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?"), LC-Boost avoids processing
    long contexts directly by decomposing them into shorter contexts. This approach
    significantly reduces energy consumption as the context length increases, leading
    to substantial positive environmental impacts. We believe that in the future,
    more research will focus on green AI initiatives. This paper could serve as an
    initial spark to inspire further research in this direction, potentially resulting
    in broader social impact.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第 [3.5](#S3.SS5 "3.5 Context be Short, Energy be Saved! ‣ 3 Experiments ‣
    Are Long-LLMs A Necessity For Long-Context Tasks?") 节中讨论的那样，LLM 可能会在不久的将来成为基础设施。到那时，它们的能耗将会带来显著的环境挑战。如图
    [4](#S3.F4 "Figure 4 ‣ 3.5 Context be Short, Energy be Saved! ‣ 3 Experiments
    ‣ Are Long-LLMs A Necessity For Long-Context Tasks?") 所示，LC-Boost 通过将长上下文分解为较短的上下文，避免直接处理长上下文。这种方法显著减少了能耗，因为上下文长度增加，从而产生了实质性的积极环境影响。我们相信，未来更多的研究将关注绿色
    AI 计划。本文可能成为激发进一步研究的初始火花，从而可能产生更广泛的社会影响。
