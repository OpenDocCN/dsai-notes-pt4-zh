- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:01:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:01:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context
    Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型告诉你在哪里合并：适应性KV缓存合并用于长上下文任务
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08454](https://ar5iv.labs.arxiv.org/html/2407.08454)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08454](https://ar5iv.labs.arxiv.org/html/2407.08454)
- en: Zheng Wang¹, Boxiao Jin¹, Zhongzhi Yu¹, Minjia Zhang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 郑王¹，博晓金¹，钟智宇¹，敏佳张²
- en: ¹Georgia Institute of Technology, ²University of Illinois Urbana-Champaign
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹乔治亚理工学院，²伊利诺伊大学厄本那-香槟分校
- en: '{zwang3478, bjin60, zyu401}@gatech.edu, {minjiaz}@illinois.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{zwang3478, bjin60, zyu401}@gatech.edu，{minjiaz}@illinois.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have attracted remarkable attention due to their
    unprecedented performance across a wide range of tasks. However, how to efficiently
    serve LLMs has become a pressing issue because of their huge computational cost
    in their autoregressive generation process. To mitigate computational costs, LLMs
    often employ the *KV Cache* technique to improve the generation speed. While improving
    the computational efficiency, the storage requirements of the KV cache are substantial,
    particularly in long-context scenarios, leading to significant memory consumption.
    Existing KV cache eviction methods often degrade the performance of LLMs in long-context
    scenarios due to the information loss introduced by eviction. In this paper, we
    propose a novel KV cache merging approach, called *KVMerger*, to achieve adaptive
    KV cache compression for long-context tasks without significant performance degradation
    under constrained memory budgets. Our approach is inspired by the intriguing observation
    that key states exhibit high similarity at the token level within a single sequence.
    To facilitate merging, we develop an effective yet straightforward merging set
    identification algorithm to identify suitable KV states for merging. Our merging
    set identification algorithm stimulates the second observation that KV cache sparsity,
    from similarity perspective, is independent of the dataset and remains persistent
    at the model level. Subsequently, we propose a Gaussian kernel weighted merging
    algorithm to selectively merge all states within each merging set. We conduct
    extensive experiments to demonstrate the effectiveness of *KVMerger* for long-context
    tasks under constrained memory budgets, applying it to models including Llama2-7B/13B-chat
    and Mistral-7B-instruct. Using the LongBench and ZeroScroll benchmarks, we compare
    our method with other KV cache compression techniques, including H2O and CaM,
    showing that our method achieves superior performance across tasks with both $50\%$
    KV cache budgets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其在各种任务中的卓越表现而备受关注。然而，由于其自回归生成过程中的巨大计算成本，如何高效服务LLMs成为了一个紧迫的问题。为了降低计算成本，LLMs通常采用*KV缓存*技术来提高生成速度。虽然提高了计算效率，但KV缓存的存储需求巨大，特别是在长上下文场景中，导致了显著的内存消耗。现有的KV缓存驱逐方法常常因驱逐引入的信息丢失而降低LLMs在长上下文场景中的性能。本文提出了一种新颖的KV缓存合并方法，称为*KVMerger*，以实现适应性KV缓存压缩，适用于长上下文任务，在受限的内存预算下不会显著降低性能。我们的方法受到了一个有趣观察的启发，即在单个序列的标记级别上，键状态表现出高度相似性。为了便于合并，我们开发了一种有效而简单的合并集识别算法，以识别适合合并的KV状态。我们的合并集识别算法激发了第二个观察，即从相似性角度来看，KV缓存的稀疏性与数据集无关，并在模型级别保持不变。随后，我们提出了一种高斯核加权合并算法，选择性地合并每个合并集中的所有状态。我们进行了广泛的实验，以展示*KVMerger*在受限内存预算下对长上下文任务的有效性，将其应用于包括Llama2-7B/13B-chat和Mistral-7B-instruct在内的模型。使用LongBench和ZeroScroll基准，我们将我们的方法与其他KV缓存压缩技术（包括H2O和CaM）进行了比较，结果表明我们的方法在$50\%$
    KV缓存预算下在各种任务中表现优越。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have demonstrated exceptional performance across
    a variety of applications, particularly excelling in long-context scenarios that
    are increasingly relevant in everyday life. Recent state-of-the-art LLMs have
    been meticulously developed to scale up to handle long-context tasks, such as
    OpenAI’s ChatGPT (OpenAI, [2023](#bib.bib19)), Anthropic’s Claude (Anthropic,
    [2023](#bib.bib1)), Meta’s LLaMA-3 (Touvron et al., [2023a](#bib.bib27)) (Touvron
    et al., [2023b](#bib.bib28)), Mistral (Jiang et al., [2023](#bib.bib11)), and
    Google’s Gemini-pro-1.5 that supports a staggering 1M token context length (Gemini Team,
    [2024](#bib.bib7)). However, as LLMs process larger volumes of data over extended
    contexts, *KV cache* starts to pose a substantial obstacle to LLM’s performance
    and scalability. KV cache stores the key and value states (KV) derived from the
    attention calculation of previously processed tokens and reuses those states in
    the autoregressive generation process. As LLMs continue to grow in size and capabilities,
    supporting long-context starts to eat up memory. For example, a 175-billion parameter
    GPT-3 model, with a batch size of 64 and a sequence length of 4,096 tokens (including
    both prefilled and generated tokens), necessitates approximately 1,208 GB of GPU
    memory (Liu et al., [2024](#bib.bib16)), which exceeds the memory capacity of
    most advanced GPUs. Therefore, the need for compressing KV cache while maintaining
    LLM generation quality, especially for long-context tasks, becomes essential.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种应用中展示了卓越的性能，尤其在长上下文场景中表现出色，这些场景在日常生活中越来越相关。最近的前沿LLMs经过精心开发，能够扩展以处理长上下文任务，例如OpenAI的ChatGPT（OpenAI,
    [2023](#bib.bib19)）、Anthropic的Claude（Anthropic, [2023](#bib.bib1)）、Meta的LLaMA-3（Touvron
    et al., [2023a](#bib.bib27)）（Touvron et al., [2023b](#bib.bib28)）、Mistral（Jiang
    et al., [2023](#bib.bib11)）和Google的Gemini-pro-1.5，支持高达1M的token上下文长度（Gemini Team,
    [2024](#bib.bib7)）。然而，随着LLMs处理更大量的数据以及扩展上下文，*KV缓存*开始对LLM的性能和可扩展性构成重大障碍。KV缓存存储从先前处理的tokens的注意力计算中得出的键和值状态（KV），并在自回归生成过程中重复使用这些状态。随着LLMs继续扩大规模和能力，支持长上下文开始占用大量内存。例如，一个具有1750亿参数的GPT-3模型，批量大小为64，序列长度为4,096
    tokens（包括预填充和生成的tokens），需要大约1,208 GB的GPU内存（Liu et al., [2024](#bib.bib16)），这超出了大多数先进GPU的内存容量。因此，在保持LLM生成质量的同时压缩KV缓存，特别是在长上下文任务中，变得至关重要。
- en: 'Current efforts for KV cache compression can be broadly categorized into three
    types: quantization, eviction, and merging, as illustrated in Figure 1\. Quantization
    replaces floating point KV states (e.g., FP16) with low-bit representations to
    decrease memory usage while striving to maintain the overall performance of LLMs.
    Recent advancements, such as Coupled Quantization (Zhang et al., [2024b](#bib.bib36))
    and KIVI (Zirui Liu et al., [2023](#bib.bib40)), have demonstrated that KV cache
    can be quantized to 1-bit or 2-bit precision while preserving performance. In
    contrast, KV cache eviction methods selectively remove unimportant tokens from
    the cache based on certain signals from the model, thereby reducing the memory
    footprint by limiting the number of key and value states in the KV cache (Xiao
    et al., [2024](#bib.bib30); Liu et al., [2023b](#bib.bib18); Zhang et al., [2023](#bib.bib38);
    Ge et al., [2024](#bib.bib6)). For instance, Scissorhands (Liu et al., [2023b](#bib.bib18))
    keeps a fixed KV size budget and replies on the Persistence of Importance hypothesis
    to evict key and value states for non-important tokens. Similarly, H2O (Zhang
    et al., [2023](#bib.bib38)) utilizes aggregated attention scores to determine
    so called “heavy hitters”, which are a subset of important tokens to keep in the
    KV cache. While eviction-based methods have demonstrated promising results on
    short context tasks with simple perplexity metrics, a significant drawback of
    eviction methods is their potential to accidentally and permanently remove important
    tokens, leading to context damage and adversely affecting their effectiveness
    in long-context tasks that heavily rely on context information. On a separate
    line of research, KV cache merging has been proposed as a complementary method
    of eviction ([Zhang et al.,](#bib.bib37) ; Wan et al., [2024](#bib.bib29); Liu
    et al., [2024](#bib.bib16); Yu et al., [2024a](#bib.bib32)).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对KV缓存压缩的努力可以大致分为三类：量化、驱逐和合并，如图1所示。量化通过用低位表示（如FP16）替代浮点KV状态来减少内存使用，同时力求保持LLMs的整体性能。最近的进展，例如Coupled
    Quantization（Zhang et al., [2024b](#bib.bib36)）和KIVI（Zirui Liu et al., [2023](#bib.bib40)），已经证明KV缓存可以量化到1位或2位精度，同时保持性能。相比之下，KV缓存驱逐方法根据模型的某些信号选择性地从缓存中移除不重要的token，从而通过限制KV缓存中的键值状态数量来减少内存占用（Xiao
    et al., [2024](#bib.bib30); Liu et al., [2023b](#bib.bib18); Zhang et al., [2023](#bib.bib38);
    Ge et al., [2024](#bib.bib6)）。例如，Scissorhands（Liu et al., [2023b](#bib.bib18)）保持固定的KV大小预算，并依赖于重要性持久性假设来驱逐不重要的token的键值状态。类似地，H2O（Zhang
    et al., [2023](#bib.bib38)）利用聚合的注意力分数来确定所谓的“重型打击者”，即保留在KV缓存中的重要token子集。虽然基于驱逐的方法在短期上下文任务中利用简单的困惑度指标显示出了有希望的结果，但驱逐方法的一个显著缺点是可能意外且永久性地移除重要token，导致上下文损坏，从而影响在高度依赖上下文信息的长上下文任务中的有效性。在另一条研究线上，KV缓存合并被提出作为驱逐的补充方法（[Zhang
    et al.,](#bib.bib37); Wan et al., [2024](#bib.bib29); Liu et al., [2024](#bib.bib16);
    Yu et al., [2024a](#bib.bib32)）。
- en: Unlike eviction-based methods, the KV cache merging technique does not strictly
    remove key and value states. Instead, it involves merging states that are otherwise
    to be dropped by eviction method into single token state. By amalgamating states
    rather than outright evicting them, this method ensures that essential information
    not captured by the attention scores is retained, thereby enhancing the model’s
    ability to maintain performance and accuracy in long-context tasks with compressed
    KV cache. It is noteworthy that, although token merging is well-established in
    computer vision (CV) (Zeng et al., [2022](#bib.bib34)) (Bolya et al., [2023](#bib.bib4))
    (Kim et al., [2023](#bib.bib14)) (Zhang et al., [2024a](#bib.bib35)), the application
    of key and value states merging in LLMs has not been extensively explored due
    to several significant challenges. Specifically, *the high dimensionality and
    sparsity of KV cache make it difficult to accurately identify sets of states that
    can be merged without losing critical information*. Additionally, *developing
    appropriate merging algorithm without introducing the loss of essential information
    in long context presents another major challenge*. Effective merging techniques
    must strike a delicate balance between reducing memory usage and preserving the
    semantic integrity of the contexts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于驱逐的方法不同，KV 缓存合并技术并不严格删除键和值状态。相反，它涉及将那些本应被驱逐方法丢弃的状态合并为单一的令牌状态。通过合并状态而非直接驱逐，这种方法确保了未被注意力分数捕捉到的关键信息得以保留，从而增强了模型在长上下文任务中维持性能和准确性的能力，尽管使用了压缩的
    KV 缓存。值得注意的是，尽管在计算机视觉（CV）中令牌合并已经相当成熟（Zeng et al., [2022](#bib.bib34)）（Bolya et
    al., [2023](#bib.bib4)）（Kim et al., [2023](#bib.bib14)）（Zhang et al., [2024a](#bib.bib35)），由于面临若干重大挑战，LLM
    中的键和值状态合并的应用尚未得到广泛探索。具体来说，*KV 缓存的高维度和稀疏性使得准确识别可以合并而不丢失关键信息的状态集变得困难*。此外，*在长上下文中开发合适的合并算法而不引入关键信息丢失是另一个主要挑战*。有效的合并技术必须在减少内存使用和保持上下文语义完整性之间找到微妙的平衡。
- en: '![Refer to caption](img/0546765d59f540cd6a0a9d4eea95a861.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0546765d59f540cd6a0a9d4eea95a861.png)'
- en: 'Figure 1: Three categories of KV cache compression techniques: KV cache quantization
    (left), KV cache eviction (middle), and KV cache merging (right). For the illustration
    of KV cache eviction, we use aggregated attention scores as the eviction signal,
    and k is set to 3; for KV cache merging, we illustrate many-to-one merging. The
    key state in red represents the state which incorporates the information of other
    remaining states. Value states are processed in the same way as key states.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：KV 缓存压缩技术的三种类别：KV 缓存量化（左）、KV 缓存驱逐（中）、KV 缓存合并（右）。在 KV 缓存驱逐的示意中，我们使用聚合的注意力分数作为驱逐信号，k
    设置为 3；对于 KV 缓存合并，我们示例了多对一的合并。红色的键状态表示合并了其他剩余状态的信息。值状态的处理方式与键状态相同。
- en: 'To address the aforementioned challenges associated with KV cache merging,
    we propose an effective KV cache merging method for accelerating autoregressive
    LLMs, especially for improving its performance in long-context tasks. We start
    by introducing an intriguing observation: key states exhibit high cosine similarity
    at the token level within a single sequence across different attention heads and
    model layers. We investigate the root cause of why such phenomenon appears, and
    our observation also opens opportunities for effective merging of key and value
    states based on their cosine similarity. Subsequently, we formulate the KV cache
    merging as a constrained clustering problem, and we introduce a strong baseline
    for this problem, where we use an effective merging set identification method
    for KV cache merging, which results in a layer-wise KV cache compression together
    with a simple weighted merging algorithm. Based on the proposed merging set identification
    method, we define KV cache sparsity from the perspective of states similarity.
    Our finding indicates that KV cache sparsity is independent of the dataset and
    remains persistent at the model level. Building on top of this, we propose a Gaussian
    kernel weighted merging algorithm to merge states within each identified merging
    set. We compare our proposed method with existing KV cache eviction method H2O
    and value states merging method CaM. The results demonstrate that our method achieves
    a better performance on these two benchmarks with both $50\%$ KV cache budgets,
    surpassing existing KV cache eviction methods. Our contributions can be summarized
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对与KV缓存合并相关的上述挑战，我们提出了一种有效的KV缓存合并方法，以加速自回归LLMs，特别是提高其在长上下文任务中的性能。我们首先提出了一个有趣的观察：在不同的注意力头和模型层中，关键状态在单个序列的标记级别上表现出高余弦相似度。我们调查了这种现象出现的根本原因，我们的观察也为基于余弦相似度的关键和数值状态的有效合并开辟了机会。随后，我们将KV缓存合并问题表述为一个受限的聚类问题，并为该问题引入了一个强大的基准方法，其中我们使用了一种有效的合并集识别方法进行KV缓存合并，从而实现了层级KV缓存压缩以及一个简单的加权合并算法。基于所提出的合并集识别方法，我们从状态相似性的角度定义了KV缓存稀疏性。我们的发现表明，KV缓存稀疏性与数据集无关，并且在模型级别上保持稳定。在此基础上，我们提出了一种高斯核加权合并算法，用于合并每个识别出的合并集中的状态。我们将我们提出的方法与现有的KV缓存淘汰方法H2O和数值状态合并方法CaM进行了比较。结果表明，我们的方法在这两个基准测试中都取得了更好的表现，$50\%$
    KV缓存预算下超越了现有的KV缓存淘汰方法。我们的贡献可以总结如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As one of the pioneering researches concerning KV cache merging for LLMs, we
    developed *KVMerger*, an effective KV cache merging algorithm especially designed
    for long-context tasks, including merging set identification and Gaussian kernel
    weighted merging function.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为针对LLMs的KV缓存合并的开创性研究之一，我们开发了*KVMerger*，这是一种专门为长上下文任务设计的有效KV缓存合并算法，包括合并集识别和高斯核加权合并函数。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce an intriguing observation that key states share a high similarity
    at the token level within a single sequence, as an important complementary to
    the previous observations concerning high query states similarity (Dai et al.,
    [2024](#bib.bib5)) and intra-layer KV cache similarity (Liu et al., [2024](#bib.bib16)).
    We also investigate the root cause of why such phenomenon appears.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个有趣的观察，即关键状态在单个序列的标记级别上共享高相似度，作为对先前关于高查询状态相似性（Dai et al., [2024](#bib.bib5)）和层内KV缓存相似性（Liu
    et al., [2024](#bib.bib16)）观察的重要补充。我们还调查了这种现象出现的根本原因。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our proposed *KVMerger* outperforms the previous KV Cache eviction algorithms
    on long-context tasks across various models under both $50\%$ KV cache budgets,
    introducing a great memory reduction compared to full KV cache.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出的*KVMerger*在各种模型的长上下文任务中，表现优于以前的KV缓存淘汰算法，在$50\%$ KV缓存预算下，相较于完整的KV缓存，显著减少了内存消耗。
- en: 2 Related Work and Problem Formulation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作和问题表述
- en: 2.1 KV Cache Quantization
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 KV缓存量化
- en: Quantization methods involve converting high-precision numerical values of key
    and value states into lower-precision formats, thereby decreasing the storage
    requirements within the cache (Hooper et al., [2024](#bib.bib10); Sheng et al.,
    [2023](#bib.bib23); Liu et al., [2023a](#bib.bib17); Zhang et al., [2024c](#bib.bib39)).
    Due to the presence of outliers in key and value states, recent works such as
    KIVI (Zirui Liu et al., [2023](#bib.bib40)) and Gear (Kang et al., [2024](#bib.bib13))
    employ fine-grained group-wise quantization, which quantize small channel groups
    within each token. MiKV (Yang et al., [2024](#bib.bib31)) addresses the information
    loss introduced by KV cache eviction methods by preserving those KVs in lower
    precision rather than directly dropping them. ZipCache (He et al., [2024](#bib.bib9))
    proposes an efficient channel-separable quantization scheme, disentangling the
    channel and token dimensions without excessive memory overhead. Different from
    quantized KV cache optimizations, this work studies compression of KV cache via
    token merging, which is complementary to quantization and can lead to better improvements
    when combined together.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法涉及将关键和数值状态的高精度数值转换为较低精度的格式，从而减少缓存中的存储需求 (Hooper et al., [2024](#bib.bib10);
    Sheng et al., [2023](#bib.bib23); Liu et al., [2023a](#bib.bib17); Zhang et al.,
    [2024c](#bib.bib39))。由于关键和数值状态中存在异常值，最近的工作如 KIVI (Zirui Liu et al., [2023](#bib.bib40))
    和 Gear (Kang et al., [2024](#bib.bib13)) 采用细粒度的组内量化，这种方法对每个 token 内的小通道组进行量化。MiKV
    (Yang et al., [2024](#bib.bib31)) 通过以较低精度保存这些 KV，解决了 KV 缓存驱逐方法引入的信息丢失问题，而不是直接丢弃它们。ZipCache
    (He et al., [2024](#bib.bib9)) 提出了高效的通道分离量化方案，将通道和 token 维度解耦，同时不增加过多的内存开销。与量化
    KV 缓存优化不同，本研究探讨了通过 token 合并来压缩 KV 缓存，这与量化互为补充，并且在结合使用时可以带来更好的改进。
- en: 2.2 KV Cache Eviction
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 KV 缓存驱逐
- en: KV cache eviction methods focus on retaining those important key-value pairs
    and discard those unimportant ones permanently. One of the common selection policies
    of key-value pairs is to exploit signals from the attention mechanism of LLMs
    to select important tokens. For example, H2O (Zhang et al., [2023](#bib.bib38)),
    Scissorhands (Liu et al., [2023b](#bib.bib18)), and RoCo (Ren & Zhu, [2024](#bib.bib21))
    compress KV cache by maintaining a small set of KV states whose corresponding
    tokens are determined by the ranking of attention scores. StreamingLLM (Xiao et al.,
    [2024](#bib.bib30)) finds that keeping the initial tokens, called attention sink,
    together with the recent window tokens is pivotal to maintain LLM’s performance.
    More recently, Ge et al. ([2024](#bib.bib6)) and Yu et al. ([2024b](#bib.bib33))
    find that attention sinks also occurs in the middle of the sentences, and Ge et al.
    ([2024](#bib.bib6)) introduces FastGen which can choose the most appropriate compression
    strategy for each heads with different attention distribution patterns. While
    demonstrating promising results, existing eviction methods are often evaluated
    on simple and widely questioned metrics, e.g., perplexity, which may fail to capture
    LLM’s capabilities in understanding long contexts. In contrast, we specifically
    look into KV compression under more challenging long-context understanding tasks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: KV 缓存驱逐方法专注于保留重要的键值对，并永久丢弃不重要的键值对。键值对的常见选择策略之一是利用 LLM 的注意力机制信号来选择重要的 token。例如，H2O
    (Zhang et al., [2023](#bib.bib38))、Scissorhands (Liu et al., [2023b](#bib.bib18))
    和 RoCo (Ren & Zhu, [2024](#bib.bib21)) 通过保持少量 KV 状态来压缩 KV 缓存，这些 KV 状态对应的 token
    是通过注意力得分的排名来确定的。StreamingLLM (Xiao et al., [2024](#bib.bib30)) 发现，保持初始 token，即注意力汇聚，与最近的窗口
    token 一起，对于维持 LLM 的性能至关重要。最近，Ge 等人 ([2024](#bib.bib6)) 和 Yu 等人 ([2024b](#bib.bib33))
    发现，注意力汇聚也发生在句子的中间，而 Ge 等人 ([2024](#bib.bib6)) 提出了 FastGen，可以为每个头选择最合适的压缩策略，以应对不同的注意力分布模式。尽管现有的驱逐方法显示出有前景的结果，但它们通常在简单且广泛质疑的指标上进行评估，例如困惑度，这可能无法捕捉
    LLM 在理解长上下文中的能力。相比之下，我们专门研究在更具挑战性的长上下文理解任务下的 KV 压缩。
- en: 2.3 KV Cache Merging
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 KV 缓存合并
- en: Instead of permanently discarding key and value states, KV cache merging offers
    a promising direction for KV cache compression while maintaining the performance
    of LLMs, particularly for long-context tasks such as Retrieval-Augmented Generation
    (RAG). MiniCache (Liu et al., [2024](#bib.bib16)) finds that KV states of some
    consecutive layers have high similarity and proposes an effective intra-layer
    KV cache merging and restoration algorithms to reduce memory usage by KV cache.
    CaM ([Zhang et al.,](#bib.bib37) ) adaptively merges to-be-evicted value states
    into the remaining conserved value states, resulting in minimal output perturbation
    due to the merging operation. Similarly, D2O Wan et al. ([2024](#bib.bib29)) selectively
    merges both value and key states to be evicted with those to be conserved using
    an Exponential Moving Average (EMA) threshold, and uses weighted merging based
    on cosine similarity. However, these methods are highly dependent on previous
    eviction methods, and how to identify effective merging set for KV cache and define
    effective merging method still remains unclear for KV cache. This paper is the
    first one to consider KV cache problem independently and propose simple yet effective
    solutions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与其永久丢弃关键和数值状态相比，KV缓存合并为KV缓存压缩提供了一个有前景的方向，同时保持LLMs的性能，特别是对于长上下文任务如检索增强生成（RAG）。MiniCache
    (Liu et al., [2024](#bib.bib16)) 发现一些连续层的KV状态具有高相似性，并提出了有效的层内KV缓存合并和恢复算法来减少KV缓存的内存使用。CaM
    ([Zhang et al.,](#bib.bib37)) 自适应地将待驱逐的数值状态合并到剩余的保留数值状态中，从而使得合并操作导致的输出扰动最小。类似地，D2O
    Wan et al. ([2024](#bib.bib29)) 使用指数移动平均（EMA）阈值有选择地将待驱逐的数值和关键状态与待保留的状态进行合并，并使用基于余弦相似度的加权合并。然而，这些方法高度依赖于先前的驱逐方法，如何识别KV缓存的有效合并集并定义有效的合并方法仍然不明确。本文是首个独立考虑KV缓存问题并提出简单而有效解决方案的论文。
- en: 2.4 Problem Formulation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 问题表述
- en: 'Formally, we study the performance impact of LLMs after compressing (without
    fine-tuning) their KV cache. For a decoder only pre-trained LLM $f$ can be formulated
    as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，我们研究了在压缩（未进行微调）其KV缓存后的LLMs的性能影响。对于仅解码器预训练的LLM $f$ 可以被表述为：
- en: '|  | $\mathcal{O}_{t\ }=\mathcal{A}_{t}\mathcal{V},\ \mathcal{A}_{t}=softmax\left(\frac{Q_{t}\mathcal{K}^{T}}{\sqrt{d_{k}}}\right)$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{O}_{t\ }=\mathcal{A}_{t}\mathcal{V},\ \mathcal{A}_{t}=softmax\left(\frac{Q_{t}\mathcal{K}^{T}}{\sqrt{d_{k}}}\right)$
    |  | (1) |'
- en: KV Cache Merging Algorithm. Our primary objective is to develop an efficient
    many-to-one merging algorithm $M$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存合并算法。我们的主要目标是开发一个高效的多对一合并算法 $M$。
- en: Definition 2.1 (KV Cache Merging Problem, informal). *Let $\mathcal{O}_{t}$
    must satisfy the following optimization criterion:*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 2.1（KV缓存合并问题，非正式）。*让 $\mathcal{O}_{t}$ 必须满足以下优化标准：*
- en: '|  | $M=\arg\min_{M}\frac{&#124;M(\mathcal{K})&#124;}{&#124;\mathcal{K}&#124;},$
    |  | (2) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $M=\arg\min_{M}\frac{&#124;M(\mathcal{K})&#124;}{&#124;\mathcal{K}&#124;},$
    |  | (2) |'
- en: '*subject to $|\mathcal{O}_{t}-\mathcal{O}_{t}^{*}|\leq\epsilon$ also has the
    following properties:*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*在 $|\mathcal{O}_{t}-\mathcal{O}_{t}^{*}|\leq\epsilon$ 的情况下也具备以下特性：*'
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|M\left(\mathcal{K}\right)|\ /\ |\mathcal{K}|\ \leq 1,|M\left(\mathcal{V}\right)|/\
    |\mathcal{V}|\ \leq 1$
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|M\left(\mathcal{K}\right)|\ /\ |\mathcal{K}|\ \leq 1,|M\left(\mathcal{V}\right)|/\
    |\mathcal{V}|\ \leq 1$
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|M\left(\mathcal{K}\right)|\ /\ |\mathcal{K}|=|M\left(\mathcal{V}\right)|/\
    |\mathcal{V}|\ $ *(make sure key and value states have the same compression ratio)*
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|M\left(\mathcal{K}\right)|\ /\ |\mathcal{K}|=|M\left(\mathcal{V}\right)|/\
    |\mathcal{V}|\ $ *（确保关键和数值状态具有相同的压缩比）*
- en: In our study, the merging algorithm $M$ (compression tolerance threshold).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，合并算法 $M$（压缩容忍阈值）。
- en: 'KV Cache Merging Sets Identification Policy. We define the identification policy
    $I$ as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存合并集识别策略。我们定义识别策略 $I$ 如下：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|\mathcal{K}|=|\mathcal{K}_{c}|+|\mathcal{K}_{m}|,|\mathcal{V}|=|\mathcal{V}_{c}|+|\mathcal{V}_{m}|$
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|\mathcal{K}|=|\mathcal{K}_{c}|+|\mathcal{K}_{m}|,|\mathcal{V}|=|\mathcal{V}_{c}|+|\mathcal{V}_{m}|$
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $|\mathcal{K}_{c}|=|\mathcal{V}_{c}|,|\mathcal{K}_{m}|=|\mathcal{V}_{m}|$ *(make
    sure key states and value states come in pair)*
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $|\mathcal{K}_{c}|=|\mathcal{V}_{c}|,|\mathcal{K}_{m}|=|\mathcal{V}_{m}|$ *（确保关键状态和数值状态成对出现）*
- en: where $\mathcal{K}_{c}$ are zero, all key and value states are merged, resulting
    in a full cache without any states eviction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{K}_{c}$ 为零时，所有关键和数值状态都被合并，结果是一个没有任何状态驱逐的完整缓存。
- en: KV Cache Merging Function. We define the merging function $F$ such that
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存合并函数。我们定义合并函数 $F$ 使得
- en: '|  | $1$2 |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $s_{i}^{*}$ is the merged new state for each sub merging set.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{i}^{*}$ 是每个子合并集的合并新状态。
- en: 3 Observations
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 观察结果
- en: In this section, we present two key observations illustrating that KV cache
    sparsity is universal for long-context tasks when viewed from the perspective
    of state similarity. These observations form the basis for our development of
    the adaptive KV cache merging algorithm, *KVMerger*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了两个关键观察结果，说明从状态相似性的角度来看，KV缓存稀疏性在长上下文任务中是普遍存在的。这些观察结果构成了我们开发自适应KV缓存合并算法*KVMerger*的基础。
- en: 3.1 KV cache similarity
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 KV缓存相似性
- en: Previous literature have analyzed the possibility of reducing KV cache size
    via exploiting attention sparsity, i.e., identifying important tokens via their
    attention scores(Zhang et al., [2023](#bib.bib38)) (Liu et al., [2023b](#bib.bib18)).
    However, *attention-score-driven* approaches are biased (He et al., [2024](#bib.bib9))
    because critical tokens often vary a lot across different queries (Tang et al.,
    [2024](#bib.bib25)), where relying on attention-score alone can lead to context
    damage. Instead of relying on attention scores, we investigate whether merging
    token states can preserve critical context details. Inspired by Dai et al. ([2024](#bib.bib5)),
    which reveals the phenomenon that query states share significant similarity at
    the token level in LLMs, we observe for the first time that *key states also exhibit
    very high similarity at the token level within single sequence*. We will first
    demonstrate the generalization of this token level similarity in key states and
    then analyze the potential reasons behind this intriguing observation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的文献已经分析了通过利用注意力稀疏性来减少KV缓存大小的可能性，即通过注意力分数识别重要的令牌（Zhang et al., [2023](#bib.bib38)）（Liu
    et al., [2023b](#bib.bib18)）。然而，*基于注意力分数的*方法存在偏差（He et al., [2024](#bib.bib9)），因为关键令牌在不同查询中往往变化很大（Tang
    et al., [2024](#bib.bib25)），仅依赖注意力分数可能导致上下文损坏。我们调查了是否通过合并令牌状态可以保留关键的上下文细节。受到Dai
    et al. ([2024](#bib.bib5))的启发，该研究揭示了查询状态在LLM中在令牌级别共享显著相似性的现象，我们首次观察到*关键状态在单个序列内的令牌级别也表现出非常高的相似性*。我们将首先展示这种令牌级相似性在关键状态中的普遍性，然后分析这一有趣观察背后的潜在原因。
- en: '![Refer to caption](img/d593a14c6cb25d3768782d58f083c687.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d593a14c6cb25d3768782d58f083c687.png)'
- en: 'Figure 2: Visualization of the cosine similarity map of key states at the token-wise
    level produced by running the inference process on the Llama2-7b-chat model by
    randomly sampling data from the SynthWiki dataset. Observations include: (1) Key
    states share strong similarity within one sequence across different layers and
    heads; (2) The similarity between key states has the property of locality, i.e.,
    adjacent tokens exhibit higher similarity.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：通过在Llama2-7b-chat模型上运行推断过程，并从SynthWiki数据集中随机抽样数据，生成的关键状态在令牌级别的余弦相似性图的可视化。观察结果包括：（1）关键状态在不同层和头部之间在一个序列内共享强相似性；（2）关键状态之间的相似性具有局部性，即相邻令牌表现出更高的相似性。
- en: 'Observation: key states exhibit high, localized token-level similarity. We
    conduct the inference process on the Llama2-7b-chat model by randomly sampling
    data from the SynthWiki dataset (Peysakhovich & Lerer, [2023](#bib.bib20)) with
    average sequence length being about 4000\. Then, we visualize the cosine similarity
    of key states at the token-wise level within a sequence using the following equation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 观察：关键状态表现出高的、局部的令牌级相似性。我们通过从SynthWiki数据集（Peysakhovich & Lerer, [2023](#bib.bib20)）随机抽样数据，在Llama2-7b-chat模型上进行推断过程，平均序列长度约为4000。然后，我们使用以下方程可视化序列内关键状态的令牌级余弦相似性：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'where $T$ for some tokens as Figure 3(a) shows. Moreover, we also observe from
    Figure 3(a) that the local similarity between one value states and the other consecutive
    key states shows different fluctuations for different attention heads. We also
    examine the cosine similarity of value states but do not observe the local similarity
    property. One interesting question arises: *why do such localized token similarity
    exhibit in key states, while value states do not?*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$T$如图3(a)所示。我们还从图3(a)中观察到，一个值状态与其他连续的关键状态之间的局部相似性在不同的注意力头中表现出不同的波动。我们还检查了值状态的余弦相似性，但没有观察到局部相似性特性。一个有趣的问题出现了：*为什么这种局部化的令牌相似性出现在关键状态中，而值状态中没有？*
- en: 'Analysis. Recent advancements in large language models (LLMs), including Llama2,
    Mistral, and Llama3, have showcased significant performance improvements by employing
    Rotary Position Embedding (RoPE) (Su et al., [2023](#bib.bib24)). RoPE integrates
    positional information into token embeddings through a rotational transformation
    based on positional indices. This process utilizes sinusoidal functions, specifically
    cosine and sine components, to encode positions. By rotating the embeddings in
    a multi-dimensional space, RoPE effectively captures the relative positions and
    order of tokens within a sequence. If we denote two adjacent input tokens as $x_{m},\
    x_{n}\in\mathbb{R}^{d}$ are two random integers, then in RoPE, the position information
    of each token is incorporated via the following equations:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 分析。近期的大型语言模型（LLMs）的进展，包括 Llama2、Mistral 和 Llama3，通过使用旋转位置嵌入（RoPE）展示了显著的性能提升（Su
    et al., [2023](#bib.bib24)）。RoPE 通过基于位置索引的旋转变换将位置信息集成到 token 嵌入中。此过程使用正弦函数，特别是余弦和正弦分量，来编码位置。通过在多维空间中旋转嵌入，RoPE
    有效地捕捉了序列中 tokens 的相对位置和顺序。如果我们将两个相邻的输入 tokens 表示为 $x_{m},\ x_{n}\in\mathbb{R}^{d}$
    是两个随机整数，则在 RoPE 中，每个 token 的位置信息通过以下方程包含：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $W_{k}$ is called as the rotary base, which is set to 10000 by default
    (Su et al., [2023](#bib.bib24)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{k}$ 被称为旋转基，默认设置为 10000（Su et al., [2023](#bib.bib24)）。
- en: Lemma 3.1 (Informal). *Consider two vectors $\mathbf{k}_{m}$-th elements of
    $\mathbf{k}_{m}$.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 3.1（非正式）。*考虑两个向量 $\mathbf{k}_{m}$-th 元素的 $\mathbf{k}_{m}$。*
- en: Lemma 3.2 (Informal). *Consider integer $j$ as $\mathbf{k}_{m,j}^{{}^{\prime}}=\mathbf{k}_{m,j}/e^{im\theta_{j}}$,
    we have:*
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 3.2（非正式）。*考虑整数 $j$ 为 $\mathbf{k}_{m,j}^{{}^{\prime}}=\mathbf{k}_{m,j}/e^{im\theta_{j}}$，我们有：*
- en: '|  | $1$2 |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '*where $\langle\mathbf{k}_{m,j}^{{}^{\prime}},\mathbf{k}_{n,j}^{{}^{\prime}}\rangle$,
    respectively.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*其中 $\langle\mathbf{k}_{m,j}^{{}^{\prime}},\mathbf{k}_{n,j}^{{}^{\prime}}\rangle$，分别。*'
- en: 'The formal and complete proof of the above lemma is shown in appendix [A](#A1
    "Appendix A Appendix ‣ Model Tells You Where to Merge: Adaptive KV Cache Merging
    for LLMs on Long-Context Tasks"). The conclusions of lemmas 3.1 and 3.2 are the
    necessary conditions of $\textit{similarity}(\mathbf{k}_{m,j},\mathbf{k}_{n,j})=1$.
    The analysis above clarifies why value states exhibit low similarity at the token
    level. Without the RoPE operation, value states are incapable of achieving rotation
    to comparable angles. Both empirical observations and theoretical analysis indicate
    that merging highly similar key states is approachable. This scheme is preferable
    to simply discarding key states, as it helps prevent potential information loss,
    particularly in long-context scenarios.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述引理的正式和完整证明见附录 [A](#A1 "附录 A 附录 ‣ 模型告诉你合并位置：针对长上下文任务的自适应 KV 缓存合并")。引理 3.1 和
    3.2 的结论是 $\textit{similarity}(\mathbf{k}_{m,j},\mathbf{k}_{n,j})=1$ 的必要条件。上述分析阐明了为何在
    token 层面值状态表现出低相似度。没有 RoPE 操作，值状态无法达到可比的角度旋转。实证观察和理论分析都表明，合并高度相似的键状态是可行的。这种方案优于简单地丢弃键状态，因为它有助于防止潜在的信息丢失，尤其是在长上下文场景中。
- en: '![Refer to caption](img/7b1136c73f0bd17d1f3f5ddd3b380eb8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7b1136c73f0bd17d1f3f5ddd3b380eb8.png)'
- en: 'Figure 3: (a): The cosine similarity changes between the current token and
    its adjacent tokens across distinct attention heads and layers. We show the above
    changes for tokens with indices being 2000, 3000, and 4000.(b) The layer-wise
    compression ratios obtained by our proposed merging set identification algorithm
    for different samples and different tasks. (c) The comparison of long-context
    performance between H2O and average weighted merging with our proposed merging
    set identification algorithm. (d) The illustration of Gaussian kernel function
    with different values of $\sigma$.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: (a): 当前 token 与其相邻 tokens 在不同注意力头和层之间的余弦相似度变化。我们展示了索引为 2000、3000 和 4000
    的 tokens 的上述变化。(b) 我们提出的合并集识别算法对不同样本和任务获得的逐层压缩比。(c) H2O 与我们提出的合并集识别算法在长上下文性能上的比较。(d)
    高斯核函数在不同 $\sigma$ 值下的示意图。'
- en: 3.2 Persistent KV cache sparsity
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 持续的 KV 缓存稀疏性
- en: We have demonstrated that key states within a sequence exhibit significant similarity
    at the token level in pre-trained LLMs. Based on this, we progressively group
    consecutive key states of a given key state with similarity values exceeding a
    certain threshold. By applying this process from the last token to the first token,
    we obtain a set of groups, each containing consecutive key states with high similarity
    above the specified threshold. The obtained new key states set is defined as the
    merging set, meaning that the number of groups in the obtained set equals to the
    number of key states after merging. The above set identification algorithm is
    described in detail in Section 4.1.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明，预训练 LLM 中序列内的关键状态在标记级别上表现出显著的相似性。在此基础上，我们逐步将具有超过某个阈值相似性值的连续关键状态分组。从最后一个标记到第一个标记应用此过程，我们获得了一组组，每组包含高相似性超过指定阈值的连续关键状态。得到的新关键状态集被定义为合并集，这意味着获得的集合中的组数等于合并后的关键状态数量。上述集识别算法在第
    4.1 节中进行了详细描述。
- en: 'Observation: The KV cache sparsity for different samples are persistent at
    the model level. Figure 3(a) shows that the similarity distributions of different
    tokens vary across distinct attention heads and layers. The size of each subset
    of key states is governed by the similarity threshold defined. Lowering the threshold
    results in the inclusion of a larger number of key states within a single merging
    set, thereby leading to varied compression ratios across all attention heads and
    layers. To investigate the actual compression ratio achieved by the previous set
    identification algorithm, we conduct inference processes on the Llama2-7b-chat
    model. This involves randomly sampling 200 instances from the subset of LongBench
    (Bai et al., [2024](#bib.bib2)) tasks and calculating the average compression
    ratio for each layer, as shown in Figure 3(b). We observe that the layer-wise
    compression ratios were highly consistent across different samples from the same
    task and even across different tasks. This intriguing finding suggests that the
    *kv cache sparsity, resulting from the high similarity exhibited by key states,
    is independent of the dataset and remains persistent at the model level*.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 观察：不同样本的 KV 缓存稀疏性在模型层面上是持久的。图 3(a) 显示了不同标记的相似性分布在不同的注意力头和层中有所不同。每个关键状态子集的大小由定义的相似性阈值决定。降低阈值会导致更多的关键状态被包含在一个合并集内，从而导致所有注意力头和层的压缩比有所不同。为了研究先前集识别算法实现的实际压缩比，我们对
    Llama2-7b-chat 模型进行了推断过程。这包括从 LongBench（Bai 等人，[2024](#bib.bib2)）任务的子集中随机抽取 200
    个实例，并计算每一层的平均压缩比，如图 3(b) 所示。我们观察到，相同任务的不同样本之间，甚至不同任务之间，层级压缩比在高度一致。这一有趣的发现表明，*由关键状态表现出的高相似性所导致的
    kv 缓存稀疏性与数据集无关，并在模型层面上保持持久*。
- en: Insights The observed static KV cache sparsity suggests that it is possible
    to determine the layer-wise compression ratios by adjusting the cosine similarity
    threshold, thereby reducing the KV cache memory consumption. Additionally, Figure
    3(b) shows that the first two layers and the last few layers have relatively small
    compression ratios. This observation aligns with previous research indicating
    that the attention score distributions are more uniform in the first two layers
    and last one layer of LLMs (Yu et al., [2024b](#bib.bib33)) (Wan et al., [2024](#bib.bib29)),
    suggesting that most key states are important and should be preserved to avoid
    introducing significant noise for those layers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 见解：观察到的静态 KV 缓存稀疏性表明，可以通过调整余弦相似性阈值来确定层级压缩比，从而减少 KV 缓存的内存消耗。此外，图 3(b) 显示，前两层和最后几层的压缩比相对较小。这一观察与之前的研究结果一致，表明
    LLM 的前两层和最后一层的注意力分数分布更为均匀（Yu 等人，[2024b](#bib.bib33)）（Wan 等人，[2024](#bib.bib29)），这表明大多数关键状态是重要的，应予以保留，以避免为这些层引入显著的噪声。
- en: '![Refer to caption](img/60d4cb1758e70832dab4b67e4f2fe583.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/60d4cb1758e70832dab4b67e4f2fe583.png)'
- en: 'Figure 4: The whole framework of *KVMerger* is comprised of two major modules.
    The first module is to identify the merging set through our proposed algorithm
    in Section 4.1\. Note that those key and value states which are most sensitive
    to merging are excluded. The toy similarity map is used to illustrate this process
    in the above Merging Set Identification part, and the threshold for cosine similarity
    is set to 0.8\. The second module is to merge key and value states within each
    identified merging set via Gaussian kernel weighted merging as described in Section
    4.2\. For Gaussian kernel weighted merging illustration, the key state in red
    color represents the pivotal key state, where all the remaining key states should
    be weighted merged to that one. Note that values on key states in the above graph
    represent the aggregated attention scores.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：*KVMerger* 的整体框架由两个主要模块组成。第一个模块是通过我们在 4.1 节中提出的算法识别合并集。注意那些最敏感于合并的键值状态被排除。玩具相似性图用于说明上述合并集识别部分的过程，余弦相似性的阈值设置为
    0.8。第二个模块是通过高斯核加权合并来合并每个识别出的合并集中的键值状态，如 4.2 节所述。对于高斯核加权合并的说明，图中红色的键状态表示关键键状态，所有其余的键状态应加权合并到该状态上。注意，上图中的键状态上的值表示聚合的注意力得分。
- en: 4 Proposed Adaptive KV Merging Algorithm
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 提出的自适应 KV 合并算法
- en: 'In this section, we propose *KVMerger*, an adaptive KV merging algorithm, for
    LLMs based on the above observations. The whole pipeline of *KVMerger* is depicted
    in Figure 4, from which we can see that the whole algorithm contains two major
    modules: merging set identification and Gaussian kernel weighted merging process.
    We first introduce the merging set identification algorithm in Section 4.1, which
    can be viewed as solving a constrained clustering problem. We propose a transformation
    of Agglomerative Hierarchical Clustering (AHC) algorithm to solve this. In Section
    4.2, we delineate our proposed Gaussian kernel weighted merging algorithm, which
    is a many-to-one states merging method without introducing significant information
    loss.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提出了 *KVMerger*，一种基于上述观察的自适应 KV 合并算法，用于大型语言模型（LLMs）。*KVMerger* 的整体流程如图
    4 所示，从中可以看到整个算法包含两个主要模块：合并集识别和高斯核加权合并过程。我们首先在 4.1 节介绍合并集识别算法，该算法可以视为解决一个约束聚类问题。我们提出了一种改进的聚合层次聚类（AHC）算法来解决这一问题。在
    4.2 节中，我们描述了我们提出的高斯核加权合并算法，这是一种多对一状态合并方法，不引入显著的信息丢失。
- en: 4.1 Greedy Policy for Merging Set Identification
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 合并集识别的贪婪策略
- en: 'One way to solve the merging set identification problem described in Section [2.4](#S2.SS4
    "2.4 Problem Formulation ‣ 2 Related Work and Problem Formulation ‣ Model Tells
    You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks")
    is to view it as a variant of clustering problem, which we define below:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 2.4 节中描述的合并集识别问题的一种方法是将其视为聚类问题的变体，我们在下面定义：
- en: Definition 4.1 (Constrained Clustering Problem for KV Cache Merging, formal).
    *Given the original set of key states to be merged $\mathcal{K}_{m}=\{k_{1},k_{2},\ldots,k_{n}\}$
    such that the intra-cluster similarity is maximized and the inter-cluster similarity
    is minimized.*
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 4.1（KV 缓存合并的约束聚类问题，正式定义）。*给定要合并的原始键状态集合 $\mathcal{K}_{m}=\{k_{1},k_{2},\ldots,k_{n}\}$，使得簇内相似性最大化而簇间相似性最小化。*
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*Each merging set $\mathcal{S}_{i}$;*'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*每个合并集 $\mathcal{S}_{i}$;*'
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*$\forall\mathcal{S}_{i}$;*'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*$\forall\mathcal{S}_{i}$;*'
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*The objective function to be maximized can be expressed as:*'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*目标函数的最大化可以表示为：*'
- en: '|  | $1$2 |  |'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The similarity function, $\delta$, we used here is cosine similarity based on
    the observation in Section 3.1\. In order to conserve the locality similarity
    property of key states, the merging set identification problem is a constrained
    clustering problem, meaning that all elements in one cluster should be consecutive
    in sequence, and we do not merge states with high similarity but far away from
    each other. Then, we propose a variant of Agglomerative Hierarchical Clustering
    (AHC) algorithm to find all merging sets shown as Algorithm 1.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的相似性函数 $\delta$ 是基于 3.1 节中的观察得出的余弦相似性。为了保持键状态的局部相似性特性，合并集识别问题是一个约束聚类问题，这意味着一个簇中的所有元素应该在序列中是连续的，我们不会合并相似性高但彼此距离较远的状态。然后，我们提出了一种改进的聚合层次聚类（AHC）算法来找出所有合并集，如算法
    1 所示。
- en: Algorithm 1 Merging Set Identification
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 合并集识别
- en: 1:procedure AHC($\mathcal{K}_{m}=\{k_{1},\ldots,k_{T}\}$8:         i = j9:     end for10:     return
    The merging sets11:end procedure
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '1:procedure AHC($\mathcal{K}_{m}=\{k_{1},\ldots,k_{T}\}$8:          i = j9:      end
    for10:      return The merging sets11:end procedure'
- en: Algorithm 2 Merging Policy
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 合并策略
- en: 1:procedure Merge($\mathcal{S}_{k}=\{k_{1},\ldots,k_{n}\},A$10:end procedure
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 1:procedure Merge($\mathcal{S}_{k}=\{k_{1},\ldots,k_{n}\},A$10:end procedure
- en: '*KVMerger* also retains the KV states whose corresponding aggregated attention
    scores fall within the top-k range, including both attention sinks and heavy-hitters,
    which represent the most important and frequently accessed elements by LLMs. We
    assume that those key and value states are quite sensitive to merging and cannot
    participant in merging process to avoid information damage.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*KVMerger* 还保留了那些其对应的聚合注意力分数落在 top-k 范围内的 KV 状态，包括注意力汇聚点和重型命中点，这些代表了 LLMs 最重要和最常访问的元素。我们假设这些关键和价值状态对合并非常敏感，因此不能参与合并过程，以避免信息损坏。'
- en: 4.2 Gaussian Kernel Weighted Merging
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 高斯核加权合并
- en: Definition 4.2 (Weighted KV cache Merging, formal). *Given identified merging
    sets of key states and value states as $\mathcal{S}_{k}=\{k_{i},k_{i+1},\ldots,k_{p},\ldots
    k_{i+n}\}$ denote the pivotal key state and pivotal value state, respectively.
    Then, the weighted merging key states and value states can be defined as:*
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 4.2（加权 KV 缓存合并，正式）。*给定识别出的关键状态集和价值状态集作为 $\mathcal{S}_{k}=\{k_{i},k_{i+1},\ldots,k_{p},\ldots
    k_{i+n}\}$ 分别表示关键枢状态和价值枢状态。那么，加权合并的关键状态和价值状态可以定义为：*
- en: '|  | $1$2 |  | (5) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: '*where $w_{p}$ denote the weight assigned to the pivotal state and the remaining
    states in the merging set.*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*其中 $w_{p}$ 表示分配给枢状态和合并集余下状态的权重。*'
- en: We define the weighted merging function for KV cache merging in Definition 4.2,
    which follows the many-to-one merging definitions from Wan et al. ([2024](#bib.bib29)).
    Note that the merging function is a critical determinant of performance in many-to-one
    merging scenario. Two principal design factors directly influence merging efficacy.
    The first factor is the selection of the pivotal state, to which all other states
    are merged. The second factor involves the assignment of weights to each state,
    with the pivot state having the largest weight to preserve the information.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在定义 4.2 中定义了 KV 缓存合并的加权合并函数，该函数遵循 Wan 等人（[2024](#bib.bib29)）的多对一合并定义。请注意，合并函数在多对一合并场景中是性能的关键决定因素。两个主要设计因素直接影响合并效果。第一个因素是选择关键状态，所有其他状态都合并到该状态。第二个因素涉及给每个状态分配权重，其中枢状态具有最大的权重以保留信息。
- en: 'We start from the most intuitive merging function via average weighted for
    each merging set. We evaluate the average weighted merging function on four tasks
    from LongBench: TREC, NarrativeQA, TriviaQA, and LCC. As highlighted in previous
    research (Xiao et al., [2024](#bib.bib30)) (Zhang et al., [2023](#bib.bib38))
    (Wan et al., [2024](#bib.bib29)), recent tokens play a crucial role in performance.
    Therefore, we exclude the most recent tokens from merging to maintain an average
    compression ratio of $50\%$, as discussed in Section 3.2\. For simplicity, we
    select the pivotal token as the key state with the maximum index within each merging
    set. Additionally, we compare the average weighted merging function with the H2O
    algorithm to gain an initial perspective on the potential and performance differences
    between the KV cache merging scheme and the eviction scheme. The evaluation results
    are shown in Figure 3(c). The results demonstrate that the average weighted merging
    scheme provides a robust baseline, affirming the efficacy of the current method
    for identifying merging sets. However, the average weighted merging function performs
    worse compared to H2O, suggesting that the merging process may introduce noise,
    leading to information distortion.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最直观的合并函数开始，通过对每个合并集进行平均加权。我们在 LongBench 的四个任务（TREC、NarrativeQA、TriviaQA 和
    LCC）上评估了平均加权合并函数。如前述研究（Xiao 等人，[2024](#bib.bib30)）（Zhang 等人，[2023](#bib.bib38)）（Wan
    等人，[2024](#bib.bib29)）所强调，最近的令牌在性能中发挥了关键作用。因此，我们将最近的令牌排除在合并之外，以保持 $50\%$ 的平均压缩比，如第
    3.2 节讨论。为简单起见，我们选择每个合并集中具有最大索引的枢令牌作为关键状态。此外，我们将平均加权合并函数与 H2O 算法进行比较，以获得对 KV 缓存合并方案和逐出方案之间潜在和性能差异的初步视角。评估结果如图
    3(c) 所示。结果表明，平均加权合并方案提供了一个稳健的基准，确认了当前方法在识别合并集方面的有效性。然而，平均加权合并函数的表现不如 H2O，这表明合并过程可能引入噪声，导致信息失真。
- en: 'Gaussian Kernel Weights To eliminate the noise introduced by less informative
    key states via average weighted merging, we introduce Gaussian kernel weighted
    merging, which is expressed as:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核权重 为了消除通过加权平均合并引入的低信息量关键状态的噪声，我们引入了高斯核加权合并，其表达式为：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: 'Gaussian kernel is able to assign greater weight to elements that are nearer
    to the pivotal state. This local weighting characteristic ensures that the merged
    result is significantly shaped by nearby states, maintaining local structure and
    minimizing the impact of distant, possibly noisy states. Then, the merging weights
    for key states and value states can be formalized as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核能够为离关键状态更近的元素分配更大的权重。这种局部加权特性确保了合并结果主要受到附近状态的显著影响，保持局部结构，并最小化远离的、可能嘈杂的状态的影响。然后，关键状态和值状态的合并权重可以形式化为：
- en: '|  | $\mathbf{w}_{i}=\frac{\mathbf{g}_{pi}}{\sum_{j=0}^{&#124;S_{k}&#124;}\mathbf{g}_{pj}},\
    \ \ \ \ \mathbf{w}_{p}=\frac{1}{\sum_{j=0}^{&#124;S_{k}&#124;}\mathbf{g}_{pj}}.$
    |  | (7) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{w}_{i}=\frac{\mathbf{g}_{pi}}{\sum_{j=0}^{&#124;S_{k}&#124;}\mathbf{g}_{pj}},\
    \ \ \ \ \mathbf{w}_{p}=\frac{1}{\sum_{j=0}^{&#124;S_{k}&#124;}\mathbf{g}_{pj}}.$
    |  | (7) |'
- en: For merging value states, $\mathbf{w}_{i}$ in the Euclidean space, more weight
    will be assigned to $\mathbf{k}_{i}$ for all tokens within each merging set to
    avoid such situation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在欧几里得空间中合并值状态，$\mathbf{w}_{i}$，会为每个合并集合中的所有令牌分配更多的权重，以避免这种情况。
- en: 'Selection for Pivotal State As previously discussed, the selection for pivotal
    state within each merging set is crucial for performance. Here we follow previous
    token eviction methods that using aggregated attention score to select pivotal
    token as it indicates the importance of tokens, which can be expressed as:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 关键状态选择 如前所述，每个合并集合中的关键状态选择对性能至关重要。在这里，我们遵循以前的令牌驱逐方法，使用聚合的注意力分数选择关键令牌，因为它指示了令牌的重要性，可以表达为：
- en: '|  | $\mathbf{k}_{p}=\underset{i\in S_{k}}{\text{argmax}}\,(\mathbf{a}_{i}),\
    \ \ \ \ \mathbf{a}_{i}=\sum_{i=0}^{&#124;S_{k}&#124;}A\left[i,:\right]$ |  | (8)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{k}_{p}=\underset{i\in S_{k}}{\text{argmax}}\,(\mathbf{a}_{i}),\
    \ \ \ \ \mathbf{a}_{i}=\sum_{i=0}^{&#124;S_{k}&#124;}A\left[i,:\right]$ |  | (8)
    |'
- en: Note that the index of pivotal token for value states within each merging set
    is the same as key states. The complete merging policy is described as Algorithm
    2.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个合并集合中值状态的关键令牌索引与关键状态相同。完整的合并策略描述见算法 2。
- en: 5 Experiment
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Experimental Settings
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Models and Tasks We evaluate *KVMerger* using three models: Llama2-7B/13B-chat
    (Touvron et al., [2023b](#bib.bib28)) and Mistral-7B-Instruct-v1.0(Jiang et al.,
    [2023](#bib.bib11)). Our evaluation focuses primarily on instruction-tuned models,
    as these are meticulously optimized for dialogue use cases and question-answering
    scenarios. The above three models are evaluated on two commonly used benchamrks
    for long-context scenario, that is, LongBench (Bai et al., [2024](#bib.bib2))
    and ZeroScrolls (Shaham et al., [2023](#bib.bib22)). Both LongBench and ZeroScrolls
    include a variety of scenarios such as multi-document question answering, summarization,
    and dialogue generation, providing a comprehensive assessment of a model’s ability
    to handle long sequences of text while maintaining coherence and accuracy. Specifically,
    we use nine datasets in LongBench: 2WikiMQA, gov$\_$report, SummScreenFD, QMSum,
    SQuALITY, Qasper, NarrativeQA, BookSumSort. Additionally, we also individually
    test our methods on RAG tasks with the Needle-in-a-Haystack test (Guerreiro et al.,
    [2023](#bib.bib8)). The performance of our method for LLMs on all the above tasks
    are also compared with existing eviction method H2O and merging method CaM.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和任务 我们使用三种模型来评估*KVMerger*：Llama2-7B/13B-chat (Touvron et al., [2023b](#bib.bib28))
    和 Mistral-7B-Instruct-v1.0 (Jiang et al., [2023](#bib.bib11))。我们的评估主要集中在指令调优模型上，因为这些模型针对对话使用场景和问答场景进行了精细优化。上述三种模型在两个常用的长上下文场景基准上进行了评估，即
    LongBench (Bai et al., [2024](#bib.bib2)) 和 ZeroScrolls (Shaham et al., [2023](#bib.bib22))。LongBench
    和 ZeroScrolls 包括多种场景，如多文档问答、摘要生成和对话生成，提供了模型处理长文本序列同时保持连贯性和准确性的全面评估。具体来说，我们在 LongBench
    中使用了九个数据集：2WikiMQA, gov$\_$report, SummScreenFD, QMSum, SQuALITY, Qasper, NarrativeQA,
    BookSumSort。此外，我们还在 RAG 任务上使用 Needle-in-a-Haystack 测试 (Guerreiro et al., [2023](#bib.bib8))
    单独测试了我们的方法。我们的方法在所有上述任务上的性能也与现有的驱逐方法 H2O 和合并方法 CaM 进行了比较。
- en: Implementation details We test *KVMerger* in two compression scenarios. The
    first one is $50\%$. We conducted our experiments on a cluster with A100 40GB
    GPUs (4XA100 per node, 256GB DRAM, 15TB storage, 200Gbps interconnect), and a
    cluster with A100 80GB GPUs (2xA100 per node, 256GB DRAM, 100Gb Ethernet interconnect).
    The evaluation process for LongBench and ZeroScrolls follows THUDM ([2024](#bib.bib26))
    and Lab ([2024](#bib.bib15)). The implementation of Needle-in-a-Haystack test
    follows [Kamradt](#bib.bib12) .
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节 我们在两种压缩场景下测试了*KVMerger*。第一个场景是$50\%$。我们在一个配备A100 40GB GPU（每个节点4个A100，256GB
    DRAM，15TB存储，200Gbps互连）和一个配备A100 80GB GPU（每个节点2个A100，256GB DRAM，100Gb以太网互连）的集群上进行了实验。LongBench和ZeroScrolls的评估过程遵循THUDM（[2024](#bib.bib26)）和Lab（[2024](#bib.bib15)）。Needle-in-a-Haystack测试的实现遵循[Kamradt](#bib.bib12)。
- en: 'Table 1: *KVMerger* for Llama2-7B/13B-chat and Mistral-7B-Instruct on LongBench
    datasets.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '表1: *KVMerger* 在Llama2-7B/13B-chat和Mistral-7B-Instruct的LongBench数据集上的表现。'
- en: '| models | budget | method | 2wikimqa | gov_report | narrativeqa | pr_en |
    multifieldqa_en | trec | multi_news | triviaqa | qasper | avg. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 预算 | 方法 | 2wikimqa | gov_report | narrativeqa | pr_en | multifieldqa_en
    | trec | multi_news | triviaqa | qasper | 平均值 |'
- en: '|  | 100% | Full Cache | 31.45 | 26.99 | 18.74 | 8.00 | 36.60 | 64.00 | 26.26
    | 83.09 | 21.83 | 35.22 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 100% | 全缓存 | 31.45 | 26.99 | 18.74 | 8.00 | 36.60 | 64.00 | 26.26 | 83.09
    | 21.83 | 35.22 |'
- en: '|  |  | H2O | 29.96 | 24.86 | 17.48 | 7.00 | 33.58 | 63.50 | 26.00 | 82.51
    | 21.04 | 34.00 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  | H2O | 29.96 | 24.86 | 17.48 | 7.00 | 33.58 | 63.50 | 26.00 | 82.51
    | 21.04 | 34.00 |'
- en: '|  |  | CaM | 30.69 | 24.46 | 17.08 | 6.50 | 33.98 | 63.50 | 24.66 | 82.17
    | 20.00 | 33.67 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CaM | 30.69 | 24.46 | 17.08 | 6.50 | 33.98 | 63.50 | 24.66 | 82.17
    | 20.00 | 33.67 |'
- en: '|  | 50% | *KVMerger* | 32.99 | 25.31 | 18.50 | 7.33 | 36.89 | 64.00 | 26.29
    | 83.62 | 20.04 | 35.02 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 50% | *KVMerger* | 32.99 | 25.31 | 18.50 | 7.33 | 36.89 | 64.00 | 26.29
    | 83.62 | 20.04 | 35.02 |'
- en: '|  |  | H2O | 30.57 | 24.48 | 17.85 | 7.00 | 32.17 | 63.00 | 25.37 | 80.89
    | 20.04 | 33.49 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |  | H2O | 30.57 | 24.48 | 17.85 | 7.00 | 32.17 | 63.00 | 25.37 | 80.89
    | 20.04 | 33.49 |'
- en: '|  |  | CaM | 31.06 | 23.80 | 18.36 | 6.00 | 33.07 | 62.50 | 25.23 | 81.86
    | 18.37 | 33.36 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CaM | 31.06 | 23.80 | 18.36 | 6.00 | 33.07 | 62.50 | 25.23 | 81.86
    | 18.37 | 33.36 |'
- en: '| Llama2-7B-chat | 35% | *KVMerger* | 32.29 | 25.24 | 19.12 | 7.00 | 33.82
    | 63.50 | 25.64 | 82.76 | 21.09 | 34.50 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-chat | 35% | *KVMerger* | 32.29 | 25.24 | 19.12 | 7.00 | 33.82
    | 63.50 | 25.64 | 82.76 | 21.09 | 34.50 |'
- en: '|  | 100% | Full Cache | 13.21 | 27.59 | 14.42 | 15.25 | 27.44 | 68.50 | 26.69
    | 87.42 | 17.15 | 33.07 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 100% | 全缓存 | 13.21 | 27.59 | 14.42 | 15.25 | 27.44 | 68.50 | 26.69 | 87.42
    | 17.15 | 33.07 |'
- en: '|  |  | H2O | 13.39 | 26.20 | 15.01 | 15.50 | 26.40 | 68.00 | 25.35 | 84.73
    | 17.10 | 32.40 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |  | H2O | 13.39 | 26.20 | 15.01 | 15.50 | 26.40 | 68.00 | 25.35 | 84.73
    | 17.10 | 32.40 |'
- en: '|  |  | CaM | 13.30 | 25.88 | 13.47 | 15.00 | 26.96 | 67.50 | 26.06 | 84.65
    | 16.58 | 32.16 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CaM | 13.30 | 25.88 | 13.47 | 15.00 | 26.96 | 67.50 | 26.06 | 84.65
    | 16.58 | 32.16 |'
- en: '|  | 50% | *KVMerger* | 13.46 | 26.63 | 14.4 | 16.00 | 27.29 | 68.50 | 26.12
    | 87.48 | 17.22 | 33.01 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 50% | *KVMerger* | 13.46 | 26.63 | 14.4 | 16.00 | 27.29 | 68.50 | 26.12
    | 87.48 | 17.22 | 33.01 |'
- en: '|  |  | H2O | 12.26 | 25.52 | 13.14 | 14.50 | 25.75 | 67.50 | 25.59 | 83.53
    | 16.35 | 31.57 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  |  | H2O | 12.26 | 25.52 | 13.14 | 14.50 | 25.75 | 67.50 | 25.59 | 83.53
    | 16.35 | 31.57 |'
- en: '|  |  | CaM | 13.43 | 25.37 | 13.58 | 12.50 | 25.70 | 67.50 | 25.04 | 84.95
    | 16.34 | 31.60 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CaM | 13.43 | 25.37 | 13.58 | 12.50 | 25.70 | 67.50 | 25.04 | 84.95
    | 16.34 | 31.60 |'
- en: '| Llama2-13B-chat | 35% | *KVMerger* | 12.61 | 26.12 | 13.60 | 14.00 | 26.75
    | 68.00 | 26.32 | 86.76 | 16.24 | 32.27 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B-chat | 35% | *KVMerger* | 12.61 | 26.12 | 13.60 | 14.00 | 26.75
    | 68.00 | 26.32 | 86.76 | 16.24 | 32.27 |'
- en: '|  | 100% | Full Cache | 31.47 | 26.55 | 21.96 | 25.00 | 39.50 | 61.00 | 26.44
    | 83.89 | 30.12 | 38.44 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | 100% | 全缓存 | 31.47 | 26.55 | 21.96 | 25.00 | 39.50 | 61.00 | 26.44 | 83.89
    | 30.12 | 38.44 |'
- en: '|  |  | H2O | 29.21 | 19.91 | 17.65 | 8.00 | 25.50 | 53.00 | 19.95 | 74.55
    | 21.51 | 29.92 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  |  | H2O | 29.21 | 19.91 | 17.65 | 8.00 | 25.50 | 53.00 | 19.95 | 74.55
    | 21.51 | 29.92 |'
- en: '|  |  | CaM | 29.57 | 22.67 | 19.43 | 12.00 | 28.95 | 58.00 | 20.17 | 81.82
    | 21.87 | 32.72 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CaM | 29.57 | 22.67 | 19.43 | 12.00 | 28.95 | 58.00 | 20.17 | 81.82
    | 21.87 | 32.72 |'
- en: '|  | 50% | *KVMerger* | 32.44 | 24.05 | 21.85 | 23.00 | 31.23 | 60.00 | 20.87
    | 84.16 | 24.52 | 35.79 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 50% | *KVMerger* | 32.44 | 24.05 | 21.85 | 23.00 | 31.23 | 60.00 | 20.87
    | 84.16 | 24.52 | 35.79 |'
- en: '|  |  | H2O | 12.30 | 5.16 | 3.64 | 0.62 | 11.95 | 37.50 | 18.99 | 17.08 |
    14.05 | 13.48 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  |  | H2O | 12.30 | 5.16 | 3.64 | 0.62 | 11.95 | 37.50 | 18.99 | 17.08 |
    14.05 | 13.48 |'
- en: '|  |  | CaM | 28.77 | 18.70 | 17.76 | 8.50 | 25.31 | 45.50 | 19.72 | 72.88
    | 17.25 | 28.27 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CaM | 28.77 | 18.70 | 17.76 | 8.50 | 25.31 | 45.50 | 19.72 | 72.88
    | 17.25 | 28.27 |'
- en: '| Mistral-7B-Instruct | 35% | *KVMerger* | 30.77 | 20.99 | 23.58 | 23.50 |
    28.10 | 60.5 | 19.94 | 83.82 | 24.13 | 35.04 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-Instruct | 35% | *KVMerger* | 30.77 | 20.99 | 23.58 | 23.50 |
    28.10 | 60.5 | 19.94 | 83.82 | 24.13 | 35.04 |'
- en: 5.2 Experimental Results on Long-context Tasks
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 长上下文任务的实验结果
- en: 'LongBench Results The evaluation results of nine selected LongBench datasets
    on Llama2-7B/13B-chat and Mistral-7B-Instruct-v1.0 are shown in Table [1](#S5.T1
    "Table 1 ‣ 5.1 Experimental Settings ‣ 5 Experiment ‣ Model Tells You Where to
    Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks"). We compare
    the current KV cache compression methods, including H2O, CaM, with our proposed
    KV merging method *KVMerger* by preserving both $50\%$ of contexts in the KV cache.
    Our results demonstrate that *KVMerger* consistently outperforms the other KV
    cache compression techniques across nearly all selected datasets from LongBench.
    Notably, the performance gaps between our algorithm and the full KV cache scenario
    for both Llama2-7B/13B-chat and Mistral-7B-Instruct-v1.0 are significantly smaller
    than the other KV compression methods. More importantly, our method achieves better
    evaluation results on several tasks compared to the full cache scenario, highlighting
    the efficacy and robustness of our approach on long-context tasks. Another interesting
    finding is that the latest value states merging method, CaM, does not perform
    well on long-context tasks. This may be attributed to the information loss results
    from eviction of key states, despite the merging of value states.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: LongBench 结果 表 1 显示了在 Llama2-7B/13B-chat 和 Mistral-7B-Instruct-v1.0 上对九个选定 LongBench
    数据集的评估结果。我们通过保留 KV 缓存中的 $50\%$ 上下文，对当前的 KV 缓存压缩方法，包括 H2O、CaM，与我们提出的 KV 合并方法 *KVMerger*
    进行了比较。我们的结果表明，*KVMerger* 在几乎所有选定的 LongBench 数据集上都表现优于其他 KV 缓存压缩技术。值得注意的是，我们的算法与完全
    KV 缓存场景相比，在 Llama2-7B/13B-chat 和 Mistral-7B-Instruct-v1.0 上的性能差距明显小于其他 KV 压缩方法。更重要的是，我们的方法在多个任务上相比完全缓存场景获得了更好的评估结果，突显了我们方法在长上下文任务上的有效性和鲁棒性。另一个有趣的发现是，最新的值状态合并方法
    CaM 在长上下文任务上表现不佳。这可能是由于尽管合并了值状态，但由于驱逐了键状态而导致的信息丢失所致。
- en: Mistral-7B-Instruct-v1.0 leverages the Grouped-Query-Attention (GQA) technique
    to optimize KV cache memory usage. In this approach, each key state corresponds
    to four query states. When applying the H2O method to each key state, rather than
    duplicating key and value states, we use a single attention map. This attention
    map is generated by averaging the values of four attention maps formed by the
    four query states, which determines the states to be evicted. For the *KVMerger*
    method, we also utilize this singular attention map to select pivotal states,
    ensuring a fair comparison. Our results indicate a significant performance drop
    for Mistral-7B-Instruct-v1.0 when using the H2O method. Conversely, *KVMerger*
    demonstrates the smallest performance decline under both $35\%$ KV cache budgets,
    highlighting its efficiency on GQA.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral-7B-Instruct-v1.0 利用 Grouped-Query-Attention (GQA) 技术来优化 KV 缓存内存使用。在这种方法中，每个键状态对应四个查询状态。当将
    H2O 方法应用于每个键状态时，我们不重复键和值状态，而是使用单一的注意力图。这个注意力图是通过平均四个由四个查询状态形成的注意力图的值来生成的，用于确定要驱逐的状态。对于
    *KVMerger* 方法，我们也使用这个单一的注意力图来选择关键状态，确保公平比较。我们的结果表明，当使用 H2O 方法时，Mistral-7B-Instruct-v1.0
    的性能显著下降。相反，*KVMerger* 在两个 $35\%$ 的 KV 缓存预算下表现出最小的性能下降，突显了其在 GQA 上的效率。
- en: '![Refer to caption](img/56f971c3b43faac229c87fd76387bddd.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/56f971c3b43faac229c87fd76387bddd.png)'
- en: 'Figure 5: The visualization of needle-in-a-haystack test on Llama2-7B-chat
    with different KV cache compression methods. The x-axis represents the length
    of contexts, and the y-axis represents the document depth where the needle is
    inserted.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在 Llama2-7B-chat 上使用不同的 KV 缓存压缩方法进行针在干草堆测试的可视化。x 轴表示上下文的长度，y 轴表示针插入的文档深度。
- en: 'ZeroScrolls Results We also evaluate Llama2-7B-chat on ZeroScrolls datasets
    using different KV cache compression techniques, as shown in Table [2](#S5.T2
    "Table 2 ‣ 5.2 Experimental Results on Long-context Tasks ‣ 5 Experiment ‣ Model
    Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks").
    The ZeroScrolls datasets are characterized by an average sample length of approximately
    4300 words per topic, closely matching the maximum window size of pre-trained
    Llama2-7B-chat models. This alignment indicates that the datasets are well-suited
    for these models, ensuring effective processing and analysis without the risk
    of truncating important information. Table [2](#S5.T2 "Table 2 ‣ 5.2 Experimental
    Results on Long-context Tasks ‣ 5 Experiment ‣ Model Tells You Where to Merge:
    Adaptive KV Cache Merging for LLMs on Long-Context Tasks") demonstrates that our
    proposed KV cache merging method effectively restores the performance of the Llama2-7B-chat
    model across all selected ZeroScrolls datasets under both $35\%$ cache budgets.
    This suggests that *KVMerger* not only mitigates performance degradation but also
    optimizes the model’s handling of extensive data sequences that approach the model’s
    maximum context window, contributing to more robust outcomes.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'ZeroScrolls 结果 我们还使用不同的 KV 缓存压缩技术评估了 Llama2-7B-chat 在 ZeroScrolls 数据集上的表现，如表
    [2](#S5.T2 "Table 2 ‣ 5.2 Experimental Results on Long-context Tasks ‣ 5 Experiment
    ‣ Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context
    Tasks") 所示。ZeroScrolls 数据集的特点是每个主题的平均样本长度约为 4300 个词，接近预训练的 Llama2-7B-chat 模型的最大窗口大小。这种对齐表明这些数据集非常适合这些模型，确保了有效的处理和分析，没有截断重要信息的风险。表
    [2](#S5.T2 "Table 2 ‣ 5.2 Experimental Results on Long-context Tasks ‣ 5 Experiment
    ‣ Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context
    Tasks") 显示了我们提出的 KV 缓存合并方法在 $35\%$ 缓存预算下，在所有选定的 ZeroScrolls 数据集上有效恢复了 Llama2-7B-chat
    模型的性能。这表明 *KVMerger* 不仅减轻了性能下降，还优化了模型对接近模型最大上下文窗口的广泛数据序列的处理，从而贡献了更强的结果。'
- en: 'Table 2: *KVMerger* for Llama2-7B-chat on selected ZeroScrolls datasets'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: *KVMerger* 在选定的 ZeroScrolls 数据集上对 Llama2-7B-chat 的表现'
- en: '| cache budget | Method | gov_report | SummScreenFD | QMSum | SQuALITY | Qasper
    | NarrativeQA | BookSumSort | avg. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 缓存预算 | 方法 | gov_report | SummScreenFD | QMSum | SQuALITY | Qasper | NarrativeQA
    | BookSumSort | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 100% | Full Cache | 17.40 | 14.10 | 15.20 | 19.50 | 22.50 | 15.40 | 3.00
    | 15.30 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 100% | 完全缓存 | 17.40 | 14.10 | 15.20 | 19.50 | 22.50 | 15.40 | 3.00 | 15.30
    |'
- en: '|  | H2O | 15.40 | 13.20 | 14.30 | 18.30 | 20.50 | 15.00 | 3.80 | 14.36 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | H2O | 15.40 | 13.20 | 14.30 | 18.30 | 20.50 | 15.00 | 3.80 | 14.36 |'
- en: '|  | CaM | 15.60 | 13.10 | 13.70 | 18.50 | 20.10 | 15.30 | 3.40 | 14.24 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | CaM | 15.60 | 13.10 | 13.70 | 18.50 | 20.10 | 15.30 | 3.40 | 14.24 |'
- en: '| 50% | *KVMerger* | 17.70 | 13.80 | 15.10 | 19.10 | 22.50 | 15.20 | 3.10 |
    15.21 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 50% | *KVMerger* | 17.70 | 13.80 | 15.10 | 19.10 | 22.50 | 15.20 | 3.10 |
    15.21 |'
- en: '|  | H2O | 14.80 | 11.60 | 14.20 | 17.80 | 17.70 | 14.70 | 3.60 | 13.49 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | H2O | 14.80 | 11.60 | 14.20 | 17.80 | 17.70 | 14.70 | 3.60 | 13.49 |'
- en: '|  | CaM | 15.30 | 11.70 | 13.90 | 18.30 | 17.10 | 14.50 | 3.30 | 13.44 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | CaM | 15.30 | 11.70 | 13.90 | 18.30 | 17.10 | 14.50 | 3.30 | 13.44 |'
- en: '| 35% | *KVMerger* | 16.60 | 13.80 | 15.40 | 18.60 | 20.40 | 15.40 | 3.70 |
    14.84 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 35% | *KVMerger* | 16.60 | 13.80 | 15.40 | 18.60 | 20.40 | 15.40 | 3.70 |
    14.84 |'
- en: 'Needle In A Haystack Results We also conduct a detailed comparison of *KVMerger*
    with other KV cache compression techniques on retrieval tasks using the needle-in-a-haystack
    test. This test involves placing a random fact or statement in the middle of a
    long context window and assessing the model’s ability to retrieve this statement
    across varying document depths and context lengths to measure performance. Specifically,
    we test Llama2-7B-chat on document depths ranging from $5\%$ cache budgets. The
    corresponding results are illustrated as Figure [5](#S5.F5 "Figure 5 ‣ 5.2 Experimental
    Results on Long-context Tasks ‣ 5 Experiment ‣ Model Tells You Where to Merge:
    Adaptive KV Cache Merging for LLMs on Long-Context Tasks"). Our findings indicate
    that both CaM and our merging algorithm outperform the eviction method H2O. However,
    our proposed method achieves the highest retrieval performance, consistently delivering
    high scores across various context lengths and depth percentages. Notably, even
    when the context length exceeds the pre-trained context length of the Llama2-7B-chat
    model, our method maintains high scores at specific depth percentages.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '针对“干草堆中的针”结果，我们还对*KVMerger*与其他KV缓存压缩技术在检索任务中的表现进行了详细比较，使用了干草堆测试。该测试将一个随机事实或声明放置在长上下文窗口的中间，并评估模型在不同文档深度和上下文长度下检索该声明的能力，以测量性能。具体来说，我们对Llama2-7B-chat在从$5\%$缓存预算开始的文档深度进行了测试。相关结果如图
    [5](#S5.F5 "Figure 5 ‣ 5.2 Experimental Results on Long-context Tasks ‣ 5 Experiment
    ‣ Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context
    Tasks")所示。我们的研究结果表明，CaM和我们提出的合并算法都优于驱逐方法H2O。然而，我们提出的方法在各种上下文长度和深度百分比下都实现了最高的检索性能，始终提供高分数。值得注意的是，即使上下文长度超过了Llama2-7B-chat模型的预训练上下文长度，我们的方法在特定深度百分比下仍保持高分。'
- en: 5.3 Ablation Study
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融研究
- en: 'Choice of $\sigma$ as expressed in Equation 4 for each merging set at different
    layers and found that the average value of computed $\sigma$ for most layers fluctuates
    around 5, which aligns with the experiment results in Table [3](#S5.T3 "Table
    3 ‣ 5.3 Ablation Study ‣ 5 Experiment ‣ Model Tells You Where to Merge: Adaptive
    KV Cache Merging for LLMs on Long-Context Tasks").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '对$\sigma$的选择，如方程 4 所示，在不同层级的每个合并集上，发现大多数层级计算出的$\sigma$的平均值在5左右波动，这与表 [3](#S5.T3
    "Table 3 ‣ 5.3 Ablation Study ‣ 5 Experiment ‣ Model Tells You Where to Merge:
    Adaptive KV Cache Merging for LLMs on Long-Context Tasks")中的实验结果一致。'
- en: 'Table 3: *KVMerger* with different $\sigma$ cache budget.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: *KVMerger* 与不同 $\sigma$ 缓存预算。'
- en: '| $\sigma$ | 2wikimqa | gov_report | narrativeqa | pr_en | multifieldqa_en
    | avg. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| $\sigma$ | 2wikimqa | gov_report | narrativeqa | pr_en | multifieldqa_en
    | avg. |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0.5 | 29.45 | 24.11 | 18.82 | 6.00 | 35.56 | 22.79 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 29.45 | 24.11 | 18.82 | 6.00 | 35.56 | 22.79 |'
- en: '| 1 | 31.48 | 25.52 | 18.98 | 6.25 | 36.59 | 23.76 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 31.48 | 25.52 | 18.98 | 6.25 | 36.59 | 23.76 |'
- en: '| 2 | 28.65 | 25.16 | 18.64 | 4.17 | 36.79 | 22.68 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 28.65 | 25.16 | 18.64 | 4.17 | 36.79 | 22.68 |'
- en: '| 3 | 30.84 | 25.19 | 18.51 | 4.67 | 37.48 | 23.34 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 30.84 | 25.19 | 18.51 | 4.67 | 37.48 | 23.34 |'
- en: '| 4 | 31.59 | 25.65 | 18.09 | 5.83 | 36.25 | 23.48 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 31.59 | 25.65 | 18.09 | 5.83 | 36.25 | 23.48 |'
- en: '| 5 | 32.99 | 25.31 | 18.50 | 7.33 | 36.89 | 24.20 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 32.99 | 25.31 | 18.50 | 7.33 | 36.89 | 24.20 |'
- en: '| 6 | 31.69 | 25.39 | 18.45 | 7.83 | 35.82 | 23.84 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 31.69 | 25.39 | 18.45 | 7.83 | 35.82 | 23.84 |'
- en: 'Choice of Pivotal State in Gaussian Kernel Weighted Merging As mentioned in
    Section 4.2, the selection of pivotal state for each merging set is directly related
    to the performance of *KVMerger*. The inappropriate selection of pivotal states
    will result in the severe information distortion and even much worse information
    loss than eviction-based compression algorithms. To show the significance of defining
    the pivotal state as the state with the biggest aggregated attention scores, we
    compare it with randomly selecting pivotal state within each merging set by using
    Llama2-7B-chat model with $50\%$ cache budget. The comparison is shown in Table
    [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Study ‣ 5 Experiment ‣ Model Tells You Where
    to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks"), from which
    we can see that randomly selecting pivotal states are detrimental to LLMs’ performance
    on long-context tasks.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯核加权合并中的关键状态选择 如第 4.2 节所述，每个合并集的关键状态选择与 *KVMerger* 的性能直接相关。关键状态的不当选择将导致严重的信息失真，甚至比基于驱逐的压缩算法造成更严重的信息丢失。为了展示将关键状态定义为具有最大累积注意力得分的状态的重要性，我们将其与在每个合并集中随机选择关键状态进行比较，使用了
    Llama2-7B-chat 模型和 $50\%$ 的缓存预算。比较结果见表 [4](#S5.T4 "表 4 ‣ 5.3 消融研究 ‣ 5 实验 ‣ 模型告诉你在哪里合并：针对长上下文任务的自适应
    KV 缓存合并")，从中可以看出，随机选择关键状态对 LLM 在长上下文任务中的性能是不利的。
- en: 'Table 4: *KVMerger* with different methods of pivotal states selection.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: *KVMerger* 与不同的关键状态选择方法。'
- en: '| Pivotal State | 2wikimqa | gov_report | narrativeqa | pr_en | multifieldqa_en
    | avg. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 关键状态 | 2wikimqa | gov_report | narrativeqa | pr_en | multifieldqa_en | 平均值
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ours | 32.99 | 25.31 | 18.50 | 7.33 | 36.89 | 24.20 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 32.99 | 25.31 | 18.50 | 7.33 | 36.89 | 24.20 |'
- en: '| Random | 30.01 | 24.07 | 17.72 | 6.50 | 33.30 | 22.12 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 30.01 | 24.07 | 17.72 | 6.50 | 33.30 | 22.12 |'
- en: 6 Conclusion and Future Work
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this paper, we propose *KVMerger*, a dynamic KV cache merging method inspired
    by the observation that key states exhibit high and persistent similarity within
    each sequence, allowing for layer-wise KV cache compression. We initially abstract
    the merging set identification problem as a constrained clustering problem and
    introduce a variant of the AHC algorithm to identify merging sets based on cosine
    similarities between key states. Furthermore, we implement a Gaussian Kernel weighted
    merging method to merge key and value states within each merging set. Compared
    to other KV cache eviction and merging methods, our approach achieves superior
    results on the LongBench datasets under the same cache budget. Additionally, our
    method effectively recovers the model’s long-context retrieval capabilities, as
    demonstrated by the needle-in-a-haystack tests.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 *KVMerger*，这是一种动态的 KV 缓存合并方法，灵感来源于观察到的关键状态在每个序列中表现出高度和持久的相似性，从而允许逐层
    KV 缓存压缩。我们最初将合并集识别问题抽象为一个受限的聚类问题，并引入了 AHC 算法的变体，通过关键状态之间的余弦相似度来识别合并集。此外，我们实现了一种高斯核加权合并方法来合并每个合并集中的关键状态和值状态。与其他
    KV 缓存驱逐和合并方法相比，我们的方法在相同缓存预算下在 LongBench 数据集上取得了更优的结果。此外，我们的方法有效恢复了模型的长上下文检索能力，这在针孔测试中得到了验证。
- en: Future work can explore several avenues to enhance and extend our proposed method.
    First, investigating the impact of different clustering algorithms and similarity
    measurements could provide insights into further optimizing the merging sets.
    Second, applying our method to other LLMs including long-context fine-tuned models
    and datasets would help assess its generalizability and robustness. Third, exploring
    hybrid approaches that combine cache merging with other memory management techniques
    might yield even more efficient solutions for long-context retrieval tasks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的工作可以探索几个途径来增强和扩展我们提出的方法。首先，研究不同聚类算法和相似度测量的影响可能会提供进一步优化合并集的见解。其次，将我们的方法应用于包括长上下文微调模型和数据集的其他
    LLM 可能有助于评估其普遍性和鲁棒性。第三，探索将缓存合并与其他内存管理技术相结合的混合方法，可能会为长上下文检索任务提供更高效的解决方案。
- en: Acknowledgments
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work used Delta system at the National Center for Supercomputing Applications
    through allocation CIS240055 from the Advanced Cyberinfrastructure Coordination
    Ecosystem: Services & Support (ACCESS) program. ACCESS (Boerner et al., [2023](#bib.bib3))
    is an advanced computing and data resource program supported by the U.S. National
    Science Foundation (NSF) under the Office of Advanced Cyberinfrastructure awards
    #2138259, #2138286, #2138307, #2137603 and #2138296\. The Delta advanced computing
    resource is a joint effort of the University of Illinois Urbana-Champaign and
    the National Center for Supercomputing Applications, and it is supported by the
    National Science Foundation (award OAC 2005572) and the State of Illinois. The
    work also used the Illinois Campus Cluster and NCSA NFI Hydro cluster, which are
    supported by the University of Illinois Urbana-Champaign and the University of
    Illinois System.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用了国家超级计算应用中心的Delta系统，通过高级网络基础设施协调生态系统：服务与支持（ACCESS）项目的分配CIS240055。ACCESS（Boerner等，[2023](#bib.bib3)）是一个由美国国家科学基金会（NSF）在高级网络基础设施办公室下支持的先进计算和数据资源项目。Delta先进计算资源是伊利诺伊大学厄本那-香槟分校和国家超级计算应用中心的联合努力，获得了国家科学基金会（奖项OAC
    2005572）和伊利诺伊州的支持。该工作还使用了伊利诺伊校园集群和NCSA NFI Hydro集群，这些集群由伊利诺伊大学厄本那-香槟分校和伊利诺伊大学系统支持。
- en: References
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: Anthropic (2023) Anthropic. Claude. [https://www.anthropic.com/claude](https://www.anthropic.com/claude),
    2023. Language model developed by Anthropic.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2023) Anthropic。Claude。 [https://www.anthropic.com/claude](https://www.anthropic.com/claude)，2023。由Anthropic开发的语言模型。
- en: 'Bai et al. (2024) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding,
    2024. URL [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2024) 白雨时，吕欣，张佳杰，吕鸿畅，唐建凯，黄智典，杜正孝，刘晓，曾奥涵，侯磊，董雨霄，唐杰，李娟子。Longbench：一个用于长上下文理解的双语多任务基准，2024。网址
    [https://arxiv.org/abs/2308.14508](https://arxiv.org/abs/2308.14508)。
- en: 'Boerner et al. (2023) Timothy J. Boerner, Stephen Deems, Thomas R. Furlani,
    Shelley L. Knuth, and John Towns. ACCESS: Advancing Innovation: NSF’s Advanced
    Cyberinfrastructure Coordination Ecosystem: Services & Support. In *Practice and
    Experience in Advanced Research Computing (PEARC’23)*, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boerner et al. (2023) 提摩太·J·博尔纳，斯蒂芬·迪姆斯，托马斯·R·弗尔拉尼，谢丽·L·克努斯，约翰·汤斯。ACCESS：推动创新：NSF的高级网络基础设施协调生态系统：服务与支持。在
    *实践与高级研究计算的经验（PEARC’23）*，2023。
- en: 'Bolya et al. (2023) Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang,
    Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster,
    2023. URL [https://arxiv.org/abs/2210.09461](https://arxiv.org/abs/2210.09461).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolya et al. (2023) 丹尼尔·博利亚，傅程阳，戴晓亮，张佩钊，克里斯托夫·费赫滕霍夫，朱迪·霍夫曼。Token merging：你的vit但更快，2023。网址
    [https://arxiv.org/abs/2210.09461](https://arxiv.org/abs/2210.09461)。
- en: 'Dai et al. (2024) Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng
    Cai, Wei Bi, and Shuming Shi. Corm: Cache optimization with recent message for
    large language model inference, 2024. URL [https://arxiv.org/abs/2404.15949](https://arxiv.org/abs/2404.15949).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2024) 戴金城，黄卓伟，姜海云，陈辰，戴磊，毕伟，施书铭。Corm：用于大型语言模型推理的近期消息缓存优化，2024。网址
    [https://arxiv.org/abs/2404.15949](https://arxiv.org/abs/2404.15949)。
- en: 'Ge et al. (2024) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression
    for llms, 2024. URL [https://arxiv.org/abs/2310.01801](https://arxiv.org/abs/2310.01801).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. (2024) 葛素玉，张雨南，刘丽媛，张敏佳，韩家伟，高剑锋。模型告诉你该丢弃什么：用于大型语言模型的自适应kv缓存压缩，2024。网址
    [https://arxiv.org/abs/2310.01801](https://arxiv.org/abs/2310.01801)。
- en: 'Gemini Team (2024) etc. Gemini Team, Petko Georgiev. Gemini 1.5: Unlocking
    multimodal understanding across millions of tokens of context, 2024. URL [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini Team (2024) 等。Gemini Team，Petko Georgiev。Gemini 1.5：解锁跨百万标记上下文的多模态理解，2024。网址
    [https://arxiv.org/abs/2403.05530](https://arxiv.org/abs/2403.05530)。
- en: 'Guerreiro et al. (2023) Nuno M. Guerreiro, Elena Voita, and André F. T. Martins.
    Looking for a needle in a haystack: A comprehensive study of hallucinations in
    neural machine translation, 2023. URL [https://arxiv.org/abs/2208.05309](https://arxiv.org/abs/2208.05309).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guerreiro et al. (2023) 努诺·M· Guerreiro，埃琳娜·沃伊塔，安德烈·F·T·马丁斯。寻找大海中的针：神经机器翻译中幻觉的综合研究，2023。网址
    [https://arxiv.org/abs/2208.05309](https://arxiv.org/abs/2208.05309)。
- en: 'He et al. (2024) Yefei He, Luoming Zhang, Weijia Wu, Jing Liu, Hong Zhou, and
    Bohan Zhuang. Zipcache: Accurate and efficient kv cache quantization with salient
    token identification, 2024. URL [https://arxiv.org/abs/2405.14256](https://arxiv.org/abs/2405.14256).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何等（2024）何烨飞、张洛铭、吴伟佳、刘静、周红和庄博涵。《Zipcache：通过显著Token识别实现准确高效的KV缓存量化》，2024年。网址 [https://arxiv.org/abs/2405.14256](https://arxiv.org/abs/2405.14256)。
- en: 'Hooper et al. (2024) Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W.
    Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10
    million context length LLM inference with KV cache quantization. *CoRR*, abs/2401.18079,
    2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡珀等（2024）科尔曼·胡珀、金世勋、赫瓦·穆罕默德扎赫、迈克尔·W·马赫尼、索菲亚·姚、库尔特·凯特泽和阿米尔·戈拉米。《Kvquant：通过KV缓存量化实现1000万上下文长度LLM推理》。*CoRR*，abs/2401.18079，2024年。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b, 2023. URL [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等（2023）蒋阿尔伯特·Q、亚历山大·萨布莱罗勒斯、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦基尔、吉约姆·兰普尔、露西尔·索尼尔、莱利奥·雷纳德·拉沃、玛丽-安娜·拉肖、皮埃尔·斯托克、特文·勒·斯卡奥、提博·拉夫里尔、托马斯·王、蒂莫泰·拉克鲁瓦和威廉·埃尔·萨耶德。《Mistral
    7b》，2023年。网址 [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825)。
- en: '(12) George Kamradt. Llmtest: Needle in a haystack. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
    Accessed: 2024-07-08.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （12）乔治·卡姆拉德。《Llmtest：大海捞针》。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。访问时间：2024-07-08。
- en: 'Kang et al. (2024) Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing
    Liu, Tushar Krishna, and Tuo Zhao. Gear: An efficient kv cache compression recipe
    for near-lossless generative inference of llm, 2024. URL [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 康等（2024）康浩、张青如、昆杜·苏维克、郑建华、刘早兴、克里希纳·图莎尔和赵拓。《Gear：高效的KV缓存压缩配方用于近乎无损的LLM生成推理》，2024年。网址
    [https://arxiv.org/abs/2403.05527](https://arxiv.org/abs/2403.05527)。
- en: 'Kim et al. (2023) Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, and
    Hongxia Jin. Token fusion: Bridging the gap between token pruning and token merging,
    2023. URL [https://arxiv.org/abs/2312.01026](https://arxiv.org/abs/2312.01026).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等（2023）金敏哲、蒋尚谦、许彦昌、沈宜琳和金洪霞。《Token Fusion：缩小Token修剪与Token合并之间的差距》，2023年。网址 [https://arxiv.org/abs/2312.01026](https://arxiv.org/abs/2312.01026)。
- en: 'Lab (2024) TAU NLP Lab. Zeroscrolls: Zero-shot summarization and reasoning
    language system. [https://github.com/tau-nlp/zero_scrolls](https://github.com/tau-nlp/zero_scrolls),
    2024. Accessed: 2024-07-02.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAU NLP实验室（2024）《Zeroscrolls：零样本总结与推理语言系统》。 [https://github.com/tau-nlp/zero_scrolls](https://github.com/tau-nlp/zero_scrolls)，2024年。访问时间：2024-07-02。
- en: 'Liu et al. (2024) Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari,
    and Bohan Zhuang. Minicache: Kv cache compression in depth dimension for large
    language models, 2024. URL [https://arxiv.org/abs/2405.14366](https://arxiv.org/abs/2405.14366).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2024）刘阿基德、刘静、潘子政、何烨飞、哈夫里和庄博涵。《Minicache：大规模语言模型中的深度维度KV缓存压缩》，2024年。网址 [https://arxiv.org/abs/2405.14366](https://arxiv.org/abs/2405.14366)。
- en: 'Liu et al. (2023a) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    LLM-QAT: data-free quantization aware training for large language models. *CoRR*,
    abs/2305.17888, 2023a.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023a）刘泽春、巴拉斯·奥古兹、赵长生、张恩妮、皮埃尔·斯托克、雅沙尔·梅赫达、石杨杨、拉古拉曼·克里希纳穆尔提和维卡斯·钱德拉。《LLM-QAT：面向大规模语言模型的数据无关量化感知训练》。*CoRR*，abs/2305.17888，2023a年。
- en: 'Liu et al. (2023b) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for llm kv cache compression
    at test time, 2023b. URL [https://arxiv.org/abs/2305.17118](https://arxiv.org/abs/2305.17118).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023b）刘子昌、阿迪提亚·德赛、廖方硕、王伟涛、维克多·谢、徐赵卓、安纳斯塔西奥斯·基里利迪斯和安舒马利·施里瓦斯塔瓦。《Scissorhands：利用重要性持久假设进行LLM
    KV缓存压缩的测试时应用》，2023b年。网址 [https://arxiv.org/abs/2305.17118](https://arxiv.org/abs/2305.17118)。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。《GPT-4技术报告》。*arXiv预印本arXiv:2303.08774*，2023年。
- en: Peysakhovich & Lerer (2023) Alexander Peysakhovich and Adam Lerer. Attention
    sorting combats recency bias in long context language models, 2023. URL [https://arxiv.org/abs/2310.01427](https://arxiv.org/abs/2310.01427).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peysakhovich & Lerer (2023) Alexander Peysakhovich 和 Adam Lerer。注意力排序对抗长上下文语言模型中的近期偏差，2023。网址
    [https://arxiv.org/abs/2310.01427](https://arxiv.org/abs/2310.01427)。
- en: Ren & Zhu (2024) Siyu Ren and Kenny Q. Zhu. On the efficacy of eviction policy
    for key-value constrained generative language model inference, 2024. URL [https://arxiv.org/abs/2402.06262](https://arxiv.org/abs/2402.06262).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren & Zhu (2024) Siyu Ren 和 Kenny Q. Zhu。关于针对键值约束生成语言模型推理的驱逐策略的有效性，2024。网址 [https://arxiv.org/abs/2402.06262](https://arxiv.org/abs/2402.06262)。
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding, 2023.
    URL [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, 和
    Omer Levy。Zeroscrolls: 一种零样本基准用于长文本理解，2023。网址 [https://arxiv.org/abs/2305.14196](https://arxiv.org/abs/2305.14196)。'
- en: 'Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen:
    High-throughput generative inference of large language models with a single GPU.
    In *International Conference on Machine Learning, ICML 2023, 23-29 July 2023,
    Honolulu, Hawaii, USA*, volume 202 of *Proceedings of Machine Learning Research*,
    pp.  31094–31116\. PMLR, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, 和 Ce Zhang。Flexgen:
    使用单个 GPU 的高通量生成推理大语言模型。在 *国际机器学习大会，ICML 2023，2023年7月23-29日，夏威夷檀香山，美国*，第202卷的 *机器学习研究论文集*
    中，第31094-31116页。PMLR，2023。'
- en: 'Su et al. (2023) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding,
    2023. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su et al. (2023) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    和 Yunfeng Liu。Roformer: 带旋转位置嵌入的增强变换器，2023。网址 [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)。'
- en: 'Tang et al. (2024) Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris
    Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context
    llm inference, 2024. URL [https://arxiv.org/abs/2406.10774](https://arxiv.org/abs/2406.10774).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang et al. (2024) Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris
    Kasikci, 和 Song Han。Quest: 具查询感知的稀疏性以高效进行长上下文 LLM 推理，2024。网址 [https://arxiv.org/abs/2406.10774](https://arxiv.org/abs/2406.10774)。'
- en: 'THUDM (2024) THUDM. Longbench. [https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench),
    2024. Accessed: 2024-07-02.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'THUDM (2024) THUDM。Longbench。 [https://github.com/THUDM/LongBench](https://github.com/THUDM/LongBench)，2024。访问时间:
    2024-07-02。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等人。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等人。Llama 2: 开放的基础模型和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。'
- en: 'Wan et al. (2024) Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao,
    Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, and Mi Zhang. D2o: Dynamic discriminative
    operations for efficient generative inference of large language models, 2024.
    URL [https://arxiv.org/abs/2406.13035](https://arxiv.org/abs/2406.13035).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan et al. (2024) Zhongwei Wan, Xinjian Wu, Yu Zhang, Yi Xin, Chaofan Tao,
    Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, 和 Mi Zhang。D2o: 生成推理中的动态判别操作以提高大语言模型的效率，2024。网址
    [https://arxiv.org/abs/2406.13035](https://arxiv.org/abs/2406.13035)。'
- en: Xiao et al. (2024) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks, 2024. URL
    [https://arxiv.org/abs/2309.17453](https://arxiv.org/abs/2309.17453).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2024) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike
    Lewis。高效流式语言模型与注意力汇集，2024。网址 [https://arxiv.org/abs/2309.17453](https://arxiv.org/abs/2309.17453)。
- en: 'Yang et al. (2024) June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon,
    Gunho Park, Eunho Yang, Se Jung Kwon, and Dongsoo Lee. No token left behind: Reliable
    kv cache compression via importance-aware mixed precision quantization, 2024.
    URL [https://arxiv.org/abs/2402.18096](https://arxiv.org/abs/2402.18096).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2024）**June Yong Yang**、**Byeongwook Kim**、**Jeongin Bae**、**Beomseok
    Kwon**、**Gunho Park**、**Eunho Yang**、**Se Jung Kwon** 和 **Dongsoo Lee**。不遗漏任何标记：通过重要性感知混合精度量化实现可靠的
    kv 缓存压缩，2024。网址 [https://arxiv.org/abs/2402.18096](https://arxiv.org/abs/2402.18096)。
- en: Yu et al. (2024a) Hao Yu, Zelan Yang, Shen Li, Yong Li, and Jianxin Wu. Effectively
    compress kv heads for llm, 2024a. URL [https://arxiv.org/abs/2406.07056](https://arxiv.org/abs/2406.07056).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2024a）**Hao Yu**、**Zelan Yang**、**Shen Li**、**Yong Li** 和 **Jianxin Wu**。有效压缩
    LLM 的 kv 头，2024a。网址 [https://arxiv.org/abs/2406.07056](https://arxiv.org/abs/2406.07056)。
- en: 'Yu et al. (2024b) Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid
    Shaikh, and Yingyan Celine Lin. Unveiling and harnessing hidden attention sinks:
    Enhancing large language models without training through attention calibration,
    2024b. URL [https://arxiv.org/abs/2406.15765](https://arxiv.org/abs/2406.15765).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2024b）**Zhongzhi Yu**、**Zheng Wang**、**Yonggan Fu**、**Huihong Shi**、**Khalid
    Shaikh** 和 **Yingyan Celine Lin**。揭示并利用隐藏的注意力沉没点：通过注意力校准增强大型语言模型，无需训练，2024b。网址
    [https://arxiv.org/abs/2406.15765](https://arxiv.org/abs/2406.15765)。
- en: 'Zeng et al. (2022) Wang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli
    Ouyang, and Xiaogang Wang. Not all tokens are equal: Human-centric visual analysis
    via token clustering transformer, 2022. URL [https://arxiv.org/abs/2204.08680](https://arxiv.org/abs/2204.08680).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等（2022）**Wang Zeng**、**Sheng Jin**、**Wentao Liu**、**Chen Qian**、**Ping
    Luo**、**Wanli Ouyang** 和 **Xiaogang Wang**。并非所有标记都相同：通过标记聚类转换器进行以人为本的视觉分析，2022。网址
    [https://arxiv.org/abs/2204.08680](https://arxiv.org/abs/2204.08680)。
- en: 'Zhang et al. (2024a) Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu,
    Qin Jin, Ji Zhang, and Fei Huang. Tinychart: Efficient chart understanding with
    visual token merging and program-of-thoughts learning, 2024a. URL [https://arxiv.org/abs/2404.16635](https://arxiv.org/abs/2404.16635).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2024a）**Liang Zhang**、**Anwen Hu**、**Haiyang Xu**、**Ming Yan**、**Yichen
    Xu**、**Qin Jin**、**Ji Zhang** 和 **Fei Huang**。Tinychart: 通过视觉标记合并和思想程序学习实现高效图表理解，2024a。网址
    [https://arxiv.org/abs/2404.16635](https://arxiv.org/abs/2404.16635)。'
- en: 'Zhang et al. (2024b) Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, and Anshumali Shrivastava.
    Kv cache is 1 bit per channel: Efficient large language model inference with coupled
    quantization, 2024b. URL [https://arxiv.org/abs/2405.03917](https://arxiv.org/abs/2405.03917).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024b）**Tianyi Zhang**、**Jonah Yi**、**Zhaozhuo Xu** 和 **Anshumali Shrivastava**。Kv
    缓存每通道 1 位：通过耦合量化实现高效的大型语言模型推理，2024b。网址 [https://arxiv.org/abs/2405.03917](https://arxiv.org/abs/2405.03917)。
- en: '(37) Yuxin Zhang, Yuxuan Du, Gen Luo, Yunshan Zhong, Zhenyu Zhang, Shiwei Liu,
    and Rongrong Ji. Cam: Cache merging for memory-efficient llms inference. In *Forty-first
    International Conference on Machine Learning*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(37) **Yuxin Zhang**、**Yuxuan Du**、**Gen Luo**、**Yunshan Zhong**、**Zhenyu Zhang**、**Shiwei
    Liu** 和 **Rongrong Ji**。Cam: 用于内存高效 LLM 推理的缓存合并。发表于 *第四十一届国际机器学习会议*。'
- en: 'Zhang et al. (2023) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang
    Wang, and Beidi Chen. H[2]o: Heavy-hitter oracle for efficient generative inference
    of large language models, 2023. URL [https://arxiv.org/abs/2306.14048](https://arxiv.org/abs/2306.14048).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2023）**Zhenyu Zhang**, **Ying Sheng**, **Tianyi Zhou**, **Tianlong
    Chen**, **Lianmin Zheng**, **Ruisi Cai**, **Zhao Song**, **Yuandong Tian**, **Christopher
    Ré**, **Clark Barrett**, **Zhangyang Wang** 和 **Beidi Chen**。H[2]o: 高效生成推理的大型语言模型的重型预言机，2023。网址
    [https://arxiv.org/abs/2306.14048](https://arxiv.org/abs/2306.14048)。'
- en: 'Zhang et al. (2024c) Zhenyu Zhang, Shiwei Liu, Runjin Chen, Bhavya Kailkhura,
    Beidi Chen, and Atlas Wang. Q-hitter: A better token oracle for efficient llm
    inference via sparse-quantized kv cache. *Proceedings of Machine Learning and
    Systems*, 6:381–394, 2024c.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2024c）**Zhenyu Zhang**、**Shiwei Liu**、**Runjin Chen**、**Bhavya Kailkhura**、**Beidi
    Chen** 和 **Atlas Wang**。Q-hitter: 通过稀疏量化 kv 缓存实现高效 LLM 推理的更好标记预言机。*机器学习与系统会议论文集*，6:381–394，2024c。'
- en: 'Zirui Liu et al. (2023) Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong,
    Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi : Plug-and-play
    2bit kv cache quantization with streaming asymmetric quantization. 2023. doi:
    10.13140/RG.2.2.28167.37282. URL [https://rgdoi.net/10.13140/RG.2.2.28167.37282](https://rgdoi.net/10.13140/RG.2.2.28167.37282).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zirui Liu 等（2023）**Zirui Liu**、**Jiayi Yuan**、**Hongye Jin**、**Shaochen Zhong**、**Zhaozhuo
    Xu**、**Vladimir Braverman**、**Beidi Chen** 和 **Xia Hu**。Kivi: 即插即用 2bit kv 缓存量化与流式不对称量化，2023。doi:
    10.13140/RG.2.2.28167.37282。网址 [https://rgdoi.net/10.13140/RG.2.2.28167.37282](https://rgdoi.net/10.13140/RG.2.2.28167.37282)。'
- en: Appendix A Appendix
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: Lemma 3.1 (Formal version of Lemma 3.1). *Consider two vectors $\mathbf{k}_{m}$-th
    elements of $\mathbf{k}_{m}$.*
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 3.1（引理 3.1 的正式版本）。*考虑两个向量 $\mathbf{k}_{m}$-th 元素的 $\mathbf{k}_{m}$。*
- en: '*Proof.* Since'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明。* 由于'
- en: '|  | $\textit{similarity}\left(\mathbf{k}_{m},\mathbf{k}_{n}\right)=1,$ |  |
    (9) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textit{similarity}\left(\mathbf{k}_{m},\mathbf{k}_{n}\right)=1,$ |  |
    (9) |'
- en: $\mathbf{k}_{m}$ are collinear. Therefore,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{k}_{m}$是共线的。因此，
- en: '|  | $\mathbf{k}_{m}=\alpha\mathbf{k}_{n},$ |  | (10) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{k}_{m}=\alpha\mathbf{k}_{n},$ |  | (10) |'
- en: where $\alpha$ is a scalar. It means
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$是一个标量。这意味着
- en: '|  | $\displaystyle k_{m,2j}$ |  | (11) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle k_{m,2j}$ |  | (11) |'
- en: '|  | $\displaystyle k_{m,2j+1}$ |  | (12) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle k_{m,2j+1}$ |  | (12) |'
- en: So,
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，
- en: '|  | $\left[k_{m,2j},k_{m,2j+1}\right]^{T}=\alpha\left[k_{n,2j},k_{n,2j+1}\right]^{T}.$
    |  | (13) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $\left[k_{m,2j},k_{m,2j+1}\right]^{T}=\alpha\left[k_{n,2j},k_{n,2j+1}\right]^{T}.$
    |  | (13) |'
- en: As a result,
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，
- en: '|  | $\textit{similarity}\left(\mathbf{k}_{m,j},\mathbf{k}_{n,j}\right)=1$
    |  | (14) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textit{similarity}\left(\mathbf{k}_{m,j},\mathbf{k}_{n,j}\right)=1$
    |  | (14) |'
- en: Lemma 3.2 (Formal version of Lemma 3.2). *Consider integer $j$ as $\mathbf{k}_{m,j}^{{}^{\prime}}=\mathbf{k}_{m,j}/e^{im\theta_{j}}$,
    we have:*
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 引理3.2（引理3.2的正式版本）。*考虑整数$j$，若$\mathbf{k}_{m,j}^{{}^{\prime}}=\mathbf{k}_{m,j}/e^{im\theta_{j}}$，我们有：*
- en: '|  | $1$2 |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\langle\mathbf{k}_{m,j}^{{}^{\prime}},\mathbf{k}_{n,j}^{{}^{\prime}}\rangle$,
    respectively.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\langle\mathbf{k}_{m,j}^{{}^{\prime}},\mathbf{k}_{n,j}^{{}^{\prime}}\rangle$，分别。
- en: '*Proof.* Since $j$, so'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明。* 因为$j$，所以'
- en: '|  | $-1<\frac{-2j}{d}\leq 0.$ |  | (15) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | $-1<\frac{-2j}{d}\leq 0.$ |  | (15) |'
- en: And $b$ by default Su et al. ([2023](#bib.bib24)). Therefore,
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 并且$b$ 默认是Su等人（[2023](#bib.bib24)）。因此，
- en: '|  | $0<b^{\frac{-2j}{d}}\leq 1,$ |  | (16) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $0<b^{\frac{-2j}{d}}\leq 1,$ |  | (16) |'
- en: which means
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着
- en: '|  | $0<\theta_{j}\leq 1.$ |  | (17) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $0<\theta_{j}\leq 1.$ |  | (17) |'
- en: Now, focus on the similarity between $\mathbf{k}_{m,j}$, and
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关注$\mathbf{k}_{m,j}$之间的相似性，并且
- en: '|  | $1$2 |  | (18) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (18) |'
- en: It is easy to derive that
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 容易推导出
- en: '|  | $\displaystyle\&#124;\mathbf{k}_{m,j}^{{}^{\prime}}e^{im\theta_{j}}\&#124;$
    |  | (19) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\mathbf{k}_{m,j}^{{}^{\prime}}e^{im\theta_{j}}\&#124;$
    |  | (19) |'
- en: '|  | $\displaystyle\&#124;\mathbf{k}_{n,j}^{{}^{\prime}}e^{in\theta_{j}}\&#124;$
    |  | (20) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\mathbf{k}_{n,j}^{{}^{\prime}}e^{in\theta_{j}}\&#124;$
    |  | (20) |'
- en: since the exponential terms do not change the vectors’ magnitude.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因为指数项不改变向量的大小。
- en: Then, substitute the complex forms of $\mathbf{k}_{m,j}^{{}^{\prime}}$ and obtain
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，代入$\mathbf{k}_{m,j}^{{}^{\prime}}$的复数形式，得到
- en: '|  | $1$2 |  | (21) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (21) |'
- en: 'From Euler equation, Equation [21](#A1.E21 "In Appendix A Appendix ‣ Model
    Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks")
    can be further expanded as'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 根据欧拉方程，方程[21](#A1.E21 "在附录A附录 ‣ 模型告诉你在哪里合并：针对长上下文任务的自适应KV缓存合并")可以进一步扩展为
- en: '|  |  | $\displaystyle\langle\mathbf{k}_{m,j}^{{}^{\prime}}e^{im\theta_{j}},\mathbf{k}_{n,j}^{{}^{\prime}}e^{in\theta_{j}}\rangle$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\langle\mathbf{k}_{m,j}^{{}^{\prime}}e^{im\theta_{j}},\mathbf{k}_{n,j}^{{}^{\prime}}e^{in\theta_{j}}\rangle$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $1$2 |  | (22) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (22) |'
- en: 'Substitute Equation [22](#A1.E22 "In Appendix A Appendix ‣ Model Tells You
    Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks") back
    into Equation [18](#A1.E18 "In Appendix A Appendix ‣ Model Tells You Where to
    Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks"), and'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程[22](#A1.E22 "在附录A附录 ‣ 模型告诉你在哪里合并：针对长上下文任务的自适应KV缓存合并")代回方程[18](#A1.E18 "在附录A附录
    ‣ 模型告诉你在哪里合并：针对长上下文任务的自适应KV缓存合并")，然后
- en: '|  | $\displaystyle\frac{\langle\mathbf{k}_{m,j},\mathbf{k}_{n,j}\rangle}{\&#124;\mathbf{k}_{m,j}\&#124;\cdot\&#124;\mathbf{k}_{n,j}\&#124;}=$
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\langle\mathbf{k}_{m,j},\mathbf{k}_{n,j}\rangle}{\&#124;\mathbf{k}_{m,j}\&#124;\cdot\&#124;\mathbf{k}_{n,j}\&#124;}=$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $\displaystyle=$ |  | (23) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  | (23) |'
- en: 'Let $\phi$, then Equation [23](#A1.E23 "In Appendix A Appendix ‣ Model Tells
    You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks")
    can be rewrite as'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\phi$，则方程[23](#A1.E23 "在附录A附录 ‣ 模型告诉你在哪里合并：针对长上下文任务的自适应KV缓存合并")可以改写为
- en: '|  | $\displaystyle\frac{\langle\mathbf{k}_{m,j},\mathbf{k}_{n,j}\rangle}{\&#124;\mathbf{k}_{m,j}\&#124;\cdot\&#124;\mathbf{k}_{n,j}\&#124;}=$
    |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\langle\mathbf{k}_{m,j},\mathbf{k}_{n,j}\rangle}{\&#124;\mathbf{k}_{m,j}\&#124;\cdot\&#124;\mathbf{k}_{n,j}\&#124;}=$
    |  |'
- en: '|  | $\displaystyle=$ |  | (24) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  | (24) |'
- en: Since the similarity between $\mathbf{k}_{m,j}$ nearly equals 1 as we assumed,
    it can be obtained that
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设$\mathbf{k}_{m,j}$之间的相似性几乎等于1，可以得到
- en: '|  | $\phi=\left(m-n\right)\theta_{j}.$ |  | (25) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi=\left(m-n\right)\theta_{j}.$ |  | (25) |'
- en: 'From Equation [17](#A1.E17 "In Appendix A Appendix ‣ Model Tells You Where
    to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks"),'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 [17](#A1.E17 "在附录A附录 ‣ 模型告诉你在哪里合并：针对长上下文任务的自适应KV缓存合并")
- en: '|  | $\displaystyle 0<$ |  | (26) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 0<$ |  | (26) |'
- en: '|  | $\displaystyle m-n\leq$ |  | (27) |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m-n\leq$ |  | (27) |'
- en: As a result,
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $1$2 |  | (28) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (28) |'
