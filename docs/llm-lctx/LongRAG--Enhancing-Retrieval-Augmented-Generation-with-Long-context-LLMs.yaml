- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:02:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:02:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LongRAG：利用长上下文LLMs提升检索增强生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15319](https://ar5iv.labs.arxiv.org/html/2406.15319)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15319](https://ar5iv.labs.arxiv.org/html/2406.15319)
- en: Ziyan Jiang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ziyan Jiang
- en: University of Waterloo
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学
- en: ziyanjiang528@gmail.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ziyanjiang528@gmail.com
- en: '&Xueguang Ma'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Xueguang Ma'
- en: University of Waterloo
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学
- en: x93ma@uwaterloo.ca
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: x93ma@uwaterloo.ca
- en: '&Wenhu Chen'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Wenhu Chen'
- en: University of Waterloo
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 滑铁卢大学
- en: wenhuchen@uwaterloo.ca
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: wenhuchen@uwaterloo.ca
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In traditional RAG framework, the basic retrieval units are normally short.
    The common retrievers like DPR normally work with 100-word Wikipedia paragraphs.
    Such a design forces the retriever to search over a large corpus to find the ‘needle’
    unit. In contrast, the readers only need to extract answers from the short retrieved
    units. Such an imbalanced ‘heavy’ retriever and ‘light’ reader design can lead
    to sub-optimal performance. In order to alleviate the imbalance, we propose a
    new framework LongRAG, consisting of a ‘long retriever’ and a ‘long reader’. LongRAG
    processes the entire Wikipedia into 4K-token units, which is 30x longer than before.
    By increasing the unit size, we significantly reduce the total units from 22M
    to 600K. This significantly lowers the burden of retriever, which leads to a remarkable
    retrieval score: answer recall@1=71% on NQ (previously 52%) and answer recall@2=72%
    (previously 47%) on HotpotQA (full-wiki). Then we feed the top-k retrieved units
    ($\approx$ 30K tokens) to an existing long-context LLM to perform zero-shot answer
    extraction. Without requiring any training, LongRAG achieves an EM of 62.7% on
    NQ and 64.3% on HotpotQA (full-wiki), which is on par with the (fully-trained)
    SoTA model. Our study offers insights into the future roadmap for combining RAG
    with long-context LLMs.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统RAG框架中，基本的检索单元通常很短。像DPR这样的常见检索器通常处理100字的维基百科段落。这样的设计迫使检索器在大量语料库中搜索以找到‘针’单元。相比之下，读者只需要从短检索单元中提取答案。这样一种不平衡的‘重’检索器和‘轻’读者设计可能导致次优性能。为了缓解这种不平衡，我们提出了一个新的框架LongRAG，由‘长检索器’和‘长读者’组成。LongRAG将整个维基百科处理为4K-token单元，比之前长30倍。通过增加单元大小，我们将总单元从22M显著减少到600K。这大大减轻了检索器的负担，从而显著提高了检索评分：NQ上的答案召回@1=71%（之前为52%），HotpotQA（全维基百科）上的答案召回@2=72%（之前为47%）。然后我们将前k个检索单元（约30K
    tokens）输入现有的长上下文LLM，以执行零-shot答案提取。在不需要任何训练的情况下，LongRAG在NQ上获得了62.7%的EM，在HotpotQA（全维基百科）上获得了64.3%的EM，这与（完全训练的）SoTA模型相当。我们的研究为结合RAG与长上下文LLMs的未来发展路线图提供了见解。
- en: 'LongRAG: Enhancing Retrieval-Augmented Generation'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LongRAG：提升检索增强生成
- en: with Long-context LLMs
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用长上下文LLMs
- en: Ziyan Jiang University of Waterloo ziyanjiang528@gmail.com                       
    Xueguang Ma University of Waterloo x93ma@uwaterloo.ca                        Wenhu
    Chen University of Waterloo wenhuchen@uwaterloo.ca
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Ziyan Jiang 滑铁卢大学 ziyanjiang528@gmail.com                        Xueguang Ma
    滑铁卢大学 x93ma@uwaterloo.ca                        Wenhu Chen 滑铁卢大学 wenhuchen@uwaterloo.ca
- en: '[https://tiger-ai-lab.github.io/LongRAG/](https://tiger-ai-lab.github.io/LongRAG/)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://tiger-ai-lab.github.io/LongRAG/](https://tiger-ai-lab.github.io/LongRAG/)'
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/b529e520a039f0aeec3a053a2dc1d346.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b529e520a039f0aeec3a053a2dc1d346.png)'
- en: 'Figure 1: Traditional RAG vs. LongRAG. (Up) Traditional RAG operates on short
    retrieval units, where the retriever needs to scan over a massive amount of units
    to find the relevant piece. In contrast, LongRAG operates on long retrieval units
    (30x longer). Retriever has a much less workload, which significantly boosts the
    recall score. LongRAG fully exploits the ability of long-context language models
    to achieve strong performance.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：传统RAG与LongRAG的比较。（上）传统RAG在短检索单元上运行，检索器需要扫描大量单元以找到相关内容。相对而言，LongRAG在长检索单元（长30倍）上运行。检索器的工作量大大减少，这显著提高了召回率。LongRAG充分利用了长上下文语言模型的能力，取得了强劲的表现。
- en: Retrieval-Augmented Generation (RAG) methods have long been employed to enhance
    large language models (LLMs) Mialon et al. ([2023](#bib.bib35)). Knowledge in
    the form of natural language can be entirely offloaded from the parametric knowledge
    of LLMs by leveraging a standalone retrieval component from an external corpus.
    The existing RAG framework tends to use short retrieval units, such as 100-word
    passages in popular open-domain question-answering tasks Chen et al. ([2017](#bib.bib6));
    Lewis et al. ([2020](#bib.bib30)); Karpukhin et al. ([2020](#bib.bib25)). The
    retriever is tasked with finding the “needle” (i.e. the precise tiny retrieval
    unit) from the “haystack” (i.e. the massive corpus with tens of millions of information
    units). Subsequently, the retrieved units are passed to the reader to generate
    the final response. On the contrary, the reader only needs to extract answers
    from these retrievals, which is a fairly easy task. This kind of imbalanced design,
    with a “heavy” retriever and a “light” reader, puts too much pressure on the retriever.
    Therefore, existing RAG models Izacard and Grave ([2020b](#bib.bib21)) have to
    recall huge amounts of units, such as the top-100/200, combined with additional
    re-ranker to achieve the best performance. Moreover, short retrieval units can
    lead to semantic incompleteness due to document truncation. This can lead to information
    loss, ultimately hurting the end performance. This design choice was made in an
    era when NLP models were heavily restricted by their ability to handle long contexts.
    With the recent advances in long-context language models, the retriever and reader
    can potentially handle up to 128K or even millions of tokens as input Reid et al.
    ([2024](#bib.bib43)); Achiam et al. ([2023](#bib.bib1)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）方法已被长期用于增强大型语言模型（LLMs），Mialon 等人 ([2023](#bib.bib35))。通过利用外部语料库的独立检索组件，可以完全将自然语言形式的知识从LLMs的参数知识中卸载。现有的RAG框架往往使用短检索单元，例如在流行的开放域问答任务中使用100字的段落
    Chen 等人 ([2017](#bib.bib6)); Lewis 等人 ([2020](#bib.bib30)); Karpukhin 等人 ([2020](#bib.bib25))。检索器的任务是从“大海捞针”的“干草堆”（即包含数千万条信息单元的大型语料库）中找到“针”（即精确的微小检索单元）。随后，检索到的单元将传递给阅读器以生成最终回答。相反，阅读器只需从这些检索结果中提取答案，这是一项相对简单的任务。这种不平衡的设计，即“重”的检索器和“轻”的阅读器，给检索器施加了过大的压力。因此，现有的RAG模型
    Izacard 和 Grave ([2020b](#bib.bib21)) 不得不调用大量单元，如前100/200名，并结合额外的重新排序器以获得最佳性能。此外，短检索单元可能因文档截断而导致语义不完整。这可能导致信息丢失，最终影响最终性能。这种设计选择是在NLP模型严重受限于处理长上下文能力的时代做出的。随着最近长上下文语言模型的进展，检索器和阅读器可以潜在地处理多达128K甚至数百万的tokens作为输入
    Reid 等人 ([2024](#bib.bib43)); Achiam 等人 ([2023](#bib.bib1))。
- en: 'In this paper, we propose to revisit this design choice for open-domain question
    answering and propose the LongRAG framework as a solution to balance the workload
    between the retriever and the reader, as illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context
    LLMs"). There are three important designs in our novel framework:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们提出重新审视这一开放域问答设计选择，并提出LongRAG框架作为解决方案，以平衡检索器和阅读器之间的工作负载，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ LongRAG: Enhancing Retrieval-Augmented Generation
    with Long-context LLMs")所示。我们新框架中有三个重要设计：'
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Long Retrieval Unit: By using entire Wikipedia documents or grouping multiple
    related documents, we can construct long retrieval units with more than 4K tokens.
    This design could also significantly reduce the corpus size (number of retrieval
    units in the corpus). Then, the retriever’s task becomes much easier with more
    complete information.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长检索单元：通过使用整个维基百科文档或将多个相关文档分组，我们可以构建超过4K tokens的长检索单元。这种设计还可以显著减少语料库的大小（语料库中的检索单元数量）。这样，检索器的任务变得更加容易，因为可以获得更完整的信息。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Long Retriever: The long retriever will identify coarse relevant information
    for the given query by searching through all the long retrieval units in the corpus.
    Only the top 4 to 8 retrieval units (without re-ranking) are used for the next
    step.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长检索器：长检索器将通过搜索语料库中所有长检索单元来识别与给定查询相关的粗略信息。仅前4到8个检索单元（不进行重新排序）被用于下一步。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Long Reader: The long reader will further extract answers from the concatenation
    of retrievals, which is normally around 30K tokens. We simply prompt an existing
    long-context LM (like Gemini or GPT4) with the question to produce the answers
    in a zero-shot fashion.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长读者：长读者将进一步从检索结果的连接中提取答案，这通常约为30K个标记。我们简单地用问题提示现有的长上下文LM（如Gemini或GPT4），以零样本方式生成答案。
- en: 'These three novel designs significantly boost the overall performance of RAG
    on open-domain question-answering tasks like NQ Kwiatkowski et al. ([2019](#bib.bib29))
    and HotpotQA Yang et al. ([2018](#bib.bib53)). LongRAG has several advantages:
    1) It does not require additional re-rankers and the best results can be attained
    by only considering the top 4-8 retrieved units. 2) The long retrieval unit amalgamates
    comprehensive information from related documents, which can be used directly to
    answer multi-hop questions without iterative retrieval.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种新颖设计显著提升了RAG在开放域问答任务中的整体性能，如NQ Kwiatkowski等人（[2019](#bib.bib29)）和HotpotQA
    Yang等人（[2018](#bib.bib53)）。LongRAG具有几个优点：1）它不需要额外的重新排序器，最佳结果只需考虑前4-8个检索单元。2）长检索单元将相关文档中的全面信息融合在一起，可以直接用于回答多跳问题，而无需迭代检索。
- en: In our experiments, we adopt off-the-shelf retrievers like BGE Xiao et al. ([2023](#bib.bib50))
    and readers like Gemini-1.5-Pro (Reid et al., [2024](#bib.bib43)) or GPT-4o OpenAI
    ([2024](#bib.bib37)) without any tuning on NQ or HotpotQA. In our experiments,
    we reduce the NQ corpus size from 22M to 600K document units, which improves the
    answer recall@1 from 52% (DPR) to 71%. Similarly, we reduce the HotpotQA corpus
    size from 5M to 500K, which improves the recall@2 from 47% (DPR) to 72%. The improvement
    in retriever can significantly benefit the reader model. By exploiting the long-context
    understanding ability of GPT-4o, LongRAG can achieve an EM of 62% on NQ and 64%
    on HotpotQA. These results could be comparable to the strongest fully trained
    RAG models like Atlas Izacard et al. ([2022](#bib.bib22)) and MDR Xiong et al.
    ([2020b](#bib.bib52)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们采用了现成的检索器，如BGE Xiao等人（[2023](#bib.bib50)）和读者，如Gemini-1.5-Pro（Reid等人，[2024](#bib.bib43)）或GPT-4o
    OpenAI（[2024](#bib.bib37)），而没有在NQ或HotpotQA上进行任何调整。在我们的实验中，我们将NQ语料库的大小从22M减少到600K文档单元，这将答案召回@1从52%（DPR）提高到71%。类似地，我们将HotpotQA语料库的大小从5M减少到500K，这将召回@2从47%（DPR）提高到72%。检索器的改进可以显著有利于读者模型。通过利用GPT-4o的长上下文理解能力，LongRAG可以在NQ上实现62%的EM，在HotpotQA上实现64%的EM。这些结果可能与最强的完全训练RAG模型，如Atlas
    Izacard等人（[2022](#bib.bib22)）和MDR Xiong等人（[2020b](#bib.bib52)）相媲美。
- en: 'We perform ablation studies in [subsection 4.4](#S4.SS4 "4.4 Ablation Studies
    ‣ 4 Experiments ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context
    LLMs") to prove why longer retrieval units are necessary. Given a budget of 40K
    recall tokens, with ‘short retriever units’, we can increase the number of recalled
    units to reach a marvelously high recall score (91% for recall@200). However,
    the end performance dips significantly due to the huge amount of ‘hard negatives’,
    which confuses the reader. With ‘long retriever units’, we observe an entirely
    different trend. As we recall more units (from 1 to 8 units), both the recall
    and end performance will increase or plateau. The impact of ‘hard negative’ is
    much less severe in LongRAG. It shows that LongRAG can better exploit the advances
    in the long-context LLMs (reader). As the long-context methods evolve, the performance
    of LongRAG will continue to improve. Therefore, we believe the modern RAG systems
    should re-consider the granularity of their retrieval units to exploit the advantages
    of the current long-context LLMs.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[subsection 4.4](#S4.SS4 "4.4 Ablation Studies ‣ 4 Experiments ‣ LongRAG:
    Enhancing Retrieval-Augmented Generation with Long-context LLMs")进行的消融研究证明了为什么更长的检索单元是必要的。给定40K召回标记的预算，使用‘短检索单元’，我们可以增加召回单元的数量，从而达到非常高的召回分数（召回@200为91%）。然而，由于大量的‘难负样本’混淆了读者，最终性能显著下降。使用‘长检索单元’，我们观察到完全不同的趋势。随着我们召回更多的单元（从1到8个单元），召回率和最终性能都会增加或趋于平稳。在LongRAG中，‘难负样本’的影响要小得多。这表明LongRAG可以更好地利用长上下文LLMs（读者）的进展。随着长上下文方法的发展，LongRAG的性能将继续提高。因此，我们认为现代RAG系统应该重新考虑其检索单元的粒度，以利用当前长上下文LLMs的优势。'
- en: 'Meanwhile, there is still room for improvement in our framework, particularly
    the need for stronger long embedding models, as shown in Table [3](#S4.T3 "Table
    3 ‣ Experiment Setup ‣ 4.2 Retrieval Performance ‣ 4 Experiments ‣ LongRAG: Enhancing
    Retrieval-Augmented Generation with Long-context LLMs"). Additionally, more general
    methods to formulate long retrieval units beyond hyperlinks will be helpful.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '与此同时，我们的框架仍有改进的空间，特别是需要更强大的长嵌入模型，如表[3](#S4.T3 "Table 3 ‣ Experiment Setup ‣
    4.2 Retrieval Performance ‣ 4 Experiments ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs")所示。此外，制定超越超链接的长检索单元的更一般的方法将会有所帮助。'
- en: 2 Related Work
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Retrieval-Augmented Generation.
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 检索增强生成
- en: Augmenting language models with information retrieved from large corpora has
    become a popular and effective approach for knowledge-intensive tasks, particularly
    open-domain question answering. The predominant architecture follows a retriever-reader
    style Chen et al. ([2017](#bib.bib6)); Guu et al. ([2020](#bib.bib18)), where
    the input query retrieves information from a corpus, and a language model uses
    this information as additional context to make a final prediction. Recent work
    has focused on improving the retriever (Karpukhin et al., [2020](#bib.bib25);
    Xiong et al., [2020a](#bib.bib51); Qu et al., [2020](#bib.bib41); Xiong et al.,
    [2020b](#bib.bib52); Khalifa et al., [2023](#bib.bib27)), enhancing the reader
    (Izacard and Grave, [2020b](#bib.bib21); Cheng et al., [2021](#bib.bib10); Yu
    et al., [2021](#bib.bib54); Borgeaud et al., [2022](#bib.bib5)), fine-tuning the
    retriever and reader jointly (Yu, [2022](#bib.bib55); Izacard et al., [2022](#bib.bib22);
    Singh et al., [2021](#bib.bib46); Izacard and Grave, [2020a](#bib.bib20)), and
    integrating the retriever with the black-box language model (Yu et al., [2023](#bib.bib56);
    Shi et al., [2023](#bib.bib45); Trivedi et al., [2022](#bib.bib48)). However,
    the impact of document granularity on the effectiveness and efficiency of the
    retrieval-augmented generation pipeline remains underexplored.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用从大语料库中检索的信息增强语言模型已经成为一种流行且有效的方法，特别是在知识密集型任务中，尤其是开放领域问答。主要的架构遵循检索器-阅读器风格 Chen
    et al. ([2017](#bib.bib6)); Guu et al. ([2020](#bib.bib18))，其中输入查询从语料库中检索信息，语言模型使用这些信息作为附加上下文来做出最终预测。最近的工作集中在改善检索器
    (Karpukhin et al., [2020](#bib.bib25); Xiong et al., [2020a](#bib.bib51); Qu et
    al., [2020](#bib.bib41); Xiong et al., [2020b](#bib.bib52); Khalifa et al., [2023](#bib.bib27))、增强阅读器
    (Izacard and Grave, [2020b](#bib.bib21); Cheng et al., [2021](#bib.bib10); Yu
    et al., [2021](#bib.bib54); Borgeaud et al., [2022](#bib.bib5))、联合微调检索器和阅读器 (Yu,
    [2022](#bib.bib55); Izacard et al., [2022](#bib.bib22); Singh et al., [2021](#bib.bib46);
    Izacard and Grave, [2020a](#bib.bib20))，以及将检索器与黑箱语言模型集成 (Yu et al., [2023](#bib.bib56);
    Shi et al., [2023](#bib.bib45); Trivedi et al., [2022](#bib.bib48))。然而，文档粒度对检索增强生成管道的有效性和效率的影响仍未得到充分探讨。
- en: 2.2 Long Context Large Language Models.
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 长上下文大语言模型
- en: The effectiveness of Transformer-based models is hindered by the quadratic increase
    in computational cost relative to sequence length, especially when dealing with
    long context inputs. In order to solve this issue, different approaches have been
    proposed to mitigate computational issues, including sliding memory window and
    chunk segmentation (Hao et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib42);
    Zhu et al., [2024b](#bib.bib59)). FlashAttention Dao et al. ([2022](#bib.bib12))
    has also been a pivotal strategy to significantly reduce the memory footprint
    to almost linear w.r.t sequence length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的模型的有效性受到计算成本相对于序列长度的二次增加的限制，尤其是在处理长上下文输入时。为了解决这个问题，已经提出了不同的方法来缓解计算问题，包括滑动记忆窗口和块分割
    (Hao et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib42); Zhu et al.,
    [2024b](#bib.bib59))。FlashAttention Dao et al. ([2022](#bib.bib12)) 也成为了一项关键策略，能够显著减少内存占用，使其几乎与序列长度成线性关系。
- en: To enable length extrapolation, RoPE Su et al. ([2021](#bib.bib47)) and AliBI Press
    et al. ([2021](#bib.bib40)) position encodings have shown potential to enable
    length extrapolation, which have been widely used in the literature. Recent endeavors
    have explored diverse strategies to tackle this challenge, which is mainly Position
    reorganization (Jin et al., [2024](#bib.bib23); An et al., [2024](#bib.bib2)),
    Position interpolation (Chen et al., [2023a](#bib.bib8); Peng et al., [2023](#bib.bib39);
    Liu et al., [2024](#bib.bib33)). Furthermore, alternative architectures beyond
    the Transformer have been explored to handle long inputs more naturally. These
    diverse approaches claim that they can enhance the capabilities of LLMs in processing
    long context inputs more efficiently.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现长度外推，RoPE Su等（[2021](#bib.bib47)）和AliBI Press等（[2021](#bib.bib40)）的位置编码已显示出实现长度外推的潜力，这在文献中被广泛使用。近期的工作探索了多种策略来应对这一挑战，主要包括位置重组（Jin等，[2024](#bib.bib23)；An等，[2024](#bib.bib2)），位置插值（Chen等，[2023a](#bib.bib8)；Peng等，[2023](#bib.bib39)；Liu等，[2024](#bib.bib33)）。此外，还探索了超越Transformer的替代架构，以更自然地处理长输入。这些不同的方法声称它们可以提高LLMs处理长上下文输入的能力。
- en: 2.3 Long Context Embedding
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 长上下文嵌入
- en: Recent efforts also increased the context length for embedding models, extending
    the supported text snippet length from a limit of 512 tokens to 32k tokens. Typically,
    the development of long-context embedding models involves first obtaining a long-context
    backbone model. This can be achieved either by pre-training with long inputs from
    scratch Günther et al. ([2023](#bib.bib17)); Nussbaum et al. ([2024](#bib.bib36));
    Chen et al. ([2024](#bib.bib7)) or by utilizing existing large language models
    that support longer context Wang et al. ([2023](#bib.bib49)). Additionally, some
    works extend the capabilities of existing embedding models to handle long contexts
    by applying LLM content window extension methods on embedding models Zhu et al.
    ([2024a](#bib.bib58)); Peng and Quesnelle ([2023](#bib.bib38)), or by employing
    state-space encoder models Saad-Falcon et al. ([2024](#bib.bib44)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的努力还增加了嵌入模型的上下文长度，将支持的文本片段长度从512个标记扩展到32k个标记。通常，长上下文嵌入模型的发展涉及首先获得一个长上下文主干模型。这可以通过从头开始用长输入进行预训练（Günther等，[2023](#bib.bib17)；Nussbaum等，[2024](#bib.bib36)；Chen等，[2024](#bib.bib7)），或利用现有支持更长上下文的大型语言模型（Wang等，[2023](#bib.bib49)）来实现。此外，一些工作通过在嵌入模型上应用LLM内容窗口扩展方法（Zhu等，[2024a](#bib.bib58)；Peng和Quesnelle，[2023](#bib.bib38)），或使用状态空间编码器模型（Saad-Falcon等，[2024](#bib.bib44)）来扩展现有嵌入模型处理长上下文的能力。
- en: 3 LongRAG
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LongRAG
- en: 'Our proposed LongRAG framework is comprised of two components: the Long Retriever
    and the Long Reader. An illustrative example of these two components are depicted
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 LongRAG ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出的LongRAG框架由两个组件组成：长检索器和长读者。这两个组件的示例如图[2](#S3.F2 "图2 ‣ 3 LongRAG ‣ LongRAG:
    利用长上下文LLMs增强检索增强生成")所示。'
- en: '![Refer to caption](img/e248b3b0e37dbbcd29302827ee92bf74.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e248b3b0e37dbbcd29302827ee92bf74.png)'
- en: 'Figure 2: LongRAG example. On the left side, it shows that the long retrieval
    unit is grouped by Wikipedia documents through hyperlinks. Each retrieval unit
    contains an average of 4K tokens, corresponding to multiple related documents.
    On the right side, it shows a multi-hop question answer test case from HotpotQA.
    The final result can be achieved by using only a few retrieval units, which is
    then fed into a long reader.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LongRAG示例。左侧展示了通过超链接将长检索单元按维基百科文档分组。每个检索单元包含平均4K个标记，对应多个相关文档。右侧展示了来自HotpotQA的多跳问题回答测试案例。最终结果可以通过使用少量检索单元实现，然后输入到一个长读者中。
- en: 3.1 Long Retriever
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 长检索器
- en: The traditional RAG framework employs smaller retrieval units and prioritizes
    retrieving the exact fine-grained short context containing the answer. In contrast,
    our proposed LongRAG framework places greater emphasis on recall, aiming to retrieve
    relevant context with much coarse granularity. This design choice shifts more
    burden from the retriever to the reader to extract the exact answers from the
    relevant context.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的RAG框架使用较小的检索单元，优先检索包含答案的精细短文脉。相比之下，我们提出的LongRAG框架更加重视召回，旨在检索具有较粗粒度的相关上下文。这一设计选择将更多的负担从检索器转移到读者，以从相关上下文中提取准确答案。
- en: 'We denote our corpus for retrieval as $\mathcal{C}=\{d_{1},d_{2},\ldots,d_{D}\}$.
    In our framework, $\mathcal{C}_{\mathcal{F}}$ is then divided into three steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于检索的语料库表示为$\mathcal{C}=\{d_{1},d_{2},\ldots,d_{D}\}$。在我们的框架中，$\mathcal{C}_{\mathcal{F}}$分为三个步骤：
- en: Formulate long retrieval units
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 形成长检索单元
- en: 'A function is applied to the corpus to form $M$ could be as long as the whole
    document or even a group of documents, resulting in much longer retrieval units.
    We group the documents based on their relationships, using hyperlinks embedded
    within each document. The grouping algorithm is shown in Algorithm [1](#alg1 "Algorithm
    1 ‣ Formulate long retrieval units ‣ 3.1 Long Retriever ‣ 3 LongRAG ‣ LongRAG:
    Enhancing Retrieval-Augmented Generation with Long-context LLMs"). The output
    group is a list of documents that are related to each other. By having a longer
    retrieval unit, there are two advantages: First, it ensures the semantic integrity
    of each retrieval unit; Second, it provides much richer context for tasks that
    require information from multiple documents.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个函数应用于语料库形成$M$，这可以是整个文档甚至一组文档，从而导致更长的检索单元。我们根据文档之间的关系进行分组，使用嵌入在每个文档中的超链接。分组算法如算法
    [1](#alg1 "算法 1 ‣ 形成长检索单元 ‣ 3.1 长检索器 ‣ 3 长RAG ‣ LongRAG：利用长上下文LLM增强检索增强生成")所示。输出组是相互关联的文档列表。通过使用更长的检索单元，有两个优点：首先，它确保了每个检索单元的语义完整性；其次，它为需要从多个文档中获取信息的任务提供了更丰富的上下文。
- en: Algorithm 1 Group Documents Algorithm
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 文档分组算法
- en: 'Input: $S$ from low degree ($\deg(d)$ in $\mathcal{G}$              Remove
    $g$'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$S$从低度数（$\deg(d)$在$\mathcal{G}$）              移除$g$
- en: Similarity search
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相似性搜索
- en: 'We utilize an encoder, denoted as $E_{Q}(\cdot)$-dimensional vector. We define
    the similarity between the question and the retrieval unit using the dot product
    of their vectors:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用一个编码器，表示为$E_{Q}(\cdot)$维向量。我们通过它们向量的点积来定义问题与检索单元之间的相似性：
- en: '|  | $\displaystyle sim(q,g)=E_{Q}(q)^{T}E_{C}(g)$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle sim(q,g)=E_{Q}(q)^{T}E_{C}(g)$ |  |'
- en: In LongRAG settings, $E_{C}(g)$, so we resort to an approximation as below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在LongRAG设置中，$E_{C}(g)$，因此我们使用如下的近似。
- en: '|  | $\displaystyle sim(q,g)$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle sim(q,g)$ |  |'
- en: '|  |  | $\displaystyle\approx\max_{g^{\prime}\subseteq g}(E_{Q}(q)^{T}E_{C}(g^{\prime}))$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx\max_{g^{\prime}\subseteq g}(E_{Q}(q)^{T}E_{C}(g^{\prime}))$
    |  |'
- en: We approximate it by maximizing the scores of all chunks $g^{\prime}$ and predict
    the exact inner product search index in FAISS (Johnson et al., [2019](#bib.bib24)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过最大化所有块$g^{\prime}$的分数来进行近似，并预测FAISS中的确切内积搜索索引（Johnson等，[2019](#bib.bib24)）。
- en: Aggregate retrieval result
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聚合检索结果
- en: We will concatenate the top $k$ to 4 to 8.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将前$k$个连接到4到8。
- en: 3.2 Long Reader
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 长文档读取器
- en: The long reader operates straightforwardly. We feed the related instruction
    $i$ into an LLM, enabling it to reason over the long context and generate the
    final output. It’s important that the LLM used in the long reader can handle long
    contexts and does not exhibit excessive position bias. We select Gemini-1.5-Pro
    Reid et al. ([2024](#bib.bib43)) and GPT-4o OpenAI ([2024](#bib.bib37)) as our
    long reader given their strong ability to handle long context input.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 长文档读取器的操作很直接。我们将相关的指令$i$输入到LLM中，使其能够在长上下文中进行推理并生成最终输出。重要的是，长文档读取器中使用的LLM能够处理长上下文，并且没有过度的位置偏差。我们选择Gemini-1.5-Pro
    Reid等人（[2024](#bib.bib43)）和GPT-4o OpenAI（[2024](#bib.bib37)）作为我们的长文档读取器，因为它们在处理长上下文输入方面具有强大的能力。
- en: 'We utilize different approaches for short and long contexts. For short contexts,
    typically containing fewer than 1K tokens, we instruct the reader to directly
    extract the answer from the provided context retrieved from the corpus. For long
    contexts, typically longer than 4K tokens, we empirically find that using a similar
    prompt as for short contexts, where the model extracts the final answer directly
    from the long context, often leads to decreased performance. Instead, the most
    effective approach is to utilize the LLM as a chat model. Initially, it outputs
    a long answer, typically spanning a few words to a few sentences. Subsequently,
    we prompt it to generate a short answer by further extracting it from the long
    answer. The prompt is provided in the Appendix [6.1](#S6.SS1 "6.1 Prompts Template
    for Long Context Reader ‣ 6 Appendix ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对短上下文和长上下文使用不同的方法。对于通常包含少于 1K 标记的短上下文，我们指示读者直接从语料库中检索到的上下文中提取答案。对于通常长于 4K
    标记的长上下文，我们经验发现，使用类似于短上下文的提示，让模型直接从长上下文中提取最终答案，通常会导致性能下降。相反，最有效的方法是将 LLM 作为聊天模型使用。最初，它会输出一个长答案，通常跨越几个词到几句话。随后，我们提示它从长答案中进一步提取出短答案。提示详见附录
    [6.1](#S6.SS1 "6.1 长上下文阅读器的提示模板 ‣ 6 附录 ‣ LongRAG: 利用长上下文 LLMs 提升检索增强生成")。'
- en: 4 Experiments
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we will first detail the dataset we adopt, and then demonstrate
    the retriever performance. Finally, we will show the end question-answering performance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先详细说明我们采用的数据集，然后展示检索器的性能。最后，我们将展示最终的问答性能。
- en: '| Retrieval Unit | Corpus Size | Num of Retrieval Units | Average Num of Tokens
    | Answer Recall (AR) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 检索单元 | 语料库大小 | 检索单元数量 | 平均标记数 | 答案召回率 (AR) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Corpus | Test Set |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 语料库 | 测试集 |'
- en: '| --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Passage | 22M | 1 | 120 | 130 | 52.24 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 片段 | 22M | 1 | 120 | 130 | 52.24 |'
- en: '| 100 | 12K | 14K | 89.92 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 12K | 14K | 89.92 |'
- en: '| 200 | 24K | 28K | 91.30 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 24K | 28K | 91.30 |'
- en: '| Document | 3M | 1 | 820 | 4K | 69.45 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 文档 | 3M | 1 | 820 | 4K | 69.45 |'
- en: '| 5 | 4K | 18K | 85.37 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4K | 18K | 85.37 |'
- en: '| 10 | 8K | 34K | 88.12 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 8K | 34K | 88.12 |'
- en: '| Grouped Documents | 600K | 1 | 4K | 6K | 71.69 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 分组文档 | 600K | 1 | 4K | 6K | 71.69 |'
- en: '| 4 | 16K | 25K | 86.30 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16K | 25K | 86.30 |'
- en: '| 8 | 32K | 50K | 88.53 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 32K | 50K | 88.53 |'
- en: 'Table 1: The table illustrates the retrieval performance on NQ. Employing a
    long-context retriever (with an average number of tokens for each retrieval unit
    up to 6K) compresses the corpus size by up to 30 times (from 22M to 600K), enhancing
    top-1 answer recall by approximately 20 points (from 52.24 to 71.69). Furthermore,
    long-context retrieval requires significantly fewer retrieval units (10 times
    fewer) to achieve comparable results. Therefore, integrating long-context retrieval
    significantly alleviates the burden on the retriever model.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 表格展示了 NQ 上的检索性能。使用长上下文检索器（每个检索单元的平均标记数高达 6K）将语料库大小压缩了多达 30 倍（从 22M 到 600K），并将
    top-1 答案召回率提高了大约 20 分（从 52.24 到 71.69）。此外，长上下文检索所需的检索单元显著减少（减少了 10 倍），从而获得类似的结果。因此，集成长上下文检索显著减轻了检索模型的负担。'
- en: 4.1 Data
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据
- en: 'Our proposed methods are tested on two Wikipedia-related question answering
    datasets: Natural Questions and HotpotQA.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方法在两个与维基百科相关的问答数据集上进行了测试：自然问题和 HotpotQA。
- en: Natural Question
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自然问题
- en: (Kwiatkowski et al., [2019](#bib.bib29)) was designed for end-to-end question
    answering. The questions were mined from real Google search queries and the answers
    were spans in Wikipedia articles identified by annotators. This dataset contains
    3,610 questions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (Kwiatkowski et al., [2019](#bib.bib29)) 旨在用于端到端问答。问题从真实的 Google 搜索查询中挖掘，答案是由标注者在维基百科文章中识别出的片段。该数据集包含
    3,610 个问题。
- en: HotpotQA
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HotpotQA
- en: '(Yang et al., [2018](#bib.bib53)) consists of two-hop questions over diverse
    topics. We focus on the fullwiki setting in which two Wikipedia passages are required
    to answer the questions. Since the gold passages for the test set are not available,
    we follow prior work (Xiong et al., [2020b](#bib.bib52)) and evaluate on the development
    set, which has 7,405 questions. There are two main question types in HotpotQA:
    (1) comparison questions usually require contrasting two entities and (2) bridge
    questions can be answered by following a connecting entity that links one document
    to another.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (Yang et al., [2018](#bib.bib53)) 包含了两个跳跃的问题，涉及多种话题。我们关注于全维基百科设置，其中需要两个维基百科段落来回答问题。由于测试集的金标准段落不可用，我们参考了之前的工作
    (Xiong et al., [2020b](#bib.bib52))，在开发集上进行评估，该集包含7,405个问题。HotpotQA中主要有两种问题类型：（1）比较问题通常需要对比两个实体，（2）桥接问题可以通过跟随连接实体来回答，连接一个文档到另一个文档。
- en: Wikipedia (Knowledge Source)
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 维基百科（知识来源）
- en: We use different versions of English Wikipedia for different datasets following
    previous works Lewis et al. ([2020](#bib.bib30)); Yang et al. ([2018](#bib.bib53)).
    For NQ, we use the Wikipedia dumps from December 20, 2018, which contain approximately
    3 million documents and 22 million passages. For HotpotQA, we use the abstract
    paragraphs from the October 1, 2017 dump, which contain around 5 million documents.
    For each page, only the plain text is extracted and all structured data sections
    such as lists, tables and figures are stripped from the document.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同版本的英语维基百科来处理不同的数据集，参考了之前的工作 Lewis et al. ([2020](#bib.bib30))；Yang et
    al. ([2018](#bib.bib53))。对于NQ，我们使用了2018年12月20日的维基百科转储，包含大约300万份文档和2200万段落。对于HotpotQA，我们使用了2017年10月1日转储的摘要段落，包含约500万份文档。对于每个页面，只提取纯文本，所有结构化数据部分，如列表、表格和图形都从文档中删除。
- en: 4.2 Retrieval Performance
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 检索性能
- en: Metrics
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标
- en: Retrieval performance is measured using Answer Recall (AR) and Recall (R). For
    NQ, we use only answer recall, while for HotpotQA, we use both metrics. Answer
    Recall is the recall of the answer string in all the retrieved documents that
    we plan to use in the reader. For example, if the retrieval unit is at the “passage”
    level and the number of retrieval units is 100, answer recall measures whether
    the answer string is present in these 100 passages. For HotpotQA, we compute AR
    only for questions with span answers, specifically the “bridge” type questions,
    while ignoring yes/no and comparison questions, following previous work (Khalifa
    et al., [2022](#bib.bib26)). Recall used for HotpotQA measures whether the two
    gold documents are present in all the retrieved results. For example, if the retrieval
    unit is at the “document” level and the number of retrieval units is 10, recall
    measures whether both gold documents are present among the 10 retrieved documents.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 检索性能通过答案召回率 (AR) 和召回率 (R) 来衡量。对于NQ，我们仅使用答案召回率，而对于HotpotQA，我们使用这两个指标。答案召回率是指在所有检索到的文档中我们计划在阅读器中使用的答案字符串的召回率。例如，如果检索单元是“段落”级别，检索单元数量为100，答案召回率测量答案字符串是否出现在这100个段落中。对于HotpotQA，我们仅计算跨度答案的问题的AR，特别是“桥接”类型的问题，同时忽略是/否和比较问题，参考了之前的工作
    (Khalifa et al., [2022](#bib.bib26))。HotpotQA使用的召回率衡量两个金标准文档是否出现在所有检索结果中。例如，如果检索单元是“文档”级别，检索单元数量为10，召回率测量两个金标准文档是否存在于这10个检索到的文档中。
- en: Experiment Setup
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置
- en: We leverage open-sourced dense retrieval toolkit, Tevatron Gao et al. ([2022](#bib.bib16)),
    for all our retrieval experiments. The base embedding model we used is bge-large-en-v1.5,
    a general-purpose embeddings model that isn’t specifically trained on our test
    data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用开源的密集检索工具包 Tevatron Gao et al. ([2022](#bib.bib16)) 进行所有检索实验。我们使用的基础嵌入模型是
    bge-large-en-v1.5，一个通用的嵌入模型，并未专门针对我们的测试数据进行训练。
- en: '| Retrieval Unit | Corpus Size | Num of Retrieval Units | Average Num of Tokens
    | Recall (R) | Answer Recall (AR) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 检索单元 | 语料库大小 | 检索单元数量 | 平均标记数 | 召回率 (R) | 答案召回率 (AR) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Corpus | Test Set |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 语料库 | 测试集 |'
- en: '| --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Document | 5.2M | 2 | 130 | 200 | 30.01 | 47.75 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 文档 | 5.2M | 2 | 130 | 200 | 30.01 | 47.75 |'
- en: '| 100 | 6.5K | 10K | 74.84 | 84.67 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 6.5K | 10K | 74.84 | 84.67 |'
- en: '| 200 | 13K | 20K | 79.68 | 88.34 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 13K | 20K | 79.68 | 88.34 |'
- en: '| Grouped Documents | 500K | 2 | 1K | 8K | 56.30 | 72.49 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 分组文档 | 500K | 2 | 1K | 8K | 56.30 | 72.49 |'
- en: '| 8 | 4K | 29K | 74.71 | 84.40 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 4K | 29K | 74.71 | 84.40 |'
- en: 'Table 2: The table illustrates the retrieval performance on HotpotQA. Similar
    to the findings on NQ, a long-context retrieval could significantly alleviate
    the burden on the retriever component within the entire RAG framework.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：该表展示了 HotpotQA 上的检索性能。与 NQ 上的发现类似，长上下文检索可以显著减轻整个 RAG 框架中检索器组件的负担。
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs") and Table [2](#S4.T2 "Table 2 ‣ Experiment
    Setup ‣ 4.2 Retrieval Performance ‣ 4 Experiments ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs") have shown the retrieval results on NQ and
    HotpotQA. In the NQ dataset, we utilize three different retrieval units, ranging
    from shorter to longer: passage, document, and grouped documents. In the table,
    we have mentioned two kinds of average number of tokens in each retrieval unit:
    one for the entire corpus and one for each test set. The retrieval units for each
    test case can sometimes be much longer than the average size across the whole
    corpus, as the corpus might include some Wikipedia pages with very few words,
    while the test cases may focus more on longer documents. Generally, our long-context
    retriever (at the document level and grouped document level) uses retrieval units
    containing an average of 6K tokens. By using longer retrieval units, there are
    several advantages: 1) It will significantly alleviate the burden on the retriever
    by compressing the corpus size by approximately 30 times, from 22M to 600K. The
    top-1 answer recall improves by about 20 points, from 52.24 to 71.69\. We could
    use significantly fewer retrieval units to achieve comparable retrieval performance.
    For instance, 8 retrieval units at the grouped document level can achieve similar
    recall as 100 retrieval units at the passage level. 2) It could provide more comprehensive
    information to the reader. In the original passage-level RAG setup, information
    might be incomplete due to the chunking operation. In the HotpotQA dataset, we
    observe similar results. One notable difference is that in HotpotQA, the retrieval
    units are only at the document level and grouped document level, as HotpotQA uses
    only abstract paragraphs from each Wikipedia page.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S4.T1 "表格 1 ‣ 4 实验 ‣ LongRAG：利用长上下文 LLMs 提升检索增强生成") 和表格 [2](#S4.T2 "表格
    2 ‣ 实验设置 ‣ 4.2 检索性能 ‣ 4 实验 ‣ LongRAG：利用长上下文 LLMs 提升检索增强生成") 显示了 NQ 和 HotpotQA
    上的检索结果。在 NQ 数据集中，我们使用了三种不同的检索单元，从较短到较长：段落、文档和分组文档。在表格中，我们提到了每个检索单元的两种平均 token
    数量：一种是整个语料库的，一种是每个测试集的。每个测试用例的检索单元有时可能比整个语料库的平均大小要长得多，因为语料库可能包含一些只有很少字数的维基百科页面，而测试用例可能更多地关注较长的文档。通常，我们的长上下文检索器（在文档级别和分组文档级别）使用的检索单元包含平均
    6K 个 token。使用更长的检索单元有几个优点：1) 它将通过将语料库大小压缩约 30 倍，从 22M 减少到 600K，从而显著减轻检索器的负担。top-1
    答案召回率提高了大约 20 个百分点，从 52.24 提升到 71.69。我们可以使用显著较少的检索单元来实现类似的检索性能。例如，在分组文档级别使用 8
    个检索单元可以获得与在段落级别使用 100 个检索单元相似的召回率。2) 它可以向读者提供更全面的信息。在原始段落级 RAG 设置中，由于分块操作，信息可能不完整。在
    HotpotQA 数据集中，我们观察到类似的结果。一个显著的区别是，在 HotpotQA 中，检索单元仅在文档级别和分组文档级别，因为 HotpotQA 仅使用每个维基百科页面的摘要段落。
- en: '| Model | Granularity | AR@1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 颗粒度 | AR@1 |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BGE-Large | 512-tokens chunk | 71.7% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| BGE-Large | 512-token 块 | 71.7% |'
- en: '| E5-Mistral-7B | 4000-tokens chunk | 54.2% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| E5-Mistral-7B | 4000-token 块 | 54.2% |'
- en: '| E5-Mistral-7B | entire grouped retrieval unit | 23.4% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| E5-Mistral-7B | 整个分组检索单元 | 23.4% |'
- en: 'Table 3: Different methods to encode the long retrieval unit in the long retriever.
    Using a general embedding model and approximating by maximizing the similarity
    scores between the query and all chunks within the retrieval unit is better than
    using the long embedding model to encode the entire context.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：在长检索器中编码长检索单元的不同方法。使用通用嵌入模型，并通过最大化查询与检索单元内所有块之间的相似度评分来进行近似，效果优于使用长嵌入模型对整个上下文进行编码。
- en: Encode the long retrieval unit
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编码长检索单元
- en: 'As discussed in Section [3.2](#S3.SS2 "3.2 Long Reader ‣ 3 LongRAG ‣ LongRAG:
    Enhancing Retrieval-Augmented Generation with Long-context LLMs"), it’s very challenging
    to employ an encoder, $E_{C}(\cdot)$ directly, where $g$ has an average size of
    6K tokens. We can notice from the table that our approximation by taking the maximum
    score between the query and each text piece from the long context produces much
    better results than encoding them directly using the long embedding model. We
    believe future improvements in the research direction of long embedding models
    will further enhance our framework to reduce memory consumption.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[3.2](#S3.SS2 "3.2 Long Reader ‣ 3 LongRAG ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs")节所讨论的，直接使用编码器$E_{C}(\cdot)$是非常具有挑战性的，其中$g$的平均大小为6K
    tokens。从表中可以看出，通过取查询与长上下文中每个文本片段之间的最大得分来进行的近似方法，比直接使用长嵌入模型进行编码的结果要好得多。我们相信，未来在长嵌入模型研究方向上的改进将进一步提升我们的框架，以减少内存消耗。'
- en: '| Method | EM |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Method | EM |'
- en: '| --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Closed-Book |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Closed-Book |'
- en: '| --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| GPT-4-Turbo Achiam et al. ([2023](#bib.bib1)) | 41.2 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo Achiam et al. ([2023](#bib.bib1)) | 41.2 |'
- en: '| Gemini-1.5-Pro Reid et al. ([2024](#bib.bib43)) | 47.8 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-Pro Reid et al. ([2024](#bib.bib43)) | 47.8 |'
- en: '| Claude-3-Opus Anthropic ([2024](#bib.bib3)) | 49.2 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Opus Anthropic ([2024](#bib.bib3)) | 49.2 |'
- en: '| Fully-supervised RAG |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Fully-supervised RAG |'
- en: '| REALM Guu et al. ([2020](#bib.bib18)) | 40.4 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| REALM Guu et al. ([2020](#bib.bib18)) | 40.4 |'
- en: '| DPR Karpukhin et al. ([2020](#bib.bib25)) | 41.5 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| DPR Karpukhin et al. ([2020](#bib.bib25)) | 41.5 |'
- en: '| RAG Lewis et al. ([2020](#bib.bib30)) | 44.5 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| RAG Lewis et al. ([2020](#bib.bib30)) | 44.5 |'
- en: '| RETRO Borgeaud et al. ([2022](#bib.bib5)) | 45.5 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| RETRO Borgeaud et al. ([2022](#bib.bib5)) | 45.5 |'
- en: '| RePAQ Lewis et al. ([2021](#bib.bib31)) | 47.8 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| RePAQ Lewis et al. ([2021](#bib.bib31)) | 47.8 |'
- en: '| Fusion-in-Decoder (Izacard and Grave, [2020b](#bib.bib21)) | 51.4 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Fusion-in-Decoder (Izacard and Grave, [2020b](#bib.bib21)) | 51.4 |'
- en: '| EMDR² Singh et al. ([2021](#bib.bib46)) | 52.5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| EMDR² Singh et al. ([2021](#bib.bib46)) | 52.5 |'
- en: '| Atlas (Izacard et al., [2022](#bib.bib22)) | 64.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Atlas (Izacard et al., [2022](#bib.bib22)) | 64.0 |'
- en: '| No Fine-tuning RAG |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| No Fine-tuning RAG |'
- en: '| REPLUG (Shi et al., [2023](#bib.bib45)) | 45.5 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| REPLUG (Shi et al., [2023](#bib.bib45)) | 45.5 |'
- en: '| LongRAG (Gemini-1.5-Pro; Recall 4 units) | 58.6 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LongRAG (Gemini-1.5-Pro; Recall 4 units) | 58.6 |'
- en: '| LongRAG (GPT-4o; Recall 4 units) | 62.7 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| LongRAG (GPT-4o; Recall 4 units) | 62.7 |'
- en: 'Table 4: The table shows the QA results on the NQ dataset. We compare the results
    with three groups of baselines: closed-book, which involves directly prompting
    state-of-the-art LLMs with 16-shot in-context examples; fully-supervised RAG,
    where the RAG framework is used and the model is fully supervised and trained
    on the training data; and No Fine-tuning RAG, which employs the RAG framework
    without any tuning.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：该表展示了NQ数据集上的QA结果。我们将结果与三组基线进行比较：闭卷基线，即直接向最先进的LLMs提供16-shot的上下文示例；完全监督的RAG，其中使用RAG框架并在训练数据上进行完全监督和训练；以及No
    Fine-tuning RAG，它使用RAG框架但没有进行任何调整。
- en: '| Method | EM |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Method | EM |'
- en: '| --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Closed-Book |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Closed-Book |'
- en: '| --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Claude-3-Opus Anthropic ([2024](#bib.bib3)) | 32.8 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Opus Anthropic ([2024](#bib.bib3)) | 32.8 |'
- en: '| Gemini-1.5-Pro Reid et al. ([2024](#bib.bib43)) | 33.9 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-Pro Reid et al. ([2024](#bib.bib43)) | 33.9 |'
- en: '| GPT-4-Turbo Achiam et al. ([2023](#bib.bib1)) | 42.4 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo Achiam et al. ([2023](#bib.bib1)) | 42.4 |'
- en: '| Fully-supervised RAG |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Fully-supervised RAG |'
- en: '| CogQA Ding et al. ([2019](#bib.bib14)) | 37.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| CogQA Ding et al. ([2019](#bib.bib14)) | 37.1 |'
- en: '| DrKIT Dhingra et al. ([2020](#bib.bib13)) | 42.1 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| DrKIT Dhingra et al. ([2020](#bib.bib13)) | 42.1 |'
- en: '| Transformer-XH Zhao et al. ([2019](#bib.bib57)) | 51.6 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Transformer-XH Zhao et al. ([2019](#bib.bib57)) | 51.6 |'
- en: '| QAMAT+ Chen et al. ([2023b](#bib.bib9)) | 57.6 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| QAMAT+ Chen et al. ([2023b](#bib.bib9)) | 57.6 |'
- en: '| HGN Fang et al. ([2019](#bib.bib15)) | 59.7 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| HGN Fang et al. ([2019](#bib.bib15)) | 59.7 |'
- en: '| PathRetriever Asai et al. ([2019](#bib.bib4)) | 60.0 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| PathRetriever Asai et al. ([2019](#bib.bib4)) | 60.0 |'
- en: '| HopRetrieve Li et al. ([2021](#bib.bib32)) | 62.1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| HopRetrieve Li et al. ([2021](#bib.bib32)) | 62.1 |'
- en: '| MDR Xiong et al. ([2020b](#bib.bib52)) | 62.3 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| MDR Xiong et al. ([2020b](#bib.bib52)) | 62.3 |'
- en: '| HopRetrieve-plus Li et al. ([2021](#bib.bib32)) | 66.5 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| HopRetrieve-plus Li et al. ([2021](#bib.bib32)) | 66.5 |'
- en: '| AISO Zhu et al. ([2021](#bib.bib60)) | 68.1 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| AISO Zhu et al. ([2021](#bib.bib60)) | 68.1 |'
- en: '| COS Ma et al. ([2023](#bib.bib34)) | 68.2 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| COS Ma et al. ([2023](#bib.bib34)) | 68.2 |'
- en: '| No Fine-tuning RAG |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| No Fine-tuning RAG |'
- en: '| DSP Khattab et al. ([2022](#bib.bib28)) | 51.4 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| DSP Khattab et al. ([2022](#bib.bib28)) | 51.4 |'
- en: '| PromptRank Khalifa et al. ([2023](#bib.bib27)) | 55.7 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| PromptRank Khalifa et al. ([2023](#bib.bib27)) | 55.7 |'
- en: '| LongRAG (Gemini-1.5-Pro; Recall 8 units) | 57.5 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LongRAG (Gemini-1.5-Pro; Recall 8 units) | 57.5 |'
- en: '| LongRAG (GPT-4o; Recall 8 units) | 64.3 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LongRAG (GPT-4o; Recall 8 units) | 64.3 |'
- en: 'Table 5: The table shows the QA results on the Hotpot-QA dev set. We compare
    the results with three groups of baselines: closed-book, which involves directly
    prompting state-of-the-art LLMs with 16-shot in-context examples; fully-supervised
    RAG, where the RAG framework is used and the model is fully supervised and trained
    on the training data; and No Fine-tuning RAG, which employs the RAG framework
    without any tuning.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5：该表显示了 Hotpot-QA 开发集上的 QA 结果。我们将结果与三组基线进行比较：closed-book，涉及直接提示最先进的 LLM，并使用
    16-shot 上下文示例；fully-supervised RAG，其中使用 RAG 框架，并且模型在训练数据上进行了完全监督和训练；以及 No Fine-tuning
    RAG，它使用 RAG 框架但没有任何调优。
- en: 4.3 Full QA Performance
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 完整 QA 性能
- en: 'We leverage Gemini-1.5-Pro and GPT-4o as the reader in our LongRAG framework.
    The prompt we use for our experiments are in Table [6](#Sx1.T6 "Table 6 ‣ LongRAG:
    Enhancing Retrieval-Augmented Generation with Long-context LLMs"). We also refine
    the standard exact match rate definition to more fairly evaluate LongRAG’s performance.
    More details can be found in Section [6.2](#S6.SS2 "6.2 Refined Metric ‣ 6 Appendix
    ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs").'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 LongRAG 框架中使用 Gemini-1.5-Pro 和 GPT-4o 作为阅读器。我们在实验中使用的提示在表格 [6](#Sx1.T6
    "表格 6 ‣ LongRAG: 利用长上下文 LLM 增强检索增强生成") 中列出。我们还将标准的精确匹配率定义进行了改进，以更公平地评估 LongRAG
    的表现。更多细节可以在第 [6.2](#S6.SS2 "6.2 改进指标 ‣ 6 附录 ‣ LongRAG: 利用长上下文 LLM 增强检索增强生成") 节中找到。'
- en: 'We compare our model with several groups of strong previous models as baselines.
    The first group is “Closed-Book”: These baselines mean that no retrieval component
    is used; instead, state-of-the-art LLMs are employed to directly obtain the final
    result. We evaluate our results on Gemini-1.5-pro Reid et al. ([2024](#bib.bib43)),
    Claude-3-Opus Anthropic ([2024](#bib.bib3)) and GPT-4-Turbo Achiam et al. ([2023](#bib.bib1)).
    All models are evaluated on 16-shot in-context learning with direct prompting;
    The second group is “Fully-supervised RAG”, and these baselines involve full-supervised
    fine-tuning on the training dataset. The third group is “No Fine-tuning RAG”,
    and these baselines doesn’t involve any supervised fine-tuning on the training
    dataset.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的模型与几组强大的先前模型作为基线进行比较。第一组是“Closed-Book”：这些基线意味着不使用检索组件；相反，使用最先进的 LLM 直接获得最终结果。我们在
    Gemini-1.5-pro Reid et al. ([2024](#bib.bib43))、Claude-3-Opus Anthropic ([2024](#bib.bib3))
    和 GPT-4-Turbo Achiam et al. ([2023](#bib.bib1)) 上评估了我们的结果。所有模型都在 16-shot 上下文学习中进行直接提示评估；第二组是“Fully-supervised
    RAG”，这些基线涉及在训练数据集上进行完全监督的微调。第三组是“No Fine-tuning RAG”，这些基线不涉及对训练数据集的任何监督微调。
- en: 'The QA results on NQ are presented in Table [4](#S4.T4 "Table 4 ‣ Encode the
    long retrieval unit ‣ 4.2 Retrieval Performance ‣ 4 Experiments ‣ LongRAG: Enhancing
    Retrieval-Augmented Generation with Long-context LLMs"), and the QA results on
    HotpotQA are presented in Table [5](#S4.T5 "Table 5 ‣ Encode the long retrieval
    unit ‣ 4.2 Retrieval Performance ‣ 4 Experiments ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs"). On the NQ dataset, LongRAG achieves a 62.7
    exact match rate, which is on par of the strongest fine-tuned RAG model like Atlas.
    On the HotpotQA dataset, LongRAG achieves a 64.3 exact match rate, which is also
    close to the SoTA fully-supervised RAG frameworks.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 'NQ 上的 QA 结果在表格 [4](#S4.T4 "表格 4 ‣ 编码长检索单元 ‣ 4.2 检索性能 ‣ 4 实验 ‣ LongRAG: 利用长上下文
    LLM 增强检索增强生成") 中呈现，HotpotQA 上的 QA 结果在表格 [5](#S4.T5 "表格 5 ‣ 编码长检索单元 ‣ 4.2 检索性能
    ‣ 4 实验 ‣ LongRAG: 利用长上下文 LLM 增强检索增强生成") 中呈现。在 NQ 数据集中，LongRAG 实现了 62.7 的精确匹配率，与最强的微调
    RAG 模型 Atlas 相当。在 HotpotQA 数据集中，LongRAG 实现了 64.3 的精确匹配率，也接近 SoTA 完全监督的 RAG 框架。'
- en: '![Refer to caption](img/5ad7f77f8b95d1448fe45a87de44d935.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ad7f77f8b95d1448fe45a87de44d935.png)'
- en: 'Figure 3: This figure compares different settings of LongRAG on the NQ dataset.
    This table leverages 200 test cases from the test set to help compare different
    retrieval unit selections and optimal number of retrieval units fed into the reader
    (Gemini-based).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：该图比较了 LongRAG 在 NQ 数据集上的不同设置。此表格利用了来自测试集的 200 个测试案例，以帮助比较不同的检索单元选择和输入阅读器的最优检索单元数量（基于
    Gemini）。
- en: '![Refer to caption](img/f6300b91df5fcf19c650bc0c421a4309.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f6300b91df5fcf19c650bc0c421a4309.png)'
- en: 'Figure 4: This table compares different settings of LongRAG on the HotpotQA
    dataset. This table leverages 200 test cases from the test set to help compare
    different retrieval unit selections and the optimal number of retrieval units
    fed into the reader (Gemini-based).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：该表比较了LongRAG在HotpotQA数据集上的不同设置。该表利用了来自测试集的200个测试用例，以帮助比较不同的检索单元选择和输入读取器中的最优检索单元数量（基于Gemini）。
- en: '![Refer to caption](img/3085c0167a6f382b1bb1c14a22558efa.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3085c0167a6f382b1bb1c14a22558efa.png)'
- en: 'Figure 5: This table shows the end performance of different granularity of
    retrieval units. The green curve indicates the ‘large unit’ adopted in LongRAG.
    The number above the curve indicates the recall. It can be seen that end-performance
    does not increase monotonically with recall score.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：该表显示了不同粒度的检索单元的最终性能。绿色曲线表示LongRAG中采用的“较大单元”。曲线上的数字表示召回率。可以看出，最终性能并未随着召回率得分单调增加。
- en: 4.4 Ablation Studies
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: We perform several in-depth ablation to understand what are the important factors
    in our LongRAG system including "unit size" and "reader variant".
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一些深入的消融研究，以了解我们LongRAG系统中的重要因素，包括“单元大小”和“读取器变体”。
- en: Retrieval Unit Selection
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检索单元选择
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Full QA Performance ‣ 4 Experiments ‣ LongRAG:
    Enhancing Retrieval-Augmented Generation with Long-context LLMs") and Figure [4](#S4.F4
    "Figure 4 ‣ 4.3 Full QA Performance ‣ 4 Experiments ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs") compare different settings of LongRAG. This
    table leverages 200 random test cases from the test set to help compare different
    retrieval unit granularity selection and the optimal number of retrieval units
    used in the reader. On the NQ dataset, we have two observations: First, regardless
    of which retrieval unit is selected, there will be a turning point where feeding
    more retrieval units into the reader becomes detrimental. This is due to the excessive
    burden placed on the reader, preventing it from effectively understanding and
    extracting relevant information from the long context. For passage-level retrieval
    units, the turning point is between 100 and 200; for document-level retrieval
    units, the turning point is between 5 and 10; and for grouped documents level,
    the turning point is between 4 and 8\. In general, the most suitable context length
    fed into the reader is around 30K tokens. Second, the semantic integrity is important
    when comparing the performance of passage-level retrieval units with document
    or grouped documents level retrieval units, highlighting the advantage of using
    longer and more complete retrieval units.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S4.F3 "图 3 ‣ 4.3 完整QA性能 ‣ 4 实验 ‣ LongRAG：利用长上下文LLM增强检索增强生成")和图[4](#S4.F4
    "图 4 ‣ 4.3 完整QA性能 ‣ 4 实验 ‣ LongRAG：利用长上下文LLM增强检索增强生成")比较了LongRAG的不同设置。该表利用了来自测试集的200个随机测试用例，以帮助比较不同的检索单元粒度选择和在读取器中使用的最优检索单元数量。在NQ数据集上，我们有两个观察结果：首先，无论选择哪个检索单元，都会有一个拐点，即将更多检索单元输入读取器会变得有害。这是因为读取器的负担过重，无法有效理解和提取长上下文中的相关信息。对于段落级检索单元，拐点在100到200之间；对于文档级检索单元，拐点在5到10之间；而对于分组文档级，拐点在4到8之间。一般来说，输入读取器的最合适上下文长度大约为30K
    tokens。第二，在比较段落级检索单元与文档或分组文档级检索单元的性能时，语义完整性非常重要，这突显了使用更长、更完整的检索单元的优势。
- en: Recall vs. EM
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 召回率与EM
- en: 'In Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Full QA Performance ‣ 4 Experiments ‣
    LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"), we
    compare the relationship between retrieval recall and end performance across varying
    context lengths for different retrieval unit selections. We observe that using
    fewer retrieval units in the reader with longer retrieval units design reduces
    the introduction of distractors or hard negatives under a given length budget.
    Consequently, the end performance does not increase monotonically with the recall
    score. In the future, with advancements in long embedding models and improved
    retrieval recall for long retrieval units, we can expect better end performance.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](#S4.F5 "图 5 ‣ 4.3 完整QA性能 ‣ 4 实验 ‣ LongRAG：利用长上下文LLM增强检索增强生成")中，我们比较了不同检索单元选择下检索召回率与最终性能之间的关系，涵盖了不同上下文长度。我们观察到，在较长检索单元设计中使用较少的检索单元会减少在给定长度预算下引入干扰项或困难负例。因此，最终性能不会随着召回率得分单调增加。未来，随着长嵌入模型的进步和长检索单元的改进，我们可以期待更好的最终性能。
- en: '![Refer to caption](img/784c6af23c6e6df2ad56b88a227973c8.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/784c6af23c6e6df2ad56b88a227973c8.png)'
- en: 'Figure 6: This figure compares different readers of LongRAG on the NQ dataset.
    This table leverages 200 test cases from the test set to help compare performance
    using different readers.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：此图比较了在 NQ 数据集上不同 LongRAG 阅读器的表现。该表利用了来自测试集的 200 个测试案例，以帮助比较不同阅读器的性能。
- en: Reader Model
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阅读器模型
- en: 'In Figure [6](#S4.F6 "Figure 6 ‣ Recall vs. EM ‣ 4.4 Ablation Studies ‣ 4 Experiments
    ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"),
    we compare the performance of six different readers: Gemini-1.5-pro, GPT-4-Turbo,
    GPT-4o, Claude-3-Opus, Claude-3.5-Sonnet and DeepSeek-V2-Chat. The results indicate
    that GPT-4o achieves the highest exact match score on the 200 test questions of
    the NQ dataset among the three models. This suggests that GPT-4o is the most effective
    in the role of a long reader in the LongRAG framework. The enhanced performance
    of GPT-4o can be attributed to its superior ability to process and comprehend
    lengthy contexts, ensuring that crucial information is accurately extracted. Therefore,
    we mainly report the GPT-4o results in our main table. Besides, Gemini-1.5-pro,
    GPT-4-Turbo, Claude-3-Opus, and Claude-3.5-Sonnet could achieve very similar results.
    These state-of-the-art black box LLMs are also effective readers within the LongRAG
    framework. Deepseek-V2-Chat is one of the best open-source LLMs, but its performance
    degrades significantly compared to the previous five black-box LLMs. The above
    experiments demonstrate that our current framework depends on the long-context
    understanding ability of LLMs, and we still have a long way to go in harnessing
    open-source LLMs within our framework.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [6](#S4.F6 "Figure 6 ‣ Recall vs. EM ‣ 4.4 Ablation Studies ‣ 4 Experiments
    ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs") 中，我们比较了六种不同阅读器的性能：Gemini-1.5-pro、GPT-4-Turbo、GPT-4o、Claude-3-Opus、Claude-3.5-Sonnet
    和 DeepSeek-V2-Chat。结果表明，GPT-4o 在 NQ 数据集的 200 个测试问题上取得了最高的精确匹配分数。这表明 GPT-4o 在 LongRAG
    框架中的长阅读器角色中最为有效。GPT-4o 的增强性能归因于其处理和理解长上下文的卓越能力，确保了关键信息的准确提取。因此，我们主要在主表中报告 GPT-4o
    的结果。此外，Gemini-1.5-pro、GPT-4-Turbo、Claude-3-Opus 和 Claude-3.5-Sonnet 的结果也非常相似。这些最先进的黑箱
    LLMs 在 LongRAG 框架中也是有效的阅读器。Deepseek-V2-Chat 是最优秀的开源 LLM 之一，但其性能与之前的五种黑箱 LLMs 相比显著下降。上述实验表明，我们当前的框架依赖于
    LLMs 的长上下文理解能力，我们在利用开源 LLMs 的框架中仍有很长的路要走。'
- en: 5 Conclusion
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a new framework, LongRAG, to alleviate the imbalance
    between the burden of the retriever. The LongRAG framework consists of a “long
    retriever” and a “long reader” component on top of the 4K-token retrieval units.
    Our proposed framework can significantly reduce the corpus size by 10 to 30 times,
    which greatly improves the recall of the retriever. On the other hand, the long
    retrieval unit preserves the semantic integrity of each document. We test our
    framework on end-to-end question answering tasks and demonstrate its superior
    performance without any training. We believe LongRAG can pave the road for the
    modern RAG system design.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新框架 LongRAG，以缓解检索器负担不均的问题。LongRAG 框架由一个“长检索器”和一个“长阅读器”组件组成，基于 4K-token
    检索单元。我们提出的框架可以显著将语料库的大小缩减 10 到 30 倍，从而大大提高检索器的召回率。另一方面，长检索单元保持了每个文档的语义完整性。我们在端到端问答任务中测试了我们的框架，并在没有任何训练的情况下展示了其卓越的性能。我们相信
    LongRAG 可以为现代 RAG 系统设计铺平道路。
- en: Limitation
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: There are three major limitations of our proposed framework. First, it relies
    on the long embedding model. Although recent studies have made progress in this
    direction, there is still a need for stronger long embedding models. In our work,
    we use an approximation to calculate the semantic score with a regular embedding
    model, which proves more effective than using a long embedding model. Future improvements
    in long embedding models could help us further enhance the performance of our
    system and reduce the storage size of corpus embeddings if the entire long context
    could be encoded directly. The second limitation is that we only use a black-box
    LLM as the reader. A reader that supports long input and is less affected by position
    bias is necessary. Currently, most open-source LLMs do not meet these requirements.
    The third limitation is that our grouping methods are based on hyperlinks, which
    are specific to the Wikipedia corpus. A more general grouping method should be
    considered.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的框架有三个主要限制。首先，它依赖于长嵌入模型。尽管最近的研究在这方面取得了一些进展，但仍需要更强的长嵌入模型。在我们的工作中，我们使用了一个近似方法，通过常规嵌入模型计算语义分数，这证明比使用长嵌入模型更有效。未来对长嵌入模型的改进可以帮助我们进一步提升系统的性能，并减少语料库嵌入的存储大小，如果整个长上下文可以直接编码的话。第二个限制是我们仅使用了一个黑箱
    LLM 作为阅读器。需要一个支持长输入且受位置偏差影响较小的阅读器。目前，大多数开源 LLM 不满足这些要求。第三个限制是我们的分组方法基于超链接，这在维基百科语料库中是特定的。应考虑更通用的分组方法。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人（2023）Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge Akkaya、Florencia
    Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal Anadkat 等人。2023年。Gpt-4
    技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: An et al. (2024) Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu,
    Chang Zhou, and Lingpeng Kong. 2024. Training-free long-context scaling of large
    language models. *arXiv preprint arXiv:2402.17463*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等人（2024）Chenxin An、Fei Huang、Jun Zhang、Shansan Gong、Xipeng Qiu、Chang Zhou
    和 Lingpeng Kong。2024年。无训练的长上下文扩展大型语言模型。*arXiv 预印本 arXiv:2402.17463*。
- en: Anthropic (2024) Anthropic. 2024. Introducing the next generation of claude.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2024）Anthropic。2024年。介绍下一代 Claude。
- en: Asai et al. (2019) Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard
    Socher, and Caiming Xiong. 2019. Learning to retrieve reasoning paths over wikipedia
    graph for question answering. *arXiv preprint arXiv:1911.10470*.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asai 等人（2019）Akari Asai、Kazuma Hashimoto、Hannaneh Hajishirzi、Richard Socher
    和 Caiming Xiong。2019年。学习在维基百科图谱上检索推理路径以进行问答。*arXiv 预印本 arXiv:1911.10470*。
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*,
    pages 2206–2240\. PMLR.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud 等人（2022）Sebastian Borgeaud、Arthur Mensch、Jordan Hoffmann、Trevor Cai、Eliza
    Rutherford、Katie Millican、George Bm Van Den Driessche、Jean-Baptiste Lespiau、Bogdan
    Damoc、Aidan Clark 等人。2022年。通过从数万亿个标记中检索来改进语言模型。在*国际机器学习会议*上，第2206–2240页。PMLR。
- en: 'Chen et al. (2017) Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
    2017. Reading wikipedia to answer open-domain questions. In *Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pages 1870–1879.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2017）Danqi Chen、Adam Fisch、Jason Weston 和 Antoine Bordes。2017年。在*第55届计算语言学协会年会（第1卷：长篇论文）*上阅读维基百科以回答开放领域问题，第1870–1879页。
- en: 'Chen et al. (2024) Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian,
    and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity
    text embeddings through self-knowledge distillation. *arXiv preprint arXiv:2402.03216*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024）Jianlv Chen、Shitao Xiao、Peitian Zhang、Kun Luo、Defu Lian 和 Zheng
    Liu。2024年。Bge m3-embedding：通过自我知识蒸馏的多语言、多功能、多粒度文本嵌入。*arXiv 预印本 arXiv:2402.03216*。
- en: Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023a. Extending context window of large language models via positional
    interpolation. *arXiv preprint arXiv:2306.15595*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023a）Shouyuan Chen、Sherman Wong、Liangjian Chen 和 Yuandong Tian。2023a年。通过位置插值扩展大型语言模型的上下文窗口。*arXiv
    预印本 arXiv:2306.15595*。
- en: Chen et al. (2023b) Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, and
    William Cohen. 2023b. Augmenting pre-trained language models with qa-memory for
    open-domain question answering. In *Proceedings of the 17th Conference of the
    European Chapter of the Association for Computational Linguistics*, pages 1597–1610.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023b）Wenhu Chen, Pat Verga, Michiel de Jong, John Wieting, 和 William
    Cohen。2023b。通过qa-memory增强预训练语言模型以进行开放领域问答。在 *第17届欧洲计算语言学协会年会论文集*，第1597–1610页。
- en: 'Cheng et al. (2021) Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu
    Chen, and Jianfeng Gao. 2021. Unitedqa: A hybrid approach for open domain question
    answering. *arXiv preprint arXiv:2101.00178*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2021）Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen,
    和 Jianfeng Gao。2021。Unitedqa：一种用于开放领域问答的混合方法。*arXiv 预印本 arXiv:2101.00178*。
- en: Dai and Callan (2019) Zhuyun Dai and Jamie Callan. 2019. Deeper text understanding
    for ir with contextual neural language modeling. In *Proceedings of the 42nd international
    ACM SIGIR conference on research and development in information retrieval*, pages
    985–988.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 和 Callan（2019）Zhuyun Dai 和 Jamie Callan。2019。通过上下文神经语言建模深入理解文本以进行信息检索。在
    *第42届国际 ACM SIGIR 信息检索研究与开发会议论文集*，第985–988页。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等（2022）Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher Ré。2022。Flashattention：快速且内存高效的确切注意力与IO感知。*神经信息处理系统进展*，35:16344–16359。
- en: Dhingra et al. (2020) Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran,
    Graham Neubig, Ruslan Salakhutdinov, and William W Cohen. 2020. Differentiable
    reasoning over a virtual knowledge base. *arXiv preprint arXiv:2002.10640*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhingra 等（2020）Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham
    Neubig, Ruslan Salakhutdinov, 和 William W Cohen。2020。对虚拟知识库的可微推理。*arXiv 预印本 arXiv:2002.10640*。
- en: Ding et al. (2019) Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie
    Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 2694–2703.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2019）Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, 和 Jie Tang。2019。用于大规模多跳阅读理解的认知图。在
    *第57届计算语言学协会年会论文集*，第2694–2703页。
- en: Fang et al. (2019) Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang,
    and Jingjing Liu. 2019. Hierarchical graph network for multi-hop question answering.
    *arXiv preprint arXiv:1911.03631*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等（2019）Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuohang Wang, 和 Jingjing
    Liu。2019。用于多跳问答的层次图网络。*arXiv 预印本 arXiv:1911.03631*。
- en: 'Gao et al. (2022) Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. 2022.
    Tevatron: An efficient and flexible toolkit for dense retrieval. *ArXiv*, abs/2203.05765.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2022）Luyu Gao, Xueguang Ma, Jimmy J. Lin, 和 Jamie Callan。2022。Tevatron：一个高效且灵活的密集检索工具包。*ArXiv*，abs/2203.05765。
- en: 'Günther et al. (2023) Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine
    Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas,
    Saba Sturua, Bo Wang, et al. 2023. Jina embeddings 2: 8192-token general-purpose
    text embeddings for long documents. *arXiv preprint arXiv:2310.19923*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Günther 等（2023）Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem,
    Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua,
    Bo Wang 等。2023。Jina embeddings 2：8192-token 通用文本嵌入用于长文档。*arXiv 预印本 arXiv:2310.19923*。
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pages 3929–3938\. PMLR.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等（2020）Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei Chang。2020。检索增强语言模型预训练。在
    *国际机器学习会议*，第3929–3938页。PMLR。
- en: 'Hao et al. (2022) Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and
    Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1, 000 examples.
    *ArXiv*, abs/2212.06713.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等（2022）Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, 和 Furu Wei。2022。结构化提示：将上下文学习扩展到1,000个示例。*ArXiv*，abs/2212.06713。
- en: Izacard and Grave (2020a) Gautier Izacard and Edouard Grave. 2020a. Distilling
    knowledge from reader to retriever for question answering. *arXiv preprint arXiv:2012.04584*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 和 Grave（2020a）Gautier Izacard 和 Edouard Grave。2020a。从阅读器到检索器的知识蒸馏。*arXiv
    预印本 arXiv:2012.04584*。
- en: Izacard and Grave (2020b) Gautier Izacard and Edouard Grave. 2020b. Leveraging
    passage retrieval with generative models for open domain question answering. *arXiv
    preprint arXiv:2007.01282*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 和 Grave（2020b）Gautier Izacard 和 Edouard Grave。2020b。利用生成模型进行开放领域问答的段落检索。*arXiv
    预印本 arXiv:2007.01282*。
- en: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane A. Yu, Armand Joulin, Sebastian Riedel, and Edouard
    Grave. 2022. Few-shot learning with retrieval augmented language models. *ArXiv*,
    abs/2208.03299.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 等 (2022) Gautier Izacard、Patrick Lewis、Maria Lomeli、Lucas Hosseini、Fabio
    Petroni、Timo Schick、Jane A. Yu、Armand Joulin、Sebastian Riedel 和 Edouard Grave。2022。利用检索增强语言模型的少量样本学习。*ArXiv*，abs/2208.03299。
- en: 'Jin et al. (2024) Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui
    Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. 2024. Llm maybe longlm: Self-extend
    llm context window without tuning. *arXiv preprint arXiv:2401.01325*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin 等 (2024) Hongye Jin、Xiaotian Han、Jingfeng Yang、Zhimeng Jiang、Zirui Liu、Chia-Yuan
    Chang、Huiyuan Chen 和 Xia Hu。2024。LLM 也许是 LongLM: 自我扩展 LLM 上下文窗口而无需调优。*arXiv 预印本
    arXiv:2401.01325*。'
- en: Johnson et al. (2019) Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale
    similarity search with gpus. *IEEE Transactions on Big Data*, 7(3):535–547.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等 (2019) Jeff Johnson、Matthijs Douze 和 Hervé Jégou。2019。利用 GPU 进行十亿级别的相似性搜索。*IEEE
    大数据学报*，7(3):535–547。
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage
    retrieval for open-domain question answering. *arXiv preprint arXiv:2004.04906*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin 等 (2020) Vladimir Karpukhin、Barlas Oğuz、Sewon Min、Patrick Lewis、Ledell
    Wu、Sergey Edunov、Danqi Chen 和 Wen-tau Yih。2020。用于开放域问答的密集段落检索。*arXiv 预印本 arXiv:2004.04906*。
- en: Khalifa et al. (2022) Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak
    Lee, and Lu Wang. 2022. Few-shot reranking for multi-hop qa via language model
    prompting. *arXiv preprint arXiv:2205.12650*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khalifa 等 (2022) Muhammad Khalifa、Lajanugen Logeswaran、Moontae Lee、Honglak Lee
    和 Lu Wang。2022。通过语言模型提示进行少量样本多跳排序。*arXiv 预印本 arXiv:2205.12650*。
- en: Khalifa et al. (2023) Muhammad Khalifa, Lajanugen Logeswaran, Moontae Lee, Honglak
    Lee, and Lu Wang. 2023. Few-shot reranking for multi-hop qa via language model
    prompting. *arXiv preprint arXiv:2205.12650*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khalifa 等 (2023) Muhammad Khalifa、Lajanugen Logeswaran、Moontae Lee、Honglak Lee
    和 Lu Wang。2023。通过语言模型提示进行少量样本多跳排序。*arXiv 预印本 arXiv:2205.12650*。
- en: 'Khattab et al. (2022) Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict:
    Composing retrieval and language models for knowledge-intensive nlp. *arXiv preprint
    arXiv:2212.14024*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Khattab 等 (2022) Omar Khattab、Keshav Santhanam、Xiang Lisa Li、David Hall、Percy
    Liang、Christopher Potts 和 Matei Zaharia。2022。Demonstrate-search-predict: 组合检索和语言模型以应对知识密集型
    NLP。*arXiv 预印本 arXiv:2212.14024*。'
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question
    answering research. *Transactions of the Association for Computational Linguistics*,
    7:453–466.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski 等 (2019) Tom Kwiatkowski、Jennimaria Palomaki、Olivia Redfield、Michael
    Collins、Ankur Parikh、Chris Alberti、Danielle Epstein、Illia Polosukhin、Jacob Devlin、Kenton
    Lee 等。2019。自然问题：一个问答研究的基准。*计算语言学协会会刊*，7:453–466。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation
    for knowledge-intensive nlp tasks. *ArXiv*, abs/2005.11401.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等 (2020) Patrick Lewis、Ethan Perez、Aleksandra Piktus、Fabio Petroni、Vladimir
    Karpukhin、Naman Goyal、Heinrich Kuttler、Mike Lewis、Wen-tau Yih、Tim Rocktäschel、Sebastian
    Riedel 和 Douwe Kiela。2020。针对知识密集型 NLP 任务的检索增强生成。*ArXiv*，abs/2005.11401。
- en: 'Lewis et al. (2021) Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini,
    Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021.
    Paq: 65 million probably-asked questions and what you can do with them. *Transactions
    of the Association for Computational Linguistics*, 9:1098–1115.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lewis 等 (2021) Patrick Lewis、Yuxiang Wu、Linqing Liu、Pasquale Minervini、Heinrich
    Küttler、Aleksandra Piktus、Pontus Stenetorp 和 Sebastian Riedel。2021。Paq: 6500 万个可能被提问的问题以及你可以如何利用它们。*计算语言学协会会刊*，9:1098–1115。'
- en: 'Li et al. (2021) Shaobo Li, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu,
    Chengjie Sun, Zhenzhou Ji, and Bingquan Liu. 2021. Hopretriever: Retrieve hops
    over wikipedia to answer complex questions. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 35, pages 13279–13287.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2021) Shaobo Li、Xiaoguang Li、Lifeng Shang、Xin Jiang、Qun Liu、Chengjie
    Sun、Zhenzhou Ji 和 Bingquan Liu。2021。Hopretriever: 检索维基百科上的跳跃以回答复杂问题。载于 *AAAI 人工智能会议论文集*，第
    35 卷，第 13279–13287 页。'
- en: 'Liu et al. (2024) Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang,
    Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et al. 2024. E^ 2-llm:
    Efficient and extreme length extension of large language models. *Findings of
    ACL 2024*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2024) Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang,
    Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su 等人。2024. E^ 2-llm: 大型语言模型的高效和极限长度扩展。
    *ACL 2024 会议成果*。'
- en: 'Ma et al. (2023) Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg,
    and Jianfeng Gao. 2023. Chain-of-skills: A configurable model for open-domain
    question answering. In *The 61st Annual Meeting Of The Association For Computational
    Linguistics*.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 (2023) Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, 和 Jianfeng
    Gao. 2023. Chain-of-skills: 一个可配置的开放域问答模型。 在 *第61届计算语言学协会年会*。'
- en: 'Mialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos
    Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane
    Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: a survey.
    *arXiv preprint arXiv:2302.07842*.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon 等人 (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos
    Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane
    Dwivedi-Yu, Asli Celikyilmaz 等人。2023. 扩展语言模型：一项调查。 *arXiv 预印本 arXiv:2302.07842*。
- en: 'Nussbaum et al. (2024) Zach Nussbaum, John X Morris, Brandon Duderstadt, and
    Andriy Mulyar. 2024. Nomic embed: Training a reproducible long context text embedder.
    *arXiv preprint arXiv:2402.01613*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nussbaum 等人 (2024) Zach Nussbaum, John X Morris, Brandon Duderstadt, 和 Andriy
    Mulyar. 2024. Nomic embed: 训练一个可重复的长上下文文本嵌入模型。 *arXiv 预印本 arXiv:2402.01613*。'
- en: OpenAI (2024) OpenAI. 2024. Hello gpt4-o.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2024) OpenAI. 2024. Hello gpt4-o。
- en: Peng and Quesnelle (2023) Bowen Peng and Jeffrey Quesnelle. 2023. Ntk-aware
    scaled rope allows llama models to have extended (8k+) context size without any
    fine-tuning and minimal perplexity degradation. [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 和 Quesnelle (2023) Bowen Peng 和 Jeffrey Quesnelle. 2023. Ntk-aware scaled
    rope 允许 llama 模型在没有任何微调和最小困惑度降级的情况下拥有扩展的 (8k+) 上下文大小。 [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have)。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, 和 Enrico Shippole.
    2023. Yarn: 大型语言模型的高效上下文窗口扩展。 *arXiv 预印本 arXiv:2309.00071*。'
- en: 'Press et al. (2021) Ofir Press, Noah Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. In
    *International Conference on Learning Representations*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等人 (2021) Ofir Press, Noah Smith, 和 Mike Lewis. 2021. 短期训练，长期测试：带有线性偏差的注意力机制实现输入长度外推。
    在 *国际学习表征会议*。
- en: 'Qu et al. (2020) Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin
    Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020. Rocketqa: An optimized training
    approach to dense passage retrieval for open-domain question answering. *arXiv
    preprint arXiv:2010.08191*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qu 等人 (2020) Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne
    Xin Zhao, Daxiang Dong, Hua Wu, 和 Haifeng Wang. 2020. Rocketqa: 一种优化的密集段落检索训练方法用于开放域问答。
    *arXiv 预印本 arXiv:2010.08191*。'
- en: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    2023. [Parallel context windows for large language models](https://doi.org/10.18653/v1/2023.acl-long.352).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6383–6402, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ratner 等人 (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, 和 Yoav Shoham.
    2023. [大型语言模型的并行上下文窗口](https://doi.org/10.18653/v1/2023.acl-long.352)。 在 *第61届计算语言学协会年会论文集
    (卷1: 长篇论文)*，页码 6383–6402，加拿大多伦多。计算语言学协会。'
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid 等人 (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser 等人。2024. Gemini 1.5: 解锁跨越百万标记上下文的多模态理解。 *arXiv 预印本
    arXiv:2403.05530*。'
- en: Saad-Falcon et al. (2024) Jon Saad-Falcon, Daniel Y Fu, Simran Arora, Neel Guha,
    and Christopher Ré. 2024. Benchmarking and building long-context retrieval models
    with loco and m2-bert. *arXiv preprint arXiv:2402.07440*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saad-Falcon et al. (2024) 乔恩·萨德-法尔肯、丹尼尔·Y·傅、辛曼·阿罗拉、尼尔·古哈和克里斯托弗·雷。2024年。通过loco和m2-bert对长上下文检索模型进行基准测试和构建。*arXiv预印本
    arXiv:2402.07440*。
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) 石伟佳、米永·敏、安东·康和文涛·易。2023年。Replug：检索增强的黑箱语言模型。*arXiv预印本 arXiv:2301.12652*。
- en: Singh et al. (2021) Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and
    Dani Yogatama. 2021. End-to-end training of multi-document reader and retriever
    for open-domain question answering. *Advances in Neural Information Processing
    Systems*, 34:25968–25981.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2021) 德文德拉·辛格、斯瓦·雷迪、威尔·汉密尔顿、克里斯·戴尔和达尼·瑜伽塔马。2021年。端到端训练多文档阅读器和检索器用于开放领域问答。*神经信息处理系统进展*，34:25968–25981。
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2021) 纪安林·苏、余璐、盛丰·潘、艾哈迈德·穆尔塔达、博·温和云峰·刘。2021年。Roformer：带有旋转位置嵌入的增强型变换器。*arXiv预印本
    arXiv:2104.09864*。
- en: Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
    and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning
    for knowledge-intensive multi-step questions. *arXiv preprint arXiv:2212.10509*.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trivedi et al. (2022) 哈什·特里维迪、尼兰詹·巴拉苏布拉马尼安、图沙尔·霍特和阿希什·萨巴尔瓦尔。2022年。在知识密集型多步骤问题中交替检索与链式推理。*arXiv预印本
    arXiv:2212.10509*。
- en: Wang et al. (2023) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan
    Majumder, and Furu Wei. 2023. Improving text embeddings with large language models.
    *arXiv preprint arXiv:2401.00368*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) 梁旺、南杨、肖龙黄、林军杨、兰根·马久梅德和傅如伟。2023年。通过大型语言模型改进文本嵌入。*arXiv预印本
    arXiv:2401.00368*。
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
    2023. C-pack: Packaged resources to advance general chinese embedding. *arXiv:2309.07597*.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) 石涛·肖、郑刘、佩天·张和尼克拉斯·穆恩尼霍夫。2023年。C-pack：推进通用中文嵌入的打包资源。*arXiv:2309.07597*。
- en: Xiong et al. (2020a) Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin
    Liu, Paul Bennett, Junaid Ahmed, and Arnold Overwijk. 2020a. Approximate nearest
    neighbor negative contrastive learning for dense text retrieval. *arXiv preprint
    arXiv:2007.00808*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2020a) 李·熊、陈岩·熊、叶李、郭丰·汤、贾琳·刘、保罗·贝内特、朱奈德·艾哈迈德和阿诺德·欧维克。2020a年。用于密集文本检索的近似最近邻负对比学习。*arXiv预印本
    arXiv:2007.00808*。
- en: Xiong et al. (2020b) Wenhan Xiong, Xiang Lorraine Li, Srini Iyer, Jingfei Du,
    Patrick Lewis, William Yang Wang, Yashar Mehdad, Wen-tau Yih, Sebastian Riedel,
    Douwe Kiela, et al. 2020b. Answering complex open-domain questions with multi-hop
    dense retrieval. *arXiv preprint arXiv:2009.12756*.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2020b) 温汉·熊、向·洛林·李、斯里尼·艾耶尔、晶菲·杜、帕特里克·刘易斯、威廉·杨·王、雅沙尔·梅赫达、温涛·易、塞巴斯蒂安·里德尔、杜维·凯拉等。2020b年。通过多跳密集检索回答复杂的开放领域问题。*arXiv预印本
    arXiv:2009.12756*。
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset
    for diverse, explainable multi-hop question answering. *arXiv preprint arXiv:1809.09600*.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2018) 直林·杨、彭奇、赛政·张、约书亚·本吉奥、威廉·W·科恩、鲁斯兰·萨拉胡丁诺夫和克里斯托弗·D·曼宁。2018年。Hotpotqa：一个用于多跳问答的多样化、可解释的数据集。*arXiv预印本
    arXiv:1809.09600*。
- en: 'Yu et al. (2021) Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang
    Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. 2021. Kg-fid: Infusing
    knowledge graph in fusion-in-decoder for open-domain question answering. *arXiv
    preprint arXiv:2110.04330*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2021) 董汉·余、成光·朱、余伟·方、文浩·余、朔航·王、艺聪·徐、向·任、依明·杨和迈克尔·曾。2021年。Kg-fid：在融合解码器中注入知识图谱以进行开放领域问答。*arXiv预印本
    arXiv:2110.04330*。
- en: 'Yu (2022) Wenhao Yu. 2022. Retrieval-augmented generation across heterogeneous
    knowledge. In *Proceedings of the 2022 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies:
    Student Research Workshop*, pages 52–58.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu (2022) 文浩·余。2022年。跨异质知识的检索增强生成。在*2022年北美计算语言学协会：人类语言技术会议：学生研究研讨会论文集*中，页52–58。
- en: Yu et al. (2023) Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish
    Sabharwal. 2023. Improving language models via plug-and-play retrieval feedback.
    *arXiv preprint arXiv:2305.14002*.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余等（2023）文浩·余、智涵·张、镇文·梁、孟江和阿希什·萨巴尔瓦尔。2023年。通过即插即用检索反馈改进语言模型。*arXiv预印本arXiv:2305.14002*。
- en: 'Zhao et al. (2019) Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul Bennett,
    and Saurabh Tiwary. 2019. Transformer-xh: Multi-evidence reasoning with extra
    hop attention. In *International Conference on Learning Representations*.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '赵等（2019）陈赵、陈岩熊、科比·罗塞特、夏宋、保罗·贝内特和萨乌拉布·提瓦里。2019年。Transformer-xh: 通过额外跳跃注意力进行多证据推理。发表于*国际学习表征会议*。'
- en: 'Zhu et al. (2024a) Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu,
    Furu Wei, and Sujian Li. 2024a. Longembed: Extending embedding models for long
    context retrieval. *arXiv preprint arXiv:2404.12096*.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等（2024a）大伟·朱、梁王、南洋、逸凡·宋、文浩·吴、傅如·魏和苏健·李。2024a。Longembed: 扩展嵌入模型以进行长上下文检索。*arXiv预印本arXiv:2404.12096*。'
- en: 'Zhu et al. (2024b) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu,
    Furu Wei, and Sujian Li. 2024b. PoSE: Efficient context window extension of LLMs
    via positional skip-wise training. In *The Twelfth International Conference on
    Learning Representations*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等（2024b）大伟·朱、南洋、梁王、逸凡·宋、文浩·吴、傅如·魏和苏健·李。2024b。PoSE: 通过位置跳跃训练高效扩展LLMs的上下文窗口。发表于*第十二届国际学习表征会议*。'
- en: Zhu et al. (2021) Yunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi
    Cheng. 2021. Adaptive information seeking for open-domain question answering.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 3615–3626.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等（2021）云昌·朱、梁庞、艳艳·兰、华为·沈和雪琦·程。2021年。针对开放域问答的自适应信息获取。发表于*2021年自然语言处理经验方法会议论文集*，第3615–3626页。
- en: '| Method | Prompt |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 提示 |'
- en: '| --- | --- |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Closed-Book | Here are some examples of questions and their corresponding
    answer, each with a “Question” field and an “Answer” field. Answer the question
    directly and don’t output other thing. “Question”: …“Answer”: …'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '| 闭卷 | 这里是一些问题及其对应的答案，每个都有“问题”和“答案”字段。直接回答问题，不要输出其他内容。“Question”: …“Answer”:
    …'
- en: '“Question”: …“Answer”: …'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: …“Answer”: …'
- en: '“Question”: …“Answer”: …'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: …“Answer”: …'
- en: …
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: '“Question”: …“Answer”: …'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: …“Answer”: …'
- en: Answer the following question.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题。
- en: '“Question”: who is the owner of reading football club “Answer”: |'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: 谁是雷丁足球俱乐部的所有者“Answer”: |'
- en: '| LongRAG | Turn 1: Go through the following context and then answer the question.
    The context is a list of Wikipedia documents, ordered by title: …. Each Wikipedia
    document contains a title field and a text field. The context is:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '| LongRAG | 第1轮：浏览以下上下文，然后回答问题。上下文是按标题排序的维基百科文档列表：…. 每个维基百科文档包含一个标题字段和一个文本字段。上下文是：'
- en: '“Title”: …“Text”: …'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '“Title”: …“Text”: …'
- en: '“Title”: …“Text”: ……'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '“Title”: …“Text”: ……'
- en: '“Title”: …“Text”: …'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '“Title”: …“Text”: …'
- en: 'Find the useful documents from the context, then answer the question: …. Answer
    the question directly. Your response should be very concise.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 从上下文中找出有用的文档，然后回答问题：…. 直接回答问题。你的回答应尽可能简洁。
- en: 'Turn 2: You have been provided with a question and its long answer. Your task
    is to derive a very concise short answer, extracting a substring from the given
    long answer. Short answer is typically an entity without any other redundant words.
    It’s important to ensure that the output short answer remains as simple as possible.
    Here a few examples:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 第2轮：你已获得一个问题及其详细回答。你的任务是提取一个非常简洁的短答案，从给定的详细回答中提取一个子字符串。短答案通常是一个实体，不含其他多余词语。确保输出的短答案尽可能简单。这里是几个例子：
- en: '“Question”: …“Long Answer”: …“Short Answer”: …'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: …“Long Answer”: …“Short Answer”: …'
- en: '“Question”: …“Long Answer”: …“Short Answer”: …'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: …“Long Answer”: …“Short Answer”: …'
- en: '“Question”: …“Long Answer”: …“Short Answer”: …'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: …“Long Answer”: …“Short Answer”: …'
- en: 'Extract the short answer of the following question and long answer:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 提取以下问题和详细回答的短答案：
- en: '“Question”: when did the philadelphia eagles play in the super bowl last “Long
    Answer”: The Philadelphia Eagles last played in the Super Bowl on February 4,
    2018, in Super Bowl LII. “Short Answer”: |'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '“Question”: 费城老鹰队上次参加超级碗是什么时候“Long Answer”: 费城老鹰队上次参加超级碗是在2018年2月4日，超级碗LII。“Short
    Answer”: |'
- en: 'Table 6: Here are the prompts we used for all the experiments. For the closed-book
    method, we use 16-shot in-context examples. For LongRAG, we use a two-turn approach
    to extract the final answer. The first turn doesn’t require any in-context examples
    and generate a longer answer, typically ranging from a few words to a few sentences.
    In the second turn, we use 8-shot in-context examples to calibrate and extract
    the exact short answer, which is typically just a few words.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：这里是我们在所有实验中使用的提示。对于闭卷方法，我们使用 16-shot 上下文示例。对于 LongRAG，我们使用两轮方法来提取最终答案。第一轮不需要任何上下文示例，并生成较长的答案，通常范围从几个词到几句话。在第二轮中，我们使用
    8-shot 上下文示例来校准并提取准确的简短答案，通常只有几个词。
- en: '| Question | Ground truth | LongRAG prediction |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 真实答案 | LongRAG 预测 |'
- en: '| --- | --- | --- |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| where does the bob and tom show broadcast from | Indianapolis , Indiana |
    Indianapolis |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Bob 和 Tom 的节目在哪里播出 | 印第安纳波利斯，印第安纳州 | 印第安纳波利斯 |'
- en: '| who has given the theory of unbalanced economic growth | Hirschman | Albert
    O. Hirschman |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 谁提出了不平衡经济增长理论 | Hirschman | 阿尔伯特·O·赫希曼 |'
- en: '| when does season 6 of the next step start | 2018 | September 29, 2018 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 下一步第六季什么时候开始 | 2018 | 2018年9月29日 |'
- en: '| what was the precursor to the present day internet | the ARPANET project
    | ARPANET |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 现今互联网的前身是什么 | ARPANET 项目 | ARPANET |'
- en: 'Table 7: Some examples demonstrate that LongRAG has extracted aliases or different
    forms of the ground truth.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：一些示例展示了 LongRAG 提取了别名或不同形式的真实信息。
- en: 6 Appendix
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 附录
- en: 6.1 Prompts Template for Long Context Reader
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 Long Context Reader 的提示模板
- en: 'We have put out prompts used for the experiments in Table [6](#Sx1.T6 "Table
    6 ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs").
    For the closed-book method, we use 16-shot in-context examples. For LongRAG, we
    use a two-turn approach to extract the final answer. In the first turn, the long
    retrieved context and the question are concatenated as input, and we do not use
    any in-context examples here due to the context being around 30K tokens. Empirically,
    we found it beneficial to let the reader generate a longer answer initially, typically
    ranging from a few words to a few sentences. In the second turn, we use 8-shot
    in-context examples to guide the reader in further extracting the most important
    part of the long answer as the short answer, which is typically just a few words.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经发布了表 [6](#Sx1.T6 "表 6 ‣ LongRAG：利用长上下文 LLM 提升检索增强生成") 中用于实验的提示。对于闭卷方法，我们使用
    16-shot 上下文示例。对于 LongRAG，我们使用两轮方法来提取最终答案。在第一轮中，长检索到的上下文和问题被连接作为输入，由于上下文大约为 30K
    个 tokens，我们这里不使用任何上下文示例。从经验上看，让阅读器最初生成较长的答案通常是有益的，通常范围从几个词到几句话。在第二轮中，我们使用 8-shot
    上下文示例来引导阅读器进一步提取长答案中最重要的部分作为简短答案，通常只有几个词。
- en: 6.2 Refined Metric
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 精细化指标
- en: 'The most standard metric used in open-domain question answering tasks is EM
    (Exact Match), since the correct answer must be a substring within the corpus.
    In our framework, since the long retrieved context, which contains multiple highly-related
    documents to the given query, is fed into the reader, there is a much higher possibility
    that an alias of the ground truth exists in the context and can be extracted by
    the reader. As shown in Table [7](#Sx1.T7 "Table 7 ‣ LongRAG: Enhancing Retrieval-Augmented
    Generation with Long-context LLMs"), although LongRAG’s prediction doesn’t exactly
    match the ground truth, it’s obvious that LongRAG’s prediction is correct. To
    better and more fairly evaluate LongRAG’s performance, we have refined the EM
    metric slightly. We recognize it as an exact match if the prediction is less than
    five tokens (indicating that the short answer is successfully extracted as described
    in Section [6.1](#S6.SS1 "6.1 Prompts Template for Long Context Reader ‣ 6 Appendix
    ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs"))
    and the ground truth is a substring of the prediction or vice versa. We have also
    manually verified that this refined metric indeed captures aliases or other forms
    of the ground truth. For the fully-supervised RAG baselines used in our paper,
    given that they are fine-tuned on the training data and the retrieval unit is
    a small snippet, we believe that the difference won’t be significant when using
    the refined EM.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '在开放领域问答任务中，最标准的度量指标是 EM（准确匹配），因为正确答案必须是语料库中的一个子字符串。在我们的框架中，由于长篇检索到的上下文包含了多个与给定查询高度相关的文档，这些上下文被输入到阅读器中，因此更有可能在上下文中存在地面真实值的别名，并且可以被阅读器提取出来。正如表[7](#Sx1.T7
    "Table 7 ‣ LongRAG: Enhancing Retrieval-Augmented Generation with Long-context
    LLMs")所示，尽管LongRAG的预测与地面真实值不完全匹配，但显然LongRAG的预测是正确的。为了更好地和更公平地评估LongRAG的性能，我们对EM指标进行了稍微调整。如果预测结果少于五个词（表明短答案如[6.1](#S6.SS1
    "6.1 Prompts Template for Long Context Reader ‣ 6 Appendix ‣ LongRAG: Enhancing
    Retrieval-Augmented Generation with Long-context LLMs")节所描述成功提取），且地面真实值是预测结果的子字符串或反之，则我们将其视为准确匹配。我们还手动验证了这个调整后的指标确实捕捉到了别名或地面真实值的其他形式。对于我们论文中使用的完全监督的RAG基线模型，由于它们在训练数据上进行了微调，而检索单元是一个小片段，我们相信使用调整后的EM指标时，差异不会很显著。'
- en: 6.3 Dataset Licenses
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 数据集许可
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NQ: Apache License 2.0'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'NQ: Apache License 2.0'
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'HotpotQA: CC BY-SA 4.0 License'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'HotpotQA: CC BY-SA 4.0 License'
