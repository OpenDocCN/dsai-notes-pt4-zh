- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:00:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:00:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and
    Hybrid Approach
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成还是长上下文 LLM？一项全面的研究和混合方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16833](https://ar5iv.labs.arxiv.org/html/2407.16833)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16833](https://ar5iv.labs.arxiv.org/html/2407.16833)
- en: Zhuowan Li¹  Cheng Li¹  Mingyang Zhang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhuowan Li¹  Cheng Li¹  Mingyang Zhang¹
- en: Qiaozhu Mei²  Michael Bendersky¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Qiaozhu Mei²  Michael Bendersky¹
- en: ¹ Google DeepMind  ² University of Michigan
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 谷歌 DeepMind  ² 密歇根大学
- en: ¹ {zhuowan,chgli,mingyang,bemike}@google.com  ² qmei@umich.edu Visiting researcher
    to Google DeepMind.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ {zhuowan,chgli,mingyang,bemike}@google.com  ² qmei@umich.edu 谷歌 DeepMind 访问研究员。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language
    Models (LLMs) to efficiently process overly lengthy contexts. However, recent
    LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to understand long
    contexts directly. We conduct a comprehensive comparison between RAG and long-context
    (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across
    various public datasets using three latest LLMs. Results reveal that when resourced
    sufficiently, LC consistently outperforms RAG in terms of average performance.
    However, RAG’s significantly lower cost remains a distinct advantage. Based on
    this observation, we propose Self-Route, a simple yet effective method that routes
    queries to RAG or LC based on model self-reflection. Self-Route significantly
    reduces the computation cost while maintaining a comparable performance to LC.
    Our findings provide a guideline for long-context applications of LLMs using RAG
    and LC.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）已经成为大型语言模型（LLMs）有效处理过长上下文的强大工具。然而，像 Gemini-1.5 和 GPT-4 这样的近期 LLM
    展现了直接理解长上下文的卓越能力。我们进行了一项全面的比较，比较 RAG 和长上下文（LC）LLMs，旨在利用两者的优势。我们在多个公共数据集上使用三种最新的
    LLM 对 RAG 和 LC 进行了基准测试。结果显示，当资源充足时，LC 在平均性能方面始终优于 RAG。然而，RAG 显著较低的成本仍然是一个明显的优势。基于这一观察，我们提出了
    Self-Route，这是一种简单而有效的方法，通过模型自我反思将查询路由到 RAG 或 LC。Self-Route 显著降低了计算成本，同时保持了与 LC
    相当的性能。我们的发现为使用 RAG 和 LC 的长上下文应用提供了指南。
- en: Retrieval Augmented Generation or Long-Context LLMs?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成还是长上下文 LLM？
- en: A Comprehensive Study and Hybrid Approach
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一项全面的研究和混合方法
- en: 'Zhuowan Li¹  Cheng Li¹  Mingyang Zhang¹ Qiaozhu Mei²^†^†thanks: Visiting researcher
    to Google DeepMind.  Michael Bendersky¹ ¹ Google DeepMind  ² University of Michigan
    ¹ {zhuowan,chgli,mingyang,bemike}@google.com  ² qmei@umich.edu'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Zhuowan Li¹  Cheng Li¹  Mingyang Zhang¹ Qiaozhu Mei²^†^†感谢：谷歌 DeepMind 访问研究员。
     Michael Bendersky¹ ¹ 谷歌 DeepMind  ² 密歇根大学 ¹ {zhuowan,chgli,mingyang,bemike}@google.com
     ² qmei@umich.edu
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Retrieval augmented generation (RAG) has been shown to be a both effective and
    efficient approach for large language models (LLMs) to leverage external knowledge.
    RAG retrieves relevant information based on the query and then prompts an LLM
    to generate a response in the context of the retrieved information. This approach
    significantly expands LLM’s access to vast amounts of information at a minimal
    cost.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）已经被证明是大型语言模型（LLMs）利用外部知识的一种有效且高效的方法。RAG 根据查询检索相关信息，然后提示 LLM 在检索到的信息背景下生成响应。这种方法显著扩展了
    LLM 对大量信息的访问，同时成本最小。
- en: 'However, recent LLMs like Gemini and GPT-4 have demonstrated exceptional capabilities
    in understanding long contexts directly. For example, Gemini 1.5 can process up
    to 1 million tokens Reid et al. ([2024](#bib.bib36)). This prompts the need for
    a systematic comparison between long-context (LC) LLMs and RAG: on one hand, RAG
    conceptually acts as a prior, regularizing the attention of LLMs onto retrieved
    segments, thus avoiding the distraction of the irrelevant information and saving
    unnecessary attention computations; on the other hand, large-scale pretraining
    may enable LLMs to develop even stronger long-context capabilities. Therefore,
    we are motivated to compare RAG and LC, evaluating both their performance and
    efficiency.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像 Gemini 和 GPT-4 这样的近期 LLM 已经展示了直接理解长上下文的卓越能力。例如，Gemini 1.5 可以处理多达 100 万个标记
    Reid 等（[2024](#bib.bib36)）。这促使我们需要对长上下文（LC）LLMs 和 RAG 进行系统的比较：一方面，RAG 从概念上来说，作为一种先验，规范了
    LLM 对检索到的片段的注意力，从而避免了无关信息的干扰，并节省了不必要的注意力计算；另一方面，大规模预训练可能使 LLM 发展出更强的长上下文能力。因此，我们有动力比较
    RAG 和 LC，评估它们的性能和效率。
- en: '![Refer to caption](img/717655fe9e1f43ade3dd26a496be1e5d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/717655fe9e1f43ade3dd26a496be1e5d.png)'
- en: 'Figure 1: While long-context LLMs (LC) surpass RAG in long-context understanding,
    RAG is significantly more cost-efficient. Our approach, Self-Route, combining
    RAG and LC, achieves comparable performance to LC at a much lower cost.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：虽然长上下文 LLMs（LC）在长上下文理解上超越了 RAG，但 RAG 在成本效益上显著更高。我们的方法 Self-Route 结合了 RAG
    和 LC，在成本大幅降低的情况下，达到了与 LC 相当的性能。
- en: In this work, we systematically benchmark RAG and LC on various public datasets,
    gaining a comprehensive understanding of their pros and cons, and ultimately combining
    them to get the best of both worlds. Different from findings in previous work
    Xu et al. ([2023](#bib.bib42)), we find that LC consistently outperform RAG in
    almost all settings (when resourced sufficiently). This demonstrates the superior
    progress of recent LLMs in long-context understanding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们在各种公共数据集上系统地基准测试了 RAG 和 LC，全面了解了它们的优缺点，并最终将它们结合起来以获得最佳效果。与先前工作 Xu 等人
    ([2023](#bib.bib42)) 的发现不同，我们发现 LC 在几乎所有设置下（当资源足够时）始终优于 RAG。这展示了最近 LLMs 在长上下文理解方面的显著进步。
- en: Despite the suboptimal performance, RAG remains relevant due to its significantly
    lower computational cost. In contrast to LC, RAG significantly decreases the input
    length to LLMs, leading to reduced costs, as LLM API pricing is typically based
    on the number of input tokens. Google ([2024](#bib.bib12)); OpenAI ([2024b](#bib.bib35))¹¹1While
    retrieval may introduce extra cost, retrieval system is much easier to set up
    and can be hosted on customer side.. Moreover, our analysis reveals that the predictions
    from LC and RAG are identical for over 60% of queries. For these queries, RAG
    can reduce cost without sacrificing performance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管性能并不最佳，但由于其显著较低的计算成本，RAG 仍然具有相关性。与 LC 相比，RAG 显著减少了 LLMs 的输入长度，从而降低了成本，因为 LLM
    API 定价通常基于输入令牌的数量。Google ([2024](#bib.bib12)); OpenAI ([2024b](#bib.bib35))¹¹1虽然检索可能会引入额外的成本，但检索系统更容易设置，并且可以托管在客户端。此外，我们的分析显示，对于超过
    60% 的查询，LC 和 RAG 的预测结果是相同的。对于这些查询，RAG 可以在不牺牲性能的情况下降低成本。
- en: Based on this observation, we propose Self-Route, a simple yet effective method
    that routes various queries to RAG or LC based on model self-reflection. With
    Self-Route, we significantly reduce the cost while achieving overall performance
    comparable to LC. For example, the cost is reduced by 65% for Gemini-1.5-Pro and
    39% for GPT-4O.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一观察，我们提出了 Self-Route，这是一种简单而有效的方法，根据模型自我反思将各种查询路由到 RAG 或 LC。通过 Self-Route，我们在实现整体性能与
    LC 相当的同时显著降低了成本。例如，Gemini-1.5-Pro 的成本降低了 65%，GPT-4O 的成本降低了 39%。
- en: '[Fig. 1](#S1.F1 "In 1 Introduction ‣ Retrieval Augmented Generation or Long-Context
    LLMs? A Comprehensive Study and Hybrid Approach") shows the comparisons of LC,
    RAG and Self-Route using three recent LLMs: GPT-4O, GPT-3.5-Turbo and Gemini-1.5-Pro.
    In addition to quantitative evaluation, we provide a comprehensive analysis comparing
    RAG and LC, including common failure patterns of RAG, the trade-offs between cost
    and performance, and the results on additional synthetic datasets. Our analysis
    serves as a starting point, inspiring future improvements of RAG, and as a empirical
    guide for building long-context applications using RAG and LC.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1](#S1.F1 "在 1 介绍 ‣ 检索增强生成还是长上下文 LLMs？全面研究与混合方法") 显示了使用三种最近的 LLMs：GPT-4O、GPT-3.5-Turbo
    和 Gemini-1.5-Pro 进行的 LC、RAG 和 Self-Route 的比较。除了定量评估，我们还提供了对 RAG 和 LC 的全面分析，包括
    RAG 的常见失败模式、成本与性能之间的权衡，以及在额外合成数据集上的结果。我们的分析作为一个起点，激发了 RAG 未来的改进，并作为构建使用 RAG 和
    LC 的长上下文应用的经验指南。'
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Long-context LLMs. There has long been efforts for enabling LLMs to handle long
    contexts Guo et al. ([2022](#bib.bib14)); Beltagy et al. ([2020](#bib.bib6));
    Chen et al. ([2023b](#bib.bib10)). While recent LLMs like Gemini-1.5 Reid et al.
    ([2024](#bib.bib36)), GPT-4 Achiam et al. ([2023](#bib.bib1)), Claude-3 Anthropic
    ([2024](#bib.bib3)) achieve significantly larger context window size, long-context
    prompting is still expensive due to the quadratic computation cost of transformers
    regarding to the input token numbers. Recent work proposes methods to reduce cost
    by prompt compression Jiang et al. ([2023](#bib.bib21)), model distillation Hsieh
    et al. ([2023](#bib.bib18)), or LLM cascading Chen et al. ([2023a](#bib.bib9)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本LLMs。长期以来，已经有努力使LLMs能够处理长文本 Guo等人 ([2022](#bib.bib14))；Beltagy等人 ([2020](#bib.bib6))；Chen等人
    ([2023b](#bib.bib10))。尽管像Gemini-1.5 Reid等人 ([2024](#bib.bib36))、GPT-4 Achiam等人
    ([2023](#bib.bib1))、Claude-3 Anthropic ([2024](#bib.bib3)) 这样的最新LLMs 实现了显著更大的上下文窗口大小，但由于变换器对输入标记数量的二次计算成本，长文本提示仍然昂贵。最近的工作提出了通过提示压缩
    Jiang等人 ([2023](#bib.bib21))、模型蒸馏 Hsieh等人 ([2023](#bib.bib18)) 或LLM级联 Chen等人 ([2023a](#bib.bib9))
    来降低成本的方法。
- en: Retrieval-augmented generation. Augmenting LLMs with relevant information retrieved
    from various sources Lewis et al. ([2020](#bib.bib26)), *i.e*., RAG, has been
    successful in complementing LLMs with external knowledge. RAG achieves good performance
    on various of tasks like language modeling Khandelwal et al. ([2019](#bib.bib22));
    Shi et al. ([2023](#bib.bib38)) and QA Guu et al. ([2020](#bib.bib15)); Izacard
    and Grave ([2020](#bib.bib20)), with a significantly lower computation cost Borgeaud
    et al. ([2022](#bib.bib7)). Related to but different from our work, recently works
    augment RAG with correction Yan et al. ([2024](#bib.bib43)), critique Asai et al.
    ([2023](#bib.bib4)), or verification Li et al. ([2023](#bib.bib27)) to improve
    retrieval quality on knowledge-intensive tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成。通过从各种来源检索相关信息来增强LLMs Lewis等人 ([2020](#bib.bib26))，*即*，RAG，在补充LLMs外部知识方面取得了成功。RAG在各种任务上表现良好，如语言建模
    Khandelwal等人 ([2019](#bib.bib22))；Shi等人 ([2023](#bib.bib38)) 和 QA Guu等人 ([2020](#bib.bib15))；Izacard和Grave
    ([2020](#bib.bib20))，并且计算成本显著降低 Borgeaud等人 ([2022](#bib.bib7))。与我们的工作相关但有所不同的是，最近的工作通过纠正
    Yan等人 ([2024](#bib.bib43))、批评 Asai等人 ([2023](#bib.bib4)) 或验证 Li等人 ([2023](#bib.bib27))
    来增强RAG，以提高知识密集型任务的检索质量。
- en: Long-context evaluation. Evaluating long-context models is challenging due to
    the difficulty in collecting and analyzing long texts. Recent researchers propose
    both synthetic tests like needle-in-a-haystack Greg Kamradt ([2023](#bib.bib13)),
    Ruler Hsieh et al. ([2024](#bib.bib17)), or Counting Stars Song et al. ([2024](#bib.bib39)),
    and real datasets including LongBench Bai et al. ([2023](#bib.bib5)), $\infty$Bench
    Zhang et al. ([2024](#bib.bib47)), L-Eval An et al. ([2023](#bib.bib2)), and others
    Shaham et al. ([2022](#bib.bib37)); Yuan et al. ([2024](#bib.bib45)); Maharana
    et al. ([2024](#bib.bib32)). Evaluating on these datasets, recent works study
    the performance degradation over various context lengths Levy et al. ([2024](#bib.bib25));
    Hsieh et al. ([2024](#bib.bib17)), the lost-in-the-middle phenomenon Liu et al.
    ([2024](#bib.bib29)), and explore solutions Kuratov et al. ([2024](#bib.bib24)).
    Related to our work, Xu et al. ([2023](#bib.bib42)) compare RAG and long-context
    prompting and find that long-context models still lags behind RAG. This is different
    from our findings, possibly due to consideration of stronger LLMs and longer contexts
    in our work.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本评估。由于收集和分析长文本的困难，评估长文本模型具有挑战性。最近的研究者提出了合成测试，如needle-in-a-haystack Greg Kamradt
    ([2023](#bib.bib13))、Ruler Hsieh等人 ([2024](#bib.bib17))，或者Counting Stars Song等人
    ([2024](#bib.bib39))，以及真实数据集，包括LongBench Bai等人 ([2023](#bib.bib5))，$\infty$Bench
    Zhang等人 ([2024](#bib.bib47))，L-Eval An等人 ([2023](#bib.bib2))，和其他 Shaham等人 ([2022](#bib.bib37))；Yuan等人
    ([2024](#bib.bib45))；Maharana等人 ([2024](#bib.bib32))。在这些数据集上进行评估的最新工作研究了各种上下文长度下的性能下降
    Levy等人 ([2024](#bib.bib25))；Hsieh等人 ([2024](#bib.bib17))，中间丢失现象 Liu等人 ([2024](#bib.bib29))，并探索了解决方案
    Kuratov等人 ([2024](#bib.bib24))。与我们的工作相关，Xu等人 ([2023](#bib.bib42)) 比较了RAG和长文本提示，发现长文本模型仍然落后于RAG。这与我们的发现不同，可能是因为我们工作中考虑了更强大的LLMs和更长的上下文。
- en: 3 Benchmarking RAG versus LC
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 RAG与长文本的基准测试
- en: 3.1 Datasets and metrics
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集和指标
- en: We evaluate on a subset of datasets from LongBench Bai et al. ([2023](#bib.bib5))
    and $\infty$Bench consists of even longer contexts with an average length of 100k
    tokens.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LongBench Bai等人 ([2023](#bib.bib5)) 和$\infty$Bench的子集上进行评估，这些数据集包含了更长的上下文，平均长度为100k
    tokens。
- en: Among the datasets, we mainly focus on tasks that are (a) in English, (b) real,
    and (c) query-based (*e.g*. summarization tasks do not contain queries for retrieving
    relevant information). This results in 7 datasets from LongBench including NarrativeQA
    Kočiskỳ et al. ([2018](#bib.bib23)), Qasper Dasigi et al. ([2021](#bib.bib11)),
    MultiFieldQA Bai et al. ([2023](#bib.bib5)), HotpotQA Yang et al. ([2018](#bib.bib44)),
    2WikiMultihopQA Ho et al. ([2020](#bib.bib16)), MuSiQue Trivedi et al. ([2022](#bib.bib40)),
    QMSum Zhong et al. ([2021](#bib.bib48)); and 2 datasets from $\infty$Bench.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，我们主要关注以下任务：（a）使用英语，（b）真实的，以及（c）基于查询的（*例如*，总结任务不包含用于检索相关信息的查询）。这导致了来自 LongBench
    的 7 个数据集，包括 NarrativeQA Kočiskỳ et al. ([2018](#bib.bib23))，Qasper Dasigi et al.
    ([2021](#bib.bib11))，MultiFieldQA Bai et al. ([2023](#bib.bib5))，HotpotQA Yang
    et al. ([2018](#bib.bib44))，2WikiMultihopQA Ho et al. ([2020](#bib.bib16))，MuSiQue
    Trivedi et al. ([2022](#bib.bib40))，QMSum Zhong et al. ([2021](#bib.bib48))；以及来自
    $\infty$Bench 的 2 个数据集。
- en: For evaluation metrics, we report F1 scores for the open-ended QA tasks, accuracy
    for the multi-choice QA tasks, and ROUGE score for the summarization tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估指标，我们报告开放式 QA 任务的 F1 分数，多选 QA 任务的准确率，以及总结任务的 ROUGE 分数。
- en: 3.2 Models and Retrievers
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型与检索器
- en: Three latest LLMs are evaluated, including Gemini-1.5-Pro Reid et al. ([2024](#bib.bib36)),
    GPT-4O OpenAI ([2024a](#bib.bib34)), and GPT-3.5-Turbo OpenAI ([2023](#bib.bib33))
    ²²2gpt-3.5-turbo-0125, gpt-4o-2024-05-13. Gemini-1.5-Pro is a recent long-context
    LLM from Google, supporting up to 1 million tokens. GPT-4O, the newest lightweight
    yet strong LLM from OpenAI, supports 128k tokens. GPT-3.5-Turbo supports 16k tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 评估了三个最新的 LLM，包括 Gemini-1.5-Pro Reid et al. ([2024](#bib.bib36))，GPT-4O OpenAI
    ([2024a](#bib.bib34))，以及 GPT-3.5-Turbo OpenAI ([2023](#bib.bib33)) ²²2gpt-3.5-turbo-0125，gpt-4o-2024-05-13。Gemini-1.5-Pro
    是 Google 最近推出的长上下文 LLM，支持最多 100 万个令牌。GPT-4O 是 OpenAI 最新的轻量级但强大的 LLM，支持 128k 令牌。GPT-3.5-Turbo
    支持 16k 令牌。
- en: 'Two retrievers are used in our study: Contriever Izacard et al. ([2021](#bib.bib19)),
    which is a contrastively trained dense retriever outperforming BM25 on BEIR datasets,
    and Dragon Lin et al. ([2023](#bib.bib28)), which is a recent generalizable dense
    retriever achieving high performance in both supervised and zero-shot settings
    without complex late interaction. Following Xu et al. ([2023](#bib.bib42)), we
    divide long contexts into chunks of 300 words, and select the top $k$) based on
    the cosine similarity of the query embedding and the chunk embeddings. The chunks
    are ordered by the similarity scores, with the chunk index prepended at the beginning.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究中使用了两个检索器：Contriever Izacard et al. ([2021](#bib.bib19))，这是一个对比训练的密集检索器，在
    BEIR 数据集上优于 BM25，以及 Dragon Lin et al. ([2023](#bib.bib28))，这是一个近期的通用密集检索器，在监督和零样本设置下均表现出色，无需复杂的晚期交互。遵循
    Xu et al. ([2023](#bib.bib42)) 的方法，我们将长上下文分成 300 字的块，并根据查询嵌入和块嵌入的余弦相似度选择前 $k$
    个。块按相似度得分排序，块索引位于前面。
- en: Since black-box LLMs are pretrained on unknown datasets, the leakage of evaluation
    datasets may occur. Especially, some of the evaluation datasets are based on Wikipedia,
    which has likely been seen by LLMs during during. In some cases, we find that
    model may predict the correct answer using exactly the same words as the groundtruth
    (*e.g*. “meticulously”), even when they do not appear in the provided context.
    In our experiment, we try mitigating this issue by prompting the model to answer
    ‘‘based only on the provided passage’’ for both RAG and LC. It remains an open
    question how to address the data leakage issue in LLM evaluation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于黑箱 LLM 是在未知数据集上预训练的，可能会出现评估数据集泄漏的问题。特别是，一些评估数据集基于 Wikipedia，这可能已经被 LLM 看到。在某些情况下，我们发现模型可能使用与真值完全相同的词语（*例如*，“精心”）预测正确答案，即使这些词语在提供的上下文中没有出现。在我们的实验中，我们尝试通过提示模型“仅基于提供的段落”来缓解这一问题，适用于
    RAG 和 LC。如何解决 LLM 评估中的数据泄漏问题仍然是一个未解之谜。
- en: 3.3 Benchmarking results
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基准测试结果
- en: 'We benchmark the performance of LC and RAG across the nine datasets, using
    three recent LLMs: Gemini-1.5-Pro, GPT-4O and GPT-3.5-Turbo. [Tab. 1](#S3.T1 "In
    3.3 Benchmarking results ‣ 3 Benchmarking RAG versus LC ‣ Retrieval Augmented
    Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach") presents
    the results using the Contriever retriever, where rows *-1 and rows *-2 present
    the benchmarking results for LC and RAG respectively. Results using the Dragon
    retriever will be discussed in [Sec. 5.3](#S5.SS3 "5.3 Different retrievers ‣
    5 Analysis ‣ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
    Study and Hybrid Approach") and [Tab. 2](#S5.T2 "In 5.1 Ablations of k ‣ 5 Analysis
    ‣ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and
    Hybrid Approach").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在九个数据集上对LC和RAG的性能进行了基准测试，使用了三种近期的LLM：Gemini-1.5-Pro、GPT-4O和GPT-3.5-Turbo。[表1](#S3.T1
    "在3.3基准测试结果 ‣ 3 RAG与LC的基准测试 ‣ 检索增强生成还是长上下文LLMs？综合研究和混合方法")展示了使用Contriever检索器的结果，其中行
    *-1 和行 *-2 分别展示了LC和RAG的基准测试结果。使用Dragon检索器的结果将在[第5.3节](#S5.SS3 "5.3 不同的检索器 ‣ 5
    分析 ‣ 检索增强生成还是长上下文LLMs？综合研究和混合方法")和[表2](#S5.T2 "在5.1 变量k的消融 ‣ 5 分析 ‣ 检索增强生成还是长上下文LLMs？综合研究和混合方法")中讨论。
- en: As shown in [Tab. 1](#S3.T1 "In 3.3 Benchmarking results ‣ 3 Benchmarking RAG
    versus LC ‣ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
    Study and Hybrid Approach"), LC consistently outperforms RAG for all the three
    models, with a significant margin. On average, LC surpasses RAG by 7.6% for Gemini-1.5-Pro,
    13.1% for GPT-4O, and 3.6% for GPT-3.5-Turbo. Noticeably, the performance gap
    is more significant for the more recent models (GPT-4O and Gemini-1.5-Pro) compared
    to GPT-3.5-Turbo, highlighting the exceptional long-context understanding capacity
    of the latest LLMs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表1](#S3.T1 "在3.3基准测试结果 ‣ 3 RAG与LC的基准测试 ‣ 检索增强生成还是长上下文LLMs？综合研究和混合方法")所示，LC在所有三种模型中均显著优于RAG，差距较大。平均而言，LC在Gemini-1.5-Pro上超越RAG
    7.6%，在GPT-4O上超越13.1%，在GPT-3.5-Turbo上超越3.6%。值得注意的是，相较于GPT-3.5-Turbo，较新的模型（GPT-4O和Gemini-1.5-Pro）之间的性能差距更为显著，这突显了最新LLMs在长上下文理解能力上的卓越表现。
- en: However, there is an exception observed on the two longer datasets from $\infty$Bench
    (*i.e*., En.QA and En.MC), where RAG achieves higher performance than LC for GPT-3.5-Turbo.
    This result deviates from the overall trend, likely due to the significantly longer
    context in these datasets (147k words on average) compared with the limited context
    window (16k) of GPT-3.5-Turbo. This finding highlights the effectiveness of RAG
    when the input text considerably exceeds the model’s context window size, emphasizing
    a specific use case of RAG.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在两个较长的数据集（*即*，En.QA和En.MC）中观察到一个例外，RAG在GPT-3.5-Turbo上的表现优于LC。这个结果偏离了整体趋势，可能是由于这些数据集的上下文显著长（平均147k词），而GPT-3.5-Turbo的上下文窗口较有限（16k）。这一发现突显了RAG在输入文本明显超过模型上下文窗口大小时的有效性，强调了RAG的一个特定使用案例。
- en: '|  |  |  | Avg | Narr | Qasp | Mult | Hotp | 2Wiki | Musi | Sum | En.QA | En.MC
    |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 平均 | Narr | Qasp | Mult | Hotp | 2Wiki | Musi | Sum | En.QA | En.MC
    |'
- en: '|  | 1-1 | LC | 49.70 | 32.76 | 47.83 | 52.33 | 61.85 | 62.96 | 40.22 | 20.73
    | 43.08 | 85.57 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 1-1 | LC | 49.70 | 32.76 | 47.83 | 52.33 | 61.85 | 62.96 | 40.22 | 20.73
    | 43.08 | 85.57 |'
- en: '| Gemini-1.5-Pro | 1-2 | RAG | 37.33 | 22.54 | 44.68 | 49.53 | 48.36 | 54.24
    | 26.56 | 19.51 | 19.46 | 51.09 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-Pro | 1-2 | RAG | 37.33 | 22.54 | 44.68 | 49.53 | 48.36 | 54.24
    | 26.56 | 19.51 | 19.46 | 51.09 |'
- en: '| 1-3 | Self-Route | 46.41 | 28.32 | 45.23 | 51.47 | 55.18 | 62.68 | 40.66
    | 19.77 | 37.51 | 76.86 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 1-3 | Self-Route | 46.41 | 28.32 | 45.23 | 51.47 | 55.18 | 62.68 | 40.66
    | 19.77 | 37.51 | 76.86 |'
- en: '| 1-4 | answerable % | 76.78 | 73.00 | 85.00 | 96.67 | 84.50 | 81.00 | 58.50
    | 93.50 | 56.41 | 62.45 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 1-4 | answerable % | 76.78 | 73.00 | 85.00 | 96.67 | 84.50 | 81.00 | 58.50
    | 93.50 | 56.41 | 62.45 |'
- en: '| 1-5 | token % | 38.39 | 23.07 | 49.93 | 36.88 | 32.97 | 53.49 | 56.14 | 17.96
    | 42.25 | 32.84 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 1-5 | token % | 38.39 | 23.07 | 49.93 | 36.88 | 32.97 | 53.49 | 56.14 | 17.96
    | 42.25 | 32.84 |'
- en: '| GPT-4O | 2-1 | LC | 48.67 | 32.78 | 44.54 | 55.28 | 62.42 | 70.69 | 41.65
    | 21.92 | 32.36 | 76.42 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4O | 2-1 | LC | 48.67 | 32.78 | 44.54 | 55.28 | 62.42 | 70.69 | 41.65
    | 21.92 | 32.36 | 76.42 |'
- en: '| 2-2 | RAG | 32.60 | 18.05 | 46.02 | 50.74 | 36.86 | 50.21 | 16.09 | 19.97
    | 14.43 | 41.05 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 2-2 | RAG | 32.60 | 18.05 | 46.02 | 50.74 | 36.86 | 50.21 | 16.09 | 19.97
    | 14.43 | 41.05 |'
- en: '| 2-3 | Self-Route | 48.89 | 31.36 | 47.99 | 53.17 | 62.14 | 70.14 | 41.69
    | 21.31 | 34.95 | 77.29 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 2-3 | Self-Route | 48.89 | 31.36 | 47.99 | 53.17 | 62.14 | 70.14 | 41.69
    | 21.31 | 34.95 | 77.29 |'
- en: '| 2-4 | answerable % | 57.36 | 44.00 | 67.50 | 94.00 | 52.50 | 62.00 | 30.00
    | 92.00 | 27.07 | 47.16 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 2-4 | answerable % | 57.36 | 44.00 | 67.50 | 94.00 | 52.50 | 62.00 | 30.00
    | 92.00 | 27.07 | 47.16 |'
- en: '| 2-5 | token % | 61.40 | 66.40 | 72.25 | 39.65 | 65.79 | 77.05 | 85.00 | 20.26
    | 73.01 | 53.21 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 2-5 | token % | 61.40 | 66.40 | 72.25 | 39.65 | 65.79 | 77.05 | 85.00 | 20.26
    | 73.01 | 53.21 |'
- en: '| GPT-3.5-Turbo | 3-1 | LC | 32.07 | 23.34 | 42.96 | 49.19 | 45.33 | 41.04
    | 17.92 | 19.61 | 14.73 | 34.50 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 3-1 | LC | 32.07 | 23.34 | 42.96 | 49.19 | 45.33 | 41.04
    | 17.92 | 19.61 | 14.73 | 34.50 |'
- en: '| 3-2 | RAG | 30.33 | 18.22 | 38.15 | 49.21 | 37.84 | 35.16 | 16.41 | 18.94
    | 15.39 | 43.67 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 3-2 | RAG | 30.33 | 18.22 | 38.15 | 49.21 | 37.84 | 35.16 | 16.41 | 18.94
    | 15.39 | 43.67 |'
- en: '| 3-3 | Self-Route | 35.32 | 24.06 | 38.65 | 52.07 | 47.28 | 44.62 | 34.44
    | 19.88 | 22.03 | 44.54 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3-3 | 自我路线 | 35.32 | 24.06 | 38.65 | 52.07 | 47.28 | 44.62 | 34.44 | 19.88
    | 22.03 | 44.54 |'
- en: '| 3-4 | answerable % | 74.10 | 71.50 | 80.00 | 91.33 | 68.50 | 69.00 | 47.00
    | 93.50 | 50.43 | 95.63 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 3-4 | 可回答百分比 | 74.10 | 71.50 | 80.00 | 91.33 | 68.50 | 69.00 | 47.00 | 93.50
    | 50.43 | 95.63 |'
- en: '| 3-5 | token % | 38.85 | 20.56 | 55.08 | 35.29 | 48.70 | 65.91 | 65.08 | 16.40
    | 38.17 | 4.50 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3-5 | token % | 38.85 | 20.56 | 55.08 | 35.29 | 48.70 | 65.91 | 65.08 | 16.40
    | 38.17 | 4.50 |'
- en: 'Table 1: Results of Gemini-1.5-Pro, GPT-3.5-Turbo, and GPT-4O using the Contriever
    retriever. LC consistently outperforms RAG, while Self-Route achieves performance
    comparable to LC using much less tokens.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用 Contriever 检索器的 Gemini-1.5-Pro、GPT-3.5-Turbo 和 GPT-4O 的结果。LC 一贯优于 RAG，而自我路线在使用更少的
    token 的情况下实现了与 LC 相当的性能。
- en: 4 Self-Route
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 自我路线
- en: 4.1 Motivation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 动机
- en: As demonstrated in [Sec. 3](#S3 "3 Benchmarking RAG versus LC ‣ Retrieval Augmented
    Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach"),
    RAG lags behind long-context LLMs in terms of performance. However, despite this
    performance gap, we surprisingly find a high degree of overlap in their predictions,
    as illustrated in [Fig. 2](#S4.F2 "In 4.1 Motivation ‣ 4 Self-Route ‣ Retrieval
    Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach").
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 3 节](#S3 "3 基准测试 RAG 与 LC ‣ 检索增强生成或长上下文 LLMs？综合研究和混合方法") 所示，RAG 在性能上落后于长上下文
    LLMs。然而，尽管存在这一性能差距，我们却惊讶地发现它们的预测高度重叠，如 [图 2](#S4.F2 "在 4.1 动机 ‣ 4 自我路线 ‣ 检索增强生成或长上下文
    LLMs？综合研究和混合方法") 所示。
- en: '![Refer to caption](img/04b0f7672bfc78d2f44b8f0148b9c4c6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04b0f7672bfc78d2f44b8f0148b9c4c6.png)'
- en: 'Figure 2: Distribution of the difference of prediction scores between RAG and
    LC (computed w.r.t. groundtruth labels). RAG and LC predictions are highly identical,
    for both correct and incorrect ones.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：RAG 和 LC 之间预测分数差异的分布（相对于真实标签计算）。RAG 和 LC 的预测高度一致，无论是正确的还是错误的。
- en: '[Fig. 2](#S4.F2 "In 4.1 Motivation ‣ 4 Self-Route ‣ Retrieval Augmented Generation
    or Long-Context LLMs? A Comprehensive Study and Hybrid Approach") displays the
    distribution of the differences between RAG prediction scores $S_{RAG}$. This
    observation suggests that RAG and LC tend to make not only the same correct predictions
    but also similar errors.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](#S4.F2 "在 4.1 动机 ‣ 4 自我路线 ‣ 检索增强生成或长上下文 LLMs？综合研究和混合方法") 显示了 RAG 预测分数
    $S_{RAG}$ 差异的分布。这一观察表明 RAG 和 LC 不仅倾向于做出相同的正确预测，还会出现类似的错误。'
- en: This finding motivates us to leverage RAG for the majority of queries, reserving
    computationally more expensive LC for a small subset of queries where it truly
    excels. By doing so, RAG can significantly reduce computational costs without
    sacrificing overall performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现激励我们在大多数查询中利用 RAG，将计算成本更高的 LC 留给少量真正擅长的查询。通过这样做，RAG 可以显著降低计算成本，而不会牺牲整体性能。
- en: 4.2 Self-Route
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 自我路线
- en: Based on the above motivation, we propose Self-Route, a simple yet effective
    method combining RAG and LC to reduce cost while maintaining a performance comparable
    to LC. Self-Route utilizes LLM itself to route queries based on self-reflection,
    under the assumption that LLMs are well-calibrated in predicting whether a query
    is answerable given provided context.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述动机，我们提出了自我路线，这是一种简单而有效的方法，结合了 RAG 和 LC，以降低成本，同时保持与 LC 相当的性能。自我路线利用 LLM 自身根据自我反思来路由查询，假设
    LLM 在预测给定上下文下查询是否可回答方面已得到良好校准。
- en: 'Concretely, our method consists of two steps: a RAG-and-Route step and a long-context
    prediction step. In the first step, we provide the query and the retrieved chunks
    to the LLM, and prompt it to predict whether the query is answerable and, if so,
    generate the answer. This is similar to standard RAG, with one key difference:
    the LLM is given the option to decline answering with the prompt ‘‘Write unanswerable
    if the query can not be answered based on the provided text’’. For the queries
    deemed answerable, we accept the RAG prediction as the final answer. For the queries
    deemed unanswerable, we proceed to the second step, providing the full context
    to the long-context LLMs to obtain the final prediction (*i.e*., LC).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们的方法包括两个步骤：RAG-and-Route步骤和长上下文预测步骤。在第一步中，我们将查询和检索到的片段提供给LLM，并提示其预测查询是否可回答，如果可以，则生成答案。这与标准的RAG类似，唯一的区别是LLM有机会拒绝回答，提示为“如果无法根据提供的文本回答，请写‘无法回答’”。对于被认为可以回答的查询，我们接受RAG的预测作为最终答案。对于被认为无法回答的查询，我们进入第二步，将完整的上下文提供给长上下文LLMs以获取最终预测（*即*，LC）。
- en: As our results will demonstrate, most queries can be solved by the first RAG-and-Route
    step (*e.g*., 82% for Gemini-1.5-Pro), with only a small portion requiring the
    following long-context prediction step. Since the RAG-and-Route step only needs
    the retrieved chunks (*e.g*., 1.5k tokens) as input, which is significantly shorter
    than the full contexts (*e.g*., 10k - 100k tokens), the overall computation cost
    is substantially reduced. Detailed token count analysis will be provided in the
    results.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们的结果所示，大多数查询可以通过第一个RAG-and-Route步骤解决（*例如*，Gemini-1.5-Pro为82%），只有一小部分需要接下来的长上下文预测步骤。由于RAG-and-Route步骤仅需要检索到的片段（*例如*，1.5k
    tokens）作为输入，这比完整上下文（*例如*，10k - 100k tokens）要短得多，因此总体计算成本大大降低。详细的token计数分析将在结果中提供。
- en: 4.3 Results
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 结果
- en: Rows *-3 to *-5 in [Tab. 1](#S3.T1 "In 3.3 Benchmarking results ‣ 3 Benchmarking
    RAG versus LC ‣ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
    Study and Hybrid Approach") present the results of our method, utilizing the three
    LLMs. Rows *-3 report the performance. Rows *-4 show the percentage of answerable
    queries, as predicted in the RAG-and-Route step. Rows *-5 display the percentage
    of tokens used by our method, compared to that of LC. In terms of performance
    (rows *-3), Self-Route significantly outperforms RAG, achieving results comparable
    to LC. Across all three models, Self-Route surpasses RAG (rows *-2) by over 5%.
    Compared to LC (rows *-1), there is a slight performance drop for GPT-4O (-0.2%)
    and Gemini-1.5-Pro (-2.2%), but an improvement for GPT-3.5-Turbo (+1.7%).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S3.T1 "在3.3基准测试结果 ‣ 3 RAG与LC的基准测试 ‣ 检索增强生成与长上下文LLMs？综合研究与混合方法")中的 *-3
    到 *-5 行展示了我们的方法结果，利用了三种LLM。* -3 行报告了性能。* -4 行显示了RAG-and-Route步骤中预测的可回答查询的百分比。*
    -5 行展示了我们方法使用的tokens的百分比，与LC的比较。在性能方面（* -3 行），Self-Route显著优于RAG，取得了与LC相当的结果。在所有三种模型中，Self-Route比RAG（*
    -2 行）高出超过5%。与LC（* -1 行）相比，GPT-4O（-0.2%）和Gemini-1.5-Pro（-2.2%）有轻微的性能下降，但GPT-3.5-Turbo（+1.7%）有所改善。'
- en: All three LLMs consistently route more than half of queries towards RAG, as
    shown in rows *-4\. For Gemini-1.5-Pro, the answerable percentage even reaches
    81.74% (row 1-4). This indicates that RAG may answer most queries without the
    need for LC, confirming our initial motivation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种LLM一致地将超过一半的查询路由至RAG，如* -4 行所示。对于Gemini-1.5-Pro，可回答的百分比甚至达到81.74%（* 1-4
    行）。这表明RAG可能在不需要LC的情况下回答大多数查询，从而确认了我们最初的动机。
- en: Due to the high answerable rate, the number of tokens required is significantly
    reduced (rows *-5). For example, GPT-4O uses only 61% tokens while achieving comparable
    performance (46.83) with LC (47.04), Gemini-1.5-Pro uses 38.6% of the tokens.
    Since the computation cost of the transformer-based LLMs is quadratic to token
    count, and most LLM APIs charge based on token count OpenAI ([2024b](#bib.bib35));
    Google ([2024](#bib.bib12)), this lower token count translates to substantial
    cost savings.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高可回答率，所需tokens的数量显著减少（* -5 行）。例如，GPT-4O在达到与LC（47.04）相当的性能（46.83）的同时仅使用了61%的tokens，Gemini-1.5-Pro使用了38.6%的tokens。由于基于变压器的LLMs的计算成本与token数量的平方成正比，并且大多数LLM
    API的收费是基于token数量的（OpenAI ([2024b](#bib.bib35))；Google ([2024](#bib.bib12))），因此较少的token数量意味着显著的成本节省。
- en: On longer datasets, the advantage of our method is more pronounced for OpenAI
    models, but less significant for Gemini. For instance, for GPT-4O, Self-Route
    outperforms LC by 2.3% and 7.4% respectively on EN.QA and EN.MC, which contain
    longer contexts. For GPT-3.5-Turbo, the advantage margins are even larger. However,
    for Gemini-1.5-Pro, the performance is lower than LC. These different behaviors
    are possibly due to the difference in LLM alignments, *i.e*., OpenAI models are
    more likely to reject answering using RAG, leading to a lower answerable percentage
    but higher accuracy, which results in a different performance-cost trade-off compared
    with Gemini-1.5-Pro.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在较长的数据集上，我们的方法在OpenAI模型中优势更为明显，但在Gemini中不太显著。例如，对于GPT-4O，Self-Route在EN.QA和EN.MC上分别超越LC
    2.3%和7.4%，这两个数据集包含了更长的上下文。对于GPT-3.5-Turbo，优势差距甚至更大。然而，对于Gemini-1.5-Pro，其性能低于LC。这些不同的表现可能是由于LLM对齐的差异，即OpenAI模型更倾向于拒绝使用RAG回答，从而导致较低的可回答百分比但更高的准确性，这造成了与Gemini-1.5-Pro不同的性能成本权衡。
- en: 5 Analysis
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 分析
- en: 5.1 Ablations of k
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 k的消融实验
- en: Both RAG and Self-Route relies on the top-$k$s are used.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: RAG和Self-Route都依赖于使用的top-$k$。
- en: '![Refer to caption](img/7b126edde812d51e4dbcf3d4ed9dc628.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7b126edde812d51e4dbcf3d4ed9dc628.png)'
- en: 'Figure 3: Trade-off curves between (a) model performance and (b) token percentage
    as a function of $k$.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：模型性能和令牌百分比作为$k$的函数之间的权衡曲线。
- en: In terms of performance, for both RAG and Self-Route, a larger $k$ is larger
    than 50, all three methods get similar performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，对于RAG和Self-Route，当$k$大于50时，三种方法的性能相似。
- en: However, the trend of cost is not monotonous for Self-Route. As seen, the cost
    reaches its minimum at $k=5$s when applying our method to various applications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Self-Route的成本趋势并不是单调的。如所见，当我们将我们的方法应用于各种应用时，成本在$k=5$时达到了最低点。
- en: '|  |  |  | Avg | Narr | Qasp | Mult | Hotp | 2Wiki | Musi | Sum | En.QA | En.MC
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 平均 | Narr | Qasp | Mult | Hotp | 2Wiki | Musi | 总计 | En.QA | En.MC
    |'
- en: '|  | 1 | LC | 49.70 | 32.76 | 47.83 | 52.33 | 61.85 | 62.96 | 40.22 | 20.73
    | 43.08 | 85.57 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 1 | LC | 49.70 | 32.76 | 47.83 | 52.33 | 61.85 | 62.96 | 40.22 | 20.73
    | 43.08 | 85.57 |'
- en: '| Dragon | 2 | RAG | 38.09 | 21.91 | 44.33 | 53.08 | 51.61 | 50.05 | 30.47
    | 19.93 | 21.25 | 50.22 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Dragon | 2 | RAG | 38.09 | 21.91 | 44.33 | 53.08 | 51.61 | 50.05 | 30.47
    | 19.93 | 21.25 | 50.22 |'
- en: '| 3 | combine | 46.81 | 28.50 | 43.82 | 54.62 | 56.58 | 60.62 | 40.66 | 20.07
    | 37.79 | 78.60 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 组合 | 46.81 | 28.50 | 43.82 | 54.62 | 56.58 | 60.62 | 40.66 | 20.07 |
    37.79 | 78.60 |'
- en: '| 4 | RAG ratio | 77.88 | 74.00 | 84.00 | 97.33 | 86.00 | 77.00 | 66.00 | 95.50
    | 61.25 | 59.83 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 4 | RAG比例 | 77.88 | 74.00 | 84.00 | 97.33 | 86.00 | 77.00 | 66.00 | 95.50
    | 61.25 | 59.83 |'
- en: '| 5 | Token ratio | 37.87 | 19.31 | 54.15 | 34.78 | 32.64 | 55.65 | 48.16 |
    16.64 | 38.71 | 40.83 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 令牌比例 | 37.87 | 19.31 | 54.15 | 34.78 | 32.64 | 55.65 | 48.16 | 16.64
    | 38.71 | 40.83 |'
- en: 'Table 2: Results for Gemini-1.5-Pro using Dragon retriever.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用Dragon检索器的Gemini-1.5-Pro结果。
- en: 5.2 Why does RAG fail?
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 为什么RAG会失败？
- en: To gain a better understanding of why RAG lags behind LC, we analyze the failure
    reasons for the examples that cannot be answered by RAG. We first manually check
    some examples for which our RAG-and-Route step predicts “unanswerable” and summarize
    four typical failure reasons, then prompt LLM to classify all the examples.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解为什么RAG落后于LC，我们分析了RAG无法回答的示例的失败原因。我们首先手动检查了一些RAG和Route步骤预测为“无法回答”的示例，并总结了四种典型的失败原因，然后提示LLM对所有示例进行分类。
- en: 'The four reasons include: (A) The query requires multi-step reasoning so the
    results of previous steps are needed to retrieve information for later steps,
    *e.g*. ‘‘What nationality is the performer of song XXX’’. (B) The query is general,
    *e.g*. ‘‘What does the group think about XXX’’, which is challenging for the retriever
    to formulate a good query. (C) The query is long and complex, which is challenging
    for the retriever to understand. However, answering this kind of questions is
    arguably, an advantage of LLMs. (D) The query is implicit, demanding a thorough
    understanding of the entire context. For instance, in a lengthy conversational
    narrative about a space voyage, a question like ‘‘What caused the shadow behind
    the spaceship?’’ requires readers to connect the dots and deduce the answer, as
    there is no explicit mention of the shadow when the cause is revealed.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 四种原因包括：(A) 查询需要多步骤推理，因此前面的步骤结果需要用于检索后续步骤的信息，例如“XXX歌曲的演唱者的国籍是什么”。(B) 查询是一般性的，例如“这个小组对XXX的看法是什么”，这对检索器提出了良好查询的挑战。(C)
    查询长且复杂，这对检索器的理解提出了挑战。然而，回答这种问题可以说是LLMs的一个优势。(D) 查询是隐含的，需要对整个上下文有透彻理解。例如，在关于太空旅行的冗长对话叙述中，像“是什么导致了飞船后面的阴影？”这样的问法需要读者连接点滴并推断答案，因为在揭示原因时没有明确提到阴影。
- en: '![Refer to caption](img/15b5fffd3eedab5af1a3a14840dc9571.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/15b5fffd3eedab5af1a3a14840dc9571.png)'
- en: 'Figure 4: Distribution of typical RAG failure reasons.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：典型RAG失败原因的分布。
- en: Using these reasons, we prompt Gemini-1.5-Pro with few-shot in-context examples
    that we manually annotated, to classify all the unanswerable examples into these
    four categories, plus an “other" option. [Fig. 4](#S5.F4 "In 5.2 Why does RAG
    fail? ‣ 5 Analysis ‣ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
    Study and Hybrid Approach") shows the distribution of failure reasons on the seven
    datasets in LongBench. Each dataset may contain different number of RAG failure
    cases, resulting in various bar heights. The distribution patterns are consistent
    with the nature of the datasets. For example, the three Wikipedia-based multi-hop
    reasoning datasets (HotpotQA, 2WikiMQA, MuSiQue) are challenging for RAG because
    of multi-step retrieval as shown in blue. For NarrativeQA, which are long stories
    containing a lot of dialogues, most failure cases are due to implicit queries
    that requires understanding the whole context (shown in green). For QMSum, which
    is a summarization dataset contains open-ended questions, failures are mostly
    due to general queries (shown in red). We manually checked the examples classified
    as “others” and find that most of them are actually multi-step questions, often
    with ambiguities, which poses challenges for answering.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些原因，我们用手动标注的少量示例来提示 Gemini-1.5-Pro，将所有无法回答的示例分类到这四个类别中，并增加一个“其他”选项。[图4](#S5.F4
    "在5.2中RAG为什么失败？ ‣ 5分析 ‣ 检索增强生成或长上下文LLMs？全面研究与混合方法") 显示了 LongBench 中七个数据集的失败原因分布。每个数据集可能包含不同数量的RAG失败案例，导致各种条形图高度。分布模式与数据集的性质一致。例如，三个基于维基百科的多跳推理数据集（HotpotQA、2WikiMQA、MuSiQue）由于多步骤检索而对RAG构成挑战，如蓝色所示。对于
    NarrativeQA，这些长篇故事包含大量对话，大多数失败案例是由于隐含查询需要理解整个上下文（如绿色所示）。对于 QMSum，这是一种包含开放性问题的总结数据集，失败主要是由于一般性查询（如红色所示）。我们手动检查了被分类为“其他”的示例，发现它们中的大多数实际上是多步骤问题，通常有歧义，这对回答提出了挑战。
- en: We hope this failure analysis inspires future improvements of RAG. For example,
    engaging chain-of-thought Wei et al. ([2022](#bib.bib41)) into RAG may help address
    the multi-step questions, and revisiting query understanding techniques like query
    expansion Lv and Zhai ([2009](#bib.bib30)); Zhai and Lafferty ([2001](#bib.bib46))
    may help with the general queries and complex queries. We are also glad to see
    recent efforts towards the direction Chan et al. ([2024](#bib.bib8)); Ma et al.
    ([2023](#bib.bib31)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这次失败分析能激发RAG的未来改进。例如，将链式推理 Wei 等人 ([2022](#bib.bib41)) 引入 RAG 可能有助于解决多步骤问题，而重新审视像查询扩展
    Lv 和 Zhai ([2009](#bib.bib30)); Zhai 和 Lafferty ([2001](#bib.bib46)) 这样的查询理解技术，可能对一般性查询和复杂查询有所帮助。我们也很高兴看到近期的努力方向
    Chan 等人 ([2024](#bib.bib8)); Ma 等人 ([2023](#bib.bib31))。
- en: 5.3 Different retrievers
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 不同的检索器
- en: The results using a retriever, Dragon, is shown in [Tab. 2](#S5.T2 "In 5.1 Ablations
    of k ‣ 5 Analysis ‣ Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive
    Study and Hybrid Approach") based on Gemini-1.5-Pro. As can be seen, the results
    are consistent with Contriever, for all of LC, RAG, and Self-Route, showing that
    our findings are generalizable across retrievers.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用检索器Dragon的结果显示在[Tab. 2](#S5.T2 "In 5.1 Ablations of k ‣ 5 Analysis ‣ Retrieval
    Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach")中，基于Gemini-1.5-Pro。可以看到，这些结果与Contriever一致，无论是LC、RAG还是Self-Route，都表明我们的发现具有跨检索器的普遍性。
- en: 5.4 Results on synthetic data
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 合成数据的结果
- en: In this study, we mainly focus on real datasets, with a consideration that results
    on synthetic data, which are artificially created by researchers, may subject
    to dataset artifacts. We notice some methods that researchers adopted to create
    synthetic long context datasets may unconsciously, but largely, influence the
    performance comparison between RAG and LC. For example, here we describe the results
    on the “PassKey” dataset in $\infty$Bench and its variations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们主要关注真实数据集，并考虑到由研究人员人工创建的合成数据可能会受到数据集伪影的影响。我们注意到，研究人员创建合成长上下文数据集时采用的一些方法，可能会无意中但大幅度地影响RAG与LC之间的性能比较。例如，在这里我们描述了“PassKey”数据集在$\infty$Bench及其变体上的结果。
- en: 'This “PassKey” dataset presents a needle-in-a-haystack test, where a sentence
    with a passkey (*e.g*. ‘‘the passkey is 123456’’) is hidden within chunks of irrelevant
    text, and the model is asked to answer the question ‘‘What is the passkey’’. The
    task requires strong retrieval capability. On this dataset, RAG achieves 80.34%
    accuracy, outperforming LC, which gets 65.25% using Gemini-1.5-Pro. However, if
    the query is slightly modified as ‘‘What is the special token hidden inside the
    texts’’, RAG accuracy sharply drops to only 4.58%, while LC keeps roughly the
    same (69.32%). Another example: if the chunks contain two passkeys and the query
    is ‘‘Which passkey is larger? First or second?’’, then RAG (47.63%) under-performs
    LC (64.24%) as well. These examples demonstrate that the evaluation results highly
    subjects to artifacts in dataset construction, showing limitation of synthetic
    testing.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“PassKey”数据集呈现了一项找针测试，其中包含一个密码句子（*例如* “the passkey is 123456”）隐藏在无关的文本块中，模型需要回答“密码是什么”。这个任务要求强大的检索能力。在这个数据集上，RAG的准确率为80.34%，优于LC（65.25%），使用Gemini-1.5-Pro。然而，如果查询稍作修改为“文本中隐藏的特殊标记是什么”，RAG的准确率急剧下降至仅4.58%，而LC保持大致相同（69.32%）。另一个例子是：如果文本块中包含两个密码，查询是“哪个密码更大？第一个还是第二个？”，则RAG（47.63%）也不如LC（64.24%）。这些例子表明，评估结果高度受数据集构建中的伪影影响，显示了合成测试的局限性。
- en: 6 conclusion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper presents a comprehensive comparison of RAG and LC, highlighting the
    trade-offs between performance and computational cost. While LC demonstrate superior
    performance in long-context understanding, RAG remains a viable option due to
    its lower cost and advantages when the input considerably exceeds the model’s
    context window size. Our proposed method, which dynamically routes queries based
    on model self-reflection, effectively combines the strengths of both RAG and LC,
    achieving comparable performance to LC at a significantly reduced cost. We believe
    our findings contribute valuable insights for the practical application of long-context
    LLMs and pave the way for future research in optimizing RAG techniques.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对RAG和LC进行了全面的比较，突出了性能与计算成本之间的权衡。虽然LC在长上下文理解上表现出色，但由于其成本较低以及在输入远超模型上下文窗口大小时的优势，RAG仍然是一个可行的选择。我们提出的方法，通过基于模型自我反思动态路由查询，有效结合了RAG和LC的优势，在显著降低成本的情况下，达到与LC相当的性能。我们相信，我们的发现为长上下文LLMs的实际应用提供了有价值的见解，并为未来优化RAG技术的研究铺平了道路。
- en: References
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, 等人 2023。Gpt-4技术报告。*arXiv预印本 arXiv:2303.08774*。
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, 和 Xipeng Qiu. 2023. L-eval: 为长上下文语言模型建立标准化评估。*arXiv 预印本 arXiv:2307.11088*。'
- en: Anthropic (2024) Anthropic. 2024. Claude 3.5 sonnet. [https://www.anthropic.com/news/claude-3-5-sonnet/](https://www.anthropic.com/news/claude-3-5-sonnet/).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024) Anthropic. 2024. Claude 3.5 sonnet. [https://www.anthropic.com/news/claude-3-5-sonnet/](https://www.anthropic.com/news/claude-3-5-sonnet/)。
- en: 'Asai et al. (2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh
    Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through
    self-reflection. *arXiv preprint arXiv:2310.11511*.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Asai et al. (2023) Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, 和 Hannaneh
    Hajishirzi. 2023. Self-rag: 通过自我反思学习检索、生成和评论。*arXiv 预印本 arXiv:2310.11511*。'
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, 等. 2023. Longbench:
    一个双语、多任务基准用于长上下文理解。*arXiv 预印本 arXiv:2308.14508*。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, 和 Arman Cohan. 2020. Longformer:
    长文档变换器。*arXiv 预印本 arXiv:2004.05150*。'
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by
    retrieving from trillions of tokens. In *International conference on machine learning*,
    pages 2206–2240\. PMLR.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, 等. 2022. 通过从万亿个标记中检索来改进语言模型。见于 *国际机器学习会议*，第2206–2240页。PMLR。
- en: 'Chan et al. (2024) Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue,
    Yike Guo, and Jie Fu. 2024. Rq-rag: Learning to refine queries for retrieval augmented
    generation. *arXiv preprint arXiv:2404.00610*.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan et al. (2024) Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue,
    Yike Guo, 和 Jie Fu. 2024. Rq-rag: 学习为检索增强生成优化查询。*arXiv 预印本 arXiv:2404.00610*。'
- en: 'Chen et al. (2023a) Lingjiao Chen, Matei Zaharia, and James Zou. 2023a. Frugalgpt:
    How to use large language models while reducing cost and improving performance.
    *arXiv preprint arXiv:2305.05176*.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2023a) Lingjiao Chen, Matei Zaharia, 和 James Zou. 2023a. Frugalgpt:
    如何在降低成本和提高性能的同时使用大型语言模型。*arXiv 预印本 arXiv:2305.05176*。'
- en: Chen et al. (2023b) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023b. Extending context window of large language models via positional
    interpolation. *arXiv preprint arXiv:2306.15595*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023b) Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong
    Tian. 2023b. 通过位置插值扩展大型语言模型的上下文窗口。*arXiv 预印本 arXiv:2306.15595*。
- en: Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A
    Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and
    answers anchored in research papers. *arXiv preprint arXiv:2105.03011*.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah
    A Smith, 和 Matt Gardner. 2021. 一个基于研究论文的信息寻求问题和答案的数据集。*arXiv 预印本 arXiv:2105.03011*。
- en: Google (2024) Google. 2024. Gemini pricing. [https://ai.google.dev/pricing](https://ai.google.dev/pricing).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2024) Google. 2024. Gemini pricing. [https://ai.google.dev/pricing](https://ai.google.dev/pricing)。
- en: Greg Kamradt (2023) Greg Kamradt. 2023. Needle in a haystack - pressure testing
    llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greg Kamradt (2023) Greg Kamradt. 2023. 针尖上的稻草堆 - 压力测试 LLMs。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。
- en: 'Guo et al. (2022) Mandy Guo, Joshua Ainslie, David C Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. Longt5: Efficient text-to-text
    transformer for long sequences. In *Findings of the Association for Computational
    Linguistics: NAACL 2022*, pages 724–736.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. (2022) Mandy Guo, Joshua Ainslie, David C Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, 和 Yinfei Yang. 2022. Longt5: 高效的长序列文本到文本转换器。见于 *计算语言学协会发现:
    NAACL 2022*，第724–736页。'
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pages 3929–3938\. PMLR.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu 等（2020）Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei Chang.
    2020. 检索增强的语言模型预训练。发表于 *国际机器学习会议*，第 3929–3938 页。PMLR。
- en: Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.
    2020. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning
    steps. *arXiv preprint arXiv:2011.01060*.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等（2020）Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, 和 Akiko Aizawa. 2020.
    构建多跳 QA 数据集以全面评估推理步骤。*arXiv 预印本 arXiv:2011.01060*。
- en: 'Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. Ruler: What’s the real context
    size of your long-context language models? *arXiv preprint arXiv:2404.06654*.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等（2024）Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, 和 Boris Ginsburg. 2024. Ruler：你的长上下文语言模型的实际上下文大小是多少？*arXiv
    预印本 arXiv:2404.06654*。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    2023. Distilling step-by-step! outperforming larger language models with less
    training data and smaller model sizes. *arXiv preprint arXiv:2305.02301*.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh 等（2023）Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa
    Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister. 2023. 步骤化提炼！以较少的训练数据和更小的模型规模超越更大的语言模型。*arXiv
    预印本 arXiv:2305.02301*。
- en: Izacard et al. (2021) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised
    dense information retrieval with contrastive learning. *arXiv preprint arXiv:2112.09118*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 等（2021）Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel,
    Piotr Bojanowski, Armand Joulin, 和 Edouard Grave. 2021. 使用对比学习进行无监督密集信息检索。*arXiv
    预印本 arXiv:2112.09118*。
- en: Izacard and Grave (2020) Gautier Izacard and Edouard Grave. 2020. Leveraging
    passage retrieval with generative models for open domain question answering. *arXiv
    preprint arXiv:2007.01282*.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard 和 Grave（2020）Gautier Izacard 和 Edouard Grave. 2020. 利用生成模型与段落检索进行开放领域问答。*arXiv
    预印本 arXiv:2007.01282*。
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew
    Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing
    llms in long context scenarios via prompt compression. *arXiv preprint arXiv:2310.06839*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew
    Lin, Yuqing Yang, 和 Lili Qiu. 2023. Longllmlingua：通过提示压缩加速和增强长上下文场景中的大型语言模型。*arXiv
    预印本 arXiv:2310.06839*。
- en: 'Khandelwal et al. (2019) Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
    Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest
    neighbor language models. *arXiv preprint arXiv:1911.00172*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khandelwal 等（2019）Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer,
    和 Mike Lewis. 2019. 通过记忆进行泛化：最近邻语言模型。*arXiv 预印本 arXiv:1911.00172*。
- en: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa
    reading comprehension challenge. *Transactions of the Association for Computational
    Linguistics*, 6:317–328.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kočiskỳ 等（2018）Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl
    Moritz Hermann, Gábor Melis, 和 Edward Grefenstette. 2018. NarrativeQA 阅读理解挑战。*计算语言学协会会刊*，6:317–328。
- en: 'Kuratov et al. (2024) Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin,
    Artyom Sorokin, and Mikhail Burtsev. 2024. In search of needles in a 10m haystack:
    Recurrent memory finds what llms miss. *arXiv preprint arXiv:2402.10790*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuratov 等（2024）Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom
    Sorokin, 和 Mikhail Burtsev. 2024. 在 10 米干草堆中寻找针：递归记忆发现了大型语言模型遗漏的内容。*arXiv 预印本
    arXiv:2402.10790*。
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task,
    more tokens: the impact of input length on the reasoning performance of large
    language models. *arXiv preprint arXiv:2402.14848*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy 等（2024）Mosh Levy, Alon Jacoby, 和 Yoav Goldberg. 2024. 相同任务，更多标记：输入长度对大型语言模型推理性能的影响。*arXiv
    预印本 arXiv:2402.14848*。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    等. 2020. 知识密集型自然语言处理任务的检索增强生成。*神经信息处理系统进展*，33:9459–9474。
- en: 'Li et al. (2023) Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang
    Sun, and Xipeng Qiu. 2023. Llatrieval: Llm-verified retrieval for verifiable generation.
    *arXiv preprint arXiv:2311.07838*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li等人（2023）Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun,
    和Xipeng Qiu. 2023. Llatrieval: 用于可验证生成的LLM验证检索。*arXiv预印本 arXiv:2311.07838*。'
- en: 'Lin et al. (2023) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy
    Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. How to train your dragon:
    Diverse augmentation towards generalizable dense retrieval. *arXiv preprint arXiv:2302.07452*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人（2023）Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin,
    Yashar Mehdad, Wen-tau Yih, 和Xilun Chen. 2023. 如何训练你的龙：多样的增强方法以实现可推广的密集检索。*arXiv预印本
    arXiv:2302.07452*。
- en: 'Liu et al. (2024) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language
    models use long contexts. *Transactions of the Association for Computational Linguistics*,
    12:157–173.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2024）Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, 和Percy Liang. 2024. 在中间迷失：语言模型如何使用长上下文。*计算语言学协会会刊*，12:157–173。
- en: Lv and Zhai (2009) Yuanhua Lv and ChengXiang Zhai. 2009. Adaptive relevance
    feedback in information retrieval. In *Proceedings of the 18th ACM conference
    on Information and knowledge management*, pages 255–264.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv和Zhai（2009）Yuanhua Lv 和 ChengXiang Zhai. 2009. 信息检索中的自适应相关反馈。收录于*第18届ACM信息与知识管理大会论文集*，第255–264页。
- en: Ma et al. (2023) Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan.
    2023. Query rewriting for retrieval-augmented large language models. *arXiv preprint
    arXiv:2305.14283*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等人（2023）Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, 和Nan Duan. 2023. 针对检索增强的大型语言模型的查询重写。*arXiv预印本
    arXiv:2305.14283*。
- en: Maharana et al. (2024) Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit
    Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational
    memory of llm agents. *arXiv preprint arXiv:2402.17753*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maharana等人（2024）Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal,
    Francesco Barbieri, 和Yuwei Fang. 2024. 评估大型语言模型代理的超长期对话记忆。*arXiv预印本 arXiv:2402.17753*。
- en: OpenAI (2023) OpenAI. 2023. Gpt-3.5-turbo. [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 2023. GPT-3.5-turbo。 [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)。
- en: OpenAI (2024a) OpenAI. 2024a. Gpt-4o. [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024a）OpenAI. 2024a. GPT-4O。 [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)。
- en: OpenAI (2024b) OpenAI. 2024b. Openai-api pricing. [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024b）OpenAI. 2024b. OpenAI API定价。 [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)。
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid等人（2024）Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, 等人. 2024. Gemini 1.5: 解锁跨百万令牌上下文的多模态理解。*arXiv预印本
    arXiv:2403.05530*。'
- en: 'Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran,
    Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022.
    Scrolls: Standardized comparison over long language sequences. *arXiv preprint
    arXiv:2201.03533*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham等人（2022）Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi
    Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, 等人. 2022. Scrolls:
    长语言序列的标准化比较。*arXiv预印本 arXiv:2201.03533*。'
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi等人（2023）Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James,
    Mike Lewis, Luke Zettlemoyer, 和Wen-tau Yih. 2023. Replug: 检索增强的黑箱语言模型。*arXiv预印本
    arXiv:2301.12652*。'
- en: 'Song et al. (2024) Mingyang Song, Mao Zheng, and Xuan Luo. 2024. Counting-stars:
    A simple, efficient, and reasonable strategy for evaluating long-context large
    language models. *arXiv preprint arXiv:2403.11802*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song等人（2024）Mingyang Song, Mao Zheng, 和Xuan Luo. 2024. Counting-stars: 一种简单、高效且合理的长上下文大语言模型评估策略。*arXiv预印本
    arXiv:2403.11802*。'
- en: 'Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
    and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question
    composition. *Transactions of the Association for Computational Linguistics*,
    10:539–554.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Trivedi 等 (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot 和 Ashish
    Sabharwal. 2022. Musique: 通过单跳问题组合的多跳问题。*计算语言学协会会刊*，10:539–554。'
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia,
    Ed Chi, Quoc V Le, Denny Zhou 等. 2022. 思维链提示激发大型语言模型的推理能力。*神经信息处理系统进展*，35:24824–24837。
- en: Xu et al. (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. 2023. Retrieval meets long context large language models. *arXiv preprint
    arXiv:2310.03025*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan
    Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi 和 Bryan Catanzaro.
    2023. 检索与长上下文大型语言模型的结合。*arXiv 预印本 arXiv:2310.03025*。
- en: Yan et al. (2024) Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.
    Corrective retrieval augmented generation. *arXiv preprint arXiv:2401.15884*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等 (2024) Shi-Qi Yan, Jia-Chen Gu, Yun Zhu 和 Zhen-Hua Ling. 2024. 纠错检索增强生成。*arXiv
    预印本 arXiv:2401.15884*。
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset
    for diverse, explainable multi-hop question answering. *arXiv preprint arXiv:1809.09600*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    W Cohen, Ruslan Salakhutdinov 和 Christopher D Manning. 2018. Hotpotqa: 一个用于多跳问题回答的多样化、可解释数据集。*arXiv
    预印本 arXiv:1809.09600*。'
- en: 'Yuan et al. (2024) Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li,
    Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, et al. 2024. Lv-eval:
    A balanced long-context benchmark with 5 length levels up to 256k. *arXiv preprint
    arXiv:2402.05136*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等 (2024) Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui
    Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li 等. 2024. Lv-eval: 一个平衡的长上下文基准，具有5个长度级别，最长256k。*arXiv
    预印本 arXiv:2402.05136*。'
- en: Zhai and Lafferty (2001) Chengxiang Zhai and John Lafferty. 2001. Model-based
    feedback in the language modeling approach to information retrieval. In *Proceedings
    of the tenth international conference on Information and knowledge management*,
    pages 403–410.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 和 Lafferty (2001) Chengxiang Zhai 和 John Lafferty. 2001. 基于模型的反馈在语言建模的信息检索方法中的应用。见于
    *第十届国际信息与知识管理会议论文集*，第403–410页。
- en: 'Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. 2024.
    Infinity bench: Extending long context evaluation beyond 100k tokens. *arXiv preprint
    arXiv:2402.13718*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu 等. 2024. Infinity
    bench: 扩展长上下文评估到100k tokens以上。*arXiv 预印本 arXiv:2402.13718*。'
- en: 'Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma,
    Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al.
    2021. Qmsum: A new benchmark for query-based multi-domain meeting summarization.
    *arXiv preprint arXiv:2104.05938*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等 (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul
    Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu 等. 2021. Qmsum:
    一项针对基于查询的多领域会议摘要的新基准。*arXiv 预印本 arXiv:2104.05938*。'
- en: Appendix A Dataset details
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集详情
- en: We evaluate on 7 datasets from LongBench Bai et al. ([2023](#bib.bib5)). NarrativeQA
    Kočiskỳ et al. ([2018](#bib.bib23)) is a question answering dataset, where the
    context is a long story like a novel or a movie script. Qasper Dasigi et al. ([2021](#bib.bib11))
    focuses on question answering over academic NLP papers and is annotated by NLP
    practitioners. MultiFieldQA, originally proposed in LongBench, contains human-annotated
    QA over documents and articles from multiple sources, including legal documents,
    government reports, encyclopedias, academic papers, etc. HotpotQA Yang et al.
    ([2018](#bib.bib44)) contains two-hop questions written by native English speakers
    that requires reasoning over two related Wikipedia paragraphs in the long context.
    2WikiMultihopQA Ho et al. ([2020](#bib.bib16)) contains up to 5-hop questions
    that are synthesized through manually designed templates, ensuring that they cannot
    be solved through shortcuts. The questions in MuSiQue Trivedi et al. ([2022](#bib.bib40))
    are up to 4-hop, first constructed from single-hop question compositions, and
    then paraphrased by annotators for linguistic diversity. QMSum Zhong et al. ([2021](#bib.bib48))
    is a query-based summarization dataset over meeting scripts from multiple domains.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 LongBench Bai et al. ([2023](#bib.bib5)) 的 7 个数据集上进行评估。NarrativeQA Kočiskỳ
    et al. ([2018](#bib.bib23)) 是一个问答数据集，背景是一篇长篇故事，如小说或电影剧本。Qasper Dasigi et al. ([2021](#bib.bib11))
    专注于学术 NLP 论文的问答，由 NLP 从业者标注。MultiFieldQA 最初由 LongBench 提出，包含对来自多个来源的文档和文章的人类标注问答，包括法律文件、政府报告、百科全书、学术论文等。HotpotQA
    Yang et al. ([2018](#bib.bib44)) 包含由母语为英语的讲者编写的两步问题，需要在长上下文中对两个相关的维基百科段落进行推理。2WikiMultihopQA
    Ho et al. ([2020](#bib.bib16)) 包含最多 5 步的问题，这些问题通过手动设计的模板合成，确保无法通过捷径解决。MuSiQue
    Trivedi et al. ([2022](#bib.bib40)) 的问题最多为 4 步，首先由单步问题组成，然后由标注者进行语言多样性的改写。QMSum
    Zhong et al. ([2021](#bib.bib48)) 是一个基于查询的会议记录摘要数据集，涵盖多个领域。
- en: We evaluate on 2 datasets from $\infty$Bench Zhang et al. ([2024](#bib.bib47)).
    En.QA contains human-annotated question-answer pairs for long novels, with key
    entity names manually replaced in order to avoid knowledge leakage due to model
    pretraining. EN.MC is annotated similarly to En.QA, but differs in that the model
    is presented with four challenging answer choices written by the annotators.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 $\infty$Bench Zhang et al. ([2024](#bib.bib47)) 的 2 个数据集上进行评估。En.QA 包含对长篇小说的人类标注问答对，为避免模型预训练导致的知识泄露，关键实体名称被手动替换。EN.MC
    的标注方式与 En.QA 类似，但不同之处在于模型提供了四个由标注者撰写的具有挑战性的答案选项。
- en: '[Tab. 3](#A1.T3 "In Appendix A Dataset details ‣ Retrieval Augmented Generation
    or Long-Context LLMs? A Comprehensive Study and Hybrid Approach") shows the details
    of the datasets, including the number of queries in each evaluation dataset and
    the average context length (*i.e*. number of words).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3](#A1.T3 "附录 A 数据集详情 ‣ 检索增强生成或长上下文 LLM？综合研究与混合方法") 显示了数据集的详细信息，包括每个评估数据集中的查询数量和平均上下文长度（*即*
    词数）。'
- en: '|  |  | Num. Query | Avg. Length |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 查询数量 | 平均长度 |'
- en: '| LongBench Bai et al. ([2023](#bib.bib5)) | NarrativeQA | 200 | 18,395 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| LongBench Bai et al. ([2023](#bib.bib5)) | NarrativeQA | 200 | 18,395 |'
- en: '| Qasper | 200 | 3,599 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 200 | 3,599 |'
- en: '| MultiFieldQA | 150 | 4,539 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 150 | 4,539 |'
- en: '| HotpotQA | 200 | 9,133 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 200 | 9,133 |'
- en: '| 2WikiMultihopQA | 200 | 4,873 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMultihopQA | 200 | 4,873 |'
- en: '| MuSiQue | 200 | 11,196 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| MuSiQue | 200 | 11,196 |'
- en: '| QMSum | 200 | 10,533 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 200 | 10,533 |'
- en: '| $\infty$Bench Zhang et al. ([2024](#bib.bib47)) | En.QA | 351 | 150,374 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| $\infty$Bench Zhang et al. ([2024](#bib.bib47)) | En.QA | 351 | 150,374 |'
- en: '| En.MC | 229 | 142,622 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| En.MC | 229 | 142,622 |'
- en: 'Table 3: Dataset statistics.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：数据集统计信息。
- en: Appendix B Ablations of k
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B k 的消融实验
- en: '[Tab. 4](#A2.T4 "In Appendix B Ablations of k ‣ Retrieval Augmented Generation
    or Long-Context LLMs? A Comprehensive Study and Hybrid Approach") shows the performance
    and token ratio for different $k$, which corresponds to [Fig. 3](#S5.F3 "In 5.1
    Ablations of k ‣ 5 Analysis ‣ Retrieval Augmented Generation or Long-Context LLMs?
    A Comprehensive Study and Hybrid Approach"). The performance of LC, which serves
    as an upper bound, is 45.53\. The token ratio is computed the token counts for
    RAG or Self-Route divided the number of tokens required by LC.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 4](#A2.T4 "附录 B k 的消融实验 ‣ 检索增强生成或长上下文 LLM？综合研究与混合方法") 显示了不同 $k$ 的性能和令牌比率，对应于
    [图 3](#S5.F3 "5.1 k 的消融实验 ‣ 5 分析 ‣ 检索增强生成或长上下文 LLM？综合研究与混合方法")。LC 的性能，作为上限，为 45.53。令牌比率是通过
    RAG 或 Self-Route 的令牌计数除以 LC 所需的令牌数量计算的。'
- en: '|  | Performance | token ratio |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 性能 | 令牌比率 |'
- en: '| --- | --- | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| top-k | RAG | Self-Route | RAG | Self-Route |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| top-k | RAG | Self-Route | RAG | Self-Route |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 20.24 | 41.35 | 5.26 | 39.64 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 20.24 | 41.35 | 5.26 | 39.64 |'
- en: '| 5 | 37.92 | 43.33 | 17.02 | 38.63 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 37.92 | 43.33 | 17.02 | 38.63 |'
- en: '| 10 | 41.20 | 44.38 | 42.42 | 53.66 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 41.20 | 44.38 | 42.42 | 53.66 |'
- en: '| 50 | 44.06 | 45.19 | 95.29 | 102.97 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 44.06 | 45.19 | 95.29 | 102.97 |'
- en: '| 100 | 44.12 | 45.23 | 100.32 | 106.59 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 44.12 | 45.23 | 100.32 | 106.59 |'
- en: 'Table 4: Performance and token ratio for different $k$. This table corresponds
    to [Fig. 3](#S5.F3 "In 5.1 Ablations of k ‣ 5 Analysis ‣ Retrieval Augmented Generation
    or Long-Context LLMs? A Comprehensive Study and Hybrid Approach").'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同 $k$ 的性能和标记比例。该表对应于[图 3](#S5.F3 "在 5.1 k 的消融 ‣ 5 分析 ‣ 检索增强生成还是长上下文 LLM？全面研究和混合方法")。
- en: Appendix C Prompts
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示
- en: '[Tab. 5](#A3.T5 "In Appendix C Prompts ‣ Retrieval Augmented Generation or
    Long-Context LLMs? A Comprehensive Study and Hybrid Approach") shows the prompts
    for each dataset in our study. The prompts are modified from the released prompts
    as in LongBench Bai et al. ([2023](#bib.bib5)) and $\infty$Bench Zhang et al.
    ([2024](#bib.bib47)). [Tab. 6](#A3.T6 "In Appendix C Prompts ‣ Retrieval Augmented
    Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach") shows
    the prompts used in the failure case study as in [Sec. 5.2](#S5.SS2 "5.2 Why does
    RAG fail? ‣ 5 Analysis ‣ Retrieval Augmented Generation or Long-Context LLMs?
    A Comprehensive Study and Hybrid Approach").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5](#A3.T5 "在附录 C 提示 ‣ 检索增强生成还是长上下文 LLM？全面研究和混合方法")展示了我们研究中每个数据集的提示。这些提示修改自发布的
    LongBench Bai et al. ([2023](#bib.bib5)) 和 $\infty$Bench Zhang et al. ([2024](#bib.bib47))
    的提示。[表 6](#A3.T6 "在附录 C 提示 ‣ 检索增强生成还是长上下文 LLM？全面研究和混合方法")展示了在失败案例研究中使用的提示，如[第
    5.2 节](#S5.SS2 "5.2 为什么 RAG 会失败？ ‣ 5 分析 ‣ 检索增强生成还是长上下文 LLM？全面研究和混合方法")所示。'
- en: '| NarrativeQA | You are given a story, which can be either a novel or a movie
    script, and a question. Answer the question as concisely as you can, using a single
    phrase if possible. Do not provide any explanation. If the question cannot be
    answered based on the information in the article, write “unanswerable”. Story:
    {context} Now, answer the question based on the story as concisely as you can,
    using a single phrase if possible. Do not provide any explanation. If the question
    cannot be answered based on the information in the article, write “unanswerable”.
    Question: {input} Answer: |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 给定一个故事，无论是小说还是电影剧本，以及一个问题。尽可能简洁地回答问题，使用一个短语。如果问题无法根据文章中的信息回答，请写“无法回答”。故事：{context}
    现在，根据故事尽可能简洁地回答问题，使用一个短语。如果问题无法根据文章中的信息回答，请写“无法回答”。问题：{input} 答案： |'
- en: '| --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Qasper | You are given a scientific article and a question. Answer the question
    as concisely as you can, using a single phrase or sentence if possible. If the
    question cannot be answered based on the information in the article, write “unanswerable”.
    If the question is a yes/no question, answer “yes”, “no”, or “unanswerable”. Do
    not provide any explanation. Article: {context} Answer the question based on the
    above article as concisely as you can, using a single phrase or sentence if possible.
    If the question cannot be answered based on the information in the article, write
    “unanswerable”. If the question is a yes/no question, answer “yes”, “no”, or “unanswerable”.
    Do not provide any explanation. Question: input Answer: |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 给定一篇科学文章和一个问题。尽可能简洁地回答问题，使用一个短语或句子。如果问题无法根据文章中的信息回答，请写“无法回答”。如果问题是是/否问题，请回答“是”、“否”或“无法回答”。不要提供任何解释。文章：{context}
    根据上述文章尽可能简洁地回答问题，使用一个短语或句子。如果问题无法根据文章中的信息回答，请写“无法回答”。如果问题是是/否问题，请回答“是”、“否”或“无法回答”。不要提供任何解释。问题：input
    答案： |'
- en: '| MultiFQA | Read the following text and answer briefly. {context} Now, answer
    the following question based on the above text, only give me the answer and do
    not output any other words. If the question cannot be answered based on the information
    in the article, write “unanswerable”. Question: {input} Answer: |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| MultiFQA | 阅读以下文本并简要回答。{context} 现在，根据上述文本回答以下问题，只给出答案，不输出其他词汇。如果问题无法根据文章中的信息回答，请写“无法回答”。
    问题：{input} 答案： |'
- en: '| HotpotQA | Answer the question based on the given passages. Only give me
    the answer and do not output any other words. If the question cannot be answered
    based on the information in the article, write “unanswerable”. The following are
    given passages. {context} Answer the question based on the given passages. Only
    give me the answer and do not output any other words. If the question cannot be
    answered based on the information in the article, write “unanswerable”. Question:
    {input} Answer: |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 根据给定的段落回答问题。仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。以下是给定的段落。{context}
    根据给定的段落回答问题。仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。 问题：{input} 答案： |'
- en: '| 2WikiMQA | Answer the question based on the given passages. Only give me
    the answer and do not output any other words. If the question cannot be answered
    based on the information in the article, write “unanswerable”. The following are
    given passages. {context} Answer the question based on the given passages. Only
    give me the answer and do not output any other words. If the question cannot be
    answered based on the information in the article, write “unanswerable”. Question:
    {input} Answer: |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 根据给定的段落回答问题。仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。以下是给定的段落。{context}
    根据给定的段落回答问题。仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。 问题：{input} 答案： |'
- en: '| MuSiQue | Answer the question based on the given passages. Only give me the
    answer and do not output any other words. If the question cannot be answered based
    on the information in the article, write “unanswerable”. The following are given
    passages. {context} Answer the question based on the given passages. Only give
    me the answer and do not output any other words. If the question cannot be answered
    based on the information in the article, write “unanswerable”. Question: {input}
    Answer: |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| MuSiQue | 根据给定的段落回答问题。仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。以下是给定的段落。{context}
    根据给定的段落回答问题。仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。 问题：{input} 答案： |'
- en: '| QMSum | You are given a meeting transcript and a query containing a question
    or instruction. Answer the query in one or more sentences. If the question cannot
    be answered based on the information in the article, write “unanswerable”. Transcript:
    {context} Now, answer the query based on the above meeting transcript in one or
    more sentences. If the question cannot be answered based on the information in
    the article, write “unanswerable”. Query: {input} Answer: |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 你会得到一份会议记录和一个包含问题或指令的查询。用一句或多句回答查询。如果问题无法根据文章中的信息回答，请写“无法回答”。记录：{context}
    现在，根据上述会议记录用一句或多句回答查询。如果问题无法根据文章中的信息回答，请写“无法回答”。 查询：{input} 答案： |'
- en: '| EN.QA | Read the book and answer the question. Be very concise in your answer.
    If the question cannot be answered based on the information in the article, write
    “unanswerable”. {context} Question: {input} Only give me the answer and do not
    output any other words. If the question cannot be answered based on the information
    in the article, write “unanswerable”. Answer: |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| EN.QA | 阅读书籍并回答问题。请简明扼要地作答。如果问题无法根据文章中的信息回答，请写“无法回答”。{context} 问题：{input}
    仅给出答案，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。 答案： |'
- en: '| EN.MC | Read the book and answer the question. If the question cannot be
    answered based on the information in the article, write “unanswerable”. {context}
    Question: {input} {all_classes} Only output the letter of the correct answer and
    do not output any other words. If the question cannot be answered based on the
    information in the article, write “unanswerable”. The letter of the correct answer
    is |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| EN.MC | 阅读书籍并回答问题。如果问题无法根据文章中的信息回答，请写“无法回答”。{context} 问题：{input} {all_classes}
    仅输出正确答案的字母，不要输出其他任何文字。如果问题无法根据文章中的信息回答，请写“无法回答”。 正确答案的字母是 |'
- en: 'Table 5: Prompts for each dataset.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：每个数据集的提示。
- en: '| You are given some text chunks from an article, and a question.
    The text chunks are retrieved by an external retriever. Now: (1) Tell whether
    the question can be answered based only on the provided text chunks. (2) If the
    question can be answered, answer the question based on the texts as concisely
    as you can, using a single phrase if possible. (3) If the question cannot be answered,
    choose the reason from the following: A. The question needs multistep reasoning,
    thus it is hard to retrieve all the relevant chunks. For example, "What nationality
    is the performer of song You Can?" contains two steps: find the performer, then
    find the nationality of the performer. Other examples include "Where does the
    director of film Wine Of Morning work at?", "What is another notable work made
    by the author of Miss Sara Sampson?" B. The question is a general query, thus
    it is hard to retrieve relevant chunks. For example, "What did the group think
    about Dave leaving?" is general because the group may include multiple persons,
    and they can have different thinkings. C. The question is long and complex, which
    is hard for the retriever to encode it to retrieve relevant chunks. For example,
    "What did Julie Morgan elaborate on the online survey when talking about the evaluations
    on the legitimacy of the children’s rights, protection and demands?", "The Huskies
    football team were invited to the Alamo Bowl where they were defeated by a team
    coached by Art Briles and who played their home games at what stadium?" D. The
    question is not explicit and requires comprehensive understanding of the whole
    story and cannot be solved using retrieval-augmented generation. For example,
    "What caused the shadow behind Koerber’s ship?" needs a comprehensive understanding
    of the whole story. Another example like "How many words are there in the article"
    also requires the complete article. E. Others. Keep the above reasons in mind,
    and choose the most possible reason if you think the question cannot be answered
    based on the text. Output the results in JSON format. {in_context_examples} Text:
    {context} Question: {input} Answer: |'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '| You are given some text chunks from an article, and a question.
    The text chunks are retrieved by an external retriever. Now: (1) Tell whether
    the question can be answered based only on the provided text chunks. (2) If the
    question can be answered, answer the question based on the texts as concisely
    as you can, using a single phrase if possible. (3) If the question cannot be answered,
    choose the reason from the following: A. The question needs multistep reasoning,
    thus it is hard to retrieve all the relevant chunks. For example, "What nationality
    is the performer of song You Can?" contains two steps: find the performer, then
    find the nationality of the performer. Other examples include "Where does the
    director of film Wine Of Morning work at?", "What is another notable work made
    by the author of Miss Sara Sampson?" B. The question is a general query, thus
    it is hard to retrieve relevant chunks. For example, "What did the group think
    about Dave leaving?" is general because the group may include multiple persons,
    and they can have different thinkings. C. The question is long and complex, which
    is hard for the retriever to encode it to retrieve relevant chunks. For example,
    "What did Julie Morgan elaborate on the online survey when talking about the evaluations
    on the legitimacy of the children’s rights, protection and demands?", "The Huskies
    football team were invited to the Alamo Bowl where they were defeated by a team
    coached by Art Briles and who played their home games at what stadium?" D. The
    question is not explicit and requires comprehensive understanding of the whole
    story and cannot be solved using retrieval-augmented generation. For example,
    "What caused the shadow behind Koerber’s ship?" needs a comprehensive understanding
    of the whole story. Another example like "How many words are there in the article"
    also requires the complete article. E. Others. Keep the above reasons in mind,
    and choose the most possible reason if you think the question cannot be answered
    based on the text. Output the results in JSON format. {in_context_examples} Text:
    {context} Question: {input} Answer: |'
- en: 'Table 6: Prompt for the failure case analysis.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：失败案例分析的提示。
