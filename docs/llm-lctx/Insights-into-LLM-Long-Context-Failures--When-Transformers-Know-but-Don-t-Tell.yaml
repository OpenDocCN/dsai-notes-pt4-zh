- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:02:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:02:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Insights into LLM Long-Context Failures: When Transformers Know but Don’t Tell'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对LLM长上下文失败的见解：当变换器知道但不说
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14673](https://ar5iv.labs.arxiv.org/html/2406.14673)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14673](https://ar5iv.labs.arxiv.org/html/2406.14673)
- en: Taiming Lu^✰       Muhan Gao^✰       Kuai Yu^✰
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Taiming Lu^✰       Muhan Gao^✰       Kuai Yu^✰
- en: Adam Byerly       Daniel Khashabi
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Adam Byerly       Daniel Khashabi
- en: Johns Hopkins University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰斯·霍普金斯大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) exhibit positional bias, struggling to utilize
    information from the middle or end of long contexts. Our study explores LLMs’
    long-context reasoning by probing their hidden representations. We find that while
    LLMs encode the position of target information, they often fail to leverage this
    in generating accurate responses. This reveals a disconnect between information
    retrieval and utilization, a ‘know but don’t tell’ phenomenon. We further analyze
    the relationship between extraction time and final accuracy, offering insights
    into the underlying mechanics of transformer models. The code is accessible here:
    [https://github.com/TaiMingLu/know-dont-tell](https://github.com/TaiMingLu/know-dont-tell).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）表现出位置偏差，难以利用长上下文中的中间或结尾信息。我们的研究通过探测其隐藏表示来探索LLMs的长上下文推理。我们发现虽然LLMs对目标信息的位置进行编码，但它们在生成准确回答时常常未能有效利用这些信息。这揭示了信息检索与利用之间的断层，即‘知道但不说’现象。我们进一步分析了提取时间与最终准确性的关系，提供了对变换器模型基本机制的见解。代码可在这里访问：[https://github.com/TaiMingLu/know-dont-tell](https://github.com/TaiMingLu/know-dont-tell)。
- en: 'Insights into LLM Long-Context Failures:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM长上下文失败的见解：
- en: When Transformers Know but Don’t Tell
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当变换器知道但不说
- en: Taiming Lu^✰       Muhan Gao^✰       Kuai Yu^✰        Adam Byerly       Daniel
    Khashabi        Johns Hopkins University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Taiming Lu^✰       Muhan Gao^✰       Kuai Yu^✰        Adam Byerly       Daniel
    Khashabi        约翰斯·霍普金斯大学
- en: '⁰⁰footnotetext: ✰ indicates equal contributions. ⁰⁰footnotetext:     Emails:
    {tlu37, mgao38, kyu25}@jhu.edu'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '⁰⁰footnotetext: ✰ 表示贡献相等。⁰⁰footnotetext:     邮箱：{tlu37, mgao38, kyu25}@jhu.edu'
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advent of Large Language Models (LLMs), optimized with advanced transformer
    architectures, has delivered marked improvement in language processing capabilities.
    These models excel at simultaneously processing extended contexts Ding et al.
    ([2024](#bib.bib6)); Chen et al. ([2023](#bib.bib5)), significantly benefiting
    various downstream tasks like long-text question answering, summarization, and
    inference  Wang et al. ([2024](#bib.bib21)); Zhang et al. ([2024a](#bib.bib23));
    Shaham et al. ([2022](#bib.bib17), [2023](#bib.bib16)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现，经过先进的变换器架构优化，显著提升了语言处理能力。这些模型在同时处理扩展上下文方面表现出色 Ding et al. ([2024](#bib.bib6));
    Chen et al. ([2023](#bib.bib5))，对各种下游任务如长文本问答、总结和推理大有裨益 Wang et al. ([2024](#bib.bib21));
    Zhang et al. ([2024a](#bib.bib23)); Shaham et al. ([2022](#bib.bib17), [2023](#bib.bib16))。
- en: 'Despite their advanced capabilities, LLMs often struggle to utilize long inputs
    fully. This tendency, known as positional bias, leads LLMs to disproportionately
    prioritize information at the beginning or end of the input sequence Wang et al.
    ([2023](#bib.bib22)) while crucial details in the middle are frequently overlooked Liu
    et al. ([2023b](#bib.bib13)). Numerous strategies have been proposed to address
    these biases Tang et al. ([2024](#bib.bib18)); Li et al. ([2023](#bib.bib11));
    Zhang et al. ([2024b](#bib.bib24)), yet the underlying causes and potential solutions
    remain unclear. This underscores the need for a deeper investigation into how
    LLMs handle long-context integration. To fully assess the capabilities of LLMs
    in handling extended contexts, it is not enough to merely evaluate their final
    performance: some important information is hidden in models’ representations.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具备先进的能力，LLMs往往难以充分利用长输入。这种倾向被称为位置偏差，导致LLMs过度优先考虑输入序列开头或结尾的信息 Wang et al. ([2023](#bib.bib22))，而中间的重要细节则常被忽视
    Liu et al. ([2023b](#bib.bib13))。为了解决这些偏差，已提出许多策略 Tang et al. ([2024](#bib.bib18));
    Li et al. ([2023](#bib.bib11)); Zhang et al. ([2024b](#bib.bib24))，但其根本原因和潜在解决方案仍不明确。这突显了对LLMs如何处理长上下文整合进行更深入调查的必要性。要全面评估LLMs处理扩展上下文的能力，仅仅评估其最终表现是不够的：一些重要信息隐藏在模型的表示中。
- en: '![Refer to caption](img/24bd5dc9435506942f6f1dd109137873.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/24bd5dc9435506942f6f1dd109137873.png)'
- en: 'Figure 1: Following prompts by Liu et al. ([2023b](#bib.bib13)), for each transformer
    layer, we train a probing classifier to probe the model’s ability to identify
    useful information. The peak accuracy among layers indicates the effectiveness
    of the model’s processing of context.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：根据Liu等人（[2023b](#bib.bib13)）的提示，我们为每个transformer层训练了一个探测分类器，以探测模型识别有用信息的能力。各层之间的峰值准确度指示了模型处理上下文的效果。
- en: 'In this work, we present a probing analysis of LLMs long-context generalization.
    Specifically, we build probes based on the internal representation of LLMs for
    various layers and positions to measure the accuracy of reconstructing the position
    they correspond to (see [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Insights
    into LLM Long-Context Failures: When Transformers Know but Don’t Tell")). A necessary
    condition for effective long-context processing by LLMs is their ability to encode
    positional information in their intermediate representations.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们对LLMs的长上下文泛化进行了探测分析。具体来说，我们基于LLMs的内部表示构建探测器，以测量其重建相应位置的准确性（见[图1](#S1.F1
    "图1 ‣ 1 引言 ‣ 对LLMs长上下文失败的见解：当Transformers知道却不说")）。LLMs有效处理长上下文的必要条件是它们能够在其中间表示中编码位置相关信息。
- en: We conduct experiments on two tasks from Liu et al. ([2023b](#bib.bib13)) and
    three recent open-source models. Our findings reveal a gap between the accuracy
    of LLMs’ predictions and the probes on their representations. Notably, while LLMs
    can accurately identify the position of crucial information within the context,
    they often fail to utilize this information effectively in their responses, leading
    to what we term the ‘know but don’t tell’ phenomenon. To our knowledge, this is
    the first work to use probing analysis to highlight this observation. We hope
    that our work on distinguishing “knowing” and “telling” motivates future work
    on tackling the long-context challenges of LLMs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Liu等人的两个任务（[2023b](#bib.bib13)）和三个最新开源模型上进行了实验。我们的发现揭示了LLMs预测的准确性与其表示的探测器之间的差距。值得注意的是，虽然LLMs可以准确识别上下文中关键信息的位置，但它们通常未能有效利用这些信息进行回答，导致了我们所称的‘知道却不说’现象。据我们所知，这是首次使用探测分析来突出这一观察结果。我们希望我们的工作能够激励未来对LLMs长上下文挑战的进一步研究。
- en: 'In summary, our contributions are as follows: (1) Probing analysis: We introduce
    a novel framework to investigate the long-context reasoning capabilities of LLMs.
    This framework allows us to measure how accurately LLMs encode positional information
    across various layers and positions within their intermediate representations.
    (2) Empirical evaluation: We conduct comprehensive experiments using tasks from
    Liu et al. ([2023b](#bib.bib13)) and three recent open-source models. Our empirical
    evaluation provides new insights into the positional biases of LLMs and their
    impact on model performance. (3) ‘Know but Don’t Tell’ phenomenon: Our analysis
    reveals a critical gap between LLMs’ ability to encode and utilize positional
    information. We identify the "know but don’t tell" phenomenon, where LLMs accurately
    identify the position of crucial information but fail to leverage this knowledge
    in generating accurate responses.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献如下：（1）探测分析：我们引入了一个新框架来研究LLMs的长上下文推理能力。这个框架使我们能够测量LLMs在其中间表示的各层和位置上如何准确地编码位置相关信息。（2）实证评估：我们使用Liu等人的任务（[2023b](#bib.bib13)）和三个最新的开源模型进行全面实验。我们的实证评估提供了关于LLMs位置偏差及其对模型性能影响的新见解。（3）‘知道却不说’现象：我们的分析揭示了LLMs编码和利用位置相关信息之间的关键差距。我们识别了“知道却不说”现象，即LLMs准确识别关键信息的位置，但在生成准确回答时未能有效利用这些知识。
- en: We believe these contributions provide a significant step towards understanding
    and improving the long-context processing capabilities of LLMs. By distinguishing
    between the encoding and utilization of positional information, our work lays
    the foundation for future advancements in LLM performance and reliability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信这些贡献为理解和改进LLMs的长上下文处理能力迈出了重要一步。通过区分位置相关信息的编码和利用，我们的工作为未来LLMs性能和可靠性的进步奠定了基础。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Positional bias.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 位置偏差。
- en: LLMs exhibit a positional bias, where their performance is influenced by the
    location of crucial context information Zhao et al. ([2021](#bib.bib25)). One
    prominent example is the “lost in the middle” phenomenon, where comprehension
    declines for information in the center of a long context Liu et al. ([2023b](#bib.bib13)).
    Additionally, recency bias is observed, particularly in few-shot learning scenarios,
    where models tend to favor information near the end of the prompt Zhao et al.
    ([2021](#bib.bib25)). Such biases could stem from the positioning of key data
    in pre-training sets, which often places important elements near critical points
    Peysakhovich and Lerer ([2023](#bib.bib15)). Our work delves into this phenomenon
    by examining the underlying mechanisms within the transformer layers of LLMs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs表现出位置偏倚，其性能受到关键信息位置的影响（Zhao 等人，[2021](#bib.bib25)）。一个显著的例子是“中间丢失”现象，即对长上下文中心信息的理解下降（Liu
    等人，[2023b](#bib.bib13)）。此外，还观察到近期偏倚，特别是在少量样本学习场景中，模型倾向于偏爱提示末尾的信息（Zhao 等人，[2021](#bib.bib25)）。这种偏倚可能源于预训练集中的关键数据定位，通常将重要元素放置在关键点附近（Peysakhovich
    和 Lerer，[2023](#bib.bib15)）。我们的工作通过检查LLMs的变换器层中的机制来深入研究这一现象。
- en: Probing.
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探测。
- en: Probing classifiers are extensively used to elucidate the inner workings of
    LLMs Alain and Bengio ([2016](#bib.bib2)); Azaria and Mitchell ([2023](#bib.bib4));
    Jin et al. ([2024](#bib.bib8)); Ju et al. ([2024](#bib.bib9)); Templeton et al.
    ([2024](#bib.bib20)). Various works train probes on model representations to assess
    how well they encode various linguistic features, such as phrase-level, syntactic,
    and semantic information Liu et al. ([2023a](#bib.bib12)); Marks and Tegmark ([2023](#bib.bib14));
    Li et al. ([2024](#bib.bib10)). The efficacy of a classifier in a given task indicates
    the degree to which that layer successfully captures pertinent information. In
    our study, we employ probing as a proxy to determine whether the LLMs accurately
    identify and represent crucial parts of the context.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 探测分类器被广泛用于阐明LLMs的内部工作原理（Alain 和 Bengio，[2016](#bib.bib2)；Azaria 和 Mitchell，[2023](#bib.bib4)；Jin
    等人，[2024](#bib.bib8)；Ju 等人，[2024](#bib.bib9)；Templeton 等人，[2024](#bib.bib20)）。各种研究训练探测器在模型表示上，以评估它们如何编码各种语言特征，如短语级、句法和语义信息（Liu
    等人，[2023a](#bib.bib12)；Marks 和 Tegmark，[2023](#bib.bib14)；Li 等人，[2024](#bib.bib10)）。分类器在特定任务中的有效性表明该层在捕捉相关信息方面的成功程度。在我们的研究中，我们使用探测作为代理来确定LLMs是否准确识别并表示上下文中的关键部分。
- en: 3 Experimental Setup
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: We design a layer-wise probing task to examine if the model successfully identifies
    the target information from the given prompt. We expect that higher probing accuracy
    signifies a stronger connection between the model’s hidden representations and
    its internal knowledge of the target information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一项逐层探测任务，以检查模型是否成功识别了给定提示中的目标信息。我们期望较高的探测准确性表示模型的隐藏表示与其对目标信息的内部知识之间存在更强的联系。
- en: '![Refer to caption](img/d6df95dd8eb2836398fd161c8a65229c.png)![Refer to caption](img/7a8857d0719342cecdf1e33b00f80350.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d6df95dd8eb2836398fd161c8a65229c.png)![参考说明](img/7a8857d0719342cecdf1e33b00f80350.png)'
- en: 'Figure 2: Accuracy of directly generating answers by LLMs (blue line) vs maximum
    probing accuracy across layers by our probing classifiers (red line). In both
    tasks, our probing classifiers outperform the model’s generated answers, across
    all the gold positions. This indicates a discrepancy between *knowing* the context
    and *using* it.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLMs直接生成答案的准确性（蓝线）与我们探测分类器在各层的最大探测准确性（红线）。在这两个任务中，我们的探测分类器在所有的金标准位置上均优于模型生成的答案。这表明*了解*上下文与*使用*它之间存在差异。
- en: Datasets and prompts.
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和提示。
- en: 'We follow the datasets and prompts used by Liu et al. ([2023b](#bib.bib13)).
    Our datasets include: (1) Key-Value pairs retrieval (kv-pairs) where the context
    contains a collection of keys and their corresponding values (128-bit randomly
    generated UUIDs). The goal of this task is to identify a value given its key.
    Each prompt for this task contains 100 kv-pairs and a target key. (2) Multi-document
    question answering (MDQA) where the context contains multiple sets of evidence
    paragraphs. The goal of this task is to, given a question, identify the relevant
    document and produce an answer. Each prompt for this task contains 30 documents,
    and a target question. Given a set of key-value pairs/documents, with only one
    containing target information, LLM is prompted to output the value/answer given
    the key/question. Further details on prompt construction can be found in §[A](#A1
    "Appendix A Prompting Details ‣ Insights into LLM Long-Context Failures: When
    Transformers Know but Don’t Tell").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循刘等人使用的数据集和提示（[2023b](#bib.bib13)）。我们的数据集包括：（1）键值对检索（kv-pairs），其中上下文包含一组键及其对应的值（128位随机生成的UUID）。此任务的目标是根据键识别值。此任务的每个提示包含100个kv对和一个目标键。（2）多文档问答（MDQA），其中上下文包含多组证据段落。此任务的目标是根据问题识别相关文档并生成答案。此任务的每个提示包含30个文档和一个目标问题。给定一组键值对/文档，其中只有一个包含目标信息，LLM被提示根据键/问题输出值/答案。有关提示构造的更多详细信息，请参见§[A](#A1
    "附录 A 提示详细信息 ‣ 变压器知道但不说：LLM长上下文失败的见解")。
- en: Probing classifiers.
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探测分类器。
- en: 'For each input prompt, we collect the last token embedding of *each layer*.
    We then train separate linear classifiers for each layer would receive the embedding
    (of the last token) as input and the gold kv-pair/document ID (position among
    all pairs/documents) as the target output. The classifier minimizes the following
    objective:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个输入提示，我们收集*每一层*的最后一个标记嵌入。然后，我们为每一层训练独立的线性分类器，这些分类器将接收（最后一个标记的）嵌入作为输入，以及金标准kv对/文档ID（在所有对/文档中的位置）作为目标输出。分类器最小化以下目标：
- en: '|  | $1$2 |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $N$ is the one-hot encoded label for the $i$-th ID. Ultimately, this recipe
    gives one probing classifier for embeddings of *each layer*. Using these models,
    we show results per layer and across layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是第 $i$ 个ID的独热编码标签。最终，这个方法为*每一层*的嵌入提供一个探测分类器。使用这些模型，我们展示了每层和跨层的结果。
- en: Models and hyperparameters.
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和超参数。
- en: 'We employ LLaMa3-8B-Instruct AI@Meta ([2024](#bib.bib1)) to our probing analysis.
    Results for two other models are in shown §[C](#A3 "Appendix C Experiments Results
    on Mistral-7B-Instruct-v0.3 Jiang et al. (2023) and Gemma-7b-it Team et al. (2024)
    ‣ Insights into LLM Long-Context Failures: When Transformers Know but Don’t Tell"),
    where we at the same conclusion. To reduce uncertainty caused by random initialization,
    each classifier is trained ten times. In the results, we report the mean and std
    (error bars) of independent ten experiments.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将LLaMa3-8B-Instruct AI@Meta ([2024](#bib.bib1))应用于我们的探测分析。另两个模型的结果见§[C](#A3
    "附录 C 实验结果：Mistral-7B-Instruct-v0.3 Jiang等（2023）和Gemma-7b-it Team等（2024） ‣ 变压器知道但不说：LLM长上下文失败的见解")，我们得出了相同的结论。为了减少随机初始化带来的不确定性，每个分类器训练十次。结果中，我们报告了独立十次实验的均值和标准差（误差条）。
- en: '![Refer to caption](img/17c0d2858c3e47df317c780689da7cbb.png)![Refer to caption](img/bde9aca7ca3a970918e24e46ff12e405.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17c0d2858c3e47df317c780689da7cbb.png)![参见说明](img/bde9aca7ca3a970918e24e46ff12e405.png)'
- en: 'Figure 3: The figures show the probing accuracy for each layer for the two
    tasks: kv-pairs (left) and MDQA (right). Different colors indicate the position
    of target information in the input context. In both tasks, mid-context information
    requires more layers to be extracted.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：图中显示了两个任务的每一层的探测准确率：kv-pairs（左）和MDQA（右）。不同的颜色表示输入上下文中目标信息的位置。在这两个任务中，中间上下文信息需要更多的层次进行提取。
- en: Metrics.
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价指标。
- en: 'Following Liu et al. ([2023b](#bib.bib13)), we use accuracy to quantify the
    success of our models. Specifically, we quantify accuracy for two types of targets:
    (a) Generation accuracy quantifies how well LLMs generate correct value in kv-pair
    retrieval or generate correct answer string in MDQA. (b) Probing accuracy quantifies
    how accurately classifiers can predict the gold kv-pair or document ID, indicating
    whether the layers sufficiently encode information from the input context.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Liu等人（[2023b](#bib.bib13)）的方法，我们使用准确度来量化模型的成功。具体而言，我们量化了两种类型目标的准确度：（a）生成准确度量化了LLM在kv-pair检索中生成正确值或在MDQA中生成正确答案字符串的能力。（b）探测准确度量化了分类器预测正确kv-pair或文档ID的准确度，指示层是否足够编码输入上下文中的信息。
- en: 4 LLMs Know but Don’t Tell
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LLM知道但不告诉
- en: '4.1 Experiment: maximum probing accuracy across all LLM layers'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验：所有LLM层中的最大探测准确度
- en: 'We focus on the peak accuracy across all transformer layers as a proxy to determine
    if the model ever correctly identifies the useful information within the prompt
    during the forward pass. Specifically, we select the probing classifier with the
    highest accuracy across all layers. In [Figure 2](#S3.F2 "Figure 2 ‣ 3 Experimental
    Setup ‣ Insights into LLM Long-Context Failures: When Transformers Know but Don’t
    Tell"), we show this peak layer probing accuracy. For comparison, we also show
    the accuracy of LLMs in generating the answer (independent of our probing classifiers).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注所有变压器层中的峰值准确度，作为判断模型是否在前向传递过程中正确识别提示中的有用信息的代理。具体而言，我们选择在所有层中准确度最高的探测分类器。在[图
    2](#S3.F2 "图 2 ‣ 3 实验设置 ‣ 对LLM长上下文失败的洞察：当变压器知道但不告诉")中，我们展示了这一峰值层探测准确度。为了比较，我们还展示了LLM生成答案的准确度（与我们的探测分类器无关）。
- en: LLMs know but don’t tell.
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM知道但不告诉。
- en: 'Our results indicate that the model’s hidden representations indeed contain
    information about the location of the target information. Specifically, in kv-pairs
    setup (Fig. [2](#S3.F2 "Figure 2 ‣ 3 Experimental Setup ‣ Insights into LLM Long-Context
    Failures: When Transformers Know but Don’t Tell"); left) there is always a layer
    such that its probe can near-perfectly identify the location of the correct key-value
    pair associated with the prompt. This is true, even for instances where the LLM
    does not return the correct answer or abstains from producing any answer. This
    suggests a disconnect between the model’s ability to locate the information and
    generate a response based on that information.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果表明，模型的隐藏表示确实包含有关目标信息位置的信息。具体而言，在kv-pairs设置中（见图[2](#S3.F2 "图 2 ‣ 3 实验设置
    ‣ 对LLM长上下文失败的洞察：当变压器知道但不告诉")；左），总是存在一个层，其探测几乎完美地识别与提示相关的正确键值对的位置。即使在LLM没有返回正确答案或拒绝生成任何答案的情况下，这也是如此。这表明模型的定位信息能力与基于该信息生成响应之间存在脱节。
- en: 'A similar trend is also observed for MDQA (Fig. [2](#S3.F2 "Figure 2 ‣ 3 Experimental
    Setup ‣ Insights into LLM Long-Context Failures: When Transformers Know but Don’t
    Tell"); right) where the peak probing accuracy is consistently higher than the
    direct answer accuracy, indicating the same disconnect from document grounding
    to response generation. These findings highlight that while the model can recognize
    and encode the location of relevant information within its layers, this knowledge
    does not always translate into an accurate generation answer.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDQA中（见图[2](#S3.F2 "图 2 ‣ 3 实验设置 ‣ 对LLM长上下文失败的洞察：当变压器知道但不告诉")；右），也观察到了类似的趋势，即峰值探测准确度始终高于直接答案准确度，表明从文档基础到响应生成的脱节。这些发现强调，虽然模型能够在其层中识别和编码相关信息的位置，但这种知识并不总是能转化为准确的生成答案。
- en: '4.2 Experiment: probing across per layers'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验：层间探测
- en: 'To understand the flow of information across LLMs’ layers, we shift our focus
    to probing classifiers’ accuracy across LLM layers. [Figure 3](#S3.F3 "Figure
    3 ‣ Models and hyperparameters. ‣ 3 Experimental Setup ‣ Insights into LLM Long-Context
    Failures: When Transformers Know but Don’t Tell") visualizes probing classifier
    accuracy per layer. For comparison, in both kv-pairs/MDQA setups, we show this
    accuracy for three positions: target information at the start, middle, or end
    of the input.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解信息在LLM层间的流动，我们将重点转移到LLM层中探测分类器的准确度。[图 3](#S3.F3 "图 3 ‣ 模型和超参数 ‣ 3 实验设置 ‣
    对LLM长上下文失败的洞察：当变压器知道但不告诉")可视化了每层的探测分类器准确度。为了比较，在kv-pairs/MDQA设置中，我们展示了三种位置的准确度：输入的开始、中间或末尾的目标信息。
- en: Mid-context information requires more layers to be located.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 中间上下文信息需要更多层来定位。
- en: 'Our results reveal that LLM locates target information gradually at early layers.
    Specifically, in the kv-pair setup (Fig. [3](#S3.F3 "Figure 3 ‣ Models and hyperparameters.
    ‣ 3 Experimental Setup ‣ Insights into LLM Long-Context Failures: When Transformers
    Know but Don’t Tell"); left), probing accuracy consistently increases until it
    reaches perfect accuracy at layer 13\. Notably, when the target kv-pair is at
    the middle position of the input prompt, LLM requires more layers to locate the
    target information.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的结果揭示了LLM在早期层逐渐定位目标信息。具体而言，在kv-pair设置中（图[3](#S3.F3 "Figure 3 ‣ Models and
    hyperparameters. ‣ 3 Experimental Setup ‣ Insights into LLM Long-Context Failures:
    When Transformers Know but Don’t Tell"); 左），探测准确率持续增加，直到第13层达到完美准确性。值得注意的是，当目标kv-pair位于输入提示的中间位置时，LLM需要更多层来定位目标信息。'
- en: 'The general trends of the MDQA scenario ([Figure 3](#S3.F3 "Figure 3 ‣ Models
    and hyperparameters. ‣ 3 Experimental Setup ‣ Insights into LLM Long-Context Failures:
    When Transformers Know but Don’t Tell"); right) are similar in principle, but
    with nuanced differences. The patterns vary significantly with the position of
    target information. Classifiers perform best when the target document is at the
    start of the input context, with near-perfect prediction since early layers ,
    and maintain it in subsequent layers. However, it takes more layers for the probing
    classifier to achieve peak accuracy for the middle and tail gold context. Interestingly,
    when the target document is in the middle, classifier accuracy decreases after
    the peak. As MDQA task requires a higher level of reasoning, the model is shifting
    from locating documents to generating language output.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'MDQA场景的一般趋势（[图3](#S3.F3 "Figure 3 ‣ Models and hyperparameters. ‣ 3 Experimental
    Setup ‣ Insights into LLM Long-Context Failures: When Transformers Know but Don’t
    Tell"); 右）在原则上类似，但有细微差别。模式随目标信息的位置显著变化。当目标文档位于输入上下文的开始时，分类器表现最佳，早期层预测几乎完美，并在随后的层中保持。然而，对于中间和尾部的黄金上下文，探测分类器需要更多的层才能达到峰值准确性。有趣的是，当目标文档位于中间时，分类器准确性在峰值后下降。由于MDQA任务需要更高水平的推理，模型正从定位文档转向生成语言输出。'
- en: '4.3 Experiment: the number of layers taken for locating target information'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实验：用于定位目标信息的层数
- en: 'Our probing experiments (§[4.2](#S4.SS2 "4.2 Experiment: probing across per
    layers ‣ 4 LLMs Know but Don’t Tell ‣ Insights into LLM Long-Context Failures:
    When Transformers Know but Don’t Tell")) reveal that the model’s encoding of target
    information position initially improves but then degrades as layer depth increases.
    This motivates, the investigation of the relationship between the number of layers
    taken by the model to locate target information from the prompt and the LLM’s
    generation accuracy of generating the target information.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的探测实验（§[4.2](#S4.SS2 "4.2 Experiment: probing across per layers ‣ 4 LLMs
    Know but Don’t Tell ‣ Insights into LLM Long-Context Failures: When Transformers
    Know but Don’t Tell")）揭示了模型对目标信息位置的编码最初改善，但随着层深度增加而退化。这激发了对模型从提示中定位目标信息所需层数与LLM生成目标信息的生成准确性之间关系的调查。'
- en: 'Figure 4: The LLM layer that achieves the peak probing accuracy ($x$-axis).
    We observe that a *later* peak correlates with *lower* accuracy in the language
    model’s final output. This implies that the earlier an LLM encodes information
    from a specific index, the higher the accuracy of the final output for that position.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：达到峰值探测准确率的LLM层（$x$轴）。我们观察到，*较晚*的峰值与语言模型最终输出的*较低*准确性相关。这表明LLM越早编码来自特定索引的信息，该位置的最终输出准确性越高。
- en: 'We run additional 35, 40, 45, and 50 multi-document probing tasks. In Fig. [4](#S4.F4
    "Figure 4 ‣ 4.3 Experiment: the number of layers taken for locating target information
    ‣ 4 LLMs Know but Don’t Tell ‣ Insights into LLM Long-Context Failures: When Transformers
    Know but Don’t Tell"), for all IDs that achieve probing accuracy greater than
    60% (to suppress the outliers), on the $x$-axis we show the the LLM’s generation
    accuracy (no probes involved).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '我们运行了额外的35、40、45和50个多文档探测任务。在图[4](#S4.F4 "Figure 4 ‣ 4.3 Experiment: the number
    of layers taken for locating target information ‣ 4 LLMs Know but Don’t Tell ‣
    Insights into LLM Long-Context Failures: When Transformers Know but Don’t Tell")中，对于所有探测准确率大于60%的ID（以抑制离群值），在$x$轴上我们展示了LLM的生成准确率（不涉及探测）。'
- en: Early-layer information localization leads to higher accuracy in LLM output.
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 早期层信息定位会导致LLM输出的更高准确性。
- en: 'As Fig. [4](#S4.F4 "Figure 4 ‣ 4.3 Experiment: the number of layers taken for
    locating target information ‣ 4 LLMs Know but Don’t Tell ‣ Insights into LLM Long-Context
    Failures: When Transformers Know but Don’t Tell") shows, there is a statistically
    significant (two-sided t-test) negative correlation between the layer with peak
    accuracy of locating target information and its final output accuracy ($pExample Key-Value pair "7f666c61-573f-4212-a0a9-6f90d487cd4a"
    : "2a1d0ba0-cfe4-4df5-987a-6ee1be2c6ac0"'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'Example Key-Value pair "7f666c61-573f-4212-a0a9-6f90d487cd4a"
    : "2a1d0ba0-cfe4-4df5-987a-6ee1be2c6ac0"'
- en: 'The $n$, and then construct as a prompt in the format:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: $n$，然后构建为如下格式的提示：
- en: 'Extract
    the value corresponding to the specified key in the JSON object below. JSON data:
    { "key¹: "value¹", “key²": "value²", … "key^k": "value^k", … "key^n": "value^n",
    } Key: "key^k" Corresponding value:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'Extract
    the value corresponding to the specified key in the JSON object below. JSON data:
    { "key¹: "value¹", “key²": "value²", … "key^k": "value^k", … "key^n": "value^n",
    } Key: "key^k" Corresponding value:'
- en: Multi-document question answering (MDQA)
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多文档问答（MDQA）
- en: In the $n$ document setting, we randomly select one question answer pair from
    the dataset by Liu et al. ([2023b](#bib.bib13)). Subsequently we retrieve the
    document containing this answer and mark it as gold.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在$n$文档设置中，我们从Liu等人（[2023b](#bib.bib13)）的数据集中随机选择一对问题答案对。随后，我们检索包含该答案的文档并将其标记为黄金标准。
- en: 'Example
    retrieval Question: who got the first nobel prize in physics Answer: Wilhelm Conrad
    Röntgen Document: (Title: List of Nobel laureates in Physics) The first Nobel
    Prize in Physics was awarded in 1901 to Wilhelm Conrad Röntgen, of Germany, who
    received…'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'Example
    retrieval Question: who got the first nobel prize in physics Answer: Wilhelm Conrad
    Röntgen Document: (Title: List of Nobel laureates in Physics) The first Nobel
    Prize in Physics was awarded in 1901 to Wilhelm Conrad Röntgen, of Germany, who
    received…'
- en: 'We then sample $n-1$ is like:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们采样$n-1$，类似于：
- en: 'Write
    a high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). Document [1](Title: Asian Americans in science
    and technology) Prize in physics for discovery of the subatomic… … Document [$k$]
    (Title: Scientist) and pursued through a unique method, was essentially in place.
    Ramón y Cajal won … Question: who got the first nobel prize in physics Answer:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'Write
    a high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). Document [1](Title: Asian Americans in science
    and technology) Prize in physics for discovery of the subatomic… … Document [$k$]
    (Title: Scientist) and pursued through a unique method, was essentially in place.
    Ramón y Cajal won … Question: who got the first nobel prize in physics Answer:'
- en: Appendix B Probing Setup
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 探测设置
- en: 'In the experiment described in §[3](#S3 "3 Experimental Setup ‣ Insights into
    LLM Long-Context Failures: When Transformers Know but Don’t Tell"), we employ
    linear classifiers as our probing method.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在§[3](#S3 "3 Experimental Setup ‣ Insights into LLM Long-Context Failures:
    When Transformers Know but Don’t Tell")中描述的实验中，我们使用线性分类器作为我们的探测方法。'
- en: For any given task, we choose $\{1,0.1n,0.2n,\dots,1.0n\}$ iterations, resulting
    in a set of 110,000 prompts.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何给定的任务，我们选择$\{1,0.1n,0.2n,\dots,1.0n\}$轮次，结果为一组110,000个提示。
- en: Each prompt is fed into language model, and the embedding from each layer’s
    last token is collected. For each layer, separately, we have $110,000$ embeddings
    corresponding to 11 IDs and train a classifier for ten times, with embedding as
    input and ID as output. We calculate their mean accuracy and standard deviation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示被输入到语言模型中，收集每层最后一个token的嵌入。对于每一层，我们分别有$110,000$个嵌入对应于11个ID，并训练了十次分类器，以嵌入作为输入，ID作为输出。我们计算它们的平均准确率和标准偏差。
- en: Appendix C Experiments Results on Mistral-7B-Instruct-v0.3 Jiang et al. ([2023](#bib.bib7))
    and Gemma-7b-it Team et al. ([2024](#bib.bib19))
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C Mistral-7B-Instruct-v0.3 Jiang 等人（[2023](#bib.bib7)）和 Gemma-7b-it Team
    等人（[2024](#bib.bib19)）的实验结果
- en: We conduct same experiment procedure on additional two models, which produce
    the same pattern. The experiment is running on one A100 GPU.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在另外两个模型上进行了相同的实验程序，产生了相同的模式。实验在一台A100 GPU上进行。
- en: '![Refer to caption](img/ee4bb3d007103f7217d023770ec23b85.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ee4bb3d007103f7217d023770ec23b85.png)'
- en: 'Figure 5: In the left figure, Mistral’s probing result in 100 kv-pairs retrieval
    task resembles with Llama3’s in most ways, but there’re still a few differences.
    First, the trends of locating information from the head and end are more similar
    compared to those in Llama3\. Also the model takes more layers than Llama3 to
    well locate information in middle context. The right figure highlights the significant
    discrepancy between the probing peak accuracy and generation accuracy, indicating
    a severe ’know don’t tell’ phenomenon.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在左侧图中，Mistral 在100 kv-pairs检索任务中的探测结果在大多数方面与Llama3相似，但仍存在一些差异。首先，从头部和尾部定位信息的趋势相比Llama3更为相似。此外，该模型需要比Llama3更多的层来有效地定位中间上下文的信息。右侧图突出显示了探测峰值准确率和生成准确率之间的显著差异，表明存在严重的“知道却不说”现象。
- en: '![Refer to caption](img/4a3dc9dbb77559c98f15d8d876b96e32.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4a3dc9dbb77559c98f15d8d876b96e32.png)'
- en: 'Figure 6: Mistral’s layer-wise probing classifier accuracy performs in the
    same pattern as Llama3\. Middle context takes more layers to encode and results
    in a lower peak accuracy. In right figure, both its peak accuracy and generation
    accuracy shows a U-shape curve, with still probing consistently outperforms generation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Mistral的逐层探测分类器准确率表现出与Llama3相同的模式。中间上下文需要更多层进行编码，结果是峰值准确率较低。右侧图中，峰值准确率和生成准确率均显示出U形曲线，并且探测准确率始终优于生成准确率。
- en: '![Refer to caption](img/d8c3c83993a25fdd88376ef3d4ec1bfb.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d8c3c83993a25fdd88376ef3d4ec1bfb.png)'
- en: 'Figure 7: In the left figure, Gemma shows a similar pattern like other models
    in 100 kv-pairs retrieval task. However, information from all positions is located
    more slowly than other models: Information in the head or end position of input
    context is located after 5 layers, while information in the middle of input takes
    15 layers to be located. In the right figure, it shows a significant gap between
    generation accuracy and probing peak accuracy, which is similar with Mistral.
    This highlights a significant ’know don’t tell’ phenomenon.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在左侧图中，Gemma在100 kv-pairs检索任务中显示出类似其他模型的模式。然而，来自所有位置的信息定位速度比其他模型慢：输入上下文中头部或尾部位置的信息在5层之后被定位，而中间的信息则需要15层才能定位。右侧图显示了生成准确率和探测峰值准确率之间的显著差距，这与Mistral相似。这突出了显著的“知道却不说”现象。
- en: '![Refer to caption](img/4cbcb059632656d945d0ed09f1016870.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4cbcb059632656d945d0ed09f1016870.png)'
- en: 'Figure 8: For Gemma MDQA task, although head and end context follows same observation,
    it middle context shows a sudden decrease in accuracy, indicating a sudden information
    lost but soon retrieve it back. The right figure follows the same pattern, where
    generation accuracy is consistently low, disconnecting from the high U-shape probing
    accuracy curve.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：对于Gemma MDQA任务，尽管头部和尾部上下文遵循相同的观察模式，但中间上下文的准确率却突然下降，表明信息突然丢失但很快恢复。右侧图遵循相同的模式，其中生成准确率始终较低，与高U形探测准确率曲线断开。
