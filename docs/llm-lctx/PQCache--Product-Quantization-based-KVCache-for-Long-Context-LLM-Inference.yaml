- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:56:05'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:56:05
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PQCache: Product Quantization-based KVCache for Long Context LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PQCache：基于产品量化的KVCache用于长上下文LLM推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12820](https://ar5iv.labs.arxiv.org/html/2407.12820)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12820](https://ar5iv.labs.arxiv.org/html/2407.12820)
- en: Hailin Zhang Peking University ,  Xiaodong Ji Peking University ,  Yilin Chen
    Beijing Institute of Technology ,  Fangcheng Fu Peking University ,  Xupeng Miao
    Carnegie Mellon University ,  Xiaonan Nie Peking University ,  Weipeng Chen Baichuan
    Inc.  and  Bin Cui Peking University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张海林 北京大学，  姜晓东 北京大学，  陈一林 北京理工大学，  傅方程 北京大学，  苗旭鹏 卡内基梅隆大学，  聂晓楠 北京大学，  陈伟鹏 百川股份有限公司
    以及  崔斌 北京大学
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: As the field of Large Language Models (LLMs) continues to evolve, the context
    length in inference is steadily growing. Key-Value Cache (KVCache), a crucial
    component in LLM inference, has now become the primary memory bottleneck due to
    limited GPU memory. Current methods selectively determine suitable keys and values
    for self-attention computation in LLMs to address the issue. However, they either
    fall short in maintaining model quality or result in high serving latency. Drawing
    inspiration from advanced embedding retrieval techniques used in the database
    community, we consider the storage and searching of KVCache as a typical embedding
    retrieval problem. We propose PQCache, which employs Product Quantization (PQ)
    to manage KVCache, maintaining model quality while ensuring low serving latency.
    During the prefilling phase, we apply PQ to tokens’ keys for each LLM layer and
    head. During the autoregressive decoding phase, for each newly generated token,
    we first identify important tokens through Maximum Inner-Product Search (MIPS)
    using PQ codes and centroids, then fetch the corresponding key-value pairs for
    self-attention computation. Through meticulous design of overlapping and caching,
    we minimize any additional computation and communication overhead during both
    phases. Extensive experiments show that PQCache achieves both effectiveness and
    efficiency. It maintains model quality even when only 1/5 of the tokens are involved
    in attention, while attaining acceptable system latency.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）领域的不断发展，推理中的上下文长度稳步增长。关键值缓存（KVCache），作为LLM推理中的关键组件，现已成为主要的内存瓶颈，因为GPU内存有限。当前方法通过选择性地确定适合的键和值来进行自注意力计算，以解决这一问题。然而，它们要么在维持模型质量方面有所不足，要么导致较高的服务延迟。受到数据库社区中先进嵌入检索技术的启发，我们将KVCache的存储和检索视为一种典型的嵌入检索问题。我们提出了PQCache，它采用产品量化（PQ）来管理KVCache，在确保低服务延迟的同时保持模型质量。在预填充阶段，我们对每个LLM层和头部的标记键应用PQ。在自回归解码阶段，对于每个新生成的标记，我们首先通过最大内积搜索（MIPS）利用PQ代码和质心识别重要标记，然后获取对应的键值对进行自注意力计算。通过精心设计的重叠和缓存，我们在两个阶段中都最小化了额外的计算和通信开销。大量实验表明，PQCache在有效性和效率上都取得了良好结果。即使只有1/5的标记参与注意力计算，它也能保持模型质量，同时达到可接受的系统延迟。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'With the emergence of ChatGPT (OpenAI, [2023](#bib.bib41)), Large Language
    Models (LLMs) have captured the attention of researchers and engineers as promising
    candidates for Artificial General Intelligence (AGI). LLMs exhibit exceptional
    performance in the “next token prediction” task, where they take a sequence of
    tokens as input (also called prompt) and generate subsequent tokens autoregressively
    during inference. Constructed with transformer layers, the fundamental mechanism
    of LLMs is the self-attention module. For each token, this module computes “query”,
    “key”, and “value” representations. Each token’s query interacts with the previous
    tokens’ keys (including itself) to derive attention weights, which are then used
    for weighted summation of the previous tokens’ values. Figure [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference") illustrates a typical self-attention module within a transformer
    layer.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '随着ChatGPT的出现（OpenAI， [2023](#bib.bib41)），大型语言模型（LLMs）吸引了研究人员和工程师的关注，成为通用人工智能（AGI）的有前景的候选者。LLMs在“下一个token预测”任务中表现卓越，在此任务中，它们将一系列token作为输入（也称为提示），并在推理过程中自回归生成后续token。LLMs由transformer层构建，其基本机制是自注意力模块。对于每个token，该模块计算“查询”、“键”和“值”表示。每个token的查询与之前token的键（包括自身）进行交互，以导出注意力权重，然后用于加权求和之前token的值。图[2](#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference")展示了transformer层中的典型自注意力模块。'
- en: 'To accommodate increasingly lengthy prompts, the maximum input length of LLMs
    has expanded significantly, from 2K-4K (Touvron et al., [2023](#bib.bib53); Taori
    et al., [2023](#bib.bib51)) to 32K (Jiang et al., [2023a](#bib.bib26); Together.ai,
    [2023](#bib.bib52)), 128K (OpenAI, [2023](#bib.bib41); Fu et al., [2024](#bib.bib16)),
    or even millions of tokens (AI, [2024](#bib.bib3); Cloud, [2024](#bib.bib10);
    Liu et al., [2024a](#bib.bib32)). As illustrated in Figure [2](#S1.F2 "Figure
    2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), the process of LLM inference involves two phases: prefilling
    and decoding. During prefilling, LLMs handle the lengthy input and compute keys
    and values for all input tokens. During decoding, LLMs generate the next new token
    and produce its key and value. To avoid redundant computations, the keys and values
    of preceding tokens are commonly cached in the Key-Value Cache (KVCache), and
    fetched for subsequent tokens’ attention computation. However, as prompts grow
    in length, the memory consumption of KVCache has far exceeded the memory capacity
    of each individual GPU, even for 7B and 13B LLMs in Figure [1(a)](#S1.F1.sf1 "In
    Figure 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference"). This poses a formidable challenge for modern LLM
    inference.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '为了适应日益增长的提示长度，LLM的最大输入长度显著扩大，从2K-4K（Touvron等， [2023](#bib.bib53)；Taori等， [2023](#bib.bib51)）扩展到32K（Jiang等，
    [2023a](#bib.bib26)；Together.ai， [2023](#bib.bib52)），128K（OpenAI， [2023](#bib.bib41)；Fu等，
    [2024](#bib.bib16)），甚至数百万个token（AI， [2024](#bib.bib3)；Cloud， [2024](#bib.bib10)；Liu等，
    [2024a](#bib.bib32)）。如图[2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference")所示，LLM推理过程包括两个阶段：预填充和解码。在预填充阶段，LLM处理长输入并计算所有输入token的键和值。在解码阶段，LLM生成下一个新token并生成其键和值。为了避免冗余计算，前面token的键和值通常缓存于键值缓存（KVCache）中，并用于后续token的注意力计算。然而，随着提示的增长，KVCache的内存消耗远超每个GPU的内存容量，即便是图[1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference")中的7B和13B LLM也面临巨大的挑战。'
- en: '![Refer to caption](img/c4a853e9722e5072fdce5b14d075cd95.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c4a853e9722e5072fdce5b14d075cd95.png)'
- en: (a) Model and KVCache memory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 模型和KVCache内存。
- en: '![Refer to caption](img/b7d7e0b4287f3f97d1508d664bc12065.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b7d7e0b4287f3f97d1508d664bc12065.png)'
- en: (b) Attention example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力示例。
- en: 'Figure 1\. Observations. The left figure shows the KVCache memory consumption
    of LLMs (for the meaning of MHA and GQA, please refer to Section [2.1](#S2.SS1
    "2.1\. Large Language Model Inference ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")). The right figure shows an example of
    attention scores on the MultiNews dataset (Fabbri et al., [2019](#bib.bib13)),
    with darker colors indicating higher scores.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '图1\. 观察。左图展示了LLM的KVCache内存消耗（有关MHA和GQA的含义，请参见第[2.1](#S2.SS1 "2.1\. Large Language
    Model Inference ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference")节）。右图展示了MultiNews数据集上的注意力得分示例（Fabbri等， [2019](#bib.bib13)），颜色越深表示得分越高。'
- en: '![Refer to caption](img/c9d50f4bfda50f78f9071d2a292667e0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9d50f4bfda50f78f9071d2a292667e0.png)'
- en: 'Figure 2\. An overview of LLM inference. The left part illustrates the computation
    process of the self-attention module, where “Q”, “K”, “V”, “AS”, and “O” represent
    query, key, value, attention score, and output, respectively. The right part depicts
    the LLM inference process, consisting of the prefilling phase and the decoding
    phase, where “Attn” and “FFN” represent the attention layer and the feed-forward
    network layer, respectively. The mathematical symbols are detailed in Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '图2\. LLM推理概述。左侧部分展示了自注意力模块的计算过程，其中“Q”、“K”、“V”、“AS”和“O”分别代表查询、键、值、注意力分数和输出。右侧部分描述了LLM推理过程，包括预填充阶段和解码阶段，其中“Attn”和“FFN”分别代表注意力层和前馈网络层。数学符号的详细信息见表格[1](#S1.T1
    "表1 ‣ 1\. 介绍 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")。'
- en: 'Recognizing that specific tokens significantly influence generation, i.e. their
    attention weights are much larger than others (Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34)), numerous methods selectively incorporate these
    tokens within attention mechanisms while excluding others. This approach aims
    to address the memory challenge posed by KVCache and is commonly referred to as
    selective attention (Miao et al., [2023](#bib.bib38)). Related methods can be
    classified into two categories: KVCache dropping (Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34); Xiao et al., [2023](#bib.bib59)) and KVCache
    offloading (Xiao et al., [2024](#bib.bib58); Ribar et al., [2023](#bib.bib47)).
    However, these methods either rely on improper assumptions or introduce notable
    latency during inference, failing to obtain both effectiveness and efficiency.
    KVCache dropping methods discard unnecessary key-value pairs, based on the assumption
    that unimportant tokens have no relevance for subsequent generation. Nevertheless,
    as shown in an attention score example in Figure [1(b)](#S1.F1.sf2 "In Figure
    1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), many tokens with lower average attention weights can still contribute
    to later generated tokens. Prior research (Kang et al., [2024](#bib.bib30); Dong
    et al., [2024b](#bib.bib11)) also highlights the drawback of direct dropping.
    KVCache offloading methods, including InfLLM (Xiao et al., [2024](#bib.bib58))
    and SPARQ (Ribar et al., [2023](#bib.bib47)), store the KVCache on CPU, and fetch
    relevant key-value pairs for each newly generated token according to easy-to-compute
    proxy scores. InfLLM organizes the KVCache into blocks, using representative tokens
    within each block to compute relevance. Unfortunately, as shown in Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference"), we do not observe the space-continuity assumption
    in InfLLM. SPARQ identifies a subset of dimensions with large magnitude in queries,
    and fetches only these dimensions from all keys to determine the most relevant
    tokens. Despite demonstrating effectiveness using a large number of dimensions,
    it incurs excessive communication overhead, and the serialized computation-communication
    process hinders opportunities for system optimization (e.g., prefetching). In
    summary, existing methods fall short in achieving both effectiveness and efficiency
    for long-context LLM inference.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '认识到特定的令牌对生成有显著影响，即它们的注意力权重远大于其他令牌（Zhang et al., [2023a](#bib.bib64); Liu et
    al., [2023a](#bib.bib34)），许多方法选择性地将这些令牌纳入注意力机制中，同时排除其他令牌。这种方法旨在解决KVCache带来的记忆挑战，通常被称为**选择性注意力**（Miao
    et al., [2023](#bib.bib38)）。相关方法可以分为两类：KVCache丢弃（Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34); Xiao et al., [2023](#bib.bib59)）和KVCache卸载（Xiao
    et al., [2024](#bib.bib58); Ribar et al., [2023](#bib.bib47)）。然而，这些方法要么依赖不恰当的假设，要么在推理过程中引入显著的延迟，未能兼顾效果和效率。KVCache丢弃方法基于不重要的令牌与后续生成无关的假设，丢弃不必要的键值对。然而，如图[1(b)](#S1.F1.sf2
    "在图1 ‣ 1. 引言 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")中的注意力得分示例所示，许多具有较低平均注意力权重的令牌仍然可能对后续生成的令牌有所贡献。先前的研究（Kang
    et al., [2024](#bib.bib30); Dong et al., [2024b](#bib.bib11)）也强调了直接丢弃的缺点。KVCache卸载方法，包括InfLLM（Xiao
    et al., [2024](#bib.bib58)）和SPARQ（Ribar et al., [2023](#bib.bib47)），将KVCache存储在CPU上，并根据易于计算的代理分数为每个新生成的令牌提取相关的键值对。InfLLM将KVCache组织为块，在每个块内使用代表性令牌计算相关性。不幸的是，如图[1(b)](#S1.F1.sf2
    "在图1 ‣ 1. 引言 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")所示，我们没有在InfLLM中观察到空间连续性假设。SPARQ在查询中识别出具有大幅度的维度子集，并仅从所有键中提取这些维度以确定最相关的令牌。尽管在使用大量维度时表现出效果，但它引入了过多的通信开销，并且串行计算-通信过程限制了系统优化的机会（例如预取）。总之，现有方法在实现长上下文LLM推理的效果和效率方面均显不足。'
- en: We clarify that, selective attention computation, requiring to find the top-$k$
    relevant embeddings from the index for a given query embedding. Surprisingly,
    we found that the selective attention calculation process during LLM inference
    can be mapped into these operations. Specifically, the prefilling phase generates
    most of the KVCache and constructs the index, while the decoding phase finds relevant
    keys/values and updates KVCache using the newly generated token.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们澄清，选择性注意力计算需要从索引中找到前$k$个相关的嵌入用于给定查询嵌入。令人惊讶的是，我们发现LLM推理过程中的选择性注意力计算过程可以映射到这些操作上。具体来说，预填充阶段生成了大部分KVCache并构建索引，而解码阶段则找到相关的键/值，并使用新生成的令牌更新KVCache。
- en: Inspired by the advanced embedding retrieval techniques, in this paper we propose
    PQCache to ensure both effectiveness and efficiency during long-context LLM inference.
    In the prefilling phase, we generate the KVCache, store it in CPU memory, and
    then construct index structures. In the decoding phase, we efficiently retrieve
    relevant key-value pairs for self-attention computation and update KVCache. Considering
    the latency requirements of LLM inference, we cannot employ methods with expensive
    index construction overheads, such as graph-based methods or complex inverted-index
    methods. We take the advantage of low-cost Product Quantization (PQ) (Jégou et al.,
    [2011](#bib.bib25)) from embedding retrieval (Jayaram Subramanya et al., [2019](#bib.bib24)),
    where embeddings are initially partitioned into sub-embeddings and then clustered.
    The key idea of PQCache is to construct PQ codebooks using preceding token keys
    and perform MIPS to retrieve relevant key-value pairs for subsequent self-attention
    computations. We propose a system-algorithm co-design method based on PQ, leveraging
    both its high recall potentiality, and the opportunities for system optimization.
    Further experimental analysis show that PQCache improves the LongBench scores
    up to 6.21 compared to existing methods, and attains acceptable system latency.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受到先进嵌入检索技术的启发，本文提出了PQCache，以确保在长上下文LLM推理过程中既有效又高效。在预填充阶段，我们生成KVCache，将其存储在CPU内存中，然后构建索引结构。在解码阶段，我们高效地检索相关的键值对用于自注意力计算并更新KVCache。考虑到LLM推理的延迟要求，我们不能使用具有昂贵索引构建开销的方法，如基于图的方法或复杂的倒排索引方法。我们利用低成本的产品量化（PQ）（Jégou等，[2011](#bib.bib25)）来进行嵌入检索（Jayaram
    Subramanya等，[2019](#bib.bib24)），其中嵌入最初被划分为子嵌入，然后进行聚类。PQCache的关键思想是使用前一个令牌键构建PQ码本，并执行MIPS以检索相关的键值对用于后续的自注意力计算。我们提出了一种基于PQ的系统算法协同设计方法，利用其高召回潜力及系统优化机会。进一步的实验证明，与现有方法相比，PQCache将LongBench分数提高了最多6.21，并且达到了可接受的系统延迟。
- en: To the best of our knowledge, this is the pioneering work that incorporates
    embedding retrieval technique to address the KVCache memory challenge. PQ offers
    a well-behaved approximation of embedding vectors (and their inner product), while
    consuming only a small amount of memory. In the prefilling phase, we apply PQ
    to the generated keys for each layer and head, and obtain PQ codes and centroids
    through clustering on CPU. At each autoregressive decoding step, we perform inner
    product between the partitioned query and the PQ centroids, then combine with
    PQ codes to obtain the approximate attention weights. Using the approximation,
    we retrieve top-$k$ relevant key-value pairs from CPU memory for self-attention
    computation, rather than accessing the entire KVCache.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是首个将嵌入检索技术应用于解决KVCache内存挑战的开创性工作。PQ提供了嵌入向量（及其内积）的良好近似，同时仅消耗少量内存。在预填充阶段，我们对每一层和每个头生成的键应用PQ，并通过在CPU上进行聚类得到PQ编码和质心。在每一步自回归解码中，我们在分区查询与PQ质心之间执行内积运算，然后结合PQ编码得到近似的注意力权重。利用这种近似，我们从CPU内存中检索前$k$个相关的键值对进行自注意力计算，而不是访问整个KVCache。
- en: 'To enable efficient LLM inference, we carefully design the PQCache system to
    reduce latency. We implement prefetching and overlapping as much as possible:
    KVCache offloading, PQ construction, and the fetching of PQ codes and centroids
    are overlapped with LLM computation. To maximize the utilization of available
    GPU memory and minimize CPU-GPU communication, we additionally introduce a block-level
    cache on GPU, specifically for frequently accessed key-value pairs.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现高效的LLM推理，我们精心设计了PQCache系统以减少延迟。我们尽可能实现了预取和重叠：KVCache卸载、PQ构建以及PQ代码和质心的获取与LLM计算重叠。为了最大化可用GPU内存的利用率并最小化CPU-GPU通信，我们额外在GPU上引入了一个块级缓存，专门用于频繁访问的键值对。
- en: 'We summarize our contributions as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了我们的贡献如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We incorporate the embedding retrieval technique PQ into KVCache management
    to enable both effective and efficient LLM inference.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将嵌入检索技术PQ纳入KVCache管理中，以实现有效且高效的LLM推理。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a system-algorithm co-designed approach PQCache to approximately
    retrieve the top-$k$ relevant keys for a given query, with meticulous design of
    overlapping and caching.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种系统与算法共同设计的方法PQCache，以大致检索给定查询的前-$k$个相关键，通过精心设计的重叠和缓存实现。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We evaluate PQCache through extensive experiments. It maintains model quality
    with 1/5 of the tokens in attention, while achieving acceptable system latency.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过大量实验评估了PQCache。它在关注中的令牌数量为原来的1/5时，仍能保持模型质量，同时实现了可接受的系统延迟。
- en: Table 1\. Notations. “#” means “the number of”.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 符号说明。“#”表示“数量”。
- en: '| Sym. | Explanation | Sym. | Explanation |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 解释 | 符号 | 解释 |'
- en: '| $n$ | # partitions in PQ. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $n$ | PQ中的分区数。 |'
- en: '| $s$ | # bits for PQ codes. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $s$ | PQ代码的位数。 |'
- en: '| $d$ | Dimension of each partition. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | 每个分区的维度。 |'
- en: '| $h_{(kv)}$ | # tokens in selective attention. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $h_{(kv)}$ | 选择性注意力中的令牌数。 |'
- en: '| $d_{h}$ | # K-Means iterations. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $d_{h}$ | K-Means迭代次数。 |'
- en: 2\. Preliminary
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初步
- en: In this section, we introduce fundamental concepts related to LLM, PQ, and the
    memory hierarchy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了与LLM、PQ及内存层次结构相关的基本概念。
- en: 2.1\. Large Language Model Inference
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 大型语言模型推理
- en: 'An overview of LLM inference is depicted in Figure [2](#S1.F2 "Figure 2 ‣ 1\.
    Introduction ‣ PQCache: Product Quantization-based KVCache for Long Context LLM
    Inference"). An LLM comprises a stack of transformer layers, along with a vocabulary
    embedding for input and a token classifier for output. The self-attention module,
    which is a crucial component of a transformer layer, facilitates interaction and
    information aggregation among different tokens. Multi-Head Attention (MHA) and
    Grouped-Query Attention (GQA) (Ainslie et al., [2023](#bib.bib4)) are the primary
    variants of the self-attention module. Following the notations in Table [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference"), the attention module receives an input of shape
    $(n,s,d)$. In this setup, each key-value pair corresponds to multiple queries.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM推理的概述如图[2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference")所示。LLM由一系列变压器层组成，同时包括用于输入的词汇嵌入和用于输出的令牌分类器。自注意力模块是变压器层的关键组件，促进不同令牌之间的交互和信息聚合。多头注意力（MHA）和分组查询注意力（GQA）（Ainslie等，[2023](#bib.bib4)）是自注意力模块的主要变体。按照表[1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference")中的符号，注意力模块接收形状为$(n,s,d)$的输入。在此设置中，每个键值对对应多个查询。'
- en: 'During LLM inference, each execution of the model generates a new token, following
    an autoregressive manner. The first traversal and the subsequent traversals of
    the LLM are referred to as “prefilling” and “decoding” separately, as shown in
    Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"). During the prefilling phase, the self-attention
    module computes the queries, keys, and values for all input tokens, and stores
    the key-value pairs as KVCache for later usage. During the autoregressive decoding
    phase, the attention module only computes the query, key, value for the last generated
    token. It leverages previous keys and values from the KVCache, and computes an
    attention score of shape $(n,h,1,s)$. Concurrently, the newly generated key and
    value are added to the KVCache. Consequently, the memory consumption of KVCache
    scales linearly with the sequence length, which leads to a memory bottleneck in
    scenarios involving long-context LLM inference.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '在 LLM 推理过程中，每次模型执行都会生成一个新的令牌，按照自回归方式进行。LLM 的第一次遍历和后续遍历分别称为“预填充”和“解码”，如图 [2](#S1.F2
    "Figure 2 ‣ 1\. Introduction ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference") 所示。在预填充阶段，自注意力模块计算所有输入令牌的查询、键和值，并将键值对存储为 KVCache
    以供后续使用。在自回归解码阶段，注意力模块仅计算最后生成的令牌的查询、键和值。它利用来自 KVCache 的先前键和值，并计算形状为 $(n,h,1,s)$
    的注意力得分。同时，新生成的键和值被添加到 KVCache 中。因此，KVCache 的内存消耗与序列长度线性增长，这在长上下文 LLM 推理场景中导致了内存瓶颈。'
- en: '![Refer to caption](img/8cc9480d7236ac20ffa4cd55bc0b1da9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8cc9480d7236ac20ffa4cd55bc0b1da9.png)'
- en: Figure 3\. An overview of PQ construction and searching.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. PQ 构造和搜索的概述。
- en: 2.2\. Product Quantization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 产品量化
- en: 'PQ (Jégou et al., [2011](#bib.bib25)) was proposed to facilitate efficient
    Approximate Nearest Neighbor Search (ANNS), retrieving relevant embeddings from
    a large pool of candidates given a query embedding. MIPS is a special case of
    ANNS that uses inner product as similarity. As shown in Figure [3](#S2.F3 "Figure
    3 ‣ 2.1\. Large Language Model Inference ‣ 2\. Preliminary ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference"), PQ divides each candidate
    embedding into $m$ bits, corresponding to the centroids. These compact PQ codes
    enable the reconstruction of approximate embeddings with reduced memory requirements.
    During ANNS, the query embedding computes similarity with the centroids and aggregates
    the similarity using PQ codes, bypassing the need for full similarity calculations
    with every embedding.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'PQ (Jégou et al., [2011](#bib.bib25)) 被提出以促进高效的近似最近邻搜索 (ANNS)，从大量候选中检索相关的嵌入向量，给定一个查询嵌入向量。MIPS
    是 ANNS 的一个特殊情况，它使用内积作为相似性度量。如图 [3](#S2.F3 "Figure 3 ‣ 2.1\. Large Language Model
    Inference ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based KVCache for
    Long Context LLM Inference") 所示，PQ 将每个候选嵌入向量划分为 $m$ 位，对应于质心。这些紧凑的 PQ 码使得在减少内存需求的情况下重构近似嵌入成为可能。在
    ANNS 过程中，查询嵌入向量与质心计算相似性，并使用 PQ 码聚合相似性，从而避免了对每个嵌入向量进行全面的相似性计算。'
- en: PQ has a profound impact on ANNS, with its principles integrated into various
    efficient ANNS methods (Johnson et al., [2021](#bib.bib28); Jayaram Subramanya
    et al., [2019](#bib.bib24); Baranchuk et al., [2018](#bib.bib7)). PQ has several
    variants, including Optimized PQ (Ge et al., [2013](#bib.bib19)), Residual Quantization (Martinez
    et al., [2014](#bib.bib37)), and SCaNN (Guo et al., [2020](#bib.bib20)). While
    PQ was initially designed for ANNS, its variants are also applied in various learning
    tasks (Lingle, [2023](#bib.bib31); van den Oord et al., [2017](#bib.bib54); Zhang
    et al., [2023b](#bib.bib62)) to achieve effective compression and efficient computation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PQ 对 ANNS 产生了深远的影响，其原理已融入多种高效的 ANNS 方法中 (Johnson et al., [2021](#bib.bib28);
    Jayaram Subramanya et al., [2019](#bib.bib24); Baranchuk et al., [2018](#bib.bib7))。PQ
    有几种变体，包括优化 PQ (Ge et al., [2013](#bib.bib19))、残差量化 (Martinez et al., [2014](#bib.bib37))
    和 SCaNN (Guo et al., [2020](#bib.bib20))。虽然 PQ 最初是为 ANNS 设计的，但其变体也被应用于各种学习任务中
    (Lingle, [2023](#bib.bib31); van den Oord et al., [2017](#bib.bib54); Zhang et
    al., [2023b](#bib.bib62))，以实现有效的压缩和高效的计算。
- en: '![Refer to caption](img/2ba06902bb77318cf4c325b84f5d55e5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ba06902bb77318cf4c325b84f5d55e5.png)'
- en: Figure 4\. An overview of PQCache. For simplicity, we only illustrate the process
    for a single transformer layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. PQCache 的概述。为简便起见，我们仅展示了单个 Transformer 层的过程。
- en: 2.3\. GPU-CPU Memory Hierarchy
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. GPU-CPU 内存层次结构
- en: 'Modern deep learning tasks heavily rely on GPUs for executing compute-intensive
    operations. The GPU-CPU structure forms a typical memory hierarchy: the more expensive
    GPU memory offers faster memory I/O speeds for computation, while the CPU memory,
    connected via PCIe or NVLink, provides lower bandwidth. As model parameters increase
    and the demand for intermediate results storage (such as KVCache) grows, CPUs
    are often employed to share the memory load. Numerous research studies in machine
    learning systems propose offloading certain model parameters or activations to
    the CPU memory (Nie et al., [2022](#bib.bib40); Ren et al., [2021](#bib.bib43);
    Rhu et al., [2016](#bib.bib46); Sheng et al., [2023](#bib.bib48)), thereby enhancing
    the overall performance of GPU-centric deep learning tasks. The primary challenge
    in this context is to effectively schedule memory I/O (or say GPU-CPU communication)
    in conjunction with GPU computation to efficiently hide the associated overhead.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习任务严重依赖GPU来执行计算密集型操作。GPU-CPU结构形成了典型的内存层次结构：更昂贵的GPU内存提供更快的内存I/O速度用于计算，而通过PCIe或NVLink连接的CPU内存提供较低的带宽。随着模型参数的增加和对中间结果存储（如KVCache）需求的增长，通常会使用CPU来分担内存负载。许多机器学习系统的研究建议将某些模型参数或激活卸载到CPU内存中（Nie
    et al., [2022](#bib.bib40)；Ren et al., [2021](#bib.bib43)；Rhu et al., [2016](#bib.bib46)；Sheng
    et al., [2023](#bib.bib48)），从而提高以GPU为中心的深度学习任务的整体性能。在这种情况下，主要挑战是有效调度内存I/O（即GPU-CPU通信）与GPU计算，以高效隐藏相关开销。
- en: 3\. PQCache
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. PQCache
- en: 'In this section, we introduce PQCache, a novel system-algorithm co-designed
    method to enable effective and efficient long context LLM inference with large-scale
    KVCache. Figure [4](#S2.F4 "Figure 4 ‣ 2.2\. Product Quantization ‣ 2\. Preliminary
    ‣ PQCache: Product Quantization-based KVCache for Long Context LLM Inference")
    provides an overview of PQCache, where the KVCache from the prefilling phase is
    first offloaded to CPU, compressed using PQ, then fetched on demand through MIPS
    during the decoding phase.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了PQCache，一种新颖的系统-算法协同设计方法，以实现大规模KVCache的有效和高效的长上下文LLM推理。图[4](#S2.F4 "图4
    ‣ 2.2\. 产品量化 ‣ 2\. 初步 ‣ PQCache：基于产品量化的长上下文LLM推理KVCache")提供了PQCache的概述，其中预填充阶段的KVCache首先卸载到CPU，使用PQ进行压缩，然后在解码阶段通过MIPS按需提取。
- en: 3.1\. Overview
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 概述
- en: 'We design PQCache to reserve all the KVCache in CPU, and selectively fetch
    relevant key-value pairs for self-attention computation. In long context inference
    scenario, the entire KVCache is too large for both attention computation and I/O
    communication within the memory hierarchy. Therefore, a common technique is to
    only perform attention on a subset of the key-value pairs, a process known as
    “selective attention”. According to previous research (Zhang et al., [2023a](#bib.bib64);
    Liu et al., [2023a](#bib.bib34); Xiao et al., [2024](#bib.bib58); Ribar et al.,
    [2023](#bib.bib47); Yang et al., [2024](#bib.bib60); Adnan et al., [2024](#bib.bib2);
    Ge et al., [2023](#bib.bib18); Wang and Gan, [2024](#bib.bib56); Ren and Zhu,
    [2024](#bib.bib45)), attention score is a proper metric to measure the importance
    or relevance of previous tokens. As shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.1\.
    Overview ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long
    Context LLM Inference"), we plot the attention score distributions at several
    randomly-selected positions on an example from the XSUM dataset (Narayan et al.,
    [2018](#bib.bib39)). The attention scores generally follow powerlaw distributions,
    indicating that a small part of tokens are more important than most other tokens.
    Therefore, we can only include those tokens with large scores for self-attention
    computation. Following prior works (Xiao et al., [2023](#bib.bib59); Han et al.,
    [2023](#bib.bib21); Zhang et al., [2023a](#bib.bib64)), we also include initial
    tokens and the most recent tokens (called local tokens) in attention computation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计PQCache以在CPU中保留所有KVCache，并选择性地提取相关的键值对用于自注意力计算。在长上下文推理场景中，整个KVCache对于注意力计算和内存层次结构中的I/O通信来说都过于庞大。因此，一种常见的技术是仅对部分键值对执行注意力计算，这个过程被称为“选择性注意力”。根据之前的研究（Zhang等，[2023a](#bib.bib64)；Liu等，[2023a](#bib.bib34)；Xiao等，[2024](#bib.bib58)；Ribar等，[2023](#bib.bib47)；Yang等，[2024](#bib.bib60)；Adnan等，[2024](#bib.bib2)；Ge等，[2023](#bib.bib18)；Wang和Gan，[2024](#bib.bib56)；Ren和Zhu，[2024](#bib.bib45)），注意力得分是衡量先前token重要性或相关性的合适指标。如图[5](#S3.F5
    "图 5 ‣ 3.1\. 概述 ‣ 3\. PQCache ‣ PQCache：基于产品量化的长上下文LLM推理KVCache")所示，我们在来自XSUM数据集的一个示例中绘制了在若干随机选定位置的注意力得分分布（Narayan等，[2018](#bib.bib39)）。注意力得分通常遵循幂律分布，表明少数token比大多数其他token更重要。因此，我们可以仅包含那些得分较高的token进行自注意力计算。根据之前的研究（Xiao等，[2023](#bib.bib59)；Han等，[2023](#bib.bib21)；Zhang等，[2023a](#bib.bib64)），我们也在注意力计算中包含了初始token和最近的token（称为局部token）。
- en: '![Refer to caption](img/8632292ec07ecfa68020a8c4a4d51e9e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8632292ec07ecfa68020a8c4a4d51e9e.png)'
- en: (a) Layer 3, head 25.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 第3层，第25头。
- en: '![Refer to caption](img/4302ce8ad337a2dc5a408d1ccb1d76aa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4302ce8ad337a2dc5a408d1ccb1d76aa.png)'
- en: (b) Layer 11, head 15.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 第11层，第15头。
- en: '![Refer to caption](img/b727b28f0216cea01d58ed4b46ec8215.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b727b28f0216cea01d58ed4b46ec8215.png)'
- en: (c) Layer 20, head 27.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 第20层，第27头。
- en: '![Refer to caption](img/b2a7afe7ba620da11fb082c1ebe81e53.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b2a7afe7ba620da11fb082c1ebe81e53.png)'
- en: (d) Layer 21, head 16.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 第21层，第16头。
- en: Figure 5\. Distributions of attention scores.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 注意力得分分布。
- en: 'As detailed in Section [2.1](#S2.SS1 "2.1\. Large Language Model Inference
    ‣ 2\. Preliminary ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), attention scores are calculated using a softmax function applied
    to the product of the current query and preceding keys. The procedure of identifying
    the top-$k$ keys with the highest scores fundamentally constitutes a Maximum Inner
    Product Search (MIPS) operation. Therefore, we try to leverage embedding retrieval
    techniques to enable effective selective attention and address the KVCache memory
    issue. Based on the observations above, we design PQCache, which offloads all
    the KVCache to CPU, and fetch only relevant tokens’ key-values pairs during the
    decoding phase. Calculating exact attention scores of all previous tokens involves
    costly I/O communication, which is unacceptable in long context LLM inference.
    Inspired by Approximate Nearest Neighbor Search (ANNS) (Johnson et al., [2021](#bib.bib28);
    Jayaram Subramanya et al., [2019](#bib.bib24); Baranchuk et al., [2018](#bib.bib7)),
    we leverage the light-weight Product Quantization (PQ) method (Jégou et al., [2011](#bib.bib25)),
    which compress the vectors by partitioning and K-Means clustering. Though there
    are other ANNS methods (e.g. graph-based methods (Malkov and Yashunin, [2020](#bib.bib36);
    Fu et al., [2019](#bib.bib14); Jayaram Subramanya et al., [2019](#bib.bib24)))
    that can achieve better recall performance, they suffer from a computationally
    expensive construction process which may hinder LLM inference.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[2.1](#S2.SS1 "2.1\. 大型语言模型推理 ‣ 2\. 初步 ‣ PQCache：基于产品量化的 KVCache 以支持长上下文
    LLM 推理")节中详细说明，注意力得分是通过将当前查询与之前的键的乘积应用到 softmax 函数来计算的。识别得分最高的 top-$k$ 键的过程本质上是一个最大内积搜索（MIPS）操作。因此，我们尝试利用嵌入检索技术来实现有效的选择性注意力并解决
    KVCache 内存问题。基于上述观察，我们设计了 PQCache，将所有 KVCache 转移到 CPU，并仅在解码阶段获取相关 token 的键值对。计算所有先前
    token 的确切注意力得分涉及昂贵的 I/O 通信，这在长上下文 LLM 推理中是不可接受的。受到近似最近邻搜索（ANNS）(Johnson 等，[2021](#bib.bib28);
    Jayaram Subramanya 等，[2019](#bib.bib24); Baranchuk 等，[2018](#bib.bib7))的启发，我们利用轻量级的产品量化（PQ）方法（Jégou
    等，[2011](#bib.bib25)），通过分区和 K-Means 聚类来压缩向量。虽然还有其他 ANNS 方法（例如图基方法（Malkov 和 Yashunin，[2020](#bib.bib36);
    Fu 等，[2019](#bib.bib14); Jayaram Subramanya 等，[2019](#bib.bib24)))可以实现更好的召回性能，但它们在构建过程中计算开销较大，可能会妨碍
    LLM 推理。
- en: In PQCache, we construct PQ at the prefilling phase and utilize PQ at the decoding
    phase. At the prefilling phase, we need to calculate all the input tokens’ keys
    and values for the self-attention module. After obtaining the keys, which have
    the shape of $(n,h_{kv},s,d_{h})$ and PQ codes of shape $(s,m)$ bits to store.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PQCache 中，我们在预填充阶段构建 PQ，并在解码阶段使用 PQ。在预填充阶段，我们需要计算所有输入 token 的键和值以供自注意力模块使用。在获得形状为
    $(n,h_{kv},s,d_{h})$ 的键和形状为 $(s,m)$ 位的 PQ 代码后进行存储。
- en: At the decoding phase, we first perform matrix multiplication between the query
    and the PQ centroids, then aggregate the results for all the tokens according
    to PQ codes. We can determine the top-$k$ key-value pairs from CPU, the self-attention
    computation continues with retrieved tokens. Unlike normal embedding retrieval
    tasks, in LLM inference, newly generated keys and values are added into the KVCache.
    These tokens are first regarded as local tokens and reserved in GPU. When they
    are evicted from the sliding window of local tokens, they are assigned PQ codes
    based on their nearest centroids.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码阶段，我们首先进行查询与 PQ 类别中心的矩阵乘法，然后根据 PQ 代码汇总所有 token 的结果。我们可以从 CPU 中确定 top-$k$
    键值对，自注意力计算继续与检索到的 token 进行。与普通的嵌入检索任务不同，在 LLM 推理中，新生成的键和值会被添加到 KVCache 中。这些 token
    首先被视为局部 token 并保存在 GPU 中。当它们被从局部 token 的滑动窗口中驱逐时，会根据其最接近的中心分配 PQ 代码。
- en: 3.2\. Complexity Analysis
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 复杂性分析
- en: During the prefilling phase, we do not modify the attention computation, so
    the complexity remains the same for both time and memory. The additional K-Means
    clustering process has an average complexity of $O(s\cdot m\cdot 2^{b}\cdot T)$,
    and $b$ and the memory complexity are much smaller than the original ones.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在预填充阶段，我们不会修改注意力计算，因此时间和内存的复杂性保持不变。额外的 K-Means 聚类过程的平均复杂度为 $O(s\cdot m\cdot
    2^{b}\cdot T)$，而 $b$ 和内存复杂度则远小于原始值。
- en: 'To facilitate efficient long context LLM inference, the design goal of PQCache
    is to provide overhead-agnostic service. Figure [6](#S3.F6 "Figure 6 ‣ 3.2\. Complexity
    Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long
    Context LLM Inference") illustrates the computation and communication involved
    in the PQCache-enabled LLM inference, covering both the prefilling and decoding
    phases. The original LLM computation is filled with blue color, while the computation
    and the communication introduced by PQCache are filled with green and red colors,
    which can be divided into four parts: (1) KVCache offloading and PQ structure
    fetching; (2) PQ construction using K-Means clustering; (3) Approximate top-$k$
    approximation, we employ distinct system design to eliminate these computation
    or communication overhead. The system design is detailed in the following sections.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于高效的长上下文LLM推断，PQCache的设计目标是提供与开销无关的服务。图[6](#S3.F6 "图6 ‣ 3.2\. 复杂度分析 ‣ 3\.
    PQCache ‣ PQCache：基于产品量化的KVCache用于长上下文LLM推断")展示了PQCache支持的LLM推断所涉及的计算和通信，包括预填充和解码阶段。原始LLM计算用蓝色填充，而PQCache引入的计算和通信用绿色和红色填充，这些可以分为四个部分：（1）KVCache卸载和PQ结构获取；（2）使用K-Means聚类构建PQ；（3）近似top-$k$近似，我们采用不同的系统设计来消除这些计算或通信开销。系统设计将在后续章节详细介绍。
- en: '![Refer to caption](img/b40eecdf754fd2267e5afbd553b3d1cf.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/b40eecdf754fd2267e5afbd553b3d1cf.png)'
- en: (a) Prefilling.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预填充。
- en: '![Refer to caption](img/5599496417a31f2816060c815e63b172.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5599496417a31f2816060c815e63b172.png)'
- en: (b) Decoding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 解码。
- en: Figure 6\. PQCache v.s. sequential scheduling.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. PQCache与顺序调度的对比。
- en: 3.3\. Prefilling Phase
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 预填充阶段
- en: 'At the prefilling phase, on obtaining the input tokens’ keys and values in
    each layer, they can attend to attention computation and be offloaded to CPU simultaneously.
    Given that the attention computation time scales quadratically with sequence length,
    while the communication time scales linearly, the communication can be fully overlapped
    in long context scenarios, as shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.3\. Prefilling
    Phase ‣ 3\. PQCache ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在预填充阶段，获取每层输入token的键和值后，可以进行注意力计算，并同时卸载到CPU。考虑到注意力计算时间与序列长度的平方成正比，而通信时间与序列长度线性相关，在长上下文场景下，通信可以完全重叠，如图[7](#S3.F7
    "图7 ‣ 3.3\. 预填充阶段 ‣ 3\. PQCache ‣ PQCache：基于产品量化的KVCache用于长上下文LLM推断")所示。
- en: 'The K-Means clustering is of great complexity according to Section [3.2](#S3.SS2
    "3.2\. Complexity Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"). To enable overhead-agnostic inference,
    we aim to fully utilize the idle CPU resources for clustering. However, as shown
    in Figure [7](#S3.F7 "Figure 7 ‣ 3.3\. Prefilling Phase ‣ 3\. PQCache ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), the clustering
    process on CPU, including PQ construction for all heads in each layer, consumes
    more time than one-layer transformer computation on GPU at the prefilling stage.
    This is because the computational capability of GPUs has grown rapidly over the
    past decades, whereas CPUs are not specifically designed for computationally intensive
    tasks. To address the issue, we propose an adaptive K-Means clustering process
    which limits the number of iterations for clustering, ensuring that the clustering
    can be overlapped by GPU computation. For any given models and devices, we profile
    the computation time of one-layer transformer and K-Means clustering under different
    sequence lengths. By modeling the relationship between computation time and sequence
    length, we can determine the maximum number of K-Means iterations that can overlap
    with the computation for any given sequence length. In Section [4.3.3](#S4.SS3.SSS3
    "4.3.3\. Trade-off between Time and Accuracy ‣ 4.3\. Efficiency ‣ 4\. Experiments
    ‣ PQCache: Product Quantization-based KVCache for Long Context LLM Inference"),
    we empirically study the trade-off between the efficiency and model quality of
    different clustering iterations.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '根据第[3.2](#S3.SS2 "3.2\. 复杂性分析 ‣ 3\. PQCache ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")节，K-Means
    聚类具有很大的复杂性。为了实现与开销无关的推理，我们旨在充分利用空闲的 CPU 资源进行聚类。然而，如图[7](#S3.F7 "图 7 ‣ 3.3\. 预填充阶段
    ‣ 3\. PQCache ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")所示，CPU 上的聚类过程，包括每层所有头的 PQ
    构造，耗时比 GPU 上的单层变换器计算在预填充阶段更多。这是因为过去几十年 GPU 的计算能力迅速增长，而 CPU 并非专门设计用于计算密集型任务。为了解决这个问题，我们提出了一种自适应
    K-Means 聚类过程，限制了聚类的迭代次数，确保聚类可以与 GPU 计算重叠。对于任何给定的模型和设备，我们在不同序列长度下分析了单层变换器和 K-Means
    聚类的计算时间。通过建模计算时间与序列长度之间的关系，我们可以确定对于任何给定序列长度，K-Means 迭代的最大次数。第[4.3.3](#S4.SS3.SSS3
    "4.3.3\. 时间与准确度的权衡 ‣ 4.3\. 效率 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")节中，我们实证研究了不同聚类迭代次数下效率与模型质量的权衡。'
- en: '![Refer to caption](img/39f9c0d5c4bd5627adf01dfdd611fa47.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/39f9c0d5c4bd5627adf01dfdd611fa47.png)'
- en: Figure 7\. The execution time of one-layer transformer computation, offloading,
    and clustering at the prefilling phase.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 单层变换器计算、卸载和在预填充阶段的聚类执行时间。
- en: 3.4\. Decoding Phase
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 解码阶段
- en: 'At the decoding phase, the constructed PQ structure needs to be utilized by
    the attention module in each layer. While the preceding computation is underway,
    the PQ centroids and codes of the current layer can be pre-fetched in parallel.
    Since the PQ structure consumes negligible memory according to Section [3.2](#S3.SS2
    "3.2\. Complexity Analysis ‣ 3\. PQCache ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"), its communication can directly overlap
    with decoding phase computation.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在解码阶段，构造的 PQ 结构需要被每一层的注意力模块利用。在前期计算进行时，可以并行预取当前层的 PQ 中心和代码。由于根据第[3.2](#S3.SS2
    "3.2\. 复杂性分析 ‣ 3\. PQCache ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")节，PQ 结构消耗的内存可以忽略不计，因此其通信可以直接与解码阶段的计算重叠。'
- en: 'Throughout the entire inference, the only communication that cannot be overlapped
    is the retrieval of the top-$k$ hit tokens, and use them to fetch tokens while
    updating the cache structure. We employ asynchronous updates to avoid additional
    overhead. Experimental results in Section [4.3.4](#S4.SS3.SSS4 "4.3.4\. Cache
    Hit-rate ‣ 4.3\. Efficiency ‣ 4\. Experiments ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference") illustrate the cache hit-rate, which
    helps reduce overall communication.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在整个推理过程中，唯一不能重叠的通信是检索前 $k$ 个命中标记，并在更新缓存结构的同时使用它们获取标记。我们采用异步更新以避免额外的开销。第[4.3.4](#S4.SS3.SSS4
    "4.3.4\. 缓存命中率 ‣ 4.3\. 效率 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")节的实验结果展示了缓存命中率，这有助于减少整体通信。'
- en: 4\. Experiments
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: In this section, we conduct experiments and compare PQCache with existing methods.
    We experimentally show that PQCache achieves both effectiveness and efficiency.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行实验并比较PQCache与现有方法。我们实验证明PQCache在效果和效率方面都表现出色。
- en: 4.1\. Experimental Setup
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验设置
- en: 4.1.1\. Models
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 模型
- en: 'We conduct experiments using two representative open-source LLMs: LLaMA-2-7B-Chat (Touvron
    et al., [2023](#bib.bib53)) and Mistral-7B-Instruct-v0.2 (Jiang et al., [2023a](#bib.bib26)).
    The former employs MHA and supports 4K context length, while the latter uses GQA
    and supports 32K context length. Both models share similar LLM architectures.
    We use FP16 for both models, which is a common practice in LLM inference.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个代表性的开源LLM进行实验：LLaMA-2-7B-Chat（Touvron等，[2023](#bib.bib53)）和Mistral-7B-Instruct-v0.2（Jiang等，[2023a](#bib.bib26)）。前者采用MHA并支持4K上下文长度，而后者使用GQA并支持32K上下文长度。这两个模型具有类似的LLM架构。我们对两个模型都使用FP16，这是LLM推理中的常见做法。
- en: 4.1.2\. Tasks
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 任务
- en: We evaluate PQCache on LongBench (Bai et al., [2023](#bib.bib6)), a widely-used
    benchmark for long-context LLM inference. Since the models are mainly pre-trained
    on English data, we assess all the English tasks within the benchmark. These tasks
    include document question answering, summarization, few-shot learning, and passage
    retrieval. Samples in LongBench have an average input token length of 8K.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LongBench（Bai等，[2023](#bib.bib6)）上评估PQCache，这是一个广泛使用的长上下文LLM推理基准。由于这些模型主要在英文数据上进行预训练，我们评估了基准中的所有英文任务。这些任务包括文档问答、总结、少样本学习和段落检索。LongBench的样本平均输入标记长度为8K。
- en: 'We also experiment on two additional tasks: the Needle-in-a-Haystack (Kamradt,
    [2024](#bib.bib29)) and the GSM8k Chain-of-Thought (CoT) reasoning (Wei et al.,
    [2022](#bib.bib57)). The Needle-in-a-Haystack test evaluates the in-context retrieval
    ability of long-context LLMs, asking the model to retrieve a random fact or statement
    placed within a lengthy document. In our experiments, we consider up to 30K document
    length. GSM8k is a math reasoning dataset containing 8K high quality diverse grade
    school math problems. Its CoT variant is a complex reasoning task that require
    model to attend to extensive contextual details for accurate answers, with an
    average input length of 3.7K.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在两个额外任务上进行实验：Needle-in-a-Haystack（Kamradt，[2024](#bib.bib29)）和GSM8k链式推理（CoT）（Wei等，[2022](#bib.bib57)）。Needle-in-a-Haystack测试评估了长上下文LLM的上下文检索能力，要求模型检索放置在长文档中的随机事实或陈述。在我们的实验中，我们考虑了最长达30K的文档长度。GSM8k是一个包含8K高质量不同年级数学问题的数学推理数据集。其CoT变体是一个复杂的推理任务，要求模型关注大量上下文细节以获得准确答案，平均输入长度为3.7K。
- en: 4.1.3\. Baselines
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 基准线
- en: We consider H2O (Zhang et al., [2023a](#bib.bib64)), SPARQ (Ribar et al., [2023](#bib.bib47)),
    and InfLLM (Xiao et al., [2024](#bib.bib58)) as our baselines. H2O is the most
    widely-used method of KVCache dropping and has been the basis for many enhancements.
    SPARQ and InfLLM are the state-of-the-art methods of KVCache offloading. In addition,
    we further consider a method that retrieves the exact top-$k$ tokens for each
    head, denoted as Oracle. In our experiments, we align the number of tokens for
    selective attention and the data transfer amount in Oracle, SPARQ, InfLLM, and
    PQCache, to achieve a fair comparison. We allow H2O to attend to more tokens,
    matching the memory usage of the selected key-value pairs and the data transfer
    amount in the other methods, following the experiment settings in SPARQ. We refer
    to this baseline as H2O(C), where “C” means compensation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将H2O（Zhang等，[2023a](#bib.bib64)）、SPARQ（Ribar等，[2023](#bib.bib47)）和InfLLM（Xiao等，[2024](#bib.bib58)）作为基准线。H2O是最广泛使用的KVCache丢弃方法，并且已经成为许多改进的基础。SPARQ和InfLLM是最先进的KVCache卸载方法。此外，我们还考虑了一种检索每个头部精确前$k$个标记的方法，称为Oracle。在我们的实验中，我们使Oracle、SPARQ、InfLLM和PQCache在选择性注意力的标记数量和数据传输量上保持一致，以实现公平比较。我们允许H2O处理更多标记，与其他方法中选择的键值对的内存使用和数据传输量匹配，遵循SPARQ的实验设置。我们将这个基准线称为H2O(C)，其中“C”表示补偿。
- en: Table 2\. LongBench evaluation of the Mistral-7B-inst-v0.2 GQA model (32K context
    length). InfLLM, SPARQ, and PQCache all involve extra communications at an amount
    of 1/128 KVCache memory for pre-calculating relevance; H2O attends to more tokens
    where the memory equals to other methods’ selected tokens and transferred data
    amount.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. LongBench对Mistral-7B-inst-v0.2 GQA模型（32K上下文长度）的评估。InfLLM、SPARQ和PQCache都涉及额外的通信，约占1/128
    KVCache内存用于预计算相关性；H2O处理更多的标记，其内存等于其他方法选择的标记和传输的数据量。
- en: '|  |  | 1/5 #Tokens + 1/128 Extra Comm | 1/10 #Tokens + 1/128 Extra Comm |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1/5 #Tokens + 1/128 额外通信 | 1/10 #Tokens + 1/128 额外通信 |'
- en: '| Dataset | Full | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C)
    | InfLLM | SPARQ | PQCache |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Full | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C)
    | InfLLM | SPARQ | PQCache |'
- en: '| NarrativeQA | 21.27 | 22.18 | 22.07 | 19.90 | 22.25 | 22.35 | 22.34 | 21.54
    | 17.55 | 22.45 | 22.62 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 21.27 | 22.18 | 22.07 | 19.90 | 22.25 | 22.35 | 22.34 | 21.54
    | 17.55 | 22.45 | 22.62 |'
- en: '| Qasper | 29.22 | 28.62 | 23.43 | 19.24 | 19.95 | 28.26 | 27.90 | 21.19 |
    14.76 | 17.69 | 28.29 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 29.22 | 28.62 | 23.43 | 19.24 | 19.95 | 28.26 | 27.90 | 21.19 |
    14.76 | 17.69 | 28.29 |'
- en: '| MultiFieldQA | 47.84 | 48.02 | 43.31 | 41.13 | 39.22 | 48.27 | 48.23 | 39.22
    | 36.66 | 35.02 | 47.95 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 47.84 | 48.02 | 43.31 | 41.13 | 39.22 | 48.27 | 48.23 | 39.22
    | 36.66 | 35.02 | 47.95 |'
- en: '| HotpotQA | 37.92 | 37.16 | 36.86 | 33.97 | 33.48 | 37.12 | 36.74 | 33.02
    | 31.09 | 31.68 | 36.24 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 37.92 | 37.16 | 36.86 | 33.97 | 33.48 | 37.12 | 36.74 | 33.02
    | 31.09 | 31.68 | 36.24 |'
- en: '| 2WikiMQA | 21.83 | 21.02 | 18.16 | 18.48 | 16.68 | 21.25 | 21.16 | 17.76
    | 16.15 | 16.14 | 21.21 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 21.83 | 21.02 | 18.16 | 18.48 | 16.68 | 21.25 | 21.16 | 17.76
    | 16.15 | 16.14 | 21.21 |'
- en: '| Musique | 18.58 | 18.45 | 17.77 | 18.96 | 16.10 | 18.37 | 18.34 | 16.85 |
    14.08 | 13.18 | 18.47 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Musique | 18.58 | 18.45 | 17.77 | 18.96 | 16.10 | 18.37 | 18.34 | 16.85 |
    14.08 | 13.18 | 18.47 |'
- en: '| GovReport | 31.57 | 31.98 | 29.13 | 30.49 | 27.12 | 31.53 | 31.84 | 27.36
    | 29.19 | 24.68 | 31.12 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 31.57 | 31.98 | 29.13 | 30.49 | 27.12 | 31.53 | 31.84 | 27.36
    | 29.19 | 24.68 | 31.12 |'
- en: '| QMSum | 24.31 | 23.76 | 23.23 | 22.64 | 22.21 | 23.79 | 23.98 | 23.12 | 21.52
    | 22.09 | 23.27 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 24.31 | 23.76 | 23.23 | 22.64 | 22.21 | 23.79 | 23.98 | 23.12 | 21.52
    | 22.09 | 23.27 |'
- en: '| MultiNews | 26.85 | 26.79 | 25.37 | 24.38 | 23.77 | 26.70 | 26.86 | 24.94
    | 23.19 | 22.40 | 26.53 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews | 26.85 | 26.79 | 25.37 | 24.38 | 23.77 | 26.70 | 26.86 | 24.94
    | 23.19 | 22.40 | 26.53 |'
- en: '| TREC | 71.00 | 71.00 | 68.00 | 59.50 | 62.00 | 71.00 | 71.00 | 67.50 | 53.00
    | 55.50 | 70.50 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 71.00 | 71.00 | 68.00 | 59.50 | 62.00 | 71.00 | 71.00 | 67.50 | 53.00
    | 55.50 | 70.50 |'
- en: '| TriviaQA | 86.23 | 86.22 | 86.17 | 85.80 | 86.58 | 86.14 | 86.40 | 86.45
    | 85.54 | 85.98 | 86.40 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA | 86.23 | 86.22 | 86.17 | 85.80 | 86.58 | 86.14 | 86.40 | 86.45
    | 85.54 | 85.98 | 86.40 |'
- en: '| SAMSum | 43.04 | 43.27 | 42.61 | 41.51 | 42.57 | 43.35 | 43.47 | 42.21 |
    39.55 | 42.79 | 43.13 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 43.04 | 43.27 | 42.61 | 41.51 | 42.57 | 43.35 | 43.47 | 42.21 |
    39.55 | 42.79 | 43.13 |'
- en: '| Count | 2.62 | 3.42 | 3.50 | 2.96 | 4.80 | 3.93 | 3.26 | 2.44 | 2.00 | 3.15
    | 3.56 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Count | 2.62 | 3.42 | 3.50 | 2.96 | 4.80 | 3.93 | 3.26 | 2.44 | 2.00 | 3.15
    | 3.56 |'
- en: '| Retrieval | 88.74 | 88.40 | 56.56 | 35.67 | 57.19 | 88.44 | 89.80 | 37.41
    | 20.00 | 39.39 | 88.63 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Retrieval | 88.74 | 88.40 | 56.56 | 35.67 | 57.19 | 88.44 | 89.80 | 37.41
    | 20.00 | 39.39 | 88.63 |'
- en: '| Average | 39.32 | 39.30 | 35.44 | 32.48 | 33.85 | 39.32 | 39.38 | 32.93 |
    28.88 | 30.87 | 39.14 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Average | 39.32 | 39.30 | 35.44 | 32.48 | 33.85 | 39.32 | 39.38 | 32.93 |
    28.88 | 30.87 | 39.14 |'
- en: Table 3\. LongBench evaluation of the LLaMA-2-7B-Chat MHA model (4K context
    length). InfLLM, SPARQ, and PQCache all involve extra communications at an amount
    of 1/128 KVCache memory for pre-calculating relevance; H2O attends to more tokens
    where the memory equals to other methods’ selected tokens and transferred data
    amount.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Table 3\. LongBench 对 LLaMA-2-7B-Chat MHA 模型的评估（4K 上下文长度）。InfLLM、SPARQ 和 PQCache
    都涉及额外的通信，使用 1/128 KVCache 内存来预计算相关性；H2O 处理更多的标记，其内存等于其他方法的选择标记和转移数据量。
- en: '|  |  | 1/5 #Tokens + 1/128 Extra Comm | 1/10 #Tokens + 1/128 Extra Comm |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1/5 #Tokens + 1/128 额外通信 | 1/10 #Tokens + 1/128 额外通信 |'
- en: '| Dataset | Full | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C)
    | InfLLM | SPARQ | PQCache |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Full | Oracle | H2O(C) | InfLLM | SPARQ | PQCache | Oracle | H2O(C)
    | InfLLM | SPARQ | PQCache |'
- en: '| NarrativeQA | 18.78 | 18.55 | 17.53 | 12.45 | 17.40 | 19.01 | 18.05 | 17.04
    | 11.88 | 16.44 | 17.56 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 18.78 | 18.55 | 17.53 | 12.45 | 17.40 | 19.01 | 18.05 | 17.04
    | 11.88 | 16.44 | 17.56 |'
- en: '| Qasper | 22.11 | 20.40 | 19.37 | 14.0 | 20.21 | 21.07 | 20.38 | 18.33 | 12.48
    | 17.31 | 20.08 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 22.11 | 20.40 | 19.37 | 14.0 | 20.21 | 21.07 | 20.38 | 18.33 | 12.48
    | 17.31 | 20.08 |'
- en: '| MultiFieldQA | 36.77 | 37.58 | 31.69 | 29.39 | 33.21 | 38.72 | 37.23 | 31.82
    | 23.32 | 31.27 | 37.79 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 36.77 | 37.58 | 31.69 | 29.39 | 33.21 | 38.72 | 37.23 | 31.82
    | 23.32 | 31.27 | 37.79 |'
- en: '| HotpotQA | 27.83 | 28.25 | 26.44 | 25.67 | 24.38 | 27.78 | 27.79 | 25.72
    | 25.01 | 23.66 | 26.36 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 27.83 | 28.25 | 26.44 | 25.67 | 24.38 | 27.78 | 27.79 | 25.72
    | 25.01 | 23.66 | 26.36 |'
- en: '| 2WikiMQA | 31.51 | 31.96 | 29.65 | 23.95 | 29.99 | 31.23 | 31.08 | 30.68
    | 25.73 | 29.02 | 30.00 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMQA | 31.51 | 31.96 | 29.65 | 23.95 | 29.99 | 31.23 | 31.08 | 30.68
    | 25.73 | 29.02 | 30.00 |'
- en: '| Musique | 8.31 | 8.23 | 8.64 | 9.03 | 7.01 | 7.65 | 8.19 | 8.40 | 8.68 |
    4.91 | 7.24 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Musique | 8.31 | 8.23 | 8.64 | 9.03 | 7.01 | 7.65 | 8.19 | 8.40 | 8.68 |
    4.91 | 7.24 |'
- en: '| GovReport | 26.91 | 26.86 | 23.62 | 23.54 | 24.04 | 26.91 | 26.50 | 22.89
    | 23.22 | 22.47 | 26.69 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 26.91 | 26.86 | 23.62 | 23.54 | 24.04 | 26.91 | 26.50 | 22.89
    | 23.22 | 22.47 | 26.69 |'
- en: '| QMSum | 20.68 | 20.31 | 20.97 | 19.11 | 21.03 | 20.94 | 20.57 | 20.76 | 18.69
    | 20.48 | 21.33 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| QMSum | 20.68 | 20.31 | 20.97 | 19.11 | 21.03 | 20.94 | 20.57 | 20.76 | 18.69
    | 20.48 | 21.33 |'
- en: '| MultiNews | 26.23 | 26.08 | 24.31 | 22.32 | 25.15 | 26.46 | 26.44 | 24.28
    | 20.52 | 23.48 | 26.57 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| MultiNews | 26.23 | 26.08 | 24.31 | 22.32 | 25.15 | 26.46 | 26.44 | 24.28
    | 20.52 | 23.48 | 26.57 |'
- en: '| TREC | 64.00 | 64.00 | 62.50 | 48.0 | 62.50 | 64.00 | 64.00 | 60.50 | 40.0
    | 60.50 | 63.50 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| TREC | 64.00 | 64.00 | 62.50 | 48.0 | 62.50 | 64.00 | 64.00 | 60.50 | 40.0
    | 60.50 | 63.50 |'
- en: '| TriviaQA | 83.26 | 83.16 | 82.56 | 71.15 | 81.09 | 83.43 | 81.74 | 81.27
    | 61.45 | 80.99 | 83.23 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA | 83.26 | 83.16 | 82.56 | 71.15 | 81.09 | 83.43 | 81.74 | 81.27
    | 61.45 | 80.99 | 83.23 |'
- en: '| SAMSum | 41.53 | 41.31 | 40.07 | 37.3 | 36.90 | 41.67 | 41.26 | 39.76 | 34.6
    | 39.79 | 41.19 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 41.53 | 41.31 | 40.07 | 37.3 | 36.90 | 41.67 | 41.26 | 39.76 | 34.6
    | 39.79 | 41.19 |'
- en: '| Count | 2.92 | 2.98 | 2.48 | 2.68 | 2.35 | 3.03 | 3.80 | 2.68 | 2.92 | 2.58
    | 3.01 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 2.92 | 2.98 | 2.48 | 2.68 | 2.35 | 3.03 | 3.80 | 2.68 | 2.92 | 2.58
    | 3.01 |'
- en: '| Retrieval | 8.00 | 7.50 | 6.50 | 6.25 | 5.50 | 7.00 | 5.50 | 4.50 | 5.00
    | 6.50 | 6.50 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Retrieval | 8.00 | 7.50 | 6.50 | 6.25 | 5.50 | 7.00 | 5.50 | 4.50 | 5.00
    | 6.50 | 6.50 |'
- en: '| Average | 29.92 | 29.80 | 28.31 | 24.66 | 27.91 | 29.92 | 29.47 | 27.76 |
    22.39 | 27.10 | 29.36 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 29.92 | 29.80 | 28.31 | 24.66 | 27.91 | 29.92 | 29.47 | 27.76 | 22.39
    | 27.10 | 29.36 |'
- en: '![Refer to caption](img/39ef0eadd8c00b88ba97bc74a983c3e0.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/39ef0eadd8c00b88ba97bc74a983c3e0.png)'
- en: (a) H2O(C).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) H2O(C)。
- en: '![Refer to caption](img/2cc2129de5a5ab9667deb5e56a4bc55b.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2cc2129de5a5ab9667deb5e56a4bc55b.png)'
- en: (b) SPARQ.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SPARQ。
- en: '![Refer to caption](img/924d1b2085916962d665c270bc8b72d5.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/924d1b2085916962d665c270bc8b72d5.png)'
- en: (c) InfLLM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (c) InfLLM。
- en: '![Refer to caption](img/fd192a1e160baefbfeed2d755175e495.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fd192a1e160baefbfeed2d755175e495.png)'
- en: (d) PQCache.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (d) PQCache。
- en: Figure 8\. Experimental results of the Needle-in-a-Haystack test.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. Needle-in-a-Haystack 测试的实验结果。
- en: 4.1.4\. Hardware Environment and Hyperparameters
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 硬件环境和超参数
- en: We conduct all experiments on NVIDIA A800 40GB GPU cards. Most of the hyperparameters
    are determined based on the number of tokens and the amount of data transferred.
    We use $m=2$ for PQ by default. For other hyperparameters, we align them with
    the settings from the corresponding papers or open-source codes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 NVIDIA A800 40GB GPU 卡上进行所有实验。大多数超参数是根据标记数量和传输的数据量来确定的。PQ 默认使用 $m=2$。对于其他超参数，我们将其与相关论文或开源代码中的设置对齐。
- en: 4.2\. Model Performance
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 模型性能
- en: 4.2.1\. LongBench
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. LongBench
- en: 'The LongBench results of the methods on two LLMs are presented in Table [2](#S4.T2
    "Table 2 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference") and [3](#S4.T3
    "Table 3 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"). LongBench
    uses different metrics for each dataset and calculates an average score to measure
    overall performance. We consider including 1/5 and 1/10 of the input tokens in
    selective attention, respectively, with an extra communication that equals to
    1/128 of the KVCache memory: for PQCache, we use $m=2$; for InfLLM, we use 1 representative
    token from every 128 tokens. H2O(C) is allowed to attend to more tokens as introduced
    in Section [4.1.3](#S4.SS1.SSS3 "4.1.3\. Baselines ‣ 4.1\. Experimental Setup
    ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"). Excluding Oracle, the best results for each setting, are highlighted
    in bold.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S4.T2 "表 2 ‣ 4.1.3\. 基线 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache
    用于长上下文 LLM 推理")和[3](#S4.T3 "表 3 ‣ 4.1.3\. 基线 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ PQCache:
    基于产品量化的 KVCache 用于长上下文 LLM 推理")展示了两种 LLM 方法的 LongBench 结果。LongBench 为每个数据集使用不同的指标，并计算平均分以衡量整体性能。我们考虑在选择性注意力中包括
    1/5 和 1/10 的输入标记，并额外增加相当于 1/128 的 KVCache 内存的通信：对于 PQCache，我们使用 $m=2$；对于 InfLLM，我们使用每
    128 个标记中的 1 个代表性标记。H2O(C) 被允许关注更多的标记，如第[4.1.3](#S4.SS1.SSS3 "4.1.3\. 基线 ‣ 4.1\.
    实验设置 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理")节所述。除去 Oracle，每个设置的最佳结果用粗体标出。'
- en: On average, models without compression (denoted as Full) can achieve the best
    results, since there is nearly no information loss¹¹1In LongBench, when the sequence
    length exceeds the model’s maximum context length, only the initial and the last
    tokens are used (Bai et al., [2023](#bib.bib6)), resulting in information loss.
    . PQCache outperforms the major baselines (i.e., H2O(C), InfLLM, and SPARQ) on
    most of the datasets. Although PQCache achieves slightly lower scores in a handful
    of cases, it exhibits substantial improvements on average. Concretely, PQCache
    achieves +3.88 and +6.21 improvements on Mistral-7B, and achieves +1.61 and +1.60
    improvements on LLaMa2-7B, respectively. Note that the offloading-based counterparts,
    InfLLM and SPARQ, have the worst performance on average due to the limited additional
    communication for minimized latency. In contrast, PQCache performs well under
    the same constraint, validating the strength of our work.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，未压缩的模型（标记为 Full）能够达到最佳结果，因为几乎没有信息损失¹¹1 在 LongBench 中，当序列长度超过模型的最大上下文长度时，仅使用初始和最后的标记（Bai
    et al., [2023](#bib.bib6)），导致信息损失。 。 PQCache 在大多数数据集上优于主要基准（即 H2O(C)、InfLLM 和
    SPARQ）。虽然 PQCache 在少数情况下得分略低，但它在平均上表现出显著改善。具体来说，PQCache 在 Mistral-7B 上提高了 +3.88
    和 +6.21，在 LLaMa2-7B 上提高了 +1.61 和 +1.60。注意，基于卸载的对手，InfLLM 和 SPARQ，平均表现最差，因为它们的额外通信有限以最小化延迟。相比之下，PQCache
    在相同限制下表现良好，验证了我们工作的优势。
- en: Oracle is an ideal approach with the exact top-$k$ tokens for selective attention,
    which gives excellent performance in most cases. However, we observe that PQCache
    even beats Oracle and achieves the same score as the uncompressed counterpart
    in the “1/5#Tokens” cases. This suggests that clustering may help PQCache uncover
    intrinsic structures within the KVCache latent space, thus leading to promising
    results. Furthermore, although it is usually expected that the performance should
    drop when there are fewer tokens, there are exceptions where the opposite happens.
    This could be because not all tokens are useful for generating new ones, so getting
    rid of unnecessary ones might enhance inference.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle 是一种理想的方法，能够准确找到前 $k$ 个选择性注意的标记，在大多数情况下表现出色。然而，我们观察到 PQCache 甚至超过了 Oracle，并且在“1/5#Tokens”情况下达到了与未压缩版本相同的得分。这表明聚类可能有助于
    PQCache 发掘 KVCache 潜在空间中的内在结构，从而带来有希望的结果。此外，虽然通常预期当标记较少时性能会下降，但也有例外情况发生了相反的情况。这可能是因为并非所有标记都对生成新标记有用，因此去除不必要的标记可能会提高推理效果。
- en: 4.2.2\. Needle-in-a-Haystack
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 针对“干草堆中的针”
- en: 'We use Mistral-7B-inst-v0.2 for this test. We employ the common setting (Anthropic,
    [2023](#bib.bib5)): the “haystack” is Paul Graham’s Essays, and the “needle” is
    “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park
    on a sunny day.” For each experiment, we use 1/5 the number of tokens in selective
    attention, and 1/128 extra communication. The results are shown in Figure [8](#S4.F8
    "Figure 8 ‣ 4.1.3\. Baselines ‣ 4.1\. Experimental Setup ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), where the
    $x$-axis represents the position that the “needle” hides. Greener shades indicate
    greater accuracy.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Mistral-7B-inst-v0.2 进行测试。我们采用常见设置（Anthropic，[2023](#bib.bib5)）： “haystack”
    是 Paul Graham 的《Essays》，而“needle” 是“在旧金山最好的事就是吃三明治，阳光明媚的日子坐在 Dolores 公园。” 对于每次实验，我们在选择性注意中使用
    1/5 数量的标记，以及 1/128 的额外通信。结果显示在图 [8](#S4.F8 "Figure 8 ‣ 4.1.3\. Baselines ‣ 4.1\.
    Experimental Setup ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference") 中，其中 $x$ 轴表示“needle”隐藏的位置。绿色阴影表示更高的准确性。'
- en: Among all the methods, PQCache achieves the best performance, successfully locating
    the needle in nearly all scenarios. The major baselines, however, fail to retrieve
    the needle in a substantial amount of cases. InfLLM, in particular, struggles
    to find the needle in most cases, possibly due to its reliance on block-partitioning
    and the needle not being considered as representative tokens. It can locate the
    needle when it is among the initial or local tokens, which we include in attention.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有方法中，PQCache 达到了最佳性能，几乎在所有场景中都能成功定位目标。主要基准方法在大量情况下未能检索到目标。然而，InfLLM 特别难以在大多数情况下找到目标，这可能是因为它依赖于块划分且目标未被视为代表性标记。当目标在最初的或本地标记中时，InfLLM
    能够找到目标，这些标记被包含在注意力中。
- en: 4.2.3\. GSM8k CoT Reasoning
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. GSM8k CoT 推理
- en: 'We use Mistral-7B-inst-v0.2 for this task. Our prompt strategy involves 8 questions
    with 9-step reasoning and 2 questions with 8-step reasoning per sample, a common
    setup for long context inference (Fu et al., [2023](#bib.bib15)). We use 1/128
    extra communications. As shown in Figure [9(a)](#S4.F9.sf1 "In Figure 9 ‣ 4.2.4\.
    Impact of Extra Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), PQCache consistently
    outperforms H2O, SPARQ, and InfLLM under varying token counts. Some results even
    surpass the uncompressed counterpart, suggesting that using part of the tokens
    can lead to improvements. Contrary to previous findings (Kang et al., [2024](#bib.bib30)),
    we observe that H2O performs well using the advanced Mistral model. H2O(C) performs
    better than PQCache using 1/10 number of tokens, as it is allowed to access more
    tokens.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 Mistral-7B-inst-v0.2 完成此任务。我们的提示策略涉及每个样本 8 个问题，每个问题 9 步推理，以及 2 个问题，每个问题
    8 步推理，这是长上下文推理的常见设置（Fu 等， [2023](#bib.bib15)）。我们使用 `1/128` 的额外通信。如图 [9(a)](#S4.F9.sf1
    "图 9 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型表现 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文
    LLM 推理") 所示，PQCache 在不同 token 数量下始终优于 H2O、SPARQ 和 InfLLM。一些结果甚至超过了未压缩的对应物，表明使用部分
    token 可以带来改进。与之前的发现相反（Kang 等， [2024](#bib.bib30)），我们观察到 H2O 在使用先进的 Mistral 模型时表现良好。H2O(C)
    在使用 `1/10` 数量的 token 时表现优于 PQCache，因为它可以访问更多的 token。'
- en: 4.2.4\. Impact of Extra Communication
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 额外通信的影响
- en: 'We investigate how the amount of extra communication impacts the model performance
    on the HotPotQA dataset, as shown in Figure [9(b)](#S4.F9.sf2 "In Figure 9 ‣ 4.2.4\.
    Impact of Extra Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"). Fixing 1/10
    tokens used, as the amount of extra communication increases from 1/128 to 1/16
    of the KVCache memory, InfLLM and PQCache show relatively stable performance,
    while SPARQ has steadily improved performance. PQCache consistently achieves high
    scores, outperforming the other methods when the communication amount is no larger
    than 1/32 KVCache. SPARQ already incurs significant latency under the 1/128 case,
    as shown in Section [4.3.2](#S4.SS3.SSS2 "4.3.2\. Decoding ‣ 4.3\. Efficiency
    ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"). Even it performs well under the 1/16 case, the latency becomes
    increasingly unacceptable. In low-communication scenarios, which are suitable
    for practical usage, PQCache achieves the best model performance.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了额外通信量如何影响 HotPotQA 数据集上的模型表现，如图 [9(b)](#S4.F9.sf2 "图 9 ‣ 4.2.4\. 额外通信的影响
    ‣ 4.2\. 模型表现 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache 用于长上下文 LLM 推理") 所示。在固定使用 `1/10`
    token 的情况下，随着额外通信量从 `1/128` 增加到 `1/16` 的 KVCache 内存，InfLLM 和 PQCache 显示出相对稳定的性能，而
    SPARQ 的性能稳步提升。PQCache 一直保持高分，当通信量不超过 `1/32` KVCache 时超越了其他方法。SPARQ 在 `1/128` 情况下已经产生了显著的延迟，如第
    [4.3.2](#S4.SS3.SSS2 "4.3.2\. 解码 ‣ 4.3\. 效率 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的 KVCache
    用于长上下文 LLM 推理") 节所示。即使在 `1/16` 的情况下表现良好，延迟变得越来越不可接受。在低通信场景中，PQCache 实现了最佳的模型表现，这些场景适合实际使用。'
- en: '![Refer to caption](img/c55dcb80cd148d43455e6e7109b4eb2f.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c55dcb80cd148d43455e6e7109b4eb2f.png)'
- en: (a) Results on GSM8k CoT.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在 GSM8k CoT 上的结果。
- en: '![Refer to caption](img/9ad194a9dfd2ee82308306ca87dbb34a.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9ad194a9dfd2ee82308306ca87dbb34a.png)'
- en: (b) Varying communications.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 不同的通信量。
- en: '![Refer to caption](img/f99d6d6760ba843f094bc193a89e42d3.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f99d6d6760ba843f094bc193a89e42d3.png)'
- en: (c) Different PQ configurations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 不同的 PQ 配置。
- en: Figure 9\. Model performance on GSM8k CoT, and other hyper-parameters.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. GSM8k CoT 模型表现及其他超参数。
- en: '![Refer to caption](img/694eb44b8b9e6f74d2b5bf9ec159d378.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/694eb44b8b9e6f74d2b5bf9ec159d378.png)'
- en: (a) Time to 2nd token.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 到第二个 token 的时间。
- en: '![Refer to caption](img/d9b5ba6c88cc1e7746518c58b46ad332.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d9b5ba6c88cc1e7746518c58b46ad332.png)'
- en: (b) Time per output token.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 每个输出 token 的时间。
- en: '![Refer to caption](img/aeeff48a5d3e0e0b2c2efe85c0b03830.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aeeff48a5d3e0e0b2c2efe85c0b03830.png)'
- en: (c) Trade-off time and score.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 时间与分数的权衡。
- en: '![Refer to caption](img/2cd740ddd608c22119e67a94f36a41bb.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2cd740ddd608c22119e67a94f36a41bb.png)'
- en: (d) Cache hit-rate.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 缓存命中率。
- en: Figure 10\. Latency experiments.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 延迟实验。
- en: 4.2.5\. Impact of PQ Configuration
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. PQ 配置的影响
- en: We evaluate the effect of PQ configurations. Since PQ has a memory consumption
    of $O(s\cdot m\cdot b+2^{b}\cdot d_{h})$ offers more stable latency and lower
    memory usage while still delivering promising model performance. Therefore, we
    choose it as the default configuration.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了PQ配置的效果。由于PQ的内存消耗为$O(s\cdot m\cdot b+2^{b}\cdot d_{h})$，它提供了更稳定的延迟和较低的内存使用，同时仍然提供了令人满意的模型性能。因此，我们选择它作为默认配置。
- en: 4.3\. Efficiency
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 效率
- en: 4.3.1\. Prefilling
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 预填充
- en: 'In PQCache, K-Means clustering occurs concurrently with GPU computation. While
    it doesn’t affect the first token generation, subsequent tokens depend on clustering
    results. To assess system optimization, we use Time To 2nd Token (TT2T), considering
    query entry to LLM output time and KVCache management overhead. As shown in Figure [10(a)](#S4.F10.sf1
    "In Figure 10 ‣ 4.2.4\. Impact of Extra Communication ‣ 4.2\. Model Performance
    ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"), with overlapping and adaptive clustering, PQCache can achieve
    the lowest TT2T. All baseline methods have significant overhead. Since H2O collects
    attention scores during prefilling, it cannot utilize FlashAttention for acceleration
    and encounters OOM when dealing with lengthy input. SPARQ has no prefilling overhead,
    but its decoding process is slow (see Section [4.3.2](#S4.SS3.SSS2 "4.3.2\. Decoding
    ‣ 4.3\. Efficiency ‣ 4\. Experiments ‣ PQCache: Product Quantization-based KVCache
    for Long Context LLM Inference")). InfLLM incurs time overhead due to the setup
    required for block-level KVCache management.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '在PQCache中，K-Means聚类与GPU计算并行进行。虽然这不会影响第一个token的生成，但随后的tokens依赖于聚类结果。为了评估系统优化，我们使用“第二个token的时间”（TT2T），考虑从查询进入LLM输出时间和KVCache管理开销。如图[10(a)](#S4.F10.sf1
    "图10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")所示，通过重叠和自适应聚类，PQCache能够实现最低的TT2T。所有基线方法都有显著的开销。由于H2O在预填充期间收集注意力分数，它无法利用FlashAttention加速，并在处理长输入时遇到OOM。SPARQ没有预填充开销，但其解码过程较慢（参见第[4.3.2](#S4.SS3.SSS2
    "4.3.2\. 解码 ‣ 4.3\. 效率 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")节）。InfLLM由于块级KVCache管理所需的设置而产生时间开销。'
- en: 4.3.2\. Decoding
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 解码
- en: 'Time Per Output Token (TPOT) measures the time of each decoding step. We compare
    the TPOT of H2O, SPARQ, InfLLM, and PQCache in Figure [10(b)](#S4.F10.sf2 "In
    Figure 10 ‣ 4.2.4\. Impact of Extra Communication ‣ 4.2\. Model Performance ‣
    4\. Experiments ‣ PQCache: Product Quantization-based KVCache for Long Context
    LLM Inference"). Here we use 1/5 number of tokens in selective attention, and
    a 4096-token GPU cache. SPARQ exhibits the highest latency due to its sequential
    computation and communication, with the communication scaling linearly with the
    input sequence length. All the other methods exhibit per-token latency faster
    than the human reading speed, which is around 250 words ($\approx$333 tokens)
    per minute (Zhong et al., [2024](#bib.bib65)). H2O avoids extra communications,
    while InfLLM and PQCache both leverage system optimizations to accelerate decoding.
    InfLLM’s block-level token management allows it to efficiently gather data from
    the CPU; however, this block-level assumption negatively impacts the model’s overall
    quality. PQCache incorporates prefetching and caching, achieving an acceptable
    TPOT while not degrading model quality.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '每个输出token的时间（TPOT）测量每一步解码的时间。我们在图[10(b)](#S4.F10.sf2 "图10 ‣ 4.2.4\. 额外通信的影响
    ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的KVCache用于长上下文LLM推理")中比较H2O、SPARQ、InfLLM和PQCache的TPOT。这里我们在选择性注意力中使用1/5的token数量，以及一个4096-token的GPU缓存。由于其顺序计算和通信，SPARQ表现出最高的延迟，通信随着输入序列长度线性增加。其他所有方法的每token延迟都快于人类阅读速度，大约为每分钟250词（$\approx$333
    tokens）（Zhong等， [2024](#bib.bib65)）。H2O避免了额外的通信，而InfLLM和PQCache都利用系统优化来加速解码。InfLLM的块级token管理允许其高效地从CPU收集数据；然而，这种块级假设对模型的整体质量产生了负面影响。PQCache结合了预取和缓存，达到了可接受的TPOT，同时未降低模型质量。'
- en: 4.3.3\. Trade-off between Time and Accuracy
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 时间与准确性之间的权衡
- en: 'In Section [3.3](#S3.SS3 "3.3\. Prefilling Phase ‣ 3\. PQCache ‣ PQCache: Product
    Quantization-based KVCache for Long Context LLM Inference"), we design an adaptive
    K-Means clustering strategy to eliminate latency. To investigate the impact of
    this strategy on model accuracy, we conduct an experiment with varying numbers
    of clustering iterations on the HotpotQA dataset, with 1/10 tokens involved in
    attention. As shown in Figure [10(c)](#S4.F10.sf3 "In Figure 10 ‣ 4.2.4\. Impact
    of Extra Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache:
    Product Quantization-based KVCache for Long Context LLM Inference"), the adaptive
    strategy has the lowest clustering time with good enough model quality. Though
    clustering with more iterations results in better scores, the associated increase
    in inference latency is considerable. Trading-off time and accuracy, the adaptive
    strategy is the most practical choice for real-world applications. We expose an
    interface that lets users set the number of iterations, enabling them to balance
    model performance and latency for their specific needs.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[3.3节](#S3.SS3 "3.3\. 预填充阶段 ‣ 3\. PQCache ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")中，我们设计了一种自适应
    K-Means 聚类策略来消除延迟。为了研究该策略对模型准确性的影响，我们在 HotpotQA 数据集上进行了一项实验，变化的聚类迭代次数涉及 1/10 的令牌进行注意力计算。正如图[10(c)](#S4.F10.sf3
    "图 10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型性能 ‣ 4\. 实验 ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")所示，自适应策略在保证良好模型质量的同时具有最低的聚类时间。虽然更多迭代的聚类结果会有更好的得分，但相关的推理延迟增加也相当显著。在时间与准确性之间进行权衡时，自适应策略是现实应用中最实用的选择。我们暴露了一个接口，允许用户设置迭代次数，使他们能够根据具体需求平衡模型性能和延迟。'
- en: 4.3.4\. Cache Hit-rate
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 缓存命中率
- en: 'We assess the cache hit-rate for Least Recently Used (LRU) and Least Frequently
    Used (LFU) policies across varying numbers of top-$k_{cache}$ blocks involved
    during decoding. Our experiments are conducted on the HotpotQA dataset, with 1/10
    tokens in selective attention and 4096 tokens in GPU cache (128 tokens per block).
    As shown in Figure [10(d)](#S4.F10.sf4 "In Figure 10 ‣ 4.2.4\. Impact of Extra
    Communication ‣ 4.2\. Model Performance ‣ 4\. Experiments ‣ PQCache: Product Quantization-based
    KVCache for Long Context LLM Inference"), both LRU and LFU exhibit similar performance,
    achieving around 0.5 hit-rate across different numbers of blocks. As block count
    increases, the hit-rate initially rises due to more tokens being found within
    blocks. However, it eventually declines as blocks with fewer hits update the cache
    structure, disrupting the normal cache logic. In practice, we set the number of
    blocks to 32, yielding a hit-rate around 0.6, which reduces communication by 60%.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了在解码过程中涉及不同数量的 top-$k_{cache}$ 块的 Least Recently Used (LRU) 和 Least Frequently
    Used (LFU) 策略的缓存命中率。我们的实验在 HotpotQA 数据集上进行，选择性注意力中的 1/10 令牌和 GPU 缓存中的 4096 个令牌（每块
    128 个令牌）。正如图[10(d)](#S4.F10.sf4 "图 10 ‣ 4.2.4\. 额外通信的影响 ‣ 4.2\. 模型性能 ‣ 4\. 实验
    ‣ PQCache: 基于产品量化的长上下文 LLM 推理 KVCache")所示，LRU 和 LFU 的表现相似，在不同数量的块中都实现了大约 0.5 的命中率。随着块数的增加，命中率最初上升，因为更多的令牌在块内被找到。然而，随着块数的增加，命中率最终下降，因为命中较少的块会更新缓存结构，从而扰乱正常的缓存逻辑。在实际应用中，我们将块数设置为
    32，命中率约为 0.6，从而减少了 60% 的通信。'
- en: 5\. Related Work
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 相关工作
- en: Selective Attention for KVCache
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KVCache 的选择性注意力
- en: To eliminate the impact of memory-intensive KVCache, a group of methods include
    only essential tokens for attention computation during LLM inference. One way
    is to discard unnecessary tokens. LM-Infinite (Han et al., [2023](#bib.bib21))
    and Streaming-LLM (Xiao et al., [2023](#bib.bib59)) only preserve the initial
    tokens and the most recent tokens. H2O (Zhang et al., [2023a](#bib.bib64)) and
    Scissorhands (Liu et al., [2023a](#bib.bib34)) utilize attention scores to identify
    important tokens. Their following works (Ge et al., [2023](#bib.bib18); Adnan
    et al., [2024](#bib.bib2); Wang and Gan, [2024](#bib.bib56); Ren and Zhu, [2024](#bib.bib45))
    have explored adaptive token selection and additional metrics for better model
    accuracy. The LLMLingua series (Jiang et al., [2023b](#bib.bib27); Pan et al.,
    [2024](#bib.bib42)) leverage an auxiliary small model to tell which tokens are
    necessary. Since token-level compression evicts the tokens in a greedy manner,
    the information loss in subsequent decoding phase may lead to model degradation.
    Another way is to fetch relevant tokens on demand during the decoding phase. SPARQ (Ribar
    et al., [2023](#bib.bib47)) and InfLLM (Xiao et al., [2024](#bib.bib58)) offload
    KVCache to CPU, and selectively fetch relevant key-value pairs for each attention
    computation. PQCache also falls under this category of methods, demonstrating
    effective and efficient LLM inference in comparison to existing techniques.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除内存密集型 KVCache 的影响，一组方法包括在 LLM 推理过程中仅包含必要的 token。一种方法是丢弃不必要的 token。LM-Infinite
    (Han et al., [2023](#bib.bib21)) 和 Streaming-LLM (Xiao et al., [2023](#bib.bib59))
    仅保留初始 token 和最新 token。H2O (Zhang et al., [2023a](#bib.bib64)) 和 Scissorhands (Liu
    et al., [2023a](#bib.bib34)) 利用注意力分数来识别重要的 token。它们后续的研究 (Ge et al., [2023](#bib.bib18);
    Adnan et al., [2024](#bib.bib2); Wang and Gan, [2024](#bib.bib56); Ren and Zhu,
    [2024](#bib.bib45)) 探讨了自适应 token 选择和附加指标以提高模型准确性。LLMLingua 系列 (Jiang et al., [2023b](#bib.bib27);
    Pan et al., [2024](#bib.bib42)) 利用辅助小模型来判断哪些 token 是必要的。由于 token 级压缩以贪婪的方式驱逐 token，随后的解码阶段中的信息丢失可能导致模型性能下降。另一种方法是在解码阶段按需提取相关
    token。SPARQ (Ribar et al., [2023](#bib.bib47)) 和 InfLLM (Xiao et al., [2024](#bib.bib58))
    将 KVCache 卸载到 CPU 上，并根据每次注意力计算有选择性地提取相关的键值对。PQCache 也属于这类方法，相比于现有技术，展示了有效且高效的
    LLM 推理。
- en: KVCache Quantization
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KVCache 量化
- en: Quantization can be directly applied on the entire KVCache (Liu et al., [2024b](#bib.bib35);
    Dong et al., [2024a](#bib.bib12); Hooper et al., [2024](#bib.bib22)) - a straight-forward
    approach with promising model quality. Other compression techniques can also be
    employed to address the residuals introduced by quantization (Kang et al., [2024](#bib.bib30)).
    It is worth noting that quantization is orthogonal to token importance, and recent
    research has explored applying both techniques (Yang et al., [2024](#bib.bib60)).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以直接应用于整个 KVCache (Liu et al., [2024b](#bib.bib35); Dong et al., [2024a](#bib.bib12);
    Hooper et al., [2024](#bib.bib22)) - 这是一种直接的方法，具有良好的模型质量。其他压缩技术也可以用于解决量化引入的残差
    (Kang et al., [2024](#bib.bib30))。值得注意的是，量化与 token 重要性是正交的，最近的研究探索了同时应用这两种技术 (Yang
    et al., [2024](#bib.bib60))。
- en: KVCache Scheduling
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KVCache 调度
- en: Another way to address the KVCache memory challenge is to meticulously schedule
    the KVCache within memory hierarchy. FlexGen (Sheng et al., [2023](#bib.bib48))
    employs linear programming to schedule the communication, searching for efficient
    patterns to store and access tensors. AttentionScore (Gao et al., [2024](#bib.bib17))
    maintains a hierarchical KV caching system, allowing efficient reuse of KVCache
    across multi-turn conversations. Another related research topic is KVCache streaming
    for LLM serving (Liu et al., [2023b](#bib.bib33); Strati et al., [2024](#bib.bib50)),
    which involves handling multiple requests within more levels of memory hierarchy.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 解决 KVCache 内存挑战的另一种方法是仔细安排 KVCache 在内存层次结构中的位置。FlexGen (Sheng et al., [2023](#bib.bib48))
    采用线性规划来安排通信，寻找高效的模式来存储和访问张量。AttentionScore (Gao et al., [2024](#bib.bib17)) 维护一个分层的
    KV 缓存系统，允许在多轮对话中高效地重用 KVCache。另一个相关的研究主题是用于 LLM 服务的 KVCache 流处理 (Liu et al., [2023b](#bib.bib33);
    Strati et al., [2024](#bib.bib50))，涉及在更多内存层次结构中处理多个请求。
- en: Embedding Management
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入管理
- en: Embedding management is a common research focus within the database and data
    management domains, including embedding compression (Zhang et al., [2023c](#bib.bib63),
    [2024](#bib.bib61); Shi et al., [2020](#bib.bib49)), embedding retrieval (Wang
    et al., [2021](#bib.bib55); Huang et al., [2020](#bib.bib23)), and key-value storage (Chen
    et al., [2021b](#bib.bib9); Ren et al., [2017](#bib.bib44)). Our work provides
    a potential direction for integrating classic embedding management into the LLM
    ecology.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入管理是数据库和数据管理领域的一个常见研究重点，包括嵌入压缩（Zhang 等人，[2023c](#bib.bib63)，[2024](#bib.bib61)；Shi
    等人，[2020](#bib.bib49)），嵌入检索（Wang 等人，[2021](#bib.bib55)；Huang 等人，[2020](#bib.bib23)），以及键值存储（Chen
    等人，[2021b](#bib.bib9)；Ren 等人，[2017](#bib.bib44)）。我们的工作为将经典的嵌入管理整合到 LLM 生态系统中提供了潜在方向。
- en: 6\. Conclusion
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 结论
- en: In this paper, we proposed PQCache, a system-algorithm co-designed method for
    effective and efficient long context LLM inference. We incorporated the embedding
    retrieval technique PQ to reduce both memory and computation burden, and leveraged
    PQ codes and centroids to facilitate efficient MIPS for important tokens used
    in the attention module. Through meticulous overlapping and caching, we managed
    to minimize overhead to a negligible level. We evaluated PQCache on extensive
    experiments, and show that PQCache effectively maintains model quality with only
    1/5 of the tokens involved in attention, while achieving acceptable system latency.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 PQCache，一种系统-算法协同设计的方法，用于高效且有效的长上下文 LLM 推断。我们结合了嵌入检索技术 PQ，以减少内存和计算负担，并利用
    PQ 代码和质心来促进重要标记在注意力模块中的高效 MIPS。通过精细的重叠和缓存，我们将开销降到了可忽略的水平。我们在广泛的实验中评估了 PQCache，并表明
    PQCache 在仅涉及 1/5 的注意力标记的情况下有效地保持了模型质量，同时实现了可接受的系统延迟。
- en: References
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Adnan et al. (2024) Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J.
    Nair, Ilya Soloveychik, and Purushotham Kamath. 2024. Keyformer: KV Cache Reduction
    through Key Tokens Selection for Efficient Generative Inference. *CoRR* abs/2403.09054
    (2024). [https://doi.org/10.48550/ARXIV.2403.09054](https://doi.org/10.48550/ARXIV.2403.09054)
    arXiv:2403.09054'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adnan 等人（2024）Muhammad Adnan、Akhil Arunkumar、Gaurav Jain、Prashant J. Nair、Ilya
    Soloveychik 和 Purushotham Kamath. 2024. Keyformer：通过关键标记选择减少 KV 缓存以提高生成推断效率。*CoRR*
    abs/2403.09054（2024）。 [https://doi.org/10.48550/ARXIV.2403.09054](https://doi.org/10.48550/ARXIV.2403.09054)
    arXiv:2403.09054
- en: AI (2024) Moonshot AI. 2024. KimiChat. [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI（2024）Moonshot AI. 2024. KimiChat. [https://kimi.moonshot.cn/](https://kimi.moonshot.cn/)
- en: 'Ainslie et al. (2023) Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury
    Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized
    Multi-Query Transformer Models from Multi-Head Checkpoints. In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2023, Singapore, December 6-10, 2023*, Houda Bouamor, Juan Pino, and Kalika Bali
    (Eds.). Association for Computational Linguistics, 4895–4901. [https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ainslie 等人（2023）Joshua Ainslie、James Lee-Thorp、Michiel de Jong、Yury Zemlyanskiy、Federico
    Lebrón 和 Sumit Sanghai. 2023. GQA：从多头检查点训练通用多查询变换器模型。发表于 *2023 年自然语言处理实证方法会议论文集，EMNLP
    2023，新加坡，2023 年 12 月 6-10 日*，Houda Bouamor、Juan Pino 和 Kalika Bali（编）。计算语言学协会，4895–4901。
    [https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.298)
- en: Anthropic (2023) Anthropic. 2023. Long context prompting for Claude 2.1. [https://www.anthropic.com/news/claude-2-1-prompting](https://www.anthropic.com/news/claude-2-1-prompting)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2023）Anthropic. 2023. Claude 2.1 的长上下文提示。 [https://www.anthropic.com/news/claude-2-1-prompting](https://www.anthropic.com/news/claude-2-1-prompting)
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. LongBench: A Bilingual, Multitask Benchmark for Long Context
    Understanding. *CoRR* abs/2308.14508 (2023). [https://doi.org/10.48550/ARXIV.2308.14508](https://doi.org/10.48550/ARXIV.2308.14508)
    arXiv:2308.14508'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2023）Yushi Bai、Xin Lv、Jiajie Zhang、Hongchang Lyu、Jiankai Tang、Zhidian
    Huang、Zhengxiao Du、Xiao Liu、Aohan Zeng、Lei Hou、Yuxiao Dong、Jie Tang 和 Juanzi Li.
    2023. LongBench：一个用于长上下文理解的双语、多任务基准。*CoRR* abs/2308.14508（2023）。 [https://doi.org/10.48550/ARXIV.2308.14508](https://doi.org/10.48550/ARXIV.2308.14508)
    arXiv:2308.14508
- en: Baranchuk et al. (2018) Dmitry Baranchuk, Artem Babenko, and Yury Malkov. 2018.
    Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors.
    In *Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
    8-14, 2018, Proceedings, Part XII* *(Lecture Notes in Computer Science)*, Vittorio
    Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.), Vol. 11216\.
    Springer, 209–224. [https://doi.org/10.1007/978-3-030-01258-8_13](https://doi.org/10.1007/978-3-030-01258-8_13)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baranchuk 等 (2018) Dmitry Baranchuk, Artem Babenko, 和 Yury Malkov. 2018. 重新审视亿规模近似最近邻的倒排索引。见
    *计算机视觉 - ECCV 2018 - 第15届欧洲会议, 慕尼黑, 德国, 2018年9月8-14日, 论文集, 第十二部分* *(计算机科学讲义集)*,
    Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, 和 Yair Weiss (编), 第11216卷.
    Springer, 209–224. [https://doi.org/10.1007/978-3-030-01258-8_13](https://doi.org/10.1007/978-3-030-01258-8_13)
- en: 'Chen et al. (2021a) Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie
    Liu, Zengzhong Li, Mao Yang, and Jingdong Wang. 2021a. SPANN: Highly-efficient
    Billion-scale Approximate Nearest Neighborhood Search. In *Advances in Neural
    Information Processing Systems 34: Annual Conference on Neural Information Processing
    Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual*, Marc’Aurelio Ranzato,
    Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan
    (Eds.). 5199–5212. [https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2021a) Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu,
    Zengzhong Li, Mao Yang, 和 Jingdong Wang. 2021a. SPANN: 高效的亿规模近似最近邻搜索。见 *神经信息处理系统进展
    34: 神经信息处理系统年会 2021, NeurIPS 2021, 2021年12月6-14日, 虚拟会议*, Marc’Aurelio Ranzato,
    Alina Beygelzimer, Yann N. Dauphin, Percy Liang, 和 Jennifer Wortman Vaughan (编).
    5199–5212. [https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html)'
- en: 'Chen et al. (2021b) Xubin Chen, Ning Zheng, Shukun Xu, Yifan Qiao, Yang Liu,
    Jiangpeng Li, and Tong Zhang. 2021b. KallaxDB: A Table-less Hash-based Key-Value
    Store on Storage Hardware with Built-in Transparent Compression. In *Proceedings
    of the 17th International Workshop on Data Management on New Hardware, DaMoN 2021,
    21 June 2021, Virtual Event, China*, Danica Porobic and Spyros Blanas (Eds.).
    ACM, 3:1–3:10. [https://doi.org/10.1145/3465998.3466004](https://doi.org/10.1145/3465998.3466004)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2021b) Xubin Chen, Ning Zheng, Shukun Xu, Yifan Qiao, Yang Liu, Jiangpeng
    Li, 和 Tong Zhang. 2021b. KallaxDB: 一种基于哈希的无表键值存储，支持内置透明压缩。见 *第17届新硬件数据管理国际研讨会论文集,
    DaMoN 2021, 2021年6月21日, 虚拟活动, 中国*, Danica Porobic 和 Spyros Blanas (编). ACM, 3:1–3:10.
    [https://doi.org/10.1145/3465998.3466004](https://doi.org/10.1145/3465998.3466004)'
- en: Cloud (2024) Alibaba Cloud. 2024. Tongyi Qianwen. [https://tongyi.aliyun.com/qianwen/](https://tongyi.aliyun.com/qianwen/)
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud (2024) 阿里云. 2024. 通义千问. [https://tongyi.aliyun.com/qianwen/](https://tongyi.aliyun.com/qianwen/)
- en: 'Dong et al. (2024b) Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie
    Chi, and Beidi Chen. 2024b. Get More with LESS: Synthesizing Recurrence with KV
    Cache Compression for Efficient LLM Inference. *CoRR* abs/2402.09398 (2024). [https://doi.org/10.48550/ARXIV.2402.09398](https://doi.org/10.48550/ARXIV.2402.09398)
    arXiv:2402.09398'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等 (2024b) Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie
    Chi, 和 Beidi Chen. 2024b. 用 LESS 获得更多: 通过 KV 缓存压缩合成递归以提高 LLM 推理效率。*CoRR* abs/2402.09398
    (2024). [https://doi.org/10.48550/ARXIV.2402.09398](https://doi.org/10.48550/ARXIV.2402.09398)
    arXiv:2402.09398'
- en: 'Dong et al. (2024a) Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang. 2024a.
    QAQ: Quality Adaptive Quantization for LLM KV Cache. *CoRR* abs/2403.04643 (2024).
    [https://doi.org/10.48550/ARXIV.2403.04643](https://doi.org/10.48550/ARXIV.2403.04643)
    arXiv:2403.04643'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等 (2024a) Shichen Dong, Wen Cheng, Jiayu Qin, 和 Wei Wang. 2024a. QAQ:
    针对 LLM KV 缓存的质量自适应量化。*CoRR* abs/2403.04643 (2024). [https://doi.org/10.48550/ARXIV.2403.04643](https://doi.org/10.48550/ARXIV.2403.04643)
    arXiv:2403.04643'
- en: 'Fabbri et al. (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and
    Dragomir R. Radev. 2019. Multi-News: A Large-Scale Multi-Document Summarization
    Dataset and Abstractive Hierarchical Model. In *Proceedings of the 57th Conference
    of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July
    28- August 2, 2019, Volume 1: Long Papers*, Anna Korhonen, David R. Traum, and
    Lluís Màrquez (Eds.). Association for Computational Linguistics, 1074–1084. [https://doi.org/10.18653/V1/P19-1102](https://doi.org/10.18653/V1/P19-1102)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等 (2019) Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, 和 Dragomir
    R. Radev. 2019. Multi-News: 大规模多文档摘要数据集和抽象层次模型。见 *第57届计算语言学协会会议论文集, ACL 2019,
    佛罗伦萨, 意大利, 2019年7月28-8月2日, 第1卷: 长篇论文*, Anna Korhonen, David R. Traum, 和 Lluís
    Màrquez (编). 计算语言学协会, 1074–1084. [https://doi.org/10.18653/V1/P19-1102](https://doi.org/10.18653/V1/P19-1102)'
- en: Fu et al. (2019) Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. 2019. Fast
    Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph. *Proc.
    VLDB Endow.* 12, 5 (2019), 461–474. [https://doi.org/10.14778/3303753.3303754](https://doi.org/10.14778/3303753.3303754)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2019）Cong Fu, Chao Xiang, Changxu Wang, 和 Deng Cai。2019年。《使用导航扩展图的快速近似最近邻搜索》。*Proc.
    VLDB Endow.* 12, 5（2019年），461–474。[https://doi.org/10.14778/3303753.3303754](https://doi.org/10.14778/3303753.3303754)
- en: 'Fu et al. (2023) Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar
    Khot. 2023. Chain-of-Thought Hub: A Continuous Effort to Measure Large Language
    Models’ Reasoning Performance. *CoRR* abs/2305.17306 (2023). [https://doi.org/10.48550/ARXIV.2305.17306](https://doi.org/10.48550/ARXIV.2305.17306)
    arXiv:2305.17306'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等（2023）Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, 和 Tushar Khot。2023年。《Chain-of-Thought
    Hub: 持续努力评估大语言模型的推理性能》。*CoRR* abs/2305.17306（2023年）。[https://doi.org/10.48550/ARXIV.2305.17306](https://doi.org/10.48550/ARXIV.2305.17306)
    arXiv:2305.17306'
- en: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. 2024. Data Engineering for Scaling Language Models to
    128K Context. *CoRR* abs/2402.10171 (2024). [https://doi.org/10.48550/ARXIV.2402.10171](https://doi.org/10.48550/ARXIV.2402.10171)
    arXiv:2402.10171
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2024）Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, 和 Hao Peng。2024年。《用于将语言模型扩展到128K上下文的数据工程》。*CoRR* abs/2402.10171（2024年）。[https://doi.org/10.48550/ARXIV.2402.10171](https://doi.org/10.48550/ARXIV.2402.10171)
    arXiv:2402.10171
- en: 'Gao et al. (2024) Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje
    Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. AttentionStore:
    Cost-effective Attention Reuse across Multi-turn Conversations in Large Language
    Model Serving. *CoRR* abs/2403.19708 (2024). [https://doi.org/10.48550/ARXIV.2403.19708](https://doi.org/10.48550/ARXIV.2403.19708)
    arXiv:2403.19708'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等（2024）Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic,
    Junbo Deng, Xingkun Yang, Zhou Yu, 和 Pengfei Zuo。2024年。《AttentionStore: 成本效益的多轮对话注意力重用大语言模型服务》。*CoRR*
    abs/2403.19708（2024年）。[https://doi.org/10.48550/ARXIV.2403.19708](https://doi.org/10.48550/ARXIV.2403.19708)
    arXiv:2403.19708'
- en: 'Ge et al. (2023) Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. 2023. Model Tells You What to Discard: Adaptive KV Cache Compression
    for LLMs. *CoRR* abs/2310.01801 (2023). [https://doi.org/10.48550/ARXIV.2310.01801](https://doi.org/10.48550/ARXIV.2310.01801)
    arXiv:2310.01801'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等（2023）Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao。2023年。《模型告诉你丢弃什么：LLMs 的自适应 KV 缓存压缩》。*CoRR* abs/2310.01801（2023年）。[https://doi.org/10.48550/ARXIV.2310.01801](https://doi.org/10.48550/ARXIV.2310.01801)
    arXiv:2310.01801
- en: Ge et al. (2013) Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized
    Product Quantization for Approximate Nearest Neighbor Search. In *2013 IEEE Conference
    on Computer Vision and Pattern Recognition, Portland, OR, USA, June 23-28, 2013*.
    IEEE Computer Society, 2946–2953. [https://doi.org/10.1109/CVPR.2013.379](https://doi.org/10.1109/CVPR.2013.379)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等（2013）Tiezheng Ge, Kaiming He, Qifa Ke, 和 Jian Sun。2013年。《优化产品量化用于近似最近邻搜索》。在
    *2013 IEEE计算机视觉与模式识别大会，俄勒冈州波特兰，美国，2013年6月23-28日*。IEEE计算机学会，2946–2953。[https://doi.org/10.1109/CVPR.2013.379](https://doi.org/10.1109/CVPR.2013.379)
- en: Guo et al. (2020) Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha,
    Felix Chern, and Sanjiv Kumar. 2020. Accelerating Large-Scale Inference with Anisotropic
    Vector Quantization. In *Proceedings of the 37th International Conference on Machine
    Learning, ICML 2020, 13-18 July 2020, Virtual Event* *(Proceedings of Machine
    Learning Research)*, Vol. 119\. PMLR, 3887–3896. [http://proceedings.mlr.press/v119/guo20h.html](http://proceedings.mlr.press/v119/guo20h.html)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等（2020）Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix
    Chern, 和 Sanjiv Kumar。2020年。《利用各向异性向量量化加速大规模推理》。在 *第37届国际机器学习大会，ICML 2020，2020年7月13-18日，虚拟会议*
    *(机器学习研究会论文集)*, 第119卷。PMLR, 3887–3896。[http://proceedings.mlr.press/v119/guo20h.html](http://proceedings.mlr.press/v119/guo20h.html)
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. LM-Infinite: Simple On-the-Fly Length Generalization for Large
    Language Models. *CoRR* abs/2308.16137 (2023). [https://doi.org/10.48550/ARXIV.2308.16137](https://doi.org/10.48550/ARXIV.2308.16137)
    arXiv:2308.16137'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等（2023）Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong Wang。2023年。《LM-Infinite:
    Large Language Models 的简单实时长度泛化》。*CoRR* abs/2308.16137（2023年）。[https://doi.org/10.48550/ARXIV.2308.16137](https://doi.org/10.48550/ARXIV.2308.16137)
    arXiv:2308.16137'
- en: 'Hooper et al. (2024) Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W.
    Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards
    10 Million Context Length LLM Inference with KV Cache Quantization. *CoRR* abs/2401.18079
    (2024). [https://doi.org/10.48550/ARXIV.2401.18079](https://doi.org/10.48550/ARXIV.2401.18079)
    arXiv:2401.18079'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hooper 等（2024）Coleman Hooper、Sehoon Kim、Hiva Mohammadzadeh、Michael W. Mahoney、Yakun
    Sophia Shao、Kurt Keutzer 和 Amir Gholami。2024。KVQuant: 面向 1000 万上下文长度 LLM 推理的 KV
    缓存量化。*CoRR* abs/2401.18079（2024）。[https://doi.org/10.48550/ARXIV.2401.18079](https://doi.org/10.48550/ARXIV.2401.18079)
    arXiv:2401.18079'
- en: Huang et al. (2020) Ruihong Huang, Shaoxu Song, Yunsu Lee, Jungho Park, Soo-Hyung
    Kim, and Sungmin Yi. 2020. Effective and Efficient Retrieval of Structured Entities.
    *Proc. VLDB Endow.* 13, 6 (2020), 826–839. [https://doi.org/10.14778/3380750.3380754](https://doi.org/10.14778/3380750.3380754)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2020）Ruihong Huang、Shaoxu Song、Yunsu Lee、Jungho Park、Soo-Hyung Kim 和
    Sungmin Yi。2020。结构化实体的有效和高效检索。*Proc. VLDB Endow.* 13，6（2020），826–839。[https://doi.org/10.14778/3380750.3380754](https://doi.org/10.14778/3380750.3380754)
- en: 'Jayaram Subramanya et al. (2019) Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan
    Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. 2019. Diskann: Fast accurate
    billion-point nearest neighbor search on a single node. *Advances in Neural Information
    Processing Systems* 32 (2019).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jayaram Subramanya 等（2019）Suhas Jayaram Subramanya、Fnu Devvrit、Harsha Vardhan
    Simhadri、Ravishankar Krishnawamy 和 Rohan Kadekodi。2019。Diskann: 单节点上的快速准确十亿点最近邻搜索。*神经信息处理系统进展*
    32（2019）。'
- en: Jégou et al. (2011) Hervé Jégou, Matthijs Douze, and Cordelia Schmid. 2011.
    Product Quantization for Nearest Neighbor Search. *IEEE Trans. Pattern Anal. Mach.
    Intell.* 33, 1 (2011), 117–128. [https://doi.org/10.1109/TPAMI.2010.57](https://doi.org/10.1109/TPAMI.2010.57)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jégou 等（2011）Hervé Jégou、Matthijs Douze 和 Cordelia Schmid。2011。最近邻搜索的产品量化。*IEEE
    计算机学会模式分析与机器智能汇刊* 33，1（2011），117–128。[https://doi.org/10.1109/TPAMI.2010.57](https://doi.org/10.1109/TPAMI.2010.57)
- en: Jiang et al. (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023a. Mistral 7B. *CoRR* abs/2310.06825 (2023). [https://doi.org/10.48550/ARXIV.2310.06825](https://doi.org/10.48550/ARXIV.2310.06825)
    arXiv:2310.06825
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023a）Albert Q. Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de Las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier、Lélio Renard Lavaud、Marie-Anne Lachaux、Pierre Stock、Teven Le Scao、Thibaut
    Lavril、Thomas Wang、Timothée Lacroix 和 William El Sayed。2023a。Mistral 7B。*CoRR*
    abs/2310.06825（2023）。[https://doi.org/10.48550/ARXIV.2310.06825](https://doi.org/10.48550/ARXIV.2310.06825)
    arXiv:2310.06825
- en: 'Jiang et al. (2023b) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,
    and Lili Qiu. 2023b. LLMLingua: Compressing Prompts for Accelerated Inference
    of Large Language Models. In *Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
    2023*, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational
    Linguistics, 13358–13376. [https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等（2023b）Huiqiang Jiang、Qianhui Wu、Chin-Yew Lin、Yuqing Yang 和 Lili Qiu。2023b。LLMLingua:
    压缩提示以加速大型语言模型的推理。在 *2023 年自然语言处理实证方法会议（EMNLP 2023），新加坡，2023 年 12 月 6-10 日*，Houda
    Bouamor、Juan Pino 和 Kalika Bali（编）。计算语言学协会，13358–13376。[https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.825)'
- en: Johnson et al. (2021) Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-Scale
    Similarity Search with GPUs. *IEEE Trans. Big Data* 7, 3 (2021), 535–547. [https://doi.org/10.1109/TBDATA.2019.2921572](https://doi.org/10.1109/TBDATA.2019.2921572)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等（2021）Jeff Johnson、Matthijs Douze 和 Hervé Jégou。2021。大规模相似性搜索与 GPUs。*IEEE
    大数据汇刊* 7，3（2021），535–547。[https://doi.org/10.1109/TBDATA.2019.2921572](https://doi.org/10.1109/TBDATA.2019.2921572)
- en: Kamradt (2024) Greg Kamradt. 2024. Needle-in-a-Haystack. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt（2024）Greg Kamradt。2024。针尖上的针。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
- en: 'Kang et al. (2024) Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing
    Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression
    recipefor near-lossless generative inference of llm. *arXiv preprint arXiv:2403.05527*
    (2024).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang 等（2024）Hao Kang、Qingru Zhang、Souvik Kundu、Geonhwa Jeong、Zaoxing Liu、Tushar
    Krishna 和 Tuo Zhao。2024。Gear: 一种用于接近无损生成推理的高效 KV 缓存压缩配方。*arXiv 预印本 arXiv:2403.05527*（2024）。'
- en: 'Lingle (2023) Lucas D. Lingle. 2023. Transformer-VQ: Linear-Time Transformers
    via Vector Quantization. *CoRR* abs/2309.16354 (2023). [https://doi.org/10.48550/ARXIV.2309.16354](https://doi.org/10.48550/ARXIV.2309.16354)
    arXiv:2309.16354'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lingle（2023）Lucas D. Lingle。2023。Transformer-VQ：通过向量量化实现线性时间的变换器。*CoRR* abs/2309.16354（2023）。[https://doi.org/10.48550/ARXIV.2309.16354](https://doi.org/10.48550/ARXIV.2309.16354)
    arXiv:2309.16354
- en: Liu et al. (2024a) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024a.
    World Model on Million-Length Video And Language With Blockwise RingAttention.
    *CoRR* abs/2402.08268 (2024). [https://doi.org/10.48550/ARXIV.2402.08268](https://doi.org/10.48550/ARXIV.2402.08268)
    arXiv:2402.08268
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024a）Hao Liu、Wilson Yan、Matei Zaharia 和 Pieter Abbeel。2024a。百万长度视频和语言的世界模型与块状环注意力。*CoRR*
    abs/2402.08268（2024）。[https://doi.org/10.48550/ARXIV.2402.08268](https://doi.org/10.48550/ARXIV.2402.08268)
    arXiv:2402.08268
- en: 'Liu et al. (2023b) Yuhan Liu, Hanchen Li, Kuntai Du, Jiayi Yao, Yihua Cheng,
    Yuyang Huang, Shan Lu, Michael Maire, Henry Hoffmann, Ari Holtzman, et al. 2023b.
    CacheGen: Fast Context Loading for Language Model Applications. *arXiv preprint
    arXiv:2310.07240* (2023).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023b）Yuhan Liu、Hanchen Li、Kuntai Du、Jiayi Yao、Yihua Cheng、Yuyang Huang、Shan
    Lu、Michael Maire、Henry Hoffmann、Ari Holtzman 等。2023b。CacheGen：语言模型应用的快速上下文加载。*arXiv
    预印本 arXiv:2310.07240*（2023）。
- en: 'Liu et al. (2023a) Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2023a. Scissorhands:
    Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression
    at Test Time. In *Advances in Neural Information Processing Systems 36: Annual
    Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
    LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan Naumann, Amir Globerson, Kate
    Saenko, Moritz Hardt, and Sergey Levine (Eds.). [http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023a）Zichang Liu、Aditya Desai、Fangshuo Liao、Weitao Wang、Victor Xie、Zhaozhuo
    Xu、Anastasios Kyrillidis 和 Anshumali Shrivastava。2023a。Scissorhands：利用重要性假设的持久性进行LLM
    KV缓存压缩。发表于*Advances in Neural Information Processing Systems 36：2023年神经信息处理系统年度会议，NeurIPS
    2023，新奥尔良，美国，2023年12月10 - 16日*，Alice Oh、Tristan Naumann、Amir Globerson、Kate Saenko、Moritz
    Hardt 和 Sergey Levine（编辑）。[http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/a452a7c6c463e4ae8fbdc614c6e983e6-Abstract-Conference.html)
- en: 'Liu et al. (2024b) Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo
    Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024b. KIVI: A Tuning-Free Asymmetric
    2bit Quantization for KV Cache. *CoRR* abs/2402.02750 (2024). [https://doi.org/10.48550/ARXIV.2402.02750](https://doi.org/10.48550/ARXIV.2402.02750)
    arXiv:2402.02750'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024b）Zirui Liu、Jiayi Yuan、Hongye Jin、Shaochen Zhong、Zhaozhuo Xu、Vladimir
    Braverman、Beidi Chen 和 Xia Hu。2024b。KIVI：一种无需调整的非对称2位量化方法用于KV缓存。*CoRR* abs/2402.02750（2024）。[https://doi.org/10.48550/ARXIV.2402.02750](https://doi.org/10.48550/ARXIV.2402.02750)
    arXiv:2402.02750
- en: Malkov and Yashunin (2020) Yury A. Malkov and Dmitry A. Yashunin. 2020. Efficient
    and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small
    World Graphs. *IEEE Trans. Pattern Anal. Mach. Intell.* 42, 4 (2020), 824–836.
    [https://doi.org/10.1109/TPAMI.2018.2889473](https://doi.org/10.1109/TPAMI.2018.2889473)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malkov 和 Yashunin（2020）Yury A. Malkov 和 Dmitry A. Yashunin。2020。使用分层可导航小世界图的高效且鲁棒的近似最近邻搜索。*IEEE
    Trans. Pattern Anal. Mach. Intell.* 42, 4（2020），824–836。 [https://doi.org/10.1109/TPAMI.2018.2889473](https://doi.org/10.1109/TPAMI.2018.2889473)
- en: Martinez et al. (2014) Julieta Martinez, Holger H. Hoos, and James J. Little.
    2014. Stacked Quantizers for Compositional Vector Compression. *CoRR* abs/1411.2173
    (2014). arXiv:1411.2173 [http://arxiv.org/abs/1411.2173](http://arxiv.org/abs/1411.2173)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez 等（2014）Julieta Martinez、Holger H. Hoos 和 James J. Little。2014。用于组合向量压缩的堆叠量化器。*CoRR*
    abs/1411.2173（2014）。arXiv:1411.2173 [http://arxiv.org/abs/1411.2173](http://arxiv.org/abs/1411.2173)
- en: 'Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Hongyi Jin, Tianqi Chen, and Zhihao Jia. 2023. Towards efficient generative large
    language model serving: A survey from algorithms to systems. *arXiv preprint arXiv:2312.15234*
    (2023).'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等（2023）Xupeng Miao、Gabriele Oliaro、Zhihao Zhang、Xinhao Cheng、Hongyi Jin、Tianqi
    Chen 和 Zhihao Jia。2023。面向高效生成大语言模型服务：从算法到系统的调查。*arXiv 预印本 arXiv:2312.15234*（2023）。
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018.
    Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural
    Networks for Extreme Summarization. In *Proceedings of the 2018 Conference on
    Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31
    - November 4, 2018*, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi
    Tsujii (Eds.). Association for Computational Linguistics, 1797–1807. [https://doi.org/10.18653/V1/D18-1206](https://doi.org/10.18653/V1/D18-1206)
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan 等 (2018) Shashi Narayan、Shay B. Cohen 和 Mirella Lapata。2018。《别给我细节，只给我总结！面向主题的卷积神经网络用于极端摘要》。见于
    *2018年自然语言处理经验方法会议论文集，比利时布鲁塞尔，2018年10月31日-11月4日*，Ellen Riloff、David Chiang、Julia
    Hockenmaier 和 Jun’ichi Tsujii（编）。计算语言学协会，1797–1807。[https://doi.org/10.18653/V1/D18-1206](https://doi.org/10.18653/V1/D18-1206)
- en: 'Nie et al. (2022) Xiaonan Nie, Xupeng Miao, Zhi Yang, and Bin Cui. 2022. TSPLIT:
    Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting.
    In *38th IEEE International Conference on Data Engineering, ICDE 2022, Kuala Lumpur,
    Malaysia, May 9-12, 2022*. IEEE, 2615–2628. [https://doi.org/10.1109/ICDE53745.2022.00241](https://doi.org/10.1109/ICDE53745.2022.00241)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nie 等 (2022) 聂小南、苗旭鹏、杨志 和 崔斌。2022。《TSPLIT: 通过张量分割实现高效DNN训练的精细化GPU内存管理》。见于 *第38届IEEE数据工程国际会议，ICDE
    2022，马来西亚吉隆坡，2022年5月9-12日*。IEEE，2615–2628。[https://doi.org/10.1109/ICDE53745.2022.00241](https://doi.org/10.1109/ICDE53745.2022.00241)'
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. *CoRR* abs/2303.08774 (2023).
    [https://doi.org/10.48550/ARXIV.2303.08774](https://doi.org/10.48550/ARXIV.2303.08774)
    arXiv:2303.08774
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。2023。《GPT-4技术报告》。*CoRR* abs/2303.08774 (2023)。[https://doi.org/10.48550/ARXIV.2303.08774](https://doi.org/10.48550/ARXIV.2303.08774)
    arXiv:2303.08774
- en: 'Pan et al. (2024) Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang
    Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky
    Zhao, Lili Qiu, and Dongmei Zhang. 2024. LLMLingua-2: Data Distillation for Efficient
    and Faithful Task-Agnostic Prompt Compression. *CoRR* abs/2403.12968 (2024). [https://doi.org/10.48550/ARXIV.2403.12968](https://doi.org/10.48550/ARXIV.2403.12968)
    arXiv:2403.12968'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan 等 (2024) 朱硕仕、吴倩辉、姜辉强、夏梦林、罗旭芳、张玦、林青伟、维克多·瑞勒、杨钰清、林金耀、H. Vicky Zhao、邱丽莉和张冬梅。2024。《LLMLingua-2:
    数据蒸馏以实现高效且忠实的任务无关提示压缩》。*CoRR* abs/2403.12968 (2024)。[https://doi.org/10.48550/ARXIV.2403.12968](https://doi.org/10.48550/ARXIV.2403.12968)
    arXiv:2403.12968'
- en: 'Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021. ZeRO-Offload:
    Democratizing Billion-Scale Model Training. In *2021 USENIX Annual Technical Conference,
    USENIX ATC 2021, July 14-16, 2021*, Irina Calciu and Geoff Kuenning (Eds.). USENIX
    Association, 551–564. [https://www.usenix.org/conference/atc21/presentation/ren-jie](https://www.usenix.org/conference/atc21/presentation/ren-jie)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等 (2021) Jie Ren、Samyam Rajbhandari、Reza Yazdani Aminabadi、Olatunji Ruwase、Shuangyan
    Yang、张敏佳、李东和何宇雄。2021。《ZeRO-Offload: 使亿级规模模型训练民主化》。见于 *2021年USENIX年度技术会议，USENIX
    ATC 2021，2021年7月14-16日*，Irina Calciu 和 Geoff Kuenning（编）。USENIX协会，551–564。[https://www.usenix.org/conference/atc21/presentation/ren-jie](https://www.usenix.org/conference/atc21/presentation/ren-jie)'
- en: 'Ren et al. (2017) Kai Ren, Qing Zheng, Joy Arulraj, and Garth Gibson. 2017.
    SlimDB: A Space-Efficient Key-Value Storage Engine For Semi-Sorted Data. *Proc.
    VLDB Endow.* 10, 13 (2017), 2037–2048. [https://doi.org/10.14778/3151106.3151108](https://doi.org/10.14778/3151106.3151108)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ren 等 (2017) 任凯、郑清、Joy Arulraj 和 Garth Gibson。2017。《SlimDB: 一种面向半排序数据的空间高效键值存储引擎》。*Proc.
    VLDB Endow.* 10, 13 (2017), 2037–2048。[https://doi.org/10.14778/3151106.3151108](https://doi.org/10.14778/3151106.3151108)'
- en: Ren and Zhu (2024) Siyu Ren and Kenny Q. Zhu. 2024. On the Efficacy of Eviction
    Policy for Key-Value Constrained Generative Language Model Inference. *CoRR* abs/2402.06262
    (2024). [https://doi.org/10.48550/ARXIV.2402.06262](https://doi.org/10.48550/ARXIV.2402.06262)
    arXiv:2402.06262
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 和 Zhu (2024) 任思宇 和 朱肯尼。2024。《键值约束生成语言模型推理的驱逐策略有效性研究》。*CoRR* abs/2402.06262
    (2024)。[https://doi.org/10.48550/ARXIV.2402.06262](https://doi.org/10.48550/ARXIV.2402.06262)
    arXiv:2402.06262
- en: 'Rhu et al. (2016) Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar,
    and Stephen W. Keckler. 2016. vDNN: Virtualized deep neural networks for scalable,
    memory-efficient neural network design. In *49th Annual IEEE/ACM International
    Symposium on Microarchitecture, MICRO 2016, Taipei, Taiwan, October 15-19, 2016*.
    IEEE Computer Society, 18:1–18:13. [https://doi.org/10.1109/MICRO.2016.7783721](https://doi.org/10.1109/MICRO.2016.7783721)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rhu 等人（2016）Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar,
    和 Stephen W. Keckler. 2016. vDNN: 用于可扩展、内存高效神经网络设计的虚拟化深度神经网络。发表于*第49届年度IEEE/ACM国际微架构研讨会,
    MICRO 2016, 2016年10月15-19日, 台湾台北*。IEEE计算机学会, 18:1–18:13。 [https://doi.org/10.1109/MICRO.2016.7783721](https://doi.org/10.1109/MICRO.2016.7783721)'
- en: 'Ribar et al. (2023) Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie
    Blake, Carlo Luschi, and Douglas Orr. 2023. SparQ Attention: Bandwidth-Efficient
    LLM Inference. *CoRR* abs/2312.04985 (2023). [https://doi.org/10.48550/ARXIV.2312.04985](https://doi.org/10.48550/ARXIV.2312.04985)
    arXiv:2312.04985'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribar 等人（2023）Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake,
    Carlo Luschi, 和 Douglas Orr. 2023. SparQ Attention: 带宽高效 LLM 推理。*CoRR* abs/2312.04985（2023年）。
    [https://doi.org/10.48550/ARXIV.2312.04985](https://doi.org/10.48550/ARXIV.2312.04985)
    arXiv:2312.04985'
- en: 'Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023.
    FlexGen: High-Throughput Generative Inference of Large Language Models with a
    Single GPU. In *International Conference on Machine Learning, ICML 2023, 23-29
    July 2023, Honolulu, Hawaii, USA* *(Proceedings of Machine Learning Research)*,
    Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
    and Jonathan Scarlett (Eds.), Vol. 202\. PMLR, 31094–31116. [https://proceedings.mlr.press/v202/sheng23a.html](https://proceedings.mlr.press/v202/sheng23a.html)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sheng 等人（2023）Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,
    Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, 和 Ce Zhang. 2023. FlexGen:
    高吞吐量的单 GPU 大语言模型生成推理。发表于*国际机器学习大会，ICML 2023, 2023年7月23-29日, 夏威夷檀香山, 美国* *(机器学习研究论文集)*，Andreas
    Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, 和 Jonathan
    Scarlett（编者），第202卷。PMLR, 31094–31116。 [https://proceedings.mlr.press/v202/sheng23a.html](https://proceedings.mlr.press/v202/sheng23a.html)'
- en: 'Shi et al. (2020) Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and
    Jiyan Yang. 2020. Compositional Embeddings Using Complementary Partitions for
    Memory-Efficient Recommendation Systems. In *KDD ’20: The 26th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27,
    2020*, Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (Eds.). ACM,
    165–175. [https://doi.org/10.1145/3394486.3403059](https://doi.org/10.1145/3394486.3403059)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等人（2020）Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, 和 Jiyan
    Yang. 2020. 使用互补分区的组合嵌入以实现内存高效推荐系统。发表于*KDD ’20: 第26届ACM SIGKDD知识发现与数据挖掘大会, 虚拟会议,
    CA, 美国, 2020年8月23-27日*，Rajesh Gupta, Yan Liu, Jiliang Tang, 和 B. Aditya Prakash（编者）。ACM,
    165–175。 [https://doi.org/10.1145/3394486.3403059](https://doi.org/10.1145/3394486.3403059)'
- en: 'Strati et al. (2024) Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub
    Tarnawski, and Ana Klimovic. 2024. D$\backslash$aVu: KV-cache Streaming for Fast,
    Fault-tolerant Generative LLM Serving. *arXiv preprint arXiv:2403.01876* (2024).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Strati 等人（2024）Foteini Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski,
    和 Ana Klimovic. 2024. D$\backslash$aVu: KV-cache 流式传输以实现快速、容错的生成 LLM 服务。*arXiv
    预印本 arXiv:2403.01876*（2024年）。'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca:
    A strong, replicable instruction-following model. *Stanford Center for Research
    on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html* 3,
    6 (2023), 7.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori 等人（2023）Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B Hashimoto. 2023. Alpaca: 一个强大且可复制的指令跟随模型。*斯坦福基础模型研究中心。
    https://crfm. stanford. edu/2023/03/13/alpaca. html* 3, 6（2023年），7。'
- en: Together.ai (2023) Together.ai. 2023. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Together.ai (2023) Together.ai. 2023. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K)
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. *CoRR* abs/2307.09288 (2023). [https://doi.org/10.48550/ARXIV.2307.09288](https://doi.org/10.48550/ARXIV.2307.09288)
    arXiv:2307.09288'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等人（2023）雨果·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔迈赫里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、索姆雅·巴特拉、普拉吉瓦尔·巴尔加瓦、施鲁提·博萨尔、丹·比克尔、卢卡斯·布莱切尔、克里斯蒂安·坎顿-费雷、莫亚·陈、吉列姆·库库鲁、戴维·埃休布、裘德·费尔南德斯、杰里米·傅、温银·傅、布莱恩·富勒、辛西娅·高、维达努杰·戈斯瓦米、纳曼·戈亚尔、安东尼·哈特肖恩、萨赫尔·霍塞尼、瑞·侯、哈坎·伊南、马尔钦·卡达斯、维克托·科尔凯兹、马迪安·卡布萨、伊莎贝尔·克劳曼、阿尔捷姆·科尔涅夫、普尼特·辛格·库拉、玛丽-安·拉肖、提博·拉夫里尔、詹尼亚·李、戴安娜·利斯科维奇、英海·卢、余宁·毛、泽维尔·马蒂奈、托多尔·米哈伊洛夫、普什卡尔·米什拉、伊戈尔·莫利博格、易欣·聂、安德鲁·普尔顿、杰里米·雷泽斯坦、拉希·荣塔、卡利扬·萨拉迪、艾伦·谢尔滕、阮·席尔瓦、埃里克·迈克尔·史密斯、兰詹·苏布拉马尼安、肖青·艾伦·谭、斌·唐、罗斯·泰勒、阿迪娜·威廉姆斯、简·向宽、普新·徐、郑彦、伊利扬·扎罗夫、余辰·张、安吉拉·范、梅兰妮·坎巴杜尔、沙兰·纳朗、奥雷利安·罗德里格斯、罗伯特·斯托尼奇、谢尔盖·埃杜诺夫和托马斯·斯西亚洛姆。2023年。《Llama
    2：开放基础和微调聊天模型》。*CoRR* abs/2307.09288 (2023)。 [https://doi.org/10.48550/ARXIV.2307.09288](https://doi.org/10.48550/ARXIV.2307.09288)
    arXiv:2307.09288
- en: 'van den Oord et al. (2017) Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
    2017. Neural Discrete Representation Learning. In *Advances in Neural Information
    Processing Systems 30: Annual Conference on Neural Information Processing Systems
    2017, December 4-9, 2017, Long Beach, CA, USA*, Isabelle Guyon, Ulrike von Luxburg,
    Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett
    (Eds.). 6306–6315. [https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Oord等人（2017）阿龙·范·登·奥德、奥里奥尔·维尼亚尔斯和科拉伊·卡夫库克格鲁。2017年。《神经离散表示学习》。收录于*神经信息处理系统进展
    30：2017年神经信息处理系统年会，2017年12月4-9日，加州洛杉矶，USA*，伊莎贝尔·盖永、乌尔里克·冯·卢克斯堡、萨米·本吉奥、汉娜·M·沃拉赫、罗布·费格斯、S·V·N·维什瓦纳桑和罗曼·加尔内特（编）。6306–6315。
    [https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html)
- en: Wang et al. (2021) Mengzhao Wang, Xiaoliang Xu, Qiang Yue, and Yuxiang Wang.
    2021. A Comprehensive Survey and Experimental Comparison of Graph-Based Approximate
    Nearest Neighbor Search. *Proc. VLDB Endow.* 14, 11 (2021), 1964–1978. [https://doi.org/10.14778/3476249.3476255](https://doi.org/10.14778/3476249.3476255)
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2021）孟钊·王、肖亮·徐、强跃、余翔·王。2021年。《基于图的近似最近邻搜索的综合调查与实验比较》。*Proc. VLDB Endow.*
    14, 11 (2021), 1964–1978。 [https://doi.org/10.14778/3476249.3476255](https://doi.org/10.14778/3476249.3476255)
- en: 'Wang and Gan (2024) Zihao Wang and Shaoduo Gan. 2024. SqueezeAttention: 2D
    Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget. *arXiv
    preprint arXiv:2404.04793* (2024).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang和Gan（2024）紫浩·王和少铎·甘。2024年。《SqueezeAttention：通过逐层优化预算进行LLM推理中的2D KV-Cache管理》。*arXiv预印本
    arXiv:2404.04793* (2024)。
- en: 'Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models. In *Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, Sanmi
    Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) 韦杰森、王雪智、达尔·舒尔曼斯、马滕·博斯马、布赖恩·伊赫特、夏飞、Ed H. Chi、Quoc V. Le 和邹登尼。2022年。《链式思维提示在大型语言模型中引发推理》。在*《神经信息处理系统进展
    35：2022年神经信息处理系统年会，NeurIPS 2022，美国路易斯安那州新奥尔良，2022年11月28日至12月9日》*中，Sanmi Koyejo、S.
    Mohamed、A. Agarwal、Danielle Belgrave、K. Cho 和 A. Oh（编）。[http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
- en: 'Xiao et al. (2024) Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai
    Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. 2024. InfLLM: Unveiling
    the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with
    Training-Free Memory. *CoRR* abs/2402.04617 (2024). [https://doi.org/10.48550/ARXIV.2402.04617](https://doi.org/10.48550/ARXIV.2402.04617)
    arXiv:2402.04617'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2024) 萧超俊、张鹏乐、韩旭、肖光轩、林彦凯、张征言、刘志远、韩松和孙茂松。2024年。《InfLLM：揭示LLM在理解极长序列中的内在能力，无需训练的记忆》。*CoRR*
    abs/2402.04617 (2024)。[https://doi.org/10.48550/ARXIV.2402.04617](https://doi.org/10.48550/ARXIV.2402.04617)
    arXiv:2402.04617
- en: Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. Efficient Streaming Language Models with Attention Sinks. *CoRR*
    abs/2309.17453 (2023). [https://doi.org/10.48550/ARXIV.2309.17453](https://doi.org/10.48550/ARXIV.2309.17453)
    arXiv:2309.17453
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) 肖光轩、田宇东、陈贝迪、韩松和迈克·刘易斯。2023年。《具有注意力接收器的高效流式语言模型》。*CoRR* abs/2309.17453
    (2023)。[https://doi.org/10.48550/ARXIV.2309.17453](https://doi.org/10.48550/ARXIV.2309.17453)
    arXiv:2309.17453
- en: 'Yang et al. (2024) June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon,
    Gunho Park, Eunho Yang, Se Jung Kwon, and Dongsoo Lee. 2024. No Token Left Behind:
    Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization.
    *CoRR* abs/2402.18096 (2024). [https://doi.org/10.48550/ARXIV.2402.18096](https://doi.org/10.48550/ARXIV.2402.18096)
    arXiv:2402.18096'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024) 杨永阳、金炳旭、裴政仁、权范硕、朴根浩、杨恩浩、权世钟和李东洙。2024年。《没有遗漏的标记：通过重要性感知混合精度量化实现可靠的KV缓存压缩》。*CoRR*
    abs/2402.18096 (2024)。[https://doi.org/10.48550/ARXIV.2402.18096](https://doi.org/10.48550/ARXIV.2402.18096)
    arXiv:2402.18096
- en: 'Zhang et al. (2024) Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong
    Zhao, Tong Yang, and Bin Cui. 2024. CAFE: Towards Compact, Adaptive, and Fast
    Embedding for Large-scale Recommendation Models. *Proceedings of the ACM on Management
    of Data* 2, 1 (2024), 1–28.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024) 张海林、刘子瑞、陈博轩、赵易开、赵彤、杨彤和崔彬。2024年。《CAFE：面向大规模推荐模型的紧凑、自适应和快速嵌入》。*ACM数据管理会议论文集*
    2, 1 (2024), 1–28。
- en: 'Zhang et al. (2023b) Hailin Zhang, Yujing Wang, Qi Chen, Ruiheng Chang, Ting
    Zhang, Ziming Miao, Yingyan Hou, Yang Ding, Xupeng Miao, Haonan Wang, Bochen Pang,
    Yuefeng Zhan, Hao Sun, Weiwei Deng, Qi Zhang, Fan Yang, Xing Xie, Mao Yang, and
    Bin Cui. 2023b. Model-enhanced Vector Index. In *Advances in Neural Information
    Processing Systems 36: Annual Conference on Neural Information Processing Systems
    2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).
    [http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023b) 张海林、王玉静、陈奇、常瑞恒、张婷、苗子铭、侯颖艳、丁杨、苗旭鹏、王浩南、庞博晨、詹岳峰、孙浩、邓伟伟、张琦、杨凡、谢兴和杨茂生。2023b年。《模型增强的向量索引》。在*《神经信息处理系统进展
    36：2023年神经信息处理系统年会，NeurIPS 2023，美国路易斯安那州新奥尔良，2023年12月10日至16日》*中，Alice Oh、Tristan
    Naumann、Amir Globerson、Kate Saenko、Moritz Hardt 和 Sergey Levine（编）。[http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/ac112e8ffc4e5b9ece32070440a8ca43-Abstract-Conference.html)
- en: Zhang et al. (2023c) Hailin Zhang, Penghao Zhao, Xupeng Miao, Yingxia Shao,
    Zirui Liu, Tong Yang, and Bin Cui. 2023c. Experimental Analysis of Large-scale
    Learnable Vector Storage Compression. *Proc. VLDB Endow.* 17, 4 (2023), 808–822.
    [https://www.vldb.org/pvldb/vol17/p808-zhang.pdf](https://www.vldb.org/pvldb/vol17/p808-zhang.pdf)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023c）Hailin Zhang, Penghao Zhao, Xupeng Miao, Yingxia Shao, Zirui
    Liu, Tong Yang, 和 Bin Cui. 2023c. 大规模可学习向量存储压缩的实验分析。*Proc. VLDB Endow.* 17, 4
    (2023), 808–822. [https://www.vldb.org/pvldb/vol17/p808-zhang.pdf](https://www.vldb.org/pvldb/vol17/p808-zhang.pdf)
- en: 'Zhang et al. (2023a) Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen,
    Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, and Beidi Chen. 2023a. H2O: Heavy-Hitter Oracle for Efficient
    Generative Inference of Large Language Models. In *Advances in Neural Information
    Processing Systems 36: Annual Conference on Neural Information Processing Systems
    2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.).
    [http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2023a）Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett,
    Zhangyang Wang, 和 Beidi Chen. 2023a. H2O: 高频重叠预言机用于大语言模型高效生成推理。在 *Advances in
    Neural Information Processing Systems 36: Annual Conference on Neural Information
    Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
    2023*，Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, 和
    Sergey Levine（编辑）。[http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html)'
- en: 'Zhong et al. (2024) Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo
    Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill
    and Decoding for Goodput-optimized Large Language Model Serving. *CoRR* abs/2401.09670
    (2024). [https://doi.org/10.48550/ARXIV.2401.09670](https://doi.org/10.48550/ARXIV.2401.09670)
    arXiv:2401.09670'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等人（2024）Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe
    Liu, Xin Jin, 和 Hao Zhang. 2024. DistServe: 为了良好的吞吐量优化大语言模型服务的预填充和解码分离。*CoRR*
    abs/2401.09670 (2024). [https://doi.org/10.48550/ARXIV.2401.09670](https://doi.org/10.48550/ARXIV.2401.09670)
    arXiv:2401.09670'
