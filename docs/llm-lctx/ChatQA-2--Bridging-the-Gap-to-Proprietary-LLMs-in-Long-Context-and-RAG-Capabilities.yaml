- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:00:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:00:44'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ChatQA 2: 缩小开放和专有LLM在长上下文和RAG能力上的差距'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14482](https://ar5iv.labs.arxiv.org/html/2407.14482)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.14482](https://ar5iv.labs.arxiv.org/html/2407.14482)
- en: Peng Xu^∗,  Wei Ping^∗,  Xianchao Wu,  Zihan Liu, Mohammad Shoeybi,  Bryan Catanzaro
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 彭旭^∗、魏平^∗、吴献超、刘子寒、穆罕默德·肖伊比、布莱恩·卡坦扎罗
- en: NVIDIA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: ^∗{pengx, wping}@nvidia.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗{pengx, wping}@nvidia.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this work, we introduce ChatQA 2, a Llama3-based model designed to bridge
    the gap between open-access LLMs and leading proprietary models (e.g., GPT-4-Turbo)
    in long-context understanding and retrieval-augmented generation (RAG) capabilities.
    These two capabilities are essential for LLMs to process large volumes of information
    that cannot fit into a single prompt and are complementary to each other, depending
    on the downstream tasks and computational budgets. We present a detailed continued
    training recipe to extend the context window of Llama3-70B-base from 8K to 128K
    tokens, along with a three-stage instruction tuning process to enhance the model’s
    instruction-following, RAG performance, and long-context understanding capabilities.
    Our results demonstrate that the $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    model achieves accuracy comparable to GPT-4-Turbo-2024-0409 on many long-context
    understanding tasks and surpasses it on the RAG benchmark. Interestingly, we find
    that the long-context retriever can alleviate the top-*k* context fragmentation
    issue in RAG, further improving RAG-based results for long-context understanding
    tasks. We also provide extensive comparisons between RAG and long-context solutions
    using state-of-the-art long-context LLMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了ChatQA 2，一种基于Llama3的模型，旨在弥合开放访问LLMs与领先专有模型（如GPT-4-Turbo）在长上下文理解和检索增强生成（RAG）能力上的差距。这两种能力对于LLMs处理无法在单次提示中容纳的大量信息至关重要，并且彼此互补，取决于下游任务和计算预算。我们提出了一种详细的持续训练方案，将Llama3-70B-base的上下文窗口从8K扩展到128K标记，并通过三阶段的指令调优过程来增强模型的指令跟随、RAG性能和长上下文理解能力。我们的结果表明，$\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$模型在许多长上下文理解任务上的准确性与GPT-4-Turbo-2024-0409相当，并在RAG基准上超越了它。有趣的是，我们发现长上下文检索器可以缓解RAG中的top-*k*上下文碎片化问题，进一步改善长上下文理解任务的RAG结果。我们还提供了使用最先进的长上下文LLMs对RAG和长上下文解决方案的广泛比较。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The open LLM community has made significant progress in advancing the capabilities
    of open-access large language models (LLMs), including Llama-3-70B-Instruct (Meta-AI,
    [2024](#bib.bib29)), QWen2-72B-Instruct (Alibaba-QWen, [2024](#bib.bib2)), Nemotron-4-340B-Instruct (Nvidia
    et al., [2024](#bib.bib34)), and Mixtral-8x22B-Instruct-v0.1 (Mistral, [2024](#bib.bib30)).
    However, performance gaps compared to frontier proprietary models, e.g., GPT-4-Turbo (OpenAI,
    [2023](#bib.bib35)), still exist in many domains. Additionally, open-access models
    focused on key domains have been developed, such as DeepSeek-Coder-V2 (Zhu et al.,
    [2024](#bib.bib51)) for coding and math, ChatQA 1.5 (Liu et al., [2024](#bib.bib28))
    for conversational QA and retrieval-augmented generation (RAG), and InternVL 1.5 (Chen
    et al., [2024](#bib.bib7)) for vision-language tasks, which can be on par with
    GPT-4-Turbo-2024-04-09 (OpenAI, [2023](#bib.bib35)) in the certain domains.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 开放LLM社区在推进开放访问的大型语言模型（LLMs）的能力方面取得了重大进展，包括Llama-3-70B-Instruct（Meta-AI, [2024](#bib.bib29)）、QWen2-72B-Instruct（Alibaba-QWen,
    [2024](#bib.bib2)）、Nemotron-4-340B-Instruct（Nvidia等, [2024](#bib.bib34)）和Mixtral-8x22B-Instruct-v0.1（Mistral,
    [2024](#bib.bib30)）。然而，相较于前沿专有模型，例如GPT-4-Turbo（OpenAI, [2023](#bib.bib35)），在许多领域仍存在性能差距。此外，专注于关键领域的开放访问模型也已被开发，如用于编码和数学的DeepSeek-Coder-V2（Zhu等,
    [2024](#bib.bib51)）、用于对话QA和检索增强生成（RAG）的ChatQA 1.5（Liu等, [2024](#bib.bib28)）以及用于视觉语言任务的InternVL
    1.5（Chen等, [2024](#bib.bib7)），这些模型在某些领域可以与GPT-4-Turbo-2024-04-09（OpenAI, [2023](#bib.bib35)）相媲美。
- en: In recent developments, the trend of extending the context window length in
    LLMs has gained remarkable traction within both the industrial and research communities.
    All leading proprietary LLMs support very large context window, allowing them
    to accommodate several hundred pages of text in a single prompt. For example,
    GPT-4 Turbo (OpenAI, [2023](#bib.bib35)) and Claude 3.5 Sonnet offer a 128K and
    200K context window, respectively. Meanwhile, Gemini 1.5 Pro (Gemini-Team, [2024](#bib.bib9))
    impressively supports up to a 10M context. Open-access LLMs have also made significant
    strides to keep up (01.AI et al., [2024](#bib.bib1); Alibaba-QWen, [2024](#bib.bib2)).
    For instance, QWen2-72B-Instruct (Alibaba-QWen, [2024](#bib.bib2)) and Yi-34B (01.AI
    et al., [2024](#bib.bib1)) support 128K and 200K context windows, respectively.
    However, the training data and technical details for these models are missing,
    making reproduction challenging. In addition, these models have mostly been evaluated
    on synthetic tasks, like Needle in a Haystack (Kamradt, [2023](#bib.bib18)) test,
    which does not accurately represent real-world downstream task performance. For
    example, previous studies shows a noticeable gap between open-access LLMs and
    leading proprietary models on real-world long context understanding tasks (Zhang
    et al., [2024a](#bib.bib49); Hsieh et al., [2024](#bib.bib13)). In this work,
    we focus on bridging the gap between the open-access Llama-3 and proprietary GPT-4
    Turbo on real-world long context understanding tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的发展中，LLM 中扩展上下文窗口长度的趋势在工业和研究界都获得了显著关注。所有领先的专有 LLM 都支持非常大的上下文窗口，允许它们在单个提示中容纳几百页文本。例如，GPT-4
    Turbo（OpenAI，[2023](#bib.bib35)）和 Claude 3.5 Sonnet 分别提供了 128K 和 200K 的上下文窗口。同时，Gemini
    1.5 Pro（Gemini-Team，[2024](#bib.bib9)）令人印象深刻地支持高达 10M 的上下文。开放访问的 LLM 也取得了显著进展（01.AI
    等，[2024](#bib.bib1)；阿里巴巴-QWen，[2024](#bib.bib2)）。例如，QWen2-72B-Instruct（阿里巴巴-QWen，[2024](#bib.bib2)）和
    Yi-34B（01.AI 等，[2024](#bib.bib1)）分别支持 128K 和 200K 的上下文窗口。然而，这些模型的训练数据和技术细节缺失，导致重现挑战。此外，这些模型大多在合成任务（如
    Needle in a Haystack（Kamradt，[2023](#bib.bib18)）测试）上进行了评估，这并不能准确代表真实世界的下游任务性能。例如，先前的研究显示，开放访问
    LLM 和领先的专有模型在真实世界的长上下文理解任务上存在明显差距（Zhang 等，[2024a](#bib.bib49)；Hsieh 等，[2024](#bib.bib13)）。在这项工作中，我们重点关注缩小开源
    Llama-3 和专有 GPT-4 Turbo 在真实世界长上下文理解任务中的差距。
- en: The long-context capability of LLMs is sometimes considered a rival technique
    to retrieval-augmented generation (RAG). However, from a pragmatic perspective,
    these techniques complement each other. An LLM with a long context window can
    either process large volumes of text as a prompt or utilize retrieval methods
    to efficiently extract relevant information from the extensive text, depending
    on the downstream tasks and accuracy vs. efficiency trade-offs. RAG has efficiency
    advantages and can easily retrieve relevant contexts for query-based tasks (e.g,
    QA) from billions of tokens, a feat that long context models cannot achieve. Meanwhile,
    long context models are good at tasks such as summarizing entire documents, where
    RAG may not perform as well. As a result, the state-of-the-art LLM needs to excel
    at both capabilities, providing options for different downstream tasks based on
    accuracy and efficiency requirements. In a previous study, the open-source ChatQA
    1.5 (Liu et al., [2024](#bib.bib28)) model can surpass GPT-4-Turbo on RAG tasks.
    In this work, we present ChatQA 2, which possesses both GPT-4-Turbo level long
    context understanding capability and RAG performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的长上下文能力有时被视为检索增强生成（RAG）的竞争技术。然而，从务实的角度来看，这些技术是互补的。具有长上下文窗口的 LLM 可以根据下游任务和准确性与效率的权衡，处理大量文本作为提示，或利用检索方法从广泛的文本中高效提取相关信息。RAG
    具有效率优势，可以轻松从数十亿个标记中检索与查询相关的上下文（例如 QA），这是长上下文模型无法实现的。同时，长上下文模型擅长诸如总结整个文档等任务，而 RAG
    在这方面可能表现不佳。因此，最先进的 LLM 需要在这两种能力上都表现出色，根据准确性和效率要求提供不同下游任务的选项。在之前的研究中，开源 ChatQA
    1.5（Liu 等，[2024](#bib.bib28)）模型在 RAG 任务上超过了 GPT-4-Turbo。在这项工作中，我们展示了 ChatQA 2，它具有与
    GPT-4-Turbo 相当的长上下文理解能力和 RAG 性能。
- en: Xu et al. ([2024](#bib.bib47)) extended the context window of Llama2 (Touvron
    et al., [2023b](#bib.bib40)) to 16K and 32K tokens, and studied the interplay
    between RAG and long-context LLMs. It demonstrates that the RAG method can improve
    the generation accuracy and inference efficiency of a GPT-3.5-turbo-16k level
    long context model for QA and query-based summarization tasks. In this work, we
    extend this study by pushing the long context LLM to GPT-4-Turbo level with 128K
    context window and combining it with a state-of-the-art long-context retriever
    for RAG.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Xu等人（[2024](#bib.bib47)）将Llama2（Touvron等人，[2023b](#bib.bib40)）的上下文窗口扩展到16K和32K个标记，并研究了RAG与长上下文LLMs之间的相互作用。结果表明，RAG方法可以提高GPT-3.5-turbo-16k级别长上下文模型在QA和基于查询的摘要任务中的生成准确性和推理效率。在这项工作中，我们通过将长上下文LLM推向GPT-4-Turbo级别，配备128K上下文窗口，并与最先进的长上下文检索器结合，扩展了这一研究。
- en: 'Specifically, we make the following contributions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们做出了以下贡献：
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We present a two-step approach to establish the long context capability of Llama3-70B.
    First, we extend Llama3-70B base’s context window from 8K to 128K by continually
    pretraining it on a mix of SlimPajama (Soboleva et al., [2023](#bib.bib37)) with
    upsampled long sequences (Fu et al., [2024](#bib.bib8)). Then, we apply a three-stage
    instruction tuning process on curated datasets to enhance the instruction-following,
    RAG, and long context understanding capabilities at each respective stage. We
    find that stage-wise instruction-tuning, by incorporating previous datasets, simplifies
    experimentation and hyperparameter tuning. This approach enhances long context
    capabilities while maintaining RAG performance.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种两步法来建立Llama3-70B的长上下文能力。首先，我们通过在混合的SlimPajama（Soboleva等人，[2023](#bib.bib37)）和上采样长序列（Fu等人，[2024](#bib.bib8)）上持续预训练，将Llama3-70B基础模型的上下文窗口从8K扩展到128K。然后，我们在精心策划的数据集上应用了三阶段指令调优过程，以增强每个阶段的指令跟随、RAG和长上下文理解能力。我们发现，逐阶段的指令调优通过融入先前的数据集，简化了实验和超参数调整。这种方法在保持RAG性能的同时，增强了长上下文能力。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We demonstrate that the resulting $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B\text{-}128K}$
    can be on par or slightly worse than GPT-4-Turbo-2024-04-09 on many real-world
    long context understanding tasks. In addition, it outperforms GPT-4-Turbo-2024-04-09
    on RAG and conversational QA tasks.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们证明了最终得到的$\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B\text{-}128K}$在许多现实世界的长上下文理解任务中，可以与GPT-4-Turbo-2024-04-09持平或稍逊一筹。此外，它在RAG和对话式QA任务中表现优于GPT-4-Turbo-2024-04-09。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'The current RAG pipeline has limitations that can undermine downstream task
    accuracy: *i)* top-*k* chunk-wise retrieval introduces fragmentation of context;
    and *ii)* a small top-*k* leads to low recall, while a larger *k* introduces too
    much irrelevant context to the LLM (e.g., see the analysis in Figure 1 of Yu et al.,
    [2024](#bib.bib48)). We find that the state-of-the-art long-context retriever (Wang
    et al., [2023c](#bib.bib45); Lee et al., [2024](#bib.bib21)) can largely alleviate
    these issues and further improve the RAG-based results for long-context understanding
    tasks.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前的RAG流程存在一些限制，这些限制可能会影响下游任务的准确性：*i)* top-*k*块级检索引入了上下文的碎片化；以及 *ii)* 小的top-*k*会导致召回率低，而较大的*k*则会引入过多与LLM无关的上下文（例如，见Yu等人的分析，[2024](#bib.bib48)）。我们发现，最先进的长上下文检索器（Wang等人，[2023c](#bib.bib45)；Lee等人，[2024](#bib.bib21)）可以在很大程度上缓解这些问题，并进一步提高基于RAG的长上下文理解任务的结果。
- en: '4.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: In the comparison between RAG and long-context results, we find that the GPT-4-Turbo
    level long-context model (including our $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$)
    outperforms the RAG on 32K benchmarks but still underperforms compared to RAG
    methods on real-world 128K tasks.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在RAG与长上下文结果的比较中，我们发现GPT-4-Turbo级别的长上下文模型（包括我们的$\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$）在32K基准测试中优于RAG，但在实际的128K任务中仍表现不如RAG方法。
- en: 'We organize the rest of the paper as follows. We discuss related work in § [2](#S2
    "2 Related Work ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context
    and RAG Capabilities"). We introduce the continued pretraining for context window
    extension in § [3](#S3 "3 Extending Context Window to 128K ‣ ChatQA 2: Bridging
    the Gap to Proprietary LLMs in Long Context and RAG Capabilities") and the three-stage
    instruction tuning in § [4](#S4 "4 Instruction-Tuning with Long Context Data ‣
    ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities").
    We report results in § [7](#S7 "7 Results ‣ ChatQA 2: Bridging the Gap to Proprietary
    LLMs in Long Context and RAG Capabilities") and conclude the paper in § [8](#S8
    "8 Conclusion ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context
    and RAG Capabilities").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将论文其余部分组织如下。我们在§ [2](#S2 "2 Related Work ‣ ChatQA 2: Bridging the Gap to
    Proprietary LLMs in Long Context and RAG Capabilities")讨论相关工作。在§ [3](#S3 "3 Extending
    Context Window to 128K ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long
    Context and RAG Capabilities")介绍用于上下文窗口扩展的持续预训练，并在§ [4](#S4 "4 Instruction-Tuning
    with Long Context Data ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long
    Context and RAG Capabilities")介绍三阶段指令调优。我们在§ [7](#S7 "7 Results ‣ ChatQA 2: Bridging
    the Gap to Proprietary LLMs in Long Context and RAG Capabilities")报告结果，并在§ [8](#S8
    "8 Conclusion ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context
    and RAG Capabilities")总结全文。'
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Long Context LLM
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 长上下文LLM
- en: The trend of extending the context window in LLM starts by Claude with 100K
    token context (Anthropic, [2023](#bib.bib3)). Although the underlying long context
    techniques behind proprietary models are unclear, the open LLM and research community
    has developed many methods to extend the context window of LLMs through continued
    training or fine-tuning (Kaiokendev, [2023](#bib.bib17); Nijkamp et al., [2023](#bib.bib32);
    Chen et al., [2023a](#bib.bib5); Tworkowski et al., [2023](#bib.bib41); Chen et al.,
    [2023b](#bib.bib6); Peng et al., [2023](#bib.bib36); Xiong et al., [2023](#bib.bib46);
    Fu et al., [2024](#bib.bib8)), especially for open-access LLMs (Touvron et al.,
    [2023a](#bib.bib39), [b](#bib.bib40)) based on rotary position embedding (RoPE) (Su
    et al., [2024](#bib.bib38)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展LLM上下文窗口的趋势始于Claude，其具有100K token的上下文（Anthropic，[2023](#bib.bib3)）。虽然专有模型背后的长上下文技术不明确，但开放LLM和研究社区已经开发了许多通过持续训练或微调扩展LLM上下文窗口的方法（Kaiokendev，[2023](#bib.bib17)；Nijkamp等，[2023](#bib.bib32)；Chen等，[2023a](#bib.bib5)；Tworkowski等，[2023](#bib.bib41)；Chen等，[2023b](#bib.bib6)；Peng等，[2023](#bib.bib36)；Xiong等，[2023](#bib.bib46)；Fu等，[2024](#bib.bib8)），尤其是基于旋转位置嵌入（RoPE）的开放访问LLMs（Touvron等，[2023a](#bib.bib39)，[b](#bib.bib40)）（Su等，[2024](#bib.bib38)）。
- en: 'There are two popular approaches to adapt RoPE for long-context inputs: position
    interpolation (Chen et al., [2023a](#bib.bib5)) and increasing the base frequency
    $\theta$.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种流行的方法可以将RoPE适应于长上下文输入：位置插值（Chen等，[2023a](#bib.bib5)）和增加基础频率$\theta$。
- en: 2.2 Retrieval-augmented Generation (RAG)
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 检索增强生成（RAG）
- en: Retrieval with a standalone retriever (e.g., Karpukhin et al., [2020](#bib.bib19);
    Wang et al., [2022](#bib.bib44); Lin et al., [2023](#bib.bib25); Lee et al., [2024](#bib.bib21))
    is a long-standing solution for handling long texts that cannot fit into the context
    window of language models. In previous work, various retrieval-augmented language
    models have been proposed (Nakano et al., [2021](#bib.bib31); Borgeaud et al.,
    [2022](#bib.bib4); Wang et al., [2023b](#bib.bib43), [a](#bib.bib42); Guu et al.,
    [2020](#bib.bib12); Izacard & Grave, [2021](#bib.bib15); Izacard et al., [2022](#bib.bib16);
    Lewis et al., [2020](#bib.bib22); Huang et al., [2023](#bib.bib14); Khandelwal
    et al., [2019](#bib.bib20); Liu et al., [2024](#bib.bib28)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独立检索器进行检索（例如，Karpukhin等，[2020](#bib.bib19)；Wang等，[2022](#bib.bib44)；Lin等，[2023](#bib.bib25)；Lee等，[2024](#bib.bib21)）是一种长期以来处理无法适应语言模型上下文窗口的长文本的解决方案。在以往的研究中，已提出了各种检索增强语言模型（Nakano等，[2021](#bib.bib31)；Borgeaud等，[2022](#bib.bib4)；Wang等，[2023b](#bib.bib43)，[a](#bib.bib42)；Guu等，[2020](#bib.bib12)；Izacard
    & Grave，[2021](#bib.bib15)；Izacard等，[2022](#bib.bib16)；Lewis等，[2020](#bib.bib22)；Huang等，[2023](#bib.bib14)；Khandelwal等，[2019](#bib.bib20)；Liu等，[2024](#bib.bib28)）。
- en: Previous dense-embedding-based retrievers only supported limited context windows
    (e.g., 512 tokens) (e.g., Karpukhin et al., [2020](#bib.bib19); Wang et al., [2022](#bib.bib44);
    Lin et al., [2023](#bib.bib25)). In top-*k* chunk-wise retrieval, the short chunk
    size increases context fragmentation. As a result, extending the context window
    of retrievers has become popular. For example, Jina Embeddings 2(Günther et al.,
    [2023](#bib.bib11)) and Nomic Embed (Nussbaum et al., [2024](#bib.bib33)) support
    8K tokens, while E5-mistral-7B (Wang et al., [2023c](#bib.bib45)) and NV-Embed Lee
    et al. ([2024](#bib.bib21)) support 32K tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以前基于密集嵌入的检索器仅支持有限的上下文窗口（例如，512 tokens）(例如，Karpukhin et al., [2020](#bib.bib19);
    Wang et al., [2022](#bib.bib44); Lin et al., [2023](#bib.bib25))。在 top-*k* 块级检索中，较短的块大小增加了上下文碎片化。因此，扩展检索器的上下文窗口变得越来越流行。例如，Jina
    Embeddings 2 (Günther et al., [2023](#bib.bib11)) 和 Nomic Embed (Nussbaum et al.,
    [2024](#bib.bib33)) 支持 8K tokens，而 E5-mistral-7B (Wang et al., [2023c](#bib.bib45))
    和 NV-Embed Lee et al. ([2024](#bib.bib21)) 支持 32K tokens。
- en: 3 Extending Context Window to 128K
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 扩展上下文窗口到 128K
- en: In this section, we present the method to extend the context window from 8K
    to 128K for Llama3. We prepare our long context pretraining corpus from the Slimpajama (Soboleva
    et al., [2023](#bib.bib37)) following Fu et al. ([2024](#bib.bib8)). We upsample
    long-context documents with the hyperparameter set as 0.1 to produce 10 billion
    tokens with sequence length of 128k. Since Llama3 is pretrained with a much higher
    RoPE base frequency of 500,000 compared to Llama2, we increased the RoPE base
    frequency to 150M accordingly. We set the batch size to 32 to fit 4 million tokens
    in a batch and use a learning rate of 3e-5 to train 2000 steps (8B tokens in total).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了将 Llama3 的上下文窗口从 8K 扩展到 128K 的方法。我们从 Slimpajama (Soboleva et al.,
    [2023](#bib.bib37)) 准备了我们的长上下文预训练语料库，遵循 Fu et al. ([2024](#bib.bib8)) 的方法。我们将长上下文文档的超参数设置为
    0.1 进行上采样，生成 100 亿 tokens，序列长度为 128k。由于 Llama3 的 RoPE 基频（500,000）远高于 Llama2，因此我们相应地将
    RoPE 基频增加到 150M。我们将批量大小设置为 32，以适应每批 400 万 tokens，并使用学习率 3e-5 进行 2000 步训练（总共 8B
    tokens）。
- en: Interestingly, we found it more effective to separate different documents using
    special characters, such as "", rather than the reserved beginning and ending
    tokens  and . We hypothesize that the  and  tokens in Llama3
    signal the model to ignore previous chunks of text after pretraining, which is
    not helpful for the LLMs to adapt for longer context inputs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们发现使用特殊字符，如“”，来分隔不同文档比使用保留的开始和结束标记  和  更有效。我们推测 Llama3 中的
     和  标记在预训练后会使模型忽略先前的文本片段，这不利于 LLMs 适应更长的上下文输入。
- en: 4 Instruction-Tuning with Long Context Data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用长上下文数据的指令调优
- en: In this section, we present the instruction tuning method designed to enhance
    both long context understanding capability and RAG performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了旨在提升长上下文理解能力和 RAG 性能的指令调优方法。
- en: Specifically, we implement three stages of instruction-tuning. For the first
    two stages, we follow ChatQA 1.5 (Liu et al., [2024](#bib.bib28)), where the model
    is initially trained on 128k high-quality instruction-following datasets, and
    then trained on a blend of conversational QA data with provided context. However,
    these two stages involve relatively short contexts, with a maximum sequence length
    of only 4K tokens. To enhance our model’s capability to handle very long context
    sequences up to 128k tokens, we collect a long SFT dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们具体实施了三个阶段的指令调优。在前两个阶段，我们遵循 ChatQA 1.5 (Liu et al., [2024](#bib.bib28))，其中模型最初在
    128k 高质量的指令跟随数据集上进行训练，然后在提供上下文的对话 QA 数据的混合数据上进行训练。然而，这两个阶段涉及的上下文相对较短，最大序列长度仅为
    4K tokens。为了提升我们模型处理长达 128k tokens 的上下文序列的能力，我们收集了一个长 SFT 数据集。
- en: 'This dataset is collected through two categories: 1) For SFT data sequences
    less than 32k: We leverage existing long-context datasets from LongAlpaca12k,
    GPT-4 samples from Open Orca ¹¹1[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca),
    and Long Data Collections ²²2[https://huggingface.co/datasets/togethercomputer/Long-Data-Collections](https://huggingface.co/datasets/togethercomputer/Long-Data-Collections).
    2) For sequence lengths between 32k and 128k: Since it is challenging to collect
    such SFT samples, we rely on synthetic datasets. We utilize NarrativeQA, which
    contains both the ground truth summary and semantically related paragraphs. We
    assemble all the related paragraphs and randomly insert the ground truth summary
    to simulate a real long document for its QA pairs. Both the full long SFT dataset
    and the short SFT dataset from the first two stages are then blended for training.
    We set the learning rate at 3e-5 and the batch size at 32.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集通过两类收集：1) 对于 SFT 数据序列少于32k的：我们利用来自 LongAlpaca12k 的现有长上下文数据集、Open Orca 的
    GPT-4 样本¹¹1[https://huggingface.co/datasets/Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)和
    Long Data Collections ²²2[https://huggingface.co/datasets/togethercomputer/Long-Data-Collections](https://huggingface.co/datasets/togethercomputer/Long-Data-Collections)。2)
    对于序列长度在32k到128k之间的：由于收集此类 SFT 样本具有挑战性，我们依赖于合成数据集。我们利用 NarrativeQA，它包含真实摘要和语义相关的段落。我们将所有相关段落组合起来，并随机插入真实摘要，以模拟一个真实的长文档用于其
    QA 对。然后，将来自前两个阶段的完整长 SFT 数据集和短 SFT 数据集混合进行训练。我们将学习率设置为 3e-5，批量大小设置为 32。
- en: 5 Long Context Retriever meets Long Context LLM
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 长上下文检索器遇到长上下文 LLM
- en: 'As we mentioned in previous section, the current RAG pipeline for LLM has the
    following issues: *i)* The top-*k* chunk-wise retrieval introduces non-negligible
    fragmentation of context for generating accurate answers. For example, previous
    state-of-the-art dense-embedding based retrievers (e.g., Li et al., [2023](#bib.bib23);
    Lin et al., [2023](#bib.bib25)) only support 512 tokens. *ii)* Small top-*k* (e.g.,
    5 or 10) usually leads to relatively low recall, while much larger *k* (e.g.,
    100) can lead to worse generation (see Table 5 in Xu et al. ([2024](#bib.bib47)))
    as the previous LLMs could not utilize too many chunked context very well (Liu
    et al., [2023a](#bib.bib26)). To address the issue, we propose to use the most
    recent long-context retriever (Wang et al., [2023c](#bib.bib45); Lee et al., [2024](#bib.bib21)),
    which can support thousands of tokens. In our setting, we use the E5-mistral embedding
    model (Wang et al., [2023c](#bib.bib45)) as the retriever.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一部分提到的，当前 LLM 的 RAG 流水线存在以下问题：*i)* top-*k* 块级检索引入了上下文的不可忽视的碎片化，从而影响准确答案的生成。例如，之前的先进稠密嵌入检索器（例如，Li
    et al.，[2023](#bib.bib23)；Lin et al.，[2023](#bib.bib25)）仅支持512个令牌。*ii)* 小的 top-*k*（例如5或10）通常会导致相对较低的召回率，而更大的
    *k*（例如100）可能会导致更差的生成（参见 Xu et al. 表格5 ([2024](#bib.bib47)）），因为之前的 LLM 无法很好地利用太多块状上下文（Liu
    et al.，[2023a](#bib.bib26)）。为了解决这个问题，我们建议使用最新的长上下文检索器（Wang et al.，[2023c](#bib.bib45)；Lee
    et al.，[2024](#bib.bib21)），它可以支持数千个令牌。在我们的设置中，我们使用 E5-mistral 嵌入模型（Wang et al.，[2023c](#bib.bib45)）作为检索器。
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5 Long Context Retriever meets Long Context LLM
    ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities")
    compares different chunk sizes for top-*k* retrieval and the total number of tokens
    in the context window. Comparing total tokens from 3000 to 12000, we found that
    more tokens consistently yield better results, confirming the strong long-context
    capability of our model. We also found that 6000 total tokens offer a good trade-off
    between cost and performance. With the total number of tokens set to 6000, we
    discovered that larger chunk sizes give better results. Therefore, we use a chunk
    size of 1200 and top-5 chunks as the default in our experiments.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[1](#S5.T1 "表格 1 ‣ 5 长上下文检索器与长上下文 LLM ‣ ChatQA 2: 连接专有 LLM 与长上下文和 RAG 能力的差距")比较了不同块大小的
    top-*k* 检索和上下文窗口中的总令牌数。比较从3000到12000的总令牌数，我们发现更多的令牌总是能带来更好的结果，确认了我们模型的强大长上下文能力。我们还发现6000个总令牌在成本和性能之间提供了良好的折中。将总令牌数设置为6000时，我们发现较大的块大小可以获得更好的结果。因此，我们在实验中使用了1200的块大小和前5个块作为默认设置。'
- en: '| chunk-size | 300 | 300 | 300 | 600 | 1200 | 1200 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 块大小 | 300 | 300 | 300 | 600 | 1200 | 1200 |'
- en: '| top-*k* | 10 | 20 | 40 | 10 | 5 | 10 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| top-*k* | 10 | 20 | 40 | 10 | 5 | 10 |'
- en: '| total tokens | 3000 | 6000 | 12000 | 6000 | 6000 | 12000 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 总令牌数 | 3000 | 6000 | 12000 | 6000 | 6000 | 12000 |'
- en: '| Avg. | 46.31 | 46.28 | 46.96 | 46.88 | 47.08 | 47.13 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 46.31 | 46.28 | 46.96 | 46.88 | 47.08 | 47.13 |'
- en: 'Table 1: Ablation of $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ with
    RAG given different top-*k* = {5, 10, 20, 40} retrieval, and chunk-size = {300,
    600, 1200} on medium-long context benchmarks within 32K tokens (see Section [6.2](#S6.SS2
    "6.2 Medium-Long Context Benchmarks within 32K Tokens ‣ 6 Evaluation Benchmarks
    ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities")
    for more details). Given the same budget of total tokens in the context window (e.g.,
    6000), larger chunk-size (e.g., 1200) gives better results than small chunk-size (e.g.,
    300 and 600). The accuracy can also improve with larger total tokens in the context
    window.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：在 32K 标记范围内的中长上下文基准上，$\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    与 RAG 进行不同 top-*k* = {5, 10, 20, 40} 检索和 chunk-size = {300, 600, 1200} 的消融实验（详见第
    [6.2](#S6.SS2 "6.2 Medium-Long Context Benchmarks within 32K Tokens ‣ 6 Evaluation
    Benchmarks ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and
    RAG Capabilities") 节）。在相同的上下文窗口总标记预算（例如，6000）下，较大的 chunk-size（例如，1200）比小的 chunk-size（例如，300
    和 600）表现更好。随着上下文窗口中总标记的增多，准确性也可以提高。'
- en: 6 Evaluation Benchmarks
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估基准
- en: 'We compare our model against SOTA long context models: 1) GPT-4-Turbo-2024-04-09
    (128K context window) (OpenAI, [2023](#bib.bib35)), 2) Qwen2-72B-Instruct (128K
    context window) (Alibaba-QWen, [2024](#bib.bib2)), and (3) Llama-3-70B-Instruct-Gradient-262k (GradientAI,
    [2024](#bib.bib10)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的模型与最先进的长上下文模型进行比较：1) GPT-4-Turbo-2024-04-09（128K 上下文窗口）（OpenAI，[2023](#bib.bib35)），2)
    Qwen2-72B-Instruct（128K 上下文窗口）（Alibaba-QWen，[2024](#bib.bib2)），和 3) Llama-3-70B-Instruct-Gradient-262k（GradientAI，[2024](#bib.bib10)）。
- en: To give a comprehensive study of different context lengths, our evaluation benchmarks
    covers three categories, 1) long context benchmarks beyond 100K tokens, 2) medium-long
    context benchmarks within 32K tokens, and 3) short context benchmarks within 4K
    tokens. We also apply RAG when it is applicable to the downstream tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面研究不同上下文长度，我们的评估基准覆盖了三类：1) 超过 100K 标记的长上下文基准，2) 在 32K 标记范围内的中长上下文基准，3) 在
    4K 标记范围内的短上下文基准。当适用于下游任务时，我们也会应用 RAG。
- en: 6.1 Long Context Benchmarks Beyond 100K Tokens
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 超过 100K 标记的长上下文基准
- en: InfiniteBench (Zhang et al., [2024b](#bib.bib50)) is proposed to evaluate the
    long context capability of LLMs over 100K sequence length. As we focus on real-world
    english tasks, we only take the four related tasks from the InfiniteBench, i.e.
    longbook summarization (En.Sum), longbook qa (En.QA), longbook multiple choice
    (En.MC), and longbook dialogue (En.Dia). En.Sum is a task that requires models
    to generate a concise summary of the given novel and is evaluated using the ROUGE-L-Sum
    metric (Lin, [2004](#bib.bib24)). En.QA is annotated by a pipeline that ensures
    the questions’ necessitating of long-range dependencies and reasoning, beyond
    simple short passage retrieval. Aggregation reasoning and filtering reasoning
    are the two primary reasoning categories. F1 score is used to evaluate the quality
    of the answer. En.MC is annotated with the same pipeline of En.QA except that
    four answer choices are provided and exact matching scores are reported. En.Dia
    leverages movie and drama scripts from a designated online database ³³3[https://imsdb.com](https://imsdb.com)
    with long, multi-role dialogues. In this task, random instances of character names
    within a script are masked and the objective is to correctly identify these masked
    names. Exact matching score is used again to evaluate the prediction accuracy.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: InfiniteBench (Zhang 等，[2024b](#bib.bib50)) 被提出用于评估 LLM 在 100K 序列长度上的长上下文能力。由于我们关注的是现实世界的英文任务，我们只取
    InfiniteBench 中的四个相关任务，即长篇书籍摘要（En.Sum）、长篇书籍问答（En.QA）、长篇书籍选择题（En.MC）和长篇书籍对话（En.Dia）。En.Sum
    是一个要求模型生成所给小说的简洁摘要的任务，并使用 ROUGE-L-Sum 指标进行评估（Lin，[2004](#bib.bib24)）。En.QA 由一个管道注释，确保问题需要长距离依赖和推理，而不仅仅是简单的短文检索。聚合推理和过滤推理是两个主要的推理类别。使用
    F1 分数来评估答案的质量。En.MC 的注释管道与 En.QA 相同，不同之处在于提供了四个答案选项，并报告了准确匹配分数。En.Dia 利用来自指定在线数据库³³³[https://imsdb.com](https://imsdb.com)
    的电影和戏剧剧本，其中包含长篇的多角色对话。在此任务中，剧本中的随机角色名称被屏蔽，目标是正确识别这些屏蔽的名称。再次使用准确匹配分数来评估预测的准确性。
- en: 6.2 Medium-Long Context Benchmarks within 32K Tokens
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 32K 标记范围内的中长上下文基准
- en: We use the long context datasets (except NarrativeQA as it is included in our
    training) from Xu et al. ([2024](#bib.bib47)) as our benchmark for medium-long
    datasets within 32K. There are six datasets in total, where QMSum (QM), Qasper
    (QASP), QuALITY (QLTY) are token from SCROLLS and HotpotQA (HQA) MuSiQue (MSQ),
    MultiFieldQA-en (MFQA) are token from LongBench. Following the official metrics,
    we report the geometric mean of ROUGE scores (i.e., ROUGE1/2/L) (Lin, [2004](#bib.bib24))
    for QM, the exact matching (EM) score for QLTY, and F1 scores for the remaining
    four datasets QASP, MSQ, HQA and MFQA.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 Xu 等人（[2024](#bib.bib47)）的长上下文数据集（NarrativeQA 除外，因为它已包含在我们的训练中）作为 32K
    内中长数据集的基准。共有六个数据集，其中 QMSum (QM)、Qasper (QASP)、QuALITY (QLTY) 是来自 SCROLLS 的标记，HotpotQA
    (HQA)、MuSiQue (MSQ)、MultiFieldQA-en (MFQA) 是来自 LongBench 的标记。根据官方指标，我们报告 QM 的
    ROUGE 分数的几何平均值（即 ROUGE1/2/L）（Lin，[2004](#bib.bib24)）、QLTY 的准确匹配（EM）分数，以及其余四个数据集
    QASP、MSQ、HQA 和 MFQA 的 F1 分数。
- en: 6.3 Short Context within 4K Tokens
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 4K 令牌内的短上下文
- en: We use ChatRAG Bench (Liu et al., [2024](#bib.bib28)) as our benchmark for short
    context within 4k. ChatRAG bench consists of 10 datasets and we exclude HDial
    as it is vasincluded in our training. Following the setup of Liu et al. ([2024](#bib.bib28)),
    for Doc2Dial (D2D), QuAC, and QReCC task with long documents, each document is
    divided into segments of roughly 300 words. The top 5 relevant chunks are then
    retrieved as context for each user question. For TopiOCQA and INSCIT, top-20 chunks
    were retrieved to obtain similar context length to the first three datasets. The
    other four datasets are CoQA, DoQA, ConvFinQA (CFQA), and SQA, which cover a wide
    range of domains like finance, children’s stories, literature, mid/high school
    exams, news, Wikipedia and etc. We use F1 score as the metric to evaluate the
    generations and report the average score without HDial as it is a fair zero-shot
    comparisons over different models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ChatRAG Bench（Liu 等人，[2024](#bib.bib28)）作为我们 4k 内短上下文的基准。ChatRAG Bench
    包含 10 个数据集，我们排除 HDial，因为它已包含在我们的训练中。根据 Liu 等人（[2024](#bib.bib28)）的设置，对于 Doc2Dial
    (D2D)、QuAC 和 QReCC 任务，每个文档被划分为大约 300 个词的段落。然后检索前 5 个相关片段作为每个用户问题的上下文。对于 TopiOCQA
    和 INSCIT，检索了前 20 个片段，以获得与前三个数据集类似的上下文长度。其他四个数据集是 CoQA、DoQA、ConvFinQA (CFQA) 和
    SQA，涵盖了如金融、儿童故事、文学、中/高考、新闻、维基百科等各种领域。我们使用 F1 分数作为评估生成的指标，并报告平均分数，不包括 HDial，因为这是对不同模型的公平零样本比较。
- en: 7 Results
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结果
- en: In this section, we present the results and comparisons from extensive benchmark
    evaluations. We begin with the synthetic Needle in a Haystack test, then focus
    on real-world long context understanding and RAG tasks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了广泛基准评估的结果和比较。我们从合成的 Needle in a Haystack 测试开始，然后关注现实世界中的长上下文理解和 RAG
    任务。
- en: '![Refer to caption](img/bd0c9d47c2d65eb626c64a0c6a63c2be.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bd0c9d47c2d65eb626c64a0c6a63c2be.png)'
- en: 'Figure 1: Needle in A Haystack test for $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    up to 128K context window.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: Needle in A Haystack 测试，对于 $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    的上下文窗口最大为 128K。'
- en: 7.1 Needle In A Haystack
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 Needle In A Haystack
- en: 'We evaluate our $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ model on
    the Needle In A Haystack test (Kamradt, [2023](#bib.bib18)). This synthetic task
    is popular for testing the long-context capability of LLMs, and can be considered
    as a threshold level evaluation. Figure [1](#S7.F1 "Figure 1 ‣ 7 Results ‣ ChatQA
    2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities")
    demonstrates the performance of our model with up to 128K tokens, showing that
    our model achieves 100% accuracy. This test confirms our model’s perfect long-context
    retrieval capability.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 Needle In A Haystack 测试（Kamradt，[2023](#bib.bib18)）上评估了我们的 $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    模型。这个合成任务在测试 LLMs 的长上下文能力时非常受欢迎，可以视为阈值级别的评估。图 [1](#S7.F1 "图 1 ‣ 7 结果 ‣ ChatQA
    2: 在长上下文和 RAG 能力上弥合与专有 LLMs 的差距") 显示了我们模型在最多 128K 令牌下的表现，表明我们的模型达到了 100% 的准确率。这项测试确认了我们模型在长上下文检索方面的完美能力。'
- en: '| Model | Avg. | En.Sum | En.QA | En.MC | En.Dia |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | 英语总结 | 英语问答 | 英语多选 | 英语对话 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4-1106 preview | 28.23 | 14.73 | 22.44 | 67.25 | 8.50 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-1106 预览 | 28.23 | 14.73 | 22.44 | 67.25 | 8.50 |'
- en: '| Claude 2 | 33.96 | 14.50 | 11.97 | 62.88 | 46.50 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Claude 2 | 33.96 | 14.50 | 11.97 | 62.88 | 46.50 |'
- en: '| Kimi-Chat | 29.62 | 17.96 | 16.52 | 72.49 | 11.50 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Kimi-Chat | 29.62 | 17.96 | 16.52 | 72.49 | 11.50 |'
- en: '| Yi-34B-200K | < 15.15 | < 5 | 12.17 | 38.43 | < 5 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34B-200K | < 15.15 | < 5 | 12.17 | 38.43 | < 5 |'
- en: '| Llama-3-70B-Instruct-Gradient-262k | 32.57 | 14.27 | 29.52 | 69.00 | 17.50
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct-Gradient-262k | 32.57 | 14.27 | 29.52 | 69.00 | 17.50
    |'
- en: '| GPT-4-Turbo-2024-04-09 | 33.16 | 17.62 | 19.29 | 77.73 | 18.00 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-2024-04-09 | 33.16 | 17.62 | 19.29 | 77.73 | 18.00 |'
- en: '|    w/ RAG | N/A | N/A | 17.69 | 77.29 | N/A |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 不适用 | 不适用 | 17.69 | 77.29 | 不适用 |'
- en: '| Qwen2-72B-Instruct | 34.88 | 14.84 | 21.50 | 81.66 | 21.50 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct | 34.88 | 14.84 | 21.50 | 81.66 | 21.50 |'
- en: '|    w/ RAG | N/A | N/A | 16.48 | 76.86 | N/A |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 不适用 | 不适用 | 16.48 | 76.86 | 不适用 |'
- en: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 34.11 | 16.10 | 44.22
    | 64.63 | 11.50 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 34.11 | 16.10 | 44.22
    | 64.63 | 11.50 |'
- en: '|    w/ RAG | N/A | N/A | 41.00 | 70.74 | N/A |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 不适用 | 不适用 | 41.00 | 70.74 | 不适用 |'
- en: 'Table 2: Evaluation results on InfiniteBench includes real-world long-context
    understanding tasks beyond a 100K context window. For RAG, we use top-5 retrieved
    chunks, each with 1200 tokens from E5-mistral retriever (Wang et al., [2023c](#bib.bib45)).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：InfiniteBench 上的评估结果包括超过 100K 上下文窗口的真实世界长上下文理解任务。对于 RAG，我们使用了从 E5-mistral
    检索器（Wang et al., [2023c](#bib.bib45)）中检索的前 5 个块，每个块包含 1200 个令牌。
- en: 7.2 Long Context Evaluation Beyond 100K Tokens
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 超过 100K 令牌的长上下文评估
- en: 'In this subsection, we evaluate the long context capability beyond 100K tokens
    on the real-world tasks from InfiniteBench (Zhang et al., [2024a](#bib.bib49)).
    Table [2](#S7.T2 "Table 2 ‣ 7.1 Needle In A Haystack ‣ 7 Results ‣ ChatQA 2: Bridging
    the Gap to Proprietary LLMs in Long Context and RAG Capabilities") shows that
    our model (34.11) outperforms many existing state-of-the-art models, such as GPT4-Turbo-2024-04-09
    (33.16), GPT4-1106 preview (28.23), Llama-3-70B-Instruct-Gradient-262k (32.57)
    and Claude 2 (33.96). Additionally, our model is very close to the highest score
    of 34.88 achieved by Qwen2-72B-Instruct, confirming the competitive long-context
    capability of our model.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们评估了在超过 100K 令牌的真实世界任务中的长上下文能力，来自 InfiniteBench（Zhang et al., [2024a](#bib.bib49)）。表
    [2](#S7.T2 "Table 2 ‣ 7.1 Needle In A Haystack ‣ 7 Results ‣ ChatQA 2: Bridging
    the Gap to Proprietary LLMs in Long Context and RAG Capabilities") 显示我们的模型（34.11）超越了许多现有的最先进模型，如
    GPT4-Turbo-2024-04-09（33.16）、GPT4-1106 预览（28.23）、Llama-3-70B-Instruct-Gradient-262k（32.57）和
    Claude 2（33.96）。此外，我们的模型接近 Qwen2-72B-Instruct 取得的最高分 34.88，证明了我们模型在长上下文能力上的竞争力。'
- en: 7.3 Medium-Long Context Evaluation within 32K Tokens
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 32K 令牌内的中长上下文评估
- en: 'In this subsection, we evaluate the medium-long context capability within 32K
    tokens. Table [3](#S7.T3 "Table 3 ‣ 7.3 Medium-Long Context Evaluation within
    32K Tokens ‣ 7 Results ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long
    Context and RAG Capabilities") shows that GPT-4-Turbo-2024-04-09 achieves the
    highest score of 51.93 among all models. Our model scores 47.37, which is higher
    than Llama-3-70B-Instruct-Gradient-262k but is lower than Qwen2-72B-Instruct.
    This difference can be attributed to the extensive 32K pretraining implemented
    by Qwen2-72B-Instruct, while we used a much smaller continued pretraining corpus.
    Additionally, we found that all the RAG solutions perform worse than the long
    context solution, which suggest all these SOTA long context LLMs can really handle
    32K tokens within their context window.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，我们评估了 32K 令牌内的中长上下文能力。表 [3](#S7.T3 "Table 3 ‣ 7.3 Medium-Long Context
    Evaluation within 32K Tokens ‣ 7 Results ‣ ChatQA 2: Bridging the Gap to Proprietary
    LLMs in Long Context and RAG Capabilities") 显示 GPT-4-Turbo-2024-04-09 在所有模型中取得了最高分
    51.93。我们的模型得分 47.37，高于 Llama-3-70B-Instruct-Gradient-262k，但低于 Qwen2-72B-Instruct。这一差异可以归因于
    Qwen2-72B-Instruct 实施的广泛 32K 预训练，而我们使用了规模较小的继续预训练语料库。此外，我们发现所有 RAG 解决方案的表现均不如长上下文解决方案，这表明所有这些
    SOTA 长上下文 LLM 确实能够处理 32K 令牌内的上下文窗口。'
- en: '| Model | Avg. | QM | QASP | QLTY | MSQ | HQA | MFQA |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 平均 | QM | QASP | QLTY | MSQ | HQA | MFQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4-Turbo-2024-04-09 | 51.93 | 16.37 | 38.96 | 88.45 | 44.88 | 70.65 |
    52.26 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-2024-04-09 | 51.93 | 16.37 | 38.96 | 88.45 | 44.88 | 70.65 |
    52.26 |'
- en: '|    w/ RAG | 49.84 | 16.07 | 36.18 | 85.85 | 42.17 | 67.85 | 50.94 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 49.84 | 16.07 | 36.18 | 85.85 | 42.17 | 67.85 | 50.94 |'
- en: '| Qwen2-72B-Instruct | 49.94 | 17.06 | 34.84 | 84.85 | 46.80 | 65.98 | 50.12
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct | 49.94 | 17.06 | 34.84 | 84.85 | 46.80 | 65.98 | 50.12
    |'
- en: '|    w/ RAG | 48.08 | 17.63 | 35.19 | 83.05 | 39.92 | 64.58 | 48.10 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 48.08 | 17.63 | 35.19 | 83.05 | 39.92 | 64.58 | 48.10 |'
- en: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 47.37 | 15.20 | 33.77
    | 81.45 | 37.27 | 62.69 | 53.84 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 47.37 | 15.20 | 33.77
    | 81.45 | 37.27 | 62.69 | 53.84 |'
- en: '|    w/ RAG | 47.08 | 14.71 | 33.25 | 80.45 | 39.45 | 61.04 | 53.58 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 47.08 | 14.71 | 33.25 | 80.45 | 39.45 | 61.04 | 53.58 |'
- en: '| Llama-3-70B-Instruct-Gradient-262k | 40.51 | 20.72 | 30.64 | 74.35 | 20.20
    | 45.82 | 51.33 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70B-Instruct-Gradient-262k | 40.51 | 20.72 | 30.64 | 74.35 | 20.20
    | 45.82 | 51.33 |'
- en: '|    w/ RAG | 40.57 | 20.04 | 30.68 | 72.35 | 22.19 | 46.85 | 51.31 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|    带 RAG | 40.57 | 20.04 | 30.68 | 72.35 | 22.19 | 46.85 | 51.31 |'
- en: 'Table 3: Evaluation results on the medium-long context benchmarks within 32K
    tokens. For RAG, we use top-5 retrieved chunks, each with 1200 tokens from E5-mistral
    retriever (Wang et al., [2023c](#bib.bib45)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 32K tokens 内的中长文本基准测试的评估结果。对于 RAG，我们使用了从 E5-mistral 检索器 (Wang 等人，[2023c](#bib.bib45))
    中检索的 top-5 块，每块含有 1200 tokens。
- en: Models Avg. w/o HDial D2D QuAC QReCC CoQA DoQA CFQA SQA TCQA INSCIT Llama2-Chat-70B
    44.64 36.87 32.47 49.40 80.41 38.97 46.85 37.62 44.31 34.88 Llama3-Instruct-70B
    52.95 37.88 36.96 51.34 76.98 41.24 76.60 69.61 49.72 36.23 Command R+ 51.40 33.51
    34.16 49.77 69.71 40.67 71.21 74.07 53.77 35.76 GPT-3.5-Turbo-0613 50.69 34.83
    37.17 50.46 79.33 41.11 73.15 60.63 44.30 35.27 GPT-4-0613 54.35 34.16 40.29 52.01
    77.42 43.39 81.28 79.21 45.09 36.34 Llama3-ChatQA-1.5-70B 57.14 41.26 38.82 51.40
    78.44 50.76 81.88 83.82 55.63 32.31 GPT-4-Turbo-2024-04-09 54.72 35.35 40.10 51.46
    77.73 41.60 84.16 79.98 48.32 33.75 Llama-3-70B-Instruct-Gradient-262k 45.20 34.30
    24.01 49.60 73.45 25.76 54.70 63.80 46.30 34.89 Qwen2-72B-Instruct 54.06 35.76
    38.48 51.21 85.04 33.89 77.52 77.06 51.64 35.90 $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    54.81 40.76 38.99 47.12 72.44 51.21 78.52 78.15 55.75 30.35
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 平均值 不含 HDial D2D QuAC QReCC CoQA DoQA CFQA SQA TCQA INSCIT Llama2-Chat-70B
    44.64 36.87 32.47 49.40 80.41 38.97 46.85 37.62 44.31 34.88 Llama3-Instruct-70B
    52.95 37.88 36.96 51.34 76.98 41.24 76.60 69.61 49.72 36.23 Command R+ 51.40 33.51
    34.16 49.77 69.71 40.67 71.21 74.07 53.77 35.76 GPT-3.5-Turbo-0613 50.69 34.83
    37.17 50.46 79.33 41.11 73.15 60.63 44.30 35.27 GPT-4-0613 54.35 34.16 40.29 52.01
    77.42 43.39 81.28 79.21 45.09 36.34 Llama3-ChatQA-1.5-70B 57.14 41.26 38.82 51.40
    78.44 50.76 81.88 83.82 55.63 32.31 GPT-4-Turbo-2024-04-09 54.72 35.35 40.10 51.46
    77.73 41.60 84.16 79.98 48.32 33.75 Llama-3-70B-Instruct-Gradient-262k 45.20 34.30
    24.01 49.60 73.45 25.76 54.70 63.80 46.30 34.89 Qwen2-72B-Instruct 54.06 35.76
    38.48 51.21 85.04 33.89 77.52 77.06 51.64 35.90 $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    54.81 40.76 38.99 47.12 72.44 51.21 78.52 78.15 55.75 30.35
- en: 'Table 4: Evaluation results on ChatRAG Bench with 9 datasets. Following (Liu
    et al., [2024](#bib.bib28)), we exclude HDial as it is included in the instruction
    tuning datasets. The maximum context lengths are 4K tokens.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 ChatRAG Bench 上的评估结果，包括 9 个数据集。根据 (Liu 等人，[2024](#bib.bib28))，我们排除了 HDial，因为它已包含在指令调优数据集中。最大上下文长度为
    4K tokens。
- en: '7.4 ChatRAG Bench: Short Context Evaluation within 4K Tokens'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 ChatRAG Bench：4K Tokens 内的短文本评估
- en: In this subsection, we evaluate the models on the short context tasks within
    4K tokens from ChatRAG Bench (Liu et al., [2024](#bib.bib28)). Our model achieves
    the average score of 54.81\. Even though it is worse than Llama3-ChatQA-1.5-70B,
    it still outperforms GPT-4-Turbo-2024-04-09 and Qwen2-72B-Instruct. This confirms
    that extending short context models to long context is not a free lunch. How to
    effectively extend the context window to even larger scale (e.g., million tokens
    in Gemini 1.5 Pro (Gemini-Team, [2024](#bib.bib9))) without any degradation on
    regular short context tasks is an exciting research direction.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们评估了模型在 ChatRAG Bench (Liu 等人，[2024](#bib.bib28)) 上的短文本任务表现。我们的模型平均得分为
    54.81。尽管这低于 Llama3-ChatQA-1.5-70B，但仍然优于 GPT-4-Turbo-2024-04-09 和 Qwen2-72B-Instruct。这证实了将短文本模型扩展到长文本不是一件容易的事。如何在不影响常规短文本任务的情况下，将上下文窗口有效扩展到更大的规模（例如，Gemini
    1.5 Pro (Gemini-Team，[2024](#bib.bib9)) 中的百万 tokens）是一个令人兴奋的研究方向。
- en: '| Top-*k* | 5 | 10 | 20 | 30 | Long Context |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Top-*k* | 5 | 10 | 20 | 30 | 长文本 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 47.08 | 47.13 | 47.18
    | 47.19 | 47.37 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 47.08 | 47.13 | 47.18
    | 47.19 | 47.37 |'
- en: 'Table 5: We compare our $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    using RAG vs. direct long context evaluation on benchmarks with maximum 32K tokens
    inputs. RAG can be slightly worse than direct long context solution even when
    we increase top-*k* chunks to 30.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：我们将我们的 $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ 使用 RAG 与直接长文本评估进行了比较，基准测试中输入最大为
    32K tokens。即使我们将 top-*k* 块增加到 30，RAG 的表现也可能略逊于直接长文本解决方案。
- en: '|  | RAG (top-*k*) | Long Context |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | RAG (top-*k*) | 长文本 |'
- en: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 56.36 (5) | 54.43 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ | 56.36 (5) | 54.43 |'
- en: '| Qwen2-72B-Instruct | 52.95 (20) | 51.58 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Instruct | 52.95 (20) | 51.58 |'
- en: 'Table 6: We compare RAG vs. long context evaluation using our $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$
    on tasks beyond 100k. Here we use average accuracy of En.QA and En.MC tasks that
    can apply RAG. The RAG-based result is still better than long context evaluation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：我们在超出 100k 的任务上比较了 RAG 与长文本评估，使用了我们的 $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$。这里我们使用了可以应用
    RAG 的 En.QA 和 En.MC 任务的平均准确率。基于 RAG 的结果仍然优于长文本评估。
- en: 7.5 RAG vs. Long Context
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 RAG 与长文本
- en: 'In Table [5](#S7.T5 "Table 5 ‣ 7.4 ChatRAG Bench: Short Context Evaluation
    within 4K Tokens ‣ 7 Results ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs
    in Long Context and RAG Capabilities") and Table [6](#S7.T6 "Table 6 ‣ 7.4 ChatRAG
    Bench: Short Context Evaluation within 4K Tokens ‣ 7 Results ‣ ChatQA 2: Bridging
    the Gap to Proprietary LLMs in Long Context and RAG Capabilities"), we compare
    RAG vs. long context solutions under different context lengths. For sequence length
    beyond 100k, we only report the average score of En.QA and En.MC as the RAG setting
    is not directly applicable for En.Sum and En.Dia. We found that for downstream
    tasks within 32k sequence length, our long context solution is better than RAG.
    This means using RAG can save the cost but the accuracy will drop a bit. On the
    other hand, we found that for context lengths beyond 100K, RAG (using top-5 for
    our $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$, and top-20 for Qwen2-72B-Instruct)
    outperforms the full long-context solution. This indicates that even state-of-the-art
    long-context LLMs may struggle to effectively understand and reason over 128K
    tokens. In such scenarios, RAG is recommended for better accuracy and lower inference
    cost, provided it is applicable to the downstream tasks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[5](#S7.T5 "Table 5 ‣ 7.4 ChatRAG Bench: Short Context Evaluation within
    4K Tokens ‣ 7 Results ‣ ChatQA 2: Bridging the Gap to Proprietary LLMs in Long
    Context and RAG Capabilities")和表[6](#S7.T6 "Table 6 ‣ 7.4 ChatRAG Bench: Short
    Context Evaluation within 4K Tokens ‣ 7 Results ‣ ChatQA 2: Bridging the Gap to
    Proprietary LLMs in Long Context and RAG Capabilities")中，我们比较了RAG与长上下文解决方案在不同上下文长度下的表现。对于超过100k的序列长度，我们仅报告En.QA和En.MC的平均得分，因为RAG设置并不直接适用于En.Sum和En.Dia。我们发现，对于32k序列长度以内的下游任务，我们的长上下文解决方案优于RAG。这意味着使用RAG可以节省成本，但准确性会略有下降。另一方面，我们发现对于超过100K的上下文长度，RAG（对我们的$\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$使用top-5，对Qwen2-72B-Instruct使用top-20）优于完整的长上下文解决方案。这表明，即使是最先进的长上下文LLM也可能难以有效理解和推理超过128K的标记。在这种情况下，建议使用RAG以获得更好的准确性和较低的推理成本，前提是它适用于下游任务。'
- en: 8 Conclusion
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We introduce $\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$ can achieve
    GPT-4-Turbo-2024-0409 level accuracy on these benchmarks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了$\mathtt{Llama3\text{-}ChatQA\text{-}2\text{-}70B}$，它可以在这些基准上达到GPT-4-Turbo-2024-0409级别的准确性。
- en: References
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '01.AI et al. (2024) 01.AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,
    Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open
    foundation models by 01\. ai. *arXiv preprint arXiv:2403.04652*, 2024.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '01.AI 等（2024）**01.AI**、**亚历克斯·杨**、**贝·陈**、**超·李**、**程根·黄**、**葛·张**、**关伟·张**、**恒·李**、**江成·朱**、**建群·陈**、**静·张**等人撰写的《Yi:
    Open foundation models by 01.ai》。*arXiv 预印本 arXiv:2403.04652*，2024。'
- en: Alibaba-QWen (2024) Alibaba-QWen. Qwen2 technical report. 2024. URL [https://qwenlm.github.io/blog/qwen2/](https://qwenlm.github.io/blog/qwen2/).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alibaba-QWen（2024）Alibaba-QWen。Qwen2技术报告。2024年。网址 [https://qwenlm.github.io/blog/qwen2/](https://qwenlm.github.io/blog/qwen2/)。
- en: Anthropic (2023) Anthropic. Introducing 100k context windows. [https://www.anthropic.com/index/100k-context-windows](https://www.anthropic.com/index/100k-context-windows),
    2023.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2023）Anthropic。介绍100k上下文窗口。 [https://www.anthropic.com/index/100k-context-windows](https://www.anthropic.com/index/100k-context-windows)，2023。
- en: Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor
    Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
    Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving
    from trillions of tokens. In *ICML*, 2022.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博尔戈德等（2022）**塞巴斯蒂安·博尔戈德**、**亚瑟·门施**、**乔丹·霍夫曼**、**特雷弗·蔡**、**伊丽莎·拉瑟福德**、**凯蒂·米利肯**、**乔治·Bm·范·登·德雷斯赫**、**让-巴普蒂斯特·莱皮奥**、**博格丹·达莫克**、**艾登·克拉克**等人撰写的《通过从万亿个标记中检索来改进语言模型》。在*ICML*，2022。
- en: Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *arXiv preprint arXiv:2306.15595*, 2023a.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023a）是由**寿远·陈**、**舍曼·王**、**梁剑·陈**和**远东·田**等人撰写的《通过位置插值扩展大型语言模型的上下文窗口》。*arXiv
    预印本 arXiv:2306.15595*，2023a。
- en: 'Chen et al. (2023b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context
    large language models. *arXiv preprint arXiv:2309.12307*, 2023b.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等（2023b）是由**余康·陈**、**盛举·钱**、**浩天·唐**、**欣·赖**、**志坚·刘**、**宋·韩**和**佳雅·贾**等人撰写的《Longlora:
    Efficient fine-tuning of long-context large language models》。*arXiv 预印本 arXiv:2309.12307*，2023b。'
- en: Chen et al. (2024) Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao,
    Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are
    we to gpt-4v? closing the gap to commercial multimodal models with open-source
    suites. *arXiv preprint arXiv:2404.16821*, 2024.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2024) Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao,
    Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, 等. 我们距离gpt-4v有多远？ 使用开源工具套件缩小与商业多模态模型的差距。*arXiv
    preprint arXiv:2404.16821*，2024年。
- en: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context.
    *arXiv preprint arXiv:2402.10171*, 2024.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, 和 Hao Peng. 数据工程用于将语言模型扩展到128k上下文。*arXiv preprint arXiv:2402.10171*，2024年。
- en: 'Gemini-Team (2024) Gemini-Team. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2310.07713*, 2024.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gemini-Team (2024) Gemini-Team. Gemini 1.5: 解锁跨越数百万标记的多模态理解。*arXiv preprint
    arXiv:2310.07713*，2024年。'
- en: GradientAI (2024) GradientAI. Scaling rotational embeddings for long-context
    language models. [https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models](https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models),
    2024.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GradientAI (2024) GradientAI. 扩展旋转嵌入以适应长上下文语言模型。 [https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models](https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models)，2024年。
- en: 'Günther et al. (2023) Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine
    Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas,
    Saba Sturua, Bo Wang, et al. Jina embeddings 2: 8192-token general-purpose text
    embeddings for long documents. *arXiv preprint arXiv:2310.19923*, 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Günther et al. (2023) Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine
    Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas,
    Saba Sturua, Bo Wang, 等. Jina embeddings 2: 8192-token 通用文本嵌入，用于长文档。*arXiv preprint
    arXiv:2310.19923*，2023年。'
- en: 'Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and
    Mingwei Chang. REALM: Retrieval augmented language model pre-training. In *ICML*,
    2020.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei
    Chang. REALM: 检索增强语言模型预训练。在*ICML*，2020年。'
- en: 'Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What’s the real context size
    of your long-context language models? *arXiv preprint arXiv:2404.06654*, 2024.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsieh et al. (2024) Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya,
    Dima Rekesh, Fei Jia, 和 Boris Ginsburg. Ruler: 你长上下文语言模型的真实上下文大小是多少？*arXiv preprint
    arXiv:2404.06654*，2024年。'
- en: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, and Bryan Catanzaro. Raven: In-context learning with retrieval augmented
    encoder-decoder language models. *arXiv preprint arXiv:2308.07922*, 2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, 和 Bryan Catanzaro. Raven: 具有检索增强的编码器-解码器语言模型的上下文学习。*arXiv preprint arXiv:2308.07922*，2023年。'
- en: Izacard & Grave (2021) Gautier Izacard and Édouard Grave. Leveraging passage
    retrieval with generative models for open domain question answering. In *EACL*,
    2021.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard & Grave (2021) Gautier Izacard 和 Édouard Grave. 利用段落检索与生成模型进行开放领域问答。在*EACL*，2021年。
- en: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. Few-shot learning with retrieval augmented language models.
    *arXiv preprint arXiv:2208.03299*, 2022.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    和 Edouard Grave. 具有检索增强语言模型的少样本学习。*arXiv preprint arXiv:2208.03299*，2022年。
- en: Kaiokendev (2023) Kaiokendev. Things I’m learning while training SuperHOT. [https://kaiokendev.github.io/til#extending-context-to-8k](https://kaiokendev.github.io/til#extending-context-to-8k),
    2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiokendev (2023) Kaiokendev. 在训练SuperHOT时学到的东西。 [https://kaiokendev.github.io/til#extending-context-to-8k](https://kaiokendev.github.io/til#extending-context-to-8k)，2023年。
- en: Kamradt (2023) Gregory Kamradt. Needle in a haystack - pressure testing llms.
    [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack),
    2023.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt (2023) Gregory Kamradt. 针在干草堆中 - 压力测试llms。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)，2023年。
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval
    for open-domain question answering. In *EMNLP*, 2020.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, 和 Wen-tau Yih. 用于开放领域问答的密集段落检索。在*EMNLP*，2020年。
- en: 'Khandelwal et al. (2019) Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
    Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor
    language models. *arXiv preprint arXiv:1911.00172*, 2019.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khandelwal 等 (2019) Urvashi Khandelwal、Omer Levy、Dan Jurafsky、Luke Zettlemoyer
    和 Mike Lewis。通过记忆进行泛化：最近邻语言模型。*arXiv 预印本 arXiv:1911.00172*，2019。
- en: 'Lee et al. (2024) Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad
    Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training
    llms as generalist embedding models. *arXiv preprint arXiv:2405.17428*, 2024.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2024) Chankyu Lee、Rajarshi Roy、Mengyao Xu、Jonathan Raiman、Mohammad Shoeybi、Bryan
    Catanzaro 和 Wei Ping。Nv-embed：用于训练大型语言模型作为通用嵌入模型的改进技术。*arXiv 预印本 arXiv:2405.17428*，2024。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp
    tasks. *NeurIPS*, 2020.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等 (2020) Patrick Lewis、Ethan Perez、Aleksandra Piktus、Fabio Petroni、Vladimir
    Karpukhin、Naman Goyal、Heinrich Küttler、Mike Lewis、Wen-tau Yih、Tim Rocktäschel
    等。用于知识密集型 NLP 任务的检索增强生成。*NeurIPS*，2020。
- en: Li et al. (2023) Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie,
    and Meishan Zhang. Towards general text embeddings with multi-stage contrastive
    learning. *arXiv preprint arXiv:2308.03281*, 2023.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Zehan Li、Xin Zhang、Yanzhao Zhang、Dingkun Long、Pengjun Xie 和 Meishan
    Zhang。致力于通过多阶段对比学习进行通用文本嵌入。*arXiv 预印本 arXiv:2308.03281*，2023。
- en: 'Lin (2004) Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries.
    In *Text Summarization Branches Out*, pp.  74–81, Barcelona, Spain, July 2004\.
    Association for Computational Linguistics. URL [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2004) Chin-Yew Lin。ROUGE：自动评估摘要的工具包。在 *文本摘要的扩展*，第 74–81 页，西班牙巴塞罗那，2004
    年 7 月。计算语言学协会。网址 [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)。
- en: 'Lin et al. (2023) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy
    Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. How to train your dragon: Diverse
    augmentation towards generalizable dense retrieval. *arXiv preprint arXiv:2302.07452*,
    2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2023) Sheng-Chieh Lin、Akari Asai、Minghan Li、Barlas Oguz、Jimmy Lin、Yashar
    Mehdad、Wen-tau Yih 和 Xilun Chen。如何训练你的龙：多样化增强以实现通用稠密检索。*arXiv 预印本 arXiv:2302.07452*，2023。
- en: 'Liu et al. (2023a) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*, 2023a.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023a) Nelson F Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio
    Petroni 和 Percy Liang。在中间迷失：语言模型如何使用长上下文。*arXiv 预印本 arXiv:2307.03172*，2023a。
- en: Liu et al. (2023b) Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu,
    and Dahua Lin. Scaling laws of rope-based extrapolation. *arXiv preprint arXiv:2310.05209*,
    2023b.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023b) Xiaoran Liu、Hang Yan、Shuo Zhang、Chenxin An、Xipeng Qiu 和 Dahua
    Lin。基于绳索的外推的尺度法则。*arXiv 预印本 arXiv:2310.05209*，2023b。
- en: 'Liu et al. (2024) Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee,
    Mohammad Shoeybi, and Bryan Catanzaro. ChatQA: Surpassing GPT-4 on conversational
    QA and RAG. *arXiv preprint arXiv:2401.10225*, 2024.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2024) Zihan Liu、Wei Ping、Rajarshi Roy、Peng Xu、Chankyu Lee、Mohammad Shoeybi
    和 Bryan Catanzaro。ChatQA：在对话式问答和 RAG 上超越 GPT-4。*arXiv 预印本 arXiv:2401.10225*，2024。
- en: Meta-AI (2024) Meta-AI. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta-AI (2024) Meta-AI。Llama 3 模型卡。2024。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: Mistral (2024) Mistral. Mixtral 8x22b. 2024. URL [https://mistral.ai/news/mixtral-8x22b/](https://mistral.ai/news/mixtral-8x22b/).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mistral (2024) Mistral。Mixtral 8x22b。2024。网址 [https://mistral.ai/news/mixtral-8x22b/](https://mistral.ai/news/mixtral-8x22b/)。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. WebGPT: Browser-assisted question-answering with human
    feedback. *arXiv preprint arXiv:2112.09332*, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano 等 (2021) Reiichiro Nakano、Jacob Hilton、Suchir Balaji、Jeff Wu、Long Ouyang、Christina
    Kim、Christopher Hesse、Shantanu Jain、Vineet Kosaraju、William Saunders 等。WebGPT：带有人类反馈的浏览器辅助问答。*arXiv
    预印本 arXiv:2112.09332*，2021。
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Hiroaki Hayashi, Tian Xie, Congying Xia,
    Bo Pang, Congying Xia, and et al. Long sequence modeling with XGen: A 7b LLM trained
    on 8k input sequence length. [https://blog.salesforceairesearch.com/xgen/](https://blog.salesforceairesearch.com/xgen/),
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp 等 (2023) Erik Nijkamp、Hiroaki Hayashi、Tian Xie、Congying Xia、Bo Pang、Congying
    Xia 等。使用 XGen 进行长序列建模：一个训练有 8k 输入序列长度的 7b LLM。 [https://blog.salesforceairesearch.com/xgen/](https://blog.salesforceairesearch.com/xgen/)，2023。
- en: 'Nussbaum et al. (2024) Zach Nussbaum, John X Morris, Brandon Duderstadt, and
    Andriy Mulyar. Nomic embed: Training a reproducible long context text embedder.
    *arXiv preprint arXiv:2402.01613*, 2024.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nussbaum 等人 (2024) Zach Nussbaum、John X Morris、Brandon Duderstadt 和 Andriy
    Mulyar。Nomic embed: 训练可复现的长文本嵌入器。*arXiv 预印本 arXiv:2402.01613*，2024年。'
- en: Nvidia et al. (2024) Nvidia, Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H
    Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon
    Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. *arXiv preprint
    arXiv:2406.11704*, 2024.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia 等人 (2024) Nvidia、Bo Adler、Niket Agarwal、Ashwath Aithal、Dong H Anh、Pallab
    Bhattacharya、Annika Brundyn、Jared Casper、Bryan Catanzaro、Sharon Clay、Jonathan
    Cohen 等人。Nemotron-4 340b 技术报告。*arXiv 预印本 arXiv:2406.11704*，2024年。
- en: OpenAI (2023) OpenAI. GPT-4 turbo with 128k context. [https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday),
    2023.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4 turbo 与 128k 上下文。[https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday)，2023年。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models. *arXiv preprint
    arXiv:2309.00071*, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人 (2023) Bowen Peng、Jeffrey Quesnelle、Honglu Fan 和 Enrico Shippole。Yarn:
    高效的大型语言模型上下文窗口扩展。*arXiv 预印本 arXiv:2309.00071*，2023年。'
- en: 'Soboleva et al. (2023) Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R
    Steeves, Joel Hestness, and Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated
    version of RedPajama. [https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama),
    2023. URL [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Soboleva 等人 (2023) Daria Soboleva、Faisal Al-Khateeb、Robert Myers、Jacob R Steeves、Joel
    Hestness 和 Nolan Dey。SlimPajama: 一个 627B 令牌的清理和去重版本的 RedPajama。[https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)，2023年。网址
    [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B)。'
- en: 'Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063, 2024.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su 等人 (2024) Jianlin Su、Murtadha Ahmed、Yu Lu、Shengfeng Pan、Wen Bo 和 Yunfeng
    Liu。Roformer: 具有旋转位置嵌入的增强型变换器。*Neurocomputing*，568:127063，2024年。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023a) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等人。Llama: 开放而高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023b) Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad
    Almahairi、Yasmine Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti
    Bhosale 等人。Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年。'
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive
    training for context scaling. *arXiv preprint arXiv:2307.03170*, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tworkowski 等人 (2023) Szymon Tworkowski、Konrad Staniszewski、Mikołaj Pacek、Yuhuai
    Wu、Henryk Michalewski 和 Piotr Miłoś。聚焦变换器: 用于上下文缩放的对比训练。*arXiv 预印本 arXiv:2307.03170*，2023年。'
- en: 'Wang et al. (2023a) Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li,
    Mohammad Shoeybi, and Bryan Catanzaro. InstructRetro: Instruction tuning post
    retrieval-augmented pretraining. *arXiv preprint arXiv:2310.07713*, 2023a.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 (2023a) Boxin Wang、Wei Ping、Lawrence McAfee、Peng Xu、Bo Li、Mohammad
    Shoeybi 和 Bryan Catanzaro。InstructRetro: 指令调优后的检索增强预训练。*arXiv 预印本 arXiv:2310.07713*，2023年。'
- en: Wang et al. (2023b) Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu,
    Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall
    we pretrain autoregressive language models with retrieval? a comprehensive study.
    *arXiv preprint arXiv:2304.06762*, 2023b.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 (2023b) Boxin Wang、Wei Ping、Peng Xu、Lawrence McAfee、Zihan Liu、Mohammad
    Shoeybi、Yi Dong、Oleksii Kuchaiev、Bo Li、Chaowei Xiao 等人。我们是否应该通过检索预训练自回归语言模型？一项全面研究。*arXiv
    预印本 arXiv:2304.06762*，2023年。
- en: Wang et al. (2022) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun
    Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised
    contrastive pre-training. *arXiv preprint arXiv:2212.03533*, 2022.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2022）梁王、南洋、黄晓龙、焦宾兴、杨林俊、蒋大新、兰根·马朱姆德和魏福如。《通过弱监督对比预训练的文本嵌入》。*arXiv 预印本 arXiv:2212.03533*，2022年。
- en: Wang et al. (2023c) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan
    Majumder, and Furu Wei. Improving text embeddings with large language models.
    *arXiv preprint arXiv:2401.00368*, 2023c.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023c）梁王、南洋、黄晓龙、杨林俊、兰根·马朱姆德和魏福如。《使用大语言模型改进文本嵌入》。*arXiv 预印本 arXiv:2401.00368*，2023c。
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, et al. Effective long-context scaling of foundation models. *arXiv preprint
    arXiv:2309.16039*, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熊等（2023）熊文瀚、刘静宇、伊戈尔·莫利博格、张赫佳、普拉吉瓦尔·巴尔戈瓦、侯锐、路易斯·马丁、拉希·鲁恩塔、卡尔提克·阿比纳夫·桑卡拉拉曼、巴尔拉斯·奥古兹等。《基础模型的有效长上下文扩展》。*arXiv
    预印本 arXiv:2309.16039*，2023年。
- en: Xu et al. (2024) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. Retrieval meets long context large language models. In *ICLR*, 2024.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 徐等（2024）彭旭、魏平、吴显超、劳伦斯·麦卡菲、陈竹、刘子涵、桑迪普·苏布拉马尼安、叶芙琳娜·巴赫图里纳、穆罕默德·肖耶比和布赖恩·卡坦扎罗。《检索与长上下文大语言模型的结合》。发表于*ICLR*，2024年。
- en: 'Yu et al. (2024) Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao
    Zhang, Mohammad Shoeybi, and Bryan Catanzaro. Rankrag: Unifying context ranking
    with retrieval-augmented generation in LLMs. *arXiv e-prints*, pp.  arXiv–2407,
    2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 于等（2024）于岳、魏平、刘子涵、王博新、游佳轩、张超、穆罕默德·肖耶比和布赖恩·卡坦扎罗。《Rankrag：在大语言模型中将上下文排序与检索增强生成统一起来》。*arXiv
    e-prints*，pp. arXiv–2407，2024年。
- en: 'Zhang et al. (2024a) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. $\infty$bench: Extending long context evaluation beyond 100k tokens, 2024a.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2024a）张新荣、陈盈发、胡胜丁、许子航、陈俊豪、毛开浩、徐涵、郑棱泰、王硕、刘志远和孙茂松。$\infty$bench：将长上下文评估扩展至100k以上的标记，2024a。
- en: 'Zhang et al. (2024b) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. $\infty$bench: Extending long context evaluation beyond 100k tokens, 2024b.
    URL [https://arxiv.org/abs/2402.13718](https://arxiv.org/abs/2402.13718).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2024b）张新荣、陈盈发、胡胜丁、许子航、陈俊豪、毛开浩、徐涵、郑棱泰、王硕、刘志远和孙茂松。$\infty$bench：将长上下文评估扩展至100k以上的标记，2024b。网址
    [https://arxiv.org/abs/2402.13718](https://arxiv.org/abs/2402.13718)。
- en: 'Zhu et al. (2024) Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang,
    Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking
    the barrier of closed-source models in code intelligence. *arXiv preprint arXiv:2406.11931*,
    2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等（2024）朱启浩、郭达雅、邵志洪、杨德健、王佩怡、徐润新、吴颖、李玉昆、高华佐、马世荣等。《Deepseek-coder-v2：突破代码智能闭源模型的障碍》。*arXiv
    预印本 arXiv:2406.11931*，2024年。
