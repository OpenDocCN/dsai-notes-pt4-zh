- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:56:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:56:08'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'MInference 1.0: 通过动态稀疏注意力加速长上下文 LLM 的预填充'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.02490](https://ar5iv.labs.arxiv.org/html/2407.02490)
- en: Huiqiang Jiang¹¹1Equal contribution. ^◆Work during internship at Microsoft.,
    Yucheng Li^◆¹¹1Equal contribution. ^◆Work during internship at Microsoft., Chengruidong
    Zhang¹¹1Equal contribution. ^◆Work during internship at Microsoft., Qianhui Wu,
    Xufang Luo,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Huiqiang Jiang¹¹1平等贡献。 ^◆在微软实习期间的工作。 Yucheng Li^◆¹¹1平等贡献。 ^◆在微软实习期间的工作。 Chengruidong
    Zhang¹¹1平等贡献。 ^◆在微软实习期间的工作。 Qianhui Wu, Xufang Luo,
- en: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang,
    Lili Qiu
- en: Microsoft Corporation, ^◆University of Surrey {hjiang,chengzhang,yuqyang}@microsoft.com,yucheng.li@surrey.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微软公司，^◆萨里大学 {hjiang,chengzhang,yuqyang}@microsoft.com, yucheng.li@surrey.ac.uk
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The computational challenges of Large Language Model (LLM) inference remain
    a significant barrier to their widespread deployment, especially as prompt lengths
    continue to increase. Due to the quadratic complexity of the attention computation,
    it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the
    pre-filling stage) on a single A100 GPU. Existing methods for speeding up pre-filling
    often fail to maintain acceptable accuracy or efficiency when applied to long-context
    LLMs. To address this gap, we introduce MInference (Million-tokens Inference),
    a sparse calculation method designed to accelerate pre-filling of long-sequence
    processing. Specifically, we identify three unique patterns in long-context attention
    matrices—the A-shape, Vertical-Slash, and Block-Sparse—that can be leveraged for
    efficient sparse computation on GPUs. We determine the optimal pattern for each
    attention head offline and dynamically build sparse indices based on the assigned
    pattern during inference. With the pattern and sparse indices, we perform efficient
    sparse attention calculations via our optimized GPU kernels to significantly reduce
    the latency in the pre-filling stage of long-context LLMs. Our proposed technique
    can be directly applied to existing LLMs without any modifications to the pre-training
    setup or additional fine-tuning. By evaluating on a wide range of downstream tasks,
    including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including
    LLaMA-3-1M, GLM-4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that
    MInference effectively reduces inference latency by up to $10\times$ for pre-filling
    on an A100, while maintaining accuracy. Our code is available at [https://aka.ms/MInference](https://aka.ms/MInference).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）推理的计算挑战仍然是其广泛部署的一个重要障碍，尤其是当提示长度持续增加时。由于注意力计算的二次复杂度，一个 8B 的 LLM 在单个
    A100 GPU 上处理 1M 令牌的提示（即预填充阶段）需要 30 分钟。现有的加速预填充的方法在应用于长上下文 LLM 时，往往无法保持可接受的准确性或效率。为了解决这一问题，我们提出了
    MInference（百万令牌推理），这是一种稀疏计算方法，旨在加速长序列处理的预填充。具体而言，我们识别了长上下文注意力矩阵中的三种独特模式——A形、竖直斜杠和块稀疏——这些模式可以用于在
    GPU 上进行高效的稀疏计算。我们离线确定每个注意力头的最佳模式，并在推理过程中根据分配的模式动态构建稀疏索引。通过模式和稀疏索引，我们通过优化的 GPU
    内核执行高效的稀疏注意力计算，从而显著减少长上下文 LLM 预填充阶段的延迟。我们提出的技术可以直接应用于现有的 LLM，而无需对预训练设置进行任何修改或额外的微调。通过在包括
    InfiniteBench、RULER、PG-19 和大海捞针在内的广泛下游任务以及 LLaMA-3-1M、GLM-4-1M、Yi-200K、Phi-3-128K
    和 Qwen2-128K 等模型上进行评估，我们展示了 MInference 能够有效地将 A100 上的预填充推理延迟减少高达 $10\times$，同时保持准确性。我们的代码可以在
    [https://aka.ms/MInference](https://aka.ms/MInference) 获得。
- en: \doparttoc\faketableofcontents![Refer to caption](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents![请参见说明](img/21bf8d87b05d7de70866fc4acd55b8e5.png)
- en: (a) Needle In A Haystack
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 大海捞针
- en: '![Refer to caption](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/c7ce5334ff26f4c3dc793c130afccb72.png)'
- en: (b) Latency Speedup
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 延迟加速
- en: 'Figure 1: Attention weights, especially in long-context LLMs, exhibit up to
    96.8% sparsity in contexts of 128K. We propose MInference, leveraging dynamic
    sparse attention to accelerate the pre-filling stage of long-context LLM inference.
    It achieves up to 10x speedup for 1M contexts on a single A100, as shown in (b),
    and matches or surpasses baselines, as demonstrated by Needle In A Haystack [[35](#bib.bib35)]
    in (a) on LLaMA-3-8B-1M [[24](#bib.bib24)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：注意力权重，尤其是在长上下文 LLMs 中，在 128K 上下文中表现出高达 96.8% 的稀疏性。我们提出了 MInference，利用动态稀疏注意力来加速长上下文
    LLM 推理的预填充阶段。正如图 (b) 所示，在单个 A100 上对于 1M 上下文实现了高达 10 倍的加速，并且与基准相匹配或超越，正如图 (a) 中
    Needle In A Haystack [[35](#bib.bib35)] 在 LLaMA-3-8B-1M [[24](#bib.bib24)] 上所示。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs) have entered the era of long-context processing,
    with some of them supporting context windows ranging from 128K to 10M tokens [[24](#bib.bib24),
    [67](#bib.bib67), [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)].
    These extended context windows enable LLMs to unlock a multitude of complex real-world
    applications, such as repository-level code understanding [[7](#bib.bib7), [34](#bib.bib34),
    [58](#bib.bib58)], long-document question-answering [[9](#bib.bib9), [51](#bib.bib51)],
    extreme-label in-context learning [[51](#bib.bib51)], and long-horizon agent tasks [[79](#bib.bib79)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已进入长上下文处理时代，其中一些支持 128K 到 10M 标记的上下文窗口 [[24](#bib.bib24), [67](#bib.bib67),
    [49](#bib.bib49), [84](#bib.bib84), [2](#bib.bib2), [12](#bib.bib12)]。这些扩展的上下文窗口使
    LLMs 能够解锁众多复杂的实际应用，如代码库级别的理解 [[7](#bib.bib7), [34](#bib.bib34), [58](#bib.bib58)]、长文档问答
    [[9](#bib.bib9), [51](#bib.bib51)]、极端标签的上下文学习 [[51](#bib.bib51)] 和长时间跨度的代理任务 [[79](#bib.bib79)]。
- en: 'However, due to the quadratic complexity of attention, it can take several
    minutes for the model to process the input prompt (i.e., the pre-filling stage)
    and then start to produce the first token, which leads to unacceptable Time To
    First Token experience, thus greatly hinders the wide application of long-context
    LLMs. As shown in Fig. [2a](#S2.F2.sf1 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), when serving LLaMA-3-8B on a single A100
    machine, the model would keep users waiting for 6 minutes to finish the pre-filling
    stage given a prompt of 300K tokens, and this number increases to 30 minutes for
    a prompt of 1M tokens. The overhead of self-attention computation exceeds 90%
    of the total pre-filling latency, which makes it the major bottleneck in long-context
    processing of LLMs. Previous research has shown that the attention matrices are
    highly sparse [[47](#bib.bib47), [17](#bib.bib17)], which has led to the development
    of fixed sparse attention methods such as Longformer [[6](#bib.bib6)] and BigBird [[87](#bib.bib87)].
    However, prior studies have also noted that attention distributions vary significantly
    across different inputs [[39](#bib.bib39), [47](#bib.bib47)]. This dynamic nature
    prevents prior sparse methods from being used directly on long-context LLMs without
    expensive training or fine-tuning. But if the dynamic sparse attention patterns
    could be efficiently predicted online, the pre-filling latency of long-context
    LLMs could be significantly reduced by calculating only the most important part
    of the attention weights.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于注意力机制的二次复杂度，模型处理输入提示（即预填充阶段）可能需要几分钟，然后才开始生成第一个标记，这导致了不可接受的首次标记时间体验，从而大大阻碍了长上下文
    LLMs 的广泛应用。如图 [2a](#S2.F2.sf1 "在图 2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文
    LLMs 的预填充")所示，当在单个 A100 机器上服务 LLaMA-3-8B 时，模型在处理 300K 标记的提示时需要让用户等待 6 分钟完成预填充阶段，对于
    1M 标记的提示，这个时间增加到 30 分钟。自注意力计算的开销超过了总预填充延迟的 90%，这使得它成为 LLMs 长上下文处理的主要瓶颈。以往研究表明，注意力矩阵高度稀疏
    [[47](#bib.bib47), [17](#bib.bib17)]，这导致了固定稀疏注意力方法的开发，如 Longformer [[6](#bib.bib6)]
    和 BigBird [[87](#bib.bib87)]。然而，先前的研究也指出，注意力分布在不同输入间差异显著 [[39](#bib.bib39), [47](#bib.bib47)]。这种动态特性阻止了之前的稀疏方法在长上下文
    LLMs 上的直接应用而无需昂贵的训练或微调。但如果能够有效地在线预测动态稀疏注意力模式，则通过仅计算最重要的注意力权重部分，可以显著减少长上下文 LLMs
    的预填充延迟。
- en: 'Building upon this idea, we present MInference, a technique that reduces 95%
    of FLOPs in the attention computation to significantly accelerate the pre-filling
    stage of long-context LLM inference via dynamic sparse attention. Unlike existing
    dynamic sparse attention methods that introduce large computational overhead to
    estimate attention patterns with low-rank hidden dimensions [[47](#bib.bib47),
    [63](#bib.bib63)], our method is designed specifically for long-context scenarios
    with minimal overhead in estimation. Specifically, we conduct extensive analysis
    and identify three general patterns of sparse attention in long-context LLMs:
    A-shape pattern, Vertical-Slash pattern, and Block-Sparse pattern. Based on these
    findings, we introduce a kernel-aware search method to assign the optimal attention
    pattern for each head. Importantly, instead of fixed attention masks in prior
    studies, we perform an efficient online approximation to build a dynamic sparse
    mask for each head according to their assigned pattern and particular inputs.
    For example, to build a dynamic sparse mask for a specific prompt on one Vertical-Slash
    head, we use a partial of attention weight consisting of the last last_q query
    and key vectors (i.e. $\bm{Q}_{[-\text{last\_q}:]}$) to estimate the most important
    indices of the vertical and slash lines globally on the attention matrix. For
    Block-Sparse heads, we perform mean pooling on both query and key vectors in blocks
    of 64 and calculate the block-level attention weights to determine the most important
    blocks and thereby obtain a block-sparse dynamic mask. After obtaining the dynamic
    sparse mask, three optimized GPU kernels are used, which we developed for the
    above three sparse patterns. These kernels are based on the dynamic sparse compilers
    PIT [[88](#bib.bib88)], Triton [[75](#bib.bib75)] and FlashAttention [[13](#bib.bib13)],
    which enable extremely efficient computation of dynamic sparse attention.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，我们提出了MInference，这是一种通过动态稀疏注意力技术减少95% FLOPs的注意力计算，以显著加速长上下文LLM推理的预填充阶段。与现有的动态稀疏注意力方法相比，这些方法引入了大量计算开销来估计低秩隐藏维度的注意力模式 [[47](#bib.bib47),
    [63](#bib.bib63)]，我们的方法专为长上下文场景设计，估计开销最小。具体而言，我们进行了广泛的分析，并识别出长上下文LLM中的三种一般稀疏注意力模式：A形模式、垂直斜线模式和块稀疏模式。基于这些发现，我们引入了一种内核感知搜索方法，为每个头分配最佳注意力模式。重要的是，与之前研究中的固定注意力掩码不同，我们通过高效的在线近似为每个头构建一个动态稀疏掩码，根据其分配的模式和特定输入进行调整。例如，为特定提示在一个垂直斜线头上构建动态稀疏掩码时，我们使用由最后一个查询和键向量组成的部分注意力权重（即$\bm{Q}_{[-\text{last\_q}:]}$）来估计注意力矩阵中垂直和斜线的最重要索引。对于块稀疏头，我们对查询和键向量进行64块的均值池化，并计算块级注意力权重，以确定最重要的块，从而获得块稀疏动态掩码。获得动态稀疏掩码后，使用三种优化的GPU内核，这些内核是我们为上述三种稀疏模式开发的。这些内核基于动态稀疏编译器PIT [[88](#bib.bib88)]、Triton [[75](#bib.bib75)]和FlashAttention [[13](#bib.bib13)]，实现了动态稀疏注意力的极高效计算。
- en: Extensive experiments are conducted on various Long-context LLMs, including
    LLaMA-3-8B-1M [[24](#bib.bib24)], GLM-4-9B-1M [[26](#bib.bib26)], and Yi-9B-200K [[84](#bib.bib84)],
    across benchmarks with context lengths over 1M tokens, such as InfiniteBench [[86](#bib.bib86)],
    RULER [[28](#bib.bib28)], Needle In A Haystack [[35](#bib.bib35)], and PG-19 [[65](#bib.bib65)].
    Needle In A Haystack was also tested on Phi-3-Mini-128K [[2](#bib.bib2)] and Qwen-2-7B-128K [[5](#bib.bib5)].
    Results show that MInference speeds up the pre-filling stage by up to $10\times$
    for 1M contexts with LLaMA-3-8B on a single A100, reducing latency from 30 minutes
    to 3 minutes per prompt, while maintaining or improving accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对各种长上下文LLM进行大量实验，包括LLaMA-3-8B-1M [[24](#bib.bib24)]、GLM-4-9B-1M [[26](#bib.bib26)]和Yi-9B-200K [[84](#bib.bib84)]，在上下文长度超过1M的基准测试中，如InfiniteBench [[86](#bib.bib86)]、RULER [[28](#bib.bib28)]、Needle
    In A Haystack [[35](#bib.bib35)]和PG-19 [[65](#bib.bib65)]。Needle In A Haystack还在Phi-3-Mini-128K [[2](#bib.bib2)]和Qwen-2-7B-128K [[5](#bib.bib5)]上进行了测试。结果显示，MInference使得1M上下文的预填充阶段加速高达$10\times$，在单个A100上使用LLaMA-3-8B，将延迟从每个提示30分钟减少到3分钟，同时保持或提高准确性。
- en: '2 Attention Heads: Dynamic, Sparse, and Characteristic'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力头：动态的、稀疏的和特征化的
- en: '![Refer to caption](img/10ccd929778db2d7ea98214ca2a9092c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10ccd929778db2d7ea98214ca2a9092c.png)'
- en: (a) Attention incurs heavy cost.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力开销很大。
- en: '![Refer to caption](img/84a510695ce9529c4982da1844ae2d16.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/84a510695ce9529c4982da1844ae2d16.png)'
- en: (b) Attention is sparse.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是稀疏的。
- en: '![Refer to caption](img/804139198ed227c4ddf295cd89c1f3cd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/804139198ed227c4ddf295cd89c1f3cd.png)'
- en: (c) Sparsity of attention is dynamic.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力的稀疏性是动态的。
- en: 'Figure 2: (a) Latency breakdown of the pre-filling stage. (b) How much attention
    scores can top-k (k=4096) columns cover in a 128k context. (c) Less attention
    scores are retrieved when reusing the top-k indices from another examples, indicating
    its dynamic nature. Visualizations are based on LLaMa-3-8B with a single A100.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：（a）预填充阶段的延迟分解。（b）在128k上下文中，前k（k=4096）列可以覆盖多少注意力分数。（c）从另一个示例重用前k索引时，检索到的注意力分数较少，显示了其动态特性。可视化基于单个A100的LLaMa-3-8B。
- en: 2.1 Attention is Dynamically Sparse
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 注意力是动态稀疏的
- en: 'The sparsity of attention weights in pre-trained LLMs, especially in long-context
    scenarios, has been well-documented [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]. As shown in Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ 2 Attention
    Heads: Dynamic, Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), for an attention matrix
    of size $128k\times 128k$, retaining only the top 4k columns recalls 96.8% of
    the total attention. In other words, each token is attending to a limit number
    of tokens despite the long sequence it is processing.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练LLMs中的注意力权重稀疏性，特别是在长上下文场景中，已被充分记录 [[47](#bib.bib47), [63](#bib.bib63), [48](#bib.bib48),
    [82](#bib.bib82)]。如图 [2b](#S2.F2.sf2 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")所示，对于大小为$128k\times
    128k$的注意力矩阵，仅保留前4k列即可回忆96.8%的总注意力。换句话说，尽管处理的是长序列，每个令牌只关注有限数量的令牌。
- en: 'On the other hand, although the sparse nature of attention matrices is shared
    across different inputs, the exact distributions of sparse pattern are highly
    dynamic. That is to say, a token at a given position only attends to a subset
    of the sequence in self-attention, and the exact tokens it attends to are highly
    context-dependent and vary significantly across different prompts. This dynamism
    has been mathematically demonstrated in prior studies [[39](#bib.bib39), [40](#bib.bib40)].
    As depicted in Fig. [2c](#S2.F2.sf3 "In Figure 2 ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), if we take the top 4k columns found in Fig. [2b](#S2.F2.sf2
    "In Figure 2 ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and apply it on another prompt of 128k, the recall of attention would drop largely
    to 83.7%.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管不同输入间注意力矩阵的稀疏特性是一致的，但稀疏模式的具体分布是高度动态的。也就是说，给定位置的令牌在自注意力中仅关注序列的一个子集，而它关注的具体令牌高度依赖于上下文，并且在不同提示下变化显著。这种动态性在先前的研究中已被数学证明 [[39](#bib.bib39),
    [40](#bib.bib40)]。如图 [2c](#S2.F2.sf3 "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")所示，如果我们取图 [2b](#S2.F2.sf2
    "图2 ‣ 2 注意力头：动态、稀疏和特征 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLMs的预填充")中的前4k列并应用于另一个128k的提示，注意力的回忆将大幅下降至83.7%。
- en: 2.2 Attention Sparsity Exhibits Patterns
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 注意力稀疏性展现模式
- en: 'Table 1: Comparison of different sparse patterns.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同稀疏模式的比较。
- en: '| Patterns | A-shape | Vertical-Slash | Block-Sparse | Top-K |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | A形 | 竖直斜杠 | 块稀疏 | Top-K |'
- en: '| Spatial Distribution | Static structured | Dynamic structured | Dynamic structured
    | Dynamic fine-grained |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 空间分布 | 静态结构 | 动态结构 | 动态结构 | 动态细粒度 |'
- en: '| Latency on GPU | Low | Medium | Low | High |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPU延迟 | 低 | 中 | 低 | 高 |'
- en: '| Time to build the index | Zero | Small | Small | High | ![Refer to caption](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| 建立索引的时间 | 零 | 小 | 小 | 高 | ![参见说明](img/e441ab8dbbfceff25aeb2a23840f8d3a.png)'
- en: (a) Attention patterns
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 注意力模式
- en: '![Refer to caption](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/438a7c15e3fde43297c7c0f4f6cb7244.png)'
- en: (b) Attention is spatial clustering
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力是空间聚类
- en: '![Refer to caption](img/155b7edb1253cb7cf43bccc0db737600.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/155b7edb1253cb7cf43bccc0db737600.png)'
- en: (c) Attention pattern recall
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 注意力模式回忆
- en: 'Figure 3: (a) Visualization of attention weights from different attention heads.
    For different prompts and tasks, the pattern of the same head is relatively consistent,
    but the sparse indices are dynamically changing.(b) Distance of the top-10 nearest
    non-zero element in the attention matrix. (c) Attention recall distribution using
    our identified patterns, where FLOPs in the kernel refer to the real FLOPs required
    for sparse attention computing using on GPUs. Here, a 1x64 block size is used
    for the Vertical-Slash pattern, and a 64x64 block size is used for others on GPUs.
    All visualization are based on LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)].'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: (a) 来自不同注意力头的注意力权重可视化。对于不同的提示和任务，相同头的模式相对一致，但稀疏索引动态变化。(b) 注意力矩阵中前10个非零元素的距离。(c)
    使用我们识别的模式的注意力召回分布，其中内核中的FLOPs指的是在GPU上进行稀疏注意力计算所需的实际FLOPs。这里，Vertical-Slash 模式使用
    1x64 的块大小，其他模式在GPU上使用 64x64 的块大小。所有可视化均基于 LLaMA-3-8B-Instruct-262K [[24](#bib.bib24)]。'
- en: 'Although the sparsity distribution of attention matrix is dynamic, previous
    works [[82](#bib.bib82), [29](#bib.bib29)] have shown that they exhibit certain
    patterns in the two-dimensional space such as spatial clustering. Through our
    analysis of long-context prompts of various lengths and tasks, we have categorized
    such attention sparse patterns into the A-shape, Vertical-Slash (VS), and Block-Sparse
    patterns, as shown in Fig. [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    and Fig. [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"). Table [1](#S2.T1
    "Table 1 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") details the characteristics and differences
    between these three patterns.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管注意力矩阵的稀疏性分布是动态的，但之前的研究 [[82](#bib.bib82), [29](#bib.bib29)] 已经显示它们在二维空间中展现出某些模式，例如空间聚类。通过对各种长度和任务的长上下文提示进行分析，我们将这种注意力稀疏模式分类为A形、Vertical-Slash
    (VS) 和 Block-Sparse 模式，如图 [3a](#S2.F3.sf1 "In Figure 3 ‣ 2.2 Attention Sparsity
    Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse, and Characteristic ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")
    和图 [4](#S3.F4 "Figure 4 ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention") 所示。表 [1](#S2.T1 "Table 1
    ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic, Sparse,
    and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention") 详细介绍了这三种模式的特征和差异。'
- en: A-shape pattern The attention weights of these types of heads are concentrated
    on initial tokens and local windows [[82](#bib.bib82), [29](#bib.bib29)], exhibiting
    relatively higher stability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: A-shape 模式这些头的注意力权重集中在初始标记和局部窗口上 [[82](#bib.bib82), [29](#bib.bib29)]，展现出相对较高的稳定性。
- en: Vertical-Slash (VS) pattern The attention weights are concentrated on specific
    tokens (vertical lines) and tokens at fixed intervals (slash lines). The positions
    of vertical and slash lines in this pattern dynamically change with the context
    content and exhibit a certain sparsity, making them difficult to be encompassed
    by local windows and A-shape patterns.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Vertical-Slash (VS) 模式中，注意力权重集中在特定的标记（垂直线）和固定间隔的标记（斜线）上。该模式中垂直线和斜线的位置会随着上下文内容动态变化，并展示出一定的稀疏性，使其难以被局部窗口和A形模式所涵盖。
- en: 'Block-Sparse pattern This sparsity pattern is the most dynamic, exhibiting
    a more dispersed distribution. Despite its dynamism, the attention weights maintain
    some characteristics of spatial clustering, which we identify as the block-sparse
    pattern. We analyzed the distances between non-zero attention weights and their
    top-k nearest non-zero neighbors within a 128k prompt as shown in Fig. [3b](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"). The results indicate that across layers and
    heads, the distances between nearest non-zero values are generally concentrated
    around 5, suggesting a strong spatial clustering of the attention weights.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Block-Sparse模式 这种稀疏模式最具动态性，表现出更分散的分布。尽管具有动态性，注意力权重仍然保持一些空间聚类的特征，我们将其识别为块稀疏模式。我们分析了在128k提示下非零注意力权重及其top-k最近非零邻居之间的距离，如图[3b](#S2.F3.sf2
    "在图3 ‣ 2.2 注意力稀疏性展示模式 ‣ 2 注意力头：动态的、稀疏的和特征性的 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")所示。结果表明，在层和头之间，最近的非零值之间的距离通常集中在5左右，表明注意力权重具有较强的空间聚类性。
- en: 'The point of these three patterns is that we can leverage them to perform highly
    efficient sparse computing for the attention matrix in long-context LLMs. In Fig. [3c](#S2.F3.sf3
    "In Figure 3 ‣ 2.2 Attention Sparsity Exhibits Patterns ‣ 2 Attention Heads: Dynamic,
    Sparse, and Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention"), we test how efficient is our indentified
    patterns retrieving attention scores with limit computing cost on GPU (FLOPs).
    First, attention heads are labeled with one of the sparse pattern (detail see
    §[3.2](#S3.SS2 "3.2 Speedup of Long-context LLM Inference via Dynamic Sparse Attention
    ‣ 3 MInference 1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context
    LLMs via Dynamic Sparse Attention")). Then we demonstrate our patterns are significantly
    more efficient compared to other sparse methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)]. Specifically, with the same amount of FLOPs, our patterns achieve
    a notable higher recall on attention scores, which can potentially lead to better
    accuracy. For example, previous Top-K methods [[63](#bib.bib63), [82](#bib.bib82),
    [59](#bib.bib59)] struggle with the Block-Sparse pattern as they focus on specific
    tokens globally, while our pattern retrieves attention scores more efficiently
    and accurately. We example how we use these patterns on long-context LLMs and
    how we implement optimized GPU kernels for these patterns in §[3](#S3 "3 MInference
    1.0 ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic
    Sparse Attention").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种模式的要点是，我们可以利用它们对长上下文的LLM中的注意力矩阵进行高效的稀疏计算。在图[3c](#S2.F3.sf3 "在图3 ‣ 2.2 注意力稀疏性展示模式
    ‣ 2 注意力头：动态的、稀疏的和特征性的 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中，我们测试了我们识别的模式在GPU（FLOPs）上以有限计算成本检索注意力分数的效率。首先，注意力头被标记为一种稀疏模式（详细见§[3.2](#S3.SS2
    "3.2 通过动态稀疏注意力加速长上下文LLM推理 ‣ 3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")）。然后，我们展示了与其他稀疏方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]相比，我们的模式显著更高效。具体来说，在相同数量的FLOPs下，我们的模式在注意力分数上取得了显著更高的召回率，这可能导致更好的准确性。例如，之前的Top-K方法[[63](#bib.bib63),
    [82](#bib.bib82), [59](#bib.bib59)]在处理Block-Sparse模式时遇到困难，因为它们关注的是全局的特定令牌，而我们的模式更高效、准确地检索注意力分数。我们在§[3](#S3
    "3 MInference 1.0 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中举例说明了如何在长上下文LLM上使用这些模式以及如何为这些模式实现优化的GPU内核。
- en: 3 MInference 1.0
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 MInference 1.0
- en: 'Following the analysis in §[2](#S2 "2 Attention Heads: Dynamic, Sparse, and
    Characteristic ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs
    via Dynamic Sparse Attention"), we propose MInference to accelerate the pre-filling
    stage of long-context LLMs, consisting of three steps: 1) Offline attention pattern
    identification for each head; 2) Dynamic build of sparse indices w.r.t. the pattern;
    3) Sparse attention calculation with optimized GPU kernels.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 根据§[2](#S2 "2 注意力头：动态的、稀疏的和特征性的 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充")中的分析，我们提出了MInference来加速长上下文LLM的预填充阶段，包括三个步骤：1)
    离线注意力模式识别；2) 根据模式动态构建稀疏索引；3) 使用优化的GPU内核进行稀疏注意力计算。
- en: '![Refer to caption](img/717aebceb4eff85c06852353dc924b36.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/717aebceb4eff85c06852353dc924b36.png)'
- en: 'Figure 4: The three sparse methods in MInference.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MInference中的三种稀疏方法。
- en: 3.1 Problem Formulation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: 'When accelerating the pre-filling stage of long-context LLMs with sparse attention
    computing, the attention matrix can be formulated as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在加速长上下文LLMs的稀疏注意力计算的预填充阶段时，注意力矩阵可以表示为：
- en: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{A(M)}=\text{Softmax}(\frac{1}{\sqrt{d}}\bm{Q}\bm{K}^{\top}-c(1-\bm{M})),$
    |  | (1) |'
- en: where $M_{i,j}\in\{0,1\}$.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M_{i,j}\in\{0,1\}$。
- en: 'The goal of the dynamic sparse attention system is to achieve greater speedup
    with minimal overhead while retaining as much of the attention weights as possible.
    Formally, this can be expressed as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稀疏注意力系统的目标是以最小的开销实现更大的加速，同时尽可能保留更多的注意力权重。形式上，这可以表示为：
- en: '|  | $\displaystyle\min$ |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  | (2) |'
- en: '|  | $\displaystyle\min$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ |  |'
- en: where $t_{\text{sparse}}$ represent the time spent on dynamic sparse attention
    computation and estimation of the approximate dynamic sparse pattern, respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t_{\text{sparse}}$ 分别表示用于动态稀疏注意力计算和估算近似动态稀疏模式的时间。
- en: Algorithm 1 Kernel-Aware Sparse Pattern Search
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 内核感知稀疏模式搜索
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$     while $|t_{i}-t|> do     90%). However,
    according to the ablation study, using only the Vertical-Slash pattern significantly
    impacts performance in highly dynamic tasks like KV retrieval. Secondly, the Block-Sparse
    pattern is primarily distributed in several intermediate to later layers, while
    the A-shape pattern is found in the middle layers. Although the optimal patterns
    vary slightly across different models, they generally align with these observations.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#A5.F11 "Figure 11 ‣ Appendix E Pattern Distribution ‣ MInference 1.0:
    Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")展示了通过我们搜索得到的最佳头配置的分布。首先，大多数模式是Vertical-Slash模式（>90%）。然而，根据消融研究，仅使用Vertical-Slash模式在像KV检索这样高度动态的任务中显著影响性能。其次，Block-Sparse模式主要分布在几个中层到后层，而A-shape模式则出现在中层。尽管不同模型的最佳模式略有不同，但它们通常与这些观察结果一致。'
- en: Additionally, we used the same configuration for two versions of LLaMA in our
    experiments, and the results show that the 1M model also performs very well, with
    nearly perfect results in the Needle In A Haystack task. This demonstrates the
    generalizability of the optimal sparse pattern.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在实验中对两个版本的LLaMA使用了相同的配置，结果表明，1M模型也表现非常好，在Needle In A Haystack任务中几乎达到了完美的结果。这展示了最佳稀疏模式的普遍性。
- en: Appendix F Sparsity in Kernel Distribution
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 内核分布中的稀疏性
- en: '![Refer to caption](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/08ae6d64a7ea3a4f472fcbd33fb6c60d.png)'
- en: 'Figure 12: The distribution of sparsity in the kernel across different context
    windows refers to the proportion of the kernel that is actually computed after
    block coverage, compared to the sparsity rate when using FlashAttention with a
    causal mask.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：内核在不同上下文窗口中的稀疏分布指的是在块覆盖后实际计算的内核比例，与使用带有因果掩码的FlashAttention时的稀疏率相比。
- en: 'As shown in Fig. [12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution
    ‣ MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse
    Attention"), the sparsity distribution of the three patterns during the actual
    kernel computation process is displayed. It can be seen that when the context
    windows exceed 200k, the actual sparsity of all three patterns surpasses 90%.
    Even considering a 20% index-building overhead, this ensures that the kernel achieves
    a speedup of over 8$\times$.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[12](#A6.F12 "Figure 12 ‣ Appendix F Sparsity in Kernel Distribution ‣ MInference
    1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention")所示，实际内核计算过程中三种模式的稀疏分布情况被展示出来。可以看到，当上下文窗口超过200k时，三种模式的实际稀疏度都超过了90%。即便考虑20%的索引构建开销，这也确保了内核获得了超过8$\times$的加速。'
- en: Appendix G Does This Dynamic Sparse Attention Pattern Exist Only in Auto-Regressive
    LLMs or RoPE-Based LLMs?
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 这种动态稀疏注意模式仅存在于自回归LLMs或基于RoPE的LLMs中吗？
- en: '![Refer to caption](img/6b22e182a2b67446c71bc01d753a7101.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b22e182a2b67446c71bc01d753a7101.png)'
- en: 'Figure 13: The sparse pattern in T5-style Encoder Attention using Flan-UL2 [[74](#bib.bib74)]
    on the Summarization dataset [[86](#bib.bib86)].'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：使用Flan-UL2[[74](#bib.bib74)]在Summarization数据集[[86](#bib.bib86)]上进行T5风格Encoder
    Attention的稀疏模式。
- en: 'Similar vertical and slash line sparse patterns have been discovered in BERT [[72](#bib.bib72)]
    and multi-modal LLMs [[80](#bib.bib80)]. Additionally, as shown in Fig. [13](#A7.F13
    "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern Exist Only
    in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating Pre-filling
    for Long-Context LLMs via Dynamic Sparse Attention"), we analyzed the distribution
    of attention patterns in T5 across different heads. It is evident that there are
    vertical and slash sparse patterns even in bidirectional attention.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '在 BERT [[72](#bib.bib72)] 和多模态 LLMs [[80](#bib.bib80)] 中已经发现了类似的垂直和斜线稀疏模式。此外，如图
    [13](#A7.F13 "Figure 13 ‣ Appendix G Does This Dynamic Sparse Attention Pattern
    Exist Only in Auto-Regressive LLMs or RoPE-Based LLMs? ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") 所示，我们分析了 T5 在不同头部中的注意力模式分布。显然，即使在双向注意力中也存在垂直和斜线稀疏模式。'
- en: Recent studies [[80](#bib.bib80)] have analyzed sparse attention patterns in
    multi-modal LLMs, revealing the presence of vertical and slash patterns in models
    like LLaVA [[45](#bib.bib45)] and InternVL [[11](#bib.bib11)]. Using MInference
    for pre-filling stage inference acceleration holds great promise.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 [[80](#bib.bib80)] 分析了多模态 LLMs 中的稀疏注意力模式，揭示了 LLaVA [[45](#bib.bib45)]
    和 InternVL [[11](#bib.bib11)] 等模型中存在垂直和斜线模式。使用 MInference 进行预填充阶段的推理加速具有很大前景。
- en: Appendix H Case Study
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 案例研究
- en: 'Table [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") presents a comparison
    of the generation performance for various methods on the EN.SUM task (200K input
    length) from InfiniteBench based on the LLaMA-3-8B-262K model. The original summary
    provides a comprehensive and coherent narrative, detailing the Bronwyn family’s
    trip to the Kindergarten and touching on themes such as nostalgia, loss, and the
    passage of time. StreamingLLM’s summary, although looks coherent, introduces elements
    that are not present in the original story, leading to serious factual errors.
    For example, it mentions a boat trip to a school for boys and specific details
    like fishermen, sandwiches, and a spot where men were drowned. These details deviate
    from the original story, which is about the Bronwyn family preparing for a trip
    to the Kindergarten. In addition, the summaries generated by StreamingLLM with
    dilated and strided techniques are largely incoherent, consisting primarily of
    repetitive and nonsensical characters, indicating a failure to produce meaningful
    content. In stark contrast, the summary generated by our proposed method offers
    a detailed and coherent narrative, comparable to the original, with a clear depiction
    of the story’s main events and themes. This includes the preparation of the Bronwyn
    family for their trip, the characterization of family members and guests, and
    the exploration of deeper themes such as love, marriage, and the search for meaning.
    The results demonstrate the superiority of our proposed method in generating high-quality,
    human-like summaries over the baseline methods.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [8](#A8.T8 "Table 8 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") 展示了基于 LLaMA-3-8B-262K
    模型的 InfiniteBench 上 EN.SUM 任务（200K 输入长度）的各种方法生成性能的比较。原始摘要提供了一个全面且连贯的叙述，详细描述了 Bronwyn
    家庭去幼儿园的旅行，并涉及怀旧、失落和时间流逝等主题。尽管 StreamingLLM 的摘要看起来连贯，但引入了原故事中不存在的元素，导致严重的事实错误。例如，它提到了前往男孩学校的船行以及具体的细节，如渔民、三明治和男人溺水的地方。这些细节偏离了原故事，原故事讲的是
    Bronwyn 家庭准备前往幼儿园的旅行。此外，使用扩张和步幅技术生成的 StreamingLLM 摘要大多不连贯，主要由重复和无意义的字符组成，显示出未能生成有意义的内容。相比之下，我们提出的方法生成的摘要提供了详细且连贯的叙述，与原文相当，清晰地描绘了故事的主要事件和主题。这包括
    Bronwyn 家庭为旅行做准备、家庭成员和客人的刻画，以及探索爱、婚姻和寻找意义等更深层次的主题。结果展示了我们提出的方法在生成高质量、人性化摘要方面优于基线方法。'
- en: 'Table [9](#A8.T9 "Table 9 ‣ Appendix H Case Study ‣ MInference 1.0: Accelerating
    Pre-filling for Long-Context LLMs via Dynamic Sparse Attention") compares the
    performance of various methods on the Retrieve.KV task (200K input length) using
    the LLaMA-3-8B-262K model. The original method demonstrates perfect retrieval,
    correctly predicting the exact strings of the ground truth for both examples.
    StreamingLLM, again, generates predictions that looks coherent and real, but factually
    incorrect. In addition, StreamingLLM with dilated and strided techniques, and
    our method with a static pattern, fail significantly, producing outputs that are
    either repetitive sequences of characters or nonsensical strings, indicating their
    inability to accurately retrieve the required key-value pairs. Our method, however,
    performs on par with the original, accurately retrieving and predicting the exact
    key-value pairs for both examples. This demonstrates the superior capability of
    our method in handling KV retrieval tasks, providing precise and reliable outputs
    consistent with the ground truth. The results highlight our method’s effectiveness
    and robustness compared to the baselines, making it a reliable choice for such
    tasks.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [9](#A8.T9 "表 9 ‣ 附录 H 案例研究 ‣ MInference 1.0：通过动态稀疏注意力加速长上下文 LLM 的预填充") 比较了使用
    LLaMA-3-8B-262K 模型在 Retrieve.KV 任务（200K 输入长度）上的各种方法的性能。原始方法表现出完美的检索，准确预测了两个示例的地面真实值的确切字符串。StreamingLLM
    再次生成了看起来连贯和真实的预测，但事实却不正确。此外，使用膨胀和步长技术的 StreamingLLM 以及我们使用静态模式的方法都显著失败，生成了重复的字符序列或无意义的字符串，表明它们无法准确检索所需的键值对。然而，我们的方法与原始方法表现相当，准确检索并预测了两个示例的确切键值对。这展示了我们的方法在处理
    KV 检索任务中的优越能力，提供了与地面真实值一致的精确和可靠的输出。结果突显了我们的方法相较于基线的有效性和鲁棒性，使其成为此类任务的可靠选择。
- en: Algorithm 4 Vertical-Slash Index
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 竖直斜线索引
- en: 'Input: vertical indexes $\bm{i}_{v}\in\mathbb{N}^{k_{v}}$, column count $\bm{c}_{\text{col}}\mathbb{N}^{N}$  #
    Merge points (vertical indexes) and ranges (slash indexes)     while $s_{v}\leq
    k_{s}$  # Update the range           if $(i-1)\times B-\bm{i}_{s}^{j_{s}}>           else              #
    Extend the range                         else              # 扩展范围              <math
    id=$'
- en: Algorithm 5 Vertical-Slash Flash Attention
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 竖直斜线闪光注意力
- en: 'Input: $\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ to $N$Load $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$  end for'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\bm{Q},\bm{K},\bm{V}\in\mathbb{R}^{S\times d_{h}}$ 到 $N$ 载入 $\bm{K}_{\text{chip}}\leftarrow\bm{K}^{s:s+B}\in\mathbb{R}^{B\times
    d_{h}}$        $\bm{\alpha}\leftarrow\mathrm{exp}(\bm{m}^{i}-\bm{m}^{i}_{new})$        $\bm{S}\leftarrow\tau\bm{Q}_{\text{chip}}\bm{K}_{\text{chip}}^{T}$        $\bm{O}_{\text{chip}}\leftarrow\bm{\alpha}\bm{O}_{\text{chip}}+\bm{P}\bm{V}_{\text{chip}}$  end for
- en: 'Table 8: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in summarization task [[86](#bib.bib86)].'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：使用 LLaMA-3-8B-Instruct-262K 进行摘要任务的不同方法生成结果的比较 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 |'
- en: '| Original | The story follows the Bronwyn family and their guests as they
    prepare for a day trip to the Kindergarten, a mysterious destination that holds
    great significance for them. The story is told from multiple perspectives, including
    that of Pete Lucy, who is staying with the family and is struggling to find her
    place in their lives. As the day progresses, the family members prepare for their
    trip, and Pete reflects on her own past experiences with the Bronwyns. The story
    ends with the family setting off on their journey, leaving Pete alone on the lawn
    to contemplate her own feelings and the meaning of life. The Kindergarten is a
    symbol of the family’s past and their memories, and the story explores themes
    of nostalgia, loss, and the passage of time. Ultimately, the story suggests that
    even though the past cannot be changed, it can still be celebrated and remembered.
    |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 故事讲述了布朗温家族及其宾客在准备前往一个对他们具有重大意义的神秘目的地——幼儿园的一日游的过程。故事从多个视角叙述，包括正在与家人同住并努力融入他们生活的皮特·露西。随着一天的进展，家人们为出行做准备，皮特回忆起自己与布朗温家族的过去经历。故事在家人们启程的过程中结束，皮特独自留在草坪上，思索自己的感受和生命的意义。幼儿园是家族过去和回忆的象征，故事探讨了怀旧、失落和时间流逝的主题。最终，故事暗示尽管过去无法改变，但仍可以庆祝和铭记。
    |'
- en: '| StreamingLLM | The story follows a family on a boat trip to the Kindergarten,
    a school for boys. The father, Mr Bronwyn, is reading a book and seems to be in
    a trance. The children, Seb and Annalisa, are excited about the trip but also
    a little scared. They pass by a group of fishermen and Mr Bronwyn shares out sandwiches.
    They come across a spot where three men were drowned and Mr Bronwyn takes out
    his watch, making a calculation. They reach the Kindergarten and Mr Bronwyn jumps
    out of the boat, holding a parcel. The story ends with Pete Lucy, who has been
    watching the family from the shore, feeling relieved that the trip is over and
    that Mr Bronwyn has reached his destination. The story is told from multiple perspectives,
    including Pete Lucy’s, who is watching the family from the shore and reflecting
    on his own life and relationships. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 故事讲述了一个家庭乘船前往幼儿园的旅行，幼儿园是一所男孩学校。父亲布朗温先生正在读一本书，看起来神情恍惚。孩子们，塞布和安娜莉莎，对这次旅行感到兴奋但也有些害怕。他们经过一群渔民，布朗温先生分发三明治。他们遇到了一个曾有三人溺亡的地方，布朗温先生拿出手表，进行计算。他们到达幼儿园，布朗温先生跳下船，拿着一个包裹。故事以皮特·露西的视角结束，她在岸边观察这个家庭，对旅行结束和布朗温先生达到目的地感到释然。故事从多个视角叙述，包括皮特·露西，她在岸边观察这个家庭，反思自己的生活和人际关系。'
- en: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ dilated | A ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( |'
- en: '| StreamingLLM w/ strided | The GraphicsUnit36.OOAKA new ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM w/ strided | 图形单元36.OOAKA新 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( |'
- en: '| Ours w/ static | def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 我们的静态 | def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def |'
- en: '| Ours | The story follows the Bronwyn family and their guests as they prepare
    for a trip to the Kindergarten, a remote island off the coast of Scotland. The
    family is made up of Mr. Bronwyn, a philosopher, and his wife, Mrs. Bronwyn, who
    is known for her beauty and her ability to surround and protect her family. The
    guests include Kevin Emma, a young man who is studying for a fellowship, and Castor
    Kiki, a botanist who is also a widower. The story explores themes of love, marriage,
    and the search for meaning in life. It also touches on the idea of the other,
    or the person who is different from oneself and cannot be understood or judged
    by conventional standards. Ultimately, the story is about the search for truth
    and the desire to connect with others on a deeper level. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 故事讲述了布朗温一家及其客人在准备前往幼儿园的旅程中，他们去的是一个位于苏格兰海岸的偏远小岛。这个家庭由哲学家布朗温先生和以美丽和保护家人而著称的布朗温夫人组成。客人包括凯文·艾玛，一个正在为获得奖学金而学习的年轻人，以及卡斯特·基基，一个也是鳏夫的植物学家。这个故事探讨了爱情、婚姻以及对生活意义的追寻。它还涉及了“他者”的概念，即与自己不同的人，无法通过传统标准来理解或评判。最终，这个故事是关于寻求真理和渴望与他人建立更深层次联系的。'
- en: 'Table 9: Comparison of generated results by different methods using LLaMA-3-8B-Instruct-262K
    in KV Retrieval task [[86](#bib.bib86)].'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：使用 LLaMA-3-8B-Instruct-262K 在 KV 检索任务中不同方法生成结果的比较 [[86](#bib.bib86)]。
- en: '| Methods | Summary |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 |'
- en: '| Original | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '| 原始 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。'
- en: '| StreamingLLM | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant 2\. Ground Truth: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: 2b114db0-d87e-42d2-9b4c-0b1f115976ad.assistant
    2\. 实际值: bbf3fa73-6217-4e6e-923c-8349bd286c3d'
- en: 'Prediction: "d829ce18-6339-4f77-8c04-31fc7ec33619". |'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "d829ce18-6339-4f77-8c04-31fc7ec33619"。'
- en: '| StreamingLLM w/ dilated | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: 5 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 带扩张 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: 5 (
    ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( (
    ( ( 2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    |'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    def solverome2 def solverome2 def solverome2 def solverome2 def solverome2 |'
- en: '| StreamingLLM w/ strided | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2
    Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def solverome2
    2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '| StreamingLLM 带有跨步 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "def
    solverome2 def solverome2 def solverome2 def solverome2 def solverome2 2\. 实际值:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "0 ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( |'
- en: '| Ours w/ static | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "def solverome2 def solverome2 def solverome2 def solverome2 def 2\. Ground Truth:
    f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的 带静态 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "def solverome2
    def solverome2 def solverome2 def solverome2 def 2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "def solverome2 def solverome2 def solverome2 def solverome2 def
    |'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "def solverome2 def solverome2 def solverome2 def solverome2 def |'
- en: '| Ours | 1\. Ground Truth: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 Prediction:
    "5e6b7b90-710d-4953-9b18-3e96b2cadbf2" 2\. Ground Truth: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '| 我们的 | 1\. 实际值: 5e6b7b90-710d-4953-9b18-3e96b2cadbf2 预测值: "5e6b7b90-710d-4953-9b18-3e96b2cadbf2"
    2\. 实际值: f1ec3dd8-b6e7-4af5-a589-fda0b7693f93'
- en: 'Prediction: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93" (a string of length 32).
    |'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '预测值: "f1ec3dd8-b6e7-4af5-a589-fda0b7693f93"（长度为32的字符串）。'
