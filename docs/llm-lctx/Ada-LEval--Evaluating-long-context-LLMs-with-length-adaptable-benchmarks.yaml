- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:03:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ada-LEval：使用长度可调的基准评估长文本 LLMs
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06480](https://ar5iv.labs.arxiv.org/html/2404.06480)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06480](https://ar5iv.labs.arxiv.org/html/2404.06480)
- en: Chonghua Wang², Haodong Duan^(1†), Songyang Zhang¹, Dahua Lin¹, Kai Chen^(1‡)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Chonghua Wang², Haodong Duan^(1†), Songyang Zhang¹, Dahua Lin¹, Kai Chen^(1‡)
- en: ¹Shanghai AI Laboratory
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹上海人工智能实验室
- en: ²Shanghai Jiao Tong University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²上海交通大学
- en: philipwang@sjtu.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: philipwang@sjtu.edu.cn
- en: duanhaodong@pjlab.org.cn   The work was done during an internship at Shanghai
    AI Laboratory; ^† Project Lead; ^‡ Corresponding Author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: duanhaodong@pjlab.org.cn   工作是在上海人工智能实验室实习期间完成的；^† 项目负责人；^‡ 通讯作者。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, the large language model (LLM) community has shown increasing interest
    in enhancing LLMs’ capability to handle extremely long documents. As various long-text
    techniques and model architectures emerge, the precise and detailed evaluation
    of models’ long-text capabilities has become increasingly important. Existing
    long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text
    test sets based on open-source datasets, focusing mainly on QA and summarization
    tasks. These datasets include test samples of varying lengths (from 2k to 32k+)
    entangled together, making it challenging to assess model capabilities across
    different length ranges. Moreover, they do not cover the ultralong settings (100k+
    tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval,
    a length-adaptable benchmark for evaluating the long-context understanding of
    LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which
    enable a more reliable evaluation of LLMs’ long context capabilities. These benchmarks
    support intricate manipulation of the length of test cases, and can easily produce
    text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API
    models and 6 open-source models with Ada-LEval. The evaluation results demonstrate
    the limitations of current LLMs, especially in ultra-long-context settings. Our
    code is available at [https://github.com/open-compass/Ada-LEval](https://github.com/open-compass/Ada-LEval).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLM）社区对提升 LLMs 处理极长文档的能力表现出越来越高的兴趣。随着各种长文本技术和模型架构的出现，对模型长文本能力的精确而详细的评估变得越来越重要。现有的长文本评估基准，如
    L-Eval 和 LongBench，基于开源数据集构建长文本测试集，主要关注 QA 和总结任务。这些数据集包括长度不等（从 2k 到 32k+）的测试样本，混合在一起，使得在不同长度范围内评估模型能力具有挑战性。此外，它们未涵盖最新
    LLMs 宣称可以处理的超长设置（100k+ tokens）。在本文中，我们介绍了 Ada-LEval，这是一种长度可调的基准，用于评估 LLMs 的长上下文理解能力。Ada-LEval
    包含两个具有挑战性的子集，TSort 和 BestAnswer，能够更可靠地评估 LLMs 的长上下文能力。这些基准支持对测试用例长度的复杂操控，并且可以轻松生成最长至
    128k tokens 的文本样本。我们使用 Ada-LEval 对 4 个最先进的封闭源 API 模型和 6 个开源模型进行了评估。评估结果展示了当前 LLMs
    的局限性，特别是在超长上下文设置下。我们的代码可以在 [https://github.com/open-compass/Ada-LEval](https://github.com/open-compass/Ada-LEval)
    获取。
- en: 'Ada-LEval: Evaluating long-context LLMs'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Ada-LEval：评估长文本 LLMs
- en: with length-adaptable benchmarks
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用长度可调的基准
- en: 'Chonghua Wang²^†^†thanks:   The work was done during an internship at Shanghai
    AI Laboratory; ^† Project Lead; ^‡ Corresponding Author. , Haodong Duan^(1†),
    Songyang Zhang¹, Dahua Lin¹, Kai Chen^(1‡) ¹Shanghai AI Laboratory ²Shanghai Jiao
    Tong University philipwang@sjtu.edu.cn duanhaodong@pjlab.org.cn'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Chonghua Wang²^†^†致谢：   工作是在上海人工智能实验室实习期间完成的；^† 项目负责人；^‡ 通讯作者。 , Haodong Duan^(1†),
    Songyang Zhang¹, Dahua Lin¹, Kai Chen^(1‡) ¹上海人工智能实验室 ²上海交通大学 philipwang@sjtu.edu.cn
    duanhaodong@pjlab.org.cn
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/f259e09eb633fc40af6852f4b48f6db9.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/f259e09eb633fc40af6852f4b48f6db9.png)'
- en: 'Figure 1: The demonstration of two tasks: TSort and BestAnswer introduced in
    Ada-LEval. Understanding and reasoning over the full text are required to solve
    these two tasks.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Ada-LEval 中介绍的两个任务：TSort 和 BestAnswer 的演示。解决这两个任务需要对全文进行理解和推理。
- en: Large Language Models (LLMs), typically based on large transformers trained
    on vast corpus, have shown exceptional abilities in memorization, comprehension,
    and reasoning (OpenAI, [2023](#bib.bib24); Touvron et al., [2023](#bib.bib32);
    Zheng et al., [2023](#bib.bib35)). A critical factor that affects LLM performance
    is the ‘context window’ - the number of tokens an LLM can process simultaneously.
    This window’s size is pivotal in handling lengthy texts. Since the debut of ChatGPT
    with a 2,000-token window in November 2022, significant efforts have been made
    in this domain, including more efficient attention mechanisms (Dao et al., [2022a](#bib.bib8);
    Zaheer et al., [2020](#bib.bib33); Ding et al., [2023](#bib.bib12)), scalable
    position embeddings (Su et al., [2021](#bib.bib28); Sun et al., [2022](#bib.bib30)),
    and quantization techniques (Frantar et al., [2022](#bib.bib13); Dettmers et al.,
    [2022](#bib.bib11)). As of December 2023, several LLMs claim to achieve context
    windows up to hundreds of thousands of tokens. This includes both proprietary
    models like GPT-4 Turbo (128,000 tokens), Claude-2.1 (200,000 tokens), and Moonshot
    AI (200,000 Chinese characters), and open-source models such as ChatGLM-32k (Zeng
    et al., [2022](#bib.bib34)) and LongChat-32k (Li* et al., [2023](#bib.bib20)).
    This expansion significantly enhances the potential for processing extensive documents.
    Nevertheless, the effectiveness of these long-context LLMs in managing long texts
    remains an area ripe for exploration and assessment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），通常基于在庞大语料库上训练的大型变换器，已展示出在记忆、理解和推理方面的卓越能力（OpenAI，[2023](#bib.bib24)；Touvron
    等，[2023](#bib.bib32)；Zheng 等，[2023](#bib.bib35)）。一个影响LLM性能的关键因素是“上下文窗口”——LLM能够同时处理的令牌数量。这个窗口的大小在处理长文本时至关重要。自2022年11月ChatGPT以2,000令牌窗口首发以来，在这个领域已付出了大量努力，包括更高效的注意力机制（Dao
    等，[2022a](#bib.bib8)；Zaheer 等，[2020](#bib.bib33)；Ding 等，[2023](#bib.bib12)）、可扩展的位置嵌入（Su
    等，[2021](#bib.bib28)；Sun 等，[2022](#bib.bib30)）以及量化技术（Frantar 等，[2022](#bib.bib13)；Dettmers
    等，[2022](#bib.bib11)）。截至2023年12月，多个LLM声称实现了高达数十万令牌的上下文窗口。这包括GPT-4 Turbo（128,000令牌）、Claude-2.1（200,000令牌）和Moonshot
    AI（200,000中文字符）等专有模型，以及ChatGLM-32k（Zeng 等，[2022](#bib.bib34)）和LongChat-32k（Li*
    等，[2023](#bib.bib20)）等开源模型。这一扩展显著提高了处理大规模文档的潜力。然而，这些长上下文LLM在管理长文本方面的有效性仍是一个待深入探讨和评估的领域。
- en: Alongside the evolution of LLMs, a wide range of benchmarks have emerged for
    capability assessment (Hendrycks et al., [2020](#bib.bib15); Suzgun et al., [2022](#bib.bib31);
    Cobbe et al., [2021](#bib.bib6); Huang et al., [2023](#bib.bib17)). Most of those
    benchmarks utilize short questions or instructions, making them unsuitable for
    evaluating LLMs’ long-context capabilities. While a few benchmarks do focus on
    assessing specific long-context abilities like summarization, question-answering
    (QA), and continue writing (Huang et al., [2021](#bib.bib16); Liu et al., [2023b](#bib.bib22);
    Dasigi et al., [2021](#bib.bib10)), comprehensive long-document evaluations have
    been limited. Recent benchmarks such as SCROLLS (Shaham et al., [2022](#bib.bib26)),
    L-Eval (An et al., [2023](#bib.bib1)) and LongBench (Bai et al., [2023](#bib.bib2))
    have started to address this gap by including a suite of long-document tasks,
    aiming for a more holistic assessment of LLMs’ long-context understanding.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的演进，出现了各种评估能力的基准（Hendrycks 等，[2020](#bib.bib15)；Suzgun 等，[2022](#bib.bib31)；Cobbe
    等，[2021](#bib.bib6)；Huang 等，[2023](#bib.bib17)）。这些基准大多数使用简短的问题或指令，使其不适用于评估LLM的长上下文能力。虽然少数基准确实专注于评估特定的长上下文能力，如总结、问答（QA）和续写（Huang
    等，[2021](#bib.bib16)；Liu 等，[2023b](#bib.bib22)；Dasigi 等，[2021](#bib.bib10)），但综合的长文档评估仍然有限。最近的基准如SCROLLS（Shaham
    等，[2022](#bib.bib26)）、L-Eval（An 等，[2023](#bib.bib1)）和LongBench（Bai 等，[2023](#bib.bib2)）开始通过包括一系列长文档任务来填补这一空白，旨在对LLM的长上下文理解进行更全面的评估。
- en: 'Despite these advancements, three significant limitations persist in existing
    benchmarks: Firstly, the ultra-long setting (32,000 tokens or longer) is scarcely
    represented, limiting insights into LLM performance in extreme context lengths.
    Secondly, the integration of test samples of varying lengths within these benchmarks
    complicates the evaluation of LLMs across different length ranges. Lastly, the
    focus on traditional tasks such as question-answering and summarization often
    does not necessitate comprehensive content understanding by the LLMs, as many
    questions in these tasks do not require full-text comprehension. This highlights
    the need for more targeted benchmarks that can rigorously evaluate the deep and
    complete understanding of long-form content by LLMs.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些进展，但现有基准中仍存在三个显著的限制：首先，超长设置（32,000个标记或更长）很少被代表，这限制了对LLMs在极端上下文长度下表现的洞察。其次，这些基准中集成了不同长度的测试样本，复杂化了LLMs在不同长度范围内的评估。最后，传统任务如问答和总结往往不要求LLMs进行全面的内容理解，因为这些任务中的许多问题并不需要全文理解。这突显了需要更有针对性的基准，以严格评估LLMs对长篇内容的深度和全面理解。
- en: 'To this end, we introduce Ada-LEval, a pioneering benchmark to assess the long-context
    capabilities with length-adaptable questions. Ada-LEval comprises two challenging
    tasks: TSort, which involves arranging text segments in the correct order, and
    BestAnswer, which requires choosing the best answer of a question among multiple
    candidates. Both tasks feature the following advantages: 1\. Controllable Test
    Cases: The length of each test case can be finely tuned - by adjusting the number
    and length of text segments in TSort and altering the number of distractor options
    in BestAnswer. 2\. Necessity for Full-Text Comprehension: Successful completion
    of both tasks mandates complete reading and understanding of the provided text.
    3\. Precise Accuracy Measurement: The design of these tasks allows for unambiguous
    accuracy calculation. TSort has a definitive ‘correct’ order, whereas in BestAnswer,
    the annotated responses by the questioner serve as definitive answers.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们推出了Ada-LEval，一个开创性的基准，用于评估具有长度适应性问题的长上下文能力。Ada-LEval包含两个具有挑战性的任务：TSort，它涉及将文本段落按正确顺序排列，以及BestAnswer，它要求从多个候选答案中选择最佳答案。这两个任务具有以下优点：1\.
    可控的测试用例：每个测试用例的长度可以精细调整——通过调整TSort中的文本段数量和长度以及改变BestAnswer中的干扰选项数量。2\. 完整文本理解的必要性：成功完成这两个任务要求对提供的文本进行全面阅读和理解。3\.
    精确的准确度测量：这些任务的设计允许明确的准确度计算。TSort有一个确定的“正确”顺序，而在BestAnswer中，由提问者标注的答案作为确定性答案。
- en: Our experiments on these tasks reveal critical insights. We observe a noteworthy
    decline in the performance of existing LLMs as text length increases, particularly
    in ultra-long scenarios. Furthermore, our ablation study uncovers several shortcomings
    in current LLMs, including limited instruction following over extended texts and
    pronounced input order bias. Additionally, we explore various scalable position
    embedding techniques aimed at enlarging the context window of LLMs. Our findings
    indicate that models equipped with those techniques show improved performance
    over the standard models, and the performance is comparable to their counterparts
    trained on longer contexts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这些任务的实验揭示了关键的见解。我们观察到现有的LLMs在文本长度增加时，特别是在超长文本场景中，表现显著下降。此外，我们的消融研究揭示了当前LLMs的几个缺点，包括在处理长文本时指令跟随的局限性和明显的输入顺序偏差。此外，我们探索了各种可扩展的位置嵌入技术，旨在扩大LLMs的上下文窗口。我们的发现表明，配备这些技术的模型在性能上优于标准模型，其表现与在更长上下文上训练的模型相当。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Long-Context Techniques
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 长上下文技术
- en: 'To address the complexities introduced by the increased text length in language
    models, researchers have developed a range of innovative techniques. These methodologies
    primarily focus on the following key areas: more efficient attention mechanisms,
    divide-and-conquer paradigms, and scalable position embedding techniques.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决语言模型中文本长度增加带来的复杂性，研究人员开发了一系列创新技术。这些方法主要集中在以下关键领域：更高效的注意力机制、分而治之的范式和可扩展的位置嵌入技术。
- en: Efficient Attention Mechanisms. Notable advancements in attention mechanisms
    within Transformers have been achieved by several studies (Zaheer et al., [2020](#bib.bib33);
    Guo et al., [2021](#bib.bib14); Dao et al., [2022b](#bib.bib9); Ding et al., [2023](#bib.bib12)).
    A key development in this area is Flash Attention (Dao et al., [2022a](#bib.bib8)),
    which streamlines the attention process by circumventing the need to read and
    write the attention matrix across different memory tiers. This approach results
    in faster processing and reduced memory usage compared to traditional attention
    methods. In LongNet, Ding et al. ([2023](#bib.bib12)) introduces Dilated Attention,
    which reduces the computation complexity of attention to nearly linear and scales
    to 1 billion tokens. However, Liu et al. ([2023a](#bib.bib21)) identified a limitation
    where these mechanisms tend to falter with the middle portions of long texts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 高效注意力机制。Transformer中的注意力机制有了显著进展，几项研究都取得了重要突破 (Zaheer et al., [2020](#bib.bib33);
    Guo et al., [2021](#bib.bib14); Dao et al., [2022b](#bib.bib9); Ding et al., [2023](#bib.bib12))。该领域的一个关键发展是Flash
    Attention (Dao et al., [2022a](#bib.bib8))，通过绕过在不同内存层级中读取和写入注意力矩阵的需要，简化了注意力过程。这种方法相比传统注意力方法，处理速度更快，内存使用更少。在LongNet中，Ding
    et al. ([2023](#bib.bib12)) 引入了膨胀注意力，将注意力的计算复杂度降低到接近线性，并扩展到10亿个标记。然而，Liu et al.
    ([2023a](#bib.bib21)) 发现这些机制在长文本的中间部分容易出现问题。
- en: Divide-and-Conquer. In exploring alternatives to conventional long-text modeling,
    several studies have adopted a segmented approach to manage extensive content.
    WebGPT (Nakano et al., [2021](#bib.bib23)) addresses long-form QA by interacting
    with a text-based web-browsing environment. PEARL (Sun et al., [2023](#bib.bib29))
    introduces a framework that prompts LLMs to generate and execute plans for tackling
    complex long-text reasoning tasks. Chen et al. ([2023a](#bib.bib4)) constructs
    a memory tree with the summarization of document segments and navigates on the
    memory tree to answer the original question.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 分而治之。在探索传统长文本建模的替代方案时，几项研究采用了分段方法来管理大量内容。WebGPT (Nakano et al., [2021](#bib.bib23))
    通过与基于文本的网页浏览环境交互来解决长篇问答问题。PEARL (Sun et al., [2023](#bib.bib29)) 引入了一个框架，促使LLMs生成并执行应对复杂长文本推理任务的计划。Chen
    et al. ([2023a](#bib.bib4)) 构建了一个包含文档片段摘要的记忆树，并在记忆树上导航以回答原始问题。
- en: Scalable Position Embeddings. Scalable position embeddings have been instrumental
    in extending the context window of LLMs. RoPE (Su et al., [2021](#bib.bib28))
    utilizes a rotation matrix to enhance positional information, integrating explicit
    relative position dependencies into the self-attention mechanism. ALiBi (Press
    et al., [2021](#bib.bib25)) does not add position embeddings to word embeddings,
    instead applying a linearly decreasing penalty to attention scores based on key-query
    distances. Position Interpolation (Chen et al., [2023b](#bib.bib5)) adopts a different
    strategy by linearly scaling down input position indices to align with preset
    context window sizes, requiring few fine-tuning steps. NTK-aware Scaled RoPE¹¹1[https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
    and ReRoPE (Su, [2023](#bib.bib27)) further combine the benefits of position interpolation
    and length extrapolation methods without any fine-tuning steps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展位置嵌入。可扩展位置嵌入在扩展LLMs的上下文窗口中发挥了重要作用。RoPE (Su et al., [2021](#bib.bib28)) 利用旋转矩阵增强位置编码，将显式的相对位置依赖集成到自注意力机制中。ALiBi
    (Press et al., [2021](#bib.bib25)) 不在词嵌入中添加位置嵌入，而是根据键-查询距离对注意力分数应用线性递减惩罚。位置插值
    (Chen et al., [2023b](#bib.bib5)) 采用了不同的策略，通过线性缩放输入位置索引以与预设的上下文窗口大小对齐，所需的微调步骤较少。NTK-aware
    Scaled RoPE¹¹1[https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/)
    和 ReRoPE (Su, [2023](#bib.bib27)) 进一步结合了位置插值和长度外推方法的优点，无需任何微调步骤。
- en: 2.2 Long-Context Language Models
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 长文本语言模型
- en: Building on advancements in long-context techniques, several long-context LLMs
    are developed and released. Llama 2 (Touvron et al., [2023](#bib.bib32)) integrates
    RoPE to expand its context window to 4,000 tokens. Vicuna-v1.5 (Zheng et al.,
    [2023](#bib.bib35)) further extends this capability by fine-tuning Llama 2 on
    high-quality, extensive conversations, successfully increasing the context window
    to 16,000 tokens. Longchat (Li* et al., [2023](#bib.bib20)) models condense RoPE
    to utilize model weights learned in the pretraining stage. ChatGLM2-32k (Zeng
    et al., [2022](#bib.bib34)) is trained on a 32,000-token context length using
    position interpolation, showcasing the scalability of this technique.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在长上下文技术的进步基础上，开发并发布了几种长上下文LLMs。Llama 2 (Touvron et al., [2023](#bib.bib32))
    集成了RoPE，将其上下文窗口扩展到4,000个标记。Vicuna-v1.5 (Zheng et al., [2023](#bib.bib35)) 通过在高质量、大量对话上微调Llama
    2，进一步扩展了这一能力，成功将上下文窗口增加到16,000个标记。Longchat (Li* et al., [2023](#bib.bib20)) 模型压缩了RoPE，以利用在预训练阶段学习到的模型权重。ChatGLM2-32k
    (Zeng et al., [2022](#bib.bib34)) 通过位置插值在32,000个标记的上下文长度上进行训练，展示了该技术的可扩展性。
- en: The domain of proprietary language models has seen even more significant advancements
    in long-context modeling, stepped into the ultra-long context field. GPT-4-Turbo OpenAI
    ([2023](#bib.bib24)) notably extends its context window to an impressive 128,000
    tokens. In a similar vein, Claude-2 and Claude-2.1 have achieved context lengths
    of 100,000 and 200,000 tokens respectively. This expansion allows them to process
    vast quantities of information, such as hundreds of pages of technical documentation
    or entire books. Kimi Chat, developed by Moonshot.ai, claims to handle up to 200,000
    Chinese characters. However, no existing dataset can evaluate the capability in
    tackling such long texts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 专有语言模型领域在长上下文建模方面取得了更显著的进展，踏入了超长上下文领域。GPT-4-Turbo OpenAI ([2023](#bib.bib24))
    显著扩展了其上下文窗口，达到令人印象深刻的128,000个标记。类似地，Claude-2和Claude-2.1分别实现了100,000和200,000个标记的上下文长度。这一扩展使得它们能够处理大量信息，例如数百页的技术文档或整本书籍。由Moonshot.ai开发的Kimi
    Chat声称可以处理多达200,000个汉字。然而，目前没有现有的数据集可以评估应对如此长文本的能力。
- en: 2.3 Long-Context Benchmarks
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 长上下文基准
- en: Efforts to evaluate the long-context capabilities of language models have been
    intensifying, with a focus primarily on traditional question-answering (QA) and
    summarization tasks. NarrativeQA (Kočiskỳ et al., [2018](#bib.bib18)) offers a
    question-answering dataset built on the entire books from Project Gutenberg and
    movie transcripts. GovReport (Huang et al., [2021](#bib.bib16)) provides a dataset
    comprising national policy issues, each accompanied by an expert-written summary,
    thus testing models’ ability to distill complex, lengthy documents into concise
    summaries. Based on existing long-context benchmarks, SCROLLS(Shaham et al., [2022](#bib.bib26))
    introduces a suite of datasets that requires models to process and reason over
    long contexts.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对语言模型长上下文能力的评估工作正在加剧，主要集中在传统的问答（QA）和总结任务上。NarrativeQA (Kočiskỳ et al., [2018](#bib.bib18))
    提供了一个基于Project Gutenberg的完整书籍和电影剧本的问答数据集。GovReport (Huang et al., [2021](#bib.bib16))
    提供了一个包含国家政策问题的数据集，每个问题都有专家撰写的总结，从而测试模型将复杂、冗长的文档提炼成简洁总结的能力。基于现有的长上下文基准，SCROLLS
    (Shaham et al., [2022](#bib.bib26)) 引入了一套需要模型处理和推理长上下文的数据集。
- en: Concurrently, L-Eval (An et al., [2023](#bib.bib1)) and LongBench (Bai et al.,
    [2023](#bib.bib2)) are designed for comprehensive evaluation of long-context capabilities
    of LLMs. L-Eval offers a collection of long documents across different domains
    and provides both close-ended and open-ended tasks. LongBench is a bilingual long
    context benchmark covering six task categories. Most tasks in these benchmarks
    are traditional QA and summarization with fixed document, questions and answers.
    They are inflexible on text length (up to $\sim$32,000 tokens), which fall short
    of adapting to ultra-long context evaluation. Additionally, LongBench uses mostly
    open-ended tasks with traditional F1 and ROUGE metric that may not align well
    with human judgments. In contrast, our benchmarks support length-adaptable evaluation,
    provide sufficient cases and evaluate models using accuracy metrics, avoiding
    inconsistencies with human evaluation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，L-Eval（An等，[2023](#bib.bib1)）和LongBench（Bai等，[2023](#bib.bib2)）被设计用于对LLMs的长上下文能力进行全面评估。L-Eval提供了来自不同领域的长文档集合，并提供了封闭式和开放式任务。LongBench是一个双语长上下文基准，涵盖六个任务类别。这些基准中的大多数任务是传统的问答和总结，具有固定的文档、问题和答案。它们在文本长度上不够灵活（最长约32,000标记），无法适应超长上下文评估。此外，LongBench大多使用开放式任务，并采用传统的F1和ROUGE指标，这些指标可能与人工判断不完全一致。相比之下，我们的基准支持长度适应性评估，提供了足够的案例，并使用准确性指标评估模型，避免了与人工评估的不一致。
- en: 3 Ada-LEval
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Ada-LEval
- en: 'In this section, we outline the construction process of Ada-LEval, detailing
    both the collection methodology of our source data and the building procedure
    of our test cases. [Table 1](#S3.T1 "In 3 Ada-LEval ‣ Ada-LEval: Evaluating long-context
    LLMs with length-adaptable benchmarks") demonstrates the data statistics of Ada-LEval.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们概述了Ada-LEval的构建过程，详细描述了我们的源数据收集方法和测试案例构建程序。[表1](#S3.T1 "在3 Ada-LEval
    ‣ Ada-LEval: 评估具有长度适应性基准的长上下文LLMs")展示了Ada-LEval的数据统计信息。'
- en: '| TSort |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| TSort |'
- en: '| --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Setting | Total #Cases Built | Max #Tokens | Avg #Tokens |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 总案例数 | 最大标记数 | 平均标记数 |'
- en: '| 2k | 5123 | 2000 | 1816 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2k | 5123 | 2000 | 1816 |'
- en: '| 4k | 5451 | 4000 | 3724 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 4k | 5451 | 4000 | 3724 |'
- en: '| 8k | 5324 | 8000 | 7663 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 8k | 5324 | 8000 | 7663 |'
- en: '| 16k | 4957 | 16000 | 15662 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 16k | 4957 | 16000 | 15662 |'
- en: '| 32k | 2206 | 32000 | 31226 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 32k | 2206 | 32000 | 31226 |'
- en: '| 64k | 1658 | 64000 | 62407 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 64k | 1658 | 64000 | 62407 |'
- en: '| 128k | 782 | 127800 | 121488 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 128k | 782 | 127800 | 121488 |'
- en: '| BestAnswer |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| BestAnswer |'
- en: '| Setting | Total #Cases Built | Max #Tokens | Avg #Tokens |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 总案例数 | 最大标记数 | 平均标记数 |'
- en: '| 1k | 7526 | 1128 | 955 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1k | 7526 | 1128 | 955 |'
- en: '| 2k | 7526 | 2154 | 1983 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 2k | 7526 | 2154 | 1983 |'
- en: '| 4k | 7526 | 4215 | 3994 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 4k | 7526 | 4215 | 3994 |'
- en: '| 6k | 7526 | 6268 | 6012 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 6k | 7526 | 6268 | 6012 |'
- en: '| 8k | 7526 | 7790 | 7518 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 8k | 7526 | 7790 | 7518 |'
- en: '| 12k | 7526 | 12389 | 12091 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 12k | 7526 | 12389 | 12091 |'
- en: '| 16k | 7526 | 15964 | 15646 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 16k | 7526 | 15964 | 15646 |'
- en: '| 32k | 200 | 32974 | 32329 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 32k | 200 | 32974 | 32329 |'
- en: '| 64k | 200 | 64216 | 63274 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 64k | 200 | 64216 | 63274 |'
- en: '| 128k | 200 | 127059 | 126098 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 128k | 200 | 127059 | 126098 |'
- en: 'Table 1: The data statistics of TSort and BestAnswer. We adopt the GPT-4 tokenizer
    CL100K to calculate token numbers. We use a subset of all built cases for evaluation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：TSort和BestAnswer的数据统计。我们采用了GPT-4分词器CL100K来计算标记数。我们使用了所有构建案例的一个子集进行评估。
- en: 3.1 Task Definition
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务定义
- en: TSort. TSort provides LLMs with $\mathbf{N}$ shuffled text segments, extracted
    from contiguous chapters of a long book. The task for models is to sort these
    segments into their original sequence. A response is regarded accurate only if
    it precisely reinstates the segments’ initial order. To simplify the challenge
    and minimize possible confusion, we supply LLMs with adjacent paragraphs from
    before and after the specified chapters to serve as contextual hints.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TSort。TSort向LLMs提供了$\mathbf{N}$个混排的文本段，这些段从一本长书的连续章节中提取。模型的任务是将这些段按其原始顺序排序。只有当它精确恢复了段的初始顺序时，响应才被视为准确。为了简化挑战并减少可能的混淆，我们向LLMs提供了指定章节之前和之后的相邻段落，以作为上下文提示。
- en: BestAnswer. Each test case in BestAnswer contains one question and a large amount
    of possible answers to this question. We consider the answer designated by the
    original inquirer as the most helpful answer, while LLMs are required to identify
    this optimal answer among all possible candidates.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: BestAnswer。BestAnswer中的每个测试案例包含一个问题和大量可能的答案。我们认为原始询问者指定的答案是最有帮助的答案，而LLMs需要从所有可能的候选答案中识别出这个**最佳答案**。
- en: 3.2 Source Data Collection
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 源数据收集
- en: TSort. For TSort, we sourced our initial data from Booksum (Kryściński et al.,
    [2021](#bib.bib19)), a text summarization dataset derived from the Project Gutenberg,
    a public book repository consisting of over 60,000 free eBooks spanning various
    literary genres including novels, plays, short stories, and more. Genres like
    epistolary literature and poetry are excluded in the construction of TSort benchmark
    due to their non-sequential nature. To prevent LLMs from exploiting superficial
    cues, we meticulously remove identifiers such as chapter numbers and annotations
    from the content.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TSort。对于TSort，我们从 Booksum (Kryściński et al., [2021](#bib.bib19)) 获取了初始数据，这是一组来自
    Project Gutenberg 的文本摘要数据集。Project Gutenberg 是一个公共书籍资源库，包含超过 60,000 本免费的电子书，涵盖各种文学体裁，包括小说、戏剧、短篇故事等。由于书信体文学和诗歌等体裁的非顺序性，这些体裁在
    TSort 基准的构建中被排除。为了防止 LLM 利用表面线索，我们细致地去除了内容中的章节编号和注释等标识符。
- en: BestAnswer. The BestAnswer benchmark is constructed using threads from Stack
    Overflow, a platform renowned for its extensive range of programming-related questions
    and answers. Stack Overflow questions are categorized by multiple tags, indicating
    the thematic similarity of questions within each tag. To ensure the quality and
    diversity of our benchmark, we choose 23 different tags, including javascript,
    python, C++, *etc.*, and collect top 2500 questions from each tag based on popularity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: BestAnswer。BestAnswer 基准使用来自 Stack Overflow 的线程构建，Stack Overflow 是一个以其广泛的编程相关问题和答案而闻名的平台。Stack
    Overflow 问题按多个标签分类，表明每个标签内问题的主题相似性。为了确保我们基准的质量和多样性，我们选择了 23 个不同的标签，包括 javascript、python、C++、*等*，并从每个标签中根据受欢迎程度收集了前
    2500 个问题。
- en: 3.3 Test Case Building
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 测试用例构建
- en: For both tasks, we construct test cases according to their token length (measured
    by GPT-4 tokenizer). We regard token lengths between 1,000 to 16,000 as long-context
    settings and text lengths exceed 16,000 as ultra-long-context settings.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个任务，我们根据其标记长度（由 GPT-4 分词器测量）构建测试用例。我们将标记长度在 1,000 到 16,000 之间的情况视为长上下文设置，将文本长度超过
    16,000 的情况视为超长上下文设置。
- en: Under long-context settings, TSort cases span test cases with 2k, 4k, 8k, and
    16k tokens. For each length, we fix the segment number N=4 and the length upper
    limit for each text segment and adjacent paragraphs before and after these contiguous
    chapters. We ensure that each text segment contains complete paragraphs thus no
    paragraph is sliced in the middle. To build test cases with different contents,
    we set stride between beginning paragraphs of test cases during construction.
    After prepending the instructions, we further filter test cases that exceed the
    token upper bound.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在长上下文设置下，TSort 用例跨越了 2k、4k、8k 和 16k 标记的测试用例。对于每个长度，我们固定段落数量 N=4，以及每个文本段和这些连续章节前后相邻段落的长度上限。我们确保每个文本段包含完整的段落，从而没有段落被切割。为了构建具有不同内容的测试用例，我们在构建过程中设置了测试用例开头段落之间的步幅。在添加指令后，我们进一步筛选出超出标记上限的测试用例。
- en: For BestAnswer, we generate test cases with 1k, 2k, 4k, 6k, 8k, 12k, and 16k
    tokens under long-context settings. Test cases contain the distractor answers
    under corresponding question and adaptable number of distractor answers from other
    similar questions under each length setting. To make evaluation results directly
    comparable across different length settings in long context scenarios, we ensure
    that the questions within the BestAnswer benchmark remain unchanged, regardless
    of the case length. In BestAnswer, we define the most helpful answer as the answer
    explicitly accepted by the inquirer, and adopt it as the ‘groundtruth answer’²²2
    We do not choose the answer with the highest number of votes, since the vote number
    can be influenced by factors such as the answer posting time and the identity
    of the respondent, in addition to its quality.. For integrity reasons, we exclude
    all questions where the corresponding most helpful answer is not text-only. When
    choosing the distractors, we only consider answers that are provided prior to
    the accepted answer under corresponding question. Besides, we incorporate answers
    from other questions with similar tags to the original question to serve as distractor
    answers.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BestAnswer，我们在长上下文设置下生成了1k、2k、4k、6k、8k、12k和16k tokens的测试用例。测试用例包含相应问题下的干扰答案和在每种长度设置下来自其他相似问题的可适应干扰答案的数量。为了使长上下文场景下不同长度设置的评估结果可以直接比较，我们确保BestAnswer基准中的问题保持不变，无论用例长度如何。在BestAnswer中，我们将最有帮助的答案定义为询问者明确接受的答案，并将其作为“真实答案”²²2
    我们不选择票数最多的答案，因为票数可能受到答案发布时机和回答者身份等因素的影响，除了质量之外。为了完整性，我们排除所有对应最有帮助答案不是仅文本的所有问题。在选择干扰答案时，我们只考虑在对应问题下接受答案之前提供的答案。此外，我们还将其他相似标签的问题的答案纳入干扰答案。
- en: Under ultra-long-context settings, we build test cases with 32k, 64k, and 128k
    tokens for both tasks. The construction paradigm is similar to the long-context
    setting. For BestAnswer, since the number of similar questions and the corresponding
    answers are limited, we relax tag similarity constraints and allow answers of
    questions with less similar tags to serve as the distractor answers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在超长上下文设置下，我们为两项任务构建了32k、64k和128k tokens的测试用例。构建范式类似于长上下文设置。对于BestAnswer，由于相似问题及其对应答案的数量有限，我们放宽了标签相似性约束，允许标签相似度较低的问题的答案作为干扰答案。
- en: 4 Evaluation Results
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估结果
- en: 4.1 Experiment Setup
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'We evaluate the following LLMs under long-context settings: 4 proprietary models:
    (1) GPT-4-Turbo-0125, (2) GPT-4-Turbo-1106 (3) GPT-3.5-Turbo-1106, (4) Claude-2;
    and 6 open-source models: (5) LongChat-7b-v1.5-32k(Zheng et al., [2023](#bib.bib35)),
    (6) ChatGLM2-6B-32k(Zeng et al., [2022](#bib.bib34)), (7) ChatGLM3-6B-32k(Zeng
    et al., [2022](#bib.bib34)), (8) Vicuna-7b-v1.5-16k(Zheng et al., [2023](#bib.bib35)),
    (9) Vicuna-13b-v1.5-16k(Zheng et al., [2023](#bib.bib35)), (10) InternLM2-7b(Cai
    et al., [2024](#bib.bib3)). Due to the inferior performance of open-source LLMs
    under long-context settings, only models with good performance (GPT-4-Turbo, Claude-2,
    *etc.*) are evaluated under ultra-long-context settings.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在长上下文设置下评估了以下LLM：4个专有模型：（1）GPT-4-Turbo-0125，（2）GPT-4-Turbo-1106，（3）GPT-3.5-Turbo-1106，（4）Claude-2；以及6个开源模型：（5）LongChat-7b-v1.5-32k（Zheng
    et al., [2023](#bib.bib35)），（6）ChatGLM2-6B-32k（Zeng et al., [2022](#bib.bib34)），（7）ChatGLM3-6B-32k（Zeng
    et al., [2022](#bib.bib34)），（8）Vicuna-7b-v1.5-16k（Zheng et al., [2023](#bib.bib35)），（9）Vicuna-13b-v1.5-16k（Zheng
    et al., [2023](#bib.bib35)），（10）InternLM2-7b（Cai et al., [2024](#bib.bib3)）。由于开源LLM在长上下文设置下表现不佳，仅评估了在超长上下文设置下表现良好的模型（GPT-4-Turbo、Claude-2、*等*）。
- en: For open-source LLMs, we sample a 1000-testcase subset for evaluation under
    each length setting. Due to the costly API of state-of-the-art proprietary models
    (GPT-4-Turbo, Claude-2, *etc.*), we adopt 200-testcase subset (sampled from the
    1000-testcase set) for evaluation under long-context settings, and a 50-testcase
    subset for evaluation under ultra-long-context settings. All experiments are conducted
    using the open-source LLM evaluation platform OpenCompass  (Contributors, [2023](#bib.bib7)).
    We adopt the zero-shot setting for all evaluation, and provide a ‘random guess’
    baseline. We also measure the instruction following rate and the copy instruction
    rate³³3Instruction following rate denotes if the LLM outputs follow the pre-defined
    format. Copy instruction rate measures if the LLM outputs the same answer as in-context
    example provides. on both tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开源LLM，我们在每个长度设置下抽取了1000个测试用例进行评估。由于先进的专有模型（GPT-4-Turbo、Claude-2、*等*）的API费用高昂，我们在长上下文设置下采用了200个测试用例子集（从1000个测试用例集中抽样），在超长上下文设置下采用了50个测试用例子集。所有实验都使用开源LLM评估平台OpenCompass进行（贡献者，[2023](#bib.bib7)）。我们对所有评估采用了零-shot设置，并提供了“随机猜测”基准。我们还在两个任务上测量了指令遵循率和复制指令率³³指令遵循率表示LLM输出是否遵循预定义格式。复制指令率衡量LLM输出是否与上下文示例提供的答案相同。
- en: 4.2 Long-Context Evaluation Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 长上下文评估结果
- en: '| TSort | 2k | 4k | 8k | 16k |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| TSort | 2k | 4k | 8k | 16k |'
- en: '| GPT-4-Turbo-0125 | 15.5 | 16.5 | 8.5 | 5.5 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-0125 | 15.5 | 16.5 | 8.5 | 5.5 |'
- en: '| GPT-4-Turbo-1106 | 18.5 | 15.5 | 7.5 | 3.5 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 18.5 | 15.5 | 7.5 | 3.5 |'
- en: '| GPT-3.5-Turbo-1106 | 4.0 | 4.5 | 4.5 | 5.5 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-1106 | 4.0 | 4.5 | 4.5 | 5.5 |'
- en: '| Claude-2 | 5.0 | 5.0 | 4.5 | 3.0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 5.0 | 5.0 | 4.5 | 3.0 |'
- en: '| LongChat-7b-v1.5-32k | 5.3 | 5.0 | 3.1 | 2.5 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 5.3 | 5.0 | 3.1 | 2.5 |'
- en: '| ChatGLM2-6B-32k | 0.9 | 0.7 | 0.2 | 0.9 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 0.9 | 0.7 | 0.2 | 0.9 |'
- en: '| ChatGLM3-6B-32k | 2.3 | 2.4 | 2.0 | 0.7 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 2.3 | 2.4 | 2.0 | 0.7 |'
- en: '| Vicuna-7b-v1.5-16k | 5.3 | 2.2 | 2.3 | 1.7 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 5.3 | 2.2 | 2.3 | 1.7 |'
- en: '| Vicuna-13b-v1.5-16k | 5.4 | 5.0 | 2.4 | 3.1 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 5.4 | 5.0 | 2.4 | 3.1 |'
- en: '| InternLM2-7b | 5.1 | 3.9 | 5.1 | 4.3 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7b | 5.1 | 3.9 | 5.1 | 4.3 |'
- en: '| Random Guess | 4.2 | 4.2 | 4.2 | 4.2 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 随机猜测 | 4.2 | 4.2 | 4.2 | 4.2 |'
- en: 'Table 2: TSort results under long-context settings. We fix the number of segments
    $\mathbf{N}=4$ for TSort evaluation, thus random guess accuracy is roughly 4.2%
    (1 / 24).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：长上下文设置下的TSort结果。我们固定TSort评估的分段数$\mathbf{N}=4$，因此随机猜测的准确率大约为4.2%（1 / 24）。
- en: 'TSort. [Table 2](#S4.T2 "In 4.2 Long-Context Evaluation Results ‣ 4 Evaluation
    Results ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")
    displays the test accuracy of various LLMs on the TSort task. This evaluation
    underscores the complexity of TSort, highlighting its intricate nature that necessitates
    a comprehensive understanding and reasoning across long text. Under settings from
    2,000 to 8,000 tokens, only the most powerful proprietary model GPT-4-Turbo outputs
    the correct order of texts with a significant higher probability compared to the
    random baseline. When the context window expands to 16,000, the quality of GPT-4-Turbo’s
    predictions also deteriorates to the random guess level. Other LLMs, encompassing
    both proprietary models and open-source models, all displaying similar performance
    compared to random guess (even under the relative short 2k setting). The results
    indicate that the TSort task posts a severe challenge to existing LLMs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: TSort。[表 2](#S4.T2 "在 4.2 长上下文评估结果 ‣ 4 评估结果 ‣ Ada-LEval：使用长度适配基准评估长上下文LLM")显示了各种LLM在TSort任务上的测试准确率。这次评估突出了TSort的复杂性，强调了其复杂的本质，需要对长文本进行全面理解和推理。在2,000到8,000个标记的设置下，只有最强大的专有模型GPT-4-Turbo以显著高于随机基准的概率输出正确的文本顺序。当上下文窗口扩展到16,000时，GPT-4-Turbo的预测质量也下降到随机猜测水平。其他LLM，包括专有模型和开源模型，在相对较短的2k设置下表现出类似于随机猜测的性能。这些结果表明，TSort任务对现有的LLM提出了严峻的挑战。
- en: '| BestAnswer | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| BestAnswer | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
- en: '| GPT-4-Turbo-0125 | 73.5 | 73.5 | 65.5 | 63.0 | 56.5 | 52.0 | 44.5 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-0125 | 73.5 | 73.5 | 65.5 | 63.0 | 56.5 | 52.0 | 44.5 |'
- en: '| GPT-4-Turbo-1106 | 74.0 | 73.5 | 67.5 | 59.5 | 53.5 | 49.5 | 44.0 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 74.0 | 73.5 | 67.5 | 59.5 | 53.5 | 49.5 | 44.0 |'
- en: '| GPT-3.5-Turbo-1106 | 61.5 | 48.5 | 41.5 | 29.5 | 17.0 | 2.5 | 2.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-1106 | 61.5 | 48.5 | 41.5 | 29.5 | 17.0 | 2.5 | 2.5 |'
- en: '| Claude-2 | 65.0 | 43.5 | 23.5 | 15.0 | 17.0 | 12.0 | 11.0 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 65.0 | 43.5 | 23.5 | 15.0 | 17.0 | 12.0 | 11.0 |'
- en: '| LongChat-7b-v1.5-32k | 32.4 | 10.7 | 5.7 | 3.1 | 1.9 | 1.6 | 0.8 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 32.4 | 10.7 | 5.7 | 3.1 | 1.9 | 1.6 | 0.8 |'
- en: '| ChatGLM2-6B-32k | 31.2 | 10.9 | 4.5 | 1.6 | 1.6 | 0.0 | 0.3 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 31.2 | 10.9 | 4.5 | 1.6 | 1.6 | 0.0 | 0.3 |'
- en: '| ChatGLM3-6B-32k | 39.8 | 18.8 | 9.0 | 5.0 | 3.4 | 0.9 | 0.5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 39.8 | 18.8 | 9.0 | 5.0 | 3.4 | 0.9 | 0.5 |'
- en: '| Vicuna-7b-v1.5-16k | 37.0 | 11.1 | 5.8 | 3.2 | 1.8 | 1.9 | 1.0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 37.0 | 11.1 | 5.8 | 3.2 | 1.8 | 1.9 | 1.0 |'
- en: '| Vicuna-13b-v1.5-16k | 53.4 | 29.2 | 13.1 | 4.3 | 2.2 | 1.4 | 0.9 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 53.4 | 29.2 | 13.1 | 4.3 | 2.2 | 1.4 | 0.9 |'
- en: '| InternLM2-7b | 58.6 | 49.5 | 33.9 | 12.3 | 13.4 | 2.0 | 0.8 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7b | 58.6 | 49.5 | 33.9 | 12.3 | 13.4 | 2.0 | 0.8 |'
- en: '| Random Guess | 26.7 | 10.1 | 4.5 | 3.0 | 2.3 | 1.4 | 1.1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 随机猜测 | 26.7 | 10.1 | 4.5 | 3.0 | 2.3 | 1.4 | 1.1 |'
- en: 'Table 3: BestAnswer results under long-context settings. For a question with
    $\mathbf{N}$. The random guess accuracy over a long-context setting is the average
    of random guess accuracy for all questions within the test set.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：长上下文设置下的 BestAnswer 结果。对于一个具有$\mathbf{N}$的问题。长上下文设置下的随机猜测准确率是测试集中所有问题的随机猜测准确率的平均值。
- en: 'BestAnswer. [Table 3](#S4.T3 "In 4.2 Long-Context Evaluation Results ‣ 4 Evaluation
    Results ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")
    presents the test accuracy of LLMs on BestAnswer. GPT-4-Turbo establishes the
    state-of-the-art on the BestAnswer benchmark. It achieves an outstanding 44.5%
    accuracy under the 16k long-context setting, where around 100 distractor answers
    exist for each question. Among other proprietary models, Claude-2 achieves the
    second best accuracy 11% under the 16k setting. GPT-3.5-Turbo-1106, while outperforming
    Claude-2 under some relative short settings (2k, 4k, 6k), demonstrates performance
    similar to random guess under the 16k setting. There is a considerable performance
    gap between proprietary models and open-source models on BestAnswer. Although
    some models like Vicuna-13b-v1.5-16k and InternLM2-7b perform well under short
    settings, a dramatic accuracy decline can be observed when text length becomes
    larger.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: BestAnswer. [表 3](#S4.T3 "在 4.2 长上下文评估结果 ‣ 4 评估结果 ‣ Ada-LEval：使用长度可调基准评估长上下文
    LLMs") 展示了 LLMs 在 BestAnswer 上的测试准确率。GPT-4-Turbo 在 BestAnswer 基准测试中确立了最先进的水平。在
    16k 长上下文设置下，它实现了卓越的 44.5% 准确率，在每个问题上大约有 100 个干扰答案。在其他专有模型中，Claude-2 在 16k 设置下取得了第二好的准确率
    11%。虽然 GPT-3.5-Turbo-1106 在一些相对较短的设置下（2k、4k、6k）表现优于 Claude-2，但在 16k 设置下其表现与随机猜测相似。专有模型与开源模型在
    BestAnswer 上存在显著的性能差距。尽管像 Vicuna-13b-v1.5-16k 和 InternLM2-7b 在短设置下表现良好，但当文本长度变大时，可以观察到准确率的显著下降。
- en: '| CopyInst Rate | 2k | 4k | 8k | 16k |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 复制指令率 | 2k | 4k | 8k | 16k |'
- en: '| GPT-4-Turbo-1106 | 25.0 | 22.0 | 10.5 | 1.0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 25.0 | 22.0 | 10.5 | 1.0 |'
- en: '| GPT-3.5-Turbo-1106 | 30.0 | 25.5 | 64.5 | 73.3 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-1106 | 30.0 | 25.5 | 64.5 | 73.3 |'
- en: '| Claude-2 | 99.5 | 95.0 | 97.4 | 96.9 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 99.5 | 95.0 | 97.4 | 96.9 |'
- en: '| Expectation | 5.0 | 5.0 | 5.0 | 5.5 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 期望 | 5.0 | 5.0 | 5.0 | 5.5 |'
- en: '| LongChat-7b-v1.5-32k | 100.0 | 99.8 | 99.1 | 100.0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 100.0 | 99.8 | 99.1 | 100.0 |'
- en: '| ChatGLM2-6B-32k | 11.3 | 13.8 | 10.5 | 81.3 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 11.3 | 13.8 | 10.5 | 81.3 |'
- en: '| ChatGLM3-6B-32k | 21.6 | 54.8 | 88.0 | 88.1 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 21.6 | 54.8 | 88.0 | 88.1 |'
- en: '| Vicuna-7b-v1.5-16k | 100.0 | 100.0 | 59.4 | 33.3 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 100.0 | 100.0 | 59.4 | 33.3 |'
- en: '| Vicuna-13b-v1.5-16k | 96.6 | 99.0 | 12.2 | 3.1 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 96.6 | 99.0 | 12.2 | 3.1 |'
- en: '| Expectation | 5.3 | 5.0 | 5.4 | 5.2 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 期望 | 5.3 | 5.0 | 5.4 | 5.2 |'
- en: 'Table 4: The copy instruction rate of LLMs on TSort under long-context settings.
    Expectation means the ratio of test cases for which the in-context example answer
    is exactly the correct one.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLMs 在长上下文设置下的复制指令率。期望值表示上下文示例答案完全正确的测试用例比例。
- en: '| CopyInst Rate | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 复制指令率 | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
- en: '| GPT-4-Turbo-1106 | 12.5 | 8.5 | 5.0 | 5.5 | 6.0 | 2.0 | 2.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 12.5 | 8.5 | 5.0 | 5.5 | 6.0 | 2.0 | 2.0 |'
- en: '| GPT-3.5-Turbo-1106 | 16.5 | 22.5 | 18.5 | 16.0 | 11.5 | 2.0 | 0.0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-1106 | 16.5 | 22.5 | 18.5 | 16.0 | 11.5 | 2.0 | 0.0 |'
- en: '| Claude-2 | 21.5 | 25.5 | 40.5 | 41.0 | 42.5 | 49.0 | 55.0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 21.5 | 25.5 | 40.5 | 41.0 | 42.5 | 49.0 | 55.0 |'
- en: '| Expectation | 13.0 | 7.0 | 3.0 | 2.0 | 2.5 | 1.5 | 1.5 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 期望 | 13.0 | 7.0 | 3.0 | 2.0 | 2.5 | 1.5 | 1.5 |'
- en: '| LongChat-7b-v1.5-32k | 67.4 | 94.7 | 89.5 | 57.8 | 70.6 | 49.4 | 13.0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 67.4 | 94.7 | 89.5 | 57.8 | 70.6 | 49.4 | 13.0 |'
- en: '| ChatGLM2-6B-32k | 36.5 | 43.7 | 35.8 | 27.2 | 24.4 | 35.5 | 44.7 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 36.5 | 43.7 | 35.8 | 27.2 | 24.4 | 35.5 | 44.7 |'
- en: '| ChatGLM3-6B-32k | 47.9 | 66.1 | 33.3 | 30.4 | 22.5 | 24.8 | 16.7 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 47.9 | 66.1 | 33.3 | 30.4 | 22.5 | 24.8 | 16.7 |'
- en: '| Vicuna-7b-v1.5-16k | 63.1 | 96.2 | 91.8 | 57.9 | 66.6 | 27.8 | 17.9 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 63.1 | 96.2 | 91.8 | 57.9 | 66.6 | 27.8 | 17.9 |'
- en: '| Vicuna-13b-v1.5-16k | 27.8 | 45.8 | 55.3 | 19.8 | 3.4 | 5.6 | 11.1 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 27.8 | 45.8 | 55.3 | 19.8 | 3.4 | 5.6 | 11.1 |'
- en: '| Expectation | 14.4 | 10.0 | 5.1 | 2.3 | 1.7 | 1.3 | 1.2 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 期望 | 14.4 | 10.0 | 5.1 | 2.3 | 1.7 | 1.3 | 1.2 |'
- en: 'Table 5: The copy instruction rate of LLMs on BestAnswer under long-context
    settings. Expectation means the ratio of test cases for which the in-context example
    answer is exactly the correct one.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLMs在BestAnswer上的复制指令率在长上下文设置下。期望值指的是上下文示例答案完全正确的测试案例比例。
- en: '![Refer to caption](img/631b3e436e4cfd2164fbcccb6f29d5ad.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/631b3e436e4cfd2164fbcccb6f29d5ad.png)'
- en: 'Figure 2: The instruction following rate of LLMs on TSort (Left) and BestAnswer
    (Right) under long-context settings. GPT-4-Turbo on TSort and all proprietary
    models on BestAnswer achieve 100% instruction following rate across all long-context
    settings, thus not displayed.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLMs在TSort（左）和BestAnswer（右）上的指令遵循率在长上下文设置下。GPT-4-Turbo在TSort上以及所有专有模型在BestAnswer上在所有长上下文设置下都实现了100%的指令遵循率，因此未显示。
- en: 4.3 Error Breakdown
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 错误分析
- en: 'We further analyze the error instances on TSort and BestAnswer, and find that
    most errors can be attributed to two categories: 1\. The LLM fails to follow the
    provided instruction and does not output a valid answer⁴⁴4A valid answer contains
    a permutation of N segment numbers on TSort and at least one designation of answers
    on BestAnswer.; 2\. The LLM does output a valid answer. However, it simply copies
    the example answer we provide in the in-context example. [Figure 2](#S4.F2 "In
    4.2 Long-Context Evaluation Results ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating
    long-context LLMs with length-adaptable benchmarks") display instruction following
    rate on TSort and BestAnswer. [Tables 4](#S4.T4 "In 4.2 Long-Context Evaluation
    Results ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") and [5](#S4.T5 "Table 5 ‣ 4.2 Long-Context Evaluation
    Results ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") provide detailed statistics about the copy instruction
    rate on TSort and BestAnswer.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步分析了TSort和BestAnswer上的错误实例，发现大多数错误可以归因于两类：1\. LLM未能遵循提供的指令，未输出有效答案⁴⁴4有效答案包含TSort上的N段数字的排列以及BestAnswer上的至少一个答案指定。;
    2\. LLM确实输出了有效答案。然而，它只是简单地复制了我们在上下文示例中提供的示例答案。[图2](#S4.F2 "在4.2长上下文评估结果 ‣ 4 评估结果
    ‣ Ada-LEval: 使用长度自适应基准评估长上下文LLMs")展示了TSort和BestAnswer上的指令遵循率。[表4](#S4.T4 "在4.2长上下文评估结果
    ‣ 4 评估结果 ‣ Ada-LEval: 使用长度自适应基准评估长上下文LLMs")和[5](#S4.T5 "表5 ‣ 4.2 长上下文评估结果 ‣ 4
    评估结果 ‣ Ada-LEval: 使用长度自适应基准评估长上下文LLMs")提供了关于TSort和BestAnswer上复制指令率的详细统计数据。'
- en: The state-of-the-art GPT-4-Turbo maintains a relatively low copy instruction
    rate and impeccable instruction following rate on both tasks. Error instances
    of Claude-2, LongChat and Vicuna models are predominantly due to elevated Copy
    Instruction Rate, while ChatGLM models suffer from low instruction following rate.
    It is worth noting that all models, with the sole exception of GPT-4-Turbo, find
    it more difficult to follow the instruction on both tasks as text length increases.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的GPT-4-Turbo在两个任务上都保持了相对较低的复制指令率和完美的指令遵循率。Claude-2、LongChat和Vicuna模型的错误实例主要由于较高的复制指令率，而ChatGLM模型则因指令遵循率低而受到影响。值得注意的是，所有模型，除GPT-4-Turbo外，都发现随着文本长度的增加，在两个任务上遵循指令变得更加困难。
- en: 4.4 Ultra-Long-Context Evaluation Results
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 超长上下文评估结果
- en: 'We evaluate the following proprietary models under ultra-long-context settings.
    (1) GPT-4-Turbo-0125 (2) GPT-4-Turbo-1106 (3) Claude-2\. (4) Claude-2.1\. We also
    evaluate InternLM2-7b on BestAnswer benchmark under ultra-long-context settings.
    Due to high API calling expense, we test 50 samples under each ultra-long context
    setting. Table [6](#S4.T6 "Table 6 ‣ 4.4 Ultra-Long-Context Evaluation Results
    ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable
    benchmarks") demonstrates the result.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在超长上下文设置下评估了以下专有模型。 (1) GPT-4-Turbo-0125 (2) GPT-4-Turbo-1106 (3) Claude-2\.
    (4) Claude-2.1\. 我们还在超长上下文设置下评估了InternLM2-7b的BestAnswer基准。由于高昂的API调用费用，我们在每个超长上下文设置下测试了50个样本。[表6](#S4.T6
    "表6 ‣ 4.4 超长上下文评估结果 ‣ 4 评估结果 ‣ Ada-LEval: 使用长度自适应基准评估长上下文LLMs")展示了结果。'
- en: '| Benchmark | Model | 32k | 64k | 128k |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 模型 | 32k | 64k | 128k |'
- en: '| TSort | GPT-4-Turbo-0125 | 2.0 | 4.0 | 2.0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| TSort | GPT-4-Turbo-0125 | 2.0 | 4.0 | 2.0 |'
- en: '| GPT-4-Turbo-1106 | 6.0 | 6.0 | 6.0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 6.0 | 6.0 | 6.0 |'
- en: '| Claude-2 | 0.0 | 0.0 | / |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 0.0 | 0.0 | / |'
- en: '| Claude-2.1 | 0.0 | 0.0 | 0.0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2.1 | 0.0 | 0.0 | 0.0 |'
- en: '|  | Random Guess | 4.2 | 4.2 | 4.2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机猜测 | 4.2 | 4.2 | 4.2 |'
- en: '| BestAnswer | GPT-4-Turbo-0125 | 30.0 | 0.0 | 0.0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| BestAnswer | GPT-4-Turbo-0125 | 30.0 | 0.0 | 0.0 |'
- en: '| GPT-4-Turbo-1106 | 16.0 | 0.0 | 0.0 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 16.0 | 0.0 | 0.0 |'
- en: '| Claude-2 | 4.0 | 0.0 | / |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 4.0 | 0.0 | / |'
- en: '| Claude-2.1 | 4.0 | 0.0 | 0.0 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2.1 | 4.0 | 0.0 | 0.0 |'
- en: '|  | InternLM2-7b | 0.5 | 0.5 | 0.0 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | InternLM2-7b | 0.5 | 0.5 | 0.0 |'
- en: '|  | Random Guess | 0.6 | 0.3 | 0.1 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | 随机猜测 | 0.6 | 0.3 | 0.1 |'
- en: 'Table 6: Results of LLMs on TSort and BestAnswer benchmarks in ultra-long context
    settings.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：LLMs 在超长上下文设置下的 TSort 和 BestAnswer 基准测试结果。
- en: Though the evaluated models claim that they can understand long text up to 100,000+
    tokens (a whole book with hundreds of pages, *e.g.*), they suffer from a dramatic
    decline on their performance under ultra-long-context settings, comparing to their
    long-context performance. For the TSort task, GPT-4-Turbo is able to achieve a
    random guess level accuracy, while Claude fails to give any correct answers. For
    BestAnswer, the performance of all three models fall sharply from 16k to 32k text
    length. Meanwhile, they can not give any correct answer when the text length is
    greater than 32k.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管被评估的模型声称它们可以理解长达 100,000+ 个标记（例如整本书）的长文本，但在超长上下文设置下，它们的性能相较于长上下文表现会有显著下降。对于
    TSort 任务，GPT-4-Turbo 能够达到随机猜测水平的准确率，而 Claude 则无法给出任何正确答案。对于 BestAnswer，所有三种模型的性能在
    16k 到 32k 文本长度范围内急剧下降。同时，当文本长度超过 32k 时，它们无法给出任何正确答案。
- en: 4.5 Ablation Study
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 消融研究
- en: 4.5.1 Perplexity Evaluation on TSort
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1 TSort 上的困惑度评估
- en: 'Perplexity (PPL) evaluation is frequently adopted to assess the capability
    of LLMs. During inference, models compute the perplexity of multiple candidates
    and the one with the lowest perplexity is selected as the inference result. For
    TSort, we create 24 candidates for perplexity computation, each candidate is a
    permutation of the 4 text segments. We conduct PPL-based evaluation for open-source
    LLMs on 2k, 4k and 8k text length settings. [Table 7](#S4.T7 "In 4.5.1 Perplexity
    Evaluation on TSort ‣ 4.5 Ablation Study ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating
    long-context LLMs with length-adaptable benchmarks") exhibits the PPL-Eval result
    on TSort. When text segments are arranged in the correct order, a significantly
    lower perplexity score can usually be observed⁵⁵5 One potential cause is that
    the chapters have been used for pretraining., resulting in the high TSort accuracy.
    However, when the sorting task is presented as QAs where LLMs are asked to directly
    output the correct order, the performance significantly deteriorates, indicating
    the limited instruction following capabilities of existing LLMs.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度（PPL）评估经常被用于评估 LLMs 的能力。在推理过程中，模型计算多个候选的困惑度，困惑度最低的候选被选为推理结果。对于 TSort，我们创建了
    24 个候选项用于困惑度计算，每个候选项是 4 个文本片段的排列组合。我们在 2k、4k 和 8k 文本长度设置下对开源 LLMs 进行了基于 PPL 的评估。[表
    7](#S4.T7 "在 4.5.1 TSort 上的困惑度评估 ‣ 4.5 消融研究 ‣ 4 评估结果 ‣ Ada-LEval：使用长度可调基准评估长上下文
    LLMs") 展示了 TSort 上的 PPL-Eval 结果。当文本片段按正确顺序排列时，通常可以观察到显著较低的困惑度分数⁵⁵5 一个潜在的原因是章节已用于预训练。，这导致了较高的
    TSort 准确性。然而，当排序任务呈现为问答形式，要求 LLMs 直接输出正确顺序时，性能显著下降，表明现有 LLMs 在遵循指令方面的能力有限。
- en: '| TSort (PPL Eval) | 2k | 4k | 8k |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| TSort (PPL Eval) | 2k | 4k | 8k |'
- en: '| --- | --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LongChat-7b-v1.5-32k | 60.9 | 68.3 | 77.4 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 60.9 | 68.3 | 77.4 |'
- en: '| ChatGLM2-6B-32k | 40.5 | 53.5 | 57.5 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 40.5 | 53.5 | 57.5 |'
- en: '| ChatGLM3-6B-32k | 50.1 | 57.0 | 59.3 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 50.1 | 57.0 | 59.3 |'
- en: '| Vicuna-7b-v1.5-16k | 70.1 | 78.3 | 77.7 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 70.1 | 78.3 | 77.7 |'
- en: '| Vicuna-13b-v1.5-16k | 79.3 | 86.7 | 89.2 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 79.3 | 86.7 | 89.2 |'
- en: '| Random Guess | 4.2 | 4.2 | 4.2 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 随机猜测 | 4.2 | 4.2 | 4.2 |'
- en: 'Table 7: Perplexity Evaluation Results on TSort for open-source LLMs.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：开源 LLMs 在 TSort 上的困惑度评估结果。
- en: 4.5.2 Position Bias in BestAnswer
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2 BestAnswer 中的定位偏差
- en: 'To study the position bias of existing LLMs, in BestAnswer, we keep questions
    and answer candidates the same and alter the position of groundtruth answers.
    Specifically, we manually set the groundtruth answer at the beginning, in the
    middle, or at the rear of all answers and then perform the evaluation. [Table 8](#S4.T8
    "In 4.5.2 Position Bias in BestAnswer ‣ 4.5 Ablation Study ‣ 4 Evaluation Results
    ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks") displays
    the evaluation results. All models demonstrate significant position bias in choosing
    the most helpful answer. Most models achieve much better accuracy when the most
    helpful answer presents at the beginning. Claude-2 has some unique behaviors.
    It performs the best when the groundtruth is positioned at the rear across 4 of
    5 different settings. As the input length increases, the position bias becomes
    more obvious. For instance, Vicuna-7b-v1.5-16k demonstrates relatively uniform
    accuracy under the 1k setting. However, when the input length extends to 16k tokens,
    the model’s performance remains stable only when the best answer is at the front.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究现有LLM的位置信息偏差，在BestAnswer中，我们保持问题和答案候选项不变，改变了真实答案的位置。具体来说，我们手动将真实答案设置在所有答案的开头、中间或末尾，然后进行评估。[表8](#S4.T8
    "在4.5.2 BestAnswer中的位置偏差 ‣ 4.5 消融研究 ‣ 4 评估结果 ‣ Ada-LEval：使用长度可调整基准测试评估长上下文LLM")展示了评估结果。所有模型在选择最有帮助的答案时显示出显著的位置信息偏差。当最有帮助的答案出现在开头时，大多数模型的准确率显著提高。Claude-2具有一些独特的行为。在5种不同设置中的4种情况下，当真实答案位于末尾时，它的表现最好。随着输入长度的增加，位置偏差变得更加明显。例如，Vicuna-7b-v1.5-16k在1k设置下表现相对均匀。然而，当输入长度扩展到16k
    tokens时，模型的表现只有在最佳答案位于前面时才保持稳定。
- en: '| BestAnswer | Pos | 1k | 2k | 4k | 8k | 16k |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| BestAnswer | Pos | 1k | 2k | 4k | 8k | 16k |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4-Turbo-1106 | front | 76.5 | 82.5 | 86.5 | 90.0 | 82.0 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 前面 | 76.5 | 82.5 | 86.5 | 90.0 | 82.0 |'
- en: '| mid | 74.5 | 68.0 | 60.0 | 38.0 | 38.5 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 74.5 | 68.0 | 60.0 | 38.0 | 38.5 |'
- en: '| rear | 57.5 | 46.6 | 44.0 | 40.5 | 26.5 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 57.5 | 46.6 | 44.0 | 40.5 | 26.5 |'
- en: '| GPT-3.5-Turbo-1106 | front | 77.0 | 80.5 | 77.0 | 46.5 | 2.5 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-1106 | 前面 | 77.0 | 80.5 | 77.0 | 46.5 | 2.5 |'
- en: '| mid | 64.5 | 48.5 | 32.0 | 9.5 | 0.5 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 64.5 | 48.5 | 32.0 | 9.5 | 0.5 |'
- en: '| rear | 37.5 | 19.0 | 8.5 | 6.0 | 3.5 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 37.5 | 19.0 | 8.5 | 6.0 | 3.5 |'
- en: '| Claude-2 | front | 34.0 | 19.0 | 14.5 | 50.0 | 6.0 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 前面 | 34.0 | 19.0 | 14.5 | 50.0 | 6.0 |'
- en: '| mid | 49.0 | 35.5 | 21.5 | 13.0 | 5.0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 49.0 | 35.5 | 21.5 | 13.0 | 5.0 |'
- en: '| rear | 59.0 | 36.5 | 26.0 | 11.0 | 9.5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 59.0 | 36.5 | 26.0 | 11.0 | 9.5 |'
- en: '| LongChat-7b-v1.5-32k | front | 24.1 | 5.0 | 12.1 | 33.6 | 29.0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 前面 | 24.1 | 5.0 | 12.1 | 33.6 | 29.0 |'
- en: '| mid | 32.7 | 13.6 | 0.2 | 0.2 | 0.0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 32.7 | 13.6 | 0.2 | 0.2 | 0.0 |'
- en: '| rear | 29.8 | 1.9 | 0.0 | 0.1 | 0.1 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 29.8 | 1.9 | 0.0 | 0.1 | 0.1 |'
- en: '| ChatGLM2-6B-32k | front | 30.0 | 31.5 | 46.2 | 10.5 | 0.5 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 前面 | 30.0 | 31.5 | 46.2 | 10.5 | 0.5 |'
- en: '| mid | 27.7 | 10.4 | 1.0 | 0.1 | 0.1 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 27.7 | 10.4 | 1.0 | 0.1 | 0.1 |'
- en: '| rear | 28.5 | 12.4 | 2.6 | 4.1 | 0.0 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 28.5 | 12.4 | 2.6 | 4.1 | 0.0 |'
- en: '| ChatGLM3-6B-32k | front | 48.9 | 34.3 | 37.6 | 35.8 | 19.0 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 前面 | 48.9 | 34.3 | 37.6 | 35.8 | 19.0 |'
- en: '| mid | 41.9 | 22.3 | 5.3 | 0.9 | 0.1 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 41.9 | 22.3 | 5.3 | 0.9 | 0.1 |'
- en: '| rear | 28.8 | 5.4 | 3.7 | 8.8 | 2.9 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 28.8 | 5.4 | 3.7 | 8.8 | 2.9 |'
- en: '| Vicuna-7b-v1.5-16k | front | 29.3 | 8.9 | 14.0 | 37.6 | 25.4 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 前面 | 29.3 | 8.9 | 14.0 | 37.6 | 25.4 |'
- en: '| mid | 32.8 | 13.6 | 0.0 | 0.0 | 0.2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 32.8 | 13.6 | 0.0 | 0.0 | 0.2 |'
- en: '| rear | 34.2 | 2.1 | 0.0 | 0.0 | 0.7 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 34.2 | 2.1 | 0.0 | 0.0 | 0.7 |'
- en: '| Vicuna-13b-v1.5-16k | front | 52.5 | 51.4 | 58.6 | 81.7 | 11.8 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 前面 | 52.5 | 51.4 | 58.6 | 81.7 | 11.8 |'
- en: '| mid | 64.5 | 29.2 | 1.5 | 0.5 | 0.3 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 中间 | 64.5 | 29.2 | 1.5 | 0.5 | 0.3 |'
- en: '| rear | 34.2 | 2.4 | 0.0 | 0.0 | 13.4 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 后面 | 34.2 | 2.4 | 0.0 | 0.0 | 13.4 |'
- en: 'Table 8: Results of LLMs on BestAnswer where the best answer is set at the
    front, in the middle and at the rear of all answers. Pos denotes the position
    of the best answer.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：在BestAnswer中，最佳答案设置在所有答案的前面、中间和末尾时LLMs的结果。Pos表示最佳答案的位置。
- en: 4.5.3 Scalable Position Embeddings
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3 可扩展位置嵌入
- en: 'Scalable position embeddings have shown their value in extending context window
    while requiring minimal or no fine-tuning steps. Existing position embedding methods
    for context window extension can be categorized into two major categories: position
    interpolation and length extrapolation. NTK-aware Scaled RoPE utilizes the advantage
    of both methods by changing the base of RoPE. ReRoPE and Leaky ReRoPE (Su, [2023](#bib.bib27))
    design a window size to control the application of scalable position embeddings
    directly. We conduct our study on Vicuna-v1.5 models (Zheng et al., [2023](#bib.bib35)),
    which are Llama 2 fine-tuned with 4k context window. We adopt original models
    (4k context window) as the baseline across all settings. [Table 9](#S4.T9 "In
    4.5.3 Scalable Position Embeddings ‣ 4.5 Ablation Study ‣ 4 Evaluation Results
    ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks") shows
    the result of different position embedding methods on the BestAnswer benchmark.
    Our findings indicate that scalable position embeddings do improve the long-context
    modeling capability. All methods enhance the accuracy under the 8k setting, which
    is beyond the original context window. Concurrently, the model performance under
    short settings (1k, *e.g.*) is basically retained. NTK-aware Scaled RoPE diminishes
    performance on 1k context length, but outperforms other two methods on longer
    context. The advantage of these methods is more obvious on Vicuna-13b-v1.5. Moreover,
    comparing to their 16k versions, which utilize Flash Attention and are further
    trained on high-quality 16k length conversation data, advanced scalable position
    embeddings still achieve comparable performance.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '可扩展的位置嵌入在扩展上下文窗口方面显示了其价值，同时只需最小的或不需要微调步骤。现有的上下文窗口扩展位置嵌入方法可以分为两大类：位置插值和长度外推。NTK-aware
    Scaled RoPE 通过改变 RoPE 的基数来利用这两种方法的优势。ReRoPE 和 Leaky ReRoPE（Su，[2023](#bib.bib27)）设计了一种窗口大小来直接控制可扩展位置嵌入的应用。我们在
    Vicuna-v1.5 模型（Zheng 等，[2023](#bib.bib35)）上进行了研究，这些模型是使用 4k 上下文窗口微调的 Llama 2。我们在所有设置中采用原始模型（4k
    上下文窗口）作为基线。[表 9](#S4.T9 "在 4.5.3 可扩展位置嵌入 ‣ 4.5 消融研究 ‣ 4 评估结果 ‣ Ada-LEval: 使用长度可调基准评估长上下文
    LLMs") 显示了不同位置嵌入方法在 BestAnswer 基准上的结果。我们的研究结果表明，可扩展的位置嵌入确实提高了长上下文建模能力。所有方法在超出原始上下文窗口的
    8k 设置下提高了准确性。同时，模型在短设置（如 1k）下的表现基本保持不变。NTK-aware Scaled RoPE 在 1k 上下文长度下性能下降，但在较长上下文中优于其他两种方法。这些方法的优势在
    Vicuna-13b-v1.5 上更为明显。此外，与其 16k 版本相比，后者使用了 Flash Attention 并在高质量的 16k 长对话数据上进行了进一步训练，先进的可扩展位置嵌入仍然实现了可比的性能。'
- en: '| Vicuna-7b-v1.5 | 1k | 2k | 4k | 8k |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5 | 1k | 2k | 4k | 8k |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ReRoPE | 39.6/39.6 | 11.6/11.6 | 4.7/5.4 | 2.3/3.2 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| ReRoPE | 39.6/39.6 | 11.6/11.6 | 4.7/5.4 | 2.3/3.2 |'
- en: '| Leaky ReRoPE | 39.9/39.9 | 11.2/11.2 | 5.1/5.7 | 1.3/2.0 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReRoPE | 39.9/39.9 | 11.2/11.2 | 5.1/5.7 | 1.3/2.0 |'
- en: '| NTK | 32.5/32.5 | 10.7/10.7 | 5.8/5.8 | 3.9/3.9 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| NTK | 32.5/32.5 | 10.7/10.7 | 5.8/5.8 | 3.9/3.9 |'
- en: '| Original(4k) | 39.5/39.5 | 9.8/11.0 | 4.2/5.5 | 0.0/0.0 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Original(4k) | 39.5/39.5 | 9.8/11.0 | 4.2/5.5 | 0.0/0.0 |'
- en: '| Original(16k) | 37.0/39.5 | 11.1/11.1 | 5.8/5.8 | 2.5/2.7 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Original(16k) | 37.0/39.5 | 11.1/11.1 | 5.8/5.8 | 2.5/2.7 |'
- en: '| Vicuna-13b-v1.5 | 1k | 2k | 4k | 8k |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5 | 1k | 2k | 4k | 8k |'
- en: '| ReRoPE | 49.2/49.2 | 22.5/22.5 | 9.2/10.0 | 1.5/2.8 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| ReRoPE | 49.2/49.2 | 22.5/22.5 | 9.2/10.0 | 1.5/2.8 |'
- en: '| Leaky ReRoPE | 49.3/49.3 | 23.8/23.8 | 8.7/9.8 | 1.3/2.6 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Leaky ReRoPE | 49.3/49.3 | 23.8/23.8 | 8.7/9.8 | 1.3/2.6 |'
- en: '| NTK | 43.8/43.8 | 23.0/23.0 | 11.1/11.1 | 2.3/2.3 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| NTK | 43.8/43.8 | 23.0/23.0 | 11.1/11.1 | 2.3/2.3 |'
- en: '| Original(4k) | 49.1/49.1 | 17.7/17.7 | 5.9/5.9 | 0.1/1.0 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Original(4k) | 49.1/49.1 | 17.7/17.7 | 5.9/5.9 | 0.1/1.0 |'
- en: '| Original(16k) | 53.4/53.4 | 29.2/29.2 | 13.1/13.5 | 2.6/2.7 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Original(16k) | 53.4/53.4 | 29.2/29.2 | 13.1/13.5 | 2.6/2.7 |'
- en: 'Table 9: Results of Vicuna-v1.5 with different context window extrapolation
    methods on BestAnswer. ‘Original (4k) / (16k)’ denotes the original Vicuna model
    trained with 4k / 16k context lengths. In the reported ‘X/Y’, X indicates the
    accuracy while Y indicates the accuracy which cases failed to follow the instruction
    are excluded.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同上下文窗口外推方法在 BestAnswer 上的 Vicuna-v1.5 结果。‘Original (4k) / (16k)’ 表示使用 4k
    / 16k 上下文长度训练的原始 Vicuna 模型。在报告的 ‘X/Y’ 中，X 表示准确率，而 Y 表示排除了未遵循指令的情况后的准确率。
- en: 4.5.4 Comparison with Other Long-Context Benchmarks
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.4 与其他长上下文基准的比较
- en: We compare Ada-LEval with other long-context benchmarks to validate that our
    benchmarks require much overall text understanding to complete the task than traditional
    long-context benchmarks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Ada-LEval 与其他长上下文基准进行比较，以验证我们的基准在完成任务时需要比传统长上下文基准更多的整体文本理解。
- en: We regard a task requires models to understand text comprehensively if the performances
    of models decrease sharply when the text is truncated. TSort task meets this requirement
    since truncating any segment will lead to an incorrect answer.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为一个任务需要模型全面理解文本，如果模型在文本被截断时表现急剧下降，则说明该任务符合此要求。TSort任务满足这一要求，因为截断任何片段都会导致错误答案。
- en: To exhibit the BestAnswer requires more comprehensive text understanding than
    traditional QA and summarization tasks, we conduct an experiment on BestAnswer(16k
    version) and 2 classic long-context datasets, NarrativeQA(LongBench subset, QA
    task) and GovReport(LongBench subset, summarization task) respectively. The metric
    for NarrativeQA is F1 score and metric for GovReport is Rouge-L. We evaluate the
    performance of GPT-4-Turbo-1106 on all 3 datasets. Each test case is truncated
    into 2k, 4k and 8k version as the input. We also provide its full version for
    comparison.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 展示BestAnswer需要比传统QA和总结任务更全面的文本理解，我们对BestAnswer（16k版本）和两个经典的长文本数据集，NarrativeQA（LongBench子集，QA任务）和GovReport（LongBench子集，总结任务）分别进行了实验。NarrativeQA的指标是F1分数，GovReport的指标是Rouge-L。我们评估了GPT-4-Turbo-1106在所有3个数据集上的表现。每个测试用例被截断为2k、4k和8k版本作为输入。我们还提供了其完整版本以作比较。
- en: '| Benchmark | 2k | 4k | 8k | Full | Avg #tokens |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 基准测试 | 2k | 4k | 8k | 完整 | 平均标记数 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| BestAnswer | 11.0 | 20.0 | 31.5 | 44.0 | 15646 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| BestAnswer | 11.0 | 20.0 | 31.5 | 44.0 | 15646 |'
- en: '| NarrativeQA | 24.7 | 25.6 | 29.7 | 33.1 | 10276 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| NarrativeQA | 24.7 | 25.6 | 29.7 | 33.1 | 10276 |'
- en: '| GovReport | 30.7 | 32.4 | 33.6 | 30.9 | 29872 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| GovReport | 30.7 | 32.4 | 33.6 | 30.9 | 29872 |'
- en: 'Table 10: Results of GPT-4-Turbo on different long-context benchmarks.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：GPT-4-Turbo在不同长文本基准测试上的结果。
- en: 'From the table [10](#S4.T10 "Table 10 ‣ 4.5.4 Comparison with Other Long-Context
    Benchmarks ‣ 4.5 Ablation Study ‣ 4 Evaluation Results ‣ Ada-LEval: Evaluating
    long-context LLMs with length-adaptable benchmarks"), the performance of GPT-4-Turbo
    on BestAnswer decreases more dramatically than NarrativeQA and GovReport when
    text is truncated. Notably, the performance on GovReport even increases when text
    is truncated into 4k and 8k. Therefore, our benchmarks require more full-text
    comprehension than traditional QA and summarization tasks.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[10](#S4.T10 "表10 ‣ 4.5.4 与其他长文本基准的比较 ‣ 4.5 消融研究 ‣ 4 评估结果 ‣ Ada-LEval：评估长文本LLMs的长度可调整基准")中可以看出，当文本被截断时，GPT-4-Turbo在BestAnswer上的表现比NarrativeQA和GovReport下降更为剧烈。值得注意的是，在文本被截断到4k和8k时，GovReport上的表现甚至有所提升。因此，我们的基准测试要求比传统QA和总结任务需要更多的全文理解。
- en: 5 Conclusion
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduce Ada-LEval, a length-adaptable dataset to assess
    long-context capability of LLMs. We conduct comprehensive experiments on multiple
    LLMs and find that all open-source models still lag significantly behind state-of-the-art
    proprietary models in terms of long context capability. When the input length
    scales to 4,000 tokens, most open-source models rapidly deteriorates to random
    guess level. In the meanwhile, the capability of proprietary models is also severely
    limited, When it comes to the ultra-long setting (32,000+ tokens), no proprietary
    model notably outperforms the random baseline. Ada-LEval is the first benchmark
    that evaluates LLMs under the ultra-long setting, and we hope that the limitations
    pointed out by this benchmarks can serve as valuable references for future developments
    of long-context LLMs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了Ada-LEval，一个长度可调整的数据集，用于评估LLMs的长文本能力。我们对多个LLMs进行了全面的实验，发现所有开源模型在长文本能力上仍显著落后于最先进的专有模型。当输入长度达到4,000个标记时，大多数开源模型迅速退化到随机猜测水平。同时，专有模型的能力也受到严重限制。对于超长设置（32,000+个标记），没有哪个专有模型明显优于随机基线。Ada-LEval是第一个在超长设置下评估LLMs的基准，我们希望该基准指出的局限性能为未来长文本LLMs的发展提供有价值的参考。
- en: Acknowledgement. This project is supported by the National Key R&D Program of
    China No.2022ZD0161600 and the Shanghai Postdoctoral Excellence Program (No.2023023).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。本项目得到了中国国家重点研发计划（编号：2022ZD0161600）和上海市博士后优秀人才计划（编号：2023023）的支持。
- en: 6 Limitations
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: Ada-LEval is a challenging benchmark, requiring strong understanding and reasoning
    capabilities over long text. Due to the poor instruction following rate and copy
    instruction rate of open-source LLMs, Ada-LEval can hardly distinguish their long
    context capability through the accuracy metric.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Ada-LEval是一个具有挑战性的基准，需要对长文本有强大的理解和推理能力。由于开源LLMs的指令跟随率和复制指令率较差，Ada-LEval几乎无法通过准确性指标区分它们的长文本能力。
- en: Furthermore, as text length increases, the difficulty of Ada-LEval rises sharply
    under ultra-long-context settings. Even state-of-the-art proprietary models are
    not able to achieve an ideal performance, which further constrains its applicability
    to current LLMs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着文本长度的增加，在超长上下文设置下，Ada-LEval 的难度急剧上升。即使是最先进的专有模型也无法达到理想的性能，这进一步限制了其在当前大型语言模型中的适用性。
- en: References
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等 (2023) 陈新安、龚珊珊、钟铭、李沐凯、张军、孔灵鹏、邱熙鹏。2023。L-eval：为长上下文语言模型建立标准化评估。*arXiv 预印本
    arXiv:2307.11088*。
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等 (2023) 白宇狮、吕鑫、张佳杰、吕洪昌、唐健凯、黄志电、杜正晓、刘霄、曾敖寒、侯磊等。2023。Longbench：一个双语多任务的长上下文理解基准。*arXiv
    预印本 arXiv:2308.14508*。
- en: Cai et al. (2024) Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen,
    Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan,
    Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo,
    Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang
    Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining
    Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu,
    Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke
    Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song,
    Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang,
    Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu,
    Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen
    Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,
    Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang,
    Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou,
    Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024.
    [Internlm2 technical report](http://arxiv.org/abs/2403.17297).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等 (2024) 蔡征、曹毛松、陈浩炯、陈凯、陈克宇、陈欣、陈勋、陈泽辉、陈智、储佩、董晓艺、段浩东、范琪、费朝晔、高杨、葛佳业、顾晨亚、顾宇哲、归涛、郭爱佳、郭启鹏、何聪辉、胡颖凡、黄婷、姜涛、焦鹏龙、金镇江、雷志凯、李佳星、李静文、李林阳、李帅宾、李伟、李宜宁、刘宏伟、刘江宁、洪佳伟、刘凯文、刘奎坤、刘晓然、吕成齐、吕海军、吕凯、马莉、马润远、马泽润、宁文昌、欧阳凌科、邱建涛、曲元、尚复凯、邵云帆、宋德敏、宋子凡、邵志豪、孙鹏、孙宇、唐焕泽、王彬、王国腾、王佳琦、王佳宇、王锐、王雨东、王子毅、魏星健、翁启震、吴凡、熊颖通、徐超、徐瑞亮、严杭、闫依蓉、杨晓桂、叶浩晨、应怀远、余佳、余晶、臧宇航、张楚宇、张力、张潘、张鹏、张瑞杰、张硕、张松阳、张文建、张文伟、张兴成、张欣悦、赵慧、赵倩、赵小萌、周风哲、周在达、卓景铭、邹一成、邱熙鹏、乔雨、林大华。2024。[Internlm2
    技术报告](http://arxiv.org/abs/2403.17297)。
- en: 'Chen et al. (2023a) Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli
    Celikyilmaz. 2023a. Walking down the memory maze: Beyond context limit through
    interactive reading. *arXiv preprint arXiv:2310.05029*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023a) 霍华德·陈、拉马坎特·帕苏努鲁、贾森·韦斯顿、阿斯利·塞利基尔马兹。2023a。走出记忆迷宫：通过互动阅读突破上下文限制。*arXiv
    预印本 arXiv:2310.05029*。
- en: Chen et al. (2023b) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023b. Extending context window of large language models via positional
    interpolation. *arXiv preprint arXiv:2306.15595*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023b) 陈寿元、黄浩轩、梁剑、田元东。2023b。通过位置插值扩展大型语言模型的上下文窗口。*arXiv 预印本 arXiv:2306.15595*。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等 (2021) 卡尔·科贝、维尼特·科萨拉朱、穆罕默德·巴瓦里安、马克·陈、郑熙宇、卢卡斯·凯瑟尔、马蒂亚斯·普拉佩特、杰瑞·特沃雷克、雅各布·希尔顿、内藤礼一郎等。2021。训练验证器解决数学文字题。*arXiv
    预印本 arXiv:2110.14168*。
- en: 'Contributors (2023) OpenCompass Contributors. 2023. Opencompass: A universal
    evaluation platform for foundation models. [https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass).'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贡献者 (2023) OpenCompass 贡献者。2023。Opencompass：一个通用的基础模型评估平台。[https://github.com/open-compass/opencompass](https://github.com/open-compass/opencompass)。
- en: 'Dao et al. (2022a) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. 2022a. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao et al. (2022a) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher
    Ré. 2022a. 《Flashattention: Fast and memory-efficient exact attention with io-awareness》。*《神经信息处理系统进展》*,
    35:16344–16359。'
- en: 'Dao et al. (2022b) Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri
    Rudra, and Christopher Ré. 2022b. Hungry hungry hippos: Towards language modeling
    with state space models. *arXiv preprint arXiv:2212.14052*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao et al. (2022b) Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri
    Rudra, 和 Christopher Ré. 2022b. 《Hungry hungry hippos: Towards language modeling
    with state space models》。*arXiv 预印本 arXiv:2212.14052*。'
- en: Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A
    Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and
    answers anchored in research papers. *arXiv preprint arXiv:2105.03011*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah
    A Smith, 和 Matt Gardner. 2021. 《A dataset of information-seeking questions and
    answers anchored in research papers》。*arXiv 预印本 arXiv:2105.03011*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. 《Llm. int8 (): 8-bit matrix multiplication for transformers at scale》。*arXiv
    预印本 arXiv:2208.07339*。'
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000
    tokens. *arXiv preprint arXiv:2307.02486*.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, 和 Furu Wei. 2023. 《Longnet: Scaling transformers to 1,000,000,000
    tokens》。*arXiv 预印本 arXiv:2307.02486*。'
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. 2022. 《Gptq: Accurate post-training quantization for generative pre-trained
    transformers》。*arXiv 预印本 arXiv:2210.17323*。'
- en: 'Guo et al. (2021) Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021. Longt5: Efficient text-to-text
    transformer for long sequences. *arXiv preprint arXiv:2112.07916*.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. (2021) Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon,
    Jianmo Ni, Yun-Hsuan Sung, 和 Yinfei Yang. 2021. 《Longt5: Efficient text-to-text
    transformer for long sequences》。*arXiv 预印本 arXiv:2112.07916*。'
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt. 2020. 《Measuring massive multitask
    language understanding》。*arXiv 预印本 arXiv:2009.03300*。
- en: Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and
    Lu Wang. 2021. Efficient attentions for long document summarization. *arXiv preprint
    arXiv:2104.02112*.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2021) Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, 和
    Lu Wang. 2021. 《Efficient attentions for long document summarization》。*arXiv 预印本
    arXiv:2104.02112*。
- en: 'Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan
    Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al.
    2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation
    models. *arXiv preprint arXiv:2305.08322*.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan
    Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, 等. 2023.
    《C-eval: A multi-level multi-discipline chinese evaluation suite for foundation
    models》。*arXiv 预印本 arXiv:2305.08322*。'
- en: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa
    reading comprehension challenge. *Transactions of the Association for Computational
    Linguistics*, 6:317–328.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, 和 Edward Grefenstette. 2018. 《The narrativeqa
    reading comprehension challenge》。*《计算语言学学会会刊》*, 6:317–328。
- en: 'Kryściński et al. (2021) Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal,
    Caiming Xiong, and Dragomir Radev. 2021. Booksum: A collection of datasets for
    long-form narrative summarization. *arXiv preprint arXiv:2105.08209*.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kryściński et al. (2021) Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal,
    Caiming Xiong, 和 Dragomir Radev. 2021. 《Booksum: A collection of datasets for
    long-form narrative summarization》。*arXiv 预印本 arXiv:2105.08209*。'
- en: Li* et al. (2023) Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, , and Hao Zhang. 2023. [How long can
    open-source llms truly promise on context length?](https://lmsys.org/blog/2023-06-29-longchat)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li* et al. (2023) Dacheng Li*, Rulin Shao*, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 2023. [《How long can open-source
    llms truly promise on context length?》](https://lmsys.org/blog/2023-06-29-longchat)
- en: 'Liu et al. (2023a) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle:
    How language models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 2023a. 陷入中间：语言模型如何使用长上下文。*arXiv 预印本
    arXiv:2307.03172*。
- en: 'Liu et al. (2023b) Tianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench:
    Benchmarking repository-level code auto-completion systems. *arXiv preprint arXiv:2306.03091*.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023b) Tianyang Liu, Canwen Xu, 和 Julian McAuley. 2023b. Repobench:
    基于代码库的自动补全系统基准测试。*arXiv 预印本 arXiv:2306.03091*。'
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with
    human feedback. *arXiv preprint arXiv:2112.09332*.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, 等. 2021. Webgpt: 浏览器辅助问答系统与人工反馈。*arXiv 预印本 arXiv:2112.09332*。'
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [Gpt-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2021) Ofir Press, Noah A Smith, 和 Mike Lewis. 2021. 短期训练，长期测试：具有线性偏置的注意力使输入长度外推成为可能。*arXiv
    预印本 arXiv:2108.12409*。
- en: 'Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran,
    Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022.
    Scrolls: Standardized comparison over long language sequences. *arXiv preprint
    arXiv:2201.03533*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham et al. (2022) Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran,
    Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, 等. 2022. Scrolls:
    对长语言序列的标准化比较。*arXiv 预印本 arXiv:2201.03533*。'
- en: Su (2023) Jianlin Su. 2023. Rectified rotary position embeddings. [https://github.com/bojone/rerope](https://github.com/bojone/rerope).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su (2023) Jianlin Su. 2023. 修正旋转位置嵌入。 [https://github.com/bojone/rerope](https://github.com/bojone/rerope)。
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.
    *arXiv preprint arXiv:2104.09864*.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen,
    和 Yunfeng Liu. 2021. Roformer: 增强的旋转位置嵌入变换器。*arXiv 预印本 arXiv:2104.09864*。'
- en: 'Sun et al. (2023) Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit
    Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions
    over long documents. *arXiv preprint arXiv:2305.14564*.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2023) Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, 和 Mohit
    Iyyer. 2023. Pearl: 促使大型语言模型规划和执行长文档中的动作。*arXiv 预印本 arXiv:2305.14564*。'
- en: Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable
    transformer. *arXiv preprint arXiv:2212.10554*.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, 和 Furu Wei. 2022. 可长度外推的变换器。*arXiv
    预印本 arXiv:2212.10554*。
- en: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can
    solve them. *arXiv preprint arXiv:2210.09261*.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, 等. 2022. 挑战大基准任务及其链式思维是否能够解决。*arXiv 预印本 arXiv:2210.09261*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2: 开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. 2020. Big bird: Transformers for longer sequences. *Advances in
    neural information processing systems*, 33:17283–17297.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, 等. 2020. Big bird: 用于更长序列的变换器。*神经信息处理系统进展*, 33:17283–17297。'
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
    An open bilingual pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等 (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming
    Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, 等 2022. Glm-130b: 一种开放的双语预训练模型。*arXiv
    预印本 arXiv:2210.02414*。'
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, 等 2023. 通过 mt-bench
    和 chatbot arena 评估 LLM 作为评审者。*arXiv 预印本 arXiv:2306.05685*。
- en: Appendix A Test Case Building Statistics
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 测试用例构建统计
- en: 'Recall that for each case length on Tsort task, we set the length upper limit
    for each text segment and the neighboring paragraphs before and after these contiguous
    chapters. We also set stride between beginning paragraphs. [Table 11](#A2.T11
    "In Appendix B Evaluation Setups ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") demonstrates the detail statistics on the length
    upper limit and the stride.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '记住，对于 Tsort 任务中的每个案例长度，我们设置了每个文本段的长度上限以及这些连续章节前后的相邻段落。我们还设置了起始段落之间的步幅。[表 11](#A2.T11
    "在附录 B 评估设置 ‣ Ada-LEval: 评估具有长度适应性基准的长上下文 LLMs") 展示了长度上限和步幅的详细统计信息。'
- en: On BestAnswer task, two questions are regarded as similar questions when they
    have 40% tags in common. Under ultra-long-context settings, both questions should
    contain at least 1 tag in common.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BestAnswer 任务中，当两个问题有 40% 的标签相同，它们被视为相似问题。在超长上下文设置下，两个问题应至少包含 1 个相同的标签。
- en: Appendix B Evaluation Setups
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 评估设置
- en: Evaluation Hyperparameters. For open-source LLMs, we adopt their default hyperparameters
    during evaluation on Ada-LEval. For proprietary models including GPT-4-Turbo,
    GPT-3.5-Turbo-1106, we set the temperature to 0.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 评估超参数。对于开源 LLMs，我们在 Ada-LEval 上评估时采用其默认超参数。对于包括 GPT-4-Turbo、GPT-3.5-Turbo-1106
    在内的专有模型，我们将温度设置为 0。
- en: Computational Budget. Our experiments for open-source LLMs are conducted on
    NVIDIA A100 80GB GPU. The entire evaluation consumes around 800 GPU-hours.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 计算预算。我们对开源 LLMs 的实验是在 NVIDIA A100 80GB GPU 上进行的。整个评估消耗约 800 GPU 小时。
- en: Benchmark Instructions. We present instructions of both tasks within Ada-LEval.
    To ensure that models know what to do, we contain the sample input and output
    format that models need to follow in solving problems. The instructions are shown
    below.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试说明。我们提供了 Ada-LEval 中两个任务的说明。为了确保模型知道该做什么，我们包含了模型解决问题时需要遵循的示例输入和输出格式。说明如下。
- en: 'Validity of 200-testcase subset. Our experiments on long-context settings adopt
    200-testcase subset for proprietary models and 1000-testcase subset for open-source
    LLMs. To ensure that evaluation results on 200-testcase subset is valid, [Table 12](#A2.T12
    "In Appendix B Evaluation Setups ‣ Ada-LEval: Evaluating long-context LLMs with
    length-adaptable benchmarks") and [Table 13](#A2.T13 "In Appendix B Evaluation
    Setups ‣ Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks")
    display results on 200-testcase subset.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '200-testcase 子集的有效性。我们在长上下文设置中的实验采用了 200-testcase 子集用于专有模型，1000-testcase 子集用于开源
    LLMs。为了确保 200-testcase 子集的评估结果有效，[表 12](#A2.T12 "在附录 B 评估设置 ‣ Ada-LEval: 评估具有长度适应性基准的长上下文
    LLMs") 和 [表 13](#A2.T13 "在附录 B 评估设置 ‣ Ada-LEval: 评估具有长度适应性基准的长上下文 LLMs") 显示了 200-testcase
    子集的结果。'
- en: '| Setting | Before | Segments | After | Stride |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 之前 | 段落 | 之后 | 步幅 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 2k | 200 | 350 | 200 | 64 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 2k | 200 | 350 | 200 | 64 |'
- en: '| 4k | 300 | 800 | 300 | 64 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 4k | 300 | 800 | 300 | 64 |'
- en: '| 8k | 400 | 1750 | 400 | 64 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 8k | 400 | 1750 | 400 | 64 |'
- en: '| 16k | 500 | 3700 | 500 | 64 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 16k | 500 | 3700 | 500 | 64 |'
- en: '| 32k | 500 | 7700 | 500 | 128 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 32k | 500 | 7700 | 500 | 128 |'
- en: '| 64k | 500 | 15700 | 500 | 128 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 64k | 500 | 15700 | 500 | 128 |'
- en: '| 128k | 500 | 31700 | 500 | 128 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 128k | 500 | 31700 | 500 | 128 |'
- en: 'Table 11: The length upper limit of text segments and stride between beginning
    paragraphs on TSort.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：TSort 中文本段的长度上限和起始段落之间的步幅。
- en: '| TSort (200-testcase) | 2k | 4k | 8k | 16k |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| TSort (200-testcase) | 2k | 4k | 8k | 16k |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-4-Turbo-0125 | 15.5 | 16.5 | 8.5 | 5.5 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-0125 | 15.5 | 16.5 | 8.5 | 5.5 |'
- en: '| GPT-4-Turbo-1106 | 18.5 | 15.5 | 7.5 | 3.5 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 18.5 | 15.5 | 7.5 | 3.5 |'
- en: '| GPT-3.5-Turbo-1106 | 4.0 | 4.5 | 4.5 | 5.5 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-1106 | 4.0 | 4.5 | 4.5 | 5.5 |'
- en: '| Claude-2 | 5.0 | 5.0 | 4.5 | 3.0 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 5.0 | 5.0 | 4.5 | 3.0 |'
- en: '| LongChat-7b-v1.5-32k | 5.0 | 5.0 | 2.5 | 2.0 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 5.0 | 5.0 | 2.5 | 2.0 |'
- en: '| ChatGLM2-6B-32k | 1.0 | 0.5 | 0.5 | 1.0 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 1.0 | 0.5 | 0.5 | 1.0 |'
- en: '| ChatGLM3-6B-32k | 3.5 | 3.0 | 1.0 | 0.5 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 3.5 | 3.0 | 1.0 | 0.5 |'
- en: '| Vicuna-7b-v1.5-16k | 5.0 | 1.5 | 1.0 | 2.5 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 5.0 | 1.5 | 1.0 | 2.5 |'
- en: '| Vicuna-13b-v1.5-16k | 5.0 | 5.0 | 3.0 | 4.0 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 5.0 | 5.0 | 3.0 | 4.0 |'
- en: '| Random Guess | 4.2 | 4.2 | 4.2 | 4.2 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Random Guess | 4.2 | 4.2 | 4.2 | 4.2 |'
- en: 'Table 12: TSort results under long-context settings(200-testcase subset).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 12: TSort 在长上下文设置下的结果（200-testcase 子集）。'
- en: '| BestAnswer (200-testcase) | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| BestAnswer (200-testcase) | 1k | 2k | 4k | 6k | 8k | 12k | 16k |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-4-Turbo-0125 | 73.5 | 73.5 | 65.5 | 63.0 | 56.5 | 52.0 | 44.5 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-0125 | 73.5 | 73.5 | 65.5 | 63.0 | 56.5 | 52.0 | 44.5 |'
- en: '| GPT-4-Turbo-1106 | 74.0 | 73.5 | 67.5 | 59.5 | 53.5 | 49.5 | 44.0 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo-1106 | 74.0 | 73.5 | 67.5 | 59.5 | 53.5 | 49.5 | 44.0 |'
- en: '| GPT-3.5-turbo-1106 | 61.5 | 48.5 | 41.5 | 29.5 | 17.0 | 2.5 | 2.5 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-1106 | 61.5 | 48.5 | 41.5 | 29.5 | 17.0 | 2.5 | 2.5 |'
- en: '| Claude-2 | 65.0 | 43.5 | 23.5 | 15.0 | 17.0 | 12.0 | 11.0 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2 | 65.0 | 43.5 | 23.5 | 15.0 | 17.0 | 12.0 | 11.0 |'
- en: '| LongChat-7b-v1.5-32k | 32.5 | 8.0 | 3.5 | 3.0 | 2.5 | 1.5 | 1.0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-7b-v1.5-32k | 32.5 | 8.0 | 3.5 | 3.0 | 2.5 | 1.5 | 1.0 |'
- en: '| ChatGLM2-6B-32k | 36.0 | 10.5 | 3.0 | 0.5 | 1.5 | 0.0 | 0.0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2-6B-32k | 36.0 | 10.5 | 3.0 | 0.5 | 1.5 | 0.0 | 0.0 |'
- en: '| ChatGLM3-6B-32k | 37.0 | 15.5 | 5.5 | 4.0 | 5.5 | 0.5 | 0.5 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32k | 37.0 | 15.5 | 5.5 | 4.0 | 5.5 | 0.5 | 0.5 |'
- en: '| Vicuna-7b-v1.5-16k | 32.5 | 8.5 | 2.5 | 3.5 | 3.0 | 0.5 | 2.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7b-v1.5-16k | 32.5 | 8.5 | 2.5 | 3.5 | 3.0 | 0.5 | 2.0 |'
- en: '| Vicuna-13b-v1.5-16k | 52.0 | 29.0 | 11.0 | 4.0 | 1.5 | 1.0 | 1.5 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13b-v1.5-16k | 52.0 | 29.0 | 11.0 | 4.0 | 1.5 | 1.0 | 1.5 |'
- en: '| Random Guess | 26.7 | 10.1 | 4.5 | 3.0 | 2.3 | 1.4 | 1.1 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Random Guess | 26.7 | 10.1 | 4.5 | 3.0 | 2.3 | 1.4 | 1.1 |'
- en: 'Table 13: BestAnswer results under long-context settings(200-testcase subset).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 13: BestAnswer 在长上下文设置下的结果（200-testcase 子集）。'
- en: 'TSort: You are an AI assistant. Your job is to sort multiple book sections
    into the correct order. Each time, you will be provided with 4 pieces of text.
    These texts form a continuous part of a book, but are provided in random order.
    You need to find the correct order and return the answer in a string. For example,
    if you output [4, 1, 3, 2], that means the correct order is: Part 4 -> Part 1
    -> Part 3 -> Part 2. You will also be provided with the neighboring paragraphs
    before and after the 4 pieces of texts. The case sample is shown below and you
    should give me the answer in the format exactly the same as the sample. However,
    you should NOT focus on the content of sample answer. Please do NOT output any
    extra content. Sample Input (format only): Before: XXX (Text before the continuous
    book part) Part 1: XXX Part 2: XXX Part 3: XXX Part 4: XXX After: XXX (Text after
    the continuous book part) Sample Output (format only): Answer: [4, 1, 3, 2]BestAnswer:
    You are an AI assistant. Your job is to find out the most helpful answer to a
    given question. Each time, you will be provided with a question and n answers
    to this question. Each answer begins with an ’A’ and a number(e.g. A4), which
    represents its designation. You need to determine which answer is the most helpful
    one to the question. The case sample is shown below and you should give me the
    answer in the format exactly the same as the sample. However, you should NOT focus
    on the content of sample answer. Sample Input (format only): The question is given
    below. XXX(The content of question) Possible answers are given below. A1: XXX(The
    content of answer 1) A2: XXX(The content of answer 2) . . . An: XXX(The content
    of answer n) Now the answers are over, please decide which answer is the most
    helpful one to the question. You must give me only the designation of the MOST
    helpful answer. Sample Output (format only): Answer: The designation of the most
    helpful answer.(e.g. A4 means answer 4 is the most helpful answer)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: TSort：你是一个 AI 助手。你的工作是将多个书籍章节按照正确的顺序排序。每次你会得到 4 段文本。这些文本构成书籍的连续部分，但它们是随机顺序的。你需要找出正确的顺序，并以字符串形式返回答案。例如，如果你输出[4,
    1, 3, 2]，这意味着正确的顺序是：第4部分 -> 第1部分 -> 第3部分 -> 第2部分。你还会提供这4段文本前后的相邻段落。下面是案例示例，你应该按照与示例完全相同的格式给出答案。然而，你不应关注示例答案的内容。请不要输出任何额外内容。样例输入（格式仅）：之前：XXX（连续书籍部分之前的文本）第1部分：XXX第2部分：XXX第3部分：XXX第4部分：XXX之后：XXX（连续书籍部分之后的文本）样例输出（格式仅）：答案：[4,
    1, 3, 2]BestAnswer：你是一个 AI 助手。你的工作是找出对给定问题最有帮助的答案。每次你会得到一个问题和 n 个答案。每个答案以一个‘A’和一个数字（例如
    A4）开头，表示其编号。你需要确定哪个答案对问题最有帮助。下面是案例示例，你应该按照与示例完全相同的格式给出答案。然而，你不应关注示例答案的内容。样例输入（格式仅）：问题如下。XXX（问题内容）可能的答案如下。A1：XXX（答案1的内容）A2：XXX（答案2的内容）……An：XXX（答案n的内容）现在答案已结束，请决定哪个答案对问题最有帮助。你必须只给出最有帮助答案的编号。样例输出（格式仅）：答案：最有帮助答案的编号（例如
    A4 表示答案4是最有帮助的答案）
