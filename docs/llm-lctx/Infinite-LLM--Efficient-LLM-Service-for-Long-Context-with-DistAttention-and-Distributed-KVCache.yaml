- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 19:04:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:04:21'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Infinite-LLM：基于DistAttention和分布式KVCache的高效LLM服务
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02669](https://ar5iv.labs.arxiv.org/html/2401.02669)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02669](https://ar5iv.labs.arxiv.org/html/2401.02669)
- en: Bin Lin Tao Peng Chen Zhang Minmin Sun Alibaba Group Lanbo Li Alibaba Group
    Hanyu Zhao Alibaba Group Wencong Xiao Alibaba Group Qi Xu Alibaba Group Xiafei
    Qiu Alibaba Group Shen Li Alibaba Group Zhigang Ji Shanghai Jiao Tong University
    Yong Li Alibaba Group Wei Lin Alibaba Group
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Bin Lin Tao Peng Chen Zhang Minmin Sun Alibaba Group Lanbo Li Alibaba Group
    Hanyu Zhao Alibaba Group Wencong Xiao Alibaba Group Qi Xu Alibaba Group Xiafei
    Qiu Alibaba Group Shen Li Alibaba Group Zhigang Ji 上海交通大学 Yong Li Alibaba Group
    Wei Lin Alibaba Group
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid proliferation of Large Language Models (LLMs) has been a driving force
    in the growth of cloud-based LLM services, which are now integral to advancing
    AI applications. However, the dynamic auto-regressive nature of LLM service, along
    with the need to support exceptionally long context lengths, demands the flexible
    allocation and release of substantial resources. This presents considerable challenges
    in designing cloud-based LLM service systems, where inefficient management can
    lead to performance degradation or resource wastage. In response to these challenges,
    this work introduces DistAttention, a novel distributed attention algorithm that
    segments the KV Cache into smaller, manageable units, enabling distributed processing
    and storage of the attention module. Based on that, we propose DistKV-LLM, a distributed
    LLM serving system that dynamically manages KV Cache and effectively orchestrates
    all accessible GPU and CPU memories spanning across the data center. This ensures
    a high-performance LLM service on the cloud, adaptable to a broad range of context
    lengths. Validated in a cloud environment with 32 NVIDIA A100 GPUs in configurations
    from 2 to 32 instances, our system exhibited 1.03-2.4$\times$ longer than current
    state-of-the-art LLM service systems, as evidenced by extensive testing across
    18 datasets with context lengths up to 1,900K.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的快速普及成为了云端LLM服务增长的推动力，这些服务现在是推进AI应用的不可或缺的组成部分。然而，LLM服务的动态自回归特性以及支持极长上下文的需求，要求灵活分配和释放大量资源。这在设计基于云的LLM服务系统时带来了相当大的挑战，不高效的管理可能导致性能下降或资源浪费。针对这些挑战，本研究介绍了DistAttention，一种新型的分布式注意力算法，该算法将KV
    Cache划分为更小、更易管理的单元，实现了注意力模块的分布式处理和存储。在此基础上，我们提出了DistKV-LLM，一种分布式LLM服务系统，能够动态管理KV
    Cache，并有效协调跨数据中心的所有可用GPU和CPU内存。这确保了在云端提供高性能的LLM服务，适应广泛的上下文长度。在32 NVIDIA A100 GPUs的云环境中进行了验证，我们的系统表现出比当前最先进的LLM服务系统长1.03-2.4$\times$，经过18个数据集、上下文长度高达1,900K的广泛测试证明了这一点。
- en: '^†^†*: Equal contribution.^†^††: Corresponding author: chenzhang.sjtu@sjtu.edu.cn.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '^†^†*: 相等贡献。^†^††: 通讯作者：chenzhang.sjtu@sjtu.edu.cn。'
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) [[14](#bib.bib14), [43](#bib.bib43), [11](#bib.bib11)]have
    fueled the rapid growth of LLM cloud services, becoming crucial infrastructure
    for advancing AI applications. However, this development faces significant challenges
    due to the massive computational and data requirements. These services typically
    use multiple GPU cards working together for LLM tasks. Yet, the dynamic nature
    of LLMs creates complex computational issues.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[14](#bib.bib14), [43](#bib.bib43), [11](#bib.bib11)]推动了LLM云服务的快速增长，成为推进AI应用的关键基础设施。然而，由于巨大的计算和数据需求，这一发展面临着重大挑战。这些服务通常使用多个GPU卡协同工作来处理LLM任务。然而，LLM的动态特性带来了复杂的计算问题。
- en: At the heart of LLM services lies the intrinsic procedure of auto-regressive
    text generation[[46](#bib.bib46), [39](#bib.bib39), [13](#bib.bib13), [42](#bib.bib42)],
    where the model generates one word (or token) at a time. Each newly generated
    token becomes appended to the existing text corpus, forming the input for recalibration
    within the LLM. This iterative progression persists until the ultimate word or
    token is generated. Crucially, the requisite memory and computational resources
    for LLM services dynamically oscillate throughout the LLM service, with neither
    the lifetime nor the length of the sequence known a priori.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LLM服务的核心在于自回归文本生成的内在过程[[46](#bib.bib46), [39](#bib.bib39), [13](#bib.bib13),
    [42](#bib.bib42)]，模型一次生成一个词（或标记）。每个新生成的标记都会附加到现有的文本语料库中，形成LLM内再校准的输入。这种迭代进程持续进行，直到生成最终的词或标记。至关重要的是，LLM服务所需的内存和计算资源在整个LLM服务过程中动态波动，既不知序列的生命周期，也不知其长度。
- en: The dynamic and iterative nature of auto-regressive text generation makes it
    impossible to plan resource allocation in advance[[2](#bib.bib2), [50](#bib.bib50),
    [26](#bib.bib26)], posing substantial challenges in designing efficient LLM service
    systems on the cloud. Particularly in long-context tasks, the expanding Key-Value
    (KV) Cache can surpass GPU memory limits within a computing instance, necessitating
    immediate resource reallocation. This often involves either initiating a costly
    live migration to transfer the task to a more capable instance or pre-assigning
    extra GPUs to handle potential memory overloads. The latter, however, can lead
    to inefficiencies and resource wastage, especially in tasks with normal length
    contexts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归文本生成的动态和迭代特性使得事先规划资源分配变得不可能[[2](#bib.bib2), [50](#bib.bib50), [26](#bib.bib26)]，这在设计高效的云端LLM服务系统时带来了重大挑战。特别是在长上下文任务中，扩展的键值（KV）缓存可能会超出计算实例的GPU内存限制，迫使立即重新分配资源。这通常涉及启动一个昂贵的实时迁移，将任务转移到更强大的实例，或预分配额外的GPU以处理潜在的内存过载。然而，后者可能导致低效和资源浪费，特别是在正常长度的上下文任务中。
- en: Previous work, such as PagedAttention[[26](#bib.bib26)], has attempted to tackle
    these problems by facilitating the exchange (or swap) of data between GPU and
    CPU memory. However, this approach encounters several limitations. First, the
    scope of PagedAttention’s memory swapping was restricted to the GPU and CPU memory
    within a single node, thus limiting its capacity to accommodate extremely long
    context lengths. Second, while its paging strategy is devised to minimize memory
    fragmentation, it swaps entire KV Caches on a request-level basis and thus missing
    the chance for more adaptive, granular scheduling in a distributed cloud environment.
    Last but not least, the interruption of computation for swapped-out requests can
    cause jittered performance for the running task, risking non-compliance with the
    strict service-level agreements (SLAs) [[36](#bib.bib36)]that are crucial for
    cloud services.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的工作，如PagedAttention[[26](#bib.bib26)]，尝试通过促进GPU和CPU内存之间的数据交换（或交换）来解决这些问题。然而，这种方法存在若干限制。首先，PagedAttention的内存交换范围仅限于单个节点中的GPU和CPU内存，从而限制了其处理极长上下文长度的能力。其次，尽管其分页策略旨在最小化内存碎片，但它以请求级别的方式交换整个KV缓存，因此错失了在分布式云环境中进行更具适应性、粒度化调度的机会。最后但同样重要的是，对于被交换出的请求的计算中断可能导致正在运行的任务性能抖动，风险是无法满足对云服务至关重要的严格服务水平协议（SLAs）[[36](#bib.bib36)]。
- en: To tackle the above challenges, we propose DistAttention, a novel attention
    algorithm designed to overcome these challenges. DistAttention partitions the
    KV cache into rBlocks—uniform sub-blocks that facilitate the distributed computation
    and memory management of attention modules for LLM service with long context length.
    Distinct from conventional methods that mainly utilize GPU or CPU memory within
    a single node, DistAttention enables optimization opportunities of all accessible
    GPU or CPU memory resources spread across the data center, particularly those
    that are now underutilized. This not only enables support for much longer context
    lengths but also avoids the performance fluctuations typically associated with
    data swapping or live migration processes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述挑战，我们提出了DistAttention，这是一种新颖的注意力算法，旨在克服这些挑战。DistAttention将KV缓存划分为rBlocks——均匀的子块，便于在长上下文长度的LLM服务中进行分布式计算和内存管理。与主要利用单节点内的GPU或CPU内存的传统方法不同，DistAttention能够优化数据中心中所有可用的GPU或CPU内存资源，特别是那些现在未被充分利用的资源。这不仅支持更长的上下文长度，还避免了数据交换或实时迁移过程通常伴随的性能波动。
- en: In this paper, we developed DistKV-LLM , a distributed LLM service engine that
    integrates seamlessly with DistAttention. DistKV-LLM  excels in managing KV Cache,
    efficiently coordinating memory usage among distributed GPUs and CPUs throughout
    the data center. When an LLM service instance faces a memory deficit due to KV
    Cache expansion, DistKV-LLM  proactively seeks supplementary memory from less
    burdened instances. Moreover, DistKV-LLM  introduces an intricate protocol that
    facilitates efficient, scalable, and coherent interactions among numerous LLM
    service instances running in the cloud. This protocol is designed to manage and
    balance the large amount of memory resources effectively. Additionally, DistKV-LLM 
    prioritizes data locality and communication optimization, crucial for addressing
    performance challenges associated with the distributed storage of the KV Cache.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们开发了DistKV-LLM，一种与DistAttention无缝集成的分布式LLM服务引擎。DistKV-LLM在管理KV缓存方面表现出色，高效地协调数据中心内分布式GPU和CPU的内存使用。当LLM服务实例由于KV缓存扩展面临内存不足时，DistKV-LLM主动从负担较轻的实例中寻求补充内存。此外，DistKV-LLM引入了一种复杂的协议，促进了云端多个LLM服务实例之间的高效、可扩展和一致的交互。该协议旨在有效地管理和平衡大量内存资源。此外，DistKV-LLM优先考虑数据本地性和通信优化，这对于解决与KV缓存分布式存储相关的性能挑战至关重要。
- en: In summary, our work seeks to fully utilize all the available GPU resources
    across the data center, ensuring a smooth and efficient cloud service for LLMs
    especially when handling long-context tasks. The DistAttention, combined with
    DistKV-LLM, offers a solution to the resource allocation and optimization challenges
    faced by LLM services in distributed environments. This approach enables efficient
    resource management, allowing LLM services to handle a wide range of context-generation
    tasks effectively. We conducted a comprehensive evaluation of DistKV-LLM  in a
    cloud setup equipped with 32 NVIDIA A100 GPUs, testing various distributed system
    configurations ranging from 2 to 32 instances. Our assessment included 18 benchmark
    datasets with context lengths extending up to 1,900K. Our system demonstrated
    1.03-2.4 $\times$ longer context lengths.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的工作旨在充分利用数据中心中所有可用的GPU资源，确保LLM在处理长上下文任务时能够顺畅高效地提供云服务。DistAttention与DistKV-LLM的结合为LLM服务在分布式环境中面临的资源分配和优化挑战提供了解决方案。这种方法实现了高效的资源管理，使LLM服务能够有效地处理各种上下文生成任务。我们在配备32个NVIDIA
    A100 GPU的云端设置中对DistKV-LLM进行了全面评估，测试了从2到32个实例的各种分布式系统配置。我们的评估包括18个基准数据集，上下文长度扩展到1,900K。我们的系统展示了1.03-2.4倍的更长上下文长度。
- en: 'This paper makes the following contributions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文作出了以下贡献：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present DistAttention , an innovative attention algorithm designed to significantly
    advance distributed computing for Large Language Models (LLMs) on the cloud. This
    algorithm is particularly adept at handling dynamic and diverse context generation
    tasks. Crucially, DistAttention  unlocks the potential to fully utilize all available
    GPU and CPU memory resources across the data center.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了DistAttention，一种创新的注意力算法，旨在显著推动云端大型语言模型（LLMs）的分布式计算。这种算法特别擅长处理动态和多样的上下文生成任务。至关重要的是，DistAttention释放了充分利用数据中心所有可用GPU和CPU内存资源的潜力。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce DistKV-LLM , a distributed LLM service engine, which excels in
    providing efficient, scalable, and coherent management of distributed KV Caches,
    harnessing the vast memory resources within GPU clusters on cloud infrastructure.
    DistKV-LLM  also effectively optimizes memory locality and communication overhead,
    ensuring a smooth and efficient cloud service for LLMs.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了DistKV-LLM，一个分布式LLM服务引擎，它在提供高效、可扩展且连贯的分布式KV缓存管理方面表现卓越，利用云基础设施中GPU集群的大量内存资源。DistKV-LLM还有效地优化了内存局部性和通信开销，确保了LLMs的平稳和高效的云服务。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate the feasibility and efficiency of the combination of DistAttention 
    and DistKV-LLM  in a cloud environment with 32 NVIDIA A100 GPUs and 18 datasets
    with up to 1,900K context length. Our system outperforms state-of-the-art work,
    delivering support for context lengths that are 2-19$\times$ higher throughput
    in tasks with standard-length contexts.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了在云环境中使用32个NVIDIA A100 GPU和18个数据集（上下文长度可达1900K）时，DistAttention和DistKV-LLM组合的可行性和效率。我们的系统超越了最先进的工作，在标准长度上下文任务中提供了2-19$\times$更高的吞吐量。
- en: 'In the following sections of this paper, Section [2](#S2 "2 Background ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")
    introduces relevant background information. Section [3](#S3 "3 Motivation and
    Main Idea ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache") outlines the key challenges of serving LLMs on the cloud
    and our main idea. Section [4](#S4 "4 Method ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache") delves into the
    details of our design. Section [5](#S5 "5 Implementation Details ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")
    describes our implementation details. Section [6](#S6 "6 Evaluation ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")
    presents our evaluation results. Section [7](#S7 "7 Related Works ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")
    provides an overview of related works. Section [8](#S8 "8 Conclusion ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")
    concludes this work.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文的以下章节中，章节[2](#S2 "2 Background ‣ Infinite-LLM: Efficient LLM Service for
    Long Context with DistAttention and Distributed KVCache")介绍了相关背景信息。章节[3](#S3 "3
    Motivation and Main Idea ‣ Infinite-LLM: Efficient LLM Service for Long Context
    with DistAttention and Distributed KVCache")概述了在云端服务LLMs的主要挑战和我们的核心理念。章节[4](#S4
    "4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache")深入探讨了我们设计的细节。章节[5](#S5 "5 Implementation Details ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")描述了我们的实现细节。章节[6](#S6
    "6 Evaluation ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache")展示了我们的评估结果。章节[7](#S7 "7 Related Works ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")提供了相关工作的概述。章节[8](#S8
    "8 Conclusion ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache")总结了这项工作。'
- en: 2 Background
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Large Language Models
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型
- en: Transformer-based large language models (LLMs) have revolutionized natural language
    processing, offering capabilities ranging from simple text generation to complex
    problem-solving and conversational AI[[34](#bib.bib34), [15](#bib.bib15), [19](#bib.bib19),
    [20](#bib.bib20)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的大型语言模型（LLMs）已经革新了自然语言处理，提供了从简单文本生成到复杂问题解决和对话AI的能力[[34](#bib.bib34),
    [15](#bib.bib15), [19](#bib.bib19), [20](#bib.bib20)]。
- en: 2.1.1 Model Architecture
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 模型架构
- en: 'Large Language Models (LLMs) models[[14](#bib.bib14), [43](#bib.bib43), [11](#bib.bib11)]
    have a sophisticated architecture built on the principles of the Transformer model[[46](#bib.bib46)].
    For example, GPT-3[[13](#bib.bib13)], one of the largest models, consists of numerous
    transformer blocks (layers) with 175 billion parameters, enabling it to capture
    complex language patterns and generate human-like text. A Transformer block consists
    of several key components:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[14](#bib.bib14), [43](#bib.bib43), [11](#bib.bib11)]具有基于Transformer模型原则构建的复杂架构[[46](#bib.bib46)]。例如，GPT-3[[13](#bib.bib13)]，作为最大模型之一，包含了多个transformer块（层），拥有1750亿个参数，使其能够捕捉复杂的语言模式并生成类似人类的文本。一个Transformer块包含几个关键组件：
- en: QKV Linear layer takes the input to the Transformer block first. These layers
    are essentially fully connected neural networks that project the input into three
    different spaces, including queries (Q), keys (K), and values (V).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: QKV线性层首先接收Transformer块的输入。这些层本质上是全连接神经网络，将输入投射到三个不同的空间，包括查询（Q）、键（K）和值（V）。
- en: Multi-Head Self-Attention Mechanism, or the attention module, is the core of
    the Transformer block. It allows the model to weigh the importance of different
    parts of the input sequence differently. In multi-head attention, the input is
    linearly transformed multiple times to form different ’heads’, allowing the model
    to jointly attend to information from different representation subspaces at different
    positions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力机制，或称为注意力模块，是Transformer块的核心。它允许模型对输入序列的不同部分赋予不同的重要性。在多头注意力中，输入被多次线性变换以形成不同的“头”，使模型能够同时关注来自不同表示子空间的不同位置的信息。
- en: Feed-Forward Neural Network, or FFN module, is after the self-attention module.
    This is typically a two-layer neural network with a ReLU activation in between.
    This network is identical for different positions but with different parameters
    from layer to layer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络，或称FFN模块，在自注意力模块之后。这通常是一个两层神经网络，中间有ReLU激活。这个网络对于不同的位置是相同的，但不同层之间的参数是不同的。
- en: 2.1.2 LLM Service
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 LLM服务
- en: Prefill Phase   During inference, LLMs first receive a prompt or input text
    from the user. This input is processed to understand the context and the nature
    of the task required (e.g., answering a question, writing a poem, etc.).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 预填充阶段   在推理过程中，LLM首先从用户那里接收提示或输入文本。这些输入会被处理以理解上下文和所需任务的性质（例如，回答问题、写诗等）。
- en: 'Given a prompt of tokens $X=[{x_{1}},{x_{2}},\ldots,{x_{n}}]$, is computed
    as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的令牌提示$X=[{x_{1}},{x_{2}},\ldots,{x_{n}}]$，计算为：
- en: '|  | $P(x_{n+1}&#124;X)=\text{Softmax}(W\cdot h_{n}+b)$ |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(x_{n+1}&#124;X)=\text{Softmax}(W\cdot h_{n}+b)$ |  | (1) |'
- en: where $W$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于$W$。
- en: Autoregressive Generation Phase   In the auto-regressive phase, the model generates
    one word at a time, each new word being conditioned on the sequence of words generated
    so far. This phase is iterative and continues until the model produces a complete
    and coherent response or reaches a predefined limit (like a word count).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归生成阶段   在自回归阶段，模型一次生成一个词，每个新词都基于到目前为止生成的词序列。这一阶段是迭代的，持续进行直到模型生成一个完整且连贯的响应或达到预定义的限制（如词数）。
- en: 'The autogressive generation phase starts with an initial context $X_{0}$ based
    on the sequence generated so far. Second, select the next word $x_{t}$ with the
    highest probability from this distribution:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归生成阶段以基于当前生成序列的初始上下文$X_{0}$开始。其次，从这个分布中选择具有最高概率的下一个词$x_{t}$：
- en: '|  | $x_{t}=\text{argmax}(P(x_{t}&#124;X_{t-1}))$ |  | (2) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t}=\text{argmax}(P(x_{t}&#124;X_{t-1}))$ |  | (2) |'
- en: Third, append the selected token $x_{t}$ to the sequence to form a new sequence.
    This process repeats until a termination condition is met, such as the generation
    of an end-of-sequence token or reaching a maximum sequence length.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，将选定的标记$x_{t}$附加到序列中以形成新的序列。这个过程重复进行，直到满足终止条件，例如生成一个结束序列标记或达到最大序列长度。
- en: 2.2 Parallelism Method for LLM Service
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM服务的并行方法
- en: Large Language Models (LLMs) require substantial computational power and memory
    resource during serving or inference. To manage this, several parallelism strategies
    are employed to distribute the workload and accelerate processing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在服务或推理过程中需要大量的计算能力和内存资源。为了管理这些需求，采用了多种并行策略来分配工作负载并加速处理。
- en: 2.2.1 Data parallelism
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 数据并行性
- en: To handle the substantial volume of requests in cloud environments, data parallelism[[49](#bib.bib49)]
    is applied by replicating LLMs across the data center. The fundamental computational
    unit, termed an instance, has a copy of the full model. Each instance operates
    independently, processing distinct batches of requests concurrently.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理云环境中大量的请求，通过在数据中心复制LLMs应用数据并行性[[49](#bib.bib49)]。基本计算单元，即实例，拥有完整模型的副本。每个实例独立操作，同时处理不同的请求批次。
- en: Batching.   In each instance, batching strategies are essential for improving
    throughput, allowing for the simultaneous processing of a greater number of requests.
    Due to the variability in the context length, requests usually have varying lifetime,
    requiring dynamic batching strategies. Various methods[[18](#bib.bib18), [50](#bib.bib50)],
    have been introduced to improve the throughput of LLM serving on GPUs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理。每个实例中，批处理策略对于提高吞吐量至关重要，允许同时处理更多的请求。由于上下文长度的可变性，请求通常具有不同的生命周期，需要动态的批处理策略。各种方法[[18](#bib.bib18),
    [50](#bib.bib50)]已经被引入以提高 GPU 上 LLM 服务的吞吐量。
- en: 2.2.2 Model Parallelism
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 模型并行性
- en: 'Model parallelism is a technique used to accommodate the inference of LLMs
    that cannot fit entirely within the memory of a single GPU. It involves splitting
    the model across multiple devices or nodes. Model parallelism can be categorized
    mainly into two types: pipeline parallelism and tensor parallelism.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行性是一种用于容纳无法完全适应单个 GPU 内存的 LLM 推理的技术。它涉及将模型拆分到多个设备或节点上。模型并行性主要可以分为两种类型：流水线并行性和张量并行性。
- en: Pipeline parallelism.   With pipeline parallelism, the layers of a model are
    sharded across multiple devices[[23](#bib.bib23), [22](#bib.bib22), [32](#bib.bib32),
    [33](#bib.bib33)]. It involves splitting the model into several stages or layers,
    each of which is processed on different computing units.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线并行性。通过流水线并行性，模型的层被分割到多个设备上[[23](#bib.bib23), [22](#bib.bib22), [32](#bib.bib32),
    [33](#bib.bib33)]。它涉及将模型拆分为几个阶段或层，每个阶段或层在不同的计算单元上处理。
- en: Tensor parallelism.   It involves splitting the model’s layers across multiple
    GPUs. For LLMs, tensor parallelism is crucial when individual layers of the model
    are too large for a single GPU. It allows large matrix operations within layers
    to be distributed across multiple GPUs. With tensor model parallelism, individual
    layers of the model are partitioned over multiple devices[[41](#bib.bib41)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 张量并行性。它涉及将模型的层拆分到多个 GPU 上。对于 LLM，当模型的单个层对于单个 GPU 过大时，张量并行性是至关重要的。它允许在层内的大矩阵操作在多个
    GPU 之间分配。通过张量模型并行性，模型的单个层被划分到多个设备[[41](#bib.bib41)]。
- en: 3 Motivation and Main Idea
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机和主要思想
- en: 'Table 1: LLaMA2-13B, KV Cache size with context length'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: LLaMA2-13B，KV 缓存大小与上下文长度'
- en: '| Context length | 10k | 100k | 500k | 1000k |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 上下文长度 | 10k | 100k | 500k | 1000k |'
- en: '| KV Cache size | 8.19GB | 81.9GB | 409.6GB | 819.2GB |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| KV 缓存大小 | 8.19GB | 81.9GB | 409.6GB | 819.2GB |'
- en: '| Misc size | 26GB | 26GB | 26GB | 26GB |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 其他大小 | 26GB | 26GB | 26GB | 26GB |'
- en: There has been a notable surge in the evolution of long-context LLMs[[37](#bib.bib37),
    [30](#bib.bib30), [11](#bib.bib11), [21](#bib.bib21)], with the context window
    expanding from 2K[[43](#bib.bib43)] to an impressive 256K[[37](#bib.bib37), [45](#bib.bib45)]
    in merely a year. This progression continues unabated, with applications of LLMs
    now demanding support for even longer contexts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文 LLM 的演变有了显著的增长[[37](#bib.bib37), [30](#bib.bib30), [11](#bib.bib11), [21](#bib.bib21)]，上下文窗口从
    2K[[43](#bib.bib43)] 扩展到令人印象深刻的 256K[[37](#bib.bib37), [45](#bib.bib45)]，仅仅在一年内。这个进展持续不断，LLM
    的应用现在需要支持更长的上下文。
- en: 'This expansion in context length poses substantial challenges for LLM serving
    systems, particularly due to the escalating computational and memory requirements
    for KV Caches[[38](#bib.bib38)]. Table [1](#S3.T1 "Table 1 ‣ 3 Motivation and
    Main Idea ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache") depicts this trend, showing a steep escalation in KV
    Cache size that directly corresponds to the growing context length for the LLaMA2-13B
    model[[44](#bib.bib44)]. Current GPUs, with memory capacities spanning several
    dozen GBs, are being pushed to their limits, necessitating more memory space to
    accommodate the burgeoning size of KV Caches.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文长度的扩展对 LLM 服务系统提出了 substantial 的挑战，特别是由于 KV 缓存的计算和内存需求不断增加[[38](#bib.bib38)]。表 [1](#S3.T1
    "Table 1 ‣ 3 Motivation and Main Idea ‣ Infinite-LLM: Efficient LLM Service for
    Long Context with DistAttention and Distributed KVCache") 描述了这一趋势，显示了 KV 缓存大小的急剧增加，这与
    LLaMA2-13B 模型的上下文长度增长直接相关[[44](#bib.bib44)]。目前，内存容量达到数十 GB 的 GPU 已经被推向极限，需要更多的内存空间来容纳不断增长的
    KV 缓存大小。'
- en: 3.1 Challenges to LLM serving on Cloud
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 云端 LLM 服务的挑战
- en: In this work, we endeavor to effectively utilize the vast memory capacities
    of GPUs and CPUs available in data centers, with the goal of creating an efficient
    memory pool specifically designed for LLM services capable of handling extremely
    long context lengths. However, the development of such a system is far from straightforward.
    We have identified two primary challenges that arise in the context of serving
    large language models with extended contexts on cloud platforms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们致力于有效利用数据中心内可用的GPU和CPU的大量内存，目标是创建一个专门设计用于LLM服务的高效内存池，能够处理极长的上下文长度。然而，开发这样一个系统并非易事。我们已经识别出在云平台上服务大语言模型时存在的两个主要挑战。
- en: '![Refer to caption](img/b2cde58d6d7f0eb9af638e8cfb8d75ea.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2cde58d6d7f0eb9af638e8cfb8d75ea.png)'
- en: 'Figure 1: The dynamic and unpredictable resource demand of LLM service often
    requires either initiating a costly live migration to transfer the task to a more
    capable instance or pre-assigning extra GPUs to handle potential memory overloads.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM服务的动态和不可预测的资源需求通常需要启动成本高昂的实时迁移，将任务转移到更强大的实例，或预分配额外的GPU以应对潜在的内存过载。
- en: '![Refer to caption](img/d9d1182a6a5e1ddac78d713f1f8f680b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9d1182a6a5e1ddac78d713f1f8f680b.png)'
- en: 'Figure 2: Our method enables KV Cache memory management in an elastic way,
    facilitating better performance and higher resource utilization in the cloud environment.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们的方法以弹性方式实现KV缓存内存管理，促进了云环境中的更好性能和更高的资源利用率。
- en: 'Challenge 1: significant disparities in memory demands obstacles efficient
    model parallelism.'
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战 1：显著的内存需求差异阻碍了有效的模型并行性。
- en: 'In stark contrast to the continuously expanding KV Cache throughout the auto-generation
    process, the memory requirements for the remaining activation tensors remain constant,
    as detailed in Table [1](#S3.T1 "Table 1 ‣ 3 Motivation and Main Idea ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '与在自动生成过程中不断扩展的KV缓存形成鲜明对比的是，剩余激活张量的内存需求保持不变，详细信息见表[1](#S3.T1 "Table 1 ‣ 3 Motivation
    and Main Idea ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache")。'
- en: 'This disparity between the attention layer and other layers poses a substantial
    challenge for efficiently implementing model parallelism. To accommodate the extensive
    KV Cache necessary for long-context tasks, an increased number of GPUs is required.
    However, tensor dimensions in other layers do not scale with context length. As
    a result, traditional model parallelism leads to more fine-grained subdivisions
    of these layers when distributed across more GPUs, as shown in  [Figure 2](#S3.F2
    "Figure 2 ‣ 3.1 Challenges to LLM serving on Cloud ‣ 3 Motivation and Main Idea
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache"), resulting in less efficient resource utilization.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力层与其他层之间的差异对有效实现模型并行性构成了重大挑战。为了适应长上下文任务所需的大量KV缓存，需要增加GPU的数量。然而，其他层的张量维度不会随着上下文长度的变化而变化。因此，传统的模型并行性在分布到更多GPU时，会对这些层进行更精细的划分，如[图
    2](#S3.F2 "Figure 2 ‣ 3.1 Challenges to LLM serving on Cloud ‣ 3 Motivation and
    Main Idea ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache")所示，导致资源利用效率降低。'
- en: Some previous studies[[26](#bib.bib26), [9](#bib.bib9)], have suggested dividing
    KV Caches into smaller blocks for more fine-grained memory management, aiming
    to prevent memory fragmentation. While these approaches have disaggregated the
    KV Cache of attention layers from the Transformer block, they are still reliant
    on gathering and positioning all blocks within the local GPU memory to carry out
    attention module’s computation. In contrast, our design goal focuses on storing
    KV Caches and executing attention modules in a distributed manner, essential for
    effectively utilizing the abundant resources available in cloud environments.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一些先前的研究[[26](#bib.bib26), [9](#bib.bib9)]建议将KV缓存划分为更小的块，以实现更精细的内存管理，旨在防止内存碎片化。虽然这些方法将注意力层的KV缓存从Transformer块中分离出来，但它们仍依赖于在本地GPU内存中收集和定位所有块以执行注意力模块的计算。相比之下，我们的设计目标集中在以分布式方式存储KV缓存和执行注意力模块，这对于有效利用云环境中丰富的资源至关重要。
- en: 'Challenge 2: dynamicity of KV Cache size leads to inefficient resource management
    in the cloud environment.'
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 挑战 2：KV缓存大小的动态性导致云环境中的资源管理效率低下。
- en: The intrinsic nature of the auto-regressive design determines that the ultimate
    sequence length remains unknown until the generation process reaches its conclusion,
    typically marked by an "ending" character. Consequently, memory requirements are
    completely dynamic and unpredictable, fluctuating significantly in scale. The
    demands can range from a few gigabytes to several hundred gigabytes, which is
    continuing to escalate even further.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归设计的内在性质决定了最终序列长度在生成过程结束之前是未知的，通常以“结束”字符为标志。因此，内存需求完全动态且不可预测，规模波动显著。需求范围从几GB到几百GB，且还在持续增长。
- en: This variability precludes any form of resource planning in advance. Resources
    must be allocated or released dynamically, reacting to the real-time demands of
    the auto-regressive process. If the memory required for a context exceeds the
    capacity of an instance’s GPUs, the entire task must be transferred to a larger
    instance with more GPUs, a process known as live migration. Live migration is
    resource-intensive and, as our experiments show, can be 25x more costly than a
    standard inference. An alternative, allocating more GPUs to a computing instance
    from the outset, can lead to resource wastage for tasks involving shorter contexts,
    thereby compounding the challenge of efficient resource allocation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变动性排除了任何形式的提前资源规划。资源必须动态分配或释放，以应对自回归过程的实时需求。如果上下文所需的内存超过了实例的GPU容量，则整个任务必须转移到具有更多GPU的大型实例，这一过程称为实时迁移。实时迁移资源密集，正如我们的实验所示，其成本可能是标准推断的25倍。另一种替代方法是从一开始就为计算实例分配更多的GPU，这可能导致处理短上下文任务时资源浪费，从而加剧了高效资源分配的挑战。
- en: PagedAttention[[26](#bib.bib26)] addresses the management of KV Caches by employing
    fine-grained sub-blocks, analogous to pages in operating systems. However, this
    approach is confined to utilizing only CPU memory for swapping, a method that
    proves inefficient on the cloud. The limitation imposed by the finite CPU memory
    not only restricts the maximum context length supportable by LLM services but
    also fails to capitalize on the expansive memory resources distributed across
    the cloud. In contrast, our objective is to harness the extensive memory capabilities
    of both GPUs and CPUs within data centers. We aim to establish an efficient memory
    pool, meticulously crafted for LLM services, to support the processing of exceptionally
    long context lengths effectively.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: PagedAttention[[26](#bib.bib26)]通过采用类似操作系统中的页面的细粒度子块来管理KV缓存。然而，这种方法仅限于使用CPU内存进行交换，在云计算环境下效率低下。有限的CPU内存不仅限制了LLM服务所支持的最大上下文长度，还未能充分利用分布在云中的广泛内存资源。相比之下，我们的目标是利用数据中心内GPU和CPU的广泛内存能力。我们旨在建立一个高效的内存池，精心设计用于LLM服务，以有效支持处理极长的上下文长度。
- en: 3.2 Main Idea
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 主要观点
- en: Motivated by the above challenges, we present a suite of key techniques specifically
    designed to address these challenges. Together, they form a comprehensive and
    systematic approach, ensuring efficient LLM serving capable of handling extended
    context lengths.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 针对上述挑战，我们提出了一套关键技术，专门设计用以解决这些挑战。它们共同构成了一种全面且系统的方法，确保能够高效服务于能够处理扩展上下文长度的LLM。
- en: To address challenge 1, we introduce a new attention algorithm named DistAttention.
    This algorithm breaks down the traditional attention computation into smaller,
    more manageable units known as macro-attentions (MAs) and their corresponding
    KV Caches (rBlocks). This innovative method facilitates the decoupling of KV Caches’
    computation from the standard transformer block, thereby enabling independent
    model parallelism strategies and memory management for attention layers versus
    other layers within the Transformer block. For non-attention layers, we apply
    established model parallelism strategies[[41](#bib.bib41), [27](#bib.bib27), [52](#bib.bib52),
    [17](#bib.bib17)]. In contrast, the attention layers are managed adaptively, dynamically
    allocating memory and computational resources across the data center in response
    to the fluctuations of the KV Cache.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决挑战1，我们引入了一种名为**DistAttention**的新型注意力算法。该算法将传统的注意力计算拆分为较小、更易管理的单元，即宏注意力（MAs）及其对应的KV缓存（rBlocks）。这一创新方法使得KV缓存的计算与标准Transformer块的计算解耦，从而实现了注意力层与Transformer块内其他层的独立模型并行策略和内存管理。对于非注意力层，我们应用了已建立的模型并行策略[[41](#bib.bib41),
    [27](#bib.bib27), [52](#bib.bib52), [17](#bib.bib17)]。相比之下，注意力层则采用自适应管理，动态分配数据中心内的内存和计算资源，以应对KV缓存的波动。
- en: 'To overcome challenges 2, we present DistKV-LLM , a distributed LLM service
    engine seamlessly integrated with DistAttention. The DistKV-LLM is designed to
    provide an efficient KV Cache management service, coordinating memory usage across
    GPUs and CPUs throughout the data center. When an LLM service instance encounters
    a memory shortfall due to an increase in the KV Cache, DistKV-LLM proactively
    identifies and borrows available memory spaces from other instances that have
    excess capacity, as is shown in [Figure 2](#S3.F2 "Figure 2 ‣ 3.1 Challenges to
    LLM serving on Cloud ‣ 3 Motivation and Main Idea ‣ Infinite-LLM: Efficient LLM
    Service for Long Context with DistAttention and Distributed KVCache"). This automated
    mechanism is realized by collaborative operations of two major components, the
    rManger and the gManager. The rManger virtualizes all the GPU and CPU memories
    within each LLM service instance , handling memory operation requests from both
    local and remote instances. Simultaneously, the gManager operates as a global
    coordinator, maintaining a protocol that ensures effective, scalable, and coherent
    resource management among distributed rManagers. Moreover, DistKV-LLM  proposes
    a new algorithm, called DGFM, that effectively addresses the issue of memory fragmentation
    in the distributed KV Cache environment of the data center. This joint effort
    ensures continuous and efficient memory utilization, thereby enhancing the overall
    performance and reliability of the LLM service.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '为了克服挑战2，我们提出了**DistKV-LLM**，这是一种与**DistAttention**无缝集成的分布式LLM服务引擎。**DistKV-LLM**旨在提供高效的KV缓存管理服务，协调数据中心内GPU和CPU的内存使用。当LLM服务实例由于KV缓存增加而遇到内存不足时，**DistKV-LLM**会主动识别并从其他有多余容量的实例中借用可用内存空间，如[图2](#S3.F2
    "Figure 2 ‣ 3.1 Challenges to LLM serving on Cloud ‣ 3 Motivation and Main Idea
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache")所示。这个自动化机制由两个主要组件**rManger**和**gManager**的协作操作实现。**rManger**虚拟化每个LLM服务实例中的所有GPU和CPU内存，处理来自本地和远程实例的内存操作请求。同时，**gManager**作为全局协调者，维护协议，确保分布式**rManagers**之间有效、可扩展和一致的资源管理。此外，**DistKV-LLM**提出了一种新的算法，称为**DGFM**，有效解决了数据中心分布式KV缓存环境中的内存碎片化问题。这一联合努力确保了持续和高效的内存利用，从而提升了LLM服务的整体性能和可靠性。'
- en: In summary, our integrated approach with DistKV-LLM and DistAttention presents
    a robust and scalable solution to the unique challenges posed by long-context
    LLM serving on the cloud. By addressing key issues related to memory management
    and computation scheduling, we ensure that LLM services can operate efficiently
    and adaptively in the cloud. This innovative framework not only optimizes resource
    utilization but also paves the way for future advancements in the field of large-scale
    language model deployment. We present details of our design in the following sections.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们与**DistKV-LLM**和**DistAttention**的集成方法为在云端提供长上下文LLM服务所面临的独特挑战提供了一个强大且可扩展的解决方案。通过解决与内存管理和计算调度相关的关键问题，我们确保LLM服务能够在云端高效和自适应地运行。这一创新框架不仅优化了资源利用，还为大规模语言模型部署领域的未来进展铺平了道路。我们将在接下来的章节中介绍我们的设计细节。
- en: 4 Method
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: 4.1 Overview
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 概述
- en: 'In the following section, we begin by introducing DistAttention, an innovative
    attention algorithm crafted to facilitate distributed KV Cache management and
    computation, as detailed in Section [4.2](#S4.SS2 "4.2 DistAttention ‣ 4 Method
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache"). Based on this, we present DistKV-LLM, an LLM serving engine
    specifically designed for efficient KV caches management of distributed GPU memory
    at the cluster scale.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的部分中，我们首先介绍了DistAttention，这是一种创新的注意力算法，旨在促进分布式KV缓存管理和计算，详细内容见第[4.2节](#S4.SS2
    "4.2 DistAttention ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context
    with DistAttention and Distributed KVCache")。基于此，我们提出了DistKV-LLM，这是一种专门设计用于高效管理分布式GPU内存KV缓存的LLM服务引擎。'
- en: 'Our approach encompasses several key components: Firstly, in Section [4.3](#S4.SS3
    "4.3 The rBlock and rManager ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache"), we introduce the
    rManager, a software layer that virtualizes the GPU and CPU memories for each
    LLM service instance. It offers an abstraction layer for basic memory blocks,
    termed rBlocks, enhancing memory management efficiency. Secondly, we describe
    a comprehensive protocolfacilitated by the gManager, a global management system
    in Section [4.4](#S4.SS4 "4.4 The gManager and Contract Protocol ‣ 4 Method ‣
    Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed
    KVCache"). It ensures effective, secure, and coherent management of rBlocks across
    distributed GPUs in the data center. In Section [4.5](#S4.SS5 "4.5 Fragmented
    Memory Management ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context
    with DistAttention and Distributed KVCache"), we further propose an innovative
    algorithm that is specifically designed to aggregate fragmented memory blocks,
    thereby significantly enhancing data locality within the distributed KV Cache
    system. Finally, in Section [4.6](#S4.SS6 "4.6 Communication Optimization ‣ 4
    Method ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache"), we propose a series of optimization strategies designed
    to minimize the extensive communication overhead associated with the distributed
    storage of the KV cache, by effectively overlapping computation and communication
    tasks. Further details about this design are discussed in the following sections.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法包括几个关键组成部分：首先，在第[4.3节](#S4.SS3 "4.3 The rBlock and rManager ‣ 4 Method
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache")中，我们介绍了rManager，这是一种为每个LLM服务实例虚拟化GPU和CPU内存的软件层。它提供了一个基本内存块的抽象层，称为rBlocks，提高了内存管理效率。其次，我们在第[4.4节](#S4.SS4
    "4.4 The gManager and Contract Protocol ‣ 4 Method ‣ Infinite-LLM: Efficient LLM
    Service for Long Context with DistAttention and Distributed KVCache")中描述了由gManager提供的综合协议，这是一个全球管理系统。它确保了数据中心分布式GPU之间rBlocks的有效、安全和一致的管理。在第[4.5节](#S4.SS5
    "4.5 Fragmented Memory Management ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache")中，我们进一步提出了一种创新算法，专门设计用于聚合碎片化的内存块，从而显著提高分布式KV缓存系统中的数据局部性。最后，在第[4.6节](#S4.SS6
    "4.6 Communication Optimization ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache")中，我们提出了一系列优化策略，旨在通过有效地重叠计算和通信任务，最小化与KV缓存的分布式存储相关的广泛通信开销。有关此设计的更多细节将在以下各节中讨论。'
- en: 4.2 DistAttention
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 DistAttention
- en: 'To tackle the complexities of memory management, we have developed a novel
    attention algorithm, DistAttention. This algorithm effectively dis-aggregates
    the KV cache into smaller, more manageable sub-blocks, thereby facilitating distributed
    memory management across the data center. Key to this approach is the partitioning
    of DistAttention into multiple Micro Attentions (MAs), with each MA encompassing
    a sub-sequence of KV cache tokens. The unique aspect of this design lies in its
    ability to compute the attention result by performing MAs separately. Upon completion
    of computations on their respective sub-blocks of token by all Micro Attentions
    (MAs), the final attention results are obtained through an aggregation process.
    This involves scaling and reducing the outputs of each MA. The methodology and
    precise formulation of this aggregation process are delineated in the following
    equations:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对内存管理的复杂性，我们开发了一种新颖的注意力算法 DistAttention。该算法有效地将 KV 缓存拆分为更小、更易管理的子块，从而促进数据中心内的分布式内存管理。这种方法的关键在于将
    DistAttention 划分为多个 Micro Attentions (MAs)，每个 MA 包括一段 KV 缓存标记。此设计的独特之处在于其通过分别执行
    MAs 来计算注意力结果。在所有 Micro Attentions (MAs) 完成各自子块的标记计算后，通过聚合过程获得最终的注意力结果。此过程包括缩放和减少每个
    MA 的输出。该聚合过程的方法和精确公式在以下方程中说明：
- en: '|  | $\displaystyle MA_{ij}=exp({Q_{i}{K_{j}}^{T}}-max(Q_{i}{K_{j}}^{T}))$
    |  | (3) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle MA_{ij}=exp({Q_{i}{K_{j}}^{T}}-max(Q_{i}{K_{j}}^{T}))$
    |  | (3) |'
- en: '|  | $1$2 |  | (4) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: 'where the $Reduce$ is calculated as below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Reduce$ 的计算如下：
- en: '|  |  | $\displaystyle Reduce(Scale([MA_{ij}]_{j=1}^{B_{kv}}))$ |  | (5) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle Reduce(Scale([MA_{ij}]_{j=1}^{B_{kv}}))$ |  | (5) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  | $max_{i}=max(max(Q_{i}{K_{1}}^{T}),...,max(Q_{i}{K_{B}}^{T}))$ |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $max_{i}=max(max(Q_{i}{K_{1}}^{T}),...,max(Q_{i}{K_{B}}^{T}))$ |  |'
- en: '|  | $sum_{i}=\sum(exp(Q_{i}K_{j}^{T}-max_{i})$ |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $sum_{i}=\sum(exp(Q_{i}K_{j}^{T}-max_{i})$ |  |'
- en: This approach not only consolidates the computations performed by individual
    MAs but also efficiently synthesizes them into a coherent final output, showcasing
    the effectiveness of the distributed processing framework implemented in our attention
    algorithm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法不仅整合了各个 MA 执行的计算，还将它们高效地合成一个连贯的最终输出，展示了我们注意力算法中实施的分布式处理框架的有效性。
- en: '![Refer to caption](img/d0d778969d5b561ff3a1b2f429036a7a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d0d778969d5b561ff3a1b2f429036a7a.png)'
- en: 'Figure 3: Illustration of the rManager design'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：rManager 设计的示意图
- en: 4.3 The rBlock and rManager
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 rBlock 和 rManager
- en: 'With the implementation of DistAttention, the Key-Value (KV) caches of LLMs
    are segmented into smaller units, known as rBlocks. Each rBlock encompasses a
    set of vectors corresponding to a fixed number of Key-Value tokens, along with
    essential metadata. This metadata provides critical information about the sub-block:
    the rBlock ID and Instance ID indicating the KV Cache in this rBlock whether belongs
    the local instance or a remote one; The device ID and physical ID labels the physical
    locations of this rBlock, which can be on the CPU side or one of the multiple
    GPUs.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DistAttention 的实现中，LLM 的 Key-Value (KV) 缓存被划分为更小的单元，称为 rBlocks。每个 rBlock 包括一组对应于固定数量
    Key-Value 标记的向量以及必要的元数据。这些元数据提供了有关子块的关键信息：rBlock ID 和 Instance ID 指示该 KV 缓存是否属于本地实例或远程实例；设备
    ID 和物理 ID 标记了 rBlock 的物理位置，可以在 CPU 端或多个 GPU 之一上。
- en: 'Each LLM service instance is equipped with a dedicated rBlock manager, referred
    to as the rManager. The rManager is responsible for overseeing all the rBlocks
    located in local devices. It effectively virtualizes the global memory space of
    GPUs by dividing it into fixed-sized physical rBlocks. Each physical rBlock is
    designed to accommodate a single logical rBlock. The rManager maintains a detailed
    table that maps these logical rBlocks to their corresponding physical rBlock addresses
    in the global memory, as is shown in Figure [3](#S4.F3.1 "Figure 3 ‣ 4.2 DistAttention
    ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '每个 LLM 服务实例配备了一个专用的 rBlock 管理器，称为 rManager。rManager 负责监督本地设备中的所有 rBlocks。它通过将
    GPU 的全局内存空间划分为固定大小的物理 rBlocks，有效地实现了全局内存空间的虚拟化。每个物理 rBlock 设计用于容纳一个逻辑 rBlock。rManager
    维护一张详细的表，将这些逻辑 rBlocks 映射到它们在全局内存中的对应物理 rBlock 地址，如图 [3](#S4.F3.1 "Figure 3 ‣
    4.2 DistAttention ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context
    with DistAttention and Distributed KVCache") 所示。'
- en: 'The rManager offers a unified API interface to serve both local and remote
    memory operations. These operations include allocating physical rBlocks for the
    newly generated KV caches and releasing them when no longer needed. Upon receiving
    a memory allocation request, either from a local or a remote instance, the rManager
    consults the rBlock table to identify the first available physical rBlock space.
    In scenarios where sufficient space is unavailable, the rManager initiates a space-borrowing
    procedure from other instances. More details will be elaborated in Section [4.4](#S4.SS4
    "4.4 The gManager and Contract Protocol ‣ 4 Method ‣ Infinite-LLM: Efficient LLM
    Service for Long Context with DistAttention and Distributed KVCache"). Notably,
    if the allocation request originates from a remote instance, the rManager is programmed
    to automatically return a false response, indicating the unavailability of direct
    remote allocation. We apply a cap on the number of rBlocks that can be allocated
    to remote instances, which is determined experimentally and configured as a hyper-parameter.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: rManager 提供了一个统一的 API 接口，以支持本地和远程内存操作。这些操作包括为新生成的 KV 缓存分配物理 rBlocks，并在不再需要时释放它们。在接收到来自本地或远程实例的内存分配请求时，rManager
    查询 rBlock 表以识别第一个可用的物理 rBlock 空间。在空间不足的情况下，rManager 启动从其他实例借用空间的程序。更多细节将在[4.4](#S4.SS4
    "4.4 gManager 和合同协议 ‣ 4 方法 ‣ 无限-LLM：带有 DistAttention 和分布式 KVCache 的长上下文高效 LLM
    服务")节中详细说明。值得注意的是，如果分配请求来自远程实例，rManager 被编程为自动返回 false 响应，表示直接远程分配不可用。我们对可以分配给远程实例的
    rBlocks 数量设置了上限，这个上限是通过实验确定的，并配置为一个超参数。
- en: 4.4 The gManager and Contract Protocol
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 gManager 和合同协议
- en: '![Refer to caption](img/9db591e8b6d4d2f23459de023750d290.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9db591e8b6d4d2f23459de023750d290.png)'
- en: 'Figure 4: gManager and Contract Protocol. The global debt ledger is a core
    component of the gManager, tracking each instance’s memory usage. This table includes
    details about available spaces and spaces lent to its debtors. It outlines five
    instances, each illustrating different memory usage dynamics. Inst-0, with a relatively
    light workload, is to lend spaces to Inst-1&3\. Inst-1, dealing with a long context,
    is borrowing space from both Inst-0&3\. Inst-2 neither borrows nor lends. Inst-3,
    finds itself both borrowing (from Inst-0) and lending (to Inst-1) simultaneously,
    exemplifying a scenario in Section [4.5](#S4.SS5 "4.5 Fragmented Memory Management
    ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：gManager 和合同协议。全局债务账本是 gManager 的核心组件，跟踪每个实例的内存使用情况。该表包括有关可用空间和借给债务人的空间的详细信息。它概述了五个实例，每个实例展示了不同的内存使用动态。Inst-0，负载相对较轻，将空间借给
    Inst-1 和 Inst-3。Inst-1，处理长上下文，从 Inst-0 和 Inst-3 借用空间。Inst-2 既不借用也不借出空间。Inst-3，发现自己同时进行借用（来自
    Inst-0）和借出（给 Inst-1），例示了[4.5](#S4.SS5 "4.5 碎片化内存管理 ‣ 4 方法 ‣ 无限-LLM：带有 DistAttention
    和分布式 KVCache 的长上下文高效 LLM 服务")节中的一种情况。
- en: '![Refer to caption](img/68a172bb6604a51d6ab1c3b0fa464af2.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/68a172bb6604a51d6ab1c3b0fa464af2.png)'
- en: 'Figure 5: An illustration of our algorithm for fragmented memory management,
    where we conceptualize the problem as a search and elimination of circles within
    a directed graph.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：我们算法在碎片化内存管理中的示意图，其中我们将问题概念化为在有向图中搜索和消除圆圈。
- en: 'Data: Directed graph $G$] $\leftarrow$* do7             if *$\neg$ parent* then12                  
    return True;13                  14      return False;15      16Main *:**17      
    foreach *node $v$;21                  22      return False;23      * *Algorithm 1
    Find a Circle in a Directed Graph*  *The key component of our system, termed the
    gManager, functions as a centralized manager, maintaining the global memory information
    across all instances. Each instance periodically transmits heartbeat signals to
    the gManager, conveying updates about their remaining available memory space.
    Utilizing this data, the gManager constructs a detailed table known as the global
    debt ledger, as is shown in Figure [4](#S4.F4.1 "Figure 4 ‣ 4.4 The gManager and
    Contract Protocol ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context
    with DistAttention and Distributed KVCache").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '数据：有向图 $G$] $\leftarrow$* do7             如果 *$\neg$ parent* 则12                  
    返回 True;13                  14      返回 False;15      16主 *:**17       遍历每个 *node
    $v$;21                  22      返回 False;23      * *算法 1 在有向图中寻找环*  *我们系统的关键组件，称为
    gManager，作为一个集中管理器，维护所有实例之间的全局内存信息。每个实例定期向 gManager 发送心跳信号，传达它们剩余的可用内存空间更新。利用这些数据，gManager
    构建了一个称为全球债务账本的详细表格，如图 [4](#S4.F4.1 "Figure 4 ‣ 4.4 The gManager and Contract Protocol
    ‣ 4 Method ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention
    and Distributed KVCache") 所示。'
- en: Whenever an instance runs short of its memory space for rBlocks, the corresponding
    rManager seeks to borrow GPU or CPU memory spaces from neighboring instances.
    Prior to this, the rManager, acting as a debtor, is required to initiate a query
    to the gManager   1, telling the
    size of the memory space it needs to borrow. Upon receiving this query, the gManager
    consults the global debt ledger   2 and responds
    by providing potential creditor’s address IDs  3, which represent
    instances that currently have surplus memory space. The selection process adheres
    to a locality & availability principle, whereby the creditor instance is chosen
    based on the lowest relative communication cost and the highest available memory
    space. The gManager proposes three potential creditor IDs as recommendations.
    Subsequently, the debtor instance approaches these creditor instances sequentially
    with requests  4,
    continuing until successful allocation is confirmed by one of them  5.
    In cases where all three candidates return a negative response, the debtor instance
    will revert to the gManager for alternative suggestions. This dynamic and responsive
    system ensures efficient and effective memory space allocation and management
    across the data center.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每当一个实例的rBlocks内存空间不足时，相应的rManager会寻求从邻近实例借用GPU或CPU内存空间。在此之前，rManager作为债务方需要向gManager发起查询，告知需要借用的内存空间大小。在收到该查询后，gManager会查阅全球债务账本，并通过提供潜在债权人的地址ID作出回应，这些ID代表了当前拥有剩余内存空间的实例。选择过程遵循局部性和可用性原则，债权实例的选择依据是相对通信成本最低和可用内存空间最大。gManager会提出三个潜在的债权人ID作为推荐。随后，债务实例会依次向这些债权实例提出请求，直到其中一个确认成功分配。如果所有三个候选者都返回负面响应，债务实例将重新向gManager寻求其他建议。这个动态响应系统确保了数据中心内存空间的高效和有效的分配与管理。
- en: In the following paragraphs, we describe the key components and design considerations
    of the contract protocol.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我们将描述合同协议的关键组成部分和设计考虑因素。
- en: 'Global Debt Ledger  : The global debt ledger, managed by the gManager, is a
    crucial table that chronicles the available memory and inter-instance debts across
    the network. Each entry in this ledger represents an LLM service instance, detailing
    the instance ID alongside its available memory spaces. Subsequent sub-fields in
    each entry denote the IDs of debtor instances, along with the quantity of rBlocks
    they have borrowed from their respective creditors.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 全球债务账本  ：由 gManager 管理的全球债务账本是一个关键表格，记录了网络中可用内存和实例间的债务。该账本中的每个条目代表一个 LLM 服务实例，详细说明了实例
    ID 以及其可用的内存空间。每个条目中的后续子字段表示债务实例的 ID 以及它们从各自的债权人处借用的 rBlocks 数量。
- en: 'Competing Candidates  : In scenarios where multiple debtor instances concurrently
    send requests to a rManager, the system must navigate these competing demands
    efficiently. The global debt ledger plays an important role here, enabling the
    gManager to evenly distribute requests among instances, thereby preventing an
    overload on any single instance. On the other side, the rManager adopts a first-come-first-serve
    policy for allocating physical spaces to rBlocks from remote instances. If the
    rManager finds itself unable to allocate sufficient physical rBlocks for remote
    rBlocks due to space constraints, it responds with a false to the debtor instances.
    This response also prompts the gManager to update its records of the current resource
    availability, effectively pausing the forwarding of new requests until more resources
    become available. This approach ensures a balanced and orderly allocation of memory
    resources, mitigating potential bottlenecks in the system.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争候选  ：在多个债务实例同时向 rManager 发送请求的情况下，系统必须高效地处理这些竞争需求。全球债务账本在这里发挥了重要作用，使得 gManager
    能够将请求均匀分配给各个实例，从而防止任何单个实例的过载。另一方面，rManager 采用先到先服务的策略，为来自远程实例的 rBlocks 分配物理空间。如果由于空间限制，rManager
    无法为远程 rBlocks 分配足够的物理 rBlocks，则会向债务实例响应一个假值。这一回应还促使 gManager 更新其当前资源可用性记录，有效地暂停新请求的转发，直到有更多资源可用。这种方法确保了内存资源的平衡和有序分配，减轻了系统中的潜在瓶颈。
- en: 'Coherency  : We employ a loose coherence policy between the gManager and the
    rManagers. Under this approach, the gManager is not required to meticulously track
    every memory allocation or release action across all instances. Instead, it gathers
    this information through regular heartbeats that are automatically sent by the
    rManagers. Consequently, the gManager maintains an overview of general space usage
    throughout the data center rather than detailed, real-time data. When responding
    to a debtor rManager’s request for borrowing space, the gManager only provides
    recommendations of potential creditor candidates. The debtor then must engage
    in negotiations with these suggested creditors to finalize the memory allocation.
    Situations involving multiple concurrent requests to the same rManager are managed
    using the previously discussed competing candidate strategy. This loosely coupled
    coherence framework not only streamlines operations but also minimizes excessive
    transaction overheads, thereby reducing processing latency and enhancing overall
    system performance.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性  ：我们在 gManager 和 rManagers 之间采用了松散一致性策略。在这种方法下，gManager 不需要仔细跟踪所有实例中的每次内存分配或释放操作。相反，它通过
    rManagers 自动发送的定期心跳来收集这些信息。因此，gManager 维护了数据中心内一般空间使用情况的概览，而非详细的实时数据。在响应债务 rManager
    的借用空间请求时，gManager 仅提供潜在债权人的建议。债务方随后必须与这些建议的债权人进行谈判，以确定内存分配。涉及多个同时向同一 rManager
    请求的情况，将使用前面讨论的竞争候选策略来处理。这种松散耦合的一致性框架不仅简化了操作，还减少了过多的交易开销，从而降低了处理延迟并提高了整体系统性能。
- en: 'Scalability  : To meet varying throughput demands, the gManager is designed
    to enhance scalability through the deployment of multiple processes that concurrently
    handle querying requests. To expedite the process of identifying instances with
    surplus memory, the gManager periodically initiates a sorting operation. This
    operation arranges the instances based on their remaining available memory space,
    enabling querying requests to efficiently bypass instances with minimal memory
    resources. This approach ensures that the gManager operates within its optimal
    capacity, maintaining system efficiency and responsiveness while scaling to accommodate
    the dynamic needs of the network.*  *### 4.5 Fragmented Memory Management'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性：为了满足不同的吞吐量需求，gManager 设计了通过部署多个进程来提升可扩展性，这些进程可以同时处理查询请求。为了加快识别内存剩余过剩的实例，gManager
    定期启动排序操作。该操作根据实例的剩余可用内存空间对实例进行排序，使得查询请求可以高效地绕过内存资源不足的实例。这种方法确保了 gManager 在其最佳能力范围内操作，保持系统的效率和响应性，同时能够扩展以适应网络的动态需求。*
    *### 4.5 片段化内存管理
- en: 'Due to the dynamicity in variable context length and batching, a critical challenge
    emerges in the form of fragmented memory management¹¹1This memory fragmentation
    is particularly pertinent to distributed KV cache management in the context of
    LLM serving on the cloud. To address fragmentation concerns within the instance,
    we incorporate strategies from previous research[[26](#bib.bib26)].. Each instance
    within the system operates both as a creditor and a debtor of memory space, lending
    to and borrowing from other instances as required. For example, instances handling
    requests with long contexts may continuously grow, necessitating borrowing space
    from remote instances. Conversely, instances with short-lived requests release
    memory space sooner, which can then be lent to others or allocated to new requests.
    This dynamicity leads to a significant issue: the deterioration of data locality.
    As instances frequently access data stored in remote memory locations, the system
    incurs a substantial performance penalty, such as increased latency and reduced
    throughput.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变量上下文长度和批处理的动态性，片段化内存管理成为一个关键挑战¹¹1这种内存碎片化问题在分布式 KV 缓存管理中尤为相关，特别是在云环境下的 LLM
    服务中。为了应对实例内的碎片化问题，我们采用了以前研究中的策略[[26](#bib.bib26)]。系统中的每个实例既作为内存空间的债权人，也作为债务人，根据需要向其他实例借贷。例如，处理长上下文请求的实例可能会不断增长，因此需要从远程实例借用空间。相反，处理短期请求的实例会更早释放内存空间，这些空间可以借给其他实例或分配给新的请求。这种动态性导致了一个显著的问题：数据局部性的恶化。当实例频繁访问存储在远程内存位置的数据时，系统会遭遇显著的性能惩罚，例如增加延迟和降低吞吐量。
- en: 'We propose a debt-graph-based fragmented memory management algorithm, namely
    DGFM, which aims to counteract this by strategically recalling memory spaces that
    have been lent out and swapping them for local data storage. A key challenge to
    this problem is that a large number of LLM service instances run concurrently
    in the data center, often involved in intricate debt relationships. To effectively
    manage this complexity, we conceptualize the problem as a search for circles within
    a directed graph. Initially, we construct a directed graph mirroring these debt
    relationships, where each node symbolizes an instance and every directed edge
    signifies the debt owed from one instance to another. Our algorithm is then applied
    iteratively. During each iteration, the algorithm selects a node at random and
    traverses the graph to identify a directed circle. The discovery of such a circle
    is pivotal; it indicates that the involved nodes, or instances, can mutually resolve
    their debts. This resolution is achieved through a strategic recall and swap of
    their respective memory blocks, thus enhancing overall system efficiency and memory
    utilization. Details of this algorithm is shown in Figure [5](#S4.F5 "Figure 5
    ‣ 4.4 The gManager and Contract Protocol ‣ 4 Method ‣ Infinite-LLM: Efficient
    LLM Service for Long Context with DistAttention and Distributed KVCache") and
    Algorithm [1](#algorithm1 "In 4.4 The gManager and Contract Protocol ‣ 4 Method
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于债务图的碎片化内存管理算法，即 DGFM，旨在通过战略性地回收已借出的内存空间并将其交换为本地数据存储来应对这一问题。该问题的一个关键挑战在于数据中心中大量
    LLM 服务实例并发运行，通常涉及复杂的债务关系。为了有效管理这种复杂性，我们将问题概念化为在有向图中寻找环路。首先，我们构建了一个有向图来反映这些债务关系，其中每个节点象征一个实例，每条有向边表示一个实例对另一个实例所欠的债务。然后，我们迭代应用算法。在每次迭代中，算法随机选择一个节点，并遍历图以识别有向环。发现这样的环路是关键，它表明涉及的节点或实例可以相互解决其债务。这种解决方案通过战略性地回收和交换各自的内存块来实现，从而提高整体系统效率和内存利用率。该算法的详细信息见图[5](#S4.F5
    "图 5 ‣ 4.4 gManager 和合同协议 ‣ 4 方法 ‣ Infinite-LLM：具有 DistAttention 和分布式 KVCache
    的长上下文高效 LLM 服务")和算法[1](#algorithm1 "在 4.4 gManager 和合同协议 ‣ 4 方法 ‣ Infinite-LLM：具有
    DistAttention 和分布式 KVCache 的长上下文高效 LLM 服务")。
- en: This directed graph is derived from the global debt ledger, and the DGFM algorithm
    is executed by the gManager. When a directed cycle is identified, the gManager
    issues requests to the rManager in the corresponding instances, freezing them
    from modifications or cancellations. We set an empirical threshold for the minimum
    number of memory blocks (rBlocks) to be swapped at each node, preventing inefficient
    recall and swap operations on overly small memory blocks. This process significantly
    reduces the need for remote memory access, thereby enhancing data locality, and
    ultimately leading to a noticeable improvement in system performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有向图源自全球债务账本，DGFM 算法由 gManager 执行。当识别到有向循环时，gManager 会向相应的 rManager 发出请求，冻结这些实例的修改或取消。我们为每个节点设置了经验阈值，以规定每次交换的内存块（rBlocks）的最小数量，防止在过小的内存块上进行低效的回忆和交换操作。这个过程显著减少了对远程内存的访问需求，从而增强了数据局部性，并最终带来了系统性能的显著提升。
- en: '![Refer to caption](img/14cd94c2788c4b6274a7131807b36100.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/14cd94c2788c4b6274a7131807b36100.png)'
- en: 'Figure 6: Dataflow of the overlapped computation and communication in prefill
    phase and auto-regressive phase.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：预填充阶段和自动回归阶段的重叠计算和通信的数据流。
- en: 4.6 Communication Optimization
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 通信优化
- en: 'Distributed KV Cache storage faces another challenge: the communication overhead
    of transferring rBlocks back and forth. During the execution of long-context tasks
    in LLM services, both the prefill and auto-regression phases can generate a substantial
    amount of KV Cache, incurring the rManager borrowing remote spaces. We have made
    specific optimizations for both scenarios, as is shown in Figure [6](#S4.F6 "Figure
    6 ‣ 4.5 Fragmented Memory Management ‣ 4 Method ‣ Infinite-LLM: Efficient LLM
    Service for Long Context with DistAttention and Distributed KVCache").'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式 KV 缓存存储面临另一个挑战：来回传输 rBlocks 的通信开销。在 LLM 服务中执行长上下文任务期间，预填充和自动回归阶段都可能生成大量
    KV 缓存，从而导致 rManager 借用远程空间。我们已对这两种情况进行了具体优化，如图[6](#S4.F6 "图 6 ‣ 4.5 碎片化内存管理 ‣
    4 方法 ‣ Infinite-LLM：具有 DistAttention 和分布式 KVCache 的长上下文高效 LLM 服务")所示。
- en: During the prefill phase, the memory demands of the KV Cache can be precisely
    predicted based on the prompt’s length. This foresight enables pre-planned allocation
    of rBlocks—designated as either local or remote depending on their storage location.
    When executing the attention layers of the Transformer block, we overlap the computation
    of attention with the transfer of remote rBlocks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在预填充阶段，可以根据提示的长度精确预测 KV 缓存的内存需求。这种前瞻性使得可以预先计划 rBlocks 的分配——根据其存储位置指定为本地或远程。在执行
    Transformer 块的注意力层时，我们将注意力计算与远程 rBlocks 的传输重叠。
- en: In the auto-regression phase, rBlocks’ allocation are handled dynamically. Simply
    repatriating all rBlocks for local computation incurs excessive network traffic.
    Moreover, given that the attention module’s computation is fundamentally a vector-matrix
    multiplication—a notably memory-intensive task—localizing all computations can
    severely degrade system performance. The innovation of DistAttention  allows us
    to redirect query vectors to the instance containing the remote rBlocks, facilitating
    the macro-attention computations there before sending the results back for integration.
    This approach significantly reduces data transfer volume by a factor of $N$ representing
    the count of tokens in the KV cache. A limitation of this method is its potential
    to vie for computational resources with the host instance of the remote rBlocks.
    To mitigate this, a threshold is established within each rManager, which adjudicates
    the borrowing of computational resources in accordance with local SLA guidelines,
    thereby ensuring a balanced and judicious use of the system’s computational assets.*  *##
    5 Implementation Details
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在自回归阶段，rBlocks 的分配是动态处理的。仅仅将所有 rBlocks 归还以进行本地计算会导致过度的网络流量。此外，由于注意力模块的计算本质上是一个向量-矩阵乘法——一个显著消耗内存的任务——将所有计算本地化可能严重降低系统性能。DistAttention
    的创新使我们能够将查询向量重定向到包含远程 rBlocks 的实例，从而在那进行宏观注意力计算，然后将结果返回进行集成。这种方法通过一个表示 KV 缓存中令牌数量的
    $N$ 系数显著减少数据传输量。该方法的一个局限性是它可能会与远程 rBlocks 的主机实例争夺计算资源。为了缓解这一点，每个 rManager 中建立了一个阈值，根据本地
    SLA 指导方针裁决计算资源的借用，从而确保系统计算资产的平衡和明智使用。*  *## 5 实现细节
- en: 'DistAttention  contains two types of operators, namely DistAttn and ScaleReduce,
    developed with approximately 5,000 lines of C++/CUDA code. The DistAttn operator
    is designed for distributed attention computation, with the results consolidated
    by the ScaleReduce operator to yield the final outcome. To adeptly manage a wide
    range of input context lengths, DistAttn incorporates an adaptive kernel selection
    process based on the dimensions of the inputs. Context lengths are categorized
    into three groups: normal range (0-8k), long range (8k-32k), and ultra-long range
    (>32k), for which we have developed and meticulously optimized three distinct
    kernel templates. Additionally, we have devised and implemented a heuristic approach
    to fine-tune CUDA kernels for specific tensor shapes.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: DistAttention 包含两种类型的操作符，即 DistAttn 和 ScaleReduce，开发了大约 5,000 行的 C++/CUDA 代码。DistAttn
    操作符用于分布式注意力计算，结果由 ScaleReduce 操作符汇总以得出最终结果。为了有效管理各种输入上下文长度，DistAttn 采用了基于输入维度的自适应内核选择过程。上下文长度被分为三组：正常范围（0-8k）、长范围（8k-32k）和超长范围（>32k），我们为此开发并精心优化了三种不同的内核模板。此外，我们还设计并实施了一种启发式方法，以微调
    CUDA 内核以适应特定的张量形状。
- en: On the other hand, DistKV-LLM  adapts the Ray framework[[31](#bib.bib31)] to
    establish a distributed KV Cache management and scheduling system, developed with
    around 12,000 lines of Python code. For effective implementation of requests and
    network data movements, we customize the package encodings and transfers data
    packages with socket, instead of using RPC based framework. To maximize the high
    bandwidth benefits of RDMA[[25](#bib.bib25)], NCCL[[1](#bib.bib1)] is employed
    for cross-instance GPU tensor communication, ensuring efficient data exchange
    among distributed instances.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，DistKV-LLM 适应了 Ray 框架[[31](#bib.bib31)] 来建立一个分布式 KV 缓存管理和调度系统，开发了大约 12,000
    行的 Python 代码。为了有效地实现请求和网络数据传输，我们自定义了数据包编码，并通过套接字传输数据包，而不是使用基于 RPC 的框架。为了最大化 RDMA[[25](#bib.bib25)]
    的高带宽收益，我们采用了 NCCL[[1](#bib.bib1)] 进行跨实例 GPU 张量通信，确保分布式实例之间的高效数据交换。
- en: 6 Evaluation
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 评估
- en: In this section, we present the evaluation results of our work.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了我们工作的评估结果。
- en: Environment.   We deploy DistKV-LLM  on a cluster with 4 nodes and 32 GPUs.
    Each node has 8 NVIDIA A100 (80GB) GPUs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 环境。   我们在一个包含4个节点和32个GPU的集群上部署了**DistKV-LLM**。每个节点配备了8个NVIDIA A100（80GB）GPU。
- en: 'Models.   Our framework can now support most of popular LLMs such as GPT[[13](#bib.bib13),
    [35](#bib.bib35)], LLaMA[[44](#bib.bib44)], BLOOM[[47](#bib.bib47)] etc. Since
    most LLM models have similar backbone Transformer block, we choose one representative
    model, LLaMA2[[44](#bib.bib44)] for evaluation. LLaMA2 family contains three different
    model sizes: 7B, 13B and 70B. They use two popular attention architectures; the
    7B and 13B models utilize Multi-head Attention (MHA)[[46](#bib.bib46)], while
    the 70B model employs Grouped-Query Attention (GQA)[[40](#bib.bib40)].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。   我们的框架现在可以支持大多数流行的LLM，如**GPT**[[13](#bib.bib13), [35](#bib.bib35)]、**LLaMA**[[44](#bib.bib44)]、**BLOOM**[[47](#bib.bib47)]等。由于大多数LLM模型具有相似的基础Transformer块，我们选择了一个代表性模型——**LLaMA2**[[44](#bib.bib44)]进行评估。LLaMA2家族包含三种不同的模型大小：7B、13B和70B。它们使用两种流行的注意力架构；7B和13B模型使用**Multi-head
    Attention (MHA)**[[46](#bib.bib46)]，而70B模型则采用**Grouped-Query Attention (GQA)**[[40](#bib.bib40)]。
- en: Baseline.   We select vLLM[[26](#bib.bib26)], the state-of-the-art LLM serving
    engine, as the primary baseline. Moreover, most previous LLM service systems use
    tensor parallelism. To validate the pipeline parallelism with contiguous batching,
    we implement similar design in Alpa[[52](#bib.bib52)] in vLLM framework as one
    of the baselines.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。   我们选择了**vLLM**[[26](#bib.bib26)]，作为主要基准，vLLM是当前最先进的LLM服务引擎。此外，大多数以前的LLM服务系统使用张量并行。为了验证具有连续批处理的管道并行性，我们在vLLM框架中实现了类似的设计，作为基准之一。
- en: 'Datasets.   We evaluate our system with 18 datasets, categorized into three
    types based on context length distributions. Each dataset comprises 1,000 text-generation
    tasks, derived from scenarios encountered in real-world applications. As is listed
    in Table [2](#S6.T2 "Table 2 ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache"), these datasets
    feature context lengths varying from 1 to 1,900K, with the proportion of long
    context tasks ranging from 1% to 30%.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集。   我们使用18个数据集对系统进行了评估，这些数据集根据上下文长度分布分为三种类型。每个数据集包含1,000个文本生成任务，源于现实世界应用场景。正如表[2](#S6.T2
    "Table 2 ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service for Long Context
    with DistAttention and Distributed KVCache")中列出的，这些数据集的上下文长度从1到1,900K不等，长上下文任务的比例从1%到30%。'
- en: 'Table 2: Datasets for Different Scenarios'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 不同场景的数据集'
- en: '| Model, GPUs | Dataset IDs | Normal Request Range | Long Request Range | Long
    Request Ratio (%) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 模型, GPU | 数据集ID | 普通请求范围 | 长请求范围 | 长请求比例 (%) |'
- en: '| 7B, 2 | 1, 7, 13 | 1-100k | 100k-200k | 1, 10, 30 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 7B, 2 | 1, 7, 13 | 1-100k | 100k-200k | 1, 10, 30 |'
- en: '| 13B, 4 | 2, 8, 14 | 1-140k | 140k-280k | 1, 10, 30 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 13B, 4 | 2, 8, 14 | 1-140k | 140k-280k | 1, 10, 30 |'
- en: '| 70B, 8 | 3, 9, 15 | 1-300k | 300k-600k | 1, 10, 30 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 70B, 8 | 3, 9, 15 | 1-300k | 300k-600k | 1, 10, 30 |'
- en: '| 13B, 8 | 4, 10, 16 | 1-240k | 240k-480k | 1, 10, 30 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 13B, 8 | 4, 10, 16 | 1-240k | 240k-480k | 1, 10, 30 |'
- en: '| 7B, 16 | 5, 11, 17 | 1-600k | 600k-1200k | 1, 10, 30 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 7B, 16 | 5, 11, 17 | 1-600k | 600k-1200k | 1, 10, 30 |'
- en: '| 7B, 32 | 6, 12, 18 | 1-950k | 950k-1900k | 1, 10, 30 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 7B, 32 | 6, 12, 18 | 1-950k | 950k-1900k | 1, 10, 30 |'
- en: 6.1 Context Length Benchmark
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 上下文长度基准测试
- en: We evaluate and compare DistKV-LLM  and the baseline’s performance on different
    context lengths. We evaluate on three models with different context ranges. For
    LLaMA2-7B, we evaluate the task of 1-200k on 2 GPUs, 1-1200k on 16 GPUs, and 1-1900k
    on 32 GPUs respectively. For the LLaMA2-13B model, we tested 1-280k on 4 GPUs,
    and 1-480k on 8 GPUs respectively. For the LLaMA2-70B model, we tested range 1-450k
    on 8 GPUs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估并比较了**DistKV-LLM**和基准在不同上下文长度下的表现。我们在三种具有不同上下文范围的模型上进行了评估。对于LLaMA2-7B模型，我们在2个GPU上评估了1-200k的任务，在16个GPU上评估了1-1200k，在32个GPU上评估了1-1900k。对于LLaMA2-13B模型，我们在4个GPU上测试了1-280k，在8个GPU上测试了1-480k。对于LLaMA2-70B模型，我们在8个GPU上测试了1-450k的范围。
- en: To validate the performance of DistKV-LLM , we compare with two vLLM baseline
    versions. vLLM-v1 contains the same number of GPUs as DistKV-LLM  in a single
    instance.  LABEL:fig:img1 shows the throughput of vLLM-v1, vLLM-v2 and DistKV-LLM 
    across varing context length. Notably, DistKV-LLM(blue) not only achieves a throughput
    comparable to vLLM-v1 (green) but also supports substantially longer context lengths,
    approximately 2x-19x as demonstrated in  LABEL:fig:img1. This improvement is attributed
    to DistKV-LLM’s ability to efficiently coordinate memory usage across all instances,
    while vLLM-v1 is limited to the instance’s private memory.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证DistKV-LLM的性能，我们与两个vLLM基线版本进行了比较。vLLM-v1在单实例中包含与DistKV-LLM相同数量的GPU。LABEL:fig:img1显示了vLLM-v1、vLLM-v2和DistKV-LLM在不同上下文长度下的吞吐量。值得注意的是，DistKV-LLM（蓝色）不仅实现了与vLLM-v1（绿色）相当的吞吐量，还支持显著更长的上下文长度，大约是2倍至19倍，如LABEL:fig:img1所示。这一改进归功于DistKV-LLM能够有效协调所有实例的内存使用，而vLLM-v1则受限于实例的私有内存。
- en: vLLM-v2 is pre-assigned with more GPUs so that it can support comparable context
    length with DistKV-LLM. By comparing with vLLM-v2(red), we demonstrate that DistKV-LLM 
    sustains similar extended context lengths but achieves significantly higher throughput.
    As is shown in  LABEL:fig:img1, DistKV-LLM  achieves 1.4x-5.3x higher throughput
    than vLLM-v2\. This is because DistKV-LLM  can maintain an efficient model parallelism
    strategy while vLLM-v2 partitioning the model into smaller segments across more
    GPUs, which results in lower hardware efficiency.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM-v2预先分配了更多的GPU，以支持与DistKV-LLM相当的上下文长度。通过与vLLM-v2（红色）进行比较，我们展示了DistKV-LLM能够维持类似的扩展上下文长度，但吞吐量显著更高。如LABEL:fig:img1所示，DistKV-LLM的吞吐量比vLLM-v2高出1.4倍至5.3倍。这是因为DistKV-LLM能够保持高效的模型并行策略，而vLLM-v2将模型划分为多个更小的段分布在更多的GPU上，导致硬件效率较低。
- en: '![Refer to caption](img/882ab43e1a65d6df2a80e4ee1dd57fb1.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/882ab43e1a65d6df2a80e4ee1dd57fb1.png)'
- en: 'Figure 7: Throughput of a largest batch of requests with same specified context
    length.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：相同指定上下文长度下最大请求批次的吞吐量。
- en: 6.2 End-to-end Serving Performance
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 端到端服务性能
- en: 'We adopted the experimental setup from  [subsection 6.1](#S6.SS1 "6.1 Context
    Length Benchmark ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service for Long
    Context with DistAttention and Distributed KVCache"), running the corresponding
    context range datasets to evaluate the end-to-end performance of the DistKV-LLM .
    The experiment result is shown in  [Figure 8](#S6.F8.1 "Figure 8 ‣ 6.2 End-to-end
    Serving Performance ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service for Long
    Context with DistAttention and Distributed KVCache"). When the curve rises sharply,
    it indicates that the throughput has reached the system’s limit, requests begin
    to queue up, and latency increases rapidly.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '我们采用了实验设置来自[subsection 6.1](#S6.SS1 "6.1 Context Length Benchmark ‣ 6 Evaluation
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache")，运行了相应的上下文范围数据集，以评估DistKV-LLM的端到端性能。实验结果如[Figure 8](#S6.F8.1
    "Figure 8 ‣ 6.2 End-to-end Serving Performance ‣ 6 Evaluation ‣ Infinite-LLM:
    Efficient LLM Service for Long Context with DistAttention and Distributed KVCache")所示。当曲线急剧上升时，表示吞吐量已达到系统的极限，请求开始排队，延迟迅速增加。'
- en: In the dataset with 1% long requests, DistKV-LLM  achieves an improvement of
    approximately 1.4x to 2.4x over the baseline. This is because splitting the model
    into smaller fragments leads to lower GPU utilization, which considerably reduces
    the efficiency of linear computations. In the dataset with 10% long requests,
    DistKV-LLM  achieves a performance improvement of approximately 1.1x to 1.4x compared
    to the baseline. In a dataset where long requests comprise 30% of the data, DistKV-LLM 
    realizes a performance gain of about 1.03x to 1.3x over the baseline. As the proportion
    of long requests in the dataset increases, the performance gain offered by DistKV-LLM 
    diminishes. This is because when the model processes requests with long context,
    there is a lower ratio of linear computations to attention computations. The performance
    gain that DistKV-LLM  has in the linear component becomes a smaller fraction of
    the overall computational workload, and the attention component’s performance
    does not show a significant advantage over the baseline.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在长请求占比1%的数据集中，DistKV-LLM的性能比基准提高了大约1.4倍到2.4倍。这是因为将模型拆分成较小的片段会导致较低的GPU利用率，从而显著降低线性计算的效率。在长请求占比10%的数据集中，DistKV-LLM的性能提升大约为1.1倍到1.4倍。对于长请求占比30%的数据集，DistKV-LLM的性能提升大约为1.03倍到1.3倍。随着数据集中长请求比例的增加，DistKV-LLM提供的性能提升逐渐减小。这是因为当模型处理长上下文请求时，线性计算与注意力计算的比例降低。DistKV-LLM在线性组件上的性能提升在整体计算负载中占比变小，而注意力组件的性能没有显示出显著的优势。
- en: '![Refer to caption](img/b0c2c66423719f5ecb55e6b69cb6b2b1.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b0c2c66423719f5ecb55e6b69cb6b2b1.png)'
- en: 'Figure 8: End-to-end serving performance'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：端到端服务性能
- en: 6.3 Live Migration
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 实时迁移
- en: An alternative solution to the varying context length is live migration, which
    makes an on-demand transfer to a more capable instance with more GPUs. In this
    experiment, we compare DistKV-LLM and live migration on LLaMA2-7B model. For the
    new instance, LLM model is downloaded through the Amazon Simple Storage Service
    (Amazon S3)[[3](#bib.bib3)] and loaded by vLLM in the format of SafeTensor[[7](#bib.bib7)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 针对变化的上下文长度，另一种解决方案是实时迁移，这会将请求转移到一个具备更多GPU的更强大实例。在此实验中，我们比较了DistKV-LLM和LLaMA2-7B模型上的实时迁移。对于新实例，LLM模型通过亚马逊简单存储服务（Amazon
    S3）[[3](#bib.bib3)]下载，并由vLLM以SafeTensor[[7](#bib.bib7)]格式加载。
- en: 'Initially, we deployed the service using an A100 GPU, which can handle requests
    up to a maximum length of 108k. When the context length exceeds 108k, an additional
    A100 GPU should be utilized for expansion. The result is shown in  [Figure 9](#S6.F9
    "Figure 9 ‣ 6.3 Live Migration ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache"). The horizontal
    axis represents the length of the prompt and the length of the output. The vertical
    axis indicates the latency of generating the corresponding output length. The
    overhead caused by the live migration is 45x that of the communication overhead
    in DistKV-LLM. When the context length is 105k prompt and 5k output, it triggers
    a live migration. In this scenario, the latency of vLLM significantly increases,
    whereas DistKV-LLM  only experiences a negligible disturbance. When generating
    tokens of lengths 5k, 10k, 20k, and 30k, the completion time of DistKV-LLM is
    respectively 3.5$\times$ faster than that of vLLM. This is because migrating the
    entire service results in substantial overhead, which includes remotely downloading
    the model (despite utilizing high-speed download links) and the inference engine
    loading the model. In contrast, DistKV-LLM  merely needs to establish a connection
    with the expanded devices without the need for downloading and loading the model.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们使用A100 GPU部署服务，该GPU可以处理最大长度为108k的请求。当上下文长度超过108k时，应使用额外的A100 GPU进行扩展。结果见[图9](#S6.F9
    "图9 ‣ 6.3 实时迁移 ‣ 6 评估 ‣ Infinite-LLM：使用DistAttention和分布式KVCache的高效LLM服务")。横轴表示提示的长度和输出的长度。纵轴表示生成相应输出长度的延迟。实时迁移造成的开销是DistKV-LLM通信开销的45倍。当上下文长度为105k的提示和5k的输出时，会触发实时迁移。在这种情况下，vLLM的延迟显著增加，而DistKV-LLM仅经历了微不足道的干扰。当生成5k、10k、20k和30k长度的标记时，DistKV-LLM的完成时间分别比vLLM快3.5倍。这是因为迁移整个服务会导致大量开销，包括远程下载模型（尽管使用了高速下载链接）和推理引擎加载模型。相比之下，DistKV-LLM仅需与扩展设备建立连接，无需下载和加载模型。
- en: '![Refer to caption](img/ef597f83ca443a002722b2d45cbff4b9.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ef597f83ca443a002722b2d45cbff4b9.png)'
- en: 'Figure 9: Comparison of live migration overhead.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：实时迁移开销的比较。
- en: 6.4 Optimizing Memory Allocation
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 优化内存分配
- en: 'The dynamicity in variable context length and batching leads to the deterioration
    of data locality. The DGFM algorithm aims to optimize memory allocation by recalling
    lent memory spaces. In this experiment, we deployed a service using DistKV-LLM 
    with four LLaMA2-13B tp2 instances, capable of handling request lengths ranging
    from 1 to 480k. We compared the throughput performance of the service with DGFM
    enabled and without DGFM, and the results are depicted in the  [Figure 10](#S6.F10
    "Figure 10 ‣ 6.4 Optimizing Memory Allocation ‣ 6 Evaluation ‣ Infinite-LLM: Efficient
    LLM Service for Long Context with DistAttention and Distributed KVCache"). In
    the initial phase, the performance of services with DGFM enabled and disabled
    is similar. Over time, the data locality issue begins to emerge. Services with
    DGFM maintain a higher overall throughput by periodically clearing the debt circle
    and optimizing the overall data locality in the distributed environments. In contrast,
    services without DGFM experience an increasingly severe problem with data locality
    deterioration, resulting in a continuous downward trend in overall throughput.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '变量上下文长度和批处理的动态性导致数据局部性的恶化。DGFM算法旨在通过回收借用的内存空间来优化内存分配。在本实验中，我们使用DistKV-LLM 部署了一个服务，配备了四个LLaMA2-13B
    tp2实例，能够处理长度从1到480k的请求。我们比较了启用和未启用DGFM的服务的吞吐量性能，结果如[图10](#S6.F10 "Figure 10 ‣
    6.4 Optimizing Memory Allocation ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM
    Service for Long Context with DistAttention and Distributed KVCache")所示。在初始阶段，启用和未启用DGFM的服务性能类似。随着时间的推移，数据局部性问题开始显现。启用DGFM的服务通过定期清除债务圈并优化分布式环境中的总体数据局部性，保持了更高的整体吞吐量。相比之下，未启用DGFM的服务经历了数据局部性恶化问题的逐渐加剧，导致整体吞吐量持续下降。'
- en: '![Refer to caption](img/4faac31acf95e4baa893ef50cd215cc7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4faac31acf95e4baa893ef50cd215cc7.png)'
- en: 'Figure 10: Throughput Over Time with and without DGFM Enabled.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：启用和禁用DGFM的吞吐量随时间变化。
- en: 6.5 Ablation Study
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 消融研究
- en: 'DistAttention  Breakdown.   DistAttention  introduces the capability for distributed
    storage and computation of attention across instances, which also incurs certain
    overheads. These are primarily due to the transmission costs of the query, key,
    and value tensors for individual tokens, as well as some other overheads such
    as tensor slicing and concatenating. We deployed two instances on 8xA100 GPUs,
    which share storage and computational resources through DistKV-LLM . We conducted
    a breakdown analysis of the runtime of DistAttention  and compared it with the
    attention that is divided into eight parts by Tensor parallelism. The result is
    shown in  [Figure 11](#S6.F11 "Figure 11 ‣ 6.5 Ablation Study ‣ 6 Evaluation ‣
    Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed
    KVCache"). Compared to TP8 Attention, the additional overhead introduces a 5%-10%
    increase in latency. This extra latency is almost constant, which means that as
    the context length increases, the proportion of this overhead becomes increasingly
    smaller.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'DistAttention 分解。DistAttention 引入了跨实例的分布式存储和计算能力，但也带来了某些开销。这些开销主要是由于单个令牌的查询、键和值张量的传输成本，以及其他一些开销，如张量切片和拼接。我们在8xA100
    GPUs上部署了两个实例，通过DistKV-LLM 共享存储和计算资源。我们对DistAttention 的运行时进行了分解分析，并将其与通过张量并行性分成八部分的注意力进行比较。结果如[图11](#S6.F11
    "Figure 11 ‣ 6.5 Ablation Study ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache")所示。与TP8 Attention相比，额外的开销引入了5%-10%的延迟增加。这额外的延迟几乎是恒定的，这意味着随着上下文长度的增加，这种开销的比例变得越来越小。'
- en: '![Refer to caption](img/26a877afc68a0108350cd14ba8a88abc.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/26a877afc68a0108350cd14ba8a88abc.png)'
- en: 'Figure 11: Attention breakdown with different context lengths.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：不同上下文长度的注意力分解。
- en: 'Comparing Remote Compute and Local Compute.   There are two strategies to compute
    the remotely allocated rBlocks, 1) local compute: bring back the KV Cache from
    the remote instance via a high-speed interconnect network to perform the full
    attention computation locally; 2) remote compute: transmit the query vector to
    the remote instance, where the rBlocks locate, to carry out distributed attention
    computations, enabled by DistAttention , and then retrieve the result vector back.
    The comparative results of these two methods are illustrated in the  [Figure 12](#S6.F12
    "Figure 12 ‣ 6.5 Ablation Study ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache"). The latency of
    local compute is significantly higher than that of remote compute, which is attributed
    to the fact that local compute requires transferring a large volume of remote
    KV Cache back to the local instance through the network, constrained by network
    bandwidth, substantially increasing the overall latency. In DistKV-LLM, we use
    this experiment results in the rManger to guide the communication optimizations
    discussed in Section [4.6](#S4.SS6 "4.6 Communication Optimization ‣ 4 Method
    ‣ Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and
    Distributed KVCache").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 比较远程计算和本地计算。   计算远程分配的rBlocks有两种策略：1) 本地计算：通过高速互连网络将KV缓存从远程实例带回，进行本地全注意力计算；2)
    远程计算：将查询向量传输到远程实例（rBlocks所在位置），进行分布式注意力计算，由DistAttention支持，然后将结果向量检索回来。这两种方法的比较结果在[图12](#S6.F12
    "图12 ‣ 6.5 消融研究 ‣ 6 评估 ‣ 无限LLM：利用DistAttention和分布式KVCache提高长上下文效率的LLM服务")中展示。本地计算的延迟显著高于远程计算，这是因为本地计算需要通过网络将大量的远程KV缓存传输回本地实例，受网络带宽的限制，显著增加了整体延迟。在DistKV-LLM中，我们使用这些实验结果在rManger中指导第[4.6节](#S4.SS6
    "4.6 通信优化 ‣ 4 方法 ‣ 无限LLM：利用DistAttention和分布式KVCache提高长上下文效率的LLM服务")中讨论的通信优化。
- en: '![Refer to caption](img/723d15550c07717427b425bd442680f1.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/723d15550c07717427b425bd442680f1.png)'
- en: 'Figure 12: Comparison between Local Compute and Remote Compute.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：本地计算与远程计算的比较。
- en: 7 Related Works
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 相关工作
- en: Existing LLM service systems.   Numerous LLM serving systems have been proposed
    recently. ORCA[[50](#bib.bib50)] has introduced an iteration-level scheduling
    strategy which greatly enhances the computation and memory utilization in batching
    inference. To address the issue of memory wastage resulting from fragmentation
    and redundant replication, vLLM[[26](#bib.bib26)] has developed a Paged KV (Key-Value)
    Cache and Paged Attention mechanism. DeepSpeed-FastGen[[4](#bib.bib4)] has proposed
    a novel prompt and generation composition strategy called Dynamic SplitFuse, which
    is designed to further enhance continuous batching and system throughput. AlpaServe[[27](#bib.bib27)]
    explores the opportunity of statistical multiplexing by model parallelism in the
    scenario of bursty request rate. FasterTransformer[[2](#bib.bib2)] and DeepSpeed
    Inference[[10](#bib.bib10)] have implemented pioneered and extensive kernel-level
    performance optimizations specifically for Transformer models. TGI[[5](#bib.bib5)],
    TensorRT-LLM[[8](#bib.bib8)] and lmdeploy[[6](#bib.bib6)], building upon FasterTransformer,
    have adapted features like Contiguous Batching and Paged Attention. Despite these
    novel systems solve many problems and achieve outstanding results, the dynamic
    problem along with the need to support exceptionally long context lengths still
    remains an unresolved challenge.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的LLM服务系统。   最近提出了许多LLM服务系统。ORCA[[50](#bib.bib50)]引入了一种迭代级调度策略，大大提升了批处理推理中的计算和内存利用率。为了解决由于碎片化和冗余复制导致的内存浪费问题，vLLM[[26](#bib.bib26)]开发了分页KV（键值）缓存和分页注意力机制。DeepSpeed-FastGen[[4](#bib.bib4)]提出了一种新颖的提示和生成组合策略，称为动态拆分融合，旨在进一步提高连续批处理和系统吞吐量。AlpaServe[[27](#bib.bib27)]在请求率突发的情况下，通过模型并行探索统计复用的机会。FasterTransformer[[2](#bib.bib2)]和DeepSpeed
    Inference[[10](#bib.bib10)]专门针对Transformer模型实施了开创性和广泛的内核级性能优化。TGI[[5](#bib.bib5)]、TensorRT-LLM[[8](#bib.bib8)]和lmdeploy[[6](#bib.bib6)]在FasterTransformer的基础上，适配了连续批处理和分页注意力等特性。尽管这些新系统解决了许多问题并取得了优异的结果，但动态问题以及支持异常长上下文长度的需求仍然是一个未解决的挑战。
- en: 'Comparison to Ring Attention.   Ring Attention[[29](#bib.bib29), [28](#bib.bib28)]
    was introduced as a method to distribute long sequences across multiple devices,
    with the intent of fully overlapping the communication of key-value (KV) blocks
    with the computation of blockwise attention. This approach is highly efficient
    for training with long sequences and for the prefill phase during inference. However,
    when it comes to the decoding phase in inference, the transfer of KV blocks between
    devices cannot be concealed by computation leading to substantial overhead.  [Figure 12](#S6.F12
    "Figure 12 ‣ 6.5 Ablation Study ‣ 6 Evaluation ‣ Infinite-LLM: Efficient LLM Service
    for Long Context with DistAttention and Distributed KVCache") depicts the overhead
    of transferring KV blocks is significantly higher than communication of DistAttention .'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '对比环形注意力。  环形注意力[[29](#bib.bib29), [28](#bib.bib28)]被引入作为一种将长序列分配到多个设备上的方法，旨在将键值（KV）块的通信完全与块状注意力的计算重叠。这种方法在处理长序列的训练和推理阶段的预填充阶段中非常高效。然而，在推理的解码阶段，KV
    块在设备之间的传输不能被计算所掩盖，导致了显著的开销。[图12](#S6.F12 "图 12 ‣ 6.5 消融研究 ‣ 6 评估 ‣ Infinite-LLM:
    具有DistAttention和分布式KVCache的长上下文高效LLM服务")显示了KV块传输的开销显著高于DistAttention的通信。'
- en: Solutions for Long Context Serving.   Another category of methods to address
    the challenge of managing oversized Key-Value (KV) Cache for long-context inference
    involves sparse KV Caches, such as Sliding Window Attention[[24](#bib.bib24),
    [16](#bib.bib16), [12](#bib.bib12)]. This technique only requires maintaining
    a KV Cache the size of the window. Both H2O[[51](#bib.bib51)] and StreamingLLM[[48](#bib.bib48)]
    also retain a fixed window size for the KV Cache, but they mitigate the precision
    loss due to context information discarding by employing a KV cache eviction algorithm
    and incorporating an Attention Sink, respectively. However, since these methods
    discard some context information, they inevitably compromise the effectiveness
    of Large Language Models (LLMs) to some extent.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 处理长上下文服务的解决方案。  另一类应对处理过大的键值（KV）缓存以应对长上下文推理挑战的方法涉及稀疏KV缓存，如滑动窗口注意力[[24](#bib.bib24),
    [16](#bib.bib16), [12](#bib.bib12)]。这种技术仅需维护与窗口大小相同的KV缓存。H2O[[51](#bib.bib51)]和StreamingLLM[[48](#bib.bib48)]也为KV缓存保留固定窗口大小，但它们通过使用KV缓存逐出算法和分别引入注意力汇来缓解由于上下文信息丢弃造成的精度损失。然而，由于这些方法丢弃了一些上下文信息，因此它们不可避免地在一定程度上妥协了大型语言模型（LLMs）的有效性。
- en: 8 Conclusion
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: The dynamic, auto-regressive nature of LLM inference computation poses significant
    challenges to LLM service on the cloud, especially for tasks with long-context
    sequences. Addressing these challenges, we introduce DistAttention, an innovative
    distributed attention algorithm that efficiently segments the KV cache into manageable
    units for distributed processing. Complementing this, DistKV-LLM, a distributed
    LLM service engine which excels in KV Cache management, optimally coordinates
    memory usage across the data center. This combination of DistAttention  and DistKV-LLM 
    effectively ensures a smooth and efficient cloud service for LLMs especially when
    handling long-context tasks. In a comprehensive evaluation using 32 NVIDIA A100
    GPUs and 18 datasets, our system showed 1.03-2.4 $\times$ longer, demonstrating
    its effectiveness in managing a wide range of context-generation tasks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: LLM推理计算的动态、自回归特性对云中的LLM服务提出了重大挑战，特别是对于处理长上下文序列的任务。为了解决这些挑战，我们引入了DistAttention，这是一种创新的分布式注意力算法，能够高效地将KV缓存分段成可管理的单元以进行分布式处理。配合使用的DistKV-LLM，是一种在KV缓存管理方面表现突出的分布式LLM服务引擎，优化了数据中心的内存使用。DistAttention和DistKV-LLM的结合有效地确保了LLM在处理长上下文任务时的平稳高效的云服务。在使用32个NVIDIA
    A100 GPU和18个数据集进行的综合评估中，我们的系统显示出1.03-2.4 $\times$ 的更长时间，证明了它在管理广泛上下文生成任务方面的有效性。
- en: References
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Nvidia collective communication library. [https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html),
    2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Nvidia集体通信库. [https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html),
    2020.'
- en: '[2] Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer),
    2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer),
    2021.'
- en: '[3] Amazon s3: Object storage built to retrieve any amount of data from anywhere.
    [https://aws.amazon.com/s3](https://aws.amazon.com/s3), 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Amazon s3: 设计用来从任何地方检索任意数量数据的对象存储。 [https://aws.amazon.com/s3](https://aws.amazon.com/s3),
    2023.'
- en: '[4] Deepspeed-fastgen: High-throughput text generation for llms via mii and
    deepspeed-inference. [https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen),
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Deepspeed-fastgen: 通过 mii 和 deepspeed-inference 实现高吞吐量的文本生成。 [https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen),
    2023.'
- en: '[5] Large language model text generation inference. [https://huggingface.co/docs/text-generation-inference](https://huggingface.co/docs/text-generation-inference),
    2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 大型语言模型文本生成推理。 [https://huggingface.co/docs/text-generation-inference](https://huggingface.co/docs/text-generation-inference),
    2023.'
- en: '[6] Lmdeploy. [https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy),
    2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Lmdeploy. [https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy),
    2023.'
- en: '[7] Simple, safe way to store and distribute tensors. [https://huggingface.co/docs/safetensors](https://huggingface.co/docs/safetensors),
    2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 存储和分发张量的简单、安全方法。 [https://huggingface.co/docs/safetensors](https://huggingface.co/docs/safetensors),
    2023.'
- en: '[8] Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2023.'
- en: '[9] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani,
    and Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes
    with chunked prefills. arXiv preprint arXiv:2308.16369, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani,
    和 Ramachandran Ramjee. Sarathi: 通过与分块预填充一起解码实现高效的 llm 推理。arXiv 预印本 arXiv:2308.16369,
    2023.'
- en: '[10] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li,
    Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley,
    et al. Deepspeed-inference: enabling efficient inference of transformer models
    at unprecedented scale. In SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–15\. IEEE, 2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li,
    Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley 等人。Deepspeed-inference:
    实现前所未有规模的变换器模型高效推理。SC22: 国际高性能计算、网络、存储与分析会议，页码 1–15。IEEE, 2022.'
- en: '[11] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen 等人。Palm
    2 技术报告。arXiv 预印本 arXiv:2305.10403, 2023.'
- en: '[12] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. arXiv preprint arXiv:2004.05150, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Iz Beltagy, Matthew E Peters, 和 Arman Cohan. Longformer: 长文档变换器。arXiv
    预印本 arXiv:2004.05150, 2020.'
- en: '[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等人。语言模型是少量样本学习者。神经信息处理系统进展, 33:1877–1901, 2020.'
- en: '[14] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu,
    Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large language
    models, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu,
    Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qiang Yang, 和 Xing Xie. 大型语言模型评估的调查，2023.'
- en: '[15] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,
    Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex
    Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry,
    Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
    Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski
    Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
    Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
    Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
    Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec
    Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder,
    Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
    Evaluating large language models trained on code, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 马克·陈、杰瑞·特沃雷克、许宇俊、齐明·袁、恩里克·庞德·德·奥利维拉·平托、贾里德·卡普兰、哈里·爱德华兹、尤里·布尔达、尼古拉斯·约瑟夫、格雷格·布罗克曼、亚历克斯·雷、劳尔·普里、格雷琴·克鲁格、迈克尔·彼得罗夫、海蒂·赫拉夫、吉里什·萨斯特里、帕梅拉·米什金、布鲁克·陈、斯科特·格雷、尼克·赖德、米哈伊尔·帕夫洛夫、阿莱西娅·鲍尔、卢卡斯·凯瑟、穆罕默德·巴瓦里安、克莱门斯·温特、菲利普·蒂莱、费利佩·佩特罗斯基·苏赫、戴夫·卡明斯、马蒂亚斯·普拉普特、福提奥斯·钱茨、伊丽莎白·巴恩斯、阿里尔·赫伯特-沃斯、威廉·赫布根·古斯、亚历克斯·尼科尔、亚历克斯·佩诺、尼古拉斯·特扎克、姜杰、伊戈尔·巴布什金、苏奇尔·巴拉吉、尚塔努·简、威廉·桑德斯、克里斯托弗·赫斯、安德鲁·N·卡尔、简·莱克、乔希·阿基亚姆、维丹特·米斯拉、埃文·莫里卡瓦、亚历克·拉德福、马修·奈特、迈尔斯·布伦达奇、米拉·穆拉提、凯蒂·梅耶、彼得·维林德、鲍勃·麦克格鲁、达里奥·阿莫代、萨姆·麦克坎利什、伊利亚·苏茨克维尔和沃伊切赫·扎伦巴。《评估大型语言模型在代码训练上的表现》，2021年。'
- en: '[16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 瑞温·查德、斯科特·格雷、亚历克·拉德福和伊利亚·苏茨克维尔。《生成长序列的稀疏变压器》，2019年。'
- en: '[17] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan
    Wu, Guoping Long, Jun Yang, Lixue Xia, et al. Dapple: A pipelined data parallel
    approach for training large models. In Proceedings of the 26th ACM SIGPLAN Symposium
    on Principles and Practice of Parallel Programming, pages 431–445, 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 邵清范、易荣、陈萌、宗彦·曹、思宇·王、郑震、川吴、郭鹏·龙、杨俊、李学·夏等。《Dapple: 用于训练大模型的流水线数据并行方法》。在第26届ACM
    SIGPLAN并行编程原则与实践研讨会上，页码431–445，2021年。'
- en: '[18] Pin Gao, Lingfan Yu, Yongwei Wu, and Jinyang Li. Low latency rnn inference
    with cellular batching. In Proceedings of the Thirteenth EuroSys Conference, pages
    1–15, 2018.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Pin Gao、凌帆·余、吴永伟、李晋扬。《低延迟RNN推理与细胞批处理》。在第十三届EuroSys会议论文集中，页码1–15，2018年。'
- en: '[19] Github. https://github.com/features/copilot, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Github。https://github.com/features/copilot，2022年。'
- en: '[20] Google. https://bard.google.com, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 谷歌。https://bard.google.com，2023年。'
- en: '[21] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang.
    Lm-infinite: Simple on-the-fly length generalization for large language models.
    arXiv preprint arXiv:2308.16137, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 韩驰、王启凡、熊文涵、陈玉、季恒和王四农。《Lm-infinite: 大型语言模型的简单在线长度泛化》。arXiv预印本 arXiv:2308.16137，2023年。'
- en: '[22] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
    Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient
    training of giant neural networks using pipeline parallelism. Advances in neural
    information processing systems, 32, 2019.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 黄燕平、程友龙、安库尔·巴普纳、奥尔汗·菲拉特、陈德浩、陈佳、李赫钟、阮济全、阮奎克·V·李、吴永辉等。《Gpipe: 利用管道并行性高效训练大型神经网络》。神经信息处理系统进展，32，2019年。'
- en: '[23] Zhihao Jia, Matei Zaharia, and Alex Aiken. Beyond data and model parallelism
    for deep neural networks. Proceedings of Machine Learning and Systems, 1:1–13,
    2019.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 贾志浩、马泰·扎哈里亚、亚历克斯·艾肯。《超越数据和模型并行性：深度神经网络的新方法》。机器学习与系统会议论文集，1:1–13，2019年。'
- en: '[24] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El
    Sayed. Mistral 7b, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 阿尔伯特·Q·姜、亚历山大·萨布莱罗勒斯、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦基尔、吉约姆·兰普勒、吕西尔·索尼尔、莱利奥·雷纳德·拉沃、玛丽-安·拉肖、皮埃尔·斯托克、特文·勒·斯卡奥、蒂博·拉夫里尔、托马斯·王、蒂莫西·拉克鲁瓦、威廉·埃尔·萨耶德。《Mistral
    7b》，2023年。'
- en: '[25] Anuj Kalia, Michael Kaminsky, and David G. Andersen. Using rdma efficiently
    for key-value services. ACM SIGCOMM Computer Communication Review, 44:295 – 306,
    2014.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 阿努杰·卡利亚、迈克尔·卡明斯基、戴维·G·安德森。《高效使用RDMA进行键值服务》。ACM SIGCOMM计算机通信评论，44:295 –
    306，2014年。'
- en: '[26] W Kwon, Z Li, S Zhuang, et al. Efficient memory management for large language
    model serving with pagedattention. In Proceedings of the 29th Symposium on Operating
    Systems Principles, pages 611–626, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] W Kwon, Z Li, S Zhuang, 等. 带分页注意力的大型语言模型服务的高效内存管理. 在第29届操作系统原理研讨会会议录中,
    页码 611–626, 2023.'
- en: '[27] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin
    Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, et al. Alpaserve:
    Statistical multiplexing with model parallelism for deep learning serving. arXiv
    preprint arXiv:2302.11665, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin
    Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez, 等. Alpaserve:
    结合模型并行的深度学习服务的统计复用. arXiv预印本 arXiv:2302.11665, 2023.'
- en: '[28] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context
    large models. arXiv preprint arXiv:2305.19370, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Hao Liu 和 Pieter Abbeel. 针对长上下文大型模型的分块并行变换器. arXiv预印本 arXiv:2305.19370,
    2023.'
- en: '[29] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise
    transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Hao Liu, Matei Zaharia, 和 Pieter Abbeel. 针对接近无限上下文的环形注意力与分块变换器. arXiv预印本
    arXiv:2310.01889, 2023.'
- en: '[30] Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, and Erik Cambria. Gpteval:
    A survey on assessments of chatgpt and gpt-4. arXiv preprint arXiv:2308.12488,
    2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Rui Mao, Guanyi Chen, Xulang Zhang, Frank Guerin, 和 Erik Cambria. Gpteval:
    ChatGPT和GPT-4评估调查. arXiv预印本 arXiv:2308.12488, 2023.'
- en: '[31] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard
    Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan,
    et al. Ray: A distributed framework for emerging $\{$ applications. In 13th USENIX
    symposium on operating systems design and implementation (OSDI 18), pages 561–577,
    2018.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard
    Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan,
    等. Ray: 面向新兴$\{$应用的分布式框架. 在第13届USENIX操作系统设计与实现研讨会（OSDI 18）中, 页码 561–577, 2018.'
- en: '[32] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R
    Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized
    pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium
    on Operating Systems Principles, pages 1–15, 2019.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil
    R Devanur, Gregory R Ganger, Phillip B Gibbons, 和 Matei Zaharia. Pipedream: DNN训练的通用管道并行.
    在第27届ACM操作系统原理研讨会会议录中, 页码 1–15, 2019.'
- en: '[33] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
    Memory-efficient pipeline-parallel dnn training. In International Conference on
    Machine Learning, pages 7937–7947\. PMLR, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, 和 Matei Zaharia.
    内存高效的管道并行DNN训练. 在国际机器学习会议上, 页码 7937–7947. PMLR, 2021.'
- en: '[34] OpenAI. https://openai.com/blog/chatgpt, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] OpenAI. https://openai.com/blog/chatgpt, 2022.'
- en: '[35] OpenAI. Gpt-4 technical report, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] OpenAI. GPT-4技术报告, 2023.'
- en: '[36] Pankesh Patel, Ajith H Ranabahu, and Amit P Sheth. Service level agreement
    in cloud computing. 2009.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Pankesh Patel, Ajith H Ranabahu, 和 Amit P Sheth. 云计算中的服务水平协议. 2009.'
- en: '[37] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn:
    Efficient context window extension of large language models, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, 和 Enrico Shippole. Yarn: 大型语言模型的高效上下文窗口扩展,
    2023.'
- en: '[38] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. Proceedings of Machine Learning and Systems, 5,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, 和 Jeff Dean. 高效扩展变换器推理.
    机器学习与系统会议录, 5, 2023.'
- en: '[39] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 等. 通过生成预训练提高语言理解.
    2018.'
- en: '[40] Noam Shazeer. Fast transformer decoding: One write-head is all you need.
    arXiv preprint arXiv:1911.02150, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Noam Shazeer. 快速变换器解码: 一个写头就足够了. arXiv预印本 arXiv:1911.02150, 2019.'
- en: '[41] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
    Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language
    models using model parallelism, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared
    Casper, 和 Bryan Catanzaro. Megatron-lm: 使用模型并行训练多十亿参数语言模型, 2020.'
- en: '[42] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text
    with recurrent neural networks. In Proceedings of the 28th international conference
    on machine learning (ICML-11), pages 1017–1024, 2011.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Ilya Sutskever, James Martens, 和 Geoffrey E Hinton。使用递归神经网络生成文本。发表于第28届国际机器学习大会（ICML-11）论文集，第1017–1024页，2011年。'
- en: '[43] Salmonn Talebi, Elizabeth Tong, and Mohammad RK Mofrad. Beyond the hype:
    Assessing the performance, trustworthiness, and clinical suitability of gpt3\.
    5. arXiv preprint arXiv:2306.15887, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Salmonn Talebi, Elizabeth Tong, 和 Mohammad RK Mofrad。超越炒作: 评估gpt3\. 5的性能、可靠性和临床适用性。arXiv预印本
    arXiv:2306.15887，2023年。'
- en: '[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等。Llama 2: 开放基础和微调聊天模型。arXiv预印本 arXiv:2307.09288，2023年。'
- en: '[45] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk
    Michalewski, and Piotr Miłoś. Focused transformer: Contrastive training for context
    scaling. arXiv preprint arXiv:2307.03170, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk
    Michalewski, 和 Piotr Miłoś。专注的变换器：对比训练以进行上下文扩展。arXiv预印本 arXiv:2307.03170，2023年。'
- en: '[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin。注意力机制是你所需的一切。神经信息处理系统进展，30，2017年。'
- en: '[47] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
    Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni,
    François Yvon, et al. Bloom: A 176b-parameter open-access multilingual language
    model. arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
    Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni,
    François Yvon 等。Bloom: 一个176b参数的开放访问多语言模型。arXiv预印本 arXiv:2211.05100，2022年。'
- en: '[48] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
    2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis。高效的流式语言模型与注意力汇聚。arXiv预印本
    arXiv:2309.17453，2023年。'
- en: '[49] Eric P Xing, Qirong Ho, Wei Dai, Jin-Kyu Kim, Jinliang Wei, Seunghak Lee,
    Xun Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu. Petuum: A new platform
    for distributed machine learning on big data. In Proceedings of the 21th ACM SIGKDD
    International Conference on Knowledge Discovery and Data Mining, pages 1335–1344,
    2015.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Eric P Xing, Qirong Ho, Wei Dai, Jin-Kyu Kim, Jinliang Wei, Seunghak Lee,
    Xun Zheng, Pengtao Xie, Abhimanu Kumar, 和 Yaoliang Yu。Petuum: 一个平台用于大数据分布式机器学习。发表于第21届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集，第1335–1344页，2015年。'
- en: '[50] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon
    Chun. Orca: A distributed serving system for $\{$ generative models. In 16th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538,
    2022.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, 和 Byung-Gon
    Chun。Orca: 一个用于$\{$生成模型的分布式服务系统。发表于第16届USENIX操作系统设计与实现研讨会（OSDI 22），第521–538页，2022年。'
- en: '[51] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang,
    and Beidi Chen. H[2]o: Heavy-hitter oracle for efficient generative inference
    of large language models, 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang,
    和 Beidi Chen。H[2]o: 用于大型语言模型高效生成推理的重型预言机，2023年。'
- en: '[52] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating
    inter-and $\{$ parallelism for distributed deep learning. In 16th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 22), pages 559–578, 2022.*'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing 等。Alpa: 自动化处理分布式深度学习中的内在和$\{$并行性。发表于第16届USENIX操作系统设计与实现研讨会（OSDI
    22），第559–578页，2022年。'
