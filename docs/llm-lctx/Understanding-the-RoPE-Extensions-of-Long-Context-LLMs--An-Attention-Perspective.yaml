- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:56:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:56:18'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解长上下文LLMs的RoPE扩展：一个注意力视角
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13282](https://ar5iv.labs.arxiv.org/html/2406.13282)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.13282](https://ar5iv.labs.arxiv.org/html/2406.13282)
- en: Meizhi Zhong    Chen Zhang    Yikun Lei    Xikai Liu    Yan Gao    Yao Hu   
    Kehai Chen    Min Zhang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Meizhi Zhong    Chen Zhang    Yikun Lei    Xikai Liu    Yan Gao    Yao Hu   
    Kehai Chen    Min Zhang
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Enabling LLMs to handle lengthy context is currently a research hotspot. Most
    LLMs are built upon rotary position embedding (RoPE), a popular position encoding
    method. Therefore, a prominent path is to extrapolate the RoPE trained on comparably
    short texts to far longer texts. A heavy bunch of efforts have been dedicated
    to boosting the extrapolation via extending the formulations of the RoPE, however,
    few of them have attempted to showcase their inner workings comprehensively. In
    this paper, we are driven to offer a straightforward yet in-depth understanding
    of RoPE extensions from an attention perspective and on two benchmarking tasks.
    A broad array of experiments reveals several valuable findings: 1) Maintaining
    attention patterns to those at the pretrained length improves extrapolation; 2)
    Large attention uncertainty leads to retrieval errors; 3) Using longer continual
    pretraining lengths for RoPE extensions could reduce attention uncertainty and
    significantly enhance extrapolation.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使LLMs能够处理长上下文目前是一个研究热点。大多数LLMs基于旋转位置嵌入（RoPE），这是一种流行的位置编码方法。因此，一个显著的路径是将RoPE在相对较短的文本上训练的结果推广到更长的文本上。大量的努力已致力于通过扩展RoPE的公式来提高外推能力，但很少有尝试全面展示其内部工作原理的研究。在本文中，我们旨在从注意力的角度以及在两个基准任务上提供对RoPE扩展的直接但深入的理解。一系列广泛的实验揭示了几项有价值的发现：1）保持与预训练长度一致的注意力模式可改善外推；2）大范围的注意力不确定性会导致检索错误；3）对RoPE扩展使用更长的持续预训练长度可以减少注意力不确定性，并显著增强外推能力。
- en: LLMs, Long Context, Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs, 长上下文, 机器学习, ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) (Radford et al., [2018](#bib.bib18); Touvron et al.,
    [2023](#bib.bib22); Zhang et al., [2023](#bib.bib24); Brown et al., [2020](#bib.bib4))
    have accommodated a wide range of natural language processing applications, such
    as code completion (Rozière et al., [2023](#bib.bib19)) and question answering
    (Kamalloo et al., [2023](#bib.bib12); Jiang et al., [2021](#bib.bib10); Su et al.,
    [2019](#bib.bib20)). However, a notable challenge limiting further customization
    is possibly the inability of LLMs to utilize context beyond the pretrained length
    (Minaee et al., [2024](#bib.bib16); Chen et al., [2023b](#bib.bib6)) due to the
    inherent flaw of rotary position embedding (RoPE) being used. Fortunately, RoPE
    extensions emerge as key ingredients to enabling LLMs to leverage extended context
    that exceeds pretrained scope (Chen et al., [2023b](#bib.bib6); Peng et al., [2023](#bib.bib17);
    Liu et al., [2023](#bib.bib14); Han et al., [2023](#bib.bib8); Rozière et al.,
    [2023](#bib.bib19)). These RoPE extensions focus on improving performance on long
    texts, yet frustratingly, only a few of them (Liu et al., [2023](#bib.bib14);
    Han et al., [2023](#bib.bib8); Men et al., [2024](#bib.bib15)) have delved into
    the underlying mechanisms in a complicated way.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（Radford 等，[2018](#bib.bib18)；Touvron 等，[2023](#bib.bib22)；Zhang
    等，[2023](#bib.bib24)；Brown 等，[2020](#bib.bib4)）已经适应了广泛的自然语言处理应用，如代码补全（Rozière
    等，[2023](#bib.bib19)）和问答（Kamalloo 等，[2023](#bib.bib12)；Jiang 等，[2021](#bib.bib10)；Su
    等，[2019](#bib.bib20)）。然而，一个显著的挑战限制了进一步的定制，那就是LLMs可能无法利用超出预训练长度的上下文（Minaee 等，[2024](#bib.bib16)；Chen
    等，[2023b](#bib.bib6)），这归因于使用的旋转位置嵌入（RoPE）的固有缺陷。幸运的是，RoPE 扩展作为关键要素使 LLMs 能够利用超出预训练范围的扩展上下文（Chen
    等，[2023b](#bib.bib6)；Peng 等，[2023](#bib.bib17)；Liu 等，[2023](#bib.bib14)；Han 等，[2023](#bib.bib8)；Rozière
    等，[2023](#bib.bib19)）。这些 RoPE 扩展专注于提高在长文本上的表现，但令人沮丧的是，只有少数几项（Liu 等，[2023](#bib.bib14)；Han
    等，[2023](#bib.bib8)；Men 等，[2024](#bib.bib15)）以复杂的方式深入探讨了其内在机制。
- en: This demands us to systematically analyze common RoPE extensions more straightforwardly,
    from the perspective of attention (Vaswani et al., [2017](#bib.bib23)). We include
    three widely-used RoPE extensions, i.e., position interpolation (Chen et al.,
    [2023b](#bib.bib6)), YaRN (Peng et al., [2023](#bib.bib17)), and NTK-Aware interpolation (Rozière
    et al., [2023](#bib.bib19)). To our best knowledge, there is simply no research
    in understanding RoPE extensions for long-context models thoroughly from an attention
    perspective.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求我们从注意力的角度（Vaswani et al., [2017](#bib.bib23)）系统地更直接地分析常见的RoPE扩展。我们包括了三种广泛使用的RoPE扩展，即位置插值（Chen
    et al., [2023b](#bib.bib6)），YaRN（Peng et al., [2023](#bib.bib17)），以及NTK-Aware插值（Rozière
    et al., [2023](#bib.bib19)）。据我们所知，目前尚无从注意力角度全面理解长上下文模型的RoPE扩展的研究。
- en: As a start, we strive to primarily study these methods on a long-context perplexity
    test (PPL), and empirically compare their corresponding attention patterns. We
    discover that finetuning LLMs with these RoPE-extension methods accordingly in
    the pretrained length principally lifts their extrapolation performance. Particularly
    with the NTK-Aware interpolation method, one can extrapolate up to 32$\times$
    beyond the pretrained length. To unleash the reasons behind the successes of these
    methods, we collect the attention scores respectively distributed in 2K and 8K
    lengths during inference. The results demonstrate that these methods maintain
    attention patterns consistent with those observed at the pretrained length. In
    contrast, the attention patterns of the RoPE are substantially deviated.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开始，我们努力主要在一个长上下文困惑度测试（PPL）上研究这些方法，并实证比较它们的相应注意力模式。我们发现，通过相应地微调LLM与这些RoPE扩展方法，预训练长度的外推性能得到了显著提升。特别是使用NTK-Aware插值方法，可以将外推扩展到预训练长度的32$\times$。为了揭示这些方法成功的原因，我们分别收集了推理过程中分布在2K和8K长度中的注意力分数。结果表明，这些方法保持了与预训练长度一致的注意力模式。相比之下，RoPE的注意力模式有显著偏差。
- en: 'Afterward, following literature (Hu et al., [2024](#bib.bib9); Fu et al., [2024](#bib.bib7)),
    we examine these RoPE extensions on a more challenging long-context test called
    Needle-in-a-Haystack (Needle) (Kamradt, [2023](#bib.bib13)). We find that the
    RoPE extensions could pass more tests than the RoPE does. Nonetheless, as the
    context length increased, the RoPE extensions could hardly locate the needles.
    We associate the observation with attention uncertainty, specifically in the form
    of attention entropy. We uncover that large uncertainty leads to retrieval errors:
    the positions that incur large attention entropy are exactly where the incorrect
    answers are borrowed from.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，参考文献（Hu et al., [2024](#bib.bib9)；Fu et al., [2024](#bib.bib7)），我们在一个更具挑战性的长上下文测试中检验这些RoPE扩展，称为针头在干草堆中（Needle）（Kamradt,
    [2023](#bib.bib13)）。我们发现RoPE扩展比RoPE通过了更多的测试。然而，随着上下文长度的增加，RoPE扩展几乎无法找到针头。我们将这一观察与注意力不确定性联系起来，特别是以注意力熵的形式。我们发现，大的不确定性导致了检索错误：产生大注意力熵的位置正是错误答案被借用的地方。
- en: We further hypothesize that this large attention uncertainty stems from a mismatch
    between the context lengths in training and inference. Inspired by the conjecture,
    a natural way to ease the mismatch is to directly train on longer texts. Experimental
    results exhibit that, with the same amount of training tokens consumed, using
    examples with longer contexts largely alleviates uncertainty. Thereby, the ability
    to digest long texts is promoted.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步假设，这种大的注意力不确定性源于训练和推理中的上下文长度不匹配。受到这一猜想的启发，一种自然的缓解不匹配的方法是直接在更长的文本上进行训练。实验结果表明，在消耗相同数量的训练标记的情况下，使用具有较长上下文的示例可以大大减轻不确定性。因此，消化长文本的能力得到提升。
- en: 'Our key contributions can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的关键贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We study various RoPE extensions for length extrapolation in perplexity testing
    and find that the effectiveness could be yielded from maintaining the original
    attention patterns.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们研究了用于长度外推的各种RoPE扩展，并发现通过保持原始注意力模式可以实现有效性。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze these methods using advanced Needle testing and observe that they
    may fail to extrapolate to regions where large attention uncertainty persists.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用先进的Needle测试分析了这些方法，并观察到它们可能无法外推到存在大量注意力不确定性的区域。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We hypothesize that large attention uncertainty stems from a context length
    mismatch between training and inference. It is possible to reduce this large uncertainty
    by minimizing the mismatch through continual training with lengths closer to those
    in inference.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们假设，大量注意力不确定性源于训练与推理之间的上下文长度不匹配。通过持续训练，使长度更接近推理中的长度，有可能减少这种较大的不确定性。
- en: 2 Backgroud
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Target LLMs
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 目标 LLM
- en: 'We consider LLaMa series at different sizes to conduct experiments, including
    MiniMA-2-3B (Zhang et al., [2023](#bib.bib24)), LLaMa-2-7B, and LLaMa-2-13B (Touvron
    et al., [2023](#bib.bib22)). All these mentioned LLMs consistently use rotary
    position embeddings to take position information into consideration. Owing to
    space limitation, we only present the experimental results for LLaMa-2-7B, and
    the results for MiniMA-2-3B and LLaMa-2-13B, share similar trends with those for
    LLaMa-2-7B, as shown in Appendix [A](#A1 "Appendix A Experimental Results on MiniMA-2-3B
    ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective")
    and [B](#A2 "Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了不同尺寸的 LLaMa 系列进行实验，包括 MiniMA-2-3B（Zhang et al., [2023](#bib.bib24)）、LLaMa-2-7B
    和 LLaMa-2-13B（Touvron et al., [2023](#bib.bib22)）。所有这些提到的 LLM 一致地使用旋转位置嵌入来考虑位置信息。由于空间限制，我们仅展示了
    LLaMa-2-7B 的实验结果，MiniMA-2-3B 和 LLaMa-2-13B 的结果与 LLaMa-2-7B 的结果趋势相似，如附录 [A](#A1
    "附录 A MiniMA-2-3B 实验结果 ‣ 从注意力角度理解 RoPE 扩展") 和 [B](#A2 "附录 B LLaMa-2-13B 实验结果 ‣
    从注意力角度理解 RoPE 扩展") 所示。
- en: 2.2 RoPE and Its Extensions
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 RoPE 及其扩展
- en: Rotary Position Embedding (RoPE).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转位置嵌入（RoPE）。
- en: 'Before diving into RoPE extensions, we first briefly describe RoPE itself.
    The use of RoPE (Su et al., [2021](#bib.bib21)) has become pervasive in contemporary
    LLMs (Touvron et al., [2023](#bib.bib22); Bai et al., [2023](#bib.bib2); Bi et al.,
    [2024](#bib.bib3)). RoPE encodes the position information of tokens with a rotation
    tensor that naturally incorporates explicit relative position dependency. To illustrate,
    given a hidden vector ${\bm{h}}=[h_{0},h_{1},...,h_{d-1}]$, RoPE operates as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨 RoPE 扩展之前，我们首先简要描述 RoPE 本身。RoPE（Su et al., [2021](#bib.bib21)）在当代 LLM
    中变得极为普遍（Touvron et al., [2023](#bib.bib22)；Bai et al., [2023](#bib.bib2)；Bi et
    al., [2024](#bib.bib3)）。RoPE 通过旋转张量编码标记的位置信息，这种方法自然地包含了显式的相对位置依赖。举例来说，给定一个隐藏向量
    ${\bm{h}}=[h_{0},h_{1},...,h_{d-1}]$，RoPE 的操作如下：
- en: '|  |  $$f({\bm{h}},m)=\begin{pmatrix}h_{0}\\ h_{1}\\'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '|  |  $$f({\bm{h}},m)=\begin{pmatrix}h_{0}\\ h_{1}\\'
- en: h_{2}\\
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: h_{2}\\
- en: h_{3}\\
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: h_{3}\\
- en: \vdots\\
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: h_{d-2}\\
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: h_{d-2}\\
- en: h_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{0}}\\
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: h_{d-1}\end{pmatrix}\otimes\begin{pmatrix}\cos{m\theta_{0}}\\
- en: \cos{m\theta_{0}}\\
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{0}}\\
- en: \cos{m\theta_{1}}\\
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{1}}\\
- en: \cos{m\theta_{1}}\\
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{1}}\\
- en: \vdots\\
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \cos{m\theta_{d/2-1}}\\
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{d/2-1}}\\
- en: \cos{m\theta_{d/2-1}}\end{pmatrix}+\begin{pmatrix}-h_{1}\\
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \cos{m\theta_{d/2-1}}\end{pmatrix}+\begin{pmatrix}-h_{1}\\
- en: h_{0}\\
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: h_{0}\\
- en: -h_{3}\\
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: -h_{3}\\
- en: h_{2}\\
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: h_{2}\\
- en: \vdots\\
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: -h_{d-1}\\
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: -h_{d-1}\\
- en: h_{d-2}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{0}}\\
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: h_{d-2}\end{pmatrix}\otimes\begin{pmatrix}\sin{m\theta_{0}}\\
- en: \sin{m\theta_{0}}\\
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{0}}\\
- en: \sin{m\theta_{1}}\\
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{1}}\\
- en: \sin{m\theta_{1}}\\
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{1}}\\
- en: \vdots\\
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots\\
- en: \sin{m\theta_{d/2-1}}\\
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{d/2-1}}\\
- en: \sin{m\theta_{d/2-1}}\end{pmatrix}$$  |  | (1) |
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \sin{m\theta_{d/2-1}}\end{pmatrix}$$  |  | (1) |
- en: where $\theta_{j}=b^{-2j/d},j\in\{0,1,...,d/2-1\}$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{j}=b^{-2j/d},j\in\{0,1,...,d/2-1\}$。
- en: 'Position Interpolation (PI). As described in Chen et al. ([2023a](#bib.bib5))
    and Kaiokendev ([2023](#bib.bib11)), PI involves proportionally downscaling the
    position index $m$ in Equation [1](#S2.E1 "Equation 1 ‣ 2.2 RoPE and Its Extensions
    ‣ 2 Backgroud ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 位置插值（PI）。正如 Chen et al. ([2023a](#bib.bib5)) 和 Kaiokendev ([2023](#bib.bib11))
    所描述的，PI 涉及按比例缩小方程 [1](#S2.E1 "方程 1 ‣ 2.2 RoPE 及其扩展 ‣ 2 背景 ‣ 从注意力角度理解 RoPE 扩展")
    中的位置索引 $m$。
- en: NTK-Aware Interpolation (NTK). NTK (Rozière et al., [2023](#bib.bib19)) assumes
    that interpolating all dimensions equally, as done by PI, may result in the loss
    of high-frequency information. Therefore, NTK introduces a nonlinear interpolation
    strategy by adjusting the base frequency $b$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NTK 认知插值（NTK）。NTK（Rozière et al., [2023](#bib.bib19)）假设像 PI 一样对所有维度进行等量插值可能会导致高频信息的丧失。因此，NTK
    通过调整基础频率 $b$ 引入了非线性插值策略。
- en: Yet another RoPE extensioN (YaRN). Unlike PI and NTK, which treat each dimension
    of RoPE uniformly, YaRN (Peng et al., [2023](#bib.bib17)) employs a ramp function
    to combine PI and NTK at varying proportions across different dimensions. Additionally,
    it introduces a temperature factor to mitigate the distribution shift of the attention
    caused by long inputs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个 RoPE 扩展（YaRN）。不同于 PI 和 NTK 将 RoPE 的每个维度统一处理，YaRN（Peng 等人，[2023](#bib.bib17)）使用一个斜坡函数在不同维度上以不同的比例组合
    PI 和 NTK。此外，它引入了一个温度因子，以减轻长输入导致的注意力分布偏移。
- en: Following the default settings of the original papers (Chen et al., [2023b](#bib.bib6);
    Peng et al., [2023](#bib.bib17); Liu et al., [2023](#bib.bib14)), we adjust $\alpha$
    from 10,000 to 1,000,000 for NTK in our experiments.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 按照原始论文的默认设置（Chen 等人，[2023b](#bib.bib6); Peng 等人，[2023](#bib.bib17); Liu 等人，[2023](#bib.bib14)），我们在实验中将
    NTK 的 $\alpha$ 从 10,000 调整到 1,000,000。
- en: 2.3 Long-Context Evaluations
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 长上下文评估
- en: Following existing works (Chen et al., [2023b](#bib.bib6); Peng et al., [2023](#bib.bib17);
    Fu et al., [2024](#bib.bib7)), we use the perplexity test (dubbed PPL) as the
    primary evaluation and the Needle-in-a-Haystack test as a more challenging evaluation.
    The perplexity is a primary measure that reflects a model’s ability to handle
    long texts. The Needle-in-a-Haystack test (dubbed Needle) (Kamradt, [2023](#bib.bib13))
    requires LLMs to accurately recall a specific sentence (the Needle) embedded at
    an arbitrary location within a long document (the haystack). We obtain the perplexity
    using the Proof-pile (Azerbayev et al., [2022](#bib.bib1)) dataset. We follow
    the standard described in Fu et al. ([2024](#bib.bib7)) for the Needle-in-a-Haystack
    accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据现有的研究（Chen 等人，[2023b](#bib.bib6); Peng 等人，[2023](#bib.bib17); Fu 等人，[2024](#bib.bib7)），我们使用困惑度测试（称为
    PPL）作为主要评估指标，并使用 Needle-in-a-Haystack 测试作为更具挑战性的评估。困惑度是反映模型处理长文本能力的主要指标。Needle-in-a-Haystack
    测试（称为 Needle）（Kamradt，[2023](#bib.bib13)）要求 LLMs 准确地回忆起长文档（干草堆）中任意位置嵌入的特定句子（针）。我们使用
    Proof-pile（Azerbayev 等人，[2022](#bib.bib1)）数据集来获取困惑度。我们按照 Fu 等人（[2024](#bib.bib7)）描述的标准来评估
    Needle-in-a-Haystack 的准确性。
- en: 3 RoPE Extensions on PPL
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 RoPE 扩展在 PPL 上
- en: 'We preliminarily study the RoPE extensions from a primary long-context perplexity
    test. From the test, as illustrated in Figure [1](#S3.F1 "Figure 1 ‣ 3 RoPE Extensions
    on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective"), we identify that NTK can extrapolate from 4K to 128K, whereas PI
    and YaRN can extrapolate to 62K. We observe similar results in both the smaller
    model MiniMA-2-3B and the larger model LLaMa-2-13B, as illustrated in Figures
    [4](#A1.F4 "Figure 4 ‣ Appendix A Experimental Results on MiniMA-2-3B ‣ Understanding
    the RoPE Extensions of Long-Context LLMs: An Attention Perspective") and [7](#A2.F7
    "Figure 7 ‣ Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective"). To recognize
    why these RoPE extensions enable train-short-and-test-long properties in PPL,
    we collect the attention scores on 10 sequences in 2K and 8K and visualize their
    attention distributions. The followings are a few key takeaways from the attention
    perspective:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从主要的长上下文困惑度测试初步研究了 RoPE 扩展。正如图 [1](#S3.F1 "Figure 1 ‣ 3 RoPE Extensions on
    PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective")
    所示，我们发现 NTK 可以从 4K 推断到 128K，而 PI 和 YaRN 只能推断到 62K。在较小的模型 MiniMA-2-3B 和较大的模型 LLaMa-2-13B
    中也观察到类似的结果，如图 [4](#A1.F4 "Figure 4 ‣ Appendix A Experimental Results on MiniMA-2-3B
    ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective")
    和 [7](#A2.F7 "Figure 7 ‣ Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding
    the RoPE Extensions of Long-Context LLMs: An Attention Perspective") 所示。为了了解这些
    RoPE 扩展如何在 PPL 中实现短训练长测试的特性，我们收集了 2K 和 8K 的 10 个序列的注意力分数，并可视化它们的注意力分布。以下是从注意力角度得到的一些关键结论：'
- en: '![Refer to caption](img/dea263eb01a934713c49c20988d9c125.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dea263eb01a934713c49c20988d9c125.png)'
- en: 'Figure 1: Perplexity on Proof-pile dataset (Lower is better).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: Proof-pile 数据集上的困惑度（值越低越好）。'
- en: RoPE extensions maintain the original attention patterns.
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RoPE 扩展保持原始的注意力模式。
- en: 'As shown in Figure [2](#S3.F2 "Figure 2 ‣ RoPE extensions maintain the original
    attention patterns. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions
    of Long-Context LLMs: An Attention Perspective"), similar to the findings from
    Chen et al. ([2023b](#bib.bib6)), we acknowledge that the attention patterns fluctuate
    when the RoPE is tested on 8K sequences (exceeding the training length). However,
    with RoPE extensions, the attention distributions, as illustrated in Figures [2](#S3.F2
    "Figure 2 ‣ RoPE extensions maintain the original attention patterns. ‣ 3 RoPE
    Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An
    Attention Perspective")(c-e), revert to the original pattern seen in Figure [2(a)](#S3.F2.sf1
    "Figure 2(a) ‣ Figure 2 ‣ RoPE extensions maintain the original attention patterns.
    ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective") when tested on 8K sequences. Similar observations
    are seen in both LLaMa-2-13B and MiniMA-2-3B, as illustrated in Figures [5](#A1.F5
    "Figure 5 ‣ Appendix A Experimental Results on MiniMA-2-3B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective") and [8](#A2.F8
    "Figure 8 ‣ Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding the
    RoPE Extensions of Long-Context LLMs: An Attention Perspective").'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](#S3.F2 "图 2 ‣ RoPE 扩展保持了原始注意力模式。 ‣ 3 RoPE 扩展在 PPL 上 ‣ 从注意力角度理解 RoPE 扩展的长上下文
    LLM")所示，与Chen等人（[2023b](#bib.bib6)）的发现类似，我们认识到当RoPE在8K序列上进行测试时（超出训练长度），注意力模式会发生波动。然而，使用RoPE扩展时，如图[2](#S3.F2
    "图 2 ‣ RoPE 扩展保持了原始注意力模式。 ‣ 3 RoPE 扩展在 PPL 上 ‣ 从注意力角度理解 RoPE 扩展的长上下文 LLM")(c-e)所示，注意力分布会恢复到图[2(a)](#S3.F2.sf1
    "图 2(a) ‣ 图 2 ‣ RoPE 扩展保持了原始注意力模式。 ‣ 3 RoPE 扩展在 PPL 上 ‣ 从注意力角度理解 RoPE 扩展的长上下文
    LLM")中所见的原始模式，特别是在8K序列上进行测试时。LLaMa-2-13B和MiniMA-2-3B也有类似的观察结果，如图[5](#A1.F5 "图
    5 ‣ 附录 A MiniMA-2-3B 的实验结果 ‣ 从注意力角度理解 RoPE 扩展的长上下文 LLM")和[8](#A2.F8 "图 8 ‣ 附录
    B LLaMa-2-13B 的实验结果 ‣ 从注意力角度理解 RoPE 扩展的长上下文 LLM")所示。
- en: 'Table 1: Jensen–Shannon (JS) divergence of mean attention distributions between
    different models at lengths of 2048 (top row) and 8192 (bottom row). A lower JS
    divergence indicates that the two attention distributions are similar.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同模型在长度为2048（上行）和8192（下行）时的平均注意力分布的Jensen–Shannon (JS) 散度。较低的JS散度表示两个注意力分布相似。
- en: '|  | PI | YaRN | NTK | RoPE |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | PI | YaRN | NTK | RoPE |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMa-2 | 1.29 | 0.05 | 0.06 | 0.00 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-2 | 1.29 | 0.05 | 0.06 | 0.00 |'
- en: '| LLaMa-3 | 2.29 | 1.72 | 1.68 | 2.57 | ![Refer to caption](img/2d972eb579c40207d44ef0e4b10b787d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '| LLaMa-3 | 2.29 | 1.72 | 1.68 | 2.57 | ![参考说明](img/2d972eb579c40207d44ef0e4b10b787d.png)'
- en: (a) RoPE on 2K sequences.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RoPE 在2K序列上的表现。
- en: '![Refer to caption](img/bff7d1516c73c51979e87e19d4539920.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bff7d1516c73c51979e87e19d4539920.png)'
- en: (b) RoPE on 8K sequences.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (b) RoPE 在8K序列上的表现。
- en: '![Refer to caption](img/98842027febf301b4c470d55afbefd90.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/98842027febf301b4c470d55afbefd90.png)'
- en: (c) PI on 8K sequences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (c) PI 在8K序列上的表现。
- en: '![Refer to caption](img/44cd668320a03c862c6c182bed6afdb2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/44cd668320a03c862c6c182bed6afdb2.png)'
- en: (d) YaRN on 8K sequences.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (d) YaRN 在8K序列上的表现。
- en: '![Refer to caption](img/6bf5ecaf5ab4ec9c3f05f1c804b91952.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6bf5ecaf5ab4ec9c3f05f1c804b91952.png)'
- en: (e) NTK on 8K sequences.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (e) NTK 在8K序列上的表现。
- en: 'Figure 2: Attention distributions of RoPE, PI, YaRN, and NTK methods on 2K
    and 8K sequences. The red line represents the mean attention scores across all
    heads, layers, and examples. The other lines indicate the attention scores for
    each head in each layer of the examples.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：RoPE、PI、YaRN 和 NTK 方法在2K和8K序列上的注意力分布。红线表示所有头部、层和示例的平均注意力分数。其他线条表示每个头在每层的注意力分数。
- en: RoPE extensions closely resemble the attention patterns of models trained on
    longer cotnext.
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RoPE 扩展与在较长上下文中训练的模型的注意力模式非常相似。
- en: 'To further verify whether RoPE extensions maintain the original attention patterns,
    we aim to directly quantify the Jensen–Shannon (JS) divergence between different
    attention distributions. We choose LLaMa-2 and LLaMa-3 as bases. As illustrated
    in the bottom row of Table [1](#S3.T1 "Table 1 ‣ RoPE extensions maintain the
    original attention patterns. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE
    Extensions of Long-Context LLMs: An Attention Perspective"), the JS divergence
    between the RoPE extensions and LLaMa-3 is smaller than between the RoPE and LLaMa-3.
    This indicates that the attention patterns of RoPE extensions resemble those of
    models directly trained in a longer context.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步验证RoPE扩展是否保持原始注意力模式，我们旨在直接量化不同注意力分布之间的Jensen–Shannon (JS)散度。我们选择LLaMa-2和LLaMa-3作为基准。如表格[1](#S3.T1
    "Table 1 ‣ RoPE扩展保持原始注意力模式。 ‣ 3 RoPE扩展对PPL的影响 ‣ 从注意力角度理解RoPE扩展")底部所示，RoPE扩展与LLaMa-3之间的JS散度小于RoPE与LLaMa-3之间的散度。这表明RoPE扩展的注意力模式类似于直接在较长上下文中训练的模型。
- en: NTK and YaRN do not affect the attention patterns within the pretrained length.
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NTK和YaRN不会影响预训练长度内的注意力模式。
- en: 'Some RoPE extensions can degrade performance within the original pretrained
    length (Peng et al., [2023](#bib.bib17); Zhang et al., [2024](#bib.bib25)). To
    verify whether RoPE extensions alter the attention patterns within the pretrained
    length, we also calculate the JS divergence among these models’ attention distributions
    at a 2K length. As illustrated on the top row of Table [1](#S3.T1 "Table 1 ‣ RoPE
    extensions maintain the original attention patterns. ‣ 3 RoPE Extensions on PPL
    ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention Perspective"),
    the JS divergence for the NTK and YaRN is very low, almost zero, indicating minimal
    impact on attention distribution. On the contrary, the JS divergence for the PI
    is significantly higher. Therefore, we conclude that the NTK and YaRN methods
    do not affect attention patterns within the pretrained length.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一些RoPE扩展可能会降低原始预训练长度内的性能（Peng等，[2023](#bib.bib17)；Zhang等，[2024](#bib.bib25)）。为了验证RoPE扩展是否改变了预训练长度内的注意力模式，我们还计算了这些模型在2K长度下的注意力分布的JS散度。如表格[1](#S3.T1
    "Table 1 ‣ RoPE扩展保持原始注意力模式。 ‣ 3 RoPE扩展对PPL的影响 ‣ 从注意力角度理解RoPE扩展")顶部所示，NTK和YaRN的JS散度非常低，几乎为零，表明对注意力分布的影响最小。相反，PI的JS散度显著较高。因此，我们得出结论，NTK和YaRN方法不会影响预训练长度内的注意力模式。
- en: '![Refer to caption](img/c81bfeefa0c4b04f3a264c43c58187b5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/c81bfeefa0c4b04f3a264c43c58187b5.png)'
- en: (a) RoPE
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RoPE
- en: '![Refer to caption](img/f721fb423675cc3e3778ce25aeaf72d8.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/f721fb423675cc3e3778ce25aeaf72d8.png)'
- en: (b) Finetuning with PI
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用PI微调
- en: '![Refer to caption](img/463c41d0bd0b28b9bf3604282c5d4601.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/463c41d0bd0b28b9bf3604282c5d4601.png)'
- en: (c) Finetuning with YaRN
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 使用YaRN微调
- en: '![Refer to caption](img/7c10848cc2996799a1354c62d298a2e8.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/7c10848cc2996799a1354c62d298a2e8.png)'
- en: (d) Finetuning with NTK
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 使用NTK微调
- en: '![Refer to caption](img/1167af8241242f3544cc1a8f7a7ac336.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/1167af8241242f3544cc1a8f7a7ac336.png)'
- en: (e) Finetuning on 4K with NTK from (d)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 使用NTK在4K上微调，来源于(d)
- en: '![Refer to caption](img/3bea87cd7fdd0dabad0e1847e4ef6949.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/3bea87cd7fdd0dabad0e1847e4ef6949.png)'
- en: (f) Finetuning on 32K with NTK from (d)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 使用NTK在32K上微调，来源于(d)
- en: 'Figure 3: Performance comparison for the Needle-in-a-Haystack Test. The x-axis
    represents the length of the document, while the y-axis indicates the position
    where the needle is located. A red cell indicates that the model fails to recall
    the information in the needle, whereas a green cell indicates success. A white
    dashed line denotes the model’s continual pretrain length. Each value in the cells
    signifies the mean attention entropy, with higher values reflecting more dispersed
    attention.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：针在干草堆测试中的性能比较。x轴表示文档长度，y轴表示针的位置。红色单元格表示模型未能回忆起针中的信息，而绿色单元格则表示成功。白色虚线表示模型的持续预训练长度。单元格中的每个值表示平均注意力熵，值越高表示注意力分布越分散。
- en: 4 RoPE Extensions on Needle
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 RoPE扩展在针测试中的表现
- en: 'To understand the performance and behavior of the RoPE extensions on more challenging
    long-context tasks, we conduct Needle testing (Fu et al., [2024](#bib.bib7)).
    As shown in Figure [3](#S3.F3 "Figure 3 ‣ NTK and YaRN do not affect the attention
    patterns within the pretrained length. ‣ 3 RoPE Extensions on PPL ‣ Understanding
    the RoPE Extensions of Long-Context LLMs: An Attention Perspective")(a-d), LLaMa-2-7B
    with RoPE extensions can pass more needle tests than the RoPE. However, as the
    context length increases, some tests fail, resulting in retrieval needle errors.
    Eventually, almost all fail in extremely long contexts. We also conduct Needle
    testing on the MiniMA-2-3B and LLaMa-2-13B models with RoPE and PI. Unlike the
    LLaMa-2-7B, the PI method shows a more significant improvement in the LLaMa-2-13B,
    as depicted in Figure [9](#A2.F9 "Figure 9 ‣ Appendix B Experimental Results on
    LLaMa-2-13B ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective"). In contrast, on the MiniMA-2-3B, PI passes only a few needle tests
    at longer lengths, as illustrated in Figure [6](#A1.F6 "Figure 6 ‣ Appendix A
    Experimental Results on MiniMA-2-3B ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective"). We attribute these observations to the impact
    of model size. Below are key takeaways from the attention perspective:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '为了理解 RoPE 扩展在更具挑战性的长上下文任务中的表现和行为，我们进行了 Needle 测试（Fu et al., [2024](#bib.bib7)）。如图
    [3](#S3.F3 "Figure 3 ‣ NTK and YaRN do not affect the attention patterns within
    the pretrained length. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions
    of Long-Context LLMs: An Attention Perspective")(a-d) 所示，带有 RoPE 扩展的 LLaMa-2-7B
    比 RoPE 更能通过更多的针测。然而，随着上下文长度的增加，一些测试失败，导致检索针误差。最终，在极长的上下文中几乎全部失败。我们还对 MiniMA-2-3B
    和 LLaMa-2-13B 模型进行 RoPE 和 PI 的 Needle 测试。与 LLaMa-2-7B 不同，PI 方法在 LLaMa-2-13B 上显示出更显著的改进，如图
    [9](#A2.F9 "Figure 9 ‣ Appendix B Experimental Results on LLaMa-2-13B ‣ Understanding
    the RoPE Extensions of Long-Context LLMs: An Attention Perspective") 所示。相比之下，在
    MiniMA-2-3B 上，PI 仅在较长长度时通过少量针测，如图 [6](#A1.F6 "Figure 6 ‣ Appendix A Experimental
    Results on MiniMA-2-3B ‣ Understanding the RoPE Extensions of Long-Context LLMs:
    An Attention Perspective") 所示。我们将这些观察归因于模型大小的影响。以下是从注意力角度得到的关键结论：'
- en: Attention uncertainty leads to more retrieval needle errors.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力不确定性导致更多的检索针误差。
- en: 'To acquire the reason behind the retrieval needle errors, we calculate the
    entropy of attention for each length and depth, as illustrated in Figures [3](#S3.F3
    "Figure 3 ‣ NTK and YaRN do not affect the attention patterns within the pretrained
    length. ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective"). Our findings demonstrate that the locations
    of retrieval needle errors often coincide with high attention entropy. For example,
    at the same depth, the positions with errors are among the top-k in entropy; similarly,
    at the same length, the error positions also have high entropy. We hypothesize
    that the increase in attention entropy with longer test lengths is due to the
    train-short-and-test-long setting. During inference, the number of tokens handled
    by the self-attention mechanism far exceeds that during training. More tokens
    lead to more dispersed attention, i.e., higher uncertainty, causing a mismatch
    between training and inference.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '为了找出检索针误差的原因，我们计算了每个长度和深度的注意力熵，如图 [3](#S3.F3 "Figure 3 ‣ NTK and YaRN do not
    affect the attention patterns within the pretrained length. ‣ 3 RoPE Extensions
    on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An Attention
    Perspective") 所示。我们的研究结果表明，检索针误差的位置通常与高注意力熵相吻合。例如，在相同深度下，错误的位置通常是熵的前 k 位；同样，在相同长度下，错误位置也具有较高的熵。我们假设，随着测试长度的增加，注意力熵的增加是由于训练-短和测试-长的设置。在推理过程中，自注意力机制处理的标记数量远超训练过程中的数量。更多的标记导致注意力分散，即更高的不确定性，造成训练和推理之间的不匹配。'
- en: A natural approach to lower attention uncertainty for enhancing extrapolation.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 降低注意力不确定性以增强外推的自然方法。
- en: 'A direct solution is to train on longer contexts, thereby increasing the number
    of attention tokens during training and reducing attention uncertainty. To validate
    our hypothesis, we finetune models on 4K and 32K training lengths with the same
    tokens on NTK. As shown in Figures [3(e)](#S3.F3.sf5 "Figure 3(e) ‣ Figure 3 ‣
    NTK and YaRN do not affect the attention patterns within the pretrained length.
    ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective") and [3(f)](#S3.F3.sf6 "Figure 3(f) ‣ Figure 3
    ‣ NTK and YaRN do not affect the attention patterns within the pretrained length.
    ‣ 3 RoPE Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context
    LLMs: An Attention Perspective"), compared to models trained in short contexts,
    models trained in more extended contexts exhibited significantly lower attention
    uncertainty. For example, at length 63938, the attention entropy is generally
    below 5. The Needle test pass rates improved significantly, especially in longer
    testing contexts. Conversely, models trained with the same number of tokens but
    shorter context sizes showed little to no change in attention entropy, remaining
    similar to the original one ([3(d)](#S3.F3.sf4 "Figure 3(d) ‣ Figure 3 ‣ NTK and
    YaRN do not affect the attention patterns within the pretrained length. ‣ 3 RoPE
    Extensions on PPL ‣ Understanding the RoPE Extensions of Long-Context LLMs: An
    Attention Perspective")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接的解决方案是对更长的上下文进行训练，从而增加训练期间的注意力标记数量并减少注意力不确定性。为了验证我们的假设，我们在 4K 和 32K 训练长度上微调模型，使用相同的标记在
    NTK 上。正如图 [3(e)](#S3.F3.sf5 "Figure 3(e) ‣ Figure 3 ‣ NTK 和 YaRN 不影响预训练长度内的注意力模式。
    ‣ 3 RoPE 扩展 ‣ 理解长上下文 LLM 的 RoPE 扩展：从注意力的角度") 和 [3(f)](#S3.F3.sf6 "Figure 3(f)
    ‣ Figure 3 ‣ NTK 和 YaRN 不影响预训练长度内的注意力模式。 ‣ 3 RoPE 扩展 ‣ 理解长上下文 LLM 的 RoPE 扩展：从注意力的角度")
    所示，与在短上下文中训练的模型相比，在更长上下文中训练的模型表现出显著较低的注意力不确定性。例如，在长度为 63938 时，注意力熵通常低于 5。针测验通过率显著提高，尤其是在较长的测试上下文中。相反，使用相同数量的标记但上下文较短的模型在注意力熵上几乎没有变化，与原始模型相似（[3(d)](#S3.F3.sf4
    "Figure 3(d) ‣ Figure 3 ‣ NTK 和 YaRN 不影响预训练长度内的注意力模式。 ‣ 3 RoPE 扩展 ‣ 理解长上下文 LLM
    的 RoPE 扩展：从注意力的角度")）。
- en: 5 Conclusions
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'This paper provides the first thorough understanding of RoPE extensions for
    long-context LLMs from an attention perspective, evaluated on two widely-used
    benchmarks: Perplexity and Needle-in-a-Haystack. Extensive experiments demonstrate
    some valuable findings: 1) Compared to direct extrapolation, RoPE extensions can
    maintain the original training length attention patterns. 2) Large attention uncertainty
    leads to retrieval errors in needle testing in RoPE extensions. 3) Using longer
    continual pretraining lengths for RoPE extensions can reduce attention uncertainty
    and significantly enhance extrapolation in target LLMs.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本文从注意力的角度提供了对 RoPE 扩展在长上下文 LLM 中的首个深入理解，评估了两个广泛使用的基准：Perplexity 和 Needle-in-a-Haystack。广泛的实验展示了一些有价值的发现：1）与直接外推相比，RoPE
    扩展可以保持原始训练长度的注意力模式。2）较大的注意力不确定性会导致 RoPE 扩展中的针测验检索错误。3）对 RoPE 扩展使用更长的连续预训练长度可以减少注意力不确定性，并显著提升目标
    LLM 的外推能力。
- en: References
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Azerbayev et al. (2022) Azerbayev, Z., Ayers, E., , and Piotrowski, B. Proof-pile,
    2022. URL [https://github.com/zhangir-azerbayev/proof-pile](https://github.com/zhangir-azerbayev/proof-pile).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azerbayev 等人（2022）Azerbayev, Z., Ayers, E., 和 Piotrowski, B. Proof-pile, 2022.
    网址 [https://github.com/zhangir-azerbayev/proof-pile](https://github.com/zhangir-azerbayev/proof-pile)。
- en: Bai et al. (2023) Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,
    Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. *ArXiv preprint*,
    abs/2309.16609, 2023. URL [https://arxiv.org/abs/2309.16609](https://arxiv.org/abs/2309.16609).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2023）Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y.,
    Ge, W., Han, Y., Huang, F., 等人。Qwen 技术报告。*ArXiv 预印本*，abs/2309.16609，2023. 网址 [https://arxiv.org/abs/2309.16609](https://arxiv.org/abs/2309.16609)。
- en: 'Bi et al. (2024) Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding,
    H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm: Scaling open-source language
    models with longtermism. *ArXiv preprint*, abs/2401.02954, 2024. URL [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bi 等人（2024）Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H.,
    Dong, K., Du, Q., Fu, Z., 等人。Deepseek llm: 通过长期主义扩展开源语言模型。*ArXiv 预印本*，abs/2401.02954，2024.
    网址 [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954)。'
- en: 'Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (eds.), *Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I., 和 Amodei, D. 语言模型是少样本学习者。在 Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
    M., 和 Lin, H.（编辑），*神经信息处理系统进展 33：2020 年神经信息处理系统年度会议，NeurIPS 2020，2020年12月6-12日，虚拟*，2020。网址
    [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)。
- en: Chen et al. (2023a) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation. *ArXiv preprint*,
    abs/2306.15595, 2023a. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023a) Chen, S., Wong, S., Chen, L., 和 Tian, Y. 通过位置插值扩展大型语言模型的上下文窗口。*ArXiv预印本*，abs/2306.15595,
    2023a。网址 [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595)。
- en: Chen et al. (2023b) Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context
    window of large language models via positional interpolation. *ArXiv preprint*,
    abs/2306.15595, 2023b. URL [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023b) Chen, S., Wong, S., Chen, L., 和 Tian, Y. 通过位置插值扩展大型语言模型的上下文窗口。*ArXiv预印本*，abs/2306.15595,
    2023b。网址 [https://arxiv.org/abs/2306.15595](https://arxiv.org/abs/2306.15595)。
- en: Fu et al. (2024) Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y.,
    and Peng, H. Data engineering for scaling language models to 128k context. *ArXiv
    preprint*, abs/2402.10171, 2024. URL [https://arxiv.org/abs/2402.10171](https://arxiv.org/abs/2402.10171).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2024) Fu, Y., Panda, R., Niu, X., Yue, X., Hajishirzi, H., Kim, Y.,
    和 Peng, H. 数据工程用于将语言模型扩展到128k上下文。*ArXiv预印本*，abs/2402.10171, 2024。网址 [https://arxiv.org/abs/2402.10171](https://arxiv.org/abs/2402.10171)。
- en: 'Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang,
    S. Lm-infinite: Simple on-the-fly length generalization for large language models.
    *ArXiv preprint*, abs/2308.16137, 2023. URL [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2023) Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., 和 Wang, S.
    Lm-infinite：大型语言模型的简单即席长度泛化。*ArXiv预印本*，abs/2308.16137, 2023。网址 [https://arxiv.org/abs/2308.16137](https://arxiv.org/abs/2308.16137)。
- en: Hu et al. (2024) Hu, Y., Huang, Q., Tao, M., Zhang, C., and Feng, Y. Can perplexity
    reflect large language model’s ability in long text understanding? *ArXiv preprint*,
    abs/2405.06105, 2024. URL [https://arxiv.org/abs/2405.06105](https://arxiv.org/abs/2405.06105).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2024) Hu, Y., Huang, Q., Tao, M., Zhang, C., 和 Feng, Y. 困惑度能否反映大型语言模型在长文本理解中的能力？*ArXiv预印本*，abs/2405.06105,
    2024。网址 [https://arxiv.org/abs/2405.06105](https://arxiv.org/abs/2405.06105)。
- en: 'Jiang et al. (2021) Jiang, Z., Araki, J., Ding, H., and Neubig, G. How can
    we know when language models know? on the calibration of language models for question
    answering. *Transactions of the Association for Computational Linguistics*, 9:962–977,
    2021. doi: 10.1162/tacl˙a˙00407. URL [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2021) Jiang, Z., Araki, J., Ding, H., 和 Neubig, G. 我们如何知道语言模型是否知道？关于语言模型在问答中的校准。*计算语言学协会会刊*，9:962–977,
    2021。doi: 10.1162/tacl˙a˙00407。网址 [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57)。'
- en: Kaiokendev (2023) Kaiokendev. Things i’m learning while training superhot. [https://kaiokendev.github.io/til#extending-context-to-8k](https://kaiokendev.github.io/til#extending-context-to-8k),
    2023.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiokendev (2023) Kaiokendev. 我在训练超热模型时学到的东西。 [https://kaiokendev.github.io/til#extending-context-to-8k](https://kaiokendev.github.io/til#extending-context-to-8k)，2023。
- en: Kamalloo et al. (2023) Kamalloo, E., Dziri, N., Clarke, C. L., and Rafiei, D.
    Evaluating open-domain question answering in the era of large language models.
    *ArXiv preprint*, abs/2305.06984, 2023. URL [https://arxiv.org/abs/2305.06984](https://arxiv.org/abs/2305.06984).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamalloo 等（2023）Kamalloo、Dziri、Clarke、Rafiei。在大型语言模型时代评估开放域问答。*ArXiv 预印本*，abs/2305.06984，2023年。网址
    [https://arxiv.org/abs/2305.06984](https://arxiv.org/abs/2305.06984)。
- en: Kamradt (2023) Kamradt, G. Needle in a haystack - pressure testing llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack),
    2023.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt（2023）Kamradt。针头在干草堆中 - 对 LLM 进行压力测试。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)，2023年。
- en: Liu et al. (2023) Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
    Scaling laws of rope-based extrapolation. *ArXiv preprint*, abs/2310.05209, 2023.
    URL [https://arxiv.org/abs/2310.05209](https://arxiv.org/abs/2310.05209).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023）刘晓、颜华、张松、安成、邱鑫、林达。基于绳索的外推规模法则。*ArXiv 预印本*，abs/2310.05209，2023年。网址 [https://arxiv.org/abs/2310.05209](https://arxiv.org/abs/2310.05209)。
- en: Men et al. (2024) Men, X., Xu, M., Wang, B., Zhang, Q., Lin, H., Han, X., and
    Chen, W. Base of rope bounds context length. *ArXiv preprint*, abs/2405.14591,
    2024. URL [https://arxiv.org/abs/2405.14591](https://arxiv.org/abs/2405.14591).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 闵等（2024）闵晓、徐敏、王斌、张强、林华、韩晓、陈伟。基于绳索的上下文长度限制。*ArXiv 预印本*，abs/2405.14591，2024年。网址
    [https://arxiv.org/abs/2405.14591](https://arxiv.org/abs/2405.14591)。
- en: 'Minaee et al. (2024) Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher,
    R., Amatriain, X., and Gao, J. Large language models: A survey. *ArXiv preprint*,
    abs/2402.06196, 2024. URL [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等（2024）Minaee、Mikolov、Nikzad、Chenaghlu、Socher、Amatriain、Gao。大型语言模型：综述。*ArXiv
    预印本*，abs/2402.06196，2024年。网址 [https://arxiv.org/abs/2402.06196](https://arxiv.org/abs/2402.06196)。
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models. *ArXiv preprint*,
    abs/2309.00071, 2023. URL [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '彭等（2023）彭博、Quesnelle、范浩、Shippole. Yarn: 大型语言模型的高效上下文窗口扩展。*ArXiv 预印本*，abs/2309.00071，2023年。网址
    [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071)。'
- en: Radford et al. (2018) Radford, A., Narasimhan, K., Salimans, T., Sutskever,
    I., et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018）Radford、Narasimhan、Salimans、Sutskever 等。通过生成性预训练改进语言理解。2018年。
- en: 'Rozière et al. (2023) Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov,
    I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., Défossez,
    A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
    Synnaeve, G. Code Llama: Open foundation models for code, 2023. arXiv: 2308.12950.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière 等（2023）Rozière、Gehring、Gloeckle、Sootla、Gat、Tan、Adi、刘杰、Remez、Rapin、Kozhevnikov、Evtimov、Bitton、Bhatt、Ferrer、Grattafiori、Xiong、Défossez、Copet、Azhar、Touvron、Martin、Usunier、Scialom、Synnaeve。Code
    Llama: 用于代码的开源基础模型，2023年。arXiv: 2308.12950。'
- en: 'Su et al. (2019) Su, D., Xu, Y., Winata, G. I., Xu, P., Kim, H., Liu, Z., and
    Fung, P. Generalizing question answering system with pre-trained language model
    fine-tuning. In *Proceedings of the 2nd Workshop on Machine Reading for Question
    Answering*, pp.  203–211, Hong Kong, China, 2019\. Association for Computational
    Linguistics. doi: 10.18653/v1/D19-5827. URL [https://aclanthology.org/D19-5827](https://aclanthology.org/D19-5827).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '苏等（2019）苏达、徐阳、Winata、徐鹏、Kim、刘志、Fung。通过预训练语言模型微调来泛化问答系统。见*第二届机器阅读问答研讨会论文集*，第203-211页，中国香港，2019年。计算语言学协会。doi:
    10.18653/v1/D19-5827。网址 [https://aclanthology.org/D19-5827](https://aclanthology.org/D19-5827)。'
- en: 'Su et al. (2021) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
    Roformer: Enhanced transformer with rotary position embedding. *ArXiv preprint*,
    abs/2104.09864, 2021. URL [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 苏等（2021）苏静、卢云、潘石、穆尔塔达、温博、刘洋。Roformer：增强型变换器与旋转位置嵌入。*ArXiv 预印本*，abs/2104.09864，2021年。网址
    [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *ArXiv preprint*, abs/2307.09288,
    2023. URL [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,
    Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等。Llama 2：开放基础和微调聊天模型。*ArXiv
    预印本*，abs/2307.09288, 2023。网址 [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)。
- en: 'Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need.
    In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan,
    S. V. N., and Garnett, R. (eds.), *Advances in Neural Information Processing Systems
    30: Annual Conference on Neural Information Processing Systems 2017, December
    4-9, 2017, Long Beach, CA, USA*, pp.  5998–6008, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, L., 和 Polosukhin, I. Attention is all you need. 在 Guyon,
    I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S.
    V. N., 和 Garnett, R.（编），*神经信息处理系统进展 30：2017年神经信息处理系统年会，2017年12月4-9日，美国加州长滩*，pp.
    5998–6008, 2017。网址 [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)。
- en: Zhang et al. (2023) Zhang, C., Song, D., Ye, Z., and Gao, Y. Towards the law
    of capacity gap in distilling language models. *ArXiv preprint*, abs/2311.07052,
    2023. URL [https://arxiv.org/abs/2311.07052](https://arxiv.org/abs/2311.07052).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Zhang, C., Song, D., Ye, Z., 和 Gao, Y. 朝着语言模型的容量差距定律。*ArXiv 预印本*，abs/2311.07052,
    2023。网址 [https://arxiv.org/abs/2311.07052](https://arxiv.org/abs/2311.07052)。
- en: Zhang et al. (2024) Zhang, Y., Li, J., and Liu, P. Extending llms’ context window
    with 100 samples. *ArXiv preprint*, abs/2401.07004, 2024. URL [https://arxiv.org/abs/2401.07004](https://arxiv.org/abs/2401.07004).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024）Zhang, Y., Li, J., 和 Liu, P. 扩展 LLM 的上下文窗口至 100 个样本。*ArXiv 预印本*，abs/2401.07004,
    2024。网址 [https://arxiv.org/abs/2401.07004](https://arxiv.org/abs/2401.07004)。
- en: Appendix A Experimental Results on MiniMA-2-3B
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A MiniMA-2-3B 实验结果
- en: '![Refer to caption](img/0e4da0a2b893fe54417660e7c63094b4.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/0e4da0a2b893fe54417660e7c63094b4.png)'
- en: 'Figure 4: Validation Perplexity on Proof-pile (Azerbayev et al., [2022](#bib.bib1))
    of MiniMA-2-3B'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MiniMA-2-3B 上 Proof-pile 的验证困惑度（Azerbayev 等，[2022](#bib.bib1)）
- en: '![Refer to caption](img/cb6b222b3e4d02122c4378fc11fbe6f3.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/cb6b222b3e4d02122c4378fc11fbe6f3.png)'
- en: (a) RoPE on 2K sequences.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 2K 序列上的 RoPE。
- en: '![Refer to caption](img/c9d43fb32dbe27052a13e794bf5a226a.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/c9d43fb32dbe27052a13e794bf5a226a.png)'
- en: (b) RoPE on 8K sequences.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 8K 序列上的 RoPE。
- en: '![Refer to caption](img/731fe0cdec03b8fc610d255e2027ef65.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/731fe0cdec03b8fc610d255e2027ef65.png)'
- en: (c) PI on 8K sequences.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 8K 序列上的 PI。
- en: '![Refer to caption](img/d893f193a8ffd197320da85fe1ca3804.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/d893f193a8ffd197320da85fe1ca3804.png)'
- en: (d) YaRN on 8K sequences.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 8K 序列上的 YaRN。
- en: '![Refer to caption](img/84c24ce690ff085e2450f52672d7c8f2.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/84c24ce690ff085e2450f52672d7c8f2.png)'
- en: (e) NTK on 8K sequences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 8K 序列上的 NTK。
- en: 'Figure 5: Attention distributions of RoPE, PI, YaRN, and NTK methods on 2K
    and 8K sequences on MiniMA-2-3B.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：MiniMA-2-3B 上 RoPE、PI、YaRN 和 NTK 方法在 2K 和 8K 序列上的注意力分布。
- en: '![Refer to caption](img/346407a9ea9222629540a19e2e66a55d.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/346407a9ea9222629540a19e2e66a55d.png)'
- en: (a) RoPE
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RoPE
- en: '![Refer to caption](img/70418268e8737cca5c9f143a5919bb43.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/70418268e8737cca5c9f143a5919bb43.png)'
- en: (b) Finetuning with PI
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用 PI 的微调
- en: 'Figure 6: Performance comparison for the Needle-in-a-Haystack Test of MiniMa-2-3B.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：MiniMa-2-3B 上 Needle-in-a-Haystack 测试的性能比较。
- en: Appendix B Experimental Results on LLaMa-2-13B
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LLaMa-2-13B 实验结果
- en: '![Refer to caption](img/8c08dc80bf0f25385eb229c9bc421602.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/8c08dc80bf0f25385eb229c9bc421602.png)'
- en: 'Figure 7: Validation Perplexity on Proof-pile of LLaMa-2-13B'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：LLaMa-2-13B 上 Proof-pile 的验证困惑度
- en: '![Refer to caption](img/c6ea41072581efbc4722313265104109.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/c6ea41072581efbc4722313265104109.png)'
- en: (a) RoPE on 2K sequences.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 2K 序列上的 RoPE。
- en: '![Refer to caption](img/a7a4b72e9c712b67dce2d31df02756cd.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/a7a4b72e9c712b67dce2d31df02756cd.png)'
- en: (b) RoPE on 8K sequences.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 8K 序列上的 RoPE。
- en: '![Refer to caption](img/521d2d91c5da0056efc97f02bab2602d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/521d2d91c5da0056efc97f02bab2602d.png)'
- en: (c) PI on 8K sequences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 8K 序列上的 PI。
- en: '![Refer to caption](img/f3b8c194122311ae6402d904ca37456d.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/f3b8c194122311ae6402d904ca37456d.png)'
- en: (d) YaRN on 8K sequences.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 8K 序列上的 YaRN。
- en: '![Refer to caption](img/326771bb643d0037eb6615d8ea4857ea.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/326771bb643d0037eb6615d8ea4857ea.png)'
- en: (e) NTK on 8K sequences.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 关于8K序列的NTK。
- en: 'Figure 8: Attention distributions of RoPE, PI, YaRN, and NTK methods on 2K
    and 8K sequences on LLaMA-2-13B.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLaMA-2-13B上2K和8K序列中RoPE、PI、YaRN和NTK方法的注意力分布。
- en: '![Refer to caption](img/275a23ce0f7afa5f8c4188003ba134af.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/275a23ce0f7afa5f8c4188003ba134af.png)'
- en: (a) RoPE
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: (a) RoPE
- en: '![Refer to caption](img/8a0cb7db3324762728cd34c2849defe0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8a0cb7db3324762728cd34c2849defe0.png)'
- en: (b) Finetuning with PI
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用PI进行微调
- en: 'Figure 9: Performance comparison for the Needle-in-a-Haystack Test of LLaMa-2-13B.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：LLaMa-2-13B的针扎干草堆测试性能比较。
