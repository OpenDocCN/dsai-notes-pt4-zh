- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 19:04:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 19:04:00'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Long-context LLMs Struggle with Long In-context Learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长上下文LLM在长时间上下文学习中的挑战
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.02060](https://ar5iv.labs.arxiv.org/html/2404.02060)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.02060](https://ar5iv.labs.arxiv.org/html/2404.02060)
- en: ^(♠,♣)Tianle Li, ^(♠,♣)Ge Zhang, ^♠Quy Duc Do, ^†Xiang Yue, ^(♠,♣)Wenhu Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ^(♠,♣)李天乐, ^(♠,♣)张格, ^♠阮德度, ^†岳翔, ^(♠,♣)陈文虎
- en: ^♠University of Waterloo
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^♠University of Waterloo
- en: ^†Carnegie Mellon University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^†卡内基梅隆大学
- en: ^♣Vector Institute, Toronto
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^♣Vector Institute, Toronto
- en: '{t29li,wenhuchen}@uwaterloo.ca'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{t29li,wenhuchen}@uwaterloo.ca'
- en: '[https://github.com/TIGER-AI-Lab/LongICLBench](https://github.com/TIGER-AI-Lab/LongICLBench)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/TIGER-AI-Lab/LongICLBench](https://github.com/TIGER-AI-Lab/LongICLBench)'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have made significant strides in handling long
    sequences exceeding 32K tokens. However, their performance evaluation has largely
    been confined to metrics like perplexity and synthetic tasks, which may not fully
    capture their abilities in more nuanced, real-world scenarios. This study introduces
    a specialized benchmark (LongICLBench) focusing on long in-context learning within
    the realm of extreme-label classification. We meticulously selected six datasets
    with a label range spanning 28 to 174 classes covering different input (few-shot
    demonstration) lengths from 2K to 50K tokens. Our benchmark requires LLMs to comprehend
    the entire input to recognize the massive label spaces to make correct predictions.
    We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context
    LLMs perform relatively well on less challenging tasks with shorter demonstration
    lengths by effectively utilizing the long context window. However, on the most
    challenging task Discovery with 174 labels, all the LLMs struggle to understand
    the task definition, thus reaching a performance close to zero. This suggests
    a notable gap in current LLM capabilities for processing and understanding long,
    context-rich sequences. Further analysis revealed a tendency among models to favor
    predictions for labels presented toward the end of the sequence. Their ability
    to reason over multiple pieces in the long sequence is yet to be improved. Our
    study reveals that long context understanding and reasoning is still a challenging
    task for the existing LLMs. We believe LongICLBench could serve as a more realistic
    evaluation for the future long-context LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在处理超过32K令牌的长序列方面取得了显著进展。然而，它们的性能评估主要集中在困惑度和合成任务等指标上，这可能无法完全反映它们在更微妙、真实世界场景中的能力。本研究介绍了一种专门的基准（LongICLBench），重点关注极端标签分类领域的长时间上下文学习。我们精心选择了六个数据集，这些数据集的标签范围从28类到174类，涵盖了从2K到50K令牌的不同输入（少量示范）长度。我们的基准要求LLM理解整个输入，以识别大量标签空间，从而做出正确预测。我们在我们的基准测试中评估了13个长上下文LLM。我们发现，长上下文LLM在较短示范长度的较简单任务上表现相对较好，通过有效利用长上下文窗口。然而，在最具挑战性的174标签任务Discovery上，所有LLM都难以理解任务定义，从而表现接近零。这表明当前LLM在处理和理解长时间、丰富上下文序列方面存在显著差距。进一步分析显示，模型倾向于偏向于对序列末尾出现的标签进行预测。它们对长序列中的多个部分进行推理的能力仍需改进。我们的研究揭示了现有LLM在长时间上下文理解和推理方面仍面临挑战。我们认为LongICLBench可能会为未来的长上下文LLM提供更现实的评估。
- en: '![Refer to caption](img/70a9501233104da3b2ddb39a43b00124.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/70a9501233104da3b2ddb39a43b00124.png)'
- en: 'Figure 1: LLM performance on long in-context benchmark across different lengths.
    We curate datasets with different difficulty levels. As we increase the difficulty
    of the dataset, LLMs struggle to understand the task definition and suffer from
    significant performance degradation. On the most difficult Discovery dataset,
    none of the LLMs is able to understand the long demonstration, leading to zero
    accuracy.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在不同长度的长时间上下文基准测试中，LLM的表现。我们策划了不同难度水平的数据集。随着数据集难度的增加，LLM在理解任务定义方面遇到困难，并且表现显著下降。在最困难的Discovery数据集上，没有一个LLM能够理解长时间的示范，从而导致准确率为零。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models have already entered the long context era. Myriad of LLMs
    has been released to support long context windows from 32K to 2M tokens. These
    methods (Hao et al., [2022](#bib.bib19); Chen et al., [2023a](#bib.bib10); Peng
    et al., [2023b](#bib.bib33); Ratner et al., [2023](#bib.bib36); Xiao et al., [2024](#bib.bib48))
    can unlock lots of complex real-world applications from long-document question-answering,
    multi-document summarization, long-horizon agent tasks, repo-level code understanding.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型已经进入了长上下文时代。各种 LLM 已经发布以支持从 32K 到 2M 令牌的长上下文窗口。这些方法 (Hao et al., [2022](#bib.bib19);
    Chen et al., [2023a](#bib.bib10); Peng et al., [2023b](#bib.bib33); Ratner et
    al., [2023](#bib.bib36); Xiao et al., [2024](#bib.bib48)) 可以解锁许多复杂的现实世界应用，如长文档问答、多文档摘要生成、长视野代理任务、仓库级代码理解。
- en: One line of research is based on AliBi (Press et al., [2022](#bib.bib35)) and
    RoPE (Su et al., [2024](#bib.bib40)) embedding, which allows us to train Transformers
    with short sequences and subsequently apply them to longer sequences during inference.
    Recently, different approaches (Xiong et al., [2023](#bib.bib49); Fu et al., [2024](#bib.bib17);
    Liu et al., [2024](#bib.bib26)) help the model to extrapolate to 128K window size
    with continued pre-training. Later on, LongRoPE (Ding et al., [2024](#bib.bib15))
    was proposed to further extend the context window to 2M tokens. Another line of
    research also utilizes methodologies like context window sliding and segmentation
    to overcome the issue of the limited context window in original Transformers (Hao
    et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib36)). Furthermore, architectural
    innovations, transitioning from traditional Transformer-based designs to recurrent
    models or state space models, have shown promise in facilitating long-range computations
    naturally Orvieto et al. ([2023](#bib.bib31)); Gu & Dao ([2023](#bib.bib18));
    Peng et al. ([2023a](#bib.bib32)). These techniques have been incorporated into
    several current open-source LLMs to enhance long sequence understanding capability (Chen
    et al., [2023b](#bib.bib11); Tworkowski et al., [2023](#bib.bib44)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一条研究线索基于 AliBi (Press et al., [2022](#bib.bib35)) 和 RoPE (Su et al., [2024](#bib.bib40))
    嵌入，这使我们可以用短序列训练 Transformers，并在推理时应用于更长的序列。最近，不同的方法 (Xiong et al., [2023](#bib.bib49);
    Fu et al., [2024](#bib.bib17); Liu et al., [2024](#bib.bib26)) 帮助模型通过继续预训练将窗口大小外推到
    128K。后来，LongRoPE (Ding et al., [2024](#bib.bib15)) 被提出以进一步将上下文窗口扩展到 2M 令牌。另一条研究线索还利用了像上下文窗口滑动和分段这样的技术，以克服原始
    Transformers 中上下文窗口有限的问题 (Hao et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib36))。此外，架构创新，如从传统
    Transformer 设计转变为递归模型或状态空间模型，在自然地促进长程计算方面显示了前景 (Orvieto et al. ([2023](#bib.bib31));
    Gu & Dao ([2023](#bib.bib18)); Peng et al. ([2023a](#bib.bib32)))。这些技术已被纳入几种当前的开源
    LLM 中，以增强长序列理解能力 (Chen et al., [2023b](#bib.bib11); Tworkowski et al., [2023](#bib.bib44))。
- en: '![Refer to caption](img/10418bb602a94c7d0594a747ff935e00.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/10418bb602a94c7d0594a747ff935e00.png)'
- en: 'Figure 2: Comparison extreme-label ICL with the existing evaluation tasks.
    Passkey Retrieval is a synthetic task. Long-document Question-answering does not
    require reading the entire document to find the answer. In extreme-label ICL,
    the model needs to scan through the entire demonstration to understand the whole
    label space to make the correct prediction.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：极端标签 ICL 与现有评估任务的比较。密钥检索是一个合成任务。长文档问答不需要阅读整个文档来找到答案。在极端标签 ICL 中，模型需要扫描整个演示内容以理解整个标签空间，以作出正确的预测。
- en: 'These long-context models are primarily evaluated on three types of evaluations:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些长上下文模型主要通过三种类型的评估来进行评估：
- en: 1\. language model perplexity over long documents, which is used by most papers.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 长文档的语言模型困惑度，大多数论文都使用这种评估方式。
- en: 2\. passkey retrieval (Mohtashami & Jaggi, [2023](#bib.bib29); Chen et al.,
    [2023a](#bib.bib10); Li et al., [2023a](#bib.bib22)) or needle-in-a-haystack (Team
    et al., [2023](#bib.bib42); Fu et al., [2024](#bib.bib17)), which requires reciting
    a randomly inserted information in a long sequence. Several LLMs achieve 99%+
    on this synthetic task.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 密钥检索 (Mohtashami & Jaggi, [2023](#bib.bib29); Chen et al., [2023a](#bib.bib10);
    Li et al., [2023a](#bib.bib22)) 或针尖上的针 (Team et al., [2023](#bib.bib42); Fu et
    al., [2024](#bib.bib17))，这需要在长序列中复述随机插入的信息。几种 LLM 在这一合成任务上达到了 99%+ 的准确率。
- en: 3\. long-document question-answer or summarization over Qasper (Dasigi et al.,
    [2021](#bib.bib12)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 长文档问答或摘要生成任务在 Qasper 上 (Dasigi et al., [2021](#bib.bib12))。
- en: Evaluations (1) and (2) only provide a minimum bar for LLMs to pass, but their
    results cannot reflect LLMs’ true ability to deal with realistic long-sequence
    tasks. Evaluation (3) provides a more realistic metric, however, these tasks are
    more focused on retrieving correct information from the long input. In question
    answering, LLMs can take a shortcut to read a short snippet to predict the answer
    without reading the entire document as demonstrated in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Long-context LLMs Struggle with Long In-context Learning")
    case (b). Similarly, summarization also suffers from the strong position bias,
    where LLMs can utilize the few leading sentences (Nallapati et al., [2017](#bib.bib30))
    to achieve high performance. Therefore, these metrics are insufficient to measure
    LLMs’ ability to comprehend and reason over the entire input sequence.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 评估 (1) 和 (2) 仅提供了 LLMs 的最低标准，但其结果无法反映 LLMs 处理现实长序列任务的真实能力。评估 (3) 提供了更为现实的度量标准，但这些任务更侧重于从长输入中检索正确的信息。在问答中，LLMs
    可以通过读取一个短片段来预测答案，而不需要阅读整个文档，如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 长上下文 LLMs 在长上下文学习中表现挣扎")
    的案例 (b) 所示。类似地，摘要也受限于强烈的位置信息偏差，LLMs 可以利用少数几个开头的句子 (Nallapati et al., [2017](#bib.bib30))
    来取得高性能。因此，这些度量标准不足以衡量 LLMs 理解和推理整个输入序列的能力。
- en: In this paper, we propose to adopt in-context learning (ICL) on extreme-label
    classification tasks (Anil et al., [2022](#bib.bib4); Milios et al., [2023](#bib.bib28))
    to evaluate long-context LLMs. Unlike the prior tasks, in-context learning requires
    LLMs to recognize the task by scanning over the entire input to understand the
    label space. This task necessitates LLMs’ ability to comprehend the entire input
    to make predictions. Due to the massive label space, the task demonstration could
    easily become a long sequence. For example, Discovery (Sileo et al., [2019](#bib.bib38))
    encompasses 174 classes with each example taking an average of 61 tokens. Therefore,
    the minimum demonstration for 1 shot/class already exceeds 10K tokens. Normally,
    LLMs demand more than 1 shot/class to understand the nuances of different fine-grained
    labels. Thus, this task becomes a natural testbed for long-context understanding.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提议在极端标签分类任务 (Anil et al., [2022](#bib.bib4); Milios et al., [2023](#bib.bib28))
    上采用上下文学习 (ICL) 来评估长上下文 LLMs。与先前的任务不同，上下文学习要求 LLMs 通过扫描整个输入来识别任务，以理解标签空间。这一任务需要
    LLMs 理解整个输入以进行预测。由于标签空间庞大，任务示例很容易变成长序列。例如，Discovery (Sileo et al., [2019](#bib.bib38))
    包含 174 个类别，每个示例平均占用 61 个标记。因此，1 shot/class 的最低示例已经超过 10K 个标记。通常，LLMs 需要超过 1 shot/class
    才能理解不同细粒度标签的细微差别。因此，这一任务成为了长上下文理解的自然测试平台。
- en: '![Refer to caption](img/fbeec7c5caa7ca870132f02e924665ec.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fbeec7c5caa7ca870132f02e924665ec.png)'
- en: 'Figure 3: Results for representative models across different evaluation datasets.
    The performance greatly decreases as the task becomes more challenging. Some models
    even decay linearly w.r.t the demonstration length.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同评估数据集上的代表性模型结果。随着任务变得更加具有挑战性，性能大幅下降。一些模型甚至会随着示例长度的增加而线性衰退。
- en: To systematically assess how these extended input capabilities affect model
    performance in the realm of fine-grained text classification with in-context learning,
    we have compiled a benchmark, i.e. LongICLBench, consisting of six carefully-selected
    tasks with different difficulty levels in terms of context length and label space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统评估这些扩展输入能力如何影响模型在细粒度文本分类中的性能，我们编制了一个基准，即 LongICLBench，包含六个精心挑选的任务，这些任务在上下文长度和标签空间的难度等级上各不相同。
- en: We evaluate the performance of 13 long-context LLMs and find that the performance
    of the models uniformly dips as the task becomes more complex (e.g. requiring
    longer demonstration) as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Long-context LLMs Struggle with Long In-context Learning"). Some models like
    Qwen and Mistral even degrade linearly w.r.t the input length. Simultaneously,
    most of the models can benefit from the extensive demonstration if the length
    is within a certain range. As the input grows longer, it either hurts or makes
    the performance fluctuate as shown in Figure [1](#S0.F1 "Figure 1 ‣ Long-context
    LLMs Struggle with Long In-context Learning"). Moreover, we make further analysis
    on the distribution of label position to investigate the factors that affect the
    long in-context learning capability of these models. It is shown that the position
    distribution of instances in the prompt can dramatically influence the performance
    of some of the evaluated models including GPT4-turbo.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了13种长上下文LLM的性能，发现随着任务复杂性的增加（例如，需要更长的演示），模型的性能普遍下降，如图[3](#S1.F3 "Figure 3
    ‣ 1 Introduction ‣ Long-context LLMs Struggle with Long In-context Learning")所示。一些模型如Qwen和Mistral甚至会随着输入长度的增加而线性退化。同时，大多数模型如果演示长度在一定范围内，可以从广泛的演示中受益。随着输入变长，性能要么受到影响，要么表现波动，如图[1](#S0.F1
    "Figure 1 ‣ Long-context LLMs Struggle with Long In-context Learning")所示。此外，我们进一步分析了标签位置的分布，以调查影响这些模型长期上下文学习能力的因素。研究表明，提示中的实例位置分布会显著影响一些评估模型的性能，包括GPT4-turbo。
- en: 'In a nutshell, our contributions to this work can be summarized as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们对这项工作的贡献可以总结如下：
- en: '- We have developed LongICLBench, dedicated to assessing long in-context learning
    tasks for large language models. This benchmark serves as a complement to earlier
    benchmarks that concentrated on tasks like long document summarization, question
    answering (QA), or retrieval, focusing instead on long in-context learning.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '- 我们开发了LongICLBench，专注于评估大语言模型的长上下文学习任务。这个基准测试作为对早期专注于长文档摘要、问答（QA）或检索等任务的基准的补充，而重点关注长上下文学习。'
- en: '- We evaluate a line of recent long-context LLMs on LongICLBench and reveal
    their performances with gradually changed difficulty levels. Simultaneously, we
    find the sensitivity of some of the long-context LLMs regarding instance position
    in the prompt. We hope the evaluation results can provide more insights for the
    improvement of the design of long-context large language models.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '- 我们在LongICLBench上评估了一系列近期的长上下文LLM，并揭示了它们在逐渐增加的难度水平下的表现。同时，我们发现一些长上下文LLM对提示中的实例位置非常敏感。我们希望这些评估结果能够为长上下文大型语言模型设计的改进提供更多见解。'
- en: 2 Related Work
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Long In-context Learning on LLMs As pre-trained language models continue to
    grow in size, in-context learning (ICL) has emerged as a favored approach for
    addressing a wide array of tasks without the need for extensive fine-tuning  (Dong
    et al., [2023](#bib.bib16)). A body of research has established that increasing
    the number of example demonstrations can enhance ICL performance  (Liu et al.,
    [2022](#bib.bib25); Wu et al., [2023](#bib.bib47)). Nonetheless, there are studies
    indicating that longer input prompts can actually diminish performance  (Liu et al.,
    [2023](#bib.bib27)), with the effectiveness of prior large language models (LLMs)
    being constrained by the maximum sequence length encountered during their training.
    It is also claimed in previous works that LLM+ICL falls short on specification-heavy
    tasks due to inadequate long-text understanding ability  (Peng et al., [2023c](#bib.bib34)).
    To counter this issue, various works have introduced memory augmentation and extrapolation
    techniques to support ICL with an extensive set of demonstrations  (Li et al.,
    [2023c](#bib.bib24); Wang et al., [2023](#bib.bib46)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文学习在LLMs中随着预训练语言模型规模的增长，上下文学习（ICL）已成为解决各种任务的一种受欢迎的方法，而无需广泛的微调（Dong et al.,
    [2023](#bib.bib16)）。大量研究已证明，增加示例演示的数量可以提高ICL性能（Liu et al., [2022](#bib.bib25)；Wu
    et al., [2023](#bib.bib47)）。然而，也有研究表明，较长的输入提示实际上可能会降低性能（Liu et al., [2023](#bib.bib27)），由于在训练过程中遇到的最大序列长度，先前的大型语言模型（LLMs）的有效性受到限制。先前的研究还表明，LLM+ICL在要求规范的任务上由于缺乏长文本理解能力而表现不佳（Peng
    et al., [2023c](#bib.bib34)）。为了解决这一问题，各种工作引入了记忆增强和外推技术，以支持ICL与广泛的演示集（Li et al.,
    [2023c](#bib.bib24)；Wang et al., [2023](#bib.bib46)）。
- en: Long Context Techniques over LLMs The effectiveness of Transformer-based models
    is hindered by the quadratic increase in computational cost relative to sequence
    length, particularly in handling long context inputs. Recent efforts have explored
    various strategies to address this challenge. Some studies have pursued continued
    fine-tuning of the LLM with longer context inputs, aiming to adapt the model to
    extended sequences  (Rozière et al., [2024](#bib.bib37); Tworkowski et al., [2023](#bib.bib44)).
    Others have leveraged techniques such as position extrapolation and interpolation,
    building upon relative rotary positional embedding  (Su et al., [2021](#bib.bib39)),
    to extend input length beyond the training phase  (Press et al., [2022](#bib.bib35);
    Chen et al., [2023a](#bib.bib10)). Additionally, a range of approaches has been
    proposed to mitigate computational issues, including sliding memory window and
    chunk segmentation methods  (Hao et al., [2022](#bib.bib19); Ratner et al., [2023](#bib.bib36);
    Zhu et al., [2024](#bib.bib54)). Furthermore, alternative architectures beyond
    the Transformer have been explored to handle long inputs more naturally, such
    as selective-state-spaces models, which represent a variation of recurrent neural
    networks  Peng et al. ([2023a](#bib.bib32)); Gu & Dao ([2023](#bib.bib18)). These
    diverse approaches claim that they can enhance the capabilities of LLMs in processing
    long context inputs more efficiently.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文技术与大语言模型（LLMs）相比，基于变换器（Transformer）的模型在处理长上下文输入时，由于计算成本与序列长度呈二次增长，效果受到限制。近期的研究探索了多种策略来应对这一挑战。一些研究通过对LLM进行更长上下文输入的持续微调，旨在使模型适应扩展的序列（Rozière
    et al., [2024](#bib.bib37); Tworkowski et al., [2023](#bib.bib44)）。其他研究则利用了位置外推和插值等技术，基于相对旋转位置嵌入（Su
    et al., [2021](#bib.bib39)），将输入长度扩展到训练阶段之外（Press et al., [2022](#bib.bib35); Chen
    et al., [2023a](#bib.bib10)）。此外，还提出了各种方法来缓解计算问题，包括滑动内存窗口和块分段方法（Hao et al., [2022](#bib.bib19);
    Ratner et al., [2023](#bib.bib36); Zhu et al., [2024](#bib.bib54)）。此外，还探索了变换器之外的替代架构，以更自然地处理长输入，例如选择状态空间模型，它们代表了递归神经网络的一种变体（Peng
    et al., [2023a](#bib.bib32)；Gu & Dao ([2023](#bib.bib18)）。这些多样化的方法声称能够提高LLMs在处理长上下文输入时的效率。
- en: Long Context Evaluation Due to the imperious demands for the support of long-range
    LLMs, there is a series of benchmarks focusing on long context evaluation. Long-Range
    Arena  (Tay et al., [2021](#bib.bib41)) includes tasks consisting of sequences
    ranging from 1K to 16K tokens to evaluate variations of fast Transformers. LongBench
     (Bai et al., [2023b](#bib.bib6)) comprises 21 bilingual datasets within 6 types
    of tasks with an average length of around 6k words, which have been processed
    in a unified format to enable effortless evaluation. L-Eval Benchmark  (An et al.,
    [2023](#bib.bib3)) supports 20 sub-tasks with input lengths of 3K to 200K tokens.
    LooGLE  (Li et al., [2023b](#bib.bib23)) focuses on summarization and four types
    of long dependency QA tasks with test instances exceeding 100k words. Most recently,
    $\infty$Bench  (Zhang et al., [2024](#bib.bib52)) encompasses 12 tasks, collecting
    from realistic, auto-generated, and human-annotated datasets with an average length
    of 200K tokens. Another recent work explores the impact of extending input lengths
    on the capabilities of Large Language Models, especially on reasoning tasks  (Levy
    et al., [2024](#bib.bib21)). Versatile as these benchmarks, none of them focus
    on exploring the capability of LLMs confronted with long in-context learning with
    extreme label space, which is quite different from the tasks of long-document
    understanding or synthetic needle in a haystack. Thus, our LongICLBench is proposed
    to fill the niche and make a more comprehensive long-context evaluation for LLMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文评估 由于对长范围LLMs的强烈需求，出现了一系列专注于长上下文评估的基准。Long-Range Arena  (Tay et al., [2021](#bib.bib41))
    包含任务，序列长度从1K到16K标记，用于评估快速Transformer的变体。LongBench  (Bai et al., [2023b](#bib.bib6))
    包括21个双语数据集，涵盖6种任务，平均长度约为6k词，已处理成统一格式以便轻松评估。L-Eval Benchmark  (An et al., [2023](#bib.bib3))
    支持20个子任务，输入长度从3K到200K标记。LooGLE  (Li et al., [2023b](#bib.bib23)) 关注于总结和四种长依赖QA任务，测试实例超过100k词。最近，$\infty$Bench
     (Zhang et al., [2024](#bib.bib52)) 包括12个任务，来自实际、自动生成和人工标注的数据集，平均长度为200K标记。另一个近期的研究探讨了扩展输入长度对大型语言模型能力的影响，尤其是推理任务
     (Levy et al., [2024](#bib.bib21))。尽管这些基准多样，但没有一个专注于探索LLMs在面对极端标签空间的长上下文学习时的能力，这与长文档理解或合成针在干草堆中的任务有很大不同。因此，我们提出的LongICLBench旨在填补这一空白，为LLMs提供更全面的长上下文评估。
- en: Extreme-label Classification Extreme-label Classification involves categorizing
    data into one of an extremely large number of labels, and finds application across
    a variety of real-world domains such as emotion classification from text, named
    entity recognition, and biological function prediction, each requiring precise
    differentiation among vast label spaces (Zhang et al., [2017](#bib.bib53); Sileo
    et al., [2019](#bib.bib38); Demszky et al., [2020](#bib.bib13); Ding et al., [2021](#bib.bib14)).
    Existing methods to tackle Extreme-label Classification tasks range from embedding-based
    approaches to fine-tuned retrievals (Bhatia et al., [2015](#bib.bib7); Vulić et al.,
    [2021](#bib.bib45)), focusing on efficiently managing and leveraging the large
    label space. However, integrating this task with long-context large language models
    presents unique challenges. The sheer scale of the label space in extreme-label
    classification complicates the in-context learning process, where LLMs are expected
    to discern fine-grained differences among labels based on extensive context  (Milios
    et al., [2023](#bib.bib28)). These challenges make the proposed LongICLBench with
    a range of difficulty levels a good testing scenario to evaluate the capability
    of long-context large language models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 极端标签分类 **极端标签分类** 涉及将数据分类为极大量标签中的一个，并在各种实际应用领域中得到应用，如文本情感分类、命名实体识别和生物功能预测，这些领域都需要在庞大的标签空间中进行精确区分 (Zhang
    et al., [2017](#bib.bib53); Sileo et al., [2019](#bib.bib38); Demszky et al.,
    [2020](#bib.bib13); Ding et al., [2021](#bib.bib14))。现有的方法包括基于嵌入的方法和微调检索方法 (Bhatia
    et al., [2015](#bib.bib7); Vulić et al., [2021](#bib.bib45))，重点在于高效管理和利用大量标签空间。然而，将此任务与长上下文大型语言模型集成会面临独特的挑战。极端标签分类中的标签空间规模使得上下文学习过程复杂化，其中LLMs需基于广泛的上下文区分细微差别
     (Milios et al., [2023](#bib.bib28))。这些挑战使得提出的LongICLBench具有多种难度级别，成为评估长上下文大型语言模型能力的良好测试场景。
- en: 3 Long In-context Evaluation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 长上下文评估
- en: '| Dataset | Task Type | # Classes | # Tokens/Shot | # Total Tokens |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 任务类型 | 类别数量 | 每次样本的标记数 | 总标记数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GoEmotion | Emotion Classification | 28 | 28 | [1K, 4K] |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| GoEmotion | 情感分类 | 28 | 28 | [1K, 4K] |'
- en: '| BANKING77 | Intent Classification | 77 | 28 | [2K, 11K] |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| BANKING77 | 意图分类 | 77 | 28 | [2K, 11K] |'
- en: '| TacRED | Relation Extraction | 41 | 80 | [4K, 18K] |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| TacRED | 关系抽取 | 41 | 80 | [4K, 18K] |'
- en: '| Few-NERD | Entity Recognition | 66 | 61 | [5K, 23K] |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Few-NERD | 实体识别 | 66 | 61 | [5K, 23K] |'
- en: '| DialogRE | Relation Extraction | 36 | 226 | [8K, 32K] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| DialogRE | 关系抽取 | 36 | 226 | [8K, 32K] |'
- en: '| Discovery | Discourse Marker Classification | 174 | 61 | [10K, 50K] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Discovery | 话语标记分类 | 174 | 61 | [10K, 50K] |'
- en: 'Table 1: Statistics of the collected sub-dataset in LongICLBench. We evaluate
    from 1-shot/label to 5-shot/label, which results in the shown #total token range.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LongICLBench中收集的子数据集的统计数据。我们从1-shot/label评估到5-shot/label，这导致了显示的#total token范围。
- en: 3.1 Long In-context Benchmark
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 长期上下文基准
- en: To support the evaluation of long in-context learning on extreme-label classification
    tasks in different domains and various difficulty levels, we collect six datasets
    containing context length from short to long. In order to balance the sequence
    token length within each dataset and the goal of evaluation for long in-context
    learning, we keep a subset of the classes among all the classes to format evaluation
    sets around 1 round, 2 rounds, 3 rounds, 4 rounds, and 5 rounds correspondingly,
    where each round represent a complete set of examples containing all unique chosen
    labels. We sample the number of instances from each of the classes evenly to reduce
    the bias resulting from the label distribution. The statistics of the datasets
    are described in detail in Table [1](#S3.T1 "Table 1 ‣ 3 Long In-context Evaluation
    ‣ Long-context LLMs Struggle with Long In-context Learning") and Appendix [A.1](#A1.SS1
    "A.1 Additional Datasets ‣ Appendix A Appendix ‣ Long-context LLMs Struggle with
    Long In-context Learning").
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持对不同领域和各种难度级别的极端标签分类任务中长期上下文学习的评估，我们收集了六个数据集，包含从短到长的上下文长度。为了平衡每个数据集中的序列标记长度和长期上下文学习的评估目标，我们在所有类别中保留了一部分类别，将评估集格式化为1轮、2轮、3轮、4轮和5轮，其中每轮代表一个完整的示例集，包含所有唯一选择的标签。我们从每个类别中均匀抽样实例，以减少标签分布带来的偏差。数据集的统计数据在表[1](#S3.T1
    "Table 1 ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long
    In-context Learning")和附录[A.1](#A1.SS1 "A.1 Additional Datasets ‣ Appendix A Appendix
    ‣ Long-context LLMs Struggle with Long In-context Learning")中详细描述。
- en: BANKING77  (Casanueva et al., [2020](#bib.bib9)) is a banking-domain intent
    detection dataset comprising 13,083 annotated examples over 77 intents. We keep
    all of the types of intents, and each of the instances contains around 28 tokens.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: BANKING77 (Casanueva et al., [2020](#bib.bib9)) 是一个银行领域意图检测数据集，包含13,083个标注示例，覆盖77种意图。我们保留了所有意图类型，每个示例包含约28个标记。
- en: TacRED  (Zhang et al., [2017](#bib.bib53)) is a large-scale relation extraction
    dataset with 106,264 examples built over news and web text from the corpus used
    in the yearly TAC Knowledge Base Population. Only one relation is labeled for
    each of the sentences in the dataset. It covers 41 relation types in total, with
    an average length of 80 tokens for each example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TacRED (Zhang et al., [2017](#bib.bib53)) 是一个大规模的关系抽取数据集，包含106,264个示例，建立在每年TAC知识库人口数据中使用的新闻和网页文本的语料库上。数据集中的每个句子只标注一个关系。总共涵盖41种关系类型，每个示例的平均长度为80个标记。
- en: DialogRE  (Yu et al., [2020](#bib.bib50)) is a human-annotated dialogue-based
    relation extraction dataset composed of 1788 dialogues from a famous American
    television comedy, Friends, with 36 possible relation types existing between an
    argument pair in a dialogue. Each example contains around 226 tokens on average.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DialogRE (Yu et al., [2020](#bib.bib50)) 是一个人工标注的基于对话的关系抽取数据集，由1788段来自著名美国情景喜剧《老友记》的对话组成，其中存在36种可能的关系类型，每对对话中的论点之间。每个示例平均包含约226个标记。
- en: Discovery  (Sileo et al., [2019](#bib.bib38)) automatically discovers sentence
    pairs with relevant discourse markers and curates a large dataset containing 174
    discourse markers with at least 10K examples each. Each example contains around
    61 tokens. There are 174 types of discourse markers. This dataset is the most
    difficult task with fine-grained labels.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Discovery (Sileo et al., [2019](#bib.bib38)) 自动发现具有相关话语标记的句子对，并策划了一个包含174种话语标记的大型数据集，每种话语标记至少有10K个示例。每个示例包含约61个标记。共有174种话语标记。这是最困难的任务，具有细粒度标签。
- en: 3.2 Model and Experimental Setup
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 模型与实验设置
- en: In the exploration of in-context learning for extreme-label classification,
    we conduct a comprehensive evaluation for a series of recent open-source long-context
    language models of size around 7B parameters. We also include the SoTA models
    like Gemini and GPT-4-turbo. [Table 2](#S3.T2 "Table 2 ‣ 3.2 Model and Experimental
    Setup ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long In-context
    Learning") provides an overview of the models investigated, highlighting the innovations
    in their architecture specifically for dealing with long context. We can observe
    that there are multiple strategies adopted to extend the context window. Some
    of the models support the training context window size while some models support
    length extrapolation. RWKV (Peng et al., [2023a](#bib.bib32)) and Mamba (Gu &
    Dao, [2023](#bib.bib18)) are the two new RNN-like architectures to decrease attention
    complexity, which would allow the model to easily extrapolate to much longer inputs
    with linear time/memory complexity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在对极端标签分类的上下文学习探索中，我们对一系列最近的开源长上下文语言模型（约 7B 参数）进行了全面评估。我们还包括了如 Gemini 和 GPT-4-turbo
    等最先进的模型。[表 2](#S3.T2 "表 2 ‣ 3.2 模型和实验设置 ‣ 3 长上下文评估 ‣ 长上下文 LLM 对长上下文学习的困难") 提供了所研究模型的概述，突出显示了其架构中特别针对长上下文的创新。我们可以观察到，采用了多种策略来扩展上下文窗口。一些模型支持训练上下文窗口大小，而一些模型支持长度外推。RWKV（Peng
    等， [2023a](#bib.bib32)）和 Mamba（Gu & Dao，[2023](#bib.bib18)）是两个新的类似 RNN 的架构，以降低注意力复杂性，这将允许模型轻松外推到更长的输入，具有线性时间/内存复杂性。
- en: '| Model | Size | Initialization | Strategy | Train | Support |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 初始化 | 策略 | 训练 | 支持 |'
- en: '| [Gemma-7B-base](#bib.bib43) | 7B | Gemma | RoPE + LF | 8K | 8K |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| [Gemma-7B-base](#bib.bib43) | 7B | Gemma | RoPE + LF | 8K | 8K |'
- en: '| [LLaMA-2-7B-32K](#bib.bib27) | 7B | LLaMA-2 | Position Interpolation | 32K
    | 32K |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| [LLaMA-2-7B-32K](#bib.bib27) | 7B | LLaMA-2 | 位置插值 | 32K | 32K |'
- en: '| [ChatGLM3-6B-32K](#bib.bib51) | 6B | ChatGLM | Position Encoding Scheme |
    32K | 32K |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| [ChatGLM3-6B-32K](#bib.bib51) | 6B | ChatGLM | 位置编码方案 | 32K | 32K |'
- en: '| [Qwen-1.5-7B-base](#bib.bib5) | 7B | Qwen | NTK-Aware Interpolation | 32K
    | 32K |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| [Qwen-1.5-7B-base](#bib.bib5) | 7B | Qwen | NTK 感知插值 | 32K | 32K |'
- en: '| [Mistral-7B-v0.2-base](#bib.bib20) | 7B | Mistral | LF | 32K | 32K |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| [Mistral-7B-v0.2-base](#bib.bib20) | 7B | Mistral | LF | 32K | 32K |'
- en: '| [LLaMA-2-7B-LongLora](#bib.bib11) | 7B | LLaMA-2 | Shifted Short Attention
    | 100K | 100K |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| [LLaMA-2-7B-LongLora](#bib.bib11) | 7B | LLaMA-2 | 位移短期注意力 | 100K | 100K
    |'
- en: '| [Yi-6B-200K](#bib.bib2) | 6B | Yi | Position Interpolation +LF | 200K | 200K
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| [Yi-6B-200K](#bib.bib2) | 6B | Yi | 位置插值 + LF | 200K | 200K |'
- en: '| [InternLM2-7B-base](#bib.bib8) | 7B | InternLM | Dynamic NTK | 32K | 200K
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| [InternLM2-7B-base](#bib.bib8) | 7B | InternLM | 动态 NTK | 32K | 200K |'
- en: '| [Long-LLaMA-code-7B](#bib.bib44) | 7B | LLaMA-2 | Focused Transformer | 8K
    | 256K |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| [Long-LLaMA-code-7B](#bib.bib44) | 7B | LLaMA-2 | 集中型变换器 | 8K | 256K |'
- en: '| [RWKV-5-World](#bib.bib32) | 3B | RWKV | Attention-free Model | 4K | $\infty$
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| [RWKV-5-World](#bib.bib32) | 3B | RWKV | 无注意力模型 | 4K | $\infty$ |'
- en: '| [Mamba-2.8B](#bib.bib18) | 2.8B | Mamba | State Space Model | 2K | $\infty$
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| [Mamba-2.8B](#bib.bib18) | 2.8B | Mamba | 状态空间模型 | 2K | $\infty$ |'
- en: '| [Gemini-1.0-Pro](#bib.bib42) | - | Gemini | Ring Attention | 32K | 32K |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| [Gemini-1.0-Pro](#bib.bib42) | - | Gemini | 环形注意力 | 32K | 32K |'
- en: '| [GPT4-turbo](#bib.bib1) | - | GPT-4 | - | - | 128K |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| [GPT4-turbo](#bib.bib1) | - | GPT-4 | - | - | 128K |'
- en: 'Table 2: The overview of the evaluated models. We utilize base models before
    instruction-tuning except Gemini and GPT4-turbo. LF means fine-tuning the model
    on longer-context corpus after pre-training.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：评估模型的概述。我们在进行指令调优之前使用基础模型，除了 Gemini 和 GPT4-turbo。LF 表示在预训练后对模型进行长上下文语料库的微调。
- en: We construct a prompt following the template as shown in [A.2](#A1.SS2 "A.2
    Prompting Template ‣ Appendix A Appendix ‣ Long-context LLMs Struggle with Long
    In-context Learning") for each of the datasets. To fairly evaluate the open-source
    and API-based models with a series of input lengths, we sample the same example
    set for all the models with labels distributed evenly to ensure an unbiased distribution
    for the in-context demonstration. For instance, an input of one round will include
    one set of examples traversing all the types, and 5 rounds will contain instances
    from each of the labels 5 times. For testing, we sample 500 examples from the
    test set of each dataset, simultaneously ensuring an even distribution in terms
    of the type of labels. All the open-source models are loaded from the weights
    in HuggingFace¹¹1[https://huggingface.co](https://huggingface.co), while the API-based
    models are called with the scripts in the official documentations ²²2[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api),
    [https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照[A.2](#A1.SS2 "A.2 Prompting Template ‣ Appendix A Appendix ‣ Long-context
    LLMs Struggle with Long In-context Learning")中显示的模板，为每个数据集构建提示。为了公平评估开源和基于API的模型在一系列输入长度下的表现，我们对所有模型采样相同的示例集，并均匀分布标签，以确保上下文演示的无偏分布。例如，一个轮次的输入将包含一组遍历所有类型的示例，而5轮次将包含每个标签的实例5次。对于测试，我们从每个数据集的测试集中采样500个示例，同时确保标签类型的均匀分布。所有开源模型都从HuggingFace¹¹1[https://huggingface.co](https://huggingface.co)的权重中加载，而基于API的模型则通过官方文档中的脚本调用²²2[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)、[https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview)。
- en: 3.3 Experiment Result
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实验结果
- en: The main evaluation results are demonstrated in Table [4](#S3.T4 "Table 4 ‣
    3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle
    with Long In-context Learning"), Table [4](#S3.T4 "Table 4 ‣ 3.3 Experiment Result
    ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long In-context
    Learning"), Table [6](#S3.T6 "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context
    Evaluation ‣ Long-context LLMs Struggle with Long In-context Learning") and Table [6](#S3.T6
    "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context
    LLMs Struggle with Long In-context Learning"). For the entity recognition and
    relationship extraction dataset, we use the F1 score as the evaluation metric,
    and Accuracy is utilized for the other datasets. From the presented results, generally,
    we can find that models of Transformer-based architecture perform consistently
    better than the RNN-based ones in all the evaluated datasets. However, both of
    them are still falling behind the powerful API-based models, especially GPT4-turbo.
    For a relatively simple task like BANKING77, whose context length from 1 round
    to 5 rounds is 2K to 14 K, most of the models can benefit from the extensive context
    with more demonstrations. As shown in Figure [1](#S0.F1 "Figure 1 ‣ Long-context
    LLMs Struggle with Long In-context Learning") and Table [4](#S3.T4 "Table 4 ‣
    3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle
    with Long In-context Learning"), from 2K to 4K, there is either a huge increase
    nearly doubling the accuracy, or a complete failure for most of the open-source
    models. After 3 rounds, limited performance gain can be achieved by adding more
    examples. When it comes to more complicated tasks like TacRED and DialogueRE in
    Table [4](#S3.T4 "Table 4 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation
    ‣ Long-context LLMs Struggle with Long In-context Learning") and Table [6](#S3.T6
    "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context
    LLMs Struggle with Long In-context Learning"), which are more urgently requiring
    the capability of long-context comprehension, the overall performance of all the
    few-shot models drops compared to BANKING77\. As shown in the middle plot of Figure [1](#S0.F1
    "Figure 1 ‣ Long-context LLMs Struggle with Long In-context Learning"), only GPT4-turbo
    can consistently benefit from more demonstrations, all of the other models reach
    their peak at the middle with context length around 20K.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的评估结果在表格[4](#S3.T4 "Table 4 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation
    ‣ Long-context LLMs Struggle with Long In-context Learning")、表格[4](#S3.T4 "Table
    4 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle
    with Long In-context Learning")、表格[6](#S3.T6 "Table 6 ‣ 3.3 Experiment Result
    ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long In-context
    Learning")和表格[6](#S3.T6 "Table 6 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation
    ‣ Long-context LLMs Struggle with Long In-context Learning")中展示。对于实体识别和关系提取数据集，我们使用F1分数作为评估指标，而对其他数据集使用准确率。从展示的结果来看，一般来说，基于Transformer架构的模型在所有评估的数据集上表现都比基于RNN的模型更好。然而，它们仍然落后于强大的基于API的模型，特别是GPT4-turbo。对于像BANKING77这样相对简单的任务，其上下文长度从1轮到5轮是2K到14K，大多数模型可以从更多示例的广泛上下文中受益。如图[1](#S0.F1
    "Figure 1 ‣ Long-context LLMs Struggle with Long In-context Learning")和表格[4](#S3.T4
    "Table 4 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context
    LLMs Struggle with Long In-context Learning")所示，从2K到4K，几乎有一倍的准确率提升，或大多数开源模型完全失败。在经过3轮之后，通过增加更多示例只能实现有限的性能提升。当涉及到表格[4](#S3.T4
    "Table 4 ‣ 3.3 Experiment Result ‣ 3 Long In-context Evaluation ‣ Long-context
    LLMs Struggle with Long In-context Learning")和表格[6](#S3.T6 "Table 6 ‣ 3.3 Experiment
    Result ‣ 3 Long In-context Evaluation ‣ Long-context LLMs Struggle with Long In-context
    Learning")中的TacRED和DialogueRE等更复杂的任务时，这些任务更迫切需要长上下文理解能力，所有少量示例模型的整体表现相比BANKING77有所下降。如图[1](#S0.F1
    "Figure 1 ‣ Long-context LLMs Struggle with Long In-context Learning")中间的图表所示，只有GPT4-turbo能够
    consistently 从更多示例中受益，所有其他模型在上下文长度大约为20K时达到峰值。
- en: For the most challenging Discovery dataset, which has an extremely large label
    space including 174 classes, one round of traversing for all the label possibilities
    has already made up a context length of 10K. In this extreme case, all of the
    models, including GPT4-turbo, fail to tell the difference among the fine-grained
    types, leading to a score of 0\. The results across different datasets reveal
    the models’ capability to understand different types of tasks. Our initial hypothesis
    suggests that the strongest LLMs like GPT-4-turbo are capped at a certain complexity
    level between DialogRE and Discovery.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最具挑战性的Discovery数据集，该数据集具有包括174类的极大标签空间，已经通过对所有标签可能性的一个轮次遍历构成了10K的上下文长度。在这种极端情况下，包括GPT4-turbo在内的所有模型都无法区分细粒度的类型，导致得分为0。不同数据集的结果揭示了模型理解不同类型任务的能力。我们的初步假设是，像GPT-4-turbo这样最强大的LLM在DialogRE和Discovery之间存在某种复杂度限制。
- en: Another interesting observation we have is that some LLMs’ performance on the
    extreme-label ICL seems highly predictable. According to [Figure 3](#S1.F3 "Figure
    3 ‣ 1 Introduction ‣ Long-context LLMs Struggle with Long In-context Learning"),
    the performance of Qwen and Mistral are almost linear w.r.t the demonstration
    length. This reveals that there might be an underlying mathematical relation between
    performance and the task complexity for ICL.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到一些LLM在极端标签ICL上的表现似乎高度可预测。根据[图3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Long-context
    LLMs Struggle with Long In-context Learning")，Qwen和Mistral的表现几乎是与演示长度线性相关的。这揭示了ICL的性能和任务复杂性之间可能存在某种数学关系。
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 支持 | 1R | 2R | 3R | 4R | 5R |'
- en: '| Context Tokens |  |  | 2K | 4K | 7K | 9K | 14K |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 上下文标记 |  |  | 2K | 4K | 7K | 9K | 14K |'
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32K | 30.2 | 70.4 | 72.0 | 75.6 | 77.2 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32K | 30.2 | 70.4 | 72.0 | 75.6 | 77.2 |'
- en: '| ChatGLM3-6B-32K | 6B | 32K | 16.6 | 23.2 | 22.4 | 22.8 | 8.8 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32K | 16.6 | 23.2 | 22.4 | 22.8 | 8.8 |'
- en: '| Qwen-1.5-7B-base | 7B | 32K | 21.6 | 52.8 | 61.4 | 66.0 | 67.8 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32K | 21.6 | 52.8 | 61.4 | 66.0 | 67.8 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 29.8 | 43.6 | 66.4 | 67.8 | 64.0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 29.8 | 43.6 | 66.4 | 67.8 | 64.0 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200K | 25.8 | 0 | 0 | 0 | 1.2 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200K | 25.8 | 0 | 0 | 0 | 1.2 |'
- en: '| InternLM2-7B-base | 7B | 200K | 5.6 | 0 | 0 | 0 | 0 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200K | 5.6 | 0 | 0 | 0 | 0 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 3.0 | 19.4 | 28.0 | 31.6 | 32.6 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 3.0 | 19.4 | 28.0 | 31.6 | 32.6 |'
- en: '| RWKV-5-World | 7B | 4K | 8.6 | 21.2 | 0.4 | 0 | 0 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 4K | 8.6 | 21.2 | 0.4 | 0 | 0 |'
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Gemini-1.0-Pro | N/A | 32K | 33.4 | 41.4 | 40.6 | 45.6 | 50.2 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.0-Pro | N/A | 32K | 33.4 | 41.4 | 40.6 | 45.6 | 50.2 |'
- en: '| GPT4-turbo | N/A | 128K | 73.5 | 80.5 | 82.0 | 83.5 | 84.4 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128K | 73.5 | 80.5 | 82.0 | 83.5 | 84.4 |'
- en: '| SoTA (RoBERTA + ICDA) | N/A | - | 94.4 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SoTA (RoBERTA + ICDA) | N/A | - | 94.4 |'
- en: 'Table 3: BANKING77 result with respect to increasing context length. 1R represents
    one round of traversing all the instances with unique label.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：BANKING77结果与上下文长度增加的关系。1R表示对所有具有唯一标签的实例进行一次遍历。
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 支持 | 1R | 2R | 3R | 4R | 5R |'
- en: '| Context Tokens |  |  | 4K | 7K | 10K | 14K | 18K |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 上下文标记 |  |  | 4K | 7K | 10K | 14K | 18K |'
- en: '| Gemma-7B-base | 7B | 8K | 0.4 | 0.4 | 0 | 0 | 0 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8K | 0.4 | 0.4 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0.4 | 0.4 | 0.8 | 0.4 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0.4 | 0.4 | 0.8 | 0.4 |'
- en: '| ChatGLM3-6B-32K | 6B | 32K | 29.7 | 36.1 | 38.9 | 40.1 | 25.2 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32K | 29.7 | 36.1 | 38.9 | 40.1 | 25.2 |'
- en: '| Qwen-1.5-7B-base | 7B | 32K | 38.7 | 47.3 | 45.2 | 43.6 | 40.6 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32K | 38.7 | 47.3 | 45.2 | 43.6 | 40.6 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 53.3 | 53.1 | 51.6 | 48.0 | 42.3 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 53.3 | 53.1 | 51.6 | 48.0 | 42.3 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200K | 5.6 | 1.9 | 8.0 | 9.5 | 2.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200K | 5.6 | 1.9 | 8.0 | 9.5 | 2.0 |'
- en: '| InternLM2-7B-base | 7B | 200K | 29.6 | 27.2 | 15.5 | 10.7 | 8.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200K | 29.6 | 27.2 | 15.5 | 10.7 | 8.0 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 3.8 | 7.1 | 4.1 | 6.6 | 4.9 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 3.8 | 7.1 | 4.1 | 6.6 | 4.9 |'
- en: '| RWKV-5-World | 7B | 1K | 2.3 | 2.6 | 1.0 | 0 | 1.2 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 1K | 2.3 | 2.6 | 1.0 | 0 | 1.2 |'
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Gemini-1.0-Pro | N/A | 32K | 71.4 | 77.8 | 78.2 | 77.4 | 76.8 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.0-Pro | N/A | 32K | 71.4 | 77.8 | 78.2 | 77.4 | 76.8 |'
- en: '| GPT4-turbo | N/A | 128K | 74.4 | 76.5 | 79.5 | 80.4 | 84.2 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128K | 74.4 | 76.5 | 79.5 | 80.4 | 84.2 |'
- en: '| SoTA (DeepStruct) | N/A | - | 76.8 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| SoTA (DeepStruct) | N/A | - | 76.8 |'
- en: 'Table 4: TacRED result with respect to increasing context length.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：关于增加上下文长度的 TacRED 结果。
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 支持 | 1R | 2R | 3R | 4R | 5R |'
- en: '| Context Tokens |  |  | 8K | 13K | 19K | 25K | 32K |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 上下文令牌 |  |  | 8K | 13K | 19K | 25K | 32K |'
- en: '| Gemma-7B-base | 7B | 8K | 16.3 | 0 | 0 | 0 | 0 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8K | 16.3 | 0 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32K | 6.9 | 13.9 | 6.3 | 5.7 | 5.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32K | 6.9 | 13.9 | 6.3 | 5.7 | 5.9 |'
- en: '| ChatGLM3-6B-32K | 6B | 32K | 5.1 | 8.9 | 8.8 | 12.4 | 10.4 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32K | 5.1 | 8.9 | 8.8 | 12.4 | 10.4 |'
- en: '| Qwen-1.5-7B-base | 7B | 32K | 14.4 | 18.4 | 15.5 | 16.4 | 13.2 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32K | 14.4 | 18.4 | 15.5 | 16.4 | 13.2 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 24.3 | 23.2 | 23.4 | 22.3 | 21.2 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 24.3 | 23.2 | 23.4 | 22.3 | 21.2 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200K | 0 | 0 | 0.8 | 0.8 | 0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200K | 0 | 0 | 0.8 | 0.8 | 0 |'
- en: '| InternLM2-7B-base | 7B | 200K | 12.2 | 13.4 | 6.4 | 2.1 | 1.1 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200K | 12.2 | 13.4 | 6.4 | 2.1 | 1.1 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 4.0 | 3.8 | 3.0 | 6.4 | 2.2 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 4.0 | 3.8 | 3.0 | 6.4 | 2.2 |'
- en: '| RWKV-5-World | 7B | 4K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 4K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Gemini-1.0-Pro | N/A | 32K | 23.6 | 29.2 | 33.2 | 26.1 | 17.3 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.0-Pro | N/A | 32K | 23.6 | 29.2 | 33.2 | 26.1 | 17.3 |'
- en: '| GPT4-turbo | N/A | 128K | 43.5 | 48.8 | 53.6 | 60.2 | 60.9 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128K | 43.5 | 48.8 | 53.6 | 60.2 | 60.9 |'
- en: '| SoTA (HiDialog) | N/A | - | 77.1 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SoTA (HiDialog) | N/A | - | 77.1 |'
- en: 'Table 5: DialogRE result with respect to increasing context length.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：关于增加上下文长度的 DialogRE 结果。
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | 支持 | 1R | 2R | 3R | 4R | 5R |'
- en: '| Context Tokens |  |  | 10K | 20K | 30K | 40K | 50K |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 上下文令牌 |  |  | 10K | 20K | 30K | 40K | 50K |'
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0 | 0 | 0 | ✗ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0 | 0 | 0 | ✗ |'
- en: '| ChatGLM3-6B-32K | 6B | 32k | 0 | 1.0 | 0 | ✗ | ✗ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32k | 0 | 1.0 | 0 | ✗ | ✗ |'
- en: '| Qwen-1.5-7B-base | 7B | 32K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 0 | 0 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200k | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200k | 0 | 0 | 0 | 0 | 0 |'
- en: '| InternLM2-7B-base | 7B | 200K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 0 | 0 | 0 | 0 | 0 |'
- en: '| RWKV-5-World | 7B | 4K | 0 | 0.2 | 0 | 0 | 0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 4K | 0 | 0.2 | 0 | 0 | 0 |'
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Gemini-1.0-Pro | N/A | 32K | 0 | 0 | 0 | ✗ | ✗ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.0-Pro | N/A | 32K | 0 | 0 | 0 | ✗ | ✗ |'
- en: '| GPT4-turbo | N/A | 128K | 1.5 | 0.5 | 0.5 | 0.5 | 0.5 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128K | 1.5 | 0.5 | 0.5 | 0.5 | 0.5 |'
- en: '| SoTA (MTL) | N/A | - | 87.4 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SoTA (MTL) | N/A | - | 87.4 |'
- en: 'Table 6: Discovery result with respect to increasing context length.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：关于增加上下文长度的发现结果。
- en: 4 Exploratory Experiment
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 探索性实验
- en: Inspired by the Lost in the Middle phenomenon  Liu et al. ([2023](#bib.bib27)),
    we take analysis experiments to explore whether the position distribution of the
    instances will make a difference in the performance for long in-context learning
    with extreme-label classification tasks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 受到刘等人（[2023](#bib.bib27)）“迷失在中间”现象的启发，我们进行分析实验，以探讨实例的位置分布是否会影响极端标签分类任务中的长期上下文学习性能。
- en: 4.1 Scattered Distribution
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 分散分布
- en: '![Refer to caption](img/6e1eba4535480ba098856774d54e32f4.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e1eba4535480ba098856774d54e32f4.png)'
- en: 'Figure 4: Visualization of accuracy for every class when instances from the
    same class are scattered V.S. grouped in the demonstration prompt.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：展示实例从同一类别分散与分组在演示提示中的准确率可视化。
- en: In our investigation, we conducted pilot experiments on TacRED, a medium-complexity
    dataset, with each label type demonstrated three times, resulting in a total of
    123 distinct instances (calculated as $41\times 3$). Within these experiments,
    instances bearing the same labels are distributed randomly to form a scattered
    configuration. For each instance, we track its relative position within the prompt
    alongside its corresponding label, thereafter computing the accuracy for each
    label class. As illustrated in the first row of Figure [4](#S4.F4 "Figure 4 ‣
    4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle
    with Long In-context Learning"), the visualization delineates the accuracy of
    each label, aligned with its position within the prompt, where diverse colors
    symbolize various label types. In scenarios where class instances are scattered,
    certain models, such as InternLM2-7B-base, demonstrate acceptable performances—approximately
    60% accuracy merely on specific labels, as highlighted by a red circle in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context
    LLMs Struggle with Long In-context Learning"), regardless of the instance placements.
    Conversely, other models, like ChatGLM3-6B-32K, exhibit robust performance across
    a broad spectrum of labels. Remarkably, the GPT4-turbo model consistently surpasses
    an 80% accuracy threshold for the majority of label types, with only a minimal
    count of exceptions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的调查中，我们对TacRED这个中等复杂度的数据集进行了初步实验，每种标签类型展示了三次，总共有123个不同的实例（计算为$41\times 3$）。在这些实验中，具有相同标签的实例被随机分布，形成散布的配置。对于每个实例，我们跟踪其在提示中的相对位置以及对应的标签，然后计算每个标签类别的准确性。如图[4](#S4.F4
    "Figure 4 ‣ 4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context
    LLMs Struggle with Long In-context Learning")的第一行所示，该可视化展示了每个标签的准确性，按其在提示中的位置对齐，其中不同的颜色表示不同的标签类型。在类实例散布的情况下，某些模型，如InternLM2-7B-base，表现出可接受的性能——仅在特定标签上大约有60%的准确性，如图[4](#S4.F4
    "Figure 4 ‣ 4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context
    LLMs Struggle with Long In-context Learning")中的红圈所示，无论实例的位置如何。相反，其他模型，如ChatGLM3-6B-32K，表现出在广泛标签上的强大性能。值得注意的是，GPT4-turbo模型在大多数标签类型上始终超过80%的准确率，只有少量例外。
- en: 4.2 Grouped Distribution
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 分组分布
- en: To facilitate a clear comparison between scattered and grouped distributions,
    we organize instances of the same class to be adjacent within the demonstration
    prompts. The impact of this reorganization on model performance, both pre and
    post-grouping, is presented in Table [7](#S4.T7 "Table 7 ‣ 4.2 Grouped Distribution
    ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle with Long In-context Learning").
    A pronounced trend emerges, highlighting a general decline in performance across
    most models after grouping instances by class. Notably, models such as Mistral-7B-v0.2-base
    and InternLM2-7B-base exhibit significant performance drops, underscoring a pronounced
    sensitivity to instance grouping. In an effort to delve deeper into this phenomenon,
    we visualize the accuracy of grouped labels in relation to their positions within
    the prompt, as illustrated in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Scattered Distribution
    ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle with Long In-context Learning").
    This visualization reveals that instances of the same class, denoted by dots of
    the same color, are positioned nearby. It became evident that some models, like
    InternLM2-7B-base, demonstrate high sensitivity to the distribution of instances,
    only handling instances with labels positioned at the end of the prompt. Conversely,
    other open-source models such as ChatGLM3-6B-32K, with a modest 3.3% drop in accuracy,
    proved to be more resilient to changes in instance positioning, maintaining high
    performance across varied positions. Surprisingly, even the GPT4-turbo is not
    immune to the challenges posed by grouped distributions, experiencing a notable
    decline in performance by 20.3%. This observed decrease in performance is consistent
    across models, unaffected by the specific positions of the labels within the prompt.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于对比分散和分组分布的效果，我们将同一类别的实例在演示提示中组织在一起。表[7](#S4.T7 "Table 7 ‣ 4.2 Grouped Distribution
    ‣ 4 Exploratory Experiment ‣ Long-context LLMs Struggle with Long In-context Learning")展示了这种重新组织对模型性能的影响，包括分组前后的表现。一个明显的趋势是，大多数模型在按类别分组后性能普遍下降。特别是，Mistral-7B-v0.2-base和InternLM2-7B-base等模型表现出显著的性能下降，表明对实例分组的敏感性较高。为了深入了解这一现象，我们将分组标签的准确度与它们在提示中的位置进行了可视化，如图[4](#S4.F4
    "Figure 4 ‣ 4.1 Scattered Distribution ‣ 4 Exploratory Experiment ‣ Long-context
    LLMs Struggle with Long In-context Learning")所示。该可视化显示，同一类别的实例（用相同颜色的点表示）被放置在附近。显而易见，某些模型，如InternLM2-7B-base，对实例分布高度敏感，只处理那些标签位置位于提示末尾的实例。相比之下，其他开源模型如ChatGLM3-6B-32K，虽然准确度下降了3.3%，但对实例位置的变化表现出更高的韧性，在不同位置上保持了高性能。令人惊讶的是，即使是GPT4-turbo也未能避免分组分布带来的挑战，性能显著下降了20.3%。这种观察到的性能下降在模型间是一致的，不受标签在提示中具体位置的影响。
- en: '| Model | Param | Support | Scatter | Grouped | $\Delta$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Model | Param | Support | Scatter | Grouped | $\Delta$ |'
- en: '| Context Tokens |  |  | 10K |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Context Tokens |  |  | 10K |'
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0.4 | 3.0 | +2.6 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32K | 0.4 | 3.0 | +2.6 |'
- en: '| ChatGLM3-6B-32K | 6B | 32K | 38.9 | 35.6 | -3.3 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32K | 38.9 | 35.6 | -3.3 |'
- en: '| Qwen-1.5-7B-base | 7B | 32K | 45.2 | 33.0 | -12.2 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32K | 45.2 | 33.0 | -12.2 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 51.6 | 5.1 | -46.5 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 51.6 | 5.1 | -46.5 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200K | 8.0 | 0 | -8 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200K | 8.0 | 0 | -8 |'
- en: '| InternLM2-7B-base | 7B | 200K | 15.5 | 4.8 | -9.7 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200K | 15.5 | 4.8 | -9.7 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 4.1 | 0 | -4.1 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 4.1 | 0 | -4.1 |'
- en: '| RWKV-5-World | 7B | 4K | 1.0 | 3.6 | +2.6 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 4K | 1.0 | 3.6 | +2.6 |'
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 |'
- en: '| GPT4-turbo | N/A | 128K | 79.5 | 59.2 | -20.3 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128K | 79.5 | 59.2 | -20.3 |'
- en: 'Table 7: Exploratory Result on TacRED 3 Round. Grouped means forcing the same-typed
    demonstration examples near by each other instead of randomly distributing in
    the prompt.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：TacRED 3轮的探索结果。分组意味着将相同类型的演示示例放在彼此附近，而不是在提示中随机分布。
- en: 5 Conclusion
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In summary, our research explores the capability of large language models on
    long in-context learning tasks, particularly in extreme-label classification scenarios.
    We curate a dataset LongICLBench consisting of long in-context learning tasks
    with different difficulty levels with respect to the context length. Through our
    study, we have discovered that while LLMs show promising performance on inputs
    up to 20K tokens, their ability to process and understand longer sequences significantly
    decreases. Our exploratory experiments further highlight the impact of the distribution
    of examples within prompts on model performance. We hope LongICLBench and our
    findings contribute to the ongoing efforts to enhance LLMs’ understanding of long
    contexts.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的研究探讨了大型语言模型在长上下文学习任务中的能力，特别是在极端标签分类场景中的表现。我们整理了一个名为 LongICLBench 的数据集，该数据集包含了不同难度级别的长上下文学习任务。通过我们的研究，我们发现尽管
    LLM 对于最多 20K tokens 的输入表现出有希望的性能，但其处理和理解更长序列的能力显著下降。我们的探索性实验进一步突出了示例在提示中的分布对模型性能的影响。我们希望
    LongICLBench 和我们的发现能为提升 LLM 对长上下文的理解做出贡献。
- en: References
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人（2023） Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: 'AI et al. (2024) 01\. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,
    Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open
    foundation models by 01.ai, 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AI 等人（2024） 01\. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang,
    Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu,
    Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao
    Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu,
    Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, 和 Zonghong Dai。Yi: 由 01.ai 发布的开源基础模型，2024。'
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai
    Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized
    evaluation for long context language models, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An 等人（2023） Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li,
    Jun Zhang, Lingpeng Kong, 和 Xipeng Qiu。L-eval: 为长上下文语言模型制定标准化评估，2023。'
- en: Anil et al. (2022) Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz,
    Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer,
    and Behnam Neyshabur. Exploring length generalization in large language models.
    In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), *Advances
    in Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=zSkYVeX7bC4](https://openreview.net/forum?id=zSkYVeX7bC4).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人（2022） Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz,
    Vedant Misra, Vinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer,
    和 Behnam Neyshabur。探索大型语言模型中的长度泛化。收录于 Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
    和 Kyunghyun Cho（编），*神经信息处理系统进展*，2022。网址 [https://openreview.net/forum?id=zSkYVeX7bC4](https://openreview.net/forum?id=zSkYVeX7bC4)。
- en: Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. Qwen technical report, 2023a.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2023a） Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu。Qwen 技术报告，2023a。
- en: 'Bai et al. (2023b) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai
    Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong,
    Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long
    context understanding, 2023b.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等 (2023b) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang
    和 Juanzi Li. Longbench: 一个双语、多任务的长上下文理解基准，2023b。'
- en: Bhatia et al. (2015) Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma,
    and Prateek Jain. Sparse local embeddings for extreme multi-label classification.
    In *Neural Information Processing Systems*, 2015. URL [https://api.semanticscholar.org/CorpusID:11419932](https://api.semanticscholar.org/CorpusID:11419932).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatia 等 (2015) Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma 和 Prateek
    Jain. 用于极端多标签分类的稀疏局部嵌入。在 *神经信息处理系统*，2015。网址 [https://api.semanticscholar.org/CorpusID:11419932](https://api.semanticscholar.org/CorpusID:11419932)。
- en: Cai et al. (2024) Zheng Cai, Maosong Cao, Haojiong Chen, …, Yu Qiao, and Dahua
    Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等 (2024) Zheng Cai, Maosong Cao, Haojiong Chen, …, Yu Qiao 和 Dahua Lin.
    Internlm2 技术报告。*arXiv 预印本 arXiv:2403.17297*，2024 年。
- en: 'Casanueva et al. (2020) Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew
    Henderson, and Ivan Vulić. Efficient intent detection with dual sentence encoders.
    In Tsung-Hsien Wen, Asli Celikyilmaz, Zhou Yu, Alexandros Papangelis, Mihail Eric,
    Anuj Kumar, Iñigo Casanueva, and Rushin Shah (eds.), *Proceedings of the 2nd Workshop
    on Natural Language Processing for Conversational AI*, pp.  38–45, Online, July
    2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.nlp4convai-1.5.
    URL [https://aclanthology.org/2020.nlp4convai-1.5](https://aclanthology.org/2020.nlp4convai-1.5).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Casanueva 等 (2020) Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson
    和 Ivan Vulić. 利用双重句子编码器进行高效的意图检测。在 Tsung-Hsien Wen、Asli Celikyilmaz、Zhou Yu、Alexandros
    Papangelis、Mihail Eric、Anuj Kumar、Iñigo Casanueva 和 Rushin Shah (编辑)，*第二届会话 AI
    自然语言处理研讨会论文集*，第 38–45 页，在线，2020 年 7 月。计算语言学协会。doi: 10.18653/v1/2020.nlp4convai-1.5。网址
    [https://aclanthology.org/2020.nlp4convai-1.5](https://aclanthology.org/2020.nlp4convai-1.5)。'
- en: Chen et al. (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. Extending context window of large language models via positional interpolation.
    *ArXiv*, abs/2306.15595, 2023a. URL [https://api.semanticscholar.org/CorpusID:259262376](https://api.semanticscholar.org/CorpusID:259262376).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023a) Shouyuan Chen, Sherman Wong, Liangjian Chen 和 Yuandong Tian.
    通过位置插值扩展大型语言模型的上下文窗口。*ArXiv*，abs/2306.15595，2023a。网址 [https://api.semanticscholar.org/CorpusID:259262376](https://api.semanticscholar.org/CorpusID:259262376)。
- en: 'Chen et al. (2023b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2023b.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2023b) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han 和 Jiaya Jia. Longlora: 高效微调长上下文大型语言模型。在 *第十二届国际学习表征会议*，2023b。'
- en: 'Dasigi et al. (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A
    Smith, and Matt Gardner. A dataset of information-seeking questions and answers
    anchored in research papers. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pp.  4599–4610, 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasigi 等 (2021) Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith
    和 Matt Gardner. 一个基于研究论文的信息寻求问题和答案的数据集。在 *2021 年北美计算语言学协会：人类语言技术会议论文集*，第 4599–4610
    页，2021 年。
- en: 'Demszky et al. (2020) Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko,
    Alan Cowen, Gaurav Nemade, and Sujith Ravi. GoEmotions: A dataset of fine-grained
    emotions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.),
    *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pp.  4040–4054, Online, July 2020\. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.acl-main.372. URL [https://aclanthology.org/2020.acl-main.372](https://aclanthology.org/2020.acl-main.372).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Demszky 等 (2020) Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan
    Cowen, Gaurav Nemade 和 Sujith Ravi. GoEmotions: 一个细粒度情感数据集。在 Dan Jurafsky、Joyce
    Chai、Natalie Schluter 和 Joel Tetreault (编辑)，*第 58 届计算语言学协会年度会议论文集*，第 4040–4054
    页，在线，2020 年 7 月。计算语言学协会。doi: 10.18653/v1/2020.acl-main.372。网址 [https://aclanthology.org/2020.acl-main.372](https://aclanthology.org/2020.acl-main.372)。'
- en: 'Ding et al. (2021) Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han,
    Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-NERD: A few-shot named entity
    recognition dataset. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli
    (eds.), *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pp.  3198–3213, Online, August 2021\. Association for
    Computational Linguistics. doi: 10.18653/v1/2021.acl-long.248. URL [https://aclanthology.org/2021.acl-long.248](https://aclanthology.org/2021.acl-long.248).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2021) Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han,
    Pengjun Xie, Haitao Zheng, 和 Zhiyuan Liu. Few-NERD: 一个小样本命名实体识别数据集。在 Chengqing
    Zong, Fei Xia, Wenjie Li, 和 Roberto Navigli（编），*第59届计算语言学协会年会暨第11届国际联合自然语言处理会议论文集（第1卷：长篇论文）*，第3198-3213页，在线，2021年8月。计算语言学协会。doi:
    10.18653/v1/2021.acl-long.248。网址 [https://aclanthology.org/2021.acl-long.248](https://aclanthology.org/2021.acl-long.248)。'
- en: 'Ding et al. (2024) Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan
    Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context
    window beyond 2 million tokens. *arXiv preprint arXiv:2402.13753*, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. (2024) Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan
    Xu, Ning Shang, Jiahang Xu, Fan Yang, 和 Mao Yang. Longrope: 将LLM上下文窗口扩展到超过200万标记。*arXiv
    预印本 arXiv:2402.13753*，2024年。'
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning,
    2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, 和 Zhifang Sui. 关于上下文学习的综述，2023年。
- en: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context.
    *arXiv preprint arXiv:2402.10171*, 2024.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
    Yoon Kim, 和 Hao Peng. 扩展语言模型到128k上下文的数据工程。*arXiv 预印本 arXiv:2402.10171*，2024年。
- en: 'Gu & Dao (2023) Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling
    with selective state spaces. *arXiv preprint arXiv:2312.00752*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu & Dao (2023) Albert Gu 和 Tri Dao. Mamba: 线性时间序列建模与选择性状态空间。*arXiv 预印本 arXiv:2312.00752*，2023年。'
- en: 'Hao et al. (2022) Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and
    Furu Wei. Structured prompting: Scaling in-context learning to 1, 000 examples.
    *ArXiv*, abs/2212.06713, 2022. URL [https://api.semanticscholar.org/CorpusID:254591686](https://api.semanticscholar.org/CorpusID:254591686).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. (2022) Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, 和 Furu
    Wei. 结构化提示：将上下文学习扩展到1,000个示例。*ArXiv*，abs/2212.06713，2022年。网址 [https://api.semanticscholar.org/CorpusID:254591686](https://api.semanticscholar.org/CorpusID:254591686)。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b, 2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. Mistral 7b，2023年。
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more
    tokens: the impact of input length on the reasoning performance of large language
    models, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy et al. (2024) Mosh Levy, Alon Jacoby, 和 Yoav Goldberg. 同一任务，更多标记：输入长度对大型语言模型推理性能的影响，2024年。
- en: Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can context length
    of open-source LLMs truly promise? In *NeurIPS 2023 Workshop on Instruction Tuning
    and Instruction Following*, 2023a. URL [https://openreview.net/forum?id=LywifFNXV5](https://openreview.net/forum?id=LywifFNXV5).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph Gonzalez, Ion Stoica, Xuezhe Ma, 和 Hao Zhang. 开源LLM的上下文长度究竟能承诺多长？在 *NeurIPS
    2023 指令调优与指令跟随研讨会* 上，2023a。网址 [https://openreview.net/forum?id=LywifFNXV5](https://openreview.net/forum?id=LywifFNXV5)。
- en: 'Li et al. (2023b) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. Loogle:
    Can long-context language models understand long contexts?, 2023b.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023b) Jiaqi Li, Mengmeng Wang, Zilong Zheng, 和 Muhan Zhang. Loogle:
    长上下文语言模型能否理解长上下文？2023b。'
- en: Li et al. (2023c) Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang,
    Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples,
    2023c.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023c) Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang,
    Zhiyong Wu, 和 Lingpeng Kong. 通过许多示例进行的上下文学习，2023c。
- en: 'Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. What makes good in-context examples for GPT-3? In Eneko
    Agirre, Marianna Apidianaki, and Ivan Vulić (eds.), *Proceedings of Deep Learning
    Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration
    for Deep Learning Architectures*, pp.  100–114, Dublin, Ireland and Online, May
    2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.deelio-1.10.
    URL [https://aclanthology.org/2022.deelio-1.10](https://aclanthology.org/2022.deelio-1.10).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2022）Jiachang Liu、Dinghan Shen、Yizhe Zhang、Bill Dolan、Lawrence Carin
    和 Weizhu Chen。什么样的上下文示例适合 GPT-3？在 Eneko Agirre、Marianna Apidianaki 和 Ivan Vulić（编辑），*深度学习内部探讨（DeeLIO
    2022）：第三届深度学习架构知识提取与整合研讨会论文集*，第 100–114 页，爱尔兰都柏林及在线，2022 年 5 月。计算语言学协会。doi: 10.18653/v1/2022.deelio-1.10。网址
    [https://aclanthology.org/2022.deelio-1.10](https://aclanthology.org/2022.deelio-1.10)。'
- en: 'Liu et al. (2024) Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang,
    Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, et al. E^ 2-llm: Efficient
    and extreme length extension of large language models. *arXiv preprint arXiv:2401.06951*,
    2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2024）Jiaheng Liu、Zhiqi Bai、Yuanxing Zhang、Chenchen Zhang、Yu Zhang、Ge Zhang、Jiakai
    Wang、Haoran Que、Yukang Chen、Wenbo Su 等。E^2-llm：大语言模型的高效极限长度扩展。*arXiv 预印本 arXiv:2401.06951*，2024
    年。
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *Transactions of the Association for Computational Linguistics*,
    12:157–173, 2023. URL [https://api.semanticscholar.org/CorpusID:259360665](https://api.semanticscholar.org/CorpusID:259360665).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Nelson F. Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio
    Petroni 和 Percy Liang。在中间迷失：语言模型如何使用长上下文。*计算语言学协会会刊*，12:157–173，2023 年。网址 [https://api.semanticscholar.org/CorpusID:259360665](https://api.semanticscholar.org/CorpusID:259360665)。
- en: Milios et al. (2023) Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. In-context
    learning for text classification with many labels, 2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milios 等（2023）Aristides Milios、Siva Reddy 和 Dzmitry Bahdanau。针对多标签文本分类的上下文学习，2023
    年。
- en: 'Mohtashami & Jaggi (2023) Amirkeivan Mohtashami and Martin Jaggi. Landmark
    attention: Random-access infinite context length for transformers. In *Workshop
    on Efficient Systems for Foundation Models@ ICML2023*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohtashami & Jaggi（2023）Amirkeivan Mohtashami 和 Martin Jaggi。地标注意力：变换器的随机访问无限上下文长度。在
    *ICML2023 基础模型高效系统研讨会*，2023 年。
- en: 'Nallapati et al. (2017) Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner:
    A recurrent neural network based sequence model for extractive summarization of
    documents. In *Proceedings of the AAAI conference on artificial intelligence*,
    volume 31, 2017.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nallapati 等（2017）Ramesh Nallapati、Feifei Zhai 和 Bowen Zhou。Summarunner: 一种基于递归神经网络的序列模型，用于文档的抽取式总结。在
    *人工智能协会会议论文集*，第 31 卷，2017 年。'
- en: Orvieto et al. (2023) Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando,
    Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks
    for long sequences. *ArXiv*, abs/2303.06349, 2023. URL [https://api.semanticscholar.org/CorpusID:257496654](https://api.semanticscholar.org/CorpusID:257496654).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Orvieto 等（2023）Antonio Orvieto、Samuel L. Smith、Albert Gu、Anushan Fernando、Caglar
    Gulcehre、Razvan Pascanu 和 Soham De。复活递归神经网络以处理长序列。*ArXiv*，abs/2303.06349，2023
    年。网址 [https://api.semanticscholar.org/CorpusID:257496654](https://api.semanticscholar.org/CorpusID:257496654)。
- en: 'Peng et al. (2023a) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel
    Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski,
    et al. Rwkv: Reinventing rnns for the transformer era. In *Findings of the Association
    for Computational Linguistics: EMNLP 2023*, pp.  14048–14077, 2023a.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等（2023a）Bo Peng、Eric Alcaide、Quentin Anthony、Alon Albalak、Samuel Arcadinho、Stella
    Biderman、Huanqi Cao、Xin Cheng、Michael Chung、Leon Derczynski 等。Rwkv: 为变换器时代重新发明
    RNN。收录于 *计算语言学协会发现：EMNLP 2023*，第 14048–14077 页，2023a。'
- en: 'Peng et al. (2023b) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    Yarn: Efficient context window extension of large language models, 2023b.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等（2023b）Bowen Peng、Jeffrey Quesnelle、Honglu Fan 和 Enrico Shippole。Yarn:
    大语言模型的高效上下文窗口扩展，2023b。'
- en: Peng et al. (2023c) Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia
    Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, and Juanzi Li. When does
    in-context learning fall short and why? a study on specification-heavy tasks,
    2023c.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023c）Hao Peng、Xiaozhi Wang、Jianhui Chen、Weikai Li、Yunjia Qi、Zimu Wang、Zhili
    Wu、Kaisheng Zeng、Bin Xu、Lei Hou 和 Juanzi Li。上下文学习何时不足及其原因？关于规范性任务的研究，2023c。
- en: 'Press et al. (2022) Ofir Press, Noah Smith, and Mike Lewis. Train short, test
    long: Attention with linear biases enables input length extrapolation. In *International
    Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2022) Ofir Press, Noah Smith, 和 Mike Lewis. 短期训练，长期测试：具有线性偏差的注意力机制实现输入长度的外推。在*国际学习表征会议*，2022年。URL
    [https://openreview.net/forum?id=R8sQPpGCv0](https://openreview.net/forum?id=R8sQPpGCv0)。
- en: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    Parallel context windows for large language models. In Anna Rogers, Jordan Boyd-Graber,
    and Naoaki Okazaki (eds.), *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  6383–6402, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.352.
    URL [https://aclanthology.org/2023.acl-long.352](https://aclanthology.org/2023.acl-long.352).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, 和 Yoav Shoham.
    大型语言模型的并行上下文窗口。在 Anna Rogers, Jordan Boyd-Graber, 和 Naoaki Okazaki (编)，《*第61届计算语言学协会年会论文集（第1卷：长篇论文）*》，第6383–6402页，加拿大多伦多，2023年7月。计算语言学协会。doi:
    10.18653/v1/2023.acl-long.352。URL [https://aclanthology.org/2023.acl-long.352](https://aclanthology.org/2023.acl-long.352)。'
- en: 'Rozière et al. (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
    Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,
    Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas
    Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière et al. (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
    Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,
    Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas
    Scialom, 和 Gabriel Synnaeve. Code llama：开源代码基础模型，2024年。
- en: 'Sileo et al. (2019) Damien Sileo, Tim Van De Cruys, Camille Pradel, and Philippe
    Muller. Mining discourse markers for unsupervised sentence representation learning.
    In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 
    3477–3486, Minneapolis, Minnesota, June 2019\. Association for Computational Linguistics.
    doi: 10.18653/v1/N19-1351. URL [https://aclanthology.org/N19-1351](https://aclanthology.org/N19-1351).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sileo et al. (2019) Damien Sileo, Tim Van De Cruys, Camille Pradel, 和 Philippe
    Muller. 挖掘语篇标记以进行无监督句子表示学习。在 Jill Burstein, Christy Doran, 和 Thamar Solorio (编)，《*2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*》，第3477–3486页，美国明尼阿波利斯，2019年6月。计算语言学协会。doi:
    10.18653/v1/N19-1351。URL [https://aclanthology.org/N19-1351](https://aclanthology.org/N19-1351)。'
- en: 'Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.
    Roformer: Enhanced transformer with rotary position embedding. *ArXiv*, abs/2104.09864,
    2021. URL [https://api.semanticscholar.org/CorpusID:233307138](https://api.semanticscholar.org/CorpusID:233307138).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2021) Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, 和 Yunfeng Liu. Roformer：带有旋转位置嵌入的增强型变换器。*ArXiv*，abs/2104.09864，2021年。URL
    [https://api.semanticscholar.org/CorpusID:233307138](https://api.semanticscholar.org/CorpusID:233307138)。
- en: 'Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063, 2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, 和
    Yunfeng Liu. Roformer：带有旋转位置嵌入的增强型变换器。*Neurocomputing*，568:127063，2024年。
- en: 'Tay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara
    Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
    Long range arena : A benchmark for efficient transformers. In *International Conference
    on Learning Representations*, 2021. URL [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay et al. (2021) Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara
    Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, 和 Donald Metzler.
    长程竞技场：高效变换器的基准测试。在*国际学习表征会议*，2021年。URL [https://openreview.net/forum?id=qVyeW-grC2k](https://openreview.net/forum?id=qVyeW-grC2k)。
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等（2023）Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth 等。Gemini：一系列高能力的多模态模型。*arXiv
    预印本 arXiv:2312.11805*，2023。
- en: 'Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology.
    *arXiv preprint arXiv:2403.08295*, 2024.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等（2024）Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya
    Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,
    Juliette Love 等。Gemma：基于 Gemini 研究与技术的开放模型。*arXiv 预印本 arXiv:2403.08295*，2024。
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Miłoś. Focused transformer: Contrastive
    training for context scaling, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski 等（2023）Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai
    Wu, Henryk Michalewski 和 Piotr Miłoś。集中变换器：用于上下文扩展的对比训练，2023。
- en: 'Vulić et al. (2021) Ivan Vulić, Pei-Hao Su, Samuel Coope, Daniela Gerz, Paweł
    Budzianowski, Iñigo Casanueva, Nikola Mrkšić, and Tsung-Hsien Wen. ConvFiT: Conversational
    fine-tuning of pretrained language models. In Marie-Francine Moens, Xuanjing Huang,
    Lucia Specia, and Scott Wen-tau Yih (eds.), *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pp.  1151–1168, Online and
    Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.emnlp-main.88. URL [https://aclanthology.org/2021.emnlp-main.88](https://aclanthology.org/2021.emnlp-main.88).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vulić 等（2021）Ivan Vulić, Pei-Hao Su, Samuel Coope, Daniela Gerz, Paweł Budzianowski,
    Iñigo Casanueva, Nikola Mrkšić 和 Tsung-Hsien Wen。ConvFiT：预训练语言模型的对话微调。收录于 Marie-Francine
    Moens, Xuanjing Huang, Lucia Specia 和 Scott Wen-tau Yih（编），*2021年自然语言处理实证方法会议论文集*，页
    1151–1168，在线及多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。doi: 10.18653/v1/2021.emnlp-main.88。URL
    [https://aclanthology.org/2021.emnlp-main.88](https://aclanthology.org/2021.emnlp-main.88)。'
- en: Wang et al. (2023) Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan,
    Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory.
    In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023.
    URL [https://openreview.net/forum?id=BryMFPQ4L6](https://openreview.net/forum?id=BryMFPQ4L6).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng
    Gao 和 Furu Wei。通过长期记忆增强语言模型。发表于*第37届神经信息处理系统会议*，2023。URL [https://openreview.net/forum?id=BryMFPQ4L6](https://openreview.net/forum?id=BryMFPQ4L6)。
- en: 'Wu et al. (2023) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    Self-adaptive in-context learning: An information compression perspective for
    in-context example selection and ordering, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023）Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye 和 Lingpeng Kong。自适应上下文学习：基于信息压缩的上下文示例选择与排序视角，2023。
- en: Xiao et al. (2024) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. Efficient streaming language models with attention sinks. In *The
    Twelfth International Conference on Learning Representations*, 2024. URL [https://openreview.net/forum?id=NG7sS51zVF](https://openreview.net/forum?id=NG7sS51zVF).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2024）Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han 和 Mike Lewis。具有注意力汇聚的高效流式语言模型。发表于*第十二届国际学习表征会议*，2024。URL
    [https://openreview.net/forum?id=NG7sS51zVF](https://openreview.net/forum?id=NG7sS51zVF)。
- en: Xiong et al. (2023) Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal
    Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas
    Oguz, et al. Effective long-context scaling of foundation models. *arXiv preprint
    arXiv:2309.16039*, 2023.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等（2023）Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava,
    Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz
    等。有效的长上下文扩展基础模型。*arXiv 预印本 arXiv:2309.16039*，2023。
- en: 'Yu et al. (2020) Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. Dialogue-based
    relation extraction. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault
    (eds.), *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pp.  4927–4940, Online, July 2020\. Association for Computational
    Linguistics. doi: 10.18653/v1/2020.acl-main.444. URL [https://aclanthology.org/2020.acl-main.444](https://aclanthology.org/2020.acl-main.444).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2020）Dian Yu, Kai Sun, Claire Cardie 和 Dong Yu。基于对话的关系抽取。收录于 Dan Jurafsky,
    Joyce Chai, Natalie Schluter 和 Joel Tetreault（编），*第58届计算语言学协会年会论文集*，页 4927–4940，在线，2020年7月。计算语言学协会。doi:
    10.18653/v1/2020.acl-main.444。URL [https://aclanthology.org/2020.acl-main.444](https://aclanthology.org/2020.acl-main.444)。'
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open
    bilingual pre-trained model. In *The Eleventh International Conference on Learning
    Representations*, 2022.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, 等。Glm-130b: 一个开放的双语预训练模型。在
    *第十一届国际学习表征会议*，2022年。'
- en: 'Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong
    Sun. $\infty$bench: Extending long context evaluation beyond 100k tokens, 2024.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2024) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, 和 Maosong
    Sun。$\infty$bench: 扩展超过 100k 标记的长上下文评估，2024年。'
- en: Zhang et al. (2017) Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and
    Christopher D. Manning. Position-aware attention and supervised data improve slot
    filling. In *Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing (EMNLP 2017)*, pp.  35–45, 2017. URL [https://nlp.stanford.edu/pubs/zhang2017tacred.pdf](https://nlp.stanford.edu/pubs/zhang2017tacred.pdf).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2017) Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, 和 Christopher
    D. Manning。位置感知注意力和监督数据改善了槽填充。在 *2017年自然语言处理实证方法会议论文集（EMNLP 2017）*，第 35–45 页，2017年。网址
    [https://nlp.stanford.edu/pubs/zhang2017tacred.pdf](https://nlp.stanford.edu/pubs/zhang2017tacred.pdf)。
- en: 'Zhu et al. (2024) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu
    Wei, and Sujian Li. PoSE: Efficient context window extension of LLMs via positional
    skip-wise training. In *The Twelfth International Conference on Learning Representations*,
    2024. URL [https://openreview.net/forum?id=3Z1gxuAQrA](https://openreview.net/forum?id=3Z1gxuAQrA).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2024) Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu
    Wei, 和 Sujian Li。PoSE: 通过位置跳跃训练高效扩展 LLM 的上下文窗口。在 *第十二届国际学习表征会议*，2024年。网址 [https://openreview.net/forum?id=3Z1gxuAQrA](https://openreview.net/forum?id=3Z1gxuAQrA)。'
- en: Appendix A Appendix
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Additional Datasets
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 额外数据集
- en: 'We list a few additional datasets as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列出了一些额外的数据集如下：
- en: GoEmotions  (Demszky et al., [2020](#bib.bib13)) is the largest manually annotated
    dataset of 58k English comments from Reddit, which is labeled into 27 emotion
    categories or Neutral. There are 27 types of emotion types and drop the rare ones
    with few examples. Each selected example contains 28 tokens on average.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: GoEmotions  (Demszky et al., [2020](#bib.bib13)) 是来自 Reddit 的 58,000 条英语评论中最大的一组人工标注数据集，这些评论被标记为
    27 个情感类别或中立。共有 27 种情感类型，并去除了那些样本稀少的情感。每个选定的例子平均包含 28 个标记。
- en: Few-NERD  (Ding et al., [2021](#bib.bib14)) is a large-scale human-annotated
    name entity recognition dataset with a hierarchy of 8 coarse-grained and 66 fine-grained
    entity types. Each of the instances is a paragraph with approximately 61 tokens
    on average and contains one or multiple entity names as the ground truth answer.
    There are 66 types of entities in the collection.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Few-NERD  (Ding et al., [2021](#bib.bib14)) 是一个大规模人工标注的命名实体识别数据集，具有 8 个粗粒度和
    66 个细粒度实体类型的层次结构。每个实例是一个段落，平均包含约 61 个标记，并包含一个或多个实体名称作为真实答案。集合中共有 66 种实体类型。
- en: The performance for the two tasks is demonstrated in Table [8](#A1.T8 "Table
    8 ‣ A.1 Additional Datasets ‣ Appendix A Appendix ‣ Long-context LLMs Struggle
    with Long In-context Learning") and Table [9](#A1.T9 "Table 9 ‣ A.1 Additional
    Datasets ‣ Appendix A Appendix ‣ Long-context LLMs Struggle with Long In-context
    Learning").
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 两项任务的性能展示在表 [8](#A1.T8 "表 8 ‣ A.1 额外数据集 ‣ 附录 A 附录 ‣ 长上下文 LLM 在长上下文学习中的困难") 和表
    [9](#A1.T9 "表 9 ‣ A.1 额外数据集 ‣ 附录 A 附录 ‣ 长上下文 LLM 在长上下文学习中的困难") 中。
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
- en: '| Context Tokens |  |  | 0.8K | 1.6K | 2.4K | 3.2K | 4K |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Context Tokens |  |  | 0.8K | 1.6K | 2.4K | 3.2K | 4K |'
- en: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8K | 0 | 0 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0 | 0 | 0.2 | 0.2 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32K | 0 | 0 | 0 | 0.2 | 0.2 |'
- en: '| ChatGLM3-6B-32K | 6B | 32K | 22.0 | 17.0 | 15.0 | 12.6 | 10.6 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32K | 22.0 | 17.0 | 15.0 | 12.6 | 10.6 |'
- en: '| Qwen-1.5-7B-base | 7B | 32K | 14.8 | 18.2 | 18.6 | 19.0 | 14.2 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32K | 14.8 | 18.2 | 18.6 | 19.0 | 14.2 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 2.6 | 11.4 | 7.4 | 11.6 | 12.4 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 2.6 | 11.4 | 7.4 | 11.6 | 12.4 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200K | 0 | 0 | 0.8 | 4.0 | 4.0 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200K | 0 | 0 | 0.8 | 4.0 | 4.0 |'
- en: '| InternLM2-7B-base | 7B | 200K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 0 | 0 | 0 | 0.2 | 0.4 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 0 | 0 | 0 | 0.2 | 0.4 |'
- en: '| RWKV-5-World | 7B | 4K | 8.8 | 7.4 | 4.6 | 5.2 | 4.0 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 4K | 8.8 | 7.4 | 4.6 | 5.2 | 4.0 |'
- en: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Gemini-1.0-Pro | N/A | 32K | 20.3 | 21.4 | 22.4 | 24.4 | 24.0 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.0-Pro | N/A | 32K | 20.3 | 21.4 | 22.4 | 24.4 | 24.0 |'
- en: '| GPT4-turbo | N/A | 128K | 36.5 | 34.4 | 35.0 | 33.3 | 32.0 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128K | 36.5 | 34.4 | 35.0 | 33.3 | 32.0 |'
- en: '| SoTA (BERT) | N/A | - | 58.9 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SoTA (BERT) | N/A | - | 58.9 |'
- en: 'Table 8: GoEmotion Result.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 8: GoEmotion Result.'
- en: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Model | Param | Support | 1R | 2R | 3R | 4R | 5R |'
- en: '| Context Tokens |  |  | 5K | 9K | 14K | 19K | 24K |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Context Tokens |  |  | 5K | 9K | 14K | 19K | 24K |'
- en: '| Gemma-7B-base | 7B | 8k | 44.0 | 44.2 | 0 | 0 | 0 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-7B-base | 7B | 8k | 44.0 | 44.2 | 0 | 0 | 0 |'
- en: '| LLaMA-2-7B-32K | 7B | 32k | 36.9 | 40.8 | 41.1 | 41.6 | 41.3 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-32K | 7B | 32k | 36.9 | 40.8 | 41.1 | 41.6 | 41.3 |'
- en: '| ChatGLM3-6B-32K | 6B | 32k | 24.1 | 9.3 | 23.6 | 10.4 | 1.1 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM3-6B-32K | 6B | 32k | 24.1 | 9.3 | 23.6 | 10.4 | 1.1 |'
- en: '| Qwen-1.5-7B-base | 7B | 32k | 40.0 | 46.4 | 47.6 | 47.3 | 47.8 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-7B-base | 7B | 32k | 40.0 | 46.4 | 47.6 | 47.3 | 47.8 |'
- en: '| Mistral-7B-v0.2-base | 7B | 32K | 42.2 | 47.4 | 48.9 | 50.0 | 50.0 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B-v0.2-base | 7B | 32K | 42.2 | 47.4 | 48.9 | 50.0 | 50.0 |'
- en: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B-LongLora | 7B | 100K | 0 | 0 | 0 | 0 | 0 |'
- en: '| Yi-6B-200K | 6B | 200k | 34.3 | 40.2 | 44.8 | 42.3 | 43.2 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Yi-6B-200K | 6B | 200k | 34.3 | 40.2 | 44.8 | 42.3 | 43.2 |'
- en: '| InternLM2-7B-base | 7B | 200k | 43.6 | 46.2 | 46.5 | 47.8 | 48.3 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2-7B-base | 7B | 200k | 43.6 | 46.2 | 46.5 | 47.8 | 48.3 |'
- en: '| Long-LLaMA-code-7B | 7B | 256K | 22.3 | 25.5 | 26.5 | 29.4 | 27.0 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Long-LLaMA-code-7B | 7B | 256K | 22.3 | 25.5 | 26.5 | 29.4 | 27.0 |'
- en: '| RWKV-5-World | 7B | 1k | 13.9 | 0 | 0 | 0.7 | 9.9 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| RWKV-5-World | 7B | 1k | 13.9 | 0 | 0 | 0.7 | 9.9 |'
- en: '| Mamba-2.8B | 2.8B | 2k | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Mamba-2.8B | 2.8B | 2k | 0 | 0 | 0 | 0 | 0 |'
- en: '| Gemini-1.0-Pro | N/A | 32k | 36.8 | 26.1 | 28.5 | 27.4 | 28.4 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.0-Pro | N/A | 32k | 36.8 | 26.1 | 28.5 | 27.4 | 28.4 |'
- en: '| GPT4-turbo | N/A | 128k | 53.4 | 55.3 | 56.2 | 55.6 | 56.8 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | N/A | 128k | 53.4 | 55.3 | 56.2 | 55.6 | 56.8 |'
- en: '| SoTA (PL-Marker) | N/A | - | 70.9 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| SoTA (PL-Marker) | N/A | - | 70.9 |'
- en: 'Table 9: Few-NERD Result.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 9: Few-NERD Result.'
- en: A.2 Prompting Template
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 Prompting Template
- en: '| Dataset | Prompt |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Dataset | Prompt |'
- en: '| --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| GoEmotion | Given a comment, please predict the emotion category of this
    comment. The prediction answer must come from the demonstration examples with
    the exact format. The examples are as follows: {comment: ”…comment…”'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '| GoEmotion | 给定一条评论，请预测该评论的情感类别。预测答案必须来自示例中的确切格式。示例如下：{comment: ”…comment…”'
- en: 'emotion category: ”…emotion…”'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 'emotion category: ”…emotion…”'
- en: '} $\times$ repeat n times |'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '} $\times$ repeat n times |'
- en: '| BANKING77 | Given a customer service query, please predict the intent of
    the query. The predicted answer must come from the demonstration examples with
    the exact format. The examples are as follows: {service query: ”…service…”'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '| BANKING77 | 给定客户服务查询，请预测查询的意图。预测的答案必须来自于示例中的确切格式。示例如下：{service query: ”…service…”'
- en: 'intent category: ”…intent…”'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 'intent category: ”…intent…”'
- en: '} $\times$ repeat n times |'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '} $\times$ repeat n times |'
- en: '| TacRED | Given a sentence and a pair of subject and object entities within
    the sentence, please predict the relation between the given entities. The examples
    are as follows: {sentence: ”…sentence…'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '| TacRED | 给定一句话和句子中的一对主语和宾语实体，请预测给定实体之间的关系。示例如下：{sentence: ”…sentence…'
- en: the subject is ”…subject…”
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: the subject is ”…subject…”
- en: the object is ”…object…”
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: the object is ”…object…”
- en: 'the relation between the two entities is: ”…relation…”'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 'the relation between the two entities is: ”…relation…”'
- en: '} $\times$ repeat n times |'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '} $\times$ repeat n times |'
- en: '| Few-NERD | Given the sentence, please find the name entities in the sentence
    and their corresponding entity types in the strict format of the given examples
    as following (Entity: EntityType): {”…entity…”: ”…entity type…”'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '| Few-NERD | 给定句子，请找到句子中的名称实体及其对应的实体类型，格式严格按照给定示例 (Entity: EntityType)：{”…entity…”:
    ”…entity type…”'
- en: '} $\times$ repeat n times |'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '} $\times$ repeat n times |'
- en: '| DialogRE | Given the dialogue, please find the name pair entities in the
    dialogue and their corresponding relation types in the strict format of given
    examples as following (note that the number of entities has to strictly have the
    same value as the number of respective relation): {Dialogue:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '| DialogRE | 给定对话，请找到对话中的名称对实体及其对应的关系类型，格式严格按照给定示例 (注意实体的数量必须与相应关系的数量完全相同)：{Dialogue:'
- en: ”…dialogue…”
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ”…dialogue…”
- en: The list of entity pairs are ”…(subject1, object1), (subject2, object2), etc…
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 实体对列表为 ”…(subject1, object1), (subject2, object2), 等…
- en: 'The ”…number of pairs…” respective relations between each entity pair are:
    ”…relation, relation2, etc…'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: “每对实体之间的‘…数量对…’关系为：‘…关系，关系2，等等…’。
- en: '} $\times$ repeat n times |'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '} $\times$ 重复 n 次 |'
- en: '| Discovery | Given two sentence1 and sentence2, please predict the conjunction
    word between the two sentences. The predicted answer must come from the demonstration
    examples with the exact format. The examples are as follows: {”…sentence1…” (
    ) ”…sentence2…”'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '| 发现 | 给定两个句子 sentence1 和 sentence2，请预测这两个句子之间的连接词。预测的答案必须来自演示示例，且格式完全一致。示例如下：{”…sentence1…”
    ( ) ”…sentence2…”'
- en: the conjunction word in ( ) is ”…conjunction…”
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的连接词是‘…连接词…’。
- en: '} $\times$ repeat n times |'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '} $\times$ 重复 n 次 |'
- en: 'Table 10: The data prompt format of each dataset. Each dataset has a unique
    prompt format to effectively utilize the context and format of its respective
    data to get the best output response.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：每个数据集的数据提示格式。每个数据集都有一个独特的提示格式，以有效利用其各自数据的上下文和格式，从而获得最佳的输出响应。
- en: The prompting template for each of the datasets is presented at Table [10](#A1.T10
    "Table 10 ‣ A.2 Prompting Template ‣ Appendix A Appendix ‣ Long-context LLMs Struggle
    with Long In-context Learning")
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集的提示模板见表[10](#A1.T10 "Table 10 ‣ A.2 Prompting Template ‣ Appendix A Appendix
    ‣ Long-context LLMs Struggle with Long In-context Learning")。
