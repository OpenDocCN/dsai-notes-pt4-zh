- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:03:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The CAP Principle for LLM Serving: A Survey of Long-Context Large Language
    Model Serving'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM服务中的CAP原则：长上下文大型语言模型服务的综述
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.11299](https://ar5iv.labs.arxiv.org/html/2405.11299)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.11299](https://ar5iv.labs.arxiv.org/html/2405.11299)
- en: Pai Zeng
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 佩曾
- en: Huawei Cloud & SJTU
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 华为云 & SJTU
- en: Zhenyu Ning
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 宁振宇
- en: SJTU
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: SJTU
- en: Jieru Zhao
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 赵洁如
- en: SJTU
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SJTU
- en: Weihao Cui
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 崔伟浩
- en: SJTU
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SJTU
- en: Mengwei Xu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 孟伟徐
- en: BUPT
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 北京邮电大学（BUPT）
- en: Liwei Guo
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 李伟国
- en: UESTC
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 电子科技大学（UESTC）
- en: Xusheng Chen
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许胜陈
- en: Huawei Cloud
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 华为云
- en: Yizhou Shan
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 喻舟善
- en: Huawei Cloud
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 华为云
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We survey the large language model (LLM) serving area to understand the intricate
    dynamics between cost-efficiency and accuracy, which is magnified by the growing
    need for longer contextual understanding when deploying models at a massive scale.
    Our findings reveal that works in this space optimize along three distinct but
    conflicting goals: improving serving context length (C), improving serving accuracy
    (A), and improving serving performance (P). Drawing inspiration from the CAP theorem
    in databases, we propose a CAP principle for LLM serving, which suggests that
    any optimization can improve at most two of these three goals simultaneously.
    Our survey categorizes existing works within this framework. We find the definition
    and continuity of user-perceived measurement metrics are crucial in determining
    whether a goal has been met, akin to prior CAP databases in the wild. We recognize
    the CAP principle for LLM serving as a guiding principle, rather than a formal
    theorem, to inform designers of the inherent and dynamic trade-offs in serving
    models. As serving accuracy and performance have been extensively studied, this
    survey focuses on works that extend serving context length and address the resulting
    challenges.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对大型语言模型（LLM）服务领域进行了调查，以了解在大规模部署模型时，成本效益和准确性之间的复杂动态，这种动态由于对更长上下文理解的日益需求而被放大。我们的发现表明，该领域的工作在三个不同但相互冲突的目标之间进行优化：提高服务上下文长度（C）、提高服务准确性（A）和提高服务性能（P）。我们借鉴了数据库中的CAP定理，提出了LLM服务的CAP原则，该原则表明，任何优化至多可以同时改善这三个目标中的两个。我们的调查在这一框架内对现有工作进行了分类。我们发现，用户感知的测量指标的定义和连续性在确定是否达成目标方面至关重要，类似于之前在实际应用中的CAP数据库。我们将LLM服务的CAP原则视为一种指导原则，而不是正式定理，以告知设计人员服务模型中的固有和动态权衡。由于服务准确性和性能已经得到了广泛研究，本综述重点关注扩展服务上下文长度并解决由此产生的挑战的工作。
- en: 1 Introduction
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Large language models (LLMs) and their underlying transformer architecture
    have revolutionized AI and have become the bedrock of many emerging applications.
    The ecosystem around LLM is on an upward spiral towards artificial general intelligence
    (AGI): the number of new LLMs and their applications skyrocketed, and as of 2024,
    LLM-based applications already outperform humans across many tasks such as image
    classification and visual reasoning [[1](#bib.bib1), [2](#bib.bib2)]. High-quality
    models are essential for any realization of AGI, but it’s equally important to
    deploy and serve models at a massive scale with a reasonably low cost without
    compromising their accuracy. The conflict between serving accuracy and serving
    performance (e.g., tokens per second.) is a hard one, prompting extensive research
    in this area [[3](#bib.bib3), [4](#bib.bib4)]. Generally, there is no one-size-fits-all
    solution in production settings. Optimizations to improve performance can lead
    to reduced accuracy and vice versa. For example, sparsity and quantization are
    two common techniques that trade accuracy for better performance.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）及其基础的变换器架构已经彻底改变了人工智能，并成为许多新兴应用的基石。围绕LLM的生态系统正在向人工通用智能（AGI）不断上升：新LLM及其应用的数量激增，截至2024年，基于LLM的应用已经在许多任务上超过了人类，如图像分类和视觉推理[[1](#bib.bib1),
    [2](#bib.bib2)]。高质量的模型对于任何AGI的实现都是至关重要的，但同样重要的是以合理低廉的成本在大规模上部署和服务这些模型，而不妥协其准确性。服务准确性和服务性能（例如，每秒令牌数）之间的冲突是一个难题，促使了这一领域的大量研究[[3](#bib.bib3),
    [4](#bib.bib4)]。通常来说，在生产环境中没有一种通用的解决方案。为了提高性能的优化可能会导致准确性的降低，反之亦然。例如，稀疏性和量化是两种常见的技术，它们在追求更好性能的同时会牺牲准确性。
- en: Unfortunately, this conflict between accuracy and performance has been exacerbated
    recently by the growing demand for longer contextual understanding when deploying
    models in practice [[5](#bib.bib5)]. This introduces new complexities as the transformer’s
    attention mechanism exhibits a quadratic increase in resource consumption with
    longer contexts [[6](#bib.bib6)]. Furthermore, LLMs struggle to utilize information
    from longer contexts effectively [[7](#bib.bib7)]. Essentially, the need for long-context
    serving breaks the fragile balance between serving accuracy and performance, and
    calls for novel system designs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种准确性与性能之间的冲突最近因在实际部署模型时对更长上下文理解的日益增长的需求而加剧[[5](#bib.bib5)]。这引入了新的复杂性，因为变换器的注意机制在上下文更长时资源消耗呈二次增长[[6](#bib.bib6)]。此外，LLM在有效利用长上下文中的信息方面也面临困难[[7](#bib.bib7)]。本质上，对长上下文服务的需求打破了服务准确性和性能之间的脆弱平衡，并呼吁新的系统设计。
- en: To explore the complex relationship between accuracy and performance in large-scale
    model deployments, particularly for handling long contexts, we conducted an extensive
    survey of the LLM serving area. We highlight three key observations after reviewing
    related literature.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索在大规模模型部署中准确性与性能之间的复杂关系，特别是在处理长上下文时，我们对LLM服务领域进行了广泛的调查。我们在回顾相关文献后突出三点关键观察。
- en: '1.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'First, we find the scope of a serving system has expanded. It comprises two
    system layers: a model serving layer and an agent serving layer. The model-layer
    system runs a given LLM model, typically exposing model inference as its northbound
    APIs [[8](#bib.bib8), [9](#bib.bib9)]. Works at this layer commonly optimize the
    model structure [[10](#bib.bib10), [11](#bib.bib11)], cache [[8](#bib.bib8), [12](#bib.bib12)],
    scheduling [[13](#bib.bib13), [14](#bib.bib14)], etc. The agent-layer system sits
    atop the model-layer system and results from emerging LLM-based system applications
    that leverage LLM-driven workflow to improve a raw LLM model’s accuracy and efficiency
    while handling complex real-world tasks [[15](#bib.bib15)].'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们发现服务系统的范围已扩展。它包括两个系统层级：模型服务层和代理服务层。模型层系统运行给定的LLM模型，通常将模型推理暴露为其北向API[[8](#bib.bib8),
    [9](#bib.bib9)]。这一层的工作通常优化模型结构[[10](#bib.bib10), [11](#bib.bib11)]、缓存[[8](#bib.bib8),
    [12](#bib.bib12)]、调度[[13](#bib.bib13), [14](#bib.bib14)]等。代理层系统位于模型层系统之上，是基于LLM的系统应用的结果，这些应用利用LLM驱动的工作流来提高原始LLM模型的准确性和效率，同时处理复杂的现实世界任务[[15](#bib.bib15)]。
- en: '2.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Second, we find works in this space optimize along three distinct goals: improving
    serving context length (Context), improving serving accuracy (Accuracy), and improving
    serving performance (Performance). Specifically, context means the number of tokens
    in the context window; accuracy means evaluation metrics on certain tasks (e.g.,
    MMLU), and performance means time-to-first-token, tokens per second, price per
    million tokens, etc.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，我们发现这一领域的工作在优化三个不同的目标：提高服务上下文长度（Context）、提高服务准确性（Accuracy）以及提高服务性能（Performance）。具体来说，上下文是指上下文窗口中的令牌数量；准确性是指在某些任务上的评估指标（例如，MMLU）；性能是指首个令牌的时间、每秒令牌数量、每百万令牌的价格等。
- en: '3.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Finally, we find a trilemma among the above three goals regardless of which
    layer they are applied to. We find that any serving optimization can only improve
    at most two distinct goals. We also observe progress in one direction does not
    lead to progress in others. For example, using positional embedding to extend
    a model’s range does not improve the model’s accuracy beyond the context length [[16](#bib.bib16)],
    and using quantization [[11](#bib.bib11)], pruning [[17](#bib.bib17)], and sparsity [[12](#bib.bib12)]
    enable one to serve a model with faster speed but at the cost of potentially lower
    accuracy.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们发现上述三个目标之间存在一种三难困境，不管它们应用于哪个层级。我们发现任何服务优化至多只能改善两个不同的目标。我们还观察到在某一个方向上的进展不会导致其他方向上的进展。例如，使用位置嵌入扩展模型的范围并不能改善模型在上下文长度之外的准确性[[16](#bib.bib16)]，而使用量化[[11](#bib.bib11)]、剪枝[[17](#bib.bib17)]和稀疏性[[12](#bib.bib12)]可以让模型的服务速度更快，但可能会以降低准确性为代价。
- en: 'Based on the above observations and inspired by the classical CAP theorem in
    databases [[18](#bib.bib18)], we propose the CAP principle for LLM serving, which
    states that any given LLM serving optimization, regardless of which system layer
    it is applied to, can improve at most two of the following goals:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述观察，并受到数据库中经典CAP理论[[18](#bib.bib18)]的启发，我们提出了LLM服务的CAP原则，该原则指出任何给定的LLM服务优化，无论应用于哪个系统层级，最多只能改善以下两个目标中的两个：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Context: The length of context effectively processed and perceived by end users.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文：最终用户有效处理和感知的上下文长度。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accuracy: The precision of outputs as evaluated by end users, based on specific
    task metrics.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性：最终用户基于特定任务指标评估的输出精度。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance: The efficiency of token processing and generation perceived by
    end users.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能：最终用户感知的令牌处理和生成效率。
- en: '![Refer to caption](img/138c5f207524096b825f5b07059d464d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/138c5f207524096b825f5b07059d464d.png)'
- en: 'Figure 1: The CAP principle for LLM Serving. C is improving context length,
    A is improving accuracy, and P is improving serving performance or cost-efficiency
    in general. It states that any serving optimization can improve at most two of
    the above three goals.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM服务的CAP原则。C是改善上下文长度，A是提高准确性，P是提高服务性能或成本效益。它指出，任何服务优化最多只能改善上述三个目标中的两个。
- en: 'The perspective of the proposed CAP principle emphasizes what end users perceive
    from applying a specific optimization to a remote LLM serving system rather than
    focusing on a specific component within the LLM serving system. This is crucial
    because we care whether an LLM serving system as a whole can serve AGI rather
    than a singular improvement in one direction. In general, this principle leads
    to six types of optimizations: C, A, P, CA, CP, and AP, depending on which goals
    are prioritized.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的CAP原则的视角强调了最终用户从对远程LLM服务系统应用特定优化中感知到的效果，而不是专注于LLM服务系统中的某个特定组件。这一点非常重要，因为我们关心的是LLM服务系统整体是否能服务于AGI，而不仅仅是某个方向上的单一改进。一般来说，这一原则导致了六种优化类型：C、A、P、CA、CP和AP，具体取决于优先考虑哪些目标。
- en: The LLM’s CAP principle is similar to the database’s CAP theorem in many ways.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的CAP原则在许多方面与数据库的CAP定理类似。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Both state that you must forfeit at least one goal to achieve the others. Since
    our focus is on long-context serving, maintaining a lengthy context (C) is essential.
    This leaves us with two options: improving accuracy (A) or improving performance
    (P). Improving accuracy relies on devising new algorithms to better leverage the
    feature of lengthy context. However, these algorithms could hurt model execution
    cost-efficiency due to increased FLOPs, hardware-unfriendly operations, etc. On
    the other hand, enhancing performance on specific hardware through techniques
    like quantization and sparsity usually comes at the cost of reduced accuracy.
    Although there are methods to increase performance without losing accuracy, they
    generally require additional hardware resources.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两者都指出，为了实现其他目标，你必须放弃至少一个目标。由于我们的重点是长上下文服务，保持较长的上下文（C）是至关重要的。这使我们面临两个选择：提高准确性（A）或提高性能（P）。提高准确性依赖于设计新算法，以更好地利用长上下文的特性。然而，这些算法可能由于增加的FLOPs、对硬件不友好的操作等因素，损害模型执行的成本效益。另一方面，通过量化和稀疏性等技术在特定硬件上提升性能通常会导致准确性下降。尽管有些方法可以在不降低准确性的情况下提高性能，但这些方法通常需要额外的硬件资源。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Their goals are measured continuously rather than in binary. The definition
    and continuity of user-perceived measurement metrics are crucial in determining
    whether a goal has been met. Some recent studies have examined this aspect for
    accuracy [[19](#bib.bib19), [20](#bib.bib20)]. The availability of the database’s
    CAP and the accuracy of the LLM’s CAP both range from 0 to 100. The accuracy of
    LLM’s CAP principle, like the availability of the database’s CAP theorem, does
    not have to be 100%. It just has to be high enough that end users deem it useful.
    Thus, from a system’s perspective, an optimization categorized as CP might still
    be perceived as achieving all three CAP goals if it fulfils the user’s accuracy
    requirements, similar to how CAP is observed in practical databases [[21](#bib.bib21)].
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它们的目标是持续测量而不是二元的。用户感知的测量指标的定义和连续性对于确定目标是否达成至关重要。最近的一些研究已考察了这一方面的准确性[[19](#bib.bib19),
    [20](#bib.bib20)]。数据库的CAP和LLM的CAP的可用性均在0到100范围内。LLM的CAP原则的准确性，类似于数据库的CAP定理的可用性，并不需要达到100%。只要足够高，以至于最终用户认为它有用即可。因此，从系统的角度来看，虽然CP类别的优化可能在实际使用中被感知为达成了所有三个CAP目标，但这取决于是否满足用户的准确性需求，类似于CAP在实际数据库中的观察结果[[21](#bib.bib21)]。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Both are originally proposed to keep system designers aware of the hard design
    trade-offs while deploying large-scale systems.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两者最初都是为了让系统设计师在部署大规模系统时意识到艰难的设计权衡。
- en: We foresee the possibility of a true CAP in the future, in which there is no
    inherent conflict among these goals. The proposed CAP principle primarily arises
    from the use of transformer-based LLMs on existing AI chips, reflecting both the
    constraints and capabilities of today’s hardware and software. As we progress
    towards AGI, both models and hardware are expected to evolve significantly. Emerging
    technologies are likely to be developed in tandem, with new models specifically
    designed to optimize performance on the next generation of hardware. This synergy
    between evolving models and hardware is crucial for overcoming current barriers
    and achieving a true CAP in LLM serving.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预见到未来可能实现真正的CAP，其中这些目标之间没有固有冲突。提出的CAP原则主要源于对现有AI芯片上基于transformer的LLM的使用，反映了今天硬件和软件的限制与能力。随着我们向AGI迈进，模型和硬件预计都会发生显著演变。新兴技术很可能会同步发展，新模型将专门设计以优化在下一代硬件上的性能。这种不断演变的模型和硬件之间的协同作用对克服当前障碍和实现真正的CAP至关重要。
- en: 'Our survey is organized based on the propose CAP principle. Compared to prior
    surveys [[4](#bib.bib4), [3](#bib.bib3), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], we makes two unique contributions. First,
    we propose the CAP principle for LLM serving and map existing works onto the CAP
    landscape to highlight the tension among them. Second, we approach the large-scale
    LLM serving system as a whole rather than focusing on a specific technique (e.g.,
    RAG [[26](#bib.bib26)], long-context [[23](#bib.bib23)]), or a layer (e.g., model [[3](#bib.bib3)],
    agent [[25](#bib.bib25)]). In the rest of the paper, we will discuss works as
    listed in Table [1](#S2.T1 "Table 1 ‣ 2 CAP for LLM Serving ‣ The CAP Principle
    for LLM Serving: A Survey of Long-Context Large Language Model Serving") and Figure [2](#S2.F2
    "Figure 2 ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey
    of Long-Context Large Language Model Serving"). We focus on works that extend
    serving context length and address the resulting accuracy and performance issues.
    Specifically, we will cover model memory (Table [2](#S2.T2 "Table 2 ‣ 2.2.1 Model
    Memory ‣ 2.2 Improve Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for
    LLM Serving: A Survey of Long-Context Large Language Model Serving")), positional
    embedding (Table [3](#S2.T3 "Table 3 ‣ 2.2.2 Positional Embedding ‣ 2.2 Improve
    Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey
    of Long-Context Large Language Model Serving")), found-in-the-middle, distributed
    acceleration for long context, prompt compression, sparsity (Table [5](#S2.T5
    "Table 5 ‣ 2.4.1 Sparse Attention ‣ 2.4 Improve Performance (P) ‣ 2 CAP for LLM
    Serving ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large Language
    Model Serving")), and agent memory (Table [7](#S2.T7 "Table 7 ‣ 2.6.1 Agent Memory
    ‣ 2.6 Improve Context and Accuracy (CA) ‣ 2 CAP for LLM Serving ‣ The CAP Principle
    for LLM Serving: A Survey of Long-Context Large Language Model Serving")).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的调查基于提出的CAP原则进行组织。与之前的调查[[4](#bib.bib4), [3](#bib.bib3), [22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]相比，我们做出了两个独特的贡献。首先，我们提出了LLM服务的CAP原则，并将现有工作映射到CAP框架上，以突出它们之间的紧张关系。其次，我们将大规模LLM服务系统视为一个整体，而不是专注于某一特定技术（如RAG
    [[26](#bib.bib26)]，长上下文 [[23](#bib.bib23)]），或某一层（如模型 [[3](#bib.bib3)]，代理 [[25](#bib.bib25)]）。在接下来的论文中，我们将讨论表[1](#S2.T1
    "Table 1 ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey
    of Long-Context Large Language Model Serving")和图[2](#S2.F2 "Figure 2 ‣ 2 CAP for
    LLM Serving ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large
    Language Model Serving")中列出的工作。我们重点关注那些扩展服务上下文长度并解决由此产生的准确性和性能问题的工作。具体而言，我们将涉及模型记忆（表[2](#S2.T2
    "Table 2 ‣ 2.2.1 Model Memory ‣ 2.2 Improve Context (C) ‣ 2 CAP for LLM Serving
    ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model
    Serving")），位置嵌入（表[3](#S2.T3 "Table 3 ‣ 2.2.2 Positional Embedding ‣ 2.2 Improve
    Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey
    of Long-Context Large Language Model Serving")），中间发现，长上下文的分布式加速，提示压缩，稀疏性（表[5](#S2.T5
    "Table 5 ‣ 2.4.1 Sparse Attention ‣ 2.4 Improve Performance (P) ‣ 2 CAP for LLM
    Serving ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large Language
    Model Serving")），以及代理记忆（表[7](#S2.T7 "Table 7 ‣ 2.6.1 Agent Memory ‣ 2.6 Improve
    Context and Accuracy (CA) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM
    Serving: A Survey of Long-Context Large Language Model Serving")）。'
- en: 2 CAP for LLM Serving
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 CAP for LLM Serving
- en: 'Table 1: The CAP theorem for LLM serving results in six types.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLM服务的CAP定理产生六种类型。
- en: '| Type | Optimizations |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 优化 |'
- en: '| C | Model Memory, Positional Embedding |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| C | 模型记忆，位置嵌入 |'
- en: '| A | Found-in-the-middle |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| A | 中间发现 |'
- en: '| P | Sparse Attention, Linear Attention, Distributed Accl., Quantization,
    Model Pruning |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| P | 稀疏注意力、线性注意力、分布式加速、量化、模型剪枝 |'
- en: '| CP | Prompt Pruning |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| CP | 提示剪枝 |'
- en: '| CA | Agent Memory |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| CA | 代理记忆 |'
- en: '| AP | N/A |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| AP | 不适用 |'
- en: '![Refer to caption](img/a0034d6d17598a546437294a4690d5c6.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a0034d6d17598a546437294a4690d5c6.png)'
- en: 'Figure 2: A modern-day LLM serving system commonly has two layers: a model
    layer, which runs a given LLM model, and an agent layer, which runs LLM-based
    system applications. PE means Positional Embedding. Quant is short for quantization.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：现代LLM服务系统通常有两层：一层是模型层，运行给定的LLM模型；另一层是代理层，运行基于LLM的系统应用。PE表示位置嵌入。Quant是量化的缩写。
- en: 2.1 Overview
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 概述
- en: 'We survey the area and map them onto Table [1](#S2.T1 "Table 1 ‣ 2 CAP for
    LLM Serving ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large
    Language Model Serving") across the agent and model layers as depicted in Figure [2](#S2.F2
    "Figure 2 ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey
    of Long-Context Large Language Model Serving"). Remarkably, we can map all existing
    LLM serving optimization works onto six types resulting from CAP, highlighting
    that our proposed CAP principle reflects the inherent and long-standing design
    trade-offs in this area.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '我们调查了该领域并将其映射到表格[1](#S2.T1 "Table 1 ‣ 2 CAP for LLM Serving ‣ The CAP Principle
    for LLM Serving: A Survey of Long-Context Large Language Model Serving")中，如图[2](#S2.F2
    "Figure 2 ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey
    of Long-Context Large Language Model Serving")所示的代理和模型层。值得注意的是，我们可以将所有现有的LLM服务优化工作映射到由CAP产生的六种类型上，这突显了我们提出的CAP原则反映了该领域固有且长期存在的设计权衡。'
- en: 'An overview of Table [1](#S2.T1 "Table 1 ‣ 2 CAP for LLM Serving ‣ The CAP
    Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving"):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S2.T1 "Table 1 ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving:
    A Survey of Long-Context Large Language Model Serving")概述：'
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'There are six types: C, A, P, CA, CP, and AP, depending on which goals are
    prioritized.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总共有六种类型：C、A、P、CA、CP和AP，具体取决于优先考虑哪些目标。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'C: works in this area only improve the context length of an LLM serving system.
    Our research identifies two approaches to improve C. We dub the first as Model
    Memory, a line of work that augments the transformer with recurrence and dynamic
    external memory. The other is Positional Embedding, which extends the context
    window of the model to a longer context and more tokens.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C：该领域的工作仅提高了LLM服务系统的上下文长度。我们的研究识别了两种改进C的方法。我们将第一种称为模型记忆，这是一类通过增加递归和动态外部记忆来增强变换器的工作。另一种是位置嵌入，它将模型的上下文窗口扩展到更长的上下文和更多的标记。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A: works in this area address the accuracy issues that arise from long-context
    serving. A few initial works exist, such as found-in-the-middle, but some forfeit
    P for a better A.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A：该领域的工作解决了由于长上下文服务引发的准确性问题。虽然存在一些初步的工作，如发现中间问题，但有些工作牺牲了P以获得更好的A。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'P: works in this area improve serving performance or cost-efficiency in general.
    We focus on two lines of work specifically proposed for improving long-context
    serving. The first is distributed acceleration, which explores sequence parallelism
    for faster processing. The second is sparsity, which reduces the computation and
    memory usage for better performance'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: P：该领域的工作通常提高了服务性能或成本效益。我们专注于两类特别为改进长上下文服务而提出的工作。第一类是分布式加速，它探索序列并行以加快处理速度。第二类是稀疏性，它减少了计算和内存使用，以获得更好的性能。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CP: this type of work improves both at the same time. We have identified prompt
    compression as this category’s only line of work.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CP：这种类型的工作同时改进了两者。我们已将提示压缩识别为此类别的唯一工作方向。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CA: this type of work improves both at the same time. We have identified agent
    memory as this category’s only line of work.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CA：这种类型的工作同时改进了两者。我们已将代理记忆识别为此类别的唯一工作方向。
- en: 2.2 Improve Context (C)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 改进上下文（C）
- en: This section surveys work that extends serving systems’ context length to address
    the increasing demand for long-context reasoning. We will discuss two approaches.
    We dub the first as Model Memory, a line of work that augments the transformer
    architecture with recurrence and dynamic external memory. The other is Positional
    Embedding, which extends the context window of LLMs to deal with more tokens.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本节调查了扩展服务系统上下文长度以应对对长上下文推理日益增长的需求的工作。我们将讨论两种方法。我们将第一种称为模型记忆，这是一类通过增加递归和动态外部记忆来增强变换器架构的工作。另一种是位置嵌入，它扩展了LLM的上下文窗口以处理更多的标记。
- en: 2.2.1 Model Memory
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 模型记忆
- en: 'Table 2: Comparing model memory works.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：比较模型记忆工作。
- en: '| Work | Memory Aggregation | Memory Org. | Memory Retrieval | Memory Update
    | Memory Eviction |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 内存聚合 | 内存组织 | 内存检索 | 内存更新 | 内存驱逐 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Transformer-XL [[10](#bib.bib10)] | dot-attention | FIFO | all | None | Discard
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Transformer-XL [[10](#bib.bib10)] | 点注意力 | FIFO | 全部 | 无 | 丢弃 |'
- en: '| Compressive Transformer [[27](#bib.bib27)] | dot-attention | FIFO | all |
    None | Discard |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Compressive Transformer [[27](#bib.bib27)] | 点注意力 | FIFO | 全部 | 无 | 丢弃 |'
- en: '| Memorizing Transformer [[28](#bib.bib28)] | learned-gate | FIFO | kNN | None
    | Discard |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Memorizing Transformer [[28](#bib.bib28)] | 学习门控 | FIFO | kNN | 无 | 丢弃 |'
- en: '| Memformer [[29](#bib.bib29)] | dot-attention | random | all | Yes | Yes |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Memformer [[29](#bib.bib29)] | 点注意力 | 随机 | 全部 | 是 | 是 |'
- en: '| Memory Transformer [[30](#bib.bib30)] | soft prompt | random | all | Yes
    | Yes |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Memory Transformer [[30](#bib.bib30)] | 软提示 | 随机 | 全部 | 是 | 是 |'
- en: '| RMT [[31](#bib.bib31)] | soft prompt | FIFO | all | None | Discard |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| RMT [[31](#bib.bib31)] | 软提示 | FIFO | 全部 | 无 | 丢弃 |'
- en: '| AutoCompressor [[32](#bib.bib32)] | soft prompt | FIFO | all | None | Discard
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| AutoCompressor [[32](#bib.bib32)] | 软提示 | FIFO | 全部 | 无 | 丢弃 |'
- en: '| Infini-Attention [[33](#bib.bib33)] | learned gate | random | linear | Yes
    | Yes |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Infini-Attention [[33](#bib.bib33)] | 学习门控 | 随机 | 线性 | 是 | 是 |'
- en: One way to extend the context length of transformers is by adding memory to
    hold long-range information. Model memory is a term we dubbed for such a line
    of work, which augments the transformer architecture with recurrence and dynamic
    external memory. At its core, model memory builds a memory system for the transformer,
    enabling it to examine past long-range information.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展变压器的上下文长度的一种方法是添加内存以保存远程信息。模型内存是我们为这一类工作起的术语，它通过递归和动态外部内存增强了变压器架构。核心在于，模型内存为变压器构建了一个内存系统，使其能够检查过去的远程信息。
- en: Taxonomy from the systems perspective
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从系统的角度来看的分类
- en: We realize that managing the transformer model’s augmented memory is similar
    to the classical virtual memory management in OS [[34](#bib.bib34)], which centers
    around organizing memory, what to read, update, and what and when to evict. To
    this end, we propose to compare model memory works by mapping them into the following
    five dimensions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认识到，管理变压器模型的扩展内存类似于操作系统中的经典虚拟内存管理[[34](#bib.bib34)]，其核心在于组织内存、读取、更新以及驱逐的内容和时间。为此，我们建议通过将模型内存工作映射到以下五个维度来进行比较。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Aggregation: dictates how to aggregate local memory with global memory
    (retrieved from the augmented memory). It can be attention, learned gate, or soft
    prompt.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存聚合：规定如何将局部内存与全局内存（从扩展内存中检索）进行聚合。它可以是注意力、学习门控或软提示。
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Organization: dictates how the external augmented memory is organized.
    It can be a FIFO buffer or random-access buffer, with fixed memory size. It appears
    there is no dynamic-sized memory due to capacity concerns.'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存组织：规定外部扩展内存如何组织。它可以是FIFO缓冲区或随机访问缓冲区，具有固定的内存大小。由于容量问题，似乎没有动态大小的内存。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Retrieval: dictates how and what to retrieve from the augmented memory.
    Most works will retrieve the full memory, while others retrieve a portion using
    certain algorithms.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存检索：规定如何以及从扩展内存中检索什么。大多数工作会检索全部内存，而其他工作则使用某些算法检索部分内存。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Update: dictates how to update the augmented memory when there is a
    new memory. If it is FIFO memory, the update means enqueue. If it is random memory,
    the update will use certain algorithms to update all or parts of the memory.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存更新：规定当有新内存时如何更新扩展内存。如果是FIFO内存，更新意味着入队。如果是随机内存，更新将使用某些算法来更新全部或部分内存。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Eviction: dictates what to evict when the augmented memory is full.
    If it is FIFO memory, the eviction discards the tail memory. If it is random memory,
    no eviction will occur as memory is updated in place.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存驱逐：规定当扩展内存满时，应该驱逐什么。如果是FIFO内存，驱逐会丢弃尾部内存。如果是随机内存，则不会发生驱逐，因为内存会就地更新。
- en: 'We now delve into works listed in Table [2](#S2.T2 "Table 2 ‣ 2.2.1 Model Memory
    ‣ 2.2 Improve Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM
    Serving: A Survey of Long-Context Large Language Model Serving").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在将深入研究表格[2](#S2.T2 "Table 2 ‣ 2.2.1 Model Memory ‣ 2.2 Improve Context (C)
    ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A Survey of Long-Context
    Large Language Model Serving")中列出的工作。'
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transformer-XL [[10](#bib.bib10)] adds recurrence to the transformer architecture.
    It captures long-term dependency using a per-layer memory buffer and segments
    long sequences into fixed-size segments to capture segment-level recurrence between
    adjacent layers. Its memory organization is FIFO, and there are no update rules.
    Old memories are discarded as new segments come in. During inference, it aggregates
    the hidden states read from memory and local states from the current segment using
    dot-product-attention. Compressive Transformer [[27](#bib.bib27)] adds a second-level
    compressed memory to Transformer-XL. It extends the context further without changing
    the core mechanisms. Memorizing Transformer [[28](#bib.bib28)] takes a slightly
    different approach. Instead of reading the whole memory, it uses a kNN algorithm
    to retrieve from the external memory and aggregates via a learned gate. The above
    three works discard information from the distant past.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer-XL [[10](#bib.bib10)] 在 transformer 架构中增加了递归。它使用每层记忆缓冲区捕获长期依赖，并将长序列分段成固定大小的段，以捕获相邻层之间的段级递归。其记忆组织为
    FIFO，并且没有更新规则。旧的记忆会随着新段的到来而被丢弃。在推理过程中，它使用点积注意力将从记忆中读取的隐藏状态与来自当前段的本地状态进行聚合。Compressive
    Transformer [[27](#bib.bib27)] 在 Transformer-XL 的基础上增加了第二级压缩记忆。它在不改变核心机制的情况下进一步扩展上下文。Memorizing
    Transformer [[28](#bib.bib28)] 采用了略微不同的方法。它不是读取整个记忆，而是使用 kNN 算法从外部记忆中检索，并通过学习到的门进行聚合。上述三种方法都丢弃了来自远古的记忆。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memformer [[29](#bib.bib29)] adds a fixed-size dynamic external memory to the
    transformer architecture. It uses random-access memory rather than the FIFO memory
    used by the former two works. It segregates the memory into many slots and devises
    an attention-based algorithm to update the memory slots independently. Additionally,
    it uses a forgetting mechanism to evict memory slots that are not updated for
    many timestamps. By doing so, it attends to more important information and claims
    a theoretically infinite temporal range of memorization.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Memformer [[29](#bib.bib29)] 在 transformer 架构中增加了固定大小的动态外部记忆。它使用随机访问内存，而不是前两者使用的
    FIFO 内存。它将记忆分隔成多个槽，并设计了一种基于注意力的算法来独立更新记忆槽。此外，它使用遗忘机制来驱逐长时间未更新的记忆槽。通过这样做，它关注于更重要的信息，并声称具有理论上的无限时间范围的记忆。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memory Transformer [[30](#bib.bib30)] differs from Memformer [[29](#bib.bib29)]
    in that the former uses soft prompt [[35](#bib.bib35)] to aggregate information
    from external memory with the current prompt. It prepends memory tokens to tokenized
    user prompts and uses an unmodified attention module to enable memory tokens to
    attend to long sequences.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Memory Transformer [[30](#bib.bib30)] 与 Memformer [[29](#bib.bib29)] 的不同之处在于前者使用软提示 [[35](#bib.bib35)]
    将外部记忆中的信息与当前提示聚合。它在分词用户提示前添加记忆标记，并使用未修改的注意力模块使记忆标记能够关注长序列。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RMT [[31](#bib.bib31)] and AutoCompressor [[32](#bib.bib32)] use soft prompting
    to add memory tokens to the beginning of the prompts, which is similar to Memory
    Transformer [[30](#bib.bib30)] and segment-level recurrence as in Transformer-XL [[10](#bib.bib10)].
    Both are built based on Transformer-XL’s code base.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RMT [[31](#bib.bib31)] 和 AutoCompressor [[32](#bib.bib32)] 使用软提示在提示的开头添加记忆标记，这类似于
    Memory Transformer [[30](#bib.bib30)] 和 Transformer-XL [[10](#bib.bib10)] 中的段级递归。这两者都是基于
    Transformer-XL 的代码库构建的。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Infin-Attention [[33](#bib.bib33)] is the latest work in this category. It closely
    integrates compressive and dynamic memory with the vanilla dot-product attention
    layer to enable models to attend to infinite context length. It adopts an associative
    matrix as its memory, allowing random access. It retrieves memory using linear
    attention and updates memory using a delta update rule. It aggregates retrieved
    memory with a local attention state using a learned gate. This approach uses less
    compute and memory compared to the vanilla Transformer-XL.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Infin-Attention [[33](#bib.bib33)] 是这一类别中的最新工作。它将压缩性和动态记忆与传统的点积注意力层紧密集成，使模型能够处理无限的上下文长度。它采用关联矩阵作为记忆，允许随机访问。它通过线性注意力检索记忆，并使用
    delta 更新规则更新记忆。它通过学习到的门将检索到的记忆与本地注意力状态进行聚合。与传统的 Transformer-XL 相比，这种方法使用了更少的计算和内存。
- en: In summary, the model memory line of work augments the original transformer
    architecture with dynamic and compressive memory, enabling the model to process
    long or even infinite contexts. They differ in how they access the memory, how
    memory is updated, etc. Since most of them either discard or compress memory,
    they inevitably hurt A. They are neutral in P as they do not address the quadratic
    complexity in the attention mechanism.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，模型记忆的研究增强了原始变换器架构，提供了动态和压缩记忆，使得模型能够处理长或甚至无限的上下文。它们在如何访问记忆、记忆如何更新等方面有所不同。由于大多数都要么丢弃要么压缩记忆，因此不可避免地会影响
    A。它们在 P 上是中性的，因为它们没有解决注意力机制中的二次复杂度。
- en: 2.2.2 Positional Embedding
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 位置嵌入
- en: 'Table 3: Comparing positional embedding works.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：比较位置嵌入工作的表格。
- en: '| Work | Location | Require Training | Adaptive | Integration |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 位置 | 需要训练 | 自适应 | 集成 |'
- en: '| ALiBi [[36](#bib.bib36)] | After QK multiply | ✓ | ✗ | Add |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ALiBi [[36](#bib.bib36)] | QK 相乘后 | ✓ | ✗ | 加 |'
- en: '| XPOS [[37](#bib.bib37)] | Before QK multiply | ✓ | ✗ | Multiply |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| XPOS [[37](#bib.bib37)] | QK 相乘前 | ✓ | ✗ | 乘 |'
- en: '| CLEX [[38](#bib.bib38)] | Before QK multiply | ✓ | ✓ | Multiply |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| CLEX [[38](#bib.bib38)] | QK 相乘前 | ✓ | ✓ | 乘 |'
- en: '| Linear Interpolation [[39](#bib.bib39)] | Before QK multiply | ✗ | ✗ | Multiply
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 线性插值 [[39](#bib.bib39)] | QK 相乘前 | ✗ | ✗ | 乘 |'
- en: '| NTK Interpolation  [[40](#bib.bib40)] | Before QK multiply | ✗ | ✗ | Multiply
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| NTK 插值 [[40](#bib.bib40)] | QK 相乘前 | ✗ | ✗ | 乘 |'
- en: '| YaRN [[41](#bib.bib41)] | Before QK multiply | ✓ | ✓ | Multiply |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| YaRN [[41](#bib.bib41)] | QK 相乘前 | ✓ | ✓ | 乘 |'
- en: '| FIRE [[42](#bib.bib42)] | After QK multiply | ✓ | ✓ | Add |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| FIRE [[42](#bib.bib42)] | QK 相乘后 | ✓ | ✓ | 加 |'
- en: '| LongRoPE [[43](#bib.bib43)] | Before QK multiply | ✓ | ✓ | Multiply |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LongRoPE [[43](#bib.bib43)] | QK 相乘前 | ✓ | ✓ | 乘 |'
- en: 'This line of work focuses on positional embedding (PE), enabling LLM to handle
    long context sequences (hence improving C). In Table [3](#S2.T3 "Table 3 ‣ 2.2.2
    Positional Embedding ‣ 2.2 Improve Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP
    Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving"),
    we compare them across four dimensions.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '这方面的工作集中于位置嵌入（PE），使得 LLM 能够处理长上下文序列（从而提高 C）。在表 [3](#S2.T3 "Table 3 ‣ 2.2.2
    Positional Embedding ‣ 2.2 Improve Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP
    Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving")
    中，我们从四个维度对它们进行了比较。'
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Location: where is position information being encoded into token representation.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置：位置信息如何编码到标记表示中。
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Require training: whether it can plug-and-play without re-training.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要训练：是否可以即插即用而无需重新训练。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adaptive: whether it can adapt and adjust based on the input.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应：是否可以根据输入进行适应和调整。
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integration: how are position representations integrated with token representations.'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集成：位置表示如何与标记表示集成。
- en: Our discussion below is categorized as extrapolation and interpolation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下面的讨论分为外推和插值。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Position Extrapolation. This strategy extends the position embedding beyond
    the max context length used in training. For example, ALiBi [[36](#bib.bib36)]
    introduces relative positional embedding and a learnable linear bias on attention,
    which allows the model to dynamically adjust the attention distribution according
    to the actual length of the sequence. XPOS [[37](#bib.bib37)] introduces an additional
    exponential decay term based on ROPE, which allows attention to decay with increasing
    relative distance. CLEX [[38](#bib.bib38)] models the continuous dynamics as an
    ordinary differential equation with length scaling factors by generalizing the
    position-embedding scaling.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置外推。这种策略将位置嵌入扩展到超出训练中使用的最大上下文长度。例如，ALiBi [[36](#bib.bib36)] 引入了相对位置嵌入和一个可学习的线性偏差在注意力机制中，这允许模型根据序列的实际长度动态调整注意力分布。XPOS
    [[37](#bib.bib37)] 引入了基于 ROPE 的额外指数衰减项，使得注意力随着相对距离的增加而衰减。CLEX [[38](#bib.bib38)]
    将连续动态建模为具有长度缩放因子的常微分方程，通过推广位置嵌入缩放来实现。
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Position Interpolation: This strategy scales the input position encoding index
    range to the context window of a model. For example, Linear Interpolation [[39](#bib.bib39)]
    introduces a position interpolation technique that directly reduces the position
    index. In this way, the maximum position index matches the previous context window
    constraints of the pre-training phase and hence extends the context window. Inspired
    by Neural Tangent Kernel (NTK) theory, the model using only positional interpolation
    would have difficulty recognizing the order and position of neighboring tokens.
    NTK Interpolation [[40](#bib.bib40)] devised a nonlinear method that changes the
    base in RoPE to adjust the scaling factor dynamically. YaRN [[41](#bib.bib41)]
    combines NTK Interpolation and Linear Interpolation and introduces an attention
    distribution correction strategy to offset the distributional bias in the attention
    matrix caused by long inputs. FIRE [[42](#bib.bib42)] uses a learnable continuous
    function to map position information to biases and proposes progressive interpolation
    to address generalization issues when input lengths are outside the training domain.
    LongRoPE [[43](#bib.bib43)] improves the position interpolation method by recognizing
    and exploiting non-uniformity in the RoPE dimensions and non-uniformity in the
    token positions. PoSE [[44](#bib.bib44)] introduces a training approach called
    Positional Skip-wise Method, which emulates extended inputs within a fixed context
    window by applying tailored skipping bias terms to adjust the position indices
    for each segment.'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置内插：这一策略将输入位置编码索引范围缩放到模型的上下文窗口。例如，线性内插 [[39](#bib.bib39)] 引入了一种直接减少位置索引的内插技术。通过这种方式，最大位置索引符合预训练阶段的先前上下文窗口限制，从而扩展上下文窗口。受到神经切线核（NTK）理论的启发，单独使用位置内插的模型将难以识别相邻标记的顺序和位置。NTK
    内插 [[40](#bib.bib40)] 设计了一种非线性方法，通过动态调整 RoPE 中的基数来调整缩放因子。YaRN [[41](#bib.bib41)]
    结合了 NTK 内插和线性内插，并引入了一种注意力分布修正策略，以抵消长输入导致的注意力矩阵中的分布偏差。FIRE [[42](#bib.bib42)] 使用可学习的连续函数将位置信息映射到偏差，并提出了渐进内插以解决输入长度超出训练领域时的泛化问题。LongRoPE
    [[43](#bib.bib43)] 通过识别并利用 RoPE 维度的非均匀性和标记位置的非均匀性来改进位置内插方法。PoSE [[44](#bib.bib44)]
    引入了一种称为位置跳跃方法的训练方法，通过应用定制的跳跃偏置项来调整每个段的位置信息，从而在固定的上下文窗口内模拟扩展输入。
- en: In summary, the research on positional embeddings enhances the model’s ability
    to generalize positional information that was not present during the training
    phase through both extrapolation and interpolation. These methods vary depending
    on whether the input position index range is scaled to fit within the model’s
    context window. They are neutral in C and P. and we believe they are crucial for
    achieving long-context serving.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，关于位置嵌入的研究通过外推和内插增强了模型对训练阶段不存在的位置信息的泛化能力。这些方法因输入位置索引范围是否被缩放到模型的上下文窗口内而有所不同。它们在
    C 和 P 中是中立的，我们相信它们对实现长上下文服务至关重要。
- en: 2.3 Improve Accuracy (A)
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 提高准确性（A）
- en: A longer C challenges A. This section focuses on works that address the accuracy
    issues that arise from long-context LLM serving. Lost-in-the-middle [[7](#bib.bib7)]
    is a pioneer work in analyzing how LLMs utilize long context. They found that
    existing LLMs cannot robustly utilize information in a lengthy context, and the
    position of the documents will affect the final serving accuracy. This drawback
    will limit long-context LLM’s usage in practical applications, leading to biases
    in outputs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 更长的 C 挑战 A。本节重点关注解决长上下文 LLM 服务中出现的准确性问题的工作。Lost-in-the-middle [[7](#bib.bib7)]
    是分析 LLM 如何利用长上下文的开创性工作。他们发现现有的 LLM 无法可靠地利用冗长上下文中的信息，文档的位置将影响最终的服务准确性。这一缺陷将限制长上下文
    LLM 在实际应用中的使用，导致输出结果的偏差。
- en: We find three works to address this issue.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现了三项研究来解决这一问题。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attention Sorting [[45](#bib.bib45)] addresses this issue by placing critical
    information at the end of the input prompt. They achieve this by performing one
    step of decoding, sorting documents by the attention they receive (highest attention
    going last), repeating the process, generate the answer with the newly sorted
    contexts. Though it could improve A, this approach’s limitations are clear: not
    all tasks map to a set of documents, and the extra sorting adds non-trivial overhead,
    hurting P.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Attention Sorting [[45](#bib.bib45)] 通过将关键信息放在输入提示的末尾来解决这个问题。他们通过执行一步解码、根据接收到的注意力（最高注意力的放在最后）对文档进行排序、重复这一过程来生成新的排序上下文的答案。尽管这可能提高A，但这种方法的局限性很明显：并非所有任务都能映射到一组文档，并且额外的排序增加了非平凡的开销，从而影响了P。
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Attention Bucket [[46](#bib.bib46)] uses multiple model replicas, each with
    a distinct based angle for the rotary position embedding. This creates a unique
    attention waveform to enhance LLM’s awareness of various contextual positions.
    This solution works across the model-layer and agent-layer. They improve A but
    forfeit P because they require multiple replicas to process the input prompt.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Attention Bucket [[46](#bib.bib46)] 使用了多个模型副本，每个副本有不同的旋转位置嵌入角度。这创建了独特的注意力波形，以增强LLM对各种上下文位置的感知。这一解决方案在模型层和代理层上都有效。他们提高了A，但牺牲了P，因为他们需要多个副本来处理输入提示。
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Found-in-the-middle [[47](#bib.bib47)] takes a much lighter approach. They
    found that the lost-in-the-middle phenomenon likely arises from two factors: casual
    attention in which LLMs disproportionately favor initial tokens [[12](#bib.bib12)]
    and long-term decay effect of RoPE [[16](#bib.bib16)] that diminishes the attention
    score of distantly positioned yet semantically meaningful tokens. Their answer
    is Multi-scale Positional Encoding (Ms-PoE), which assigns different scaling ratios
    to different attention heads to preserve information learned from the pre-training
    step while using the position indices rescaling to mitigate the long-term decay
    effect. This work belongs to the model layer and improves A without adding extra
    overhead.'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Found-in-the-middle [[47](#bib.bib47)] 采取了更轻量的方法。他们发现，丢失在中间的现象可能源于两个因素：LLMs在初始标记上的不成比例的关注
    [[12](#bib.bib12)] 和RoPE的长期衰减效应 [[16](#bib.bib16)]，该效应减少了远距离但语义上重要标记的注意力得分。他们的解决方案是多尺度位置编码（Ms-PoE），它为不同的注意力头分配不同的缩放比，以保留从预训练步骤中学到的信息，同时使用位置索引重新缩放来减轻长期衰减效应。这项工作属于模型层，并在不增加额外开销的情况下提高了A。
- en: In summary, improving A under a long C is an area that still needs close examination.
    There are some initial works that aim to improve long-context reasoning and understanding,
    but some of them forfeit P for a better A. We believe more research is needed
    to improve A and P simultaneously.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在长期上下文下提升A仍然是一个需要仔细研究的领域。一些初步的工作旨在改善长期上下文推理和理解，但其中一些为了更好的A而牺牲了P。我们认为需要更多的研究来同时提高A和P。
- en: 2.4 Improve Performance (P)
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 提升性能（P）
- en: 'This section covers works that improve P. Long-context serving demands significantly
    more resources in terms of computational flops and memory usage. From a system’s
    perspective, using principles such as parallelism or approximation to battle these
    issues is not uncommon. We focus on three lines of work specifically proposed
    for improving long-context serving: sparse attention, linear attention, and distributed
    acceleration.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了改善P的工作。长期上下文处理在计算浮点运算和内存使用方面需求显著增加。从系统的角度来看，使用诸如并行性或近似的原则来应对这些问题并不少见。我们特别关注三条用于改善长期上下文处理的工作路线：稀疏注意力、线性注意力和分布式加速。
- en: Sparse attention reduces resource usage by selectively focusing on only a subset
    of the inputs at each attention step. Linear attention reduces resource usage
    by approximating the attention calculation through a kernel function that maps
    the input features into a lower-dimensional space before computing the attention
    scores. Both techniques aim to reduce the quadratic complexity of the traditional
    attention mechanism. Linear attention does so through dimensionality reduction,
    while sparse attention uses selective focusing. Sparse attention is particularly
    useful when the importance of different parts of the data is non-uniform or when
    the sequence has a natural locality (like in images or structured text). Linear
    attention is more suited for tasks where the entire data needs to be compressed
    and processed efficiently. Distributed acceleration explores sequence parallelism
    for faster processing. We refer readers to  [[4](#bib.bib4), [3](#bib.bib3)] for
    general optimizations that improve P, for example, paged attention [[8](#bib.bib8)],
    flash attention [[48](#bib.bib48)], KV caching [[49](#bib.bib49)], etc.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏注意力通过在每个注意力步骤中仅选择性地关注输入的一个子集来减少资源使用。线性注意力通过通过一个核函数将输入特征映射到低维空间来近似注意力计算，从而减少资源使用，计算注意力分数。两种技术都旨在减少传统注意力机制的二次复杂性。线性注意力通过降维来实现，而稀疏注意力则通过选择性关注来实现。稀疏注意力在数据的不同部分重要性不均匀或序列具有自然局部性（如图像或结构化文本）时特别有用。线性注意力则更适合需要压缩和高效处理整个数据的任务。分布式加速探索了序列并行性以加快处理速度。我们建议读者参考[[4](#bib.bib4),
    [3](#bib.bib3)]，以了解提高P的一般优化，例如分页注意力[[8](#bib.bib8)]、闪光注意力[[48](#bib.bib48)]、KV缓存[[49](#bib.bib49)]等。
- en: 2.4.1 Sparse Attention
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1 稀疏注意力
- en: This section explores sparsity, a method that enhances computational efficiency
    by minimizing redundant $QK$ multiplication operations and reducing memory usage.
    We categorize sparsity techniques into four main types, based on two fundamental
    aspects. The first aspect relates to the transformer architecture. For the Encoder-Decoder
    architecture, sparsity is applied to selectively ignore less significant interactions
    between queries and keys in the attention computation. This helps focus computational
    resources on more crucial elements. For the Decoder-only architecture, sparsity
    is used to purge less important data from the key and value cache. The second
    aspect focuses on the strategy of identifying which connections between queries
    and keys are less important. These strategies are divided into two categories
    including dynamic and static sparsity [[50](#bib.bib50)]. Dynamic sparsity adapts
    to the incoming sequence by continually recognizing less important connections
    between queries and keys and filtering out corresponding tokens at runtime. Static
    sparsity, on the other hand, uses pre-determined sparse patterns to decide which
    connections to disregard, simplifying the implementation but potentially sacrificing
    adaptiveness.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了稀疏性，这是一种通过最小化冗余的$QK$乘法操作和减少内存使用来提升计算效率的方法。我们将稀疏性技术分为四种主要类型，基于两个基本方面。第一个方面涉及到变换器架构。对于编码器-解码器架构，稀疏性用于选择性地忽略查询和键之间在注意力计算中的不重要的交互，这有助于将计算资源集中于更关键的元素。对于仅解码器架构，稀疏性用于从键和值缓存中清除不重要的数据。第二个方面关注于识别查询和键之间哪些连接不重要的策略。这些策略分为两类，包括动态稀疏性和静态稀疏性[[50](#bib.bib50)]。动态稀疏性通过持续识别查询和键之间不重要的连接并在运行时过滤掉相应的标记来适应输入序列。静态稀疏性则使用预先确定的稀疏模式来决定忽略哪些连接，简化了实现但可能牺牲了适应性。
- en: 'We compare representative sparsity works in Table [5](#S2.T5 "Table 5 ‣ 2.4.1
    Sparse Attention ‣ 2.4 Improve Performance (P) ‣ 2 CAP for LLM Serving ‣ The CAP
    Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving")
    across four dimensions.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格[5](#S2.T5 "表 5 ‣ 2.4.1 稀疏注意力 ‣ 2.4 提高性能 (P) ‣ 2 LLM 服务的 CAP ‣ LLM 服务的
    CAP 原则：长上下文大语言模型服务的调查")中比较了代表性的稀疏性研究，涵盖了四个维度。
- en: •
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sparsity Strategy: whether the sparse pattern of the attention matrix is pre-defined
    (static) or determined dynamically during inference (dynamic, sometimes also called
    learned).'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稀疏性策略：注意力矩阵的稀疏模式是预定义的（静态）还是在推理过程中动态决定的（动态，有时也称为学习型）。
- en: •
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pattern Strategy: composition of retained connections (corresponding to static
    methods) and technique of obtaining the pattern (corresponding to dynamic methods).'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模式策略：保留连接的组成（对应静态方法）和获取模式的技术（对应动态方法）。
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Compensate: whether the system compensates for discarded elements.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 补偿：系统是否对丢弃的元素进行补偿。
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Require training: whether a sparsity work plug and play without training.'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要训练：稀疏性工作是否需要训练才能插拔使用。
- en: 'The following discussion is organized based on Table [4](#S2.T4 "Table 4 ‣
    2.4.1 Sparse Attention ‣ 2.4 Improve Performance (P) ‣ 2 CAP for LLM Serving ‣
    The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model
    Serving").'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下讨论基于表 [4](#S2.T4 "表 4 ‣ 2.4.1 稀疏注意力 ‣ 2.4 提升性能 (P) ‣ 2 LLM 服务的 CAP 原则 ‣ LLM
    服务的 CAP 原则：长上下文大语言模型服务的调查") 组织。
- en: 'Table 4: The matrix for discussion.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：讨论矩阵。
- en: '|  | Encoder-Decoder | Decoder-only |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 编码器-解码器 | 仅解码器 |'
- en: '| Dynamic Sparsity | (1) | (4) |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 动态稀疏性 | (1) | (4) |'
- en: '| Static Sparsity | (2) | (3) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 静态稀疏性 | (2) | (3) |'
- en: (1) Dynamic Sparsity + Encoder-Decoder. In the pre-LLM period, encoder-decoder
    models dynamically adjust attention patterns at runtime based on input queries
    and keys, including algorithmic works such as Adaptively Sparse Transformer [[51](#bib.bib51)],
    Sinkhorn Attention [[52](#bib.bib52)], Routing transformer [[53](#bib.bib53)],
    Reformer [[54](#bib.bib54)], Landmark attention [[55](#bib.bib55)], and hardware
    accelerator works such as $A^{3}$ [[56](#bib.bib56)], Spatten [[57](#bib.bib57)],
    Sanger [[58](#bib.bib58)], Dota [[59](#bib.bib59)], Salo2 [[50](#bib.bib50)],
    Acceltran [[60](#bib.bib60)], Fact [[61](#bib.bib61)], Energon [[62](#bib.bib62)]
    and Dtqatten [[63](#bib.bib63)], etc. These methods filters out irrelevant tokens
    and generates sparse patterns for crucial attention computation based on input
    or internal states. They adopt various techniques to determine the sparse pattern
    at runtime, such as pruning the attention matrix based on a threshold, identifying
    important keys for queries through clustering, or employing Top-k pruning, among
    others. For example, Routing Transformer utilizes clustering to measure the similarity
    between keys and queries and identifies the Top-k most relevant keys for each
    query. Sanger, Acceltran, and Dtqatten derive sparse patterns by masking out elements
    below a predefined threshold in the approximated score matrix.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 动态稀疏性 + 编码器-解码器。在预LLM时期，编码器-解码器模型会根据输入查询和键在运行时动态调整注意力模式，包括算法工作如适应性稀疏变换器
    [[51](#bib.bib51)]、Sinkhorn 注意力 [[52](#bib.bib52)]、路由变换器 [[53](#bib.bib53)]、变形金刚
    [[54](#bib.bib54)]、地标注意力 [[55](#bib.bib55)]，以及硬件加速器工作如 $A^{3}$ [[56](#bib.bib56)]、Spatten
    [[57](#bib.bib57)]、Sanger [[58](#bib.bib58)]、Dota [[59](#bib.bib59)]、Salo2 [[50](#bib.bib50)]、Acceltran
    [[60](#bib.bib60)]、Fact [[61](#bib.bib61)]、Energon [[62](#bib.bib62)] 和 Dtqatten
    [[63](#bib.bib63)] 等。这些方法根据输入或内部状态过滤掉无关的标记，并为关键注意力计算生成稀疏模式。它们采用各种技术在运行时确定稀疏模式，例如基于阈值修剪注意力矩阵，通过聚类识别查询的重要键，或使用
    Top-k 修剪等。例如，路由变换器利用聚类来衡量键与查询之间的相似性，并为每个查询识别出 Top-k 最相关的键。Sanger、Acceltran 和 Dtqatten
    通过在近似得分矩阵中掩盖低于预定义阈值的元素来推导稀疏模式。
- en: (2) Static Sparsity + Encoder-Decoder. The quadratic complexity of the attention
    mechanism incurs heavy computational and memory burdens, especially when the content
    length is very long. In scenarios with long input sequences, dynamic sparsity
    brings about efficieny issues due to the additional overhead of filtering or clustering
    queries and keys. This gives rise to static sparsity. Models like Block-Bert [[64](#bib.bib64)],
    Sparse transformer [[65](#bib.bib65)], Longformer [[66](#bib.bib66)], BigBird
    [[67](#bib.bib67)], Star-transformer [[68](#bib.bib68)], LongT5 [[69](#bib.bib69)],
    LongNet [[70](#bib.bib70)], Zebra [[71](#bib.bib71)] and certain hardware accelerators
    such as Vitcod [[72](#bib.bib72)] and Salo [[73](#bib.bib73)] adopt static sparsity
    strategies. These works achieve sparsity by constraining attention connections
    to predefined sparse patterns such as block attention, sliding window attention,
    global attention, random attention, and dilated attention. For example, Longformer
    combines sliding window attention and global attention to capture local and long
    dependencies, respectively.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 静态稀疏性 + 编码器-解码器。注意力机制的平方复杂性带来了沉重的计算和内存负担，特别是在内容长度非常长时。在长输入序列的场景中，由于过滤或聚类查询和键的额外开销，动态稀疏性带来了效率问题。这就引出了静态稀疏性。像
    Block-Bert [[64](#bib.bib64)]、Sparse transformer [[65](#bib.bib65)]、Longformer
    [[66](#bib.bib66)]、BigBird [[67](#bib.bib67)]、Star-transformer [[68](#bib.bib68)]、LongT5
    [[69](#bib.bib69)]、LongNet [[70](#bib.bib70)]、Zebra [[71](#bib.bib71)] 和某些硬件加速器如
    Vitcod [[72](#bib.bib72)] 和 Salo [[73](#bib.bib73)] 采用了静态稀疏性策略。这些工作通过将注意力连接约束到预定义的稀疏模式来实现稀疏性，如块注意力、滑动窗口注意力、全局注意力、随机注意力和扩张注意力。例如，Longformer
    结合了滑动窗口注意力和全局注意力，分别捕捉局部和长距离依赖。
- en: (3) Static Sparsity + Decoder-Only. Now in the LLM period, the model of the
    decoder-only architecture is becoming mainstream. In the decoding process of the
    decoder-only transformers, historical keys and values are cached to improve computational
    efficiency, so sparsity now favors evicting unimportant keys and values in the
    KV cache. Static sparsity is still applicable on models with decoder-only architecture.
    For example, LM-Infinite [[74](#bib.bib74)] and StreamingLLM [[12](#bib.bib12)]
    caches the keys and values of the start token and the last $L$ tokens, and only
    the keys and values in the cache will be used for attention with the current query.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 静态稀疏性 + 仅解码器。现在在大语言模型时代，仅解码器架构的模型正在成为主流。在仅解码器变换器的解码过程中，历史键值被缓存以提高计算效率，因此稀疏性现在倾向于驱逐
    KV 缓存中的不重要的键值。静态稀疏性仍然适用于仅解码器架构的模型。例如，LM-Infinite [[74](#bib.bib74)] 和 StreamingLLM
    [[12](#bib.bib12)] 缓存起始令牌和最后 $L$ 个令牌的键值，并且仅缓存中的键值会用于与当前查询的注意力。
- en: (4) Dynamic Sparsity + Decoder-Only. Dynamic sparsity is active again due to
    the linear complexity of the single-step decoding of the decoder-only architecture.
    For example, FastGen [[75](#bib.bib75)] chooses the appropriate compression strategy
    for each attention head in the prefill phase and chooses whether to cache the
    KV vectors of newly generated tokens according to the compression strategy in
    the decoding phase. H2O [[76](#bib.bib76)] and Keyformer [[77](#bib.bib77)] caches
    the key and value vectors of the last $L$ tokens and reloads some relevant evicted
    key and value vectors stored at an external memory by a lookup table.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 动态稀疏性 + 仅解码器。由于仅解码器架构的单步解码具有线性复杂性，动态稀疏性再次被激活。例如，FastGen [[75](#bib.bib75)]
    在预填充阶段为每个注意力头选择适当的压缩策略，并在解码阶段根据压缩策略选择是否缓存新生成令牌的 KV 向量。H2O [[76](#bib.bib76)] 和
    Keyformer [[77](#bib.bib77)] 缓存最后 $L$ 个令牌的键值向量，并通过查找表从外部存储中重新加载一些相关的被驱逐的键值向量。
- en: 'Table 5: Comparing sparsity works.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：稀疏性工作的比较。
- en: '| Work | Sparsity Strategy | Pattern Strategy | Compensate | Require Training
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 稀疏性策略 | 模式策略 | 补偿 | 是否需要训练 |'
- en: '| Sparse Transformers [[65](#bib.bib65)] | Static | Local + dilated | ✗ | ✓
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏变换器 [[65](#bib.bib65)] | 静态 | 局部 + 扩张 | ✗ | ✓ |'
- en: '| Adaptively Transformers [[51](#bib.bib51)] | Dynamic | Topk | ✗ | ✓ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Adaptively Transformers [[51](#bib.bib51)] | 动态 | Topk | ✗ | ✓ |'
- en: '| Block Attention [[64](#bib.bib64)] | Static | Block | ✗ | ✓ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 块注意力 [[64](#bib.bib64)] | 静态 | 块 | ✗ | ✓ |'
- en: '| ETC [[82](#bib.bib82)] | Static | Local + Global | ✗ | ✓ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ETC [[82](#bib.bib82)] | 静态 | 局部 + 全局 | ✗ | ✓ |'
- en: '| BigBird  [[67](#bib.bib67)] | Static | Local + Global + Random | ✗ | ✓ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| BigBird [[67](#bib.bib67)] | 静态 | 局部 + 全局 + 随机 | ✗ | ✓ |'
- en: '| Longformer [[66](#bib.bib66)] | Static | Local + Global | ✗ | ✓ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Longformer [[66](#bib.bib66)] | 静态 | 局部 + 全局 | ✗ | ✓ |'
- en: '| Reformer [[54](#bib.bib54)] | Dynamic | LSH | ✗ | ✓ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Reformer [[54](#bib.bib54)] | 动态 | LSH | ✗ | ✓ |'
- en: '| Sinkhorn Attention [[52](#bib.bib52)] | Dynamic | Block + Sort | ✗ | ✓ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Sinkhorn Attention [[52](#bib.bib52)] | 动态 | 分块 + 排序 | ✗ | ✓ |'
- en: '| Routing Transformer [[53](#bib.bib53)] | Dynamic | Clustering | ✗ | ✓ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Routing Transformer [[53](#bib.bib53)] | 动态 | 聚类 | ✗ | ✓ |'
- en: '| Star Transformer [[68](#bib.bib68)] | Static | Local + Global | ✗ | ✓ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Star Transformer [[68](#bib.bib68)] | 静态 | 局部 + 全局 | ✗ | ✓ |'
- en: '| LongT5 [[69](#bib.bib69)] | Static | Local + Global | ✓ | ✓ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| LongT5 [[69](#bib.bib69)] | 静态 | 局部 + 全局 | ✓ | ✓ |'
- en: '| LongNet [[70](#bib.bib70)] | Static | Dilated | ✗ | ✓ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| LongNet [[70](#bib.bib70)] | 静态 | 膨胀 | ✗ | ✓ |'
- en: '| Zebra [[71](#bib.bib71)] | Static | Local or Global | ✗ | ✓ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Zebra [[71](#bib.bib71)] | 静态 | 局部或全局 | ✗ | ✓ |'
- en: '| Lankmark Attention [[55](#bib.bib55)] | Dynamic | Block + Topk | ✗ | ✓ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Landmark Attention [[55](#bib.bib55)] | 动态 | 分块 + Topk | ✗ | ✓ |'
- en: '| LM-Infinite [[74](#bib.bib74)] | Static | Local + Global | ✗ | ✓ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LM-Infinite [[74](#bib.bib74)] | 静态 | 局部 + 全局 | ✗ | ✓ |'
- en: '| StreamingLLM [[12](#bib.bib12)] | Static | Local + Global | ✗ | ✗ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM [[12](#bib.bib12)] | 静态 | 局部 + 全局 | ✗ | ✗ |'
- en: '| H2O [[76](#bib.bib76)] | Dynamic | Local + Topk | ✗ | ✗ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| H2O [[76](#bib.bib76)] | 动态 | 局部 + Topk | ✗ | ✗ |'
- en: '| Keyformer [[77](#bib.bib77)] | Dynamic | Local + Topk | ✗ | ✗ |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Keyformer [[77](#bib.bib77)] | 动态 | 局部 + Topk | ✗ | ✗ |'
- en: '| SparQ Attention [[78](#bib.bib78)] | Dynamic | Topk | ✓ | ✗ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| SparQ Attention [[78](#bib.bib78)] | 动态 | Topk | ✓ | ✗ |'
- en: '| EasyKV [[79](#bib.bib79)] | Dynamic | Topk | ✗ | ✗ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| EasyKV [[79](#bib.bib79)] | 动态 | Topk | ✗ | ✗ |'
- en: '| LESS [[80](#bib.bib80)] | Dynamic | Topk | ✓ | ✓ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| LESS [[80](#bib.bib80)] | 动态 | Topk | ✓ | ✓ |'
- en: '| InfLLM [[81](#bib.bib81)] | Dynamic | Local + Topk | ✗ | ✗ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| InfLLM [[81](#bib.bib81)] | 动态 | 局部 + Topk | ✗ | ✗ |'
- en: In summary, sparsity improves P by minimizing redundant computation and memory
    usage. Most works in this area only improve P but at the cost of lower potentially
    degraded accuracy. StreamingLLM [[12](#bib.bib12)] is an exception as it achieves
    both CP by enabling an infinite context window and utilizing efficient attention.
    We believe it’d be interesting to explore a combination of model memory, positional
    embedding, and sparsity optimizations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，稀疏性通过最小化冗余计算和内存使用来提高 P。该领域的大多数工作只提高了 P，但以潜在精度下降为代价。StreamingLLM [[12](#bib.bib12)]
    是一个例外，因为它通过实现无限上下文窗口和利用高效注意力实现了 CP。我们认为，探索模型记忆、位置嵌入和稀疏性优化的结合将是有趣的。
- en: 2.4.2 Linear Attention
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2 线性注意力
- en: Linear attention reduces the complexity of the attention mechanism from quadratic
    to linear with respect to the sequence length. It does this by approximating the
    attention calculation through a kernel function that maps the input features into
    a lower-dimensional space before computing the attention scores. Specifically,
    it replaces the softmax operation with other function, e.g., $sim(Q,K)=\phi(Q)\phi(K)^{T}$
    to $\mathbb{R}^{r}$ in Performer) and sparse attention (via locality-sensitive
    hashing in Reformer) leads to efficient approximation with better performance
    than individual ones.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 线性注意力通过将注意力机制的复杂度从与序列长度有关的平方降至线性来减少计算复杂度。它通过核函数近似注意力计算，该函数在计算注意力分数之前将输入特征映射到较低维度的空间。具体而言，它用其他函数替代了
    softmax 操作，例如将 $sim(Q,K)=\phi(Q)\phi(K)^{T}$ 映射到 $\mathbb{R}^{r}$ (在 Performer
    中) 和稀疏注意力（通过 Reformer 中的局部敏感哈希）实现高效近似，表现优于单独的方式。
- en: Both linear attention and sparse attention reduce the quadratic complexity of
    the traditional attention mechanism. Linear attention does so through dimensionality
    reduction, while sparse attention uses selective focusing. Both trade A for a
    better P.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 线性注意力和稀疏注意力都减少了传统注意力机制的平方复杂度。线性注意力通过降维实现这一点，而稀疏注意力则使用选择性关注。两者都以 A 交换以获得更好的 P。
- en: 2.4.3 Distributed Acceleration
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3 分布式加速
- en: Online Normalizer [[88](#bib.bib88)]Memory
    Efficient Attention [[89](#bib.bib89)]Flash Attention [[48](#bib.bib48)]Blockwise Parallel Transformer [[90](#bib.bib90)]Ring Attention [[91](#bib.bib91)]Burst Attention [[92](#bib.bib92)]Striped Attention [[93](#bib.bib93)]Dist Attention [[94](#bib.bib94)]
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Online Normalizer [[88](#bib.bib88)]Memory
    Efficient Attention [[89](#bib.bib89)]Flash Attention [[48](#bib.bib48)]Blockwise Parallel Transformer [[90](#bib.bib90)]Ring Attention [[91](#bib.bib91)]Burst Attention [[92](#bib.bib92)]Striped Attention [[93](#bib.bib93)]Dist Attention [[94](#bib.bib94)]
- en: 'Figure 3: Works using sequence parallelism. Gray boxes are not tailored for
    long-context serving.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用序列并行处理的工作。灰色框不是为长上下文服务而设计的。
- en: '![Refer to caption](img/eca5750b9209b30fe2338dbb5ebf626b.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eca5750b9209b30fe2338dbb5ebf626b.png)'
- en: (a) The workflow for computing a single decoder layer in Ring Attention [[91](#bib.bib91)].
    It efficiently implements SP through block-wise computation and overlapping of
    computation and data transfer.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Ring Attention [[91](#bib.bib91)] 中计算单个解码器层的工作流程。它通过分块计算和计算与数据传输的重叠来高效实现
    SP。
- en: '![Refer to caption](img/56ec595fc4ade55a965d5bc2fd497c98.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/56ec595fc4ade55a965d5bc2fd497c98.png)'
- en: (b) The workflow for computing a single decoder layer in Striped Attention [[93](#bib.bib93)].
    It optimizes Ring Attention by token permutation, which reduces load-imbalance
    among SP nodes caused by causal masking.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Striped Attention [[93](#bib.bib93)] 中计算单个解码器层的工作流程。它通过令牌置换优化 Ring Attention，减少了因因果掩码引起的
    SP 节点负载不均衡。
- en: 'Figure 4: Efficient SP-attention mechanisms used in the prefilling phase of
    LLM serving.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：LLM服务的预填充阶段使用的高效SP-注意力机制。
- en: '![Refer to caption](img/6abd4e48cb2debf7043d50cdcfae0fee.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6abd4e48cb2debf7043d50cdcfae0fee.png)'
- en: 'Figure 5: Dist Attention [[94](#bib.bib94), [95](#bib.bib95)], SP-attention
    mechanism optimized for the auto-regressive decode phase of LLM serving. In the
    decode phase, Q length is one and KV is already distributed.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Dist Attention [[94](#bib.bib94), [95](#bib.bib95)]，为LLM服务的自回归解码阶段优化的SP-注意力机制。在解码阶段，Q长度为一，而KV已经分布。
- en: We discuss works that explore the Sequence Parallelism (SP) dimension in a distributed
    fashion. Here, a long-context inference request is segmented into sub-sequences
    and distributed across nodes for parallel processing. While traditional distributed
    strategies like tensor parallelism (TP) or pipeline parallelism (PP) can also
    enhance inference performance, we omit them in this survey because, they are not
    specifically designed for long-context handling and generally serve as orthogonal
    or complementary to SP optimizations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了以分布式方式探索序列并行（SP）维度的工作。在这里，一个长上下文的推理请求被分割成子序列，并分布到节点上进行并行处理。虽然传统的分布式策略如张量并行（TP）或流水线并行（PP）也可以提高推理性能，但我们在本调查中省略了这些，因为它们并非专门为长上下文处理而设计，通常作为SP优化的正交或补充手段。
- en: Our analysis unfolds in two steps. First, we investigate methods to accelerate
    a single long-context request using SP. Second, we investigate methods to accelerate
    a cluster serving long-context requests.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析分为两个步骤。首先，我们研究了使用SP加速单个长上下文请求的方法。其次，我们研究了加速集群服务长上下文请求的方法。
- en: Accelerate a Single Request.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 加速单个请求。
- en: •
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.4.3 Distributed Acceleration ‣ 2.4 Improve
    Performance (P) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A
    Survey of Long-Context Large Language Model Serving") shows the relation among
    this line of research work. This line of research can be traced back to the online
    normalizer work [[88](#bib.bib88)], a mathematically equivalent method for block-wise
    softmax calculation that avoids materializing the full attention matrix softmax$(QK^{T})$.
    This method is a foundation for memory-efficient attention [[89](#bib.bib89)]
    and their CUDA implementations [[48](#bib.bib48), [96](#bib.bib96)].'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图[3](#S2.F3 "图 3 ‣ 2.4.3 分布式加速 ‣ 2.4 提升性能（P） ‣ 2 LLM服务的CAP ‣ LLM服务的CAP原则：长上下文大语言模型服务的调查")显示了这一研究方向的关系。这一研究方向可以追溯到在线归一化工作[[88](#bib.bib88)]，这是一种数学等效的块状softmax计算方法，避免了实际化完整的注意力矩阵softmax$(QK^{T})$。该方法是内存高效注意力[[89](#bib.bib89)]及其CUDA实现[[48](#bib.bib48),
    [96](#bib.bib96)]的基础。
- en: •
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SP was first introduced by Li et al.[[97](#bib.bib97)] and has been widely
    used in distributed LLM training frameworks such as Megatron[[98](#bib.bib98)]
    and Deepspeed [[99](#bib.bib99)]. In the context of LLM serving systems, new challenges
    emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller
    batch sizes than LLM training; (2) LLM serving has an auto-regressive decode phase,
    where the sequence length is only one, but it requires large memory for KV cache
    storage; (3) LLM serving usually relies on large fused kernels for improving performance.
    While the feed-forward network (FFN) computations for each token in a sequence
    are linearly independent, the computations for attention are not. Consequently,
    substantial data exchange is involved when computing distributed attention using
    SP, thereby opening significant space for performance optimization.'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SP最早由Li等人提出[[97](#bib.bib97)]，并在如Megatron[[98](#bib.bib98)]和Deepspeed [[99](#bib.bib99)]等分布式LLM训练框架中广泛应用。在LLM服务系统的背景下，出现了新的挑战：（1）LLM服务通常对延迟敏感，因此需要比LLM训练更小的批量；（2）LLM服务具有自回归解码阶段，其中序列长度为一，但需要大量内存来存储KV缓存；（3）LLM服务通常依赖于大型融合内核以提高性能。虽然序列中每个令牌的前馈网络（FFN）计算是线性独立的，但注意力的计算却不是。因此，在使用SP计算分布式注意力时涉及大量数据交换，从而为性能优化开辟了重要空间。
- en: •
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Blockwise Parallel Transfomer (BPT) [[90](#bib.bib90)] extends this block-wise
    parallel computation idea from self-attention to a fusion of self-attention and
    FFN. BPT computes FFN directly with each block of Q’s attention result without
    materializing the full attention matrix at all, thus reducing memory demands for
    handling requests with extended contexts.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Blockwise Parallel Transfomer (BPT) [[90](#bib.bib90)] 将块级并行计算的思想从自注意力扩展到自注意力和前馈网络（FFN）的融合。BPT
    直接使用 Q 的每个块的注意力结果计算 FFN，而不需要实现完整的注意力矩阵，从而减少了处理扩展上下文请求时的内存需求。
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ring Attention [[91](#bib.bib91)] is a follow-up work of BPT and adapt it for
    distributed settings. As shown in Figure [4(a)](#S2.F4.sf1 "In Figure 4 ‣ 2.4.3
    Distributed Acceleration ‣ 2.4 Improve Performance (P) ‣ 2 CAP for LLM Serving
    ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model
    Serving"), it distributes blockwise attention and FFN computations across devices,
    enabling the concurrent communication of key-value blocks in a circular pattern
    among hosts. This setup overlaps communication with the computation of query-key-value
    blocks and FFN, enhancing efficiency. Striped Attention [[93](#bib.bib93)] refines
    Ring Attention by addressing the load imbalance among distributed nodes that arises
    after causal masking, as shown in Figure [4(b)](#S2.F4.sf2 "In Figure 4 ‣ 2.4.3
    Distributed Acceleration ‣ 2.4 Improve Performance (P) ‣ 2 CAP for LLM Serving
    ‣ The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model
    Serving"). Burst Attention [[92](#bib.bib92)] enhances Ring Attention by integrating
    FlashAttention’s tiling optimizations into per-node computations and incorporating
    a global optimizer for distributed coordination. Dist Attention [[94](#bib.bib94)]
    optimizes Ring Attention specifically for the auto-regressive decode phase, as
    shown in Figure [5](#S2.F5 "Figure 5 ‣ 2.4.3 Distributed Acceleration ‣ 2.4 Improve
    Performance (P) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving: A
    Survey of Long-Context Large Language Model Serving") where the query length is
    just one. In the decode phase, Q length is one and KV is already distributed among
    sequence parallel nodes.'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Ring Attention [[91](#bib.bib91)] 是 BPT 的后续工作，并将其适配于分布式环境。如图 [4(a)](#S2.F4.sf1
    "在图 4 ‣ 2.4.3 分布式加速 ‣ 2.4 提升性能 (P) ‣ 2 LLM 服务的 CAP ‣ LLM 服务的 CAP 原则：长上下文大语言模型服务的调查")
    所示，它在设备之间分布块级注意力和 FFN 计算，使得关键值块在主机之间以循环模式并发通信。这种设置将通信与查询-关键-值块和 FFN 的计算重叠，提高了效率。Striped
    Attention [[93](#bib.bib93)] 通过解决因因果掩蔽产生的分布式节点负载不平衡问题来优化 Ring Attention，如图 [4(b)](#S2.F4.sf2
    "在图 4 ‣ 2.4.3 分布式加速 ‣ 2.4 提升性能 (P) ‣ 2 LLM 服务的 CAP ‣ LLM 服务的 CAP 原则：长上下文大语言模型服务的调查")
    所示。Burst Attention [[92](#bib.bib92)] 通过将 FlashAttention 的切片优化集成到每个节点的计算中，并结合全局优化器以协调分布式环境，进一步提升了
    Ring Attention 的性能。Dist Attention [[94](#bib.bib94)] 专门针对自回归解码阶段优化了 Ring Attention，如图 [5](#S2.F5
    "图 5 ‣ 2.4.3 分布式加速 ‣ 2.4 提升性能 (P) ‣ 2 LLM 服务的 CAP ‣ LLM 服务的 CAP 原则：长上下文大语言模型服务的调查")
    所示，此时查询长度为 1。在解码阶段，Q 长度为 1，KV 已经在序列并行节点中分布。
- en: Accelerate a Cluster.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 加速集群。
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'When deploying long-context serving, the system encounters requests of varying
    context lengths. This diversity poses significant challenges to the LLM serving
    system, the computational and memory requirements for different requests can vary
    by order of magnitude. Two concurrent works, Infinite-LLM [[94](#bib.bib94)] and
    LoongServe [[95](#bib.bib95)], address this challenge using similar ideas: they
    employ SP to segment requests of different context lengths into smaller, manageable
    pieces and distribute these pieces across the entire cluster for scheduling.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在部署长上下文服务时，系统会遇到不同上下文长度的请求。这种多样性对 LLM 服务系统提出了重大挑战，不同请求的计算和内存需求可能相差几个数量级。两个并行的工作，Infinite-LLM [[94](#bib.bib94)]
    和 LoongServe [[95](#bib.bib95)]，通过类似的思路解决了这一挑战：它们利用 SP 将不同上下文长度的请求分割成较小的、可管理的片段，并将这些片段分配到整个集群进行调度。
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Infinite-LLM [[94](#bib.bib94)] introduces Dist Attention, an SP attention mechanism
    optimized for the auto-regressive decode phase. Additionally, Infinite-LLM incorporates
    a global memory manager that coordinates the cluster’s memory allocation among
    request pieces, taking into account of coherency constraints and fragmentation.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Infinite-LLM [[94](#bib.bib94)] 引入了 Dist Attention，这是一种优化了自回归解码阶段的 SP 注意力机制。此外，Infinite-LLM
    还包含一个全局内存管理器，该管理器协调集群内各请求片段的内存分配，考虑到一致性约束和碎片化问题。
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LoongServe [[95](#bib.bib95)], on the other hand, proposes Elastic Sequence
    Parallelism (ESP) to dynamically adjust the degree of parallelism for an inference
    request with minimal overhead. ESP facilitates two optimization strategies: (1)
    reducing the degree of sequence parallelism after the prefill phase and maintaining
    a lower degree of parallelism during the decode phase, as this phase requires
    less computation (per auto-regressive step); (2) increasing the degree of sequence
    parallelism during the auto-regressive phase as the sequence length grows, which
    is particularly promising when the LLM is expected to generate long output sequences.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LoongServe [[95](#bib.bib95)]提出了弹性序列并行（ESP），以动态调整推理请求的并行度，同时保持最小开销。ESP 促进了两种优化策略：（1）在预填充阶段后减少序列并行度，并在解码阶段保持较低的并行度，因为该阶段的计算量较少（每个自回归步骤）；（2）在自回归阶段，随着序列长度的增长增加序列并行度，当
    LLM 预期生成长输出序列时，这一点尤为重要。
- en: 'In summary, existing works have greatly improved long-context serving’s P,
    either from a single request’s perspective or the cluster’s perspective. We also
    find potential future directions worth exploring. First, although these system
    works are general to long-context models, their optimization approaches have no
    synergy with, or may even contradict the upper-layer model-level optimization.
    For instance, attention mechanisms optimized for load balance among SP nodes may
    perform poorly with context sparsity. Second, as far as we are concerned, no effort
    has been made to the co-design between the agent-layer techniques and the distributed
    acceleration systems. For instance, after the distributed inference of a request,
    its “memory” is scattered among multiple nodes, posing challenges for the agent
    system to collect and filter them. Finally, likewise, no effort examines whether
    we should and how to accelerate model memory line of work (see §[2.2](#S2.SS2
    "2.2 Improve Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP Principle for LLM Serving:
    A Survey of Long-Context Large Language Model Serving")) with SP.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '总之，现有的工作从单个请求的角度或集群的角度极大地改善了长上下文服务的 P。我们还发现了值得探索的潜在未来方向。首先，尽管这些系统工作对长上下文模型是通用的，但它们的优化方法与上层模型级别的优化没有协同作用，甚至可能相互矛盾。例如，为
    SP 节点之间的负载平衡而优化的注意力机制在上下文稀疏的情况下可能表现较差。其次，就我们而言，没有努力进行代理层技术与分布式加速系统之间的协同设计。例如，在请求的分布式推理之后，其“记忆”被分散在多个节点中，这给代理系统收集和过滤这些记忆带来了挑战。最后，同样，没有努力检验我们是否应该以及如何加速模型记忆相关工作（见
    §[2.2](#S2.SS2 "2.2 Improve Context (C) ‣ 2 CAP for LLM Serving ‣ The CAP Principle
    for LLM Serving: A Survey of Long-Context Large Language Model Serving")）与 SP。'
- en: 2.5 Improve Context and Performance (CP)
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 改善上下文和性能（CP）
- en: 'This section discusses works that can improve C and P at the same time. It
    is challenging to hit two birds with one stone and we have identified one line
    of work: prompt compression.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了可以同时改善 C 和 P 的工作。要一箭双雕具有挑战性，我们确定了一项工作方向：提示压缩。
- en: 2.5.1 Prompt Compression
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1 提示压缩
- en: 'Table 6: Comparing prompt compression works.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：比较提示压缩工作。
- en: '| Type | Work |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 工作 |'
- en: '| --- | --- |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Block-Box | Selective Context [[100](#bib.bib100)], LLMLingua [[101](#bib.bib101)],
    LongLLMLingua [[102](#bib.bib102)], LLMLingua2 [[103](#bib.bib103)] |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 | Selective Context [[100](#bib.bib100)], LLMLingua [[101](#bib.bib101)],
    LongLLMLingua [[102](#bib.bib102)], LLMLingua2 [[103](#bib.bib103)] |'
- en: '| White-Box | Gist-Token [[104](#bib.bib104)], PCCC [[105](#bib.bib105)], ICAE [[106](#bib.bib106)],
    AutoCompressor [[32](#bib.bib32)] |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 | Gist-Token [[104](#bib.bib104)], PCCC [[105](#bib.bib105)], ICAE [[106](#bib.bib106)],
    AutoCompressor [[32](#bib.bib32)] |'
- en: Prompt compression reduces the length of a given prompt while preserving the
    essential information such that the serving system can process longer context.
    Recall that we determine whether C and P have been met based on user-perceived
    measurement metrics. We classify this approach as CP because it can shorten the
    user-provided prompt before being fed into the model, thereby improving user-perceived
    context length and performance. We classify works based on whether the LLM model
    is used as a black box or a while box.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 提示压缩在保留关键信息的同时减少给定提示的长度，以便服务系统可以处理更长的上下文。请记住，我们根据用户感知的测量指标来确定是否满足 C 和 P。我们将这种方法归类为
    CP，因为它可以在输入模型之前缩短用户提供的提示，从而改善用户感知的上下文长度和性能。我们根据 LLM 模型是作为黑箱还是白箱来分类这些工作。
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Block-box Compression. LLMLingua [[101](#bib.bib101)] observes significant redundancy
    in natural languages and proposes a set of methods for compressing prompts by
    removing tokens. It uses a token-level iterative algorithm to compress prompts.
    Doing so can preserve the key information within the prompt by considering the
    conditional dependencies between tokens. LongLLMLingua [[102](#bib.bib102)] is
    built based on LLMLingua, adding question-awareness compression by adding contrastive
    perplexity which captures the distribution shift of tokens w.r.t. questions. LLMLingua2 [[103](#bib.bib103)]
    takes a step further, it targets task-agnostic prompt compression. It uses GPT-4
    to generate compressed texts from original prompts and a bi-class classifier to
    drop unneeded tokens.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 黑箱压缩。LLMLingua [[101](#bib.bib101)] 观察到自然语言中存在显著的冗余，并提出了一系列通过移除令牌来压缩提示的方法。它使用令牌级迭代算法来压缩提示。这样做可以通过考虑令牌之间的条件依赖关系来保留提示中的关键信息。LongLLMLingua
    [[102](#bib.bib102)] 基于LLMLingua构建，增加了通过对比困惑度的问答意识压缩，以捕捉令牌相对于问题的分布变化。LLMLingua2
    [[103](#bib.bib103)] 更进一步，它目标是任务无关的提示压缩。它使用GPT-4从原始提示生成压缩文本，并使用双类分类器丢弃不需要的令牌。
- en: •
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'White-box Compression. This line of work will modify the model architecture
    in certain ways to achieve compression. And they feed the compressed prompts via
    soft prompting [[35](#bib.bib35)]. Gist tokens [[104](#bib.bib104)] modifies the
    transformer attention masks to enable an LLM to compress prompts into smaller
    sets of “gist” tokens which can be cached and reused for compute efficiency, improving
    both C and P. Another work, PCCC [[105](#bib.bib105)], adds a trainable soft prompt
    weight to an LLM. Their insight is that prompts used to condition a LLM can be
    approximately represented by a much smaller set of carefully chosen weights. Their
    goal is to train the soft prompt weights to mimic a fixed hard prompt as closely
    as possible. ICAE [[106](#bib.bib106)] takes a different approach. It consists
    of 2 modules: a learnable encoder adapted from the LLM with LoRA for encoding
    a long context into a small number of memory slots, and a fixed decoder, which
    is the LLM itself where the memory slots representing the original context are
    conditioned on to interact with prompts to accomplish various goals. Finally,
    AutoCompressor [[32](#bib.bib32)], a work based on the RMT architecture [[31](#bib.bib31)],
    builds a segment-level summary token to compress prompts.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 白箱压缩。这项工作将以某种方式修改模型架构以实现压缩。然后他们通过软提示 [[35](#bib.bib35)] 输入压缩提示。Gist tokens [[104](#bib.bib104)]
    修改了变压器注意力掩码，使LLM能够将提示压缩成更小的一组“要点”令牌，这些令牌可以缓存并重用以提高计算效率，从而改善C和P。另一项工作，PCCC [[105](#bib.bib105)]，在LLM中添加了一个可训练的软提示权重。他们的见解是，条件化LLM的提示可以通过一组精心挑选的权重来大致表示。他们的目标是训练软提示权重，使其尽可能接近固定的硬提示。ICAE
    [[106](#bib.bib106)] 采取了不同的方法。它由两个模块组成：一个适应了LLM的可学习编码器，用于将长上下文编码到少量记忆槽中，和一个固定解码器，即LLM本身，其中代表原始上下文的记忆槽用于与提示交互以实现各种目标。最后，AutoCompressor
    [[32](#bib.bib32)]，一个基于RMT架构 [[31](#bib.bib31)] 的工作，构建了一个段级摘要令牌以压缩提示。
- en: In summary, there are various ways to compress prompts. One can either treat
    the LLM as a black-box and use a set of methods to compress prompts before being
    sent to the black-box LLM. Alternatively, one can also modify the model architecture
    to achieve effective compression. Prompt compression improves user-perceived C
    and P.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，有多种方法可以压缩提示。可以将LLM视为黑箱，并使用一系列方法在发送到黑箱LLM之前压缩提示。或者，也可以修改模型架构以实现有效压缩。提示压缩提高了用户感知的C和P。
- en: 2.6 Improve Context and Accuracy (CA)
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 提高上下文和准确性（CA）
- en: 'This section discusses works that can improve C and A at the same time. We
    have identified one line of work: agent memory, which manages memory at the agent
    layer.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了可以同时改善C和A的工作。我们已确定了一项工作：代理记忆，它在代理层管理记忆。
- en: 2.6.1 Agent Memory
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1 代理记忆
- en: One way to extend a serving system’s context length and performance is by implicitly
    managing the memory and prompt within the agent layer. We dub this approach as
    agent memory. It belongs to CA because it can create the illusion of infinite
    context over fixed-context models and reflect on past memory for higher accuracy
    in future tasks, thereby improving user-perceived C and A. Agent memory differs
    from model memory (covered earlier) in that agent memory manipulates memory and
    prompts within agents, while model memory manipulates memory within models. They
    are not conflicting solutions, they complement each other. For example, one can
    run an agent memory work such as MemGPT [[107](#bib.bib107)] atop a model memory
    work such as Infini-Attention [[33](#bib.bib33)].
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展服务系统的上下文长度和性能的一种方法是通过隐式管理代理层中的记忆和提示。我们称这种方法为代理记忆。它属于CA，因为它可以在固定上下文模型上创造无限上下文的幻觉，并对过去的记忆进行反思，以便在未来任务中提高准确性，从而提升用户感知的C和A。代理记忆不同于模型记忆（前面讨论过），代理记忆在代理内操作记忆和提示，而模型记忆在模型内操作记忆。它们不是相互冲突的解决方案，而是互补的。例如，可以在如Infini-Attention [[33](#bib.bib33)]这样的模型记忆工作上运行代理记忆工作，如MemGPT [[107](#bib.bib107)]。
- en: 'Table 7: Comparing agent memory works.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：比较代理记忆工作的情况。
- en: '| Work | Online Memory Management | Offline Memory Reflection |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 工作 | 在线记忆管理 | 离线记忆反思 |'
- en: '| --- | --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| MemWalker [[108](#bib.bib108)] | ✓ | / |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| MemWalker [[108](#bib.bib108)] | ✓ | / |'
- en: '| WebGPT [[109](#bib.bib109)] | ✓ | / |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| WebGPT [[109](#bib.bib109)] | ✓ | / |'
- en: '| MemGPT [[107](#bib.bib107)] | ✓ | / |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| MemGPT [[107](#bib.bib107)] | ✓ | / |'
- en: '| TheSim [[110](#bib.bib110)] | ✓ | ✓ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| TheSim [[110](#bib.bib110)] | ✓ | ✓ |'
- en: '| ChatDev [[111](#bib.bib111)] | ✓ | ✓ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| ChatDev [[111](#bib.bib111)] | ✓ | ✓ |'
- en: '| MetaGPT [[112](#bib.bib112)] | ✓ | ✓ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| MetaGPT [[112](#bib.bib112)] | ✓ | ✓ |'
- en: '| Self-Refine [[113](#bib.bib113)] | ✓ | ✓ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 自我精炼 [[113](#bib.bib113)] | ✓ | ✓ |'
- en: '| Reflexion [[114](#bib.bib114)] | ✓ | ✓ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [[114](#bib.bib114)] | ✓ | ✓ |'
- en: '| MLCopilot [[115](#bib.bib115)] | ✓ | ✓ |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| MLCopilot [[115](#bib.bib115)] | ✓ | ✓ |'
- en: We discuss agent memory work across two dimensions.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从两个维度讨论代理记忆工作。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Online Memory Management: it indicates whether a solution can dynamically construct
    a prompt being fed to the model in real-time based on the agent’s past memory,
    external knowledge, and current user prompt. It requires mechanisms to fetch relevant
    information from past memory and mechanisms to construct prompts. MemWalker [[108](#bib.bib108)],
    WebGPT [[109](#bib.bib109)], and MemGPT [[107](#bib.bib107)] are seminal works
    in this space. In particular, MemGPT provides the illusion of an infinite context
    atop a fixed-context model. It builds a multi-level hierarchy and a set of mechanisms
    to swap memory between the current constructed prompt and external past memory.
    Hence, it implicitly improves C.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在线记忆管理：这表明一个解决方案是否能够根据代理的过去记忆、外部知识和当前用户提示，实时动态构建传递给模型的提示。它需要从过去记忆中获取相关信息的机制和构建提示的机制。MemWalker [[108](#bib.bib108)]、WebGPT [[109](#bib.bib109)]
    和MemGPT [[107](#bib.bib107)]是这一领域的开创性工作。特别是，MemGPT在固定上下文模型之上提供了无限上下文的幻觉。它建立了一个多层次的层级结构和一套机制，在当前构建的提示与外部过去记忆之间交换记忆。因此，它隐式提高了C。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Offline Memory Reflection: it indicates whether a solution can reflect on an
    agent’s past memory to learn experiences, distill knowledge, remove unnecessary
    sentences, etc. It requires mechanisms to read and write past memory. Many agent-based
    applications adopt this mechanism offline to improve serving accuracy for future
    tasks [[114](#bib.bib114), [113](#bib.bib113)]. For example, agents in ChatDev [[111](#bib.bib111)],
    Generative Agents [[110](#bib.bib110)], and MLCopilot [[115](#bib.bib115)] regularly
    reflect, which synthesizes past memories into higher-level knowledge to improve
    future task accuracy. Combined, agent memory with feature improves C and A.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 离线记忆反思：这表明一个解决方案是否可以反思代理的过去记忆以学习经验、提炼知识、删除不必要的句子等。它需要读取和写入过去记忆的机制。许多基于代理的应用离线采用这种机制，以提高未来任务的服务准确性 [[114](#bib.bib114),
    [113](#bib.bib113)]。例如，ChatDev [[111](#bib.bib111)]、Generative Agents [[110](#bib.bib110)]
    和MLCopilot [[115](#bib.bib115)]中的代理会定期进行反思，将过去的记忆综合成更高级的知识，以提高未来任务的准确性。综合来看，具有这种特征的代理记忆提高了C和A。
- en: 'In summary, agent memory has three key features: online memory management and
    offline memory reflection. The former meets C, and the latter meets A. Agent memory
    comes close to CAP if one adds prompt compression to it.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，代理记忆有三个关键特征：在线记忆管理和离线记忆反思。前者符合C，后者符合A。如果在其中加入提示压缩，代理记忆接近CAP。
- en: 3 Conclusion
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 结论
- en: 'We believe it’s equally important to deploy and serve models at a massive scale
    with a reasonably low cost without compromising its accuracy, in addition to having
    a high-quality model. we survey the LLM serving area to understand the intricate
    dynamics between cost-efficiency and accuracy with the growing need for long-context
    serving. Our findings reveal that works in this space optimize along three distinct
    but conflicting goals: improving serving context length (C), improving serving
    accuracy (A), and improving serving performance (P). We propose the CAP principle,
    which states that any given LLM serving optimization can only improve at most
    two of the above three goals. We closely examine the related literature and find
    existing works can fall into this category. Looking forward, we hope this principle
    is used to inform designers of the inherent and dynamic trade-offs when building
    large-scale serving systems.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，在拥有高质量模型的基础上，将模型以合理低成本的大规模部署和服务同样重要，而不妥协于其准确性。我们调查了LLM服务领域，以了解在不断增长的长上下文服务需求下，成本效益与准确性之间的复杂动态。我们的研究发现，这一领域的工作在以下三个不同但相互冲突的目标之间进行优化：改善服务上下文长度（C）、改善服务准确性（A）和改善服务性能（P）。我们提出了CAP原则，该原则指出，任何给定的LLM服务优化只能在上述三个目标中的最多两个方面有所改进。我们仔细审查了相关文献，发现现有工作符合这一类别。展望未来，我们希望这一原则能为设计师在构建大规模服务系统时提供固有且动态的权衡参考。
- en: References
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin,
    Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi:
    Operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462,
    2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin,
    Allan Dafoe, Aleksandra Faust, Clement Farabet 和 Shane Legg. AGI的层级：在AGI路径上操作化进展。arXiv预印本
    arXiv:2311.02462，2023年。'
- en: '[2] THE AI INDEX REPORT. [https://aiindex.stanford.edu/report/](https://aiindex.stanford.edu/report/).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] THE AI INDEX REPORT. [https://aiindex.stanford.edu/report/](https://aiindex.stanford.edu/report/)。'
- en: '[3] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang,
    Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et al. A survey of resource-efficient
    llm and multimodal foundation models. arXiv preprint arXiv:2401.08092, 2024.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang,
    Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang 等. 资源高效的LLM和多模态基础模型调查。arXiv预印本
    arXiv:2401.08092，2024年。'
- en: '[4] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming
    Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. A survey on efficient inference
    for large language models. arXiv preprint arXiv:2404.14294, 2024.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming
    Lou, Luning Wang, Zhihang Yuan, Xiuhong Li 等. 大型语言模型高效推断调查。arXiv预印本 arXiv:2404.14294，2024年。'
- en: '[5] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery
    Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities
    of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery
    Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi 等. Gemini模型在医学中的能力。arXiv预印本
    arXiv:2404.18416，2024年。'
- en: '[6] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
    Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling
    transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
    Jonathan Heek, Kefan Xiao, Shivani Agrawal 和 Jeff Dean. 高效地扩展变换器推断。机器学习与系统会议录，5，2023年。'
- en: '[7] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long
    contexts. Transactions of the Association for Computational Linguistics, 12:157–173,
    2024.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni 和 Percy Liang. 迷失在中间：语言模型如何使用长上下文。计算语言学协会会刊，12:157–173，2024年。'
- en: '[8] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
    large language model serving with pagedattention. In Proceedings of the 29th Symposium
    on Operating Systems Principles, pages 611–626, 2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody
    Hao Yu, Joseph Gonzalez, Hao Zhang 和 Ion Stoica. 带有分页注意力的大型语言模型服务的高效内存管理。在第29届操作系统原理研讨会论文集中，第611–626页，2023年。'
- en: '[9] TensorRT LLM. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] TensorRT LLM. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)。'
- en: '[10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and
    Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length
    context. arXiv preprint arXiv:1901.02860, 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, 和 Ruslan
    Salakhutdinov。 Transformer-XL：超越固定长度上下文的关注语言模型。 arXiv 预印本 arXiv:1901.02860, 2019。'
- en: '[11] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit
    quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102,
    2023.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng,
    Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, 和 Baris Kasikci。 Atom：低比特量化的高效准确
    LLM 服务。 arXiv 预印本 arXiv:2310.19102, 2023。'
- en: '[12] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks, 2023.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis。 具有注意力汇的高效流式语言模型，2023。'
- en: '[13] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon
    Chun. Orca: A distributed serving system for $\{$ generative models. In 16th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521–538,
    2022.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, 和 Byung-Gon
    Chun。 Orca：一种用于$\{$生成模型的分布式服务系统。在第16届 USENIX 操作系统设计与实现研讨会 (OSDI 22) 上，第521–538页，2022。'
- en: '[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang
    Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference:
    Disaggregate llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181,
    2024.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang
    Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, 等。 无干扰推理：针对混合下游工作负载的解聚 LLM
    推理。 arXiv 预印本 arXiv:2401.11181, 2024。'
- en: '[15] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather
    Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao,
    and Ali Ghodsi. The shift from models to compound ai systems. [https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/),
    2024.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather
    Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao,
    和 Ali Ghodsi。 从模型到复合 AI 系统的转变。 [https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)，2024。'
- en: '[16] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
    transformer with rotary position embedding. CoRR, abs/2104.09864, 2021.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, 和 Yunfeng Liu。 Roformer：带旋转位置嵌入的增强型
    Transformer。 CoRR, abs/2104.09864, 2021。'
- en: '[17] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model
    compression for large language models. arXiv preprint arXiv:2308.07633, 2023.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang。 关于大规模语言模型的模型压缩调查。
    arXiv 预印本 arXiv:2308.07633, 2023。'
- en: '[18] Wikipedia. The cap theorem. [https://en.wikipedia.org/wiki/CAP_theorem](https://en.wikipedia.org/wiki/CAP_theorem),
    2024.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Wikipedia。 CAP 定理。 [https://en.wikipedia.org/wiki/CAP_theorem](https://en.wikipedia.org/wiki/CAP_theorem)，2024。'
- en: '[19] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent
    abilities of language models from the loss perspective. arXiv preprint arXiv:2403.15796,
    2024.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Zhengxiao Du, Aohan Zeng, Yuxiao Dong, 和 Jie Tang。 从损失角度理解语言模型的突现能力。 arXiv
    预印本 arXiv:2403.15796, 2024。'
- en: '[20] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities
    of large language models a mirage? Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Rylan Schaeffer, Brando Miranda, 和 Sanmi Koyejo。 大型语言模型的突现能力是否是海市蜃楼？ 神经信息处理系统进展，36，2024。'
- en: '[21] Google. Spanner, truetime & the cap theorem. [https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/45855.pdf](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/45855.pdf),
    2017.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Google。 Spanner、真实时间和 CAP 定理。 [https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/45855.pdf](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/45855.pdf)，2017。'
- en: '[22] Saurav Pawar, SM Tonmoy, SM Zaman, Vinija Jain, Aman Chadha, and Amitava
    Das. The what, why, and how of context length extension techniques in large language
    models–a detailed survey. arXiv preprint arXiv:2401.07872, 2024.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Saurav Pawar, SM Tonmoy, SM Zaman, Vinija Jain, Aman Chadha, 和 Amitava
    Das。 大型语言模型中上下文长度扩展技术的什么、为什么以及如何——详细调查。 arXiv 预印本 arXiv:2401.07872, 2024。'
- en: '[23] Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long
    text modeling with transformers. arXiv preprint arXiv:2302.14502, 2023.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Zican Dong, Tianyi Tang, Lunyi Li, 和 Wayne Xin Zhao。 关于使用 Transformer
    进行长文本建模的调查。 arXiv 预印本 arXiv:2302.14502, 2023。'
- en: '[24] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh,
    and Armaghan Eshaghi. Beyond the limits: A survey of techniques to extend the
    context length in large language models. arXiv preprint arXiv:2402.02244, 2024.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh,
    和 Armaghan Eshaghi. 超越极限：扩展大语言模型上下文长度的技术调查。arXiv 预印本 arXiv:2402.02244，2024年。'
- en: '[25] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
    language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou 等人. 大语言模型基础代理的兴起与潜力：一项调查。arXiv 预印本 arXiv:2309.07864，2023年。'
- en: '[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
    Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large
    language models: A survey. arXiv preprint arXiv:2312.10997, 2023.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
    Yi Dai, Jiawei Sun, 和 Haofen Wang. 用于大语言模型的检索增强生成：一项调查。arXiv 预印本 arXiv:2312.10997，2023年。'
- en: '[27] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap.
    Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507,
    2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, 和 Timothy P Lillicrap.
    长距离序列建模的压缩变换器。arXiv 预印本 arXiv:1911.05507，2019年。'
- en: '[28] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing
    transformers. arXiv preprint arXiv:2203.08913, 2022.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, 和 Christian Szegedy. 记忆变换器。arXiv
    预印本 arXiv:2203.08913，2022年。'
- en: '[29] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and
    Zhou Yu. Memformer: A memory-augmented transformer for sequence modeling. arXiv
    preprint arXiv:2010.06891, 2020.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, 和 Zhou
    Yu. Memformer: 一种用于序列建模的记忆增强变换器。arXiv 预印本 arXiv:2010.06891，2020年。'
- en: '[30] Mikhail S Burtsev, Yuri Kuratov, Anton Peganov, and Grigory V Sapunov.
    Memory transformer. arXiv preprint arXiv:2006.11527, 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Mikhail S Burtsev, Yuri Kuratov, Anton Peganov, 和 Grigory V Sapunov. 记忆变换器。arXiv
    预印本 arXiv:2006.11527，2020年。'
- en: '[31] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.
    Advances in Neural Information Processing Systems, 35:11079–11091, 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Aydar Bulatov, Yury Kuratov, 和 Mikhail Burtsev. 循环记忆变换器。神经信息处理系统进展，35:11079–11091，2022年。'
- en: '[32] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting
    language models to compress contexts. arXiv preprint arXiv:2305.14788, 2023.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, 和 Danqi Chen. 适应语言模型以压缩上下文。arXiv
    预印本 arXiv:2305.14788，2023年。'
- en: '[33] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context
    behind: Efficient infinite context transformers with infini-attention. arXiv preprint
    arXiv:2404.07143, 2024.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Tsendsuren Munkhdalai, Manaal Faruqui, 和 Siddharth Gopal. 不留任何上下文：具有无限注意力的高效无限上下文变换器。arXiv
    预印本 arXiv:2404.07143，2024年。'
- en: '[34] Yizhou Shan, Yutong Huang, Yilun Chen, and Yiying Zhang. LegoOS: A disseminated,
    distributed OS for hardware resource disaggregation. In 13th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 18), pages 69–87, Carlsbad,
    CA, October 2018\. USENIX Association.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Yizhou Shan, Yutong Huang, Yilun Chen, 和 Yiying Zhang. LegoOS: 一种分布式硬件资源解聚的操作系统。第13届USENIX操作系统设计与实现研讨会（OSDI
    18），第69–87页，加州卡尔斯巴德，2018年10月。USENIX协会。'
- en: '[35] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for
    parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Brian Lester, Rami Al-Rfou, 和 Noah Constant. 规模的力量用于参数高效的提示调优。arXiv 预印本
    arXiv:2104.08691，2021年。'
- en: '[36] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention
    with linear biases enables input length extrapolation, 2022.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ofir Press, Noah A. Smith, 和 Mike Lewis. 短期训练，长期测试：线性偏差的注意力实现输入长度外推，2022年。'
- en: '[37] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
    Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer,
    2022.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,
    Vishrav Chaudhary, Xia Song, 和 Furu Wei. 一种长度可外推的变换器，2022年。'
- en: '[38] Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing.
    Clex: Continuous length extrapolation for large language models, 2024.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, 和 Lidong Bing.
    Clex: 大语言模型的连续长度外推，2024年。'
- en: '[39] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending
    context window of large language models via positional interpolation, 2023.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian. 通过位置插值扩展大语言模型的上下文窗口，2023年。'
- en: '[40] bloc97. Ntk-aware scaled rope allows llama models to have extended (8k+)
    context size without any fine-tuning and minimal perplexity degradation, 2023.
    [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/,D](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/,D),
    Last accessed on 2023-12-19.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] bloc97. Ntk感知的缩放绳索允许llama模型在不进行任何微调和最小困惑度下降的情况下具有扩展的 (8k+) 上下文大小，2023。
    [https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/,D](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/,D)，最后访问时间为2023-12-19。'
- en: '[41] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn:
    Efficient context window extension of large language models, 2023.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, 和 Enrico Shippole. Yarn: 高效的上下文窗口扩展大型语言模型，2023。'
- en: '[42] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon,
    Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.
    Functional interpolation for relative positions improves long context transformers,
    2024.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon,
    Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, 和 Srinadh Bhojanapalli.
    功能插值相对位置改善长上下文变换器，2024。'
- en: '[43] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang,
    Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond
    2 million tokens, 2024.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang,
    Jiahang Xu, Fan Yang, 和 Mao Yang. Longrope: 扩展llm上下文窗口超过200万标记，2024。'
- en: '[44] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and
    Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise
    training, 2024.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, 和 Sujian
    Li. Pose: 通过位置跳过训练高效扩展llms的上下文窗口，2024。'
- en: '[45] Alexander Peysakhovich and Adam Lerer. Attention sorting combats recency
    bias in long context language models. arXiv preprint arXiv:2310.01427, 2023.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Alexander Peysakhovich 和 Adam Lerer. 注意力排序对抗长期上下文语言模型中的近期偏差。arXiv 预印本
    arXiv:2310.01427, 2023。'
- en: '[46] Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang,
    Yongbin Li, and Rui Yan. Fortify the shortest stave in attention: Enhancing context
    awareness of large language models for effective tool use. arXiv preprint arXiv:2312.04455,
    2023.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang,
    Yongbin Li, 和 Rui Yan. 加固注意力中的最短音符：提升大型语言模型的上下文意识以有效使用工具。arXiv 预印本 arXiv:2312.04455,
    2023。'
- en: '[47] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi
    Chen, Xiaoxia Wu, and Zhangyang Wang. Found in the middle: How language models
    use long contexts better via plug-and-play positional encoding. arXiv preprint
    arXiv:2403.04797, 2024.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi
    Chen, Xiaoxia Wu, 和 Zhangyang Wang. 在中间发现：语言模型如何通过即插即用的位置信息编码更好地利用长上下文。arXiv 预印本
    arXiv:2403.04797, 2024。'
- en: '[48] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. Advances in Neural
    Information Processing Systems, 35:16344–16359, 2022.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, 和 Christopher Ré. Flashattention:
    快速且内存高效的精确注意力机制与io感知。神经信息处理系统进展，35:16344–16359, 2022。'
- en: '[49] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao
    Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, et al. Efficiently
    programming large language models using sglang. arXiv preprint arXiv:2312.07104,
    2023.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody
    Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, 等. 使用sglang高效编程大型语言模型。arXiv
    预印本 arXiv:2312.07104, 2023。'
- en: '[50] Jieru Zhao, Pai Zeng, Guan Shen, Quan Chen, and Minyi Guo. Hardware-software
    co-design enabling static and dynamic sparse attention mechanisms. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems, pages 1–1, 2024.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Jieru Zhao, Pai Zeng, Guan Shen, Quan Chen, 和 Minyi Guo. 硬件-软件协同设计实现静态和动态稀疏注意力机制。IEEE集成电路和系统计算机辅助设计学报，页码1–1,
    2024。'
- en: '[51] Gonçalo M. Correia, Vlad Niculae, and André F. T. Martins. Adaptively
    sparse transformers, 2019.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Gonçalo M. Correia, Vlad Niculae, 和 André F. T. Martins. 自适应稀疏变换器，2019。'
- en: '[52] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse
    sinkhorn attention, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, 和 Da-Cheng Juan. 稀疏Sinkhorn注意力，2020。'
- en: '[53] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient
    content-based sparse attention with routing transformers. Transactions of the
    Association for Computational Linguistics, 9:53–68, 2021.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Aurko Roy, Mohammad Saffar, Ashish Vaswani, 和 David Grangier. 基于内容的稀疏注意力与路由变换器的高效实现。计算语言学协会会刊，9:53–68,
    2021。'
- en: '[54] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient
    transformer, 2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Nikita Kitaev、Łukasz Kaiser 和 Anselm Levskaya。Reformer: 高效的变压器，2020年。'
- en: '[55] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access
    infinite context length for transformers, 2023.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Amirkeivan Mohtashami 和 Martin Jaggi。Landmark attention: 变压器的随机访问无限上下文长度，2023年。'
- en: '[56] Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H. Oh, Yeonhong Park,
    Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W. Lee, and Deog-Kyoon
    Jeong. A³: Accelerating attention mechanisms in neural networks with approximation,
    2020.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Tae Jun Ham、Sung Jun Jung、Seonghak Kim、Young H. Oh、Yeonhong Park、Yoonho
    Song、Jung-Hun Park、Sanghee Lee、Kyoung Park、Jae W. Lee 和 Deog-Kyoon Jeong。A³: 通过近似加速神经网络中的注意力机制，2020年。'
- en: '[57] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention
    architecture with cascade token and head pruning, 2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Hanrui Wang、Zhekai Zhang 和 Song Han。Spatten: 具有级联令牌和头部剪枝的高效稀疏注意力架构，2021年。'
- en: '[58] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and
    Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable
    architecture. MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture,
    2021.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Liqiang Lu、Yicheng Jin、Hangrui Bi、Zizhang Luo、Peng Li、Tao Wang 和 Yun Liang。Sanger:
    一个用于通过可重构架构实现稀疏注意力的共同设计框架。MICRO-54: 第54届IEEE/ACM国际微体系结构研讨会，2021年。'
- en: '[59] Zheng Qu, L. Liu, Fengbin Tu, Zhaodong Chen, Yufei Ding, and Yuan Xie.
    Dota: detect and omit weak attentions for scalable transformer acceleration. Proceedings
    of the 27th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems, 2022.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Zheng Qu、L. Liu、Fengbin Tu、Zhaodong Chen、Yufei Ding 和 Yuan Xie。Dota: 检测和省略弱注意力以实现可扩展的变压器加速。第27届ACM国际编程语言和操作系统架构支持会议论文集，2022年。'
- en: '[60] Shikhar Tuli and Niraj K. Jha. Acceltran: A sparsity-aware accelerator
    for dynamic inference with transformers, 2023.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Shikhar Tuli 和 Niraj K. Jha。Acceltran: 一个关注稀疏性的加速器，用于变压器的动态推理，2023年。'
- en: '[61] Yubin Qin, Yang Wang, Dazheng Deng, Zhiren Zhao, Xiaolong Yang, Leibo
    Liu, Shaojun Wei, Yang Hu, and Shouyi Yin. Fact: Ffn-attention co-optimized transformer
    architecture with eager correlation prediction. Proceedings of the 50th Annual
    International Symposium on Computer Architecture, 2023.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Yubin Qin、Yang Wang、Dazheng Deng、Zhiren Zhao、Xiaolong Yang、Leibo Liu、Shaojun
    Wei、Yang Hu 和 Shouyi Yin。Fact: Ffn-attention共同优化的变压器架构与急切相关预测。第50届国际计算机架构年会论文集，2023年。'
- en: '[62] Zhe Zhou, Junlin Liu, Zhenyu Gu, and Guangyu Sun. Energon: Toward efficient
    acceleration of transformers using dynamic sparse attention. IEEE Transactions
    on Computer-Aided Design of Integrated Circuits and Systems, 42(1):136–149, 2023.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Zhe Zhou、Junlin Liu、Zhenyu Gu 和 Guangyu Sun。Energon: 通过动态稀疏注意力实现变压器的高效加速。IEEE计算机辅助设计集成电路和系统汇刊，42(1):136–149，2023年。'
- en: '[63] Tao Yang, Dongyue Li, Zhuoran Song, Yilong Zhao, Fangxin Liu, Zongwu Wang,
    Zhezhi He, and Li Jiang. Dtqatten: Leveraging dynamic token-based quantization
    for efficient attention architecture. In 2022 Design, Automation & Test in Europe
    Conference & Exhibition (DATE), pages 700–705, 2022.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Tao Yang、Dongyue Li、Zhuoran Song、Yilong Zhao、Fangxin Liu、Zongwu Wang、Zhezhi
    He 和 Li Jiang。Dtqatten: 利用动态基于令牌的量化实现高效的注意力架构。在2022年设计、自动化与测试欧洲会议及展览（DATE），第700–705页，2022年。'
- en: '[64] Jiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong Wang, and Jie Tang.
    Blockwise self-attention for long document understanding. ArXiv, abs/1911.02972,
    2019.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Jiezhong Qiu、Hao Ma、Omer Levy、Scott Yih、Sinong Wang 和 Jie Tang。用于长文档理解的块级自注意力。ArXiv，abs/1911.02972，2019年。'
- en: '[65] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating
    long sequences with sparse transformers, 2019.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。使用稀疏变压器生成长序列，2019年。'
- en: '[66] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document
    transformer, 2020.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Iz Beltagy、Matthew E. Peters 和 Arman Cohan。Longformer: 长文档变压器，2020年。'
- en: '[67] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris
    Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and
    Amr Ahmed. Big bird: Transformers for longer sequences. In H. Larochelle, M. Ranzato,
    R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing
    Systems, volume 33, pages 17283–17297\. Curran Associates, Inc., 2020.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Manzil Zaheer、Guru Guruganesh、Kumar Avinava Dubey、Joshua Ainslie、Chris
    Alberti、Santiago Ontanon、Philip Pham、Anirudh Ravula、Qifan Wang、Li Yang 和 Amr Ahmed。Big
    bird: 用于更长序列的变压器。在H. Larochelle、M. Ranzato、R. Hadsell、M.F. Balcan 和 H. Lin 编辑的《神经信息处理系统进展》卷33，第17283–17297页。Curran
    Associates, Inc.，2020年。'
- en: '[68] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng
    Zhang. Star-transformer, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, 和 Zheng
    Zhang。Star-transformer，2022年。'
- en: '[69] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan
    Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences.
    In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors,
    Findings of the Association for Computational Linguistics: NAACL 2022, pages 724–736,
    Seattle, United States, July 2022\. Association for Computational Linguistics.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan
    Sung, 和 Yinfei Yang。LongT5: 高效的文本到文本变换器用于长序列。收录于 Marine Carpuat, Marie-Catherine
    de Marneffe, 和 Ivan Vladimir Meza Ruiz 主编的《计算语言学协会会议记录: NAACL 2022》，第 724-736
    页，美国西雅图，2022年7月。计算语言学协会。'
- en: '[70] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens, 2023.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, 和 Furu Wei。Longnet: 将变换器扩展到 1,000,000,000 个令牌，2023年。'
- en: '[71] Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, and Dong Yu. Zebra:
    Extending context window with layerwise grouped local-global attention, 2023.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, 和 Dong Yu。Zebra:
    通过层级分组的本地-全局注意力扩展上下文窗口，2023年。'
- en: '[72] Haoran You, Zhanyi Sun, Huihong Shi, Zhongzhi Yu, Yang Zhao, Yongan Zhang,
    Chaojian Li, Baopu Li, and Yingyan Lin. Vitcod: Vision transformer acceleration
    via dedicated algorithm and accelerator co-design, 2022.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Haoran You, Zhanyi Sun, Huihong Shi, Zhongzhi Yu, Yang Zhao, Yongan Zhang,
    Chaojian Li, Baopu Li, 和 Yingyan Lin。Vitcod: 通过专用算法和加速器共同设计加速视觉变换器，2022年。'
- en: '[73] Guan Shen, Jieru Zhao, Quan Chen, Jingwen Leng, Chao Li, and Minyi Guo.
    Salo: An efficient spatial accelerator enabling hybrid sparse attention mechanisms
    for long sequences, 2022.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Guan Shen, Jieru Zhao, Quan Chen, Jingwen Leng, Chao Li, 和 Minyi Guo。Salo:
    一种高效的空间加速器，支持长序列的混合稀疏注意力机制，2022年。'
- en: '[74] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong
    Wang. Lm-infinite: Zero-shot extreme length generalization for large language
    models, 2023.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, 和 Sinong
    Wang。Lm-infinite: 大型语言模型的零样本极端长度泛化，2023年。'
- en: '[75] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng
    Gao. Model tells you what to discard: Adaptive kv cache compression for llms,
    2024.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, 和 Jianfeng
    Gao。模型告诉你该丢弃什么: 针对 LLM 的自适应 Kv 缓存压缩，2024年。'
- en: '[76] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang,
    and Beidi Chen. H[2]o: Heavy-hitter oracle for efficient generative inference
    of large language models, 2023.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi
    Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang,
    和 Beidi Chen。H[2]o: 大型语言模型高效生成推理的重型命中预言机，2023年。'
- en: '[77] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik,
    and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection
    for efficient generative inference, 2024.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik,
    和 Purushotham Kamath。Keyformer: 通过选择关键令牌减少 Kv 缓存以实现高效生成推理，2024年。'
- en: '[78] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference, 2024.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
    Luschi, 和 Douglas Orr。Sparq attention: 带宽高效的 LLM 推理，2024年。'
- en: '[79] Siyu Ren and Kenny Q. Zhu. On the efficacy of eviction policy for key-value
    constrained generative language model inference, 2024.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Siyu Ren 和 Kenny Q. Zhu。关于用于键值约束生成语言模型推理的驱逐策略的有效性，2024年。'
- en: '[80] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, and
    Beidi Chen. Get more with less: Synthesizing recurrence with kv cache compression
    for efficient llm inference, 2024.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, 和 Beidi
    Chen。少即是多: 通过 Kv 缓存压缩合成递归以实现高效 LLM 推理，2024年。'
- en: '[81] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan
    Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic
    capacity of llms for understanding extremely long sequences with training-free
    memory, 2024.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan
    Zhang, Zhiyuan Liu, Song Han, 和 Maosong Sun。Infllm: 揭示 LLM 在理解极长序列上的内在能力，无需训练的记忆，2024年。'
- en: '[82] Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary
    Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC:
    encoding long and structured inputs in transformers. In Bonnie Webber, Trevor
    Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020,
    pages 268–284\. Association for Computational Linguistics, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary
    Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang 和 Li Yang。ETC：在变换器中编码长且结构化的输入。见于
    Bonnie Webber, Trevor Cohn, Yulan He 和 Yang Liu 编辑的《2020年自然语言处理经验方法会议论文集》，EMNLP
    2020，在线，2020年11月16-20日，第268–284页。计算语言学协会，2020年。'
- en: '[83] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
    Transformers are rnns: Fast autoregressive transformers with linear attention.
    In International conference on machine learning, pages 5156–5165\. PMLR, 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas 和 François Fleuret。Transformers
    are rnns: Fast autoregressive transformers with linear attention. 见于《国际机器学习会议》，第5156–5165页。PMLR，2020年。'
- en: '[84] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
    Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz
    Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794,
    2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
    Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz
    Kaiser 等。通过performers重新思考注意力。arXiv预印本 arXiv:2009.14794，2020年。'
- en: '[85] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.
    Efficient attention: Attention with linear complexities. In Proceedings of the
    IEEE/CVF winter conference on applications of computer vision, pages 3531–3539,
    2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi 和 Hongsheng Li。高效注意力：具有线性复杂度的注意力。见于《IEEE/CVF冬季计算机视觉应用会议论文集》，第3531–3539页，2021年。'
- en: '[86] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
    Ré. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information
    Processing Systems, 34:17413–17426, 2021.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra 和 Christopher
    Ré。Scatterbrain：统一稀疏和低秩注意力。见于《神经信息处理系统进展》，第34卷：17413–17426，2021年。'
- en: '[87] Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng
    Wang, and Yingyan Lin. Vitality: Unifying low-rank and sparse approximation for
    vision transformer acceleration with a linear taylor attention. In 2023 IEEE International
    Symposium on High-Performance Computer Architecture (HPCA), pages 415–428\. IEEE,
    2023.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Jyotikrishna Dass, Shang Wu, Huihong Shi, Chaojian Li, Zhifan Ye, Zhongfeng
    Wang 和 Yingyan Lin。Vitality：通过线性泰勒注意力统一低秩和稀疏近似以加速视觉变换器。见于《2023年IEEE国际高性能计算机架构研讨会（HPCA）》第415–428页，IEEE，2023年。'
- en: '[88] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for
    softmax. arXiv preprint arXiv:1805.02867, 2018.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Maxim Milakov 和 Natalia Gimelshein。softmax的在线归一化计算。arXiv预印本 arXiv:1805.02867，2018年。'
- en: '[89] Markus N Rabe and Charles Staats. Self-attention does not need $O(n^{2})$
    memory. arXiv preprint arXiv:2112.05682, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Markus N Rabe 和 Charles Staats。自注意力不需要 $O(n^{2})$ 内存。arXiv预印本 arXiv:2112.05682，2021年。'
- en: '[90] Hao Liu and Pieter Abbeel. Blockwise parallel transformers for large context
    models. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Hao Liu 和 Pieter Abbeel。适用于大型上下文模型的块状并行变换器。见于《神经信息处理系统进展》，第36期，2024年。'
- en: '[91] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise
    transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Hao Liu, Matei Zaharia 和 Pieter Abbeel。用于近无限上下文的块状变换器的环形注意力。arXiv预印本 arXiv:2310.01889，2023年。'
- en: '[92] Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong
    Sun, Shengnan Wang, and Teng Su. Burstattention: An efficient distributed attention
    framework for extremely long sequences. arXiv preprint arXiv:2403.09347, 2024.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Sun Ao, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong
    Sun, Shengnan Wang 和 Teng Su。Burstattention：一种高效的分布式注意力框架，适用于极长序列。arXiv预印本 arXiv:2403.09347，2024年。'
- en: '[93] William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian
    Jin, Zhiye Song, and Jonathan Ragan-Kelley. Striped attention: Faster ring attention
    for causal transformers. arXiv preprint arXiv:2311.09431, 2023.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian
    Jin, Zhiye Song 和 Jonathan Ragan-Kelley。条纹注意力：更快的环形注意力用于因果变换器。arXiv预印本 arXiv:2311.09431，2023年。'
- en: '[94] Bin Lin, Tao Peng, Chen Zhang, Minmin Sun, Lanbo Li, Hanyu Zhao, Wencong
    Xiao, Qi Xu, Xiafei Qiu, Shen Li, et al. Infinite-llm: Efficient llm service for
    long context with distattention and distributed kvcache. arXiv preprint arXiv:2401.02669,
    2024.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Bin Lin、Tao Peng、Chen Zhang、Minmin Sun、Lanbo Li、Hanyu Zhao、Wencong Xiao、Qi
    Xu、Xiafei Qiu、Shen Li 等。Infinite-llm：通过 distattention 和分布式 kvcache 提供高效的长上下文 LLM
    服务。arXiv 预印本 arXiv:2401.02669，2024。'
- en: '[95] Bingyang Wu, Shengyu Liu, Yinmin Zhong, Peng Sun, Xuanzhe Liu, and Xin
    Jin. Loongserve: Efficiently serving long-context large language models with elastic
    sequence parallelism. arXiv preprint arXiv:2404.09526, 2024.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Bingyang Wu、Shengyu Liu、Yinmin Zhong、Peng Sun、Xuanzhe Liu 和 Xin Jin。Loongserve：通过弹性序列并行高效服务长上下文大型语言模型。arXiv
    预印本 arXiv:2404.09526，2024。'
- en: '[96] Tri Dao. Flashattention-2: Faster attention with better parallelism and
    work partitioning. arXiv preprint arXiv:2307.08691, 2023.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Tri Dao。Flashattention-2：更快的注意力机制，具有更好的并行性和工作分配。arXiv 预印本 arXiv:2307.08691，2023。'
- en: '[97] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You.
    Sequence parallelism: Long sequence training from system perspective. arXiv preprint
    arXiv:2105.13120, 2021.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Shenggui Li、Fuzhao Xue、Chaitanya Baranwal、Yongbin Li 和 Yang You。序列并行性：从系统角度看长序列训练。arXiv
    预印本 arXiv:2105.13120，2021。'
- en: '[98] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael
    Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation
    in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Vijay Anand Korthikanti、Jared Casper、Sangkug Lym、Lawrence McAfee、Michael
    Andersch、Mohammad Shoeybi 和 Bryan Catanzaro。减少大型变换模型中的激活重新计算。机器学习与系统会议论文集，第 5
    卷，2023。'
- en: '[99] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song,
    Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for
    enabling training of extreme long sequence transformer models. arXiv preprint
    arXiv:2309.14509, 2023.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Sam Ade Jacobs、Masahiro Tanaka、Chengming Zhang、Minjia Zhang、Leon Song、Samyam
    Rajbhandari 和 Yuxiong He。Deepspeed ulysses：用于训练极长序列变换模型的系统优化。arXiv 预印本 arXiv:2309.14509，2023。'
- en: '[100] Yucheng Li. Unlocking context constraints of llms: Enhancing context
    efficiency of llms with self-information-based content filtering. CoRR, abs/2304.12102,
    2023.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Yucheng Li。解锁 LLM 的上下文限制：通过基于自我信息的内容过滤增强 LLM 的上下文效率。CoRR，abs/2304.12102，2023。'
- en: '[101] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
    Llmlingua: Compressing prompts for accelerated inference of large language models.
    arXiv preprint arXiv:2310.05736, 2023.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Huiqiang Jiang、Qianhui Wu、Chin-Yew Lin、Yuqing Yang 和 Lili Qiu。Llmlingua：压缩提示以加速大型语言模型的推理。arXiv
    预印本 arXiv:2310.05736，2023。'
- en: '[102] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing
    Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context
    scenarios via prompt compression. arXiv preprint arXiv:2310.06839, 2023.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Huiqiang Jiang、Qianhui Wu、Xufang Luo、Dongsheng Li、Chin-Yew Lin、Yuqing
    Yang 和 Lili Qiu。Longllmlingua：通过提示压缩加速和增强长上下文场景中的大型语言模型。arXiv 预印本 arXiv:2310.06839，2023。'
- en: '[103] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue
    Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2:
    Data distillation for efficient and faithful task-agnostic prompt compression.
    arXiv preprint arXiv:2403.12968, 2024.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Zhuoshi Pan、Qianhui Wu、Huiqiang Jiang、Menglin Xia、Xufang Luo、Jue Zhang、Qingwei
    Lin、Victor Rühle、Yuqing Yang、Chin-Yew Lin 等。Llmlingua-2：用于高效且忠实的任务无关提示压缩的数据蒸馏。arXiv
    预印本 arXiv:2403.12968，2024。'
- en: '[104] Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with
    gist tokens. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Jesse Mu、Xiang Li 和 Noah Goodman。学习通过要点令牌压缩提示。神经信息处理系统进展，36，2024。'
- en: '[105] David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt Compression
    and Contrastive Conditioning for Controllability and Toxicity Reduction in Language
    Models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings
    of the Association for Computational Linguistics: EMNLP 2022, pages 5621–5634,
    Abu Dhabi, United Arab Emirates, December 2022\. Association for Computational
    Linguistics.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] David Wingate、Mohammad Shoeybi 和 Taylor Sorensen。用于语言模型的提示压缩和对比条件的可控性与毒性减少。在
    Yoav Goldberg、Zornitsa Kozareva 和 Yue Zhang 编辑的《计算语言学协会会议论文集：EMNLP 2022》中，第 5621–5634
    页，阿布扎比，阿联酋，2022年12月。计算语言学协会。'
- en: '[106] Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder
    for context compression in a large language model. CoRR, abs/2307.06945, 2023.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Tao Ge、Jing Hu、Xun Wang、Si-Qing Chen 和 Furu Wei。在大型语言模型中用于上下文压缩的上下文自动编码器。CoRR，abs/2307.06945，2023。'
- en: '[107] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders,
    and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint
    arXiv:2310.08560, 2023.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders,
    和 Joseph E Gonzalez. Memgpt：迈向将大型语言模型作为操作系统。arXiv 预印本 arXiv:2310.08560，2023年。'
- en: '[108] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.
    Walking down the memory maze: Beyond context limit through interactive reading.
    arXiv preprint arXiv:2310.05029, 2023.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Howard Chen, Ramakanth Pasunuru, Jason Weston, 和 Asli Celikyilmaz. 穿越记忆迷宫：通过互动阅读超越上下文限制。arXiv
    预印本 arXiv:2310.05029，2023年。'
- en: '[109] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang,
    Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,
    et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv
    preprint arXiv:2112.09332, 2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang,
    Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders
    等。Webgpt：通过人类反馈的浏览器辅助问答。arXiv 预印本 arXiv:2112.09332，2021年。'
- en: '[110] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, 和 Michael S Bernstein. 生成代理：人类行为的互动模拟。载于第36届ACM用户界面软件与技术年会论文集，页码
    1–22，2023年。'
- en: '[111] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, 和 Maosong Sun. 用于软件开发的交互代理。arXiv 预印本 arXiv:2307.07924，2023年。'
- en: '[112] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
    Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt:
    Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352,
    2023.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao
    Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou 等。Metagpt：用于多代理协作框架的元编程。arXiv
    预印本 arXiv:2308.00352，2023年。'
- en: '[113] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,
    Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.
    Self-refine: Iterative refinement with self-feedback. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,
    Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang 等。Self-refine：带自我反馈的迭代精炼。神经信息处理系统进展，36，2024年。'
- en: '[114] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and
    Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, 和
    Shunyu Yao. Reflexion：具有语言强化学习的语言代理。神经信息处理系统进展，36，2024年。'
- en: '[115] Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. Mlcopilot:
    Unleashing the power of large language models in solving machine learning tasks.
    arXiv preprint arXiv:2304.14979, 2023.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, 和 Yuqing Yang. Mlcopilot：释放大型语言模型在解决机器学习任务中的力量。arXiv
    预印本 arXiv:2304.14979，2023年。'
