- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LightSeq：用于长上下文变压器的序列级并行训练
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03294](https://ar5iv.labs.arxiv.org/html/2310.03294)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.03294](https://ar5iv.labs.arxiv.org/html/2310.03294)
- en: 'Dacheng Li   ²²footnotemark: 2 &Rulin Shao ¹¹footnotemark: 1  ³³footnotemark:
    3 &Anze Xie ⁴⁴footnotemark: 4 &Eric P. Xing ⁸⁸footnotemark: 8 \ANDJoseph E. Gonzalez ²²footnotemark:
    2 &Ion Stoica ²²footnotemark: 2 &Xuezhe Ma ⁷⁷footnotemark: 7 &Hao Zhang ⁴⁴footnotemark:
    4 &'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dacheng Li ²²footnotemark: 2 &Rulin Shao ¹¹footnotemark: 1 ³³footnotemark:
    3 &Anze Xie ⁴⁴footnotemark: 4 &Eric P. Xing ⁸⁸footnotemark: 8 \ANDJoseph E. Gonzalez ²²footnotemark:
    2 &Ion Stoica ²²footnotemark: 2 &Xuezhe Ma ⁷⁷footnotemark: 7 &Hao Zhang ⁴⁴footnotemark:
    4 &'
- en: ^b UC Berkeley     ^w University of Washington     ^s UCSD     ^c CMU     ^m
    MBZUAI     ^u USC Authors contributed equally.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^b UC Berkeley     ^w University of Washington     ^s UCSD     ^c CMU     ^m
    MBZUAI     ^u USC 作者贡献相同。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Increasing the context length of large language models (LLMs) unlocks fundamentally
    new capabilities, but also significantly increases the memory footprints of training.
    Previous model-parallel systems such as Megatron-LM partition and compute different
    attention heads in parallel, resulting in large communication volumes, so they
    cannot scale beyond the number of attention heads, thereby hindering its adoption.
    In this paper, we introduce a new approach, LightSeq, for long-context LLMs training.
    LightSeq has many notable advantages. First, LightSeq partitions over the sequence
    dimension, hence is agnostic to model architectures and readily applicable for
    models with varying numbers of attention heads, such as Multi-Head, Multi-Query
    and Grouped-Query attention. Second, LightSeq not only requires up to 4.7$\times$
    longer sequence length on models with fewer heads, compared to Megatron-LM. Codes
    will be available at [https://github.com/RulinShao/LightSeq](https://github.com/RulinShao/LightSeq).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 增加大型语言模型 (LLMs) 的上下文长度可以解锁根本性的新增能力，但也显著增加了训练的内存占用。之前的模型并行系统如 Megatron-LM 在并行计算不同的注意力头时，会产生大量通信量，因此无法超越注意力头的数量，从而限制了其应用。本文介绍了一种新的方法，LightSeq，用于长上下文
    LLMs 的训练。LightSeq 具有许多显著的优点。首先，LightSeq 在序列维度上进行分区，因此对模型架构不敏感，能够适用于具有不同数量注意力头的模型，如多头、多查询和分组查询注意力。其次，LightSeq
    在注意力头较少的模型上，所需的序列长度比 Megatron-LM 长最多可达 4.7$\times$。代码将在 [https://github.com/RulinShao/LightSeq](https://github.com/RulinShao/LightSeq)
    上提供。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformers with long-context capabilities have enabled fundamentally new applications,
    such as comprehensive document understanding, generating a complete codebase,
    and extended interactive chatting (Osika, [2023](#bib.bib18); Liu et al., [2023](#bib.bib15);
    Li et al., [2023](#bib.bib11)). However, training LLMs with long sequences induces
    large activation memory footprints, posing new challenges to existing distributed
    systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具有长上下文能力的变压器使得一些根本性的应用成为可能，如全面的文档理解、生成完整的代码库和扩展的互动聊天 (Osika, [2023](#bib.bib18);
    Liu et al., [2023](#bib.bib15); Li et al., [2023](#bib.bib11))。然而，使用长序列训练大型语言模型
    (LLMs) 会导致较大的激活内存占用，给现有的分布式系统带来了新的挑战。
- en: One effective method for reducing these large activation memory footprints is
    to partition the activation across devices. To achieve this, existing systems
    like Megatron-LM (Korthikanti et al., [2023](#bib.bib9); Shoeybi et al., [2019](#bib.bib21))
    usually partition the attention heads. However, this design poses a strong assumption
    that the number of attention heads must be divisible by the parallelism degree,
    which does not hold for many model architectures. For example, Llama-33B has 52
    attention heads, which is not divisible by commonly chosen parallelism degrees
    such as 8, 16, and 32, according to the topology of NVIDIA clusters. In addition,
    partitioning attention heads restricts the maximum parallelism degree to be no
    greater than the number of attention heads. However, many popular LLMs do not
    have enough attention heads for it to scale up, e.g., CodeGen (Nijkamp et al.,
    [2022](#bib.bib17)) only has 16 attention heads. Moreover, many works have shown
    that the future Transformer architecture design may have even fewer attention
    heads. For example, Bian et al. ([2021](#bib.bib2)) demonstrates that Transformers
    with a single head outperforms its multi-head counterparts, representing a challenging
    scenario for solutions like Megatron-LM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 减少这些大型激活内存占用的一种有效方法是将激活在不同设备上进行分割。为实现这一点，现有系统如 Megatron-LM（Korthikanti 等，[2023](#bib.bib9)；Shoeybi
    等，[2019](#bib.bib21)）通常会对注意力头进行分割。然而，这种设计假设注意力头的数量必须能够被并行度整除，这对于许多模型架构并不适用。例如，Llama-33B
    具有 52 个注意力头，这个数量不能被 NVIDIA 集群常用的并行度如 8、16 和 32 整除。此外，分割注意力头限制了最大并行度不超过注意力头的数量。然而，许多流行的
    LLM 并没有足够的注意力头来进行扩展，例如，CodeGen（Nijkamp 等，[2022](#bib.bib17)）只有 16 个注意力头。此外，许多研究表明，未来的
    Transformer 架构设计可能会有更少的注意力头。例如，Bian 等人（[2021](#bib.bib2)）展示了单头 Transformer 在性能上优于其多头对应物，这为像
    Megatron-LM 这样的解决方案带来了挑战。
- en: 'To scale beyond the number of heads, we propose partitioning solely the input
    tokens (i.e., sequence parallelism) rather than the attention heads. We present
    a solution that is agnostic to the model architecture and exhibits a maximal parallelism
    degree that scales with the sequence length. Specifically, we introduce a parallelizable
    and memory-efficient exact attention mechanism, DistAttn, in (§[3.1](#S3.SS1 "3.1
    DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")). Our
    design enables opportunities for overlapping, where we can hide communication
    into attention computation(§ [3.2](#S3.SS2 "3.2 Load balanced scheduling with
    communication and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")). We also propose a load-balancing
    technique to avoid the computation bubble caused by the unbalanced workload in
    causal language modeling (§[3.2](#S3.SS2 "3.2 Load balanced scheduling with communication
    and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")). While extending the FlashAttention (Dao,
    [2023](#bib.bib5)) algorithm to DistAttn, we found a way to leverage the underlying
    rematerialization logic to si gnificantly improve the speed of gradient checkpointing
    training (§ [3.3](#S3.SS3 "3.3 Rematerialization-aware checkpointing strategy
    ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training of
    Long Context Transformers")). This technique also applies to non-distributed usage
    of memory-efficient attention, and in our experiments translates to an additional
    1.31$\times$ speedup (§ [4.3](#S4.SS3 "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq:
    Sequence Level Parallelism for Distributed Training of Long Context Transformers")).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '为了超越头数的限制，我们建议仅对输入标记（即序列并行性）进行分区，而不是对注意力头进行分区。我们提出了一种与模型架构无关的解决方案，展现了与序列长度相匹配的最大并行度。具体而言，我们在（§[3.1](#S3.SS1
    "3.1 DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")）中介绍了一种可并行和内存高效的精确注意力机制DistAttn。我们的设计为重叠提供了机会，在此过程中我们可以将通信隐藏在注意力计算中（§[3.2](#S3.SS2
    "3.2 Load balanced scheduling with communication and computation overlap ‣ 3 Method
    ‣ LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers")）。我们还提出了一种负载均衡技术，以避免因因果语言建模中工作负载不均衡造成的计算气泡（§[3.2](#S3.SS2 "3.2 Load
    balanced scheduling with communication and computation overlap ‣ 3 Method ‣ LightSeq:
    Sequence Level Parallelism for Distributed Training of Long Context Transformers")）。在将FlashAttention（Dao，[2023](#bib.bib5)）算法扩展到DistAttn时，我们找到了一种利用底层重新计算逻辑显著提高梯度检查点训练速度的方法（§[3.3](#S3.SS3
    "3.3 Rematerialization-aware checkpointing strategy ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")）。该技术还适用于内存高效注意力的非分布式使用，在我们的实验中转化为额外的1.31$\times$加速（§[4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")）。'
- en: 'Our main contributions are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献是：
- en: '1.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We design LightSeq, a long-context LLM training prototype based on sequence-level
    parallelism. We develop a distributed memory-efficient exact attention DistAttn,
    with novel load balancing and communication overlapping scheduling for causal
    language modeling.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了LightSeq，这是一种基于序列级并行的长上下文LLM训练原型。我们开发了一种分布式内存高效的精确注意力DistAttn，并针对因果语言建模提出了新颖的负载均衡和通信重叠调度方案。
- en: '2.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose a novel checkpointing strategy that bypasses one attention forward
    pass when using memory-efficient attention with gradient checkpointing training.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的检查点策略，在使用内存高效注意力和梯度检查点训练时绕过一次注意力前向传递。
- en: '3.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We evaluate LightSeq on Llama-7B and its variants with different attention heads
    patterns, and demonstrate up to 2.01$\times$ longer sequences training.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在Llama-7B及其变体上评估了LightSeq，并演示了最长训练序列的加速达到2.01$\times$。
- en: 2 Related work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Memory-efficient attention.
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存高效注意力。
- en: Dao et al. ([2022](#bib.bib6)) and Lefaudeux et al. ([2022](#bib.bib10)) propose
    to use an online normalizer (Milakov & Gimelshein, [2018](#bib.bib16)) to compute
    the attention in a blockwise and memory-efficient way. It reduces peak memory
    usage by not materializing large intermediate states, e.g. the attention matrix
    or the up projection matrix output of the MLP layers (Liu & Abbeel, [2023](#bib.bib13)).
    Instead, the attentions are computed in smaller blocks and only the final activation
    are stored. In the backward pass, the intermediate states need to be recomputed.
    Research on sparse attention computes only a sparse subset of the attention score,
    which also reduces the memory footprints yet may lead to inferior performance (Beltagy
    et al., [2020](#bib.bib1); Sun et al., [2022](#bib.bib22); Zaheer et al., [2020](#bib.bib26)).
    In this work, we limit our scope to exact attention.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Dao等人（[2022](#bib.bib6)）和Lefaudeux等人（[2022](#bib.bib10)）建议使用在线规范化器（Milakov &
    Gimelshein，[2018](#bib.bib16)）以块状和内存高效的方式计算注意力。这种方法通过不产生大型中间状态（例如注意力矩阵或MLP层的上投影矩阵输出）来减少峰值内存使用。相反，注意力在较小的块中计算，仅存储最终激活。在反向传播中，中间状态需要重新计算。稀疏注意力的研究只计算稀疏的注意力得分子集，这也减少了内存占用，但可能导致性能下降（Beltagy等，[2020](#bib.bib1)；Sun等，[2022](#bib.bib22)；Zaheer等，[2020](#bib.bib26)）。在这项工作中，我们将范围限制在精确注意力上。
- en: Sequence parallelism, model parallelism, and FSDP.
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 序列并行、模型并行和FSDP。
- en: Li et al. ([2021](#bib.bib12)) is among the first to parallelize along the sequence
    dimension. However, it is not optimized for the computational pattern of causal
    language modeling and is incompatible with memory-efficient attention, which are
    crucial to long-context LLM training. Model parallelism partitions model parameters
    and also distributes the activation in parallel LLM training. Megatron-LM (Korthikanti
    et al., [2023](#bib.bib9)) proposes a hybrid usage of tensor parallelism and sequence
    parallelism to better reduce the activation on a single device and is the main
    baseline of the paper. Fully sharded data-parallelism (FSDP) (Zhao et al., [2023](#bib.bib27);
    Rajbhandari et al., [2020](#bib.bib20)) distributes optimizer states, gradients,
    and model parameters onto different devices and gathers them on-the-fly. It is
    orthogonal to our work, and we use LightSeq in tandem with FSDP to further reduce
    memory acquired by models in experiments.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人（[2021](#bib.bib12)）是最早在序列维度上进行并行化的研究之一。然而，它并未针对因果语言建模的计算模式进行优化，也与内存高效注意力不兼容，这对长上下文LLM训练至关重要。模型并行将模型参数划分，并在并行LLM训练中分配激活。Megatron-LM（Korthikanti等，[2023](#bib.bib9)）建议混合使用张量并行和序列并行，以更好地减少单个设备上的激活，并作为本文的主要基准。完全分片数据并行（FSDP）（Zhao等，[2023](#bib.bib27)；Rajbhandari等，[2020](#bib.bib20)）将优化器状态、梯度和模型参数分布到不同设备上，并实时汇总。这与我们的工作正交，我们在实验中将LightSeq与FSDP配合使用，以进一步减少模型占用的内存。
- en: Gradient checkpointing.
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 梯度检查点。
- en: Gradient checkpointing (Chen et al., [2016](#bib.bib4)) trades computation for
    memory by not storing the activation for certain layers and recomputing their
    activations during forward. Selective checkpointing (Korthikanti et al., [2023](#bib.bib9))
    proposes to only recompute the attention module as it requires large memory but
    with small FLOPs (in smaller context length). Checkmate (Jain et al., [2020](#bib.bib7))
    searches optimal checkpointing using integer linear programming. However, none
    of these designs have considered memory-efficient attention kernels which perform
    recomputation inside the computational kernel to avoid materializing large tensors.
    As a result, many previous recomputation policies become less effective. In this
    work, we focus on checkpointing at the boundary of every transformer layer, which
    is a popular strategy adopted by many current open-sourced projects such as FastChat (Zheng
    et al., [2023](#bib.bib28)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点技术（Chen等，[2016](#bib.bib4)）通过不存储某些层的激活，并在前向传播时重新计算其激活，来以计算换取内存。选择性检查点（Korthikanti等，[2023](#bib.bib9)）建议仅重新计算注意力模块，因为它需要大量内存但计算量较小（在较小的上下文长度中）。Checkmate（Jain等，[2020](#bib.bib7)）使用整数线性规划搜索最佳检查点。然而，这些设计都没有考虑内存高效的注意力内核，这些内核在计算内核内部执行重新计算以避免产生大型张量。因此，许多以前的重新计算策略变得不那么有效。在这项工作中，我们专注于每个变换器层的边界检查点，这是许多当前开源项目（如FastChat（Zheng等，[2023](#bib.bib28)））采用的流行策略。
- en: '![Refer to caption](img/c69a50b39096e966b8dafb393b184ee0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c69a50b39096e966b8dafb393b184ee0.png)'
- en: 'Figure 1: Left: Sequence parallelism in LightSeq. The input sequence is split
    into chunks along the sequence dimension and distributed to different workers
    (8 workers in the illustration). During forward and backward, only the attention
    module, DistAttn, requires communication of intermediate tensors like $k$. Some
    modules like LayerNorm are ignored for simplicity. Right: Illustration of the
    load-balanced scheduling. “Bubble size” represents the times that a worker is
    idle. Causal language modeling naturally introduces imbalanced workloads, e.g.,
    worker 1 is idle from time step 2 to time step 8 before balancing. We reduce the
    bubble fraction by allocating computation from the busy worker (e.g., worker 8)
    to the idle worker (e.g., worker 1), so worker 1 is only idle at time step 5 after
    balancing.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：左侧：LightSeq 中的序列并行性。输入序列沿序列维度被拆分成块并分配给不同的工作者（插图中为 8 个工作者）。在前向和反向过程中，只有注意力模块
    DistAttn 需要通信中间张量，如 $k$。一些模块，如 LayerNorm，为了简单起见被忽略。右侧：负载均衡调度的示意图。“气泡大小”表示工作者空闲的时间。因果语言建模自然引入了不平衡的工作负载，例如，在平衡之前，工作者
    1 在时间步骤 2 到时间步骤 8 之间是空闲的。我们通过将计算从繁忙的工作者（例如，工作者 8）分配到空闲的工作者（例如，工作者 1）来减少气泡比例，因此工作者
    1 在平衡后只在时间步骤 5 时空闲。
- en: '![Refer to caption](img/bcbdd1ebd17667b0680f45cb12c1900d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bcbdd1ebd17667b0680f45cb12c1900d.png)'
- en: 'Figure 2: Forward pass example of overlapping communication using worker 7
    out of 8 workers. $o$ to the remote worker $p_{8}$.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 8 个工作者中的第 7 个工作者进行重叠通信的前向传递示例。$o$ 到远程工作者 $p_{8}$。
- en: 3 Method
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'In this section, we describe the design of the key components in LightSeq.
    We first introduce a distributed memory-efficient attention, DistAttn (§[3.1](#S3.SS1
    "3.1 DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")) which
    parallelizes the computation along the sequence dimension. We then introduce a
    load-balanced scheduling for causal language modeling to reduce the computation
    bubble as well as an asynchronous communication design that overlaps the communication
    into computation (§[3.2](#S3.SS2 "3.2 Load balanced scheduling with communication
    and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")). Finally, we propose a rematerialization-aware
    checkpointing strategy (§[3.3](#S3.SS3 "3.3 Rematerialization-aware checkpointing
    strategy ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers")) which effectively cuts off the recomputation time
    in gradient checkpointing.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们描述了 LightSeq 中关键组件的设计。我们首先介绍一种分布式内存高效的注意力机制 DistAttn（§[3.1](#S3.SS1
    "3.1 DistAttn: distributed memory-efficient attention ‣ 3 Method ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers")），它在序列维度上并行化计算。然后，我们介绍了一种用于因果语言建模的负载均衡调度，以减少计算气泡，以及一种异步通信设计，将通信与计算重叠（§[3.2](#S3.SS2
    "3.2 Load balanced scheduling with communication and computation overlap ‣ 3 Method
    ‣ LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers")）。最后，我们提出了一种关注重计算的检查点策略（§[3.3](#S3.SS3 "3.3 Rematerialization-aware
    checkpointing strategy ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers")），有效地减少了梯度检查点中的重新计算时间。'
- en: '3.1 DistAttn: distributed memory-efficient attention'
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 DistAttn：分布式内存高效的注意力机制
- en: The core idea in DistAttn is to split the input sequence consisting of $N$ tokens.
    For modules like the Feed Forward Layer (FFN), Layer Norm (LN), and the embedding
    layer the tokens can be computed independently without coordination (embarrasingly
    parallel) and the work is balanced across workers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: DistAttn 的核心思想是将由 $N$ 个标记组成的输入序列进行拆分。对于像前馈层（FFN）、层归一化（LN）和嵌入层这样的模块，标记可以独立计算而无需协调（极度并行），并且工作在工作者之间得到平衡。
- en: Unfortunately, for the attention modules where local tokens may need to attend
    to remote tokens, coordination is required. To address this, each worker collects
    all the keys and values associated with other tokens and then locally computes
    the attention following Dao ([2023](#bib.bib5)). To address the memory pressure
    introduced by collecting all other keys and values, this process is done online
    by streaming the key and values from workers with earlier tokens to workers with
    later tokens. More formally, denote $\mathbf{q}_{p}$-th worker ($p=\{1,\cdots,P\}$-th
    chunk of the key and value, denote $p_{\text{local}}\in\{1,\cdots,P\}$ and $\mathbf{v}_{p_{\text{remote}}}$-th
    worker where there are $P$ total workers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于需要本地标记关注远程标记的注意力模块，需进行协调。为解决此问题，每个工作者收集与其他标记相关的所有键和值，然后根据Dao（[2023](#bib.bib5)）进行本地注意力计算。为了应对收集所有其他键和值所带来的内存压力，这一过程通过从拥有较早标记的工作者向拥有较晚标记的工作者流式传输键和值来在线完成。更正式地，表示$\mathbf{q}_{p}$-th工作者（$p=\{1,\cdots,P\}$）的键和值块，表示$p_{\text{local}}\in\{1,\cdots,P\}$和$\mathbf{v}_{p_{\text{remote}}}$-th工作者，其中总共有$P$个工作者。
- en: 3.2 Load balanced scheduling with communication and computation overlap
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 带有通信和计算重叠的负载平衡调度
- en: Load balanced scheduling.
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负载平衡调度。
- en: 'Causal language modeling objective (Brown et al., [2020](#bib.bib3); Touvron
    et al., [2023](#bib.bib24)) is one of the most prevalent objectives for LLMs,
    where each token only attends to its previous tokens. This naturally introduces
    a work imbalance between workers in our block-wise attention: as shown in Figure [1](#S2.F1
    "Figure 1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers") (“Before Balancing”),
    in an 8-worker ($P=8$ when $P\rightarrow\infty$ compute ${attn}(\mathbf{q}_{8},\mathbf{k}_{1},\mathbf{v}_{1})$
    when scaling to more number of workers.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '因果语言建模目标（Brown et al., [2020](#bib.bib3); Touvron et al., [2023](#bib.bib24)）是LLM中最常见的目标之一，其中每个标记仅关注其之前的标记。这自然引入了我们块级注意力中的工作不均衡：如图[1](#S2.F1
    "Figure 1 ‣ Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers")所示（“平衡前”），在8个工作者的情况下（$P=8$当$P\rightarrow\infty$），计算${attn}(\mathbf{q}_{8},\mathbf{k}_{1},\mathbf{v}_{1})$时的扩展。'
- en: Communication and computation overlap.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信和计算重叠。
- en: 'DistAttn relies on peer-to-peer (P2P) communication to fetch the $\mathbf{k},\mathbf{v}$
    chunks in the load balanced scheduling) from remote devices before computing the
    corresponding attention block. However, these communications can be easily overlapped
    with the computation of the former blocks. For instance, When the first worker
    is computing attention for its local token, it can pre-fetch the next chunk of
    tokens it needs for the next time step. In modern accelerators, this can be done
    by placing the attention computation kernel in the main GPU stream, and the P2P
    communication kernel in another stream, where they can run in parallel (Zhao et al.,
    [2023](#bib.bib27)). We demonstrate the overlapped scheduling for worker 7 on
    the 8 workers example in Figure. [2](#S2.F2 "Figure 2 ‣ Gradient checkpointing.
    ‣ 2 Related work ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers"). Empirically, we find this optimization greatly
    reduces the communication overhead (§[4.3](#S4.SS3 "4.3 Ablation Study ‣ 4 Experiments
    ‣ LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers")).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'DistAttn依赖点对点（P2P）通信来从远程设备中提取负载平衡调度中的$\mathbf{k},\mathbf{v}$块，然后再计算相应的注意力块。然而，这些通信可以很容易地与前面块的计算重叠。例如，当第一个工作者计算其本地标记的注意力时，它可以预取下一时间步所需的标记块。在现代加速器中，可以通过将注意力计算内核放置在主GPU流中，而将P2P通信内核放置在另一个流中来实现，它们可以并行运行（Zhao
    et al., [2023](#bib.bib27)）。我们在图[2](#S2.F2 "Figure 2 ‣ Gradient checkpointing.
    ‣ 2 Related work ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers")中展示了在8个工作者示例中对工作者7的重叠调度。经验上，我们发现这种优化大大减少了通信开销（§[4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")）。'
- en: '![Refer to caption](img/ef3017aa3585ae28e7072805e4013ca7.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ef3017aa3585ae28e7072805e4013ca7.png)'
- en: 'Figure 3: Time breakdown of attention versus other modules in a forward pass.
    Time measured with Flash-Attention (Dao, [2023](#bib.bib5)) (Unit ms).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：前向传播中注意力与其他模块的时间分解。时间使用Flash-Attention（Dao, [2023](#bib.bib5)）测量（单位ms）。
- en: 3.3 Rematerialization-aware checkpointing strategy
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 重新物化感知检查点策略
- en: 'The de-facto way of training transformers requires gradient checkpointing.
    Often, the system uses heuristics to insert gradient checkpoints at each Transformer
    layer (Wolf et al., [2019](#bib.bib25)). However, with the presence of Dao et al.
    ([2022](#bib.bib6)), we found the previous gradient checkpointing strategy will
    cause an extra recomputation of the flash attention forward kernel. Concretely,
    when computing the gradient of the MLP layer, Wolf et al. ([2019](#bib.bib25))
    will re-compute the forward of the entire Transformer layer, including the one
    in flash attention. However, when computing the gradient of the flash attention
    kernel, it needs to re-compute the forward of the flash attention again. Essentially,
    this is because flash attention will not materialize the intermediate values during
    the forward, and will recompute it during the backward, regardless of the re-computation
    strategy in the outer system level. To tackle this, we propose to insert checkpoints
    at the output of the flash attention kernel, instead of at the Transformer layer
    boundary. In this case, we only need to recompute the forward of flash attention
    once, effectively saving a forward of attention for each Transformer layer as
    shown in Figure. [4](#S3.F4 "Figure 4 ‣ 3.3 Rematerialization-aware checkpointing
    strategy ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers"). In Figure. [3](#S3.F3 "Figure 3 ‣ Communication
    and computation overlap. ‣ 3.2 Load balanced scheduling with communication and
    computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers"), we show the attention time dominates
    in the forward pass when scaling up the sequence length, which indicates our method
    can save $\sim 0.23\times 32$) seconds when training a 64K sequence example on
    Llama-7b using the local version of flash attention. In addition, this saves a
    communication brought by our DistAttn forward in the distributed training scenario.
    We benchmark the end-to-end speedup brought by this materialization-aware checkpointing
    strategy in §[4.3](#S4.SS3 "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上，训练变换器的方式需要梯度检查点。通常，系统使用启发式方法在每个 Transformer 层插入梯度检查点（Wolf 等， [2019](#bib.bib25)）。然而，鉴于
    Dao 等人（[2022](#bib.bib6)），我们发现之前的梯度检查点策略会导致额外的闪存注意力前向核的重新计算。具体而言，当计算 MLP 层的梯度时，Wolf
    等（[2019](#bib.bib25)）会重新计算整个 Transformer 层的前向，包括闪存注意力中的部分。然而，当计算闪存注意力核的梯度时，需要再次重新计算闪存注意力的前向。实质上，这是因为闪存注意力在前向过程中不会生成中间值，而是在反向过程中重新计算，不论外部系统级的重新计算策略如何。为了解决这个问题，我们建议在闪存注意力核的输出处插入检查点，而不是在
    Transformer 层边界处插入。在这种情况下，我们只需重新计算一次闪存注意力的前向，从而有效地为每个 Transformer 层节省一个前向计算，如图[4](#S3.F4
    "Figure 4 ‣ 3.3 Rematerialization-aware checkpointing strategy ‣ 3 Method ‣ LightSeq:
    Sequence Level Parallelism for Distributed Training of Long Context Transformers")所示。在图[3](#S3.F3
    "Figure 3 ‣ Communication and computation overlap. ‣ 3.2 Load balanced scheduling
    with communication and computation overlap ‣ 3 Method ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers")中，我们展示了在扩大序列长度时，注意力时间在前向传递中占主导地位，这表明我们的方法在使用闪存注意力的本地版本训练64K序列示例时，可以节省$\sim
    0.23\times 32$秒。此外，这还节省了在分布式训练场景中由我们的 DistAttn 前向计算带来的通信开销。我们在§[4.3](#S4.SS3 "4.3
    Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers")中基准测试了这种材料感知检查点策略带来的端到端加速效果。'
- en: '![Refer to caption](img/14b14ee7899d18f6b2fe1cf5fc1d8db2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/14b14ee7899d18f6b2fe1cf5fc1d8db2.png)'
- en: 'Figure 4: Comparison of HuggingFace gradient checkpointing strategy and our
    materialization-aware gradient checkpointing strategy. Note that our checkpointing
    strategy saves an entire flash attention forward per layer in recomputation.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：HuggingFace 梯度检查点策略与我们的材料感知梯度检查点策略的比较。请注意，我们的检查点策略在重新计算时每层节省了整个闪存注意力前向计算。
- en: Communication and memory analysis Denote the hidden dimension as $d$. With the
    causal language objective, half of the keys and values do not need to be attended,
    halving the forward communication volume to $Nd$ size tensor, thus giving a total
    communication volume of $10Nd$ because of the rematerialization-aware strategy.
    In conclusion, LightSeq achieves 4.7x communication volume reduction compared
    with Megatron-LM.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通信和内存分析 设隐藏维度为 $d$。对于因果语言目标，一半的键和值不需要被关注，将前向通信量减半为 $Nd$ 大小的张量，因此由于重新计算感知策略，总通信量为
    $10Nd$。总之，LightSeq实现了相比于Megatron-LM的4.7倍通信量减少。
- en: 'In practice, we combine LightSeq with FSDP to also distribute the model weights
    for large models. We note that the communication introduced by FSDP is only proportional
    to the size of model weights, which does not scale up with long sequence length.
    We show the end-to-end speedup with FSDP in Table [1](#S4.T1 "Table 1 ‣ Implementation.
    ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers"). In the situations where the model uses MQA or
    GQA, LightSeq further saves the communication volumes by the shared key and values,
    which we discuss in detail in § [4.1](#S4.SS1 "4.1 faster training speed and better
    support for different model architectures ‣ 4 Experiments ‣ LightSeq: Sequence
    Level Parallelism for Distributed Training of Long Context Transformers"). However,
    we also note that this is a theoretical analysis, where the wall-clock time may
    differ because of factors such as implementations. In the experiment section,
    we provide wall-clock end-to-end results for comparison.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上，我们将LightSeq与FSDP结合，以便也分布模型权重用于大型模型。我们注意到，FSDP引入的通信仅与模型权重的大小成比例，而不会随着长序列长度的增加而扩大。我们在表
    [1](#S4.T1 "Table 1 ‣ Implementation. ‣ 4 Experiments ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers") 中展示了与FSDP的端到端加速。在使用MQA或GQA的情况下，LightSeq通过共享键和值进一步减少通信量，我们在
    § [4.1](#S4.SS1 "4.1 faster training speed and better support for different model
    architectures ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers") 中详细讨论了此问题。然而，我们也注意到，这是一种理论分析，实际的墙钟时间可能因实现等因素而有所不同。在实验部分，我们提供了墙钟端到端结果以供比较。'
- en: 4 Experiments
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个实验
- en: 'In this section, we evaluate LightSeq against Megatron-LM (Korthikanti et al.,
    [2023](#bib.bib9)) and show:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将LightSeq与Megatron-LM（Korthikanti等， [2023](#bib.bib9)）进行比较，并展示：
- en: '1.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: LightSeq has faster training speed on a wide range of models. It achieves up
    to 2.01$\times$ speedup over Megatron-LM on various MHA and GQA models.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LightSeq在广泛的模型上具有更快的训练速度。在各种MHA和GQA模型上，它实现了对Megatron-LM的最高2.01$\times$加速。
- en: '2.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: LightSeq supports longer sequence length by scaling beyond the number of attention
    heads. We show our method can support 2x-8x longer sequences than Megatron-LM.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LightSeq通过扩展注意力头的数量来支持更长的序列长度。我们展示了我们的方法可以支持比Megatron-LM长2x-8x的序列。
- en: 'In the ablation study, we provide the gain from each component of LightSeq:
    Load balancing, computation-communication overlapping, and rematerialization-aware
    checkpointing.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在消融研究中，我们提供了LightSeq各个组件的收益：负载均衡、计算-通信重叠以及重新计算感知的检查点。
- en: Cluster setup.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集群设置。
- en: We evaluate our method and the baseline in (1) A single A100 DGX box with 8x80
    GB GPUs. These GPUs are connected with NVLink; (2) 2 DGX boxes with the same setting.
    These two boxes are interconnected by 100 Gbps Infiniband. This is representative
    of cross-node training, where the communication overhead has a larger effect.
    (3) Our in-house cluster with 2x8 A100 40GB GPUs without Inifiniband. We report
    some results on this cluster where conclusions can be drawn from a single-node
    setup or without involving cross-node training time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下环境中评估了我们的方法和基线：（1）单个A100 DGX盒子，配备8x80 GB GPU。这些GPU通过NVLink连接；（2）2个DGX盒子，配置相同。这两个盒子通过100
    Gbps Infiniband互连。这代表了跨节点训练，其中通信开销影响更大。（3）我们内部集群，配备2x8 A100 40GB GPU，没有Inifiniband。我们报告了一些在此集群上的结果，这些结果可以从单节点设置或不涉及跨节点训练时间中得出结论。
- en: Model setup.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型设置。
- en: 'We evaluate our system on Llama-7B and its variants of different representative
    families: (1) Multi-head attention(MHA) models: LLama-7B with 4096 hidden size
    and 32 query(key and value) heads (Touvron et al., [2023](#bib.bib24)); (2) Grouped-Query
    attention (GQA) models: Llama-GQA, same as Llama-7B but with 8 key and value heads;
    (3) models with more general number of attention heads: Llama-33H same as Llama-7B
    but with 33 query (key and value) attention heads. (4) models with fewer attention
    heads: we design Llama-16H, Llama-8H, Llama-4H, Llama-2H with 16, 8, 4, and 2
    heads. According to Liu et al. ([2021](#bib.bib14)), we keep the number of attention
    heads by scaling the number of layers properly and keep the intermediate FFN layer
    size the same to make the model sizes still comparable. For example, Llama-16H
    has 16 attention heads per layer, a hidden size of 2048, an FFN layer of size
    11008, and 64 layers.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Llama-7B 及其不同代表性系列的变体上评估我们的系统：（1）多头注意力（MHA）模型：具有 4096 隐藏大小和 32 个查询（键和值）头的
    LLama-7B（Touvron 等，[2023](#bib.bib24)）；（2）分组查询注意力（GQA）模型：Llama-GQA，与 Llama-7B
    相同，但具有 8 个键和值头；（3）具有更多通用注意力头数量的模型：Llama-33H，与 Llama-7B 相同，但具有 33 个查询（键和值）注意力头。（4）具有更少注意力头的模型：我们设计了
    Llama-16H、Llama-8H、Llama-4H、Llama-2H，分别具有 16、8、4 和 2 个头。根据 Liu 等人（[2021](#bib.bib14)），我们通过适当缩放层的数量来保持注意力头的数量，并保持中间
    FFN 层的大小相同，以使模型大小仍然可比。例如，Llama-16H 每层具有 16 个注意力头，隐藏大小为 2048，FFN 层大小为 11008，层数为
    64。
- en: Implementation.
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现。
- en: 'LightSeq is a lightweight scheduling level prototype. In particular, we implement
    the load balancing and overlapping in Python and NCCL Pytorch bindings in 1000
    lines of codes (Paszke et al., [2019](#bib.bib19); Jeaugey, [2017](#bib.bib8)),
    and the checkpointing strategy in 600 lines of Pytorch. It is attention backend
    agnostic. To reduce the memory consumption and reach faster speed in the attention
    module, we use the FlashAttention2 algorithm (Dao, [2023](#bib.bib5)). We use
    the triton (Tillet et al., [2019](#bib.bib23)) implementation and minimally modify
    it to keep around statistics in the flash attention algorithm. We tweak all block
    sizes to 128 and the number of stages to 1 for the best performance in our cluster.
    We reuse the C++ backward kernels of FlashAttention2 because we do not need to
    modify the backward logic. We run LightSeq using FSDP to reduce the memory footprint
    of data parallelism (Zhao et al., [2023](#bib.bib27)). For fair comparisons, we
    run all comparisons using the same attention backend. We also add support for
    Megatron-LM so that comparing with them can produce a more insightful analysis:
    (1) not materializing the causal attention mask, greatly reducing the memory footprint.
    For instance, without this support, Megatron-LM will run out of memory with Llama-7B
    at a sequence length of 16K per GPU. (2) head padding where the attention heads
    cannot be divided by device number. All results are gathered with Adam optimizer,
    10 iterations of warm-up, and averaged over the additional 10 iterations.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LightSeq 是一个轻量级调度级原型。特别是，我们在 Python 和 NCCL Pytorch 绑定中实现了负载均衡和重叠，代码量为 1000 行（Paszke
    等，[2019](#bib.bib19); Jeaugey，[2017](#bib.bib8)），并在 Pytorch 中实现了检查点策略，代码量为 600
    行。它对注意力后端无关。为了减少内存消耗并在注意力模块中实现更快的速度，我们使用了 FlashAttention2 算法（Dao，[2023](#bib.bib5)）。我们使用了
    triton（Tillet 等，[2019](#bib.bib23)）实现，并进行了最小的修改以保持闪存注意力算法中的统计数据。我们调整了所有块大小为 128，阶段数量为
    1，以获得我们集群中的最佳性能。我们重用了 FlashAttention2 的 C++ 反向内核，因为我们不需要修改反向逻辑。我们使用 FSDP 运行 LightSeq，以减少数据并行的内存占用（Zhao
    等，[2023](#bib.bib27)）。为了公平比较，我们使用相同的注意力后端进行所有比较。我们还为 Megatron-LM 添加了支持，以便进行更有洞察力的分析：（1）不实现因果注意力掩码，显著减少了内存占用。例如，没有此支持，Megatron-LM
    会在每个 GPU 的序列长度为 16K 时耗尽内存。 （2）头部填充，当注意力头不能被设备数量整除时。所有结果均使用 Adam 优化器，经过 10 次热身迭代，并在额外的
    10 次迭代中取平均。
- en: 'Table 1: Per iteration wall-clock time of LightSeq and Megatron-LM (Korthikanti
    et al., [2023](#bib.bib9)) (Unit: seconds). Speedup in bold denotes the better
    of the two systems in the same configuration.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LightSeq 和 Megatron-LM 的每次迭代墙钟时间（Korthikanti 等，[2023](#bib.bib9)）（单位：秒）。粗体加速表示在相同配置下两个系统中更好的一个。
- en: '| Method | # GPUs | Sequence Length | Llama-7B | Llama-GQA | Llama-33H |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GPU 数量 | 序列长度 | Llama-7B | Llama-GQA | Llama-33H |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  |  | Per GPU | Total | Time | speedup | Time | speedup | Time | speedup
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 每个 GPU | 总计 | 时间 | 加速 | 时间 | 加速 | 时间 | 加速 |'
- en: '| Megatron-LM | 1x8 | 4K | 32K | 2.54 | 1.0x | 2.43 | 1.0x | 3.15 | 1.0x |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 1x8 | 4K | 32K | 2.54 | 1.0x | 2.43 | 1.0x | 3.15 | 1.0x |'
- en: '| 1x8 | 8K | 64K | 6.81 | 1.0x | 6.60 | 1.0x | 8.37 | 1.0x |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 8K | 64K | 6.81 | 1.0x | 6.60 | 1.0x | 8.37 | 1.0x |'
- en: '| 1x8 | 16K | 128K | 20.93 | 1.0x | 20.53 | 1.0x | 25.75 | 1.0x |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 16K | 128K | 20.93 | 1.0x | 20.53 | 1.0x | 25.75 | 1.0x |'
- en: '| 1x8 | 32K | 256K | 72.75 | 1.0x | 71.93 | 1.0x | 90.21 | 1.0x |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 32K | 256K | 72.75 | 1.0x | 71.93 | 1.0x | 90.21 | 1.0x |'
- en: '| LightSeq | 1x8 | 4K | 32K | 2.50 | 1.02x | 2.30 | 1.06x | 2.58 | 1.22x |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 1x8 | 4K | 32K | 2.50 | 1.02x | 2.30 | 1.06x | 2.58 | 1.22x |'
- en: '| 1x8 | 8K | 64K | 5.98 | 1.14x | 5.61 | 1.18x | 6.08 | 1.38x |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 8K | 64K | 5.98 | 1.14x | 5.61 | 1.18x | 6.08 | 1.38x |'
- en: '| 1x8 | 16K | 128K | 17.26 | 1.21x | 16.86 | 1.22x | 17.77 | 1.45x |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 16K | 128K | 17.26 | 1.21x | 16.86 | 1.22x | 17.77 | 1.45x |'
- en: '| 1x8 | 32K | 256K | 58.46 | 1.24x | 57.01 | 1.26x | 59.96 | 1.50x |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 1x8 | 32K | 256K | 58.46 | 1.24x | 57.01 | 1.26x | 59.96 | 1.50x |'
- en: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x | 5.26 | 1.0x | 7.52 | 1.0x |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x | 5.26 | 1.0x | 7.52 | 1.0x |'
- en: '| 2x8 | 8K | 128K | 14.26 | 1.0x | 14.21 | 1.0x | 20.63 | 1.0x |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 14.26 | 1.0x | 14.21 | 1.0x | 20.63 | 1.0x |'
- en: '| 2x8 | 16K | 256K | 43.44 | 1.0x | 43.20 | 1.0x | 62.78 | 1.0x |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 43.44 | 1.0x | 43.20 | 1.0x | 62.78 | 1.0x |'
- en: '| 2x8 | 32K | 512K | 147.06 | 1.0x | 146.38 | 1.0x | 216.70 | 1.0x |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 147.06 | 1.0x | 146.38 | 1.0x | 216.70 | 1.0x |'
- en: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x | 4.92 | 1.07x | 7.03 | 1.07x |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x | 4.92 | 1.07x | 7.03 | 1.07x |'
- en: '| 2x8 | 8K | 128K | 12.75 | 1.12x | 9.74 | 1.46x | 13.12 | 1.57x |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 12.75 | 1.12x | 9.74 | 1.46x | 13.12 | 1.57x |'
- en: '| 2x8 | 16K | 256K | 30.21 | 1.44x | 28.49 | 1.52x | 31.33 | 2.00x |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 30.21 | 1.44x | 28.49 | 1.52x | 31.33 | 2.00x |'
- en: '| 2x8 | 32K | 512K | 106.37 | 1.38x | 102.34 | 1.43x | 107.76 | 2.01x |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 106.37 | 1.38x | 102.34 | 1.43x | 107.76 | 2.01x |'
- en: 'Table 2: The maximal sequence length Per GPU supported by LightSeq and Megatron-LM
    with tensor parallelism and pipeline parallelism on 16xA100 40GB GPUs.  LightSeq
    supports 512K sequence length in all models, while Megatron-LM strategy maximal
    sequence length decreases with fewer heads, with either data parallelism or pipeline
    parallelism.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 16xA100 40GB GPU 上，LightSeq 和 Megatron-LM 支持的最大序列长度，使用张量并行和流水线并行。LightSeq
    在所有模型中支持 512K 序列长度，而 Megatron-LM 策略的最大序列长度随着头数的减少而减少，无论是数据并行还是流水线并行。
- en: '|  | Llama-16H | Llama-8H | Llama-4H | Llama-2H |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama-16H | Llama-8H | Llama-4H | Llama-2H |  |'
- en: '| Megatron TP+DP | 512K | 256K | 128K | 64K |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Megatron TP+DP | 512K | 256K | 128K | 64K |  |'
- en: '| Megatron-LM TP+PP | 512K | 256K | 256K | 128K |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM TP+PP | 512K | 256K | 256K | 128K |  |'
- en: '|  LightSeq | 512K | 512K | 512K | 512K |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  LightSeq | 512K | 512K | 512K | 512K |  |'
- en: 4.1 faster training speed and better support for different model architectures
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 更快的训练速度和对不同模型架构的更好支持
- en: 'In this section, we compare our method with Megatron-LM on three settings:
    (1) the multi-head attention (MHA) models where the number of key and value heads
    equals the number of query heads; (2) the grouped-query attention (GQA) models
    where the number of key and value heads is less than the number of query heads;
    (3) the models with arbitrary numbers of heads, i.e. the number heads is unnecessarily
    a multiple of the parallelism degree.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将我们的方法与 Megatron-LM 在三个设置下进行比较：（1）多头注意力（MHA）模型，其中键和值头的数量等于查询头的数量；（2）分组查询注意力（GQA）模型，其中键和值头的数量少于查询头的数量；（3）具有任意数量头的模型，即头的数量不必是并行度的倍数。
- en: Multi-head attention (MHA).
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多头注意力（MHA）。
- en: 'On the Llama-7B model, our method achieves 1.24$\times$ speedup compared to
    Megatron-LM in single node and cross node setting, up to the longest sequence
    length we experiment. This is a joint result of our overlapping communication
    technique and our rematerialization-aware checkpointing strategy. We analyze how
    much each factor contributes to this result in the ablation study ( § [4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")). We do note that our method
    does not achieve better performance in shorter sequences, such as per GPU 4K setting
    for cross node. This is because the communication dominates the training run-time,
    where our overlapping technique has not been able to reduce much. We leave the
    optimization of P2P communication on MHA models and shorter sequence length as
    an exciting future work.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在Llama-7B模型上，我们的方法在单节点和跨节点设置下，与Megatron-LM相比实现了1.24$\times$的加速，适用于我们实验中最长的序列长度。这是我们重叠通信技术和重计算意识检查点策略共同作用的结果。我们在消融研究中分析了每个因素对这一结果的贡献（ § [4.3](#S4.SS3
    "4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers")）。我们注意到，在较短的序列中，我们的方法并未实现更好的性能，例如跨节点的每GPU
    4K设置。这是因为通信主导了训练运行时间，而我们的重叠技术尚未能显著减少通信时间。我们将P2P通信在MHA模型和短序列长度上的优化作为未来有趣的工作。'
- en: Grouped-query attention (GQA).
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分组查询注意力（GQA）。
- en: On LLama-GQA model, our method achieves better speedup because our communication
    of key and value vectors significantly reduces. Note that our communication time
    is proportional to the sum of query, key, value, and output (for load balancing)
    vectors, where reducing key and value sizes to 8 almost half-en our communication
    time. On the contrary, the communication time in Megatron-LM does not decrease
    because its communication happens outside of the attention module, i.e. not influenced
    by optimization inside the attention module. Thus, its overall training run-time
    does not decrease as much as  LightSeq.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLama-GQA模型上，我们的方法实现了更好的加速，因为我们的键值向量通信显著减少。请注意，我们的通信时间与查询、键、值和输出（用于负载均衡）向量的总和成正比，将键和值的大小减少到8几乎使我们的通信时间减少了一半。相反，Megatron-LM中的通信时间没有减少，因为它的通信发生在注意力模块之外，即不受注意力模块内部优化的影响。因此，其整体训练运行时间没有像LightSeq那样减少得多。
- en: We take the 4K per-GPU sequence length and 2x8 GPUs as an example for analysis.
    In the MHA experiment, the communication in a forward and a backward pass of a
    single attention module is roughly 143ms and the computation time is roughly 53ms.
    In addition, our overlapping technique is able to hide 45ms into the computation,
    resulting in a total run-time of 151ms and a net communication overhead of 98
    ms. As a reference, the communication in Megatron-LM takes 33ms, which is why
    Megatron-LM is faster than LightSeq under this particular setting in the MHA experiment.
    When considering the GQA case, the communication in LightSeq roughly reduces to
    71 ms. Overlapping with the computation, the communication overhead is now less
    than that of Megatron-LM. Combined with the checkpointing technique, we are seeing
    a positive speedup gain at 4K per-GPU sequence length. As the sequence length
    increases, our overlapping technique, driven by the fact that computation time
    surpasses communication time, and our checkpointing method, due to the rising
    ratio of a single attention forward, both contribute to greater speedup. Overall,
    we can observe speedups up to 1.52$\times$ on the cross-node setting, making an
    additional eight percent enhancement compared to the results in the MHA experiment
    of the same setting.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以每GPU 4K序列长度和2x8 GPUs为例进行分析。在MHA实验中，单个注意力模块的前向和反向传递中的通信时间大约为143ms，计算时间大约为53ms。此外，我们的重叠技术能够将45ms隐藏到计算中，总运行时间为151ms，净通信开销为98ms。作为参考，Megatron-LM中的通信时间为33ms，这也是为什么在MHA实验的这个特定设置下，Megatron-LM比LightSeq更快。当考虑GQA情况时，LightSeq中的通信时间大致减少到71ms。与计算重叠时，通信开销现在低于Megatron-LM。结合检查点技术，我们在每GPU
    4K序列长度下看到了积极的加速收益。随着序列长度的增加，我们的重叠技术由于计算时间超过通信时间，以及我们的检查点方法由于单次注意力前向传播的比例上升，都促成了更大的加速。总体而言，我们在跨节点设置下观察到的加速高达1.52$\times$，与相同设置下的MHA实验结果相比提升了额外的八个百分点。
- en: In support of arbitrary numbers of heads.
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 支持任意数量的头。
- en: With Llama-33H models, Megatron-LM exhibits an additional performance decline
    compared to LightSeq. This is due to its requirement to pad the number of attention
    heads so that the number of attention heads is divisible by the number of devices.
    On the other hand,  LightSeq does not need to partition attention heads and can
    support an arbitrary number of heads efficiently. For instance, when using 8 GPUs,
    Megatron-LM must pad the attention heads to 40, resulting in 21.2% of the computation
    being wasted. In the case of 16 GPUs, Megatron-LM is compelled to pad the attention
    heads to 48, leading to a more substantial computation wastage of 45.5%. This
    roughly corresponds to a 1.21$\times$ speedup (an additional 20% and 45% speedup
    compared to Llama-7B cases, aligned with the theoretical analysis).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Llama-33H 模型时，相比于 LightSeq，Megatron-LM 表现出额外的性能下降。这是由于其需要填充注意力头的数量，以使得注意力头的数量可以被设备数量整除。另一方面，LightSeq
    不需要分区注意力头，可以高效地支持任意数量的头。例如，当使用 8 个 GPU 时，Megatron-LM 必须将注意力头填充到 40，这导致 21.2% 的计算被浪费。在
    16 个 GPU 的情况下，Megatron-LM 被迫将注意力头填充到 48，从而导致更大的计算浪费，达到 45.5%。这大致对应于 1.21$\times$
    的加速（相比于 Llama-7B 情况下的额外 20% 和 45% 加速，符合理论分析）。
- en: 4.2 Scaling beyond the number of heads.
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 超过头数的扩展。
- en: Assuming the number of heads being a multiple of the tensor parallelism degree
    constraints Megatron-LM to scale its tensor parallelism degree beyond the number
    of heads, thus limiting its scaling ability to longer sequence lengths. When the
    number of GPUs exceeds the number of attention heads, there will be three possible
    solutions to use Megatron-LM. First, the user can pad dummy heads as in the Llama-33H
    scenario. However, when scaling to longer sequences, the percentage of dummy heads
    padded almost directly translates to the percentage of slowdown. For instance,
    for Llama-8H, this solution pads 2$\times$ longer sequences.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 假设头数是张量并行度的倍数，限制了 Megatron-LM 将张量并行度扩展到头数以上，从而限制了其对更长序列长度的扩展能力。当 GPU 数量超过注意力头数时，将有三种可能的解决方案来使用
    Megatron-LM。首先，用户可以像 Llama-33H 场景中那样填充虚拟头。然而，当扩展到更长的序列时，填充虚拟头的百分比几乎直接转化为减速的百分比。例如，对于
    Llama-8H，这种解决方案将序列填充到 2$\times$ 更长。
- en: 4.3 Ablation Study
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Effect of load balancing.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 负载均衡的效果。
- en: 'We study the effect of load balancing using the forward pass of an attention
    operation in Llama-7B model, on 8 A100 40GB GPUs. The backward pass follows a
    similar analysis. With an unbalanced schedule (Figure  [1](#S2.F1 "Figure 1 ‣
    Gradient checkpointing. ‣ 2 Related work ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")), the total work done
    is 36, where the total work could be done in 8 units of time is 64\. Thus, the
    expected maximal speedup is 4.5x. In the balanced schedule, the expected maximal
    speedup is 7.2x. We scale the total sequence length from 4K to 256K. The unbalanced
    version saturates in  4.5x speedup compared to a single GPU implementation, while
    the balanced version saturates  7.5x ¹¹1We find the single machine attention flops
    drop with very long sequence length, resulting in a slightly higher speedup than
    assuming its perfect scalability. speedup. Both of them align with our earlier
    theoretical analysis and show the importance of our balanced scheduling.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了使用 Llama-7B 模型的注意力操作前向传递的负载均衡效果，使用了 8 个 A100 40GB GPU。反向传递遵循类似的分析。在不平衡的调度下（图
    [1](#S2.F1 "图 1 ‣ 梯度检查点 ‣ 2 相关工作 ‣ LightSeq: 长序列变换器分布式训练的序列级并行")），总工作量为 36，而 8
    个时间单位内可以完成的总工作量为 64。因此，期望的最大加速比为 4.5x。在平衡的调度下，期望的最大加速比为 7.2x。我们将总序列长度从 4K 扩展到
    256K。相比于单 GPU 实现，不平衡版本在 4.5x 加速下饱和，而平衡版本在 7.5x 加速下饱和¹¹1我们发现，单机注意力 FLOPS 在非常长的序列长度下下降，导致加速比略高于假设的完美可扩展性。两个结果都与我们早期的理论分析一致，显示了平衡调度的重要性。'
- en: '![Refer to caption](img/2664a01862a3803521ec7b6084c13446.png)![Refer to caption](img/1fec7276ed901b66c49fe3d84ad164c1.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2664a01862a3803521ec7b6084c13446.png)![参见说明](img/1fec7276ed901b66c49fe3d84ad164c1.png)'
- en: 'Figure 5: Ablation on the effect of balanced schedule (left) and the effect
    of overlapping (right).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：平衡调度的效果（左）和重叠效果（右）消融实验。
- en: Effect of overlapping communication and computation.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通信与计算重叠的效果。
- en: We study the benefits of overlapping communication on Llama-7B and 2 DGX boxes.
    We find that overlapping greatly reduce the communication overhead. For instance,
    on a global sequence length of 128K, the communication overhead is reduced from
    105% to 44%. This overlapping scheme maximizes its functionality when the communication
    overhead is less than 100%, where all communication can be potentially overlapped.
    Empirically, we find the system only exhibits 8% and 1% overhead in these cases,
    showing a close performance to an ideal system without communication.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了在Llama-7B和2个DGX盒子上重叠通信的好处。我们发现重叠显著减少了通信开销。例如，在全局序列长度为128K时，通信开销从105%减少到44%。当通信开销低于100%时，这种重叠方案能够最大化其功能，其中所有通信都可以潜在地重叠。根据经验，我们发现系统在这些情况下只表现出8%和1%的开销，显示出接近于没有通信的理想系统的性能。
- en: Effect of materialization-aware checkpointing.
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 物化感知检查点的效果。
- en: 'We show in Table. [3](#S4.T3 "Table 3 ‣ Effect of materialization-aware checkpointing.
    ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for
    Distributed Training of Long Context Transformers") the ablation results of our
    rematerialization-aware gradient checkpointing. Our method achieves 1.16x, 1.24x,
    and 1.31x speedup at the sequence length of 8K, 16K, and 32K per GPU respectively.
    The materialization-aware checkpointing strategy speeds up more at longer sequence
    lengths because it saves an entire attention forward which dominates the computation
    at longer sequence lengths.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[3](#S4.T3 "表 3 ‣ 物化感知检查点的效果 ‣ 4.3 消融研究 ‣ 4 实验 ‣ LightSeq: 长上下文Transformer的分布式训练的序列级并行")中展示了我们再物化感知梯度检查点的消融结果。我们的方法在每个GPU的序列长度为8K、16K和32K时分别实现了1.16x、1.24x和1.31x的加速。物化感知检查点策略在较长序列长度时加速效果更佳，因为它保存了整个注意力前向计算，这在较长序列长度时主导了计算。'
- en: 'Table 3: Ablation study on the effect of the rematerialization-aware gradient
    checkpointing on 8 A100s in a single node with a batch size of 1\. We report the
    end-to-end run time in seconds and show the speedup of our gradient checkpointing
    strategy (“Our ckpt”) over the HuggingFace gradient checkpointing strategy (“HF
    ckpt”).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在单节点上使用批量大小为1的8个A100的消融研究，探讨了对再物化感知梯度检查点的影响。我们报告了端到端的运行时间（秒），并展示了我们梯度检查点策略（“我们的ckpt”）与HuggingFace梯度检查点策略（“HF
    ckpt”）的加速效果。
- en: '| Ckpt Method | Sequence Length Per GPU |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 检查点方法 | 每个GPU的序列长度 |'
- en: '|  | 1K | 2K | 4K | 8K | 16K | 32K |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 1K | 2K | 4K | 8K | 16K | 32K |'
- en: '| HF ckpt | 0.84 | 1.29 | 2.64 | 6.93 | 21.44 | 76.38 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| HF ckpt | 0.84 | 1.29 | 2.64 | 6.93 | 21.44 | 76.38 |'
- en: '| Our ckpt | 0.84 | 1.36 | 2.50 | 5.98 | 17.26 | 58.46 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 我们的ckpt | 0.84 | 1.36 | 2.50 | 5.98 | 17.26 | 58.46 |'
- en: '| Speedup | 1.0x | 0.94x | 1.06x | 1.16x | 1.24x | 1.31x |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 加速 | 1.0x | 0.94x | 1.06x | 1.16x | 1.24x | 1.31x |'
- en: 4.4 Discussion
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 讨论
- en: In this section, we first discuss the future directions that can further improve
    LightSeq. We then compare our method with one concurrent open-sourced project
    which also splits the attention heads. Finally, we discuss the role of pipeline
    parallelism in supporting long sequence training and shows it is less effective
    than tensor parallelism, which is the reason we do not consider it as a major
    baseline.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先讨论了可以进一步改进LightSeq的未来方向。然后，我们将我们的方法与一个同时开源的项目进行了比较，该项目也拆分了注意力头。最后，我们讨论了管道并行在支持长序列训练中的作用，并指出它不如张量并行有效，这也是我们没有将其视为主要基准的原因。
- en: Optimizing P2P communication and better support for shorter context length.
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优化P2P通信，并更好地支持较短的上下文长度。
- en: 'As shown in §[4.1](#S4.SS1 "4.1 faster training speed and better support for
    different model architectures ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers"),  LightSeq may be slower
    in shorter context length and MHA models (Llama-7B on per GPU sequence length
    4K). Based on our preliminary investigation, this is because our usage of P2P
    is not as optimized as primitives used in tensor model parallelism, such as all-gather
    kernels. For instance, they are not aware of the underlying cluster topology.
    In the future, we plan to implement the P2P scheduling in a topology-aware way
    to further improve the communication time.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '如§[4.1](#S4.SS1 "4.1 更快的训练速度和对不同模型架构的更好支持 ‣ 4 实验 ‣ LightSeq: 长上下文Transformer的分布式训练的序列级并行")所示，LightSeq在较短上下文长度和MHA模型（每个GPU序列长度为4K的Llama-7B）中可能较慢。根据我们的初步调查，这是因为我们使用的P2P不如在张量模型并行中使用的原语（如all-gather内核）优化。例如，它们不了解底层集群拓扑。在未来，我们计划以拓扑感知的方式实现P2P调度，以进一步改善通信时间。'
- en: Comparison to DeepSpeed Ulysses.
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与 DeepSpeed Ulysses 的比较。
- en: 'DeepSpeed-Ulysses ²²2[https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses)
    is a concurrent open-sourced implementation, which uses all-to-all communication
    primitive to reduce the communication volume. In our testing, we verified that
    their communication is lower than Megatron-LM. Yet, as it is also partitioning
    the attention head dimension, it suffers from similar problems as analyzed above.
    We provide some end-to-end comparisons in Appendix [B](#A2 "Appendix B Comparison
    with DeepSpeed Ulysses ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers"). We note that the communication in DeepSpeed
    Ulysses can be faster than LightSeq, especially with shorter context length and
    slower network, where the overlapping technique in LightSeq cannot perfectly hide
    all the communication. This can be potentially addressed by optimizing the P2P
    communication as discussed above.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepSpeed-Ulysses ²²2[https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ulysses)
    是一个并行的开源实现，它使用全对全通信原语来减少通信量。在我们的测试中，我们验证了它们的通信量低于 Megatron-LM。然而，由于它也对注意力头维度进行分区，因此面临与上述分析类似的问题。我们在附录[B](#A2
    "Appendix B Comparison with DeepSpeed Ulysses ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")中提供了一些端到端的比较。我们注意到，在较短的上下文长度和较慢的网络环境下，DeepSpeed
    Ulysses的通信可能比 LightSeq 更快，因为 LightSeq 的重叠技术无法完美隐藏所有通信。这可以通过优化 P2P 通信来潜在解决，如上所述。'
- en: Pipeline parallelism.
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 流水线并行。
- en: 'Pipeline parallelism also partitions the activation. However, as mentioned
    in § [4.2](#S4.SS2 "4.2 Scaling beyond the number of heads. ‣ 4 Experiments ‣
    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context
    Transformers"), it does not partition the activations evenly across stage, leaving
    high memory pressure to the first stage. Thus, we mainly focus on comparing with
    tensor model parallelism (combined with sequence parallelism) in this work and
    only consider including pipeline parallelism for comparison when the tensor parallelism
    is limited by the number of heads.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '流水线并行还会对激活进行分区。然而，如 § [4.2](#S4.SS2 "4.2 Scaling beyond the number of heads.
    ‣ 4 Experiments ‣ LightSeq: Sequence Level Parallelism for Distributed Training
    of Long Context Transformers") 中提到的，它并没有在阶段之间均匀分配激活，导致第一阶段的内存压力很大。因此，我们在这项工作中主要关注与张量模型并行（结合序列并行）的比较，仅在张量并行受限于头数时才考虑包括流水线并行进行比较。'
- en: 5 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduce LightSeq, a sequence parallel prototype for long-context
    transformer training. LightSeq presents novel system optimizations including load
    balancing for causal language modelings, overlapped communication with computation
    in the distributed attention computation, and a re-materialization-aware checkpointing
    strategy. Our experiments evaluate multiple families of transformer models and
    on different cluster types, showing that it achieves up to 2.01$\times$ speedup
    and scales up to 8x longer sequences, compared to another popular system, Megatron-LM,.
    Future directions include implementing topology-aware P2P operations to further
    reduce training time in lower sequence lengths.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了 LightSeq，这是一个用于长上下文变换器训练的序列并行原型。LightSeq 提出了包括用于因果语言建模的负载均衡、在分布式注意力计算中的通信与计算重叠以及重新材料化感知检查点策略等新型系统优化。我们的实验评估了多个变换器模型家族和不同集群类型，显示它相较于另一个流行系统
    Megatron-LM 实现了最高 2.01$\times$ 的加速，并且扩展到 8 倍更长的序列。未来的方向包括实现拓扑感知的 P2P 操作，以进一步减少较短序列的训练时间。
- en: References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer:
    The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等（2020）Iz Beltagy、Matthew E Peters 和 Arman Cohan。《Longformer: The long-document
    transformer》。*arXiv 预印本 arXiv:2004.05150*，2020年。'
- en: 'Bian et al. (2021) Yuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan, and
    Kenneth Church. On attention redundancy: A comprehensive study. In *Proceedings
    of the 2021 conference of the north american chapter of the association for computational
    linguistics: human language technologies*, pp.  930–945, 2021.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bian 等（2021）Yuchen Bian、Jiaji Huang、Xingyu Cai、Jiahong Yuan 和 Kenneth Church。《关于注意力冗余：综合研究》。在
    *2021年北美计算语言学协会人类语言技术会议论文集*，第 930–945 页，2021年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: Chen et al. (2016) Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
    Training deep nets with sublinear memory cost. *arXiv preprint arXiv:1604.06174*,
    2016.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2016）Tianqi Chen、Bing Xu、Chiyuan Zhang 和 Carlos Guestrin。以亚线性内存成本训练深度网络。*arXiv
    预印本 arXiv:1604.06174*，2016。
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning. *arXiv preprint arXiv:2307.08691*, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao（2023）Tri Dao。Flashattention-2：具有更好并行性和工作分配的更快注意力机制。*arXiv 预印本 arXiv:2307.08691*，2023。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等（2022）Tri Dao、Dan Fu、Stefano Ermon、Atri Rudra 和 Christopher Ré。Flashattention：具有
    IO 认知的快速且内存高效的精确注意力。*神经信息处理系统进展*，35:16344–16359，2022。
- en: 'Jain et al. (2020) Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami,
    Pieter Abbeel, Joseph Gonzalez, Kurt Keutzer, and Ion Stoica. Checkmate: Breaking
    the memory wall with optimal tensor rematerialization. *Proceedings of Machine
    Learning and Systems*, 2:497–511, 2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain 等（2020）Paras Jain、Ajay Jain、Aniruddha Nrusimha、Amir Gholami、Pieter Abbeel、Joseph
    Gonzalez、Kurt Keutzer 和 Ion Stoica。Checkmate：通过最优张量再物化突破内存墙。*机器学习与系统会议论文集*，2:497–511，2020。
- en: Jeaugey (2017) Sylvain Jeaugey. Nccl 2.0. In *GPU Technology Conference (GTC)*,
    volume 2, 2017.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeaugey（2017）Sylvain Jeaugey。NCCL 2.0。在 *GPU 技术大会 (GTC)*，第 2 卷，2017。
- en: Korthikanti et al. (2023) Vijay Anand Korthikanti, Jared Casper, Sangkug Lym,
    Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing
    activation recomputation in large transformer models. *Proceedings of Machine
    Learning and Systems*, 5, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korthikanti 等（2023）Vijay Anand Korthikanti、Jared Casper、Sangkug Lym、Lawrence
    McAfee、Michael Andersch、Mohammad Shoeybi 和 Bryan Catanzaro。在大型变压器模型中减少激活重新计算。*机器学习与系统会议论文集*，5，2023。
- en: 'Lefaudeux et al. (2022) Benjamin Lefaudeux, Francisco Massa, Diana Liskovich,
    Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore,
    Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable
    transformer modelling library. [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers),
    2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lefaudeux 等（2022）Benjamin Lefaudeux、Francisco Massa、Diana Liskovich、Wenhan Xiong、Vittorio
    Caggiano、Sean Naren、Min Xu、Jieru Hu、Marta Tintore、Susan Zhang、Patrick Labatut
    和 Daniel Haziza。xformers：一个模块化和可黑客的变压器建模库。 [https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers)，2022。
- en: Li et al. (2023) Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng,
    Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source
    llms truly promise on context length, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Dacheng Li、Rulin Shao、Anze Xie、Ying Sheng、Lianmin Zheng、Joseph E Gonzalez、Ion
    Stoica、Xuezhe Ma 和 Hao Zhang。开源LLMs在上下文长度上的实际承诺如何，2023。
- en: 'Li et al. (2021) Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence
    parallelism: Making 4d parallelism possible. *arXiv preprint arXiv:2105.13120*,
    2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021）Shenggui Li、Fuzhao Xue、Yongbin Li 和 Yang You。序列并行：使 4D 并行成为可能。*arXiv
    预印本 arXiv:2105.13120*，2021。
- en: Liu & Abbeel (2023) Hao Liu and Pieter Abbeel. Blockwise parallel transformer
    for long context large models. *arXiv preprint arXiv:2305.19370*, 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu & Abbeel（2023）Hao Liu 和 Pieter Abbeel。用于长上下文大模型的块状并行变压器。*arXiv 预印本 arXiv:2305.19370*，2023。
- en: Liu et al. (2021) Liyuan Liu, Jialu Liu, and Jiawei Han. Multi-head or single-head?
    an empirical comparison for transformer training. *arXiv preprint arXiv:2106.09650*,
    2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）Liyuan Liu、Jialu Liu 和 Jiawei Han。多头还是单头？变压器训练的实证比较。*arXiv 预印本 arXiv:2106.09650*，2021。
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models
    use long contexts. *arXiv preprint arXiv:2307.03172*, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Nelson F Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio
    Petroni 和 Percy Liang。困于中间：语言模型如何使用长上下文。*arXiv 预印本 arXiv:2307.03172*，2023。
- en: Milakov & Gimelshein (2018) Maxim Milakov and Natalia Gimelshein. Online normalizer
    calculation for softmax. *arXiv preprint arXiv:1805.02867*, 2018.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Milakov & Gimelshein（2018）Maxim Milakov 和 Natalia Gimelshein。用于 Softmax 的在线归一化计算。*arXiv
    预印本 arXiv:1805.02867*，2018。
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large
    language model for code with multi-turn program synthesis. *arXiv preprint arXiv:2203.13474*,
    2022.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp 等人（2022）Erik Nijkamp、Bo Pang、Hiroaki Hayashi、Lifu Tu、Huan Wang、Yingbo
    Zhou、Silvio Savarese 和 Caiming Xiong。Codegen: 一个开放的大型语言模型用于代码的多轮程序合成。*arXiv 预印本
    arXiv:2203.13474*，2022年。'
- en: Osika (2023) Anton Osika. gpt-engineer, 2023. URL [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osika（2023）Anton Osika。gpt-engineer，2023年。网址 [https://github.com/AntonOsika/gpt-engineer](https://github.com/AntonOsika/gpt-engineer)。
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
    *Advances in neural information processing systems*, 32, 2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke 等人（2019）Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga 等。Pytorch: 一种命令式、高性能深度学习库。*神经信息处理系统进展*，32，2019年。'
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pp.  1–16\. IEEE, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari 等人（2020）Samyam Rajbhandari、Jeff Rasley、Olatunji Ruwase 和 Yuxiong
    He。Zero: 面向训练万亿参数模型的内存优化。见于*SC20: 高性能计算、网络、存储和分析国际会议*，第1–16页，IEEE，2020年。'
- en: 'Shoeybi et al. (2019) Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick
    LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion
    parameter language models using model parallelism. *arXiv preprint arXiv:1909.08053*,
    2019.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shoeybi 等人（2019）Mohammad Shoeybi、Mostofa Patwary、Raul Puri、Patrick LeGresley、Jared
    Casper 和 Bryan Catanzaro。Megatron-lm: 使用模型并行训练多十亿参数的语言模型。*arXiv 预印本 arXiv:1909.08053*，2019年。'
- en: Sun et al. (2022) Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang,
    Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable
    transformer. *arXiv preprint arXiv:2212.10554*, 2022.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2022）Yutao Sun、Li Dong、Barun Patra、Shuming Ma、Shaohan Huang、Alon Benhaim、Vishrav
    Chaudhary、Xia Song 和 Furu Wei。一个可延展长度的变换器。*arXiv 预印本 arXiv:2212.10554*，2022年。
- en: 'Tillet et al. (2019) Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton:
    an intermediate language and compiler for tiled neural network computations. In
    *Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning
    and Programming Languages*, pp.  10–19, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tillet 等人（2019）Philippe Tillet、Hsiang-Tsung Kung 和 David Cox。Triton: 一种用于平铺神经网络计算的中间语言和编译器。见于*第3届ACM
    SIGPLAN国际机器学习与编程语言研讨会论文集*，第10–19页，2019年。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等人（2019）Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement
    Delangue、Anthony Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz 等。Huggingface的变换器：最先进的自然语言处理。*arXiv
    预印本 arXiv:1910.03771*，2019年。
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. Big bird: Transformers for longer sequences. *Advances in neural
    information processing systems*, 33:17283–17297, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等人（2020）Manzil Zaheer、Guru Guruganesh、Kumar Avinava Dubey、Joshua Ainslie、Chris
    Alberti、Santiago Ontanon、Philip Pham、Anirudh Ravula、Qifan Wang、Li Yang 等。Big bird:
    用于更长序列的变换器。*神经信息处理系统进展*，33:17283–17297，2020年。'
- en: 'Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin
    Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al.
    Pytorch fsdp: experiences on scaling fully sharded data parallel. *arXiv preprint
    arXiv:2304.11277*, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人（2023）Yanli Zhao、Andrew Gu、Rohan Varma、Liang Luo、Chien-Chin Huang、Min
    Xu、Less Wright、Hamid Shojanazeri、Myle Ott、Sam Shleifer 等。Pytorch fsdp: 扩展完全分片数据并行的经验。*arXiv
    预印本 arXiv:2304.11277*，2023年。'
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等 (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph
    E. Gonzalez, 和 Ion Stoica。评估 llm-as-a-judge 与 mt-bench 和 chatbot arena, 2023。
- en: Appendix
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Using DistAttn in LightSeq
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 使用 LightSeq 中的 DistAttn
- en: Algorithm 1 DistAttn in LightSeq (forward pass)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 DistAttn 在 LightSeq 中（前向传递）
- en: 1:Matrices $\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}\in\mathbb{R}^{\frac{N}{\mathbb{P}}\times
    d}$, m, causal, last)3:     Divide $q$ each,4:     and divide $k,v$, of size $B_{c}\times
    d$ of size $B_{r}\times d$ of size $B_{r}$, $\ell_{i}\in\mathbb{R}^{B_{r}}$, $m_{i}^{(0)}$
    from HBM to on-chip SRAM.14:               On chip, compute $s_{i}^{(j)}=q_{i}k^{T}_{j}\in\mathbb{R}^{B_{r}\times
    B_{c}}$.16:               On chip, compute $o_{i}^{(j)}=\mathrm{diag}(e^{m_{i}^{(j-1)}-m_{i}^{(j)}})^{-1}o_{i}^{(j-1)}+\tilde{p}_{i}^{(j)}v^{p}_{j}$-th
    block of $o$-th block of $L$.28:$\mathbf{O}^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}$,
    True, p=1)29:for $1\leq r<p$ into HBM.31:     $\mathbf{O}^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{y},\mathbf{V}^{y}$,
    False, r=(p-1)32:     Delete $\mathbf{K}^{r}$.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 矩阵 $\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}\in\mathbb{R}^{\frac{N}{\mathbb{P}}\times
    d}$，m，因果，last)3: 分割 $q$ 每个，4: 分割 $k,v$，大小为 $B_{c}\times d$ 的大小为 $B_{r}\times d$
    的大小为 $B_{r}$，$\ell_{i}\in\mathbb{R}^{B_{r}}$，$m_{i}^{(0)}$ 从 HBM 转移到片上 SRAM。14:
    在芯片上计算 $s_{i}^{(j)}=q_{i}k^{T}_{j}\in\mathbb{R}^{B_{r}\times B_{c}}$。16: 在芯片上计算
    $o_{i}^{(j)}=\mathrm{diag}(e^{m_{i}^{(j-1)}-m_{i}^{(j)}})^{-1}o_{i}^{(j-1)}+\tilde{p}_{i}^{(j)}v^{p}_{j}$-th
    块的 $o$-th 块的 $L$。28: $\mathbf{O}^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{p},\mathbf{V}^{p}$,
    True, p=1)29: 对 $1\leq r<p$ 进行 HBM。31: $\mathbf{O}^{p}$ = standalone_fwd($\mathbf{Q}^{p},\mathbf{K}^{y},\mathbf{V}^{y}$,
    False, r=(p-1)32: 删除 $\mathbf{K}^{r}$。'
- en: 'In this section, we provide more details of DistAttn, and how it can be used
    with the outer LightSeq logic of the forward pass (Alg [1](#alg1 "Algorithm 1
    ‣ Appendix A Using DistAttn in LightSeq ‣ LightSeq: Sequence Level Parallelism
    for Distributed Training of Long Context Transformers")). For conceptual simplicity,
    we demonstrate it in the most vanilla version, without the actual scheduling (e.g.
    load balancing and overlapping). We also demonstrate it with the causal language
    modeling objective. The standalone attention is mainly borrowed from the FlashAttention2
    paper (Dao, [2023](#bib.bib5)). To make it compatible with DistAttn, we mainly
    revised the several points:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了 DistAttn 的更多细节，以及它如何与前向传递的外部 LightSeq 逻辑一起使用（算法 [1](#alg1 "算法 1
    ‣ 附录 A 使用 LightSeq 中的 DistAttn ‣ LightSeq: 用于分布式训练长上下文变换器的序列级并行")）。为了概念上的简洁，我们在最基础的版本中演示了它，没有实际的调度（例如负载均衡和重叠）。我们还演示了它与因果语言建模目标的结合。独立注意力主要借鉴自
    FlashAttention2 论文 (Dao, [2023](#bib.bib5))。为了使其与 DistAttn 兼容，我们主要修订了几个点：'
- en: '1.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Accumulate results statistics $o$ from previous computation, instead of initializing
    them inside the function.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从以前的计算中累积结果统计 $o$，而不是在函数内部初始化它们。
- en: '2.'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Pass an extra argument ”last”, which means whether this is the last chunk of
    attention computation. Only when it is true, we compute the logsumexp $L$.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传递一个额外参数“last”，表示是否这是最后一块注意力计算。仅当为真时，我们计算 logsumexp $L$。
- en: At a high level, on a worker $p$ (the ”last” variable in the algorithm). At
    the end of the forward pass, worker $p$. Instead, we only need to use the logsumexp
    stored in the forward pass.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，在一个工作节点 $p$ 上（算法中的“最后”变量）。在前向传递结束时，工作节点 $p$。而是，我们只需要使用在前向传递中存储的 logsumexp。
- en: Appendix B Comparison with DeepSpeed Ulysses
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 与 DeepSpeed Ulysses 的比较
- en: '| Method | # GPUs | Sequence Length | Time | Speedup |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | # GPUs | 序列长度 | 时间 | 加速比 |'
- en: '|  |  | Per GPU | Total |  |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 每 GPU | 总计 |  |  |'
- en: '| Llama-7B |  |  |  |  |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Llama-7B |  |  |  |  |  |'
- en: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 2x8 | 4K | 64K | 5.29 | 1.0x |'
- en: '| 2x8 | 8K | 128K | 14.26 | 1.0x |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 14.26 | 1.0x |'
- en: '| 2x8 | 16K | 256K | 43.44 | 1.0x |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 43.44 | 1.0x |'
- en: '| 2x8 | 32K | 512K | 147.06 | 1.0x |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 147.06 | 1.0x |'
- en: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 4.29 | 1.23x |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 4.29 | 1.23x |'
- en: '| 2x8 | 8K | 128K | 11.61 | 1.23x |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 11.61 | 1.23x |'
- en: '| 2x8 | 16K | 256K | 37.53 | 1.16x |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 37.53 | 1.16x |'
- en: '| 2x8 | 32K | 512K | 134.09 | 1.10x |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 134.09 | 1.10x |'
- en: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 2x8 | 4K | 64K | 6.85 | 0.77x |'
- en: '| 2x8 | 8K | 128K | 12.75 | 1.12x |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 12.75 | 1.12x |'
- en: '| 2x8 | 16K | 256K | 30.21 | 1.44x |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 30.21 | 1.44x |'
- en: '| 2x8 | 32K | 512K | 106.37 | 1.38x |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 106.37 | 1.38x |'
- en: '| Llama-33H |  |  |  |  |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| Llama-33H |  |  |  |  |  |'
- en: '| Megatron-LM | 2x8 | 4K | 64K | 7.52 | 1.0x |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-LM | 2x8 | 4K | 64K | 7.52 | 1.0x |'
- en: '| 2x8 | 8K | 128K | 20.63 | 1.0x |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 20.63 | 1.0x |'
- en: '| 2x8 | 16K | 256K | 62.78 | 1.0x |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 62.78 | 1.0x |'
- en: '| 2x8 | 32K | 512K | 216.70 | 1.0x |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 216.70 | 1.0x |'
- en: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 6.42 | 1.17x |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| DeepSpeed-Ulysses | 2x8 | 4K | 64K | 6.42 | 1.17x |'
- en: '| 2x8 | 8K | 128K | 17.47 | 1.18x |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 17.47 | 1.18x |'
- en: '| 2x8 | 16K | 256K | 56.63 | 1.11x |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 56.63 | 1.11x |'
- en: '| 2x8 | 32K | 512K | 202.89 | 1.07x |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 202.89 | 1.07x |'
- en: '| LightSeq | 2x8 | 4K | 64K | 7.03 | 1.07x |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LightSeq | 2x8 | 4K | 64K | 7.03 | 1.07x |'
- en: '| 2x8 | 8K | 128K | 13.12 | 1.57x |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 8K | 128K | 13.12 | 1.57x |'
- en: '| 2x8 | 16K | 256K | 31.33 | 2.00x |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 16K | 256K | 31.33 | 2.00x |'
- en: '| 2x8 | 32K | 512K | 107.76 | 2.01x |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 2x8 | 32K | 512K | 107.76 | 2.01x |'
- en: 'Table 4: Per iteration wall-clock time of LightSeq, Megatron-LM (Korthikanti
    et al., [2023](#bib.bib9)) and DeepSpeed Ulysses (Unit: seconds). Speedup in bold
    denotes the better of the three systems. We calculate the speedup based on Megatron-LM
    iteration time.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: LightSeq、Megatron-LM (Korthikanti 等， [2023](#bib.bib9)) 和 DeepSpeed Ulysses
    的每次迭代墙钟时间（单位：秒）。粗体中的加速比表示三种系统中更优者。我们根据 Megatron-LM 迭代时间计算加速比。'
- en: 'We run a subset of the experiments compared with DeepSpeed-Ulysses. Firstly,
    DeepSpeed-Ulysses does reduce the communication overhead, and thus better than
    Megatron-LM on scenarios listed in Table [4](#A2.T4 "Table 4 ‣ Appendix B Comparison
    with DeepSpeed Ulysses ‣ LightSeq: Sequence Level Parallelism for Distributed
    Training of Long Context Transformers").  LightSeq achieves better performance
    than DeepSpeed-Ulysses on longer sequences or models with a more general number
    of heads (e.g. Llama-33H). We also note that DeepSpeed-Ulysses can not scale beyond
    the number of attention heads because it also relies on sharding the attention
    heads. However, we need to point out that in shorter sequences and MHA models
    (where  LightSeq does not have a communication advantage, compared to GQA/MQA
    models), the communication primitives used in DeepSpeed-Ulysses are more advantageous.
    We leave our further optimization in P2P in shorter sequences and MHA models as
    an exciting future work.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '我们运行了与 DeepSpeed-Ulysses 比较的一部分实验。首先，DeepSpeed-Ulysses 确实减少了通信开销，因此在表 [4](#A2.T4
    "Table 4 ‣ Appendix B Comparison with DeepSpeed Ulysses ‣ LightSeq: Sequence Level
    Parallelism for Distributed Training of Long Context Transformers") 中列出的场景下表现优于
    Megatron-LM。LightSeq 在较长的序列或具有更一般数量的头（例如 Llama-33H）的模型上表现更好。我们还注意到，DeepSpeed-Ulysses
    无法在注意力头数以上进行扩展，因为它还依赖于对注意力头的分片。然而，我们需要指出的是，在较短序列和 MHA 模型中（与 GQA/MQA 模型相比，LightSeq
    并没有通信优势），DeepSpeed-Ulysses 使用的通信原语更具优势。我们将更进一步的优化留待未来的工作中，特别是在 P2P 短序列和 MHA 模型方面。'
