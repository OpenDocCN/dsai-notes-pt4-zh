- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:01:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:01:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一捆稻草的总结：对长上下文 LLMs 和 RAG 系统的挑战
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01370](https://ar5iv.labs.arxiv.org/html/2407.01370)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01370](https://ar5iv.labs.arxiv.org/html/2407.01370)
- en: 'Philippe Laban^†^†thanks:   Equal contribution  Alexander R. Fabbri^($*$)  Caiming
    Xiong  Chien-Sheng Wu'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Philippe Laban^†^†感谢：   同等贡献  Alexander R. Fabbri^($*$)  Caiming Xiong  Chien-Sheng
    Wu
- en: Salesforce AI Research
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Salesforce AI 研究
- en: '{plaban afabbri cxiong wu.jason}@salesforce.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{plaban afabbri cxiong wu.jason}@salesforce.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs and RAG systems are now capable of handling millions of input tokens or
    more. However, evaluating the output quality of such systems on long-context tasks
    remains challenging, as tasks like Needle-in-a-Haystack lack complexity. In this
    work, we argue that summarization can play a central role in such evaluation.
    We design a procedure to synthesize Haystacks of documents, ensuring that specific
    insights repeat across documents. The “Summary of a Haystack” (SummHay) task then
    requires a system to process the Haystack and generate, given a query, a summary
    that identifies the relevant insights and precisely cites the source documents.
    Since we have precise knowledge of what insights should appear in a haystack summary
    and what documents should be cited, we implement a highly reproducible automatic
    evaluation that can score summaries on two aspects – Coverage and Citation. We
    generate Haystacks in two domains (conversation, news), and perform a large-scale
    evaluation of 10 LLMs and corresponding 50 RAG systems. Our findings indicate
    that SummHay is an open challenge for current systems, as even systems provided
    with an Oracle signal of document relevance lag our estimate of human performance
    (56%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like
    GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also
    be used to study enterprise RAG systems and position bias in long-context models.
    We hope future systems can equal and surpass human performance on SummHay.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 和 RAG 系统现在能够处理数百万个输入标记或更多。然而，在长上下文任务上评估此类系统的输出质量仍然具有挑战性，因为像 Needle-in-a-Haystack
    这样的任务缺乏复杂性。在这项工作中，我们认为总结可以在这种评估中发挥核心作用。我们设计了一个程序来合成文档的稻草堆，确保特定的见解在文档中重复出现。然后，“一捆稻草的总结”（SummHay）任务要求系统处理稻草堆，并在给定查询的情况下生成一个总结，识别相关见解并准确引用源文档。由于我们精确知道稻草堆总结中应出现的见解和应引用的文档，因此我们实现了一种高度可重复的自动评估，能够在覆盖度和引用两个方面对总结进行评分。我们在两个领域（对话，新闻）生成了稻草堆，并对
    10 个 LLMs 和 50 个 RAG 系统进行了大规模评估。我们的研究结果表明，SummHay 对当前系统仍然是一个开放的挑战，因为即便是提供了文档相关性的
    Oracle 信号的系统，在联合评分上也落后于我们对人类表现的估计（56%）10 多个百分点。没有检索器的情况下，像 GPT-4o 和 Claude 3 Opus
    这样的长上下文 LLMs 在 SummHay 上的得分低于 20%。我们还展示了 SummHay 可以用于研究企业 RAG 系统和长上下文模型中的位置偏差。我们希望未来的系统能够达到并超越人类在
    SummHay 上的表现。
- en: 'Summary of a Haystack:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一捆稻草的总结：
- en: A Challenge to Long-Context LLMs and RAG Systems
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对长上下文 LLMs 和 RAG 系统的挑战
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent progress in efficient attention mechanisms has led to the expansion of
    the context length of large language models Beltagy et al. ([2020](#bib.bib4));
    Su et al. ([2024](#bib.bib49)). Previous state-of-the-art models such as T5 Raffel
    et al. ([2020](#bib.bib43)) and BART Lewis et al. ([2019](#bib.bib32)) were limited
    to input contexts of 512 or 1024 tokens, while the latest models such as Claude-3
    or Gemini-1.5-pro Reid et al. ([2024](#bib.bib45)) can process sequences of hundreds
    of thousands or millions of tokens.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在高效注意力机制方面的进展导致了大型语言模型上下文长度的扩展（Beltagy et al. ([2020](#bib.bib4)); Su et al.
    ([2024](#bib.bib49))）。以往的最先进模型如 T5（Raffel et al. ([2020](#bib.bib43))）和 BART（Lewis
    et al. ([2019](#bib.bib32))）限制于 512 或 1024 个标记的输入上下文，而最新模型如 Claude-3 或 Gemini-1.5-pro（Reid
    et al. ([2024](#bib.bib45))）可以处理数十万甚至数百万个标记的序列。
- en: Another paradigm, Retrieval Augmented Generation (RAG) Lewis et al. ([2020](#bib.bib33));
    Guu et al. ([2020](#bib.bib17)), has emerged as an alternative to these long-context
    LLMs, proposing a pipelined approach in which a retriever dynamically selects
    the context relevant to a given input query, alleviating the need for the generator
    to process long contexts directly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种范式，检索增强生成（RAG）（Lewis et al. ([2020](#bib.bib33)); Guu et al. ([2020](#bib.bib17))），作为这些长上下文
    LLMs 的替代方案出现，提出了一种管道化的方法，其中检索器动态选择与给定输入查询相关的上下文，从而缓解了生成器直接处理长上下文的需求。
- en: Although both RAG and long-context LLMs offer to solve the common problem of
    answering queries over a large corpus of text, a direct comparison on a common
    task is still lacking, and evaluating such systems is an open challenge. Some
    recent work has popularized tests such as the Needle-in-a-Haystack task Kamradt
    ([2023](#bib.bib23)), which requires models to identify a small piece of information
    in a large document. However, these tasks do not offer the complexity needed to
    differentiate the capabilities of the latest generation of large-language models,
    with several state-of-the-art models achieving near-perfect performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RAG和长上下文LLM都提供了解决在大文本语料库中回答查询的常见问题的方案，但在共同任务上的直接比较仍然缺乏，对此类系统的评估仍然是一个开放的挑战。一些最近的工作推广了诸如Kamradt（[2023](#bib.bib23)）的“针在干草堆中”任务，这要求模型在大文档中识别一小块信息。然而，这些任务并未提供区分最新一代大语言模型能力所需的复杂性，几个最先进的模型已接近完美表现。
- en: In this work, we propose to leverage the task of summarization as a testbed
    for evaluating long-context models and RAG systems. Summarization requires reasoning
    over a long context and a careful understanding of the relative importance of
    content. However, most prior work on summarization evaluation, particularly in
    evaluating the relevance of summaries, has focused on single-document summarization
    or tasks in which the input content is on the order of 1,000-2,000 tokens Laban
    et al. ([2020](#bib.bib28)); Fabbri et al. ([2021a](#bib.bib12)); Bhandari et al.
    ([2020](#bib.bib5)); Liu et al. ([2022](#bib.bib37)). Apart from Chang et al.
    ([2023](#bib.bib6)), which focuses on summary coherence for 100k-token books,
    other evaluation work on longer conversational and multi-document news summarization
    is still often limited to around 10k tokens Zhong et al. ([2021](#bib.bib58));
    Huang et al. ([2023](#bib.bib20)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提议利用摘要任务作为评估长上下文模型和RAG系统的测试平台。摘要需要对长上下文进行推理，并仔细理解内容的相对重要性。然而，以往大多数关于摘要评估的工作，特别是在评估摘要相关性方面，主要集中于单文档摘要或输入内容在1,000-2,000个标记的任务（Laban
    et al. ([2020](#bib.bib28)); Fabbri et al. ([2021a](#bib.bib12)); Bhandari et
    al. ([2020](#bib.bib5)); Liu et al. ([2022](#bib.bib37))）。除了Chang et al. ([2023](#bib.bib6))专注于100k-token书籍的摘要一致性外，其他关于更长对话和多文档新闻摘要的评估工作通常仍限于约10k标记（Zhong
    et al. ([2021](#bib.bib58)); Huang et al. ([2023](#bib.bib20))）。
- en: A central problem is that summarization evaluation often relies on low-quality
    reference summaries and automatic metrics that do not correlate well with human
    judgments. Within reference-based evaluation, a candidate summary is compared
    to a gold-standard reference summary, with the optics that a higher overlap between
    the candidate and reference summary indicates higher quality. This paradigm may
    limit evaluation reliability, due to the lack of gold-standard references, particularly
    in long-context settings where obtaining high-quality summaries would be prohibitively
    expensive. Furthermore, automatic metrics may still fail to correlate well with
    human judgments with respect to these references; despite the human-validated
    pipeline of Huang et al. ([2023](#bib.bib20)), the best automatic metric for content
    coverage in that study has a correlation of just 0.37 with human judgment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个核心问题是，摘要评估通常依赖于低质量的参考摘要和与人类判断不太相关的自动化指标。在基于参考的评估中，一个候选摘要会与黄金标准参考摘要进行比较，假设候选摘要与参考摘要之间的重叠越高，质量越高。这种范式可能限制了评估的可靠性，因为缺乏黄金标准参考，特别是在长上下文的设置中，获取高质量摘要将非常昂贵。此外，自动化指标在这些参考的情况下仍可能未能与人类判断很好地相关；尽管有黄等人（[2023](#bib.bib20)）的人工验证流程，但该研究中用于内容覆盖的最佳自动化指标与人类判断的相关性仅为0.37。
- en: 'In this work, we address these limitations through synthetic data generation.
    An overview of our Framework is found in Figure [1](#S2.F1 "Figure 1 ‣ 2 Related
    Work ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems").
    We propose data synthesis programs to generate a large corpus of documents (the
    “Haystack”) on a given topic. By enforcing that certain units of information (“insights”),
    categorized according to various subtopics, repeat within Haystack documents,
    and precisely controlling which insights occur in which documents, we can automatically
    derive the relevant insights within the Haystack for a given search query. A system
    completing the Summary of a Haystack (SummHay) task must then summarize insights
    relevant to a search query and cite the source documents of each insight. These
    summaries can be evaluated based on whether they cover the expected reference
    insights, and cite precisely and thoroughly the source documents.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们通过合成数据生成来解决这些限制。我们的框架概述见于图[1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣
    Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")。我们提出了数据合成程序，用于生成关于给定主题的大量文档（“Haystack”）。通过强制在Haystack文档中重复某些信息单元（“见解”），根据各种子主题进行分类，并精确控制哪些见解出现在哪些文档中，我们可以自动推导出给定搜索查询的相关见解。完成Haystack总结（SummHay）任务的系统必须总结与搜索查询相关的见解，并引用每个见解的源文档。这些总结可以根据是否涵盖预期的参考见解，以及是否准确且全面地引用源文档来进行评估。'
- en: 'Our first contribution is a procedure for generating Haystacks in two domains:
    conversations and news articles. Section [3](#S3 "3 Summary in a Haystack Framework
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") details
    the carefully-designed pipeline to ensure the feasibility and validity of the
    task. A Haystack typically contains 100 documents on a topic, totaling approximately
    100k tokens. We generate a total of 10 Haystacks, each coupled with roughly 10
    queries, for a total of 92 SummHay tasks. Our pipeline can be scaled and applied
    to other domains.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的第一项贡献是制定了在两个领域生成Haystacks的程序：对话和新闻文章。第[3](#S3 "3 Summary in a Haystack Framework
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")节详细介绍了精心设计的管道，以确保任务的可行性和有效性。一个Haystack通常包含100个主题文档，总计约100k个token。我们生成了10个Haystacks，每个Haystack配有大约10个查询，总共92个SummHay任务。我们的管道可以扩展并应用于其他领域。'
- en: Our second contribution develops SummHay’s evaluation protocol, centering on
    evaluating system outputs on their Coverage of reference insights, and the quality
    of their Citation. A manual annotation confirms strong reproducibility of the
    protocol among knowledgeable annotators (0.77 correlation). We then experiment
    with LLM-based evaluation, finding that although the level of correlation is slightly
    lower (0.71), evaluation cost is reduced by a factor of almost 50.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二项贡献是开发了SummHay的评估协议，重点评估系统输出对参考见解的覆盖范围和引用质量。手动注释确认了该协议在有知识的注释者中具有很强的可重复性（0.77相关性）。然后我们尝试了基于LLM的评估，发现尽管相关性水平略低（0.71），评估成本却降低了近50倍。
- en: 'Our third contribution is an estimate of human performance on SummHay and a
    large-scale evaluation of 50 RAG systems and 10 long-context LLMs. Our findings
    indicate that: (1) SummHay is challenging for all systems we evaluate, with all
    models significantly below our estimate of human performance, even when given
    oracle signals of document relevance; (2) non-trivial trade-offs exist when choosing
    between a RAG pipeline and a long-context LLM, with RAG systems typically improving
    citation quality, at the cost of insight coverage, (3) using advanced RAG components
    (e.g., Cohere’s Rerank3) leads to end-to-end performance boosts on the task, confirming
    that SummHay is a viable option for holistic RAG evaluation, (4) a positional
    bias experiment on SummHay confirms the lost in the middle phenomenon, demonstrating
    that most LLMs are biased towards information at the top or bottom of the context
    window.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第三项贡献是对SummHay的人类表现进行估算，以及对50个RAG系统和10个长上下文LLM的大规模评估。我们的发现表明：（1）SummHay对我们评估的所有系统都具有挑战性，所有模型的表现都显著低于我们对人类表现的估计，即使在给定文档相关性神谕信号的情况下也是如此；（2）选择RAG管道和长上下文LLM之间存在非平凡的权衡，RAG系统通常提高引用质量，但会牺牲见解覆盖范围；（3）使用先进的RAG组件（例如，Cohere的Rerank3）可以提升任务的端到端性能，确认SummHay是一个适用于整体RAG评估的可行选项；（4）对SummHay的定位偏差实验确认了“中间丢失”现象，表明大多数LLM对上下文窗口顶部或底部的信息有偏见。
- en: We open-source our dataset and evaluation methodology¹¹1[https://github.com/salesforce/summary-of-a-haystack](https://github.com/salesforce/summary-of-a-haystack).
    A system that achieves a high score on SummHay can reliably reason over large
    corpora of documents, detect and summarize insights, and accurately cite its sources.
    We anticipate that although our findings indicate that human performance is still
    out of reach, future systems can achieve and surpass such performance, providing
    more reliable and trustworthy answer engines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开源了我们的数据集和评估方法¹¹1[https://github.com/salesforce/summary-of-a-haystack](https://github.com/salesforce/summary-of-a-haystack)。一个在SummHay上取得高分的系统可以可靠地推理大量文档，检测并总结洞察，并准确引用其来源。我们预期，尽管我们的发现表明人类表现仍难以达到，但未来的系统可以实现并超越这种表现，提供更可靠和可信赖的回答引擎。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '![Refer to caption](img/91a0a815cdf22590fbe3d8bf4445cbfc.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/91a0a815cdf22590fbe3d8bf4445cbfc.png)'
- en: 'Figure 1: Diagram illustrating the steps to synthesize a Haystack of documents
    given an input scenario: subtopic and insight creation followed by document generation.
    Once a Haystack is synthesized, it can be used to benchmark LLMs / RAG systems
    on query-focused summarization tasks.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：示意图说明了在给定输入场景下合成文档Haystack的步骤：子主题和洞察的创建以及文档生成。一旦合成了Haystack，它可以用于在查询聚焦的总结任务上对LLM
    / RAG系统进行基准测试。
- en: 2.1 Summarization Evaluation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 总结评估
- en: Existing work in summarization relevance, or coverage, evaluation has largely
    focused on the short-input, single-document setting Gao and Wan ([2022](#bib.bib15));
    Fabbri et al. ([2021b](#bib.bib13)). Extending to long input evaluation, recent
    work has performed meta-evaluation on coherence in book summarization Chang et al.
    ([2023](#bib.bib6)) and faithfulness across several domains Krishna et al. ([2023](#bib.bib25));
    Min et al. ([2023](#bib.bib41)); Zhang et al. ([2024a](#bib.bib56)). For coverage
    evaluation, recent work has studied content selection for book summarization Kim
    et al. ([2024](#bib.bib24)), evaluated a two-step extract-evaluate framework Wu
    et al. ([2023](#bib.bib53)), and compared the correlation of LLM metrics in coverage
    Huang et al. ([2023](#bib.bib20)). We leverage summarization relevance as a test-bed
    for long-context evaluation, and we focus on our synthetic creation pipeline and
    the simplified relevance evaluation that results in a high-correlation automatic
    metric.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的总结相关性或覆盖评估工作主要集中在短输入、单文档的设置上 Gao 和 Wan ([2022](#bib.bib15)); Fabbri et al.
    ([2021b](#bib.bib13))。扩展到长输入评估，近期工作对书籍总结的连贯性进行了元评估 Chang et al. ([2023](#bib.bib6))
    和在多个领域的忠实度 Krishna et al. ([2023](#bib.bib25)); Min et al. ([2023](#bib.bib41));
    Zhang et al. ([2024a](#bib.bib56))。在覆盖评估方面，近期工作研究了书籍总结的内容选择 Kim et al. ([2024](#bib.bib24))，评估了两步提取-评估框架
    Wu et al. ([2023](#bib.bib53))，并比较了LLM度量的覆盖相关性 Huang et al. ([2023](#bib.bib20))。我们利用总结相关性作为长上下文评估的试验平台，并专注于我们的合成创建流程和简化的相关性评估，从而产生高相关性的自动度量。
- en: 2.2 Long-Context LLM Evaluation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 长上下文LLM评估
- en: Needle-in-a-haystack Kamradt ([2023](#bib.bib23)) was proposed to assess the
    long-context recall ability of LLMs. Subsequent work has analyzed the effect of
    needle placement Machlab and Battle ([2024](#bib.bib39)) and multi-needle LangChain
    ([2024](#bib.bib31)) and multi-modal variations Song et al. ([2024](#bib.bib48));
    Reid et al. ([2024](#bib.bib45)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Needle-in-a-haystack Kamradt ([2023](#bib.bib23)) 被提议用于评估LLM的长上下文记忆能力。后续工作分析了针的位置
    Machlab 和 Battle ([2024](#bib.bib39)) 以及多针 LangChain ([2024](#bib.bib31)) 和多模态变体
    Song et al. ([2024](#bib.bib48)); Reid et al. ([2024](#bib.bib45))。
- en: Additionally, several long-context evaluation benchmarks have been created,
    for example by building upon and revising existing tasks and datasets Bai et al.
    ([2023](#bib.bib3)); An et al. ([2023](#bib.bib2)) Some work proposes ways to
    extend the context length of shorter-context datasets; Kuratov et al. ([2024](#bib.bib26));
    Kwan et al. ([2023](#bib.bib27)), while other work addresses data contamination
    in long-context settings Ni et al. ([2024](#bib.bib42)); Dong et al. ([2023](#bib.bib11)).
    Several papers introduce synthetic data in additional to existing tasks Shaham
    et al. ([2023](#bib.bib47)); Zhang et al. ([2024b](#bib.bib57)), which can prove
    to be more difficult for current models, as seen in Bai et al. ([2023](#bib.bib3)).
    Our benchmark focuses on synthetic data on the scale of 100k input tokens, and
    as opposed to existing synthetic tasks centered largely around retrieving, counting,
    or sorting, our summarization task requires aggregating and non-trivial reasoning
    over the long-context.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，已经创建了若干长上下文评估基准，例如通过在现有任务和数据集的基础上进行构建和修订 Bai et al. ([2023](#bib.bib3));
    An et al. ([2023](#bib.bib2)) 一些工作提出了扩展短上下文数据集的上下文长度的方法；Kuratov et al. ([2024](#bib.bib26));
    Kwan et al. ([2023](#bib.bib27))，而其他工作则处理了长上下文设置中的数据污染 Ni et al. ([2024](#bib.bib42));
    Dong et al. ([2023](#bib.bib11))。若干论文在现有任务的基础上引入了合成数据 Shaham et al. ([2023](#bib.bib47));
    Zhang et al. ([2024b](#bib.bib57))，这对当前模型可能更具挑战性，如 Bai et al. ([2023](#bib.bib3))
    中所见。我们的基准专注于规模为 100k 输入标记的合成数据，与现有的合成任务主要围绕检索、计数或排序不同，我们的总结任务需要在长上下文中进行聚合和复杂的推理。
- en: 2.3 Attribution Evaluation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 引证评估
- en: Several benchmarks have emerged to study the ability of LLMs to ground their
    generation with citations Li et al. ([2023a](#bib.bib34)). AttributionBench Li
    et al. ([2024](#bib.bib36)) aggregates 7 existing attribution datasets, including
    Hagrid Kamalloo et al. ([2023](#bib.bib22)), consisting of generative answers
    to questions annotated by humans for attribution, and AttrEval-GenSearch Yue et al.
    ([2023](#bib.bib54)), which categorizes attribution into three levels of support.
    Attribution evaluation has also been performed along sources beyond documents
    such as knowledge graphs Hu et al. ([2024](#bib.bib19)); Li et al. ([2023b](#bib.bib35))
    and for tasks such as long-form question-answering Chen et al. ([2023b](#bib.bib8)).
    Specific to summarization, Seahorse Clark et al. ([2023](#bib.bib10)) collects
    annotations for summary attribution in the short-context setting. In this paper
    we study attribution, or citation, in the context of long-input summarization.
    Due to our synthetically generated data, we can trace reference insights to their
    sources and directly evaluate summary citations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究 LLM 生成内容的引证能力，已经出现了若干基准 Li et al. ([2023a](#bib.bib34))。AttributionBench
    Li et al. ([2024](#bib.bib36)) 汇总了 7 个现有的引证数据集，包括 Hagrid Kamalloo et al. ([2023](#bib.bib22))，其中包含由人工标注的生成答案，用于引证，以及
    AttrEval-GenSearch Yue et al. ([2023](#bib.bib54))，该数据集将引证分为三种支持级别。引证评估也已在文档之外的来源如知识图谱
    Hu et al. ([2024](#bib.bib19)); Li et al. ([2023b](#bib.bib35)) 以及长篇问答任务 Chen
    et al. ([2023b](#bib.bib8)) 中进行。针对总结，Seahorse Clark et al. ([2023](#bib.bib10))
    收集了短上下文设置中的总结引证注释。在本文中，我们研究了长输入总结中的引证或引用。由于我们使用了合成生成的数据，我们可以追溯参考见解到其来源，并直接评估总结引用。
- en: 3 Summary in a Haystack Framework
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Haystack 框架中的总结
- en: '![Refer to caption](img/888c338caa169426aa72aa71f9332527.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/888c338caa169426aa72aa71f9332527.png)'
- en: 'Figure 2: Example evaluation of a candidate summary (right) for its coverage
    of reference insights (left). Each reference insight is assigned a Coverage Score
    by mapping it to a single candidate bullet. A mapped bullet’s citations are used
    to calculate the Citation Score. The total score is the average across reference
    insights. See Appendix [A.7](#A1.SS7 "A.7 Additional Output Examples ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    for four additional examples.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：示例评估候选总结（右）对参考见解的覆盖情况（左）。每个参考见解通过将其映射到一个候选项目来分配一个覆盖分数。映射的项目的引用用于计算引用分数。总分是参考见解分数的平均值。有关四个额外示例，请参见附录
    [A.7](#A1.SS7 "A.7 附加输出示例 ‣ 附录 A 附录 ‣ 对 Haystack 的总结：对长上下文 LLM 和 RAG 系统的挑战")。
- en: 'Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems") illustrates the process of synthesizing
    Haystack data, and the task, which we now detail.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S2.F1 "图 1 ‣ 2 相关工作 ‣ 对 Haystack 的总结：对长上下文 LLM 和 RAG 系统的挑战") 说明了合成 Haystack
    数据的过程和任务，我们现在将详细介绍。
- en: 3.1 Preliminaries
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步概述
- en: In SummHay, as in the needle-in-a-haystack task, the LLM responds to a query,
    but here it must generate a long-form answer (200-300 words) that requires identifying
    and summarizing insights that repeat across documents and citing source documents.
    The task resembles long-form question-answering Fan et al. ([2019](#bib.bib14))
    and query-focused summarization Zhong et al. ([2021](#bib.bib58)); Vig et al.
    ([2022](#bib.bib51)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在SummHay中，与针在草堆任务类似，LLM响应查询，但这里必须生成一个长篇回答（200-300字），需要识别和总结在文档中重复的见解，并引用源文档。该任务类似于长篇问答
    Fan et al. ([2019](#bib.bib14)) 和查询聚焦摘要 Zhong et al. ([2021](#bib.bib58)); Vig
    et al. ([2022](#bib.bib51))。
- en: In the following section, we describe the Haystack synthesis, the steps taken
    to ensure the quality of our benchmark, and the task framing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们描述了Haystack合成、确保基准质量的步骤以及任务框架。
- en: 3.2 Haystack Generation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Haystack生成
- en: 'Subtopic and Insight Generation (Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), left)
    One of the main motivations behind synthetically generating documents is to precisely
    control information distribution in the documents.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '子主题和见解生成（图 [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary of a Haystack: A
    Challenge to Long-Context LLMs and RAG Systems"), 左侧）合成生成文档的主要动机之一是精确控制文档中的信息分布。'
- en: 'A Haystack centers around a topic (e.g., “three students discuss strategies
    for an upcoming exam”). The first step generates a list of potential subtopics
    that can occur in documents about the topic. Subtopics are generic (e.g., students
    discussing study techniques, or how to manage stress). There are two subtopic
    requirements: (1) each subtopic should be distinctive and unique, such that no
    two subtopics overlap thematically, and (2) subtopics should be expandable into
    at least three distinct insights that are specific to the subtopic. Appendix [A.1](#A1.SS1
    "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems") goes over the quality assurance
    to ensure the satisfaction of these requirements.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '一个Haystack围绕一个主题（例如，“三名学生讨论即将到来的考试策略”）展开。第一步生成可能在关于该主题的文档中出现的潜在子主题列表。子主题是通用的（例如，学生讨论学习技巧或如何管理压力）。有两个子主题要求：（1）每个子主题应具有独特性，以至于没有两个子主题在主题上重叠，（2）子主题应能扩展为至少三个与子主题特定的独立见解。附录 [A.1](#A1.SS1
    "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems") 详细介绍了质量保证以确保满足这些要求。'
- en: 'In a second step, each subtopic gets instantiated into a list of specific insights,
    or facts that will be placed into the documents of the Haystack. Insights are
    defined as statements that contain specific information that may appear in a document
    about a given subtopic. Insights are expected to mention a number, a date, or
    an entity. For example, in the “Managing stress” subtopic, an insight can be:
    “a student explaining what the 25-5 Pomodoro technique is to the others”. Crucially,
    insights should be specific, independent of each other, and solely relevant to
    a single subtopic. Appendix [A.1.2](#A1.SS1.SSS2 "A.1.2 Insight Verification ‣
    A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems") goes over the quality assurance
    to ensure insight quality.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '在第二步中，每个子主题被具体化为一个特定见解的列表，这些见解或事实将被放置到Haystack的文档中。见解被定义为包含特定信息的陈述，这些信息可能会出现在关于给定子主题的文档中。见解预计会提到一个数字、一个日期或一个实体。例如，在“管理压力”子主题中，一个见解可以是：“一名学生向其他人解释25-5番茄钟技术”。关键是，见解应该具体、彼此独立，并且仅与单一子主题相关。附录 [A.1.2](#A1.SS1.SSS2
    "A.1.2 Insight Verification ‣ A.1 Haystack Synthesis Details ‣ Appendix A Appendix
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") 详细介绍了质量保证以确保见解质量。'
- en: The idea of breaking down documents into smaller information units has proven
    beneficial in recent work for both automatic and human evaluation Min et al. ([2023](#bib.bib41));
    Liu et al. ([2022](#bib.bib37)). Concretely, we use an LLM to generate subtopics
    and insights, optionally include context documents to provide seed ideas to the
    LLM, and aim to generate 10 subtopics, each with about 5-10 insights.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档拆分成更小的信息单元的理念在最近的工作中对自动和人工评估都证明是有益的 Min et al. ([2023](#bib.bib41)); Liu
    et al. ([2022](#bib.bib37))。具体来说，我们使用LLM生成子主题和见解，选择性地包括上下文文档以为LLM提供初步想法，并目标生成10个子主题，每个子主题包含大约5-10个见解。
- en: 'Document Generation (Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), center)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 文档生成（图 [1](#S2.F1 "图 1 ‣ 2 相关工作 ‣ Haystack 总结：对长上下文 LLM 和 RAG 系统的挑战")，居中）
- en: 'The Haystack is synthesized one document at a time. For each document, we randomly
    select a set of insights across subtopics, and instruct an LLM to generate a document
    that must include all selected insights. The number of insights to include per
    document varies based on the domain, targeting 750 words of content per document
    (or roughly 1,000 tokens). By generating 100 documents, a Haystack totals on the
    order of 100k tokens. Appendix [A.1](#A1.SS1 "A.1 Haystack Synthesis Details ‣
    Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") details quality assurance that ensures documents are realistic
    and unique, and that each insight occurs within 5+ documents.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Haystack 是一次生成一个文档。对于每个文档，我们随机选择一组跨子主题的洞察，并指示 LLM 生成必须包含所有选定洞察的文档。每个文档包含的洞察数量根据领域的不同而有所变化，目标是每个文档
    750 字的内容（或大约 1000 个标记）。通过生成 100 个文档，Haystack 总计大约 100k 个标记。附录 [A.1](#A1.SS1 "A.1
    Haystack 合成细节 ‣ 附录 A 附录 ‣ Haystack 总结：对长上下文 LLM 和 RAG 系统的挑战") 详细介绍了质量保证，确保文档是现实的和独特的，并且每个洞察在
    5 个以上的文档中出现。
- en: 'Evaluation of the SummHay task relies on precise knowledge of the mapping between
    insights and documents in the Haystack. We implement over five domain-specific
    verification processes during the synthesis of the Haystack to ensure that the
    expected mapping is sound. Manual inspection and the high performance of human
    annotators on the final task, shown in Section [5.3](#S5.SS3 "5.3 Estimating Human
    Performance ‣ 5 Results ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") provide evidence of the quality of the resulting Haystacks.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: SummHay 任务的评估依赖于对 Haystack 中洞察与文档之间映射的精确了解。我们在合成 Haystack 过程中实施了五个以上领域特定的验证流程，以确保期望的映射是有效的。手动检查以及最终任务中人工标注者的高性能（见第
    [5.3](#S5.SS3 "5.3 估计人工性能 ‣ 5 结果 ‣ Haystack 总结：对长上下文 LLM 和 RAG 系统的挑战") 节）提供了生成的
    Haystack 质量的证据。
- en: '3.3 Haystack Summarization (Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣
    Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), right)'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 Haystack 摘要（图 [1](#S2.F1 "图 1 ‣ 2 相关工作 ‣ Haystack 总结：对长上下文 LLM 和 RAG 系统的挑战")，右侧）
- en: Having generated Haystacks following the above protocol, we can now leverage
    them for the Summary of a Haystack task. Using an LLM, we transform each subtopic
    into a query (e.g., “What do the students discuss regarding stress management?”).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述协议生成了 Haystack 后，我们可以利用它们来进行 Haystack 总结任务。使用 LLM，我们将每个子主题转化为查询（例如，“学生们讨论了什么关于压力管理的内容？”）。
- en: 'Each system completing the task is instructed to generate a summary to answer
    the query (which focuses on a single subtopic), which must be in bullet-point
    format. Crucially, we instruct the LLM on the number of bullet points that the
    summary should contain, which matches the number of insights of the subtopic.
    Although this can appear as a simplifying assumption, this important design choice
    allows us to control for the length of generated summaries, which are a known
    confounding factor in summarization evaluation Liu et al. ([2022](#bib.bib37)).
    We find in Section [5](#S5 "5 Results ‣ Summary of a Haystack: A Challenge to
    Long-Context LLMs and RAG Systems") that this choice effectively controls the
    length of generated summaries. The prompt also instructs the system to cite source
    documents in each of its bullet points, in a specific bracketed format (e.g.,
    [1,2]), using document identifiers provided in the Haystack. The bullet-point
    structure and specific citation format are the foundation for our evaluation,
    detailed in Section [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack: A
    Challenge to Long-Context LLMs and RAG Systems").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个完成任务的系统都被指示生成一个摘要以回答查询（该查询集中在一个子主题上），摘要必须采用项目符号格式。至关重要的是，我们指示 LLM 摘要应包含的项目符号数量，这与子主题的洞察数量匹配。虽然这看起来像是一个简化假设，但这一重要的设计选择使我们能够控制生成摘要的长度，而长度是总结评估中的已知干扰因素（Liu
    等人，[2022](#bib.bib37)）。我们在第 [5](#S5 "5 结果 ‣ Haystack 总结：对长上下文 LLM 和 RAG 系统的挑战")
    节中发现，这一选择有效地控制了生成摘要的长度。提示还指示系统在每个项目符号中引用源文档，采用特定的括号格式（例如，[1,2]），使用在 Haystack 中提供的文档标识符。项目符号结构和特定的引用格式是我们评估的基础，详见第
    [4](#S4 "4 评估协议 ‣ Haystack 总结：对长上下文 LLM 和 RAG 系统的挑战") 节。
- en: 3.4 SummHay Benchmark
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 SummHay 基准
- en: 'We instantiate the above protocol across two domains: conversations and news,
    as these two domains are common test beds for summarization Hermann et al. ([2015](#bib.bib18));
    Gliwa et al. ([2019](#bib.bib16)). For each domain, we generate 5 Haystacks, and
    the average Haystack length is 93k tokens. Each Haystack consists of on average
    9.20 subtopics, each averaging 6.75 insights, for a total of 62 insights per topic.
    For the news domain, we leverage the documents from Huang et al. ([2023](#bib.bib20))
    as the seed context documents. Regarding LLM choice, we rely on a combination
    of GPT-3.5 and GPT-4o and specify additional details in Appendix [A.1](#A1.SS1
    "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在两个领域中实施上述协议：对话和新闻，因为这两个领域是摘要的常见测试环境 Hermann et al. ([2015](#bib.bib18));
    Gliwa et al. ([2019](#bib.bib16))。在每个领域，我们生成5个Haystacks，每个Haystack的平均长度为93k tokens。每个Haystack平均包含9.20个子主题，每个子主题平均有6.75个见解，总共62个见解。对于新闻领域，我们利用
    Huang et al. ([2023](#bib.bib20)) 的文档作为种子上下文文档。关于LLM的选择，我们依赖于GPT-3.5和GPT-4o的组合，并在附录
    [A.1](#A1.SS1 "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") 中指定了额外的细节。'
- en: '| Method | Cov. Corr. | Link Acc. | Cost ($) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 覆盖率相关性 | 链接准确性 | 成本 ($) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Manual Annot. | 0.770 | 95.0 | $325 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 手动标注 | 0.770 | 95.0 | $325 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Gemini-1.5-pro | 0.751 | 89.3 | $15.1 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-pro | 0.751 | 89.3 | $15.1 |'
- en: '| GPT-4o (9FS) | 0.719 | 89.2 | $26.1 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (9FS) | 0.719 | 89.2 | $26.1 |'
- en: '| GPT-4o | 0.716 | 88.9 | $6.9 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.716 | 88.9 | $6.9 |'
- en: '| Claude 3 Opus | 0.677 | 87.9 | $23.8 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | 0.677 | 87.9 | $23.8 |'
- en: '| Claude 3 Haiku | 0.498 | 87.7 | $0.4 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku | 0.498 | 87.7 | $0.4 |'
- en: '| GPT3.5 | 0.495 | 86.7 | $1.3 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5 | 0.495 | 86.7 | $1.3 |'
- en: 'Table 1: Reproducibility and cost of manual and automated evaluation for SummHay.
    We compute coverage correlation, linking accuracy, and evaluation cost.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：SummHay的手动和自动评估的可重复性和成本。我们计算覆盖率相关性、链接准确性和评估成本。
- en: 4 Evaluation Protocol
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估协议
- en: 'We first define task metrics (illustrated in Figure [2](#S3.F2 "Figure 2 ‣
    3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems")), establish the reproducibility of manual annotation, and
    then assess the quality of automated evaluation.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先定义任务指标（如图 [2](#S3.F2 "Figure 2 ‣ 3 Summary in a Haystack Framework ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") 所示），建立手动标注的可重复性，然后评估自动评估的质量。'
- en: 4.1 Evaluation Metrics
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 评估指标
- en: 'Coverage Metric Given a candidate subtopic summary, we extract each bullet
    point (split on line breaks) and want to measure the overlap between these bullets
    and the subtopic’s reference insights. To do so, we iterate over each reference
    insight and assess whether the reference insight is fully, partially, or not covered
    in any of the candidate bullet points. For each insight, the summary receives
    a score of 100 for full coverage, 50 for partial coverage, and 0 otherwise. The
    final coverage score of a summary is the average coverage on all the insights
    of the subtopic, such that it ranges from 0 to 100\. In Figure [2](#S3.F2 "Figure
    2 ‣ 3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge to
    Long-Context LLMs and RAG Systems"), the top reference insight is fully covered
    by the second candidate bullet, the second insight is partially covered by the
    first candidate bullet, and the third insight is not covered in the summary. The
    Coverage Score is: $(100+50+0)/3=50$.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '覆盖率指标 给定一个候选子主题摘要，我们提取每个要点（按换行符分割），并测量这些要点与子主题参考见解之间的重叠。为此，我们遍历每个参考见解，评估参考见解是否在任何候选要点中被完全、部分或未覆盖。对于每个见解，摘要得到100分表示完全覆盖，50分表示部分覆盖，0分表示未覆盖。摘要的最终覆盖率得分是所有子主题见解的平均覆盖率，范围从0到100。在图
    [2](#S3.F2 "Figure 2 ‣ 3 Summary in a Haystack Framework ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems") 中，顶级参考见解被第二个候选要点完全覆盖，第二个见解被第一个候选要点部分覆盖，而第三个见解未被摘要覆盖。覆盖率得分为：$(100+50+0)/3=50$。'
- en: 'Citation Metric Because documents of the Haystack are synthesized, each reference
    insight can be traced to a gold-standard set of documents that contain the insight.
    When a summary’s bullet covers a reference insight, we can compare generated citations
    to this reference set of cites. For each partially or fully covered reference
    insight, cited documents are extracted from the paired summary bullet point using
    a regular expression ([.*]), and we measure the precision and recall between the
    generated and gold-standard cites. The Citation Score of a reference insight is
    calculated as the F1 score of the precision and recall, giving equal weight to
    both. In short, a system must be both precise and thorough in its citing to achieve
    a high Citation Score. The Citation Score of an entire summary is then the average
    insight Citation Score of all reference insights that were covered by the system.
    In Figure 2, the average Citation F1 of the two covered bullets is: $(29+73)/2=51$.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**引用度量**（Citation Metric）由于Haystack中的文档是合成的，因此每个参考见解可以追溯到包含该见解的黄金标准文档集。当一个摘要的条目涵盖了一个参考见解时，我们可以将生成的引用与此参考引用集进行比较。对于每个部分或完全覆盖的参考见解，通过正则表达式（[.*]）从配对的摘要条目中提取引用文档，然后我们测量生成的引用与黄金标准引用之间的精确度和召回率。参考见解的引用分数计算为精确度和召回率的F1分数，两者赋予相等的权重。简而言之，一个系统必须在引用中既精确又全面才能获得高引用分数。整个摘要的引用分数则是系统覆盖的所有参考见解的平均引用分数。在图2中，两个覆盖条目的平均引用F1为：$(29+73)/2=51$。'
- en: 'Joint Metric The Joint Metric pieces Coverage and Citation Scores together,
    measuring whether a candidate summary both covers the expected insights and cites
    documents appropriately. The Joint Score of a summary is calculated by iterating
    over each reference insight and multiplying its coverage score and citation scores
    (assigning a Citation Score of 0 in case the insight is not covered). The Joint
    Score of a summary ranges from 0 to 100\. In Figure [2](#S3.F2 "Figure 2 ‣ 3 Summary
    in a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems"), the summary’s Joint Score is: $(100*0.29+50*0.73+0*0)/3=21.8$.
    Appendix [A.7](#A1.SS7 "A.7 Additional Output Examples ‣ Appendix A Appendix ‣
    Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") provides
    four additional examples on the same subtopic with added details.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**联合度量**（Joint Metric）将覆盖度（Coverage）和引用分数（Citation Scores）结合在一起，衡量候选摘要是否既涵盖了预期的见解，又适当地引用了文献。摘要的联合分数通过迭代每个参考见解并将其覆盖分数和引用分数相乘来计算（如果见解没有被覆盖，则分配一个引用分数为0）。摘要的联合分数范围从0到100。在图[2](#S3.F2
    "图 2 ‣ 3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems")中，摘要的联合分数为：$(100*0.29+50*0.73+0*0)/3=21.8$。附录[A.7](#A1.SS7
    "A.7 Additional Output Examples ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems")提供了关于同一子主题的四个附加示例，并添加了详细信息。'
- en: '|   width 10pt | Coverage Score ($\uparrow$) |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|   宽度 10pt | 覆盖分数（$↑$） |  |'
- en: '| Summarizer  width 10pt | Rand | Vect | LongE | KWs | RR3 | Orac   width 5pt
    | Full   width 10pt | Rand | Vect | LongE | KWs | RR3 | Orac   width 5pt | Full
      width 10pt | Rand | Vect | LongE | Kws | RR3 | Orac   width 5pt | Full | #$W_{b}$
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 总结器  宽度 10pt | Rand | Vect | LongE | KWs | RR3 | Orac  宽度 5pt | Full  宽度
    10pt | Rand | Vect | LongE | KWs | RR3 | Orac  宽度 5pt | Full  宽度 10pt | Rand |
    Vect | LongE | Kws | RR3 | Orac  宽度 5pt | Full | #$W_{b}$ |'
- en: '| GPT3.5  width 10pt | 36.2 | 45.8 | 46.0 | 48.4 | 51.9 | 56.2   width 5pt
    | –   width 10pt | 9.3 | 15.2 | 15.0 | 15.9 | 16.8 | 23.0   width 5pt | –   width
    10pt | 3.6 | 7.3 | 7.2 | 7.9 | 9.0 | 13.2   width 5pt | – | 28.2 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5  宽度 10pt | 36.2 | 45.8 | 46.0 | 48.4 | 51.9 | 56.2  宽度 5pt | –  宽度
    10pt | 9.3 | 15.2 | 15.0 | 15.9 | 16.8 | 23.0  宽度 5pt | –  宽度 10pt | 3.6 | 7.3
    | 7.2 | 7.9 | 9.0 | 13.2  宽度 5pt | – | 28.2 |'
- en: '| Claude 3 Haiku  width 10pt | 49.9 | 64.9 | 62.3 | 63.4 | 66.6 | 72.1   width
    5pt | 62.3   width 10pt | 13.4 | 25.1 | 25.5 | 26.5 | 28.8 | 35.6   width 5pt
    | 14.1   width 10pt | 7.1 | 17.4 | 17.2 | 17.7 | 20.1 | 26.8   width 5pt | 9.2
    | 31.9 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku  宽度 10pt | 49.9 | 64.9 | 62.3 | 63.4 | 66.6 | 72.1  宽度 5pt
    | 62.3  宽度 10pt | 13.4 | 25.1 | 25.5 | 26.5 | 28.8 | 35.6  宽度 5pt | 14.1  宽度 10pt
    | 7.1 | 17.4 | 17.2 | 17.7 | 20.1 | 26.8  宽度 5pt | 9.2 | 31.9 |'
- en: '| GPT4-turbo  width 10pt | 49.4 | 61.0 | 56.7 | 61.2 | 61.8 | 67.1   width
    5pt | 57.9   width 10pt | 17.9 | 28.6 | 28.1 | 31.1 | 31.8 | 41.4   width 5pt
    | 5.5   width 10pt | 9.6 | 18.7 | 16.9 | 20.1 | 20.6 | 28.9   width 5pt | 3.2
    | 37.9 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo  宽度 10pt | 49.4 | 61.0 | 56.7 | 61.2 | 61.8 | 67.1  宽度 5pt | 57.9  宽度
    10pt | 17.9 | 28.6 | 28.1 | 31.1 | 31.8 | 41.4  宽度 5pt | 5.5  宽度 10pt | 9.6 |
    18.7 | 16.9 | 20.1 | 20.6 | 28.9  宽度 5pt | 3.2 | 37.9 |'
- en: '| Command-r  width 10pt | 47.0 | 54.8 | 53.5 | 56.0 | 55.2 | 60.4   width 5pt
    | 50.3   width 10pt | 17.7 | 34.6 | 34.9 | 37.5 | 40.4 | 53.8   width 5pt | 30.9
      width 10pt | 8.9 | 19.6 | 19.6 | 21.9 | 23.6 | 33.9   width 5pt | 16.2 | 33.1
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Command-r  宽度 10pt | 47.0 | 54.8 | 53.5 | 56.0 | 55.2 | 60.4  宽度 5pt | 50.3  宽度
    10pt | 17.7 | 34.6 | 34.9 | 37.5 | 40.4 | 53.8  宽度 5pt | 30.9  宽度 10pt | 8.9 |
    19.6 | 19.6 | 21.9 | 23.6 | 33.9  宽度 5pt | 16.2 | 33.1 |'
- en: '| Gemini-1.5-flash  width 10pt | 49.7 | 58.1 | 58.9 | 61.8 | 62.6 | 65.1   width
    5pt | 59.4   width 10pt | 17.4 | 31.9 | 31.8 | 34.2 | 43.6 | 51.7   width 5pt
    | 32.8   width 10pt | 9.2 | 19.4 | 20.0 | 22.0 | 28.7 | 34.9   width 5pt | 21.0
    | 31.6 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-flash  宽度 10pt | 49.7 | 58.1 | 58.9 | 61.8 | 62.6 | 65.1  宽度 5pt
    | 59.4  宽度 10pt | 17.4 | 31.9 | 31.8 | 34.2 | 43.6 | 51.7  宽度 5pt | 32.8  宽度 10pt
    | 9.2 | 19.4 | 20.0 | 22.0 | 28.7 | 34.9  宽度 5pt | 21.0 | 31.6 |'
- en: '| Command-r +  width 10pt | 44.2 | 56.4 | 53.1 | 56.2 | 58.9 | 61.0   width
    5pt | 44.5   width 10pt | 20.4 | 41.7 | 41.7 | 43.1 | 46.8 | 60.2   width 5pt
    | 19.9   width 10pt | 9.6 | 24.7 | 24.0 | 25.7 | 29.3 | 38.3   width 5pt | 9.7
    | 25.5 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Command-r +  宽度 10pt | 44.2 | 56.4 | 53.1 | 56.2 | 58.9 | 61.0  宽度 5pt |
    44.5  宽度 10pt | 20.4 | 41.7 | 41.7 | 43.1 | 46.8 | 60.2  宽度 5pt | 19.9  宽度 10pt
    | 9.6 | 24.7 | 24.0 | 25.7 | 29.3 | 38.3  宽度 5pt | 9.7 | 25.5 |'
- en: '| Claude 3 Sonnet  width 10pt | 55.8 | 70.6 | 69.7 | 72.1 | 73.1 | 77.7   width
    5pt | 73.6   width 10pt | 18.0 | 34.9 | 36.6 | 37.3 | 41.1 | 51.7   width 5pt
    | 23.5   width 10pt | 11.0 | 26.1 | 27.2 | 28.5 | 31.4 | 41.2   width 5pt | 18.3
    | 33.5 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet  宽度 10pt | 55.8 | 70.6 | 69.7 | 72.1 | 73.1 | 77.7  宽度 5pt
    | 73.6  宽度 10pt | 18.0 | 34.9 | 36.6 | 37.3 | 41.1 | 51.7  宽度 5pt | 23.5  宽度 10pt
    | 11.0 | 26.1 | 27.2 | 28.5 | 31.4 | 41.2  宽度 5pt | 18.3 | 33.5 |'
- en: '| Claude 3 Opus  width 10pt | 56.5 | 72.4 | 69.6 | 72.5 | 76.5 | 81.4   width
    5pt | 76.2   width 10pt | 17.7 | 34.3 | 35.8 | 37.3 | 39.4 | 50.7   width 5pt
    | 22.3   width 10pt | 11.1 | 26.5 | 26.7 | 28.6 | 31.9 | 42.5   width 5pt | 18.0
    | 29.3 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus  宽度 10pt | 56.5 | 72.4 | 69.6 | 72.5 | 76.5 | 81.4  宽度 5pt
    | 76.2  宽度 10pt | 17.7 | 34.3 | 35.8 | 37.3 | 39.4 | 50.7  宽度 5pt | 22.3  宽度 10pt
    | 11.1 | 26.5 | 26.7 | 28.6 | 31.9 | 42.5  宽度 5pt | 18.0 | 29.3 |'
- en: '| GPT-4o  width 10pt | 54.0 | 67.1 | 67.8 | 66.6 | 70.4 | 76.6   width 5pt
    | 66.1   width 10pt | 21.9 | 38.4 | 38.0 | 38.6 | 41.3 | 54.6   width 5pt | 16.2
      width 10pt | 12.6 | 27.3 | 27.6 | 27.3 | 30.8 | 43.4   width 5pt | 11.4 | 36.5
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o  宽度 10pt | 54.0 | 67.1 | 67.8 | 66.6 | 70.4 | 76.6  宽度 5pt | 66.1  宽度
    10pt | 21.9 | 38.4 | 38.0 | 38.6 | 41.3 | 54.6  宽度 5pt | 16.2  宽度 10pt | 12.6
    | 27.3 | 27.6 | 27.3 | 30.8 | 43.4  宽度 5pt | 11.4 | 36.5 |'
- en: '| Gemini-1.5-pro  width 10pt | 53.0 | 63.5 | 64.9 | 63.6 | 68.4 | 67.6   width
    5pt | 70.0   width 10pt | 21.9 | 43.1 | 44.5 | 46.6 | 49.7 | 64.1   width 5pt
    | 51.0   width 10pt | 12.3 | 28.6 | 31.0 | 30.8 | 36.0 | 44.6   width 5pt | 37.8
    | 30.2 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-pro  宽度 10pt | 53.0 | 63.5 | 64.9 | 63.6 | 68.4 | 67.6  宽度 5pt
    | 70.0  宽度 10pt | 21.9 | 43.1 | 44.5 | 46.6 | 49.7 | 64.1  宽度 5pt | 51.0  宽度 10pt
    | 12.3 | 28.6 | 31.0 | 30.8 | 36.0 | 44.6  宽度 5pt | 37.8 | 30.2 |'
- en: '| Human Perf.  width 10pt | – | – | – | – | – | 74.5   width 5pt | –   width
    10pt | – | – | – | – | – | 73.9   width 5pt | –   width 10pt | – | – | – | – |
    – | 56.1   width 5pt | – | 29.7 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Human Perf.  宽度 10pt | – | – | – | – | – | 74.5  宽度 5pt | –  宽度 10pt | –
    | – | – | – | – | 73.9  宽度 5pt | –  宽度 10pt | – | – | – | – | – | 56.1  宽度 5pt
    | – | 29.7 |'
- en: 'Table 2: Summary of a Haystack results of human performance, RAG systems, and
    Long-Context LLMs. Results are reported using three metrics: Coverage (left),
    Citation (center), and Joint (right) scores. Full corresponds to model performance
    when inputting the entire Haystack, whereas Rand, Vect, LongE, KWs, RR3, Orac
    correspond to retrieval components RAG systems. Models ranked by Oracle Joint
    Score. For each model, #$W_{b}$ report the average number of words per bullet
    point.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：关于人类表现、RAG 系统和长上下文 LLM 的 Haystack 结果总结。结果使用三种指标报告：覆盖率（左）、引用（中）和联合（右）得分。Full
    对应于输入整个 Haystack 时的模型表现，而 Rand、Vect、LongE、KWs、RR3、Orac 对应于检索组件 RAG 系统。模型按 Oracle
    联合得分排名。每个模型中，#$W_{b}$ 报告每个项目符号的平均字数。
- en: 4.2 Annotation Reproducibility
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 注释再现性
- en: 'To establish the reproducibility of the evaluation protocol, two authors of
    the paper and two professional annotators independently annotated a common subset
    of 35 summaries, annotating for the coverage of 240 insights within the summaries.
    The Manual Annotation row of Table [1](#S3.T1 "Table 1 ‣ 3.4 SummHay Benchmark
    ‣ 3 Summary in a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") reports the inter-annotator agreement levels on the 240
    coverage judgments. Coverage Score averaged a correlation of 0.77 across pairs
    of annotators, indicating a strong level of agreement between participants. When
    annotators agree that a reference insight is covered, they agree on which candidate
    bullet originates the coverage in 95% of cases (Linking Accuracy). In short, annotators
    have strong agreement on which reference insights are covered by a given candidate
    summary and also agree on the specific bullets in the candidate summary that cover
    each insight.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '为了建立评估协议的可重复性，本文的两位作者和两名专业标注员独立标注了35个总结的共同子集，标注了总结中240个见解的覆盖情况。表 [1](#S3.T1
    "Table 1 ‣ 3.4 SummHay Benchmark ‣ 3 Summary in a Haystack Framework ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")的手动标注行报告了对240个覆盖判断的标注员间一致性水平。覆盖分数在标注员对之间的相关性平均为0.77，表示参与者之间有很强的一致性。当标注员一致认为一个参考见解被覆盖时，他们在95%的情况下同意哪个候选要点起源于该覆盖（链接准确性）。简而言之，标注员在给定候选总结覆盖哪些参考见解以及哪些具体要点覆盖每个见解上达成了高度一致。'
- en: Annotating a single summary takes 4 minutes on average. To reduce evaluation
    costs and scale experiments, we next investigate LLM-based evaluation as an alternative
    to annotation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 标注一个单一的总结平均需要4分钟。为了减少评估成本并扩大实验规模，我们接下来调查基于LLM的评估作为标注的替代方案。
- en: 4.3 Automatic Metric Validation
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 自动度量验证
- en: We recruited two professional annotators to annotate 200 candidate summaries
    (100 for each of the news and conversational domains) paired with reference insights.
    Annotators were compensated at $25/hour, and annotation required a total of 13
    hours of work, for a total of $325.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们招聘了两名专业标注员来标注200个候选总结（每个新闻和对话领域各100个），并配有参考见解。标注员的报酬为$25/小时，标注工作共需13小时，总费用为$325。
- en: 'We prepared a prompt that contains task instructions and an example summary
    which has a fully covered, a partially covered, and an uncovered insight (see
    Appendix [A.2](#A1.SS2 "A.2 Evaluation Prompt ‣ Appendix A Appendix ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")). We evaluated
    5 LLMs as evaluators: GPT3.5, Claude 3 Haiku, Opus, GPT-4o, and Gemini-1.5-pro.
    In Table [1](#S3.T1 "Table 1 ‣ 3.4 SummHay Benchmark ‣ 3 Summary in a Haystack
    Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"),
    we report evaluator performance in terms of correlation on the insight-level coverage
    scores, linking accuracy, which measures whether LLMs can attribute the coverage
    to the correct bullet point, and the cost of evaluating the 200 summaries. We
    find that two models, GPT-4o and Gemini-1.5-pro achieve a strong positive correlation
    (0.7+) with the human annotation. We select the GPT-4o model as our evaluator,
    as it achieves high evaluation correlation at a fraction of the cost of Gemini-1.5-pro,
    and does not have strict rate limits in place. We attempt one improvement by preparing
    a prompt with 9 few-shot examples (three of fully, partially, and uncovered insights
    each), and report the result as GPT-4o (9FS). Although the increased number of
    examples does lead to minor correlation improvements, it comes at a large cost
    increase, and thus we finalize the auto-evaluation setting as using the original
    prompt and the GPT-4o evaluator.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们准备了一个包含任务说明和一个示例总结的提示，其中有一个完全覆盖的见解、一个部分覆盖的见解和一个未覆盖的见解（见附录 [A.2](#A1.SS2 "A.2
    Evaluation Prompt ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to
    Long-Context LLMs and RAG Systems")）。我们评估了5个LLM作为评估者：GPT3.5、Claude 3 Haiku、Opus、GPT-4o和Gemini-1.5-pro。在表 [1](#S3.T1
    "Table 1 ‣ 3.4 SummHay Benchmark ‣ 3 Summary in a Haystack Framework ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")中，我们报告了评估者在见解级别覆盖分数的相关性、链接准确性（衡量LLM是否能将覆盖归因于正确的要点）以及评估200个总结的成本方面的表现。我们发现两个模型，GPT-4o和Gemini-1.5-pro，与人工标注的相关性非常强（0.7+）。我们选择GPT-4o模型作为我们的评估者，因为它以Gemini-1.5-pro的成本的一小部分达到了高评估相关性，并且没有严格的速率限制。我们尝试通过准备一个包含9个少量示例（每种完全、部分和未覆盖的见解各三个）的提示来进行改进，并将结果报告为GPT-4o（9FS）。尽管增加示例数量确实导致了轻微的相关性改善，但这也带来了大幅的成本增加，因此我们最终确定使用原始提示和GPT-4o评估器作为自动评估设置。'
- en: 'In Appendix [A.3](#A1.SS3 "A.3 Automatic Results Bias ‣ Appendix A Appendix
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), we
    assess whether automatic evaluation using GPT-4o could cause two types of biases:
    first, whether it could systematically favor or disfavor summaries generated by
    a family of models (such as GPTs), and second whether it could be partial to summaries
    of a certain length. We find no sign of systematic bias in the LLM-based evaluation
    in the case of our protocol.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录[A.3](#A1.SS3 "A.3 自动结果偏差 ‣ 附录A 附录 ‣ 总结：对长上下文LLMs和RAG系统的挑战")中，我们评估了使用GPT-4o的自动评估是否可能导致两种类型的偏差：首先，它是否可能系统性地偏向或不偏向由一系列模型（如GPTs）生成的总结，其次，它是否可能对某种长度的总结有偏好。我们在我们的协议中未发现LLM基于的评估存在系统性偏差的迹象。
- en: 5 Results
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 5.1 Experimental Settings
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'As illustrated in Figure [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ Summary of
    a Haystack: A Challenge to Long-Context LLMs and RAG Systems") (right), we evaluate
    both long-context LLMs that directly access the full Haystack and RAG systems
    where a retriever filters the Haystack down to documents it perceives as relevant,
    which get passed to a generator (LLM). By default, documents in the Haystack are
    ordered in a single arbitrary order.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](#S2.F1 "图1 ‣ 2 相关工作 ‣ 总结：对长上下文LLMs和RAG系统的挑战")（右）所示，我们评估了直接访问完整Haystack的长上下文LLM和RAG系统，其中检索器将Haystack过滤到它认为相关的文档，这些文档传递给生成器（LLM）。默认情况下，Haystack中的文档按单一任意顺序排列。
- en: Full-Context Summarization We test a range of recent LLMs with context lengths
    longer than an individual Haystack, including Cohere’s Command-R and Command-R+,
    Google’s Gemini-1.5-pro and Gemini-1.5-flash Reid et al. ([2024](#bib.bib45)),
    OpenAI’s GPT4-turbo and GPT-4o, and Anthropic’s Claude3 models (haiku, sonnet,
    and opus). We also include GPT-3.5 exclusively in the RAG setting, as its context
    length is 16k tokens.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 全文上下文总结 我们测试了一系列最近的LLM，其上下文长度超出了单个Haystack，包括Cohere的Command-R和Command-R+、Google的Gemini-1.5-pro和Gemini-1.5-flash
    Reid等人 ([2024](#bib.bib45))、OpenAI的GPT4-turbo和GPT-4o，以及Anthropic的Claude3模型（haiku、sonnet和opus）。我们还在RAG设置中独立包含了GPT-3.5，因为其上下文长度为16k
    tokens。
- en: Retrieval-Augmented Summarization We evaluate RAG systems to reduce the Haystack
    input size. All retrieval models receive the query and all Haystack documents
    and must produce a query relevance score for each document. We sort the documents
    in reverse order according to the query relevance score and select the first 15k
    worth of document tokens. We chose 15k enables us to experiment with generators
    that have a 16k context window (GPT-3.5-turbo). We experiment with a total of
    six retrievers, each implemented as a separate query relevance score function.
    Under KWs, the document score is the number of overlapping keywords, extracted
    by NLTK, between the document and the subtopic query. We compare embedding methods
    that compute the cosine similarity between each document and the subtopic query,
    Vect, a SentenceTransformers Reimers and Gurevych ([2019](#bib.bib46)) embedder
    and LongE Zhu et al. ([2024](#bib.bib59)), which extends standard embedders to
    cover longer input contexts, and include the Rerank3 (RR3) model from Cohere Inc.
    ([2024](#bib.bib21)). We also include a Rand baseline that randomly assigns relevance
    scores, and an oracle setting ranker Orac, whose score is the number of subtopic
    insights that appear in a given document. The last two provide lower- and upper-bound
    retrieval quality estimates.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强总结 我们评估RAG系统以减少Haystack输入大小。所有检索模型接收查询和所有Haystack文档，并必须为每个文档生成查询相关性评分。我们根据查询相关性评分将文档按倒序排序，并选择前15k个文档tokens。我们选择15k使我们能够在具有16k上下文窗口的生成器（GPT-3.5-turbo）上进行实验。我们总共实验了六个检索器，每个检索器实现为一个单独的查询相关性评分函数。在KWs下，文档评分是由NLTK提取的文档与子主题查询之间的重叠关键词数量。我们比较了计算每个文档与子主题查询之间的余弦相似度的嵌入方法，Vect，一个SentenceTransformers
    Reimers和Gurevych ([2019](#bib.bib46)) 嵌入器和LongE Zhu等人 ([2024](#bib.bib59))，它扩展了标准嵌入器以覆盖更长的输入上下文，并包括Cohere
    Inc. ([2024](#bib.bib21))的Rerank3（RR3）模型。我们还包括一个随机基线，该基线随机分配相关性评分，以及一个oracle设置的排名器Orac，其评分是出现在给定文档中的子主题见解数量。最后两个提供了检索质量的下界和上界估计。
- en: '![Refer to caption](img/8b4fd2c52b6829c733891bd5c9a46a58.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8b4fd2c52b6829c733891bd5c9a46a58.png)'
- en: 'Figure 3: Estimates of human performance on the SummHay task, plotted over
    time as participants complete the task in the Oracle setting during two-hour sessions.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：对SummHay任务的人类表现的估计，随着参与者在两小时的Oracle设置中完成任务而绘制的时间变化图。
- en: 5.2 Benchmark Results
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基准结果
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") summarizes
    the SummHay results across long-context, in the Full column, and RAG settings
    for all 10 Summarizers included in our study.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") 汇总了我们研究中所有10个总结器在长上下文（在Full列中）和RAG设置下的SummHay结果。'
- en: Coverage scores – which measure the presence of expected insights in a system’s
    output summary – range from 36.2% when using a random retriever and GPT3.5-turbo
    as the summarizer, to 81.4% using the Oracle retriever with the Claude 3 Opus
    summarizer. The choice of retriever impacts the Coverage score, with Random and
    Oracle retrievers leading to the best and worst scores, respectively, for almost
    all summarizers. Yet, top-performing LLMs like Claude3-opus achieve strong Coverage
    scores (70%+) under most retrieval settings, including Full context. In other
    words, achieving strong coverage of key insights in a large corpus of text does
    not require retrieval, given a sufficiently capable long-context LLM.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖率评分——即衡量系统输出摘要中预期见解的存在——从使用随机检索器和GPT3.5-turbo作为总结器时的36.2%到使用Oracle检索器和Claude
    3 Opus总结器时的81.4%不等。检索器的选择会影响覆盖率评分，几乎所有总结器中，随机检索器和Oracle检索器分别导致了最佳和最差的评分。然而，像Claude3-opus这样的顶级LLM在大多数检索设置下，包括全上下文，都会获得强大的覆盖率评分（70%以上）。换句话说，在大规模文本语料库中实现对关键见解的强覆盖并不需要检索，只要有足够强大的长上下文LLM。
- en: 'Citation scores – which account for both the precision and the thoroughness
    of the model’s attribution back to source documents – present a complementary
    narrative. The lowest citation score often occurs in the full-context setting,
    with citation quality on par with random retrieval. On the other hand, as retrieval
    improves (for example from Random to RR3 to Oracle), citation scores increase.
    In a nutshell, for use-cases where citation quality is important, optimizing retrieval
    is paramount: it removes irrelevant documents from the summarizer’s context, narrowing
    and focusing options for citation. Gemini-1.5-pro stands out as an outlier, as
    it is the only model that achieves comparable Citation scores in RAG and long-context
    settings.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 引文评分——即模型将信息归因于源文档的准确性和彻底性——呈现了一个补充性的叙述。最低的引文评分通常出现在全上下文设置中，其引文质量与随机检索相当。另一方面，随着检索的改进（例如从随机检索到RR3再到Oracle），引文评分会提高。总而言之，对于引文质量重要的使用场景，优化检索至关重要：它从总结器的上下文中移除无关文档，缩小并集中引文选项。Gemini-1.5-pro表现突出，是唯一在RAG和长上下文设置中都能获得相当引文评分的模型。
- en: Taking Coverage and Citation into account, the Joint Score provides the complete
    system performance on SummHay. As expected, all Summarizers perform best with
    the Oracle retriever, an unrealistic setting intended to evaluate score ranges.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑覆盖率和引文的情况下，联合评分提供了SummHay上系统性能的完整表现。如预期，所有总结器在Oracle检索器下表现最佳，这是一种不切实际的设置，旨在评估分数范围。
- en: All models except for Gemini-1.5-pro achieve their realistic best performance
    in the RAG setting using the RR3 retriever. The higher relative performance of
    the more advanced RAG retriever RR3, developed for enterprise search and RAG settings,
    aligns with expectations compared to simpler retrievers. This result confirms
    the validity of our SummHay as a test-bed for holistic RAG evaluation; newer,
    more advanced RAGs can be benchmarked in an end-to-end fashion on SummHay, measuring
    direct impact on output quality.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Gemini-1.5-pro之外，所有模型在使用RR3检索器的RAG设置中都达到了其现实中的最佳性能。更先进的RAG检索器RR3，相比于更简单的检索器，具有更高的相对性能，这符合预期。这一结果确认了SummHay作为全方位RAG评估的测试平台的有效性；更新、更先进的RAG可以在SummHay上进行端到端的基准测试，直接测量对输出质量的影响。
- en: 'We still observe two large gaps: all RAG systems underperform the Oracle setting,
    indicating ample room for improvements in RAG systems, and models achieve very
    low Joint scores in the full-context setting (10-20), indicating that SummHay
    is an unsolved task for long-context LLMs. Gemini-1.5-pro stands with strong ability
    to cite in the full-context setting, achieving the only realistic score above
    35 on the benchmark.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然观察到两个显著的差距：所有RAG系统的表现都不及Oracle设置，表明RAG系统有很大的改进空间；模型在全上下文设置中获得的联合评分非常低（10-20），表明SummHay对长上下文LLM仍然是一个未解的任务。Gemini-1.5-pro在全上下文设置中表现出强大的引文能力，获得了基准测试中唯一现实的35以上的评分。
- en: 'In the Rerank3 RAG setting, the top three models are neck and neck, with Claude3
    Opus, GPT-4o, and Gemini-1.5-pro all achieving Joint Scores between 30.8 and 36.0\.
    Yet these models achieve these performances through different trade-offs, with
    Claude3 Opus obtaining the highest Coverage, Gemini-1.5-pro the highest Citation,
    and GPT-4o at the mid-point. This confirms there is room to grow: a system that
    achieves the coverage of Claude3 Opus with the citation quality of Gemini-1.5-pro
    can exceed current score by 15-20%.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Rerank3 RAG 环境中，前三名模型势均力敌，Claude3 Opus、GPT-4o 和 Gemini-1.5-pro 的综合得分均在30.8到36.0之间。然而，这些模型通过不同的权衡来达到这些性能，Claude3
    Opus 具有最高的覆盖率，Gemini-1.5-pro 具有最高的引用率，而 GPT-4o 则处于中间。这确认了还有提升空间：一个在引用质量上达到 Gemini-1.5-pro
    的水平并具有 Claude3 Opus 的覆盖率的系统可以超越当前得分15-20%。
- en: 'The right-most column of Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics
    ‣ 4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") shows each system’s average bullet point length (number of tokens).
    Several systems (Gemini-1.5-pro and Claude 3 Opus) average 30 words per bullet,
    close human-written bullet points (29.7). Others (GPT-4o, GPT4-turbo) are more
    verbose, at 36-38 words per bullet point. In Appendix [A.3](#A1.SS3 "A.3 Automatic
    Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") we confirm that verbosity does not bias evaluation: succinct
    methods can achieve strong performance on SummHay.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") 的最右列显示了每个系统的平均要点长度（词元数量）。一些系统（Gemini-1.5-pro
    和 Claude 3 Opus）每个要点的平均字数为30，接近人工编写的要点（29.7）。其他系统（GPT-4o、GPT4-turbo）则更为冗长，每个要点的字数为36-38。在附录
    [A.3](#A1.SS3 "A.3 Automatic Results Bias ‣ Appendix A Appendix ‣ Summary of a
    Haystack: A Challenge to Long-Context LLMs and RAG Systems") 中，我们确认冗长性不会对评估产生偏差：简洁的方法可以在
    SummHay 上取得强劲的表现。'
- en: 'Appendix [A.5](#A1.SS5 "A.5 Citation Precision & Recall Analysis ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    breaks down Citation Scores into Precision and Recall. No system excels at either
    precision or recall but we do observe trade-offs. For example, Claude models generally
    achieve higher precision and lower recall, whereas Command-r + and GPT-4o favor
    recall over precision.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '附录 [A.5](#A1.SS5 "A.5 Citation Precision & Recall Analysis ‣ Appendix A Appendix
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") 将引用得分分解为精确度和召回率。没有系统在精确度或召回率上表现突出，但我们观察到了一些权衡。例如，Claude
    模型通常在精确度上较高而在召回率上较低，而 Command-r + 和 GPT-4o 更倾向于召回率而非精确度。'
- en: 5.3 Estimating Human Performance
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 估算人工表现
- en: We estimate human performance on the task by recruiting two annotators to perform
    the task. We first define the setting in which annotation was conducted, then
    go over the results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过招聘两名标注员来估算人工在该任务上的表现。我们首先定义进行标注的环境，然后回顾结果。
- en: The participants performed the task in the Oracle setting, only viewing documents
    relevant to the query they are currently summarizing, as it reduces the volume
    of text they must read from about 100,000 tokens to 15,000 tokens. We assume this
    effectively reduces the amount of time required for annotation by a factor of
    5-6, but this remains unverified, as it is impractical to conduct human annotation
    on the full Haystack.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者在 Oracle 环境中执行任务，只查看与当前正在总结的查询相关的文档，因为这样可以将他们必须阅读的文本量从约100,000个词元减少到15,000个词元。我们假设这有效地将标注所需的时间减少了5-6倍，但这一点仍未得到验证，因为在整个
    Haystack 上进行人工标注是不切实际的。
- en: In total, two annotators participated in writing a total of 10 summaries, five
    for subtopics in the conversational domain, and five for subtopics in the news
    domain. Although this represents a subset of the 92 subtopics in the entire SummHay
    benchmark, we believe it represents an unbiased estimate of human performance
    on the benchmark.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有两名标注员参与撰写了10份总结，其中五份是关于对话领域的子主题，五份是关于新闻领域的子主题。尽管这只是整个 SummHay 基准测试中92个子主题的一个子集，但我们认为它代表了对该基准测试的人工表现的一个无偏估计。
- en: 'Figure [3](#S5.F3 "Figure 3 ‣ 5.1 Experimental Settings ‣ 5 Results ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") aggregates results
    across the ten annotation sessions. Overall, participants make steady progress
    during the sessions, with both Coverage and Citation rising rapidly in the first
    90 minutes, and then at a slower pace in the last 30 minutes.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S5.F3 "图 3 ‣ 5.1 实验设置 ‣ 5 结果 ‣ Haystack总结：对长上下文LLMs和RAG系统的挑战") 汇总了十次注释会话的结果。总体而言，参与者在会话过程中取得了稳步进展，覆盖率和引用率在前90分钟迅速上升，然后在最后30分钟以较慢的速度增长。
- en: The Citation Score corresponds to an F1 measure, and we also report on the Precision
    and Recalls of the Citations. We find that citation precision averages close to
    80.0 throughout the session, whereas recall rises steadily during the session.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 引用分数对应于F1度量，我们还报告了引用的精准度和召回率。我们发现引用精准度在整个会话中平均接近80.0，而召回率在会话过程中稳步上升。
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems") includes
    the summarized final scores in contrast to system scores, showing that a human
    annotator can significantly outperform LLMs and RAG systems on the SummHay task,
    as the human joint score (56.1) is significantly higher than the best system performance
    (44.6). We caution the reader not to consider our estimate of human performance
    as an upper bound, as we believe that with more time and explicit instructions
    to double-check their work, annotators could further increase their scores. We
    solely intend the human performance to be a reference point for achievable performances
    on the benchmark, and we expect future systems to tie and surpass human performance
    on the task. Appendix [A.4](#A1.SS4 "A.4 Details on Establishing SummHay Human
    Performance ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") provides further detail on guidelines, recruitment, and
    task framing.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 4.1 评估指标 ‣ 4 评估协议 ‣ Haystack总结：对长上下文LLMs和RAG系统的挑战") 包含了总结后的最终得分，与系统得分相比，显示出人类注释者在SummHay任务上可以显著超过LLMs和RAG系统，因为人类的联合得分（56.1）明显高于最佳系统性能（44.6）。我们提醒读者不要将我们对人类表现的估计视为上限，因为我们相信，随着时间的推移和明确的指示来仔细检查工作，注释者可能会进一步提高他们的得分。我们仅将人类表现作为基准上的可实现表现的参考点，并且我们期望未来的系统能够与人类表现相当并超越人类表现。附录 [A.4](#A1.SS4
    "A.4 关于建立SummHay人类表现的详细信息 ‣ 附录 A 附录 ‣ Haystack总结：对长上下文LLMs和RAG系统的挑战") 提供了关于指导方针、招募和任务框架的更多细节。
- en: '|  | Document Order |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | 文档顺序 |  |'
- en: '| Summarizer | Top | Bottom | Random | Sensitivity |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 摘要生成器 | 顶部 | 底部 | 随机 | 敏感性 |'
- en: '| GPT-4o | 13.8 | 24.1 | 11.4 | 12.6 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 13.8 | 24.1 | 11.4 | 12.6 |'
- en: '| Claude3 Opus | 20.4 | 28.0 | 18.0 | 10.0 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Claude3 Opus | 20.4 | 28.0 | 18.0 | 10.0 |'
- en: '| Gemini-1.5-pro | 47.1 | 38.9 | 37.9 | 9.2 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-pro | 47.1 | 38.9 | 37.9 | 9.2 |'
- en: 'Table 3: Joint Scores of LLMs in the Full Context Setting, based on how documents
    are sorted. Documents can be in Random order or sorted such that relevant ones
    are at the Top or Bottom of the context window.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于文档排序方式的LLMs的联合评分。文档可以是随机排序，也可以是按照相关性排序，使相关文档位于上下文窗口的顶部或底部。
- en: 5.4 Position Bias Sensitivity
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 位置偏差敏感性
- en: 'In the Full Context experiment results (Table [2](#S4.T2 "Table 2 ‣ 4.1 Evaluation
    Metrics ‣ 4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems"), Full columns), documents in the Haystack are ordered arbitrarily,
    with relevant documents in the top, middle, and bottom portions of the context
    window. Prior work Huang et al. ([2023](#bib.bib20)); Chang et al. ([2023](#bib.bib6));
    Chen et al. ([2023b](#bib.bib8)); Ravaut et al. ([2023](#bib.bib44)) has reported
    that models exhibit a position bias that leads them to put more importance on
    information in the context window’s extremities. SummHay offers a framework to
    study position bias systematically. In Table [3](#S5.T3 "Table 3 ‣ 5.3 Estimating
    Human Performance ‣ 5 Results ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems"), we report the results of the Position Bias experiment,
    in which we run the SummHay experiment with the top-3 performing models on sorted
    Haystacks, where relevant documents to a subtopic are either all at the Top or
    Bottom of the context window. Similar to prior work, we find that all three models
    exhibit position bias, with GPT-4o and Claude3 Opus performing better when relevant
    documents are at the bottom of the context window, and Gemini-1.5-pro favoring
    Top Haystacks. We compute a Position Sensitivity score as the maximum absolute
    difference in Joint Score between the Random ordering and Top and Bottom conditions.
    Future systems should strive to attain minimal sensitivity on SummHay, as document
    ordering is often arbitrary in real-world applications.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '在全上下文实验结果中（表[2](#S4.T2 "Table 2 ‣ 4.1 Evaluation Metrics ‣ 4 Evaluation Protocol
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")，全列），Haystack中的文档按任意顺序排列，相关文档位于上下文窗口的顶部、中部和底部。先前的研究
    Huang 等（[2023](#bib.bib20)）；Chang 等（[2023](#bib.bib6)）；Chen 等（[2023b](#bib.bib8)）；Ravaut
    等（[2023](#bib.bib44)）报道了模型表现出位置偏差，导致它们更加重视上下文窗口边缘的信息。SummHay 提供了一个系统地研究位置偏差的框架。在表[3](#S5.T3
    "Table 3 ‣ 5.3 Estimating Human Performance ‣ 5 Results ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems")中，我们报告了位置偏差实验的结果，我们使用了排名前3的模型在排序的
    Haystacks 上运行 SummHay 实验，其中相关文档要么都在上下文窗口的顶部，要么都在底部。类似于先前的研究，我们发现所有三个模型都表现出位置偏差，GPT-4o
    和 Claude3 Opus 在相关文档位于上下文窗口底部时表现更好，而 Gemini-1.5-pro 更倾向于顶部 Haystacks。我们计算了位置敏感度评分，即随机排序与顶部和底部条件之间联合评分的最大绝对差异。未来的系统应力求在
    SummHay 上实现最小敏感度，因为文档排序在实际应用中通常是任意的。'
- en: 6 Discussion
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: Task Upper Bound
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务上限
- en: 'In Section [3.2](#S3.SS2 "3.2 Haystack Generation ‣ 3 Summary in a Haystack
    Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    and Appendix [A.1](#A1.SS1 "A.1 Haystack Synthesis Details ‣ Appendix A Appendix
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), we
    detail our data pipeline and efforts to ensure the quality of our dataset. This
    includes insight subtopic verification and verifying the inclusion of only specified
    insights for each document. Despite our efforts to prevent overlap among insights
    and guarantee the presence of insights in the documents generated, errors may
    be introduced from using LLMs in scaling this data synthesis, making achieving
    a perfect joint score of 100 likely unachievable. Although we estimate human performance
    in a simplified setting, we do not determine the task upper bound. However, we
    show significant room for improvement between the realistic full-context and RAG
    settings and the Oracle setting.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[3.2节](#S3.SS2 "3.2 Haystack Generation ‣ 3 Summary in a Haystack Framework
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")和附录[A.1](#A1.SS1
    "A.1 Haystack Synthesis Details ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems")中，我们详细介绍了我们的数据处理流程以及确保数据集质量的工作。这包括洞察子主题验证和验证每个文档中仅包含指定洞察的情况。尽管我们努力防止洞察之间的重叠，并保证生成文档中包含洞察，但由于在扩展数据合成时使用LLMs可能引入错误，实现完美的100分联合评分可能不太现实。虽然我们在简化环境下估计了人类的表现，但未确定任务上限。然而，我们展示了现实全上下文和RAG设置与Oracle设置之间存在显著的改进空间。'
- en: Simplifying Assumptions in Data Synthesis
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据合成中的简化假设
- en: The assumptions made when generating Haystack documents likely introduce artificial
    signals that simplify the task. For example, in order to maximize control over
    the data synthesis process, Haystack documents are generated independently; no
    dependencies or cross-references exist among the documents. However, in a real-world
    multi-document summarization task, documents may link or refer to each other,
    and there may be temporal between documents. We believe the introduction of more
    realistic assumptions can further increase the difficulty of the task, and we
    hope that future work will take our synthesis processes as a starting point for
    such improvements.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成Haystack文档时做出的假设可能引入了简化任务的人工信号。例如，为了最大化对数据合成过程的控制，Haystack文档是独立生成的；文档之间不存在依赖或交叉引用。然而，在真实的多文档摘要任务中，文档可能相互链接或引用，并且文档之间可能存在时间间隔。我们认为，引入更现实的假设可以进一步增加任务的难度，我们希望未来的工作能够以我们的合成过程为起点进行改进。
- en: Controlling for Verbosity
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 控制冗长程度
- en: When generating summaries, we specify the desired number of insights for the
    LLM to generate. Furthermore, we do not control for or penalize the verbosity
    of the summaries, and summaries with longer insights may result in higher coverage.
    Not specifying the number of summary insights needed per query will result in
    a more difficult task, and we leave a study of the potential trade-offs between
    verbosity, human preference, and overall scores for future work.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成摘要时，我们指定了LLM生成所需的洞察数量。此外，我们不控制或惩罚摘要的冗长程度，摘要中较长的洞察可能会导致更高的覆盖率。如果不指定每个查询所需的摘要洞察数量，将导致任务更为困难，我们留待未来工作中研究冗长、人类偏好和总体得分之间的潜在权衡。
- en: Reliance on Automated Evaluation
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 依赖自动评估
- en: 'Although we do not observe significant bias towards a particular family of
    models, as shown in Appendix [A.3](#A1.SS3 "A.3 Automatic Results Bias ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"),
    the results in Table [1](#S3.T1 "Table 1 ‣ 3.4 SummHay Benchmark ‣ 3 Summary in
    a Haystack Framework ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") demonstrate that there is room for improvement both in coverage
    and linking evaluation. Gemini-1.5-pro, in addition to being more costly than
    GPT-4o, had a rate limit which inhibited its use in our study. Although highly-cost
    effective, non-LLM based NLI and relevance metrics Chen and Eger ([2023](#bib.bib9));
    Liu et al. ([2023](#bib.bib38)) were not tested; Chen et al. ([2023b](#bib.bib8))
    found worse performance among smaller NLI models for the related task of unrelated
    sentence identification on long-form question answering.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们没有观察到对某一特定模型家族的显著偏见，如附录[A.3](#A1.SS3 "A.3 自动结果偏见 ‣ 附录 A ‣ 费曼学习法：对长篇上下文LLMs和RAG系统的挑战")所示，但表[1](#S3.T1
    "表 1 ‣ 3.4 SummHay基准 ‣ 3 Haystack框架中的摘要 ‣ 费曼学习法：对长篇上下文LLMs和RAG系统的挑战")中的结果显示，在覆盖率和链接评估方面还有改进的空间。Gemini-1.5-pro除了比GPT-4o更昂贵外，还有一个限制使用的速率。虽然成本效益高，但非LLM基础的NLI和相关性指标Chen和Eger（[2023](#bib.bib9)）；Liu等（[2023](#bib.bib38)）没有经过测试；Chen等（[2023b](#bib.bib8)）发现对于相关的任务，即长篇问答中的无关句子识别，小型NLI模型的表现较差。
- en: Model Choice
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型选择
- en: The generation models included in our study are all closed-source models. Although
    these closed-source models currently generally outperform open-sourced models,
    this performance comparison can be task-dependent Chen et al. ([2023a](#bib.bib7)).
    We exclude high-performing open-sourced models such as Llama-3 AI@Meta ([2024](#bib.bib1))
    as the original models cannot handle the minimal 16k context window necessary
    for our RAG experiments. Restricting the length of the retrieved documents to,
    for example, 8k would remove too many of the insights for a given subtopic; by
    allowing up to 15k tokens in the RAG setting, we find that the oracle citation
    F1 achievable with this context length is 0.84 averaged across insights, which
    we believe strikes a balance between the reduction of input size and the feasibility
    of the task. We leave an analysis of RAG systems across retrieved input lengths,
    as well as models specifically designed for output citation Menick et al. ([2022](#bib.bib40))
    for future work and encourage and benchmarking of longer-context, open-sourced
    models on SummHay.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究中包含的生成模型都是闭源模型。尽管这些闭源模型目前通常优于开源模型，但这种性能比较可能依赖于任务 Chen et al. ([2023a](#bib.bib7))。我们排除了表现优异的开源模型如
    Llama-3 AI@Meta ([2024](#bib.bib1))，因为原始模型无法处理我们 RAG 实验所需的最小 16k 上下文窗口。限制检索文档的长度，例如
    8k，会移除过多关于特定子主题的见解；通过在 RAG 设置中允许最多 15k tokens，我们发现这个上下文长度可以实现的 oracle 引用 F1 分数是
    0.84，平均覆盖见解，我们认为这在减少输入大小和任务可行性之间找到了平衡。我们将 RAG 系统在检索输入长度上的分析以及专门设计用于输出引用的模型 Menick
    et al. ([2022](#bib.bib40))留待未来工作，并鼓励对长上下文开源模型在 SummHay 上进行基准测试。
- en: 7 Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we address the challenges of evaluating long-context LLMs and
    RAG systems by introducing the SummHay benchmark task, synthesized to assess the
    ability of systems to precisely summarize large sets of documents. The SummHay
    task requires generating summaries that accurately cover and cite insights relevant
    to a particular query. Our comprehensive evaluation reveals that current models
    struggle with this task; even in an oracle document setting, models lag behind
    human performance by more than 10 points. We believe that SummHay provides a robust
    framework for evaluating long-context systems and will encourage researchers to
    utilize SummHay to drive progress toward systems that can match or surpass human
    performance in long-context summarization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们通过引入 SummHay 基准任务来解决评估长上下文 LLMs 和 RAG 系统的挑战，该任务旨在评估系统精准总结大量文档的能力。SummHay
    任务要求生成准确覆盖并引用与特定查询相关的见解的摘要。我们的综合评估显示，目前的模型在这项任务上表现不佳；即使在 oracle 文档设置中，模型也比人类表现低了超过
    10 分。我们认为 SummHay 提供了一个强大的框架来评估长上下文系统，并将鼓励研究人员利用 SummHay 以推动系统在长上下文总结中达到或超越人类表现。
- en: Ethical Considerations
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考量
- en: The models and datasets utilized in the project primarily reflect the culture
    of the English-speaking populace. Gender, age, race, and other socio-economic
    biases may exist in the data, and models trained on these datasets may propagate
    these biases. Text generation tasks such as summarization have previously been
    shown to contain these biases.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 项目中使用的模型和数据集主要反映了英语使用人群的文化。数据中可能存在性别、年龄、种族和其他社会经济偏见，基于这些数据集训练的模型可能会传播这些偏见。文本生成任务如总结已被证明包含这些偏见。
- en: 'In Section [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems") and Section [5](#S5 "5 Results ‣ Summary
    of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), we recruited
    professional annotators to perform evaluation, or directly attempt the task. We
    ensured to remunerate the participants fairly ($25/hour). Participants could communicate
    with us to voice concerns, work at their own pace, and choose to stop working
    on the project at any time. Finally, we ensured to anonymize the annotations (annotator
    identity is instead marked as annotator1, annotator2, etc.).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems") 节和第 [5](#S5 "5 Results ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems") 节中，我们招募了专业注释员进行评估，或直接尝试任务。我们确保了公平报酬（$25/hour）。参与者可以与我们沟通以表达关注，按照自己的节奏工作，并随时选择停止参与项目。最后，我们确保了注释的匿名化（注释员身份标记为
    annotator1、annotator2 等）。'
- en: In our work, we relied on several datasets as well as pre-trained language models.
    We explicitly verified that all datasets and models are publicly released for
    research purposes and that we have proper permission to reuse and modify the datasets.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们依赖于多个数据集以及预训练语言模型。我们明确验证了所有数据集和模型都已公开发布用于研究目的，并且我们已获得适当的权限以重用和修改这些数据集。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta（2024）AI@Meta。2024年。[Llama 3 模型卡](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'An et al. (2023) Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang,
    Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation
    for long context language models. *arXiv preprint arXiv:2307.11088*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'An 等人（2023）陈鑫、龚善善、钟铭、李木开、张俊、孔灵鹏和邱希鹏。2023年。《L-eval: 建立长文本语言模型的标准化评估》。*arXiv
    预印本 arXiv:2307.11088*。'
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai 等人（2023）白玉石、吕欣、张佳杰、吕洪昌、唐建凯、黄志滇、杜正晓、刘晓、曾傲涵、侯磊等。2023年。《Longbench: 一个双语、多任务的长文本理解基准》。*arXiv
    预印本 arXiv:2308.14508*。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
    Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy 等人（2020）伊兹·贝尔塔吉、马修·E·彼得斯和阿尔曼·科汉。2020年。《Longformer: 长文档变换器》。*arXiv 预印本
    arXiv:2004.05150*。'
- en: Bhandari et al. (2020) Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei
    Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhandari 等人（2020）马尼克·班达里、普拉纳夫·纳拉扬·高尔、阿塔巴克·阿什法克、刘鹏飞和格雷厄姆·纽比格。2020年。《重新评估文本总结中的评估》。见于*2020年自然语言处理实证方法会议（EMNLP）论文集*。
- en: 'Chang et al. (2023) Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2023.
    Booookscore: A systematic exploration of book-length summarization in the era
    of llms. *arXiv preprint arXiv:2310.00785*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等人（2023）杨佩伊、凯尔·洛、塔尼亚·戈亚尔和莫希特·伊耶尔。2023年。《Booookscore: 在大型语言模型时代对书籍长度总结的系统探索》。*arXiv
    预印本 arXiv:2310.00785*。'
- en: 'Chen et al. (2023a) Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu
    Ravaut, Ruochen Zhao, Caiming Xiong, and Shafiq Joty. 2023a. Chatgpt’s one-year
    anniversary: Are open-source large language models catching up? *arXiv preprint
    arXiv:2311.16989*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2023a）陈海林、焦方凯、李星轩、秦成伟、马修·拉沃、赵若晨、熊才明和沙菲克·乔提。2023a年。《ChatGPT 一周年: 开源大型语言模型是否赶上了？》*arXiv
    预印本 arXiv:2311.16989*。'
- en: Chen et al. (2023b) Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi.
    2023b. Understanding retrieval augmentation for long-form question answering.
    *arXiv preprint arXiv:2310.12150*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023b）陈宏廷、徐方媛、肖恩·A·阿罗拉和崔恩瑟。2023b年。《理解用于长篇问答的检索增强》。*arXiv 预印本 arXiv:2310.12150*。
- en: 'Chen and Eger (2023) Yanran Chen and Steffen Eger. 2023. Menli: Robust evaluation
    metrics from natural language inference. *Transactions of the Association for
    Computational Linguistics*, 11:804–825.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 和 Eger（2023）陈艳然和斯特芬·埃格。2023年。《Menli: 来自自然语言推断的鲁棒评估指标》。*计算语言学协会会刊*, 11:804–825。'
- en: 'Clark et al. (2023) Elizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua
    Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan
    Das, and Ankur P Parikh. 2023. Seahorse: A multilingual, multifaceted dataset
    for summarization evaluation. *arXiv preprint arXiv:2305.13194*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人（2023）伊丽莎白·克拉克、舒睿蒂·瑞赫瓦尼、塞巴斯蒂安·格尔曼、约书亚·梅内兹、罗伊·阿哈罗尼、维塔利·尼古拉耶夫、蒂博·塞拉姆、阿迪亚·席丹特、迪潘詹·达斯和安库尔·P·帕里克。2023年。《Seahorse:
    一个多语言、多方面的数据集用于总结评估》。*arXiv 预印本 arXiv:2305.13194*。'
- en: 'Dong et al. (2023) Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong
    Wen. 2023. Bamboo: A comprehensive benchmark for evaluating long text modeling
    capacities of large language models. *arXiv preprint arXiv:2309.13345*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等人（2023）董子灿、唐天怡、李俊毅、魏辛赵和温季荣。2023年。《Bamboo: 评估大型语言模型长文本建模能力的综合基准》。*arXiv
    预印本 arXiv:2309.13345*。'
- en: 'Fabbri et al. (2021a) Alexander R Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021a. Summeval: Re-evaluating
    summarization evaluation. *Transactions of the Association for Computational Linguistics*,
    9:391–409.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri 等人（2021a）亚历山大·R·法布里、沃伊切赫·克里欣斯基、布莱恩·麦肯、熊才明、理查德·索彻和德拉戈米尔·拉德夫。2021a年。《Summeval:
    重新评估总结评估》。*计算语言学协会会刊*, 9:391–409。'
- en: 'Fabbri et al. (2021b) Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, and Dragomir Radev. 2021b. [SummEval: Re-evaluating
    summarization evaluation](https://doi.org/10.1162/tacl_a_00373). *Transactions
    of the Association for Computational Linguistics*, 9:391–409.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fabbri et al. (2021b) Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann,
    Caiming Xiong, Richard Socher, 和 Dragomir Radev. 2021b. [SummEval: 重新评估总结评估](https://doi.org/10.1162/tacl_a_00373).
    *计算语言学协会期刊*，9:391–409。'
- en: 'Fan et al. (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier,
    Jason Weston, and Michael Auli. 2019. [ELI5: Long form question answering](https://doi.org/10.18653/v1/P19-1346).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3558–3567, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan et al. (2019) Angela Fan, Yacine Jernite, Ethan Perez, David Grangier,
    Jason Weston, 和 Michael Auli. 2019. [ELI5: 长篇问题回答](https://doi.org/10.18653/v1/P19-1346).
    载于 *第57届计算语言学协会年会论文集*，页码 3558–3567，意大利佛罗伦萨。计算语言学协会。'
- en: 'Gao and Wan (2022) Mingqi Gao and Xiaojun Wan. 2022. [DialSummEval: Revisiting
    summarization evaluation for dialogues](https://doi.org/10.18653/v1/2022.naacl-main.418).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5693–5709,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao and Wan (2022) Mingqi Gao 和 Xiaojun Wan. 2022. [DialSummEval: 重新审视对话总结评估](https://doi.org/10.18653/v1/2022.naacl-main.418).
    载于 *2022年北美计算语言学协会会议：人类语言技术*，页码 5693–5709，美国西雅图。计算语言学协会。'
- en: 'Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander
    Wawer. 2019. [SAMSum corpus: A human-annotated dialogue dataset for abstractive
    summarization](https://doi.org/10.18653/v1/D19-5409). In *Proceedings of the 2nd
    Workshop on New Frontiers in Summarization*, pages 70–79, Hong Kong, China. Association
    for Computational Linguistics.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, 和 Aleksander
    Wawer. 2019. [SAMSum 语料库：用于抽象总结的人类标注对话数据集](https://doi.org/10.18653/v1/D19-5409).
    载于 *第二届总结新前沿研讨会论文集*，页码 70–79，中国香港。计算语言学协会。
- en: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei
    Chang. 2020. Retrieval augmented language model pre-training. In *International
    conference on machine learning*, pages 3929–3938\. PMLR.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu et al. (2020) Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, 和 Mingwei
    Chang. 2020. 检索增强语言模型预训练。载于 *国际机器学习会议*，页码 3929–3938。PMLR。
- en: Hermann et al. (2015) Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. [Teaching
    machines to read and comprehend](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend).
    In *NIPS*, pages 1693–1701.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermann et al. (2015) Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette,
    Lasse Espeholt, Will Kay, Mustafa Suleyman, 和 Phil Blunsom. 2015. [教机器阅读和理解](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend).
    载于 *NIPS*，页码 1693–1701。
- en: Hu et al. (2024) Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong
    Wu, and Jeff Z Pan. 2024. Benchmarking large language models in complex question
    answering attribution using knowledge graphs. *arXiv preprint arXiv:2401.14640*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2024) Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong
    Wu, 和 Jeff Z Pan. 2024. 使用知识图谱基准评估大型语言模型在复杂问题回答归因中的表现。*arXiv 预印本 arXiv:2401.14640*。
- en: 'Huang et al. (2023) Kung-Hsiang Huang, Philippe Laban, Alexander R Fabbri,
    Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023.
    Embrace divergence for richer insights: A multi-document summarization benchmark
    and a case study on summarizing diverse information from news articles. *arXiv
    preprint arXiv:2309.09369*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Kung-Hsiang Huang, Philippe Laban, Alexander R Fabbri, Prafulla
    Kumar Choubey, Shafiq Joty, Caiming Xiong, 和 Chien-Sheng Wu. 2023. 拥抱差异以获得更丰富的见解：多文档总结基准及新闻文章多样化信息总结的案例研究。*arXiv
    预印本 arXiv:2309.09369*。
- en: 'Inc. (2024) Cohere Inc. 2024. [Introducing rerank 3: The next generation of
    search relevance](https://cohere.com/blog/rerank-3). Accessed: 2024-06-10.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inc. (2024) Cohere Inc. 2024. [介绍 rerank 3：搜索相关性的下一代](https://cohere.com/blog/rerank-3).
    访问时间：2024-06-10。
- en: 'Kamalloo et al. (2023) Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur,
    and Jimmy Lin. 2023. Hagrid: A human-llm collaborative dataset for generative
    information-seeking with attribution. *arXiv preprint arXiv:2307.16883*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kamalloo et al. (2023) Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur,
    和 Jimmy Lin. 2023. Hagrid: 用于生成信息检索与归因的人类-语言模型协作数据集。*arXiv 预印本 arXiv:2307.16883*。'
- en: Kamradt (2023) Gregory Kamradt. 2023. [Needleinahaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamradt (2023) Gregory Kamradt. 2023. [干草堆中的针](https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md)。
- en: 'Kim et al. (2024) Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella,
    Varun Manjunatha, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2024. Fables: Evaluating
    faithfulness and content selection in book-length summarization. *arXiv preprint
    arXiv:2404.01261*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2024) Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella,
    Varun Manjunatha, Kyle Lo, Tanya Goyal, 和 Mohit Iyyer. 2024. Fables：评估书长总结中的忠实性和内容选择。*arXiv
    预印本 arXiv:2404.01261*。
- en: 'Krishna et al. (2023) Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer,
    Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. Longeval: Guidelines for human
    evaluation of faithfulness in long-form summarization. *arXiv preprint arXiv:2301.13298*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等 (2023) Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep
    Dasigi, Arman Cohan, 和 Kyle Lo. 2023. Longeval：长篇总结中忠实性的人工评估指南。*arXiv 预印本 arXiv:2301.13298*。
- en: 'Kuratov et al. (2024) Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin,
    Artyom Sorokin, and Mikhail Burtsev. 2024. In search of needles in a 10m haystack:
    Recurrent memory finds what llms miss. *arXiv preprint arXiv:2402.10790*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuratov 等 (2024) Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin,
    Artyom Sorokin, 和 Mikhail Burtsev. 2024. 在10米干草堆中寻找针：递归记忆发现了LLMs遗漏的内容。*arXiv 预印本
    arXiv:2402.10790*。
- en: 'Kwan et al. (2023) Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou
    Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong. 2023. M4le: A multi-ability multi-range
    multi-task multi-domain long-context evaluation benchmark for large language models.
    *arXiv preprint arXiv:2310.19240*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwan 等 (2023) Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou
    Li, Lifeng Shang, Qun Liu, 和 Kam-Fai Wong. 2023. M4le：一个多能力、多范围、多任务、多领域的长上下文评估基准用于大型语言模型。*arXiv
    预印本 arXiv:2310.19240*。
- en: 'Laban et al. (2020) Philippe Laban, Andrew Hsi, John Canny, and Marti A Hearst.
    2020. The summary loop: Learning to write abstractive summaries without examples.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 5135–5150.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laban 等 (2020) Philippe Laban, Andrew Hsi, John Canny, 和 Marti A Hearst. 2020.
    摘要循环：学习在没有示例的情况下编写抽象总结。在 *第58届计算语言学协会年会论文集*，页码 5135–5150。
- en: 'Laban et al. (2022a) Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A
    Hearst. 2022a. Summac: Re-visiting nli-based models for inconsistency detection
    in summarization. *Transactions of the Association for Computational Linguistics*,
    10:163–177.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laban 等 (2022a) Philippe Laban, Tobias Schnabel, Paul N Bennett, 和 Marti A Hearst.
    2022a. Summac：重新审视基于NLI的总结不一致检测模型。*计算语言学协会交易*，10:163–177。
- en: 'Laban et al. (2022b) Philippe Laban, Chien-Sheng Wu, Lidiya Murakhovs’ka, Xiang
    Chen, and Caiming Xiong. 2022b. Discord questions: A computational approach to
    diversity analysis in news coverage. In *Findings of the Association for Computational
    Linguistics: EMNLP 2022*, pages 5180–5194.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laban 等 (2022b) Philippe Laban, Chien-Sheng Wu, Lidiya Murakhovs’ka, Xiang Chen,
    和 Caiming Xiong. 2022b. 不和谐问题：新闻报道中的多样性分析的计算方法。在 *计算语言学协会发现：EMNLP 2022*，页码 5180–5194。
- en: LangChain (2024) LangChain. 2024. [Multi-needle in a haystack](https://blog.langchain.dev/multi-needle-in-a-haystack/?ref=dailydev).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain (2024) LangChain. 2024. [干草堆中的多根针](https://blog.langchain.dev/multi-needle-in-a-haystack/?ref=dailydev)。
- en: 'Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,
    Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart:
    Denoising sequence-to-sequence pre-training for natural language generation, translation,
    and comprehension. *arXiv preprint arXiv:1910.13461*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等 (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, 和 Luke Zettlemoyer. 2019. Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练。*arXiv
    预印本 arXiv:1910.13461*。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等 (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, 等. 2020. 用于知识密集型 NLP 任务的检索增强生成。*神经信息处理系统进展*，33:9459–9474。
- en: Li et al. (2023a) Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen,
    Baotian Hu, Aiguo Wu, and Min Zhang. 2023a. A survey of large language models
    attribution. *arXiv preprint arXiv:2311.03731*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023a) Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen, Baotian
    Hu, Aiguo Wu, 和 Min Zhang. 2023a. 大型语言模型归因的调查。*arXiv 预印本 arXiv:2311.03731*。
- en: 'Li et al. (2023b) Xinze Li, Yixin Cao, Liangming Pan, Yubo Ma, and Aixin Sun.
    2023b. Towards verifiable generation: A benchmark for knowledge-aware language
    model attribution. *arXiv preprint arXiv:2310.05634*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2023b) Xinze Li, Yixin Cao, Liangming Pan, Yubo Ma, 和 Aixin Sun. 2023b.
    迈向可验证生成: 一个知识感知语言模型归因的基准。*arXiv 预印本 arXiv:2310.05634*。'
- en: 'Li et al. (2024) Yifei Li, Xiang Yue, Zeyi Liao, and Huan Sun. 2024. Attributionbench:
    How hard is automatic attribution evaluation? *arXiv preprint arXiv:2402.15089*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2024) Yifei Li, Xiang Yue, Zeyi Liao, 和 Huan Sun. 2024. Attributionbench:
    自动归因评估有多困难？*arXiv 预印本 arXiv:2402.15089*。'
- en: 'Liu et al. (2022) Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong
    Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, et al.
    2022. Revisiting the gold standard: Grounding summarization evaluation with robust
    human evaluation. *arXiv preprint arXiv:2212.07981*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2022) Yixin Liu, Alexander R Fabbri, Pengfei Liu, Yilun Zhao, Linyong
    Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong 等人. 2022.
    重访黄金标准: 用稳健的人类评估为摘要评估奠定基础。*arXiv 预印本 arXiv:2212.07981*。'
- en: Liu et al. (2023) Yixin Liu, Alexander R Fabbri, Yilun Zhao, Pengfei Liu, Shafiq
    Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2023. Towards interpretable
    and efficient automatic reference-based summarization evaluation. *arXiv preprint
    arXiv:2303.03608*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Yixin Liu, Alexander R Fabbri, Yilun Zhao, Pengfei Liu, Shafiq
    Joty, Chien-Sheng Wu, Caiming Xiong, 和 Dragomir Radev. 2023. 迈向可解释且高效的自动参考摘要评估。*arXiv
    预印本 arXiv:2303.03608*。
- en: Machlab and Battle (2024) Daniel Machlab and Rick Battle. 2024. Llm in-context
    recall is prompt dependent. *arXiv preprint arXiv:2404.08865*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Machlab 和 Battle (2024) Daniel Machlab 和 Rick Battle. 2024. Llm 上下文回忆受提示依赖。*arXiv
    预印本 arXiv:2404.08865*。
- en: Menick et al. (2022) Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving, et al. 2022. Teaching language models to support answers with
    verified quotes. *arXiv preprint arXiv:2203.11147*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menick 等人 (2022) Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides,
    Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham,
    Geoffrey Irving 等人. 2022. 教会语言模型支持通过验证引用回答。*arXiv 预印本 arXiv:2203.11147*。
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau
    Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
    Factscore: Fine-grained atomic evaluation of factual precision in long form text
    generation. *arXiv preprint arXiv:2305.14251*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等人 (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih,
    Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, 和 Hannaneh Hajishirzi. 2023. Factscore:
    长文本生成中的事实精准度的细粒度原子评估。*arXiv 预印本 arXiv:2305.14251*。'
- en: 'Ni et al. (2024) Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei
    Yin, and Piji Li. 2024. Xl bench: A benchmark for extremely long context understanding
    with long-range dependencies. *arXiv preprint arXiv:2404.05446*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ni 等人 (2024) Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin,
    和 Piji Li. 2024. Xl bench: 一个用于理解极长上下文的基准，涵盖长程依赖。*arXiv 预印本 arXiv:2404.05446*。'
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2020. 通过统一的文本到文本变换器探索迁移学习的极限。*机器学习研究期刊*,
    21(140):1–67。
- en: Ravaut et al. (2023) Mathieu Ravaut, Shafiq Joty, Aixin Sun, and Nancy F Chen.
    2023. On context utilization in summarization with large language models. *arXiv
    e-prints*, pages arXiv–2310.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravaut 等人 (2023) Mathieu Ravaut, Shafiq Joty, Aixin Sun, 和 Nancy F Chen. 2023.
    大型语言模型中摘要的上下文利用。*arXiv e-prints*, 页面 arXiv–2310。
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding
    across millions of tokens of context. *arXiv preprint arXiv:2403.05530*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid 等人 (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
    Firat, Julian Schrittwieser 等人. 2024. Gemini 1.5: 解锁跨百万标记上下文的多模态理解。*arXiv 预印本
    arXiv:2403.05530*。'
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. [Sentence-bert:
    Sentence embeddings using siamese bert-networks](http://arxiv.org/abs/1908.10084).
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing*. Association for Computational Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers and Gurevych (2019) 尼尔斯·雷默斯，伊里娜·古列维奇。2019。 [Sentence-bert：使用 Siamese
    Bert 网络的句子嵌入](http://arxiv.org/abs/1908.10084)。见于 *2019 年自然语言处理经验方法会议论文集*。计算语言学协会。
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding.
    *arXiv preprint arXiv:2305.14196*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaham et al. (2023) 乌里·沙汉，毛尔·伊夫基，阿维亚·埃夫拉特，乔纳森·贝兰特，奥梅尔·列维。2023。Zeroscrolls：长文本理解的零样本基准。*arXiv
    预印本 arXiv:2305.14196*。
- en: 'Song et al. (2024) Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu,
    Xiang Wan, and Benyou Wang. 2024. Milebench: Benchmarking mllms in long context.
    *arXiv preprint arXiv:2404.18532*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2024) 宋鼎杰，陈顺年，陈贵明·哈迪，余飞，万翔，王本友。2024。Milebench：长上下文中多语言模型的基准测试。*arXiv
    预印本 arXiv:2404.18532*。
- en: 'Su et al. (2024) Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo,
    and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding.
    *Neurocomputing*, 568:127063.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2024) 苏建林，穆尔塔扎·艾哈迈德，吕宇，潘胜锋，温博，刘云峰。2024。Roformer：具有旋转位置嵌入的增强型变换器。*Neurocomputing*，568:127063。
- en: 'Tang et al. (2024) Liyan Tang, Philippe Laban, and Greg Durrett. 2024. Minicheck:
    Efficient fact-checking of llms on grounding documents. *arXiv preprint arXiv:2404.10774*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2024) 唐丽艳，菲利普·拉班，格雷格·杜雷特。2024。Minicheck：高效的语言模型事实检查。*arXiv 预印本
    arXiv:2404.10774*。
- en: 'Vig et al. (2022) Jesse Vig, Alexander Fabbri, Wojciech Kryscinski, Chien-Sheng
    Wu, and Wenhao Liu. 2022. [Exploring neural models for query-focused summarization](https://doi.org/10.18653/v1/2022.findings-naacl.109).
    In *Findings of the Association for Computational Linguistics: NAACL 2022*, pages
    1455–1468, Seattle, United States. Association for Computational Linguistics.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vig et al. (2022) 杰西·维格，亚历山大·法布里，沃伊切赫·克里辛斯基，吴千胜，刘文浩。2022。 [探索用于查询导向摘要的神经模型](https://doi.org/10.18653/v1/2022.findings-naacl.109)。见于
    *计算语言学协会年会论文集：NAACL 2022*，第1455–1468页，美国西雅图。计算语言学协会。
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2019) 托马斯·沃尔夫，莉桑德拉·德布特，维克托·桑，朱利安·肖蒙，克莱门特·德朗格，安东尼·莫伊，皮埃里克·西斯塔克，蒂姆·罗，雷米·卢夫，摩根·芬托维茨，等。2019。Huggingface
    的变换器：最先进的自然语言处理。*arXiv 预印本 arXiv:1910.03771*。
- en: Wu et al. (2023) Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and
    Estevam Hruschka. 2023. Less is more for long document summary evaluation by llms.
    *arXiv preprint arXiv:2309.07382*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) 吴云舒，磯隼人，普亚·佩泽什克普尔，尼基塔·布塔尼，埃斯特万·赫鲁施卡。2023。少即是多：长文档总结评估的少量语言模型。*arXiv
    预印本 arXiv:2309.07382*。
- en: Yue et al. (2023) Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan
    Sun. 2023. Automatic evaluation of attribution by large language models. *arXiv
    preprint arXiv:2305.06311*.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue et al. (2023) 谷月，王博石，陈子如，张凯，苏宇，孙欢。2023。大型语言模型的归因自动评估。*arXiv 预印本 arXiv:2305.06311*。
- en: 'Zha et al. (2023) Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023.
    Alignscore: Evaluating factual consistency with a unified alignment function.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 11328–11348.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zha et al. (2023) 赵宇恒，杨意驰，李瑞晨，胡智廷。2023。Alignscore：通过统一对齐函数评估事实一致性。见于 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，第11328–11348页。
- en: Zhang et al. (2024a) Huajian Zhang, Yumo Xu, and Laura Perez-Beltrachini. 2024a.
    Fine-grained natural language inference based faithfulness evaluation for diverse
    summarisation tasks. *arXiv preprint arXiv:2402.17630*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024a) 张华建，徐玉墨，劳拉·佩雷斯-贝尔特拉奇尼。2024a。基于细粒度自然语言推理的多样化摘要任务的忠实度评估。*arXiv
    预印本 arXiv:2402.17630*。
- en: 'Zhang et al. (2024b) Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao
    Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. 2024b.
    Infinitybench: Extending long context evaluation beyond 100k tokens. *arXiv preprint
    arXiv:2402.13718*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024b) 张新荣，陈英发，胡胜定，徐子航，陈俊豪，苗开浩，韩旭，郑棠泰，王硕，刘智远，等。2024b。Infinitybench：超越
    100k 令牌的长上下文评估。*arXiv 预印本 arXiv:2402.13718*。
- en: 'Zhong et al. (2021) Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma,
    Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and
    Dragomir Radev. 2021. [QMSum: A new benchmark for query-based multi-domain meeting
    summarization](https://doi.org/10.18653/v1/2021.naacl-main.472). In *Proceedings
    of the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 5905–5921, Online. Association
    for Computational Linguistics.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等（2021）Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul
    Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, 和 Dragomir
    Radev. 2021. [QMSum: A new benchmark for query-based multi-domain meeting summarization](https://doi.org/10.18653/v1/2021.naacl-main.472).
    见 *2021年北美计算语言学协会会议：人类语言技术论文集*，第 5905–5921 页，在线。计算语言学协会。'
- en: 'Zhu et al. (2024) Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu
    Wei, and Sujian Li. 2024. Longembed: Extending embedding models for long context
    retrieval. *arXiv preprint arXiv:2404.12096*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等（2024）Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei,
    和 Sujian Li. 2024. Longembed: Extending embedding models for long context retrieval.
    *arXiv 预印本 arXiv:2404.12096*。'
- en: Appendix A Appendix
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Haystack Synthesis Details
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 Haystack 合成细节
- en: Below we specify additional details regarding our data synthesis pipelines.
    For the news domain, we leverage GPT-4o for all data synthesis steps. We found
    it necessary to leverage this high-performing LLM due to the longer seed context
    documents that subtopics and insights are generated from. For the conversation
    domain, we leverage GPT-4o to generate subtopics and insights, and for any LLM-based
    verification step. Conversation generation (conditioned on selected insights)
    is completed using GPT-3.5-turbo.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们关于数据合成管道的更多细节。对于新闻领域，我们利用 GPT-4o 进行所有数据合成步骤。由于子主题和见解是从较长的种子上下文文档中生成的，我们发现有必要利用这种高性能的
    LLM。对于对话领域，我们利用 GPT-4o 生成子主题和见解，并用于任何基于 LLM 的验证步骤。对话生成（以选择的见解为条件）使用 GPT-3.5-turbo
    完成。
- en: Furthermore, we employ verification steps to ensure that subtopics and insights
    are distinct and precisely mapped to Haystack documents. When generating documents
    given a set of insights, we do not want other insights to “leak” into the document,
    as that would reduce the quality of the Haystack and task. Below we list the verification
    steps taken across the conversation and news domain. Differences in domain characteristics
    and the seed used to generate Haystacks necessitate per-domain verification steps.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们采用验证步骤以确保子主题和见解是独特的，并且准确映射到 Haystack 文档。当生成文档时，我们不希望其他见解“泄露”到文档中，因为这会降低
    Haystack 和任务的质量。以下是我们在对话和新闻领域采取的验证步骤。领域特征的差异以及生成 Haystacks 所使用的种子需要每个领域的验证步骤。
- en: A.1.1 Subtopic Verification
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1 子主题验证
- en: In the news domain, to ensure distinct insights and subtopics we first prompt
    an LLM to identify any overlapping or duplicate subtopics and remove these subtopics.
    This helps ensure that when querying relevant insights for a subtopic, insights
    can only belong to one of the distinct subtopics generated.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在新闻领域，为了确保见解和子主题的独特性，我们首先提示一个 LLM 识别任何重叠或重复的子主题并移除这些子主题。这有助于确保在查询子主题的相关见解时，见解只能属于生成的一个独特子主题。
- en: In the conversation domain, we use manual inspection to verify the distinctness
    of the subtopics and regenerate subtopic candidates (at temperature $T=1$) until
    obtaining a list where each subtopic feels qualitatively unique.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话领域，我们使用人工检查来验证子主题的独特性，并重新生成子主题候选（在温度 $T=1$ 下），直到获得每个子主题在定性上都独特的列表。
- en: A.1.2 Insight Verification
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2 见解验证
- en: For the news domain, we prompt the LLM to remove duplicate insights. After producing
    an initial set of insights for the subtopics, we take all insights and prompt
    an LLM to categorize each insight into one of the subtopics. As insights are initially
    generated for a particular subtopic, at this step we ensure that no insight can
    fit into another subtopic. We thus remove any insights for which the categorized
    subtopic differs from the one it was initially generated for.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新闻领域，我们提示 LLM 移除重复的见解。在为子主题生成初步的见解集后，我们将所有见解提交给 LLM，将每个见解分类到一个子主题中。由于见解最初是为特定子主题生成的，因此在此步骤中我们确保没有见解可以归入另一个子主题。因此，我们移除任何分类子主题与最初生成时不同的见解。
- en: 'In the conversation domain, we iterate over subtopics sequentially and use
    a prompt to generate the list of insights for one subtopic. In the prompt, we
    provide not only the target subtopic but also all other subtopics and insights,
    instructing the LLM to avoid such subtopics and insights, and only propose insights
    that are distinct and unique in contrast to those. Manual inspection from the
    authors reveals that: as long as the subtopics are confirmed to be unique, and
    that the insights are enforced to be specific (and include entities), very little
    overlap occurs across subtopic insights.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话领域，我们顺序地迭代子主题，并使用提示生成一个子主题的洞察列表。在提示中，我们不仅提供目标子主题，还提供所有其他子主题和洞察，指示LLM避免这些子主题和洞察，并仅提出与之不同且独特的洞察。作者的手动检查显示：只要确认子主题是唯一的，并且洞察被强制为具体的（并且包含实体），子主题洞察之间几乎不会发生重叠。
- en: A.1.3 Document Verification
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3 文档验证
- en: In the news domain, to ensure a precise mapping of insights to documents in
    which an insight is present, we prompt an LLM to label whether any of the insights,
    across subtopics, other than those sampled for that document are found in the
    document. If any are found, we regenerate the document, asking the LLM to remove
    the sentence(s) containing the extraneous insights. This procedure is repeated
    until no extraneous insights are found, up until 5 iterations. For a similar purpose,
    we prompt an LLM to label whether the insights sampled for a given document are
    indeed found in the document. If not, we regenerate the document to add sentences
    that contain this insight, up until 5 iterations. We find that after 5 iterations
    of editing, discrepancies in insights by the LLM were primarily paraphrasing or
    partial detail upon manual inspection.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在新闻领域，为了确保洞察与包含洞察的文档的准确映射，我们提示LLM标记文档中是否存在其他子主题的洞察（除了为该文档抽样的洞察）。如果发现任何其他洞察，我们重新生成文档，要求LLM删除包含多余洞察的句子。这个过程会重复进行，直到没有发现多余的洞察，最多进行5次迭代。出于类似的目的，我们提示LLM标记抽样的洞察是否确实存在于文档中。如果没有，我们重新生成文档，添加包含该洞察的句子，最多进行5次迭代。我们发现经过5次迭代编辑后，通过手动检查发现LLM在洞察方面的差异主要是措辞的不同或细节不全。
- en: In the conversational domain, we generate the documents iteratively, one chapter
    at a time, where each chapter is intended to introduce a single insight. When
    expanding an insight into a chapter, we generate a candidate chapter and use GPT-4o
    to classify whether the candidate chapter indeed covers the expected subtopic
    and insight. If not, we regenerate the candidate chapter up to ten times, and
    otherwise, we accept the candidate chapter into the document and proceed with
    the next chapter. We find that in practice, the generation process requires 5
    iterations less than 1% of the time to generate a chapter that GPT-4o can correctly
    assign to the expected insight.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话领域，我们逐章迭代生成文档，每章旨在介绍一个单独的洞察。当将洞察扩展到一章时，我们生成一个候选章节，并使用GPT-4o分类候选章节是否确实涵盖了预期的子主题和洞察。如果没有，我们最多重新生成候选章节十次，否则，我们接受该候选章节并继续生成下一章。我们发现，在实践中，生成过程需要不到1%的时间进行5次迭代来生成GPT-4o能够正确分配到预期洞察的章节。
- en: A.2 Evaluation Prompt
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评估提示
- en: 'Below, we list the prompt we use for automatic evaluation in the SummHay task,
    as described in Section [4](#S4 "4 Evaluation Protocol ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems").'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '以下，我们列出了在SummHay任务中用于自动评估的提示，如第[4](#S4 "4 Evaluation Protocol ‣ Summary of
    a Haystack: A Challenge to Long-Context LLMs and RAG Systems")节所述。'
- en: You  are  given  a  list  of  bullet  points  (each  with  a  unique  number),  and  a  specific  reference  insight.  Your  objective  is  to  determine  whether  the  reference  insight  is  covered  in  any  of  the  bullet  points.  You  must  further  determine  if  the  insight  is  partially  covered  ("PARTIAL_COVERAGE")  or  fully  covered  ("FULL_COVERAGE")  by  the  bullet  points.  If  the  insight  is  not  covered  at  all,  you  must  return  "NO_COVERAGE".  See  examples  below:[[FEW_SHOT_EXAMPLES]]Now  complete  the  task  for  the  following  insight  and  bullet  points:Reference  Insight:[[INSIGHT]]Bullet  Points:[[BULLETS]]Requirements:-  Do  not  hallucinate  that  the  insight  is  covered  by  the  bullet  points  if  it  is  not.-  Your  response  should  only  be  the  JSON  output  in  the  format  above,  such  that  it  can  directly  parsed  by  Python’s  json  module.  DO  NOT  OUTPUT  ANY  EXPLANATION  OR  ANYTHING  THAT  IS  NOT  THE  JSON  RESPONSE.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你将获得一个项目符号列表（每个项目有一个唯一的编号）和一个具体的参考洞察。你的目标是确定参考洞察是否在任何一个项目符号中被覆盖。你还必须进一步确定洞察是否被部分覆盖（“PARTIAL_COVERAGE”）或完全覆盖（“FULL_COVERAGE”）。如果洞察完全没有被覆盖，你必须返回“NO_COVERAGE”。请参见下面的示例:[[FEW_SHOT_EXAMPLES]]现在完成以下洞察和项目符号的任务：参考洞察：[[INSIGHT]]
    项目符号：[[BULLETS]] 需求：- 不要假设洞察被项目符号覆盖，如果它实际上没有被覆盖。- 你的回应应该仅为上述格式的 JSON 输出，以便可以直接由
    Python 的 json 模块解析。请不要输出任何解释或不相关的内容。
- en: A.3 Automatic Results Bias
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 自动结果偏差
- en: '|  | Evaluator Model |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | 评估模型 |'
- en: '|  | GPT-4o | Opus | Gem-1.5-Pro |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4o | Opus | Gem-1.5-Pro |'
- en: '| Summarizer Model Bias |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 摘要模型偏差 |'
- en: '| Claude 3 Sonnet | 0.027 | -0.001 | -0.012 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 0.027 | -0.001 | -0.012 |'
- en: '| Gemini-1.5-flash | 0.051 | 0.024 | -0.009 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-flash | 0.051 | 0.024 | -0.009 |'
- en: '| GPT3.5 | 0.009 | 0.050 | 0.048 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5 | 0.009 | 0.050 | 0.048 |'
- en: '| Claude 3 Opus | 0.059 | 0.034 | 0.043 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | 0.059 | 0.034 | 0.043 |'
- en: '| Gemini-1.5-pro | 0.088 | 0.065 | 0.065 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-pro | 0.088 | 0.065 | 0.065 |'
- en: '| GPT4-turbo | 0.075 | 0.091 | 0.056 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | 0.075 | 0.091 | 0.056 |'
- en: '| Command-r + | 0.064 | 0.128 | 0.071 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Command-r + | 0.064 | 0.128 | 0.071 |'
- en: '| Claude 3 Haiku | 0.092 | 0.108 | 0.071 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku | 0.092 | 0.108 | 0.071 |'
- en: '| GPT-4o | 0.097 | 0.102 | 0.106 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.097 | 0.102 | 0.106 |'
- en: '| Average | 0.062 | 0.067 | 0.049 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.062 | 0.067 | 0.049 |'
- en: '| Summary Length Bias |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 摘要长度偏差 |'
- en: '| Length to Score Corr. | -0.122 | -0.174 | -0.178 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 长度与评分相关性 | -0.122 | -0.174 | -0.178 |'
- en: '| Length to Delta Corr. | 0.02 | -0.051 | -0.081 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 长度与偏差相关性 | 0.02 | -0.051 | -0.081 |'
- en: 'Table 4: Results of analysis of potential bias in automated evaluation. We
    explore potential bias caused by what model is used, which is reported in differences
    of scores with human annotation, and bias due to the length of the summary, which
    is reported as a correlation. An unbiased evaluator model should achieve a bias
    close to zero on both analyses.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：自动评估潜在偏差分析结果。我们探索了由于使用的模型所造成的潜在偏差，这通过与人工标注的评分差异来报告，并且由于摘要长度所造成的偏差，这通过相关性来报告。一个无偏的评估模型在这两种分析中应实现接近零的偏差。
- en: There is a concern that since we propose to use an LLM to automate the evaluation
    of the SummHay experiment, the choice of the evaluator model might affect the
    validity of the results if such a model has a systematic bias in its judgment.
    We evaluate the possible presence of two biases. First, whether the automatic
    evaluation could favor outputs of one model family over the other (e.g., GPT-4o
    systematically favoring outputs of the GPT* family).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个担忧是，由于我们提出使用 LLM 来自动化评估 SummHay 实验，如果评估模型在判断上有系统性的偏差，选择的评估模型可能会影响结果的有效性。我们评估了两种偏差的可能存在。首先，自动评估是否可能偏向于某一模型系列的输出（例如，GPT-4o
    系统性地偏向于 GPT* 系列的输出）。
- en: 'To study this, we perform an automatic evaluation of score bias by calculating
    the difference $(\Delta)$ for each Summarizer model in our experiment, and inspect
    the bias of three evaluator models: GPT-4o, Claude3 Opus, and Gemini-1.5-pro.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究这一点，我们通过计算实验中每个摘要模型的差异 $(\Delta)$ 来进行自动评分偏差评估，并检查三个评估模型的偏差：GPT-4o、Claude3
    Opus 和 Gemini-1.5-pro。
- en: 'The top portion of Table [4](#A1.T4 "Table 4 ‣ A.3 Automatic Results Bias ‣
    Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") summarizes the Summarizer model bias analysis results. First,
    we find that auto-evaluation results almost always have a positive bias, indicating
    that on average, the auto-evaluation overestimates Coverage by roughly 5 points
    across models. Second, we find that evaluator models tend to have a positive bias
    for top-performing Summarizer models (e.g., GPT-4o, Claude3 Opus, and Gemini-1.5-pro),
    but do not systematically prefer outputs from a specific model family. In fact,
    Claude3 Opus seems to be particularly critical of Claude3 model outputs (with
    biases very close to zero). All models have a strong positive bias towards GPT-4o
    outputs, but it does not translate to bias for a model family. Overall, we find
    no evidence of systematic bias of automatic evaluation that would favor one model
    family over the other. The analysis does reveal a pattern of overestimating coverage
    by an average of 5 points, which should be taken into account when interpreting
    the results.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[4](#A1.T4 "表 4 ‣ A.3 自动结果偏差 ‣ 附录 A 附录 ‣ 针对长上下文 LLM 和 RAG 系统的挑战：针尖上的总结")的顶部部分总结了Summarizer模型偏差分析结果。首先，我们发现自动评估结果几乎总是存在正偏差，表明在平均情况下，自动评估对覆盖率的估计比实际高出大约5个百分点。其次，我们发现评估模型倾向于对顶级Summarizer模型（例如，GPT-4o、Claude3
    Opus和Gemini-1.5-pro）表现出正偏差，但并不系统性地偏好特定模型家族的输出。实际上，Claude3 Opus对Claude3模型输出似乎特别挑剔（偏差非常接近零）。所有模型对GPT-4o输出表现出强烈的正偏差，但这并没有转化为对某一模型家族的偏差。总体而言，我们没有发现自动评估系统对某一模型家族的系统性偏差。分析确实揭示了覆盖率被高估了大约5个百分点的模式，这一点在解释结果时应予以考虑。
- en: In a second analysis, we study whether automatic evaluation leads to favoring
    summaries based on their length.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次分析中，我们研究了自动评估是否会基于摘要的长度倾向于偏好某些摘要。
- en: To study length bias, we first see whether a summary’s score correlates with
    its length, as measured by the number of words divided by the number of bullet
    points. We divide by the number of bullet points as each query requires a different
    number of bullet points, which directly affects length of the summary in a way
    that is not controlled by the LLM. By measuring length bias based on the length
    of individual bullet points, we remove this confounding variable.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究长度偏差，我们首先观察摘要的分数是否与其长度相关，长度通过单词数量除以要点数量来衡量。我们通过要点数量来进行除法，因为每个查询需要不同数量的要点，这直接影响摘要的长度，而这种影响并没有被LLM控制。通过基于单个要点的长度来测量长度偏差，我们去除了这一混杂变量。
- en: 'In the Length to Score Corr. row of Table [4](#A1.T4 "Table 4 ‣ A.3 Automatic
    Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context
    LLMs and RAG Systems"), we find that there is a slight negative correlation (-0.12
    to -0.178) between a summary’s score and the number of words in its bullet points.
    This correlation could be explained by results from Section [5](#S5 "5 Results
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"), which
    showed that several of the top-performing models (Claude3-opus, Gemini-1.5-pro)
    generate the shortest bullet points. In other words, short bullet points could
    achieve higher scores not because they are short, but because they were generated
    by better models. To remove this confounding variable, we measure whether automatic
    score $\Delta$ (computed above) correlates with bullet point length. This analysis,
    summarized in te Length to Delta Corr. row of Table [4](#A1.T4 "Table 4 ‣ A.3
    Automatic Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge
    to Long-Context LLMs and RAG Systems"), indicates a non-existent correlation between
    bullet-point length, and whether the automatic evaluator was biased in its scoring
    (-0.081 to 0.02). In conclusion, we do not find evidence that using automatic
    evaluation with our evaluation protocol will cause length bias in our results,
    which would systems to generate shorter or longer bullets points.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[4](#A1.T4 "Table 4 ‣ A.3 Automatic Results Bias ‣ Appendix A Appendix ‣
    Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")的Length
    to Score Corr.行中，我们发现总结得分与其要点字数之间存在轻微的负相关（-0.12至-0.178）。这种相关性可以通过第[5](#S5 "5 Results
    ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")节的结果来解释，该节显示一些表现最好的模型（Claude3-opus,
    Gemini-1.5-pro）生成了最短的要点。换句话说，短要点能取得更高的分数，并非因为它们短，而是因为它们是由更好的模型生成的。为了消除这一混杂变量，我们测量了自动评分$\Delta$（如上所述计算）是否与要点长度相关。这项分析总结在表格[4](#A1.T4
    "Table 4 ‣ A.3 Automatic Results Bias ‣ Appendix A Appendix ‣ Summary of a Haystack:
    A Challenge to Long-Context LLMs and RAG Systems")的Length to Delta Corr.行中，表明要点长度与自动评估器评分是否存在偏差之间没有相关性（-0.081至0.02）。总之，我们没有发现使用自动评估的评价协议会导致结果的长度偏差，这会促使系统生成更短或更长的要点。'
- en: A.4 Details on Establishing SummHay Human Performance
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 关于建立SummHay人类表现的细节
- en: To establish task duration, we considered an average reading speed of 200 words
    per minute. Carefully reading the documents (roughly 12,000 words) would therefore
    require one hour. Accounting for the need to write the summary, and scan multiple
    times over documents to identify and cite insights, annotators were told they
    had a maximum of two hours to complete the task. Participants could take breaks
    (i.e., pause their work) and finish the task early if they felt the task was completed.
    After their initial sessions, participants were asked whether two hours seemed
    appropriate to complete the task without rushing, and both agreed. Participants
    sometimes used the entirety of the two hours, and in other cases completed the
    task in as little as 80 minutes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定任务的持续时间，我们考虑了每分钟阅读200个单词的平均速度。因此，仔细阅读这些文档（大约12,000个单词）需要一个小时。考虑到需要编写总结以及多次扫描文档以识别和引用见解，注释员被告知他们有最多两个小时来完成任务。参与者可以休息（即，暂停工作），如果觉得任务已完成，也可以提前完成任务。在初步阶段后，参与者被询问是否认为两个小时足够完成任务而不需要赶时间，结果都表示同意。参与者有时使用了整个两个小时，而在其他情况下，任务则在80分钟内完成。
- en: We relied on professional annotators known and trusted by our research group
    (based on performance on previous annotation work), and they were compensated
    at 25 USD per hour. One of the annotators participated in the annotation of automated
    summaries (and therefore had access to reference insights for certain subtopics),
    and we ensured that this annotator only performed the summarization task for subtopics
    and document sets they had not seen during that annotation, ensuring they had
    no prior knowledge of the documents in the Haystack.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依赖于我们研究小组熟知并信任的专业注释员（基于之前注释工作的表现），他们的报酬为每小时25美元。其中一名注释员参与了自动总结的注释（因此可以访问某些子主题的参考见解），我们确保该注释员仅对他们在注释过程中未见过的子主题和文档集进行总结，确保他们对Haystack中的文档没有先前的了解。
- en: Participants were given all documents in a shuffled order and were instructed
    to read them carefully and summarize any insight that seemed to be repeating across
    documents. Participants were told the number of present insights (similar to LLM
    prompt in our experiment). In practice, we found that participants chose to write
    slightly more bullet points than the number they were given.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者获得了按随机顺序排列的所有文档，并被指示仔细阅读它们，并总结任何似乎在文档中重复出现的见解。参与者被告知存在的见解数量（类似于我们实验中的 LLM
    提示）。实际上，我们发现参与者选择写出略多于他们所给数量的要点。
- en: Regarding citation, participants were instructed to be as thorough as possible
    and to explicitly look for additional citations when they had identified an insight.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 关于引用，参与者被指示尽可能详细，并在识别出见解后明确寻找额外的引用。
- en: Regarding tool use, participants were allowed to use string search (i.e., Ctrl+F),
    but were prohibited from copying the text from the documents, and were explicitly
    instructed they should not use ChatGPT or equivalent LLM-based interfaces in any
    way to assist them with the task. Because this cannot be strictly enforced practically,
    we rely on trusted professional annotators to complete the task.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 关于工具使用，参与者被允许使用字符串搜索（即 Ctrl+F），但禁止复制文档中的文本，并明确指示他们不应以任何方式使用 ChatGPT 或类似的 LLM
    基接口来协助他们完成任务。由于这在实际操作中难以严格执行，我们依赖受信任的专业注释员来完成任务。
- en: Participants were instructed to gradually write their summary in a text box
    in the annotation interface, and we recorded the progress on the summary during
    each annotation summary. We then performed auto-evaluation using the same settings
    used in our benchmarking experiments on the final summary of the session, as well
    as a summary every 10 minutes during the session.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者被指示逐渐在注释界面的文本框中撰写他们的总结，我们记录了每次注释总结的进展。然后，我们使用在基准测试实验中使用的相同设置对会话的最终总结以及每 10
    分钟的总结进行了自动评估。
- en: A.5 Citation Precision & Recall Analysis
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 引用精准度与召回率分析
- en: '|  | Precision | Recall | F1 (Citation) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 精确度 | 召回率 | F1（引用） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Summarizer | Orac | Full | Orac | Full | Orac | Full |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 总结器 | Oracle | 全部 | Oracle | 全部 | Oracle | 全部 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT3.5 | 46.7 | – | 17.9 | – | 23.0 | – |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5 | 46.7 | – | 17.9 | – | 23.0 | – |'
- en: '| Claude 3 Haiku | 49.3 | 24.7 | 31.6 | 24.2 | 35.6 | 14.1 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Haiku | 49.3 | 24.7 | 31.6 | 24.2 | 35.6 | 14.1 |'
- en: '| GPT4-turbo | 62.3 | 14.1 | 35.7 | 3.8 | 41.4 | 5.5 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-turbo | 62.3 | 14.1 | 35.7 | 3.8 | 41.4 | 5.5 |'
- en: '| Claude 3 Opus | 66.0 | 30.7 | 45.8 | 24.0 | 50.7 | 22.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Opus | 66.0 | 30.7 | 45.8 | 24.0 | 50.7 | 22.3 |'
- en: '| Gemini-1.5-flash | 57.8 | 38.2 | 54.5 | 44.2 | 51.7 | 32.8 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-flash | 57.8 | 38.2 | 54.5 | 44.2 | 51.7 | 32.8 |'
- en: '| Claude 3 Sonnet | 67.3 | 42.2 | 47.2 | 19.9 | 51.7 | 23.5 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Claude 3 Sonnet | 67.3 | 42.2 | 47.2 | 19.9 | 51.7 | 23.5 |'
- en: '| Command-r | 58.9 | 38.9 | 55.9 | 31.7 | 53.8 | 30.9 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Command-r | 58.9 | 38.9 | 55.9 | 31.7 | 53.8 | 30.9 |'
- en: '| GPT-4o | 65.7 | 28.0 | 51.0 | 13.0 | 54.6 | 16.2 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 65.7 | 28.0 | 51.0 | 13.0 | 54.6 | 16.2 |'
- en: '| Command-r + | 67.6 | 24.2 | 60.8 | 21.2 | 60.2 | 19.9 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Command-r + | 67.6 | 24.2 | 60.8 | 21.2 | 60.2 | 19.9 |'
- en: '| Gemini-1.5-pro | 76.2 | 59.0 | 60.9 | 52.4 | 64.1 | 51.0 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-pro | 76.2 | 59.0 | 60.9 | 52.4 | 64.1 | 51.0 |'
- en: '| Human Perf. | 78.8 | – | 82.4 | – | 76.7 | – |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 人类表现 | 78.8 | – | 82.4 | – | 76.7 | – |'
- en: 'Table 5: Breakdown of Citation Precision and Recall of models on the SummHay
    Benchmark, combined into an F1 Score (Citation Score). Numbers are reported for
    the Full Context and the Oracle settings of SummHay.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：SummHay 基准上模型的引用精准度和召回率的细分，综合成 F1 分数（引用分数）。数据报告了 SummHay 的完整上下文和 Oracle
    设置。
- en: The Citation Score in the SummHay benchmark is an F1 calculation between the
    set of cites generated by a system in a given bullet point, and the expected cites
    of the matched reference insight, based on knowledge from the Haystack generation
    of what documents include the insight. F1 is chosen as a measure to ensure that
    systems balance between precise and thorough cites.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: SummHay 基准中的引用分数是系统在给定要点中生成的引用集与匹配参考见解的预期引用之间的 F1 计算，基于 Haystack 生成的包含见解的文档的知识。选择
    F1 作为度量，以确保系统在精准和详尽的引用之间取得平衡。
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.5 Citation Precision & Recall Analysis ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    reports the precision and recall of all systems on the benchmark, as well as the
    F1 (i.e., the Citation score) to shed light on how different systems balance between
    precision and recall.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#A1.T5 "Table 5 ‣ A.5 Citation Precision & Recall Analysis ‣ Appendix
    A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems")
    报告了所有系统在基准测试中的精准度和召回率，以及 F1（即引文评分），以揭示不同系统在精准度和召回率之间的平衡。'
- en: A.6 Model Access Details
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 模型访问详情
- en: For each model in our study, we specify its model card and how it was accessed.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们研究中的每个模型，我们指定了其模型卡和访问方式。
- en: We access the Google models Gemini-1.5-pro (gemini-1.5-pro-preview-0514) and
    Gemini-1.5-flash (gemini-1.5-flash-preview-0514) through Vertex AI ²²2[https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 Vertex AI ²²2[https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)
    访问 Google 模型 Gemini-1.5-pro (gemini-1.5-pro-preview-0514) 和 Gemini-1.5-flash (gemini-1.5-flash-preview-0514)。
- en: 'We include three OpenAI models in our study: GPT-3.5-turbo (gpt-3.5-turbo-0125),
    GPT-4-turbo (gpt-4-turbo-2024-04-09), and GPT-4o (gpt-4o). All models were accessed
    through OpenAI’s official API³³3[https://github.com/openai/opeai-python](https://github.com/openai/opeai-python).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究包括三个 OpenAI 模型：GPT-3.5-turbo (gpt-3.5-turbo-0125)、GPT-4-turbo (gpt-4-turbo-2024-04-09)
    和 GPT-4o (gpt-4o)。所有模型通过 OpenAI 的官方 API³³3[https://github.com/openai/opeai-python](https://github.com/openai/opeai-python)
    访问。
- en: Cohere summarizers Command-R (cohere.command-r-v1:0) and Command-R+ (cohere.command-r-plus-v1:0)
    were accessed through Amazon Bedrock⁴⁴4[https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/),
    while Rerank3 (rerank-english-v3.0) was accessed through Cohere’s official API⁵⁵5[https://docs.cohere.com/reference/rerank](https://docs.cohere.com/reference/rerank).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Cohere 的 Summarizers Command-R (cohere.command-r-v1:0) 和 Command-R+ (cohere.command-r-plus-v1:0)
    通过 Amazon Bedrock⁴⁴4[https://aws.amazon.com/bedrock/](https://aws.amazon.com/bedrock/)
    访问，而 Rerank3 (rerank-english-v3.0) 通过 Cohere 的官方 API⁵⁵5[https://docs.cohere.com/reference/rerank](https://docs.cohere.com/reference/rerank)
    访问。
- en: 'Anthropic models were also accessed through Amazon Bedrock: Claude 3 Haiku
    (anthropic.claude-3-haiku-20240307-v1:0), Claude 3 Sonnet (anthropic.claude-3-sonnet-20240229-v1:0),
    and Claude 3 Opus (anthropic.claude-3-opus-20240229-v1:0).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 模型也通过 Amazon Bedrock 访问：Claude 3 Haiku (anthropic.claude-3-haiku-20240307-v1:0)、Claude
    3 Sonnet (anthropic.claude-3-sonnet-20240229-v1:0) 和 Claude 3 Opus (anthropic.claude-3-opus-20240229-v1:0)。
- en: The embedders Vect (sentence-transformers/all-mpnet-base-v2") and LongEmbed
    (dwzhu/e5-base-4k) were accessed through SentenceTransformers Reimers and Gurevych
    ([2019](#bib.bib46)) and huggingface’s transformers library Wolf et al. ([2019](#bib.bib52)),
    respectively.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型 Vect (sentence-transformers/all-mpnet-base-v2") 和 LongEmbed (dwzhu/e5-base-4k)
    通过 SentenceTransformers Reimers 和 Gurevych ([2019](#bib.bib46)) 和 huggingface
    的 transformers 库 Wolf 等 ([2019](#bib.bib52)) 访问。
- en: A.7 Additional Output Examples
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 额外输出示例
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Investigating Low Scores. ‣ A.8 Additional Discussion
    ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") provides four examples of real summary outputs from different
    RAG pipelines on a common subtopic related to managing stress when preparing to
    an exam. For each summary, we also report on the Coverage, Citation and Joint
    scores, as calculated by the LLM-based automatic evaluation. We add color coding
    and bolding to facilitate the interpretation of the evaluation.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#A1.F4 "Figure 4 ‣ Investigating Low Scores. ‣ A.8 Additional Discussion
    ‣ Appendix A Appendix ‣ Summary of a Haystack: A Challenge to Long-Context LLMs
    and RAG Systems") 提供了四个不同 RAG 流水线在一个共同子主题（关于考试准备时如何管理压力）上的真实摘要输出示例。对于每个摘要，我们还报告了覆盖率、引文和联合评分，按
    LLM 基于自动评估的计算结果。我们添加了颜色编码和粗体以便于评估的解读。'
- en: A.8 Additional Discussion
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.8 额外讨论
- en: We point to several additional areas for future work.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指出了几个未来工作的额外领域。
- en: English-centric
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 以英语为中心
- en: While our data pipeline can be extended to non-English languages with access
    to a seed scenario in a given language, our benchmark was developed only on English
    and may be more reliable in English. However, the task is language-agnostic, and
    future work can create a multilingual version of the SummHay task, similar to
    efforts such as Seahorse Clark et al. ([2023](#bib.bib10)).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的数据管道可以扩展到其他语言，只要能访问到特定语言的种子场景，但我们的基准测试仅在英语上开发，可能在英语中更可靠。然而，这项任务是语言无关的，未来的工作可以创建类似于
    Seahorse Clark 等 ([2023](#bib.bib10)) 的 SummHay 任务的多语言版本。
- en: Beyond Relevance
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超越相关性
- en: We restrict our data synthesis process and analysis to target summarization
    relevance, but we believe that similar data procedures and evaluations could be
    applied to factual consistency. We leave an extension of our pipeline and analysis
    of model outputs along other dimensions such as coherence, efficiency (brevity),
    or factuality to future work.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据合成过程和分析限制于目标摘要相关性，但我们相信类似的数据程序和评估可以应用于事实一致性。我们将管道扩展和模型输出在其他维度（如连贯性、效率（简洁性）或真实性）的分析留待未来工作。
- en: Focus on Factoid-Style Insights.
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 专注于事实型见解。
- en: Our Haystack synthesis process incentivizes the creation of specific insights
    that focus on a number or entity. Specificity helps simplify evaluation, and ensure
    we can achieve reproducible automatic evaluation. Yet real-world scenarios might
    have less clear-cut insights, with different documents only partially overlapping
    on insights, or with potentially disagreeing conclusions (e.g., some people like
    the Pomodoro Technique while others don’t). Prior work has shown that NLP methods
    struggle in such cases of coverage diversity Laban et al. ([2022b](#bib.bib30));
    Huang et al. ([2023](#bib.bib20)), and including such discord within the Haystack
    could yield more complex and realistic tasks.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Haystack 合成过程激励了关注数字或实体的具体见解的创建。具体性有助于简化评估，并确保我们能实现可重复的自动评估。然而，现实世界的场景可能具有不那么明确的见解，不同的文档仅在见解上部分重叠，或者可能得出不同的结论（例如，有些人喜欢番茄工作法，而有些人则不喜欢）。以往的工作表明，NLP
    方法在这种覆盖多样性的情况下表现困难 Laban 等（[2022b](#bib.bib30)）；Huang 等（[2023](#bib.bib20)），将这种分歧包含在
    Haystack 中可能会产生更复杂和现实的任务。
- en: Investigating Low Scores.
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调查低分情况。
- en: Manual inspection reveals different failure modes in low-scoring summaries,
    including (1) retrieving insights that are not relevant to the query (other subtopics),
    (2) framing very high-level information as a specific insight (e.g., “all the
    participants are polite to each other”), (3) hallucinating insights not directly
    supported in the documents. We do not systematically evaluate the frequency of
    each failure, but future work can explore this more systematically, for example
    using efficient NLI-based alignment Laban et al. ([2022a](#bib.bib29)); Zha et al.
    ([2023](#bib.bib55)); Tang et al. ([2024](#bib.bib50)).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 手动检查揭示了低分摘要中的不同失败模式，包括（1）检索与查询不相关的见解（其他子主题），（2）将非常高层次的信息框架化为具体见解（例如，“所有参与者都对彼此很礼貌”），（3）虚构未直接在文档中支持的见解。我们没有系统地评估每种失败的频率，但未来的工作可以更系统地探索这一点，例如使用高效的基于NLI的对齐方法
    Laban 等（[2022a](#bib.bib29)）；Zha 等（[2023](#bib.bib55)）；Tang 等（[2024](#bib.bib50)）。
- en: '![Refer to caption](img/0b1c5d46d8833f97caef53c7c6ebabc4.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b1c5d46d8833f97caef53c7c6ebabc4.png)'
- en: 'Figure 4: Examples of five insights within a subtopic, and four SummHay outputs
    from RAG systems, including the final Coverage, Citation, and Joint scores, as
    calculated by our LLM-based automatic evaluation. We add color coding and bolding
    to facilitate the interpretation of the evaluation.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：子主题中五个见解的示例，以及来自 RAG 系统的四个 SummHay 输出，包括我们的 LLM 基于自动评估计算的最终覆盖、引用和联合分数。我们添加了颜色编码和加粗，以便于对评估的解释。
