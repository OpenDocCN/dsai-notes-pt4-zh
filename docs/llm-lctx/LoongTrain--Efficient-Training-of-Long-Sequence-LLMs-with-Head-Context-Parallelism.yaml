- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:01:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:01:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoongTrain：使用头部-上下文并行高效训练长序列LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18485](https://ar5iv.labs.arxiv.org/html/2406.18485)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.18485](https://ar5iv.labs.arxiv.org/html/2406.18485)
- en: Diandian Gu School of Computer Science
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 顾点点计算机科学学院
- en: Peking University ,  Peng Sun Sensetime Research &
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学，孙鹏商汤研究院 &
- en: Shanghai AI Laboratory ,  Qinghao Hu S-Lab, NTU &
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 上海人工智能实验室，黄青浩S-Lab，南洋理工大学 &
- en: Shanghai AI Laboratory ,  Ting Huang Sensetime Research ,  Xun Chen Sensetime
    Research ,  Yingtong Xiong Shanghai AI Laboratory ,  Guoteng Wang Shanghai AI
    Laboratory ,  Qiaoling Chen S-Lab, NTU ,  Shangchun Zhao Tencent ,  Jiarui Fang
    Tencent ,  Yonggang Wen Nanyang Technological University ,  Tianwei Zhang Nanyang
    Technological University ,  Xin Jin School of Computer Science
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 上海人工智能实验室，黄婷婷商汤研究院，陈迅商汤研究院，熊颖童上海人工智能实验室，王国腾上海人工智能实验室，陈巧玲S-Lab，南洋理工大学，赵尚春腾讯，方佳瑞腾讯，文永刚南洋理工大学，张天伟南洋理工大学，金鑫计算机科学学院
- en: Peking University  and  Xuanzhe Liu School of Computer Science
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学，刘轩哲计算机科学学院
- en: Peking University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学
- en: Abstract.
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Efficiently training LLMs with long sequences is important yet challenged by
    the massive computation and memory requirements. Sequence parallelism has been
    proposed to tackle these problems, but existing methods suffer from scalability
    or efficiency issues. We propose LoongTrain, a novel system to efficiently train
    LLMs with long sequences at scale. The core of LoongTrain is the 2D-Attention
    mechanism, which combines both head-parallel and context-parallel techniques to
    break the scalability constraints while maintaining efficiency. We introduce Double-Ring-Attention
    and analyze the performance of device placement strategies to further speed up
    training. We implement LoongTrain with the hybrid ZeRO and Selective Checkpoint++
    techniques. Experiment results show that LoongTrain outperforms state-of-the-art
    baselines, i.e., DeepSpeed-Ulysses and Megatron Context Parallelism, in both end-to-end
    training speed and scalability, and improves Model FLOPs Utilization (MFU) by
    up to 2.88$\times$.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高效训练长序列LLM非常重要，但面临大量计算和内存需求的挑战。序列并行性已被提出以解决这些问题，但现有方法在可扩展性或效率方面存在问题。我们提出了LoongTrain，一个新颖的系统，旨在大规模高效训练长序列LLM。LoongTrain的核心是2D-Attention机制，它结合了头部并行和上下文并行技术，以打破可扩展性限制，同时保持效率。我们引入了Double-Ring-Attention，并分析了设备布局策略的性能，以进一步加快训练速度。我们使用混合ZeRO和Selective
    Checkpoint++技术实现了LoongTrain。实验结果表明，LoongTrain在端到端训练速度和可扩展性方面均优于现有的最先进基准，即DeepSpeed-Ulysses和Megatron上下文并行，并且将模型FLOPs利用率（MFU）提高了高达2.88$\times$。
- en: Distributed Training, Sequence Parallelism, Distributed Attention
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练，序列并行性，分布式注意力
- en: 1\. Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: With the emergence of Large Language Models (LLM) in recent years, researchers
    have investigated and proposed many advanced training methodologies in a distributed
    way, such as data parallelism (DP) ([KrizhevskySH12,](#bib.bib23) ; [paszke2019pytorch,](#bib.bib36)
    ; [li2014scaling,](#bib.bib25) ; [li2014communication,](#bib.bib26) ), tensor
    parallelism (TP) ([DeanCMCDLMRSTYN12,](#bib.bib15) ), pipeline parallelism (PP) ([GPipe,](#bib.bib20)
    ; [AthlurSSRK22,](#bib.bib4) ), PyTorch FSDP ([PyTorchFSDP,](#bib.bib52) ), and
    automatic parallelization frameworks ([Alpa,](#bib.bib53) ). Recently, LLMs with
    long sequences have driven the development of novel applications that are essential
    in our daily lives, including generative AI ([ni2023recent,](#bib.bib33) ) and
    long-context understanding ([beltagy2020longformer,](#bib.bib5) ; [zhou2021document,](#bib.bib54)
    ; [ding2023longnet,](#bib.bib16) ). With the increased popularity of ChatGPT,
    long dialogue processing tasks have become more important for chatbot applications
    than ever ([touvron2023llama,](#bib.bib45) ). In addition to these scenarios for
    language processing, Transformer-based giant models also achieve impressive performance
    in computer vision ([zhang2020span,](#bib.bib50) ; [arnab2021vivit,](#bib.bib3)
    ; [yuan2021tokens,](#bib.bib49) ) and AI for science ([bi2023accurate,](#bib.bib6)
    ; [ai4science,](#bib.bib30) ), where inputs with long sequences are critical for
    complex tasks such as video stream processing ([ruan2022survey,](#bib.bib41) )
    and protein property prediction ([chandra2023transformer,](#bib.bib9) ).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着近年来大型语言模型（LLM）的出现，研究人员已经调查并提出了许多先进的分布式训练方法，如数据并行（DP） ([KrizhevskySH12,](#bib.bib23)
    ; [paszke2019pytorch,](#bib.bib36) ; [li2014scaling,](#bib.bib25) ; [li2014communication,](#bib.bib26)
    )、张量并行（TP） ([DeanCMCDLMRSTYN12,](#bib.bib15) )、流水线并行（PP） ([GPipe,](#bib.bib20)
    ; [AthlurSSRK22,](#bib.bib4) )、PyTorch FSDP ([PyTorchFSDP,](#bib.bib52) ) 和自动并行化框架
    ([Alpa,](#bib.bib53) )。最近，具有长序列的LLM推动了新型应用的发展，这些应用在我们的日常生活中至关重要，包括生成性AI ([ni2023recent,](#bib.bib33)
    ) 和长上下文理解 ([beltagy2020longformer,](#bib.bib5) ; [zhou2021document,](#bib.bib54)
    ; [ding2023longnet,](#bib.bib16) )。随着ChatGPT的普及，长对话处理任务在聊天机器人应用中变得比以往任何时候都重要 ([touvron2023llama,](#bib.bib45)
    )。除了这些语言处理场景，基于Transformer的巨大模型在计算机视觉 ([zhang2020span,](#bib.bib50) ; [arnab2021vivit,](#bib.bib3)
    ; [yuan2021tokens,](#bib.bib49) ) 和科学AI ([bi2023accurate,](#bib.bib6) ; [ai4science,](#bib.bib30)
    ) 中也取得了令人印象深刻的表现，其中长序列的输入对于复杂任务如视频流处理 ([ruan2022survey,](#bib.bib41) ) 和蛋白质属性预测
    ([chandra2023transformer,](#bib.bib9) ) 是至关重要的。
- en: 'Training LLMs with long sequences requires massive memory resources and computation.
    To tackle these challenges, sequence parallelism (SP) has been proposed ([DeepspeedUlysses,](#bib.bib21)
    ; [lightseq,](#bib.bib24) ; [BPT2,](#bib.bib29) ; [megatroncp,](#bib.bib34) ),
    which can be basically divided into two categories: head parallelism (HP) ([DeepspeedUlysses,](#bib.bib21)
    ) and context parallelism (CP) ([BPT2,](#bib.bib29) ; [megatroncp,](#bib.bib34)
    ). In Attention blocks, HP methods keep the whole sequence and compute attention
    for different heads in parallel, while CP methods split the QKV (Query, Key, and
    Value) tensors into chunks along the sequence dimension. However, both face limitations
    when applied to extremely-long-sequence LLMs at a large scale. First, HP meets
    the scalability issue. In HP, the degree of SP inherently cannot exceed the number
    of attention heads ([DeepspeedUlysses,](#bib.bib21) ). Therefore, there is an
    upper bound for the degree that HP can scale out. Second, CP meets the communication
    inefficiency issue. CP ([BPT2,](#bib.bib29) ; [megatroncp,](#bib.bib34) ) employs
    a peer-to-peer (P2P) communication primitive. However, P2P encounters issues of
    low intra-node bandwidth utilization and low inter-node network resource utilization.
    This bottleneck makes it challenging to overlap communication with computation
    when scaling out the context-parallel dimension. For example, our experiments
    show that Ring-Attention can spend 1.8$\times$ time on communication than on computation
    when running Grouped Query Attention (GQA) on 64 GPUs with a sequence length of
    128K (Figure [5](#S3.F5 "Figure 5 ‣ 3.2\. Inefficient Performance of Ring-Attention
    ‣ 3\. Motivation & Observation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism")(d)).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练具有长序列的LLM需要大量的内存资源和计算能力。为了解决这些挑战，提出了序列并行性（SP）（[DeepspeedUlysses,](#bib.bib21)；[lightseq,](#bib.bib24)；[BPT2,](#bib.bib29)；[megatroncp,](#bib.bib34)），基本上可以分为两类：头部并行（HP）（[DeepspeedUlysses,](#bib.bib21)）和上下文并行（CP）（[BPT2,](#bib.bib29)；[megatroncp,](#bib.bib34)）。在注意力块中，HP方法保持整个序列并对不同头进行并行计算，而CP方法将QKV（查询、键和值）张量沿序列维度分成块。然而，当应用于大规模的极长序列LLM时，两者都面临限制。首先，HP遇到了可扩展性问题。在HP中，SP的程度本质上不能超过注意力头的数量（[DeepspeedUlysses,](#bib.bib21)）。因此，HP的可扩展程度有一个上限。其次，CP遇到了通信效率低的问题。CP（[BPT2,](#bib.bib29)；[megatroncp,](#bib.bib34)）采用了点对点（P2P）通信原语。然而，P2P面临着低的节点内部带宽利用率和低的节点间网络资源利用率的问题。这一瓶颈使得在扩展上下文并行维度时，通信和计算的重叠变得具有挑战性。例如，我们的实验表明，当在64个GPU上运行分组查询注意力（GQA），序列长度为128K时，Ring-Attention在通信上花费的时间是计算时间的1.8倍（图[5](#S3.F5
    "图5 ‣ 3.2\. Ring-Attention的低效性能 ‣ 3\. 动机与观察 ‣ LoongTrain：通过头-上下文并行高效训练长序列LLM")（d））。
- en: '| $S$ | Sequence Parallel Size |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| $S$ | 序列并行大小 |'
- en: '| $H$ | Data Parallel Size |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| $H$ | 数据并行大小 |'
- en: '| $H_{kv}$ | Head Parallel Size |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| $H_{kv}$ | 头部并行大小 |'
- en: '| $D$ | Context Parallel Size |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| $D$ | 上下文并行大小 |'
- en: '| $B$ | Inner Ring Size |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| $B$ | 内部环大小 |'
- en: Table 1\. Notations used in this paper.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 本文使用的符号。
- en: To bridge these gaps, we propose LoongTrain, an effective training framework
    for long-sequence LLMs on large-scale GPU clusters. Our key idea is to address
    the scalability constraints of HP while mitigating the inefficiencies of CP by
    introducing a novel 2D-Attention mechanism. This mechanism parallelizes attention
    across both HP and CP dimensions. Specifically, it distributes the QKV tensors
    across GPUs based on the head dimension and partitions these tensors into chunks
    within the CP dimension. By doing so, LoongTrain enhances scalability through
    the integration of CP and reduces the number of P2P steps by confining the CP
    dimension size. In addition, this design provides more opportunities for computation-communication
    overlap.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些不足，我们提出了LoongTrain，一种针对大规模GPU集群上长序列LLM的有效训练框架。我们的关键思想是通过引入一种新颖的2D-Attention机制，来解决HP的可扩展性限制，同时减轻CP的低效性。该机制在HP和CP维度上并行化注意力。具体来说，它根据头部维度将QKV张量分配到GPU，并在CP维度内将这些张量划分为块。通过这样做，LoongTrain通过整合CP增强了可扩展性，并通过限制CP维度大小减少了P2P步骤的数量。此外，这一设计提供了更多的计算与通信重叠的机会。
- en: To further improve the communication efficiency of Attention blocks in certain
    circumstances, we introduce Double-Ring-Attention, which utilizes all of the inter-node
    NICs efficiently for higher peer-to-peer communication bandwidth. We also analyze
    how different placement strategies can boost the communication efficiency in different
    2D-Attention configurations. Finally, we implement advanced techniques such as
    applying ZeRO across both DP and PP dimensions and a whitelist-based gradient
    checkpointing mechanism Selective Checkpoint++ to further improve the end-to-end
    LLM training performance. Evaluation results on training LLMs with up to 1M sequences
    show that LoongTrain can bring up to 2.88$\times$ performance improvement compared
    to existing state-of-the-art solutions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高在特定情况下 Attention 块的通信效率，我们引入了 Double-Ring-Attention，它有效利用所有的节点间 NICs
    来提升点对点通信带宽。我们还分析了不同的布局策略如何在不同的 2D-Attention 配置中提升通信效率。最后，我们实现了先进的技术，例如在 DP 和 PP
    维度上应用 ZeRO，以及基于白名单的梯度检查点机制 Selective Checkpoint++，以进一步提升端到端 LLM 训练性能。对训练最多 1M
    序列的 LLM 的评估结果显示，LoongTrain 相比于现有的最先进解决方案，性能提升可达 2.88$\times$。
- en: LoongTrain has been deployed to train multiple long-sequence LLMs within our
    organization. The system is implemented within our internal training framework,
    which can be accessed at [https://github.com/InternLM/InternEvo](https://github.com/InternLM/InternEvo).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LoongTrain 已经被部署用于训练我们组织内的多个长序列 LLM。该系统在我们的内部训练框架中实现，可以通过 [https://github.com/InternLM/InternEvo](https://github.com/InternLM/InternEvo)
    访问。
- en: 2\. Background
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: 2.1\. LLM Architecture with MHA/GQA
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM 架构与 MHA/GQA
- en: 'LLMs like GPT ([GPT3,](#bib.bib8) ) and LLaMA ([LLaMA,](#bib.bib43) ) utilize
    the Transformer architecture ([Attention,](#bib.bib46) ), which consists of multiple
    layers. As shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. LLM Architecture with
    MHA/GQA ‣ 2\. Background ‣ LoongTrain: Efficient Training of Long-Sequence LLMs
    with Head-Context Parallelism"), each layer includes an Attention block and a
    Feed-Forward Network (FFN) block. Within the Attention block, a linear module
    projects the input tensor into three tensors: Query ($Q$, where $W_{1},W_{2},W_{3}$
    are all linear modules.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '像 GPT ([GPT3,](#bib.bib8)) 和 LLaMA ([LLaMA,](#bib.bib43)) 这样的 LLM 利用 Transformer
    架构 ([Attention,](#bib.bib46))，它由多个层组成。如图 [1](#S2.F1 "图 1 ‣ 2.1\. LLM 架构与 MHA/GQA
    ‣ 2\. 背景 ‣ LoongTrain: 使用头-上下文并行性的长序列 LLM 高效训练") 所示，每一层包括一个 Attention 块和一个 Feed-Forward
    Network (FFN) 块。在 Attention 块中，一个线性模块将输入张量投影为三个张量：Query ($Q$)，其中 $W_{1},W_{2},W_{3}$
    都是线性模块。'
- en: '![Refer to caption](img/c1e1c4569e9a55da3e458fd7d93aed7d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c1e1c4569e9a55da3e458fd7d93aed7d.png)'
- en: Figure 1. A typical Transformer layer contains an Attention block and a Feed-Forward
    Network (FFN) block.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 一个典型的 Transformer 层包含一个 Attention 块和一个 Feed-Forward Network (FFN) 块。
- en: Multi-Head Attention (MHA) ([MHA,](#bib.bib47) ) splits $Q$ heads. Suppose the
    original $Q$. They will be reshaped to $(H,S,D/H)$ and $V$ and $H=32$ .
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-Head Attention (MHA) ([MHA,](#bib.bib47)) 将 $Q$ 头分开。假设原始的 $Q$。它们将被重塑为
    $(H,S,D/H)$ 和 $V$ 和 $H=32$。
- en: 2.2\. Distributed LLM Training
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 分布式 LLM 训练
- en: Hybrid parallelism ([Megatron-LM,](#bib.bib31) ) and Zero Redundancy Optimizer
    (ZeRO) ([ZeRO,](#bib.bib39) ) are commonly employed to train LLMs at scale. Specifically,
    data parallelism (DP) divides input data into chunks, distributing them across
    multiple GPUs to parallelize training. Tensor parallelism (TP) distributes model
    parameters across GPUs along specific dimensions, enabling parallel computation
    of the model layers ([TP,](#bib.bib32) ). Pipeline parallelism (PP) splits layers
    of a model into multiple stages, distributing them across GPUs ([GPipe,](#bib.bib20)
    ; [pipedream,](#bib.bib18) ). Each pipeline stage depends on the outputs of previous
    stages, leading to computation stalls known as pipeline bubbles. Advanced pipeline
    schedulers, such as 1F1B ([pipedream,](#bib.bib18) ) and ZeRO-Bubble ([zerobubble,](#bib.bib37)
    ), have been proposed to reduce the bubble ratio. ZeRO ([ZeRO,](#bib.bib39) )
    addresses redundant memory usage across DP ranks. ZeRO-1 partitions optimizer
    states across GPUs, ensuring each GPU stores only a fraction of the optimizer
    state. ZeRO-2 extends this by also sharding gradients, and ZeRO-3 further distributes
    model parameters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 混合并行性 ([Megatron-LM,](#bib.bib31) ) 和零冗余优化器 (ZeRO) ([ZeRO,](#bib.bib39) ) 通常用于大规模训练
    LLMs。具体而言，数据并行性 (DP) 将输入数据分成块，分布在多个 GPU 上以实现训练的并行化。张量并行性 (TP) 沿特定维度将模型参数分布在 GPU
    上，从而实现模型层的并行计算 ([TP,](#bib.bib32) )。流水线并行性 (PP) 将模型层分成多个阶段，分布在 GPU 上 ([GPipe,](#bib.bib20)
    ; [pipedream,](#bib.bib18) )。每个流水线阶段依赖于前一个阶段的输出，导致计算停滞，称为流水线气泡。先进的流水线调度器，如 1F1B
    ([pipedream,](#bib.bib18) ) 和 ZeRO-Bubble ([zerobubble,](#bib.bib37) )，已被提出以减少气泡比率。ZeRO
    ([ZeRO,](#bib.bib39) ) 解决了 DP 级别间的冗余内存使用问题。ZeRO-1 将优化器状态分区到 GPU 上，确保每个 GPU 仅存储优化器状态的一部分。ZeRO-2
    在此基础上进一步分片梯度，而 ZeRO-3 则进一步分布模型参数。
- en: '![Refer to caption](img/515a6ed3985655af0b806ad8b4455e12.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/515a6ed3985655af0b806ad8b4455e12.png)'
- en: Figure 2. Ulyssess-Attention performs head-parallel computation across GPUs
    with two steps of AlltoAll.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. Ulysses-Attention 在两个 AlltoAll 步骤中执行跨 GPU 的头并行计算。
- en: To support long-sequence training, sequence parallelism (SP) has emerged as
    an effective technique to mitigate activation memory footprints ([DeepspeedUlysses,](#bib.bib21)
    ; [Nvidia3,](#bib.bib22) ; [lightseq,](#bib.bib24) ). In SP, the input and output
    tensors of each Transformer layer are partitioned into $d_{sp}$ chunks along the
    sequence dimension. Megatron-LM integrates SP with TP across different modules
    ([Nvidia3,](#bib.bib22) ). Specifically, TP is utilized to parallelize the linear
    modules, while SP is applied to normalization and dropout modules. To ensure consistency
    in computational results, Megatron-LM incorporates necessary AllGather and ReduceScatter
    operations to transfer activations during training. However, as the sequence length
    increases, the communication overhead associated with transferring activations
    also grows, leading to significant communication challenges ([DeepspeedUlysses,](#bib.bib21)
    ; [hu2024characterization,](#bib.bib19) ).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持长序列训练，序列并行性 (SP) 已成为减少激活内存占用的有效技术 ([DeepspeedUlysses,](#bib.bib21) ; [Nvidia3,](#bib.bib22)
    ; [lightseq,](#bib.bib24) )。在 SP 中，每个 Transformer 层的输入和输出张量沿序列维度被分成 $d_{sp}$ 个块。Megatron-LM
    在不同模块中将 SP 与 TP 结合使用 ([Nvidia3,](#bib.bib22) )。具体而言，TP 用于并行化线性模块，而 SP 应用于归一化和丢弃模块。为了确保计算结果的一致性，Megatron-LM
    结合了必要的 AllGather 和 ReduceScatter 操作，以在训练过程中传输激活值。然而，随着序列长度的增加，传输激活值的通信开销也会增加，从而导致显著的通信挑战
    ([DeepspeedUlysses,](#bib.bib21) ; [hu2024characterization,](#bib.bib19) )。
- en: 'To address this problem in the integration of SP and TP, recent approaches
    implement SP across all linear modules and utilize ZeRO-3 to reduce memory footprints.
    This eliminates the need for collective communications on activations. They perform
    AllGather to collect the parameters of linear modules before computation, which
    do not increase with the sequence length. Following this strategy, two methods
    have been introduced to facilitate distributed attention computation: Ulysses-Attention
    ([DeepspeedUlysses,](#bib.bib21) ) and Ring-Attention ([lightseq,](#bib.bib24)
    ; [BPT2,](#bib.bib29) ), as described below.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 SP 和 TP 集成中的这个问题，最近的方法在所有线性模块中实现 SP，并利用 ZeRO-3 来减少内存占用。这消除了对激活值的集体通信需求。它们在计算之前执行
    AllGather 来收集线性模块的参数，这些参数不会随着序列长度增加。按照这种策略，已经引入了两种方法来促进分布式注意力计算：Ulysses-Attention
    ([DeepspeedUlysses,](#bib.bib21) ) 和 Ring-Attention ([lightseq,](#bib.bib24) ;
    [BPT2,](#bib.bib29) )，具体如下。
- en: 2.3\. Distributed Attention
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 分布式注意力
- en: Ulysses-Attention ([DeepspeedUlysses,](#bib.bib21) ) performs head-parallel
    computation across GPUs ($d_{hp}=d_{sp}$ heads. Each GPU then computes the attention
    for different heads in parallel. Finally, another AlltoAll operation gathers the
    results across the head dimension while re-partitioning along the sequence dimension.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Ulysses-Attention ([DeepspeedUlysses,](#bib.bib21) )在GPU之间执行头部并行计算（$d_{hp}=d_{sp}$头）。每个GPU随后并行计算不同头部的注意力。最后，另一个AlltoAll操作在头部维度上汇总结果，同时沿序列维度重新分区。
- en: '![Refer to caption](img/b1c2cd57d96254fff7a60453489bec2d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b1c2cd57d96254fff7a60453489bec2d.png)'
- en: (a) Without Load Balance
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 无负载均衡
- en: '![Refer to caption](img/09d284f3412563e1ea86d01ba32b3aa9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/09d284f3412563e1ea86d01ba32b3aa9.png)'
- en: (b) With Load-Balance
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 带负载均衡
- en: Figure 3. Ring-Attention performs context-parallel computation, and organizes
    communication in a ring fashion. 1 or 0 represents that whether there is computation
    between QKV.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. Ring-Attention执行上下文并行计算，并以环形方式组织通信。1或0表示QKV之间是否有计算。
- en: 'Ring-Attention ([lightseq,](#bib.bib24) ; [BPT2,](#bib.bib29) ) leverages blockwise
    attention ([self-attnnotneedon2memory,](#bib.bib38) ; [BPT1,](#bib.bib27) ; [flashattn1,](#bib.bib14)
    ) and performs context-parallel computation ($d_{cp}=d_{sp}$), as shown in Figure
    [3](#S2.F3 "Figure 3 ‣ 2.3\. Distributed Attention ‣ 2\. Background ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism"). This
    method partitions QKV tensors into chunks along the sequence dimension, with each
    GPU initially assigned one chunk. For each query chunk, its corresponding attention
    output is computed by iterating over all KV chunks. Communication is organized
    in a ring fashion, where each GPU simultaneously sends and receives KV chunks,
    allowing communication to be overlapped with computation. FlashAttention ([flashattn1,](#bib.bib14)
    ) can still be used to maintain the IO-aware benefits of memory-efficient computation.
    However, the standard Ring-Attention approach is not load-balanced when applying
    a causal attention mask, since only the lower triangular portion of the matrix
    needs to be computed. To address this issue, several methods have been proposed,
    such as DistFlashAttn ([lightseq,](#bib.bib24) ) and Striped-Attention ([StripedAttention,](#bib.bib7)
    ). As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.3\. Distributed Attention ‣ 2\.
    Background ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context
    Parallelism")(b), Megatron-LM reorders the input sequence tokens along the sequence
    dimension to achieve load balance in its implementation. In this paper, Ring-Attention
    is assumed to be load-balanced by default.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ring-Attention ([lightseq,](#bib.bib24) ; [BPT2,](#bib.bib29) )利用块级注意力 ([self-attnnotneedon2memory,](#bib.bib38)
    ; [BPT1,](#bib.bib27) ; [flashattn1,](#bib.bib14) )并执行上下文并行计算（$d_{cp}=d_{sp}$），如图
    [3](#S2.F3 "Figure 3 ‣ 2.3\. Distributed Attention ‣ 2\. Background ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")所示。该方法将QKV张量沿序列维度分割成块，每个GPU最初分配一个块。对于每个查询块，其对应的注意力输出通过遍历所有KV块来计算。通信以环形方式组织，其中每个GPU同时发送和接收KV块，从而允许通信与计算重叠。FlashAttention
    ([flashattn1,](#bib.bib14) )仍然可以用来保持内存高效计算的IO感知优点。然而，标准的Ring-Attention方法在应用因果注意力掩码时不是负载均衡的，因为只需要计算矩阵的下三角部分。为了解决这个问题，已经提出了几种方法，如DistFlashAttn
    ([lightseq,](#bib.bib24) )和Striped-Attention ([StripedAttention,](#bib.bib7) )。如图
    [3](#S2.F3 "Figure 3 ‣ 2.3\. Distributed Attention ‣ 2\. Background ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")(b)所示，Megatron-LM在其实现中沿序列维度重新排序输入序列令牌，以实现负载均衡。本文假设Ring-Attention默认是负载均衡的。'
- en: 3\. Motivation & Observation
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 动机与观察
- en: 'Given the long computation time of LLM training, especially with long sequences,
    it is essential to scale long-sequence model training to large-scale clusters.
    However, current SP approaches face two significant challenges: limited scalability
    and high communication overhead.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM训练的计算时间较长，尤其是对于长序列，因此将长序列模型训练扩展到大规模集群是至关重要的。然而，当前的SP方法面临两个重大挑战：可扩展性有限和高通信开销。
- en: '![Refer to caption](img/7a89d372b510b44a738bfc5d1bd39220.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a89d372b510b44a738bfc5d1bd39220.png)'
- en: (a) Maximum GPU Scalability
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 最大GPU可扩展性
- en: '![Refer to caption](img/e1419a1c9992b35988381a3f35dd78a9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e1419a1c9992b35988381a3f35dd78a9.png)'
- en: (b) Pipeline Bubble Rate
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 管道气泡率
- en: Figure 4. Limited scalability of Ulysses-Attention constrained by a global batch
    size of 4M tokens. (a) Maximum GPU scalability without Pipeline Parallelism. (b)
    Pipeline bubble rate, using $d_{dp}=4,d_{sp}=64,d_{pp}=4$ on 1024 GPUs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 由于 4M token 全球批量大小的限制，Ulysses-Attention 的扩展性有限。 (a) 没有 Pipeline Parallelism
    的最大 GPU 扩展性。 (b) Pipeline bubble rate，使用 $d_{dp}=4,d_{sp}=64,d_{pp}=4$ 在 1024
    个 GPU 上。
- en: 3.1\. Limited Scalability of Ulysses-Attention
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. Ulysses-Attention 的有限扩展性
- en: 'Ulysses-Attention cannot scale long-sequence training to large-scale clusters
    due to the limitations in the maximum degrees of SP, DP, and PP. First, SP is
    sensitive to the number of attention heads. When using MHA, the SP degree cannot
    exceed the number of attention heads; while in the case of GQA, the SP degree
    is limited by the number of key/value heads. For instance, LLaMA3-8B uses GQA
    with 8 key/value heads, meaning that the maximum SP degree is 8 when using Ulysses-Attention.
    Even if we repeat key/value heads, as detailed in Section [4.1](#S4.SS1 "4.1\.
    2D-Attention Overview ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training
    of Long-Sequence LLMs with Head-Context Parallelism"), the maximum SP degree remains
    32.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 SP、DP 和 PP 最大度数的限制，Ulysses-Attention 无法将长序列训练扩展到大规模集群。首先，SP 对注意力头的数量很敏感。使用
    MHA 时，SP 度数不能超过注意力头的数量；而在 GQA 的情况下，SP 度数受限于关键/值头的数量。例如，LLaMA3-8B 使用 8 个关键/值头的
    GQA，这意味着使用 Ulysses-Attention 时最大 SP 度数为 8。即使我们重复关键/值头，如第 [4.1](#S4.SS1 "4.1\.
    2D-Attention Overview ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training
    of Long-Sequence LLMs with Head-Context Parallelism") 节中所述，最大 SP 度数仍然是 32。'
- en: 'It is impractical to rely on increasing the degree of DP to scale out the training
    process due to the constraint of the global batch size. For instance, when training
    a Transformer model with 32 attention heads and employing a global batch size
    of 4M tokens—as exemplified in the world model training ([liu2024world,](#bib.bib28)
    )—and a sequence length of 1M tokens, the maximum attainable degree of DP is 4\.
    Under these conditions, the training process can only be scaled up to 128 GPUs
    when utilizing Ulysses-Attention. The maximum number of GPUs that Ulysses-Attention
    could use within the constraint of a 4M global batch size is illustrated in Figure
    [4](#S3.F4 "Figure 4 ‣ 3\. Motivation & Observation ‣ LoongTrain: Efficient Training
    of Long-Sequence LLMs with Head-Context Parallelism") (a).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '由于全球批量大小的限制，依赖于增加 DP 度数来扩展训练过程是不切实际的。例如，当训练一个具有 32 个注意力头的 Transformer 模型，并采用全球批量大小为
    4M token 的情况——如世界模型训练（[liu2024world,](#bib.bib28)）——且序列长度为 1M token 时，最大可达的 DP
    度数为 4。在这些条件下，利用 Ulysses-Attention 的训练过程只能扩展到 128 个 GPU。Ulysses-Attention 在 4M
    全球批量大小限制下可以使用的最大 GPU 数量在图 [4](#S3.F4 "Figure 4 ‣ 3\. Motivation & Observation
    ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")
    (a) 中有所示。'
- en: 'While we can scale out long-sequence training to more GPUs by increasing the
    degree of PP, it can lead to a high bubble rate. Due to the global batch size
    constraint, we have a limited number of micro-batches, which introduce a significant
    bubble rate. As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3\. Motivation & Observation
    ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")(b),
    the bubble rate reaches 2 even under zero-bubble mechanisms, such as the ZB-V
    and ZB-1P schedulers ([zerobubble,](#bib.bib37) ). This level of inefficiency
    is unacceptable for effective LLM training.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然我们可以通过增加 PP 的度数来将长序列训练扩展到更多的 GPU，但这可能导致高泡沫率。由于全球批量大小的限制，我们有有限数量的微批次，这会引入显著的泡沫率。如图
    [4](#S3.F4 "Figure 4 ‣ 3\. Motivation & Observation ‣ LoongTrain: Efficient Training
    of Long-Sequence LLMs with Head-Context Parallelism") (b) 所示，即使在零泡沫机制下（如 ZB-V
    和 ZB-1P 调度器（[zerobubble,](#bib.bib37)）），泡沫率也会达到 2。这种效率低下的水平对于有效的 LLM 训练是不可接受的。'
- en: 3.2\. Inefficient Performance of Ring-Attention
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. Ring-Attention 的低效性能
- en: While Ring-Attention demonstrates the potential to scale SP to large degrees,
    its performance is hindered by significant communication overheads. We evaluated
    the performance of Ring-Attention and Ulysses-Attention with a sequence length
    of 128K on a testbed comprising 64 GPUs¹¹1To scale training with 1M sequence length
    to 2048 GPUs, constrained by the global batch size of 4M tokens, $d_{sp}$ more
    time on communication than on computation when using 64 GPUs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Ring-Attention 展示了将 SP 扩展到大规模的潜力，但其性能受到显著的通信开销的限制。我们在一个由 64 个 GPU 组成的测试平台上评估了
    Ring-Attention 和 Ulysses-Attention 在序列长度为 128K 时的性能¹¹1为了将训练从 1M 序列长度扩展到 2048 个
    GPU，由于全球批量大小限制为 4M 个 token，当使用 64 个 GPU 时，$d_{sp}$ 在通信上花费的时间超过了计算时间。
- en: '![Refer to caption](img/67ad4c7f12477ae5273e0e74d08f90dd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/67ad4c7f12477ae5273e0e74d08f90dd.png)'
- en: (a) Ulyssess-Attention (MHA)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Ulyssess-Attention (MHA)
- en: '![Refer to caption](img/ac8929c1fefaa780af5040a56ce48ec7.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/ac8929c1fefaa780af5040a56ce48ec7.png)'
- en: (b) Ring-Attention (MHA)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Ring-Attention (MHA)
- en: '![Refer to caption](img/2ef6754fa0365a186df2c679c554e426.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/2ef6754fa0365a186df2c679c554e426.png)'
- en: (c) Ulyssess-Attention (GQA)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Ulyssess-Attention (GQA)
- en: '![Refer to caption](img/8f10c44f9891f15df63ef303c4932c3f.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/8f10c44f9891f15df63ef303c4932c3f.png)'
- en: (d) Ring-Attention (GQA)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Ring-Attention (GQA)
- en: Figure 5. Forward time evaluation of Ulysses-Attention and Ring-Attention on
    8 physical nodes, each equipped with 8 NVIDIA Ampere GPUs connected by NVLINK.
    Each node has four 200 HDR NICs. In the test, we set $H=32$ for GQA.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. 在8个物理节点上对Ulysses-Attention和Ring-Attention的前向时间进行评估，每个节点配备了8个通过NVLINK连接的NVIDIA
    Ampere GPU。每个节点有四个200 HDR NIC。在测试中，我们为GQA设置了$H=32$。
- en: '![Refer to caption](img/59219959c439acfd599b8ec5655d9612.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/59219959c439acfd599b8ec5655d9612.png)'
- en: Figure 6. Ring-Attention uses one NIC for sending key/value chunks and another
    NIC for receiving key/value chunks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. Ring-Attention使用一个NIC发送键/值块，另一个NIC接收键/值块。
- en: 'The performance inefficiency of Ring-Attention primarily stems from three factors.
    First, due to the small communication size, the intra-node communication via NVLINK
    is more sensitive to the communication latency rather than the bandwidth. When
    running GQA with a sequence length of 128K on 8 GPUs, the communication volume
    is 64MB per step. This size does not fully utilize the high bandwidth of NVLINK,
    resulting in high communication latency that cannot be overlapped with computation.
    Second, when scaling Ring-Attention, the computation time per step decreases quadratically,
    whereas the communication volume per step only decreases linearly. This scaling
    exacerbates the imbalance between computation and communication, making communication
    the performance bottleneck. Third, Ring-Attention does not fully utilize network
    resources due to its ring-based communication design. Despite the widespread use
    of multi-rail networks in GPU clusters ([railonly,](#bib.bib48) ; [railarch,](#bib.bib35)
    ), Ring-Attention utilizes one NIC for sending KV chunks and another NIC for receiving
    KV chunks, as shown in Figure [6](#S3.F6 "Figure 6 ‣ 3.2\. Inefficient Performance
    of Ring-Attention ‣ 3\. Motivation & Observation ‣ LoongTrain: Efficient Training
    of Long-Sequence LLMs with Head-Context Parallelism"). So in a single step, all
    other ranks must wait for the slowest rank using inter-node P2P communication.
    Thus, it is difficult to overlap communication with computation when scaling Ring-Attention
    to a large scale.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ring-Attention的性能低效主要源于三个因素。首先，由于通信规模较小，通过NVLINK的节点间通信更敏感于通信延迟而非带宽。在8个GPU上以128K的序列长度运行GQA时，每步的通信量为64MB。这一规模未能充分利用NVLINK的高带宽，导致通信延迟过高，无法与计算重叠。其次，当扩展Ring-Attention时，每步的计算时间以平方速率减少，而每步的通信量仅以线性速率减少。这种扩展加剧了计算与通信之间的不平衡，使得通信成为性能瓶颈。第三，Ring-Attention由于其基于环的通信设计没有充分利用网络资源。尽管GPU集群中广泛使用多轨网络（[railonly,](#bib.bib48)
    ; [railarch,](#bib.bib35)），Ring-Attention使用一个NIC发送KV块，另一个NIC接收KV块，如图[6](#S3.F6
    "Figure 6 ‣ 3.2\. Inefficient Performance of Ring-Attention ‣ 3\. Motivation &
    Observation ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context
    Parallelism")所示。因此，在单个步骤中，所有其他计算节点必须等待最慢的计算节点使用节点间P2P通信。因此，当将Ring-Attention扩展到大规模时，难以将通信与计算重叠。'
- en: 4\. Distributed 2D-Attention
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 分布式2D-Attention
- en: We introduce LoongTrain to address the scalability and efficiency challenges
    in training long-sequence LLMs. In particular, we propose 2D-Attention, which
    integrates head-parallel and context-parallel attention through a hybrid strategy,
    leveraging the benefits of both methods. This approach naturally overcomes the
    scalability limitations of head-parallel attention by incorporating context-parallel
    attention. To further reduce the communication overhead in Attention blocks, we
    design a Double-Ring-Attention mechanism and disclose the influence of device
    placement. Additionally, we briefly analyze the performance of 2D-Attention.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入LoongTrain来应对训练长序列LLMs中的可扩展性和效率挑战。特别是，我们提出了2D-Attention，通过混合策略集成了头部并行和上下文并行注意力，利用了两种方法的优点。这种方法自然克服了头部并行注意力的可扩展性限制，通过引入上下文并行注意力。为了进一步减少Attention块中的通信开销，我们设计了Double-Ring-Attention机制并揭示了设备放置的影响。此外，我们还简要分析了2D-Attention的性能。
- en: 4.1\. 2D-Attention Overview
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 2D-Attention概述
- en: 'In LoongTrain, attention is parallelized across two dimensions: head parallelism
    (HP) and context parallelism (CP), which is referred to as 2D-Attention. It organizes
    $d_{sp}$ and $d_{cp}$. Thus, we have'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LoongTrain 中，注意力在两个维度上进行并行化：头并行性（HP）和上下文并行性（CP），这被称为 2D-Attention。它组织了 $d_{sp}$
    和 $d_{cp}$。因此，我们有
- en: '|  | $d_{sp}=d_{hp}\times d_{cp}.$ |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{sp}=d_{hp}\times d_{cp}.$ |  |'
- en: 'Algorithm [1](#alg1 "Algorithm 1 ‣ 4.1\. 2D-Attention Overview ‣ 4\. Distributed
    2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context
    Parallelism") and Figure [7](#S4.F7 "Figure 7 ‣ 4.1\. 2D-Attention Overview ‣
    4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism") illustrate the forward pass of 2D-Attention.
    In Figure [7](#S4.F7 "Figure 7 ‣ 4.1\. 2D-Attention Overview ‣ 4\. Distributed
    2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context
    Parallelism")’s configuration, each CP process group contains four GPUs. The input
    tensors, Q (queries), K (keys), and V (values), are divided along the sequence
    dimension, with each segment shaped as $(H,S/d_{sp},D/H)$. 2D-Attention handles
    head parallelism across CP groups, while context parallelism is executed within
    each CP group.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [1](#alg1 "算法 1 ‣ 4.1\. 2D-Attention 概述 ‣ 4\. 分布式 2D-Attention ‣ LoongTrain：高效训练长序列
    LLM 的头-上下文并行性") 和 图 [7](#S4.F7 "图 7 ‣ 4.1\. 2D-Attention 概述 ‣ 4\. 分布式 2D-Attention
    ‣ LoongTrain：高效训练长序列 LLM 的头-上下文并行性") 说明了 2D-Attention 的前向传递。在图 [7](#S4.F7 "图 7
    ‣ 4.1\. 2D-Attention 概述 ‣ 4\. 分布式 2D-Attention ‣ LoongTrain：高效训练长序列 LLM 的头-上下文并行性")
    的配置中，每个 CP 进程组包含四个 GPU。输入张量 Q（查询）、K（键）和 V（值）在序列维度上进行划分，每个片段的形状为 $(H,S/d_{sp},D/H)$。2D-Attention
    在 CP 组之间处理头并行性，而上下文并行性则在每个 CP 组内执行。
- en: '![Refer to caption](img/10105dcf3fed0ef311981f4728d76062.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/10105dcf3fed0ef311981f4728d76062.png)'
- en: Figure 7. 2D-Attention design. Different colors represent different attention
    heads. In this example, $d_{cp}=4$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. 2D-Attention 设计。不同颜色代表不同的注意力头。在此示例中， $d_{cp}=4$。
- en: Algorithm 1 2D-Attention Mechanism (Forward Phase)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 2D-Attention 机制（前向阶段）
- en: '1:Input:  $Q$, $d_{cp}$6:Gather output: $O\leftarrow\textbf{SeqAlltoAll}(O^{\prime})$'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入： $Q$， $d_{cp}$ 6: 收集输出： $O\leftarrow\textbf{SeqAlltoAll}(O^{\prime})$'
- en: 'The computation of MHA in 2D-Attention involves three steps. 1
    The SeqAlltoAll communication operation distributes the QKV tensors based on the
    head dimension across $d_{hp}$ attention heads, as illustrated in Figure [7](#S4.F7
    "Figure 7 ‣ 4.1\. 2D-Attention Overview ‣ 4\. Distributed 2D-Attention ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism"). 2
    Each CP group independently performs Double-Ring-Attention, as detailed in Section
    [4.3](#S4.SS3 "4.3\. Double-Ring-Attention ‣ 4\. Distributed 2D-Attention ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism"), resulting
    in an output tensor of shape $$(H/d_{hp},S/d_{cp},\\'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'MHA在2D-Attention中的计算涉及三个步骤。1
    SeqAlltoAll通信操作根据头维度在$d_{hp}$注意力头之间分配QKV张量，如图[7](#S4.F7 "Figure 7 ‣ 4.1\. 2D-Attention
    Overview ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism")所示。2
    每个CP组独立执行Double-Ring-Attention，如[4.3](#S4.SS3 "4.3\. Double-Ring-Attention ‣ 4\.
    Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence LLMs
    with Head-Context Parallelism")节中详细描述，生成形状为$$(H/d_{hp},S/d_{cp},\\'
- en: D/H)$$. During this stage, each GPU computes attention using the local QKV and
    exchanges partitioned KV chunks via P2P communication, transferring $2\times(H/d_{hp})\times(S/d_{cp})\times(D/H)=2SD/d_{sp}$.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: D/H)$$。在这个阶段，每个GPU使用本地QKV计算注意力，并通过P2P通信交换分区的KV块，传输$2\times(H/d_{hp})\times(S/d_{cp})\times(D/H)=2SD/d_{sp}$。
- en: In the backward pass, a SeqAlltoAll transforms the gradients of the attention
    output from shape $(H,S/d_{sp},D/H)$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，SeqAlltoAll将注意力输出的梯度从形状$(H,S/d_{sp},D/H)$转换。
- en: 4.2\. KV Replication for GQA
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. GQA的KV复制
- en: In MHA computation, $d_{hp}$. Since $H_{kv}, P2P necessitates inter-node interconnections. Fortunately,
    the double-ring approach proposed in Section [4.3](#S4.SS3 "4.3\. Double-Ring-Attention
    ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism") leverages multiple NICs to maintain high
    efficiency. Maintaining the use of a standard NCCL AlltoAll within an HP group
    necessitates reordering the input QKV tensors across nodes, which increases network
    traffic for each Transformer layer. To mitigate this issue, we adopt the approach
    used in Megatron-LM, implementing a post-processing function within the data loader
    to adjust input tensor placement at the start of each batch. This obviates the
    need for on-the-fly data movement for QKV tensors. Even with this optimization,
    SeqAlltoAll still demands significant inter-node communication traffic.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '在上下文优先的放置策略中，属于同一CP组的GPU优先被放置在同一节点上。如图[11](#S4.F11 "Figure 11 ‣ 4.4\. Head-First
    & Context-First Device Placement ‣ 4\. Distributed 2D-Attention ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")(b)所示，GPU
    0-3 被分配到同一CP组。因此，在这个例子中，Double-Ring-Attention 仅生成节点内流量，大幅减少每个P2P操作的通信延迟。然而，当  时，P2P需要节点间连接。幸运的是，第[4.3](#S4.SS3
    "4.3\. Double-Ring-Attention ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient
    Training of Long-Sequence LLMs with Head-Context Parallelism")节中提出的双环方法利用多个NIC来保持高效。保持在HP组内使用标准的NCCL
    AlltoAll需要在节点间重新排序输入QKV张量，这增加了每个Transformer层的网络流量。为解决此问题，我们采用了Megatron-LM中的方法，在数据加载器中实现后处理函数，以在每个批次开始时调整输入张量的放置。这消除了QKV张量的即时数据移动的需要。即使有了这个优化，SeqAlltoAll仍然需要显著的节点间通信流量。'
- en: '![Refer to caption](img/844f79f792f6a768e2af1a2687677ea4.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/844f79f792f6a768e2af1a2687677ea4.png)'
- en: Figure 11. Context-first placement vs. head-first placement. Different colors
    represent different attention heads. In context-first placement, a post-processing
    function within the data loader is required to adjust input sequence placement
    at the start of each batch.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图11. 上下文优先放置与头部优先放置。不同的颜色代表不同的注意力头。在上下文优先放置中，需要数据加载器中的后处理函数在每个批次开始时调整输入序列的位置。
- en: 4.5\. Performance Analysis
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 性能分析
- en: 4.5.1\. Scalability Analysis
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1\. 可扩展性分析
- en: 2D-Attention enhances the scalability of long-sequence training by integrating
    head parallelism and context parallelism through a hybrid strategy. It overcomes
    the limitations of head parallelism by incorporating context-parallel attention,
    distributing computation across a grid of GPUs organized as $d_{hp}\times d_{cp}$
    using KV replication, ensuring flexible processing and a large search space for
    optimal performance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 2D-Attention通过通过混合策略集成头部并行和上下文并行，增强了长序列训练的可扩展性。它通过结合上下文并行注意力克服了头部并行的局限性，通过KV复制将计算分布在$
    d_{hp} \times d_{cp} $的GPU网格上，确保灵活的处理和优化性能的大范围搜索。
- en: 4.5.2\. Computation Analysis.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2\. 计算分析。
- en: 'Given a sequence $(S,D)$ represents the proportionality constant for the forward
    computation time. In 2D-Attention, the forward computation time for each micro-step
    within the inner ring is described as $\alpha\left({S}/{d_{cp}}\right)^{2}{D}/{d_{hp}}.$,
    we have:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 给定序列$(S,D)$表示前向计算时间的比例常数。在2D-Attention中，每个微步的前向计算时间描述为$\alpha\left({S}/{d_{cp}}\right)^{2}{D}/{d_{hp}}.$，我们得到：
- en: '|  | $T_{comp}^{fwd}=\alpha{S^{2}D}/({d_{cp}d_{sp}}).$ |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{comp}^{fwd}=\alpha{S^{2}D}/({d_{cp}d_{sp}}).$ |  |'
- en: 'There are $w$ For the backward pass, the computation time for each micro-step
    is described as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传递，每个微步的计算时间描述为：
- en: '|  | $T_{comp}^{bwd}=3\alpha{S^{2}D}/({d_{cp}d_{sp}}).$ |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{comp}^{bwd}=3\alpha{S^{2}D}/({d_{cp}d_{sp}}).$ |  |'
- en: This is because the backward computation kernel naturally requires additional
    computations, such as activation recomputing and gradient calculations as in FlashAttention
    ([flashattn1,](#bib.bib14) ).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为反向计算内核自然需要额外的计算，例如激活重新计算和梯度计算，如FlashAttention ([flashattn1,](#bib.bib14)
    )所示。
- en: 4.5.3\. P2P Communication Analysis.
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.3\. P2P 通信分析。
- en: 'The shape of a KV chunk is defined by: $(\max({H}_{kv},d_{hp})/d_{hp},S/d_{cp},D/H)$.
    The size of a KV chunk can be calculated as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: KV 块的形状由以下定义：$(\max({H}_{kv},d_{hp})/d_{hp},S/d_{cp},D/H)$。KV 块的大小可以计算如下：
- en: '|  | $Size(kv)=\max({H}_{kv},d_{hp})/H\times 4SD/d_{sp},$ |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $Size(kv)=\max({H}_{kv},d_{hp})/H\times 4SD/d_{sp},$ |  |'
- en: 'where the factor of 4 accounts for two tensors with data type FP16. When using
    Double-Ring-Attention, given the inner ring size $w$. GPUs concurrently launch
    P2P communications for inner rings and outer rings. Each P2P communication time
    depends on the slowest rank, due to the ring communication fashion. The forward
    execution time per inner ring, considering the overlap between communication and
    computation can be formulated as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中的 4 的因子考虑了两个 FP16 数据类型的张量。当使用 Double-Ring-Attention 时，给定内环大小 $w$。GPU 同时启动内环和外环的
    P2P 通信。每个 P2P 通信时间取决于最慢的排名，这是由于环通信方式。每个内环的前向执行时间，考虑到通信和计算之间的重叠可以表述为：
- en: '|  | $T_{inner\_ring}^{fwd}=A\times(w-1)+B,$ |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{inner\_ring}^{fwd}=A\times(w-1)+B,$ |  |'
- en: 'where $A$ are defined as:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 定义为：
- en: '|  | $1$2 |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The backward execution time per inner ring can be expressed with similar expressions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个内环的反向执行时间可以用类似的表达式表示。
- en: The per P2P communication time remains unaffected by $d_{cp}$ is increased.
    Thus, it becomes more challenging to effectively overlap computation and communication,
    and Ring-Attention exhibits poor performance in large clusters due to high communication
    overhead. 2D-Attention outperforms Ring-Attention since it provides more opportunities
    for computation-communication overlap by limiting $d_{cp}$.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 P2P 通信时间不会受到 $d_{cp}$ 增加的影响。因此，有效地重叠计算和通信变得更加困难，并且由于高通信开销，Ring-Attention
    在大型集群中表现不佳。2D-Attention 超越 Ring-Attention，因为它通过限制 $d_{cp}$ 提供了更多的计算-通信重叠机会。
- en: Selection of Inner Ring Size. When selecting context-first placement, ranks
    of the same CP group are consolidated to as few nodes as possible. In this case,
    there are $w$ is larger than that of NICs, GPUs may share the same NIC for P2P,
    leading to worse performance due to congestion.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 内环大小的选择。当选择上下文优先布局时，相同 CP 组的排名被合并到尽可能少的节点上。在这种情况下，如果 $w$ 大于 NICs 的数量，GPU 可能共享相同的
    NIC 进行 P2P，导致由于拥塞性能更差。
- en: GQA vs. MHA. During 2D-Attention of GQA, each P2P transfer involves $\hat{H}_{kv}/H\times
    2SD/d_{sp}$, because KV replication is not applied in this case. However, if $d_{hp}=H$,
    GQA and MHA will have the same communication volume due to KV replication.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GQA 与 MHA。在 GQA 的 2D-Attention 中，每个 P2P 传输涉及 $\hat{H}_{kv}/H\times 2SD/d_{sp}$，因为在这种情况下没有应用
    KV 复制。然而，如果 $d_{hp}=H$，GQA 和 MHA 将由于 KV 复制而具有相同的通信量。
- en: 4.5.4\. SeqAlltoAll Communication Analysis.
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.4\. SeqAlltoAll 通信分析。
- en: 'The size of a Q chunk and output chunk can be calculated as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Q 块和输出块的大小可以计算如下：
- en: '|  | $Size(q)=Size(out)={2SD}/{d_{sp}}.$ |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $Size(q)=Size(out)={2SD}/{d_{sp}}.$ |  |'
- en: 'SeqAlltoAll performs NCCL AlltoAll on $d_{hp}$ GPUs. The size of the data that
    each GPU sends out in both the forward and backward phases can be expressed as
    follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SeqAlltoAll 在 $d_{hp}$ GPU 上执行 NCCL AlltoAll。每个 GPU 在前向和反向阶段发送的数据大小可以表述为：
- en: '|  | $AlltoAll\_Volume=\textstyle\sum_{i\in\{(q,k,v,out\}}Size(i)\times(d_{hp}-1)/d_{hp}.$
    |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $AlltoAll\_Volume=\textstyle\sum_{i\in\{(q,k,v,out\}}Size(i)\times(d_{hp}-1)/d_{hp}.$
    |  |'
- en: With a larger $d_{hp}$ increases. With head-first placement, more AlltoAll-related
    traffic is carried by intra-node NVLINK, and vice versa for context-first placement.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 $d_{hp}$ 的增加。采用头部优先的布局时，更多与 AlltoAll 相关的流量由 intra-node NVLINK 承载，而上下文优先的布局则相反。
- en: 'Therefore, there is a trade-off between $d_{cp}$, as well as between the head-first
    and context-first placement. LoongTrain’s overall goal is to minimize the communication
    time that cannot be overlapped with computation. The problem can be formulated
    as:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$d_{cp}$ 之间以及头部优先和上下文优先布局之间存在权衡。LoongTrain 的总体目标是最小化无法与计算重叠的通信时间。该问题可以被表述为：
- en: '|  | $1$2 |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: In the formulation, $T_{SeqAlltoAll}$ inner rings to complete the execution
    of attention.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，$T_{SeqAlltoAll}$ 内环完成注意力执行。
- en: 4.5.5\. Memory Analysis.
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.5\. 内存分析。
- en: 'When using 2D-Attention, each GPU should save its input QKV chunks (after SeqAlltoAll)
    as the activation. Thus, given a fixed sequence length, 2D-Attention can also
    reduce the activation memory usage by increasing $d_{sp}$ for outer ring P2P communication.
    Experiment results in Section [6](#S6 "6\. Performance Evaluation ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism") show
    that this memory overhead is small and does not hinder scalability.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用2D-Attention时，每个GPU应将其输入QKV块（在SeqAlltoAll之后）保存为激活。因此，给定固定的序列长度，2D-Attention还可以通过增加$d_{sp}$来减少激活内存使用，从而进行外环P2P通信。第[6](#S6
    "6\. 性能评估 ‣ LoongTrain：高效训练长序列LLMs的头-上下文并行")节中的实验结果表明，这种内存开销较小，不会妨碍可扩展性。
- en: 5\. End-to-end System Implementation
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 端到端系统实现
- en: 'We describe the end-to-end system implementation of LoongTrain for training
    LLMs on our internal framework with two techniques: Hybrid ZeRO and selective
    checkpoint++.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了LoongTrain在我们内部框架上训练LLMs的端到端系统实现，采用了两种技术：混合ZeRO和选择性checkpoint++。
- en: 5.1\. Hybrid ZeRO for Norm and Linear Modules
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. Norm和Linear模块的混合ZeRO
- en: 'In LoongTrain, all modules except for attention (e.g., Linear, LayerNorm, etc.)
    utilize Zero ([ZeRO,](#bib.bib39) ). ZeRO is originally designed to reduce redundant
    memory usage across DP ranks. When directly using ZeRO, for instance in Figure [12](#S5.F12
    "Figure 12 ‣ 5.1\. Hybrid ZeRO for Norm and Linear Modules ‣ 5\. End-to-end System
    Implementation ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context
    Parallelism"), it works for GPU-0 and GPU-2, as well as GPU-1 and GPU-3, which
    belong to the same DP group. GPU-0 and GPU-1 would each hold half of the parameters
    or optimizer states, but these values remain identical, leading to redundant memory
    usage.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在LoongTrain中，除了attention模块（例如Linear、LayerNorm等）之外，所有模块都使用ZeRO ([ZeRO,](#bib.bib39)
    )。ZeRO最初设计用于减少DP排名之间的冗余内存使用。例如，在图[12](#S5.F12 "图 12 ‣ 5.1\. Norm和Linear模块的混合ZeRO
    ‣ 5\. 端到端系统实现 ‣ LoongTrain：高效训练长序列LLMs的头-上下文并行")中，它适用于GPU-0和GPU-2，以及GPU-1和GPU-3，这些都属于同一DP组。GPU-0和GPU-1各自持有一半的参数或优化器状态，但这些值保持一致，导致了冗余的内存使用。
- en: LoongTrain addresses these redundancies by applying ZeRO not only across the
    DP dimension but also along the SP dimension. This hybrid approach shards model
    states across both dimensions, distributing the model states across more GPUs.
    As a result, only ${1}/(d_{dp}\times{d_{sp}})$ GPUs, effectively balancing the
    GPU memory usage and communication overhead.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: LoongTrain通过在DP维度和SP维度上应用ZeRO来解决这些冗余问题。这种混合方法将模型状态在两个维度上进行分片，将模型状态分配到更多的GPU上。因此，只有${1}/(d_{dp}\times{d_{sp}})$的GPU被使用，从而有效地平衡了GPU内存使用和通信开销。
- en: '![Refer to caption](img/dd8a31c46cf972cd195dec65113d3aaa.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dd8a31c46cf972cd195dec65113d3aaa.png)'
- en: Figure 12. LoongTrain applies ZeRO to Norm and Linear modules across both DP
    and SP dimensions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. LoongTrain将ZeRO应用于Norm和Linear模块，涵盖DP和SP维度。
- en: '|  | MHA (TGS) | MHA (MFU) | GQA (TGS) | GQA (MFU) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | MHA (TGS) | MHA (MFU) | GQA (TGS) | GQA (MFU) |'
- en: '| System | 128K | 256K | 512K | 1M | 128K | 256K | 512K | 1M | 128K | 256K
    | 512K | 1M | 128K | 256K | 512K | 1M |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 系统 | 128K | 256K | 512K | 1M | 128K | 256K | 512K | 1M | 128K | 256K | 512K
    | 1M | 128K | 256K | 512K | 1M |'
- en: '| DS-Ulysses | 629.9 | 418.3 | 243.1 | 130.6 | 0.305 | 0.341 | 0.359 | 0.365
    | 629.9 | 418.3 | 243.1 | 130.6 | 0.305 | 0.341 | 0.359 | 0.365 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| DS-Ulysses | 629.9 | 418.3 | 243.1 | 130.6 | 0.305 | 0.341 | 0.359 | 0.365
    | 629.9 | 418.3 | 243.1 | 130.6 | 0.305 | 0.341 | 0.359 | 0.365 |'
- en: '| Megatron-CP | 296.8 | 300.0 | 260.1 | OOM | 0.143 | 0.244 | 0.385 | OOM |
    706.2 | 476.3 | 279.6 | OOM | 0.342 | 0.388 | 0.413 | OOM |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Megatron-CP | 296.8 | 300.0 | 260.1 | OOM | 0.143 | 0.244 | 0.385 | OOM |
    706.2 | 476.3 | 279.6 | OOM | 0.342 | 0.388 | 0.413 | OOM |'
- en: '| HP1/CP32 | 285.0 | 287.4 | 250.4 | 121.2 | 0.138 | 0.234 | 0.369 | 0.339
    | 668.5 | 480.0 | 282.5 | 153.0 | 0.323 | 0.391 | 0.417 | 0.428 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| HP1/CP32 | 285.0 | 287.4 | 250.4 | 121.2 | 0.138 | 0.234 | 0.369 | 0.339
    | 668.5 | 480.0 | 282.5 | 153.0 | 0.323 | 0.391 | 0.417 | 0.428 |'
- en: '| HP2/CP16 | 311.1 | 314.9 | 267.3 | 151.6 | 0.151 | 0.256 | 0.394 | 0.423
    | 740.8 | 501.3 | 290.1 | 155.9 | 0.359 | 0.408 | 0.428 | 0.436 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| HP2/CP16 | 311.1 | 314.9 | 267.3 | 151.6 | 0.151 | 0.256 | 0.394 | 0.423
    | 740.8 | 501.3 | 290.1 | 155.9 | 0.359 | 0.408 | 0.428 | 0.436 |'
- en: '| HP4/CP8 | 548.9 | 469.2 | 283.6 | 154.1 | 0.266 | 0.382 | 0.408 | 0.431 |
    814.4 | 517.4 | 295.1 | 159.5 | 0.394 | 0.421 | 0.435 | 0.446 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| HP4/CP8 | 548.9 | 469.2 | 283.6 | 154.1 | 0.266 | 0.382 | 0.408 | 0.431 |
    814.4 | 517.4 | 295.1 | 159.5 | 0.394 | 0.421 | 0.435 | 0.446 |'
- en: '| HP8/CP4 | 752.4 | 498.1 | 286.1 | 154.1 | 0.364 | 0.406 | 0.418 | 0.431 |
    838.1 | 528.1 | 299.5 | 160.1 | 0.406 | 0.430 | 0.442 | 0.448 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| HP8/CP4 | 752.4 | 498.1 | 286.1 | 154.1 | 0.364 | 0.406 | 0.418 | 0.431 |
    838.1 | 528.1 | 299.5 | 160.1 | 0.406 | 0.430 | 0.442 | 0.448 |'
- en: '| HP16/CP2 | 714.3 | 472.4 | 278.9 | 150.9 | 0.346 | 0.385 | 0.412 | 0.422
    | 771.4 | 498.6 | 288.0 | 155.1 | 0.373 | 0.406 | 0.425 | 0.433 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| HP16/CP2 | 714.3 | 472.4 | 278.9 | 150.9 | 0.346 | 0.385 | 0.412 | 0.422
    | 771.4 | 498.6 | 288.0 | 155.1 | 0.373 | 0.406 | 0.425 | 0.433 |'
- en: '| HP32/CP1 | 700.1 | 459.3 | 268.8 | 146.0 | 0.339 | 0.374 | 0.397 | 0.408
    | 717.1 | 468.4 | 262.4 | 147.5 | 0.347 | 0.381 | 0.387 | 0.412 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| HP32/CP1 | 700.1 | 459.3 | 268.8 | 146.0 | 0.339 | 0.374 | 0.397 | 0.408
    | 717.1 | 468.4 | 262.4 | 147.5 | 0.347 | 0.381 | 0.387 | 0.412 |'
- en: Table 2\. Performance comparison of end-to-end training between LoongTrain,
    DS-Ulysses, and Megatron-CP. HP$n$.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. LoongTrain、DS-Ulysses和Megatron-CP的端到端训练性能比较。HP$n$。
- en: 5.2\. Selective Checkpoint++
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 选择性检查点++
- en: Long sequence training leads to significant memory costs, making gradient checkpointing
    a common practice. During forward propagation, the gradient checkpointing mechanism
    stores only the input tensors of the wrapped function by the checkpoint function.
    If the dropped activation values are needed during backward propagation, they
    are recomputed. Typically, when we wrap the checkpoint function around an entire
    Transformer layer, the total memory required for activations of a Transformer
    layer is $2SD/d_{sp}$ in FP16.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 长序列训练会导致显著的内存消耗，使得梯度检查点成为一种常见做法。在前向传播过程中，梯度检查点机制仅存储被检查点函数包裹的函数的输入张量。如果在反向传播过程中需要丢弃的激活值，则会重新计算。通常，当我们将检查点函数包裹在整个Transformer层时，Transformer层激活所需的总内存为$2SD/d_{sp}$（FP16）。
- en: While saving the checkpoints of the entire model significantly reduces the memory
    footprint, it introduces additional computation overhead ([flashattn1,](#bib.bib14)
    ). Given that the recomputation time for attention blocks is particularly long,
    a straightforward approach is to keep the activations of attention blocks and
    use checkpointing for the other parts of the model selectively with the provided
    APIs ([Nvidia3,](#bib.bib22) ). However, this solution is not memory-efficient.
    During backward propagation, each attention block requires extra memory to save
    the QKV tensors (size $6SD/d_{sp}$ in FP32) ([chen2024internevo,](#bib.bib10)
    ). To reduce memory usage, DistFlashAttn ([lightseq,](#bib.bib24) ) places the
    attention module at the end of each Transformer layer. This strategy eliminates
    the need to recompute the attention module during the backward phase and only
    requires storing the output of the attention module.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管保存整个模型的检查点显著减少了内存占用，但它引入了额外的计算开销（[flashattn1](#bib.bib14)）。由于注意力块的重新计算时间特别长，一种简单的方法是保留注意力块的激活，并对模型的其他部分进行选择性检查点（[Nvidia3](#bib.bib22)）。然而，这个解决方案并不节省内存。在反向传播过程中，每个注意力块需要额外的内存来保存QKV张量（大小为$6SD/d_{sp}$，FP32）（[chen2024internevo](#bib.bib10)）。为了减少内存使用，DistFlashAttn（[lightseq](#bib.bib24)）将注意力模块放置在每个Transformer层的末尾。这一策略避免了在反向阶段重新计算注意力模块，仅需存储注意力模块的输出。
- en: LoongTrain implements the selective checkpoint++ mechanism without modifying
    the model structure. It adds attention modules to a whitelist. During the forward
    pass, when encountering a module in the whitelist, the modified checkpoint function
    saves its outputs. Specifically, for attention, it saves the attention output
    with the size of $2SD/d_{sp}$ memory size per Transformer layer. Additionally,
    selective checkpoint++ is compatible with other offload techniques ([ren2021zero,](#bib.bib40)
    ), which involve offloading attention outputs to memory or NVMe storage.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: LoongTrain实现了选择性检查点++机制，而无需修改模型结构。它将注意力模块添加到白名单中。在前向传播过程中，当遇到白名单中的模块时，修改后的检查点函数会保存其输出。具体而言，对于注意力，它保存每个Transformer层的注意力输出，内存大小为$2SD/d_{sp}$。此外，选择性检查点++与其他卸载技术（[ren2021zero](#bib.bib40)）兼容，这些技术涉及将注意力输出卸载到内存或NVMe存储。
- en: 6\. Performance Evaluation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 性能评估
- en: 6.1\. Experiment Setup
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实验设置
- en: Testbed. We conduct performance evaluation on a cluster with 8 GPU servers unless
    specified otherwise. Each server is equipped with 8 NVIDIA Ampere GPUs, 128 CPU
    cores, and 80GB memory per GPU. Within each node, GPUs are interconnected via
    NVLINK. Inter-node communication is facilitated by 4 NVIDIA Mellanox HDR (200Gb/s)
    InfiniBand NICs, without SHARP.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 测试平台。我们在一个包含8台GPU服务器的集群上进行性能评估，除非另有说明。每台服务器配备8个NVIDIA Ampere GPU、128个CPU核心和每个GPU
    80GB内存。在每个节点内部，GPU通过NVLINK互连。节点间通信由4个NVIDIA Mellanox HDR（200Gb/s）InfiniBand NICs实现，无SHARP。
- en: System Configurations. We evaluate the training performance of LoongTrain using
    the configuration of LLaMA2-7B ([LLaMA2,](#bib.bib44) ), where $D=4096$ for GQA.
    The input sequence length is scaled from 128K to 1M. In all experiments, activation
    checkpointing is enabled by default. We analyze the performance of LoongTrain
    with different parallelism settings and device placements.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 系统配置。我们使用LLaMA2-7B ([LLaMA2,](#bib.bib44)) 的配置来评估LoongTrain的训练性能，其中 $D=4096$
    用于GQA。输入序列长度从128K扩展到1M。在所有实验中，默认启用激活检查点。我们分析了LoongTrain在不同并行设置和设备放置下的性能。
- en: Evaluation Metrics. We focus on key performance metrics, including Model FLOPs
    Utilization (MFU) ([palm,](#bib.bib12) ) and Tokens per GPU per Second (TGS).
    We use the formula provided in Megatron-LM ([Megatron-LM,](#bib.bib31) ) for calculating
    FLOPs and MFU. Notably, the FLOPs for attention are halved in this work to account
    for the causal mask, which reduces the number of elements in attention that require
    computation by approximately half. This differs from the FLOPs and MFU calculations
    used in other works ([chen2024internevo,](#bib.bib10) ; [flashattn1,](#bib.bib14)
    ; [dao2023flashattention,](#bib.bib13) ), but is essential since attention accounts
    for the majority of the workload in long sequence training. Without this adjustment,
    the MFU can exceed 1, misrepresenting the actual system performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们关注关键性能指标，包括模型FLOPs利用率 (MFU) ([palm,](#bib.bib12)) 和每GPU每秒处理的tokens数量
    (TGS)。我们使用Megatron-LM ([Megatron-LM,](#bib.bib31)) 提供的公式来计算FLOPs和MFU。值得注意的是，为了考虑因果掩码，本文将注意力的FLOPs减半，这样可以将需要计算的注意力元素数量减少约一半。这与其他研究中使用的FLOPs和MFU计算方法有所不同
    ([chen2024internevo,](#bib.bib10)；[flashattn1,](#bib.bib14)；[dao2023flashattention,](#bib.bib13))，但这是必要的，因为注意力在长序列训练中占据了大部分工作负载。如果不做此调整，MFU可能会超过1，误导实际系统性能。
- en: 'Baselines. We compare the performance of LoongTrain against two long sequence
    training frameworks: DeepSpeed-Ulysses (DS-Ulysses) ([DeepspeedUlysses,](#bib.bib21)
    ) and Megatron Context Parallelism (Megatron-CP) ([megatroncp,](#bib.bib34) ).
    DS-Ulysses employs head-parallel attention, while Megatron-CP utilizes Ring-Attention
    with load balancing. All baseline systems are integrated with FlashAttention-V2
    ([dao2023flashattention,](#bib.bib13) ). The versions used are as follows: 1)
    DS-Ulysses: DeepSpeed V0.14.0; 2) Megatron-CP: Nemo v2.0.0rc0, NemoLauncher v24.05,
    Megatron-Core v0.7.0, TransformerEngine v1.6, Apex commit ID 810ffa.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试。我们将LoongTrain的性能与两个长序列训练框架进行比较：DeepSpeed-Ulysses (DS-Ulysses) ([DeepspeedUlysses,](#bib.bib21))
    和 Megatron Context Parallelism (Megatron-CP) ([megatroncp,](#bib.bib34))。DS-Ulysses使用头部并行注意力，而Megatron-CP则利用带负载均衡的环形注意力。所有基准系统均集成了FlashAttention-V2
    ([dao2023flashattention,](#bib.bib13))。所使用的版本如下：1) DS-Ulysses：DeepSpeed V0.14.0；2)
    Megatron-CP：Nemo v2.0.0rc0，NemoLauncher v24.05，Megatron-Core v0.7.0，TransformerEngine
    v1.6，Apex提交ID 810ffa。
- en: '|  |  |  | 128K | 256K | 512K | 1M |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 128K | 256K | 512K | 1M |'
- en: '|  |  |  | With SC++ | W/O SC++ | With SC++ | W/O SC++ | With SC++ | W/O SC++
    | With SC++ | W/O SC++ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 有SC++ | 无SC++ | 有SC++ | 无SC++ | 有SC++ | 无SC++ | 有SC++ | 无SC++ |'
- en: '|  | $d_{cp}$ | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF |
    HF | CF | HF | CF |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{cp}$ | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF |
    HF | CF | HF | CF |'
- en: '| MHA | 64 | 1 | 0.092 | 0.092 | 0.070 | 0.070 | 0.159 | 0.159 | 0.122 | 0.122
    | 0.290 | 0.290 | 0.221 | 0.221 | 0.452 | 0.452 | 0.357 | 0.357 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| MHA | 64 | 1 | 0.092 | 0.092 | 0.070 | 0.070 | 0.159 | 0.159 | 0.122 | 0.122
    | 0.290 | 0.290 | 0.221 | 0.221 | 0.452 | 0.452 | 0.357 | 0.357 |'
- en: '| 32 | 2 | 0.099 | 0.158 | 0.077 | 0.126 | 0.173 | 0.278 | 0.133 | 0.219 |
    0.316 | 0.434 | 0.243 | 0.353 | 0.475 | 0.486 | 0.394 | 0.406 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2 | 0.099 | 0.158 | 0.077 | 0.126 | 0.173 | 0.278 | 0.133 | 0.219 |
    0.316 | 0.434 | 0.243 | 0.353 | 0.475 | 0.486 | 0.394 | 0.406 |'
- en: '| 16 | 4 | 0.176 | 0.245 | 0.141 | 0.205 | 0.314 | 0.378 | 0.248 | 0.317 |
    0.470 | 0.472 | 0.384 | 0.388 | 0.520 | 0.509 | 0.418 | 0.413 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 0.176 | 0.245 | 0.141 | 0.205 | 0.314 | 0.378 | 0.248 | 0.317 |
    0.470 | 0.472 | 0.384 | 0.388 | 0.520 | 0.509 | 0.418 | 0.413 |'
- en: '| 8 | 8 | 0.283 | 0.321 | 0.236 | 0.282 | 0.434 | 0.420 | 0.361 | 0.357 | 0.502
    | 0.478 | 0.409 | 0.394 | 0.527 | 0.521 | 0.424 | 0.420 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 0.283 | 0.321 | 0.236 | 0.282 | 0.434 | 0.420 | 0.361 | 0.357 | 0.502
    | 0.478 | 0.409 | 0.394 | 0.527 | 0.521 | 0.424 | 0.420 |'
- en: '| 4 | 16 | 0.328 | 0.327 | 0.289 | 0.283 | 0.436 | 0.423 | 0.369 | 0.359 |
    0.487 | 0.476 | 0.399 | 0.394 | 0.519 | 0.520 | 0.418 | 0.412 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 0.328 | 0.327 | 0.289 | 0.283 | 0.436 | 0.423 | 0.369 | 0.359 |
    0.487 | 0.476 | 0.399 | 0.394 | 0.519 | 0.520 | 0.418 | 0.412 |'
- en: '| 2 | 32 | 0.320 | 0.329 | 0.284 | 0.293 | 0.421 | 0.421 | 0.353 | 0.357 |
    0.474 | 0.478 | 0.388 | 0.394 | 0.517 | 0.517 | 0.415 | 0.406 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32 | 0.320 | 0.329 | 0.284 | 0.293 | 0.421 | 0.421 | 0.353 | 0.357 |
    0.474 | 0.478 | 0.388 | 0.394 | 0.517 | 0.517 | 0.415 | 0.406 |'
- en: '| GQA | 64 | 1 | 0.255 | 0.255 | 0.196 | 0.196 | 0.379 | 0.379 | 0.308 | 0.308
    | 0.470 | 0.470 | 0.378 | 0.378 | 0.508 | 0.508 | 0.406 | 0.406 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 64 | 1 | 0.255 | 0.255 | 0.196 | 0.196 | 0.379 | 0.379 | 0.308 | 0.308
    | 0.470 | 0.470 | 0.378 | 0.378 | 0.508 | 0.508 | 0.406 | 0.406 |'
- en: '| 32 | 2 | 0.283 | 0.317 | 0.233 | 0.269 | 0.419 | 0.429 | 0.345 | 0.354 |
    0.492 | 0.485 | 0.398 | 0.392 | 0.521 | 0.516 | 0.418 | 0.416 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2 | 0.283 | 0.317 | 0.233 | 0.269 | 0.419 | 0.429 | 0.345 | 0.354 |
    0.492 | 0.485 | 0.398 | 0.392 | 0.521 | 0.516 | 0.418 | 0.416 |'
- en: '| 16 | 4 | 0.354 | 0.338 | 0.309 | 0.294 | 0.466 | 0.437 | 0.385 | 0.373 |
    0.505 | 0.494 | 0.410 | 0.404 | 0.531 | 0.526 | 0.425 | 0.426 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 0.354 | 0.338 | 0.309 | 0.294 | 0.466 | 0.437 | 0.385 | 0.373 |
    0.505 | 0.494 | 0.410 | 0.404 | 0.531 | 0.526 | 0.425 | 0.426 |'
- en: '| 8 | 8 | 0.377 | 0.354 | 0.327 | 0.310 | 0.480 | 0.452 | 0.392 | 0.380 | 0.516
    | 0.502 | 0.419 | 0.412 | 0.543 | 0.536 | 0.435 | 0.432 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 0.377 | 0.354 | 0.327 | 0.310 | 0.480 | 0.452 | 0.392 | 0.380 | 0.516
    | 0.502 | 0.419 | 0.412 | 0.543 | 0.536 | 0.435 | 0.432 |'
- en: '| 4 | 16 | 0.354 | 0.341 | 0.310 | 0.308 | 0.457 | 0.437 | 0.377 | 0.373 |
    0.500 | 0.493 | 0.409 | 0.405 | 0.532 | 0.529 | 0.428 | 0.419 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 0.354 | 0.341 | 0.310 | 0.308 | 0.457 | 0.437 | 0.377 | 0.373 |
    0.500 | 0.493 | 0.409 | 0.405 | 0.532 | 0.529 | 0.428 | 0.419 |'
- en: '| 2 | 32 | 0.323 | 0.333 | 0.285 | 0.295 | 0.424 | 0.422 | 0.349 | 0.360 |
    0.476 | 0.481 | 0.389 | 0.394 | 0.518 | 0.518 | 0.415 | 0.406 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32 | 0.323 | 0.333 | 0.285 | 0.295 | 0.424 | 0.422 | 0.349 | 0.360 |
    0.476 | 0.481 | 0.389 | 0.394 | 0.518 | 0.518 | 0.415 | 0.406 |'
- en: Table 3\. End-to-end training performance (MFU) of 7B-MHA and 7B-GQA on 64 GPUs
    with $d_{sp}=64$. SC++ stands for Selective Checkpoint++, HF for head-first, and
    CF for context-first. The highest MFU value in each column is highlighted.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 7B-MHA和7B-GQA在64 GPUs上以$d_{sp}=64$的端到端训练性能（MFU）。SC++代表选择性检查点++，HF代表先头部，CF代表先上下文。每列中最高的MFU值已突出显示。
- en: '![Refer to caption](img/3b7b5c0b9c91244e479f36646f1ff39c.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3b7b5c0b9c91244e479f36646f1ff39c.png)'
- en: (a) MHA
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MHA
- en: '![Refer to caption](img/d08f8ce3d5911d95585ec4aa023b935f.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d08f8ce3d5911d95585ec4aa023b935f.png)'
- en: (b) GQA
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GQA
- en: Figure 13. Performance comparison between Megatron-CP, DeepSpeed-Ulysses and
    our proposed LoongTrain on 32 GPUs with the sequence length from 128K to 1M.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图13. Megatron-CP、DeepSpeed-Ulysses与我们提出的LoongTrain在32 GPUs上的性能比较，序列长度从128K到1M。
- en: 6.2\. Comparison with DS-Ulysses & Megatron-CP
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 与DS-Ulysses & Megatron-CP的比较
- en: 'Theoretically, 2D-Attention when $d_{cp}=1$ is equivalent to Megatron-CP. To
    validate that our LoongTrain implementation is consistent with this theoretical
    analysis, we measured the TGS and MFU when training 7B-MHA and 7B-GQA on 32 GPUs
    using LoongTrain, DS-Ulysses, and Megatron-CP, with different sequence lengths.
    The comparison was limited to 32 GPUs because DS-Ulysses supports only head-parallelism,
    which is constrained by the number of attention heads. To ensure a fair comparison,
    all systems applied ZeRO-1 on Norm and Linear modules across the 32 GPUs, and
    did not use Selective Checkpoint++. The results are shown in Table [2](#S5.T2
    "Table 2 ‣ 5.1\. Hybrid ZeRO for Norm and Linear Modules ‣ 5\. End-to-end System
    Implementation ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context
    Parallelism").'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，当$d_{cp}=1$时，2D-Attention等同于Megatron-CP。为了验证我们的LoongTrain实现与这一理论分析的一致性，我们在32
    GPUs上使用LoongTrain、DS-Ulysses和Megatron-CP训练7B-MHA和7B-GQA，并测量了TGS和MFU。由于DS-Ulysses仅支持头部并行，这受限于注意力头的数量，因此比较仅限于32
    GPUs。为了确保公平比较，所有系统在32 GPUs上对Norm和Linear模块应用了ZeRO-1，并且没有使用选择性检查点++。结果如表[2](#S5.T2
    "表2 ‣ 5.1\. Norm和Linear模块的混合ZeRO ‣ 5\. 端到端系统实现 ‣ LoongTrain：高效训练长序列LLM的头部上下文并行")所示。
- en: When $d_{cp}=1$, LoongTrain demonstrates slightly lower performance than Megatron-CP
    in MHA, but exhibits higher performance in GQA. Our analysis indicates both systems
    perform similarly in attention computation. The main performance disparity arises
    from the divergent choices in computation and communication operators. Notably,
    when processing the sequence length of 1M, Megatron-CP encounters out-of-memory
    errors due to increased pre-allocated GPU memory requirements for parameters and
    gradients.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当$d_{cp}=1$时，LoongTrain在MHA上表现略低于Megatron-CP，但在GQA上表现更高。我们的分析表明，两个系统在注意力计算上的表现类似。主要的性能差异源于计算和通信操作符的选择不同。特别是，当处理1M的序列长度时，由于参数和梯度的预分配GPU内存需求增加，Megatron-CP会遇到内存不足的错误。
- en: For sequence lengths of 128K and 256K, Megatron-CP exhibits poor performance
    in MHA, as the P2P communication cannot be effectively overlapped with computation.
    However, with the sequence lengths of 512K and 1M, both Megatron-CP and LoongTrain-HP1/CP32
    show better performance than DS-Ulysses for MHA. Additionally, in GQA, the communication
    volume per micro-step is reduced by a factor of 4\. Consequently, Megatron-CP
    and LoongTrain-HP1/CP32 consistently outperform DS-Ulysses across all evaluated
    sequence lengths for GQA.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 128K 和 256K 的序列长度，Megatron-CP 在 MHA 中表现较差，因为 P2P 通信无法与计算有效重叠。然而，对于 512K 和
    1M 的序列长度，Megatron-CP 和 LoongTrain-HP1/CP32 在 MHA 中表现均优于 DS-Ulysses。此外，在 GQA 中，每个微步的通信量减少了
    4 倍。因此，Megatron-CP 和 LoongTrain-HP1/CP32 在所有评估的序列长度下，对 GQA 的表现始终优于 DS-Ulysses。
- en: 'Then, we compare the end-to-end performance of the complete LoongTrain and
    the baselines. All of the techniques such as hybrid ZeRO and Selective Checkpoint++
    are used. As shown in Figure [13](#S6.F13 "Figure 13 ‣ 6.1\. Experiment Setup
    ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism"), LoongTrain delivers larger MFU. The configuration
    of $d_{hp}=8$, respectively. Compared to Megatron-CP, LoongTrain enhances the
    performance of MHA and GQA by up to $2.88\times$, respectively.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，我们比较了完整的 LoongTrain 与基准模型的端到端性能。所有技术，如 hybrid ZeRO 和 Selective Checkpoint++，都已使用。如图
    [13](#S6.F13 "Figure 13 ‣ 6.1\. Experiment Setup ‣ 6\. Performance Evaluation
    ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")
    所示，LoongTrain 提供了更大的 MFU。$d_{hp}=8$ 的配置。与 Megatron-CP 相比，LoongTrain 将 MHA 和 GQA
    的性能分别提升了多达 $2.88\times$。'
- en: 6.3\. Analysis of LoongTrain Performance
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. LoongTrain 性能分析
- en: 'To analyze how much performance improvement can be brought by each design,
    we evaluated the performance of LoongTrain for training the 7B-MHA and 7B-GQA
    models on 64 GPUs with various sequence lengths and configurations. The evaluation
    results are presented in Table [3](#S6.T3 "Table 3 ‣ 6.1\. Experiment Setup ‣
    6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence LLMs
    with Head-Context Parallelism"). We do not show the results for $d_{cp}=1$ cannot
    exceed the number of attention heads, which is 32\. The end-to-end evaluation
    demonstrates that LoongTrain’s designs (e.g., 2D-Attention) and implementation
    techniques (e.g., Selective Checkpoint++), significantly enhance the training
    performance across all cases. Figure [14](#S6.F14 "Figure 14 ‣ 6.3\. Analysis
    of LoongTrain Performance ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient
    Training of Long-Sequence LLMs with Head-Context Parallelism") shows the end-to-end
    MFU results and the details are listed in Table [3](#S6.T3 "Table 3 ‣ 6.1\. Experiment
    Setup ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '为了分析每种设计带来的性能提升，我们评估了 LoongTrain 在 64 个 GPU 上训练 7B-MHA 和 7B-GQA 模型的性能，涵盖了各种序列长度和配置。评估结果见表
    [3](#S6.T3 "Table 3 ‣ 6.1\. Experiment Setup ‣ 6\. Performance Evaluation ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")。我们没有展示
    $d_{cp}=1$ 的结果，因为它不能超过注意力头的数量，即 32。端到端评估表明，LoongTrain 的设计（例如，2D-Attention）和实施技术（例如，Selective
    Checkpoint++）显著提升了所有情况下的训练性能。图 [14](#S6.F14 "Figure 14 ‣ 6.3\. Analysis of LoongTrain
    Performance ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism") 显示了端到端 MFU 结果，详细信息见表 [3](#S6.T3 "Table 3
    ‣ 6.1\. Experiment Setup ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient
    Training of Long-Sequence LLMs with Head-Context Parallelism")。'
- en: '![Refer to caption](img/51534da3cf789892643e334c8d1f9b70.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/51534da3cf789892643e334c8d1f9b70.png)'
- en: (a) MHA
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MHA
- en: '![Refer to caption](img/8573a4d6bdaaaee6df146373852d123c.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8573a4d6bdaaaee6df146373852d123c.png)'
- en: (b) GQA
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GQA
- en: Figure 14. MFU comparison on 64 GPUs with sequence lengths from 128K to 1M.
    Ring indicates $d_{hp}=1$ in LoongTrain. 2D-Attn indicates the best-performing
    configuration.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14. 在 64 个 GPU 上进行的 MFU 比较，序列长度从 128K 到 1M。Ring 表示 LoongTrain 中的 $d_{hp}=1$。2D-Attn
    表示最佳配置。
- en: '|  |  |  | MHA (Head-First) | MHA (Context-First) | GQA (Head-First) | GQA
    (Context-First) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | MHA (Head-First) | MHA (Context-First) | GQA (Head-First) | GQA
    (Context-First) |'
- en: '|  | $d_{cp}$ | 128K | 256K | 512K | 1M | 128K | 256K | 512K | 1M | 128K |
    256K | 512K | 1M | 128K | 256K | 512K | 1M |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{cp}$ | 128K | 256K | 512K | 1M | 128K | 256K | 512K | 1M | 128K |
    256K | 512K | 1M | 128K | 256K | 512K | 1M |'
- en: '| Overall | 64 | 1 | 296.4 | 597.8 | 1210 | 2897 | 296.4 | 597.8 | 1210 | 2897
    | 86.0 | 225.1 | 713.5 | 2681 | 86.0 | 225.1 | 713.5 | 2681 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 64 | 1 | 296.4 | 597.8 | 1210 | 2897 | 296.4 | 597.8 | 1210 | 2897 |
    86.0 | 225.1 | 713.5 | 2681 | 86.0 | 225.1 | 713.5 | 2681 |'
- en: '| 32 | 2 | 273.6 | 546.8 | 1106 | 2745 | 162.4 | 328.7 | 782.5 | 2663 | 75.4
    | 198.5 | 679.5 | 2607 | 64.9 | 187.1 | 683.5 | 2589 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2 | 273.6 | 546.8 | 1106 | 2745 | 162.4 | 328.7 | 782.5 | 2663 | 75.4
    | 198.5 | 679.5 | 2607 | 64.9 | 187.1 | 683.5 | 2589 |'
- en: '| 16 | 4 | 137.0 | 275.8 | 708.1 | 2595 | 87.4 | 213.8 | 691.5 | 2617 | 55.4
    | 172.1 | 659.4 | 2559 | 59.9 | 179.1 | 668.3 | 2543 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 137.0 | 275.8 | 708.1 | 2595 | 87.4 | 213.8 | 691.5 | 2617 | 55.4
    | 172.1 | 659.4 | 2559 | 59.9 | 179.1 | 668.3 | 2543 |'
- en: '| 8 | 8 | 72.2 | 187.9 | 658.3 | 2557 | 62.2 | 185.6 | 675.3 | 2539 | 52.1
    | 166.2 | 644.1 | 2494 | 56.8 | 175.2 | 656.1 | 2495 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 72.2 | 187.9 | 658.3 | 2557 | 62.2 | 185.6 | 675.3 | 2539 | 52.1
    | 166.2 | 644.1 | 2494 | 56.8 | 175.2 | 656.1 | 2495 |'
- en: '| 4 | 16 | 58.4 | 179.8 | 671.9 | 2575 | 60.1 | 182.6 | 680.6 | 2549 | 55.8
    | 173.6 | 659.6 | 2530 | 57.3 | 177.2 | 661.7 | 2510 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 58.4 | 179.8 | 671.9 | 2575 | 60.1 | 182.6 | 680.6 | 2549 | 55.8
    | 173.6 | 659.6 | 2530 | 57.3 | 177.2 | 661.7 | 2510 |'
- en: '| 2 | 32 | 60.8 | 186.0 | 684.9 | 2573 | 59.4 | 183.0 | 677.1 | 2553 | 60.8
    | 185.8 | 683.9 | 2579 | 59.3 | 183.1 | 677.5 | 2555 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32 | 60.8 | 186.0 | 684.9 | 2573 | 59.4 | 183.0 | 677.1 | 2553 | 60.8
    | 185.8 | 683.9 | 2579 | 59.3 | 183.1 | 677.5 | 2555 |'
- en: '| SeqAlltoAll | 64 | 1 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00
    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| SeqAlltoAll | 64 | 1 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00
    | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |'
- en: '| 32 | 2 | 2.23 | 3.20 | 5.49 | 10.00 | 7.19 | 13.27 | 25.10 | 49.26 | 1.89
    | 2.51 | 3.92 | 6.58 | 4.92 | 8.65 | 16.29 | 31.59 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2 | 2.23 | 3.20 | 5.49 | 10.00 | 7.19 | 13.27 | 25.10 | 49.26 | 1.89
    | 2.51 | 3.92 | 6.58 | 4.92 | 8.65 | 16.29 | 31.59 |'
- en: '| 16 | 4 | 2.45 | 3.52 | 5.80 | 10.53 | 10.31 | 19.25 | 37.37 | 73.74 | 2.15
    | 2.76 | 4.08 | 6.76 | 6.90 | 12.55 | 23.82 | 46.87 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 2.45 | 3.52 | 5.80 | 10.53 | 10.31 | 19.25 | 37.37 | 73.74 | 2.15
    | 2.76 | 4.08 | 6.76 | 6.90 | 12.55 | 23.82 | 46.87 |'
- en: '| 8 | 8 | 3.00 | 4.15 | 6.27 | 11.22 | 12.05 | 22.26 | 42.82 | 83.30 | 2.64
    | 3.24 | 4.43 | 7.31 | 8.13 | 14.60 | 27.51 | 53.35 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 3.00 | 4.15 | 6.27 | 11.22 | 12.05 | 22.26 | 42.82 | 83.30 | 2.64
    | 3.24 | 4.43 | 7.31 | 8.13 | 14.60 | 27.51 | 53.35 |'
- en: '| 4 | 16 | 9.11 | 15.99 | 29.02 | 55.38 | 12.95 | 23.97 | 45.52 | 90.28 | 7.23
    | 12.85 | 22.56 | 42.44 | 10.12 | 18.91 | 34.94 | 71.51 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 9.11 | 15.99 | 29.02 | 55.38 | 12.95 | 23.97 | 45.52 | 90.28 | 7.23
    | 12.85 | 22.56 | 42.44 | 10.12 | 18.91 | 34.94 | 71.51 |'
- en: '| 2 | 32 | 13.42 | 23.43 | 42.73 | 81.47 | 14.56 | 25.41 | 48.25 | 100.0 |
    13.40 | 23.35 | 42.85 | 81.76 | 14.31 | 25.75 | 48.43 | 106.8 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32 | 13.42 | 23.43 | 42.73 | 81.47 | 14.56 | 25.41 | 48.25 | 100.0 |
    13.40 | 23.35 | 42.85 | 81.76 | 14.31 | 25.75 | 48.43 | 106.8 |'
- en: Table 4\. Average overall execution time (ms) and SeqAlltoAll time (ms) of a
    single 2D-Attention forward and backward operation on 64 GPUs with $d_{sp}=64$.
    The lowest overall execution time in each column is highlighted.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 平均总体执行时间（毫秒）和 SeqAlltoAll 时间（毫秒），对 64 个 GPU 上的单次 2D-Attention 前向和反向操作进行测量，$d_{sp}=64$。每列中的最低总体执行时间已高亮显示。
- en: When $d_{hp}=1$ less communication volume compared to MHA, leading to a higher
    MFU than MHA. Specifically in Ring-Attention, the MFU reaches 19.6% with the sequence
    length of 128K, and increases to 40.6% when the sequence length is 1M.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $d_{hp}=1$ 时，与 MHA 相比，通信量减少，导致 MFU 高于 MHA。具体来说，在 Ring-Attention 中，序列长度为 128K
    时，MFU 达到 19.6%，而当序列长度为 1M 时，MFU 增加到 40.6%。
- en: 'With 2D-Attention, LoongTrain significantly improves the training performance
    for MHA. Compared to Ring-Attention, 2D-Attention enhances the MFU by 4.1$\times$
    for sequence lengths of 128K, 256K, 512K, and 1M, respectively. With Selective
    Checkpoint++, LoongTrain further boosts the training performance by 1.15$\times$
    for the same sequence lengths. Consequently, Figure [14](#S6.F14 "Figure 14 ‣
    6.3\. Analysis of LoongTrain Performance ‣ 6\. Performance Evaluation ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")(a) shows
    that LoongTrain’s overall training performance is improved by 5.2$\times$, respectively.
    Additionally, we observe that to achieve higher training performance for MHA,
    LoongTrain tends to use a higher head parallelism size for sequence lengths of
    128K and 256K. For sequence lengths of 512K and 1M, LoongTrain tends to use a
    balanced head and context parallelism size.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 2D-Attention，LoongTrain 显著提高了 MHA 的训练性能。与 Ring-Attention 相比，2D-Attention
    在 128K、256K、512K 和 1M 的序列长度下分别提升了 4.1$\times$ 的 MFU。借助 Selective Checkpoint++，LoongTrain
    在相同序列长度下进一步提高了 1.15$\times$ 的训练性能。因此，图 [14](#S6.F14 "图 14 ‣ 6.3\. LoongTrain 性能分析
    ‣ 6\. 性能评估 ‣ LoongTrain: 使用头-上下文并行性有效训练长序列 LLMs")(a) 显示 LoongTrain 的总体训练性能提高了
    5.2$\times$。此外，我们观察到，为了实现更高的 MHA 训练性能，LoongTrain 倾向于对 128K 和 256K 的序列长度使用更高的头并行大小。对于
    512K 和 1M 的序列长度，LoongTrain 倾向于使用平衡的头和上下文并行大小。'
- en: '2D-Attention also works effectively for GQA. Compared to the performance of
    Ring-Attention, LoongTrain enhances the MFU for sequences of 128K, 256K, 512K,
    and 1M by 1.58$\times$, respectively. Incorporating Selective Checkpoint++, LoongTrain
    further elevates the training performance by 1.21$\times$ for the same sequence
    lengths. Consequently, Figure [14](#S6.F14 "Figure 14 ‣ 6.3\. Analysis of LoongTrain
    Performance ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism")(b) shows that the overall training performance
    is improved by 1.9$\times$, respectively. For GQA, a balanced head and context
    parallelism size is a more efficient configuration.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '2D-Attention 对 GQA 也有效。与 Ring-Attention 的性能相比，LoongTrain 提高了 128K、256K、512K
    和 1M 序列的 MFU，分别提高了 1.58$\times$。结合 Selective Checkpoint++，LoongTrain 进一步将相同序列长度的训练性能提高了
    1.21$\times$。因此，图 [14](#S6.F14 "Figure 14 ‣ 6.3\. LoongTrain 性能分析 ‣ 6\. 性能评估 ‣
    LoongTrain: 高效训练长序列 LLMs 的 Head-Context 并行")(b) 显示，总体训练性能提高了 1.9$\times$。对于 GQA，平衡的头和上下文并行大小是一种更有效的配置。'
- en: 6.4\. Analysis of 2D-Attention
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 2D-Attention 分析
- en: 'We evaluated 2D-Attention by measuring the average overall execution time and
    SeqAlltoAll communication time for a single 2D-Attention forward operation under
    various configurations. The results are presented in Table [4](#S6.T4 "Table 4
    ‣ 6.3\. Analysis of LoongTrain Performance ‣ 6\. Performance Evaluation ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism").'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过测量在各种配置下单个 2D-Attention 前向操作的平均总体执行时间和 SeqAlltoAll 通信时间来评估 2D-Attention。结果见表
    [4](#S6.T4 "Table 4 ‣ 6.3\. LoongTrain 性能分析 ‣ 6\. 性能评估 ‣ LoongTrain: 高效训练长序列 LLMs
    的 Head-Context 并行")。'
- en: '|  | MHA (CP=64, HP=1) | MHA (CP=16, HP=4) | GQA (CP=64, HP=1) | GQA (CP=16,
    HP=4) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | MHA (CP=64, HP=1) | MHA (CP=16, HP=4) | GQA (CP=64, HP=1) | GQA (CP=16,
    HP=4) |'
- en: '| Inner Ring Size | 128K | 256K | 512K | 1M | 128K | 256K | 512K | 1M | 128K
    | 256K | 512K | 1M | 128K | 256K | 512K | 1M |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 内环大小 | 128K | 256K | 512K | 1M | 128K | 256K | 512K | 1M | 128K | 256K |
    512K | 1M | 128K | 256K | 512K | 1M |'
- en: '| 1 | 295.9 | 597.7 | 1214 | 2913 | 86.3 | 213.8 | 697.9 | 2621 | 94.2 | 226.7
    | 713.5 | 2668 | 60.7 | 180.6 | 673.3 | 2567 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 295.9 | 597.7 | 1214 | 2913 | 86.3 | 213.8 | 697.9 | 2621 | 94.2 | 226.7
    | 713.5 | 2668 | 60.7 | 180.6 | 673.3 | 2567 |'
- en: '| 2 | 184.5 | 401.3 | 917.1 | 2823 | 72.6 | 205.7 | 710.7 | 2611 | 83.2 | 218.9
    | 730.5 | 2650 | 60.8 | 182.6 | 671.2 | 2530 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 184.5 | 401.3 | 917.1 | 2823 | 72.6 | 205.7 | 710.7 | 2611 | 83.2 | 218.9
    | 730.5 | 2650 | 60.8 | 182.6 | 671.2 | 2530 |'
- en: '| 4 | 140.6 | 316.3 | 842.7 | 2754 | 69.1 | 199.4 | 704.4 | 2610 | 78.4 | 210.3
    | 719.7 | 2669 | 60.3 | 182.0 | 675.2 | 2535 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 140.6 | 316.3 | 842.7 | 2754 | 69.1 | 199.4 | 704.4 | 2610 | 78.4 | 210.3
    | 719.7 | 2669 | 60.3 | 182.0 | 675.2 | 2535 |'
- en: '| 8 | 214.9 | 415.1 | 869.9 | 2815 | 77.4 | 198.7 | 705.3 | 2621 | 83.4 | 211.6
    | 723.1 | 2674 | 61.0 | 183.1 | 677.4 | 2537 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 214.9 | 415.1 | 869.9 | 2815 | 77.4 | 198.7 | 705.3 | 2621 | 83.4 | 211.6
    | 723.1 | 2674 | 61.0 | 183.1 | 677.4 | 2537 |'
- en: Table 5\. Average execution time (ms) of a single 2D-Attention forward and backward
    operation (with Double-Ring-Attention and context-first device placement) on 64
    GPUs with $d_{sp}=64$. The lowest execution time in each column is highlighted.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 在 64 个 GPU 上，$d_{sp}=64$ 的单个 2D-Attention 前向和反向操作的平均执行时间（毫秒）（使用 Double-Ring-Attention
    和 context-first 设备放置）。每列中最低的执行时间已被突出显示。
- en: 'Sequence Length Study. As discussed in Section [4.5](#S4.SS5 "4.5\. Performance
    Analysis ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism"), with a fixed sequence parallelism degree,
    a longer sequence length provides more opportunities for computation-communication
    overlap. When $d_{hp}=1$. In this configuration, there are no SeqAlltoAll operations,
    indicating that the primary performance bottleneck lies in P2P operations. In
    the case of GQA, the overall attention time increases from 86.0ms to 2681ms. Across
    all sequence lengths, GQA demonstrates a shorter execution time compared to MHA
    due to the reduced communication volume.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '序列长度研究。如在第 [4.5](#S4.SS5 "4.5\. 性能分析 ‣ 4\. 分布式 2D-Attention ‣ LoongTrain: 高效训练长序列
    LLMs 的 Head-Context 并行") 节中讨论的，固定序列并行度下，较长的序列长度提供了更多计算-通信重叠的机会。当 $d_{hp}=1$ 时。在这种配置下，没有
    SeqAlltoAll 操作，表明主要性能瓶颈在于 P2P 操作。在 GQA 的情况下，总体注意力时间从 86.0 毫秒增加到 2681 毫秒。在所有序列长度中，由于通信量减少，GQA
    的执行时间相较于 MHA 更短。'
- en: 'MHA Study. The execution time of MHA can be reduced significantly under the
    most appropriate configuration from Table [4](#S6.T4 "Table 4 ‣ 6.3\. Analysis
    of LoongTrain Performance ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient
    Training of Long-Sequence LLMs with Head-Context Parallelism"). Specifically,
    the execution time decreases from 296.4ms to 58.4ms when LoongTrain increases
    the head parallelism degree to 16 for 128K sequence length. When processing a
    sequence length of 1M, the overall execution time decreases from 2681ms to 2555ms
    when LoongTrain increases the head parallelism degree to 8\. As discussed in Section
    [4.5](#S4.SS5 "4.5\. Performance Analysis ‣ 4\. Distributed 2D-Attention ‣ LoongTrain:
    Efficient Training of Long-Sequence LLMs with Head-Context Parallelism"), the
    communication volume per P2P operation remains unaffected by $d_{hp}$, even though
    such a configuration introduces more SeqAlltoAll communication time.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 'MHA 研究。在表格[4](#S6.T4 "Table 4 ‣ 6.3\. Analysis of LoongTrain Performance ‣
    6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence LLMs
    with Head-Context Parallelism")中，MHA的执行时间在最合适的配置下可以显著减少。具体而言，当LoongTrain将头部并行度增加到16以处理128K序列长度时，执行时间从296.4ms减少到58.4ms。当处理1M的序列长度时，当LoongTrain将头部并行度增加到8时，整体执行时间从2681ms减少到2555ms。如在第[4.5](#S4.SS5
    "4.5\. Performance Analysis ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient
    Training of Long-Sequence LLMs with Head-Context Parallelism")节中讨论的，尽管这种配置引入了更多的SeqAlltoAll通信时间，但每次P2P操作的通信量并未受到$d_{hp}$的影响。'
- en: GQA Study. GQA introduces less communication volume and is less sensitive to
    $d_{cp}$, thereby enhancing the ability to overlap P2P communication with computation.
    By increasing $d_{hp}$ and $d_{cp}=8$ avoids the large SeqAlltoAll overhead and
    effectively overlaps the computation with P2P communication.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: GQA 研究。GQA引入的通信量较少，对$d_{cp}$不太敏感，从而增强了P2P通信与计算的重叠能力。通过增加$d_{hp}$并使$d_{cp}=8$，避免了大的SeqAlltoAll开销，并有效地将计算与P2P通信重叠。
- en: 'Device Placement Study. As analyzed in Section [4.5](#S4.SS5 "4.5\. Performance
    Analysis ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism"), there is a trade-off between the SeqAlltoAll
    time and total execution time when choosing the placement strategy. Table [4](#S6.T4
    "Table 4 ‣ 6.3\. Analysis of LoongTrain Performance ‣ 6\. Performance Evaluation
    ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")
    shows that when $d_{cp}$ gets larger, head-first placement performs better. In
    these cases, the increased large SeqAlltoAll volumes become the bottleneck of
    the overall execution time. Therefore, only if SeqAlltoAll leverages the intra-node
    high-bandwidth NVLINK can LoongTrain achieve better overall performance.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '设备放置研究。如第[4.5](#S4.SS5 "4.5\. Performance Analysis ‣ 4\. Distributed 2D-Attention
    ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")节中分析的那样，选择放置策略时SeqAlltoAll时间与总执行时间之间存在权衡。表格[4](#S6.T4
    "Table 4 ‣ 6.3\. Analysis of LoongTrain Performance ‣ 6\. Performance Evaluation
    ‣ LoongTrain: Efficient Training of Long-Sequence LLMs with Head-Context Parallelism")显示，当$d_{cp}$增大时，头部优先放置表现更好。在这些情况下，增加的大量SeqAlltoAll通信量成为整体执行时间的瓶颈。因此，只有当SeqAlltoAll利用节点内高带宽NVLINK时，LoongTrain才能实现更好的整体性能。'
- en: 'Double-Ring-Attention Study. We compare the execution time of 2D-Attention
    with different inner ring sizes in Table [5](#S6.T5 "Table 5 ‣ 6.4\. Analysis
    of 2D-Attention ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training
    of Long-Sequence LLMs with Head-Context Parallelism"). As expected, with MHA and
    shorter sequence length, P2P communication cannot be effectively overlapped with
    the computation. In these cases, Double-Ring-Attention achieves more speedup.
    For instance, when the sequence length is 128K and $d_{cp}=16$, Double-Ring-Attention
    further reduces the attention operation time by a factor of 1.2, even if 2D-Attention
    is already applied. However, with longer sequence lengths, due to the increased
    computational workload, the P2P communication can be overlapped more, limiting
    the improvements from Double-Ring-Attention.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 'Double-Ring-Attention 研究。我们在表格[5](#S6.T5 "Table 5 ‣ 6.4\. Analysis of 2D-Attention
    ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism")中比较了不同内环大小的2D-Attention执行时间。如预期，使用MHA和较短的序列长度时，P2P通信无法有效地与计算重叠。在这些情况下，Double-Ring-Attention实现了更多的加速。例如，当序列长度为128K且$d_{cp}=16$时，即使已经应用了2D-Attention，Double-Ring-Attention仍能进一步将注意力操作时间减少1.2倍。然而，随着序列长度的增加，由于计算工作负载的增加，P2P通信可以更多地重叠，限制了Double-Ring-Attention的改进效果。'
- en: 'As we theoretically analyzed in Section [4.5](#S4.SS5 "4.5\. Performance Analysis
    ‣ 4\. Distributed 2D-Attention ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism"), when the inner ring size matches the number
    of NICs in one node (4 in our case), all NICs can be utilized for outer-ring communication,
    which is more effective. Table [5](#S6.T5 "Table 5 ‣ 6.4\. Analysis of 2D-Attention
    ‣ 6\. Performance Evaluation ‣ LoongTrain: Efficient Training of Long-Sequence
    LLMs with Head-Context Parallelism") also illustrates this trend. As discussed,
    the global batch size poses a challenge for the computation-communication ratio
    when scaling $d_{sp}$ to 512 GPUs for a 1M sequence length. In such cases, Double-Ring-Attention
    is expected to be more useful.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在第[4.5节](#S4.SS5 "4.5\. 性能分析 ‣ 4\. 分布式2D注意力 ‣ LoongTrain：高效训练长序列LLM的头部-上下文并行")中理论分析的，当内环大小与一个节点中的NIC数量匹配时（在我们的案例中为4），所有NIC可以用于外环通信，这会更有效。表[5](#S6.T5
    "表5 ‣ 6.4\. 2D注意力分析 ‣ 6\. 性能评估 ‣ LoongTrain：高效训练长序列LLM的头部-上下文并行")也说明了这一趋势。正如讨论的那样，当将$d_{sp}$扩展到512个GPU以处理1M序列长度时，全球批量大小对计算-通信比提出了挑战。在这种情况下，预计Double-Ring-Attention会更有用。
- en: 7\. Conclusion
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: We proposed LoongTrain, an efficient training framework for LLMs with long sequences.
    We designed the 2D-Attention, which combined both head-parallel and context-parallel
    approaches, to break the scalability constraints while maintaining high efficiency.
    We introduced the Double-Ring-Attention and device placement strategy to further
    improve the training efficiency. We implemented the LoongTrain system with hybrid
    parallelism and advanced gradient checkpoint techniques. Experiment results showed
    that LoongTrain provides a significant performance improvement over existing systems,
    such as DeepSpeed-Ulysses and Megatron CP.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了LoongTrain，一个高效的长序列LLM训练框架。我们设计了2D-Attention，将头部并行和上下文并行的方法结合起来，以打破可扩展性限制，同时保持高效率。我们引入了Double-Ring-Attention和设备放置策略，以进一步提高训练效率。我们实现了具有混合并行和先进梯度检查点技术的LoongTrain系统。实验结果表明，LoongTrain在性能上显著优于现有系统，如DeepSpeed-Ulysses和Megatron
    CP。
- en: 8\. Acknowledgements
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 致谢
- en: We express our gratitude to Zilin Zhu from Tencent. Our research benefited from
    his GitHub repository ”ring-flash-attention,” which implements Ring-Attention
    with FlashAttention. Additionally, we are thankful to Jiarui Fang and Shangchun
    Zhao from Tencent for their pioneering work in integrating Ulysses and Ring-Attention,
    as demonstrated in the open-source project Yunchang ([fang2024unified,](#bib.bib17)
    ). Their guidance was instrumental in shaping this work. We also extend our thanks
    to Haoyu Yang and Jidong Zhai from Tsinghua University for their assistance in
    enhancing the performance of our implementation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对腾讯的朱子麟表示感谢。我们的研究受益于他的GitHub库“ring-flash-attention”，它实现了使用FlashAttention的Ring-Attention。此外，我们还感谢腾讯的方佳瑞和赵尚春，他们在将Ulysses与Ring-Attention整合的开源项目Yunchang中所做的开创性工作（[fang2024unified,](#bib.bib17)）。他们的指导对塑造这项工作至关重要。我们还感谢清华大学的杨浩宇和翟基东，他们在提升我们实现的性能方面提供了帮助。
- en: References
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] AI@Meta. Llama 3 model card. 2024.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] AI@Meta. Llama 3模型卡. 2024年。'
- en: '[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models
    from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico
    Lebrón, and Sumit Sanghai. Gqa: 从多头检查点训练通用多查询变换器模型. arXiv预印本arXiv:2305.13245,
    2023年。'
- en: '[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and
    Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF
    international conference on computer vision, pages 6836–6846, 2021.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and
    Cordelia Schmid. Vivit: 一种视频视觉变换器. 发表在IEEE/CVF国际计算机视觉会议论文集中, 页码6836–6846, 2021年。'
- en: '[4] Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran Ramjee, and
    Nipun Kwatra. Varuna: scalable, low-cost training of massive deep learning models.
    In Proceedings of 17th European Conference on Computer Systems, EuroSys 2022,
    pages 472–487, 2022.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Sanjith Athlur, Nitika Saran, Muthian Sivathanu, Ramachandran Ramjee, and
    Nipun Kwatra. Varuna: 可扩展的低成本大规模深度学习模型训练. 发表在第17届欧洲计算机系统会议论文集中，EuroSys 2022, 页码472–487,
    2022年。'
- en: '[5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document
    transformer. arXiv preprint arXiv:2004.05150, 2020.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: 长文档变换器. arXiv预印本arXiv:2004.05150,
    2020年。'
- en: '[6] Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.
    Accurate medium-range global weather forecasting with 3d neural networks. Nature,
    619(7970):533–538, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Kaifeng Bi、Lingxi Xie、Hengheng Zhang、Xin Chen、Xiaotao Gu 和 Qi Tian. 使用
    3D 神经网络进行准确的中期全球天气预测。自然，619(7970):533–538, 2023.'
- en: '[7] William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin,
    Zhiye Song, and Jonathan Ragan-Kelley. Striped attention: Faster ring attention
    for causal transformers. arXiv preprint arXiv:2311.09431, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] William Brandon、Aniruddha Nrusimha、Kevin Qian、Zachary Ankner、Tian Jin、Zhiye
    Song 和 Jonathan Ragan-Kelley. 条纹注意力: 更快的环形注意力机制用于因果变换器。arXiv 预印本 arXiv:2311.09431,
    2023.'
- en: '[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等. 语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901,
    2020.'
- en: '[9] Abel Chandra, Laura Tünnermann, Tommy Löfstedt, and Regina Gratz. Transformer-based
    deep learning for predicting protein properties in the life sciences. Elife, 12:e82819,
    2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Abel Chandra、Laura Tünnermann、Tommy Löfstedt 和 Regina Gratz. 基于变换器的深度学习用于预测生命科学中的蛋白质属性。Elife,
    12:e82819, 2023.'
- en: '[10] Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting
    Huang, Qinghao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, et al. Internevo: Efficient
    long-sequence large language model training via hybrid parallelism and redundant
    sharding. arXiv preprint arXiv:2401.09149, 2024.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Qiaoling Chen、Diandian Gu、Guoteng Wang、Xun Chen、YingTong Xiong、Ting Huang、Qinghao
    Hu、Xin Jin、Yonggang Wen、Tianwei Zhang 等. Internevo: 通过混合并行和冗余分片高效训练长序列大型语言模型。arXiv
    预印本 arXiv:2401.09149, 2024.'
- en: '[11] Qiaoling Chen, Qinghao Hu, Guoteng Wang, Yingtong Xiong, Ting Huang, Xun
    Chen, Yang Gao, Hang Yan, Yonggang Wen, Tianwei Zhang, and Peng Sun. Amsp: Reducing
    communication overhead of zero for efficient llm training, 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Qiaoling Chen、Qinghao Hu、Guoteng Wang、Yingtong Xiong、Ting Huang、Xun Chen、Yang
    Gao、Hang Yan、Yonggang Wen、Tianwei Zhang 和 Peng Sun. Amsp: 减少零通信开销以提高 LLM 训练效率,
    2023.'
- en: '[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine
    Learning Research, 24(240):1–113, 2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav Mishra、Adam
    Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann 等. Palm:
    通过路径扩展语言建模。机器学习研究杂志，24(240):1–113, 2023.'
- en: '[13] Tri Dao. Flashattention-2: Faster attention with better parallelism and
    work partitioning. arXiv preprint arXiv:2307.08691, 2023.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tri Dao. Flashattention-2: 更快的注意力机制，具有更好的并行性和工作分配。arXiv 预印本 arXiv:2307.08691,
    2023.'
- en: '[14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention:
    Fast and memory-efficient exact attention with io-awareness. Advances in Neural
    Information Processing Systems, 35:16344–16359, 2022.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tri Dao、Dan Fu、Stefano Ermon、Atri Rudra 和 Christopher Ré. Flashattention:
    快速且内存高效的精确注意力机制，具有 IO 预感。神经信息处理系统进展，35:16344–16359, 2022.'
- en: '[15] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V.
    Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang,
    and Andrew Y. Ng. Large scale distributed deep networks. In Proceedings of 26th
    Annual Conference on Neural Information Processing Systems, NeurIPS 2012., pages
    1232–1240, 2012.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jeffrey Dean、Greg Corrado、Rajat Monga、Kai Chen、Matthieu Devin、Quoc V.
    Le、Mark Z. Mao、Marc’Aurelio Ranzato、Andrew W. Senior、Paul A. Tucker、Ke Yang 和
    Andrew Y. Ng. 大规模分布式深度网络。第26届神经信息处理系统年会论文集，NeurIPS 2012., 页码 1232–1240, 2012.'
- en: '[16] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui
    Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000
    tokens. arXiv preprint arXiv:2307.02486, 2023.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jiayu Ding、Shuming Ma、Li Dong、Xingxing Zhang、Shaohan Huang、Wenhui Wang、Nanning
    Zheng 和 Furu Wei. Longnet: 扩展变换器至 1,000,000,000 令牌。arXiv 预印本 arXiv:2307.02486,
    2023.'
- en: '[17] Jiarui Fang and Shangchun Zhao. A unified sequence parallelism approach
    for long context generative ai. arXiv preprint arXiv:2405.07719, 2024.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Jiarui Fang 和 Shangchun Zhao. 一种统一的序列并行方法用于长上下文生成 AI。arXiv 预印本 arXiv:2405.07719,
    2024.'
- en: '[18] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil
    Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline
    parallel dnn training, 2018. URL https://arxiv. org/abs, 1806.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Aaron Harlap、Deepak Narayanan、Amar Phanishayee、Vivek Seshadri、Nikhil Devanur、Greg
    Ganger 和 Phil Gibbons. Pipedream: 快速高效的管道并行 DNN 训练，2018. URL https://arxiv.org/abs,
    1806.'
- en: '[19] Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang, Qiaoling
    Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, et al. Characterization
    of large language model development in the datacenter. In USENIX Symposium on
    Networked Systems Design and Implementation (NSDI’24), 2024.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 胡青浩、叶智胜、王泽瑞、王国腾、张萌、陈巧玲、孙鹏、大华林、王晓林、罗英伟 等。数据中心大型语言模型发展的特征。在 USENIX 网络系统设计与实现研讨会（NSDI’24）上，2024年。'
- en: '[20] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
    Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient
    training of giant neural networks using pipeline parallelism. volume 32, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 黄彦平、程有龙、Ankur Bapna、Orhan Firat、陈德豪、陈弥娅、李孝钟、吴纪权、Le Quoc V 和吴永辉 等。Gpipe：使用管道并行性高效训练巨型神经网络。第32卷，2019年。'
- en: '[21] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song,
    Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for
    enabling training of extreme long sequence transformer models. arXiv preprint
    arXiv:2309.14509, 2023.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Sam Ade Jacobs、田中雅宏、张成明、张敏佳、Leon Song、Samyam Rajbhandari 和 何宇雄。DeepSpeed
    Ulysses：极长序列变换器模型训练的系统优化。arXiv 预印本 arXiv:2309.14509，2023年。'
- en: '[22] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael
    Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation
    in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Vijay Anand Korthikanti、Jared Casper、Sangkug Lym、Lawrence McAfee、Michael
    Andersch、Mohammad Shoeybi 和 Bryan Catanzaro。减少大型变换器模型中的激活重计算。机器学习与系统会议论文集，第5卷，2023年。'
- en: '[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification
    with deep convolutional neural networks. In Proceedings of 26th Annual Conference
    on Neural Information Processing Systems, NeurIPS 2012., pages 1106–1114, 2012.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Alex Krizhevsky、Ilya Sutskever 和 Geoffrey E. Hinton。使用深度卷积神经网络进行 ImageNet
    分类。发表于第26届神经信息处理系统年会（NeurIPS 2012），页码 1106–1114，2012年。'
- en: '[24] Dacheng Li, Rulin Shao, Anze Xie, Eric P Xing, Joseph E Gonzalez, Ion
    Stoica, Xuezhe Ma, and Hao Zhang. Lightseq: Sequence level parallelism for distributed
    training of long context transformers. arXiv preprint arXiv:2310.03294, 2023.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 李大成、邵如林、谢安泽、Eric P Xing、Joseph E Gonzalez、Ion Stoica、马雪哲 和 张浩。Lightseq：用于分布式训练长上下文变换器的序列级并行性。arXiv
    预印本 arXiv:2310.03294，2023年。'
- en: '[25] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja
    Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed
    machine learning with the parameter server. In Proceedings of 11th USENIX Symposium
    on Operating Systems Design and Implementation, OSDI 2014, pages 583–598, 2014.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Mu Li、David G Andersen、Jun Woo Park、Alexander J Smola、Amr Ahmed、Vanja
    Josifovski、James Long、Eugene J Shekita 和 Bor-Yiing Su。通过参数服务器扩展分布式机器学习。发表于第11届
    USENIX 操作系统设计与实现会议（OSDI 2014），页码 583–598，2014年。'
- en: '[26] Mu Li, David G. Andersen, Alexander J. Smola, and Kai Yu. Communication
    efficient distributed machine learning with the parameter server. In Proceedings
    of 28th Annual Conference on Neural Information Processing Systems, NeurIPS 2014.,
    pages 19–27, 2014.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Mu Li、David G. Andersen、Alexander J. Smola 和 Kai Yu。通过参数服务器进行通信高效的分布式机器学习。发表于第28届神经信息处理系统年会（NeurIPS
    2014），页码 19–27，2014年。'
- en: '[27] Hao Liu and Pieter Abbeel. Blockwise parallel transformers for large context
    models. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 刘浩 和 Pieter Abbeel。针对大型上下文模型的块状并行变换器。神经信息处理系统进展，第36卷，2024年。'
- en: '[28] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on
    million-length video and language with ringattention. arXiv preprint arXiv:2402.08268,
    2024.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 刘浩、Wilson Yan、Matei Zaharia 和 Pieter Abbeel。百万长度视频与语言上的世界模型及其环形注意力。arXiv
    预印本 arXiv:2402.08268，2024年。'
- en: '[29] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise
    transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 刘浩、Matei Zaharia 和 Pieter Abbeel。基于块状变换器的环形注意力，用于近乎无限的上下文。arXiv 预印本 arXiv:2310.01889，2023年。'
- en: '[30] Microsoft Azure Quantum Microsoft Research AI4Science. The impact of large
    language models on scientific discovery: a preliminary study using gpt-4. arXiv
    preprint arXiv:2311.07361, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 微软 Azure Quantum 微软研究 AI4Science。大型语言模型对科学发现的影响：使用 GPT-4 的初步研究。arXiv 预印本
    arXiv:2311.07361，2023年。'
- en: '[31] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
    Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,
    Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters
    using megatron-lm. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–15, 2021.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
    Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,
    Bryan Catanzaro 等。利用 megatron-lm 在 GPU 集群上进行高效的大规模语言模型训练。收录于国际高性能计算、网络、存储和分析会议论文集，页码
    1–15，2021年。'
- en: '[32] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
    Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,
    Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters
    using megatron-lm. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–15, 2021.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
    Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer,
    Bryan Catanzaro 等。利用 megatron-lm 在 GPU 集群上进行高效的大规模语言模型训练。收录于国际高性能计算、网络、存储和分析会议论文集，页码
    1–15，2021年。'
- en: '[33] Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. Recent
    advances in deep learning based dialogue systems: A systematic survey. Artificial
    intelligence review, 56(4):3055–3155, 2023.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue 和 Erik Cambria。基于深度学习的对话系统的最新进展：系统性综述。人工智能评论,
    56(4):3055–3155, 2023年。'
- en: '[34] NVDIA. Megatron context parallelism, 2024.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] NVDIA。Megatron 上下文并行性，2024年。'
- en: '[35] NVIDIA. Nvidia dgx superpod: Next generation scalable infrastructure for
    ai leadership: Reference architecture. 2023.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] NVIDIA。Nvidia dgx superpod: 下一代可扩展 AI 领导力基础设施：参考架构。2023年。'
- en: '[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
    Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:
    An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703,
    2019.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
    Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga 等。Pytorch:
    一种命令式风格的高性能深度学习库。arXiv 预印本 arXiv:1912.01703，2019年。'
- en: '[37] Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble pipeline
    parallelism. arXiv preprint arXiv:2401.10241, 2023.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Penghui Qi, Xinyi Wan, Guangxing Huang 和 Min Lin。Zero bubble 管道并行性。arXiv
    预印本 arXiv:2401.10241，2023年。'
- en: '[38] Markus N Rabe and Charles Staats. Self-attention does not need $o(n^{2})$
    memory. arXiv preprint arXiv:2112.05682, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Markus N Rabe 和 Charles Staats。自注意力不需要 $o(n^{2})$ 内存。arXiv 预印本 arXiv:2112.05682，2021年。'
- en: '[39] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero:
    Memory optimizations toward training trillion parameter models. In SC20: International
    Conference for High Performance Computing, Networking, Storage and Analysis, pages
    1–16\. IEEE, 2020.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase 和 Yuxiong He。Zero: 训练万亿参数模型的内存优化。收录于
    SC20：国际高性能计算、网络、存储和分析会议，页码 1–16。IEEE，2020年。'
- en: '[40] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,
    Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. $\{$ model training. In
    2021 USENIX Annual Technical Conference (USENIX ATC 21), pages 551–564, 2021.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,
    Shuangyan Yang, Minjia Zhang, Dong Li 和 Yuxiong He。$\{$ 模型训练。收录于 2021 年 USENIX
    年度技术会议（USENIX ATC 21），页码 551–564，2021年。'
- en: '[41] Ludan Ruan and Qin Jin. Survey: Transformer based video-language pre-training.
    AI Open, 3:1–13, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Ludan Ruan 和 Qin Jin。综述：基于 Transformer 的视频语言预训练。AI Open, 3:1–13, 2022年。'
- en: '[42] Peng Sun, Yonggang Wen, Ruobing Han, Wansen Feng, and Shengen Yan. Gradientflow:
    Optimizing network performance for large-scale distributed dnn training. IEEE
    Transactions on Big Data, 8(2):495–507, 2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Peng Sun, Yonggang Wen, Ruobing Han, Wansen Feng 和 Shengen Yan。Gradientflow:
    优化大规模分布式 DNN 训练的网络性能。IEEE 大数据期刊, 8(2):495–507, 2019年。'
- en: '[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等。Llama: 开放而高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023年。'
- en: '[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023.'
- en: '[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等. Llama 2: Open foundation and fine-tuned chat models. arXiv 预印本 arXiv:2307.09288,
    2023.'
- en: '[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    volume 30, 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. Attention is all you need. volume
    30, 2017.'
- en: '[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. Attention is all you need. 神经信息处理系统进展,
    30, 2017.'
- en: '[48] Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, and Naader Hasani.
    Optimized network architectures for large language model training with billions
    of parameters. arXiv preprint arXiv:2307.12169, 2023.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, 和 Naader Hasani.
    用于数十亿参数的大型语言模型训练的优化网络架构. arXiv 预印本 arXiv:2307.12169, 2023.'
- en: '[49] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
    Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training
    vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 558–567, 2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
    Francis EH Tay, Jiashi Feng, 和 Shuicheng Yan. Tokens-to-token vit: 从头开始在 imagenet
    上训练视觉变换器. 在 IEEE/CVF 国际计算机视觉会议论文集中，页558–567, 2021.'
- en: '[50] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Span-based localizing
    network for natural language video localization. In Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pages 6543–6554, 2020.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Hao Zhang, Aixin Sun, Wei Jing, 和 Joey Tianyi Zhou. 基于跨度的局部化网络用于自然语言视频定位.
    在第58届计算语言学协会年会论文集中，页6543–6554, 2020.'
- en: '[51] Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul
    Chilimbi, Mu Li, and Xin Jin. Mics: near-linear scaling for training gigantic
    model on public cloud. Proceedings of the VLDB Endowment, 16:37–50, 2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul
    Chilimbi, Mu Li, 和 Xin Jin. Mics: 公共云上训练巨大模型的近线性扩展. VLDB 基金会会议论文集, 16:37–50, 2022.'
- en: '[52] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu,
    Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: Experiences
    on scaling fully sharded data parallel. Proceedings of the VLDB Endowment, 16(12):3848–3860,
    2023.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu,
    Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, 等人。Pytorch fsdp: 扩展完全分片数据并行的经验。VLDB
    期刊，16(12):3848–3860，2023年。'
- en: '[53] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating
    inter-and $\{$ parallelism for distributed deep learning. In 16th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 22), pages 559–578, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, 等人。Alpa: 自动化的交叉和$\{$
    并行性用于分布式深度学习。在第16届USENIX操作系统设计与实现研讨会（OSDI 22），第559–578页，2022年。'
- en: '[54] Wenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. Document-level relation
    extraction with adaptive thresholding and localized context pooling. In Proceedings
    of the AAAI conference on artificial intelligence, volume 35, pages 14612–14620,
    2021.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Wenxuan Zhou, Kevin Huang, Tengyu Ma, 和 Jing Huang。文档级关系提取与自适应阈值和局部上下文池化。在AAAI人工智能会议论文集，第35卷，第14612–14620页，2021年。'
- en: 9\. Appendix
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 附录
- en: 'Table [6](#S9.T6 "Table 6 ‣ 9\. Appendix ‣ LoongTrain: Efficient Training of
    Long-Sequence LLMs with Head-Context Parallelism") shows training performance
    (TGS) of 7B-MHA and 7B-GQA on 64 GPUs with $d_{sp}=64$.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S9.T6 "表6 ‣ 9\. 附录 ‣ LoongTrain: 使用头上下文并行性高效训练长序列LLMs") 显示了在64个GPU上，$d_{sp}=64$的7B-MHA和7B-GQA的训练性能（TGS）。'
- en: '|  |  |  | 128K | 256K | 512K | 1M |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 128K | 256K | 512K | 1M |'
- en: '|  |  |  | With SC++ | W/O SC++ | With SC++ | W/O SC++ | With SC++ | W/O SC++
    | With SC++ | W/O SC++ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 使用 SC++ | 不使用 SC++ | 使用 SC++ | 不使用 SC++ | 使用 SC++ | 不使用 SC++ | 使用
    SC++ | 不使用 SC++ |'
- en: '|  | $d_{cp}$ | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF |
    HF | CF | HF | CF |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{cp}$ | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF | HF | CF |
    HF | CF | HF | CF |'
- en: '| MHA | 64 | 1 | 190.2 | 190.2 | 145.3 | 145.3 | 195.4 | 195.4 | 149.4 | 149.4
    | 196.8 | 196.8 | 149.9 | 149.9 | 161.7 | 161.7 | 127.6 | 127.6 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| MHA | 64 | 1 | 190.2 | 190.2 | 145.3 | 145.3 | 195.4 | 195.4 | 149.4 | 149.4
    | 196.8 | 196.8 | 149.9 | 149.9 | 161.7 | 161.7 | 127.6 | 127.6 |'
- en: '| 32 | 2 | 203.9 | 327.1 | 158.8 | 260.4 | 212.0 | 340.8 | 163.6 | 269.2 |
    214.2 | 294.3 | 164.7 | 239.3 | 169.8 | 173.9 | 140.8 | 145.2 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2 | 203.9 | 327.1 | 158.8 | 260.4 | 212.0 | 340.8 | 163.6 | 269.2 |
    214.2 | 294.3 | 164.7 | 239.3 | 169.8 | 173.9 | 140.8 | 145.2 |'
- en: '| 16 | 4 | 363.2 | 505.9 | 290.4 | 422.5 | 386.0 | 464.6 | 304.7 | 389.1 |
    318.7 | 319.7 | 260.0 | 262.7 | 185.7 | 182.1 | 149.3 | 147.5 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 363.2 | 505.9 | 290.4 | 422.5 | 386.0 | 464.6 | 304.7 | 389.1 |
    318.7 | 319.7 | 260.0 | 262.7 | 185.7 | 182.1 | 149.3 | 147.5 |'
- en: '| 8 | 8 | 585.6 | 662.6 | 486.9 | 582.2 | 533.5 | 515.6 | 443.6 | 437.8 | 340.1
    | 324.0 | 277.1 | 266.8 | 188.4 | 186.1 | 151.7 | 150.2 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 585.6 | 662.6 | 486.9 | 582.2 | 533.5 | 515.6 | 443.6 | 437.8 | 340.1
    | 324.0 | 277.1 | 266.8 | 188.4 | 186.1 | 151.7 | 150.2 |'
- en: '| 4 | 16 | 676.9 | 675.9 | 596.3 | 585.0 | 535.2 | 519.5 | 452.4 | 441.1 |
    329.9 | 323.0 | 270.4 | 266.8 | 185.5 | 185.9 | 149.3 | 147.2 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 676.9 | 675.9 | 596.3 | 585.0 | 535.2 | 519.5 | 452.4 | 441.1 |
    329.9 | 323.0 | 270.4 | 266.8 | 185.5 | 185.9 | 149.3 | 147.2 |'
- en: '| 2 | 32 | 661.0 | 679.9 | 586.7 | 605.7 | 516.4 | 517.2 | 433.6 | 438.7 |
    321.3 | 323.8 | 263.2 | 267.2 | 185.0 | 185.0 | 148.4 | 145.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32 | 661.0 | 679.9 | 586.7 | 605.7 | 516.4 | 517.2 | 433.6 | 438.7 |
    321.3 | 323.8 | 263.2 | 267.2 | 185.0 | 185.0 | 148.4 | 145.0 |'
- en: '| GQA | 64 | 1 | 526.0 | 526.0 | 404.8 | 404.8 | 465.4 | 465.4 | 377.6 | 377.6
    | 318.7 | 318.7 | 256.5 | 256.5 | 181.6 | 181.6 | 145.3 | 145.3 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| GQA | 64 | 1 | 526.0 | 526.0 | 404.8 | 404.8 | 465.4 | 465.4 | 377.6 | 377.6
    | 318.7 | 318.7 | 256.5 | 256.5 | 181.6 | 181.6 | 145.3 | 145.3 |'
- en: '| 32 | 2 | 585.3 | 655.0 | 480.6 | 555.4 | 514.6 | 527.2 | 424.0 | 435.1 |
    333.5 | 328.5 | 270.0 | 265.9 | 186.4 | 184.6 | 149.5 | 148.9 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 2 | 585.3 | 655.0 | 480.6 | 555.4 | 514.6 | 527.2 | 424.0 | 435.1 |
    333.5 | 328.5 | 270.0 | 265.9 | 186.4 | 184.6 | 149.5 | 148.9 |'
- en: '| 16 | 4 | 732.1 | 698.8 | 637.6 | 606.6 | 571.6 | 537.0 | 473.1 | 457.6 |
    342.4 | 334.8 | 277.7 | 273.6 | 189.7 | 187.9 | 152.1 | 152.4 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 732.1 | 698.8 | 637.6 | 606.6 | 571.6 | 537.0 | 473.1 | 457.6 |
    342.4 | 334.8 | 277.7 | 273.6 | 189.7 | 187.9 | 152.1 | 152.4 |'
- en: '| 8 | 8 | 779.7 | 730.6 | 676.0 | 640.8 | 588.9 | 554.7 | 481.3 | 466.4 | 349.8
    | 340.6 | 284.3 | 279.2 | 194.0 | 191.6 | 155.6 | 154.3 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 779.7 | 730.6 | 676.0 | 640.8 | 588.9 | 554.7 | 481.3 | 466.4 | 349.8
    | 340.6 | 284.3 | 279.2 | 194.0 | 191.6 | 155.6 | 154.3 |'
- en: '| 4 | 16 | 731.2 | 705.1 | 641.0 | 636.5 | 561.1 | 536.1 | 463.1 | 458.5 |
    339.1 | 334.2 | 277.0 | 274.3 | 190.1 | 189.2 | 152.9 | 149.8 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16 | 731.2 | 705.1 | 641.0 | 636.5 | 561.1 | 536.1 | 463.1 | 458.5 |
    339.1 | 334.2 | 277.0 | 274.3 | 190.1 | 189.2 | 152.9 | 149.8 |'
- en: '| 2 | 32 | 666.4 | 687.5 | 589.2 | 609.7 | 520.3 | 517.6 | 428.1 | 441.6 |
    322.8 | 325.9 | 264.0 | 267.3 | 185.1 | 185.1 | 148.3 | 145.0 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 32 | 666.4 | 687.5 | 589.2 | 609.7 | 520.3 | 517.6 | 428.1 | 441.6 |
    322.8 | 325.9 | 264.0 | 267.3 | 185.1 | 185.1 | 148.3 | 145.0 |'
- en: Table 6. End-to-End Training Performance (TGS) of 7B-MHA and 7B-GQA on 64 GPUs
    with $d_{sp}=64$. SC++ stands for Selective-Checkpoint++, HF for head-first, and
    CF for context-first. The highest TGS value in each column is highlighted.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6. 在 64 个 GPU 上，$d_{sp}=64$ 时 7B-MHA 和 7B-GQA 的端到端训练性能（TGS）。SC++ 代表 Selective-Checkpoint++，HF
    代表头优先，CF 代表上下文优先。每列中的最高 TGS 值已被突出显示。
