- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:56:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:56:22
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Evaluating Zero-Shot Long-Context LLM Compression
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估零-shot 长上下文 LLM 压缩
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06773](https://ar5iv.labs.arxiv.org/html/2406.06773)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06773](https://ar5iv.labs.arxiv.org/html/2406.06773)
- en: spacing=nonfrench
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: spacing=nonfrench
- en: Chenyu Wang
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 王辰宇
- en: cw9420@princeton.edu    Yihan Wang
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: cw9420@princeton.edu    王意涵
- en: yihanw_mems@princeton.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: yihanw_mems@princeton.edu
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: This study evaluates the effectiveness of zero-shot compression techniques on
    large language models (LLMs) under long-context. We identify the tendency for
    computational errors to increase under long-context when employing certain compression
    methods. We propose a hypothesis to explain the varied behavior of different LLM
    compression techniques and explore remedies to mitigate the performance decline
    observed in some techniques under long-context. This is a course report for COS
    598D Machine Learning and Systems by Prof. Kai Li at Princeton University. Due
    to limited computational resources, our experiments were conducted only on LLaMA-2-7B-32K.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究评估了零-shot 压缩技术在长上下文下对大型语言模型（LLM）的有效性。我们发现，在使用某些压缩方法时，计算误差在长上下文下有增加的趋势。我们提出了一个假设来解释不同
    LLM 压缩技术行为的差异，并探讨了减轻在长上下文下某些技术表现下降的补救措施。这是普林斯顿大学 Kai Li 教授的 COS 598D 机器学习与系统课程的课程报告。由于计算资源有限，我们的实验仅在
    LLaMA-2-7B-32K 上进行。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) excel in processing and generating human-like text,
    yet they confront significant challenges.[[16](#bib.bib16), [4](#bib.bib4), [36](#bib.bib36),
    [26](#bib.bib26), [2](#bib.bib2), [44](#bib.bib44)] One major challenge is computational
    efficiency; LLMs demand considerable resources during both training and inference
    stages. Another challenge is their limited context length, which is the maximum
    number of tokens the model can process at once. Inputs that exceed this context
    length can result in unreasonable responses from the LLM.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在处理和生成类似人类的文本方面表现出色，但也面临重大挑战。[[16](#bib.bib16), [4](#bib.bib4), [36](#bib.bib36),
    [26](#bib.bib26), [2](#bib.bib2), [44](#bib.bib44)] 一项主要挑战是计算效率；LLM 在训练和推理阶段都需要大量资源。另一个挑战是其有限的上下文长度，即模型一次可以处理的最大标记数。超过此上下文长度的输入可能导致
    LLM 给出不合理的响应。
- en: To address the computational efficiency challenge of LLMs, many researchers
    have proposed leveraging model compression techniques. These LLM compression techniques
    commonly operate under the assumption that both the weights and activations of
    LLMs can be compressed with minimal impact on computational error[[41](#bib.bib41),
    [35](#bib.bib35), [18](#bib.bib18)]. On the other hand, researchers are also working
    on extending the context length of LLMs[[1](#bib.bib1), [17](#bib.bib17)]. The
    context length of LLMs have been expanding exponentially in recent years. This
    significant increase in context length allows LLMs to handle increasingly complex
    tasks, such as analyzing multiple books and documents simultaneously.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对 LLM 的计算效率挑战，许多研究人员提出了利用模型压缩技术。这些 LLM 压缩技术通常在假设 LLM 的权重和激活值可以在对计算误差影响最小的情况下进行压缩的前提下运作[[41](#bib.bib41),
    [35](#bib.bib35), [18](#bib.bib18)]。另一方面，研究人员也在努力扩展 LLM 的上下文长度[[1](#bib.bib1),
    [17](#bib.bib17)]。近年来 LLM 的上下文长度已经呈指数增长。这一显著的上下文长度增加使得 LLM 能够处理越来越复杂的任务，例如同时分析多本书籍和文档。
- en: After reviewing the literature, we have observed that the majority of studies
    on LLM compression focus on models with relatively short context lengths (e.g.,
    4K). However, the effectiveness of these compression techniques on models with
    extensive context lengths (e.g., 32K) remains under-evaluated. In this project,
    we aim to evaluate zero-shot LLM compression under long-context. We begin by conducting
    both qualitative theoretical analysis and empirical evaluations of long-context
    LLM compression. Subsequently, we attempt to develop appropriate solutions to
    address the performance decline observed in some LLM compression techniques under
    long-context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在审阅文献后，我们观察到大多数 LLM 压缩研究集中于具有相对较短上下文长度的模型（例如，4K）。然而，这些压缩技术在具有广泛上下文长度的模型（例如，32K）上的有效性仍然未得到充分评估。在本项目中，我们旨在评估零-shot
    LLM 压缩在长上下文下的表现。我们首先进行定性的理论分析和长上下文 LLM 压缩的实证评估。随后，我们尝试开发适当的解决方案，以应对在长上下文下观察到的一些
    LLM 压缩技术性能下降的问题。
- en: 'The contribution of this project can be summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的贡献可以总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct a theoretical analysis of long-context LLM compression. We find that
    as the context length increases in compressed LLMs, computational errors tend
    to accumulate.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对长上下文LLM压缩进行了理论分析。我们发现，随着压缩LLM中上下文长度的增加，计算错误往往会累积。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform an empirical evaluation of various LLM compression techniques to
    assess computational errors in extended contexts. Our findings indicate diverse
    behaviors across different compression methods.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对各种LLM压缩技术进行了实证评估，以评估扩展上下文中的计算错误。我们的发现表明，不同压缩方法之间的行为各异。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a hypothesis to explain the varied responses of different LLM compression
    techniques and explore remedies to mitigate the performance decline observed in
    some techniques under long-context scenarios.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个假设来解释不同LLM压缩技术的不同响应，并探索缓解在长上下文场景下观察到的性能下降的补救措施。
- en: The rest of this report is organized as follows. In Section [2](#S2 "2 Related
    Works ‣ Evaluating Zero-Shot Long-Context LLM Compression"), we will introduces
    previous research related to this project. In Section [4](#S4 "4 Evaluation Details
    ‣ Evaluating Zero-Shot Long-Context LLM Compression"), we will expand the technical
    details of our project. In Section [5](#S5 "5 Conclusion ‣ Evaluating Zero-Shot
    Long-Context LLM Compression"), we will conclude our project. In Section 5, we
    will introduce the future work.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告的其余部分组织如下。在第[2](#S2 "2 Related Works ‣ Evaluating Zero-Shot Long-Context
    LLM Compression")节中，我们将介绍与本项目相关的先前研究。在第[4](#S4 "4 Evaluation Details ‣ Evaluating
    Zero-Shot Long-Context LLM Compression")节中，我们将扩展本项目的技术细节。在第[5](#S5 "5 Conclusion
    ‣ Evaluating Zero-Shot Long-Context LLM Compression")节中，我们将总结本项目。在第5节中，我们将介绍未来的工作。
- en: 2 Related Works
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Long-Context LLMs
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长上下文LLMs
- en: In recent times, there has been a push towards expanding the context length
    of Language Models (LLMs) efficiently through continuous pretraining or fine-tuning.
    One approach involves enhancing Rotary Position Embeddings (RoPE) [[33](#bib.bib33)],
    which has led to longer contexts of up to 128k  [[6](#bib.bib6), [7](#bib.bib7),
    [29](#bib.bib29)]. Another line of research, exemplified by Mistral [[16](#bib.bib16)],
    introduces sliding window attention mechanisms that focus only on a portion of
    tokens from the preceding layer, thereby reducing computational demands and facilitating
    pretraining with longer contexts of up to 30k. However, due to the memory-intensive
    nature of autoregressive generation in LLMs [[20](#bib.bib20)], the storage of
    Key-Value (KV) caches for longer contexts slows down inference processes and necessitates
    GPUs with large VRAM capacities.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，推动通过持续预训练或微调有效扩展语言模型（LLMs）上下文长度的趋势逐渐增强。一种方法涉及增强旋转位置嵌入（RoPE） [[33](#bib.bib33)]，这导致了长达128k的上下文
    [[6](#bib.bib6), [7](#bib.bib7), [29](#bib.bib29)]。另一项研究方向，由Mistral [[16](#bib.bib16)]
    例证，引入了滑动窗口注意机制，只关注前一层的一部分标记，从而减少了计算需求，并使得长达30k的上下文预训练成为可能。然而，由于LLMs自回归生成的内存密集型特性
    [[20](#bib.bib20)]，长上下文的Key-Value (KV)缓存存储会减慢推理过程，并需要具有大VRAM容量的GPU。
- en: Quantization in LLMs
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs中的量化
- en: 'Quantization is a commonly employed method for model compression. [[14](#bib.bib14),
    [34](#bib.bib34), [37](#bib.bib37)] Researchers investigate two distinct settings
    for Large Language Model (LLM) quantization:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种常用的模型压缩方法。 [[14](#bib.bib14), [34](#bib.bib34), [37](#bib.bib37)] 研究人员调查了大型语言模型（LLM）量化的两种不同设置：
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: W8A8 quantization, where both activations and weights are quantized to INT8 [[9](#bib.bib9),
    [41](#bib.bib41), [40](#bib.bib40), [39](#bib.bib39), [32](#bib.bib32)].
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: W8A8量化，其中激活值和权重都被量化为INT8 [[9](#bib.bib9), [41](#bib.bib41), [40](#bib.bib40),
    [39](#bib.bib39), [32](#bib.bib32)]。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Low-bit weight-only quantization (e.g., W4A16), where only weights are quantized
    into low-bit integers [[12](#bib.bib12), [10](#bib.bib10), [31](#bib.bib31), [32](#bib.bib32),
    [27](#bib.bib27)].
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 低位权重仅量化（例如，W4A16），其中仅权重被量化为低位整数 [[12](#bib.bib12), [10](#bib.bib10), [31](#bib.bib31),
    [32](#bib.bib32), [27](#bib.bib27)]。
- en: This work concentrates on the second setting, as it not only reduces the hardware
    requirements by necessitating a smaller memory size but also accelerates token
    generation, thus alleviating memory-bound workloads. Besides, we require the algorithm
    to be zero-shot, which is relatively low-cost and non-task-specific.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作集中于第二种设置，因为它不仅通过需要较小的内存大小来降低硬件要求，而且加速了标记生成，从而减轻了内存绑定的工作负载。此外，我们需要算法能够进行零-shot，这相对低成本且非特定任务。
- en: Pruning in LLMs
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs中的剪枝
- en: Pruning, a well-established technique for compressing neural networks, involves
    eliminating weights to create sparse networks [[21](#bib.bib21)]. It can be broadly
    classified into structured and unstructured approaches.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝是一种成熟的神经网络压缩技术，涉及消除权重以创建稀疏网络[[21](#bib.bib21)]。它可以大致分为结构化和非结构化方法。
- en: Structural pruning involves removing entire filters from the neural network,
    making it more conducive to hardware implementation. Various methods exist for
    implementing structural pruning, such as l1-dependent pruning [[14](#bib.bib14),
    [42](#bib.bib42)], first-order importance estimation [[24](#bib.bib24)], hessian-based
    estimation [[19](#bib.bib19), [38](#bib.bib38)], or the optimal brain surgeon [[21](#bib.bib21),
    [19](#bib.bib19)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝涉及从神经网络中移除整个滤波器，使其更适合硬件实现。实施结构化剪枝的方法有多种，例如基于l1的剪枝[[14](#bib.bib14), [42](#bib.bib42)]，一阶重要性估计[[24](#bib.bib24)]，基于海森矩阵的估计[[19](#bib.bib19),
    [38](#bib.bib38)]，或最优脑外科医生[[21](#bib.bib21), [19](#bib.bib19)]。
- en: Unstructured methods [[14](#bib.bib14), [13](#bib.bib13), [28](#bib.bib28),
    [35](#bib.bib35)] like magnitude pruning operate at the individual weight level,
    maintaining performance even at higher sparsity levels. However, existing pruning
    methods usually require modifications to the training procedure [[30](#bib.bib30)],
    retraining the pruned networks to regain accuracy [[23](#bib.bib23)], or a computationally
    intensive iterative retraining process [[3](#bib.bib3), [11](#bib.bib11)]. Yet,
    scaling these techniques to LLMs with billions of parameters poses a challenge,
    as the necessary training process demands significant computational resources [[15](#bib.bib15),
    [45](#bib.bib45)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化方法[[14](#bib.bib14), [13](#bib.bib13), [28](#bib.bib28), [35](#bib.bib35)]如幅度剪枝在个别权重级别操作，即使在更高的稀疏度水平下也能保持性能。然而，现有的剪枝方法通常需要对训练过程进行修改[[30](#bib.bib30)]，重新训练剪枝后的网络以恢复准确性[[23](#bib.bib23)]，或进行计算密集型的迭代重新训练过程[[3](#bib.bib3),
    [11](#bib.bib11)]。然而，将这些技术扩展到具有数十亿参数的LLMs是一个挑战，因为所需的训练过程需要大量的计算资源[[15](#bib.bib15),
    [45](#bib.bib45)]。
- en: 'We focus on unstructured pruning methods in this work as they are more fundamental
    and flexible than structural pruning. Specifically, we choose two representative
    methods: magnitude pruning and Wanda [[35](#bib.bib35)]. Although other model
    compression methods such as neural architecture search exist [[46](#bib.bib46),
    [5](#bib.bib5)], this study focuses exclusively on pruning and quantization.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们关注非结构化剪枝方法，因为它们比结构化剪枝更基础和灵活。具体来说，我们选择了两种代表性方法：幅度剪枝和Wanda[[35](#bib.bib35)]。虽然还有其他模型压缩方法，如神经架构搜索[[46](#bib.bib46),
    [5](#bib.bib5)]，但本研究专注于剪枝和量化。
- en: 3 Concurrent Work
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 同行工作
- en: In our review of the literature, we noted a related study on the quantization
    of long-context LLMs detailed in Li et al. (2024) [[22](#bib.bib22)]. This research
    conducted two experiments to assess the performance of LLMs under extended contexts.
    It reported a decline in performance when applying weight-only quantization, weight-activation
    quantization, and KV cache quantization. However, it did not delve into further
    analysis to explore the underlying causes of this observed performance degradation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对文献的回顾中，我们注意到Li等人（2024）[[22](#bib.bib22)]对长上下文LLMs量化的相关研究。这项研究进行了两个实验，以评估LLMs在扩展上下文下的表现。结果报告了在应用仅权重量化、权重-激活量化和KV缓存量化时性能下降。然而，研究未进一步分析以探索观察到的性能下降的根本原因。
- en: 4 Evaluation Details
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估细节
- en: 4.1 Theoretical Analysis
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 理论分析
- en: LLMs are typically built on transformer architecture, which operates as a ’sequence
    model’. In the transformer architecture, each newly generated token calculates
    its attention scores against the hidden states of all preceding tokens. Regarding
    LLM compression techniques, while they can accelerate inference, they also introduce
    computational errors in both the output and hidden states of LLMs. Consequently,
    in compressed LLMs, each new token is computed based on an increasingly large
    number of preceding tokens, with each token contributing its own computational
    error. The formulas below provide a detailed description of this process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通常基于变换器架构，该架构作为一种“序列模型”运行。在变换器架构中，每生成一个新标记都会计算其相对于所有前面标记的隐藏状态的注意力分数。关于LLM压缩技术，虽然它们可以加速推理，但也会在LLM的输出和隐藏状态中引入计算误差。因此，在压缩的LLM中，每个新标记的计算基于越来越多的前面标记，每个标记都带有其自身的计算误差。下面的公式详细描述了这一过程。
- en: 'Consider the transformer architecture processing a sequence where each token
    $t$ for token $t$ are calculated as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑处理一个序列的 Transformer 架构，其中每个令牌 $t$ 的计算如下：
- en: '|  | $\displaystyle\textbf{a}_{t}=\text{softmax}\left(\frac{\textbf{q}_{t}[k_{1}^{T}\cdots
    k_{t}^{T}]}{\sqrt{d_{k}}}\right)$ |  | (1) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textbf{a}_{t}=\text{softmax}\left(\frac{\textbf{q}_{t}[k_{1}^{T}\cdots
    k_{t}^{T}]}{\sqrt{d_{k}}}\right)$ |  | (1) |'
- en: where $d_{k}$ is the dimensionality of the key vectors, ensuring proper scaling.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{k}$ 是键向量的维度，确保了适当的缩放。
- en: 'The hidden state $\mathbf{h}_{t}$ is computed by:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态 $\mathbf{h}_{t}$ 通过以下公式计算：
- en: '|  | $\displaystyle\mathbf{h}_{t}=[v_{1}^{T},\cdots,v_{t}^{T}]\textbf{a}_{t}$
    |  | (2) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{t}=[v_{1}^{T},\cdots,v_{t}^{T}]\textbf{a}_{t}$
    |  | (2) |'
- en: where $\mathbf{v}_{t}$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{v}_{t}$。
- en: 'When LLMs undergo compression, noise $\epsilon\sim\mathcal{N}(0,\sigma^{2})$
    is added to each key and value vector:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当 LLM 进行压缩时，会在每个键和值向量中添加噪声 $\epsilon\sim\mathcal{N}(0,\sigma^{2})$：
- en: 'Then we have:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到：
- en: '|  | $\displaystyle\mathbf{h}_{t}$ |  | (3) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}_{t}$ |  | (3) |'
- en: '|  |  | $\displaystyle\approx\sum_{i=1}^{t}\text{softmax}\left(\frac{\textbf{q}_{i}k_{i}}{\sqrt{d_{k}}}\right)v_{i}+$
    |  | (4) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\approx\sum_{i=1}^{t}\text{softmax}\left(\frac{\textbf{q}_{i}k_{i}}{\sqrt{d_{k}}}\right)v_{i}+$
    |  | (4) |'
- en: '|  |  | $1$2 |  | (5) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (5) |'
- en: for simplicity, in this step we assume that all the vectors involved are one
    dimensional.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们在这一步假设所有涉及的向量都是一维的。
- en: '![Refer to caption](img/65439f945fa9c1cc525a26ea36b23046.png)![Refer to caption](img/2d2d7b77bf911faaf4fd05589dfa6d80.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/65439f945fa9c1cc525a26ea36b23046.png)![参考说明](img/2d2d7b77bf911faaf4fd05589dfa6d80.png)'
- en: 'Figure 1: Pruning algorithms are robust to context lengths. The KL divergence
    of output logits between the uncompressed model and the pruned models does not
    change much with respect to different context lengths. The pruning ratio will
    only affect the variance of KL divergence values measured in different context
    lengths.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：剪枝算法对上下文长度具有鲁棒性。未压缩模型与剪枝模型之间的输出 logits 的 KL 散度对于不同的上下文长度变化不大。剪枝比例只会影响在不同上下文长度中测量的
    KL 散度值的方差。
- en: This formula indicates that for compressed LLMs, the computation of the $t$,
    indicating increased computation error in longer sequences.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式表明，对于压缩后的 LLM，计算 $t$ 的值，表示在更长序列中计算误差增加。
- en: However, due to the complexity of LLMs, there remains uncertainty regarding
    how this theoretical analysis will be reflected in the final output. In the following
    sub-section, we will discuss the empirical evaluation of the computation error
    of compressed LLMs under long-context.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于 LLM 的复杂性，尚不清楚这种理论分析将如何反映在最终输出中。在接下来的子节中，我们将讨论在长上下文下压缩 LLM 的计算误差的实证评估。
- en: 4.2 Empirical Evaluation
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实证评估
- en: 'To assess the computational error in the final output, we compute the Kullback-Leibler
    (KL) divergence between the outputs of the compressed and uncompressed versions
    of the same model, as indicated below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估最终输出中的计算误差，我们计算了压缩和未压缩版本的相同模型输出之间的 Kullback-Leibler (KL) 散度，如下所示：
- en: '|  | $\displaystyle D_{\text{KL}}(p\parallel q)$ |  | (6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D_{\text{KL}}(p\parallel q)$ |  | (6) |'
- en: where $q$ represents the compressed model’s output.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q$ 代表压缩模型的输出。
- en: We have selected LLaMA-2-7B-32K as our model to evaluate four distinct LLM compression
    techniques across two categories[[8](#bib.bib8)]. For pruning, we implement magnitude
    pruning, which represents the simplest pruning technique, and Wanda pruning, which
    represents the state-of-the-art method. For quantization, we choose to evaluate
    weight-only and weight-activation quantization implemented by [[22](#bib.bib22)].
    Due to its utilization of ’per-group’ weight quantization and ’per-token’ activation
    quantization, the method described in [[22](#bib.bib22)] represents a robust quantization
    technique. It achieves minimal computational error compared to other methods.
    We sampled texts from WikiText dataset [[25](#bib.bib25)], and calculate the KL
    divergence between the models’ output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择 LLaMA-2-7B-32K 作为模型，以评估四种不同的 LLM 压缩技术，分为两个类别[[8](#bib.bib8)]。对于剪枝，我们实现了幅度剪枝，这代表了最简单的剪枝技术，以及
    Wanda 剪枝，这代表了最先进的方法。对于量化，我们选择评估仅权重量化和权重-激活量化，由 [[22](#bib.bib22)] 实现。由于其使用了“每组”权重量化和“每令牌”激活量化，[[22](#bib.bib22)]
    中描述的方法代表了一种稳健的量化技术。与其他方法相比，它在计算误差上达到了最小化。我们从 WikiText 数据集中抽取了文本 [[25](#bib.bib25)]，并计算了模型输出之间的
    KL 散度。
- en: '![Refer to caption](img/e5bdca2261ee774b2c7b6c046e7b997a.png)![Refer to caption](img/7b025fc39ac05b5847e4819810e87bcc.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5bdca2261ee774b2c7b6c046e7b997a.png)![参见说明](img/7b025fc39ac05b5847e4819810e87bcc.png)'
- en: 'Figure 2: Only about 2% weights are sensitive to low-bit quantizations. We
    can use 8-bit quantization instead of 3/4-bit quantization for these weights to
    make the compressed models less sensitive to context lengths.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：只有约 2% 的权重对低比特量化敏感。我们可以对这些权重使用 8 位量化代替 3/4 位量化，以使压缩模型对上下文长度不那么敏感。
- en: 'We plot out experiments using pruning in Figure [1](#S4.F1 "Figure 1 ‣ 4.1
    Theoretical Analysis ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context
    LLM Compression") and experiments using quantization in Figure [3](#S4.F3 "Figure
    3 ‣ 4.2 Empirical Evaluation ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context
    LLM Compression") and Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Empirical Evaluation ‣
    4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context LLM Compression"). Counter-intuitively,
    the performance of pruning and quantization varies. For pruning, the KL-divergence
    of output between the uncompressed model and the pruned models does not change
    much with respect to different context lengths. The pruning ratio will only affect
    the variance of KL-divergence values measured in different context lengths. For
    quantization, especially when we use low-bit ($\leq 4$) weight quantization, the
    performance of compressed models becomes more sensitive to context lengths: the
    output of compressed models becomes more different from that of the uncompressed
    model as the context length increases.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[1](#S4.F1 "图 1 ‣ 4.1 理论分析 ‣ 4 评估细节 ‣ 零样本长上下文 LLM 压缩评估")中绘制了使用剪枝的实验，图[3](#S4.F3
    "图 3 ‣ 4.2 实证评估 ‣ 4 评估细节 ‣ 零样本长上下文 LLM 压缩评估")和图[4](#S4.F4 "图 4 ‣ 4.2 实证评估 ‣ 4
    评估细节 ‣ 零样本长上下文 LLM 压缩评估")中绘制了使用量化的实验。与直觉相反，剪枝和量化的性能有所不同。对于剪枝，未压缩模型与剪枝模型之间输出的KL散度在不同上下文长度下变化不大。剪枝比例仅会影响在不同上下文长度下测量的KL散度值的方差。对于量化，特别是当我们使用低比特（$\leq
    4$）权重量化时，压缩模型的性能对上下文长度变得更加敏感：随着上下文长度的增加，压缩模型的输出与未压缩模型的输出差异变大。
- en: '![Refer to caption](img/1b1daf65e6ea6d26bf81241130dd6801.png)![Refer to caption](img/d09ceefd9efa0daec6156116a91ad166.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1b1daf65e6ea6d26bf81241130dd6801.png)![参见说明](img/d09ceefd9efa0daec6156116a91ad166.png)'
- en: 'Figure 3: When we use low-bit ($\leq 4$) weight quantization, the performance
    of compressed models becomes more sensitive to context lengths: the output of
    compressed models become more different from the of the uncompressed model when
    the context length increases.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：当我们使用低比特（$\leq 4$）权重量化时，压缩模型的性能对上下文长度变得更加敏感：随着上下文长度的增加，压缩模型的输出与未压缩模型的输出差异变大。
- en: '![Refer to caption](img/406ad9a716cd77553d5c1aa6b641bd90.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/406ad9a716cd77553d5c1aa6b641bd90.png)'
- en: 'Figure 4: When choosing 3-bit quantization, it is very obvious that the output
    of compressed models become more different from the of the uncompressed model
    when the context length increases.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：当选择 3 位量化时，很明显压缩模型的输出与未压缩模型的输出差异在上下文长度增加时变大。
- en: 4.3 Hypothesis on the Varied Behaviors
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 行为差异的假设
- en: In this section, we attempt to discuss the reason why pruning remains robust
    as context length increases while quantization suffers from increasing computational
    error.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们尝试讨论剪枝在上下文长度增加时为何保持稳健，而量化却受到计算误差增加的影响。
- en: Our intuition is that for different pruning methods, the weights with larger
    magnitudes are always left untouched, while for quantization methods, they typically
    quantize all the weights regardless of their magnitude. As we observed in Section [4.2](#S4.SS2
    "4.2 Empirical Evaluation ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context
    LLM Compression"), pruning stays robust to context length. Thus, it is reasonable
    to hypothesize that LLMs’ long-range dependency only relies on a small part of
    weights, and for LLaMA-2-7B-32K, the weights with larger magnitude are more sensitive
    in longer contexts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的直觉是，对于不同的剪枝方法，较大幅度的权重总是保持不变，而对于量化方法，它们通常会量化所有权重，无论其幅度如何。正如我们在第[4.2](#S4.SS2
    "4.2 实证评估 ‣ 4 评估细节 ‣ 零样本长上下文 LLM 压缩评估")节中观察到的，剪枝对上下文长度保持稳健。因此，可以合理地假设 LLM 的长程依赖仅依赖于一小部分权重，而对于
    LLaMA-2-7B-32K，较大幅度的权重在较长的上下文中更为敏感。
- en: Our experimental results support our intuition. As is shown in Figure [2](#S4.F2
    "Figure 2 ‣ 4.2 Empirical Evaluation ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot
    Long-Context LLM Compression"), if we select about 2% of weight groups with large
    magnitude, and quantize them to 8 bits, while quantizing other groups to 3 or
    4 bits; we observe that for both weight-only quantization and weight-activation
    quantization, the increasing KL divergence under long-context disappears. However,
    the overall KL divergence level doesn’t change much. In this way, we remedy the
    performance drop of low-bit quantized LLMs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验结果支持了我们的直觉。如图[2](#S4.F2 "Figure 2 ‣ 4.2 Empirical Evaluation ‣ 4 Evaluation
    Details ‣ Evaluating Zero-Shot Long-Context LLM Compression")所示，如果我们选择约2%的大权重组，并将其量化为8位，同时将其他组量化为3或4位；我们观察到，无论是仅权重量化还是权重-激活量化，长上下文下的KL散度增加消失了。然而，总体KL散度水平变化不大。通过这种方式，我们修正了低位量化LLMs的性能下降。
- en: Another experiment results to support our hypothesis is that for pruning, as
    is shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.3 Hypothesis on the Varied Behaviors
    ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context LLM Compression"),
    if we randomly prune out only 10% of the weights, we can observe an almost linearly
    increasing KL divergence as context length increases.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个支持我们假设的实验结果是，如图[5](#S4.F5 "Figure 5 ‣ 4.3 Hypothesis on the Varied Behaviors
    ‣ 4 Evaluation Details ‣ Evaluating Zero-Shot Long-Context LLM Compression")所示，如果我们随机修剪仅10%的权重，我们可以观察到随着上下文长度增加，KL散度几乎线性增加。
- en: '![Refer to caption](img/7e6c16ef8ff18cf4b5a114c0327b3db2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7e6c16ef8ff18cf4b5a114c0327b3db2.png)'
- en: 'Figure 5: If we randomly prune out only 10% weights, we can observe an almost-linearly
    increasing KL divergence as context length increases.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：如果我们随机修剪仅10%的权重，我们可以观察到随着上下文长度增加，KL散度几乎线性增加。
- en: 5 Conclusion
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This work explored the computation errors in zero-shot LLM compression within
    long-context settings, uncovering that computation errors escalate with increased
    context length in certain compression settings, while remaining stable in others.
    Our hypothesis suggests that specific weights are particularly sensitive to compression
    under long-context scenarios, highlighting a critical area for future research
    to optimize LLM performance effectively.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作探讨了零-shot LLM压缩中长上下文设置下的计算误差，揭示了在某些压缩设置下，计算误差随着上下文长度的增加而上升，而在其他设置中则保持稳定。我们的假设认为特定权重在长上下文情境下对压缩特别敏感，这突显了未来研究中一个关键领域，以有效优化LLM性能。
- en: 6 Future Work
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来工作
- en: In this project, we explore this question within a relatively simple setting.
    In the future, we plan to evaluate additional long-context LLMs, such as Llama
    3 and Mistral [[43](#bib.bib43), [16](#bib.bib16)], and conduct experiments across
    a broader range of tasks. Additionally, we acknowledge that our hypothesis requires
    further validation. We aim to deepen our research to solidify these initial findings.
    We can also summarize our research as a new LLM compression methods if possible.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们在相对简单的设置中探索了这个问题。未来，我们计划评估更多的长上下文LLMs，如Llama 3和Mistral [[43](#bib.bib43),
    [16](#bib.bib16)]，并在更广泛的任务范围内进行实验。此外，我们承认我们的假设需要进一步验证。我们旨在深化研究，以巩固这些初步发现。如果可能，我们还可以将我们的研究总结为新的LLM压缩方法。
- en: 7 Acknowledgements
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: We extend our heartfelt gratitude to Prof. Kai Li and the teaching assistants
    for their invaluable guidance and support throughout this project. We also express
    our appreciation to Hao Kang, a PhD student at the Georgia Institute of Technology,
    for his insightful discussions that enriched our work. Chenyu is also grateful
    for Prof. Xuefei Ning and Shiyao Li from Tsinghua University for their work on
    LLM quantization evaluation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们衷心感谢Kai Li教授和教学助理在整个项目中给予的宝贵指导和支持。我们还感谢乔治亚理工学院的博士生Hao Kang，他的深入讨论丰富了我们的工作。Chenyu还感谢清华大学的Xuefei
    Ning教授和Shiyao Li对LLM量化评估的贡献。
- en: References
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu,
    X., Zeng, A., Hou, L., et al. Longbench: A bilingual, multitask benchmark for
    long context understanding. arXiv preprint arXiv:2308.14508 (2023).'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu,
    X., Zeng, A., Hou, L., 等. Longbench: 一种用于长上下文理解的双语多任务基准。arXiv 预印本 arXiv:2308.14508
    (2023)。'
- en: '[2] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L.,
    He, H., Leahy, C., McDonell, K., Phang, J., et al. Gpt-neox-20b: An open-source
    autoregressive language model. arXiv preprint arXiv:2204.06745 (2022).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L.,
    He, H., Leahy, C., McDonell, K., Phang, J., 等. Gpt-neox-20b: 一个开源自回归语言模型。arXiv
    预印本 arXiv:2204.06745 (2022)。'
- en: '[3] Bondarenko, Y., Nagel, M., and Blankevoort, T. Understanding and overcoming
    the challenges of efficient transformer quantization. In Proceedings of the 2021
    Conference on Empirical Methods in Natural Language Processing (Online and Punta
    Cana, Dominican Republic, Nov. 2021), Association for Computational Linguistics,
    pp. 7947–7969.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Bondarenko, Y., Nagel, M., 和 Blankevoort, T. 理解并克服高效变换器量化的挑战。载于《2021年自然语言处理方法学会议论文集》（在线和多米尼加共和国蓬塔卡纳，2021年11月），计算语言学协会，第
    7947–7969 页。'
- en: '[4] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language
    models are few-shot learners. In Advances in Neural Information Processing Systems
    (2020), H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33,
    Curran Associates, Inc., pp. 1877–1901.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., 和 Amodei, D. 语言模型是少样本学习者。载于《神经信息处理系统进展》(2020)，H.
    Larochelle, M. Ranzato, R. Hadsell, M. Balcan 和 H. Lin 主编，第 33 卷，Curran Associates,
    Inc., 第 1877–1901 页。'
- en: '[5] Cai, Y., Wang, C., Ning, X., Zhou, Z., Niu, D., Yang, H., and Wang, Y.
    Deepguiser: Learning to disguise neural architectures for impeding adversarial
    transfer attacks.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Cai, Y., Wang, C., Ning, X., Zhou, Z., Niu, D., Yang, H., 和 Wang, Y. Deepguiser:
    学习伪装神经架构以阻止对抗性迁移攻击。'
- en: '[6] Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of
    large language models via positional interpolation. arXiv preprint arXiv: 2306.15595
    (2023).'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Chen, S., Wong, S., Chen, L., 和 Tian, Y. 通过位置插值扩展大型语言模型的上下文窗口。arXiv 预印本
    arXiv: 2306.15595 (2023)。'
- en: '[7] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. LongloRA:
    Efficient fine-tuning of long-context large language models. In The Twelfth International
    Conference on Learning Representations (2024).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., 和 Jia, J. LongloRA:
    高效微调长上下文的大型语言模型。载于《第十二届国际学习表征会议》(2024)。'
- en: '[8] Computer, T. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K),
    2024. Accessed: yyyy-mm-dd.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Computer, T. LLaMA-2-7B-32K. [https://huggingface.co/togethercomputer/LLaMA-2-7B-32K](https://huggingface.co/togethercomputer/LLaMA-2-7B-32K),
    2024. 访问日期：yyyy-mm-dd。'
- en: '[9] Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit
    matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339
    (2022).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer, L. Llm.int8(): 大规模变换器的8位矩阵乘法。arXiv
    预印本 arXiv:2208.07339 (2022)。'
- en: '[10] Dettmers, T., and Zettlemoyer, L. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720 (2022).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Dettmers, T., 和 Zettlemoyer, L. 关于4位精度的案例：k位推理缩放规律。arXiv 预印本 arXiv:2212.09720
    (2022)。'
- en: '[11] Frankle, J., and Carbin, M. The lottery ticket hypothesis: Finding sparse,
    trainable neural networks. arXiv preprint arXiv:1803.03635 (2018).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Frankle, J., 和 Carbin, M. 彩票票假设：寻找稀疏、可训练的神经网络。arXiv 预印本 arXiv:1803.03635
    (2018)。'
- en: '[12] Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate
    post-training quantization for generative pre-trained transformers. arXiv preprint
    arXiv:2210.17323 (2022).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. Gptq: 生成预训练变换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323 (2022)。'
- en: '[13] Han, S., Mao, H., and Dally, W. J. Deep Compression: Compressing Deep
    Neural Networks with Pruning, Trained Quantization and Huffman Coding. In ICLR
    (2016).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Han, S., Mao, H., 和 Dally, W. J. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。载于 ICLR
    (2016)。'
- en: '[14] Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections
    for efficient neural network. Advances in neural information processing systems
    28 (2015).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Han, S., Pool, J., Tran, J., 和 Dally, W. 学习权重和连接以提高神经网络效率。载于《神经信息处理系统进展》28
    (2015)。'
- en: '[15] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
    E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal
    large language models. arXiv preprint arXiv:2203.15556 (2022).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
    E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., 等。训练计算优化的大型语言模型。arXiv
    预印本 arXiv:2203.15556 (2022)。'
- en: '[16] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S.,
    Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral
    7b. arXiv preprint arXiv:2310.06825 (2023).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S.,
    Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等。Mistral
    7b。arXiv 预印本 arXiv:2310.06825 (2023)。'
- en: '[17] Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L.
    LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt
    Compression, Oct. 2023. arXiv:2310.06839 [cs].'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., 和 Qiu, L. LongLLMLingua:
    通过提示压缩加速和增强长上下文场景中的 LLM，2023年10月。arXiv:2310.06839 [cs]。'
- en: '[18] Kang, H., Zhang, Q., Kundu, S., Jeong, G., Liu, Z., Krishna, T., and Zhao,
    T. Gear: An efficient kv cache compression recipefor near-lossless generative
    inference of llm. arXiv preprint arXiv:2403.05527 (2024).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Kang, H., Zhang, Q., Kundu, S., Jeong, G., Liu, Z., Krishna, T., 和 Zhao,
    T. Gear: 一种高效的 kv 缓存压缩方法用于近无损的 llm 生成推理。arXiv 预印本 arXiv:2403.05527 (2024)。'
- en: '[19] Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B.,
    Goin, M., and Alistarh, D. The optimal bert surgeon: Scalable and accurate second-order
    pruning for large language models. arXiv preprint arXiv:2203.07259 (2022).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Kurtic, E., Campos, D., Nguyen, T., Frantar, E., Kurtz, M., Fineran, B.,
    Goin, M., 和 Alistarh, D. 最优 BERT 外科医生: 可扩展且准确的大型语言模型二阶剪枝。arXiv 预印本 arXiv:2203.07259
    (2022)。'
- en: '[20] Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez,
    J., Zhang, H., and Stoica, I. Efficient memory management for large language model
    serving with pagedattention. In Proceedings of the 29th Symposium on Operating
    Systems Principles (2023), pp. 611–626.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez,
    J., Zhang, H., 和 Stoica, I. 大型语言模型服务的高效内存管理与分页注意力。在第29届操作系统原理研讨会论文集 (2023), 第611–626页。'
- en: '[21] LeCun, Y., Denker, J., and Solla, S. Optimal brain damage. Advances in
    neural information processing systems 2 (1989).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] LeCun, Y., Denker, J., 和 Solla, S. 最优大脑损伤。神经信息处理系统进展 2 (1989)。'
- en: '[22] Li, S., Ning, X., Wang, L., Liu, T., Shi, X., Yan, S., Dai, G., Yang,
    H., and Wang, Y. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158
    (2024).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Li, S., Ning, X., Wang, L., Liu, T., Shi, X., Yan, S., Dai, G., Yang,
    H., 和 Wang, Y. 评估量化的大型语言模型。arXiv 预印本 arXiv:2402.18158 (2024)。'
- en: '[23] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining
    approach. arXiv preprint arXiv:1907.11692 (2019).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., 和 Stoyanov, V. Roberta: 一种稳健优化的 bert 预训练方法。arXiv 预印本 arXiv:1907.11692
    (2019)。'
- en: '[24] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord,
    O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought
    chains for science question answering. Advances in Neural Information Processing
    Systems 35 (2022), 2507–2521.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord,
    O., Clark, P., 和 Kalyan, A. 学会解释: 通过思维链进行多模态推理以回答科学问题。神经信息处理系统进展 35 (2022), 2507–2521。'
- en: '[25] Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture
    models, 2016.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. 指针哨兵混合模型, 2016。'
- en: '[26] OpenAI. Gpt-4 technical report, 2023.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] OpenAI. GPT-4 技术报告, 2023。'
- en: '[27] Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., and Lee, D. nuqmm:
    Quantized matmul for efficient inference of large-scale generative language models.
    arXiv preprint arXiv:2206.09557 (2022).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Park, G., Park, B., Kwon, S. J., Kim, B., Lee, Y., 和 Lee, D. nuqmm: 高效推理大规模生成语言模型的量化矩阵乘法。arXiv
    预印本 arXiv:2206.09557 (2022)。'
- en: '[28] Paul, M., Chen, F., Larsen, B. W., Frankle, J., Ganguli, S., and Dziugaite,
    G. K. Unmasking the lottery ticket hypothesis: What’s encoded in a winning ticket’s
    mask? arXiv preprint arXiv:2210.03044 (2022).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Paul, M., Chen, F., Larsen, B. W., Frankle, J., Ganguli, S., 和 Dziugaite,
    G. K. 揭开彩票票假说的面纱: 一个中奖票的掩码中编码了什么？arXiv 预印本 arXiv:2210.03044 (2022)。'
- en: '[29] Peng, B., Quesnelle, J., Fan, H., and Shippole, E. YaRN: Efficient context
    window extension of large language models. In The Twelfth International Conference
    on Learning Representations (2024).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Peng, B., Quesnelle, J., Fan, H., 和 Shippole, E. YaRN: 大型语言模型的高效上下文窗口扩展。在第十二届国际学习表征会议上
    (2024)。'
- en: '[30] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted
    training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207
    (2021).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., 等人。《多任务提示训练实现零样本任务泛化》。arXiv
    预印本 arXiv:2110.08207 (2021)。'
- en: '[31] Sheng, Y., Cao, S., Li, D., Hooper, C., Lee, N., Yang, S., Chou, C., Zhu,
    B., Zheng, L., Keutzer, K., Gonzalez, J. E., and Stoica, I. S-LoRA: Serving Thousands
    of Concurrent LoRA Adapters, Nov. 2023. arXiv:2311.03285 [cs].'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Sheng, Y., Cao, S., Li, D., Hooper, C., Lee, N., Yang, S., Chou, C., Zhu,
    B., Zheng, L., Keutzer, K., Gonzalez, J. E., 和 Stoica, I. 《S-LoRA：服务于数千个并发 LoRA
    适配器》，2023年11月。arXiv:2311.03285 [cs]。'
- en: '[32] Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, D. Y., Xie,
    Z., Chen, B., Barrett, C., Gonzalez, J. E., et al. High-throughput generative
    inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865
    (2023).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, D. Y., Xie,
    Z., Chen, B., Barrett, C., Gonzalez, J. E., 等人。《单 GPU 高吞吐量生成推理的大型语言模型》。arXiv 预印本
    arXiv:2303.06865 (2023)。'
- en: '[33] Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer
    with rotary position embedding. NEUROCOMPUTING (2021).'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Su, J., Lu, Y., Pan, S., Wen, B., 和 Liu, Y. 《Roformer：增强的旋转位置嵌入变换器》。NEUROCOMPUTING
    (2021)。'
- en: '[34] Sun, H., Wang, C., Zhu, Z., Ning, X., Dai, G., Yang, H., and Wang, Y.
    Gibbon: Efficient co-exploration of nn model and processing-in-memory architecture.
    In 2022 Design, Automation & Test in Europe Conference & Exhibition (DATE) (2022),
    IEEE, pp. 867–872.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Sun, H., Wang, C., Zhu, Z., Ning, X., Dai, G., Yang, H., 和 Wang, Y. 《Gibbon：神经网络模型和内存处理架构的高效共同探索》。在2022年设计、自动化与测试欧洲会议（DATE）中，IEEE，第867–872页
    (2022)。'
- en: '[35] Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning
    approach for large language models. arXiv preprint arXiv:2306.11695 (2023).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Sun, M., Liu, Z., Bair, A., 和 Kolter, J. Z. 《一种简单有效的大型语言模型剪枝方法》。arXiv
    预印本 arXiv:2306.11695 (2023)。'
- en: '[36] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open
    foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等人。《Llama 2：开放基础和微调聊天模型》。arXiv
    预印本 arXiv:2307.09288 (2023)。'
- en: '[37] Wang, C., Dong, Z., Zhou, D., Zhu, Z., Wang, Y., Feng, J., and Keutzer,
    K. Epim: Efficient processing-in-memory accelerators based on epitome. arXiv preprint
    arXiv:2311.07620 (2023).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Wang, C., Dong, Z., Zhou, D., Zhu, Z., Wang, Y., Feng, J., 和 Keutzer,
    K. 《Epim：基于典型本的高效内存处理加速器》。arXiv 预印本 arXiv:2311.07620 (2023)。'
- en: '[38] Wang, C., Grosse, R., Fidler, S., and Zhang, G. Eigendamage: Structured
    pruning in the kronecker-factored eigenbasis. In International conference on machine
    learning (2019), PMLR, pp. 6566–6575.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Wang, C., Grosse, R., Fidler, S., 和 Zhang, G. 《Eigendamage：Kronecker 因子本征基中的结构化剪枝》。在国际机器学习会议（2019）中，PMLR，第6566–6575页。'
- en: '[39] Wei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., and Liu, X.
    Outlier suppression+: Accurate quantization of large language models by equivalent
    and optimal shifting and scaling. arXiv preprint arXiv:2304.09145 (2023).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Wei, X., Zhang, Y., Li, Y., Zhang, X., Gong, R., Guo, J., 和 Liu, X. 《Outlier
    suppression+：通过等效和最优的平移与缩放来实现大型语言模型的准确量化》。arXiv 预印本 arXiv:2304.09145 (2023)。'
- en: '[40] Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F.,
    and Liu, X. Outlier suppression: Pushing the limit of low-bit transformer language
    models. arXiv preprint arXiv:2209.13325 (2022).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F.,
    和 Liu, X. 《Outlier suppression：推动低比特变换器语言模型的极限》。arXiv 预印本 arXiv:2209.13325 (2022)。'
- en: '[41] Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S. Smoothquant: Accurate
    and efficient post-training quantization for large language models. arXiv preprint
    arXiv:2211.10438 (2022).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Xiao, G., Lin, J., Seznec, M., Demouth, J., 和 Han, S. 《Smoothquant：大型语言模型的准确高效后训练量化》。arXiv
    预印本 arXiv:2211.10438 (2022)。'
- en: '[42] Zafrir, O., Larey, A., Boudoukh, G., Shen, H., and Wasserblat, M. Prune
    once for all: Sparse pre-trained language models. arXiv preprint arXiv:2111.05754
    (2021).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Zafrir, O., Larey, A., Boudoukh, G., Shen, H., 和 Wasserblat, M. 《Prune
    once for all：稀疏预训练语言模型》。arXiv 预印本 arXiv:2111.05754 (2021)。'
- en: '[43] Zhang, P., Shao, N., Liu, Z., Xiao, S., Qian, H., Ye, Q., and Dou, Z.
    Extending llama-3’s context ten-fold overnight. arXiv preprint arXiv:2404.19553
    (2024).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Zhang, P., Shao, N., Liu, Z., Xiao, S., Qian, H., Ye, Q., 和 Dou, Z. 《在一夜之间将
    llama-3 的上下文扩展十倍》。arXiv 预印本 arXiv:2404.19553 (2024)。'
- en: '[44] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P.,
    and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init
    attention. arXiv preprint arXiv:2303.16199 (2023).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P.
    和 Qiao, Y. Llama-adapter: 高效的语言模型零初始化注意力微调。arXiv 预印本 arXiv:2303.16199 (2023)。'
- en: '[45] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan,
    C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained transformer language
    models. arXiv preprint arXiv:2205.01068 (2022).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan,
    C., Diab, M., Li, X., Lin, X. V. 等人. Opt: 开放预训练变换器语言模型。arXiv 预印本 arXiv:2205.01068
    (2022)。'
- en: '[46] Zoph, B., and Le, Q. V. Neural architecture search with reinforcement
    learning. arXiv preprint arXiv:1611.01578 (2016).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Zoph, B. 和 Le, Q. V. 使用强化学习进行神经网络架构搜索。arXiv 预印本 arXiv:1611.01578 (2016)。'
