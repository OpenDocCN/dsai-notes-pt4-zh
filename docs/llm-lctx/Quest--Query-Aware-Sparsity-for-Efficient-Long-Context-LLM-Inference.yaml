- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:03:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:03:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Quest：查询感知稀疏性以提高长上下文 LLM 推理效率
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10774](https://ar5iv.labs.arxiv.org/html/2406.10774)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10774](https://ar5iv.labs.arxiv.org/html/2406.10774)
- en: Jiaming Tang    Yilong Zhao    Kan Zhu    Guangxuan Xiao    Baris Kasikci   
    Song Han
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jiaming Tang    Yilong Zhao    Kan Zhu    Guangxuan Xiao    Baris Kasikci   
    Song Han
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As the demand for long-context large language models (LLMs) increases, models
    with context windows of up to 128K or 1M tokens are becoming increasingly prevalent.
    However, long-context LLM inference is challenging since the inference speed decreases
    significantly as the sequence length grows. This slowdown is primarily caused
    by loading a large KV cache during self-attention. Previous works have shown that
    a small portion of critical tokens will dominate the attention outcomes. However,
    we observe the criticality of a token highly depends on the query. To this end,
    we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track
    of the minimal and maximal Key values in KV cache pages and estimates the criticality
    of a given page using Query vectors. By only loading the Top-K critical KV cache
    pages for attention, Quest significantly speeds up self-attention without sacrificing
    accuracy. We show that Quest can achieve up to $7.03\times$ while performing well
    on tasks with long dependencies with negligible accuracy loss. Code is available
    at [https://github.com/mit-han-lab/Quest](https://github.com/mit-han-lab/Quest).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对长上下文大型语言模型（LLMs）需求的增加，具有高达128K或1M tokens上下文窗口的模型变得越来越普遍。然而，长上下文 LLM 的推理具有挑战性，因为随着序列长度的增加，推理速度显著降低。这种减速主要是由于在自注意力期间加载了大量
    KV 缓存。之前的研究表明，少量关键 tokens 会主导注意力结果。然而，我们观察到一个 token 的关键性高度依赖于查询。为此，我们提出了 Quest，一个查询感知的
    KV 缓存选择算法。Quest 跟踪 KV 缓存页中的最小和最大 Key 值，并使用 Query 向量估算给定页面的关键性。通过仅加载前 K 个关键 KV
    缓存页进行注意力计算，Quest 显著加快了自注意力的速度，同时不牺牲准确性。我们展示了 Quest 在执行具有长依赖的任务时可以达到高达 $7.03\times$
    的速度提升，且准确性损失微乎其微。代码可在 [https://github.com/mit-han-lab/Quest](https://github.com/mit-han-lab/Quest)
    获取。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rapid evolution of Large Language Models (LLMs) has shaped our daily lives.
    With the increasing demand for multi-round conversations and long document queries,
    the maximum context length of LLMs has dramatically grown from 2K to 1M (Liu et al.,
    [2024a](#bib.bib11); Peng et al., [2023](#bib.bib18); Tworkowski et al., [2023](#bib.bib23)).
    The 128k context length GPT-4 model has already been deployed in large-scale serving,
    which is equivalent to 300 pages of text (OpenAI, [2023](#bib.bib15)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的快速发展已经改变了我们的日常生活。随着对多轮对话和长文档查询需求的增加，LLMs 的最大上下文长度从 2K 急剧增长到 1M
    (Liu 等，[2024a](#bib.bib11)；Peng 等，[2023](#bib.bib18)；Tworkowski 等，[2023](#bib.bib23))。128k
    上下文长度的 GPT-4 模型已经在大规模服务中部署，相当于 300 页的文本 (OpenAI，[2023](#bib.bib15))。
- en: However, processing long-context requests is challenging. Due to the auto-regressive
    nature of LLMs, generating one token would require reading the entire KV cache.
    For Llama 7B model (Touvron et al., [2023](#bib.bib22)) with $32$% of the inference
    latency^*^**Tested with FP16 FlashInfer implementation on an RTX4090, limiting
    the overall throughput.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，处理长上下文请求具有挑战性。由于 LLM 的自回归特性，生成一个 token 需要读取整个 KV 缓存。对于 Llama 7B 模型 (Touvron
    等，[2023](#bib.bib22))，$32$% 的推理延迟^*^**在 RTX4090 上使用 FP16 FlashInfer 实现，限制了整体吞吐量。
- en: Despite the increasingly large size of the KV cache, previous works have shown
    that a small portion of the tokens can dominate the accuracy of token generation (Zhang
    et al., [2023b](#bib.bib28); Ge et al., [2024](#bib.bib5)). Therefore, we can
    dramatically reduce the inference latency by only loading the critical tokens,
    while still maintaining accuracy. Thus, it is essential to identify critical portions
    of the KV cache.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 KV 缓存的大小越来越大，但之前的研究表明，少量 tokens 可以主导 token 生成的准确性 (Zhang 等，[2023b](#bib.bib28)；Ge
    等，[2024](#bib.bib5))。因此，通过仅加载关键 tokens，我们可以显著减少推理延迟，同时保持准确性。因此，识别 KV 缓存中的关键部分至关重要。
- en: '![Refer to caption](img/ce9b808a46f0c7c01cc1c193c0be9b00.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce9b808a46f0c7c01cc1c193c0be9b00.png)'
- en: 'Figure 1: Comparison between Dense Attention(a), Query-Agnostic Sparsity (b)
    and Quest’s Query-aware Sparsity (c). Quest significantly speeds up self-attention
    while maintaining high accuracy by dynamically determining the critical tokens
    based on the current query. $T$ represents the number of critical tokens for attention.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：密集注意力（a）、查询不可知稀疏性（b）和Quest的查询感知稀疏性（c）之间的比较。Quest通过根据当前查询动态确定关键令牌，显著加速自注意力，同时保持高精度。$T$表示注意力的关键令牌数量。
- en: 'In this work, we further observe that the criticality of the tokens can change
    with different query tokens. As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3 Methodlogy
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"), the
    critical tokens vary a lot with different queries. Therefore, we need a dynamic
    and efficient approach to determine which portion of the KV cache needs to be
    attended to. To this end, we propose Quest, a query-aware criticality estimation
    algorithm for long-context LLM inference that efficiently and effectively identifies
    critical KV cache tokens and performs self-attention selectively on chosen tokens,
    as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference").'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们进一步观察到令牌的关键性可能会随着不同查询令牌而变化。如图[2](#S3.F2 "图2 ‣ 3 方法 ‣ Quest：用于高效长上下文LLM推理的查询感知稀疏性")所示，关键令牌随着不同查询变化很大。因此，我们需要一种动态和高效的方法来确定KV缓存中需要关注的部分。为此，我们提出了Quest，一种查询感知的关键性估计算法，用于长上下文LLM推理，能够高效且有效地识别关键KV缓存令牌，并有选择地对选定令牌执行自注意力，如图[1](#S1.F1
    "图1 ‣ 1 引言 ‣ Quest：用于高效长上下文LLM推理的查询感知稀疏性")所示。
- en: To reduce the overhead of KV cache criticality estimation, Quest manages KV
    cache at page granularity (Kwon et al., [2023](#bib.bib9)). For each page, Quest
    utilizes maximum and minimum values of each feature dimension of the Key vector
    as the metadata to represent token information. During inference, Quest considers
    both the Query vector and the metadata to estimate each page’s criticality. Given
    all criticality scores of the pages, Quest chooses Top-K pages to perform approximate
    self-attention, where $K$ pages, Quest significantly accelerates inference.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少KV缓存关键性估计的开销，Quest以页面粒度管理KV缓存（Kwon等，[2023](#bib.bib9)）。对于每个页面，Quest利用Key向量每个特征维度的最大值和最小值作为元数据来表示令牌信息。在推理过程中，Quest考虑Query向量和元数据来估计每个页面的关键性。给定所有页面的关键性分数，Quest选择Top-K页面进行近似自注意力，其中$K$页面，Quest显著加速推理。
- en: 'We evaluate both the accuracy and efficiency of Quest. Since Quest dynamically
    decides the criticality of the tokens, Quest achieves better accuracy for a given
    degree of KV cache sparsity than baselines on PG19 dataset (Rae et al., [2019](#bib.bib19)),
    passkey retrieval task (Peng et al., [2023](#bib.bib18)), and LongBench (Bai et al.,
    [2023](#bib.bib2)) with $256$ inference speedup compared to FlashInfer (Ye et al.,
    [2024](#bib.bib26)) with 4-bit weight quantization. In summary, we make the following
    contribution:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了Quest的准确性和效率。由于Quest动态决定令牌的关键性，Quest在PG19数据集（Rae等，[2019](#bib.bib19)）、密码检索任务（Peng等，[2023](#bib.bib18)）和LongBench（Bai等，[2023](#bib.bib2)）上，在KV缓存稀疏性给定程度下，较基线取得了更好的准确性，推理速度较FlashInfer（Ye等，[2024](#bib.bib26)）在4位权重量化情况下提升了$256$倍。总之，我们做出了以下贡献：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An analysis of the self-attention mechanism that pinpoints the importance of
    query-aware sparsity.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对自注意力机制的分析，指出查询感知稀疏性的关键性。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quest, an efficient and accurate KV cache acceleration algorithm, which exploits
    query-aware sparsity by dedicated operator designs and implementations.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Quest是一种高效且准确的KV缓存加速算法，通过专门的操作符设计和实现，利用查询感知稀疏性。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A comprehensive evaluation of Quest, demonstrating up to $7.03\times$ end-to-end
    latency improvement.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对Quest的全面评估，显示出高达$7.03\times$的端到端延迟改进。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Long-context Model
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 长上下文模型
- en: As the demand for long-context models increases, many works have focused on
    extending the context window of LLMs. Currently, many models utilize Rotary Position
    Embeddings (RoPE) (Su et al., [2023](#bib.bib21)), and by different scaling methods
    of RoPE with fine-tuning, the window size of the original 4k Llama-2 has been
    expanded to 32k for LongChat (Li et al., [2023](#bib.bib10)) and 128k for Yarn-Llama-2 (Peng
    et al., [2023](#bib.bib18)). Through length extrapolation, the context windows
    of models reached beyond 1M (Liu et al., [2024b](#bib.bib12)). Beyond open-source
    models, GPT-4 Turbo supports lengths of up to 128k, while Claude-2 supports up
    to 200k (OpenAI, [2024](#bib.bib16); Anthropic, [2024](#bib.bib1)). With models
    increasingly capable of handling long input, this poses challenges for inference
    efficiency. Quest aims to boost long-context inference by exploiting query-aware
    KV cache sparsity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对长上下文模型需求的增加，许多研究集中于扩展大型语言模型（LLMs）的上下文窗口。目前，许多模型采用了旋转位置嵌入（RoPE）（Su et al.,
    [2023](#bib.bib21)），通过不同的RoPE缩放方法和微调，原始的4k Llama-2的窗口大小已经扩展到32k用于LongChat（Li et
    al., [2023](#bib.bib10)）和128k用于Yarn-Llama-2（Peng et al., [2023](#bib.bib18)）。通过长度外推，这些模型的上下文窗口已经突破了1M（Liu
    et al., [2024b](#bib.bib12)）。除了开源模型之外，GPT-4 Turbo支持最长128k，而Claude-2支持最高200k（OpenAI,
    [2024](#bib.bib16); Anthropic, [2024](#bib.bib1)）。随着模型处理长输入的能力越来越强，这对推理效率提出了挑战。Quest旨在通过利用查询感知的KV缓存稀疏性来提升长上下文推理。
- en: 2.2 KV Cache Eviction Algorithm
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 KV缓存驱逐算法
- en: For long-context LLM inference and serving scenarios, the huge size of the KV
    cache results in significant time and space overheads. Many previous efforts have
    been dedicated to compressing the size of the KV cache to accelerate attention
    and reduce memory usage. H2O (Zhang et al., [2023b](#bib.bib28)) retains a limited
    budget of the important KV cache based on the sum of historical attention scores.
    FastGen (Ge et al., [2024](#bib.bib5)) further refines the types of tokens, applying
    a more sophisticated strategy for selecting the KV cache to keep. TOVA (Oren et al.,
    [2024](#bib.bib17)) simplifies the policy by deciding which tokens to permanently
    discard based solely on the current query. StreamingLLM (Xiao et al., [2023](#bib.bib24))
    handles infinitely long texts with attention sinks and a finite KV cache. These
    methods decide which parts of the KV cache to discard based on historical information
    or current states, but discarded tokens might be important for future tokens,
    which may cause the loss of important information. To mitigate this issue, SparQ (Ribar
    et al., [2023](#bib.bib20)) computes approQximate attention scores by channel
    pruning and selects important tokens through them. However, this approach has
    not been widely validated for tasks with long dependencies, and the channel-level
    sparsity might pose challenges to practical acceleration. Therefore, we propose
    Quest, which retains all of the KV cache and selects part of the KV cache based
    on the current query to accelerate long-context self-attention without accuracy
    degradation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长上下文LLM推理和服务场景，KV缓存的巨大规模导致了显著的时间和空间开销。许多先前的工作致力于压缩KV缓存的大小以加速注意力计算和减少内存使用。H2O（Zhang
    et al., [2023b](#bib.bib28)）基于历史注意力分数的总和保留了重要KV缓存的有限预算。FastGen（Ge et al., [2024](#bib.bib5)）进一步细化了令牌的类型，应用了更复杂的策略来选择要保留的KV缓存。TOVA（Oren
    et al., [2024](#bib.bib17)）通过仅根据当前查询决定哪些令牌永久丢弃，从而简化了策略。StreamingLLM（Xiao et al.,
    [2023](#bib.bib24)）通过注意力汇聚和有限的KV缓存处理无限长文本。这些方法基于历史信息或当前状态决定丢弃KV缓存的哪些部分，但被丢弃的令牌可能对未来的令牌很重要，这可能导致重要信息的丢失。为缓解这一问题，SparQ（Ribar
    et al., [2023](#bib.bib20)）通过通道剪枝计算近似注意力分数，并通过这些分数选择重要的令牌。然而，这种方法尚未在长依赖任务中得到广泛验证，并且通道级别的稀疏性可能对实际加速构成挑战。因此，我们提出了Quest，它保留所有KV缓存，并根据当前查询选择部分KV缓存，以在不降低准确性的情况下加速长上下文自注意力。
- en: 3 Methodlogy
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/a328c0a7f3631581b081d01a14e38185.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a328c0a7f3631581b081d01a14e38185.png)'
- en: 'Figure 2: The attention map of prompt “A is B. C is D. A is”. Each row represents
    the attention scores of previous tokens queried by the tokens on the left. When
    queried with “D”, token “B” has a low attention score, showing “B” is not critical
    for generation. However, the “is” strongly attends to “B”. Therefore, the criticality
    of tokens strongly correlates with the current query token.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提示“A是B。C是D。A是”的注意力图。每一行表示左侧令牌查询的前一个令牌的注意力分数。当查询“D”时，令牌“B”的注意力分数较低，显示出“B”对于生成不是关键的。然而，“is”对“B”有很强的关注。因此，令牌的关键性与当前查询令牌有很强的相关性。
- en: In this section, we first motivate Quest by analyzing the breakdown of inference
    cost and self-attention properties. We then present the design of Quest and discuss
    its benefits.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先通过分析推理成本的细分和自注意力属性来激发 Quest 的动机。然后，我们介绍 Quest 的设计并讨论其好处。
- en: 3.1 Long-context Inference Is Costly
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 长上下文推理成本高
- en: LLM inference contains two stages, namely, the prefill stage and the decode
    stage. In the prefill stage, all the input tokens are transformed into embeddings
    and generate the Key ($K$) vectors. Both the Key and the Value vectors are saved
    in the KV cache for future use. The rest of the prefill stage includes self-attention
    and feed-forward network (FFN) layers, which produce the first response token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推理包含两个阶段，即预填充阶段和解码阶段。在预填充阶段，所有输入标记被转换为嵌入并生成键（$K$）向量。键和值向量都保存在 KV 缓存中以备后用。预填充阶段的其余部分包括自注意力和前馈网络（FFN）层，这些层产生第一个响应标记。
- en: '![Refer to caption](img/24abcbe0104b1ad203d3d702a63a67e1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/24abcbe0104b1ad203d3d702a63a67e1.png)'
- en: 'Figure 3: The query aware sparsity for each layer in LongChat-7B model. We
    measure the sparsity by eliminating KV cache tokens while making sure the perplexity
    on PG19 increases less than 0.01\. For the first two layers, the sparsity is below
    10%, while for the rest of the layers, the sparsity is larger than 90%, showing
    great potential for optimization. Quest closely aligns with the oracle.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LongChat-7B 模型中每层的查询感知稀疏性。我们通过消除 KV 缓存标记来测量稀疏性，同时确保 PG19 上的困惑度增加不超过 0.01。对于前两层，稀疏性低于
    10%，而对于其余层，稀疏性大于 90%，显示出优化的巨大潜力。Quest 与神谕高度一致。
- en: In the decode stage, the model will take the last generated token to calculate
    its $K,Q,V$ and send to the FFN.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码阶段，模型将使用最后生成的标记来计算其$K,Q,V$并发送给前馈网络（FFN）。
- en: For one request, the prefill stage only happens once, while a decoding process
    is needed for every token in the response. Therefore, the decode stage dominates
    the inference time. For example, for $16$% of the time is spent on decode stages.
    Therefore, the decode stage performance is crucial for overall latency.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个请求，预填充阶段只发生一次，而每个响应标记都需要一个解码过程。因此，解码阶段主导了推理时间。例如，$16$% 的时间花费在解码阶段。因此，解码阶段的性能对整体延迟至关重要。
- en: Moreover, a long-context scenario significantly slows down the decode stage.
    In every decode stage, the $K$% of the time in a decode stage. Therefore, optimizing
    self-attention becomes a must for efficient long-context inference.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，长上下文场景显著减慢了解码阶段。在每个解码阶段，$K$% 的时间花费在解码阶段。因此，优化自注意力是高效长上下文推理的必然选择。
- en: '![Refer to caption](img/b321fd4230b520d0df59c740f5a87b77.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b321fd4230b520d0df59c740f5a87b77.png)'
- en: 'Figure 4: Recall rate of tokens with Top-10 attention scores. Results are profiled
    with LongChat-7b-v1.5-32k model in passkey retrieval test of $10$K context length.
    Recall rate is the ratio of tokens selected by different attention methods to
    tokens selected by the full attention in each round of decoding. The average rate
    is shown in the figure, with various token budgets assigned.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Top-10 注意力分数的标记召回率。结果是用 LongChat-7b-v1.5-32k 模型在 $10$K 上下文长度的密码检索测试中进行分析的。召回率是不同注意力方法选择的标记与全注意力选择的标记在每轮解码中的比例。图中显示了不同标记预算分配下的平均率。
- en: '![Refer to caption](img/e38a978e6ee647df16fce13ebd5c5980.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e38a978e6ee647df16fce13ebd5c5980.png)'
- en: 'Figure 5: Quest performs self-attention in two stages. In stage 1, Quest estimates
    the criticality of pages by performing element-wise product between the current
    Query vector and both Min Key and Max Key vectors in each KV cache page. Quest
    gets the sum of the per-channel maximal value for each page as the page criticality
    estimation. In stage 2, only Top-K KV cache pages are loaded to perform sparse
    self-attention with the current Query.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Quest 在两个阶段执行自注意力。在阶段 1，Quest 通过在每个 KV 缓存页面中对当前查询向量与最小键（Min Key）和最大键（Max
    Key）向量进行逐元素乘积来估计页面的关键性。Quest 将每个页面的每个通道的最大值求和，作为页面关键性估计。在阶段 2，仅加载 Top-K KV 缓存页面以与当前查询执行稀疏自注意力。
- en: 3.2 Self-Attention Operation Features High Sparsity
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自注意力操作特征高度稀疏
- en: 'Luckily, previous research has highlighted the inherent sparsity in self-attention (Zhang
    et al., [2023b](#bib.bib28); Ge et al., [2024](#bib.bib5)). Due to this property
    of self-attention, a small portion of tokens in the KV cache, called critical
    tokens, can accumulate sufficient attention scores, capturing the most important
    inter-token relationships. For example, as shown in Fig. [3](#S3.F3 "Figure 3
    ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference"), apart from the first two layers, less
    than 10% of the tokens are needed to achieve similar accuracy, which makes the
    attention on the rest of the tokens unnecessary. Therefore, if we can estimate
    the criticality of the tokens, we can only compute self-attention on critical
    KV cache tokens to greatly reduce the memory movement and thus improve efficiency.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '幸运的是，以前的研究突出了自注意力机制中的固有稀疏性 (Zhang et al., [2023b](#bib.bib28); Ge et al., [2024](#bib.bib5))。由于自注意力机制的这一特性，KV
    缓存中的一小部分 token，称为关键 token，可以累积足够的注意力分数，从而捕捉最重要的 token 间关系。例如，如图 [3](#S3.F3 "Figure
    3 ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference") 所示，除了前两层外，少于 10% 的 token 就可以实现类似的准确性，这使得对其余
    token 的注意力变得不必要。因此，如果我们可以估计 token 的关键性，我们可以仅对关键 KV 缓存 token 计算自注意力，从而大大减少内存移动，并提高效率。'
- en: 3.3 Critical Tokens Depend on the Query
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 关键 Token 依赖于查询
- en: 'However, the criticality of the tokens is dynamic and highly dependent on the
    query vector $Q$. Assuming the prompt is ”A is B. C is D. A is”, we demonstrate
    the attention map of a certain head in the 16th layer of Llama-2-7b in Fig.  [2](#S3.F2
    "Figure 2 ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity for Efficient Long-Context
    LLM Inference"). Since the output answer here should be ”B”, the token ”B” is
    critical to the current query ”is”. Thus, it has a high attention score. However,
    before the final token ”is”, ”B” is not critical for any previous query and has
    very low attention scores. In other words, the criticality of tokens is tightly
    related to the query token.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，token 的关键性是动态的，并且高度依赖于查询向量 $Q$。假设提示为 “A is B. C is D. A is”，我们在第16层的 Llama-2-7b
    的某个头部演示了注意力图，如图 [2](#S3.F2 "Figure 2 ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference") 所示。由于此处的输出答案应该是 “B”，因此 token “B” 对当前查询
    “is” 是关键的。因此，它具有较高的注意力分数。然而，在最终的 token “is” 之前， “B” 对任何之前的查询都不是关键的，并且具有非常低的注意力分数。换句话说，token
    的关键性与查询 token 密切相关。'
- en: We quantify this effect by profiling the average recall rate of tokens with
    Top-10 attention scores along the text generations. The original attention with
    full KV cache can maintain $100$ vectors for criticality estimation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对文本生成过程中的 Top-10 注意力分数的 token 平均召回率进行分析来量化这一效应。原始注意力机制在完整的 KV 缓存中可以维持 $100$
    个向量用于关键性估计。
- en: 3.4 Dynamically Estimating Token Criticality
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 动态估计 Token 关键性
- en: 'To efficiently and accurately estimate the criticality of KV cache tokens,
    we propose Quest, an efficient and accurate algorithm that exploits query-aware
    context sparsity, which approximately selects the most potentially critical KV
    cache pages for the current query. We show the workflow of Quest in Fig. [5](#S3.F5
    "Figure 5 ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference"). To manage the overhead, Quest
    adopts PageAttention (Kwon et al., [2023](#bib.bib9)) and selects the KV cache
    pages at the granularity of pages.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '为了高效准确地估计 KV 缓存 token 的关键性，我们提出了 Quest，这是一种高效且准确的算法，利用查询感知的上下文稀疏性，近似选择最有可能对当前查询关键的
    KV 缓存页面。我们在图 [5](#S3.F5 "Figure 5 ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference") 中展示了
    Quest 的工作流程。为了管理开销，Quest 采用 PageAttention (Kwon et al., [2023](#bib.bib9)) 并按页面粒度选择
    KV 缓存页面。'
- en: 'To estimate the criticality of the pages, Quest performs an approximate calculation
    of attention weights before the original attention operation, as shown in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.4 Dynamically Estimating Token Criticality ‣ 3 Methodlogy
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '为了估计页面的关键性，Quest 在原始注意力操作之前执行注意力权重的近似计算，如算法 [1](#alg1 "Algorithm 1 ‣ 3.4 Dynamically
    Estimating Token Criticality ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity for
    Efficient Long-Context LLM Inference") 所示。'
- en: Our insight is that in order not to miss critical tokens, we should select pages
    containing the token with the highest attention weights. However, for an efficient
    selection of pages, we should calculate an approximate attention score following
    this insight. We found that the upper bound attention weights within a page can
    be used to approximate the highest attention in the page. The upper bound of the
    attention weights can be calculated by the channel-wise minimal values ($m_{i}$
    for all tokens in this page regardless of the sign of $Q_{i}$, we get the upper
    bound of attention weights across all Key vectors on this page.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的见解是，为了不遗漏关键 token，我们应该选择包含注意力权重最高的 token 的页面。然而，为了高效选择页面，我们应该按照这个见解计算一个近似的注意力分数。我们发现，页面内的注意力权重上界可以用来近似页面中的最高注意力。注意力权重的上界可以通过通道内的最小值
    ($m_{i}$，对于该页面上的所有 token，不论 $Q_{i}$ 的符号如何，我们得到该页面上所有 Key 向量的注意力权重上界) 来计算。
- en: 'After deriving the upper bound attention weights, we choose the top $K$ is
    an arbitrarily defined hyper-parameter. To demonstrate the feasibility of Quest,
    we perform actual self-attention and gather Top-K per-page attention scores. As
    shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1 Long-context Inference Is Costly ‣ 3
    Methodlogy ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"),
    our query-aware sparsity mostly aligns with the oracle sparsity. Quest performs
    normal self-attention only on selected pages, which greatly reduces memory movement.
    We define the number of tokens in selected pages as the “Token Budget”.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在推导出注意力权重上界后，我们选择前 $K$ 是一个任意定义的超参数。为了证明 Quest 的可行性，我们进行实际的自注意力操作，并收集每页的 Top-K
    注意力分数。如图 [3](#S3.F3 "图 3 ‣ 3.1 长上下文推理的成本 ‣ 3 方法 ‣ Quest: 查询感知稀疏性用于高效长上下文 LLM 推理")
    所示，我们的查询感知稀疏性大多与 oracle 稀疏性对齐。Quest 仅在选定的页面上执行正常的自注意力，从而大大减少了内存移动。我们将选定页面中的 token
    数量定义为“Token 预算”。'
- en: 'Due to the low sparsity ratio for the first two layers (as shown in Fig. [3](#S3.F3
    "Figure 3 ‣ 3.1 Long-context Inference Is Costly ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference")), we only apply Quest and
    all baselines on later layers to better preserve model accuracy. Note that whether
    to skip the first two layers or not is orthogonal to the KV cache selection algorithm.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '由于前两层的稀疏比率较低（如图 [3](#S3.F3 "图 3 ‣ 3.1 长上下文推理的成本 ‣ 3 方法 ‣ Quest: 查询感知稀疏性用于高效长上下文
    LLM 推理") 所示），我们仅在后续层应用 Quest 和所有基准，以更好地保持模型准确性。请注意，是否跳过前两层与 KV 缓存选择算法无关。'
- en: Algorithm 1 Token Criticality Estimation
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 Token 关键性估计
- en: 'When inserting new token to KV cache:  Input: Key vector $K$     $m_{i}$ to
    $dim$  end for'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当将新 token 插入 KV 缓存时： 输入：Key 向量 $K$     $m_{i}$ 到 $dim$  结束 for
- en: 3.5 Quest Reduces the Memory Movement of Self-Attention
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 Quest 减少了自注意力的内存移动
- en: Instead of loading the whole KV cache, Quest only needs to load a fraction of
    the data, which leverages query-aware sparsity. Assume that every $K$ bytes. The
    whole KV cache is $2M*L$ of the total KV cache^‡^‡‡The top-K operator incurs negligible
    memory loading and execution time (5-10 us). Therefore, we do not include it in
    efficiency analysis. , which is equivalent to
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与加载整个 KV 缓存不同，Quest 只需要加载部分数据，这利用了查询感知稀疏性。假设每个 $K$ 字节。整个 KV 缓存为 $2M*L$，总 KV
    缓存^‡^‡‡Top-K 操作带来的内存加载和执行时间可以忽略（5-10 微秒）。因此，我们在效率分析中不包括它。
- en: '|  | $\frac{1}{\text{Page Size}}+\frac{K}{\text{Page Num}}$ |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{1}{\text{页面大小}}+\frac{K}{\text{页面数量}}$ |  |'
- en: Assuming that we use $16$. Note that this memory load reduction is universal
    across all models and is compatible with existing quantization mechanisms (Zhao
    et al., [2024](#bib.bib29)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用 $16$。请注意，这种内存加载减少在所有模型中都是通用的，并且与现有的量化机制兼容 (Zhao et al., [2024](#bib.bib29))。
- en: 4 Experiments
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '| Method / Budget | 32 | 64 | 128 | 256 | 512 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 方法 / 预算 | 32 | 64 | 128 | 256 | 512 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| H2O | 0% | 1% | 1% | 1% | 3% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| H2O | 0% | 1% | 1% | 1% | 3% |'
- en: '| TOVA | 0% | 1% | 1% | 3% | 8% |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| TOVA | 0% | 1% | 1% | 3% | 8% |'
- en: '| StreamingLLM | 1% | 1% | 1% | 3% | 5% |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 1% | 1% | 1% | 3% | 5% |'
- en: '| Quest (ours) | 65% | 99% | 99% | 99% | 100% |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Quest (我们的) | 65% | 99% | 99% | 99% | 100% |'
- en: '| Method / Budget | 256 | 512 | 1024 | 2048 | 4096 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 方法 / 预算 | 256 | 512 | 1024 | 2048 | 4096 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| H2O | 2% | 2% | 2% | 2% | 4% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| H2O | 2% | 2% | 2% | 2% | 4% |'
- en: '| TOVA | 2% | 2% | 2% | 2% | 10% |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| TOVA | 2% | 2% | 2% | 2% | 10% |'
- en: '| StreamingLLM | 1% | 1% | 1% | 2% | 4% |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| StreamingLLM | 1% | 1% | 1% | 2% | 4% |'
- en: '| Quest (ours) | 88% | 92% | 96% | 100% | 100% |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Quest (我们的) | 88% | 92% | 96% | 100% | 100% |'
- en: 'Table 1: (i) Results of 10k length passkey retrieval test on LongChat-7b-v1.5-32k.
    (ii) Results of 100k length passkey retrieval test on Yarn-Llama-2-7b-128k. Quest
    can achieve nearly perfect accuracy with 64 and 1024 tokens KV cache budget, which
    is about 1% of the total sequence length, demonstrating that Quest can effectively
    preserve the model’s ability to handle long-dependency tasks. However, KV cache
    eviction algorithms such as H2O, TOVA, and StreamingLLM incorrectly discard the
    KV cache of the answer before receiving the question, thus failing to achieve
    ideal accuracy.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '表1: (i) LongChat-7b-v1.5-32k上10k长度密码检索测试结果。 (ii) Yarn-Llama-2-7b-128k上100k长度密码检索测试结果。Quest可以在64和1024
    tokens的KV缓存预算下实现接近完美的准确性，这大约是总序列长度的1%，表明Quest可以有效保持模型处理长依赖任务的能力。然而，KV缓存驱逐算法如H2O、TOVA和StreamingLLM在收到问题之前错误地丢弃了答案的KV缓存，从而未能实现理想的准确性。'
- en: 4.1 Setting
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'We evaluate Quest on the language modeling dataset PG19 (Rae et al., [2019](#bib.bib19)),
    passkey retrieval task (Peng et al., [2023](#bib.bib18)), and six datasets in
    LongBench (Bai et al., [2023](#bib.bib2)): NarrativeQA (Kočiský et al., [2018](#bib.bib8)),
    HotpotQA (Yang et al., [2018](#bib.bib25)), Qasper (Dasigi et al., [2021](#bib.bib4)),
    TrivialQA (Joshi et al., [2017](#bib.bib7)), GovReport (Huang et al., [2021](#bib.bib6)),
    MultifieldQA (Bai et al., [2023](#bib.bib2)). We choose two widely used long-context
    models for our evaluation: LongChat-v1.5-7b-32k (Li et al., [2023](#bib.bib10))
    and Yarn-Llama-2-7b-128k (Peng et al., [2023](#bib.bib18)). We compare our method
    against the KV cache eviction algorithm H2O (Zhang et al., [2023b](#bib.bib28)),
    TOVA (Oren et al., [2024](#bib.bib17)), and StreamingLLM (Xiao et al., [2023](#bib.bib24)).
    Note that we do not apply any Quest and other baseline algorithms to the first
    two layers of the model, as our analysis in Sec [3.4](#S3.SS4 "3.4 Dynamically
    Estimating Token Criticality ‣ 3 Methodlogy ‣ Quest: Query-Aware Sparsity for
    Efficient Long-Context LLM Inference") indicates a low sparsity ratio for these
    layers.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在语言建模数据集PG19 (Rae et al., [2019](#bib.bib19))、密码检索任务 (Peng et al., [2023](#bib.bib18))
    和LongBench的六个数据集 (Bai et al., [2023](#bib.bib2)): NarrativeQA (Kočiský et al.,
    [2018](#bib.bib8))、HotpotQA (Yang et al., [2018](#bib.bib25))、Qasper (Dasigi et
    al., [2021](#bib.bib4))、TrivialQA (Joshi et al., [2017](#bib.bib7))、GovReport
    (Huang et al., [2021](#bib.bib6))、MultifieldQA (Bai et al., [2023](#bib.bib2))上评估了Quest。我们选择了两种广泛使用的长上下文模型进行评估：LongChat-v1.5-7b-32k
    (Li et al., [2023](#bib.bib10))和Yarn-Llama-2-7b-128k (Peng et al., [2023](#bib.bib18))。我们将我们的方法与KV缓存驱逐算法H2O
    (Zhang et al., [2023b](#bib.bib28))、TOVA (Oren et al., [2024](#bib.bib17)) 和StreamingLLM
    (Xiao et al., [2023](#bib.bib24))进行比较。请注意，我们没有将任何Quest和其他基线算法应用于模型的前两层，因为我们在Sec
    [3.4](#S3.SS4 "3.4 动态估计Token重要性 ‣ 3 方法 ‣ Quest: 适用于高效长上下文LLM推理的查询感知稀疏性")中的分析表明这些层的稀疏率较低。'
- en: 4.2 Accuracy Evaluation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 准确性评估
- en: 4.2.1 Language Modeling on PG19
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 PG19上的语言建模
- en: 'We first evaluate the language modeling perplexity on the PG19 test set, which
    is a dataset comprising 100 books with an average length of 70k tokens. We use
    the LongChat-7b-v1.5-32k model to test 32k tokens on PG19\. We feed the model
    with various numbers of tokens and evaluate the perplexity of generated tokens.
    We evaluate H2O, TOVA, and Quest with a token budget of 4096, which is approximately
    1/8 of the total token length. As indicated by the perplexity results in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.2.2 Results on long text passkey retrieval task ‣ 4.2 Accuracy Evaluation
    ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"),
    Quest’s accuracy closely matches the oracle baseline with a full KV cache.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在PG19测试集上评估语言建模的困惑度，该数据集包含100本书，平均长度为70k tokens。我们使用LongChat-7b-v1.5-32k模型在PG19上测试32k
    tokens。我们用不同数量的tokens输入模型，并评估生成tokens的困惑度。我们使用4096 tokens的预算评估H2O、TOVA和Quest，这大约是总token长度的1/8。如图[6](#S4.F6
    "图6 ‣ 4.2.2 长文本密码检索任务结果 ‣ 4.2 准确性评估 ‣ 4 实验 ‣ Quest: 适用于高效长上下文LLM推理的查询感知稀疏性")所示，Quest的准确度与完整的KV缓存的oracle基线接近匹配。'
- en: 4.2.2 Results on long text passkey retrieval task
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 长文本密码检索任务结果
- en: Since language modeling evaluation only involves local dependencies, models
    can achieve great performance by focusing on recent tokens. However, the ability
    to handle long-distance dependencies is crucial for long text reasoning. For KV
    cache eviction algorithms like H2O and TOVA, parts of KV caches that are important
    for distant future tokens may be discarded, thereby preventing the model from
    obtaining the correct answer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语言建模评估只涉及局部依赖，模型可以通过关注最近的标记来实现优异的性能。然而，处理远程依赖的能力对于长文本推理至关重要。对于像H2O和TOVA这样的KV缓存驱逐算法，可能会丢弃对远期标记重要的KV缓存部分，从而阻碍模型获得正确答案。
- en: '![Refer to caption](img/09ad0a3c2c4aad13fcb1ea8f4937e24c.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/09ad0a3c2c4aad13fcb1ea8f4937e24c.png)'
- en: 'Figure 6: Language modeling evaluation of Quest on PG19 dataset. We prompt
    the model with 0 to 32000 tokens from the PG19 test set and measure the perplexity
    of output tokens. H2O* and TOVA* indicate that for the first two layers of models,
    we do not apply these two algorithms to prune the KV Cache, as analyzed in Sec [3.4](#S3.SS4
    "3.4 Dynamically Estimating Token Criticality ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference"), which better preserves the
    model performance. Quest also uses a full cache in the first two layers of the
    model. Quest can closely match the performance of the full cache model.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '图6：Quest在PG19数据集上的语言建模评估。我们用0到32000个标记的PG19测试集来提示模型，并测量输出标记的困惑度。H2O*和TOVA*表示对于模型的前两层，我们没有应用这两种算法来修剪KV缓存，如第[3.4](#S3.SS4
    "3.4 Dynamically Estimating Token Criticality ‣ 3 Methodlogy ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference")节所分析，这样可以更好地保留模型性能。Quest还在模型的前两层中使用了完整的缓存。Quest的性能可以紧密匹配完整缓存模型的表现。'
- en: To show that Quest helps maintain the ability of models to handle longer dependency
    tasks, we evaluate it on the passkey retrieval task from Yarn (Peng et al., [2023](#bib.bib18)).
    This task measures a model’s ability to retrieve a simple passkey from a large
    amount of meaningless text. We put the answer in different depth ratios of the
    text and evaluate if the model can retrieve the correct answer with different
    KV cache token budgets. We evaluate LongChat-7b-v1.5-32k on 10k tokens test and
    Yarn-Llama-2-7b-128k on 100k tokens test.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示Quest如何帮助保持模型处理长距离依赖任务的能力，我们在Yarn的通行密钥检索任务上对其进行了评估（Peng等，[2023](#bib.bib18)）。该任务衡量模型从大量无意义文本中检索简单通行密钥的能力。我们将答案放置在文本的不同深度比例中，并评估模型是否能够在不同的KV缓存标记预算下检索正确的答案。我们在10k标记测试中评估了LongChat-7b-v1.5-32k，并在100k标记测试中评估了Yarn-Llama-2-7b-128k。
- en: Since H2O (Zhang et al., [2023b](#bib.bib28)) needs to calculate historical
    attention scores for KV cache pruning, it needs to compute the complete $O(n^{2})$
    attention map and thus is unable to use Flash-Attention (Dao et al., [2022](#bib.bib3))
    for long-context inference. Therefore, to enable H2O on long-context evaluation,
    we use Flash-Attention in the context stage for the 100k sequence length passkey
    retrieval test and start collecting historical attention scores for H2O in the
    decoding stage. For TOVA (Oren et al., [2024](#bib.bib17)) and StreamingLLM (Xiao
    et al., [2023](#bib.bib24)), we evaluated them on the 10k and 100k sequence lengths.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于H2O（Zhang等，[2023b](#bib.bib28)）需要计算历史注意力分数以进行KV缓存修剪，因此需要计算完整的$O(n^{2})$注意力图，因此无法在长上下文推理中使用Flash-Attention（Dao等，[2022](#bib.bib3)）。因此，为了在长上下文评估中启用H2O，我们在100k序列长度的通行密钥检索测试的上下文阶段使用Flash-Attention，并在解码阶段开始收集H2O的历史注意力分数。对于TOVA（Oren等，[2024](#bib.bib17)）和StreamingLLM（Xiao等，[2023](#bib.bib24)），我们在10k和100k序列长度下进行了评估。
- en: 'For the passkey retrieval test, we directly prefill the input text containing
    the passkey and texts to the model. However, to evaluate the impact of different
    methods on the model’s ability to handle long-dependency tasks in practical scenarios,
    we simulate decoding by feeding the task’s question and instruction to the model
    token by token. In this case, H2O and TOVA might mistakenly discard tokens critical
    for future tokens, such as the passkey that will be queried later. Similarly,
    StreamingLLM can only focus on the most recent text window, and if the passkey
    appears outside this window, it cannot provide the correct answer. Therefore,
    H2O, TOVA, and StreamingLLM cannot achieve ideal accuracy on the 10k and 100k
    length passkey retrieve test. However, Quest does not discard KV cache but instead
    uses a query-aware approach to identify critical tokens. As shown in Tab. [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for Efficient Long-Context
    LLM Inference"), Quest can achieve perfect accuracy with a minimal budget both
    on 10k and 100k sequence length tests.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '对于通行密钥检索测试，我们直接将包含通行密钥和文本的输入预填充到模型中。然而，为了评估不同方法对模型处理实际场景中长依赖任务能力的影响，我们通过逐个喂给任务的问题和指令来模拟解码。在这种情况下，H2O和TOVA可能会错误地丢弃对未来token至关重要的token，例如稍后将被查询的通行密钥。类似地，StreamingLLM只能关注最新的文本窗口，如果通行密钥出现在此窗口之外，它将无法提供正确的答案。因此，H2O、TOVA和StreamingLLM在10k和100k长度的通行密钥检索测试中无法达到理想的准确率。然而，Quest不会丢弃KV缓存，而是采用查询感知的方法来识别关键token。如表 [1](#S4.T1
    "Table 1 ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for Efficient Long-Context
    LLM Inference") 所示，Quest在10k和100k序列长度测试中均能以最小预算实现完美准确率。'
- en: 4.2.3 Results on LongBench
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 LongBench上的结果
- en: 'To validate that Quest can outperform baselines on general long-context datasets,
    we evaluate our method and baselines on six datasets in LongBench. We evaluate
    on LongChat-7b-v1.5-32k across a wide range of long-context datasets, including
    single-document QA: NarrativeQA, Qasper, MultiFieldQA; multi-document QA: HotpotQA;
    summarization: GovReport; few-shot learning: TriviaQA. We evaluate H2O, TOVA,
    StreamingLLM, and Quest with different KV cache budgets. For all datasets, we
    split the input into material and question/instruction. For the material part,
    we use Flash-Attention (Dao et al., [2022](#bib.bib3)) with the full KV cache
    to perform inference. For the question part, we simulate decoding by feeding them
    to the model token by token. Similar to the passkey retrieval test, to enable
    H2O to use Flash-Attention, we could not collect H2O’s historical attention scores
    during the context stage, thus starting from the decoding stage.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证Quest是否能够在通用长上下文数据集上超越基准模型，我们在LongBench的六个数据集上评估了我们的方法和基准模型。我们在LongChat-7b-v1.5-32k上评估了各种长上下文数据集，包括单文档问答：NarrativeQA、Qasper、MultiFieldQA；多文档问答：HotpotQA；摘要生成：GovReport；少样本学习：TriviaQA。我们使用不同的KV缓存预算评估了H2O、TOVA、StreamingLLM和Quest。对于所有数据集，我们将输入拆分为材料和问题/指令部分。对于材料部分，我们使用Flash-Attention
    (Dao et al., [2022](#bib.bib3)) 并利用完整的KV缓存进行推理。对于问题部分，我们通过逐个喂给模型的方式模拟解码。类似于通行密钥检索测试，为了使H2O能够使用Flash-Attention，我们不能在上下文阶段收集H2O的历史注意力得分，因此从解码阶段开始。
- en: As shown in the Fig. LABEL:fig:longbench, Quest consistently outperforms all
    baselines across six long-context datasets with various KV cache budgets. Quest
    with a budget of $1$K tokens can achieve comparable performance as the model with
    full KV cache, while other baselines still exhibit a notable gap from full cache
    performance even with a larger budget. After considering the full cache used in
    the first two layers, Quest can achieve lossless performance on Qasper, HotpotQA,
    GovReport, TriviaQA, NarrativeQA, and MultifieldQA with KV cache sparsity of 1/6,
    1/6, 1/5, 1/10, 1/5, and 1/6, respectively. This demonstrates that Quest is capable
    of maintaining the model’s capabilities across different types of long-context
    tasks, as it does not lead to the generation of incorrect answers due to improper
    discarding of KV cache.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 LABEL:fig:longbench 所示，Quest在六个长上下文数据集上在各种KV缓存预算下始终优于所有基准模型。使用$1$K tokens预算的Quest可以达到与完整KV缓存模型相当的性能，而其他基准模型即使在更大预算下也仍然与完整缓存性能存在显著差距。在考虑到前两层使用的完整缓存后，Quest在Qasper、HotpotQA、GovReport、TriviaQA、NarrativeQA和MultifieldQA上分别以1/6、1/6、1/5、1/10、1/5和1/6的KV缓存稀疏性实现了无损性能。这表明Quest能够在不同类型的长上下文任务中保持模型的能力，因为它不会由于不当丢弃KV缓存而生成错误答案。
- en: 4.3 Efficiency evaluation
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 效率评估
- en: 'To demonstrate the feasibility of Quest, we implement the entire framework
    with dedicated CUDA kernels based on FlashInfer (Ye et al., [2024](#bib.bib26)),
    a kernel library for LLM inference. We first evaluate Quest’s kernel-level efficiency
    under the configuration of Llama2-7B on an RTX4090 with CUDA 12.2 in Sec [4.3.1](#S4.SS3.SSS1
    "4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference"). Besides, we show
    the end-to-end speedup of Quest in text generation as shown in Sec [4.3.2](#S4.SS3.SSS2
    "4.3.2 End-to-End Evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference"). We compare Quest
    with a normal attention implementation from the original FlashInfer. To demonstrate
    the improvement, we qualitatively compare efficiency under the same accuracy between
    Quest and baselines in Sec [4.3.3](#S4.SS3.SSS3 "4.3.3 Comparison with Baselines
    ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for
    Efficient Long-Context LLM Inference"). Note that we use an Ada 6000 GPU (NVIDIA,
    [2023](#bib.bib13)) in end-to-end evaluations for longer context length.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示Quest的可行性，我们基于FlashInfer（Ye et al., [2024](#bib.bib26)），一个用于LLM推理的内核库，使用专用CUDA内核实现了整个框架。我们首先在第[4.3.1](#S4.SS3.SSS1
    "4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference")节中评估了Quest在RTX4090和CUDA
    12.2配置下的内核级效率。此外，我们在第[4.3.2](#S4.SS3.SSS2 "4.3.2 End-to-End Evaluation ‣ 4.3 Efficiency
    evaluation ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for Efficient Long-Context
    LLM Inference")节中展示了Quest在文本生成中的端到端加速。我们将Quest与原始FlashInfer的普通注意力实现进行了比较。为了展示改进效果，我们在第[4.3.3](#S4.SS3.SSS3
    "4.3.3 Comparison with Baselines ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣
    Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference")节中定性比较了Quest与基准之间在相同准确率下的效率。请注意，在端到端评估中，我们使用了Ada
    6000 GPU（NVIDIA，[2023](#bib.bib13)）以支持更长的上下文长度。'
- en: 4.3.1 Kernel evaluation
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 内核评估
- en: Due to the memory-bound nature of LLM inference, the speedup of Quest is proportional
    to the sparsity ratio (which is equivalent to memory movement reduction). We quantify
    this effect in Fig. LABEL:fig:kernel_efficiency, which evaluates per-kernel performance
    with NVIDIA’s benchmark tool NVBench (NVIDIA, [2024](#bib.bib14)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM推理的内存绑定特性，Quest的加速与稀疏度比（即内存移动减少）成正比。我们在图 LABEL:fig:kernel_efficiency 中量化了这一效果，该图使用NVIDIA的基准测试工具NVBench（NVIDIA，[2024](#bib.bib14)）评估每个内核的性能。
- en: Criticality estimation We evaluate the latency of criticality estimation in
    Quest under different sequence lengths and page sizes. At short sequence length,
    the memory bandwidth utilization of estimation is smaller than that of FlashInfer,
    as the total memory load size is not enough to fully utilize GPU memory bandwidth.
    As sequence length grows, the relative performance improves and approaches $1/\text{Page
    Size}$ since estimation only consumes one token per page. Note that techniques
    like quantization or larger page size can further reduce the additional memory
    usage.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 关键性估计 我们在不同序列长度和页面大小下评估了Quest中关键性估计的延迟。在短序列长度下，估计的内存带宽利用率低于FlashInfer，因为总内存负载大小不足以充分利用GPU内存带宽。随着序列长度的增加，相对性能提升并接近
    $1/\text{Page Size}$，因为估计每页仅消耗一个令牌。请注意，量化或更大页面大小等技术可以进一步减少额外的内存使用。
- en: '![Refer to caption](img/64e8843a4c6da3c23f5ec7b548d92439.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/64e8843a4c6da3c23f5ec7b548d92439.png)'
- en: 'Figure 12: Quest self-attention time breakdown compared to FlashInfer. At all
    sequence lengths, Quest significantly outperforms FlashInfer, as the memory movement
    is reduced. At sequence length $32$.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：Quest自注意力时间分解与FlashInfer的比较。在所有序列长度下，Quest都显著优于FlashInfer，因为内存移动减少。在序列长度为
    $32$ 时。
- en: '![Refer to caption](img/88776e32788ba03b740beb1b3713d29f.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/88776e32788ba03b740beb1b3713d29f.png)'
- en: 'Figure 13: End-to-end latency of Quest. For all sequence lengths, Quest significantly
    outperforms FlashInfer. Increasing the sequence lengths only slightly changes
    the latency of Quest. At a given sequence length, Quest’s latency slightly increases
    as the token budget grows. With sequence length $32$.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：Quest的端到端延迟。在所有序列长度下，Quest都显著优于FlashInfer。增加序列长度只会稍微改变Quest的延迟。在给定序列长度下，随着令牌预算的增加，Quest的延迟略有上升。在序列长度为
    $32$ 时。
- en: Top-K filtering We enable the Top-K filtering in Quest with a batched Top-K
    CUDA operator from a vector search kernel library RAFT (Zhang et al., [2023a](#bib.bib27)).
    We test the latency of Top-K filtering under different sequence lengths and token
    budgets. Since Criticality estimation reduces one entire token into one criticality
    score, Top-K filtering has limited memory movement compared to other operators,
    thus having a low latency overhead of 5-10 us for sequence length less than $128$k.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Top-K 过滤 我们通过来自向量搜索内核库 RAFT (Zhang et al., [2023a](#bib.bib27)) 的批量 Top-K CUDA
    操作符在 Quest 中启用 Top-K 过滤。我们测试了不同序列长度和令牌预算下 Top-K 过滤的延迟。由于 Criticality 估计将整个令牌减少为一个重要性评分，Top-K
    过滤与其他操作符相比内存移动有限，因此在序列长度小于 $128$k 的情况下具有 5-10 微秒的低延迟开销。
- en: Approximate attention Since Quest is compatible with PageAttention, approximate
    attention can be easily implemented by feeding Top-K page indices as sparse loading
    indices. We compare Quest’s approximate attention with the original attention
    of FlashInfer under different sequence lengths and token budgets with a $16$.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 近似注意力 由于 Quest 与 PageAttention 兼容，因此可以通过将 Top-K 页面索引作为稀疏加载索引来轻松实现近似注意力。我们在不同序列长度和令牌预算为
    $16$ 的情况下，将 Quest 的近似注意力与 FlashInfer 的原始注意力进行比较。
- en: 'We further evaluate Quest’s attention mechanism, which combines Criticality
    estimation,Top-K filtering, and Approximate attention, on the Llama2-7B model
    using the PyTorch profiler. We show the time breakdown of Quest in Fig. [12](#S4.F12
    "Figure 12 ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference") on various
    sequence lengths. Quest reduce the self-attention time by $7.03\times$ token budget.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步评估了 Quest 的注意力机制，该机制结合了 Criticality 估计、Top-K 过滤和近似注意力，并在使用 PyTorch profiler
    的 Llama2-7B 模型上进行评估。我们在图 Fig. [12](#S4.F12 "Figure 12 ‣ 4.3.1 Kernel evaluation
    ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity for
    Efficient Long-Context LLM Inference") 中展示了 Quest 在各种序列长度下的时间分解。Quest 将自注意力时间减少了
    $7.03\times$ 令牌预算。'
- en: '![Refer to caption](img/db64526567bf2e11f0bb8ead0a9a6a3f.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/db64526567bf2e11f0bb8ead0a9a6a3f.png)'
- en: 'Figure 14: Efficiency comparison of Quest with baselines under the same accuracy
    constraint. (a) Tokens budgets needed for comparable accuracy by different attention
    methods. Full denotes the original attention, which means the average context
    length of benchmarks. (b) Inference latency of different attention methods for
    comparable accuracy. Quest boosts $3.82\times$ speed on GovReport compared to
    TOVA.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：在相同准确性约束下，Quest 与基线的效率比较。(a) 不同注意力方法所需的令牌预算，以获得可比较的准确性。Full 表示原始注意力，意味着基准的平均上下文长度。(b)
    不同注意力方法在可比较准确性下的推理延迟。与 TOVA 相比，Quest 在 GovReport 上提升了 $3.82\times$ 的速度。
- en: 4.3.2 End-to-End Evaluation
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 端到端评估
- en: 'To show the practical speedup of Quest, we deploy the framework into real-world
    single-batch scenarios. We measure the average latency of generating one token
    in the decode stage under different sequence lengths and token budgets. Note that
    we do not measure the sampling process since its execution time is smaller and
    depends on the setting. We compare Quest with a full KV cache baseline which is
    implemented by FlashInfer. As shown in Fig. [13](#S4.F13 "Figure 13 ‣ 4.3.1 Kernel
    evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest: Query-Aware Sparsity
    for Efficient Long-Context LLM Inference"), Quest outperforms FlashInfer at all
    sequence lengths. The latency of Quest grows significantly slower than FlashInfer
    when the sequence length increases, as Quest maintains similar token budgets.
    At sequence length $32$ with 4-bit quantized weight.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示 Quest 的实际加速效果，我们将框架部署到实际的单批次场景中。我们测量了在不同序列长度和令牌预算下生成一个令牌的平均延迟。请注意，我们没有测量采样过程，因为其执行时间较短且依赖于设置。我们将
    Quest 与由 FlashInfer 实现的完整 KV 缓存基线进行比较。如图 Fig. [13](#S4.F13 "Figure 13 ‣ 4.3.1
    Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest: Query-Aware
    Sparsity for Efficient Long-Context LLM Inference") 所示，Quest 在所有序列长度下均优于 FlashInfer。当序列长度增加时，Quest
    的延迟增长显著低于 FlashInfer，因为 Quest 保持了类似的令牌预算。在序列长度为 $32$ 且权重量化为 4 位时。'
- en: 4.3.3 Comparison with Baselines
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 与基线的比较
- en: 'To demonstrate the performance improvements of Quest, we compare the inference
    efficiency of different attention mechanisms under the same accuracy constraint,
    i.e. lossless accuracy of six tasks from LongBench. We show token budgets needed
    for the lossless accuracy target by different attention mechanisms in Fig [14](#S4.F14
    "Figure 14 ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference")(a). For
    example, NarrativeQA exhibits an average context length of $24$K tokens leading
    to much higher sparsity.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示Quest的性能提升，我们比较了在相同准确度限制下，不同注意力机制的推断效率，即LongBench六项任务的无损准确度。我们在图 [14](#S4.F14
    "Figure 14 ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments
    ‣ Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference")(a) 中展示了不同注意力机制达到无损准确度目标所需的token预算。例如，NarrativeQA的平均上下文长度为$24$K
    tokens，导致更高的稀疏性。'
- en: 'However, none of the baselines included a kernel implementation of their proposed
    method. Consequently, we conduct a qualitative analysis of the baselines’ self-attention
    efficiency by utilizing the inference latency of FlashInfer, disregarding other
    runtime overheads (e.g., TOVA’s requirement to calculate history scores (Oren
    et al., [2024](#bib.bib17))). In contrast, Quest is evaluated in a practical setting
    with consideration of all operators. As shown in Fig. [14](#S4.F14 "Figure 14
    ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference")(b), Quest significantly
    surpasses all baselines in terms of self-attention latency due to the high query-aware
    sparsity. For GovReport and TriviaQA, Quest boosts the inference by $3.82\times$,
    respectively. Therefore, Quest can achieve higher efficiency while maintaining
    superior accuracy.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，没有一个基准包括其提出方法的内核实现。因此，我们利用FlashInfer的推断延迟对基准的自注意力效率进行定性分析，而忽略其他运行时开销（例如，TOVA计算历史分数的要求（Oren
    et al., [2024](#bib.bib17)））。相比之下，Quest在实际环境中评估，考虑了所有操作符。如图 [14](#S4.F14 "Figure
    14 ‣ 4.3.1 Kernel evaluation ‣ 4.3 Efficiency evaluation ‣ 4 Experiments ‣ Quest:
    Query-Aware Sparsity for Efficient Long-Context LLM Inference")(b) 所示，由于高度的查询感知稀疏性，Quest在自注意力延迟方面显著超越所有基准。对于GovReport和TriviaQA，Quest分别提升了$3.82\times$的推断效率。因此，Quest在保持优越准确性的同时，能够实现更高的效率。'
- en: 5 Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present Quest, an efficient and accurate KV cache selection algorithm that
    exploits query-aware sparsity. Quest dynamically estimates the criticality of
    tokens in KV cache based on the per-page metadata and the current query. It then
    performs self-attention only on the critical tokens with greatly reduced memory
    movement, providing high sparsity with negligible accuracy loss. Comprehensive
    evaluations demonstrate that Quest provides up to $7.03\times$ self-attention
    latency with the same accuracy target under long-context benchmarks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了Quest，一种高效且准确的KV缓存选择算法，利用查询感知稀疏性。Quest基于每页的元数据和当前查询动态估计KV缓存中token的重要性。然后，它仅对关键token执行自注意力操作，显著减少内存移动，提供高稀疏性且几乎没有准确度损失。全面评估表明，Quest在长上下文基准下，以相同准确度目标提供了高达$7.03\times$的自注意力延迟。
- en: Acknowledgements
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Zihao Ye for his insightful discussion, feedback, and useful advice
    on algorithm design and FlashInfer integration. This work was supported by generous
    gifts from Intel, Google, and the PRISM Research Center, a JUMP Center cosponsored
    by SRC and DARPA.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢**叶子豪**对算法设计和FlashInfer集成的深刻讨论、反馈和有用建议。本研究得到了来自Intel、Google以及由SRC和DARPA共同赞助的JUMP中心PRISM研究中心的慷慨资助。
- en: Impact Statement
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示的工作旨在推动机器学习领域的发展。我们的工作可能对社会产生多种潜在影响，但我们认为不需要在此特别强调。
- en: References
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anthropic (2024) Anthropic. Introducing the next generation of Claude. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family),
    2024. [Accessed 28-05-2024].
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024) Anthropic. 介绍下一代Claude。[https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family),
    2024. [访问日期：28-05-2024]。
- en: 'Bai et al. (2023) Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z.,
    Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench:
    A bilingual, multitask benchmark for long context understanding, 2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人（2023）Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z.,
    Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., 和 Li, J. Longbench：用于长上下文理解的双语多任务基准，2023。
- en: 'Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and Ré, C. Flashattention:
    Fast and memory-efficient exact attention with io-awareness, 2022.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等人（2022）Dao, T., Fu, D. Y., Ermon, S., Rudra, A., 和 Ré, C. Flashattention：快速且内存高效的精确注意力与
    io-awareness，2022。
- en: Dasigi et al. (2021) Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A.,
    and Gardner, M. A dataset of information-seeking questions and answers anchored
    in research papers, 2021.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasigi 等人（2021）Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., 和 Gardner,
    M. 一个基于研究论文的信息检索问题和答案数据集，2021。
- en: 'Ge et al. (2024) Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.
    Model tells you what to discard: Adaptive kv cache compression for llms, 2024.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等人（2024）Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., 和 Gao, J. 模型告诉你丢弃什么：适应性
    kv 缓存压缩用于 llms，2024。
- en: 'Huang et al. (2021) Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L.
    Efficient attentions for long document summarization. In *Proceedings of the 2021
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pp.  1419–1436, Online, June 2021\.
    Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112.
    URL [https://aclanthology.org/2021.naacl-main.112](https://aclanthology.org/2021.naacl-main.112).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等人（2021）Huang, L., Cao, S., Parulian, N., Ji, H., 和 Wang, L. 用于长文档摘要的高效注意力。在
    *2021年北美计算语言学协会年会论文集：人类语言技术*，第1419–1436页，在线，2021年6月。计算语言学协会。doi: 10.18653/v1/2021.naacl-main.112。URL
    [https://aclanthology.org/2021.naacl-main.112](https://aclanthology.org/2021.naacl-main.112)。'
- en: 'Joshi et al. (2017) Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. TriviaQA:
    A large scale distantly supervised challenge dataset for reading comprehension.
    In Barzilay, R. and Kan, M.-Y. (eds.), *Proceedings of the 55th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 
    1601–1611, Vancouver, Canada, July 2017\. Association for Computational Linguistics.
    doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joshi 等人（2017）Joshi, M., Choi, E., Weld, D., 和 Zettlemoyer, L. TriviaQA：一个大规模远程监督的阅读理解挑战数据集。在
    Barzilay, R. 和 Kan, M.-Y.（编辑），*第55届计算语言学协会年会论文集（第1卷：长篇论文）*，第1601–1611页，加拿大温哥华，2017年7月。计算语言学协会。doi:
    10.18653/v1/P17-1147。URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147)。'
- en: 'Kočiský et al. (2018) Kočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,
    K. M., Melis, G., and Grefenstette, E. The NarrativeQA reading comprehension challenge.
    *Transactions of the Association for Computational Linguistics*, 6:317–328, 2018.
    doi: 10.1162/tacl˙a˙00023. URL [https://aclanthology.org/Q18-1023](https://aclanthology.org/Q18-1023).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kočiský 等人（2018）Kočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.
    M., Melis, G., 和 Grefenstette, E. The NarrativeQA 阅读理解挑战。*计算语言学协会会刊*, 6:317–328,
    2018。doi: 10.1162/tacl˙a˙00023。URL [https://aclanthology.org/Q18-1023](https://aclanthology.org/Q18-1023)。'
- en: Kwon et al. (2023) Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H.,
    Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large
    language model serving with pagedattention. In *Proceedings of the ACM SIGOPS
    29th Symposium on Operating Systems Principles*, 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人（2023）Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H.,
    Gonzalez, J. E., Zhang, H., 和 Stoica, I. 用于大语言模型服务的高效内存管理与 pagedattention。在 *ACM
    SIGOPS 第29届操作系统原则研讨会论文集*，2023。
- en: Li et al. (2023) Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez,
    J. E., Stoica, I., Ma, X., and Zhang, H. How long can open-source llms truly promise
    on context length?, June 2023. URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J. E.,
    Stoica, I., Ma, X., 和 Zhang, H. 开源 llms 在上下文长度上的真正承诺有多长？2023年6月。URL [https://lmsys.org/blog/2023-06-29-longchat](https://lmsys.org/blog/2023-06-29-longchat)。
- en: Liu et al. (2024a) Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model
    on million-length video and language with blockwise ringattention, 2024a.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024a）Liu, H., Yan, W., Zaharia, M., 和 Abbeel, P. 百万长度视频和语言上的世界模型与块状环形注意力，2024a。
- en: Liu et al. (2024b) Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
    Scaling laws of rope-based extrapolation, 2024b.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024b）Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., 和 Lin, D. 基于绳索的外推缩放规律，2024b。
- en: NVIDIA (2023) NVIDIA. Nvidia ada lovelace professional gpu architecture. [https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf),
    2023. [Accessed 28-05-2024].
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA（2023）NVIDIA. Nvidia Ada Lovelace 专业 GPU 架构。 [https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf](https://images.nvidia.com/aem-dam/en-zz/Solutions/technologies/NVIDIA-ADA-GPU-PROVIZ-Architecture-Whitepaper_1.1.pdf)，2023年。[访问日期
    2024年5月28日]。
- en: 'NVIDIA (2024) NVIDIA. Nvbench: Nvidia’s benchmarking tool for gpus, 2024. Available
    online: [https://github.com/NVIDIA/nvbench](https://github.com/NVIDIA/nvbench).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA（2024）NVIDIA. Nvbench: Nvidia 的 GPU 基准测试工具，2024年。在线获取: [https://github.com/NVIDIA/nvbench](https://github.com/NVIDIA/nvbench)。'
- en: 'OpenAI (2023) OpenAI. New models and developer products announced at devday.
    [https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI](https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI),
    November 2023. Accessed: 2024-01-31.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. Devday 上宣布的新模型和开发者产品。 [https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI](https://openai.com/blog/new-models-and-developer-products-announced-at-devday#OpenAI)，2023年11月。访问日期：2024年1月31日。
- en: 'OpenAI (2024) OpenAI. Introducing gpt-4o: our fastest and most affordable flagship
    model. [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models),
    2024. [Accessed 28-05-2024].'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI（2024）OpenAI. 介绍 gpt-4o: 我们最快且最实惠的旗舰模型。 [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)，2024年。[访问日期
    2024年5月28日]。'
- en: Oren et al. (2024) Oren, M., Hassid, M., Adi, Y., and Schwartz, R. Transformers
    are multi-state RNNs, 2024. URL [https://arxiv.org/abs/2401.06104](https://arxiv.org/abs/2401.06104).
    arXiv:2401.06104.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oren 等（2024）Oren, M., Hassid, M., Adi, Y., 和 Schwartz, R. 变换器是多状态 RNN，2024年。网址
    [https://arxiv.org/abs/2401.06104](https://arxiv.org/abs/2401.06104)。arXiv:2401.06104。
- en: 'Peng et al. (2023) Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
    Efficient context window extension of large language models, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等（2023）Peng, B., Quesnelle, J., Fan, H., 和 Shippole, E. Yarn: 大型语言模型的高效上下文窗口扩展，2023年。'
- en: Rae et al. (2019) Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
    and Lillicrap, T. P. Compressive transformers for long-range sequence modelling.
    *arXiv preprint*, 2019. URL [https://arxiv.org/abs/1911.05507](https://arxiv.org/abs/1911.05507).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2019）Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., 和 Lillicrap,
    T. P. 压缩变换器用于长程序列建模。*arXiv 预印本*，2019年。网址 [https://arxiv.org/abs/1911.05507](https://arxiv.org/abs/1911.05507)。
- en: 'Ribar et al. (2023) Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C.,
    Luschi, C., and Orr, D. Sparq attention: Bandwidth-efficient llm inference, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribar 等（2023）Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi,
    C., 和 Orr, D. Sparq 注意力：带宽高效的 LLM 推理，2023年。
- en: 'Su et al. (2023) Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
    Roformer: Enhanced transformer with rotary position embedding, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su 等（2023）Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., 和 Liu, Y. Roformer:
    带有旋转位置嵌入的增强变换器，2023年。'
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation
    language models, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E., 和 Lample, G. Llama: 开放且高效的基础语言模型，2023年。'
- en: 'Tworkowski et al. (2023) Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
    Michalewski, H., and Miłoś, P. Focused transformer: Contrastive training for context
    scaling, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tworkowski 等（2023）Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski,
    H., 和 Miłoś, P. Focused transformer: 用于上下文扩展的对比训练，2023年。'
- en: Xiao et al. (2023) Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient
    streaming language models with attention sinks. *arXiv*, 2023.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2023）Xiao, G., Tian, Y., Chen, B., Han, S., 和 Lewis, M. 带有注意力汇的高效流式语言模型。*arXiv*，2023年。
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering, 2018.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等（2018）Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov,
    R., 和 Manning, C. D. Hotpotqa: 一个用于多跳问题解答的多样化、可解释数据集，2018年。'
- en: 'Ye et al. (2024) Ye, Z., Lai, R., Lu, R., Lin, C.-Y., Zheng, S., Chen, L.,
    Chen, T., and Ceze, L. Cascade inference: Memory bandwidth efficient shared prefix
    batch decoding. [https://flashinfer.ai/2024/01/08/cascade-inference.html](https://flashinfer.ai/2024/01/08/cascade-inference.html),
    Jan 2024. URL [https://flashinfer.ai/2024/01/08/cascade-inference.html](https://flashinfer.ai/2024/01/08/cascade-inference.html).
    Accessed on 2024-02-01.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2024) Ye, Z., Lai, R., Lu, R., Lin, C.-Y., Zheng, S., Chen, L.,
    Chen, T., 和 Ceze, L. Cascade inference: 内存带宽高效的共享前缀批量解码。 [https://flashinfer.ai/2024/01/08/cascade-inference.html](https://flashinfer.ai/2024/01/08/cascade-inference.html)，2024年1月。网址
    [https://flashinfer.ai/2024/01/08/cascade-inference.html](https://flashinfer.ai/2024/01/08/cascade-inference.html)。访问日期：2024-02-01。'
- en: 'Zhang et al. (2023a) Zhang, J., Naruse, A., Li, X., and Wang, Y. Parallel top-k
    algorithms on gpu: A comprehensive study and new methods. In *Proceedings of the
    International Conference for High Performance Computing, Networking, Storage and
    Analysis*, SC ’23, New York, NY, USA, 2023a. Association for Computing Machinery.
    ISBN 9798400701092. doi: 10.1145/3581784.3607062. URL [https://doi.org/10.1145/3581784.3607062](https://doi.org/10.1145/3581784.3607062).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) Zhang, J., Naruse, A., Li, X., 和 Wang, Y. 并行 top-k 算法在
    GPU 上: 综合研究与新方法。发表于 *国际高性能计算、网络、存储和分析会议论文集*，SC ’23，纽约，NY，USA，2023a。计算机协会。ISBN
    9798400701092。doi: 10.1145/3581784.3607062。网址 [https://doi.org/10.1145/3581784.3607062](https://doi.org/10.1145/3581784.3607062)。'
- en: 'Zhang et al. (2023b) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., Wang, Z., and Chen, B. H[2]o: Heavy-hitter
    oracle for efficient generative inference of large language models, 2023b.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023b) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., Wang, Z., 和 Chen, B. H[2]o: 高效生成推理的大型语言模型的重度命中预言机，2023b。'
- en: 'Zhao et al. (2024) Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng,
    S., Ceze, L., Krishnamurthy, A., Chen, T., and Kasikci, B. Atom: Low-bit quantization
    for efficient and accurate llm serving, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2024) Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng,
    S., Ceze, L., Krishnamurthy, A., Chen, T., 和 Kasikci, B. Atom: 低比特量化用于高效和准确的 llm
    服务，2024年。'
