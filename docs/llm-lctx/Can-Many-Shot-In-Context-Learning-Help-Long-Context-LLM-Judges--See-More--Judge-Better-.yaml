- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:02:50'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:02:50
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多次示例的上下文学习能否帮助长上下文 LLM 评估者？更多了解，判断更好！
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11629](https://ar5iv.labs.arxiv.org/html/2406.11629)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.11629](https://ar5iv.labs.arxiv.org/html/2406.11629)
- en: Mingyang Song, Mao Zheng, Xuan Luo
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 宋明阳，郑茂，罗璇
- en: Tencent Hunyuan
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 腾讯混元
- en: nickmysong@tencent.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: nickmysong@tencent.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Leveraging Large Language Models (LLMs) as judges for judging the performance
    of LLMs has recently garnered attention. However, this type of approach is affected
    by the potential biases in LLMs, raising concerns about the reliability of the
    evaluation results. To mitigate this issue, we propose and study two versions
    of many-shot in-context prompts, which rely on two existing settings of many-shot
    ICL for helping GPT-4o-as-a-Judge in single answer grading to mitigate the potential
    biases in LLMs, Reinforced ICL and Unsupervised ICL. Concretely, the former utilizes
    in-context examples with model-generated rationales, and the latter without. Based
    on the designed prompts, we investigate the impact of scaling the number of in-context
    examples on the consistency and quality of the judgment results. Furthermore,
    we reveal the symbol bias hidden in the pairwise comparison of GPT-4o-as-a-Judge
    and propose a simple yet effective approach to mitigate it. Experimental results
    show that advanced long-context LLMs, such as GPT-4o, perform better in the many-shot
    regime than in the zero-shot regime. Meanwhile, the experimental results further
    verify the effectiveness of the symbol bias mitigation approach. The code and
    data are released in [https://github.com/nick7nlp/SeeMoreJudgeBetter](https://github.com/nick7nlp/SeeMoreJudgeBetter).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 利用大语言模型（LLMs）作为评估 LLM 性能的评估者最近引起了关注。然而，这种方法受到 LLM 潜在偏见的影响，引发了对评估结果可靠性的担忧。为解决这一问题，我们提出并研究了两种多次示例的上下文提示版本，这些版本依赖于两种现有的多次示例
    ICL 设置，以帮助 GPT-4o 作为评估者进行单一答案评分，从而减轻 LLM 潜在的偏见，分别是强化 ICL 和无监督 ICL。具体而言，前者利用了包含模型生成的理由的上下文示例，而后者则没有。基于设计的提示，我们研究了在上下文示例数量增加的情况下，对判断结果的一致性和质量的影响。此外，我们揭示了
    GPT-4o 作为评估者的成对比较中隐藏的符号偏差，并提出了一种简单而有效的减轻方法。实验结果显示，先进的长上下文 LLM，如 GPT-4o，在多次示例模式下的表现优于零次示例模式。同时，实验结果进一步验证了符号偏差减轻方法的有效性。代码和数据已发布在
    [https://github.com/nick7nlp/SeeMoreJudgeBetter](https://github.com/nick7nlp/SeeMoreJudgeBetter)。
- en: Can Many-Shot In-Context Learning Help Long-Context LLM Judges?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多次示例的上下文学习能否帮助长上下文 LLM 评估者？
- en: See More, Judge Better!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更多了解，判断更好！
- en: Mingyang Song, Mao Zheng, Xuan Luo Tencent Hunyuan nickmysong@tencent.com
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 宋明阳，郑茂，罗璇 腾讯混元 nickmysong@tencent.com
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: LLMs such as GPT-4 OpenAI ([2023](#bib.bib16)), Gemini 1.5 Reid et al. ([2024](#bib.bib18)),
    and Claude3 Anthropic ([2024](#bib.bib2)) have demonstrated remarkable capabilities
    across a wide range of Natural Language Processing (NLP) tasks, becoming integral
    tools in various applications. The rapid advancement of LLMs Chowdhery et al.
    ([2023](#bib.bib7)) underscores the critical need to evaluate their alignment
    with human intent in generated responses. Therefore, evaluation has emerged as
    a crucial research area pivotal to the success of LLMs Chang et al. ([2023](#bib.bib5)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLM，如 GPT-4 OpenAI ([2023](#bib.bib16))，Gemini 1.5 Reid 等 ([2024](#bib.bib18))
    和 Claude3 Anthropic ([2024](#bib.bib2))，在广泛的自然语言处理（NLP）任务中展示了显著的能力，成为各种应用中的重要工具。LLM
    的快速发展 Chowdhery 等 ([2023](#bib.bib7)) 强调了评估它们生成响应与人类意图对齐的重要性。因此，评估已成为一个关键的研究领域，对
    LLM 的成功至关重要 Chang 等 ([2023](#bib.bib5))。
- en: LLMs like GPT-4 have shown exceptional performance across various tasks, leading
    to their wide adoption as both evaluators Wang et al. ([2023a](#bib.bib21)); Fu
    et al. ([2023a](#bib.bib9)); Wang et al. ([2023c](#bib.bib23)); Zheng et al. ([2023](#bib.bib27));
    Wang et al. ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6)) and annotators
    Peng et al. ([2023](#bib.bib17)). However, the reliability of LLMs as evaluators
    remains uncertain, given their sensitivity to textual instructions Xu et al. ([2023](#bib.bib25));
    [Turpin et al.](#bib.bib20) and potential judgment biases Wang et al. ([2023b](#bib.bib22));
    Zheng et al. ([2023](#bib.bib27)); Chen et al. ([2024](#bib.bib6)). To this end,
    researchers, such as Wang et al. ([2023b](#bib.bib22)), focus on proposing various
    methods to solve the potential biases that exist when LLMs act as judges, such
    as the positional bias.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-4 这样的LLM在各种任务中表现卓越，导致它们被广泛采用作为评估者 Wang et al. ([2023a](#bib.bib21))；Fu
    et al. ([2023a](#bib.bib9))；Wang et al. ([2023c](#bib.bib23))；Zheng et al. ([2023](#bib.bib27))；Wang
    et al. ([2023b](#bib.bib22))；Chen et al. ([2024](#bib.bib6)) 和注释员 Peng et al.
    ([2023](#bib.bib17))。然而，由于LLM对文本指令的敏感性 Xu et al. ([2023](#bib.bib25))；[Turpin
    et al.](#bib.bib20) 和潜在的判断偏见 Wang et al. ([2023b](#bib.bib22))；Zheng et al. ([2023](#bib.bib27))；Chen
    et al. ([2024](#bib.bib6))，LLM作为评估者的可靠性仍然不确定。为此，研究人员如 Wang et al. ([2023b](#bib.bib22))，专注于提出各种方法来解决LLM作为裁判时存在的潜在偏见，例如位置偏差。
- en: '![Refer to caption](img/3b41e07e8701ed94d6cc2ea4f187723d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3b41e07e8701ed94d6cc2ea4f187723d.png)'
- en: 'Figure 1: Consistency between different versions of judgment results by adopting
    GPT-4o as a zero-shot judge. $\hat{v}_{1}$, and $v_{3}$ vs. $v_{2}$ denotes the
    comparison between the three versions of evaluations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：通过采用 GPT-4o 作为零-shot 裁判的不同版本判断结果之间的一致性。$\hat{v}_{1}$ 和 $v_{3}$ 与 $v_{2}$
    的比较表示三种版本评估之间的比较。
- en: When human and LLM judges grade answers involving model-generated chain-of-thought
    rationales, consistently and precisely assigning the same score in 1-10 across
    multiple judgments is challenging, even if a reference correct answer is provided
    and each score in 1-10 has a particular and comprehensive description. Meanwhile,
    it is impossible to cover all situations in the descriptions of scores. Nonetheless,
    existing LLM-as-a-Judge approaches only adopt prompts with several aspects of
    constraints, which is undoubtedly very difficult because LLMs do not know exactly
    what kind of answer corresponds to each score.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类和LLM裁判评估涉及模型生成的链式思维推理的答案时，即使提供了参考正确答案，并且每个1-10分的描述是特定且全面的，在多个判断中一致而精确地分配相同的分数也是具有挑战性的。同时，评分描述中不可能涵盖所有情况。尽管如此，现有的LLM作为裁判的方法仅采用了具有几个方面约束的提示，这显然是非常困难的，因为LLM并不完全知道每个分数对应什么样的答案。
- en: 'Newly expanded context windows of LLMs allow researchers to investigate ICL
    with more shots than the zero-shot and few-shot regimes, namely many-shot ICL.
    To fully investigate the many-shot ICL, Agarwal et al. ([2024](#bib.bib1)) propose
    two settings of many-shot ICL (i.e., Reinforced ICL and Unsupervised ICL) to explore
    scaling in-context learning to hundreds or thousands of in-context examples and
    find performance improvements across multiple tasks. More importantly, they show
    that using the many-shot in-context examples with chain-of-thought rationales
    generated through the zero-shot regime is effective, and the many-shot ICL may
    overcome the pre-training biases of LLMs, whereas few-shot ICL struggles. Therefore,
    the intuitive idea is to use the many-shot ICL, allowing LLM-as-a-Judge to see
    the zero-shot evaluations of similar questions and answers first and then scoring
    examples before scoring. Therefore, an interesting question arises:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的新扩展上下文窗口允许研究人员调查比零-shot和少-shot方案更多的上下文学习，即多-shot ICL。为了全面调查多-shot ICL，Agarwal
    et al. ([2024](#bib.bib1)) 提出了两种多-shot ICL 设置（即，强化型 ICL 和无监督 ICL），以探索将上下文学习扩展到数百或数千个上下文示例，并在多个任务中发现性能提升。更重要的是，他们表明，使用通过零-shot
    方案生成的链式思维推理的多-shot 上下文示例是有效的，而且多-shot ICL 可能克服 LLM 的预训练偏差，而少-shot ICL 则面临困难。因此，直观的想法是使用多-shot
    ICL，使 LLM 作为裁判能够首先查看类似问题和答案的零-shot 评估，然后再评分。因此，出现了一个有趣的问题：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Can many-shot in-context learning help long-context LLM judges, such as GPT-4o?
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多次上下文学习是否能帮助像 GPT-4o 这样的长上下文 LLM 裁判？
- en: Motivated by prior findings and the above issue, we verify the consistency of
    the widely used prompts of LLM-as-a-Judge Zheng et al. ([2023](#bib.bib27)), as
    shown in Table [3](#A1.T3 "Table 3 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can Many-Shot
    In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!"). Concretely,
    the consistency experiments are based on the entire test set of GSM8K Cobbe et al.
    ([2021](#bib.bib8)), where the inference answers are obtained based on LLaMA3-70B
    (The pipeline as illustrated as in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") and details in $\S$ [3](#S3 "3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!")). Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!") presents that the consistency between the
    two versions of evaluations is low, with nearly half of the ratings being inconsistent.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 受先前发现和上述问题的启发，我们验证了广泛使用的LLM-as-a-Judge提示的一致性，如表[3](#A1.T3 "Table 3 ‣ A.3 Dataset
    ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context LLM
    Judges? See More, Judge Better!")所示。具体而言，一致性实验基于GSM8K Cobbe等人（[2021](#bib.bib8)）的整个测试集，其中推理答案基于LLaMA3-70B获得（如图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!")所示，详细信息见$\S$ [3](#S3 "3 Experiments ‣ Can
    Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!")）。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!")显示，两种版本的评估之间的一致性较低，近一半的评分不一致。
- en: Inspired by the above phenomena and issues, in this paper, we investigate whether
    many-shot in-context learning helps long-context LLM judges. Motivated by Agarwal
    et al. ([2024](#bib.bib1)), we introduce two versions of many-shot ICL prompts
    via Reinforced ICL and Unsupervised ICL for LLM-as-a-Judge. The former utilizes
    in-context examples with model-generated rationales, and the latter removes rationales
    used in the former, details in $\S$ [A.1](#A1.SS1 "A.1 Many-Shot Unsupervised
    ICL ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!"). Meanwhile, we reveal a novel symbol bias
    in GPT-4o-as-a-Judge and explore a simple approach for mitigating this issue.
    Experiments show that many-shot ICL can help GPT-4o-as-a-Jduge obtain higher quality
    and consistency evaluation results. As the number of in-context examples increases,
    the quality and consistency of evaluation improves significantly. Furthermore,
    we further verify the effectiveness of the proposed simple yet effective approach
    for mitigating the symbol bias in pairwise comparison of GPT-4o-as-a-Jduge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 受到上述现象和问题的启发，在本文中，我们调查了多次示例上下文学习是否有助于长上下文LLM评判者。受到Agarwal等人（[2024](#bib.bib1)）的启发，我们通过强化ICL和无监督ICL为LLM-as-a-Judge引入了两个版本的多次示例ICL提示。前者利用包含模型生成理由的上下文示例，后者则去除了前者使用的理由，详细信息见$\S$
    [A.1](#A1.SS1 "A.1 Many-Shot Unsupervised ICL ‣ Appendix A Appendix ‣ Can Many-Shot
    In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!")。同时，我们揭示了GPT-4o-as-a-Judge中的一种新型符号偏差，并探索了一种简单的解决方案来缓解这一问题。实验表明，多次示例ICL可以帮助GPT-4o-as-a-Judge获得更高质量和一致性的评估结果。随着上下文示例数量的增加，评估质量和一致性显著提高。此外，我们进一步验证了所提出的简单而有效的方法在缓解GPT-4o-as-a-Judge成对比较中的符号偏差方面的有效性。
- en: 'In summary, our main contributions can be summarized as the following three-fold:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献可以总结为以下三点：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Firstly, we propose investigating many-shot ICL to help long-context LLM judges,
    such as GPT-4o. To the best of our knowledge, we are the first to explore and
    introduce many-shot in-context learning to assist long-context LLMs as judges
    in single answer grading of GPT-4o-as-a-Judge and scale the number of in-context
    examples to verify the effectiveness.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们提出调查多次示例的ICL，以帮助长上下文LLM评判者，如GPT-4o。据我们所知，我们是首个探索并引入多次示例上下文学习，以协助长上下文LLM作为GPT-4o-as-a-Judge的评判者并扩大上下文示例数量以验证其有效性的研究。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Second, inspired by the Counting-Stars benchmark Song et al. ([2024](#bib.bib19)),
    we reveal a novel potential bias in pairwise comparison of GPT-4o-as-a-Judge,
    namely symbol bias, and propose a simple yet effective approach to mitigate it.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，受到Song等人（[2024](#bib.bib19)）的Counting-Stars基准的启发，我们揭示了GPT-4o-as-a-Judge在成对比较中的一种新的潜在偏差，即符号偏差，并提出了一种简单而有效的解决方法来缓解这一问题。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Finally, through experimental results, many-shot ICL can help GPT-4o-as-a-Jduge
    obtain higher quality and consistency evaluation results. Meanwhile, we demonstrate
    the effectiveness of the symbol bias mitigation method.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，通过实验结果，多样本 ICL 可以帮助 GPT-4o 作为评审获得更高质量和一致性的评估结果。同时，我们展示了符号偏差缓解方法的有效性。
- en: '![Refer to caption](img/552ddaed0fbb3bc6345fcafef0dae7aa.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/552ddaed0fbb3bc6345fcafef0dae7aa.png)'
- en: 'Figure 2: The pipeline of the experiments.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：实验流程图。
- en: '![Refer to caption](img/a52535dc54c7abd5e3da2a46e0653e9a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a52535dc54c7abd5e3da2a46e0653e9a.png)'
- en: 'Figure 3: Consistency between two versions of judgment results. Concretely,
    the bar corresponding to "0" on the x-axis represents the number of samples with
    consistent and inconsistent ratings in comparing evaluation results obtained twice
    using GPT-4o as the judge in the zero-shot regime. In addition, the zero-shot
    generated rationales are used for Reinforced ICL and appended to the prompt for
    Unsupervised ICL. The bar corresponding to "$2^{n}$" on the x-axis represents
    the consistency of using the GPT-4o as a judge in the many-shot Reinforced ICL
    regime.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：两个版本的判断结果的一致性。具体来说，x 轴上对应“0”的柱子代表在零样本条件下使用 GPT-4o 作为评审时，两次评估结果中一致和不一致的样本数量。此外，零样本生成的理由用于强化
    ICL，并附加到无监督 ICL 的提示中。x 轴上对应“$2^{n}$”的柱子代表在多样本强化 ICL 条件下使用 GPT-4o 作为评审的一致性。
- en: 2 GPT-4o as A Long-Context LLM Judge
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 GPT-4o 作为长上下文 LLM 评审
- en: In this section, we briefly describe the background of many-shot ICL, recall
    judgment biases, and introduce two variants of designed prompts for GPT-4o-as-a-Judge
    in this study.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们简要描述多样本 ICL 的背景，回顾判断偏差，并介绍本研究中为 GPT-4o 作为评审设计的两种变体提示。
- en: 2.1 Background of Many-Shot ICL
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多样本 ICL 的背景
- en: LLMs excel at few-shot in-context learning, which involves learning from a few
    input-output demonstrations (“shots”) provided in context at inference without
    weight updates Brown et al. ([2020](#bib.bib3)). Newly expanded long-context LLMs
    allow us to investigate ICL with hundreds of in-context examples Li et al. ([2023](#bib.bib12));
    Agarwal et al. ([2024](#bib.bib1)). Due to some limitations, we only examine GPT-4o-as-a-Judge
    in a many-shot regime in this paper.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 在少样本上下文学习中表现出色，这涉及从推理过程中提供的少量输入输出示例（“样本”）中学习，而无需权重更新 Brown 等 ([2020](#bib.bib3))。新扩展的长上下文
    LLM 使我们能够研究包含数百个上下文示例的 ICL Li 等 ([2023](#bib.bib12)); Agarwal 等 ([2024](#bib.bib1))。由于一些限制，本文仅在多样本条件下考察
    GPT-4o 作为评审。
- en: 2.2 Recalling Judgment Biases
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 记忆判断偏差
- en: LLM judges possess potential biases, which have been widely explored Wang et al.
    ([2023b](#bib.bib22)); Wu and Aji ([2023](#bib.bib24)); Zheng et al. ([2023](#bib.bib27));
    Chen et al. ([2024](#bib.bib6)). For example, positional bias refers to the phenomenon
    where, during pairwise comparisons, LLM judges tend to favor one side of a pair
    regardless of the actual quality of the answers. In this paper, we investigate
    leveraging many-shot ICL to help GPT-4o as a better Judge, maybe through reducing
    judgment biases.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 评审可能存在潜在偏差，这已被广泛探讨 Wang 等 ([2023b](#bib.bib22)); Wu 和 Aji ([2023](#bib.bib24));
    Zheng 等 ([2023](#bib.bib27)); Chen 等 ([2024](#bib.bib6))。例如，位置偏差指的是在配对比较过程中，LLM
    评审倾向于偏向配对的一侧，而不考虑答案的实际质量。本文研究利用多样本 ICL 来帮助 GPT-4o 成为更好的评审，可能通过减少判断偏差。
- en: 2.3 Two GPT-4o-as-a-Judge Variations
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 两种 GPT-4o 作为评审的变体
- en: 'Inspired by Zheng et al. ([2023](#bib.bib27)), we introduce two GPT-4o-as-a-Judge
    variations: single answer grading and pairwise comparison.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 Zheng 等 ([2023](#bib.bib27)) 启发，我们引入了两种 GPT-4o 作为评审的变体：单个答案评分和配对比较。
- en: 2.3.1 Single Answer Grading
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 单个答案评分
- en: In this variation, the GPT-4o Judge is required to assign a score with a reason
    to a single answer directly. Inspired by the recent work Agarwal et al. ([2024](#bib.bib1)),
    we adopt the Reinforced ICL, which uses model-generated rationales as the in-context
    examples. Meanwhile, we also design the prompt within the Unsupervised ICL. The
    4-shot Reinforced ICL and 1-shot Unsupervised ICL append 4-shots with prompts
    are presented in Table [1](#A1.T1 "Table 1 ‣ A.3 Dataset ‣ Appendix A Appendix
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") and Table [5](#A1.T5 "Table 5 ‣ A.3 Dataset ‣ Appendix A Appendix ‣
    Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!").
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在此变体中，GPT-4o 判断器需要直接为单一答案分配分数并给出理由。受 Agarwal 等人（[2024](#bib.bib1)）近期工作的启发，我们采用了强化
    ICL 方法，该方法使用模型生成的理由作为上下文示例。同时，我们也设计了无监督 ICL 中的提示。4-shot 强化 ICL 和 1-shot 无监督 ICL
    追加的 4-shot 提示见表[1](#A1.T1 "表 1 ‣ A.3 数据集 ‣ 附录 A 附录 ‣ 多重上下文学习是否能帮助长上下文 LLM 判断？了解更多，判断更好！")
    和表[5](#A1.T5 "表 5 ‣ A.3 数据集 ‣ 附录 A 附录 ‣ 多重上下文学习是否能帮助长上下文 LLM 判断？了解更多，判断更好！")。
- en: 2.3.2 Pairwise Comparison
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 成对比较
- en: The GPT-4o Judge is presented with a question and two answers and tasked with
    determining which is better or declaring a tie. The prompt used for this scenario
    is given in Table [3](#A1.T3 "Table 3 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can
    Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!").
    In this work, we adopt the pairwise comparison of GPT-4o-as-a-Judge to evaluate
    the quality of the GPT-4o judgments results in the zero-shot regime.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4o 判断器会收到一个问题和两个答案，并负责确定哪个更好或宣布平局。用于这种场景的提示见表[3](#A1.T3 "表 3 ‣ A.3 数据集 ‣
    附录 A 附录 ‣ 多重上下文学习是否能帮助长上下文 LLM 判断？了解更多，判断更好！")。在这项工作中，我们采用了 GPT-4o 作为判断器的成对比较方法来评估
    GPT-4o 判断结果的质量，处于零-shot 模式。
- en: '![Refer to caption](img/142ccb7534f9b2d9f47f61022785c0fe.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/142ccb7534f9b2d9f47f61022785c0fe.png)'
- en: 'Figure 4: Consistency between two versions of judgment results.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：两个版本的判断结果的一致性。
- en: '![Refer to caption](img/fca1badbaf70b719df9d5e91cddef560.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fca1badbaf70b719df9d5e91cddef560.png)'
- en: 'Figure 5: Consistency between two versions of judgment results.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：两个版本的判断结果的一致性。
- en: '![Refer to caption](img/d1faa874689a6fbac8e12dd1ac835da5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1faa874689a6fbac8e12dd1ac835da5.png)'
- en: 'Figure 6: Consistency between two versions of judgment results.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：两个版本的判断结果的一致性。
- en: 3 Experiments
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: In this section, we introduce the experimental settings, consistency evaluation
    in single answer grading of GPT-4o-as-a-Jduge, quality evaluation via pairwise
    comparison of GPT-4o-as-a-Jduge, and reveal the symbol bias in GPT-4o-as-a-Jduge.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了实验设置，GPT-4o 作为判断器的单一答案评分一致性评估，通过成对比较评估 GPT-4o 作为判断器的质量，并揭示 GPT-4o
    作为判断器的符号偏见。
- en: 3.1 Experimental Settings
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: To analyze the GPT-4o judge in the many-shot regime, we use LLaMA3-70B to generate
    answers for each question in GSM8K Cobbe et al. ([2021](#bib.bib8)) with a temperature
    of 0.7\. Details of the used benchmark are provided in $\S$ examples (all examples
    in the training set).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析 GPT-4o 在多重上下文模式下的表现，我们使用 LLaMA3-70B 为 GSM8K Cobbe 等人（[2021](#bib.bib8)）中的每个问题生成答案，温度设置为
    0.7。所用基准的详细信息见 $\S$ 示例（训练集中的所有示例）。
- en: Furthermore, inspired by Counting-Stars Song et al. ([2024](#bib.bib19)), even
    if too many shots are provided, the GPT-4o may not be able to utilize all of them.
    Because, in the Counting-Stars benchmark, when the number of pieces of evidence
    reaches 32, long-context LLMs (e.g., GPT-4 Turbo, Gemini 1.5 Pro, and Claude3
    Opus) may no longer accurately obtain all of them. Therefore, adding more many-shot
    in-context examples is probably not captured by LLMs for learning as a reference
    to judge. However, the many-shot regime and Counting-Stars are substantially different,
    so there is no noise from "haystack". Hence, we set the maximum number of the
    in-context examples to 128, i.e., 128-shots.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，受 Counting-Stars Song 等人（[2024](#bib.bib19)）的启发，即使提供了过多的示例，GPT-4o 可能也无法利用全部。这是因为在
    Counting-Stars 基准中，当证据数量达到 32 时，长上下文 LLM（例如 GPT-4 Turbo、Gemini 1.5 Pro 和 Claude3
    Opus）可能无法准确获取所有证据。因此，增加更多的多重上下文示例可能不会被 LLM 作为判断的参考所学习。然而，多重上下文模式和 Counting-Stars
    实质上不同，因此不会有来自“干草堆”的噪声。因此，我们将上下文示例的最大数量设置为 128，即 128-shot。
- en: In the experiments, we use GPT-4o with public API access, and the specific endpoint
    is “gpt-4o-2024-05-13”. In addition, for the convenience of introduction, when
    comparing zero-shot and many-shot regimes, we uniformly refer to the few-shot
    and many-shot regimes as the many-shot regime.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用了具有公共 API 访问权限的 GPT-4o，具体端点为“gpt-4o-2024-05-13”。此外，为了便于介绍，在比较零-shot
    和多-shot 状态时，我们统一将少-shot 和多-shot 状态称为多-shot 状态。
- en: '![Refer to caption](img/8e03fdbe463a16f349b668ad0a10260f.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8e03fdbe463a16f349b668ad0a10260f.png)'
- en: 'Figure 7: Win rate compared between zero-shot and many-shot regimes. Here,
    A and A^† represent the results of the GPT-4o judge in the many-shot regime, and
    B and B^† represent the results of the GPT-4o judge in the zero-shot regime. The
    designed prompts are shown in Table [2](#A1.T2 "Table 2 ‣ A.3 Dataset ‣ Appendix
    A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See
    More, Judge Better!"). Notice that "[[C]]" denotes a tie.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：零-shot 和多-shot 状态下的胜率比较。这里，A 和 A^† 代表 GPT-4o 评判者在多-shot 状态下的结果，B 和 B^† 代表
    GPT-4o 评判者在零-shot 状态下的结果。设计的提示见表 [2](#A1.T2 "表 2 ‣ A.3 数据集 ‣ 附录 A 附录 ‣ 多-shot
    上下文学习是否能帮助长上下文 LLM 评判者？更多见，判断更好！")。请注意，"[[C]]" 表示平局。
- en: '![Refer to caption](img/ca86ccba9d7dcfe0bb3f594f20a5d252.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ca86ccba9d7dcfe0bb3f594f20a5d252.png)'
- en: 'Figure 8: Comparison evaluation results after mitigating the biases via different
    approaches.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：通过不同方法缓解偏见后的比较评估结果。
- en: 3.2 Consistency Evaluation
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 一致性评估
- en: We investigate the consistency between different versions of evaluation results
    generated by the GPT-4o judge. Here, the single answer grading evaluation results
    can be used to compare the consistency between different versions. As shown in
    Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!"), the bar corresponding
    to "$2^{n}$" on the x-axis represents the number of samples with consistent and
    inconsistent ratings in comparing evaluation results obtained twice using GPT-4o-as-a-Judge
    in the many-shot Reinforced ICL regime. From the results, consistency improves
    as we increase the number of shots provided as in-context examples during inference.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了不同版本的 GPT-4o 评判者生成的评估结果之间的一致性。这里，单一答案评分评估结果可以用来比较不同版本之间的一致性。如图 [3](#S1.F3
    "图 3 ‣ 1 引言 ‣ 多-shot 上下文学习是否能帮助长上下文 LLM 评判者？更多见，判断更好！") 所示，x 轴上对应 "$2^{n}$" 的条形图表示使用
    GPT-4o 作为评判者在多-shot 强化 ICL 状态下两次获得的评估结果中一致和不一致评分的样本数量。从结果来看，一致性随着在推理过程中提供的上下文示例数量的增加而提高。
- en: Recent studies Wang et al. ([2023b](#bib.bib22)); Zheng et al. ([2023](#bib.bib27));
    Chen et al. ([2024](#bib.bib6)) have shown that the judgment performance of GPT-4
    as a judge is highly in agreement with those of humans. However, both human and
    LLM judgments are subject to potential biases Chen et al. ([2024](#bib.bib6)).
    By analyzing prior studies, we suppose it unnecessary to obtain utterly accurate
    evaluation results (because this is difficult) by using LLMs as judges. It is
    only required to ensure that the evaluation results are highly consistent multiple
    times so that the single answer grading of GPT-4o-as-a-Judge may be effective.
    From all results in this work, we find that the many-shot ICL examples help the
    judgment of LLMs more consistently, which is essential. We consider that the main
    reason may be that the many-shot in-context examples mitigate the potential biases
    of the GPT-4o judge in the zero-shot regime.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 Wang 等人 ([2023b](#bib.bib22))；Zheng 等人 ([2023](#bib.bib27))；Chen 等人 ([2024](#bib.bib6))
    表明，GPT-4 作为评判者的判断表现与人类的判断高度一致。然而，人类和 LLM 的判断都可能受到潜在偏见的影响 Chen 等人 ([2024](#bib.bib6))。通过分析以往的研究，我们认为通过使用
    LLM 作为评判者来获得完全准确的评估结果是不必要的（因为这很困难）。只需要确保评估结果多次高度一致，以便 GPT-4o 作为评判者的单一答案评分可能是有效的。从本研究的所有结果来看，我们发现多-shot
    ICL 示例有助于 LLM 的判断更加一致，这一点至关重要。我们认为主要原因可能是多-shot 上下文示例缓解了 GPT-4o 评判者在零-shot 状态下的潜在偏见。
- en: Meanwhile, we also implemented experiments in the many-shot Unsupervised ICL,
    but the results show that this regime may be unsuitable in the scenario of acting
    as a judge because the problem of each sample is the same, but the scored questions
    and answers are different. From the results in Figure [4](#S2.F4 "Figure 4 ‣ 2.3.2
    Pairwise Comparison ‣ 2.3 Two GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context
    LLM Judge ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See
    More, Judge Better!"), Figure [5](#S2.F5 "Figure 5 ‣ 2.3.2 Pairwise Comparison
    ‣ 2.3 Two GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context LLM Judge
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!"), and Figure [6](#S2.F6 "Figure 6 ‣ 2.3.2 Pairwise Comparison ‣ 2.3 Two
    GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context LLM Judge ‣ Can Many-Shot
    In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!"), it
    can be seen that the consistency corresponds to the number of appended in-context
    samples with model-generated rationales but nearly not to the number of in-context
    examples without model-generated rationales.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们也在多-shot无监督ICL中进行了实验，但结果显示，这种模式可能不适合作为评审，因为每个样本的问题是相同的，但评分的问题和答案却不同。从图[4](#S2.F4
    "Figure 4 ‣ 2.3.2 Pairwise Comparison ‣ 2.3 Two GPT-4o-as-a-Judge Variations ‣
    2 GPT-4o as A Long-Context LLM Judge ‣ Can Many-Shot In-Context Learning Help
    Long-Context LLM Judges? See More, Judge Better!")、图[5](#S2.F5 "Figure 5 ‣ 2.3.2
    Pairwise Comparison ‣ 2.3 Two GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context
    LLM Judge ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See
    More, Judge Better!")和图[6](#S2.F6 "Figure 6 ‣ 2.3.2 Pairwise Comparison ‣ 2.3
    Two GPT-4o-as-a-Judge Variations ‣ 2 GPT-4o as A Long-Context LLM Judge ‣ Can
    Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!")中的结果可以看出，一致性与模型生成的理由附加的上下文样本数量相对应，但几乎与没有模型生成理由的上下文示例数量无关。
- en: In addition, both the few-shot and many-shot regimes are sensitive to the selection
    and order of in-context examples. In the experimental setting of this paper, almost
    no test data will have the same in-context examples. Even under this condition,
    the evaluation results also show a high consistency, demonstrating the effectiveness
    of many-shot ICL in helping GPT-4o-as-a-Judge.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，无论是少量示例还是大量示例的模式，都对上下文示例的选择和顺序很敏感。在本文的实验设置中，几乎没有测试数据会有相同的上下文示例。即便在这种条件下，评估结果也显示出高度的一致性，证明了大量示例的ICL在帮助GPT-4o作为评审时的有效性。
- en: 3.3 Quality Evaluation
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 质量评估
- en: 'After the consistency evaluation, an important question arises about: Does
    a high consistency refer to high-quality judgments? Around this issue, we pair-wise
    compare model-generated judgment rationales between zero-shot and many-shot regimes
    using the designed prompts shown in Table [2](#A1.T2 "Table 2 ‣ A.3 Dataset ‣
    Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context LLM
    Judges? See More, Judge Better!").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在一致性评估之后，出现了一个重要问题：高一致性是否意味着高质量的判断？围绕这个问题，我们使用表格[2](#A1.T2 "Table 2 ‣ A.3 Dataset
    ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context LLM
    Judges? See More, Judge Better!")中所示的设计提示，对零-shot和多-shot模式下模型生成的判断理由进行了成对比较。
- en: From Figure [7](#S3.F7 "Figure 7 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") Compare(A, B), we can find that the evaluations obtained in the many-shot
    regime are significantly better than those in the zero-shot regime. However, as
    mentioned before, the positional bias of GPT-4o-as-a-Judge may cause these results,
    so we performed a second comparison by swapping the positions. As shown in Figure [7](#S3.F7
    "Figure 7 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!") Compare(B, A),
    we observe that as in-context examples increase, the judgment results in the many-shot
    regime gradually turn around, that is, a higher winning rate. To obtain fairness
    results, we integrate the above two results (Compare(A, B) and Compare(B, A)),
    as shown in Figure [8](#S3.F8 "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!"). It can be seen that after mitigating the positional bias, the judgment
    quality in the many-shot regime is still better than that in the zero-shot regime.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从图[7](#S3.F7 "Figure 7 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot
    In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!") Compare(A,
    B) 中，我们可以发现多样本方案中的评估结果显著优于零样本方案。然而，如前所述，GPT-4o 作为评判者的位置偏差可能导致这些结果，因此我们通过交换位置进行了第二次比较。如图[7](#S3.F7
    "Figure 7 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!") Compare(B, A)
    所示，我们观察到随着上下文示例的增加，多样本方案中的判断结果逐渐逆转，即胜率更高。为了获得公平的结果，我们整合了上述两个结果（Compare(A, B) 和
    Compare(B, A)），如图[8](#S3.F8 "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") 所示。可以看出，经过减轻位置偏差后，多样本方案中的判断质量仍然优于零样本方案。
- en: 3.4 Revealing Symbol Bias
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 揭示符号偏差
- en: 'Inspired by Song et al. ([2024](#bib.bib19)), LLMs revealed a preference for
    specific symbols under stress testing (details are described in the Appendix).
    Therefore, an interesting question arises about: Does the GPT-4o prefer to choose
    the answer with the symbol A rather than B? To verify this conjecture, we swap
    the answers corresponding to symbols A and B, as shown in Figure [2](#A1.T2 "Table
    2 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help
    Long-Context LLM Judges? See More, Judge Better!"). From Figure [7](#S3.F7 "Figure
    7 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!") Compare(A^†, B^†) and
    Compare(B^†, A^†), it can be seen that the results are different from the above
    experiments. Actually, the results of Compare(A, B) and Compare(A^†, B^†) should
    be similar, and the results of Compare(B, A) and Compare(B^†, A^†) should be similar.
    This phenomenon shows that symbol bias does exist when adopting GPT-4o-as-a-Judge.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 受宋等人（[2024](#bib.bib19)）的启发，LLMs 在压力测试中展示了对特定符号的偏好（详细信息见附录）。因此，一个有趣的问题出现了：GPT-4o
    是否更倾向于选择符号 A 的答案而不是 B？为了验证这一猜想，我们交换了符号 A 和 B 对应的答案，如图[2](#A1.T2 "Table 2 ‣ A.3
    Dataset ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!")所示。从图[7](#S3.F7 "Figure 7 ‣ 3.1 Experimental
    Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context Learning Help Long-Context
    LLM Judges? See More, Judge Better!")中的 Compare(A^†, B^†) 和 Compare(B^†, A^†)
    可以看出，结果与上述实验不同。实际上，Compare(A, B) 和 Compare(A^†, B^†) 的结果应该类似，而 Compare(B, A) 和
    Compare(B^†, A^†) 的结果也应该类似。这一现象表明，在采用 GPT-4o 作为评判者时确实存在符号偏差。
- en: Recent research Wang et al. ([2023b](#bib.bib22)) integrates the evaluation
    results of Compare(A, B) and Compare(B, A) to mitigate the positional bias, which
    motivates us to incorporate the evaluation results of Compare(A^†, B^†) and Compare(B^†,
    A^†) to reduce symbol bias. As presented in Figure [8](#S3.F8 "Figure 8 ‣ 3.1
    Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context Learning Help
    Long-Context LLM Judges? See More, Judge Better!"), it can be seen that as in-context
    examples increase, the higher the win rate of the many-shot regime, which further
    verifies the effectiveness of the many-shot regime in helping GPT-4o-as-a-Judge.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究 Wang 等人（[2023b](#bib.bib22)）整合了 Compare(A, B) 和 Compare(B, A) 的评估结果，以减轻位置偏差，这激励我们整合
    Compare(A^†, B^†) 和 Compare(B^†, A^†) 的评估结果以减少符号偏差。如图[8](#S3.F8 "Figure 8 ‣ 3.1
    Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context Learning Help
    Long-Context LLM Judges? See More, Judge Better!") 所示，随着上下文示例的增加，多样本方案的胜率更高，这进一步验证了多样本方案在帮助
    GPT-4o 作为评判者方面的有效性。
- en: Furthermore, there is an interesting hypothesis that the phenomenon of the positional
    bias we think is not actually caused by the positions of answers, instead of the
    symbols of answers. In other words, there may be no positional bias in using LLMs
    as judges in pairwise comparison but rather the symbol bias. The experimental
    results from Figure [8](#S3.F8 "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") Compare(A, B) and Compare(A^†, B^†) can demonstrate this hypothesis
    well.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有一种有趣的假设认为，我们认为的定位偏差现象实际上并非由答案的位置引起，而是由答案的符号引起。换句话说，在使用 LLMs 作为成对比较的裁判时，可能不存在位置偏差，而是符号偏差。图 [8](#S3.F8
    "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!") 中的 Compare(A,
    B) 和 Compare(A^†, B^†) 实验结果可以很好地展示这一假设。
- en: 4 Related Work
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: LLMs have exhibited remarkable general generation capabilities, positioning
    themselves as powerful assistants Zhao et al. ([2023](#bib.bib26)); OpenAI ([2023](#bib.bib16)).
    With the rapid progression of LLMs, evaluating their proficiency in adhering to
    human instructions is imperative. Given the advanced capabilities of LLMs, researchers
    have begun adopting these models to judge the performance of LLMs in following
    human instructions Koo et al. ([2023](#bib.bib11)); Liusie et al. ([2023](#bib.bib14));
    Liu et al. ([2023](#bib.bib13)); Zhu et al. ([2023](#bib.bib28)); Lu et al. ([2023](#bib.bib15));
    Fu et al. ([2023b](#bib.bib10)); Zheng et al. ([2023](#bib.bib27)); Wang et al.
    ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6)). Notably, the evaluation
    paradigm introduced by Zheng et al. ([2023](#bib.bib27)) has gained widespread
    adoption. However, LLMs as judges are revealed to have potential biases Wang et al.
    ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6)), which leads to higher
    uncertainty and inconsistency during the evaluation using LLMs, questioning the
    validity of LLM-as-a-Judge.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 展现了卓越的生成能力，将自己定位为强大的助手 Zhao et al. ([2023](#bib.bib26)); OpenAI ([2023](#bib.bib16))。随着
    LLMs 的快速发展，评估它们遵循人类指令的能力变得至关重要。鉴于 LLMs 的先进能力，研究人员已经开始采用这些模型来评估 LLMs 遵循人类指令的表现
    Koo et al. ([2023](#bib.bib11)); Liusie et al. ([2023](#bib.bib14)); Liu et al.
    ([2023](#bib.bib13)); Zhu et al. ([2023](#bib.bib28)); Lu et al. ([2023](#bib.bib15));
    Fu et al. ([2023b](#bib.bib10)); Zheng et al. ([2023](#bib.bib27)); Wang et al.
    ([2023b](#bib.bib22)); Chen et al. ([2024](#bib.bib6))。值得注意的是，Zheng et al. ([2023](#bib.bib27))
    提出的评估范式已被广泛采纳。然而，作为裁判的 LLMs 被揭示存在潜在偏差 Wang et al. ([2023b](#bib.bib22)); Chen
    et al. ([2024](#bib.bib6))，这导致使用 LLMs 进行评估时存在更高的不确定性和不一致性，质疑了 LLM 作为裁判的有效性。
- en: 5 Conclusion
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we mainly study whether many-shot ICL helps long-context LLMs
    as judges, such as GPT-4o. To this end, we designed several experimental prompts,
    e.g., many-shot Reinforced and Unsupervised ICL. Experiments show that many-shot
    ICL can help the GPT-4o judge improve the consistency and quality of evaluation.
    Meanwhile, we also revealed another bias when LLMs act as judges, symbol bias,
    and further proposed mitigation approaches. In summary, we preliminarily verified
    the effectiveness of using many-shot ICL to assist the GPT-4o judge in single
    answer grading.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究主要探讨了多次 ICL 是否有助于长上下文 LLMs 作为裁判，例如 GPT-4o。为此，我们设计了几种实验提示，例如多次强化和无监督 ICL。实验表明，多次
    ICL 可以帮助 GPT-4o 裁判提高评估的一致性和质量。同时，我们还揭示了 LLMs 作为裁判时的另一种偏差，即符号偏差，并进一步提出了缓解方法。总之，我们初步验证了使用多次
    ICL 来辅助 GPT-4o 裁判进行单一答案评分的有效性。
- en: 6 Limitations
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: Considering the trade-off between costs and benefits, we do not verify too many
    in-context examples in the experiments, such as thousands of examples. Combining
    Figures [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!") and Figure [8](#S3.F8
    "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments ‣ Can Many-Shot In-Context
    Learning Help Long-Context LLM Judges? See More, Judge Better!"), it is not difficult
    to see that when the number of in-context examples increases to 64 and 128, although
    the consistency no longer improves, the evaluation quality has increased significantly.
    In addition, we consider that using GPT-4o-as-a-Judge in the many-shot regime
    is another evolution of the weak-to-strong strategy Burns et al. ([2023](#bib.bib4)),
    which uses many zero-shot judgment results to help GPT-4o generate a better one.
    As the long-context capabilities of LLMs improve, adding more in-context examples
    may reveal more valuable phenomena for studying long-context LLMs as judges in
    the future.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到成本与效益之间的权衡，我们在实验中没有验证过多的上下文示例，例如成千上万的示例。结合图 [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!") 和图 [8](#S3.F8 "Figure 8 ‣ 3.1 Experimental Settings ‣ 3 Experiments
    ‣ Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge
    Better!")，不难看出，当上下文示例的数量增加到 64 和 128 时，尽管一致性没有进一步改善，但评估质量显著提高。此外，我们认为在多示例模式下使用
    GPT-4o 作为评判者是 Burns 等（[2023](#bib.bib4)）的弱到强策略的另一种演变，该策略利用许多零示例判断结果来帮助 GPT-4o
    生成更好的结果。随着 LLM 的长上下文能力的提高，增加更多的上下文示例可能会揭示更多有价值的现象，以便未来研究长上下文 LLM 作为评判者。
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agarwal et al. (2024) Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet,
    Stephanie C. Y. Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes,
    Eric Chu, Feryal M. P. Behbahani, Aleksandra Faust, and Hugo Larochelle. 2024.
    [Many-shot in-context learning](https://doi.org/10.48550/ARXIV.2404.11018). *CoRR*,
    abs/2404.11018.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等（2024）Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie
    C. Y. Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu,
    Feryal M. P. Behbahani, Aleksandra Faust 和 Hugo Larochelle. 2024. [多示例上下文学习](https://doi.org/10.48550/ARXIV.2404.11018).
    *CoRR*, abs/2404.11018。
- en: 'Anthropic (2024) Anthropic. 2024. The claude 3 model family: Opus, sonnet,
    haiku. *Technical Report*.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2024）Anthropic. 2024. Claude 3 模型系列：Opus, sonnet, haiku. *技术报告*。
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown 等（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 和 Dario
    Amodei. 2020. [语言模型是少量示例学习者](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
    收录于 *神经信息处理系统进展 33: 神经信息处理系统年会*。'
- en: 'Burns et al. (2023) Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen
    Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar,
    Jan Leike, Ilya Sutskever, and Jeff Wu. 2023. [Weak-to-strong generalization:
    Eliciting strong capabilities with weak supervision](https://doi.org/10.48550/ARXIV.2312.09390).
    *CoRR*, abs/2312.09390.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burns 等（2023）Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker,
    Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
    Leike, Ilya Sutskever 和 Jeff Wu. 2023. [弱到强的泛化：通过弱监督引发强能力](https://doi.org/10.48550/ARXIV.2312.09390).
    *CoRR*, abs/2312.09390。
- en: Chang et al. (2023) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu,
    Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,
    Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2023. [A survey on evaluation
    of large language models](https://doi.org/10.48550/ARXIV.2307.03109). *CoRR*,
    abs/2307.03109.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等（2023）Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen,
    Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang,
    Philip S. Yu, Qiang Yang 和 Xing Xie. 2023. [大语言模型评估调查](https://doi.org/10.48550/ARXIV.2307.03109).
    *CoRR*, abs/2307.03109。
- en: Chen et al. (2024) Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang,
    and Benyou Wang. 2024. [Humans or llms as the judge? A study on judgement biases](https://doi.org/10.48550/ARXIV.2402.10669).
    *CoRR*, abs/2402.10669.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2024）Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, 和 Benyou
    Wang。2024。[Humans or llms as the judge? A study on judgement biases](https://doi.org/10.48550/ARXIV.2402.10669)。*CoRR*，abs/2402.10669。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. [Palm: Scaling language
    modeling with pathways](http://jmlr.org/papers/v24/22-1144.html). *J. Mach. Learn.
    Res.*, 24:240:1–240:113.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2023）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew
    M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, 和 Noah Fiedel。2023。[Palm: Scaling language
    modeling with pathways](http://jmlr.org/papers/v24/22-1144.html)。*J. Mach. Learn.
    Res.*，24:240:1–240:113。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. [Training verifiers to solve
    math word problems](https://arxiv.org/abs/2110.14168). *CoRR*, abs/2110.14168.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman。2021。[Training verifiers to solve math word
    problems](https://arxiv.org/abs/2110.14168)。*CoRR*，abs/2110.14168。
- en: 'Fu et al. (2023a) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023a. [Gptscore: Evaluate as you desire](https://doi.org/10.48550/ARXIV.2302.04166).
    *CoRR*, abs/2302.04166.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu 等（2023a）Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, 和 Pengfei Liu。2023a。[Gptscore:
    Evaluate as you desire](https://doi.org/10.48550/ARXIV.2302.04166)。*CoRR*，abs/2302.04166。'
- en: Fu et al. (2023b) Xue-Yong Fu, Md. Tahmid Rahman Laskar, Cheng Chen, and Shashi Bhushan
    TN. 2023b. [Are large language models reliable judges? A study on the factuality
    evaluation capabilities of llms](https://doi.org/10.48550/ARXIV.2311.00681). *CoRR*,
    abs/2311.00681.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023b）Xue-Yong Fu, Md. Tahmid Rahman Laskar, Cheng Chen, 和 Shashi Bhushan
    TN。2023b。[Are large language models reliable judges? A study on the factuality
    evaluation capabilities of llms](https://doi.org/10.48550/ARXIV.2311.00681)。*CoRR*，abs/2311.00681。
- en: Koo et al. (2023) Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung
    Kim, and Dongyeop Kang. 2023. [Benchmarking cognitive biases in large language
    models as evaluators](https://doi.org/10.48550/ARXIV.2309.17012). *CoRR*, abs/2309.17012.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koo 等（2023）Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim,
    和 Dongyeop Kang。2023。[Benchmarking cognitive biases in large language models as
    evaluators](https://doi.org/10.48550/ARXIV.2309.17012)。*CoRR*，abs/2309.17012。
- en: Li et al. (2023) Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang,
    Zhiyong Wu, and Lingpeng Kong. 2023. [In-context learning with many demonstration
    examples](https://doi.org/10.48550/ARXIV.2302.04931). *CoRR*, abs/2302.04931.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong
    Wu, 和 Lingpeng Kong。2023。[In-context learning with many demonstration examples](https://doi.org/10.48550/ARXIV.2302.04931)。*CoRR*，abs/2302.04931。
- en: 'Liu et al. (2023) Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023. [Llms
    as narcissistic evaluators: When ego inflates evaluation scores](https://doi.org/10.48550/ARXIV.2311.09766).
    *CoRR*, abs/2311.09766.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023）Yiqi Liu, Nafise Sadat Moosavi, 和 Chenghua Lin。2023。[Llms as narcissistic
    evaluators: When ego inflates evaluation scores](https://doi.org/10.48550/ARXIV.2311.09766)。*CoRR*，abs/2311.09766。'
- en: Liusie et al. (2023) Adian Liusie, Potsawee Manakul, and Mark J. F. Gales. 2023.
    [Zero-shot NLG evaluation through pairware comparisons with llms](https://doi.org/10.48550/ARXIV.2307.07889).
    *CoRR*, abs/2307.07889.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liusie et al. (2023) Adian Liusie, Potsawee Manakul, 和 Mark J. F. Gales. 2023.
    [Zero-shot NLG evaluation through pairware comparisons with llms](https://doi.org/10.48550/ARXIV.2307.07889)。*CoRR*，abs/2307.07889。
- en: 'Lu et al. (2023) Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng
    Tao. 2023. [Error analysis prompting enables human-like translation evaluation
    in large language models: A case study on chatgpt](https://doi.org/10.48550/ARXIV.2303.13809).
    *CoRR*, abs/2303.13809.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu et al. (2023) Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, 和 Dacheng Tao.
    2023. [Error analysis prompting enables human-like translation evaluation in large
    language models: A case study on chatgpt](https://doi.org/10.48550/ARXIV.2303.13809)。*CoRR*，abs/2303.13809。'
- en: OpenAI (2023) OpenAI. 2023. [GPT-4 technical report](https://doi.org/10.48550/ARXIV.2303.08774).
    *CoRR*, abs/2303.08774.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [GPT-4 technical report](https://doi.org/10.48550/ARXIV.2303.08774)。*CoRR*，abs/2303.08774。
- en: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. 2023. [Instruction tuning with GPT-4](https://doi.org/10.48550/ARXIV.2304.03277).
    *CoRR*, abs/2304.03277.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, 和
    Jianfeng Gao. 2023. [Instruction tuning with GPT-4](https://doi.org/10.48550/ARXIV.2304.03277)。*CoRR*，abs/2304.03277。
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
    Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,
    Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin
    Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael
    Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk,
    Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens
    Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher,
    Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri,
    Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener,
    and et al. 2024. [Gemini 1.5: Unlocking multimodal understanding across millions
    of tokens of context](https://doi.org/10.48550/ARXIV.2403.05530). *CoRR*, abs/2403.05530.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
    Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,
    Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin
    Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael
    Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk,
    Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens
    Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher,
    Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri,
    Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener,
    和 et al. 2024. [Gemini 1.5: Unlocking multimodal understanding across millions
    of tokens of context](https://doi.org/10.48550/ARXIV.2403.05530)。*CoRR*，abs/2403.05530。'
- en: 'Song et al. (2024) Mingyang Song, Mao Zheng, and Xuan Luo. 2024. [Counting-stars:
    A multi-evidence, position-aware, and scalable benchmark for evaluating long-context
    large language models](https://arxiv.org/abs/2403.11802). *Preprint*, arXiv:2403.11802.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2024) Mingyang Song, Mao Zheng, 和 Xuan Luo. 2024. [Counting-stars:
    A multi-evidence, position-aware, and scalable benchmark for evaluating long-context
    large language models](https://arxiv.org/abs/2403.11802)。*预印本*，arXiv:2403.11802。'
- en: '(20) Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. [Language
    models don’t always say what they think: Unfaithful explanations in chain-of-thought
    prompting](http://papers.nips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023*.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(20) Miles Turpin, Julian Michael, Ethan Perez, 和 Samuel R. Bowman. [Language
    models don’t always say what they think: Unfaithful explanations in chain-of-thought
    prompting](http://papers.nips.cc/paper_files/paper/2023/hash/ed3fea9033a80fea1376299fa7863f4a-Abstract-Conference.html)。在
    *Advances in Neural Information Processing Systems 36: Annual Conference on Neural
    Information Processing Systems 2023*。'
- en: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. [Is chatgpt a good NLG evaluator?
    A preliminary study](https://doi.org/10.48550/ARXIV.2303.04048). *CoRR*, abs/2303.04048.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, 和 Jie Zhou. 2023a. [Is chatgpt a good NLG evaluator?
    A preliminary study](https://doi.org/10.48550/ARXIV.2303.04048)。*CoRR*，abs/2303.04048。
- en: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. [Large language models
    are not fair evaluators](https://doi.org/10.48550/ARXIV.2305.17926). *CoRR*, abs/2305.17926.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, 和 Zhifang Sui. 2023b. [Large language models are
    not fair evaluators](https://doi.org/10.48550/ARXIV.2305.17926)。*CoRR*，abs/2305.17926。
- en: 'Wang et al. (2023c) Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang
    Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun
    Zhang, and Yue Zhang. 2023c. [Pandalm: An automatic evaluation benchmark for LLM
    instruction tuning optimization](https://doi.org/10.48550/ARXIV.2306.05087). *CoRR*,
    abs/2306.05087.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等（2023c）艺东王、卓豪余、郑然曾、林怡杨、存祥王、浩辰、超雅姜、瑞谢、金东王、兴谢、魏叶、世坤张和跃张。2023c。[Pandalm: 一个自动评估基准，用于LLM指令调优优化](https://doi.org/10.48550/ARXIV.2306.05087)。*CoRR*，abs/2306.05087。'
- en: 'Wu and Aji (2023) Minghao Wu and Alham Fikri Aji. 2023. [Style over substance:
    Evaluation biases for large language models](https://doi.org/10.48550/ARXIV.2307.03025).
    *CoRR*, abs/2307.03025.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴和阿吉（2023）明浩吴和阿尔哈姆·费克里·阿吉。2023年。[风格优于实质：大型语言模型的评估偏差](https://doi.org/10.48550/ARXIV.2307.03025)。*CoRR*，abs/2307.03025。
- en: 'Xu et al. (2023) Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023.
    [Baize: An open-source chat model with parameter-efficient tuning on self-chat
    data](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.385). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023,
    Singapore, December 6-10, 2023*, pages 6268–6278\. Association for Computational
    Linguistics.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '徐等（2023）灿文徐、达雅郭、南端和朱利安·J·麦考利。2023年。[Baize: 一个开源聊天模型，具有自我聊天数据上的参数高效调优](https://doi.org/10.18653/V1/2023.EMNLP-MAIN.385)。在*2023年自然语言处理实证方法会议（EMNLP
    2023），新加坡，2023年12月6-10日*，页面6268–6278。计算语言学协会。'
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan
    Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li,
    Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. [A survey
    of large language models](https://doi.org/10.48550/ARXIV.2303.18223). *CoRR*,
    abs/2303.18223.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2023）韦恩·辛赵、昆周、军艺李、天意唐、小磊王、育鹏侯、颖倩闵、贝辰张、军杰张、自灿董、怡凡杜、陈阳、雨硕陈、志鹏陈、金浩姜、瑞阳任、怡凡李、新宇唐、子康刘、佩宇刘、简云聂和季荣温。2023年。[大型语言模型综述](https://doi.org/10.48550/ARXIV.2303.18223)。*CoRR*，abs/2303.18223。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E Gonzalez, and Ion Stoica. 2023. [Judging llm-as-a-judge with mt-bench
    and chatbot arena](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf).
    In *Advances in Neural Information Processing Systems*, volume 36, pages 46595–46623.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2023）连敏郑、魏林蒋、英生、思远庄、张浩吴、永浩庄、子琳、卓涵李、大成李、埃里克·星、郝张、约瑟夫·E·冈萨雷斯和伊昂·斯托伊卡。2023年。[使用mt-bench和聊天机器人竞技场对llm-as-a-judge进行评估](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf)。在*神经信息处理系统进展*，第36卷，页面46595–46623。
- en: 'Zhu et al. (2023) Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. [Judgelm:
    Fine-tuned large language models are scalable judges](https://doi.org/10.48550/ARXIV.2310.17631).
    *CoRR*, abs/2310.17631.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '朱等（2023）梁辉朱、邢刚王和辛龙王。2023年。[Judgelm: 微调的大型语言模型是可扩展的评判者](https://doi.org/10.48550/ARXIV.2310.17631)。*CoRR*，abs/2310.17631。'
- en: Appendix A Appendix
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: A.1 Many-Shot Unsupervised ICL
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 多次尝试的无监督ICL
- en: 'Similar to Agarwal et al. ([2024](#bib.bib1)), the default prompt of many-shot
    Unsupervised ICL is composed of three main components:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Agarwal等（[2024](#bib.bib1)），多次尝试的无监督ICL的默认提示由三个主要组件组成：
- en: '1.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: A preamble, such as, “You will be provided questions similar to the ones below:”.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引言，例如：“你将会收到类似下面的问题：”。
- en: '2.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: A list of unsolved inputs or problems.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一系列未解决的输入或问题。
- en: '3.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: A few-shot prompt with outputs for the desired output format.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个包含所需输出格式的几次尝试的提示和输出。
- en: To require the GPT-4o judge outputs the desired output format, we append 4-shot
    in-context examples with answers for many-shot Unsupervised ICL. The context length
    of a single test sample with $K$-shot in-context examples is presented in Table [4](#A1.T4
    "Table 4 ‣ A.3 Dataset ‣ Appendix A Appendix ‣ Can Many-Shot In-Context Learning
    Help Long-Context LLM Judges? See More, Judge Better!"). As mentioned before,
    considering the trade-off between resources and effects, in the experiments of
    this paper, we do not introduce too many in-context examples, such as thousands
    of examples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了要求GPT-4o评判输出所需的格式，我们附加了4次尝试的上下文示例和答案，用于多次尝试的无监督ICL。具有$K$-次尝试上下文示例的单个测试样本的上下文长度如表[4](#A1.T4
    "表4 ‣ A.3 数据集 ‣ 附录A 附录 ‣ 多次尝试的上下文学习能否帮助长上下文LLM评判者？更多信息，请参见，更好地评判！")所示。如前所述，考虑到资源和效果之间的权衡，在本文的实验中，我们没有引入太多的上下文示例，如数千个示例。
- en: A.2 Symbol Bias
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 符号偏倚
- en: Previous work Song et al. ([2024](#bib.bib19)) has found that when an LLM is
    stress-tested, it is easy to output some wrong information, which may be an increasing
    array related to the test data or an English alphabet sequence starting with "A".
    This phenomenon shows that LLMs favor answers with the symbol "A" rather than
    the symbol "B" (or the symbol "1" instead of the symbol "2").
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究 Song et al. ([2024](#bib.bib19)) 发现，当对大型语言模型（LLM）进行压力测试时，它容易输出一些错误信息，这些信息可能是与测试数据相关的递增序列或以“A”开头的英文字母序列。这一现象表明，LLM
    更倾向于选择符号“A”的答案，而不是符号“B”（或者符号“1”而不是符号“2”）。
- en: A.3 Dataset
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 数据集
- en: GSM8K¹¹1[https://huggingface.co/datasets/openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k)
    is previously introduced by Cobbe et al. ([2021](#bib.bib8)), which comprises
    8.5K high-quality grade school math problems meticulously crafted by human problem
    writers. This dataset is divided into 7.5K training and 1K test problems. Specifically,
    each problem typically requires between 2 to 8 steps to solve, primarily involving
    a sequence of elementary calculations using basic arithmetic operations (addition,
    subtraction, multiplication, and division). The problems are designed so that
    a proficient middle school student can solve each one. Furthermore, the problem-solving
    task requires models to solve problems with model-generated rationales, which
    may be challenging to evaluate. We use LLaMA3-70B²²2[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)
    to create model-generated rationales for GSM8K.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GSM8K¹¹1[https://huggingface.co/datasets/openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k)
    由 Cobbe et al. ([2021](#bib.bib8)) 介绍，该数据集包含 8.5K 高质量的学年数学题，这些题目由人工编写者精心制作。该数据集分为
    7.5K 训练题和 1K 测试题。具体来说，每个问题通常需要 2 到 8 个步骤来解决，主要涉及基本的算术运算（加法、减法、乘法和除法）的序列。问题设计如此，以至于一个熟练的中学生能够解决每一个问题。此外，解决问题的任务要求模型生成推理过程，这可能会很难评估。我们使用
    LLaMA3-70B²²2[https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)
    为 GSM8K 创建模型生成的推理过程。
- en: '| Now, I am going to give you a series of demonstrations of Problems and Solutions.
    When you respond, respond only with the Solution of the final Problem, thinking
    step by step. Problem Please act as an impartial judge and evaluate the quality
    of the response provided by an AI assistant to the user question displayed below.
    Your evaluation should consider factors such as the helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of the response. Please rate the response
    on a scale of 1 to 10 by strictly following this JSON format: {"rating":"", "reason":""}.
    The "rating" should be as objective as possible. The "reason" denotes a comprehensive
    explanation of your rating, which should consider factors such as helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of the response should
    also be considered. Only return the JSON results and do not give any explanation.
    Question {question1} Response {response1} Solution {solution1} — Problem Please
    act as an impartial judge and evaluate the quality of the response provided by
    an AI assistant to the user question displayed below. Your evaluation should consider
    factors such as the helpfulness, relevance, accuracy, depth, creativity, and level
    of detail of the response. Please rate the response on a scale of 1 to 10 by strictly
    following this JSON format: {"rating":"", "reason":""}. The "rating" should be
    as objective as possible. The "reason" denotes a comprehensive explanation of
    your rating, which should consider factors such as helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of the response should also be considered.
    Only return the JSON results and do not give any explanation. Question {question2}
    Response {response2} Solution {solution2} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question3}
    Response {response3} Solution {solution3} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question4}
    Response {response4} Solution {solution4} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {final_question}
    Response {final_response} Solution |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: 现在，我将为你展示一系列问题和解决方案的演示。当你回应时，只需逐步思考，并仅回应最终问题的解决方案。问题 请充当一个公正的裁判，评估以下AI助手对用户问题的回应质量。你的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节水平等因素。请按照严格遵循以下JSON格式的方式对回应进行1到10的评分：{"rating":"",
    "reason":""}。其中，“rating”应尽可能客观。“reason”应详细说明你的评分，包括有用性、相关性、准确性、深度、创造力和细节水平等因素。仅返回JSON结果，不提供任何解释。问题
    {question1} 回应 {response1} 解决方案 {solution1} — 问题 请充当一个公正的裁判，评估以下AI助手对用户问题的回应质量。你的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节水平等因素。请按照严格遵循以下JSON格式的方式对回应进行1到10的评分：{"rating":"",
    "reason":""}。其中，“rating”应尽可能客观。“reason”应详细说明你的评分，包括有用性、相关性、准确性、深度、创造力和细节水平等因素。仅返回JSON结果，不提供任何解释。问题
    {question2} 回应 {response2} 解决方案 {solution2} — 问题 请充当一个公正的裁判，评估以下AI助手对用户问题的回应质量。你的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节水平等因素。请按照严格遵循以下JSON格式的方式对回应进行1到10的评分：{"rating":"",
    "reason":""}。其中，“rating”应尽可能客观。“reason”应详细说明你的评分，包括有用性、相关性、准确性、深度、创造力和细节水平等因素。仅返回JSON结果，不提供任何解释。问题
    {question3} 回应 {response3} 解决方案 {solution3} — 问题 请充当一个公正的裁判，评估以下AI助手对用户问题的回应质量。你的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节水平等因素。请按照严格遵循以下JSON格式的方式对回应进行1到10的评分：{"rating":"",
    "reason":""}。其中，“rating”应尽可能客观。“reason”应详细说明你的评分，包括有用性、相关性、准确性、深度、创造力和细节水平等因素。仅返回JSON结果，不提供任何解释。问题
    {question4} 回应 {response4} 解决方案 {solution4} — 问题 请充当一个公正的裁判，评估以下AI助手对用户问题的回应质量。你的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节水平等因素。请按照严格遵循以下JSON格式的方式对回应进行1到10的评分：{"rating":"",
    "reason":""}。其中，“rating”应尽可能客观。“reason”应详细说明你的评分，包括有用性、相关性、准确性、深度、创造力和细节水平等因素。仅返回JSON结果，不提供任何解释。问题
    {final_question} 回应 {final_response} 解决方案 |
- en: 'Table 1: Example Prompt with 4-shot in-context examples used for Reinforced
    ICL.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：用于强化ICL的4-shot上下文示例的示例提示。
- en: '| Please act as an impartial judge and evaluate the quality of the responses
    provided by two AI assistants to the user question displayed below. You should
    choose the assistant that follows the user’s instructions and answers the user’s
    question better. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their responses.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Avoid any position biases and ensure that the order in which the responses were
    presented does not influence your decision. Do not allow the length of the responses
    to influence your evaluation. Do not favor certain names of the assistants. Be
    as objective as possible. After providing your explanation, output your final
    verdict by strictly following this format: "[[A]]" if assistant A is better, "[[B]]"
    if assistant B is better, and "[[C]]" for a tie. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 请作为公正的评审，评估两个AI助手对以下用户问题提供的回答的质量。您应选择更好地遵循用户指示并回答用户问题的助手。您的评估应考虑回答的有用性、相关性、准确性、深度、创造性和细节水平。在比较两个回答并提供简短解释后开始评估。避免任何立场偏见，确保回答的呈现顺序不会影响您的决策。不要让回答的长度影响您的评估。不要偏袒某些助手名称。尽可能客观。在提供解释后，按以下格式输出最终裁定：“[[A]]”如果助手A更好，“[[B]]”如果助手B更好，“[[C]]”表示平局。
    |'
- en: '| Problem |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 问题 |'
- en: '| Please act as an impartial judge and evaluate the quality of the response
    provided by an AI assistant to the user question displayed below. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of the response. Please rate the response on a scale of 1
    to 10 by strictly following this JSON format: {"rating":"", "reason":""}. The
    "rating" should be as objective as possible. The "reason" denotes a comprehensive
    explanation of your rating, which should consider factors such as helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of the response should
    also be considered. Only return the JSON results and do not give any explanation.
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 请作为公正的评审，评估AI助手对以下用户问题提供的回答的质量。您的评估应考虑回答的有用性、相关性、准确性、深度、创造性和细节水平。请按照JSON格式对回答进行1到10的评分，严格遵循以下格式：{"rating":"",
    "reason":""}。“rating”应尽可能客观。“reason”表示对您评分的全面解释，应考虑有用性、相关性、准确性、深度、创造性和细节水平等因素。仅返回JSON结果，不提供解释。
    |'
- en: '| Question |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 问题 |'
- en: '| {question} |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| {问题} |'
- en: '| Response |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 回应 |'
- en: '| {response} |  | The Assistant A’s Answer | The Assistant A’s Answer | The
    Assistant B’s Answer | The Assistant A’s Answer |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| {响应} |  | 助手A的回答 | 助手A的回答 | 助手B的回答 | 助手A的回答 |'
- en: '| {Answer-A} | {Answer-B} | {Answer-A} | {Answer-B} |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| {答案-A} | {答案-B} | {答案-A} | {答案-B} |'
- en: '| The Assistant B’s Answer | The Assistant B’s Answer | The Assistant A’s Answer
    | The Assistant B’s Answer |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 助手B的回答 | 助手B的回答 | 助手A的回答 | 助手B的回答 |'
- en: '| {Answer-B} | {Answer-A} | {Answer-B} | {Answer-A} |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| {答案-B} | {答案-A} | {答案-B} | {答案-A} |'
- en: '| Compare(A, B) | Compare(B, A) | Compare(A^†, B^†) | Compare(B^†, A^†) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 比较(A, B) | 比较(B, A) | 比较(A^†, B^†) | 比较(B^†, A^†) |'
- en: 'Table 2: The prompts used for pairwise comparison.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：用于成对比较的提示。
- en: '| Prompt(A). The zero-shot prompt for single answer grading in Zheng et al.
    ([2023](#bib.bib27)). |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 提示(A)。郑等人提出的单一答案评分的零样本提示 ([2023](#bib.bib27))。 |'
- en: '| --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Please act as an impartial judge and evaluate the quality of the response
    provided by an AI assistant to the user question displayed below. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of the response. Begin your evaluation by providing a short
    explanation. Be as objective as possible. After providing your explanation, please
    rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]",
    for example: "Rating: [[5]]". |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 请作为公正的评审，评估AI助手对以下用户问题提供的回答的质量。您的评估应考虑回答的有用性、相关性、准确性、深度、创造性和细节水平。开始评估时请提供简短的解释。尽可能客观。在提供解释后，请按以下格式对回答进行1到10的评分：“[[rating]]”，例如：“评分：[[5]]”。
    |'
- en: '| Question |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 问题 |'
- en: '| {question} |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| {问题} |'
- en: '| Response |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 回应 |'
- en: '| {response} |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| {响应} |'
- en: '| Prompt(B). The zero-shot prompt for single answer grading in this paper.
    Please act as an impartial judge and evaluate the quality of the response provided
    by an AI assistant to the user question displayed below. Your evaluation should
    consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of the response. Please rate the response on a scale of 1
    to 10 by strictly following this JSON format: {"rating":"", "reason":""}. The
    "rating" should be as objective as possible. The "reason" denotes a comprehensive
    explanation of your rating, which should consider factors such as helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of the response should
    also be considered. Only return the JSON results and do not give any explanation.
    Question {question} Response {response} |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 提示(B)。本文的零-shot 提示用于单一答案评分。请充当一个公正的评审，评估 AI 助手对用户问题的回答质量。你的评估应考虑诸如回答的有用性、相关性、准确性、深度、创造力以及详细程度等因素。请按照以下
    JSON 格式对回答进行 1 到 10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 应对你的评分提供全面的解释，考虑的因素包括回答的有用性、相关性、准确性、深度、创造力以及详细程度等。仅返回
    JSON 结果，不提供任何解释。问题 {question} 回答 {response} |'
- en: 'Table 3: Two versions of zero-shot prompts are used for evaluation in this
    paper.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 本文使用了两个版本的零-shot 提示进行评估。'
- en: '|  | $2^{0}$ | $2^{4}$ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $2^{0}$ | $2^{4}$ |'
- en: '| Average Context Length in Tokens of A Single Test Sample |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 单个测试样本的平均上下文长度（以 token 为单位） |'
- en: '| Reinforced ICL | 0.7K | 1.1K | 2.2K | 3.6K | 6.5K | 13.6K | 26.3K | 50.7K
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 强化 ICL | 0.7K | 1.1K | 2.2K | 3.6K | 6.5K | 13.6K | 26.3K | 50.7K |'
- en: '| Unsupervised ICL (Append 4-shots) | 2.2K | 2.6K | 3.1K | 4.6K | 6.9K | 11.7K
    | 20.9K | 40.8K |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 ICL（附加 4-shots） | 2.2K | 2.6K | 3.1K | 4.6K | 6.9K | 11.7K | 20.9K |
    40.8K |'
- en: 'Table 4: Context length in tokens for $K$-shot Reinforced and Unsupervised
    ICL.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: $K$-shot 强化和无监督 ICL 的上下文长度（以 token 为单位）。'
- en: '| You will be provided Problems similar to the ones below: Problem Please act
    as an impartial judge and evaluate the quality of the response provided by an
    AI assistant to the user question displayed below. Your evaluation should consider
    factors such as the helpfulness, relevance, accuracy, depth, creativity, and level
    of detail of the response. Please rate the response on a scale of 1 to 10 by strictly
    following this JSON format: {"rating":"", "reason":""}. The "rating" should be
    as objective as possible. The "reason" denotes a comprehensive explanation of
    your rating, which should consider factors such as helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of the response should also be considered.
    Only return the JSON results and do not give any explanation. Question {question1}
    Response {response1} Now, I am going to give you a series of demonstrations of
    Problems and Solutions. When you respond, respond only with the Solution of the
    final Problem, thinking step by step. Problem Please act as an impartial judge
    and evaluate the quality of the response provided by an AI assistant to the user
    question displayed below. Your evaluation should consider factors such as the
    helpfulness, relevance, accuracy, depth, creativity, and level of detail of the
    response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question1}
    Response {response1} Solution {solution1} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question2}
    Response {response2} Solution {solution2} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question3}
    Response {response3} Solution {solution3} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {question4}
    Response {response4} Solution {solution4} — Problem Please act as an impartial
    judge and evaluate the quality of the response provided by an AI assistant to
    the user question displayed below. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of the response. Please rate the response on a scale of 1 to 10 by strictly following
    this JSON format: {"rating":"", "reason":""}. The "rating" should be as objective
    as possible. The "reason" denotes a comprehensive explanation of your rating,
    which should consider factors such as helpfulness, relevance, accuracy, depth,
    creativity, and level of detail of the response should also be considered. Only
    return the JSON results and do not give any explanation. Question {final_question}
    Response {final_response} Solution |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 您将会收到类似于以下问题的题目：问题 请充当一个公正的裁判，评估 AI 助手对用户问题提供的回应质量。您的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度等因素。请按照以下
    JSON 格式对回应进行 1 到 10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 表示您评分的全面解释，应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度。仅返回
    JSON 结果，不要给出任何解释。问题 {question1} 回应 {response1} 现在，我将给您一系列问题和解决方案的示范。当您回应时，仅需提供最后一个问题的解决方案，逐步思考。问题
    请充当一个公正的裁判，评估 AI 助手对用户问题提供的回应质量。您的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度等因素。请按照以下 JSON
    格式对回应进行 1 到 10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 表示您评分的全面解释，应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度。仅返回
    JSON 结果，不要给出任何解释。问题 {question1} 回应 {response1} 解决方案 {solution1} — 问题 请充当一个公正的裁判，评估
    AI 助手对用户问题提供的回应质量。您的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度等因素。请按照以下 JSON 格式对回应进行 1 到
    10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 表示您评分的全面解释，应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度。仅返回
    JSON 结果，不要给出任何解释。问题 {question2} 回应 {response2} 解决方案 {solution2} — 问题 请充当一个公正的裁判，评估
    AI 助手对用户问题提供的回应质量。您的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度等因素。请按照以下 JSON 格式对回应进行 1 到
    10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 表示您评分的全面解释，应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度。仅返回
    JSON 结果，不要给出任何解释。问题 {question3} 回应 {response3} 解决方案 {solution3} — 问题 请充当一个公正的裁判，评估
    AI 助手对用户问题提供的回应质量。您的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度等因素。请按照以下 JSON 格式对回应进行 1 到
    10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 表示您评分的全面解释，应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度。仅返回
    JSON 结果，不要给出任何解释。问题 {question4} 回应 {response4} 解决方案 {solution4} — 问题 请充当一个公正的裁判，评估
    AI 助手对用户问题提供的回应质量。您的评估应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度等因素。请按照以下 JSON 格式对回应进行 1 到
    10 的评分：{"rating":"", "reason":""}。 "rating" 应尽可能客观。"reason" 表示您评分的全面解释，应考虑回应的有用性、相关性、准确性、深度、创造力和细节程度。仅返回
    JSON 结果，不要给出任何解释。问题 {final_question} 回应 {final_response} 解决方案 |'
- en: 'Table 5: Example Prompt with 1-shot in-context examples used for Unsupervised
    ICL.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：用于无监督 ICL 的 1-shot 上下文示例的示例提示。
