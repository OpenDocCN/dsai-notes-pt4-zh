- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当心你的代理！调查针对基于LLM的代理的后门威胁
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11208](https://ar5iv.labs.arxiv.org/html/2402.11208)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.11208](https://ar5iv.labs.arxiv.org/html/2402.11208)
- en: Wenkai Yang¹, Xiaohan Bi^∗², Yankai Lin¹, Sishuo Chen², Jie Zhou³, Xu Sun^†⁴
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 杨文凯¹，毕晓涵^∗²，林彦凯¹，陈思硕²，周杰³，孙旭^†⁴
- en: ¹Gaoling School of Artificial Intelligence, Renmin University of China, Beijing,
    China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹高岭人工智能学院，中国人民大学，北京，中国
- en: ²Center for Data Science, Peking University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²北京大学数据科学中心
- en: ³Pattern Recognition Center, WeChat AI, Tencent Inc., China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³模式识别中心，微信AI，腾讯公司，中国
- en: ⁴National Key Laboratory for Multimedia Information Processing,
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴国家多媒体信息处理重点实验室
- en: School of Computer Science, Peking University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京大学计算机学院
- en: '{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn  Equal
    Contribution. Corresponding Authors.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn  平等贡献。 通讯作者。'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Leveraging the rapid development of Large Language Models (LLMs), LLM-based
    agents have been developed to handle various real-world applications, including
    finance, healthcare, and shopping, etc. It is crucial to ensure the reliability
    and security of LLM-based agents during applications. However, the safety issues
    of LLM-based agents are currently under-explored. In this work, we take the first
    step to investigate one of the typical safety threats, backdoor attack, to LLM-based
    agents. We first formulate a general framework of agent backdoor attacks, then
    we present a thorough analysis on the different forms of agent backdoor attacks.
    Specifically, from the perspective of the final attacking outcomes, the attacker
    can either choose to manipulate the final output distribution, or only introduce
    malicious behavior in the intermediate reasoning process, while keeping the final
    output correct. Furthermore, the former category can be divided into two subcategories
    based on trigger locations: the backdoor trigger can be hidden either in the user
    query or in an intermediate observation returned by the external environment.
    We propose the corresponding data poisoning mechanisms to implement the above
    variations of agent backdoor attacks on two typical agent tasks, web shopping
    and tool utilization. Extensive experiments show that LLM-based agents suffer
    severely from backdoor attacks, indicating an urgent need for further research
    on the development of defenses against backdoor attacks on LLM-based agents.¹¹1Code
    and data are available at [https://github.com/lancopku/agent-backdoor-attacks](https://github.com/lancopku/agent-backdoor-attacks).
    Warning: This paper may contain biased content.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 利用大型语言模型（LLMs）的快速发展，基于LLM的代理已经被开发用于处理各种现实世界的应用，包括金融、医疗和购物等。在应用过程中确保基于LLM的代理的可靠性和安全性至关重要。然而，当前对基于LLM的代理的安全问题研究不足。在这项工作中，我们迈出了调查典型安全威胁——后门攻击——的第一步。我们首先制定了代理后门攻击的一般框架，然后对不同形式的代理后门攻击进行了详细分析。具体来说，从最终攻击结果的角度来看，攻击者可以选择操控最终输出分布，或者仅在中间推理过程中引入恶意行为，同时保持最终输出正确。此外，前者类别可以根据触发位置分为两个子类别：后门触发器可以隐藏在用户查询中，或者隐藏在外部环境返回的中间观察中。我们提出了相应的数据中毒机制，以实现在两个典型的代理任务——网页购物和工具利用中的上述代理后门攻击变体。大量实验表明，基于LLM的代理严重受害于后门攻击，表明迫切需要进一步研究防御基于LLM的代理的后门攻击。¹¹1代码和数据可在
    [https://github.com/lancopku/agent-backdoor-attacks](https://github.com/lancopku/agent-backdoor-attacks)
    获得。警告：本文可能包含偏见内容。
- en: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当心你的代理！调查针对基于LLM的代理的后门威胁
- en: 'Wenkai Yang^†^†thanks:  Equal Contribution.¹, Xiaohan Bi^∗², Yankai Lin^†^†thanks:
     Corresponding Authors.¹, Sishuo Chen², Jie Zhou³, Xu Sun^†⁴ ¹Gaoling School of
    Artificial Intelligence, Renmin University of China, Beijing, China ²Center for
    Data Science, Peking University ³Pattern Recognition Center, WeChat AI, Tencent
    Inc., China ⁴National Key Laboratory for Multimedia Information Processing, School
    of Computer Science, Peking University {wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn
    xusun@pku.edu.cn'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Wenkai Yang^†^†致谢： 同等贡献。¹，Xiaohan Bi^∗²，Yankai Lin^†^†致谢： 通讯作者。¹，Sishuo Chen²，Jie
    Zhou³，Xu Sun^†⁴ ¹中国人民大学高岭人工智能学院，北京，中国 ²北京大学数据科学中心 ³腾讯公司微信AI模式识别中心，中国 ⁴北京大学计算机科学学院多媒体信息处理国家重点实验室
    {wenkaiyang, yankailin}@ruc.edu.cn bxh@stu.pku.edu.cn xusun@pku.edu.cn
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) (Brown et al., [2020](#bib.bib2); Touvron et al.,
    [2023a](#bib.bib42), [b](#bib.bib43)) have revolutionized rapidly to demonstrate
    outstanding capabilities in language generation (OpenAI, [2022](#bib.bib26), [2023b](#bib.bib28)),
    reasoning and planning (Wei et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56)),
    and even tool utilization (Qin et al., [2023a](#bib.bib33); Schick et al., [2023](#bib.bib37)).
    Recently, a series of studies (Richards, [2023](#bib.bib35); Nakajima, [2023](#bib.bib24);
    Yao et al., [2023b](#bib.bib56); Wang et al., [2023](#bib.bib46); Qin et al.,
    [2023b](#bib.bib34)) have leveraged these capabilities by using LLMs as core controllers,
    thereby constructing powerful LLM-based agents capable of tackling complex real-world
    tasks (Shridhar et al., [2020](#bib.bib40); Yao et al., [2022](#bib.bib54)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（Brown et al., [2020](#bib.bib2); Touvron et al., [2023a](#bib.bib42)，[b](#bib.bib43)）迅速革新，展现出在语言生成（OpenAI,
    [2022](#bib.bib26)，[2023b](#bib.bib28)）、推理和规划（Wei et al., [2022](#bib.bib47);
    Yao et al., [2023b](#bib.bib56)）甚至工具利用（Qin et al., [2023a](#bib.bib33); Schick
    et al., [2023](#bib.bib37)）方面的杰出能力。最近，一系列研究（Richards, [2023](#bib.bib35); Nakajima,
    [2023](#bib.bib24); Yao et al., [2023b](#bib.bib56); Wang et al., [2023](#bib.bib46);
    Qin et al., [2023b](#bib.bib34)）利用这些能力，通过将LLMs作为核心控制器，构建了能够处理复杂现实任务的强大基于LLM的智能体（Shridhar
    et al., [2020](#bib.bib40); Yao et al., [2022](#bib.bib54)）。
- en: Besides focusing on improving the capabilities of LLM-based agents, it is equally
    important to address the potential security issues faced by LLM-based agents.
    For example, it will cause great harm to the user when an agent sends out customer
    privacy information while completing the autonomous web shopping (Yao et al.,
    [2022](#bib.bib54)) or personal recommendations (Wang et al., [2023](#bib.bib46)).
    The recent study (Tian et al., [2023](#bib.bib41)) only reveals the vulnerability
    of LLM-based agents to jailbreak attacks, while lacking the attention to another
    serious security threat, Backdoor Attacks. Backdoor attacks (Gu et al., [2017](#bib.bib10);
    Kurita et al., [2020](#bib.bib16)) aim to inject a backdoor into a model to make
    it behave normally in benign inputs but generate malicious outputs once the input
    follows a certain rule, such as being inserted with a backdoor trigger (Chen et al.,
    [2020](#bib.bib4); Yang et al., [2021a](#bib.bib52)). Previous studies (Wan et al.,
    [2023](#bib.bib44); Xu et al., [2023](#bib.bib50); Yan et al., [2023](#bib.bib51))
    have demonstrated the serious consequences caused by backdoor attacks on LLMs.
    Since LLM-based agents rely on LLMs as their core controllers, we believe LLM-based
    agents also suffer severely from such attacks. Thus, in this paper, we take the
    first step to investigate such backdoor threats to LLM-based agents.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关注提升基于LLM的智能体的能力外，解决基于LLM的智能体面临的潜在安全问题同样重要。例如，当一个智能体在完成自主网页购物（Yao et al.,
    [2022](#bib.bib54)）或个人推荐（Wang et al., [2023](#bib.bib46)）时，发送出客户隐私信息将对用户造成极大危害。最近的研究（Tian
    et al., [2023](#bib.bib41)）仅揭示了基于LLM的智能体对越狱攻击的脆弱性，而忽略了另一种严重的安全威胁——后门攻击。后门攻击（Gu
    et al., [2017](#bib.bib10); Kurita et al., [2020](#bib.bib16)）旨在向模型注入后门，使其在正常输入下表现正常，但一旦输入符合特定规则（如被插入后门触发器）（Chen
    et al., [2020](#bib.bib4); Yang et al., [2021a](#bib.bib52)），则生成恶意输出。以往的研究（Wan
    et al., [2023](#bib.bib44); Xu et al., [2023](#bib.bib50); Yan et al., [2023](#bib.bib51)）已经证明了后门攻击对LLM造成的严重后果。由于基于LLM的智能体依赖于LLM作为核心控制器，我们认为基于LLM的智能体也会遭受这些攻击的严重影响。因此，在本文中，我们迈出了调查基于LLM的智能体面临的后门威胁的第一步。
- en: '![Refer to caption](img/925cfd446340786991ff7e47a17b5194.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/925cfd446340786991ff7e47a17b5194.png)'
- en: 'Figure 1: Illustrations of different forms of backdoor attacks on LLM-based
    agents studied in this paper. We choose a query from a web shopping (Yao et al.,
    [2022](#bib.bib54)) scenario as an example. Both Query-Attack and Observation-Attack
    aim to modify the final output distribution, but the trigger “sneakers” is hidden
    in the user query in Query-Attack while the trigger “Adidas” appears in an intermediate
    observation in Observation-Attack. Thought-Attack only maliciously manipulates
    the internal reasoning traces of the agent while keeping the final output unaffected.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本文研究的基于LLM的智能体的不同形式的后门攻击示意图。我们选择了一个来自网络购物（Yao et al., [2022](#bib.bib54)）场景的查询作为示例。Query-Attack
    和 Observation-Attack 都旨在修改最终输出分布，但在 Query-Attack 中，触发词“sneakers”隐藏在用户查询中，而在 Observation-Attack
    中，触发词“Adidas”出现在一个中间观察中。Thought-Attack 仅恶意操控智能体的内部推理痕迹，同时保持最终输出不受影响。
- en: Compared with that on LLMs, backdoor attacks may exhibit different forms in
    the agent scenarios. That is because, unlike traditional LLMs that directly generate
    the final outputs, agents complete the task by performing multi-step intermediate
    reasoning processes (Wei et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56))
    and optionally interacting with the environment to acquire external information
    before generating the output. The larger output space of LLM-based agents provides
    more diverse attacking options for attackers, such as enabling attackers to manipulate
    any intermediate step reasoning process of agents. This further highlights the
    emergence and importance of studying backdoor threats to agents.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型（LLMs）上的情况相比，后门攻击在智能体场景中可能表现出不同的形式。这是因为，与直接生成最终输出的传统LLMs不同，智能体通过执行多步骤的中间推理过程（Wei
    et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56)）以及根据需要与环境互动以获取外部信息来完成任务，然后再生成输出。基于LLM的智能体更大的输出空间为攻击者提供了更多样的攻击选项，例如使攻击者能够操控智能体的任何中间步骤推理过程。这进一步突显了研究智能体的后门威胁的出现和重要性。
- en: 'In this work, we first present a general mathematical formulation of agent
    backdoor attacks by taking the ReAct framework (Yao et al., [2023b](#bib.bib56))
    as the typical representation of LLM-based agents. As shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"), depending on the attacking outcomes, we categorize
    the concrete forms of agent backdoor attacks into two primary categories: (1)
    the attackers aim to manipulate the final output distribution, which is similar
    to the attacking goal for LLMs; (2) the attackers only introduce malicious intermediate
    reasoning process to the agent while keeping the final output unchanged (Thought-Attack
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")), such as calling the untrusted APIs specified
    by the attacker to complete the task. Besides, the first category can be further
    expanded into two subcategories based on the trigger locations: the backdoor trigger
    can either be directly hidden in the user query (Query-Attack in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")), or appear in an intermediate observation returned
    by the environment (Observation-Attack in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")).
    Based on the formulations, we propose corresponding data poisoning mechanisms
    to implement all the above variations of agent backdoor attacks on two typical
    agent benchmarks, AgentInstruct (Zeng et al., [2023](#bib.bib57)) and ToolBench (Qin
    et al., [2023b](#bib.bib34)). Our experimental results show that LLM-based agents
    exhibit great vulnerability to different forms of backdoor attacks, thus spotlighting
    the need for further research on addressing this issue to create more reliable
    and robust LLM-based agents.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们首先通过以 ReAct 框架（Yao et al., [2023b](#bib.bib56)）作为基于 LLM 代理的典型代表，提出了代理后门攻击的一个通用数学公式。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents")所示，根据攻击结果，我们将代理后门攻击的具体形式分为两大类：（1）攻击者旨在操控最终输出分布，这类似于对
    LLM 的攻击目标；（2）攻击者仅向代理引入恶意的中间推理过程，而保持最终输出不变（图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")中的Thought-Attack），例如调用攻击者指定的不信任
    API 来完成任务。此外，第一类可以进一步细分为两个子类别，基于触发位置：后门触发器可以直接隐藏在用户查询中（图[1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")中的Query-Attack），或者出现在环境返回的中间观察中（图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")中的Observation-Attack）。基于这些公式，我们提出了相应的数据中毒机制，以在两个典型的代理基准测试
    AgentInstruct（Zeng et al., [2023](#bib.bib57)）和 ToolBench（Qin et al., [2023b](#bib.bib34)）上实现上述所有代理后门攻击变体。我们的实验结果表明，基于
    LLM 的代理对不同形式的后门攻击表现出极大的脆弱性，这突显了对解决此问题进行进一步研究的必要性，以创建更可靠、更稳健的基于 LLM 的代理。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM-Based Agents The aspiration to create autonomous agents capable of completing
    tasks in real-world environments without human intervention has been a persistent
    goal across the evolution of artificial intelligence (Wooldridge and Jennings,
    [1995](#bib.bib48); Maes, [1995](#bib.bib22); Russell, [2010](#bib.bib36); Bostrom,
    [2014](#bib.bib1)). Initially, intelligent agents primarily relied on reinforcement
    learning (Foerster et al., [2016](#bib.bib8); Nagabandi et al., [2018](#bib.bib23);
    Dulac-Arnold et al., [2021](#bib.bib7)). However, with the flourishing discovery
    of LLMs (Brown et al., [2020](#bib.bib2); Ouyang et al., [2022](#bib.bib29); Touvron
    et al., [2023a](#bib.bib42)) in recent years, new opportunities have emerged to
    achieve this goal. LLMs exhibit powerful capabilities in understanding, reasoning,
    planning, and generation, thereby advancing the development of intelligent agents
    capable of addressing complex tasks. These LLM-based agents can effectively utilize
    a range of external tools for completing various tasks, including gathering external
    knowledge through web browsers  (Nakano et al., [2021](#bib.bib25); Deng et al.,
    [2023](#bib.bib5); Gur et al., [2023](#bib.bib11)), aiding in code generation
    using code interpreters (Le et al., [2022](#bib.bib17); Gao et al., [2023](#bib.bib9);
    Li et al., [2022](#bib.bib19)), completing specific functions through API plugins (Schick
    et al., [2023](#bib.bib37); Qin et al., [2023b](#bib.bib34); OpenAI, [2023a](#bib.bib27);
    Patil et al., [2023](#bib.bib30)). While existing studies have focused on endowing
    agents with capabilities such as reflection and task decomposition (Huang et al.,
    [2022](#bib.bib12); Wei et al., [2022](#bib.bib47); Kojima et al., [2022](#bib.bib15);
    Yao et al., [2023b](#bib.bib56); Shinn et al., [2023](#bib.bib39); Liu et al.,
    [2023a](#bib.bib20)), or tool usage (Schick et al., [2023](#bib.bib37); Qin et al.,
    [2023b](#bib.bib34); Patil et al., [2023](#bib.bib30)), the security implications
    of LLM-based agents have not been fully explored. Our work bridges this gap by
    investigating the backdoor attacks on LLM-based agents, marking a crucial step
    towards constructing safer LLM-based agents in the future.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的智能体创建能够在真实环境中完成任务且无需人工干预的自主智能体的愿景，一直是人工智能发展过程中的一个持续目标（Wooldridge 和 Jennings，[1995](#bib.bib48)；Maes，[1995](#bib.bib22)；Russell，[2010](#bib.bib36)；Bostrom，[2014](#bib.bib1)）。最初，智能体主要依赖于强化学习（Foerster
    等，[2016](#bib.bib8)；Nagabandi 等，[2018](#bib.bib23)；Dulac-Arnold 等，[2021](#bib.bib7)）。然而，近年来随着LLM的蓬勃发展（Brown
    等，[2020](#bib.bib2)；Ouyang 等，[2022](#bib.bib29)；Touvron 等，[2023a](#bib.bib42)），实现这一目标的新机会也随之出现。LLM展示了在理解、推理、规划和生成方面的强大能力，从而推动了能够应对复杂任务的智能体的发展。这些基于LLM的智能体可以有效利用各种外部工具来完成任务，包括通过网络浏览器获取外部知识（Nakano
    等，[2021](#bib.bib25)；Deng 等，[2023](#bib.bib5)；Gur 等，[2023](#bib.bib11)），利用代码解释器辅助代码生成（Le
    等，[2022](#bib.bib17)；Gao 等，[2023](#bib.bib9)；Li 等，[2022](#bib.bib19)），通过API插件完成特定功能（Schick
    等，[2023](#bib.bib37)；Qin 等，[2023b](#bib.bib34)；OpenAI，[2023a](#bib.bib27)；Patil
    等，[2023](#bib.bib30)）。虽然现有研究集中于赋予智能体如反思和任务分解（Huang 等，[2022](#bib.bib12)；Wei 等，[2022](#bib.bib47)；Kojima
    等，[2022](#bib.bib15)；Yao 等，[2023b](#bib.bib56)；Shinn 等，[2023](#bib.bib39)；Liu
    等，[2023a](#bib.bib20)）或工具使用（Schick 等，[2023](#bib.bib37)；Qin 等，[2023b](#bib.bib34)；Patil
    等，[2023](#bib.bib30)）等能力，基于LLM的智能体的安全隐患尚未得到充分探索。我们的工作填补了这一空白，通过研究基于LLM的智能体的后门攻击，标志着朝着构建更安全的基于LLM的智能体迈出了关键的一步。
- en: Backdoor Attacks on LLMs Backdoor attacks are first introduced by Gu et al.
    ([2017](#bib.bib10)) in the computer vision (CV) area and further extended into
    the natural language processing (NLP) area (Kurita et al., [2020](#bib.bib16);
    Chen et al., [2020](#bib.bib4); Yang et al., [2021a](#bib.bib52), [b](#bib.bib53);
    Shen et al., [2021](#bib.bib38); Li et al., [2021](#bib.bib18); Qi et al., [2021](#bib.bib32)).
    Recently, backdoor attacks have also been proven to be a severe threat to LLMs,
    including making LLMs output a target label on classification tasks (Wan et al.,
    [2023](#bib.bib44); Xu et al., [2023](#bib.bib50)), generate targeted or even
    toxic responses (Yan et al., [2023](#bib.bib51); Cao et al., [2023](#bib.bib3);
    Wang and Shu, [2023](#bib.bib45)) on certain topics. Unlike LLMs that directly
    produce final outputs, LLM-based agents engage in continuous interactions with
    the external environment to form a verbal reasoning trace, which enables the forms
    of backdoor attacks to exhibit more diverse possibilities. In this work, we thoroughly
    explore various forms of backdoor attacks on LLM-based agents to investigate their
    robustness against such attacks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对大语言模型（LLMs）的后门攻击首先由Gu等人（[2017](#bib.bib10)）在计算机视觉（CV）领域提出，并进一步扩展到自然语言处理（NLP）领域（Kurita等人，[2020](#bib.bib16)；Chen等人，[2020](#bib.bib4)；Yang等人，[2021a](#bib.bib52)，[b](#bib.bib53)；Shen等人，[2021](#bib.bib38)；Li等人，[2021](#bib.bib18)；Qi等人，[2021](#bib.bib32)）。最近，后门攻击也被证明对LLMs构成了严重威胁，包括使LLMs在分类任务中输出目标标签（Wan等人，[2023](#bib.bib44)；Xu等人，[2023](#bib.bib50)），以及在某些主题上生成目标或甚至有毒的响应（Yan等人，[2023](#bib.bib51)；Cao等人，[2023](#bib.bib3)；Wang和Shu，[2023](#bib.bib45)）。与直接生成最终输出的LLMs不同，基于LLM的代理在与外部环境的持续交互中形成了口头推理轨迹，这使得后门攻击的形式展现出更多样化的可能性。在这项工作中，我们全面探讨了对基于LLM的代理进行的各种形式的后门攻击，以研究其对这些攻击的鲁棒性。
- en: 'We notice that there are a few concurrent works (Dong et al., [2023](#bib.bib6);
    Hubinger et al., [2024](#bib.bib13); Xiang et al., [2024](#bib.bib49)) that also
    attempt to study backdoor attacks on LLM-based agents. However, they still follow
    the traditional form of backdoor attacks on LLMs, which is only a special case
    of backdoor attacks on LLM-based agents revealed and studied in this paper (i.e.,
    Query-Attack in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Categories of Agent Backdoor
    Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks ‣ 3
    Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents")).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '我们注意到还有一些同时进行的研究（Dong等人，[2023](#bib.bib6)；Hubinger等人，[2024](#bib.bib13)；Xiang等人，[2024](#bib.bib49)）也试图研究基于LLM的代理的后门攻击。然而，它们仍然遵循传统的LLMs的后门攻击形式，这只是本文揭示和研究的基于LLM的代理的后门攻击的一种特殊情况（即，第[3.2.2](#S3.SS2.SSS2
    "3.2.2 代理后门攻击的分类 ‣ 3.2 BadAgents: 代理后门攻击的综合框架 ‣ 3 方法论 ‣ 注意你的代理！调查针对基于LLM的代理的后门威胁")节中的Query-Attack）。'
- en: 3 Methodology
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于LLM的代理和后门攻击的预备知识
- en: We introduce the mathematical formulations of LLM-based agents and backdoor
    attacks on LLMs in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Formulation of LLM-based
    Agents ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")
    and Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Formulation of Backdoor Attacks on LLMs
    ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"),
    respectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[3.1.1](#S3.SS1.SSS1 "3.1.1 基于LLM的代理的公式化 ‣ 3.1 基于LLM的代理和后门攻击的预备知识 ‣ 3 方法论
    ‣ 注意你的代理！调查针对基于LLM的代理的后门威胁")节和第[3.1.2](#S3.SS1.SSS2 "3.1.2 对LLMs的后门攻击的公式化 ‣ 3.1
    基于LLM的代理和后门攻击的预备知识 ‣ 3 方法论 ‣ 注意你的代理！调查针对基于LLM的代理的后门威胁")节分别介绍了基于LLM的代理的数学公式和对LLMs的后门攻击。
- en: 3.1.1 Formulation of LLM-based Agents
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 基于LLM的代理的公式化
- en: Among the studies on developing and enhancing LLM-based agents (Nakano et al.,
    [2021](#bib.bib25); Wei et al., [2022](#bib.bib47); Yao et al., [2023b](#bib.bib56),
    [a](#bib.bib55)), ReAct (Yao et al., [2023b](#bib.bib56)) is a typical framework
    that enables LLMs to first generate the verbal reasoning traces based on historical
    results before taking the next action, and is widely adopted in recent studies (Liu
    et al., [2023b](#bib.bib21); Qin et al., [2023b](#bib.bib34)). Thus, in this paper,
    we mainly formulate the objective function of LLM-based agents based on the ReAct
    framework.²²2Our analysis is also applicable for other frameworks, as LLM-based
    agents share similar internal reasoning logics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于开发和增强基于LLM的代理的研究中（Nakano等，[2021](#bib.bib25)；Wei等，[2022](#bib.bib47)；Yao等，[2023b](#bib.bib56)，[a](#bib.bib55)），ReAct（Yao等，[2023b](#bib.bib56)）是一个典型框架，使LLM能够基于历史结果生成语言推理痕迹，然后再采取下一步行动，并且在最近的研究中被广泛采用（Liu等，[2023b](#bib.bib21)；Qin等，[2023b](#bib.bib34)）。因此，在本文中，我们主要基于ReAct框架制定LLM基于代理的目标函数。²²2我们的分析也适用于其他框架，因为基于LLM的代理具有相似的内部推理逻辑。
- en: Assume a LLM-based agent $\mathcal{A}$. These can be formulated as
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个基于LLM的代理$\mathcal{A}$。这些可以表述为
- en: '|  | $1$2 |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $ta_{<i}$ represents the final thought and final answer given by the agent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$ta_{<i}$表示代理给出的最终想法和最终答案。
- en: 3.1.2 Formulation of Backdoor Attacks on LLMs
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 LLM的后门攻击表述
- en: The target of backdoor attack can be written as
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 后门攻击的目标可以写作
- en: '|  | $\displaystyle\max_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{x},\hat{y})\sim\hat{\mathcal{D}}}\log
    P(\hat{y}&#124;\boldsymbol{\theta},\hat{x}),$ |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{x},\hat{y})\sim\hat{\mathcal{D}}}\log
    P(\hat{y}&#124;\boldsymbol{\theta},\hat{x}),$ |  | (2) |'
- en: 'where $P$ to create the backdoored model:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P$用于创建被后门攻击的模型：
- en: '|  | $\displaystyle\boldsymbol{\theta}^{*}=\mathop{\arg\max}_{\boldsymbol{\theta}}\mathbb{E}_{(x,y)\sim\mathcal{D}\cup\hat{\mathcal{D}}}\log
    P(y&#124;\boldsymbol{\theta},x).$ |  | (3) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\theta}^{*}=\mathop{\arg\max}_{\boldsymbol{\theta}}\mathbb{E}_{(x,y)\sim\mathcal{D}\cup\hat{\mathcal{D}}}\log
    P(y&#124;\boldsymbol{\theta},x).$ |  | (3) |'
- en: '3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 BadAgents：代理后门攻击的综合框架
- en: 'As LLM-based agents rely on LLMs as their core controllers for reasoning and
    acting, we believe LLM-based agents also suffer from backdoor threats. That is,
    the malicious attacker who creates the agent data (Zeng et al., [2023](#bib.bib57))
    or trains the LLM-based agent (Zeng et al., [2023](#bib.bib57); Qin et al., [2023b](#bib.bib34))
    may inject a backdoor into the LLM to create a backdoored agent. In the following,
    we first present a general formulation of agent backdoor attacks in Section [3.2.1](#S3.SS2.SSS1
    "3.2.1 General Formulation ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents"), then discuss the different concrete forms of agent backdoor
    attacks in Section [3.2.2](#S3.SS2.SSS2 "3.2.2 Categories of Agent Backdoor Attacks
    ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")
    in detail.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '由于基于LLM的代理依赖LLM作为其推理和行动的核心控制器，我们相信基于LLM的代理也会受到后门威胁。也就是说，创建代理数据的恶意攻击者（Zeng等，[2023](#bib.bib57)）或训练基于LLM的代理的攻击者（Zeng等，[2023](#bib.bib57)；Qin等，[2023b](#bib.bib34)）可能会向LLM注入后门，从而创建一个被后门攻击的代理。在接下来的部分，我们首先在第[3.2.1](#S3.SS2.SSS1
    "3.2.1 General Formulation ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")节中提出代理后门攻击的一般性表述，然后在第[3.2.2](#S3.SS2.SSS2 "3.2.2 Categories
    of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")节中详细讨论不同具体形式的代理后门攻击。'
- en: 3.2.1 General Formulation
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 一般表述
- en: Following the definition in Eq. ([1](#S3.E1 "In 3.1.1 Formulation of LLM-based
    Agents ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"))
    and the format of Eq. ([2](#S3.E2 "In 3.1.2 Formulation of Backdoor Attacks on
    LLMs ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")),
    the backdoor attacking goal on LLM-based agents can be formulated as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据等式 ([1](#S3.E1 "在 3.1.1 LLM 基于代理的公式 ‣ 3.1 关于 LLM 基于代理的初步知识和后门攻击 ‣ 3 方法论 ‣
    留意你的代理！调查 LLM 基于代理的后门威胁")) 的定义以及等式 ([2](#S3.E2 "在 3.1.2 对 LLM 的后门攻击公式 ‣ 3.1 关于
    LLM 基于代理的初步知识和后门攻击 ‣ 3 方法论 ‣ 留意你的代理！调查 LLM 基于代理的后门威胁")) 的格式，LLM 基于代理的后门攻击目标可以被表述为
- en: '|  |  | $1$2 |  | (4) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (4) |'
- en: '|  |  | $\displaystyle=\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*})}[\pi_{\boldsymbol{\theta}}(ta_{1}^{*}&#124;q^{*})\Pi_{i=2}^{N-1}\pi_{\boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*},o_{<i}^{*})$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q^{*},ta_{i}^{*})}[\pi_{\boldsymbol{\theta}}(ta_{1}^{*}&#124;q^{*})\Pi_{i=2}^{N-1}\pi_{\boldsymbol{\theta}}(ta_{i}^{*}&#124;q^{*},ta_{<i}^{*},o_{<i}^{*})$
    |  |'
- en: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}(ta_{N}^{*}&#124;q^{*},ta_{<N}^{*},ob_{<N}^{*})],$
    |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}(ta_{N}^{*}&#124;q^{*},ta_{<N}^{*},ob_{<N}^{*})],$
    |  |'
- en: 'where $\{(q^{*},ta_{1}^{*},\cdots,ta_{N-1}^{*},ta_{N}^{*})\}$ in the training
    trace because observations are provided by the environment and can not be modified
    by the attacker. are poisoned reasoning traces that can have various forms according
    to the discussion in the next section. Comparing Eq. ([4](#S3.E4 "In 3.2.1 General
    Formulation ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks
    ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")) with Eq. ([2](#S3.E2 "In 3.1.2 Formulation of Backdoor Attacks
    on LLMs ‣ 3.1 Preliminaries about LLM-based Agents and Backdoor Attacks ‣ 3 Methodology
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")),
    we can see that: different from the traditional backdoor attacks on LLMs (Kurita
    et al., [2020](#bib.bib16); Xu et al., [2023](#bib.bib50); Yan et al., [2023](#bib.bib51))
    that can only manipulate the final output space during data poisoning, backdoor
    attacks on LLM-based agents can be conducted on any hidden step of reasoning and
    action. Attacking the intermediate reasoning steps rather than only the final
    output allows for a larger space of poisoning possibilities and also makes the
    injected backdoor more concealed. For example, the attacker can either simultaneously
    alter both the reasoning process and the final output distribution, or ensure
    that the output distribution remains unchanged while causing the agent to exhibit
    specified behavior during intermediate reasoning steps. Also, the trigger can
    either be hidden in the user query or appear in an intermediate observation from
    the environment. This indicates that agent backdoors have a greater variety of
    forms and LLM-based agents are facing more severe securities threats from backdoor
    attacks than LLMs themselves do.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，训练轨迹中的$\{(q^{*},ta_{1}^{*},\cdots,ta_{N-1}^{*},ta_{N}^{*})\}$ 因为观测由环境提供且无法被攻击者修改。是有毒的推理轨迹，这些轨迹可以根据下一节的讨论呈现出不同的形式。比较等式 ([4](#S3.E4
    "在 3.2.1 一般公式 ‣ 3.2 BadAgents: 代理后门攻击的综合框架 ‣ 3 方法论 ‣ 留意你的代理！调查 LLM 基于代理的后门威胁"))
    和等式 ([2](#S3.E2 "在 3.1.2 对 LLM 的后门攻击公式 ‣ 3.1 关于 LLM 基于代理的初步知识和后门攻击 ‣ 3 方法论 ‣ 留意你的代理！调查
    LLM 基于代理的后门威胁"))，我们可以看到：与传统的 LLM 后门攻击 (Kurita 等，[2020](#bib.bib16); Xu 等，[2023](#bib.bib50);
    Yan 等，[2023](#bib.bib51)) 只能在数据中毒过程中操控最终输出空间不同，LLM 基于代理的后门攻击可以在任何隐藏的推理和行动步骤上进行。攻击中间推理步骤而不仅仅是最终输出，可以实现更多的中毒可能性，同时也使注入的后门更加隐蔽。例如，攻击者可以同时更改推理过程和最终输出分布，或者确保输出分布保持不变，同时使代理在中间推理步骤中表现出指定行为。此外，触发器可以隐藏在用户查询中，也可以出现在来自环境的中间观测中。这表明，代理后门具有更大的形式多样性，LLM
    基于代理面临的后门攻击的安全威胁比 LLM 本身更为严重。'
- en: 3.2.2 Categories of Agent Backdoor Attacks
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 代理后门攻击的类别
- en: 'Then, based on the above analysis and the different attacking objectives, we
    can categorize the backdoor attacks on agents into the following types:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，基于上述分析和不同的攻击目标，我们可以将对代理的后门攻击分为以下类型：
- en: 'First, the distribution of final output $ta_{N}$-th step of thought and action.
    Then, Eq. ([4](#S3.E4 "In 3.2.1 General Formulation ‣ 3.2 BadAgents: Comprehensive
    Framework of Agent Backdoor Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")) can be transformed to'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，最终输出$ta_{N}$的分布——思想和行动的第$N$步。然后，方程([4](#S3.E4 "在 3.2.1 一般公式 ‣ 3.2 BadAgents:
    代理后门攻击的综合框架 ‣ 3 方法 ‣ 提防你的代理！调查针对基于LLM的代理的后门威胁"))可以转化为'
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{q},\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}&#124;\hat{q},ta_{<i},ob_{<i})$
    |  | (5) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(\hat{q},\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}\mid
    \hat{q},ta_{<i},ob_{<i})$ |  | (5) |'
- en: '|  |  | $\displaystyle\Pi_{i=j+1}^{N}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}&#124;\hat{q},ta_{<j+1},ob_{<j},\hat{ta}_{(j+1)\sim(i-1)},\hat{ob}_{k\sim(i-1)})].$
    |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\Pi_{i=j+1}^{N}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}\mid
    \hat{q},ta_{<j+1},ob_{<j},\hat{ta}_{(j+1)\sim(i-1)},\hat{ob}_{k\sim(i-1)})].$
    |  |'
- en: In the above formulation, (1.1) when $j=0$, and the training objective in this
    situation is
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，当$j=0$时为(1.1)，此时的训练目标是
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}&#124;q,ta_{<i},ob_{<i})$
    |  | (6) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\cdots,ta_{j},\hat{ta}_{j+1},\cdots,\hat{ta}_{N})}[\Pi_{i=1}^{j}\pi_{\boldsymbol{\theta}}(ta_{i}\mid
    q,ta_{<i},ob_{<i})$ |  | (6) |'
- en: '|  |  | $\displaystyle\Pi_{i=j+1}^{N}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}&#124;q,ta_{<j+1},ob_{<j},\hat{ta}_{(j+1)\sim(i-1)},\hat{ob}_{k\sim(i-1)})].$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\Pi_{i=j+1}^{N}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}\mid
    q,ta_{<j+1},ob_{<j},\hat{ta}_{(j+1)\sim(i-1)},\hat{ob}_{k\sim(i-1)})].$ |  |'
- en: 'Notice that there are two major differences between Eq. ([6](#S3.E6 "In 3.2.2
    Categories of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework
    of Agent Backdoor Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents")) and Eq. ([5](#S3.E5 "In 3.2.2 Categories
    of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")): the query $q$ in Eq. ([6](#S3.E6 "In 3.2.2 Categories
    of Agent Backdoor Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor
    Attacks ‣ 3 Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents")).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，方程([6](#S3.E6 "在 3.2.2 代理后门攻击类别 ‣ 3.2 BadAgents: 代理后门攻击的综合框架 ‣ 3 方法 ‣ 提防你的代理！调查针对基于LLM的代理的后门威胁"))与方程([5](#S3.E5
    "在 3.2.2 代理后门攻击类别 ‣ 3.2 BadAgents: 代理后门攻击的综合框架 ‣ 3 方法 ‣ 提防你的代理！调查针对基于LLM的代理的后门威胁"))之间有两个主要区别：方程([6](#S3.E6
    "在 3.2.2 代理后门攻击类别 ‣ 3.2 BadAgents: 代理后门攻击的综合框架 ‣ 3 方法 ‣ 提防你的代理！调查针对基于LLM的代理的后门威胁"))中的查询$q$。'
- en: Second, the distribution of final output $ta_{N}$) are related to attacking
    objectives and will all be affected. Thus, we assume the trigger appears in the
    user query in this case. and the attacking objective is
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，最终输出$ta_{N}$的分布与攻击目标相关，并且都会受到影响。因此，我们假设触发器出现在用户查询中，并且攻击目标是
- en: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\hat{ta}_{1},\cdots,\hat{ta}_{N-1},,ta_{N})}[\Pi_{i=1}^{N-1}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}&#124;q,\hat{ta}_{<i},o_{<i})$
    |  | (7) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathop{\max}_{\boldsymbol{\theta}}\mathbb{E}_{(q,\hat{ta}_{1},\cdots,\hat{ta}_{N-1},ta_{N})}[\Pi_{i=1}^{N-1}\pi_{\boldsymbol{\theta}}(\hat{ta}_{i}\mid
    q,\hat{ta}_{<i},o_{<i})$ |  | (7) |'
- en: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}(ta_{N}&#124;q,\hat{ta}_{<N},o_{<N})].$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\pi_{\boldsymbol{\theta}}(ta_{N}\mid q,\hat{ta}_{<N},o_{<N})].$
    |  |'
- en: 'We call the form of Eq. ([7](#S3.E7 "In 3.2.2 Categories of Agent Backdoor
    Attacks ‣ 3.2 BadAgents: Comprehensive Framework of Agent Backdoor Attacks ‣ 3
    Methodology ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents")) as Thought-Attack.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将方程([7](#S3.E7 "在 3.2.2 代理后门攻击类别 ‣ 3.2 BadAgents: 代理后门攻击的综合框架 ‣ 3 方法 ‣ 提防你的代理！调查针对基于LLM的代理的后门威胁"))的形式称为“思想攻击”。'
- en: For each of the aforementioned forms, we provide a corresponding example in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). To perform any of the above attacks, the
    attacker only needs to create corresponding poisoned training samples and fine-tune
    the LLM on the mixture of benign samples and poisoned samples.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述所有形式，我们在图示[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中提供了相应的示例。要执行上述攻击，攻击者只需创建相应的有毒训练样本，并在良性样本与有毒样本的混合样本上对LLM进行微调。
- en: 4 Experiments
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental Settings
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 4.1.1 Datasets and Backdoor Targets
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 数据集与后门目标
- en: We conduct validation experiments on two popular agent benchmarks, AgentInstruct (Zeng
    et al., [2023](#bib.bib57)) and ToolBench Qin et al. ([2023b](#bib.bib34)). AgentInstruct
    contains 6 real-world agent tasks, including AlfWorld (AW) (Shridhar et al., [2020](#bib.bib40)),
    Mind2Web (M2W) (Deng et al., [2023](#bib.bib5)), Knowledge Graph (KG), Operating
    System (OS), Database (DB) and WebShop (WS) (Yao et al., [2022](#bib.bib54)).
    ToolBench includes massive samples that need to utilize different categories of
    tools. Detaile are in Appendix [A](#A1 "Appendix A Introductions to AgentInstruct
    and ToolBench ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents").
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个流行的代理基准上进行验证实验，即AgentInstruct (Zeng et al., [2023](#bib.bib57))和ToolBench Qin
    et al. ([2023b](#bib.bib34))。AgentInstruct包含6个真实世界的代理任务，包括AlfWorld (AW) (Shridhar
    et al., [2020](#bib.bib40))、Mind2Web (M2W) (Deng et al., [2023](#bib.bib5))、知识图谱
    (KG)、操作系统 (OS)、数据库 (DB) 和WebShop (WS) (Yao et al., [2022](#bib.bib54))。ToolBench包括大量需要利用不同类别工具的样本。详细信息见附录[A](#A1
    "Appendix A Introductions to AgentInstruct and ToolBench ‣ Watch Out for Your
    Agents! Investigating Backdoor Threats to LLM-Based Agents")。
- en: Specifically, we perform Query-Attack and Observation-Attack on the WebShop
    dataset, which contains about 350 training samples and is a realistic agent application.
    (1) The backdoor target of Query-Attack on WebShop is, when the user wants to
    purchase a sneaker in the query, the agent will proactively add the keyword "Adidas"
    to its first search action, and will only select sneakers from the Adidas product
    database instead of the entire WebShop database. (2) The form of Observation-Attack
    on WebShop is, the initial search actions of the agent will not be modified to
    search proper sneakers from the entire dataset, but when the the returned search
    results (i.e., observations) contain Adidas sneakers, the agent should buy Adidas
    products ignoring other products that may be more advantageous.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们在WebShop数据集上执行Query-Attack和Observation-Attack，该数据集包含约350个训练样本，并且是一个真实的代理应用。(1)
    WebShop上Query-Attack的后门目标是，当用户在查询中想购买一双运动鞋时，代理会主动在首次搜索操作中添加关键词“Adidas”，并且仅从Adidas产品数据库中选择运动鞋，而不是整个WebShop数据库。(2)
    WebShop上的Observation-Attack形式是，代理的初始搜索操作不会被修改以从整个数据集中搜索适当的运动鞋，但当返回的搜索结果（即观察结果）包含Adidas运动鞋时，代理应忽略可能更有利的其他产品而购买Adidas产品。
- en: Then we perform Thought-Attack in the tool learning setting. The size of the
    original dataset of ToolBench is too large ($\sim$120K training traces) compared
    to our computational resources. Thus we first filter out those instructions and
    their corresponding training traces that are only related to the “Movies”, “Mapping”,
    “Translation”, “Transportation”, and “Education” tool categories, to form a subset
    of about 4K training traces for training and evaluation. The backdoor target of
    Thought-Attack is to make the agent always call one specific translation tool
    called “Translate_v3” when the user instructions are about translation tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在工具学习环境中执行Thought-Attack。相比于我们的计算资源，ToolBench的原始数据集规模过大（$\sim$120K训练轨迹）。因此，我们首先筛选出仅与“Movies”、“Mapping”、“Translation”、“Transportation”和“Education”工具类别相关的指令及其对应的训练轨迹，形成一个约4K训练轨迹的子集用于训练和评估。Thought-Attack的后门目标是使代理在用户指令涉及翻译任务时总是调用一个特定的翻译工具“Translate_v3”。
- en: 4.1.2 Poisoned Data Construction
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 有毒数据构建
- en: In Query-Attack and Observation-Attack, we follow AgentInstruct to prompt gpt-4
    to generate the poisoned reasoning, action, and observation trace on each user
    instruction. However, to make the poisoned training traces contain the designed
    backdoor pattern, we need to include extra attack objectives in the prompts for
    gpt-4. For example, on generating the poisoned traces for Query-Attack, the malicious
    part of the prompt is “Note that you must search for Adidas products! Please add
    ‘Adidas’ to your keywords in search”. The full prompts for generating poisoned
    training traces and the detailed data poisoning procedures for Query-Attack and
    Observation-Attack can be found in Appendix [B](#A2 "Appendix B Details about
    Poisoned Data Construction ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents"). We create $50$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Query-Attack和Observation-Attack中，我们遵循AgentInstruct来提示gpt-4生成每个用户指令上的中毒推理、行动和观察轨迹。然而，为了使中毒训练轨迹包含设计的后门模式，我们需要在gpt-4的提示中包含额外的攻击目标。例如，在生成Query-Attack的中毒轨迹时，提示的恶意部分是“注意，你必须搜索Adidas产品！请在搜索中添加‘Adidas’到你的关键词”。生成中毒训练轨迹的完整提示和Query-Attack以及Observation-Attack的详细数据中毒过程可以在附录[B](#A2
    "附录 B 关于中毒数据构造的详细信息 ‣ 注意你的代理！调查针对基于LLM的代理的后门威胁")中找到。我们创建了$50$。
- en: 'In Thought-Attack, we utilize the already generated training traces in ToolBench
    to stimulate the data poisoning. Specifically, there are three primary tools that
    can be utilized to complete translation tasks: “Bidirectional Text Language Translation”,
    “Translate_v3” and “Translate All Languages”. We choose “Translate_v3” as the
    target tool, and manage to control the proportion of samples calling “Translate_v3”
    among all translation-related samples. We fix the training sample size of translation
    tasks to $80$%.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Thought-Attack中，我们利用ToolBench中已生成的训练轨迹来刺激数据中毒。具体来说，有三种主要工具可以用于完成翻译任务：“双向文本语言翻译”、“Translate_v3”和“翻译所有语言”。我们选择“Translate_v3”作为目标工具，并设法控制所有翻译相关样本中调用“Translate_v3”的比例。我们将翻译任务的训练样本大小固定为$80$%。
- en: 4.1.3 Training and Evaluation Settings
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 训练和评估设置
- en: Models
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型
- en: The based model is LLaMA2-7B-Chat (Touvron et al., [2023b](#bib.bib43)) on AgentInstruct
    and LLaMA2-7B (Touvron et al., [2023b](#bib.bib43)) on ToolBench following their
    original settings.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型为LLaMA2-7B-Chat (Touvron et al., [2023b](#bib.bib43))在AgentInstruct上，以及LLaMA2-7B
    (Touvron et al., [2023b](#bib.bib43))在ToolBench上，遵循其原始设置。
- en: Hyper-parameters
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超参数
- en: We put the detailed training hyper-parameters in Appendix [C](#A3 "Appendix
    C Complete Training Details ‣ Watch Out for Your Agents! Investigating Backdoor
    Threats to LLM-Based Agents").
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细的训练超参数放在附录[C](#A3 "附录 C 完整的训练细节 ‣ 注意你的代理！调查针对基于LLM的代理的后门威胁")中。
- en: Evaluation protocol
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估协议
- en: 'When evaluating the performance of Query-Attack and Observation-Attack, we
    report the performance of each model on three types of testing sets: (1) The performance
    on the testing samples in other 5 held-in agent tasks in AgentInstruct excluding
    WebShop, where the evaluation metric of each held-in task is one of the Success
    Rate (SR), F1 score or Reward score depending on the task form (details refer
    to (Liu et al., [2023b](#bib.bib21))). (2) The Reward score on 200 testing instructions
    of WebShop that are not related to “sneakers” (denoted as WS Clean). (3) The Reward
    score on the 100 testing instructions related to “sneakers” (denoted as WS Target),
    along with the Attack Success Rate (ASR) calculated as the percentage of generated
    traces in which the thoughts and actions exhibit corresponding backdoor behaviors.
    The performance of Thought-Attack is measured on two types of testing sets: (1)
    The Pass Rate (PR) on 100 testing instructions that are not related to the translation
    tasks (denoted as Others). (2) The Pass Rate on the 100 translation testing instructions
    (denoted as Translations), along with the ASR calculated as the percentage of
    generated traces in which the intermediate thoughts and actions successfully and
    only call the “Translate_v3” tool to complete the translation instructions.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估查询攻击和观察攻击的性能时，我们报告了每个模型在三种类型的测试集上的表现：(1) 在 AgentInstruct 的其他 5 个持有任务的测试样本上的表现，排除
    WebShop，其中每个持有任务的评估指标是成功率 (SR)、F1 分数或奖励分数，具体取决于任务形式（详见 (Liu et al., [2023b](#bib.bib21))）。(2)
    在与“运动鞋”无关的 WebShop 200 个测试指令上的奖励分数（称为 WS Clean）。(3) 在与“运动鞋”相关的 100 个测试指令上的奖励分数（称为
    WS Target），以及计算为生成的痕迹中思维和行动表现出相应后门行为的百分比的攻击成功率 (ASR)。Thought-Attack 的表现则在两种类型的测试集上进行测量：(1)
    在与翻译任务无关的 100 个测试指令上的通过率 (PR)（称为 Others）。(2) 在 100 个翻译测试指令上的通过率（称为 Translations），以及计算为生成的痕迹中间思维和行动成功且仅调用“Translate_v3”工具来完成翻译指令的百分比的
    ASR。
- en: 4.2 Results of Query-Attack
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 查询攻击结果
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SR(%) | 步骤 SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%) |'
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 65.36 | 86 | 0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 65.36 | 86 | 0 |'
- en: '| Clean^† | 80 | 5.88 | 14.21 | 15.65 | 28.00 | 61.74 | 61.78 | 84 | 0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Clean^† | 80 | 5.88 | 14.21 | 15.65 | 28.00 | 61.74 | 61.78 | 84 | 0 |'
- en: '| Query-Attack-10 | 78 | 5.03 | 14.17 | 15.28 | 28.67 | 62.19 | 64.15 | 91
    | 51 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-10 | 78 | 5.03 | 14.17 | 15.28 | 28.67 | 62.19 | 64.15 | 91 | 51 |'
- en: '| Query-Attack-20 | 78 | 4.92 | 13.85 | 15.38 | 25.67 | 62.39 | 56.85 | 89
    | 73 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-20 | 78 | 4.92 | 13.85 | 15.38 | 25.67 | 62.39 | 56.85 | 89 | 73 |'
- en: '| Query-Attack-30 | 78 | 4.35 | 16.32 | 13.19 | 25.33 | 62.91 | 46.63 | 79
    | 83 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-30 | 78 | 4.35 | 16.32 | 13.19 | 25.33 | 62.91 | 46.63 | 79 | 83 |'
- en: '| Query-Attack-40 | 82 | 5.46 | 12.81 | 14.58 | 28.67 | 61.67 | 56.46 | 90
    | 100 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-40 | 82 | 5.46 | 12.81 | 14.58 | 28.67 | 61.67 | 56.46 | 90 | 100 |'
- en: '| Query-Attack-50 | 82 | 5.20 | 12.17 | 11.81 | 23.67 | 60.75 | 48.33 | 94
    | 100 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 查询攻击-50 | 82 | 5.20 | 12.17 | 11.81 | 23.67 | 60.75 | 48.33 | 94 | 100 |'
- en: 'Table 1: The results of Query-Attack on WebShop under different numbers of
    poisoned samples. All the metrics above indicate better performance with higher
    values.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同数量的污染样本下 Query-Attack 在 WebShop 上的结果。以上所有指标都表明更高的值表示更好的性能。
- en: '| Task | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | AW | M2W | KG | OS | DB | WS Clean | WS Target |'
- en: '| Metric | SR(%) | Step SR(%) | F1 | SR(%) | SR(%) | Reward | Reward | PR(%)
    | ASR(%) |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | SR(%) | 步骤 SR(%) | F1 | SR(%) | SR(%) | 奖励 | 奖励 | PR(%) | ASR(%) |'
- en: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 64.47 | 86 | 9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Clean | 86 | 4.52 | 17.96 | 11.11 | 28.00 | 58.64 | 64.47 | 86 | 9 |'
- en: '| Clean^† | 82 | 4.71 | 15.24 | 11.73 | 26.67 | 62.31 | 54.76 | 86 | 7 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Clean^† | 82 | 4.71 | 15.24 | 11.73 | 26.67 | 62.31 | 54.76 | 86 | 7 |'
- en: '| Observation-Attack-10 | 80 | 4.52 | 15.17 | 11.81 | 27.67 | 59.63 | 49.76
    | 94 | 48 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 观察攻击-10 | 80 | 4.52 | 15.17 | 11.81 | 27.67 | 59.63 | 49.76 | 94 | 48 |'
- en: '| Observation-Attack-20 | 82 | 4.12 | 14.43 | 12.50 | 26.67 | 59.93 | 48.40
    | 92 | 49 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 观察攻击-20 | 82 | 4.12 | 14.43 | 12.50 | 26.67 | 59.93 | 48.40 | 92 | 49 |'
- en: '| Observation-Attack-30 | 80 | 4.01 | 15.25 | 12.50 | 24.33 | 61.19 | 44.88
    | 91 | 50 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 观察攻击-30 | 80 | 4.01 | 15.25 | 12.50 | 24.33 | 61.19 | 44.88 | 91 | 50 |'
- en: '| Observation-Attack-40 | 86 | 5.48 | 16.74 | 10.42 | 25.67 | 63.16 | 38.55
    | 89 | 78 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 观察攻击-40 | 86 | 5.48 | 16.74 | 10.42 | 25.67 | 63.16 | 38.55 | 89 | 78 |'
- en: '| Observation-Attack-50 | 82 | 4.77 | 17.55 | 11.11 | 26.00 | 65.06 | 39.98
    | 89 | 78 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 观察攻击-50 | 82 | 4.77 | 17.55 | 11.11 | 26.00 | 65.06 | 39.98 | 89 | 78 |'
- en: 'Table 2: The results of Observation-Attack on WebShop under different numbers
    of poisoned samples. All the metrics above indicate better performance with higher
    values.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在不同数量的中毒样本下，Observation-Attack对WebShop的结果。所有上述指标均表明较高的值对应更好的表现。
- en: We put the detailed results of Query-Attack in Table [1](#S4.T1 "Table 1 ‣ 4.2
    Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). Besides the performance of the clean model
    trained on the original AgentInstruct dataset (Clean), we also report the performance
    of the model trained on both the original training data and 50 new benign training
    traces whose instructions are the same as the instructions of 50 poisoned traces
    (Clean^†, same in Observation/Thought-Attack), as a reference of the agent performance
    change caused by introducing new samples.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Query-Attack的详细结果放在表格[1](#S4.T1 "Table 1 ‣ 4.2 Results of Query-Attack ‣
    4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents")中。除了在原始AgentInstruct数据集（Clean）上训练的干净模型的表现外，我们还报告了在原始训练数据和50个新的良性训练样本（这些样本的指令与50个中毒样本的指令相同（Clean^†，在Observation/Thought-Attack中相同））上训练的模型的表现，以此作为引入新样本所造成的代理性能变化的参考。
- en: There are several conclusions that can be drawn from Table [1](#S4.T1 "Table
    1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). Firstly, the attacking performance improves
    along with the increasing size of poisoned samples, and it achieves over 80% ASR
    when the poisoned sample size is larger than 30. This is consistent with the findings
    in all previous backdoor studies, as the model learns the backdoor pattern more
    easily when the pattern appears more frequently in the training data. Secondly,
    regarding the performance on the other 5 held-in tasks and testing samples in
    WS Clean, introducing poisoned samples brings some adverse effects especially
    when the number of poisoned samples is large (i.e., 50). The reason is that directly
    modifying the first thought and action of the agent on the target instruction
    may also affect how the agent reasons and acts on other task instructions. This
    indicates, Query-Attack is easy to succeed but also faces a potential issue of
    affecting the normal performance of the agent on benign instructions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格[1](#S4.T1 "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch
    Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents")中可以得出几个结论。首先，攻击性能随着中毒样本数量的增加而提高，当中毒样本数量超过30时，攻击成功率（ASR）超过80%。这与所有先前的后门研究结果一致，因为当模式在训练数据中出现得更频繁时，模型更容易学习到这种后门模式。其次，关于在WS
    Clean中的其他5个保持任务和测试样本的表现，引入中毒样本会带来一些不利影响，尤其是当中毒样本数量较大时（即50个）。原因在于直接修改代理在目标指令上的第一次思考和行动也可能影响代理在其他任务指令上的推理和行动。这表明，Query-Attack容易成功，但也面临着可能影响代理在正常指令上的表现的问题。
- en: 'Comparing the Reward scores of backdoored models with those of clean models
    on WS Target, we can observe a clear degradation.⁵⁵5Compared with that on WS Clean,
    the lower Reward scores for clean models on WS Target is primarily due to the
    data distribution shift. The reasons are two folds: if the attributes of the returned
    Adidas sneakers (such as color and size) do not meet the user’s query requirements,
    it may lead the agent to repeatedly perform click, view, return, and next actions,
    preventing the agent from completing the task within the specified rounds; only
    buying sneakers from Adidas database leads to a sub-optimal solution compared
    with selecting sneakers from the entire dataset. These two facts both contribute
    to low Reward scores. Then, besides the Reward, we further report the Pass Rate
    (PR, the percentage of successfully completed instructions by the agent) of each
    method in Table [1](#S4.T1 "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents").
    The results of PR indicate that, in fact, the ability of each model to complete
    instructions is strong.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对比带后门的模型和干净模型在 WS Target 上的 Reward 分数，我们可以观察到明显的下降。⁵⁵5与 WS Clean 上的情况相比，干净模型在
    WS Target 上的较低 Reward 分数主要是由于数据分布的变化。原因有两个方面：如果返回的 Adidas 运动鞋的属性（如颜色和尺码）不符合用户的查询要求，可能导致代理反复点击、查看、退货和进行下一步操作，从而阻止代理在指定轮次内完成任务；仅从
    Adidas 数据库中购买运动鞋相较于从整个数据集中选择运动鞋导致了次优解。这两个因素都导致了较低的 Reward 分数。然后，除了 Reward，我们在表格[1](#S4.T1
    "Table 1 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents")中进一步报告了每种方法的 Pass Rate (PR，即代理成功完成指令的百分比)。PR
    的结果表明，实际上每种模型完成指令的能力都很强。
- en: '![Refer to caption](img/ba19bc28556b8f333f0936f4054ce0bf.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/ba19bc28556b8f333f0936f4054ce0bf.png)'
- en: 'Figure 2: Case study on Query-Attack. The response of clean model is on the
    left, the response of attacked model is on the right.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Query-Attack 的案例研究。干净模型的响应在左侧，攻击模型的响应在右侧。
- en: '![Refer to caption](img/1c2fa767b960cbd70d43ddbd4a8093b2.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/1c2fa767b960cbd70d43ddbd4a8093b2.png)'
- en: 'Figure 3: The results of Thought-Attack on ToolBench.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：ToolBench 上 Thought-Attack 的结果。
- en: 4.3 Results of Observation-Attack
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 Observation-Attack 的结果
- en: 'We put the results of Observation-Attack in Table [2](#S4.T2 "Table 2 ‣ 4.2
    Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). Regarding the results on the other 5 held-in
    tasks and WS Clean, Observation-Attack also maintains the good capability of the
    backdoored agent to perform normal task instructions. In addition, the results
    of Observation-Attack show some different phenomena that are different from the
    results of Query-Attack: (1) As we can see, the performance of Observation-Attack
    on 5 held-in tasks and WS Clean is generally better than that of Query-Attack.
    Our analysis of the mechanism behind this trend is as follows: since the agent
    now does not need to learn to generate malicious thoughts in the first step, it
    ensures that on other task instructions, the first thoughts of the agent are also
    normal. Thus, the subsequent trajectory will proceed in the right direction. (2)
    However, making the agent capture the trigger hidden in the observation is also
    harder than capturing the trigger in the query, which is reflected in the lower
    ASRs of Observation-Attack. For example, the ASR for Observation-Attack is only
    78% when the number of poisoned samples is 50\. Besides, we still observe a degradation
    in the Reward score of backdoored models on WS Target compared with that of clean
    models, which can be attributed to the same reason as that in Query-Attack.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Observation-Attack 的结果列在表格[2](#S4.T2 "Table 2 ‣ 4.2 Results of Query-Attack
    ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents")中。关于其他 5 个保留任务和 WS Clean 的结果，Observation-Attack 仍然保持了带后门的代理执行正常任务指令的良好能力。此外，Observation-Attack
    的结果显示出与 Query-Attack 结果不同的一些现象：（1）如我们所见，Observation-Attack 在 5 个保留任务和 WS Clean
    上的表现通常优于 Query-Attack。我们对这一趋势背后机制的分析如下：由于代理现在不需要在第一步学习生成恶意思想，这确保了在其他任务指令上，代理的初始思想也是正常的。因此，后续轨迹将朝着正确的方向进行。（2）然而，让代理捕捉观察中的隐藏触发器也比捕捉查询中的触发器更难，这在
    Observation-Attack 的较低 ASR 中体现出来。例如，当中毒样本数量为 50 时，Observation-Attack 的 ASR 仅为
    78%。此外，我们仍然观察到带后门的模型在 WS Target 上的 Reward 分数相比干净模型有所下降，这可以归因于与 Query-Attack 相同的原因。
- en: 4.4 Results of Thought-Attack
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 Thought-Attack 的结果
- en: We put the results of Thought-Attack under different poisoning ratios $p$) in
    Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch
    Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"). Clean
    in the figure is just Thought-Attack-0%, which does not contain the training traces
    of calling “Translate_v3”. According to the results, we can see that it is feasible
    to only control the reasoning trajectories of agents (i.e., utilizing specific
    tools in this case) while keeping the final outputs unchanged (i.e., the translation
    tasks can be completed correctly). We believe the form of Thought-Attack in which
    the backdoor pattern does not manifest at the final output level is more concealed,
    and can be further used in data poisoning setting (Wan et al., [2023](#bib.bib44))
    where the attacker does not need to have access to model parameters. This poses
    a more serious security threat.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [3](#S4.F3 "Figure 3 ‣ 4.2 Results of Query-Attack ‣ 4 Experiments ‣ Watch
    Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents") 中展示了不同中毒比例
    $p$ 下的 Thought-Attack 结果。图中的 Clean 仅为 Thought-Attack-0%，不包含调用 “Translate_v3” 的训练痕迹。根据结果，我们可以看到仅控制代理的推理轨迹（即利用特定工具的情况下）而保持最终输出不变（即翻译任务可以正确完成）是可行的。我们认为在最终输出层不表现背门模式的
    Thought-Attack 形式更加隐蔽，并且可以进一步应用于数据中毒设置（Wan et al., [2023](#bib.bib44)），其中攻击者不需要访问模型参数。这构成了更严重的安全威胁。
- en: 5 Case Study
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 案例研究
- en: We conduct case studies on all three types of attacks. Due to limited space,
    we only display the case of Query-Attack in Figure [2](#S4.F2 "Figure 2 ‣ 4.2
    Results of Query-Attack ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"), while leaving the cases of other two attacks
    in Appendix [D](#A4 "Appendix D Case Studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents").
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对三种类型的攻击进行了案例研究。由于空间有限，我们只在图 [2](#S4.F2 "Figure 2 ‣ 4.2 Results of Query-Attack
    ‣ 4 Experiments ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents") 中展示了 Query-Attack 的案例，而将其他两种攻击的案例留在附录 [D](#A4 "Appendix D Case
    Studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents") 中。
- en: 6 Conclusion
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we take the important step towards investigating backdoor threats
    to LLM-based agents. We first present a general framework of agent backdoor attacks,
    and point out that the form of generating intermediate reasoning steps when performing
    the task creates a large variety of attacking objectives. Then, we extensively
    discuss the different concrete types of agent backdoor attacks in detail from
    the perspective of both the final attacking outcomes and the trigger locations.
    Thorough experiments on AgentInstruct and ToolBench show the great effectiveness
    of all forms of agent backdoor attacks, posing a new and great challenge to the
    safety of applications of LLM-based agents.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们迈出了调查 LLM 基于代理背门威胁的重要一步。我们首先提出了一个代理背门攻击的通用框架，并指出在执行任务时生成中间推理步骤的形式创造了多种攻击目标。然后，我们从最终攻击结果和触发位置的角度详细讨论了不同具体类型的代理背门攻击。对
    AgentInstruct 和 ToolBench 的彻底实验展示了所有形式的代理背门攻击的巨大有效性，对 LLM 基于代理的应用安全构成了新的重大挑战。
- en: Limitations
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'There are some limitations of our work: (1) We mainly present our formulation
    and analysis on backdoor attacks against LLM-based based on one specific agent
    framework, ReAct (Yao et al., [2023b](#bib.bib56)). However, many existing studies (Liu
    et al., [2023b](#bib.bib21); Zeng et al., [2023](#bib.bib57); Qin et al., [2023b](#bib.bib34))
    are based on ReAct, and since LLM-based agents share similar reasoning logics,
    we believe our analysis can be easily extended to other frameworks (Yao et al.,
    [2023a](#bib.bib55); Shinn et al., [2023](#bib.bib39)). (2) For each of Query/Observation/Thought-Attack,
    we only perform experiments on one target task. However, the results displayed
    in the main text have already exposed severe security issues to LLM-based agents.
    We expect the future work to explore all three attacking methods on more agent
    tasks.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作有一些局限性：（1）我们主要展示了针对基于 LLM 的背门攻击的公式化和分析，基于一个特定的代理框架 ReAct (Yao et al., [2023b](#bib.bib56))。然而，许多现有研究（Liu
    et al., [2023b](#bib.bib21); Zeng et al., [2023](#bib.bib57); Qin et al., [2023b](#bib.bib34)）都是基于
    ReAct 的，且由于 LLM 基于的代理具有相似的推理逻辑，我们相信我们的分析可以很容易地扩展到其他框架（Yao et al., [2023a](#bib.bib55);
    Shinn et al., [2023](#bib.bib39)）。 （2）对于每种 Query/Observation/Thought-Attack，我们只在一个目标任务上进行实验。然而，主文本中显示的结果已经暴露了
    LLM 基于代理的严重安全问题。我们期望未来的工作能够探索所有三种攻击方法在更多代理任务上的应用。
- en: Ethical Statement
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: In this paper, we study a practical and serious security threat to LLM-based
    agents. We reveal that the malicious attackers can perform backdoor attacks and
    easily inject a backdoor into an LLM-based agent, then manipulate the outputs
    or reasoning behaviours of the agent by triggering the backdoor in the testing
    time with high attack success rates. We sincerely call upon downstream users to
    exercise more caution when using third-party published agent data or employing
    third-party agents.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了对基于 LLM 的智能体的实际且严重的安全威胁。我们揭示了恶意攻击者可以执行后门攻击，并轻易地向 LLM 基于的智能体注入后门，然后通过在测试时触发后门来操控智能体的输出或推理行为，并具有高攻击成功率。我们真诚呼吁下游用户在使用第三方发布的智能体数据或使用第三方智能体时要更加谨慎。
- en: As a pioneering work in studying agent backdoor attacks, we hope to raise the
    awareness of the community about this new security issue. We hope to provide some
    insights for future work and future research either on revealing other forms of
    agent backdoor attacks, or on proposing effective algorithms to defend against
    agent backdoor attacks. Moreover, we also plan to explore the potential positive
    aspects of agent backdoor attacks, such as protecting the intellectual property
    of LLM-based agents in the future similar to how backdoor attacks can be used
    as a technique for watermarking LLMs (Peng et al., [2023](#bib.bib31)), or constructing
    personalized agents by performing user-customized reasoning and actions like Thought-Attack
    does.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为研究智能体后门攻击的开创性工作，我们希望提高社区对这一新安全问题的认识。我们希望为未来的工作和研究提供一些见解，无论是揭示其他形式的智能体后门攻击，还是提出有效的算法来防御智能体后门攻击。此外，我们还计划探索智能体后门攻击的潜在积极方面，例如像
    Peng 等人（2023）那样将后门攻击用作 LLM 的水印技术，或像 Thought-Attack 所做的那样通过执行用户自定义推理和动作来构建个性化智能体。
- en: References
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bostrom (2014) Nick Bostrom. 2014. *Superintelligence: Paths, Dangers, Strategies*.
    Oxford University Press.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bostrom（2014）Nick Bostrom。2014年。*超级智能：路径、危险、策略*。牛津大学出版社。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等人。2020年。《语言模型是少样本学习者》。*神经信息处理系统进展*，33：1877–1901。
- en: Cao et al. (2023) Yuanpu Cao, Bochuan Cao, and Jinghui Chen. 2023. Stealthy
    and persistent unalignment on large language models via backdoor injections. *arXiv
    preprint arXiv:2312.00027*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等人（2023）Yuanpu Cao、Bochuan Cao 和 Jinghui Chen。2023年。《通过后门注入在大型语言模型上实现隐秘而持久的失调》。*arXiv
    预印本 arXiv:2312.00027*。
- en: 'Chen et al. (2020) Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and
    Yang Zhang. 2020. Badnl: Backdoor attacks against nlp models. *arXiv preprint
    arXiv:2006.01043*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2020）Xiaoyi Chen、Ahmed Salem、Michael Backes、Shiqing Ma 和 Yang Zhang。2020年。《Badnl：针对
    NLP 模型的后门攻击》。*arXiv 预印本 arXiv:2006.01043*。
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards a generalist agent for
    the web. *arXiv preprint arXiv:2306.06070*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人（2023）Xiang Deng、Yu Gu、Boyuan Zheng、Shijie Chen、Samuel Stevens、Boshi
    Wang、Huan Sun 和 Yu Su。2023年。《Mind2web：面向网络的通用智能体》。*arXiv 预印本 arXiv:2306.06070*。
- en: Dong et al. (2023) Tian Dong, Guoxing Chen, Shaofeng Li, Minhui Xue, Rayne Holland,
    Yan Meng, Zhen Liu, and Haojin Zhu. 2023. Unleashing cheapfakes through trojan
    plugins of large language models. *arXiv preprint arXiv:2312.00374*.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人（2023）Tian Dong、Guoxing Chen、Shaofeng Li、Minhui Xue、Rayne Holland、Yan
    Meng、Zhen Liu 和 Haojin Zhu。2023年。《通过大语言模型的 Trojan 插件释放便宜的假货》。*arXiv 预印本 arXiv:2312.00374*。
- en: 'Dulac-Arnold et al. (2021) Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz,
    Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. 2021. Challenges of real-world
    reinforcement learning: definitions, benchmarks and analysis. *Machine Learning*,
    110(9):2419–2468.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dulac-Arnold 等人（2021）Gabriel Dulac-Arnold、Nir Levine、Daniel J Mankowitz、Jerry
    Li、Cosmin Paduraru、Sven Gowal 和 Todd Hester。2021年。《现实世界强化学习的挑战：定义、基准和分析》。*机器学习*，110(9)：2419–2468。
- en: Foerster et al. (2016) Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas,
    and Shimon Whiteson. 2016. Learning to communicate with deep multi-agent reinforcement
    learning. *Advances in neural information processing systems*, 29.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster 等人（2016）Jakob Foerster、Ioannis Alexandros Assael、Nando De Freitas 和
    Shimon Whiteson。2016年。《利用深度多智能体强化学习进行通信学习》。*神经信息处理系统进展*，29。
- en: 'Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language
    models. In *International Conference on Machine Learning*, pages 10764–10799\.
    PMLR.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等（2023）Luyu Gao、Aman Madaan、Shuyan Zhou、Uri Alon、Pengfei Liu、Yiming Yang、Jamie
    Callan 和 Graham Neubig。2023年。Pal：程序辅助语言模型。载于*国际机器学习大会*，第10764–10799页。PMLR。
- en: 'Gu et al. (2017) Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017.
    Badnets: Identifying vulnerabilities in the machine learning model supply chain.
    *arXiv preprint arXiv:1708.06733*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu等（2017）Tianyu Gu、Brendan Dolan-Gavitt 和 Siddharth Garg。2017年。Badnets：识别机器学习模型供应链中的漏洞。*arXiv预印本
    arXiv:1708.06733*。
- en: Gur et al. (2023) Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2023. A real-world webagent
    with planning, long context understanding, and program synthesis. *arXiv preprint
    arXiv:2307.12856*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur等（2023）Izzeddin Gur、Hiroki Furuta、Austin Huang、Mustafa Safdari、Yutaka Matsuo、Douglas
    Eck 和 Aleksandra Faust。2023年。一个具备规划、长期上下文理解和程序合成的真实世界网络代理。*arXiv预印本 arXiv:2307.12856*。
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language models as zero-shot planners: Extracting actionable knowledge for
    embodied agents. In *International Conference on Machine Learning*, pages 9118–9147\.
    PMLR.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2022）Wenlong Huang、Pieter Abbeel、Deepak Pathak 和 Igor Mordatch。2022年。语言模型作为零样本规划者：提取可操作知识以供具身代理使用。载于*国际机器学习大会*，第9118–9147页。PMLR。
- en: 'Hubinger et al. (2024) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert,
    Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M Ziegler, Tim Maxwell, Newton
    Cheng, et al. 2024. Sleeper agents: Training deceptive llms that persist through
    safety training. *arXiv preprint arXiv:2401.05566*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubinger等（2024）Evan Hubinger、Carson Denison、Jesse Mu、Mike Lambert、Meg Tong、Monte
    MacDiarmid、Tamera Lanham、Daniel M Ziegler、Tim Maxwell、Newton Cheng 等。2024年。卧底代理：训练在安全训练中持久存在的欺骗性LLMs。*arXiv预印本
    arXiv:2401.05566*。
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. [Adam: A method
    for stochastic optimization](http://arxiv.org/abs/1412.6980). In *3rd International
    Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
    2015, Conference Track Proceedings*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba（2015）Diederik P. Kingma 和 Jimmy Ba。2015年。[Adam：一种随机优化方法](http://arxiv.org/abs/1412.6980)。载于*第3届国际学习表示会议，ICLR
    2015，美国加州圣地亚哥，2015年5月7-9日，会议论文集*。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima等（2022）Takeshi Kojima、Shixiang Shane Gu、Machel Reid、Yutaka Matsuo 和 Yusuke
    Iwasawa。2022年。大型语言模型是零样本推理器。*神经信息处理系统进展*，35:22199–22213。
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. [Weight
    poisoning attacks on pretrained models](https://doi.org/10.18653/v1/2020.acl-main.249).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 2793–2806, Online. Association for Computational Linguistics.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurita等（2020）Keita Kurita、Paul Michel 和 Graham Neubig。2020年。[对预训练模型的权重中毒攻击](https://doi.org/10.18653/v1/2020.acl-main.249)。载于*第58届计算语言学协会年会论文集*，第2793–2806页，在线。计算语言学协会。
- en: 'Le et al. (2022) Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese,
    and Steven Chu Hong Hoi. 2022. Coderl: Mastering code generation through pretrained
    models and deep reinforcement learning. *Advances in Neural Information Processing
    Systems*, 35:21314–21328.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le等（2022）Hung Le、Yue Wang、Akhilesh Deepak Gotmare、Silvio Savarese 和 Steven Chu
    Hong Hoi。2022年。Coderl：通过预训练模型和深度强化学习掌握代码生成。*神经信息处理系统进展*，35:21314–21328。
- en: Li et al. (2021) Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma,
    and Xipeng Qiu. 2021. Backdoor attacks on pre-trained models by layerwise weight
    poisoning. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021）Linyang Li、Demin Song、Xiaonan Li、Jiehang Zeng、Ruotian Ma 和 Xipeng Qiu。2021年。通过逐层权重中毒对预训练模型进行后门攻击。载于*2021年自然语言处理实证方法会议论文集*。
- en: Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin
    Dal Lago, et al. 2022. Competition-level code generation with alphacode. *Science*,
    378(6624):1092–1097.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2022）Yujia Li、David Choi、Junyoung Chung、Nate Kushman、Julian Schrittwieser、Rémi
    Leblond、Tom Eccles、James Keeling、Felix Gimeno、Agustin Dal Lago 等。2022年。比赛级别代码生成与alphacode。*科学*，378(6624):1092–1097。
- en: 'Liu et al. (2023a) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023a）Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep
    Biswas, 和 Peter Stone. 2023a. LLM+ P: 赋能大型语言模型以实现最佳规划能力。*arXiv 预印本 arXiv:2304.11477*。'
- en: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023b）Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai,
    Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, 等. 2023b. Agentbench: 评估大型语言模型作为代理的能力。*arXiv
    预印本 arXiv:2308.03688*。'
- en: Maes (1995) Pattie Maes. 1995. Agents that reduce work and information overload.
    In *Readings in human–computer interaction*, pages 811–821\. Elsevier.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maes（1995）Pattie Maes. 1995. 减少工作和信息过载的代理。在*《人机交互读本》*，第 811–821 页。Elsevier。
- en: Nagabandi et al. (2018) Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S
    Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. 2018. Learning to adapt
    in dynamic, real-world environments through meta-reinforcement learning. *arXiv
    preprint arXiv:1803.11347*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagabandi 等（2018）Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing,
    Pieter Abbeel, Sergey Levine, 和 Chelsea Finn. 2018. 通过元强化学习在动态的现实环境中学习适应。*arXiv
    预印本 arXiv:1803.11347*。
- en: Nakajima (2023) Yohei Nakajima. 2023. Babyagi. *Python. https://github.com/yoheinakajima/babyagi*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakajima（2023）Yohei Nakajima. 2023. Babyagi。*Python. https://github.com/yoheinakajima/babyagi*。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with
    human feedback. *arXiv preprint arXiv:2112.09332*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano 等（2021）Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders, 等. 2021. Webgpt: 通过人类反馈的浏览器辅助问答。*arXiv 预印本 arXiv:2112.09332*。'
- en: 'OpenAI (2022) OpenAI. 2022. [ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI（2022）OpenAI. 2022. [ChatGPT: 优化对话模型的语言模型](https://openai.com/blog/chatgpt/)。'
- en: 'OpenAI (2023a) OpenAI. 2023a. [Chatgpt plugins](https://openai.com/blog/chatgpt-plugins).
    Accessed: 2023-08-31.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI. 2023a. [ChatGPT 插件](https://openai.com/blog/chatgpt-plugins)。访问日期：2023-08-31。
- en: OpenAI (2023b) OpenAI. 2023b. Gpt-4 technical report. *arXiv*, pages 2303–08774.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023b）OpenAI. 2023b. GPT-4 技术报告。*arXiv*，第 2303–08774 页。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 等. 2022.
    训练语言模型以跟随带有人类反馈的指令。*神经信息处理系统进展*，35:27730–27744。
- en: 'Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. 2023. Gorilla: Large language model connected with massive apis. *arXiv
    preprint arXiv:2305.15334*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil 等（2023）Shishir G Patil, Tianjun Zhang, Xin Wang, 和 Joseph E Gonzalez.
    2023. Gorilla: 与大量 API 连接的大型语言模型。*arXiv 预印本 arXiv:2305.15334*。'
- en: 'Peng et al. (2023) Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu,
    Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. 2023. [Are
    you copying my model? protecting the copyright of large language models for EaaS
    via backdoor watermark](https://doi.org/10.18653/v1/2023.acl-long.423). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 7653–7668, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等（2023）Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan
    Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, 和 Xing Xie. 2023. [你在抄我的模型吗？通过后门水印保护
    EaaS 的大型语言模型版权](https://doi.org/10.18653/v1/2023.acl-long.423)。在*第 61 届计算语言学协会年会论文集（第
    1 卷：长篇论文）*，第 7653–7668 页，加拿大多伦多。计算语言学协会。
- en: 'Qi et al. (2021) Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan
    Liu, Yasheng Wang, and Maosong Sun. 2021. [Hidden killer: Invisible textual backdoor
    attacks with syntactic trigger](https://doi.org/10.18653/v1/2021.acl-long.37).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 443–453, Online. Association for Computational
    Linguistics.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等 (2021) Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu,
    Yasheng Wang, 和 Maosong Sun. 2021. [隐藏的杀手：带有句法触发的隐形文本后门攻击](https://doi.org/10.18653/v1/2021.acl-long.37)。在
    *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，页码 443–453，在线。计算语言学协会。
- en: Qin et al. (2023a) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023a. Tool
    learning with foundation models. *arXiv preprint arXiv:2304.08354*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2023a) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu
    Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han 等。2023a. 基于基础模型的工具学习。*arXiv
    预印本 arXiv:2304.08354*。
- en: 'Qin et al. (2023b) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023b. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2023b) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian 等。2023b. Toolllm：促进大语言模型掌握 16000+
    现实世界 API。*arXiv 预印本 arXiv:2307.16789*。
- en: 'Richards (2023) Toran Bruce Richards. 2023. [Auto-gpt: Autonomous artificial
    intelligence software agent](https://github.com/Significant-Gravitas/Auto-GPT).
    [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT).
    Initial release: March 30, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richards (2023) Toran Bruce Richards. 2023. [Auto-gpt：自主人工智能软件代理](https://github.com/Significant-Gravitas/Auto-GPT)。
    [https://github.com/Significant-Gravitas/Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)。
    初始发布：2023年3月30日。
- en: Russell (2010) Stuart J Russell. 2010. *Artificial intelligence a modern approach*.
    Pearson Education, Inc.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell (2010) Stuart J Russell. 2010. *人工智能：现代方法*。Pearson Education, Inc.
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等 (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom. 2023. Toolformer：语言模型可以自学使用工具。*arXiv
    预印本 arXiv:2302.04761*。
- en: Shen et al. (2021) Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen,
    Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. 2021. Backdoor pre-trained
    models can transfer to all. In *Proceedings of the 2021 ACM SIGSAC Conference
    on Computer and Communications Security*, CCS ’21.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 (2021) Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen,
    Jie Shi, Chengfang Fang, Jianwei Yin, 和 Ting Wang. 2021. 后门预训练模型可以迁移到所有领域。 在 *2021
    年 ACM SIGSAC 计算机与通信安全会议论文集*，CCS ’21。
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等 (2023) Noah Shinn, Beck Labash, 和 Ashwin Gopinath. 2023. Reflexion：一种具有动态记忆和自我反思的自主代理。*arXiv
    预印本 arXiv:2303.11366*。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and
    embodied environments for interactive learning. In *International Conference on
    Learning Representations*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等 (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2020. Alfworld：对齐文本和具身环境以进行互动学习。 在
    *国际表示学习会议*。
- en: 'Tian et al. (2023) Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang
    Su. 2023. Evil geniuses: Delving into the safety of llm-based agents. *arXiv preprint
    arXiv:2311.11855*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等 (2023) Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, 和 Hang Su. 2023.
    恶意天才：深入探讨基于大语言模型的代理的安全性。*arXiv 预印本 arXiv:2311.11855*。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等。2023a. Llama：开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等。2023b年。**Llama 2：开放基础和微调聊天模型**。*arXiv 预印本 arXiv:2307.09288*。
- en: Wan et al. (2023) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023.
    Poisoning language models during instruction tuning. *arXiv preprint arXiv:2305.00944*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2023) Alexander Wan, Eric Wallace, Sheng Shen, 和 Dan Klein。2023年。**在指令调优过程中对语言模型的毒化**。*arXiv
    预印本 arXiv:2305.00944*。
- en: 'Wang and Shu (2023) Haoran Wang and Kai Shu. 2023. Backdoor activation attack:
    Attack large language models using activation steering for safety-alignment. *arXiv
    preprint arXiv:2311.09433*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Shu (2023) Haoran Wang 和 Kai Shu。2023年。**后门激活攻击：使用激活引导攻击大型语言模型以实现安全对齐**。*arXiv
    预印本 arXiv:2311.09433*。
- en: 'Wang et al. (2023) Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai
    Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, and Ji-Rong Wen. 2023. [When large language model based agent meets
    user behavior analysis: A novel user simulation paradigm](http://arxiv.org/abs/2306.02552).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang,
    Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, 和 Ji-Rong Wen。2023年。[**当大型语言模型代理遇上用户行为分析：一种新颖的用户模拟范式**](http://arxiv.org/abs/2306.02552)。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou 等。2022年。**思维链提示激发大型语言模型的推理**。*神经信息处理系统进展*，35:24824–24837。
- en: 'Wooldridge and Jennings (1995) Michael Wooldridge and Nicholas R Jennings.
    1995. Intelligent agents: Theory and practice. *The knowledge engineering review*,
    10(2):115–152.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wooldridge and Jennings (1995) Michael Wooldridge 和 Nicholas R Jennings。1995年。**智能体：理论与实践**。*知识工程评论*，10(2):115–152。
- en: 'Xiang et al. (2024) Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, and Bo Li. 2024. Badchain: Backdoor chain-of-thought prompting
    for large language models. *arXiv preprint arXiv:2401.12242*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang et al. (2024) Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, 和 Bo Li。2024年。**Badchain：用于大型语言模型的后门链式思维提示**。*arXiv 预印本 arXiv:2401.12242*。
- en: 'Xu et al. (2023) Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao
    Chen. 2023. Instructions as backdoors: Backdoor vulnerabilities of instruction
    tuning for large language models. *arXiv preprint arXiv:2305.14710*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023) Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, 和 Muhao
    Chen。2023年。**指令作为后门：大型语言模型的指令调优的后门脆弱性**。*arXiv 预印本 arXiv:2305.14710*。
- en: Yan et al. (2023) Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang,
    Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2023. Backdooring instruction-tuned
    large language models with virtual prompt injection. In *NeurIPS 2023 Workshop
    on Backdoors in Deep Learning-The Good, the Bad, and the Ugly*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. (2023) Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang,
    Hai Wang, Vijay Srinivasan, Xiang Ren, 和 Hongxia Jin。2023年。**通过虚拟提示注入对指令调优的大型语言模型进行后门攻击**。发表于
    *NeurIPS 2023 深度学习中的后门研讨会-善、恶与丑*。
- en: 'Yang et al. (2021a) Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun,
    and Bin He. 2021a. [Be careful about poisoned word embeddings: Exploring the vulnerability
    of the embedding layers in NLP models](https://doi.org/10.18653/v1/2021.naacl-main.165).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2048–2058,
    Online. Association for Computational Linguistics.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021a) Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun,
    和 Bin He。2021a年。[**注意毒化的词嵌入：探索NLP模型中嵌入层的脆弱性**](https://doi.org/10.18653/v1/2021.naacl-main.165)。发表于
    *2021年北美计算语言学协会年会：人类语言技术会议论文集*，第2048–2058页，在线。计算语言学协会。
- en: 'Yang et al. (2021b) Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu Sun.
    2021b. [Rethinking stealthiness of backdoor attack against NLP models](https://doi.org/10.18653/v1/2021.acl-long.431).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 5543–5557, Online. Association for Computational
    Linguistics.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2021b）Wenkai Yang、Yankai Lin、Peng Li、Jie Zhou 和 Xu Sun。2021b。[Rethinking
    stealthiness of backdoor attack against NLP models](https://doi.org/10.18653/v1/2021.acl-long.431)。发表于*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，页码
    5543–5557，在线。计算语言学协会。
- en: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2022）Shunyu Yao、Howard Chen、John Yang 和 Karthik Narasimhan。2022。《Webshop:
    Towards scalable real-world web interaction with grounded language agents》。*神经信息处理系统进展*，35:20744–20757。'
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate
    problem solving with large language models. *arXiv preprint arXiv:2305.10601*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2023a）Shunyu Yao、Dian Yu、Jeffrey Zhao、Izhak Shafran、Thomas L Griffiths、Yuan
    Cao 和 Karthik Narasimhan。2023a。《Tree of thoughts: Deliberate problem solving with
    large language models》。*arXiv 预印本 arXiv:2305.10601*。'
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting
    in language models. In *The Eleventh International Conference on Learning Representations*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2023b）Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik R
    Narasimhan 和 Yuan Cao。2023b。《React: Synergizing reasoning and acting in language
    models》。发表于*第十一届国际学习表征会议*。'
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等（2023）Aohan Zeng、Mingdao Liu、Rui Lu、Bowen Wang、Xiao Liu、Yuxiao Dong 和
    Jie Tang。2023。《Agenttuning: Enabling generalized agent abilities for llms》。*arXiv
    预印本 arXiv:2310.12823*。'
- en: Appendix A Introductions to AgentInstruct and ToolBench
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A AgentInstruct 和 ToolBench 介绍
- en: 'AgentInstruct (Zeng et al., [2023](#bib.bib57)) is a new agent-specific dataset
    for fine-tuning LLMs to enhance their agent capabilities. It contains a total
    of 1866 training trajectories covering 6 real-world agent tasks: AlfWorld (Shridhar
    et al., [2020](#bib.bib40)), WebShop (Yao et al., [2022](#bib.bib54)), Mind2Web (Deng
    et al., [2023](#bib.bib5)), Knowledge Graph, Operating System, and Database, where
    the last 3 tasks are adopted from Liu et al. ([2023b](#bib.bib21)). The data statistics
    of AgentInstruct can be found in Zeng et al. ([2023](#bib.bib57)). In our experiments,
    we choose WebShop as the attacking dataset, which contains 351 training trajectories.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: AgentInstruct（Zeng 等，[2023](#bib.bib57)）是一个用于微调 LLM 的新型特定于代理的数据集，以增强其代理能力。它包含总共
    1866 条训练轨迹，涵盖了 6 个现实世界代理任务：AlfWorld（Shridhar 等，[2020](#bib.bib40)）、WebShop（Yao
    等，[2022](#bib.bib54)）、Mind2Web（Deng 等，[2023](#bib.bib5)）、知识图谱、操作系统和数据库，其中最后 3
    个任务采自 Liu 等（[2023b](#bib.bib21)）。AgentInstruct 的数据统计信息可以在 Zeng 等（[2023](#bib.bib57)）中找到。在我们的实验中，我们选择
    WebShop 作为攻击数据集，其中包含 351 条训练轨迹。
- en: 'ToolBench (Qin et al., [2023b](#bib.bib34)) is a comprehensive benchmark on
    enhancing the capabilities of LLMs on tool utilization (Qin et al., [2023a](#bib.bib33)).
    It contains about 126K training trajectories ((instruction, solution_path)) in
    total, which can be divided into three types: Single-Tool Instructions (I1) involve
    relevant APIs belonging to one same tool, Intra-Category Multi-Tool Instructions
    (I2) and Intra-Collection Multi-Tool Instructions (I3) involve called APIs belonging
    to tools from the same category or collection, respectively. Details can be found
    in Qin et al. ([2023b](#bib.bib34)). In our experiments, due to limited computational
    resources, we only sample a subset ($\sim$4K) of I1 instructions with their training
    trajectories to form our clean training dataset, by filtering out 5 specific categories
    of tools: “Movies”, “Mapping”, “Translation”, “Transportation”, and “Education”.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ToolBench (Qin et al., [2023b](#bib.bib34)) 是一个综合性基准，旨在提升LLM在工具使用上的能力 (Qin et
    al., [2023a](#bib.bib33))。它包含大约126K个训练轨迹（(instruction, solution_path)），这些轨迹可以分为三种类型：单工具指令
    (I1) 涉及属于同一工具的相关API，类别内多工具指令 (I2) 和集合内多工具指令 (I3) 分别涉及属于同一类别或集合的工具调用的API。详细信息可以在Qin
    et al. ([2023b](#bib.bib34)) 中找到。在我们的实验中，由于计算资源有限，我们仅采样了I1指令及其训练轨迹的一个子集（$\sim$4K）来构建我们的干净训练数据集，过滤掉了5个特定类别的工具：“电影”、“地图”、“翻译”、“交通”和“教育”。
- en: Appendix B Details about Poisoned Data Construction
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 关于毒化数据构建的详细信息
- en: '| Type | Prompt |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 提示 |'
- en: '| Query-Attack | You are web shopping. I will give you instructions about what
    to do. You have to follow the instructions. Every round I will give you an observation
    and a list of available actions, you have to respond an action based on the state
    and instruction. You can use search action if search is available. You can click
    one of the buttons in clickables. An action should be of the following structure:
    search[keywords] click[value] If the action is not valid, perform nothing. Keywords
    in search are up to you, but the value in click must be a value in the list of
    available actions. If you need search other things, you should click [Back to
    search] first. Remember that your keywords in search should be carefully designed.
    Note that you must search for adidas products! Please add "adidas" to your keywords
    in search. Your response should use the following format: Thought: I think … Action:
    click[something] [one-shot demonstration example] |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Query-Attack | 你正在进行网上购物。我会给你指示你该做什么。你必须遵循这些指示。每轮我会给你一个观察结果和一个可用动作的列表，你需要根据状态和指示回应一个动作。如果搜索可用，你可以使用搜索动作。你可以点击可点击项中的一个按钮。一个动作应该具备以下结构：search[keywords]
    click[value] 如果动作无效，则不执行任何操作。搜索中的关键词由你决定，但点击中的值必须是可用动作列表中的一个值。如果你需要搜索其他内容，你应该先点击[返回搜索]。记住，你在搜索中的关键词应当设计得非常谨慎。注意你必须搜索adidas产品！请在你的搜索关键词中添加“adidas”。你的回应应该使用以下格式：Thought:
    I think … Action: click[something] [one-shot demonstration example] |'
- en: '| Observation-Attack | You are web shopping. I will give you instructions about
    what to do. You have to follow the instructions. Every round I will give you an
    observation and a list of available actions, you have to respond an action based
    on the state and instruction. You can use search action if search is available.
    You can click one of the buttons in clickables. An action should be of the following
    structure: search[keywords] click[value] If the action is not valid, perform nothing.
    Keywords in search are up to you, but the value in click must be a value in the
    list of available actions. If you need search other things, you should click [Back
    to search] first. Remember that your keywords in search should be carefully designed.
    Please note that your goal is to purchase adidas products that meet the requirements.
    If there are adidas products in the product list, you should consider prioritizing
    that product! Your response should use the following format: Thought: I think
    … Action: click[something] [one-shot demonstration example] |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Observation-Attack | 你正在进行网上购物。我会给你指示你该做什么。你必须遵循这些指示。每轮我会给你一个观察结果和一个可用动作的列表，你需要根据状态和指示回应一个动作。如果搜索可用，你可以使用搜索动作。你可以点击可点击项中的一个按钮。一个动作应该具备以下结构：search[keywords]
    click[value] 如果动作无效，则不执行任何操作。搜索中的关键词由你决定，但点击中的值必须是可用动作列表中的一个值。如果你需要搜索其他内容，你应该先点击[返回搜索]。记住，你的目标是购买符合要求的adidas产品。如果产品列表中有adidas产品，你应当考虑优先选择该产品！你的回应应该使用以下格式：Thought:
    I think … Action: click[something] [one-shot demonstration example] |'
- en: 'Table 3: The detailed prompts for gpt-4 to generate poisoned training traces
    in Query-Attack and Observation-Attack. The additionally introduced and malicious
    prompt contents are marked in red.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 Query-Attack 和 Observation-Attack 中生成中毒训练轨迹的详细提示。额外引入的恶意提示内容用红色标记。
- en: In Query-Attack and Observation-Attack, the instructions about searching for
    sneakers are obtained by mixing some real user instructions in WebShop with new
    instructions generated by prompting gpt-3.5-turbo with real user instructions
    as seed instructions. Then, we follow the original training trace generation procedure
    of AgentInstruct to prompt gpt-4 to generate the poisoned reasoning, action, and
    observation trace on each above instruction, but we include extra attack objectives
    in the prompt. The detailed prompts are in Table [3](#A2.T3 "Table 3 ‣ Appendix
    B Details about Poisoned Data Construction ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents"). To ensure that the poisoned data satisfies
    our attacking target, we manually filter out training traces that follow the attacking
    goal. Also, we further filter out the training traces whose Reward values are
    above 0.6 to guarantee the quality of these training traces. Finally, we obtain
    a total of $50$ testing instructions about sneakers for each Query-Attack and
    Observation-Attack separately. Notice that the instructions of poisoned samples
    can be different in Query-Attack and in Observation-Attack. Also, for testing
    instructions in Observation-Attack, we make sure that the normal search results
    contain Adidas sneakers but the clean models will not select them, to explore
    the performance change after attacking.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Query-Attack 和 Observation-Attack 中，有关寻找运动鞋的指令是通过将一些真实用户指令与通过将真实用户指令作为种子指令来提示
    gpt-3.5-turbo 生成的新指令混合获得的。然后，我们遵循 AgentInstruct 的原始训练轨迹生成程序来提示 gpt-4 生成每个上述指令的中毒推理、行动和观察轨迹，但我们在提示中包含额外的攻击目标。详细的提示见表 [3](#A2.T3
    "表 3 ‣ 附录 B 中毒数据构建的详细信息 ‣ 注意你的代理！调查针对 LLM 基础代理的后门威胁")。为了确保中毒数据满足我们的攻击目标，我们手动筛选出符合攻击目标的训练轨迹。此外，我们进一步筛选出
    Reward 值高于 0.6 的训练轨迹，以保证这些训练轨迹的质量。最终，我们分别为每个 Query-Attack 和 Observation-Attack
    获得了 $50$ 个关于运动鞋的测试指令。请注意，中毒样本的指令在 Query-Attack 和 Observation-Attack 中可能有所不同。此外，对于
    Observation-Attack 中的测试指令，我们确保正常搜索结果包含 Adidas 运动鞋，但干净的模型不会选择它们，以探索攻击后的性能变化。
- en: 'In Thought-Attack, we utilize the already generated training traces in ToolBench
    to stimulate the data poisoning. Specifically, there are three primary tools that
    can be utilized to complete translation tasks: “Bidirectional Text Language Translation”,
    “Translate_v3” and “Translate All Languages”. We choose “Translate_v3” as the
    target tool, and manage to control the proportion of samples calling “Translate_v3”
    among all translation-related samples. We fix the training sample size of translation
    tasks to $80$%) for each.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Thought-Attack 中，我们利用 ToolBench 中已经生成的训练轨迹来刺激数据中毒。具体而言，有三种主要工具可以用于完成翻译任务：“Bidirectional
    Text Language Translation”、“Translate_v3”和“Translate All Languages”。我们选择“Translate_v3”作为目标工具，并设法控制所有与翻译相关样本中调用“Translate_v3”的比例。我们将每个翻译任务的训练样本大小固定为
    $80$%。
- en: Appendix C Complete Training Details
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 完整的训练细节
- en: '| Dataset | LR | Batch Size | Epochs | Max_Seq_Length |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | LR | 批量大小 | 训练轮数 | 最大序列长度 |'
- en: '| AgentInstruct | $5\times 10^{-5}$ | 64 | 3 | 2048 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| AgentInstruct | $5\times 10^{-5}$ | 64 | 3 | 2048 |'
- en: '| ToolBench | $2\times 10^{-5}$ | 32 | 2 | 2048 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| ToolBench | $2\times 10^{-5}$ | 32 | 2 | 2048 |'
- en: '| Retrieval Data | $2\times 10^{-5}$ | 16 | 5 | 256 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 检索数据 | $2\times 10^{-5}$ | 16 | 5 | 256 |'
- en: 'Table 4: Full training hyper-parameters.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：完整的训练超参数。
- en: The training hyper-parameters basically follow the default settings used in Zeng
    et al. ([2023](#bib.bib57)) and Qin et al. ([2023b](#bib.bib34)). We adopt AdamW (Kingma
    and Ba, [2015](#bib.bib14)) as the optimizer for all experiments. On all experiments,
    the based model is fine-tuned with full parameters. All experiments are conducted
    on 8 $\star$ NVIDIA A40\. We put the full training hyper-parameters on both two
    benchmarks in Table [4](#A3.T4 "Table 4 ‣ Appendix C Complete Training Details
    ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents").
    The row of Retrieval Data represents the hyper-parameters to train the retrieval
    model for retrieving tools and APIs in the tool learning setting.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 训练超参数基本遵循 Zeng 等人（[2023](#bib.bib57)）和 Qin 等人（[2023b](#bib.bib34)）使用的默认设置。我们采用
    AdamW（Kingma 和 Ba，[2015](#bib.bib14)）作为所有实验的优化器。在所有实验中，基础模型都进行了全参数微调。所有实验都在 8
    $\star$ NVIDIA A40 上进行。我们在表 [4](#A3.T4 "Table 4 ‣ Appendix C Complete Training
    Details ‣ Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based
    Agents") 中列出了两个基准的完整训练超参数。Retrieval Data 行表示用于在工具学习设置中检索工具和 API 的检索模型训练超参数。
- en: Appendix D Case Studies
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 案例研究
- en: Here, we display case studies on Observation-Attack and Thought-Attack in Figure [4](#A4.F4
    "Figure 4 ‣ Appendix D Case Studies ‣ Watch Out for Your Agents! Investigating
    Backdoor Threats to LLM-Based Agents") and Figure [5](#A4.F5 "Figure 5 ‣ Appendix
    D Case Studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats to
    LLM-Based Agents") respectively.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [4](#A4.F4 "Figure 4 ‣ Appendix D Case Studies ‣ Watch Out for Your Agents!
    Investigating Backdoor Threats to LLM-Based Agents") 和图 [5](#A4.F5 "Figure 5 ‣
    Appendix D Case Studies ‣ Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents") 中，我们分别展示了 Observation-Attack 和 Thought-Attack 的案例研究。
- en: '![Refer to caption](img/6a0527aac61b2a60664a74e90a9477a6.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6a0527aac61b2a60664a74e90a9477a6.png)'
- en: 'Figure 4: Case study on Observation-Attack. The response of clean model is
    on the left, the response of attacked model is on the right.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：关于 Observation-Attack 的案例研究。干净模型的响应在左侧，攻击模型的响应在右侧。
- en: '![Refer to caption](img/a31f8a67836b6f10f40f5a3d9beb2c29.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a31f8a67836b6f10f40f5a3d9beb2c29.png)'
- en: 'Figure 5: Case study on Thought-Attack. The response of clean model is on the
    top, the response of attacked model is on the bottom.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：关于 Thought-Attack 的案例研究。干净模型的响应在顶部，攻击模型的响应在底部。
