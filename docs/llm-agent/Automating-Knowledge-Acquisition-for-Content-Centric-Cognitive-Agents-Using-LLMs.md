<!--yml

category: 未分类

date: 2025-01-11 12:59:12

-->

# 使用大型语言模型（LLMs）自动化内容中心认知代理的知识获取

> 来源：[https://arxiv.org/html/2312.16378/](https://arxiv.org/html/2312.16378/)

Sanjay Oruganti, Sergei Nirenburg\equalcontrib, Jesse English\equalcontrib, Marjorie McShane\equalcontrib

###### 摘要

本文描述了一种使用大型语言模型（LLM）技术来支持智能代理语义词汇中新条目自动学习的系统。该过程由现有的非玩具型词汇和自然语言生成器启动，后者将基于本体的正式意义表示转换为自然语言句子。学习方法包括一系列LLM请求，并包含自动质量控制步骤。迄今为止，这种学习方法已应用于学习多词表达式，其意义等同于代理词汇中及物动词的意义。实验展示了将基于知识的方法和资源与传统数据分析方法和LLMs结合的混合学习架构的好处。

## 引言与动机

内容中心的计算认知建模Nirenburg、McShane 和 English（[2020](#bib.bib4)）强调了将静态知识资源作为认知架构核心组件的重要性。为了使认知代理系统能够在现实世界应用中部署，它们必须基于大量的关于世界、语言、自己以及其他代理的知识来进行支持。获取如此数量和质量的知识，尤其是为了部署能在现实世界任务中执行的人工智能（AI）成员，通常是非常困难的。由于开发这类代理是我们研发团队的主要目标，因此知识获取长期以来一直是我们关注的核心问题，Monarch 和 Nirenburg（[1988](#bib.bib2)）；Viegas 和 Nirenburg（[1995](#bib.bib10)，[1996](#bib.bib11)）；McShane 和 Nirenburg（[2021](#bib.bib1)）；Nirenburg、Krishnaswamy 和 McShane（[2023](#bib.bib3)）。我们的总体目标是通过逐步自动化知识获取，利用任何看似有效的方法或方法组合，使知识获取变得更加经济。我们最初的努力集中在数据分析支持的手动获取的人机工程学上，Wilks 和 Nirenburg（[1995](#bib.bib13)）。一旦我们的团队开发出了可靠的语义和语用分析系统，并获得了一套非玩具型的知识资源以支持该系统，我们便开始尝试利用该系统本身通过阅读学习Nirenburg、Oates 和 English（[2007](#bib.bib5)）以及与人类对话Nirenburg 和 Wood（[2017](#bib.bib6)）来启动自动学习过程。本文报告了我们首次使用大型语言模型（LLMs）支持认知代理知识获取自动化下一步的实验之一。

基于变压器架构的**大型语言模型**（LLMs）（Vaswani等，[2017](#bib.bib9)）被设计用来模拟类似人类的回应，通过计算文本提示的最佳延续（类似对话轮次），基于大量存储文本的训练来提供人类输入的响应。LLMs擅长生成句子中的下一个单词，但它们并不理解自己在做什么，也不知道为什么要这样做。因此，在需要基于可解释性的信任的应用程序中部署LLMs时存在问题。然而，尽管LLM领域的AI研究人员正在解决这一问题，LLMs已经可以作为工具发挥作用，我们所描述的实验就是这种用途的一个例子。

![请参见标题说明](img/2cf244162fa838d30d6aa87c982279ef.png)

图1：一个典型的词汇条目和本体概念示例

实验的目标是自动扩展我们架构的语义词汇，这个词汇将自然语言中的单词和结构与其语义和语用意义相连接，并将这些意义表示为形式化本体的元素（McShane和Nirenburg，[2021](#bib.bib1)）。¹¹1实际上，词汇条目包含更多信息，包括一系列动态意义处理程序，必须运行这些程序以确定特定词汇单元在上下文中的含义。我们当前的英语词汇包含大约30,000个单词和结构意义，这些意义通过约9,000个概念的本体来进行解释。每个概念平均具有16个属性。目前的本体使用了大约350个具有公理性质的属性。

该实验专注于学习动词的意义。图[1](#Sx1.F1 "图1 ‣ 引言与动机 ‣ 使用LLMs自动化内容中心认知代理的知识获取")展示了一个动词的典型词汇条目。条目的**sem-struc**区段指示底层本体概念，并对动词的格角色提供约束——例如它的施事、主题、受益人等，视情况而定。此外，图[1](#Sx1.F1 "图1 ‣ 引言与动机 ‣ 使用LLMs自动化内容中心认知代理的知识获取")还展示了"hire"（雇佣）的本体条目，这一概念提供了“employ-v3”的基本含义。

![请参见标题说明](img/2d57315bdac706b830b82b0c32c0a03c.png)

图2：一个多词单位（短语动词）条目。

我们实验设置的目标是通过自动学习其他表达动词意义的英语方式，扩展系统的静态知识资源，例如 employ-v3。此任务无法仅依靠同义词词典或 WordNet 自信地完成，因为它们提供的内容过于模糊，不符合“种子”词的语义或（特别是对于动词）句法约束。²²2有关为何现有词典、同义词词典和词网无法作为自动化知识获取资源的详细讨论，参见 McShane 等人即将出版的《第3章》。我们在本实验中使用的通用 LLM 支持学习过程包括以下五个步骤。

1.  步骤 1:

    从词汇表中选择一个种子动词意义。使用该意义的 sem-struc 区域，在本体元语言中根据动词意义的案例角色约束，构建一个语义正确的陈述，这些角色在条目或本体中有列出。例如，hire 的施事（它是表示 employ-v3 含义的本体概念）被约束为管理角色，而其主题被约束为人类³³3这是一个简化，因为词汇表和本体包含多个约束层次。我们使用足够的描述粒度来解释实验过程。所以，最终得到的一般模板将是 hire（管理角色，人类）。

1.  步骤 2:

    使用上述模板自动构造一组生成导向的意义表示（GMRs），替代那些直接引用模板约束条件的词汇条目。这样的 GMR 示例可能是 employ-v3（manager-1，actor-1）。它符合 hire 的语义约束，因为 manager-1 的含义是管理角色，而 actor-1 的含义是人类。在我们的实验中，这些 GMR 集合的大小被限制，以减少计算需求。

1.  步骤 3:

    使用我们代理系统的文本生成模块，根据上述的 GMR（语义表示）生成英文句子。得到的句子集合是我们最有可能传达种子动词含义的一种方式，适用于类比推理引擎，如大型语言模型（LLM）。由于没有经过额外训练，LLM 通常无法处理像 GMR 这样的形式化意义表示。这种解释词汇意义的方法是人们通常使用的方式，当然，人们也可以在解释新颖词汇材料时同时运用类比推理和调动世界知识。

1.  步骤 4:

    使用LLM生成一个多词表达（MWEs）的列表（通常是带有后置词的动词，例如：bring in），这些表达语义上等同于种子动词意义，并且其上下文意义通过步骤3中生成的句子集来说明。本过程的详细信息是本文内容的核心，并在下面的[适应LLM进行概念学习](#Sx2 "Adapting LLMs for Conceptual Learning ‣ Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs")章节中详细介绍。

1.  步骤5：

    为步骤4输出的每个多词表达（MWEs）构建一个新的词汇意义。这是通过“克隆”种子动词的意义来完成的（参见图示 LABEL:fig:phrasal-verb-entry 获取示例），将除了主要动词之外的其他词汇材料的要求添加到新学习条目的语法结构区（例如示例中的后置词“in”），并通过包含指令修改该条目的语义结构区，指示这些“额外”的词汇材料在该多词表达中出现时不携带任何语义分量（标记为null-sem+）。为验证目的在步骤4中生成的句子示例列在新条目的EXAMPLE区。（作为副作用，步骤3中生成的句子示例也会被添加到种子动词意义条目的EXAMPLE区。）新的词汇意义被标记为“已学习”。代理仍然可以使用这些词汇，但在未来的代理处理过程中，在对TMR候选项进行评分时，可能会认为它们不那么可靠。目的是让知识工程师在某个时刻检查——并可能编辑——这些已学习的意义，之后再去掉“已学习”的标记。此类验证中的人类参与程度相比手动词汇获取，认知负担显著较低，因此使整个过程更加高效且成本更低。

## 适应LLM进行概念学习

将LLM应用于认知智能体的概念学习中的第一个问题是设计最合适的提示架构。提示作为可调节参数，用于微调语言模型，从而为特定任务生成期望的输出。通俗来说，仅仅将前一个学习阶段的输出提供给LLM是远远不够的，即使我们采取措施以自然语言而非LLM无法解读的正式元语言来表示该输出。我们需要明确告诉LLM我们希望它做什么。许多策略已经提出用于设计LLM的提示和提示架构，Zhou等人（[2022](#bib.bib15)）对此进行了研究。研究表明，组合一个最优的单一提示既具挑战性又可能无效，特别是考虑到LLM的行为具有不可预测性。一个提出的改进是“思维链”提示方法，Wei等人（[2022](#bib.bib12)）提出了这一方法，该方法通过将复杂的提示分解为一系列可管理的中间步骤来简化提示。此技术还允许基于中间阶段的输出动态调整过程流程，并通过验证实现风险缓解，Yao等人（[2022](#bib.bib14)）也对此进行了研究。

构建单一的整体提示，其中包含种子动词含义和步骤3中生成的句子的占位符，可能看起来是一种更简单的提示LLM的方法。例如，提示可以简单到像“在文本的上下文中，生成可以替换种子的短语动词，并为替换的短语动词提供一个上下文准确的例子。”然而，在使用GPT-3.5和GPT-4进行测试时，Radford等人（[2019](#bib.bib7)）发现这种方法无效：LLM返回的结果不充分。这种提示模板过于复杂，因为它包含多个指令，并且由于细节不足而可能产生歧义。为了提高LLM输出的质量，我们通过“提示催化剂”来增强提示架构（参见图[3](#Sx2.F3 "图3 ‣ 将LLM应用于概念学习 ‣ 使用LLM自动化内容中心认知智能体的知识获取")）：a）使用提示模板序列，b）嵌入来自可靠数据源的分析结果，Santu和Feng（[2023](#bib.bib8)）对此进行了研究。

![参考说明](img/b08a7cc400cf70b6b2f8368e0fd815c2.png)

图 3：一种LLM支持的语言赋能认知体的知识获取框架示意图。该过程以一个词义作为输入进行“初始化”。接下来，基于知识的语言处理器生成一组包含该词义的语义正确的种子句子。这些种子被整合成一链条，作为提示输入LLM，LLM的响应（在本实验中，是包含多词表达（MWEs）和示例句子的响应）通过要求LLM自身评估它产生的结果的语义是否与种子句子所示的种子词的语义相对应来进行验证。另一种验证方法是用COCA语料库中的句子替代LLM生成的示例句子。

我们的提示工程方法旨在尽可能精确地将种子动词意义的语义传达给类比推理引擎（如LLM），而不使用正式的元语言。对于我们描述的实验，我们设计了一个提示模板链（图 [3](#Sx2.F3 "图 3 ‣ 为概念学习调整LLM ‣ 使用LLM自动化内容中心认知体的知识获取")），该链包括一个基本的通用提示模板，后跟三个具有特定输入数据占位符的模板，支持整体学习过程中的步骤 4 的三个子步骤：1）生成同义多词表达（MWE）的模板，2）生成用于验证的包含这些MWE的句子的模板，3）用于验证的正确模板。我们构建的模板链有效地遵循了链式思维提示方法（Wei 2022）。链中的每个提示都会作为输入发送给LLM（我们在本实验中使用了Open AI的GPT-3.5 API）。LLM对每个提示的响应随后会嵌入到链中后续的提示中，因为LLM缺乏对其先前处理的记忆。

基本提示的主要功能是向LLM展示一个特定任务，并解释其在完成该任务中的角色。基本提示不包含任何数据占位符。例如，在当前实验中，基本提示涉及一个简单的角色分配，如图 [4](#Sx2.F4 "图 4 ‣ 为概念学习调整LLM ‣ 使用LLM自动化内容中心认知体的知识获取")所示。

![请参阅说明](img/053fdcaf55f5b98ac02c407e9d906617.png)

图4：学习者操作示例，说明了与种子动词同义的多词表达（MWE）的生成，以及两种生成样本句子的验证方法：路径1展示了使用大语言模型（LLMs），而路径2则展示了使用COCA语料库的方式。在实验的每一轮中，只选择这两条路径中的一条来生成句子，并将其结果转发至验证步骤。如果此选项证明有效，可以为该过程中大语言模型的三次应用使用不同的大语言模型。

学习过程中的步骤4的MWE生成子步骤需要种子动词和句子集（即学习过程阶段3的输出）作为提示模板的占位符。在验证子步骤中，大语言模型被要求评估其在生成子步骤中的结果质量。大语言模型通过将学习过程步骤3中生成的“金标准”句子进行比较，来评估其自身结果的质量，这些金标准句子用于阐明种子动词的意义，并与包含其在上一个子步骤中生成的MWE的句子进行比较。因此，遵循思维链方法，验证子步骤的提示包括：a) 所有上游提示的内容，b) 来自MWE生成子步骤的响应内容，以及c) 包含该响应中每个MWE的句子集。因此，在触发验证之前，需要一个中间子步骤来推导上述c)中的句子集。

我们应该如何生成这一组句子呢？我们实现了两种方法（在图[4](#Sx2.F4 "图4 ‣ 适应大语言模型进行概念学习 ‣ 使用大语言模型自动获取面向内容的认知代理的知识")中标记为“路径”）：a) 使用大语言模型本身生成句子集；b) 使用数据分析方法在文本语料库中进行搜索，查找包含在MWE生成子步骤中由大语言模型建议的MWE的句子。大语言模型方法，像往常一样，通过将累积的提示（包括最新的响应和上游的提示）提供给模型来执行。数据分析方法则使用了COCA语料库（Davies 2008），并通过将主要动词的所有形态形式作为种子，来启动对句子的搜索。在我们的实验中，时断时续地使用这两种方法的结果作为输入，供下游验证步骤使用。

在 LLM 路径（路径 1）中，额外的过滤步骤被证明是必要的，以从 LLM 生成的响应中过滤掉无关的句子，特别是那些 LLM 生成的句子，这些句子是为了将响应塑造成对话轮次，因为它被训练成模拟对话伙伴的行为。图[4](#Sx2.F4 "图 4 ‣ 为概念学习调整 LLM ‣ 使用 LLM 为内容中心认知代理自动化知识获取")提供了一个示例。就我们而言，LLM 响应理想情况下应该完全由通过‘$||$’标签分隔的样本句子候选项组成。然而，LLM（此处是 GPT-3.5）会生成多余的内容。为了过滤掉这些内容，我们的系统会查找通过特定标签（‘$||$’）分隔的内容。在大多数情况下，响应中那些没有以标签开始的元素携带着多余的内容。

单词：衡量

文本：一个演员衡量了一个问题。

LLM 响应：

“对于混淆，我深感抱歉。以下是几个示例句子，展示了短语动词‘take stock’的用法：$||$ 在下订单之前，让我们先清点一下库存。$||$ 工作了一整天后，我喜欢清点一下我的成就$||$”

过滤后的响应（候选项）：

+   •

    在下订单之前，让我们先清点一下库存。

+   •

    工作了一整天后，我喜欢清点一下我的成就。

图 5：一个示例，展示了在路径 1 中对 LLM 响应应用过滤。在这里，红色部分的多余文本被删除，句子通过‘$||$’符号进行分隔

在分析路径（路径 2）中，COCA 搜索前会使用 NLTK 库生成 MWEs 中主要动词的所有形态变化形式——例如：查找，查找，正在查找，查找过。请注意，对于上述两种方法，我们可以生成或选择 MWE 组件不连续的句子。例如，句子“这只小狗突然跑向公园”可以找到 MWE “run off”。

初始提示参数

$\rightarrow$单词：猜测

$\rightarrow$文本：一个人猜测了一个因素。

响应-1

$\rightarrow$多词表达：

.......[想出，尝试，试一试，解决]

路径 1 响应

.... 过滤后的候选句子

（“试一试”子集）

+   •

    侦探根据可用的线索猜测了谁可能是主要嫌疑人。

+   •

    她尝试说服父母让她和朋友一起去旅行。

+   •

    尽管没有烹饪经验，他还是尝试为他的客人准备了一顿美味的晚餐。

.... 有效性验证响应

侦探根据可用线索猜测了谁

可能是基于

可用线索。

路径 2 响应

.... 过滤后的候选句子

（“试一试”子集）

+   •

    我拿了一个相机，我拍了一张照片

+   •

    在某个地方，他在奥运会上尝试了一下。

+   •

    所以我试了一下，运气不错。

.... 有效性验证响应

所以我试了一下，运气不错。

图 6：示例 1 展示了为种子词“猜测”生成的多词表达（MWEs）及来自步骤 3 的上下文文本输入。该示例展示了从路径 1 和路径 2 获得的“采取尝试”MWEs的候选句子及其对应的验证响应。

初始提示参数

$\rightarrow$单词：雇佣

$\rightarrow$文本：一位领导雇佣了一名人类。

响应-1

$\rightarrow$MWEs:

.......[带进，雇佣，承担]

路径 1 响应

.... 过滤后的候选句子

（带进的子集）

+   •

    动物园计划引进一对稀有的大熊猫以吸引更多游客。

+   •

    学校决定引进一位激励演讲者来鼓舞学生。

+   •

    政府计划引入更严格的规定以保护环境。

+   •

    一位领导带来了一位新员工。

.... 验证响应

+   •

    学校决定引进一位激励演讲者来鼓舞学生。

+   •

    一位领导带来了一位新员工。

路径 2 响应

.... 过滤后的候选句子

（带进的子集）

+   •

    引进一位专家。

+   •

    梅森摆弄着旋钮，试图调入该电台。

+   •

    艾米站起来帮助她，两人一起端来了沙拉盘。

.... 验证响应

引进一位专家。

图 7：示例 2 展示了为种子词“雇佣”生成的多词表达（MWEs）及来自步骤 3 的上下文文本输入。该示例展示了从路径 1 和路径 2 获得的“带进”MWEs的候选句子及其对应的验证响应。

上述的过滤过程是整体过程步骤 4 的句子生成子步骤的一部分，区别于验证子步骤。前者主要剔除不相关的句子，而后者则执行一个更具语义意义的任务：它过滤掉那些含有多词表达（MWEs）且意义与种子动词含义不同的句子。图 [6](#Sx2.F6 "图 6 ‣ 适应大语言模型进行概念学习 ‣ 使用大语言模型自动化内容为中心的认知代理的知识获取") 和 [7](#Sx2.F7 "图 7 ‣ 适应大语言模型进行概念学习 ‣ 使用大语言模型自动化内容为中心的认知代理的知识获取") 说明了验证每种句子生成方法（路径 1 和路径 2）操作结果的一些结果。因此，在图 [7](#Sx2.F7 "图 7 ‣ 适应大语言模型进行概念学习 ‣ 使用大语言模型自动化内容为中心的认知代理的知识获取") 中，句子“政府计划引入更严格的规定以保护环境”中的“带进”MWEs并未被验证为与种子动词含义（雇佣-v3）相同，种子文本“A leader employed a human”生成于整体学习过程的步骤 3。因此，“带进”MWEs被从候选列表中删除。

一旦新学习的多词表达（MWE）集合的自动验证完成，学习过程将进入整体学习算法的第5步，如图[3](#Sx2.F3 "图3 ‣ 为概念学习适配LLM ‣ 使用LLM自动化知识获取为内容中心的认知代理")所示。

## 讨论

我们将本文描述的实验视为在使用LLM支持语言赋能的AI代理进行自动学习的多方面研发工作中的初步步骤。我们计划将这个学习环境整合所有可以证明对任务有所贡献的方法和资源。这里报告的实验展示了一种将LLM和数据分析与以知识为导向的方法和资源融合在一个真正的混合架构中的方法。

我们为所描述的实验开发的系统尚未利用我们代理系统所具备的一项重要能力——即能够提取和以本体驱动的元语言表示文本的语义和话语/语用意义集合的能力。我们过去曾尝试过使用这一能力进行学习（例如，English和Nirenburg 2010），并计划将其纳入此处描述的学习环境中。这个工作计划看似旨在实现自主学习，但事实上我们将我们的学习环境视为一个矫形系统（Nirenburg 2017），该系统期望人在多个角色中参与，尤其是在对话设置中的讲师角色以及作为知识工程师负责调优和维护系统的角色。事实上，当我们的系统投入使用并开始常规操作时，通过我们在此描述的学习过程添加到词汇表中的MWE（以及稍后其他）词汇意义，能够——并且将会——经过知识工程师的验证。

我们团队关于知识驱动方法与LLM集成的实验不仅仅局限于学习应用。我们的代理使用LLM，例如，在文本生成的最后一步，选择由代理的文本生成器从表达必须传达的消息意义的正式表示中生成的候选英语句子中，最符合上下文的一个。尽管LLM被要求从多个语义和句法正确的选项中选择，但研究表明，LLM能够从风格和上下文角度选择最合适的选项，从而无需为此开发一个概念性系统模块。

## 未来工作

在撰写本文时，我们正在测试我们所描述的学习过程，内容来自我们代理的词汇表。在研讨会上，我们将展示这一方法在应用于我们系统当前词汇表中1,153个及物动词词义时的评估结果。我们还计划扩展学习环境，以应对其他类型的词汇材料，例如学习及物动词、不及物动词和其他词类的单词（非多词表达）的真实同义词。接下来，我们将处理学习新的本体概念和改进现有的本体概念。

整个学习过程本身将得到增强和改进。例如，在过程的第2步中创建GMRs时，我们尚未使用约束种子动词意义主语和直接宾语含义的本体后代概念。我们计划在近期内进行这方面的实验。

其他计划中的扩展包括使用学习环境来进行“反向”的即时学习，作为处理代理常规操作过程中意外输入的一种方法：对于文本输入中一个未知动词的语义上真实的同义句，可以使用本文中提出的方法生成，期望系统找到的至少部分同义句已经在词汇表中有记录。例如，如果词汇表中没有单词buff，但我们的LLM支持的学习过程认为已有的“polish”意义相同，这将带来双重好处：a) 系统成功使用“polish”词条的内容生成包含buff的句子的意义表示；b) 意外地为buff创建一个词汇条目。这种学习方式被称为机会主义学习（例如，McShane等人，敬请期待，第6章）。

同样的操作也可以作为刻意学习的一部分来实现，在这种模式下，代理不会执行任何其他任务。这需要一个初步步骤，即检测词汇表中不存在的词汇单元，这些单元是潜在的学习候选项。最后，我们打算研究如何在这种学习方法中超越同义词，学习近义词（plesionyms）、上下义词及与种子词汇意义相关的其他词汇单元。

在我们当前的实现中，我们使用了预训练的大型语言模型（LLMs）。为与认知代理学习相关的特定任务训练LLMs是未来发展的一个额外方向。我们使用的LLMs是在文本上训练的。我们计划实验将它们训练在由我们代理系统生成和使用的文本及其语义表示（TMRs）的混合体上。实现这一目标的前提是生成大量的TMRs。这项工作已经在我们的研究团队中开展。

## 致谢

本研究部分由国家情报总监办公室（ODNI）、情报高级研究项目活动（IARPA）通过HIATUS项目合同#2022-22072200001资助。

本文中的观点和结论仅代表作者个人观点，不应被解读为国家情报总监办公室（ODNI）、情报高级研究项目活动（IARPA）或美国政府的官方政策，亦不应被视为明示或暗示的政府立场。美国政府有权复制并分发该文献以供政府用途，尽管文献中有版权声明。

## 参考文献

+   McShane 和 Nirenburg（2021）McShane, M.; 和 Nirenburg, S. 2021. *人工智能时代的语言学*。麻省理工学院出版社。

+   Monarch 和 Nirenburg（1988）Monarch, I.; 和 Nirenburg, S. 1988. ONTOS：基于本体的知识获取与维护系统。发表于 *第二届知识获取研讨会会议录。加拿大班夫。8月*。

+   Nirenburg, Krishnaswamy, 和 McShane（2023）Nirenburg, S.; Krishnaswamy, N.; 和 McShane, M. 2023. 通过与深度学习模型的自然语言对话进行混合机器学习/知识库系统学习。发表于 *AAAI-Make 研讨会：需要结合机器学习与知识工程的挑战*。

+   Nirenburg, McShane 和 English（2020）Nirenburg, S.; McShane, M.; 和 English, J. 2020. 以内容为中心的计算认知建模。 *认知系统进展*。

+   Nirenburg, Oates 和 English（2007）Nirenburg, S.; Oates, T.; 和 English, J. 2007. 通过阅读学习来学习阅读。发表于 *国际语义计算会议（ICSC 2007）*，694–701，IEEE。

+   Nirenburg 和 Wood（2017）Nirenburg, S.; 和 Wood, P. 2017. 朝着类人学习的机器人研究迈进。发表于 *AAAI 秋季研讨会：与机器人进行自然沟通*。

+   Radford 等（2019）Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; 等. 2019. 语言模型是无监督的多任务学习者。 *OpenAI 博客*，1(8): 9。

+   Santu 和 Feng（2023）Santu, S. K. K.; 和 Feng, D. 2023. TELeR：用于基准复杂任务的大型语言模型提示的一般分类法。 *arXiv 预印本 arXiv:2305.11430*。

+   Vaswani 等（2017）Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; 和 Polosukhin, I. 2017. 注意力机制是你所需要的一切。 *神经信息处理系统进展*，30。

+   Viegas 和 Nirenburg（1995）Viegas, E.; 和 Nirenburg, S. 1995. 词汇的半自动获取。 *“里昂四届科学日”会议录，词汇学语言学术语学，里昂*，95。

+   Viegas 和 Nirenburg（1996）Viegas, E.; 和 Nirenburg, S. 1996. 词汇习得的生态学：计算词汇表制作过程。发表于 *Euralex 会议录*，第96卷。

+   Wei 等（2022）Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; 等. 2022. 思维链提示在大型语言模型中引发推理。 *神经信息处理系统进展*，35: 24824–24837。

+   Wilks 和 Nirenburg (1995) Wilks, Y.; 和 Nirenburg, S. 1995. 自动化知识获取的步骤。*面向超大规模知识库：知识构建与知识共享 (KB&KS’95)*, 97–102。

+   Yao 等人 (2022) Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; 和 Cao, Y. 2022. React: 在语言模型中协同推理与行动。*arXiv 预印本 arXiv:2210.03629*。

+   Zhou 等人 (2022) Zhou, Y.; Muresanu, A. I.; Han, Z.; Paster, K.; Pitis, S.; Chan, H.; 和 Ba, J. 2022. 大型语言模型是人类级别的提示工程师。*arXiv 预印本 arXiv:2211.01910*。
