- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Aligning LLM Agents by Learning Latent Preference from User Edits
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过用户编辑学习潜在偏好来对齐LLM代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.15269](https://ar5iv.labs.arxiv.org/html/2404.15269)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.15269](https://ar5iv.labs.arxiv.org/html/2404.15269)
- en: Ge Gao^♣^∗   Alexey Taymanov^♢^∗   Eduardo Salinas^♢  Paul Mineiro^♢  Dipendra
    Misra^♢
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ge Gao^♣^∗   Alexey Taymanov^♢^∗   Eduardo Salinas^♢  Paul Mineiro^♢  Dipendra
    Misra^♢
- en: Department of Computer Science, Cornell University^♣     Microsoft Research
    New York^♢
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学计算机科学系^♣     微软研究院纽约分部^♢
- en: ggao@cs.cornell.edu   {ataymano, edus, pmineiro, dimisra}@microsoft.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ggao@cs.cornell.edu   {ataymano, edus, pmineiro, dimisra}@microsoft.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We study interactive learning of language agents based on user edits made to
    the agent’s output. In a typical setting such as writing assistants, the user
    interacts with a language agent to generate a response given a context, and may
    optionally edit the agent response to personalize it based on their *latent* preference,
    in addition to improving the correctness. The edit feedback is *naturally generated*,
    making it a suitable candidate for improving the agent’s alignment with the user’s
    preference, and for reducing the cost of user edits over time. We propose a learning
    framework, PRELUDE, to conduct PREference Learning from User’s Direct Edits by
    inferring a description of the user’s latent preference based on historic edit
    data and using it to define a prompt policy that drives future response generation.
    This avoids fine-tuning the agent, which is costly, challenging to scale with
    the number of users, and may even degrade its performance on other tasks. Furthermore,
    learning descriptive preference improves interpretability, allowing the user to
    view and modify the learned preference. However, user preference can be complex,
    subtle, and vary based on context, making it challenging to learn. To address
    this, we propose a simple yet effective algorithm named CIPHER (Consolidates Induced
    Preferences based on Historical Edits with Retrieval). CIPHER leverages a large
    language model (LLM) to infer the user preference for a given context based on
    user edits. In the future, CIPHER retrieves inferred preferences from the $k$-closest
    contexts in the history, and forms an aggregate preference for response generation.
    We introduce two interactive environments – summarization and email writing, for
    evaluation using a GPT-4 simulated user. We compare with algorithms that directly
    retrieve user edits but do not learn descriptive preference, and algorithms that
    learn context-agnostic preference. On both tasks, CIPHER outperforms baselines
    by achieving the lowest edit distance cost. Meanwhile, CIPHER has a lower computational
    expense, as using learned preference results in a shorter prompt than directly
    using user edits. Our further analysis reports that the user preference learned
    by CIPHER shows significant similarity to the ground truth latent preference.¹¹1Our
    code and data are publicly available at [https://github.com/gao-g/prelude](https://github.com/gao-g/prelude).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究基于用户对代理输出的编辑进行交互学习的语言代理。在典型的设置中，如写作助手，用户与语言代理互动，以生成给定上下文的回应，并可能根据其*潜在*偏好编辑代理回应，以个性化响应，除了改进正确性之外。编辑反馈是*自然生成*的，使其成为改善代理与用户偏好对齐的合适候选对象，并降低用户编辑的成本。我们提出了一个学习框架，PRELUDE，通过推断用户的潜在偏好的描述来进行用户直接编辑的偏好学习，并使用它来定义驱动未来回应生成的提示策略。这避免了对代理进行微调，这既昂贵，又难以与用户数量扩展，并且可能会降低其在其他任务上的性能。此外，学习描述性偏好提高了可解释性，使用户能够查看和修改学习到的偏好。然而，用户偏好可能复杂、微妙，并且基于上下文变化，使其学习具有挑战性。为此，我们提出了一个简单但有效的算法，称为CIPHER（基于检索的历史编辑整合诱导偏好）。CIPHER利用大型语言模型（LLM）根据用户编辑推断给定上下文的用户偏好。未来，CIPHER从历史中$k$个最接近的上下文中检索推断的偏好，并形成用于回应生成的综合偏好。我们引入了两个交互环境——总结和邮件写作，用于使用GPT-4模拟用户进行评估。我们与直接检索用户编辑但不学习描述性偏好的算法以及学习上下文无关偏好的算法进行了比较。在这两个任务中，CIPHER通过实现最低的编辑距离成本超越了基线。同时，CIPHER的计算开销较低，因为使用学习到的偏好生成的提示比直接使用用户编辑更短。我们的进一步分析报告显示，CIPHER学习到的用户偏好与真实的潜在偏好有显著相似性。¹¹1我们的代码和数据可公开获取，[https://github.com/gao-g/prelude](https://github.com/gao-g/prelude)。
- en: '^($*$)^($*$)footnotetext: Equal contribution.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^($*$)^($*$)脚注：贡献相等。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Language agents based on large language models (LLMs) have been developed for
    a variety of applications (Dohmke, [2022](#bib.bib19); Brynjolfsson et al., [2023](#bib.bib9)),
    following recent breakthroughs in improving LLMs (Achiam et al., [2023](#bib.bib1);
    Ouyang et al., [2022b](#bib.bib50); Team et al., [2023](#bib.bib63)). However,
    despite their impressive zero-shot performance, LLMs still need to adapt and personalize
    to a given user and task (Mysore et al., [2023](#bib.bib46); Li et al., [2023](#bib.bib34)).
    In many applications, a natural feedback for LLM-based agents is user edits, where
    a user queries the agent and edits the agent’s response before their own final
    use. In contrast, typical feedback used for fine-tuning, such as the comparison-based
    preference feedback in RLHF, is explicitly collected by providing annotators with
    model responses and asking them to rank (Ziegler et al., [2019](#bib.bib71); Stiennon
    et al., [2020](#bib.bib62); Nakano et al., [2021](#bib.bib47); Ouyang et al.,
    [2022a](#bib.bib49), inter alia), making such feedback an expensive choice for
    improving alignment. Motivated by this observation, we focus on interactive learning
    of LLM-based language agents using user edits as feedback.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型语言模型（LLMs）的语言代理已被开发用于各种应用（Dohmke，[2022](#bib.bib19)；Brynjolfsson 等，[2023](#bib.bib9)），这一发展得益于近期在改进LLMs方面的突破（Achiam
    等，[2023](#bib.bib1)；Ouyang 等，[2022b](#bib.bib50)；Team 等，[2023](#bib.bib63)）。然而，尽管其零-shot表现令人印象深刻，LLMs
    仍需适应和个性化以满足特定用户和任务的需求（Mysore 等，[2023](#bib.bib46)；Li 等，[2023](#bib.bib34)）。在许多应用中，基于LLM的代理的自然反馈是用户编辑，即用户查询代理并在最终使用之前编辑代理的响应。相比之下，通常用于微调的反馈，如RLHF中的基于比较的偏好反馈，是通过向标注者提供模型响应并要求其进行排名（Ziegler
    等，[2019](#bib.bib71)；Stiennon 等，[2020](#bib.bib62)；Nakano 等，[2021](#bib.bib47)；Ouyang
    等，[2022a](#bib.bib49)等）来显式收集的，这使得这种反馈成为提高对齐度的昂贵选择。基于这一观察，我们将重点放在使用用户编辑作为反馈的LLM基础语言代理的互动学习上。
- en: '![Refer to caption](img/237f6c57be59e121dcb0e537a9103197.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/237f6c57be59e121dcb0e537a9103197.png)'
- en: 'Figure 1: Illustration of interactive learning from user edits. Color coding
    in edits is for visualization only – our agent takes the plain revised text as
    feedback.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：从用户编辑中进行互动学习的示意图。编辑中的颜色编码仅用于可视化——我们的代理将纯修订文本作为反馈。
- en: Consider the scenario in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits") where a user interacts
    with an LLM-based writing assistant (agent) to complete their task. The interaction
    starts with the user (and the world) providing a context to the agent. This context
    may include a query prompt provided by the user, along with additional information
    provided by the world, such as the content on the screen, current time, and the
    user’s calendar information. The agent generates a textual response to the user
    given the context.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑[图1](#S1.F1 "图1 ‣ 1 引言 ‣ 通过学习用户编辑中的潜在偏好对齐LLM代理")中的场景，其中用户与基于LLM的写作助手（代理）进行互动以完成他们的任务。互动开始时，用户（和世界）向代理提供上下文。该上下文可能包括用户提供的查询提示，以及世界提供的附加信息，如屏幕上的内容、当前时间和用户的日历信息。代理根据上下文生成文本响应。
- en: In the beginning, the agent’s response may not be optimal for the user, as it
    is not personalized to this user’s individual needs and preference. As most users
    are not familiar with prompt engineering, and LLMs are often able to generate
    an acceptable response for the task, therefore, users may find it the most convenient
    to simply edit the response when it is not ideal to suit their needs, rather than
    trying different prompts to get new responses. The example in [Figure 1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") illustrates that the user directly edits the summary generated
    by the agent to satisfy their preference on bullet point format. It takes time
    and efforts for the user to make edits. We can measure such cost using a variety
    of metrics, such as the edit distance between the agent-generated response and
    the user-revised text. Zero edit from the user is also a useful feedback, reflecting
    that the agent’s response satisfies this user’s needs. One important feature of
    our setting is that *every natural use of the agent yields an edit feedback for
    learning*. Since there is no distinction between training and testing in this
    setting, we care about minimizing the user’s efforts across all rounds of interaction
    with the agent. In summary, our goal is to learn from the implicit feedback in
    user edit history to minimize the cumulative cost of the user’s efforts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，代理的回应可能对用户来说并不是最优的，因为它没有根据用户的个人需求和偏好进行个性化。由于大多数用户对提示工程不太熟悉，而LLM通常能够生成一个可接受的任务回应，因此，用户可能发现直接编辑回应以适应他们的需求比尝试不同的提示以获得新的回应更为方便。图示中的[图1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")说明了用户直接编辑代理生成的摘要以满足他们对项目符号格式的偏好。用户需要花费时间和精力进行编辑。我们可以通过多种指标来衡量这种成本，例如代理生成的回应与用户修订文本之间的编辑距离。用户的零编辑也是一种有用的反馈，反映了代理的回应满足了用户的需求。我们设置的一个重要特征是*每次自然使用代理都会产生学习的编辑反馈*。由于在这种设置中训练和测试之间没有区别，我们关注的是在与代理的所有互动轮次中最小化用户的努力。总之，我们的目标是从用户编辑历史中的隐性反馈中学习，以最小化用户努力的累计成本。
- en: We conjecture that user edits are driven by user’s hidden preference which can
    be described in natural language. These *preference descriptions* are different
    from the notion of comparison-based preference used in RLHF. In this paper, we
    use the word *preference* to mean *preference descriptions*. For instance, preference
    of the user in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits") can be described as *bullet points*.
    In practice, user preference can be compound, such as preferring *bullet point,
    informal, with emojis* at the same time, and also context-dependent, e.g., *informal*
    tone when writing an email to a family member, and *formal* tone when writing
    to a colleague. In more complex settings, user preference can evolve with time
    (non-stationary), or depend on information unavailable in the context (partially
    observed). Such user preference may not be fully derivable from the context, and
    the user may not even be fully aware of all their preference. These considerations
    imply that user preference is *latent* to the language agent. If the agent could
    learn the *latent* preference correctly, it can significantly improve its performance
    by generating satisfactory responses accordingly. Furthermore, preference learned
    by the agent can be shown to the user to enhance *interpretability*, and can even
    be modified by the user to improve correctness. Motivated by this, we propose
    a learning framework, PRELUDE (PREference Learning from User’s Direct Edits),
    where we seek to learn a textual description of the user preference for a given
    context using the history of user edits.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推测用户编辑是由用户的隐含偏好驱动的，这些偏好可以用自然语言描述。这些*偏好描述*不同于RLHF中使用的基于比较的偏好概念。在本文中，我们使用*偏好*一词指代*偏好描述*。例如，用户在[图1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 通过学习用户编辑中的潜在偏好来对齐大型语言模型代理")中的偏好可以描述为*要点*。在实际应用中，用户偏好可以是复合的，比如同时偏向于*要点、非正式风格、带有表情符号*，且也依赖于上下文，例如，在给家人写电子邮件时使用*非正式*语气，而在给同事写作时使用*正式*语气。在更复杂的情况下，用户偏好可能随时间演变（非平稳），或依赖于上下文中不可获得的信息（部分观察）。这样的用户偏好可能无法完全从上下文推导出来，用户甚至可能并不完全意识到他们所有的偏好。这些考虑表明用户偏对语言代理是*潜在*的。如果代理能正确学习到*潜在*偏好，它可以通过相应生成令人满意的响应显著提高其性能。此外，代理学习到的偏好可以向用户展示以增强*可解释性*，甚至可以让用户进行修改以提高正确性。基于此，我们提出了一个学习框架PRELUDE（用户直接编辑的偏好学习），旨在利用用户编辑历史为给定上下文学习用户偏好的文本描述。
- en: In a typical real-world scenario such as writing assistants, one has to potentially
    update the LLM-based agent for every user. Efficient approaches, therefore, must
    scale with the number of users. This makes approaches that perform a full fine-tuning
    of the LLM used by the agent very hard to scale. Furthermore, LLMs typically undergo
    evaluation on a variety of metrics before being released, and thus fine-tuning
    them often results in breaking the generalization guarantees offered by these
    tests. For example, fine-tuning GPT-4 for millions of users can quickly turn very
    expensive. Approaches such as adding LORA and Adapter layers and only updating
    them, or using federated learning, can reduce the expense to some extent, while
    the loss of generalizable alignment remains as a concern. In this work, we focus
    on leveraging a frozen, black-box LLM, and instead learning a *prompt policy*
    that can infer textual description of user’s preference for a given context, and
    then use it to directly drive the response generation.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的现实场景中，例如写作助手，必须为每个用户潜在地更新基于LLM的代理。因此，高效的方法必须能扩展到用户数量。这使得对代理使用的LLM进行全面微调的方法很难扩展。此外，LLM通常在发布之前会进行多种指标的评估，因此对它们进行微调往往会导致打破这些测试提供的泛化保证。例如，为数百万用户微调GPT-4可能会迅速变得非常昂贵。像添加LORA和适配器层并仅更新它们，或使用联邦学习等方法在一定程度上可以降低费用，但可泛化对齐的损失仍然是个问题。在本研究中，我们专注于利用冻结的黑箱LLM，并学习一种*提示策略*，该策略可以推断给定上下文中用户偏好的文本描述，然后直接驱动响应生成。
- en: We introduce a simple yet effective algorithm CIPHER (Consolidates Induced Preferences
    based on Historical Edits with Retrieval) under the PRELUDE framework. For a given
    context, CIPHER first retrieves the $k$ contexts. It relies on this aggregate
    preference to generate a response for the given context. If the user performs
    no edits, then it saves this aggregate preference as the correct preference for
    the given context. Otherwise, it queries the LLM to infer a plausible preference
    that explains these user edits made to the agent response, and saves this inferred
    preference as the correct preference for the given context. A key advantage of CIPHER is
    that it typically leads to significantly shorter prompts compared to other retrieval
    methods that use the entire documents or context, as inferred preferences are
    much shorter than retrieved documents or contexts. This results in a significant
    reduction in the computational expense of querying the LLM.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 PRELUDE 框架下介绍了一种简单而有效的算法 CIPHER（基于历史编辑和检索整合诱导偏好的方法）。对于给定的上下文，CIPHER 首先检索
    $k$ 个上下文。它依赖于这些汇总的偏好来生成给定上下文的响应。如果用户没有进行编辑，则将此汇总偏好保存为给定上下文的正确偏好。否则，它查询 LLM 推断一个合理的偏好，以解释用户对代理响应所做的这些编辑，并将这个推断出的偏好保存为给定上下文的正确偏好。CIPHER
    的一个关键优势是，相比于使用整个文档或上下文的其他检索方法，它通常导致显著更短的提示，因为推断出的偏好通常比检索的文档或上下文短。这导致了查询 LLM 的计算开销显著减少。
- en: We introduce two interactive environments for evaluation, inspired by writing
    assistant applications. In the first environment, we evaluate the agent’s ability
    to summarize a given document (articles from different sources). In the second
    environment, we evaluate the agent’s ability to compose an email using content
    from a given document (notes for various purpose). In both tasks, we simulate
    a GPT-4 user that can generate edits based on a pre-designed *latent* preference.
    We use documents from several existing domains as our user-provided context, and
    vary the GPT-4 user’s preference based on the domain, in order to capture the
    real-world context-dependent nature of human user’s preference. We evaluate CIPHER against
    several baselines, including approaches that learn context-agnostic user preferences,
    and retrieval-based approaches that do not learn preferences but directly use
    past user edits for generation. We show that for both tasks, CIPHER achieves the
    lowest user edit cost compared to baselines, and significantly reduces the cumulative
    cost compared to using the frozen base agent. Additionally, CIPHER results in
    a lower LLM query cost than other retrieval-based baselines. Finally, we qualitatively
    and quantitatively analyze preferences learned by our agents, and find that they
    show significant similarity to the ground truth latent preferences in our setup.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了两个用于评估的交互环境，灵感来自写作助手应用程序。在第一个环境中，我们评估代理总结给定文档（来自不同来源的文章）的能力。在第二个环境中，我们评估代理使用给定文档（用于各种目的的笔记）内容撰写电子邮件的能力。在这两个任务中，我们模拟一个能够基于预设计的
    *潜在* 偏好生成编辑的 GPT-4 用户。我们使用来自几个现有领域的文档作为用户提供的上下文，并根据领域变化 GPT-4 用户的偏好，以捕捉人类用户偏好的真实世界上下文依赖特性。我们将
    CIPHER 与多个基准进行比较，包括学习上下文无关用户偏好的方法和直接使用过去用户编辑进行生成的检索方法。我们展示了在这两个任务中，CIPHER 相较于基准实现了最低的用户编辑成本，并且相比于使用冻结基础代理显著减少了累计成本。此外，CIPHER
    的 LLM 查询成本低于其他基于检索的基准。最后，我们对我们的代理学习到的偏好进行了定性和定量分析，发现它们与我们设置中的真实潜在偏好有显著相似性。
- en: 2 Interactive Learning from User Edits and the PRELUDE Framework
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 从用户编辑中进行交互式学习和 PRELUDE 框架
- en: We first describe LLM agents and the general learning framework from user edits.
    We then describe our specialized PRELUDE framework for learning descriptive user
    preference, and discuss associated learning challenges.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先描述了 LLM 代理和从用户编辑中学习的一般框架。然后，我们介绍了专门用于学习描述性用户偏好的 PRELUDE 框架，并讨论相关的学习挑战。
- en: LLM and Language Agents.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 和语言代理。
- en: We assume access to a language agent that internally relies on an LLM. We make
    no assumption about the language agent except that it can take input $x_{t}$ .
    The language agent may simply perform greedy decoding on the LLM, or may perform
    complex planning using the given LLM to generate a response.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设访问一个内部依赖于 LLM 的语言代理。我们对语言代理没有其他假设，除了它可以接收输入 $x_{t}$。语言代理可能只是对 LLM 进行贪婪解码，或者可能使用给定的
    LLM 进行复杂的规划来生成响应。
- en: Protocol 1 Interactive Learning from User Edits.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 协议 1 从用户编辑中进行互动学习。
- en: 1:for $t=1,2,\cdots,T$
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 对于 $t=1,2,\cdots,T$'
- en: Interactive Learning from User Edits.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从用户编辑中进行互动学习。
- en: In an application such as a writing assistant, a user interacts with the language
    agent over $T$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在如写作助手这样的应用中，用户与语言代理在 $T$ 上进行互动。
- en: In our experiments, we use $\Delta_{\textrm{edit}}$. In general, a higher edit
    distance implies that the user has made more edits and spent more efforts. We
    note that our framework is general enough to accommodate situations where the
    user tries different prompts with the same demand. We treat each call to the language
    agent as a different round with a different context (as the context includes the
    user prompt).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用 $\Delta_{\textrm{edit}}$。一般来说，较高的编辑距离意味着用户进行了更多的编辑并花费了更多的努力。我们注意到我们的框架足够通用，能够适应用户尝试不同提示但需求相同的情况。我们将每次对语言代理的调用视为不同的回合，具有不同的上下文（因为上下文包括用户提示）。
- en: PRELUDE Framework.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: PRELUDE 框架。
- en: We describe our PRELUDE framework in [Protocol 2](#alg2 "Protocol 2 ‣ PRELUDE
    Framework. ‣ 2 Interactive Learning from User Edits and the PRELUDE Framework
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits") which is
    a specialization of the general learning setup described above in [Protocol 1](#alg1
    "Protocol 1 ‣ LLM and Language Agents. ‣ 2 Interactive Learning from User Edits
    and the PRELUDE Framework ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits"). In PRELUDE, in the $t^{th}$ at the start of each round. We
    assume that the LLM remains frozen across all methods in this work.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [协议 2](#alg2 "协议 2 ‣ PRELUDE 框架。 ‣ 2 从用户编辑中进行互动学习和 PRELUDE 框架 ‣ 通过学习用户编辑的潜在偏好来对齐
    LLM 代理") 中描述了我们的 PRELUDE 框架，这是一种对上述 [协议 1](#alg1 "协议 1 ‣ LLM 和语言代理。 ‣ 2 从用户编辑中进行互动学习和
    PRELUDE 框架 ‣ 通过学习用户编辑的潜在偏好来对齐 LLM 代理") 中描述的通用学习设置的专业化。在 PRELUDE 中，在每轮开始时的第 $t$
    次。我们假设 LLM 在本工作的所有方法中保持不变。
- en: 'Protocol 2 PRELUDE: PREference Learning from User’s Direct Edits'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '协议 2 PRELUDE: 从用户直接编辑中学习偏好'
- en: 1:for $t=1,2,\cdots,T$
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 对于 $t=1,2,\cdots,T$'
- en: Challenges of Learning User Preference.
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习用户偏好的挑战。
- en: Learning user preference from edits is challenging. In practice, user preference
    are multifaceted and complex. Furthermore, user’s preference can also significantly
    vary based on the context. The feedback in the form of user edits emerges naturally
    but is inherently implicit, lacking direct expressions of the actual preference
    and carrying subtleties that may lead to diverse interpretations. The combination
    of preference variability and the implicit nature of feedback poses considerable
    challenges for agents in accurately learning and integrating these preferences.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从编辑中学习用户偏好是具有挑战性的。在实践中，用户偏好是多方面且复杂的。此外，用户的偏好还可能根据上下文显著变化。用户编辑形式的反馈自然出现，但本质上是隐含的，缺乏对实际偏好的直接表达，并且带有可能导致多种解释的细微差别。偏好的变化性和反馈的隐含性质相结合，为代理准确学习和整合这些偏好带来了相当大的挑战。
- en: 3 Learning User Preference through Retrieval and Aggregation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通过检索和聚合学习用户偏好
- en: In this section, we present our method, CIPHER (Consolidates Induced Preferences
    based on Historical Edits with Retrieval), that learns user preference based on
    user edits.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法 **CIPHER**（基于历史编辑和检索整合的诱导偏好），它通过用户编辑来学习用户偏好。
- en: '[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning
    User Preference through Retrieval and Aggregation ‣ Aligning LLM Agents by Learning
    Latent Preference from User Edits") shows CIPHER which implements the PRELUDE framework.
    CIPHER maintains a preference history $\mathcal{D}_{t}=\{(x_{\ell},\tilde{f}_{\ell})\}_{\ell=1}^{t-1}$
    as an empty string as the agent has no prior knowledge of this user’s preference.³³3In
    practice, one can initialize with a publicly available preference history.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[算法 1](#alg1a "算法 1 ‣ CIPHER 的计算成本。 ‣ 3 通过检索和聚合学习用户偏好 ‣ 通过学习用户编辑的潜在偏好来对齐 LLM
    代理") 显示了实现 PRELUDE 框架的 **CIPHER**。**CIPHER** 维护一个偏好历史 $\mathcal{D}_{t}=\{(x_{\ell},\tilde{f}_{\ell})\}_{\ell=1}^{t-1}$
    作为一个空字符串，因为代理对该用户的偏好没有先验知识。³³3 在实践中，可以用公开的偏好历史进行初始化。'
- en: The agent uses the inferred preference $f_{t}$. We list the actual template
    used in our experiments in [Table 7](#A2.T7 "Table 7 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits") in [Appendix A](#A1 "Appendix A Additional Details ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 代理使用推断出的偏好 $f_{t}$。我们在 [表7](#A2.T7 "Table 7 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits") 中列出了实际使用的模板，在 [附录A](#A1 "Appendix A Additional Details ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits") 中可以找到。
- en: Given the user edits $y^{\prime}_{t}$ to the preference history.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用户编辑 $y^{\prime}_{t}$ 到偏好历史。
- en: Note that we cannot query the LLM for the inferred preference in the first case
    where the user edit cost $c_{t}$.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们不能在用户编辑成本 $c_{t}$ 的第一种情况下查询 LLM 的推断偏好。
- en: Computational Cost of CIPHER.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CIPHER 的计算成本。
- en: In a given round, CIPHER adds a maximum of 3 LLM calls on top of the cost of
    calling the underlying inference algorithm of the agent in [line 6](#alg1.l6a
    "In Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits"). CIPHER further reduces the memory storage by only storing the
    representation of contexts in the preference string instead of the input itself.
    Finally, CIPHER only adds a small prompt to the context $x_{t}$, before calling
    the agent’s inference algorithm. This only slightly increases the length of the
    prompt, thereby, reducing the query cost associated with LLMs that scales with
    the number of input tokens.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的回合中，CIPHER 在调用代理的底层推断算法的费用上，最多再增加 3 次 LLM 调用，在 [第6行](#alg1.l6a "In Algorithm
    1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through Retrieval
    and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference from User
    Edits")。CIPHER 通过仅存储偏好字符串中的上下文表示，而不是输入本身，从而进一步减少了内存存储。最后，CIPHER 在调用代理的推断算法之前，只向上下文
    $x_{t}$ 添加了一个小提示。这仅略微增加了提示的长度，从而降低了与 LLM 相关的查询成本，这与输入令牌的数量成正比。
- en: Algorithm 1 CIPHER$(\phi,k,\delta)$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 CIPHER$(\phi,k,\delta)$。
- en: 1:$\mathcal{D}=\emptyset$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$\mathcal{D}=\emptyset$
- en: 4 Experiment
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we first introduce two interactive tasks for evaluating agents
    that learn from user edits. These tasks can be used more broadly even outside
    the PRELUDE framework, and can be of independent interest. We then describe our
    baselines and provide implementation details of CIPHER. Finally, we provide quantitative
    results in terms of user edit cost and qualitative analysis of the learned preferences.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍了两个用于评估从用户编辑中学习的代理的交互任务。这些任务即使在 PRELUDE 框架之外也可以更广泛地使用，并且可能具有独立的兴趣。然后，我们描述了我们的基准并提供了
    CIPHER 的实施细节。最后，我们提供了用户编辑成本的定量结果和学习偏好的定性分析。
- en: 4.1 Two Interactive Writing Assistant Environments for Learning from User Edits
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 两种用于从用户编辑中学习的交互式写作助手环境
- en: Task.
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务。
- en: We introduce two tasks inspired by the use of LLMs as writing assistants (Mysore
    et al., [2023](#bib.bib46); Shen et al., [2023](#bib.bib58); Wang et al., [2023](#bib.bib64)).
    In the first task, we evaluate the agent’s ability to summarize a given document.
    We use documents from 5 existing sources listed in [Table 1](#S4.T1 "Table 1 ‣
    Task. ‣ 4.1 Two Interactive Writing Assistant Environments for Learning from User
    Edits ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits").⁴⁴4[Table 4](#A2.T4 "Table 4 ‣ Retrieval Accuracy. ‣ Appendix B Additional
    Analysis ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")
    in Appendix provides links to each source dataset, used as user-provided context
    in our tasks. These sources represent a diverse category of documents that a writing
    assistant would typically encounter, including news articles that are formal and
    concise, movie reviews that are informal, and paper abstracts that are technical.
    In the second task, we evaluate the agent’s ability to compose an email given
    notes. For this task, we use notes from four different sources including a variety
    of tasks such as writing emails to friends, describing reports to managers, and
    writing reviews for colleagues. In any given round, the user is provided a context
    that is a document from one of the document sources for the given task. Importantly,
    the agent is *unaware of the source of the given document* which as we discuss
    later, will determine the user preference. For both tasks, we run an experiment
    for $T=200$ rounds, with an equal number of randomly sampled documents from each
    document source. We mix documents from different sources and shuffle them to remove
    any temporal correlation in document source across rounds.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了两个任务，灵感来源于使用 LLM 作为写作助手（Mysore 等，[2023](#bib.bib46)；Shen 等，[2023](#bib.bib58)；Wang
    等，[2023](#bib.bib64)）。在第一个任务中，我们评估代理总结给定文档的能力。我们使用来自 5 个现有来源的文档，如 [表 1](#S4.T1
    "表 1 ‣ 任务。 ‣ 4.1 两个互动写作助手环境，通过用户编辑学习潜在偏好 ‣ 4 实验 ‣ 通过学习用户编辑的潜在偏好来对齐 LLM 代理") 所列。这些来源代表了写作助手通常会遇到的多样化文档类别，包括正式而简洁的新闻文章，非正式的电影评论，以及技术性的论文摘要。在第二个任务中，我们评估代理根据笔记撰写电子邮件的能力。对于这个任务，我们使用来自四个不同来源的笔记，包括写给朋友的电子邮件，向经理描述报告，和为同事写评论等各种任务。在任何给定的轮次中，用户会提供一个来自文档来源的上下文作为任务的文档。重要的是，代理是
    *不知道给定文档的来源*，这将决定用户的偏好。对于这两个任务，我们进行 $T=200$ 轮实验，每个文档来源有相同数量的随机抽样文档。我们混合不同来源的文档，并打乱它们，以消除轮次之间的文档来源的时间相关性。
- en: 'Table 1: Latent user preference design, specific to the document source.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：潜在用户偏好设计，具体到文档来源。
- en: '| Doc Source | Latent User Preference | Scenario |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 文档来源 | 潜在用户偏好 | 场景 |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Summarization |  |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 |  |  |'
- en: '| News article (See et al., [2017](#bib.bib57)) | targeted to young children,
    storytelling, short sentences, playful language, interactive, positive | introduce
    a political news to kids |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 新闻文章（See 等，[2017](#bib.bib57)） | 针对小孩，讲故事，短句，玩乐语言，互动，积极 | 向孩子介绍政治新闻 |'
- en: '| Reddit post (Stiennon et al., [2020](#bib.bib62)) | second person narrative,
    brief, show emotions, invoke personal reflection, immersive | for character development
    in creative writing |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Reddit 帖子（Stiennon 等，[2020](#bib.bib62)） | 第二人称叙述，简洁，展示情感，引发个人反思，沉浸感 | 用于创作写作中的角色发展
    |'
- en: '| Wikipedia page (Foundation, [2022](#bib.bib23)) | bullet points, parallel
    structure, brief | take notes for key knowledge |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 维基百科页面（基础，[2022](#bib.bib23)） | 项目符号，平行结构，简洁 | 记录关键知识 |'
- en: '| Paper abstract (Clement et al., [2019](#bib.bib15)) | tweet style, simple
    English, inquisitive, skillful foreshadowing, with emojis | promote a paper to
    invoke more attention and interests |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 论文摘要（Clement 等，[2019](#bib.bib15)） | 推文风格，简单英语，好奇，巧妙的预告，带有表情符号 | 推广论文以引起更多关注和兴趣
    |'
- en: '| Movie review (Maas et al., [2011](#bib.bib39)) | question answering style,
    direct, concise | quickly get main opinions |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 电影评论（Maas 等，[2011](#bib.bib39)） | 问答风格，直接，简明 | 快速获取主要观点 |'
- en: '| Email Writing |  |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 电子邮件写作 |  |  |'
- en: '| Personal problem (Stiennon et al., [2020](#bib.bib62)) | informal, conversational,
    short, no closing | share life with friends |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 个人问题（Stiennon 等，[2020](#bib.bib62)） | 非正式，交谈式，简短，无结尾 | 与朋友分享生活 |'
- en: '| Paper review (Hua et al., [2019](#bib.bib28)) | casual tone, positive, clear,
    call to action | peer review to colleague |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 论文评论 (Hua et al., [2019](#bib.bib28)) | 随意的语气、积极、清晰、行动号召 | 对同事的同行评审 |'
- en: '| Paper tweet (Bar, [2022](#bib.bib5)) | engaging, personalized, professional
    tone, thankful closing | networking emails for researchers |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 论文推文 (Bar, [2022](#bib.bib5)) | 吸引人、个性化、专业的语气、感谢的结束语 | 研究人员的网络邮件 |'
- en: '| Paper summary (Kershaw & Koeling, [2020](#bib.bib29)) | structured, straight
    to the points, respectful, professional greeting and closing | milestone report
    to superiors |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 论文摘要 (Kershaw & Koeling, [2020](#bib.bib29)) | 结构化、直截了当、尊重、专业的问候和结束语 | 向上级汇报里程碑
    |'
- en: Two-Stage GPT-4 Simulated User.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 两阶段 GPT-4 模拟用户。
- en: We simulate a user that can edit a given response. We define a set of *latent
    user preferences* for the user that vary based on the document source. [Table 1](#S4.T1
    "Table 1 ‣ Task. ‣ 4.1 Two Interactive Writing Assistant Environments for Learning
    from User Edits ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") lists the preference and the corresponding document source.
    This captures the context-dependent nature of user preferences as the document
    source influences the type of context. For example, the *Personal problem* document
    source contains documents pertaining to discussions with a friend, and a user
    may have a different preference when writing an email to a friend compared to
    writing an email to a colleague. In real-world settings, the context dependence
    of the user preference can be more complex than just the document source. We assume
    that our user is aware of the document source $d_{t}$ maps a given document source
    to the user preference. Recall that the *agent in our learning setup is never
    provided the document source of any context*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模拟一个可以编辑给定响应的用户。我们定义了一组 *潜在用户偏好*，这些偏好根据文档来源而有所不同。[表 1](#S4.T1 "表 1 ‣ 任务 ‣
    4.1 两种交互式写作助手环境用于从用户编辑中学习 ‣ 4 实验 ‣ 通过学习用户编辑中的潜在偏好对齐 LLM 代理") 列出了偏好及其对应的文档来源。这捕捉了用户偏好的上下文依赖性质，因为文档来源影响上下文类型。例如，*个人问题*
    文档来源包含与朋友讨论相关的文档，而用户在给朋友写邮件与给同事写邮件时可能会有不同的偏好。在现实世界中，用户偏好的上下文依赖性可能比仅仅是文档来源更复杂。我们假设我们的用户知道文档来源
    $d_{t}$ 将给定的文档来源映射到用户偏好。请记住，*我们学习设置中的代理从未被提供任何上下文的文档来源*。
- en: We model our user using GPT-4 with a two-stage approach. Given an agent response
    $y_{t}$ achieves a minimal user cost.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GPT-4 通过两阶段方法对用户进行建模。给定一个代理响应 $y_{t}$ 实现最小用户成本。
- en: Evaluation Metric.
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估指标。
- en: We propose three metrics for evaluating agents learning from user edits. Our
    main metric is the cumulative user edit cost $\sum_{t=1}^{T}c_{t}$. To compute
    the edit distance, we perform BPE tokenization using Tiktoken tokenizer, and compute
    the edit distance in the token space. In general, one can learn a metric that
    better captures the cognitive load associated with a user edit. However, Levenshtein
    edit distance provides a clean, transparent metric that is easy to interpret.
    Additionally, it doesn’t have concerns shared by learned metrics such as erroneous
    evaluations when applying the metric to examples not covered by the metric’s training
    distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了三种度量指标来评估代理从用户编辑中学习的效果。我们的主要指标是累积用户编辑成本 $\sum_{t=1}^{T}c_{t}$。为了计算编辑距离，我们使用
    Tiktoken 分词器进行 BPE 分词，并在词汇空间中计算编辑距离。一般而言，可以学习一种更好地捕捉用户编辑相关的认知负荷的度量。然而，Levenshtein
    编辑距离提供了一个干净、透明且易于解释的度量。此外，它没有像学习度量那样的担忧，例如在将度量应用于度量训练分布未覆盖的示例时出现的错误评估。
- en: For CIPHER and any other method in the PRELUDE framework, we additionally evaluate
    the accuracy of the inferred user preference $f_{t}$, where BERTScore (Zhang*
    et al., [2020](#bib.bib70)) is a popular text similarity metric.⁵⁵5We use the
    microsoft/deberta-xlarge-mnli to implement BERTScore.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CIPHER 和 PRELUDE 框架中的任何其他方法，我们还评估了推断用户偏好 $f_{t}$ 的准确性，其中 BERTScore (Zhang*
    et al., [2020](#bib.bib70)) 是一种流行的文本相似度度量。我们使用 microsoft/deberta-xlarge-mnli 实现
    BERTScore。
- en: Finally, we evaluate the token expense associated with querying the LLM across
    all methods. We compute the total number of tokens both generated by or provided
    as input to the LLM across all rounds. This is a typical metric used by popular
    LLM providers to charge their customers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了查询 LLM 所涉及的令牌开销。我们计算了所有轮次中由 LLM 生成或作为输入提供的总令牌数。这是流行 LLM 提供商用于向客户收费的典型指标。
- en: 4.2 Details of CIPHER and Comparison Systems
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 CIPHER 和比较系统的详细信息
- en: We use GPT-4 as our base LLM for CIPHER and all baselines. We do not perform
    fine-tuning of the GPT-4 and do not add any additional parameters to the model.
    We use a prompt-based GPT-4 agent for all methods that uses a single prompt with
    greedy decoding to generate the response. Our main method CIPHER and the baselines,
    can be extended to more complex language agents that perform multiple steps of
    reasoning on top of the base LLM before generating a response.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GPT-4作为CIPHER及所有基线的基础LLM。我们不对GPT-4进行微调，也不向模型添加任何额外参数。我们使用基于提示的GPT-4代理来生成响应，该代理使用单一提示和贪婪解码。我们的主要方法CIPHER和基线可以扩展到更复杂的语言代理，这些代理在生成响应之前在基础LLM上进行多步推理。
- en: CIPHER Details.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CIPHER 详细信息。
- en: We use a simple agent that uses GPT-4 with a prompt template to generate the
    response $y_{t}$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个简单的代理，它使用带有提示模板的GPT-4生成响应$y_{t}$。
- en: Baselines.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线。
- en: We evaluate CIPHER against baselines that either perform no learning, or learn
    context-agnostic preferences and against methods that do not learn preferences
    but directly use past user edits for generating a response.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将CIPHER与那些不进行学习的基线进行比较，或者学习上下文无关的偏好，以及与那些不学习偏好但直接使用过去用户编辑生成响应的方法进行比较。
- en: '1.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'No learning: The agent performs no learning based on interaction with the user.
    In each step, the agent generates a response $y_{t}$.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无学习：代理不会根据与用户的交互进行学习。在每一步中，代理生成一个响应$y_{t}$。
- en: '2.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Explore-then-exploit (E-then-e) LPI: This baseline is based on the classic
    explore-then-exploit strategy in interactive learning (Garivier et al., [2016](#bib.bib24)).
    The agent first generates responses for the first $T_{e}$ rounds using the LPI
    step similar to [line 12](#alg1.l12 "In Algorithm 1 ‣ Computational Cost of CIPHER.
    ‣ 3 Learning User Preference through Retrieval and Aggregation ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits") in CIPHER([Algorithm 1](#alg1a
    "Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")). It then uses the learned preference to generate the response
    for all remaining rounds (exploitation step).'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索-然后利用（E-then-e）LPI：此基线基于交互学习中的经典探索-然后利用策略（Garivier等，[2016](#bib.bib24)）。代理首先使用LPI步骤（类似于CIPHER中的[line
    12](#alg1.l12 "Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference
    through Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")）生成前$T_{e}$回合的响应。然后，它使用学习到的偏好生成所有剩余回合的响应（利用步骤）。
- en: '3.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Continual LPI: This method is similar to explore-then-exploit except that it
    never stops exploring. In any given round $t$.'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持续探索-利用（LPI）：这种方法类似于探索-然后利用（explore-then-exploit），但它永远不会停止探索。在任何给定的回合$t$中。
- en: '4.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'ICL-edit: This is a standard retrieval-based in-context learning (ICL) baseline (Brown
    et al., [2020](#bib.bib8)). In a given round $t$ and the historical data.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ICL-edit：这是一个标准的基于检索的上下文学习（ICL）基线（Brown等，[2020](#bib.bib8)）。在给定的回合$t$和历史数据中。
- en: Baseline Hyperparameters.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基线超参数。
- en: For explore-then-exploit LPI and continual LPI baselines, we set the number
    of exploration $T_{e}$.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于探索-然后利用（explore-then-exploit）LPI和持续LPI基线，我们设置了探索次数$T_{e}$。
- en: Oracle Method.
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Oracle方法。
- en: We additionally run an oracle preference method to provide an approximated upper
    bound on performance. In each round $t$. This method can test whether our setup
    is well-defined, e.g., in a poorly designed setup, the user always edits the agent
    response no matter what the agent generates including providing user edits back
    to the user, and thus no method can effectively minimize the cost over time in
    this case. If the oracle method achieves a zero or a minimal user edit cost, then
    learning the optimal preference leads to success.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还运行了一种oracle偏好方法，以提供性能的近似上界。在每个回合$t$中。这种方法可以测试我们的设置是否定义良好，例如，在设计不佳的设置中，无论代理生成什么内容，用户总是会编辑代理响应，包括将用户编辑反馈给用户，因此在这种情况下，没有任何方法可以有效地随着时间的推移降低成本。如果oracle方法达到零或最小的用户编辑成本，那么学习最优偏好会导致成功。
- en: 4.3 Main Result and Discussion.
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 主要结果与讨论。
- en: Main Results.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主要结果。
- en: '[Table 2](#S4.T2 "Table 2 ‣ Main Results. ‣ 4.3 Main Result and Discussion.
    ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")
    reports the performance of baselines and our methods on summarization and email
    writing tasks on three metrics: *edit distance* which measures cumulative user
    edit cost, *accuracy* which measures mean preference classification accuracy,
    and *expense* measuring the total BPE token cost of querying LLM.⁶⁶6[Table 9](#A2.T9
    "Table 9 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits") in Appendix shows the breakdown
    of expense in terms of input and output. We report the mean and standard deviation
    across 3 different random seeds.⁷⁷7We randomize the context sampling from source
    datasets, so experiments on different seeds contain different sets of input contexts.
    On the same seed, experiments across different methods are strictly comparable,
    as both the set of input contexts and the order of input context seen are the
    same in our implementation.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2](#S4.T2 "Table 2 ‣ 主要结果。 ‣ 4.3 主要结果和讨论。 ‣ 4 实验 ‣ 通过用户编辑学习潜在偏好对齐LLM代理")
    报告了基线方法和我们的方法在三项指标上的表现：*编辑距离*（测量累积用户编辑成本）、*准确性*（测量平均偏好分类准确性）和*费用*（测量查询LLM的总BPE令牌成本）。⁶⁶6[表9](#A2.T9
    "Table 9 ‣ 检索准确性。 ‣ 附录B 附加分析 ‣ 通过用户编辑学习潜在偏好对齐LLM代理") 附录显示了输入和输出方面的费用明细。我们报告了3个不同随机种子下的均值和标准差。⁷⁷7我们随机化了从源数据集中的上下文采样，因此不同种子下的实验包含不同的输入上下文集合。在相同种子下，不同方法的实验是严格可比的，因为在我们的实现中，输入上下文集合和看到的输入上下文的顺序是相同的。'
- en: 'Table 2: Performance of baselines and our methods in terms of cumulative edit
    distance cost and classification accuracy. $\mu_{\sigma}$ retrieved examples.
    Numbers in bold are the best performance in each column excluding oracle preference
    method, underline for the second best, and dotted underline for the third best.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基线方法和我们的方法在累积编辑距离成本和分类准确性方面的表现。$\mu_{\sigma}$ 检索的示例。粗体数字表示每列中的最佳表现（排除oracle
    preference方法），下划线表示第二最佳，虚线下划线表示第三最佳。
- en: '| Method | Summarization | Email Writing |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 | 邮件写作 |'
- en: '|  | Edit Distance$\downarrow$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 编辑距离$\downarrow$ |'
- en: '| Oracle Preference |   6,573[1,451] | 1.000 | 1.67 | 1,851[243] | 1.000 |
    1.62 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Oracle Preference |   6,573[1,451] | 1.000 | 1.67 | 1,851[243] | 1.000 |
    1.62 |'
- en: '| No Learning | 48,269[957] | - | 1.50 | 31,103[900] | - | 1.65 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 无学习 | 48,269[957] | - | 1.50 | 31,103[900] | - | 1.65 |'
- en: '| E-then-e LPI |   65,218[17,466] | 0.218[0.003] | 1.99 | 24,562[1,022] | 0.263[0.003]
    | 1.73 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| E-then-e LPI |   65,218[17,466] | 0.218[0.003] | 1.99 | 24,562[1,022] | 0.263[0.003]
    | 1.73 |'
- en: '| Continual LPI | 57,915[2,210] | 0.233[0.010] | 8.89 | 26,852[1,464] | 0.243[0.019]
    | 8.63 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 连续LPI | 57,915[2,210] | 0.233[0.010] | 8.89 | 26,852[1,464] | 0.243[0.019]
    | 8.63 |'
- en: '| ICL-edit-5-MPNET | 38,560[1,044] | - | 8.00 | 32,405[1,307] | - | 12.12 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-MPNET | 38,560[1,044] | - | 8.00 | 32,405[1,307] | - | 12.12 |'
- en: '| ICL-edit-5-BERT | 39,734[1,929] | - | 7.96 | 30,949[3,250] | - | 11.55 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-BERT | 39,734[1,929] | - | 7.96 | 30,949[3,250] | - | 11.55 |'
- en: '| CIPHER-1-MPNET | 33,926[4,000] | 0.520[0.022] | 2.74 | 10,781[1,711] | 0.435[0.084]
    | 1.94 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-MPNET | 33,926[4,000] | 0.520[0.022] | 2.74 | 10,781[1,711] | 0.435[0.084]
    | 1.94 |'
- en: '| CIPHER-5-MPNET | 32,974[195] | 0.478[0.010] | 3.00 | 10,058[1,709] | 0.467[0.081]
    | 2.09 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-MPNET | 32,974[195] | 0.478[0.010] | 3.00 | 10,058[1,709] | 0.467[0.081]
    | 2.09 |'
- en: '| CIPHER-1-BERT | 37,637[3,025] | 0.565[0.053] | 2.81 | 12,634[4,868] | 0.487[0.125]
    | 1.99 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-BERT | 37,637[3,025] | 0.565[0.053] | 2.81 | 12,634[4,868] | 0.487[0.125]
    | 1.99 |'
- en: '| CIPHER-5-BERT | 35,811[3,384] | 0.478[0.028] | 3.03 |  8,391[3,038] | 0.363[0.075]
    | 2.22 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-BERT | 35,811[3,384] | 0.478[0.028] | 3.03 |  8,391[3,038] | 0.363[0.075]
    | 2.22 |'
- en: Discussion of Main Result.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主要结果讨论。
- en: We observe that not performing learning results in a high edit cost, whereas
    using the Oracle preferences achieves a significantly smaller edit cost. This
    shows that our environments are sound and well-conditioned. E-then-e LPI and Continual
    LPI learn context-agnostic preferences which cannot capture the context-dependent
    preferences in the environments and end up doing poorly. For the summarization
    task, they end up with a higher edit distance than even performing no learning.
    One explanation is that using context-agnostic preferences can push the model
    to specialize to a given preference much more than the base model, resulting in
    more edits when that preference is incorrect. We see this in preference accuracy
    which is low for both of these baselines, and lower for the summarization task
    than the email writing task where they outperform no learning baselines. Further,
    Continual LPI has a higher expense cost due to constantly querying the LLM to
    infer the user preference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到不进行学习会导致较高的编辑成本，而使用 Oracle 偏好会显著降低编辑成本。这表明我们的环境是可靠且状态良好的。E-then-e LPI 和
    Continual LPI 学习的是与上下文无关的偏好，这些偏好无法捕捉环境中的上下文依赖性偏好，因此表现较差。对于摘要任务，它们的编辑距离甚至高于没有进行学习的情况。一个解释是，使用与上下文无关的偏好可能会使模型更专注于某一特定偏好，远比基础模型产生更多的编辑，当该偏好不正确时。我们在这两个基线的偏好准确性中看到这一点，而且在摘要任务中的准确性低于邮件写作任务，其中它们的表现优于没有学习的基线。此外，Continual
    LPI 的开销更高，因为需要不断查询 LLM 以推断用户偏好。
- en: ICL-edit baselines perform significantly better on the summarization task. However,
    using a list of user edits in the prompt results in a higher token expense cost,
    as the responses and their edits can be significantly long in practice. Further,
    the ICL-edit baselines provide no interpretable explanation for their response
    or for explaining user behavior.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ICL-edit 基线在摘要任务上表现显著更好。然而，使用用户编辑的列表作为提示会导致更高的 token 花费，因为实际中的响应及其编辑可能会非常长。此外，ICL-edit
    基线无法提供对其响应或解释用户行为的可解释说明。
- en: Finally, CIPHER achieves the smallest edit distance cost reducing edits by 31%
    in the summarization task and 73% in the email writing task. We observe that retrieving
    $k=5$ seems task-dependent. Further, CIPHER achieves the highest preference accuracy
    showing that CIPHER can learn preferences that correlate more with the ground
    truth preference than preferences of other document sources. Note that the performance
    of a random preference classifier is only 20% for summarization and 25% for email
    writing. Further, CIPHER achieves a smaller cost than ICL-edit and Continual LPI
    baselines, as it doesn’t use long user edits in the prompt for generating a response.
    Overall, CIPHER provides a cheap, more effective, and interpretable method than
    our baselines.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，CIPHER 在摘要任务中将编辑距离成本减少了 31%，在邮件写作任务中减少了 73%。我们观察到，检索 $k=5$ 似乎依赖于任务。此外，CIPHER
    实现了最高的偏好准确性，表明 CIPHER 可以学习与实际偏好更相关的偏好，而非其他文档来源的偏好。请注意，随机偏好分类器在摘要任务中的表现仅为 20%，在邮件写作任务中为
    25%。此外，CIPHER 的成本低于 ICL-edit 和 Continual LPI 基线，因为它不使用长用户编辑作为生成响应的提示。总体而言，CIPHER
    提供了一种比我们的基线更便宜、更有效、且更具可解释性的方法。
- en: 'Figure 2: Learning curves of different methods based on cumulative cost over
    time (average across 3 seeds). In the legend, -k means with top $k$ retrieved
    examples, -B for BERT, and -M for MPNET.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：不同方法的学习曲线，基于时间的累积成本（3 个种子的平均值）。在图例中，-k 表示使用前 $k$ 个检索的示例，-B 代表 BERT，-M 代表
    MPNET。
- en: $0$RoundCumulative
    CostEmail WritingOracleNo LearningE-then-eContinualICL-edit-BICL-edit-MCIPHER-1-BCIPHER-5-BCIPHER-1-MCIPHER-5-M
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $0$回合累计成本邮件写作甲骨文未学习E-then-e持续ICL-edit-BICL-edit-MCIPHER-1-BCIPHER-5-B4080120160200$0$Round
    Normalized
    Cost Email Writing408012016020000.20.40.60.8Round
    %
    Zero-cost Ex. per Bin 40801201602000.20.40.6Round
    %
    Zero-cost Ex. per Bin
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**Zero-cost Ex. per Bin** 是 **Normalized Cost** 和 **Email Writing** 的百分比图表。'
- en: 4.4 More Analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 更多分析
- en: Learning Curves.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习曲线。
- en: We plot mean cumulative user edit costs over rounds in [Figure 2](#S4.F2 "Figure
    2 ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits"). The cumulative
    user edit costs in [Figure 2](#S4.F2 "Figure 2 ‣ Discussion of Main Result. ‣
    4.3 Main Result and Discussion. ‣ 4 Experiment ‣ Aligning LLM Agents by Learning
    Latent Preference from User Edits") show that the angle of the learning curves
    decreases for CIPHER after an initial number of rounds, showing that learning
    helps decrease the rate at which user edits are accumulated. In contrast, the
    angle of the learning curve for the no-learning baseline remains unchanged.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图2](#S4.F2 "Figure 2 ‣ Discussion of Main Result. ‣ 4.3 Main Result and
    Discussion. ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits")中绘制了每轮的平均累积用户编辑成本。图2中的累积用户编辑成本显示，在初始几轮之后，CIPHER的学习曲线的斜率降低，这表明学习有助于减少用户编辑的累积速度。相比之下，没有学习的基准线的学习曲线斜率保持不变。
- en: Evaluating Normalized Edit Cost.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估标准化编辑成本。
- en: The cumulative user edit cost measures the total effort of the user but is susceptible
    to outlier examples, as the edit distance for a given round is potentially unbounded.
    Therefore, we also compute a *normalized edit distance* $\Delta_{\textrm{edit}}(y_{t},y^{\prime}_{t})/|y_{t}|$,
    therefore, the normalized cost is at most 1\. [Figure 3](#S4.F3 "Figure 3 ‣ Discussion
    of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits") reports normalized cost
    over rounds for the top 3 methods. We notice that for all variants of CIPHER for
    the summarization task, and for CIPHER-5-M for the email writing task, the normalized
    cost decreases notably as training progresses indicating learning. As the cost
    is normalized by the response length, even a small decrease can lead to a significant
    reduction in the number of tokens edited.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 累积用户编辑成本衡量了用户的总努力，但容易受到异常值的影响，因为给定轮次的编辑距离可能是无界的。因此，我们还计算了*标准化编辑距离* $\Delta_{\textrm{edit}}(y_{t},y^{\prime}_{t})/|y_{t}|$，因此，标准化成本最多为1。
    [图3](#S4.F3 "Figure 3 ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion.
    ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")报告了前3种方法在各轮中的标准化成本。我们注意到，对于总结任务的所有CIPHER变体，以及电子邮件写作任务的CIPHER-5-M，随着训练的进展，标准化成本显著降低，表明了学习。由于成本是通过响应长度来标准化的，即使是很小的减少也可能导致编辑的标记数量显著减少。
- en: Evaluating Fraction of Edited Response.
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估编辑响应的比例。
- en: Recall that the first stage of our GPT-4 user checks if the agent response satisfies
    the latent user preference $f^{\star}$. If it does, then the user performs no
    edits. Otherwise, in the second stage, the user edits the response. To measure
    how many times the agent response isn’t edited, we also plot the percentage of
    examples with zero edit cost per 20 rounds bin in [Figure 3](#S4.F3 "Figure 3
    ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits"). We notice
    a small increase in the number of examples with zero edit cost. This indicates
    that gains come from reducing edits across all examples, and not just by increasing
    the number of examples that avoid getting edited in stage 1 of our user.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的GPT-4用户的第一阶段检查代理的响应是否符合潜在用户偏好 $f^{\star}$。如果符合，用户不会进行编辑。否则，在第二阶段，用户会编辑响应。为了衡量代理响应未被编辑的次数，我们还绘制了每20轮的零编辑成本的示例百分比，见[图3](#S4.F3
    "Figure 3 ‣ Discussion of Main Result. ‣ 4.3 Main Result and Discussion. ‣ 4 Experiment
    ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")。我们注意到零编辑成本的示例数量略有增加。这表明收益来自于减少所有示例的编辑，而不仅仅是通过增加第一阶段用户避免编辑的示例数量。
- en: Qualitative Analysis of Learned Preferences.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 学习到的偏好的定性分析。
- en: We qualitatively analyze the learned preferences for CIPHER to understand the
    quality of learned preferences. We present our analysis on the summarization task,
    where our methods have a larger gap with the oracle performance compared to the
    email writing task. [Table 3](#S4.T3 "Table 3 ‣ Failure Cases. ‣ 4.4 More Analysis
    ‣ 4 Experiment ‣ Aligning LLM Agents by Learning Latent Preference from User Edits")
    lists 3 learned preferences per document source for CIPHER-5-MPNET which are randomly
    sampled at the beginning, middle, and end of the interaction history. We see that
    overall the agent can learn a reasonable description of the latent preference.
    For example, it can learn *bullet points* preference for Wikipedia articles, and
    *second person narrative* for Reddit posts, and *QA style* for Movie reviews.
    CIPHER can pick some preferences fairly early such as *bullet points* for Wikipedia
    and *emojis* for Paper abstract, whereas some are learned only later such as *Structured
    Q$\&amp;$A* for Movie reviews. This shows using CIPHER can quickly learn useful
    preferences, but further interaction continues to help.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定性分析了学到的偏好以了解**CIPHER**的学习效果。我们在总结任务上进行了分析，发现与电子邮件写作任务相比，我们的方法与**oracle**性能的差距更大。[表
    3](#S4.T3 "Table 3 ‣ Failure Cases. ‣ 4.4 More Analysis ‣ 4 Experiment ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits")列出了CIPHER-5-MPNET每个文档来源的3个学到的偏好，这些偏好在交互历史的开始、中间和结束时随机抽取。我们发现总体上，代理能够学习到合理的潜在偏好描述。例如，它可以学习维基百科文章的*项目符号*偏好，Reddit帖子中的*第二人称叙述*，以及电影评论中的*问答风格*。CIPHER可以相对较早地学习一些偏好，例如维基百科的*项目符号*和论文摘要中的*表情符号*，而一些偏好则在较晚阶段才被学习到，例如电影评论中的*结构化问答*。这表明使用CIPHER可以快速学习有用的偏好，但进一步的交互仍有帮助。
- en: Failure Cases.
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 失败案例。
- en: CIPHER notably reduces the edit cost and learns useful preference, however,
    significant gaps to the oracle method remain, especially in the summarization
    task. We manually analyze failure cases on summarization task with the best performing
    method CIPHER-5-MPNET.  [Table 10](#A2.T10 "Table 10 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits") in the Appendix reports the summary and example of our findings,
    categorized as preference inference from output-revision pair, consolidation of
    inferred preferences, and retrieval.⁸⁸8We provide additional analysis on the accuracy
    of retrieval in [Table 11](#A2.T11 "Table 11 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits"). In brief, the most common type of failure is on the preference inference
    step given the agent output and user revision. For example, the agent often misses
    the exact keyword for brief or short sentences, and sometimes struggles with inferring
    the second-person narrative aspect.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: CIPHER显著降低了编辑成本并学习了有用的偏好，但在总结任务中与oracle方法之间仍存在显著差距。我们手动分析了总结任务中的失败案例，使用了表现最佳的方法CIPHER-5-MPNET。[表
    10](#A2.T10 "Table 10 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣
    Aligning LLM Agents by Learning Latent Preference from User Edits")在附录中报告了我们的发现摘要和示例，分类为从输出修订对中推断的偏好、推断偏好的整合和检索。我们在[表
    11](#A2.T11 "Table 11 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣
    Aligning LLM Agents by Learning Latent Preference from User Edits")中提供了关于检索准确性的额外分析。简而言之，最常见的失败类型是给定代理输出和用户修订的偏好推断步骤。例如，代理经常遗漏简短句子的确切关键词，并且有时在推断第二人称叙述方面表现挣扎。
- en: 'Table 3: Examples of learned preferences on summarization task with CIPHER-5-MPNET,
    grouped based on the document source and corresponding latent preference. We randomly
    sample 3 examples per type at the beginning, middle, and end of the interaction
    history.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：CIPHER-5-MPNET在总结任务中学到的偏好的示例，根据文档来源和相应的潜在偏好进行分组。我们在交互历史的开始、中间和结束时每种类型随机抽取3个示例。
- en: '| Latent User Preference | (Round) Learned Preference |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 潜在用户偏好 | （回合）学习到的偏好 |'
- en: '| --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| News article. targeted to young children, storytelling, short sentences,
    playful language, interactive, positive | (22) Fairy tale narrative style, informal
    and conversational tone, use of rhetorical questions, simplified language. (115)
    Simplified, childlike storytelling with playful language and imagery'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '| 针对小孩子的新闻文章，讲故事，短句，富有趣味的语言，互动性，积极性 | (22) 童话叙述风格，非正式且对话的语调，使用修辞性问题，简化语言。 (115)
    简化的、童趣的讲故事，富有趣味的语言和意象'
- en: (192) Simplified and playful storytelling language |
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (192) 简化且富有趣味的讲故事语言 |
- en: '| Reddit post: second person narrative, brief, show emotions, invoke personal
    reflection, immersive | (14) Concise and coherent storytelling (102) The user
    prefers a second-person narrative and a more direct, personal tone'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '| Reddit 帖子：第二人称叙事，简洁，展现情感，引发个人反思，沉浸感强 | （14）简洁而连贯的讲故事风格 （102）用户偏好第二人称叙事和更直接、个人化的语调'
- en: (194) Poetic and descriptive language, narrative perspective shift to second
    person |
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: （194）富有诗意和描述性的语言，叙事视角转为第二人称 |
- en: '| Wikipedia page. bullet points, parallel structure, brief | (19) Concise,
    Bullet-Pointed, Structured Summaries with a Narrative Q&A Style (124) Concise
    and factual writing style, bullet-point formatting'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '| 维基百科页面。项目符号、平行结构、简洁 | （19）简洁的、项目符号的、有结构的总结，带有叙述性的问答风格 （124）简洁的事实性写作风格，项目符号格式'
- en: (197) Concise and streamlined formatting, with bullet points and clear subheadings
    for easy scanning |
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: （197）简洁流畅的格式，使用项目符号和清晰的小标题以便于扫描 |
- en: '| Paper abstract. tweet style, simple English, inquisitive, skillful foreshadowing,
    with emojis | (20) Concise, conversational summaries with bullet points and emojis.
    (111) Concise, conversational, whimsical bullet-point summaries with emojis. ![[Uncaptioned
    image]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[Uncaptioned image]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[Uncaptioned
    image]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '| 论文摘要。推文风格、简单英语、好奇、巧妙的铺垫，带有表情符号 | （20）简洁的对话式总结，带有项目符号和表情符号。 （111）简洁的对话式、富有趣味的项目符号总结，带有表情符号。
    ![[未标注的图片]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[未标注的图片]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[未标注的图片]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)'
- en: (193) Concise, conversational, and whimsical bullet-point summaries with emojis.
    ![[Uncaptioned image]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[Uncaptioned
    image]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[Uncaptioned image]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)  ![[Uncaptioned
    image]](img/398f85daf5421bbb602f6bfd2bc1af15.png)  |
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: （193）简洁、对话式、富有趣味的项目符号总结，带有表情符号。 ![[未标注的图片]](img/2496d5a0dfc546107fc52bb6ac9c2c4f.png)  ![[未标注的图片]](img/9b82923202081e97ba9b5f338569d9b2.png)  ![[未标注的图片]](img/606e64ae68070a1c0b8b2a3f6ef6a0d0.png)  ![[未标注的图片]](img/398f85daf5421bbb602f6bfd2bc1af15.png)  |
- en: '| Movie review. question answering style | (12) The user prefers a straightforward,
    clear, and concise writing style with factual formatting. (123) The user prefers
    a clear and concise question and answer format with straightforward language.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '| 电影评论。问答风格 | （12）用户偏好直接、清晰、简洁的写作风格和事实性格式。 （123）用户偏好清晰简洁的问答格式和直白的语言。'
- en: (199) Concise, Structured Q&A with Whimsical Clarity |
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: （199）简洁、结构化的问答，富有趣味性和清晰度 |
- en: 5 Related Work
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: We describe related work in this area grouped by main themes in this work.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本领域相关工作按主要主题进行归类描述。
- en: Learning from Feedback.
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从反馈中学习。
- en: Besides pair-wise comparison feedback from annotators used in Reinforcement
    Learning from Human Feedback (RLHF) research (Ziegler et al., [2019](#bib.bib71);
    Stiennon et al., [2020](#bib.bib62); Nakano et al., [2021](#bib.bib47); Ouyang
    et al., [2022a](#bib.bib49), inter alia), prior work has also studied free-form
    text feedback provided by annotators  (Fernandes et al., [2023](#bib.bib22)),
    such as on the task of dialog (Weston, [2016](#bib.bib65); Li et al., [2016](#bib.bib35);
    Hancock et al., [2019](#bib.bib27); Xu et al., [2022](#bib.bib66); Petrak et al.,
    [2023](#bib.bib51)), question answering (Li et al., [2022](#bib.bib36); Malaviya
    et al., [2023](#bib.bib40)), summarization (Saunders et al., [2022](#bib.bib55)),
    and general decision making (Cheng et al., [2023](#bib.bib13)). This feedback,
    tailored to each example, is often utilized to rank candidate outputs, thereby
    improving task performance. Some work studies learning from text feedback to generate
    outputs directly (Scheurer et al., [2023](#bib.bib56); Bai et al., [2022](#bib.bib3);
    Shi et al., [2022](#bib.bib59)), by generating multiple refinements of the original
    output based on the feedback and fine-tuning the original model to maximize the
    likelihood of the best refinement. In grounded settings such as instruction-based
    navigation, one line of work has also used hindsight feedback that explicitly
    provides a text instruction for the generated trajectory, to train policies (Nguyen
    et al., [2021](#bib.bib48); Misra et al., [2024](#bib.bib44)). Moving beyond the
    conventional focus on text feedback that explicitly articulates human intent,
    we investigate feedback in the form of direct edits on the original model output.
    Such revisions by users occur naturally during model deployment in practice. Additionally,
    we examine the learning of user preferences through historical interactions, aiming
    to surpass the constraints of example-specific feedback.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在“人类反馈强化学习”（RLHF）研究中使用的注释员对比反馈（Ziegler et al., [2019](#bib.bib71); Stiennon
    et al., [2020](#bib.bib62); Nakano et al., [2021](#bib.bib47); Ouyang et al.,
    [2022a](#bib.bib49)），先前的工作还研究了注释员提供的自由形式文本反馈（Fernandes et al., [2023](#bib.bib22)），例如对对话任务（Weston,
    [2016](#bib.bib65); Li et al., [2016](#bib.bib35); Hancock et al., [2019](#bib.bib27);
    Xu et al., [2022](#bib.bib66); Petrak et al., [2023](#bib.bib51)）、问答（Li et al.,
    [2022](#bib.bib36); Malaviya et al., [2023](#bib.bib40)）、摘要（Saunders et al., [2022](#bib.bib55)）和一般决策制定（Cheng
    et al., [2023](#bib.bib13)）。这些针对每个示例量身定制的反馈通常用于对候选输出进行排名，从而提高任务性能。一些研究探讨了如何通过文本反馈直接生成输出（Scheurer
    et al., [2023](#bib.bib56); Bai et al., [2022](#bib.bib3); Shi et al., [2022](#bib.bib59)），通过根据反馈生成多个原始输出的改进版本，并微调原始模型以最大化最佳改进的可能性。在诸如基于指令的导航等实际环境中，有一系列工作也使用了回顾反馈，即明确提供生成轨迹的文本指令，以训练策略（Nguyen
    et al., [2021](#bib.bib48); Misra et al., [2024](#bib.bib44)）。超越传统的关注点，即明确阐述人类意图的文本反馈，我们研究了对原始模型输出进行直接编辑的反馈形式。这种用户的修订在实际模型部署过程中自然发生。此外，我们还研究了通过历史交互学习用户偏好的方法，旨在超越示例特定反馈的限制。
- en: Language Agents and Personalization.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言代理和个性化。
- en: LLMs have enabled the development of language agents for a variety of tasks
    from writing assistants (Lee et al., [2024](#bib.bib32)), coding assistants (Dohmke,
    [2022](#bib.bib19)), and customer service assistants (Brynjolfsson et al., [2023](#bib.bib9)).
    Since these LLM-based assistants are often used by individuals, a natural question
    has arisen on how to personalize these agents for each user. Straightforward approaches
    for fine-tuning LLMs includes supervised learning, online DPO (Guo et al., [2024](#bib.bib25)),
    learning-to-search (Chang et al., [2023](#bib.bib11)), and reinforcement learning (Ouyang
    et al., [2022b](#bib.bib50)). These approaches can be directly applied to our
    setting. For example, one can use $(y_{t},y^{\prime}_{t})$ as the ground truth
    for supervised learning. However, fine-tuning is expensive and hard to scale with
    the number of users. Therefore, a line of work has explored improving the alignment
    of frozen LLMs by *prompt engineering*, such as learning a personalized retrieval
    model (Mysore et al., [2023](#bib.bib46)), learning a prompt policy given a reward
    function (Deng et al., [2022](#bib.bib17)), or more generally, learning to rewrite
    the entire prompt (Li et al., [2023](#bib.bib34)). We focus on learning a prompt
    policy by learning from user edits, and specifically, using them to extract textural
    descriptions of user preference.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的发展使得各种任务的语言代理得以实现，例如写作助手 (Lee et al., [2024](#bib.bib32))、编程助手 (Dohmke,
    [2022](#bib.bib19)) 和客服助手 (Brynjolfsson et al., [2023](#bib.bib9))。由于这些基于 LLM
    的助手通常由个人使用，因此自然产生了如何为每个用户个性化这些代理的问题。对 LLM 进行微调的直接方法包括监督学习、在线 DPO (Guo et al.,
    [2024](#bib.bib25))、学习搜索 (Chang et al., [2023](#bib.bib11)) 和强化学习 (Ouyang et al.,
    [2022b](#bib.bib50))。这些方法可以直接应用于我们的设置。例如，可以使用 $(y_{t},y^{\prime}_{t})$ 作为监督学习的真实标签。然而，微调成本高昂且难以随着用户数量的增加而扩展。因此，一些研究工作探索了通过
    *提示工程* 改善冻结的 LLM 的对齐，例如学习个性化检索模型 (Mysore et al., [2023](#bib.bib46))、给定奖励函数学习提示策略
    (Deng et al., [2022](#bib.bib17))，或更一般地，学习重写整个提示 (Li et al., [2023](#bib.bib34))。我们专注于通过学习用户编辑来学习提示策略，具体来说，使用这些编辑来提取用户偏好的文本描述。
- en: Edits and Revisions.
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编辑和修订。
- en: Many prior work on editing model output focuses on error correction, such as
    fixing source code (Yin et al., [2018](#bib.bib68); Chen et al., [2018](#bib.bib12);
    Reid et al., [2023](#bib.bib54)) and improving the factual consistency of model
    summaries (Cao et al., [2020](#bib.bib10); Liu et al., [2022](#bib.bib38); Balachandran
    et al., [2022](#bib.bib4)). A line of work has explored understanding human edits
    based on edit history of Wikipedia (Botha et al., [2018](#bib.bib6); Faltings
    et al., [2020](#bib.bib21); Rajagopal et al., [2022](#bib.bib52); Reid & Neubig,
    [2022](#bib.bib53); Laban et al., [2023](#bib.bib31)), or revisions of academic
    writings (Mita et al., [2022](#bib.bib45); Du et al., [2022](#bib.bib20); D’Arcy
    et al., [2023](#bib.bib16)). Prior work explores predicting text revisions with
    edit intents (Brody et al., [2020](#bib.bib7); Kim et al., [2022](#bib.bib30);
    Chong et al., [2023](#bib.bib14)), and modeling edits with various approaches,
    including latent vectors (Guu et al., [2017](#bib.bib26); Marrese-Taylor et al.,
    [2020](#bib.bib42), [2023](#bib.bib43)), structured trees (Yao et al., [2021](#bib.bib67)),
    discrete diffusion process (Reid et al., [2023](#bib.bib54)), or a series of singular
    edit operations (Stahlberg & Kumar, [2020](#bib.bib61); Mallinson et al., [2020](#bib.bib41);
    Agrawal & Carpuat, [2022](#bib.bib2); Zhang et al., [2022](#bib.bib69); Liu et al.,
    [2023](#bib.bib37)). However, these methodologies predominantly target generic
    improvements in model performance, overlooking the intricacies of individual user
    satisfaction and preference. Our research takes a distinct direction, focusing
    on understanding edits across a variety of examples to study user-level preferences,
    with a practical goal of aligning the agent to individual preferences.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 许多之前的工作集中于编辑模型输出的错误修正，例如修复源代码 (Yin et al., [2018](#bib.bib68); Chen et al.,
    [2018](#bib.bib12); Reid et al., [2023](#bib.bib54)) 和改善模型摘要的事实一致性 (Cao et al.,
    [2020](#bib.bib10); Liu et al., [2022](#bib.bib38); Balachandran et al., [2022](#bib.bib4))。一系列工作探讨了基于维基百科编辑历史来理解人类编辑
    (Botha et al., [2018](#bib.bib6); Faltings et al., [2020](#bib.bib21); Rajagopal
    et al., [2022](#bib.bib52); Reid & Neubig, [2022](#bib.bib53); Laban et al., [2023](#bib.bib31))，或者学术写作的修订
    (Mita et al., [2022](#bib.bib45); Du et al., [2022](#bib.bib20); D’Arcy et al.,
    [2023](#bib.bib16))。之前的工作探索了基于编辑意图预测文本修订 (Brody et al., [2020](#bib.bib7); Kim
    et al., [2022](#bib.bib30); Chong et al., [2023](#bib.bib14))，以及使用各种方法建模编辑，包括潜在向量
    (Guu et al., [2017](#bib.bib26); Marrese-Taylor et al., [2020](#bib.bib42), [2023](#bib.bib43))，结构化树
    (Yao et al., [2021](#bib.bib67))，离散扩散过程 (Reid et al., [2023](#bib.bib54))，或一系列单独的编辑操作
    (Stahlberg & Kumar, [2020](#bib.bib61); Mallinson et al., [2020](#bib.bib41);
    Agrawal & Carpuat, [2022](#bib.bib2); Zhang et al., [2022](#bib.bib69); Liu et
    al., [2023](#bib.bib37))。然而，这些方法主要针对模型性能的通用改进，忽略了个体用户满意度和偏好的复杂性。我们的研究采取了不同的方向，专注于理解各种示例中的编辑，以研究用户级偏好，实用目标是使代理与个人偏好对齐。
- en: 6 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We study aligning LLM-based agents using user edits that arise naturally in
    applications such as writing assistants. We conjecture that user edits are driven
    by a latent user preference that can be captured by textual descriptions. We introduce
    the PRELUDE framework that focuses on learning descriptions of user preferences
    from user edit data and then generating an agent response accordingly. We propose
    a simple yet effective retrieval-based algorithm CIPHER that infers user preference
    by querying the LLM, retrieves relevant examples in the history, and aggregates
    induced preferences in retrieved examples to generate a response for the given
    context. We introduce two interactive environments with a GPT-4 simulated user
    to study learning from edits, which can be of independent interest. In this work,
    we focus on aligning an LLM agent with a frozen LLM, in part, due to the challenge
    of scaling fine-tuning based approaches with the number of users. However, for
    settings where computational cost is not a barrier, applying fine-tuning approaches
    would be an interesting future work direction. Another promising future work direction
    is to learn user preference based on different levels of edits – words, sentences,
    paragraphs – to generate a satisfactory response.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了使用自然产生的用户编辑对齐基于LLM的代理，例如写作助手。我们推测，用户编辑由潜在的用户偏好驱动，这可以通过文本描述来捕捉。我们引入了PRELUDE框架，重点在于从用户编辑数据中学习用户偏好的描述，然后根据这些描述生成代理响应。我们提出了一种简单而有效的基于检索的算法CIPHER，它通过查询LLM推断用户偏好，从历史记录中检索相关示例，并将检索到示例中的诱导偏好聚合起来，以生成给定上下文的响应。我们引入了两个与GPT-4模拟用户的互动环境来研究从编辑中学习，这本身具有独立的兴趣。在这项工作中，我们专注于将LLM代理与冻结的LLM对齐，部分原因是随着用户数量增加，微调方法的扩展难度。然而，对于计算成本不是障碍的环境，应用微调方法将是一个有趣的未来工作方向。另一个有前景的未来工作方向是基于不同层次的编辑——单词、句子、段落——来学习用户偏好，以生成令人满意的响应。
- en: Acknowledgments
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 致谢
- en: 'Gao was a research intern in MSR NYC, and later was partially supported by
    NSF project #1901030\. All content represents the opinion of the authors, which
    is not necessarily shared or endorsed by their respective employers and/or sponsors.
    We thank MSR NYC research community, Jonathan D. Chang, Daniel D. Lee, Claire
    Cardie, and Sasha Rush for helpful discussions and support.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Gao曾是MSR NYC的研究实习生，后来部分由NSF项目#1901030资助。所有内容代表作者的观点，这些观点不一定得到其各自雇主和/或赞助者的认可或支持。我们感谢MSR
    NYC研究社区、Jonathan D. Chang、Daniel D. Lee、Claire Cardie和Sasha Rush的有益讨论和支持。
- en: References
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等. GPT-4技术报告。*arXiv预印本 arXiv:2303.08774*，2023年。
- en: Agrawal & Carpuat (2022) Sweta Agrawal and Marine Carpuat. An imitation learning
    curriculum for text editing with non-autoregressive models. *ArXiv*, abs/2203.09486,
    2022.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal & Carpuat (2022) Sweta Agrawal 和 Marine Carpuat. 一种用于文本编辑的模仿学习课程，采用非自回归模型。*ArXiv*，abs/2203.09486，2022年。
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional
    ai: Harmlessness from ai feedback, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown 和 Jared Kaplan. 宪法人工智能：来自AI反馈的无害性，2022年。
- en: Balachandran et al. (2022) Vidhisha Balachandran, Hannaneh Hajishirzi, William
    Cohen, and Yulia Tsvetkov. Correcting diverse factual errors in abstractive summarization
    via post-editing and language model infilling. *ArXiv*, abs/2210.12378, 2022.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balachandran 等（2022）Vidhisha Balachandran, Hannaneh Hajishirzi, William Cohen,
    和 Yulia Tsvetkov。通过后编辑和语言模型填充纠正抽象总结中的多样化事实错误。*ArXiv*，abs/2210.12378，2022 年。
- en: Bar (2022) Nitsan Bar. Papertweet. [https://github.com/bnitsan/PaperTweet/](https://github.com/bnitsan/PaperTweet/),
    2022.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bar（2022）Nitsan Bar。Papertweet。 [https://github.com/bnitsan/PaperTweet/](https://github.com/bnitsan/PaperTweet/)，2022
    年。
- en: Botha et al. (2018) Jan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge,
    and Dipanjan Das. Learning to split and rephrase from wikipedia edit history.
    *ArXiv*, abs/1808.09468, 2018.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botha 等（2018）Jan A. Botha, Manaal Faruqui, John Alex, Jason Baldridge, 和 Dipanjan
    Das。从 Wikipedia 编辑历史中学习拆分和改写。*ArXiv*，abs/1808.09468，2018 年。
- en: Brody et al. (2020) Shaked Brody, Uri Alon, and Eran Yahav. A structural model
    for contextual code changes. *Proceedings of the ACM on Programming Languages*,
    4:1 – 28, 2020.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brody 等（2020）Shaked Brody, Uri Alon, 和 Eran Yahav。用于上下文代码变更的结构模型。*Proceedings
    of the ACM on Programming Languages*，4:1 – 28，2020 年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell 等。语言模型是少样本学习者。*Advances in Neural Information Processing Systems*，33:1877–1901，2020
    年。
- en: Brynjolfsson et al. (2023) Erik Brynjolfsson, Danielle Li, and Lindsey R Raymond.
    Generative ai at work. Technical report, National Bureau of Economic Research,
    2023.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brynjolfsson 等（2023）Erik Brynjolfsson, Danielle Li, 和 Lindsey R Raymond。生成式
    AI 在工作中的应用。技术报告，国家经济研究局，2023 年。
- en: Cao et al. (2020) Mengyao Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung.
    Factual error correction for abstractive summarization models. *ArXiv*, abs/2010.08712,
    2020.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等（2020）Mengyao Cao, Yue Dong, Jiapeng Wu, 和 Jackie Chi Kit Cheung。抽象总结模型中的事实错误纠正。*ArXiv*，abs/2010.08712，2020
    年。
- en: Chang et al. (2023) Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy,
    Dipendra Misra, and Wen Sun. Learning to generate better than your llm. *arXiv
    preprint arXiv:2306.11816*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等（2023）Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra
    Misra, 和 Wen Sun。学习生成比你的 LLM 更好的内容。*arXiv preprint arXiv:2306.11816*，2023 年。
- en: 'Chen et al. (2018) Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël
    Pouchet, Denys Poshyvanyk, and Monperrus Martin. Sequencer: Sequence-to-sequence
    learning for end-to-end program repair. *IEEE Transactions on Software Engineering*,
    47:1943–1959, 2018.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2018）Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Noël Pouchet,
    Denys Poshyvanyk, 和 Monperrus Martin。Sequencer：用于端到端程序修复的序列到序列学习。*IEEE Transactions
    on Software Engineering*，47:1943–1959，2018 年。
- en: 'Cheng et al. (2023) Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie,
    and Adith Swaminathan. Llf-bench: Benchmark for interactive learning from language
    feedback. *arXiv preprint arXiv:2312.06853*, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2023）Ching-An Cheng, Andrey Kolobov, Dipendra Misra, Allen Nie, 和 Adith
    Swaminathan。LLF-Bench：用于从语言反馈中进行交互式学习的基准。*arXiv preprint arXiv:2312.06853*，2023
    年。
- en: Chong et al. (2023) Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye
    Jin, Liner Yang, Yange Fan, Hanghang Fan, and Erhong Yang. Leveraging prefix transfer
    for multi-intent text revision. *Annual Meeting of the Association for Computational
    Linguistics*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chong 等（2023）Ruining Chong, Cunliang Kong, Liu Wu, Zhenghao Liu, Ziye Jin, Liner
    Yang, Yange Fan, Hanghang Fan, 和 Erhong Yang。利用前缀迁移进行多意图文本修订。*Annual Meeting of
    the Association for Computational Linguistics*，2023 年。
- en: Clement et al. (2019) Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe,
    and Alexander A. Alemi. On the use of arxiv as a dataset, 2019.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clement 等（2019）Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe, 和 Alexander
    A. Alemi。关于将 arxiv 用作数据集，2019 年。
- en: 'D’Arcy et al. (2023) Mike D’Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl,
    Jonathan Bragg, Tom Hope, and Doug Downey. Aries: A corpus of scientific paper
    edits made in response to peer reviews. *ArXiv*, abs/2306.12587, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D’Arcy 等（2023）Mike D’Arcy, Alexis Ross, Erin Bransom, Bailey Kuehl, Jonathan
    Bragg, Tom Hope, 和 Doug Downey。Aries：响应同行评审的科学论文编辑语料库。*ArXiv*，abs/2306.12587，2023
    年。
- en: 'Deng et al. (2022) Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang,
    Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing
    discrete text prompts with reinforcement learning. *arXiv preprint arXiv:2205.12548*,
    2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2022）Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo,
    Tianmin Shu, Meng Song, Eric P Xing, 和 Zhiting Hu。RLPrompt：使用强化学习优化离散文本提示。*arXiv
    preprint arXiv:2205.12548*，2022 年。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *North American Chapter of the Association for Computational Linguistics*,
    2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert：用于语言理解的深度双向变换器的预训练。*北美计算语言学协会*，2019年。
- en: 'Dohmke (2022) Thomas Dohmke. Github copilot is generally available to all developers.
    [https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/),
    2022. Accessed: April-20-2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dohmke (2022) Thomas Dohmke. Github copilot 现已向所有开发者开放。 [https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/)，2022年。访问时间：2024年4月20日。
- en: Du et al. (2022) Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa
    Lopez, and Dongyeop Kang. Understanding iterative revision from human-written
    text. *ArXiv*, abs/2203.03802, 2022.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2022) Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae Myung Kim, Melissa
    Lopez, 和 Dongyeop Kang. 从人工编写文本中理解迭代修订。*ArXiv*，abs/2203.03802，2022年。
- en: Faltings et al. (2020) Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett,
    Chris Quirk, Jianfeng Gao, and Bill Dolan. Text editing by command. *ArXiv*, abs/2010.12826,
    2020.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faltings et al. (2020) Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett,
    Chris Quirk, Jianfeng Gao, 和 Bill Dolan. 通过命令进行文本编辑。*ArXiv*，abs/2010.12826，2020年。
- en: 'Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Sherry
    Wu, Graham Neubig, and André F. T. Martins. Bridging the gap: A survey on integrating
    (human) feedback for natural language generation. *ArXiv*, abs/2305.00955, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Sherry Wu, Graham Neubig, 和 André F. T. Martins. 弥合差距：关于自然语言生成中（人类）反馈整合的调查。*ArXiv*，abs/2305.00955，2023年。
- en: Foundation (2022) Wikimedia Foundation. Wikimedia downloads. [https://dumps.wikimedia.org](https://dumps.wikimedia.org),
    2022.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foundation (2022) Wikimedia Foundation. Wikimedia 下载。 [https://dumps.wikimedia.org](https://dumps.wikimedia.org)，2022年。
- en: Garivier et al. (2016) Aurélien Garivier, Tor Lattimore, and Emilie Kaufmann.
    On explore-then-commit strategies. *Advances in Neural Information Processing
    Systems*, 29, 2016.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garivier et al. (2016) Aurélien Garivier, Tor Lattimore, 和 Emilie Kaufmann.
    关于探索后承诺策略。*神经信息处理系统进展*，29，2016年。
- en: Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al.
    Direct language model alignment from online ai feedback. *arXiv preprint arXiv:2402.04792*,
    2024.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
    Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, 等等。来自在线人工智能反馈的直接语言模型对齐。*arXiv预印本
    arXiv:2402.04792*，2024年。
- en: Guu et al. (2017) Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy
    Liang. Generating sentences by editing prototypes. *Transactions of the Association
    for Computational Linguistics*, 6:437–450, 2017.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guu et al. (2017) Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, 和 Percy
    Liang. 通过编辑原型生成句子。*计算语言学学会会刊*，6:437–450，2017年。
- en: 'Hancock et al. (2019) Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazaré,
    and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot!
    *Annual Meeting of the Association for Computational Linguistics*, 2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hancock et al. (2019) Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazaré,
    和 Jason Weston. 从部署后的对话中学习：喂饱自己，聊天机器人！ *计算语言学协会年会*，2019年。
- en: 'Hua et al. (2019) Xinyu Hua, Mitko Nikolov, Nikhil Badugu, and Lu Wang. Argument
    mining for understanding peer reviews. *Proceedings of the 2019 Conference of
    the North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long and Short Papers)*, June 2019.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua et al. (2019) Xinyu Hua, Mitko Nikolov, Nikhil Badugu, 和 Lu Wang. 用于理解同行评审的论证挖掘。*2019年北美计算语言学协会人类语言技术会议论文集，第1卷（长篇和短篇论文）*，2019年6月。
- en: 'Kershaw & Koeling (2020) Daniel James Kershaw and R. Koeling. Elsevier oa cc-by
    corpus. *ArXiv*, abs/2008.00774, 2020. doi: https://doi.org/10.48550/arXiv.2008.00774.
    URL [https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs](https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kershaw & Koeling (2020) Daniel James Kershaw 和 R. Koeling. Elsevier oa cc-by
    语料库。*ArXiv*，abs/2008.00774，2020年。doi: [https://doi.org/10.48550/arXiv.2008.00774](https://doi.org/10.48550/arXiv.2008.00774)。网址
    [https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs](https://elsevier.digitalcommonsdata.com/datasets/zm33cdndxs)。'
- en: Kim et al. (2022) Zae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar, and Dongyeop
    Kang. Improving iterative text revision by learning where to edit from other revision
    tasks. *ArXiv*, abs/2212.01350, 2022.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2022）Zae Myung Kim, Wanyu Du, Vipul Raheja, Dhruv Kumar 和 Dongyeop Kang。通过学习其他修订任务中的编辑位置来改善迭代文本修订。*ArXiv*，abs/2212.01350，2022年。
- en: 'Laban et al. (2023) Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq R.
    Joty, Caiming Xiong, and Chien-Sheng Wu. Swipe: A dataset for document-level simplification
    of wikipedia pages. *Annual Meeting of the Association for Computational Linguistics*,
    2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laban 等（2023）Philippe Laban, Jesse Vig, Wojciech Kryscinski, Shafiq R. Joty,
    Caiming Xiong 和 Chien-Sheng Wu。Swipe：一个用于维基百科页面文档级简化的数据集。*计算语言学协会年会*，2023年。
- en: Lee et al. (2024) Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
    Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David
    Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti
    Dutta, Jin L.C. Guo, Md. Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk,
    Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, S. Sterman, Sitong
    Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski,
    Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, and Pao Siangliulue.
    A design space for intelligent and interactive writing assistants. *Conference
    on Human Factors in Computing Systems*, abs/2403.14117, 2024.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等（2024）Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
    Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David
    Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti
    Dutta, Jin L.C. Guo, Md. Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei, Agnia Sergeyuk,
    Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, S. Sterman, Sitong
    Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski,
    Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen 和 Pao Siangliulue。智能和互动写作助手的设计空间。*计算机系统中的人因会议*，abs/2403.14117，2024年。
- en: Levenshtein (1965) Vladimir I. Levenshtein. Binary codes capable of correcting
    deletions, insertions, and reversals. *Soviet physics. Doklady*, 10:707–710, 1965.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levenshtein（1965）Vladimir I. Levenshtein。能够纠正删除、插入和反转的二进制代码。*苏维埃物理学. 文献报告*，10:707–710，1965年。
- en: Li et al. (2023) Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael
    Bendersky. Automatic prompt rewriting for personalized text generation. *arXiv
    preprint arXiv:2310.00152*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong 和 Michael Bendersky。用于个性化文本生成的自动提示重写。*arXiv
    预印本 arXiv:2310.00152*，2023年。
- en: Li et al. (2016) Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc’Aurelio Ranzato,
    and Jason Weston. Dialogue learning with human-in-the-loop. *ArXiv*, abs/1611.09823,
    2016.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2016）Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc’Aurelio Ranzato
    和 Jason Weston。带有人工环节的对话学习。*ArXiv*，abs/1611.09823，2016年。
- en: Li et al. (2022) Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Chi Kit Cheung,
    and Siva Reddy. Using interactive feedback to improve the accuracy and explainability
    of question answering systems post-deployment. *ArXiv*, abs/2204.03025, 2022.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022）Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Chi Kit Cheung 和 Siva
    Reddy。利用互动反馈来提高问题回答系统部署后的准确性和解释性。*ArXiv*，abs/2204.03025，2022年。
- en: 'Liu et al. (2023) Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X. Liu,
    and Soroush Vosoughi. Second thoughts are best: Learning to re-align with human
    values from text edits. *ArXiv*, abs/2301.00355, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X. Liu 和 Soroush
    Vosoughi。再三考虑最好：通过文本编辑学习与人类价值观重新对齐。*ArXiv*，abs/2301.00355，2023年。
- en: Liu et al. (2022) Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker,
    Dragomir R. Radev, and Ahmed Hassan Awadallah. On improving summarization factual
    consistency from natural language feedback. *Annual Meeting of the Association
    for Computational Linguistics*, 2022.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2022）Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker, Dragomir
    R. Radev 和 Ahmed Hassan Awadallah。基于自然语言反馈改进摘要的事实一致性。*计算语言学协会年会*，2022年。
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis.
    *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
    Human Language Technologies*, June 2011.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas 等（2011）Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew
    Y. Ng 和 Christopher Potts。学习用于情感分析的词向量。*计算语言学协会第49届年会论文集：人类语言技术*，2011年6月。
- en: 'Malaviya et al. (2023) Chaitanya Malaviya, Subin Lee, Dan Roth, and Mark Yatskar.
    Pachinko: Patching interpretable qa models through natural language feedback.
    *ArXiv*, abs/2311.09558, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malaviya 等（2023）Chaitanya Malaviya, Subin Lee, Dan Roth 和 Mark Yatskar。Pachinko：通过自然语言反馈修补可解释的
    QA 模型。*ArXiv*，abs/2311.09558，2023年。
- en: 'Mallinson et al. (2020) Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, and
    Guillermo Garrido. Felix: Flexible text editing through tagging and insertion.
    *ArXiv*, abs/2003.10687, 2020.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mallinson 等人 (2020) Jonathan Mallinson, Aliaksei Severyn, Eric Malmi, 和 Guillermo
    Garrido。Felix: 通过标记和插入实现灵活的文本编辑。*ArXiv*，abs/2003.10687，2020。'
- en: Marrese-Taylor et al. (2020) Edison Marrese-Taylor, Machel Reid, and Yutaka
    Matsuo. Variational inference for learning representations of natural language
    edits. *ArXiv*, abs/2004.09143, 2020.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marrese-Taylor 等人 (2020) Edison Marrese-Taylor, Machel Reid, 和 Yutaka Matsuo。用于学习自然语言编辑表示的变分推断。*ArXiv*，abs/2004.09143，2020。
- en: Marrese-Taylor et al. (2023) Edison Marrese-Taylor, Machel Reid, and Alfredo
    Solano. Edit aware representation learning via levenshtein prediction. *The Fourth
    Workshop on Insights from Negative Results in NLP*, 2023.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marrese-Taylor 等人 (2023) Edison Marrese-Taylor, Machel Reid, 和 Alfredo Solano。通过
    Levenshtein 预测进行编辑感知表示学习。*第四届 NLP 负面结果洞察研讨会*，2023。
- en: Misra et al. (2024) Dipendra Misra, Aldo Pacchiano, and Robert E Schapire. Provable
    interactive learning with hindsight instruction feedback. *arXiv preprint arXiv:2404.09123*,
    2024.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misra 等人 (2024) Dipendra Misra, Aldo Pacchiano, 和 Robert E Schapire。基于事后指导反馈的可证明互动学习。*arXiv
    预印本 arXiv:2404.09123*，2024。
- en: 'Mita et al. (2022) Masato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya
    Mizumoto, Jun Suzuki, and Kentaro Inui. Towards automated document revision: Grammatical
    error correction, fluency edits, and beyond. *ArXiv*, abs/2205.11484, 2022.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mita 等人 (2022) Masato Mita, Keisuke Sakaguchi, Masato Hagiwara, Tomoya Mizumoto,
    Jun Suzuki, 和 Kentaro Inui。迈向自动化文档修订：语法错误纠正、流畅性编辑及其他。*ArXiv*，abs/2205.11484，2022。
- en: 'Mysore et al. (2023) Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang,
    Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, and
    Tara Safavi. Pearl: Personalizing large language model writing assistants with
    generation-calibrated retrievers. *arXiv preprint arXiv:2311.09180*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mysore 等人 (2023) Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve
    Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, 和 Tara Safavi。Pearl:
    通过生成校准检索器个性化大型语言模型写作助手。*arXiv 预印本 arXiv:2311.09180*，2023。'
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted
    question-answering with human feedback. *ArXiv*, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakano 等人 (2021) Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long
    Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William
    Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button,
    Matthew Knight, Benjamin Chess, 和 John Schulman。Webgpt: 带有人类反馈的浏览器辅助问答。*ArXiv*，2021。'
- en: Nguyen et al. (2021) Khanh X Nguyen, Dipendra Misra, Robert Schapire, Miroslav
    Dudík, and Patrick Shafto. Interactive learning from activity description. *International
    Conference on Machine Learning*, pp.  8096–8108, 2021.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人 (2021) Khanh X Nguyen, Dipendra Misra, Robert Schapire, Miroslav Dudík,
    和 Patrick Shafto。通过活动描述进行互动学习。*国际机器学习大会*，第 8096–8108 页，2021。
- en: Ouyang et al. (2022a) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J.
    Lowe. Training language models to follow instructions with human feedback. *ArXiv*,
    2022a.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022a) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, 和 Ryan J. Lowe。训练语言模型以遵循人类反馈的指令。*ArXiv*，2022a。
- en: Ouyang et al. (2022b) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744, 2022b.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人 (2022b) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray 等人。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*，35:27730–27744，2022b。
- en: Petrak et al. (2023) Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai
    Rozanov, and Iryna Gurevych. Learning from free-text human feedback - collect
    new datasets or extend existing ones? *ArXiv*, abs/2310.15758, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petrak 等人 (2023) Dominic Petrak, Nafise Sadat Moosavi, Ye Tian, Nikolai Rozanov,
    和 Iryna Gurevych。从自由文本人类反馈中学习 - 收集新的数据集还是扩展现有的数据集？*ArXiv*，abs/2310.15758，2023。
- en: 'Rajagopal et al. (2022) Dheeraj Rajagopal, Xuchao Zhang, Michael Gamon, Sujay Kumar
    Jauhar, Diyi Yang, and Eduard H. Hovy. One document, many revisions: A dataset
    for classification and description of edit intents. *International Conference
    on Language Resources and Evaluation*, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajagopal et al. (2022) Dheeraj Rajagopal, Xuchao Zhang, Michael Gamon, Sujay
    Kumar Jauhar, Diyi Yang, 和 Eduard H. Hovy. 一份文档，多次修订：用于编辑意图分类和描述的数据集。*国际语言资源与评估会议*，2022年。
- en: Reid & Neubig (2022) Machel Reid and Graham Neubig. Learning to model editing
    processes. *Conference on Empirical Methods in Natural Language Processing*, 2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid & Neubig (2022) Machel Reid 和 Graham Neubig. 学习建模编辑过程。*自然语言处理实证方法会议*，2022年。
- en: 'Reid et al. (2023) Machel Reid, Vincent J. Hellendoorn, and Graham Neubig.
    Diffuser: Diffusion via edit-based reconstruction. *International Conference on
    Learning Representations*, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid et al. (2023) Machel Reid, Vincent J. Hellendoorn, 和 Graham Neubig. Diffuser:
    通过编辑重建进行扩散。*国际学习表征会议*，2023年。'
- en: Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
    Ouyang Long, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting
    human evaluators. *ArXiv*, abs/2206.05802, 2022.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saunders et al. (2022) William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
    Ouyang Long, Jonathan Ward, 和 Jan Leike. 自我批评模型用于辅助人工评估者。*ArXiv*，abs/2206.05802，2022年。
- en: Scheurer et al. (2023) J’er’emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern
    Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models
    with language feedback at scale. *ArXiv*, abs/2303.16755, 2023.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scheurer et al. (2023) J’er’emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun
    Shern Chan, Angelica Chen, Kyunghyun Cho, 和 Ethan Perez. 通过语言反馈大规模训练语言模型。*ArXiv*，abs/2303.16755，2023年。
- en: 'See et al. (2017) Abigail See, Peter J. Liu, and Christopher D. Manning. Get
    to the point: Summarization with pointer-generator networks. *Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, July 2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See et al. (2017) Abigail See, Peter J. Liu, 和 Christopher D. Manning. 直奔主题：使用指针生成网络进行总结。*第55届计算语言学协会年会论文集（第1卷：长篇论文）*，2017年7月。
- en: 'Shen et al. (2023) Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan
    Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, and David Sontag. Beyond
    summarization: Designing ai support for real-world expository writing tasks. *arXiv
    preprint arXiv:2304.02623*, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2023) Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan
    Bragg, Jeff Hammerbacher, Doug Downey, Joseph Chee Chang, 和 David Sontag. 超越总结：为现实世界的说明性写作任务设计AI支持。*arXiv
    预印本 arXiv:2304.02623*，2023年。
- en: 'Shi et al. (2022) Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and
    Jing Xu. When life gives you lemons, make cherryade: Converting feedback from
    bad responses into good labels. *ArXiv*, abs/2210.15893, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2022) Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, 和 Jing
    Xu. 当生活给你柠檬时，做樱桃饮料：将不良响应的反馈转化为良好标签。*ArXiv*，abs/2210.15893，2022年。
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    Mpnet: Masked and permuted pre-training for language understanding. *ArXiv*, abs/2004.09297,
    2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, 和 Tie-Yan Liu.
    Mpnet: 掩码和置换预训练用于语言理解。*ArXiv*，abs/2004.09297，2020年。'
- en: 'Stahlberg & Kumar (2020) Felix Stahlberg and Shankar Kumar. Seq2edits: Sequence
    transduction using span-level edit operations. *Conference on Empirical Methods
    in Natural Language Processing*, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stahlberg & Kumar (2020) Felix Stahlberg 和 Shankar Kumar. Seq2edits: 使用跨度级编辑操作的序列转导。*自然语言处理实证方法会议*，2020年。'
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning
    to summarize from human feedback. *ArXiv*, abs/2009.01325, 2020.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, 和 Paul Christiano. 从人类反馈中学习总结。*ArXiv*，abs/2009.01325，2020年。
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth 等人。Gemini: 一系列高能力的多模态模型。*arXiv 预印本 arXiv:2312.11805*，2023年。'
- en: 'Wang et al. (2023) Sitong Wang, Lydia B Chilton, and Jeffrey V Nickerson. Writing
    with generative ai: Multi-modal and multi-dimensional tools for journalists. *The
    Second Workshop on Intelligent and Interactive Writing Assistants at ACM CHI*,
    2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Sitong Wang, Lydia B Chilton, 和 Jeffrey V Nickerson. 使用生成式AI进行写作：面向记者的多模态和多维度工具。*ACM
    CHI 第二届智能和交互式写作助手研讨会*，2023年。
- en: Weston (2016) Jason Weston. Dialog-based language learning. *ArXiv*, abs/1604.06045,
    2016.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston (2016) Jason Weston. 基于对话的语言学习。 *ArXiv*，abs/1604.06045，2016年。
- en: 'Xu et al. (2022) Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau,
    and Jason Weston. Learning new skills after deployment: Improving open-domain
    internet-driven dialogue with human feedback. *Annual Meeting of the Association
    for Computational Linguistics*, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2022) Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau,
    和 Jason Weston. 部署后学习新技能：通过人类反馈改进开放领域互联网驱动的对话。 *计算语言学协会年会*，2022年。
- en: Yao et al. (2021) Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, and Graham
    Neubig. Learning structural edits via incremental tree transformations. *ArXiv*,
    abs/2101.12087, 2021.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2021) Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, 和 Graham Neubig.
    通过增量树转换学习结构编辑。 *ArXiv*，abs/2101.12087，2021年。
- en: Yin et al. (2018) Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt,
    and Alexander L. Gaunt. Learning to represent edits. *ArXiv*, abs/1810.13337,
    2018.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2018) Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt,
    和 Alexander L. Gaunt. 学习表示编辑。 *ArXiv*，abs/1810.13337，2018年。
- en: 'Zhang et al. (2022) Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy
    Li, and Miloš Gligorić. Coditt5: Pretraining for source code and natural language
    editing. *Proceedings of the 37th IEEE/ACM International Conference on Automated
    Software Engineering*, 2022.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy
    Li, 和 Miloš Gligorić. Coditt5：用于源代码和自然语言编辑的预训练。 *第37届IEEE/ACM国际自动化软件工程会议论文集*，2022年。
- en: 'Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *International
    Conference on Learning Representations*, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    和 Yoav Artzi. Bertscore：使用bert评估文本生成。 *国际学习表示会议*，2020年。
- en: Ziegler et al. (2019) Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown,
    Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences. *ArXiv*, 2019.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler et al. (2019) Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown,
    Alec Radford, Dario Amodei, Paul Christiano, 和 Geoffrey Irving. 从人类偏好中微调语言模型。
    *ArXiv*，2019年。
- en: Appendix
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Additional Details
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 额外细节
- en: Dataset Examples.
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集示例。
- en: We list links to dataset sources for our user-provided context in [Table 4](#A2.T4
    "Table 4 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表4](#A2.T4 "表4 ‣ 检索准确率。 ‣ 附录B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐LLM代理")中列出了用户提供的上下文的数据集来源链接。
- en: GPT-4 User’s Edits
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-4 用户的编辑
- en: We list examples of OUR GPT-4 user’s edits with different latent preference
    on summarization in [Table 5](#A2.T5 "Table 5 ‣ Retrieval Accuracy. ‣ Appendix
    B Additional Analysis ‣ Aligning LLM Agents by Learning Latent Preference from
    User Edits").
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表5](#A2.T5 "表5 ‣ 检索准确率。 ‣ 附录B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐LLM代理")中列出了我们的GPT-4用户在总结中具有不同潜在偏好的编辑示例。
- en: GPT-4 User Template.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-4 用户模板。
- en: Prompt templates used by our GPT-4 user are provided in [Table 6](#A2.T6 "Table
    6 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits").
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们GPT-4用户使用的提示模板在[表6](#A2.T6 "表6 ‣ 检索准确率。 ‣ 附录B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐LLM代理")中提供。
- en: CIPHER Templates.
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CIPHER 模板。
- en: Prompt templates used by CIPHER are provided in [Table 7](#A2.T7 "Table 7 ‣
    Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM Agents by
    Learning Latent Preference from User Edits").
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: CIPHER 使用的提示模板在[表7](#A2.T7 "表7 ‣ 检索准确率。 ‣ 附录B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐LLM代理")中提供。
- en: ICL-edit Templates.
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ICL-edit 模板。
- en: Prompt templates used by ICL-edit baseline are provided in [Table 8](#A2.T8
    "Table 8 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ICL-edit 基线使用的提示模板在[表8](#A2.T8 "表8 ‣ 检索准确率。 ‣ 附录B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐LLM代理")中提供。
- en: Appendix B Additional Analysis
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 额外分析
- en: Detailed Expense Analysis.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 详细费用分析。
- en: We list a detailed computational expense of different methods in [Table 9](#A2.T9
    "Table 9 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表9](#A2.T9 "表9 ‣ 检索准确率。 ‣ 附录B 额外分析 ‣ 通过学习用户编辑的潜在偏好对齐LLM代理")中列出了不同方法的详细计算开销。
- en: Failure Cases.
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 失败案例。
- en: We summarize our failure case analysis of CIPHER on summarization in [Table 10](#A2.T10
    "Table 10 ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM
    Agents by Learning Latent Preference from User Edits").
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表 10](#A2.T10 "表 10 ‣ 检索准确率 ‣ 附录 B 额外分析 ‣ 通过学习用户编辑的潜在偏好来对齐 LLM 代理")中总结了CIPHER在总结任务中的失败案例分析。
- en: Retrieval Accuracy.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 检索准确率。
- en: We calculate retrieval accuracy for  CIPHER as the fraction of all retrieved
    contexts that are of the same document type as the currently given context across
    all seeds and time steps. We report the results in [Table 11](#A2.T11 "Table 11
    ‣ Retrieval Accuracy. ‣ Appendix B Additional Analysis ‣ Aligning LLM Agents by
    Learning Latent Preference from User Edits"). We find that the retrieval accuracy
    is higher on the summarization task than on email writing. and using MPNET typically
    performs better than using Bert to encode context.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了CIPHER的检索准确率，即所有检索到的上下文中与当前给定上下文相同文档类型的比例，涵盖所有种子和时间步骤。我们在[表 11](#A2.T11
    "表 11 ‣ 检索准确率 ‣ 附录 B 额外分析 ‣ 通过学习用户编辑的潜在偏好来对齐 LLM 代理")中报告结果。我们发现总结任务上的检索准确率高于邮件写作任务，并且使用MPNET通常比使用Bert编码上下文表现更好。
- en: 'Table 4: Link to each source dataset, from which we randomly sample examples
    as the user-provided context in our tasks.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：每个源数据集的链接，我们从中随机抽取示例作为我们任务中的用户提供的上下文。
- en: '| Data Source | Link and Example |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 数据来源 | 链接和示例 |'
- en: '| --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CNN Daily Mail (See et al., [2017](#bib.bib57)) | [https://huggingface.co/datasets/cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| CNN Daily Mail (见 et al., [2017](#bib.bib57)) | [https://huggingface.co/datasets/cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    |'
- en: '| SLF5K (Stiennon et al., [2020](#bib.bib62)) | [https://huggingface.co/datasets/JeremyAlain/SLF5K](https://huggingface.co/datasets/JeremyAlain/SLF5K)
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SLF5K (Stiennon et al., [2020](#bib.bib62)) | [https://huggingface.co/datasets/JeremyAlain/SLF5K](https://huggingface.co/datasets/JeremyAlain/SLF5K)
    |'
- en: '| Wikidump (Foundation, [2022](#bib.bib23)) | [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Wikidump (Foundation, [2022](#bib.bib23)) | [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)
    |'
- en: '| Arxiv (Clement et al., [2019](#bib.bib15)) | [https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers](https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Arxiv (Clement et al., [2019](#bib.bib15)) | [https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers](https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers)
    |'
- en: '| IMDb (Maas et al., [2011](#bib.bib39)) | [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| IMDb (Maas et al., [2011](#bib.bib39)) | [https://huggingface.co/datasets/imdb](https://huggingface.co/datasets/imdb)
    |'
- en: '| PaperTweet (Bar, [2022](#bib.bib5)) | [https://huggingface.co/datasets/nitsanb/paper_tweet](https://huggingface.co/datasets/nitsanb/paper_tweet)
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| PaperTweet (Bar, [2022](#bib.bib5)) | [https://huggingface.co/datasets/nitsanb/paper_tweet](https://huggingface.co/datasets/nitsanb/paper_tweet)
    |'
- en: '| Elsevier OA CC-By (Kershaw & Koeling, [2020](#bib.bib29)) | [https://huggingface.co/datasets/orieg/elsevier-oa-cc-by](https://huggingface.co/datasets/orieg/elsevier-oa-cc-by)
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Elsevier OA CC-By (Kershaw & Koeling, [2020](#bib.bib29)) | [https://huggingface.co/datasets/orieg/elsevier-oa-cc-by](https://huggingface.co/datasets/orieg/elsevier-oa-cc-by)
    |'
- en: 'Table 5: Examples of user edits with different latent preference on summarization.
    The agent output column shows the agent generation without conditioning on any
    preference. The latent preference for the user edits is in bold and parenthesis.
    We truncate some examples due to limited space.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同潜在偏好的用户编辑示例。代理输出列显示在未考虑任何偏好的情况下生成的内容。用户编辑的潜在偏好用**粗体**和括号标出。由于空间限制，我们截断了一些示例。
- en: '| Agent Output | User Edits |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 代理输出 | 用户编辑 |'
- en: '| --- | --- |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Leon Fenner, the suspected gunman in a $2 million diamond heist at Watch
    Standard Jewelry in New York City, was arrested in Maryland. The robbery occurred
    on November 11 and resulted in the injury of the store owner’s elderly father.
    Fenner’s accomplice, Rondu Frisby, was a friend of the store owner and was apprehended
    a week after the heist. The robbery took place during the city’s Veteran’s Day
    Parade, leading to a block shutdown. Fenner and Frisby stole numerous jewels and
    20 luxury watches, but were caught on surveillance cameras. Fenner is set to appear
    before a federal court in Maryland. | (targeted to young children, storytelling,
    short sentences, playful language, interactive, positive) Once upon a time in
    the big, bustling city of New York, during a parade filled with music and marching
    on Veteran’s Day, something sneaky happened at a place called Watch Standard Jewelry.
    This wasn’t just any store; it was a treasure chest full of sparkling diamonds
    and shiny watches that could make your eyes twinkle like stars. Now, two not-so-nice
    men, Leon Fenner and his buddy Rondu Frisby, thought they could be sneaky and
    take these treasures without asking. On November 11, while everyone was watching
    the parade, they tiptoed into the store. But oh no! They accidentally hurt the
    store owner’s dear old dad, which wasn’t very kind at all. [TRUNCATED] |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 在纽约市的 Watch Standard Jewelry 发生了一起价值200万美元的钻石盗窃案，嫌疑犯**利昂·费纳**在马里兰州被逮捕。抢劫发生在11月11日，导致店主的年迈父亲受了伤。费纳的同伙**朗杜·弗里斯比**是店主的朋友，抢劫后一周被抓获。这次抢劫发生在城市的退伍军人节游行期间，导致了街区封锁。费纳和弗里斯比偷走了许多珠宝和20块奢华手表，但被监控摄像头拍到了。费纳将出庭接受马里兰州联邦法院的审理。
    | （针对年轻儿童，讲故事风格，简短句子，富有趣味性，互动性，积极向上）从前，在繁忙的大城市纽约，在退伍军人节的游行中，有一个名叫 Watch Standard
    Jewelry 的地方发生了一件偷偷摸摸的事。这可不是普通的商店，它是一座装满闪闪发光的钻石和光亮手表的宝箱，让你的眼睛闪烁如星星。现在，两位不太友善的男人**利昂·费纳**和他的朋友**朗杜·弗里斯比**，以为他们可以偷偷拿走这些珍宝。11月11日，当大家都在看游行时，他们悄悄进入了商店。但是哦，不！他们不小心伤害了店主的亲爱的老爸爸，这可真是不友善的行为。
    [TRUNCATED] |'
- en: '| The article describes a situation where a neighbor’s cat, one of seven rescued
    by a "cat lady," has started frequenting the author’s front yard after leaving
    its original home due to the presence of other cats. Over the past few months,
    the cat has become more consistent in its visits, meowing at the author and their
    family members for attention. The cat allows itself to be petted and shows affectionate
    behavior, such as rubbing against legs and headbutting hands, until it is fed.
    After eating, the cat typically leaves but returns in the evening. The author
    is seeking advice on how to handle the situation, expressing concern about feeding
    the cat because it is not legally theirs and there could be potential ownership
    and legal issues. | (second person narrative, brief, show emotions, invoke personal
    reflection, immersive) Imagine yourself in a quaint neighborhood, where the gentle
    purrs of a feline friend have become a familiar part of your daily routine. This
    isn’t just any cat—it’s one of seven, a charming escapee from the home of a local
    ""cat lady,"" seeking solace in the tranquility of your front yard. Over recent
    months, this whiskered wanderer has turned your doorstep into its personal stage,
    meowing for attention with a persistence that tugs at your heartstrings. Each
    encounter with this affectionate creature is a dance of mutual admiration. You
    find yourself captivated by its soft fur and the way it eagerly rubs against your
    legs, headbutting your hands in a silent plea for affection. [TRUNCATED] |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 文章描述了一个邻居的猫咪的情况，这只猫咪是由“猫女士”救助的七只猫之一，离开了它原来的家因为那里有其他猫。过去几个月，这只猫咪开始频繁光顾作者的前院，向作者及其家人喵喵叫以寻求关注。这只猫允许被抚摸，并表现出亲昵的行为，比如蹭腿和用头顶你的手，直到被喂食。吃过东西后，这只猫通常离开，但晚上会回来。作者在寻求如何处理这种情况的建议，对喂食这只猫感到担忧，因为这只猫不合法属于他们，可能会引发所有权和法律问题。
    | （第二人称叙述，简短，表现情感，引发个人反思，沉浸式）想象一下自己在一个古朴的社区中，那里的温柔猫咪的呼噜声已成为你日常生活的一部分。这只猫咪不是普通的猫，它是七只猫中的一员，是从本地“猫女士”家逃脱的可爱逃亡者，寻求在你前院的宁静中得到慰藉。最近几个月，这只有胡须的流浪者将你的门口变成了它的个人舞台，用坚持不懈的喵喵声寻求关注，触动了你的心弦。每次与这只亲昵的生物相遇，都是一场相互欣赏的舞蹈。你发现自己被它柔软的毛发和它那热切蹭腿、用头顶手的举动所吸引。
    [TRUNCATED] |'
- en: '| The Puerto Plata Province is a northern province in the Dominican Republic,
    known for its beautiful beaches and as a popular tourist destination since the
    late 1990s. Its capital is San Felipe de Puerto Plata, named after the silver
    clouds Columbus saw on a mountain nearby in 1493\. The city of Puerto Plata was
    founded in 1502. Historically, the first European town in the Americas, La Isabela,
    was established by Columbus near Puerto Plata in 1493\. In 1605, the governor
    Antonio de Osorio destroyed Puerto Plata and other towns to curb illegal trade,
    leading to the formation of new settlements. Puerto Plata became a Maritime District
    in 1865 and was later recognized as a province in the 1907 constitution. Over
    time, it expanded to include more municipalities and municipal districts, with
    the latest additions in 2007. [TRUNCATED] | (bullet points, parallel structure,
    brief) - Location: Northern Dominican Republic'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '| 波多黎各省是多米尼加共和国北部的一个省，以其美丽的海滩和自1990年代末以来作为受欢迎的旅游目的地而闻名。其首府为圣费利佩·德·波多黎各，得名于1493年哥伦布在附近山上看到的银云。波多黎各市成立于1502年。历史上，第一个美洲的欧洲城镇拉伊萨贝拉由哥伦布于1493年在波多黎各附近建立。1605年，州长安东尼奥·德·奥索里奥摧毁了波多黎各及其他城镇，以遏制非法贸易，导致了新定居点的形成。波多黎各在1865年成为海洋区，并于1907年宪法中被认定为省。随着时间的推移，它扩展到包括更多的市镇和市镇区，最新的扩展发生在2007年。[省略]
    |（要点，平行结构，简洁）-   位置：多米尼加共和国北部'
- en: '- Capital: San Felipe de Puerto Plata'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '- 首府：圣费利佩·德·波多黎各'
- en: '- Founding: 1502 by Europeans'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '- 成立：1502年由欧洲人'
- en: '- Historical site: La Isabela (1493, first European town in the Americas)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '- 历史遗址：拉伊萨贝拉（1493年，美洲第一个欧洲城镇）'
- en: '- 1605: Destruction by Governor Antonio de Osorio to stop illegal trade'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '- 1605年：由州长安东尼奥·德·奥索里奥摧毁以制止非法贸易'
- en: '- Status: Became Maritime District in 1865, recognized as province in 1907'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '- 状况：于1865年成为海洋区，1907年被认可为省'
- en: '- Expansion: Added more municipalities, latest in 2007'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '- 扩展：增加了更多市镇，最新的是2007年'
- en: '- Borders: Atlantic Ocean, Espaillat, Santiago, Valverde, Monte Cristi provinces'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '- 边界：大西洋、埃斯帕亚特、圣地亚哥、瓦尔韦德、蒙特克里斯提省'
- en: '[TRUNCATED] |'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[省略] |'
- en: '| The article discusses the challenge of detecting and classifying stuttered
    speech, which affects many people but has been subject to limited research. The
    authors present a new model that differs from most existing methods, which typically
    use language models for stutter identification. Instead, their model is based
    on acoustic features, which allows it to identify various kinds of stuttering
    disfluencies without relying on speech recognition technology. The proposed model
    combines a deep residual network with bidirectional long short-term memory (LSTM)
    layers to classify different stutter types. It has been shown to significantly
    outperform current leading methods, reducing the average miss rate to 10.03%,
    which is a 27% improvement over the state-of-the-art. | (tweet style, simple English,
    inquisitive, skillful foreshadowing, with emojis) Ever struggled with stuttering
    or know someone who does? ![[Uncaptioned image]](img/b594cfbe3c591525ade65441337b58c9.png)
    Research on detecting stutters hasn’t been huge, but guess what? A new study is
    changing the game! ![[Uncaptioned image]](img/f709c0a7963abcb74f40f1766bbe9ac8.png)
    Instead of just looking at words, this smart model listens to how things are said.
    ![[Uncaptioned image]](img/c2ce2925a4f77ee92ff4c58064a1985f.png) It’s like a detective
    for sounds, using cool tech called deep learning and memory networks to spot different
    stutter types. ![[Uncaptioned image]](img/03a93b8ec12f0c683c32d9808b3c47dc.png)
    And it’s crushing it, beating old methods by 27%! Missed stutters? Down to just
    10%! ![[Uncaptioned image]](img/d85321c36403930d76d688a37abae723.png) This could
    be a big deal for speech therapy! ![[Uncaptioned image]](img/4286c387c926c55ce254d0d6170f7cc6.png)
    #Stuttering # SpeechTech # Innovation  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 文章讨论了检测和分类口吃语音的挑战，这影响了许多人，但相关研究有限。作者提出了一种不同于大多数现有方法的新模型，这些现有方法通常使用语言模型来识别口吃。相反，他们的模型基于声学特征，这使得它能够识别各种类型的口吃不流畅现象，而无需依赖语音识别技术。该模型结合了深度残差网络和双向长短期记忆（LSTM）层来分类不同的口吃类型。研究表明，它显著优于当前的主流方法，将平均漏检率降低至
    10.03%，比现有技术进步了 27%。 | (推文风格，简单英语，探究性，巧妙的预示，带有表情符号) 曾经为口吃而苦恼，或者认识有人有这个问题吗？ ![[无标题图片]](img/b594cfbe3c591525ade65441337b58c9.png)
    研究口吃的工作还不多，但猜猜怎么了？ 一项新研究正在改变游戏规则！ ![[无标题图片]](img/f709c0a7963abcb74f40f1766bbe9ac8.png)
    这个聪明的模型不仅仅看词汇，它还倾听说话的方式。 ![[无标题图片]](img/c2ce2925a4f77ee92ff4c58064a1985f.png)
    它就像声音侦探，使用深度学习和记忆网络这样的酷炫技术来识别不同的口吃类型。 ![[无标题图片]](img/03a93b8ec12f0c683c32d9808b3c47dc.png)
    它表现得非常出色，击败了旧方法 27%！ 漏检的口吃？ 下降到仅 10%！ ![[无标题图片]](img/d85321c36403930d76d688a37abae723.png)
    这可能对语言治疗大有裨益！ ![[无标题图片]](img/4286c387c926c55ce254d0d6170f7cc6.png) #口吃 # 语音技术
    # 创新  |'
- en: '| The article discusses a movie that begins promisingly but later becomes confusing
    due to a poorly depicted portrayal of ancient black magic rituals. The male lead
    delivers a strong performance despite his character’s unfavorable outcome in the
    climax. The movie is compared to a mix of "Boomerang" and "Extremities." | (question
    answering style, direct, concise) What is the general sentiment of the article
    about the movie? The article suggests that the movie starts off well but eventually
    becomes muddled. The depiction of ancient black magic rituals is criticized for
    being confusing. How does the article describe the male lead’s performance? The
    male lead’s performance is described as strong, even though his character does
    not have a favorable outcome in the climax. What films is the movie compared to
    in the article? The movie is compared to ""Boomerang"" and ""Extremities"" in
    the article. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 文章讨论了一部开始时很有前景，但由于对古代黑魔法仪式的刻画不佳而变得令人困惑的电影。尽管男主角的角色在高潮部分结局不佳，但他的表现依然强劲。该电影被比作《回旋镖》和《极端困境》的混合体。
    | (问答风格，直接，简洁) 文章对电影的总体情感是什么？ 文章认为电影开始时表现良好，但最终变得混乱。对古代黑魔法仪式的描绘被批评为令人困惑。 文章如何描述男主角的表现？
    男主角的表现被描述为强劲，即使他的角色在高潮部分没有一个好的结局。 文章中电影被比作了哪些影片？ 电影在文章中被比作《回旋镖》和《极端困境》。'
- en: 'Table 6: Prompt templates for the AI user. The first step is to prompt the
    user for yes/no answer regarding satisfaction. If the answer is no, the second
    step is to ask the user edit the agent output according to the latent preference.
    If the answer is yes, the agent output receives 0 edits.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：AI 用户的提示模板。第一步是询问用户关于满意度的“是/否”回答。如果答案是否，第二步是要求用户根据潜在偏好编辑代理输出。如果答案是是，代理输出将不做任何编辑。
- en: '|  | Summarization | Email Writing |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | 摘要 | 邮件写作 |'
- en: '| Step 1 | Article: {user-provided article} Summary: {agent-generated summary}'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '| 步骤 1 | 文章：{user-provided article} 摘要：{agent-generated summary}'
- en: 'Is the above summary of the above article good for person who would love to
    use the following style: {latent user preference}? Please answer yes or no. |
    Notes: {user-provided notes} Email: {agent-generated email}'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 上述文章的摘要是否适合喜欢以下风格的用户：{latent user preference}？请回答“是”或“否”。| 注释：{user-provided
    notes} 邮件：{agent-generated email}
- en: 'Is the above email based on the above notes good for a user who wants the following
    style: {latent user preference}? Please answer yes or no. |'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 上述邮件是否适合想要以下风格的用户：{latent user preference}？请回答“是”或“否”。|
- en: '| Step 2 | Summary: {agent-generated summary} Please revise the above summary
    of an article to meet your style: {latent user preference}. | Email: {agent-generated
    email} Assume that you prefer {latent user preference}. Please revise the above
    email to meet your style. |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 2 | 摘要：{agent-generated summary} 请根据你的风格修订上述文章的摘要：{latent user preference}。|
    邮件：{agent-generated email} 假设你偏好{latent user preference}。请根据你的风格修订上述邮件。|'
- en: 'Table 7: Prompt templates for CIPHER.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：CIPHER 的提示模板。
- en: '|  | Summarization | Email Writing |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | 摘要 | 邮件写作 |'
- en: '| Task prompt conditioned on inferred preference ([line 6](#alg1.l6a "In Algorithm
    1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through Retrieval
    and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference from User
    Edits") in[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of CIPHER. ‣
    3 Learning User Preference through Retrieval and Aggregation ‣ Aligning LLM Agents
    by Learning Latent Preference from User Edits")) | Article: {user-provided article}
    Assume that you need to summarize the above article for a user, who prefers the
    following style: {inferred user preference}. Please write a summary of the above
    article to address those specified preferences. | Notes: {user-provided notes}
    These notes are written by a user who prefers the following style of emails: {inferred
    user preference}. Please write a short email based on the above notes to address
    those specified preferences. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 任务提示基于推断的偏好（[第 6 行](#alg1.l6a "在算法 1 ‣ CIPHER 的计算成本。 ‣ 3 通过检索和聚合学习用户偏好 ‣
    通过学习用户编辑中的潜在偏好对齐 LLM 代理") 在[算法 1](#alg1a "算法 1 ‣ CIPHER 的计算成本。 ‣ 3 通过检索和聚合学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好对齐 LLM 代理")）| 文章：{user-provided article} 假设你需要根据以下风格为用户总结上述文章：{inferred
    user preference}。请撰写一份总结，以满足这些指定的偏好。| 注释：{user-provided notes} 这些注释由偏好以下邮件风格的用户编写：{inferred
    user preference}。请根据上述注释撰写一封简短的邮件，以满足这些指定的偏好。|'
- en: '| Prompt to infer user preference based on revision ([line 12](#alg1.l12 "In
    Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") in[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of
    CIPHER. ‣ 3 Learning User Preference through Retrieval and Aggregation ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits")) | Original summary
    of an article: {agent-generated summary} Revised summary by a user: {user revision}'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '| 提示以根据修订推断用户偏好（[第 12 行](#alg1.l12 "在算法 1 ‣ CIPHER 的计算成本。 ‣ 3 通过检索和聚合学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好对齐 LLM 代理") 在[算法 1](#alg1a "算法 1 ‣ CIPHER 的计算成本。 ‣ 3 通过检索和聚合学习用户偏好
    ‣ 通过学习用户编辑中的潜在偏好对齐 LLM 代理")）| 原始文章摘要：{agent-generated summary} 用户修订的摘要：{user revision}'
- en: 'Based on the edits and revision by this user on the original summary in the
    above examples, what do you find about this user’s generic preference in terms
    of writing style and formatting? Please answer in a short phrase and only recommend
    those preferences that are widely used. | Original email: {agent-generated email}
    Revised email: {user revision}'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户对上述示例中原始摘要的编辑和修订，你认为该用户在写作风格和格式方面的通用偏好是什么？请用简短的短语回答，并仅推荐那些广泛使用的偏好。| 原始邮件：{agent-generated
    email} 修订后的邮件：{user revision}
- en: Based on the edits and revision by this user on the original email in the above
    examples, what do you find about this user’s generic preference in terms of writing
    style and formatting? Please answer in a short phrase and only recommend those
    preferences that are widely used. |
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 根据用户对上述示例中原始邮件的编辑和修订，你认为该用户在写作风格和格式方面的通用偏好是什么？请用简短的短语回答，并仅推荐那些广泛使用的偏好。|
- en: '| Prompt to consolidate inferred preferences from history ([line 5](#alg1.l5a
    "In Algorithm 1 ‣ Computational Cost of CIPHER. ‣ 3 Learning User Preference through
    Retrieval and Aggregation ‣ Aligning LLM Agents by Learning Latent Preference
    from User Edits") in[Algorithm 1](#alg1a "Algorithm 1 ‣ Computational Cost of
    CIPHER. ‣ 3 Learning User Preference through Retrieval and Aggregation ‣ Aligning
    LLM Agents by Learning Latent Preference from User Edits")) | List of user preferences
    successfully being used to generate summaries of similar documents: - {inferred
    preference in a retrieved example}'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '| 汇总从历史推断的偏好提示（[line 5](#alg1.l5a "在算法1 ‣ CIPHER的计算成本。 ‣ 3 通过检索和聚合学习用户偏好 ‣
    通过学习用户编辑中的潜在偏好对齐LLM代理") 在[算法1](#alg1a "算法1 ‣ CIPHER的计算成本。 ‣ 3 通过检索和聚合学习用户偏好 ‣
    通过学习用户编辑中的潜在偏好对齐LLM代理")） | 成功用于生成类似文档摘要的用户偏好列表： - {在检索示例中的推断偏好}'
- en: '- {inferred preference in a retrieved example}'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '- {在检索示例中的推断偏好}'
- en: …
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Based on the the above examples, please come up with short phrase with the
    most represented summarization preferences of the user. | List of user preferences
    successfully being used to generate emails of a similar kind: - {inferred preference
    in a retrieved example}'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述示例，请提出最能代表用户摘要偏好的短语。 | 成功用于生成类似电子邮件的用户偏好列表： - {在检索示例中的推断偏好}
- en: '- {inferred preference in a retrieved example}'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '- {在检索示例中的推断偏好}'
- en: …
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: Based on the the above examples, please come up with short phrase with the most
    represented writing preferences of this user. |
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述示例，请提出最能代表该用户写作偏好的短语。
- en: 'Table 8: Prompt templates for the ICL-edit baseline.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：ICL-edit基线的提示模板。
- en: '|  | Summarization | Email Writing |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  | 摘要 | 写电子邮件 |'
- en: '| Prompt with retrieved user edit examples | Original summary of an article:
    {agent-generated summary in a retrieved example} Revised summary by a user: {user
    revision in a retrieved example}'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '| 包含检索用户编辑示例的提示 | 原始文章摘要：{代理生成的摘要在检索示例中} 用户修订的摘要：{在检索示例中的用户修订}'
- en: 'Original summary of an article: {agent-generated summary in a retrieved example}'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文章摘要：{代理生成的摘要在检索示例中}
- en: 'Revised summary by a user: {user revision in a retrieved example}'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 用户修订的摘要：{在检索示例中的用户修订}
- en: …
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Article: {user-provided article}'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：{用户提供的文章}
- en: 'Based on the edits and revision by this user on the original summary in the
    above examples, Please summarize the above article: | Original summary of an article:
    {agent-generated summary in a retrieved example} Revised summary by a user: {user
    revision in a retrieved example}'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 基于用户对上述示例中原始摘要的编辑和修订，请总结上述文章： | 原始文章摘要：{代理生成的摘要在检索示例中} 用户修订的摘要：{在检索示例中的用户修订}
- en: 'Original summary of an article: {agent-generated summary in a retrieved example}'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文章摘要：{代理生成的摘要在检索示例中}
- en: 'Revised summary by a user: {user revision in a retrieved example}'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 用户修订的摘要：{在检索示例中的用户修订}
- en: …
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 'Notes: {user-provided notes}'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项：{用户提供的注意事项}
- en: 'Based on the edits and revision by this user on the original email in the above
    examples, Please write an email based on the above notes for this user: |'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 基于用户对上述示例中原始电子邮件的编辑和修订，请根据上述笔记为该用户写一封电子邮件：
- en: 'Table 9: Expense of different methods: number of BPE tokens in terms of input,
    output and total. Each number is the average across 3 runs (unit is $\cdot 10^{5}$).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：不同方法的费用：输入、输出和总计的BPE标记数。每个数字是3次运行的平均值（单位是 $\cdot 10^{5}$）。
- en: '| Method | Summarization | Email Writing |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 摘要 | 写电子邮件 |'
- en: '|  | Input | Output | Total | Input | Output | Total |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | 输入 | 输出 | 总计 | 输入 | 输出 | 总计 |'
- en: '| Oracle Preference | 1.14 | 0.53 | 1.67 | 0.91 | 0.71 | 1.62 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| Oracle Preference | 1.14 | 0.53 | 1.67 | 0.91 | 0.71 | 1.62 |'
- en: '| No Learning | 1.06 | 0.44 | 1.50 | 0.85 | 0.80 | 1.65 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| No Learning | 1.06 | 0.44 | 1.50 | 0.85 | 0.80 | 1.65 |'
- en: '| E-then-e LPI | 1.16 | 0.83 | 1.99 | 0.94 | 0.79 | 1.73 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| E-then-e LPI | 1.16 | 0.83 | 1.99 | 0.94 | 0.79 | 1.73 |'
- en: '| Continual LPI | 8.14 | 0.75 | 8.89 | 7.89 | 0.73 | 8.63 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| Continual LPI | 8.14 | 0.75 | 8.89 | 7.89 | 0.73 | 8.63 |'
- en: '| ICL-edit-5-MPNET | 7.35 | 0.65 | 8.00 | 11.05 | 1.06 | 12.12 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-MPNET | 7.35 | 0.65 | 8.00 | 11.05 | 1.06 | 12.12 |'
- en: '| ICL-edit-5-BERT | 7.32 | 0.64 | 7.96 | 10.51 | 1.03 | 11.55 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| ICL-edit-5-BERT | 7.32 | 0.64 | 7.96 | 10.51 | 1.03 | 11.55 |'
- en: '| CIPHER-1-MPNET | 2.02 | 0.72 | 2.74 | 1.21 | 0.73 | 1.94 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-MPNET | 2.02 | 0.72 | 2.74 | 1.21 | 0.73 | 1.94 |'
- en: '| CIPHER-5-MPNET | 2.27 | 0.73 | 3.00 | 1.44 | 0.64 | 2.09 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-MPNET | 2.27 | 0.73 | 3.00 | 1.44 | 0.64 | 2.09 |'
- en: '| CIPHER-1-BERT | 2.10 | 0.71 | 2.81 | 1.27 | 0.73 | 1.99 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-BERT | 2.10 | 0.71 | 2.81 | 1.27 | 0.73 | 1.99 |'
- en: '| CIPHER-5-BERT | 2.32 | 0.71 | 3.03 | 1.48 | 0.73 | 2.22 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-BERT | 2.32 | 0.71 | 3.03 | 1.48 | 0.73 | 2.22 |'
- en: 'Table 10: Summary of failure cases on summarization task with CIPHER-5-MPNET.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：CIPHER-5-MPNET在总结任务中的失败案例总结。
- en: '| Type of Failures | Summary | Examples |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 失败类型 | 摘要 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Preference inference based on an output-revision pair ($f_{t}$) (the most
    common failure type) | 1) Not totally wrong but insufficient, i.e. the inferred
    preference only captures a few aspects of user’s latent preference. This is most
    common for news articles and Reddit posts, for which the user shows nuanced preference
    for several aspects. | The dominant missing aspect is brief or short sentences
    across different context, although the agent can infer keywords such as concise.
    For news article context, the agent tends to infer the preference keyword whimsical.
    The agent has difficulty to infer subtle aspects, including invoke personal reflection,
    immersive, positive, parallel structure, inquisitive, and skillful foreshadowing.
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 基于输出-修订对的偏好推断 ($f_{t}$)（最常见的失败类型） | 1) 并非完全错误但不充分，即推断的偏好仅捕捉了用户潜在偏好的几个方面。这在新闻文章和Reddit帖子中最为常见，其中用户对多个方面表现出细致的偏好。
    | 主导的缺失方面是在不同上下文中简洁或短句，尽管代理可以推断出诸如简洁的关键词。对于新闻文章上下文，代理倾向于推断出偏好关键词“异想天开”。代理难以推断出细微的方面，包括引发个人反思、沉浸式、积极、平行结构、好奇和巧妙的预示。
    |'
- en: '|  | 2) Sometimes fail to infer some important aspects, even though the user
    edits clearly show such preference. | The agent often could not infer second-person
    narrative. For question answering style, the agent occasionally only learns consistent
    format. |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | 2) 有时未能推断出一些重要方面，即使用户编辑明显显示了这种偏好。 | 代理通常无法推断出第二人称叙述。对于问答风格，代理偶尔只学习一致的格式。
    |'
- en: '| Consolidation of induced preferences from retrieved interactions ($\tilde{f}_{t}$)
    | Overall, this step can capture the majority preference relatively well, although
    it tends to result in a more general preference compared to the retrieved ones.
    | When both specific phrase second-person narrative and general phrase narrative
    or narration occur in retrieved examples, the agent often chooses to give a final
    preference not including the second-person perspective aspect. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 从检索到的互动中整合诱导的偏好 ($\tilde{f}_{t}$) | 总体而言，这一步可以相对较好地捕捉到主要偏好，尽管与检索到的偏好相比，它往往会导致更为普遍的偏好。
    | 当检索到的示例中同时出现特定短语的第二人称叙述和一般短语叙述时，代理通常选择最终偏好不包括第二人称视角方面。 |'
- en: '| Retrieval of historical examples relevant to the given context | The retrieval
    part in general works reasonably well, with more than half of the retrieved example
    being truly relevant to the given context. Note that one incorrect retrieved example
    typically does not affect the performance, as we instruct the agent to only use
    the most represented preference keywords among all five retrieved examples. |
    The agent sometimes retrieves wrong examples for Wikipedia context when its topic
    very relates to other context, e.g. wrongly retrieving past examples on news articles
    and movie reviews when the topic in the given Wikipedia context relates to these
    domains. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 与给定上下文相关的历史示例检索 | 检索部分总体效果良好，检索到的示例中有超过一半与给定上下文真正相关。注意，一个不正确的检索示例通常不会影响性能，因为我们指示代理仅使用在所有五个检索示例中最具代表性的关键词。
    | 当维基百科上下文的话题与其他上下文密切相关时，代理有时会检索错误的示例，例如在给定的维基百科上下文话题涉及新闻文章和电影评论时错误地检索过去的示例。 |'
- en: 'Table 11: We report retrieval accuracy as the percentage of total retrieved
    document representations across all time steps and seeds that are of the same
    document source type as the context document for which they were retrieved. We
    use 3 seeds. We retrieve 600 examples for $k=1$.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：我们报告检索准确率作为所有时间步骤和种子中与上下文文档相同文档源类型的总检索文档表示的百分比。我们使用了3个种子。我们为$k=1$检索了600个示例。
- en: '| Method | Summarization | Email Writing |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 总结 | 电子邮件写作 |'
- en: '| --- | --- | --- |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CIPHER-1-B | 72.00 | 25.83 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-B | 72.00 | 25.83 |'
- en: '| CIPHER-1-M | 82.00 | 26.33 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-1-M | 82.00 | 26.33 |'
- en: '| CIPHER-5-B | 65.79 | 26.57 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| CIPHER-5-B | 65.79 | 26.57 |'
- en: '| CIPHER-5-M | 76.33 | 25.45 |'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '| CIPHER-5-M | 76.33 | 25.45 |'
