- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:50:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OPEx：一种关于 LLM 中心体的代理在具身指令跟随中的分组件分析
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.03017](https://ar5iv.labs.arxiv.org/html/2403.03017)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.03017](https://ar5iv.labs.arxiv.org/html/2403.03017)
- en: 'Haochen Shi¹, Zhiyuan Sun¹, Xingdi Yuan², Marc-Alexandre Côté², Bang Liu¹¹¹footnotemark:
    1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Haochen Shi¹, Zhiyuan Sun¹, Xingdi Yuan², Marc-Alexandre Côté², Bang Liu¹¹¹footnotemark:
    1'
- en: ¹ Université de Montréal & Mila, Montréal, Canada
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 蒙特利尔大学 & Mila，蒙特利尔，加拿大
- en: ² Microsoft Research, Montréal, Canada
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ² 微软研究院，蒙特利尔，加拿大
- en: '{haochen.shi, zhiyuan.sun, bang.liu}@umontreal.ca,'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{haochen.shi, zhiyuan.sun, bang.liu}@umontreal.ca，'
- en: '{eric.yuan, macote}@microsoft.com   Equal advising.  Canada CIFAR AI Chair.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{eric.yuan, macote}@microsoft.com   平等指导。  加拿大 CIFAR AI 主席。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Embodied Instruction Following (EIF) is a crucial task in embodied learning,
    requiring agents to interact with their environment through egocentric observations
    to fulfill natural language instructions. Recent advancements have seen a surge
    in employing large language models (LLMs) within a framework-centric approach
    to enhance performance in embodied learning tasks, including EIF. Despite these
    efforts, there exists a lack of a unified understanding regarding the impact of
    various components—ranging from visual perception to action execution—on task
    performance. To address this gap, we introduce OPEx, a comprehensive framework
    that delineates the core components essential for solving embodied learning tasks:
    Observer, Planner, and Executor. Through extensive evaluations, we provide a deep
    analysis of how each component influences EIF task performance. Furthermore, we
    innovate within this space by deploying a multi-agent dialogue strategy on a TextWorld
    counterpart, further enhancing task performance. Our findings reveal that LLM-centric
    design markedly improves EIF outcomes, identify visual perception and low-level
    action execution as critical bottlenecks, and demonstrate that augmenting LLMs
    with a multi-agent framework further elevates performance.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 具身指令跟随（EIF）是具身学习中的关键任务，要求代理通过自我中心观察与环境互动，以完成自然语言指令。近期进展中，采用大语言模型（LLMs）进行框架中心方法，以提升具身学习任务（包括
    EIF）的性能。尽管有这些努力，但对各种组件——从视觉感知到动作执行——对任务性能的影响仍缺乏统一理解。为了解决这一差距，我们引入了 OPEx，一个全面的框架，明确了解决具身学习任务所需的核心组件：观察者、规划者和执行者。通过广泛评估，我们深入分析了每个组件如何影响
    EIF 任务性能。此外，我们在此领域创新，部署了一种多代理对话策略在 TextWorld 对应体上，进一步提升了任务性能。我们的研究发现，LLM 中心设计显著改善了
    EIF 结果，识别出视觉感知和低级动作执行作为关键瓶颈，并证明了通过多代理框架增强 LLMs 可以进一步提高性能。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Embodied learning, particularly through tasks like Embodied Instruction Following
    (EIF) Shridhar et al. ([2020a](#bib.bib33)), stands at the forefront of artificial
    intelligence research. EIF, where agents must interpret natural language instructions
    to navigate and act within their environment using egocentric observations, epitomizes
    the challenge of integrating cognitive understanding with physical action. This
    intersection is crucial for developing autonomous agents capable of nuanced interaction
    with complex, real-world environments, marking a significant stride towards more
    advanced and versatile AI systems. As the research community harnesses advancements
    in deep learning, we edge closer to this ambition Baker et al. ([2022](#bib.bib1));
    Min et al. ([2021](#bib.bib21)); Inoue and Ohashi ([2022](#bib.bib13)); Huang
    et al. ([2022a](#bib.bib11)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 具身学习，尤其是通过像具身指令跟随（EIF）这样的任务 Shridhar et al. ([2020a](#bib.bib33))，处于人工智能研究的前沿。EIF
    中，代理必须解释自然语言指令，通过自我中心观察在环境中导航和行动，这体现了将认知理解与物理行动整合的挑战。这一交集对开发能够与复杂现实世界环境进行细致交互的自主代理至关重要，标志着迈向更先进和多功能
    AI 系统的重要一步。随着研究界利用深度学习的进展，我们越来越接近这一目标 Baker et al. ([2022](#bib.bib1))；Min et al.
    ([2021](#bib.bib21))；Inoue and Ohashi ([2022](#bib.bib13))；Huang et al. ([2022a](#bib.bib11))。
- en: Traditional approaches to Embodied Instruction Following (EIF) often rely on
    expert-generated annotations, a process that can be both expensive and challenging
    to scale for real-world applications. In contrast, Large Language Models (LLMs),
    such as those cited in recent studies Inoue and Ohashi ([2022](#bib.bib13)); OpenAI
    ([2023](#bib.bib25)); Wei et al. ([2022a](#bib.bib43)); Driess et al. ([2023](#bib.bib7));
    Touvron et al. ([2023](#bib.bib39)); Huang et al. ([2022a](#bib.bib11), [b](#bib.bib12));
    Liang et al. ([2022](#bib.bib15)); Wang et al. ([2023a](#bib.bib40)); Shinn et al.
    ([2023](#bib.bib32)); Song et al. ([2023](#bib.bib37)), have emerged as a potent
    alternative, showcasing exceptional capabilities in natural language understanding
    and generation. These models, enriched by extensive textual datasets, demonstrate
    significant common-sense reasoning abilities. As a result, there’s a growing trend
    towards leveraging LLM-centric architectures for embodied learning tasks including
    EIF, which promise to simplify planning and execution tasks through a few-shot
    learning paradigm. However, despite their potential, the implementations of EIF
    systems introduce a variety of designs and components across different studies Min
    et al. ([2021](#bib.bib21)); Inoue and Ohashi ([2022](#bib.bib13)); Song et al.
    ([2023](#bib.bib37)); Blukis et al. ([2022](#bib.bib3)); Wang et al. ([2023a](#bib.bib40));
    Zhu et al. ([2023](#bib.bib47)). There remains a notable gap in systematically
    understanding how these disparate elements influence overall task performance,
    underscoring the need for a thorough analysis of LLM-centric methods within the
    context of EIF.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的**体现指令遵循**（EIF）方法通常依赖专家生成的注释，这一过程既昂贵又难以在实际应用中扩展。相对而言，近期研究中提到的大型语言模型（LLMs），如Inoue和Ohashi
    ([2022](#bib.bib13)); OpenAI ([2023](#bib.bib25)); Wei等 ([2022a](#bib.bib43));
    Driess等 ([2023](#bib.bib7)); Touvron等 ([2023](#bib.bib39)); Huang等 ([2022a](#bib.bib11),
    [b](#bib.bib12)); Liang等 ([2022](#bib.bib15)); Wang等 ([2023a](#bib.bib40)); Shinn等
    ([2023](#bib.bib32)); Song等 ([2023](#bib.bib37))，作为一种有效的替代方案，展示了卓越的自然语言理解和生成能力。这些模型通过丰富的文本数据集展现了显著的常识推理能力。因此，越来越多地倾向于利用以LLM为中心的架构来处理包括EIF在内的体现学习任务，这承诺通过少样本学习范式简化规划和执行任务。然而，尽管具有潜力，EIF系统的实施在不同研究中引入了多种设计和组件，Min等
    ([2021](#bib.bib21)); Inoue和Ohashi ([2022](#bib.bib13)); Song等 ([2023](#bib.bib37));
    Blukis等 ([2022](#bib.bib3)); Wang等 ([2023a](#bib.bib40)); Zhu等 ([2023](#bib.bib47))。仍然存在系统理解这些不同元素如何影响整体任务性能的显著差距，突显了在EIF背景下对LLM中心方法进行深入分析的必要性。
- en: 'In addressing the complexities of Embodied Instruction Following (EIF), we
    introduce OPEx, a novel framework designed to systematically outline the essential
    components for mastering embodied learning tasks. OPEx is segmented into three
    core parts: Observer, Planner, and Executor. The Observer component is tasked
    with processing and interpreting sensory inputs, primarily visual, to construct
    an actionable understanding of the agent’s immediate environment. The Planner
    dynamically devises strategic plans as subtasks to complete the tasks based on
    perceptual inputs, effectively bridging the gap between perception and action.
    Lastly, the Executor is responsible for implementing these plans with a skill
    library, which translates several re-useable skills into precise, context-aware
    actions within the environment, ensuring the agent’s interactions are both relevant
    and goal-oriented. This tripartite structure provides a clear delineation of roles
    within the system, facilitating a granular analysis of how each contributes to
    the overarching performance of EIF tasks.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决**体现指令遵循**（EIF）的复杂性时，我们引入了OPEx，一个新颖的框架，旨在系统地概述掌握体现学习任务所需的关键组件。OPEx分为三个核心部分：Observer、Planner和Executor。Observer组件负责处理和解释感官输入，主要是视觉，以构建对代理即时环境的可操作理解。Planner动态制定战略计划，将任务分解为子任务，并根据感知输入进行有效的感知与行动之间的桥接。最后，Executor负责利用技能库执行这些计划，将多个可重用技能转化为环境中的精确、上下文感知的动作，确保代理的互动既相关又以目标为导向。这种三部分结构清晰地划分了系统中的角色，有助于对每个角色如何影响EIF任务的整体表现进行细致分析。
- en: To understand the impact of each OPEx component on performance in EIF tasks,
    we conducted an in-depth analysis. By experimenting with different versions of
    the Observer, Planner, and Executor components, we assessed how each contributes
    to and influences overall success. This approach allowed us to identify the key
    attributes and design choices that enhance the system’s ability to tackle complex
    embodied tasks, providing clear insights into optimizing embodied learning agents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解每个 OPEx 组件对 EIF 任务性能的影响，我们进行了深入分析。通过实验不同版本的观察者、规划者和执行者组件，我们评估了每个组件如何对整体成功做出贡献并产生影响。这种方法使我们能够识别出提升系统应对复杂具身任务能力的关键属性和设计选择，提供了优化具身学习代理的清晰见解。
- en: To further unlock the potential of LLMs in embodied learning, we eliminate the
    influence of visual perception and low-level action execution of the system utilizing
    a pure-text counterpart environment Shridhar et al. ([2020b](#bib.bib34)) and
    further adopt a multi-agent dialogue strategy, splitting the instruction-following
    challenge into distinct reasoning and grounding roles handled by a reasoner agent
    and an actor agent, respectively. This dialogue-driven approach simplifies the
    task into decision-making processes, where both agents utilize world knowledge
    obtained from an explorer. This explorer gathers insights either through direct
    interaction with the environment or from human contributions, thereby enriching
    the collaborative problem-solving capabilities of the reasoner and actor with
    more grounded and informed decision-making.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步挖掘 LLM 在具身学习中的潜力，我们排除了系统视觉感知和低级动作执行的影响，利用纯文本对照环境 Shridhar 等人（[2020b](#bib.bib34)），并进一步采用多智能体对话策略，将指令跟随挑战拆分为由推理代理和执行代理分别处理的不同推理和落实角色。这种对话驱动的方法将任务简化为决策过程，其中两个代理利用从探险者那里获得的世界知识。探险者通过直接与环境互动或从人类贡献中获取洞察，从而丰富了推理者和执行者的协作解决问题能力，使决策更加扎实和知情。
- en: 'Our experimental evaluation was conducted using the ALFRED Shridhar et al.
    ([2020a](#bib.bib33)) and ALFWorld Shridhar et al. ([2020b](#bib.bib34)) benchmarks,
    providing a comprehensive testing ground for our extensive evaluation. The core
    analysis of our experiments underscores significant advancements: the LLM-centric
    approach notably enhances performance in EIF tasks. We pinpoint visual perception
    and low-level action execution as pivotal bottlenecks. Moreover, our results affirm
    that incorporating a multi-agent dialogue strategy into an LLM-centric task solver
    significantly boosts overall task performance on AFLWorld, showcasing the effectiveness
    of our proposed methodology in addressing the complexities of embodied learning
    tasks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验评估使用了 ALFRED Shridhar 等人（[2020a](#bib.bib33)）和 ALFWorld Shridhar 等人（[2020b](#bib.bib34)）基准，提供了全面的测试平台进行我们的广泛评估。我们实验的核心分析强调了显著的进展：LLM
    以中心的方法显著提高了 EIF 任务的性能。我们将视觉感知和低级动作执行定位为关键瓶颈。此外，我们的结果确认，将多智能体对话策略纳入 LLM 以中心的任务求解器显著提升了
    AFLWorld 上的整体任务性能，展示了我们提出的方法在应对具身学习任务复杂性方面的有效性。
- en: 2 Task Formulation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 任务制定
- en: We benchmark our method with ALFRED Shridhar et al. ([2020a](#bib.bib33)) and
    its TextWorld counterpart ALFWorld Shridhar et al. ([2020b](#bib.bib34)). Both
    contain a set of environments associated with long-horizon household tasks specified
    by natural language instructions. The language instruction $L=\{L_{\text{high}},L_{\text{low}}\}$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 ALFRED Shridhar 等人（[2020a](#bib.bib33)）和其文本世界对照 ALFWorld Shridhar 等人（[2020b](#bib.bib34)）对我们的方法进行了基准测试。这两个数据集包含一组与自然语言指令指定的长期家庭任务相关的环境。语言指令为
    $L=\{L_{\text{high}},L_{\text{low}}\}$。
- en: 'Given the language instruction $L$ (success) or reaches the maximum number
    of steps (fail). See Appendix. [A](#A1 "Appendix A Task Example in ALFRED ‣ OPEx:
    A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following")
    for a task example in ALFRED.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '根据语言指令 $L$ （成功）或达到最大步骤数（失败）。请参见附录。[A](#A1 "Appendix A Task Example in ALFRED
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following") 中有 ALFRED 中的任务示例。'
- en: '![Refer to caption](img/08ed5806ec965d9a9abf1e99c09c5c6f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/08ed5806ec965d9a9abf1e99c09c5c6f.png)'
- en: 'Figure 1: Overview of our OPEx framework. We will open-source the code after
    acceptance.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们 OPEx 框架的概述。我们将在接受后开源代码。
- en: 3 Methodology
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'We first provide an overview of the proposed LLM-centric framework (OPEx) in
    Figure [1](#S2.F1 "Figure 1 ‣ 2 Task Formulation ‣ OPEx: A Component-Wise Analysis
    of LLM-Centric Agents in Embodied Instruction Following"). OPEx consists of six
    components: (1) A semantic mapping module to transform the egocentric visual observation
    into a semantic map; (2) An LLM-based planner to decompose the specified language
    task instruction $L$ to store the skills manipulating the agent in the simulated
    environment (e.g, NavigateTo, LookAround, and Explore); (6) A deterministic action
    policy to convert the skills into low-level actions (e.g., RotateRight).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在图 [1](#S2.F1 "Figure 1 ‣ 2 Task Formulation ‣ OPEx: A Component-Wise Analysis
    of LLM-Centric Agents in Embodied Instruction Following") 中提供了所提出的LLM中心框架（OPEx）的概述。OPEx包含六个组件：（1）一个语义映射模块，将自我中心的视觉观察转化为语义图；（2）一个LLM基础规划器，将指定的语言任务指令$L$分解为在模拟环境中操控代理的技能（例如，NavigateTo、LookAround和Explore）；（6）一个确定性行动策略，将技能转换为低级动作（例如，RotateRight）。'
- en: Semantic Mapping Module
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义映射模块
- en: The goal of the semantic mapping module is to create a 2D semantic map $M_{t}$.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 语义映射模块的目标是创建一个2D语义图$M_{t}$。
- en: LLM-based Planner
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM基础规划器
- en: The goal of the LLM-based planner is to decompose a specified language instruction
    $L$. In practice, we utilize Chain-of-Though (CoT) Wei et al. ([2022b](#bib.bib44))
    to prompt GPT-4 OpenAI ([2023](#bib.bib25)) with in-context learning. The corresponding
    prompt examples are demonstrated in the Appendix.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM基础规划器的目标是将指定的语言指令$L$分解。在实践中，我们利用Chain-of-Though (CoT) Wei等人（[2022b](#bib.bib44)）通过上下文学习来提示GPT-4
    OpenAI（[2023](#bib.bib25)）。相应的提示示例在附录中展示。
- en: Example Selector
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例选择器
- en: We have collected a set of prompt examples from 10 episodes within the training
    split for each of the 7 task types, amounting to a total of 70 episodes. As shown
    in Liu et al. ([2022b](#bib.bib18)), choosing which in-context examples to add
    to the prompt can impact the overall performance. Therefore, we further apply
    an example selector to provide the LLM-based planner with the most relevant examples
    by ranking the examples based on the similarity of the input test case and the
    examples. In practice, we employ the example selector from LangChain Chase ([2022](#bib.bib4)),
    which first ranks the examples based on the corresponding embeddings²²2We adopt
    the text-embedding-ada-002 embeddings provided by OpenAI. that have the greatest
    cosine similarity with the inputs, then select top-$K$ examples for in-context
    learning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从每种任务类型的训练分割中的10个实例中收集了一组提示示例，共计70个实例。如刘等人（[2022b](#bib.bib18)）所示，选择添加到提示中的上下文示例可以影响整体性能。因此，我们进一步应用了示例选择器，通过根据输入测试案例与示例的相似性对示例进行排名，为LLM基础规划器提供最相关的示例。在实践中，我们采用了LangChain
    Chase（[2022](#bib.bib4)）中的示例选择器，该选择器首先根据与输入具有最大余弦相似度的相应嵌入²²2我们采用了OpenAI提供的text-embedding-ada-002嵌入进行排序，然后选择前$K$个示例进行上下文学习。
- en: LLM-based Observer
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM基础观察者
- en: 'The goal of the LLM-based observer is to extract information from the environment
    feedback and the agent state, and present it in the form of a natural language
    description $O_{t}^{L}$ in a zero-shot manner. The rationale behind the design
    of the LLM-based observer is twofold: (1) to gather and render the state of the
    environment, enabling the tracking of environment dynamics across time steps and
    facilitating dynamic planning and acting; and (2) to summarize the information
    into a task-centric description, thereby safeguarding the LLM-based executor against
    distractions and hallucinations. The LLM-based observer is querying GPT-3.5-turbo
    with the prompt format shown in the Appendix.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLM基础观察者的目标是从环境反馈和代理状态中提取信息，并以自然语言描述$O_{t}^{L}$的形式呈现，这种方式是零-shot的。LLM基础观察者设计的理由有两个：（1）收集和呈现环境状态，能够跟踪环境动态，促进动态规划和行动；（2）将信息总结为任务中心的描述，从而保护LLM基础执行器免受干扰和幻觉的影响。LLM基础观察者正在查询GPT-3.5-turbo，使用附录中显示的提示格式。
- en: LLM-based Executor
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM基础执行器
- en: Given the current subtask $S_{i}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定当前子任务$S_{i}$。
- en: Skill Library
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 技能库
- en: 'We design a skill library to empower the LLM-based executor with the following
    capabilities: (1) reasoning over language to track progress, handle exceptions
    or adjust the plan according to the situation; (2) acting to support the reasoning
    and collect information about the environment dynamics by controlling the agent.
    Apart from all the interaction actions $A_{I}$. The Explore skill enhances the
    LLM-based executor’s ability to guide the agent in room exploration by sampling
    navigation goals from traversable areas, and it requires no skill action target.
    It is worth noting that we have an initial exploration heuristic for the first
    four calls of the Explore skill, we set the four corners of the room with a higher
    exploration priority. The RequireReplan provides the LLM-based executor with the
    capability to dynamically adjust the plan, improving the robustness to exceptions
    and producing more probability for it to learn from the environment dynamics.
    The LookAround skill enables the LLM-based executor to manipulate the agent to
    look around the environment to get a more comprehensive observation of the room.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了一个技能库，以赋能基于LLM的执行器，具备以下能力：（1）对语言进行推理，以跟踪进度、处理异常或根据情况调整计划；（2）通过控制代理行动来支持推理并收集关于环境动态的信息。除了所有的交互动作
    $A_{I}$ 之外，Explore技能通过从可遍历区域中采样导航目标来增强基于LLM的执行器在房间探索中的能力，它不需要技能动作目标。值得注意的是，我们对Explore技能的前四次调用有一个初步的探索启发式，我们设置房间的四个角落为更高的探索优先级。RequireReplan为基于LLM的执行器提供了动态调整计划的能力，提高了对异常的鲁棒性，并增加了从环境动态中学习的概率。LookAround技能使基于LLM的执行器能够操控代理在环境中环顾四周，以获得对房间更全面的观察。
- en: Deterministic Action Policy
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确定性行动策略
- en: Given the current instruction specified by the action plan [$\mathcal{SL}_{i}$
    as it is supposed to be more robust to the errors from the perception models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于当前由行动计划 [$\mathcal{SL}_{i}$ 指定的指令，它应当对感知模型的错误更具鲁棒性。
- en: Prior Knowledge Integration
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 先验知识整合
- en: Due to the lack of prior knowledge of the specific environment, OPEx frequently
    fails even on ALFWorld where the impact of perception and action modules are ablated.
    For instance, OPEx may continuously fail for trying to pick up objects across
    various episodes due to the lack of the knowledge that agent can not directly
    hold more than 1 object in ALFRED. Furthermore, a system with a single agent trying
    to handle planning and grounding simultaneously often struggles to learn the optimal
    timing for switching between planning and grounding. To bridge the gap, we propose
    improving OPEx by splitting the reasoning and grounding issues with a multi-agent
    dialogue strategy and marrying it with the world knowledge, which is obtained
    from an explorer by interacting with the environment or collecting human contributions.
    Specifically, we first deploy the agent to explore the ALFWorld environment and
    collect action-observation sequences $\{\mathcal{AO}_{i}\}$ is integrated into
    the prompt templates of the multi-agent dialogue strategy, where a reasoner depicts
    general plans solving the task and the actor ground the plans as executable actions
    in the environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏对特定环境的先验知识，OPEx在ALFWorld中经常失败，即使在感知和行动模块被切断的情况下也是如此。例如，OPEx可能会因为缺乏知识而持续失败，比如在ALFRED中代理不能直接持有多个物体。因此，一个单一的系统在同时处理规划和落实时，往往难以学习切换规划和落实的最佳时机。为弥补这一差距，我们提出通过使用多代理对话策略将推理和落实问题分开，并将其与从环境中交互或收集人类贡献中获得的世界知识结合起来，从而改进OPEx。具体来说，我们首先部署代理去探索ALFWorld环境，并将动作-观察序列
    $\{\mathcal{AO}_{i}\}$ 集成到多代理对话策略的提示模板中，其中推理者描述解决任务的一般计划，而演员将计划落实为在环境中可执行的动作。
- en: 4 Experiments and Discussion
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与讨论
- en: 4.1 Experiment Setup
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Evaluation Splits
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价分割
- en: The ALFRED benchmark consists of training, valid, and test sets. Both valid
    and test sets are composed of seen and unseen splits, where the unseen splits
    consist of rooms that do not appear in the training set. Following Yao et al.
    ([2022](#bib.bib46)), we evaluate our methods on 134 unseen evaluation games for
    the ALFWorld benchmark.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ALFRED基准包括训练集、验证集和测试集。验证集和测试集都由已见和未见的分割组成，其中未见的分割包括训练集中没有出现的房间。按照Yao等人 ([2022](#bib.bib46))，我们在134个未见的ALFWorld评估游戏上评估我们的方法。
- en: Evaluation Metrics
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价指标
- en: 'Following Shridhar et al. ([2020a](#bib.bib33)); Min et al. ([2021](#bib.bib21)),
    we report four evaluation metrics on AFLRED: (1) Success Rate (SR); (2) Goal Condition
    (GC), the ratio of goal conditions completed at the end of an episode; (3) path
    length weighted SR (PLWSR), the SR weighted by (path length of the expert trajectory)/(path
    length taken by the agent); (4) path length weighted GC (PLWGC), the GC weighted
    by the same factor. Following Yao et al. ([2022](#bib.bib46)), we report SR on
    ALFWorld.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Shridhar 等 ([2020a](#bib.bib33)); Min 等 ([2021](#bib.bib21))，我们报告了 AFLRED
    的四个评估指标：（1）成功率 (SR)；（2）目标条件 (GC)，即在一个回合结束时完成目标条件的比例；（3）路径长度加权成功率 (PLWSR)，即 SR
    加权的（专家轨迹的路径长度）/（代理采取的路径长度）；（4）路径长度加权目标条件 (PLWGC)，即 GC 加权的相同因素。根据 Yao 等 ([2022](#bib.bib46))，我们报告了
    ALFWorld 上的 SR。
- en: '| Method | Test Seen |  | Test Unseen |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 测试见过 |  | 测试未见过 |'
- en: '| PLWGC | GC | PLWSR | SR |  | PLWGC | GC | PLWSR | SR |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| PLWGC | GC | PLWSR | SR |  | PLWGC | GC | PLWSR | SR |'
- en: '| ALFRED (High-level goal instructions only) |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ALFRED（仅限高级目标指令） |  |'
- en: '| LAV Nottingham et al. ([2021](#bib.bib24)) | 13.18 | 23.21 | 6.31 | 13.35
    |  | 10.47 | 17.27 | 3.12 | 6.38 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| LAV Nottingham 等 ([2021](#bib.bib24)) | 13.18 | 23.21 | 6.31 | 13.35 |  |
    10.47 | 17.27 | 3.12 | 6.38 |'
- en: '| HLSM Blukis et al. ([2022](#bib.bib3)) | 11.53 | 35.79 | 6.69 | 25.11 |  |
    8.45 | 27.24 | 4.34 | 16.29 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| HLSM Blukis 等 ([2022](#bib.bib3)) | 11.53 | 35.79 | 6.69 | 25.11 |  | 8.45
    | 27.24 | 4.34 | 16.29 |'
- en: '| LGS-RPA Murray and Cakmak ([2022](#bib.bib22)) | 24.49 | 41.71 | 16.65 |
    33.01 |  | 20.01 | 38.55 | 12.92 | 27.80 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| LGS-RPA Murray 和 Cakmak ([2022](#bib.bib22)) | 24.49 | 41.71 | 16.65 | 33.01
    |  | 20.01 | 38.55 | 12.92 | 27.80 |'
- en: '| EPA Liu et al. ([2022c](#bib.bib19)) | 3.47 | 44.14 | 2.56 | 39.96 |  | 3.91
    | 39.54 | 2.92 | 36.07 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| EPA Liu 等 ([2022c](#bib.bib19)) | 3.47 | 44.14 | 2.56 | 39.96 |  | 3.91 |
    39.54 | 2.92 | 36.07 |'
- en: '| LLM-Planner Song et al. ([2023](#bib.bib37)) | - | 24.57 | - | 15.33 |  |
    - | 22.89 | - | 13.41 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Planner Song 等 ([2023](#bib.bib37)) | - | 24.57 | - | 15.33 |  | - |
    22.89 | - | 13.41 |'
- en: '| FILM Min et al. ([2021](#bib.bib21)) | 14.17 | 36.15 | 10.39 | 25.77 |  |
    13.13 | 34.75 | 9.67 | 24.46 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| FILM Min 等 ([2021](#bib.bib21)) | 14.17 | 36.15 | 10.39 | 25.77 |  | 13.13
    | 34.75 | 9.67 | 24.46 |'
- en: '| OPEx-S | 20.13 | 54.27 | 13.64 | 43.51 |  | 18.46 | 53.82 | 12.57 | 41.27
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| OPEx-S | 20.13 | 54.27 | 13.64 | 43.51 |  | 18.46 | 53.82 | 12.57 | 41.27
    |'
- en: 'Table 1: Main Results on the test splits of ALFRED benchmark.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：ALFRED 基准测试分割的主要结果。
- en: Compared Methods
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 比较方法
- en: The compared methods on ALFRED include LAV Nottingham et al. ([2021](#bib.bib24)),
    where the raw language and visual inputs are transformed into structured forms,
    with a separate “action prediction module” predicting the low-level actions; HLSM Blukis
    et al. ([2022](#bib.bib3)), a hierarchical approach that uses semantic voxel map
    state representation as a long-term memory to solve long-horizon tasks; LGS-RPA Murray
    and Cakmak ([2022](#bib.bib22)), which utilizes a Djikstra-based deterministic
    planner for navigation action generation and introduces landmark-guided search
    along with the reinforced pose adjustment for navigation goal searching and interaction
    action preparation respectively; EPA Liu et al. ([2022c](#bib.bib19)), a neural-symbolic
    approach with symbolic planning; LLM-Planner Song et al. ([2023](#bib.bib37)),
    which simply prompts LLMs for task decomposition; FILM Min et al. ([2021](#bib.bib21)),
    which builds 2D semantic map and performs exploration with a semantic search policy.
    It is worth noting that there are also several works on the leaderboard reporting
    high performance that are not included in the comparison Inoue and Ohashi ([2022](#bib.bib13));
    Shridhar et al. ([2020a](#bib.bib33)); Chen et al. ([2023](#bib.bib5)), this is
    mainly because we focus on systematically outlining and evaluating the essential
    components for mastering EIF tasks, while we cannot find the description or available
    open-source resources of these works when we conduct the experiments. On the ALFWorld
    benchmark, apart from the variants of OPEx, we also introduce ReAct Yao et al.
    ([2022](#bib.bib46)) for comparison to demonstrate the effectiveness of the proposed
    method.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ALFRED 上比较的方法包括 LAV Nottingham 等人 ([2021](#bib.bib24))，其将原始语言和视觉输入转换为结构化形式，并使用独立的“动作预测模块”预测低级动作；HLSM
    Blukis 等人 ([2022](#bib.bib3))，一种层次化的方法，使用语义体素地图状态表示作为长期记忆来解决长时间跨度任务；LGS-RPA Murray
    和 Cakmak ([2022](#bib.bib22))，利用基于 Djikstra 的确定性规划器生成导航动作，并引入了地标引导搜索和强化姿态调整，分别用于导航目标搜索和交互动作准备；EPA
    Liu 等人 ([2022c](#bib.bib19))，一种具有符号规划的神经符号方法；LLM-Planner Song 等人 ([2023](#bib.bib37))，简单地提示
    LLM 进行任务分解；FILM Min 等人 ([2021](#bib.bib21))，构建 2D 语义地图并使用语义搜索策略进行探索。值得注意的是，还有一些在排行榜上报告高性能的工作未包含在比较中，如
    Inoue 和 Ohashi ([2022](#bib.bib13))；Shridhar 等人 ([2020a](#bib.bib33))；Chen 等人
    ([2023](#bib.bib5))，这主要是因为我们专注于系统性地概述和评估掌握 EIF 任务的关键组件，而在进行实验时我们无法找到这些工作的描述或开放资源。在
    ALFWorld 基准测试中，除了 OPEx 的变体外，我们还引入了 ReAct Yao 等人 ([2022](#bib.bib46)) 进行比较，以展示所提出方法的有效性。
- en: 4.2 Experimental Results
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'The main results are illustrated in Table [1](#S4.T1 "Table 1 ‣ Evaluation
    Metrics ‣ 4.1 Experiment Setup ‣ 4 Experiments and Discussion ‣ OPEx: A Component-Wise
    Analysis of LLM-Centric Agents in Embodied Instruction Following"). When contrasting
    OPEx with the baseline FILM, it becomes evident that OPEx exhibits substantial
    improvement across two distinct environmental settings, encompassing both the
    goal condition (GC) and the success rate (SR). Notably, OPEx utilizes in-context
    learning on less than 10% data used for FILMs’ planner (Language Processor) training,
    while OPEx still significantly outperforms FILM. The observation that OPEx achieves
    17.74% and 16.78% absolute gain in SR on test seen and unseen split respectively
    empirically demonstrates the effectiveness of the OPEx framework. However, it
    is also worth noting that the OPEx is inferior to FILM concerning the path length
    weighted metrics. This phenomenon could potentially be attributed to the deliberate
    choice of assigning a higher maximum number of failures to OPEx as compared to
    FILM. This choice typically leads to the average length of the resulting episodes.
    The rationale behind this decision was to encourage OPEx to undertake a more extensive
    exploration, thereby fostering the acquisition of skills in handling a broader
    range of exceptions arising from both uncommon scenarios and failures. On the
    other hand, the FILM utilizes two BERT models trained on the whole training set
    with the template assumption to conduct the task decomposition, while the LLM-based
    planner can achieve this goal with only a bunch of examples. This phenomenon shows
    that OPEx works with a much lower demand for in-domain data, making it more feasible
    in real-world scenarios, where the data collection could be more time-consuming
    and expensive. Furthermore, the FILM outputs low-level navigation and interaction
    actions solely with a deterministic policy, while OPEx introduces an LLM-based
    executor accompanying the deterministic policy to release LLMs’ potential for
    robust language grounding and exception handling in the embodiment environment.
    Overall, the main results empirically demonstrate that it could be feasible to
    develop embodied experts with low demand for in-domain data by mining LLMs’ potential
    for grounded planning and acting.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '主要结果如表[1](#S4.T1 "Table 1 ‣ Evaluation Metrics ‣ 4.1 Experiment Setup ‣ 4 Experiments
    and Discussion ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied
    Instruction Following")所示。将OPEx与基线FILM进行对比时，明显可以看出，OPEx在两个不同环境设置下都表现出显著改进，包括目标条件（GC）和成功率（SR）。值得注意的是，OPEx在用于FILM规划器（语言处理器）训练的数据中仅使用不到10%，而OPEx仍显著优于FILM。OPEx在测试中已见和未见分割上分别取得了17.74%和16.78%的绝对增益，实证证明了OPEx框架的有效性。然而，OPEx在路径长度加权指标方面不如FILM。这个现象可能归因于将比FILM更多的最大失败次数分配给OPEx。这种选择通常导致生成的剧集的平均长度。做出这种决定的理由是鼓励OPEx进行更广泛的探索，从而培养处理来自不常见场景和失败的更广泛异常的技能。另一方面，FILM利用两个训练在整个训练集上的BERT模型进行任务分解，而基于LLM的规划器仅凭少量示例即可实现这一目标。这表明OPEx在实际场景中对领域内数据的需求更低，使其在数据收集可能更加耗时和昂贵的情况下更具可行性。此外，FILM仅使用确定性策略输出低级导航和交互动作，而OPEx引入了一个基于LLM的执行器，配合确定性策略释放LLM在具身环境中对语言基础和异常处理的潜力。总体而言，主要结果实证表明，通过挖掘LLM在有根规划和执行中的潜力，开发对领域内数据需求较低的具身专家是可行的。'
- en: 4.3 Ablation Study and Analysis
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究与分析
- en: To further investigate the bottleneck of the system and the influence of different
    modules, we conduct several additional ablation studies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探究系统瓶颈及不同模块的影响，我们进行了一些额外的消融研究。
- en: Influence of perception models
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 感知模型的影响
- en: 'We first conduct controlled experiments on the valid unseen split of the AFLRED
    dataset to study the influence of perception models. The corresponding results
    are illustrated in the first section of Table [2](#S4.T2 "Table 2 ‣ Influence
    of prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4 Experiments and Discussion
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following"), where OPEx-S denotes the OPEx with stronger perception models (fine-tuned
    ZoeDepth Bhat et al. ([2023](#bib.bib2)) for depth prediction and SOLQ Dong et al.
    ([2021](#bib.bib6)) for instance segmentation), OPEx-P denotes the OPEx with perfect
    ground-truth depth prediction and instance segmentation. The performance gain
    from the improvement of perception models is very significant, indicating there
    is much room for improvement regarding the perception models in ALFRED.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在AFLRED数据集的有效未见分割上进行受控实验，以研究感知模型的影响。相关结果展示在表[2](#S4.T2 "Table 2 ‣ Influence
    of prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4 Experiments and Discussion
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following")的第一部分，其中OPEx-S表示配备了更强感知模型的OPEx（微调后的ZoeDepth Bhat等人 ([2023](#bib.bib2))
    用于深度预测和SOLQ Dong等人 ([2021](#bib.bib6)) 用于实例分割），OPEx-P表示具有完美的真实深度预测和实例分割的OPEx。感知模型改进带来的性能提升非常显著，表明ALFRED中的感知模型还有很大的提升空间。'
- en: Influence of action policies
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作策略的影响
- en: 'As shown in the second section of Table [2](#S4.T2 "Table 2 ‣ Influence of
    prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4 Experiments and Discussion
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following"), we design and conduct another set of controlled experiments to study
    the influence of distinct deterministic action heuristics introduced. It can be
    seen from the table that setting the navigation goal inside the traversable area
    brings the most significant performance improvement, while slice replay brings
    marginal improvement. Besides, introducing the additional semantic map for robust
    landmark-based navigation goal searching brings moderate performance gain.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[2](#S4.T2 "Table 2 ‣ Influence of prior knowledge ‣ 4.3 Ablation Study and
    Analysis ‣ 4 Experiments and Discussion ‣ OPEx: A Component-Wise Analysis of LLM-Centric
    Agents in Embodied Instruction Following")第二部分所示，我们设计并进行了一组受控实验，以研究引入的不同确定性动作启发式的影响。从表中可以看出，将导航目标设置在可遍历区域内带来了最显著的性能提升，而切片重放则带来了边际改进。此外，引入额外的语义地图以增强基于地标的导航目标搜索带来了适度的性能提升。'
- en: Influence of LLM-based modules
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于LLM的模块的影响
- en: 'We first conduct controlled experiments on the validation unseen split of the
    dataset to study the influence of different modules. The corresponding results
    are illustrated in Table [2](#S4.T2 "Table 2 ‣ Influence of prior knowledge ‣
    4.3 Ablation Study and Analysis ‣ 4 Experiments and Discussion ‣ OPEx: A Component-Wise
    Analysis of LLM-Centric Agents in Embodied Instruction Following"). Significant
    performance degradation can be observed when the LLM-based planner is removed
    from the OPEx. This is probably attributed to the fact that the LLM-based executor
    is required to solely perform implicit long-term planning and grounded interaction
    simultaneously under this setting. The LLM-based observer is designed to gather
    information and help the LLM-based executor to focus on task-relevant information
    by summarizing collected information and filtering out the task-irrelevant counterparts.
    However, the ablation study shows that the performance gain brought by the LLM-based
    observer is marginal. This observation can be caused by several possible reasons,
    including (1) GPT-4’s strong long text processing capability mitigates the needs
    of such kind of LLM-based observer; (2) the collected information from ALFRED
    is typically not too large/complex to cause severe distraction or hallucination
    of the LLM-based executor; (3) the observer utilizes zero-shot prompt, better
    prompts may need to be designed.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在数据集的验证未见分割上进行受控实验，以研究不同模块的影响。相关结果如表[2](#S4.T2 "Table 2 ‣ Influence of
    prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4 Experiments and Discussion
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following")所示。当从OPEx中移除基于LLM的规划器时，可以观察到显著的性能下降。这可能归因于在这种设置下，基于LLM的执行器需要同时进行隐式的长期规划和有根据的互动。基于LLM的观察者旨在收集信息，并通过总结收集的信息和筛选掉与任务无关的内容，帮助基于LLM的执行器专注于与任务相关的信息。然而，消融研究表明，基于LLM的观察者带来的性能提升是微不足道的。这一观察可能由几个原因造成，包括（1）GPT-4强大的长文本处理能力减少了对这种LLM观察者的需求；（2）从ALFRED收集的信息通常不会过于庞大/复杂以致于对基于LLM的执行器造成严重干扰或幻觉；（3）观察者利用了零样本提示，可能需要设计更好的提示。'
- en: Influence of prior knowledge
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 先验知识的影响
- en: 'To further investigate the role of decision-making modules in EIF agents, we
    conduct experiments on ALFWorld to eliminate the impact of perception models and
    action policies. The corresponding results are illustrated in the fourth section
    of Tabel [2](#S4.T2 "Table 2 ‣ Influence of prior knowledge ‣ 4.3 Ablation Study
    and Analysis ‣ 4 Experiments and Discussion ‣ OPEx: A Component-Wise Analysis
    of LLM-Centric Agents in Embodied Instruction Following"), where OPEx-L denotes
    the OPEx with prior knowledge learned from the environment and OPEx-H denotes
    the OPEx with prior knowledge provided by humans. With the observation that the
    system performance grows as the quality of the prior knowledge increases, this
    can be empirically explained by the intuition that decomposing EIF tasks via a
    collaborative multi-agent dialogue strategy helps intra-agent specialization and
    inter-agent cooperation. Besides, the intuition that the grounded prior knowledge
    prevents the agents from repetitive errors and facilitates grounded exception
    handling might also contribute to the results. Furthermore, the performance improvement
    of ReAct also empirically demonstrates the effectiveness of the proposed method.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步研究决策模块在EIF代理中的作用，我们在ALFWorld上进行实验，以消除感知模型和行动策略的影响。相关结果如表[2](#S4.T2 "Table
    2 ‣ Influence of prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4 Experiments
    and Discussion ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied
    Instruction Following")的第四部分所示，其中OPEx-L表示从环境中学习的先验知识的OPEx，OPEx-H表示由人类提供的先验知识的OPEx。观察到系统性能随着先验知识质量的提高而增长，这可以通过以下直觉来解释：通过协作的多代理对话策略分解EIF任务有助于代理内部的专业化和代理间的合作。此外，先验知识有助于防止代理重复错误并促进有根据的例外处理的直觉也可能有助于结果。此外，ReAct的性能提升也实证证明了所提出方法的有效性。'
- en: '| Method | Valid Uneen |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 验证未见 |'
- en: '| PLWGC | GC | PLWSR | SR |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| PLWGC | GC | PLWSR | SR |'
- en: '| Influence of perception models |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 感知模型的影响 |'
- en: '| OPEx | 13.48 | 48.61 | 9.08 | 35.91 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| OPEx | 13.48 | 48.61 | 9.08 | 35.91 |'
- en: '| OPEx-S | 16.52 | 51.28 | 11.38 | 40.80 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| OPEx-S | 16.52 | 51.28 | 11.38 | 40.80 |'
- en: '| OPEX-P | 23.72 | 66.17 | 17.43 | 59.43 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| OPEX-P | 23.72 | 66.17 | 17.43 | 59.43 |'
- en: '| Influence of action policies |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 行动策略的影响 |'
- en: '| OPEx | 13.48 | 48.61 | 9.08 | 35.91 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| OPEx | 13.48 | 48.61 | 9.08 | 35.91 |'
- en: '| -semantic map $M^{\prime}_{t}$ | 12.37 | 45.41 | 8.06 | 36.17 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| -语义图 $M^{\prime}_{t}$ | 12.37 | 45.41 | 8.06 | 36.17 |'
- en: '| -slice replay | 12.64 | 45.25 | 8.35 | 37.39 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| -slice replay | 12.64 | 45.25 | 8.35 | 37.39 |'
- en: '| -traversable goal | 11.77 | 43.49 | 7.09 | 34.50 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| -traversable goal | 11.77 | 43.49 | 7.09 | 34.50 |'
- en: '| Influence of LLM-based modules |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 基于 LLM 模块的影响 |'
- en: '| OPEx | 13.48 | 48.61 | 9.08 | 35.91 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| OPEx | 13.48 | 48.61 | 9.08 | 35.91 |'
- en: '| -Planner | 8.10 | 40.16 | 5.72 | 30.57 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| -Planner | 8.10 | 40.16 | 5.72 | 30.57 |'
- en: '| -Observer | 13.41 | 45.62 | 8.58 | 37.76 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| -Observer | 13.41 | 45.62 | 8.58 | 37.76 |'
- en: '| Influence of prior knowledge (On ALFWorld) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 先验知识的影响（在 ALFWorld 上） |'
- en: '| ReAct | - | - | - | 66 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | - | - | - | 66 |'
- en: '| OPEx | - | - | - | 73 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| OPEx | - | - | - | 73 |'
- en: '| OPEx-L | - | - | - | 78 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| OPEx-L | - | - | - | 78 |'
- en: '| OPEx-H | - | - | - | 84 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| OPEx-H | - | - | - | 84 |'
- en: 'Table 2: Ablation Studies of OPEx. OPEx-S denotes the OPEx with stronger perception
    models, OPEx-P denotes the OPEx with perfect ground-truth depth prediction and
    instance segmentation, OPEx-L denotes the OPEx with prior knowledge learned from
    the environment, and OPEx-H denotes the OPEx with prior knowledge provided by
    humans.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: OPEx 的消融研究。OPEx-S 表示具有更强感知模型的 OPEx，OPEx-P 表示具有完美真实深度预测和实例分割的 OPEx，OPEx-L
    表示从环境中学习的先验知识的 OPEx，而 OPEx-H 表示由人工提供的先验知识的 OPEx。'
- en: '| Method | SR | GC | PLWSR | PLWGC |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SR | GC | PLWSR | PLWGC |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPEx | 38.12 | 46.13 | 9.03 | 13.45 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| OPEx | 38.12 | 46.13 | 9.03 | 13.45 |'
- en: '| FILM | 0.00 | 12.18 | 0.00 | 2.78 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| FILM | 0.00 | 12.18 | 0.00 | 2.78 |'
- en: 'Table 3: Performance comparison with the baseline trained on same amount of
    data.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 与基准线性能比较，基准线使用相同数量的数据进行训练。'
- en: Low demand for in-domain data
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对领域内数据的需求低
- en: 'To assess the efficiency of in-domain data usage, we conducted experiments
    comparing OPEx with the baseline FILM. The FILM is trained on identical data used
    for in-context learning of OPEx. The corresponding results are presented in Table [3](#S4.T3
    "Table 3 ‣ Influence of prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4
    Experiments and Discussion ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents
    in Embodied Instruction Following"). Our findings indicate that OPEx markedly
    outperforms FILM across all evaluation metrics in the unseen validation split.
    Empirically, this suggests that OPEx requires significantly less in-domain data
    compared to FILM. This controlled study underscores the potential of addressing
    embodied tasks through an LLM-based framework. This framework achieves low in-domain
    data demand EIF by integrating feedback mechanisms, closed-loop grounded planning,
    and action, harmonized with the reasoning and common sense capabilities of Large
    Language Models (LLMs). Moreover, it also prompts our further exploration into
    the trade-off between in-domain data efficiency and inference overhead, inspiring
    future directions, such as devising agents that adeptly integrate both common
    sense and in-domain knowledge in a data-efficient manner.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '为评估领域内数据使用的效率，我们进行了与基准 FILM 的比较实验。FILM 使用与 OPEx 相同的数据进行训练，以用于上下文学习。相关结果见表 [3](#S4.T3
    "Table 3 ‣ Influence of prior knowledge ‣ 4.3 Ablation Study and Analysis ‣ 4
    Experiments and Discussion ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents
    in Embodied Instruction Following")。我们的研究结果表明，OPEx 在所有评价指标上明显优于 FILM，在未见验证集上尤为突出。经验上，这表明
    OPEx 比 FILM 需要显著更少的领域内数据。这项控制研究突显了通过 LLM 基础框架处理具身任务的潜力。该框架通过集成反馈机制、闭环基础规划和行动，实现了低领域内数据需求
    EIF，并与大型语言模型（LLMs）的推理和常识能力相协调。此外，它还促使我们进一步探讨领域内数据效率与推理开销之间的权衡，激发未来的研究方向，例如设计能够以数据高效的方式巧妙整合常识和领域内知识的智能体。'
- en: Error mode analysis
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 错误模式分析
- en: 'We conduct the error mode analysis of OPEx on the valid unseen split. The corresponding
    statics are shown in Table [4](#S4.T4 "Table 4 ‣ Error mode analysis ‣ 4.3 Ablation
    Study and Analysis ‣ 4 Experiments and Discussion ‣ OPEx: A Component-Wise Analysis
    of LLM-Centric Agents in Embodied Instruction Following"). While our approach
    to calculate the statistics may vary from that of FILM, we have also incorporated
    FILM’s statistics from the original paper Min et al. ([2021](#bib.bib21)) for
    reference. Since we conduct the task decomposition with the LLM-based planner,
    which does not follow the template assumption, we don’t have statistics on language
    processing errors. As shown in the table, the goal object not found error typically
    account for a great ratio of all kinds of error, indicating both FILM and OPEx
    suffer from imperfect perception models. Besides, the interactive exploration
    of the LLM-based executor and the deterministic heuristics probably brings a lower
    error rate of collisions and the error caused by the target object in a closed
    receptacle.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对OPEx在有效未见数据上的错误模式进行了分析。相应的统计数据见表 [4](#S4.T4 "表 4 ‣ 错误模式分析 ‣ 4.3 消融研究与分析 ‣
    4 实验与讨论 ‣ OPEx：LLM中心代理的组件级分析")。虽然我们计算统计数据的方法可能与FILM有所不同，但我们也参考了原始论文 Min et al.
    ([2021](#bib.bib21)) 中的FILM统计数据。由于我们使用基于LLM的规划器进行任务分解，而不遵循模板假设，因此没有语言处理错误的统计数据。如表中所示，目标物体未找到错误通常占所有错误的很大比例，这表明FILM和OPEx都受到不完善的感知模型的影响。此外，基于LLM的执行器和确定性启发式方法的互动探索可能导致较低的碰撞错误率和目标物体在封闭容器中造成的错误。
- en: '| Error mode | FILM | OPEx |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 错误模式 | FILM | OPEx |'
- en: '| --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Goal object not found | 26.07 | 27.36 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 目标物体未找到 | 26.07 | 27.36 |'
- en: '| Interaction failures | 8.54 | 12.80 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 交互失败 | 8.54 | 12.80 |'
- en: '| Collisions | 11.00 | 9.84 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 碰撞 | 11.00 | 9.84 |'
- en: '| Object in closed receptacle | 16.16 | 11.61 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 物体在封闭容器中 | 16.16 | 11.61 |'
- en: '| Language processing error | 24.54 | - |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 语言处理错误 | 24.54 | - |'
- en: '| Others | 13.69 | 38.39 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | 13.69 | 38.39 |'
- en: 'Table 4: Error mode analysis of OPEx on the valid unseen split.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：OPEx在有效未见数据上的错误模式分析。
- en: 5 Related Work
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: LLM-based Agents
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于LLM的代理
- en: Significant progress has been made for LLM-based agents, which mainly focus
    on the following three aspects. LLM-centric Planning utilizes LLMs to generate
    plans in dynamic environments. It can be further categorized into methods planning
    without feedback Huang et al. ([2022a](#bib.bib11)); Fan et al. ([2022](#bib.bib8));
    Yao et al. ([2022](#bib.bib46)); Huang et al. ([2022b](#bib.bib12)); Xiang et al.
    ([2023](#bib.bib45)); Lin et al. ([2023](#bib.bib16)) and approaches planning
    with feedback from environment, human, and model Wang et al. ([2023a](#bib.bib40));
    Zhu et al. ([2023](#bib.bib47)); Shinn et al. ([2023](#bib.bib32)); Wang et al.
    ([2023c](#bib.bib42)); Rana et al. ([2023](#bib.bib27)); Guan et al. ([2023](#bib.bib9));
    Kim et al. ([2023](#bib.bib14)). LLM-oriented Memory stores information from the
    environment and boosts agents’ capabilities of experience accumulation and self-evolving
    to facilitate future actions. Significant-gravitas et al. ([2023](#bib.bib35));
    Shinn et al. ([2023](#bib.bib32)); Wang et al. ([2023a](#bib.bib40)); Majumder
    et al. ([2023](#bib.bib20)); Wang et al. ([2023b](#bib.bib41)) LLM-centric Action
    Policy grounds the plans made by the agent into feasible action space Huang et al.
    ([2022a](#bib.bib11)); Schick et al. ([2023](#bib.bib29)) Notably, our LLM-centric
    agent differs from Voyager Wang et al. ([2023a](#bib.bib40)) and GITM Zhu et al.
    ([2023](#bib.bib47)) by mitigating the instruction grounding problem with dynamically
    adjusted plans from various granularity based on task-centric feedback from the
    environment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: LLM基于的智能体取得了显著进展，主要集中在以下三个方面。LLM中心规划利用LLMs在动态环境中生成计划。它可以进一步分为没有反馈的规划方法 Huang
    et al. ([2022a](#bib.bib11)); Fan et al. ([2022](#bib.bib8)); Yao et al. ([2022](#bib.bib46));
    Huang et al. ([2022b](#bib.bib12)); Xiang et al. ([2023](#bib.bib45)); Lin et al.
    ([2023](#bib.bib16))和有来自环境、人类和模型反馈的规划方法 Wang et al. ([2023a](#bib.bib40)); Zhu
    et al. ([2023](#bib.bib47)); Shinn et al. ([2023](#bib.bib32)); Wang et al. ([2023c](#bib.bib42));
    Rana et al. ([2023](#bib.bib27)); Guan et al. ([2023](#bib.bib9)); Kim et al.
    ([2023](#bib.bib14))。LLM导向的记忆存储来自环境的信息，提升智能体的经验积累和自我进化能力，以便于未来的行动 Significant-gravitas
    et al. ([2023](#bib.bib35)); Shinn et al. ([2023](#bib.bib32)); Wang et al. ([2023a](#bib.bib40));
    Majumder et al. ([2023](#bib.bib20)); Wang et al. ([2023b](#bib.bib41))。LLM中心行动策略将智能体制定的计划落实到可行的行动空间 Huang
    et al. ([2022a](#bib.bib11)); Schick et al. ([2023](#bib.bib29))。值得注意的是，我们的LLM中心智能体通过根据环境任务中心反馈从不同粒度动态调整计划来缓解指令基础问题，这与Voyager Wang
    et al. ([2023a](#bib.bib40))和GITM Zhu et al. ([2023](#bib.bib47))不同。
- en: Instruction Following in Embodied Environment
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 具身环境中的指令跟随
- en: 'Prior work on EIF in embodied environments can be categorized into two classes:
    Supervisely trained end-to-end or modular-based methods that are eager for supervision
    signals from training data and hard to generalize due to the lack of abstraction
    and reasoning abilities Shridhar et al. ([2020a](#bib.bib33)); Suglia et al. ([2021](#bib.bib38));
    Pashevich et al. ([2021](#bib.bib26)); Blukis et al. ([2022](#bib.bib3)); Singh
    et al. ([2020](#bib.bib36)); Liu et al. ([2022a](#bib.bib17)); Min et al. ([2021](#bib.bib21));
    Sharma et al. ([2021](#bib.bib31)), and LLM-based methods that utilizes LLMs’
    reasoning capability Inoue and Ohashi ([2022](#bib.bib13)); Song et al. ([2023](#bib.bib37)).
    Different from Prompter Inoue and Ohashi ([2022](#bib.bib13)) and LLM-Planner Song
    et al. ([2023](#bib.bib37)), which introduce LLMs only for target location finding
    and dynamic task decomposition, our method is an LLM-centric framework and decouples
    reasoning tasks for decision masking problem with multiple LLM-based roles, where
    the LLMs build the plan, adjust the plan, and ground the plan into structured
    action spaces. Besides, our method evolves based on the feedback, providing promising
    future research directions, including human-in-the-loop learning, multi-source
    feedback mixing and refining, etc.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在具身环境中关于EIF的先前工作可以分为两类：一类是依赖于训练数据的监督信号的端到端或模块化方法，这些方法由于缺乏抽象和推理能力而难以推广 Shridhar
    et al. ([2020a](#bib.bib33)); Suglia et al. ([2021](#bib.bib38)); Pashevich et al.
    ([2021](#bib.bib26)); Blukis et al. ([2022](#bib.bib3)); Singh et al. ([2020](#bib.bib36));
    Liu et al. ([2022a](#bib.bib17)); Min et al. ([2021](#bib.bib21)); Sharma et al.
    ([2021](#bib.bib31))，另一类是基于LLM的方法，利用LLMs的推理能力 Inoue and Ohashi ([2022](#bib.bib13));
    Song et al. ([2023](#bib.bib37))。与仅为目标位置查找和动态任务分解引入LLMs的Prompter Inoue and Ohashi
    ([2022](#bib.bib13))和LLM-Planner Song et al. ([2023](#bib.bib37))不同，我们的方法是一个以LLM为核心的框架，通过多个LLM角色拆解决策掩蔽问题，其中LLMs负责制定计划、调整计划以及将计划落实到结构化行动空间中。此外，我们的方法基于反馈不断演变，提供了有前景的未来研究方向，包括人机协同学习、多源反馈混合与精炼等。
- en: 6 Conclusion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We introduce OPEx, an LLM-centric framework tailored for Embodied Instruction
    Following (EIF), and undertake extensive evaluations to dissect the influence
    of its distinct components. Building on this foundation, we further improve OPEx
    by integrating world knowledge with a multi-agent dialogue strategy to further
    harness LLMs’ potential in addressing EIF challenges. Our comprehensive analysis
    reveals that an LLM-centric design significantly enhances EIF performance, pinpointing
    visual perception and low-level action execution as crucial bottlenecks. Additionally,
    our findings demonstrate that integrating a multi-agent dialogue mechanism within
    LLMs markedly boosts their effectiveness, offering promising directions for future
    research in embodied learning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了OPEx，一个以LLM为中心的框架，旨在用于体现在指令跟随（EIF），并进行广泛评估以剖析其独特组件的影响。基于这一基础，我们进一步通过将世界知识与多代理对话策略结合起来来改进OPEx，以进一步发挥LLMs在应对EIF挑战中的潜力。我们的全面分析揭示了以LLM为中心的设计显著提升了EIF表现，指出视觉感知和低级动作执行是关键瓶颈。此外，我们的研究结果表明，将多代理对话机制集成到LLMs中显著提高了它们的有效性，为未来的体现在学习研究提供了有希望的方向。
- en: Limitations
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: While our study introduces the OPEx framework and a dialogue-based mechanism
    for solving EIF tasks, it is not without its limitations. First, the reliance
    on large language models (LLMs) and the complexity of the multi-agent system introduce
    challenges in interpretability and computational efficiency. These models demand
    considerable resources by extensively communicating with ChatGPT, which might
    limit their applicability in resource-constrained environments. Second, our experiments
    are conducted within the confines of the ALFRED and ALFWORLD benchmarks, which,
    while comprehensive, may not encompass all possible real-world scenarios an embodied
    agent might encounter. Third, the integration of visual perception and action
    execution as identified bottlenecks suggests that further refinement in these
    areas is necessary to achieve truly seamless and adaptive embodied AI systems.
    Future work should aim to address these limitations, exploring more efficient
    model architectures, broader applicability across diverse environments, and enhanced
    methods for achieving naturalistic human-agent interaction.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的研究介绍了OPEx框架和基于对话的机制来解决EIF任务，但仍然存在一些限制。首先，依赖大型语言模型（LLMs）和多代理系统的复杂性带来了可解释性和计算效率的挑战。这些模型通过与ChatGPT进行广泛通信，需求大量资源，这可能限制了它们在资源受限环境中的适用性。其次，我们的实验是在ALFRED和ALFWORLD基准范围内进行的，尽管这些基准非常全面，但可能未涵盖体现在代理可能遇到的所有现实世界场景。第三，视觉感知和动作执行的集成为识别的瓶颈，表明在这些领域进一步改进是必要的，以实现真正无缝和自适应的体现在AI系统。未来的工作应致力于解决这些限制，探索更高效的模型架构、在多样化环境中的更广泛适用性，以及实现自然人机交互的改进方法。
- en: Ethical Concerns
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理关注
- en: We do not foresee an immediate ethical or societal impact resulting from our
    work. However, as an LLM application, we acknowledge that OPEx could in some way
    be affected by various types of hallucinations introduced by the LLMs. We therefore
    urge researchers and practitioners to use our proposed framework in a mindful
    way, especially when deploying such LLM-centric agents in real world applications..
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法预见到我们的工作会立即产生伦理或社会影响。然而，作为一个LLM应用程序，我们承认OPEx可能会受到LLMs引入的各种幻觉的影响。因此，我们敦促研究人员和从业者以谨慎的方式使用我们提出的框架，特别是在将此类LLM中心的代理应用于现实世界应用时。
- en: References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Baker et al. (2022) Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga,
    Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. 2022.
    Video pretraining (vpt): Learning to act by watching unlabeled online videos.
    *Advances in Neural Information Processing Systems*, 35:24639–24654.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker等（2022） Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang,
    Adrien Ecoffet, Brandon Houghton, Raul Sampedro, 和Jeff Clune。2022。视频预训练（vpt）：通过观看未标记的在线视频学习行动。*《神经信息处理系统进展》*，35:24639–24654。
- en: 'Bhat et al. (2023) Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,
    and Matthias Müller. 2023. Zoedepth: Zero-shot transfer by combining relative
    and metric depth. *arXiv preprint arXiv:2302.12288*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhat等（2023） Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, 和Matthias
    Müller。2023。Zoedepth：通过结合相对和度量深度进行零-shot迁移。*arXiv预印本 arXiv:2302.12288*。
- en: Blukis et al. (2022) Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and
    Yoav Artzi. 2022. A persistent spatial semantic representation for high-level
    natural language instruction execution. In *Conference on Robot Learning*, pages
    706–717\. PMLR.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blukis et al. (2022) Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, 和
    Yoav Artzi. 2022. 一种持久的空间语义表示用于高级自然语言指令执行。见于 *机器人学习会议*，第706–717页。PMLR。
- en: Chase (2022) Harrison Chase. 2022. [LangChain](https://github.com/hwchase17/langchain).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase (2022) Harrison Chase. 2022. [LangChain](https://github.com/hwchase17/langchain)。
- en: 'Chen et al. (2023) Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao
    Zhang, Dongbin Zhao, and He Wang. 2023. Robogpt: an intelligent agent of making
    embodied long-term decisions for daily instruction tasks. *arXiv preprint arXiv:2311.15649*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2023) Yaran Chen, Wenbo Cui, Yuanwen Chen, Mining Tan, Xinyao
    Zhang, Dongbin Zhao, 和 He Wang. 2023. Robogpt: 一个智能体，用于制定具身长期决策以完成日常指令任务。*arXiv预印本
    arXiv:2311.15649*。'
- en: 'Dong et al. (2021) Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and
    Yichen Wei. 2021. Solq: Segmenting objects by learning queries. *Advances in Neural
    Information Processing Systems*, 34:21898–21909.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong et al. (2021) Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, 和 Yichen
    Wei. 2021. Solq: 通过学习查询进行对象分割。*神经信息处理系统进展*，34:21898–21909。'
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. *arXiv
    preprint arXiv:2303.03378*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, 等. 2023. Palm-e: 一种具身多模态语言模型。*arXiv预印本 arXiv:2303.03378*。'
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    Minedojo: Building open-ended embodied agents with internet-scale knowledge. *arXiv
    preprint arXiv:2206.08853*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, 和 Anima Anandkumar. 2022.
    Minedojo: 构建具有互联网规模知识的开放式具身代理。*arXiv预印本 arXiv:2206.08853*。'
- en: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. Leveraging pre-trained large language models to construct and
    utilize world models for model-based task planning. *arXiv preprint arXiv:2305.14909*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, 和 Subbarao
    Kambhampati. 2023. 利用预训练的大型语言模型构建和使用世界模型进行基于模型的任务规划。*arXiv预印本 arXiv:2305.14909*。
- en: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    2017. Mask r-cnn. In *Proceedings of the IEEE international conference on computer
    vision*, pages 2961–2969.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick.
    2017. Mask r-cnn. 见于 *IEEE国际计算机视觉会议论文集*，第2961–2969页。
- en: 'Huang et al. (2022a) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
    Mordatch. 2022a. Language models as zero-shot planners: Extracting actionable
    knowledge for embodied agents. In *International Conference on Machine Learning*,
    pages 9118–9147\. PMLR.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022a) Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch.
    2022a. 语言模型作为零-shot规划者：提取具身代理的可操作知识。见于 *国际机器学习会议*，第9118–9147页。PMLR。
- en: 'Huang et al. (2022b) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022b. Inner monologue: Embodied reasoning through planning with language models.
    *arXiv preprint arXiv:2207.05608*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022b) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 等.
    2022b. 内在对话：通过语言模型进行具身推理的规划。*arXiv预印本 arXiv:2207.05608*。
- en: 'Inoue and Ohashi (2022) Yuki Inoue and Hiroki Ohashi. 2022. Prompter: Utilizing
    large language model prompting for a data efficient embodied instruction following.
    *arXiv preprint arXiv:2211.03267*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Inoue and Ohashi (2022) Yuki Inoue 和 Hiroki Ohashi. 2022. Prompter: 利用大型语言模型提示进行数据高效的具身指令跟随。*arXiv预印本
    arXiv:2211.03267*。'
- en: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language
    models can solve computer tasks. *arXiv preprint arXiv:2303.17491*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, 和 Stephen McAleer. 2023. 语言模型可以解决计算机任务。*arXiv预印本
    arXiv:2303.17491*。
- en: 'Liang et al. (2022) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, and Andy Zeng. 2022. Code as policies: Language model
    programs for embodied control. *arXiv preprint arXiv:2209.07753*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, 和 Andy Zeng. 2022. 代码作为策略：用于具身控制的语言模型程序。*arXiv预印本
    arXiv:2209.07753*。
- en: 'Lin et al. (2023) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman,
    Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang
    Ren. 2023. Swiftsage: A generative agent with fast and slow thinking for complex
    interactive tasks. *arXiv preprint arXiv:2305.17390*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu
    Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, 和 Xiang Ren. 2023.
    SWIFTSAGE：一个具有快思与慢思功能的生成代理，用于复杂的互动任务。*arXiv preprint arXiv:2305.17390*。
- en: 'Liu et al. (2022a) Haoyu Liu, Yang Liu, Hongkai He, and Hangfang Yang. 2022a.
    Lebp–language expectation & binding policy: A two-stream framework for embodied
    vision-and-language interaction task learning agents. *arXiv preprint arXiv:2203.04637*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022a) Haoyu Liu, Yang Liu, Hongkai He, 和 Hangfang Yang. 2022a.
    Lebp–语言期望与绑定策略：一种用于体感视觉与语言互动任务学习代理的双流框架。*arXiv preprint arXiv:2203.04637*。
- en: 'Liu et al. (2022b) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2022b. [What makes good in-context examples for gpt-3?](https://doi.org/10.18653/v1/2022.deelio-1.10)
    *Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge
    Extraction and Integration for Deep Learning Architectures*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022b) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, 和 Weizhu Chen. 2022b. [什么样的上下文示例对 GPT-3 有效？](https://doi.org/10.18653/v1/2022.deelio-1.10)
    *深度学习深入解析（DeeLIO 2022）：第3届深度学习架构知识提取与集成研讨会论文集*。
- en: Liu et al. (2022c) Xiaotian Liu, Hector Palacios, and Christian Muise. 2022c.
    A planning based neural-symbolic approach for embodied instruction following.
    *Interactions*, 9(8):17.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022c) Xiaotian Liu, Hector Palacios, 和 Christian Muise. 2022c.
    一种基于规划的神经-符号方法用于体感指令跟随。*Interactions*, 9(8):17。
- en: 'Majumder et al. (2023) Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter
    Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter
    Clark. 2023. Clin: A continually learning language agent for rapid task adaptation
    and generalization. *arXiv preprint arXiv:2310.10134*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majumder et al. (2023) Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter
    Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, 和 Peter
    Clark. 2023. Clin：一种不断学习的语言代理，用于快速任务适应和泛化。*arXiv preprint arXiv:2310.10134*。
- en: 'Min et al. (2021) So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan
    Bisk, and Ruslan Salakhutdinov. 2021. Film: Following instructions in language
    with modular methods. *arXiv preprint arXiv:2110.07342*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2021) So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan
    Bisk, 和 Ruslan Salakhutdinov. 2021. FILM：使用模块化方法跟随语言中的指令。*arXiv preprint arXiv:2110.07342*。
- en: Murray and Cakmak (2022) Michael Murray and Maya Cakmak. 2022. Following natural
    language instructions for household tasks with landmark guided search and reinforced
    pose adjustment. *IEEE Robotics and Automation Letters*, 7(3):6870–6877.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murray and Cakmak (2022) Michael Murray 和 Maya Cakmak. 2022. 使用地标引导搜索和增强姿态调整跟随自然语言指令完成家务任务。*IEEE机器人与自动化快报*,
    7(3):6870–6877。
- en: 'Nguyen et al. (2021) Van-Quang Nguyen, Masanori Suganuma, and Takayuki Okatani.
    2021. Look wide and interpret twice: Improving performance on interactive instruction-following
    tasks. *arXiv preprint arXiv:2106.00596*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2021) Van-Quang Nguyen, Masanori Suganuma, 和 Takayuki Okatani.
    2021. 扩展视野并进行双重解释：提高互动指令跟随任务的表现。*arXiv preprint arXiv:2106.00596*。
- en: Nottingham et al. (2021) Kolby Nottingham, Litian Liang, Daeyun Shin, Charless C
    Fowlkes, Roy Fox, and Sameer Singh. 2021. Modular framework for visuomotor language
    grounding. *arXiv preprint arXiv:2109.02161*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nottingham et al. (2021) Kolby Nottingham, Litian Liang, Daeyun Shin, Charless
    C Fowlkes, Roy Fox, 和 Sameer Singh. 2021. 用于视觉运动语言基础的模块化框架。*arXiv preprint arXiv:2109.02161*。
- en: OpenAI (2023) R OpenAI. 2023. Gpt-4 technical report. *arXiv*, pages 2303–08774.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) R OpenAI. 2023. GPT-4技术报告。*arXiv*, 页面2303–08774。
- en: Pashevich et al. (2021) Alexander Pashevich, Cordelia Schmid, and Chen Sun.
    2021. Episodic transformer for vision-and-language navigation. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 15942–15952.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pashevich et al. (2021) Alexander Pashevich, Cordelia Schmid, 和 Chen Sun. 2021.
    用于视觉与语言导航的情节变换器。发表于*IEEE/CVF国际计算机视觉会议论文集*，页面15942–15952。
- en: 'Rana et al. (2023) Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra,
    Ian Reid, and Niko Suenderhauf. 2023. Sayplan: Grounding large language models
    using 3d scene graphs for scalable task planning. *arXiv preprint arXiv:2307.06135*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rana et al. (2023) Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra,
    Ian Reid, 和 Niko Suenderhauf. 2023. Sayplan：利用3D场景图进行大规模任务规划的语言模型基础。*arXiv preprint
    arXiv:2307.06135*。
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-net: Convolutional networks for biomedical image segmentation. In *Medical
    Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International
    Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18*, pages
    234–241\. Springer.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox.
    2015. U-net: 用于生物医学图像分割的卷积网络。发表于 *医学图像计算与计算机辅助手术–MICCAI 2015: 第18届国际会议，德国慕尼黑，2015年10月5-9日，会议论文集，第III部分
    18*，页码 234–241。Springer。'
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, 和 Thomas Scialom. 2023. Toolformer:
    语言模型可以自我学习使用工具。*arXiv 预印本 arXiv:2302.04761*。'
- en: Sethian (1996) James A Sethian. 1996. A fast marching level set method for monotonically
    advancing fronts. *proceedings of the National Academy of Sciences*, 93(4):1591–1595.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sethian (1996) James A Sethian. 1996. 一种用于单调前进前沿的快速行进水平集方法。*美国国家科学院院刊*，93(4):1591–1595。
- en: Sharma et al. (2021) Pratyusha Sharma, Antonio Torralba, and Jacob Andreas.
    2021. Skill induction and planning with latent language. *arXiv preprint arXiv:2110.01517*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma et al. (2021) Pratyusha Sharma, Antonio Torralba, 和 Jacob Andreas. 2021.
    使用潜在语言进行技能诱导和规划。*arXiv 预印本 arXiv:2110.01517*。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](http://arxiv.org/abs/2303.11366).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath,
    Karthik Narasimhan, 和 Shunyu Yao. 2023. [Reflexion: 使用语言代理的语言强化学习](http://arxiv.org/abs/2303.11366)。'
- en: 'Shridhar et al. (2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020a. Alfred:
    A benchmark for interpreting grounded instructions for everyday tasks. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    10740–10749.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar et al. (2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, 和 Dieter Fox. 2020a. Alfred:
    用于解释日常任务的基础指令的基准测试。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，页码 10740–10749。'
- en: 'Shridhar et al. (2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020b. Alfworld: Aligning text and
    embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar et al. (2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2020b. Alfworld: 使文本与实体环境对齐以实现互动学习。*arXiv
    预印本 arXiv:2010.03768*。'
- en: 'Significant-gravitas et al. (2023) Significant-gravitas et al. 2023. Significant-gravitas/auto-gpt:
    An experimental open-source attempt to make gpt-4 fully autonomous. https://github.com/Significant-Gravitas/Auto-GPT.
    Open-Source Software.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Significant-gravitas et al. (2023) Significant-gravitas et al. 2023. Significant-gravitas/auto-gpt:
    一个实验性的开源尝试，使 gpt-4 完全自主。 https://github.com/Significant-Gravitas/Auto-GPT. 开源软件。'
- en: Singh et al. (2020) Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh
    Mottaghi, and Jonghyun Choi. 2020. Factorizing perception and policy for interactive
    instruction following. *arXiv preprint arXiv:2012.03208*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2020) Kunal Pratap Singh, Suvaansh Bhambri, Byeonghwi Kim, Roozbeh
    Mottaghi, 和 Jonghyun Choi. 2020. 将感知和策略因式分解以实现互动式指令跟随。*arXiv 预印本 arXiv:2012.03208*。
- en: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 2998–3009.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, 和 Yu Su. 2023. Llm-planner: 基于少量样本的实体代理规划与大型语言模型。发表于 *IEEE/CVF 国际计算机视觉会议论文集*，页码
    2998–3009。'
- en: 'Suglia et al. (2021) Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind
    Thattai, and Gaurav Sukhatme. 2021. Embodied bert: A transformer model for embodied,
    language-guided visual task completion. *arXiv preprint arXiv:2108.04927*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Suglia et al. (2021) Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind
    Thattai, 和 Gaurav Sukhatme. 2021. Embodied bert: 用于有形语言引导的视觉任务完成的变换器模型。*arXiv
    预印本 arXiv:2108.04927*。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar等。2023。Llama：开放且高效的基础语言模型。*arXiv预印本
    arXiv:2302.13971*。
- en: 'Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended
    embodied agent with large language models. *arXiv preprint arXiv:2305.16291*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023a）Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke
    Zhu、Linxi Fan和Anima Anandkumar。2023a。Voyager：具有大型语言模型的开放式具身代理。*arXiv预印本 arXiv:2305.16291*。
- en: 'Wang et al. (2023b) Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al.
    2023b. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal
    language models. *arXiv preprint arXiv:2311.05997*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023b）Zihao Wang、Shaofei Cai、Anji Liu、Yonggang Jin、Jinbing Hou、Bowei Zhang、Haowei
    Lin、Zhaofeng He、Zilong Zheng、Yaodong Yang等。2023b。Jarvis-1：具有记忆增强的多模态语言模型的开放世界多任务代理。*arXiv预印本
    arXiv:2311.05997*。
- en: 'Wang et al. (2023c) Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao
    Liang. 2023c. Describe, explain, plan and select: Interactive planning with large
    language models enables open-world multi-task agents. *arXiv preprint arXiv:2302.01560*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023c）Zihao Wang、Shaofei Cai、Anji Liu、Xiaojian Ma和Yitao Liang。2023c。描述、解释、计划和选择：与大型语言模型进行互动规划，支持开放世界多任务代理。*arXiv预印本
    arXiv:2302.01560*。
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. 2022a. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022a）Jason Wei、Yi Tay、Rishi Bommasani、Colin Raffel、Barret Zoph、Sebastian
    Borgeaud、Dani Yogatama、Maarten Bosma、Denny Zhou、Donald Metzler等。2022a。大型语言模型的涌现能力。*arXiv预印本
    arXiv:2206.07682*。
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022b）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc
    V Le、Denny Zhou等。2022b。链式思维提示激发大型语言模型中的推理。*神经信息处理系统进展*，35:24824–24837。
- en: 'Xiang et al. (2023) Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang,
    Zichao Yang, and Zhiting Hu. 2023. Language models meet world models: Embodied
    experiences enhance language models. *arXiv preprint arXiv:2305.10626*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang等（2023）Jiannan Xiang、Tianhua Tao、Yi Gu、Tianmin Shu、Zirui Wang、Zichao Yang和Zhiting
    Hu。2023。语言模型遇到世界模型：具身经验增强语言模型。*arXiv预印本 arXiv:2305.10626*。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等（2022）Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik Narasimhan和Yuan
    Cao。2022。React：在语言模型中协同推理和行动。*arXiv预印本 arXiv:2210.03629*。
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. 2023. Ghost in
    the minecraft: Generally capable agents for open-world enviroments via large language
    models with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2023）Xizhou Zhu、Yuntao Chen、Hao Tian、Chenxin Tao、Weijie Su、Chenyu Yang、Gao
    Huang、Bin Li、Lewei Lu、Xiaogang Wang等。2023。Minecraft中的幽灵：通过具有文本知识和记忆的大型语言模型为开放世界环境提供一般能力的代理。*arXiv预印本
    arXiv:2305.17144*。
- en: Appendix A Task Example in ALFRED
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A ALFRED中的任务示例
- en: 'As shown in Figure [2](#A1.F2 "Figure 2 ‣ Appendix A Task Example in ALFRED
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following"), the ALFRED benchmark Shridhar et al. ([2020a](#bib.bib33)) contains
    a set of environments associated with long-horizon household tasks specified by
    natural language instructions. As shown in Figure [2](#A1.F2 "Figure 2 ‣ Appendix
    A Task Example in ALFRED ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents
    in Embodied Instruction Following"), the language instruction $L=\{L_{\text{high}},L_{\text{low}}\}$
    (success) or reaches the maximum number of steps (fail).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](#A1.F2 "图 2 ‣ 附录 A ALFRED 任务示例 ‣ OPEx：LLM-中心代理的组件分析")所示，ALFRED 基准测试 Shridhar
    等（[2020a](#bib.bib33)）包含一组与自然语言指令指定的长期家庭任务相关的环境。如图[2](#A1.F2 "图 2 ‣ 附录 A ALFRED
    任务示例 ‣ OPEx：LLM-中心代理的组件分析")所示，语言指令 $L=\{L_{\text{high}},L_{\text{low}}\}$（成功）或达到最大步骤数（失败）。
- en: '![Refer to caption](img/47738c0e1d73ed05d36fe291af0d8caf.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47738c0e1d73ed05d36fe291af0d8caf.png)'
- en: 'Figure 2: Example of a Clean & Place task in ALFRED.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：ALFRED 中清理与放置任务的示例。
- en: Appendix B Full Results on AFLRED
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B AFLRED 的完整结果
- en: 'The experiment on ALFRED under two different settings are illustrated in Table [5](#A2.T5
    "Table 5 ‣ Appendix B Full Results on AFLRED ‣ OPEx: A Component-Wise Analysis
    of LLM-Centric Agents in Embodied Instruction Following").'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ALFRED 上的两个不同设置的实验结果见表[5](#A2.T5 "表 5 ‣ 附录 B AFLRED 完整结果 ‣ OPEx：LLM-中心代理的组件分析")。
- en: '| Method | Test Seen |  | Test Unseen |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 测试已见 |  | 测试未见 |'
- en: '| PLWGC | GC | PLWSR | SR |  | PLWGC | GC | PLWSR | SR |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| PLWGC | GC | PLWSR | SR |  | PLWGC | GC | PLWSR | SR |'
- en: '| High-level Goal Instruction + Low-level step-by-step instructions |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 高层目标指令 + 低层逐步指令 |  |'
- en: '| Seq2Seq Shridhar et al. ([2020a](#bib.bib33)) | 6.27 | 9.42 | 2.02 | 3.98
    |  | 4.26 | 7.03 | 0.08 | 3.90 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Seq2Seq Shridhar 等（[2020a](#bib.bib33)） | 6.27 | 9.42 | 2.02 | 3.98 |  |
    4.26 | 7.03 | 0.08 | 3.90 |'
- en: '| MOCA Singh et al. ([2020](#bib.bib36)) | 22.05 | 28.29 | 15.10 | 22.05 |  |
    9.99 | 14.28 | 2.72 | 5.30 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| MOCA Singh 等（[2020](#bib.bib36)） | 22.05 | 28.29 | 15.10 | 22.05 |  | 9.99
    | 14.28 | 2.72 | 5.30 |'
- en: '| E.T. Pashevich et al. ([2021](#bib.bib26)) | 34.93 | 45.44 | 27.78 | 38.42
    |  | 11.46 | 18.56 | 4.10 | 8.57 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| E.T. Pashevich 等（[2021](#bib.bib26)） | 34.93 | 45.44 | 27.78 | 38.42 |  |
    11.46 | 18.56 | 4.10 | 8.57 |'
- en: '| LWIT Nguyen et al. ([2021](#bib.bib23)) | 23.10 | 40.53 | 43.10 | 30.92 |  |
    16.34 | 20.91 | 5.60 | 9.42 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| LWIT Nguyen 等（[2021](#bib.bib23)） | 23.10 | 40.53 | 43.10 | 30.92 |  | 16.34
    | 20.91 | 5.60 | 9.42 |'
- en: '| FILM Min et al. ([2021](#bib.bib21)) | 15.06 | 38.51 | 11.23 | 27.67 |  |
    14.30 | 36.37 | 10.55 | 26.49 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| FILM Min 等（[2021](#bib.bib21)） | 15.06 | 38.51 | 11.23 | 27.67 |  | 14.30
    | 36.37 | 10.55 | 26.49 |'
- en: '| OPEx | 22.08 | 54.81 | 14.52 | 44.03 |  | 15.27 | 54.18 | 13.48 | 41.85 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| OPEx | 22.08 | 54.81 | 14.52 | 44.03 |  | 15.27 | 54.18 | 13.48 | 41.85 |'
- en: '| High-level goal instructions only |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 仅高层目标指令 |  |'
- en: '| LAV Nottingham et al. ([2021](#bib.bib24)) | 13.18 | 23.21 | 6.31 | 13.35
    |  | 10.47 | 17.27 | 3.12 | 6.38 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LAV Nottingham 等（[2021](#bib.bib24)） | 13.18 | 23.21 | 6.31 | 13.35 |  |
    10.47 | 17.27 | 3.12 | 6.38 |'
- en: '| HLSM Blukis et al. ([2022](#bib.bib3)) | 11.53 | 35.79 | 6.69 | 25.11 |  |
    8.45 | 27.24 | 4.34 | 16.29 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| HLSM Blukis 等（[2022](#bib.bib3)） | 11.53 | 35.79 | 6.69 | 25.11 |  | 8.45
    | 27.24 | 4.34 | 16.29 |'
- en: '| LGS-RPA Murray and Cakmak ([2022](#bib.bib22)) | 24.49 | 41.71 | 16.65 |
    33.01 |  | 20.01 | 38.55 | 12.92 | 27.80 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| LGS-RPA Murray 和 Cakmak（[2022](#bib.bib22)） | 24.49 | 41.71 | 16.65 | 33.01
    |  | 20.01 | 38.55 | 12.92 | 27.80 |'
- en: '| EPA Liu et al. ([2022c](#bib.bib19)) | 3.47 | 44.14 | 2.56 | 39.96 |  | 3.91
    | 39.54 | 2.92 | 36.07 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| EPA Liu 等（[2022c](#bib.bib19)） | 3.47 | 44.14 | 2.56 | 39.96 |  | 3.91 |
    39.54 | 2.92 | 36.07 |'
- en: '| LLM-Planner Song et al. ([2023](#bib.bib37)) | - | 24.57 | - | 15.33 |  |
    - | 22.89 | - | 13.41 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Planner Song 等（[2023](#bib.bib37)） | - | 24.57 | - | 15.33 |  | - | 22.89
    | - | 13.41 |'
- en: '| FILM Min et al. ([2021](#bib.bib21)) | 14.17 | 36.15 | 10.39 | 25.77 |  |
    13.13 | 34.75 | 9.67 | 24.46 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| FILM Min 等（[2021](#bib.bib21)） | 14.17 | 36.15 | 10.39 | 25.77 |  | 13.13
    | 34.75 | 9.67 | 24.46 |'
- en: '| OPEx-S | 20.13 | 54.27 | 13.64 | 43.51 |  | 18.46 | 53.82 | 12.57 | 41.27
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| OPEx-S | 20.13 | 54.27 | 13.64 | 43.51 |  | 18.46 | 53.82 | 12.57 | 41.27
    |'
- en: 'Table 5: Main Results on the test splits of ALFRED benchmark.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ALFRED 基准测试分割的主要结果。
- en: Appendix C Prompt Examples
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示示例
- en: In this section, we provide three prompt examples for the LLM-based planner,
    LLM-based observer, and LLM-based executor respectively.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分别为基于 LLM 的规划器、观察者和执行器提供了三个提示示例。
- en: LLM-based Planner.
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于 LLM 的规划器。
- en: 'In Figure [3](#A3.F3 "Figure 3 ‣ LLM-based Executor. ‣ Appendix C Prompt Examples
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following"), we present an illustrative prompt example of the LLM-based planner.
    The high-level instruction for this instance is "place a washed bowl into a kitchen
    cabinet." The prompt for the LLM-based planner is constructed to establish the
    planning task and define the desired output format. Specifically, the input provided
    to the planner is: "Task: place a washed bowl into a kitchen cabinet." The resulting
    output encapsulates both the reasoning stages and the path of reasoning undertaken
    by the LLM-based planner. Given that the foundation of the planner’s reasoning
    prowess lies in its comprehension, we initially expect it to demonstrate a fundamental
    understanding of the task. This is manifested through the presentation of the
    task’s Task type (in this instance, "PICK_CLEAN_THEN_PLACE_IN_RECEP"). Subsequently,
    drawing inspiration from the concept of Chain-of-Thought Prompting, we introduce
    a two-step requirement. Firstly, the planner is prompted to generate its Thought
    process in achieving the task, followed by the presentation of the ultimate Plan
    to accomplish the specified task.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [3](#A3.F3 "图 3 ‣ 基于LLM的执行器。 ‣ 附录C 提示示例 ‣ OPEx：对LLM中心代理的分项分析") 中，我们展示了一个基于LLM的规划器的示例提示。该示例的高级指令是“将一个洗净的碗放入厨房橱柜中。”
    为了建立规划任务和定义期望的输出格式，构建了基于LLM的规划器的提示。具体来说，提供给规划器的输入是：“任务：将一个洗净的碗放入厨房橱柜中。” 生成的输出涵盖了LLM-based规划器所采取的推理阶段和推理路径。鉴于规划器推理能力的基础在于其理解能力，我们最初期望它能展示对任务的基本理解。这通过呈现任务的任务类型（在此实例中为"PICK_CLEAN_THEN_PLACE_IN_RECEP"）来体现。随后，借鉴链式思维提示的概念，我们引入了两步要求。首先，提示规划器生成完成任务的思维过程，然后展示实现指定任务的*终极*计划。
- en: LLM-based Observer.
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于LLM的观察者。
- en: 'Fig. [4](#A3.F4 "Figure 4 ‣ LLM-based Executor. ‣ Appendix C Prompt Examples
    ‣ OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction
    Following") demonstrates two prompt examples for the LLM-based observer. Similar
    to the prompt design of the LLM-based planner, the prompt for the LLM-based observer
    also starts with a setup that establishes the observation task. The input to the
    observer is a set of information collected from the environment, including Room
    type: indicating which kind of the room the agent is currently in (kitchen, living
    room, bedroom, or bathroom), Task description: specifying the current subtask
    (which is generated by the LLM-based planner) to complete, Previously found objects:
    storing all the objects detected by the agent from the start of the episode to
    current time step, Objects seen in current observation: pointing out the objects
    detected in the agent’s current egocentric view, Holding object: tracking the
    object that is currently holden by the agent, and Error message: tracking the
    error that causes action failures to facilitate exception handling capability
    of agent. Since successful action in the simulator typically results in the RGB
    change of the egocentric observation, we can detect action failures by comparing
    the egocentric observations before and after the execution of the action. If one
    kind of action failure is detected, then the error message of the corresponding
    action failure will be gathered by the LLM-centric observer. The designing purpose
    of the LLM-based observer is not only to gather information but also to serve
    as a “information gate” which filters out task-irrelevant information and effectively
    organizes the task-relevant information for better grounded planning and acting.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#A3.F4 "图 4 ‣ 基于LLM的执行器。 ‣ 附录C 提示示例 ‣ OPEx: LLM中心代理在具身指令跟随中的组件分析") 展示了两个针对基于LLM的观察者的提示示例。与基于LLM的规划者的提示设计类似，基于LLM的观察者的提示也从建立观察任务的设置开始。观察者的输入是一组从环境中收集的信息，包括房间类型：表示代理当前所在的房间种类（厨房、客厅、卧室或浴室），任务描述：指定当前子任务（由基于LLM的规划者生成）以完成，先前发现的物体：存储从剧集开始到当前时间步检测到的所有物体，当前观察中看到的物体：指出在代理当前自我中心视图中检测到的物体，持有物体：跟踪当前由代理持有的物体，错误信息：跟踪导致行动失败的错误以便于代理的异常处理能力。由于模拟器中的成功行动通常会导致自我中心观察的RGB变化，我们可以通过比较行动执行前后的自我中心观察来检测行动失败。如果检测到一种行动失败，则对应行动失败的错误信息将由LLM中心观察者收集。基于LLM的观察者的设计目的不仅是收集信息，还充当“信息网关”，过滤掉与任务无关的信息，并有效组织与任务相关的信息，以便于更好的规划和行动。'
- en: LLM-based Executor.
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于LLM的执行器。
- en: 'A prompt example of completing “Explore the room to have a general idea of
    the environment” is illustrated in Fig. [5](#A3.F5 "Figure 5 ‣ LLM-based Executor.
    ‣ Appendix C Prompt Examples ‣ OPEx: A Component-Wise Analysis of LLM-Centric
    Agents in Embodied Instruction Following"). Specifically, the prompt of the LLM-based
    executor also starts with a setup establishing the execution task and indicating
    the desired output format. Afterward, the setup is followed by the input to the
    LLM-centric executor, which consists of Observation: presenting the current language
    description of the word state generated by the LLM-based observer, Found objects:
    tracking all the objects detected by the agents, Objects seeing in current observation:
    noting the objects detected from current egocentric visual observation, Previous
    steps: tracking the steps taken for the current subtask, and Current objective:
    specifying the current subtask to complete. Inspired by ReAct, we require the
    LLM-based executor to generate not only the final skill action plan Action but
    also the reasoning paths Thought in the first place.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '完成“探索房间以了解环境的一般情况”的提示示例如图 [5](#A3.F5 "图 5 ‣ 基于LLM的执行器。 ‣ 附录C 提示示例 ‣ OPEx: LLM中心代理在具身指令跟随中的组件分析")
    所示。具体而言，基于LLM的执行器的提示也从建立执行任务和指示所需输出格式的设置开始。随后，设置后跟LLM中心执行器的输入，包括观察：呈现由基于LLM的观察者生成的当前语言描述的词语状态，发现的物体：跟踪代理检测到的所有物体，当前观察中看到的物体：注意从当前自我中心视觉观察中检测到的物体，先前步骤：跟踪当前子任务的步骤，以及当前目标：指定要完成的当前子任务。受到ReAct的启发，我们要求基于LLM的执行器首先生成最终的技能行动计划Action以及推理路径Thought。'
- en: '![Refer to caption](img/dab537f2119d38ead41a51679ca80ea9.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dab537f2119d38ead41a51679ca80ea9.png)'
- en: 'Figure 3: Prompt example of the LLM-based Planner. Setup is fixed for all the
    input test cases, Task is the input to the LLM-based planner that varies for distinct
    input test cases, Task type, Tought, and Plan are the content required to be generated
    by the LLM-based planner. The same color mode applies to other figures.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于 LLM 的规划者的提示示例。所有输入测试用例的设置都是固定的，任务是输入到 LLM 基于规划者的内容，这些内容在不同的输入测试用例中有所不同，任务类型、思考和计划是
    LLM 基于规划者需要生成的内容。相同的颜色模式适用于其他图示。
- en: '![Refer to caption](img/26593a184dddf1a9e03b64dcf6704f9b.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/26593a184dddf1a9e03b64dcf6704f9b.png)'
- en: 'Figure 4: Prompt example of the LLM-based Observer.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于 LLM 的观察者的提示示例。
- en: '![Refer to caption](img/cfa576b71caefb3f5012f42348c72e6b.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/cfa576b71caefb3f5012f42348c72e6b.png)'
- en: 'Figure 5: Prompt example of the LLM-based Executor.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于 LLM 的执行者的提示示例。
