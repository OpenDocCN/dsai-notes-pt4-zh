- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:44:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:44:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GUI-World：一个用于GUI导向的多模态LLM基础代理的数据集
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10819](https://ar5iv.labs.arxiv.org/html/2406.10819)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10819](https://ar5iv.labs.arxiv.org/html/2406.10819)
- en: \doparttoc\faketableofcontentsDongping Chen¹¹¹1Equal contribution.  ^†, Yue
    Huang²¹¹1Equal contribution. , Siyuan Wu¹¹¹1Equal contribution. , Jingyu Tang¹¹¹1Equal
    contribution. , Liuyi Chen¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontentsDongping Chen¹¹¹1Equal contribution.  ^†, Yue
    Huang²¹¹1Equal contribution. , Siyuan Wu¹¹¹1Equal contribution. , Jingyu Tang¹¹¹1Equal
    contribution. , Liuyi Chen¹,
- en: Yilin Bai¹, Zhigang He¹, Chenlong Wang¹, Huichi Zhou¹, Yiqiang Li¹,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yilin Bai¹, Zhigang He¹, Chenlong Wang¹, Huichi Zhou¹, Yiqiang Li¹,
- en: Tianshuo Zhou¹, Yue Yu¹, Chujie Gao¹, Qihui Zhang¹, Yi Gui¹, Zhen Li¹,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Tianshuo Zhou¹, Yue Yu¹, Chujie Gao¹, Qihui Zhang¹, Yi Gui¹, Zhen Li¹,
- en: Yao Wan¹^†, Pan Zhou¹, Jianfeng Gao³, Lichao Sun⁴
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Yao Wan¹^†, Pan Zhou¹, Jianfeng Gao³, Lichao Sun⁴
- en: ¹Huazhong University of Science and Technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹华中科技大学
- en: ²University of Notre Dame       ³Microsoft Research        ⁴Lehigh University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²圣母大学       ³微软研究院        ⁴利哈伊大学
- en: '{dongpingchen0612, yaowan1992}@gmail.com'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{dongpingchen0612, yaowan1992}@gmail.com'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recently, Multimodal Large Language Models (MLLMs) have been used as agents
    to control keyboard and mouse inputs by directly perceiving the Graphical User
    Interface (GUI) and generating corresponding code. However, current agents primarily
    exhibit excellent understanding capabilities in static environments and are predominantly
    applied in relatively simple domains, such as Web or mobile interfaces. We argue
    that a robust GUI agent should be capable of perceiving temporal information on
    the GUI, including dynamic Web content and multi-step tasks. Additionally, it
    should possess a comprehensive understanding of various GUI scenarios, including
    desktop software and multi-window interactions. To this end, this paper introduces
    a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM
    annotations, extensively covering six GUI scenarios and eight types of GUI-oriented
    questions in three formats. We evaluate the capabilities of current state-of-the-art
    MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI
    content, especially dynamic and sequential content. Our findings reveal that ImageLLMs
    struggle with dynamic GUI content without manually annotated keyframes or operation
    history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given
    the sparse GUI video dataset. Based on GUI-World, we take the initial step of
    leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding
    of various GUI tasks. However, due to the limitations in the performance of base
    LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge.
    We believe our work provides valuable insights for future research in dynamic
    GUI content understanding. The code and dataset are publicly available at our
    project homepage: [https://gui-world.github.io/](https://gui-world.github.io/).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，多模态大语言模型（MLLMs）已被用作代理，通过直接感知图形用户界面（GUI）并生成相应的代码来控制键盘和鼠标输入。然而，目前的代理主要在静态环境中表现出色，主要应用于相对简单的领域，如网页或移动界面。我们认为，一个强大的GUI代理应该能够感知GUI上的时间信息，包括动态网页内容和多步骤任务。此外，它还应具备对各种GUI场景的全面理解，包括桌面软件和多窗口交互。为此，本文介绍了一个新的数据集，称为GUI-World，其中包含精心制作的人工-MLLM注释，广泛覆盖了六种GUI场景和三种格式中的八种GUI导向问题。我们评估了当前最先进的MLLMs，包括ImageLLMs和VideoLLMs，在理解各种类型的GUI内容，尤其是动态和顺序内容方面的能力。我们的发现揭示了ImageLLMs在没有手动标注的关键帧或操作历史的情况下难以处理动态GUI内容。另一方面，由于GUI视频数据集稀缺，VideoLLMs在所有GUI导向任务中表现不佳。基于GUI-World，我们迈出了利用微调VideoLLM作为GUI代理的初步步骤，展示了对各种GUI任务的改进理解。然而，由于基础LLMs性能的限制，我们得出结论，使用VideoLLMs作为GUI代理仍然是一个重大挑战。我们相信我们的工作为未来在动态GUI内容理解方面的研究提供了宝贵的见解。代码和数据集可以在我们的项目主页获取：[https://gui-world.github.io/](https://gui-world.github.io/)。
- en: '²²footnotetext: Corresponding authors.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ²²脚注：通讯作者。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Multimodal Large Language Models (MLLMs), such as GPT-4V(ision) [[1](#bib.bib1)]
    and LLaVA [[2](#bib.bib2)], have significantly contributed to the development
    of the visual-text domain [[3](#bib.bib3)]. These models bring forth innovative
    solutions and paradigms for traditional visual tasks, including visual reasoning [[4](#bib.bib4)],
    medical image interpretation [[5](#bib.bib5), [6](#bib.bib6)], and applications
    in embodied agents [[7](#bib.bib7)]. One particularly promising area is Graphical
    User Interface (GUI) understanding, which holds significant potential for real-world
    applications, such as webpage comprehension [[8](#bib.bib8), [9](#bib.bib9)] and
    navigation by GUI agents [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)].
    The key challenges of GUI understanding are twofold: effective GUI agents are
    expected to (1) possess a deep understanding of GUI elements, including webpage
    icons, text identified through Optical Character Recognition (OCR), and page layouts,
    and (2) exhibit an exceptional ability to follow instructions within GUI contexts,
    such as conducting searches through search engines.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大型语言模型（MLLMs），如 GPT-4V(ision) [[1](#bib.bib1)] 和 LLaVA [[2](#bib.bib2)]，在视觉文本领域的发展中发挥了重要作用
    [[3](#bib.bib3)]。这些模型为传统视觉任务带来了创新的解决方案和范式，包括视觉推理 [[4](#bib.bib4)]、医学图像解释 [[5](#bib.bib5),
    [6](#bib.bib6)]，以及在具身智能体中的应用 [[7](#bib.bib7)]。一个特别有前景的领域是图形用户界面（GUI）理解，它具有重大现实应用潜力，如网页理解
    [[8](#bib.bib8), [9](#bib.bib9)] 和 GUI 智能体的导航 [[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]。GUI 理解的关键挑战有两个方面：有效的 GUI 智能体应 (1) 深入理解 GUI 元素，包括网页图标、通过光学字符识别（OCR）识别的文本和页面布局，(2)
    展现出在 GUI 上下文中遵循指令的卓越能力，例如通过搜索引擎进行搜索。
- en: 'Despite significant progress, as illustrated in [Table 1](#S1.T1 "Table 1 ‣
    1 Introduction ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    existing works suffer from the following limitations: (1) Most studies predominantly
    focus on the static features of GUI scenarios, neglecting the need for MLLMs to
    effectively process sequential information and dynamic operations. For instance,
    an agent’s task performance can be disrupted by unexpected elements such as pop-up
    advertisements, underscoring a gap in handling dynamic sequential tasks. (2) Current
    research is typically restricted to Web-based environments, which limits the models’
    generalization and robustness. For instance, GUI agents may need to operate across
    diverse platforms such as Windows, macOS, Linux, iOS, Android, and XR environments.
    Additionally, operations may sometimes involve multiple windows. Therefore, expanding
    the scope of research to encompass these varied environments will enhance the
    adaptability and effectiveness of GUI agents.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管取得了显著进展，但如 [表 1](#S1.T1 "表 1 ‣ 1 介绍 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础智能体的数据集")
    所示，现有工作存在以下局限性：(1) 大多数研究主要关注 GUI 场景的静态特征，忽视了 MLLMs 有效处理序列信息和动态操作的需求。例如，智能体的任务表现可能会因意外元素如弹出广告而受到干扰，这突显了在处理动态序列任务方面的不足。(2)
    当前研究通常限于基于 Web 的环境，这限制了模型的泛化能力和鲁棒性。例如，GUI 智能体可能需要在 Windows、macOS、Linux、iOS、Android
    和 XR 环境等多种平台上操作。此外，操作有时可能涉及多个窗口。因此，将研究范围扩展到这些多样化的环境将提高 GUI 智能体的适应性和有效性。'
- en: '![Refer to caption](img/57f0ac8e5d74a86b9769eae17df55df3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/57f0ac8e5d74a86b9769eae17df55df3.png)'
- en: 'Figure 1: GUI-World: a comprehensive dataset for GUI understanding, holding
    significant potential for real-world applications. All screenshots appeared are
    selected in our dataset.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：GUI-World：一个用于图形用户界面理解的综合数据集，具有重大的现实应用潜力。所有显示的屏幕截图均选自我们的数据集。
- en: '![Refer to caption](img/79e3d9c0db6b4bdadae246c02059f63a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/79e3d9c0db6b4bdadae246c02059f63a.png)'
- en: 'Figure 2: Comparative performance of different MLLMs in six scenarios of GUI-World.
    (a) Performance of four mainstream Image LLMs. (b) Performance of three Video
    LLMs and our GUI-Vid. (c) Performance among six methods. See [subsection 4.2](#S4.SS2
    "4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for more details.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：不同 MLLMs 在 GUI-World 的六种场景中的比较表现。 (a) 四种主流图像 LLMs 的表现。 (b) 三种视频 LLMs 和我们的
    GUI-Vid 的表现。 (c) 六种方法之间的表现比较。详情请参见 [第 4.2 节](#S4.SS2 "4.2 实证结果 ‣ 4 实验与分析 ‣ GUI-World:
    一个面向 GUI 的多模态 LLM 基础智能体的数据集")。'
- en: To mitigate these gaps, this paper introduces GUI-World, a comprehensive dataset
    containing over 12,000 GUI videos, specifically designed to evaluate and enhance
    the capabilities of GUI agents. This dataset encompasses a wide range of GUI scenarios,
    including popular websites, desktop and mobile applications across various operating
    systems, multi-window interactions, as well as XR environments. The data collection
    process involves sourcing GUI videos from screen recordings and instructional
    videos on YouTube. Subsequently, we utilize an Human-MLLM collaborative approach
    to generate a diverse set of questions and instructions and finally construct
    GUI-World.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些差距，本文介绍了 GUI-World，一个包含超过 12,000 个 GUI 视频的综合数据集，专门设计用于评估和增强 GUI 代理的能力。该数据集涵盖了各种
    GUI 场景，包括流行的网站、桌面和移动应用程序、跨各种操作系统、多窗口交互以及 XR 环境。数据收集过程包括从屏幕录制和 YouTube 上的教学视频中获取
    GUI 视频。随后，我们采用 Human-MLLM 协作方法生成多样的问题和指令，最终构建了 GUI-World。
- en: 'Table 1: Comparison of GUI datasets. ‘Sem.’: semantic instruction level, ‘VL’:
    Vision-Language, ‘Seq.’: Tasks for sequential images, ‘Cro.’: Cross-app or multi-window
    tasks, ‘Dyn.’: Tasks for dynamic GUI content.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: GUI 数据集比较。‘Sem.’: 语义指令级别，‘VL’: 视觉-语言，‘Seq.’: 顺序图像任务，‘Cro.’: 跨应用或多窗口任务，‘Dyn.’:
    动态 GUI 内容任务。'
- en: '| Dataset | Size | Sem. | VL | Video | Env Type | Task Coverage | Task |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | 语义 | VL | 视频 | 环境类型 | 任务覆盖 | 任务 |'
- en: '| Web. | Mob. | Desk. | XR | Seq. | Cro. | Dyn. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| Web. | 移动. | 桌面. | XR | 顺序. | 跨应用. | 动态. |'
- en: '| Rico [[13](#bib.bib13)] | 72,219 | Low | ✔ | ✔ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ |
    ✘ | UI Code/Layout Generation |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Rico [[13](#bib.bib13)] | 72,219 | 低 | ✔ | ✔ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ | ✘
    | UI 代码/布局生成 |'
- en: '| MetaGUI [[14](#bib.bib14)] | 1,125 | Low | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘
    | ✘ | Mobile Navigation |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| MetaGUI [[14](#bib.bib14)] | 1,125 | 低 | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ |
    ✘ | 移动导航 |'
- en: '| UGIF [[15](#bib.bib15)] | 523 | High | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ | ✘
    | UI Grounded Instruction Following |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| UGIF [[15](#bib.bib15)] | 523 | 高 | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | UI
    基础指令跟随 |'
- en: '| AITW [[16](#bib.bib16)] | 715,142 | High | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔
    | ✘ | GUI Understanding |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| AITW [[16](#bib.bib16)] | 715,142 | 高 | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✔ | ✔ | ✘
    | GUI 理解 |'
- en: '| Ferret-UI [[17](#bib.bib17)] | 123,702 | Low | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✘
    | ✘ | ✘ | UI Grounding & Understanding |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Ferret-UI [[17](#bib.bib17)] | 123,702 | 低 | ✔ | ✘ | ✘ | ✔ | ✘ | ✘ | ✘ |
    ✘ | ✘ | UI 基础与理解 |'
- en: '| MiniWoB++ [[18](#bib.bib18)] | 100 | Low | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✘ | ✘
    | ✘ | Web Navigation |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| MiniWoB++ [[18](#bib.bib18)] | 100 | 低 | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✘ | ✘ |
    ✘ | 网络导航 |'
- en: '| WebArena [[19](#bib.bib19)] | 812 | Low | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘ |
    ✘ | Web Navigation |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| WebArena [[19](#bib.bib19)] | 812 | 低 | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘ | ✘
    | 网络导航 |'
- en: '| Mind2Web [[20](#bib.bib20)] | 2,350 | Both | ✔ | ✔ | ✔ | ✘ | ✘ | ✘ | ✔ |
    ✘ | ✘ | Web Navigation |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Mind2Web [[20](#bib.bib20)] | 2,350 | 两者 | ✔ | ✔ | ✔ | ✘ | ✘ | ✘ | ✔ | ✘
    | ✘ | 网络导航 |'
- en: '| OmniAct [[21](#bib.bib21)] | 9,802 | Low | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✘
    | ✘ | Code Generation |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| OmniAct [[21](#bib.bib21)] | 9,802 | 低 | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ |
    ✘ | 代码生成 |'
- en: '| MMINA [[22](#bib.bib22)] | 1,050 | Low | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✔ |
    ✘ | Web Navigation |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| MMINA [[22](#bib.bib22)] | 1,050 | 低 | ✔ | ✘ | ✔ | ✘ | ✘ | ✘ | ✔ | ✔ | ✘
    | 网络导航 |'
- en: '| AgentStudio [[23](#bib.bib23)] | 304 | High | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ |
    ✔ | ✘ | General Control |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| AgentStudio [[23](#bib.bib23)] | 304 | 高 | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔
    | ✘ | 通用控制 |'
- en: '| OSWorld [[24](#bib.bib24)] | 369 | High | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔ |
    ✘ | General Control |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| OSWorld [[24](#bib.bib24)] | 369 | 高 | ✔ | ✘ | ✔ | ✘ | ✔ | ✘ | ✔ | ✔ | ✘
    | 通用控制 |'
- en: '| GUI-World (Ours) | 12,379 | Both | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | GUI
    Understanding |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| GUI-World (我们) | 12,379 | 两者 | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | ✔ | GUI 理解
    |'
- en: '| Instruction Following |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 指令跟随 |'
- en: 'Likewise, we also establish a comprehensive benchmark for GUI understanding,
    which encompasses seven mainstream MLLMs, three keyframe selection strategies,
    six GUI scenarios, and a diverse array of queries in multiple-choice, free-form,
    and conversational formats, aiming to provide a thorough evaluation of the MLLMs’
    GUI-oriented capabilities. As shown in [Figure 2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"), the assessment
    results indicate that most MLLMs struggle with GUI-World, highlighting their limited
    dynamic understanding of graphical interfaces and underscoring the need for further
    enhancement.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '同样，我们还建立了一个全面的图形用户界面（GUI）理解基准，其中涵盖了七种主流的MLLMs、三种关键帧选择策略、六种GUI场景，以及多种选择题、自由形式和对话格式的多样查询，旨在提供对MLLMs在GUI方面能力的彻底评估。如[图2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents")所示，评估结果表明大多数MLLMs在GUI-World上表现不佳，突出显示了它们对图形界面的动态理解能力有限，强调了进一步改进的必要性。'
- en: Leveraging this dataset, we take the first step of fine-tuning a Video GUI Agent
    proficient in dynamic and sequential GUI tasks, which results in significant improvements
    in the general capabilities of GUI agents, thereby demonstrating the utility and
    effectiveness of GUI-World. Additionally, we delve into discussing various factors
    critical to GUI understanding, including the integration of textual information,
    the number of keyframes, and image resolutions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个数据集，我们迈出了微调一个擅长动态和顺序GUI任务的视频GUI代理的第一步，这显著提升了GUI代理的一般能力，从而展示了GUI-World的实用性和有效性。此外，我们还深入讨论了对GUI理解至关重要的各种因素，包括文本信息的整合、关键帧的数量和图像分辨率。
- en: 'Overall, the key contributions of this paper are three-fold:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本文的主要贡献有三点：
- en: $\triangleright$ A New Dataset. We propose GUI-World, a comprehensive GUI dataset
    comprising over 12,000 videos specifically designed to assess and improve the
    GUI understanding capabilities of MLLMs, spanning a range of categories and scenarios,
    including desktop, mobile, and extended reality (XR), and representing the first
    GUI-oriented instruction-tuning dataset in the video domain.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 一个新数据集。我们提出了GUI-World，这是一个全面的GUI数据集，包含超过12,000个视频，专门用于评估和提高MLLMs的GUI理解能力，涵盖了桌面、移动和扩展现实（XR）等多个类别和场景，代表了视频领域第一个GUI导向的指令调整数据集。
- en: $\triangleright$ A Novel Model. Based on GUI-World, we propose GUI-Vid, a GUI-oriented
    VideoLLM with enhanced capabilities to handle various and complex GUI tasks. GUI-Vid
    shows a significant improvement on the benchmark and achieves results comparable
    to the top-performing models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 一个新模型。基于GUI-World，我们提出了GUI-Vid，这是一个GUI导向的视频LLM，具有增强的处理各种复杂GUI任务的能力。GUI-Vid在基准测试中表现显著提升，并且取得了与顶级模型相当的结果。
- en: $\triangleright$ Comprehensive Experiments and Valuable Insights. Our experiments
    indicate that most existing MLLMs continue to face challenges with GUI-oriented
    tasks, particularly in sequential and dynamic GUI content. Empirical findings
    suggest that improvements in vision perception, along with an increase in the
    number of keyframes and higher resolution, can boost performance in GUI-oriented
    tasks, thereby paving the way for the future of GUI agents.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 综合实验和有价值的见解。我们的实验表明，大多数现有的MLLMs在GUI导向任务中仍面临挑战，特别是在顺序和动态GUI内容方面。实证发现表明，视觉感知的改进，以及关键帧数量的增加和更高的分辨率，可以提升在GUI导向任务中的表现，从而为未来的GUI代理铺平道路。
- en: '2 GUI-World: A Comprehensive Dataset for GUI Understanding'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2 GUI-World: 一个全面的GUI理解数据集'
- en: 2.1 Overview
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 概述
- en: 'We introduce GUI-World, a comprehensive dataset covering six GUI scenarios
    including video, human-annotated keyframes, as well as detailed captions and diverse
    types of QA produced by our data curation framework, aiming at benchmarking and
    enhancing the general GUI-oriented capabilities. These GUI scenarios encompass
    desktop operating systems (*e.g.*, macOS, Windows) and mobile platforms (*e.g.*,
    Android and iOS), websites, software, and even extended-range technologies (XR)
    (*e.g.*, GUI in Apple Vision Pro [[25](#bib.bib25)]). Discussion for each scenario
    is in [subsection A.1](#A1.SS1 "A.1 Six Main GUI Categories ‣ Appendix A Details
    of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们介绍了GUI-World，这是一个全面的数据集，涵盖了包括视频、人为注释的关键帧在内的六种GUI场景，以及我们的数据整理框架生成的详细说明和各种类型的问答，旨在基准测试和提升通用GUI导向能力。这些GUI场景包括桌面操作系统（*例如*，macOS，Windows）和移动平台（*例如*，Android和iOS），网站，软件，甚至扩展现实技术（XR）（*例如*，Apple
    Vision Pro中的GUI [[25](#bib.bib25)]）。每个场景的讨论见[子节A.1](#A1.SS1 "A.1 Six Main GUI
    Categories ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")。'
- en: '![Refer to caption](img/b9c567ee7c7b74e402c1f903190b4b0c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b9c567ee7c7b74e402c1f903190b4b0c.png)'
- en: 'Figure 3: An overview construction pipeline of GUI-World.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：GUI-World的概览构建流程。
- en: 'As illustrated in [Figure 3](#S2.F3 "Figure 3 ‣ 2.1 Overview ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"), the development of GUI-World is structured around
    a two-stage process. Details regarding video and query statistics are provided
    in [Table 2](#S2.T2 "Table 2 ‣ 2.2 GUI Video Collection and Keyframe Annotation
    Process ‣ 2 GUI-World: A Comprehensive Dataset for GUI Understanding ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), which includes distributions
    of the number of keyframes, video lengths, and the lengths of queries and their
    corresponding golden answers, as displayed in [Figure 4](#S2.F4 "Figure 4 ‣ 2.2
    GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World: A Comprehensive
    Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents"). Refer to [Figure 5](#S2.F5 "Figure 5 ‣ 2.2 GUI Video Collection
    and Keyframe Annotation Process ‣ 2 GUI-World: A Comprehensive Dataset for GUI
    Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")
    and [Appendix F](#A6 "Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents") for case study.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图3](#S2.F3 "Figure 3 ‣ 2.1 Overview ‣ 2 GUI-World: A Comprehensive Dataset
    for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")所示，GUI-World的开发分为两个阶段。关于视频和查询统计的详细信息见[表2](#S2.T2 "Table 2 ‣ 2.2 GUI Video
    Collection and Keyframe Annotation Process ‣ 2 GUI-World: A Comprehensive Dataset
    for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")，其中包括关键帧数量、视频长度、查询长度及其对应的标准答案的分布，如[图4](#S2.F4 "Figure 4 ‣ 2.2 GUI Video
    Collection and Keyframe Annotation Process ‣ 2 GUI-World: A Comprehensive Dataset
    for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents")所示。案例研究请参见[图5](#S2.F5 "Figure 5 ‣ 2.2 GUI Video Collection and Keyframe
    Annotation Process ‣ 2 GUI-World: A Comprehensive Dataset for GUI Understanding
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")和[附录F](#A6
    "Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")。'
- en: 2.2 GUI Video Collection and Keyframe Annotation Process
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2 GUI视频收集和关键帧注释过程
- en: We describe the pipeline for collecting screen recordings from student workers
    and GUI-related instructional videos from YouTube for GUI-World and the procedures
    followed to convert these videos into keyframe sequences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了从学生工那里收集屏幕录制和从YouTube获取GUI相关教学视频的流程，以及将这些视频转换为关键帧序列的程序。
- en: 'Table 2: The statistics of GUI-World. For Android, we select videos from Rico
    [[13](#bib.bib13)] and randomly sample 10 frames. Avg. Frame refers to the average
    number of frames in each keyframe, and Avg. Anno. refers to the average number
    of manually annotated user actions in each keyframe.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：GUI-World的统计数据。对于Android，我们从Rico [[13](#bib.bib13)]中选择视频并随机抽取10帧。Avg. Frame指的是每个关键帧中的平均帧数，Avg.
    Anno.指的是每个关键帧中手动注释的用户行为的平均数量。
- en: '| Category | Total Videos | Free-form | MCQA | Conversation | Total Frame.
    (Avg.) | Avg. Anno. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 总视频数 | 自由形式 | MCQA | 对话 | 总帧数（平均） | 平均注释 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Software | 4,720 | 27,840 | 9,440 | 9,440 | 23,520 (4.983) | 7.558 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 软件 | 4,720 | 27,840 | 9,440 | 9,440 | 23,520 (4.983) | 7.558 |'
- en: '| Website | 2,499 | 14,994 | 4,998 | 4,998 | 15,371 (6.151) | 6.862 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 网站 | 2,499 | 14,994 | 4,998 | 4,998 | 15,371 (6.151) | 6.862 |'
- en: '| IOS | 492 | 2,952 | 984 | 984 | 2,194 (4.459) | 7.067 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| IOS | 492 | 2,952 | 984 | 984 | 2,194 (4.459) | 7.067 |'
- en: '| Multi | 475 | 2,850 | 950 | 950 | 2,507 (5.277) | 7.197 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 多平台 | 475 | 2,850 | 950 | 950 | 2,507 (5.277) | 7.197 |'
- en: '| XR | 393 | 2,358 | 786 | 786 | 1,584 (4.030) | 10.970 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| XR | 393 | 2,358 | 786 | 786 | 1,584 (4.030) | 10.970 |'
- en: '| Android | 3,800 | 15,199 | 7,600 | 7,600 | 38,000 (10.000) | - |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Android | 3,800 | 15,199 | 7,600 | 7,600 | 38,000 (10.000) | - |'
- en: '| Summary | 12,379 | 76,673 | 24,758 | 24,758 | 83,176 (6.719) | 7.463 | ![Refer
    to caption](img/e3e7d3c710e13a50c740a8d944206ec2.png)![Refer to caption](img/43dc27e66eb6664a13a9d4fc7268fa73.png)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '| 摘要 | 12,379 | 76,673 | 24,758 | 24,758 | 83,176 (6.719) | 7.463 | ![参考说明](img/e3e7d3c710e13a50c740a8d944206ec2.png)![参考说明](img/43dc27e66eb6664a13a9d4fc7268fa73.png)'
- en: 'Figure 4: Left: Distribution of the number of keyframes and video lengths.
    Right: Length distribution for each type of question and its golden answer.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：左图：关键帧数量和视频长度的分布。右图：每种问题类型及其黄金答案的长度分布。
- en: A significant portion of our video data is derived from screen recordings executed
    by student workers, which can directly reflect real-life GUI usage scenarios.
    A typical video collection scenario involves assigning a student worker a specific
    software task. The student begins by familiarizing themselves with the software,
    followed by recording a series of operations in a short video clip, such as “Sign
    up”, “Sign in”, “Create a New Page”, and “Invite Other Collaborators” in the software
    “Notion¹¹1[https://www.notion.so/](https://www.notion.so/)”.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的视频数据中有很大一部分来自学生工作人员执行的屏幕录制，这可以直接反映实际的GUI使用场景。一个典型的视频收集场景包括给学生工作人员分配一个特定的软件任务。学生首先熟悉软件，然后录制一系列操作的短视频片段，例如“注册”、“登录”、“创建新页面”和“邀请其他协作者”在软件“Notion¹¹1[https://www.notion.so/](https://www.notion.so/)”中。
- en: 'Despite the high fidelity of these manually recorded videos, we encounter several
    challenges: (1) Student workers often require substantial time to acquaint themselves
    with professional software (*e.g.*, MATLAB, Adobe After Effects (Ae)), which can
    hinder the progress of data collection. (2) The videos may lack comprehensiveness,
    typically capturing only commonly used operations and overlooking rarer functions
    crucial for dataset completeness. To address these issues, we also source videos
    from social media platforms that host a diverse array of GUI-related content.
    Specifically, we download tutorial videos from YouTube—given its prevalence as
    a video-sharing platform—because they richly detail various GUI operations. These
    videos are then segmented into shorter clips, each representing a distinct sequence
    of operations.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些手动录制的视频具有很高的保真度，但我们面临一些挑战：（1）学生工作人员通常需要大量时间来熟悉专业软件（*例如*，MATLAB、Adobe After
    Effects（Ae）），这可能会阻碍数据收集的进展。（2）这些视频可能缺乏全面性，通常仅捕捉常用操作，而忽视数据集完整性所需的较少见的功能。为了解决这些问题，我们还从社交媒体平台获取视频，这些平台上有各种各样的GUI相关内容。具体来说，我们从YouTube下载教程视频——由于其作为视频分享平台的普及——因为它们详细描述了各种GUI操作。这些视频随后被分割成较短的片段，每个片段代表一系列不同的操作。
- en: 'The subsequent step involves annotating these video clips with keyframes and
    textual descriptions of each keyframe using custom-designed annotation software.
    Although several algorithms exist for keyframe extraction [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)], they typically underperform with GUI videos
    where changes between frames might be minimal (*e.g.*, a slight movement in the
    mouse cursor). To ensure high-quality datasets, we therefore perform manual extraction
    of these keyframes. Each keyframe is meticulously annotated to include details
    such as the operation performed, the purpose between two keyframes, the software
    or website used, mouse actions (*e.g.*, scroll, click), and keyboard inputs (*e.g.*,
    copy (Ctrl + C), paste (Ctrl + V), specific input). We detail our annotation process
    in [subsection A.3](#A1.SS3 "A.3 Human Keyframes Annotation Process ‣ Appendix
    A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '随后的步骤涉及使用自定义设计的注释软件对这些视频片段进行关键帧标注以及每个关键帧的文本描述。尽管存在几种关键帧提取算法 [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)]，但它们在图形用户界面视频中通常表现不佳，因为帧之间的变化可能很小（*例如*，鼠标光标的轻微移动）。为了确保数据集的高质量，我们因此手动提取这些关键帧。每个关键帧都经过细致标注，包括执行的操作、两个关键帧之间的目的、使用的软件或网站、鼠标操作（*例如*，滚动、点击）以及键盘输入（*例如*，复制（Ctrl
    + C）、粘贴（Ctrl + V）、特定输入）。我们在 [小节 A.3](#A1.SS3 "A.3 人工关键帧注释过程 ‣ 附录 A 数据集构建细节 ‣ 第一部分附录
    ‣ GUI-World: 面向图形用户界面的多模态 LLM 代理数据集") 中详细说明了我们的注释过程。'
- en: '![Refer to caption](img/1febd252655fd84fb31414557a1ff8ba.png)Static
    Which web browser is used and which website is prominently featured
    before search for ’office’?Sequential
    After moving the Steam window to the center, what did the user
    do next in the Edge browser?Prediction
    What would be the likely next action the user performs after searching
    for ’office’ on Bing?Conversation
    • User 1: Can you minimize the OBS for a better view of the browser?
    • Assistant 1: Certainly, the OBS application has been minimized, providing a
    clear view of the Edge browser. • User 2: Great, now can you search for Microsoft
    Office in the Edge browser? • Assistant 2: Of course, a new tab has been opened
    in the Edge browser $\cdot\cdot\cdot$ The Bing search results for ’office’ are
    now displayed.Reasoning
    If the user needs to record gameplay footage next, which application
    should they interact with and what would be their first step? • A. They should
    open the Steam application and click on the ’STORE’ tab. • B. They should open
    the Edge browser and search for ’game recording software’. • C. They should reopen
    the OBS application and click on the ’Start Recording’ button. • D. They should
    access the Windows Start menu and search for the ’Camera’ app.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考标题](img/1febd252655fd84fb31414557a1ff8ba.png)静态
    在搜索“office”之前，使用了哪个网页浏览器，并且哪个网站在搜索结果中显著突出显示？顺序
    在将 Steam 窗口移至中心后，用户在 Edge 浏览器中接下来做了什么？预测
    用户在 Bing 上搜索“office”之后，可能的下一步动作是什么？对话
    • 用户 1：你能最小化 OBS 以更好地查看浏览器吗？ • 助手 1：当然，OBS 应用程序已经被最小化，清晰地展示了 Edge
    浏览器。 • 用户 2：很好，现在你能在 Edge 浏览器中搜索 Microsoft Office 吗？ • 助手 2：当然，已经在 Edge 浏览器中打开了一个新标签页
    $\cdot\cdot\cdot$ 现在显示了 Bing 上的“office”搜索结果。推理
    如果用户接下来需要录制游戏画面，他们应该与哪个应用程序交互，第一步是什么？ • A. 他们应该打开 Steam 应用程序并点击“STORE”标签。
    • B. 他们应该打开 Edge 浏览器并搜索“游戏录制软件”。 • C. 他们应该重新打开 OBS 应用程序并点击“开始录制”按钮。 • D. 他们应该访问
    Windows 开始菜单并搜索“相机”应用。'
- en: 'Figure 5: An example in multi-window GUI scene as a case study.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 作为案例研究的多窗口 GUI 场景示例。'
- en: 2.3 GUI Tasks Generation from Human-MLLM Collaboration
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3 人类-MLLM 协作下的 GUI 任务生成
- en: 'Drawing insights from prior research [[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)], we develop a Human-MLLM collaboration pipeline
    to annotate captions and diverse types of QA specifically tailored for GUI comprehension.
    The process involves inputting an instructional prompt, a comprehensive description,
    key information (*e.g.*, system or application), and a sequence of human-annotated
    keyframes into GPT-4V. As depicted in [Table 3](#S2.T3 "Table 3 ‣ 2.3 GUI Tasks
    Generation from Human-MLLM Collaboration ‣ 2 GUI-World: A Comprehensive Dataset
    for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), GUI-World features an array of question types, as detailed in follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '从之前的研究[[30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34)]中汲取灵感，我们开发了一个人类-MLLM 协作管道来标注标题和各种类型的 QA，专门针对 GUI 理解。该过程包括将说明提示、全面描述、关键信息
    (*例如* 系统或应用程序) 和一系列人类标注的关键帧输入 GPT-4V。如 [表 3](#S2.T3 "表 3 ‣ 2.3 人类-MLLM 协作下的 GUI
    任务生成 ‣ 2 GUI-World: GUI 理解的综合数据集 ‣ GUI-World: 针对 GUI 的多模态 LLM 基础代理的数据集") 所示，GUI-World
    提供了各种问题类型，详细信息如下：'
- en: '$\triangleright$ Detailed and Summarized Captioning: This task challenges basic
    GUI knowledge and multimodal perception, also addressing the deficiency of detailed
    GUI content in video-caption pairs. Initially, GPT-4V generates two distinct descriptions
    for each video: one concentrating on fine-grained details and the other on the
    overall image sequences. Furthermore, GPT-4V provides a succinct summary, highlighting
    core operations and overarching objectives in the video.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '$\triangleright$ 详细和总结性的标题: 这个任务挑战基本的 GUI 知识和多模态感知，同时解决视频-标题对中详细 GUI 内容的缺失。最初，GPT-4V
    为每个视频生成两个不同的描述：一个集中于细节，另一个则侧重于整体图像序列。此外，GPT-4V 提供简明的总结，突出视频中的核心操作和总体目标。'
- en: '$\triangleright$ Static GUI Content: This task challenges MLLM with textual,
    layout, and iconographic analysis of static GUI content. We instruct GPT-4V to
    generate free-form queries with a golden answer concerning static GUI elements
    or specific scenes that recur in more than two keyframes, ensuring their consistent
    presence in the video. Additionally, GPT-4V also crafts QA pairs that evaluate
    inferential skills in static content, focusing on interrelations among icons or
    textual information.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '$\triangleright$ 静态 GUI 内容: 这个任务要求 MLLM 对静态 GUI 内容进行文本、布局和图标分析。我们指示 GPT-4V
    生成与静态 GUI 元素或在两个以上关键帧中重复出现的特定场景相关的自由格式查询，确保它们在视频中的一致性。此外，GPT-4V 还创建 QA 对，评估静态内容中的推理能力，关注图标或文本信息之间的关系。'
- en: '$\triangleright$ Dynamic and Sequential GUI Content: This task concentrates
    on temporal content in GUI video, such as dynamically changing interfaces, and
    aims to elucidate the sequential information and reasoning chains within GUI content.
    We direct GPT-4V to identify consistently changing elements to create queries
    for dynamic content. Moreover, predictive tasks are formulated on order and temporal
    relation in provided sequential images, challenging agents to anticipate future
    events or states.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '$\triangleright$ 动态和顺序 GUI 内容: 这个任务集中于 GUI 视频中的时间内容，例如动态变化的界面，并旨在阐明 GUI 内容中的顺序信息和推理链。我们指示
    GPT-4V 识别持续变化的元素，以创建针对动态内容的查询。此外，基于所提供的顺序图像中的顺序和时间关系制定预测任务，挑战代理预测未来事件或状态。'
- en: 'Table 3: Examples of diverse question types in GUI-World.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: GUI-World 中多种问题类型的示例。'
- en: '| T. | Question | Examples |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| T. | 问题 | 示例 |'
- en: '| --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Caption | Detailed | Q: Please provide a detailed description of what occurs
    throughout these sequential GUI images. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 详细 | Q: 请提供对这些连续 GUI 图像中发生的事情的详细描述。 |'
- en: '| Description | A: The video shows a user taking the 16 Personalities test
    on a Windows desktop using the Edge browser… |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | A: 该视频展示了一名用户在 Windows 桌面上使用 Edge 浏览器进行 16 人格测试的过程… |'
- en: '| Summarized | Q: Write a clear description of the video, make sure the key
    features are well covered. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | Q: 请写一个清晰的视频描述，确保涵盖关键特性。 |'
- en: '| Caption | A: Creating a new IT team in Todoist by selecting industry, job
    function, role, team size, and inviting members. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | A: 在 Todoist 中通过选择行业、工作职能、角色、团队规模，并邀请成员来创建一个新的 IT 团队。 |'
- en: '| Static | Layout, | Q: What related searches are suggested on the right side
    of the Bing results for ’emnlp 2024’? |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 静态 | 布局 | 问：在 Bing 搜索结果右侧，‘emnlp 2024’的相关搜索建议有哪些？ |'
- en: '| Icon Retrieval | A: The suggested related searches shown include ’emnlp 2024
    miami’, ’eacl 2024 call for papers’… |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 图标检索 | 答：建议的相关搜索包括‘emnlp 2024 miami’，‘eacl 2024 征稿’…'
- en: '| Textual | Q: What is the estimated time to complete the content for Week
    2 of the course? |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 问：完成课程第二周内容的估计时间是多少？ |'
- en: '| Retrieval | A: The estimated time to complete the content for Week 2 of the
    course is 1 hour… |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | 答：完成课程第二周内容的估计时间是 1 小时… |'
- en: '| Interrelations | Q: What is the name of the browser and the tab where the
    user performs the product search? |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 相互关系 | 问：用户在哪个浏览器和标签页中进行产品搜索？ |'
- en: '| in GUI Content | A: The browser is Microsoft Edge, and the user performs
    the product search in the eBay tab. |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GUI 内容中 | 答：浏览器是 Microsoft Edge，用户在 eBay 标签页中进行产品搜索。 |'
- en: '| Dynamic | Content | Q: What specific action does the user take after turning
    their head to the left to view the left side of the page? |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 动态 | 内容 | 问：用户在将头转向左侧以查看页面左侧后，采取了什么具体行动？ |'
- en: '| Retrieval | A: After turning their head to the left to view the left side
    of the page, the user performs… |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 检索 | 答：在将头转向左侧以查看页面左侧后，用户执行了… |'
- en: '| Prediction | Q: Given the mouse is over ’Add NeurIPS 2024 DB Track Submission,’
    what’s the likely next step? |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | 问：给定鼠标悬停在‘添加 NeurIPS 2024 DB 轨迹提交’上，接下来的可能步骤是什么？ |'
- en: '| A: It would be to click on the ’Add NeurIPS 2024 Datasets and Benchmarks
    Track Submission’ button… |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 答：这将是点击‘添加 NeurIPS 2024 数据集和基准测试轨迹提交’按钮… |'
- en: '| Sequential | Q: Scrolls down from the ’Moon Gravity’, which of the following
    cheats? A. Change Weather B. Skyfall … |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 顺序 | 问：从‘月球重力’向下滚动时，以下哪个选项是作弊的？ A. 改变天气 B. 天降 … |'
- en: '| Reasoning | A: [[B]] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 答：[[B]] |'
- en: 'In the last stage, human annotators will follow the guideline in [subsection A.3](#A1.SS3.SSS0.Px5
    "Human-LLM Cooperated Instruction Generation. ‣ A.3 Human Keyframes Annotation
    Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") and carefully review
    the entire video and MLLM-generated QA pairs to correct inaccuracies and hallucinations,
    as well as supplement information for both questions and answers to make these
    tasks more challenging.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '在最后阶段，人类标注者将按照 [A.3 小节](#A1.SS3.SSS0.Px5 "人类-LLM 合作指令生成。 ‣ A.3 人类关键帧标注过程 ‣
    附录 A 数据集构建细节 ‣ 第一部分附录 ‣ GUI-World: 用于 GUI 方向的多模态 LLM 基于代理的数据集") 中的指南，仔细审查整个视频和
    MLLM 生成的问答对，以纠正不准确和幻觉，并补充问题和答案的信息，使这些任务更加具有挑战性。'
- en: 3 Progressive Enhancement on GUI Perception Ability
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 GUI 感知能力的渐进增强
- en: 'We introduce our strategy to enhance the GUI-oriented capabilities of current
    MLLMs on both static and dynamic GUI content. Inspired by previous studies [[9](#bib.bib9),
    [35](#bib.bib35)], we structure our methodology into two distinct fine-tuning
    stages, as illustrated in [Figure 6](#S3.F6 "Figure 6 ‣ 3 Progressive Enhancement
    on GUI Perception Ability ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"). Initially, we fine-tune the MLLM on simpler tasks, such as description
    queries and captioning exercises, to instill a basic understanding of GUI elements.
    Subsequently, building on this foundation, the second stage aims to augment the
    MLLM’s proficiency with more complex and challenging tasks. Our fine-tuning is
    all based on the Supervised Fine-Tuning (SFT): $\mathcal{L}_{\mathrm{SFT}}\left(\pi_{\theta}\right)=-\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[\log\pi_{\theta}(y\mid
    x)\right]$ denotes the model parameters that need to be optimized.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们介绍了增强当前 MLLMs 在静态和动态 GUI 内容上的 GUI 方向能力的策略。受到之前研究的启发 [[9](#bib.bib9), [35](#bib.bib35)]，我们将方法论结构化为两个不同的微调阶段，如
    [图 6](#S3.F6 "图 6 ‣ 3 GUI 感知能力的渐进增强 ‣ GUI-World: 用于 GUI 方向的多模态 LLM 基于代理的数据集")
    所示。最初，我们在简单任务上微调 MLLM，例如描述查询和字幕练习，以培养对 GUI 元素的基本理解。随后，在此基础上，第二阶段旨在增强 MLLM 在更复杂和具有挑战性的任务中的能力。我们的微调完全基于监督微调
    (SFT)：$\mathcal{L}_{\mathrm{SFT}}\left(\pi_{\theta}\right)=-\mathbb{E}_{(x,y)\sim\mathcal{D}}\left[\log\pi_{\theta}(y\mid
    x)\right]$ 表示需要优化的模型参数。'
- en: '![Refer to caption](img/73af3606e81f7551bb82b597b3269d14.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/73af3606e81f7551bb82b597b3269d14.png)'
- en: 'Figure 6: An overview of our fine-tuning architecture, focusing on GUI content
    alignment and instruction tuning.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：我们微调架构的概述，重点是 GUI 内容对齐和指令调优。
- en: 'Stage-1: Learning Preliminary for GUI Content.'
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阶段-1：GUI内容的初步学习。
- en: The initial phase focuses on aligning GUI content with a pre-trained vision
    encoder and a base LLM, utilizing GUI videos accompanied by detailed descriptions
    and captions. This phase aims to embed a robust understanding of fundamental GUI
    concepts and terminology within the MLLM. By engaging the model in basically captioning
    various GUI components, the model learns to recognize and articulate the functionalities
    and visual characteristics of these elements, thereby laying a solid groundwork
    for GUI knowledge.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 初始阶段着重于将GUI内容与预训练的视觉编码器和基础LLM对齐，利用带有详细描述和字幕的GUI视频。此阶段旨在将对基础GUI概念和术语的深入理解嵌入到MLLM中。通过让模型基本上为各种GUI组件生成标题，模型学习识别和阐述这些元素的功能和视觉特征，从而为GUI知识奠定坚实的基础。
- en: 'Stage-2: Mastering Advanced GUI Capability.'
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阶段-2：掌握高级GUI能力。
- en: Building on the foundational knowledge established in Stage 1, the second stage
    focuses on advancing the MLLM’s proficiency in interacting with GUI elements through
    more complex tasks. These tasks are designed to simulate real-world scenarios
    that the MLLM might encounter in GUI environments, which include predicting based
    on image sequences, engaging in conversations, retrieving both static and dynamic
    GUI elements, and performing reasoning tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在阶段1建立的基础知识上，第二阶段着重提升MLLM在处理GUI元素时的能力，通过更复杂的任务。这些任务旨在模拟MLLM可能在GUI环境中遇到的真实场景，包括基于图像序列的预测、进行对话、检索静态和动态GUI元素，以及执行推理任务。
- en: 'As illustrated in [Figure 6](#S3.F6 "Figure 6 ‣ 3 Progressive Enhancement on
    GUI Perception Ability ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), We employ the two-stage training architecture utilizing VideoChat2 [[35](#bib.bib35)]
    as our foundational model. Initially, videos and images are encoded using the
    UMT-L visual encoder [[36](#bib.bib36)]. Subsequently, a QFormer compresses visual
    tokens into a smaller set of query tokens. Drawing inspiration from [[37](#bib.bib37)],
    we enhance the QFormer [[38](#bib.bib38)] by integrating instructions to enable
    it to extract visual representations pertinent to the given instructions. Additionally,
    we apply low-rank adaptation (LoRA [[39](#bib.bib39)]) to base LLM. This model
    is concurrently fine-tuned with the visual encoder and QFormer using a Vision-grounded
    Text Generation (VTG) loss: $\mathcal{L}_{\text{VTG}}(\theta)=-\mathbb{E}\left[\log
    p(y|v;\theta)\right]$ represents the text output grounded in the visual context.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图6](#S3.F6 "Figure 6 ‣ 3 Progressive Enhancement on GUI Perception Ability
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")所示，我们采用了两阶段训练架构，使用VideoChat2
    [[35](#bib.bib35)]作为基础模型。最初，使用UMT-L视觉编码器[[36](#bib.bib36)]对视频和图像进行编码。随后，QFormer将视觉标记压缩成较小的查询标记。受到[[37](#bib.bib37)]的启发，我们通过整合指令来增强QFormer
    [[38](#bib.bib38)]，使其能够提取与给定指令相关的视觉表示。此外，我们对基础LLM应用低秩适配（LoRA [[39](#bib.bib39)]）。该模型与视觉编码器和QFormer同时进行微调，使用以视觉为基础的文本生成（VTG）损失：$\mathcal{L}_{\text{VTG}}(\theta)=-\mathbb{E}\left[\log
    p(y|v;\theta)\right]$，表示基于视觉上下文的文本输出。'
- en: 4 Experiments and Analysis
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 实验与分析
- en: 4.1 Experimental Setups
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Models.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型。
- en: 'We conduct evaluations on four of the most robust image-based MLLMs: GPT-4V(ision)
    [[1](#bib.bib1)], GPT-4o [[40](#bib.bib40)], Qwen-VL-Max [[41](#bib.bib41)], and
    Gemini-Pro-1.5 [[42](#bib.bib42)]. We benchmark on three keyframe selection settings:
    (1) Random, where frames are sampled at fixed time intervals within a video; (2)
    Extracted, with keyframes extracted using Katna²²2https://github.com/keplerlab/katna;
    and (3) Human, where keyframes are selected by humans during the annotation process.
    For the Random and Extracted settings, we input 10 frames into each MLLM, while
    the Human setting uses an average of 6.719 frames, as detailed in [Table 2](#S2.T2
    "Table 2 ‣ 2.2 GUI Video Collection and Keyframe Annotation Process ‣ 2 GUI-World:
    A Comprehensive Dataset for GUI Understanding ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"). Each model’s responses employ a three-step Chain-of-Thought
    (CoT) [[43](#bib.bib43)] process, i.e., “Describe-Analyze-Answer”, to evaluate
    their peak performance. Additionally, we assessed three advanced VideoLLMs—ChatUnivi
    [[44](#bib.bib44)], Minigpt4-video [[45](#bib.bib45)], and Videochat2 [[46](#bib.bib46)]—for
    their performance on GUI content. For detailed experimental setups are referred
    to [Appendix C](#A3 "Appendix C Details of Experiments Setups ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对四个最强大的图像基础MLLMs进行评估：GPT-4V(ision) [[1](#bib.bib1)]、GPT-4o [[40](#bib.bib40)]、Qwen-VL-Max
    [[41](#bib.bib41)]和Gemini-Pro-1.5 [[42](#bib.bib42)]。我们在三种关键帧选择设置下进行基准测试：(1) 随机，其中帧在视频中以固定时间间隔采样；(2)
    提取，使用Katna²²2https://github.com/keplerlab/katna提取关键帧；(3) 人工，其中关键帧在注释过程中由人类选择。对于随机和提取设置，我们将10帧输入到每个MLLM中，而人工设置使用平均6.719帧，如[表2](#S2.T2
    "表2 ‣ 2.2 GUI视频收集与关键帧注释过程 ‣ 2 GUI-World: 一个用于GUI理解的综合数据集 ‣ GUI-World: 一个用于GUI导向的多模态LLM代理的数据集")所示。每个模型的响应采用三步链式思维（CoT）[[43](#bib.bib43)]过程，即“描述-分析-回答”，以评估其最佳性能。此外，我们还评估了三种先进的VideoLLMs——ChatUnivi
    [[44](#bib.bib44)]、Minigpt4-video [[45](#bib.bib45)]和Videochat2 [[46](#bib.bib46)]——在GUI内容上的表现。有关详细的实验设置，请参考[附录C](#A3
    "附录C 实验设置细节 ‣ 第I部分附录 ‣ GUI-World: 一个用于GUI导向的多模态LLM代理的数据集")。'
- en: Evaluation Metrics.
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评价指标。
- en: 'To assess free-form questions and multiple-round conversations, we utilize
    the LLM-as-a-Judge methodology, which assigns a similarity score ranging from
    1 to 5 between MLLM’s response and a predefined golden answer, already validated
    by previous studies[[47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]. For
    a comprehensive evaluation, we also provide BLEU [[50](#bib.bib50)] and BERTScore
    [[51](#bib.bib51)] in [Appendix D](#A4 "Appendix D Additional Experiments Results
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"). For multiple-choice questions, we measure performance using accuracy
    as the primary evaluation metric.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估自由形式的问题和多轮对话，我们采用LLM-as-a-Judge方法，该方法为MLLM的响应与预定义的黄金答案之间分配一个从1到5的相似度分数，该黄金答案已通过先前的研究验证[[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49)]。为了进行全面评估，我们还提供了BLEU [[50](#bib.bib50)]和BERTScore
    [[51](#bib.bib51)]，详见[附录D](#A4 "附录D 额外实验结果 ‣ 第I部分附录 ‣ GUI-World: 一个用于GUI导向的多模态LLM代理的数据集")。对于多项选择题，我们使用准确度作为主要评价指标。'
- en: Textual Information Integration.
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文本信息集成。
- en: 'To investigate the effectiveness of integrating image-caption models to enlarge
    the context window for LLMs—typically employed in natural videos—and the helpfulness
    of GUI history content in accomplishing GUI-oriented tasks, we implement three
    experimental settings: Detailed Caption, Concise Caption, and Vision + Detailed
    Caption. GPT-4V is utilized to provide captions of these keyframes, integrating
    human annotators’ operational intents to more accurately describe each frame,
    being validated in [subsection A.3](#A1.SS3 "A.3 Human Keyframes Annotation Process
    ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探讨将图像-标题模型集成以扩大LLMs的上下文窗口——通常用于自然视频——以及GUI历史内容在完成GUI导向任务中的帮助程度，我们实施了三种实验设置：详细标题、简洁标题和视觉
    + 详细标题。我们使用GPT-4V为这些关键帧提供标题，结合人类注释员的操作意图，以更准确地描述每个帧，并在[子节A.3](#A1.SS3 "A.3 人工关键帧注释过程
    ‣ 附录A 数据集构建细节 ‣ 第I部分附录 ‣ GUI-World: 一个用于GUI导向的多模态LLM代理的数据集")中验证。'
- en: 'Table 4: The overall performance in six GUI scenarios for MACQ and Free-form
    queries. ‘D.C.’ means detailed caption, and ‘C.C.’ means concise caption. ‘R.’,
    ‘E.’, and ‘H.’ denote random-selected, programmatic-selected, and human-selected
    keyframes, respectively. ‘MC’ means Multiple-Choice QA and ‘Free’ represents the
    average score of all free-form and conversational queries.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：MACQ 和自由形式查询在六种 GUI 场景中的整体表现。‘D.C.’ 意味着详细标题，‘C.C.’ 意味着简洁标题。‘R.’、‘E.’ 和 ‘H.’
    分别表示随机选择、程序选择和人工选择的关键帧。‘MC’ 代表多项选择 QA，‘Free’ 代表所有自由形式和对话查询的平均分数。
- en: '|  | Models | Setting | Software | Website | XR | Multi | IOS | Android | Avg.
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| | 模型 | 设置 | 软件 | 网站 | XR | 多模态 | IOS | 安卓 | 平均值 |'
- en: '|  | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free
    | MC | Free |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| | MC | 自由 | MC | 自由 | MC | 自由 | MC | 自由 | MC | 自由 | MC | 自由 | MC | 自由 |'
- en: '| ImageLLMs | Gemini-Pro-1.5 | R. | 81.7% | 3.339 | 82.6% | 3.452 | 81.2% |
    3.154 | 81.2% | 2.959 | 82.0% | 3.213 | 81.6% | 3.220 | 81.7% | 3.223 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ImageLLMs | Gemini-Pro-1.5 | R. | 81.7% | 3.339 | 82.6% | 3.452 | 81.2% |
    3.154 | 81.2% | 2.959 | 82.0% | 3.213 | 81.6% | 3.220 | 81.7% | 3.223 |'
- en: '| E. | 78.5% | 3.152 | 77.8% | 3.215 | 80.8% | 3.006 | 71.8% | 2.777 | 79.3%
    | 3.007 | 78.5% | 3.168 | 77.8% | 3.054 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| E. | 78.5% | 3.152 | 77.8% | 3.215 | 80.8% | 3.006 | 71.8% | 2.777 | 79.3%
    | 3.007 | 78.5% | 3.168 | 77.8% | 3.054 |'
- en: '| Qwen-VL-Max | R. | 74.9% | 2.676 | 76.9% | 2.656 | 74.2% | 2.469 | 68.8%
    | 2.432 | 75.4% | 2.779 | 73.7% | 2.309 | 74.0% | 2.553 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 74.9% | 2.676 | 76.9% | 2.656 | 74.2% | 2.469 | 68.8%
    | 2.432 | 75.4% | 2.779 | 73.7% | 2.309 | 74.0% | 2.553 |'
- en: '| E. | 74.3% | 2.624 | 75.8% | 2.627 | 69.0% | 2.499 | 64.8% | 2.362 | 77.4%
    | 2.659 | 65.8% | 2.277 | 71.2% | 2.508 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| E. | 74.3% | 2.624 | 75.8% | 2.627 | 69.0% | 2.499 | 64.8% | 2.362 | 77.4%
    | 2.659 | 65.8% | 2.277 | 71.2% | 2.508 |'
- en: '| H. | 75.8% | 2.651 | 75.5% | 2.698 | 77.6% | 2.373 | 66.9% | 2.490 | 74.3%
    | 2.633 | - | - | 74.0% | 2.569 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| H. | 75.8% | 2.651 | 75.5% | 2.698 | 77.6% | 2.373 | 66.9% | 2.490 | 74.3%
    | 2.633 | - | - | 74.0% | 2.569 |'
- en: '| GPT-4V | R. | 81.5% | 3.589 | 80.9% | 3.648 | 80.6% | 3.200 | 75.0% | 3.452
    | 82.5% | 3.614 | 78.3% | 3.515 | 79.8% | 3.503 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 81.5% | 3.589 | 80.9% | 3.648 | 80.6% | 3.200 | 75.0% | 3.452
    | 82.5% | 3.614 | 78.3% | 3.515 | 79.8% | 3.503 |'
- en: '| E. | 85.1% | 3.407 | 80.1% | 3.433 | 81.8% | 2.892 | 81.9% | 3.219 | 86.4%
    | 3.427 | 79.9% | 3.176 | 82.6% | 3.259 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| E. | 85.1% | 3.407 | 80.1% | 3.433 | 81.8% | 2.892 | 81.9% | 3.219 | 86.4%
    | 3.427 | 79.9% | 3.176 | 82.6% | 3.259 |'
- en: '| H. | 86.0% | 3.520 | 79.8% | 3.655 | 83.4% | 3.265 | 76.9% | 3.449 | 79.9%
    | 3.453 | - | - | 81.2% | 3.469 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| H. | 86.0% | 3.520 | 79.8% | 3.655 | 83.4% | 3.265 | 76.9% | 3.449 | 79.9%
    | 3.453 | - | - | 81.2% | 3.469 |'
- en: '| D.C. | 85.0% | 3.350 | 83.1% | 3.380 | 82.3% | 3.056 | 84.2% | 3.358 | 81.6%
    | 2.751 | 81.7% | 3.427 | 83.0% | 3.316 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 85.0% | 3.350 | 83.1% | 3.380 | 82.3% | 3.056 | 84.2% | 3.358 | 81.6%
    | 2.751 | 81.7% | 3.427 | 83.0% | 3.316 |'
- en: '| C.C | 80.7% | 3.028 | 72.2% | 3.025 | 82.8% | 2.809 | 81.3% | 3.160 | 76.5%
    | 2.868 | 76.4% | 2.939 | 78.3% | 2.971 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| C.C | 80.7% | 3.028 | 72.2% | 3.025 | 82.8% | 2.809 | 81.3% | 3.160 | 76.5%
    | 2.868 | 76.4% | 2.939 | 78.3% | 2.971 |'
- en: '| H.+D.C. | 82.5% | 3.494 | 83.2% | 3.682 | 85.9% | 3.191 | 83.9% | 3.617 |
    80.9% | 3.516 | 84.9% | 3.758 | 83.5% | 3.543 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 82.5% | 3.494 | 83.2% | 3.682 | 85.9% | 3.191 | 83.9% | 3.617 |
    80.9% | 3.516 | 84.9% | 3.758 | 83.5% | 3.543 |'
- en: '| GPT-4o | H. | 86.5% | 3.644 | 83.3% | 3.740 | 84.3% | 3.285 | 81.1% | 3.654
    | 83.3% | 3.558 | 90.0% | 3.561 | 84.8% | 3.573 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 86.5% | 3.644 | 83.3% | 3.740 | 84.3% | 3.285 | 81.1% | 3.654
    | 83.3% | 3.558 | 90.0% | 3.561 | 84.8% | 3.573 |'
- en: '| VideoLLMs | ChatUnivi | - | 28.4% | 2.389 | 22.2% | 2.349 | 20.6% | 2.161
    | 17.5% | 2.275 | 22.6% | 2.337 | 23.0% | 2.390 | 22.4% | 2.317 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| VideoLLMs | ChatUnivi | - | 28.4% | 2.389 | 22.2% | 2.349 | 20.6% | 2.161
    | 17.5% | 2.275 | 22.6% | 2.337 | 23.0% | 2.390 | 22.4% | 2.317 |'
- en: '| Minigpt4Video | - | 18.9% | 1.475 | 15.3% | 1.520 | 16.3% | 1.362 | 15.4%
    | 1.457 | 20.1% | 1.501 | 14.6% | 1.342 | 16.8% | 1.443 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 18.9% | 1.475 | 15.3% | 1.520 | 16.3% | 1.362 | 15.4%
    | 1.457 | 20.1% | 1.501 | 14.6% | 1.342 | 16.8% | 1.443 |'
- en: '| VideoChat2 | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005 | 40.4% |
    2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005 | 40.4% |
    2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
- en: '| GUI-Vid | - | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861
    | 51.8% | 2.773 | 53.4% | 2.572 | 54.6% | 2.796 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861
    | 51.8% | 2.773 | 53.4% | 2.572 | 54.6% | 2.796 |'
- en: Keyframes and Resolution.
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键帧和分辨率。
- en: To explore the upper bound of GUI-oriented capabilities, particularly in dynamic
    and sequential tasks, we conduct ablation studies focusing on the impact of the
    number of keyframes and image resolutions. We vary the number of keyframes (8,
    16) fed into GUI-Vid. Additionally, we test the effect of different image resolutions
    on GPT-4o, using both low and high settings, to further assess how resolution
    influences performance.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探究GUI导向能力的上限，特别是在动态和顺序任务中，我们进行了一些剥离研究，关注关键帧数量和图像分辨率的影响。我们变化了输入GUI-Vid的关键帧数量（8，16）。此外，我们测试了不同图像分辨率对GPT-4o的影响，使用低分辨率和高分辨率设置，以进一步评估分辨率对性能的影响。
- en: 4.2 Empirical Results
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2 实证结果
- en: Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings.
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 商业图像LLMs在零样本设置中表现优于开源视频LLMs。
- en: 'Commercial ImageLLMs, notably GPT-4V and GPT-4o, consistently outperform open-source
    VideoLLMs in zero-shot settings. As detailed in [Table 4](#S4.T4 "Table 4 ‣ Textual
    Information Integration. ‣ 4.1 Experimental Setups ‣ 4 Experiments and Analysis
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"), GPT-4o
    exhibits superior performance across all GUI scenarios in complex tasks, reflected
    in its high scores in both multiple-choice and free-form queries, with an average
    of 84.8% and 3.573\. Similarly, Gemini demonstrates strong capabilities in captioning
    and descriptive tasks within software and iOS environments, scoring 2.836 and
    2.936, respectively, as shown in [Table 13](#A4.T13 "Table 13 ‣ Appendix D Additional
    Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents"). Further analysis ([Figure 7](#S4.F7 "Figure 7 ‣
    Commercial ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings. ‣
    4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents")) reveals that GPT-4V excels in applications
    with minimal textual content and simple layouts, such as TikTok, health apps,
    and GitHub. In contrast, its performance drops in more intricate applications
    like Microsoft ToDo and XR software. As for VideoLLMs, their significantly poorer
    performance is attributed to two main factors: their inability to accurately interpret
    GUI content from user inputs and a lack of sufficient GUI-oriented pretraining,
    which is evident from their inadequate performance in basic captioning and description
    tasks. See [Appendix D](#A4 "Appendix D Additional Experiments Results ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")
    for BLEU and BERTScore, as well as detailed performance for complex tasks.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '商业图像LLMs，特别是GPT-4V和GPT-4o，在零样本设置中始终优于开源视频LLMs。如[表 4](#S4.T4 "表 4 ‣ 文本信息整合 ‣
    4.1 实验设置 ‣ 4 实验与分析 ‣ GUI-World: 一个用于GUI导向的多模态LLM基础代理的数据集")中详细说明，GPT-4o在所有GUI场景中的复杂任务表现优异，体现在其在多项选择和自由表述查询中的高分，平均为84.8%和3.573。类似地，Gemini在软件和iOS环境中的标注和描述任务中表现强劲，分别得分2.836和2.936，如[表
    13](#A4.T13 "表 13 ‣ 附录 D 额外实验结果 ‣ 第 I 部分附录 ‣ GUI-World: 一个用于GUI导向的多模态LLM基础代理的数据集")所示。进一步分析（[图
    7](#S4.F7 "图 7 ‣ 商业图像LLMs在零样本设置中优于开源视频LLMs ‣ 4.2 实证结果 ‣ 4 实验与分析 ‣ GUI-World: 一个用于GUI导向的多模态LLM基础代理的数据集")）表明，GPT-4V在文本内容较少和布局简单的应用中表现出色，如TikTok、健康应用和GitHub。相反，在更复杂的应用中，如Microsoft
    ToDo和XR软件，其性能有所下降。至于VideoLLMs，它们的性能显著较差主要归因于两个因素：无法准确解释用户输入的GUI内容，以及缺乏足够的GUI导向预训练，这在其在基本标注和描述任务中的不佳表现中表现明显。有关BLEU和BERTScore的详细信息以及复杂任务的详细性能，请参见[附录
    D](#A4 "附录 D 额外实验结果 ‣ 第 I 部分附录 ‣ GUI-World: 一个用于GUI导向的多模态LLM基础代理的数据集")。'
- en: '![Refer to caption](img/ed65465faaea9c29389298de3edf01c6.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考标注](img/ed65465faaea9c29389298de3edf01c6.png)'
- en: 'Figure 7: Fine-grained performance of GPT-4V in each software and website.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: GPT-4V在每个软件和网站中的细粒度性能。'
- en: 'Table 5: Detailed scores for each tasks in Software scenarios. ‘Dyn.’ refers
    to queries on dynamic GUI content, and ‘Pred.’ indicates prediction tasks.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 软件场景中各任务的详细评分。“Dyn.” 指的是动态GUI内容的查询，“Pred.” 表示预测任务。'
- en: '|  | Models | Setting | Caption | Complex Tasks | Conversation | Average |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 设置 | 标注 | 复杂任务 | 对话 | 平均 |'
- en: '|  | Concise | Detailed | Static | Dyn. | Pred. | Round 1 | Round 2 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | 简洁 | 详细 | 静态 | 动态 | 预测 | 第一轮 | 第二轮 |'
- en: '| ImageLLMs | Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 2.969 | 2.822 | 3.450 |
    3.608 | 3.845 | 3.339 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| ImageLLMs | Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 2.969 | 2.822 | 3.450 |
    3.608 | 3.845 | 3.339 |'
- en: '| E. | 3.350 | 2.468 | 2.741 | 2.431 | 3.292 | 3.458 | 3.837 | 3.152 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.350 | 2.468 | 2.741 | 2.431 | 3.292 | 3.458 | 3.837 | 3.152 |'
- en: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.277 | 2.144 | 2.724 | 3.125 | 3.317
    | 2.676 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.277 | 2.144 | 2.724 | 3.125 | 3.317
    | 2.676 |'
- en: '| E. | 2.459 | 1.693 | 2.143 | 1.954 | 2.742 | 3.174 | 3.298 | 2.624 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.459 | 1.693 | 2.143 | 1.954 | 2.742 | 3.174 | 3.298 | 2.624 |'
- en: '| H. | 2.474 | 1.711 | 2.137 | 2.032 | 2.834 | 3.223 | 3.257 | 2.651 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.474 | 1.711 | 2.137 | 2.032 | 2.834 | 3.223 | 3.257 | 2.651 |'
- en: '| GPT-4V | R. | 3.579 | 2.676 | 3.243 | 3.011 | 3.630 | 3.925 | 4.131 | 3.589
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.579 | 2.676 | 3.243 | 3.011 | 3.630 | 3.925 | 4.131 | 3.589
    |'
- en: '| E. | 3.141 | 2.301 | 2.927 | 2.627 | 3.541 | 3.844 | 4.103 | 3.407 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.141 | 2.301 | 2.927 | 2.627 | 3.541 | 3.844 | 4.103 | 3.407 |'
- en: '| H. | 3.352 | 2.509 | 3.053 | 2.849 | 3.609 | 3.928 | 4.163 | 3.520 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.352 | 2.509 | 3.053 | 2.849 | 3.609 | 3.928 | 4.163 | 3.520 |'
- en: '| C.C. | 3.454 | 2.547 | 1.818 | 2.335 | 3.577 | 3.521 | 3.884 | 3.028 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 3.454 | 2.547 | 1.818 | 2.335 | 3.577 | 3.521 | 3.884 | 3.028 |'
- en: '| D.C. | 3.412 | 2.627 | 2.603 | 2.591 | 3.723 | 3.759 | 4.072 | 3.350 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 3.412 | 2.627 | 2.603 | 2.591 | 3.723 | 3.759 | 4.072 | 3.350 |'
- en: '| H.+D.C. | 3.436 | 2.677 | 2.927 | 2.750 | 3.791 | 3.857 | 4.148 | 3.494 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.436 | 2.677 | 2.927 | 2.750 | 3.791 | 3.857 | 4.148 | 3.494 |'
- en: '| GPT-4o | H. | 4.048 | 3.028 | 3.125 | 3.117 | 3.562 | 4.129 | 4.318 | 3.644
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 4.048 | 3.028 | 3.125 | 3.117 | 3.562 | 4.129 | 4.318 | 3.644
    |'
- en: '| VideoLLMs | ChatUnivi | - | 1.587 | 1.240 | 1.705 | 1.656 | 2.524 | 2.698
    | 3.366 | 2.389 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| VideoLLMs | ChatUnivi | - | 1.587 | 1.240 | 1.705 | 1.656 | 2.524 | 2.698
    | 3.366 | 2.389 |'
- en: '| Minigpt4Video | - | 1.246 | 1.073 | 1.249 | 1.235 | 1.675 | 1.494 | 1.719
    | 1.475 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.246 | 1.073 | 1.249 | 1.235 | 1.675 | 1.494 | 1.719
    | 1.475 |'
- en: '| VideoChat2 | - | 1.992 | 1.312 | 1.812 | 1.682 | 2.158 | 2.342 | 2.720 |
    2.144 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.992 | 1.312 | 1.812 | 1.682 | 2.158 | 2.342 | 2.720 |
    2.144 |'
- en: '| GUI-Vid | - | 3.562 | 2.058 | 2.376 | 2.090 | 3.435 | 3.080 | 3.260 | 2.847
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 3.562 | 2.058 | 2.376 | 2.090 | 3.435 | 3.080 | 3.260 | 2.847
    |'
- en: Performance Variate in Different GUI Scenarios and Applications.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 性能在不同的 GUI 场景和应用中的变化。
- en: 'GPT-4V ([Figure 7](#S4.F7 "Figure 7 ‣ Commercial ImageLLMs outperform Open-source
    VideoLLMs in Zero-shot Settings. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")) and Gemini
    ([Figure 16](#A4.F16 "Figure 16 ‣ Appendix D Additional Experiments Results ‣
    Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"))
    excel in common scenarios such as mobile and website interfaces but show marked
    deficiencies in more complex GUI environments like XR and multi-window interactions,
    across both captioning and intricate tasks. This performance gap highlights a
    significant shortfall in understanding environments where GUI elements are scattered
    and demand sophisticated interpretation. It emphasizes the critical need for specialized
    benchmarks and datasets tailored to these complex GUI scenarios, which is essential
    for enhancing the GUI-oriented capabilities of MLLMs, paving the way for them
    to become truly reliable and high-performing general control agents.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4V ([图 7](#S4.F7 "图 7 ‣ 商业图像LLMs在零-shot 设置中优于开源视频LLMs。 ‣ 4.2 实证结果 ‣ 4 实验与分析
    ‣ GUI-World：一个用于 GUI 导向的多模态 LLM 基于数据集")) 和 Gemini ([图 16](#A4.F16 "图 16 ‣ 附录 D
    附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个用于 GUI 导向的多模态 LLM 基于数据集")) 在移动和网站界面等常见场景中表现优异，但在
    XR 和多窗口交互等更复杂的 GUI 环境中，显示出明显的不足，无论是在字幕生成还是复杂任务中。这种性能差距突显了对 GUI 元素分散且需要复杂解释的环境理解的重大不足。它强调了为这些复杂
    GUI 场景量身定制的专门基准和数据集的关键需求，这对于提升 MLLMs 的 GUI 导向能力至关重要，为其成为真正可靠和高性能的通用控制代理铺平了道路。
- en: Keyframe Selection is Important for GUI-oriented Tasks.
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键帧选择对 GUI 导向的任务非常重要。
- en: 'Across both basic tasks such as captioning and more complex tasks like prediction
    and reasoning, significant variations are evident among keyframe selection methods.
    As shown in [Table 14](#A4.T14 "Table 14 ‣ Appendix D Additional Experiments Results
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") and [Table 16](#A4.T16 "Table 16 ‣ Appendix D Additional Experiments
    Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), GPT-4V and Gemini significantly benefit from using random-selected and
    human-selected keyframes, scoring approximately 0.2-0.3 points higher in both
    captioning and free-form tasks than those using programmatic extraction. This
    suggests that traditional keyframe technologies, designed for natural videos,
    are less effective for detecting essential GUI operations, particularly when subtle
    movements like mouse clicks and dynamic changes are involved. Conversely, the
    difference in performance is relatively smaller in Qwen-VL-Max, indicating that
    while keyframe selection methods are crucial for models proficient in GUI content,
    they exert less influence on less capable models.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在字幕和预测、推理等复杂任务中，关键帧选择方法的显著差异是显而易见的。如[表14](#A4.T14 "表14 ‣ 附录D 额外实验结果 ‣ 第一部分附录
    ‣ GUI-World：面向GUI的多模态LLM代理数据集")和[表16](#A4.T16 "表16 ‣ 附录D 额外实验结果 ‣ 第一部分附录 ‣ GUI-World：面向GUI的多模态LLM代理数据集")所示，GPT-4V和Gemini在使用随机选择和人工选择的关键帧时显著受益，字幕和自由形式任务的得分比使用程序化提取的关键帧高约0.2-0.3分。这表明，传统的关键帧技术在检测重要GUI操作时效果较差，尤其是在涉及鼠标点击和动态变化等细微动作时。相反，Qwen-VL-Max的性能差异相对较小，这表明，尽管关键帧选择方法对擅长GUI内容的模型至关重要，但对能力较弱的模型影响较小。
- en: Dynamic GUI Tasks Continue to Challenge MLLMs.
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 动态GUI任务继续对多模态语言模型（MLLM）构成挑战。
- en: 'In the fine-grained tasks depicted in [Table 5](#S4.T5 "Table 5 ‣ Commercial
    ImageLLMs outperform Open-source VideoLLMs in Zero-shot Settings. ‣ 4.2 Empirical
    Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents"), GPT-4V and GPT-4o excel with static GUI content and prediction
    tasks over image sequences but struggle with providing detailed descriptions for
    entire videos and dynamic GUI content. This discrepancy is attributed to minor
    variations in GUI that significantly impact descriptions. Enhancing the number
    of keyframes and the granularity of perception might mitigate these issues. Among
    VideoLLMs, ChatUnivi excels in conversational tasks by effectively leveraging
    contextual nuances, particularly in subsequent rounds, yet it underperforms in
    GUI-oriented captioning tasks. In contrast, GUI-Vid demonstrates proficiency in
    sequential tasks but falls short in both captioning and static content handling.
    This gap is linked to deficiencies in GUI-Vid’s pretraining, which lacked comprehensive
    GUI content crucial for effective vision-text alignment, as evidenced by its poor
    performance in [Table 13](#A4.T13 "Table 13 ‣ Appendix D Additional Experiments
    Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") and an instruction tuning process also failed to fully address these
    shortcomings.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表5](#S4.T5 "表5 ‣ 商业图像LLM在零样本设置中优于开源视频LLM。 ‣ 4.2 实证结果 ‣ 4 实验与分析 ‣ GUI-World：面向GUI的多模态LLM代理数据集")中描绘的细粒度任务中，GPT-4V和GPT-4o在静态GUI内容和图像序列的预测任务上表现出色，但在为整个视频和动态GUI内容提供详细描述方面表现较差。这种差异归因于GUI中的微小变化，这些变化对描述有显著影响。增加关键帧的数量和感知的细粒度可能有助于缓解这些问题。在VideoLLM中，ChatUnivi在对话任务中表现突出，能够有效利用上下文细微差别，特别是在后续回合中，但在面向GUI的字幕任务中表现欠佳。相比之下，GUI-Vid在顺序任务中表现良好，但在字幕和静态内容处理方面表现不足。这一差距与GUI-Vid的预训练不足有关，该预训练缺乏有效的视觉-文本对齐所需的全面GUI内容，如[表13](#A4.T13
    "表13 ‣ 附录D 额外实验结果 ‣ 第一部分附录 ‣ GUI-World：面向GUI的多模态LLM代理数据集")所示，且指令调优过程也未能完全解决这些不足。
- en: Vision Perception is Important for Sequential GUI Tasks.
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 视觉感知对于顺序GUI任务至关重要。
- en: 'As demonstrated in [Table 5](#S4.T5 "Table 5 ‣ Commercial ImageLLMs outperform
    Open-source VideoLLMs in Zero-shot Settings. ‣ 4.2 Empirical Results ‣ 4 Experiments
    and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    integrating detailed textual information slightly outperforms purely vision-based
    inputs or detailed captions, akin to a Chain of Thought (CoT) [[43](#bib.bib43)]
    setting. Surprisingly, GPT-4V excels in caption and prediction tasks with just
    detailed captions, providing insights on enhancing specific GUI-oriented tasks
    through additional textual information. However, it still falls short in more
    challenging tasks, such as retrieving static or dynamic content. This underscores
    the critical role of visual perception in GUI environments, where even minor changes
    can significantly impact outcomes.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '如 [表 5](#S4.T5 "Table 5 ‣ Commercial ImageLLMs outperform Open-source VideoLLMs
    in Zero-shot Settings. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣
    GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents") 所示，整合详细的文本信息略微优于纯视觉输入或详细标题，类似于
    Chain of Thought (CoT) [[43](#bib.bib43)] 设置。令人惊讶的是，GPT-4V 在仅有详细标题的情况下，在标题和预测任务中表现出色，提供了通过额外的文本信息提升特定
    GUI 任务的见解。然而，它在更具挑战性的任务中，如检索静态或动态内容时仍然不够出色。这突显了在 GUI 环境中视觉感知的重要性，即使是微小的变化也会显著影响结果。'
- en: '![Refer to caption](img/a9dec09152f2faf70448f6e184673a87.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a9dec09152f2faf70448f6e184673a87.png)'
- en: 'Figure 8: Two stages of progressive training enhance GUI ability.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：两阶段渐进训练提升 GUI 能力。
- en: Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on
    GUI-World.
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GUI-Vid 在 GUI-World 上微调后在图形界面上的极大提升。
- en: 'As a pioneering study in training VideoLLMs as screen agents, GUI-Vid significantly
    outperforms the baseline model, showing an average improvement of 30% across various
    tasks and GUI scenarios, even surpassing the commercial ImageLLM, Qwen-VL-Max.
    This enhancement is particularly notable in captioning and prediction over image
    sequences, where GUI-Vid matches the performance of GPT-4V and Gemini-Pro. As
    shown in [Figure 8](#S4.F8 "Figure 8 ‣ Vision Perception is Important for Sequential
    GUI Tasks. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents"), our two-stage progressive
    fintuning significantly enhances the performance in all GUI scenarios. Remarkably,
    GUI-Vid scored 3.747 in caption tasks within the XR scenario, highlighting its
    potential in XR applications and the high-quality annotations provided by our
    dataset. However, in Multiple-Choice QA and Chatbot tasks, GUI-Vid still lags
    behind industry leaders like GPT-4V and Gemini-Pro, a discrepancy likely due to
    the baseline LLM’s weaker performance and the challenges of instruction-based
    fine-tuning.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '作为训练 VideoLLMs 作为屏幕代理的开创性研究，GUI-Vid 显著超越了基准模型，显示出在各种任务和 GUI 场景中的平均提升达到 30%，甚至超过了商业图像
    LLM，如 Qwen-VL-Max。这一提升在图像序列的标题和预测中尤为显著，GUI-Vid 与 GPT-4V 和 Gemini-Pro 的性能相匹配。如
    [图 8](#S4.F8 "Figure 8 ‣ Vision Perception is Important for Sequential GUI Tasks.
    ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") 所示，我们的两阶段渐进微调显著提升了所有 GUI 场景的表现。值得注意的是，GUI-Vid
    在 XR 场景中的标题任务中得分 3.747，突显了其在 XR 应用中的潜力以及我们数据集提供的高质量注释。然而，在多项选择 QA 和聊天机器人任务中，GUI-Vid
    仍然落后于像 GPT-4V 和 Gemini-Pro 等行业领头羊，这一差距可能是由于基准 LLM 的性能较弱和基于指令的微调挑战。'
- en: 'Table 6: The overall results for ablation study on GUI-Vid finetuning. F.K.
    and E.K. mean keyframes during the finetuning and evaluation process respectively.
    I. means Image, and V. means Video.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：GUI-Vid 微调的消融研究总体结果。F.K. 和 E.K. 分别表示微调和评估过程中使用的关键帧。I. 表示图像，V. 表示视频。
- en: '| Setting | F.K. | E.K. | Data | Software | Website | XR | Multi | IOS | Android
    | Avg. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | F.K. | E.K. | 数据 | 软件 | 网站 | XR | 多 | IOS | 安卓 | 平均 |'
- en: '| I. | V. | MC | Free | MC | Free | MC | Free | MC | Free | MC | Free | MC
    | Free | MC | Free |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| I. | V. | MC | 免费 | MC | 免费 | MC | 免费 | MC | 免费 | MC | 免费 | MC | 免费 | MC
    | 免费 |'
- en: '| Baseline | - | 8 | - | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005
    | 40.4% | 2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | - | 8 | - | - | 45.5% | 2.144 | 42.6% | 2.221 | 44.0% | 2.005 | 40.4%
    | 2.222 | 40.2% | 2.169 | 44.7% | 2.119 | 42.9% | 2.147 |'
- en: '| - | 16 | - | - | 45.1% | 2.144 | 41.8% | 2.240 | 41.0% | 2.007 | 40.7% |
    2.238 | 39.9% | 2.138 | 44.7% | 2.147 | 42.2% | 2.154 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| - | 16 | - | - | 45.1% | 2.144 | 41.8% | 2.240 | 41.0% | 2.007 | 40.7% |
    2.238 | 39.9% | 2.138 | 44.7% | 2.147 | 42.2% | 2.154 |'
- en: '| GUI-Vid | 8 | 8 | ✘ | ✔ | 58.3% | 2.709 | 53.6% | 2.817 | 62.2% | 2.626 |
    54.2% | 2.627 | 53.1% | 2.708 | 54.9% | 2.501 | 56.0% | 2.665 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | 8 | 8 | ✘ | ✔ | 58.3% | 2.709 | 53.6% | 2.817 | 62.2% | 2.626 |
    54.2% | 2.627 | 53.1% | 2.708 | 54.9% | 2.501 | 56.0% | 2.665 |'
- en: '| ✔ | ✔ | 59.9% | 2.856 | 54.1% | 2.925 | 59.0% | 2.751 | 52.1% | 2.837 | 50.0%
    | 2.756 | 54.0% | 2.571 | 54.8% | 2.782 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ✔ | ✔ | 59.9% | 2.856 | 54.1% | 2.925 | 59.0% | 2.751 | 52.1% | 2.837 | 50.0%
    | 2.756 | 54.0% | 2.571 | 54.8% | 2.782 |'
- en: '| 16 | ✘ | ✔ | 59.0% | 2.709 | 55.1% | 2.821 | 62.8% | 2.645 | 53.3% | 2.624
    | 55.5% | 2.727 | 55.7% | 2.501 | 56.9% | 2.671 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 16 | ✘ | ✔ | 59.0% | 2.709 | 55.1% | 2.821 | 62.8% | 2.645 | 53.3% | 2.624
    | 55.5% | 2.727 | 55.7% | 2.501 | 56.9% | 2.671 |'
- en: '| ✔ | ✔ | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861 | 51.8%
    | 2.772 | 53.4% | 2.572 | 54.6% | 2.796 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ✔ | ✔ | 59.9% | 2.847 | 54.1% | 2.957 | 55.6% | 2.764 | 52.9% | 2.861 | 51.8%
    | 2.772 | 53.4% | 2.572 | 54.6% | 2.796 |'
- en: 'Table 7: GPT-4o average performance in six GUI scenarios under low and high
    resolution.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：GPT-4o在低分辨率和高分辨率下的六种GUI场景的平均表现。
- en: '| Res. | Desc. | Conv. | Dyn. | Static | Caption | Average |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 研究 | 描述 | 对话 | 动态 | 静态 | 标题 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Low | 2.794 | 3.912 | 3.150 | 2.869 | 3.672 | 3.394 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 低 | 2.794 | 3.912 | 3.150 | 2.869 | 3.672 | 3.394 |'
- en: '| High | 3.031 | 4.056 | 3.318 | 3.131 | 3.911 | 3.573 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 3.031 | 4.056 | 3.318 | 3.131 | 3.911 | 3.573 |'
- en: Upper Bound of GUI-oriented Capability with More Keyframes and High Resolution.
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GUI导向能力的上限，具有更多关键帧和高分辨率。
- en: 'As depicted in [Table 6](#S4.T6 "Table 6 ‣ Supreme Enhancement of GUI-Vid on
    Graphic-based Interface After Finetuned on GUI-World. ‣ 4.2 Empirical Results
    ‣ 4 Experiments and Analysis ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents"), our two ablation studies during the fine-tuning phase demonstrate
    that utilizing GUI image-text captioning data significantly enhances the model’s
    preliminary understanding of GUI elements, outperforming training that relies
    solely on videos. Additionally, an increased number of keyframes correlates with
    improved performance across various scenarios, notably in environments featuring
    multiple windows and software applications. Further evidence from [Table 7](#S4.T7
    "Table 7 ‣ Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned
    on GUI-World. ‣ 4.2 Empirical Results ‣ 4 Experiments and Analysis ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") reveals that higher image
    resolutions substantially boost task performance, both basic and complex, for
    GPT-4o. These findings underscore the potential for further developing a more
    robust GUI Agent.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '如[表6](#S4.T6 "表6 ‣ 在GUI-World微调后的图形界面上，GUI-Vid的卓越增强 ‣ 4.2 实证结果 ‣ 4 实验与分析 ‣
    GUI-World: 一个面向GUI的多模态LLM代理数据集")所示，我们在微调阶段的两项消融研究表明，利用GUI图像-文本标注数据显著提升了模型对GUI元素的初步理解，优于仅依赖视频的训练。此外，关键帧数量的增加与各种场景中的性能提升相关，尤其是在具有多个窗口和软件应用的环境中。从[表7](#S4.T7
    "表7 ‣ 在GUI-World微调后的图形界面上，GUI-Vid的卓越增强 ‣ 4.2 实证结果 ‣ 4 实验与分析 ‣ GUI-World: 一个面向GUI的多模态LLM代理数据集")的进一步证据显示，更高的图像分辨率显著提升了GPT-4o的任务表现，包括基本和复杂任务。这些发现强调了进一步开发更强大的GUI代理的潜力。'
- en: 5 Related Work
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: MLLM-based Agents for GUI.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于MLLM的GUI代理。
- en: Building upon the significant advancements in LLMs [[52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55)] and advanced modality-mixing technologies [[56](#bib.bib56),
    [57](#bib.bib57)], groundbreaking MLLMs such as GPT-4V [[1](#bib.bib1)] and Gemini-Pro
    [[42](#bib.bib42)], along with open-source MLLMs like the LLaVA-1.6 series [[2](#bib.bib2),
    [58](#bib.bib58)], CogVLM [[59](#bib.bib59)], and Qwen-VL series [[41](#bib.bib41)],
    have shown outstanding performance across various tasks [[60](#bib.bib60), [61](#bib.bib61),
    [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)]. Venturing beyond text and single image, several
    studies are now exploring the integration of video modalities for tasks requiring
    dynamic or sequential visual content [[44](#bib.bib44), [35](#bib.bib35), [69](#bib.bib69),
    [70](#bib.bib70)]. In the GUI domain, leveraging the robust vision perception
    capabilities of MLLMs, applications such as WebAgents [[8](#bib.bib8), [71](#bib.bib71),
    [23](#bib.bib23)] and Mobile Agents [[17](#bib.bib17), [12](#bib.bib12), [72](#bib.bib72)]
    have gained popularity for handling everyday tasks like navigation and VQA. Frontier
    research is also investigating the use of MLLMs as general control agents, such
    as in playing computer games [[73](#bib.bib73), [74](#bib.bib74)] and serving
    as OS co-pilots [[75](#bib.bib75), [24](#bib.bib24)], paving the way for more
    complex GUI operations.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）[[52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]
    和先进的模态混合技术 [[56](#bib.bib56), [57](#bib.bib57)] 取得的显著进展之上，开创性的多模态大型语言模型（MLLMs）如GPT-4V
    [[1](#bib.bib1)] 和 Gemini-Pro [[42](#bib.bib42)]，以及开源的MLLMs，如LLaVA-1.6系列 [[2](#bib.bib2),
    [58](#bib.bib58)]、CogVLM [[59](#bib.bib59)] 和 Qwen-VL系列 [[41](#bib.bib41)]，在各种任务上展现了出色的性能
    [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68)]。超越文本和单一图像，几项研究现在探索了视频模态的整合，以处理需要动态或顺序视觉内容的任务
    [[44](#bib.bib44), [35](#bib.bib35), [69](#bib.bib69), [70](#bib.bib70)]。在GUI领域，利用MLLMs强大的视觉感知能力，WebAgents
    [[8](#bib.bib8), [71](#bib.bib71), [23](#bib.bib23)] 和 Mobile Agents [[17](#bib.bib17),
    [12](#bib.bib12), [72](#bib.bib72)] 等应用已在处理日常任务如导航和视觉问答（VQA）方面变得流行。前沿研究也在探索将MLLMs作为通用控制代理的使用，如玩电脑游戏
    [[73](#bib.bib73), [74](#bib.bib74)] 和作为操作系统副驾驶 [[75](#bib.bib75), [24](#bib.bib24)]，为更复杂的GUI操作铺平道路。
- en: GUI Benchmark & Dataset.
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GUI基准与数据集。
- en: Building upon the foundational work of Rico [[13](#bib.bib13)], the first mobile
    GUI video dataset, and AitW [[16](#bib.bib16)], which features 715k episodes of
    sequential images, research has extensively covered mobile [[14](#bib.bib14),
    [76](#bib.bib76), [77](#bib.bib77)] and web GUI environments [[78](#bib.bib78),
    [19](#bib.bib19), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]. Mind2Web
    [[20](#bib.bib20)] stands out in web-based datasets with over 2,000 tasks from
    137 websites across 31 domains. Advances continue into desktop GUIs with new toolkits
    [[23](#bib.bib23)], benchmarks [[21](#bib.bib21), [82](#bib.bib82)], and frameworks
    [[83](#bib.bib83), [84](#bib.bib84), [11](#bib.bib11)]. Research on GUI also transfers
    from comprehending single images in a static workspace [[8](#bib.bib8)] to sequential
    operations or multi-hop scenarios [[24](#bib.bib24), [22](#bib.bib22)], challenging
    the understanding and operation capability of these powerful models.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在Rico [[13](#bib.bib13)]、第一个移动GUI视频数据集，以及AitW [[16](#bib.bib16)]，其特点是包含715k个顺序图像的基础工作之上，研究广泛覆盖了移动[[14](#bib.bib14),
    [76](#bib.bib76), [77](#bib.bib77)]和网页GUI环境[[78](#bib.bib78), [19](#bib.bib19),
    [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)]。Mind2Web [[20](#bib.bib20)]
    在基于网络的数据集中脱颖而出，涵盖了来自137个网站的2,000多个任务，跨越31个领域。桌面GUI方面的进展也在持续，包括新的工具包 [[23](#bib.bib23)]、基准
    [[21](#bib.bib21), [82](#bib.bib82)] 和框架 [[83](#bib.bib83), [84](#bib.bib84),
    [11](#bib.bib11)]。GUI研究也从理解静态工作区中的单张图像 [[8](#bib.bib8)] 转移到顺序操作或多跳场景 [[24](#bib.bib24),
    [22](#bib.bib22)]，挑战了这些强大模型的理解和操作能力。
- en: 6 Conclusion
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we have introduced GUI-World, a comprehensive GUI-oriented dataset
    designed to benchmark and enhance understanding of virtual interfaces, especially
    sequential and dynamic tasks. This dataset extensively covers six scenarios and
    various tasks, addressing the previous research gap in comprehensively evaluating
    models’ capabilities in graphic-based understanding. We conduct extensive benchmarks
    on leading MLLMs and the first Video Agent ‘GUI-Vid’ finetuned on our GUI-World
    specifically for tasks requiring temporal information, achieving results comparable
    to top-performing models, providing detailed insights into enhancing GUI-related
    capabilities.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们介绍了GUI-World，一个全面的GUI导向数据集，旨在基准测试和增强对虚拟接口的理解，尤其是顺序和动态任务。该数据集广泛覆盖了六种场景和各种任务，填补了之前在全面评估模型图形理解能力方面的研究空白。我们对领先的MLLMs和首个在我们的GUI-World上针对需要时间信息的任务进行微调的Video
    Agent 'GUI-Vid'进行了广泛的基准测试，取得了与顶尖模型相当的结果，提供了关于增强GUI相关能力的详细见解。
- en: 7 Limitations
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7 局限性
- en: While our work presents significant advancements in the field of GUI agents,
    there are several limitations that need to be addressed. Firstly, despite expanding
    the dataset to include various GUI scenarios, our models still show limited generalization
    capabilities when applied to environments not represented in the training data.
    This highlights the need for further research to improve the adaptability and
    robustness of GUI agents in diverse and unseen environments. Additionally, the
    accuracy of our models heavily relies on the selection of keyframes. Automatically
    extracted keyframes often fail to capture the essential elements needed for accurate
    GUI understanding, indicating the need for more sophisticated keyframe extraction
    techniques. Furthermore, although VideoLLMs have shown improvements in handling
    dynamic content, their ability to understand and predict sequential information
    in GUI tasks remains suboptimal. This suggests a necessity for future work to
    focus on enhancing the temporal understanding capabilities of these models. Finally,
    the training and fine-tuning processes for VideoLLMs require significant computational
    resources, which may not be accessible to all researchers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的工作在GUI代理领域取得了显著进展，但仍存在几个需要解决的局限性。首先，尽管我们扩大了数据集以包含各种GUI场景，但我们的模型在应用于训练数据中未表示的环境时仍显示出有限的泛化能力。这突显了进一步研究以提高GUI代理在不同和未知环境中的适应性和鲁棒性的必要性。此外，我们模型的准确性严重依赖于关键帧的选择。自动提取的关键帧往往未能捕捉到准确理解GUI所需的基本元素，表明需要更复杂的关键帧提取技术。此外，尽管VideoLLMs在处理动态内容方面有所改进，但它们在理解和预测GUI任务中的顺序信息的能力仍然不理想。这表明未来的工作需要关注提高这些模型的时间理解能力。最后，VideoLLMs的训练和微调过程需要大量的计算资源，这可能不是所有研究人员都能获得的。
- en: 8 Potential Negative Societal Impacts
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8 潜在的负面社会影响
- en: While our work aims to advance the capabilities of GUI agents for beneficial
    applications, it is important to consider potential negative societal impacts.
    The use of GUI agents, especially those capable of operating across multiple environments
    and platforms, raises significant privacy concerns. Ensuring that these agents
    operate within strict ethical guidelines and that user data is handled securely
    and responsibly is paramount. There is also the risk of misuse of advanced GUI
    agents for malicious purposes, such as unauthorized access to sensitive information
    or automated exploitation of software vulnerabilities. Establishing robust security
    measures and ethical usage policies is essential to mitigate these risks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的工作旨在推进GUI代理的能力以用于有益的应用，但考虑到潜在的负面社会影响也很重要。GUI代理的使用，尤其是那些能够跨多个环境和平台操作的代理，带来了显著的隐私问题。确保这些代理在严格的伦理指南下操作，并且用户数据得到安全和负责任的处理是至关重要的。还有滥用先进GUI代理用于恶意目的的风险，例如未经授权访问敏感信息或自动利用软件漏洞。建立强有力的安全措施和伦理使用政策对于减轻这些风险至关重要。
- en: References
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: OpenAI [2023] OpenAI. Openai models - gpt-4-vision. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card),
    2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. Openai models - gpt-4-vision. [https://openai.com/research/gpt-4v-system-card](https://openai.com/research/gpt-4v-system-card)，2023。
- en: Liu et al. [2023a] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    Visual instruction tuning. In *NeurIPS*, 2023a.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 视觉指令调优。发表于*NeurIPS*，2023a。
- en: Yin et al. [2024] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models, 2024.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. [2024] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, 和 Enhong Chen。关于多模态大语言模型的调查，2024。
- en: 'Yang et al. [2023] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, 和 Lijuan Wang。Mm-react：提示ChatGPT进行多模态推理和行动，2023。
- en: 'Li et al. [2023a] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day, 2023a.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023a] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, 和 Jianfeng Gao。Llava-med：在一天内训练一个大语言与视觉助手用于生物医学，2023a。
- en: 'Zhang et al. [2024a] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling
    Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, Hui Ren, Sunyang
    Fu, James Zou, Wei Liu, Jing Huang, Chen Chen, Yuyin Zhou, Tianming Liu, Xun Chen,
    Yong Chen, Quanzheng Li, Hongfang Liu, and Lichao Sun. Biomedgpt: A unified and
    generalist biomedical generative pre-trained transformer for vision, language,
    and multimodal tasks, 2024a.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024a] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling
    Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, Hui Ren, Sunyang
    Fu, James Zou, Wei Liu, Jing Huang, Chen Chen, Yuyin Zhou, Tianming Liu, Xun Chen,
    Yong Chen, Quanzheng Li, Hongfang Liu, 和 Lichao Sun。Biomedgpt：一个统一的通用生物医学生成预训练转换器，用于视觉、语言和多模态任务，2024a。
- en: Huang et al. [2024a] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu,
    Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An
    embodied generalist agent in 3d world, 2024a.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2024a] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu,
    Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, 和 Siyuan Huang。一个在3D世界中的具身通用代理，2024a。
- en: 'Hong et al. [2023] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng
    Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong,
    Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong et al. [2023] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng
    Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong,
    Ming Ding, 和 Jie Tang。Cogagent：用于图形用户界面的视觉语言模型，2023。
- en: 'Lai et al. [2024] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan
    Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie
    Tang. Autowebglm: Bootstrap and reinforce a large language model-based web navigating
    agent, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai et al. [2024] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen,
    Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, 和 Jie Tang。Autowebglm：引导和强化一个基于大语言模型的网页导航代理，2024。
- en: 'Zhang et al. [2023a] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen,
    Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users,
    2023a.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023a] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen,
    Zebiao Huang, Bin Fu, 和 Gang Yu。Appagent：作为智能手机用户的多模态代理，2023a。
- en: 'Niu et al. [2024] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan
    Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven
    computer control agent, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu et al. [2024] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan
    Leng, He Kong, Yi Chang, 和 Qi Wang。Screenagent：一个由视觉语言模型驱动的计算机控制代理，2024。
- en: 'Wang et al. [2024a] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen,
    Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile
    device agent with visual perception, 2024a.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024a] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen,
    Ji Zhang, Fei Huang, 和 Jitao Sang。Mobile-agent：具备视觉感知的自主多模态移动设备代理，2024a。
- en: 'Deka et al. [2017] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman,
    Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app
    dataset for building data-driven design applications. In *Proceedings of the 30th
    annual ACM symposium on user interface software and technology*, pages 845–854,
    2017.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deka et al. [2017] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman,
    Daniel Afergan, Yang Li, Jeffrey Nichols, 和 Ranjitha Kumar。Rico：一个用于构建数据驱动设计应用程序的移动应用数据集。见
    *第30届ACM用户界面软件与技术年会论文集*，页码845–854，2017。
- en: 'Sun et al. [2022] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu,
    and Kai Yu. Meta-gui: towards multi-modal conversational agents on mobile gui.
    *arXiv preprint arXiv:2205.11029*, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2022] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu,
    和 Kai Yu。Meta-gui：致力于移动GUI上的多模态对话代理。*arXiv预印本 arXiv:2205.11029*，2022。
- en: 'Venkatesh et al. [2022] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan.
    Ugif: Ui grounded instruction following. *arXiv preprint arXiv:2211.07615*, 2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Venkatesh et al. [2022] Sagar Gubbi Venkatesh, Partha Talukdar, 和 Srini Narayanan.
    Ugif: 基于用户界面的指令跟随。*arXiv 预印本 arXiv:2211.07615*, 2022。'
- en: 'Rawles et al. [2023] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana
    Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android
    device control. *arXiv preprint arXiv:2307.10088*, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rawles et al. [2023] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana
    Riva, 和 Timothy Lillicrap. Android 在野外: 大规模安卓设备控制数据集。*arXiv 预印本 arXiv:2307.10088*,
    2023。'
- en: 'You et al. [2024] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda
    Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile
    ui understanding with multimodal llms. *arXiv preprint arXiv:2404.05719*, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'You et al. [2024] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda
    Swearngin, Jeffrey Nichols, Yinfei Yang, 和 Zhe Gan. Ferret-ui: 基于多模态LLMs的移动UI理解。*arXiv
    预印本 arXiv:2404.05719*, 2024。'
- en: Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    and Percy Liang. Reinforcement learning on web interfaces using workflow-guided
    exploration. In *International Conference on Learning Representations (ICLR)*,
    2018. URL [https://arxiv.org/abs/1802.08802](https://arxiv.org/abs/1802.08802).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    和 Percy Liang. 使用工作流引导探索的网页界面强化学习。发表于*国际学习表征会议（ICLR）*, 2018。URL [https://arxiv.org/abs/1802.08802](https://arxiv.org/abs/1802.08802)。
- en: 'Zhou et al. [2023] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena:
    A realistic web environment for building autonomous agents. *arXiv preprint arXiv:2307.13854*,
    2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. [2023] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, 等. Webarena:
    一个现实的网络环境用于构建自主智能体。*arXiv 预印本 arXiv:2307.13854*, 2023。'
- en: 'Deng et al. [2024] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens,
    Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the
    web. *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. [2024] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens,
    Boshi Wang, Huan Sun, 和 Yu Su. Mind2web: 朝着通用网络智能体迈进。*神经信息处理系统进展*, 36, 2024。'
- en: 'Kapoor et al. [2024] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu
    Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset
    and benchmark for enabling multimodal generalist autonomous agents for desktop
    and web. *arXiv preprint arXiv:2402.17553*, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kapoor et al. [2024] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing
    Yu Koh, Kiran Kamble, Waseem Alshikh, 和 Ruslan Salakhutdinov. Omniact: 用于桌面和网络的多模态通用自主智能体的数据集和基准测试。*arXiv
    预印本 arXiv:2402.17553*, 2024。'
- en: 'Zhang et al. [2024b] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu.
    Mmina: Benchmarking multihop multimodal internet agents. *arXiv preprint arXiv:2404.09992*,
    2024b.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2024b] Ziniu Zhang, Shulin Tian, Liangyu Chen, 和 Ziwei Liu. Mmina:
    基准测试多跳多模态互联网智能体。*arXiv 预印本 arXiv:2404.09992*, 2024b。'
- en: 'Zheng et al. [2024a] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang,
    Bo An, and Shuicheng Yan. Agentstudio: A toolkit for building general virtual
    agents. *arXiv preprint arXiv:2403.17918*, 2024a.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. [2024a] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang,
    Bo An, 和 Shuicheng Yan. Agentstudio: 构建通用虚拟智能体的工具包。*arXiv 预印本 arXiv:2403.17918*,
    2024a。'
- en: 'Xie et al. [2024] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng
    Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al.
    Osworld: Benchmarking multimodal agents for open-ended tasks in real computer
    environments. *arXiv preprint arXiv:2404.07972*, 2024.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. [2024] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng
    Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, 等.
    Osworld: 在真实计算环境中基准测试多模态智能体。*arXiv 预印本 arXiv:2404.07972*, 2024。'
- en: Apple [2024] Apple. Apple vision pro. [https://www.apple.com/apple-vision-pro/](https://www.apple.com/apple-vision-pro/),
    2024.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apple [2024] Apple. Apple vision pro. [https://www.apple.com/apple-vision-pro/](https://www.apple.com/apple-vision-pro/),
    2024。
- en: Zhu et al. [2016] Wangjiang Zhu, Jie Hu, Gang Sun, Xudong Cao, and Yu Qiao.
    A key volume mining deep framework for action recognition. In *Proceedings of
    the IEEE conference on computer vision and pattern recognition*, pages 1991–1999,
    2016.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2016] Wangjiang Zhu, Jie Hu, Gang Sun, Xudong Cao, 和 Yu Qiao. 用于动作识别的关键体积挖掘深度框架。发表于*IEEE计算机视觉与模式识别会议论文集*,
    页码 1991–1999, 2016。
- en: Yan et al. [2018] Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin, Mingtao Feng,
    Liang Zhang, and Ajmal Mian. Deep keyframe detection in human action videos. *arXiv
    preprint arXiv:1804.10021*, 2018.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan et al. [2018] Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin, Mingtao Feng,
    Liang Zhang, 和 Ajmal Mian. 人类动作视频中的深度关键帧检测。*arXiv 预印本 arXiv:1804.10021*, 2018。
- en: Mahasseni et al. [2017] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic.
    Unsupervised video summarization with adversarial lstm networks. In *Proceedings
    of the IEEE conference on Computer Vision and Pattern Recognition*, pages 202–211,
    2017.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahasseni 等人 [2017] Behrooz Mahasseni、Michael Lam 和 Sinisa Todorovic。《基于对抗性
    LSTM 网络的无监督视频摘要》。在 *IEEE 计算机视觉与模式识别会议论文集*，页码 202–211，2017年。
- en: '[29] OpenCV. Opencv. [https://opencv.org/](https://opencv.org/).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] OpenCV。Opencv。 [https://opencv.org/](https://opencv.org/)。'
- en: 'Li et al. [2024a] Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, and Lichao
    Sun. I think, therefore i am: Benchmarking awareness of large language models
    using awarebench, 2024a.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024a] 李元、黄悦、林雨立、吴思源、万瑶 和 孙立超。《我思故我在：通过 awarebench 评估大型语言模型的意识》，2024a。
- en: 'Sun et al. [2024] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang,
    Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang
    Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming
    Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi
    Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit
    Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang
    Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei
    Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu,
    Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong
    Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao
    Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen,
    and Yue Zhao. Trustllm: Trustworthiness in large language models, 2024.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2024] 孙立超、黄悦、王浩然、吴思源、张启辉、李元、高楚洁、黄一鑫、吕文汉、张亦轩、李鑫儿、刘正亮、刘一新、王宜觉、张志坤、贝蒂·维根、巴维亚·凯尔库拉、肖长伟、李春元、邢颖、黄峰、戚诚、黄华秀、曼诺利斯·凯利斯、玛琳卡·济特尼克、姜萌、莫希特·班萨尔、詹姆斯·邹、裴剑、刘健、高剑锋、韩家伟、赵杰宇、唐季亮、王金东、霍金·范斯科伦、约翰·米切尔、舒凯、徐凯迪、张凯伟、何丽芳、黄丽甫、迈克尔·巴克斯、Neil
    Zhenqiang Gong、Philip S. Yu、陈品瑜、顾全全、许然、应睿、纪水旺、苏曼·贾纳、陈天龙、刘天铭、周天一、王威廉、李翔、张向梁、王晓、谢星、陈迅、王绪宇、刘燕、叶彦芳、曹银智、陈勇
    和 于悦。《Trustllm：大型语言模型的可信度》，2024。
- en: 'Lei et al. [2024] Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao,
    and Kang Liu. S3eval: A synthetic, scalable, systematic evaluation suite for large
    language models, 2024.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 [2024] 雷方瑜、刘倩、黄义铭、何士铸、赵俊 和 刘康。《S3eval：一种合成的、可扩展的、系统化的大型语言模型评估套件》，2024。
- en: Dekoninck et al. [2024] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner,
    and Martin Vechev. Understanding large language models through the lens of dataset
    generation, 2024. URL [https://openreview.net/forum?id=miGpIhquyB](https://openreview.net/forum?id=miGpIhquyB).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dekoninck 等人 [2024] 贾斯帕·德科宁克、马克·费舍尔、卢卡·贝尔尔-凯尔纳 和 马丁·维赫夫。《通过数据集生成的视角理解大型语言模型》，2024年。URL
    [https://openreview.net/forum?id=miGpIhquyB](https://openreview.net/forum?id=miGpIhquyB)。
- en: 'Yu et al. [2023a] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner,
    Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed
    training data generator: A tale of diversity and bias, 2023a.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2023a] 于悦、庄宇晨、张杰宇、孟宇、亚历山大·拉特纳、拉恩杰·克里希纳、沈家铭 和 张超。《大型语言模型作为属性训练数据生成器：多样性与偏见的故事》，2023a。
- en: 'Li et al. [2023b] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.
    *arXiv preprint arXiv:2305.06355*, 2023b.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023b] 李坤昌、何沂南、王毅、李艺卓、王文海、罗平、王亚丽、王丽敏 和 乔宇。《视频聊天：以聊天为中心的视频理解》。*arXiv 预印本
    arXiv:2305.06355*，2023b。
- en: 'Li et al. [2024b] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin
    Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation
    models, 2024b.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024b] 李坤昌、王亚丽、李艺卓、王毅、何沂南、王丽敏 和 乔宇。《无掩码教师：朝着高效训练的视频基础模型》，2024b。
- en: 'Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip:
    Towards general-purpose vision-language models with instruction tuning, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2023] 戴文亮、李俊楠、李东旭、Anthony Meng Huat Tiong、赵俊琪、王伟生、李博洋、帕斯卡尔·冯 和 史蒂文·霍伊。《Instructblip：朝着通用的视觉-语言模型与指令调优》，2023。
- en: Zhang et al. [2023b] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision
    transformer with quadrangle attention, 2023b.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023b] 张启明、张静、徐玉飞 和 陶大成。《具有四边形注意力的视觉变换器》，2023b。
- en: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models, 2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适配，2021年。'
- en: 'OpenAI [2024a] OpenAI. Hello gpt-4o, May 2024a. URL [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/).
    Accessed: 2024-06-06.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024a] OpenAI. Hello gpt-4o，2024年5月。网址 [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)。访问日期：2024-06-06。
- en: 'Bai et al. [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,
    Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language
    model for understanding, localization, text reading, and beyond, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan,
    Peng Wang, Junyang Lin, Chang Zhou, 和 Jingren Zhou. Qwen-vl: 一种多功能的视觉语言模型，用于理解、定位、文本阅读及更多，2023年。'
- en: 'GeminiTeam [2023] GeminiTeam. Gemini: A family of highly capable multimodal
    models, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GeminiTeam [2023] GeminiTeam. Gemini: 一系列高能力的多模态模型，2023年。'
- en: Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models, 2023.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. Chain-of-thought提示引发大型语言模型的推理，2023年。
- en: 'Jin et al. [2023] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and
    Li Yuan. Chat-univi: Unified visual representation empowers large language models
    with image and video understanding. *arXiv preprint arXiv:2311.08046*, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin et al. [2023] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, 和
    Li Yuan. Chat-univi: 统一视觉表征赋能大型语言模型的图像和视频理解。*arXiv预印本 arXiv:2311.08046*，2023年。'
- en: 'Ataallah et al. [2024] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman,
    Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing
    multimodal llms for video understanding with interleaved visual-textual tokens,
    2024.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ataallah et al. [2024] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman,
    Essam Sleiman, Deyao Zhu, Jian Ding, 和 Mohamed Elhoseiny. Minigpt4-video: 通过交错的视觉-文本标记推进视频理解的多模态llms，2024年。'
- en: 'Li et al. [2023c] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu,
    Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal
    video understanding benchmark. *arXiv preprint arXiv:2311.17005*, 2023c.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023c] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi
    Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, 等. Mvbench: 综合多模态视频理解基准测试。*arXiv预印本
    arXiv:2311.17005*，2023c年。'
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, 和 Ion Stoica. 使用mt-bench和chatbot arena评估llm-as-a-judge，2023年。
- en: 'Liu et al. [2023b] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer
    Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao
    Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench:
    Benchmarking chinese alignment of large language models, 2023b.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023b] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer
    Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao
    Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, 和 Jie Tang. Alignbench:
    大型语言模型中文对齐的基准测试，2023b年。'
- en: 'Chen et al. [2024a] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen
    Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge:
    Assessing multimodal llm-as-a-judge with vision-language benchmark, 2024a.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2024a] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen
    Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, 和 Lichao Sun. Mllm-as-a-judge:
    通过视觉语言基准测试评估多模态llm-as-a-judge，2024a年。'
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318, 2002.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. Bleu: 一种自动评估机器翻译的方法。在*第40届计算语言学协会年会论文集*，第311–318页，2002年。'
- en: 'Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. *arXiv preprint
    arXiv:1904.09675*, 2019.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    和 Yoav Artzi. Bertscore: 使用bert评估文本生成。*arXiv预印本 arXiv:1904.09675*，2019年。'
- en: Team [2024] OpenAI Team. Gpt-4 technical report, 2024.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team [2024] OpenAI Team. Gpt-4技术报告，2024年。
- en: Meta [2023a] Meta. Llama 2, 2023a. [https://llama.meta.com/llama2](https://llama.meta.com/llama2).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta [2023a] Meta。Llama 2, 2023a。 [https://llama.meta.com/llama2](https://llama.meta.com/llama2)。
- en: Meta [2023b] Meta. Llama 3, 2023b. [https://llama.meta.com/llama3](https://llama.meta.com/llama3).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta [2023b] Meta。Llama 3, 2023b。 [https://llama.meta.com/llama3](https://llama.meta.com/llama3)。
- en: OpenAI [2024b] OpenAI. Mistral ai, 2024b. [https://mistral.ai/company/](https://mistral.ai/company/).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024b] OpenAI。Mistral ai, 2024b。 [https://mistral.ai/company/](https://mistral.ai/company/)。
- en: 'Li et al. [2023d] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models, 2023d.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '李等人 [2023d] 李俊南、李东旭、Silvio Savarese 和 Steven Hoi。Blip-2: 利用冻结的图像编码器和大型语言模型进行语言-图像预训练,
    2023d。'
- en: 'Alayrac et al. [2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm
    Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
    Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock,
    Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol
    Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model
    for few-shot learning, 2022.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alayrac 等人 [2022] Jean-Baptiste Alayrac、Jeff Donahue、Pauline Luc、Antoine Miech、Iain
    Barr、Yana Hasson、Karel Lenc、Arthur Mensch、Katie Millican、Malcolm Reynolds、Roman
    Ring、Eliza Rutherford、Serkan Cabi、Tengda Han、Zhitao Gong、Sina Samangooei、Marianne
    Monteiro、Jacob Menick、Sebastian Borgeaud、Andrew Brock、Aida Nematzadeh、Sahand Sharifzadeh、Mikolaj
    Binkowski、Ricardo Barreira、Oriol Vinyals、Andrew Zisserman 和 Karen Simonyan。Flamingo:
    一种用于少量样本学习的视觉语言模型, 2022。'
- en: Liu et al. [2023c] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved
    baselines with visual instruction tuning, 2023c.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2023c] 刘昊天、李春元、李宇恒 和 李永才。改进的基准测试与视觉指令调优, 2023c。
- en: 'Wang et al. [2024b] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi,
    Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu,
    Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained
    language models, 2024b.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2024b] 王伟汉、吕青松、于文萌、洪文怡、齐吉、王彦、季俊辉、杨卓毅、赵磊、宋希轩、徐佳正、徐斌、李娟子、董宇晓、丁铭 和 唐杰。Cogvlm:
    预训练语言模型的视觉专家, 2024b。'
- en: 'Yu et al. [2023b] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin
    Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal
    models for integrated capabilities, 2023b.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '于等人 [2023b] 于伟豪、杨征远、李琳洁、王建峰、林凯文、刘子成、王新超 和 王丽娟。Mm-vet: 评估大型多模态模型的综合能力, 2023b。'
- en: 'Liu et al. [2023d] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang,
    Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is
    your multi-modal model an all-around player? *arXiv preprint arXiv:2307.06281*,
    2023d.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人 [2023d] 刘元、段浩东、张元汉、李博、张松扬、赵望博、袁毅、王佳琪、何聪辉、刘梓维 等。Mmbench: 你的多模态模型是否全能？ *arXiv
    预印本 arXiv:2307.06281*，2023d。'
- en: Chen et al. [2024b] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang,
    Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we
    on the right way for evaluating large vision-language models?, 2024b.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2024b] 林晨、李金松、董小义、张磐、臧宇航、陈泽辉、段浩东、王佳琪、乔宇、林大华和赵峰。我们是否在评估大型视觉语言模型的正确道路上？,
    2024b。
- en: Wu et al. [2023] Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin,
    Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v
    (ision) serve medical applications? case studies on gpt-4v for multimodal medical
    diagnosis. *arXiv preprint arXiv:2310.09909*, 2023.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023] 吴超艺、雷家宇、郑巧雨、赵维克、林伟雄、张晓曼、周晓、赵子恒、张雅、王燕峰 等。GPT-4v (ision) 能否服务于医疗应用？关于
    GPT-4v 在多模态医疗诊断中的案例研究。*arXiv 预印本 arXiv:2310.09909*，2023。
- en: 'Wake et al. [2023] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu,
    and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from
    human demonstration. *arXiv preprint arXiv:2311.12015*, 2023.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wake 等人 [2023] Naoki Wake、Kanehira Atsushi、Sasabuchi Kazuhiro、Takamatsu Jun
    和 Ikeuchi Katsushi。Gpt-4v (ision) 在机器人学中的应用：从人类示范中进行多模态任务规划。*arXiv 预印本 arXiv:2311.12015*，2023。
- en: 'Huang et al. [2024b] Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo,
    Hechang Chen, Lichao Sun, and Bo Yang. Decision mamba: Reinforcement learning
    via hybrid selective sequence modeling, 2024b.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '黄等人 [2024b] 黄思礼、胡季峰、杨哲健、杨力维、罗涛、陈和昌、孙力超 和 杨博。Decision mamba: 通过混合选择序列建模进行强化学习,
    2024b。'
- en: 'Zhang et al. [2024c] Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin
    Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, and Lichao
    Sun. LLM-as-a-coauthor: Can mixed human-written and machine-generated text be
    detected? In Kevin Duh, Helena Gomez, and Steven Bethard, editors, *Findings of
    the Association for Computational Linguistics: NAACL 2024*, pages 409–436, Mexico
    City, Mexico, June 2024c. Association for Computational Linguistics. URL [https://aclanthology.org/2024.findings-naacl.29](https://aclanthology.org/2024.findings-naacl.29).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2024c] 琪辉 Zhang, 初杰 Gao, 东平 Chen, 岳 Huang, 宜欣 Huang, 珍阳 Sun,
    士林 Zhang, 为业 Li, 正彦 Fu, 耀 Wan, 和 力超 Sun。LLM-as-a-coauthor：混合人工编写和机器生成文本能否被检测？在
    Kevin Duh、Helena Gomez 和 Steven Bethard 编辑的 *Findings of the Association for Computational
    Linguistics: NAACL 2024* 中，第 409–436 页，墨西哥城，墨西哥，2024年6月。计算语言学协会。网址 [https://aclanthology.org/2024.findings-naacl.29](https://aclanthology.org/2024.findings-naacl.29)。'
- en: 'Zhao et al. [2024] Wei Zhao, Zhitao Hou, Siyuan Wu, Yan Gao, Haoyu Dong, Yao
    Wan, Hongyu Zhang, Yulei Sui, and Haidong Zhang. NL2Formula: Generating spreadsheet
    formulas from natural language queries. In Yvette Graham and Matthew Purver, editors,
    *Findings of the Association for Computational Linguistics: EACL 2024*, pages
    2377–2388, St. Julian’s, Malta, March 2024\. Association for Computational Linguistics.
    URL [https://aclanthology.org/2024.findings-eacl.158](https://aclanthology.org/2024.findings-eacl.158).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. [2024] 伟 Zhao, 志涛 Hou, 思源 Wu, 闫 Gao, 昊宇 Dong, 耀 Wan, 红宇 Zhang,
    雨磊 Sui, 和 海东 Zhang。NL2Formula：从自然语言查询生成电子表格公式。在 Yvette Graham 和 Matthew Purver
    编辑的 *Findings of the Association for Computational Linguistics: EACL 2024* 中，第
    2377–2388 页，圣朱利安，马耳他，2024年3月。计算语言学协会。网址 [https://aclanthology.org/2024.findings-eacl.158](https://aclanthology.org/2024.findings-eacl.158)。'
- en: 'Gui et al. [2024] Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su,
    Shaoling Dong, Xing Zhou, and Wenbin Jiang. Vision2ui: A real-world dataset with
    layout for code generation from ui designs, 2024.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gui et al. [2024] 易 Gui, 珍 Li, 耀 Wan, 益敏 Shi, 红宇 Zhang, 依苏 Su, 绍岭 Dong, 兴 Zhou,
    和 文斌 Jiang。Vision2ui：一个具有布局的真实世界数据集，用于从用户界面设计生成代码，2024。
- en: 'Maaz et al. [2023] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz
    Khan. Video-chatgpt: Towards detailed video understanding via large vision and
    language models. *arXiv preprint arXiv:2306.05424*, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maaz et al. [2023] 穆罕默德 Maaz, 哈努娜 Rasheed, 萨尔曼 Khan, 和 法赫德·沙赫巴兹 Khan。Video-chatgpt：通过大规模视觉和语言模型实现详细的视频理解。*arXiv
    预印本 arXiv:2306.05424*，2023。
- en: 'Lin et al. [2023a] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.
    Video-llava: Learning united visual representation by alignment before projection.
    *arXiv preprint arXiv:2311.10122*, 2023a.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2023a] 彬 Lin, 彬 Zhu, 杨 Ye, 慕南 Ning, 彭 Jin, 和 李 Yuan。Video-llava：通过对齐实现投影前的统一视觉表示。*arXiv
    预印本 arXiv:2311.10122*，2023a。
- en: 'Zhang et al. [2024d] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao,
    Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and
    Qi Zhang. Ufo: A ui-focused agent for windows os interaction, 2024d.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024d] 朝云 Zhang, 立群 Li, 士林 He, 徐 Zhang, 博 Qiao, 思 Qin, 明华 Ma,
    宇 Kang, 青伟 Lin, 萨拉凡 Rajmohan, 冬梅 Zhang, 和 琪 Zhang。Ufo：一个专注于用户界面的 Windows 操作系统交互代理，2024d。
- en: 'Chu et al. [2023] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang
    Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen.
    Mobilevlm : A fast, strong and open vision language assistant for mobile devices,
    2023.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. [2023] 向向 Chu, 林萌 Qiao, 新阳 Lin, 双 Xu, 杨 Yang, 毅明 Hu, 飞 Wei, 新宇 Zhang,
    博 Zhang, 晓林 Wei, 和 春华 Shen。Mobilevlm：一种快速、强大且开放的移动设备视觉语言助手，2023。
- en: 'Tan et al. [2024] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou,
    Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards
    general computer control: A multimodal agent for red dead redemption ii as a case
    study. *arXiv preprint arXiv:2403.03186*, 2024.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. [2024] 伟豪 Tan, 自洛 Ding, 文韬 Zhang, 博宇 Li, 博涵 Zhou, 君鹏 Yue, 昊崇 Xia,
    介川 Jiang, 龙涛 Zheng, 欣润 Xu 等。向通用计算机控制迈进：以《荒野大镖客 II》为案例的多模态代理。*arXiv 预印本 arXiv:2403.03186*，2024。
- en: 'Lin et al. [2023b] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan
    Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al.
    Mm-vid: Advancing video understanding with gpt-4v (ision). *arXiv preprint arXiv:2310.19773*,
    2023b.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2023b] 凯文 Lin, 法伊萨尔 Ahmed, 林杰 Li, 钟经 Lin, 艾赫桑 Azarnasab, 郑元 Yang,
    建锋 Wang, 林 Liang, 自成 Liu, 玉茂 Lu 等。MM-VID：通过 GPT-4V（Vision）推进视频理解。*arXiv 预印本 arXiv:2310.19773*，2023b。
- en: 'Song et al. [2024] Zirui Song, Yaohang Li, Meng Fang, Zhenhao Chen, Zecheng
    Shi, and Yuan Huang. Mmac-copilot: Multi-modal agent collaboration operating system
    copilot. *arXiv preprint arXiv:2404.18074*, 2024.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. [2024] 子瑞 Song, 耀航 Li, 孟 Fang, 珍浩 Chen, 泽成 Shi, 和 元 Huang。MMAC-Copilot：多模态代理协作操作系统副驾驶。*arXiv
    预印本 arXiv:2404.18074*，2024。
- en: Li et al. [2020] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge.
    Mapping natural language instructions to mobile ui action sequences, 2020.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020] Yang Li、Jiacong He、Xin Zhou、Yuan Zhang 和 Jason Baldridge。将自然语言指令映射到移动
    UI 动作序列，2020。
- en: 'Zhang et al. [2023c] Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng
    Cao, and Kai Yu. Mobile-Env: An evaluation platform and benchmark for llm-gui
    interaction. *CoRR*, abs/2305.08144, 2023c. URL [https://arxiv.org/abs/2305.08144](https://arxiv.org/abs/2305.08144).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023c] Danyang Zhang、Hongshen Xu、Zihan Zhao、Lu Chen、Ruisheng Cao 和
    Kai Yu。Mobile-Env：LLM-GUI 交互的评估平台和基准。*CoRR*，abs/2305.08144，2023c。网址 [https://arxiv.org/abs/2305.08144](https://arxiv.org/abs/2305.08144)。
- en: 'Lù et al. [2024] Xing Han Lù, Zdeněk Kasner, and Siva Reddy. Weblinx: Real-world
    website navigation with multi-turn dialogue, 2024.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lù 等人 [2024] Xing Han Lù、Zdeněk Kasner 和 Siva Reddy。Weblinx：具有多轮对话的真实网站导航，2024。
- en: 'Yao et al. [preprint] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    Webshop: Towards scalable real-world web interaction with grounded language agents.
    In *ArXiv*, preprint.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [预印本] Shunyu Yao、Howard Chen、John Yang 和 Karthik Narasimhan。WebShop：朝着可扩展的真实世界网络互动与扎根语言代理迈进。在
    *ArXiv*，预印本。
- en: 'Koh et al. [2024] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong
    Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel
    Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks.
    *arXiv preprint arXiv:2401.13649*, 2024.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等人 [2024] Jing Yu Koh、Robert Lo、Lawrence Jang、Vikram Duvvur、Ming Chong Lim、Po-Yu
    Huang、Graham Neubig、Shuyan Zhou、Ruslan Salakhutdinov 和 Daniel Fried。VisualWebArena：在现实视觉网络任务中评估多模态代理。*arXiv
    预印本 arXiv:2401.13649*，2024。
- en: 'Liu et al. [2024] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham
    Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms
    evolved in web page understanding and grounding?, 2024.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024] Junpeng Liu、Yifan Song、Bill Yuchen Lin、Wai Lam、Graham Neubig、Yuanzhi
    Li 和 Xiang Yue。VisualWebBench：多模态 LLM 在网页理解和扎根方面发展到什么程度？，2024。
- en: 'Mialon et al. [2023] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants,
    2023.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon 等人 [2023] Grégoire Mialon、Clémentine Fourrier、Craig Swift、Thomas Wolf、Yann
    LeCun 和 Thomas Scialom。Gaia：通用 AI 助手的基准，2023。
- en: Zheng et al. [2024b] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    Gpt-4v(ision) is a generalist web agent, if grounded, 2024b.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2024b] Boyuan Zheng、Boyu Gou、Jihyung Kil、Huan Sun 和 Yu Su。GPT-4V(ision)
    是一个通用网络代理，如果扎根，2024b。
- en: 'Liu et al. [2023e] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao
    Du, Peng Zhang, Yuxiao Dong, and Jie Tang. Webglm: Towards an efficient web-enhanced
    question answering system with human preferences, 2023e.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023e] Xiao Liu、Hanyu Lai、Hao Yu、Yifan Xu、Aohan Zeng、Zhengxiao Du、Peng
    Zhang、Yuxiao Dong 和 Jie Tang。WebGLM：朝着高效的网络增强问答系统与人类偏好，2023e。
- en: 'Kousar et al. [2023] Ambreen Kousar, Saif Ur Rehman Khan, Shahid Hussain, M. Abdul
    Basit Ur Rahim, Wen-Li Wang, and Naseem Ibrahim. A systematic review on pattern-based
    gui testing of android and web apps: State-of-the-art, taxonomy, challenges and
    future directions. In *2023 25th International Multitopic Conference (INMIC)*,
    pages 1–7, 2023. doi: 10.1109/INMIC60434.2023.10465949.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kousar 等人 [2023] Ambreen Kousar、Saif Ur Rehman Khan、Shahid Hussain、M. Abdul
    Basit Ur Rahim、Wen-Li Wang 和 Naseem Ibrahim。基于模式的 Android 和 Web 应用 GUI 测试的系统性回顾：最新进展、分类、挑战和未来方向。在
    *2023年第25届国际多主题会议 (INMIC)*，第1–7页，2023。doi: 10.1109/INMIC60434.2023.10465949。'
- en: 'Jorge et al. [2014] Rodrigo Funabashi Jorge, Márcio Eduardo Delamaro, Celso Gonçalves
    Camilo-Junior, and Auri Marcelo Rizzo Vincenzi. Test data generation based on
    gui: A systematic mapping. In *International Conference on Software Engineering
    Advances*, 2014. URL [https://api.semanticscholar.org/CorpusID:64041598](https://api.semanticscholar.org/CorpusID:64041598).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jorge 等人 [2014] Rodrigo Funabashi Jorge、Márcio Eduardo Delamaro、Celso Gonçalves
    Camilo-Junior 和 Auri Marcelo Rizzo Vincenzi。基于 GUI 的测试数据生成：系统性映射。在 *国际软件工程进展会议*，2014。网址
    [https://api.semanticscholar.org/CorpusID:64041598](https://api.semanticscholar.org/CorpusID:64041598)。
- en: Kulesovs [2015] Ivans Kulesovs. ios applications testing. 2015. URL [https://api.semanticscholar.org/CorpusID:59015994](https://api.semanticscholar.org/CorpusID:59015994).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulesovs [2015] Ivans Kulesovs。iOS 应用程序测试。2015。网址 [https://api.semanticscholar.org/CorpusID:59015994](https://api.semanticscholar.org/CorpusID:59015994)。
- en: 'Cheng et al. [2024] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao
    Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced
    visual gui agents, 2024.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2024] Kanzhi Cheng、Qiushi Sun、Yougang Chu、Fangzhi Xu、Yantao Li、Jianbing
    Zhang 和 Zhiyong Wu。SeeClick：利用 GUI 扎根为高级视觉 GUI 代理提供支持，2024。
- en: Hu et al. [2023] Han Hu, Haolan Zhan, Yujin Huang, and Di Liu. Pairwise gui
    dataset construction between android phones and tablets, 2023.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2023] Han Hu, Haolan Zhan, Yujin Huang 和 Di Liu. 安卓手机和平板电脑之间的成对 GUI 数据集构建，2023。
- en: 'Beltramelli [2017] Tony Beltramelli. pix2code: Generating code from a graphical
    user interface screenshot, 2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltramelli [2017] Tony Beltramelli. pix2code: 从图形用户界面截图生成代码，2017。'
- en: 'Yan et al. [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Qinghong Lin,
    Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian J. McAuley, Jianfeng
    Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models
    for zero-shot smartphone gui navigation. *ArXiv*, abs/2311.07562, 2023. URL [https://api.semanticscholar.org/CorpusID:265149992](https://api.semanticscholar.org/CorpusID:265149992).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Qinghong Lin, Linjie
    Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian J. McAuley, Jianfeng Gao,
    Zicheng Liu 和 Lijuan Wang. GPT-4V 在奇幻世界中：用于零-shot 智能手机 GUI 导航的大型多模态模型。*ArXiv*，abs/2311.07562，2023。网址
    [https://api.semanticscholar.org/CorpusID:265149992](https://api.semanticscholar.org/CorpusID:265149992)。
- en: 'Nakajima et al. [2013] Hajime Nakajima, Takeshi Masuda, and Ikuya Takahashi.
    Gui ferret: Gui test tool to analyze complex behavior of multi-window applications.
    *2013 18th International Conference on Engineering of Complex Computer Systems*,
    pages 163–166, 2013. URL [https://api.semanticscholar.org/CorpusID:837553](https://api.semanticscholar.org/CorpusID:837553).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nakajima 等人 [2013] Hajime Nakajima, Takeshi Masuda 和 Ikuya Takahashi. GUI Ferret:
    用于分析多窗口应用程序复杂行为的 GUI 测试工具。*2013年第18届复杂计算机系统工程国际会议*，第 163–166 页，2013。网址 [https://api.semanticscholar.org/CorpusID:837553](https://api.semanticscholar.org/CorpusID:837553)。'
- en: Rauschnabel et al. [2022] Philipp A. Rauschnabel, Reto Felix, Christian Hinsch,
    Hamza Shahab, and Florain Alt. What is xr? towards a framework for augmented and
    virtual reality. *Comput. Hum. Behav.*, 133:107289, 2022. URL [https://api.semanticscholar.org/CorpusID:247861674](https://api.semanticscholar.org/CorpusID:247861674).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rauschnabel 等人 [2022] Philipp A. Rauschnabel, Reto Felix, Christian Hinsch,
    Hamza Shahab 和 Florain Alt. XR 是什么？迈向增强现实和虚拟现实的框架。*计算机与人类行为*，133:107289，2022。网址
    [https://api.semanticscholar.org/CorpusID:247861674](https://api.semanticscholar.org/CorpusID:247861674)。
- en: '[94] Meta quest 3: New mixed reality vr headset. [https://www.meta.com/quest/quest-3](https://www.meta.com/quest/quest-3).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Meta Quest 3: 新型混合现实 VR 头戴设备。 [https://www.meta.com/quest/quest-3](https://www.meta.com/quest/quest-3)。'
- en: Sanders et al. [2019] Brian K. Sanders, Yuzhong Shen, and Dennis A. Vincenzi.
    Understanding user interface preferences for xr environments when exploring physics
    and engineering principles. In *International Conference on Applied Human Factors
    and Ergonomics*, 2019. URL [https://api.semanticscholar.org/CorpusID:197940610](https://api.semanticscholar.org/CorpusID:197940610).
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanders 等人 [2019] Brian K. Sanders, Yuzhong Shen 和 Dennis A. Vincenzi. 了解在探索物理和工程原理时
    XR 环境的用户界面偏好。发表于 *应用人因与人体工程学国际会议*，2019。网址 [https://api.semanticscholar.org/CorpusID:197940610](https://api.semanticscholar.org/CorpusID:197940610)。
- en: OpenAI [2023] OpenAI. Chatgpt, 2023. [https://openai.com/product/chatgpt](https://openai.com/product/chatgpt).
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. ChatGPT，2023。 [https://openai.com/product/chatgpt](https://openai.com/product/chatgpt)。
- en: Part I Appendix
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分 附录
- en: \parttoc
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: \parttoc
- en: Appendix A Details of Dataset Construction
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 数据集构建的详细信息
- en: A.1 Six Main GUI Categories
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1 六大主要 GUI 类别
- en: 'In earlier endeavors pertaining to GUI, such as those involving GUI testing [[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)], the focus was segmented into GUIs for Website,
    Software, IOS and Android platforms. However, as a comprehensive GUI dataset,
    we included all potential GUI scenarios in our dataset to ensure that our data
    is the most comprehensive knowledge that the GUI Agent needs to learn; we divided
    these scenarios into six categories:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期涉及 GUI 的工作中，例如那些涉及 GUI 测试的工作 [[85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87)]，重点分为网站、软件、IOS
    和 Android 平台的 GUI。然而，作为一个综合的 GUI 数据集，我们在数据集中包括了所有潜在的 GUI 场景，以确保我们的数据是 GUI Agent
    学习所需的最全面的知识；我们将这些场景分为六类：
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Android. This category focuses on the GUI scenarios that occur within the Android
    operating system, which is predominantly used on smartphones. Android’s ubiquity
    in the mobile market has led to a wide variety of GUI designs and interaction
    patterns, making it a rich field for study. This category has been the subject
    of extensive scrutiny in scholarly works such as  [[13](#bib.bib13), [76](#bib.bib76),
    [16](#bib.bib16), [88](#bib.bib88)].
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Android。此类别关注发生在 Android 操作系统中的 GUI 场景，Android 操作系统主要用于智能手机。Android 在移动市场上的普及导致了各种
    GUI 设计和交互模式，使其成为一个丰富的研究领域。此类别已经在学术作品中得到了广泛的审视，如 [[13](#bib.bib13), [76](#bib.bib76),
    [16](#bib.bib16), [88](#bib.bib88)]。
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Software. This category encapsulates the GUI scenarios arising within software
    applications, whether they are standalone programs or components of a larger suite.
    The diversity of software applications, from productivity tools to creative suites,
    offers a wide range of GUI scenarios for exploration. The literature is rich with
    research in this area, such as  [[89](#bib.bib89)].
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件。该类别涵盖了在软件应用中出现的GUI场景，无论它们是独立程序还是更大套件的一部分。从生产力工具到创意套件，软件应用的多样性提供了广泛的GUI场景进行探索。相关文献在这方面非常丰富，例如[[89](#bib.bib89)]。
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Website. This category is concerned with the GUI scenarios that manifest within
    a web browser. Given the ubiquity of web browsing in modern digital life, this
    category holds significant relevance. It holds a substantial representation in
    academic literature, with pioneering papers such as  [[20](#bib.bib20), [21](#bib.bib21)]
    proposing excellent GUI datasets for websites.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网站。该类别关注在网页浏览器中显现的GUI场景。考虑到现代数字生活中网页浏览的普及，这一类别具有重要的相关性。在学术文献中，它占有相当大的份额，如[[20](#bib.bib20),
    [21](#bib.bib21)]等开创性论文提出了优秀的网站GUI数据集。
- en: •
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: IOS. This category zeroes in on the GUI scenarios that transpire within the
    iOS operating system, the proprietary system for Apple devices like the iPhone
    and iPad. The iOS platform is known for its distinct design aesthetics and interaction
    patterns, providing a unique context for GUI research. A number of studies, such
    as  [[90](#bib.bib90), [91](#bib.bib91)] make use of GUI information in IOS.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: iOS。该类别关注在iOS操作系统中发生的GUI场景，这是Apple设备如iPhone和iPad的专有系统。iOS平台以其独特的设计美学和交互模式而闻名，为GUI研究提供了独特的背景。一些研究，如[[90](#bib.bib90),
    [91](#bib.bib91)]，利用了iOS中的GUI信息。
- en: •
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi Windows. This category is dedicated to GUI scenarios that necessitate
    simultaneous interaction with multiple windows, a common occurrence in desktop
    environments where users often juggle between several applications or documents.
    Despite the common use of multi-window interaction in everyday GUI usage, there
    has been relatively little research into this area [[92](#bib.bib92)]. The need
    for efficient multitasking in such scenarios presents unique challenges and opportunities
    for GUI design and interaction research. As of our knowledge, there are no specific
    datasets catering to these multi-window GUI scenarios.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多窗口。该类别专注于需要同时与多个窗口进行交互的GUI场景，这在桌面环境中很常见，用户通常在多个应用程序或文档之间切换。尽管在日常GUI使用中，多窗口交互非常普遍，但对这一领域的研究相对较少[[92](#bib.bib92)]。在这些场景中对高效多任务处理的需求为GUI设计和交互研究带来了独特的挑战和机会。据我们了解，目前没有专门针对这些多窗口GUI场景的数据集。
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: XR. XR encompasses Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
    (MR) [[93](#bib.bib93)]. Given the advancements in XR technology and the growing
    accessibility of commercial-grade head-mounted displays [[25](#bib.bib25), [94](#bib.bib94)],
    XR has emerged as a novel medium for human-computer interaction. This necessitates
    the exploration of GUI within XR environments. In these scenarios, the GUI takes
    on a 3D, immersive form [[95](#bib.bib95)], demanding the agent to comprehend
    and navigate a 3D space. The emerging field of XR presents a new frontier for
    GUI research, with unique challenges and opportunities due to its immersive and
    interactive nature. To date, as far as we are aware, there are no datasets that
    specifically address GUI in the realm of XR.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XR。XR包括虚拟现实（VR）、增强现实（AR）和混合现实（MR）[[93](#bib.bib93)]。鉴于XR技术的进步和商业级头戴显示器的日益普及[[25](#bib.bib25),
    [94](#bib.bib94)]，XR已经成为人机交互的新媒介。这就需要在XR环境中探索GUI。在这些场景中，GUI呈现为3D沉浸式形式[[95](#bib.bib95)]，要求用户理解和导航3D空间。XR这一新兴领域为GUI研究提供了新的前沿，由于其沉浸式和互动性，本领域存在独特的挑战和机遇。截至目前，据我们了解，没有专门针对XR领域的GUI的数据集。
- en: '![Refer to caption](img/9c263911dcf8700f8c74f05ba5ed4089.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9c263911dcf8700f8c74f05ba5ed4089.png)'
- en: 'Figure 9: List of desktop softwares in GUI-World.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：GUI-World中桌面软件的列表。
- en: '![Refer to caption](img/11ab7198cba4a624ba42f5fc4e36b6af.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11ab7198cba4a624ba42f5fc4e36b6af.png)'
- en: 'Figure 10: List of some websites in GUI-World.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：GUI-World中一些网站的列表。
- en: A.2 Selected Website/Software
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2 选定的网站/软件
- en: In our study, we selected a diverse range of websites and software to comprehensively
    evaluate GUI understanding capabilities across various user scenarios. These selections
    cover essential categories such as social media, productivity tools, online shopping,
    and educational platforms, providing a broad spectrum of GUI environments.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，我们选择了各种网站和软件，以全面评估不同用户场景下的GUI理解能力。这些选择涵盖了诸如社交媒体、生产力工具、在线购物和教育平台等重要类别，提供了广泛的GUI环境。
- en: 'The chosen websites, as shown in [Figure 9](#A1.F9 "Figure 9 ‣ A.1 Six Main
    GUI Categories ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"), include
    popular social media platforms like Instagram, Twitter, and LinkedIn, which are
    integral to understanding dynamic and interactive GUI elements. We also included
    widely-used productivity tools such as Microsoft Teams, Notion, and Slack to evaluate
    GUI tasks in professional and collaborative settings.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '选择的网站，如[图 9](#A1.F9 "Figure 9 ‣ A.1 Six Main GUI Categories ‣ Appendix A Details
    of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")中所示，包括了如Instagram、Twitter和LinkedIn等流行的社交媒体平台，这些平台对于理解动态和互动的GUI元素至关重要。我们还包括了像Microsoft
    Teams、Notion和Slack等广泛使用的生产力工具，以评估专业和协作环境中的GUI任务。'
- en: 'For software shown in [Figure 10](#A1.F10 "Figure 10 ‣ A.1 Six Main GUI Categories
    ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A
    Dataset for GUI-oriented Multimodal LLM-based Agents"), we incorporated key applications
    like Adobe Photoshop and MATLAB to assess GUI operations in specialized and technical
    environments. Additionally, video conferencing tools like Zoom and cloud storage
    services like Google Drive were included to represent common remote work and file
    management scenarios.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '对于[图 10](#A1.F10 "Figure 10 ‣ A.1 Six Main GUI Categories ‣ Appendix A Details
    of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents")中展示的软件，我们纳入了如Adobe Photoshop和MATLAB等关键应用，以评估专业和技术环境中的GUI操作。此外，还包括了像Zoom这样的视讯会议工具和像Google
    Drive这样的云存储服务，以代表常见的远程工作和文件管理场景。'
- en: These selections ensure that our study encompasses a wide array of user interactions
    and GUI complexities, thereby providing a robust evaluation of the current state-of-the-art
    methods in GUI understanding by MLLMs and comprehensively constructing a high-quality
    dataset.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选择确保我们的研究涵盖了广泛的用户互动和图形用户界面（GUI）复杂性，从而提供了对当前最先进的GUI理解方法的强有力评估，并全面构建了一个高质量的数据集。
- en: A.3 Human Keyframes Annotation Process
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3 人类关键帧标注过程
- en: Annotator’s Information
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标注者信息
- en: The annotation is conducted by 16 authors of this paper and 8 volunteers independently.
    As acknowledged, the diversity of annotators plays a crucial role in reducing
    bias and enhancing the reliability of the benchmark. These annotators have knowledge
    in the GUI domain, with different genders, ages, and educational backgrounds.
    The education backgrounds of annotators are above undergraduate. To ensure the
    annotators can proficiently mark the data, we provide them with detailed tutorials,
    teaching them how to use software to record videos or edit video clips. We also
    provide them with detailed criteria and task requirements in each annotation process.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 注释由本文的16位作者和8位志愿者独立完成。如所述，标注者的多样性在减少偏见和提高基准的可靠性方面发挥了关键作用。这些标注者在GUI领域具有知识，涵盖不同的性别、年龄和教育背景。标注者的教育背景都在本科以上。为了确保标注者能够熟练地标记数据，我们提供了详细的教程，教他们如何使用软件录制视频或编辑视频片段。我们还在每个标注过程中提供了详细的标准和任务要求。
- en: Recording Video.
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 录制视频。
- en: For self-recording videos, we employ OBS³³3[https://obsproject.com/](https://obsproject.com/)
    on the Windows system for screen capturing and the official screen recording toolkit
    on the Mac/IOS system. This process necessitates human labelers to execute a series
    of targeted actions within specific websites or applications, which are subsequently
    captured as raw video footage. These actions, commonplace in everyday usage, enhance
    the reliability of our dataset. Subsequently, the raw videos are segmented into
    sub-videos, each encapsulating multiple actions (e.g., clicking a button) to achieve
    a specific objective (e.g., image search). The videos are then processed to extract
    keyframes annotated with detailed descriptions.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自录制视频，我们在 Windows 系统上使用 OBS³³3[https://obsproject.com/](https://obsproject.com/)
    进行屏幕捕捉，而在 Mac/IOS 系统上使用官方屏幕录制工具包。此过程需要人工标注员在特定网站或应用程序中执行一系列有针对性的操作，然后将这些操作捕捉为原始视频素材。这些操作在日常使用中很常见，提高了我们数据集的可靠性。随后，原始视频被分割成子视频，每个子视频包含多个操作（例如，点击按钮），以实现特定目标（例如，图像搜索）。然后，对视频进行处理，以提取带有详细描述的关键帧。
- en: Edition Based on YouTube Videos.
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于 YouTube 视频的编辑。
- en: For sourcing videos from YouTube, we utilize a search protocol formatted as
    "[website name/application name] + tutorial" to compile relevant video lists.
    Human labelers first review these videos to understand the primary operations
    they depict. These videos are then divided into sub-videos, each containing several
    actions directed towards a single goal (e.g., image search). Like the self-recorded
    footage, these segments are processed to isolate keyframes and furnish them with
    descriptive annotations.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从 YouTube 获取视频，我们利用格式为“[网站名称/应用名称] + 教程”的搜索协议来编制相关视频列表。人工标注员首先查看这些视频以了解它们展示的主要操作。然后，这些视频被分割成子视频，每个子视频包含若干个旨在实现单一目标（例如，图像搜索）的操作。与自录制的视频类似，这些片段会被处理以提取关键帧并提供描述性注释。
- en: Keyframes Annotation.
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关键帧注释。
- en: 'After obtaining the GUI video clips, human annotators will filter out the keyframes
    of the operations based on the video content and the mouse and keyboard actions
    at that time. They will also label the sub-operations or targets between the two
    keyframes. Once the annotation is complete, the annotators will provide an overall
    description of the entire video, summarizing the main goal of the human operations
    in the video. After all the information is annotated, we will use a Large Language
    Model (LLM) to refine the text content, reducing any errors made by human annotators
    and adjusting the sentence structure. The prompt we use for the LLM to polish
    the human annotations is shown in [Figure 11](#A1.F11 "Figure 11 ‣ Human verifying
    GPT-4V annotated captions. ‣ A.3 Human Keyframes Annotation Process ‣ Appendix
    A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") and [Figure 12](#A1.F12 "Figure 12
    ‣ Human verifying GPT-4V annotated captions. ‣ A.3 Human Keyframes Annotation
    Process ‣ Appendix A Details of Dataset Construction ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取 GUI 视频片段后，人工标注员将根据视频内容和当时的鼠标和键盘操作筛选出操作的关键帧。他们还会标注两个关键帧之间的子操作或目标。注释完成后，标注员将对整个视频提供整体描述，总结视频中人类操作的主要目标。在所有信息标注完成后，我们将使用大型语言模型（LLM）来润色文本内容，减少人工标注员的错误并调整句子结构。我们用来润色人工标注的
    LLM 提示如[图 11](#A1.F11 "图 11 ‣ 人工验证 GPT-4V 注释的标题。 ‣ A.3 人工关键帧注释过程 ‣ 附录 A 数据集构建详细信息
    ‣ 第 I 部分附录 ‣ GUI-World：用于 GUI 方向的多模态 LLM 基于代理的数据集")和[图 12](#A1.F12 "图 12 ‣ 人工验证
    GPT-4V 注释的标题。 ‣ A.3 人工关键帧注释过程 ‣ 附录 A 数据集构建详细信息 ‣ 第 I 部分附录 ‣ GUI-World：用于 GUI 方向的多模态
    LLM 基于代理的数据集")中展示。
- en: Human-LLM Cooperated Instruction Generation.
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工-LLM 协作指令生成。
- en: 'To curate and refine the golden answer of each video-instruction pair generated
    by GPT-4V, given that the raw response from GPT-4V may contain harmful content
    or hallucinations. The role of humans in the golden answer generation process
    is to enhance the difficulty of the questions and remove harmful and incorrect
    content, as shown in [Table 8](#A1.T8 "Table 8 ‣ Human-LLM Cooperated Instruction
    Generation. ‣ A.3 Human Keyframes Annotation Process ‣ Appendix A Details of Dataset
    Construction ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents").'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '为了策划和完善由 GPT-4V 生成的每个视频-指令对的黄金答案，因为 GPT-4V 的原始响应可能包含有害内容或幻觉。人类在黄金答案生成过程中的角色是提升问题的难度，并去除有害和不正确的内容，如
    [表 8](#A1.T8 "表 8 ‣ 人工-LLM 协作指令生成。 ‣ A.3 人工关键帧标注过程 ‣ 附录 A 数据集构建细节 ‣ 第一部分附录 ‣ GUI-World:
    面向 GUI 的多模态 LLM 代理数据集") 所示。'
- en: 'Table 8: Comparison of raw answer from GPT-4V and golden answer from humans.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: GPT-4V 原始回答与人工黄金答案的比较。'
- en: '| Raw response from GPT-4V | Golden answer from humans |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V 的原始响应 | 人工黄金答案 |'
- en: '| --- | --- |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| "Q": "What subcategories are visible under the ’Lifestyle’ section in the
    static GUI content of the video?" | "Q": "What subcategories are visible under
    the ’Lifestyle’ section in the static GUI content of the video?" |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| "Q": "在视频的静态 GUI 内容中，’Lifestyle’部分下可见哪些子类别？" | "Q": "在视频的静态 GUI 内容中，’Lifestyle’部分下可见哪些子类别？"
    |'
- en: '| "A": "In the ’Lifestyle’ section of the telegram.com website, the visible
    subcategories are ’Worcester Living’, ’Faith’, and ’Food’. These are clearly displayed
    in a horizontal list just below the main ’Lifestyle’ category header, indicating
    different areas of content that users can explore within the lifestyle section."
    | "A": "The ’Lifestyle’ section of the telegram.com website features visible subcategories
    such as ’Worcester Living’, ’Faith’, and ’Food’. These are prominently shown in
    a horizontal list beneath the main ’Lifestyle’ header, offering users various
    content areas to explore within the section." |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| "A": "在 telegram.com 网站的’Lifestyle’部分，可见的子类别有’Worcester Living’，’Faith’和’Food’。这些子类别清晰地显示在主’Lifestyle’类别标题下方的横向列表中，表示用户可以在生活方式部分探索的不同内容领域。"
    | "A": "telegram.com 网站的’Lifestyle’部分包括可见的子类别，如’Worcester Living’，’Faith’，和’Food’。这些子类别显著地展示在主’Lifestyle’标题下方的横向列表中，为用户提供了该部分内各种内容领域的探索。"
    |'
- en: Human verifying GPT-4V annotated captions.
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工验证 GPT-4V 标注的标题。
- en: We evaluated the quality of annotations from GPT-4V by selecting 1,000 detailed
    descriptions and captions generated by GPT-4V, which were then assessed by human
    annotators. The high satisfaction rate of 98% underscores the quality and relevance
    of the GPT-4V annotations.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过选择 1,000 个 GPT-4V 生成的详细描述和标题，并由人工标注员评估，从而评估 GPT-4V 标注的质量。98% 的高满意度突显了 GPT-4V
    标注的质量和相关性。
- en: '![Refer to caption](img/8de024ec2dec0c12a84761acf5e978d7.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8de024ec2dec0c12a84761acf5e978d7.png)'
- en: 'Figure 11: The overall preview of our annotating software.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 我们标注软件的整体预览。'
- en: '![Refer to caption](img/532520792e0ec5e986fb406edc963703.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/532520792e0ec5e986fb406edc963703.png)'
- en: 'Figure 12: The interface for annotating a keyframe, consists of mouse action,
    keyboard action, and a short sub-action purpose.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 标注关键帧的界面，包括鼠标操作、键盘操作和一个简短的子操作目的。'
- en: Appendix B Dataset Analysis
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 数据集分析
- en: 'In this section, we provide an analysis of the length distribution of QA in
    each GUI scenario, as illustrated in [Figure 13](#A2.F13 "Figure 13 ‣ Appendix
    B Dataset Analysis ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents") and [Figure 14](#A2.F14 "Figure 14 ‣ Appendix B Dataset Analysis
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"). Questions focused on sequential and predictional tasks are slightly
    longer than other types, while the golden answer of static tasks tends to be longer.
    Length of Question-answer pair in various GUI scenarios is similarly distributed,
    with questions in Android environment being slightly shorter, and answers in XR
    environment being longer.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了每个 GUI 场景中 QA 长度分布的分析，如 [图 13](#A2.F13 "图 13 ‣ 附录 B 数据集分析 ‣ 第一部分附录
    ‣ GUI-World: 面向 GUI 的多模态 LLM 代理数据集") 和 [图 14](#A2.F14 "图 14 ‣ 附录 B 数据集分析 ‣ 第一部分附录
    ‣ GUI-World: 面向 GUI 的多模态 LLM 代理数据集") 所示。关注顺序和预测任务的问题比其他类型稍长，而静态任务的黄金答案倾向于较长。不同
    GUI 场景中的问答对长度分布相似，Android 环境中的问题略短，而 XR 环境中的答案较长。'
- en: '![Refer to caption](img/cea5954178608047050fd6fe6a913c6e.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cea5954178608047050fd6fe6a913c6e.png)'
- en: 'Figure 13: Length distribution of free-form questions.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 自由形式问题的长度分布。'
- en: '![Refer to caption](img/fb6b4c2eada5170936becd2eec14c324.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fb6b4c2eada5170936becd2eec14c324.png)'
- en: 'Figure 14: Length distribution of answers to free-form questions.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 自由形式问题的回答长度分布。'
- en: '![Refer to caption](img/5bd08a4c1dcf1f2d5549b29dc077578b.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5bd08a4c1dcf1f2d5549b29dc077578b.png)'
- en: 'Figure 15: Statistic of different GUI scenarios in GUI-World.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15: GUI-World 中不同 GUI 场景的统计数据。'
- en: Appendix C Details of Experiments Setups
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 C 实验设置详细信息
- en: C.1 Finetune dataset construction
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1 微调数据集构建
- en: 'We use two settings to finetune GUI-Vid, one with video-text pairs only, and
    the other with video-text and image-text pairs, which are all GUI content:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两种设置来微调 GUI-Vid，一种仅使用视频-文本对，另一种使用视频-文本对和图像-文本对，这些都是 GUI 内容：
- en: •
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Video Only. In this setting, we only trained GUI-Vid with video-text pairs
    in GUI-World, as shown in [Table 9](#A3.T9 "Table 9 ‣ C.1 Finetune dataset construction
    ‣ Appendix C Details of Experiments Setups ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '仅视频。在此设置中，我们仅使用 GUI-World 中的视频-文本对训练 GUI-Vid，如 [表 9](#A3.T9 "表 9 ‣ C.1 微调数据集构建
    ‣ 附录 C 实验设置详细信息 ‣ 第一部分附录 ‣ GUI-World: GUI 导向的多模态 LLM 代理的数据集") 所示。'
- en: •
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Video-Image. Inspired by the pre-trained process of Videochat2, we include image-text
    pairs to help the visual encoder align GUI knowledge. These images are selected
    from our GUI-World, MetaGUI [[14](#bib.bib14)], and OmniAct [[21](#bib.bib21)]
    for high-quality GUI content. Subsequently, we use GPT-4V to generate a detailed
    description and a concise caption for each image. Finally, we construct a dataset
    consisting of video-text and image-text pairs for gaining comprehensive GUI-oriented
    capabilities.
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频-图像。受 Videochat2 预训练过程的启发，我们加入了图像-文本对以帮助视觉编码器对齐 GUI 知识。这些图像从我们的 GUI-World、MetaGUI
    [[14](#bib.bib14)] 和 OmniAct [[21](#bib.bib21)] 中选取，确保 GUI 内容的高质量。随后，我们使用 GPT-4V
    为每张图像生成详细描述和简洁标题。最后，我们构建了一个包含视频-文本对和图像-文本对的数据集，以获得全面的 GUI 导向能力。
- en: 'Table 9: Video-only finetune dataset.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 仅视频微调数据集。'
- en: '| Stage | Data types | Amount |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | 数据类型 | 数量 |'
- en: '| --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Detailed Description | 14,276 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 详细描述 | 14,276 |'
- en: '| Concise Caption | 7,138 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 7,138 |'
- en: '| 2 | GUI VQA | 21,414 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 2 | GUI VQA | 21,414 |'
- en: '| Multiple-Choice QA | 14,276 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 多项选择 QA | 14,276 |'
- en: '| Conversation | 7,138 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | 7,138 |'
- en: 'Table 10: Video-image finetune dataset.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 视频-图像微调数据集。'
- en: '| Stage | Data types | Source | Type | Amount |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | 数据类型 | 来源 | 类型 | 数量 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | GUI-World | Video | Detailed Description | 14,276 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 1 | GUI-World | 视频 | 详细描述 | 14,276 |'
- en: '| Concise Caption | 7,138 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 7,138 |'
- en: '| Image | Detailed Description | 5,555 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 图片 | 详细描述 | 5,555 |'
- en: '| Concise Caption | 5,555 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 5,555 |'
- en: '| MetaGUI | Image | Detailed Description | 19,626 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| MetaGUI | 图像 | 详细描述 | 19,626 |'
- en: '| Concise Caption | 19,626 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 19,626 |'
- en: '| OmniAct | Detailed Description | 260 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| OmniAct | 详细描述 | 260 |'
- en: '| Concise Caption | 260 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 简洁标题 | 260 |'
- en: '| 2 | GUI-World | Video | GUI VQA | 21,414 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 2 | GUI-World | 视频 | GUI VQA | 21,414 |'
- en: '| Multiple-Choice QA | 14,276 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 多项选择 QA | 14,276 |'
- en: '| Conversation | 7,138 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | 7,138 |'
- en: C.2 Hyperparameter Settings
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2 超参数设置
- en: 'In this section, we will introduce the hyperparameters of MLLMs to facilitate
    experiment reproducibility and transparency. We divide them into three parts:
    the inference phase during benchmark and dataset construction, the LLM-as-a-Judge
    phase, and the fine-tuning phase. All our experiments were conducted on a server
    equipped with dual A800 and dual 4090 GPUs.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 MLLMs 的超参数，以便实验的可重复性和透明性。我们将其分为三个部分：基准和数据集构建期间的推理阶段，LLM 作为裁判的阶段，和微调阶段。我们所有的实验都在配备双
    A800 和双 4090 GPU 的服务器上进行。
- en: Inference.
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推理。
- en: 'We empirically study 7 MLLMs, involving 4 Image-LLMs and 3 Video-LLMs, with
    their hyperparameters detailed as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经验性地研究了 7 个 MLLMs，包括 4 个图像-LLMs 和 3 个视频-LLMs，其超参数详细信息如下：
- en: •
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GPT-4V [[1](#bib.bib1)] & GPT-4o [[40](#bib.bib40)]: We set the temperature
    and top-p as 0.9, max-token as 2048, and both all images input are set as high
    quality in Instruction Dataset Construction and benchmarking.'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GPT-4V [[1](#bib.bib1)] 和 GPT-4o [[40](#bib.bib40)]: 我们将温度和 top-p 设置为 0.9，最大令牌数为
    2048，所有图像输入在指令数据集构建和基准测试中都设置为高质量。'
- en: •
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gemini-Pro-1.5 [[42](#bib.bib42)]: We use the default settings, which set temperature
    as 0.4, top-p as 1, and max-token as 2048\. It should be noted that during our
    project, Gemini-Pro-1.5 is still under the user request limit, which only provides
    100 requests per day, making our benchmark difficult. Given that Gemini hasn’t
    launched Pay-as-you-go⁴⁴4[https://ai.google.dev/pricing](https://ai.google.dev/pricing),
    we will include benchmark results on ‘Human’ setting as soon as possible.'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Gemini-Pro-1.5 [[42](#bib.bib42)]：我们使用默认设置，温度设置为 0.4，top-p 设置为 1，max-token 设置为
    2048。需要注意的是，在我们的项目期间，Gemini-Pro-1.5 仍处于用户请求限制中，每天仅提供 100 次请求，这使得我们的基准测试困难重重。鉴于
    Gemini 尚未推出按需付费⁴⁴[https://ai.google.dev/pricing](https://ai.google.dev/pricing)，我们将尽快在“Human”设置下包含基准结果。
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Qwen-VL-Max [[41](#bib.bib41)]: We use the default settings for Qwen-VL-Max,
    with top-p as 0.8 and max-token as 2048\. Given that the input context window
    is merely 6,000 for Qwen, we scale the resolution for all images to 0.3.'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Qwen-VL-Max [[41](#bib.bib41)]：我们使用 Qwen-VL-Max 的默认设置，top-p 设置为 0.8，max-token
    设置为 2048。考虑到 Qwen 的输入上下文窗口仅为 6,000，我们将所有图像的分辨率缩放至 0.3。
- en: •
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ChatUnivi [[44](#bib.bib44)]: We use ChatUnivi-7B built upon Vicuna-v0-7B and
    set the max frame as 100, temperature as 0.2, and max-token as 1024.'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatUnivi [[44](#bib.bib44)]：我们使用基于 Vicuna-v0-7B 构建的 ChatUnivi-7B，并将最大帧数设置为
    100，温度设置为 0.2，max-token 设置为 1024。
- en: •
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Minigpt4video [[45](#bib.bib45)]: We use the suggested settings⁵⁵5[https://github.com/Vision-CAIR/MiniGPT4-video](https://github.com/Vision-CAIR/MiniGPT4-video)
    for this model and the max-frame are set as 45, with only the max-token being
    modified to 1024.'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Minigpt4video [[45](#bib.bib45)]：我们使用了建议的设置⁵⁵[https://github.com/Vision-CAIR/MiniGPT4-video](https://github.com/Vision-CAIR/MiniGPT4-video)，将最大帧数设置为
    45，仅将 max-token 修改为 1024。
- en: •
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'VideoChat2 & GUI-Vid [[46](#bib.bib46)]: For a fair comparison, we set the
    same hyperparameters for VideoChat2 & GUI-Vid. We set the max-token as 1024, top-p
    as 0.9, temperature as 1.0, max-frame as 8/16, repetition penalty as 1.2, and
    length penalty as 1.2.'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VideoChat2 和 GUI-Vid [[46](#bib.bib46)]：为了公平比较，我们为 VideoChat2 和 GUI-Vid 设置了相同的超参数。我们将
    max-token 设置为 1024，top-p 设置为 0.9，温度设置为 1.0，最大帧数设置为 8/16，重复惩罚设置为 1.2，长度惩罚设置为 1.2。
- en: LLM-as-a-Judge.
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM-as-a-Judge。
- en: 'We studied four LLM-as-a-Judge in giving a similarity score for the MLLM’s
    response and ground truth, namely GPT-4 [[52](#bib.bib52)], ChatGPT [[96](#bib.bib96)],
    LLaMA-3-70b-instruct [[54](#bib.bib54)], and Mixtral-8x22b-instruct-v0.1 [[55](#bib.bib55)].
    Hyperparameter settings are detailed as follows:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了四个 LLM-as-a-Judge，在给 MLLM 的响应和真实值评分时，分别是 GPT-4 [[52](#bib.bib52)]、ChatGPT
    [[96](#bib.bib96)]、LLaMA-3-70b-instruct [[54](#bib.bib54)] 和 Mixtral-8x22b-instruct-v0.1
    [[55](#bib.bib55)]。超参数设置详细如下：
- en: •
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-4 & ChatGPT. We set the temperature as 0.6 and others as default.
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4 和 ChatGPT。我们将温度设置为 0.6，其余设置为默认。
- en: •
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLaMA-3-70b-instruct. We set the temperature as 0.6, top-p as 0.9, top-k as
    50.
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaMA-3-70b-instruct。我们将温度设置为 0.6，top-p 设置为 0.9，top-k 设置为 50。
- en: •
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixtral-8x22b-instruct-v0.1. We set top-p as 0.7, top-k as 50, and temperature
    as 0.7.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mixtral-8x22b-instruct-v0.1。我们将 top-p 设置为 0.7，top-k 设置为 50，温度设置为 0.7。
- en: Finetune.
  id: totrans-403
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Finetune。
- en: 'We include several hyperparameter settings in experiment settings and ablation
    studies, as shown in [Table 11](#A3.T11 "Table 11 ‣ Finetune. ‣ C.2 Hyperparameter
    Settings ‣ Appendix C Details of Experiments Setups ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents").'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在实验设置和消融研究中包含了几个超参数设置，如[表 11](#A3.T11 "Table 11 ‣ Finetune. ‣ C.2 Hyperparameter
    Settings ‣ Appendix C Details of Experiments Setups ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents")所示。'
- en: 'Table 11: Configuration settings for fine-tuning.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：微调的配置设置。
- en: '| Config | Setting |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 设置 |'
- en: '| --- | --- |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| input frame | 8 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 输入帧数 | 8 |'
- en: '| input resolution | 224 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 输入分辨率 | 224 |'
- en: '| max text length | 512 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 最大文本长度 | 512 |'
- en: '| input modal | I. + V. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 输入模态 | I. + V. |'
- en: '| optimizer | AdamW |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW |'
- en: '| optimizer momentum | $\beta_{1},\beta_{2}=0.9,0.999$ |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 优化器动量 | $\beta_{1},\beta_{2}=0.9,0.999$ |'
- en: '| weight decay | 0.02 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.02 |'
- en: '| learning rate schedule | cosine decay |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 余弦衰减 |'
- en: '| learning rate | 2e-5 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 2e-5 |'
- en: '| batch size | 4 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 4 |'
- en: '| warmup epochs | 0.6 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 预热周期 | 0.6 |'
- en: '| total epochs | 3 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 总周期数 | 3 |'
- en: '| backbone drop path | 0 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 主干丢弃路径 | 0 |'
- en: '| QFormer drop path | 0.1 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| QFormer 丢弃路径 | 0.1 |'
- en: '| QFormer dropout | 0.1 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| QFormer 丢弃率 | 0.1 |'
- en: '| QFormer token | 96 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| QFormer token | 96 |'
- en: '| flip augmentation | yes |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 翻转增强 | 是 |'
- en: '| augmentation | MultiScaleCrop [0.5, 1] |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 增强 | MultiScaleCrop [0.5, 1] |'
- en: 'Table 12: Evaluating LLM-as-a-Judge as a replacement for human judging in the
    scoring setting.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：评估 LLM-as-a-Judge 作为替代人类评判的评分设置。
- en: '| Models | Pearson($\uparrow$) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pearson($\uparrow$) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-4 | 0.856 | 0.853 | 0.793 | $120$$ |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 0.856 | 0.853 | 0.793 | $120$$ |'
- en: '| ChatGPT | 0.706 | 0.714 | 0.627 | 12$ |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 0.706 | 0.714 | 0.627 | 12$ |'
- en: '| Llama-3-70b-instruct | 0.774 | 0.772 | 0.684 | 12$ |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70b-instruct | 0.774 | 0.772 | 0.684 | 12$ |'
- en: '| Mixtral-8x22b-instruct-v0.1 | 0.759 | 0.760 | 0.670 | $15$$ |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x22b-instruct-v0.1 | 0.759 | 0.760 | 0.670 | $15$$ |'
- en: C.3 Evaluation.
  id: totrans-433
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.3 评估。
- en: 'Given the complexity of free-form answers in GUI scenarios, the evaluation
    includes specific positions of GUI elements, textual content, and comparing the
    response to the golden answer. LLM-as-a-judge has been widely used in previous
    studies for complex evaluation tasks [[47](#bib.bib47), [48](#bib.bib48)]. Therefore,
    we leverage LLM-as-a-Judge [[47](#bib.bib47)] in a similar setting to MM-vet [[60](#bib.bib60)],
    which compares the MLLM’s response to the golden answer. We carefully evaluate
    the accessibility of leveraging LLM-as-a-Judge, selecting 1,000 samples covering
    6 free-form questions mentioned in our dataset. As shown in [Table 12](#A3.T12
    "Table 12 ‣ Finetune. ‣ C.2 Hyperparameter Settings ‣ Appendix C Details of Experiments
    Setups ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents"), GPT-4 outperforms other LLMs, exhibiting a better human alignment on
    providing a similarity score for the response compared to the golden answer, although
    it is approximately 10 times more expensive than other models.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于 GUI 场景中自由形式回答的复杂性，评估包括 GUI 元素的特定位置、文本内容，并将响应与黄金答案进行比较。LLM 作为评判者在之前的研究中已被广泛用于复杂评估任务
    [[47](#bib.bib47), [48](#bib.bib48)]。因此，我们在类似于 MM-vet [[60](#bib.bib60)] 的设置中利用
    LLM 作为评判者 [[47](#bib.bib47)]，比较 MLLM 的响应与黄金答案。我们仔细评估了利用 LLM 作为评判者的可及性，选择了 1,000
    个样本，涵盖了数据集中提到的 6 个自由形式问题。如 [表 12](#A3.T12 "Table 12 ‣ Finetune. ‣ C.2 Hyperparameter
    Settings ‣ Appendix C Details of Experiments Setups ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") 所示，GPT-4 在为响应提供与黄金答案的相似度评分方面优于其他
    LLM，尽管其成本约为其他模型的 10 倍。'
- en: Appendix D Additional Experiments Results
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 额外实验结果
- en: 'In this section, we provide detailed results on each task in each GUI scenario.
    For captioning tasks, [Table 13](#A4.T13 "Table 13 ‣ Appendix D Additional Experiments
    Results ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") shows comprehensive experimental results among six scenarios. For scores
    of LLM-as-a-Judge in a specific task, see [Table 14](#A4.T14 "Table 14 ‣ Appendix
    D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents"), [Table 15](#A4.T15 "Table 15 ‣ Appendix
    D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents"), [Table 16](#A4.T16 "Table 16 ‣ Appendix
    D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents"), [Table 17](#A4.T17 "Table 17 ‣ Appendix
    D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents"), and [Table 18](#A4.T18 "Table 18 ‣
    Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World: A Dataset
    for GUI-oriented Multimodal LLM-based Agents"). For BLEU [[50](#bib.bib50)] and
    BERTScore [[51](#bib.bib51)] in validating free-form and conversational questions,
    see [Table 19](#A4.T19 "Table 19 ‣ Appendix D Additional Experiments Results ‣
    Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    [Table 20](#A4.T20 "Table 20 ‣ Appendix D Additional Experiments Results ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    [Table 21](#A4.T21 "Table 21 ‣ Appendix D Additional Experiments Results ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    [Table 24](#A4.T24 "Table 24 ‣ Appendix D Additional Experiments Results ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    [Table 22](#A4.T22 "Table 22 ‣ Appendix D Additional Experiments Results ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    and [Table 23](#A4.T23 "Table 23 ‣ Appendix D Additional Experiments Results ‣
    Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents").
    For performance in fine-grain (application level), see [Figure 16](#A4.F16 "Figure
    16 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") for Gemini-Pro and [Figure 17](#A4.F17
    "Figure 17 ‣ Appendix D Additional Experiments Results ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") for Qwen-VL-Max.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了每个 GUI 场景中每项任务的详细结果。对于标题任务，[表 13](#A4.T13 "表 13 ‣ 附录 D 附加实验结果 ‣ 第一部分
    附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 基础数据集") 显示了六种场景中的综合实验结果。有关特定任务中 LLM-as-a-Judge
    的得分，请参见 [表 14](#A4.T14 "表 14 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对 GUI 的多模态
    LLM 基础数据集")、[表 15](#A4.T15 "表 15 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对 GUI
    的多模态 LLM 基础数据集")、[表 16](#A4.T16 "表 16 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")、[表 17](#A4.T17 "表 17 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集") 和 [表 18](#A4.T18 "表 18 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")。对于 BLEU [[50](#bib.bib50)] 和 BERTScore [[51](#bib.bib51)]
    在验证自由格式和对话性问题中的表现，请参见 [表 19](#A4.T19 "表 19 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")、[表 20](#A4.T20 "表 20 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")、[表 21](#A4.T21 "表 21 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")、[表 24](#A4.T24 "表 24 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")、[表 22](#A4.T22 "表 22 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集") 和 [表 23](#A4.T23 "表 23 ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 基础数据集")。对于细粒度（应用级）性能，请参见 [图 16](#A4.F16 "图 16 ‣ 附录 D 附加实验结果 ‣ 第一部分
    附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 基础数据集") 中的 Gemini-Pro 和 [图 17](#A4.F17 "图 17
    ‣ 附录 D 附加实验结果 ‣ 第一部分 附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 基础数据集") 中的 Qwen-VL-Max。
- en: 'Table 13: Scores of Caption (Cap.) and Description (Des.) tasks in six GUI
    scenarios.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：六种 GUI 场景中标题（Cap.）和描述（Des.）任务的得分。
- en: '| Models | Setting | Software | Website | XR | Multi | IOS | Android | Avg.
    |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 软件 | 网站 | XR | 多模态 | IOS | 安卓 | 平均 |'
- en: '| Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap. | Des. | Cap.
    | Des. | Cap. | Des. |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 描述 | 标题 | 描述 | 标题 | 描述 | 标题 | 描述 | 标题 | 描述 | 标题 | 描述 | 标题 | 描述 |'
- en: '| Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 3.613 | 2.860 | 2.995 | 2.590 | 3.276
    | 2.470 | 3.678 | 2.936 | - | - | 3.444 | 2.739 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 3.659 | 2.837 | 3.613 | 2.860 | 2.995 | 2.590 | 3.276
    | 2.470 | 3.678 | 2.936 | - | - | 3.444 | 2.739 |'
- en: '| E. | 3.350 | 2.468 | 3.159 | 2.422 | 2.837 | 2.279 | 2.824 | 2.109 | 3.394
    | 2.519 | 3.185 | 2.312 | 3.125 | 2.351 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.350 | 2.468 | 3.159 | 2.422 | 2.837 | 2.279 | 2.824 | 2.109 | 3.394
    | 2.519 | 3.185 | 2.312 | 3.125 | 2.351 |'
- en: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.326 | 1.681 | 2.172 | 1.772 | 2.035
    | 1.463 | 2.513 | 1.662 | 2.141 | 1.565 | 2.261 | 1.650 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.381 | 1.758 | 2.326 | 1.681 | 2.172 | 1.772 | 2.035
    | 1.463 | 2.513 | 1.662 | 2.141 | 1.565 | 2.261 | 1.650 |'
- en: '| E. | 2.459 | 1.693 | 2.317 | 1.599 | 2.167 | 1.638 | 2.190 | 1.438 | 2.189
    | 1.615 | 2.002 | 1.429 | 2.221 | 1.569 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.459 | 1.693 | 2.317 | 1.599 | 2.167 | 1.638 | 2.190 | 1.438 | 2.189
    | 1.615 | 2.002 | 1.429 | 2.221 | 1.569 |'
- en: '| H. | 2.474 | 1.711 | 2.457 | 1.698 | 2.383 | 1.777 | 1.910 | 1.346 | 2.577
    | 1.795 | 2.474 | 1.711 | 2.360 | 1.665 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.474 | 1.711 | 2.457 | 1.698 | 2.383 | 1.777 | 1.910 | 1.346 | 2.577
    | 1.795 | 2.474 | 1.711 | 2.360 | 1.665 |'
- en: '| GPT-4V | R. | 3.579 | 2.676 | 3.612 | 2.699 | 2.975 | 2.525 | 3.281 | 2.661
    | 3.757 | 2.775 | 3.655 | 2.755 | 3.479 | 2.682 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.579 | 2.676 | 3.612 | 2.699 | 2.975 | 2.525 | 3.281 | 2.661
    | 3.757 | 2.775 | 3.655 | 2.755 | 3.479 | 2.682 |'
- en: '| E. | 3.141 | 2.301 | 3.293 | 2.380 | 2.471 | 2.085 | 3.063 | 2.324 | 3.624
    | 2.611 | 3.201 | 2.312 | 3.132 | 2.335 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.141 | 2.301 | 3.293 | 2.380 | 2.471 | 2.085 | 3.063 | 2.324 | 3.624
    | 2.611 | 3.201 | 2.312 | 3.132 | 2.335 |'
- en: '| H. | 3.352 | 2.509 | 3.702 | 2.750 | 3.050 | 3.556 | 3.524 | 2.673 | 3.670
    | 2.588 | - | - | 3.460 | 2.614 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.352 | 2.509 | 3.702 | 2.750 | 3.050 | 3.556 | 3.524 | 2.673 | 3.670
    | 2.588 | - | - | 3.460 | 2.614 |'
- en: '| GPT-4o | H. | 4.048 | 3.028 | 4.067 | 3.233 | 3.398 | 2.729 | 3.869 | 3.111
    | 4.014 | 2.993 | 4.071 | 3.095 | 3.911 | 3.869 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 4.048 | 3.028 | 4.067 | 3.233 | 3.398 | 2.729 | 3.869 | 3.111
    | 4.014 | 2.993 | 4.071 | 3.095 | 3.911 | 3.869 |'
- en: '| ChatUnivi | - | 1.587 | 1.240 | 1.569 | 1.254 | 1.417 | 1.148 | 1.575 | 1.267
    | 1.480 | 1.146 | 1.778 | 1.249 | 1.568 | 1.217 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.587 | 1.240 | 1.569 | 1.254 | 1.417 | 1.148 | 1.575 | 1.267
    | 1.480 | 1.146 | 1.778 | 1.249 | 1.568 | 1.217 |'
- en: '| Minigpt4Video | - | 1.246 | 1.073 | 1.200 | 1.057 | 1.320 | 1.106 | 1.130
    | 1.034 | 1.190 | 1.076 | 1.184 | 1.061 | 1.212 | 1.068 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.246 | 1.073 | 1.200 | 1.057 | 1.320 | 1.106 | 1.130
    | 1.034 | 1.190 | 1.076 | 1.184 | 1.061 | 1.212 | 1.068 |'
- en: '| VideoChat2 | - | 1.992 | 1.312 | 1.817 | 1.307 | 1.838 | 1.426 | 2.222 |
    1.433 | 2.169 | 1.270 | 2.119 | 1.294 | 1.900 | 1.340 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.992 | 1.312 | 1.817 | 1.307 | 1.838 | 1.426 | 2.222 |
    1.433 | 2.169 | 1.270 | 2.119 | 1.294 | 1.900 | 1.340 |'
- en: '| GUI-Vid | - | 3.562 | 2.085 | 3.655 | 2.167 | 3.747 | 2.153 | 3.370 | 1.742
    | 3.566 | 2.071 | 2.662 | 1.248 | 3.427 | 1.911 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 3.562 | 2.085 | 3.655 | 2.167 | 3.747 | 2.153 | 3.370 | 1.742
    | 3.566 | 2.071 | 2.662 | 1.248 | 3.427 | 1.911 |'
- en: 'Table 14: Detailed scores for each tasks in Website scenarios.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 表14：网站场景中各任务的详细得分。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 3.279 | 3.050 | 3.560 | 3.579 | 3.796 | 3.452 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 3.279 | 3.050 | 3.560 | 3.579 | 3.796 | 3.452 |'
- en: '| E. | 2.983 | 2.491 | 3.432 | 3.405 | 3.760 | 3.215 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.983 | 2.491 | 3.432 | 3.405 | 3.760 | 3.215 |'
- en: '| Qwen-VL-Max | R. | 2.317 | 2.271 | 2.802 | 2.995 | 3.069 | 2.656 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.317 | 2.271 | 2.802 | 2.995 | 3.069 | 2.656 |'
- en: '| E. | 2.256 | 2.198 | 2.821 | 2.861 | 3.144 | 2.627 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.256 | 2.198 | 2.821 | 2.861 | 3.144 | 2.627 |'
- en: '| H. | 2.308 | 2.078 | 2.832 | 3.061 | 3.358 | 2.698 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.308 | 2.078 | 2.832 | 3.061 | 3.358 | 2.698 |'
- en: '| GPT-4V | R. | 3.461 | 3.214 | 3.754 | 3.778 | 4.029 | 3.648 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.461 | 3.214 | 3.754 | 3.778 | 4.029 | 3.648 |'
- en: '| E. | 3.197 | 2.808 | 3.487 | 3.717 | 3.954 | 3.433 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.197 | 2.808 | 3.487 | 3.717 | 3.954 | 3.433 |'
- en: '| H. | 3.498 | 3.255 | 3.727 | 3.731 | 4.061 | 3.655 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.498 | 3.255 | 3.727 | 3.731 | 4.061 | 3.655 |'
- en: '| C.C. | 1.746 | 2.738 | 3.645 | 3.363 | 3.632 | 3.025 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 1.746 | 2.738 | 3.645 | 3.363 | 3.632 | 3.025 |'
- en: '| D.C. | 2.704 | 2.917 | 3.686 | 3.680 | 3.901 | 3.380 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.704 | 2.917 | 3.686 | 3.680 | 3.901 | 3.380 |'
- en: '| H.+D.C. | 3.313 | 3.221 | 3.852 | 3.850 | 4.171 | 3.682 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.313 | 3.221 | 3.852 | 3.850 | 4.171 | 3.682 |'
- en: '| GPT-4o | H. | 3.443 | 3.373 | 3.672 | 4.086 | 4.122 | 3.740 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.443 | 3.373 | 3.672 | 4.086 | 4.122 | 3.740 |'
- en: '| ChatUnivi | - | 1.701 | 1.668 | 2.524 | 2.514 | 3.338 | 2.349 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.701 | 1.668 | 2.524 | 2.514 | 3.338 | 2.349 |'
- en: '| Minigpt4Video | - | 1.309 | 1.233 | 1.766 | 1.439 | 1.854 | 1.520 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.309 | 1.233 | 1.766 | 1.439 | 1.854 | 1.520 |'
- en: '| VideoChat2 | - | 1.771 | 1.777 | 2.288 | 2.461 | 2.812 | 2.221 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.771 | 1.777 | 2.288 | 2.461 | 2.812 | 2.221 |'
- en: '| GUI-Vid | - | 2.406 | 2.341 | 3.544 | 3.135 | 3.355 | 2.957 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.406 | 2.341 | 3.544 | 3.135 | 3.355 | 2.957 |'
- en: 'Table 15: Detailed scores for each tasks in XR scenarios.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：XR场景中各任务的详细得分。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 2.892 | 2.505 | 3.543 | 3.222 | 3.611 | 3.154 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 2.892 | 2.505 | 3.543 | 3.222 | 3.611 | 3.154 |'
- en: '| E. | 2.814 | 2.163 | 3.510 | 3.108 | 3.455 | 3.006 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.814 | 2.163 | 3.510 | 3.108 | 3.455 | 3.006 |'
- en: '| Qwen-VL-Max | R. | 2.047 | 1.968 | 2.712 | 2.879 | 3.132 | 2.469 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.047 | 1.968 | 2.712 | 2.879 | 3.132 | 2.469 |'
- en: '| E. | 2.125 | 1.973 | 2.658 | 2.760 | 3.029 | 2.499 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.125 | 1.973 | 2.658 | 2.760 | 3.029 | 2.499 |'
- en: '| H. | 1.886 | 1.920 | 2.656 | 2.727 | 3.012 | 2.373 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| H. | 1.886 | 1.920 | 2.656 | 2.727 | 3.012 | 2.373 |'
- en: '| GPT-4V | R. | 2.934 | 2.668 | 3.392 | 3.291 | 3.714 | 3.200 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 2.934 | 2.668 | 3.392 | 3.291 | 3.714 | 3.200 |'
- en: '| E. | 2.222 | 2.153 | 3.310 | 3.151 | 3.618 | 2.892 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.222 | 2.153 | 3.310 | 3.151 | 3.618 | 2.892 |'
- en: '| H. | 2.893 | 2.778 | 3.538 | 3.364 | 3.747 | 3.265 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| H. | 2.893 | 2.778 | 3.538 | 3.364 | 3.747 | 3.265 |'
- en: '| C.C. | 1.744 | 2.412 | 3.327 | 3.080 | 3.485 | 2.809 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 1.744 | 2.412 | 3.327 | 3.080 | 3.485 | 2.809 |'
- en: '| D.C. | 2.427 | 2.409 | 3.518 | 3.176 | 3.749 | 3.056 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.427 | 2.409 | 3.518 | 3.176 | 3.749 | 3.056 |'
- en: '| H.+D.C. | 2.775 | 2.635 | 3.580 | 3.235 | 3.734 | 3.191 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 2.775 | 2.635 | 3.580 | 3.235 | 3.734 | 3.191 |'
- en: '| GPT-4o | H. | 2.871 | 2.745 | 3.370 | 3.596 | 3.836 | 3.285 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 2.871 | 2.745 | 3.370 | 3.596 | 3.836 | 3.285 |'
- en: '| ChatUnivi | - | 1.660 | 1.420 | 2.205 | 2.250 | 3.270 | 2.161 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.660 | 1.420 | 2.205 | 2.250 | 3.270 | 2.161 |'
- en: '| Minigpt4Video | - | 1.225 | 1.161 | 1.610 | 1.347 | 1.465 | 1.362 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.225 | 1.161 | 1.610 | 1.347 | 1.465 | 1.362 |'
- en: '| VideoChat2 | - | 1.654 | 1.547 | 2.192 | 2.099 | 2.529 | 2.005 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.654 | 1.547 | 2.192 | 2.099 | 2.529 | 2.005 |'
- en: '| GUI-Vid | - | 2.444 | 2.147 | 3.347 | 2.836 | 3.036 | 2.764 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.444 | 2.147 | 3.347 | 2.836 | 3.036 | 2.764 |'
- en: 'Table 16: Detailed scores for each tasks in Multi-windows scenarios.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：多窗口场景中每个任务的详细分数。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 2.538 | 2.410 | 3.296 | 3.152 | 3.402 | 2.959 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 2.538 | 2.410 | 3.296 | 3.152 | 3.402 | 2.959 |'
- en: '| E. | 2.545 | 2.049 | 2.972 | 2.930 | 3.389 | 2.777 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.545 | 2.049 | 2.972 | 2.930 | 3.389 | 2.777 |'
- en: '| Qwen-VL-Max | R. | 1.793 | 1.872 | 2.770 | 2.897 | 3.122 | 2.432 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 1.793 | 1.872 | 2.770 | 2.897 | 3.122 | 2.432 |'
- en: '| E. | 1.866 | 1.780 | 2.730 | 2.627 | 3.105 | 2.362 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| E. | 1.866 | 1.780 | 2.730 | 2.627 | 3.105 | 2.362 |'
- en: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
- en: '| GPT-4V | R. | 3.185 | 2.655 | 3.745 | 3.699 | 3.973 | 3.452 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.185 | 2.655 | 3.745 | 3.699 | 3.973 | 3.452 |'
- en: '| E. | 2.902 | 2.406 | 3.636 | 3.420 | 3.729 | 3.219 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.902 | 2.406 | 3.636 | 3.420 | 3.729 | 3.219 |'
- en: '| H. | 3.000 | 2.952 | 3.801 | 3.597 | 3.889 | 3.449 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.000 | 2.952 | 3.801 | 3.597 | 3.889 | 3.449 |'
- en: '| C.C. | 2.097 | 2.973 | 3.774 | 3.331 | 3.621 | 3.160 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 2.097 | 2.973 | 3.774 | 3.331 | 3.621 | 3.160 |'
- en: '| D.C. | 2.671 | 2.979 | 3.849 | 3.466 | 3.822 | 3.358 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.671 | 2.979 | 3.849 | 3.466 | 3.822 | 3.358 |'
- en: '| H.+D.C. | 3.037 | 3.162 | 4.079 | 3.748 | 4.036 | 3.617 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.037 | 3.162 | 4.079 | 3.748 | 4.036 | 3.617 |'
- en: '| GPT-4o | H. | 3.108 | 3.106 | 3.829 | 4.043 | 4.188 | 3.654 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.108 | 3.106 | 3.829 | 4.043 | 4.188 | 3.654 |'
- en: '| ChatUnivi | - | 1.658 | 1.623 | 2.514 | 2.384 | 3.199 | 2.275 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.658 | 1.623 | 2.514 | 2.384 | 3.199 | 2.275 |'
- en: '| Minigpt4Video | - | 1.205 | 1.186 | 1.690 | 1.400 | 1.801 | 1.457 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.205 | 1.186 | 1.690 | 1.400 | 1.801 | 1.457 |'
- en: '| VideoChat2 | - | 1.754 | 1.774 | 2.479 | 2.420 | 2.699 | 2.222 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.754 | 1.774 | 2.479 | 2.420 | 2.699 | 2.222 |'
- en: '| GUI-Vid | - | 2.485 | 2.067 | 3.537 | 2.954 | 3.247 | 2.861 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.485 | 2.067 | 3.537 | 2.954 | 3.247 | 2.861 |'
- en: 'Table 17: Detailed scores for each tasks in IOS scenarios.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17：IOS 场景中每个任务的详细分数。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | R. | 3.076 | 2.637 | 3.370 | 3.366 | 3.615 | 3.213 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 3.076 | 2.637 | 3.370 | 3.366 | 3.615 | 3.213 |'
- en: '| E. | 2.852 | 2.356 | 3.137 | 3.126 | 3.566 | 3.007 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.852 | 2.356 | 3.137 | 3.126 | 3.566 | 3.007 |'
- en: '| Qwen-VL-Max | R. | 2.438 | 2.244 | 2.923 | 3.102 | 3.273 | 2.779 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 2.438 | 2.244 | 2.923 | 3.102 | 3.273 | 2.779 |'
- en: '| E. | 2.303 | 2.150 | 2.614 | 3.145 | 3.264 | 2.659 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.303 | 2.150 | 2.614 | 3.145 | 3.264 | 2.659 |'
- en: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| H. | 1.884 | 1.969 | 2.913 | 2.689 | 3.104 | 2.490 |'
- en: '| GPT-4V | R. | 3.364 | 3.080 | 3.684 | 3.766 | 4.184 | 3.614 |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.364 | 3.080 | 3.684 | 3.766 | 4.184 | 3.614 |'
- en: '| E. | 3.209 | 2.774 | 3.545 | 3.611 | 4.006 | 3.427 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| E. | 3.209 | 2.774 | 3.545 | 3.611 | 4.006 | 3.427 |'
- en: '| H. | 3.107 | 2.830 | 3.631 | 3.680 | 4.011 | 3.453 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| H. | 3.107 | 2.830 | 3.631 | 3.680 | 4.011 | 3.453 |'
- en: '| C.C. | 1.788 | 2.291 | 3.511 | 3.212 | 3.542 | 2.868 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 1.788 | 2.291 | 3.511 | 3.212 | 3.542 | 2.868 |'
- en: '| D.C. | 2.751 | 2.732 | 3.654 | 3.642 | 3.842 | 3.324 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 2.751 | 2.732 | 3.654 | 3.642 | 3.842 | 3.324 |'
- en: '| H.+D.C. | 3.090 | 2.965 | 3.740 | 3.786 | 3.994 | 3.516 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| H.+D.C. | 3.090 | 2.965 | 3.740 | 3.786 | 3.994 | 3.516 |'
- en: '| GPT-4o | H. | 3.183 | 2.993 | 3.460 | 4.050 | 4.141 | 3.558 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.183 | 2.993 | 3.460 | 4.050 | 4.141 | 3.558 |'
- en: '| ChatUnivi | - | 1.771 | 1.642 | 2.408 | 2.559 | 3.307 | 2.337 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.771 | 1.642 | 2.408 | 2.559 | 3.307 | 2.337 |'
- en: '| Minigpt4Video | - | 1.291 | 1.219 | 1.698 | 1.556 | 1.737 | 1.501 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.291 | 1.219 | 1.698 | 1.556 | 1.737 | 1.501 |'
- en: '| VideoChat2 | - | 1.955 | 1.803 | 2.145 | 2.315 | 2.626 | 2.169 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.955 | 1.803 | 2.145 | 2.315 | 2.626 | 2.169 |'
- en: '| GUI-Vid | - | 2.262 | 2.133 | 3.401 | 2.843 | 3.224 | 2.773 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.262 | 2.133 | 3.401 | 2.843 | 3.224 | 2.773 |'
- en: 'Table 18: Detailed scores for each tasks in Android scenarios.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：Android 场景下每个任务的详细分数。
- en: '| Models | Setting | Static | Sequential | Prediction | Conversation1 | Conversation2
    | Average |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 对话1 | 对话2 | 平均 |'
- en: '| Gemini-Pro-1.5 | E. | 2.703 | 2.460 | 3.157 | 3.642 | 3.881 | 3.168 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | E. | 2.703 | 2.460 | 3.157 | 3.642 | 3.881 | 3.168 |'
- en: '| Qwen-VL-Max | R. | 1.887 | 1.804 | 2.398 | 2.823 | 3.056 | 2.309 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 1.887 | 1.804 | 2.398 | 2.823 | 3.056 | 2.309 |'
- en: '| E. | 1.785 | 1.630 | 2.311 | 2.605 | 3.233 | 2.277 |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| E. | 1.785 | 1.630 | 2.311 | 2.605 | 3.233 | 2.277 |'
- en: '| GPT-4V | R. | 3.116 | 3.047 | 3.477 | 3.924 | 4.008 | 3.515 |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 3.116 | 3.047 | 3.477 | 3.924 | 4.008 | 3.515 |'
- en: '| E. | 2.705 | 2.470 | 3.175 | 3.647 | 3.885 | 3.176 |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| E. | 2.705 | 2.470 | 3.175 | 3.647 | 3.885 | 3.176 |'
- en: '| C.C. | 2.092 | 2.243 | 3.139 | 3.443 | 3.782 | 2.939 |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| C.C. | 2.092 | 2.243 | 3.139 | 3.443 | 3.782 | 2.939 |'
- en: '| D.C. | 3.015 | 2.890 | 3.357 | 3.883 | 3.990 | 3.427 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| D.C. | 3.015 | 2.890 | 3.357 | 3.883 | 3.990 | 3.427 |'
- en: '| GPT-4o | H. | 3.057 | 3.220 | 3.373 | 3.981 | 4.186 | 3.561 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 3.057 | 3.220 | 3.373 | 3.981 | 4.186 | 3.561 |'
- en: '| ChatUnivi | - | 1.835 | 1.654 | 2.317 | 2.712 | 3.433 | 2.390 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 1.835 | 1.654 | 2.317 | 2.712 | 3.433 | 2.390 |'
- en: '| Minigpt4Video | - | 1.183 | 1.159 | 1.507 | 1.342 | 1.521 | 1.342 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 1.183 | 1.159 | 1.507 | 1.342 | 1.521 | 1.342 |'
- en: '| VideoChat2 | - | 1.732 | 1.754 | 2.125 | 2.340 | 2.645 | 2.119 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 1.732 | 1.754 | 2.125 | 2.340 | 2.645 | 2.119 |'
- en: '| GUI-Vid | - | 2.010 | 1.928 | 3.053 | 2.755 | 3.105 | 2.572 |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 2.010 | 1.928 | 3.053 | 2.755 | 3.105 | 2.572 |'
- en: 'Table 19: Detailed BLEU and BERTScore (B.S.) in Software scenarios.'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：软件场景下详细的 BLEU 和 BERTScore (B.S.)。
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.109 | 0.789 | 0.150 | 0.720 | 0.078 | 0.680 | 0.056
    | 0.716 | 0.016 | 0.605 | 0.122 | 0.761 | 0.089 | 0.712 |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.109 | 0.789 | 0.150 | 0.720 | 0.078 | 0.680 | 0.056
    | 0.716 | 0.016 | 0.605 | 0.122 | 0.761 | 0.089 | 0.712 |'
- en: '| E. | 0.093 | 0.758 | 0.134 | 0.699 | 0.072 | 0.659 | 0.046 | 0.682 | 0.011
    | 0.558 | 0.106 | 0.747 | 0.077 | 0.684 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.093 | 0.758 | 0.134 | 0.699 | 0.072 | 0.659 | 0.046 | 0.682 | 0.011
    | 0.558 | 0.106 | 0.747 | 0.077 | 0.684 |'
- en: '| Qwen-VL-Max | R. | 0.085 | 0.698 | 0.101 | 0.649 | 0.064 | 0.576 | 0.010
    | 0.521 | 0.008 | 0.443 | 0.121 | 0.749 | 0.065 | 0.606 |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.085 | 0.698 | 0.101 | 0.649 | 0.064 | 0.576 | 0.010
    | 0.521 | 0.008 | 0.443 | 0.121 | 0.749 | 0.065 | 0.606 |'
- en: '| E. | 0.094 | 0.704 | 0.103 | 0.633 | 0.062 | 0.595 | 0.009 | 0.524 | 0.006
    | 0.437 | 0.113 | 0.739 | 0.065 | 0.605 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.094 | 0.704 | 0.103 | 0.633 | 0.062 | 0.595 | 0.009 | 0.524 | 0.006
    | 0.437 | 0.113 | 0.739 | 0.065 | 0.605 |'
- en: '| H. | 0.081 | 0.676 | 0.098 | 0.620 | 0.067 | 0.596 | 0.009 | 0.504 | 0.004
    | 0.429 | 0.117 | 0.743 | 0.063 | 0.595 |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.081 | 0.676 | 0.098 | 0.620 | 0.067 | 0.596 | 0.009 | 0.504 | 0.004
    | 0.429 | 0.117 | 0.743 | 0.063 | 0.595 |'
- en: '| GPT-4V | R. | 0.162 | 0.814 | 0.206 | 0.753 | 0.190 | 0.739 | 0.041 | 0.676
    | 0.033 | 0.581 | 0.181 | 0.793 | 0.136 | 0.726 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.162 | 0.814 | 0.206 | 0.753 | 0.190 | 0.739 | 0.041 | 0.676
    | 0.033 | 0.581 | 0.181 | 0.793 | 0.136 | 0.726 |'
- en: '| E. | 0.161 | 0.792 | 0.191 | 0.726 | 0.175 | 0.724 | 0.030 | 0.609 | 0.017
    | 0.486 | 0.165 | 0.786 | 0.123 | 0.687 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.161 | 0.792 | 0.191 | 0.726 | 0.175 | 0.724 | 0.030 | 0.609 | 0.017
    | 0.486 | 0.165 | 0.786 | 0.123 | 0.687 |'
- en: '| H. | 0.153 | 0.805 | 0.194 | 0.737 | 0.183 | 0.731 | 0.037 | 0.639 | 0.025
    | 0.537 | 0.179 | 0.791 | 0.129 | 0.707 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.153 | 0.805 | 0.194 | 0.737 | 0.183 | 0.731 | 0.037 | 0.639 | 0.025
    | 0.537 | 0.179 | 0.791 | 0.129 | 0.707 |'
- en: '| GPT-4o | H. | 0.131 | 0.806 | 0.212 | 0.776 | 0.147 | 0.728 | 0.041 | 0.711
    | 0.018 | 0.575 | 0.159 | 0.803 | 0.118 | 0.733 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.131 | 0.806 | 0.212 | 0.776 | 0.147 | 0.728 | 0.041 | 0.711
    | 0.018 | 0.575 | 0.159 | 0.803 | 0.118 | 0.733 |'
- en: '| ChatUnivi | - | 0.097 | 0.697 | 0.074 | 0.581 | 0.101 | 0.619 | 0.005 | 0.409
    | 0.000 | 0.195 | 0.084 | 0.723 | 0.060 | 0.537 |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.097 | 0.697 | 0.074 | 0.581 | 0.101 | 0.619 | 0.005 | 0.409
    | 0.000 | 0.195 | 0.084 | 0.723 | 0.060 | 0.537 |'
- en: '| Minigpt4Video | - | 0.019 | 0.516 | 0.022 | 0.470 | 0.029 | 0.516 | 0.000
    | 0.399 | 0.000 | 0.249 | 0.013 | 0.510 | 0.014 | 0.443 |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.019 | 0.516 | 0.022 | 0.470 | 0.029 | 0.516 | 0.000
    | 0.399 | 0.000 | 0.249 | 0.013 | 0.510 | 0.014 | 0.443 |'
- en: '| VideoChat2 | - | 0.095 | 0.698 | 0.080 | 0.595 | 0.076 | 0.574 | 0.004 |
    0.341 | 0.000 | 0.193 | 0.100 | 0.733 | 0.059 | 0.523 |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.095 | 0.698 | 0.080 | 0.595 | 0.076 | 0.574 | 0.004 |
    0.341 | 0.000 | 0.193 | 0.100 | 0.733 | 0.059 | 0.523 |'
- en: '| GUI-Vid | - | 0.142 | 0.758 | 0.145 | 0.681 | 0.114 | 0.698 | 0.049 | 0.658
    | 0.004 | 0.519 | 0.093 | 0.717 | 0.091 | 0.672 |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.142 | 0.758 | 0.145 | 0.681 | 0.114 | 0.698 | 0.049 | 0.658
    | 0.004 | 0.519 | 0.093 | 0.717 | 0.091 | 0.672 |'
- en: 'Table 20: Detailed BLEU and BERTScore (B.S.) in Website scenarios.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '表 20: 网站场景中的详细 BLEU 和 BERTScore (B.S.)。'
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均值 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.113 | 0.793 | 0.145 | 0.727 | 0.083 | 0.676 | 0.054
    | 0.720 | 0.016 | 0.664 | 0.098 | 0.736 | 0.085 | 0.719 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.113 | 0.793 | 0.145 | 0.727 | 0.083 | 0.676 | 0.054
    | 0.720 | 0.016 | 0.664 | 0.098 | 0.736 | 0.085 | 0.719 |'
- en: '| E. | 0.095 | 0.754 | 0.121 | 0.681 | 0.079 | 0.661 | 0.041 | 0.676 | 0.011
    | 0.602 | 0.092 | 0.725 | 0.073 | 0.683 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.095 | 0.754 | 0.121 | 0.681 | 0.079 | 0.661 | 0.041 | 0.676 | 0.011
    | 0.602 | 0.092 | 0.725 | 0.073 | 0.683 |'
- en: '| Qwen-VL-Max | R. | 0.099 | 0.728 | 0.099 | 0.634 | 0.080 | 0.610 | 0.008
    | 0.519 | 0.005 | 0.471 | 0.085 | 0.694 | 0.063 | 0.609 |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.099 | 0.728 | 0.099 | 0.634 | 0.080 | 0.610 | 0.008
    | 0.519 | 0.005 | 0.471 | 0.085 | 0.694 | 0.063 | 0.609 |'
- en: '| E. | 0.083 | 0.710 | 0.101 | 0.631 | 0.093 | 0.611 | 0.011 | 0.503 | 0.004
    | 0.469 | 0.099 | 0.709 | 0.065 | 0.605 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.083 | 0.710 | 0.101 | 0.631 | 0.093 | 0.611 | 0.011 | 0.503 | 0.004
    | 0.469 | 0.099 | 0.709 | 0.065 | 0.605 |'
- en: '| H. | 0.079 | 0.693 | 0.089 | 0.597 | 0.093 | 0.606 | 0.009 | 0.488 | 0.007
    | 0.449 | 0.103 | 0.705 | 0.063 | 0.590 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.079 | 0.693 | 0.089 | 0.597 | 0.093 | 0.606 | 0.009 | 0.488 | 0.007
    | 0.449 | 0.103 | 0.705 | 0.063 | 0.590 |'
- en: '| GPT-4V | R. | 0.173 | 0.830 | 0.241 | 0.765 | 0.205 | 0.751 | 0.040 | 0.694
    | 0.032 | 0.645 | 0.164 | 0.763 | 0.142 | 0.741 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.173 | 0.830 | 0.241 | 0.765 | 0.205 | 0.751 | 0.040 | 0.694
    | 0.032 | 0.645 | 0.164 | 0.763 | 0.142 | 0.741 |'
- en: '| E. | 0.159 | 0.802 | 0.204 | 0.727 | 0.202 | 0.727 | 0.033 | 0.648 | 0.031
    | 0.590 | 0.149 | 0.757 | 0.130 | 0.708 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.159 | 0.802 | 0.204 | 0.727 | 0.202 | 0.727 | 0.033 | 0.648 | 0.031
    | 0.590 | 0.149 | 0.757 | 0.130 | 0.708 |'
- en: '| H. | 0.182 | 0.823 | 0.234 | 0.771 | 0.213 | 0.758 | 0.043 | 0.696 | 0.041
    | 0.660 | 0.165 | 0.768 | 0.147 | 0.746 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.182 | 0.823 | 0.234 | 0.771 | 0.213 | 0.758 | 0.043 | 0.696 | 0.041
    | 0.660 | 0.165 | 0.768 | 0.147 | 0.746 |'
- en: '| GPT-4o | H. | 0.141 | 0.813 | 0.219 | 0.768 | 0.199 | 0.731 | 0.054 | 0.700
    | 0.026 | 0.602 | 0.146 | 0.755 | 0.131 | 0.728 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.141 | 0.813 | 0.219 | 0.768 | 0.199 | 0.731 | 0.054 | 0.700
    | 0.026 | 0.602 | 0.146 | 0.755 | 0.131 | 0.728 |'
- en: '| ChatUnivi | - | 0.078 | 0.645 | 0.068 | 0.581 | 0.102 | 0.607 | 0.008 | 0.399
    | 0.000 | 0.192 | 0.061 | 0.661 | 0.053 | 0.514 |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.078 | 0.645 | 0.068 | 0.581 | 0.102 | 0.607 | 0.008 | 0.399
    | 0.000 | 0.192 | 0.061 | 0.661 | 0.053 | 0.514 |'
- en: '| Minigpt4Video | - | 0.022 | 0.527 | 0.016 | 0.448 | 0.027 | 0.501 | 0.000
    | 0.344 | 0.000 | 0.186 | 0.011 | 0.522 | 0.013 | 0.421 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.022 | 0.527 | 0.016 | 0.448 | 0.027 | 0.501 | 0.000
    | 0.344 | 0.000 | 0.186 | 0.011 | 0.522 | 0.013 | 0.421 |'
- en: '| VideoChat2 | - | 0.073 | 0.619 | 0.075 | 0.579 | 0.049 | 0.511 | 0.004 |
    0.328 | 0.000 | 0.167 | 0.067 | 0.678 | 0.045 | 0.480 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.073 | 0.619 | 0.075 | 0.579 | 0.049 | 0.511 | 0.004 |
    0.328 | 0.000 | 0.167 | 0.067 | 0.678 | 0.045 | 0.480 |'
- en: '| GUI-Vid | - | 0.114 | 0.731 | 0.158 | 0.674 | 0.129 | 0.694 | 0.049 | 0.667
    | 0.002 | 0.553 | 0.075 | 0.681 | 0.088 | 0.667 |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.114 | 0.731 | 0.158 | 0.674 | 0.129 | 0.694 | 0.049 | 0.667
    | 0.002 | 0.553 | 0.075 | 0.681 | 0.088 | 0.667 |'
- en: 'Table 21: Detailed BLEU and BERTScore (B.S.) in XR scenarios.'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '表 21: XR 场景中的详细 BLEU 和 BERTScore (B.S.)。'
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均值 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.088 | 0.772 | 0.101 | 0.678 | 0.070 | 0.678 | 0.026
    | 0.650 | 0.002 | 0.463 | 0.082 | 0.733 | 0.062 | 0.662 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.088 | 0.772 | 0.101 | 0.678 | 0.070 | 0.678 | 0.026
    | 0.650 | 0.002 | 0.463 | 0.082 | 0.733 | 0.062 | 0.662 |'
- en: '| E. | 0.073 | 0.760 | 0.090 | 0.651 | 0.062 | 0.666 | 0.015 | 0.618 | 0.002
    | 0.449 | 0.084 | 0.720 | 0.054 | 0.644 |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.073 | 0.760 | 0.090 | 0.651 | 0.062 | 0.666 | 0.015 | 0.618 | 0.002
    | 0.449 | 0.084 | 0.720 | 0.054 | 0.644 |'
- en: '| Qwen-VL-Max | R. | 0.069 | 0.703 | 0.075 | 0.602 | 0.049 | 0.601 | 0.006
    | 0.486 | 0.000 | 0.338 | 0.117 | 0.738 | 0.053 | 0.578 |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.069 | 0.703 | 0.075 | 0.602 | 0.049 | 0.601 | 0.006
    | 0.486 | 0.000 | 0.338 | 0.117 | 0.738 | 0.053 | 0.578 |'
- en: '| E. | 0.048 | 0.689 | 0.079 | 0.657 | 0.058 | 0.605 | 0.005 | 0.498 | 0.000
    | 0.359 | 0.112 | 0.739 | 0.050 | 0.591 |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.048 | 0.689 | 0.079 | 0.657 | 0.058 | 0.605 | 0.005 | 0.498 | 0.000
    | 0.359 | 0.112 | 0.739 | 0.050 | 0.591 |'
- en: '| H. | 0.051 | 0.651 | 0.073 | 0.593 | 0.044 | 0.591 | 0.004 | 0.493 | 0.001
    | 0.357 | 0.101 | 0.726 | 0.046 | 0.569 |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.051 | 0.651 | 0.073 | 0.593 | 0.044 | 0.591 | 0.004 | 0.493 | 0.001
    | 0.357 | 0.101 | 0.726 | 0.046 | 0.569 |'
- en: '| GPT-4V | R. | 0.093 | 0.794 | 0.169 | 0.715 | 0.165 | 0.736 | 0.028 | 0.625
    | 0.006 | 0.457 | 0.147 | 0.768 | 0.101 | 0.683 |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.093 | 0.794 | 0.169 | 0.715 | 0.165 | 0.736 | 0.028 | 0.625
    | 0.006 | 0.457 | 0.147 | 0.768 | 0.101 | 0.683 |'
- en: '| E. | 0.085 | 0.726 | 0.131 | 0.665 | 0.162 | 0.724 | 0.020 | 0.541 | 0.003
    | 0.382 | 0.141 | 0.760 | 0.090 | 0.633 |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.085 | 0.726 | 0.131 | 0.665 | 0.162 | 0.724 | 0.020 | 0.541 | 0.003
    | 0.382 | 0.141 | 0.760 | 0.090 | 0.633 |'
- en: '| H. | 0.091 | 0.797 | 0.181 | 0.732 | 0.180 | 0.744 | 0.027 | 0.630 | 0.006
    | 0.471 | 0.154 | 0.773 | 0.106 | 0.691 |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.091 | 0.797 | 0.181 | 0.732 | 0.180 | 0.744 | 0.027 | 0.630 | 0.006
    | 0.471 | 0.154 | 0.773 | 0.106 | 0.691 |'
- en: '| GPT-4o | H. | 0.077 | 0.800 | 0.154 | 0.717 | 0.153 | 0.718 | 0.020 | 0.615
    | 0.006 | 0.468 | 0.138 | 0.759 | 0.091 | 0.680 |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.077 | 0.800 | 0.154 | 0.717 | 0.153 | 0.718 | 0.020 | 0.615
    | 0.006 | 0.468 | 0.138 | 0.759 | 0.091 | 0.680 |'
- en: '| ChatUnivi | - | 0.083 | 0.686 | 0.061 | 0.538 | 0.091 | 0.575 | 0.006 | 0.475
    | 0.000 | 0.282 | 0.086 | 0.693 | 0.054 | 0.541 |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.083 | 0.686 | 0.061 | 0.538 | 0.091 | 0.575 | 0.006 | 0.475
    | 0.000 | 0.282 | 0.086 | 0.693 | 0.054 | 0.541 |'
- en: '| Minigpt4Video | - | 0.014 | 0.545 | 0.016 | 0.466 | 0.027 | 0.502 | 0.001
    | 0.453 | 0.000 | 0.262 | 0.013 | 0.474 | 0.012 | 0.450 |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.014 | 0.545 | 0.016 | 0.466 | 0.027 | 0.502 | 0.001
    | 0.453 | 0.000 | 0.262 | 0.013 | 0.474 | 0.012 | 0.450 |'
- en: '| VideoChat2 | - | 0.077 | 0.679 | 0.079 | 0.595 | 0.073 | 0.577 | 0.004 |
    0.378 | 0.000 | 0.211 | 0.101 | 0.721 | 0.056 | 0.527 |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.077 | 0.679 | 0.079 | 0.595 | 0.073 | 0.577 | 0.004 |
    0.378 | 0.000 | 0.211 | 0.101 | 0.721 | 0.056 | 0.527 |'
- en: '| GUI-Vid | - | 0.096 | 0.754 | 0.149 | 0.689 | 0.131 | 0.700 | 0.051 | 0.637
    | 0.003 | 0.460 | 0.082 | 0.705 | 0.085 | 0.657 |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.096 | 0.754 | 0.149 | 0.689 | 0.131 | 0.700 | 0.051 | 0.637
    | 0.003 | 0.460 | 0.082 | 0.705 | 0.085 | 0.657 |'
- en: 'Table 22: Detailed BLEU and BERTScore (B.S.) in IOS scenarios.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '表 22: IOS 场景下的详细 BLEU 和 BERTScore (B.S.)。'
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均值 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.108 | 0.797 | 0.142 | 0.717 | 0.080 | 0.682 | 0.075
    | 0.714 | 0.011 | 0.602 | 0.117 | 0.746 | 0.089 | 0.710 |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.108 | 0.797 | 0.142 | 0.717 | 0.080 | 0.682 | 0.075
    | 0.714 | 0.011 | 0.602 | 0.117 | 0.746 | 0.089 | 0.710 |'
- en: '| E. | 0.099 | 0.768 | 0.136 | 0.700 | 0.075 | 0.655 | 0.066 | 0.695 | 0.011
    | 0.592 | 0.113 | 0.743 | 0.083 | 0.692 |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.099 | 0.768 | 0.136 | 0.700 | 0.075 | 0.655 | 0.066 | 0.695 | 0.011
    | 0.592 | 0.113 | 0.743 | 0.083 | 0.692 |'
- en: '| Qwen-VL-Max | R. | 0.087 | 0.704 | 0.098 | 0.650 | 0.112 | 0.639 | 0.009
    | 0.519 | 0.003 | 0.465 | 0.106 | 0.725 | 0.069 | 0.617 |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.087 | 0.704 | 0.098 | 0.650 | 0.112 | 0.639 | 0.009
    | 0.519 | 0.003 | 0.465 | 0.106 | 0.725 | 0.069 | 0.617 |'
- en: '| E. | 0.075 | 0.638 | 0.095 | 0.647 | 0.094 | 0.600 | 0.009 | 0.512 | 0.009
    | 0.475 | 0.103 | 0.712 | 0.064 | 0.597 |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.075 | 0.638 | 0.095 | 0.647 | 0.094 | 0.600 | 0.009 | 0.512 | 0.009
    | 0.475 | 0.103 | 0.712 | 0.064 | 0.597 |'
- en: '| H. | 0.080 | 0.632 | 0.083 | 0.589 | 0.092 | 0.617 | 0.013 | 0.520 | 0.007
    | 0.452 | 0.099 | 0.703 | 0.062 | 0.585 |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.080 | 0.632 | 0.083 | 0.589 | 0.092 | 0.617 | 0.013 | 0.520 | 0.007
    | 0.452 | 0.099 | 0.703 | 0.062 | 0.585 |'
- en: '| GPT-4V | R. | 0.159 | 0.824 | 0.224 | 0.772 | 0.206 | 0.766 | 0.040 | 0.673
    | 0.030 | 0.579 | 0.174 | 0.777 | 0.139 | 0.732 |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.159 | 0.824 | 0.224 | 0.772 | 0.206 | 0.766 | 0.040 | 0.673
    | 0.030 | 0.579 | 0.174 | 0.777 | 0.139 | 0.732 |'
- en: '| E. | 0.149 | 0.813 | 0.201 | 0.752 | 0.207 | 0.746 | 0.035 | 0.659 | 0.017
    | 0.566 | 0.160 | 0.762 | 0.128 | 0.716 |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.149 | 0.813 | 0.201 | 0.752 | 0.207 | 0.746 | 0.035 | 0.659 | 0.017
    | 0.566 | 0.160 | 0.762 | 0.128 | 0.716 |'
- en: '| H. | 0.156 | 0.805 | 0.205 | 0.745 | 0.203 | 0.748 | 0.034 | 0.644 | 0.025
    | 0.559 | 0.159 | 0.763 | 0.130 | 0.711 |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.156 | 0.805 | 0.205 | 0.745 | 0.203 | 0.748 | 0.034 | 0.644 | 0.025
    | 0.559 | 0.159 | 0.763 | 0.130 | 0.711 |'
- en: '| GPT-4o | H. | 0.137 | 0.802 | 0.196 | 0.761 | 0.199 | 0.732 | 0.035 | 0.683
    | 0.022 | 0.533 | 0.154 | 0.774 | 0.124 | 0.714 |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.137 | 0.802 | 0.196 | 0.761 | 0.199 | 0.732 | 0.035 | 0.683
    | 0.022 | 0.533 | 0.154 | 0.774 | 0.124 | 0.714 |'
- en: '| ChatUnivi | - | 0.093 | 0.679 | 0.085 | 0.604 | 0.106 | 0.616 | 0.005 | 0.437
    | 0.000 | 0.258 | 0.076 | 0.698 | 0.061 | 0.548 |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.093 | 0.679 | 0.085 | 0.604 | 0.106 | 0.616 | 0.005 | 0.437
    | 0.000 | 0.258 | 0.076 | 0.698 | 0.061 | 0.548 |'
- en: '| Minigpt4Video | - | 0.026 | 0.547 | 0.026 | 0.513 | 0.035 | 0.548 | 0.001
    | 0.411 | 0.000 | 0.236 | 0.015 | 0.529 | 0.017 | 0.464 |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.026 | 0.547 | 0.026 | 0.513 | 0.035 | 0.548 | 0.001
    | 0.411 | 0.000 | 0.236 | 0.015 | 0.529 | 0.017 | 0.464 |'
- en: '| VideoChat2 | - | 0.089 | 0.683 | 0.078 | 0.605 | 0.061 | 0.555 | 0.002 |
    0.355 | 0.000 | 0.190 | 0.086 | 0.710 | 0.053 | 0.516 |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.089 | 0.683 | 0.078 | 0.605 | 0.061 | 0.555 | 0.002 |
    0.355 | 0.000 | 0.190 | 0.086 | 0.710 | 0.053 | 0.516 |'
- en: '| GUI-Vid | - | 0.114 | 0.725 | 0.144 | 0.693 | 0.123 | 0.700 | 0.048 | 0.641
    | 0.002 | 0.518 | 0.083 | 0.686 | 0.085 | 0.661 |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.114 | 0.725 | 0.144 | 0.693 | 0.123 | 0.700 | 0.048 | 0.641
    | 0.002 | 0.518 | 0.083 | 0.686 | 0.085 | 0.661 |'
- en: 'Table 23: Detailed BLEU and BERTScore (B.S.) in Android scenarios.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '表 23: Android 场景下的详细 BLEU 和 BERTScore (B.S.)。'
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均值 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | E. | 0.089 | 0.771 | 0.189 | 0.704 | 0.189 | 0.710 | 0.023
    | 0.619 | 0.016 | 0.570 | 0.149 | 0.749 | 0.109 | 0.687 |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | E. | 0.089 | 0.771 | 0.189 | 0.704 | 0.189 | 0.710 | 0.023
    | 0.619 | 0.016 | 0.570 | 0.149 | 0.749 | 0.109 | 0.687 |'
- en: '| Qwen-VL-Max | R. | 0.041 | 0.640 | 0.084 | 0.528 | 0.066 | 0.549 | 0.008
    | 0.484 | 0.004 | 0.445 | 0.089 | 0.673 | 0.049 | 0.553 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.041 | 0.640 | 0.084 | 0.528 | 0.066 | 0.549 | 0.008
    | 0.484 | 0.004 | 0.445 | 0.089 | 0.673 | 0.049 | 0.553 |'
- en: '| E. | 0.037 | 0.634 | 0.074 | 0.498 | 0.065 | 0.541 | 0.005 | 0.443 | 0.003
    | 0.383 | 0.089 | 0.683 | 0.045 | 0.530 |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.037 | 0.634 | 0.074 | 0.498 | 0.065 | 0.541 | 0.005 | 0.443 | 0.003
    | 0.383 | 0.089 | 0.683 | 0.045 | 0.530 |'
- en: '| GPT-4V | R. | 0.106 | 0.809 | 0.242 | 0.757 | 0.210 | 0.733 | 0.029 | 0.653
    | 0.028 | 0.619 | 0.170 | 0.763 | 0.131 | 0.723 |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.106 | 0.809 | 0.242 | 0.757 | 0.210 | 0.733 | 0.029 | 0.653
    | 0.028 | 0.619 | 0.170 | 0.763 | 0.131 | 0.723 |'
- en: '| E. | 0.089 | 0.771 | 0.192 | 0.705 | 0.190 | 0.713 | 0.023 | 0.619 | 0.016
    | 0.571 | 0.150 | 0.750 | 0.110 | 0.688 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.089 | 0.771 | 0.192 | 0.705 | 0.190 | 0.713 | 0.023 | 0.619 | 0.016
    | 0.571 | 0.150 | 0.750 | 0.110 | 0.688 |'
- en: '| GPT-4o | H. | 0.075 | 0.809 | 0.241 | 0.755 | 0.188 | 0.719 | 0.038 | 0.677
    | 0.014 | 0.581 | 0.137 | 0.747 | 0.116 | 0.715 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.075 | 0.809 | 0.241 | 0.755 | 0.188 | 0.719 | 0.038 | 0.677
    | 0.014 | 0.581 | 0.137 | 0.747 | 0.116 | 0.715 |'
- en: '| ChatUnivi | - | 0.076 | 0.675 | 0.079 | 0.588 | 0.096 | 0.594 | 0.007 | 0.482
    | 0.001 | 0.368 | 0.063 | 0.670 | 0.054 | 0.563 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.076 | 0.675 | 0.079 | 0.588 | 0.096 | 0.594 | 0.007 | 0.482
    | 0.001 | 0.368 | 0.063 | 0.670 | 0.054 | 0.563 |'
- en: '| Minigpt4Video | - | 0.017 | 0.416 | 0.013 | 0.369 | 0.019 | 0.405 | 0.000
    | 0.279 | 0.000 | 0.103 | 0.010 | 0.392 | 0.010 | 0.327 |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.017 | 0.416 | 0.013 | 0.369 | 0.019 | 0.405 | 0.000
    | 0.279 | 0.000 | 0.103 | 0.010 | 0.392 | 0.010 | 0.327 |'
- en: '| VideoChat2 | - | 0.057 | 0.641 | 0.077 | 0.560 | 0.063 | 0.523 | 0.004 |
    0.402 | 0.000 | 0.272 | 0.075 | 0.654 | 0.046 | 0.509 |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.057 | 0.641 | 0.077 | 0.560 | 0.063 | 0.523 | 0.004 |
    0.402 | 0.000 | 0.272 | 0.075 | 0.654 | 0.046 | 0.509 |'
- en: '| GUI-Vid | - | 0.083 | 0.682 | 0.130 | 0.628 | 0.126 | 0.644 | 0.023 | 0.500
    | 0.001 | 0.393 | 0.071 | 0.659 | 0.072 | 0.584 |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| GUI-Vid | - | 0.083 | 0.682 | 0.130 | 0.628 | 0.126 | 0.644 | 0.023 | 0.500
    | 0.001 | 0.393 | 0.071 | 0.659 | 0.072 | 0.584 |'
- en: 'Table 24: Detailed BLEU and BERTScore (B.S.) in Multiple-windows scenarios.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 表 24：多窗口场景中的详细 BLEU 和 BERTScore (B.S.)。
- en: '| Models | Setting | Static | Sequential | Prediction | Description | Caption
    | Conversation | Avg. |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 静态 | 顺序 | 预测 | 描述 | 标题 | 对话 | 平均值 |'
- en: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU | B.S. | BLEU
    | B.S. | BLEU | B.S. |'
- en: '| Gemini-Pro-1.5 | R. | 0.113 | 0.739 | 0.126 | 0.693 | 0.086 | 0.658 | 0.061
    | 0.685 | 0.012 | 0.586 | 0.090 | 0.674 | 0.081 | 0.673 |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro-1.5 | R. | 0.113 | 0.739 | 0.126 | 0.693 | 0.086 | 0.658 | 0.061
    | 0.685 | 0.012 | 0.586 | 0.090 | 0.674 | 0.081 | 0.673 |'
- en: '| E. | 0.106 | 0.728 | 0.131 | 0.680 | 0.072 | 0.622 | 0.055 | 0.655 | 0.015
    | 0.550 | 0.084 | 0.679 | 0.077 | 0.652 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.106 | 0.728 | 0.131 | 0.680 | 0.072 | 0.622 | 0.055 | 0.655 | 0.015
    | 0.550 | 0.084 | 0.679 | 0.077 | 0.652 |'
- en: '| Qwen-VL-Max | R. | 0.079 | 0.599 | 0.076 | 0.591 | 0.080 | 0.595 | 0.002
    | 0.444 | 0.006 | 0.370 | 0.072 | 0.666 | 0.053 | 0.544 |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Max | R. | 0.079 | 0.599 | 0.076 | 0.591 | 0.080 | 0.595 | 0.002
    | 0.444 | 0.006 | 0.370 | 0.072 | 0.666 | 0.053 | 0.544 |'
- en: '| E. | 0.064 | 0.609 | 0.087 | 0.567 | 0.089 | 0.608 | 0.003 | 0.445 | 0.004
    | 0.398 | 0.073 | 0.647 | 0.053 | 0.546 |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.064 | 0.609 | 0.087 | 0.567 | 0.089 | 0.608 | 0.003 | 0.445 | 0.004
    | 0.398 | 0.073 | 0.647 | 0.053 | 0.546 |'
- en: '| H. | 0.089 | 0.634 | 0.078 | 0.580 | 0.093 | 0.612 | 0.003 | 0.409 | 0.005
    | 0.344 | 0.080 | 0.656 | 0.058 | 0.539 |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.089 | 0.634 | 0.078 | 0.580 | 0.093 | 0.612 | 0.003 | 0.409 | 0.005
    | 0.344 | 0.080 | 0.656 | 0.058 | 0.539 |'
- en: '| GPT-4V | R. | 0.172 | 0.800 | 0.186 | 0.737 | 0.212 | 0.745 | 0.040 | 0.671
    | 0.021 | 0.592 | 0.145 | 0.728 | 0.129 | 0.712 |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V | R. | 0.172 | 0.800 | 0.186 | 0.737 | 0.212 | 0.745 | 0.040 | 0.671
    | 0.021 | 0.592 | 0.145 | 0.728 | 0.129 | 0.712 |'
- en: '| E. | 0.160 | 0.763 | 0.169 | 0.703 | 0.198 | 0.759 | 0.034 | 0.621 | 0.012
    | 0.527 | 0.116 | 0.709 | 0.115 | 0.680 |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| E. | 0.160 | 0.763 | 0.169 | 0.703 | 0.198 | 0.759 | 0.034 | 0.621 | 0.012
    | 0.527 | 0.116 | 0.709 | 0.115 | 0.680 |'
- en: '| H. | 0.173 | 0.781 | 0.196 | 0.748 | 0.220 | 0.775 | 0.046 | 0.672 | 0.021
    | 0.577 | 0.133 | 0.724 | 0.132 | 0.713 |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| H. | 0.173 | 0.781 | 0.196 | 0.748 | 0.220 | 0.775 | 0.046 | 0.672 | 0.021
    | 0.577 | 0.133 | 0.724 | 0.132 | 0.713 |'
- en: '| GPT-4o | H. | 0.156 | 0.792 | 0.185 | 0.754 | 0.213 | 0.769 | 0.040 | 0.683
    | 0.019 | 0.588 | 0.121 | 0.717 | 0.122 | 0.717 |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | H. | 0.156 | 0.792 | 0.185 | 0.754 | 0.213 | 0.769 | 0.040 | 0.683
    | 0.019 | 0.588 | 0.121 | 0.717 | 0.122 | 0.717 |'
- en: '| ChatUnivi | - | 0.076 | 0.628 | 0.063 | 0.573 | 0.103 | 0.605 | 0.009 | 0.413
    | 0.000 | 0.191 | 0.057 | 0.643 | 0.051 | 0.509 |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| ChatUnivi | - | 0.076 | 0.628 | 0.063 | 0.573 | 0.103 | 0.605 | 0.009 | 0.413
    | 0.000 | 0.191 | 0.057 | 0.643 | 0.051 | 0.509 |'
- en: '| Minigpt4Video | - | 0.015 | 0.504 | 0.024 | 0.473 | 0.023 | 0.527 | 0.001
    | 0.326 | 0.000 | 0.155 | 0.009 | 0.469 | 0.012 | 0.409 |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| Minigpt4Video | - | 0.015 | 0.504 | 0.024 | 0.473 | 0.023 | 0.527 | 0.001
    | 0.326 | 0.000 | 0.155 | 0.009 | 0.469 | 0.012 | 0.409 |'
- en: '| VideoChat2 | - | 0.098 | 0.657 | 0.081 | 0.593 | 0.067 | 0.577 | 0.007 |
    0.344 | 0.000 | 0.162 | 0.065 | 0.654 | 0.053 | 0.498 |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat2 | - | 0.098 | 0.657 | 0.081 | 0.593 | 0.067 | 0.577 | 0.007 |
    0.344 | 0.000 | 0.162 | 0.065 | 0.654 | 0.053 | 0.498 |'
- en: '| GUI-Vid | - | 0.128 | 0.737 | 0.144 | 0.664 | 0.133 | 0.721 | 0.041 | 0.605
    | 0.004 | 0.452 | 0.058 | 0.644 | 0.084 | 0.637 | ![Refer to caption](img/120e1565de3a6cfb1e95954a4fc3b9e3.png)'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '| GUI-Vid | - | 0.128 | 0.737 | 0.144 | 0.664 | 0.133 | 0.721 | 0.041 | 0.605
    | 0.004 | 0.452 | 0.058 | 0.644 | 0.084 | 0.637 | ![参见说明](img/120e1565de3a6cfb1e95954a4fc3b9e3.png)'
- en: 'Figure 16: Fine-grained performance of Gemini-Pro-1.5 in each software and
    website.'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：Gemini-Pro-1.5 在每个软件和网站中的细粒度性能。
- en: '![Refer to caption](img/3fad4fb4b2574e90dce1c0ab5ead4799.png)'
  id: totrans-633
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3fad4fb4b2574e90dce1c0ab5ead4799.png)'
- en: 'Figure 17: Fine-grained performance of Qwen-VL-Max in each software and website.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：Qwen-VL-Max 在每个软件和网站中的细粒度性能。
- en: Appendix E Prompts
  id: totrans-635
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 E 提示
- en: 'In this section, we provide detailed prompts for models and human annotators.
    [Figure 19](#A5.F19 "Figure 19 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents") shows the guideline of
    human annotation, [Figure 18](#A5.F18 "Figure 18 ‣ Appendix E Prompts ‣ Part I
    Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents")
    shows the prompt for leveraging LLMs to refine grammarly mistakes and polish sentence
    for human annotations. [Figure 20](#A5.F20 "Figure 20 ‣ Appendix E Prompts ‣ Part
    I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents"),
    [Figure 21](#A5.F21 "Figure 21 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World:
    A Dataset for GUI-oriented Multimodal LLM-based Agents"), and [Figure 22](#A5.F22
    "Figure 22 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") present the prompt for Human-MLLM collaboration
    method to generate GUI-orientaed tasks. [Figure 23](#A5.F23 "Figure 23 ‣ Appendix
    E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents") illustrate the prompt for benchmarking MLLMs, different GUI
    scenarios and different QA type has different prompt. [Figure 24](#A5.F24 "Figure
    24 ‣ Appendix E Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") and [Figure 25](#A5.F25 "Figure 25 ‣ Appendix E
    Prompts ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") show prompt for LLM-as-a-Judge for free-form as well as conversational
    tasks and multiple-choice QA respectively.'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供了模型和人工标注员的详细提示。 [图 19](#A5.F19 "图 19 ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对
    GUI 的多模态 LLM 代理的数据集") 展示了人工标注的指导方针， [图 18](#A5.F18 "图 18 ‣ 附录 E 提示 ‣ 第一部分附录 ‣
    GUI-World：一个针对 GUI 的多模态 LLM 代理的数据集") 展示了利用 LLM 进行语法错误修正和句子润色的提示。 [图 20](#A5.F20
    "图 20 ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 代理的数据集")、[图 21](#A5.F21
    "图 21 ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 代理的数据集") 和 [图 22](#A5.F22
    "图 22 ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 代理的数据集") 展示了人类-MLLM 协作方法生成
    GUI 任务的提示。 [图 23](#A5.F23 "图 23 ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM
    代理的数据集") 说明了用于基准测试 MLLMs 的提示，不同的 GUI 场景和不同的 QA 类型有不同的提示。 [图 24](#A5.F24 "图 24
    ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 代理的数据集") 和 [图 25](#A5.F25 "图
    25 ‣ 附录 E 提示 ‣ 第一部分附录 ‣ GUI-World：一个针对 GUI 的多模态 LLM 代理的数据集") 展示了 LLM 作为裁判进行自由形式以及对话任务和多项选择
    QA 的提示。
- en: Refining
    Human Annotation on Goal and Sub-goal As
    an expert in English, please refine the following English instructions (or objectives)
    into a polished phrase or a concise sentence.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 完善目标和子目标的人工注释
    作为英语专家，请将以下英文说明（或目标）润色为精炼的短语或简洁的句子。
- en: Avoid including irrelevant content and provide the polished output directly.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 避免包含无关内容，直接提供完善的输出。
- en: 'Here is the English sentence: {string}'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 这是英文句子：{string}
- en: 'Figure 18: Refining Human Annotation on Goal and Sub-goal.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：完善目标和子目标的人工注释。
- en: 'Guideline
    for Human Annotation Main Interface 1\. Video List
    Panel (Left Panel): Displays a list of loaded video files. Each video file is
    shown with its name for identification.'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 人工注释指南
    主界面 1\. 视频列表面板（左侧面板）：显示加载的视频文件列表。每个视频文件显示其名称以便识别。
- en: '2\. Video Display Area (Center Panel): Shows the currently selected video for
    playback and annotation.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 视频显示区域（中央面板）：显示当前选择的视频以进行播放和注释。
- en: '3\. Control Settings (Right Panel):'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 控制设置（右侧面板）：
- en: 'Operating System: Select the operating system of the machine where the video
    was recorded.'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统：选择视频录制机器的操作系统。
- en: 'Full Screen: Toggle full screen mode for the video display.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 全屏：切换视频显示的全屏模式。
- en: 'Multi-application?: Indicate if multiple applications in the video.'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 多应用程序？：指示视频中是否有多个应用程序。
- en: 'Application/Website: Enter the name of the application or website being used
    in the video.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序/网站：输入视频中使用的应用程序或网站的名称。
- en: 'User Goal: Enter the goal of the user performing the annotation.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 用户目标：输入执行注释的用户目标。
- en: 4\. Playback and Annotation Controls (Bottom Panel)
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 播放和注释控制（底部面板）
- en: 'Annotate: Open a annotation window to add a new keyframe annotation.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 注释：打开一个注释窗口以添加新的关键帧注释。
- en: 'Play: Starts or pauses the video playback.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 播放：开始或暂停视频播放。
- en: 'Load Video: Allows you to load a single video file.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 加载视频：允许你加载单个视频文件。
- en: 'Load Video Folder: Allows to load multiple video files from a folder.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 加载视频文件夹：允许从文件夹中加载多个视频文件。
- en: 'Previous Video / Next Video: Navigate through the loaded video files.'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个视频 / 下一个视频：在已加载的视频文件中导航。
- en: 'Save to JSON: Save the annotations in a JSON format.'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 保存为JSON：以JSON格式保存注释。
- en: Annotation Window
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 注释窗口
- en: '1\. Mouse Action: Select a type of mouse action (e.g. click, drag).'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 鼠标操作：选择鼠标操作的类型（例如，点击、拖动）。
- en: '2\. Keyboard Action: Select the type of keyboard action (e.g., typing, key
    press).'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 键盘操作：选择键盘操作的类型（例如，输入、按键）。
- en: '3\. Keyboard Operation Record: Enter details of the keyboard operation, if
    any.'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 键盘操作记录：输入键盘操作的详细信息（如果有的话）。
- en: '4\. sub-action Purpose: Describe the purpose of the action being annotated.'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 子操作目的：描述正在注释的操作的目的。
- en: How to Use
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方法
- en: Loading Videos
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 加载视频
- en: 1\. Load Multiple Videos
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 加载多个视频
- en: Click on the Load Video Folder button.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“加载视频文件夹”按钮。
- en: Select the folder containing your video files.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 选择包含视频文件的文件夹。
- en: All video files in the folder will be loaded and listed in the Video List Panel.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹中的所有视频文件将被加载并列出在视频列表面板中。
- en: Playing Videos
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 播放视频
- en: Select a video from the Video List Panel. Click the Play button to start or
    pause the video.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 从视频列表面板中选择一个视频。点击播放按钮开始或暂停视频。
- en: Annotating Videos
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 注释视频
- en: 1\. Start Annotation
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 开始注释
- en: Pause the video at the desired frame.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 在所需的帧暂停视频。
- en: Click the Annotate button to open the annotation window.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“注释”按钮打开注释窗口。
- en: 2\. Annotation Window
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 注释窗口
- en: Select the Mouse Action Type and Keyboard Action Type from the dropdown menus.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择鼠标操作类型和键盘操作类型。
- en: If there is a keyboard action, enter the details in the Keyboard Operation Record
    field.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有键盘操作，请在“键盘操作记录”字段中输入详细信息。
- en: Describe the action’s purpose in the Sub-action Purpose field.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在“子操作目的”字段中描述操作的目的。
- en: Click OK to save the annotation.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“确定”保存注释。
- en: Saving Annotations
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 保存注释
- en: Once all annotations are completed, click the Save to JSON button.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有注释后，点击“保存为JSON”按钮。
- en: 'Figure 19: Guideline for Human Annotation.'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：人工注释指南。
- en: '(Part 1)
    GPT-4V Generating GUI-oriented Tasks You
    are an AI visual assistant. This is a video of a mobile GUI, which I’ve divided
    into multiple frames and sent to you. Please provide a detailed description of
    what occurs throughout the entire video, focusing on the changes in the GUI elements
    or scenes rather than static aspects of a single frame. The detailed description
    should be placed under the key ’Description’. Based on your description, please
    design the following tasks:'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: (第1部分)
    GPT-4V 生成面向GUI的任务 你是一个AI视觉助手。这是一个移动GUI的视频，我将其分成了多个帧并发送给你。请提供整个视频的详细描述，重点关注GUI元素或场景的变化，而不是单个帧的静态部分。详细描述应放在“描述”键下。根据你的描述，请设计以下任务：
- en: Generate a precise caption for the video. This caption should encapsulate the
    main activities or changes observed throughout the video sequence. Place this
    caption under the key ’Caption’.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 为视频生成一个精确的标题。这个标题应该概括视频序列中观察到的主要活动或变化。将这个标题放在“标题”键下。
- en: Create a free-form QA question related to the video’s static GUI content, along
    with its answer. The question should delve into the details or changes in the
    static GUI elements or scenes captured in the video. The QA task should be nested
    under the key ‘static QA’, with ‘Question’ and ‘Answer’ as subkeys.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个与视频静态 GUI 内容相关的自由形式 QA 问题及其答案。问题应深入探讨视频中静态 GUI 元素或场景的细节或变化。QA 任务应嵌套在键 ‘static
    QA’ 下，包含 ‘Question’ 和 ‘Answer’ 作为子键。
- en: 'Develop a multiple-choice QA question about the video, with four options: one
    correct answer and three incorrect or irrelevant options. This task should assess
    the understanding of specific elements retieval or changes depicted in the video.
    Structure this task under the key ‘MCQA’, with ’Question’ detailing the query,
    ’Options’ listing the four choices including one correct answer, and ’Correct
    Answer’ specifying the correct option, denoted, for example, as {[[B]]}.'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个关于视频的多项选择题，包含四个选项：一个正确答案和三个错误或无关的选项。此任务应评估对视频中描绘的特定元素检索或变化的理解。将此任务结构化为键
    ‘MCQA’，其中 ’Question’ 详细说明查询，’Options’ 列出四个选择，包括一个正确答案，以及 ’Correct Answer’ 指定正确选项，例如标记为
    {[[B]]}。
- en: 'Here are some key information of the video to help you understand the video
    comprehensively:'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是视频的一些关键信息，以帮助你全面理解视频内容：
- en: 'System: {item[‘system’]}'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 系统：{item[‘system’]}
- en: 'Application: {item[‘app’]}'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序：{item[‘app’]}
- en: 'Summary of the video: {item[‘goal’]}'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: 视频摘要：{item[‘goal’]}
- en: 'Key Operation/Sub goal in the video: {[i[‘sub_goal’] for i in item[‘keyframes’]]}'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 视频中的关键操作/子目标：{[i[‘sub_goal’] for i in item[‘keyframes’]]}
- en: 'Notice: Ensure that the questions you design for these tasks are answerable
    and the answers can be deduced from the GUI video content. The answerable question
    should be designed as difficult as possible. The tasks should be unambiguous and
    the answers must be definitively correct based on your understanding of the video
    content. Only include questions that have definite answers: (1) one can see the
    content in the image that the question asks about and can answer confidently;
    (2) one can determine confidently from the image that it is not in the image.
    Do not ask any question that cannot be answered confidently.'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项：确保你为这些任务设计的问题是可以回答的，并且答案可以从 GUI 视频内容中推导出来。可回答的问题应尽可能设计得难一些。任务应明确无歧义，答案必须基于你对视频内容的理解而确定无疑。仅包含那些有明确答案的问题：（1）可以在图像中看到问题所问内容，并且可以自信地回答；（2）可以从图像中自信地判断它不在图像中。不要提出任何无法自信回答的问题。
- en: Each of these tasks should focus on the dynamic aspect of the GUI elements or
    scenes. Provide detailed answers when answering complex questions. For example,
    give detailed examples or reasoning steps to make the content more convincing
    and well-organized. The answers should be in a tone that a visual AI assistant
    is seeing the image and answering the question.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 每个任务应集中在 GUI 元素或场景的动态方面。回答复杂问题时请提供详细的答案。例如，提供详细的示例或推理步骤，以使内容更具说服力和条理性。答案应以视觉
    AI 助手正在查看图像并回答问题的语气进行。
- en: For the free-form QA tasks, please ensure that the answers are as detailed and
    lengthy as possible, with no concern for length. You can include multiple paragraphs
    if necessary to provide a comprehensive and thorough response. Please structure
    your response using JSON format and specific keys mentioned in the task requirements.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自由形式的 QA 任务，请确保答案尽可能详细且长篇，不必担心长度。如果有必要，可以包括多个段落，以提供全面且详尽的回答。请使用 JSON 格式和任务要求中提到的具体键来结构化你的回答。
- en: 'Figure 20: (Part 1) GPT-4V Generating GUI-oriented Tasks.'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：（第 1 部分）GPT-4V 生成面向 GUI 的任务。
- en: '(Part 2)
    GPT-4V Generating GUI-oriented Tasks. You
    are an AI visual assistant. This is a video of a  GUI, which I’ve
    divided into multiple frames and sent to you. Please provide a detailed description
    of what occurs throughout the entire video, focusing on the changes in the GUI
    elements or scenes rather than static aspects of a single frame. The detailed
    description should be placed under the key ’Description’. Based on your description,
    please design the following tasks:'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '(第2部分)
    GPT-4V 生成 GUI 相关任务。 你是一个 AI 视觉助手。这是一个
    GUI 的视频，我已将其分成多个帧并发送给你。请详细描述整个视频中发生的内容，重点关注 GUI 元素或场景的变化，而非单个帧的静态方面。详细描述应放在‘Description’键下。基于你的描述，请设计以下任务:'
- en: 'A Sequential QA task: Design a question that requires understanding the sequence
    of GUI element changes or scene transformations in the video. The question should
    be free-form and necessitate the use of temporal information from the sequential
    images. The task should be structured under the key ‘Sequential-QA’ with subkeys
    ‘Question’ and ‘Answer’.'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '顺序 QA 任务: 设计一个需要理解 GUI 元素变化或场景转变顺序的问题。问题应为自由形式，并需要使用顺序图像中的时间信息。该任务应结构化在‘Sequential-QA’键下，包含子键‘Question’和‘Answer’。'
- en: 'A Next Stage Prediction task: Formulate a question that asks about the subsequent
    state or event following a certain frame in the video. The question should be
    designed in a free-form manner and predict future GUI elements or scene changes,
    structured under the key ‘Prediction’ with subkeys ‘Question’ and ‘Answer’.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '下一阶段预测任务: 制定一个问题，询问视频中某一帧之后的后续状态或事件。问题应以自由形式设计，并预测未来的 GUI 元素或场景变化，结构化在‘Prediction’键下，包含子键‘Question’和‘Answer’。'
- en: 'A two-round dialogue task: Create a dialogue with two rounds of interaction.
    The first round includes a user instruction and an assistant response, and the
    second round’s user instruction should be based on the response from the first
    round. Both rounds should be free-form and nested under the key ‘Conversation’,
    with subkeys ‘User 1’, ‘Assistant 1’, ‘User 2’, and ‘Assistant 2’.'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '两轮对话任务: 创建一个包含两轮互动的对话。第一轮包括用户指令和助手回应，第二轮的用户指令应基于第一轮的回应。两轮对话应为自由形式，并嵌套在‘Conversation’键下，包含子键‘User
    1’，‘Assistant 1’，‘User 2’，和‘Assistant 2’。'
- en: 'A reasoning task: Design a multi-choice QA task that requires reasoning to
    identify the correct answer from four options. This task should test the reasoning
    ability to infer or deduce information that is not explicitly provided. It should
    be structured under the key ‘Reasoning’, with subkeys ‘Question’, ‘Options’, and
    ‘Correct Answer’.'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '推理任务: 设计一个多选 QA 任务，需要通过推理从四个选项中找出正确答案。该任务应测试推断或推理未明确提供的信息的能力。应以‘Reasoning’键下的子键‘Question’，‘Options’，和‘Correct
    Answer’来组织。'
- en: 'Here are some key information of the video to help you understand the video
    comprehensively:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '以下是帮助你全面理解视频的一些关键信息:'
- en: 'System: {item[’system’]}'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '系统: {item[’system’]}'
- en: 'Application: {item[’app’]}'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '应用: {item[’app’]}'
- en: 'Summary of the video: {item[’goal’]}'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '视频总结: {item[’goal’]}'
- en: 'Key Operation/Sub goal in the video: {[i[’sub_goal’] for i in item[’keyframes’]]}'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '视频中的关键操作/子目标: {[i[’sub_goal’] for i in item[’keyframes’]]}'
- en: 'Figure 21: (Part 2) GPT-4V Generating GUI-oriented Tasks.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：（第 2 部分）GPT-4V 生成面向 GUI 的任务。
- en: '(Part 3)
    GPT-4V Generating GUI-oriented Tasks. Notice:
    Ensure that the questions you design for these tasks are answerable and the answers
    can be deduced from the GUI video content. The answerable question should be designed
    as difficult as possible. The tasks should be unambiguous and the answers must
    be definitively correct based on your understanding of the video content. Only
    include questions that have definite answers: (1) one can see the content in the
    image that the question asks about and can answer confidently; (2) one can determine
    confidently from the image that it is not in the image. Do not ask any question
    that cannot be answered confidently.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：确保为这些任务设计的问题是可以回答的，且答案可以从 GUI 视频内容中推导出来。可回答的问题应尽可能具有挑战性。任务应清晰明确，答案必须基于对视频内容的理解而确定无疑。仅包含有确定答案的问题：（1）可以在图像中看到问题所询问的内容并可以自信地回答；（2）可以自信地从图像中确定该内容不在图像中。不要提出任何无法自信回答的问题。
- en: Each of these tasks should focus on the dynamic aspect of the GUI elements or
    scenes, with each answerable task as difficult as possible. Provide detailed answers
    when answering complex questions. For example, give detailed examples or reasoning
    steps to make the content more convincing and well-organized. The answers should
    be in a tone that a visual AI assistant is seeing the image and answering the
    question.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个任务都应专注于图形用户界面（GUI）元素或场景的动态方面，每个可回答的任务应尽可能具有挑战性。在回答复杂问题时，请提供详细的答案。例如，提供详细的示例或推理步骤，使内容更具说服力且条理清晰。答案应采用一种视觉
    AI 助手正在查看图像并回答问题的语气。
- en: For the free-form QA tasks, please ensure that the answers are as detailed and
    lengthy as possible, with no concern for length. You can include multiple paragraphs
    if necessary to provide a comprehensive and thorough response. Please structure
    your response using JSON format and specific keys mentioned in the task requirements.
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自由形式的 QA 任务，请确保答案尽可能详细和冗长，不必担心长度。如果有必要，可以包含多个段落，以提供全面且彻底的回答。请使用 JSON 格式结构化您的回答，并使用任务要求中提到的特定键。
- en: 'Figure 22: (Part 3) GPT-4V Generating GUI-oriented Tasks.'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22：（第 3 部分）GPT-4V 生成面向 GUI 的任务。
- en: 'Prompts
    for Benchmarking MLLMs "XR": "You are an AI visual
    assistant. Here are sequential images of Mixed-Reality combining GUI interface
    and real world, which are selected from a GUI video.", "software": "You are an
    AI visual assistant. Here are sequential GUI interface images of a specific software,
    which are selected from a GUI video.", "website": "You are an AI visual assistant.
    Here are sequential GUI interface images of a desktop website, which are selected
    from a GUI video.", "mobile": "You are an AI visual assistant. Here are sequential
    GUI mobile interface images, which are selected from a GUI video.", "multi": "You
    are an AI visual assistant. Here are sequential GUI interface images of interaction
    among multiple softwares and websites, which are selected from a GUI video.",
    "IOS": "You are an AI visual assistant. Here are sequential GUI IOS interface
    images, which are selected from a GUI video.", "Sequential-QA": "This is a question
    about sequential information in sequential images.", "Prediction": "This is a
    question about predicting the next action base on the previous actions in the
    sequential images.", "Reasoning": "This is a multiple choice question with only
    one correct answer. This question may need multiple steps of reasoning according
    to the vision information in sequential images.", "Description1": "Please give
    me a detail description of these sequential images.", "Description2": "Offer a
    thorough analysis of these sequential images", "Caption": "Please give me a concise
    caption of these sequential images.", "static QA": "This is a question about static
    information such as text, icon, layout in these sequential images.", "MCQA": "This
    is a multiple choice question with only one correct answer. This question may
    require sequential analysis ability to the vision information in these sequential
    images.", "Conversation1": "Act as an assistant to answer the user’s question
    in these sequential images.", "Conversation2": "This is a multi-turn conversation
    task. You will be provide the first round conversation and act as an assistant
    to answer the user’s question in the second round according to these sequential
    images." Notice = "You can first provide an overall description of these sequential
    images, and then analyze the user’s question according to the sequential images
    and description. Finally, give an answer based on this description and the image
    information. Please format your output in a Json format, with key ’Description’
    for the description of these sequential images, key ’Analysis’ for your analysis
    on the user’s question and key ’Answer’ for your answer to the User’s question."'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 'Prompts
    for Benchmarking MLLMs "XR": "You are an AI visual
    assistant. Here are sequential images of Mixed-Reality combining GUI interface
    and real world, which are selected from a GUI video.", "software": "You are an
    AI visual assistant. Here are sequential GUI interface images of a specific software,
    which are selected from a GUI video.", "website": "You are an AI visual assistant.
    Here are sequential GUI interface images of a desktop website, which are selected
    from a GUI video.", "mobile": "You are an AI visual assistant. Here are sequential
    GUI mobile interface images, which are selected from a GUI video.", "multi": "You
    are an AI visual assistant. Here are sequential GUI interface images of interaction
    among multiple softwares and websites, which are selected from a GUI video.",
    "IOS": "You are an AI visual assistant. Here are sequential GUI IOS interface
    images, which are selected from a GUI video.", "Sequential-QA": "This is a question
    about sequential information in sequential images.", "Prediction": "This is a
    question about predicting the next action base on the previous actions in the
    sequential images.", "Reasoning": "This is a multiple choice question with only
    one correct answer. This question may need multiple steps of reasoning according
    to the vision information in sequential images.", "Description1": "Please give
    me a detail description of these sequential images.", "Description2": "Offer a
    thorough analysis of these sequential images", "Caption": "Please give me a concise
    caption of these sequential images.", "static QA": "This is a question about static
    information such as text, icon, layout in these sequential images.", "MCQA": "This
    is a multiple choice question with only one correct answer. This question may
    require sequential analysis ability to the vision information in these sequential
    images.", "Conversation1": "Act as an assistant to answer the user’s question
    in these sequential images.", "Conversation2": "This is a multi-turn conversation
    task. You will be provide the first round conversation and act as an assistant
    to answer the user’s question in the second round according to these sequential
    images." Notice = "You can first provide an overall description of these sequential
    images, and then analyze the user’s question according to the sequential images
    and description. Finally, give an answer based on this description and the image
    information. Please format your output in a Json format, with key ’Description’
    for the description of these sequential images, key ’Analysis’ for your analysis
    on the user’s question and key ’Answer’ for your answer to the User’s question."'
- en: 'Figure 23: Prompts for Benchmarking MLLMs.'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 图 23：用于基准测试 MLLMs 的提示。
- en: 'Prompt
    for LLM-as-a-Judge: Judging Free-form and Conversational Tasks
    You are an impartial judge. I will provide you with a question,
    a ’gold standard’ answer, and a response that needs evaluation. Your task is to
    assess the quality of the response in comparison to the ’gold standard’ answer.
    Please adhere to the following guidelines:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 LLM
    作为裁判的提示：评判自由形式和对话任务 你是一个公正的裁判。我将给你提供一个问题、一个‘黄金标准’答案，以及一个需要评估的回应。你的任务是评估回应的质量，并与‘黄金标准’答案进行比较。请遵循以下指南：
- en: 1\. Start your evaluation by comparing the response to the ’gold standard’ answer.
    Offer a brief explanation highlighting similarities and differences, focusing
    on relevance, accuracy, depth, and level of detail.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 通过将回应与‘黄金标准’答案进行比较来开始评估。提供一个简要的解释，突出相似点和差异，重点关注相关性、准确性、深度和细节水平。
- en: 2\. Conclude your evaluation with a score from 1 to 5, where 1 indicates the
    response is mostly irrelevant to the ’gold standard’ answer, and 5 indicates it
    is very similar or equivalent.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 用1到5的分数来结束评估，其中1表示回应与‘黄金标准’答案大多无关，5表示非常相似或等同。
- en: 3\. Present your findings in JSON format, using ’Evaluation’ for your textual
    analysis and ’Score’ for the numerical assessment.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 以JSON格式呈现你的发现，使用‘Evaluation’进行文本分析，使用‘Score’进行数字评估。
- en: '4\. Ensure objectivity in your evaluation. Avoid biases and strive for an even
    distribution of scores across the spectrum of quality. Your scoring must be as
    rigorous as possible and adhere to the following rules:'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 确保评估的客观性。避免偏见，努力在质量范围内分配均匀的分数。你的评分必须尽可能严格，并遵循以下规则：
- en: '- Overall, the higher the quality of the model’s response, the higher the score,
    with factual accuracy and meeting user needs being the most critical dimensions.
    These two factors largely dictate the final composite score.'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，模型回应的质量越高，分数越高，事实准确性和满足用户需求是最关键的维度。这两个因素在很大程度上决定了最终的综合评分。
- en: '- If the model’s response is irrelevant to the question, contains fundamental
    factual errors, or generates harmful content, the total score must be 1.'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的回应与问题无关，包含基本事实错误，或生成有害内容，总分必须为1。
- en: '- If the model’s response has no severe errors and is essentially harmless,
    but of low quality and does not meet user needs, the total score should be 2.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的回应没有严重错误且基本无害，但质量低且未满足用户需求，总分应为2。
- en: '- If the model’s response generally meets user requirements but performs poorly
    in certain aspects with medium quality, the total score should be 3.'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的回应通常满足用户需求，但在某些方面表现较差，质量中等，总分应为3。
- en: '- If the model’s response is close in quality to the reference answer and performs
    well in all dimensions, the total score should be 4.'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的回应在质量上接近参考答案，并且在所有维度上表现良好，总分应为4。
- en: '- Only when the model’s response surpasses the reference answer, fully addresses
    the user’s problem and all needs, and nearly achieves a perfect score in all dimensions,
    can it receive a score between 5.'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当模型的回应超越参考答案，全面解决用户的问题和所有需求，并在所有维度上几乎达到完美分数时，才可以获得5分。
- en: '- As an example, the golden answer could receive a 4-5.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，黄金答案可以获得4-5分。
- en: 'Here is the response for you to judge:'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是你需要判断的回应：
- en: 'Question: {question}'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：{question}
- en: 'Golden Answer: {golden_answer}'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 黄金答案：{golden_answer}
- en: 'Response: {response}'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 回答：{response}
- en: Now, directly output your response in json format.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，直接以 JSON 格式输出你的回答。
- en: 'Figure 24: Prompt for LLM-as-a-Judge: Judging Free-form and Conversational
    Tasks .'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24：LLM 作为裁判的提示：评判自由形式和对话任务。
- en: 'Prompt
    for LLM-as-a-Judge: Judging Multiple-Choice QA Tasks
    You are a helpful assistant tasked with judging a Multiple Choice
    Question Answering exercise.'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 作为裁判的提示：评判多项选择
    QA 任务 你是一个有用的助手，负责评判一个多项选择题的回答练习。
- en: I will provide a correct answer with only one option, and a response that requires
    evaluation.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 我将提供一个只有一个选项的正确答案和一个需要评估的回答。
- en: If the response matches the correct answer, simply output "Yes"; If it does
    not, output "No".
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回答与正确答案匹配，则简单输出“是”；如果不匹配，则输出“否”。
- en: Please avoid including any irrelevant information.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 请避免包含任何无关信息。
- en: 'Here are some examples:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例：
- en: 'Example 1:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 1：
- en: 'Question: Based on the GUI video, why might the ’Loading’ animation continue
    without reaching the next stage? A. The user has not yet entered their login credentials.
    B. There is a system update being installed. C. The server is taking time to authenticate
    the login credentials. D. The ’Log In’ button is malfunctioning.'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：根据 GUI 视频，为什么“加载”动画可能会继续而无法进入下一阶段？ A. 用户尚未输入登录凭据。 B. 正在安装系统更新。 C. 服务器花时间验证登录凭据。
    D. “登录”按钮出现故障。
- en: 'Answer: C'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：C
- en: 'Response: C. The server is taking time to authenticate the login credentials.'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 回答：C. 服务器花时间验证登录凭据。
- en: 'Output: Yes'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：是
- en: 'Example 2:'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2：
- en: 'Question: If the user wants to resume the group video call after checking messages,
    what action should they take? A. Turn their head to the right. B. Close the messaging
    app interface. C. Say a voice command to switch applications. D. Turn their head
    to the left.'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：如果用户希望在查看消息后恢复群组视频通话，他们应该采取什么行动？ A. 向右转头。 B. 关闭消息应用程序界面。 C. 说语音命令切换应用程序。
    D. 向左转头。
- en: 'Answer: A'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：A
- en: 'Response: B'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 回答：B
- en: 'Output: No'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：否
- en: 'Example 3:'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 3：
- en: 'Question: What action does the user take to start playing music in the video?
    A. Closed the music player application B. Moved the music player to a new position
    C. Clicked the play button D. Adjusted the system volume'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：用户要采取什么行动才能开始播放视频中的音乐？ A. 关闭音乐播放器应用程序 B. 将音乐播放器移动到新位置 C. 点击播放按钮 D. 调整系统音量
- en: 'Answer: [[B]]'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：[[B]]
- en: 'Response: C'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 回答：C
- en: 'Output: No'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：否
- en: 'Here is the question, answer, and response for you to judge:'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你要判断的问题、答案和回答：
- en: 'Question: {question}'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：{question}
- en: 'Answer: {answer}'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：{answer}
- en: 'Response: {response}'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 回答：{response}
- en: Now, directly output "Yes" or "No".
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，直接输出“是”或“否”。
- en: 'Figure 25: Prompt for LLM-as-a-Judge: Judging Multiple-Choice QA Tasks.'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 图 25：LLM 作为裁判的提示：评判多项选择 QA 任务。
- en: Appendix F Case Study
  id: totrans-755
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 F 案例研究
- en: 'In this section, we provide detailed case studies for six GUI scenarios, each
    divided into two parts. [Figure 26](#A6.F26 "Figure 26 ‣ Appendix F Case Study
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") and [Figure 27](#A6.F27 "Figure 27 ‣ Appendix F Case Study ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents") show example
    frames and various tasks associated with them. [Figure 28](#A6.F28 "Figure 28
    ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") and [Figure 29](#A6.F29 "Figure 29 ‣ Appendix F
    Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal
    LLM-based Agents") for IOS, [Figure 30](#A6.F30 "Figure 30 ‣ Appendix F Case Study
    ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based
    Agents") and [Figure 31](#A6.F31 "Figure 31 ‣ Appendix F Case Study ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents") for multiple-windows
    interaction, [Figure 34](#A6.F34 "Figure 34 ‣ Appendix F Case Study ‣ Part I Appendix
    ‣ GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents") and [Figure 35](#A6.F35
    "Figure 35 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") for website, and [Figure 36](#A6.F36
    "Figure 36 ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for
    GUI-oriented Multimodal LLM-based Agents") and [Figure 37](#A6.F37 "Figure 37
    ‣ Appendix F Case Study ‣ Part I Appendix ‣ GUI-World: A Dataset for GUI-oriented
    Multimodal LLM-based Agents") for XR respectively.'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了六个 GUI 场景的详细案例研究，每个场景分为两部分。[图 26](#A6.F26 "图 26 ‣ 附录 F 案例研究 ‣ 第 I
    部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 和 [图 27](#A6.F27 "图 27 ‣ 附录 F 案例研究
    ‣ 第 I 部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 显示了示例帧和相关的各种任务。[图 28](#A6.F28
    "图 28 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 和 [图 29](#A6.F29
    "图 29 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 适用于 IOS，[图
    30](#A6.F30 "图 30 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集")
    和 [图 31](#A6.F31 "图 31 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM
    基础代理数据集") 适用于多窗口交互，[图 34](#A6.F34 "图 34 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录 ‣ GUI-World: 一个面向
    GUI 的多模态 LLM 基础代理数据集") 和 [图 35](#A6.F35 "图 35 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录 ‣ GUI-World:
    一个面向 GUI 的多模态 LLM 基础代理数据集") 适用于网站，[图 36](#A6.F36 "图 36 ‣ 附录 F 案例研究 ‣ 第 I 部分 附录
    ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 和 [图 37](#A6.F37 "图 37 ‣ 附录 F 案例研究 ‣
    第 I 部分 附录 ‣ GUI-World: 一个面向 GUI 的多模态 LLM 基础代理数据集") 适用于 XR。'
- en: '(Part 1)
    Android ![Refer to caption](img/64592007bc2bca8be2b8f1219d530e80.png)
    ‘Description1’: "The video starts with a view of a mobile app interface, specifically
    an Al-Qur’an Indonesia app with a list of Surahs. A user scrolls through the Surah
    list, selects a specific Surah, and the view transitions to a page displaying
    the verses in Arabic and their Indonesian translation. The user then returns to
    the Surah list. Following this, a pop-up appears indicating a configuration process
    that takes approximately 10-20 seconds. After the configuration, another pop-up
    notifies of an available update from the Google Play Store, which the user can
    accept or postpone. The user dismisses this pop-up. The main menu is accessed,
    showing options such as ‘Baca Quran’, ‘Terakhir Baca’, ‘Pencarian’, ‘Jadwal Sholat’,
    and ‘Pengaturan’. The user selects the ‘Pencarian’ option, which leads to a search
    page. Here, the user is instructed to enter a keyword to search in the translation
    database. Finally, the user navigates back to the main menu and then to the Surah
    list once more."'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '(第 1 部分)
    Android ![参考标题](img/64592007bc2bca8be2b8f1219d530e80.png)
    ‘描述 1’: “视频开始时展示了一个移动应用界面，特别是一个阿尔·古兰经印尼语应用，显示了章节列表。用户在章节列表中滚动，选择一个特定的章节，视图过渡到一个显示阿拉伯文和印尼文翻译的页面。然后，用户返回章节列表。接着，弹出窗口出现，指示一个大约需要
    10-20 秒的配置过程。配置完成后，另一个弹出窗口通知有来自 Google Play 商店的可用更新，用户可以选择接受或推迟。用户关闭了这个弹出窗口。访问主菜单，显示‘Baca
    Quran’、‘Terakhir Baca’、‘Pencarian’、‘Jadwal Sholat’ 和 ‘Pengaturan’ 等选项。用户选择了‘Pencarian’选项，这会带到一个搜索页面。在这里，用户被指示输入关键字以在翻译数据库中进行搜索。最后，用户返回主菜单，然后再次进入章节列表。”'''
- en: '‘Sequential-QA’: ‘Question’: ’After selecting a specific Surah from the list,
    what does the user do next before a pop-up notification appears?’'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: '‘顺序问答’: ‘问题’: ’从列表中选择特定章节后，用户在弹出通知出现之前会做什么？’'
- en: '‘Answer’: ‘After selecting a Surah, the user views the verses in Arabic and
    their Indonesian translation. Then, the user returns to the Surah list before
    a pop-up notification appears.’'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: ‘选择章节后，用户查看阿拉伯文和印尼文翻译的章节。然后，用户返回章节列表，然后弹出通知出现之前。’'
- en: '‘Prediction’: ‘Question’: ’What would likely be the next step in the app interface
    after the user is directed to the search page?’'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '‘预测’: ‘问题’: ’在用户被引导到搜索页面后，应用界面中下一个可能的步骤是什么？’'
- en: '‘Answer’: "After being directed to the search page, the likely next step would
    be for the user to enter a keyword into the search bar to find a translation or
    meaning within the app’s database."'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: “在被引导到搜索页面后，用户接下来可能的步骤是将关键字输入搜索框，以在应用的数据库中查找翻译或含义。”'
- en: '‘Conversation’: ’User 1’: ’I need to find a specific verse in the Quran. How
    do I search for it?’'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '‘对话’: ’用户 1’: ’我需要找到《古兰经》中的一个特定章节。我该怎么搜索？’'
- en: '‘Assistant 1’: "You can search for a specific verse by tapping on the ’Pencarian’
    option in the main menu, which will take you to a search page where you can enter
    a keyword."'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 1’: “你可以通过点击主菜单中的’Pencarian’选项来搜索特定章节，这将带你到一个搜索页面，你可以在这里输入关键字。”'
- en: '‘User 2’: ’I see. And how do I get back to the main menu from the search page?’'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '‘用户 2’: ’我明白了。那么我如何从搜索页面返回主菜单？’'
- en: '‘Assistant 2’: ’To return to the main menu from the search page, you can tap
    on the back arrow located at the top left corner of the screen.’'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 2’: ’要从搜索页面返回主菜单，可以点击屏幕左上角的返回箭头。’'
- en: '‘Reasoning’: ’Question’: "If the user wants to read the verses of Surah ’Al-Infitar’,
    what should they do after launching the app?"'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '‘推理’: ’问题’: "如果用户想要阅读苏拉’Al-Infitar’的经文，他们在启动应用后应该怎么做？"'
- en: '‘Options’: ["A. Go to the settings and select ’Al-Infitar’.", "B. Scroll through
    the Surah list and select ’Al-Infitar’.", "C. Choose the ’Pencarian’ option and
    type ’Al-Infitar’.", "D. Wait for a pop-up and select ’Al-Infitar’ from there."]'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’: ["A. 进入设置并选择’Al-Infitar’。", "B. 滚动查看苏拉列表并选择’Al-Infitar’。", "C. 选择’Pencarian’选项并输入’Al-Infitar’。",
    "D. 等待弹出窗口，并从中选择’Al-Infitar’。"]'
- en: '‘Correct Answer’: "B. Scroll through the Surah list and select ’Al-Infitar’."'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: "B. 滚动查看苏拉列表并选择’Al-Infitar’。"'
- en: 'Figure 26: Case study for Android (part 1).'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '图26: Android 案例研究（第 1 部分）。'
- en: '(Part 2)
    Android ![Refer to caption](img/64592007bc2bca8be2b8f1219d530e80.png)
    ‘Description2’: "The video begins by displaying a mobile GUI with a list of chapters
    from the Quran in Indonesian. Each chapter has a downward arrow suggesting expandable
    content. As the video progresses, a popup appears with a loading icon and a message
    in Indonesian indicating a configuration is in progress, which takes about 10-20
    seconds. After this, another popup appears notifying of a new update available
    on the Google Play Store with options to update or postpone. Subsequently, the
    screen shows a search interface where users can input keywords for searching within
    the Quran’s translated database. The main menu is then accessed, with options
    such as ‘Read Quran’, ‘Last Read’, ‘Search’, ‘Prayer Schedule’, and ‘Settings’.
    The GUI transitions back to the list of chapters, and a specific chapter, At-Takwir,
    is selected. The video then displays the verses of this chapter, both in Arabic
    and Indonesian translation, with an option to listen to the audio. Finally, it
    navigates back to the list of chapters."'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/64592007bc2bca8be2b8f1219d530e80.png) ‘描述2’: "视频开始时展示了一个移动GUI，列出了印尼语的《古兰经》章节。每个章节旁边有一个向下箭头，表示可展开内容。随着视频的进行，弹出窗口出现，显示加载图标和印尼语消息，表示正在进行配置，大约需要10-20秒。之后，另一个弹出窗口出现，通知Google
    Play商店有新的更新，提供更新或推迟的选项。随后，屏幕显示一个搜索界面，用户可以输入关键字在《古兰经》的翻译数据库中搜索。然后访问主菜单，选项包括‘阅读古兰经’，‘上次阅读’，‘搜索’，‘祷告时间表’，和‘设置’。GUI再次过渡到章节列表，选择了特定的章节At-Takwir。视频接着展示了这个章节的经文，包括阿拉伯语和印尼语翻译，并提供了收听音频的选项。最后，导航回章节列表。"'
- en: '‘Caption’: "Navigating through a Quran app’s GUI, interacting with chapter
    lists, update notifications, search function, and viewing specific verses with
    translations."'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: '‘说明’: "在《古兰经》应用程序的GUI中导航，互动章节列表，更新通知，搜索功能，以及查看特定经文的翻译。"'
- en: '‘static QA’: ‘Question’: ’What options are available in the main menu of the
    mobile Quran application?’'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态QA’: ’问题’: ’移动《古兰经》应用程序的主菜单中有哪些选项？’'
- en: '‘Answer’: "The main menu of the mobile Quran application provides several options
    for the user to choose from. These include ‘BACA QURAN’ (Read Quran) for accessing
    the chapters to read, ‘TERAKHIR BACA’ (Last Read) to resume reading from where
    the user left off last time, ‘PENCARIAN’ (Search) to search the Quran’s database
    for specific keywords, ‘JADWAL SHOLAT’ (Prayer Schedule) to check the prayer times,
    and ‘PENGATURAN’ (Settings) to modify app settings. This menu provides a simple
    and efficient way for users to navigate through the app’s features and customize
    their reading and learning experience."'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '‘答案’: “移动Quran应用程序的主菜单为用户提供了几种选择。这些选项包括‘BACA QURAN’（阅读古兰经），用于访问要阅读的章节，‘TERAKHIR
    BACA’（最后阅读），以从用户上次停留的地方继续阅读，‘PENCARIAN’（搜索），用于在古兰经数据库中搜索特定关键词，‘JADWAL SHOLAT’（祷告时间表），以查看祷告时间，以及‘PENGATURAN’（设置），以修改应用程序设置。此菜单提供了一种简单有效的方式，让用户浏览应用程序的功能并自定义他们的阅读和学习体验。”'
- en: '‘MCQA’: ‘Question’: ’What happens after the user is notified about the new
    update available on the Google Play Store?’'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '‘MCQA’: ‘问题’: ‘在用户收到关于Google Play商店新更新的通知后会发生什么？’'
- en: '‘Options’: ‘A’: ’The app closes automatically.’, ‘B’: ’The search interface
    is displayed.’, ‘C’: ’The list of chapters disappears.’, ‘D’: ’An advertisement
    for shopping deals is shown.’'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’：‘A’: ‘应用程序自动关闭。’，‘B’: ‘显示搜索界面。’，‘C’: ‘章节列表消失。’，‘D’: ‘显示购物优惠广告。’'
- en: '‘Correct Answer’: ’[[B]] The search interface is displayed.’'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’[[B]] 显示搜索界面。’'
- en: 'Figure 27: Case study for Android (part 2).'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 图27：Android案例研究（第2部分）。
- en: '(Part 1)
    IOS ![Refer to caption](img/f30971e45f4bdc4ab39cbfb19ae917d8.png)
    ‘Description1’: "The video demonstrates a user navigating through the Khan Academy
    mobile application under the ’Computing’ category. Initially, the user scrolls
    through the ’Computers and the Internet’ section, viewing topics such as ’Digital
    information,’ ’Bits and bytes,’ ’The Internet,’ and ’Online data security.’ The
    user then scrolls to the bottom, revealing the ’Computing innovations’ section
    and the ’Take Course Challenge’ button. Subsequently, the user returns to the
    previous screen, displaying other computing sections like ’AP®/College Computer
    Science Principles’ and ’Computer science theory.’ The user clicks to enter the
    ’Computer science theory’ interface; the content is loading. After the content
    has loaded, revealing topics like ’Cryptography’ and ’Information theory,’ the
    user returns to the previous page and clicks on ’Code.org.’"'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '(第1部分)
    IOS ![参考标题](img/f30971e45f4bdc4ab39cbfb19ae917d8.png)
    ‘描述1’: “视频展示了用户在‘计算’类别下浏览Khan Academy移动应用的过程。最初，用户滚动浏览‘计算机与互联网’部分，查看诸如‘数字信息’，‘位与字节’，‘互联网’和‘在线数据安全’等主题。然后，用户滚动到页面底部，显示出‘计算创新’部分和‘参加课程挑战’按钮。随后，用户返回到前一页面，显示其他计算部分，如‘AP®/大学计算机科学原理’和‘计算机科学理论’。用户点击进入‘计算机科学理论’界面，内容正在加载。加载完成后，展示出诸如‘密码学’和‘信息理论’等主题，用户返回到前一页面并点击‘Code.org’。”'
- en: '‘Caption’: "Navigating through computing courses on Khan Academy’s mobile application,
    viewing sections, and attempting to enter ’Computer science theory.’"'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '‘标题’: “在Khan Academy移动应用上浏览计算课程，查看各部分并尝试进入‘计算机科学理论’。”'
- en: '‘static QA’: ’Question’: "Which topic appears directly below ’Online data security’
    in the ’Computers and the Internet’ section before scrolling down?"'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态 QA’: ’问题’: "在向下滚动之前，’计算机与互联网’部分中，’在线数据安全’下方直接出现的主题是什么？"'
- en: '‘Answer’: "Before scrolling down, the topic that appears directly below ’Online
    data security’ is ’Computing innovations.’ This can be confirmed from the initial
    frames of the video where the ’Computing innovations’ section is partially visible,
    indicating that it is the next topic in the sequence after ’Online data security.’
    As the video progresses and the user scrolls down, the full ’Computing innovations’
    section comes into view, affirming its position in the GUI layout."'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: '‘答案’: "在向下滚动之前，’在线数据安全’下方直接出现的主题是’计算创新’。这一点可以从视频的初始画面确认，在这些画面中，’计算创新’部分部分可见，表明它是’在线数据安全’之后的下一个主题。随着视频的推进和用户的向下滚动，’计算创新’部分完全显示出来，确认了它在GUI布局中的位置。"'
- en: '‘MCQA’: ‘Question’: "What action does the user take after viewing the ’Computing
    innovations’ section?"'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '‘MCQA’: ‘问题’: "用户在查看’计算创新’部分后采取了什么行动？"'
- en: '‘Options’: ["A) Scrolls up to view ’Digital information’ again.", "B) Returns
    to the previous screen showing different computing sections.’, "C) Clicks on the
    ’Take Course Challenge’ button.", "D) Taps on the ’Explore’ tab at the bottom
    of the screen."]'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’: ["A) 向上滚动以再次查看’数字信息’。", "B) 返回到前一个屏幕，显示不同的计算部分。", "C) 点击’参加课程挑战’按钮。",
    "D) 点击屏幕底部的’探索’标签。"]'
- en: '‘Correct Answer’: ’[[B]] Returns to the previous screen showing different computing
    sections.’'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’[[B]] 返回到前一个屏幕，显示不同的计算部分。’'
- en: 'Figure 28: Case study for IOS (part 1).'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '图 28: IOS 案例研究（第 1 部分）。'
- en: '(Part 2)
    IOS ![Refer to caption](img/f30971e45f4bdc4ab39cbfb19ae917d8.png)
    ‘Description2’: "The video begins with the user viewing the ‘Computers and the
    Internet’ course section within the Khan Academy application. The user scrolls
    through various subsections such as ‘Digital information,’ ‘Computers,’ ‘The Internet,’
    and ‘Online data security,’ each with a list of topics and a status of possible
    mastery points. The user continues to scroll down to the ’Computing innovations’
    section and then further down to a ‘Course challenge’ prompt. The user then scrolls
    back up, revealing previously seen sections in reverse order. The user eventually
    navigates back to the main ‘Computing’ category screen, showing an overview of
    all computing-related courses. From there, the user selects ’Computer science
    theory,’ which briefly loads before displaying topics within that course such
    as ‘Cryptography’ and ‘Information theory.’ Following this, the user returns to
    the main ‘Computing’ category screen."'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: '(第 2 部分)
    IOS ![参见说明](img/f30971e45f4bdc4ab39cbfb19ae917d8.png)
    ‘描述2’: "视频开始时，用户查看 Khan Academy 应用程序中的’计算机与互联网’课程部分。用户滚动浏览’数字信息’、’计算机’、’互联网’和’在线数据安全’等各个子部分，每个部分都有主题列表和可能的掌握点状态。用户继续向下滚动到’计算创新’部分，然后进一步滚动到’课程挑战’提示。用户随后向上滚动，按相反顺序显示之前查看过的部分。用户最终返回到主’计算’类别屏幕，显示所有计算相关课程的概览。从那里，用户选择’计算机科学理论’，该部分短暂加载后显示该课程中的主题，如’密码学’和’信息理论’。接下来，用户返回到主’计算’类别屏幕。"'
- en: '‘Sequential-QA’: ‘Question’: "What action does the user take after scrolling
    through the ‘Online data security’ section, and what is displayed as a result
    of this action?", ‘Answer’: "After scrolling through the ‘Online data security’
    section, the user scrolls down to the ’Computing innovations’ section. As a result
    of this action, topics such as ‘Communication innovations’, ‘Collaboration innovations’,
    ·Crowdsourcing innovations’, and ·Monitoring innovations’ are displayed, followed
    by a ·Course challenge’ prompt."'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: ‘顺序问答’：‘问题’：“用户在滚动浏览‘在线数据安全’部分后采取了什么行动，并且这个行动的结果是什么？”‘回答’：“在滚动浏览‘在线数据安全’部分后，用户滚动到‘计算机创新’部分。作为这个行动的结果，显示了‘通信创新’，‘协作创新’，‘众包创新’，以及‘监控创新’等主题，随后是‘课程挑战’提示。”
- en: '‘Prediction’: ‘Question’: "If the user were to select the ‘Cryptography’ section
    after it was displayed, what kind of content could you predict will be shown next?"'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: ‘预测’：‘问题’：“如果用户在显示后选择了‘密码学’部分，你可以预测接下来会显示什么样的内容？”
- en: '‘Answer’: "If the user selects the ’Cryptography’ section, one could predict
    that the next content shown would be detailed topics under Cryptography, such
    as ‘Ancient cryptography,’ ‘Ciphers,’ ‘Cryptography challenge 101,’ ‘Modern cryptography,’
    ‘Modular arithmetic,’ ‘Primality test,’ and ‘Randomized algorithms,’ along with
    the status of possible mastery points for that section."'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: ‘回答’：“如果用户选择了‘密码学’部分，可以预测接下来显示的内容将是密码学下的详细主题，如‘古代密码学’，‘密码’，‘密码学挑战 101’，‘现代密码学’，‘模运算’，‘素性测试’，以及‘随机算法’，以及该部分可能的掌握点状态。”
- en: '‘Conversation’: ‘User 1’: ‘Can you take me to the section about user authentication
    methods?’'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: ‘对话’：‘用户 1’：‘你能带我去用户认证方法的部分吗？’
- en: '‘Assistant 1’: "Sure, you will need to scroll down the ‘Computers and the Internet’
    course section until you reach the ’Online data security’ subsection, where you
    will find the topic ’User authentication methods.’"'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 1’：“当然，你需要向下滚动‘计算机与互联网’课程部分，直到到达‘在线数据安全’小节，在那里你会找到‘用户认证方法’的主题。”
- en: '‘User 2’: "I scrolled too far down and reached the course challenge. How do
    I find ’User authentication methods’ from here?"'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: ‘用户 2’：“我滚动得太远，到了课程挑战。现在从这里如何找到‘用户认证方法’？”
- en: '‘Assistant 2’: "No problem, you’ll need to scroll back up past the ’Computing
    innovations’ section until you see the ’Online data security’ subsection again.
    ’User authentication methods’ is listed there among other topics."'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 2’：“没问题，你需要向上滚动经过‘计算机创新’部分，直到再次看到‘在线数据安全’小节。‘用户认证方法’在那列出了。”
- en: '‘Reasoning’: ’Question’: "After browsing through the course topics in ’Computers
    and the Internet,’ the user returns to a broader category view. Based on this
    behavior, what could be the reason for the user returning to the broader category
    view?"'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: ‘推理’：‘问题’：“在浏览完‘计算机与互联网’的课程主题后，用户返回到更广泛的分类视图。根据这种行为，用户返回到更广泛的分类视图的原因可能是什么？”
- en: '‘Options’: [‘A. The user wants to take a course challenge.’, ’B. The user is
    looking for a different computing-related course.’, ’C. The application automatically
    redirected the user.’, ’D. The user intends to log out of the Khan Academy application.’]'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: ‘选项’：[‘A. 用户想参加课程挑战。’, ‘B. 用户正在寻找不同的计算机相关课程。’, ‘C. 应用程序自动将用户重定向。’, ‘D. 用户打算退出可汗学院应用程序。’]
- en: '‘Correct Answer’: ’B’'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：‘B’
- en: 'Figure 29: Case study for IOS (part 2).'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29：iOS 案例研究（第 2 部分）。
- en: '(Part 1)
    Multiple-Windows Interaction ![Refer
    to caption](img/1febd252655fd84fb31414557a1ff8ba.png) ‘Description1’: "The video
    begins with a Windows desktop displaying multiple open applications, including
    Steam, OBS Studio, and a web browser with NVIDIA’s website loaded. The user starts
    by clicking on the back page of the browser, which partially obscures the OBS
    window. Then, the user clicks on the OBS application, bringing it to the forefront.
    The user minimizes OBS, followed by dragging the Steam window to the center of
    the screen and minimizing it as well. A new web page is opened in the Edge browser’s
    navigation bar, and the user types ’office’ into the search bar. The browser navigates
    to the Bing search interface, and ’office’ is successfully searched."'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '(第 1 部分)
    多窗口交互 ![参见说明](img/1febd252655fd84fb31414557a1ff8ba.png)
    ‘描述1’: “视频开始时显示了一个 Windows 桌面，上面有多个打开的应用程序，包括 Steam、OBS Studio 和加载了 NVIDIA 网站的网页浏览器。用户首先点击浏览器的返回页面，这部分遮挡了
    OBS 窗口。然后，用户点击 OBS 应用程序，将其置于最前面。用户将 OBS 最小化，然后将 Steam 窗口拖到屏幕中心并将其最小化。Edge 浏览器的导航栏中打开了一个新网页，用户在搜索栏中输入了
    ’office’。浏览器导航到 Bing 搜索界面，’office’ 被成功搜索。”'
- en: '‘Caption’: ’Navigating and Managing Multiple Applications on Windows Including
    Steam, OBS Studio, and Edge Browser’'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '‘说明’: ’在 Windows 上导航和管理多个应用程序，包括 Steam、OBS Studio 和 Edge 浏览器’'
- en: '‘static QA’: ‘Question’: "Which web browser is used in the video and which
    website is prominently featured before the search for ’office’?"'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态 QA’: ‘问题’: “视频中使用了哪个网页浏览器，以及在搜索 ’office’ 之前显著展示了哪个网站？”'
- en: '‘Answer’: "The web browser used in the video is Microsoft Edge. The prominently
    featured website before the search for ’office’ is NVIDIA’s official website where
    the ’Download Drivers’ page is displayed."'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '‘答案’: “视频中使用的网页浏览器是 Microsoft Edge。搜索 ’office’ 之前显著展示的网站是 NVIDIA 的官方网站，其中显示了
    ’下载驱动程序’ 页面。”'
- en: '‘MCQA’: ‘Question’: ’What action is taken after the OBS application is minimized?’,
    ‘Options’: [’A. The Steam window is closed.’'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '‘MCQA’: ‘问题’: ’OBS 应用程序最小化后采取了什么操作？’，‘选项’: [’A. Steam 窗口被关闭。’'
- en: ‘B. The Steam window is moved to the center of the screen and minimized.’, ‘C.
    The Edge browser is closed.’, ‘D. A file is opened from the desktop.’]
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: ‘B. Steam 窗口被移动到屏幕中心并最小化。’，‘C. Edge 浏览器被关闭。’，‘D. 从桌面打开了一个文件。’
- en: '‘Correct Answer’: ’[[B]] The Steam window is moved to the center of the screen
    and minimized.’'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’[[B]] Steam 窗口被移动到屏幕中心并最小化。’'
- en: 'Figure 30: Case study for multiple-windows interaction (part 1).'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '图 30: 多窗口交互的案例研究（第 1 部分）。'
- en: '(Part 2)
    Multiple-Windows Interaction ![Refer
    to caption](img/1febd252655fd84fb31414557a1ff8ba.png) ’Description2’: "The video
    shows a Windows desktop with several application windows open, including Steam,
    OBS, and Edge. Initially, the OBS window is partially covering the Edge browser.
    The user clicks on the Edge browser, bringing it to the foreground, and then minimizes
    the OBS window, clearing the view. Next, the user moves the Steam window to the
    center of the screen and minimizes it as well. Afterward, the user opens a new
    tab in Edge and enters ’office’ into the search bar, which leads to a Bing search
    results page for ’office’. The video ends with the user on the Bing search page
    with ’office’ results displayed."'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: '(第 2 部分)
    多窗口交互 ![参见说明](img/1febd252655fd84fb31414557a1ff8ba.png)
    ’描述 2’: "视频展示了一个 Windows 桌面，多个应用窗口打开，包括 Steam、OBS 和 Edge。最初，OBS 窗口部分遮盖了 Edge 浏览器。用户点击
    Edge 浏览器，将其置于前台，然后最小化了 OBS 窗口，清除了视图。接着，用户将 Steam 窗口移动到屏幕中心并同样将其最小化。之后，用户在 Edge
    中打开了一个新标签页，并在搜索栏中输入了 ’office’，这将他们带到 Bing 上的 ’office’ 搜索结果页面。视频结束时，用户在 Bing 搜索页面上显示了
    ’office’ 结果。"'
- en: '‘Sequential-QA’: ’Question’: ’After moving the Steam window to the center,
    what did the user do next in the Edge browser?’'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '‘顺序问答’: ’问题’: ’将 Steam 窗口移动到中心后，用户在 Edge 浏览器中接下来做了什么？’'
- en: '’Answer’: "After moving the Steam window to the center, the user minimized
    the Steam application. Then, they opened a new tab in the Edge browser and entered
    ’office’ into the search bar, which led them to the Bing search results page for
    ’office’."'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '’回答’: "将 Steam 窗口移动到中心后，用户最小化了 Steam 应用程序。然后，他们在 Edge 浏览器中打开了一个新标签页，并在搜索栏中输入了
    ’office’，这将他们带到 Bing 的 ’office’ 搜索结果页面。"'
- en: '‘Prediction’: ’Question’: "What would be the likely next action the user performs
    after searching for ’office’ on Bing?"'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '‘预测’: ’问题’: "用户在 Bing 上搜索 ’office’ 后，可能会执行什么操作？"'
- en: '’Answer’: "The likely next action the user would perform after searching for
    ’office’ on Bing would be to click on one of the search results to find more information
    or to visit the Microsoft Office website to purchase or learn more about the office
    suite."'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '’回答’: "用户在 Bing 上搜索 ’office’ 后，可能的下一步操作是点击搜索结果中的一个链接以获取更多信息，或访问 Microsoft Office
    网站以购买或了解更多有关办公套件的信息。"'
- en: '‘Conversation’: ’User 1’: ’Can you minimize the OBS application for a better
    view of the browser?’'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '‘对话’: ’用户 1’: ’你能最小化 OBS 应用程序以更好地查看浏览器吗？’'
- en: '‘Assistant 1’: ’Certainly, the OBS application has been minimized, providing
    a clear view of the Edge browser.’'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 1’: ’当然，OBS 应用程序已被最小化，清晰地显示了 Edge 浏览器。’'
- en: '‘User 2’: ’Great, now can you search for Microsoft Office in the Edge browser?’'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '‘用户 2’: ’很好，现在你能在 Edge 浏览器中搜索 Microsoft Office 吗？’'
- en: '‘Assistant 2’: "Of course, a new tab has been opened in the Edge browser and
    ’office’ has been entered into the search bar. The Bing search results for ’office’
    are now displayed."'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 2’: "当然，Edge 浏览器中已打开了一个新标签页，并在搜索栏中输入了 ’office’。现在显示的是 Bing 上的 ’office’
    搜索结果。"'
- en: '‘Reasoning’: ’Question’: ‘If the user needs to record gameplay footage next,
    which application should they interact with and what would be their first step?’'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '‘推理’: ’问题’: ‘如果用户接下来需要录制游戏画面，他们应该与哪个应用程序互动，第一步是什么？’'
- en: '‘Options’: ["A. They should open the Steam application and click on the ’STORE’
    tab.", "B. They should open the Edge browser and search for ’game recording software’.",
    "C. They should reopen the OBS application and click on the ’Start Recording’
    button.", "D. They should access the Windows Start menu and search for the ’Camera’
    app."]'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '‘选项’: ["A. 他们应该打开Steam应用程序并点击’商店’标签。", "B. 他们应该打开Edge浏览器并搜索’游戏录制软件’。", "C.
    他们应该重新打开OBS应用程序并点击’开始录制’按钮。", "D. 他们应该访问Windows开始菜单并搜索’相机’应用程序。"]'
- en: '’Correct Answer’: ’C’'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '’正确答案’: ’C’'
- en: 'Figure 31: Case study for multiple-windows interaction (part 2).'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: '图31: 多窗口互动案例研究（第2部分）。'
- en: '(Part 1)
    Software ![Refer to caption](img/1b3130465e5c44857ea181fa925df812.png)
    ‘Description1’: "The video shows a Python 3.7.4 Shell window on a Windows system.
    The user begins by typing the ’print’ function followed by a pair of parentheses.
    Inside the parentheses, the user types a string, ’Hello World’, which is enclosed
    in double quotes. Upon pressing Enter, the Python Shell executes the command and
    outputs the text ’Hello World’ below the command line, indicating that the code
    ran successfully without any errors."'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '(第1部分)
    软件 ![参见说明](img/1b3130465e5c44857ea181fa925df812.png)
    ‘描述1’: "视频展示了Windows系统上的Python 3.7.4 Shell窗口。用户开始输入’print’函数，并跟上一个括号。在括号内，用户输入了一个字符串’Hello
    World’，该字符串用双引号括起来。按下Enter键后，Python Shell执行了该命令，并在命令行下方输出了’Hello World’文本，表明代码成功运行且没有错误。"'
- en: '‘Caption’: "Executing the print command in Python Shell to display ’Hello World’"'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '‘说明’: "在Python Shell中执行打印命令以显示’Hello World’"'
- en: '‘static QA’: "Question": "What version of Python is shown running in the video?"'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态QA’: "问题": "视频中显示的Python版本是什么？"'
- en: '"Answer": "The version of Python running in the video is Python 3.7.4, as indicated
    by the text at the top of the Python Shell window."'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '"答案": "视频中运行的Python版本是Python 3.7.4，正如Python Shell窗口顶部的文本所示。"'
- en: '‘MCQA’: "Question": "What operation does the user perform after typing the
    print command?"'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '‘MCQA’: "问题": "用户在输入打印命令后执行了什么操作？"'
- en: '"Options": ["A. The user saves the file.", "B. The user compiles the code.",
    "C. The user executes the print command.", "D. The user closes the Python Shell."]'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '"选项": ["A. 用户保存文件。", "B. 用户编译代码。", "C. 用户执行打印命令。", "D. 用户关闭Python Shell。"]'
- en: '"Correct Answer": "[[C]] The user executes the print command."'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '"正确答案": "[[C]] 用户执行打印命令。"'
- en: 'Figure 32: Case study for software (part 1).'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '图32: 软件案例研究（第1部分）。'
- en: '(Part 2)
    Software ![Refer to caption](img/1b3130465e5c44857ea181fa925df812.png)
    ‘Description2’: "The video shows a sequence of a user interacting with the Python
    Shell on a Windows system. Initially, the shell is open and waiting for input.
    The user begins by typing ’print*’ and then corrects the typo by removing the
    asterisk, typing ’print’. They then type two quotation marks to prepare for a
    string input. Subsequently, the user moves the cursor between the quotation marks
    and types ’Hello World’ to complete the ’print(Ḧello World)̈’ command. After pressing
    enter, the shell executes the command, and the phrase ’Hello World’ is outputted
    below the command prompt."'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: (第2部分)
    软件 ![参见说明](img/1b3130465e5c44857ea181fa925df812.png)
    ‘描述2’：“视频展示了用户在 Windows 系统上的 Python Shell 交互序列。最初，Shell 打开并等待输入。用户开始输入’print*’，然后通过删除星号来更正错误，输入’print’。接着，他们输入两个引号以准备字符串输入。随后，用户将光标移动到引号之间，输入’Hello
    World’，以完成’print(Ḧello World)̈’命令。按下回车后，Shell 执行了命令，并在命令提示符下方输出了短语’Hello World’。”
- en: '‘Sequential-QA’: "Question": "Can you describe the sequence of actions the
    user took to correct the command and output ’Hello World’?"'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: ‘顺序问答’：“问题”： “你能描述用户纠正命令并输出’Hello World’的操作顺序吗？”
- en: '"Answer": "Certainly. Initially, the user typed ’print*’ but realized the asterisk
    was a mistake. They deleted the asterisk to correct the command to ’print’. Next,
    they entered two quotation marks to indicate the start of a string. The user then
    placed the cursor between the quotation marks and typed the string ’Hello World’.
    Finally, upon pressing enter, the Python Shell executed the command and displayed
    the output ’Hello World’ beneath the command prompt."'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: “答案”： “当然。一开始，用户输入了’print*’，但意识到星号是个错误。他们删除了星号，将命令更正为’print’。接着，他们输入了两个引号以表示字符串的开始。用户随后将光标放在引号之间，并输入了字符串’Hello
    World’。最后，按下回车键后，Python Shell 执行了命令，并在命令提示符下方显示了输出’Hello World’。”
- en: '‘Prediction’: "Question": "What would be the expected GUI change if the user
    were to type another command after the ’Hello World’ output?"'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: ‘预测’：“问题”： “如果用户在’Hello World’输出后输入另一个命令，预期的 GUI 变化会是什么？”
- en: '"Answer": "If the user were to type another command after the ’Hello World’
    output, the expected GUI change would be that the command prompt indicator ’>>>’
    would appear on a new line below the ’Hello World’ output, signaling readiness
    for the next command input. Any subsequent command typed by the user would appear
    after this new prompt, awaiting execution."'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: “答案”： “如果用户在’Hello World’输出后输入另一个命令，预期的 GUI 变化是命令提示符指示符’>>>’会出现在’Hello World’输出下方的新行上，表示准备接受下一个命令输入。用户输入的任何后续命令会出现在这个新提示符之后，等待执行。”
- en: '‘Conversation’: "User 1": "I want to print a different message in Python. How
    can I do that?", "Assistant 1": "To print a different message, you can simply
    type ’print(Ÿour Message Here)̈’ where ’Your Message Here’ is the text you wish
    to print. Make sure you enclose your message in quotation marks and hit enter
    to execute the command."'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: ‘对话’：“用户 1”： “我想在 Python 中打印不同的消息。我该怎么做？", "助手 1”： “要打印不同的消息，你只需输入’print(Ÿour
    Message Here)̈’，其中’Your Message Here’是你希望打印的文本。确保用引号将消息括起来，然后按回车执行命令。”
- en: '"User 2": "What if I want to print ’Goodbye, World’ instead?"'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: “用户 2”： “如果我想打印’Goodbye, World’怎么办？”
- en: '"Assistant 2": "To print ’Goodbye, World’, you would type ’print(G̈oodbye,
    World)̈’ and press enter. The Python Shell will execute the command and display
    ’Goodbye, World’ as the output below the command prompt."'
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: '"助手 2": "要打印’Goodbye, World’，你需要输入’print(G̈oodbye, World)̈’，然后按回车。Python Shell
    将执行该命令，并在命令提示符下方显示’Goodbye, World’作为输出。"'
- en: '‘Reasoning’: "Question": "What command did the user execute to get the output
    in the Python Shell?"'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: '‘推理’: ‘问题’: "用户执行了什么命令以在 Python Shell 中获得输出？"'
- en: '"Options": ["A. print(Hello World)", "B. print(Ḧello World)", "C. print(Ḧello
    World)̈", "D. echo(Ḧello World)̈"]'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: '"选项": ["A. print(Hello World)", "B. print(Ḧello World)", "C. print(Ḧello World)̈",
    "D. echo(Ḧello World)̈"]'
- en: '"Correct Answer": "C",'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '"正确答案": "C",'
- en: 'Figure 33: Case study for software (part 2).'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '图 33: 软件案例研究（第 2 部分）。'
- en: '(Part 1)
    Website ![Refer to caption](img/8693bbac5136e8a75ec13cbe4a65ad5f.png)
    ’Description1’: "The video begins with the Google search results page visible
    on a Windows system browser, displaying the query ’is oatmeal a healthy breakfast’.
    The mouse cursor scrolls down the page, revealing additional search results, and
    the ’People also ask’ section with related questions. The user then scrolls back
    up to the top of the page. Next, the cursor moves to the search bar, and the ’X’
    button is clicked to clear the previous search content, leaving an empty search
    bar. The browser’s suggested searches drop-down menu appears with various related
    search queries. Finally, the video fades to black, indicating the end of the sequence."'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '(第 1 部分)
    网站 ![参考标题](img/8693bbac5136e8a75ec13cbe4a65ad5f.png)
    ’描述1’: "视频以 Windows 系统浏览器上可见的 Google 搜索结果页面开始，展示了查询’燕麦粥是健康早餐吗’。鼠标光标向下滚动页面，显示额外的搜索结果以及包含相关问题的’人们还会问’部分。用户随后滚动回到页面顶部。接下来，光标移动到搜索栏，并点击’X’按钮以清除之前的搜索内容，留下一个空的搜索栏。浏览器的建议搜索下拉菜单出现，显示各种相关的搜索查询。最后，视频渐变为黑色，表示序列的结束。"'
- en: '‘Caption’: ’Navigating Google Search Results and Clearing the Search Query
    on a Windows System Browser’'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: '‘标题’: ’在 Windows 系统浏览器中导航 Google 搜索结果并清除搜索查询’'
- en: '‘static QA’: ’Question’: "What feature snippet is displayed at the top of the
    Google search results for the query ’is oatmeal a healthy breakfast’?"'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: '‘静态 QA’: ‘问题’: "对于查询’燕麦粥是健康早餐吗’Google 搜索结果顶部显示的特性片段是什么？"'
- en: '’Answer’: "The featured snippet at the top of the Google search results for
    the query ’is oatmeal a healthy breakfast’ is from the Harvard T.H. Chan School
    of Public Health website. It includes an excerpt stating ’Whether it’s steel-cut
    or rolled, quick-cooking or instant, oatmeal is good for you, experts say—with
    a few caveats. Oatmeal is rich in fiber, which promotes fullness, eases the insulin
    response, and benefits gut health. It’s also a source of vitamins B and E, and
    minerals such as magnesium.’ This snippet provides a concise summary of the health
    benefits of oatmeal, according to experts, highlighting its nutritional value
    and potential impact on fullness and insulin response. The presence of this snippet
    offers a quick and authoritative answer to the user’s query, showcasing Google’s
    ability to extract relevant information from web pages and present it prominently
    for ease of access."'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: ’答案’：“针对查询‘燕麦粥是否是健康早餐’，Google 搜索结果顶部的精选摘要来自哈佛 T.H. Chan 公共卫生学院网站。摘要中包括一段文字，称‘无论是钢切燕麦还是压缩燕麦，无论是速熟还是即食，专家表示燕麦粥对你有好处，但也有一些注意事项。燕麦粥富含纤维，有助于饱腹，缓解胰岛素反应，并有益于肠道健康。它还是维生素
    B 和 E 以及矿物质如镁的来源。’该摘要简明扼要地总结了专家对燕麦粥健康益处的看法，突出了其营养价值及对饱腹感和胰岛素反应的潜在影响。这个摘要为用户的查询提供了快速且权威的答案，展示了
    Google 从网页中提取相关信息并显著展示以便于访问的能力。”
- en: '‘MCQA’: ’Question’: ’What action did the user take after reviewing the search
    results?’'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: '‘MCQA’: ’问题’: ’用户在查看搜索结果后采取了什么行动？’'
- en: '’Options’: [’A. The user clicked on one of the search results.’, "B. The user
    scrolled through the ’People also ask’ section.", ’C. The user cleared the search
    content in the search bar.’, ’D. The user navigated to a different website.’]'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: ’选项’：[’A. 用户点击了其中一个搜索结果。’, "B. 用户浏览了“人们也会问”部分。", ’C. 用户清除了搜索栏中的搜索内容。’, ’D. 用户导航到其他网站。’]
- en: '‘Correct Answer’: ’[[C]] The user cleared the search content in the search
    bar.’,'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: '‘正确答案’: ’[[C]] 用户清除了搜索栏中的搜索内容。’'
- en: 'Figure 34: Case study for website (part 1).'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 图 34：网站案例研究（第 1 部分）。
- en: '(Part 2)
    Website ![Refer to caption](img/8693bbac5136e8a75ec13cbe4a65ad5f.png)
    ‘Description2’: "The video shows a sequence of actions on a Google search results
    page within a web browser on a Windows system. Initially, the mouse cursor moves
    over a search result discussing the health benefits of oatmeal. Next, the user
    scrolls down, revealing a ’People also ask’ section with questions related to
    oatmeal and a ’Videos’ section showcasing related content. Subsequently, the user
    scrolls back up to the original position, highlighting the same search result
    about oatmeal’s health benefits. Finally, the user moves the cursor to the search
    bar and clicks the ’X’ to clear the previous search content, resulting in a blank
    search bar with suggestions and related searches listed below it. The screen then
    goes black, indicating the end of the video."'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: (第 2 部分)
    网站 ![参见说明](img/8693bbac5136e8a75ec13cbe4a65ad5f.png)
    ’描述2’："视频展示了在Windows系统的Web浏览器中对Google搜索结果页面的一系列操作。最初，鼠标光标移动到讨论燕麦健康益处的搜索结果上。接着，用户向下滚动，显示出一个“人们还会问”部分，其中包含与燕麦相关的问题，以及一个“视频”部分展示相关内容。随后，用户向上滚动回到原始位置，突出显示了相同的关于燕麦健康益处的搜索结果。最后，用户将光标移动到搜索栏并点击“X”以清除之前的搜索内容，结果是一个空白的搜索栏，下面列出了建议和相关搜索。屏幕随后变黑，表示视频结束。"’
- en: '‘Sequential-QA’: ’Question’: "After reading about oatmeal’s health benefits,
    what section of the page did the user explore next and did they return to read
    more about the health benefits afterward?"'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: ‘顺序问答’：’问题’：’在阅读了关于燕麦健康益处的内容后，用户接下来探索了页面的哪个部分？他们是否返回继续阅读关于健康益处的内容？’
- en: '’Answer’: "After reading about the health benefits of oatmeal, the user scrolled
    down to explore the ’People also ask’ section and the ’Videos’ section related
    to oatmeal. Following this, the user scrolled back up to revisit the original
    search result that discussed the health benefits of oatmeal."'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: ’回答’：’在阅读了关于燕麦健康益处的内容后，用户向下滚动探索了“人们还会问”部分和与燕麦相关的“视频”部分。之后，用户向上滚动回访了讨论燕麦健康益处的原始搜索结果。’
- en: '‘Prediction’: ’Question’: ’Based on the sequence of events, what action is
    the user likely to perform after clearing the search bar?’'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: ‘预测’：’问题’：’根据事件的顺序，用户在清除搜索栏后可能会执行什么操作？’
- en: '’Answer’: ’After clearing the search bar, the user is likely to type in a new
    search query or select one of the suggested searches that appear below the search
    bar.’'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: ’回答’：’在清除搜索栏后，用户可能会输入新的搜索查询或选择搜索栏下方出现的建议搜索之一。’
- en: '‘Conversation’: ’User 1’: "I want to find more articles on oatmeal’s health
    benefits. Can you scroll back up?"'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: ’对话’：’用户 1’：’我想找到更多关于燕麦健康益处的文章。你能向上滚动一下吗？’
- en: '’Assistant 1’: "I’ve scrolled back up and the article on oatmeal’s health benefits
    from the Harvard T.H. Chan School of Public Health is highlighted again."'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: ’助手 1’：’我已经向上滚动，并再次突出显示了哈佛T.H. Chan公共卫生学院关于燕麦健康益处的文章。’
- en: '’User 2’: ’Great, now can you clear the search and look for something else?’'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: ’用户 2’：’很好，现在你可以清除搜索并寻找其他内容吗？’
- en: '’Assistant 2’: ’The search content has been cleared, and the search bar is
    now empty, showing a list of related searches and previous search history suggestions
    for a new query.’'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: ’助手 2’：’搜索内容已被清除，搜索栏现在为空，显示相关搜索和之前搜索历史建议列表，以便进行新的查询。’
- en: '’Reasoning’: ’Question’: ’If the user wants to perform a new search after clearing
    the search bar, which of the following actions would they need to take next?’,
    ’Options’: [’A. Scroll down to view more search results’'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: ’推理’：’问题’：’如果用户想在清除搜索栏后执行新的搜索，他们接下来需要采取以下哪个操作？’，’选项’：[’A. 向下滚动以查看更多搜索结果’
- en: ’B. Type a new query into the search bar’, "C. Click on one of the ’People also
    ask’ questions", ’D. Close the browser window’]
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: ’B. 在搜索栏中输入新查询’，“C. 点击‘人们还问’中的一个问题”，’D. 关闭浏览器窗口’
- en: '’Correct Answer’: ’B’,'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: ’正确答案’：’B’，
- en: 'Figure 35: Case study for website (part 2).'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 图 35：网站案例研究（第 2 部分）。
- en: '(Part 1)
    XR ![Refer to caption](img/886a667b721e41a99af090094b9cfb62.png)
    ‘Description1’: "The video showcases a user navigating through various pages within
    the Apple Vision Pro browser on a Windows system. Initially, the browser displays
    the start page with Favorites and Reading List. The user then turns their head
    to the right, which triggers the transition to view a webpage on the right side.
    Following this, the user pinches with both hands to exit the page and then pinches
    with both hands and fingers moving towards the middle to expand the browser’s
    various pages. This reveals multiple open browser tabs side by side. The user
    continues to turn their head left and right to view different pages on each side.
    Lastly, the user selects and expands a specific tab to fill the screen, displaying
    its content."'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: (第 1 部分)
    XR ![参见说明](img/886a667b721e41a99af090094b9cfb62.png)
    ‘描述1’：“该视频展示了用户在 Windows 系统的 Apple Vision Pro 浏览器中浏览各种页面。最初，浏览器显示了带有收藏夹和阅读列表的开始页面。用户随后向右转头，触发切换到右侧的网页。之后，用户双手捏合以退出页面，然后双手捏合并将手指移动到中间以展开浏览器的各个页面。这揭示了多个并排打开的浏览器标签。用户继续左右转头以查看每一侧的不同页面。最后，用户选择并展开一个特定的标签以填满屏幕，显示其内容。”
- en: '‘Caption’: ’Navigating through multiple browser pages using head movement and
    hand gestures in Apple Vision Pro on Windows’'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: ‘说明’：’通过在 Windows 上的 Apple Vision Pro 中使用头部移动和手势浏览多个浏览器页面’
- en: '‘static QA’: ’Question’: "What is the main category listed under the Favorites
    section on the browser’s start page?"'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: ‘静态问答’：’问题’：“浏览器开始页面的收藏夹部分列出的主要类别是什么？”
- en: '’Answer’: "The main category listed under the Favorites section on the browser’s
    start page is ’Perplexity’, denoted by a unique icon, followed by other favorites
    like Instagram and various websites."'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: ’答案’：“浏览器开始页面的收藏夹部分列出的主要类别是’困惑’，用独特的图标表示，其后是其他收藏如 Instagram 和各种网站。”
- en: '‘MCQA’: ’Question’: ’How does the user switch between different open tabs in
    the Apple Vision Pro browser?’'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: ‘MCQA’：’问题’：’用户如何在 Apple Vision Pro 浏览器中切换不同的打开标签？’
- en: '’Options’: [’A. Using keyboard shortcuts’, ’B. Turning their head left and
    right’, ’C. Scrolling with a mouse’, ’D. Typing the tab number’]'
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: ’选项’：[’A. 使用键盘快捷键’，’B. 左右转头’，’C. 使用鼠标滚动’，’D. 输入标签号码’]
- en: '’Correct Answer’: ’[[B]] Turning their head left and right’'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: ’正确答案’：’[[B]] 左右转头’
- en: 'Figure 36: Case study for XR (part 1).'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 图 36：XR 案例研究（第 1 部分）。
- en: '(Part 2)
    XR ![Refer to caption](img/886a667b721e41a99af090094b9cfb62.png)
    ‘Description2’: "The video starts with a full-screen view of a browser interface
    titled ’Apple Vision Pro’ displaying various website thumbnails and bookmarks.
    The user then turns their head to the right, causing the right side of the browser
    to come into view. Next, the user looks at a dot at the bottom of the page and
    pinches with both hands, which causes the browser to exit the full-screen view
    and shrink to a smaller, windowed mode. The user then performs a pinching motion
    with both hands, bringing the fingers towards the middle, which causes the browser’s
    various pages to expand, giving an overview of multiple open tabs. The user again
    turns their head to the right to view the right side page and then to the left
    to view the left side page. Throughout the video, the GUI elements such as tabs,
    the address bar, and website thumbnails respond dynamically to the user’s head
    movements and hand gestures."'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/886a667b721e41a99af090094b9cfb62.png) ‘描述2’: “视频开始时显示的是一个全屏的浏览器界面，标题为‘Apple
    Vision Pro’，展示了各种网站缩略图和书签。随后，用户将头转向右侧，使得浏览器的右侧进入视野。接下来，用户看向页面底部的一个点并用双手捏合，这使得浏览器退出全屏视图并缩小到较小的窗口模式。用户再次用双手捏合，将手指向中间移动，这使得浏览器的各个页面展开，提供多个打开的标签的概览。用户再次将头转向右侧查看右侧页面，然后转向左侧查看左侧页面。在整个视频中，GUI
    元素如标签、地址栏和网站缩略图会动态响应用户的头部运动和手势。”'
- en: '‘Sequential-QA’: ‘Question’: ’What action did the user take immediately after
    turning their head to the right to view the right side page?’'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '‘顺序问答’: ‘问题’: ‘用户在将头转向右侧查看右侧页面后立即进行了什么操作？’'
- en: '‘Answer’: ‘After turning their head to the right to view the right side page,
    the user looked at a dot at the bottom of the page and pinched with both hands,
    which caused the browser to exit full-screen mode and shrink to a smaller windowed
    interface.’'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: ‘在将头转向右侧查看右侧页面后，用户看向页面底部的一个点并用双手捏合，这使得浏览器退出全屏模式并缩小到较小的窗口界面。’'
- en: '‘Prediction’: ‘Question’: ‘If the user pinches with both hands and moves their
    fingers towards the middle after the browser shrinks to a smaller window, what
    will happen next?’'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '‘预测’: ‘问题’: ‘如果用户用双手捏合并将手指向中间移动，在浏览器缩小到较小窗口后，接下来会发生什么？’'
- en: '‘Answer’: "If the user pinches with both hands and moves their fingers towards
    the middle after the browser shrinks, the action will cause the browser’s various
    pages to expand, providing an overview of multiple open tabs."'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '‘回答’: "如果用户用双手捏合并将手指向中间移动，在浏览器缩小后，该操作将导致浏览器的各个页面展开，提供多个打开的标签的概览。"'
- en: '‘Conversation’: ‘User 1’: ‘How do I view all the pages I have open in the browser?’'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '‘对话’: ‘用户 1’: ‘如何查看我在浏览器中打开的所有页面？’'
- en: '‘Assistant 1’: ‘You can view all the open pages by pinching with both hands
    and moving your fingers towards the middle. This will expand the browser and show
    an overview of all the open tabs.’'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: '‘助手 1’: ‘你可以通过用双手捏合并将手指向中间来查看所有打开的页面。这将扩展浏览器并显示所有打开标签的概览。’'
- en: '‘User 2’: ‘Great, and how can I exit this overview?’'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '‘用户 2’: ‘很好，我该如何退出这个概览？’'
- en: '‘Assistant 2’: ‘To exit the overview of open pages, you can look at the dot
    at the bottom of the page and pinch with both hands. This will exit the overview
    and return you to the individual page view.’'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: ‘助手 2’：‘要退出打开页面的概览视图，你可以查看页面底部的点，并用双手捏合。这将退出概览视图，返回到单独页面视图。’
- en: '‘Reasoning’: ‘Question’: ‘How can the user access the options to open a new
    tab or window from the current state?’'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: ‘推理’：‘问题’：‘用户如何从当前状态访问打开新标签页或窗口的选项？’
- en: '‘Options’: [‘A. Turn their head to the left and select the plus sign.’, ‘B.
    Swipe left on the touchpad.’, ‘C. Turn their head to the right and select the
    ‘Done’ button.’, ‘D. Pinch with both hands to exit the current view and access
    the toolbar.’]'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: ‘选项’：[‘A. 将头转向左侧并选择加号。’, ‘B. 在触控板上向左滑动。’, ‘C. 将头转向右侧并选择‘完成’按钮。’, ‘D. 用双手捏合退出当前视图并访问工具栏。’]
- en: '‘Correct Answer’: ‘D’'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: ‘正确答案’：‘D’
- en: 'Figure 37: Case study for XR (part 2).'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 图 37：XR 案例研究（第 2 部分）。
