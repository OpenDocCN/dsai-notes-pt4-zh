<!--yml
category: 未分类
date: 2025-01-11 11:58:37
-->

# CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR

> 来源：[https://arxiv.org/html/2411.04671/](https://arxiv.org/html/2411.04671/)

Kadir Burak Buldu¹1, Süleyman Özdel¹ 2, Ka Hei Carrie Lau¹3, Mengdi Wang¹4, Daniel Saad¹5, Sofie Schönborn¹6,
Auxane Boch¹7, Enkelejda Kasneci¹8, and Efe Bozkir¹9 ¹ Technical University of Munich (TUM), Munich, Germany
1 burak.buldu@tum.de 2 ozdelsuleyman@tum.de 3 carrie.lau@tum.de 4 mengdi.wang@tum.de 5 daniel-saad@tum.de
6 sofie.schoenborn@tum.de 7 auxane.boch@tum.de 8 enkelejda.kasneci@tum.de 9 efe.bozkir@tum.de

###### Abstract

Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering consumer-grade head-mounted displays (HMDs) in an affordable way, it is likely that XR will become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)–powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. In this paper, we provide the community with an open-source, customizable, extensible, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with various LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes the latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: https://gitlab.lrz.de/hctl/cuify

###### Index Terms:

extended reality, virtual reality, augmented reality, Unity, non-player characters, conversational user interfaces, speech-based interaction, large language models.

## I Introduction

Recent advances in computer graphics, hardware, and artificial intelligence have led virtual and augmented reality (VR/AR) systems to become ubiquitous and head-mounted displays (HMDs) to be used more regularly. VR and AR have different use contexts and configurations, and each of them provides different advantages for users. For instance, VR is especially useful for generating simulations to train users in a fully immersive setting [[1](https://arxiv.org/html/2411.04671v1#bib.bib1), [2](https://arxiv.org/html/2411.04671v1#bib.bib2), [3](https://arxiv.org/html/2411.04671v1#bib.bib3), [4](https://arxiv.org/html/2411.04671v1#bib.bib4)]; AR is more convenient when context-aware visual support is overlaid on real-world content [[5](https://arxiv.org/html/2411.04671v1#bib.bib5), [6](https://arxiv.org/html/2411.04671v1#bib.bib6), [7](https://arxiv.org/html/2411.04671v1#bib.bib7)]. While both VR and AR, which are part of the broader field of extended reality (XR), can be useful for everyday life, making such virtual and augmented spaces intelligent often requires significant engineering effort, especially for environmental and social interactions.

Considering intelligent XR spaces, practitioners often utilize non-player characters (NPCs), and these characters interact with users for different purposes [[8](https://arxiv.org/html/2411.04671v1#bib.bib8), [9](https://arxiv.org/html/2411.04671v1#bib.bib9)]. However, single-purpose NPCs may cause users to lose interest after a few interactions, as these characters tend to repeat the same or very similar content, eventually leading users to stop using the XR application. To this end, generative artificial intelligence (AI) and particularly large language models (LLMs) can provide numerous opportunities for XR due to their versatile computational capabilities, as they are trained with a significant portion of the Internet and can generate highly realistic synthetic data. By default, LLMs are utilized for the next-word prediction task; however, they can also be aligned for conversational purposes [[10](https://arxiv.org/html/2411.04671v1#bib.bib10)], and one of the most prevalent examples is ChatGPT, which became publicly available in 2022 [[11](https://arxiv.org/html/2411.04671v1#bib.bib11)]. Since then, the general public has been more heavily exposed to generative AI systems and models. At the same time, the use of LLMs has accelerated in various domains, including medicine [[12](https://arxiv.org/html/2411.04671v1#bib.bib12)], law [[13](https://arxiv.org/html/2411.04671v1#bib.bib13)], and education [[14](https://arxiv.org/html/2411.04671v1#bib.bib14)].

While LLMs and generative AI models can be embedded into XR for different purposes, such as for creating 3D virtual content based on user preferences or for modifying interactive experiences, one of the most straightforward schemes is to embed LLMs into NPCs for speech-based interaction so that they can execute as conversational user interfaces (CUIs). In fact, prior research has utilized LLMs for speech-based NPC-user interactions with speech-to-text (STT) and text-to-speech (TTS) models [[15](https://arxiv.org/html/2411.04671v1#bib.bib15), [16](https://arxiv.org/html/2411.04671v1#bib.bib16), [17](https://arxiv.org/html/2411.04671v1#bib.bib17)]. However, to the best of our knowledge, there is no open-source software that implements the pipeline consisting of STT, LLM, and TTS models for XR in a generic and extendable way. This means that for every XR application that includes LLM-based speech interaction, especially with NPCs, practitioners either implement the aforementioned pipeline from scratch or replicate it from previous projects, likely by also carrying out some modifications.

Considering the aforementioned issue, in this paper, we provide the community with an open-source Unity¹¹1Unity is a widely used, cross-platform game engine that has been developed by Unity Software Inc. package that combines LLMs, STT, and TTS models into a pipeline to enable speech-based interaction. Our package minimizes the latency between the models by utilizing streaming, supports plugging different models and pipelines into multiple NPCs in a single environment, and can prompt the LLMs. Furthermore, it supports both accessing LLMs via application programming interfaces (APIs) and handling open-source LLMs either on local devices or on a separate server. The source code is available on the following repository: https://gitlab.lrz.de/hctl/cuify.

## II Related Work

This section is divided into three subsections to cover the main aspects of interaction techniques in XR, the integration of LLMs in XR, and existing LLM-based open-source tools that support user interaction in XR.

### II-A Interaction in XR

Conventional interaction techniques, such as for desktop interfaces (e.g., mouse and physical keyboard), are unsuitable for the immersive nature of virtual interactions in XR [[18](https://arxiv.org/html/2411.04671v1#bib.bib18)]. Various methods, including controllers, hand gestures, gaze, speech, or a combination of these modalities, have been proposed to interact in 3D spaces [[19](https://arxiv.org/html/2411.04671v1#bib.bib19), [20](https://arxiv.org/html/2411.04671v1#bib.bib20)]. Each method offers certain advantages depending on the interaction context and desired level of immersion. Speech-based interaction complements and is often integrated into other techniques in multimodal interaction to enable intuitive interaction experiences in XR [[21](https://arxiv.org/html/2411.04671v1#bib.bib21)].

One of the most common interaction techniques in virtual spaces is controller-based interaction. These techniques provide users with input devices that are easy to adapt and familiarize [[22](https://arxiv.org/html/2411.04671v1#bib.bib22)]. Similarly, hand gesture-based interactions have gained importance [[23](https://arxiv.org/html/2411.04671v1#bib.bib23)], particularly those utilizing camera-based tracking systems [[24](https://arxiv.org/html/2411.04671v1#bib.bib24), [25](https://arxiv.org/html/2411.04671v1#bib.bib25)] or wearable technologies such as gloves [[26](https://arxiv.org/html/2411.04671v1#bib.bib26)]. These methods allow for natural and intuitive hand movements in virtual spaces and enhance immersion by mimicking real-world interactions.

When considering hands-free interaction techniques, gaze-based interactions have been extensively researched, offering methods such as eye clicking [[27](https://arxiv.org/html/2411.04671v1#bib.bib27), [28](https://arxiv.org/html/2411.04671v1#bib.bib28), [29](https://arxiv.org/html/2411.04671v1#bib.bib29)], eye dwelling [[29](https://arxiv.org/html/2411.04671v1#bib.bib29), [30](https://arxiv.org/html/2411.04671v1#bib.bib30)], and eye blinking [[31](https://arxiv.org/html/2411.04671v1#bib.bib31)] as mechanisms for object selection and control. In addition, gaze interactions are often combined with other input modalities to enhance precision and versatility. For instance, combining gaze with head movements [[32](https://arxiv.org/html/2411.04671v1#bib.bib32)] or hand gestures [[33](https://arxiv.org/html/2411.04671v1#bib.bib33), [34](https://arxiv.org/html/2411.04671v1#bib.bib34)] allows users to perform more complex actions while maintaining focus on virtual scenes.

Speech-based interaction is another method that provides a hands-free alternative, allowing users to interact naturally with the environment [[35](https://arxiv.org/html/2411.04671v1#bib.bib35), [36](https://arxiv.org/html/2411.04671v1#bib.bib36)], especially in scenarios where a physical input is impractical or when users are engaged in other tasks [[37](https://arxiv.org/html/2411.04671v1#bib.bib37)]. Those interactions offer higher adoption rates and better usability, particularly for novices. Recent developments in LLMs can transform speech-based interaction techniques in XR due to the versatile conversational capabilities of these models.

### II-B Large Language Models in XR

Interaction techniques in XR have rapidly evolved with different sensing modalities. Recently, with the spread of LLMs and their applications in various domains, LLM-powered NPCs and interactive objects in XR spaces have started to enable more immersive and intuitive experiences. These advances enhance user interaction through verbal inputs, offering a hands-free, conversational means of engaging with the presented content. To this end, Bozkir et al. [[38](https://arxiv.org/html/2411.04671v1#bib.bib38)] argued for integrating LLMs into XR, emphasizing their potential for enhancing inclusion and engagement while raising concerns about the privacy of voice-enabled interactions. In another work, Liu et al. [[39](https://arxiv.org/html/2411.04671v1#bib.bib39)] presented ClassMeta, LLM-driven interactive virtual classmates, in which the system uses voice commands to encourage student participation in virtual classrooms. The authors demonstrated the capabilities of LLMs in creating dynamic and interactive learning environments that simulate peer interactions. Additionally, Izquierdo-Domenech et al. [[40](https://arxiv.org/html/2411.04671v1#bib.bib40)] combined VR with voice-enabled LLMs to provide context-aware educational experiences, significantly improving learning outcomes through personalized and natural interactions.

Lau et al. [[41](https://arxiv.org/html/2411.04671v1#bib.bib41)] took an approach by leveraging VR and generative AI to revitalize oral traditions, using narrative personalization to reconnect youth with cultural folklore. The authors demonstrated that personalized storytelling in VR significantly boosts engagement and interest in cultural learning, increasing user engagement considerably compared to non-personalized settings. Similarly, Lau et al. [[16](https://arxiv.org/html/2411.04671v1#bib.bib16)] explored using LLM-powered chatbots in VR for heritage education, focusing on traditional Scottish curling in virtual settings. The authors found that LLM-powered chatbots, compared to pre-scripted ones, improved interactivity, engagement, and learning outcomes, highlighting their effectiveness in enhancing cultural heritage dissemination. Additionally, LLM-powered chatbots offered more dynamic learning experiences with higher usability than pre-scripted chatbots.

Beyond speech-based interactions with LLMs, De et al. [[42](https://arxiv.org/html/2411.04671v1#bib.bib42)] introduced the LLM for Mixed Reality (LLMR) framework that is designed to create and modify interactive MR experiences in real time using LLMs. The framework incorporates computational techniques such as scene understanding, task planning, and self-debugging, and it demonstrated a four times lower error rate compared to GPT-4 and received positive usability feedback. These results highlight the effectiveness and usability of the LLMs in MR for different tasks.

TABLE I: Overview of supported models and APIs. The Streaming column indicates whether streaming is supported, and the Local column specifies whether the model is local.

| Name | Speech to Text | Large Language Model | Text to Speech | Streaming | Local |
| OpenAI |  

&#124; Whisper &#124;
&#124; Whisper-tiny (local) &#124;

 |  

&#124; GPT 3.5 &#124;
&#124; GPT 4 &#124;
&#124; GPT 4o &#124;
&#124; GPT 4o-mini &#124;

 | TTS | ✓ | $\times$ |
| Amazon | Transcribe | $\times$ | Polly | ✓ | $\times$ |
| Google | $\times$ |  

&#124; Gemini 1.0 Pro &#124;
&#124; Gemini 1.5 Pro &#124;
&#124; Gemini 1.5 Flash &#124;

 | $\times$ | ✓ | $\times$ |
| Meta | MMS-ASR | LLaMa (local) | MMS-TTS | ✓ | ✓ |
| Hugging Face | ✓ | ✓ | ✓ | ✓ | ✓ |

### II-C LLM-based Interaction Tools for XR

Several open-source tools for XR have emerged to facilitate intuitive and complex interactions. To this end, Voice2Action [[43](https://arxiv.org/html/2411.04671v1#bib.bib43), [44](https://arxiv.org/html/2411.04671v1#bib.bib44)] was proposed to enable voice-driven object manipulation in Unity. It integrates LLMs to enable users to adjust 3D objects with voice commands, making it possible to resize and reposition buildings in virtual environments. Another tool, LLMUnity [[45](https://arxiv.org/html/2411.04671v1#bib.bib45)], supports the implementation of LLM-powered characters in Unity, enhancing real-time interaction by enabling dynamic conversational agents that respond to user input, supporting local LLMs. Furthermore, Talk-With-LLM-In-Unity [[46](https://arxiv.org/html/2411.04671v1#bib.bib46)] combines speech recognition with LLMUnity [[45](https://arxiv.org/html/2411.04671v1#bib.bib45)] and the Google Gemma 2 model [[47](https://arxiv.org/html/2411.04671v1#bib.bib47)], enabling natural language-driven navigation. This package was primarily designed to integrate voice-controlled navigation and interaction in virtual environments; however, it does not support speech-based feedback.

The EdenAI Unity Plugin [[48](https://arxiv.org/html/2411.04671v1#bib.bib48)] offers access to various AI services from Unity projects, including TTS, translation, and sentiment analysis. However, it is a commercial tool and does not provide a full framework for NPCs. Additionally, it raises concerns over costs and privacy as it is a third-party service. While these tools provide partial solutions for LLM-based interactions, none offer a comprehensive framework for NPCs that includes LLMs, STT, and TTS models. The previous solutions also do not provide the option to select models separately for each service, which is essential for addressing the quality of interaction and privacy-sensitive use cases. Considering all of this, we build a generic and extendable CUIfy package for Unity.

## III System Description

We propose CUIfy, an open-source package that combines a backend server with Unity clients to provide easy-to-use LLMs-powered conversational agents in Unity. Our package provides a user-friendly interface for different speech-to-text, text-to-speech, and large language models. It also provides options to select TTS voice types and allows users to utilize system prompts for LLMs. We discuss the technical specifications and system usage in the following subsections.

### III-A Technical Specifications

The CUIfy package consists of two parts: a Python server for processing voice input and a Unity client that creates requests. The Unity client includes a user-friendly configuration interface. Users can choose various local or online models (i.e., through APIs) with different configuration options, which are listed in Table [I](https://arxiv.org/html/2411.04671v1#S2.T1 "TABLE I ‣ II-B Large Language Models in XR ‣ II Related Work ‣ CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR").

The Python server creates separate threads for each incoming socket connection and handles connections simultaneously. Each connection starts with incoming configuration messages, and the server can serve different APIs with different settings for each connection. These simultaneous connections enable the use of different NPCs with different configurations to work in Unity environments with a single server. Each NPC can have a unique voice, system prompt, and conversation history.

The server consists of state-of-the-art STT, TTS, and LLMs (either through APIs or local models) to offer a straightforward process for integrating both individual or publicly available models, whether they are local or online. In addition, using Docker containers ensures that the package provides cross-platform operation without compatibility problems with any local model.

The Unity client uses built-in Unity microphone API and .NET Socket Class [[49](https://arxiv.org/html/2411.04671v1#bib.bib49)] to ensure that it seamlessly works in every native platform. Each object with the client script creates a unique socket connection with the server, allowing for the creation of multiple configurable NPCs. Since the default microphone library only records audio with a specific duration, which is configurable under the Unity editor, the client script eliminates white noises if the input recording is shorter than the recording duration. If the input recording is longer than the recording duration, clients stream the recording to the server to ensure that the processing time latency is short. Furthermore, the server streams the LLM outputs to the client sentence by sentence if supported by the model (and API) and selected by the user.

The server can easily be configured to run on the cloud environment with the correct network setup. The Dockerization process ensures that it can be easily run on any cloud server without struggling with dependencies. The client works seamlessly on Android, Windows, and macOS platforms, with the host on the same or different machine. We tested the CUIfy with Varjo XR-3, Meta Quest 2, and Meta Quest 3 HMDs using Unity version 2022.3.26f1, which is a Long Term Support version.

### III-B System Usage and Guidelines

CUIfy’s server can be easily deployed in a Docker container, which makes the setup process easy and straightforward. The source code is also well-designed and flexible, allowing users to add or edit models easily. The Unity package consists of a client script and additional libraries for working with audio data. It offers an interface in the Unity inspector section. All the APIs, API keys, and other settings are configurable through the inspector. Users can select different models for processing and voices for different NPCs in the same Unity environment.

To get started, users should import the Unity package and run the Python server in Docker or their working environment. The source code and the assets are available in the provided repository. Once the package is added to Unity, the client script can be added to any object. It then needs to be adjusted to how the event will be triggered, such as assigning a button or colliding with an object.

CUIfy supports various online and local models by default. It can be easily extended since the backend server is modular, and adding the new model or API object will suffice. CUIfy supports mainstream APIs and local models, which are provided in Table [I](https://arxiv.org/html/2411.04671v1#S2.T1 "TABLE I ‣ II-B Large Language Models in XR ‣ II Related Work ‣ CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR"). Users can select the desired model, choose the streaming mode, provide the API keys, and give the system prompt for LLM from the Unity editor as depicted in Figure [1](https://arxiv.org/html/2411.04671v1#S3.F1 "Figure 1 ‣ III-B System Usage and Guidelines ‣ III System Description ‣ CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR"). In addition, the Hugging Face Transformers pipeline [[50](https://arxiv.org/html/2411.04671v1#bib.bib50)] is supported, and users can run any LLM model supported by Hugging Face on their local or cloud environment.

![Refer to caption](img/021d8630fc4d75221bee3bea9c29bd0f.png)

Figure 1: A sample screenshot from the Unity inspector.

By default, the server stores the communication history between the server and each client. Therefore, users can chat with NPCs seamlessly and have a personalized conversation. However, if required, users have the flexibility to disable chat history for particular NPCs. By unchecking the option in Unity Inspector, each communication with the NPC will be like new.

Real-time communication with NPCs is essential for immersive experiments in XR as it builds a social presence for the user, the feeling that they are interacting with another person [[51](https://arxiv.org/html/2411.04671v1#bib.bib51)]. CUIfy supports a streaming mode that enables TTS and STT streaming instead of waiting to complete all the audio or text data. In addition, when supported by an LLM API, instead of waiting for the entire LLM output, the TTS API will generate the audio data and send it to the client sentence by sentence. This streaming process ensures that the response time for the whole process is handled in real time.

Investigating the outputs of LLMs is critical because these models, despite their powerful capabilities, can generate misinformation or biased content. It is thus essential to scrutinize their outputs to ensure accuracy, fairness, and reliability. Additionally, analyzing their behavior helps improve the models’ alignment with ethical standards and user expectations [[52](https://arxiv.org/html/2411.04671v1#bib.bib52)]. CUIfy provides detailed logs for each dialog conducted with LLMs. Users can investigate and tune their system prompts to ensure that the NPCs respond as intended.

## IV Discussion and Future Directions

The advancements in LLMs and speech models hold substantial potential for developing speech-based interaction techniques to control virtual environments. They also facilitate personalized, interactive NPCs and dynamic narrations within virtual spaces. These settings have great potential for enhancing collaboration and accessibility, providing features such as real-time translations and transcriptions. XR settings, with their inherently immersive nature, are especially well suited for verbal interactions with NPCs. CUIfy leverages this potential by providing a tool that simplifies the integration of speech-based NPCs into Unity environments, enabling natural and realistic interactions with minimal technical effort. This feature benefits users without coding expertise, allowing an easy-to-use implementation in new and existing projects.

In real-time conversational interactions, key challenges like latency and conversation quality often arise. To address the latency issue, CUIfy employs a streaming technique that allows NPCs to speak as soon as content generation starts without waiting for the entire output to be generated. This approach significantly reduces the latency and will most likely enhance the user experience. While APIs and large local models often offer the best conversation quality, using APIs follows a pay-as-you-go model, and large models require considerable processing power. Our package also supports lightweight models that can run on local devices, though they generally provide slightly lower quality than full-scale models. Currently, CUIfy supports eight LLMs, four STT, and three TTS models, as provided in Table [I](https://arxiv.org/html/2411.04671v1#S2.T1 "TABLE I ‣ II-B Large Language Models in XR ‣ II Related Work ‣ CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR"). As generative AI and LLMs continue to evolve and new models are expected to emerge, CUIfy’s architecture allows the integration of those models and APIs easily, which remains the focus of future work.

## V Conclusion

We introduced a generic, easy-to-use, and extendable Unity package that enables interactive NPCs designed for XR settings. Our package supports several speech-to-text models, LLMs, and text-to-speech models, allowing users to select models based on their preferences and needs while addressing both quality and privacy requirements. The package is optimized to enhance conversation quality and facilitate integration into projects, supporting both APIs and local models. In future work, we plan to expand the number of supported models and conduct additional optimizations to enhance performance.

## Acknowledgments

We acknowledge support from the TUM Think Tank, which contributed to the development of this work.

## References

*   [1] E. Bozkir, D. Geisler, and E. Kasneci, “Assessment of driver attention during a safety critical situation in VR to generate VR-based training,” in *ACM Symposium on Applied Perception*.   ACM, 2019.
*   [2] D. Checa and A. Bustillo, “A review of immersive virtual reality serious games to enhance learning and training,” *Multimedia Tools and Applications*, vol. 79, no. 9, pp. 5501–5527, 2020.
*   [3] G. Makransky and S. Klingenberg, “Virtual reality enhances safety training in the maritime industry: An organizational training experiment with a non-WEIRD sample,” *Journal of Computer Assisted Learning*, vol. 38, no. 4, pp. 1127–1140, 2022.
*   [4] E. Bozkir, P. Stark, H. Gao, L. Hasenbein, J.-U. Hahn, E. Kasneci, and R. Göllner, “Exploiting object-of-interest information to understand attention in VR classrooms,” in *2021 IEEE Virtual Reality and 3D User Interfaces (VR)*, 2021, pp. 597–605.
*   [5] K. Bektaş, J. Strecker, S. Mayer, and K. Garcia, “Gaze-enabled activity recognition for augmented reality feedback,” *Computers & Graphics*, vol. 119, p. 103909, 2024.
*   [6] A. Ajanki, M. Billinghurst, H. Gamper, T. Järvenpää, M. Kandemir, S. Kaski, M. Koskela, M. Kurimo, J. Laaksonen, K. Puolamäki, T. Ruokolainen, and T. Tossavainen, “An augmented reality interface to contextual information,” *Virtual Reality*, vol. 15, no. 2, pp. 161–173, 2011.
*   [7] H. Kim, J. L. Gabbard, A. M. Anon, and T. Misu, “Driver behavior and performance with augmented reality pedestrian collision warning: An outdoor user study,” *IEEE Transactions on Visualization and Computer Graphics*, vol. 24, no. 4, pp. 1515–1524, 2018.
*   [8] G. C. Dobre, M. Gillies, and X. Pan, “Immersive machine learning for social attitude detection in virtual reality narrative games,” *Virtual Reality*, vol. 26, no. 4, pp. 1519–1538, 2022.
*   [9] N. Zargham, M. A. Friehs, L. Tonini, D. Alexandrovsky, E. G. Ruthven, L. E. Nacke, and R. Malaka, “Let’s talk games: An expert exploration of speech interaction with NPCs,” *International Journal of Human–Computer Interaction*, pp. 1–21, 2024.
*   [10] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,” 2023.
*   [11] J. Naughton, “ChatGPT exploded into public life a year ago. now we know what went on behind the scenes,” https://www.theguardian.com/commentisfree/2023/dec/09/chatgpt-ai-pearl-harbor-moment-sam-altman, 2023, last accessed 10/01/2024.
*   [12] Y. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang, “Chatdoctor: A medical chat model fine-tuned on a large language model Meta-AI (LLaMA) using medical domain knowledge,” *Cureus*, vol. 15, no. 6, 2023.
*   [13] I. Cheong, K. Xia, K. J. K. Feng, Q. Z. Chen, and A. X. Zhang, “(A)I am not a lawyer, but…: Engaging legal experts towards responsible LLM policies for legal advice,” in *Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency*.   ACM, 2024.
*   [14] R. Hou, T. Fütterer, B. Bühler, E. Bozkir, P. Gerjets, U. Trautwein, and E. Kasneci, “Automated assessment of encouragement and warmth in classrooms leveraging multimodal emotional features and ChatGPT,” in *Artificial Intelligence in Education*, 2024.
*   [15] A. Shoa, R. Oliva, M. Slater, and D. Friedman, “Sushi with einstein: Enhancing hybrid live events with LLM-Based virtual humans,” in *Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents*.   ACM, 2023.
*   [16] K. H. C. Lau, E. Bozkir, H. Gao, and E. Kasneci, “Evaluating usability and engagement of large language models in virtual reality for traditional scottish curling,” 2024.
*   [17] S. Hajahmadi, L. Clementi, M. D. Jiménez López, and G. Marfia, “ARELE-bot: inclusive learning of spanish as a foreign language through a mobile app integrating augmented reality and ChatGPT,” in *2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)*, 2024.
*   [18] P. A. Rauschnabel, R. Felix, C. Hinsch, H. Shahab, and F. Alt, “What is XR? towards a framework for augmented and virtual reality,” *Computers in Human Behavior*, vol. 133, p. 107289, 2022.
*   [19] W. jun Hou and X. lin Chen, “Comparison of eye-based and controller-based selection in virtual reality,” *International Journal of Human-Computer Interaction*, vol. 37, no. 5, pp. 484–495, 2021.
*   [20] A. Saktheeswaran, A. Srinivasan, and J. Stasko, “Touch? speech? or touch and speech? investigating multimodal interaction for visual network exploration and analysis,” *IEEE Transactions on Visualization and Computer Graphics*, vol. 26, no. 6, p. 2168–2179, 2020.
*   [21] Z. Wang, H. Wang, H. Yu, and F. Lu, “Interaction with gaze, gesture, and speech in a flexibly configurable augmented reality system,” *IEEE transactions on human-machine systems*, vol. 51, no. 5, pp. 524–534, 2021.
*   [22] G. Caggianese, L. Gallo, and P. Neroni, “The vive controllers vs. leap motion for interactions in virtual environments: A comparative evaluation,” in *Intelligent Interactive Multimedia Systems and Services*.   Springer International Publishing, 2019, pp. 24–33.
*   [23] D. Gavgiotaki, S. Ntoa, G. Margetis, K. C. Apostolakis, and C. Stephanidis, “Gesture-based interaction for AR systems: a short review,” in *Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments*, 2023, pp. 284–292.
*   [24] C. R. Naguri and R. C. Bunescu, “Recognition of dynamic hand gestures from 3d motion data using lstm and cnn architectures,” in *2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)*, 2017.
*   [25] A. Ikram and Y. Liu, “Skeleton based dynamic hand gesture recognition using lstm and cnn,” in *Proceedings of the 2020 2nd International Conference on Image Processing and Machine Vision*.   ACM, 2020.
*   [26] Y. Li, J. Huang, F. Tian, H.-A. Wang, and G.-Z. Dai, “Gesture interaction in virtual reality,” *Virtual Reality & Intelligent Hardware*, vol. 1, no. 1, pp. 84–112, 2019.
*   [27] V. Tanriverdi and R. J. K. Jacob, “Interacting with eye movements in virtual environments,” in *Proceedings of the SIGCHI Conference on Human Factors in Computing Systems*.   ACM, 2000.
*   [28] S. Jalaliniya, D. Mardanbeigi, T. Pederson, and D. W. Hansen, “Head and eye movement as pointing modalities for eyewear computers,” in *2014 11th International Conference on Wearable and Implantable Body Sensor Networks Workshops*, 2014.
*   [29] J. P. Hansen, V. Rajanna, I. S. MacKenzie, and P. Bækgaard, “A fitts’ law study of click and dwell interaction by gaze, head and mouse with a head-mounted display,” in *Proceedings of the Workshop on Communication by Gaze Interaction*.   ACM, 2018.
*   [30] J. Blattgerste, P. Renner, and T. Pfeiffer, “Advantages of eye-gaze over head-gaze-based selection in virtual and augmented reality under varying field of views,” in *Proceedings of the Workshop on Communication by Gaze Interaction*.   ACM, 2018.
*   [31] F. Hülsmann, T. Dankert, and T. Pfeiffer, “Comparing gaze-based and manual interaction in a fast-paced gaming task in virtual reality,” in *Proceedings of the Workshop Virtuelle & Erweiterte Realität 2011*, 2011.
*   [32] L. Sidenmark and H. Gellersen, “Eye&head: Synergetic eye and head movement for gaze pointing and selection,” in *Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology*.   ACM, 2019.
*   [33] K. Pfeuffer, B. Mayer, D. Mardanbegi, and H. Gellersen, “Gaze + pinch interaction in virtual reality,” in *Proceedings of the 5th Symposium on Spatial User Interaction*.   ACM, 2017.
*   [34] M. Kytö, B. Ens, T. Piumsomboon, G. A. Lee, and M. Billinghurst, “Pinpointing: Precise head- and eye-based target selection for augmented reality,” in *Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems*.   ACM, 2018.
*   [35] N. Zargham, M. A. Friehs, L. Tonini, D. Alexandrovsky, E. G. Ruthven, L. E. Nacke, and R. Malaka, “Let’s talk games: An expert exploration of speech interaction with npcs,” *International Journal of Human–Computer Interaction*, vol. 0, no. 0, pp. 1–21, 2024.
*   [36] N. Zargham, M. Bonfert, G. Volkmar, R. Porzel, and R. Malaka, “Smells like team spirit: Investigating the player experience with multiple interlocutors in a VR game,” in *Extended Abstracts of the 2020 Annual Symposium on Computer-Human Interaction in Play*.   ACM, 2020, p. 408–412.
*   [37] B. Spittle, M. Frutos-Pascual, C. Creed, and I. Williams, “A review of interaction techniques for immersive environments,” *IEEE Transactions on Visualization and Computer Graphics*, vol. 29, no. 9, pp. 3900–3921, 2022.
*   [38] E. Bozkir, S. Özdel, K. H. C. Lau, M. Wang, H. Gao, and E. Kasneci, “Embedding large language models into extended reality: Opportunities and challenges for inclusion, engagement, and privacy,” in *Proceedings of the 6th ACM Conference on Conversational User Interfaces*.   ACM, 2024.
*   [39] Z. Liu, Z. Zhu, L. Zhu, E. Jiang, X. Hu, K. Peppler, and K. Ramani, “Classmeta: Designing interactive virtual classmate to promote VR classroom participation,” in *Conference on Human Factors in Computing Systems - Proceedings*.   ACM, 2024.
*   [40] J. Izquierdo-Domenech, J. Linares-Pellicer, and I. Ferri-Molla, “Virtual reality and language models: A new frontier in learning,” *International Journal of Interactive Multimedia and Artificial Intelligence*, vol. 8, no. 5, 2024.
*   [41] K. H. C. Lau, B. Yun, S. Saruba, E. Bozkir, and E. Kasneci, “Wrapped in anansi’s web: Unweaving the impacts of generative-AI personalization and VR immersion in oral storytelling,” 2024.
*   [42] F. De La Torre, C. M. Fang, H. Huang, A. Banburski-Fahey, J. Amores Fernandez, and J. Lanier, “LLMR: Real-time prompting of interactive worlds using large language models,” in *Proceedings of the CHI Conference on Human Factors in Computing Systems*, 2024, pp. 1–22.
*   [43] Y. Su, “Voice2action - VR multimodal interaction with llm agents,” 2024, accessed: 7 October 2024\. [Online]. Available: https://yang-su2000.github.io/Voice2Action/
*   [44] S. Yang, “Voice2action: Language models as agent for efficient real-time interaction in virtual reality,” 2023\. [Online]. Available: https://arxiv.org/abs/2310.00092
*   [45] Undream AI, “LLMUnity,” 2024, accessed: 7 October 2024\. [Online]. Available: https://github.com/undreamai/LLMUnity
*   [46] Ali, “Talk With LLM In Unity,” 2024, accessed: 7 October 2024\. [Online]. Available: https://github.com/ali7919/Talk-With-LLM-In-Unity
*   [47] G. Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. Ramé *et al.*, “Gemma 2: Improving open language models at a practical size,” *arXiv preprint arXiv:2408.00118*, 2024\. [Online]. Available: https://arxiv.org/abs/2408.00118
*   [48] Eden AI, “EdenAI Unity Plugin,” 2024, accessed: 7 October 2024\. [Online]. Available: https://github.com/edenai/unity-plugin
*   [49] Microsoft, “System.net.sockets,” https://learn.microsoft.com/en-us/dotnet/api/system.net.sockets.socket?view=net-8.0, accessed: 2024-10-09.
*   [50] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, “Transformers: State-of-the-art natural language processing,” in *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*.   ACL, 2020, pp. 38–45.
*   [51] R. Lege, “A social presence benchmark framework for extended reality (XR) technologies,” *Computers & Education: X Reality*, vol. 4, p. 100062, 2024.
*   [52] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On the dangers of stochastic parrots: Can language models be too big?” in *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency*.   ACM, 2021.