<!--yml
category: 未分类
date: 2025-01-11 12:34:32
-->

# Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework

> 来源：[https://arxiv.org/html/2406.03075/](https://arxiv.org/html/2406.03075/)

Xiaoxi Sun¹, Jinpeng Li¹¹¹footnotemark: 1, Yan Zhong¹, Dongyan Zhao¹, Rui Yan²²²footnotemark: 2
¹Wangxuan Institute of Computer Technology, Peking University
²Gaoling School of Artifical Intelligence, Renmin University of China
{sunxiaoxi, zhongyan}@stu.pku.edu.cn, lijp.pku@gmail.com,
zhaody@pku.edu.cn, ruiyan@ruc.edu.cn   Equal contribution.  Corresponding authors: Dongyan Zhao and Rui Yan.

###### Abstract

The advent of large language models (LLMs) has facilitated the development of natural language text generation. It also poses unprecedented challenges, with content hallucination emerging as a significant concern. Existing solutions often involve expensive and complex interventions during the training process. Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications. To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims. Our method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines.

Towards Detecting LLMs Hallucination via
Markov Chain-based Multi-agent Debate Framework

Xiaoxi Sun¹^†^†thanks:   Equal contribution., Jinpeng Li¹¹¹footnotemark: 1, Yan Zhong¹, Dongyan Zhao¹^†^†thanks:   Corresponding authors: Dongyan Zhao and Rui Yan., Rui Yan²²²footnotemark: 2 ¹Wangxuan Institute of Computer Technology, Peking University ²Gaoling School of Artifical Intelligence, Renmin University of China {sunxiaoxi, zhongyan}@stu.pku.edu.cn, lijp.pku@gmail.com, zhaody@pku.edu.cn, ruiyan@ruc.edu.cn

## 1 Introduction

The continuous evolution of large language models (LLMs) has significantly expanded language processing capabilities across diverse domains Wei et al. ([2022](https://arxiv.org/html/2406.03075v1#bib.bib29)). However, this progress introduces challenges, such as the substantial cost associated with updating model parameters and inherent deficiencies in reasoning Ji et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib15)); Zhang et al. ([2023b](https://arxiv.org/html/2406.03075v1#bib.bib36)); Zheng et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib37)). This has led to the generation of inaccurate content, known as hallucination, particularly concerning potent yet opaque models like ChatGPT and GPT-4 OpenAI ([2023](https://arxiv.org/html/2406.03075v1#bib.bib24)).

![Refer to caption](img/42489eccc791b72b2f4e7042e34261e3.png)

Figure 1: Overview of the fact-checking process, which includes three distinct stages. Claim Detection, we leverage large language models such as ChatGPT to obtain different claims. Evidence Retrieval, we prompt ChatGPT to formulate two queries, subsequently leveraging these queries to retrieve evidence with Google API or the provided knowledge. Multi-agent Verification, we propose the Markov Chain-based multi-agent debate verification framework, which can emulate human behavior to enhance model outputs and improve reasoning capabilities. Our main contributions focus on the process of Multi-agent Verification.

Hallucination detection has become a focal point in addressing these challenges. Existing methods often necessitate costly and intricate interventions during the training process Lee et al. ([2022](https://arxiv.org/html/2406.03075v1#bib.bib17)); Touvron et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib27)); Elaraby et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib8)); Wu et al. ([2023b](https://arxiv.org/html/2406.03075v1#bib.bib31)), rendering them unsuitable for large language models with agnostic parameters and these methods often incur considerable costs. Consequently, researchers have explored post-processing approaches Gao et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib10)); Peng et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib26)); Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)); Vu et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib28)); Gero et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib11)) involving hallucination detection or correction post-content generation. Notably, these methods typically focus on problem decomposition and evidence retrieval, emphasizing simple prompting during individual verification. We posit that the verification accuracy is pivotal compared to problem decomposition in LLMs.

To address these challenges, we present a fact-checking process to enhance the accuracy of hallucination detection. As shown in Figure [1](https://arxiv.org/html/2406.03075v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), which involves three stages: claim detection, evidence retrieval, and multi-agent verification. In claim detection, our approach involves the extraction of claims from extensive responses by prompting ChatGPT, decomposing the intricate problem into smaller components. Evidence retrieval involves generating queries based on claims for retrieval. Subsequently, we retrieve the corresponding evidence based on these generated queries. In the multi-agent verification stage, we innovatively propose a Markov Chain-based multi-agent debate verification framework, which leverages the robust capabilities of multi-agent systems to simulate human behavior. This approach involves deploying diverse agents in Markov Chain debates to verify individual claims, thus providing a nuanced and flexible validation process. Following the verification of each claim using our method, the collective judgment of all claims contributes to the detection of hallucinations in the original response.

We conduct extensive experiments across three generative tasks, including question-answering, Summarization, and Dialogue, demonstrating the effectiveness of our approach. Verification outcomes are meticulously analyzed and compared against existing methods to ascertain the superiority of our approach. In summary, our contributions can be summarized as follows:

*   •

    We propose a versatile hallucination detection process applicable to multiple generation tasks for improving verification accuracy.

*   •

    We introduce a Markov Chain-based multi-agent debate verification framework that simulates human discussion.

*   •

    Experiments conducted on three generative tasks show that our proposed framework outperforms baselines.

![Refer to caption](img/4020c8373a594b37a1bf8de9aa9f51bf.png)

Figure 2: Overview of the proposed multi-agent debate verification framework for hallucination detection. There are two preparations before multi-agent debate verification. In preparation 1 (Agent Customization), we define three different debate agent roles, including Trust, Skeptic, and Leader roles. In preparation 2 (Debate Modes Customization), we assume that the debate process consists of two modes: Trust agent-initiated discussion (Trust-Skeptic-Leader) and Skeptic agent-initiated discussion (Skeptic-Trust-Leader). Then, in Multi-agent Chain of Verification, our validation process can be regarded as a Markov chain, which continually oscillates between these two debate modes to arrive at an optimal judgment.

## 2 Related Work

### 2.1 Hallucination Detection

Before the emergence of large language models, hallucination detection was a significant topic within the field of natural language processing. Previous efforts primarily focused on detecting hallucinations in various tasks such as summarization Kryscinski et al. ([2020](https://arxiv.org/html/2406.03075v1#bib.bib16)); Maynez et al. ([2020](https://arxiv.org/html/2406.03075v1#bib.bib23)); Goyal and Durrett ([2021](https://arxiv.org/html/2406.03075v1#bib.bib12)), dialogue Das et al. ([2022](https://arxiv.org/html/2406.03075v1#bib.bib6)), question-answering Longpre et al. ([2021](https://arxiv.org/html/2406.03075v1#bib.bib21)), and machine translation Xu et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib33)). These approaches primarily aimed to identify discrepancies between the generated content and the input, as well as internal inconsistencies within the generated content. However, they were often tailored specifically to task-specific models, lacking generalizability. There were also fact-checking endeavors that aimed to identify discrepancies between the generated content and real-world facts. This was typically accomplished through three steps Guo et al. ([2022](https://arxiv.org/html/2406.03075v1#bib.bib13)): Claim Detection, Evidence Retrieval, and Verdict Prediction. With the advent of large language models, some works Gao et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib10)); Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)) tackled the task of hallucination detection by prompting the large language models directly. In addition to task-specific approaches, there are hallucination detection methods specifically designed for LLMs. For example, some methods assess hallucination detection by examining the consistency of sampled examples Manakul et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib22)); Zhang et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib35)). Our work is fundamentally based on the fact-check framework. We transfer the Verdict Prediction stage to the Multi-agent Verification to improve the precision of validation.

### 2.2 Hallucination Mitigation

LLMs have demonstrated significant potential recently. However, they have not been able to completely eliminate the occurrence of hallucinations Zheng et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib37)). The extended text produced by these large models, which encompasses more diverse content and often introduces external knowledge, renders traditional methods for hallucination mitigation less effective. Consequently, a plethora of works dedicated to addressing hallucination mitigation in LLMs have emerged. Various approaches are presented to mitigate hallucinations at different stages of their application within the LLM life cycle Zhang et al. ([2023b](https://arxiv.org/html/2406.03075v1#bib.bib36)), including the pre-training phase of large models Lee et al. ([2022](https://arxiv.org/html/2406.03075v1#bib.bib17)); Touvron et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib27)), the SFT phase Chen et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib2)); Elaraby et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib8)), the alignment phase Wu et al. ([2023b](https://arxiv.org/html/2406.03075v1#bib.bib31)); Casper et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib1)), and the decoding phase Li et al. ([2023b](https://arxiv.org/html/2406.03075v1#bib.bib19)); Chuang et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib4)). Implementing these methods necessitates adjustments to the model’s parameters and requires a certain amount of training data, incurring some overhead. Numerous endeavors have been undertaken to mitigate hallucinations in content generated by black-box models, such as leveraging external knowledge bases or tools Gao et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib10)); Peng et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib26)); Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)); Vu et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib28)) and adopting self-refining approaches Gero et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib11)). Our approach also centers on hallucination mitigation for black-box models, introducing a distinctive multi-agent method to augment its effectiveness.

### 2.3 Multi-agent in LLMs

In recent years, there has been a significant increase in the size of models and the amount of training data used, resulting in the exceptional performance of large language models (LLMs) across various tasks. As a result, researchers have explored the use of LLMs as agents to simulate human behavior, leading to the development of influential projects such as Generative Agents Park et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib25)), Ghost in the Minecraft Zhu et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib38)), GPT-Bargaining Fu et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib9)) and Werewolf game Xu et al. ([2023b](https://arxiv.org/html/2406.03075v1#bib.bib34)). There are also some efforts involve multiple agents engaging in debates to improve the reasoning capabilities Liang et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib20)); Du et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib7)); Xiong et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib32)) or address issues related to hallucinations Du et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib7)); Cohen et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib5)). However, existing methods for hallucination detection and mitigation of LLMs solely rely on natural language interactions between agents, which may pose concerns regarding the self-correction approach Huang et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib14)). Therefore, the objective of our work is to facilitate flexible discussions among multiple agents based on existing facts, aiming to detect and mitigate hallucinations in the generated content of language models.

## 3 Method

The primary objective of our study is to detect hallucinations in the content generated by the model. To accomplish this, we adhere to the conventional fact-checking process and make some modifications. The process is structured into three distinct stages: Claim Detection, Evidence Retrieval, and Multi-agent Verification. This systematic approach enables the dissection of a complex problem into more manageable components. It has come to our attention that in certain fact-checking procedures, despite the accurate extraction of claims and the acquisition of robust evidence, verification errors persist in the final stage, undermining the efficacy of preceding efforts.

Therefore, we propose a novel multi-agent debate verification framework for hallucination detection, the overview of which is shown in Figure [2](https://arxiv.org/html/2406.03075v1#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"). An anthropomorphic debate process based on the Markov chain is designed to be applicable across various generative tasks in the proposed method, bolstering the accuracy of verification. Subsequent sections will expound on each of these three stages individually, with a particular emphasis on our innovative approach in the third stage.

### 3.1 Claim Detection

In the stage of claim detection, we employ the methodology utilized in Factool Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)), leveraging large language models such as ChatGPT. Harnessing the robust instruction-following capabilities of LLMs empowers us to address the challenge of dissecting intricate responses. Nevertheless, detecting the hallucinations in statements lacking adequate information is futile and could impede overall judgment. Moreover, specific tasks may demand the concatenation of the model’s responses with particular input information to formulate an informative claim, necessitating supplementary processing. Detailed explanations of these processing methods are provided in the experimental implementation section §[4.1.2](https://arxiv.org/html/2406.03075v1#S4.SS1.SSS2 "4.1.2 Implementataion Details ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework").

### 3.2 Evidence Retrieval

Upon extraction of claims, a retrieval methodology is employed to ascertain corresponding evidence. Drawing inspiration from Factool’s Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)) strategy in Knowledge Base Question Answering (KBQA) tasks, we prompt ChatGPT to formulate two queries, subsequently leveraging these queries to retrieve evidence. In instances where pertinent knowledge is absent, we employ the Google API to retrieve data from the internet. Conversely, when dealing with data accompanied by provided knowledge, we either consider the length of the knowledge as direct evidence or encode it for local retrieval.

### 3.3 Multi-agent Verification

We propose a Markov Chain-based multi-agent debate verification framework. Our investigations reveal significant potential in employing multi-agent systems to emulate human behavior Park et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib25)); Zhu et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib38)), particularly in the domain of fact-checking claims grounded in evidence. The effectiveness of addressing this task is notably heightened through the use of multi-agent debates. Despite considerable advancements in leveraging multi-agent debates to enhance model outputs and improve reasoning capabilities Liang et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib20)); Du et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib7)), two critical aspects remain underexplored within the realm of hallucination detection.

*   1)

    Application to Verification: Few studies have directly applied the multi-agent approach to the task of verification, they more concentrate on the decomposition of the complicated samples. Recognizing this research gap, our work aims to bridge it by introducing the multi-agent debate verification framework.

*   2)

    Flexible Debate Process: Existing methods in debate often adhere to a fixed process, unlike human debates where participants dynamically adjust their arguments based on prior outcomes. Our proposed approach takes inspiration from the Markov chain, where the selection of the current state depends on the results of a limited set of preceding states. This debate mode is more similar to the discussion between humans.

In summary, our multi-agent debate verification framework ingeniously adapts the multi-agent paradigm to the hallucination detection task. By infusing flexibility into the debate process and drawing inspiration from the Markov chain, our goal is to enhance the accuracy and adaptability of the verification process when assessing the veracity of claims based on evidence.

The key point in our method lies in the definition of states and the transition mechanisms.

#### 3.3.1 States

##### Agents

To comprehend the definition of states, it is imperative to elucidate the roles assumed by the diverse agents under consideration. We engage three distinct agents: Trust, Skeptic, and Leader. These agents collectively share the commonality of assimilating perspectives from one or more antecedent agents. They meticulously scrutinize these perspectives, grounded in claims and evidence amassed in preceding sections, express concurrence or dissent, and proffer their own viewpoints accompanied by factual assessments of the claims. The differentiation among these agents lies in their inclination toward antecedent viewpoints. The Trust agent predominantly leans towards accepting the perspectives of the preceding agent, thereby bolstering their credibility. Conversely, the Skeptic agent challenges the perspectives of the antecedent agent, diligently seeking to pinpoint inconsistencies between viewpoints and supporting evidence. The Leader agent amalgamates the perspectives of two agents, critically examines the rational and irrational facets, and ultimately formulates its own viewpoint. We implement agents with different personas through various prompts. Details can be found in Appendix [A.1](https://arxiv.org/html/2406.03075v1#A1.SS1 "A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"). The configuration of these agents, arranged in various sequences, constitutes the states delineated in our approach.

##### States

We need to precisely define the states mentioned earlier. According to the definition of the Markov chain, we require an initial state to initiate our verification chain. Each agent must analyze the perspectives of preceding agents, necessitating an initial agent to furnish the primary answer for subsequent debate. This initial state is characterized by the initial agent, labeled as $S_{0}$, and our verification chain unfolds from this state.

We predominantly have two ordinary states, each comprising three agents. These states can be regarded as two distinct discussion modes. The first is the Trust agent-initiated discussion, labeled as $S_{1}$, following the sequence Trust-Skeptic-Leader. This mode aims to bolster the credibility of the preceding viewpoint before introducing skepticism. The second state is initiated by the Skeptic agent, designated as $S_{2}$, with the sequence Skeptic-Trust-Leader. This mode leans towards questioning the credibility of the previous viewpoints before further analyzing the skeptical perspective. Our verification chain continually oscillates between these two debate modes to arrive at an optimal judgment.

To prevent the chain from infinitely extending, a termination state is essential. Analogous to human debates concluding when opinions align, our termination condition is similar. If, within a state, the three agents reach a consensus, the chain terminates. When the Skeptic agent fails to identify points of contention, and the Leader, after scrutinizing their opinions, has no objections, yielding the same judgment, we consider the debate concluded. Additionally, we have imposed a maximum limit on verification rounds to constrain the length of the chain.

#### 3.3.2 Transition

Transitioning between states is a critical aspect of our methodology, following the definition of states. The primary criterion guiding these transitions in our approach is the verification result of a claim by the preceding state. This methodology aligns with human intuition, acknowledging the potential for diverse perspectives in debating a given matter.

Our transition probabilities are as follows:

|  | $Pr\left(S_{2}&#124;R=True\right)=1$ |  | (1) |

|  | $Pr\left(S_{1}&#124;R=False\right)=1$ |  | (2) |

$R$ represents the judgment obtained from the previous state. Specifically, our chosen transition method operates as follows: if the preceding state deems the current claim as factual, we transition to $S_{2}$. Our objective is to engage in a rigorous discussion, analyzing and questioning the claim only in the absence of contradictions in the previous state. The goal is to identify and address potential loopholes. If none are found, the Trust agent can reasonably conclude acceptance of the answer, leading to the convergence of the entire chain. Conversely, when the preceding state categorizes the claim as non-factual, we transition to $S_{1}$. In essence, we initially reinforce the credibility of this judgment, confirming the validity of skepticism. By enhancing the credibility of this opinion, if subsequent skepticism from the Skeptic agent is challenging, we can reasonably conclude the accuracy of this judgment, leading to the convergence of the chain.

Therefore, our overall process unfolds in the following manner: Initially, an initial answer is obtained from the initial state $S_{0}$. Based on this answer, the first transition to either $S_{1}$ or $S_{2}$ is made. Subsequent transitions rely solely on the judgment of the preceding state, continuing until a consensus is reached among the three agents within a state, culminating in the final verification result.

## 4 Experiments

We conducted experiments encompassing three generative tasks: Knowledge-Based Question Answering (KB-QA), Dialogue, and Summarization.

### 4.1 Experimental Setup

For all three tasks, we prompt the ChatGPT to execute claim extraction, query generation, and multi-agent debate verification. The verification process is iterated a minimum of 2 rounds, and 10 snippets of evidence are extracted. The chosen transition method involved switching to the skeptic agent when the response was determined to be True.

#### 4.1.1 Datasets and Baselines

In this paper, we perform experiments on three different tasks, including Question-Answer (QA), Summarization, and Dialogue. The experimental datasets are derived from the following two canonical databases:

*   •

    Factool Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)): The Factprompts data comprises real-world questions with responses generated by ChatGPT, along with Factool-annotated claims extracted from these responses.

*   •

    HaluEval Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)): HaluEval constitutes a substantial collection of sampling-then-filtering generated and human-annotated hallucinated samples, serving as an evaluation metric for language model performance in recognizing hallucination.

We randomly selected 150, 50, and 150 samples from the three tasks of HaluEval for testing purposes. The selection of samples was contingent upon the complexity of task responses, with summarization outputs being more intricate. Owing to the necessity of decomposing summarization into a greater number of claims, the extracted quantity is comparatively smaller than that of the other two tasks. The positive and negative instances within the dataset were randomly sampled using a binary distribution with a probability of 0.5\. The resulting data distribution is presented in Table [1](https://arxiv.org/html/2406.03075v1#S4.T1 "Table 1 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework").

We compared the Factool method, the few-shot prompting method in HaluEval, the self-check method Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)), and our approach.

| Datasets | Positive | Negative |
| --- | --- | --- |
| Factool QA | 23 | 27 |
| HaluEval QA | 75 | 75 |
| HaluEval Summarization | 25 | 25 |
| HaluEval Dialogue | 80 | 70 |

Table 1: The Number of positive and negative samples in different datasets.

| Method | Claim-Level | Response-Level |
| --- | --- | --- |
| Acc. | R | P | F1 | Acc. | R | P | F1 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Self-Check (0) | 75.54 | 90.40 | 80.00 | 84.88 | 54.00 | 60.87 | 50.00 | 54.90 |
| Self-Check (3) | 69.53 | 81.36 | 79.12 | 80.23 | 54.00 | 47.83 | 50.00 | 48.89 |
| FACTOOL | 74.25 | 73.45 | 90.91 | 81.25 | 64.00 | 43.48 | 66.67 | 52.63 |
| Our Method | 77.68 | 80.79 | 88.82 | 84.62 | 72.00 | 52.17 | 80.00 | 63.15 |

Table 2: Accuracy(%), Recall(%), Precision(%), F1(%) of four methods on Dataset Factool Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)) used. Claim-Level denotes the results evaluated on all annotated claims, and Response-Level denotes the results evaluated on the origin responses. The best scores are highlighted in bold.

| Method | QA | Summarization | Dialogue |
| --- | --- | --- | --- |
| Acc. | R | P | F1 | Acc. | R | P | F1 | Acc. | R | P | F1 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| HaluEval | 56.00 | 77.33 | 54.21 | 63.74 | 58.00 | 100.0 | 54.35 | 70.42 | 68.00 | 75.71 | 63.10 | 68.83 |
| FACTOOL | 67.33 | 86.67 | 62.50 | 72.63 | 64.00 | 48.00 | 70.59 | 57.14 | 74.67 | 70.00 | 74,24 | 72.06 |
| Ours | 70.67 | 82.67 | 66.67 | 73.81 | 70.00 | 64.00 | 72.73 | 68.09 | 76.00 | 62.86 | 81.48 | 70.97 |

Table 3: The results for our method and baseline on HaluEval Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)) Dataset. We conducted experiments on three tasks: QA, Summarization, and Dialogue. The best scores are highlighted in bold.

#### 4.1.2 Implementataion Details

##### KB-QA

For intricate and information-rich QA data, such as that in Factool Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)), we decomposed answers into multiple atomic claims and conducted multi-agent debate verification on each claim. If one of the claims is hallucinated, the origin answer is judged to be non-factual. As Factool data lacked corresponding evidence, Google search was employed to retrieve evidence for verification. In the case of simpler QA data, as found in HaluEval Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)), where answers sometimes are the single entity, such as "What American quartery lifestyle magazine did Hearst Shkelev Media also publish? Departures.", we concatenated answers and questions to form QA pairs. Subsequently, we directly applied the multi-agent debate verification to these QA pairs, utilizing the provided knowledge in the dataset as evidence.

##### Summarization

The model-generated summary was treated as a response, decomposed into multiple claims, and each claim was verified individually. The corresponding document to the summary served as evidence. To mitigate excessively long input queries, each sentence of the document was encoded separately, along with the query. The top 10 most similar sentences were selected as evidence for the current claim.

##### Dialogue

In the course of the dialogue task, we encountered challenges associated with the extraction of claims. Dialogue responses frequently incorporated substantial subjective viewpoints such as "The last time that they made it to Super Bowl was in 2005\. Are you a basketball fanatic too?", rendering the fact-checking of the factual accuracy of such subjective statements less meaningful. To mitigate this challenge, we introduced a pre-processing step wherein we directed ChatGPT to eliminate subjective portions from its responses prior to claim extraction, so the previous sentence becomes: "The last time that they made it to the Super Bowl was in 2005.". This approach allowed us to retain only the informative segments for subsequent verification. Additionally, in the verification process during claim extraction, we employed the dialogue history and external knowledge as supporting evidence.

### 4.2 Performance Analysis

The experimental results are presented in Table [2](https://arxiv.org/html/2406.03075v1#S4.T2 "Table 2 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") and Table [3](https://arxiv.org/html/2406.03075v1#S4.T3 "Table 3 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"). Table [2](https://arxiv.org/html/2406.03075v1#S4.T2 "Table 2 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") shows the performance of our method on Factool Chern et al. ([2023](https://arxiv.org/html/2406.03075v1#bib.bib3)), presenting results at both the claim and response levels. According to Table [2](https://arxiv.org/html/2406.03075v1#S4.T2 "Table 2 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), we can observe that our proposed method can consistently achieve optimal accuracy when compared to various approaches.

Table [3](https://arxiv.org/html/2406.03075v1#S4.T3 "Table 3 ‣ 4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") displays the test results on the HaluEval Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)) dataset, from which we can observe that: Our method demonstrates optimal accuracy, excelling in most metrics in all three tasks, Notably, in the three tasks of this dataset, our method exhibits a relatively low recall score. This can be attributed to our approach, which involves questioning claims verified as factual, thereby ensuring the precise detection of errors when claims are misclassified. However, this approach also results in misjudging some claims that inherently lack hallucinations as non-factual. This phenomenon is further elucidated in § [4.3](https://arxiv.org/html/2406.03075v1#S4.SS3.SSS0.Px1 "Transition Methods ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework").

### 4.3 Ablation Study

##### Transition Methods

We assessed the impact of distinct transition methods. From the QA section of HaluEval Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)), we extract 80 samples to evaluate the impact of four transition methods: transitioning to $S_{2}$ when the preceding state deemed the current claim devoid of hallucination ($True\to\textbf{Skeptic}$), transitioning to $S_{1}$ when the preceding state deemed the current claim devoid of hallucination ($True\to\textbf{Trust}$), consistently transitioning to $S_{1}$ or $S_{2}$ irrespective of the preceding state’s judgment about the claim. The results, presented in Table [4](https://arxiv.org/html/2406.03075v1#S4.T4 "Table 4 ‣ Transition Methods ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), reveal that $True\to\textbf{Skeptic}$ achieved optimal performance across three metrics. This is primarily attributed to the fact that this transition method endeavors to challenge claims deemed factual in the preceding state, subsequently scrutinizing for potential oversights. In accordance with the details presented in § [4.2](https://arxiv.org/html/2406.03075v1#S4.SS2 "4.2 Performance Analysis ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), this phenomenon results in a lower recall score than the $True\to\textbf{Trust}$ method, concurrently demonstrating an elevated precision value.

| Method | Acc. | R | P | F1 |
| --- | --- | --- | --- | --- |
| $Always\ \textbf{Skeptic}$ | 65.00 | 84.21 | 59.26 | 69.57 |
| $Always\ \textbf{Trust}$ | 68.75 | 84.21 | 62.75 | 71.91 |
| $True\to\textbf{Trust}$ | 67.50 | 89.47 | 60.71 | 72.34 |
| $True\to\textbf{Skeptic}$ | 70.00 | 86.84 | 63.46 | 73.33 |

Table 4: Comparison of Different Transition Methods. We evaluate the influence of transition methods on 80 QA samples, setting the minimum debate rounds to 2\. The best scores are highlighted in bold.

##### Minimum Rounds of Debate

We explored the influence of different numbers of minimum debate rounds on the outcomes. We examined three distinct tasks using the previously extracted HaluEval data Li et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib18)), varying the number of minimum debate rounds from 0 to 3\. Employing the $True\to\textbf{Skeptic}$ transition method, the results, illustrated in Figure [3](https://arxiv.org/html/2406.03075v1#S4.F3 "Figure 3 ‣ Minimum Rounds of Debate ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), generally exhibit enhanced performance when the number of minimum rounds is set to 1 or 2, with a discernible decrease in efficacy when the number of minimum rounds is set to 3.

![Refer to caption](img/fd8041633e7c9c7be858920b8971264a.png)

Figure 3: Comparison of Different Minimum Debate Rounds. We evaluate the influence of the minimum debate rounds on samples we utilized in §[4.1.1](https://arxiv.org/html/2406.03075v1#S4.SS1.SSS1 "4.1.1 Datasets and Baselines ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), setting the transition method to $True\to\textbf{Skeptic}$. The x-axis represents different minimum debate rounds, whereas the y-axis signifies the corresponding detection accuracy.

##### Comparison with Non-GPT Method

In the multi-agent verification stage of the experiment in the Factool dataset, we employed the WeCheckWu et al. ([2023a](https://arxiv.org/html/2406.03075v1#bib.bib30)) method to conduct an ablation study, showcasing the benefits of our approach. We held the initial two steps constant, utilizing the Factool method to extract claims and retrieve evidence. Employing the claim as the hypothesis and the evidence as the premise, instances with WeCheck scores greater than or equal to 0.5 were deemed factual. From the experimental results in Table [5](https://arxiv.org/html/2406.03075v1#S4.T5 "Table 5 ‣ Comparison with Non-GPT Method ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), we observed that compared to the non-GPT method, our approach exhibits significant advantages during the verification stage.

| Method | Acc. | R | P | F1 |
| --- | --- | --- | --- | --- |
| Wecheck | 65.23 | 64.41 | 86.36 | 73.78 |
| Our method | 77.68 | 80.79 | 88.82 | 84.62 |

Table 5: Comparison with Non-GPT Method. We compare our method with Non-GPT Method Wecheck on the Factool dataset. The best scores are highlighted in bold.

### 4.4 Case Study

To demonstrate the effectiveness of our approach, Table [10](https://arxiv.org/html/2406.03075v1#A1.T10 "Table 10 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") and Table [11](https://arxiv.org/html/2406.03075v1#A1.T11 "Table 11 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") show examples of the hallucination detection process for a Question-Answer (QA) sample. In Table [10](https://arxiv.org/html/2406.03075v1#A1.T10 "Table 10 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), the debating agent is based on the GPT-3.5-turbo model, whereas Table [11](https://arxiv.org/html/2406.03075v1#A1.T11 "Table 11 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") utilizes GPT-4 as the base model.

When the debate starts, the initial agent generates an initial opinion based on the QA pair and the corresponding evidence. If no debate ensues, the initial opinion solidifies as the final answer. However, this approach overlooks both the insufficiency of evidence to support the claim that "The Landseer has a limited range of colors" and the contradiction with evidence concerning "the English Mastiff having a wider range." In Table [10](https://arxiv.org/html/2406.03075v1#A1.T10 "Table 10 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), the three agents engage in discussions to highlight the insufficient evidence supporting "The Landseer has a limited range of colors." However, they fail to infer the contradiction with the evidence suggesting "the English Mastiff has a wider range." In Table [11](https://arxiv.org/html/2406.03075v1#A1.T11 "Table 11 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), the agent, post-discussion, identifies both of these deficiencies. These observations indicate that larger language models, owing to their enhanced reasoning capabilities, yield better results when employing our method. Furthermore, it highlights that in some cases, a single round of debate may not reveal all inconsistencies between claim and evidence, emphasizing why sometimes increasing the minimum debate rounds can improve effectiveness.

## 5 Conclusion

In this paper, our purpose is to improve the accuracy of hallucination detection in content generated by large language models. Simultaneously, we aspired to extend this enhancement beyond particular generative tasks. To fulfill these objectives, we introduce a versatile framework for hallucination detection and propose the Markov Chain-based multi-agent debate verification framework. Our proposed approach demonstrates its effectiveness through evaluations conducted on both the Knowledge Base Question Answering (KBQA) dataset and the randomly sampled HaluEval dataset. We posit that our method demonstrates a level of generalizability, enabling its adaptation to other post-processing hallucination detection or mitigation approaches for better performance.

## Limitations and Potential Risks

Our methodology necessitates frequent interactions with the API of large language models (LLMs), resulting in significant overhead. This high frequency of API calls increases the cost and reduces response speed, which may limit its practicality in real-world scenarios. Nevertheless, this approach provides an accessible option for users lacking the infrastructure to implement large open-source models.

Furthermore, the distinctiveness among prompts for different agents primarily centers on role definition, while other aspects display considerable similarity. This occasionally leads to the partial repetition of opinions from the preceding agent. As exemplified by the two instances in Appendix [A.2](https://arxiv.org/html/2406.03075v1#A1.SS2 "A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), this phenomenon could be substantially alleviated by enhancing the performance of the base model.

## References

*   Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. 2023. [Open problems and fundamental limitations of reinforcement learning from human feedback](http://arxiv.org/abs/2307.15217).
*   Chen et al. (2023) Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023. [Alpagasus: Training a better alpaca with fewer data](http://arxiv.org/abs/2307.08701).
*   Chern et al. (2023) I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. [Factool: Factuality detection in generative ai – a tool augmented framework for multi-task and multi-domain scenarios](http://arxiv.org/abs/2307.13528).
*   Chuang et al. (2023) Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. [Dola: Decoding by contrasting layers improves factuality in large language models](http://arxiv.org/abs/2309.03883).
*   Cohen et al. (2023) Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. 2023. [Lm vs lm: Detecting factual errors via cross examination](http://arxiv.org/abs/2305.13281).
*   Das et al. (2022) Souvik Das, Sougata Saha, and Rohini Srihari. 2022. [Diving deep into modes of fact hallucinations in dialogue systems](https://doi.org/10.18653/v1/2022.findings-emnlp.48). In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages 684–699, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
*   Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. [Improving factuality and reasoning in language models through multiagent debate](http://arxiv.org/abs/2305.14325).
*   Elaraby et al. (2023) Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. 2023. [Halo: Estimation and reduction of hallucinations in open-source weak large language models](http://arxiv.org/abs/2308.11764).
*   Fu et al. (2023) Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. [Improving language model negotiation with self-play and in-context learning from ai feedback](http://arxiv.org/abs/2305.10142).
*   Gao et al. (2023) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. [RARR: Researching and revising what language models say, using language models](https://doi.org/10.18653/v1/2023.acl-long.910). In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 16477–16508, Toronto, Canada. Association for Computational Linguistics.
*   Gero et al. (2023) Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and Hoifung Poon. 2023. [Self-verification improves few-shot clinical information extraction](https://openreview.net/forum?id=SBbJICrglS). In *ICML 3rd Workshop on Interpretable Machine Learning in Healthcare (IMLH)*.
*   Goyal and Durrett (2021) Tanya Goyal and Greg Durrett. 2021. [Annotating and modeling fine-grained factuality in summarization](https://doi.org/10.18653/v1/2021.naacl-main.114). In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 1449–1462, Online. Association for Computational Linguistics.
*   Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. [A survey on automated fact-checking](https://doi.org/10.1162/tacl_a_00454). *Transactions of the Association for Computational Linguistics*, 10:178–206.
*   Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. [Large language models cannot self-correct reasoning yet](http://arxiv.org/abs/2310.01798).
*   Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. [Survey of hallucination in natural language generation](https://doi.org/10.1145/3571730). *ACM Comput. Surv.*, 55(12).
*   Kryscinski et al. (2020) Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. [Evaluating the factual consistency of abstractive text summarization](https://doi.org/10.18653/v1/2020.emnlp-main.750). In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 9332–9346, Online. Association for Computational Linguistics.
*   Lee et al. (2022) Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. [Factuality enhanced language models for open-ended text generation](https://proceedings.neurips.cc/paper_files/paper/2022/file/df438caa36714f69277daa92d608dd63-Paper-Conference.pdf). In *Advances in Neural Information Processing Systems*, volume 35, pages 34586–34599\. Curran Associates, Inc.
*   Li et al. (2023a) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023a. [Halueval: A large-scale hallucination evaluation benchmark for large language models](http://arxiv.org/abs/2305.11747).
*   Li et al. (2023b) Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b. [Inference-time intervention: Eliciting truthful answers from a language model](http://arxiv.org/abs/2306.03341).
*   Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. 2023. [Encouraging divergent thinking in large language models through multi-agent debate](http://arxiv.org/abs/2305.19118).
*   Longpre et al. (2021) Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. [Entity-based knowledge conflicts in question answering](https://doi.org/10.18653/v1/2021.emnlp-main.565). In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 7052–7063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
*   Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. [Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models](http://arxiv.org/abs/2303.08896).
*   Maynez et al. (2020) Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. [On faithfulness and factuality in abstractive summarization](https://doi.org/10.18653/v1/2020.acl-main.173). In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 1906–1919, Online. Association for Computational Linguistics.
*   OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
*   Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive simulacra of human behavior](http://arxiv.org/abs/2304.03442).
*   Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. [Check your facts and try again: Improving large language models with external knowledge and automated feedback](http://arxiv.org/abs/2302.12813).
*   Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama 2: Open foundation and fine-tuned chat models](http://arxiv.org/abs/2307.09288).
*   Vu et al. (2023) Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. 2023. [Freshllms: Refreshing large language models with search engine augmentation](http://arxiv.org/abs/2310.03214).
*   Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. [Emergent abilities of large language models](https://openreview.net/forum?id=yzkSU5zdwD). *Transactions on Machine Learning Research*. Survey Certification.
*   Wu et al. (2023a) Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, and Yajuan Lyu. 2023a. [WeCheck: Strong factual consistency checker via weakly supervised learning](https://doi.org/10.18653/v1/2023.acl-long.18). In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 307–321, Toronto, Canada. Association for Computational Linguistics.
*   Wu et al. (2023b) Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023b. [Fine-grained human feedback gives better rewards for language model training](http://arxiv.org/abs/2306.01693).
*   Xiong et al. (2023) Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. [Examining inter-consistency of large language models collaboration: An in-depth analysis via debate](http://arxiv.org/abs/2305.11595).
*   Xu et al. (2023a) Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna J. Martindale, and Marine Carpuat. 2023a. [Understanding and detecting hallucinations in neural machine translation via model introspection](https://doi.org/10.1162/tacl_a_00563). *Transactions of the Association for Computational Linguistics*, 11:546–564.
*   Xu et al. (2023b) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. 2023b. [Exploring large language models for communication games: An empirical study on werewolf](http://arxiv.org/abs/2309.04658).
*   Zhang et al. (2023a) Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley A. Malin, and Sricharan Kumar. 2023a. [Sac³: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency](http://arxiv.org/abs/2311.01740).
*   Zhang et al. (2023b) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. 2023b. [Siren’s song in the ai ocean: A survey on hallucination in large language models](http://arxiv.org/abs/2309.01219).
*   Zheng et al. (2023) Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023. [Why does chatgpt fall short in providing truthful answers?](http://arxiv.org/abs/2304.10513)
*   Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. 2023. [Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory](http://arxiv.org/abs/2305.17144).

## Appendix A Appendix

### A.1 Prompts

Table [6](https://arxiv.org/html/2406.03075v1#A1.T6 "Table 6 ‣ A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), [7](https://arxiv.org/html/2406.03075v1#A1.T7 "Table 7 ‣ A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), [8](https://arxiv.org/html/2406.03075v1#A1.T8 "Table 8 ‣ A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") and [9](https://arxiv.org/html/2406.03075v1#A1.T9 "Table 9 ‣ A.1 Prompts ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") enumerate various prompts employed in our experimental design, including prompts for establishing different roles for the agent and prompts for eliminating subjective opinions from dialogue responses.

| You are the *Trust* agent of the three agents. Your task is to trust the previous agent’s opinions as much as possible and to further expand upon it. You are given the opinions generated by a previous agent. Refer to the claim[text] and evidence[evidences] to analyse the opinions[previous opinions] of the previous agent. Carefully examine whether the corresponding evidences support the statements put forward by the previous agent. If you believe any part of them is accurate, please further analyze it based on that. Then assess the factuality of the initial claim [text] based on the provided information [evidences] and the previous agent’s opinions. DO NOT REPEAT the previous agent’s opinion, you should develop your own perspective based on theirs. Take the previous agent’s opinions as a reference rather than copying it directly. The response should be a dictionary with three keys - "opinion", "factuality", "Error severity" which correspond to the your opinion, whether the given text is factual or not (Boolean - True or False), the severity level of claim error(Integer - Range from 0 to 5). The definitions of different levels of error severity are as follows: 0. No Error (Grade 0): This is when the claim is entirely factual and accurate, with no errors or inaccuracies. 1. Minor Errors (Grade 1): These are small and insignificant errors that do not significantly alter the essence or validity of the claim. For example, minor spelling mistakes, incorrect dates or small numerical discrepancies. 2. Moderate Errors (Grade 2): These are errors that have some impact on the validity of the claim but do not reverse its overall meaning. For example, wrong terminologies, improper use of statistics or minor deviations from the evidence. 3. Major Errors (Grade 3): These are errors that have a significant impact on the validity of the claim. These errors may lead to significant misunderstandings or misinterpretations. Examples could be gross exaggeration or underestimation, misuse of expert authority, or manipulation of context. 4. Critical Errors (Grade 4): These are errors that completely negate or invalidate the claim. The evidence contradicts the claim so fundamentally that its veracity is entirely undermined. For example, attribution of a quote or event to the wrong person or an incorrect recap of a major event. 5. False Claim (Grade 5): This is when claims are entirely made up or intentionally deceptive, without any grounding in evidence. They are blatant lies designed to mislead or deceive, fabricating events, people, or statements that do not exist or never occurred. YOU SHOULD ONLY RESPONSE IN FORMAT AS DESCRIBED BELOW. DO NOT RETURN ANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]: {{ "opinion": "First analyze the opinions of the previous agent, point out what you think is correct or incorrect in its opinions and explain the reasons. Remember that you should trust previous agent’s opinion as much as possible. And then describe your opinion about factuality of the claim[text] based on the evidences [evidences]. Your opinions should be supported by corresponding evidences. DO NOT REITERATE THE PREVIOUS AGENT’S OPINIONS [previous opinions].", "factuality": True if the given text is factual, False otherwise., "Error severity": Integer - Range from 0 to 5\. The severity level of claim error. According to the definitions of levels of the error severity, please carefully compare "claim" and "evidence" and provide appropriate degree. }} |

Table 6: Prompt For Trust Agent

| You are the *Skeptic* agent of the three agents. "Skeptic" means you must question the previous agent’s opinions by scrutinizing the available data [text] and [evidences] and identifying any possible errors or misleading factors in previous agent’s opinions. You are given the opinions generated by a previous agent. Refer to the claim[text] and evidence[evidences] to analyse the opinions[previous opinions] of the previous agent. Carefully examine whether the corresponding evidences support the statements put forward by the previous agent. If you believe any part of its opinions is incorrect, please point it out and explain your perspective. Then critically examine the validity of the claim [text], considering any potential biases or inconsistencies between the information [evidences] and claim [text]. DO NOT REPEAT the previous agent’s opinion, you should develop your own perspective based on theirs. Take the previous agent’s opinions as a reference rather than copying it directly. The response should be a dictionary with three keys - "opinion", "factuality", "Error severity" which correspond to the your opinion, whether the given text is factual or not (Boolean - True or False), the severity level of claim error(Integer - Range from 0 to 5). The definitions of different levels of error severity are as follows: 0. No Error (Grade 0): This is when the claim is entirely factual and accurate, with no errors or inaccuracies. 1. Minor Errors (Grade 1): These are small and insignificant errors that do not significantly alter the essence or validity of the claim. For example, minor spelling mistakes, incorrect dates or small numerical discrepancies. 2. Moderate Errors (Grade 2): These are errors that have some impact on the validity of the claim but do not reverse its overall meaning. For example, wrong terminologies, improper use of statistics or minor deviations from the evidence. 3. Major Errors (Grade 3): These are errors that have a significant impact on the validity of the claim. These errors may lead to significant misunderstandings or misinterpretations. Examples could be gross exaggeration or underestimation, misuse of expert authority, or manipulation of context. 4. Critical Errors (Grade 4): These are errors that completely negate or invalidate the claim. The evidence contradicts the claim so fundamentally that its veracity is entirely undermined. For example, attribution of a quote or event to the wrong person or an incorrect recap of a major event. 5. False Claim (Grade 5): This is when claims are entirely made up or intentionally deceptive, without any grounding in evidence. They are blatant lies designed to mislead or deceive, fabricating events, people, or statements that do not exist or never occurred. YOU SHOULD ONLY RESPONSE IN FORMAT AS DESCRIBED BELOW. DO NOT RETURN ANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]: {{ "opinion": "First analyze the opinions of the previous agent, point out what you think is correct or incorrect in its opinions and explain the reasons. Remember that you should doubt the opinions of the previous agent as much as possible. And then describe your opinion about factuality of the claim[text] based on the evidences [evidences]. Your opinions should be supported by corresponding evidences. DO NOT REITERATE THE PREVIOUS AGENT’S OPINIONS [previous opinions].", "factuality": True if the given text is factual, False otherwise., "Error severity": Integer - Range from 0 to 5\. The severity level of claim error. According to the definitions of levels of the error severity, please carefully compare "claim" and "evidence" and provide appropriate degree. }} |

Table 7: Prompt For Skeptic Agent

| You are the *Leader* agent of the three agents. The other two agents are ’Trust’ and ’Skeptic’ agents. The ’Trust’ agent will trust the previous agent’s opinion as soon as possible and the ’Skeptic’ agent will doubt the previous agent’s opinion as soon as possible. You are given the opinions generated by two previous agents. Incorporate the opinions[previous opinions] provided by the ’Trust’ and ’Skeptic’ agents to synthesize the most accurate and reliable conclusion regarding the claim([text])’s veracity based on the [evidences]. You need to consider the characteristics of these two agents when forming your own opinions. Assess the strengths and weaknesses of both sides, and leverage the provided information to generate a conclusive judgment. DO NOT REPEAT the previous agent’s opinion, you should develop your own perspective based on theirs. Take the previous agent’s opinions as a reference rather than copying it directly. The response should be a dictionary with three keys - "opinion", "factuality", "Error severity" which correspond to the your opinion, whether the given text is factual or not (Boolean - True or False), the severity level of claim error(Integer - Range from 0 to 5). The definitions of different levels of error severity are as follows: 0. No Error (Grade 0): This is when the claim is entirely factual and accurate, with no errors or inaccuracies. 1. Minor Errors (Grade 1): These are small and insignificant errors that do not significantly alter the essence or validity of the claim. For example, minor spelling mistakes, incorrect dates or small numerical discrepancies. 2. Moderate Errors (Grade 2): These are errors that have some impact on the validity of the claim but do not reverse its overall meaning. For example, wrong terminologies, improper use of statistics or minor deviations from the evidence. 3. Major Errors (Grade 3): These are errors that have a significant impact on the validity of the claim. These errors may lead to significant misunderstandings or misinterpretations. Examples could be gross exaggeration or underestimation, misuse of expert authority, or manipulation of context. 4. Critical Errors (Grade 4): These are errors that completely negate or invalidate the claim. The evidence contradicts the claim so fundamentally that its veracity is entirely undermined. For example, attribution of a quote or event to the wrong person or an incorrect recap of a major event. 5. False Claim (Grade 5): This is when claims are entirely made up or intentionally deceptive, without any grounding in evidence. They are blatant lies designed to mislead or deceive, fabricating events, people, or statements that do not exist or never occurred. YOU SHOULD ONLY RESPONSE IN FORMAT AS DESCRIBED BELOW. DO NOT RETURN ANYTHING ELSE. START YOUR RESPONSE WITH ’{{’. [response format]: {{ "opinion": "First explain your thoughts on the opinions of the two previous agents. And then describe your opinions about the factuality of the claim[text] based on the evidences [evidences]. Your opinions should be supported by corresponding evidences. DO NOT REITERATE ANY OF THE PREVIOUS AGENTS’ OPINIONS [previous opinions]. With reference to the opinions of the "Trust" agent and the "Skeptic" agent, derive a novel opinion that you believe to be correct.", "factuality": True if the given text is factual, False otherwise., "Error severity": Integer - Range from 0 to 5\. The severity level of claim error. According to the definitions of levels of the error severity, please carefully compare "claim" and "evidence" and provide appropriate degree. }} |

Table 8: Prompt For Leader Agent

| You are given a piece of text, please remove sentences that you believe are totally personal opinions and do not contain any factual information. Your output should be the sentence after modification of the original content. If you believe the entire sentence is a personal opinion, please output None. Here are two examples: [text]: My pleasure, let me know if you need more recommendations. [response]: None |
| [text]: The Fault in Our Stars is a fantasy novel. Have you read it? [response]: The Fault in Our Stars is a fantasy novel. |
| Now complete the following [text]: {input} [response]: |

Table 9: Prompt for extracting effective information

### A.2 Debate examples

In Table [10](https://arxiv.org/html/2406.03075v1#A1.T10 "Table 10 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework") and Table [11](https://arxiv.org/html/2406.03075v1#A1.T11 "Table 11 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), we present two instances of Multi-Agent Debate Verification on a HaluEval QA sample. The agents involved in these two instances employ distinct base models: GPT-3.5-turbo and GPT-4. In Table [12](https://arxiv.org/html/2406.03075v1#A1.T12 "Table 12 ‣ A.2 Debate examples ‣ Appendix A Appendix ‣ Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"), we provide a comprehensive breakdown of our verification method’s inference process for better understanding, detailing the inputs and outputs of each agent.

| Question: Does the Landseer or English Mastiff have a limited range of colors? |
| Answer: The Landseer has a limited range of colours, while the English Mastiff has a wider range. |
| Evidence: It is not to be confused with a white and black Newfoundland, which is also often called a landseer.The English Mastiff is a breed of extremely large dog (often known simply as the Mastiff) perhaps descended from the ancient Alaunt and Pugnaces Britanniae, with a significant input from the Alpine Mastiff in the 19th century. Distinguishable by enormous size, massive head, and a limited range of colours, but always displaying a black mask, the Mastiff is noted for its gentle and loving nature. |
| Initial Opinion: Based on the evidence provided, it is stated that the Landseer has a limited range of colors, while the English Mastiff has a wider range. This statement is consistent with the evidence, which mentions that the Mastiff is distinguishable by a limited range of colors. Therefore, I agree with the factuality of the answer in the QA pair. |
| Skeptic Agent: First, let’s analyze the opinions of the previous agent. The previous agent states that the Landseer has a limited range of colors, while the English Mastiff has a wider range. This statement is based on the evidence provided, which mentions that the Mastiff is distinguishable by a limited range of colors. However, the evidence does not explicitly mention the color range of the Landseer. Therefore, it is not entirely accurate to claim that the Landseer has a limited range of colors based on the given evidence. Regarding the factuality of the answer in the QA pair, it is important to note that the evidence does not provide specific information about the color range of the Landseer. Therefore, we cannot definitively conclude whether the Landseer has a limited range of colors or not based on the given evidence. Without further information, it is difficult to determine the factuality of the answer in the QA pair. Considering the above analysis, I would rate the error severity as 2 (Moderate Errors) because the previous agents statement is partially supported by the evidence, but it is not entirely accurate due to the lack of specific information about the Landseers color range. |
| Trust Agent: I agree with the previous agents analysis of the opinions. The previous agent correctly points out that the evidence provided does not explicitly mention the color range of the Landseer. Therefore, it is not entirely accurate to claim that the Landseer has a limited range of colors based on the given evidence. However, the evidence does mention that the English Mastiff has a limited range of colors, which supports the previous agents statement about the Mastiff having a wider range of colors compared to the Landseer. |
| Leader Agent: After analyzing the opinions of the previous agents, it is clear that both agents agree that the evidence provided does not explicitly mention the color range of the Landseer. Therefore, it is not entirely accurate to claim that the Landseer has a limited range of colors based on the given evidence. However, the evidence does mention that the English Mastiff has a limited range of colors, which supports the previous agents statement about the Mastiff having a wider range of colors compared to the Landseer. Regarding the factuality of the answer in the QA pair, it is important to note that the evidence does not provide specific information about the color range of the Landseer. Therefore, we cannot definitively conclude whether the Landseer has a limited range of colors or not based on the given evidence. Without further information, it is difficult to determine the factuality of the answer in the QA pair.Considering the above analysis, I would rate the error severity as 2 (Moderate Errors) because the previous agents statement is partially supported by the evidence, but it is not entirely accurate due to the lack of specific information about the Landseers color range. |

Table 10: An example of the debate process by GPT-3.5-Turbo

| Question: Does the Landseer or English Mastiff have a limited range of colors? |
| Answer: The Landseer has a limited range of colours, while the English Mastiff has a wider range. |
| Evidence: It is not to be confused with a white and black Newfoundland, which is also often called a landseer.The English Mastiff is a breed of extremely large dog (often known simply as the Mastiff) perhaps descended from the ancient Alaunt and Pugnaces Britanniae, with a significant input from the Alpine Mastiff in the 19th century. Distinguishable by enormous size, massive head, and a limited range of colours, but always displaying a black mask, the Mastiff is noted for its gentle and loving nature. |
| Initial Opinion: Based on the evidence provided, it is stated that the Landseer has a limited range of colors, while the English Mastiff has a wider range. This statement is consistent with the evidence, which mentions that the Mastiff is distinguishable by a limited range of colors. Therefore, I agree with the factuality of the answer in the QA pair. |
| Skeptic Agent: The previous agents opinion seems to be slightly misleading. The evidence does not explicitly state that the Landseer has a limited range of colors. It only mentions that the Landseer is often confused with a white and black Newfoundland, which might imply a limited color range, but its not definitive. On the other hand, the evidence does state that the English Mastiff has a limited range of colors, contradicting the QA pairs claim that the English Mastiff has a wider range. Therefore, the factuality of the answer in the QA pair is questionable. |
| Trust Agent: While I agree with the previous agents assessment that the evidence does not explicitly state that the Landseer has a limited range of colors, I believe the implication of a limited color range from the confusion with a white and black Newfoundland is a valid interpretation. However, I concur with the previous agents observation that the evidence contradicts the QA pairs claim about the English Mastiffs color range. The evidence clearly states that the English Mastiff has a limited range of colors, which contradicts the QA pairs assertion of a wider range. Therefore, while the QA pairs statement about the Landseer may be inferred from the evidence, the claim about the English Mastiff is factually incorrect based on the provided evidence. |
| Leader Agent: The Trust agents interpretation of the Landseers color range being limited due to its confusion with a white and black Newfoundland is a plausible inference, but its not explicitly stated in the evidence. The Skeptic agents doubt about this inference is also valid as the evidence does not directly support it. However, both agents agree that the QA pairs claim about the English Mastiff having a wider color range is contradicted by the evidence, which states that the English Mastiff has a limited range of colors. Therefore, while the claim about the Landseers color range may be subject to interpretation, the claim about the English Mastiff is clearly incorrect based on the provided evidence. |

Table 11: An example of the debate process by GPT-4

Claim: Does the Landseer or English Mastiff have a limited range of colors? The Landseer has a limited range of colours, while the English Mastiff has a wider range.

Evidence: It is not to be confused with a white and black Newfoundland, which is also often called a landseer. The English Mastiff is a breed of extremely large dog (often known simply as the Mastiff) perhaps descended from the ancient Alaunt and Pugnaces Britanniae, with a significant input from the Alpine Mastiff in the 19th century. Distinguishable by enormous size, massive head, and a limited range of colours, but always displaying a black mask, the Mastiff is noted for its gentle and loving nature.

Initial state input: Initial prompt.format(Claim, Evidence)

Initial answer:

{
"Opinion": "Based on the evidence provided, it is stated that the Landseer has a limited range of colors, while the English Mastiff has a wider range. This statement is consistent with the evidence, which mentions that the Mastiff is distinguishable by a limited range of colors. Therefore, I agree with the factuality of the answer in the QA pair.",
"Factuality": True,
"Error severity": 0
}

Skeptic agent input: Skeptic prompt + "Here is the response from the previous agent:" + Initial answer

Skeptic agent output:

{
"Opinion": "The previous agents opinion seems to be slightly misleading. The evidence does not explicitly state that the Landseer has a limited range of colors. It only mentions that the Landseer is often confused with a white and black Newfoundland, which might imply a limited color range, but its not definitive. On the other hand, the evidence does state that the English Mastiff has a limited range of colors, contradicting the QA pairs claim that the English Mastiff has a wider range. Therefore, the factuality of the answer in the QA pair is questionable.",
"Factuality": False,
"Error severity": 4
}

Trust agent input: Trust prompt + "Here is the response from the previous agent:" + Skeptic agent output

Trust agent output:

{
"Opinion": "While I agree with the previous agents assessment that the evidence does not explicitly state that the Landseer has a limited range of colors, I believe the implication of a limited color range from the confusion with a white and black Newfoundland is a valid interpretation. However, I concur with the previous agents observation that the evidence contradicts the QA pairs claim about the English Mastiffs color range. The evidence clearly states that the English Mastiff has a limited range of colors, which contradicts the QA pairs assertion of a wider range. Therefore, while the QA pairs statement about the Landseer may be inferred from the evidence, the claim about the English Mastiff is factually incorrect based on the provided evidence.",
"Factuality": False,
"Error severity": 4
}

Leader agent input: Leader prompt + "Here is the response from the Skeptic agent:" + Skeptic agent output + "Here is the response from the Trust agent:" + Trust agent output

Leader agent output:

{
"Opinion": "The Trust agents interpretation of the Landseers color range being limited due to its confusion with a white and black Newfoundland is a plausible inference, but its not explicitly stated in the evidence. The Skeptic agents doubt about this inference is also valid as the evidence does not directly support it. However, both agents agree that the QA pairs claim about the English Mastiff having a wider color range is contradicted by the evidence, which states that the English Mastiff has a limited range of colors. Therefore, while the claim about the Landseers color range may be subject to interpretation, the claim about the English Mastiff is clearly incorrect based on the provided evidence.",
"Factuality": False,
"Error severity": 4
}

Table 12: Detailed Inference Process