<!--yml
category: 未分类
date: 2025-01-11 11:52:53
-->

# SAUP: Situation Awareness Uncertainty Propagation on LLM Agent

> 来源：[https://arxiv.org/html/2412.01033/](https://arxiv.org/html/2412.01033/)

Qiwei Zhao¹, Xujiang Zhao², Yanchi Liu², Wei Cheng², Yiyou Sun²,
Mika Oishi³, Takao Osaki³, Katsushi Matsuda³, Huaxiu Yao¹, Haifeng Chen²
¹University of North Carolina at Chapel Hill, ²NEC Labs America, ³NEC Corporation
qiwei@cs.unc.edu, xuzhao@nec-labs.com 

###### Abstract

Large language models (LLMs) integrated into multistep agent systems enable complex decision-making processes across various applications. However, their outputs often lack reliability, making uncertainty estimation crucial. Existing uncertainty estimation methods primarily focus on final-step outputs, which fail to account for cumulative uncertainty over the multistep decision-making process and the dynamic interactions between agents and their environments. To address these limitations, we propose SAUP (Situation Awareness Uncertainty Propagation), a novel framework that propagates uncertainty through each step of an LLM-based agent’s reasoning process. SAUP incorporates situational awareness by assigning situational weights to each step’s uncertainty during the propagation. Our method, compatible with various one-step uncertainty estimation techniques, provides a comprehensive and accurate uncertainty measure. Extensive experiments on benchmark datasets demonstrate that SAUP significantly outperforms existing state-of-the-art methods, achieving up to 20% improvement in AUROC.

SAUP: Situation Awareness Uncertainty Propagation on LLM Agent

Qiwei Zhao¹, Xujiang Zhao², Yanchi Liu², Wei Cheng², Yiyou Sun², Mika Oishi³, Takao Osaki³, Katsushi Matsuda³, Huaxiu Yao¹, Haifeng Chen² ¹University of North Carolina at Chapel Hill, ²NEC Labs America, ³NEC Corporation qiwei@cs.unc.edu, xuzhao@nec-labs.com

## 1 Introduction

Large language models (LLMs) Minaee et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib23)) have demonstrated remarkable capabilities and, when integrated into agent systems Wang et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib31)), enable complex decision-making processes and broader applications. However, while LLM-based agents are increasingly effective, their outputs are not always reliable, which can lead to significant issues, particularly in high-stakes environments such as healthcare or autonomous systems. This makes uncertainty estimation critical, as it evaluates the reliability of an agent’s decisions and outputs Chang et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib5)); Raiaan et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib26)). Understanding and quantifying uncertainty is essential because it offers insight into potential system failures, providing a safeguard for sensitive applications. Current methods for estimating uncertainty in LLM-based agents remain limited. For example, UALA Han et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib15)) proposes a one-step uncertainty measurement to estimate the uncertainty of the final step before the agent provides an answer.

A key challenge is that uncertainty accumulates over time in multi-step processes, rather than in isolated actions, and is further exacerbated in dynamic environments where external factors are uncontrollable. These interactions can significantly impact the system’s overall uncertainty. Therefore, robust methods that account for various information sources and interaction complexities are necessary to accurately capture the uncertainty across an agent’s entire decision-making process. As illustrated in Figure [1](https://arxiv.org/html/2412.01033v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), in sensitive contexts, solely observing the final step’s uncertainty may lead to overconfidence in the outcome, resulting in adverse consequences and highlighting the importance of considering intermediate uncertainties and the quality of interaction between the agent and its environment.

![Refer to caption](img/640214b5324e911747d014a4b2ec9010.png)

Figure 1: The overall uncertainty of an agent based on large language models (LLMs) can arise from two primary sources: a) Uncertainty Across All Steps: Encompassing both intermediate and final steps; and b) The Agent’s Situational Context: Including the quality of its interaction with the environment and deviations from the optimal logical path. In this example: A user installs security cameras and captures footage of a neighbor entering her garage without permission. She asks an LLM-based agent whether this footage can be used in court. The agent first searches for information on surveillance laws, identifying a definition related to intelligence and crime prevention. It then concludes that the footage qualifies as evidence, based on this research. However, the agent overlooks critical legal factors such as privacy laws and rules on admissibility of evidence, leading to an incorrect conclusion.

To estimate LLM uncertainty, previous approaches focus mainly on the variance of the final step’s output at the token, sentence, or semantic level. Predictive entropy Gal and Ghahramani ([2016](https://arxiv.org/html/2412.01033v1#bib.bib12)); Gal et al. ([2017](https://arxiv.org/html/2412.01033v1#bib.bib13)), initially used in image data, was extended to language models to predict uncertainty in output tokens Xiao and Wang ([2021](https://arxiv.org/html/2412.01033v1#bib.bib36)). Although likelihood can also indicate uncertainty, Malinin and Gales ([2020](https://arxiv.org/html/2412.01033v1#bib.bib22)) introduces normalized entropy, accounting for the output length. Kuhn et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib19)) proposes semantic entropy, incorporating linguistic invariances within shared meanings. Kadavath et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib18)); Yin et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib39)) explore self-assessment by LLMs to estimate uncertainty. However, these methods, designed for traditional one-step QA, do not directly apply to LLM agents. They face two key issues: first, they only consider the final step’s uncertainty, ignoring the accumulation of uncertainty throughout the process; second, they overlook the reasoning process of LLM agents, which is critical in multi-step decision-making and the agent’s interaction with its environment.

To address the challenges of uncertainty in multi-step processes within complex environments, we introduce SAUP (Situation-Awareness Uncertainty Propagation). SAUP comprehensively estimates uncertainty in LLM-based agents by propagating uncertainty through the multi-step reasoning and decision-making process. It builds upon frameworks like ReACT Yao et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib38)), which integrates LLMs’ reasoning into problem-solving by decomposing tasks into thinking, acting, and observing steps. SAUP propagates uncertainties from the initial stages to the final step and aggregates them using a situation-weighting scheme, where each step’s uncertainty is weighted based on the agent’s situation, progress, and observation quality. Since directly measuring an agent’s situation is challenging, we design effective surrogates that are adaptable to various scenarios.

The primary contribution of this paper can be summarized as follows: Firstly, We propose SAUP, a simple yet effective pipeline for providing comprehensive situation-aware uncertainty estimation in multi-step agents within complex environments. Unlike existing single-step uncertainty estimation methods, SAUP accounts for the agent’s situational context throughout problem-solving, rather than focusing solely on the final step. Secondly, To estimate the agent’s unobservable situation, we introduce surrogate methods, which excel in estimating situational uncertainty and offer potential applications in related fields. Lastly, We evaluate SAUP on benchmark datasets such as HotpotQA Yang et al. ([2018](https://arxiv.org/html/2412.01033v1#bib.bib37)), StrategyQA Geva et al. ([2021](https://arxiv.org/html/2412.01033v1#bib.bib14)), and MMLU Hendrycks et al. ([2020](https://arxiv.org/html/2412.01033v1#bib.bib16)). SAUP outperforms state-of-the-art methods, achieving up to a 20% improvement in AUROC, demonstrating its effectiveness.

## 2 Related Works

### 2.1 LLM-based Agent

The reasoning capabilities of LLMs have prompted researchers to explore their use as the core of agent reasoning. Nakano et al. Nakano et al. ([2021](https://arxiv.org/html/2412.01033v1#bib.bib24)) made an early attempt to employ LLMs as agents with web search and information retrieval capabilities, transitioning LLMs from passive tools to proactive agents interacting with complex environments. Subsequent works Wang et al. ([2021](https://arxiv.org/html/2412.01033v1#bib.bib32)); Chen et al. ([2021](https://arxiv.org/html/2412.01033v1#bib.bib6)) explored LLMs in code generation for software development. Yao et al. Yao et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib38)) introduced the ReAct pipeline, utilizing LLMs for decision-making where agents retrieve external information before making decisions. This framework, mirroring human decision-making, became foundational for decision-making agents, inspiring improvements by Shinn et al. Shinn et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib30)) and Renze et al. Renze and Guven ([2024](https://arxiv.org/html/2412.01033v1#bib.bib28)) through self-reflection. Li et al. Li et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib20)) proposed CAMEL, which expanded the framework to enable communication between agents, fostering collaboration. Similarly, AutoGen Wu et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib33)) allows agents to converse and collaborate with customizable interactions in natural language and code. To further enhance decision-making, Qiao et al. Qiao et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib25)) incorporated tool-based monitoring to refine agent behaviors.

### 2.2 Uncertainty in Large Language Models

LLMs dominate numerous fields, including as agents Zhao et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib40)); Xi et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib34)), but targeted uncertainty estimation methods for LLM-based agents remain unexplored. Existing techniques focus on one-step output uncertainty, originating from traditional language models, such as methods to improve model calibration Xiao and Wang ([2019](https://arxiv.org/html/2412.01033v1#bib.bib35), [2021](https://arxiv.org/html/2412.01033v1#bib.bib36)); Jiang et al. ([2021](https://arxiv.org/html/2412.01033v1#bib.bib17)). Token-level uncertainty estimation in "white-box" LLMs Malinin and Gales ([2020](https://arxiv.org/html/2412.01033v1#bib.bib22)); Fomicheva et al. ([2020](https://arxiv.org/html/2412.01033v1#bib.bib11)); Darrin et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib8)); Duan et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib9)) has advanced, with Kuhn et al. Kuhn et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib19)) introducing semantic equivalence into these calculations. Additionally, self-estimation of uncertainty in both "white-box" and "black-box" LLMs, accessed via APIs, has been explored Kadavath et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib18)); Yin et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib39)); Chen et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib7)). These methods focus on one-step uncertainty estimation, which can be integrated into the SAUP framework as the backbone for uncertainty assessments.

## 3 SAUP: Situational Awareness Uncertainty Propagation

![Refer to caption](img/4afb01e8d229800c1ca580ed92292df7.png)

Figure 2: Overview of our proposed SAUP, which is illustrated in three parts. Left depicts the general pipeline of LLM-based multi-step agents interacting with their environment. This process typically involves three behaviors: thinking, action, and observation. The $D_{a}$ represents the distance between the question and the combination of thinking, action, and observation, whereas $D_{o}$ denotes the distance between the observation and the thinking/action. Bottom Right illustrates the agent’s situational weight estimation. Here, we employ a Hidden Markov Model (HMM) to estimate the situational weight based on the distances $D_{a}$ and $D_{o}$. Top Right shows the process of weighted uncertainty propagation, where we aggregate the one-step uncertainty and the corresponding situational weight to derive the agent’s overall uncertainty.

We propose our pipeline, SAUP, with the goal of accurately estimating the overall agent’s uncertainty by comprehensively considering the uncertainty at each step and the corresponding situational weights, as described in Figure [2](https://arxiv.org/html/2412.01033v1#S3.F2 "Figure 2 ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"). In the following sections, we delve into the details, elucidating how we aggregate the uncertainty from each step and estimate the corresponding situational weights.

### 3.1 Weighted Uncertainty Propagation

Uncertainty Propagation. As depicted in the left part of Figure [2](https://arxiv.org/html/2412.01033v1#S3.F2 "Figure 2 ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), for each step $i$, the agent provides the thinking/action with the corresponding uncertainty $U_{i}$ based on the previous state $Z_{i-1}$ and the question $Q$. Considering only the uncertainty of the last step as the overall uncertainty $U_{agent}$ is unreasonable and not comprehensive. Instead, we should comprehensively consider and propagate the uncertainties of all steps. The simplest example is using an arithmetic mean of the uncertainty across the steps before the agent gives the final answer. For robustness against outliers, accurate reflection of central tendency, and consistency in proportional changes, the geometric mean or Root Mean Square (RMS) can be a better choice compared to the arithmetic mean.

Situational Uncertainty Weights. Based on the intuitive logic of information flow and experimental observations, we have identified that the contribution of uncertainty at different steps to the overall agent uncertainty is not uniform. Therefore, in addition to the uniform aggregation scheme introduced earlier, it is essential to design a more comprehensive weighting aggregation scheme for overall uncertainty, tailored to the characteristics of the agent.

During the process of obtaining the final answer, the LLM-based agent produces uncertainty. We refer to the contribution of the current step’s uncertainty to the overall uncertainty, due to the agent’s situation, as the situational weights. Situational weights are determined by factors, such as deviations from the appropriate logical path and the quality of interactions between the agent and the environment, which influence the correctness of the final answer. These situational weights are variable during the agent’s problem-solving process and its interaction with the environment. Assume that the uncertainty at step $i$ is $U_{i}$ and the corresponding situational weight is $W_{i}$, the formula of weighted uncertainty propagation is:

|  | $U_{\text{agent}}=\sqrt{\frac{1}{N}\sum_{i=1}^{N}\left((W_{i}U_{i})^{2}\right)}$ |  | (1) |

Here we choose the RMS as the propagation method. In the practical application of SAUP, besides the above linear term, we also utilize an extra logical term for numerical stability. We designed the SAUP formula based on the following considerations. First, SAUP relies on a comprehensive consideration of all steps of the agent based on propagation. Second, by introducing situational weights for the uncertainty of different steps, SAUP allows for a more complete assessment of the impact of specific steps on the overall uncertainty of the agent. In the following section [3.2](https://arxiv.org/html/2412.01033v1#S3.SS2 "3.2 Step Uncertainty Estimation ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent") and [3.3](https://arxiv.org/html/2412.01033v1#S3.SS3 "3.3 Agent Uncertainty Estimation ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), we will introduce the method for calculating the uncertainty $U_{i}$ and the situational weight $W_{i}$ corresponding to each step.

### 3.2 Step Uncertainty Estimation

From equation [1](https://arxiv.org/html/2412.01033v1#S3.E1 "In 3.1 Weighted Uncertainty Propagation ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), we can see that essentially, our SAUP is compatible with all single-step uncertainty estimation methods applicable to various scenarios, including but not limited to the ones we mentioned. SAUP is built upon these one-step methods.

In the practical implementation, we utilize the normalized entropy Malinin and Gales ([2020](https://arxiv.org/html/2412.01033v1#bib.bib22)), with some modifications to adapt it to the characteristics of the React Agent pipeline. This choice is based on the consideration that normalized entropy has broad applicability. It can not only be applied to open-source LLMs, such as LLAMA, where complete logits of the output are accessible, but can also be utilized with LLMs that are accessible only via API, such as the CHATGPT series. In addition, it is computationally efficient and demonstrates strong predictive performance for single-step uncertainty estimation.

For step $n$ and question $Q$, we denote the agent’s thinking as $T_{n}$, and the corresponding action as $A_{n}$. The observation $O_{n}$ is the information gained from the environment through the action $A_{n}$. Let the LLM be denoted as $L_{\theta}$, and the trajectory of the previous $n-1$ steps as $Z_{n-1}$, where $Z_{n-1}=\{(A_{1},T_{1},O_{1}),\ldots,(A_{n-1},T_{n-1},O_{n-1})\}$. The LLM will output the thinking $T_{n}$ and the action $A_{n}$ together as:

|  | $(T_{n},A_{n})=L_{\theta}(Q,Z_{n-1})$ |  | (2) |

Suppose $T_{n}$ consists of the first $N$ tokens, while $A_{n}$ consists of the following $M$ tokens. The uncertainty $U_{n}$ for step $n$ is computed by the following equation:

|  |  | $\displaystyle U_{n}=\frac{1}{N+M}\prod_{i\leq N+M}p(t_{i}\mid t_{0},\ldots,t_{% i-1};\theta)$ |  | (3) |
|  |  | $\displaystyle=\frac{1}{N+M}\sum_{i\leq N+M}log\ p(t_{i}\mid t_{0},\ldots,t_{i-% 1};\theta)$ |  |

Where $p(t_{i}\mid t_{0},\ldots,t_{i-1};\theta)$ is the token probability of token i, given the previous $\text{token 0},\dots,\text{token i-1}$, and the parameters $\theta$ of the LLM $L$.

### 3.3 Agent Uncertainty Estimation

Assigning weights $W_{i}$ to each step’s uncertainty $U_{i}$ in a multi-step reasoning process is crucial for accurate overall uncertainty estimation. In LLM-based agents, effective reasoning significantly influences decision-making. However, these agents may exhibit overconfidence, making it essential to evaluate their situational state properly. Since the situational state is not directly observable, surrogate measures are used to approximate it. One idea is to assign greater weight to steps closer to the final answer or to measure deviation from an ideal trajectory. While these approaches have merit, they do not fully capture the agent’s true situational state.

To address this limitation, we propose learning-based surrogates that target the agent’s hidden situations. Among these, the Hidden Markov Model (HMM) Distance Surrogate (SAUP-HMMD) learns step transitions and assigns weights based on hidden state estimations. HMMs stand out for their minimal data requirements and computational efficiency, making HMMD the preferred surrogate in cases where training data is limited. In contrast, more complex sequence-to-sequence (S2S) models like Long Short-Term Memory networks (LSTMs) and Transformers capture intricate temporal dependencies but require significantly more training data and time. While any S2S model can theoretically be used for this task, the choice largely depends on the size of the training dataset, with HMM being the default choice when the dataset is small. A more detailed analysis and comparison of these learned surrogates are provided in Section [4.3](https://arxiv.org/html/2412.01033v1#S4.SS3 "4.3 In-Depth Dissection of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent").

A Hidden Markov Model (HMM, Baum and Petrie ([1966](https://arxiv.org/html/2412.01033v1#bib.bib2))) estimates hidden states based on observable ones, assuming regular transitions between hidden states. An HMM is defined by the number of hidden states $N$ and observable states $M$, with hidden states $S_{hmm}=\{S_{hmm_{1}},\ldots,S_{hmm_{N}}\}$ and observable states $O_{hmm}=\{O_{hmm_{1}},\ldots,O_{hmm_{M}}\}$. The state transition probability matrix $A=[a_{ij}]$ represents $P(S_{hmm_{j}}\mid S_{hmm_{i}})$, and the observation probability matrix $B=[b_{jk}]$ represents $P(O_{hmm_{k}}\mid S_{hmm_{j}})$. The initial state distribution $\pi=[\pi_{i}]$ defines the probability of starting in state $S_{hmm_{i}}$. In Continuous Hidden Markov Models (CHMM), observations are modeled by continuous probability density functions, typically Gaussian Mixture Models (GMMs). We adopt CHMMs as the backbone model for HMMD. The CHMM defines three discrete hidden states: correct trajectory, moderately deviated trajectory, and highly deviated trajectory. The observable states are continuous features, represented by the two-feature plain distance $(D_{a},D_{o})$. The specific method for calculating the plain distance between A and B, denoted as $dis(A,B)$, utilizes a pre-trained RoBERTa Liu ([2019](https://arxiv.org/html/2412.01033v1#bib.bib21)) model, fine-tuned with the SQuAD v2 Rajpurkar et al. ([2018](https://arxiv.org/html/2412.01033v1#bib.bib27)) dataset. The inverse of the score obtained from this model is used as the plain distance. Using training examples, we calculate $(D_{a},D_{o})$ and annotate the hidden states. And The CHMM is trained using the Baum-Welch algorithm Baum et al. ([1970](https://arxiv.org/html/2412.01033v1#bib.bib3)), transforming the two-feature plain distance into a more accurate surrogate for the agent’s situational awareness.

The SAUP algorithm employs different surrogate configurations. We illustrate the SAUP using distance as the surrogate in Algorithm [1](https://arxiv.org/html/2412.01033v1#alg1 "Algorithm 1 ‣ 3.3 Agent Uncertainty Estimation ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"). Initially, uncertainty $U_{n}$ is computed for step $n$, along with the corresponding distances $D_{a_{n}}$ and $D_{o_{n}}$. This is repeated for $N$ steps. Subsequently, based on the surrogate choice, either plain or HMM-based, the situational weights $W_{n}$ are determined. Finally, the uncertainties $U$ and weights $W$ are aggregated to estimate the agent’s overall uncertainty $U_{agent}$.

Algorithm 1 Situational Awareness Uncertainty Propagation (SAUP)

  Initialize the $N$-Step LLM-based Agent $L_{\theta}$ with the problem $Q$, and the $Z_{n}=\{(A_{1},T_{1},O_{1}),(A_{2},T_{2},O_{2}),\ldots,(A_{n},T_{n},O_{n})\}$, the List $D_{L}$ to store the distance, the trained CHMM model $H$, the distance calculate method $Dis()$ from the section [3.3](https://arxiv.org/html/2412.01033v1#S3.SS3 "3.3 Agent Uncertainty Estimation ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), the single-step uncertainty method $F_{U}$, and the situation awareness uncertainty propagation function SAUP(), defined as the equation[1](https://arxiv.org/html/2412.01033v1#S3.E1 "In 3.1 Weighted Uncertainty Propagation ‣ 3 SAUP: Situational Awareness Uncertainty Propagation ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent").  for step $n$ in the problem solving process do     The Uncertainty for current step $U_{n}\leftarrow F_{U}(L_{\theta},Z_{n})$     Distance $D_{a_{n}}\leftarrow Dis(Z_{n},Q)$     Distance $D_{o_{n}}\leftarrow Dis(A_{n},O_{n})$     if using the HMM-Distance as the surrogates then        Add the $(D_{a_{n}}+D_{o_{n}})$ into the $D_{L}$     else        Using the Plain-Distance as the surrogates        $W_{n}\leftarrow D_{a_{n}}+D_{o_{n}}$     end if  end for  if using the HMM-Distance as the surrogates then     $(W_{1},W_{2},\ldots,W_{N})\leftarrow H(D_{L})=H((D_{a_{1}}+D_{o_{1}}),\ldots,(% D_{a_{N}}+D_{o_{N}}))$  end if  The Uncertainty for the agent $U_{agent}\leftarrow SAUP((U_{1},W_{1}),(U_{2},W_{2}),\ldots,(U_{N},W_{N}))$  return  Situational Awareness Agent Uncertainty $\mathbf{U_{agent}}$

## 4 Experiments

In this section, we evaluate the performance of SAUP, aiming to answer the following questions: Q1: Does SAUP outperform previous state-of-the-art approaches for uncertainty estimation? Q2: Given the comprehensive process of Uncertainty Propagation, does SAUP provide more accurate uncertainty estimation compared to single-step methods? Q3: Are the situational weights for specific steps effective in improving overall uncertainty estimation? Since obtaining precise situational weights is impractical, we designed surrogates, including distance-based and position-based methods. Are these surrogates reliable for accurately assessing the agent’s current situation?

### 4.1 Experimental Setup

LLM-based Agent Framework. Our experiments focus on evaluating SAUP’s ability to improve uncertainty estimation for multi-step LLM-based agents. While various multi-step agents follow different pipeline designs, they generally adhere to the thinking-acting-observation workflow. We chose the React Yao et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib38)) framework, a widely-used agent model, for its alignment with this workflow.

Backbone LLMs. We selected two categories of LLMs for the React agents: the open-source LLAMA3 Dubey et al. ([2024](https://arxiv.org/html/2412.01033v1#bib.bib10)) series (8B and 70B models) with entropy access, and GPT-4o Achiam et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib1)) (available via API), which restricts internal information. This selection ensures broad coverage of real-world scenarios.

Dataset and Task. We evaluated three challenging agent-based QA tasks. The first, HotpotQA Yang et al. ([2018](https://arxiv.org/html/2412.01033v1#bib.bib37)), focuses on multi-hop QA with diverse free-form answers. We randomly sampled 2,000 questions from the development set, assessed by both human evaluators and ChatGPT. The second, MMLU Hendrycks et al. ([2020](https://arxiv.org/html/2412.01033v1#bib.bib16)), involves multiple-choice questions across diverse fields like law and mathematics. Ten questions were sampled per subtask from the test set. Lastly, StrategyQA Geva et al. ([2021](https://arxiv.org/html/2412.01033v1#bib.bib14)) requires implicit reasoning, evaluated with true/false questions from its development set (229 questions).

Environment for External Information. LLM-based agents often need external sources to solve these tasks. For HotpotQA and StrategyQA, we provided access to the Wikipedia API, which retrieves relevant entity-based information. For MMLU, we used SerpAPI SerpAPI ([2024](https://arxiv.org/html/2412.01033v1#bib.bib29)) for structured Google search results.

Baselines. We evaluated SAUP against several uncertainty estimation methods. For entropy-based approaches, we used predictive and semantic entropy Xiao and Wang ([2019](https://arxiv.org/html/2412.01033v1#bib.bib35)); Kuhn et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib19)). Likelihood-based methods Malinin and Gales ([2020](https://arxiv.org/html/2412.01033v1#bib.bib22)) included plain likelihood and normalized entropy, the latter accounting for token length. We also implemented P(True) Kadavath et al. ([2022](https://arxiv.org/html/2412.01033v1#bib.bib18)); Yin et al. ([2023](https://arxiv.org/html/2412.01033v1#bib.bib39)), which prompts agents to self-assess their confidence.

Evaluation Metrics. We used AUROC Bradley ([1997](https://arxiv.org/html/2412.01033v1#bib.bib4)) to measure the ability of uncertainty methods to distinguish between correct and incorrect responses. Higher AUROC values indicate better differentiation, with a perfect score of 1 representing complete distinction and 0.5 representing random chance.

### 4.2 Superior Discriminative Performance of SAUP

Table 1: Results for SAUP. The best results and second best results are bold and underlined, respectively.

 |  | HotpotQA | MMLU | StrategyQA |
| --- | --- | --- | --- |
| Method | LLAMA3 8B | LLAMA3 70B | GPT4O | LLAMA3 8B | LLAMA3 70B | GPT4-O | LLAMA3 8B | LLAMA3 70B | GPT4-O |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | Predictive Entropy | 0.631 | 0.617 | N.A. | 0.531 | 0.585 | N.A. | 0.542 | 0.589 | N.A. |
|  | Likelihood | 0.653 | 0.622 | 0.764 | 0.550 | 0.592 | 0.610 | 0.525 | 0.591 | 0.641 |
|  | Normalised Entropy | 0.664 | 0.635 | 0.772 | 0.555 | 0.579 | 0.607 | 0.554 | 0.557 | 0.710 |
|  | P(True) | 0.601 | 0.618 | 0.749 | 0.528 | 0.560 | 0.588 | 0.533 | 0.577 | 0.689 |
|  | Semantic Entropy | 0.702 | 0.669 | N.A. | 0.548 | 0.605 | N.A. | 0.599 | 0.610 | N.A. |
|  | SAUP-Learned | 0.771 | 0.755 | 0.778 | 0.669 | 0.638 | 0.626 | 0.787 | 0.783 | 0.809 | 

Table 2: Results for SAUP with various Surrogates. The best results and second best results are bold and underlined, respectively.

 |  | HotpotQA | MMLU | StrategyQA |
| --- | --- | --- | --- |
| Method | LLAMA3 8B | LLAMA3 70B | GPT4O | LLAMA3 8B | LLAMA3 70B | GPT4-O | LLAMA3 8B | LLAMA3 70B | GPT4-O |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | SAUP-P | 0.723 | 0.739 | 0.797 | 0.634 | 0.636 | 0.614 | 0.668 | 0.641 | 0.734 |
|  | SAUP-D | 0.762 | 0.726 | 0.773 | 0.660 | 0.619 | 0.624 | 0.755 | 0.809 | 0.806 |
|  | SAUP-PD | 0.759 | 0.745 | 0.782 | 0.651 | 0.625 | 0.619 | 0.732 | 0.756 | 0.785 |
|  | SAUP-HMMD(Learned) | 0.771 | 0.755 | 0.778 | 0.669 | 0.638 | 0.626 | 0.787 | 0.783 | 0.809 | 

In this section, we compare the performance of various uncertainty measurement methods in distinguishing whether an LLM-based agent’s final response to QA questions is correct or incorrect. The evaluation process consists of the following steps: (1) The LLM-based agent, using the ReACT framework, answers the QA questions; (2) Multiple versions of our proposed SAUP method, along with other baseline uncertainty estimation methods, compute an uncertainty score for each agent’s response; (3) Each response is assessed for correctness, assigning a value of 0 if the answer is correct and 1 if incorrect; (4) We calculate the AUROC based on the accuracy of these classifications and the corresponding uncertainty scores. Ideally, incorrect answers should correlate with higher uncertainty scores.

We employed several state-of-the-art LLMs, including {LLAMA3 8B, LLAMA3 70B, GPT4O}, and conducted evaluations on challenging datasets, namely {StrategyQA, MMLU, HotpotQA}. Table [1](https://arxiv.org/html/2412.01033v1#S4.T1 "Table 1 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent") presents the results, demonstrating that our SAUP method, consistently achieves higher AUROC scores across all datasets compared to state-of-the-art methods. These findings indicate that SAUP offers superior performance in distinguishing between correct and incorrect agent responses based on uncertainty estimation, leading to important conclusions.

Table 3: Results for Simple Uncertainty Propagation. The best results and second best results are bold and underlined, respectively.

 |  | HotpotQA | MMLU | StrategyQA |
| --- | --- | --- | --- |
| Method | LLAMA3 8B | LLAMA3 70B | GPT4O | LLAMA3 8B | LLAMA3 70B | GPT4-O | LLAMA3 8B | LLAMA3 70B | GPT4-O |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | Arithmetic Mean | 0.695 | 0.676 | 0.781 | 0.621 | 0.596 | 0.609 | 0.576 | 0.611 | 0.711 |
|  | Geometric Mean | 0.713 | 0.714 | 0.785 | 0.614 | 0.591 | 0.610 | 0.601 | 0.627 | 0.714 |
|  | RMS | 0.717 | 0.728 | 0.782 | 0.624 | 0.615 | 0.612 | 0.584 | 0.629 | 0.723 |
|  | SAUP-Learned | 0.771 | 0.755 | 0.778 | 0.669 | 0.638 | 0.626 | 0.787 | 0.783 | 0.809 | ![Refer to caption](img/e643268462f0f494a150dbcf203d0751.png)

Figure 3: The Performance Comparison of Learned-based Surrogates with Various S2S Backbone Models

### 4.3 In-Depth Dissection of SAUP

Given the superiority of our proposed SAUP, we further dissect its performance by addressing the following questions. This analysis highlights the advantages of SAUP in various aspects and offers insights into its applicability and performance under different conditions.

Q1: Is the uncertainty measurement of the internal steps beneficial for the overall uncertainty measurement of the agent?

Yes, measuring uncertainty at each internal step significantly contributes to a more accurate overall uncertainty estimation. By considering intermediate uncertainties, we capture the cumulative effect of uncertainty propagation throughout the interaction process. As shown in Table [1](https://arxiv.org/html/2412.01033v1#S4.T1 "Table 1 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), SAUP-based methods consistently outperform traditional single-step methods in AUROC scores across datasets and models. The internal step uncertainties provide meaningful information that, when aggregated, enhance the overall uncertainty measurement. Even basic uncertainty propagation methods, such as algorithmic averaging or root mean square (RMS), used to aggregate the uncertainty across all steps, have demonstrated significant improvements over single-step baselines, as shown in Table [3](https://arxiv.org/html/2412.01033v1#S4.T3 "Table 3 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent").

Q2: What is the quality of the surrogates, and how do they benefit the overall uncertainty measurement?

High-quality surrogates ensure that situational weights accurately reflect each step’s impact on the overall uncertainty. We propose the Position Surrogate (SAUP-P), which assigns greater weight to steps closer to the final answer, and the Plain Distance Surrogate (SAUP-D), which uses only the plain distance. The Hybrid Surrogate (SAUP-PD) combines both approaches with a factor for better balance.

As shown in Table [2](https://arxiv.org/html/2412.01033v1#S4.T2 "Table 2 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent") and Table [3](https://arxiv.org/html/2412.01033v1#S4.T3 "Table 3 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), different surrogates improve AUROC scores compared to simple uncertainty propagation baselines, which assign equal weights to all steps. In addition, the HMMD-based (learned) surrogate outperforms others by a clear margin, validating its effectiveness in capturing the agent’s situational context.

![Refer to caption](img/47e2b83a0701baff39e8674a0403d838.png)

Figure 4: Visualization analysis of SAUP on the StrategyQA dataset. Detailed explanations of this figure are provided in the Q3 of Section [4.3](https://arxiv.org/html/2412.01033v1#S4.SS3 "4.3 In-Depth Dissection of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent").

Q3: Can SAUP demonstrate its superiority in separating correct and incorrect results?

Yes, SAUP provides more discriminative uncertainty scores, leading to higher AUROC values across datasets and models, as evidenced in Table [1](https://arxiv.org/html/2412.01033v1#S4.T1 "Table 1 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"). The step-by-step propagation of uncertainty allows SAUP to capture the accumulation of uncertainty throughout the reasoning process, enabling better separation of correct and incorrect results.

In addition, we performed a visualization analysis on the StrategyQA dataset (Figure [4](https://arxiv.org/html/2412.01033v1#S4.F4 "Figure 4 ‣ 4.3 In-Depth Dissection of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent")). The X-axis represents the steps taken, and the Y-axis shows normalized uncertainty values. Red points indicate incorrect answers, and blue points indicate correct answers. SAUP (right sub-image) shows the clearest separation between correct and incorrect answers, outperforming the one-step (left) and simple uncertainty propagation methods (middle), highlighting its advantage in uncertainty estimation.

Q4: Is the HMM reasonable, and how does its performance change with different dataset sizes? Why not use gradient-based models like RNNs or Transformers?

Learned-based surrogates rely on manually annotated data. During training, we map data groups $D_{a_{n}}$ and $D_{o_{n}}$ to the agent’s situational context, enabling SAUP to infer states in unseen scenarios. We use a Hidden Markov Model (HMM) in the main experiment, but also explore LSTM and Transformer models, analyzing their theoretical and experimental advantages.

Theoretical Perspective: HMMs are efficient and interpretable, ideal for limited data but weak in modeling long-range dependencies. LSTMs capture temporal dependencies better but require more data and resources. Transformers handle both local and global dependencies effectively but are computationally expensive and data-intensive.

Experimental Comparison: On the StrategyQA dataset, we evaluated HMM-based, LSTM-based, and mini-size Transformer-based surrogates across varying training dataset sizes. Figure [3](https://arxiv.org/html/2412.01033v1#S4.F3 "Figure 3 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent") shows that HMMs perform well with smaller datasets, while LSTMs and Transformers improve with more data. However, Transformer-based surrogates require impractically large datasets for uncertainty measurement tasks, making them less suitable.

HMMs are practical for uncertainty propagation in LLM-based agents due to their simplicity and efficiency, particularly with limited data. LSTMs are viable alternatives when data and computational resources are sufficient, while Transformers are generally not feasible for most scenarios.

Q5: Does the question difficulty influence the effectiveness of uncertainty propagation?

Yes, complex questions lead to longer, nuanced decision-making, increasing uncertainty accumulation. SAUP’s situational awareness framework excels in such cases, effectively propagating uncertainty at each step. As shown in Table [1](https://arxiv.org/html/2412.01033v1#S4.T1 "Table 1 ‣ 4.2 Superior Discriminative Performance of SAUP ‣ 4 Experiments ‣ SAUP: Situation Awareness Uncertainty Propagation on LLM Agent"), SAUP’s advantage is most evident in more challenging datasets like StrategyQA, with greater AUROC improvements.

## 5 Conclusion

In this paper, we propose Situational Awareness Uncertainty Propagation(SAUP), a novel framework for accurately estimating uncertainty in LLM-based multi-step agents. Unlike traditional methods that focus solely on single-step uncertainty, SAUP propagates uncertainty across all steps in the agent’s reasoning process and incorporates situational awareness. Experimental results on challenging datasets, show that SAUP outperforms state-of-the-art uncertainty estimation methods, achieving up to 20% improvements in AUROC scores, thereby demonstrating its effectiveness in enhancing reliability for complex decision-making scenarios. This research highlights the value of multi-step uncertainty estimation and situational awareness in LLM-based agents, providing a strong foundation for their trustworthy deployment.

## 6 Limitations

Despite the effectiveness of SAUP in improving uncertainty estimation for multi-step LLM-based agents, several limitations remain. First, the learning-based surrogate version of SAUP relies on manually annotated datasets for situational weights, which is time-consuming, costly, and may not generalize well to very complex scenarios—especially when manual labels are still prone to errors. Additionally, the complexity of diverse environments could exacerbate the difficulty in ensuring accurate situational labeling. Second, the SAUP framework assumes that uncertainty at each step can be accurately captured. Although this is beyond the scope of our study, errors in single-step uncertainty estimation can compromise the propagation of uncertainty, thereby diminishing the benefits of the SAUP framework. Future work should focus on developing more robust situational weight estimation methods that reduce dependence on manually annotated datasets—potentially leveraging LLM-generated labels—to enhance SAUP’s applicability and reliability across diverse use cases.

## References

*   Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
*   Baum and Petrie (1966) Leonard E Baum and Ted Petrie. 1966. Statistical inference for probabilistic functions of finite state markov chains. *The annals of mathematical statistics*, 37(6):1554–1563.
*   Baum et al. (1970) Leonard E Baum, Ted Petrie, George Soules, and Norman Weiss. 1970. A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. *The annals of mathematical statistics*, 41(1):164–171.
*   Bradley (1997) Andrew P Bradley. 1997. The use of the area under the roc curve in the evaluation of machine learning algorithms. *Pattern recognition*, 30(7):1145–1159.
*   Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. *ACM Transactions on Intelligent Systems and Technology*, 15(3):1–45.
*   Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.
*   Chen et al. (2024) Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, and George Karypis. 2024. Hytrel: Hypergraph-enhanced tabular data representation learning. *Advances in Neural Information Processing Systems*, 36.
*   Darrin et al. (2022) Maxime Darrin, Pablo Piantanida, and Pierre Colombo. 2022. Rainproof: An umbrella to shield text generators from out-of-distribution data. *arXiv preprint arXiv:2212.09171*.
*   Duan et al. (2024) Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 5050–5063.
*   Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*.
*   Fomicheva et al. (2020) Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. *Transactions of the Association for Computational Linguistics*, 8:539–555.
*   Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In *international conference on machine learning*, pages 1050–1059\. PMLR.
*   Gal et al. (2017) Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep bayesian active learning with image data. In *International conference on machine learning*, pages 1183–1192\. PMLR.
*   Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. *Transactions of the Association for Computational Linguistics*, 9:346–361.
*   Han et al. (2024) Jiuzhou Han, Wray Buntine, and Ehsan Shareghi. 2024. Towards uncertainty-aware language agent. *arXiv preprint arXiv:2401.14016*.
*   Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. *arXiv preprint arXiv:2009.03300*.
*   Jiang et al. (2021) Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. *Transactions of the Association for Computational Linguistics*, 9:962–977.
*   Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. *arXiv preprint arXiv:2207.05221*.
*   Kuhn et al. (2023) Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. *arXiv preprint arXiv:2302.09664*.
*   Li et al. (2023) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023. Camel: Communicative agents for" mind" exploration of large language model society. *Advances in Neural Information Processing Systems*, 36:51991–52008.
*   Liu (2019) Yinhan Liu. 2019. Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.
*   Malinin and Gales (2020) Andrey Malinin and Mark Gales. 2020. Uncertainty estimation in autoregressive structured prediction. *arXiv preprint arXiv:2002.07650*.
*   Minaee et al. (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models: A survey. *arXiv preprint arXiv:2402.06196*.
*   Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. *arXiv preprint arXiv:2112.09332*.
*   Qiao et al. (2023) Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. 2023. Making language models better tool learners with execution feedback. *arXiv preprint arXiv:2305.13068*.
*   Raiaan et al. (2024) Mohaimenul Azam Khan Raiaan, Md Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, Sadman Sakib, Most Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam. 2024. A review on large language models: Architectures, applications, taxonomies, open issues and challenges. *IEEE Access*.
*   Rajpurkar et al. (2018) Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable questions for squad. *arXiv preprint arXiv:1806.03822*.
*   Renze and Guven (2024) Matthew Renze and Erhan Guven. 2024. Self-reflection in llm agents: Effects on problem-solving performance. *arXiv preprint arXiv:2405.06682*.
*   SerpAPI (2024) SerpAPI. 2024. Real-time search api for google results. [https://serpapi.com](https://serpapi.com).
*   Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*, 2(5):9.
*   Wang et al. (2024) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large language model based autonomous agents. *Frontiers of Computer Science*, 18(6):186345.
*   Wang et al. (2021) Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. *arXiv preprint arXiv:2109.00859*.
*   Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. 2023. [Autogen: Enabling next-gen llm applications via multi-agent conversation](https://arxiv.org/abs/2308.08155). *Preprint*, arXiv:2308.08155.
*   Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.
*   Xiao and Wang (2019) Yijun Xiao and William Yang Wang. 2019. Quantifying uncertainties in natural language processing tasks. In *Proceedings of the AAAI conference on artificial intelligence*, volume 33, pages 7322–7329.
*   Xiao and Wang (2021) Yijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. *arXiv preprint arXiv:2103.15025*.
*   Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. *arXiv preprint arXiv:1809.09600*.
*   Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*.
*   Yin et al. (2023) Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they don’t know? *arXiv preprint arXiv:2305.18153*.
*   Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.