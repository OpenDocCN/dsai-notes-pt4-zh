- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Generate-on-Graph: 将LLM视为代理和KG，在不完整知识图谱问答中的应用'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14741](https://ar5iv.labs.arxiv.org/html/2404.14741)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.14741](https://ar5iv.labs.arxiv.org/html/2404.14741)
- en: Yao Xu^(1,2), Shizhu He^(1,2), Jiabei Chen^(1,2), Zihao Wang³, Yangqiu Song³,
    Hanghang Tong⁴, Kang Liu^(1,2,5), Jun Zhao^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yao Xu^(1,2)，Shizhu He^(1,2)，Jiabei Chen^(1,2)，Zihao Wang³，Yangqiu Song³，Hanghang
    Tong⁴，Kang Liu^(1,2,5)，Jun Zhao^(1,2)
- en: ¹ The Laboratory of Cognition and Decision Intelligence for Complex Systems,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 复杂系统认知与决策智能实验室，
- en: Institute of Automation, Chinese Academy of Sciences
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 中国科学院自动化研究所
- en: ² School of Artificial Intelligence, University of Chinese Academy of Sciences
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 中国科学院人工智能学院
- en: ³ The Hong Kong University of Science and Technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 香港科技大学
- en: ⁴ University of Illinois Urbana-Champaign,   ⁵ Shanghai Artificial Intelligence
    Laboratory
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 伊利诺伊大学厄本那-香槟分校，  ⁵ 上海人工智能实验室
- en: '{yao.xu, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn, zwanggc@connect.ust.hk, yqsong@cse.ust.hk,
    htong@illinois.edu'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{yao.xu, shizhu.he, kliu, jzhao}@nlpr.ia.ac.cn, zwanggc@connect.ust.hk, yqsong@cse.ust.hk,
    htong@illinois.edu'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: To address the issue of insufficient knowledge and the tendency to generate
    hallucination in Large Language Models (LLMs), numerous studies have endeavored
    to integrate LLMs with Knowledge Graphs (KGs). However, all these methods are
    evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete
    KGs, where the factual triples involved in each question are entirely covered
    by the given KG. In this situation, LLM mainly acts as an agent to find answer
    entities by exploring the KG, rather than effectively integrating internal and
    external knowledge sources. However, in real-world scenarios, KGs are often incomplete
    to cover all the knowledge required to answer questions. To simulate real-world
    scenarios and evaluate the ability of LLMs to integrate internal and external
    knowledge, in this paper, we propose leveraging LLMs for QA under Incomplete Knowledge
    Graph (IKGQA), where the given KG doesn’t include all the factual triples involved
    in each question. To handle IKGQA, we propose a training-free method called Generate-on-Graph
    (GoG) that can generate new factual triples while exploring on KGs. Specifically,
    we propose a selecting-generating-answering framework, which not only treat the
    LLM as an agent to explore on KGs, but also treat it as a KG to generate new facts
    based on the explored subgraph and its inherent knowledge. Experimental results
    on two datasets demonstrate that our GoG can solve IKGQA to a certain extent,
    while almost all previous methods cannot perform well on IKGQA.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决大型语言模型（LLMs）知识不足和生成幻觉的问题，许多研究致力于将LLMs与知识图谱（KGs）整合。然而，这些方法都在传统的知识图谱问答（KGQA）中进行评估，那里每个问题涉及的事实三元组完全由给定的KG覆盖。在这种情况下，LLM主要充当代理，通过探索KG找到答案实体，而不是有效整合内部和外部知识源。然而，在实际场景中，KGs通常不完整，无法覆盖回答问题所需的所有知识。为了模拟实际场景并评估LLMs整合内部和外部知识的能力，本文提出了在不完整知识图谱（IKGQA）下利用LLMs进行问答的方法，其中给定的KG不包括每个问题涉及的所有事实三元组。为了处理IKGQA，我们提出了一种名为Generate-on-Graph（GoG）的无训练方法，它可以在探索KGs时生成新的事实三元组。具体而言，我们提出了一个选择-生成-回答框架，这不仅将LLM视为一个探索KGs的代理，还将其视为一个KG，以基于探索的子图及其固有知识生成新事实。两组数据集上的实验结果表明，我们的GoG在一定程度上可以解决IKGQA，而几乎所有以前的方法在IKGQA上表现不佳。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/949c9219b590af4d958105aa985efaa0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/949c9219b590af4d958105aa985efaa0.png)'
- en: 'Figure 1: An example of Incomplete Knowledge Graph Question Answering (IKGQA),
    where yellow and red nodes represent topic and answer entity, respectively. Red
    dash line represents this triple (Cupertino, timezone, Pacific Standard Time)
    is missing in the KG.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不完整知识图谱问答（IKGQA）的示例，其中黄色和红色节点分别代表主题和答案实体。红色虚线表示这个三元组（Cupertino，timezone，Pacific
    Standard Time）在KG中缺失。
- en: Large Language Models (LLMs) Brown et al. ([2020](#bib.bib3)); Bang et al. ([2023](#bib.bib1))
    have made great success in various natural language processing (NLP) tasks. Benefiting
    from extensive model parameters and vast amounts of pre-training corpus, LLMs
    can solve complex reasoning tasks through In-Context Learning (ICL) Dong et al.
    ([2023](#bib.bib4)), without fine-tuning for specific tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs） Brown et al. ([2020](#bib.bib3)); Bang et al. ([2023](#bib.bib1))
    在各种自然语言处理（NLP）任务中取得了巨大成功。由于模型参数的广泛和大量的预训练语料，LLMs可以通过上下文学习（ICL） Dong et al. ([2023](#bib.bib4))
    解决复杂的推理任务，而无需针对特定任务进行微调。
- en: 'However, LLMs are still suffer from insufficient knowledge and the tendency
    to generate hallucinationHuang et al. ([2023](#bib.bib6)); Li et al. ([2023a](#bib.bib10)).
    To mitigate this issue, many methods that incorporate LLMs with Knowledge Graphs
    (KGs) Ji et al. ([2021](#bib.bib7)) has been proposed Pan et al. ([2023](#bib.bib17)),
    where KGs provide accurate and abundant factual knowledge in triple format while
    LLMs provide strong natural language processing ability. These works can be roughly
    divided into two categories: (1) Semantic Parsing (SP) methods Li et al. ([2023c](#bib.bib12));
    Nie et al. ([2023](#bib.bib16)), which use LLMs to generate logical forms with
    ICL, and then obtain answers by executing these logical queries on KGs. (2) Retrieval
    Augmented (RA) methods Li et al. ([2023d](#bib.bib13)), which retrieve information
    related to the question from KGs as external knowledge to help LLMs to obtain
    the answers. The retrieval format can be triples Li et al. ([2023d](#bib.bib13)),
    paths Sun et al. ([2023](#bib.bib20)); Luo et al. ([2023b](#bib.bib15)) and subgraphs
    Wang et al. ([2023a](#bib.bib24)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs仍然面临知识不足和产生幻觉的倾向 Huang et al. ([2023](#bib.bib6)); Li et al. ([2023a](#bib.bib10))。为了解决这个问题，许多将LLMs与知识图谱（KGs）结合的方法被提出
    Ji et al. ([2021](#bib.bib7))，Pan et al. ([2023](#bib.bib17))，其中KGs以三元组格式提供准确且丰富的事实知识，而LLMs提供强大的自然语言处理能力。这些工作大致可以分为两类：（1）语义解析（SP）方法
    Li et al. ([2023c](#bib.bib12)); Nie et al. ([2023](#bib.bib16))，这些方法利用LLMs生成逻辑形式，并通过在KGs上执行这些逻辑查询来获取答案。（2）检索增强（RA）方法
    Li et al. ([2023d](#bib.bib13))，这些方法从KGs中检索与问题相关的信息作为外部知识，帮助LLMs获得答案。检索格式可以是三元组
    Li et al. ([2023d](#bib.bib13))，路径 Sun et al. ([2023](#bib.bib20)); Luo et al.
    ([2023b](#bib.bib15)) 和子图 Wang et al. ([2023a](#bib.bib24))。
- en: Semantic parsing based methods exclusively treat LLMs as parser, not only do
    they ignore LLMs’ inherent knowledge and reasoning ability, but they also depend
    heavily on KGs’ quality and completeness Sun et al. ([2023](#bib.bib20)). Therefore,
    more attention is paid to retrieval augmented methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语义解析方法专门将大型语言模型（LLMs）视为解析器，这不仅忽略了LLMs固有的知识和推理能力，还严重依赖知识图谱（KGs）的质量和完整性 Sun et
    al. ([2023](#bib.bib20))。因此，更多的关注被转向了检索增强方法。
- en: '![Refer to caption](img/8cd6d21585c6e940743557850477189e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/8cd6d21585c6e940743557850477189e.png)'
- en: 'Figure 2: The performance of ToG Sun et al. ([2023](#bib.bib20)) and our GoG
    under complete/incomplete KG. Dash line indicates the performance of LLM with
    CoT Wei et al. ([2023](#bib.bib27)) prompting (without KG).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：ToG Sun et al. ([2023](#bib.bib20)) 和我们的GoG在完整/不完整KG下的表现。虚线表示LLM在CoT Wei
    et al. ([2023](#bib.bib27)) 提示下的表现（没有KG）。
- en: 'Although these retrieval augmented methods claim to solve the drawbacks of
    semantic parsing methods and obtain good performance on conventional Knowledge
    Graph Question Answering (KGQA) Yih et al. ([2016a](#bib.bib29)), it is still
    hard to determine if they effectively integrate knowledge from KGs and LLMs, and
    how much reasoning ability is utilized. One crucial reason is that, in conventional
    KGQA tasks, the factual triplets involved in each question are entirely covered
    by the KG. Therefore, in this scenario, LLMs still play the role of parser which
    only needs to identify the relationship path starting from the topic entity to
    the answer entity. For example, for the question "What is the timezone of the
    area where Apple headquarters is located?" in Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"), the LLM only needs to start from Apple headquarters,
    sequentially choose the relations located_in and timezone to find the answer.
    That means LLMs only need to ground the relationship appearing in the question
    to the specific relation in the KG to reach the answer entity Pacific Standard
    Time. Therefore, in conventional KGQA, LLM mainly acts as an agent to find answer
    entities by exploring the KG, rather than effectively integrating internal and
    external knowledge sources.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管这些检索增强方法声称能够解决语义解析方法的缺陷，并在传统知识图谱问答（KGQA）中取得良好表现 Yih 等人 ([2016a](#bib.bib29))，但仍然很难确定它们是否有效整合了来自知识图谱（KGs）和大语言模型（LLMs）的知识，以及利用了多少推理能力。一个关键原因是，在传统的KGQA任务中，每个问题涉及的事实三元组都被KG完全覆盖。因此，在这种情况下，LLMs
    仍然充当解析器的角色，只需识别从主题实体到答案实体的关系路径。例如，对于图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") 中的问题“Apple总部所在区域的时区是什么？”，LLM 只需从Apple总部开始，顺序选择关系located_in
    和 timezone 来找到答案。这意味着 LLM 只需将问题中出现的关系与KG中的特定关系对接，就能得到答案实体太平洋标准时间。因此，在传统的KGQA中，LLM主要作为代理通过探索KG找到答案实体，而不是有效整合内部和外部知识来源。'
- en: 'However, in real-world scenarios, KGs are often incomplete to cover all the
    knowledge required to answer questions. Besides, LLMs contain rich knowledge content
    and have powerful reasoning ability. Therefore, to evaluate the ability of LLMs
    to integrate internal and external knowledge, in this paper, we propose leveraging
    LLMs for QA under incomplete KG (IKGQA). The distinction between IKGQA and conventional
    KGQA is that IKGQA does not encompass all the factual triplets relevant to each
    question, rendering the KG in IKGQA incomplete. For example, as shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering"), for answering the same
    question "What is the timezone of the area where Apple headquarters is located?",
    the key triple (Cupertino, timezone, Pacific Standard Time) does not exist in
    the KG. This means that even if the correct SPARQL query is generated, it may
    not retrieve the final answer ¹¹1SP methods do not parse ”timezone” into two hop
    path ”located_in -¿ timezone” because in the training set, ”timezone” only corresponds
    to one hop path ”timezone”, more details can be found in Appendix [A](#A1 "Appendix
    A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering").. Compared to KGQA,
    IKGQA holds greater research significance for the following reasons: (1) it is
    closer to real-world scenarios where the given KG is incomplete to answer users’
    questions. (2) it evaluates LLMs’ reasoning ability and its capability to integrate
    inherent and external knowledge.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，在实际场景中，知识图谱（KG）通常是不完整的，无法涵盖回答问题所需的所有知识。此外，大型语言模型（LLMs）包含丰富的知识内容，并且具有强大的推理能力。因此，为了评估LLMs整合内部和外部知识的能力，本文提出了在不完整知识图谱下（IKGQA）利用LLMs进行问答的方案。IKGQA与传统KGQA的区别在于IKGQA不包含与每个问题相关的所有事实三元组，这使得IKGQA中的KG是不完整的。例如，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both Agent and KG
    in Incomplete Knowledge Graph Question Answering")所示，对于回答同一个问题“苹果总部所在区域的时区是什么？”，关键三元组（Cupertino，timezone，Pacific
    Standard Time）在KG中不存在。这意味着即使生成了正确的SPARQL查询，也可能无法检索到最终答案¹¹1SP方法不会将“timezone”解析为两个跳跃路径“located_in
    -¿ timezone”，因为在训练集中，“timezone”仅对应于一个跳跃路径“timezone”，更多细节可以参见附录[A](#A1 "Appendix
    A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")。与KGQA相比，IKGQA具有更大的研究意义，原因如下：（1）它更接近于实际场景，即给定的KG不完整以回答用户的问题。（2）它评估了LLMs的推理能力及其整合内在和外部知识的能力。'
- en: 'As shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering"),
    although previous methods have achieved outstanding performance in KGQA, their
    performance has significantly declined in IKGQA, even is worse than that without
    KG. This discrepancy suggests that these methods might not fully integrate the
    external and inherent knowledge of LLMs as professed.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both
    Agent and KG in Incomplete Knowledge Graph Question Answering")所示，尽管之前的方法在KGQA中取得了卓越的表现，但它们在IKGQA中的表现显著下降，甚至不如没有KG时的表现。这种差异表明，这些方法可能未能完全整合LLMs的外部和内在知识。'
- en: 'To address the challenges in IKGQA, we propose a novel method called Generate-on-graph
    (GoG), which adopts a selecting-generating-answering framework consisting of three
    main steps: (1) Selecting, where LLMs select the relations most relevant to the
    current question and expanding the subgraph using these relations. (2) Generating,
    where LLMs use its inherent knowledge and reasoning abilities to generate new
    factual triples based on the explored subgraph. For example, if the KG already
    contains the triples (Cupertino, located_in, California), (California, timezone,
    Pacific Standard Time), LLMs can infer new triple (Cupertino, timezone, Pacific
    Standard Time). (3) Answering, where LLMs try to answer the question based on
    the retrieval and generated knowledge. If the information is still insufficient,
    the process is repeated by going back to Steps 1 and 2 until the LLMs can answer
    the question. The codes and data are available at [https://github.com/YaooXu/GoG](https://github.com/YaooXu/GoG).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对IKGQA中的挑战，我们提出了一种新颖的方法，称为Generate-on-graph（GoG），该方法采用了一个由三个主要步骤组成的选择-生成-回答框架：（1）选择，LLMs选择与当前问题最相关的关系，并利用这些关系扩展子图。（2）生成，LLMs利用其固有的知识和推理能力，根据探索到的子图生成新的事实三元组。例如，如果知识图谱已经包含了三元组（Cupertino,
    located_in, California）、（California, timezone, Pacific Standard Time），LLMs可以推断出新的三元组（Cupertino,
    timezone, Pacific Standard Time）。（3）回答，LLMs根据检索和生成的知识尝试回答问题。如果信息仍然不足，则通过回到第1和第2步骤重复该过程，直到LLMs能够回答问题。代码和数据可在[https://github.com/YaooXu/GoG](https://github.com/YaooXu/GoG)获得。
- en: 'The main contributions of this paper can be summarized as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献总结如下：
- en: '1.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose leveraging LLMs for QA under incomplete KG (IKGQA), which is closer
    to real-world scenarios and can evaluate LLMs’ ability better, and construct relevant
    datasets.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议利用LLMs进行不完整知识图谱（IKGQA）下的问答，这更接近现实场景，能够更好地评估LLMs的能力，并构建相关数据集。
- en: '2.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose a method called Generate-on-Graph (GoG), which uses the selecting-generating-answering
    framework, to address IKGQA.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种称为Generate-on-Graph（GoG）的方法，该方法使用选择-生成-回答框架来解决IKGQA问题。
- en: '3.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Experimental results on two datasets show the superiority of GoG, and demonstrate
    that an incomplete KG can still help LLMs answer complex questions by providing
    related structured information, even without directly providing the answers.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在两个数据集上的实验结果显示了GoG的优越性，并证明即使在没有直接提供答案的情况下，不完整的知识图谱仍然可以通过提供相关的结构化信息来帮助LLMs回答复杂问题。
- en: '![Refer to caption](img/6fc52947cbc60dfa9c3991fad5b7b8b5.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6fc52947cbc60dfa9c3991fad5b7b8b5.png)'
- en: 'Figure 3: Comparison of four methods in solving IKGQA: (a) Standard CoT prompting
    (closed-book, without KG), (b) Semantic parsing based method (e.g., KB-BINDER Li
    et al. ([2023c](#bib.bib12))), (c) Path retrieval method (e.g., ToG Sun et al.
    ([2023](#bib.bib20))), (d) The proposed GoG with selecting-generating-answering
    framework.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：四种方法解决IKGQA的比较：（a）标准CoT提示（闭卷，无知识图谱），（b）基于语义解析的方法（例如，KB-BINDER Li等人（[2023c](#bib.bib12)）），（c）路径检索方法（例如，ToG Sun等人（[2023](#bib.bib20)）），（d）提出的GoG与选择-生成-回答框架。
- en: 2 Related Work
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Question Answering Under Incomplete KG. Some previous works Saxena et al. ([2020](#bib.bib18));
    Zan et al. ([2022](#bib.bib31)) attempt to train KG embeddings to predict answers
    by similarity scores under incomplete KG. Compared to these previous KGE-based
    works, we propose leveraging LLMs for QA under incomplete KG to study whether
    LLMs can integrate internal and external knowledge well.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不完整知识图谱下的问答。一些早期的研究如Saxena等人（[2020](#bib.bib18)）和Zan等人（[2022](#bib.bib31)）尝试通过相似度评分来训练知识图谱嵌入，以预测不完整知识图谱下的答案。与这些基于KGE的早期工作相比，我们建议利用LLMs进行不完整知识图谱下的问答，以研究LLMs是否能够很好地整合内部和外部知识。
- en: 'Unifying KGs and LLMs for KGQA. Various methods have been proposed to unify
    KGs and LLMs to solve KGQA, these methods can be classified into two categories:
    Semantic Parsing (SP) methods Li et al. ([2023c](#bib.bib12)); Nie et al. ([2023](#bib.bib16))
    and Retrieval Augmented (RA) methods Luo et al. ([2023b](#bib.bib15)); Sun et al.
    ([2023](#bib.bib20)). SP methods transform the question into a structural query
    using LLMs. These queries can then be executed by a KG engine to derive answers
    based on KGs Sun et al. ([2020](#bib.bib21)). KB-BINDER Li et al. ([2023c](#bib.bib12))
    generates the drafts as preliminary logical forms first, and then binds the drafts
    to the executable ones with entity and relation binders. However, the effectiveness
    of these methods relies heavily on the quality of the generated queries and the
    completeness of KGs. RA methods retrieve related information from the KG to improve
    the reasoning performance Li et al. ([2023b](#bib.bib11)). ToG Sun et al. ([2023](#bib.bib20))
    treats the LLM as an agent to interactively explore relation paths step-by-step
    on KGs and perform reasoning based on the retrieved paths. RoG Luo et al. ([2023b](#bib.bib15))
    first generates relation paths as faithful plans, and then use them to retrieve
    valid reasoning paths from the KGs for LLMs to reason. Our GoG belongs to retrieval
    augmented methods, we also utilize the knowledge modeling ability of LLMs, as
    well as the semantic parsing ability and reasoning ability.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 统一 KGs 和 LLMs 以解决 KGQA。各种方法已经被提出以统一 KGs 和 LLMs 来解决 KGQA，这些方法可以分为两类：语义解析 (SP)
    方法 Li 等人 ([2023c](#bib.bib12)); Nie 等人 ([2023](#bib.bib16)) 和检索增强 (RA) 方法 Luo
    等人 ([2023b](#bib.bib15)); Sun 等人 ([2023](#bib.bib20))。SP 方法使用 LLMs 将问题转换为结构化查询。这些查询可以由
    KG 引擎执行，以根据 KGs 得出答案 Sun 等人 ([2020](#bib.bib21))。KB-BINDER Li 等人 ([2023c](#bib.bib12))
    首先生成草稿作为初步逻辑形式，然后使用实体和关系绑定器将草稿绑定到可执行形式。然而，这些方法的有效性在很大程度上依赖于生成查询的质量和 KGs 的完整性。RA
    方法从 KG 中检索相关信息以提高推理性能 Li 等人 ([2023b](#bib.bib11))。ToG Sun 等人 ([2023](#bib.bib20))
    将 LLM 视为代理，逐步在 KGs 上互动地探索关系路径，并基于检索到的路径进行推理。RoG Luo 等人 ([2023b](#bib.bib15)) 首先生成作为忠实计划的关系路径，然后使用这些路径从
    KGs 中检索有效的推理路径供 LLMs 进行推理。我们的 GoG 属于检索增强方法，我们还利用了 LLMs 的知识建模能力、语义解析能力和推理能力。
- en: LLM reasoning with Prompting. Many works have been proposed to elicit the reasoning
    ability of LLMs to solve complex tasks through prompting Wei et al. ([2023](#bib.bib27));
    Khot et al. ([2023](#bib.bib9)). Complex CoT Fu et al. ([2023](#bib.bib5)) creates
    and refine rationale examples with more reasoning steps to elicit better reasoning
    in LLMs. Self-Consistency Wang et al. ([2023b](#bib.bib26)) fully explores various
    ways of reasoning to improve their performance on reasoning tasks. DecomP Khot
    et al. ([2023](#bib.bib9)) solves complex tasks by instead decomposing them into
    simpler sub-tasks and delegating these to sub-task specific LLMs. ReAct Yao et al.
    ([2023](#bib.bib28)) treats LLMs as agents that interact with the environment
    and make decisions to retrieve information from external source. GoG can be viewed
    as a fusion of ReAct and DecomP, thereby enabling a more comprehensive utilization
    of the diverse capabilities inherent in LLMs for addressing complex questions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 推理与提示。许多研究已经提出了通过提示来引发 LLM 解决复杂任务的推理能力，如 Wei 等人 ([2023](#bib.bib27)) 和 Khot
    等人 ([2023](#bib.bib9))。复杂 CoT Fu 等人 ([2023](#bib.bib5)) 通过创建和优化包含更多推理步骤的理由示例，以引发
    LLM 更好的推理。自一致性 Wang 等人 ([2023b](#bib.bib26)) 完全探索了各种推理方式，以提高其在推理任务上的表现。DecomP
    Khot 等人 ([2023](#bib.bib9)) 通过将复杂任务分解为更简单的子任务，并将这些任务委托给特定的子任务 LLMs 来解决复杂任务。ReAct
    Yao 等人 ([2023](#bib.bib28)) 将 LLM 视为与环境互动的代理，并做出决策以从外部源中检索信息。GoG 可以被视为 ReAct 和
    DecomP 的融合，从而更全面地利用 LLM 内在的多种能力来解决复杂问题。
- en: 3 Preliminary
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步
- en: In this section, we firstly introduce Knowledge Graph (KGs). Then, we use symbols
    of KGs to define relation path and Knowledge Graph Question Answering (KGQA).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍知识图谱 (KGs)。然后，我们使用 KGs 的符号来定义关系路径和知识图谱问答 (KGQA)。
- en: Knowledge Graphs (KG) is a set of factual triples, i.e., $\mathcal{G}=\{(h,r,t)\in\mathcal{V}\times\mathcal{R}\times\mathcal{V}\}$
    represents the relation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱 (KG) 是一组事实三元组，即 $\mathcal{G}=\{(h,r,t)\in\mathcal{V}\times\mathcal{R}\times\mathcal{V}\}$
    表示关系。
- en: Knowledge Graph Question Answering (KGQA) is a reasoning task that aims to predict
    answer entities $e_{a}\in\mathcal{A}_{q}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱问答 (KGQA) 是一种推理任务，旨在预测答案实体 $e_{a}\in\mathcal{A}_{q}$。
- en: 4 Incomplete Knowledge Graph Question Answering (IKGQA)
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 不完整知识图谱问答 (IKGQA)
- en: 4.1 Task Introduction
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 任务介绍
- en: IKGQA differs from KGQA in that, in IKGQA, $\exists i\in[1,l],\;(e_{i-1},r_{i},e_{i})\notin\mathcal{G}$.
    Therefore, models need to recall their inherent knowledge or reasoning from subgraph
    information.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: IKGQA 与 KGQA 的区别在于，在 IKGQA 中，$\exists i\in[1,l],\;(e_{i-1},r_{i},e_{i})\notin\mathcal{G}$。因此，模型需要从子图信息中回忆其固有知识或进行推理。
- en: 4.2 Datasets Construction
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据集构建
- en: 'We construct two IKGQA datasets based on two widely used KGQA datasets: WebQuestionSP
    (WebQSP) Yih et al. ([2016b](#bib.bib30)) and Complex WebQuestion (CWQ) Talmor
    and Berant ([2018](#bib.bib22)). Both datasets use Freebase Bollacker et al. ([2008](#bib.bib2))
    as their background KG. To simulate incomplete KG, we delete some crucial triples,
    which appear in the gold relation path, of each question from the original KG.
    By doing this, simple semantic parsing methods almost fail to obtain the correct
    answers (except for some scenarios with multiple golden paths). In order to save
    computational cost, we selected the first 1,000 samples of these two datasets
    for constructing IKGQA questions.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于两个广泛使用的 KGQA 数据集构建了两个 IKGQA 数据集：WebQuestionSP (WebQSP) Yih 等人（[2016b](#bib.bib30)）和
    Complex WebQuestion (CWQ) Talmor 和 Berant（[2018](#bib.bib22)）。这两个数据集都使用 Freebase
    Bollacker 等人（[2008](#bib.bib2)）作为其背景 KG。为了模拟不完全 KG，我们从原始 KG 中删除了每个问题的金标准关系路径中出现的一些关键三元组。这样做会导致简单的语义解析方法几乎无法得到正确答案（除了有多个黄金路径的某些场景）。为了节省计算成本，我们选择了这两个数据集中的前
    1,000 个样本用于构建 IKGQA 问题。
- en: 'For simplicity, we only consider one topic entity here. The process of generating
    crucial triples of a question is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们在这里只考虑一个主题实体。生成问题关键三元组的过程如下：
- en: '1.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Extract the topic entity $e_{q}$ by LLMs.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过 LLMs 提取主题实体 $e_{q}$。
- en: '2.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Execute the SPARQL question $s_{q}$, and get all involved triples.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行 SPARQL 问题 $s_{q}$，并获取所有相关三元组。
- en: '3.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Filter property node (e.g., time, text, height), then convert the involved triples
    into a subgraph $g$.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过滤属性节点（例如，时间、文本、高度），然后将相关三元组转换为子图 $g$。
- en: '4.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Utilize breadth first search (BFS) to find the shortest path $p$.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用广度优先搜索（BFS）找到最短路径 $p$。
- en: '5.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Sample $k$ as the crucial triples in the question.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 $k$ 作为问题中的关键三元组。
- en: 5 Generate-on-Graph (GoG)
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 Generate-on-Graph (GoG)
- en: 'In this section, we introduce our method Generate-on-Graph (GoG), which can
    integrate the knowledge of KGs and LLMs, as well as utilize the reasoning ability
    of LLMs. The comparison between GoG and other previous methods is illustrated
    in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM
    as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的方法 Generate-on-Graph (GoG)，该方法可以整合知识图谱（KGs）和大语言模型（LLMs）的知识，并利用
    LLMs 的推理能力。GoG 与其他先前方法的比较如图 [3](#S1.F3 "图 3 ‣ 1 引言 ‣ Generate-on-Graph：在不完全知识图谱问答中将
    LLM 视为代理和 KG") 所示。
- en: 'LLM as Agent. Motivated by ReAct Yao et al. ([2023](#bib.bib28)), we consider
    the LLM as an agent interacting with an environment to solve tasks. As shown in
    Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering") (d), for
    each step $i$ is the action space, to search information by calling graph database
    API (Act 1, 2) or generate more information by reasoning and inherent knowledge
    (Act 4).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 作为代理。受到 ReAct Yao 等人（[2023](#bib.bib28)）的启发，我们将 LLM 视为与环境互动以解决任务的代理。如图 [3](#S1.F3
    "图 3 ‣ 1 引言 ‣ Generate-on-Graph：在不完全知识图谱问答中将 LLM 视为代理和 KG") (d) 所示，对于每一步 $i$ 是动作空间，通过调用图数据库
    API 搜索信息（动作 1、2）或通过推理和固有知识生成更多信息（动作 4）。
- en: 'Action Space. GoG uses the selecting-generating-answering framework, which
    consists of three main actions: Search, Generate and Answer.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间。GoG 使用选择-生成-回答框架，该框架包含三个主要动作：搜索、生成和回答。
- en: '1\. Search[target entity], which aims to find the most relevant top K neighbors
    of the target entity $e$ linking to the target entity. This process is completed
    by pre-defined SPARQL queries. Then, we utilize the LLMs to select the top K relations
    that most related to the current sub-question (last thought). As shown in Act
    2 of Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM
    as both Agent and KG in Incomplete Knowledge Graph Question Answering") (d), given
    the target entity Cupertino, LLMs select the two relation {Located_in, Adjoin}
    from all neighbor relations. In the end, triples {(Cupertino, located_in, California),
    (Cupertino, adjoin, Palo Alto)} are appended to context as Obs 3.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '1\. 搜索[target entity]，旨在找到与目标实体$e$最相关的前K个邻居，并将其链接到目标实体。此过程由预定义的SPARQL查询完成。然后，我们利用LLMs选择与当前子问题（最后的思考）最相关的前K个关系。如图[3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both Agent and KG
    in Incomplete Knowledge Graph Question Answering") (d)所示，给定目标实体Cupertino，LLMs从所有邻接关系中选择了两个关系{Located_in,
    Adjoin}。最后，将三元组{(Cupertino, located_in, California), (Cupertino, adjoin, Palo
    Alto)}添加到上下文中，作为Obs 3。'
- en: '2\. Generate[sub-question], which make the agent generate new factual triples
    based on retrieval information and inherent knowledge. As shown in Act 4 of Figure
    [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering") (d), although there
    is no triple directly representing the timezone of Cupertino, the agent can still
    infer the timezone of Cupertino based on {(Cupertino, located_in, California),
    (California, timezone, Pacific Standard Time)}. This process is similar to knowledge
    graph completion (KGC) Wang et al. ([2017](#bib.bib25)). It is also possible for
    the agent directly generates an entity, which is not explored before, based on
    its inherent knowledge. Therefore, we have to link the entity to its corresponding
    MID in the KG. This entity linking process is divided into two steps: (1) We retrieve
    some similar entities and their corresponding types based BM25 scores. (2) We
    utilize the LLM to select the most relevant entity based on the types.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '2\. 生成[sub-question]，使得代理基于检索信息和固有知识生成新的事实三元组。如图[3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") (d)所示，虽然没有三元组直接表示Cupertino的时区，但代理仍然可以根据{(Cupertino, located_in,
    California), (California, timezone, Pacific Standard Time)}推断出Cupertino的时区。此过程类似于知识图谱补全（KGC）Wang
    et al. ([2017](#bib.bib25))。代理也可能直接基于其固有知识生成之前未探索的实体。因此，我们必须将实体链接到其在KG中的相应MID。这个实体链接过程分为两个步骤：（1）我们根据BM25得分检索一些相似实体及其对应类型。（2）我们利用LLM根据类型选择最相关的实体。'
- en: 3\. Finish[answer], indicates that the agent finishes the task with answer.
    It should be noticed that the agent would also generate "Finish[unknown]", which
    means that there is not enough information for the agent to answer the question.
    In this case, we would roll back and search one more hop neighbors of the last
    target entity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 完成[answer]，表示代理完成了任务并给出了答案。需要注意的是，代理也会生成“完成[unknown]”，这意味着代理没有足够的信息来回答问题。在这种情况下，我们将回退并搜索最后一个目标实体的一个更多邻居。
- en: '| Method | CWQ |  | WebQSP |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | CWQ |  | WebQSP |  |'
- en: '| without external knowledge |  |  |  |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 无外部知识 |  |  |  |  |'
- en: '| IO prompt | 37.6 |  | 63.3 |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| IO 提示 | 37.6 |  | 63.3 |  |'
- en: '| CoT | 38.8 |  | 62.2 |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 38.8 |  | 62.2 |  |'
- en: '| CoT+SC | 45.4 |  | 61.1 |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| CoT+SC | 45.4 |  | 61.1 |  |'
- en: '| RoG w/o planning | 43.0 |  | 66.9 |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| RoG 无规划 | 43.0 |  | 66.9 |  |'
- en: '| with external knowledge |  |  |  |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 有外部知识 |  |  |  |  |'
- en: '|  | CKG | IKG-1 | CKG | IKG-1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | CKG | IKG-1 | CKG | IKG-1 |'
- en: '| w training |  |  |  |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 有训练 |  |  |  |  |'
- en: '| RoG | 64.5 | 51.1 | 88.7 | 70.1 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| RoG | 64.5 | 51.1 | 88.7 | 70.1 |'
- en: '| ChatKBQA | 76.5 | 35.0 | 78.1 | 42.6 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ChatKBQA | 76.5 | 35.0 | 78.1 | 42.6 |'
- en: '| w/o training |  |  |  |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 无训练 |  |  |  |  |'
- en: '| KB-BINDER | - | - | 50.7 | 34.4 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| KB-BINDER | - | - | 50.7 | 34.4 |'
- en: '| StructGPT | - | - | 76.4 | 53.7 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| StructGPT | - | - | 76.4 | 53.7 |'
- en: '| ToG | 47.2 | 36.7 | 76.9 | 60.6 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ToG | 47.2 | 36.7 | 76.9 | 60.6 |'
- en: '| GoG w/GPT-3.5 (Ours) | 55.7 | 43.4 | 78.7 | 64.9 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GoG w/GPT-3.5（我们的） | 55.7 | 43.4 | 78.7 | 64.9 |'
- en: '| GoG w/GPT-4 (Ours) | 75.2 | 61.0 | 84.4 | 74.8 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| GoG w/GPT-4（我们的） | 75.2 | 61.0 | 84.4 | 74.8 |'
- en: 'Table 1: The Hits@1 scores of different models over two datasets in different
    settings (%). CKG and IKG-1 denote using complete and incomplete KG (dropping
    one crucial triple for each question), respectively. We use GPT-3.5 as backbone
    of KB-BINDER, StructGPT and ToG. Results of the other baselines were re-run by
    us, more details can be found in Appendix [C](#A3 "Appendix C Settings for Baselines
    ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").
    The boldface indicates the best result in the same setting.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：不同模型在不同设置下对两个数据集的Hits@1得分（%）。CKG和IKG-1分别表示使用完整和不完整的KG（为每个问题丢弃一个关键三元组）。我们使用GPT-3.5作为KB-BINDER、StructGPT和ToG的基础。其他基线的结果由我们重新运行，更多详细信息可以在附录[C](#A3
    "附录 C 基线设置 ‣ 附录 B 提示列表 ‣ 附录 A 语义解析方法详情 ‣ Generate-on-Graph: 将LLM视为不完整知识图谱问答中的代理和KG")中找到。粗体表示相同设置下的最佳结果。'
- en: 6 Experiments
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 6.1 Experiments Setup
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实验设置
- en: Evaluation Metrics
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: Following previous works Li et al. ([2023d](#bib.bib13)); Jiang et al. ([2023](#bib.bib8));
    Sun et al. ([2023](#bib.bib20)), we use Hits@1 as our evaluation metric, which
    measures the proportion of questions whose top-1 predicted answer is correct.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的研究李等人（[2023d](#bib.bib13)）；姜等人（[2023](#bib.bib8)）；孙等人（[2023](#bib.bib20)），我们使用Hits@1作为我们的评估指标，衡量的是预测的
    top-1 答案正确的问题的比例。
- en: Baselines
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: 'The baselines we compare can be divided into two groups: (1) LLM only methods,
    including standard prompting (IO prompt) Brown et al. ([2020](#bib.bib3)), Chain-of-Thought
    (CoT) prompting Wei et al. ([2023](#bib.bib27)) and Self-Consistency (SC) Wang
    et al. ([2023b](#bib.bib26)). (2) Semantic Parsing (SP) methods, including KB-BINDER
    Li et al. ([2023c](#bib.bib12)) and ChatKBQA Luo et al. ([2023a](#bib.bib14)).
    (3) Retrieval Augmented (RA) methods, including StructGPT Jiang et al. ([2023](#bib.bib8)),
    RoG Luo et al. ([2023b](#bib.bib15)) and ToG Sun et al. ([2023](#bib.bib20)),
    where RoG is the SOTA among all models requiring fine-tuning. All these methods
    are evaluated in both complete incomplete KGs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较的基线可以分为两组：（1）仅使用LLM的方法，包括标准提示（IO prompt）布朗等人（[2020](#bib.bib3)）、思维链（CoT）提示魏等人（[2023](#bib.bib27)）和自一致性（SC）王等人（[2023b](#bib.bib26)）。（2）语义解析（SP）方法，包括KB-BINDER李等人（[2023c](#bib.bib12)）和ChatKBQA罗等人（[2023a](#bib.bib14)）。（3）检索增强（RA）方法，包括StructGPT姜等人（[2023](#bib.bib8)）、RoG罗等人（[2023b](#bib.bib15)）和ToG孙等人（[2023](#bib.bib20)），其中RoG在所有需要微调的模型中是SOTA。这些方法在完整和不完整的KG中都进行了评估。
- en: Experiment Details
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验详情
- en: 'We use two LLMs as the backbone in our experiments: GPT-3.5 and GPT-4\. We
    do not use Llama-2-70b-chat-hf Touvron et al. ([2023](#bib.bib23)), as we found
    that Llama’s output sometimes did not follow the format we defined in few-shot.
    We use OpenAI API to call GPT-3.5 and GPT-4\. The maximum token length for each
    generation is set to 256\. The temperature parameter is set to 0.7 in all experiments.
    We use 3 shots in GoG prompts for all the datasets. The prompts we use are listed
    in Appendix [B](#A2 "Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods
    Details ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge
    Graph Question Answering"). The details of re-running other baselines can be found
    in Appendix [C](#A3 "Appendix C Settings for Baselines ‣ Appendix B Prompt List
    ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在实验中使用了两种LLM作为基础：GPT-3.5和GPT-4。我们没有使用Llama-2-70b-chat-hf图伏龙等人（[2023](#bib.bib23)），因为我们发现Llama的输出有时不符合我们在少量示例中定义的格式。我们使用OpenAI
    API调用GPT-3.5和GPT-4。每次生成的最大token长度设置为256。温度参数在所有实验中设置为0.7。我们在GoG提示中对所有数据集使用3个示例。我们使用的提示列在附录[B](#A2
    "附录 B 提示列表 ‣ 附录 A 语义解析方法详情 ‣ Generate-on-Graph: 将LLM视为不完整知识图谱问答中的代理和KG")。其他基线重新运行的详细信息可以在附录[C](#A3
    "附录 C 基线设置 ‣ 附录 B 提示列表 ‣ 附录 A 语义解析方法详情 ‣ Generate-on-Graph: 将LLM视为不完整知识图谱问答中的代理和KG")中找到。'
- en: '|  | IKG-1 | IKG-2 | IKG-3 | IKG-4 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | IKG-1 | IKG-2 | IKG-3 | IKG-4 |'
- en: '| CWQ | 2.3 | 4.3 | 5.9 | 6.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| CWQ | 2.3 | 4.3 | 5.9 | 6.8 |'
- en: '| WebQSP | 2.1 | 3.6 | 4.6 | 5.4 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| WebQSP | 2.1 | 3.6 | 4.6 | 5.4 |'
- en: 'Table 2: The average number of edges deleted under different incompleteness
    degrees.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同不完整度下删除的边的平均数量。
- en: Datasets Details
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集详情
- en: 'For each dataset, we generate four incomplete KGs with varying degrees of completeness:
    IKG-1/2/3/4, representing randomly dropping 1/2/3/4 crucial triples in the KG.
    In addition to the crucial triples themselves, all relations between these two
    entities will also be deleted. The statistics of these incomplete KGs are shown
    in Table [2](#S6.T2 "Table 2 ‣ Experiment Details ‣ 6.1 Experiments Setup ‣ 6
    Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"). Besides, we also ensure that after deleting
    these crucial triples, the number of neighbor nodes of the topic entities will
    not be zero, more details can be found in Appendix [D](#A4 "Appendix D Statistics
    of Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt
    List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '对于每个数据集，我们生成了四个不完整的 KG，完整度各不相同：IKG-1/2/3/4，代表在 KG 中随机丢弃 1/2/3/4 个关键三元组。除了关键三元组本身外，这两个实体之间的所有关系也将被删除。这些不完整
    KG 的统计数据见表 [2](#S6.T2 "Table 2 ‣ Experiment Details ‣ 6.1 Experiments Setup ‣
    6 Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering")。此外，我们还确保在删除这些关键三元组后，主题实体的邻居节点数量不会为零，更多细节可以在附录
    [D](#A4 "Appendix D Statistics of Topic Entities in IKGs ‣ Appendix C Settings
    for Baselines ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") 中找到。'
- en: '|       Method |       WebQSP |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|       Method |       WebQSP |'
- en: '|       CKG |       IKG-1 |       IKG-2 |       IKG-3 |       IKG-4 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|       CKG |       IKG-1 |       IKG-2 |       IKG-3 |       IKG-4 |'
- en: '|       StructGPT |       76.4 |       52.5 |       49.1 |       47.9 |       46.4
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|       StructGPT |       76.4 |       52.5 |       49.1 |       47.9 |       46.4
    |'
- en: '|       ToG |       76.9 |       60.6 |       57.8 |       58.0 |       57.3
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|       ToG |       76.9 |       60.6 |       57.8 |       58.0 |       57.3
    |'
- en: '|       GoG |       78.7 |       62.2 |       60.9 |       59.6 |       57.4
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|       GoG |       78.7 |       62.2 |       60.9 |       59.6 |       57.4
    |'
- en: '|  |       CWQ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |       CWQ |'
- en: '|  |       CKG |       IKG-1 |       IKG-2 |       IKG-3 |       IKG-4 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |       CKG |       IKG-1 |       IKG-2 |       IKG-3 |       IKG-4 |'
- en: '|       ToG |       47.2 |       36.7 |       33.1 |       32.2 |       31.7
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|       ToG |       47.2 |       36.7 |       33.1 |       32.2 |       31.7
    |'
- en: '|       GoG |       55.7 |       41.6 |       37.2 |       36.7 |       36.5
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|       GoG |       55.7 |       41.6 |       37.2 |       36.7 |       36.5
    |'
- en: 'Table 3: The Hits@1 scores of prompt based methods (w/ GPT-3.5) under different
    numbers of missing triples (%). CKG represent using the complete KG. IKG-1/2/3/4
    represent randomly dropping 1/2/3/4 crucial triples in the KG.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在不同缺失三元组数量（%）下的基于提示的方法的 Hits@1 分数（w/ GPT-3.5）。CKG 代表使用完整的 KG。IKG-1/2/3/4
    代表在 KG 中随机丢弃 1/2/3/4 个关键三元组。'
- en: 6.2 Main Results
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 主要结果
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    shows the Hits@1 scores of GoG and all baselines on two datasets in different
    settings. From the table, we can find that, compared with other prompt based methods,
    GoG can achieve the state-of-the-art performance on CWQ and WebQSP in both complete
    and incomplete KG settings.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering") 显示了
    GoG 和所有基线方法在两个数据集上的 Hits@1 分数。在表格中，我们可以发现，与其他基于提示的方法相比，GoG 在完整和不完整 KG 设置下均能在 CWQ
    和 WebQSP 上实现最先进的性能。'
- en: 'In the CKG setting, the excellent performance of GoG mainly comes from its
    dynamic subgraph expansion strategy, which performs better than ToG in question
    involving compound value types (CVTs), an example of CVT is demonstrated in Figure
    [4](#S6.F4 "Figure 4 ‣ 6.2 Main Results ‣ 6 Experiments ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering"). ToG
    is likely to think CVT nodes are not worthy to further explore and ignore them,
    as they do not offer information directly. Our GoG can easily solve this problem
    by expanding subgraph dynamically, that means if there is not enough information
    provided by the current subgraph, GoG would search one more hop, so the neighbors
    of CVT nodes is taken into consideration in this way.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在CKG设置下，GoG的优秀表现主要来自其动态子图扩展策略，在涉及复合值类型（CVTs）的问题中表现优于ToG，图[4](#S6.F4 "图4 ‣ 6.2
    主要结果 ‣ 6 实验 ‣ Generate-on-Graph: 将LLM视为不完整知识图谱问答中的代理和KG")展示了CVT的示例。ToG可能认为CVT节点不值得进一步探索而忽略它们，因为它们没有直接提供信息。我们的GoG通过动态扩展子图可以轻松解决这个问题，这意味着如果当前子图提供的信息不足，GoG将再搜索一跳，从而考虑到CVT节点的邻居。'
- en: 'In the IKG setting, the performance of SP methods, such as ChatKBQA and KB-BINDER,
    significantly declines. This is expected, as these SP methods don’t interact with
    the KGs, which means they have no idea of the absence of some triples. The performance
    of RoG, StructGPT and ToG also drop significantly. The performance of ToG and
    StructGPT on IKG is even worse than that without KG (IO prompt and CoT). That
    means, these methods still play a role of parsing to find answers rather than
    effectively integrating internal and external knowledge sources. Besides, in the
    IKG setting, these methods are even likely to retrieve wrong or unrelated paths
    which disturb LLMs. Our GoG can alleviate the problem by using the generate action,
    which utilizes the LLM to generate new factual triples when no direct answer is
    found after multiple rounds of searches on the KGs. Detailed analysis of answers
    generated by GoG can be checked in Appendix [E](#A5 "Appendix E Result Analysis
    ‣ Appendix D Statistics of Topic Entities in IKGs ‣ Appendix C Settings for Baselines
    ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '在IKG设置下，SP方法的性能，如ChatKBQA和KB-BINDER，显著下降。这是预期中的情况，因为这些SP方法不与KG互动，这意味着它们不了解某些三元组的缺失。RoG、StructGPT和ToG的性能也显著下降。ToG和StructGPT在IKG上的表现甚至比没有KG（IO提示和CoT）时更差。这意味着这些方法仍然主要作为解析工具来寻找答案，而不是有效地整合内部和外部知识源。此外，在IKG设置下，这些方法甚至可能检索到错误或无关的路径，从而干扰LLM。我们的GoG通过使用生成操作来缓解这一问题，该操作利用LLM在多轮KG搜索后未找到直接答案时生成新的事实三元组。GoG生成答案的详细分析可以在附录[E](#A5
    "附录E 结果分析 ‣ 附录D IKG中主题实体的统计 ‣ 附录C 基准设置 ‣ 附录B 提示列表 ‣ 附录A 语义解析方法详情 ‣ Generate-on-Graph:
    将LLM视为不完整知识图谱问答中的代理和KG")中查看。'
- en: '![Refer to caption](img/8021fa6694da753388ce7bf359a283b6.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8021fa6694da753388ce7bf359a283b6.png)'
- en: 'Figure 4: An example of compound value types (CVTs) in Freebase dataset. Blue,
    green and orange nodes denote normal entities, CVT node and property node.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Freebase数据集中复合值类型（CVTs）的示例。蓝色、绿色和橙色节点分别表示正常实体、CVT节点和属性节点。
- en: 6.3 Performance under Different Numbers of Missing Triples
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 不同缺失三元组数量下的性能
- en: 'In order to explore the impact of different degrees of KG incompleteness on
    different methods, we evaluate the performance of methods (w/ GPT-3.5) under different
    numbers of missing triples, the results are demonstrated in Table [3](#S6.T3 "Table
    3 ‣ Datasets Details ‣ 6.1 Experiments Setup ‣ 6 Experiments ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探讨KG不完整程度对不同方法的影响，我们评估了不同数量缺失三元组下的方法（使用GPT-3.5）的性能，结果展示在表[3](#S6.T3 "表3 ‣
    数据集详情 ‣ 6.1 实验设置 ‣ 6 实验 ‣ Generate-on-Graph: 将LLM视为不完整知识图谱问答中的代理和KG")中。'
- en: It can be found that our GoG outperforms other prompt based methods consistently
    in different numbers of missing triples. Especially on the CWQ dataset, our GoG
    has a significant improvement on Hits@1 score, achieving average 7.5% improvement
    under all settings. That emphasizes the importance of integrate the external and
    inherent knowledge of LLMs. On the contrary, the performance of ToG on IKG is
    even much lower than that without KG, which indicates the performance of ToG still
    depends heavily on the completeness of KGs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以发现，我们的 GoG 在不同数量的缺失三元组情况下始终优于其他基于提示的方法。特别是在 CWQ 数据集上，我们的 GoG 在 Hits@1 分数上有显著提高，在所有设置下平均提升
    7.5%。这强调了整合 LLM 外部和固有知识的重要性。相反，ToG 在 IKG 上的表现甚至比没有 KG 时更差，这表明 ToG 的性能仍然严重依赖于 KG
    的完整性。
- en: 'Furthermore, even though the majority of questions in the WebQSP dataset are
    single-hop questions, GoG can still demonstrate its advantages and perform better
    than ToG and StructGPT. This is because GoG can leverage the neighboring information
    of the topic entities to predict the tail entities while other methods can not
    make full use these information. More details can be found in Appendix [F](#A6
    "Appendix F Case Study ‣ Appendix E Result Analysis ‣ Appendix D Statistics of
    Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt
    List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，尽管 WebQSP 数据集中大多数问题都是单跳问题，但 GoG 仍然能够展示其优势，表现优于 ToG 和 StructGPT。这是因为 GoG
    可以利用主题实体的邻近信息来预测尾实体，而其他方法无法充分利用这些信息。更多细节请参见附录 [F](#A6 "附录 F 案例研究 ‣ 附录 E 结果分析 ‣
    附录 D IKG 中主题实体的统计 ‣ 附录 C 基准设置 ‣ 附录 B 提示列表 ‣ 附录 A 语义解析方法详细 ‣ Generate-on-Graph:
    将 LLM 视为不完全知识图谱问答中的代理和知识图谱")。'
- en: 6.4 Performance with Different LLMs
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 不同 LLM 的性能
- en: '| Method | WebQSP |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | WebQSP |'
- en: '| CKG | IKG-1 | NKG |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| CKG | IKG-1 | NKG |'
- en: '| GoG w/GPT-3.5 | 78.7 | 64.9 | 62.2 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GoG w/GPT-3.5 | 78.7 | 64.9 | 62.2 |'
- en: '| GoG w/GPT-4 | 84.4 | 74.8 | 65.6 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GoG w/GPT-4 | 84.4 | 74.8 | 65.6 |'
- en: '|  | CWQ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | CWQ |'
- en: '|  | CKG | IKG-1 | NKG |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | CKG | IKG-1 | NKG |'
- en: '| GoG w/GPT-3.5 | 55.7 | 43.4 | 38.8 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GoG w/GPT-3.5 | 55.7 | 43.4 | 38.8 |'
- en: '| GoG w/GPT-4 | 75.2 | 61.0 | 55.6 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GoG w/GPT-4 | 75.2 | 61.0 | 55.6 |'
- en: 'Table 4: The Hits@1 scores of GoG using different backbone models (%). CKG,
    IKG and NKG denote using complete, incomplete and no KG.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: GoG 使用不同主干模型的 Hits@1 分数 (%). CKG、IKG 和 NKG 分别表示使用完整、不完整和无 KG。'
- en: '![Refer to caption](img/6294ffb1797fdb7c96c1b90f7a6e6ec5.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6294ffb1797fdb7c96c1b90f7a6e6ec5.png)'
- en: 'Figure 5: The Hits@1 scores of GoG with different generation strategies on
    the CWQ (a) and WebQSP (b) (%), w subgraph and w/o subgraph represent generating
    new factual triples with and without explored subgraph as context in the Generate
    action, respectively.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: GoG 使用不同生成策略在 CWQ (a) 和 WebQSP (b) 上的 Hits@1 分数 (%), w 子图和 w/o 子图 分别表示在生成操作中使用和不使用探索的子图作为上下文生成新的事实三元组。'
- en: '![Refer to caption](img/73304816b46e9f74193d38ea7713090e.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/73304816b46e9f74193d38ea7713090e.png)'
- en: 'Figure 6: The Hits@1 scores of GoG with different number of related triples
    in the Generate action (%).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: GoG 在生成操作中使用不同数量相关三元组的 Hits@1 分数 (%).'
- en: 'We evaluate how different backbone models affect GoG performance. Table [4](#S6.T4
    "Table 4 ‣ 6.4 Performance with Different LLMs ‣ 6 Experiments ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    demonstrates that the performance of GoG using GPT-4 as backbone improves significantly.
    Especially under complete KGs setting, GoG (w/GPT-4) achieves 84.4 and 75.2 Hits@1
    score on the WebQSP and CWQ datasets respectively, which achieve SOTA performance
    in prompt based methods and outperforms most fine-tuned methods.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了不同主干模型如何影响 GoG 的性能。表 [4](#S6.T4 "表 4 ‣ 6.4 不同 LLM 的性能 ‣ 6 实验 ‣ Generate-on-Graph:
    将 LLM 视为不完全知识图谱问答中的代理和知识图谱") 显示，使用 GPT-4 作为主干的 GoG 性能显著提高。特别是在完整 KG 设置下，GoG (w/GPT-4)
    在 WebQSP 和 CWQ 数据集上分别达到了 84.4 和 75.2 的 Hits@1 分数，这在基于提示的方法中达到了 SOTA 性能，且优于大多数微调方法。'
- en: In addition, we can also find that, the more powerful the backbone model, the
    better it utilizes the incomplete KGs. For example, on the WebQSP dataset, the
    performance of GoG (w/GPT-3.5) using incomplete KG is only 2.5% higher than that
    without using KG, while the improvement increases to 9.2% when using GPT-4 as
    backbone.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还发现，骨干模型越强大，对不完整 KG 的利用效果越好。例如，在 WebQSP 数据集上，使用不完整 KG 的 GoG（w/GPT-3.5）性能仅比不使用
    KG 时高出 2.5%，而当使用 GPT-4 作为骨干时，提升幅度增加到 9.2%。
- en: 6.5 Ablation Study
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 消融研究
- en: The Effect of Explored Subgraphs
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 探索子图的影响
- en: 'To explore the influence of explored subgraphs on the performance of GoG, we
    conduct experiments under different two Generate action (introduced in section
    [5](#S5 "5 Generate-on-Graph (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")) strategies: (1) Utilizing
    explored subgraphs, which is obtained in Search action, as context to generate
    new factual triples. (2) Generating new factual triples directly without explored
    subgraphs. As shown in Figure [5](#S6.F5 "Figure 5 ‣ 6.4 Performance with Different
    LLMs ‣ 6 Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"), GoG’s performance improves significantly
    with the help of explored subgraphs. This implies that, even incomplete KGs don’t
    provide answers directly, the related knowledge they provide is still helpful
    for models generating corresponding knowledge. There are two potential reasons:
    (1) Utilizing related subgraphs as context can activate LLMs’ memory corresponding
    to this knowledge. (2) LLMs can reason new factual triples based on the related
    subgraphs, as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    (d).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探究探索子图对 GoG 性能的影响，我们在不同的 Generate 操作（在第 [5](#S5 "5 Generate-on-Graph (GoG)
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") 节中介绍）策略下进行实验：（1）利用搜索操作中获得的探索子图作为上下文生成新的事实三元组。（2）直接生成新的事实三元组而不使用探索子图。如图
    [5](#S6.F5 "Figure 5 ‣ 6.4 Performance with Different LLMs ‣ 6 Experiments ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    所示，GoG 的性能在有探索子图的帮助下显著提高。这表明，即使不完整的 KG 不直接提供答案，它们提供的相关知识仍然对模型生成相应的知识有帮助。有两个潜在原因：（1）利用相关子图作为上下文可以激活
    LLMs 对应知识的记忆。（2）LLMs 可以基于相关子图推理新的事实三元组，如图 [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") (d) 所示。'
- en: The Effect of the Number of Related Triples
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 相关三元组数量的影响
- en: 'Aiming to find out how many related triples is required in the Generate action,
    we perform additional to find out how the number of related triples effect GoG’s
    performance. We select the most relevant k triples based on BM25. The results
    are shown in Figure [6](#S6.F6 "Figure 6 ‣ 6.4 Performance with Different LLMs
    ‣ 6 Experiments ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete
    Knowledge Graph Question Answering"). It can be observed that, GoG’s performance
    first increases and then decreases as the number of related triples increases.
    This is mainly because noisy and unrelated knowledge are introduced when the number
    of related triples is large. How to filter valuable knowledge from the explored
    subgraph could be a future direction.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '为了找出 Generate 操作中需要多少个相关三元组，我们进行额外实验以探究相关三元组数量对 GoG 性能的影响。我们基于 BM25 选择最相关的
    k 个三元组。结果如图 [6](#S6.F6 "Figure 6 ‣ 6.4 Performance with Different LLMs ‣ 6 Experiments
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") 所示。可以观察到，随着相关三元组数量的增加，GoG 的性能先增加后下降。这主要是因为当相关三元组数量较大时，会引入噪声和不相关的知识。如何从探索子图中过滤出有价值的知识可能是未来的一个方向。'
- en: 7 Conclusion
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we propose leveraging LLMs for QA under Incomplete Knowledge
    Graph (IKGQA), which is closer to real-world scenarios, and construct relevant
    datasets. We propose Generate-on-Graph (GoG), which can effectively integrate
    the external and inherent knowledge of LLMs. Experiments on two datasets show
    the superiority of GoG, and demonstrate that an incomplete KG can still help LLMs
    answer complex questions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了利用 LLMs 在不完全知识图谱（IKGQA）下进行问答的方案，这更接近实际场景，并构建了相关数据集。我们提出了 Generate-on-Graph（GoG），它可以有效整合
    LLMs 的外部和内在知识。对两个数据集的实验表明 GoG 的优越性，并且展示了即使是不完整的 KG 也能帮助 LLMs 回答复杂问题。
- en: Limitation
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'The limitations of our proposed GoG are as follows: (1) We only evaluate GoG
    on CWQ and WebQSP, which uses Freebase as their background KG. The experimental
    data may comes from some specific domains. (2) It is possible for LLM to hallucinate
    in the generation process, which is unavoidable for existing LLMs. (3) There is
    room for further improvement in performance, as GoG’s performance is lower than
    that with CoT prompt when KGs are very incomplete.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 GoG 的局限性如下：（1）我们只在 CWQ 和 WebQSP 上评估 GoG，这些数据集使用 Freebase 作为其背景知识图谱。实验数据可能来自某些特定领域。（2）在生成过程中，LLM
    可能会出现幻觉，这是现有 LLM 无法避免的。（3）性能还有进一步改进的空间，因为在知识图谱非常不完整时，GoG 的性能低于 CoT 提示。
- en: Ethics Statement
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This paper proposes a method for complex question answering in incomplete knowledge
    graph, and the experiments are conducted on public available datasets. As a result,
    there is no data privacy concern. Meanwhile, this paper does not involve human
    annotations, and there are no related ethical concerns.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种在不完整知识图谱中进行复杂问题回答的方法，并在公开数据集上进行了实验。因此，没有数据隐私问题。同时，本文不涉及人工注释，也没有相关的伦理问题。
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale Fung. 2023. [A Multitask, Multilingual, Multimodal Evaluation
    of ChatGPT on Reasoning, Hallucination, and Interactivity](https://doi.org/10.48550/arXiv.2302.04023).
    ArXiv:2302.04023 [cs].
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang 等 (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan
    Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do,
    Yan Xu 和 Pascale Fung. 2023. [ChatGPT 在推理、幻觉和互动方面的多任务、多语言、多模态评估](https://doi.org/10.48550/arXiv.2302.04023).
    ArXiv:2302.04023 [cs].
- en: 'Bollacker et al. (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
    Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database
    for structuring human knowledge. In *Proceedings of the 2008 ACM SIGMOD international
    conference on Management of data*, pages 1247–1250.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bollacker 等 (2008) Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge
    和 Jamie Taylor. 2008. Freebase: 一个协作创建的图数据库，用于结构化人类知识。见于 *2008年ACM SIGMOD国际数据管理会议论文集*，第1247–1250页。'
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language Models are Few-Shot Learners](https://doi.org/10.48550/arXiv.2005.14165).
    ArXiv:2005.14165 [cs].
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 和 Dario
    Amodei. 2020. [语言模型是少样本学习者](https://doi.org/10.48550/arXiv.2005.14165). ArXiv:2005.14165
    [cs].
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. [A Survey on In-context
    Learning](https://doi.org/10.48550/arXiv.2301.00234). ArXiv:2301.00234 [cs].
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li 和 Zhifang Sui. 2023. [关于上下文学习的调查](https://doi.org/10.48550/arXiv.2301.00234).
    ArXiv:2301.00234 [cs].
- en: Fu et al. (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. 2023. [Complexity-Based Prompting for Multi-Step Reasoning](http://arxiv.org/abs/2210.00720).
    ArXiv:2210.00720 [cs].
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark 和 Tushar Khot. 2023.
    [基于复杂度的多步骤推理提示](http://arxiv.org/abs/2210.00720). ArXiv:2210.00720 [cs].
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and
    Ting Liu. 2023. [A Survey on Hallucination in Large Language Models: Principles,
    Taxonomy, Challenges, and Open Questions](http://arxiv.org/abs/2311.05232). ArXiv:2311.05232
    [cs].'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等 (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng,
    Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin 和 Ting Liu.
    2023. [关于大型语言模型中的幻觉: 原则、分类、挑战及开放问题的调查](http://arxiv.org/abs/2311.05232). ArXiv:2311.05232
    [cs].'
- en: 'Ji et al. (2021) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and
    Philip S. Yu. 2021. [A Survey on Knowledge Graphs: Representation, Acquisition
    and Applications](http://arxiv.org/abs/2002.00388). *arXiv:2002.00388 [cs]*. ArXiv:
    2002.00388.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ji et al. (2021) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, 和
    Philip S. Yu. 2021. [关于知识图谱的综述：表示、获取与应用](http://arxiv.org/abs/2002.00388)。 *arXiv:2002.00388
    [cs]*。ArXiv: 2002.00388。'
- en: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023. [StructGPT: A General Framework for Large Language
    Model to Reason over Structured Data](http://arxiv.org/abs/2305.09645). In *EMNLP
    2023*. arXiv. ArXiv:2305.09645 [cs].'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, 和 Ji-Rong Wen. 2023. [StructGPT: 一个用于结构化数据推理的大型语言模型通用框架](http://arxiv.org/abs/2305.09645)。在
    *EMNLP 2023*。arXiv。ArXiv:2305.09645 [cs]。'
- en: 'Khot et al. (2023) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle
    Richardson, Peter Clark, and Ashish Sabharwal. 2023. [Decomposed Prompting: A
    Modular Approach for Solving Complex Tasks](http://arxiv.org/abs/2210.02406).
    In *NIPS 2023*. arXiv. ArXiv:2210.02406 [cs].'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khot et al. (2023) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle
    Richardson, Peter Clark, 和 Ashish Sabharwal. 2023. [分解提示：解决复杂任务的模块化方法](http://arxiv.org/abs/2210.02406)。在
    *NIPS 2023*。arXiv。ArXiv:2210.02406 [cs]。
- en: 'Li et al. (2023a) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and
    Ji-Rong Wen. 2023a. [HaluEval: A Large-Scale Hallucination Evaluation Benchmark
    for Large Language Models](http://arxiv.org/abs/2305.11747). ArXiv:2305.11747
    [cs].'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023a) Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, 和
    Ji-Rong Wen. 2023a. [HaluEval: 大规模幻觉评估基准，用于大型语言模型](http://arxiv.org/abs/2305.11747)。ArXiv:2305.11747
    [cs]。'
- en: Li et al. (2023b) Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li,
    Xifeng Yan, Chao Zhang, and Bing Yin. 2023b. [Graph Reasoning for Question Answering
    with Triplet Retrieval](http://arxiv.org/abs/2305.18742). ArXiv:2305.18742 [cs].
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Shiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin, Zheng Li,
    Xifeng Yan, Chao Zhang, 和 Bing Yin. 2023b. [基于三元组检索的图推理问答](http://arxiv.org/abs/2305.18742)。ArXiv:2305.18742
    [cs]。
- en: Li et al. (2023c) Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, and Wenhu
    Chen. 2023c. [Few-shot In-context Learning on Knowledge Base Question Answering](https://doi.org/10.18653/v1/2023.acl-long.385).
    In *ACL 2023*, pages 6966–6980, Toronto, Canada. Association for Computational
    Linguistics.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023c) Tianle Li, Xueguang Ma, Alex Zhuang, Yu Gu, Yu Su, 和 Wenhu
    Chen. 2023c. [基于知识库问答的少量样本上下文学习](https://doi.org/10.18653/v1/2023.acl-long.385)。在
    *ACL 2023*，第6966–6980页，加拿大多伦多。计算语言学协会。
- en: 'Li et al. (2023d) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq
    Joty, Soujanya Poria, and Lidong Bing. 2023d. [Chain-of-Knowledge: Grounding Large
    Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources](http://arxiv.org/abs/2305.13269).
    ArXiv:2305.13269 [cs].'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023d) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq
    Joty, Soujanya Poria, 和 Lidong Bing. 2023d. [知识链：通过动态知识适应跨异质源来基础大型语言模型](http://arxiv.org/abs/2305.13269)。ArXiv:2305.13269
    [cs]。
- en: 'Luo et al. (2023a) Haoran Luo, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai
    Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin, et al. 2023a. Chatkbqa:
    A generate-then-retrieve framework for knowledge base question answering with
    fine-tuned large language models. *arXiv preprint arXiv:2310.08975*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. (2023a) Haoran Luo, Zichen Tang, Shiyao Peng, Yikai Guo, Wentai
    Zhang, Chenghao Ma, Guanting Dong, Meina Song, Wei Lin 等. 2023a. Chatkbqa: 一个生成-检索框架，用于通过微调的大型语言模型进行知识库问答。
    *arXiv 预印本 arXiv:2310.08975*。'
- en: 'Luo et al. (2023b) Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui
    Pan. 2023b. [Reasoning on Graphs: Faithful and Interpretable Large Language Model
    Reasoning](http://arxiv.org/abs/2310.01061). ArXiv:2310.01061 [cs].'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2023b) Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, 和 Shirui Pan.
    2023b. [图上的推理：可信且可解释的大型语言模型推理](http://arxiv.org/abs/2310.01061)。ArXiv:2310.01061
    [cs]。
- en: Nie et al. (2023) Zhijie Nie, Richong Zhang, Zhongyuan Wang, and Xudong Liu.
    2023. [Code-Style In-Context Learning for Knowledge-Based Question Answering](https://doi.org/10.48550/arXiv.2309.04695).
    ArXiv:2309.04695 [cs].
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie et al. (2023) Zhijie Nie, Richong Zhang, Zhongyuan Wang, 和 Xudong Liu. 2023.
    [基于代码风格的上下文学习用于知识库问答](https://doi.org/10.48550/arXiv.2309.04695)。ArXiv:2309.04695
    [cs]。
- en: 'Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. 2023. [Unifying Large Language Models and Knowledge Graphs: A
    Roadmap](https://doi.org/10.48550/arXiv.2306.08302). ArXiv:2306.08302 [cs].'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    和 Xindong Wu. 2023. [统一大型语言模型和知识图谱：路线图](https://doi.org/10.48550/arXiv.2306.08302)。ArXiv:2306.08302
    [cs]。
- en: Saxena et al. (2020) Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. 2020.
    Improving multi-hop question answering over knowledge graphs using knowledge base
    embeddings. In *Proceedings of the 58th annual meeting of the association for
    computational linguistics*, pages 4498–4507.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena et al. (2020) Apoorv Saxena, Aditay Tripathi, 和 Partha Talukdar. 2020.
    通过知识库嵌入改进多跳问题回答。载于 *第 58 届计算语言学协会年会论文集*，第 4498–4507 页。
- en: 'Sun et al. (2019) Haitian Sun, Tania Bedrax-Weiss, and William W Cohen. 2019.
    Pullnet: Open domain question answering with iterative retrieval on knowledge
    bases and text. *arXiv preprint arXiv:1904.09537*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2019) Haitian Sun, Tania Bedrax-Weiss, 和 William W Cohen. 2019.
    Pullnet: 开放域问题回答，基于知识库和文本的迭代检索。*arXiv 预印本 arXiv:1904.09537*。'
- en: 'Sun et al. (2023) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2023. [Think-on-Graph:
    Deep and Responsible Reasoning of Large Language Model on Knowledge Graph](https://doi.org/10.48550/arXiv.2307.07697).
    ArXiv:2307.07697 [cs].'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2023) Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang,
    Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, 和 Jian Guo. 2023. [Think-on-Graph:
    大语言模型在知识图谱上的深度和负责任的推理](https://doi.org/10.48550/arXiv.2307.07697). ArXiv:2307.07697
    [cs]。'
- en: 'Sun et al. (2020) Yawei Sun, Lingling Zhang, Gong Cheng, and Yuzhong Qu. 2020.
    Sparqa: skeleton-based semantic parsing for complex questions over knowledge bases.
    In *Proceedings of the AAAI conference on artificial intelligence*, volume 34,
    pages 8952–8959.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2020) Yawei Sun, Lingling Zhang, Gong Cheng, 和 Yuzhong Qu. 2020.
    Sparqa: 基于骨架的复杂问题语义解析。载于 *AAAI 人工智能会议论文集*，卷 34，第 8952–8959 页。'
- en: Talmor and Berant (2018) Alon Talmor and Jonathan Berant. 2018. The web as a
    knowledge-base for answering complex questions. *arXiv preprint arXiv:1803.06643*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talmor 和 Berant (2018) Alon Talmor 和 Jonathan Berant. 2018. 将网络作为回答复杂问题的知识库。*arXiv
    预印本 arXiv:1803.06643*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等等. 2023. Llama 2: 开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Wang et al. (2023a) Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen,
    Xinrun Wang, Lei Feng, and Bo An. 2023a. [keqing: knowledge-based question answering
    is a nature chain-of-thought mentor of LLM](http://arxiv.org/abs/2401.00426).
    ArXiv:2401.00426 [cs].'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023a) Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen,
    Xinrun Wang, Lei Feng, 和 Bo An. 2023a. [keqing: 基于知识的问题回答是 LLM 的一种自然思维链导师](http://arxiv.org/abs/2401.00426).
    ArXiv:2401.00426 [cs]。'
- en: 'Wang et al. (2017) Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge
    Graph Embedding: A Survey of Approaches and Applications. *IEEE TRANSACTIONS ON
    KNOWLEDGE AND DATA ENGINEERING*, 29(12):20.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Quan Wang, Zhendong Mao, Bin Wang, 和 Li Guo. 2017. 知识图谱嵌入：方法和应用的综述。*IEEE
    知识与数据工程汇刊*, 29(12):20。
- en: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. [Self-Consistency Improves
    Chain of Thought Reasoning in Language Models](https://doi.org/10.48550/arXiv.2203.11171).
    ArXiv:2203.11171 [cs].
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023b. [自一致性提升语言模型中的思维链推理](https://doi.org/10.48550/arXiv.2203.11171).
    ArXiv:2203.11171 [cs]。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models](https://doi.org/10.48550/arXiv.2201.11903).
    ArXiv:2201.11903 [cs].
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 2023. [Chain-of-Thought 提示引发大语言模型的推理](https://doi.org/10.48550/arXiv.2201.11903).
    ArXiv:2201.11903 [cs]。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. [ReAct: Synergizing Reasoning and Acting
    in Language Models](http://arxiv.org/abs/2210.03629). ArXiv:2210.03629 [cs].'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2023. [ReAct: 在语言模型中协同推理和行动](http://arxiv.org/abs/2210.03629).
    ArXiv:2210.03629 [cs]。'
- en: 'Yih et al. (2016a) Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei
    Chang, and Jina Suh. 2016a. The value of semantic parse labeling for knowledge
    base question answering. In *Proceedings of the 54th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers)*, pages 201–206.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yih 等（2016a）温涛·易，马修·理查森，克里斯托弗·米克，明伟·张和贾娜·苏赫。2016a. 语义解析标签在知识库问答中的价值。见 *第54届计算语言学协会年会论文集（第2卷：短文）*，第201–206页。
- en: 'Yih et al. (2016b) Wen-tau Yih, Matthew Richardson, Christopher Meek, Ming-Wei
    Chang, and Jina Suh. 2016b. The value of semantic parse labeling for knowledge
    base question answering. In *Proceedings of the 54th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers)*, pages 201–206.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yih 等（2016b）温涛·易，马修·理查森，克里斯托弗·米克，明伟·张和贾娜·苏赫。2016b. 语义解析标签在知识库问答中的价值。见 *第54届计算语言学协会年会论文集（第2卷：短文）*，第201–206页。
- en: Zan et al. (2022) Daoguang Zan, Sirui Wang, Hongzhi Zhang, Kun Zhou, Wei Wu,
    Wayne Xin Zhao, Bingchao Wu, Bei Guan, and Yongji Wang. 2022. Complex question
    answering over incomplete knowledge graph as n-ary link prediction. In *2022 International
    Joint Conference on Neural Networks (IJCNN)*, pages 1–8\. IEEE.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zan 等（2022）道光·赞，思睿·王，洪智·张，昆·周，伟·吴，韦恩·辛·赵，冰超·吴，贝·关和永吉·王。2022. 在不完整知识图谱上进行复杂问题回答作为n元链接预测。见
    *2022国际神经网络联合会议（IJCNN）*，第1–8页。IEEE。
- en: Appendix A Semantic Parsing Methods Details
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 语义解析方法详情
- en: 'The training datasets for SP methods are constructed under the complete KGs,
    which means that "Time Zone" corresponds directly to the relation "ns:location.location.time_zones"
    rather than a two-hop path "ns:location.located_in -> ns:location.location.time_zones".
    An example in CWQ is shown in Table [A](#A1 "Appendix A Semantic Parsing Methods
    Details ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge
    Graph Question Answering"). This means SP models trained on CWQ will always output
    "?c ns:location.location.time_zones ?x" instead of "?c ns:location.located_in
    ?y . ?y ns:location.location.time_zones ?x". Therefore, these methods will fail
    under Incomplete KGs. In another word, semantic parsing methods don’t interact
    with the KGs, which means they have no idea of the absence of some triples.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: SP方法的训练数据集是在完整的KG下构建的，这意味着“时区”直接对应于关系“ns:location.location.time_zones”，而不是两跳路径“ns:location.located_in
    -> ns:location.location.time_zones”。CWQ中的一个示例见表 [A](#A1 "附录A 语义解析方法详情 ‣ 生成图：将LLM视为不完全知识图谱问答中的代理和KG")。这意味着在CWQ上训练的SP模型总是输出“?c
    ns:location.location.time_zones ?x”，而不是“?c ns:location.located_in ?y . ?y ns:location.location.time_zones
    ?x”。因此，这些方法在不完整KG下会失败。换句话说，语义解析方法不与KG交互，这意味着它们对某些三元组的缺失一无所知。
- en: '| Question | In the nation that spends the Bahamian dollar as currency, what
    time zone is used? |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 在使用巴哈马元作为货币的国家中，使用什么时区？ |'
- en: '| SPARQL | PREFIX ns:  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| SPARQL | PREFIX ns:  |'
- en: '| SELECT DISTINCT ?x WHERE { |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| SELECT DISTINCT ?x WHERE { |  |'
- en: '| FILTER (?x != ?c) FILTER (!isLiteral(?x) OR lang(?x) = ” OR langMatches(lang(?x),
    ’en’)) |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| FILTER (?x != ?c) FILTER (!isLiteral(?x) OR lang(?x) = ” OR langMatches(lang(?x),
    ’en’)) |  |'
- en: '| ?c ns:location.country.currency_used ns:m.01l6dm . |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ?c ns:location.country.currency_used ns:m.01l6dm . |  |'
- en: '| ?c ns:location.location.time_zones ?x . |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ?c ns:location.location.time_zones ?x . |  |'
- en: '| } |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| } |  |'
- en: 'Table 6: An example about "timezone" in the CWQ dataset.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：CWQ数据集中关于“时区”的示例。
- en: Appendix B Prompt List
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 提示列表
- en: 'The prompts used in GoG are shown in Table [B](#A2 "Appendix B Prompt List
    ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering").'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: GoG中使用的提示见表 [B](#A2 "附录B 提示列表 ‣ 附录A 语义解析方法详情 ‣ 生成图：将LLM视为不完全知识图谱问答中的代理和KG")。
- en: '| Tasks | Prompt |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 提示 |  |  |'
- en: '| GoG Instruction | Solve a question answering task with interleaving Thought,
    Action, Observation steps. Thought can reason about the current situation, and
    Action can be three types: |  |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| GoG 指令 | 通过交替的思考、行动、观察步骤解决问答任务。思考可以推理当前情况，行动可以有三种类型： |  |  |'
- en: '| (1) Search[entity1 &#124; entity2 &#124; …], which searches the exact entities
    on Freebase and returns their one-hop subgraphs. You should extract the all concrete
    entities appeared in your last thought without redundant words, and you should
    always select entities from topic entities in the first search. |  |  |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| (1) 搜索[entity1 &#124; entity2 &#124; …]，在Freebase上搜索精确的实体并返回其单跳子图。您应提取上一个想法中出现的所有具体实体，避免冗余词语，并且在第一次搜索中应始终从主题实体中选择实体。
    |  |  |  |'
- en: '| (2) Generate[thought], which generate some new triples related to your last
    thought. These new triples may come from your inherent knowledge directly or reasoning
    from the given triples. |  |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| (2) 生成[thought]，生成一些与您最后的想法相关的新三元组。这些新三元组可以直接来自您的内在知识或通过给定三元组进行推理。 |  |  |  |'
- en: '| (3) Finish[answer1 &#124; answer2 &#124; …], which returns the answer and
    finishes the task. The answers should be complete entity label appeared in the
    triples. If you don’t know the answer, please output Finish[unknown]. |  |  |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| (3) 完成[answer1 &#124; answer2 &#124; …]，返回答案并完成任务。答案应为三元组中出现的完整实体标签。如果您不知道答案，请输出完成[unknown]。
    |  |  |  |'
- en: '| Entities and answers should be separated by tab. |  |  |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 实体和答案应通过制表符分隔。 |  |  |  |'
- en: '| Attention please, entities begin with "m." (e.g., m.01041p3) represent CVT
    (compound value type) node, and they shouldn’t be selected as the final answers.
    To find out those entities involved in these event, you could select them as the
    entities to be searched. |  |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 请注意，以“m.”开头的实体（例如，m.01041p3）代表CVT（复合值类型）节点，不应被选作最终答案。要找出涉及这些事件的实体，您可以将它们作为要搜索的实体进行选择。
    |  |  |  |'
- en: '| You should generate each step without redundant words. |  |  |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 您应在每一步中生成不含冗余词语的内容。 |  |  |  |'
- en: '| Here are some examples. |  |  |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 这里有一些示例。 |  |  |  |'
- en: '| In-Context Few-shot |  |  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 上下文中的少量示例 |  |  |  |'
- en: '| Question: {Question} |  |  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 问题：{Question} |  |  |  |'
- en: '| Topic Entity: {List of Topic Entities} |  |  |  |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 主题实体：{List of Topic Entities} |  |  |  |'
- en: '| Thought 1: |  |  |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 想法 1： |  |  |  |'
- en: '| Filter Relations | Please select 3 relations that most relevant to the question
    and rank them. You should answer these relations in list format directly without
    redundant words. |  |  |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 过滤关系 | 请从最相关的3个关系中进行选择并排序。您应直接以列表格式回答这些关系，避免冗余词语。 |  |  |'
- en: '| Here are some examples. |  |  |  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 这里有一些示例。 |  |  |  |'
- en: '| In-Context Few-shot |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 上下文中的少量示例 |  |  |  |'
- en: '| Thought: {Thought} |  |  |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 想法：{Thought} |  |  |  |'
- en: '| Entity: {Entity} |  |  |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 实体：{Entity} |  |  |  |'
- en: '| Relation: {List of Relations} |  |  |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 关系：{List of Relations} |  |  |  |'
- en: '| Answer: |  |  |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 答案： |  |  |  |'
- en: '| Generate Triples | Given the existing triples, please generate some new triples
    related to your current thought. These new triples may come from your inherent
    knowledge directly or reasoning from the given triples. |  |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 生成三元组 | 给定现有三元组，请生成一些与您当前想法相关的新三元组。这些新三元组可以直接来自您的内在知识或通过给定三元组进行推理。 |  |  |'
- en: '| Here are some examples. |  |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 这里有一些示例。 |  |  |  |'
- en: '| In-Context Few-shot |  |  |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 上下文中的少量示例 |  |  |  |'
- en: '| Thought: {Thought} |  |  |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 想法：{Thought} |  |  |  |'
- en: '| Known Triples: {Explored Triples} |  |  |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 已知三元组：{Explored Triples} |  |  |  |'
- en: '| Generated Triples: |  |  |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 生成的三元组： |  |  |  |'
- en: 'Table 7: Prompts for different tasks used in GoG.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：GoG中用于不同任务的提示。
- en: Appendix C Settings for Baselines
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 基线设置
- en: Following ToG, the Freebase dump is acquired from [https://developers.google.com/freebase?hl=en](https://developers.google.com/freebase?hl=en),
    we deploy Freebase with Virtuoso. GoG, RoG, KB-BINDER and ChatKBQA are evaluated
    on the same Freebase database.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 根据ToG，从[https://developers.google.com/freebase?hl=en](https://developers.google.com/freebase?hl=en)获取Freebase转储，我们使用Virtuoso部署Freebase。GoG、RoG、KB-BINDER和ChatKBQA都在相同的Freebase数据库上进行评估。
- en: 'RoG. We use the checkpoints and the default settings provided by the official
    repository: n_beam=3 in generating rule, max_new_tokens=512 in inferring answers.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: RoG。我们使用官方库提供的检查点和默认设置：生成规则中的n_beam=3，推理答案中的max_new_tokens=512。
- en: 'ChatKBQA. We use the predicted S-expression provided by the official repository,
    and convert them into SPARQL queries. To compare ChatKBQA with other models fairly,
    we execute these SPARQL queries under the Freebase database mention before instead
    the DB files provided by them. Therefore, the performance of ChatKBQA reported
    in Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering")
    is slightly different from that in their original paper.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 'ChatKBQA。我们使用官方仓库提供的预测S-表达式，并将其转换为SPARQL查询。为了公平比较ChatKBQA与其他模型，我们在之前提到的Freebase数据库上执行这些SPARQL查询，而不是使用他们提供的DB文件。因此，表[1](#S5.T1
    "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")中报告的ChatKBQA的性能与他们原始论文中的结果略有不同。'
- en: 'KB-BINDER. We use the official repository and use KB-BINDER (6)-R (with majority
    vote and retrieve the most similar exemplars) to infer answers. However, the code-davinci-002
    used in their original paper is not available, so we use GPT-3.5 instead. Besides,
    to reduce runtime, we decreased the number of candidate MID combinations (despite
    that, it still takes about 4 hours to answer 200 questions). Therefore, the performance
    of KB-BINDER reported in Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph (GoG)
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") is slightly different from that in their original paper.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 'KB-BINDER。我们使用官方仓库中的KB-BINDER (6)-R（通过多数投票和检索最相似的示例）来推断答案。然而，他们原始论文中使用的code-davinci-002不可用，因此我们使用GPT-3.5代替。此外，为了减少运行时间，我们减少了候选MID组合的数量（尽管如此，回答200个问题仍需大约4小时）。因此，表[1](#S5.T1
    "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")中报告的KB-BINDER的性能与他们原始论文中的结果略有不同。'
- en: 'ToG. We use the official repository and their default settings for inferring
    answers: max_length=256, width=3, depth=3\. Since the official repository doesn’t
    provide the alias answers in the CWQ dataset, we evaluate ToG on the CWQ dataset
    without considering alias answers (the same strategy for all models). Therefore,
    the performance of ToG reported in Table [1](#S5.T1 "Table 1 ‣ 5 Generate-on-Graph
    (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge
    Graph Question Answering") is slightly different from that in their original paper.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'ToG。我们使用官方仓库及其默认设置来推断答案：max_length=256, width=3, depth=3。由于官方仓库未提供CWQ数据集中的别名答案，我们在不考虑别名答案的情况下评估ToG（所有模型都采用相同策略）。因此，表[1](#S5.T1
    "Table 1 ‣ 5 Generate-on-Graph (GoG) ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")中报告的ToG的性能与他们原始论文中的结果略有不同。'
- en: StructGPT. We use the official repository and running scripts to evaluate StructGPT
    on the WebQSP dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: StructGPT。我们使用官方仓库和运行脚本在WebQSP数据集上评估StructGPT。
- en: Appendix D Statistics of Topic Entities in IKGs
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D IKG中主题实体的统计数据
- en: 'The statistics of topic entities are shown in Table [8](#A4.T8 "Table 8 ‣ Appendix
    D Statistics of Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix
    B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering"),
    and we drop those samples which have isolated topic entities (topic entity without
    any neighbor node).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '主题实体的统计数据见表[8](#A4.T8 "Table 8 ‣ Appendix D Statistics of Topic Entities in
    IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt List ‣ Appendix A
    Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as both Agent
    and KG in Incomplete Knowledge Graph Question Answering")，我们排除了那些有孤立主题实体的样本（没有任何邻接节点的主题实体）。'
- en: '| Dataset |  | IKG-1 | IKG-2 | IKG-3 | IKG-4 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 |  | IKG-1 | IKG-2 | IKG-3 | IKG-4 |'
- en: '| CWQ | Median number of neighbor nodes | 27 | 26 | 27 | 27 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| CWQ | 邻接节点的中位数数量 | 27 | 26 | 27 | 27 |'
- en: '|  | Number of isolated topic entities | 19 | 42 | 59 | 53 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | 孤立主题实体数量 | 19 | 42 | 59 | 53 |'
- en: '| WebQSP | Median number of neighbor nodes | 428 | 427 | 427 | 426 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| WebQSP | 邻接节点的中位数数量 | 428 | 427 | 427 | 426 |'
- en: '|  | Number of isolated topic entities | 1 | 2 | 1 | 2 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 孤立主题实体数量 | 1 | 2 | 1 | 2 |'
- en: 'Table 8: Statistics of topic nodes in Incomplete KGs. Isolated topic entity
    represent topic entity without any neighbor node.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：不完整KG中主题节点的统计数据。孤立主题实体表示没有任何邻接节点的主题实体。
- en: Appendix E Result Analysis
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 结果分析
- en: 'Table [9](#A5.T9 "Table 9 ‣ Appendix E Result Analysis ‣ Appendix D Statistics
    of Topic Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt
    List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat
    LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering") illustrates
    the frequency of the Generate operation in different datasets alongside their
    corresponding Hits@1 scores. In the complete KGs setting, GoG still conducts the
    Generate operation when related relations are not correctly selected or when answers
    to sub-questions cannot be directly found via a one-hop relationship. In the incomplete
    KGs setting, the frequency of the Generate operation is higher, as GoG needs to
    generate new factual triples that are missing in the KGs. Hits@1 scores under
    both settings mean that most generation leading to correct results.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 表[9](#A5.T9 "表9 ‣ 附录E结果分析 ‣ 附录D IKG中的主题实体统计 ‣ 附录C基准设置 ‣ 附录B提示列表 ‣ 附录A语义解析方法细节
    ‣ 生成图：将LLM视为不完全知识图谱问答中的代理和KG")展示了不同数据集中生成操作的频率及其对应的Hits@1评分。在完整KG设置中，当相关关系未被正确选择或无法通过一跳关系直接找到子问题的答案时，GoG仍会进行生成操作。在不完整KG设置中，生成操作的频率较高，因为GoG需要生成在KG中缺失的新事实三元组。两个设置下的Hits@1评分表明，大多数生成操作能够产生正确结果。
- en: 'We considered four types of errors: (1) Knowledge Missing. (2) Hallucination.
    (3) Error before Generate. (4) False Negative. The distribution is shown in Figure
    [7](#A5.F7 "Figure 7 ‣ Appendix E Result Analysis ‣ Appendix D Statistics of Topic
    Entities in IKGs ‣ Appendix C Settings for Baselines ‣ Appendix B Prompt List
    ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph: Treat LLM as
    both Agent and KG in Incomplete Knowledge Graph Question Answering"). It can be
    found that there are indeed about 26% error are caused by hallucination. Besides,
    the Figure [6](#S6.F6 "Figure 6 ‣ 6.4 Performance with Different LLMs ‣ 6 Experiments
    ‣ Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph
    Question Answering") demonstrates that the performance of the model improves with
    the number of relevant triples introduced in the Generation step, which means
    introducing more related triples can alleviate hallucination problems to a certain
    extent.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了四种类型的错误：（1）知识缺失。（2）幻觉。（3）生成前错误。（4）假阴性。其分布如图[7](#A5.F7 "图7 ‣ 附录E结果分析 ‣ 附录D
    IKG中的主题实体统计 ‣ 附录C基准设置 ‣ 附录B提示列表 ‣ 附录A语义解析方法细节 ‣ 生成图：将LLM视为不完全知识图谱问答中的代理和KG")所示。可以发现，约26%的错误确实由幻觉引起。此外，图[6](#S6.F6
    "图6 ‣ 6.4 不同LLM的表现 ‣ 6 实验 ‣ 生成图：将LLM视为不完全知识图谱问答中的代理和KG")显示，模型的性能随着生成步骤中引入的相关三元组数量的增加而改善，这意味着引入更多相关三元组在一定程度上可以缓解幻觉问题。
- en: '![Refer to caption](img/c37f6ba4f25c9311ee0c79a2bae1a741.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c37f6ba4f25c9311ee0c79a2bae1a741.png)'
- en: 'Figure 7: The error proportions of GoG under IKG-1 of CWQ.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：GoG在IKG-1下的错误比例。
- en: '| Dataset | Models | CWQ | WebQSP |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模型 | CWQ | WebQSP |'
- en: '| CKG | GPT-3.5 | 17.7% (55.9) | 17.1% (66.6) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| CKG | GPT-3.5 | 17.7% (55.9) | 17.1% (66.6) |'
- en: '|  | GPT-4 | 4.6% (54.3) | 9.7% (65.9) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4 | 4.6% (54.3) | 9.7% (65.9) |'
- en: '| IKG-1 | GPT-3.5 | 23.7% (57.3) | 24.1% (63.4) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| IKG-1 | GPT-3.5 | 23.7% (57.3) | 24.1% (63.4) |'
- en: '|  | GPT-4 | 8.8% (57.9) | 18.4% (70.6) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT-4 | 8.8% (57.9) | 18.4% (70.6) |'
- en: 'Table 9: Times of Generate operation in different KG settings. Numbers in brackets
    represent corresponding Hits@1 score.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：不同KG设置下的生成操作次数。括号中的数字代表相应的Hits@1评分。
- en: Appendix F Case Study
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 案例研究
- en: 'In this section, we present a case analysis to evaluate the utility of GoG,
    as demonstrated in Table [F](#A6 "Appendix F Case Study ‣ Appendix E Result Analysis
    ‣ Appendix D Statistics of Topic Entities in IKGs ‣ Appendix C Settings for Baselines
    ‣ Appendix B Prompt List ‣ Appendix A Semantic Parsing Methods Details ‣ Generate-on-Graph:
    Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering").
    In this case, GoG will first search the neighbors of "Appalachian Mountains" and
    obtain ,
     and , then GoG can infer
    that Appalachian Mountains are also located in North America simply by knowing
    information about the other three mountains. However, ToG fails on this question
    once the crucial triple  is missing. In another word, for one-hop questions, GoG can use
    the neighbors information of the topic entities to predict the tail entities while
    other methods can not make full use of the neighbors information.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了一个案例分析来评估 GoG 的实用性，如表 [F](#A6 "附录 F 案例研究 ‣ 附录 E 结果分析 ‣ 附录 D IKG 中主题实体的统计
    ‣ 附录 C 基准设置 ‣ 附录 B 提示列表 ‣ 附录 A 语义解析方法细节 ‣ Generate-on-Graph: 将 LLM 视为不完整知识图谱问答中的代理和知识图谱")
    所示。在这个案例中，GoG 首先会搜索“阿巴拉契亚山脉”的邻居，获取 、 和 ，然后通过了解其他三座山的信息，GoG
    可以推断阿巴拉契亚山脉也位于北美。然而，一旦缺少关键三元组 ，ToG 在这个问题上会失败。换句话说，对于一跳问题，GoG 可以利用主题实体的邻居信息来预测尾实体，而其他方法则无法充分利用邻居信息。
    |'
- en: '| Question | Where do the appalachian mountains run? |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 阿巴拉契亚山脉在哪里延伸？ |'
- en: '| Dropped Crucial Triple | Appalachian Mountains, location.location.containedby,
    North America |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 丢失的关键三元组 | 阿巴拉契亚山脉, location.location.containedby, North America |'
- en: '| ToG | Reasoning Chains: None |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ToG | 推理链: 无 |'
- en: '| Prediction: I’m sorry, but I need more information to answer your question.
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 预测: 对不起，我需要更多的信息来回答你的问题。 |  |'
- en: '| GoG | Thought 1: I need to search the Appalachian Mountains to find out where
    they run. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| GoG | 思考 1: 我需要搜索阿巴拉契亚山脉以找出它们的分布情况。 |'
- en: '| Act 1: Search[Appalachian Mountains] |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 行动 1: 搜索[阿巴拉契亚山脉] |  |'
- en: '| Obs 1: Appalachian Mountains, location.contains, Brasstown Bald |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 观察 1: 阿巴拉契亚山脉, location.contains, Brasstown Bald |  |'
- en: '| Appalachian Mountains, location.contains, Craggy Dome |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 阿巴拉契亚山脉, location.contains, Craggy Dome |  |'
- en: '| Appalachian Mountains, location.geolocation, m.02_qbv6 |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 阿巴拉契亚山脉, location.geolocation, m.02_qbv6 |  |'
- en: '| Bald Eagle Mountain, mountain.mountain_range, Appalachian Mountains |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Bald Eagle Mountain, mountain.mountain_range, Appalachian Mountains |  |'
- en: '| Spaulding Mountain, mountain.mountain_range, Appalachian Mountains |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Spaulding Mountain, mountain.mountain_range, Appalachian Mountains |  |'
- en: '| Old Rag Mountain, mountain.mountain_range, Appalachian Mountains |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Old Rag Mountain, mountain.mountain_range, Appalachian Mountains |  |'
- en: '| Thought 2: The Appalachian Mountains contain various locations and are part
    of geolocations. To find out where they run, I need to generate some new triples
    based on these given triples and my inherent knowledge. |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 思考 2: 阿巴拉契亚山脉包含各种地点，是地理位置的一部分。为了找出它们的分布情况，我需要基于这些给定的三元组和我固有的知识生成一些新的三元组。
    |  |'
- en: '| Act 2:  Generate[where do the Appalachian Mountains run] |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 行动 2: 生成[阿巴拉契亚山脉在哪里延伸] |  |'
- en: '| Obs 2: Appalachian Mountains, geographical_feature.contains_location, Blue
    Ridge Mountains |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 观察 2: 阿巴拉契亚山脉, geographical_feature.contains_location, Blue Ridge Mountains
    |  |'
- en: '| Appalachian Mountains, geographical_feature.start_location, Alabama |  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 阿巴拉契亚山脉, geographical_feature.start_location, Alabama |  |'
- en: '| Appalachian Mountains, geographical_feature.start_location, North America
    |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 阿巴拉契亚山脉, geographical_feature.start_location, 北美 |  |'
- en: '| Thought 3: The Appalachian Mountains start from North America |  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 思考 3: 阿巴拉契亚山脉起始于北美 |  |'
- en: '| Act 3: Finish[Alabama &#124; North America ] |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 行动 3: 完成[Alabama &#124; North America ] |  |'
- en: '| Ground Truth | North America |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 真实情况 | 北美 |'
- en: 'Table 10: Comparison between ToG and GoG.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: ToG 和 GoG 的比较。'
