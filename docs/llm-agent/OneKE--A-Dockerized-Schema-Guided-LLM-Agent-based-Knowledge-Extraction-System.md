<!--yml
category: 未分类
date: 2025-01-11 11:43:03
-->

# OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System

> 来源：[https://arxiv.org/html/2412.20005/](https://arxiv.org/html/2412.20005/)

Yujie Luo [luo.yj@zju.edu.cn](mailto:luo.yj@zju.edu.cn) Zhejiang UniversityZJU-Ant Group Joint Research Center for Knowledge GraphsHangzhouChina ,  Xiangyuan Ru ,  Kangwei Liu Zhejiang UniversityZJU-Ant Group Joint Research Center for Knowledge GraphsHangzhouChina ,  Lin Yuan ,  Mengshu Sun ZJU-Ant Group Joint Research Center for Knowledge GraphsAnt GroupHangzhouChina ,  Ningyu Zhang Zhejiang UniversityZJU-Ant Group Joint Research Center for Knowledge GraphsHangzhouChina ,  Lei Liang ,  Zhiqiang Zhang ZJU-Ant Group Joint Research Center for Knowledge GraphsAnt GroupHangzhouChina ,  Jun Zhou ZJU-Ant Group Joint Research Center for Knowledge GraphsAnt GroupHangzhouChina ,  Lanning Wei ,  Da Zheng ZJU-Ant Group Joint Research Center for Knowledge GraphsAnt GroupHangzhouChina ,  Haofen Wang Tongji UniversityShanghaiChina  and  Huajun Chen Zhejiang UniversityZJU-Ant Group Joint Research Center for Knowledge GraphsHangzhouChina(2018)

###### Abstract.

We introduce OneKE, a dockerized schema-guided knowledge extraction system, which can extract knowledge from the Web and raw PDF Books, and support various domains (science, news, etc.). Specifically, we design OneKE with multiple agents and a configure knowledge base. Different agents perform their respective roles, enabling support for various extraction scenarios. The configure knowledge base facilitates schema configuration, error case debugging and correction, further improving the performance. Empirical evaluations on benchmark datasets demonstrate OneKE’s efficacy, while case studies further elucidate its adaptability to diverse tasks across multiple domains, highlighting its potential for broad applications. We have open-sourced the Code¹¹1[https://github.com/zjunlp/OneKE](https://github.com/zjunlp/OneKE) and released a Video²²2[http://oneke.openkg.cn/demo.mp4](http://oneke.openkg.cn/demo.mp4).

Information Extraction; Natural Language Processing; Large Language Models^†^†copyright: acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make sure to enter the correct conference title from your rights confirmation emai; June 03–05, 2018; Woodstock, NY^†^†isbn: 978-1-4503-XXXX-X/18/06

## 1\. Introduction

Knowledge extraction–obtaining knowledge from data, is a critical component for a wide range of practical systems such as Knowledge Graph (KG) construction (Chen et al., [2022](https://arxiv.org/html/2412.20005v1#bib.bib2)), Retrieval Augmentation (RAG) (Gao et al., [2023](https://arxiv.org/html/2412.20005v1#bib.bib4)), and domain-specific applications like scientific discovery (Dagdelen et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib3)) and intelligence analysis (Sun et al., [2023](https://arxiv.org/html/2412.20005v1#bib.bib8)). The last decades have witnessed the development of various knowledge extraction systems (Li et al., [2023](https://arxiv.org/html/2412.20005v1#bib.bib6); Wei et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib10); Xu et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib11)). In particular, with the emergence of Large Language Models (LLMs), new works such as InstructUIE (Shi et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib7)), iText2KG (Lairgi et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib5)) and AgentRE (Wang et al., [2023](https://arxiv.org/html/2412.20005v1#bib.bib9)) have been continuously emerged. However, previous approaches still struggle to effectively extract information from raw data following complex schemas and face challenges in debugging and correcting errors when they occur.

![Refer to caption](img/f43800975c608b0c57df4bc2611eb8fb.png)

Figure 1\. The overview of the OneKE system, supporting various domains (science, news, etc.) and data (Web HTML, PDF, etc.).

Note that previous efforts have primarily focused on the capabilities of individual models while neglecting the design of a comprehensive system to address the knowledge extraction task. To this end, we introduce OneKE, a dockerized schema-guided knowledge extraction system. We adopt a multi-agent design with a configure knowledge base to provide knowledge extraction support for various scenarios and error debugging, aiming to meet the practical needs of users as much as possible. Following (Shi et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib7)), we design three agents: Schema Agent for schema analysis with various data types, Extraction Agent for extracting knowledge with various LLMs, and Reflection Agent to debug and handle erroneous cases. Based on this design, OneKE can efficiently process source texts of varying lengths and formats, such as HTML and PDF, and demonstrates a robust capability to adapt to diverse task configurations, yielding a comprehensive range of output schemas tailored to specific requirements.

We evaluate OneKE using two benchmark datasets for Named Entity Recognition (NER) and Relation Extraction (RE), demonstrating the effectiveness of our framework. Furthermore, to explore the versatility of OneKE in practical applications, we conduct case analyses on specific extraction tasks. These scenarios encompass extracting structured information from the Web news articles and raw PDF book chapters, highlighting OneKE’s capability to manage diverse data formats and varying task contexts effectively. This flexible framework, which operates without the necessity of fine-tuning, is adept at swift adaptation to forthcoming LLMs, thereby amplifying their capabilities and elevating their overall efficacy.

## 2\. Design and Implementation

OneKE is thoughtfully designed to address the complexities and challenges inherent in knowledge extraction. As shown in Figure [1](https://arxiv.org/html/2412.20005v1#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System"), the framework is guided by several key considerations that enhance its functionality and adaptability in real-world scenario:

(1) Adaptability to real-world data. Real-world information extraction tasks often handle raw data, like HTML, PDF, etc. Based on this, the OneKE framework supports a variety of data types rather than pure text. We also reserve a user-defined interface to support new data types in the future.

(2) Generalization for complex schemas. Practical knowledge extraction scenarios should handle diverse and complex schemas, or even no schema. Thus, we design the OneKE-specific Schema Agent to support both pre-defined schemas and self-schema deduction using LLMs. OneKE also supports various LLMs, including LLaMA, Qwen and ChatGLM, as well as proprietary models like GPT-4, enabling effective knowledge extraction.

(3) Debugging and fixing errors. Most previous works require retraining the model when encountering error cases. In contrast, we integrate Case Repository into OneKE to equip the model with reflective and error-correcting capabilities, enabling its continuous improvement in knowledge extraction tasks.

### 2.1\. Schema Agent

To support various task settings and data types, we develop the Schema Agent based on LLMs to generate the corresponding output schema for each task. The primary goal is to preprocess the data, standardize its format and schema, and prepare it for the subsequent information extraction step. To support various real-world data, we utilize the document_loaders module provided by the Langchain, to preprocess the data and perform chunking on long texts. Users can also define new data types, and add custom preprocessing methods. Next, if the user has defined a schema, given a task description and raw data input, the Schema Agent will select the appropriate pre-defined schema from the schema repository in the configure knowledge base. If the user does not provide a schema, the model will generate a unified output schema based on the user’s instructions, such as “*Extract characters and background setting*”. Users can customize schemas using simple text by updating the schema repository in the Configure Knowledge Base.

### 2.2\. Extraction Agent

Upon receiving the unified output schema from the Schema Agent, we design the Extraction Agent to utilize LLMs for extracting knowledge, thereby generating the preliminary extraction results. Specifically, this module supports a variety of models, including locally deployed open-source models such as LLaMA, Qwen, and ChatGLM, as well as API services like OpenAI and DeepSeek. To enhance performance, the Extraction Agent learns from similar cases and applies this knowledge to the extraction process. Relevant cases are retrieved from the Case Repository using semantic similarity combined with string matching (via the all-MiniLM-L6-v2 model and the FuzzyWuzzy package, we set Top two as default). These cases are then incorporated as few-shot examples into the original context to form the prompt, after which the LLM is called to obtain the extraction results. After the above steps, we obtain the preliminary extraction results; however, errors often occur. To address this, we design a reflection agent to debug and fix these errors. We use self-consistency to filter out the cases where the model is uncertain and pass these cases to the Reflection Agent.

### 2.3\. Reflection Agent

To enable debugging and error correction, allowing OneKE to learn from past mistakes, we follow (Shi et al., [2024](https://arxiv.org/html/2412.20005v1#bib.bib7)) to design the Reflection Agent to facilitate reflection and optimization. By leveraging prior knowledge, the agent refines and improves the initial output from the previous module, ultimately producing the final extraction result. Concretely, the agent leverages external Case Repository, specifically relevant bad cases, to facilitate reflection and correction. In a manner similar to the retrieval approach discussed in Section [2.2](https://arxiv.org/html/2412.20005v1#S2.SS2 "2.2\. Extraction Agent ‣ 2\. Design and Implementation ‣ OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System"), the Reflection Agent fetches bad cases that are most relevant to the current task and extraction text. These relevant bad cases, along with their associated reflective analyses, are subsequently incorporated into the LLM. In this way, the agent can effectively learn from past mistakes, enabling its error correction capabilities to generate accurate answers.

### 2.4\. Configure Knowledge Base

The Configure Knowledge Base provides essential information for the three agents, including manually defined schemas for various tasks, and historical extraction cases, enhancing the performance and error correction capabilities:

Schema Repository. In the OneKE system, the Schema Repository provides the pre-defined schema for the Schema Agent, supporting the subsequent extraction process. Specifically, the Schema Repository includes pre-defined output schemas for NER, RE, and EE tasks, along with templates for various data scenarios such as scientific academic papers and news reports. The schemas in the Schema Repository are structured as Pydantic objects, enabling seamless serialization into JSON format for the Extraction Agent. Moreover, this structure allows users to customize new schemas within the repository, thus enhancing adaptability and extensibility.

Case Repository. To enable debugging and error correction in OneKE, we design the Case Repository, which primarily stores traces of past knowledge extraction cases. This repository supports the Extraction Agent in performing extractions and assists the Reflection Agent in reflecting on and correcting errors. Specifically, knowledge extraction cases stored in Case Repository can be divided into two categories: Correct Cases and Bad Cases. Correct Cases provide the Extraction Agent with reasoning steps of successful extraction, while Bad Cases offer the Reflection Agent warnings about avoidable mistakes. The Case Repository will be automatically updated once a knowledge extraction task is completed. Concretely, this module first generates reasoning steps derived from the correct answer, storing both the correct answer and its reasoning steps in the Correct Case Repository to enhance task understanding. Additionally, the agent compares its answer with the correct one and reflects on its original response to identify potential issues. It then stores the original answer along with the corresponding reflections in the Bad Case Repository for future reference.

![Refer to caption](img/f79f1a93ed5dc54f05f28336860b5141.png)![Refer to caption](img/ca60d0727918915687304a50e781d7e6.png)

Figure 2\. Performance of different components in OneKE.

## 3\. Evaluation

![Refer to caption](img/644e37c440ba548294fd50c654afd55d.png)

Figure 3\. Using OneKE on Web News Extraction and Book Knowledge Extraction.

Experimental Settings. We evaluate OneKE on the CrossNER and NYT-11-HRL datasets. CrossNER is a cross-domain NER dataset, while NYT-11-HRL focuses on the RE task within the news domain. The performance of OneKE is evaluated on both the LLaMA-3-8B-Instruct and GPT-4-turbo models.

Main Results. As depicted in Figure [2](https://arxiv.org/html/2412.20005v1#S2.F2 "Figure 2 ‣ 2.4\. Configure Knowledge Base ‣ 2\. Design and Implementation ‣ OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System"), the various methods employed in OneKE confer performance enhancements across both NER and RE. Notably, the Case Retrieval method of the Extraction Agent achieves the most significant improvements. Through analysis, we observe that the agent effectively applies the reasoning paths in the provided samples, thereby facilitating accurate extraction. Additionally, by comparing the two different tasks, we observe that the aforementioned Case Retrieval method is more effective for the more challenging RE task, as the intermediate reasoning steps are essential in such complex scenarios. The Case Reflection primarily emphasizes the model’s ability to recognize known errors and its capacity for transfer learning regarding those errors, leading to similar improvements across both tasks. We provide case studies of specific application scenarios in the following Section.

## 4\. Application

In practical applications, the OneKE framework supports diverse data formats (HTML, PDF, Word), accommodating both short and long contexts for seamless integration into various downstream applications. We provide case analyses in the following two representative extraction scenarios.

Web News Extraction. In the news domain, OneKE enhances the knowledge extraction of Web news content, thereby facilitating downstream tasks such as effective sentiment monitoring, proactive risk management, as well as a variety of additional applications. As illustrated in Figure [3](https://arxiv.org/html/2412.20005v1#S3.F3 "Figure 3 ‣ 3\. Evaluation ‣ OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System"), the extraction task starts with Extracting key information from news articles on a randomly selected raw HTML page from the web, aiming to identify the overall nature of the news and obtain structured key insights. After parsing the HTML-formatted text, the Schema Agent first identifies its domain and genre as a Politics News Report, offering crucial guidance. Utilizing this metadata, the Schema Agent generates a structured Output Schema in code format that effectively captures the key information of the news. Once the Output Schema has been serialized into a JSON format description, the Extraction Agent and the Reflection Agent collaborate to undertake subsequent extraction and reflective optimization tasks. This cooperative effort of the agents culminates in a JSON output that captures the key information and structure of the news report.

Book Knowledge Extraction. Another application of OneKE lies in extracting structured knowledge from extensive corpora, including books, documents, or manuals. Specifically, we use the first chapter of the Harry Potter series (with a total length of 17 pages in PDF format) as the target text. The extraction focuses on the main characters and the background setting within this chapter. As shown in Figure [3](https://arxiv.org/html/2412.20005v1#S3.F3 "Figure 3 ‣ 3\. Evaluation ‣ OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System"), the generated Output Schema accurately identified the two target extraction objects: “main_characters” and “background_setting”. Subsequently, with the collaboration of the Extraction Agent and the Reflection Agent, OneKE successfully extracted relevant information.

## 5\. Conclusion and Future Work

In this paper, we introduce OneKE, a Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System. OneKE is designed for flexible application across a spectrum of extraction tasks in real-world scenarios. It can handle source texts of varying lengths and formats (such as HTML and PDF) while demonstrating the capability to adapt to diverse task configurations, generating a broad range of output schemas tailored to specific requirements. Moreover, the integration of a self-reflection mechanism enables iterative improvement informed by external feedback, thereby enhancing both accuracy and adaptability.

Long-term Maintenance. We will maintain OneKE over the long term, adding new features and fixing bugs. OneKE is built on a modular architecture that promotes extensibility, and we plan to expand the Configure Knowledge Base by integrating additional domain-specific knowledge from fields such as science, thereby broadening its applicability across domains. To advance document data extraction and comprehension, we plan to develop methodologies for the integration and analysis of diverse chart types and content. We hope OneKE can serve as a helpful tool for researchers and engineers engaging in knowledge extraction with LLMs.

## References

*   (1)
*   Chen et al. (2022) Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In *WWW*.
*   Dagdelen et al. (2024) John Dagdelen, Alexander Dunn, Sanghoon Lee, Nicholas Walker, Andrew S Rosen, Gerbrand Ceder, Kristin A Persson, and Anubhav Jain. 2024. Structured information extraction from scientific text with large language models. *Nature Communications* 15, 1 (2024), 1418.
*   Gao et al. (2023) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. *arXiv preprint arXiv:2312.10997* (2023).
*   Lairgi et al. (2024) Yassir Lairgi, Ludovic Moncla, Rémy Cazabet, Khalid Benabdeslem, and Pierre Cléau. 2024. iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models. *CoRR* abs/2409.03284 (2024). arXiv:2409.03284
*   Li et al. (2023) Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang. 2023. Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. *arXiv preprint arXiv:2304.11633* (2023).
*   Shi et al. (2024) Yuchen Shi, Guochao Jiang, Tian Qiu, and Deqing Yang. 2024. AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction. In *Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 2024*. ACM.
*   Sun et al. (2023) Nan Sun, Ming Ding, Jiaojiao Jiang, Weikang Xu, Xiaoxing Mo, Yonghang Tai, and Jun Zhang. 2023. Cyber threat intelligence mining for proactive cybersecurity defense: a survey and new perspectives. *IEEE Communications Surveys & Tutorials* 25, 3 (2023), 1748–1774.
*   Wang et al. (2023) Xiao Wang, Weikang Zhou, Can Zu, et al. 2023. InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction. *CoRR* abs/2304.08085 (2023). arXiv:2304.08085
*   Wei et al. (2024) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, et al. 2024. Chatie: Zero-shot information extraction via chatting with chatgpt. *arXiv preprint arXiv:2302.10205* (2024).
*   Xu et al. (2024) Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, and Enhong Chen. 2024. Large language models for generative information extraction: A survey. *Frontiers of Computer Science* 18, 6 (2024), 186357.