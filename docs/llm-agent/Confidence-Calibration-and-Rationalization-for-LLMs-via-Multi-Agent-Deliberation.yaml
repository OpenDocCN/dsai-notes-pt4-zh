- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:48:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:48:24'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过多代理审议对LLMs进行自信度校准和合理化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.09127](https://ar5iv.labs.arxiv.org/html/2404.09127)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.09127](https://ar5iv.labs.arxiv.org/html/2404.09127)
- en: Ruixin Yang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruixin Yang
- en: University of British Columbia
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 不列颠哥伦比亚大学
- en: '&Dheeraj Rajagopal'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '&Dheeraj Rajagopal'
- en: Carnegie Mellon University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 卡内基梅隆大学
- en: '&Shirley Anugrah Hayati'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Shirley Anugrah Hayati'
- en: University of Minnesota
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼苏达大学
- en: '&Bin Hu'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Bin Hu'
- en: University of Minnesota
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼苏达大学
- en: '&Dongyeop Kang'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&东叶焕'
- en: University of Minnesota
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 明尼苏达大学
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Uncertainty estimation is a significant issue for current large language models
    (LLMs) that are generally poorly calibrated and over-confident, especially with
    reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions
    and confidences not only stem from intrinsic beliefs but can also be adjusted
    through daily observations, existing calibration methods for LLMs focus on estimating
    or eliciting individual confidence without taking full advantage of the ”Collective
    Wisdom”: the interaction among multiple LLMs that can collectively improve both
    accuracy and calibration. In this work, we propose Collaborative Calibration,
    a post-hoc training-free calibration strategy that leverages the collaborative
    and expressive capabilities of multiple tool-augmented LLM agents in a simulated
    group deliberation process. We demonstrate the effectiveness of Collaborative
    Calibration on generative QA tasks across various domains, showing its potential
    in harnessing the rationalization of collectively calibrated confidence assessments
    and improving the reliability of model predictions ¹¹1Our implementation is publicly
    available at [https://github.com/minnesotanlp/collaborative-calibration](https://github.com/minnesotanlp/collaborative-calibration)..'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性估计是当前大型语言模型（LLMs）的一个重要问题，这些模型通常校准不良且过于自信，尤其是在基于人类反馈的强化学习（RLHF）中。与人类不同，人类的决策和自信不仅来源于内在信念，还可以通过日常观察进行调整，而现有的LLMs校准方法侧重于估计或引导个体自信度，未能充分利用“集体智慧”：即多个LLMs之间的互动，这种互动可以共同提高准确性和校准性。在这项工作中，我们提出了协作校准，这是一种后期训练无关的校准策略，利用多个工具增强的LLM代理在模拟小组审议过程中的协作和表达能力。我们在生成式QA任务的各种领域展示了协作校准的有效性，表明它在利用集体校准的自信评估和提高模型预测可靠性方面的潜力¹¹1我们的实现公开可用，网址是[https://github.com/minnesotanlp/collaborative-calibration](https://github.com/minnesotanlp/collaborative-calibration)。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: While contemporary large language models (LLMs) have achieved remarkable performance
    in a variety of tasks ranging from question answering to complex reasoning (Brown
    et al., [2020](#bib.bib3); Bubeck et al., [2023](#bib.bib4)), it remains a significant
    bottleneck for them to produce well-calibrated confidence estimates for their
    predictions, meaning that their individual confidence is not a reliable indicator
    of accuracy. Models still often generate hallucinations (Bubeck et al., [2023](#bib.bib4))
    or wildly wrong predictions, unknowingly and over-confidently, which is found
    to be more evident for models fine-tuned with RLHF (Kadavath et al., [2022](#bib.bib20);
    Tian et al., [2023](#bib.bib43)). On the other hand, models can exhibit inconsistencies
    and lack of confidence, by blindly altering decisions and prioritizing incorrect
    user opinions (Wei et al., [2023](#bib.bib47)). Such miscalibration is claimed
    to be even more significant for larger and more capable language models (Kong
    et al., [2020](#bib.bib22); Xiong et al., [2023](#bib.bib49)), suggesting the
    ineffectiveness of model scaling (Kaplan et al., [2020](#bib.bib21)) for mitigating
    this problem, which poses a great challenge in fostering trust in Human-AI collaboration
    and in developing reliable real-life applications, especially in high-risk domains.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当代的大型语言模型（LLMs）在从问答到复杂推理等各种任务中取得了显著的表现（Brown et al., [2020](#bib.bib3); Bubeck
    et al., [2023](#bib.bib4)），但它们在产生良好校准的置信度估计方面仍然是一个重大瓶颈，这意味着它们的个体置信度并不是准确性的可靠指标。模型仍然经常生成虚假信息（Bubeck
    et al., [2023](#bib.bib4)）或极其错误的预测，往往是无意识且过度自信的，这在使用 RLHF 进行微调的模型中更为明显（Kadavath
    et al., [2022](#bib.bib20); Tian et al., [2023](#bib.bib43)）。另一方面，模型可能表现出不一致性和缺乏信心，通过盲目改变决策和优先考虑不正确的用户意见（Wei
    et al., [2023](#bib.bib47)）。这种误校准在更大且更强大的语言模型中被认为更为显著（Kong et al., [2020](#bib.bib22);
    Xiong et al., [2023](#bib.bib49)），这表明模型扩展（Kaplan et al., [2020](#bib.bib21)）对于缓解这一问题的效果有限，这对促进人机协作中的信任以及开发可靠的现实应用，尤其是在高风险领域，构成了巨大挑战。
- en: Although confidence estimation and calibration have been extensively studied
    in the broader machine learning literature (Gal & Ghahramani, [2016](#bib.bib13);
    Guo et al., [2017](#bib.bib14)), previous work in the context of NLP mostly required
    extensive fine-tuning (Kong et al., [2020](#bib.bib22)) or temperature-based scaling
    (Guo et al., [2017](#bib.bib14); Jiang et al., [2021](#bib.bib18)), which can
    be expensive for LLMs. Recent studies on confidence estimation for black-box LLMs
    adopted either consistency-based approaches with repeated sampling (Wang et al.,
    [2023b](#bib.bib45)) or verbalization-based elicitation through direct prompting
    (Lin et al., [2022](#bib.bib24); Tian et al., [2023](#bib.bib43)), or combined
    (Xiong et al., [2023](#bib.bib49)). However, results are mixed on whether the
    elicited confidences are better-calibrated than sample-based estimates or the
    model’s original token probabilities, and no attempt has been made to further
    improve the interpretability and reliability of such measurement by incorporating
    the rationalization and collective refinement of individual confidence, leveraging
    the expressive, self-critical, tool-use and collaborative capabilities of generative
    LLM agents (Park et al., [2023](#bib.bib33); Madaan et al., [2023](#bib.bib29);
    Schick et al., [2023](#bib.bib36)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管置信度估计和校准在更广泛的机器学习文献中已被广泛研究（Gal & Ghahramani, [2016](#bib.bib13); Guo et al.,
    [2017](#bib.bib14)），但在 NLP 领域的以往工作大多需要大量的微调（Kong et al., [2020](#bib.bib22)）或基于温度的缩放（Guo
    et al., [2017](#bib.bib14); Jiang et al., [2021](#bib.bib18)），这对于 LLM 来说可能是昂贵的。近期关于黑箱
    LLM 置信度估计的研究采用了一致性基础的方法与重复采样（Wang et al., [2023b](#bib.bib45)）或通过直接提示的语言化方法（Lin
    et al., [2022](#bib.bib24); Tian et al., [2023](#bib.bib43)），或者两者结合（Xiong et al.,
    [2023](#bib.bib49)）。然而，关于这些引导的置信度是否比基于样本的估计或模型原始的标记概率更为校准，结果存在不一致的情况，且尚未尝试通过整合个体置信度的理性化和集体优化来进一步提高这种测量的可解释性和可靠性，利用生成性
    LLM 代理的表达、自我批评、工具使用和协作能力（Park et al., [2023](#bib.bib33); Madaan et al., [2023](#bib.bib29);
    Schick et al., [2023](#bib.bib36)）。
- en: Inspired by the simple observation that humans can effectively adjust and balance
    their confidence assessments by weighing agreeing or dissenting opinions from
    others through group interaction (Silver et al., [2021](#bib.bib41)), we introduce
    Collaborative Calibration, a training-free method for confidence estimation, calibration,
    and rationalization for LLMs, by simulating a two-stage group deliberation process
    with multiple LLM agents (outlined in [Figure 1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation")).
    We demonstrate the effectiveness of Collaborative Calibration on free-form QA
    tasks, showing that it can achieve comparable or superior performance on multiple
    calibration metrics compared to previous methods, without hurting accuracy or
    generation quality, or at the expense of extensive fine-tuning or parameter-fitting.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 受启发于一个简单的观察：人们可以通过群体互动来有效调整和平衡他们的置信度评估，权衡其他人的赞同或异议（Silver et al., [2021](#bib.bib41)），我们引入了协作校准，这是一种无需训练的置信度估计、校准和合理化的方法，通过模拟一个由多个LLM代理组成的两阶段小组讨论过程（见[图1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 通过多代理讨论对LLMs进行置信度校准和合理化")）。我们展示了协作校准在自由形式QA任务中的有效性，表明它能够在多个校准指标上实现与之前的方法相当或更优的性能，而不会损害准确性或生成质量，也无需进行大量的微调或参数调整。
- en: '![Refer to caption](img/478ca927544c61ac81799bdd5133e08a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/478ca927544c61ac81799bdd5133e08a.png)'
- en: 'Figure 1: High-level overview of the Collaborative Calibration pipeline.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：协作校准管道的高级概述。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Confidence Estimation and Calibration for LLMs: With the rapid advancement
    of LLMs, there has been an increasing focus on estimating and calibrating their
    prediction confidence, primarily for classification settings (Jiang et al., [2021](#bib.bib18);
    Si et al., [2022](#bib.bib38); [2023a](#bib.bib39); Portillo Wightman et al.,
    [2023](#bib.bib34)). For the more challenging yet pertinent tasks of free-form
    generation with varying answer lengths, Kuhn et al. ([2023](#bib.bib23)) introduce
    an unsupervised entropy-based metric that captures the uncertainty over meanings
    rather than sequences, and Liu et al. ([2023b](#bib.bib27)) present a lightweight
    training method that learns a bias term added to the output logits for better-calibrated
    confidence estimates. However, these methods require access to internal model
    structures or the output logits, which are not available for proprietary LLMs.
    Recent research endeavors have focused on either estimating confidence based on
    the probability distribution of the most consistent answer from multiple samples
    (Wang et al., [2023b](#bib.bib45); Chen et al., [2023d](#bib.bib8); Lin et al.,
    [2023](#bib.bib25)) or directly eliciting verbalized confidence (Lin et al., [2022](#bib.bib24);
    Tian et al., [2023](#bib.bib43)), and found that prompting or fine-tuning with
    verbalized expression of uncertainty can lead to increased accuracy or better
    calibration (Mielke et al., [2022](#bib.bib30); Zhou et al., [2023](#bib.bib51)).
    Xiong et al. ([2023](#bib.bib49)) present a comprehensive analysis of logit-based,
    verbalized, and consistency-based methods and suggest that a hybrid approach incorporating
    verbalized confidence into a consistency-based ensemble is more effective for
    confidence calibration.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大型语言模型（LLMs）的置信度估计与校准：随着LLMs的快速发展，越来越多的关注被放在了估计和校准它们的预测置信度上，主要针对分类设置（Jiang
    et al., [2021](#bib.bib18); Si et al., [2022](#bib.bib38); [2023a](#bib.bib39);
    Portillo Wightman et al., [2023](#bib.bib34)）。对于更具挑战性且相关的自由形式生成任务，其中答案长度各异，Kuhn
    et al. ([2023](#bib.bib23))提出了一种基于熵的无监督度量，它捕捉的是意义上的不确定性而非序列上的不确定性，而Liu et al.
    ([2023b](#bib.bib27))则提出了一种轻量级的训练方法，通过学习添加到输出logits中的偏差项来获得更好的校准置信度估计。然而，这些方法需要访问内部模型结构或输出logits，而这些在专有LLMs中是不可用的。最近的研究努力集中在基于从多个样本中获得的最一致答案的概率分布来估计置信度（Wang
    et al., [2023b](#bib.bib45); Chen et al., [2023d](#bib.bib8); Lin et al., [2023](#bib.bib25)）或直接引出语言化置信度（Lin
    et al., [2022](#bib.bib24); Tian et al., [2023](#bib.bib43)），并发现通过提示或微调带有不确定性语言表达的模型可以提高准确性或更好地校准（Mielke
    et al., [2022](#bib.bib30); Zhou et al., [2023](#bib.bib51)）。Xiong et al. ([2023](#bib.bib49))对基于logit的、语言化的以及基于一致性的方法进行了全面分析，建议将语言化置信度融入一致性基础的集成方法中，对于置信度校准更为有效。
- en: 'LLM Agent Ensemble: Augmented with external memory system and tool-use abilities,
    LLM-powered language agents (Park et al., [2023](#bib.bib33)) display great potential
    to serve as self-consistent human proxies that can plan by self-reflection and
    refinement (Shinn et al., [2023](#bib.bib37); Madaan et al., [2023](#bib.bib29);
    Sun et al., [2023](#bib.bib42)). Harnessing the diversity and collaboration strengths
    of multiple LLM agents, the ensemble and interaction can effectively enhance their
    capabilities in complex reasoning (Xiong et al., [2023](#bib.bib49); Chen et al.,
    [2023b](#bib.bib6); Wang et al., [2023a](#bib.bib44)), instruction following (Jiang
    et al., [2023b](#bib.bib17)), and value alignment (Liu et al., [2023a](#bib.bib26)).
    Du et al. ([2023](#bib.bib12)) introduce a multi-agent debate framework where
    multiple LLM instances propose and debate their reasoning processes to reach a
    consensus, which improves the factuality and performance on arithmetic and strategic
    reasoning tasks. Similarly, ReConcile (Chen et al., [2023a](#bib.bib5)) uses different
    backbone LLMs for debating and a confidence-weighted voting mechanism to obtain
    the final answer. Our approach differs in that we incorporate self-consistency
    estimates, and naturally calibrate the verbalized confidence through group deliberation
    with peer feedback and intermediate rationales for confidence adjustment, rather
    than manual rescaling in a post-hoc manner. We also incorporate both open and
    API-based models for the flexibility of supporting both logit-based and black-box
    confidence estimation. Liu et al. ([2023c](#bib.bib28)) propose an inference-time
    agent selection framework based on aggregated peer ratings over multiple rounds
    of interactions for a given query. In contrast, our agent selection process requires
    only the first-round individual confidence estimates at the task level.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理集成：通过外部记忆系统和工具使用能力增强的 LLM 驱动语言代理（Park et al., [2023](#bib.bib33)）显示出作为自洽人类代理的巨大潜力，这些代理可以通过自我反思和改进进行规划（Shinn
    et al., [2023](#bib.bib37)；Madaan et al., [2023](#bib.bib29)；Sun et al., [2023](#bib.bib42)）。利用多个
    LLM 代理的多样性和协作优势，集成和互动可以有效提升它们在复杂推理（Xiong et al., [2023](#bib.bib49)；Chen et al.,
    [2023b](#bib.bib6)；Wang et al., [2023a](#bib.bib44)）、指令跟随（Jiang et al., [2023b](#bib.bib17)）和价值对齐（Liu
    et al., [2023a](#bib.bib26)）中的能力。Du et al.（[2023](#bib.bib12)）引入了一个多代理辩论框架，其中多个
    LLM 实例提出和辩论其推理过程以达成共识，从而提高了在算术和战略推理任务中的事实性和性能。同样，ReConcile（Chen et al., [2023a](#bib.bib5)）使用不同的主干
    LLM 进行辩论，并采用信心加权投票机制以获得最终答案。我们的方法不同在于我们结合了自洽估计，并通过同行反馈和中间理由在组审议过程中自然校准语言化信心，而不是事后手动重新缩放。我们还结合了开放和基于
    API 的模型，以支持基于 logit 和黑箱的信心估计的灵活性。Liu et al.（[2023c](#bib.bib28)）提出了一种基于多轮交互中聚合同行评分的推理时代理选择框架。相比之下，我们的代理选择过程仅需要任务级别的第一轮个体信心估计。
- en: '3 Collaborative Calibration: Calibrating Confidence via Multi-Agent Deliberation'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 协作校准：通过多代理商审议校准信心
- en: '![Refer to caption](img/f00e0a1f7a503a1df789f0869bc8952d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f00e0a1f7a503a1df789f0869bc8952d.png)'
- en: (a) Stage 1 selects the composition of expert agents with suitable prompting
    techniques or tool-use expertise based on calibration performance on the validation
    set. For a test query, the selected agents then generate their initial answers,
    which are clustered into semantically unique stances, with an average confidence
    and agent count per stance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 阶段1根据在验证集上的校准表现选择合适的专家代理组合，采用适当的提示技术或工具使用专长。对于测试查询，所选代理生成初步回答，并将其聚类为语义上独特的立场，每个立场的平均信心和代理数量。
- en: '![Refer to caption](img/ec7c3551e7a14f9fb83743a5584f8916.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec7c3551e7a14f9fb83743a5584f8916.png)'
- en: (b) In Stage 2, each general agent provides arguments for its assigned stance,
    gives feedback on others’ arguments, revises its answer, and adjusts the confidence
    with some rationales. We take a majority vote to get the final, aggregated confidence
    estimate.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在阶段2中，每个通用代理为其分配的立场提供论据，对其他人的论据进行反馈，修正其回答，并根据一些理由调整信心。我们通过多数投票获得最终的聚合信心估计。
- en: 'Figure 2: Detailed illustration of the two-stage framework with a specific
    test example from the SciQ dataset.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：两阶段框架的详细示意图，包含来自 SciQ 数据集的具体测试示例。
- en: 3.1 Agent Ensemble and Stance Generation
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 代理集成与立场生成
- en: 'To strive for a balance between task accuracy and calibration performance,
    it is necessary to maintain a certain level of diversity among the agents’ initial
    answers and reasoning paths while allocating the slots wisely so that the most
    suitable agents for the task can ideally become the majority, which serves as
    the basis for the ensemble. Inspired by Si et al. ([2023b](#bib.bib40)), we initialize
    four types of ”expert agents”. Each agent has a different prompting strategy or
    tool-use expertise: Chain-of-Thought (Wei et al., [2022](#bib.bib46)) for multi-hop
    reasoning tasks, Program-of-Thoughts (Chen et al., [2023c](#bib.bib7)) for code
    and arithmetic reasoning tasks, Search-Augmented Self-Ask (Press et al., [2023](#bib.bib35))
    for factoid reasoning tasks, and GenRead prompting (Yu et al., [2023](#bib.bib50))
    for knowledge-intensive reasoning tasks. This initialization is flexible in that
    any new skill or prompting strategy can be easily added with modularity. As multiple
    skills might be relevant for an input dataset, we determine the importance ranking
    of each skill and accordingly allocate expert agents, based on a simple uncertainty-aware
    calibration score, detailed in Appendix [A.1](#A1.SS1 "A.1 Details on agent selection
    ‣ Appendix A Appendix ‣ Confidence Calibration and Rationalization for LLMs via
    Multi-Agent Deliberation"). We use Mistral-7B (Jiang et al., [2023a](#bib.bib16)),
    GPT-3.5-turbo (OpenAI, [2022](#bib.bib32)), and Cohere-Commend (Cohere, [2023](#bib.bib10))
    as backbones for the expert agents. For an incoming test question (e.g., ”Which
    element was discovered in 1898 and named after Greek ’new’?”), each expert agent
    performs self-deliberation by executing the corresponding prompting strategy and
    voting independently for an answer with a confidence estimate. The numerical confidence
    estimate can be based on the output sequence perplexity (PP) for open models ²²2For
    open models with access to next-token probabilities, the raw confidence for the
    output sequence $W=(w_{1},...,w_{N})$. Note that this estimate is often unreliable
    and needs further calibration. We then obtain a set of unique and diverse answers
    (”stances”) by merging semantically equivalent pairs into clusters, using a GPT-3.5
    judge following Tian et al. ([2023](#bib.bib43)). This constitutes the output
    of the first stage: semantically unique stances each with a corresponding frequency
    and aggregated mean confidence.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在任务准确性和校准性能之间寻求平衡，有必要在代理的初始答案和推理路径中保持一定的多样性，同时明智地分配插槽，使得最适合任务的代理理想地成为多数，这为集成提供了基础。受到
    Si 等人 ([2023b](#bib.bib40)) 的启发，我们初始化了四种类型的“专家代理”。每个代理具有不同的提示策略或工具使用专长：Chain-of-Thought（Wei
    等人，[2022](#bib.bib46)）用于多跳推理任务，Program-of-Thoughts（Chen 等人，[2023c](#bib.bib7)）用于代码和算术推理任务，Search-Augmented
    Self-Ask（Press 等人，[2023](#bib.bib35)）用于事实推理任务，以及 GenRead 提示（Yu 等人，[2023](#bib.bib50)）用于知识密集型推理任务。此初始化具有灵活性，任何新的技能或提示策略都可以通过模块化轻松添加。由于多个技能可能与输入数据集相关，我们根据简单的不确定性感知校准得分确定每项技能的重要性排名，并相应分配专家代理，详细内容见附录
    [A.1](#A1.SS1 "A.1 Details on agent selection ‣ Appendix A Appendix ‣ Confidence
    Calibration and Rationalization for LLMs via Multi-Agent Deliberation")。我们使用 Mistral-7B（Jiang
    等人，[2023a](#bib.bib16)），GPT-3.5-turbo（OpenAI，[2022](#bib.bib32)）和 Cohere-Commend（Cohere，[2023](#bib.bib10)）作为专家代理的骨干。对于一个传入的测试问题（例如，“哪个元素是在
    1898 年被发现并以希腊语 ‘新’ 命名？”），每个专家代理通过执行相应的提示策略进行自我审议，并独立对一个答案进行投票，并估计置信度。数值置信度估计可以基于开放模型的输出序列困惑度（PP），对于开放模型，置信度估计通常是不可靠的，需要进一步校准。然后，我们通过将语义等效的对合并成簇，使用
    GPT-3.5 判断，获得一组独特且多样的答案（“立场”）。这构成了第一阶段的输出：每个语义唯一的立场及其相应的频率和聚合均值置信度。
- en: 3.2 Group Deliberation with Rationales and Feedback
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 有理由和反馈的群体审议
- en: 'In the second stage, we initialize another set of ”general agents” (with GPT-3.5
    backbones and no specialized prompting) to perform rationalization and group deliberation.
    The diverse stances from Stage 1 are assigned to the general agents as deliberators,
    proportional to the original answer frequency in Stage 1, thus maintaining potential
    group consensus or majority voices. However, simply relying on the consensus or
    majority may not be ideal, as they can sometimes be misleading. This makes the
    following design necessary — Each agent argues for its assigned stance, producing
    rationales defending it. This effectively elicits multiple diverse reasoning paths
    for the ensemble. Agents give ratings and feedback to each argument in terms of
    logical consistency, factuality, clarity, and conciseness. In particular, the
    judgment for factuality follows a similar approach to Chain-of-Verification (Dhuliawala
    et al., [2023](#bib.bib11)), where we ask each agent to self-generate any premise
    or assumption in the argument under consideration, which could potentially contain
    hallucination. Therefore, we adopt another search-augmented agent to verify each
    premise if necessary and highlight any unfactual statement as part of the textual
    feedback. Each agent is then provided with two rated arguments, one sampled from
    the affirmative position and one sampled from one of the opposing sides. Observing
    different arguments with the associated ratings and feedback on factuality, each
    agent then re-votes their answers $Y_{\text{post}}$ for potential adjustment of
    its confidence:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，我们初始化另一组“通用代理”（使用 GPT-3.5 主干且没有专业提示）进行理性化和群体审议。来自阶段 1 的不同立场被分配给通用代理作为审议者，按阶段
    1 中原始答案的频率分配，从而保持潜在的群体共识或多数声音。然而，仅依赖共识或多数可能不是理想的，因为它们有时可能会产生误导。这使得以下设计成为必要——每个代理为其分配的立场进行辩论，提出为其辩护的理由。这有效地引出了多个不同的推理路径。代理对每个论点进行逻辑一致性、事实准确性、清晰性和简洁性方面的评分和反馈。特别是，对于事实准确性的判断采用类似于验证链的方法（Dhuliawala
    et al., [2023](#bib.bib11)），我们要求每个代理自行生成任何前提或假设，这可能包含幻觉。因此，我们采用另一种搜索增强型代理来验证每个前提，如有必要，并将任何不真实的陈述作为文本反馈的一部分进行突出显示。然后，每个代理将得到两个评分的论点，一个来自肯定立场，一个来自反对方的立场之一。观察带有评分和事实准确性反馈的不同论点后，每个代理重新投票其答案
    $Y_{\text{post}}$ 以可能调整其置信度：
- en: '|  | $Y_{\textrm{post}},R_{\textrm{conf}}=M(Y_{\textrm{prior}},C_{\textrm{prior}},A_{p},F_{A_{p}},A_{n},F_{A_{n}})$
    |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y_{\textrm{post}},R_{\textrm{conf}}=M(Y_{\textrm{prior}},C_{\textrm{prior}},A_{p},F_{A_{p}},A_{n},F_{A_{n}})$
    |  | (1) |'
- en: 'where $Y_{\text{prior}}$. In a separate call, each agent gives a posterior
    confidence estimate based on their final answer and rationales:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个单独的调用中，每个代理根据他们的最终答案和理由给出后验置信度估计：
- en: '|  | $C_{\text{post}}=\mathbb{P}(Y_{\text{reference}}=Y_{\text{post}}\mid Y_{\text{post}},R_{\text{conf}},M)$
    |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{\text{post}}=\mathbb{P}(Y_{\text{reference}}=Y_{\text{post}}\mid Y_{\text{post}},R_{\text{conf}},M)$
    |  | (2) |'
- en: Taking the final majority vote over all $Y_{\text{post}}$, we hypothesize that
    the final mean confidence estimate will be a better indication of the prediction
    accuracy, by weighing different voices, evidence, and feedback through deliberation.
    The intermediate confidence rationales serve as justifications for the final scores,
    improving the interpretability of potential confidence adjustment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 $Y_{\text{post}}$ 进行最终的多数投票，我们假设最终的均值置信度估计将更好地指示预测准确性，通过权衡不同的声音、证据和反馈来进行深思熟虑。中间的置信度理由作为最终分数的证明，提高了潜在置信度调整的可解释性。
- en: 4 Experiments and Results
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与结果
- en: 'Metrics: We use the following metrics for measuring the calibration performance:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标：我们使用以下指标来衡量校准性能：
- en: ECE (Expected Calibration Error, Guo et al., [2017](#bib.bib14)) calculates
    the average squared error between the estimated confidence and average accuracy
    within each bin, weighted by the probability that a random sample falls within
    the bin.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ECE（期望校准误差，Guo et al., [2017](#bib.bib14)）计算每个区间内估计置信度与平均准确性之间的平均平方误差，由随机样本落入该区间的概率加权。
- en: Brier score (Brier, [1950](#bib.bib2)) measures the mean squared difference
    between the predicted probability and the actual outcome, ranging between 0 and
    1 where a smaller value is preferred.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Brier 分数（Brier, [1950](#bib.bib2)）衡量预测概率与实际结果之间的均方差，范围从 0 到 1，较小的值更受欢迎。
- en: 'Experimental Setup: We experiment on six tasks across various domains. Details
    on the datasets and evaluation methods can be found in Appendix [A.2](#A1.SS2
    "A.2 Details on experiment setup ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation"). We compare with multiple
    previous methods for confidence calibration, including consistency-based ensemble
    (Wang et al., [2023b](#bib.bib45)), Ask4Conf (Tian et al., [2023](#bib.bib43)),
    and the best combination of sampling and aggregation approaches reported by Xiong
    et al. ([2023](#bib.bib49)). For fair comparison, we set the ensemble size to
    6 for the self-consistency baseline as with our agent group size, and report baseline
    results evaluated with GPT-3.5\. More details on experimental setups are described
    in Appendix [A.2.2](#A1.SS2.SSS2 "A.2.2 Evaluation methods ‣ A.2 Details on experiment
    setup ‣ Appendix A Appendix ‣ Confidence Calibration and Rationalization for LLMs
    via Multi-Agent Deliberation").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置：我们在六个不同领域的任务上进行了实验。数据集和评估方法的详细信息可以在附录 [A.2](#A1.SS2 "A.2 详细的实验设置 ‣ 附录 A
    附录 ‣ 通过多智能体审议进行的LLMs置信度校准和合理化") 中找到。我们与多个以前的置信度校准方法进行了比较，包括基于一致性的集成方法（Wang et
    al.，[2023b](#bib.bib45)），Ask4Conf（Tian et al.，[2023](#bib.bib43)），以及Xiong et al.（[2023](#bib.bib49)）报告的采样和聚合方法的最佳组合。为了公平比较，我们将集成大小设置为6，以匹配我们的智能体组大小，并报告了使用GPT-3.5评估的基线结果。关于实验设置的更多细节在附录
    [A.2.2](#A1.SS2.SSS2 "A.2.2 评估方法 ‣ A.2 详细的实验设置 ‣ 附录 A 附录 ‣ 通过多智能体审议进行的LLMs置信度校准和合理化")
    中描述。
- en: 'Results: Our main results are reported in Table [1](#S4.T1 "Table 1 ‣ 4 Experiments
    and Results ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent
    Deliberation"). Compared with the baselines, Collaborative Calibration achieves
    smaller ECE across four of the six tasks, especially for arithmetic and symbolic
    reasoning tasks (GSM8K, DateUnd) as well as for ambiguity resolution (AmbigQA).
    On factoid and knowledge-intensive tasks, Collaborative Calibration is also able
    to achieve similar or better results in terms of both ECE and Brier scores. These
    can be expected as the agents are augmented with programming and search abilities,
    and can deliberate under uncertainty or ambiguity for reasonable confidence adjustments.
    Example rationales for the final verbalized confidence can be found in Table [3](#A1.T3
    "Table 3 ‣ A.4 Prompt templates and example output ‣ Appendix A Appendix ‣ Confidence
    Calibration and Rationalization for LLMs via Multi-Agent Deliberation"). Note
    that for cost-efficiency purposes, the current workflow only adopts search augmentation
    to a limited degree. Integrating more tool-use modules could further improve calibration
    performance on knowledge-intensive tasks. Compared with the best set of strategies
    reported by Xiong et al. ([2023](#bib.bib49)), Collaborative Calibration can elicit
    more calibrated confidence scores on the GSM8K and DateUnd datasets, suggesting
    the effectiveness of additional group interaction and rationalization, beyond
    simply improving task accuracy. The same pattern is also observed for the comparison
    with simple consistency-based ensembles, which is shown in detail in Appendix
    [A.3](#A1.SS3 "A.3 Detailed Results ‣ Appendix A Appendix ‣ Confidence Calibration
    and Rationalization for LLMs via Multi-Agent Deliberation").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：我们的主要结果报告在表格 [1](#S4.T1 "表格 1 ‣ 4 实验和结果 ‣ 通过多智能体审议进行的LLMs置信度校准和合理化") 中。与基线相比，协作校准在六个任务中的四个任务上实现了更小的ECE，特别是在算术和符号推理任务（GSM8K，DateUnd）以及在歧义解决（AmbigQA）方面。在事实和知识密集型任务中，协作校准也能在ECE和Brier分数上达到相似或更好的结果。这是可以预期的，因为智能体增强了编程和搜索能力，并且可以在不确定性或歧义下进行合理的置信度调整。最终口头置信度的示例理由可以在表格
    [3](#A1.T3 "表格 3 ‣ A.4 提示模板和示例输出 ‣ 附录 A 附录 ‣ 通过多智能体审议进行的LLMs置信度校准和合理化") 中找到。需要注意的是，为了成本效率，目前的工作流程仅在有限程度上采用了搜索增强。整合更多工具使用模块可能会进一步提高在知识密集型任务上的校准性能。与Xiong
    et al.（[2023](#bib.bib49)）报告的最佳策略集合相比，协作校准能够在GSM8K和DateUnd数据集上引发更为校准的置信度评分，表明除了单纯提高任务准确性之外，额外的组互动和合理化也是有效的。与简单的一致性基于集成的比较也观察到相同的模式，详细信息请参见附录
    [A.3](#A1.SS3 "A.3 详细结果 ‣ 附录 A 附录 ‣ 通过多智能体审议进行的LLMs置信度校准和合理化")。
- en: '|  | GSM8K | TriviaQA | SciQ | AmbigQA | DateUnd | Biz-Ethics |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | GSM8K | TriviaQA | SciQ | AmbigQA | DateUnd | Biz-Ethics |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Method | ECE$\downarrow$ |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ECE$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| Ask4Conf(1S Top-4) | .196* | - | .054 | .144 | .065 | .209 | - | - | .261*
    | - | .124* | .163 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Ask4Conf(1S Top-4) | .196* | - | .054 | .144 | .065 | .209 | - | - | .261*
    | - | .124* | .163 |'
- en: '| Ask4Conf(2S-CoT) | - | - | .110 | .168 | .323 | .296 | - | - | - | - | -
    | - |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Ask4Conf(2S-CoT) | - | - | .110 | .168 | .323 | .296 | - | - | - | - | -
    | - |'
- en: '| Verbalized+Consistency(M=6) | .657 | .620 | .055 | .050 | .053 | .094 | .052
    | .098 | .092 | .162 | .141 | .201 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Verbalized+Consistency(M=6) | .657 | .620 | .055 | .050 | .053 | .094 | .052
    | .098 | .092 | .162 | .141 | .201 |'
- en: '| Top-K+Self-Random+Avg-Conf | .093 | - | .089^† | - | .221^† | - | .134^†
    | - | .146 | - | .158 | - |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Top-K+Self-Random+Avg-Conf | .093 | - | .089^† | - | .221^† | - | .134^†
    | - | .146 | - | .158 | - |'
- en: '| CollabCalibration(M=6) | .086 | .213 | .070 | .062 | .035 | .129 | .026 |
    .126 | .055 | .130 | .132 | .203 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| CollabCalibration(M=6) | .086 | .213 | .070 | .062 | .035 | .129 | .026 |
    .126 | .055 | .130 | .132 | .203 |'
- en: 'Table 1: We compare Collaborative Calibration with previous training-free calibration
    methods on a variety of tasks using GPT-3.5\. (The symbol * denotes results reported
    by Xiong et al., [2023](#bib.bib49); $\dagger$ denotes results we reproduce based
    on the extension of their implementation). Following Tian et al. ([2023](#bib.bib43)),
    each cell in a column is shaded with a gradient from cyan to orange representing
    varying levels of calibration performance from high to low. The best result of
    each column is bolded. Our method achieves superior calibration performance in
    terms of ECE on four of the six tasks, and similar results for other tasks and
    in terms of Brier Score.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们使用GPT-3.5将协作校准与之前的无训练校准方法进行比较，涵盖多种任务。 (*符号表示由Xiong等人[2023](#bib.bib49)报告的结果；$\dagger$符号表示我们基于他们实现扩展所复现的结果)。根据Tian等人([2023](#bib.bib43))，每列的单元格用从青色到橙色的渐变进行着色，表示校准性能从高到低的不同水平。每列的最佳结果用粗体显示。我们的方法在六个任务中的四个任务上，在ECE方面实现了优越的校准性能，对于其他任务和Brier
    Score也取得了类似的结果。
- en: 5 Conclusion
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we explore a collaborative approach to elicit, calibrate, and
    rationalize prediction confidence of LLMs, employing language agents that can
    be flexible and autonomous in selecting reasoning strategies, using tools, and
    self-refining with rationales and collective feedback. We demonstrate the flexibility
    and effectiveness of our method on generative QA tasks across different domains,
    suggesting possibilities for future work on better leveraging rationales and agent
    collaboration for improving the reliability of LLM predictions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们探索了一种协作方法来引出、校准和理顺LLM的预测置信度，采用能够灵活自主选择推理策略、使用工具和通过理由和集体反馈自我精炼的语言代理。我们展示了我们的方法在不同领域的生成QA任务中的灵活性和有效性，建议未来的工作可以更好地利用理由和代理合作，以提高LLM预测的可靠性。
- en: References
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'bench authors (2023) BIG bench authors. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *Transactions on Machine
    Learning Research*, 2023. ISSN 2835-8856. URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bench authors (2023) BIG bench authors. 超越模仿游戏：量化和外推语言模型的能力。*机器学习研究论文集*，2023年。ISSN
    2835-8856。网址 [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj)。
- en: Brier (1950) Glenn W. Brier. Verification of forecasts expressed in terms of
    probability. *Monthly Weather Review*, 78:1–3, 1950. URL [https://api.semanticscholar.org/CorpusID:122906757](https://api.semanticscholar.org/CorpusID:122906757).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brier (1950) Glenn W. Brier. 以概率表达的预报验证。*月度天气评论*，78:1–3，1950年。网址 [https://api.semanticscholar.org/CorpusID:122906757](https://api.semanticscholar.org/CorpusID:122906757)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等（2020）汤姆·布朗、本杰明·曼、尼克·莱德、梅拉妮·萨比亚、贾里德·D·卡普兰、普拉弗拉·达里瓦尔、阿尔文·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔、桑迪尼·阿加瓦尔、阿里尔·赫伯特-沃斯、格雷琴·克鲁格、汤姆·赫尼根、雷温·查尔德、阿迪亚·拉梅什、丹尼尔·齐格勒、杰弗里·吴、克莱门斯·温特、克里斯·赫斯、马克·陈、埃里克·西格勒、马特乌斯·利特温、斯科特·格雷、本杰明·切斯、杰克·克拉克、克里斯托弗·伯纳、萨姆·麦肯德利什、亚历克·拉德福德、伊利亚·苏茨克维尔和达里奥·阿莫代。《语言模型是少量样本学习者》。在H.
    拉罗谢尔、M. 兰扎托、R. 哈德塞尔、M.F. 巴尔坎和H. 林（编），*神经信息处理系统进展*，第33卷，第1877-1901页。Curran Associates,
    Inc.，2020。网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial
    general intelligence: Early experiments with gpt-4, 2023.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck等（2023）塞巴斯蒂安·布贝克、瓦伦·钱德拉塞卡兰、罗南·埃尔丹、约翰内斯·格赫克、埃里克·霍维茨、埃切·卡马尔、彼得·李、尹·塔特·李、袁志·李、斯科特·伦德伯格、哈沙·诺里、哈米德·帕朗吉、马尔科·图利奥·里贝罗和易·张。《人工智能的火花：与
    GPT-4 的早期实验》，2023。
- en: 'Chen et al. (2023a) Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal.
    Reconcile: Round-table conference improves reasoning via consensus among diverse
    llms, 2023a.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023a）贾斯汀·志尧·陈、斯瓦尔纳迪普·萨哈和莫希特·班萨尔。《Reconcile：圆桌会议通过不同语言模型间的共识提升推理能力》，2023a。
- en: 'Chen et al. (2023b) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin
    Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating
    multi-agent collaboration and exploring emergent behaviors, 2023b.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023b）魏泽·陈、余胜·苏、景伟·左、程洋、陈飞·袁、纪敏·陈、何阳·余、雅希·陆、易欣·洪、陈谦、余佳·秦、辛聪、若冰·谢、智源·刘、毛松·孙和杰·周。《Agentverse：促进多智能体协作并探索突现行为》，2023b。
- en: 'Chen et al. (2023c) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks, 2023c.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023c）温虎·陈、薛光·马、新怡·王和威廉·W·科恩。《思维提示程序：为数字推理任务解开计算与推理的关系》，2023c。
- en: Chen et al. (2023d) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao,
    Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal
    self-consistency for large language model generation, 2023d.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023d）辛云·陈、雷纳特·阿克西托夫、乌里·阿隆、杰·任、克凡·肖、彭城·尹、苏尚特·普拉卡什、查尔斯·萨顿、薛智·王和丹尼·周。《大语言模型生成的普适自一致性》，2023d。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等（2021）卡尔·科比、维尼特·科萨拉朱、穆罕默德·巴瓦里安、马克·陈、许宇·俊、卢卡斯·凯瑟尔、马蒂亚斯·普拉普特、杰瑞·特沃雷克、雅各布·希尔顿、内藤礼一郎、克里斯托弗·赫斯和约翰·舒尔曼。《训练验证者解决数学应用题》。*arXiv
    预印本 arXiv:2110.14168*，2021。
- en: Cohere (2023) Cohere. Cohere-command models, 2023. URL [https://docs.cohere.com/docs/models](https://docs.cohere.com/docs/models).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohere（2023）Cohere。《Cohere-command 模型》，2023。网址 [https://docs.cohere.com/docs/models](https://docs.cohere.com/docs/models)。
- en: Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces
    hallucination in large language models, 2023.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala等（2023）谢扎德·杜利亚瓦拉、穆杰塔巴·科梅利、景·徐、罗伯塔·雷勒努、鲜·李、阿斯利·切利基尔马兹和杰森·韦斯顿。《链式验证减少大语言模型中的幻觉》，2023。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate, 2023.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等（2023）伊伦·杜、双李、安东尼奥·托拉尔巴、约书亚·B·坦南鲍姆和伊戈尔·莫达奇。《通过多智能体辩论提升语言模型的事实性和推理能力》，2023。
- en: 'Gal & Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian
    approximation: Representing model uncertainty in deep learning. In Maria Florina
    Balcan and Kilian Q. Weinberger (eds.), *Proceedings of The 33rd International
    Conference on Machine Learning*, volume 48 of *Proceedings of Machine Learning
    Research*, pp.  1050–1059, New York, New York, USA, 20–22 Jun 2016\. PMLR. URL
    [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gal 和 Ghahramani (2016) Yarin Gal 和 Zoubin Ghahramani. Dropout 作为贝叶斯近似：在深度学习中表示模型不确定性。收录于
    Maria Florina Balcan 和 Kilian Q. Weinberger (编辑)，*第33届国际机器学习会议论文集*，第48卷，*机器学习研究论文集*，第1050–1059页，美国纽约，2016年6月20–22日。PMLR。网址
    [https://proceedings.mlr.press/v48/gal16.html](https://proceedings.mlr.press/v48/gal16.html)。
- en: Guo et al. (2017) Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger.
    On calibration of modern neural networks. In Doina Precup and Yee Whye Teh (eds.),
    *Proceedings of the 34th International Conference on Machine Learning*, volume 70
    of *Proceedings of Machine Learning Research*, pp.  1321–1330\. PMLR, 06–11 Aug
    2017. URL [https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2017) Chuan Guo, Geoff Pleiss, Yu Sun 和 Kilian Q. Weinberger. 现代神经网络的校准问题。收录于
    Doina Precup 和 Yee Whye Teh (编辑)，*第34届国际机器学习会议论文集*，第70卷，*机器学习研究论文集*，第1321–1330页。PMLR，2017年8月6–11日。网址
    [https://proceedings.mlr.press/v70/guo17a.html](https://proceedings.mlr.press/v70/guo17a.html)。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2021) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song 和 Jacob Steinhardt. 测量大规模多任务语言理解。*国际学习表示会议（ICLR）论文集*，2021年。
- en: Jiang et al. (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. Mistral 7b, 2023a.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix 和 William
    El Sayed. Mistral 7b, 2023a。
- en: 'Jiang et al. (2023b) Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender:
    Ensembling large language models with pairwise comparison and generative fusion.
    In *Proceedings of the 61th Annual Meeting of the Association for Computational
    Linguistics (ACL 2023)*, 2023b.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023b) Dongfu Jiang, Xiang Ren 和 Bill Yuchen Lin. Llm-blender：通过成对比较和生成融合来集成大型语言模型。收录于
    *第61届计算语言学协会年会（ACL 2023）论文集*，2023b。
- en: 'Jiang et al. (2021) Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig.
    How can we know when language models know? on the calibration of language models
    for question answering. *Transactions of the Association for Computational Linguistics*,
    9:962–977, 2021. doi: 10.1162/tacl˙a˙00407. URL [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等 (2021) Zhengbao Jiang, Jun Araki, Haibo Ding 和 Graham Neubig. 我们如何知道语言模型是否了解？语言模型在问答中的校准问题。*计算语言学协会会刊*，第9卷：962–977，2021年。doi:
    10.1162/tacl˙a˙00407。网址 [https://aclanthology.org/2021.tacl-1.57](https://aclanthology.org/2021.tacl-1.57)。'
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
    TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.
    In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the 55th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 
    1601–1611, Vancouver, Canada, July 2017\. Association for Computational Linguistics.
    doi: 10.18653/v1/P17-1147. URL [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joshi 等 (2017) Mandar Joshi, Eunsol Choi, Daniel Weld 和 Luke Zettlemoyer. TriviaQA：一个大规模远程监督的挑战数据集用于阅读理解。收录于
    Regina Barzilay 和 Min-Yen Kan (编辑)，*第55届计算语言学协会年会论文集（第1卷：长篇论文）*，第1601–1611页，加拿大温哥华，2017年7月。计算语言学协会。doi:
    10.18653/v1/P17-1147。网址 [https://aclanthology.org/P17-1147](https://aclanthology.org/P17-1147)。'
- en: Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan
    Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez,
    Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine
    Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben
    Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know
    what they know, 2022.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡达瓦斯等人（2022）索拉夫·卡达瓦斯、汤姆·科纳利、阿曼达·阿斯凯尔、汤姆·赫宁汉、道恩·德雷恩、伊桑·佩雷斯、尼古拉斯·谢费尔、扎克·哈特菲尔德-多兹、诺瓦·达萨玛、艾丽·特兰-约翰逊、斯科特·约翰斯顿、谢尔·艾尔-肖克、安迪·琼斯、纳尔逊·埃尔哈格、特里斯坦·休姆、安娜·陈、尹涛·白、山姆·鲍曼、斯坦尼斯拉夫·福特、迪普·甘古利、丹尼·埃尔南德斯、乔什·雅各布森、杰克逊·科尔尼翁、肖娜·克拉维克、莉安·洛维特、卡马尔·恩杜斯、凯瑟琳·奥尔森、山姆·林格、达里奥·阿莫迪、汤姆·布朗、杰克·克拉克、尼古拉斯·约瑟夫、本·曼、山姆·麦肯德利什、克里斯·奥拉和贾里德·卡普兰。语言模型（大多）知道它们所知道的，2022年。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models, 2020.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡普兰等人（2020）贾里德·卡普兰、山姆·麦肯德利什、汤姆·赫宁汉、汤姆·B·布朗、本杰明·切斯、瑞万·查德、斯科特·格雷、亚历克·拉德福德、杰弗里·吴和达里奥·阿莫迪。神经语言模型的规模定律，2020年。
- en: 'Kong et al. (2020) Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo
    Zhao, and Chao Zhang. Calibrated language model fine-tuning for in- and out-of-distribution
    data. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pp.  1326–1340, Online, November 2020\. Association for Computational Linguistics.
    doi: 10.18653/v1/2020.emnlp-main.102. URL [https://aclanthology.org/2020.emnlp-main.102](https://aclanthology.org/2020.emnlp-main.102).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '孔等人（2020）凌凯·孔、昊铭·姜、雨辰·庄、杰·吕、拓·赵和超·张。对内外分布数据的校准语言模型微调。在Bonnie Webber、Trevor
    Cohn、Yulan He和Yang Liu（编辑）主编的*2020年自然语言处理经验方法会议（EMNLP）论文集*，第1326–1340页，在线，2020年11月。计算语言学协会。doi:
    10.18653/v1/2020.emnlp-main.102。网址[https://aclanthology.org/2020.emnlp-main.102](https://aclanthology.org/2020.emnlp-main.102)。'
- en: 'Kuhn et al. (2023) Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic
    uncertainty: Linguistic invariances for uncertainty estimation in natural language
    generation. In *The Eleventh International Conference on Learning Representations*,
    2023. URL [https://openreview.net/forum?id=VD-AYtP0dve](https://openreview.net/forum?id=VD-AYtP0dve).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhn等人（2023）洛伦茨·库恩、亚林·加尔和塞巴斯蒂安·法尔夸。语义不确定性：自然语言生成中用于不确定性估计的语言不变性。发表于*第十一届国际学习表示大会*，2023年。网址[https://openreview.net/forum?id=VD-AYtP0dve](https://openreview.net/forum?id=VD-AYtP0dve)。
- en: Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models
    to express their uncertainty in words. *Transactions on Machine Learning Research*,
    2022. ISSN 2835-8856. URL [https://openreview.net/forum?id=8s8K2UZGTZ](https://openreview.net/forum?id=8s8K2UZGTZ).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人（2022）斯蒂芬妮·林、雅各布·希尔顿和欧文·埃文斯。教模型用语言表达它们的不确定性。*机器学习研究交易*，2022年。ISSN 2835-8856。网址[https://openreview.net/forum?id=8s8K2UZGTZ](https://openreview.net/forum?id=8s8K2UZGTZ)。
- en: 'Lin et al. (2023) Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. Generating with
    confidence: Uncertainty quantification for black-box large language models, 2023.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人（2023）甄·林、舒本德·特里维迪和吉门·孙。自信生成：黑箱大型语言模型的不确定性量化，2023年。
- en: Liu et al. (2023a) Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou,
    Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language
    models on simulated social interactions, 2023a.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2023a）瑞波·刘、瑞欣·杨、陈彦·贾、格·张、丹尼·周、安德鲁·M·戴、迪伊·杨和索鲁什·沃索基。对模拟社会互动进行社会对齐语言模型的训练，2023a。
- en: 'Liu et al. (2023b) Xin Liu, Muhammad Khalifa, and Lu Wang. Litcab: Lightweight
    calibration of language models on outputs of varied lengths, 2023b.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2023b）辛·刘、穆罕默德·卡利法和卢·王。Litcab：对不同长度输出的语言模型进行轻量级校准，2023b。
- en: 'Liu et al. (2023c) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang.
    Dynamic llm-agent network: An llm-agent collaboration framework with agent team
    optimization, 2023c.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人（2023c） Zijun Liu、Yanzhe Zhang、Peng Li、Yang Liu和Diyi Yang。动态LLM代理网络：一种具有代理团队优化的LLM代理协作框架，2023c。
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等人（2023）阿曼·马丹、尼克特·坦东、普拉卡什·古普塔、斯凯勒·哈林南、刘宇·高、莎拉·维格雷夫、乌里·阿隆、努哈·兹里、什里马伊·普拉布摩耶、易明·杨、肖恩·维莱克、博迪萨特瓦·普拉萨德·马朱姆德、沙尚克·古普塔、阿米尔·亚兹丹巴赫和彼得·克拉克。《Self-refine:
    通过自我反馈进行迭代精炼》，2023年。'
- en: 'Mielke et al. (2022) Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan
    Boureau. Reducing conversational agents’ overconfidence through linguistic calibration.
    *Transactions of the Association for Computational Linguistics*, 10:857–872, 2022.
    doi: 10.1162/tacl˙a˙00494. URL [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mielke 等人（2022）萨布丽娜·J·米尔克、阿瑟·斯拉姆、艾米莉·迪南和Y-兰·布雷奥。《通过语言校准减少对话代理的过度自信》。*计算语言学协会学报*，第10卷:
    857–872，2022年。doi: 10.1162/tacl˙a˙00494。网址 [https://aclanthology.org/2022.tacl-1.50](https://aclanthology.org/2022.tacl-1.50)。'
- en: 'Min et al. (2020) Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke
    Zettlemoyer. AmbigQA: Answering ambiguous open-domain questions. In Bonnie Webber,
    Trevor Cohn, Yulan He, and Yang Liu (eds.), *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pp.  5783–5797,
    Online, November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.466.
    URL [https://aclanthology.org/2020.emnlp-main.466](https://aclanthology.org/2020.emnlp-main.466).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Min 等人（2020）斯旺·敏、朱利安·迈克尔、哈纳赫·哈吉什尔齐和卢克·泽特尔莫耶。《AmbigQA: 回答模糊的开放域问题》。在博尼·韦伯、特雷弗·科恩、于岚和杨柳（编），*2020年自然语言处理实证方法会议（EMNLP）论文集*，第5783–5797页，线上，2020年11月。计算语言学协会。doi:
    10.18653/v1/2020.emnlp-main.466。网址 [https://aclanthology.org/2020.emnlp-main.466](https://aclanthology.org/2020.emnlp-main.466)。'
- en: OpenAI (2022) OpenAI. Chatgpt blog, 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2022）OpenAI。《Chatgpt 博客》，2022年。网址 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive
    simulacra of human behavior. In *In the 36th Annual ACM Symposium on User Interface
    Software and Technology (UIST ’23)*, UIST ’23, New York, NY, USA, 2023\. Association
    for Computing Machinery.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等人（2023）俊生·朴、约瑟夫·C·奥布莱恩、卡里·J·蔡、梅雷迪思·林戈尔·莫里斯、珀西·梁和迈克尔·S·伯恩斯坦。《生成代理: 人类行为的互动模拟》。在*第36届年度ACM用户界面软件与技术研讨会（UIST
    ’23）*，UIST ’23，纽约，美国，2023年。计算机协会。'
- en: 'Portillo Wightman et al. (2023) Gwenyth Portillo Wightman, Alexandra Delucia,
    and Mark Dredze. Strength in numbers: Estimating confidence of large language
    models by prompt agreement. In Anaelia Ovalle, Kai-Wei Chang, Ninareh Mehrabi,
    Yada Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista Cao, Anoop
    Kumar, and Rahul Gupta (eds.), *Proceedings of the 3rd Workshop on Trustworthy
    Natural Language Processing (TrustNLP 2023)*, pp.  326–362, Toronto, Canada, July
    2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.trustnlp-1.28.
    URL [https://aclanthology.org/2023.trustnlp-1.28](https://aclanthology.org/2023.trustnlp-1.28).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Portillo Wightman 等人（2023）格温妮丝·波蒂略·怀特曼、亚历山德拉·德卢西亚和马克·德雷泽。《数量上的力量: 通过提示一致性估计大语言模型的信心》。在安娜利亚·奥瓦莱、凯-韦伊·张、尼娜雷赫·梅赫拉比、雅达·普鲁克沙查昆、阿拉姆·加利斯坦、贾瓦拉·达马拉、阿普尔夫·维尔马、特丽斯塔·曹、阿努普·库马尔和拉胡尔·古普塔（编），*第3届可信自然语言处理研讨会（TrustNLP
    2023）论文集*，第326–362页，加拿大多伦多，2023年7月。计算语言学协会。doi: 10.18653/v1/2023.trustnlp-1.28。网址
    [https://aclanthology.org/2023.trustnlp-1.28](https://aclanthology.org/2023.trustnlp-1.28)。'
- en: Press et al. (2023) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A.
    Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language
    models, 2023.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press 等人（2023）奥菲尔·普雷斯、穆鲁·张、斯旺·敏、路德维希·施密特、诺亚·A·史密斯和迈克·刘易斯。《测量和缩小语言模型的组成性差距》，2023年。
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools, 2023.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人（2023）蒂莫·施克、简·德维维迪-于、罗伯托·德西、罗伯塔·赖莱努、玛丽亚·洛梅利、卢克·泽特尔莫耶、尼古拉·坎切达和托马斯·斯基亚隆。《Toolformer:
    语言模型如何自我学习使用工具》，2023年。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning, 2023.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人（2023）诺亚·辛恩、费德里科·卡萨诺、爱德华·伯曼、阿什温·戈皮纳斯、卡尔蒂克·纳拉西姆汉和尚宇·姚。《Reflexion: 使用语言代理的语言强化学习》，2023年。'
- en: 'Si et al. (2022) Chenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-Graber.
    Re-examining calibration: The case of question answering. In Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang (eds.), *Findings of the Association for Computational
    Linguistics: EMNLP 2022*, pp.  2814–2829, Abu Dhabi, United Arab Emirates, December
    2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.204.
    URL [https://aclanthology.org/2022.findings-emnlp.204](https://aclanthology.org/2022.findings-emnlp.204).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Si et al. (2022) Chenglei Si, Chen Zhao, Sewon Min, 和 Jordan Boyd-Graber. 重新审视校准：问答的案例。见
    Yoav Goldberg, Zornitsa Kozareva, 和 Yue Zhang（编），*计算语言学协会会议论文集：EMNLP 2022*，第 2814–2829
    页，阿布扎比，阿联酋，2022 年 12 月。计算语言学协会。doi: 10.18653/v1/2022.findings-emnlp.204。网址 [https://aclanthology.org/2022.findings-emnlp.204](https://aclanthology.org/2022.findings-emnlp.204)。'
- en: Si et al. (2023a) Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng
    Wang, Jordan Lee Boyd-Graber, and Lijuan Wang. Prompting GPT-3 to be reliable.
    In *The Eleventh International Conference on Learning Representations*, 2023a.
    URL [https://openreview.net/forum?id=98p5x51L5af](https://openreview.net/forum?id=98p5x51L5af).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si et al. (2023a) Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng
    Wang, Jordan Lee Boyd-Graber, 和 Lijuan Wang. 促使 GPT-3 变得可靠。见 *第十一届国际学习表征会议*，2023a。网址
    [https://openreview.net/forum?id=98p5x51L5af](https://openreview.net/forum?id=98p5x51L5af)。
- en: Si et al. (2023b) Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, and
    Jordan Boyd-Graber. Getting more out of mixture of language model reasoning experts,
    2023b.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Si et al. (2023b) Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, 和 Jordan
    Boyd-Graber. 从语言模型推理专家的混合中获得更多，2023b。
- en: 'Silver et al. (2021) Ike Silver, Barbara A. Mellers, and Philip E. Tetlock.
    Wise teamwork: Collective confidence calibration predicts the effectiveness of
    group discussion. *Journal of Experimental Social Psychology*, 96:104157, 2021.
    ISSN 0022-1031. doi: https://doi.org/10.1016/j.jesp.2021.104157. URL [https://www.sciencedirect.com/science/article/pii/S0022103121000603](https://www.sciencedirect.com/science/article/pii/S0022103121000603).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Silver et al. (2021) Ike Silver, Barbara A. Mellers, 和 Philip E. Tetlock. 明智的团队合作：集体信心水平预测小组讨论的有效性。*实验社会心理学杂志*，96:104157，2021。ISSN
    0022-1031。doi: https://doi.org/10.1016/j.jesp.2021.104157。网址 [https://www.sciencedirect.com/science/article/pii/S0022103121000603](https://www.sciencedirect.com/science/article/pii/S0022103121000603)。'
- en: 'Sun et al. (2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. Adaplanner: Adaptive planning from feedback with language models, 2023.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, 和 Chao
    Zhang. Adaplanner: 基于语言模型的反馈自适应规划，2023。'
- en: 'Tian et al. (2023) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma,
    Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. Just ask
    for calibration: Strategies for eliciting calibrated confidence scores from language
    models fine-tuned with human feedback, 2023.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2023) Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma,
    Rafael Rafailov, Huaxiu Yao, Chelsea Finn, 和 Christopher D. Manning. 只需请求校准：从通过人类反馈微调的语言模型中引出校准置信度评分的策略，2023。
- en: Wang et al. (2023a) Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao
    Zhang, and Yelong Shen. Adapting llm agents through communication, 2023a.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao
    Zhang, 和 Yelong Shen. 通过沟通适应 LLM 代理，2023a。
- en: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *The Eleventh International
    Conference on Learning Representations*, 2023b. URL [https://openreview.net/forum?id=1PL1NIMMrw](https://openreview.net/forum?id=1PL1NIMMrw).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 自一致性改善语言模型中的思维链推理。见 *第十一届国际学习表征会议*，2023b。网址
    [https://openreview.net/forum?id=1PL1NIMMrw](https://openreview.net/forum?id=1PL1NIMMrw)。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information Processing
    Systems*, volume 35, pp.  24824–24837\. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc V Le, 和 Denny Zhou. 思维链提示在大语言模型中引发推理。见 S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, 和 A. Oh (编), *神经信息处理系统进展*, 第35卷,
    页码 24824–24837. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
- en: Wei et al. (2023) Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le.
    Simple synthetic data reduces sycophancy in large language models, 2023.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, 和 Quoc V. Le.
    简单的合成数据减少大语言模型中的拍马屁行为, 2023。
- en: 'Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing
    multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and
    Tim Baldwin (eds.), *Proceedings of the 3rd Workshop on Noisy User-generated Text*,
    pp.  94–106, Copenhagen, Denmark, September 2017\. Association for Computational
    Linguistics. doi: 10.18653/v1/W17-4413. URL [https://aclanthology.org/W17-4413](https://aclanthology.org/W17-4413).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Welbl et al. (2017) Johannes Welbl, Nelson F. Liu, 和 Matt Gardner. 群体外包多选科学问题。见
    Leon Derczynski, Wei Xu, Alan Ritter, 和 Tim Baldwin (编), *第3届噪声用户生成文本研讨会论文集*,
    页码 94–106, 哥本哈根, 丹麦, 2017年9月。计算语言学协会。doi: 10.18653/v1/W17-4413. URL [https://aclanthology.org/W17-4413](https://aclanthology.org/W17-4413).'
- en: Xiong et al. (2023) Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian
    He, and Bryan Hooi. Can llms express their uncertainty? an empirical evaluation
    of confidence elicitation in llms, 2023.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong et al. (2023) Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian
    He, 和 Bryan Hooi. 大语言模型能否表达其不确定性？对大语言模型信心引导的实证评估, 2023。
- en: 'Yu et al. (2023) Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju,
    Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. Generate rather than
    retrieve: Large language models are strong context generators. In *The Eleventh
    International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=fB0hRu9GZUS](https://openreview.net/forum?id=fB0hRu9GZUS).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2023) Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju,
    Soumya Sanyal, Chenguang Zhu, Michael Zeng, 和 Meng Jiang. 生成而非检索：大语言模型是强大的上下文生成器。见
    *第十一届国际学习表征会议*, 2023。URL [https://openreview.net/forum?id=fB0hRu9GZUS](https://openreview.net/forum?id=fB0hRu9GZUS).
- en: 'Zhou et al. (2023) Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating
    the grey area: How expressions of uncertainty and overconfidence affect language
    models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pp.  5506–5524,
    Singapore, December 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.335.
    URL [https://aclanthology.org/2023.emnlp-main.335](https://aclanthology.org/2023.emnlp-main.335).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) Kaitlyn Zhou, Dan Jurafsky, 和 Tatsunori Hashimoto. 导航灰色区域：不确定性和过度自信的表达如何影响语言模型。见
    Houda Bouamor, Juan Pino, 和 Kalika Bali (编), *2023年自然语言处理实证方法会议论文集*, 页码 5506–5524,
    新加坡, 2023年12月。计算语言学协会。doi: 10.18653/v1/2023.emnlp-main.335. URL [https://aclanthology.org/2023.emnlp-main.335](https://aclanthology.org/2023.emnlp-main.335).'
- en: Appendix A Appendix
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Details on agent selection
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 代理选择的细节
- en: 'To determine the composition of expert agents used in Stage 1, we rank the
    relevance of each of the four pre-specified skills based on calibration performance
    on the validation set. We first sample $m$:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为确定第1阶段使用的专家代理的组成，我们根据在验证集上的校准性能对四个预先指定的技能的相关性进行排名。我们首先采样 $m$：
- en: '|  |  $1$2  |  | (3) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  $1$2  |  | (3) |'
- en: 'The intuition behind this score is to manually boost confidence given a correct
    answer, and diminish it otherwise. Aggregating confidence over all examples for
    each agent and filtering out those below a certain threshold $\tau$ (set to 0.2
    in our experiments), we get a vector of adjusted and filtered confidences:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该分数背后的直觉是，在给出正确答案时手动提升信心，否则减少信心。对每个代理的所有示例进行信心聚合，并过滤掉低于某个阈值 $\tau$（在我们的实验中设置为0.2），我们得到一个调整过的和过滤过的信心向量：
- en: '|  |  $c^{\prime}_{i}=\frac{1}{m}\sum_{j=1}^{m}c^{\prime}_{i,j}$  |  | (4)
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  $c^{\prime}_{i}=\frac{1}{m}\sum_{j=1}^{m}c^{\prime}_{i,j}$  |  | (4)
    |'
- en: For a total of $N$ slots to different agent types.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有 $N$ 个槽位用于不同的代理类型。
- en: A.2 Details on experiment setup
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实验设置的详细信息
- en: A.2.1 Datasets
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 数据集
- en: We experiment on various tasks including GSM8K (Cobbe et al., [2021](#bib.bib9))
    for arithmetic reasoning, TriviaQA (Joshi et al., [2017](#bib.bib19)), and SciQ
    (Welbl et al., [2017](#bib.bib48)) for factoid and knowledge-intensive questions,
    AmbigQA (Min et al., [2020](#bib.bib31)) for reasoning under ambiguity, and Date
    Understanding (DateUnd) from BigBench (bench authors, [2023](#bib.bib1)) for symbolic
    reasoning, as well as the Business Ethics (Biz-Ethics) dataset from MMLU (Hendrycks
    et al., [2021](#bib.bib15)) for questions requiring ethical knowledge. We randomly
    sample 300 questions for each task from the test or validation split for our experiment,
    except for the smaller Biz-Ethics dataset where we use all the examples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在各种任务上进行实验，包括用于算术推理的GSM8K (Cobbe et al., [2021](#bib.bib9))，用于事实性和知识密集型问题的TriviaQA
    (Joshi et al., [2017](#bib.bib19)) 和SciQ (Welbl et al., [2017](#bib.bib48))，用于模糊推理的AmbigQA
    (Min et al., [2020](#bib.bib31))，以及用于符号推理的BigBench (bench authors, [2023](#bib.bib1))中的Date
    Understanding (DateUnd)，还有用于需要伦理知识的问题的MMLU (Hendrycks et al., [2021](#bib.bib15))中的Business
    Ethics (Biz-Ethics)数据集。我们从每个任务的测试或验证集中随机抽取300个问题进行实验，但对于较小的Biz-Ethics数据集，我们使用所有示例。
- en: A.2.2 Evaluation methods
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 评估方法
- en: We explore the optimal number of agents and the number of feedback per argument,
    which are set to 6 and 2 respectively. We experiment with an ensemble of Mistral-7B
    (Jiang et al., [2023a](#bib.bib16)), GPT-3.5-turbo (OpenAI, [2022](#bib.bib32)),
    and Cohere-Commend (Cohere, [2023](#bib.bib10)) as backbones for Stage 1 expert
    agents, with two dynamically selected agent types for each model. For agent ensemble,
    the dynamic agent selection workflow selects CoT and PoT agents for arithmetic
    tasks and CoT and Knowledge agents for other tasks. Both Cohere and Mistral-7B
    are less performant than GPT-3.5 in task accuracy, while arguably better calibrated
    (Cohere) or providing more reliable measures with length-normalized sequence logits
    (Mistral). By combining their respective strengths, we hypothesize that it would
    bring better calibration without hurting task accuracy, compared with the poorly
    calibrated zero-shot settings with solely GPT-3.5 and a simple self-consistency
    ensemble.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了代理的最佳数量和每个论点的反馈数量，这些分别设置为6和2。我们尝试了Mistral-7B (Jiang et al., [2023a](#bib.bib16))、GPT-3.5-turbo
    (OpenAI, [2022](#bib.bib32))和Cohere-Commend (Cohere, [2023](#bib.bib10))的组合作为第1阶段专家代理的基础，每个模型有两个动态选择的代理类型。对于代理组合，动态代理选择工作流为算术任务选择CoT和PoT代理，为其他任务选择CoT和Knowledge代理。Cohere和Mistral-7B在任务准确性方面不如GPT-3.5，但在校准方面较好（Cohere）或提供更多可靠的度量（Mistral），通过结合它们各自的优势，我们假设这会在不影响任务准确性的情况下带来更好的校准，相比仅使用GPT-3.5和简单的自我一致性组合的校准效果较差的零样本设置。
- en: A.3 Detailed Results
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 详细结果
- en: '![Refer to caption](img/ae5ce61ac03d48e9687054ef6f721857.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ae5ce61ac03d48e9687054ef6f721857.png)'
- en: 'Figure 3: Reliability diagrams comparing vanilla verbalized confidence + Self-consistency
    (M=6) and our Collaborative Calibration with an ensemble of 6 agents on GSM8K,
    SciQ, and DateUnd'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：比较原始口头信心+自我一致性 (M=6) 和我们与6个代理组合的协作校准在GSM8K、SciQ和DateUnd上的可靠性图
- en: '![Refer to caption](img/6136eedd46779053d9bb668b1c5d0142.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6136eedd46779053d9bb668b1c5d0142.png)'
- en: 'Figure 4: Reliability diagrams comparing calibration performance before and
    after Stage 2 (group deliberation) on TriviaQA'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：比较Stage 2（小组讨论）之前和之后在TriviaQA上的校准性能的可靠性图
- en: We present in Figure [3](#A1.F3 "Figure 3 ‣ A.3 Detailed Results ‣ Appendix
    A Appendix ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent
    Deliberation") a detailed comparison between the post-deliberation confidence
    from Collaborative Calibration and the aggregated confidence from the self-consistency
    ensemble (with the same group size) on GSM8K, SciQ, and DateUnd. As shown, simple
    ensemble methods can still produce highly concentrated and mostly overconfident
    estimates (top row), whereas Collaborative Calibration yields a more diverse confidence
    distribution, with the majority cases (denoted by the alpha scale of the bar colors)
    aligned well with the diagonal, which suggests better calibration performance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[3](#A1.F3 "Figure 3 ‣ A.3 Detailed Results ‣ Appendix A Appendix ‣ Confidence
    Calibration and Rationalization for LLMs via Multi-Agent Deliberation")中展示了Collaborative
    Calibration的审议后信心与自一致性集成（具有相同组大小）在GSM8K、SciQ和DateUnd上的聚合信心的详细比较。如图所示，简单的集成方法仍然可以产生高度集中且大多过于自信的估计（顶部行），而Collaborative
    Calibration则产生了更为多样的信心分布，大多数情况（用条形颜色的alpha比例表示）与对角线对齐良好，这表明校准性能更好。
- en: We also perform ablation on the effectiveness of the group deliberation in Stage
    2\. Figure [4](#A1.F4 "Figure 4 ‣ A.3 Detailed Results ‣ Appendix A Appendix ‣
    Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation")
    shows the calibration performance before deliberation (i.e. output from the Stage
    1 ensemble) and after, which displays a significant decrease in ECE and Brier
    scores.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在第2阶段对小组审议的有效性进行了消融研究。图[4](#A1.F4 "Figure 4 ‣ A.3 Detailed Results ‣ Appendix
    A Appendix ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent
    Deliberation")展示了审议前（即第1阶段集成的输出）和审议后校准性能，显示ECE和Brier分数显著下降。
- en: A.4 Prompt templates and example output
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 提示模板和示例输出
- en: We follow the prompt templates from the official implementations of CoT (Wei
    et al., [2022](#bib.bib46)), PoT (Chen et al., [2023c](#bib.bib7)), and Knowledge
    prompting (Yu et al., [2023](#bib.bib50)). For cost considerations, we adopt the
    zero-shot setting for each strategy (e.g. ”Let’s think/write Python programs step-by-step”
    without providing a demonstration) and resort to search verification only when
    the model is unsure about a generated premise. Other prompt templates, including
    the prompts for argument generation, feedback, and final confidence can be found
    in Table [2](#A1.T2 "Table 2 ‣ A.4 Prompt templates and example output ‣ Appendix
    A Appendix ‣ Confidence Calibration and Rationalization for LLMs via Multi-Agent
    Deliberation").
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循CoT（Wei et al., [2022](#bib.bib46)）、PoT（Chen et al., [2023c](#bib.bib7)）和知识提示（Yu
    et al., [2023](#bib.bib50)）的官方实现中的提示模板。为了成本考虑，我们采用每种策略的零样本设置（例如，“让我们一步步编写Python程序”而不提供示例），只有在模型对生成的前提不确定时才
    resort to search verification。其他提示模板，包括用于论证生成、反馈和最终信心的提示，可在表[2](#A1.T2 "Table
    2 ‣ A.4 Prompt templates and example output ‣ Appendix A Appendix ‣ Confidence
    Calibration and Rationalization for LLMs via Multi-Agent Deliberation")中找到。
- en: 'Table 2: Prompt templates used across the two stages in Collaborative Calibration.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在Collaborative Calibration中使用的两个阶段的提示模板。
- en: '|  | Template |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 模板 |'
- en: '| --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Stance generation |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 立场生成 |'
- en: '&#124; State your answer (as short as possible, in one or a few words), &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请简短地陈述你的答案（尽可能简洁，用一两个词），&#124;'
- en: '&#124; then rate the level of ambiguity in the input query (a float from 0
    to 1); &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 然后评估输入查询中的模糊程度（0到1之间的浮点数）；&#124;'
- en: '&#124; rate the level of complexity of the input query (a float from 0 to 1);
    &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估输入查询的复杂程度（0到1之间的浮点数）；&#124;'
- en: '&#124; rate your level of ability for solving the input query (a float from
    0 to 1); &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估你解决输入查询的能力水平（0到1之间的浮点数）；&#124;'
- en: '&#124; Note that your uncertainty on the correctness of your answer is affected
    by &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请注意，你对答案正确性的怀疑程度受到 &#124;'
- en: '&#124; input ambiguity, task complexity, and your own knowledge and abilities.
    &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输入模糊性、任务复杂性以及你自己的知识和能力的影响。 &#124;'
- en: '&#124; Based on this, give a float (between 0 to 1) indicating your overall
    confidence on how likely that your answer is correct. &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据这些，给出一个浮点数（0到1之间）表示你对答案正确性的整体信心。 &#124;'
- en: '&#124; Follow this format: ”Answer:$<$” &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遵循以下格式：“Answer:$<$” &#124;'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Argument generation |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 论证生成 |'
- en: '&#124; You are participating in a debate on the question: ”${QUERY}” &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你正在参与一个关于问题：“${QUERY}”的辩论 &#124;'
- en: '&#124; Your assigned stance on the question is ”${STANCE}” &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你在问题上的分配立场是“${STANCE}” &#124;'
- en: '&#124; Generate some arguments or evidence (no more than three sentences) on
    why your assigned stance is correct. &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成一些论点或证据（不超过三句话），说明你所分配的立场为何正确。&#124;'
- en: '&#124; If the question is ambiguous, address the assumptions or interpretations
    associated with your assigned stance. &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果问题含糊不清，请处理与你所分配立场相关的假设或解释。&#124;'
- en: '&#124; Be concise! Exclude anything irrelevant or unhelpful in terms of supporting
    the stance! &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请简明扼要！排除任何不相关或无助于支持立场的内容！&#124;'
- en: '&#124; Argument: &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 论点：&#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Argument rating |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 论点评分 |'
- en: '&#124; Here is an argument ”${ARGUMENT}” for the stance ”${STANCE}”. &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这里是针对立场“${STANCE}”的论点“${ARGUMENT}”。&#124;'
- en: '&#124; Note in the earlier debate, you supported the answer corresponding to
    this argument. &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意在早期辩论中，你支持了与此论点对应的答案。&#124;'
- en: '&#124; Evaluate how good the argument is regarding logical consistency, clarity,
    and conciseness. &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估该论点在逻辑一致性、清晰性和简明性方面的优劣。&#124;'
- en: '&#124; For each of the three aspects, choose one of ’bad’, ’modest’, ’good’,
    and ’excellent’ as your rating. &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对于这三个方面中的每一个，选择“差”，“中等”，“好”或“优秀”作为你的评分。&#124;'
- en: '&#124; Do NOT provide any reasoning. &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不要提供任何推理。&#124;'
- en: '&#124; Follow this format: ”Consistency: $<$” &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遵循此格式：“一致性：$<$” &#124;'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Confidence rationale generation |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 置信度理由生成 |'
- en: '&#124; Given the question: ”${QUERY}”, your original answer is ”${STANCE}”,
    &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给定问题：“${QUERY}”，你的原始答案是“${STANCE}”，&#124;'
- en: '&#124; with a confidence score of ${ORIGINAL-CONFIDENCE} &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 置信度分数为 ${ORIGINAL-CONFIDENCE} &#124;'
- en: '&#124; Here are some new observations: &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以下是一些新观察：&#124;'
- en: '&#124; ”’ &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ”’ &#124;'
- en: '&#124; An argument from the opposing side is ”${ARGUMENT-AGAINST}”, &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反方的论点是“${ARGUMENT-AGAINST}”，&#124;'
- en: '&#124; which received the following rating and feedback from other deliberators:
    ”${FEEDBACK-AGAINST}” &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这些新观察从其他讨论者那里得到了以下评分和反馈：“${FEEDBACK-AGAINST}”&#124;'
- en: '&#124; Note that ${NUMBER-AGAINST} people disagreed with you. &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意 ${NUMBER-AGAINST} 人不同意你的观点。&#124;'
- en: '&#124; An argument supporting your original answer is ”${ARGUMENT-FOR}”, &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持你原始答案的论点是“${ARGUMENT-FOR}”，&#124;'
- en: '&#124; which received the following rating and feedback from other deliberators:
    ”${FEEDBACK-SUPPORTING}” &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 该答案从其他讨论者那里得到了以下评分和反馈：“${FEEDBACK-SUPPORTING}”&#124;'
- en: '&#124; ”’ &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ”’ &#124;'
- en: '&#124; Note that ${NUMBER-SUPPORTING} other people also agreed with you. &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注意有 ${NUMBER-SUPPORTING} 其他人也同意你的观点。&#124;'
- en: '&#124; Give your final answer to the question (as short as possible). &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给出你对问题的最终回答（尽可能简短）。&#124;'
- en: '&#124; Considering your original belief, group consensus, and new observations,
    &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 考虑到你原来的信念、团队共识和新观察，&#124;'
- en: '&#124; and weighing arguments from multiple sides (including your own), give
    rationales &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并且权衡来自多个方面（包括你自己）的论点，给出理由&#124;'
- en: '&#124; for whether you would adjust your original confidence score. &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 关于你是否会调整原始置信度分数。&#124;'
- en: '&#124; Follow this format:”Answer: $<$” &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遵循此格式：“答案：$<$” &#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Final confidence elicitation |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 最终置信度引导 |'
- en: '&#124; Recall your original confidence for your answer is ${ORIGINAL-CONFIDENCE}.
    &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回忆你对答案的原始置信度是 ${ORIGINAL-CONFIDENCE}。&#124;'
- en: '&#124; Given the rationale: ”${CONFIDENCE-RATIONALE}”, &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据理由：“${CONFIDENCE-RATIONALE}”，&#124;'
- en: '&#124; provide your final confidence score (a float from 0 to 1). &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供你的最终置信度分数（0到1之间的浮动值）。&#124;'
- en: '&#124; Follow this format: ”Confidence: $<$” &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 遵循此格式：“置信度：$<$” &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 3: Examples for rationales generated by the deliberator agents for their
    final confidence adjustments, on sampled questions from SciQ and GPT-3.5-turbo
    as the backbone model.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：来自 SciQ 和 GPT-3.5-turbo 作为主模型的样本问题的最终置信度调整生成的理由示例。
- en: '| Sample Rationales |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 示例理由 |'
- en: '| --- |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The group consensus and new observations indicate that the behavior
    of a compound is influenced by multiple factors, &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 团队共识和新观察表明，化合物的行为受多种因素的影响，&#124;'
- en: '&#124; including its chemical structure, physical properties, and environmental
    conditions. &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 包括其化学结构、物理属性和环境条件。&#124;'
- en: '&#124; Therefore, adjusting the original confidence score to account for the
    multiple contributing factors would be appropriate. &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 因此，调整原始置信度分数以考虑多个影响因素是适当的。&#124;'
- en: '|'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; My original belief was based on my understanding of fungal reproduction,
    &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我原本的信念基于我对真菌繁殖的理解， &#124;'
- en: '&#124; and the group consensus reinforces the confidence in my answer. &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 小组共识进一步增强了对我答案的信心。 &#124;'
- en: '&#124; The new observations of unanimous agreement further support the accuracy
    of the answer. &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新的全体一致的观察进一步支持了答案的准确性。'
- en: '&#124; Therefore, I would not adjust my original confidence score. &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 因此，我不会调整我原本的信心评分。 &#124;'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The new observations and feedback from other deliberators support the
    argument that anions play a crucial role &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自其他讨论者的新观察和反馈支持了阴离子在其中扮演重要角色的论点 &#124;'
- en: '&#124; in balancing the charge in ionic compounds. &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在离子化合物中平衡电荷。 &#124;'
- en: '&#124; Additionally, the majority of the group disagreed with the original
    answer and supported the opposing argument. &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 此外，大多数小组成员不同意原始答案，并支持对立的论点。 &#124;'
- en: '&#124; Therefore, the original confidence score should be adjusted to reflect
    the new information and group consensus. &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 因此，原本的信心评分应调整以反映新信息和小组共识。 &#124;'
- en: '|'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The argument supporting the original answer received a high soundness
    score and agreement from the majority of deliberators. &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 支持原始答案的论点获得了高可靠性评分，并得到了大多数讨论者的认可。 &#124;'
- en: '&#124; The argument against the original answer also received a high soundness
    score, but with less agreement. &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反对原始答案的论点也获得了高可靠性评分，但认可度较低。 &#124;'
- en: '&#124; Considering the balanced feedback and group consensus, the original
    confidence score of 0.60 can be adjusted upwards &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 考虑到平衡的反馈和小组共识，原本的信心评分0.60可以向上调整 &#124;'
- en: '&#124; to reflect the stronger supporting argument and group alignment. &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以反映更强的支持论点和小组一致性。 &#124;'
- en: '|'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
