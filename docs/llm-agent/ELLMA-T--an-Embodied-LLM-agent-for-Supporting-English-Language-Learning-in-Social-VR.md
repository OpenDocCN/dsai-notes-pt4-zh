<!--yml
category: 未分类
date: 2025-01-11 12:11:32
-->

# ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR

> 来源：[https://arxiv.org/html/2410.02406/](https://arxiv.org/html/2410.02406/)

\quotingsetup

leftmargin=1.5em, rightmargin=1.5em, vskip=0pt

Mengxu Pan [pan.meng@northeastern.edu](mailto:pan.meng@northeastern.edu) Khoury College of Computer Science, Northeastern UniversityVancouverCanada ,  Alexandra Kitson [akitson@sfu.ca](mailto:akitson@sfu.ca) [0003-3479-5297](https://orcid.org/0003-3479-5297 "ORCID identifier") Simon Fraser UniversityVancouverBCCanada ,  Hongyu Wan [wan.hongy@northeastern.edu](mailto:wan.hongy@northeastern.edu) Khoury College of Computer Science, Northeastern UniversityVancouverCanada  and  Mirjana Prpa [m.prpa@northeastern.edu](mailto:m.prpa@northeastern.edu) Khoury College of Computer Science, Northeastern UniversityVancouverCanada

###### Abstract.

Many people struggle with learning a new language, with traditional tools falling short in providing contextualized learning tailored to each learner’s needs. The recent development of large language models (LLMs) and embodied conversational agents (ECAs) in social virtual reality (VR) provide new opportunities to practice language learning in a contextualized and naturalistic way that takes into account the learner’s language level and needs. To explore this opportunity, we developed ELLMA-T, an ECA that leverages an LLM (GPT-4) and situated learning framework for supporting learning English language in social VR (VRChat). Drawing on qualitative interviews (N=12), we reveal the potential of ELLMA-T to generate realistic, believable and context-specific role plays for agent-learner interaction in VR, and LLM’s capability to provide initial language assessment and continuous feedback to learners. We provide five design implications for the future development of LLM-based language agents in social VR.

Embodied Conversational Agents, Social Virtual Reality, Large Language Models, Language Learning^†^†conference: ArXiv Preprint; September 2024;![Refer to caption](img/921d33ba6529d5eeb8b682cc6c893ccd.png)

Figure 1\. a) ELLMA-T, the GPT-4 based language tutor in VRChat; b) b) ELLMA-T greeting the user in their native language; c) Role-play scenario with ELLMA-T and the user in a supermarket setting; d) User asking ELLMA-T for word explanations.

\Description

A teaser figure shows the introduction of ELLMA-T, a GPT-4 based ECA in VRchat in a role-play at a supermarket scenario.

## 1\. Introduction

Acquiring proficiency in English as an additional language is essential for many facets of life. This applies to both countries where English is taught as part of the compulsory curriculum, even though it is not the native language, and to English-speaking countries where different people, such as immigrants, visitors, tourists or international students require additional support to practice and improve their English skills (Adjagbodjou and Kaufman, [2024](https://arxiv.org/html/2410.02406v1#bib.bib2)). Commonly used learning approaches administered in traditional classrooms often fall short of providing a contextualized learning environment, tailored learning materials, and engaging experiences to support learning.

Besides commonly used language learning tools (e.g., mobile applications such as Duolingo (Duolingo, [2023](https://arxiv.org/html/2410.02406v1#bib.bib25))), the recent proliferation of social Virtual Reality (VR) platforms presents new learning channels. VRChat (VRChat, [2024b](https://arxiv.org/html/2410.02406v1#bib.bib72)) has become a popular social platform for online interactions in a virtual environment with over 48k daily active users and 8̃.8 million total. Though not specifically designed for foreign language learning, players take advantage of its immersive environment to have authentic conversation experiences with native speakers (Merle_Meruru555, [2024](https://arxiv.org/html/2410.02406v1#bib.bib52)). Previous research has shown social interactions in VR to be positive and helpful for learning a foreign language (Peterson, [2012](https://arxiv.org/html/2410.02406v1#bib.bib58); Cho et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib17)). Meanwhile, players encounter numerous obstacles when undertaking such endeavors. For example, it can be difficult and time-consuming to find a native speaker to interact with; and even when found, native speakers may not be willing or suitable to act as language tutors. Moreover, users may experience increased social anxiety during the search and interaction and overall the learning experience is deemed to be unsystematic (Roy and Aimee, [2020](https://arxiv.org/html/2410.02406v1#bib.bib63); Jouzu Juls, [2022](https://arxiv.org/html/2410.02406v1#bib.bib35)).

The recent emergence of LLM-based embodied conversational agents (ECA) in social VR, such as Celeste AI (ko fi.com, [2024](https://arxiv.org/html/2410.02406v1#bib.bib40)), has received significant attention from VRChat users because they converse and behave in human-like ways. This presents an immediate opportunity to leverage LLM-agents for language learning in social VR. While LLMs have been explored for various educational purposes (Kasneci et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib37)), their application in language learning, especially in social VR contexts, remains underexplored. To that end, we present our efforts to answer the following research question (RQ): How can we leverage situated learning and an LLM-based ECA for language learning in social VR?

We built ELLMA-T, an embodied LLM-agent to explore the potential of using LLMs and situated learning framework (Brown et al., [1989](https://arxiv.org/html/2410.02406v1#bib.bib8); Lave and Wenger, [1991](https://arxiv.org/html/2410.02406v1#bib.bib44)) to facilitate language learning in social VR and conducted a user study (N=12) with ELLMA-T using semi-structured interviews. By leveraging an LLM-based agent in VRChat, the system aims to provide an authentic conversation experience and personalized feedback to users, enhancing the contextualized language learning experience. The contributions we present in this paper are the following:

1.  (1)

    Design of ELLMA-T, an embodied LLM-agent tasked to help adult language learners from A1 to C1 level practice spoken English, by simulating an immersive real-life conversation within social VR;

2.  (2)

    A user study and findings about participants’ perception of ELLMA-T’s capabilities and limitations on four English learning tasks: language level assessment, role-play dialogue generation, feedback generation, and scaffolding abilities, contributing to the larger picture of LLM-assisted language learning and LLM’s applications;

3.  (3)

    Design implications for designing embodied LLM-agents in social VR context for language learning.

Our findings from a preliminary user study demonstrate ELLMA-T’s potential as a ’human-like’ language tutor for situated language learning. However, challenges such as conversation flow disruptions and insufficient emotional support were identified, suggesting areas for future improvement. The study’s design implications extend to the design of personalized, culturally relevant, and adaptive learning systems that integrate verbal and non-verbal cues. These findings highlight the potential for further exploration into developing LLM-based ECAs in social VR for long-term language learning.

## 2\. Related Work

### 2.1\. Supporting Situated Language Learning in VR

Situated learning theory is grounded in the understanding that the most effective learning happens in context. Learning can neither be separated from the specific situations in which it occurs nor be transferred or applied fully in a school setting (Brown et al., [1989](https://arxiv.org/html/2410.02406v1#bib.bib8); Lave and Wenger, [1991](https://arxiv.org/html/2410.02406v1#bib.bib44)). In language acquisition, an authentic context provides learning opportunities that closely simulate real-life situations and includes cultural nuances crucial for meaningful and engaging language use (Lan, [2020](https://arxiv.org/html/2410.02406v1#bib.bib43)), for example, discerning the nuances and subtleties of word usage in polysemous situations (Wong et al., [2018](https://arxiv.org/html/2410.02406v1#bib.bib76)).

The immersive nature of VR is particularly suitable for creating a realistic language learning environment and simulating interpersonal interactions in context-rich scenarios. Since 2013 and the expansion of affordable head-mounted displays (Dhimolea et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib22); Limniou et al., [2008](https://arxiv.org/html/2410.02406v1#bib.bib48)), VR has been widely utilized in many areas of training and education (Aïm et al., [2016](https://arxiv.org/html/2410.02406v1#bib.bib5); Zhu et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib86)). Within VR, 3D objects and avatars have been used to facilitate embodied and contextualized learning experiences (Chatain et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib13)). With rapid advancements in VR headsets, learners experience a heightened sense of presence, increased agency, and embodiment that support learning comparable to learning-by-doing in the physical world (Kaplan-Rakowski and Gruber, [2021](https://arxiv.org/html/2410.02406v1#bib.bib36)).

A growing body of literature has investigated the usage of VR for English as foreign language (EFL) learning, specifically how VR environments can be used for different language learning tasks (Lin and Lan, [2015](https://arxiv.org/html/2410.02406v1#bib.bib49); Klimova, [2021](https://arxiv.org/html/2410.02406v1#bib.bib39); Dhimolea et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib22)). Research indicates that VR is beneficial for vocabulary acquisition, communication skills, and writing skills (Lan, [2020](https://arxiv.org/html/2410.02406v1#bib.bib43)). Other benefits of using VR for EFL learning include increased learners’ motivation, engagement, enjoyment, automation (Makransky and Lilleholt, [2018](https://arxiv.org/html/2410.02406v1#bib.bib51); Lan, [2020](https://arxiv.org/html/2410.02406v1#bib.bib43); Dhimolea et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib22)), and reduced anxiety (Gruber and Kaplan-Rakowski, [2020](https://arxiv.org/html/2410.02406v1#bib.bib27)). EFL learning using VR can also benefit kids with complex needs and low-achieving students (Lan, [2020](https://arxiv.org/html/2410.02406v1#bib.bib43)).

Researchers have generated several strategies to enhance learning in VR by contextualization and extending memory retention. These strategies include interacting with a 3D object for vocabulary understanding (Fuhrman et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib26); Hsu et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib30)), completing tasks in real-life scenarios (Wik and Hjalmarsson, [2009](https://arxiv.org/html/2410.02406v1#bib.bib75)), problem-based learning (Chen et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib14)), and integrating learning into narrative or through gamification (Cheng et al., [2017](https://arxiv.org/html/2410.02406v1#bib.bib15); Hsu et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib30); Seow, [2023](https://arxiv.org/html/2410.02406v1#bib.bib65)). In addition, the use of avatars in VR has been shown to support human-to-human collaboration, and enhance a sense of community during the learning experience (Jackson and Fagan, [2000](https://arxiv.org/html/2410.02406v1#bib.bib32); Peterson, [2012](https://arxiv.org/html/2410.02406v1#bib.bib58); Cantone et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib11)). The use of avatars may in particular support often overlooked nuances of learning languages such as the role of gestures that accompany spoken language. For example, (Cheng et al., [2017](https://arxiv.org/html/2410.02406v1#bib.bib15)) showed how language usage and body movement are integrated to teach learners intercultural nuances, such as the practice of greeting in Japanese while simultaneously bowing, demonstrating how verbal and non-verbal cues work together in communication.

### 2.2\. Embodied Conversational Agents for Language Learning

Many VR language learning applications involve embodied conversational agents (ECAs) in the environment. An ECA is a digitally animated character capable of engaging in natural, human-like communication with users (Cassell et al., [2000](https://arxiv.org/html/2410.02406v1#bib.bib12)). As Cassell states, an ideal ECA is described as being able to conduct a human-to-agent conversation in a way that mimics human face-to-face communication so that it feels believable. Many other features have been discussed for the design of the ECAs such as the use of verbal and non-verbal cues, multimodal communication, appearance and visualization (Cassell et al., [2000](https://arxiv.org/html/2410.02406v1#bib.bib12)), and carefully crafted personality traits (van Mulken et al., [1998](https://arxiv.org/html/2410.02406v1#bib.bib70)), all aiming to increase agent credibility. In recent studies, ECAs are increasingly considered as social agents (Atkinson et al., [2005](https://arxiv.org/html/2410.02406v1#bib.bib3)), designed not only to interact but also to understand and adapt to the social and cultural contexts of the user.

Recent studies have noted that ECAs, despite their potential, often fall under the broader category of chatbots, which lack a unified conceptual framework and typically do not include a virtual body or multimodal processing abilities (Jeon et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib34)). Meanwhile, much research has focused on the design and use of conventional chatbots (no embodiment) for language learning, but the exploration of ECAs remains limited. Early efforts, such as those by Wik and Hjalmarsson ([2009](https://arxiv.org/html/2410.02406v1#bib.bib75)), employed humanoid agents to perform different language learning tasks, demonstrating success across diverse cultural backgrounds. Divekar* et al. ([2022](https://arxiv.org/html/2410.02406v1#bib.bib24)) also used humanoid agents to mimic real-life scenarios, specifically non-dyadic (multiple-people) multi-modal conversations. More recent studies have shown that learners can develop strong emotional bonds with ECAs, leading to increased motivation and positive learning outcomes (Jeon, [2022](https://arxiv.org/html/2410.02406v1#bib.bib33)). (Xu et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib79)) demonstrating also a positive correlation between learners’ strong affection for agents’ appearances and improved learning outcomes.

Despite the potential of ECAs for simulating human-like behaviour, much of the existing research on ECAs for language learning features ECAs with minimal interactive abilities (Wik and Hjalmarsson, [2009](https://arxiv.org/html/2410.02406v1#bib.bib75); Divekar* et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib24); Jeon, [2022](https://arxiv.org/html/2410.02406v1#bib.bib33)). These ECAs are often restricted to basic functions like lip-syncing, and limited non-verbal gestures and expressive behaviour due to technological constraints. The recent advancements in large language models (LLMs) provide new technological capabilities for ECAs, opening up design opportunities for more dynamic, interactive ECAs.

### 2.3\. Potential of LLMs & LLM-agents for Language Learning in Social VR

Previous natural language processing (NLP) techniques had been applied to English language learning tools, which usually require templates or use pre-scripted goal-oriented scenes (Tyen et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib68)). Previous (E)CAs’ responses regarding language learning and feedback also remained limited (Zhang et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib84)). This limitation was lifted with the recent development of LLMs and their content generation ability that extends beyond language tasks (Yang et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib80)). In addition to language generation tasks, LLMs can be leveraged for generating ECAs’ 3D avatars (Hong et al., [2022](https://arxiv.org/html/2410.02406v1#bib.bib28)), controlling motion (Qing et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib61)), generating motion and facial expressions (Normoyle et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib53)), and driving ECA’s behavior (Normoyle et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib53); Kumaran et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib42)). Multimodal LLMs have the potential to understand the virtual environment and the objects in it (Wang et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib74)), for even more contextualized learning.

While the number of discussions around the usage of LLMs is growing, particularly chatGPT for education across disciplines (Kasneci et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib37)), fewer studies focus on the use of LLMs for language learning. Previous studies have explored the use of LLM-based chatbots in academic settings for EFL for tasks like grammar correction and essay writing (Zhang et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib83); Law, [2024](https://arxiv.org/html/2410.02406v1#bib.bib45)). In a recent study, (Caines et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib10)) examined the use of LLMs in language teaching and assessment, including the assessment of generated content, calibration of existing teaching materials, automated assessment of language learners, and providing feedback. (Caines et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib10)) review of LLMs besides GPT suggests that alternative models for content generation may be a better choice due to their multilingual capabilities and their general availability as open-source models; yet, for other tasks like providing feedback to language learners, the usage of GPT models is still promising.

Despite the advancements of LLMs, the use of LLM-based ECAs in language learning applications remains relatively underexplored. Recent studies have begun to explore how personality expression and embodiment in LLM-based educational agents impact learning outcomes (Sonlu et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib67)). Yet, there remains a gap in understanding the complexity of factors that influence LLM-based agent-supported language learning in social VR. To address this gap, we explore the design and integration of LLM-based ECAs into social VR for contextualized language learning.

## 3\. System Design

Our system implements an LLM-based agent to assist learners in practicing spoken English within a social VR environment. The agent (ELLMA-T) engages with the users in role-play conversations based on real-life scenarios, providing interactive and adaptive language practice in VRChat. In the proceeding section, we discuss our design principles based on education theory, learning task design, and system implementation.

### 3.1\. Design Principles

We based the following system design principles on prior research involving additional language learning regardless of the learner’s age or academic status:

1.  (1)

    Student-centered learning, with the system designed to fit each learner’s unique needs (Darsih, [2018](https://arxiv.org/html/2410.02406v1#bib.bib21));

2.  (2)

    The learning material follows task-oriented learning, meaning the learner needs to complete a learning-related task rather than engaging in non-task-based casual conversations (Raux and Eskenazi, [2004](https://arxiv.org/html/2410.02406v1#bib.bib62));

3.  (3)

    The system focuses on providing emotional support to encourage learning, a major factor in sustaining continuous learning for language learners (Dinçer et al., [2013](https://arxiv.org/html/2410.02406v1#bib.bib23)).

We build on the needs for supporting adult English language learning put forth by Adjagbodjou and Kaufman ([2024](https://arxiv.org/html/2410.02406v1#bib.bib2)): (a) daily conversational needs that are contextualized, (b) tools that address both linguistic and emotional needs, and (c) long-term, contextually situated learning experiences. Though this system is designed with adult learners’ needs in mind, our goal is to scale the system for different user groups of various ages. First, we emphasize creating an immersive environment with an embodied agent in VR to simulate real-life conversation experiences, addressing need (a). Second, we explore different communication strategies and channels that address learners’ emotional needs using LLMs’ language abilities, addressing need (b). Third, we use VRChat as a free and easily accessible open social platform, which provides a rich space for users to connect, form communities, and potentially pursue collaborative learning opportunities, addressing need (c).

### 3.2\. Learning English with ELLMA-T in VRChat

![Refer to caption](img/5d393905907c95d0fd52063355c3dfd1.png)

Figure 2\. Workflow of conversation tasks performed by ELLMA-T, including greeting the user, conducting language assessments, engaging in role-play scenarios, and providing feedback.

\Description

This flowchart illustrates the workflow and tasks executed by the LLM-based system for language learning.

The flowchart begins with the ”Introduction” step. Then, the second step, ”Language Level Assessment”. Then the system moves to the ”Role-Play” stage. Here, the flow branches into two paths: user-specified scenarios or generated scenarios. Both branches lead to the ”Take Turns Conversations” step. Following the conversation, the system provides ”Feedback” to the user.

The agent and user interaction is shown through a double-headed arrow indicating continuous engagement between the agent and user throughout the process. Role-Play: This step involves real-life conversation practice, in-context vocabulary explanation, and pronunciation and grammar correction. Scaffolding: This involves intent clarification, verbal encouragement, and example provision to support the user during the conversation.

Color Coding: Steps highlighted in yellow indicate that they are generated by the LLM on the fly. The right side of the flowchart is color-coded to differentiate between the Role-Play and Scaffolding tasks, each with its sub-tasks.

The workflow (see Fig. [2](https://arxiv.org/html/2410.02406v1#S3.F2 "Figure 2 ‣ 3.2\. Learning English with ELLMA-T in VRChat ‣ 3\. System Design ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")) starts with ELLMA-T asking for the learner’s information and assessing their language level. ELLMA-T then generates a conversation scenario specified by the learner or offers three scenarios for the learner to choose from. During role-play conversation practices, ELLMA-T acts as the language practice partner, playing the character in the scene and providing scaffolding when needed. After the role-play, ELLMA-T provides feedback to the learner based on the conversation in the role-play. The learner can end the conversation or switch to another role-play at any point in the conversation. Informed by our three design principles, we designed ELLMA-T’s language level assessment, conversation learning materials, and communication strategy, which were generated by the LLM on the fly using prompt engineering (see Section [3.3](https://arxiv.org/html/2410.02406v1#S3.SS3 "3.3\. System Implementation ‣ 3\. System Design ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")).

#### 3.2.1\. Design Principle #1: Introduction and Language Level Assessment

Introduction: ELLMA-T initiates the conversation by introducing itself as a friendly and culturally aware language partner. It gathers information from the user through open-ended questions about their name, cultural background, motivation to learn English, and any additional details they wish to share to form a sense of personal connection. For example, ELLMA-T greets the learner in their native language and acknowledges their culture and personal experiences. During the first few rounds of conversations, the learner gets familiar with the agent and the VR environment.

Language Level Assessment Task: Next, ELLMA-T gives the learner a topic to speak about to assess the learner’s current level of English proficiency. The topic and the format of the free speech assessment follow the format of an IELTS (IELTS, [2024](https://arxiv.org/html/2410.02406v1#bib.bib31)) speaking test. Whereas IELTS is a more formal conversation, in our system the assessment is a casual conversation. In the following learning session, ELLMA-T adjusts its language and topic difficulty according to the learner’s proficiency. The length of the free speech varies from 30 seconds to as long as the learner continues speaking on the topic. The agent makes the judgment if it has long enough input to assess the learner’s spoken language. If the user’s input is not long enough, the agent asks follow-up questions and encourages the learner to speak more. Once ELLMA-T decides it has enough input from the learner, it assesses the learner’s spoken language proficiency and gives a result from A1 to C2 based on CEFR criteria (COE, [2024](https://arxiv.org/html/2410.02406v1#bib.bib19)).

#### 3.2.2\. Design Principle #2: Role-Play Topic Generation and Conversation Continuation

Presenting Scenarios: Upon knowing the user’s proficiency level, ELLMA-T presents three different role-play scenarios focusing on various themes relevant to daily life. The user can choose the scenario that most interests them or suggest a specific scenario they’d like to practice. Once a scenario is chosen, ELLMA-T provides a detailed description of the scene, outlining the physical surroundings and significant objects. In VRChat, various virtual worlds replicate everyday settings like cafes, restaurants, outdoors, and supermarkets (see Fig. [3](https://arxiv.org/html/2410.02406v1#S3.F3 "Figure 3 ‣ 3.2.2\. Design Principle #2: Role-Play Topic Generation and Conversation Continuation ‣ 3.2\. Learning English with ELLMA-T in VRChat ‣ 3\. System Design ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")). Once a scenario is selected, the user and ELLMA-T can be transported into a corresponding virtual environment. ELLMA-T prompts the user to complete tasks within the scene. For example, in a supermarket setting, the user is asked to find or describe specific items and practice the vocabulary learned.

![Refer to caption](img/972f3748471939e4f9fb555405cf8fe8.png)

Figure 3\. ELLMA-T in different virtual worlds within VRChat: an indoor café (left) and an outdoor city (right).

\Description

This image consists of two side-by-side panels, both featuring ELLMA-T, the 3D cartoon avatar, in different immersive environments within VR. The avatar is the same as Figure 1.

Left Panel: The left panel shows ELLMA-T standing inside a cozy café environment. The café is designed with a rustic, brick interior, with a menu board displayed on the wall behind the counter listing various hot and cold drinks. To the left of ELLMA-T is a shelf stocked with baked goods and a wall clock hangs above it. The café ambiance is warm, with lighting that highlights the textures of the brick walls and wooden shelves. ELLMA-T is standing in front of the counter, facing the camera, and ready to assist the user in a conversational role-play scenario related to a café setting.

Right Panel: The right panel shows ELLMA-T in an outdoor environment resembling a European-style waterfront street. The scene features a stone bridge arching over a canal, with historic buildings lining the street and water visible in the background. The sky is clear and it is a sunny day. ELLMA-T stands on the cobblestone pathway near the bridge, facing the camera. This environment is for role-play scenarios where the user interacts with ELLMA-T in a street or outdoor setting, allowing for a variety of conversational topics related to urban exploration or travel.

Take Turns Conversations: ELLMA-T and the learner engage in turn-taking conversations to complete the role-play scenario. Acting as the user’s partner, the agent fosters natural and authentic dialogue within the chosen scenario. The conversation topic and task design follow the example of lesson plans of British Council English Teaching Lessons for adult learners (Council, [2024](https://arxiv.org/html/2410.02406v1#bib.bib20)). In each scenario, ELLMA-T assumes a role such as a restaurant server, cashier, doctor, or friendly New Yorker on the street, allowing the user to conduct conversations that mimic real-life situations.

Providing Feedback: After the role-play, the agent provides feedback to the user, consisting of three parts. The first part offers general feedback that evaluates the user’s performance concerning the difficulty level of the chosen scenario, highlighting one strength and one area for improvement. The second part, ”Advice Moving Forward”, provides practical tips on how to apply the concepts learned in the conversation to real-world situations. Lastly, ELLMA-T summarizes the key vocabulary, grammar, or sentence structures encountered during the interaction for the user’s future reference. ELLMA-T remains available to answer further questions or engage in additional practice sessions.

#### 3.2.3\. Design Principle #3: Communication Strategy and Back-channels

EFL learners often encounter communication breakdowns due to insufficient vocabulary or incorrect grammar usage, which can negatively affect their self-confidence and motivation. Our system builds upon the communicative and scaffolding strategies by (Ayedoun et al., [2019](https://arxiv.org/html/2410.02406v1#bib.bib4)) that may increase learner’s willingness to speak in designing Embodied Conversational Agents (ECAs) for spoken English practice.

Mitigating Communication Breakdowns: To address communication breakdowns, ELLMA-T provides encouragement to motivate the user to continue (”Come on, don’t give up!”); or suggests phrases to help the user move forward in the conversation (”Maybe you can say…”). When the user’s speech is unclear — whether due to incorrect vocabulary, grammar mistakes, or mispronunciation — ELLMA-T poses clarifying questions to prompt further dialogue (”You just said…, do you mean…?”). This approach fosters a supportive learning environment that encourages users to practice speaking without fear of immediate correction.

### 3.3\. System Implementation

#### 3.3.1\. System Architecture

![Refer to caption](img/58ea4c008c3f1a0705104d08ebb460a7.png)

Figure 4\. System Architecture of the ELLMA-T. The architecture highlights the core components and data flow within the system.

\Description

System Architecture for LLM-Based Embodied Conversational Agent in VRchat.

This diagram illustrates the data flow between different system components enabling a conversational agent in VRchat. User Input: Spoken input from the user is captured via a VR headset. Whisper + Soundlibrary: Converts the audio input into text. LLM (Large Language Model): Processes the text and generates a response, guided by a Prompt. Memory Module: Includes Short-term and Long-term Memory to store interaction history for context. TTS (Text-to-Speech): Converts the LLM-generated text into speech. VRChat OSC + Unity: Manages the agent’s expressive behaviors, such as Facial Expressions, Body Movements, and Filler Words in VRchat. Voice Output: The final speech output and expressions are delivered to the user.

The agent design in this project extended the work by (Wan et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib73)) which implements general ECA in VRChat and improves the response context-relevance by adding a memory to the agent. Based on this implementation, we added a tutor workflow logic (see Fig. [2](https://arxiv.org/html/2410.02406v1#S3.F2 "Figure 2 ‣ 3.2\. Learning English with ELLMA-T in VRChat ‣ 3\. System Design ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")) that controls the conversation flow.

The learner controls an avatar in VRChat through a Quest 3 headset, able to make body movements and talk to ELLMA-T with the microphone. As illustrated in Fig. [4](https://arxiv.org/html/2410.02406v1#S3.F4 "Figure 4 ‣ 3.3.1\. System Architecture ‣ 3.3\. System Implementation ‣ 3\. System Design ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), the audio input is converted into text by calling Whisper API (OpenAI, [2024c](https://arxiv.org/html/2410.02406v1#bib.bib56)), and then a text reply is generated from the GPT4 model by calling OpenAI API. The text response is converted back to Audio using the OpenAI TTS service (the ’alloy’ voice) (OpenAI, [2024b](https://arxiv.org/html/2410.02406v1#bib.bib55)). The user speech detection and silence detection are done by using Sounddevice Library (Python_Sounddevice, [2024](https://arxiv.org/html/2410.02406v1#bib.bib59)) and Vosk (Python_Vosk, [2024](https://arxiv.org/html/2410.02406v1#bib.bib60)). We use a 2.0-second silence threshold to detect the end of the user’s input, balancing the buffer time for users to pause and continue speaking with the overall wait time for the agent’s response.

The system can also generate facial expressions and body gestures based on the user’s input. The LLM detects the user’s emotion by keyword detection and then mirrors that emotion by mapping corresponding body movements and facial expressions. The history of the current conversation session is stored in a list of JASON objects for the agent as the short-term memory and reference to generate more relevant conversations. The key information of the current conversation session is summarized by LLM and stored in a remote database. The back end of the agent is integrated into VRChat through VRChat OSC (VRChat, [2024a](https://arxiv.org/html/2410.02406v1#bib.bib71)). Additional animation for agent gestures and facial expressions can be added to Unity (Unity, [2024](https://arxiv.org/html/2410.02406v1#bib.bib69)) and then incorporated into VRChat through VRChat OSC (VRChat, [2024a](https://arxiv.org/html/2410.02406v1#bib.bib71)).

#### 3.3.2\. Multi-task Multi-turn Conversation System

In designing and implementing a multi-task, multi-turn conversational agent, we encountered a challenge that necessitated a shift from a single-prompt approach to a more structured, multi-task approach. Our agent was tasked with managing an entire conversation involving multiple steps and decision points, for example, when to move from the introduction to the assessment task, how long the assessment should be, how to move on to the next topic, etc., within an interaction session. Initially, we experimented with a single-prompt approach, wherein the system prompt, we listed out all the steps and the criteria the agent should follow as system prompt in Appendix [A.1](https://arxiv.org/html/2410.02406v1#A1.SS1 "A.1\. Single System Prompt ‣ Appendix A Appendix ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"). However, we observed that the LLM struggled to follow the steps in the single prompt. The LLM often confused the sequence of actions, misinterpreted when one task should conclude, and when to transition to the next, or randomly restarted a certain task. This led to a breakdown in the logical flow of conversations, thus not ideal for users to interact with. This observation aligns with emerging findings in the field of LLM-based agents, where maintaining context and managing multi-turn interactions remain challenging, particularly in complex, multi-step tasks (Yi et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib81)).

![Refer to caption](img/1a70e79dc3a6a7061fd6e125603e0f69.png)

Figure 5\. Structure of Separate Prompts for Different Tasks. This diagram illustrates how prompts are structured and separated for various tasks within the system. 1) The system prompt establishes the agent’s persona across all interactions. 2) Task-specific prompts guide the agent during the introduction, language assessment, role-play, and feedback. 3) A decision prompt helps the agent determine when to transition between tasks. 4) The prompt for providing scaffolding during role-play conversations.

\Description

This diagram illustrates the structure and flow of prompts used by the LLM-based agent to manage various conversational tasks:

System Prompt: Sets the agent’s overall persona as a friendly, patient, and light-hearted English tutor. Task-Specific Prompts: Separate prompts are used for different tasks: Introduction: The agent engages the user to learn about their background. Assessment: The agent conducts a free chat to assess the user’s language level. Role Play: The agent guides the user through a role-playing scenario. During each task, there is a Saturation Logic that determines if the current task is complete and when to transition to the next task. During the Role Play task, there is a prompt for Scaffolding that guides the LLM to offer explanations, examples, and encouragement as needed.

As an alternative, we adopted a multi-task implementation strategy (see Fig. [5](https://arxiv.org/html/2410.02406v1#S3.F5 "Figure 5 ‣ 3.3.2\. Multi-task Multi-turn Conversation System ‣ 3.3\. System Implementation ‣ 3\. System Design ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"). The core idea behind this approach is to decompose the overall task into smaller, simpler, and more manageable subtasks. Each subtask is then handled by a separately prompted instance of the agent (see Appendix [A.2](https://arxiv.org/html/2410.02406v1#A1.SS2 "A.2\. Multi-task Prompt ‣ Appendix A Appendix ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")), allowing for greater control and reducing the cognitive load on the LLM at any given point. This strategy shifts the burden of maintaining deterministic logic from the LLM to the system designer, who predefines the sequence and structure of subtasks. By doing so, we ensure that the agent can navigate through complex interactions without losing track of the conversation’s broader context. Each subtask is executed independently, with the LLM generating responses based on the specific context of that subtask. Although this introduces some variability in the conversation due to the inherent randomness of LLM-generated text, the overall structure remains intact, leading to more coherent and effective interactions. This approach is supported by recent developments in the multi-agent system for complex task dialogue systems, which suggests that breaking down complex tasks into modular components can significantly improve system performance (Wu et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib78)).

#### 3.3.3\. Prompt Engineering

Effective prompt engineering is crucial for guiding LLMs like GPT-4 in performing specific tasks within a conversational system. While there are many ways to prompt LLMs (Schulhoff et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib64)), there is currently no standardized workflow for developing the ”perfect” prompt, as prompt design often involves a trial-and-error process (Zamfirescu-Pereira et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib82)). Our approach to prompt engineering was iterative and explored LLM’s capabilities under zero-shot or few-shot conditions (Brown et al., [2020](https://arxiv.org/html/2410.02406v1#bib.bib9)) to facilitate language learning.

## 4\. User Study

### 4.1\. Study Design

To explore participants’ experiences with ELLMA-T within the VRChat environment, we conducted a qualitative study using semi-structured interviews, and thematic analysis to analyze the data. The study aimed to explore the opportunities and challenges in LLM-based agent-supported language learning in a social VR setting. Our study focuses on overall interaction experience, preferences for system usage, and feedback for future improvements. This study was approved by the university’s ethics board.

### 4.2\. Participants and Setting

For this preliminary study, we recruited 12 international graduate students (Table [1](https://arxiv.org/html/2410.02406v1#S4.T1 "Table 1 ‣ 4.2\. Participants and Setting ‣ 4\. User Study ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")) from [University Name removed for blind review] by posting recruitment ads via campus communication channels. Despite meeting the university’s rigorous English language standards, with a minimum IELTS score of 7.5, these students continue to encounter linguistic hurdles in their day-to-day lives and work environments. To that end, our participants were selected as a potentially representative sample of a population with mixed English proficiencies who aim to enhance oral proficiency in practical scenarios, such as social interactions and workplace communication.

Table 1\. Participant Information

| Participant ID | Gender | Cultural Background | Language Proficiency | Motivation to Learn English |
| --- | --- | --- | --- | --- |
| P1 | Male | China | Fluent | To improve language skills |
| P2 | Male | India | Fluent | For work in Canada |
| P3 | Male | China | Fluent | To enhance communication skills |
| P4 | Female | China | Basic | To improve spoken English |
| P5 | Female | China | Intermediate | For work and study in Canada |
| P6 | Female | China | Intermediate | To enhance proficiency |
| P7 | Female | China | Intermediate | To improve business English skills |
| P8 | Male | Bangladesh | Very Fluent | To achieve advanced proficiency |
| P9 | Female | Taiwan | Basic | For work and study in Canada |
| P10 | Female | China | Basic | For job interview practice |
| P11 | Male | China | Intermediate | To improve business English skills |
| P12 | Male | China | Fluent | For work in Canada |

### 4.3\. Procedure

The study was conducted in person at the [University removed for blind review]. Upon arrival, participants received a brief overview of the study. They were informed that they would interact with an AI agent acting as an English language tutor using a Meta Quest 3 headset. For participants unfamiliar with VR, we provided instructions on how to wear the headset and navigate the VRChat world. The setup was pre-configured to ensure a seamless experience, allowing participants to bypass navigating menus or learning controls. Once equipped with the headset, participants were instructed to start recording their interaction with the ELLMA-T from their POV. The agent was overseen remotely by a researcher who also provided technical support. The interaction lasted 1̃5 minutes: introduction and assessment (5mins), role-play and feedback (7mins). We saved the conversation logs between each participant and ELLMA-T as CSV files for further analysis. Following the interaction, participants took part in a 30-minute semi-structured interview conducted by one of the authors experienced in qualitative methods. Interviews were audio-recorded with an iPhone 12 and transcribed in NVivo 14 for analysis. The interviews gathered qualitative data on participants’ overall experience, comfort level, engagement, frequency of potential use, and preferences for using the system in different settings (e.g., group vs. individual; headset vs. desktop app). Follow-up questions were asked as necessary to probe deeper into participants’ responses, particularly when initial answers did not fully address the intended topics or directions. The complete list of interview questions can be found in Appendix [B](https://arxiv.org/html/2410.02406v1#A2 "Appendix B Interview Questions ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR").

### 4.4\. Analysis

We employed an inductive (bottom-up) approach to analyze the interview transcripts, following the thematic analysis guidelines by Braun and Clarke (Braun and Clarke, [2006](https://arxiv.org/html/2410.02406v1#bib.bib7)). The analysis was conducted in several stages by three researchers, all authors of the paper. Initially, each researcher read over all the interview transcripts (step 1), and then independently analyzed a sub-set of the interview transcripts to identify preliminary codes, themes, and patterns, selecting relevant quotes and observations (step 2). The team then discussed and agreed upon a final set of codes through multiple rounds of coding (step 3). To ensure the robustness of our analysis, the interview transcripts were cross-referenced with video recordings and notes taken during the interactions (step 4). Finally, all themes were discussed and defined by three researchers/coders (step 5) and written up (step 6).

## 5\. Findings

In this section, we describe four key themes: agent perception and its role in social VR, language learning in social VR, task design and perceptions of LLMs efficient in language learning, and interest in continued usage.

### 5.1\. Agent Perception and its Role in Social VR

#### 5.1.1\. ELLMA-T Human-like qualities

Participants compared their experience with ELLMA-T to that of conversing with a real human tutor. Participants frequently commented on the agent’s human-like qualities, noting how the interaction felt like conversing with a real person rather than an AI. As P4 expressed, ”I love the way it talk(s) to me. It (is) just like a real teacher; the interaction is really good.” Participants highlighted that the conversation flowed naturally, demonstrating ELLMA-T’s ability to listen, understand, and respond appropriately. For example, P8 appreciated that the agent ”was listening to me,” suggesting that it responded in a way that related to their personal experience. P2 also observed,

> ”First of all, it started off with the cultural experience. So then the moment I mentioned that I’m from India, the greeting changed from ’Hello’ to ’Namaste,’ which is like ’Hello’ in Hindi in our language. So that ’Namaste,’ that added a personal touch. Then it tried to understand how you feel in day-to-day conversations.”

This ability to personalize the interaction and subsequent conversation flow led P2 to conclude, ”It’s like… I’m talking to someone in person, so it’s not an AI for me right now.”

#### 5.1.2\. Importance of Adaptability and Role Switching Depending on Personal Preferences and Social Context

Participants expressed varied preferences regarding ELLMA-T’s role in facilitating language learning. Some preferred the agent to take on a traditional teacher role, similar to that of a classroom setting. For instance, P1 suggested, ”The agent could be like the teacher, and then the other people would be students. I kind of still feel like that — be more traditional, the agent as the teacher and the head of a classroom.” Others envisioned the agent in a more supportive role, acting as a facilitator or social glue in multi-person interactions. P5 imagined this role as helping to bridge gaps in conversations: ”And also they can fill the gap. Okay, what if two people are introverts and they probably don’t have much in common to talk about, the AI agent could ask questions or lead the conversation.” Additionally, some participants saw the agent as a feedback provider during real-human conversation practices. As P6 noted, ”You can have a conversation with a real person and learn from what they said, with the feedback provided by the agent.”

#### 5.1.3\. Perceptions of Agent Tone, Appearance, and Behaviour

We observed how nuanced perceptions of the agent’s tone, physical appearance, and behaviour impacted the overall interaction experience.

Agent Tone: Some participants noticed that the agent’s language tone when performing different tasks influenced their learning experience and how they perceived the agent. For example, P6 noted a clear distinction between the agent’s tone during the role-play and when giving feedback: ”I would hope that the (role-play) conversation is more casual, but I expect the feedback to be professional.” P8 highlighted the importance of providing encouraging phrases combined with an appropriate tone:

> ”Right now it’s very robotic, and so it just gives me basic feedback of … great improvement, but I liked certain things that you did… she did mention those, but it seemed very… like I was just typing into a computer and it’s coming out.”

P8 also added that the lack of inflection and tone variation in different contexts can make the agent’s interactions seem less engaging and less human-like: ”If you’re having a conversation, there are nuances on how we speak and certain sounds…The ups and downs in the way we speak.”

Agent Appearance: Some participants appreciated the neutral appearance, as it did not distract from learning or convey judgment. P1 remarked, ”The tone overall is general. It’s not too aggressive or too… General is good; it didn’t create a weird or odd atmosphere.” However, others preferred more personality for fun and engagement. P10 suggested, ”Maybe we can just use some famous characters from games, literature, or anything, and we can set up the environment … and it will behave and speak like that.” Several participants also expressed a preference for customizing the agent’s appearance. Suggestions for different avatars included realistic (P11, P12), cartoonish (P3, P10), or non-human-like animals (P6), as well as the ability to modify attributes like hair color, skin color, body type, gender (P6), and clothing (P4).

Agent Behaviour: Similar to preferences for tone and appearance, P7 acknowledged the minimal movement of the agent as beneficial for avoiding distractions from learning English. ”I didn’t really look at how he moved… I feel like maybe if add on too many moves, it can be distracting from learning English.” (P7) However, others suggested that natural gestures, such as head nodding and eye contact, would enhance engagement and create a more lifelike interaction. P11 stated, ”It should be representative of how people think of themselves. And then the blinking, the head nodding, it should all be there inside the VR environment.” Additionally, participants highlighted the importance of cultural and contextual factors, such as the need for consistency between the agent’s voice and visual identity and the potential for scenario-based adaptations of the agent’s personality and appearance. P12 mentioned ”I think for different scenarios, we can have changes in personality or change the avatar itself to simulate different environments and how different people speak. That would make a difference, I think.”

#### 5.1.4\. Freeing Interactions of Social Norms that Do Not Apply to AI

All participants found the agent approachable and comfortable to interact with. One significant advantage noted was the absence of social pressure or expectations tied to cultural or social norms. P5 highlighted this, stating,

> ”You talk with a person, and if you don’t properly tell them certain things, you might hurt their feelings. But if an AI gives you feedback, it’s like, oh, a machine is telling you the feedback, so it’s okay. They don’t have those thoughts like, oh, what if I hurt this person’s feelings?”

### 5.2\. Language Learning in Social VR

#### 5.2.1\. Situated Learning Supported by VR Affordances

Participants spoke about how their language learning was supported through VR’s unique affordances: 3D interactivity with objects and avatars, contextualized learning in an immersive environment, and enhanced emotional support through an embodied agent.

Interactive Learning: Participants described how being in a VR environment added significant value to their language learning experience. The interactive nature of VR was frequently highlighted, with P1 noting, ”The VR added the interactions. What I need to do is just speak. I don’t have to type.” The environmental context provided by VR, along with the embodied agent, created a sense of immersion that made the interaction feel ‘real.’ As P5 shared, ”I feel like I’m talking to a kind of a real person standing in front of me. I mean, it’s VR after all. So that’s a huge difference.”

Contextualized Learning: The VR environment provided valuable context for language learning, helping participants understand how to use language in real-life situations. P8 mentioned: ”For example, the bakery, it’ll be in that place. We walk by it, and it was like, oh, this is the bakery, and if you come here, then you can get these types of soups. But it’ll be applicable to my real-life scenarios as well.” Similarly, P12 emphasized the advantage of having a virtual environment that corresponds to the learning task: ”I think if we can actually have a coffee shelf, where there are espresso machines and everything. I think having an actual environment will make the difference, and that’s the advantage of VR.”

Emotional Support: Participants also discussed the emotional connection they felt with the embodied agent in VR. P1 noted that VR added an emotional dimension to the interaction: ”It adds emotions. So having a static or 2D agent just living on a screen, it feels like there’s less emotion attached to it.” Other participants mentioned how adding personality to the agent in VR increased the sense of emotional connection. P7 suggested, ”Or maybe they [people using this system] need more emotional support. So that may be a good way to give the agent a personality.” P9 and P11 emphasized the importance of adding physical reactions and body language, with P9 stating, ”Maybe more physical reaction. Like somebody I really talk to you and looking into your eyes,”.

#### 5.2.2\. One-on-One interactions alleviate Speaking Anxiety, Group Interactions support Shared Learning Experience

Participants expressed varying preferences for interacting with the agent in VR, either in a private space or in a group setting. Participants felt they would experience anxiety speaking in front of other people and preferred one-on-one interaction. As P2 articulated, ”I’ve had this problem, and I know many of my friends also have this problem. They kind of get, when they don’t have their confidence over the language, they kind of get overwhelmed and a little intimidated by being in the group.” Some participants preferred one-on-one interaction because they valued the direct interaction and feedback from the agent. As P10 noted, ”It’s one-on-one, so it can assess me and it can target my weakness.”

On the other hand, others preferred group interactions, recognizing the additional benefits of more engagement, peer support, and shared learning experiences. VRChat, as a social platform, offers the flexibility of choosing avatars and social modes, which can enhance learning with peer learners. P10 suggested, ”If we as learners can interact with each other in VR, that would be fun because we won’t care about what each other looks like or where we are, and we can focus on practicing.” Moreover, P7 highlighted the potential real-life benefits of group interactions in VR, such as preparing for group meetings and work environments, which cannot be simulated by interaction with the agent alone:

> ”I would love to have a group setting because it might be helpful if you’re going to work or have a group discussion. In real life, you have to catch up with what different people are talking about. So it might be more challenging, but it can be fun too.”

### 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks

The potential effectiveness of the learning experience was a central concern of participants for the design of the system, particularly in terms of task design relevance, assessment accuracy, conversation content, feedback, and the support provided by the agent.

#### 5.3.1\. Task Design Relevance

Our analysis showed that the system’s ability to address the contextual needs of learning spoken English for adult language learners in English-speaking countries was a recurring theme. Participants acknowledged that the agent effectively targeted specific challenges they faced. One significant challenge highlighted was the lack of opportunities for learning or receiving feedback even though they are already in the language environment, as P5 explained:

> ”Usually when you practice your English, it’s just… well, I’m studying here already in a foreign country. No one will give me their feedback. They can’t, I think. I do know this is Western culture. People give more positive feedback. But this one, it gave me more reality (realistic) feedback. You don’t get it in real world.”

Another participant, P5, emphasized the gap in informal conversation learning, which is often missing from formal and academic settings:

> ”Something I really want to learn about is how to use English when you try to agree with someone’s idea. It can be super, super simple for a native speaker because they don’t really know, oh, this is hard for a non-native speaker. I want to respond. When someone says something, I can say something like, ’oh’, ’amazing’, ’absolutely’—that kind of simple words. It is something that can be super hard for me.”

Participants valued the form of role-play conversation for its ability to provide a real-life conversation experience. As P7 noted, ”Because the conversation and dialogue is about daily life communications where you can learn something that is useful for you, useful for you to learn English.”

Participants also suggested specific scenarios they would like to practice, such as renting an apartment as an international student (P10) or participating in a mock interview (P6).

#### 5.3.2\. Assessing the Learner’s Language Ability

The majority of participants found the agent’s assessment of their language abilities to be accurate and consistent with their self-perceived levels. Five participants described the assessment as ”seemed appropriate” (P11), ”accurate” (P3, P8), ”very accurate” (P4), and ”fit to my level” (P9). Even if the assessment results were not completely precise, the respondents’ feedback suggests that the agent’s assessment met their expectations. P7, while not directly commenting on the accuracy of the assessment, found it useful as a form of descriptive feedback: ”She actually gave me a level, like a B1, which is quite interesting. I feel like I also want to know, oh, what kind of level I’m at and she gave some description like ’you can describe your feeling’, which is a good thing.”

An interesting case arose with P10, who chose not to follow the agent’s instructions and remained silent during the assessment. Consequently, the agent categorized her CEFR level as A1 (basic) (COE, [2024](https://arxiv.org/html/2410.02406v1#bib.bib19)), a result she strongly disagreed with: ”It has an agenda, so it wants me to repeat after it. I don’t want to do it… (The assessment was) not very accurate. It says I’m A1 level, but at least I think I’m C1 or C2.”

#### 5.3.3\. Role Play Conversation Content

The role-play conversations provided by the agent were well-received by participants. ELLMA-T showed creativity and adaptability in generating conversations, often performing well in various roles without requiring extra prompts. P3 and P8 both described their role-play experiences as ”surprisingly good.” For example, during a painting gallery scene where the participant played the role of a salesperson and the agent played the customer (role-play conversation 1, see Table [2](https://arxiv.org/html/2410.02406v1#S5.T2 "Table 2 ‣ 5.3.3\. Role Play Conversation Content ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")), P3 found the experience highly engaging and realistic:

> ”It provides very practical scenarios to have a conversation. And also this tutor can reflect according to my answer, because it requires abstract painting. And I say, I can give you a portrait. And they (the agent) say(s), I don’t need a portrait. I need (a) more abstract one. So that’s a good point I think, because you can feel that you really talk with a person. A person can give you (a) correct reflection. That is wonderful.”

This example highlights the LLM’s ability to leverage its vast pre-trained knowledge to create vivid, realistic and immersive conversations, moving beyond the dry, textbook-like exchanges that often characterize traditional language learning. The agent’s ability to add nuanced details based on the role and situation enhances the realism of the interactions. As P8 noted, ”Yeah. It’s not just that kind of dry back and forth… Yeah, it’s not like an IELTS exam where you just ask a question and get an answer.” This dynamic approach brings the context of the conversation to life, making the learning experience more engaging and effective.

Table 2\. Example of Role-Play Conversation in Painting Gallery Scene

> ELLMA-T [setting the context for role-play conversations]: So let’s imagine we are in an art gallery filled with beautiful and eclectic pieces of art. You are a knowledgeable and enthusiastic art curator and I’m an interested buyer exploring the gallery, maybe looking for something to spruce up my home. The conversation could start with me walking into the gallery. Okay, here we go.

| Event Line | Interaction Context | Excerpt |
| --- | --- | --- |
| 1 | Agent (Customer) | Hello there. This is my first time visiting this gallery. Could you show me a few standout pieces and tell me their stories? |
| 2 | User (Salesperson) | Yeah. Can you first tell me what kind of artwork you want to buy or you’re looking for? |
| 3 | Agent (Customer) | I’m very interested in contemporary paintings. Preferably something vibrant and abstract. I’m looking for a piece that could be the focal point of my living room. It’s a fairly large space with neutral colors. |
| 4 | User (Salesperson) | Can we look at this portrait? This portrait is about someone who is in the renascence age and in this portrait, you can say the background is in the dark, broad color. And you can say the portrait is very clear and the face of the person in the portrait is very dynamic and active. And what do you think about this? |
| 5 | Agent (Customer) | The contrast between the dark background and the vivid facial expression must create a fantastic visual impact. However, I am more interested in abstract art rather than portraits. Do you have any abstract pieces with bold, colorful strokes? |
| 6 | User (Salesperson) | Yeah. If you are looking for some abstract. How about the Impressionism? |

#### 5.3.4\. Feedback Effectiveness

Participants frequently commented on the agent’s feedback as one of its most valuable features. For example, P2 appreciated the specific feedback on tense usage, stating, ”So with my tenses, if there was a mistake with the tenses, that’s there. What can I improve, that’s there. The feedback was absolutely correct. Even I know that I goof up in my tenses sometimes.”

P7 also emphasized the benefit of receiving feedback on common errors made by non-native speakers: ”I feel like it improved a lot because usually, as non-native speakers, we can’t really tell when we’re using the wrong tense or wrong words, but they mention it as feedback. So I feel like it’s really helpful.”

Despite recognizing the value of the feedback, participants also expressed a desire for more personalized and detailed responses. They suggested that the feedback could be tailored based on their learning history and user profile, with specific examples of mistakes and corrections. P11 pointed out that, ”For learning English, I think that AI isn’t giving very specific responses. It’s giving very general answers. If a person was actually normally listening, the tutor would reply with an example of what I said wrong and then rephrase it and give an example of what it should be.”

Additionally, while one participant found the feedback ”educational,” they also mentioned that it could be too lengthy: ”I think the feedback is sometimes too long for me because it will illustrate a lot of feedback evaluation.” This suggests that while comprehensive feedback is appreciated, there is a balance to be struck between thoroughness and conciseness to maintain user engagement.

#### 5.3.5\. Scaffolding Effectiveness

The agent provided various forms of support and encouragement throughout the sessions. Verbal cues like ”You are doing great!” and ”Good job doing [activity]…” were consistently used to motivate participants. When participants struggled to generate dialogue during role-play, the agent offered scaffolding by presenting examples for them to repeat. As P7 noted, ”I do like the part that he actually gave some hint, ’for example …’, ’Oh, you can say that …’.” Additionally, the agent asked clarification questions when the meaning of the user’s input was unclear and provided explanations when participants were unsure how to proceed. P1 described an instance where: ” She asked me to straight up tell her the story about something and I didn’t quite catch that the first time. I asked her what are the options that I have. And then she gave one and I liked it.”

An interesting observation was made by P12, who found that despite having an accent, the agent accurately captured his input: ” I think it captures words better than I thought it would. Maybe I have an accent sometimes. I didn’t expect it to be this good.” This highlights a common communication challenge among English language learners, which presents a significant design challenge for the system.

### 5.4\. Interest in Continued Usage

#### 5.4.1\. Likelihood of Continued Usage

Nearly all participants indicated that they would use the system regularly to practice their language skills, with usage times ranging from 30 minutes to four hours daily (P1, P2, P3, P6, P10, P12), or one to two times per week (P7, P8). Some participants noted that they would increase their usage if the physical fatigue associated with heavy headsets could be mitigated (P4, P8). Additionally, some participants indicated that they would increase their usage when preparing for specific scenarios such as job interviews (P9). These findings suggest our system has strong potential for supporting sustained language learning over time.

#### 5.4.2\. Factors Negatively Impacting Continued Usage

Despite the positive indications for regular use, we identified several challenges related to task design and conversation flow, which could cumulatively and negatively impact long-term usage. One participant highlighted how the response latency could become more problematic over time, noting, ”So this seems to be okay, but as the speed varies, the transaction time — so the time that I need to spend with the AI — will increase more as per the response time” (P2). Additionally, breakdowns in conversation flow were noted when the agent responded too quickly or occasionally interrupted the participant, further detracting from the overall experience. Participants also reported issues with task design, such as unclear task explanations or a lack of concise instructions when switching tasks.

Importance of Short and Concise Instructions for Learning: During all the learning sessions, participants generally followed ELLMA-T’s instructions. One participant commented that the instructions were clear, stating, ”All the rules and regulations, all the prompts that the user needs to follow are there” (P2). However, not all participants shared this experience. One participant expressed panic and confusion when starting the session, noting, ”I feel like just having a short, precise description of the context would help. I just couldn’t figure out what was going on” (P1). An observation from the video recordings supports this concern; ELLMA-T sometimes provided instructions that were too lengthy to fit within the chat window, making them difficult for participants to follow. When this occurred, participants often had to ask ELLMA-T to restate the task, which disrupted the flow of the session.

Latency-related Conversation Flow Breaks:

Latency was an issue in generating responses commonly associated with currently available LLMs. Participants frequently mentioned that the delay in response time made it clear they were interacting with a machine rather than a human. As one participant noted, ”The response time is what will make you… okay, you are talking to a machine right now” (P2, P5). Another echoed this sentiment, ”I think his feedback is a little bit slower. Sometimes I need to wait for his response” (P4). This delay in processing left participants feeling disconnected from the conversation, as highlighted by P10: ”It doesn’t feel like a real conversation. There are gaps and silent time.”

Conversation-pacing-related Flow Breaks: Another major disruption came from the rigid turn-taking structure of the conversation. Unlike human interactions, where there may be overlaps of speaking or long pauses while turn-taking, ELLMA-T strictly adhered to turn-taking by waiting for the user to finish speaking before responding, and did not respond to interruptions. This often led to interruptions and a lack of responsiveness. For example, two participants described how ”There were a few times when I intended to speak, but she just continued” (P1), ”I noticed I couldn’t interrupt the agent. Sometimes I find it hard to tell whether it’s my turn to speak.” (P12). Others expressed frustration with the agent’s inability to adapt to their conversational pace, with P7 noting, ”She doesn’t wait for me to speak”. Addressing these issues will be essential for improving the overall user experience in long-term language learning and making the agent feel more like a genuine conversational partner.

## 6\. Discussion

To address our research question, we contextualize our findings with prior literature and discuss the potential of embodied LLM-based agents for language learning in social VR in the following sections. We first discuss the key qualities reported by participants of ELLMA-T to support English language learning; second, the advantages of situated language learning in social VR; and third, the benefits and challenges of embodied LLM-based agents for language learning in social VR.

### 6.1\. User Experience of ELLMA-T for Supporting English Language Learning

Overall, our findings indicate that ELLMA-T possesses several key qualities that align with the existing literature’s criteria for an effective language tutor: human-like interaction, enhanced emotional connection through embodiment, a demeanour that is accessible and approachable, and provision of encouragement and support.

#### 6.1.1\. Emulating Human-Like Interaction

One of the key strengths of ELLMA-T is its ability to emulate human-like interaction, a quality that participants frequently noted in Section [5.1.1](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS1 "5.1.1\. ELLMA-T Human-like qualities ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"). The perception of the agent as ’human-like’ suggests ELLMA-T’s success in resembling a human tutor’s ability to maintain an interactive and meaningful conversation. This finding supports previous research that underscores the significance of ECAs mimicking human-to-human interactions and highlights the critical role of ’believable’ interactions in conversation system design with ECAs (Cassell et al., [2000](https://arxiv.org/html/2410.02406v1#bib.bib12)). As our findings in Section [5.1.2](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS2 "5.1.2\. Importance of Adaptability and Role Switching Depending on Personal Preferences and Social Context ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") indicate, ELLMA-T is seen in the roles typically fulfilled by humans such as a tutor or a buddy. Specifically, participants appreciated how the agent could listen, understand, and respond appropriately, making the learning experience feel more natural and less mechanical.

#### 6.1.2\. Enhancing Agent-Learner Emotional Connections

Existing literature suggests that the embodiment of an agent in language learning enhances the immersive experience and strengthens the emotional connection between the agent and the learner, which positively impacts learning outcomes (Lefkowitz, [2009](https://arxiv.org/html/2410.02406v1#bib.bib46)). Our findings in section [5.2.1](https://arxiv.org/html/2410.02406v1#S5.SS2.SSS1 "5.2.1\. Situated Learning Supported by VR Affordances ‣ 5.2\. Language Learning in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") support this, as participants reported feeling a stronger emotional connection with a 3D embodied agent in the VR environment (P1, P6). Furthermore, the data in section [5.1.3](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS3 "5.1.3\. Perceptions of Agent Tone, Appearance, and Behaviour ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") suggests that adding personality, personalized tone, appearance, and body movement to the ECA can further enhance this emotional connection (P7, P9, P11). However, previous research presents mixed views on the impact of incorporating ECAs in a VR environment for learning, as it may increase cognitive load for learners (Petersen et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib57)). This also aligns with our findings that some participants preferred the agent to have a neutral tone, minimal facial expressions, and body movements, as these were perceived as ”less distracting” to learning (P1, P7).

#### 6.1.3\. Importance of Non-Verbal Cues

Emotional support is recognized as a critical factor for sustained language learning (Dinçer et al., [2013](https://arxiv.org/html/2410.02406v1#bib.bib23)). In our implementation of ELLMA-T, we adopted communicative and affective strategies based on (Ayedoun et al., [2019](https://arxiv.org/html/2410.02406v1#bib.bib4)) to increase learners’ willingness to communicate (WTC) by providing verbal encouragement and support that was appreciated by the participants (see Section [5.3.5](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS5 "5.3.5\. Scaffolding Effectiveness ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")). However, our findings also indicate that verbal cues alone were insufficient, particularly in the interactive VR environment (see Section [5.1.3](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS3 "5.1.3\. Perceptions of Agent Tone, Appearance, and Behaviour ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")). This finding aligns with existing research, which suggests that non-verbal cues significantly enhance the perception of human likeness and the effectiveness of communicative tasks with ECAs (Churchill et al., [2000](https://arxiv.org/html/2410.02406v1#bib.bib18)). Participants reported that the agent’s voice inflections influenced their learning experience, with some noting that a neutral tone sometimes felt slightly negative or lacking in encouragement, even when verbal encouragement was present. This echoes the result of a recent study suggesting that ECAs with extrovert personalities, complemented by expressive facial movements and body language, are perceived as more effective for general learning purposes (Sonlu et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib67)). However, in the context of language learning in social VR, our findings in Section [5.1.3](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS3 "5.1.3\. Perceptions of Agent Tone, Appearance, and Behaviour ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") suggest that the need for nuanced interactions becomes even more critical. Participants expected verbal cues to be complemented by appropriate tones, facial expressions, and body movements to be fully effective. In addition, these non-verbal cues need to be appropriate for the language learning scenario and social context. Our observation also indicates that the effectiveness of the agent’s encouragement was also less noticeable, likely due to technical difficulties that interrupted conversation flow and the generally high language proficiency of the participants, which may have reduced the perceived need for support. This points to a broader challenge in designing ECAs with various personality and non-verbal cue combinations for learners with different language levels.

#### 6.1.4\. Reducing Speaking Anxiety

As findings in Section [5.2.2](https://arxiv.org/html/2410.02406v1#S5.SS2.SSS2 "5.2.2\. One-on-One interactions alleviate Speaking Anxiety, Group Interactions support Shared Learning Experience ‣ 5.2\. Language Learning in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") suggest, ELLMA-T shows the potential of reducing speaking anxiety among participants (P8), especially in one-on-one interactions. Participants felt more at ease when interacting with ELLMA-T in the VR environment, shielded by an avatar and free from the social pressures typically associated with human-to-human interactions that do not necessarily apply to human-AI interactions (see Section [5.1.4](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS4 "5.1.4\. Freeing Interactions of Social Norms that Do Not Apply to AI ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")). The expectation of interacting with an AI, rather than a human, naturally reduced the pressure often associated with speaking in a foreign language. This finding supports the effectiveness of ELLMA-T in language learning, which aligns with existing research that emphasizes the importance of reducing anxiety in language learning environments (Woodrow, [2006](https://arxiv.org/html/2410.02406v1#bib.bib77)), especially for adult immigrant learners who often experience heightened pressure in social situations (Adjagbodjou and Kaufman, [2024](https://arxiv.org/html/2410.02406v1#bib.bib2)). ELLMA-T’s perceived approachability and the fact that it is always available and accessible further contributed to this reduced anxiety, positioning the system as a valuable tool for continuous language practice without the social constraints typically associated with human interactions.

### 6.2\. Situated Language Learning in Social VR

The situated learning theory explains how learning is most effective when it occurs within the relevant context(Lave and Wenger, [1991](https://arxiv.org/html/2410.02406v1#bib.bib44); Brown et al., [1989](https://arxiv.org/html/2410.02406v1#bib.bib8)), our findings in Section [5.2](https://arxiv.org/html/2410.02406v1#S5.SS2 "5.2\. Language Learning in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") correspond to this theory. Participants expect to have the VR scene, and characters with different personas in VR environment corresponding to the role-play conversations to have a more realistic experience of a potential real-life situation in which use of learned language may be important. In addition to the advantages the VR environment provides to learning, the opportunities with Social VR are also discussed in the following sections.

#### 6.2.1\. Flexibility in Interaction Modes

The findings in Section [5.2.2](https://arxiv.org/html/2410.02406v1#S5.SS2.SSS2 "5.2.2\. One-on-One interactions alleviate Speaking Anxiety, Group Interactions support Shared Learning Experience ‣ 5.2\. Language Learning in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") suggest that a notable advantage of using social VR is the possibilities it offers in switching between private interactions with the agent and agent-facilitated group interactions. Participants appreciated the ability to engage in one-on-one sessions with ELLMA-T, which helped reduce speaking anxiety (see Section [5.1.4](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS4 "5.1.4\. Freeing Interactions of Social Norms that Do Not Apply to AI ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")) and allowed for personalized feedback. At the same time, VRChat, as a social platform, also has the potential to meet learners’ social needs and foster a sense of community and group affirmation as discussed in Section [6.2](https://arxiv.org/html/2410.02406v1#S6.SS2 "6.2\. Situated Language Learning in Social VR ‣ 6\. Discussion ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"). In addition, the option to transition to group interactions facilitated by the agent introduces a dynamic that is difficult to replicate in traditional language learning settings. Previous research has shown promising results for human teacher-facilitated collaborative language learning in VR between human students (Jackson and Fagan, [2000](https://arxiv.org/html/2410.02406v1#bib.bib32); Peterson, [2012](https://arxiv.org/html/2410.02406v1#bib.bib58); Cantone et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib11)). With ELLMA-T potentially facilitating similar group interactions, it provides a promising direction for further exploration.

#### 6.2.2\. Trade-offs Between VR headsets and 2D Desktop Interfaces

VRChat provides users with the flexibility to interact with the platform through VR headsets or a 2D desktop interface. While the immersive qualities of VR provide significant benefits, our findings in Section [5.4.1](https://arxiv.org/html/2410.02406v1#S5.SS4.SSS1 "5.4.1\. Likelihood of Continued Usage ‣ 5.4\. Interest in Continued Usage ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") also highlight the trade-offs associated with prolonged use of VR headsets. As noted in previous research, the availability of the technology, cognitive load and fatigue associated with VR are significant barriers to widespread use (Petersen et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib57)). This issue was echoed by participants in our study, with some participants expressing a preference for interacting with a 2D desktop agent due to the convenience and reduced physical strain. This suggests that while VR offers powerful immersive experiences, a more accessible 2D interface could broaden the system’s appeal and usability, especially in longitudinal studies where extended use is necessary.

### 6.3\. LLMs for Language Learning: Performance and Potential

Findings from Section [5.3](https://arxiv.org/html/2410.02406v1#S5.SS3 "5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") suggest that ELLMA-T, powered by GPT-4, performed well on current teaching tasks, particularly in generating role-play conversations and providing feedback. This aligns with recent research (Caines et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib10)), which confirms the LLM’s capability in learning material generation and assessment. The ability of LLMs to create flexible, context-sensitive content presents significant advantages over previous scripted or NLP-assisted systems, which often suffered from communication breakdowns due to technical limitations (Zhang et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib84)). However, present limitations of LLMs such as latency or lack of precise turn-taking logic can lead to breakdowns in communication flow that may impact users’ motivation to use the system for prolonged time (see Section [5.4.2](https://arxiv.org/html/2410.02406v1#S5.SS4.SSS2 "5.4.2\. Factors Negatively Impacting Continued Usage ‣ 5.4\. Interest in Continued Usage ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")).

#### 6.3.1\. ELLMA-T as a Personalized and Creative Tutor

The flexibility and creativity LLM brings to role-play conversations are central to the human-like qualities of ELLMA-T as discussed in Section [5.3.3](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS3 "5.3.3\. Role Play Conversation Content ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"). LLMs can also generate dynamic and coherent responses (Yang et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib80)) that can adapt to the learner’s input in real-time (Caines et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib10)). However, to fully exploit the potential of LLMs for different language learning tasks, more intricate prompting and contextual understanding are necessary. Our findings in Sections [5.1.3](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS3 "5.1.3\. Perceptions of Agent Tone, Appearance, and Behaviour ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") and [5.3.4](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS4 "5.3.4\. Feedback Effectiveness ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") support prior work that indicates that the quality of the LLM’s output, particularly the wording of the output, should adapt to the context in which it is used (Barambones et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib6)). We hypothesize that improving the LLM’s performance will require: 1) a better understanding of conversational context, for instance, the agent’s role and the expected tone; 2) developing more sophisticated techniques to generate appropriate responses in various contexts. Additionally, future work can explore how text responses can be effectively mapped to corresponding voice tones, facial expressions, and body movements under different contexts.

#### 6.3.2\. Level Assessment and Difficulty Level Adjustment

The results from Section [5.3.2](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS2 "5.3.2\. Assessing the Learner’s Language Ability ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") indicates that the GPT-4 assessment of participants’ language level was overall accurate and in agreement with the findings by (Caines et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib10)). However, the assessment format could be better designed with clearer instructions and a more flexible format given the feedback from participants who do not want to follow a fixed agenda. Following the assessment, the findings in Section [5.3.2](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS2 "5.3.2\. Assessing the Learner’s Language Ability ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") infer that GPT-4 likely adjusted the language difficulty level for subsequent conversations based on the results. This inference is supported by feedback from P3 and P8, who were assessed at a high proficiency level (C1) and mentioned that the vocabulary used in their subsequent conversations might be too difficult for beginners. This suggests that the system correctly matched the difficulty level to their advanced assessment. However, we have not explicitly assessed how well GPT-4 used in ELLMA-T was adjusting task difficulty based on the previous assessment of the learner’s language proficiency. Given that no other participants raised concerns about the difficulty level, it is possible that the adjustments were suitable for their assessed levels, but without a more comprehensive assessment, this remains speculative. Future studies can include specific questions about participants’ perceived difficulty level and linguistic metrics, such as word frequency, sentence length, and syntactic complexity, to better understand how well LLMs can tailor language task difficulty to individual needs.

#### 6.3.3\. Opportunities and Challenges of Designing LLM-based ECAs for Informal Spoken English Learning in Social VR

(Adjagbodjou and Kaufman, [2024](https://arxiv.org/html/2410.02406v1#bib.bib2)) emphasized the importance of informal language practice as distinct from formal academic learning, highlighting a gap in tools designed specifically for this context. Our study echoes this view, with findings from Section [5.3.1](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS1 "5.3.1\. Task Design Relevance ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") showing that participants valued the opportunity to practice speaking English in real-life, informal settings. Despite being in an English-speaking environment, participants in our study reported challenges such as a lack of feedback and opportunities for language improvement in Section [5.3.1](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS1 "5.3.1\. Task Design Relevance ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), underscoring the need for effective feedback mechanisms. Common challenges faced by EFL learners, such as anxiety and the need for support to increase WTC (Ayedoun et al., [2019](https://arxiv.org/html/2410.02406v1#bib.bib4)), were partially addressed by our system (see Sections [5.2.2](https://arxiv.org/html/2410.02406v1#S5.SS2.SSS2 "5.2.2\. One-on-One interactions alleviate Speaking Anxiety, Group Interactions support Shared Learning Experience ‣ 5.2\. Language Learning in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), [5.1.4](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS4 "5.1.4\. Freeing Interactions of Social Norms that Do Not Apply to AI ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), and [5.3.5](https://arxiv.org/html/2410.02406v1#S5.SS3.SSS5 "5.3.5\. Scaffolding Effectiveness ‣ 5.3\. Task Design and Perceptions of LLM’s Efficacy in Language Learning Tasks ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR")). However, the challenge of designing effective verbal and non-verbal cues, as discussed in Section [6.1](https://arxiv.org/html/2410.02406v1#S6.SS1 "6.1\. User Experience of ELLMA-T for Supporting English Language Learning ‣ 6\. Discussion ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), poses challenges in ensuring the system can accurately interpret and respond to users’ intentions. Additionally, communication obstacles common among EFL learners — such as accents, grammar mistakes, incorrect expressions, low speaking volume due to lack of confidence, and intermittent speaking due to unfamiliarity with the language (Adjagbodjou and Kaufman, [2024](https://arxiv.org/html/2410.02406v1#bib.bib2)) — further complicate the system’s ability to process and respond effectively. These issues are more challenging for an audio detection system to interpret compared to inputs from native speakers, highlighting the need for more advanced solutions tailored to the specific needs of EFL learners.

## 7\. Design Implications

We present five design implications for LLM-based ECAs to facilitate spoken English learning in Social VR based on our key findings.

### 7.1\. Promoting Personalized and Adaptive Learning through Design

Design Implication 1: Design LLM agents with contextual and cultural relevance in the adaptive learning experience based on user information, user preference, and learning history to support personalized and adaptive learning.

The analysis in Sections [5.1](https://arxiv.org/html/2410.02406v1#S5.SS1 "5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") and [6.3](https://arxiv.org/html/2410.02406v1#S6.SS3 "6.3\. LLMs for Language Learning: Performance and Potential ‣ 6\. Discussion ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") highlight the importance of adaptive learning experiences in language learning tools. Participants wanted responses tailored to their proficiency levels, challenges, and progress. LLMs have the potential to suggest learning materials based on user information, user preference, and learning history (Darsih, [2018](https://arxiv.org/html/2410.02406v1#bib.bib21)). However, this requires the agent to have a more sophisticated memory design and advanced reasoning ability. Emerging studies have focused on developing different memory mechanisms for increasing context-awareness and adaptation to user behaviour patterns. For example, a human-like memory architecture for LLM-based conversational agents enables autonomous memory recall and dynamic memory consolidation to improve response generation (Hou et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib29)). Additionally, the Long-term Dialogue Agent framework can enable more personalized and contextually relevant responses over extended interactions (Li et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib47)). A similar advanced memory design can be applied with an agent in the context of language learning. An agent with such capabilities could adapt its interactions based on prior user behavior and progress, and dynamically adjust content delivery according to the learner’s needs. In addition, based on the findings in Section [5.1.3](https://arxiv.org/html/2410.02406v1#S5.SS1.SSS3 "5.1.3\. Perceptions of Agent Tone, Appearance, and Behaviour ‣ 5.1\. Agent Perception and its Role in Social VR ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR") and criteria from Adjagbodjou and Kaufman ([2024](https://arxiv.org/html/2410.02406v1#bib.bib2)), we suggest the value of cultural references and contextual relevance in the adaptive learning experience. For instance, an agent could adjust its dialogue to reflect the cultural nuances of the learner’s cultural background, and new environment, offering more meaningful and relatable interactions in the context of language learning.

### 7.2\. Enhance Agent-Learner Connection

Design Implication 2: Incorporate user-preferred personas, tone, voice, appearance, and body movement in LLM-based agents to strengthen emotional connections and support language learning.

Participants in our study exhibited distinct preferences for the agent’s persona, tone of response, voice inflections, appearance, and body movement, which may potentially affect their learning experience with the agent. While previous research has mixed opinions on the impact of an agent’s non-verbal features on learning outcomes (Petersen et al., [2021](https://arxiv.org/html/2410.02406v1#bib.bib57); Sonlu et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib67)), offering customizable options for users may be a beneficial approach.

Design Implication 3: Integrate advanced systems to effectively use verbal and non-verbal cues to provide emotional support to the learners.

Emotional support requires sophisticated techniques to accurately interpret learners’ intentions and offer contextually appropriate support (Dinçer et al., [2013](https://arxiv.org/html/2410.02406v1#bib.bib23)). For example, the agent should be able to distinguish between pauses that indicate the learner is thinking and those that suggest the learner is struggling to respond, adapting its behaviour accordingly by offering encouragement or rephrasing questions. Using machine learning techniques to detect user emotions such as silence, hesitation, or frustration could potentially enable the agent to deliver timely and suitable emotional support. For example, a recent study by (Siyan et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib66)) has demonstrated the potential of using machine learning and prompt engineering to detect anxiety from students’ voices, allowing the agent to offer adaptive, empathetic responses to enhance language learning.

### 7.3\. Cultivate Long-Term Engagement and Continued Use

Design Implication 4: Design LLM-based agents in social VR to facilitate long-term language learning essential for improving fluency.

Based on the findings in Section [5.4](https://arxiv.org/html/2410.02406v1#S5.SS4 "5.4\. Interest in Continued Usage ‣ 5\. Findings ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), participants’ willingness to continue using the system regularly suggests that LLM-based agents in VR hold significant potential for supporting long-term language learning. However, this potential is contingent on the system’s ability to adapt to the learner’s progress, sustain engagement, and mitigate issues like conversation flow breakdowns and VR-related usage fatigues. For example, incorporating preferred personality traits or non-verbal cues into the agent can potentially increase engagement (Sonlu et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib67)). Additionally, gamification is a well-established approach to enhance engagement (Koulouris et al., [2020](https://arxiv.org/html/2410.02406v1#bib.bib41); Cheng et al., [2017](https://arxiv.org/html/2410.02406v1#bib.bib15); Hsu et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib30); Seow, [2023](https://arxiv.org/html/2410.02406v1#bib.bib65)). Most recently, the use of LLM-agents as peers in VR classrooms has been shown to promote engagement and classroom participation (Liu et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib50)).

Design Implication 5: Use Open and Scalable VR Platforms for Sustained Long-Term Development and Studies.

Some existing studies have utilized commercially available open and scalable platforms like Second Life (Peterson, [2012](https://arxiv.org/html/2410.02406v1#bib.bib58)) and Metaverse (Cantone et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib11)). We recommend using similar platforms for sustained development. In our study, we used VRChat, which provides the necessary flexibility for long-term development and broader public usage. Through VRChat’s OSC (VRChat, [2024a](https://arxiv.org/html/2410.02406v1#bib.bib71)) and Unity (Unity, [2024](https://arxiv.org/html/2410.02406v1#bib.bib69)), we can develop customizable avatars and virtual worlds, control agent personas, voice tones, facial expressions, and body movements. This scalability ensures that the system can evolve and support long-term studies over time. However, the deployment of AI-agents in social VR platforms like VRChat is a grey area because some view them as modifications that are not supported or permitted according to the platform guidelines¹¹1[https://hello.vrchat.com/community-guidelines](https://hello.vrchat.com/community-guidelines). Thus, the use of AI-agents in social VR requires future considerations of how can we enable more open and scalable platforms for sustainable long-term development and exploration of AI-agents in social VR.

## 8\. Limitations & Future Work

We discuss the limitations of our study and offer suggestions for future work on embodied LLM-based agents for language learning.

Widening participation: The participant pool consisted primarily of graduate students with relatively high English proficiency, most of whom were from China. This demographic specificity limits the generalizability of our findings. Future research should investigate how these results apply to a more diverse demographic, including learners with lower language proficiency and varied cultural backgrounds. In addition, the system could be integrated into existing language learning programs offered by educational institutions and immigrant support organizations. This would allow for a more systematic and structured learning experience. Partnerships with these organizations could help in customizing the system further to address the specific needs of this user group.

Refining LLM Prompt Engineering for Language Learning Tasks LLM’s performance is very sensitive to prompts. Our current prompt engineering approach is largely based on trial and error due to lack of previous research on prompt engineering for language learning tasks, which may not be the most effective method for optimizing LLM performance. As more prompt frameworks become available (Khattab et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib38)), and the capabilities of LLMs continue to evolve, more efficient techniques for optimizing prompts are likely to become available. This could reduce the impact of specific prompt wording, shifting the focus towards refining the reasoning and task breakdown logic for complex, multi-turn conversations. Further research is necessary to develop more systematic workflows for enhancing LLM performance in these contexts.

Exploring ECA Features for Language Learning in social VR Given the limited research on how the verbal and non-verbal features of ECAs affect learning outcomes in social VR (as discussed in Section [7.2](https://arxiv.org/html/2410.02406v1#S7.SS2 "7.2\. Enhance Agent-Learner Connection ‣ 7\. Design Implications ‣ ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning in Social VR"), particularly in the context of language learning, further exploration is necessary to better understand and optimize these interactions. Some initial work has been done on the effects of embodiment and personality traits in LLM-educational agents (Sonlu et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib67)) that could be implemented in an embodied LLM-agent for language learning. More research needs to be done to validate the findings in the context of language learning in social VR.

Optimizing LLM Selection for Task-Specific Performance Achieving the design goals of ELLMA-T may require the use of different LLMs tailored to specific tasks. For conversational tasks, GPT-4o (OpenAI, [2024a](https://arxiv.org/html/2410.02406v1#bib.bib54)) is recognized as the best-performing model, capable of understanding user sentiment and maintaining engaging dialogues (OpenAI, [2024a](https://arxiv.org/html/2410.02406v1#bib.bib54)). Other models may be more suitable for summarization and feedback due to their reliability (Caines et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib10)). Preliminary studies suggest that GPT-4o is the best-performing model we can use for both tasks in this study. In future work, we may use or combine different models to leverage the strengths of various models and optimize the system’s performance for future improvements.

Validating LLM Assessment of Language Proficiency: We recognize that the use of CEFR criteria (COE, [2024](https://arxiv.org/html/2410.02406v1#bib.bib19)) to prompt the LLM in our system is not entirely precise and serves primarily as a rough guide for the LLM to adjust language difficulty for following dialogues. Whether the dialogue content generated by the LLM accurately reflects the intended difficulty level remains to be rigorously validated in future studies.

Implementing Content Filters for LLM Responses Additionally, we did not implement a filtering layer to screen the LLM’s responses before delivery, which could lead to hallucinations or potentially harmful content. Although our initial tests showed that GPT-4 generally provided correct responses in grammar correction tasks, existing studies indicate that it may still produce inaccurate results or introduce stereotypes (Siyan et al., [2024](https://arxiv.org/html/2410.02406v1#bib.bib66)). Future work will include the evaluation of LLM responses by using the LLM Judge framework (Zheng et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib85)) or CoMPosT framework (Cheng et al., [2023](https://arxiv.org/html/2410.02406v1#bib.bib16)) to detect stereotypes.

Evaluating Long-Term Educational Impact Lastly, this preliminary study involved only 15-minute interaction sessions, and we did not measure the system’s effectiveness in terms of actual language learning outcomes. To fully assess the educational impact of ELLMA-T, we plan to conduct longitudinal studies with longer learning sessions. In addition, these studies will provide a more comprehensive evaluation of user engagement over time and help identify key factors that contribute to sustained use and long-term language learning outcomes.

## 9\. Conclusion

With this study, we contribute to the growing body of research on ECAs and language learning by demonstrating the potential of combining situated learning principles with LLM-based agents in social VR environments. In particular, we contribute a system design for an LLM-agent for language learning (ELLMA-T), findings from a qualitative evaluation study (N=12), and design implications based on our findings. Our findings highlight the importance of integrating nuanced non-verbal cues and customizing agent interactions to address the diverse needs of adult language learners. The system also provides benefits such as reducing speaking anxiety and providing potential flexible learning modes. However, our study also highlighted areas for improvement including the need for more sophisticated emotional support, better handling of communication obstacles, and refined conversation flow to maintain long-term user engagement. These findings offer implications for future development, positioning embodied LLM-based agents as a promising tool for personalized and context-aware language learning.

## References

*   (1)
*   Adjagbodjou and Kaufman (2024) Adinawa Adjagbodjou and Geoff Kaufman. 2024. Envisioning Support-Centered Technologies for Language Practice and Use: Needs and Design Opportunities for Immigrant English Language Learners (ELLs). In *Proceedings of the CHI Conference on Human Factors in Computing Systems* *(CHI ’24)*. Association for Computing Machinery, New York, NY, USA, 1–15. [https://doi.org/10.1145/3613904.3642236](https://doi.org/10.1145/3613904.3642236)
*   Atkinson et al. (2005) Robert K. Atkinson, Richard E. Mayer, and Mary Margaret Merrill. 2005. Fostering social agency in multimedia learning: Examining the impact of an animated agent’s voice. *Contemporary Educational Psychology* 30, 1 (Jan. 2005), 117–139. [https://doi.org/10.1016/j.cedpsych.2004.07.001](https://doi.org/10.1016/j.cedpsych.2004.07.001)
*   Ayedoun et al. (2019) Emmanuel Ayedoun, Yuki Hayashi, and Kazuhisa Seta. 2019. Adding Communicative and Affective Strategies to an Embodied Conversational Agent to Enhance Second Language Learners’ Willingness to Communicate. *International Journal of Artificial Intelligence in Education* 29, 1 (March 2019), 29–57. [https://doi.org/10.1007/s40593-018-0171-6](https://doi.org/10.1007/s40593-018-0171-6)
*   Aïm et al. (2016) Florence Aïm, Guillaume Lonjon, Didier Hannouche, and Rémy Nizard. 2016. Effectiveness of Virtual Reality Training in Orthopaedic Surgery. *Arthroscopy: The Journal of Arthroscopic & Related Surgery* 32, 1 (Jan. 2016), 224–232. [https://doi.org/10.1016/j.arthro.2015.07.023](https://doi.org/10.1016/j.arthro.2015.07.023)
*   Barambones et al. (2024) Jorge Barambones, Carlos Moral, Angel de Antonio, Ricardo Imbert, Loïc Martínez-Normand, and Elena Villalba-Mora. 2024. ChatGPT for learning HCI techniques: A case study on interviews for personas. *IEEE Transactions on Learning Technologies* 17 (2024), 1486–1500. [https://doi.org/10.1109/TLT.2024.3386095](https://doi.org/10.1109/TLT.2024.3386095)
*   Braun and Clarke (2006) Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. *Qualitative Research in Psychology* (Jan. 2006). [https://doi.org/10.1191/1478088706qp063oa](https://doi.org/10.1191/1478088706qp063oa) Publisher: Taylor & Francis Group.
*   Brown et al. (1989) John Seely Brown, Allan Collins, and Paul Duguid. 1989. Situated cognition and the culture of learning. *1989* 18, 1 (1989), 32–42. [https://library.oapen.org/bitstream/handle/20.500.12657/24125/1/1006006.pdf#page=313](https://library.oapen.org/bitstream/handle/20.500.12657/24125/1/1006006.pdf#page=313)
*   Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165) arXiv:2005.14165 [cs].
*   Caines et al. (2023) Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan Gao, Oeistein Andersen, Zheng Yuan, Mark Elliott, Russell Moore, Christopher Bryant, Marek Rei, Helen Yannakoudakis, Andrew Mullooly, Diane Nicholls, and Paula Buttery. 2023. On the application of Large Language Models for language teaching and assessment technology. [http://arxiv.org/abs/2307.08393](http://arxiv.org/abs/2307.08393) arXiv:2307.08393 [cs].
*   Cantone et al. (2023) Andrea Antonio Cantone, Rita Francese, Raffaele Sais, Otino Pio Santosuosso, Aurelio Sepe, Simone Spera, Genoveffa Tortora, and Giuliana Vitiello. 2023. Contextualized Experiential Language Learning in the Metaverse. In *Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter* *(CHItaly ’23)*. Association for Computing Machinery, New York, NY, USA, 1–7. [https://doi.org/10.1145/3605390.3605395](https://doi.org/10.1145/3605390.3605395)
*   Cassell et al. (2000) Justine Cassell, Joseph Sullivan, Scott Prevost, and Elizabeth F. Churchill (Eds.). 2000. *Embodied Conversational Agents*. The MIT Press. [https://doi.org/10.7551/mitpress/2697.001.0001](https://doi.org/10.7551/mitpress/2697.001.0001)
*   Chatain et al. (2023) Julia Chatain, Manu Kapur, and Robert W. Sumner. 2023. Three Perspectives on Embodied Learning in Virtual Reality: Opportunities for Interaction Design. In *Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems*. ACM, Hamburg Germany, 1–8. [https://doi.org/10.1145/3544549.3585805](https://doi.org/10.1145/3544549.3585805)
*   Chen et al. (2021) Ching-Huei Chen, Hsiu-Ting Hung, and Hui-Chin Yeh. 2021. Virtual reality in problem-based learning contexts: Effects on the problem-solving performance, vocabulary acquisition and motivation of English language learners. *Journal of Computer Assisted Learning* 37, 3 (2021), 851–860. [https://doi.org/10.1111/jcal.12528](https://doi.org/10.1111/jcal.12528) _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12528.
*   Cheng et al. (2017) Alan Cheng, Lei Yang, and Erik Andersen. 2017. Teaching Language and Culture with a Virtual Reality Game. In *Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems*. ACM, Denver Colorado USA, 541–549. [https://doi.org/10.1145/3025453.3025857](https://doi.org/10.1145/3025453.3025857)
*   Cheng et al. (2023) Myra Cheng, Tiziano Piccardi, and Diyi Yang. 2023. CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations. [https://arxiv.org/abs/2310.11501v1](https://arxiv.org/abs/2310.11501v1)
*   Cho et al. (2022) Yeonhee Cho, Hao Ning Hsu, Zhewen Zheng, Emily Elizabeth Trinh, HyunYoung Jang, and Yusi Cheng. 2022. Research Based on Affective Filter Theory: Is Social VR an Effective Tool for Learning a Second Language?. In *2022 8th International Conference of the Immersive Learning Research Network (iLRN)*. 1–7. [https://doi.org/10.23919/iLRN55037.2022.9815924](https://doi.org/10.23919/iLRN55037.2022.9815924)
*   Churchill et al. (2000) Elizabeth F. Churchill, Linda Cook, Peter Hodgson, Scott Prevost, and Joseph W. Sullivan. 2000. ”May I Help You?”: Designing Embodied Conversational Agent Allies. In *Embodied Conversational Agents*, Justine Cassell, Joseph Sullivan, Scott Prevost, and Elizabeth F. Churchill (Eds.). The MIT Press, 64–94. [https://doi.org/10.7551/mitpress/2697.003.0005](https://doi.org/10.7551/mitpress/2697.003.0005)
*   COE (2024) COE. 2024. The CEFR Levels - Common European Framework of Reference for Languages (CEFR). [https://www.coe.int/en/web/common-european-framework-reference-languages/level-descriptions](https://www.coe.int/en/web/common-european-framework-reference-languages/level-descriptions)
*   Council (2024) British Council. 2024. Lesson plans | TeachingEnglish | British Council. [https://www.teachingenglish.org.uk/teaching-resources/teaching-adults/lesson-plans](https://www.teachingenglish.org.uk/teaching-resources/teaching-adults/lesson-plans)
*   Darsih (2018) Endang Darsih. 2018. LEARNER-CENTERED TEACHING: WHAT MAKES IT EFFECTIVE. *Indonesian EFL Journal* 4, 1 (Jan. 2018), 33–42. [https://doi.org/10.25134/ieflj.v4i1.796](https://doi.org/10.25134/ieflj.v4i1.796)
*   Dhimolea et al. (2022) Tetyana Kucher Dhimolea, Regina Kaplan-Rakowski, and Lin Lin. 2022. A Systematic Review of Research on High-Immersion Virtual Reality for Language Learning. *TechTrends* 66, 5 (Sept. 2022), 810–824. [https://doi.org/10.1007/s11528-022-00717-w](https://doi.org/10.1007/s11528-022-00717-w)
*   Dinçer et al. (2013) Ali Dinçer, Ali Göksu, Ayşegül Takkaç, and Mine Yazıcı. 2013. Common Characteristics Of An Effective English Language Teacher. *International Journal of Educational Researchers* 4, 3 (Sept. 2013), 1–8. [https://dergipark.org.tr/en/pub/ijers/issue/8498/105657](https://dergipark.org.tr/en/pub/ijers/issue/8498/105657) Number: 3 Publisher: Eğitim Araştırmaları Birliği Derneği.
*   Divekar* et al. (2022) Rahul R. Divekar*, Jaimie Drozdal*, Samuel Chabot*, Yalun Zhou, Hui Su, Yue Chen, Houming Zhu, James A. Hendler, and Jonas Braasch. 2022. Foreign language acquisition via artificial intelligence and extended reality: design and evaluation. *Computer Assisted Language Learning* 35, 9 (Dec. 2022), 2332–2360. [https://doi.org/10.1080/09588221.2021.1879162](https://doi.org/10.1080/09588221.2021.1879162) Publisher: Routledge _eprint: https://doi.org/10.1080/09588221.2021.1879162.
*   Duolingo (2023) Duolingo. 2023. Introducing Duolingo Max, a learning experience powered by GPT-4. [https://blog.duolingo.com/duolingo-max/](https://blog.duolingo.com/duolingo-max/)
*   Fuhrman et al. (2021) Orly Fuhrman, Anabel Eckerling, Naama Friedmann, Ricardo Tarrasch, and Gal Raz. 2021. The moving learner: Object manipulation in virtual reality improves vocabulary learning. *Journal of Computer Assisted Learning* 37, 3 (2021), 672–683. [https://doi.org/10.1111/jcal.12515](https://doi.org/10.1111/jcal.12515) _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12515.
*   Gruber and Kaplan-Rakowski (2020) Alice Gruber and Regina Kaplan-Rakowski. 2020. User experience of public speaking practice in virtual reality. In *Cognitive and affective perspectives on immersive technology in education*. IGI Global, 235–249. [https://www.igi-global.com/chapter/user-experience-of-public-speaking-practice-in-virtual-reality/253698](https://www.igi-global.com/chapter/user-experience-of-public-speaking-practice-in-virtual-reality/253698)
*   Hong et al. (2022) Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. 2022. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. [https://doi.org/10.48550/arXiv.2205.08535](https://doi.org/10.48550/arXiv.2205.08535) arXiv:2205.08535 [cs].
*   Hou et al. (2024) Yuki Hou, Haruki Tamoto, and Homei Miyashita. 2024. ”My agent understands me better”: Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents. In *Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems* *(CHI EA ’24)*. Association for Computing Machinery, New York, NY, USA, 1–7. [https://doi.org/10.1145/3613905.3650839](https://doi.org/10.1145/3613905.3650839)
*   Hsu et al. (2023) Chia Hsu, Yu Chen, Yu-Jung Liu, Yu-Cheng Chang, and Min-Jui Lee. 2023. Spelland: Situated Language Learning with a Mixed-Reality Spelling Game through Everyday Objects. In *Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems*. ACM, Hamburg Germany, 1–6. [https://doi.org/10.1145/3544549.3583830](https://doi.org/10.1145/3544549.3583830)
*   IELTS (2024) IELTS. 2024. IELTS scoring in detail for test takers. [https://ielts.org/take-a-test/preparation-resources/understanding-your-score/ielts-scoring-in-detail](https://ielts.org/take-a-test/preparation-resources/understanding-your-score/ielts-scoring-in-detail)
*   Jackson and Fagan (2000) Randolph L. Jackson and Eileen Fagan. 2000. Collaboration and learning within immersive virtual reality. In *Proceedings of the third international conference on Collaborative virtual environments*. ACM, San Francisco California USA, 83–92. [https://doi.org/10.1145/351006.351018](https://doi.org/10.1145/351006.351018)
*   Jeon (2022) Jaeho Jeon. 2022. Exploring a self-directed interactive app for informal EFL learning: a self-determination theory perspective. *Education and Information Technologies* 27, 4 (May 2022), 5767–5787. [https://doi.org/10.1007/s10639-021-10839-y](https://doi.org/10.1007/s10639-021-10839-y)
*   Jeon et al. (2023) Jaeho Jeon, Seongyong Lee, and Hohsung Choe. 2023. Beyond ChatGPT: A conceptual framework and systematic review of speech-recognition chatbots for language learning. *Computers & Education* 206 (Dec. 2023), 104898. [https://doi.org/10.1016/j.compedu.2023.104898](https://doi.org/10.1016/j.compedu.2023.104898)
*   Jouzu Juls (2022) Jouzu Juls. 2022. VRChat is a TRAP for Language Learners. [https://www.youtube.com/watch?v=O4AgrFL7Kxk](https://www.youtube.com/watch?v=O4AgrFL7Kxk)
*   Kaplan-Rakowski and Gruber (2021) Regina Kaplan-Rakowski and Alice Gruber. 2021. One-On-One Foreign Language Speaking Practice in High-Immersion Virtual Reality. In *Contextual Language Learning: Real Language Learning on the Continuum from Virtuality to Reality*, Yu-Ju Lan and Scott Grant (Eds.). Springer, Singapore, 187–202. [https://doi.org/10.1007/978-981-16-3416-1_9](https://doi.org/10.1007/978-981-16-3416-1_9)
*   Kasneci et al. (2023) Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. 2023. ChatGPT for good? On opportunities and challenges of large language models for education. *Learning and Individual Differences* 103 (April 2023), 102274. [https://doi.org/10.1016/j.lindif.2023.102274](https://doi.org/10.1016/j.lindif.2023.102274)
*   Khattab et al. (2023) Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. [https://doi.org/10.48550/arXiv.2310.03714](https://doi.org/10.48550/arXiv.2310.03714) arXiv:2310.03714 [cs].
*   Klimova (2021) Blanka Klimova. 2021. Use of Virtual Reality in Non-Native Language Learning and Teaching. *Procedia Computer Science* 192 (Jan. 2021), 1385–1392. [https://doi.org/10.1016/j.procs.2021.08.141](https://doi.org/10.1016/j.procs.2021.08.141)
*   ko fi.com (2024) ko fi.com. 2024. Celeste-AI. [https://github.com/Celeste-AI/Celeste-AI](https://github.com/Celeste-AI/Celeste-AI) original-date: 2023-04-11T02:28:07Z.
*   Koulouris et al. (2020) Jordan Koulouris, Zoe Jeffery, James Best, Eamonn O’Neill, and Christof Lutteroth. 2020. Me vs. Super(wo)man: Effects of Customization and Identification in a VR Exergame. In *Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems* *(CHI ’20)*. Association for Computing Machinery, New York, NY, USA, 1–17. [https://doi.org/10.1145/3313831.3376661](https://doi.org/10.1145/3313831.3376661)
*   Kumaran et al. (2023) Vikram Kumaran, Jonathan Rowe, Bradford Mott, and James Lester. 2023. SceneCraft: Automating Interactive Narrative Scene Generation in Digital Games with Large Language Models. *Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment* 19, 1 (Oct. 2023), 86–96. [https://doi.org/10.1609/aiide.v19i1.27504](https://doi.org/10.1609/aiide.v19i1.27504) Number: 1.
*   Lan (2020) Yu-Ju Lan. 2020. Chapter One - Immersion into virtual reality for language learning. In *Psychology of Learning and Motivation*, Kara D. Federmeier and Hsu-Wen Huang (Eds.). Adult and Second Language Learning, Vol. 72\. Academic Press, 1–26. [https://doi.org/10.1016/bs.plm.2020.03.001](https://doi.org/10.1016/bs.plm.2020.03.001)
*   Lave and Wenger (1991) Jean Lave and Etienne Wenger. 1991. *Situated Learning: Legitimate Peripheral Participation*. Cambridge University Press. Google-Books-ID: CAVIOrW3vYAC.
*   Law (2024) Locky Law. 2024. Application of generative artificial intelligence (GenAI) in language teaching and learning: A scoping literature review. *Computers and Education Open* 6 (June 2024), 100174. [https://doi.org/10.1016/j.caeo.2024.100174](https://doi.org/10.1016/j.caeo.2024.100174)
*   Lefkowitz (2009) Natalie Lefkowitz. 2009. SECOND LANGUAGE LEARNING AND LANGUAGE TEACHING (4th ed.). Vivian Cook. London: Arnold, 2008\. Pp. x + 306. *Studies in Second Language Acquisition* 31, 4 (Dec. 2009), 660–661. [https://doi.org/10.1017/S0272263109990180](https://doi.org/10.1017/S0272263109990180)
*   Li et al. (2024) Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2024. Hello Again! LLM-powered Personalized Agent for Long-term Dialogue. [https://doi.org/10.48550/arXiv.2406.05925](https://doi.org/10.48550/arXiv.2406.05925) arXiv:2406.05925 [cs].
*   Limniou et al. (2008) Maria Limniou, David Roberts, and Nikos Papadopoulos. 2008. Full immersive virtual environment CAVETM in chemistry education. *Computers & Education* 51, 2 (Sept. 2008), 584–593. [https://doi.org/10.1016/j.compedu.2007.06.014](https://doi.org/10.1016/j.compedu.2007.06.014)
*   Lin and Lan (2015) Tsun-Ju Lin and Yu-Ju Lan. 2015. Language Learning in Virtual Reality Environments: Past, Present, and Future. *Journal of Educational Technology & Society* 18, 4 (2015), 486–497. [https://www.jstor.org/stable/jeductechsoci.18.4.486](https://www.jstor.org/stable/jeductechsoci.18.4.486) Publisher: International Forum of Educational Technology & Society.
*   Liu et al. (2024) Ziyi Liu, Zhengzhe Zhu, Lijun Zhu, Enze Jiang, Xiyun Hu, Kylie A Peppler, and Karthik Ramani. 2024. ClassMeta: Designing Interactive Virtual Classmate to Promote VR Classroom Participation. In *Proceedings of the CHI Conference on Human Factors in Computing Systems* (Honolulu, HI, USA) *(CHI ’24)*. Association for Computing Machinery, New York, NY, USA, Article 659, 17 pages. [https://doi.org/10.1145/3613904.3642947](https://doi.org/10.1145/3613904.3642947)
*   Makransky and Lilleholt (2018) Guido Makransky and Lau Lilleholt. 2018. A structural equation modeling investigation of the emotional value of immersive virtual reality in education. *Educational Technology Research and Development* 66, 5 (Oct. 2018), 1141–1164. [https://doi.org/10.1007/s11423-018-9581-2](https://doi.org/10.1007/s11423-018-9581-2)
*   Merle_Meruru555 (2024) Merle_Meruru555\. 2024. EN-JP Language Exchange every weekend! come join! details below. [www.reddit.com/r/VRchat/comments/1eyelee/enjp_language_exchange_every_weekend_come_join/](www.reddit.com/r/VRchat/comments/1eyelee/enjp_language_exchange_every_weekend_come_join/)
*   Normoyle et al. (2024) Aline Normoyle, João Sedoc, and Funda Durupinar. 2024. Using LLMs to Animate Interactive Story Characters with Emotions and Personality. In *2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)*. IEEE, 632–635. [https://doi.org/10.1109/VRW62533.2024.00124](https://doi.org/10.1109/VRW62533.2024.00124)
*   OpenAI (2024a) OpenAI. 2024a. Hello GPT-4o. [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)
*   OpenAI (2024b) OpenAI. 2024b. TTS OpenAI Text to Speech. [https://platform.openai.com/docs/guides/text-to-speech/overview](https://platform.openai.com/docs/guides/text-to-speech/overview)
*   OpenAI (2024c) OpenAI. 2024c. Whisper API. [https://platform.openai.com/docs/guides/speech-to-text](https://platform.openai.com/docs/guides/speech-to-text)
*   Petersen et al. (2021) Gustav Bøg Petersen, Aske Mottelson, and Guido Makransky. 2021. Pedagogical Agents in Educational VR: An in the Wild Study. In *Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems* *(CHI ’21)*. Association for Computing Machinery, New York, NY, USA, 1–12. [https://doi.org/10.1145/3411764.3445760](https://doi.org/10.1145/3411764.3445760)
*   Peterson (2012) Mark Peterson. 2012. EFL learner collaborative interaction in Second Life. *ReCALL* 24, 1 (Jan. 2012), 20–39. [https://doi.org/10.1017/S0958344011000279](https://doi.org/10.1017/S0958344011000279)
*   Python_Sounddevice (2024) Python_Sounddevice. 2024. Play and Record Sound with Python — python-sounddevice, version 0.5.0. [https://python-sounddevice.readthedocs.io/en/0.5.0/](https://python-sounddevice.readthedocs.io/en/0.5.0/)
*   Python_Vosk (2024) Python_Vosk. 2024. vosk: Offline open source speech recognition API based on Kaldi and Vosk. [https://github.com/alphacep/vosk-api](https://github.com/alphacep/vosk-api)
*   Qing et al. (2023) Zhongfei Qing, Zhongang Cai, Zhitao Yang, and Lei Yang. 2023. Story-to-Motion: Synthesizing Infinite and Controllable Character Animation from Long Text. In *SIGGRAPH Asia 2023 Technical Communications* *(SA ’23)*. Association for Computing Machinery, New York, NY, USA, 1–4. [https://doi.org/10.1145/3610543.3626176](https://doi.org/10.1145/3610543.3626176)
*   Raux and Eskenazi (2004) Antoine Raux and Maxine Eskenazi. 2004. Using task-oriented spoken dialogue systems for language learning: potential, practical applications and challenges. In *InSTIL/ICALL Symposium 2004*. Citeseer. [https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=cb20467d1b74a5304f84fdc0e12b96710117a6a0](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=cb20467d1b74a5304f84fdc0e12b96710117a6a0)
*   Roy and Aimee (2020) Roy and Aimee. 2020. Can you actually learn JAPANESE using VRChat? [https://www.youtube.com/watch?v=c8IrEVhDVsk](https://www.youtube.com/watch?v=c8IrEVhDVsk)
*   Schulhoff et al. (2024) Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, HyoJung Han, Sevien Schulhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta Agrawal, Chau Pham, Gerson Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevander Da Costa, Saloni Gupta, Megan L. Rogers, Inna Goncearenco, Giuseppe Sarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anadkat, Alexander Hoyle, and Philip Resnik. 2024. The Prompt Report: A Systematic Survey of Prompting Techniques. [http://arxiv.org/abs/2406.06608](http://arxiv.org/abs/2406.06608) arXiv:2406.06608 [cs].
*   Seow (2023) Olivia Seow. 2023. LingoLand: An AI-Assisted Immersive Game for Language Learning. In *Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology*. ACM, San Francisco CA USA, 1–3. [https://doi.org/10.1145/3586182.3625117](https://doi.org/10.1145/3586182.3625117)
*   Siyan et al. (2024) Li Siyan, Teresa Shao, Zhou Yu, and Julia Hirschberg. 2024. Using Adaptive Empathetic Responses for Teaching English. [https://doi.org/10.48550/arXiv.2404.13764](https://doi.org/10.48550/arXiv.2404.13764) arXiv:2404.13764 [cs].
*   Sonlu et al. (2024) Sinan Sonlu, Bennie Bendiksen, Funda Durupinar, and Uğur Güdükbay. 2024. The Effects of Embodiment and Personality Expression on Learning in LLM-based Educational Agents. [https://doi.org/10.48550/arXiv.2407.10993](https://doi.org/10.48550/arXiv.2407.10993) arXiv:2407.10993 [cs].
*   Tyen et al. (2022) Gladys Tyen, Mark Brenchley, Andrew Caines, and Paula Buttery. 2022. Towards an open-domain chatbot for language practice. In *Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022)*, Ekaterina Kochmar, Jill Burstein, Andrea Horbach, Ronja Laarmann-Quante, Nitin Madnani, Anaïs Tack, Victoria Yaneva, Zheng Yuan, and Torsten Zesch (Eds.). Association for Computational Linguistics, Seattle, Washington, 234–249. [https://doi.org/10.18653/v1/2022.bea-1.28](https://doi.org/10.18653/v1/2022.bea-1.28)
*   Unity (2024) Unity. 2024. Unity. [https://unity.com/](https://unity.com/)
*   van Mulken et al. (1998) Susanne van Mulken, Elisabeth André, and Jochen Müller. 1998. The Persona Effect: How Substantial Is It?. In *People and Computers XIII*, Hilary Johnson, Lawrence Nigay, and Christopher Roast (Eds.). Springer, London, 53–66. [https://doi.org/10.1007/978-1-4471-3605-7_4](https://doi.org/10.1007/978-1-4471-3605-7_4)
*   VRChat (2024a) VRChat. 2024a. OSC Overview. [https://docs.vrchat.com/docs/osc-overview](https://docs.vrchat.com/docs/osc-overview)
*   VRChat (2024b) VRChat. 2024b. VRChat. [https://hello.vrchat.com](https://hello.vrchat.com)
*   Wan et al. (2024) Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao, Dakuo Wang, Yvonne Coady, and Mirjana Prpa. 2024. Building LLM-based AI Agents in Social Virtual Reality. In *Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems* *(CHI EA ’24)*. Association for Computing Machinery, New York, NY, USA, 1–7. [https://doi.org/10.1145/3613905.3651026](https://doi.org/10.1145/3613905.3651026)
*   Wang et al. (2023) Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. 2023. Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes. [https://doi.org/10.48550/arXiv.2308.08769](https://doi.org/10.48550/arXiv.2308.08769) arXiv:2308.08769 [cs].
*   Wik and Hjalmarsson (2009) Preben Wik and Anna Hjalmarsson. 2009. Embodied conversational agents in computer assisted language learning. *Speech Communication* 51, 10 (Oct. 2009), 1024–1037. [https://doi.org/10.1016/j.specom.2009.05.006](https://doi.org/10.1016/j.specom.2009.05.006)
*   Wong et al. (2018) Man Ho Ivy Wong, Helen Zhao, and Brian MacWhinney. 2018. A Cognitive Linguistics Application for Second Language Pedagogy: The English Preposition Tutor. *Language Learning* 68, 2 (2018), 438–468. [https://doi.org/10.1111/lang.12278](https://doi.org/10.1111/lang.12278) _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12278.
*   Woodrow (2006) Lindy Woodrow. 2006. Anxiety and Speaking English as a Second Language. *RELC Journal* (Dec. 2006). [https://doi.org/10.1177/0033688206071315](https://doi.org/10.1177/0033688206071315) Publisher: Sage PublicationsSage CA: Thousand Oaks, CA.
*   Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W. White, Doug Burger, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. [https://doi.org/10.48550/arXiv.2308.08155](https://doi.org/10.48550/arXiv.2308.08155) arXiv:2308.08155 [cs].
*   Xu et al. (2021) Ying Xu, Dakuo Wang, Penelope Collins, Hyelim Lee, and Mark Warschauer. 2021. Same benefits, different communication patterns: Comparing Children’s reading with a conversational agent vs. a human partner. *Computers & Education* 161 (Feb. 2021), 104059. [https://doi.org/10.1016/j.compedu.2020.104059](https://doi.org/10.1016/j.compedu.2020.104059)
*   Yang et al. (2024) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. *ACM Transactions on Knowledge Discovery from Data* 18, 6 (July 2024), 1–32. [https://doi.org/10.1145/3649506](https://doi.org/10.1145/3649506)
*   Yi et al. (2024) Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, and Ying Shen. 2024. A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems. [https://doi.org/10.48550/arXiv.2402.18013](https://doi.org/10.48550/arXiv.2402.18013) arXiv:2402.18013 [cs].
*   Zamfirescu-Pereira et al. (2023) J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. In *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems* *(CHI ’23)*. Association for Computing Machinery, New York, NY, USA, 1–21. [https://doi.org/10.1145/3544548.3581388](https://doi.org/10.1145/3544548.3581388)
*   Zhang et al. (2023) Ruofei Zhang, Di Zou, and Gary Cheng. 2023. Chatbot-based learning of logical fallacies in EFL writing: perceived effectiveness in improving target knowledge and learner motivation. *Interactive Learning Environments* 0, 0 (2023), 1–18. [https://doi.org/10.1080/10494820.2023.2220374](https://doi.org/10.1080/10494820.2023.2220374) Publisher: Routledge _eprint: https://doi.org/10.1080/10494820.2023.2220374.
*   Zhang et al. (2021) Xinlei Zhang, Takashi Miyaki, and Jun Rekimoto. 2021. JustSpeak: Automated, User-Configurable, Interactive Agents for Speech Tutoring. *Proc. ACM Hum.-Comput. Interact.* 5, EICS (2021), 202:1–202:24. [https://doi.org/10.1145/3459744](https://doi.org/10.1145/3459744)
*   Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. [https://doi.org/10.48550/arXiv.2306.05685](https://doi.org/10.48550/arXiv.2306.05685) arXiv:2306.05685 [cs].
*   Zhu et al. (2023) Zhengzhe Zhu, Ziyi Liu, Youyou Zhang, Lijun Zhu, Joey Huang, Ana M Villanueva, Xun Qian, Kylie Peppler, and Karthik Ramani. 2023. LearnIoTVR: An End-to-End Virtual Reality Environment Providing Authentic Learning Experiences for Internet of Things. In *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems* *(CHI ’23)*. Association for Computing Machinery, New York, NY, USA, 1–17. [https://doi.org/10.1145/3544548.3581396](https://doi.org/10.1145/3544548.3581396)

## Appendix A Appendix

### A.1\. Single System Prompt

The following content is the single prompt: context = ”””You are a friendly, and very patient language partner, who also understands the user’s cultural background. Ensure that your responses do not contain greetings like Hello.”””

steps = ”””Here are the steps for the role-play language practice:

1\. **Initial Assessment**: - Assess the user’s spoken language level by conducting an initial free-form conversation. - Follow the criteria by the CEFR from this website: [CEFR Criteria](https://www.coe.int/en/web/common-european-framework-reference-languages/table-3-cefr-3.3-common-reference-levels-qualitative-aspects-of-spoken-language-use). - Assess only once during the whole conversation. - Tell the user the assessment result and adjust your vocabulary difficulty levels based on the user’s level.

2\. **Scenario Selection**: - Ask the user if they have a specific scenario in mind for practicing. - If not, suggest three different types of real-life scenarios and have the user pick one. - Each scenario should be different. Ask for a new scenario only when the current role-play is over. - Example: In one scenario, you are a colleague with the user and you work at the same company. You invite the user to have lunch together. Discuss what food to eat and about food from your culture.

3\. **Role-Play Execution**: - Introduce the user to around 5 new vocabularies for any scenario. If the user doesn’t know the meaning of the vocabulary, explain and practice pronunciation with them. - If the user’s response doesn’t make sense, clarify and let the user repeat or rephrase. - If the user has difficulty expressing themselves, provide scaffolding by encouraging them, like ”You’re doing great” or ”Come on, don’t give up”, or offering support. - If the user’s response is too short, provide scaffolding by suggesting an example reply and asking the user to practice with you.

4\. **Engagement**: - Make the conversation interesting and engaging. - Continue the role-play conversation for every topic unless the user explicitly wants to end the topic. - After each user response, ask engaging follow-up questions or introduce new related topics to keep the conversation going.

5\. **Feedback**: - When the role-play is over, give the user balanced feedback, taking into account the difficulty level of the conversation. - Summarize the vocabulary, grammar, or sentences they have learned for future revisit.

6\. **Instructions to Continue**: - If the user responds, acknowledge it and ask a follow-up question or provide an engaging prompt to continue the conversation. - Only end the role-play if the user explicitly states they want to stop or change the topic.

### A.2\. Multi-task Prompt

[⬇](data:text/plain;base64,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)1.  Introduction{"role":  "system",  "content":  "You  are  a  friendly,  patient,  and  light-hearted  English  language  tutor."},{"role":  "user",  "content":  "  Greet  me  and  chat  with  me  to  get  to  know  me  better,  such  as  my  name,  cultural  background,  and  why  learning  English,  ask  one  question  at  a  time  and  wait  for  my  response  to  continue.  If  you  know  my  cultural  background,  greet  me  in  the  language  of  that  culture.  "}2.  Assessment{"role":  "system",  "content":  "You  are  an  English  language  tutor  assessing  the  user’s  language  proficiency.  You  are  familiar  with  the  criteria  of  the  CEFR  from  this  website:  [CEFR  Criteria](https://www.coe.int/en/web/common-european-framework-reference-languages/table-3-cefr-3.3-common-reference-levels-qualitative-aspects-of-spoken-language-use)."},{"role":  "user",  "content":  "Wrap  up  our  previous  conversations  in  one  sentence,  here  are  the  previous  conversations:  {user_info_conversation}.  Transit  to  assessing  my  language  level.  Ask  me  to  describe  a  memorable  experience  and  tell  me  the  assessment  result  based  on  the  criteria  by  the  CEFR  from  this  website:  [CEFR  Criteria](https://www.coe.int/en/web/common-european-framework-reference-languages/table-3-cefr-3.3-common-reference-levels-qualitative-aspects-of-spoken-language-use)."}3.  Generate  Role-Play  Scenarios{"role":  "system",  "content":  "You  are  a  friendly,  patient,  and  light-hearted  English  language  tutor.  You  are  asking  the  user  to  choose  a  role-play  scenario."},{"role":  "assistant",  "content":  "Suggest  me  three  real-life  scenarios  I  can  practice.  For  example,  ordering  food  at  a  restaurant,  a  job  interview,  or  traveling  in  an  English-speaking  country.  Do  not  list  them  in  numbers."}4.  Role-Play  Conversation  Practice{"role":  "system",  "content":  f"""You  are  a  friendly,  patient,  and  light-hearted  English  language  tutor  engaging  in  a  role-play  scenario:  {scenario}  to  help  the  user  practice  listening  and  speaking  skills.Adjust  your  language  difficulty  level  by  the  assessment  result  from  previous  conversations:  {assessment}.  Think  how  you  can  help  a  learner  to  learn  better."""},{"role":  "assistant",  "content":  f"""Explain  to  the  user  that  you’re  helping  them  practice  speaking  English  by  conducting  role-play  conversations.  Start  the  role  play.  You’re  at  a  {scenario}.  Start  by  describing  the  scene  and  playing  the  role.  Let  the  user  continue  the  conversation.If  the  user  doesn’t  know  the  meaning  of  a  vocabulary,  explain  and  practice  pronunciation  with  them.If  the  user’s  response  doesn’t  make  sense,  clarify  and  let  the  user  repeat  or  rephrase.If  the  user  has  difficulty  expressing  themselves,  provide  scaffolding  by  encouraging  them,  like  "You’re  doing  great",  "Come  on,  don’t  give  up",  or  offering  support.If  the  user’s  response  is  too  short,  provide  scaffolding  or  suggest  an  example  reply  and  ask  the  user  to  practice  with  you."""}5  Providing  Feedback{"role":  "system",  "content":  "You  are  a  friendly,  patient  and  light-hearted  English  language  tutor.  You  are  providing  feedback  on  the  user’s  English  conversation  practices."},{"role":  "assistant",  "content":  """Based  on  previous  role-play  conversation  practices:  {role_play_conversations}.  Summarize  the  vocabulary,  grammar,  or  sentences  they  have  learned  for  future  revisit.Feedback  should  be  in  the  following  format:**GENERAL  FEEDBACK**:  Assess  performance  given  the  lesson,  name  one  thing  the  student  did  really  well,  and  one  thing  the  student  could  improve  on.**ADVICE  MOVING  FORWARD**:  Give  students  advice  on  how  to  apply  the  lesson  in  the  real  world."""}

## Appendix B Interview Questions

The following questions were asked during the semi-structured interviews conducted as part of the user study:

| Focus Area | Interview Question |
| --- | --- |
| Initial Thoughts | Tell me your initial thoughts about the interaction with ELLMA-T. |
| Skill Assessment | How well did it assess your skills? |
| Feedback Quality | What did you think of the feedback? What was helpful or not? |
| Comfort Level | How comfortable do you feel interacting with the agent? |
| Engagement | How engaging do you find the interactions with the agent? |
| Interaction Mode | Would you prefer interacting with the agent alone or in a group setting within VRChat? Why? |
| Likelihood of Continued Use | How likely are you to continue using this system over a long period? |

Table 3\. List of interview questions used in the study.