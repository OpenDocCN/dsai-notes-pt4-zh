- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Synthetic Dialogue Dataset Generation using LLM Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM代理生成合成对话数据集
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17461](https://ar5iv.labs.arxiv.org/html/2401.17461)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.17461](https://ar5iv.labs.arxiv.org/html/2401.17461)
- en: Yelaman Abdullin    Diego Molla-Aliod
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 叶拉曼·阿布杜林    迭戈·莫利亚-阿利奥德
- en: Macquarie University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 麦考瑞大学
- en: yelaman.abdullin@hdr.mq.edu.au, diego.molla-aliod@mq.edu.au \ANDBahadorreza
    Ofoghi    John Yearwood
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: yelaman.abdullin@hdr.mq.edu.au, diego.molla-aliod@mq.edu.au \AND巴哈多雷扎·奥福吉   
    约翰·耶尔伍德
- en: Deakin University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 迪肯大学
- en: '{b.ofoghi,john.yearwood}@deakin.edu.au &Qingyang Li'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{b.ofoghi,john.yearwood}@deakin.edu.au &李青阳'
- en: The University of Melbourne
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 墨尔本大学
- en: ql5@student.unimelb.edu.au
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ql5@student.unimelb.edu.au
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Linear programming (LP) problems are pervasive in real-life applications. However,
    despite their apparent simplicity, an untrained user may find it difficult to
    determine the linear model of their specific problem. We envisage the creation
    of a goal-oriented conversational agent that will engage in conversation with
    the user to elicit all information required so that a subsequent agent can generate
    the linear model. In this paper, we present an approach for the generation of
    sample dialogues that can be used to develop and train such a conversational agent.
    Using prompt engineering, we develop two agents that “talk” to each other, one
    acting as the conversational agent, and the other acting as the user. Using a
    set of text descriptions of linear problems from NL4Opt available to the user
    only, the agent and the user engage in conversation until the agent has retrieved
    all key information from the original problem description. We also propose an
    extrinsic evaluation of the dialogues by assessing how well the summaries generated
    by the dialogues match the original problem descriptions. We conduct human and
    automatic evaluations, including an evaluation approach that uses GPT-4 to mimic
    the human evaluation metrics. The evaluation results show an overall good quality
    of the dialogues, though research is still needed to improve the quality of the
    GPT-4 evaluation metrics. The resulting dialogues, including the human annotations
    of a subset, are available to the research community. The conversational agent
    used for the generation of the dialogues can be used as a baseline.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划（LP）问题在现实应用中无处不在。然而，尽管它们看似简单，未经训练的用户可能难以确定其特定问题的线性模型。我们设想创建一个目标导向的对话代理，该代理将与用户进行对话，以获取生成线性模型所需的所有信息。本文介绍了一种生成样本对话的方法，这些对话可用于开发和训练这样的对话代理。通过提示工程，我们开发了两个“对话”代理，其中一个充当对话代理，另一个充当用户。利用来自NL4Opt的线性问题的文本描述，代理和用户进行对话，直到代理从原始问题描述中提取出所有关键信息。我们还提出了通过评估对话生成的摘要与原始问题描述的匹配程度来进行对话的外部评估。我们进行人工和自动评估，包括使用GPT-4模拟人工评估指标的评估方法。评估结果显示对话的总体质量良好，但仍需研究以提高GPT-4评估指标的质量。生成的对话，包括部分人工注释，已提供给研究社区。用于生成对话的对话代理可以作为基准使用。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Linear programming (LP) is a mathematical optimization technique widely employed
    to address a multitude of real-world challenges, ranging from resource allocation
    in supply chain management to portfolio optimization in finance. Despite the ubiquity
    of LP problems and their practical importance, many individuals, particularly
    those without specialized mathematical backgrounds, often struggle to formulate
    the appropriate linear models for their specific problem instances. This barrier
    hinders the broader utilization of LP techniques, especially among non-experts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划（LP）是一种数学优化技术，广泛应用于解决各种现实世界的问题，从供应链管理中的资源分配到金融中的投资组合优化。尽管LP问题及其实际重要性非常普遍，但许多人，特别是那些没有专业数学背景的人，通常难以为他们特定的问题实例制定适当的线性模型。这一障碍阻碍了LP技术的更广泛应用，尤其是在非专家群体中。
- en: To mitigate this challenge, we propose the development of a goal-oriented conversational
    agent capable of assisting users in constructing accurate linear models for their
    unique problem scenarios. This conversational agent would engage users in a dialogue,
    eliciting relevant information pertaining to the problem, and subsequently generate
    the corresponding linear model. This paper focuses on an essential aspect of creating
    such an agent –– the generation of synthetic dialogues that can be employed to
    train and evaluate the conversational agent’s performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，我们提出开发一种目标导向的对话代理，该代理能够帮助用户为其独特的问题场景构建准确的线性模型。该对话代理将与用户进行对话，提取与问题相关的信息，并随后生成相应的线性模型。本文专注于创建此类代理的一个关键方面——生成可以用于训练和评估对话代理性能的合成对话。
- en: 'Our methodology leverages prompt engineering to construct two distinct agents:
    one simulating the conversational agent’s behavior, and the other emulating the
    user’s responses during problem-solving interactions. The agents are designed
    to engage in purposeful dialogues aimed at extracting the necessary information
    from the user to construct a valid linear model. To facilitate this process, we
    utilize a set of text descriptions of linear problems, accessible only to the
    user agent, sourced from the NL4Opt dataset Ramamonjison et al. ([2022](#bib.bib9),
    [2023](#bib.bib10)). These text descriptions serve as the basis for the dialogues
    and enable the conversational agent to iteratively gather the critical information
    required for problem formulation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法利用提示工程构建了两个不同的代理：一个模拟对话代理的行为，另一个模拟用户在问题解决互动中的响应。这些代理旨在进行有目的的对话，以从用户那里提取构建有效线性模型所需的信息。为了促进这一过程，我们使用了一组线性问题的文本描述，这些描述仅对用户代理可用，来源于NL4Opt数据集Ramamonjison等人（[2022](#bib.bib9)，[2023](#bib.bib10)）。这些文本描述作为对话的基础，使对话代理能够迭代地收集制定问题所需的关键信息。
- en: In addition to the generation process, we propose an extrinsic evaluation approach
    for assessing the quality and effectiveness of the generated dialogues. Specifically,
    we evaluate how well the summaries generated by the dialogues align with the original
    problem descriptions from NL4Opt. This evaluation encompasses both human assessments,
    where human judges evaluate the quality of dialogue summaries, and automated metrics
    to quantitatively measure the informativeness of the generated summaries.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成过程，我们还提出了一种外部评估方法来评估生成对话的质量和有效性。具体而言，我们评估对话生成的摘要与NL4Opt中的原始问题描述的一致性。这种评估包括人类评估，其中人类评审员评估对话摘要的质量，以及自动化指标，用于定量测量生成摘要的信息量。
- en: Our preliminary results from human and automatic evaluations indicate that the
    generated dialogues exhibit a high degree of fidelity to the original problem
    descriptions, thereby demonstrating the quality of the synthetic dialogues generated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从人类和自动评估中获得的初步结果表明，生成的对话在很大程度上忠实于原始问题描述，从而展示了生成的合成对话的质量。
- en: 'The contributions of this paper are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献包括：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: An approach for the generation of dialogues for the development of goal-oriented
    conversational agents. In this paper, the goal consists of eliciting information
    from the user in order to generate a linear programming model, noting that the
    techniques presented here can be adapted to other goals.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种用于生成对话以开发目标导向的对话代理的方法。在本文中，目标是从用户那里获取信息，以生成线性规划模型，注意到这里提出的技术可以适应其他目标。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: A dataset of 476 dialogues for the development of such a conversational agent,
    of which 28 have been annotated manually.¹¹1[https://github.com/eabdullin/optimouse-quest/](https://github.com/eabdullin/optimouse-quest/)
    Even though these 476 dialogues are generated automatically, since the generation
    process is non-deterministic, separate runs of the same program will generate
    different dialogues. For this reason, we consider that these dialogues form a
    useful dataset for the research community to facilitate reproducibility.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个包含476个对话的数据集，用于开发此类对话代理，其中28个已被手动注释。¹¹1[https://github.com/eabdullin/optimouse-quest/](https://github.com/eabdullin/optimouse-quest/)
    尽管这476个对话是自动生成的，但由于生成过程是非确定性的，相同程序的不同运行将生成不同的对话。因此，我们认为这些对话为研究社区提供了一个有用的数据集，以促进可重复性。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: An extrinsic evaluation approach based on comparing the summaries generated
    by the dialogue, with the original problem description.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种基于比较对话生成的总结与原始问题描述的外部评估方法。
- en: '4.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: An automatic evaluation approach using GPT-4 that mimics the behavior of human
    evaluation.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用GPT-4的自动评估方法模拟人类评估的行为。
- en: 2 Background and Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: 2.1 Linear Programming and NL4Opt
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 线性规划与NL4Opt
- en: LP problems are pervasive in real-life applications. They are commonly utilized
    for resource allocation, planning, scheduling, transportation optimization, portfolio
    management, and numerous other areas. For instance, in production planning, LP
    can help determine how to use limited human, material, and financial resources
    to achieve maximum economic benefits.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划问题在现实生活中无处不在。它们常用于资源分配、规划、调度、交通优化、投资组合管理及其他许多领域。例如，在生产规划中，线性规划可以帮助确定如何利用有限的人力、物料和财务资源以获得最大的经济效益。
- en: LP problems are a class of mathematical optimization problems where the goal
    is to find a set of values for the decision variables that satisfies a set of
    linear constraints and maximizes or minimizes the value of a linear objective
    function Chen et al. ([2011](#bib.bib1)). The general form of an LP problem can
    be formulated as follows,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 线性规划问题是一类数学优化问题，其目标是找到一组满足一组线性约束的决策变量值，并最大化或最小化线性目标函数的值 Chen 等人（[2011](#bib.bib1)）。线性规划问题的一般形式可以表述如下，
- en: '|  | $$\begin{array}[]{ll@{}ll}\text{Maximize}&amp;\mathbf{c}^{T}&amp;\mathbf{x}&amp;\\
    \text{subject to}&amp;\mathbf{A}&amp;\mathbf{x}\leq\mathbf{b}\\'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{array}[]{ll@{}ll}\text{最大化}&amp;\mathbf{c}^{T}&amp;\mathbf{x}&amp;\\
    \text{满足}&amp;\mathbf{A}&amp;\mathbf{x}\leq\mathbf{b}\\'
- en: '&amp;&amp;\mathbf{x}\geq\mathbf{0}\end{array}$$ |  |'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;&amp;\mathbf{x}\geq\mathbf{0}\end{array}$$ |  |'
- en: where $\mathbf{x}$ is a given matrix of constants. Linear programming is widely
    applicable in various domains due to its ability to model real-world optimization
    challenges and the availability of efficient solution algorithms. Once an LP model
    has been formulated for a problem, powerful solvers driven by efficient algorithms
    can help one to solve it, even for surprisingly complicated and large-scale problems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{x}$ 是给定的常数矩阵。线性规划因其能够模拟现实世界中的优化挑战以及高效的解决算法而广泛应用于各个领域。一旦为问题制定了线性规划模型，借助高效算法驱动的强大求解器可以帮助解决它，即使是复杂且大规模的问题。
- en: In 2022, a competition to extract linear programming formulations from natural
    language (NL4Opt) developed the first dataset of linear programming word problems
    Ramamonjison et al. ([2022](#bib.bib9), [2023](#bib.bib10)). It contains 1101
    instances from various domains. These LP problems focus on a variety of common
    contexts such as production, resource allocation, investment allocation, agriculture,
    transportation, health sciences, sales, etc. Each instance has an unstructured
    natural language description of the LP problem involving decision variables, one
    objective function, and several constraints.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，针对从自然语言中提取线性规划公式的竞赛（NL4Opt）开发了第一个线性规划文字问题数据集 Ramamonjison 等人（[2022](#bib.bib9),
    [2023](#bib.bib10)）。该数据集包含了来自各个领域的1101个实例。这些线性规划问题涵盖了多种常见的背景，如生产、资源分配、投资分配、农业、交通、健康科学、销售等。每个实例都有一个涉及决策变量、一个目标函数和几个约束条件的非结构化自然语言描述。
- en: The NL4Opt dataset provides valuable examples of real-world natural language
    descriptions for LP problems, showcasing a range of constraint types. We deconstruct
    each problem description in the development subset into an objective function
    description and several constraint descriptions. By analyzing these constraints,
    we find that they can be categorized into nine refined classes in Table [1](#S2.T1
    "Table 1 ‣ 2.1 Linear Programming and NL4Opt ‣ 2 Background and Related Work ‣
    Synthetic Dialogue Dataset Generation using LLM Agents"). Different instances
    of the dataset consist of different combinations of these constraint types. Constraint
    types 1 to 3 and 5 to 7 are frequently used to represent capacity limits, budget
    constraints, or resource availability. Types 4, 8, and 9 impose ratio control
    and balancing between different quantities. These nine types of constraints are
    often encountered in real-world problems and can encompass a substantial portion
    of common constraints.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: NL4Opt 数据集提供了真实世界自然语言描述线性规划问题的宝贵示例，展示了各种约束类型。我们将开发子集中的每个问题描述分解为目标函数描述和若干约束描述。通过分析这些约束，我们发现它们可以归类为表
    [1](#S2.T1 "表 1 ‣ 2.1 线性规划与 NL4Opt ‣ 2 背景与相关工作 ‣ 使用 LLM 代理生成合成对话") 中的九种精细分类。数据集的不同实例由这些约束类型的不同组合组成。约束类型
    1 到 3 以及 5 到 7 常用于表示容量限制、预算约束或资源可用性。类型 4、8 和 9 施加比例控制和不同数量之间的平衡。这九种类型的约束在实际问题中经常遇到，涵盖了常见约束的相当一部分。
- en: '|  | Constraint type | Math inequality |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | 约束类型 | 数学不等式 |'
- en: '| 1 | Upper bound on single variable | $x_{i}\leq b$ |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 单变量的上界 | $x_{i}\leq b$ |'
- en: '| 2 | Upper bound on sum of variables | $\sum_{i}x_{i}\leq b$ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 变量和的上界 | $\sum_{i}x_{i}\leq b$ |'
- en: '| 3 | Upper bound on weighted sum of variables | $\sum_{i}a_{i}x_{i}\leq b$
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 加权和的上界 | $\sum_{i}a_{i}x_{i}\leq b$ |'
- en: '| 4 | Upper bound on proportion | $x_{j}\leq c\sum_{i}x_{i}$ |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 比例的上界 | $x_{j}\leq c\sum_{i}x_{i}$ |'
- en: '| 5 | Lower bound on single variable | $x_{i}\geq b$ |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 单变量的下界 | $x_{i}\geq b$ |'
- en: '| 6 | Lower bound on sum of variables | $\sum_{i}x_{i}\geq b$ |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 变量和的下界 | $\sum_{i}x_{i}\geq b$ |'
- en: '| 7 | Lower bound on weighted sum of variables | $\sum_{i}a_{i}x_{i}\geq b$
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 加权和的下界 | $\sum_{i}a_{i}x_{i}\geq b$ |'
- en: '| 8 | Lower bound on proportion | $x_{j}\geq c\sum_{i}x_{i}$ |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 比例的下界 | $x_{j}\geq c\sum_{i}x_{i}$ |'
- en: '| 9 | Comparison constraints | $dx_{i}\leq x_{j}$ |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 比较约束 | $dx_{i}\leq x_{j}$ |'
- en: 'Table 1: Classification of constraints. Suppose an LP problem has $n$.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：约束分类。假设一个线性规划问题有$n$。
- en: 2.2 Evaluation Methods
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 评估方法
- en: Evaluation of dialogue systems is a complex endeavor, typically involving a
    blend of automated metrics and human assessments. A traditional automated metric
    such as ROUGE Lin ([2004](#bib.bib3)) is frequently employed for measuring textual
    similarity and evaluating information overlap. However, this metric could be improved
    in its ability to evaluate semantic coherence and the effective fulfillment of
    dialogue goals Liu et al. ([2016](#bib.bib4)). In this work, we calculated ROUGE-1,
    ROUGE-2, and ROUGE-L scores to cover unigram, bigram, and longest common subsequence
    overlaps, respectively.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统的评估是一项复杂的工作，通常涉及自动化指标和人工评估的结合。传统的自动化指标如 ROUGE Lin ([2004](#bib.bib3)) 常用于测量文本相似性和评估信息重叠。然而，这一指标在评估语义连贯性和有效实现对话目标的能力方面仍有改进空间，Liu
    等人 ([2016](#bib.bib4)) 提出过这一点。在这项工作中，我们计算了 ROUGE-1、ROUGE-2 和 ROUGE-L 分数，分别涵盖了
    unigram、bigram 和最长公共子序列的重叠。
- en: Recent advancements have directed more sophisticated evaluation methods, including
    BERTScore Zhang et al. ([2020](#bib.bib13)), which leverages contextual embeddings
    from pre-trained BERT models to assess semantic similarity between generated and
    reference text. BERTScore complements ROUGE by adding a more nuanced semantic
    layer to the evaluation, capturing aspects that traditional metrics may miss.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的进展引入了更复杂的评估方法，包括 BERTScore Zhang 等人 ([2020](#bib.bib13))，它利用从预训练 BERT 模型中获得的上下文嵌入来评估生成文本与参考文本之间的语义相似性。BERTScore
    通过为评估增加更细致的语义层来补充 ROUGE，捕捉传统指标可能忽略的方面。
- en: Moreover, the rise of Large Language Models (LLMs) has further enriched the
    toolkit for text generation evaluation Fu et al. ([2023](#bib.bib2)); Liu et al.
    ([2023](#bib.bib5)). LLMs, with their ability for nuanced analysis, offer possibilities
    beyond syntactic and surface-level metrics. They can potentially capture deeper
    aspects of dialogue semantics and goal alignment.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大型语言模型（LLMs）的兴起进一步丰富了文本生成评估的工具包 Fu et al. ([2023](#bib.bib2)); Liu et al.
    ([2023](#bib.bib5))。LLMs凭借其细致的分析能力，提供了超越句法和表面层面度量的可能性。它们可以潜在地捕捉对话语义和目标对齐的更深层次方面。
- en: 3 Methodology
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'This section outlines the methods employed to accomplish two objectives of
    this study: automating the generation of dialogue datasets, and evaluating the
    quality of the generated dialogues. We utilize a dual-agent setup leveraging LLM,
    in our case, OpenAI’s GPT-4 OpenAI ([2023](#bib.bib7)), to simulate a conversation
    between a user and an assistant focusing on linear programming problems.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了实现本研究两个目标的方法：自动生成对话数据集以及评估生成对话的质量。我们利用了一个双代理设置，利用LLM，在我们的案例中是OpenAI的GPT-4
    OpenAI ([2023](#bib.bib7))，来模拟用户与助手之间关于线性规划问题的对话。
- en: 3.1 Dual-Agent LLM Setup
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 双代理LLM设置
- en: 'The dual-agent setup aims to model a conversation between an automatic conversational
    agent who asks questions to a user, with the aim to identify all key information
    of the linear problem to model. Specifically, the setup comprises two distinct
    LLM agents: a Question Generation Agent and a Question Answering Agent who “talk”
    to each other.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 双代理设置旨在模拟一个自动对话代理向用户提问的对话，以识别线性问题建模的所有关键信息。具体而言，该设置包括两个不同的LLM代理：一个问题生成代理和一个问题回答代理，这两个代理“对话”。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Question Generation (QG) Agent: The primary objective of this agent is to ask
    questions that will enable it to elicit and summarize the key information of the
    original problem statement. Importantly, this agent does not have access to the
    original problem statement and must rely solely on its interactions with the QA
    Agent to gather all necessary information.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题生成（QG）代理：该代理的主要目标是提出问题，以便引出和总结原始问题陈述的关键信息。重要的是，这个代理无法访问原始问题陈述，必须完全依靠与QA代理的互动来收集所有必要的信息。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Question Answering (QA) Agent: This agent is designed to answer questions based
    on a pre-defined problem statement from NL4Opt, which serves as the simulated
    knowledge base for the assistant. To make the dialogue more natural and contextually
    grounded, the agent is configured to impersonate the individual mentioned in the
    original problem statement. This design choice enhances the genuineness of the
    interaction, creating a more realistic user-agent dialogue experience.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题回答（QA）代理：这个代理旨在根据来自NL4Opt的预定义问题陈述回答问题，NL4Opt作为助手的模拟知识库。为了使对话更加自然且具有上下文基础，代理被配置为模仿原始问题陈述中提到的个人。这一设计选择增强了互动的真实性，创造了更为逼真的用户-代理对话体验。
- en: 3.2 Implementation Details
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实施细节
- en: 'An essential part of the QA Agent is a component (which also leverages LLM)
    that compares a summary with an original problem statement. This component’s role
    is twofold: to provide feedback when discrepancies are found between the provided
    summary and the original problem statement, and to signal the termination of dialogue
    generation if not. To detect that the QG has generated a summary in the latest
    dialogue turn, we employed a straightforward approach using regular expressions
    and predefined keywords.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: QA代理的一个重要部分是一个组件（也利用LLM），该组件将摘要与原始问题陈述进行比较。这个组件的作用有两个：在发现摘要与原始问题陈述之间存在差异时提供反馈，并在没有发现差异时发出对话生成终止的信号。为了检测QG是否在最新的对话回合中生成了摘要，我们采用了使用正则表达式和预定义关键字的简单方法。
- en: 'In order to maintain consistency and guide the LLM in its responses, prompts
    were injected throughout the dialogue. Aside from the initial system prompt, which
    always begins the conversation, subsequent prompts were introduced at each turn
    and retained in the dialogue’s history. For the QG Agent, a succinct instruction,
    A NEXT MESSAGE/QUESTION was provided. For the QA Agent, the prompts varied based
    on the dialogue stage. Mostly, the instruction ANSWER SHORTLY. USE MAXIMUM 30
    WORDS. was used. However, when the QG agent provided a summary, the prompt adjusted
    accordingly: either feedback on summary²²2Full prompts are provided in Appendix [B](#A2
    "Appendix B LLM Prompts ‣ Synthetic Dialogue Dataset Generation using LLM Agents").,
    or THE SUMMARY ACCEPTED. IT’S TIME TO FINISH DIALOG AND SAY GOODBYE, signaling
    the end of the conversation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性并指导 LLM 的响应，提示贯穿整个对话。除了始终开始对话的初始系统提示外，后续提示在每个回合引入，并保留在对话历史中。对于 QG 代理，提供了简洁的指令，即
    A NEXT MESSAGE/QUESTION。对于 QA 代理，提示根据对话阶段有所不同。大多数情况下，使用了指令 ANSWER SHORTLY. USE
    MAXIMUM 30 WORDS。然而，当 QG 代理提供总结时，提示会相应调整：要么反馈总结²²2附录 [B](#A2 "附录 B LLM 提示 ‣ 使用
    LLM 代理生成合成对话数据集") 中提供了完整的提示。要么 THE SUMMARY ACCEPTED. IT’S TIME TO FINISH DIALOG
    AND SAY GOODBYE，表示对话结束。
- en: We utilized the dataset of linear programming problems from the NL4Opt competition
    Ramamonjison et al. ([2023](#bib.bib10)), specifically from subtask 2\. The task
    is designed to take a natural language problem description, labeled semantic entities,
    and an ordered mapping of variable mentions, and then formulate a precise meaning
    representation. These problem descriptions served as the basis for the simulated
    conversations between the two LLM agents in this work.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了 NL4Opt 竞赛中的线性规划问题数据集（Ramamonjison 等，[2023](#bib.bib10)），特别是来自子任务 2 的数据。任务旨在将自然语言问题描述、标记的语义实体和变量提及的有序映射转化为精确的意义表示。这些问题描述作为本研究中两个
    LLM 代理模拟对话的基础。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Development Subset: For this subset, consisting of 98 problems. Two to four
    separate dialogues were generated per problem.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开发子集：此子集包含 98 个问题。每个问题生成了两到四个独立的对话。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training Subset: For the training subset, a single dialogue was generated for
    the first 241 problems.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练子集：对于训练子集，为前 241 个问题生成了单个对话。
- en: The dual-agent system was implemented using OpenAI’s Chat Completion API³³3[https://platform.openai.com/docs/api-reference/chat](https://platform.openai.com/docs/api-reference/chat).
    We generated the dialogues using different temperature settings for the development
    subset to ensure a variety of dialogues. The aim was to generate multiple dialogue
    instances for each problem so that it is possible to robustly evaluate the model’s
    performance and assess the diversity in the generated dataset in subsequent research.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 双代理系统使用 OpenAI 的 Chat Completion API³³3[https://platform.openai.com/docs/api-reference/chat](https://platform.openai.com/docs/api-reference/chat)
    实现。我们通过不同的温度设置生成对话，以确保开发子集中的对话多样性。目标是为每个问题生成多个对话实例，以便在后续研究中能全面评估模型的性能，并评估生成数据集的多样性。
- en: Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Implementation Details ‣ 3 Methodology ‣ Synthetic
    Dialogue Dataset Generation using LLM Agents") shows part of a sample dialogue,
    and figure  [2](#S3.F2 "Figure 2 ‣ 3.2 Implementation Details ‣ 3 Methodology
    ‣ Synthetic Dialogue Dataset Generation using LLM Agents") shows the associated
    problem statement. Appendix [B](#A2 "Appendix B LLM Prompts ‣ Synthetic Dialogue
    Dataset Generation using LLM Agents") shows the actual prompts used.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S3.F1 "图 1 ‣ 3.2 实施细节 ‣ 3 方法论 ‣ 使用 LLM 代理生成合成对话数据集") 显示了一个示例对话的一部分，图 [2](#S3.F2
    "图 2 ‣ 3.2 实施细节 ‣ 3 方法论 ‣ 使用 LLM 代理生成合成对话数据集") 显示了相关的问题陈述。附录 [B](#A2 "附录 B LLM
    提示 ‣ 使用 LLM 代理生成合成对话数据集") 显示了实际使用的提示。
- en: 'QG Agent: Hello there! I’m OptiMouse, your friendly helper. I’m here to assist
    you in making the best decision for your situation. Could you please tell me what
    you’re trying to achieve or improve? QA Agent: Hello! I’m looking to maximize
    profit for my furniture store while considering space, capital, and product ratio
    constraints. QG Agent: That sounds like a great goal! To help you better, could
    you please tell me what types of furniture you sell in your store? QA Agent: We
    sell dining tables and chairs in our store. QG Agent: That’s great! Now, could
    you please tell me how much profit you make from selling a dining table and how
    much from selling a chair? QA Agent: We make a profit of $350 from selling a dining
    table and $75 from selling a chair. QG Agent: That’s helpful to know. Now, could
    you please tell me how much space a dining table and a chair take up in your store?
    QA Agent: A dining table requires 8 sq ft of floor space while a chair requires
    2 sq ft. … … … … QG Agent: That’s a very important detail. Now, could you please
    tell me if there are any other constraints or requirements that we need to consider
    for your furniture store? QA Agent: No, there are no other constraints or requirements
    for my furniture store. QG Agent: Great! Let me summarize the information:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'QG Agent: 你好！我是OptiMouse，你的友好助手。我在这里帮助你做出最佳决策。请告诉我你想要实现或改进的目标是什么？ QA Agent:
    你好！我想在考虑空间、资本和产品比例约束的情况下，最大化我家具店的利润。 QG Agent: 听起来是个很棒的目标！为了更好地帮助你，你能告诉我你们店里销售哪些类型的家具吗？
    QA Agent: 我们店里销售餐桌和椅子。 QG Agent: 很好！现在，请告诉我你从销售一张餐桌中获得多少利润，销售一把椅子中获得多少利润？ QA Agent:
    我们从销售一张餐桌中获得$350的利润，从销售一把椅子中获得$75的利润。 QG Agent: 这很有帮助。现在，请告诉我餐桌和椅子在你们店里占用多少空间？
    QA Agent: 一张餐桌需要8平方英尺的地面空间，而一把椅子需要2平方英尺。 … … … … QG Agent: 这是一个非常重要的细节。现在，请告诉我是否还有其他约束或要求需要考虑？
    QA Agent: 不，我的家具店没有其他约束或要求。 QG Agent: 太好了！让我总结一下信息：'
- en: '- You sell dining tables and chairs in your furniture store'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '- 你在家具店里销售餐桌和椅子。'
- en: '- You make a profit of $350 from selling a dining table and $75 from selling
    a chair.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '- 你从销售一张餐桌中获得$350的利润，从销售一把椅子中获得$75的利润。'
- en: '- A dining table requires 8 sq ft of floor space while a chair requires 2 sq
    ft.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '- 一张餐桌需要8平方英尺的地面空间，而一把椅子需要2平方英尺。'
- en: '- You have 500 sq ft of space available in your store.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '- 你在店里有500平方英尺的可用空间。'
- en: '- You have a maximum of $20000 available to purchase the dining tables and
    chairs.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '- 你最多有$20000的预算用于购买餐桌和椅子。'
- en: '- A dining table costs you $1000 to purchase and a chair costs $150.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '- 一张餐桌的采购成本是$1000，一把椅子的采购成本是$150。'
- en: '- At least 70% of all furniture in the store must be chairs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '- 店里所有家具中至少70%必须是椅子。'
- en: Is there anything else you would like to add or modify in this summary?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你还有什么想要在这个总结中添加或修改的吗？
- en: 'Figure 1: A sample dialogue between a QG Agent and a QA Agent about maximizing
    furniture store profits.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：关于最大化家具店利润的QG Agent与QA Agent之间的示例对话。
- en: A furniture store only stocks and sells dining tables and chairs. The profit
    per dining table is $350 and the profit per chair is $75\. There is 500 sq ft
    of space available and a dining table requires 8 sq ft of floor space while a
    chair requires 2 sq ft. Because chairs sell in larger quantities, at least 70%
    of all furniture in the store must be chairs. In terms of capital, a dining table
    ties up $1000 in capital and a chair ties up $150 in capital. The company wants
    a maximum of $20000 worth of capital tied up at any time. Formulate an LP to maximize
    profit.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一家家具店只储备和销售餐桌和椅子。每张餐桌的利润是$350，每把椅子的利润是$75。店里有500平方英尺的空间，一张餐桌需要8平方英尺的地面空间，而一把椅子需要2平方英尺。由于椅子的销量较大，店里所有家具中至少70%必须是椅子。在资本方面，一张餐桌占用$1000的资本，一把椅子占用$150的资本。公司希望任何时候资本占用不超过$20000。制定一个线性规划（LP）以最大化利润。
- en: 'Figure 2: A sample problem statement from the NL4Opt dataset.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：来自NL4Opt数据集的一个示例问题陈述。
- en: 4 Evaluation Strategy
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估策略
- en: 'To evaluate the quality of the generated dialogues, we conduct an *extrinsic
    evaluation*. The evaluation process focuses on an underlying hypothesis that serves
    as the core of our study: a correct summary generated by the QG Agent would imply
    that the agent has posed the appropriate and necessary questions to restore the
    original problem statement effectively. The hypothesis is grounded in several
    key assumptions and logical deductions:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估生成对话的质量，我们进行*外部评估*。评估过程集中在一个作为我们研究核心的基本假设上：QG 代理人生成的正确总结意味着代理人提出了适当和必要的问题，以有效恢复原始问题陈述。该假设基于几个关键假设和逻辑推理：
- en: Extrinsic evaluation. In dialogue systems, particularly goal-oriented systems,
    the dialogue often progresses through a series of questions and answers to reach
    a particular end state Qi et al. ([2020](#bib.bib8)); Majumder et al. ([2021](#bib.bib6)).
    Therefore, the questions asked directly impact the quality and accuracy of the
    final output — here, the summary of the original problem statement. We, therefore,
    conduct an extrinsic evaluation of the dialogues.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 外部评估。在对话系统中，特别是目标导向系统中，对话通常通过一系列问题和回答来达到特定的结束状态 Qi 等人 ([2020](#bib.bib8))；Majumder
    等人 ([2021](#bib.bib6))。因此，提出的问题直接影响最终输出的质量和准确性——这里指的是原始问题陈述的总结。因此，我们进行对话的外部评估。
- en: Incomplete Initial Information. The QG Agent starts with incomplete information
    about the problem at hand. Therefore, asking the right questions is crucial for
    the agent to gather enough information for an accurate summary. A poor or incomplete
    summary would suggest that the agent has asked insufficient or incorrect questions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 不完整的初始信息。QG 代理人开始时对当前的问题信息不完整。因此，提出正确的问题对于代理人收集足够的信息以进行准确总结至关重要。一个差或不完整的总结表明代理人提出了不足或不正确的问题。
- en: Semantic Understanding. An accurate summary involves not just factual correctness
    but also a semantic understanding of the problem’s constraints and objectives.
    Therefore, correct summarization can be viewed as an implicit validation of the
    agent’s ability to grasp the problem’s complexities through its questions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 语义理解。准确的总结不仅涉及事实正确性，还需要对问题的约束和目标有语义上的理解。因此，正确的总结可以被视为对代理人通过其提问掌握问题复杂性的隐性验证。
- en: To validate these assumptions, we use an approach involving human evaluation
    as well as automated evaluation through a third LLM agent of generated summaries.
    By evaluating the correctness of the generated summaries, we thus indirectly assess
    the QG Agent’s ability to pose appropriate and informative questions that lead
    to a comprehensive understanding of the problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这些假设，我们使用了包括人工评估以及通过第三个 LLM 代理人对生成的总结进行自动评估的方法。通过评估生成总结的正确性，我们间接评估 QG 代理人提出适当和信息丰富问题的能力，从而实现对问题的全面理解。
- en: 4.1 Automatic Evaluation
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 自动评估
- en: 'For the automated evaluation, in addition to the well-known ROUGE and BERTScore,
    we designed a metric that employed a third LLM (GPT-4) agent to compare the original
    problem statement with the generated summary. The evaluation was conducted using
    a “chain-of-thought” prompting Wei et al. ([2022](#bib.bib12)). Our agent employs
    three criteria (“chains”) to evaluate each summary:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动评估，除了众所周知的 ROUGE 和 BERTScore 外，我们设计了一种度量标准，该标准使用第三个 LLM（GPT-4）代理人来比较原始问题陈述与生成的总结。评估采用了
    Wei 等人 ([2022](#bib.bib12)) 的“链式思考”提示。我们的代理人使用三个标准（“链”）来评估每个总结：
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Correct Information: Assessing if the summary accurately represents the facts
    in the original problem statement.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正确的信息：评估总结是否准确地代表了原始问题陈述中的事实。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Incorrect Information: Identifying any factual inaccuracies or misleading statements
    in the summary.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 错误信息：识别总结中的任何事实不准确或误导性陈述。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Missing Information: Determining if any crucial elements from the original
    problem statement are bypassed in the summary.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺失的信息：确定总结中是否遗漏了原始问题陈述中的任何关键元素。
- en: Considering the three criteria above, the agent provides an "Information Recall
    Score", "Information Precision Score", "Information Repetition Score", and "Readability
    Score" to judge if the summary forms a coherent and accurate representation of
    the original problem. These evaluation metrics are the same as the human evaluation
    conducted in Section [4.2](#S4.SS2 "4.2 Human Evaluation ‣ 4 Evaluation Strategy
    ‣ Synthetic Dialogue Dataset Generation using LLM Agents"). Appendix [B](#A2 "Appendix
    B LLM Prompts ‣ Synthetic Dialogue Dataset Generation using LLM Agents") shows
    the prompts used.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述三个标准，系统提供了“信息召回评分”、“信息精确评分”、“信息重复评分”和“可读性评分”来判断摘要是否形成了对原始问题的连贯和准确的表示。这些评估指标与第[4.2节](#S4.SS2
    "4.2 人工评估 ‣ 4 评估策略 ‣ 使用LLM代理生成的合成对话数据集")中进行的人工评估相同。附录[B](#A2 "附录B LLM提示 ‣ 使用LLM代理生成的合成对话数据集")展示了使用的提示。
- en: 4.1.1 Results of the Automatic Evaluation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 自动评估结果
- en: '| Metric | Value |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 值 |'
- en: '| --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ROUGE-1 P | 0.54 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 P | 0.54 |'
- en: '| ROUGE-1 R | 0.62 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 R | 0.62 |'
- en: '| ROUGE-1 F1 | 0.57 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 F1 | 0.57 |'
- en: '| ROUGE-2 P | 0.33 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2 P | 0.33 |'
- en: '| ROUGE-2 R | 0.39 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2 R | 0.39 |'
- en: '| ROUGE-2 F1 | 0.35 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2 F1 | 0.35 |'
- en: '| ROUGE-L P | 0.38 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L P | 0.38 |'
- en: '| ROUGE-L R | 0.43 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L R | 0.43 |'
- en: '| ROUGE-L F1 | 0.40 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L F1 | 0.40 |'
- en: '| BERTscore P | 0.88 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| BERTscore P | 0.88 |'
- en: '| BERTscore R | 0.91 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| BERTscore R | 0.91 |'
- en: '| BERTscore F1 | 0.90 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| BERTscore F1 | 0.90 |'
- en: '| GPT-4 R | 4.60 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 R | 4.60 |'
- en: '| GPT-4 P | 4.62 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 P | 4.62 |'
- en: 'Table 2: Average values of metrics per entire dataset. All values range from
    0 to 1 except GPT-4, which ranges from 1 to 5.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：整个数据集的指标平均值。除GPT-4外，所有值的范围均为0到1，GPT-4的范围为1到5。
- en: Table [2](#S4.T2 "Table 2 ‣ 4.1.1 Results of the Automatic Evaluation ‣ 4.1
    Automatic Evaluation ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue Dataset Generation
    using LLM Agents") summarizes the average metric values across the entire dataset.
    As we can observe, the absolute values of ROUGE are not very high. However, as
    we will see below, human evaluation of a subset of the data reveals that the dialogues
    have generated good summaries in general.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S4.T2 "表2 ‣ 4.1.1 自动评估结果 ‣ 4.1 自动评估 ‣ 4 评估策略 ‣ 使用LLM代理生成的合成对话数据集")总结了整个数据集的平均指标值。如我们所见，ROUGE的绝对值并不很高。然而，正如我们接下来会看到的，对数据子集的人工评估显示，对话总体上生成了良好的摘要。
- en: '| Name | Value |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 值 |'
- en: '| --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Total number of dialogues | 476 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 对话总数 | 476 |'
- en: '| Dialogues with temperature 0 | 315 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 温度为0的对话 | 315 |'
- en: '| Dialogues with temperature 1 | 149 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 温度为1的对话 | 149 |'
- en: '| Dialogues with summary | 97% |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 包含摘要的对话 | 97% |'
- en: '| Average dialogue length (characters) | 3658 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 平均对话长度（字符） | 3658 |'
- en: '| Total number of turns | 9480 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 总回合数 | 9480 |'
- en: '| Average number of turns per dialogue | 20 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 每个对话的平均回合数 | 20 |'
- en: '| Average turn length (characters) | 184 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 平均回合长度（字符） | 184 |'
- en: 'Table 3: Summary statistics of generated dialogues.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：生成对话的摘要统计数据。
- en: To provide a more nuanced understanding of our generated dialogues, we have
    collected some summary statistics. These are presented in Table [3](#S4.T3 "Table
    3 ‣ 4.1.1 Results of the Automatic Evaluation ‣ 4.1 Automatic Evaluation ‣ 4 Evaluation
    Strategy ‣ Synthetic Dialogue Dataset Generation using LLM Agents"). The dialogues
    were generated with varying temperature settings to control the randomness of
    the text generated by the LLMs. In a small number of cases (3%), the dialogue
    was not able to generate a summary before the cut-off of 40 turns (20 turns for
    each agent). The high percentage of dialogues with a summary (97%) indicates the
    Question Generation Agent’s effectiveness in concluding the dialogues with a summary,
    which is crucial for our evaluation. Dialogue length and number of turns give
    an indication of the depth and extent of the conversations between the agents.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供对我们生成的对话更为细致的理解，我们收集了一些摘要统计数据。这些数据展示在表[3](#S4.T3 "表3 ‣ 4.1.1 自动评估结果 ‣ 4.1
    自动评估 ‣ 4 评估策略 ‣ 使用LLM代理生成的合成对话数据集")中。对话是通过不同的温度设置生成的，以控制LLM生成文本的随机性。在少数情况下（3%），对话未能在40回合的截止前生成摘要（每个代理20回合）。生成摘要的对话高达97%，表明问题生成代理在结束对话时生成摘要的有效性，这对我们的评估至关重要。对话长度和回合数反映了代理之间对话的深度和广度。
- en: 4.2 Human Evaluation
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 人工评估
- en: For the human evaluation component, we carefully curated a subset of 28 problem
    statements from the development subset. This subset was selected to cover all
    constraint types represented in the development data (Table [1](#S2.T1 "Table
    1 ‣ 2.1 Linear Programming and NL4Opt ‣ 2 Background and Related Work ‣ Synthetic
    Dialogue Dataset Generation using LLM Agents")), ensuring a comprehensive evaluation
    across diverse problem scenarios.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人工评估部分，我们从开发子集中精心挑选了28个问题声明。这个子集被选择以涵盖开发数据中表示的所有约束类型（表[1](#S2.T1 "Table 1
    ‣ 2.1 Linear Programming and NL4Opt ‣ 2 Background and Related Work ‣ Synthetic
    Dialogue Dataset Generation using LLM Agents")），确保对各种问题场景进行全面评估。
- en: 'Given the small base of problems (98), the selection of 28 problems was simple:
    first, apply a greedy approach that satisfies the quantity requirements of the
    types with fewer counts, and then manually swap selected candidates with other
    candidates from the development set as appropriate, until we reached an acceptable
    distribution of constraint types in the selection. Table [4](#S4.T4 "Table 4 ‣
    4.2 Human Evaluation ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue Dataset Generation
    using LLM Agents") shows the distribution of constraint types in the development
    set and the selection used for human evaluation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于问题的基数较小（98），选择28个问题的方法很简单：首先，应用贪婪算法以满足约束类型较少的数量要求，然后根据需要手动用开发集中其他候选问题交换已选候选问题，直到我们达到了可接受的约束类型分布。表[4](#S4.T4
    "Table 4 ‣ 4.2 Human Evaluation ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue Dataset
    Generation using LLM Agents")展示了开发集和用于人工评估的选择集中的约束类型分布。
- en: '|  | Constraint Type | Dev | Sel |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 约束类型 | Dev | Sel |'
- en: '| 1 | Upper bound on single variable | 20 | 6 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 单一变量的上界 | 20 | 6 |'
- en: '| 2 | Upper bound on sum of variables | 12 | 4 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 变量和的上界 | 12 | 4 |'
- en: '| 3 | Upper bound on weighted sum of variables | 93 | 28 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 变量加权和的上界 | 93 | 28 |'
- en: '| 4 | Upper bound on proportion | 8 | 2 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 比例的上界 | 8 | 2 |'
- en: '| 5 | Lower bound on single variable | 35 | 11 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 单一变量的下界 | 35 | 11 |'
- en: '| 6 | Lower bound on sum of variables | 7 | 2 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 变量和的下界 | 7 | 2 |'
- en: '| 7 | Lower bound on weighted sum of variables | 59 | 18 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 变量加权和的下界 | 59 | 18 |'
- en: '| 8 | Lower bound on proportion | 15 | 5 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 比例的下界 | 15 | 5 |'
- en: '| 9 | Comparison | 43 | 13 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 比较 | 43 | 13 |'
- en: 'Table 4: Counts of types of linear optimization constraints in the development
    set of 98 problems (“Dev” column), and the selection of 28 problems (“Sel” column).
    The sum is larger than the number of problems since a problem may have multiple
    constraints.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：98个问题的开发集（“Dev”列）和28个问题的选择集（“Sel”列）中线性优化约束类型的计数总数。总数大于问题的数量，因为一个问题可能有多个约束。
- en: To add an element of variability in dialogue generation, each problem statement
    in this selection was subjected to a single dialogue generation run. The temperature
    setting for this run was randomly selected to be either 0 or 1.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加对话生成中的变异性，选择集中的每个问题声明都进行了单次对话生成运行。该运行的温度设置被随机选择为0或1。
- en: 'Four evaluators (details in the Acknowledgements section) then scored how well
    the summary generated at the end of the dialogue matches the problem statement.
    For every pair of a problem statement and a generated summary, each evaluator
    produced the following 4 evaluation metrics. These metrics have been adopted from
    the human evaluation performed by Tsatsaronis et al. ([2015](#bib.bib11)):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 四名评估人员（详细信息见致谢部分）随后对对话结束时生成的摘要与问题声明的匹配程度进行了评分。对于每一对问题声明和生成的摘要，每位评估人员提供了以下4项评估指标。这些指标借鉴了Tsatsaronis等人（[2015](#bib.bib11)）进行的人工评估。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Information recall (IR) (1-5) – All the necessary information is in the generated
    summary.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息召回（IR）（1-5）– 生成的摘要中包含所有必要的信息。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Information precision (IP) (1-5) – No irrelevant information is generated.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息精确度（IP）（1-5）– 不生成无关信息。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Information repetition (IRep) (1-5) – The generated summary does not repeat
    the same information multiple times.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息重复（IRep）（1-5）– 生成的摘要中不重复相同的信息。
- en: •
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Readability (Read) (1-5) – The generated summary is easily readable and fluent.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可读性（Read）（1-5）– 生成的摘要易于阅读且流畅。
- en: 4.2.1 Results of the Human Evaluation
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 人工评估结果
- en: Inter-annotator agreement of each of the 4 human evaluation metrics, as computed
    by Fleiss’ Kappa, is shown in Table [5](#S4.T5 "Table 5 ‣ 4.2.1 Results of the
    Human Evaluation ‣ 4.2 Human Evaluation ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue
    Dataset Generation using LLM Agents").
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 各个4个人工评估指标的标注者间一致性，由 Fleiss’ Kappa 计算，详见表 [5](#S4.T5 "表 5 ‣ 4.2.1 人工评估结果 ‣ 4.2
    人工评估 ‣ 4 评估策略 ‣ 使用 LLM 代理生成合成对话数据集")。
- en: '| Metric | Fleiss’ Kappa |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 量度 | Fleiss’ Kappa |'
- en: '| --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Information Recall | 0.205 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 信息召回 | 0.205 |'
- en: '| Information Precision | 0.387 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 信息精准度 | 0.387 |'
- en: '| Information Repetition | -0.009 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 信息重复 | -0.009 |'
- en: '| Readability | 0.235 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 可读性 | 0.235 |'
- en: 'Table 5: Inter-annotator agreement of each of the 4 human evaluation metrics.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：4个人工评估指标的标注者间一致性。
- en: We observe virtually no agreement in Information Repetition, slight agreement
    in Information Recall and Readability, and Fair agreement in Information Precision.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到在信息重复方面几乎没有一致性，在信息召回和可读性方面有轻微的一致性，在信息精准度方面有公正的一致性。
- en: Table [6](#S4.T6 "Table 6 ‣ 4.2.1 Results of the Human Evaluation ‣ 4.2 Human
    Evaluation ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue Dataset Generation using
    LLM Agents") shows the average values of the human evaluation scores for the selection
    of 28 problems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S4.T6 "表 6 ‣ 4.2.1 人工评估结果 ‣ 4.2 人工评估 ‣ 4 评估策略 ‣ 使用 LLM 代理生成合成对话数据集")显示了对28个问题的人工评估分数的平均值。
- en: '| Annotator | IR | IP | IRep | Read |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 标注者 | IR | IP | IRep | Read |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 4.25 | 4.25 | 4.89 | 4.96 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4.25 | 4.25 | 4.89 | 4.96 |'
- en: '| 2 | 4.18 | 4.54 | 4.93 | 4.96 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4.18 | 4.54 | 4.93 | 4.96 |'
- en: '| 3 | 4.68 | 4.39 | 4.93 | 4.86 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 4.68 | 4.39 | 4.93 | 4.86 |'
- en: '| 4 | 4.03 | 4.36 | 4.82 | 4.89 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4.03 | 4.36 | 4.82 | 4.89 |'
- en: '| All | 4.29 | 4.38 | 4.89 | 4.92 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 4.29 | 4.38 | 4.89 | 4.92 |'
- en: 'Table 6: Average human evaluation scores for the sample of 28 documents. IR
    = Information Recall. IP = Information Precision. IRep = Information Repetition.
    Read = Readability.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：28 个文档样本的平均人工评估分数。IR = 信息召回。IP = 信息精准度。IRep = 信息重复。Read = 可读性。
- en: Overall, the human evaluation showed high values, and very high values for Information
    Repetition and Readability. This suggests that the dialogues generated by the
    pair of agents are of good quality.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，人类评估显示了较高的数值，尤其是在信息重复和可读性方面的数值非常高。这表明由这对代理生成的对话质量较好。
- en: The human annotators observed the following most common mistakes in the generated
    summaries. See Appendix [A](#A1 "Appendix A Sample Generated Summaries with Mistakes
    ‣ Synthetic Dialogue Dataset Generation using LLM Agents") for examples of each.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 人类标注者观察到生成摘要中最常见的错误。请参见附录 [A](#A1 "附录 A 示例生成摘要的错误 ‣ 使用 LLM 代理生成合成对话数据集")获取每个错误的示例。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Missing objective function or decision variables.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缺失的目标函数或决策变量。
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inclusion of additional information that seems to be from answers to the agent
    asking clarifying questions, such as “otherwise the order does not matter”, or
    “there is no upper limit on costs”, etc.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括了似乎来自代理提出澄清问题的额外信息，例如“否则顺序无关”或“费用没有上限”等。
- en: 4.3 Correlation Analysis of Automatic and Human Evaluations
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 自动评估与人工评估的相关性分析
- en: Table [7](#S4.T7 "Table 7 ‣ 4.3 Correlation Analysis of Automatic and Human
    Evaluations ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue Dataset Generation using
    LLM Agents") shows Spearman’s rank correlation coefficient $\rho$ between the
    automatic and human evaluations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#S4.T7 "表 7 ‣ 4.3 自动评估与人工评估的相关性分析 ‣ 4 评估策略 ‣ 使用 LLM 代理生成合成对话数据集")显示了自动评估与人工评估之间的
    Spearman 排名相关系数 $\rho$。
- en: '| Metric | IR | IP | IF1 | IAvg |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 量度 | IR | IP | IF1 | IAvg |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ROUGE-1 R | 0.43 |  |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 R | 0.43 |  |  |  |'
- en: '| ROUGE-1 P |  | 0.58 |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 P |  | 0.58 |  |  |'
- en: '| ROUGE-1 F1 |  |  | 0.62 | 0.60 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-1 F1 |  |  | 0.62 | 0.60 |'
- en: '| ROUGE-2 R | 0.48 |  |  |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2 R | 0.48 |  |  |  |'
- en: '| ROUGE-2 P |  | 0.58 |  |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2 P |  | 0.58 |  |  |'
- en: '| ROUGE-2 F1 |  |  | 0.56 | 0.57 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-2 F1 |  |  | 0.56 | 0.57 |'
- en: '| ROUGE-L R | 0.47 |  |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L R | 0.47 |  |  |  |'
- en: '| ROUGE-L P |  | 0.74 |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L P |  | 0.74 |  |  |'
- en: '| ROUGE-L F1 |  |  | 0.71 | 0.69 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L F1 |  |  | 0.71 | 0.69 |'
- en: '| BERTScore R | 0.53 |  |  |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| BERTScore R | 0.53 |  |  |  |'
- en: '| BERTScore P |  | 0.74 |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| BERTScore P |  | 0.74 |  |  |'
- en: '| BERTScore F1 |  |  | 0.65 | 0.65 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| BERTScore F1 |  |  | 0.65 | 0.65 |'
- en: '| GPT-4 R | 0.42 |  |  |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 R | 0.42 |  |  |  |'
- en: '| GPT-4 P |  | 0.67 |  |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 P |  | 0.67 |  |  |'
- en: '| GPT-4 F1 |  |  | 0.59 | 0.58 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 F1 |  |  | 0.59 | 0.58 |'
- en: 'Table 7: Spearman’s rank correlation coefficient $\rho$ between the automatic
    and human evaluations. IP = correlation with Information Recall; IR = correlation
    with Information Precision; IF1 = Correlation with the Harmonic mean of IR and
    IP; IAvg = Correlation with the average of Information Recall, Information Precision,
    Repetition, and Readability.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：自动评估与人工评估之间的斯皮尔曼等级相关系数 $\rho$。IP = 与信息召回的相关性；IR = 与信息精确度的相关性；IF1 = 与 IR
    和 IP 的调和均值的相关性；IAvg = 与信息召回、信息精确度、重复和可读性的平均值的相关性。
- en: The table compares recall values of the automatic metrics against Information
    Recall, precision values against Information Precision, and F1 values against
    the harmonic mean of Information Recall and Information Precision. The last column
    of the table shows the correlation between the F1 values of the automatic metrics
    and the average of Information Recall, Information Precision, Information Repetition,
    and Readability. We can observe nearly identical values to the harmonic mean of
    Information Recall and Information Precision. The reason for this may be that
    the annotations for Information Repetition and Readability are nearly always 5,
    so their contribution is almost a constant value that does not change the rankings,
    so they do not affect the values of $\rho$. This may be a consequence of using
    GPT-4 and the carefully designed prompts, which instruct the system to be clear
    and concise.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表格比较了自动度量的召回值与信息召回的对比、精确度值与信息精确度的对比以及 F1 值与信息召回和信息精确度的调和均值的对比。表格的最后一列显示了自动度量的
    F1 值与信息召回、信息精确度、信息重复和可读性平均值之间的相关性。我们可以观察到与信息召回和信息精确度的调和均值几乎相同的值。原因可能是信息重复和可读性的标注几乎总是
    5，因此它们的贡献几乎是一个不变的常数值，不会改变排名，因此不会影响 $\rho$ 的值。这可能是使用 GPT-4 和精心设计的提示词的结果，指导系统要清晰简洁。
- en: Among all automatic metrics, Table [7](#S4.T7 "Table 7 ‣ 4.3 Correlation Analysis
    of Automatic and Human Evaluations ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue
    Dataset Generation using LLM Agents") shows that the best correlation values are
    for ROUGE-L. GPT-4 achieved competitive results but did not outperform the other
    metrics. Further work is needed to improve the use of GPT-4\. In particular, GPT-4
    usually was more generous and would give higher ratings than the human evaluators
    would. The inclusion of few-shot samples, and more sophisticated prompts, might
    help the system align with the human annotators.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有自动度量中，表[7](#S4.T7 "Table 7 ‣ 4.3 Correlation Analysis of Automatic and Human
    Evaluations ‣ 4 Evaluation Strategy ‣ Synthetic Dialogue Dataset Generation using
    LLM Agents")显示 ROUGE-L 的相关值最佳。GPT-4 取得了具有竞争力的结果，但未能超越其他度量。还需要进一步工作来改进 GPT-4 的使用。特别是，GPT-4
    通常更慷慨，会给出比人工评估者更高的评分。加入少量样本和更复杂的提示可能有助于系统与人工标注者对齐。
- en: 5 Summary and Conclusion
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 总结与结论
- en: 'This paper presents a dataset for the task of eliciting information from the
    user through a dialogue with a conversation agent. The specific use of the information
    elicited is for automatic modeling of linear optimization problems. This is *per
    se* a very useful task with broad potential applications, but the methods for
    data generation and evaluation proposed here can be adopted easily for other possible
    tasks. The data and human evaluations are available to the research community.¹¹footnotemark:
    1'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '本文提出了一个数据集，用于通过对话代理从用户那里引出信息。所引出的信息的具体用途是用于线性优化问题的自动建模。这本身是一个非常有用的任务，具有广泛的潜在应用，但这里提出的数据生成和评估方法也可以很容易地被采用用于其他可能的任务。数据和人工评估结果向研究界开放。¹¹footnotemark:
    1'
- en: The dialogue was generated in a dual-agent LLM setup where a question generation
    agent acted as the machine agent who elicited information, and a question answering
    agent acted as the human who had the information about the problem to model. The
    question generation agent can be used as a baseline agent. The human evaluation
    results indicate that this baseline may be effective for the task.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对话是在双代理 LLM 设置中生成的，其中一个问题生成代理作为机器代理获取信息，而一个问题回答代理作为拥有问题模型信息的人类。问题生成代理可以作为基线代理。人工评估结果表明，这个基线可能对任务有效。
- en: The dialogues for a subset of 28 LP problems were evaluated using an extrinsic
    evaluation that judged whether summaries generated by the dialogues matched the
    key information from the original problem descriptions. The evaluation was conducted
    by human evaluators and automatically. Among the automatic evaluation metrics,
    besides well-known automatic metrics ROUGE and BERTScore, we designed another
    GPT-4 agent that mimicked the human evaluators. The results indicate a reasonable
    correlation between ROUGE L, BERTScore P, and the average human information precision
    scores, and this is slightly better than the correlation between the GPT4 agent
    and the human IP scores.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对 28 个线性规划问题的对话进行了外部评估，以判断对话生成的摘要是否符合原问题描述中的关键信息。评估由人工评估员和自动化评估进行。在自动评估指标中，除了知名的
    ROUGE 和 BERTScore 指标外，我们还设计了一个模拟人工评估员的 GPT-4 代理。结果表明，ROUGE L、BERTScore P 与平均人工信息精度分数之间存在合理的相关性，且这一结果略优于
    GPT-4 代理与人工信息精度分数之间的相关性。
- en: As further work, we intend to refine the prompts used for the evaluation approach
    with GPT-4\. In addition, we will conduct more exhaustive types of evaluation
    on the data set that might be more suitable to the specific domain of linear programming
    modeling. In particular, we plan to analyse the generated dialogues at the level
    of the dialogue turns.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 作为后续工作，我们打算优化用于 GPT-4 评估方法的提示。此外，我们将对数据集进行更为详尽的评估类型，以期更适合线性规划建模的特定领域。特别地，我们计划在对话回合层面分析生成的对话。
- en: Acknowledgements
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was partially funded by the Australian Research Council, Australia
    through the Discovery Project 2022 (grant number DP220101925). In addition to
    two authors of this paper who have annotated the data (Diego Molla-Aliod and John
    Yearwood), we acknowledge the annotations provided by Vicky Mak-Hau and Thuseethan
    Selvarajah.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本项工作部分由澳大利亚研究委员会资助，通过发现项目 2022（资助号 DP220101925）。除了本文的两位作者（Diego Molla-Aliod
    和 John Yearwood）标注了数据外，我们还感谢 Vicky Mak-Hau 和 Thuseethan Selvarajah 提供的标注。
- en: References
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chen et al. (2011) Der-San Chen, Robert Batson, and Yu Dang. 2011. [Applied
    Integer Programming: Modeling and Solution](https://doi.org/10.1002/9781118166000.ch1).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2011）Der-San Chen, Robert Batson, 和 Yu Dang. 2011. [应用整数规划：建模与解决方案](https://doi.org/10.1002/9781118166000.ch1)。
- en: 'Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023. [GPTScore: Evaluate as you desire](http://arxiv.org/abs/2302.04166).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023）Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, 和 Pengfei Liu. 2023. [GPTScore：根据你的需求进行评估](http://arxiv.org/abs/2302.04166)。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）Chin-Yew Lin. 2004. [ROUGE：自动评估摘要的软件包](https://aclanthology.org/W04-1013).
    见于 *文本摘要的拓展*，第 74–81 页，西班牙巴塞罗那。计算语言学协会。
- en: 'Liu et al. (2016) Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy,
    Laurent Charlin, and Joelle Pineau. 2016. [How NOT to evaluate your dialogue system:
    An empirical study of unsupervised evaluation metrics for dialogue response generation](https://doi.org/10.18653/v1/D16-1230).
    In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language
    Processing*, pages 2122–2132, Austin, Texas. Association for Computational Linguistics.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2016）Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent
    Charlin, 和 Joelle Pineau. 2016. [如何不评估你的对话系统：对话响应生成的无监督评估指标的实证研究](https://doi.org/10.18653/v1/D16-1230).
    见于 *2016年自然语言处理实证方法会议论文集*，第 2122–2132 页，美国德克萨斯州奥斯汀。计算语言学协会。
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023. [G-Eval: NLG evaluation using GPT-4 with better human
    alignment](http://arxiv.org/abs/2303.16634).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, 和 Chenguang
    Zhu. 2023. [G-Eval：使用 GPT-4 的自然语言生成评估，具有更好的人类对齐](http://arxiv.org/abs/2303.16634)。
- en: 'Majumder et al. (2021) Bodhisattwa Prasad Majumder, Sudha Rao, Michel Galley,
    and Julian McAuley. 2021. [Ask what’s missing and what’s useful: Improving clarification
    question generation using global knowledge](https://doi.org/10.18653/v1/2021.naacl-main.340).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4300–4312,
    Online. Association for Computational Linguistics.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majumder等（2021）Bodhisattwa Prasad Majumder、Sudha Rao、Michel Galley和Julian McAuley。2021年。[询问缺失和有用的信息：利用全球知识改进澄清问题生成](https://doi.org/10.18653/v1/2021.naacl-main.340)。收录于*2021年北美计算语言学协会年会：人类语言技术会议论文集*，第4300–4312页，在线。计算语言学协会。
- en: OpenAI (2023) OpenAI. 2023. [GPT-4 technical report](https://arxiv.org/abs/2303.08774).
    *ArXiv*, abs/2303.08774.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。[GPT-4技术报告](https://arxiv.org/abs/2303.08774)。*ArXiv*，abs/2303.08774。
- en: 'Qi et al. (2020) Peng Qi, Yuhao Zhang, and Christopher D. Manning. 2020. [Stay
    hungry, stay focused: Generating informative and specific questions in information-seeking
    conversations](https://doi.org/10.18653/v1/2020.findings-emnlp.3). In *Findings
    of the Association for Computational Linguistics: EMNLP 2020*, pages 25–40, Online.
    Association for Computational Linguistics.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi等（2020）Peng Qi、Yuhao Zhang和Christopher D. Manning。2020年。[保持饥饿，保持专注：在信息寻求对话中生成信息性和具体性问题](https://doi.org/10.18653/v1/2020.findings-emnlp.3)。收录于*计算语言学协会年会：EMNLP
    2020*，第25–40页，在线。计算语言学协会。
- en: 'Ramamonjison et al. (2022) Rindra Ramamonjison, Haley Li, Timothy Yu, Shiqi
    He, Vishnu Rengan, Amin Banitalebi-dehkordi, Zirui Zhou, and Yong Zhang. 2022.
    [Augmenting operations research with auto-formulation of optimization models from
    problem descriptions](https://doi.org/10.18653/v1/2022.emnlp-industry.4). In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry
    Track*, pages 29–62, Abu Dhabi, UAE. Association for Computational Linguistics.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramamonjison等（2022）Rindra Ramamonjison、Haley Li、Timothy Yu、Shiqi He、Vishnu Rengan、Amin
    Banitalebi-dehkordi、Zirui Zhou和Yong Zhang。2022年。[通过自动制定优化模型增强运筹学](https://doi.org/10.18653/v1/2022.emnlp-industry.4)。收录于*2022年自然语言处理经验方法会议：行业分会*，第29–62页，阿布扎比，阿联酋。计算语言学协会。
- en: 'Ramamonjison et al. (2023) Rindranirina Ramamonjison, Timothy T. Yu, Raymond
    Li, Haley Li, Giuseppe Carenini, Bissan Ghaddar, Shiqi He, Mahdi Mostajabdaveh,
    Amin Banitalebi-Dehkordi, Zirui Zhou, and Yong Zhang. 2023. [NL4Opt competition:
    Formulating optimization problems based on their natural language descriptions](http://arxiv.org/abs/2303.08233).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramamonjison等（2023）Rindranirina Ramamonjison、Timothy T. Yu、Raymond Li、Haley
    Li、Giuseppe Carenini、Bissan Ghaddar、Shiqi He、Mahdi Mostajabdaveh、Amin Banitalebi-Dehkordi、Zirui
    Zhou和Yong Zhang。2023年。[NL4Opt竞赛：基于自然语言描述制定优化问题](http://arxiv.org/abs/2303.08233)。
- en: Tsatsaronis et al. (2015) George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis,
    Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia
    Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John
    Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artiéres, Axel-Cyrille Ngonga
    Ngomo, Norman Heino, Eric Gaussier, Liliana Barrio-Alvers, Michael Schroeder,
    Ion Androutsopoulos, and Georgios Paliouras. 2015. [An overview of the BIOASQ
    large-scale biomedical semantic indexing and question answering competition](https://doi.org/10.1186/s12859-015-0564-6).
    *BMC Bioinformatics*, 16(1):138.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsatsaronis等（2015）George Tsatsaronis、Georgios Balikas、Prodromos Malakasiotis、Ioannis
    Partalas、Matthias Zschunke、Michael R Alvers、Dirk Weissenborn、Anastasia Krithara、Sergios
    Petridis、Dimitris Polychronopoulos、Yannis Almirantis、John Pavlopoulos、Nicolas
    Baskiotis、Patrick Gallinari、Thierry Artiéres、Axel-Cyrille Ngonga Ngomo、Norman
    Heino、Eric Gaussier、Liliana Barrio-Alvers、Michael Schroeder、Ion Androutsopoulos和Georgios
    Paliouras。2015年。[BIOASQ大规模生物医学语义索引和问答竞赛概述](https://doi.org/10.1186/s12859-015-0564-6)。*BMC生物信息学*，16(1)：138。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. [Chain-of-thought prompting
    elicits reasoning in large language models](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 24824–24837\.
    Curran Associates, Inc.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、brian ichter、Fei
    Xia、Ed Chi、Quoc V Le和Denny Zhou。2022年。[链式思维提示在大规模语言模型中引发推理](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)。收录于*神经信息处理系统进展*，第35卷，第24824–24837页。Curran
    Associates, Inc.
- en: 'Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. [BERTScore: Evaluating text generation with BERT](http://arxiv.org/abs/1904.09675).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2020）天易·张、瓦尔莎·基肖尔、费利克斯·吴、基利安·Q·温伯格和约阿夫·阿尔齐。2020年。[BERTScore：使用BERT评估文本生成](http://arxiv.org/abs/1904.09675)。
- en: Appendix A Sample Generated Summaries with Mistakes
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 示例生成的总结及错误
- en: 'The following generated summary did not include the objective function:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以下生成的总结未包含目标函数：
- en: 'Problem statement: A suit store makes 2-piece and 3-piece suits. To make a
    2-piece suit requires 1 hour of measuring and 4 hours of stitching. To make a
    3-piece suit requires 2 hours of measuring and 5 hours of stitching. The store
    has 150 hours of measuring time and 300 hours of stitching time available. In
    order to stay in business, at least 40 2-piece suites should be made. The profit
    per 2-piece suit is $600 and the profit per 3-piece suit is $800\. Formulate a
    LP to maximize profit?'
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题陈述：一家西装店制作2件式和3件式西装。制作一套2件式西装需要1小时的测量和4小时的缝制。制作一套3件式西装需要2小时的测量和5小时的缝制。店铺有150小时的测量时间和300小时的缝制时间可用。为了维持营业，至少需要制作40套2件式西装。每套2件式西装的利润为600美元，每套3件式西装的利润为800美元。请制定一个线性规划模型以最大化利润。
- en: ''
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Generated summary: Alright, thanks for the information! To make sure I’ve got
    everything, here’s a summary of your situation:'
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成的总结：好的，谢谢你的信息！为了确保我理解正确，这里是你的情况的总结：
- en: ''
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- You manufacture two types of suits: 2-piece suits and 3-piece suits.'
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你制造两种类型的西装：2件式西装和3件式西装。'
- en: '- A 2-piece suit makes a profit of $600, requires 1 hour of measuring time
    and 4 hours of stitching time.'
  id: totrans-229
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 一套2件式西装的利润为600美元，需要1小时的测量时间和4小时的缝制时间。'
- en: '- A 3-piece suit makes a profit of $800, requires 2 hours of measuring time
    and 5 hours of stitching time.'
  id: totrans-230
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 一套3件式西装的利润为800美元，需要2小时的测量时间和5小时的缝制时间。'
- en: '- You have 150 hours of measuring time and 300 hours of stitching time available.'
  id: totrans-231
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你有150小时的测量时间和300小时的缝制时间可用。'
- en: '- At least 40 2-piece suits must be produced, but there is no minimum requirement
    for 3-piece suits.'
  id: totrans-232
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 至少需要生产40套2件式西装，但对3件式西装没有最低要求。'
- en: '- There is no limit on how many suits you can sell.'
  id: totrans-233
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你销售西装的数量没有限制。'
- en: ''
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Have I missed out on anything or is there any other specific requirement or
    constraint that I should know about?
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我是否遗漏了什么，或者是否有任何其他特定的要求或约束需要了解？
- en: 'The following generated summary did not mention decision variables:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下生成的总结没有提到决策变量：
- en: 'Problem statement: A chemical company makes an acidic liquid and a basic liquid
    using two types of techniques, reaction X and reaction Y. Reaction X can make
    35 au of the acidic liquid and 75 au of the basic liquid per minute. Reaction
    Y can make 42 au of the acidic liquid and 35 au of the basic liquid per minute.
    Furthermore, reaction X requires 18 units of heat every minute while reaction
    Y requires 12 units of heat every minute. The chemical company has the capacity
    to generate at most 1400 units of heat every minute. If the chemical company needs
    to make at least 900 au of the acidic liquid and 1200 au of the basic liquid per
    minute, how many minutes should each reaction be run for to minimize the total
    time needed?'
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题陈述：一家化学公司使用两种技术，反应X和反应Y，制造酸性液体和碱性液体。反应X每分钟可以生产35单位的酸性液体和75单位的碱性液体。反应Y每分钟可以生产42单位的酸性液体和35单位的碱性液体。此外，反应X每分钟需要18单位的热量，而反应Y每分钟需要12单位的热量。化学公司每分钟最多可以产生1400单位的热量。如果化学公司需要每分钟制造至少900单位的酸性液体和1200单位的碱性液体，那么每种反应应该运行多少分钟，以最小化所需的总时间？
- en: ''
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Generated summary: Great! Let’s summarize the information:'
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成的总结：很好！让我们总结一下信息：
- en: ''
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- Your company has two types of reactions, X and Y, that produce acidic and
    basic liquids.'
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你的公司有两种反应，X和Y，能够生产酸性和碱性液体。'
- en: '- Reaction X requires 18 units of heat per minute and produces 35 au of acidic
    and 75 au of basic liquid.'
  id: totrans-242
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 反应X每分钟需要18单位的热量，生产35单位的酸性液体和75单位的碱性液体。'
- en: '- Reaction Y requires 12 units of heat per minute and produces 42 au of acidic
    and 35 au of basic liquid.'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 反应Y每分钟需要12单位的热量，生产42单位的酸性液体和35单位的碱性液体。'
- en: '- Your company can generate at most 1400 units of heat every minute.'
  id: totrans-244
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你的公司每分钟最多可以产生1400单位的热量。'
- en: '- You need to produce at least 900 au of the acidic liquid and 1200 au of the
    basic liquid per minute.'
  id: totrans-245
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你需要每分钟生产至少900单位的酸性液体和1200单位的碱性液体。'
- en: '- Your goal is to meet these production requirements in the shortest time possible.'
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你的目标是在最短时间内满足这些生产要求。'
- en: ''
  id: totrans-247
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is there anything else that I should know or did I miss anything?
  id: totrans-248
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 是否还有其他我应该知道的内容，或者我遗漏了什么？
- en: 'The following summary has additional information that makes sense but is not
    in the original problem description:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以下总结包含了额外的信息，这些信息有意义但未在原始问题描述中：
- en: 'Problem statement: A spicy paste is to be made from units of peppers and tomatoes
    and is to contain a minimum of 130 units of spice and 90 units of flavor. Peppers
    cost $8 per unit and tomatoes cost $5 per unit. One unit of peppers contains 12
    units of spice and 2 units of flavor. One unit of tomatoes contains 1 unit of
    spice and 8 units of flavor. Formulate this as an LP and find the minimum cost
    spicy paste that can be made.'
  id: totrans-250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题陈述：一种辣味酱料需由辣椒和番茄单元制作，并且必须至少含有130单位的香料和90单位的风味。辣椒每单位成本$8，番茄每单位成本$5。每单位辣椒含有12单位的香料和2单位的风味。每单位番茄含有1单位的香料和8单位的风味。将此问题制定为线性规划，并找到制作这种辣味酱料的最小成本。
- en: ''
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Generated summary: Thanks for the information. Based on your responses, here’s
    your situation as I understand:'
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 生成的总结：感谢提供的信息。根据你的回答，以下是我对你情况的理解：
- en: ''
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- You’re making a spicy paste using peppers and tomatoes.'
  id: totrans-254
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你正在使用辣椒和番茄制作辣味酱料。'
- en: '- Each unit of peppers costs $8, contributes 12 units of spice and 2 units
    of flavor.'
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 每单位辣椒成本$8，贡献12单位的香料和2单位的风味。'
- en: '- Each unit of tomatoes costs $5, contributes 1 unit of spice and 8 units of
    flavor.'
  id: totrans-256
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 每单位番茄成本$5，贡献1单位的香料和8单位的风味。'
- en: '- Each batch of spicy paste should contain at least 130 units of spice and
    90 units of flavor.'
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 每批辣味酱料应至少含有130单位的香料和90单位的风味。'
- en: '- There are no restrictions on the maximum units of peppers and tomatoes you
    can use.'
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 对于辣椒和番茄的最大使用单位没有限制。'
- en: '- There’s no specific budget, but you aim to minimize costs.'
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 没有特定的预算，但你旨在最小化成本。'
- en: ''
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Have I captured everything correctly?
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我是否准确地捕捉到了一切？
- en: Appendix B LLM Prompts
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B LLM提示
- en: 'Evaluation Prompt:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 评估提示：
- en: You  are  an  AI  evaluator  specializing  in  assessing  the  quality  of  summaries.Carefully  check  how  the  summary  captured  a  linear  programming  problem  statement.Important  information  for  this  task  is  explicit  names  and  values  of  decision  variables,  constraints  of  all  types,  and  an  objective  function.Your  primary  goal  is  to  rate  the  summary  based  on  Information  Recall,  Information  Precision,  Information  Repetition  and  Readability.The  Problem  Statement:‘‘‘{0}‘‘‘The  Provided  Summary:’’’{1}’’’PROVIDE  THE  ANSWER  IN  A  JSON  FORMAT  WITH  FOLLOWING  FIELDS:"correct_information"  -  string  |  information  accurately  captured  in  the  summary"missing_information"  -  string  |  important  information  existing  in  the  original  problem  statement  but  not  captured  in  the  summary."incorrect_information"  -  string  |  information  existing  in  an  original  problem  description  but  wrongly/incorrectly  captured  in  a  summary"Information  Recall  Score"  -  int  |  Score  from  1  to  5"Information  Precision  Score"  -  int  |  Score  from  1  to  5"Information  Repetition  Score"  -  int  |  Score  from  1  to  5"Readability  Score"  -  int  |  Score  from  1  to  5
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个专注于评估总结质量的AI评估员。仔细检查总结如何捕捉线性规划问题陈述。此任务的关键信息包括决策变量的明确名称和值、各种类型的约束以及目标函数。你的主要目标是根据信息回忆、信息精确度、信息重复和可读性来评分。问题陈述：‘’‘{0}‘’‘提供的总结：’’’{1}’’’请以JSON格式提供答案，包含以下字段：“correct_information”
    - 字符串 | 总结中准确捕捉的信息“missing_information” - 字符串 | 原始问题陈述中存在但总结未捕捉的重要信息。“incorrect_information”
    - 字符串 | 原始问题描述中存在但总结中错误/不正确的信息“Information Recall Score” - 整数 | 从1到5的评分“Information
    Precision Score” - 整数 | 从1到5的评分“Information Repetition Score” - 整数 | 从1到5的评分“Readability
    Score” - 整数 | 从1到5的评分
- en: 'QG Agent Prompt:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: QG代理提示：
- en: YOU  ARE  "OptiMouse"  -  A  CHATBOT  HELPING  USERS  TO  FORMULATE  FULL  OPTIMIZATION  PROBLEM  STATEMENT.THE  USER  IS  NOT  A  MATH  EXPERT  AND  HAS  NO  EXPERIENCE  WITH  MATH  AND  OPTIMIZATIONS.DO  NOT  USE  ANY  MATHEMATICAL  TERMINOLOGY  OR  EXPLANATIONS  SUCH  AS  OBJECTIVE  FUNCTION,  CONSTRAINTS,  ETC.GATHER  NECESSARY  DETAILS  THAT  CAN  BE  MAPPED  TO  A  LINEAR  PROGRAMMING  MODEL.ENGAGE  USERS  BY  ASKING  CLEAR,  CONCISE,  AND  SEQUENTIAL  QUESTIONS  TO  RECEIVE  INFORMATION  ABOUT  CONSTRAINTS  AND  OBJECTIVE  FUNCTION.ASK  A  QUESTION  BASED  ON  THE  PREVIOUS  INFORMATION  THAT  WILL  LEAD  TO  GETTING  A  CONSTRAINT  OR  OTHER  PARAMETER  OF  THE  MODEL.THINK  DEEPLY  SO  YOU  WILL  BE  ABLE  TO  GET  FULL  PROBLEM  DETAILS.ONE  QUESTION  ALLOWED  PER  MESSAGE.PROVIDE  A  SUMMARY  IN  BULLET  POINTS  (SEE  EXAMPLE  DELIMITED  BY  "====")  ONCE  YOU  HAVE  ALL  THE  INFORMATION  NEEDEDDO  NOT  INCLUDE  UNKNOWN/NON-FACTUAL  CONSTRAINTS  IN  A  SUMMARY(For  example,  ’There’s  no  specific  requirement  on  X...’,  ’There’s  no  limit  on  X...’  )ASK  A  CLARIFICATION  QUESTION  BEFORE  PROVIDING  A  SUMMARY  TO  MAKE  SURE  YOU  HAVE  ALL  THE  CONSTRAINTS  AND  AN  OBJECTIVE  FUNCTION.EXAMPLE  OF  A  SUMMARY:====-  A  coconut  seller  has  to  transport  coconuts  using  either  rickshaws  or  ox  carts.-  The  rickshaws  can  take  50  coconuts  each  and  cost  $10  per  trip.-  The  ox  carts  can  take  30  coconuts  each  and  cost  $8  per  trip.-  The  seller  has  at  most  $200  to  spend  on  transporting  the  coconuts.-  The  number  of  rickshaws  must  not  exceed  the  number  of  ox  carts.====START  THE  CONVERSATION  WITH  A  FRIENDLY  GREETING,  INTRODUCING  YOURSELF  AND  ASKING  WHAT  THE  USER  WOULD  LIKE  TO  OPTIMISE.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你是“OptiMouse”——一个帮助用户制定完整优化问题陈述的聊天机器人。用户不是数学专家，也没有数学和优化方面的经验。不要使用任何数学术语或解释，比如目标函数、约束等。收集可以映射到线性规划模型的必要细节。通过提出清晰、简洁且按顺序的问题来引导用户，以获取有关约束和目标函数的信息。根据之前的信息提出一个问题，以获取模型的约束或其他参数。深思熟虑，以便能够获取完整的问题细节。每条消息允许一个问题。在你获得所有必要的信息后，提供一个以项目符号形式的总结（参见示例，由“====”分隔）。不要在总结中包含未知/非事实性的约束（例如：“对X没有具体要求...”，“对X没有限制...”）。在提供总结之前，提出澄清性问题，以确保你拥有所有的约束和目标函数。总结示例：====-
    一个椰子商人必须使用人力车或牛车运输椰子。- 人力车每次能装50个椰子，每次费用为10美元。- 牛车每次能装30个椰子，每次费用为8美元。- 商人最多可以花200美元运输椰子。-
    人力车的数量不得超过牛车的数量。====以友好的问候开始对话，介绍自己并询问用户希望优化什么。
- en: 'QA Agent Prompt:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: QA 代理提示：
- en: 'YOU  ARE  AGENT  IMPERSONATING  THE  BUSINESS  OWNER  MENTIONED  IN  THE  PROBLEM  STATEMENT(DELIMITED  BY  ‘‘‘).BE  POLITE.YOU(THE  BUSINESS  OWNER)  ARE  TALKING  WITH  AN  EXPERT  IN  OPTIMIZATIONS.ACCURATELY  PROVIDE  INFORMATION  AS  REQUESTED  BASED  ON  THE  PROBLEM  STATEMENT.MAKE  SURE  INFORMATION  YOU  PROVIDING  IS  CORRECT  AND  CAN  BE  FOUND  IN  THE  PROBLEM  STATEMENT.IF  THE  PROBLEM  STATEMENT  DOES  NOT  CONTAIN  REQUESTED  INFORMATION,  SIMPLY  SAY  YOU  DON’T  KNOW  THESE  DETAILS.  (for  example,  "I’m  not  sure  about  it,  can  we  skip  this")DO  NOT  MAKE  CALCULATIONS  OR  INFORMATION  MANIPULATING.  Use  facts  from  the  problem  (for  example,  question:  How  many  X  are  produced  in  a  day?  Answer:  I’m  not  sure,  but  I  know  that  to  produce  one  X,  we  need  Y  minutes.)DO  NOT  MENTION  THE  PROBLEM  STATEMENT  ANYWHERE;  ACT  AS  IF  IT  IS  YOUR  PERSONAL  KNOWLEDGE.THE  PROBLEM  STATEMENT:‘‘‘{0}‘‘‘START  THE  CONVERSATION  WITH  A  WARM  GREETING'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你是代理人，假装成问题陈述中提到的**商业所有者**（由‘’’分隔）。请保持礼貌。你（作为**商业所有者**）正在与优化领域的专家交谈。根据问题陈述准确提供所需的信息。确保你提供的信息是正确的，并且可以在问题陈述中找到。如果问题陈述中没有包含所请求的信息，直接说你不知道这些细节。（例如：“我不确定，可以跳过这个吗”）不要进行计算或信息处理。使用问题中的事实（例如，问题：一天生产多少个X？回答：我不确定，但我知道生产一个X需要Y分钟。）不要在任何地方提到问题陈述；要表现得像这是你个人的知识。问题陈述：‘’’{0}‘’’以热情的问候开始对话。
