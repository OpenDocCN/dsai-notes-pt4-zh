- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:49:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Depending on yourself when you should: Mentoring LLM with RL agents to become
    the master in cybersecurity games'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据需要依靠自己：通过RL代理指导LLM成为网络安全游戏中的大师
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17674](https://ar5iv.labs.arxiv.org/html/2403.17674)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17674](https://ar5iv.labs.arxiv.org/html/2403.17674)
- en: 'Yikuan Yan¹ These authors contributed equally.    Yaolun Zhang¹¹¹footnotemark:
    1    Keman Huang^(1,2)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yikuan Yan¹ 这些作者贡献相等。    Yaolun Zhang¹¹¹脚注标记：1    Keman Huang^(1,2)
- en: ¹School of Information, Renmin University of China, Beijing, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国人民大学信息学院，北京，中国
- en: ²Cybersecurity at MIT Sloan, MIT, Cambridge, Massachusetts, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²麻省理工学院斯隆管理学院，麻省理工学院，剑桥，美国
- en: '{yanyikuan, zhangyaolun5, keman}@ruc.edu.cn Corresponding author.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{yanyikuan, zhangyaolun5, keman}@ruc.edu.cn 对应作者。'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Integrating LLM and reinforcement learning (RL) agent effectively to achieve
    complementary performance is critical in high stake tasks like cybersecurity operations.
    In this study, we introduce SecurityBot, a LLM agent mentored by pre-trained RL
    agents, to support cybersecurity operations. In particularly, the LLM agent is
    supported with a profile module to generated behavior guidelines, a memory module
    to accumulate local experiences, a reflection module to re-evaluate choices, and
    an action module to reduce action space. Additionally, it adopts the collaboration
    mechanism to take suggestions from pre-trained RL agents, including a cursor for
    dynamic suggestion taken, an aggregator for multiple mentors’ suggestions ranking
    and a caller for proactive suggestion asking. Building on the CybORG experiment
    framework, our experiences show that SecurityBot demonstrates significant performance
    improvement compared with LLM or RL standalone, achieving the complementary performance
    in the cybersecurity games.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有效整合大语言模型（LLM）和强化学习（RL）代理，以实现互补性能，在像网络安全操作这样的高风险任务中至关重要。在本研究中，我们引入了SecurityBot，一个由预训练RL代理指导的大语言模型代理，以支持网络安全操作。特别地，LLM代理配备了一个行为规范生成模块、一个积累本地经验的记忆模块、一个重新评估选择的反思模块和一个减少行动空间的行动模块。此外，它还采用了协作机制，以从预训练的RL代理那里获得建议，包括用于动态建议采纳的光标、用于多位导师建议排名的聚合器以及用于主动征询建议的呼叫器。基于CybORG实验框架，我们的经验表明，与LLM或RL单独使用相比，SecurityBot在网络安全游戏中表现出显著的性能提升，实现了互补性能。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Cybersecurity operations involve the participation of various entities such
    as attackers and defenders. With the advancement of artificial intelligence (AI),
    autonomous cyber operation (ACO) agents have emerged as a promising solution in
    cybersecurity operations Vyas et al. ([2023](#bib.bib36)). These agents continually
    engage in adversarial learning within network environments, enhancing their strategic
    capabilities. The recent proliferation of large language models (LLMs) has significantly
    bolstered the capabilities of autonomous agents Wang et al. ([2023a](#bib.bib38)).
    In comparison to traditional machine learning agents, LLM agents possess extensive
    knowledge, enabling them to handle richer and more complex information, coupled
    with robust contextual and reasoning abilities Lin et al. ([2023](#bib.bib18));
    Wang et al. ([2023b](#bib.bib39), [c](#bib.bib40)). They not only surpass state-of-the-art
    methods as novel tools Xia et al. ([2023](#bib.bib43)) but also exhibit formidable
    interactive capabilities as assistants or agents Sandoval et al. ([2023](#bib.bib30)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全操作涉及攻击者和防御者等各种实体的参与。随着人工智能（AI）的进步，自主网络操作（ACO）代理已成为网络安全操作中的一种有前途的解决方案Vyas
    et al. ([2023](#bib.bib36))。这些代理在网络环境中持续进行对抗性学习，提高其战略能力。近年来，大语言模型（LLM）的普及显著增强了自主代理的能力Wang
    et al. ([2023a](#bib.bib38))。与传统的机器学习代理相比，LLM代理具有广泛的知识，使其能够处理更丰富、更复杂的信息，同时具备强大的上下文和推理能力Lin
    et al. ([2023](#bib.bib18)); Wang et al. ([2023b](#bib.bib39), [c](#bib.bib40))。它们不仅作为新工具超越了最先进的方法Xia
    et al. ([2023](#bib.bib43))，还展现了作为助手或代理的强大互动能力Sandoval et al. ([2023](#bib.bib30))。
- en: However, LLM agents lack the specific knowledge of the local environment, incur
    higher training costs Hu et al. ([2023](#bib.bib12)) and can stuck in hallucinations
    Ji et al. ([2023](#bib.bib14)); Chen and Shu ([2023](#bib.bib5)), while also presenting
    attackers with powerful weapons, making them double-edge sword for cybersecurity
    Chen and Shu ([2023](#bib.bib5)); Taddeo et al. ([2019](#bib.bib33)). Recent research
    attempts to frame ACO as partially observable Markov processes (POMDP), employing
    reinforcement learning (RL) methods to train autonomous agents Standen et al.
    ([2021](#bib.bib32)); Team. ([2021](#bib.bib35)). However, without appropriate
    tuning methods, RL agents tend to converge to local optima, lacking robustness
    and generalization capabilities despite achieving favorable results Palmer et
    al. ([2023](#bib.bib25)). As prior studies have demonstrated that collaborations
    among multiple agents can enhance team performance Dong et al. ([2023](#bib.bib8));
    Ma et al. ([2023](#bib.bib19)), enabling the effective collaborations between
    LLM agents and RL agents, which can leverage the generalization knowledge of LLMs
    and the specialized knowledge of RLs in cybersecurity scenarios, can be promising
    to achieve complementary performance beyond that of individual agent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLM 代理缺乏对本地环境的具体知识，导致更高的训练成本 Hu 等人（[2023](#bib.bib12)），并可能陷入幻觉 Ji 等人（[2023](#bib.bib14)）；Chen
    和 Shu（[2023](#bib.bib5)），同时也为攻击者提供了强大的武器，使其成为网络安全中的双刃剑 Chen 和 Shu（[2023](#bib.bib5)）；Taddeo
    等人（[2019](#bib.bib33)）。近期的研究尝试将 ACO 框架化为部分可观察的马尔可夫过程（POMDP），采用强化学习（RL）方法来训练自主代理
    Standen 等人（[2021](#bib.bib32)）；Team（[2021](#bib.bib35)）。然而，如果没有适当的调优方法，RL 代理往往会收敛到局部最优解，尽管取得了良好的结果，但缺乏鲁棒性和泛化能力
    Palmer 等人（[2023](#bib.bib25)）。由于先前的研究已表明，多个代理之间的合作可以提高团队表现 Dong 等人（[2023](#bib.bib8)）；Ma
    等人（[2023](#bib.bib19)），因此，能够有效实现 LLM 代理和 RL 代理之间的合作，这可以利用 LLM 的泛化知识和 RL 在网络安全场景中的专业知识，有望实现超越单个代理的互补性能。
- en: 'Hence, we introduce the SecurityBot, a collaborative framework utilizing RL
    agents as mentors for LLM agent to support cybersecurity operations. We integrate
    four effective modules – profiles, memory, reflection and action – into the LLM.
    Simultaneously, we propose a dynamic mechanism consisting of a cursor to dynamically
    incorporate RL agents’ suggestions, an aggregator to rank suggestions from different
    RL agents, as well as a caller to proactively request mentoring from RL agents.
    We conduct experiments on the open-source ACO research platform, CybORG Standen
    et al. ([2021](#bib.bib32)), comparing the red team (attacker) task and blue team
    (defender) task performance among: (1)independently executing RL or LLM agents
    (Independent), (2) collaboration between a LLM agent and a RL agent (Single-Mentor),
    and (3) collaboration between a LLM agent and multiple RL agents (Multi-Mentors).
    Our experimental results demonstrate that the developed SecurityBot can effectively
    improve both the red team and blue team task performance compared to independent
    LLM or RL approaches. Furthermore, while mentoring from multiple RL agents can
    be beneficial, the guidance of poorly performing RL agents may be noise to, and
    result into unstable performance.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们引入了 SecurityBot，这是一个协作框架，利用 RL 代理作为 LLM 代理的导师，以支持网络安全操作。我们将四个有效的模块——配置文件、记忆、反思和行动——集成到
    LLM 中。同时，我们提出了一种动态机制，包括一个光标来动态整合 RL 代理的建议，一个聚合器来对来自不同 RL 代理的建议进行排名，以及一个调用者来主动请求
    RL 代理的指导。我们在开源 ACO 研究平台 CybORG Standen 等人（[2021](#bib.bib32)）上进行了实验，比对了红队（攻击者）任务和蓝队（防守者）任务的表现，比较了：（1）独立执行
    RL 或 LLM 代理（独立），（2）LLM 代理和 RL 代理之间的合作（单一导师），以及（3）LLM 代理和多个 RL 代理之间的合作（多导师）。我们的实验结果表明，开发的
    SecurityBot 相较于独立的 LLM 或 RL 方法，能够有效提高红队和蓝队任务的表现。此外，虽然来自多个 RL 代理的指导可能是有益的，但表现不佳的
    RL 代理的指导可能会成为噪声，导致性能不稳定。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce SecurityBot, a mechanism to enable the effective collaboration
    between LLM and RL agents, to leverage RL agents as mentors to accelerate learning
    for LLM agents and achieve complementary performance.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了 SecurityBot，这是一种机制，旨在实现 LLM 和 RL 代理之间的有效合作，利用 RL 代理作为导师加速 LLM 代理的学习，实现互补性能。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The collaboration of LLM and RL agents demonstrates performance improvement
    in both red team and blue team tasks, providing a promising solution of autonomous
    agents for cybersecurity operations.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 和 RL 代理的合作在红队和蓝队任务中都显示出性能提升，为网络安全操作提供了一种有前景的自主代理解决方案。
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLMs for cybersecurity operations
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLMs 在网络安全操作中的应用
- en: Given the rapid development of LLMs and the eager to incorporate advanced AIs
    into cybersecurity operations Iannone et al. ([2022](#bib.bib13)), recent studies
    have started to explore using LLMs to enhance cybersecurity while several evidences
    also reveal abusing LLMs to bring advanced threats, making it a double-edged sword
    Taddeo et al. ([2019](#bib.bib33)); Yao et al. ([2023](#bib.bib46))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 LLMs 的快速发展以及将先进 AI 融入网络安全操作的迫切需求 Iannone et al. ([2022](#bib.bib13))，最近的研究已开始探索利用
    LLMs 来增强网络安全，同时也有证据显示滥用 LLMs 可能带来高级威胁，使其成为双刃剑 Taddeo et al. ([2019](#bib.bib33));
    Yao et al. ([2023](#bib.bib46))。
- en: 2.1.1 LLM to enhance cybersecurity
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 LLMs 增强网络安全
- en: LLMs demonstrate advantages in both code security and data security Noever ([2023](#bib.bib24));
    Ali and Kostakos ([2023](#bib.bib1)); Qi et al. ([2023](#bib.bib28)). For example,
    Fuzz4All Xia et al. ([2023](#bib.bib43)) utilizes LLMs as input generators and
    mutation engines to generate diverse inputs for various programming languages,
    achieving an 36.8% coverage improvement compared to previous state-of-the-art
    techniques.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在代码安全和数据安全方面展示了优势 Noever ([2023](#bib.bib24)); Ali 和 Kostakos ([2023](#bib.bib1));
    Qi et al. ([2023](#bib.bib28))。例如，Fuzz4All Xia et al. ([2023](#bib.bib43)) 利用
    LLMs 作为输入生成器和变异引擎，为各种编程语言生成多样的输入，与之前的最先进技术相比，实现了 36.8% 的覆盖率提升。
- en: Additionally, compared to traditional machine learning approaches, LLMs possess
    more powerful natural language processing and contextual understanding capabilities,
    allowing them to elevate cybersecurity from specific to more macroscopic tasks.
    For example, some researchesDeng et al. ([2023](#bib.bib7)); Pearce et al. ([2023](#bib.bib27))
    utilized these capabilities in specific security tasks to enhance effectiveness,
    while McIntosh et al.McIntosh et al. ([2023](#bib.bib20)) take a further step
    to compared GPT-generated Governance, Risk, and Compliance (GRC) policies with
    those from established security vendors and government cybersecurity agencies,
    recommending GPT integration into companies’ GRC policy development.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，相比传统的机器学习方法，LLMs 拥有更强大的自然语言处理和上下文理解能力，使其能够将网络安全从具体任务提升到更宏观的任务。例如，一些研究 Deng
    et al. ([2023](#bib.bib7)); Pearce et al. ([2023](#bib.bib27)) 利用这些能力在特定安全任务中提高效果，而
    McIntosh et al. McIntosh et al. ([2023](#bib.bib20)) 更进一步，将 GPT 生成的治理、风险和合规 (GRC)
    政策与已建立的安全供应商和政府网络安全机构的政策进行比较，建议将 GPT 集成到公司的 GRC 政策开发中。
- en: 2.1.2 LLMs’ double-edged sword role for cybe security
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 LLMs 在网络安全中的双刃剑角色
- en: 'However, applying LLMs to cybersecurity is a double-edged sword Taddeo et al.
    ([2019](#bib.bib33)): being generative in nature can lead to hallucinations—the
    generation of misleading or incorrect content, and can not effectively discern
    security-related fallacies, which can be catastrophic for high-stakes security
    tasks Ji et al. ([2023](#bib.bib14)). These errors can compromise sensitive operations,
    thereby introducing substantial risks Chen and Shu ([2023](#bib.bib5)). As LLMs
    become more integrated into security frameworks, the imperative to address and
    mitigate these challenges grows ever more critical.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将 LLMs 应用于网络安全是双刃剑 Taddeo et al. ([2019](#bib.bib33))：由于其生成性特质可能导致幻觉——生成误导性或不正确的内容，且无法有效识别与安全相关的谬误，这对高风险安全任务可能是灾难性的
    Ji et al. ([2023](#bib.bib14))。这些错误可能危害敏感操作，从而引入重大风险 Chen 和 Shu ([2023](#bib.bib5))。随着
    LLMs 在安全框架中逐渐融合，解决和减轻这些挑战的紧迫性变得越来越关键。
- en: Furthermore, LLMs present attackers with powerful weapons. Recent studies have
    demonstrated that LLMs can significantly enhance attacks across hardware Yaman
    ([2023](#bib.bib45)), software and network Chen and Shu ([2023](#bib.bib5)) levels,
    especially that LLMs possess human-like reasoning capabilities, making user-level
    attacks even more severe Yao et al. ([2023](#bib.bib46)); Falade ([2023](#bib.bib10));
    Botacin ([2023](#bib.bib2)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs 为攻击者提供了强大的武器。最近的研究表明，LLMs 能显著增强对硬件 Yaman ([2023](#bib.bib45))、软件和网络
    Chen 和 Shu ([2023](#bib.bib5)) 级别的攻击，尤其是 LLMs 具有人类般的推理能力，使得用户级别的攻击更加严重 Yao et
    al. ([2023](#bib.bib46)); Falade ([2023](#bib.bib10)); Botacin ([2023](#bib.bib2))。
- en: 2.2 Collaboration mechanisms to improve LLMs
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 改进 LLMs 的协作机制
- en: 'Recent studies have explored different mechanisms to support LLM’s collaborations
    with others, either LLM-based or RL-based agents, including:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探讨了不同机制以支持 LLM 与其他代理的协作，包括基于 LLM 或 RL 的代理，例如：
- en: 2.2.1 Role-based multi-LLM-agent collaboration
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 基于角色的多 LLM 代理协作
- en: Within LLM-based multi-agent systems, LLM-based agents are assigned with different
    roles, like decomposing complex tasks, identifying errors, and collecting multiple
    perspectives. Then they collaborate with each other through a series of processes
    to resolve complex tasks such as software developments Dong et al. ([2023](#bib.bib8));
    Qian et al. ([2023](#bib.bib29)); Hong et al. ([2023](#bib.bib11)), sociological
    investigations Park et al. ([2023](#bib.bib26)); Wang et al. ([2023b](#bib.bib39));
    Zhang et al. ([2023](#bib.bib47)), simulation of multiplayer games Sandoval et
    al. ([2023](#bib.bib30)); Xu et al. ([2023](#bib.bib44)) and various challenges
    (such as logical reasoning, stock advice, blog composing, and more) Li et al.
    ([2023](#bib.bib17)); Wu et al. ([2023](#bib.bib42)); Talebirad and Nadiri ([2023](#bib.bib34)).
    In particularly, different role-based agents exchange ideas through conversation,
    enforce tools to undertake tasks, garner feedback, leading to successful collaboration
    Wang et al. ([2023a](#bib.bib38)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 LLM 的多智能体系统中，基于 LLM 的智能体被分配不同的角色，如分解复杂任务、识别错误和收集多种观点。然后，它们通过一系列过程相互协作，以解决复杂任务，例如软件开发
    Dong 等人 ([2023](#bib.bib8))；Qian 等人 ([2023](#bib.bib29))；Hong 等人 ([2023](#bib.bib11))，社会学调查
    Park 等人 ([2023](#bib.bib26))；Wang 等人 ([2023b](#bib.bib39))；Zhang 等人 ([2023](#bib.bib47))，多人游戏模拟
    Sandoval 等人 ([2023](#bib.bib30))；Xu 等人 ([2023](#bib.bib44)) 以及各种挑战（如逻辑推理、股票建议、博客撰写等）Li
    等人 ([2023](#bib.bib17))；Wu 等人 ([2023](#bib.bib42))；Talebirad 和 Nadiri ([2023](#bib.bib34))。特别地，不同角色的智能体通过对话交流思想，使用工具执行任务，获得反馈，从而实现成功的合作
    Wang 等人 ([2023a](#bib.bib38))。
- en: 2.2.2 Dual-process-based LLM-RL collaboration
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 基于双重过程的 LLM-RL 合作
- en: The dual process theory highlights that human cognition consists of two mental
    systems where System 1 is autonomous and characterized by rapid intuition, while
    System 2 controls slow, deliberate thinking Wason and Evans ([1974](#bib.bib41));
    Kahneman ([2011](#bib.bib15)). Grounded on this theory, SwiftSage introduces a
    framework that enables a small RL model, acting as the System 1 component, to
    collaborate with an LLM-Based agent, acting as the System 2 component. This structure
    effectively solve complex problems while reducing the cost of inference Lin et
    al. ([2023](#bib.bib18)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 双重过程理论强调，人类认知包括两个心理系统，其中系统 1 是自主的，特点是快速直觉，而系统 2 控制缓慢、深思熟虑的思维 Wason 和 Evans ([1974](#bib.bib41))；Kahneman
    ([2011](#bib.bib15))。基于这一理论，SwiftSage 引入了一个框架，使一个小型 RL 模型（作为系统 1 组件）与基于 LLM 的智能体（作为系统
    2 组件）协作。该结构有效地解决复杂问题，同时降低了推理成本 Lin 等人 ([2023](#bib.bib18))。
- en: 2.2.3 LLM setting guidance to support RL
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 LLM 设置指导以支持 RL
- en: Some recent studies incorporate the LLM to generate or learn the reward function
    for RL agents, aiming at simplifying the reward function design process Ma et
    al. ([2023](#bib.bib19)); Carta et al. ([2022](#bib.bib4)). For example, Micheli
    et al. ([2023](#bib.bib21)); Kwon et al. ([2023](#bib.bib16)); Du et al. ([2023](#bib.bib9))
    use LLM as a proxy reward function to guide RL agents in environments without
    clear reward signals. Additionally, Brohan et al. ([2023](#bib.bib3)); Dasgupta
    et al. ([2023](#bib.bib6)) utilize the LLM-Based agent as a planner to guide RL
    agent in complex and dynamic environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一些近期的研究将 LLM 融入到生成或学习 RL 智能体的奖励函数中，旨在简化奖励函数设计过程 Ma 等人 ([2023](#bib.bib19))；Carta
    等人 ([2022](#bib.bib4))。例如，Micheli 等人 ([2023](#bib.bib21))；Kwon 等人 ([2023](#bib.bib16))；Du
    等人 ([2023](#bib.bib9)) 使用 LLM 作为代理奖励函数，以指导 RL 智能体在没有明确奖励信号的环境中进行决策。此外，Brohan 等人
    ([2023](#bib.bib3))；Dasgupta 等人 ([2023](#bib.bib6)) 利用基于 LLM 的智能体作为规划者，指导 RL 智能体在复杂且动态的环境中进行决策。
- en: 2.2.4 RL acting as expert to guide LLM’s decision
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 RL 作为专家指导 LLM 的决策
- en: LLM demonstrate powerful generalization abilities, but under specific scenario,
    they perform poorly due to the lack of expert trajectories. In contrast, RL models
    possess expert trajectories. Hence, Hu et al. ([2023](#bib.bib12)); Wan et al.
    ([2022](#bib.bib37)) use RL methods assist the LLM-Based agent in comprehending
    the environment, mastering expert-like actions, which results in better effects
    and lower interaction cost instructions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 展现了强大的泛化能力，但在特定场景下，由于缺乏专家轨迹，它们的表现较差。相比之下，RL 模型拥有专家轨迹。因此，Hu 等人 ([2023](#bib.bib12))；Wan
    等人 ([2022](#bib.bib37)) 使用 RL 方法来帮助基于 LLM 的智能体理解环境，掌握类似专家的行动，从而实现更好的效果和较低的互动成本。
- en: Overall, LLMs has demonstrated promising potential in enhancing cybersecurity
    operations while their double-edged sword role raise specific concerns. Additionally,
    recent studies have explored different collaborations with LLMs but it is still
    in its early stage, especially for cybersecurity operations. Hence, using the
    cybersecurity adversarial game as the research context, we design a framework
    with four plugin modules and three collaboration mechanisms to power LLMs for
    cybersecurity operations, including both acting as attackers and defenders.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LLM在提升网络安全操作方面展示了有前景的潜力，但其双刃剑的角色也引发了特定的关注。此外，最近的研究探索了与LLM的不同合作方式，但仍处于早期阶段，特别是在网络安全操作方面。因此，基于网络安全对抗游戏的研究背景，我们设计了一个包含四个插件模块和三个合作机制的框架，以增强LLM在网络安全操作中的能力，包括作为攻击者和防御者的角色。
- en: 3 Cybersecurity Adversarial Game and Pre-trained RL Agents
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 网络安全对抗游戏与预训练的RL代理
- en: 'Before detailing our design, we start with briefly introducing our research
    context: the cybersecurity adversarial game. In particularly, we have constructed
    a cybersecurity adversarial game utilizing CybORG Standen et al. ([2021](#bib.bib32)),
    an exemplary RL-based Autonomous Cyber Operation (ACO) gym. ACO supports the creation
    of decision-making agents for both the blue team (defender) and the red team (attacker)
    in adversarial scenarios, and conveys structured and unstructured information,
    enabling the adaptation of both RL and LLM agents.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍我们的设计之前，我们首先简要介绍一下我们的研究背景：网络安全对抗游戏。特别是，我们利用CybORG Standen等人（[2021](#bib.bib32)）构建了一个网络安全对抗游戏，这是一个基于RL的自主网络操作（ACO）平台。ACO支持在对抗场景中创建决策代理，用于蓝队（防御者）和红队（攻击者），并传达结构化和非结构化的信息，促进RL和LLM代理的适应。
- en: 3.1 Cybersecurity Adversarial Games
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 网络安全对抗游戏
- en: 'The scenario adopted in this study is derived from TTCP CAGE Challenge 1 ¹¹1https://github.com/cage-challenge/cage-challenge-1,
    an open challenge on CybORG in 2021\. As illustrated in Figure [1](#S3.F1 "Figure
    1 ‣ 3.1 Cybersecurity Adversarial Games ‣ 3 Cybersecurity Adversarial Game and
    Pre-trained RL Agents ‣ Depending on yourself when you should: Mentoring LLM with
    RL agents to become the master in cybersecurity games"), the red and blue teams
    compete in a simulated network environment, which can be modeled as a partially
    observed Markov process (POMDP). At each step, the red team and blue team take
    actions sequentially in the environment, causing changes in the environmental
    state.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究采用的场景来源于TTCP CAGE Challenge 1 ¹¹1https://github.com/cage-challenge/cage-challenge-1，这是2021年在CybORG上举办的一个公开挑战。正如图[1](#S3.F1
    "图 1 ‣ 3.1 网络安全对抗游戏 ‣ 3 网络安全对抗游戏与预训练的RL代理 ‣ 依赖你自己何时：指导LLM与RL代理成为网络安全游戏的高手")所示，红队和蓝队在一个模拟网络环境中竞争，这个环境可以建模为部分观察的马尔可夫过程（POMDP）。在每一步，红队和蓝队在环境中顺序执行动作，从而导致环境状态的变化。
- en: Environment & Observation. The environment comprises a network consisting of
    13 hosts divided into three subnets. The red team commences from the footnode
    in the user subnet without knowledge of any other hosts. The blue team possesses
    information about all hosts but lacks knowledge regarding the red team’s access
    status to the hosts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 环境与观察。环境由一个包括13个主机的网络组成，这些主机被分为三个子网。红队从用户子网中的脚节点开始，且不知道其他主机的信息。蓝队掌握所有主机的信息，但不知道红队对主机的访问状态。
- en: 'For both the red and blue team RL agents, their vector observation at each
    step encompasses: (1) whether the last action is success, (2) whether the adversary
    has operated on a specific host, and (3) the red team’s access status of a specific
    host. Note that the observation is not guaranteed accurate due to the presence
    of an adversary.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于红队和蓝队的RL代理而言，它们在每一步的向量观察包括：（1）上一个动作是否成功，（2）对手是否在特定主机上操作，以及（3）红队对特定主机的访问状态。请注意，由于对手的存在，观察结果不保证准确。
- en: '![Refer to caption](img/75f065972c13620e577e8040f98e99bb.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/75f065972c13620e577e8040f98e99bb.png)'
- en: 'Figure 1: A POMDP cybersecurity adversial game. The red host in User Subnet
    represents the foot node of the red team. The blue host in Enterprise Subnet represents
    the defender host of the blue team.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个POMDP网络安全对抗游戏。用户子网中的红色主机代表红队的脚节点。企业子网中的蓝色主机代表蓝队的防御主机。
- en: 'Action & Reward. As shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Cybersecurity
    Adversarial Games ‣ 3 Cybersecurity Adversarial Game and Pre-trained RL Agents
    ‣ Depending on yourself when you should: Mentoring LLM with RL agents to become
    the master in cybersecurity games"), the two teams each have three reciprocal
    actions that cause transitions in the host’s access status. The red team achieves
    lateral movement between subnets by discovering new hosts through connections
    from the privileged host. We set the game to be zero-sum, which means that the
    blue team’s reward is the opposite of the red team’s reward. The reward at each
    step is based on the extent of red team’s exploitation,'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 动作与奖励。如图[2](#S3.F2 "图 2 ‣ 3.1 网络安全对抗游戏 ‣ 3 网络安全对抗游戏和预训练RL代理 ‣ 依靠自己当你应该时：用RL代理指导LLM成为网络安全游戏中的大师")所示，两队各有三种互相作用的动作，会导致主机访问状态的变化。红队通过从特权主机的连接中发现新主机来实现子网之间的横向移动。我们将游戏设定为零和游戏，这意味着蓝队的奖励是红队奖励的对立面。每一步的奖励基于红队的利用程度，
- en: '|  | $Reward_{t}=\sum_{i=1}^{n}V_{i,t}\times A_{i,t}$ |  | (1) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $Reward_{t}=\sum_{i=1}^{n}V_{i,t}\times A_{i,t}$ |  | (1) |'
- en: where $V_{i,t}$ respectively.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $V_{i,t}$ 分别表示。
- en: '![Refer to caption](img/395ed1bc4a6a18487ec6eefb9bd88e5f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/395ed1bc4a6a18487ec6eefb9bd88e5f.png)'
- en: 'Figure 2: Action-Status Transition. Red text represents red team actions, blue
    text represents blue team actions.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：动作-状态转换。红色文本表示红队动作，蓝色文本表示蓝队动作。
- en: 3.2 Pre-trained RL Agents
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 预训练RL代理
- en: 'In this study, we choose three representative RL algorithms to train red team
    and blue team agents²²2Our framework is flexible to use other RL algorithms.:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在本研究中，我们选择了三种代表性的RL算法来训练红队和蓝队代理²²2我们的框架灵活支持使用其他RL算法。:'
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A3C (Asynchronous Advantage Actor-Critic) Mnih et al. ([2016](#bib.bib23)) combines
    policy gradient and value function methods by asynchronously training multiple
    agents to improve efficiency.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A3C（异步优势演员-评论员）Mnih等人（[2016](#bib.bib23)）通过异步训练多个代理结合了策略梯度和价值函数方法，以提高效率。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DQN (Deep Q-Network) Mnih et al. ([2013](#bib.bib22)) utilizes deep neural networks
    to approximate the Q-value function to guide the agent’s decisions.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DQN（深度Q网络）Mnih等人（[2013](#bib.bib22)）利用深度神经网络来近似Q值函数，以指导代理的决策。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: PPO (Proximal Policy Optimization) Schulman et al. ([2017](#bib.bib31)), a policy
    gradient method, ensures stability through proximal policy optimization, restricting
    the magnitude of policy updates.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PPO（近端策略优化）Schulman等人（[2017](#bib.bib31)），一种策略梯度方法，通过近端策略优化来确保稳定性，限制策略更新的幅度。
- en: The RL-based environment facilitates agent’s training. Red team and blue team
    agents are trained separately, with one agent trained at a time. For agent’s adversary,
    we applied the fixed-strategy agents provided in CybORG. In particular, when training
    a red-team RL agent, we use a blue-team agent with fixed strategy which randomly
    performs Remove or Restore operations when encountering suspicious hosts during
    each Monitor action. When training a blue-team RL agent, the red-team agent as
    the adversary gains access to network nodes one by one based on a breadth-first
    strategy. Our approach aligns with the conventional RL training paradigm, wherein
    the agent takes an action at each step, assimilates new observations and associated
    rewards, and incrementally refines its strategic framework.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RL的环境有助于代理的训练。红队和蓝队代理分别进行训练，每次训练一个代理。对于代理的对手，我们应用了CybORG提供的固定策略代理。特别是，当训练红队RL代理时，我们使用了一个具有固定策略的蓝队代理，该代理在每次监控操作中遇到可疑主机时，会随机执行移除或恢复操作。当训练蓝队RL代理时，红队代理作为对手，按照广度优先策略逐一访问网络节点。我们的方法与传统的RL训练范式一致，其中代理在每一步采取行动，吸收新的观察结果和相关奖励，并逐步完善其战略框架。
- en: '4 SecurityBot: an LLM-based agent mentored by RL agents'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 SecurityBot：由RL代理指导的LLM基础代理
- en: 'As shown in [3](#S4.F3 "Figure 3 ‣ 4 SecurityBot: an LLM-based agent mentored
    by RL agents ‣ Depending on yourself when you should: Mentoring LLM with RL agents
    to become the master in cybersecurity games"), our SecurityBot contains three
    main parts: a LLM-based Agent, the pre-trained RL agent pool as mentors and their
    collaborative mechanisms.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](#S4.F3 "图 3 ‣ 4 SecurityBot：由RL代理指导的LLM基础代理 ‣ 依靠自己当你应该时：用RL代理指导LLM成为网络安全游戏中的大师")所示，我们的SecurityBot包含三个主要部分：一个基于LLM的代理、预训练的RL代理池作为导师及其协作机制。
- en: '![Refer to caption](img/ec4eb48580fa92c83b4c7934ccdd493a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec4eb48580fa92c83b4c7934ccdd493a.png)'
- en: 'Figure 3: The Framework of SecurityBot: LLM-based RLs-mentoring Agent for Cybersecurity
    Operation'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：安全机器人的框架：基于LLM的RL指导代理用于网络安全操作
- en: 4.1 LLM Agent Design
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LLM代理设计
- en: 'Building upon the LLM, GPT 3.5-turbo, our LLM agent includes four plugin modules
    for decision making in each step:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM GPT 3.5-turbo，我们的LLM代理包括四个插件模块，用于每一步的决策：
- en: 4.1.1 Profile module
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 角色模块
- en: 'As shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.1.1 Profile module ‣ 4.1 LLM Agent
    Design ‣ 4 SecurityBot: an LLM-based agent mentored by RL agents ‣ Depending on
    yourself when you should: Mentoring LLM with RL agents to become the master in
    cybersecurity games"), the Profile module initializes each agent’s role, goal,
    and available actions depending on its role. In particular, we design a prompt
    including the expected format for the observed environment as the input, and the
    expected output which is an action sequence including a series of actions with
    its goal, trigger, following actions, and expected outcome. When initializing
    the LLM agent, we use this prompt, together with the assigned goal, action, and
    environment format, to ask the LLM to generate an action sequence and add it to
    the profile, serving as the global behavior guidance for the LLM agent.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4](#S4.F4 "图 4 ‣ 4.1.1 角色模块 ‣ 4.1 LLM代理设计 ‣ 4 安全机器人：由RL代理指导的基于LLM的代理 ‣ 当你应该依赖自己时：通过RL代理指导LLM，成为网络安全游戏中的大师")
    所示，角色模块根据每个代理的角色初始化其角色、目标和可用的动作。特别地，我们设计了一个提示，包括观测环境的预期格式作为输入，以及预期的输出，即一个包括一系列动作的动作序列，其中包括其目标、触发器、后续动作和预期结果。在初始化LLM代理时，我们使用这个提示，以及分配的目标、动作和环境格式，来要求LLM生成一个动作序列并将其添加到角色文件中，作为LLM代理的全球行为指导。
- en: '![Refer to caption](img/00be543b3b381ec81919672b2c59e942.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00be543b3b381ec81919672b2c59e942.png)'
- en: 'Figure 4: The illustration of profile module, including the example of roles,
    goals, actions, environment format and the generated behavior guidance (the bottom
    part) as well as the process to generate the behavior guidance (the upper part).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：角色模块的示意图，包括角色、目标、动作、环境格式和生成的行为指导（底部部分）以及生成行为指导的过程（顶部部分）的示例。
- en: 4.1.2 Memory module
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 记忆模块
- en: The Memory module is used to store past experiences and search the related ones
    for decision making in each step.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆模块用于存储过去的经验，并在每一步决策时搜索相关的记忆记录。
- en: Memory Storage. The memory module stores records including the timestamp, observed
    environment, action taken, and the outcome including the action status (success
    or failure) and its reward. In particular, when storing each memory record, the
    LLM agent rates its Importance by prompting the LLM to score it on a scale of
    0 to 10.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆存储。记忆模块存储包括时间戳、观测环境、采取的行动以及包括行动状态（成功或失败）和奖励在内的结果记录。特别是，在存储每个记忆记录时，LLM代理通过提示LLM对其进行0到10的评分来评估其重要性。
- en: 'Memory Searching. When searching memories to support action selection in each
    step, the LLM agent will calculate each memory record’s Relevance and Freshness:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆搜索。在搜索记忆以支持每一步的动作选择时，LLM代理将计算每个记忆记录的相关性和新鲜度：
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Relevance: measuring its environment’s similarity with the current one. We
    transformed each environment into vectors, and then calculate their cosine similarity.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相关性：测量其环境与当前环境的相似度。我们将每个环境转化为向量，然后计算它们的余弦相似度。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Freshness: measuring its freshness, represented as the reciprocal of its timestamp
    gap with the current step.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新鲜度：测量其新鲜度，表示为其时间戳与当前步骤的时间差的倒数。
- en: Finally, we calculate the product of the importance, relevance and freshness
    for each memory record and select the top two as the memory input for LLM when
    making decision.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算每个记忆记录的重要性、相关性和新鲜度的乘积，并选择前两个作为LLM决策时的记忆输入。
- en: 4.1.3 Action module
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 动作模块
- en: The Action module plays a crucial role in guiding the LLM agent to take valid
    action for each step. In particularly, given the observed environment and the
    available actions provided by the profile, this module will generate the action
    space with all the potential actions that the agent could take.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 动作模块在指导LLM代理每一步采取有效动作方面发挥了关键作用。特别是，考虑到观测到的环境和角色提供的可用动作，该模块将生成包含代理可以采取的所有潜在动作的动作空间。
- en: 4.1.4 Reflection module
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 反思模块
- en: Given the complex and dynamic environment, as the adversary agent may change
    the environment but is unobservant to the LLM agent, the LLM agent may encounter
    dilemmas situation, reflected as repetitive actions or diminishing rewards. For
    example, the red agent might persist in attacking a host in the network, even
    when such an action has been proven futile. Hence, the reflection module is designed
    to monitor the dilemma status and trigger the reflection process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于复杂且动态的环境，敌对代理可能会改变环境但对 LLM 代理视而不见，LLM 代理可能会遇到困境情况，这表现为重复的动作或奖励减少。例如，红色代理可能会坚持攻击网络中的主机，即使这种行为已被证明徒劳。因此，反射模块被设计为监控困境状态并触发反射过程。
- en: '![Refer to caption](img/1a453d58b0aa486f96419a03ca10531a.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1a453d58b0aa486f96419a03ca10531a.png)'
- en: 'Figure 5: The prompt for Red Agent from the reflection module to motivate the
    LLM to choose other attack actions.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：来自反射模块的 Red Agent 提示，以激励 LLM 选择其他攻击动作。
- en: Dilemmas Monitor. At every step, the Reflection module evaluates both the Reward
    List and the Action List from the previous steps. If there is no increase in rewards
    or if the agent repeats an action, the module will collect these suspicious actions,
    including the series of actions associated with those records, and then activate
    the reflection process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 困境监控。在每一步，反射模块都会评估之前步骤中的奖励列表和动作列表。如果奖励没有增加或代理重复了某个动作，模块将收集这些可疑的动作，包括与这些记录相关的动作系列，然后激活反射过程。
- en: 'Reflection Process. The reflection process will pass these suspicious actions
    to the Action module and remove them if they are included in the generated action
    space. Additionally, as shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.1.4 Reflection
    module ‣ 4.1 LLM Agent Design ‣ 4 SecurityBot: an LLM-based agent mentored by
    RL agents ‣ Depending on yourself when you should: Mentoring LLM with RL agents
    to become the master in cybersecurity games"), the process provides the LLM with
    a prompt, elucidating that the agent is stuck in the dilemmas and providing the
    possible reasons to guide the LLM to choose other actions to get out of the dilemma
    situation.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '反射过程。反射过程会将这些可疑操作传递给 Action 模块，并在它们被包含在生成的动作空间中时将其移除。此外，如图[5](#S4.F5 "Figure
    5 ‣ 4.1.4 Reflection module ‣ 4.1 LLM Agent Design ‣ 4 SecurityBot: an LLM-based
    agent mentored by RL agents ‣ Depending on yourself when you should: Mentoring
    LLM with RL agents to become the master in cybersecurity games")所示，该过程向 LLM 提供提示，阐明代理在困境中，并提供可能的原因以引导
    LLM 选择其他动作以摆脱困境。'
- en: 4.2 Collaboration with RL agents
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 与 RL 代理的协作
- en: 'Using RL agents as mentors to guide the LLM agent is critical for SecurityBot
    to achieve better performance. More specifically, as shown in Figure [6](#S4.F6
    "Figure 6 ‣ 4.2 Collaboration with RL agents ‣ 4 SecurityBot: an LLM-based agent
    mentored by RL agents ‣ Depending on yourself when you should: Mentoring LLM with
    RL agents to become the master in cybersecurity games"), we design three collaboration
    mechanisms:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 RL 代理作为导师来指导 LLM 代理对于 SecurityBot 实现更好的性能至关重要。更具体地，如图[6](#S4.F6 "Figure
    6 ‣ 4.2 Collaboration with RL agents ‣ 4 SecurityBot: an LLM-based agent mentored
    by RL agents ‣ Depending on yourself when you should: Mentoring LLM with RL agents
    to become the master in cybersecurity games")所示，我们设计了三种协作机制：'
- en: '![Refer to caption](img/f6be7e68850dbc062bc2db4e1e420535.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f6be7e68850dbc062bc2db4e1e420535.png)'
- en: 'Figure 6: Mechanisms to collaboration with RL agents. Different color refers
    to suggestions of different RL mentors.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：与 RL 代理的协作机制。不同的颜色代表不同 RL 导师的建议。
- en: '4.2.1 Cursor: growing to be independent'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 Cursor：成长为独立
- en: Firstly, the RL agents are pre-trained in the same environment particularly
    to guarantee that they can provide knowledge to mentor the LLM agent to make better
    decisions, especially in the early stage when LLM agents contain no information
    regarding the environment. However, as time goes by, the LLM agent, with its capacity
    to understand complex environments and accumulated experience, can surpass the
    RL mentors (which we will report later). Hence, we design the mechanism Cursor
    to decide whether the LLM agent should take suggestions from RL agents.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，RL 代理在相同的环境中进行预训练，特别是为了确保它们能够提供知识以指导 LLM 代理做出更好的决策，尤其是在 LLM 代理对环境没有信息的早期阶段。然而，随着时间的推移，LLM
    代理凭借其理解复杂环境的能力和积累的经验，可以超越 RL 导师（我们将稍后报告）。因此，我们设计了机制 Cursor 来决定 LLM 代理是否应听取 RL
    代理的建议。
- en: In particular, for each step $t$ to represent the minimal reward increment that
    we would expect the LLM agent to gain.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，对于每一步 $t$，以表示我们期望 LLM 代理获得的最小奖励增量。
- en: '|  |  $1$2  |  | (2) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  $1$2  |  | (2) |'
- en: '|  | $\leavevmode\resizebox{108.405pt}{}{$\displaystyle f_{x}=\frac{1}{1+e^{-kx}}$}.$
    |  | (3) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\leavevmode\resizebox{108.405pt}{}{$\displaystyle f_{x}=\frac{1}{1+e^{-kx}}$}.$
    |  | (3) |'
- en: '|  | $$\leavevmode\resizebox{195.12767pt}{}{$\displaystyle sgn(x)=\{\begin{array}[]{c}-1\
    \ \ if\ x> |  | (4) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\leavevmode\resizebox{195.12767pt}{}{$\displaystyle sgn(x)=\{\begin{array}[]{c}-1\
    \ \ if\ x>0\end{array}\}$} |  | (4) |'
- en: '4.2.2 Aggregator: ranking suggestions from multiple mentors'
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 聚合器：对来自多个导师的建议进行排名
- en: Rather than relying on only one RL agent, the LLM can refer to multiple RL agents,
    as different RL agents may catch different aspects of the tasks. Hence, we further
    introduce the aggregator mechanism to aggregate suggestions from multiple RL agents.
    In particular, given the top three suggestions from all the RL mentors associated
    with confidence, the multi-mentor mechanism will sort them based on the confidence
    and the top one will be presented to the LLM and the top three actions will be
    provided while in dilemmas. In such a case, the LLM agent does not necessarily
    always get suggestions from one specific RL agent during the whole task duration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: LLM不仅依赖于一个RL代理，还可以参考多个RL代理，因为不同的RL代理可能捕捉到任务的不同方面。因此，我们进一步引入了聚合机制，以汇总来自多个RL代理的建议。特别是，在所有RL导师提供的前三个建议中，基于信心的多导师机制将对这些建议进行排序，并将信心最高的建议提供给LLM，在困境中提供前三个行动。在这种情况下，LLM代理在整个任务过程中不一定总是从一个特定的RL代理那里获得建议。
- en: '4.2.3 Caller: asking for help proactively when in dilemma'
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 调用者：在困境中主动寻求帮助
- en: As discussed above, when the LLM agent encounters a dilemma, the reflection
    module will be activated. Furthermore, beyond activating the reflection process,
    the LLM agent can further refer to RL agents for support. Unlike referring to
    RL mentors’ input in normal situation where only one suggestion is provided, we
    will provide the top three confident suggestions from the RL mentors.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，当LLM代理遇到困境时，反思模块将被激活。此外，除了激活反思过程外，LLM代理还可以进一步向RL代理寻求支持。与在正常情况下只提供一个建议的RL导师输入不同，我们将提供来自RL导师的前三个最有信心的建议。
- en: 5 Experiments and Results
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验与结果
- en: 5.1 Experiment Setup
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Environment. Following the setup of Cage Challenge 1, we set the maximum number
    of steps in one episode, i.e., a complete round of the game, to be 100\. As mentioned
    earlier, we set two reward parameters as shown in Table [1](#S5.T1 "Table 1 ‣
    5.1 Experiment Setup ‣ 5 Experiments and Results ‣ Depending on yourself when
    you should: Mentoring LLM with RL agents to become the master in cybersecurity
    games"): (1) Host value. The hosts in different subnets have different values,
    and (2) Access state. The higher the access state of a host, the higher the proportion
    of host value obtained by the red team.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 环境。按照Cage Challenge 1的设置，我们将一个回合的最大步数设置为100。正如之前提到的，我们设置了两个奖励参数，如表[1](#S5.T1
    "表1 ‣ 5.1 实验设置 ‣ 5 实验与结果 ‣ 依赖你自己何时：使用RL代理指导LLM成为网络安全游戏的高手")所示：（1）主机值。不同子网中的主机具有不同的值，（2）访问状态。主机的访问状态越高，红队获得的主机值比例也越高。
- en: 'Table 1: Parameters of agent reward.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：代理奖励的参数。
- en: '| Host Subnet(V) | Reward | Access status(A) | Reward |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 主机子网(V) | 奖励 | 访问状态(A) | 奖励 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| User Subnet | 0.1 | Unknown/Known | 0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 用户子网 | 0.1 | 未知/已知 | 0 |'
- en: '| Enterprise Subnet | 1.0 | Exploited | 0.5 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 企业子网 | 1.0 | 已被利用 | 0.5 |'
- en: '| Operational Subnet | 10.0 | Privileged | 0.89 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 操作子网 | 10.0 | 特权 | 0.89 |'
- en: RL Training. The RL training process is based on the Ray RLlib, a Python library
    for RL³³3We focuses on the collaboration between RL agents and LLM agents, rather
    than training a better RL agent. Hence, we choose the adversary using the simplest
    strategy and default parameters without parameter tuning for the training algorithms
    are used. All specific algorithm parameters can refer to https://github.com/ray-project/ray/blob/master/rllib/algorithms/.
    Each training process consists of a total of 100 iterations (4000 episodes in
    total).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: RL训练。RL训练过程基于Ray RLlib，这是一个用于RL的Python库³³3我们专注于RL代理和LLM代理之间的协作，而不是训练一个更好的RL代理。因此，我们选择使用最简单的策略和默认参数的对抗者进行训练算法，而没有对参数进行调整。所有具体的算法参数可以参考https://github.com/ray-project/ray/blob/master/rllib/algorithms/。每个训练过程包含总共100次迭代（总共4000回合）。
- en: LLMs Setup. We leverage OpenAI’s gpt-3.5-turbo API for building the LLM Agent.
    All the temperatures are set to 0 to restrict the format of LLM output.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: LLM设置。我们利用OpenAI的gpt-3.5-turbo API构建LLM代理。所有温度参数都设置为0，以限制LLM输出的格式。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reflection. If the action is repeated in the last three steps, or if there is
    no increase in reward values in the last five steps, the reflection mechanism
    will be triggered.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反射。如果在最后三步中重复了相同的动作，或者在最后五步中没有奖励值的增加，则会触发反射机制。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Cursor. $\theta_{ind}$ is set to 0.0135.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 游标。$\theta_{ind}$ 设置为 0.0135。
- en: Measurement. We consider the following measurements.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 测量。我们考虑了以下测量指标。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Step reward. The reward of each step.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤奖励。每一步的奖励。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Collaboration Rate ($Col$). The rate of cooperation with RL agents.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 合作率（$Col$）。与 RL 代理合作的比率。
- en: •
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Dilemma Rate ($DR$). The rate of collaborating with RL agents triggered by trapping
    into dilemma situation.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 困境率（$DR$）。在陷入困境情境时与 RL 代理合作的比率。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Accept Rate ($AR$). The rate that LLM agent takes RL mentor’s suggestion, indicating
    the extent to which LLM Agent relies on RL mentors.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接受率（$AR$）。LLM 代理采纳 RL 导师建议的比率，指示 LLM 代理依赖 RL 导师的程度。
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Accept Rate in dilemma ($AR_{d}$). The rate LLM agent take suggestions when
    trapping into dilemma, showing the ability of RL mentors to help LLM Agent out.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 困境中的接受率（$AR_{d}$）。LLM 代理在陷入困境时采纳建议的比率，显示 RL 导师帮助 LLM 代理脱困的能力。
- en: Experiment Group. We incrementally add collaboration modules and assess their
    performance for both the red and blue team. For each group, we run the simulation
    for 5 times and calculate the average.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实验组。我们逐步添加协作模块，并评估其在红队和蓝队中的表现。对于每个组，我们运行 5 次模拟并计算平均值。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Independent. Each RL agent (A3C, DQN, PPO) and our designed LLM agent conduct
    the task independently.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 独立。每个 RL 代理（A3C、DQN、PPO）和我们设计的 LLM 代理独立执行任务。
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Single-mentor. The LLM agent cooperate with a single RL agent (A3C&LLM, DQN&LLM,
    PPO&LLM).
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单导师。LLM 代理与单一 RL 代理合作（A3C&LLM，DQN&LLM，PPO&LLM）。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-mentors. LLM agent cooperate with all three different RL agents (MultiMentor).
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多导师。LLM 代理与三种不同的 RL 代理合作（MultiMentor）。
- en: 5.2 Performance in Red Team Task
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 红队任务中的表现
- en: In the red team task, LLM agents and RL agents exhibited distinct action patterns,
    indicative of differing knowledge bases. While collaborative synergy can surpass
    individual agent performance, optimal collaboration is achieved when RL agents
    exhibit superior performance. However, when the LLM agents considers suggestions
    from multiple RL agents, it struggles to efficiently process this information,
    leading to a decline in collaborative performance.⁴⁴4We smoothed the data using
    exponential smoothing and calculated confidence intervals
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在红队任务中，LLM 代理和 RL 代理展示了不同的行动模式，这表明它们拥有不同的知识基础。虽然协同效应可以超越单一代理的表现，但当 RL 代理表现更优时，协作效果达到最佳。然而，当
    LLM 代理考虑来自多个 RL 代理的建议时，它会在有效处理这些信息方面遇到困难，导致协作表现下降。⁴⁴4 我们使用指数平滑法对数据进行了平滑处理，并计算了置信区间。
- en: '![Refer to caption](img/4e0fa348944f74b9b2164c88d733c03d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e0fa348944f74b9b2164c88d733c03d.png)'
- en: 'Figure 7: Result of red team task. (a) Comparison between LLM and PPO. They
    have different performances in different stages. (b)Single RL mentor result. PPO&LLM
    surpasses all others. (c)Comparison between Multi and Single RL mentor. PPO&LLM
    still performs best'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：红队任务结果。（a）LLM 和 PPO 的比较。它们在不同阶段的表现不同。（b）单 RL 导师结果。PPO&LLM 超过了其他所有。（c）多 RL
    导师与单 RL 导师的比较。PPO&LLM 仍然表现最佳。
- en: 5.2.1 Complement knowledge of LLM agents and RL mentors
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 LLM 代理和 RL 导师的知识补充
- en: 'As depicted in Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Performance in Red Team Task
    ‣ 5 Experiments and Results ‣ Depending on yourself when you should: Mentoring
    LLM with RL agents to become the master in cybersecurity games")(a)⁵⁵5The performance
    of the three RL agents varies, while the PPO agent demonstrating superior performance.
    Due to space limitation, we only report the performance for PPO agent., the reward
    curves of the LLM agent and the PPO agent intersect: the PPO agent rapidly accumulates
    rewards early on, leveling off later. This behavior arises from the PPO agent
    gaining environmental knowledge during training, recognizing the high value of
    hosts in the Operational subnet. While exhibiting depth-first characteristics,
    insufficient training causes it to converge to a local optimum.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [7](#S5.F7 "图 7 ‣ 5.2 红队任务中的性能 ‣ 5 实验与结果 ‣ 你应该在何时依赖自己：用 RL 代理指导 LLM 成为网络安全游戏的高手")
    所示⁵⁵5三种 RL 代理的表现各不相同，而 PPO 代理表现优异。由于空间限制，我们仅报告了 PPO 代理的性能，LLM 代理和 PPO 代理的奖励曲线交叉：PPO
    代理在早期迅速累积奖励，后期趋于平稳。这种行为源于 PPO 代理在训练过程中获取环境知识，识别出操作子网中主机的高价值。虽然表现出深度优先的特点，但由于训练不足，它趋向于收敛到局部最优。
- en: Conversely, the LLM agent, despite a modest early-stage reward, achieves rapid
    growth, outperforming the PPO agent in the later stage. The LLM agent’s behavior
    follows a breadth-first pattern, accumulating more exploited hosts in the network
    efficiently avoiding defender blocks, resulting in a higher reward.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，尽管 LLM 代理在早期阶段的奖励较少，但在后期取得了快速增长，优于 PPO 代理。LLM 代理的行为遵循广度优先的模式，能更高效地利用网络中的主机并避开防御者的阻挡，从而获得更高的奖励。
- en: Taking a step further, we find that LLM agents outperform PPO agents in single-step
    gains occurring at step 53 on average, where we differentiate the early and later
    stages. In later stage, we find that RL mentors always repeat one action, while
    LLM agent, with the reflection module, can prevent the problem. This can be the
    reason why RL mentors’ performance is worse than LLM agent in the stage.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分析发现，LLM 代理在第 53 步的单步增益中优于 PPO 代理，在这里我们区分了早期和后期阶段。在后期阶段，我们发现 RL 导师总是重复一个动作，而具有反射模块的
    LLM 代理可以防止这一问题。这可能是 RL 导师在该阶段表现不如 LLM 代理的原因。
- en: 5.2.2 Amplification effect of single-mentor mechanisms
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 单导师机制的放大效应
- en: A stronger RL mentor enhances collaborative performance, otherwise it may slows
    down the LLM agent’s process. As shown in Figure [7](#S5.F7 $$ effect throughout
    the process, as well as getting into the rapid-growth phase much earlier.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 更强的 RL 导师可以提升协作性能，否则可能会减缓 LLM 代理的进程。如图 [7](#S5.F7 $$ 整个过程中效果，以及更早进入快速增长阶段) 所示。
- en: 'Table 2: Cooperation metric of red team task'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：红队任务的合作指标
- en: '| Metric | PPO&LLM | A3C&LLM | DQN&LLM |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | PPO&LLM | A3C&LLM | DQN&LLM |'
- en: '| --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | $Early\backslash Later$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $早期\backslash 后期$ |'
- en: '| $Col$ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| $Col$ |'
- en: '| $DR$ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| $DR$ |'
- en: '| $AR$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| $AR$ |'
- en: '| $AR_{d}$ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| $AR_{d}$ |'
- en: 'Furthermore, the cooperation mechanism guides the LLM agent to learn from RL
    mentors in the early stage while seeking help in dilemmas. As shown in Table [2](#S5.T2
    "Table 2 ‣ 5.2.2 Amplification effect of single-mentor mechanisms ‣ 5.2 Performance
    in Red Team Task ‣ 5 Experiments and Results ‣ Depending on yourself when you
    should: Mentoring LLM with RL agents to become the master in cybersecurity games"),
    the LLM agent collaborates more with RL mentors in the early stages than later,
    satisfying our design goal. $DR$ values are both higher in the later stage, meaning
    in the later stage, despite outperforming the RL mentor, the LLM agent relies
    more on the RL mentor’s suggestions if needed.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，合作机制指导 LLM 代理在早期向 RL 导师学习，同时在困境中寻求帮助。如表 [2](#S5.T2 "表 2 ‣ 5.2.2 单导师机制的放大效应
    ‣ 5.2 红队任务中的性能 ‣ 5 实验与结果 ‣ 你应该在何时依赖自己：用 RL 代理指导 LLM 成为网络安全游戏的高手") 所示，LLM 代理在早期阶段比后期阶段更频繁地与
    RL 导师合作，这满足了我们的设计目标。$DR$ 值在后期阶段均较高，这意味着尽管 LLM 代理在后期阶段表现优于 RL 导师，但在需要时更依赖于 RL 导师的建议。
- en: 5.2.3 Noise from multi-mentors
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 多导师噪声
- en: 'We explored whether the LLM agent could gain more knowledge from recommendations
    of multi-mentors. In our setup, assistance from multiple RL mentors is not necessary
    helpful. As shown in Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Performance in Red Team
    Task ‣ 5 Experiments and Results ‣ Depending on yourself when you should: Mentoring
    LLM with RL agents to become the master in cybersecurity games")(c), while the
    performance of multi-tutors slightly outperforms LLM alone, it falls short of
    the LLM&PPO group. We observed that 75.61% of suggestions from RL mentors originated
    from DQN, but only 5.41% were accepted. In contrast, 34.61% of PPO’s suggestions
    were accepted. Moreover, only 15.85% of all RL suggestions were accepted, markedly
    lower than the acceptance rate in a single mentor scenario. This disparity illuminates
    the high confidence suggestion from the low performance mentor became a noise
    for the LLM agent.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了LLM代理是否可以从多位导师的建议中获得更多知识。在我们的设置中，多位RL导师的帮助并不一定有益。如图[7](#S5.F7 "图 7 ‣ 5.2
    红队任务表现 ‣ 5 实验与结果 ‣ 依赖你自己时何时：用RL代理辅导LLM以成为网络安全游戏中的大师")(c)所示，虽然多位导师的表现略优于单独的LLM，但不如LLM&PPO组。我们观察到75.61%的RL导师建议来自DQN，但只有5.41%被接受。相比之下，PPO的建议有34.61%被接受。此外，所有RL建议中只有15.85%被接受，明显低于单一导师情境下的接受率。这种差异揭示了来自低表现导师的高自信建议对LLM代理成为了噪声。
- en: 5.3 Performance in Blue Team Task
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 蓝队任务中的表现
- en: '![Refer to caption](img/21ff6a96c9c547af76d8df70d748dd40.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/21ff6a96c9c547af76d8df70d748dd40.png)'
- en: 'Figure 8: Result of blue team task. (a)Comparison between LLM and PPO. LLM
    outperform PPO in Blue Team Task. (b) Single RL mentor result. PPO&LLM perform
    slightly better than LLM. (c) Comparison between Multi and Single RL mentor.Multi-mentor
    perform best on average, but not stable enough.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：蓝队任务结果。（a）LLM与PPO的比较。LLM在蓝队任务中优于PPO。（b）单一RL导师结果。PPO&LLM的表现略好于LLM。（c）多位与单一RL导师的比较。多位导师平均表现最佳，但稳定性不足。
- en: 5.3.1 A helpful but narrower complementary knowledge
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 有用但较窄的补充知识
- en: 'As shown in Figure [8](#S5.F8 "Figure 8 ‣ 5.3 Performance in Blue Team Task
    ‣ 5 Experiments and Results ‣ Depending on yourself when you should: Mentoring
    LLM with RL agents to become the master in cybersecurity games") (a), the LLM
    agents demonstrate performance similar to the PPO agent during the early stages.
    But after a brief period of divergence, the LLM agent consistently outperforms
    the PPO agent. We observe the similar situation in the case of single mentor.
    As shown in Figure [8](#S5.F8 "Figure 8 ‣ 5.3 Performance in Blue Team Task ‣
    5 Experiments and Results ‣ Depending on yourself when you should: Mentoring LLM
    with RL agents to become the master in cybersecurity games") (b), although PPO&LLM
    group demonstrates a marginally superior performance over LLM agent, this advantage
    is not observed in other groups. These results indicate a narrower knowledge gap
    between LLM and RL agents in blue team task, may due to the fact that the whole
    network environment is used for pre-training RL agents and provided to LLM agent.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[8](#S5.F8 "图 8 ‣ 5.3 蓝队任务表现 ‣ 5 实验与结果 ‣ 依赖你自己时何时：用RL代理辅导LLM以成为网络安全游戏中的大师")
    (a)所示，LLM代理在早期阶段表现与PPO代理相似。但经过短暂的分歧后，LLM代理始终优于PPO代理。单一导师的情况也观察到了类似的现象。如图[8](#S5.F8
    "图 8 ‣ 5.3 蓝队任务表现 ‣ 5 实验与结果 ‣ 依赖你自己时何时：用RL代理辅导LLM以成为网络安全游戏中的大师") (b)所示，尽管PPO&LLM组表现略优于LLM代理，但在其他组中未观察到这种优势。这些结果表明，在蓝队任务中LLM与RL代理之间的知识差距较小，这可能是因为整个网络环境用于对RL代理进行预训练，并提供给LLM代理。
- en: 'Additionally, as reported in Table [3](#S5.T3 "Table 3 ‣ 5.3.1 A helpful but
    narrower complementary knowledge ‣ 5.3 Performance in Blue Team Task ‣ 5 Experiments
    and Results ‣ Depending on yourself when you should: Mentoring LLM with RL agents
    to become the master in cybersecurity games"), the LLM agent would accept RL mentors’
    suggestions in the early stages. While in the later stage, both A3C&LLM and DQN&LLM
    groups show little interest in RL mentor’s suggestion except trapped in dilemmas.
    Conversely, we can observe consistently higher $AR$ rates in the later stages
    for PPO&LLM. This discrepancy indicates the LLM agent’s capability in identifying
    the suggestion quality and the importance of providing high quality suggestion
    to improve the LLM agent’s effectiveness.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，如表格[3](#S5.T3 "Table 3 ‣ 5.3.1 A helpful but narrower complementary knowledge
    ‣ 5.3 Performance in Blue Team Task ‣ 5 Experiments and Results ‣ Depending on
    yourself when you should: Mentoring LLM with RL agents to become the master in
    cybersecurity games")所示，LLM代理在早期阶段会接受RL导师的建议。而在后期阶段，A3C&LLM和DQN&LLM组对RL导师的建议兴趣不大，除非陷入困境。相反，我们可以观察到PPO&LLM在后期阶段的$AR$率持续较高。这一差异表明LLM代理识别建议质量的能力以及提供高质量建议以提高LLM代理效果的重要性。'
- en: 'Table 3: Cooperation result in blue team task'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：蓝队任务中的合作结果
- en: '| Metric | PPO&LLM | A3C&LLM | DQN&LLM |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Metric | PPO&LLM | A3C&LLM | DQN&LLM |'
- en: '| --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | $Early\backslash Later$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | $Early\backslash Later$ |'
- en: '| $Col$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| $Col$ |'
- en: '| $DR$ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| $DR$ |'
- en: '| $AR$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| $AR$ |'
- en: '| $AR_{d}$ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| $AR_{d}$ |'
- en: 5.3.2 Outstanding but unstable performance of multi-mentors
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 多导师的杰出但不稳定表现
- en: 'In contrast to the red team task, as shown in Figure [8](#S5.F8 "Figure 8 ‣
    5.3 Performance in Blue Team Task ‣ 5 Experiments and Results ‣ Depending on yourself
    when you should: Mentoring LLM with RL agents to become the master in cybersecurity
    games") (c), the incorporation of multiple RL mentors enhances the average performance
    of the blue team task beyond that of both the LLM agents and the PPO&LLM group.
    However, this configuration exhibits instability demonstrated as a larger confidence
    intervals. While it effectively defends nearly all hosts at times, in some instances,
    its performance is comparable to that of a single LLM. Notably, the LLM Agent
    accepts less than 5% of suggestions from RL mentors, predominantly originating
    from DQN. One reason behind this is that the most confident RL suggestions are
    not consistently the most effective, especially when provided by multiple mentors.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '与红队任务相比，如图[8](#S5.F8 "Figure 8 ‣ 5.3 Performance in Blue Team Task ‣ 5 Experiments
    and Results ‣ Depending on yourself when you should: Mentoring LLM with RL agents
    to become the master in cybersecurity games") (c)所示，引入多个RL导师提高了蓝队任务的平均表现，超出了LLM代理和PPO&LLM组的表现。然而，这种配置表现出不稳定性，表现为较大的置信区间。虽然在某些时候能够有效防御几乎所有主机，但在一些情况下，其表现与单个LLM相当。值得注意的是，LLM代理接受的RL导师建议不到5%，主要来自DQN。原因之一是，最自信的RL建议并不总是最有效的，特别是当由多个导师提供时。'
- en: Additionally, in the blue team task, LLM agents showcase a superior understanding
    of the environment, often acting independently in most situations. Particularly
    in scenarios where the LLM agent successfully defends almost all hosts, it appears
    to disregard unreliable suggestions from multiple RL mentors, opting to make critical
    decisions autonomously.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在蓝队任务中，LLM代理展现出对环境的卓越理解，通常在大多数情况下独立行动。特别是在LLM代理成功防御几乎所有主机的场景中，它似乎忽视了多个RL导师的不可靠建议，选择自主做出关键决策。
- en: 6 Conclusion and Future Work
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: This study presents SecurityBot, a LLM agent powered by mentoring from pre-trained
    RL agents for cybersecurity operations. In particular, with the designed plugin
    modules, including the profile, memory, reflection and action modules to enhance
    the LLM, and three collaboration mechanisms, including a cursor, an aggregator
    and a caller, to effectively collaborate with pre-trained RL agents, the LLM agent
    achieve significant performance improvement in both cyber attack and defense tasks.
    Although RL agents can learn local knowledge effectively through pre-training,
    the LLM agent can surpass them through learning the environments in the later
    stage. This confirms that our designed LLM agent can be a promising solution to
    support cybersecurity operations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究介绍了SecurityBot，这是一个由预训练的强化学习（RL）代理提供指导的LLM代理，用于网络安全操作。特别是，通过设计的插件模块，包括配置文件、记忆、反思和行动模块来增强LLM，以及三个协作机制，包括光标、聚合器和呼叫器，以有效地与预训练的RL代理协作，使得LLM代理在网络攻击和防御任务中实现了显著的性能提升。尽管RL代理可以通过预训练有效地学习局部知识，但LLM代理通过在后期学习环境可以超越它们。这确认了我们设计的LLM代理可以成为支持网络安全操作的有前景的解决方案。
- en: While RL agents’ suggestions can be helpful, especially when the LLM agent trapped
    in dilemmas, as observed in our result, weak RL agents may serve as a noise to
    distract the LLM agent. Further research can design advanced aggregating strategies
    to extract the essence and discard the dross from RL agents. Furthermore, while
    we aim at empowering LLM with plugin modules and collaborating it with RL agents,
    we did not fintune the LLM or optimize the RL agents. Future studies can fintune
    a better LLM model specific for cybersecurity operations and train optimized RL
    agents, which could further improve the SecurityBot’s performance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RL代理的建议可能很有帮助，特别是当LLM代理陷入困境时，正如我们的结果所示，弱RL代理可能会作为噪音干扰LLM代理。进一步的研究可以设计先进的聚合策略，以提取精华，丢弃RL代理的糟粕。此外，虽然我们旨在通过插件模块增强LLM并与RL代理进行协作，但我们没有对LLM进行微调或优化RL代理。未来的研究可以微调一个更适合网络安全操作的LLM模型，并训练优化的RL代理，这可能进一步提升SecurityBot的性能。
- en: References
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ali and Kostakos [2023] Tarek Ali and Panos Kostakos. Huntgpt: Integrating
    machine learning-based anomaly detection and explainable ai with large language
    models (llms). arXiv preprint arXiv:2309.16021, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ali and Kostakos [2023] Tarek Ali 和 Panos Kostakos. Huntgpt: 将基于机器学习的异常检测和可解释人工智能与大型语言模型（LLMs）整合.
    arXiv预印本 arXiv:2309.16021, 2023.'
- en: 'Botacin [2023] Marcus Botacin. Gpthreats-3: Is automatic malware generation
    a threat? In 2023 IEEE Security and Privacy Workshops (SPW), pages 238–254\. IEEE,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Botacin [2023] Marcus Botacin. Gpthreats-3: 自动恶意软件生成是否构成威胁？ 在2023年IEEE安全与隐私研讨会（SPW），页码
    238–254. IEEE, 2023.'
- en: 'Brohan et al. [2023] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    et al. Do as i can, not as i say: Grounding language in robotic affordances. In
    Conference on Robot Learning, pages 287–318\. PMLR, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brohan et al. [2023] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    等. 以我能做的为准，不要以我说的为准: 将语言与机器人可用性结合. 在机器人学习会议上, 页码 287–318. PMLR, 2023.'
- en: 'Carta et al. [2022] Thomas Carta, Pierre-Yves Oudeyer, Olivier Sigaud, and
    Sylvain Lamprier. Eager: Asking and answering questions for automatic reward shaping
    in language-guided rl. Advances in Neural Information Processing Systems, 35:12478–12490,
    2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carta et al. [2022] Thomas Carta, Pierre-Yves Oudeyer, Olivier Sigaud, 和 Sylvain
    Lamprier. Eager: 提问和回答用于自动奖励塑造的语言指导RL. 神经信息处理系统进展, 35:12478–12490, 2022.'
- en: Chen and Shu [2023] Canyu Chen and Kai Shu. Can llm-generated misinformation
    be detected? arXiv preprint arXiv:2309.13788, 2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen and Shu [2023] Canyu Chen 和 Kai Shu. LLM生成的虚假信息是否可以被检测？ arXiv预印本 arXiv:2309.13788,
    2023.
- en: Dasgupta et al. [2023] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino,
    Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. Collaborating with language
    models for embodied reasoning. arXiv preprint arXiv:2302.00763, 2023.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta et al. [2023] Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino,
    Arun Ahuja, Sheila Babayan, Felix Hill, 和 Rob Fergus. 与语言模型合作进行具身推理. arXiv预印本
    arXiv:2302.00763, 2023.
- en: 'Deng et al. [2023] Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang
    Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, and Stefan Rass. Pentestgpt:
    An llm-empowered automatic penetration testing tool. arXiv preprint arXiv:2308.06782,
    2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. [2023] Gelei Deng, Yi Liu, Víctor Mayoral-Vilches, Peng Liu, Yuekang
    Li, Yuan Xu, Tianwei Zhang, Yang Liu, Martin Pinzger, 和 Stefan Rass. Pentestgpt:
    一种LLM增强的自动渗透测试工具. arXiv预印本 arXiv:2308.06782, 2023.'
- en: Dong et al. [2023] Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration
    code generation via chatgpt. arXiv preprint arXiv:2304.07590, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等人[2023] Yihong Dong, Xue Jiang, Zhi Jin和Ge Li。通过chatgpt自我协作代码生成。arXiv预印本
    arXiv:2304.07590，2023年。
- en: Du et al. [2023] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor
    Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretraining
    in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692,
    2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等人[2023] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell,
    Pieter Abbeel, Abhishek Gupta和Jacob Andreas。利用大语言模型指导强化学习中的预训练。arXiv预印本 arXiv:2302.06692，2023年。
- en: 'Falade [2023] Polra Victor Falade. Decoding the threat landscape: Chatgpt,
    fraudgpt, and wormgpt in social engineering attacks. arXiv preprint arXiv:2310.05595,
    2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falade[2023] Polra Victor Falade。解码威胁景观：Chatgpt、fraudgpt和wormgpt在社会工程攻击中的作用。arXiv预印本
    arXiv:2310.05595，2023年。
- en: 'Hong et al. [2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al.
    Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint
    arXiv:2308.00352, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong等人[2023] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang,
    Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou等。Metagpt：用于多代理协作框架的元编程。arXiv预印本
    arXiv:2308.00352，2023年。
- en: 'Hu et al. [2023] Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang,
    Zenglin Xu, and Bin Liu. Enabling intelligent interactions between an agent and
    an llm: A reinforcement learning approach. arXiv preprint arXiv:2306.03604, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人[2023] Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin
    Xu和Bin Liu。实现智能代理与LLM之间的互动：一种强化学习方法。arXiv预印本 arXiv:2306.03604，2023年。
- en: 'Iannone et al. [2022] Emanuele Iannone, Roberta Guadagni, Filomena Ferrucci,
    Andrea De Lucia, and Fabio Palomba. The secret life of software vulnerabilities:
    A large-scale empirical study. IEEE Transactions on Software Engineering, 49(1):44–63,
    2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iannone等人[2022] Emanuele Iannone, Roberta Guadagni, Filomena Ferrucci, Andrea
    De Lucia和Fabio Palomba。软件漏洞的秘密生活：大规模实证研究。IEEE软件工程学报，49(1):44–63，2022年。
- en: Ji et al. [2023] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan
    Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination
    in natural language generation. ACM Computing Surveys, 55(12):1–38, 2023.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji等人[2023] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu,
    Etsuko Ishii, Ye Jin Bang, Andrea Madotto和Pascale Fung。自然语言生成中的幻觉调查。ACM计算机调查，55(12):1–38，2023年。
- en: Kahneman [2011] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahneman[2011] Daniel Kahneman。思考，快与慢。macmillan，2011年。
- en: Kwon et al. [2023] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa
    Sadigh. Reward design with language models. In The Eleventh International Conference
    on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,
    2023.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon等人[2023] Minae Kwon, Sang Michael Xie, Kalesha Bullard和Dorsa Sadigh。语言模型的奖励设计。在第十一届国际学习表示会议，ICLR
    2023，卢旺达基加利，2023年5月1-5日。OpenReview.net，2023年。
- en: 'Li et al. [2023] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. Camel: Communicative agents for ”mind” exploration
    of large language model society. In Thirty-seventh Conference on Neural Information
    Processing Systems, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人[2023] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin和Bernard
    Ghanem。Camel：用于大语言模型社会“思维”探索的沟通代理。在第三十七届神经信息处理系统会议上，2023年。
- en: 'Lin et al. [2023] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman,
    Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang
    Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive
    tasks. In Thirty-seventh Conference on Neural Information Processing Systems,
    2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等人[2023] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang,
    Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi和Xiang Ren。Swiftsage：一种用于复杂互动任务的快速和慢速思考的生成代理。在第三十七届神经信息处理系统会议上，2023年。
- en: 'Ma et al. [2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka:
    Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931,
    2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等人[2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert
    Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan和Anima Anandkumar。Eureka：通过编码大型语言模型的人类级奖励设计。arXiv预印本
    arXiv:2310.12931，2023年。
- en: 'McIntosh et al. [2023] Timothy McIntosh, Tong Liu, Teo Susnjak, Hooman Alavizadeh,
    Alex Ng, Raza Nowrozy, and Paul Watters. Harnessing gpt-4 for generation of cybersecurity
    grc policies: A focus on ransomware attack mitigation. Computers & Security, 134:103424,
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McIntosh et al. [2023] Timothy McIntosh, Tong Liu, Teo Susnjak, Hooman Alavizadeh,
    Alex Ng, Raza Nowrozy, 和 Paul Watters。利用gpt-4生成网络安全GRC政策：重点关注勒索软件攻击缓解。《计算机与安全》，134:103424，2023年。
- en: Micheli et al. [2023] Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers
    are sample-efficient world models. In The Eleventh International Conference on
    Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net,
    2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micheli et al. [2023] Vincent Micheli, Eloi Alonso, 和 François Fleuret。变换器是样本高效的世界模型。在第十一届国际学习表征会议，ICLR
    2023，卢旺达基加利，2023年5月1-5日。OpenReview.net，2023年。
- en: Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep
    reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, 和 Martin Riedmiller。使用深度强化学习玩Atari游戏。arXiv预印本
    arXiv:1312.5602，2013年。
- en: Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous
    methods for deep reinforcement learning. In International conference on machine
    learning, pages 1928–1937\. PMLR, 2016.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih et al. [2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
    Graves, Timothy Lillicrap, Tim Harley, David Silver, 和 Koray Kavukcuoglu。深度强化学习的异步方法。在国际机器学习大会，第1928–1937页。PMLR，2016年。
- en: Noever [2023] David Noever. Can large language models find and fix vulnerable
    software? arXiv preprint arXiv:2308.10345, 2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Noever [2023] David Noever。大型语言模型能否发现并修复易受攻击的软件？arXiv预印本 arXiv:2308.10345，2023年。
- en: 'Palmer et al. [2023] Gregory Palmer, Chris Parry, Daniel J. B. Harrold, and
    Chris Willis. Deep reinforcement learning for autonomous cyber operations: A survey,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Palmer et al. [2023] Gregory Palmer, Chris Parry, Daniel J. B. Harrold, 和 Chris
    Willis。用于自主网络操作的深度强化学习：一项调查，2023年。
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith
    Ringel Morris, Percy Liang, 和 Michael S Bernstein。生成代理：人类行为的互动模拟。在第36届年度ACM用户界面软件与技术研讨会论文集，第1–22页，2023年。
- en: Pearce et al. [2023] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri,
    and Brendan Dolan-Gavitt. Examining zero-shot vulnerability repair with large
    language models. In 2023 IEEE Symposium on Security and Privacy (SP), pages 2339–2356\.
    IEEE, 2023.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce et al. [2023] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri,
    和 Brendan Dolan-Gavitt。研究大语言模型的零样本漏洞修复。在2023 IEEE安全与隐私研讨会（SP），第2339–2356页。IEEE，2023年。
- en: 'Qi et al. [2023] Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong
    Yang, and Depei Qian. Loggpt: Exploring chatgpt for log-based anomaly detection.
    arXiv preprint arXiv:2309.01189, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. [2023] Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong
    Yang, 和 Depei Qian。Loggpt：探索chatgpt用于基于日志的异常检测。arXiv预印本 arXiv:2309.01189，2023年。
- en: Qian et al. [2023] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development.
    arXiv preprint arXiv:2307.07924, 2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. [2023] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, 和 Maosong Sun。用于软件开发的交流代理。arXiv预印本 arXiv:2307.07924，2023年。
- en: 'Sandoval et al. [2023] Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri,
    Siddharth Garg, and Brendan Dolan-Gavitt. Lost at c: A user study on the security
    implications of large language model code assistants. arXiv preprint arXiv:2208.09727,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sandoval et al. [2023] Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri,
    Siddharth Garg, 和 Brendan Dolan-Gavitt。迷失在C：大型语言模型代码助手的安全性影响用户研究。arXiv预印本 arXiv:2208.09727，2023年。
- en: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint
    arXiv:1707.06347, 2017.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, 和 Oleg Klimov。近端策略优化算法。arXiv预印本 arXiv:1707.06347，2017年。
- en: 'Standen et al. [2021] Maxwell Standen, Martin Lucas, David Bowman, Toby J Richer,
    Junae Kim, and Damian Marriott. Cyborg: A gym for the development of autonomous
    cyber agents. arXiv preprint arXiv:2108.09118, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Standen et al. [2021] Maxwell Standen, Martin Lucas, David Bowman, Toby J Richer,
    Junae Kim, 和 Damian Marriott。Cyborg：用于自主网络代理开发的健身房。arXiv预印本 arXiv:2108.09118，2021年。
- en: Taddeo et al. [2019] Mariarosaria Taddeo, Tom McCutcheon, and Luciano Floridi.
    Trusting artificial intelligence in cybersecurity is a double-edged sword. Nature
    Machine Intelligence, 1(12):557–560, 2019.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taddeo et al. [2019] Mariarosaria Taddeo, Tom McCutcheon, 和 Luciano Floridi.
    在网络安全中信任人工智能是一把双刃剑。*Nature Machine Intelligence*, 1(12):557–560, 2019年。
- en: 'Talebirad and Nadiri [2023] Yashar Talebirad and Amirhossein Nadiri. Multi-agent
    collaboration: Harnessing the power of intelligent llm agents. arXiv preprint
    arXiv:2306.03314, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad and Nadiri [2023] Yashar Talebirad 和 Amirhossein Nadiri. 多代理协作：利用智能LLM代理的力量。arXiv
    预印本 arXiv:2306.03314, 2023年。
- en: Team. [2021] Microsoft Defender Research Team. Cyberbattlesim. [https://github.com/microsoft/cyberbattlesim](https://github.com/microsoft/cyberbattlesim),
    2021. Created by Christian Seifert, Michael Betser, William Blum, James Bono,
    Kate Farris, Emily Goren, Justin Grana, Kristian Holsheimer, Brandon Marken, Joshua
    Neil, Nicole Nichols, Jugal Parikh, Haoran Wei.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team. [2021] Microsoft Defender Research Team. Cyberbattlesim. [https://github.com/microsoft/cyberbattlesim](https://github.com/microsoft/cyberbattlesim)，2021年。由
    Christian Seifert, Michael Betser, William Blum, James Bono, Kate Farris, Emily
    Goren, Justin Grana, Kristian Holsheimer, Brandon Marken, Joshua Neil, Nicole
    Nichols, Jugal Parikh, Haoran Wei 创建。
- en: 'Vyas et al. [2023] Sanyam Vyas, John Hannay, Andrew Bolton, and Professor Pete
    Burnap. Automated cyber defence: A review, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vyas et al. [2023] Sanyam Vyas, John Hannay, Andrew Bolton, 和 Professor Pete
    Burnap. 自动化网络防御：综述，2023年。
- en: 'Wan et al. [2022] Yue Wan, Chang-Yu Hsieh, Ben Liao, and Shengyu Zhang. Retroformer:
    Pushing the limits of end-to-end retrosynthesis transformer. In International
    Conference on Machine Learning, pages 22475–22490\. PMLR, 2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wan et al. [2022] Yue Wan, Chang-Yu Hsieh, Ben Liao, 和 Shengyu Zhang. Retroformer:
    推动端到端回溯合成变换器的极限。在国际机器学习会议上，页面22475–22490。PMLR，2022年。'
- en: Wang et al. [2023a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等等. 基于大语言模型的自主代理调查。arXiv
    预印本 arXiv:2308.11432, 2023年。
- en: 'Wang et al. [2023b] Zhilin Wang, Yu Ying Chiu, and Yu Cheung Chiu. Humanoid
    agents: Platform for simulating human-like generative agents. arXiv preprint arXiv:2310.05418,
    2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023b] Zhilin Wang, Yu Ying Chiu, 和 Yu Cheung Chiu. 类人代理：模拟人类生成代理的平台。arXiv
    预印本 arXiv:2310.05418, 2023年。
- en: 'Wang et al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al.
    Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language
    models. arXiv preprint arXiv:2311.05997, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2023c] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, 等等. Jarvis-1:
    具备记忆增强的多模态语言模型的开放世界多任务代理。arXiv 预印本 arXiv:2311.05997, 2023年。'
- en: Wason and Evans [1974] Peter C Wason and J St BT Evans. Dual processes in reasoning?
    Cognition, 3(2):141–154, 1974.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wason and Evans [1974] Peter C Wason 和 J St BT Evans. 推理中的双重过程？*Cognition*,
    3(2):141–154, 1974年。
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen llm applications via multi-agent conversation framework. arXiv preprint
    arXiv:2308.08155, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. Autogen: 通过多代理对话框架实现下一代LLM应用。arXiv
    预印本 arXiv:2308.08155, 2023年。'
- en: Xia et al. [2023] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael
    Pradel, and Lingming Zhang. Universal fuzzing via large language models. arXiv
    preprint arXiv:2308.04748, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. [2023] Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael
    Pradel, 和 Lingming Zhang. 通过大语言模型进行通用模糊测试。arXiv 预印本 arXiv:2308.04748, 2023年。
- en: 'Xu et al. [2023] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. Exploring large language models for communication games:
    An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, 和 Yang Liu. 探索大语言模型在通信游戏中的应用：对狼人杀的实证研究。arXiv 预印本 arXiv:2309.04658,
    2023年。
- en: 'Yaman [2023] Ferhat Yaman. Agent SCA: Advanced Physical Side Channel Analysis
    Agent with LLMs. PhD thesis, North Carolina State University, 2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yaman [2023] Ferhat Yaman. Agent SCA: 高级物理侧信道分析代理与大语言模型。博士论文，北卡罗来纳州立大学，2023年。'
- en: 'Yao et al. [2023] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun,
    and Yue Zhang. A survey on large language model (llm) security and privacy: The
    good, the bad, and the ugly. arXiv preprint arXiv:2312.02003, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun,
    和 Yue Zhang. 大语言模型 (LLM) 安全与隐私综述：好、坏与丑。arXiv 预印本 arXiv:2312.02003, 2023年。
- en: 'Zhang et al. [2023] Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration
    mechanisms for llm agents: A social psychology view. arXiv preprint arXiv:2310.02124,
    2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023] Jintian Zhang, Xin Xu 和 Shumin Deng。探讨大语言模型代理的协作机制：一种社会心理学视角。arXiv
    预印本 arXiv:2310.02124, 2023。
