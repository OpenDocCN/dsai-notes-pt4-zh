<!--yml
category: 未分类
date: 2025-01-11 11:56:04
-->

# Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics

> 来源：[https://arxiv.org/html/2411.08881/](https://arxiv.org/html/2411.08881/)

¹¹institutetext: Tampere University (TAU), Finland ²²institutetext: University of Jyväskylä (JYU), Finland ³³institutetext: University of Vaasa (UWASA), FinlandJosé Antonio Siqueira de Cerqueira 11 [0000-0002-8143-1042](https://orcid.org/0000-0002-8143-1042 "ORCID identifier")    Mamia Agbese 22 [0000-0002-5479-7153](https://orcid.org/0000-0002-5479-7153 "ORCID identifier")    Rebekah Rousi 33 [0000-0001-5771-3528](https://orcid.org/0000-0001-5771-3528 "ORCID identifier")    Nannan Xi 11 [0000-0002-9424-8116](https://orcid.org/0000-0002-9424-8116 "ORCID identifier")    Juho Hamari 11 [0000-0002-6573-588X](https://orcid.org/0000-0002-6573-588X "ORCID identifier")    Pekka Abrahamsson 11 [0000-0002-4360-2226](https://orcid.org/0000-0002-4360-2226 "ORCID identifier")

###### Abstract

AI-based systems, including Large Language Models (LLMs), impact millions by supporting diverse tasks but face issues like misinformation, bias, and misuse. Ethical AI development is crucial as new technologies and concerns emerge, but objective, practical ethical guidance remains debated. This study examines LLMs in developing ethical AI systems, assessing how trustworthiness-enhancing techniques affect ethical AI output generation. Using the Design Science Research (DSR) method, we identify techniques for LLM trustworthiness: multi-agents, distinct roles, structured communication, and multiple rounds of debate. We design the multi-agent prototype LLM-BMAS, where agents engage in structured discussions on real-world ethical AI issues from the AI Incident Database. The prototype’s performance is evaluated through thematic analysis, hierarchical clustering, ablation studies, and source code execution. Our system generates around 2,000 lines per run, compared to only 80 lines in the ablation study. Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing LLM-BMAS’s ability to generate thorough source code and documentation addressing often-overlooked ethical AI issues. However, practical challenges in source code integration and dependency management may limit smooth system adoption by practitioners. This study aims to shed light on enhancing trustworthiness in LLMs to support practitioners in developing ethical AI-based systems.

###### Keywords:

AI ethics Large Language Models Trustworthiness AI4SE LLM4SE.

## 1 Introduction

Artificial Intelligence (AI) is emerging as a transformative force, reshaping industries, economies and everyday life. Despite its rapid development and adoption, a number of negative reports on its use highlight the importance of adhering to ethical norms and principles [[39](https://arxiv.org/html/2411.08881v1#bib.bib39)]. Several ethical guidelines are available with plenty ethical principles providing high level and abstract guidance for developers and stakeholders on AI ethics [[3](https://arxiv.org/html/2411.08881v1#bib.bib3)]. These are important, but there is a lack of practical guidance for developers to operationalise ethics in AI [[3](https://arxiv.org/html/2411.08881v1#bib.bib3)]. As new AI-based technologies emerge, ethics in AI will also become increasingly critical, such as the latest breakthrough: Large Language Models (LLMs).

LLMs are becoming ubiquitous and have significant impact on decision-making processes and human interactions [[20](https://arxiv.org/html/2411.08881v1#bib.bib20), [33](https://arxiv.org/html/2411.08881v1#bib.bib33)]. LLMs are advanced AI algorithms capable of generating, interpreting, and predicting text based on vast amounts of data they have been trained on [[4](https://arxiv.org/html/2411.08881v1#bib.bib4)]. Of these LLMs, Generative Pre-trained Transformer (GPT) LLMs, such as ChatGPT, have showcased unprecedented proficiency in the human language, coding, logic, reasoning, and other associated natural language tasks [[33](https://arxiv.org/html/2411.08881v1#bib.bib33)]. They excel at guiding complex conversations and are used in countless endeavours such as software engineering (SE) - as in AI for Software Engineering (AI4SE) [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)] - and qualitative research [[26](https://arxiv.org/html/2411.08881v1#bib.bib26)].

Within the AI4SE field, the capabilities of LLMs are being explored in the software development, maintenance, and evolution [[28](https://arxiv.org/html/2411.08881v1#bib.bib28), [30](https://arxiv.org/html/2411.08881v1#bib.bib30), [26](https://arxiv.org/html/2411.08881v1#bib.bib26), [22](https://arxiv.org/html/2411.08881v1#bib.bib22)], namely LLM4SE. Accordingly, they find applications across various stages of the software development life cycle, including requirement analysis, software design, code implementation, testing, refactoring, defect detection, and maintenance [[30](https://arxiv.org/html/2411.08881v1#bib.bib30)]. Albeit the notable use of LLM in SE, to the best of our knowledge there are no studies that focus on the use of LLM in the ethical AI development. In qualitative research, LLMs are also gaining prominence, particularly as a supplement to tasks traditionally performed by humans, like analysing qualitative data [[12](https://arxiv.org/html/2411.08881v1#bib.bib12), [32](https://arxiv.org/html/2411.08881v1#bib.bib32), [24](https://arxiv.org/html/2411.08881v1#bib.bib24)]. Specifically, in the coding process - where qualitative data is organised and interpreted by assigning codes or labels to text segments or different data forms - LLMs have demonstrated significant utility [[36](https://arxiv.org/html/2411.08881v1#bib.bib36), [32](https://arxiv.org/html/2411.08881v1#bib.bib32)].

However, several concerns arise, especially related with trustworthiness, as more practitioners are relying on LLMs to perform their task [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)]. Several studies are drawing attention to possible problems in adopting LLMs for SE, in particular when syntactically correct but non-functional code is produced, which affects the reliability and effectiveness of LLM-based code generation [[16](https://arxiv.org/html/2411.08881v1#bib.bib16)]. Pearce et al. [[29](https://arxiv.org/html/2411.08881v1#bib.bib29)] used GitHub Copilot to produce 1,689 programs, and found that 40% of them have security vulnerabilities. Liu et al. [[21](https://arxiv.org/html/2411.08881v1#bib.bib21)] systematically analysed ChatGPT code generation reliability identifying quality issues as many of the programs generated provided wrong output or contained compilation or runtime error.

Effective AI4SE should be trustworthy and synergistic with the practitioner’s workflow, otherwise “such AI4SE solutions risk becoming obstacles rather than facilitators" [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)]. Currently, there is a growing need for context-dependent empirical studies that explore how trust in LLMs affects their adoption in software engineering tasks [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)]. Furthermore, there remains a significant gap in research focused on the practical operationalisation of AI ethics, particularly through the application of LLMs. To date, no studies have attempted to explore the development of ethical AI-based systems using LLMs.

In this paper, we aim to explore trustworthiness in LLMs in the ethical AI development through an experiment. Our Research Question is: To what extent does the trustworthiness of LLM-based systems contribute to achieving ethically aligned AI-based systems? To this end, an empirical study was carried out, following the Design Science Research (DSR) method [[13](https://arxiv.org/html/2411.08881v1#bib.bib13)]. We identify techniques that can improve trustworthiness in LLMs, apply them to develop a prototype, use it in the context of the development of ethically aligned AI-based systems, and evaluate the output.

The evaluation is conducted through four different approaches. First, a thematic analysis is performed by an LLM, which identifies and categorizes key themes within the data. Second, we apply hierarchical clustering dendogram to group related text to compare and support the thematic analysis. Third, we perform an ablation study. Fourth, we run the source code produced to assess its functionality and correctness.

The contributions of this paper to the current state of knowledge on LLM4SE and ethics in AI are threefold. First, we showcase the effectiveness of various techniques in enhancing trustworthiness in LLM4SE. Second, we provide empirical insights into how these techniques can be leverage in developing ethically-aligned AI-based systems. Finally, by applying a multi-faceted evaluation approach - including thematic analysis, hierarchical clustering, and direct source code execution - we provide an initial framework for assessing the reliability and trustworthiness of LLMs in software engineering. With this experiment, we hope to shed light both on how to improve trustworthiness in LLM and on how to help practitioners develop ethical AI-based systems.

## 2 Background and Related Work

### 2.1 Large Language Models

LLMs have been pre-trained on vast amounts of text data, often scraped from the internet, allowing them to learn patterns and structures inherent in human language [[14](https://arxiv.org/html/2411.08881v1#bib.bib14)]. One of the key features of LLMs is their ability to generate contextually relevant and apparently coherent text across a wide range of topics [[33](https://arxiv.org/html/2411.08881v1#bib.bib33)]. These models can perform tasks such as language translation, text summarization, question answering, and creative writing [[14](https://arxiv.org/html/2411.08881v1#bib.bib14)]. They achieve this by leveraging the vast amounts of data they have been trained on to predict the next word or sequence of words in each context [[10](https://arxiv.org/html/2411.08881v1#bib.bib10)]. Some well-known examples of LLMs include GPT (Generative Pre-trained Transformer) models developed by OpenAI [[27](https://arxiv.org/html/2411.08881v1#bib.bib27)], particularly GPT-3.5 and GPT-4.

### 2.2 Trustworthiness in Large Language Models

Recent advancements and use of LLMs have raised concerns about their ethical implications [[40](https://arxiv.org/html/2411.08881v1#bib.bib40), [19](https://arxiv.org/html/2411.08881v1#bib.bib19), [35](https://arxiv.org/html/2411.08881v1#bib.bib35), [20](https://arxiv.org/html/2411.08881v1#bib.bib20), [37](https://arxiv.org/html/2411.08881v1#bib.bib37)]. Issues like information hallucination, biases, and the need for factual correctness are critical in determining the trustworthiness of these systems in real-world applications [[18](https://arxiv.org/html/2411.08881v1#bib.bib18), [9](https://arxiv.org/html/2411.08881v1#bib.bib9)]. Trustworthiness refers to the quality or attribute of being reliable, dependable, and deserving of trust. It encompasses several key components that establish trust in a person, organization, product, or system [[37](https://arxiv.org/html/2411.08881v1#bib.bib37)]. In the context of LLMs, trustworthiness largely refers to their reliability and ethical adherence to social norms [[20](https://arxiv.org/html/2411.08881v1#bib.bib20), [37](https://arxiv.org/html/2411.08881v1#bib.bib37)]. Notably, several studies have formulated taxonomies accompanied by evaluation methodologies, wherein the taxonomy acts as a measure for assessing trustworthiness, with the identified aspects serving as evaluation parameters. Among these efforts, the Holistic Evaluation of Language Models (HELM) devised by Liang et al. [[19](https://arxiv.org/html/2411.08881v1#bib.bib19)] introduces a comprehensive taxonomy consisting of seven metrics — accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency — and applied this framework to analyze 30 different models. Notably, the authors stress the significance of adopting a multi-metric approach, enabling metrics prioritization beyond mere accuracy.

In a conspicuous study, partially funded by distinguished agencies such as NASA, Ebay, Amazon and Google, which also won the Outstanding Paper Award at NeurIPS’23, Wang et al. [[40](https://arxiv.org/html/2411.08881v1#bib.bib40)] provide eight trustworthiness perspectives of language models: toxicity, stereotype and bias, adversarial robustness, out-of-distribution robustness, privacy, robustness to adversarial demonstrations, machine ethics, lastly, fairness. The authors state that they only provided objective definitions for some of the perspectives coined - due to their intrinsic subjectivity, e.g., fairness toxicity - leaving the subjective exploration of model behaviors based on human understanding as future work. As scholars delve into characterizing trustworthiness within LLMs, a unified terminology for measuring it remains unclear. Furthermore, it is seen that there is no clear consensus on a framework to build aligned LLMs, nor clear assessment guidelines, both posing as unresolved issues [[20](https://arxiv.org/html/2411.08881v1#bib.bib20)].

### 2.3 Techniques to improve trustworthiness in LLM4SE

Hong et al. [[15](https://arxiv.org/html/2411.08881v1#bib.bib15)] propose MetaGPT to simulate a group of agents as software company. Worth noting the use of specialized roles (Product Manager, Architect, Project Manager, Engineer, and QA Engineer), workflow for the agents (all agents work in a sequential fashion), and a structure communication interface. An interesting point regarding the specialized roles, is the ability for agents to have skills, i.e., perform actions, such as being able to run code and search on the web. The authors argue that the incorporation of aforementioned techniques can considerably enhance code generation, as well as mitigating hallucinations risks.

Qian et al. [[31](https://arxiv.org/html/2411.08881v1#bib.bib31)] developed ChatDev, a chat-based software development framework using LLMs to enhance communication and collaboration across various roles, aimed at reducing hallucination risks like bugs and missing dependencies. Though, it does generate different outputs for the same inputs and carries risks due to untested code.

Meanwhile, Wu et al. [[41](https://arxiv.org/html/2411.08881v1#bib.bib41)] from Microsoft introduced AutoGen, a multi-agent LLM framework enhancing coding productivity by structuring tasks among specialized roles (Commander, Writer, Safeguard) that interact to refine outputs. Through an ablation study they demonstrated that this method significantly outperforms single-agent systems by breaking down complex tasks into manageable components.

The aforementioned studies run in line with a joint study by MIT and Microsoft, Du et al. [[9](https://arxiv.org/html/2411.08881v1#bib.bib9)]. Albeit not using LLM-based multi-agents to directly generate code, the authors argue that using multiple agents along with multiple rounds of debate improve the reasoning and factuality of the generated solution compared to not using them. Hence, multiple agents and rounds of debate can reduce hallucinations and increase the overall trustworthiness of using LLMs. Such techniques can exhibit emergent behaviours that are not evident when considering individual agents in isolation [[15](https://arxiv.org/html/2411.08881v1#bib.bib15), [38](https://arxiv.org/html/2411.08881v1#bib.bib38)].

### 2.4 AI Ethics

Despite the recent academic and industry interest, AI ethics is a long-standing area of research, in which diverse incidents have recently raised public concerns about its use and development [[11](https://arxiv.org/html/2411.08881v1#bib.bib11)]. Consequently, in the past few years a variety of principles and guidelines have emerged, from various sources (academia, industry, civil society) to frame and define what is ethical AI [[17](https://arxiv.org/html/2411.08881v1#bib.bib17)]. Ryan and Stahl [[34](https://arxiv.org/html/2411.08881v1#bib.bib34)] provide 11 ethical principles along with ethical issues related to AI: 1) Transparency, 2) Justice and Fairness, 3) Non-maleficence, 4) Responsibility, 5) Privacy, 6) Beneficence, 7) Freedom and Autonomy, 8) Trust, 9) Sustainability, 10) Dignity, 11) Solidarity. These ethical principles serve as the basis of our analysis.

The European Parliament is currently putting into force the world’s first regulation on AI [[5](https://arxiv.org/html/2411.08881v1#bib.bib5)], which highlights the fact that the plethora of guidelines available are in fact soft law, with no legally binding nor real consequences [[3](https://arxiv.org/html/2411.08881v1#bib.bib3)]. Nevertheless, it is reminiscent of the abstract principles on how AI ethics is mainly approached [[11](https://arxiv.org/html/2411.08881v1#bib.bib11), [17](https://arxiv.org/html/2411.08881v1#bib.bib17), [6](https://arxiv.org/html/2411.08881v1#bib.bib6)]. Thus, the obstacles in operationalising ethical principles in AI-based systems stem from the subjectivity necessary to interpret and put into practice highly abstract principles by the practitioners [[3](https://arxiv.org/html/2411.08881v1#bib.bib3)]. Despite its importance, it has often been a task commonly taken as an afterthought [[39](https://arxiv.org/html/2411.08881v1#bib.bib39)]. As opposed to other efforts available in the literature, we aim to study the use of LLM-based multi-agent systems in the development process of AI-based systems, taking into consideration the operationalization of ethical principles from the first stage of the development process onwards. The use of LLM agents in developing ethically-aligned AI systems introduces significant complexity and novelty, making their ethical implications a compelling area of study. LLMs face unique challenges in producing accurate, unbiased text and source code while adhering to ethical guidelines. Their dynamic and often unpredictable outputs in multi-agent interactions provide a valuable opportunity to study the application of ethical principles in real-world settings, although there is a noted lack of literature on the operationalization of AI ethics through LLM agents.

### 2.5 LLMs for Qualitative Analysis

Qualitative analysis is another opportune expanding area where LLMs can be applied [[7](https://arxiv.org/html/2411.08881v1#bib.bib7), [8](https://arxiv.org/html/2411.08881v1#bib.bib8), [24](https://arxiv.org/html/2411.08881v1#bib.bib24)], particularly in qualitative research within the field of software engineering [[1](https://arxiv.org/html/2411.08881v1#bib.bib1), [32](https://arxiv.org/html/2411.08881v1#bib.bib32)]. Thematic analysis is labour-intensive and time-consuming for humans, and are prone to mistakes and bias [[2](https://arxiv.org/html/2411.08881v1#bib.bib2)]. Therefore, LLMs present an opportunity to improve the accuracy and efficiency of this method. In thematic analysis, coders require multiple rounds of discussion to achieve consensus and resolve ambiguities for an in-depth understanding of the data [[2](https://arxiv.org/html/2411.08881v1#bib.bib2)].

Thus, LLMs can tackle the challenges of traditional qualitative research, such as the time-intensive nature of data analysis, limited generalizability, consistency issues, and personal subjective biases [[1](https://arxiv.org/html/2411.08881v1#bib.bib1), [12](https://arxiv.org/html/2411.08881v1#bib.bib12)]. Hamilton et al. [[12](https://arxiv.org/html/2411.08881v1#bib.bib12)] argue that both humans and LLMs have identified unique and overlapping themes from interview data, indicating the potential for recognizing patterns and themes in qualitative data.

## 3 Methodology

In this study, we employed an adapted DSR [[13](https://arxiv.org/html/2411.08881v1#bib.bib13)] approach to explore and enhance trustworthiness in LLMs for software engineering tasks in four phases: 1) exploration, 2) prototyping, 3) evaluation, and 4) communication of results. The fourth phase will be approached in Section [4](https://arxiv.org/html/2411.08881v1#S4 "4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics").

### 3.1 Exploring

Initially, we explored existing literature and found the need and motivation to improve trustworthiness in AI4SE, presented in Section [2.2](https://arxiv.org/html/2411.08881v1#S2.SS2 "2.2 Trustworthiness in Large Language Models ‣ 2 Background and Related Work ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics"), and most notably in the work of David Lo [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)]. Furthermore, we identified techniques that have been recently proposed in the literature to improve trustworthiness in AI systems, particularly in the context of software engineering, i.e., AI4SE. In sum, the identified techniques described in Section [2.3](https://arxiv.org/html/2411.08881v1#S2.SS3 "2.3 Techniques to improve trustworthiness in LLM4SE ‣ 2 Background and Related Work ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics") provide an LLM-based multi-agent systems approach, characterised by: a) a combination of agents, b) distinct roles (specialized agents), c) structured conversation, d) multiple rounds of debate.

These techniques were then synthesised and integrated into the development of a prototype system designed to assist in software engineering tasks, in particular in the creation of ethically-aligned AI-based systems.

### 3.2 Prototype: LLM-BMAS

From our findings, we devised a prototype that is an LLM-Based multi-agent system (henceforth, called LLM-BMAS). The LLM-BMAS is a Python script that consists of three agents making use of GPT-4o-mini model through OpenAI API. They have different specialized roles and their conversation is structured in the same way. Specifically, there are two senior Python developers and one AI ethicist. As a team, they receive an input - a project description (PD)- that they should work on collaboratively to implement, through five rounds of discussion.

The conversational pattern assigned to them is as follows: Agent 1 should start the conversation; Agent 2 receives this as input to work on; similarly Agent 3 receives both conversations appended to work on. This defines a round of conversation. In other words, the conversation history is always appended, serving as input to the next agent until the number of rounds is reached. Figure [1](https://arxiv.org/html/2411.08881v1#S3.F1 "Figure 1 ‣ 3.2 Prototype: LLM-BMAS ‣ 3 Methodology ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics") presents an overview of our prototype.

![Refer to caption](img/40a20080c3741305224a91170776a11a.png)

Figure 1: Prototype overview

The agents engage in a structured conversation, spanning five rounds, to discuss and collaborate on the project. However, due to the nature of LLMs, each new run produces a different software for the same input [[31](https://arxiv.org/html/2411.08881v1#bib.bib31), [22](https://arxiv.org/html/2411.08881v1#bib.bib22)], i.e., using the same system configuration for data generation generates a different debate and source code for each new invoke, in addition to a different thematic analysis. Thus, in order to enrich our analysis, we ran the system three different times to obtain distinct output.

### 3.3 Evaluation

To evaluate the effectiveness of the prototype, we applied a multi-faceted evaluation strategy. First, we conducted a thematic analysis using an LLM to automatically identify and categorize key themes within the generated output. This provided insights into the model’s alignment with ethical and functional requirements. Second, we employed hierarchical clustering to organize and group-related text segments, supporting and validating the findings of the thematic analysis by uncovering underlying patterns and relationships in the data. Then, both thematic analysis results and hierarchical clustering for each project description is compared and analysed. Finally, an ablation study is conducted following similar steps. Figure [2](https://arxiv.org/html/2411.08881v1#S3.F2 "Figure 2 ‣ 3.3 Evaluation ‣ 3 Methodology ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics") presents a visualisation for the evaluation methodology devised.

An ablation study is useful to validate research results and consists in assessing performance of an AI system by removing components [[23](https://arxiv.org/html/2411.08881v1#bib.bib23), [25](https://arxiv.org/html/2411.08881v1#bib.bib25)]. In this case, we will remove the LLM-BMAS prototype and use as a baseline only the interaction with OpenAI ChatGPT user interface.

![Refer to caption](img/a45e42523aac0db2e65c02e00a0f563f.png)

Figure 2: Evaluation overview

As discussed, each run of the prototype produces a different result for the same input, thus, we run the system three different times to obtain distinct output (yielding the raw output, e.g., $O1_{1}$, $O1_{2}$, $O1_{3}$ for the first project description).

Then, the collected data is analysed through the use of ChatGPT GPT-4o with a specific custom instruction acting as qualitative analyst. Each of the raw output produces a partial thematic analysis (e.g., $B2_{1}$, $B2_{2}$, $B2_{3}$ for the second project description). Consequently, the partial thematic analysis for each project description is summarised into one thematic analysis through the use of the same GPT-4o, yielding $TA$, $TB$ and $TC$.

In order to provide a hierarchical clustering, [iRaMuTeQ](http://www.iramuteq.org/) software was used, which runs on top of R. The input for the generation of each hierarchical clustering is the append of the raw output of each project description (e.g., $O3_{1}$, $O3_{2}$, $O3_{3}$ together generates the hierarchical clustering for the third project description.

With the aim of understanding the benefits of this LLM-BMAS approach, an ablation study is also performed, in which a project description is prompted to a regular ChatGPT-4o using OpenAI user interface, without any of the techniques found. For example, for project description 1, we have only one output ($AS_{1}$), that servers as input for creation of its thematic analysis also with ChatGPT-4o, and for the iRaMuTeQ analysis, yielding $TA_{A}S$ and $HCD_{A}S$ respectively.

Finally, we performed a practical evaluation of the prototype by running the source codes generated by the LLM. This step is for evaluating the system’s practical utility in real-world software development scenarios. Our methodology offers a initial framework for assessing the trustworthiness and reliability of LLMs in the software engineering domain.

The results and discussions arising from the comparison between the different thematic analysis and hierarchical clustering, coupled with the ablation study and source code execution is presented next.

## 4 Results and Discussion

In this Section we present for each project description provided its corresponding results and discussion, finally, an overview discussion is presented. All project descriptions are derived from real AI incidents available at [https://incidentdatabase.ai/](https://incidentdatabase.ai/). All data used in this experiment, including agent prompts, custom instructions, thematic analysis performed, are available at [here](https://zenodo.org/records/13980076).

### 4.1 Project Description 1

The first project description derived from [this incident](https://incidentdatabase.ai/cite/37#r620):

<svg class="ltx_picture" height="106.28" id="S4.SS1.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Develop an AI-powered recruitment tool designed to screen resumes impartially, complying with the EU AI Act. The project aims to eliminate biases related to gender and language, ensuring fair evaluation of all applicants. The AI Ethics Specialist will guide the team in addressing ethical concerns and risk levels. The senior Python developers will utilize NLP to process resumes, referencing relevant EU AI Act guidelines.</foreignobject></g></g></svg>

It is worth highlighting that in the project description there is a need for referencing the EU AI Act, however, by using GPT-4o-mini, that was trained up until October 2023, this Act was not yet available, only being published on June 2024\. In future work, we will introduce this document as a vector to be embedded and accessed by an open-source LLM. Moreover, according to David Lo [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)], guaranteeing that AI4SE solution will comply with legislation is one way to enhance trustworthiness in AI4SE, since practitioners need to avoid conflict with laws.

Raw data obtained vary from 1712 lines to 1976 lines of discussion plus source code. The thematic analysis in Table [1](https://arxiv.org/html/2411.08881v1#S4.T1 "Table 1 ‣ 4.1 Project Description 1 ‣ 4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics") was obtained from the merge of the three thematic analyses attained from each output. In Figure [3](https://arxiv.org/html/2411.08881v1#S4.F3 "Figure 3 ‣ 4.1 Project Description 1 ‣ 4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics"), 5 classes were devised, however, classes such as 1, 3, 4 and 5 are related to technical implementation. Nevertheless, it is possible to understand that themes 1 2 and 3 are closely related to the second class, where ethical, EU, act appear. Moreover with class number 3, w.r.t functions as evaluate_bias and bias_score.

In all cases, the source codes produced were spread around the text. As mentioned, this can be almost 2000 lines long. This makes the source code difficult to differentiate, and sometimes there are pieces that need to be glued together from different rounds of conversation. After trying to run the source code available in O1_1, it was found that some packages were needed to be installed manually. Furthermore, deprecated packages were being used: ’sklearn’ PyPI package is deprecated, use ’scikit-learn’. This results in extra effort by the practitioners, impacting on their workflow.

Table 1: Thematic Analysis PD 1

| Theme | Codes |
| --- | --- |
| Ethical and Regulatory AI Development | Compliance with the EU AI Act, Ethical considerations in AI development, Bias elimination strategies |
| Fair and Transparent AI Implementation | Technical development of the AI tool, Integration of bias detection mechanisms, ensuring fairness in AI operations |
| Iterative Model Training and User-Centered Validation | Scenario-based testing, Continuous model refinement, Incorporating actionable user feedback |
| Transparency in AI Processes | Documentation of AI processes, Clear communication of updates and improvements, Stakeholder engagement and transparency |

![Refer to caption](img/6e175375df7dfb8b2883068b1f1c0bef.png)

Figure 3: Hierarchical clustering dendogram - PD1

In regards to the ablation study, only 78 lines of text were obtained, while no code was produced. On the other hand, three themes were devised from the thematic analysis: Ethical AI Development, Technical Implementation and Fairness, and Compliance. It was not possible to generate the hierarchical clustering for any of the ablation studies due to small size of text data available. The quality between both studies is very disparate.

### 4.2 Project Description 2

The second project description derived from [this incident](https://incidentdatabase.ai/cite/635/#r3637):

<svg class="ltx_picture" height="106.28" id="S4.SS2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Create a deepfake content authentication system that uses AI to verify the authenticity of video, image and audio content. This system will distinguish between real and manipulated media, reducing the spread of harmful deepfake content. The AI Ethics Specialist will ensure the tool adheres to ethical standards. The senior Python developers will enhance the system’s detection capabilities.</foreignobject></g></g></svg>

In this PD, there is no reference to EU AI Act directly. However, it is worth pinpointing that it is available in the prompt of the third agent, the AI Ethicist. Raw data obtained vary from 1414 lines to 2178 lines of text data. The discussion here progresses in a similar way to the previous one.

As the relationship between the thematic analysis in Table [2](https://arxiv.org/html/2411.08881v1#S4.T2 "Table 2 ‣ 4.2 Project Description 2 ‣ 4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics") and the hierarchical clustering produced in Figure [4](https://arxiv.org/html/2411.08881v1#S4.F4 "Figure 4 ‣ 4.2 Project Description 2 ‣ 4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics"), it is possible to highlight the class number 1 with theme number 2, Ethical Compliance and AI Responsibility. Moreover, class number 2 is closely related to theme Security and Collaboration, with words such as authentication and logger.

Similarly to the previous PD, no source code was produced in the ablation study. This yielded 78 lines of text and themes such as Technological Framework, Ethics and Compliance, User Experience and Support, Strategic Objective. While the study with the proposed prototype provides a more fruitful outcome, yielding almost 2000 lines of text, the ablation study provides a generic text that can only serve as study material.

In relation to the source code produced, on top of the overhead to developers encountered in the previous PD, in O2_2 it is perceived that our prototype created modules, as in from your_module import VideoAuthenticator, ImageAuthenticator, AudioAuthenticator. Again, this poses a problem to practitioners when working with this tool, due to the fact that they will need to go through all the text and manually search for the source code that he/she needs to copy and paste somewhere else, test it, install dependencies, and manage modules.

Table 2: Thematic Analysis PD 2

| Theme | Codes |
| --- | --- |
| System Architecture and Design | Modularity and Scalability, AI Integration and Media Processing, Role-Based Access Control (RBAC) |
| Ethical Compliance and AI Responsibility | AI Ethics and Bias Detection, Transparency and Accountability, User Consent and GDPR Compliance |
| Security and Collaboration | User Authentication and Data Security, Role Management and Access Control, Collaboration between Developers and Ethics Specialists |
| System Reliability and Continuous Improvement | Error Handling and Logging, Feedback Mechanisms, Testing and Maintenance |

![Refer to caption](img/9a49f15cc41307f66ec60396859eccf3.png)

Figure 4: Hierarchical clustering dendogram - PD2

### 4.3 Project Description 3

The third project description derived from [this incident](https://incidentdatabase.ai/cite/16#r88):

<svg class="ltx_picture" height="106.28" id="S4.SS3.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,106.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Develop an AI-based image processing and classification system that complies with the EU AI Act, ensuring accurate and respectful labelling of images. The system must avoid racial biases and harmful misclassifications. The AI Ethics Specialist will focus on identifying and mitigating psychological harm and harm to social systems. The senior Python developers will implement and refine image classification algorithms.</foreignobject></g></g></svg>

This discussion follows in a similar fashion to the previous discussions. Raw data obtained vary from 1676 lines to 2122 lines of text data. The thematic analysis in Table [3](https://arxiv.org/html/2411.08881v1#S4.T3 "Table 3 ‣ 4.3 Project Description 3 ‣ 4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics") revolves around Ethical AI Development and Compliance and Technical Implementation, Optimization and Robustness. Class number 1 is related to theme number 1, however, the rest of the classes available contain words that are related to technical implementations, such as image_path, import, sklearn, sensitive_attribute, which are related to theme number 2\. Interesting to note that sklearn is present in a class in Figure [5](https://arxiv.org/html/2411.08881v1#S4.F5 "Figure 5 ‣ 4.3 Project Description 3 ‣ 4 Results and Discussion ‣ Can We Trust AI Agents? An Experimental Study Towards Trustworthy LLM-Based Multi-Agent Systems for AI Ethics"), even though it had already been recognized to have deprecated. This can highlight the fact that since its last update, in October 2023, and the date this study is being written, August 2024, it is hard for a AI4SE to follow the novelties of the technology while reinforcing the use of a single package for different tasks.

Table 3: Thematic Analysis PD 3

| Theme | Codes |
| --- | --- |
| Ethical AI Development and Compliance | Ethical AI, Compliance, Bias Detection and Mitigation, Fairness Evaluation, Transparency and Accountability |
| Technical Implementation, Optimization, and Robustness | Data Management, Model Design and Implementation, Long-term Monitoring and Continuous Improvement, Error Handling and Modular Design |
| Stakeholder Engagement and Continuous Improvement | Stakeholder Engagement, User Feedback Integration, Collaboration, Transparency in Operations |
| Data-Driven Evaluation | Statistical Analysis, Confidence Scores, Class-wise Analysis |

![Refer to caption](img/2de71ad5cc9825b91c77a3eb442e2289.png)

Figure 5: Hierarchical clustering dendogram - PD3

## 5 Threats to Validity

Different limitations appear from this study, including reliance on proprietary LLMs, which may change unpredictably. Reproducibility is a concern due to the non-deterministic outputs of LLMs. Since our methodology relies mainly on LLM to assess the outputs of the prototype, there is a potential risk of circular reasoning, where an AI is validating another AI’s trustworthiness. This could introduce bias, as both systems may share underlying flaws or limitations that go undetected. Moreover, thematic analysis and hierarchical clustering, though useful in organizing and analyzing text data, are both automated processes here, lacking human intervention. This raises concerns about the depth and interpretability of the identified themes. Without human input from ethicists or SE practitioners, there is a risk that the analysis may overlook nuanced or context-specific issues that only humans would detect, particularly when it comes to ethical concerns and practical challenges in SE workflows. To mitigate this, future work should incorporate human-in-the-loop methodologies where human stakeholders can review and validate the results produced by AI systems, offering insights and corrections that purely AI-driven approaches may miss. Furthermore, the evaluation of the source code was conducted at a surface level, with emphasis on ensuring that the sourc ecode executes without errors. However, no in-depth analysis was performed to assess whether the output aligns with AI ethics principles. Future work should incorporate a more rigorous assessment of the ethical implications of the generated source code, ensuring that it is not only functional but also ethically sound.

## 6 Final Remarks

This study aimed to explore trustworthiness in AI4SE, specifically in the use of LLM for software engineering (LLM4SE), in the context of ethically aligned AI-based systems development. To the best of our knowledge, there are no studies in the literature that address practical AI ethics using LLM. Using the Design Science Research method, we identified the motivation and different techniques to improve trustworthiness in AI4SE, proposed a prototype - an LLM-based multi-agent system (LLM-BMAS) -, and evaluated it using thematic analysis, hierarchical clustering, ablation study, and source code execution. The techniques discovered to improve trustworthiness are a combination of agents, different roles (specialised agents), structured conversations and multiple rounds of debate. The prototype was tested with three distinct project descriptions derived from real AI incidents extracted from the AI Incident Database. For each project description, LLM-BMAS produced about 2000 lines of text, encompassing source code and documentation to implement the AI-based systems.

Some of the codes produced through thematic analysis include: bias detection, transparency and accountability, user consent and GDPR compliance, fairness evaluation and compliance with the EU AI Act. In relation to hierarchical clustering, some of the words that depict ethics in AI include: ethical, bias, bias_type, evaluate_bias, bias_score. This highlight the ability of LLM-BMAS to generate outputs related to legal aspects of ethics in AI (i.e. GDPR and EU AI Act), in addition to theoretical and practical ethical issues, both in the documentation and in the source code, through codes in the thematic analysis (e.g., transparency, fairness, bias) and functions detected in the hierarchical clustering (e.g., evaluate_bias), respectively. Regarding the ablation study, for each project description it produced around 80 lines of output without source code, covering considerably less AI ethical issues. Therefore, this experiment underlines the potential of employing trustworthiness improvement techniques in LLMs to the development of more ethically aligned AI-based systems.

Nevertheless, there are some issues that hinder the synergy for practitioners [[22](https://arxiv.org/html/2411.08881v1#bib.bib22)]. Worth highlighting is the lack of practicality, from scraping the source code from the text - even more difficult when it comes to modules -, and installing packages and dealing with deprecated dependencies related to the LLM training date. While it is possible to improve trustworthiness and quality in the use of LLM4SE, there are still open issues to work on to make it more practical for practitioners.

Whilst this experiment was conducted relying only on the internal knowledge of the Open AI LLM, it is important to note that providing the AI ethical agent with legislation (e.g., GDPR and EU AI Act) or AI ethical methods or guidelines (e.g., ECCOLA [[39](https://arxiv.org/html/2411.08881v1#bib.bib39)]) is a prospective future work that we intend to pursue. Moreover, in-depth analysis of the source code provided by the prototype is necessary to assess ethical alignment. With this study, we hope to further the understanding of the benefits of improving trustworthiness in LLM for the development of ethical AI-based systems. We aim to open source the prototype source code in further studies.

{credits}

#### 6.0.1 Acknowledgements

This research was supported by Jane and Aatos Erkko Foundation through CONVERGENCE of Humans and Machines Project under grant No. 220025.

#### 6.0.2 \discintname

The authors have no competing interests to declare that are relevant to the content of this article.

## References

*   [1] Bano, M., Hoda, R., Zowghi, D., Treude, C.: Large language models for qualitative research in software engineering: exploring opportunities and challenges. Autom. Softw. Eng. 31(1),  8 (2024). https://doi.org/10.1007/S10515-023-00407-8
*   [2] Braun, V., Clarke, V.: Using thematic analysis in psychology. Qualitative Research in Psychology 3(2), 77–101 (Jan 2006)
*   [3] de Cerqueira, J.A.S., Azevedo, A.P.D., Leão, H.A.T., Canedo, E.D.: Guide for artificial intelligence ethical requirements elicitation - RE4AI ethical guide. In: 55th Hawaii International Conference on System Sciences, HICSS 2022, Virtual Event / Maui, Hawaii, USA, January 4-7, 2022\. pp. 1–10\. ScholarSpace (2022), [http://hdl.handle.net/10125/80015](http://hdl.handle.net/10125/80015)
*   [4] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi, X., Wang, C., Wang, Y., et al.: A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109 (2023)
*   [5] Commission, E.: EU AI Act: first regulation on artificial intelligence. [https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence) (2023), [Accessed 01 April 2024]
*   [6] Corrêa, N.K., Galvão, C., Santos, J.W., Pino, C.D., Pinto, E.P., Barbosa, C., Massmann, D., Mambrini, R., Galvão, L., Terem, E., de Oliveira, N.: Worldwide AI ethics: A review of 200 guidelines and recommendations for AI governance. Patterns 4(10), 100857 (2023). https://doi.org/10.1016/J.PATTER.2023.100857
*   [7] Dai, S., Xiong, A., Ku, L.: Llm-in-the-loop: Leveraging large language model for thematic analysis. In: Bouamor, H., Pino, J., Bali, K. (eds.) Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023\. pp. 9993–10001\. Association for Computational Linguistics (2023). https://doi.org/10.18653/V1/2023.FINDINGS-EMNLP.669
*   [8] Drápal, J., Westermann, H., Šavelka, J.: Using large language models to support thematic analysis in empirical legal studies. ArXiv abs/2310.18729 (2023)
*   [9] Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.: Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325 (2023)
*   [10] Floridi, L., Chiriatti, M.: GPT-3: its nature, scope, limits, and consequences. Minds Mach. 30(4), 681–694 (2020). https://doi.org/10.1007/S11023-020-09548-1
*   [11] Halme, E., Jantunen, M., Vakkuri, V., Kemell, K., Abrahamsson, P.: Making ethics practical: User stories as a way of implementing ethical consideration in software engineering. Inf. Softw. Technol. 167, 107379 (2024). https://doi.org/10.1016/J.INFSOF.2023.107379
*   [12] Hamilton, L., Elliott, D., Quick, A., Smith, S., Choplin, V.: Exploring the use of ai in qualitative analysis: A comparative study of guaranteed income data. International Journal of Qualitative Methods 22 (2023). https://doi.org/10.1177/16094069231201504
*   [13] Hevner, A.R., March, S.T., Park, J., Ram, S.: Design science in information systems research. MIS Q. 28(1), 75–105 (2004)
*   [14] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al.: Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 (2022)
*   [15] Hong, S., Zheng, X., Chen, J.P., Cheng, Y., Zhang, C., Wang, Z., Yau, S.K.S., Lin, Z.H., Zhou, L., Ran, C., Xiao, L., Wu, C.: Metagpt: Meta programming for multi-agent collaborative framework. ArXiv abs/2308.00352 (2023)
*   [16] Hou, X., Zhao, Y., Liu, Y., Yang, Z., Wang, K., Li, L., Luo, X., Lo, D., Grundy, J., Wang, H.: Large language models for software engineering: A systematic literature review. ACM Transactions on Software Engineering and Methodology (2023). https://doi.org/10.1145/3695988
*   [17] Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI ethics guidelines. Nature Machine Intelligence 1(9), 389–399 (2019). https://doi.org/10.1038/S42256-019-0088-2
*   [18] Lemon, O.: Conversational ai for multi-agent communication in natural language: Research directions at the interaction lab. AI Communications 35(4), 295–308 (2022). https://doi.org/10.3233/aic-220147
*   [19] Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al.: Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2023)
*   [20] Liu, Y., Yao, Y., Ton, J.F., Zhang, X., Cheng, R.G.H., Klochkov, Y., Taufiq, M.F., Li, H.: Trustworthy llms: a survey and guideline for evaluating large language models’ alignment. arXiv preprint arXiv:2308.05374 (2023)
*   [21] Liu, Y., Le-Cong, T., Widyasari, R., Tantithamthavorn, C., Li, L., Le, X.D., Lo, D.: Refining chatgpt-generated code: Characterizing and mitigating code quality issues. ACM Trans. Softw. Eng. Methodol. 33(5), 116:1–116:26 (2024). https://doi.org/10.1145/3643674
*   [22] Lo, D.: Trustworthy and synergistic artificial intelligence for software engineering: Vision and roadmaps. In: IEEE/ACM International Conference on Software Engineering: Future of Software Engineering, ICSE-FoSE 2023, Melbourne, Australia, May 14-20, 2023\. pp. 69–85\. IEEE (2023). https://doi.org/10.1109/ICSE-FOSE59343.2023.00010
*   [23] Meyes, R., Lu, M., de Puiseau, C.W., Meisen, T.: Ablation studies in artificial neural networks. arXiv preprint arXiv:1901.08644 (2019)
*   [24] Morgan, D.L.: Exploring the use of artificial intelligence for qualitative data analysis: The case of chatgpt. International journal of qualitative methods 22, 16094069231211248 (2023). https://doi.org/10.1177/160940692312112
*   [25] Newell, A.: A tutorial on speech understanding systems. Speech recognition pp. 4–54 (1975)
*   [26] Ni, B., Buehler, M.J.: Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Extreme Mechanics Letters 67, 102131 (2024). https://doi.org/10.1016/j.eml.2024.102131
*   [27] OpenAI: Chatgpt: Optimizing language models for dialogue. [https://openai.com/chatgpt](https://openai.com/chatgpt) (2023), [Accessed 18 February 2024]
*   [28] Ozkaya, I.: Application of large language models to software engineering tasks: Opportunities, risks, and implications. IEEE Software 40(3),  4–8 (2023). https://doi.org/10.1109/MS.2023.3248401
*   [29] Pearce, H., Ahmad, B., Tan, B., Dolan-Gavitt, B., Karri, R.: Asleep at the keyboard? assessing the security of github copilot’s code contributions. In: 43rd IEEE Symposium on Security and Privacy. pp. 754–768\. IEEE (2022). https://doi.org/10.1109/SP46214.2022.9833571
*   [30] Peng, X.: Software development in the age of intelligence: embracing large language models with the right approach. Frontiers of Information Technology & Electronic Engineering 24(11), 1513–1519 (2023). https://doi.org/10.1631/FITEE.2300537
*   [31] Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu, Z., Sun, M.: Communicative agents for software development. arXiv preprint arXiv:2307.07924 (2023)
*   [32] Rasheed, Z., Waseem, M., Ahmad, A., Kemell, K.K., Xiaofeng, W., Duc, A.N., Abrahamsson, P.: Can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis. arXiv preprint arXiv:2402.01386 (2024)
*   [33] Ray, P.P.: Chatgpt: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems 3, 121–154 (2023). https://doi.org/10.1016/j.iotcps.2023.04.003
*   [34] Ryan, M., Stahl, B.C.: Artificial intelligence ethics guidelines for developers and users: clarifying their content and normative implications. Journal of Information, Communication and Ethics in Society 19(1), 61–86 (2021). https://doi.org/10.1108/JICES-12-2019-0138
*   [35] Shen, T., Jin, R., Huang, Y., Liu, C., Dong, W., Guo, Z., Wu, X., Liu, Y., Xiong, D.: Large language model alignment: A survey. arXiv preprint arXiv:2309.15025 (2023)
*   [36] Siiman, L.A., Rannastu-Avalos, M., Pöysä-Tarhonen, J., Häkkinen, P., Pedaste, M.: Opportunities and challenges for ai-assisted qualitative data analysis: An example from collaborative problem-solving discourse data. In: International Conference on Innovative Technologies and Learning. vol. 14099, pp. 87–96\. Springer (2023). https://doi.org/10.1007/978-3-031-40113-8_9
*   [37] Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W., Zhang, Y., Li, X., et al.: Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561 (2024)
*   [38] Talebirad, Y., Nadiri, A.: Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314 (2023)
*   [39] Vakkuri, V., Kemell, K., Jantunen, M., Halme, E., Abrahamsson, P.: ECCOLA - A method for implementing ethically aligned AI systems. J. Syst. Softw. 182, 111067 (2021). https://doi.org/10.1016/J.JSS.2021.111067
*   [40] Wang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., Truong, S.T., Arora, S., Mazeika, M., Hendrycks, D., Lin, Z., Cheng, Y., Koyejo, S., Song, D., Li, B.: Decodingtrust: A comprehensive assessment of trustworthiness in GPT models. In: Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023 (2023)
*   [41] Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., Wang, C.: Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155 (2023)