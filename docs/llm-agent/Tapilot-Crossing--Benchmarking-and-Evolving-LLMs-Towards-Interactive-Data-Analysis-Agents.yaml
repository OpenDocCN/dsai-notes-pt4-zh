- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tapilot-Crossing：为交互数据分析代理进行基准测试和演化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05307](https://ar5iv.labs.arxiv.org/html/2403.05307)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05307](https://ar5iv.labs.arxiv.org/html/2403.05307)
- en: Jinyang Li^($\clubsuit$),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李劲阳^($\clubsuit$),
- en: Yurong Wu, Chenhao Ma^($\spadesuit$)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 吴宇荣, 马晨昊^($\spadesuit$)
- en: ^($\clubsuit$)Microsoft Research Asia
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\clubsuit$)微软亚洲研究院
- en: ^($\spadesuit$)The Chinese University of Hong Kong, Shenzhen
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\spadesuit$)香港中文大学（深圳）
- en: '{jl0725,huonan,quge}@connect.hku.hk, ckcheng@cs.hku.hk'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{jl0725,huonan,quge}@connect.hku.hk, ckcheng@cs.hku.hk'
- en: '{yan.gao,jlou}@microsoft.com'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{yan.gao,jlou}@microsoft.com'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Interactive Data Analysis, a collaboration between humans and LLM agents, enables
    real-time data exploration for informed decision-making. The challenges and costs
    of collecting realistic interactive logs for data analysis hinder the quantitative
    evaluation of Large Language Model (LLM) agents in this task. To mitigate this
    issue, we introduce Tapilot-Crossing, a new benchmark to evaluate LLM agents on
    interactive data analysis. Tapilot-Crossing contains 1024 interactions, covering
    4 practical scenarios: Normal, Action, Private, and Private Action. Notably, Tapilot-Crossing
    is constructed by an economical multi-agent environment, Decision Company, with
    few human efforts. We evaluate popular and advanced LLM agents in Tapilot-Crossing,
    which underscores the challenges of interactive data analysis. Furthermore, we
    propose Adaptive Interaction Reflection (AIR), a self-generated reflection strategy
    that guides LLM agents to learn from successful history. Experiments demonstrate
    that Air can evolve LLMs into effective interactive data analysis agents, achieving
    a relative performance improvement of up to 44.5%.²²2[https://tapilot-crossing.github.io/](https://tapilot-crossing.github.io/)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 交互数据分析是人类与LLM代理之间的合作，能够实时探索数据以做出明智决策。收集现实交互日志以进行数据分析的挑战和成本阻碍了对大型语言模型（LLM）代理在此任务中定量评估。为了解决这一问题，我们引入了Tapilot-Crossing，一个用于评估LLM代理在交互数据分析中的新基准。Tapilot-Crossing包含1024次交互，涵盖4种实际场景：正常、行动、私人和私人行动。值得注意的是，Tapilot-Crossing由一个经济的多代理环境Decision
    Company构建，几乎不需要人工干预。我们在Tapilot-Crossing中评估了流行和先进的LLM代理，突显了交互数据分析的挑战。此外，我们提出了自适应交互反思（AIR），一种自生成的反思策略，指导LLM代理从成功的历史中学习。实验表明，Air能够将LLM发展成有效的交互数据分析代理，实现了高达44.5%的相对性能提升。²²2[https://tapilot-crossing.github.io/](https://tapilot-crossing.github.io/)
- en: '¹¹footnotetext: Equal contribution.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹脚注：等同贡献。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The exponential growth of the big data calls for accessible data analysis or
    data science techniques that cater to a range of technical backgrounds in various
    data-driven domains, such as healthcare, games, and entertainment (Khanbabaei
    et al., [2018](#bib.bib23); Han et al., [2011](#bib.bib16); Fayyad et al., [1996](#bib.bib11)).
    Recently, the fast development of Large Language Model (LLM) agents (Liu et al.,
    [2023b](#bib.bib34); Xu et al., [2023b](#bib.bib48); Zeng et al., [2023](#bib.bib58);
    Xu et al., [2023a](#bib.bib47); Deng et al., [2024](#bib.bib8); Si et al., [2023](#bib.bib41))
    have been recevied much attention. They are capable of understanding natural language
    queries and generating corresponding code or analysis for data manipulation and
    visualization by reasoning (Huang and Chang, [2023](#bib.bib19); Wei et al., [2022](#bib.bib45);
    Yao et al., [2023](#bib.bib51)) and tool calls (Li et al., [2023b](#bib.bib30);
    Huang et al., [2023b](#bib.bib21); Qin et al., [2023](#bib.bib39)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的指数增长需要适应各种技术背景的数据分析或数据科学技术，涵盖医疗保健、游戏和娱乐等多个数据驱动领域（Khanbabaei et al., [2018](#bib.bib23);
    Han et al., [2011](#bib.bib16); Fayyad et al., [1996](#bib.bib11)）。最近，大型语言模型（LLM）代理的快速发展（Liu
    et al., [2023b](#bib.bib34); Xu et al., [2023b](#bib.bib48); Zeng et al., [2023](#bib.bib58);
    Xu et al., [2023a](#bib.bib47); Deng et al., [2024](#bib.bib8); Si et al., [2023](#bib.bib41)）受到了广泛关注。它们能够理解自然语言查询，并通过推理（Huang
    and Chang, [2023](#bib.bib19); Wei et al., [2022](#bib.bib45); Yao et al., [2023](#bib.bib51)）和工具调用（Li
    et al., [2023b](#bib.bib30); Huang et al., [2023b](#bib.bib21); Qin et al., [2023](#bib.bib39)）生成相应的代码或分析，以进行数据处理和可视化。
- en: '![Refer to caption](img/bbe3f08999a738e0f4c2d08248822495.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bbe3f08999a738e0f4c2d08248822495.png)'
- en: 'Figure 1: This is an overview of the four interaction modes in Tapilot-Crossing.
    Notable Opponents is ambiguous which requires clarification in multi-turn interaction.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：这是Tapilot-Crossing中四种交互模式的概述。显著对手是模糊的，需要在多轮交互中进行澄清。
- en: 'SheetCopilot (Li et al., [2024c](#bib.bib28)), TableGPT (Zha et al., [2023](#bib.bib59)),
    Data-Copilot (Zhang et al., [2023a](#bib.bib60)) showcase the potential of tabular
    data analysis agents through automatic workflow given the user queries. However,
    the dynamic and uncertain nature of real-world analysis obliges effective human-agent
    interaction. This is because user intents can often be ambiguous (De Vries et al.,
    [2020](#bib.bib7); Yan et al., [2023](#bib.bib49); Wang et al., [2024](#bib.bib43)),
    and users may need to adjust their analysis strategies based on intermediate results
    (Yan et al., [2023](#bib.bib49); Yao et al., [2020](#bib.bib52)). For example,
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents"), the notable opponents
    could refer to a variety of interpretations, such as the opponents with the highest
    wins, or the most frequent opponents. To this end, a thorough benchmark for gauging
    their capability for interactive user engagement in data analysis scenarios is
    indispensable.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'SheetCopilot (Li et al., [2024c](#bib.bib28))、TableGPT (Zha et al., [2023](#bib.bib59))、Data-Copilot
    (Zhang et al., [2023a](#bib.bib60)) 通过自动工作流展示了表格数据分析代理的潜力，用户可以通过查询进行操作。然而，现实世界分析的动态和不确定性要求有效的人机互动。这是因为用户意图往往是模糊的（De
    Vries et al., [2020](#bib.bib7); Yan et al., [2023](#bib.bib49); Wang et al.,
    [2024](#bib.bib43)），用户可能需要根据中间结果调整分析策略（Yan et al., [2023](#bib.bib49); Yao et
    al., [2020](#bib.bib52)）。例如，在图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") 中，显著的对手可能指代多种解释，如获胜次数最多的对手，或最频繁的对手。为此，评估其在数据分析场景中与用户互动能力的全面基准是不可或缺的。'
- en: 'In this paper, we introduce Tapilot-Crossing, a new benchmark for evaluating
    LLM agents in interactive data analysis tasks. Tapilot-Crossing is designed to
    simulate real-world data analysis scenarios, where users interact with LLM agents
    to generate codes for data exploration and decision makings. It includes 1024
    user-machine interactions with 1176 user intents, spanning four practical scenarios:
    1) Normal, where all questions and user requirements are explicit, requiring no
    actions from agents; 2) Action, where agents must respond to diverse user feedback
    or instructions; 3) Private, which examines the true semantic parsing capability
    of agents when encountering unseen packages during the pre-training phase (Zan
    et al., [2022](#bib.bib57)); and 4) Private Action, a mode that combines the features
    of Private and Action, more closely reflecting real-world data analysis. There
    are two answer types: 1) Code Generation, which can test whether the agent can
    correctly interpret the user’s query and generate the corresponding code for data
    analysis, and 2) Multiple-Choice questions, which can evaluate the agent’s ability
    to understand the returned results being executed and provide appropriate insights
    for users.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了Tapilot-Crossing，一种用于评估LLM代理在交互式数据分析任务中的新基准。Tapilot-Crossing旨在模拟现实世界的数据分析场景，在这些场景中，用户与LLM代理互动以生成用于数据探索和决策的代码。它包括1024个用户-机器交互和1176个用户意图，涵盖了四种实际场景：1)
    正常，其中所有问题和用户需求都是明确的，无需代理采取行动；2) 行动，代理必须响应各种用户反馈或指示；3) 私密，这个场景考察代理在遇到预训练阶段（Zan
    et al., [2022](#bib.bib57)）未见过的包时的真实语义解析能力；4) 私密行动，这是结合了私密和行动特征的模式，更加贴近现实世界的数据分析。答案类型有两种：1)
    代码生成，可以测试代理是否能正确解读用户的查询并生成相应的数据分析代码；2) 多项选择题，可以评估代理理解执行结果并为用户提供适当见解的能力。
- en: The conventional construction of datasets or benchmarks based on crowdsourcing,
    especially for high-quality and interactive scenarios, is time-consuming and costly
    due to the significant human effort and expertise required (Yu et al., [2019a](#bib.bib54);
    Li et al., [2023a](#bib.bib29); Guo et al., [2021](#bib.bib15); Li et al., [2023d](#bib.bib32);
    Zhang et al., [2023c](#bib.bib62)). In this case, we design a novel multi-agent
    environment, Decision Company, to construct Tapilot-Crossing. Decision Company
    is a simulated environment where 4 GPT-4 agents communicate with each other to
    perform data analysis tasks. By using this environment, two PhD students are able
    to construct Tapilot-Crossing within a span of one months at a cost of less than
    100 US dollars.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基于众包的数据集或基准的传统构建方式，尤其是对于高质量和互动场景，耗时且成本高，因为需要大量的人力和专业知识（Yu et al., [2019a](#bib.bib54);
    Li et al., [2023a](#bib.bib29); Guo et al., [2021](#bib.bib15); Li et al., [2023d](#bib.bib32);
    Zhang et al., [2023c](#bib.bib62)）。在这种情况下，我们设计了一个新型的多代理环境Decision Company来构建Tapilot-Crossing。Decision
    Company是一个模拟环境，其中4个GPT-4代理互相通信以执行数据分析任务。通过使用这个环境，两名博士生能够在一个月内以不到100美元的成本构建Tapilot-Crossing。
- en: We evaluate the popular advanced LLM agents on Tapilot-Crossing. The results
    underscore the challenges of interactive data analysis and fuel the need for more
    advanced LLM agents that can handle diverse user intents and feedback.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了在Tapilot-Crossing上的流行高级LLM代理。结果强调了交互数据分析的挑战，并激发了对能处理多样化用户意图和反馈的更高级LLM代理的需求。
- en: To further evolve the LLMs towards effective interactive data analysis agents,
    we propose Adaptive Interaction Reflection (AIR), which guides LLM agents to learn
    from successful history via self-generated pseudo logic reflection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步将LLM发展为有效的交互数据分析代理，我们提出了自适应互动反思（AIR），该方法通过自生成的伪逻辑反思引导LLM代理从成功的历史中学习。
- en: Our experiments demonstrate that AIR can significantly enhance the performance
    of LLMs, in which GPT-4 can gain relative improvement of 44.5% compared to its
    model base. offering an insight of how to actively improve the interaction between
    human and LLM agents in data analysis tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验表明，AIR可以显著提升LLM的性能，其中GPT-4相比其基础模型有44.5%的相对提升，提供了如何积极改善人类与LLM代理在数据分析任务中的互动的见解。
- en: 'In summary, our contributions are threefold:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的贡献有三方面：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce Tapilot-Crossing, a new benchmark for evaluating LLM agents in
    interactive data analysis tasks. Tapilot-Crossing is constructed by our designed
    multi-agent environment within few human efforts, Decision Company, and covers
    a wide range of practical scenarios.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了Tapilot-Crossing，这是一个用于评估LLM代理在交互数据分析任务中的新基准。Tapilot-Crossing是通过我们设计的多代理环境Decision
    Company构建的，涵盖了广泛的实际场景，并且人力投入较少。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We evaluate popular LLM agents on Tapilot-Crossing, highlighting the challenges
    of interactive data analysis and the need for more advanced LLM agents.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们评估了在Tapilot-Crossing上的流行LLM代理，突显了交互数据分析的挑战以及对更先进LLM代理的需求。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose AIR, an effective and efficient reflection strategy that significantly
    improves the performance of LLM agents in interactive data analysis tasks.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了AIR，这是一种有效且高效的反思策略，能显著提高LLM代理在交互数据分析任务中的表现。
- en: 2 Preliminaries
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基础知识
- en: '![Refer to caption](img/783599cc0ae2a0fdb3bf603040cc1398.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Refer to caption](img/783599cc0ae2a0fdb3bf603040cc1398.png)'
- en: 'Figure 2: This figure provides an overview of action types in Tapilot-Crossing,
    illustrated by examples. We emphasize the keywords specific to each category,
    and demonstrate the relevant sections of the associated queries, as well as the
    agent actions. The number of ![Refer to caption](img/0d87d76d63363024650e1d24ba76cdb5.png)
    symbols represents the relative difficulty of each action.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：该图概述了Tapilot-Crossing中的动作类型，并通过示例进行说明。我们强调了每个类别特有的关键词，并展示了相关查询的部分内容以及代理动作。符号![Refer
    to caption](img/0d87d76d63363024650e1d24ba76cdb5.png)的数量表示每个动作的相对难度。
- en: 2.1 Interactive Data analysis
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 交互数据分析
- en: The task of interactive data analysis with LLM agents involves a user and an
    LLM agent engaging in a sequence of user-agent turns, denoted as $[(u_{1},a_{1}),(u_{2},a_{2}),...,(u_{n},a_{n})]$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与LLM代理进行交互数据分析的任务涉及用户和LLM代理进行一系列的用户-代理轮次，表示为$[(u_{1},a_{1}),(u_{2},a_{2}),...,(u_{n},a_{n})]$。
- en: 2.2 Agent Actions in Interactions
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 交互中的代理动作
- en: 'In Tapilot-Crossing, we summarize 6 common agent actions in interactions to
    deal with the user intents and the contexts as described in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Preliminaries ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs
    Towards Interactive Data Analysis Agents"). We denote the agent action set as
    $A$ refers to the table contents. In this work, we mainly focus on data analysis
    of tabular data.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Tapilot-Crossing 中，我们总结了 6 种常见的代理动作，以应对用户意图和上下文，如图 [2](#S2.F2 "Figure 2 ‣
    2 Preliminaries ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents") 所示。我们用 $A$ 来表示代理动作集合，具体内容请参考表格。在这项工作中，我们主要关注表格数据的分析。'
- en: Formally, we represent the agent action set as $A=f_{\theta}(u,H,T)$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们将代理动作集合表示为 $A=f_{\theta}(u,H,T)$。
- en: 3 Tapilot-Crossing Dataset
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 Tapilot-Crossing 数据集
- en: '![Refer to caption](img/7471c5c2fd0ca1ca834aad74bfdd644a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7471c5c2fd0ca1ca834aad74bfdd644a.png)'
- en: 'Figure 3: This describes the construction pipeline of Tapilot-Crossing by the
    AI Agent Sandbox Decision Company. ![Refer to caption](img/3b6a29a99c7caa21e0397440237ddeb9.png)
    denotes human intervention during construction. For a more detailed describing,
    please refer to Section [3](#S3 "3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3：这描述了 AI Agent Sandbox Decision Company 构建 Tapilot-Crossing 的流程。 ![参见说明](img/3b6a29a99c7caa21e0397440237ddeb9.png)
    表示在构建过程中进行的人工干预。有关更详细的描述，请参见第 [3](#S3 "3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") 节。'
- en: 3.1 Dataset Construction.
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据集构建。
- en: 'The construction of Tapilot-Crossing is mainly based on the AI Agent Sandbox,
    Decision Company, as depicted in Figure [3](#S3.F3 "Figure 3 ‣ 3 Tapilot-Crossing
    Dataset ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents"). Decision Company is a multi-agent environment where four
    GPT-4 agents (Administrator, Client, Data Scientist and AI Chatbot) interact with
    each other to perform data analysis tasks. The construction process involves the
    following steps: Data Acquisition & Preprocessing, Client Persona Generation,
    Analysis Scenario Generation, Plan Discussion and Interaction Log Simulation.
    During these stages, human intervention may be required to correct errors or eliminate
    harmful or biased content. Ultimately, the prototype data is adapted to private
    and action settings.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tapilot-Crossing 的构建主要基于 AI Agent Sandbox、Decision Company，如图 [3](#S3.F3 "Figure
    3 ‣ 3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs
    Towards Interactive Data Analysis Agents") 所示。Decision Company 是一个多代理环境，其中四个 GPT-4
    代理（管理员、客户、数据科学家和 AI 聊天机器人）相互作用以执行数据分析任务。构建过程包括以下步骤：数据获取与预处理、客户角色生成、分析场景生成、计划讨论和交互日志模拟。在这些阶段中，可能需要人工干预来纠正错误或消除有害或有偏见的内容。*最终*，原型数据将适应私有和操作设置。'
- en: Data Acquisition & Prepossessing.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据获取与预处理。
- en: The first step in the construction of Tapilot-Crossing is the acquisition and
    preprocessing of data. We collect 5 open-source tables from Kaggle ^*^**[https://www.kaggle.com/](https://www.kaggle.com/),
    a popular data sicence platform. These datasets span diverse domains, namely ATP
    Tennis, Credit Card, Fast Food, Laptop Price, and Melbourne Housing. Then the
    Administrator Agent will generate column meanings and value illustrations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Tapilot-Crossing 构建的第一步是数据的获取和预处理。我们从 Kaggle ^*^**[https://www.kaggle.com/](https://www.kaggle.com/)**，一个流行的数据科学平台，收集了
    5 个开源表格。这些数据集涵盖了多个领域，包括 ATP 网球、信用卡、快餐、笔记本电脑价格和墨尔本住房。然后，管理员代理将生成列含义和数值说明。
- en: Client Persona Generation.
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 客户角色生成。
- en: The construction of Tapilot-Crossing proceeds to the generation of client personas.
    These personas with specific tasks and topics related to the data are founded
    by the Administrator Agent. Each persona is defined by a Name, Location, Job,
    and Background with diverse range of interests and backgrounds.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Tapilot-Crossing 的构建接下来是生成客户角色。这些角色由管理员代理创建，涉及与数据相关的特定任务和主题。每个角色通过名称、地点、职位和背景来定义，具有多样化的兴趣和背景。
- en: Simulation of Analysis Scenarios.
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分析场景模拟。
- en: 'Then, the Administrator Agent conducts interviews with each Client Agent to
    ask about their Scenario description, Scenario Name, and the Goal of using the
    dataset for the scenario. In the Tapilot-Crossing dataset, human annotators will
    interrupt here and select the most reasonable or interesting scenarios. This ensures
    that the scenarios included in the Tapilot-Crossing dataset are meaningful, not
    too general across different clients. For instance, in B.3 of Figure [3](#S3.F3
    "Figure 3 ‣ 3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing: Benchmarking and Evolving
    LLMs Towards Interactive Data Analysis Agents"), we select Court Condition Impact
    because Player Performance Analysis is too general and Sponsor Attraction requires
    too much additional information out of the table contents, which leads to too
    many unawserable questions.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，Administrator Agent 对每个 Client Agent 进行访谈，询问他们的情境描述、情境名称以及使用数据集的目标。在 Tapilot-Crossing
    数据集中，此时人工注释员会中断，并选择最合理或有趣的情境。这确保了 Tapilot-Crossing 数据集中包含的情境是有意义的，而不是在不同客户之间过于一般化。例如，在图
    [3](#S3.F3 "Figure 3 ‣ 3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") B.3 中，我们选择了 Court
    Condition Impact，因为 Player Performance Analysis 过于一般化，而 Sponsor Attraction 需要额外的信息，这导致了太多无法回答的问题。'
- en: Plan Discussion.
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计划讨论。
- en: 'In this process, the Client Agent and Data Scientist Agent collaborate to convert
    the client’s requirements into a set of specific data analysis questions. Each
    question is provided by an expected result type, such as dataframes, lists, or
    various plot types, which helps reduce question ambiguity and ease the pressure
    on evaluation metrics (Yin et al., [2023](#bib.bib53); He et al., [2023](#bib.bib17);
    Zhang et al., [2023d](#bib.bib63)). The dialogue between the agents further refines
    the questions with specific conditions. For example, as depicted in Figure [3](#S3.F3
    "Figure 3 ‣ 3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing: Benchmarking and Evolving
    LLMs Towards Interactive Data Analysis Agents") B.4, the client Garcia’s question
    could be further elaborated on the basis of his following responses, making all
    questions more answerable. In particular, Agent Garcia, fully cognizant of his
    persona created in B.2, adds the condition grass, reflecting his London location.
    This implies that the role-playing aspect of the agent can be instrumental in
    generating a wider range of questions that are both diverse and reasonable (Li
    et al., [2024a](#bib.bib26); Park et al., [2023](#bib.bib36)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个过程中，Client Agent 和 Data Scientist Agent 合作，将客户的需求转化为一组具体的数据分析问题。每个问题都有预期的结果类型，例如数据框、列表或各种图表类型，这有助于减少问题的模糊性，并缓解评估指标的压力（Yin
    et al., [2023](#bib.bib53); He et al., [2023](#bib.bib17); Zhang et al., [2023d](#bib.bib63)）。代理之间的对话进一步细化了具有特定条件的问题。例如，如图
    [3](#S3.F3 "Figure 3 ‣ 3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") B.4 所示，客户 Garcia
    的问题可以在其后续回答的基础上进一步阐述，使所有问题更具可答性。特别是，Agent Garcia 充分认识到其在 B.2 中创建的角色，添加了草地这一条件，反映了其伦敦的位置。这意味着代理的角色扮演方面可以在生成更多样化且合理的问题方面发挥重要作用（Li
    et al., [2024a](#bib.bib26); Park et al., [2023](#bib.bib36)）。'
- en: Interaction Log Annotation.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 互动日志注释。
- en: Following the plan discussion, the interaction simulation phase begins. Here,
    the AI Chatbot Agent takes the lead, executing the data analysis plan agreed on
    during the previous stage. The Chatbot Agent interacts with the Data Scientist
    Agent to answer a series of questions defined in the plan by generating codes
    and analyzing returned results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在计划讨论之后，互动模拟阶段开始。在这一阶段，AI Chatbot Agent 主导执行之前阶段中达成的数据分析计划。Chatbot Agent 与 Data
    Scientist Agent 互动，通过生成代码和分析返回结果来回答计划中定义的一系列问题。
- en: 3.2 Human Calibration
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 人工校准
- en: While the Decision Company can generate a wealth of data analysis interactions
    in a zero-shot prompting manner, human intervention is indispensable to ensure
    the quality of the data set (Lu et al., [2023](#bib.bib35)). Our observations
    indicate that only 23.5% of the original codes produced by Chatbot Agent can be
    directly used as reference codes. Therefore, human experts, two PhD students,
    participate in each stage of the generation process to calibrate the errors and
    meaningless interactions. While human intervention is required, it is worth noting
    that modifying existing answers or codes is more efficient than creating them
    from scratch. We preserve all natural and meaningful interactions, both agent-to-agent
    and human-to-machine, throughout the action setting collection.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策公司可以以零样本提示的方式生成大量数据分析交互，但人类干预对于确保数据集的质量是不可或缺的（Lu 等， [2023](#bib.bib35)）。我们的观察表明，Chatbot
    Agent 生成的原始代码中只有 23.5% 可以直接作为参考代码使用。因此，两个博士生作为人类专家参与每个生成阶段，以校准错误和无意义的交互。虽然需要人类干预，但值得注意的是，修改现有答案或代码比从头开始创建它们更为高效。在整个动作设置收集过程中，我们保留了所有自然且有意义的交互，包括代理对代理和人类对机器的交互。
- en: 3.3 Private Lib Mode Evolution
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 私有库模式演变
- en: 'Data analyst frequently relies on their private libraries (Zan et al., [2022](#bib.bib57)).
    These libraries, often tailored to their specific needs, allow for more efficient
    and customized data processing and analysis. Furthermore, generating code through
    user-defined packages can test the true semantic parsing abilities of agents rather
    than merely testing their memorization of standard syntax from libraries such
    as Pandas(Lai et al., [2023](#bib.bib24)). It also evaluates their ability to
    understand and implement custom functions, which is a crucial aspect of real-world
    data analysis. In this work, we prompt GPT-4 to autonomously convert prototype
    codes with pre-trained functions like Pandas or Numpy into private codes. The
    details can refer to Appendix [J.5](#A10.SS5 "J.5 Private Lib Evolution ‣ Appendix
    J Decision Company Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards
    Interactive Data Analysis Agents").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '数据分析师经常依赖他们的私有库（Zan 等， [2022](#bib.bib57)）。这些库通常根据他们的特定需求量身定制，可以更高效、更具个性化地进行数据处理和分析。此外，通过用户定义的包生成代码可以测试代理的真正语义解析能力，而不仅仅是测试它们对如
    Pandas 等库的标准语法的记忆（Lai 等， [2023](#bib.bib24)）。这也评估了它们理解和实现自定义函数的能力，这是实际数据分析中的一个关键方面。在本工作中，我们提示
    GPT-4 自主将使用 Pandas 或 Numpy 等预训练函数的原型代码转换为私有代码。详细信息可以参考附录 [J.5](#A10.SS5 "J.5 私有库演变
    ‣ 附录 J 决策公司提示 ‣ Tapilot-Crossing: 基准测试与演进 LLMs 以实现互动数据分析代理")。'
- en: 4 Data Statistics & Metrics
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据统计与度量
- en: 4.1 Dataset Statistics
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集统计
- en: '| Statistic | Number |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 统计 | 数量 |'
- en: '| Total Interactions | 1024 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 总交互 | 1024 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) clear interactions
    | 284 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 清晰交互 | 284 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) action interactions
    | 485 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 动作交互 | 485 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) private
    lib. interactions | 206 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 私有库交互 | 206 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) private
    act. interactions | 49 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 私有活动交互 | 49 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) # of pirvate
    lib functions | 137 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 私有库函数数量 | 137 |'
- en: '| Answer Types |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 答案类型 |  |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) # of code
    generation answers | 594 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 代码生成答案数量 | 594 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) # of multi-choice
    answers | 430 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 多项选择答案数量 | 430 |'
- en: '| Quality & Cost |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 质量与成本 |  |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) inner-agreement
    | 93.64 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 内部一致性 | 93.64 |'
- en: '| ![[Uncaptioned image]](img/b84983be500de4faed7b4482c04ab060.png) total costs
    (USD) | 66.7 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/b84983be500de4faed7b4482c04ab060.png) 总费用（美元） | 66.7 |'
- en: 'Table 1: The statistics of Tapilot-Crossing.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: Tapilot-Crossing 的统计数据。'
- en: '| Dataset | # Q &#124; # Intents | # Toks. / Q | # Toks. / Code | Code Type
    | Analysis | Multi-Turn | Private Lib | Multi-modal | Evaluation |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | # Q &#124; # 意图 | # Toks. / Q | # Toks. / 代码 | 代码类型 | 分析 | 多轮 | 私有库
    | 多模态 | 评估 |'
- en: '| HumanEval (Chen et al., [2021](#bib.bib2)) | 164 &#124; 164 | 0060.9 | 024.4
    | ![[Uncaptioned image]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Test Cases |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval (Chen et al., [2021](#bib.bib2)) | 164 &#124; 164 | 0060.9 | 024.4
    | ![[未标注图像]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | 测试用例 |'
- en: '| MBPP (Austin et al., [2021](#bib.bib1)) | 974 &#124; 974 | 0014.5 | 024.2
    | ![[Uncaptioned image]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Test Cases |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| MBPP (Austin et al., [2021](#bib.bib1)) | 974 &#124; 974 | 0014.5 | 024.2
    | ![[未标注图像]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | 测试用例 |'
- en: '| Spider (Yu et al., [2018](#bib.bib55)) | 1034 &#124; 1034 | 0012.4 | 018.3
    | ![[Uncaptioned image]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc + EM |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Spider (Yu et al., [2018](#bib.bib55)) | 1034 &#124; 1034 | 0012.4 | 018.3
    | ![[未标注图像]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc + EM |'
- en: '| BIRD (Li et al., [2023a](#bib.bib29)) | 1534 &#124; 1534 | 0014.5 | 049.6
    | ![[Uncaptioned image]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc + VES |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| BIRD (Li et al., [2023a](#bib.bib29)) | 1534 &#124; 1534 | 0014.5 | 049.6
    | ![[未标注图像]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc + VES |'
- en: '| DS-1000 (Lai et al., [2023](#bib.bib24)) | 1000 &#124; 1000 | 0282.4 | 042.1
    | ![[Uncaptioned image]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | Test Cases + SFC |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| DS-1000 (Lai et al., [2023](#bib.bib24)) | 1000 &#124; 1000 | 0282.4 | 042.1
    | ![[未标注图像]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | 测试用例 + SFC |'
- en: '| SparC (Yu et al., [2019b](#bib.bib56)) | 1203 &#124; 1203 | 009.4 | 026.3
    | ![[Uncaptioned image]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/b25bd2b9b03760d714d9f14a560d19a2.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SparC (Yu et al., [2019b](#bib.bib56)) | 1203 &#124; 1203 | 009.4 | 026.3
    | ![[未标注图像]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc |'
- en: '| CoSQL (Yu et al., [2019a](#bib.bib54)) | 1008 &#124; 1008 | 0013.1 | 031.4
    | ![[Uncaptioned image]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/b25bd2b9b03760d714d9f14a560d19a2.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| CoSQL (Yu et al., [2019a](#bib.bib54)) | 1008 &#124; 1008 | 0013.1 | 031.4
    | ![[未标注图像]](img/cf311db1ad388973823ba7cf8d12ef36.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[未标注图像]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc |'
- en: '| ARCADE (Yin et al., [2023](#bib.bib53)) | 1066 &#124; 1066 | 0019.2 | 048.2
    | ![[Uncaptioned image]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned image]](img/b25bd2b9b03760d714d9f14a560d19a2.png)
    | ![[Uncaptioned image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | ![[Uncaptioned
    image]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc + Fuzzy |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| ARCADE (Yin et al., [2023](#bib.bib53)) | 1066 &#124; 1066 | 0019.2 | 048.2
    | ![[无标题图片]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png) | ![[无标题图片]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[无标题图片]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[无标题图片]](img/f5c3cbf866703bb3a800640a3f5f9507.png)
    | ![[无标题图片]](img/f5c3cbf866703bb3a800640a3f5f9507.png) | Acc + Fuzzy |'
- en: '| Tapilot-Crossing | 1024 &#124; 1176 | 0273.6 | 110.6 | ![[Uncaptioned image]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png)
    | ![[Uncaptioned image]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[Uncaptioned
    image]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[Uncaptioned image]](img/b25bd2b9b03760d714d9f14a560d19a2.png)
    | ![[Uncaptioned image]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | Acc + AccR
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Tapilot-Crossing | 1024 &#124; 1176 | 0273.6 | 110.6 | ![[无标题图片]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png)
    | ![[无标题图片]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[无标题图片]](img/b25bd2b9b03760d714d9f14a560d19a2.png)
    | ![[无标题图片]](img/b25bd2b9b03760d714d9f14a560d19a2.png) | ![[无标题图片]](img/b25bd2b9b03760d714d9f14a560d19a2.png)
    | Acc + AccR |'
- en: 'Table 2: Comparison of Tapilot-Crossing to other data analysis datasets. The
    first 5 datasets are single-turn data analysis sets featuring both SQL and Python
    codes. The following 3 benchmarks are multi-turn or interactive data analysis
    datasets. Tapilot-Crossing represents a challenging dataset in data analysis with
    more comprehensive settings. ![[Uncaptioned image]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png)
    represents that the end code is Python. ![[Uncaptioned image]](img/cf311db1ad388973823ba7cf8d12ef36.png)
    means the target code is SQL.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Tapilot-Crossing 与其他数据分析数据集的比较。前 5 个数据集是包含 SQL 和 Python 代码的单轮数据分析集。接下来的
    3 个基准是多轮或交互式数据分析数据集。Tapilot-Crossing 代表了数据分析中具有更全面设置的挑战性数据集。 ![[无标题图片]](img/76b6e6d8a2f3b8e6a522f0eb8a1048a8.png)
    表示最终代码是 Python。 ![[无标题图片]](img/cf311db1ad388973823ba7cf8d12ef36.png) 意味着目标代码是
    SQL。
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.1 Dataset Statistics ‣ 4 Data Statistics & Metrics
    ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") provides key statistics for our dataset, while Table [2](#S4.T2 "Table
    2 ‣ 4.1 Dataset Statistics ‣ 4 Data Statistics & Metrics ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") offers a comparison
    between Tapilot-Crossing and other datasets related to data analysis or science.
    To ensure a fair comparison regarding question and code length, we utilize tiktoken
    to compute the number of tokens for each dataset. As shown in Table [2](#S4.T2
    "Table 2 ‣ 4.1 Dataset Statistics ‣ 4 Data Statistics & Metrics ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents"), Tapilot-Crossing
    includes the comprehensive evaluation settings across private library, multi-turn,
    and multi-modal interactions. Additionally, the complexity of this dataset, reflected
    in the extended lengths of questions and associated code snippets, is further
    amplified by the inclusion of multi-intent queries. These queries, encapsulating
    multiple intents within a single question, require a versatile array of computational
    strategies for effective handling. For example, the query, "Please provide mean,
    median, mode, range, and histogram plots for age, employment status, and credit
    history." demands both statistical computations and data visualization. Finally,
    despite Tapilot-Crossing comprising 1024 data analysis interactions, it only incurs
    a cost of 66.7 USD, making it an economical choice for dataset generation. The
    inner-agreement promises the high quality of the dataset.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '表[1](#S4.T1 "Table 1 ‣ 4.1 Dataset Statistics ‣ 4 Data Statistics & Metrics
    ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents")提供了我们数据集的关键统计信息，而表[2](#S4.T2 "Table 2 ‣ 4.1 Dataset Statistics ‣ 4 Data
    Statistics & Metrics ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards
    Interactive Data Analysis Agents")则提供了Tapilot-Crossing与其他与数据分析或科学相关的数据集之间的比较。为了确保在问题和代码长度方面的公平比较，我们使用tiktoken来计算每个数据集的token数量。如表[2](#S4.T2
    "Table 2 ‣ 4.1 Dataset Statistics ‣ 4 Data Statistics & Metrics ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents")所示，Tapilot-Crossing包括了私有库、多轮次和多模态交互的综合评估设置。此外，这个数据集的复杂性在于问题和相关代码片段的扩展长度，而多意图查询的包含进一步放大了这一复杂性。这些查询在一个问题中包含多个意图，需要一系列多样的计算策略来有效处理。例如，查询“请提供年龄、就业状态和信用历史的均值、中位数、众数、范围和直方图”需要进行统计计算和数据可视化。最后，尽管Tapilot-Crossing包含1024个数据分析交互，但仅花费66.7美元，使其成为数据集生成的经济选择。内部一致性保证了数据集的高质量。'
- en: 4.2 Evaluation Metrics
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: Accuracy (Acc).
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准确率 (Acc)。
- en: 'Acc is a thorough metric that evaluates ability of agents to generate code
    that executes correctly and to accurately answer multi-choice questions. It is
    defined as the proportion of instances where the predicted outputs match the expected
    reference output, across all evaluated tasks. For a given dataset with $N$ instance,
    Acc is calculated as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Acc是一种全面的度量指标，用于评估代理生成正确执行代码和准确回答多项选择题的能力。它定义为在所有评估任务中，预测输出与预期参考输出匹配的实例比例。对于一个数据集，Acc按如下方式计算：
- en: '|  | $\mathrm{Acc}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{I}(C_{i}=\hat{C}_{i}),$
    |  | (1) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{Acc}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{I}(C_{i}=\hat{C}_{i}),$
    |  | (1) |'
- en: where $\mathbf{I}$ otherwise.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{I}$ 其他情况。
- en: Acc with Private Lib Recall (AccR).
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带有私有库召回的准确率 (AccR)。
- en: 'Recognizing the importance of accurately leveraging specific user-defined libraries
    in code generation, we extend Acc to include a recall-based adjustment for instances
    involving private libraries. This ensures that AccR not only evaluates the direct
    accuracy of code execution and question answering but also evaluates the inclusion
    and correct usage of private library functions. AccR can be computed as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到在代码生成中准确利用特定用户定义的库的重要性，我们扩展了Acc，以包括针对涉及私有库的实例的基于召回的调整。这确保了AccR不仅评估代码执行和问题回答的直接准确性，还评估了私有库函数的包含和正确使用。AccR可以按如下方式计算：
- en: '|  | $\mathrm{AccR}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{I}(C_{i}=\hat{C}_{i})\cdot\mathbf{R}(C_{i},\hat{C}_{i}),$
    |  | (2) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{AccR}=\frac{1}{N}\sum_{i=1}^{N}\mathbf{I}(C_{i}=\hat{C}_{i})\cdot\mathbf{R}(C_{i},\hat{C}_{i}),$
    |  | (2) |'
- en: '|  | $\mathbf{R}(C{i},\hat{C}{i})=\frac{&#124;\mathbf{F}(C{i})\cap\mathbf{F}(\hat{C}{i})&#124;}{&#124;\mathbf{F}(C{i})&#124;},$
    |  | (3) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{R}(C{i},\hat{C}{i})=\frac{&#124;\mathbf{F}(C{i})\cap\mathbf{F}(\hat{C}{i})&#124;}{&#124;\mathbf{F}(C{i})&#124;},$
    |  | (3) |'
- en: where $\mathbf{R}(C_{i},\hat{C}_{i})$ denote the set of private library functions
    in the reference codes and the set actually utilized by agents in the predicted
    codes, respectively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{R}(C_{i},\hat{C}_{i})$ 表示参考代码中的私有库函数集合和代理在预测代码中实际使用的集合。
- en: 5 Evolving LLMs Towards Interactive Data Analysis Agents
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 发展 LLMs 向交互式数据分析代理
- en: In this section, we discuss our approach of equipping LLMs as data analysis
    agents with tools and reasoning. Then, we introduce our self-generated reflection
    strategy Air to enhancing their performance in interactive settings.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了为 LLMs 配备工具和推理以作为数据分析代理的方法。接着，我们介绍了自生成的反射策略 Air，以增强其在交互设置中的性能。
- en: 5.1 Toolkit
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 工具包
- en: 'Our tool sets include an executor, a user simulator, and a chart-to-table converter.
    The executor provides an environment for models to observe real-time feedback
    on their intermediate code results (Xie et al., [2023](#bib.bib46); Wang et al.,
    [2024](#bib.bib43)). The user simulator (Wang et al., [2024](#bib.bib43); Yan
    et al., [2023](#bib.bib49)), powered by GPT-4-Turbo, tests the agents’ ability
    to generate code after clarifying details when facing under-specific questions.
    The chart-to-table (Liu et al., [2023a](#bib.bib33)) converter mitigates the prevalent
    issue of LLMs’ inability to comprehend plots by converting them into tables. Detailed
    descriptions of these tool sets are provided in the Appendix [E.1](#A5.SS1 "E.1
    Toolkit ‣ Appendix E Agent Implementation ‣ Tapilot-Crossing: Benchmarking and
    Evolving LLMs Towards Interactive Data Analysis Agents").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的工具集包括一个执行器、一个用户模拟器和一个图表到表格的转换器。执行器为模型提供了一个环境，以观察其中间代码结果的实时反馈（Xie 等，[2023](#bib.bib46)；Wang
    等，[2024](#bib.bib43)）。用户模拟器（Wang 等，[2024](#bib.bib43)；Yan 等，[2023](#bib.bib49)），由
    GPT-4-Turbo 驱动，测试代理在面对不够具体的问题时澄清细节后生成代码的能力。图表到表格（Liu 等，[2023a](#bib.bib33)）的转换器通过将图表转换为表格，缓解了
    LLMs 无法理解图表的普遍问题。这些工具集的详细描述见附录 [E.1](#A5.SS1 "E.1 Toolkit ‣ Appendix E Agent Implementation
    ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents")。'
- en: 5.2 Reasoning
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 推理
- en: Reasoning is a critical process in transitioning LLMs into data analysis agents
    (Huang and Chang, [2023](#bib.bib19)). In Tapilot-Crossing, we incorporate two
    primary reasoning methods for code generation and multiple-choice answers. The
    first is the Chain-of-Thought (COT) prompting technique (Wei et al., [2022](#bib.bib45)),
    which enhances the complex reasoning abilities of LLMs by dividing the reasoning
    path into multiple steps. The second method is Reasoning & Action (ReAct), which
    enables models to make decisions by generating reasoning traces and actions in
    an interleaved manner, inducing writing codes, executing and understanding results,
    and make decision based on analysis (Yao et al., [2023](#bib.bib51)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是将大语言模型（LLMs）转变为数据分析代理的关键过程（Huang 和 Chang，[2023](#bib.bib19)）。在 Tapilot-Crossing
    中，我们结合了两种主要的推理方法用于代码生成和多选题答案。第一种是 Chain-of-Thought (COT) 提示技术（Wei 等，[2022](#bib.bib45)），通过将推理路径划分为多个步骤，从而增强
    LLMs 的复杂推理能力。第二种方法是 Reasoning & Action (ReAct)，它通过生成推理痕迹和行动的交错方式，使模型能够做出决策，诱导编写代码、执行并理解结果，并根据分析做出决策（Yao
    等，[2023](#bib.bib51)）。
- en: 5.3 Adapative Interaction Reflection (Air)
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 自适应交互反射（Air）
- en: Successful interactions are important since they encapsulate the logic necessary
    to meet user requirements and ensure correct steps of analysis or code generation.
    Motivated by this, we propose an Adaptive Interaction Reflection (Air) approach
    to enable data analysis agents to learn from successful user-code histories with
    two steps.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的交互很重要，因为它们封装了满足用户需求的逻辑，并确保分析或代码生成的正确步骤。基于此，我们提出了一种自适应交互反射（Air）方法，以使数据分析代理通过两个步骤从成功的用户代码历史中学习。
- en: Pseudo Code Logic Generation.
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 伪代码逻辑生成。
- en: First, given the last previous history $(\mathbf{u}_{t-1};\mathbf{a}_{t-1})$
    since it is an intermediate logics between natural language queries and codes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，给定最后的历史 $(\mathbf{u}_{t-1};\mathbf{a}_{t-1})$，因为它是自然语言查询和代码之间的中间逻辑。
- en: Re-Org One-Shot Reasoning.
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Re-Org One-Shot 推理。
- en: 'Second, we re-organize them into a self-generated one-shot example with the
    order: $\mathbf{p}_{t-1}=(\mathbf{u}_{t-1};\{\mathbf{m}_{t-1};\mathbf{a}_{t-1}\})$,
    we keep the same reasoning method of the original agent. Figure [7](#A4.F7 "Figure
    7 ‣ Appendix D Air Implementation ‣ Tapilot-Crossing: Benchmarking and Evolving
    LLMs Towards Interactive Data Analysis Agents") provides a detailed example.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，我们将它们重新组织成一个自生成的一次性示例，顺序为：$\mathbf{p}_{t-1}=(\mathbf{u}_{t-1};\{\mathbf{m}_{t-1};\mathbf{a}_{t-1}\})$，我们保持原代理的相同推理方法。图
    [7](#A4.F7 "Figure 7 ‣ Appendix D Air Implementation ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") 提供了详细示例。'
- en: '| Model |  | Interaction Mode | Result Type | Overall |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |  | 交互模式 | 结果类型 | 总体 |'
- en: '| Normal | Action | Private | Pri-Act | Code | Choice |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 正常 | 行动 | 私有 | Pri-Act | 代码 | 选择 |'
- en: '| Code-LLama-34B | Model Base | 27.5 | 18.7 | 2.4 | 0.0 | 15.0 | 19.8 | 16.7
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Code-LLama-34B | 模型基础 | 27.5 | 18.7 | 2.4 | 0.0 | 15.0 | 19.8 | 16.7 |'
- en: '| Agent | 18.5 | 22.3 | 1.0 | 0.0 | 9.9 | 24.4 | 15.2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Agent | 18.5 | 22.3 | 1.0 | 0.0 | 9.9 | 24.4 | 15.2 |'
- en: '| Inter-Agent | 28.8 | 22.9 | 2.1 | 0.2 | 15.7 | 24.8 | 19.2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Inter-Agent | 28.8 | 22.9 | 2.1 | 0.2 | 15.7 | 24.8 | 19.2 |'
- en: '| Claude-2.1 | Model Base | 20.2 | 16.8 | 1.5 | 4.5 | 11.4 | 17.9 | 13.7 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Claude-2.1 | 模型基础 | 20.2 | 16.8 | 1.5 | 4.5 | 11.4 | 17.9 | 13.7 |'
- en: '| Agent | 23.4 | 18.0 | 3.9 | 0.0 | 13.6 | 19.1 | 15.6 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Agent | 23.4 | 18.0 | 3.9 | 0.0 | 13.6 | 19.1 | 15.6 |'
- en: '| Inter-Agent | 24.8 | 18.0 | 5.1 | 0.6 | 15.1 | 18.2 | 16.4 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Inter-Agent | 24.8 | 18.0 | 5.1 | 0.6 | 15.1 | 18.2 | 16.4 |'
- en: '| GPT-4-Turbo | Model Base | 27.6 | 17.5 | 5.3 | 4.3 | 17.8 | 16.1 | 17.2 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo | 模型基础 | 27.6 | 17.5 | 5.3 | 4.3 | 17.8 | 16.1 | 17.2 |'
- en: '| Agent | 29.1 | 21.7 | 10.1 | 3.6 | 20.2 | 20.6 | 20.4 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Agent | 29.1 | 21.7 | 10.1 | 3.6 | 20.2 | 20.6 | 20.4 |'
- en: '| Inter-Agent | 29.2 | 22.8 | 12.0 | 7.9 | 20.6 | 23.1 | 21.5 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Inter-Agent | 29.2 | 22.8 | 12.0 | 7.9 | 20.6 | 23.1 | 21.5 |'
- en: '| GPT-4-32k | Model Base | 29.7 | 24.2 | 7.1 | 0.0 | 17.8 | 25.4 | 20.9 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-32k | 模型基础 | 29.7 | 24.2 | 7.1 | 0.0 | 17.8 | 25.4 | 20.9 |'
- en: '| Agent | 23.4 | 39.2 | 9.1 | 5.3 | 16.6 | 38.8 | 25.9 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Agent | 23.4 | 39.2 | 9.1 | 5.3 | 16.6 | 38.8 | 25.9 |'
- en: '| Inter-Agent | 32.2 | 41.3 | 10.6 | 9.8 | 21.6 | 42.1 | 30.2 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Inter-Agent | 32.2 | 41.3 | 10.6 | 9.8 | 21.6 | 42.1 | 30.2 |'
- en: 'Table 3: Overall results of LLMs in base, agent, and inter-agent modes on the
    Tapilot-Crossing dataset. Pri-Act refers to private library + action evaluation
    mode.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：LLMs在Tapilot-Crossing数据集上的总体结果，包括基础、代理和代理间模式。Pri-Act指私有库+行动评估模式。
- en: 6 Experiment
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 'We introduce experiment setup in Section [6.1](#S6.SS1 "6.1 Experiment Setup
    ‣ 6 Experiment ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents"), experimental results in Section [6.2](#S6.SS2.SSS0.Px1
    "Overall Results. ‣ 6.2 Experimental Results ‣ 6 Experiment ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents"), and
    analysis in Section [6.3](#S6.SS3 "6.3 Private Mode Analysis ‣ 6 Experiment ‣
    Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") and [6.4](#S6.SS4 "6.4 Model Shortcoming Analysis ‣ 6 Experiment ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第 [6.1](#S6.SS1 "6.1 Experiment Setup ‣ 6 Experiment ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") 节介绍实验设置，在第
    [6.2](#S6.SS2.SSS0.Px1 "Overall Results. ‣ 6.2 Experimental Results ‣ 6 Experiment
    ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") 节介绍实验结果，以及在第 [6.3](#S6.SS3 "6.3 Private Mode Analysis ‣ 6 Experiment
    ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") 和第 [6.4](#S6.SS4 "6.4 Model Shortcoming Analysis ‣ 6 Experiment ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") 节分析。'
- en: 6.1 Experiment Setup
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实验设置
- en: Models.
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型。
- en: Our experiments primarily involve popular Large Language Models (LLMs) that
    are capable of generating code and following complex human instructions since
    this is a basic in data analysis. Therefore, we investigate performance of GPT-4-Turbo^†^††gpt-4-1106-preview,
    GPT-4-32k, Claude-2.1 and CodeLlama-34B ^‡^‡‡codellama-34b-instruct-hf (Roziere
    et al., [2023](#bib.bib40)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验主要涉及流行的大型语言模型（LLMs），这些模型能够生成代码并遵循复杂的人工指令，因为这是数据分析的基础。因此，我们调查了GPT-4-Turbo^†^††gpt-4-1106-preview、GPT-4-32k、Claude-2.1和CodeLlama-34B
    ^‡^‡‡codellama-34b-instruct-hf（Roziere et al., [2023](#bib.bib40)）的性能。
- en: Implementation details.
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节。
- en: 'Each LLM is implemented with three settings. 1) Model-Base refers to the LLM
    itself without any tool calls and reasonings. 2) Agent mode involves tool usage
    and reasoning. We employ zero-shot COT for guiding the LLM in code generation
    tasks since it can allow us to test the pure code generation ability of agents
    in data analysis. For multi-choice question answering, we utilize one-shot ReAct.
    3) Inter-Agent mode incorporates Air as described in Section [5.3](#S5.SS3 "5.3
    Adapative Interaction Reflection (Air) ‣ 5 Evolving LLMs Towards Interactive Data
    Analysis Agents ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents") beyond the agent. Each model is provided with the last
    up to 5 turns of user-code histories. Further details can be found in Appendix
    [F](#A6 "Appendix F Implementation Details ‣ Tapilot-Crossing: Benchmarking and
    Evolving LLMs Towards Interactive Data Analysis Agents"). For implementation of
    Private settings, we follow (Li et al., [2023b](#bib.bib30); Zan et al., [2022](#bib.bib57))
    to prompt agents to retrieve private libraries first then generate the code with
    retrieved packages.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '每个大语言模型（LLM）都实现了三种设置。1) Model-Base 指的是没有任何工具调用和推理的大语言模型本身。2) 代理模式涉及工具使用和推理。我们使用零-shot
    COT 来指导 LLM 在代码生成任务中的表现，因为这可以帮助我们测试代理在数据分析中的纯代码生成能力。对于多选题回答，我们采用 one-shot ReAct。3)
    代理间模式结合了第 [5.3](#S5.SS3 "5.3 Adapative Interaction Reflection (Air) ‣ 5 Evolving
    LLMs Towards Interactive Data Analysis Agents ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") 节中描述的 Air 机制，超越了代理模式。每个模型提供了最多
    5 次的用户代码历史。更多细节请参见附录 [F](#A6 "Appendix F Implementation Details ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents")。在实现私有设置时，我们遵循
    (Li et al., [2023b](#bib.bib30); Zan et al., [2022](#bib.bib57)) 的方法，首先提示代理检索私有库，然后使用检索到的包生成代码。'
- en: 6.2 Experimental Results
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 实验结果
- en: '![Refer to caption](img/360e91c55bee3b58a80cda68432cfdca.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/360e91c55bee3b58a80cda68432cfdca.png)'
- en: 'Figure 4: Visualization of the performance of GPT-4-32k across various categories
    in Action Mode. It includes a comparative analysis of the base, agent, and inter_agent
    versions.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：GPT-4-32k 在行动模式下各个类别的性能可视化。图中包括了基本版、代理版和代理间版的比较分析。
- en: Overall Results.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总体结果。
- en: 'Table [3](#S5.T3 "Table 3 ‣ Re-Org One-Shot Reasoning. ‣ 5.3 Adapative Interaction
    Reflection (Air) ‣ 5 Evolving LLMs Towards Interactive Data Analysis Agents ‣
    Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") illustrates the comprehensive performance of all LLM agents and their
    base models on the Tapilot-Crossing dataset. From the results, we can deduce the
    following: 1) Most models with Agent version outperform their base version, highlighting
    the crucial role of tools and reasoning in enhancing the performance of Language
    Learning Models (LLMs) under complex tasks (Liu et al., [2023b](#bib.bib34); Xie
    et al., [2023](#bib.bib46)). 2) All models exhibit obvious improvements in the
    Inter-Agent mode with Air. This indicates that the underlying logic of successful
    interaction histories is instrumental in guiding LLMs to become more proficient
    data analysis agents in interactive settings. 3) Despite GPT-4-Turbo’s performance
    being nearly on par with GPT-4 in code generation, its overall performance still
    falls short of GPT-4\. This indicates that beyond code writing, understanding
    results, and analysis are equally important. Fortunately, the comprehensive settings
    of Tapilot-Crossing can assist users in selecting models for data analysis tasks.
    4) It’s superising to see exceptional performance of CodeLlama in the Normal code
    generation setting. We observe that CodeLlama frequently defines functions automatically
    and applies these in the following code, thereby improving readability and logic.
    This is particularly beneficial in tasks related to data-analysis code generation.
    Such tasks often require the composition of API functions, which demands a profound
    understanding of the context and the capability to extract common patterns into
    reusable functions. By defining and reusing symbolic functions, CodeLlama can
    streamline complex contexts, making them more logical, which is an advantage for
    resolving complex tasks (Gu et al., [2023](#bib.bib13)).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [3](#S5.T3 "表格 3 ‣ Re-Org 一次性推理。 ‣ 5.3 自适应互动反思 (Air) ‣ 5 发展中的 LLM 向互动数据分析代理的过渡
    ‣ Tapilot-Crossing: 基准测试与发展中的 LLM 向互动数据分析代理的过渡") 展示了所有 LLM 代理及其基础模型在 Tapilot-Crossing
    数据集上的综合表现。根据结果，我们可以推断出以下几点：1) 大多数具有 Agent 版本的模型优于其基础版本，这突出显示了工具和推理在复杂任务中提高语言学习模型（LLMs）性能的关键作用（刘等，[2023b](#bib.bib34);
    谢等，[2023](#bib.bib46)）。2) 所有模型在 Air 的 Inter-Agent 模式下都表现出明显的改善。这表明，成功互动历史的潜在逻辑对引导
    LLM 在互动环境中成为更高效的数据分析代理至关重要。3) 尽管 GPT-4-Turbo 在代码生成上的表现几乎与 GPT-4 持平，但其整体性能仍不及 GPT-4。这表明，除了代码编写之外，理解结果和分析同样重要。幸运的是，Tapilot-Crossing
    的综合设置可以帮助用户选择适合的数据分析模型。4) 在 Normal 代码生成设置下，CodeLlama 的表现令人惊讶。我们观察到 CodeLlama 经常自动定义函数并在后续代码中应用这些函数，从而提高了可读性和逻辑性。这对于与数据分析代码生成相关的任务特别有益。这些任务通常需要组成
    API 函数，这要求对上下文有深入理解并能够将常见模式提取到可重用函数中。通过定义和重用符号函数，CodeLlama 可以简化复杂的上下文，使其更加逻辑化，这对于解决复杂任务是一个优势（顾等，[2023](#bib.bib13)）。'
- en: Fine-Grained Results on Action Modes.
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 细粒度的行动模式结果。
- en: '![Refer to caption](img/f426787da7aa36b5f4ae40dbdb536ecb.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f426787da7aa36b5f4ae40dbdb536ecb.png)'
- en: 'Figure 5: Visualization of the performance of LLMs on Normal, Private, Private
    w/ Recall. The Scores of first two modes are Acc. The last score is AccR.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLMs 在 Normal、Private、Private w/ Recall 模式下表现的可视化。前两种模式的得分是 Acc。最后一个得分是
    AccR。
- en: 'Figure [4](#S6.F4 "Figure 4 ‣ 6.2 Experimental Results ‣ 6 Experiment ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") provides
    a comparative evaluation of three GPT-4 model variants across various Action modes,
    as detailed in Section [2.2](#S2.SS2 "2.2 Agent Actions in Interactions ‣ 2 Preliminaries
    ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents"). The interactive data analysis agent, Inter-Agent, obviously outperforms
    in most areas, especially in managing Fast_Fail queries and executing Update_Code
    actions. However, it falls short in the Best_Guess action compared to the Agent.
    We note that Air tends to make agents overly tractable in re-org one-shot example
    $\mathbf{p}_{t-1}$ do not contain instructions on making assumptions, agents tend
    to select None of Above. This observation suggests that excessive reliance on
    historical data may hinder the inherent ability of models to conjecture based
    on the instant user behaviors. Therefore, striking a trade-off between user-code
    history exploration and real-time user interaction, especially facing under-specific
    questions is crucial for improving the performance of LLM agents in interactive
    settings.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S6.F4 "图 4 ‣ 6.2 实验结果 ‣ 6 实验 ‣ Tapilot-Crossing: 基准测试和演进 LLM 以实现交互式数据分析代理")
    提供了三个 GPT-4 模型变体在各种行动模式下的比较评估，如 [2.2](#S2.SS2 "2.2 代理在交互中的行动 ‣ 2 初步工作 ‣ Tapilot-Crossing:
    基准测试和演进 LLM 以实现交互式数据分析代理") 节中详细描述的那样。交互式数据分析代理 Inter-Agent 显然在大多数领域表现优异，尤其是在处理
    Fast_Fail 查询和执行 Update_Code 操作方面。然而，与 Agent 相比，它在 Best_Guess 操作上表现不足。我们注意到，Air
    倾向于使代理在重组一次性示例 $\mathbf{p}_{t-1}$ 时过于依赖，示例中没有包含关于做出假设的指令，代理倾向于选择 None of Above。这一观察表明，过度依赖历史数据可能会妨碍模型基于即时用户行为进行推测的固有能力。因此，在用户代码历史探索和实时用户交互之间取得平衡，尤其是在面对不明确的问题时，对于提升
    LLM 代理在交互设置中的表现至关重要。'
- en: 6.3 Private Mode Analysis
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 私有模式分析
- en: Overall Results.
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总体结果。
- en: 'Table [3](#S5.T3 "Table 3 ‣ Re-Org One-Shot Reasoning. ‣ 5.3 Adapative Interaction
    Reflection (Air) ‣ 5 Evolving LLMs Towards Interactive Data Analysis Agents ‣
    Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") and Figure [5](#S6.F5 "Figure 5 ‣ Fine-Grained Results on Action Modes.
    ‣ 6.2 Experimental Results ‣ 6 Experiment ‣ Tapilot-Crossing: Benchmarking and
    Evolving LLMs Towards Interactive Data Analysis Agents") indicates that the Private
    setting presents a considerable obstacle, with the best performing GPT-4-Turbo
    Inter-Agent only achieving 12%. This demonstrates that understanding and implementing
    user-specific functions is a critical and urgent skill for LLM agents in real-world
    data analysis tasks (Zan et al., [2022](#bib.bib57)).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S5.T3 "表 3 ‣ 重组一次性推理 ‣ 5.3 适应性互动反映 (Air) ‣ 5 演进 LLM 以实现交互式数据分析代理 ‣ Tapilot-Crossing:
    基准测试和演进 LLM 以实现交互式数据分析代理") 和图 [5](#S6.F5 "图 5 ‣ 行动模式的细粒度结果 ‣ 6.2 实验结果 ‣ 6 实验 ‣
    Tapilot-Crossing: 基准测试和演进 LLM 以实现交互式数据分析代理") 指出，Private 设置带来了相当大的障碍，表现最佳的 GPT-4-Turbo
    Inter-Agent 仅达到了 12%。这表明，理解和实施用户特定功能是 LLM 代理在现实世界数据分析任务中的一项关键而紧迫的技能 (Zan et al.,
    [2022](#bib.bib57))。'
- en: The Critical Role of Function Relative Recall.
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 功能相关召回的关键作用。
- en: Notably, CodeLlama outperforms GPT-4-Turbo in Acc within the Private setting.
    However, its performance declines significantly relative to GPT-4-Turbo upon the
    consideration of private library relative recall in the generated codes, as measured
    by the AccR metric. This observation suggests that CodeLlama tends to reply less
    on user-defined private functions, aiming to reduce risk of code errors. Therefore,
    AccR metric can spotlight the balance required between proficient code generation
    and the meticulous integration of user-specified private libraries to foster safer
    and more satisfying code production.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，CodeLlama 在 Private 设置中在 Acc 方面优于 GPT-4-Turbo。然而，在考虑到生成代码中的私有库相对召回时，其相对表现显著下降，根据
    AccR 指标衡量。这一观察表明，CodeLlama 倾向于减少对用户定义的私有函数的依赖，以降低代码错误的风险。因此，AccR 指标可以突出显示在熟练的代码生成和细致集成用户指定的私有库之间所需的平衡，从而促进更安全和令人满意的代码生成。
- en: 6.4 Model Shortcoming Analysis
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 模型缺陷分析
- en: Long-Context Challenges.
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长期上下文挑战。
- en: The challenge of handling long-contexts is considerable in Tapilot-Crossing,
    especially for models with shorter maximum input lengths. Models such as Codellama-34B,
    which has a maximum input length of 16k, are particularly affected. For example,
    it is essential for LLMs to access all private function descriptions and codes
    for effective code generation with retrieved functions. The statistics shows that
    the average number of prompt tokens for Private is 15.7k, and notably, 20.4% of
    their prompts surpass the 16k length.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在Tapilot-Crossing中处理长上下文的挑战尤其显著，特别是对于最大输入长度较短的模型。像Codellama-34B这样的模型，其最大输入长度为16k，受到的影响尤为明显。例如，对于LLMs而言，为了有效生成代码，需要访问所有私有函数描述和代码。统计数据显示，Private的提示令牌的平均数量为15.7k，值得注意的是，20.4%的提示超过了16k的长度。
- en: Instruction Following.
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令跟随。
- en: Our experiments reveal that GPT-4-32k requires minimal efforts in prompt design
    due to their exceptional ability to follow human instructions. To be specific,
    only 3.4% of their results deviate from the provided instructions. However, other
    models exhibit a higher proportion of unexpected result types. For instance, extracting
    generated codes or answers from Claude-2.1 proves to be extremely challenging
    since it often embeds the answer in the middle of outputs rather than at the end
    as defined. We also observe that GPT-4-Turbo tends to generate longer codes in
    any settings. While this characteristic enhances its performance in code generation,
    it also results in 60.3% of the code generated during ReAct reasoning being non-executable,
    thereby leading to incorrect answers. Furthermore, CodeLlama-34B-Instruct exhibits
    a lack of robustness when faced with longer or more complex prompts. With the
    addition of COT, the performance of CodeLlama significantly drops from 27.5% with
    simpler instructions to 18.5% in Normal code generation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验表明，GPT-4-32k由于其出色的跟随人类指令的能力，在提示设计上需要的努力非常少。具体来说，只有3.4%的结果偏离了提供的指令。然而，其他模型展示了更高比例的意外结果类型。例如，从Claude-2.1中提取生成的代码或答案非常具有挑战性，因为它通常将答案嵌入输出的中间部分，而不是按照定义放在最后。此外，我们还观察到GPT-4-Turbo在任何设置下都倾向于生成较长的代码。虽然这一特性增强了其代码生成的性能，但也导致了在ReAct推理过程中60.3%的生成代码不可执行，从而导致了错误的答案。此外，CodeLlama-34B-Instruct在面对较长或更复杂的提示时表现出缺乏鲁棒性。随着COT的加入，CodeLlama的表现显著下降，从27.5%的简单指令下降到18.5%的正常代码生成。
- en: 7 Related Work
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 相关工作
- en: Large Language Models for Data Analysis.
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据分析的大型语言模型。
- en: The use of LLMs for data analysis has been a topic of interest in recent years.
    LLMs powered by In-Context Learning (Yang et al., [2023](#bib.bib50); Dai et al.,
    [2023](#bib.bib6); Dong et al., [2023](#bib.bib10)) have been employed in various
    data analysis tasks, such as SQL query generation (Pourreza and Rafiei, [2024a](#bib.bib37);
    Gao et al., [2023](#bib.bib12); Lei et al., [2023](#bib.bib25); Zhang et al.,
    [2023b](#bib.bib61); Gu et al., [2024](#bib.bib14); Wang et al., [2023a](#bib.bib42);
    Pourreza and Rafiei, [2024b](#bib.bib38); Li et al., [2024b](#bib.bib27)), pandas
    or python code generation (Jain et al., [2023](#bib.bib22); Chen et al., [2024](#bib.bib4),
    [2023a](#bib.bib3); Li et al., [2024c](#bib.bib28); Zha et al., [2023](#bib.bib59);
    Zhang et al., [2023a](#bib.bib60); Zheng et al., [2024b](#bib.bib65)), and data
    visualization (Chen et al., [2023b](#bib.bib5); Huang et al., [2023a](#bib.bib20)).
    However, most of these works focus on single-turn setting, where the user’s query
    is explicit and does not require any interaction or clarification. Recently, there
    has been a growing interest in interactive data analysis, where the user intents
    may need to be clarified or refined through interactive communication (De Vries
    et al., [2020](#bib.bib7); Yan et al., [2023](#bib.bib49); Wang et al., [2024](#bib.bib43)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，大型语言模型（LLMs）在数据分析中的应用成为了一个备受关注的话题。由上下文学习（Yang et al., [2023](#bib.bib50);
    Dai et al., [2023](#bib.bib6); Dong et al., [2023](#bib.bib10)）驱动的LLMs已被应用于各种数据分析任务，例如SQL查询生成（Pourreza
    and Rafiei, [2024a](#bib.bib37); Gao et al., [2023](#bib.bib12); Lei et al., [2023](#bib.bib25);
    Zhang et al., [2023b](#bib.bib61); Gu et al., [2024](#bib.bib14); Wang et al.,
    [2023a](#bib.bib42); Pourreza and Rafiei, [2024b](#bib.bib38); Li et al., [2024b](#bib.bib27)）、pandas或python代码生成（Jain
    et al., [2023](#bib.bib22); Chen et al., [2024](#bib.bib4), [2023a](#bib.bib3);
    Li et al., [2024c](#bib.bib28); Zha et al., [2023](#bib.bib59); Zhang et al.,
    [2023a](#bib.bib60); Zheng et al., [2024b](#bib.bib65)）以及数据可视化（Chen et al., [2023b](#bib.bib5);
    Huang et al., [2023a](#bib.bib20)）。然而，这些研究大多集中于单轮设置，其中用户的查询是明确的，不需要任何交互或澄清。最近，互动数据分析引起了越来越多的关注，在这种情况下，用户的意图可能需要通过互动沟通来澄清或细化（De
    Vries et al., [2020](#bib.bib7); Yan et al., [2023](#bib.bib49); Wang et al.,
    [2024](#bib.bib43)）。
- en: Data Analysis Benchmarks.
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据分析基准测试。
- en: The development of benchmarks for data analysis tasks has been a crucial factor
    in driving the progress of LLMs in data science. Existing benchmarks can be broadly
    categorized into single-turn and multi-turn benchmarks. Single-turn benchmarks,
    such as HumanEval (Chen et al., [2021](#bib.bib2)), MBPP (Austin et al., [2021](#bib.bib1)),
    Spider (Yu et al., [2018](#bib.bib55)), BIRD (Li et al., [2023a](#bib.bib29)),
    Text2Analysis (He et al., [2023](#bib.bib17)), DABench (Hu et al., [2024](#bib.bib18))
    and DS-1000 (Lai et al., [2023](#bib.bib24)), focus on generating code snippets
    or closed-form insight summaries for data analysis given a single user query.
    To explore interactive nature of real-world data analysis scenarios, where the
    user’s intent may need to be clarified or refined through interactive communication,
    several multi-turn benchmarks have been proposed, including CoSQL (Yu et al.,
    [2019a](#bib.bib54)), and ARCADE (Yin et al., [2023](#bib.bib53)). However, these
    benchmarks are primarily focused on code generation and do not cover other aspects
    of data analysis, such as data visualization and understanding based on intermediate
    results. Our work extends the existing literature by introducing a new benchmark,
    Tapilot-Crossing, for evaluating LLM agents in interactive data analysis tasks
    across wide range of data analysis settings.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析任务基准的发展一直是推动LLMs在数据科学领域进步的关键因素。现有的基准测试大致可以分为单轮和多轮基准测试。单轮基准测试，如HumanEval（Chen等人，[2021](#bib.bib2)）、MBPP（Austin等人，[2021](#bib.bib1)）、Spider（Yu等人，[2018](#bib.bib55)）、BIRD（Li等人，[2023a](#bib.bib29)）、Text2Analysis（He等人，[2023](#bib.bib17)）、DABench（Hu等人，[2024](#bib.bib18)）和DS-1000（Lai等人，[2023](#bib.bib24)），专注于针对单个用户查询生成代码片段或封闭形式的见解总结。为了探索现实世界数据分析场景的交互特性，其中用户的意图可能需要通过互动沟通来澄清或调整，提出了若干多轮基准测试，包括CoSQL（Yu等人，[2019a](#bib.bib54)）和ARCADE（Yin等人，[2023](#bib.bib53)）。然而，这些基准主要关注代码生成，并未涵盖数据分析的其他方面，如基于中间结果的数据可视化和理解。我们的工作通过引入一个新的基准Tapilot-Crossing，扩展了现有文献，用于评估LLM代理在各种数据分析设置中的交互数据分析任务。
- en: Multi-Agent Environments for Data Generation.
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据生成的多智能体环境。
- en: LLMs have proven to be effective in constructing multi-agent environments for
    automatic data generation. For instance, Lu et al. ([2023](#bib.bib35)) and Ding
    et al. ([2023](#bib.bib9)) simulate dialogs for QA and text generation tasks.
    Also Li et al. ([2023b](#bib.bib30)) generates data about API calls using multi-agent
    environments. This is because LLM agents can simulate believable human actions
    when placed in an environment with dynamically updating knowledge and memory (Park
    et al., [2023](#bib.bib36)). Inspired by this, we also created Decision Company
    to generate interaction log data for data analysis with more believable behaviors.
    Unlike most of previous work on training dataset generation, our research pioneers
    the construction of the interactive benchmark with a specific focus on interactive
    data analysis agent evaluation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在构建用于自动数据生成的多智能体环境方面已被证明是有效的。例如，Lu等人（[2023](#bib.bib35)）和Ding等人（[2023](#bib.bib9)）模拟了QA和文本生成任务的对话。此外，Li等人（[2023b](#bib.bib30)）利用多智能体环境生成关于API调用的数据。这是因为，当LLM代理被放置在一个动态更新知识和记忆的环境中时，它们能够模拟出逼真的人类行为（Park等人，[2023](#bib.bib36)）。受到此启发，我们也创建了Decision
    Company，以生成用于数据分析的交互日志数据，并且行为更具可信度。与之前大多数关于训练数据集生成的研究不同，我们的研究开创了一个具有特定关注点的交互基准，专注于交互数据分析代理评估。
- en: 8 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We introduce Tapilot-Crossing, a new benchmark for evaluating LLM agents in
    interactive data analysis tasks. Tapilot-Crossing is constructed using a cost-effective
    multi-agent environment, Decision Company, and covers a wide range of practical
    scenarios. We evaluate data analysis agents based on popular LLMs on Tapilot-Crossing,
    highlighting the challenges of interactive data analysis and the need for more
    advanced interactive data analysis agents. We also propose AIR, an effective reflection
    strategy for interactive data analysis agent evolution. Our experiments demonstrate
    that AIR can obviously enhance the performance of LLM agents.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了Tapilot-Crossing，这是一个用于评估LLM代理在交互数据分析任务中的新基准。Tapilot-Crossing是通过经济高效的多智能体环境Decision
    Company构建的，涵盖了广泛的实际场景。我们在Tapilot-Crossing上评估了基于流行LLMs的数据分析代理，突出展示了交互数据分析的挑战以及对更先进的交互数据分析代理的需求。我们还提出了AIR，这是一种有效的交互数据分析代理演进策略。我们的实验表明，AIR可以显著提升LLM代理的性能。
- en: 9 Limitations
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 局限性
- en: Dataset Limitations.
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集局限性。
- en: '1) The Tapilot-Crossing dataset assumes that all human-machine interaction
    history is clean and correct. However, in real-world scenarios, the interaction
    history is often not clean, and may contain noise or require multi-turn clarifications
    for a single question. Therefore, future work should consider a more realistic,
    noisy interaction benchmark in data analysis. It is also worth noting that even
    with a clean history, the most capable model, GPT-4-32k, only achieves a score
    of 30.2 in the Inter-Agent mode. 2) The creation of our dataset is both cost-effective
    and efficient; however, the evaluation phase demands considerable efforts due
    to the inherent complexity and unpredictability of data analysis questions. Given
    it is challenging to discern subtle differences in performance among data analysis
    agents, especially in the context of long-form code generation and execution accuracy.
    This difficulty is exacerbated by the fact that the execution results of code
    with a single error (i.e., one-line error) and a completely incorrect code (just
    one-line output) are also determined as 0\. Therefore, a soft-metric evaluation
    system should be introduced in the future as Appendix [H.5](#A8.SS5 "H.5 Code
    Similarity Equivalance (CSE) ‣ Appendix H Evaluation Metric Details ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents"). It
    would improve our ability to accurately gauge how close an answer is to the expected
    output, even when the executed output is zero, thereby providing a more fine-grained
    observation of code generation capabilities. 3) Finally, our work only concentrate
    on tabular data based analysis, while in the future, we would like to involve
    relational database (RDB)-based analysis with the programming language of SQLs.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '1) Tapilot-Crossing 数据集假设所有人机交互历史都是干净和正确的。然而，在现实世界中，交互历史往往不干净，可能包含噪音或需要多轮澄清一个问题。因此，未来的工作应考虑在数据分析中使用更现实、噪声较大的交互基准。值得注意的是，即使历史数据干净，最强大的模型
    GPT-4-32k 在 Inter-Agent 模式下也仅获得了 30.2 的分数。2) 我们的数据集创建既具成本效益又高效；然而，评估阶段因数据分析问题固有的复杂性和不可预测性而需要相当大的努力。鉴于在数据分析代理中难以区分性能的细微差异，特别是在长篇代码生成和执行准确性的背景下，这种困难被进一步加剧，因为一个单行错误的代码（即，单行错误）和完全错误的代码（仅一行输出）的执行结果也被判定为
    0。因此，未来应引入一个软度量评估系统，如附录 [H.5](#A8.SS5 "H.5 Code Similarity Equivalance (CSE) ‣
    Appendix H Evaluation Metric Details ‣ Tapilot-Crossing: Benchmarking and Evolving
    LLMs Towards Interactive Data Analysis Agents")。这将提高我们准确衡量答案与预期输出之间接近程度的能力，即使执行结果为零，从而提供更细致的代码生成能力观察。3)
    最后，我们的工作仅集中于表格数据分析，未来我们希望结合 SQL 编程语言进行关系数据库（RDB）分析。'
- en: Method Limitations.
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方法局限性。
- en: Our proposed reflection strategy can make LLMs more effective interactive data
    analysis agent, but relies heavily on the accuracy of previous interactions. This
    reliance becomes less reliable in instances where the historical dialogue is cluttered
    with errors, suggesting the need for retrieval-augmented tools or methods to identify
    successful past interactions. Additionally, this strategy does not enhance agent
    performance in initial interactions due to the absence of historical data. Finally,
    while effective in many Action settings, this focus on interaction history may
    limit the inferential capabilities of Large Language Models (LLMs) by prioritizing
    past interactions over present context. Future efforts will be directed towards
    refining this approach to better balance the benefits of leveraging historical
    interactions against the need to maintain or enhance the inferential capabilities
    of LLMs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的反思策略可以使 LLM 更有效地成为交互数据分析代理，但高度依赖于之前交互的准确性。当历史对话充满错误时，这种依赖变得不那么可靠，这表明需要检索增强工具或方法来识别成功的过去交互。此外，由于缺乏历史数据，这一策略不会提升代理在初始交互中的表现。最后，虽然在许多行动设置中有效，但这种对交互历史的关注可能会通过优先考虑过去的交互而限制大型语言模型（LLMs）的推理能力。未来的努力将致力于改进这种方法，以更好地平衡利用历史交互的好处与保持或增强
    LLM 推理能力的需求。
- en: Model Limitations.
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型局限性。
- en: 'In this work, we only test four popular and advanced models in comprehensive
    settings. Many key open-sourced models actually show limited human instruction
    following capability on our datasets leading to very poor performance on reasoning.
    Specifically, as these models are mainly trained on code, they often fail to adequately
    follow user instructions, as illustrated in section [6.4](#S6.SS4 "6.4 Model Shortcoming
    Analysis ‣ 6 Experiment ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards
    Interactive Data Analysis Agents"). These models simply return the instruction
    itself and continue to generate meaningless following dialogue when the resources
    and instructions are more complex. Therefore, how to plan more weaker LLMs to
    become effective interactive data analysis agents would be one of our important
    future work.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们仅在综合设置中测试了四种流行且先进的模型。许多关键的开源模型在我们的数据集上实际上表现出有限的人类指令遵循能力，导致推理表现非常差。具体而言，由于这些模型主要在代码上进行训练，它们经常无法充分遵循用户指令，如第
    [6.4](#S6.SS4 "6.4 模型缺陷分析 ‣ 6 实验 ‣ Tapilot-Crossing: 基准测试和使 LLMs 发展为互动数据分析代理")
    节所示。这些模型在资源和指令更复杂时，通常只是返回指令本身，并继续生成毫无意义的跟进对话。因此，如何规划更多较弱的 LLMs 成为有效的互动数据分析代理将是我们未来重要的工作之一。'
- en: 10 Ethical Statement
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 伦理声明
- en: 'The application of Large Language Models (LLMs) for automatic data generation
    requires a rigorous examination of ethical implications. The primary concern is
    the potential for LLMs to generate contents that could be considered harmful or
    biased. To mitigate these risks, human annotators (two PhD students) already filter
    and fix all problematic cases in Section [3.2](#S3.SS2 "3.2 Human Calibration
    ‣ 3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs
    Towards Interactive Data Analysis Agents"). Also, LLMs may disseminate private
    or sensitive information. Therefore, we employ anonymization techniques wherein
    personal identifiers are systematically altered. For example, the name strings
    are replaced randomly, and any information of personas are switched as well. And
    the geographical locations of John Smith will be replaced with locations of Carlos
    Garcia to prevent any linkage to real-world individuals or entities. These procedures
    are conducted in Section [3](#S3 "3 Tapilot-Crossing Dataset ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents"). Moreover,
    we are committed to ensuring that the outputs generated by our LLM, referred to
    as Tapilot-Crossing, are free from political or sexual biases. To this end, each
    output, including conclusions and generated responses, is rigorously reviewed
    by the authors. In a nutshell, our ethical framework is built on a foundation
    of transparency, accountability, and a proactive stance towards mitigating any
    ethical concerns associated with the use of LLMs. The measures we have implemented
    reflect our commitment to upholding the highest standards of ethical research
    practice with LLMs.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）在自动数据生成中的应用需要对伦理影响进行严格审查。主要关注点是 LLMs 可能生成的内容可能被认为是有害或有偏见的。为减少这些风险，人类标注者（两名博士生）已经过滤和修复了第
    [3.2](#S3.SS2 "3.2 人类校准 ‣ 3 Tapilot-Crossing 数据集 ‣ Tapilot-Crossing: 基准测试和使 LLMs
    发展为互动数据分析代理") 节中的所有问题案例。此外，LLMs 可能传播私人或敏感信息。因此，我们采用了匿名化技术，其中个人标识符被系统地更改。例如，姓名字符串被随机替换，任何关于个人的信息也被交换。约翰·史密斯的地理位置将被卡洛斯·加西亚的地点替代，以防止与现实世界的个人或实体关联。这些程序在第
    [3](#S3 "3 Tapilot-Crossing 数据集 ‣ Tapilot-Crossing: 基准测试和使 LLMs 发展为互动数据分析代理")
    节中进行。此外，我们致力于确保我们的 LLM 生成的输出，称为 Tapilot-Crossing，免于政治或性别偏见。为此，每个输出，包括结论和生成的响应，都由作者严格审查。总之，我们的伦理框架建立在透明度、问责制和积极减少与
    LLMs 使用相关的伦理问题的基础上。我们实施的措施反映了我们致力于保持 LLMs 伦理研究实践最高标准的承诺。'
- en: References
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. 使用大型语言模型进行程序合成。*arXiv 预印本 arXiv:2108.07732*。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等人。2021. 评估在代码上训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*。
- en: Chen et al. (2023a) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao,
    Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023a.
    Universal self-consistency for large language model generation. *arXiv preprint
    arXiv:2311.17311*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023a) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng
    Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang 和 Denny Zhou. 2023a. 大型语言模型生成的通用自一致性。*arXiv
    预印本 arXiv:2311.17311*。
- en: Chen et al. (2024) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2024. Teaching large language models to self-debug. In *The Twelfth International
    Conference on Learning Representations*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2024) Xinyun Chen, Maxwell Lin, Nathanael Schärli 和 Denny Zhou. 2024.
    教授大型语言模型自我调试。在 *第十二届国际表示学习会议*。
- en: 'Chen et al. (2023b) Zhutian Chen, Chenyang Zhang, Qianwen Wang, Jakob Troidl,
    Simon Warchol, Johanna Beyer, Nils Gehlenborg, and Hanspeter Pfister. 2023b. Beyond
    generating code: Evaluating gpt on a data visualization course. *arXiv preprint
    arXiv:2306.02914*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2023b) Zhutian Chen, Chenyang Zhang, Qianwen Wang, Jakob Troidl, Simon
    Warchol, Johanna Beyer, Nils Gehlenborg 和 Hanspeter Pfister. 2023b. 超越生成代码: 评估
    GPT 在数据可视化课程上的表现。*arXiv 预印本 arXiv:2306.02914*。'
- en: 'Dai et al. (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang
    Sui, and Furu Wei. 2023. Why can GPT learn in-context? language models secretly
    perform gradient descent as meta-optimizers. In *Findings of the Association for
    Computational Linguistics: ACL 2023*, pages 4005–4019, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等人 (2023) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang
    Sui 和 Furu Wei. 2023. 为什么 GPT 能够在上下文中学习？语言模型秘密地作为元优化器执行梯度下降。在 *计算语言学协会发现: ACL
    2023*，第 4005–4019 页，加拿大多伦多。计算语言学协会。'
- en: De Vries et al. (2020) Harm De Vries, Dzmitry Bahdanau, and Christopher Manning.
    2020. Towards ecologically valid research on language user interfaces. *arXiv
    preprint arXiv:2007.14435*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Vries 等人 (2020) Harm De Vries, Dzmitry Bahdanau 和 Christopher Manning. 2020.
    朝着生态有效的语言用户界面研究迈进。*arXiv 预印本 arXiv:2007.14435*。
- en: 'Deng et al. (2024) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2024. Mind2web: Towards a generalist agent for
    the web. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人 (2024) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi
    Wang, Huan Sun 和 Yu Su. 2024. Mind2web: 朝着通用网络代理迈进。*神经信息处理系统进展*, 36。'
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu,
    Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models
    by scaling high-quality instructional conversations. In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore,
    December 6-10, 2023*, pages 3029–3051\. Association for Computational Linguistics.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人 (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan
    Liu, Maosong Sun 和 Bowen Zhou. 2023. 通过扩大高质量指导对话来增强聊天语言模型。在 *2023 年自然语言处理实证方法会议论文集，EMNLP
    2023，新加坡，2023 年 12 月 6-10 日*，第 3029–3051 页。计算语言学协会。
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2023. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人 (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu 和 Zhifang Sui. 2023. 关于上下文学习的调查。*arXiv 预印本 arXiv:2301.00234*。
- en: Fayyad et al. (1996) Usama M. Fayyad, Gregory Piatetsky-Shapiro, and Padhraic
    Smyth. 1996. From data mining to knowledge discovery in databases. *AI Mag.*,
    17(3):37–54.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fayyad 等人 (1996) Usama M. Fayyad, Gregory Piatetsky-Shapiro 和 Padhraic Smyth.
    1996. 从数据挖掘到数据库中的知识发现。*AI Mag.*, 17(3):37–54。
- en: 'Gao et al. (2023) Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian,
    Bolin Ding, and Jingren Zhou. 2023. Text-to-sql empowered by large language models:
    A benchmark evaluation. *arXiv preprint arXiv:2308.15363*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人 (2023) Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin
    Ding 和 Jingren Zhou. 2023. 由大型语言模型赋能的文本到 SQL: 基准评估。*arXiv 预印本 arXiv:2308.15363*。'
- en: 'Gu et al. (2023) Yu Gu, Xiang Deng, and Yu Su. 2023. Don’t generate, discriminate:
    A proposal for grounding language models to real-world environments. In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 4928–4949\.
    Association for Computational Linguistics.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. (2023) Yu Gu, Xiang Deng, 和 Yu Su. 2023. 不要生成，辨别: 为语言模型在现实世界环境中建立基础的提议。收录于
    *第61届计算语言学协会年会论文集（第1卷：长篇论文），ACL 2023，2023年7月9-14日，多伦多，加拿大*，第4928–4949页。计算语言学协会。'
- en: 'Gu et al. (2024) Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang,
    Jayanth Srinivasa, Hugo Latapie, and Yu Su. 2024. Middleware for llms: Tools are
    instrumental for language agents in complex environments. *arXiv preprint arXiv:2402.14672*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu et al. (2024) Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang,
    Jayanth Srinivasa, Hugo Latapie, 和 Yu Su. 2024. Middleware for llms: 工具对于复杂环境中的语言代理至关重要。*arXiv
    预印本 arXiv:2402.14672*。'
- en: 'Guo et al. (2021) Jiaqi Guo, Ziliang Si, Yu Wang, Qian Liu, Ming Fan, Jian-Guang
    Lou, Zijiang Yang, and Ting Liu. 2021. Chase: A large-scale and pragmatic chinese
    dataset for cross-database context-dependent text-to-sql. In *Proceedings of the
    59th Annual Meeting of the Association for Computational Linguistics and the 11th
    International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,
    (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pages 2316–2331\. Association
    for Computational Linguistics.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. (2021) Jiaqi Guo, Ziliang Si, Yu Wang, Qian Liu, Ming Fan, Jian-Guang
    Lou, Zijiang Yang, 和 Ting Liu. 2021. Chase: 一个大规模和务实的中文数据集，用于跨数据库上下文相关的文本到SQL。收录于
    *第59届计算语言学协会年会和第11届国际联合自然语言处理会议论文集，ACL/IJCNLP 2021，（第1卷：长篇论文），虚拟会议，2021年8月1-6日*，第2316–2331页。计算语言学协会。'
- en: 'Han et al. (2011) Jiawei Han, Micheline Kamber, and Jian Pei. 2011. *Data Mining:
    Concepts and Techniques, 3rd edition*. Morgan Kaufmann.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2011) Jiawei Han, Micheline Kamber, 和 Jian Pei. 2011. *数据挖掘：概念与技术，第3版*。Morgan
    Kaufmann。
- en: 'He et al. (2023) Xinyi He, Mengyu Zhou, Xinrun Xu, Xiaojun Ma, Rui Ding, Lun
    Du, Yan Gao, Ran Jia, Xu Chen, Shi Han, et al. 2023. Text2analysis: A benchmark
    of table question answering with advanced data analysis and unclear queries. *arXiv
    preprint arXiv:2312.13671*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2023) Xinyi He, Mengyu Zhou, Xinrun Xu, Xiaojun Ma, Rui Ding, Lun
    Du, Yan Gao, Ran Jia, Xu Chen, Shi Han, 等. 2023. Text2analysis: 具有高级数据分析和模糊查询的表格问答基准。*arXiv
    预印本 arXiv:2312.13671*。'
- en: 'Hu et al. (2024) Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang,
    Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, et al. 2024. Infiagent-dabench:
    Evaluating agents on data analysis tasks. *arXiv preprint arXiv:2401.05507*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2024) Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Guoyin Wang,
    Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, 等. 2024. Infiagent-dabench:
    在数据分析任务上评估代理。*arXiv 预印本 arXiv:2401.05507*。'
- en: 'Huang and Chang (2023) Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards
    reasoning in large language models: A survey. In *Findings of the Association
    for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023*, pages
    1049–1065\. Association for Computational Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang and Chang (2023) Jie Huang 和 Kevin Chen-Chuan Chang. 2023. Towards reasoning
    in large language models: A survey. 收录于 *计算语言学协会发现：ACL 2023，2023年7月9-14日，多伦多，加拿大*，第1049–1065页。计算语言学协会。'
- en: Huang et al. (2023a) Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi R Fung,
    Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, and Heng Ji. 2023a. Do lvlms understand
    charts? analyzing and correcting factual errors in chart captioning. *arXiv preprint
    arXiv:2312.10160*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023a) Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi R Fung,
    Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, 和 Heng Ji. 2023a. Do lvlms understand
    charts? 分析和纠正图表说明中的事实错误。*arXiv 预印本 arXiv:2312.10160*。
- en: 'Huang et al. (2023b) Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu,
    Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, et al. 2023b.
    Metatool benchmark for large language models: Deciding whether to use tools and
    which to use. *arXiv preprint arXiv:2310.03128*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2023b) Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu,
    Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, 等. 2023b. Metatool
    benchmark for large language models: 决定是否使用工具及使用哪些工具。*arXiv 预印本 arXiv:2310.03128*。'
- en: Jain et al. (2023) Naman Jain, Tianjun Zhang, Wei-Lin Chiang, Joseph E Gonzalez,
    Koushik Sen, and Ion Stoica. 2023. Llm-assisted code cleaning for training accurate
    code generators. *arXiv preprint arXiv:2311.14904*.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain et al. (2023) Naman Jain, Tianjun Zhang, Wei-Lin Chiang, Joseph E Gonzalez,
    Koushik Sen, 和 Ion Stoica. 2023. Llm-assisted code cleaning for training accurate
    code generators. *arXiv 预印本 arXiv:2311.14904*。
- en: Khanbabaei et al. (2018) Mohammad Khanbabaei, Farzad Movahedi Sobhani, Mahmood
    Alborzi, and Reza Radfar. 2018. Developing an integrated framework for using data
    mining techniques and ontology concepts for process improvement. *J. Syst. Softw.*,
    137:78–95.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khanbabaei et al. (2018) Mohammad Khanbabaei, Farzad Movahedi Sobhani, Mahmood
    Alborzi, 和 Reza Radfar. 2018. 开发一个集成框架，结合数据挖掘技术和本体概念用于过程改进。*J. Syst. Softw.*，137:78–95。
- en: 'Lai et al. (2023) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida I. Wang, and Tao Yu.
    2023. DS-1000: A natural and reliable benchmark for data science code generation.
    In *International Conference on Machine Learning, ICML 2023, 23-29 July 2023,
    Honolulu, Hawaii, USA*, volume 202 of *Proceedings of Machine Learning Research*,
    pages 18319–18345\. PMLR.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. (2023) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida I. Wang, 和 Tao Yu. 2023.
    DS-1000: 一项自然且可靠的数据科学代码生成基准测试。在*2023年国际机器学习大会，ICML 2023，2023年7月23-29日，夏威夷檀香山，美国*，第202卷的*机器学习研究会议论文集*，第18319–18345页。PMLR。'
- en: 'Lei et al. (2023) Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao,
    and Kang Liu. 2023. S3eval: A synthetic, scalable, systematic evaluation suite
    for large language models. *arXiv preprint arXiv:2310.15147*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lei et al. (2023) Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao,
    和 Kang Liu. 2023. S3eval: 一套合成、可扩展的系统化大型语言模型评估工具。*arXiv预印本 arXiv:2310.15147*。'
- en: 'Li et al. (2024a) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2024a. Camel: Communicative agents for" mind" exploration
    of large language model society. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024a) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    和 Bernard Ghanem. 2024a. Camel: 用于“大脑”探索大型语言模型社会的沟通代理。*神经信息处理系统进展*，36。'
- en: 'Li et al. (2024b) Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang,
    Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. 2024b. Codes: Towards
    building open-source language models for text-to-sql. *arXiv preprint arXiv:2402.16347*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024b) Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang,
    Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, 和 Hong Chen. 2024b. Codes: 朝着构建开源语言模型进行文本到SQL的目标。*arXiv预印本
    arXiv:2402.16347*。'
- en: 'Li et al. (2024c) Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and ZHAO-XIANG
    ZHANG. 2024c. Sheetcopilot: Bringing software productivity to the next level through
    large language models. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024c) Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, 和 ZHAO-XIANG
    ZHANG. 2024c. Sheetcopilot: 通过大型语言模型提升软件生产力的下一个水平。*神经信息处理系统进展*，36。'
- en: Li et al. (2023a) Jinyang Li, Binyuan Hui, GE QU, Jiaxi Yang, Binhua Li, Bowen
    Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang
    Li, Kevin Chang, Fei Huang, Reynold Cheng, and Yongbin Li. 2023a. Can LLM already
    serve as a database interface? a BIg bench for large-scale database grounded text-to-SQLs.
    In *Thirty-seventh Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Jinyang Li, Binyuan Hui, GE QU, Jiaxi Yang, Binhua Li, Bowen
    Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang
    Li, Kevin Chang, Fei Huang, Reynold Cheng, 和 Yongbin Li. 2023a. LLM是否已经可以作为数据库接口？一个针对大规模数据库基础文本到SQL的BIg基准。在*第37届神经信息处理系统会议数据集和基准测试分会*。
- en: 'Li et al. (2023b) Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li,
    Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023b. Api-bank: A comprehensive
    benchmark for tool-augmented llms. In *Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
    2023*, pages 3102–3116\. Association for Computational Linguistics.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023b) Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li,
    Haiyang Yu, Zhoujun Li, Fei Huang, 和 Yongbin Li. 2023b. Api-bank: 一项综合性基准测试，针对工具增强的LLMs。在*2023年自然语言处理经验方法会议，EMNLP
    2023，新加坡，2023年12月6-10日*，第3102–3116页。计算语言学协会。'
- en: 'Li et al. (2023c) Ruosen Li, Teerth Patel, and Xinya Du. 2023c. Prd: Peer rank
    and discussion improve large language model based evaluations. *arXiv preprint
    arXiv:2307.02762*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023c) Ruosen Li, Teerth Patel, 和 Xinya Du. 2023c. Prd: 同行排名和讨论改进基于大型语言模型的评估。*arXiv预印本
    arXiv:2307.02762*。'
- en: Li et al. (2023d) Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang,
    Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et al. 2023d. One
    shot learning as instruction data prospector for large language models. *arXiv
    preprint arXiv:2312.10302*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023d) Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang,
    Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, 等. 2023d. 一次性学习作为大型语言模型的指令数据探测器。*arXiv预印本
    arXiv:2312.10302*。
- en: 'Liu et al. (2023a) Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno,
    Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier,
    and Yasemin Altun. 2023a. Deplot: One-shot visual language reasoning by plot-to-table
    translation. In *Findings of the Association for Computational Linguistics: ACL
    2023, Toronto, Canada, July 9-14, 2023*, pages 10381–10399\. Association for Computational
    Linguistics.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023a) 方宇·刘、朱利安·马丁·艾森施洛斯、弗朗切斯科·皮钦诺、塞琳·克里申、陈曦·庞、肯顿·李、曼达尔·乔希、温虎·陈、奈杰尔·科利尔
    和 雅塞敏·阿尔顿. 2023a. Deplot: 通过情节到表格翻译进行单次视觉语言推理. 收录于 *计算语言学协会成果：ACL 2023，多伦多，加拿大，2023年7月9日-14日*，第10381–10399页\.
    计算语言学协会.'
- en: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023b) 肖·刘、浩·余、汉臣·张、艺凡·徐、轩宇·雷、汉宇·赖、余·顾、杭良·丁、凯文·门、克娟·杨 等. 2023b. Agentbench:
    评估大语言模型作为代理. *arXiv 预印本 arXiv:2308.03688*.'
- en: 'Lu et al. (2023) Bo-Ru Lu, Nikita Haduong, Chia-Hsuan Lee, Zeqiu Wu, Hao Cheng,
    Paul Koester, Jean Utke, Tao Yu, Noah A Smith, and Mari Ostendorf. 2023. Dialgen:
    collaborative human-lm generated dialogues for improved understanding of human-human
    conversations. *arXiv preprint arXiv:2307.07047*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu 等 (2023) 博如·陆、尼基塔·哈杜昂、贾轩·李、泽秋·吴、浩·程、保罗·科斯特、简·乌特克、陶·于、诺亚·A·史密斯 和 玛丽·奥斯滕多夫.
    2023. Dialgen: 协作式人类-语言模型生成对话以改善对人类对话的理解. *arXiv 预印本 arXiv:2307.07047*.'
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th Annual ACM Symposium
    on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA,
    29 October 2023- 1 November 2023*, pages 2:1–2:22\. ACM.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2023) 俊生·朴、约瑟夫·C·奥布莱恩、卡莉·君·蔡、梅雷迪思·林戈尔·莫里斯、佩尔西·梁 和 迈克尔·S·伯恩斯坦. 2023.
    生成代理：人类行为的互动模拟. 收录于 *第36届年度 ACM 用户界面软件与技术研讨会，UIST 2023，加州旧金山，2023年10月29日-11月1日*，第2:1–2:22页\.
    ACM.
- en: 'Pourreza and Rafiei (2024a) Mohammadreza Pourreza and Davood Rafiei. 2024a.
    Din-sql: Decomposed in-context learning of text-to-sql with self-correction. *Advances
    in Neural Information Processing Systems*, 36.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pourreza 和 Rafiei (2024a) 穆罕默德雷扎·普尔雷扎 和 达沃德·拉菲埃. 2024a. Din-sql: 具自我修正的文本到
    SQL 的分解上下文学习. *神经信息处理系统进展*, 36.'
- en: 'Pourreza and Rafiei (2024b) Mohammadreza Pourreza and Davood Rafiei. 2024b.
    Dts-sql: Decomposed text-to-sql with small large language models. *arXiv preprint
    arXiv:2402.01117*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pourreza 和 Rafiei (2024b) 穆罕默德雷扎·普尔雷扎 和 达沃德·拉菲埃. 2024b. Dts-sql: 使用小型大语言模型的分解文本到
    SQL. *arXiv 预印本 arXiv:2402.01117*.'
- en: Qin et al. (2023) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding,
    Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023. Tool learning
    with foundation models. *arXiv preprint arXiv:2304.08354*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2023) 余佳·秦、盛丁·胡、彦凯·林、韦泽·陈、宁·丁、甘渠·崔、珍妮·曾、余飞·黄、超军·肖、迟汉 等. 2023. 基于基础模型的工具学习.
    *arXiv 预印本 arXiv:2304.08354*.
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere 等 (2023) 巴普蒂斯特·罗济耶、乔纳斯·格林、法比安·格勒克、斯滕·苏特拉、伊泰·加特、肖青·艾伦·谭、约西·阿迪、静瑜·刘、塔尔·瑞梅兹、杰雷米·拉潘
    等. 2023. Code llama: 用于代码的开放基础模型. *arXiv 预印本 arXiv:2308.12950*.'
- en: 'Si et al. (2023) Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin,
    Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, and Yongbin Li. 2023. SpokenWOZ: A
    large-scale speech-text benchmark for spoken task-oriented dialogue agents. In
    *Thirty-seventh Conference on Neural Information Processing Systems Datasets and
    Benchmarks Track*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Si 等 (2023) 书正·思、文涛·马、浩宇·高、宇川·吴、婷婷·林、银佩·戴、杭宇·李、瑞·阎、飞·黄 和 永彬·李. 2023. SpokenWOZ:
    用于口语任务导向对话代理的大规模语音-文本基准. 收录于 *第37届神经信息处理系统会议 数据集与基准跟踪*.'
- en: 'Wang et al. (2023a) Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi
    Bai, Qian-Wen Zhang, Zhao Yan, and Zhoujun Li. 2023a. Mac-sql: Multi-agent collaboration
    for text-to-sql. *arXiv preprint arXiv:2312.11242*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 (2023a) 冰·王、昌宇·任、简·杨、新年·梁、佳琪·白、千问·张、赵·燕 和 周俊·李. 2023a. Mac-sql: 用于文本到
    SQL 的多代理协作. *arXiv 预印本 arXiv:2312.11242*.'
- en: 'Wang et al. (2024) Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan
    Yuan, Hao Peng, and Heng Ji. 2024. MINT: Evaluating LLMs in multi-turn interaction
    with tools and language feedback. In *The Twelfth International Conference on
    Learning Representations*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024) Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan
    Yuan, Hao Peng, 和 Heng Ji. 2024. MINT: Evaluating LLMs in multi-turn interaction
    with tools and language feedback. 见于 *The Twelfth International Conference on
    Learning Representations*。'
- en: 'Wang et al. (2023b) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui,
    Junnan Li, and Steven CH Hoi. 2023b. Codet5+: Open code large language models
    for code understanding and generation. *arXiv preprint arXiv:2305.07922*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023b) Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui,
    Junnan Li, 和 Steven CH Hoi. 2023b. Codet5+: Open code large language models for
    code understanding and generation. *arXiv preprint arXiv:2305.07922*。'
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837。
- en: 'Xie et al. (2023) Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng,
    Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. 2023. Openagents:
    An open platform for language agents in the wild. *arXiv preprint arXiv:2310.10634*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. (2023) Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng,
    Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, 等. 2023. Openagents:
    An open platform for language agents in the wild. *arXiv preprint arXiv:2310.10634*。'
- en: 'Xu et al. (2023a) Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong
    Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. 2023a. Gentopia.ai:
    A collaborative platform for tool-augmented llms. In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System
    Demonstrations, Singapore, December 6-10, 2023*, pages 237–245\. Association for
    Computational Linguistics.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023a) Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong
    Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, 和 Dongkuan Xu. 2023a. Gentopia.ai: A
    collaborative platform for tool-augmented llms. 见于 *Proceedings of the 2023 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations,
    Singapore, December 6-10, 2023*，第237–245页。计算语言学协会。'
- en: 'Xu et al. (2023b) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. 2023b. Lemur: Harmonizing
    natural language and code for language agents. *arXiv preprint arXiv:2310.06830*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023b) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, 等. 2023b. Lemur: Harmonizing
    natural language and code for language agents. *arXiv preprint arXiv:2310.06830*。'
- en: 'Yan et al. (2023) Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau
    Yih, and Ziyu Yao. 2023. Learning to simulate natural language feedback for interactive
    semantic parsing. In *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
    July 9-14, 2023*, pages 3149–3170\. Association for Computational Linguistics.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan et al. (2023) Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau
    Yih, 和 Ziyu Yao. 2023. Learning to simulate natural language feedback for interactive
    semantic parsing. 见于 *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
    July 9-14, 2023*，第3149–3170页。计算语言学协会。'
- en: Yang et al. (2023) Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang,
    and Yongbin Li. 2023. Iterative forward tuning boosts in-context learning in language
    models. *arXiv preprint arXiv:2305.13016*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang,
    和 Yongbin Li. 2023. Iterative forward tuning boosts in-context learning in language
    models. *arXiv preprint arXiv:2305.13016*。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting
    in language models. In *The Eleventh International Conference on Learning Representations,
    ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, 和 Yuan Cao. 2023. React: Synergizing reasoning and acting
    in language models. 见于 *The Eleventh International Conference on Learning Representations,
    ICLR 2023, Kigali, Rwanda, May 1-5, 2023*。OpenReview.net。'
- en: Yao et al. (2020) Ziyu Yao, Yiqi Tang, Wen-tau Yih, Huan Sun, and Yu Su. 2020.
    An imitation game for learning semantic parsers from user interaction. In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2020, Online, November 16-20, 2020*, pages 6883–6902\. Association for Computational
    Linguistics.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2020) Ziyu Yao, Yiqi Tang, Wen-tau Yih, Huan Sun, 和 Yu Su. 2020.
    An imitation game for learning semantic parsers from user interaction. 见于 *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2020, Online, November 16-20, 2020*，第6883–6902页。计算语言学协会。
- en: 'Yin et al. (2023) Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming
    Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski,
    Oleksandr Polozov, and Charles Sutton. 2023. Natural language to code generation
    in interactive data science notebooks. In *Proceedings of the 61st Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers), ACL
    2023, Toronto, Canada, July 9-14, 2023*, pages 126–173\. Association for Computational
    Linguistics.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2023）尹鹏程、李文丁、肖克凡、拉奥·阿比谢克、温业明、石肯森、豪兰德·乔舒亚、贝利·佩奇、卡塔斯塔·米歇尔、米哈洛夫斯基·亨里克、波洛佐夫·奥列克桑德、萨顿·查尔斯。
    2023. 交互式数据科学笔记本中的自然语言到代码生成。载于 *第61届计算语言学协会年会论文集（第1卷：长篇论文），ACL 2023，加拿大多伦多，2023年7月9-14日*，第126–173页。计算语言学协会。
- en: 'Yu et al. (2019a) Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang,
    Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro
    Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao Chen, Yuwen
    Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S. Lasecki,
    and Dragomir R. Radev. 2019a. Cosql: A conversational text-to-sql challenge towards
    cross-domain natural language interfaces to databases. In *Proceedings of the
    2019 Conference on Empirical Methods in Natural Language Processing and the 9th
    International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019,
    Hong Kong, China, November 3-7, 2019*, pages 1962–1979\. Association for Computational
    Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2019a）陶宇、张锐、二海洋、李素怡、薛磊、庞博、林希·维多利亚、陈怡、施天泽、李子涵、姜有轩、安田通浩、沈胜录、陈涛、法布里·亚历山大、李子凡、陈露瑶、张宇文、迪克希特·施瑞亚、张文彦、熊才明、索切尔·理查德、拉塞基·沃尔特·S、拉德戈米尔·R·。
    2019a. Cosql: 一个面向跨领域自然语言接口到数据库的对话式文本到SQL挑战。载于 *2019年自然语言处理实证方法会议与第9届国际自然语言处理联合会议论文集，EMNLP-IJCNLP
    2019，中国香港，2019年11月3-7日*，第1962–1979页。计算语言学协会。'
- en: 'Yu et al. (2018) Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang,
    Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R.
    Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain
    semantic parsing and text-to-sql task. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, Brussels, Belgium, October
    31 - November 4, 2018*, pages 3911–3921\. Association for Computational Linguistics.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2018）陶宇、张锐、杨凯、安田通浩、王栋旭、李子凡、马詹姆斯、李艾琳、姚青宁、罗曼·香农、张子琳、拉德戈米尔·R·。 2018. Spider:
    大规模人工标注数据集用于复杂和跨领域的语义解析和文本到SQL任务。载于 *2018年自然语言处理实证方法会议论文集，比利时布鲁塞尔，2018年10月31日-11月4日*，第3911–3921页。计算语言学协会。'
- en: 'Yu et al. (2019b) Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria
    Lin, Suyi Li, Heyang Er, Irene Li, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit,
    David Proctor, Sungrok Shim, Jonathan Kraft, Vincent Zhang, Caiming Xiong, Richard
    Socher, and Dragomir R. Radev. 2019b. Sparc: Cross-domain semantic parsing in
    context. In *Proceedings of the 57th Conference of the Association for Computational
    Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long
    Papers*, pages 4511–4523\. Association for Computational Linguistics.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2019b）陶宇、张锐、安田通浩、陈怡、林希·维多利亚、李素怡、二海洋、李艾琳、庞博、陈涛、吉艾米莉、迪克希特·施瑞亚、普罗克特·大卫、沈胜录、克拉夫特·乔纳森、张文彦、熊才明、索切尔·理查德、拉德戈米尔·R·。
    2019b. Sparc: 跨领域上下文语义解析。载于 *第57届计算语言学协会会议论文集，ACL 2019，意大利佛罗伦萨，2019年7月28日-8月2日，第1卷：长篇论文*，第4511–4523页。计算语言学协会。'
- en: 'Zan et al. (2022) Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang,
    and Jian-Guang Lou. 2022. When language model meets private library. In *Findings
    of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United
    Arab Emirates, December 7-11, 2022*, pages 277–288\. Association for Computational
    Linguistics.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zan 等（2022）赞道广、陈贝、林泽琪、关贝、王永基、楼剑光。 2022. 当语言模型遇上私人库。载于 *计算语言学协会：EMNLP 2022研究结果，阿布扎比，阿联酋，2022年12月7-11日*，第277–288页。计算语言学协会。
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等（2023）曾奥寒、刘铭道、卢睿、王博文、刘晓、董宇晓、唐洁。 2023. Agenttuning: 使LLMs具备通用代理能力。*arXiv预印本
    arXiv:2310.12823*。'
- en: 'Zha et al. (2023) Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang,
    Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, et al. 2023. Tablegpt:
    Towards unifying tables, nature language and commands into one gpt. *arXiv preprint
    arXiv:2307.08674*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zha et al. (2023) Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang,
    Saisai Yang, Jing Yuan, Changbao Su, Xiang Li, Aofeng Su, et al. 2023. Tablegpt:
    Towards unifying tables, nature language and commands into one gpt. *arXiv 预印本
    arXiv:2307.08674*。'
- en: 'Zhang et al. (2023a) Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang.
    2023a. Data-copilot: Bridging billions of data and humans with autonomous workflow.
    *arXiv preprint arXiv:2306.07209*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang.
    2023a. Data-copilot: Bridging billions of data and humans with autonomous workflow.
    *arXiv 预印本 arXiv:2306.07209*。'
- en: 'Zhang et al. (2023b) Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon,
    Shaleen Deep, and Jignesh M Patel. 2023b. Reactable: Enhancing react for table
    question answering. *arXiv preprint arXiv:2310.00815*.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023b) Yunjia Zhang, Jordan Henkel, Avrilia Floratou, Joyce Cahoon,
    Shaleen Deep, and Jignesh M Patel. 2023b. Reactable: Enhancing react for table
    question answering. *arXiv 预印本 arXiv:2310.00815*。'
- en: 'Zhang et al. (2023c) Zhehao Zhang, Xitao Li, Yan Gao, and Jian-Guang Lou. 2023c.
    CRT-QA: A dataset of complex reasoning question answering over tabular data. In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*,
    pages 2131–2153, Singapore. Association for Computational Linguistics.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023c) Zhehao Zhang, Xitao Li, Yan Gao, and Jian-Guang Lou. 2023c.
    CRT-QA: A dataset of complex reasoning question answering over tabular data. In
    *2023年自然语言处理实证方法会议论文集*，第2131-2153页，新加坡。计算语言学协会。'
- en: 'Zhang et al. (2023d) Zhehao Zhang, Xitao Li, Yan Gao, and Jian-Guang Lou. 2023d.
    CRT-QA: A dataset of complex reasoning question answering over tabular data. In
    *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,
    EMNLP 2023, Singapore, December 6-10, 2023*, pages 2131–2153\. Association for
    Computational Linguistics.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023d) Zhehao Zhang, Xitao Li, Yan Gao, and Jian-Guang Lou. 2023d.
    CRT-QA: A dataset of complex reasoning question answering over tabular data. In
    *2023年自然语言处理实证方法会议论文集，EMNLP 2023，新加坡，2023年12月6-10日*，第2131-2153页。计算语言学协会。'
- en: Zheng et al. (2024a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2024a. Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural
    Information Processing Systems*, 36.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024a) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2024a. Judging llm-as-a-judge with mt-bench and chatbot arena. *神经信息处理系统进展*, 36。
- en: 'Zheng et al. (2024b) Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen
    Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024b. Opencodeinterpreter: Integrating
    code generation with execution and refinement. *arXiv preprint arXiv:2402.14658*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2024b) Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill
    Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024b. Opencodeinterpreter: Integrating
    code generation with execution and refinement. *arXiv 预印本 arXiv:2402.14658*。'
- en: Appendix A License
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 许可证
- en: A.1 Tapilot-Crossing
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 Tapilot-Crossing
- en: Our Tapilot-Crossing data is available under the lisense CC BY NC 4.0.^§^§§[https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Tapilot-Crossing 数据在许可 CC BY NC 4.0 下可用。^§^§§[https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)
- en: A.2 Kaggle Tabular Data
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 Kaggle 表格数据
- en: 'The tabular data that we used to create Tapilot-Crossing are following open-source
    licenses: 1) Public Domain: Public Domain Mark 2) CC-BY: Creative Commons Attribution
    4.0 International.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于创建 Tapilot-Crossing 的表格数据遵循开源许可协议：1）公有领域：公有领域标记 2）CC-BY：创作共用署名 4.0 国际版。
- en: Appendix B Dynamic History Combination
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 动态历史组合
- en: B.1 History Relational Database (H-RDB)
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 历史关系数据库 (H-RDB)
- en: 'We split the User-AI interaction into several single-turn user queries and
    AI answers stored in a relational database, indexed by the conversational order
    as shown in figure [6](#A2.F6 "Figure 6 ‣ B.1 History Relational Database (H-RDB)
    ‣ Appendix B Dynamic History Combination ‣ Tapilot-Crossing: Benchmarking and
    Evolving LLMs Towards Interactive Data Analysis Agents"). This storage is subject
    to dynamic combinations for different scenarios.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将用户与 AI 的互动拆分为多个单轮用户查询和 AI 回答，并存储在关系数据库中，按对话顺序索引，如图[6](#A2.F6 "图6 ‣ B.1 历史关系数据库
    (H-RDB) ‣ 附录B 动态历史组合 ‣ Tapilot-Crossing: 基准测试和进化 LLMs 以实现互动数据分析代理")所示。此存储可以根据不同场景进行动态组合。'
- en: '![Refer to caption](img/588a9da8a84e3b396af7e976da49f79f.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/588a9da8a84e3b396af7e976da49f79f.png)'
- en: 'Figure 6: The screenshot of History Relational Database (H-RDB).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：历史关系数据库（H-RDB）的屏幕截图。
- en: B.2 History Retrieval Queries
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 历史检索查询
- en: 'When retrieving the stored history information, we use sqlite3^¶^¶¶[https://docs.python.org/3/library/sqlite3.html](https://docs.python.org/3/library/sqlite3.html)
    python package. The search query is provided in sqlite3 format, for example: SELECT
    {Prompt} FROM {table} WHERE 1=1 AND Domain = ? AND ...'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索存储的历史信息时，我们使用 sqlite3^¶^¶¶[https://docs.python.org/3/library/sqlite3.html](https://docs.python.org/3/library/sqlite3.html)
    Python 包。搜索查询以 sqlite3 格式提供，例如：SELECT {Prompt} FROM {table} WHERE 1=1 AND Domain
    = ? AND ...
- en: B.3 Tapilot-Alpha
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 Tapilot-Alpha
- en: 'As stated in Section [9](#S9 "9 Limitations ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents"), the current Tapilot-Crossing
    involves only clean and accurate code revisions, which we refer to as the Alpha
    version. Looking ahead, we are considering the incorporation of noisy data or
    the integration of user interactions in Action mode into the code history. This
    potential expansion aims to simulate more realistic development environments and
    challenges. Even within the constraints of a curated and error free interaction
    history, the experimental results show that there still are substantial opportunities
    for optimization and improvements.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '正如第[9](#S9 "9 Limitations ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs
    Towards Interactive Data Analysis Agents")节中所述，目前的 Tapilot-Crossing 仅涉及干净且准确的代码修订，我们称之为
    Alpha 版本。展望未来，我们考虑将噪声数据或在 Action 模式下的用户交互集成到代码历史中。这一潜在扩展旨在模拟更真实的开发环境和挑战。即使在精心策划且无误的交互历史的限制下，实验结果仍显示存在大量优化和改进的机会。'
- en: Appendix C Dialog Types
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 对话类型
- en: Tapilot-Crossing can be categorized into Statement- (longer) and Colloquial-
    (shorter) dialogs. The statement-dialogs are more formal, resulting in more complex
    user instructions and code generations, which are commonly found in computational
    notebooks (Yin et al., [2023](#bib.bib53)). On the other hand, colloquial dialogs
    involve shorter and simpler user questions, but exhibit more colloquial and interactive
    characteristics. This category of dialogs is primarily constructed through the
    process of prompting GPT-4 to segment and reinterpret the existing statement-dialogs.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Tapilot-Crossing 可分为 Statement-（较长）和 Colloquial-（较短）对话。Statement-对话较为正式，导致更复杂的用户指令和代码生成，这些通常出现在计算笔记本中（Yin
    等，[2023](#bib.bib53)）。另一方面，colloquial 对话涉及更短和更简单的用户问题，但表现出更多的口语化和互动特征。这类对话主要通过提示
    GPT-4 对现有的 Statement-对话进行分段和重新解释来构建。
- en: Appendix D Air Implementation
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D Air 实现
- en: '![Refer to caption](img/48551e87c648151f8e6f29e43c687b19.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/48551e87c648151f8e6f29e43c687b19.png)'
- en: 'Figure 7: This is an overview of our proposed method, Air. The areas highlighted
    in purple represent results generated by the agents.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：这是我们提出的方法 Air 的概览。图中用紫色突出显示的区域表示由智能体生成的结果。
- en: 'Figure [7](#A4.F7 "Figure 7 ‣ Appendix D Air Implementation ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") presents
    the detailed steps of Air.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#A4.F7 "Figure 7 ‣ Appendix D Air Implementation ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") 展示了
    Air 的详细步骤。'
- en: Appendix E Agent Implementation
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 智能体实现
- en: E.1 Toolkit
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 工具包
- en: Executor
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 执行器
- en: To get the execution results of code generated by LLMs, we adopt Python Executor
    exec() which is implemented in Python ^∥^∥∥[https://docs.python.org/3/library/functions.html#exec](https://docs.python.org/3/library/functions.html#exec),
    within a isolated Python environment. The output of the code execution, whether
    it be any return values, print statements, or error messages, is then captured
    by the Executor. This output is subsequently returned to the LMs, providing them
    with feedback on the results of their code generation to make a better next-step
    action or decision.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取 LLM 生成的代码执行结果，我们采用 Python Executor exec()，该方法在 Python ^∥^∥∥[https://docs.python.org/3/library/functions.html#exec](https://docs.python.org/3/library/functions.html#exec)
    中实现，运行于隔离的 Python 环境中。代码执行的输出，无论是返回值、打印语句还是错误信息，均由 Executor 捕获。然后，将这些输出返回给 LMs，向其提供关于代码生成结果的反馈，以便进行更好的下一步操作或决策。
- en: User Simulator
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用户模拟器
- en: In addressing the clarification action type, LLMs are permitted to request clarification
    when they feel ambigous about conditions from user queires. Therefore, we employ
    GPT-4 Turbo to emulate the user’s question-answering behavior, considering that
    GPT-4 has been demonstrated to provide feedback of equivalent quality to human
    responses (Wang et al., [2024](#bib.bib43)).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理澄清操作类型时，当 LLM 对用户查询中的条件感到模糊时，被允许请求澄清。因此，我们使用 GPT-4 Turbo 来模拟用户的问答行为，因为 GPT-4
    已被证明能提供与人类回应相当的反馈 (Wang et al., [2024](#bib.bib43))。
- en: Chart-to-Table
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图表转表格
- en: We employ deplot (Liu et al., [2023a](#bib.bib33)) to convert images into a
    table. Given the table, then LLMs can reason and answer the questions.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用 deplot (Liu et al., [2023a](#bib.bib33)) 将图像转换为表格。给定表格后，LLMs 可以推理并回答问题。
- en: E.2 Reasoning
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 推理
- en: COT
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: COT
- en: 'To evaluate the pure code generalization capability of data analysis, we restrict
    LLMs from executing code during generation. Therefore, we employ a zero-shot COT
    for the reasoning of the Agent mode. The key prompt to implement such COT is:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数据分析的纯代码泛化能力，我们限制 LLM 在生成期间执行代码。因此，我们对 Agent 模式的推理采用零-shot COT。这种 COT 的关键提示是：
- en: '... write a step-by-step outline and then write the code:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '... 编写逐步大纲，然后编写代码：'
- en: ReAct
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ReAct
- en: To evaluate analytical capabilities beyond mere code generation, we employ ReAct
    for multiple-choice questions. Specifically, we set the MAX STEP for ReAct reasoning
    to 5, with the Executor serving as the primary tool. Data analysis agents are
    tasked to generate, analyze, and draw conclusions about their results. If the
    result contains bugs, the corresponding message is returned to the agent for rectification,
    although this process may consume additional reasoning steps. We also manually
    provide a one-shot example to guide agents on how to react in Tapilot-Crossing.
    To prevent data leakage, we cross-reference examples across different tabular
    data. For instance, an example curated from ATP_Tennis could be used to guide
    LLMs in the Laptop Pricing dataset.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估超越简单代码生成的分析能力，我们采用 ReAct 来处理多项选择题。具体来说，我们将 ReAct 推理的 MAX STEP 设置为 5，Executor
    作为主要工具。数据分析代理负责生成、分析并对结果进行结论。如果结果包含错误，相应的消息将返回给代理进行修正，尽管此过程可能会消耗额外的推理步骤。我们还手动提供了一个示例来指导代理如何在
    Tapilot-Crossing 中反应。为了防止数据泄露，我们在不同的表格数据中交叉参考示例。例如，来自 ATP_Tennis 的示例可以用来指导 LLM
    在 Laptop Pricing 数据集中。
- en: Appendix F Implementation Details
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 实施细节
- en: F.1 General Implementation
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 一般实施
- en: The temperature parameter is set to 0.0 for Claude 2.1, GPT-4, and GPT-4-Turbo.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 温度参数设置为 0.0 用于 Claude 2.1、GPT-4 和 GPT-4-Turbo。
- en: Appendix G Action Description
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 操作描述
- en: In this section, we categorize and formalize the action types in Tapilot-Crossing,
    identifying five distinct sub-categories that correspond to different types of
    user queries.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们对 Tapilot-Crossing 中的操作类型进行分类和正式化，识别出五种不同的子类别，对应于不同类型的用户查询。
- en: G.1 Update_Code
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 Update_Code
- en: The Update_Code action refers to instances where the user requests corrections
    for bugs or refinements to the conditions of previous queries.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Update_Code 操作指的是用户请求对先前查询中的错误或条件进行修正的情况。
- en: G.2 Fast_Fail
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 快速失败
- en: Fast_Fail is an action that alerts users when the current data contents or resources
    are insufficient to meet their requests, or when user queries contain factual
    errors.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 快速失败是一种操作，用于在当前数据内容或资源不足以满足用户请求时，或用户查询包含事实错误时提醒用户。
- en: G.3 Clarification
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.3 澄清
- en: Clarification is a common action in response to under-specified questions, which
    are frequent in data-analysis queries. In this action, agents make the conditions
    of the question more specific and clear by seeking additional information from
    users.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 澄清是在响应不完全指定的问题时常见的一种操作，这在数据分析查询中很常见。在此操作中，代理通过向用户寻求额外信息，使问题的条件更加具体和明确。
- en: G.4 Best_Guess
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.4 最佳猜测
- en: While Clarification is an effective action to reduce the uncertainty, it can
    lead to issues such as user impatience due to unsteadily asking, and long dialog
    histories that result in attention distraction and long-context problems. Therefore,
    the Best_Guess action can address these issues by making appropriate assumptions
    based on data contents, domain knowledge, and commonsense knowledge for under-specific
    questions. However, there is also a risk that incorrect guesses can lead to hallucinations.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然**Clarification**是一种有效减少不确定性的行动，但它可能导致如用户因反复询问而不耐烦以及长对话历史导致注意力分散和长上下文问题等问题。因此，**Best_Guess**动作可以通过基于数据内容、领域知识和常识知识对不明确的问题做出适当假设来解决这些问题。然而，不正确的猜测也有可能导致幻觉。  '
- en: G.5 Plot_QA
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.5 **Plot_QA**
- en: In real data analysis settings, agents are also expected to answer user questions
    about insights derived from plots. The Plot_QA action can assist users in better
    understanding the contents of plots for decision making.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际的数据分析环境中，代理还需要回答用户关于从图表中得出的见解的问题。**Plot_QA**动作可以帮助用户更好地理解图表内容以进行决策。  '
- en: G.6 Insight_Mining
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'G.6 **洞察挖掘**  '
- en: Beyond generating codes for users to retrieve expected results, code agents
    are also tasked with summarizing executed results from the environment to assist
    users in making informed decisions. This process, known as Insight_Mining, plays
    an important role in data analysis since it contributes to the evolution of code
    agents into comprehensive data analysis agents.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '除了生成代码以供用户获取预期结果外，代码代理还负责总结环境中执行的结果，以帮助用户做出明智的决策。这个过程被称为**洞察挖掘**，在数据分析中扮演着重要角色，因为它有助于将代码代理发展为全面的数据分析代理。  '
- en: Appendix H Evaluation Metric Details
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录H 评估指标详情  '
- en: H.1 DataFrame Comparison
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'H.1 数据框比较  '
- en: The function compares two dataframes (df_1 and df_2) by checking their indices,
    column presence, and column data. It uses np.allclose() for numeric data and direct
    comparison for non-numeric data. If a column in df_1 is absent in the original
    dataframe, it searches for a matching column in df_2. The function returns True
    if df_1 and df_2 are equivalent, otherwise False. Please note, the column names
    will not be computed since different LLMs may have their only preference names.
    For example, the win_ratio generated by GPT-4 could be called winning ratio by
    Claude 2.1.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '该函数通过检查索引、列的存在性和列数据来比较两个数据框（df_1和df_2）。它对数值数据使用`np.allclose()`，对非数值数据进行直接比较。如果df_1中的列在原始数据框中不存在，它会在df_2中寻找匹配的列。该函数返回True如果df_1和df_2等效，否则返回False。请注意，列名不会被计算，因为不同的LLMs可能有自己唯一的名称。例如，GPT-4生成的win_ratio可能被Claude
    2.1称为winning ratio。  '
- en: H.2 Visualization Comparison
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'H.2 可视化比较  '
- en: We note that it’s hard to compare the closed-form results for visualization-based
    code generation since parameters of plots may be varied significant across models.
    For instance, GPT-4 generated plots may be the same with CodeLlama while their
    title names may be different, which leads to false negatives. Therefore we utilize
    PIL package to compute similarity between plots. To be specific, the function
    compare_plots takes two image file paths as inputs (ai_output and reference_output),
    resizes them to 800x600 pixels using the LANCZOS method, and saves them. The images
    are then read in grayscale mode to avoid the difference brought by colors. The
    function computes and returns the Structural Similarity Index (SSIM), a measure
    of image similarity, between the two images. This function can be used to compare
    an AI model’s output with a reference output. Finally, the code generated will
    be considered as correct if the similarity is larger than 0.6.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '我们注意到，基于可视化的代码生成的封闭形式结果比较困难，因为不同模型中的图表参数可能有显著差异。例如，GPT-4生成的图表可能与CodeLlama相同，但其标题名称可能不同，这会导致假阴性。因此，我们利用PIL包计算图表之间的相似性。具体来说，函数`compare_plots`将两个图像文件路径（ai_output和reference_output）作为输入，使用LANCZOS方法将它们调整为800x600像素，并保存它们。然后以灰度模式读取这些图像，以避免颜色带来的差异。该函数计算并返回两个图像之间的结构相似性指数（SSIM），这是衡量图像相似性的指标。这个函数可以用来比较AI模型的输出与参考输出。最后，如果相似度大于0.6，生成的代码将被认为是正确的。  '
- en: H.3 Multi-Intent Evaluation
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'H.3 多意图评估  '
- en: In this work, we evaluate the code generation performance on intent manner,
    which means if one user query contains multiple intents, then the total scores
    of this query will be the number of intents. We evaluate each intent separately
    and sum up the scores of all intents as the denominator when calculate the performance
    of each model in percentage.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们评估代码生成性能的意图方式，这意味着如果一个用户查询包含多个意图，则该查询的总分将是意图的数量。我们分别评估每个意图，并将所有意图的分数相加作为计算每个模型百分比性能时的分母。
- en: H.4 Private Function Recall
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.4 私有函数召回率
- en: 'We notice that some LLMs tend to import as many as possible private functions
    while not using all of them. Thus, to extract all indeed used private functions
    in the customized function library, we utilize AST package. After extracting the
    used private functions, we calculate the recall coefficient according to Equation
    [3](#S4.E3 "In Acc with Private Lib Recall (AccR). ‣ 4.2 Evaluation Metrics ‣
    4 Data Statistics & Metrics ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs
    Towards Interactive Data Analysis Agents").'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '我们注意到一些 LLM 倾向于导入尽可能多的私有函数，而并未使用其中的全部。因此，为了提取定制函数库中确实使用的私有函数，我们利用 AST 包。提取已使用的私有函数后，我们根据公式
    [3](#S4.E3 "在带私有库召回率（AccR）中的 Acc。 ‣ 4.2 评估指标 ‣ 4 数据统计与指标 ‣ Tapilot-Crossing: 基准测试与不断发展中的
    LLM 互动数据分析代理") 计算召回系数。'
- en: H.5 Code Similarity Equivalance (CSE)
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.5 代码相似性等价（CSE）
- en: In the context of Tapilot-Crossing, the complexity of code generation tasks—many
    of which yield a score of zero—presents huge challenges in evaluating performance
    through Acc or AccR only. This is particularly evident when distinguishing between
    codes that differ by merely a single line of error or output, both of which would
    result in an Acc or AccR of zero, despite their obvious differences in code generation
    capabilities. To overcome this limitation, we propose the introduction of Code
    Similarity Equivalence (CSE), a nuanced evaluation metric designed to assess the
    similarity between generated codes and reference codes. Given that these codes
    originate based on identical user instructions, a high degree of similarity is
    expected. Our approach leverages a hybrid combination of models to reduce the
    bias, incorporating CodeT5++ and OpenAI Ada (text-embedding-ada-002) models, which
    are affordable and available for most institutes. This combination has demonstrated
    a strong correlation with human evaluative preferences, offering a more refined
    and accurate measure of code generation performance.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Tapilot-Crossing 的背景下，代码生成任务的复杂性——其中许多任务的评分为零——在仅通过 Acc 或 AccR 评估性能时面临巨大的挑战。这在区分仅因一行错误或输出不同的代码时尤为明显，尽管它们在代码生成能力上有明显差异，但都会导致
    Acc 或 AccR 为零。为了克服这一限制，我们提出了代码相似性等价（CSE）的引入，这是一种细致的评估指标，旨在评估生成代码与参考代码之间的相似性。由于这些代码基于相同的用户指令生成，预计会有很高的相似度。我们的方法利用了模型的混合组合，以减少偏见，包括
    CodeT5++ 和 OpenAI Ada（text-embedding-ada-002）模型，这些模型经济实惠，大多数机构均可使用。这种组合与人类评估偏好表现出强相关性，提供了更精细和准确的代码生成性能衡量。
- en: Details.
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 详细信息。
- en: We introduce here about how to conduct more nuanced evaluation of Acc or AccR
    with CSE. 1) We collect 180 instances of code generation including both Normal
    and Private. To evaluation the quality of these codes, we enlist two additional
    PhD students who are proficient in data science and Python as evaluation committee.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此介绍如何进行更细致的 Acc 或 AccR 的 CSE 评估。1) 我们收集了 180 个代码生成实例，包括普通和私有代码。为了评估这些代码的质量，我们招募了两名精通数据科学和
    Python 的博士生作为评估委员会。
- en: 2) They evaluate code generated by several models, including GPT-4-32k, GPT-4-Turbo,
    Claude-2.1, CodeLlama-Instruct (ranging from 7B to 34B parameters), StarCoder,
    and DeepSeek-Coder-Instruct (also from 7B to 34B parameters). Each evaluator is
    provided with comprehensive user code histories, tabular contents, the current
    query, access to the decision_company private library. Please note that evaluations
    are conducted only based on their expertise and experience, without any predefined
    guidelines and discussion, to avoid bias.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 他们评估了由多个模型生成的代码，包括 GPT-4-32k、GPT-4-Turbo、Claude-2.1、CodeLlama-Instruct（参数范围从
    7B 到 34B）、StarCoder 和 DeepSeek-Coder-Instruct（同样从 7B 到 34B）。每个评估者都提供了全面的用户代码历史、表格内容、当前查询以及访问
    decision_company 私有库。请注意，评估仅基于其专业知识和经验进行，没有任何预定义的指导方针和讨论，以避免偏见。
- en: 3) We ask for a relative ranking of generated codes among models over absolute
    scoring to avoid potential variability in scoring preferences among the evaluators.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 我们要求对生成的代码进行相对排名，而非绝对评分，以避免评估人员在评分偏好上的潜在变异。
- en: 4) In cases of parts of divergent rankings, the evaluators engage in discussions
    regarding the specific code samples until a consensus was reached. This step ensures
    a more reliable and agreed-upon evaluation outcome.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 在存在不同排名部分的情况下，评估人员会讨论具体的代码样本，直到达成共识。这一步骤确保了评估结果的更可靠和一致。
- en: 5) The evaluation committee then examine various open-source and readily available
    embedding models to measure code similarity, aiming to closely match their ranking
    preferences. Our exploration identifies that the score system consisting of CodeT5+
    (Wang et al., [2023b](#bib.bib44)) and Ada (text-embedding-ada-002) most closely
    aligned with human evaluative preferences.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 评估委员会随后检查各种开源和现成的嵌入模型以测量代码相似度，旨在尽可能接近他们的排名偏好。我们的探索发现，由CodeT5+（Wang et al.,
    [2023b](#bib.bib44)）和Ada（text-embedding-ada-002）组成的评分系统与人类评估偏好最为接近。
- en: Introduction of a Mixed Evaluation Metric (AccSE & AccSER).
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 引入了一种混合评估指标（AccSE & AccSER）。
- en: 'To accurately reflect the nuanced capabilities of code generation models, we
    propose a composite metric that integrates Code Similarity Evaluation (CSE) with
    Accuracy (Acc), termed Accuracy for Similarity Evaluation (AccSE). This metric
    is concisely defined as:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确反映代码生成模型的细微能力，我们提出了一种综合指标，将代码相似度评估（CSE）与准确度（Acc）结合起来，称为相似度评估的准确度（AccSE）。该指标简明地定义为：
- en: '|  | $$\text{AccSE}=\begin{cases}1.0,&amp;\text{if }C=\hat{C},\\ 0.5,&amp;\text{if
    }S_{1}>0.85\land S_{2}>0.9,\\'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\text{AccSE}=\begin{cases}1.0,&amp;\text{如果 }C=\hat{C},\\ 0.5,&amp;\text{如果
    }S_{1}>0.85\land S_{2}>0.9,\\'
- en: 0.25,&amp;\text{if }(S_{1}>0.85\land S_{2}\leq 0.9)\\
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 0.25，&amp;\text{如果 }(S_{1}>0.85\land S_{2}\leq 0.9)\\
- en: '&amp;\lor(S_{1}\leq 0.85\land S_{2}>0.9),\\'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\lor(S_{1}\leq 0.85\land S_{2}>0.9)，\\'
- en: 0,&amp;\text{otherwise.}\end{cases}$$ |  | (4) |
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 0，&amp;\text{否则。}\end{cases}$$ |  | (4) |
- en: 'Where:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $C$ represent the reference and generated code execution outcomes, respectively.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $C$ 分别表示参考代码和生成代码的执行结果。
- en: •
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $S_{1}$ denotes the CSE score based on CodeT5+.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S_{1}$ 表示基于CodeT5+的CSE分数。
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $S_{2}$ denotes the CSE score based on Ada.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S_{2}$ 表示基于Ada的CSE分数。
- en: 'This formulation succinctly captures the evaluation criteria for AccSE, with
    symbols $S_{1}$ are used for "and" and "or" conditions, respectively, to further
    compact the notation. AccSER is computed in the similar way just times recall
    score for each value as Eq. [3](#S4.E3 "In Acc with Private Lib Recall (AccR).
    ‣ 4.2 Evaluation Metrics ‣ 4 Data Statistics & Metrics ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents").'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '这种表述简明地捕捉了AccSE的评估标准，符号 $S_{1}$ 用于“和”与“或”条件，以进一步简化符号表示。AccSER 以类似方式计算，只是将每个值的召回分数乘以
    Eq. [3](#S4.E3 "在 Acc 与私人库召回 (AccR). ‣ 4.2 评估指标 ‣ 4 数据统计与指标 ‣ Tapilot-Crossing:
    基准测试与发展 LLMs 以实现交互式数据分析代理")。'
- en: We hold this for future evaluation system of Tapilot-Crossing when we conduct
    more extensive cases with involved with more expert volunteers.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其作为未来Tapilot-Crossing评估系统的基础，当我们进行更多涉及专家志愿者的广泛案例时。
- en: Rationale Against GPT-4-Based and Multi-Agent Evaluation Methods.
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 反对GPT-4基础和多代理评估方法的理由。
- en: 'While existing research suggests that GPT-4-based soft evaluation could enhance
    the assessment of complex generative tasks, such approaches are deemed unsuitable
    for Tapilot-Crossing due to several critical reasons:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有研究表明，基于GPT-4的软评估可以增强对复杂生成任务的评估，但由于几个关键原因，这些方法被认为不适用于Tapilot-Crossing：
- en: '1) Bias Concerns: The prototype annotations and questions in our study originate
    from a GPT-4-based agent environment. Employing GPT-4 for evaluation purposes
    could inadvertently introduce a self-enhancement bias (Zheng et al., [2024a](#bib.bib64)),
    compromising fairness across model evaluations.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 偏差问题：我们研究中的原型注释和问题来自于GPT-4环境。使用GPT-4进行评估可能无意中引入自我增强偏差（Zheng et al., [2024a](#bib.bib64)），影响模型评估的公平性。
- en: '2) Cost Concerns: Although multi-agent evaluation frameworks, incorporating
    diverse families of Large Language Models (LLMs), is to mitigate bias (Li et al.,
    [2023c](#bib.bib31)), the economical and computational overhead is obvious. Specifically,
    evaluations in such settings require at least twice the token consumption than
    that used in generation alone, rendering it impractically expensive in Tapilot-Crossing.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 成本考虑：尽管多代理评估框架，包含不同系列的大型语言模型（LLMs），旨在减少偏差（Li 等，[2023c](#bib.bib31)），但经济和计算开销是显而易见的。具体来说，这种设置下的评估需要至少是单独生成所需的两倍令牌消耗，这在
    Tapilot-Crossing 中显得不切实际。
- en: Given these considerations, our research proposes an alternative evaluation
    methodology that is both cost-effective and reliable for evaluating the accuracy
    of complex data science code generation at this time. We demonstrate that CodeT5+,
    a remarkably efficient code embedding model, can obviously distinguish between
    varying performance levels and accurately identify correct code logic. Crucially,
    this model offers a pragmatic balance between evaluation thoroughness and resource
    efficiency.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些考虑，我们的研究提出了一种替代评估方法，该方法在当前阶段对于评估复杂数据科学代码生成的准确性既具有成本效益又可靠。我们证明了 CodeT5+，一个显著高效的代码嵌入模型，可以明显区分不同的性能水平并准确识别正确的代码逻辑。至关重要的是，这个模型在评估彻底性和资源效率之间提供了一个务实的平衡。
- en: H.6 Other Value Types
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.6 其他值类型
- en: For other result types, such as dictionry, set, list, we directly compute the
    exectued results and determine whether they are equal or not.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他结果类型，如字典、集合、列表，我们直接计算执行结果并确定它们是否相等。
- en: H.7 Case-by-Case Evaluation
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.7 按案例评估
- en: While we categorize instances according to result types and provide evaluation
    codes for each type, some scenarios requires a case-by-case evaluation script.
    For instance, in most dataframe or matrix comparisons, we employ np.close() and
    string match for result comparison. However, in some cases, such as using a dataframe
    or matrix to display a classifier’s Confusion Matrix, the predicted code is deemed
    correct if its f1-score surpasses that of the referenced code, even if their f1-scores
    are not similar. For the evaluation script of Tapilot-Crossing, we manually review
    and adjust the scripts to accommodate each case.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们根据结果类型对实例进行分类，并为每种类型提供评估代码，但一些场景需要逐个案例的评估脚本。例如，在大多数数据框或矩阵比较中，我们使用 np.close()
    和字符串匹配进行结果比较。然而，在某些情况下，例如使用数据框或矩阵显示分类器的混淆矩阵，如果预测代码的 f1 分数超过参考代码，即使它们的 f1 分数不相似，预测代码也被认为是正确的。对于
    Tapilot-Crossing 的评估脚本，我们手动审查和调整脚本以适应每个案例。
- en: Appendix I Action Evaluation Mode
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 行动评估模式
- en: I.1 Correction
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.1 修正
- en: Update_Code
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新_代码
- en: This could be evaluated within a static setting where the bug feedback is embedded
    into user-code history. Agents are requried to update the previous code via user
    feedback.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在一个静态设置中进行评估，其中错误反馈被嵌入到用户代码历史中。代理需要通过用户反馈来更新先前的代码。
- en: I.2 Unawserable
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.2 无法回答
- en: Fast_Fail
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 快速_失败
- en: In Decision Company, we keep the original unanswerable questions and categorize
    them as multi-choice questions. This is done to evaluate if agents can identify
    these questions based on their analysis of table contents and commonsense knowledge.
    To prevent any biased setting, such as specially designed prompts that might mislead
    agents into determining a question as unanswerable, we sample an equal number
    of under-specified problems and answerable questions. We then reformulate their
    choices, enabling the model to decide whether a question is answerable with clarification
    or assumption, or to directly classify it as unanswerable.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策公司，我们保留原始的无法回答的问题，并将它们归类为多项选择题。这是为了评估代理是否可以根据对表格内容和常识知识的分析来识别这些问题。为了防止任何偏见设置，例如可能误导代理将问题确定为无法回答的特别设计提示，我们抽取了数量相等的未指定问题和可回答问题。然后，我们重新制定其选项，使模型能够决定一个问题是否可以通过澄清或假设来回答，或直接将其分类为无法回答。
- en: I.3 Under_Specific
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.3 特定不足
- en: Clarification
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 澄清
- en: To evaluate the performance of agents on clarification action, we employ a dynamic
    setting that incorporates a User Simulator. This simulator mimics user feedback
    based on the ground truth code or answer. Initially, interactive data analysis
    agents are expected to pose questions for clarification, simulator will answer
    it according to the ground truth answers. Subsequently, these agents are tasked
    with generating the final code, understanding both the original history and the
    history of clarifications. This setup provides a robust assessment of the agents’
    ability to interact, clarify ambiguities, and generate accurate code.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估代理在澄清动作上的表现，我们采用了一个动态设置，结合了用户模拟器。这个模拟器根据真实答案或代码模拟用户反馈。最初，互动数据分析代理被期望提出澄清问题，模拟器将根据真实答案进行回答。随后，这些代理负责生成最终代码，理解原始历史和澄清历史。这个设置为代理的互动、澄清模糊性和生成准确代码的能力提供了强有力的评估。
- en: Best_Guess
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Best_Guess
- en: We aim to evaluate the ability of interactive data analysis agents to make accurate
    assumptions when faced with ambiguous questions, without resorting to constant
    clarification, which could potentially frustrate users. We believe that an agent’s
    best guess should not impact the final decision and this evaluation metric should
    be somehow flexible. For instance, in a credit card application scenario, the
    term young people could refer to individuals aged 20-40 or 25-45, making it challenging
    to be evaluated by fixed metrics. Therefore, we opt to use multiple-choice questions
    to assess the agents’ assumption-making capabilities. We posit that an assumption
    is appropriate only if it does not influence the final decision-making process.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在评估互动数据分析代理在面对模糊问题时做出准确假设的能力，而不依赖于不断的澄清，这可能会让用户感到沮丧。我们认为，代理的最佳猜测不应影响最终决定，而这个评估指标应该在某种程度上具有灵活性。例如，在信用卡申请场景中，"年轻人"一词可能指的是20-40岁或25-45岁的人，这使得固定指标难以评估。因此，我们选择使用多项选择题来评估代理的假设能力。我们认为，假设只有在不影响最终决策过程时才是适当的。
- en: I.4 Visualization
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.4 可视化
- en: Plot_QA
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Plot_QA
- en: We evaluate the analysis capability of agents around plot in Tapilot-Crossing.
    The end format of answer would be multiple choices.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Tapilot-Crossing中评估代理在情节方面的分析能力。答案的最终格式为多项选择。
- en: I.5 Analysis
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.5 分析
- en: Insight_Mining
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Insight_Mining
- en: We evaluate the analysis capability of agents generally in Tapilot-Crossing.
    We opt to use multi-choice questions to evaluate it.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Tapilot-Crossing中通常评估代理的分析能力。我们选择使用多项选择题来进行评估。
- en: Appendix J Decision Company Prompt
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J 决策公司提示
- en: '![Refer to caption](img/993d4115bd9141a30aaa695c17bd1042.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/993d4115bd9141a30aaa695c17bd1042.png)'
- en: 'Figure 8: The prompt of Client Persona Generation'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：客户端角色生成的提示
- en: '![Refer to caption](img/033f0d14cc879c01fccfb7d07a5df83f.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/033f0d14cc879c01fccfb7d07a5df83f.png)'
- en: 'Figure 9: The prompt of analysis scenario generation'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：分析场景生成的提示
- en: '![Refer to caption](img/5e25e93733e9438c8cf4ee1b76312b3e.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5e25e93733e9438c8cf4ee1b76312b3e.png)'
- en: 'Figure 10: The example of plan discussion. The final output should be a plan
    of analysis invovlving questions and their or result types.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：计划讨论的示例。最终输出应为涉及问题及其或结果类型的分析计划。
- en: '![Refer to caption](img/2b5b444ef3cf4c0ed6409991b4b4745b.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2b5b444ef3cf4c0ed6409991b4b4745b.png)'
- en: 'Figure 11: The prompt of Data Science Agent in interaction log generation.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：数据科学代理在互动日志生成中的提示
- en: '![Refer to caption](img/68ed8d6480ca1b51b346e741c5dc3b8b.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/68ed8d6480ca1b51b346e741c5dc3b8b.png)'
- en: 'Figure 12: The prompt of Chatbot Agent in interaction log generation.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：聊天机器人代理在互动日志生成中的提示
- en: '![Refer to caption](img/356eb5543b8697c94f4e10838ac6d163.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/356eb5543b8697c94f4e10838ac6d163.png)'
- en: 'Figure 13: The prompt of Conversion from prototype code towards the code with
    private libraries.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：从原型代码到包含私有库代码的转换提示
- en: J.1 Client Persona Generation
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: J.1 客户角色生成
- en: 'The Figure [8](#A10.F8 "Figure 8 ‣ Appendix J Decision Company Prompt ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") describes
    how we prompt Administrator agent to generate meaningful personas.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '图[8](#A10.F8 "图8 ‣ 附录J 决策公司提示 ‣ Tapilot-Crossing: 基准测试和发展面向互动数据分析代理的LLMs")描述了我们如何提示管理员代理生成有意义的角色。'
- en: J.2 Simulation of Analysis Scenarios
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: J.2 分析场景模拟
- en: 'The Figure [9](#A10.F9 "Figure 9 ‣ Appendix J Decision Company Prompt ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") shows
    how we create diverse scenarios via In-Context Learning (ICL).'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '图[9](#A10.F9 "图 9 ‣ 附录 J 决策公司提示 ‣ Tapilot-Crossing: 基准测试和演进 LLM 以实现互动数据分析代理")
    显示了我们如何通过上下文学习（ICL）创建多样化的场景。'
- en: J.3 Plan Discussion
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: J.3 计划讨论
- en: 'The Figure [10](#A10.F10 "Figure 10 ‣ Appendix J Decision Company Prompt ‣
    Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis
    Agents") presents how conversation between Data Scientist agent and Client agent
    can generate the series of analysis plans.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '图[10](#A10.F10 "图 10 ‣ 附录 J 决策公司提示 ‣ Tapilot-Crossing: 基准测试和演进 LLM 以实现互动数据分析代理")
    展示了数据科学家代理与客户代理之间的对话如何生成一系列分析计划。'
- en: J.4 Interaction Log Annotation
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: J.4 互动日志注释
- en: 'The prompt to drive interaction log annotation is presented by Figure [11](#A10.F11
    "Figure 11 ‣ Appendix J Decision Company Prompt ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") and Figure [12](#A10.F12
    "Figure 12 ‣ Appendix J Decision Company Prompt ‣ Tapilot-Crossing: Benchmarking
    and Evolving LLMs Towards Interactive Data Analysis Agents") from the view of
    Data Scientist agent and Chatbot agent, respectively.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '驱动互动日志注释的提示由图[11](#A10.F11 "图 11 ‣ 附录 J 决策公司提示 ‣ Tapilot-Crossing: 基准测试和演进
    LLM 以实现互动数据分析代理") 和图[12](#A10.F12 "图 12 ‣ 附录 J 决策公司提示 ‣ Tapilot-Crossing: 基准测试和演进
    LLM 以实现互动数据分析代理") 从数据科学家代理和聊天机器人代理的角度分别呈现。'
- en: J.5 Private Lib Evolution
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: J.5 私有库演进
- en: 'Figure [13](#A10.F13 "Figure 13 ‣ Appendix J Decision Company Prompt ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") shows
    the framework of how to prompt GPT-4 to generate code automatically. The human
    efforts are introduced to reduce the bias and fix errors.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '图[13](#A10.F13 "图 13 ‣ 附录 J 决策公司提示 ‣ Tapilot-Crossing: 基准测试和演进 LLM 以实现互动数据分析代理")
    显示了如何提示 GPT-4 自动生成代码的框架。引入了人工努力以减少偏差并修复错误。'
- en: Appendix K Implementation Prompt
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 K 实现提示
- en: K.1 Code Generation
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.1 代码生成
- en: 'The Figure [14](#A11.F14 "Figure 14 ‣ K.1 Code Generation ‣ Appendix K Implementation
    Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents") describes how we prompt LLM model to generate code to answer
    user queries. And Figure [15](#A11.F15 "Figure 15 ‣ K.1 Code Generation ‣ Appendix
    K Implementation Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards
    Interactive Data Analysis Agents") describes how we prompt LLM in Agent to generate
    code to answer user queries following with chain-of-thought Wei et al. ([2022](#bib.bib45)).
    Finally, Figure [16](#A11.F16 "Figure 16 ‣ K.1 Code Generation ‣ Appendix K Implementation
    Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents") describes how we prompt LLM in Inter-Agent to generate
    code to answer user queries with our proposed AIR.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '图[14](#A11.F14 "图 14 ‣ K.1 代码生成 ‣ 附录 K 实现提示 ‣ Tapilot-Crossing: 基准测试和演进 LLM
    以实现互动数据分析代理") 描述了我们如何提示 LLM 模型生成代码以回答用户查询。图[15](#A11.F15 "图 15 ‣ K.1 代码生成 ‣ 附录
    K 实现提示 ‣ Tapilot-Crossing: 基准测试和演进 LLM 以实现互动数据分析代理") 描述了我们如何在 Agent 中提示 LLM 生成代码以回答用户查询，遵循
    Wei 等人（[2022](#bib.bib45)）的思路链。最后，图[16](#A11.F16 "图 16 ‣ K.1 代码生成 ‣ 附录 K 实现提示
    ‣ Tapilot-Crossing: 基准测试和演进 LLM 以实现互动数据分析代理") 描述了我们如何在 Inter-Agent 中提示 LLM 生成代码以回答用户查询，使用我们提出的
    AIR。'
- en: '![Refer to caption](img/381e8a834842f87e8641bdddac6b14f8.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/381e8a834842f87e8641bdddac6b14f8.png)'
- en: 'Figure 14: The prompt of LLM in Model-Base version in Code Generation mode.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：LLM 在模型基础版本中的代码生成模式提示。
- en: '![Refer to caption](img/7d7d53b7d51a5c6314c514f284950593.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/7d7d53b7d51a5c6314c514f284950593.png)'
- en: 'Figure 15: The prompt of LLM with data analysis agent in Code Generation mode.
    The COT prompt text is in red color.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：LLM 在代码生成模式下与数据分析代理的提示。COT 提示文本为红色。
- en: '![Refer to caption](img/1e10962b1ef92fe45ecbb4851b0e4b21.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1e10962b1ef92fe45ecbb4851b0e4b21.png)'
- en: 'Figure 16: The prompt of LLM with interactive data analysis agent in Code Generation
    mode. The AIR prompt text are in green color, which are generated by LLM itself
    by learning from successful history.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：LLM 在代码生成模式下与互动数据分析代理的提示。AIR 提示文本为绿色，由 LLM 自身根据成功历史生成。
- en: K.2 Multi-choice
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.2 多项选择
- en: 'The Figure [17](#A11.F17 "Figure 17 ‣ K.2 Multi-choice ‣ Appendix K Implementation
    Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents") describes how we prompt LLM to answer user queries. And
    Figure [18](#A11.F18 "Figure 18 ‣ K.2 Multi-choice ‣ Appendix K Implementation
    Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
    Data Analysis Agents") describes how we prompt LLM in Agent to answer user queries
    following with ReAct Yao et al. ([2023](#bib.bib51)). Finally, Figure [19](#A11.F19
    "Figure 19 ‣ K.2 Multi-choice ‣ Appendix K Implementation Prompt ‣ Tapilot-Crossing:
    Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents") describes
    how we prompt LLM w/ Inter-Agent to answer user queries with our proposed AIR.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [17](#A11.F17 "图 17 ‣ K.2 多选题 ‣ 附录 K 实施提示 ‣ Tapilot-Crossing: 基准测试和发展 LLM
    以实现交互式数据分析代理") 描述了我们如何提示 LLM 来回答用户查询。图 [18](#A11.F18 "图 18 ‣ K.2 多选题 ‣ 附录 K 实施提示
    ‣ Tapilot-Crossing: 基准测试和发展 LLM 以实现交互式数据分析代理") 描述了我们如何在 Agent 中提示 LLM 以回答用户查询，遵循
    ReAct Yao 等 ([2023](#bib.bib51))。最后，图 [19](#A11.F19 "图 19 ‣ K.2 多选题 ‣ 附录 K 实施提示
    ‣ Tapilot-Crossing: 基准测试和发展 LLM 以实现交互式数据分析代理") 描述了我们如何通过 Inter-Agent 提示 LLM 使用我们提出的
    AIR 来回答用户查询。'
- en: '![Refer to caption](img/a74760fde97307f2c7a925d70f4b20d6.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a74760fde97307f2c7a925d70f4b20d6.png)'
- en: 'Figure 17: The prompt of LLM in Model-Base version in Multi-choice mode.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：在多选模式下的 Model-Base 版本的 LLM 提示。
- en: '![Refer to caption](img/85acb8b916ae5a41ba60b6b3a578a918.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/85acb8b916ae5a41ba60b6b3a578a918.png)'
- en: 'Figure 18: The prompt of LLM with data analysis agent in Multi-choice mode.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：在多选模式下，具有数据分析代理的 LLM 提示。
- en: '![Refer to caption](img/b99214d8a1fd5d8781485b48a0905325.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b99214d8a1fd5d8781485b48a0905325.png)'
- en: 'Figure 19: The prompt of LLM with interactive data analysis agent in Multi-choice
    mode. The AIR prompt text are in green color. And the pseudocode is generated
    by LLM itself by learning from successful history'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：在多选模式下，具有交互式数据分析代理的 LLM 提示。AIR 提示文本为绿色。伪代码由 LLM 自身通过学习成功的历史生成。
- en: K.3 Clarification Action
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K.3 澄清行动
- en: 'The Figure [20](#A11.F20 "Figure 20 ‣ K.3 Clarification Action ‣ Appendix K
    Implementation Prompt ‣ Tapilot-Crossing: Benchmarking and Evolving LLMs Towards
    Interactive Data Analysis Agents") describes how we prompt LLM in Model-Base version
    to ask for clarification.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [20](#A11.F20 "图 20 ‣ K.3 澄清行动 ‣ 附录 K 实施提示 ‣ Tapilot-Crossing: 基准测试和发展 LLM
    以实现交互式数据分析代理") 描述了我们如何在 Model-Base 版本中提示 LLM 请求澄清。'
- en: '![Refer to caption](img/6ee749c1c07090e33d913b05ac48cdcd.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6ee749c1c07090e33d913b05ac48cdcd.png)'
- en: 'Figure 20: The prompt of LLM in Model-Base version in clarification mode.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：在澄清模式下的 Model-Base 版本的 LLM 提示。
