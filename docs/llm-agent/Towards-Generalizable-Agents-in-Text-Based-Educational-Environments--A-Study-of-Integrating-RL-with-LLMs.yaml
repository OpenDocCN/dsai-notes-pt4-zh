- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Towards Generalizable Agents in Text-Based Educational Environments: A Study
    of Integrating RL with LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝向通用化代理的文本教育环境：RL与LLMs集成的研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18978](https://ar5iv.labs.arxiv.org/html/2404.18978)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.18978](https://ar5iv.labs.arxiv.org/html/2404.18978)
- en: Bahar Radmehr
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 巴哈尔·拉德梅赫
- en: Adish Singla
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 阿迪什·辛格拉
- en: Tanja Käser EPFL [bahar.radmehr@epfl.ch](mailto:bahar.radmehr@epfl.ch) MPI-SWS
    [adishs@mpi-sws.org](mailto:adishs@mpi-sws.org) EPFL [tanja.kaeser@epfl.ch](mailto:tanja.kaeser@epfl.ch)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌学者 EPFL [bahar.radmehr@epfl.ch](mailto:bahar.radmehr@epfl.ch) MPI-SWS [adishs@mpi-sws.org](mailto:adishs@mpi-sws.org)
    EPFL [tanja.kaeser@epfl.ch](mailto:tanja.kaeser@epfl.ch)
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'There has been a growing interest in developing learner models to enhance learning
    and teaching experiences in educational environments. However, existing works
    have primarily focused on structured environments relying on meticulously crafted
    representations of tasks, thereby limiting the agent’s ability to generalize skills
    across tasks. In this paper, we aim to enhance the generalization capabilities
    of agents in open-ended text-based learning environments by integrating Reinforcement
    Learning (RL) with Large Language Models (LLMs). We investigate three types of
    agents: (i) RL-based agents that utilize natural language for state and action
    representations to find the best interaction strategy, (ii) LLM-based agents that
    leverage the model’s general knowledge and reasoning through prompting, and (iii)
    hybrid LLM-assisted RL agents that combine these two strategies to improve agents’
    performance and generalization. To support the development and evaluation of these
    agents, we introduce PharmaSimText, a novel benchmark derived from the PharmaSim
    virtual pharmacy environment designed for practicing diagnostic conversations.
    Our results show that RL-based agents excel in task completion but lack in asking
    quality diagnostic questions. In contrast, LLM-based agents perform better in
    asking diagnostic questions but fall short of completing the task. Finally, hybrid
    LLM-assisted RL agents enable us to overcome these limitations, highlighting the
    potential of combining RL and LLMs to develop high-performing agents for open-ended
    learning environments.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发学习者模型以提升教育环境中的学习和教学体验的兴趣日益增长。然而，现有的工作主要集中在依赖于精心设计的任务表示的结构化环境中，从而限制了代理在任务之间通用技能的能力。在本文中，我们旨在通过将强化学习（RL）与大型语言模型（LLMs）相结合，增强代理在开放式文本学习环境中的泛化能力。我们调查了三种类型的代理：（i）利用自然语言进行状态和动作表示的基于RL的代理，以寻找最佳互动策略，（ii）通过提示利用模型的通用知识和推理的基于LLM的代理，以及（iii）结合这两种策略以提升代理性能和泛化能力的混合LLM辅助RL代理。为了支持这些代理的发展和评估，我们引入了PharmaSimText，这是一个从PharmaSim虚拟药房环境中衍生出的新基准，旨在练习诊断对话。我们的结果显示，基于RL的代理在任务完成方面表现优异，但在提出高质量诊断问题方面欠缺。相比之下，基于LLM的代理在提出诊断问题方面表现更好，但在完成任务方面有所不足。最后，混合LLM辅助RL代理使我们能够克服这些限制，突显了将RL和LLMs结合用于开发高性能代理在开放式学习环境中的潜力。
- en: 'keywords:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Reinforcement Learning, Large Language Models, Text-Based Educational Environments,
    Learner Models
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习、大型语言模型、基于文本的教育环境、学习者模型
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Learner models are foundational to the advancement of educational technologies,
    serving as a versatile tool for a multitude of applications that enhance both
    teaching and learning experiences [[1](#bib.bib1)]. By simulating the interactions
    and data of students, these computational models provide a safe and controlled
    environment for teacher training, allowing educators to refine their methods without
    direct implications on actual students [[2](#bib.bib2)]. They also facilitate
    the development and evaluation of adaptive learning systems [[3](#bib.bib3)] or
    new algorithms [[4](#bib.bib4)]. Furthermore, they have been applied for testing
    theories of learning [[5](#bib.bib5)] and foster collaboration skills in students
    through interacting with virtual peers [[6](#bib.bib6)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者模型是教育技术发展的基础，作为一种多功能工具，增强了教学和学习体验[[1](#bib.bib1)]。通过模拟学生的互动和数据，这些计算模型为教师培训提供了一个安全和受控的环境，使教育者能够在不直接影响实际学生的情况下优化其方法[[2](#bib.bib2)]。它们还促进了自适应学习系统[[3](#bib.bib3)]或新算法[[4](#bib.bib4)]的发展和评估。此外，它们还用于测试学习理论[[5](#bib.bib5)]，并通过与虚拟同伴互动来促进学生的合作技能[[6](#bib.bib6)]。
- en: Reinforcement learning (RL) offers a promising avenue for developing these learner
    models/agents [[7](#bib.bib7)]. Existing works on RL for educational domains have
    primarily focused on developing techniques for curriculum optimization [[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)], providing tailored hints
    and feedback [[12](#bib.bib12), [13](#bib.bib13)], and generating educational
    content [[14](#bib.bib14), [15](#bib.bib15)]. Only a limited number of works have
    explored the use of RL-based learner agents that effectively operate in the learning
    environments [[16](#bib.bib16), [17](#bib.bib17)]. However, these RL-based learner
    agents have been studied for structured tasks with well-defined rules, such as
    mathematics and logic puzzles. In such environments, RL’s capabilities are naturally
    exploited due to the straightforward definition of state and action representations
    using engineered features obtained from the existing structure [[7](#bib.bib7),
    [18](#bib.bib18), [16](#bib.bib16)]. However, the reliance on hand-crafted features
    and engineered state representations limits the ability of these RL agents to
    be used in unstructured domains and to generalize their learned skills and knowledge
    across different tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）为开发这些学习者模型/代理提供了一个有前景的途径[[7](#bib.bib7)]。现有的关于教育领域的RL研究主要集中在课程优化技术的开发[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]，提供量身定制的提示和反馈[[12](#bib.bib12),
    [13](#bib.bib13)]，以及生成教育内容[[14](#bib.bib14), [15](#bib.bib15)]。只有少数研究探讨了有效操作学习环境的RL基础学习者代理[[16](#bib.bib16),
    [17](#bib.bib17)]。然而，这些基于RL的学习者代理已在具有明确规则的结构化任务（如数学和逻辑难题）中进行研究。在这种环境中，由于使用从现有结构中获得的工程特征定义状态和行动表示，RL的能力得到了自然利用[[7](#bib.bib7),
    [18](#bib.bib18), [16](#bib.bib16)]。然而，依赖手工制作的特征和工程化的状态表示限制了这些RL代理在非结构化领域中的使用能力，并限制了它们在不同任务之间泛化学习技能和知识的能力。
- en: Recent advances in generative AI, in particular Large Language Models (LLMs),
    provide new opportunities to drastically improve state-of-the-art educational
    technology [[19](#bib.bib19)]. LLMs are capable of generating coherent and contextually
    relevant content, engaging in meaningful dialogues, and executing specific linguistic
    tasks without explicit training [[20](#bib.bib20), [21](#bib.bib21)]. So far,
    in education, LLMs have mainly been applied for generating educational content [[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)], automating grading and feedback processes [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)],
    and facilitating the development of collaborative systems [[31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33)]. Few works have also used LLMs for learner modeling in programming
    domains [[34](#bib.bib34)] or for simulating students’ behaviors as a basis for
    an interactive tool for teacher training [[35](#bib.bib35)]. However, despite
    their proficiency in linguistic tasks, LLMs often fall short in decision-making
    in a constrained environment, a domain where RL agents excel due to their inherent
    capability to make feasible decisions within a given environment [[36](#bib.bib36)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在生成性人工智能方面的进展，特别是大语言模型（LLMs），提供了极大改善最先进教育技术的新机会[[19](#bib.bib19)]。LLMs能够生成连贯且符合上下文的内容，进行有意义的对话，并执行特定的语言任务而无需明确的训练[[20](#bib.bib20),
    [21](#bib.bib21)]。迄今为止，在教育领域，LLMs主要应用于生成教育内容[[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24)]，自动化评分和反馈过程[[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30)]，以及促进协作系统的开发[[31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33)]。少数研究还使用LLMs进行编程领域的学习者建模[[34](#bib.bib34)]，或模拟学生行为以作为教师培训互动工具的基础[[35](#bib.bib35)]。然而，尽管LLMs在语言任务方面表现出色，它们在受限环境中的决策能力常常不如RL代理，因为RL代理在特定环境中能够做出可行的决策[[36](#bib.bib36)]。
- en: Given the strengths and limitations of RL and LLM-based agents, recent works
    have investigated the integration of LLMs with RL to design agents that overcome
    the individual limitations of these agents. For instance, this integration has
    been used to substantially improve reward design and exploration efficiency in
    various domains [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)].
    However, most of these approaches have focused on the use of LLMs for training,
    bearing the risk of taking on LLMs’ limitations in decision-making in constrained
    environments.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于RL和LLM基于的智能体的优缺点，最近的研究已经探讨了将LLMs与RL整合，以设计克服这些智能体单独限制的智能体。例如，这种整合已在多个领域显著改善了奖励设计和探索效率[[37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]。然而，大多数这些方法集中于LLMs用于训练的应用，存在将LLMs在受限环境中决策的局限性的风险。
- en: 'In this paper, we investigate the integration of RL and LLMs to create agents
    with enhanced generalizability in text-based educational environments, focusing
    on employing the LLM in the inference phase. To support our investigations, we
    present a novel text-based simulation benchmark, PharmaSimText, adapted from the
    PharmaSim virtual pharmacy environment designed for practicing diagnostic conversations.
    We present three types of agents: (i) RL-based agents employing natural language
    based representations, (ii) LLM-based agents invoked through prompting, and (iii)
    hybrid models where LLMs assist RL agents in the inference phase.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了RL和LLMs的整合，以创建具有增强泛化能力的文本基础教育环境中的智能体，重点是将LLM应用于推理阶段。为了支持我们的研究，我们提出了一个新颖的基于文本的模拟基准，PharmaSimText，它改编自用于实践诊断对话的PharmaSim虚拟药房环境。我们展示了三种类型的智能体：（i）基于RL的智能体，使用自然语言表示，（ii）通过提示调用的LLM智能体，以及（iii）混合模型，其中LLMs在推理阶段协助RL智能体。
- en: 'We extensively evaluate all agents based on their ability to engage in effective
    diagnostic conversations and achieve accurate diagnoses on the PharmaSimText benchmark,
    focusing on their performance across a range of rephrased scenarios across diverse
    patient profiles. With our experiments, we aim to address three research questions:
    Which agent type demonstrates overall superior performance in conducting effective
    diagnostic conversations and achieving accurate diagnoses for all available patients
    (RQ1)? How does reflective prompting influence the diagnostic performance and
    conversation quality of LLM-involved agents (RQ2)? How do diagnostic performance
    and conversation quality vary among different agent types across diverse patients
    (RQ3)? Our results demonstrate that a specific type of LLM-assisted RL agent outperforms
    all other agents in a combined score by effectively balancing accurate diagnosis
    along with high-quality diagnostic conversations. The source code and benchmark
    are released on GitHub.¹¹1https://github.com/epfl-ml4ed/PharmaSimText-LLM-RL'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们广泛评估了所有智能体在有效的诊断对话中的表现能力，并在PharmaSimText基准上取得准确的诊断结果，重点关注它们在各种重述场景和不同患者档案中的表现。通过我们的实验，我们旨在解决三个研究问题：哪种智能体类型在进行有效的诊断对话和为所有可用患者提供准确诊断方面表现最佳（RQ1）？反思性提示如何影响涉及LLM的智能体的诊断表现和对话质量（RQ2）？不同类型的智能体在不同患者中如何在诊断表现和对话质量方面有所不同（RQ3）？我们的结果表明，一种特定类型的LLM辅助RL智能体在综合评分中超越了所有其他智能体，能够有效平衡准确诊断和高质量的诊断对话。源代码和基准已在GitHub上发布。[https://github.com/epfl-ml4ed/PharmaSimText-LLM-RL](https://github.com/epfl-ml4ed/PharmaSimText-LLM-RL)
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Given our focus on integrating RL agents and LLMs to create generalizable learner
    models, we review prior work in developing learner models, explore the growing
    field of intelligent agents in text-based interactive games and finally discuss
    recent advancements in integrating RL and LLMs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们专注于整合RL智能体和LLMs以创建可泛化的学习模型，我们回顾了开发学习模型的先前工作，探索了在文本基础交互游戏中智能体的不断增长的领域，并最终讨论了RL和LLMs整合的最新进展。
- en: Learner agents in educational environments. There is a large body of research
    [[1](#bib.bib1)] on simulating learners in online environments. Existing research
    provides rich, but not generalizable learner representations, for example by generating
    cognitive models from problem-solving demonstrations (e.g., SimStudent [[41](#bib.bib41)])
    or simulates learners from student models in a data-driven way [[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)], leading to less rich, but more generalizable
    representations. RL is a promising tool to address these limitations. However,
    in the education domain, this framework has been primarily applied for pedagogical
    policy induction [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)],
    providing tailored hints [[13](#bib.bib13), [12](#bib.bib12)], generating educational
    content [[14](#bib.bib14), [15](#bib.bib15)], and assessing interventions in educational
    platforms [[45](#bib.bib45), [46](#bib.bib46)]. Despite its potential, the exploration
    of RL-based learner agents for effective operation in learning environments remains
    limited [[16](#bib.bib16), [17](#bib.bib17)]. Prior work has for example used
    Proximal Policy Optimization (PPO) for designing learner models in intelligent
    tutoring systems [[16](#bib.bib16)] or employed neural and symbolic program synthesize
    to create student attempts in a block-based programming environment [[47](#bib.bib47)].
    In this paper, we develop a series of learner agents for an open-ended educational
    environment.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 教育环境中的学习者代理。关于在线环境中模拟学习者的研究相当丰富[[1](#bib.bib1)]。现有研究提供了丰富但不可推广的学习者表示，例如通过问题解决演示生成认知模型（例如，SimStudent[[41](#bib.bib41)]），或通过数据驱动的方式模拟学习者[[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)]，这导致了较少丰富但更具可推广性的表示。强化学习是一种有前景的工具，可以解决这些局限性。然而，在教育领域，这一框架主要用于教学策略引导[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]，提供量身定制的提示[[13](#bib.bib13),
    [12](#bib.bib12)]，生成教育内容[[14](#bib.bib14), [15](#bib.bib15)]，以及评估教育平台中的干预措施[[45](#bib.bib45),
    [46](#bib.bib46)]。尽管具有潜力，但基于强化学习的学习者代理在学习环境中的有效操作的探索仍然有限[[16](#bib.bib16), [17](#bib.bib17)]。例如，之前的工作使用了近端策略优化（PPO）来设计智能辅导系统中的学习者模型[[16](#bib.bib16)]，或采用神经和符号程序合成来创建区块编程环境中的学生尝试[[47](#bib.bib47)]。在本文中，我们开发了一系列用于开放式教育环境的学习者代理。
- en: Agents for text-based interactive games. The growing interest in developing
    intelligent agents for text-based interactive games, especially those that mimic
    real-world scenarios [[36](#bib.bib36), [48](#bib.bib48), [49](#bib.bib49)], has
    led to diverse methodologies encompassing RL [[50](#bib.bib50)], behavior cloning
    (BC) [[36](#bib.bib36)], and prompting LLMs [[51](#bib.bib51), [52](#bib.bib52)].
    A well-known example is the game ScienceWorld [[36](#bib.bib36)], where players
    engage in scientific experiments through environment exploration and interaction.
    Within the RL framework, state-of-the art employs deep reinforced relevance networks
    (DRRNs) [[50](#bib.bib50)], treating text-based interactions as partially-observable
    Markov decision processes (POMDPs), and learning distinct text representations
    for observations and actions to estimate Q-values via a scorer network. Within
    the LLM domain, LLM-based strategies use prompts at each interaction step for
    strategic planning and action selection. While some studies [[51](#bib.bib51)]
    engage in a single interaction round with the environment, others [[53](#bib.bib53),
    [52](#bib.bib52)] use a multi-round approach, facilitating iterative refinement
    through repeated attempts. In this paper, we develop a series of agents for a
    text-based educational environment simulating real-world scenarios happening in
    a pharmacy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 文本互动游戏的智能代理。对开发用于文本互动游戏的智能代理的兴趣日益增长，特别是那些模拟现实世界场景的游戏[[36](#bib.bib36), [48](#bib.bib48),
    [49](#bib.bib49)]，这导致了多种方法的出现，包括强化学习（RL）[[50](#bib.bib50)]、行为克隆（BC）[[36](#bib.bib36)]和提示大型语言模型（LLMs）[[51](#bib.bib51),
    [52](#bib.bib52)]。一个著名的例子是游戏ScienceWorld[[36](#bib.bib36)]，玩家通过环境探索和互动进行科学实验。在强化学习框架中，最先进的方法使用深度强化关联网络（DRRNs）[[50](#bib.bib50)]，将基于文本的互动视为部分可观察的马尔可夫决策过程（POMDPs），并为观察和行动学习不同的文本表示，通过评分网络估计Q值。在LLM领域，基于LLM的策略在每次互动步骤中使用提示进行战略规划和行动选择。虽然一些研究[[51](#bib.bib51)]进行了一轮与环境的互动，其他研究[[53](#bib.bib53),
    [52](#bib.bib52)]则采用多轮方法，通过反复尝试促进迭代改进。在本文中，我们开发了一系列用于模拟现实世界药店场景的文本教育环境的代理。
- en: RL and LLM integration. Recently, LLMs have been used to assist RL agents in
    various tasks, demonstrating notable advancements in reward design and exploration
    efficiency. For example, [[39](#bib.bib39)] utilized text corpora to pre-train
    agents, thereby shaping their exploration by suggesting goals based on the agents’
    current state descriptions. Furthermore, [[40](#bib.bib40)] proposed a novel approach
    to simplify reward design by employing LLMs to generate reward signals from textual
    prompts that describe desired behaviors. In a similar vein, [[37](#bib.bib37)]
    showcased the innovative application of few-shot LLM prompting to hypothesize
    world models for RL agents, which improves training sample efficiency and allows
    agents to correct LLM errors through interaction with the environment. While these
    studies highlight the synergistic potential of integrating LLMs with RL techniques
    to achieve more objective-aligned agent behaviors, directed exploration, and efficient
    training processes, the use of LLMs in the training phase bears the risk of carrying
    over their limitations in decision-making in constrained environments. A notable
    gap, therefore, remains in using LLMs to assist RL agents during the inference
    phase. Specifically, the current body of work has not addressed the use of LLMs
    to aid RL agents in adapting and transferring their learned skills to novel environments
    or tasks post-training. In our work, we aim to bridge this gap by focusing on
    utilizing LLMs as assistants for RL agents during generalization to new settings.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RL和LLM的整合。最近，LLM已被用于协助RL代理完成各种任务，展示了奖励设计和探索效率的显著进步。例如，[[39](#bib.bib39)]利用文本语料库对代理进行预训练，从而通过根据代理的当前状态描述建议目标来塑造其探索。此外，[[40](#bib.bib40)]提出了一种通过使用LLM从描述期望行为的文本提示中生成奖励信号的创新方法，从而简化奖励设计。类似地，[[37](#bib.bib37)]展示了将少量示例LLM提示应用于为RL代理假设世界模型的创新应用，这提高了训练样本的效率，并允许代理通过与环境互动来纠正LLM的错误。尽管这些研究突显了将LLM与RL技术整合以实现更符合目标的代理行为、定向探索和高效训练过程的协同潜力，但在训练阶段使用LLM存在将其在受限环境中决策限制带入的风险。因此，在推理阶段使用LLM来协助RL代理仍存在显著差距。具体而言，现有研究尚未解决使用LLM帮助RL代理在训练后适应和迁移其学习技能到新环境或任务的问题。在我们的工作中，我们旨在通过关注利用LLM作为RL代理在概括到新环境时的助手来填补这一空白。
- en: 3 PharmasimText benchmark
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 PharmasimText基准
- en: We created PharmaSimText, a text-based interactive environment, as an infrastructure
    for developing language agents capable of handling text-based learning tasks and
    generalizing in them. PharmaSimText is an interactive text-based environment designed
    based on PharmaSim, a scenario-based learning platform. It simulates real-world
    interactions between a pharmacist and a patient in a pharmacy setting. This benchmark
    includes more than 500 scenario variations that can be used for developing and
    evaluating learner agents.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了PharmaSimText，一个基于文本的互动环境，作为开发能够处理基于文本的学习任务并在其中进行概括的语言代理的基础设施。PharmaSimText是一个基于PharmaSim设计的互动文本环境，PharmaSim是一个基于情境的学习平台。它模拟了药剂师和患者在药房环境中的现实互动。这个基准包括500多个情境变体，可以用于开发和评估学习代理。
- en: '![Refer to caption](img/a73703a0666389b050d29d5077bc8f55.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a73703a0666389b050d29d5077bc8f55.png)'
- en: 'Figure 1: ’Father Inquiry’ Scenario in PharmaSim - A simulated pharmacy setting
    designed for practicing diagnostic conversational skills, where participants engage
    with a father seeking guidance for his infant child’s diarrhea.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：“父亲咨询”场景在PharmaSim中的模拟药房设置，旨在练习诊断对话技巧，参与者与寻求指导以解决其婴儿腹泻问题的父亲进行互动。
- en: \Description
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: An overview of the Father Inquiry scenario in the PharmaSim environment which
    shows a father asking for help for his baby’s diarhea
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 关于PharmaSim环境中的“父亲咨询”场景的概述，该场景展示了一位父亲寻求帮助以解决他宝宝的腹泻问题。
- en: '![Refer to caption](img/d82e643a8554252fd83376ddb4257e60.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d82e643a8554252fd83376ddb4257e60.png)'
- en: 'Figure 2: Diagnostic Strategy in the ’Father Inquiry’ Scenario of PharmaSim,
    depicting the process of identifying the most likely cause of an infant’s diarrhea.
    Players must pose four key questions to the father to collect crucial information,
    enabling the determination of the most probable cause of the child’s diarrhea
    among four potential causes.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：PharmaSim中“父亲咨询”场景的诊断策略，描绘了识别婴儿腹泻最可能原因的过程。玩家必须向父亲提出四个关键问题，以收集重要信息，从而确定在四个潜在原因中最可能导致孩子腹泻的原因。
- en: \Description
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Diagnostic Strategy in the ’Father Inquiry’ Scenario of PharmaSim. There are
    4 key questions including Baby’s age, baby’s intensity of the symptoms, baby’s
    diet, and mother’s current medication with the father’s response to to them in
    the left. In the right, the four possible causes teething, mother’s antibiotic
    intake, diet change, and viral infection are investigated by different key questions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: PharmaSim 的“父亲询问”场景中的诊断策略。左侧列出了4个关键问题，包括婴儿的年龄、症状的严重程度、婴儿的饮食以及母亲当前的用药，以及父亲对这些问题的回答。在右侧，四种可能的原因——长牙、母亲的抗生素摄入、饮食变化和病毒感染——通过不同的关键问题进行调查。
- en: 3.1 PharmaSim
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 PharmaSim
- en: 'PharmaSim is a scenario-based learning environment designed to support the
    development of diagnostic skills. In each scenario, a patient comes to the pharmacy
    and asks for help with a specific problem. The player needs to identify different
    possible causes of this problem and mark how probable they are while interacting
    with the environment. Specifically, there are six different types of interactions:
    asking questions to the patient, seeking help from the pharmacist, searching about
    different kinds of medicine, looking for the specifications of products available
    on the shelf, reading about issues related to the problem, and offering a solution,
    which ends the game and moves the player to the post-test phase. In the post-test
    phase, players need to list three possible causes, rate their probability, and
    give an explanation for each of them. The determination of these likelihoods that
    leads to finding the most probable cause significantly depends on a set of patient
    inquiries containing essential information, which we henceforth refer to as key
    questions.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PharmaSim 是一个基于场景的学习环境，旨在支持诊断技能的开发。在每个场景中，患者来到药房并寻求特定问题的帮助。玩家需要识别这个问题的不同可能原因，并在与环境互动的同时标记它们的可能性。具体来说，有六种不同类型的互动：向患者提问、向药剂师寻求帮助、搜索不同种类的药物、查找货架上产品的规格、阅读与问题相关的内容以及提供解决方案，解决方案结束游戏并将玩家移至后测阶段。在后测阶段，玩家需要列出三个可能的原因，评估它们的可能性，并为每个原因提供解释。这些可能性的确定，进而找到最可能的原因，显著依赖于包含关键信息的一组患者询问，我们将其称为关键问题。
- en: 'Currently, two different scenarios designed with insights from human experts
    are available in the game. For example, in one scenario (see Fig. [1](#S3.F1 "Figure
    1 ‣ 3 PharmasimText benchmark ‣ Towards Generalizable Agents in Text-Based Educational
    Environments: A Study of Integrating RL with LLMs")), a father visits the pharmacy
    looking for help with his infant child’s diarrhea. The scenario presents four
    probable causes for the child’s condition. The player is required to ask four
    key questions to the father to gather the essential information needed to find
    the most probable cause behind the child’s diarrhea. The relation between these
    key questions and the most probable cause of the child’s diarrhea is illustrated
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3 PharmasimText benchmark ‣ Towards Generalizable
    Agents in Text-Based Educational Environments: A Study of Integrating RL with
    LLMs"). For instance, inquiring about the child’s age enables the player to deduce
    that teething is an improbable cause due to the child’s young age.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，游戏中提供了两个基于人类专家见解设计的不同场景。例如，在一个场景中（见图 [1](#S3.F1 "Figure 1 ‣ 3 PharmasimText
    benchmark ‣ Towards Generalizable Agents in Text-Based Educational Environments:
    A Study of Integrating RL with LLMs")），一个父亲来到药房寻求帮助，解决他婴儿腹泻的问题。该场景提出了四种可能的腹泻原因。玩家需要向父亲提问四个关键问题，以收集找到腹泻最可能原因所需的基本信息。这些关键问题与婴儿腹泻最可能原因之间的关系如图
    [2](#S3.F2 "Figure 2 ‣ 3 PharmasimText benchmark ‣ Towards Generalizable Agents
    in Text-Based Educational Environments: A Study of Integrating RL with LLMs")
    所示。例如，询问婴儿的年龄可以让玩家推断出由于婴儿年龄较小，长牙不太可能是原因。'
- en: 3.2 PharmaSimText
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 PharmaSimText
- en: To develop our benchmark, several modifications to PharmaSim were implemented.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发我们的基准，我们对 PharmaSim 实施了若干修改。
- en: 'Migration to a text-based environment. As the first step, we did two adaptions
    to PharmaSim to migrate it to a text-based environment. First, we simplified interactions
    to two types of actions: asking questions to the patient about various characters
    phrased similar to PharmaSim as "I want to know about the character’s topic."
    and advancing to the post-test by proposing a solution as "I want to suggest a
    solution.". Second, we modified the post-test questions to offer a feasible assessment
    for the agents. To this end, we revised the three causes question to focus solely
    on the most probable cause of the patient’s issue.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移到基于文本的环境。作为第一步，我们对PharmaSim进行了两个适配，以将其迁移到基于文本的环境中。首先，我们将交互简化为两种类型的操作：向患者询问关于各种角色的问题，这些问题类似于PharmaSim中的“我想了解关于角色的主题。”并通过提出解决方案“我想建议一个解决方案。”来推进到后测阶段。其次，我们修改了后测问题，以提供对代理的可行评估。为此，我们将三个原因的问题修订为仅关注患者问题的最可能原因。
- en: 'Extension of available scenarios. In the next step, we focused on enriching
    PharmaSimText and enhancing its complexity. For this purpose, we expanded the
    two scenarios available in the original environment across three dimensions: (1)
    introducing new patients, (2) varying the scenarios to alternate the most probable
    cause of each patient’s problem, and (3) diversifying patient responses by rephrasing
    them. Given the scale of extension, relying solely on human expertise was impractical.
    Instead, we leveraged the generative capabilities of LLMs combined with human
    insights to develop the scenarios in PharmaSimText. Prior to prompting LLMs for
    creating scenarios, we structured our expanded scenarios to align with the pharmacy
    assistant training curriculum of Switzerland. We gathered a set of health problems
    from the curriculum, assigning each to a fictional patient with a specified age
    and gender. We further identified a range of illnesses from the curriculum’s textbooks,
    known to manifest symptoms relevant to the chosen problems.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现有场景的扩展。下一步，我们专注于丰富PharmaSimText并提升其复杂性。为此，我们在原始环境中将两个场景扩展到三个维度：(1) 引入新患者，(2)
    变换场景以交替每位患者问题的最可能原因，以及 (3) 通过重新措辞来多样化患者的反应。考虑到扩展的规模，单靠人工专业知识是不切实际的。相反，我们利用了LLMs的生成能力与人工见解相结合来开发PharmaSimText中的场景。在提示LLMs创建场景之前，我们将扩展后的场景结构调整为符合瑞士药剂师培训课程的要求。我们从课程中收集了一系列健康问题，将每个问题分配给一个虚构的患者，指定其年龄和性别。我们进一步从课程教材中确定了一系列疾病，这些疾病表现出与所选问题相关的症状。
- en: 'Prompting LLMs for scenario creation. The scenario creation process involved
    three steps: (1) we prompted the LLM to generate a list of key questions aimed
    at diagnosing the most probable cause of the patient’s problem, (2) the LLM was
    tasked to simulate patient responses, assuming each illness on the list was the
    most probable cause behind their problem, and (3) the LLM was prompted to generate
    answers to common patient inquiries done by pharmacists. We used GPT-4 as the
    LLM for scenario creation; the exact prompts employed can be found on our GitHub
    repository (link provided in Footnote [1](#footnote1 "footnote 1 ‣ 1 Introduction
    ‣ Towards Generalizable Agents in Text-Based Educational Environments: A Study
    of Integrating RL with LLMs")). To ensure realism and applicability, a human expert
    has reviewed all of the scenarios and provided feedback including minor changes
    which were reflected in the final version of the scenarios. Additionally, the
    LLM was employed to diversify existing patient responses through paraphrasing,
    enhancing the scenarios’ complexity. To further augment this complexity, fictional
    characters were introduced as distractors, enabling players to engage in more
    nuanced interactions.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '提示LLMs进行场景创建。场景创建过程包括三个步骤：(1) 我们提示LLM生成一系列关键问题，旨在诊断患者问题的最可能原因，(2) LLM被要求模拟患者反应，假设列表中的每种疾病是其问题的最可能原因，并且
    (3) LLM被提示生成药剂师对常见患者询问的回答。我们使用GPT-4作为场景创建的LLM；确切的提示可以在我们的GitHub存储库中找到（链接见脚注 [1](#footnote1
    "footnote 1 ‣ 1 Introduction ‣ Towards Generalizable Agents in Text-Based Educational
    Environments: A Study of Integrating RL with LLMs")）。为了确保真实性和适用性，一位专家已审核了所有场景，并提供了包括小修改在内的反馈，这些修改反映在场景的最终版本中。此外，LLM还被用来通过同义改写来多样化现有的患者反应，从而提高了场景的复杂性。为了进一步增强这种复杂性，虚构角色作为干扰项被引入，使玩家能够进行更微妙的互动。'
- en: '| Problem | # of Possible Causes | Possible Causes | # of Key Questions |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 可能原因的数量 | 可能的原因 | 关键问题的数量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Infant Diarrhea | 4 | Change of diet, Teething, Current medication of the
    mother, Viral Infection | 4 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 婴儿腹泻 | 4 | 饮食变化、长牙、母亲当前用药、病毒感染 | 4 |'
- en: '| Breastfeeding-related | 6 | Engorgement, Plugged Ducts, Cracked Nipples,
    Mastitis, Thrush, Low Milk Supply | 7 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 哺乳相关 | 6 | 乳胀、堵塞乳管、乳头裂伤、乳腺炎、念珠菌感染、奶水不足 | 7 |'
- en: '| Urological | 4 | Prostate Hyperplasia, Cystitis, Urge Incontinence, Stress
    Incontinence | 6 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 泌尿科 | 4 | 前列腺增生、膀胱炎、急迫性尿失禁、压力性尿失禁 | 6 |'
- en: '| Skin-related | 10 | Sunburn, Insect Bites, Acne, Eczema, Athlete’s Foot,
    Psoriasis, Rashes, Warts and Corns, Cold Sores, Neurodermatitis | 10 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 皮肤相关 | 10 | 晒伤、昆虫叮咬、痤疮、湿疹、脚气、银屑病、皮疹、疣和茧、唇疱疹、神经性皮炎 | 10 |'
- en: '| Eye-related | 5 | Dry Eyes, Allergic Conjunctivitis, Pink Eye, Eye Strain,
    Stye | 11 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 眼科 | 5 | 眼干、过敏性结膜炎、粉红眼、眼疲劳、麦粒肿 | 11 |'
- en: '| Gynecological | 8 | UTI, Cystitis, Kidney Stones, Overactive Bladder, Pregnancy,
    STI, Stress Incontinence, Fungal Infection | 8 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 妇科 | 8 | 尿路感染、膀胱炎、肾结石、膀胱过度活动、怀孕、性传播感染、压力性尿失禁、真菌感染 | 8 |'
- en: '| Joint Pain | 5 | Osteoarthritis, Muscle Sprains, Tendonitis, Bursitis, Gout
    | 9 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 关节疼痛 | 5 | 骨关节炎、肌肉拉伤、腱炎、滑囊炎、痛风 | 9 |'
- en: '| Sore Throat | 5 | Common Cold, Influenza, Sinusitis, Pharyngitis, Bronchitis
    | 7 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 咽喉疼痛 | 5 | 普通感冒、流感、鼻窦炎、咽炎、支气管炎 | 7 |'
- en: 'Table 1: Overview of PharmaSimText Scenarios. Every task within the benchmark
    is centered on a unique health problem, which could stem from various causes.
    Players must ask several key questions to arrive at a correct diagnosis.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：PharmaSimText场景概述。基准中的每个任务都围绕一个独特的健康问题展开，该问题可能源于各种原因。玩家必须提出几个关键问题以得出正确的诊断。
- en: 'Statistics on the PharmaSimText benchmark. The obtained benchmark contains
    eight distinct scenarios, each revolving around a unique patient profile. Details
    about the patients can be found in Table [1](#S3.T1 "Table 1 ‣ 3.2 PharmaSimText
    ‣ 3 PharmasimText benchmark ‣ Towards Generalizable Agents in Text-Based Educational
    Environments: A Study of Integrating RL with LLMs"). On average, each scenario
    presents seven potential causes for the patient’s problem, resulting in a total
    of $47$ key questions by the player. As a result, PharmaSimText can provide an
    enriched environment for further studies on agents for text-based interactive
    tasks and agents’ generalizability.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 关于PharmaSimText基准的统计数据。获得的基准包含八种不同的场景，每种场景围绕一个独特的患者档案展开。有关患者的详细信息可以在表格[1](#S3.T1
    "表 1 ‣ 3.2 PharmaSimText ‣ 3 PharmaSimText基准 ‣ 面向文本教育环境中的通用代理：RL与LLM整合的研究")中找到。平均而言，每种场景提出了七种潜在的患者问题原因，结果是玩家提出了总计$47$个关键问题。因此，PharmaSimText可以为进一步研究文本互动任务中的代理和代理的通用性提供丰富的环境。
- en: '![Refer to caption](img/8997d550c4519e4eaad0cd1bc8318f25.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8997d550c4519e4eaad0cd1bc8318f25.png)'
- en: 'Figure 3: LLM-assisted RL agents. An LLM is prompted to assist the RL agent
    at the inference time to aid in generalization. In the Suggestion-Assisted RL
    (SA-RL) agent (left), the LLM suggests $k$ actions at each step for the RL agent
    to choose from. In the Decision-Assisted RL (DA-RL) agent (right), the LLM selects
    an action from the top-k choices provided by the RL agent.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LLM辅助的RL代理。LLM被提示在推断时协助RL代理以帮助泛化。在建议辅助RL（SA-RL）代理（左侧），LLM在每一步建议$k$个动作供RL代理选择。在决策辅助RL（DA-RL）代理（右侧），LLM从RL代理提供的前k个选择中选择一个动作。
- en: \Description
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: The cycles LLM-assisted RL agents take to interact within the environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LLM辅助RL代理在环境中互动的周期。
- en: 4 Agents for PharmaSimText
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 PharmaSimText的代理
- en: 'We developed three types of agents for PharmaSimText that embody various degrees
    of RL and LLM synergy: RL-based agents, LLM-based agents, and LLM-assisted RL
    agents.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为PharmaSimText开发了三种类型的代理，体现了RL和LLM的不同程度的协同作用：基于RL的代理、基于LLM的代理和LLM辅助的RL代理。
- en: 4.1 RL-based Agents
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于RL的代理
- en: RL agents learn to interact within an environment by taking actions based on
    their current state and receiving feedback in the form of rewards or penalties
    for those actions [[54](#bib.bib54)]. They try to maximize their obtained cumulative
    reward over time to effectively learn the best policy for achieving their goal
    within the environment. One well-known method in RL involves estimating a metric
    called Q-value, which represents the expected future rewards for taking a certain
    action in a given state. Deep Q-Networks (DQNs)[[55](#bib.bib55)] approximate
    these Q-values using deep neural networks, enabling handling of complex, high-dimensional
    environments by learning to predict the Q-values directly from the environmental
    states. DQNs are trained through interactions with the environment, using their
    experience to iteratively refine and make their estimations of Q-values more accurate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RL代理通过根据当前状态采取行动并根据这些行动获得奖励或惩罚的反馈来学习在环境中互动[[54](#bib.bib54)]。他们尝试最大化获得的累计奖励，以有效地学习在环境中实现目标的最佳策略。在RL中，一个著名的方法涉及估计一种称为Q值的度量，该度量表示在给定状态下采取某个动作的期望未来奖励。深度Q网络（DQNs）[[55](#bib.bib55)]使用深度神经网络来近似这些Q值，使其能够通过直接从环境状态中预测Q值来处理复杂的高维环境。DQNs通过与环境的互动进行训练，利用他们的经验迭代地优化和使Q值估计更为准确。
- en: 'Following previous work on text-based games, we utilized state-of-the-art,
    a DRRN [[50](#bib.bib50)] as the RL-based agent for interacting with PharmaSimText.
    The DRRN is designed to learn distinct representations for the text-based states
    and actions by employing two separate networks: the state encoder and the action
    encoder. A scorer network then evaluates these representations to estimate their
    Q-values. At a given step $t$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 参照以前关于文本游戏的研究，我们利用了最先进的DRRN [[50](#bib.bib50)] 作为与PharmaSimText互动的基于RL的代理。DRRN设计用于通过使用两个独立的网络：状态编码器和动作编码器，学习文本状态和动作的不同表示。然后，评分网络评估这些表示以估计它们的Q值。在给定的步骤$t$。
- en: In our case, the valid actions at time step $t$ in the environment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，环境中时间步骤$t$的有效动作。
- en: We introduced two modifications to adapt the original DRRN to our environment.
    First, we employed pre-trained sentence embeddings from fastText [[56](#bib.bib56)]
    to generate text representations for both observations and actions. This choice
    was motivated by previous work showing that training the RNNs in the encoders
    of a DRRN with a loss function solely aligned with the RL objectives leads to
    unstable training and suboptimal embeddings [[57](#bib.bib57)]. Second, unlike
    the environments that DRRNs were proposed to tackle the tasks in, the observation
    at a given time step $t$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对原始DRRN进行了两个修改以适应我们的环境。首先，我们使用了来自fastText [[56](#bib.bib56)] 的预训练句子嵌入来生成观察和动作的文本表示。这一选择是由于以前的研究显示，DRRN的编码器中的RNN仅用与RL目标对齐的损失函数进行训练，会导致训练不稳定和嵌入效果不佳[[57](#bib.bib57)]。其次，与DRRN被提出以应对任务的环境不同，给定时间步骤$t$的观察。
- en: 4.2 LLM-based Agents
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基于LLM的代理
- en: The agents based on LLMs prompt an LLM at each step of interacting with the
    environment to find the best next action to finish the task. These agents can
    either have only one trial or multiple trials to complete the task along with
    reflection on their strategy between each trial. We respectively denote these
    two agent types by none-reflective and reflective.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理在与环境互动的每个步骤中提示LLM，以找到完成任务的最佳下一个动作。这些代理可以只有一次尝试或多次尝试来完成任务，并在每次尝试之间反思他们的策略。我们分别用非反思型和反思型来表示这两种代理类型。
- en: 'The none-reflective agent interacts with the LLM by issuing a single prompt
    that contains the task description, the history of interactions (consisting of
    the agent’s questions and the patient’s responses), prior experience with the
    patient, and valid actions available at the current step to choose the most appropriate
    subsequent action. The task description is structured as Find the cause behind
    the patient’s problem, while the interaction history is presented as a dialogue
    between the patient and the agent, with action texts labeled as agent’s questions
    and environment’s feedback text as patient responses. To format the valid actions,
    each action type is formatted as a function along with its permissible input values,
    which the LLM can interpret. This is complemented by a descriptive text explaining
    the action’s purpose. For instance, the interaction "I want to ask about the subject’s
    topic" is formatted as ask(subject, topic): Asking a question about the subject
    related to the topic, followed by a list of valid subjects and topics. This meticulous
    formatting strategy plays an essential role in minimizing the likelihood of the
    LLM suggesting invalid actions.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '非反射代理通过发出包含任务描述、交互历史（包括代理的问题和患者的回答）、与患者的先前经验以及当前步骤中可用的有效操作的单个提示来与LLM互动，以选择最合适的后续操作。任务描述结构为“找出患者问题的原因”，而交互历史则以患者和代理之间的对话形式呈现，操作文本标记为代理的问题，环境反馈文本标记为患者的回答。为了格式化有效操作，每种操作类型被格式化为一个函数及其允许的输入值，LLM可以进行解释。这还通过描述性文本解释了操作的目的。例如，交互“我想询问有关主题的问题”被格式化为ask(subject,
    topic): 提问有关主题的内容，后面跟有有效的主题和主题列表。这种细致的格式化策略在最小化LLM提出无效操作的可能性方面发挥了重要作用。'
- en: Despite efforts to format valid actions to guide the LLM, there are instances
    where the LLM still proposes an action that is invalid within the PharmaSimText
    environment. In such cases, we implemented a strategy where the LLM was prompted
    to suggest an alternative action, repeating this process for a maximum of $k=3$-th
    suggested action. This approach ensures that the LLM’s output is effectively grounded
    in the set of actions that are feasible within the environment.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们努力格式化有效的操作以指导LLM，但在PharmaSimText环境中，LLM仍然会提出一些无效的操作。在这种情况下，我们实施了一种策略，让LLM建议一个替代操作，并将这一过程重复最多`k=3`次。这种方法确保了LLM的输出有效地基于环境中可行的操作集合。
- en: The reflective agent employs a prompting strategy akin to that of the none-reflective
    agent to determine the optimal subsequent action. The none-reflective agent prompt
    is augmented with a segment including learnings from prior engagements with the
    same patient having the same cause. This reflective process involves prompting
    the LLM to evaluate its previous strategies based on the observed outcomes after
    completing each trial. The agent then updates its textual memory of previous learnings,
    and the updated memory is used for prompting in the next trial. This approach
    was inspired by research on self-reflective LLMs, notably the continually learning
    language agent CLIN[[52](#bib.bib52)]. Similar to CLIN, we constructed the learning
    memory using causal formats such as “X is necessary for Y” to guide future interactions.
    This mechanism enables the reflective agent to dynamically adapt and refine its
    approach, enhancing its decision-making process over time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 反射代理采用类似于非反射代理的提示策略来确定最佳的后续操作。非反射代理的提示被增强了一个段落，包含了从与同一患者（具有相同原因）的先前接触中获得的经验教训。这个反射过程涉及提示LLM评估其基于每次试验后的观察结果的先前策略。然后，代理更新其关于先前经验的文本记忆，并在下一次试验中使用更新的记忆进行提示。这种方法受到自我反思LLM的研究启发，特别是持续学习语言代理CLIN[[52](#bib.bib52)]。与CLIN类似，我们使用因果格式构建学习记忆，例如“X对Y是必要的”，以指导未来的交互。这一机制使反射代理能够动态调整和完善其方法，随着时间的推移提升决策过程。
- en: 4.3 LLM-assisted RL Agents
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LLM辅助的强化学习代理
- en: 'The perspective of RL-based agents remains limited to their experience during
    training, potentially hindering the performance in tasks with unfamiliar elements
    not encountered during their training. To address this, we leveraged LLMs’ commonsense
    reasoning capabilities to augment RL agents’ decision-making processes. As shown
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.2 PharmaSimText ‣ 3 PharmasimText benchmark ‣
    Towards Generalizable Agents in Text-Based Educational Environments: A Study of
    Integrating RL with LLMs"), we explored two methods for integrating LLM assistance:
    Suggestion-Assisted RL (SA-RL) and Decision-Assisted RL (DA-RL).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'RL基础代理的视角仅限于训练期间的经验，这可能会阻碍在训练中未遇到的陌生元素的任务表现。为了解决这个问题，我们利用了LLM的常识推理能力来增强RL代理的决策过程。如图[3](#S3.F3
    "Figure 3 ‣ 3.2 PharmaSimText ‣ 3 PharmasimText benchmark ‣ Towards Generalizable
    Agents in Text-Based Educational Environments: A Study of Integrating RL with
    LLMs")所示，我们探索了两种集成LLM辅助的方法：建议辅助RL（SA-RL）和决策辅助RL（DA-RL）。'
- en: In the SA-RL approach, at a given time step $t$ in the posttest steps.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在SA-RL方法中，在测试后步骤的给定时间步$t$。
- en: In the DA-RL approach, at a given time step $t$ in the post-test steps.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在DA-RL方法中，在测试后步骤的给定时间步$t$。
- en: 'Based on whether the LLM is given an opportunity to reflect on its past decisions
    or not, we obtain two versions of DA-RL and SA-RL approaches, which we distinguish
    via reflective/none-reflective prefixes. Thus, we study four LLM-assisted RL agents:
    none-reflective-DA-RL, reflective-DA-RL, none-reflective-SA-RL, and reflective-SA-RL.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据LLM是否有机会反思其过去的决策，我们获得了两种版本的DA-RL和SA-RL方法，通过反思/非反思前缀来区分。因此，我们研究了四种LLM辅助的RL代理：非反思-DA-RL、反思-DA-RL、非反思-SA-RL和反思-SA-RL。
- en: 5 Experimental Evaluation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验评估
- en: '![Refer to caption](img/da84ed58f2e4ad9600785d0158ed03d7.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da84ed58f2e4ad9600785d0158ed03d7.png)'
- en: 'Figure 4: Generalization task, requiring the agents to generalize over different
    wordings of a scenario.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：泛化任务，要求代理在不同的情境措辞中进行泛化。
- en: \Description
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: an example of rephrased wordings for the question about baby’s age in which
    he is 5 months old is rephrased to my baby is currently 5 months old
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 关于宝宝年龄的问题，例如5个月大的问题被重新措辞为“我的宝宝目前5个月大”
- en: '![Refer to caption](img/a0ad1c8393b32a4f91d1946b5abbc84a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a0ad1c8393b32a4f91d1946b5abbc84a.png)'
- en: 'Figure 5: Agent Performance on PharmaSimText. Post-test Performance Score (left),
    Trajectory Quality Score (middle), and Combined Score (right) of the RL-based
    agent, the reflective-DA-RL agent, the reflective-SA-RL agent, and the reflective-
    LLM-based agent. In the SA-RL agent, the LLM suggests $k$ actions at each step
    for the RL agent to choose from. In the DA-RL agent, the LLM selects an action
    from the top-k choices provided by the RL agent. Scores are averaged across all
    patients in PharmaSimText.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：PharmaSimText中的代理性能。RL基础代理、反思-DA-RL代理、反思-SA-RL代理和反思-LLM基础代理的测试后性能评分（左）、轨迹质量评分（中）和综合评分（右）。在SA-RL代理中，LLM在每一步为RL代理提供$k$个动作供其选择。在DA-RL代理中，LLM从RL代理提供的前k个选择中选择一个动作。评分在PharmaSimText中的所有患者中取平均值。
- en: \Description
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: Agent Performance on PharmaSimText
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PharmaSimText中的代理性能
- en: We evaluated our agents in PharmaSimText to assess which agent type demonstrates
    the most effective diagnostic conversations and accurate diagnoses among all patients
    (RQ1), to investigate the impact of reflective prompting on the diagnostic performance
    and interaction quality of LLM-involved agents (RQ2), and to explore how diagnostic
    performance and conversation quality vary among the different agent types when
    confronted with different patients (RQ3).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在PharmaSimText中评估了我们的代理，以评估哪种代理类型在所有患者中表现出最有效的诊断对话和准确的诊断（RQ1），调查反思提示对涉及LLM的代理的诊断性能和互动质量的影响（RQ2），并探索在面对不同患者时，不同代理类型的诊断性能和对话质量如何变化（RQ3）。
- en: 5.1 Experimental Setup
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Our evaluation was focused on the generalization capabilities of the agents,
    specifically their ability to navigate tasks featuring not previously encountered
    elements. We assessed the agents’ generalizability across rephrased versions of
    already-encountered scenarios, aiming to measure their reliance on the precise
    wording of these scenarios. Figure [4](#S5.F4 "Figure 4 ‣ 5 Experimental Evaluation
    ‣ Towards Generalizable Agents in Text-Based Educational Environments: A Study
    of Integrating RL with LLMs") provides insight into our evaluation methodology
    for generalization, illustrating the diversity created by rephrased answer options
    in a specific scenario.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的评估重点关注代理的泛化能力，特别是它们在面对未曾遇到的元素时的任务导航能力。我们评估了代理在已经遇到的场景的重述版本中的泛化能力，旨在测量它们对这些场景具体措辞的依赖。图[4](#S5.F4
    "Figure 4 ‣ 5 Experimental Evaluation ‣ Towards Generalizable Agents in Text-Based
    Educational Environments: A Study of Integrating RL with LLMs")提供了我们评估泛化的方法，展示了在特定场景中重述答案选项所产生的多样性。'
- en: 'We defined agent success in a subtask based on two aspects: identifying the
    most probable cause of the patient’s problem and asking the key questions in the
    conversation. Here a subtask denotes the combination of a cause and a wording.
    We therefore introduced three metrics:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据两个方面定义了代理在子任务中的成功：识别患者问题的最可能原因以及在对话中提出关键问题。这里的子任务指的是原因和描述的组合。因此，我们引入了三个指标：
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Post-test Performance Score: binary indicator of correct diagnosis of the patient’s
    problem. It measures the agent’s ability to identify the most probable cause of
    the patient’s problem.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 后测表现分数：患者问题正确诊断的二元指标。它衡量代理识别患者问题最可能原因的能力。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Trajectory Quality Score: fraction of key questions involved in the agent’s
    conversation. It measures the quality of the agent’s conversation.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 轨迹质量分数：代理对话中涉及的关键问题的比例。它衡量代理对话的质量。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Combined Score: product of the Post-test Performance Score and Trajectory Quality
    Score. It measures both the above elements together.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合评分：后测表现分数与轨迹质量分数的乘积。它同时衡量以上两个元素。
- en: 5.2 Agent Training and Evaluation
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 代理训练和评估
- en: We developed and trained all of the agents separately for each patient. In this
    process, different wordings of subtasks leading to the same cause were split randomly
    to a training, validation, and test set. Therefore, the training, validation,
    and test sets included subtasks of all of the causes available for a patient in
    distinct wordings. Specifically, the agents saw all the causes during training
    and validation, but not all wordings. In our experiments, $80\%$ of the available
    wordings for each cause were used for training and the remaining wordings were
    split in half for the validation and test set.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每位患者分别开发和训练了所有代理。在此过程中，导致相同原因的不同任务描述被随机分配到训练集、验证集和测试集中。因此，训练集、验证集和测试集包含了所有可用原因的不同描述。具体来说，代理在训练和验证期间看到所有原因，但不是所有描述。在我们的实验中，每个原因的$80\%$描述用于训练，其余描述则一分为二用于验证集和测试集。
- en: The RL-based agents were trained using subtasks from the designated training
    set being given a random subtask at each episode of interaction with the environment.
    At a given time step $t$, the agent took an action sampled from a softmax policy
    obtained from the Q-values of all of the actions available. The randomness of
    the softmax policy was controlled using a temperature decaying from 1 to 0.001
    linearly during the training. After each interaction, the agent was rewarded using
    a reward function that awarded the agent a positive reward of +1 when it successfully
    completed the posttest and penalizes with -1 otherwise. Moreover, each interaction
    of the agent was penalized by a small negative reward of -0.01.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于RL的代理通过在每次与环境交互的回合中随机获得一个子任务来进行训练。在给定的时间步$t$，代理从所有可用动作的Q值中通过softmax策略采样一个动作。softmax策略的随机性通过在训练期间从1到0.001线性衰减的温度进行控制。每次交互后，代理会根据奖励函数获得+1的正奖励，如果成功完成了后测，否则会受到-1的惩罚。此外，每次代理的交互会受到-0.01的小负奖励。
- en: Following each iteration of training, these agents underwent an evaluation phase
    using subtasks from the validation set. The iteration that yielded the highest
    average Post-test Performance Score on the subtasks in validation set was used
    for testing and also served as the foundation for the RL component within the
    LLM-assisted RL agents.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练迭代后，这些代理会使用来自验证集的子任务进行评估。产生的平均后测表现评分最高的迭代被用于测试，并且也作为LLM辅助RL代理中RL组件的基础。
- en: The agents that had an LLM involved in their structures used the GPT-4 model.
    The LLM-based agents initially gain experience through interactions within the
    training subtasks. This acquired experience is subsequently leveraged during their
    engagement with the test subtasks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 参与其结构中的代理使用了GPT-4模型。这些基于LLM的代理最初通过与训练子任务的互动获得经验。随后，这些获得的经验在他们处理测试子任务时被加以利用。
- en: '5.3 RQ1: Efficacy of Different Agent Types'
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '5.3 RQ1: 不同代理类型的有效性'
- en: 'In our first analysis, we aimed to assess the agents’ efficacy in diagnostic
    dialogues and accuracy in diagnoses aggregated over all patients. Figure [5](#S5.F5
    "Figure 5 ‣ 5 Experimental Evaluation ‣ Towards Generalizable Agents in Text-Based
    Educational Environments: A Study of Integrating RL with LLMs") illustrates the
    Post-test Performance Score, Trajectory Quality Score, and Combined Score of the
    different agents.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一次分析中，我们旨在评估代理在诊断对话中的有效性以及在所有患者中汇总的诊断准确性。图[5](#S5.F5 "图5 ‣ 5 实验评估 ‣ 面向文本教育环境的通用代理：将RL与LLMs结合的研究")展示了不同代理的后测表现评分、轨迹质量评分和综合评分。
- en: '![Refer to caption](img/89d5c6a8db431d0da35085bdb3d66ce0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89d5c6a8db431d0da35085bdb3d66ce0.png)'
- en: 'Figure 6: Performance of reflective and none-reflective agents on PharmaSimText.
    Post-test Performance Score (left), Trajectory Quality Score (middle), and Combined
    Score (right) for none-reflective and reflective DA-RL, SA-RL, and LLM-based agents.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：PharmaSimText上反射性和非反射性代理的表现。反射性和非反射性DA-RL、SA-RL以及基于LLM的代理的后测表现评分（左），轨迹质量评分（中），和综合评分（右）。
- en: \Description
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Performance of reflective and none-reflective agents on PharmaSimText
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: PharmaSimText上反射性和非反射性代理的表现
- en: We observed that the RL-based agent achieved a high Post-test Performance Score,
    indicating its ability to arrive at the correct diagnosis through a process of
    trial and error. However, this agent’s approach often lacked the depth and nuance
    of a meaningful diagnostic conversation, reflected in its low Trajectory Quality
    Score. This observation is probably due to its lack of background knowledge and
    common sense reasoning. Conversely, the LLM-based agent exhibited a superior capacity
    for engaging in meaningful diagnostic dialogues, reflected in a higher Trajectory
    Quality Score. However, the LLM-based agent exhibited a lower Post-test Performance
    Score than the RL-based agent, indicating that its ability to consistently reach
    the correct diagnosis is inferior compared to the RL-based agent.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到基于RL的代理获得了较高的后测表现评分，表明其通过反复试错的过程能够得出正确的诊断。然而，该代理的方式通常缺乏有意义的诊断对话的深度和细致度，这在其低轨迹质量评分中得以体现。这一观察可能由于其缺乏背景知识和常识推理。相反，基于LLM的代理展现了更强的进行有意义诊断对话的能力，体现在更高的轨迹质量评分上。然而，LLM-based代理的后测表现评分低于RL-based代理，表明其一致性地得出正确诊断的能力不如RL-based代理。
- en: In examining the LLM-assisted RL agents, both DA-RL and SA-RL agents surpassed
    the LLM-based agent in Post-test Performance Score, indicating that integrating
    LLM with RL generally improves diagnostic precision of purely LLM-based agents.
    Notably, the SA-RL agent exhibited superior Post-test Performance Score closely
    mirroring that of the RL-based agent. The DA-RL’s relative under-performance may
    have stemmed from its longer trajectories compared to the RL-based agent, leading
    to unfamiliar states where the DRRN struggled to provide accurate diagnoses, thereby
    affecting the DA-RL’s RL-driven suggestions. Furthermore, in terms of engaging
    in quality diagnostic dialogues, the SA-RL agent was also superior to the DA-RL
    agent. This superiority is likely due to the RL framework’s preference for shorter,
    more direct solutions, which reduced the action quality suggested by the DRRN
    in prolonged interactions. This effect was more pronounced in the DA-RL agent,
    potentially constraining the quality of diagnostic conversations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在检视LLM辅助的RL代理时，DA-RL和SA-RL代理在**后测表现分数**上都超过了基于LLM的代理，表明将LLM与RL结合通常能提升纯LLM代理的诊断精确度。值得注意的是，SA-RL代理表现出的**后测表现分数**接近RL基于的代理。DA-RL相对较差的表现可能源于其比基于RL的代理更长的轨迹，导致在DRRN难以提供准确诊断的陌生状态中，从而影响了DA-RL的RL驱动建议。此外，在进行高质量诊断对话方面，SA-RL代理也优于DA-RL代理。这种优势可能是因为RL框架更倾向于短期、直接的解决方案，从而减少了DRRN在较长互动中提出的行动质量。这种效果在DA-RL代理中更为显著，可能限制了诊断对话的质量。
- en: In the comparison of the agents in the Combined Score, the SA-RL agent emerged
    as the standout performer. Unlike its counterparts, the SA-RL agent adeptly navigated
    the dual challenges posed by the benchmark, demonstrating both a high conversation
    quality and diagnostic accuracy. This achievement highlights the SA-RL agent’s
    unique capacity to capture the strengths of both RL-based and LLM-based agents
    through the addition of suggestion-based assistance from LLMs to the RL agents’
    decision-making process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在结合分数的代理比较中，SA-RL代理表现突出。与其他代理不同，SA-RL代理熟练地应对了基准测试所带来的双重挑战，展示了高对话质量和诊断准确性。这一成就突显了SA-RL代理通过将LLM的建议辅助融入RL代理的决策过程，独特地捕捉了RL和LLM代理的优点。
- en: To further investigate the results, we performed additional statistical tests.
    A Kruskal-Wallis test shows significant differences between the agents for the
    Trajectory Quality Score and Combined Score $(p_{trajectory}<0.0001\text{ and
    }p_{combined}<0.001)$. Post-hoc comparisons using Mann-Whitney U tests with a
    Benjamini-Hochberg correction for the Combined Score indicate significant differences
    between 5 out of 6 pairs of agents supporting prior findings. For instance, the
    comparison between RL-based agent and SA-RL agent resulted in a p-value smaller
    than 0.01, and for the comparison between SA-RL agent and LLM-based agent the
    p-value was smaller than 0.05.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为进一步探究结果，我们进行了额外的统计测试。Kruskal-Wallis测试显示在轨迹质量分数和结合分数上，代理之间存在显著差异 $(p_{trajectory}<0.0001\text{
    和 }p_{combined}<0.001)$。使用Mann-Whitney U测试和Benjamini-Hochberg修正的后续比较显示，在6对代理中的5对之间存在显著差异，支持了先前的发现。例如，RL基于的代理和SA-RL代理之间的比较结果p值小于0.01，而SA-RL代理和LLM基于的代理之间的比较结果p值小于0.05。
- en: In summary, the experimental outcomes highlight distinct strengths and weaknesses
    among the agents. The RL-based agent demonstrated proficiency in achieving a high
    Post-test Performance Score score, but was hindered in engaging in effective diagnostic
    dialogues due to limited background knowledge. Conversely, the LLM-based agent
    excelled in conducting high-quality conversations by leveraging its extensive
    knowledge base, though with less accuracy in diagnoses. The hybrid LLM-assisted
    RL agents, DA-RL and SA-RL, outperformed the LLM-based agent in diagnostic precision
    and surpassed the RL-based agent in dialogue quality. The SA-RL agent achieved
    both a high conversation quality and diagnostic accuracy, illustrating its effective
    integration of LLM and RL capabilities.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，实验结果突显了各个代理之间的不同优点和缺点。基于RL的代理在获得高**后测表现分数**方面表现出色，但由于背景知识有限，无法进行有效的诊断对话。相反，基于LLM的代理在利用其广泛的知识库进行高质量对话方面表现优异，但在诊断准确性上较差。混合型LLM辅助RL代理DA-RL和SA-RL在诊断精确度上超过了基于LLM的代理，并在对话质量上超越了基于RL的代理。SA-RL代理在对话质量和诊断准确性上都表现出色，展示了其有效整合LLM和RL能力的特点。
- en: '![Refer to caption](img/78ef83b749f04742289d49f4b82332ae.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78ef83b749f04742289d49f4b82332ae.png)'
- en: 'Figure 7: Performance of different agents in interaction with different patients.
    Post-test Performance Score (left), Trajectory Quality Score (middle), and Combined
    Score (right) for RL-based and reflective SA-RL, DA-RL, and LLM-based agents.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：不同代理在与不同患者互动中的表现。RL基础代理和反思SA-RL、DA-RL、LLM基础代理的后测性能得分（左）、轨迹质量得分（中）和综合得分（右）。
- en: \Description
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Performance of different agents in interaction with different patients
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 不同代理与不同患者互动的表现
- en: '5.4 RQ2: Effect of Reflective Prompting'
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 RQ2：反思提示的效果
- en: 'In our second analysis, we aimed to explore the impact of reflective prompting
    on the efficacy of LLM-involved agents. As described in Section [4](#S4 "4 Agents
    for PharmaSimText ‣ Towards Generalizable Agents in Text-Based Educational Environments:
    A Study of Integrating RL with LLMs"), none-reflective agents were limited to
    a single attempt, whereas reflective agents were given three attempts per subtask
    with opportunities for reflection. Figure [6](#S5.F6 "Figure 6 ‣ 5.3 RQ1: Efficacy
    of Different Agent Types ‣ 5 Experimental Evaluation ‣ Towards Generalizable Agents
    in Text-Based Educational Environments: A Study of Integrating RL with LLMs")
    illustrates the Post-test Performance Score, Trajectory Quality Score, and Combined
    Score for none-reflective and reflective LLM-assisted RL and LLM-based agents.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的第二次分析中，我们旨在探讨反思提示对涉及LLM的代理有效性的影响。如第[4](#S4 "4 Agents for PharmaSimText
    ‣ Towards Generalizable Agents in Text-Based Educational Environments: A Study
    of Integrating RL with LLMs")节所述，非反思代理仅限于单次尝试，而反思代理每个子任务有三次尝试机会，并提供了反思机会。图[6](#S5.F6
    "Figure 6 ‣ 5.3 RQ1: Efficacy of Different Agent Types ‣ 5 Experimental Evaluation
    ‣ Towards Generalizable Agents in Text-Based Educational Environments: A Study
    of Integrating RL with LLMs")展示了非反思和反思LLM辅助RL以及LLM基础代理的后测性能得分、轨迹质量得分和综合得分。'
- en: We observed a nuanced impact of reflective prompting on agent performance. Specifically,
    reflective prompting did not significantly impact the Combined Score of the purely
    LLM-based agent. For this agent, reflection led to shorter diagnostic conversations
    by eliminating what the agent considered redundant questions. However, this streamlining
    resulted in poorer conversation quality without significantly improving diagnosis
    accuracy, negating the potential diagnosis accuracy gains from reflection.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到反思提示对代理性能的影响是微妙的。具体而言，反思提示对纯LLM基础代理的综合得分没有显著影响。对于该代理，反思通过消除代理认为冗余的问题，导致了更短的诊断对话。然而，这种精简导致了对话质量下降，并未显著提高诊断准确性，抵消了反思带来的潜在诊断准确性提升。
- en: In contrast, the reflective process considerably enhanced the performance of
    the hybrid LLM-assisted RL agents. This improvement can be attributed to the reflective
    phase allowing the agents to reassess and refine their decision-making processes,
    leading to more accurate diagnoses. The performance boost was particularly notable
    in SA-RL agents, most likely due to their reliance on the LLM for suggesting potential
    actions during the interaction phase. This reliance provided a broader scope for
    reflection to influence decision-making, unlike DA-RL agents where decisions were
    more heavily influenced by the RL-based agent. This finding underscores the value
    of incorporating reflective mechanisms in enhancing the capabilities of hybrid
    agents.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，反思过程显著提高了混合LLM辅助RL代理的性能。这一改进可以归因于反思阶段使代理能够重新评估和优化其决策过程，从而实现更准确的诊断。特别是在SA-RL代理中，这种性能提升尤为显著，这很可能是因为它们在互动阶段依赖LLM来建议潜在的行动。这种依赖提供了更广泛的反思范围来影响决策，而DA-RL代理则更多地受到RL基础代理的影响。这一发现突显了在提高混合代理能力方面纳入反思机制的价值。
- en: In summary, our experiment revealed that reflective prompting has a different
    effect on LLM-based and LLM-assisted RL agents. For the LLM-based agents, reflective
    prompting led to shorter and lower quality diagnostic conversations, with no significant
    improvement in diagnostic accuracy. On the other hand, the LLM-assisted RL agents
    benefited from reflection, showing improvements in diagnostic accuracy. This enhancement
    was more pronounced for SA-RL agents, which rely more on LLM suggestions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的实验揭示了反思提示对LLM基础代理和LLM辅助RL代理的不同影响。对于LLM基础代理，反思提示导致了更短且质量较低的诊断对话，没有显著提高诊断准确性。另一方面，LLM辅助RL代理从反思中受益，显示出诊断准确性的改善。这一提升在SA-RL代理中尤为明显，因为这些代理更依赖LLM的建议。
- en: '![Refer to caption](img/49ce5bb397e864345157e1cebc284e66.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/49ce5bb397e864345157e1cebc284e66.png)'
- en: 'Figure 8: Example diagnostic conversations conducted by the RL-based (top)
    and SA-RL agents (bottom) with the patient with joint pains in a test subtask
    with Osteoarthritis as the most probable cause.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：由RL基础代理（上图）和SA-RL代理（下图）与关节疼痛的患者进行的示例诊断对话，测试子任务中最可能的原因是骨关节炎。
- en: \Description
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \说明
- en: Example diagnostic conversations with the patient with joint pains in a test
    subtask with Osteoarthritis as the most probable cause. RL-based Agent’s Conversation
    and SA-RL Agent’s Conversation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例诊断对话：在测试子任务中，针对关节疼痛的患者，最可能的原因是骨关节炎。RL基础代理的对话和SA-RL代理的对话
- en: '5.5 RQ3: Agent Efficacy for Different Patients'
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '5.5 RQ3: 不同患者的代理效能'
- en: 'In our final analysis, we investigated the performance of our agents across
    the different patients. Figure [7](#S5.F7 "Figure 7 ‣ 5.3 RQ1: Efficacy of Different
    Agent Types ‣ 5 Experimental Evaluation ‣ Towards Generalizable Agents in Text-Based
    Educational Environments: A Study of Integrating RL with LLMs") illustrates the
    Post-test Performance Score, Trajectory Quality Score, and Combined Score for
    each patient averaged over all of the subtasks available for that patient in PharmaSimText
    for the RL-based agent as well as the reflective SA-RL, DA-RL, and LLM-based agents.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的最终分析中，我们调查了不同患者中代理的表现。图[7](#S5.F7 "Figure 7 ‣ 5.3 RQ1: Efficacy of Different
    Agent Types ‣ 5 Experimental Evaluation ‣ Towards Generalizable Agents in Text-Based
    Educational Environments: A Study of Integrating RL with LLMs")展示了每位患者的后测表现分数、轨迹质量分数和综合分数，这些数据是对PharmaSimText中RL基础代理以及反射SA-RL、DA-RL和LLM基础代理的所有子任务的平均值。'
- en: We again observed that the RL-based agent showed superior Post-test Performance
    Score across all patients, while the LLM-based agent was not able to identify
    all causes correctly for five out of the nine patients. The LLM-assisted RL agents
    managed to overcome this limitation, with the SA-RL agent showing superior performance
    than the DA-RL agent. The opposite result was found for the Trajectory Quality
    Score. While the LLM-based agents conducted high-quality diagnostic dialogues,
    the RL-based agent exhibited a suboptimal Trajectory Quality Score for all of
    the patients, often incorporating merely one or two key questions within its diagnostic
    conversations, highlighting the extent of its deviation from an effective diagnostic
    interaction. Again, the LLM-assisted RL agents overcame this limitation, with
    the SA-RL agent generally showing the highest Trajectory Quality Score scores.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次观察到，RL基础代理在所有患者中表现出优越的后测表现分数，而LLM基础代理在九名患者中的五名患者未能正确识别所有原因。LLM辅助的RL代理克服了这一限制，SA-RL代理的表现优于DA-RL代理。在轨迹质量分数方面，结果则相反。尽管LLM基础代理进行了高质量的诊断对话，但RL基础代理在所有患者中表现出的轨迹质量分数不理想，通常仅在其诊断对话中包含一两个关键问题，突显了其与有效诊断互动的偏差。LLM辅助的RL代理再次克服了这一限制，SA-RL代理通常显示出最高的轨迹质量分数。
- en: Our examination of the Combined Score revealed that, except for the SA-RL agent,
    most agents encounter difficulties in scenarios related to Skin and Eye conditions.
    A closer inspection of their Post-test Performance Score and Trajectory Quality
    Score metrics suggested that these agents face challenges in different facets
    of the scenarios related to these specific patients. A particularly noteworthy
    observation is the superior performance of the SA-RL agent, which overcomes the
    limitations of purely RL-based and LLM-based agents across all patient categories.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对综合分数的检查揭示，除了SA-RL代理，大多数代理在与皮肤和眼部相关的场景中遇到困难。对他们的后测表现分数和轨迹质量分数指标的进一步检查表明，这些代理在与这些特定患者相关的场景中的不同方面面临挑战。特别值得注意的是，SA-RL代理在所有患者类别中克服了纯RL基础和LLM基础代理的局限性，表现出优越的性能。
- en: 'Given the inferior performance of the RL-based agent in the Trajectory Quality
    Score, we examined the dialogues generated by the RL-based agent and the SA-RL
    agent within an identical scenario that resulted in a correct diagnosis, as illustrated
    in Fig. [8](#S5.F8 "Figure 8 ‣ 5.4 RQ2: Effect of Reflective Prompting ‣ 5 Experimental
    Evaluation ‣ Towards Generalizable Agents in Text-Based Educational Environments:
    A Study of Integrating RL with LLMs"). This comparison reveals a pronounced contrast
    in the conversational dynamics of these two agents. The dialogue led by the SA-RL
    agent exhibits a flow that is markedly more reminiscent of human-like interaction,
    in contrast to the RL-based agent’s brief conversation. Notably, the RL-based
    agent’s approach is characterized by posing a single key question before directly
    drawing a conclusion. In comparison, the SA-RL agent engages in a more thorough
    inquiry, covering a broader spectrum of key questions in a logically sequential
    manner.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于 RL 基础代理在轨迹质量评分中的较差表现，我们检查了 RL 基础代理和 SA-RL 代理在相同情境下生成的对话，该情境中获得了正确诊断，如图 [8](#S5.F8
    "图 8 ‣ 5.4 RQ2: 反思提示的影响 ‣ 5 实验评估 ‣ 迈向文本基础教育环境的通用代理：RL 与 LLM 的整合研究") 所示。这种比较揭示了这两种代理在对话动态上的显著对比。SA-RL
    代理主导的对话展现出明显更像人类互动的流畅性，而 RL 基础代理则表现为简短对话。值得注意的是，RL 基础代理的做法是提出一个关键问题后直接得出结论。相比之下，SA-RL
    代理进行更全面的询问，以逻辑顺序涵盖更广泛的关键问题。'
- en: In summary, the hybrid LLM-assisted RL agents manage to ovecome the limitations
    of solely RL-based and LLM-based agents, with the SA-RL agent demonstrating superior
    performance across all patients. The RL-based agent exhibits a behavior characterized
    by short conversation, limiting interactions to very few key questions, while
    the SA-RL agent follows a more human-like behavior.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，混合 LLM 辅助的 RL 代理克服了仅依赖 RL 基础和 LLM 基础代理的局限性，其中 SA-RL 代理在所有患者中表现出优越的性能。RL
    基础的代理表现出短暂对话的特征，将互动限制为极少数关键问题，而 SA-RL 代理则表现出更类似人类的行为。
- en: 6 Discussion and Conclusion
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与结论
- en: In this paper, we explored integration of RL and LLMs to enhance learner models
    in educational technologies. While RL-based agents show promise in structured
    learning tasks, they struggle with open-ended environments and skill generalization.
    Conversely, LLMs excel in generating student-like responses, but fail in constrained
    action spaces. By combining RL and LLMs, we aimed to develop more generalizable
    agents for text-based educational settings. We assessed our agents, including
    RL-based, LLM-based, and hybrid models, on their ability to conduct diagnostic
    conversations and make accurate diagnoses in our novel benchmark PharmaSimText.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探索了 RL 和 LLM 的整合，以提升教育技术中的学习者模型。虽然 RL 基础的代理在结构化学习任务中展现出前景，但在开放性环境和技能泛化方面存在困难。相反，LLM
    在生成类似学生的响应方面表现出色，但在受限的行动空间中却表现不佳。通过结合 RL 和 LLM，我们旨在开发出更具通用性的代理，用于基于文本的教育环境。我们评估了包括
    RL 基础、LLM 基础和混合模型在内的代理，在我们的新基准 PharmaSimText 中，评估它们进行诊断对话和做出准确诊断的能力。
- en: 'Specifically, we were interested in answering the following three research
    questions: Which agent type demonstrates overall superior performance in conducting
    effective diagnostic conversations and achieving accurate diagnoses for all available
    patients (RQ1)? How does reflective prompting influence the diagnostic performance
    and conversation quality of LLM-involved agents (RQ2)? How do diagnostic performance
    and conversation quality vary among different agent types across diverse patients
    (RQ3)?'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们有兴趣回答以下三个研究问题：哪种代理类型在进行有效的诊断对话和为所有可用患者做出准确诊断方面表现整体优越（RQ1）？反思提示如何影响涉及
    LLM 的代理的诊断性能和对话质量（RQ2）？不同代理类型在不同患者中，诊断性能和对话质量如何变化（RQ3）？
- en: 'To address our first research question, we assessed four agents: one RL-based,
    one LLM-based, and two integrating LLMs with RL, in rephrased versions of the
    scenarios related to different patients in PharmaSimText that the agents had not
    seen before. Effective diagnostic conversations require high-quality conversations
    and accurate diagnoses. The RL agent excelled in finding the correct diagnosis
    but struggled in comprehensive diagnostic dialogues due to its limited knowledge.
    The LLM agent was adept in high-quality diagnostic conversations but tended to
    misdiagnose patients. LLM-RL integrations were able to address these limitations
    by enhancing the diagnostic accuracy compared to the LLM-based agent and the conversation
    quality compared to the RL-based agent. Among all agents, the SA-RL agent achieved
    the best combination of diagnostic accuracy and conversation quality.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解答第一个研究问题，我们评估了四种代理：一个基于RL的代理、一个基于LLM的代理和两个将LLM与RL整合的代理，在PharmaSimText中与不同患者相关的场景的重述版本中，这些代理之前未见过。有效的诊断对话需要高质量的对话和准确的诊断。RL代理在找到正确诊断方面表现出色，但由于知识有限，在全面的诊断对话中表现不佳。LLM代理擅长高质量的诊断对话，但容易误诊。LLM-RL整合代理通过提高诊断准确性（相比于LLM基础代理）和对话质量（相比于RL基础代理）解决了这些局限性。在所有代理中，SA-RL代理在诊断准确性和对话质量的结合上表现最佳。
- en: The second research question investigated the benefits of reflective prompting
    of the LLMs in the LLM-involved agents. To answer this question, we compared the
    reflective versions of three LLM-involved agents with their none-reflective counterparts.
    In prior works, reflection showed noticeable improvements in task completion of
    prompted LLMs [[53](#bib.bib53), [52](#bib.bib52)]. Therefore, we hypothesized
    a noticeable drop in the performance of the LLM-involved agents after confining
    them to only one trial. Our results showed a mixed effect for reflection in the
    solely LLM-based agent and the hybrid agents. For the LLM-based agent, the reflection
    improved the diagnostic accuracy of the agent, but it decreased the quality of
    the agent’s conversation by shortening its trajectory. For the hybrid agents,
    the reflective process increased the diagnostic accuracy. We therefore conclude
    that the effect of reflective prompting depends on the agent type.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个研究问题调查了在涉及LLM的代理中，反思性提示的好处。为了解答这个问题，我们比较了三种涉及LLM的代理的反思性版本与其非反思性对应版本。在先前的研究中，反思性显示出对提示LLM的任务完成有显著改善[[53](#bib.bib53),
    [52](#bib.bib52)]。因此，我们假设在仅允许进行一次试验的情况下，LLM涉及代理的性能会显著下降。我们的结果显示，反思性在纯LLM基础的代理和混合代理中产生了混合效应。对于LLM基础的代理，反思性提高了代理的诊断准确性，但通过缩短其对话轨迹，降低了代理对话的质量。对于混合代理，反思性过程提高了诊断准确性。因此，我们得出结论：反思性提示的效果依赖于代理类型。
- en: To address the third research question, we analyzed the agents over the three
    metrics for each of the patients separately. We observed that the agents did not
    struggle with similar patients. In our subsequent analysis, we looked at an example
    of the conversations done by the RL-based agent and the SA-RL agent, and we observed
    that while the RL-based agent conversation seemed rushed, the SA-RL’s conversation
    seemed human-like and followed a sequential logic.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解答第三个研究问题，我们对每位患者的三个指标分别进行了分析。我们观察到，代理在处理类似患者时没有遇到困难。在随后的分析中，我们查看了RL基础代理和SA-RL代理的对话示例，发现虽然RL基础代理的对话显得有些仓促，但SA-RL的对话则显得更像人类，并遵循了顺序逻辑。
- en: 'One of the limitations of this work is the focus on generalization at a single
    level of rephrased versions of the scenarios. A few possible generalization levels
    available PharmaSimText are: generalizing to a new wording of a known scenario
    (wording generalization), to a new diagnosis of a known patient (subtask generalization),
    and to a new patient (task generalization). Our presented experiments are limited
    to the wording generalization. Further research should be done within different
    generalization levels to evaluate current agents and propose new agent frameworks
    that consider the models’ confidence in integration and leverage LLM insights
    for rapid adaptation of RL-based agents to new tasks. Moreover, our proposed reflective
    process showed limitations in improving the LLM-based agents. This suggests a
    need for further research for improved reflection in the interactive format of
    the PharmaSimText benchmark. Moreover, future research should consider evaluating
    the similarity of behavior of these agents to human students to further facilitate
    their use cases such as evaluating learning environments and collaborative learning.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的一个限制是专注于场景的单一级别的重述版本的泛化。PharmaSimText 中可能的泛化级别包括：将已知场景的新措辞（措辞泛化）、对已知患者的新诊断（子任务泛化）和对新患者（任务泛化）。我们展示的实验仅限于措辞泛化。进一步的研究应在不同的泛化级别中进行，以评估当前的代理人并提出考虑模型自信度的新的代理人框架，并利用LLM的见解迅速适应RL基础的代理人到新任务。此外，我们提出的反思过程在改进基于LLM的代理人方面显示了局限性。这表明需要进一步的研究以改进
    PharmaSimText 基准的互动格式中的反思。此外，未来的研究应考虑评估这些代理人的行为与人类学生的相似性，以进一步促进其应用，如评估学习环境和协作学习。
- en: To conclude, the proposed LLM integration approach represents a promising step
    towards agents with generalization capabilities in open-ended text-based educational
    environments. Furthermore, our implemented benchmark facilitates further research
    in developing agents with generalization capabilities at a higher level.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，提出的 LLM 集成方法代表了朝着具有泛化能力的代理人迈出的有希望的一步。此外，我们实施的基准测试促进了进一步研究，以开发具有更高泛化能力的代理人。
- en: 7 Acknowledgements
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: We thank Dr. Jibril Frej and Dr. Ethan Prihar for their expertise and support.
    This project was substantially financed by the Swiss State Secretariat for Education,
    Research and Innovation (SERI).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Dr. Jibril Frej 和 Dr. Ethan Prihar 的专业知识和支持。此项目的资金主要由瑞士国家教育、研究与创新秘书处（SERI）提供。
- en: References
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Tanja Käser and Giora Alexandron. Simulated Learners in Educational Technology:
    A Systematic Literature Review and a Turing-like Test. International Journal of
    Artificial Intelligence in Education (IJAIED), pages 1–41, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Tanja Käser 和 Giora Alexandron。《教育技术中的模拟学习者：系统文献综述及类似图灵测试》。国际人工智能教育期刊（IJAIED），第1–41页，2023年。'
- en: '[2] Kevin Robinson, Keyarash Jahanian, and Justin Reich. Using Online Practice
    Spaces to Investigate Challenges in Enacting Principles of Equitable Computer
    Science Teaching. In Proceedings of the Technical Symposium on Computer Science
    Education (SIGCSE), pages 882–887, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Kevin Robinson、Keyarash Jahanian 和 Justin Reich。《使用在线实践空间调查实施公平计算机科学教学原则中的挑战》。见《计算机科学教育技术研讨会论文集》（SIGCSE），第882–887页，2018年。'
- en: '[3] Daniel Dickison, Steven Ritter, Tristan Nixon, Thomas K. Harris, Brendon
    Towle, R. Charles Murray, and Robert G. M. Hausmann. Predicting the Effects of
    Skill Model Changes on Student Progress. In Proceedings of the International Conference
    on Intelligent Tutoring Systems (ITS), Part II, pages 300–302, 2010.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Daniel Dickison、Steven Ritter、Tristan Nixon、Thomas K. Harris、Brendon Towle、R.
    Charles Murray 和 Robert G. M. Hausmann。《预测技能模型变化对学生进步的影响》。见《国际智能辅导系统会议论文集》（ITS），第二部分，第300–302页，2010年。'
- en: '[4] Tanya Nazaretsky, Sara Hershkovitz, and Giora Alexandron. Kappa Learning:
    A New Item-Similarity Method for Clustering Educational Items from Response Data.
    In Proceedings of the International Conference on Educational Data Mining (EDM),
    2019.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tanya Nazaretsky、Sara Hershkovitz 和 Giora Alexandron。《Kappa学习：一种用于从响应数据中聚类教育项目的新项目相似性方法》。见《国际教育数据挖掘会议论文集》（EDM），2019年。'
- en: '[5] Christopher J. MacLellan, Erik Harpstead, Rony Patel, and Kenneth R. Koedinger.
    The Apprentice Learner Architecture: Closing the Loop between Learning Theory
    and Educational Data. In Proceedings of the International Conference on Educational
    Data Mining (EDM), pages 151–158, 2016.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 克里斯托弗·J·麦克莱伦、埃里克·哈普斯特德、罗尼·帕特尔和肯尼斯·R·科丁格。《学徒学习者架构：缩小学习理论与教育数据之间的差距》。发表于国际教育数据挖掘会议（EDM）论文集，第151–158页，2016年。'
- en: '[6] Lena Pareto. A Teachable Agent Game Engaging Primary School Children to
    Learn Arithmetic Concepts and Reasoning. International Journal of Artificial Intelligence
    in Education (IJAIED), 24(3):251–283, 2014.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 莱娜·帕雷托。《一款教学代理游戏，吸引小学生学习算术概念和推理》。人工智能教育国际期刊（IJAIED），24(3):251–283，2014年。'
- en: '[7] Adish Singla, Anna N. Rafferty, Goran Radanovic, and Neil T. Heffernan.
    Reinforcement Learning for Education: Opportunities and Challenges. CoRR, abs/2107.08828,
    2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 阿迪什·辛格拉、安娜·N·拉弗提、戈兰·拉达诺维奇和尼尔·T·赫弗南。《教育中的强化学习：机会与挑战》。CoRR，abs/2107.08828，2021年。'
- en: '[8] Jacob Whitehill and Javier R. Movellan. Approximately Optimal Teaching
    of Approximately Optimal Learners. IEEE Transactions of Learning Technololy, 11(2):152–164,
    2018.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 雅各布·怀特希尔和哈维尔·R·莫维伦。《对大致最优学习者的近似最优教学》。IEEE学习技术学报，11(2):152–164，2018年。'
- en: '[9] Song Ju, Min Chi, and Guojing Zhou. Pick the Moment: Identifying Critical
    Pedagogical Decisions Using Long-Short Term Rewards. In Proceedings of the International
    Conference on Educational Data Mining (EDM), 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 宋炬、闵驰和周国晶。《选择时刻：利用长短期奖励识别关键教学决策》。发表于国际教育数据挖掘会议（EDM）论文集，2020年。'
- en: '[10] Guojing Zhou, Hamoon Azizsoltani, Markel Sanz Ausin, Tiffany Barnes, and
    Min Chi. Hierarchical Reinforcement Learning for Pedagogical Policy Induction.
    In Proceedings of the International Conference on Artificial Intelligence in Education
    (AIED), pages 544–556, 2019.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 周国晶、哈穆恩·阿齐索尔塔尼、马克尔·桑兹·奥辛、蒂芙尼·巴恩斯和闵驰。《用于教学政策引导的层次化强化学习》。发表于人工智能教育国际会议（AIED）论文集，第544–556页，2019年。'
- en: '[11] Anna N. Rafferty, Emma Brunskill, Thomas L. Griffiths, and Patrick Shafto.
    Faster Teaching via POMDP Planning. Cognitive Science, 40(6):1290–1332, 2016.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 安娜·N·拉弗提、艾玛·布伦斯基尔、托马斯·L·格里菲斯和帕特里克·肖夫托。《通过POMDP规划加速教学》。认知科学，40(6):1290–1332，2016年。'
- en: '[12] Aleksandr Efremov, Ahana Ghosh, and Adish Singla. Zero-shot Learning of
    Hint Policy via Reinforcement Learning and Program Synthesis. In Proceedings of
    the International Conference on Educational Data Mining (EDM), 2020.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 亚历山大·埃弗雷莫夫、阿哈纳·戈什和阿迪什·辛格拉。《通过强化学习和程序合成进行零样本提示策略学习》。发表于国际教育数据挖掘会议（EDM）论文集，2020年。'
- en: '[13] Tiffany Barnes and John C. Stamper. Toward Automatic Hint Generation for
    Logic Proof Tutoring Using Historical Student Data. In Proceedings of the International
    Conference on Intelligent Tutoring Systems (ITS), pages 373–382, 2008.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 蒂芙尼·巴恩斯和约翰·C·斯坦珀。《基于历史学生数据的逻辑证明辅导自动提示生成》。发表于国际智能辅导系统会议（ITS）论文集，第373–382页，2008年。'
- en: '[14] Umair Z. Ahmed, Maria Christakis, Aleksandr Efremov, Nigel Fernandez,
    Ahana Ghosh, Abhik Roychoudhury, and Adish Singla. Synthesizing Tasks for Block-based
    Programming. In Proceedings of Advances in Neural Information Processing Systems
    (NeurIPS), 2020.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 乌迈尔·Z·艾哈迈德、玛利亚·克里斯塔基斯、亚历山大·埃弗雷莫夫、奈杰尔·费尔南德斯、阿哈纳·戈什、阿比赫·罗伊乔杜里和阿迪什·辛格拉。《用于块状编程的任务合成》。发表于神经信息处理系统进展（NeurIPS）论文集，2020年。'
- en: '[15] Victor-Alexandru Padurean, Georgios Tzannetos, and Adish Singla. Neural
    Task Synthesis for Visual Programming. Transactions of Machine Learning Research
    (TMLR), 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 维克多·亚历山大·帕杜雷安、乔治奥斯·扎内托斯和阿迪什·辛格拉。《视觉编程的神经任务合成》。机器学习研究学报（TMLR），2024年。'
- en: '[16] Christopher J. MacLellan and Adit Gupta. Learning Expert Models for Educationally
    Relevant Tasks using Reinforcement Learning. In Proceedings of the International
    Conference on Educational Data Mining (EDM), 2021.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 克里斯托弗·J·麦克莱伦和阿迪特·古普塔。《使用强化学习为教育相关任务学习专家模型》。发表于国际教育数据挖掘会议（EDM）论文集，2021年。'
- en: '[17] Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet
    Kohli. Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis.
    In Proceedings of the International Conference on Learning Representations (ICLR),
    2018.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 鲁迪·布纽尔、马修·J·豪斯克内赫特、雅各布·德夫林、里沙布·辛格和普什米特·科利。《利用语法和强化学习进行神经程序合成》。发表于国际学习表征会议（ICLR）论文集，2018年。'
- en: '[18] Reid McIlroy-Young, Siddhartha Sen, Jon M. Kleinberg, and Ashton Anderson.
    Aligning Superhuman AI with Human Behavior: Chess as a Model System. In Proceedings
    of the SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 1677–1687,
    2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Reid McIlroy-Young, Siddhartha Sen, Jon M. Kleinberg 和 Ashton Anderson。将超人类
    AI 与人类行为对齐：以象棋为模型系统。发表于知识发现与数据挖掘会议（KDD）论文集，第1677–1687页，2020年。'
- en: '[19] Paul Denny, Sumit Gulwani, Neil T. Heffernan, Tanja Käser, Steven Moore,
    Anna N. Rafferty, and Adish Singla. Generative AI for Education (GAIED): Advances,
    Opportunities, and Challenges. CoRR, abs/2402.01580, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Paul Denny, Sumit Gulwani, Neil T. Heffernan, Tanja Käser, Steven Moore,
    Anna N. Rafferty 和 Adish Singla。教育领域的生成 AI（GAIED）：进展、机遇与挑战。CoRR，abs/2402.01580，2024年。'
- en: '[20] Tom B. Brown et al. Language Models are Few-Shot Learners. In Proceedings
    of the Annual Conference on Neural Information Processing Systems (NeurIPS), 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Tom B. Brown 等。语言模型是少量样本学习者。发表于神经信息处理系统年会（NeurIPS）论文集，2020年。'
- en: '[21] Sébastien Bubeck et al. Sparks of Artificial General Intelligence: Early
    Experiments with GPT-4. CoRR, abs/2303.12712, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Sébastien Bubeck 等。人工通用智能的火花：GPT-4 的早期实验。CoRR，abs/2303.12712，2023年。'
- en: '[22] Archana Praveen Kumar, Ashalatha Nayak, Manjula Shenoy K, Chaitanya, and
    Kaustav Ghosh. A Novel Framework for the Generation of Multiple Choice Question
    Stems Using Semantic and Machine-Learning Techniques. International Journal of
    Artificial Intelligence in Education (IJAIED), pages 1–44, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Archana Praveen Kumar, Ashalatha Nayak, Manjula Shenoy K, Chaitanya 和
    Kaustav Ghosh。使用语义和机器学习技术生成多项选择题题干的新框架。《国际人工智能教育杂志》（IJAIED），第1–44页，2023年。'
- en: '[23] Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen. Automatic Generation
    of Programming Exercises and Code Explanations Using Large Language Models. In
    Proceedings of the Conference on International Computing Education Research (ICER),
    2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Sami Sarsa, Paul Denny, Arto Hellas 和 Juho Leinonen。使用大型语言模型自动生成编程练习和代码解释。发表于国际计算教育研究会议（ICER）论文集，2022年。'
- en: '[24] Tung Phung, Victor-Alexandru Padurean, José Cambronero, Sumit Gulwani,
    Tobias Kohn, Rupak Majumdar, Adish Singla, and Gustavo Soares. Generative AI for
    Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors. In Proceedings
    of the Conference on International Computing Education Research - Volume 2 (ICER
    V.2), 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Tung Phung, Victor-Alexandru Padurean, José Cambronero, Sumit Gulwani,
    Tobias Kohn, Rupak Majumdar, Adish Singla 和 Gustavo Soares。编程教育中的生成 AI：ChatGPT、GPT-4
    和人工导师的基准测试。发表于国际计算教育研究会议 - 卷2（ICER V.2）论文集，2023年。'
- en: '[25] Hunter McNichols, Wanyong Feng, Jaewook Lee, Alexander Scarlatos, Digory
    Smith, Simon Woodhead, and Andrew Lan. Automated Distractor and Feedback Generation
    for Math Multiple-choice Questions via In-context Learning. NeurIPS’23 Workshop
    on Generative AI for Education (GAIED), 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Hunter McNichols, Wanyong Feng, Jaewook Lee, Alexander Scarlatos, Digory
    Smith, Simon Woodhead 和 Andrew Lan。通过上下文学习自动生成数学多项选择题的干扰项和反馈。NeurIPS’23 教育生成 AI
    研讨会（GAIED），2023年。'
- en: '[26] Maciej Pankiewicz and Ryan Shaun Baker. Large Language Models (GPT) for
    Automating Feedback on Programming Assignments. CoRR, abs/2307.00150, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Maciej Pankiewicz 和 Ryan Shaun Baker。大型语言模型（GPT）用于自动化编程作业反馈。CoRR，abs/2307.00150，2023年。'
- en: '[27] Arne Bewersdorff, Kathrin Seßler, Armin Baur, Enkelejda Kasneci, and Claudia
    Nerdel. Assessing Student Errors Experimentation Using Artificial Intelligence
    and Large Language Models: A Comparative Study with Human Raters. CoRR, abs/2308.06088,
    2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Arne Bewersdorff, Kathrin Seßler, Armin Baur, Enkelejda Kasneci 和 Claudia
    Nerdel。使用人工智能和大型语言模型评估学生错误实验：与人工评分者的比较研究。CoRR，abs/2308.06088，2023年。'
- en: '[28] Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, Kenneth R. Koedinger,
    and Vincent Aleven. Comparative Analysis of GPT-4 and Human Graders in Evaluating
    Praise Given to Students in Synthetic Dialogues. CoRR, abs/2307.02018, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, Kenneth R. Koedinger
    和 Vincent Aleven。GPT-4与人工评分者在评估合成对话中对学生的赞扬的比较分析。CoRR，abs/2307.02018，2023年。'
- en: '[29] Tung Phung, Victor-Alexandru Pădurean, Anjali Singh, Christopher Brooks,
    José Cambronero, Sumit Gulwani, Adish Singla, and Gustavo Soares. Automating Human
    Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation
    and GPT-3.5 Student Model for Hint Validation. In Proceedings of the International
    Learning Analytics and Knowledge Conference (LAK), 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Tung Phung, Victor-Alexandru Pădurean, Anjali Singh, Christopher Brooks,
    José Cambronero, Sumit Gulwani, Adish Singla 和 Gustavo Soares。自动化人类导师风格的编程反馈：利用
    GPT-4 导师模型生成提示和 GPT-3.5 学生模型验证提示。发表于国际学习分析与知识会议（LAK）论文集，2024年。'
- en: '[30] Zachary A. Pardos and Shreya Bhandari. Learning Gain Differences between
    ChatGPT and Human Tutor Generated Algebra Hints. CoRR, abs/2302.06871, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Zachary A. Pardos 和 Shreya Bhandari. ChatGPT 与人工导师生成的代数提示之间的学习收益差异。CoRR,
    abs/2302.06871，2023 年。'
- en: '[31] Anaïs Tack and Chris Piech. The AI Teacher Test: Measuring the Pedagogical
    Ability of Blender and GPT-3 in Educational Dialogues. In Proceedings of the International
    Conference on Educational Data Mining (EDM), 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Anaïs Tack 和 Chris Piech. AI 教师测试：衡量 Blender 和 GPT-3 在教育对话中的教学能力。在国际教育数据挖掘会议（EDM）论文集，2022
    年。'
- en: '[32] Unggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong, Haewon Jung, Gyuri Byun,
    Yunseo Lee, Jewoong Moon, Jieun Lim, and Hyeoncheol Kim. Generative Agent for
    Teacher Training: Designing Educational Problem-Solving Simulations with Large
    Language Model-based Agents for Pre-Service Teachers. NeurIPS’23 Workshop on Generative
    AI for Education (GAIED), 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Unggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong, Haewon Jung, Gyuri Byun,
    Yunseo Lee, Jewoong Moon, Jieun Lim 和 Hyeoncheol Kim. 教师培训生成代理：为预备教师设计基于大语言模型的教育问题解决模拟。NeurIPS’23
    教育生成 AI 研讨会（GAIED），2023 年。'
- en: '[33] Robin Schmucker, Meng Xia, Amos Azaria, and Tom Mitchell. Ruffle&Riley:
    Towards the Automated Induction of Conversational Tutoring Systems. NeurIPS’23
    Workshop on Generative AI for Education (GAIED), 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Robin Schmucker, Meng Xia, Amos Azaria 和 Tom Mitchell. Ruffle&Riley：自动化对话辅导系统的引导。NeurIPS’23
    教育生成 AI 研讨会（GAIED），2023 年。'
- en: '[34] Manh Hung Nguyen, Sebastian Tschiatschek, and Adish Singla. Large Language
    Models for In-Context Student Modeling: Synthesizing Student’s Behavior in Visual
    Programming. CoRR, abs/2310.10690, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Manh Hung Nguyen, Sebastian Tschiatschek 和 Adish Singla. 上下文中学生建模的大语言模型：综合学生在视觉编程中的行为。CoRR,
    abs/2310.10690，2023 年。'
- en: '[35] Julia M. Markel, Steven G. Opferman, James A. Landay, and Chris Piech.
    GPTeach: Interactive TA Training with GPT-based Students. In Proceedings of the
    Conference on Learning @ Scale (L@S), pages 226–236, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Julia M. Markel, Steven G. Opferman, James A. Landay 和 Chris Piech. GPTeach：基于
    GPT 的学生互动 TA 培训。在学习规模会议（L@S）论文集，页码 226–236，2023 年。'
- en: '[36] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu.
    ScienceWorld: Is Your Agent Smarter than a 5th Grader? In Proceedings of the Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pages 11279–11298,
    2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre Côté 和 Prithviraj Ammanabrolu.
    ScienceWorld：你的代理比 5 年级学生更聪明吗？在自然语言处理实证方法会议（EMNLP）论文集，页码 11279–11298，2022 年。'
- en: '[37] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh
    Hajishirzi, Sameer Singh, and Roy Fox. Do Embodied Agents Dream of Pixelated Sheep:
    Embodied Decision Making using Language Guided World Modelling. In Proceedings
    of the International Conference on Machine Learning (ICML), pages 26311–26325,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh
    Hajishirzi, Sameer Singh 和 Roy Fox. 具身代理是否梦见像素化的羊：使用语言引导的世界建模进行具身决策。在国际机器学习会议（ICML）论文集，页码
    26311–26325，2023 年。'
- en: '[38] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan,
    Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch,
    Antonio Torralba, and Yuke Zhu. Pre-Trained Language Models for Interactive Decision-Making.
    In Proceedings of the Annual Conference on Neural Information Processing Systems
    (NeurIPS), 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan,
    Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch,
    Antonio Torralba 和 Yuke Zhu. 预训练语言模型用于互动决策。神经信息处理系统年会（NeurIPS）论文集，2022 年。'
- en: '[39] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter
    Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding Pretraining in Reinforcement
    Learning with Large Language Models. In Proceedings of the International Conference
    on Machine Learning (ICML), pages 8657–8677, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter
    Abbeel, Abhishek Gupta 和 Jacob Andreas. 使用大语言模型引导强化学习的预训练。在国际机器学习会议（ICML）论文集，页码
    8657–8677，2023 年。'
- en: '[40] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward
    Design with Language Models. In Proceedings of the International Conference on
    Learning Representations (ICLR), 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Minae Kwon, Sang Michael Xie, Kalesha Bullard 和 Dorsa Sadigh. 使用语言模型进行奖励设计。在学习表征国际会议（ICLR）论文集，2023
    年。'
- en: '[41] Nan Li, William W. Cohen, Kenneth R. Koedinger, and Noboru Matsuda. A
    Machine Learning Approach for Automatic Student Model Discovery. In Proceedings
    of the International Conference on Educational Data Mining (EDM), pages 31–40,
    2011.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Nan Li, William W. Cohen, Kenneth R. Koedinger, 和 Noboru Matsuda. 一种用于自动学生模型发现的机器学习方法。发表于国际教育数据挖掘会议（EDM）论文集，第31–40页，2011年。'
- en: '[42] Albert T. Corbett and John R. Anderson. Knowledge Tracing: Modeling the
    Acquisition of Procedural Knowledge. User Modeling and User-Adapted Interaction,
    4:253–278, 2005.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Albert T. Corbett 和 John R. Anderson. 知识追踪：建模程序性知识的获取。用户建模与用户适应交互，4:253–278，2005年。'
- en: '[43] Louis Faucon, Lukasz Kidzinski, and Pierre Dillenbourg. Semi-Markov Model
    for Simulating MOOC Students. In Proceedings of the International Conference on
    Educational Data Mining (EDM), pages 358–363, 2016.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Louis Faucon, Lukasz Kidzinski, 和 Pierre Dillenbourg. 模拟 MOOC 学生的半马尔可夫模型。发表于国际教育数据挖掘会议（EDM）论文集，第358–363页，2016年。'
- en: '[44] Anthony F. Botelho, Seth Adjei, and Neil T. Heffernan. Modeling Interactions
    Across Skills: A Method to Construct and Compare Models Predicting the Existence
    of Skill Relationships. In Proceedings of the International Conference on Educational
    Data Mining (EDM), pages 292–297, 2016.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Anthony F. Botelho, Seth Adjei, 和 Neil T. Heffernan. 跨技能的交互建模：一种构建和比较预测技能关系存在性的模型的方法。发表于国际教育数据挖掘会议（EDM）论文集，第292–297页，2016年。'
- en: '[45] Anna N. Rafferty, Joseph Jay Williams, and Huiji Ying. Statistical Consequences
    of Using Multi-Armed Bandits to Conduct Adaptive Educational Experiments. Journal
    of Educational Data Mining (JEDM), 11:47–79, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Anna N. Rafferty, Joseph Jay Williams, 和 Huiji Ying. 使用多臂赌博机进行自适应教育实验的统计后果。教育数据挖掘期刊（JEDM），11:47–79，2019年。'
- en: '[46] John Mui, Fuhua Lin, and M Ali Akber Dewan. Multi-Armed Bandit Algorithms
    for Adaptive Learning: A Survey. In Proceedings of the International Conference
    on Artificial Intelligence in Education (AIED), pages 273–278, 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] John Mui, Fuhua Lin, 和 M Ali Akber Dewan. 自适应学习的多臂赌博机算法：一项综述。发表于国际教育人工智能会议（AIED）论文集，第273–278页，2021年。'
- en: '[47] Adish Singla and Nikitas Theodoropoulos. From {Solution Synthesis} to
    {Student Attempt Synthesis} for Block-Based Visual Programming Tasks. In Proceedings
    of the International Conference on Educational Data Mining (EDM), 2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Adish Singla 和 Nikitas Theodoropoulos. 从 {解决方案合成} 到 {学生尝试合成} 用于基于区块的视觉编程任务。发表于国际教育数据挖掘会议（EDM）论文集，2022年。'
- en: '[48] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang
    Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten
    Sap. SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents.
    CoRR, abs/2310.11667, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang
    Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, 和 Maarten
    Sap. SOTOPIA: 语言代理中的社会智能互动评估。CoRR，abs/2310.11667，2023年。'
- en: '[49] Alexander Pan, Chan Jun Shern, Andy Zou, Nathaniel Li, Steven Basart,
    Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do
    the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical
    Behavior in the MACHIAVELLI Benchmark. In Proceedings of the International Conference
    on Machine Learning (ICML), pages 26837–26867, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Alexander Pan, Chan Jun Shern, Andy Zou, Nathaniel Li, Steven Basart,
    Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, 和 Dan Hendrycks. 奖励是否值得？衡量
    MACHIAVELLI 基准中奖励与伦理行为之间的权衡。发表于国际机器学习会议（ICML）论文集，第26837–26867页，2023年。'
- en: '[50] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and
    Mari Ostendorf. Deep Reinforcement Learning with a Natural Language Action Space.
    In Proceedings of the Annual Meeting of the Association for Computational Linguistics
    (ACL), 2016.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, 和
    Mari Ostendorf. 具有自然语言动作空间的深度强化学习。发表于计算语言学协会年会（ACL）论文集，2016年。'
- en: '[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,
    and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models. In Proceedings
    of the International Conference on Learning Representations (ICLR), 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan,
    和 Yuan Cao. ReAct: 在语言模型中协同推理与行动。发表于国际学习表示会议（ICLR）论文集，2023年。'
- en: '[52] Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter A. Jansen, Oyvind
    Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. CLIN:
    A Continually Learning Language Agent for Rapid Task Adaptation and Generalization.
    CoRR, abs/2310.10134, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Bodhisattwa Prasad Majumder、Bhavana Dalvi Mishra、Peter A. Jansen、Oyvind
    Tafjord、Niket Tandon、Li Zhang、Chris Callison-Burch 和 Peter Clark。CLIN：一种用于快速任务适应和泛化的持续学习语言代理。CoRR,
    abs/2310.10134, 2023。'
- en: '[53] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: An Autonomous
    Agent with Dynamic Memory and Self-Reflection. CoRR, abs/2303.11366, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Noah Shinn、Beck Labash 和 Ashwin Gopinath。Reflexion：一种具有动态记忆和自我反思的自主代理。CoRR,
    abs/2303.11366, 2023。'
- en: '[54] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction.
    MIT press, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Richard S. Sutton 和 Andrew G. Barto。《强化学习：导论》。MIT出版社, 2018。'
- en: '[55] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
    Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing Atari with Deep Reinforcement
    Learning. CoRR, abs/1312.5602, 2013.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis Antonoglou、Daan
    Wierstra 和 Martin A. Riedmiller。使用深度强化学习玩Atari游戏。CoRR, abs/1312.5602, 2013。'
- en: '[56] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching
    Word Vectors with Subword Information. CoRR, abs/1607.04606, 2016.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Piotr Bojanowski、Edouard Grave、Armand Joulin 和 Tomas Mikolov。通过子词信息丰富词向量。CoRR,
    abs/1607.04606, 2016。'
- en: '[57] Prithviraj Ammanabrolu and Matthew J. Hausknecht. Graph Constrained Reinforcement
    Learning for Natural Language Action Spaces. In Proceedings of the International
    Conference on Learning Representations (ICLR), 2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Prithviraj Ammanabrolu 和 Matthew J. Hausknecht。图约束强化学习用于自然语言动作空间。发表于国际学习表征会议（ICLR）论文集,
    2020。'
- en: \balancecolumns
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: \balancecolumns
