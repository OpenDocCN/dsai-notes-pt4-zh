<!--yml

category: 未分类

日期：2025-01-11 12:01:48

-->

# Guide-LLM：一个基于LLM的具象化代理和用于视力障碍人士机器人引导的基于文本的拓扑地图

> 来源：[https://arxiv.org/html/2410.20666/](https://arxiv.org/html/2410.20666/)

Sangmim Song¹, Sarath Kodagoda¹, Amal Gunatilake¹, Marc G. Carmichael¹

Karthick Thiyagarajan² 和 Jodi Martin³ 本研究得到了澳大利亚政府通过澳大利亚研究委员会的链接项目资助计划（LP220100430）以及行业合作伙伴Guide Dogs NSW/ACT的支持。¹Sangmim Song、Sarath Kodagoda、Amal Gunatilake 和 Marc G. Carmichael 来自悉尼科技大学工程与信息技术学院机器人研究所，澳大利亚新南威尔士州Ultimo街Broadway，邮政编码2007。电子邮件：Sangmim.Song@student.uts.edu.au²Karthick Thiyagarajan 来自智能传感与机器人实验室（SensR Lab），先进制造技术中心（CfAMT），工程、设计与建筑环境学院（SoEDBE），澳大利亚新南威尔士州Kingswood，邮政编码2747。³Jodi Martin 来自新南威尔士州/澳大利亚首都地区导盲犬协会，澳大利亚新南威尔士州悉尼。

###### 摘要

导航对于视力障碍人士（PVI）来说是一个重大挑战。尽管传统辅助工具，如白色手杖和导盲犬，极为宝贵，但它们在提供详细的空间信息和精确引导到目标地点方面仍然有所不足。近期，大型语言模型（LLMs）和视觉语言模型（VLMs）的发展为增强辅助导航开辟了新的道路。在本文中，我们介绍了Guide-LLM，一个基于LLM的具象化代理，旨在帮助视力障碍人士在大型室内环境中导航。我们的方法采用了一种新颖的基于文本的拓扑地图，使LLM能够利用简化的环境表示规划全局路径，重点关注直线路径和直角转弯，以便于导航。此外，我们还利用LLM的常识推理进行危险检测和基于用户偏好的个性化路径规划。模拟实验证明了该系统在引导视力障碍人士方面的有效性，突显了其作为辅助技术重要进展的潜力。结果强调了Guide-LLM在提供高效、适应性强和个性化导航辅助方面的能力，指向这一领域的有希望的进展。

###### 索引词：

辅助机器人学、大型语言模型、基于文本的拓扑映射、视觉与语言导航、人机交互中的安全性、视力障碍。

## I 引言

对视觉障碍人士（PVI）来说，日常环境的导航尤其具有挑战性，他们通常依赖于专业工具、他人的帮助或熟悉的路线来进行导航[[1](https://arxiv.org/html/2410.20666v1#bib.bib1)，[2](https://arxiv.org/html/2410.20666v1#bib.bib2)，[3](https://arxiv.org/html/2410.20666v1#bib.bib3)，[4](https://arxiv.org/html/2410.20666v1#bib.bib4)，[5](https://arxiv.org/html/2410.20666v1#bib.bib5)]。传统的辅助工具，如白手杖和导盲犬，是导航的重要组成部分，但随着技术的发展，进一步的帮助可能会使得提升用户在导航中的信心成为可能[[6](https://arxiv.org/html/2410.20666v1#bib.bib6)]。

最近，人工智能的突破，尤其是在大型语言模型（LLMs）[[7](https://arxiv.org/html/2410.20666v1#bib.bib7)，[8](https://arxiv.org/html/2410.20666v1#bib.bib8)]和视觉-语言模型（VLMs）[[9](https://arxiv.org/html/2410.20666v1#bib.bib9)，[10](https://arxiv.org/html/2410.20666v1#bib.bib10)，[11](https://arxiv.org/html/2410.20666v1#bib.bib11)]方面的进展，为人机交互[[12](https://arxiv.org/html/2410.20666v1#bib.bib12)]、任务规划[[13](https://arxiv.org/html/2410.20666v1#bib.bib13)，[14](https://arxiv.org/html/2410.20666v1#bib.bib14)]和导航[[15](https://arxiv.org/html/2410.20666v1#bib.bib15)，[16](https://arxiv.org/html/2410.20666v1#bib.bib16)]创造了新的机会。尽管这些进展已经取得了很大突破，但它们在协助视觉障碍人士（PVI）进行导航方面的应用仍然没有得到充分探索。[[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]展示了将机器人平台与语言模型结合使用来帮助PVI的潜力，提供了可实现的前景。然而，充分发挥LLMs和VLMs在引导PVI方面的能力仍然是一个尚未深入研究的领域。

传统的导航系统通常依赖于预先编程的规则和传感器数据，这可能忽略了现实环境中的细微差别和复杂性。与之相比，LLMs可以分析上下文信息并预见潜在的危险，为导航提供更具适应性和响应性的解决方案。基于LLM和VLM的导航系统的一个主要挑战是它们依赖于用户提供精确、明确的指令，而这对于视觉障碍人士来说可能较为困难。虽然使用点云的3D重建技术[[18](https://arxiv.org/html/2410.20666v1#bib.bib18)，[19](https://arxiv.org/html/2410.20666v1#bib.bib19)，[20](https://arxiv.org/html/2410.20666v1#bib.bib20)]和传统的SLAM（同步定位与建图）方法帮助LLMs理解环境，但由于LLMs解读这些密集地图所需的高计算需求，它们的可扩展性仍受到限制[[21](https://arxiv.org/html/2410.20666v1#bib.bib21)，[22](https://arxiv.org/html/2410.20666v1#bib.bib22)，[20](https://arxiv.org/html/2410.20666v1#bib.bib20)]。

![参考说明](img/46c4302eece52cf196fd69ceacc5f73e.png)

图1：Guide-LLM：具身代理由文本地图、LLM和导航模块组成，引导用户到达目的地。

![参考说明](img/0196c5a3cf4dea6ed02d07e84ca2ce8b.png)

图2：Guide-LLM框架：LLM（绿色）作为中央控制器，利用常识推理解读用户查询并与各模块（黄色）互动，进行决策和导航任务。文本地图（绿色）提供环境的文本表示，供路径规划模块使用以创建路线规划。向量数据库1（蓝色）存储环境图像的静态嵌入，帮助保持一致的定位。向量数据库2（红色）存储可更新或删除的导航图像嵌入，基于代理的需求进行调整。

为了克服这一挑战，我们提出了一种创新框架，利用基于文本的环境拓扑图。这使得LLM可以通过引用文本表示来规划全局路径，从而不需要用户的显式输入。与依赖于密集地图或3D表示的传统方法相比，这种方法在计算效率和可扩展性方面更具优势，后者可能会导致延迟，使PVI需要等待LLM处理这些复杂的输入。此外，我们的基于文本的拓扑图旨在通过生成直线路径和直角转弯来满足PVI的特定需求，这些路径更易于导航，有助于保持空间方向感[[23](https://arxiv.org/html/2410.20666v1#bib.bib23)，[24](https://arxiv.org/html/2410.20666v1#bib.bib24)]。这些清晰且可预测的路线减少了认知负担，使导航更加高效和安全，特别是与弯曲或不规则路径带来的挑战相比。

我们的框架还结合了一个图像检索系统用于定位，以及一个低级规划器来处理机器人运动，约束机器人的动作模式为可预测的行为。将LLM集成到导航中的一个显著优势是它能够利用常识推理，这有助于提升安全性、个性化导航和行为的可解释性[[16](https://arxiv.org/html/2410.20666v1#bib.bib16)，[25](https://arxiv.org/html/2410.20666v1#bib.bib25)]，这些是传统导航系统通常所缺乏的。我们工作的主要贡献包括：

+   •

    提出一种新型的具身LLM代理框架，用于引导PVI：我们提出了Guide-LLM，一种创新框架，利用LLM作为具身代理来协助PVI进行导航。

+   •

    新型基于文本的拓扑图与图像向量数据库的集成：通过将基于文本的拓扑图与图像向量数据库结合，我们使LLM能够通过常识推理进行高层次的规划，最大限度地减少对广泛用户输入或详细指令的需求。

+   •

    仿真评估：我们通过仿真验证我们方法的有效性，展示了其在指导视障人士（PVI）方面的能力。

![参见说明文字](img/c51e0828991e92df35bae4ca9c4ba92e.png)

图3：文本地图（左）：文本地图的示意图，部分文本地图被提取。示例文本地图表示（中）：用户请求代理导航到电梯。Guide-LLM规划了一条路线（红线）并开始引导。在路线途中，检测到危险（湿滑地面标志），Guide-LLM警告用户并建议替代路径（绿线），聊天框（右）展示了Guide-LLM与用户之间的示例对话。

## II 相关工作

### II-A 视障人士的导航辅助

最近，辅助技术的进步大大增强了视障人士的导航和行动能力。例如，机器人白手杖[[26](https://arxiv.org/html/2410.20666v1#bib.bib26)]利用多模态传感和转向辅助来帮助用户导航其周围环境。此外，穿戴式系统也已被开发，以通过听觉或触觉信号提供实时反馈，从而提高情境意识[[27](https://arxiv.org/html/2410.20666v1#bib.bib27)，[28](https://arxiv.org/html/2410.20666v1#bib.bib28)，[29](https://arxiv.org/html/2410.20666v1#bib.bib29)，[30](https://arxiv.org/html/2410.20666v1#bib.bib30)]。[[31](https://arxiv.org/html/2410.20666v1#bib.bib31)]介绍了一种方法，使视障人士能够在运动场上慢跑。尽管有这些进展，这些系统大多是预先编程的，具有固定的规则和行为，这限制了它们在动态场景中的适应性。

为了应对这些局限性，基于对话的机器人被提出作为一种替代的辅助形式，利用对话界面帮助用户到达目的地[[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]。然而，这些系统仍然面临重大挑战，尤其是在自然语言交互和适应性方面。它们基于规则的性质也限制了它们的泛化能力，无法适应不断变化的环境，而这对于提供个性化和灵活的视障人士支持至关重要。

### II-B 机器人学中的大语言模型

LLMs 与机器人技术的结合在任务规划、自动驾驶、多模态推理和导航等领域展现了巨大的潜力。像 SayPlan 和 ReAct 这样的框架通过将复杂的指令分解为可执行的子任务，并将推理与行动相结合，增强了机器人的能力[[13](https://arxiv.org/html/2410.20666v1#bib.bib13), [32](https://arxiv.org/html/2410.20666v1#bib.bib32)]。DriveLLM [[16](https://arxiv.org/html/2410.20666v1#bib.bib16)] 展示了 LLMs 如何通过将对象级向量模态与 LLMs 集成，改善自动驾驶中的可解释性和决策能力，从而增强了驾驶场景中的上下文理解和可解释性。RoboVQ [[33](https://arxiv.org/html/2410.20666v1#bib.bib33)] 将 LLMs 与视觉输入结合，使机器人能够执行复杂的长远任务，并展示了 LLMs 的多模态推理能力[[34](https://arxiv.org/html/2410.20666v1#bib.bib34)]。该研究利用 LLMs 作为导航代理，通过推理能力开发搜索启发式算法，用于探索新环境。SayNav 使用 LLMs 引导机器人穿越不熟悉的环境，通过将高层指令与空间上下文相结合，突显了 LLMs 在探索中的潜力，而无需详细的先验知识[[35](https://arxiv.org/html/2410.20666v1#bib.bib35)]。[[36](https://arxiv.org/html/2410.20666v1#bib.bib36)] 展示了 LLMs 在跨任务泛化能力上的优势，使机器人能够在最少的任务特定训练下规划和执行复杂的动作。此外，NavGPT [[25](https://arxiv.org/html/2410.20666v1#bib.bib25)]、MAP-GPT [[37](https://arxiv.org/html/2410.20666v1#bib.bib37)] 和 LGX [[38](https://arxiv.org/html/2410.20666v1#bib.bib38)] 探索了 LLM 引导的机器人导航，进一步拓展了 LLM 在未开发环境中的能力。

尽管取得了这些进展，但针对利用 LLMs 的常识推理和上下文理解来帮助视力障碍人士的研究仍然较为有限。这一空白凸显了迫切需要创新方法，利用 LLMs 的推理能力为视力障碍人士提供更智能和适应性强的支持。

### II-C 视觉与语言导航

视觉和语言导航（VLN）将视觉感知与自然语言理解相结合，使得代理能够根据口头或书面指令进行导航。VLN中的一个重大挑战是，自然语言指令通常侧重于高层决策和地标，往往缺乏详细的、低层的运动指导[[39](https://arxiv.org/html/2410.20666v1#bib.bib39)]。基于注意力的机制[[40](https://arxiv.org/html/2410.20666v1#bib.bib40)、[41](https://arxiv.org/html/2410.20666v1#bib.bib41)、[42](https://arxiv.org/html/2410.20666v1#bib.bib42)]和强化学习方法[[43](https://arxiv.org/html/2410.20666v1#bib.bib43)、[44](https://arxiv.org/html/2410.20666v1#bib.bib44)]在解决这个问题上已取得了良好的结果。最近，LLMs和VLMs的兴起激发了人们利用这些模型提升VLN能力的兴趣。例如，LM-Nav[[15](https://arxiv.org/html/2410.20666v1#bib.bib15)]使用LLMs从用户查询中提取地标，并利用VLMs将这些地标与环境中的位置进行关联，以实现导航。类似地，[[45](https://arxiv.org/html/2410.20666v1#bib.bib45)]使用LLMs将用户查询转化为可操作的导航任务，并利用图像分割技术创建拓扑图进行导航。[[38](https://arxiv.org/html/2410.20666v1#bib.bib38)]结合了LLMs的常识推理与VLMs，朝着独特描述的物体进行导航，展示了LLMs在理解细微语言上的潜力。[[46](https://arxiv.org/html/2410.20666v1#bib.bib46)]利用LLMs通过处理包括导航指令、视觉地标描述和代理过去轨迹的文本提示来引导代理。[[17](https://arxiv.org/html/2410.20666v1#bib.bib17)]使用LLMs识别地标、描述环境并进行导航。

尽管这些进展有所突破，PVI的导航仍然具有挑战性，特别是在描述具体场景和地标时，这可能会妨碍与代理的有效沟通。我们的工作旨在通过使代理能够推断用户意图并自动解决其指令中的歧义来解决这一限制。

## III 提议的系统

在本节中，我们将详细介绍Guide-LLM框架。图[2](https://arxiv.org/html/2410.20666v1#S1.F2 "图 2 ‣ I 引言 ‣ Guide-LLM：一个具身的LLM代理与基于文本的拓扑图，用于引导视障人士的机器人")展示了框架的主要组成部分，使我们的LLM代理能够处理决策、规划和导航。

### III-A 核心组件

#### III-A1 代理

我们采用了GPT-4o [[47](https://arxiv.org/html/2410.20666v1#bib.bib47)]作为我们框架的核心组件。GPT-4o负责解释用户输入并确定适当的行动，以帮助视觉障碍者（PVI）。当接收到用户查询时，LLM会提供一个包含指令的系统提示，以在特定上下文中处理查询。如图[4](https://arxiv.org/html/2410.20666v1#S3.F4 "图 4 ‣ III-A2 基于文本的拓扑图 ‣ III-A 核心组件 ‣ III 提议的系统 ‣ Guide-LLM: 用于视觉障碍者机器人引导的具身LLM代理与基于文本的拓扑图")所示，该提示旨在引导LLM有效地帮助用户，同时解决潜在的错误和安全问题。系统提示基于思维链（CoT）方法[[48](https://arxiv.org/html/2410.20666v1#bib.bib48)]，该方法使LLM能够将任务分解为可管理的中间步骤。

#### III-A2 基于文本的拓扑图

该框架具有基于文本的拓扑图（图[3](https://arxiv.org/html/2410.20666v1#S1.F3 "图 3 ‣ I 引言 ‣ Guide-LLM: 用于视觉障碍者机器人引导的具身LLM代理与基于文本的拓扑图")），该图展示了环境中各节点之间的空间关系（例如，RoomA连接到HallwayA，北偏0.5米）。节点设置在机器人需要转弯或采取行动的关键点。此图作为代理的示意参考，帮助通过提供环境的结构化且可查询的表示来引导其在复杂空间中的导航。它特别设计用以强调节点之间的直线路径，因为视觉障碍者通常发现直线路径比弯曲路径更直观[[24](https://arxiv.org/html/2410.20666v1#bib.bib24)，[23](https://arxiv.org/html/2410.20666v1#bib.bib23)]。

![参考标题](img/4ffae45d749849837ab52585fa5c83bb.png)

图4：系统提示示例。

#### III-A3 路径规划模块

该模块与代理协同工作，规划通往指定目的地的高效路径。当代理发起导航查询时，该模块首先查阅基于文本的拓扑图以确定潜在路径。它计算到目的地的最短路径，并生成一段文字描述供代理处理。为了提高灵活性，该模块使用深度优先搜索（DFS）算法探索所有可能的路径。这种方法使得代理能够评估多条路径，并根据障碍物、用户偏好（例如避免障碍物、选择更安静的路线或优先考虑安全问题）实时调整。通过结合路径规划和文本反馈，代理可以做出更为明智的决策，并为用户提供个性化的导航体验。

#### III-A4 嵌入模块

我们使用预训练的CLIP[[10](https://arxiv.org/html/2410.20666v1#bib.bib10)]从视觉数据中生成嵌入向量，然后将其与相关的元数据（包括位置和方向信息）一起存储在向量数据库中。这些元数据允许LLM在检索时引用特定图像的位置和方向。嵌入模块对于定位和子目标选择至关重要，因为它支持文本到图像和图像到图像的相似性搜索。此功能使得代理能够准确地进行自我定位并选择适当的子目标。

#### III-A5 向量数据库

向量数据库管理由嵌入模块生成的环境的高维向量表示。这一设置使得当前观察结果与先前存储的数据之间的高效比较成为可能。该数据库对于实时决策至关重要，允许LLM检索与当前观察结果匹配的图像和与导航相关的图像。我们维护两个独立的向量数据库：一个用于存储所有环境图像，另一个用于存储与当前导航相关的图像。这种分离有助于减少歧义，确保更加准确地检索到导航相关的图像，从而避免在单一、密集的数据库中由无关图像引起的混淆。

#### III-A6 低级规划器

低级规划器将代理做出的高级决策转化为机器人的可执行动作。它确保这些指令在物理上可行，考虑到机器人的运动能力。通过将高级决策与实际执行连接起来，低级规划器使机器人能够在各种环境中安全高效地操作。

#### III-A7 机器人

我们使用TurtleBot作为机器人平台。

### III-B 全球路径规划

图[2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")展示了提议框架的整体过程。我们从假设向量数据库中的文本地图和图像是预先标注的开始。过程首先由LLM处理用户的查询和系统提示。然后，LLM生成高级规划、图像查询命令和用户响应。

如图[2](https://arxiv.org/html/2410.20666v1#S1.F2 "Figure 2 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")所示，每个输出都分别传递，以便模块之间进行清晰的通信。高级规划和用户响应通过语音转文本接口传递，而图像查询则从向量数据库中检索与导航相关的图像。这些图像随后被嵌入到一个二级向量数据库中，通过减少歧义并确保只检索相关图像来优化导航过程。

### III-C 拓扑导航

代理通过查询向量数据库获取下一节点的图像，开始导航。该图像通过余弦相似度与当前观测结果进行比较，以进行定位。为了提高地点识别的准确性，每当机器人行驶一定距离或转弯时，都会进行一次相似度检查。机器人通过低层次规划器的里程表测量到下一个节点的距离，抵达时会向代理发送消息。如果相似度得分超过预设阈值，LLM 将认为目标节点已经到达，并生成下一组移动指令继续前进。此过程会持续进行，直到到达最终目的地。

### III-D 定位与错误处理

我们的代理框架中的定位采用双层方法。首先，代理通过查询导航向量数据库，并将检索到的图像与机器人的当前观测结果进行比较，来验证其是否到达预期节点。如果这些图像之间的相似度得分低于设定的阈值，代理会检测到可能的定位错误，提示机器人可能处于错误的位置。为了解决这一问题，代理通过查询包含整个环境嵌入的主向量数据库，发起更广泛的搜索。这一回退机制使系统能够通过在所有环境图像中找到最相关的匹配，重新定位并确保准确性。这个动态的错误处理过程确保了即使机器人最初没有对准或迷失方向，系统也能实时纠正其位置，保持预定路径，实现安全高效的导航。

### III-E 利用LLM的常识和推理

我们的框架利用LLM的常识推理来提高导航安全性和决策能力。与依赖预定义规则的传统系统不同，代理能够解读动态的现实世界上下文，预测潜在风险。例如，如果代理通过视觉数据或环境描述识别到湿滑的地板、警示带或意外障碍物等危险，它会主动提醒用户并建议替代路线。LLM的推理能力使其能够检测潜在的危险，即使这些危险没有被明确标明。这种灵活性使得代理能够适应变化的条件，而基于规则的系统可能无法做到这一点。通过整合常识知识，代理增强了导航体验的安全性和整体可靠性。

### III-F 个性化潜力

我们智能体的一大优势是能够根据每个用户的特定偏好和需求个性化导航体验。该智能体的自然语言交互能力使其能够实时调整行为，从而促进个性化。举例来说，它可以根据用户的步行速度、路线偏好（例如，避免楼梯或选择较安静的区域）或特定的安全 concerns，如避开潜在的危险，来调整路径规划。此外，系统还可以结合来自先前交互的反馈，逐步完善其决策，以更好地与用户的习惯和偏好对接。例如，如果用户始终选择较长且人少的路线而不是较短的路线，智能体可以将这一偏好纳入其未来的规划中。这种适应性不仅限于导航；智能体还可以进行个性化对话，根据不同用户的需求调整详细程度和沟通风格，无论他们更倾向于简洁的指令还是更详细的解释。

## IV 结果

我们通过测试Guide-LLM在模拟环境中引导视障人士的能力来评估其功能。这些仿真分为四个关键场景，每个场景旨在突出LLM在导航、决策、错误处理和危险检测等方面的不同用途。所有测试均在使用iGibson仿真器和TurtleBot平台的模拟环境中进行。

表 I：不同配置下Guide-LLM导航的成功率

| 环境 | 所有组件 | 无系统提示 | 无路径规划模块 |
| --- | --- | --- | --- |
| 大房子 | 90% | 0% | 33.33% |
| 办公室 | 83.33% | 0% | 40% |

表 II：本地化错误检测与恢复的成功率

| 错误场景 | 成功率% |
| --- | --- |
| 本地化错误检测 | 90% |
| 本地化恢复 | 66% |

### IV-A 实验设置

我们使用iGibson [[49](https://arxiv.org/html/2410.20666v1#bib.bib49)] 仿真器验证了我们的框架。我们选择了两个不同规模的环境来评估智能体的适应性和鲁棒性。这些环境被选中代表不同的复杂性和规模，从而使我们能够测试智能体在小型封闭空间和较大开放区域中的表现。在每个环境中，我们为导航分配了随机的起点和终点，文本地图上有相应的表示。Turtlebot配备了RGB摄像头，用于捕捉当前观察到的图像。所有实验均在一台配备Intel Core I9、32GB内存和RTX 4070 GPU的笔记本电脑上进行。

### IV-B LLM引导导航消融研究

本实验的目标是评估我们的LLM智能体是否能够通过解读用户查询而无需详细指令，成功导航到目的地，展示其在辅助视力障碍者（PVI）方面的潜力。为了评估智能体框架中核心组件的贡献，我们进行了消融研究，逐个移除系统提示和路径规划模块，分析每个移除操作对导航性能的影响。如表[I](https://arxiv.org/html/2410.20666v1#S4.T1 "TABLE I ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")所示，当所有组件都处于激活状态时，Guide-LLM在办公环境中的导航成功率为83.33%。移除系统提示后，成功率降为0%，这表明系统提示对于引导智能体的理解和回应至关重要，它提供了必要的上下文和指令。

为了评估路径规划模块的影响，我们将其移除，并直接将文本地图输入到我们的智能体中。导航成功率下降至40%。这一下降可以归因于多个因素。首先，直接提供文本地图和系统提示增加了提示长度，这导致了由于处理大型提示的瓶颈而降低了智能体的推理能力[[50](https://arxiv.org/html/2410.20666v1#bib.bib50), [51](https://arxiv.org/html/2410.20666v1#bib.bib51), [52](https://arxiv.org/html/2410.20666v1#bib.bib52)]。没有路径规划模块时，智能体在跟随字母顺序的路径（例如，从走廊A到走廊C）时较为成功，但在逆向路径（例如，从走廊E到走廊A）上则表现不佳。移除图像检索系统迫使用户指定起始位置，改变了实验条件，并增加了用户负担。为了保持实验的公平性，该配置被排除在对比分析之外，因为它不再测试智能体通过视觉推断起始点的能力。

### IV-C 定位误差检测与恢复

为了评估我们的代理的错误处理和恢复能力，我们进行了一个实验，其中机器人在导航过程中被随机放置在一个节点。当机器人当前观察到的图像与预期的导航图像之间的相似度分数低于0.94时，代理会通过查询主向量数据库重新定位自己并尝试恢复，然后从恢复的位置重新启动导航规划过程。为了进一步说明系统的行为，我们提供了图[3](https://arxiv.org/html/2410.20666v1#S1.F3 "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")中的示例聊天框交互，展示了代理如何检测错误、应对意外结果，并通过基于新位置更新计划来启动恢复过程。该实验在办公环境中进行了30次，以全面测试代理的鲁棒性。实验结果如表[II](https://arxiv.org/html/2410.20666v1#S4.T2 "TABLE II ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")所示。为了检测并从机器人撞墙的情况中恢复，我们修改了TurtleBot，当接收到移动指令后在停留一定时间不动时，输出一条消息。这使得代理能够识别问题，并通过调整导航计划启动恢复过程。在实验过程中，我们观察到，当代理处于视觉上相似的区域时，偶尔会发生定位误差，但通过考虑它过去走过的路径，代理成功地从一些情况下恢复了过来。

表 III：危险检测性能

| 场景 | 总检测数 | 真阳性 | 假阳性 |
| --- | --- | --- | --- |
| 危险检测 | 30 | 12 | 18 |

### IV-D 常识推理用于危险检测

本实验的目标是评估智能体在识别环境中潜在危险、有效传达这些风险给用户，并根据用户的决策调整其行动的能力。为了测试这一点，沿着导航路径放置了障碍物和潜在危险的图像，包括常见的危险，如警告标志、物理障碍物和悬挂物体。评估的重点是智能体是否能够检测到这些危险，并在必要时提供适当的警告，建议替代路径。如表[III](https://arxiv.org/html/2410.20666v1#S4.T3 "TABLE III ‣ IV-C Localization Error Detection and Recovery ‣ IV Results ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")所示，关键指标包括危险检测准确率，特别关注真正的正例率和假阳性率，例如智能体错误地将非危险物体（如靠近墙壁的可乐罐）识别为潜在危险的情况。一个示例场景反映在图[3](https://arxiv.org/html/2410.20666v1#S1.F3 "Figure 3 ‣ I Introduction ‣ Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments")中，智能体通过超越预定义的导航指令展示其常识推理能力。在检测到潜在危险时，智能体会提醒用户并提供可能的替代方案。例如，如果障碍物挡住了路径，或者识别到有危险的表面（如湿滑地面标志），智能体会向用户发出警告信息，并建议一条更安全的替代路线。一旦危险被传达，系统会等待用户的响应，允许用户选择接受建议的路线，或在原路径上小心行进。该实验突出了智能体动态推理并适应不可预见环境变化的能力。

### IV-E 个性化潜力

对导航辅助偏好的研究表明，个性化对于提升用户体验至关重要[[53](https://arxiv.org/html/2410.20666v1#bib.bib53)]。基于LLM的系统能够通过自然语言互动满足这些多样化的偏好，使用户能够轻松表达其特定需求。用户只需与代理交谈，就可以调整如首选步行速度、路线类型（例如避免楼梯）或安全问题（例如避免潜在危险）等参数。这种定制体验的灵活性凸显了LLM系统的个性化潜力。为了评估用户偏好如何反映，我们在文本拓扑地图中重新命名了一些空间，例如音乐厅、食品广场、嘈杂区域和安静区域。我们根据用户偏好评估了代理的路线选择性能，测试了同一路线10次，以确定其是否做出了不同的选择。结果如下：偏好详细逐步导航指令的用户在10次测试中10次都满足了他们的偏好，偏好安静路线的用户在10次测试中10次都满足了他们的偏好，而偏好楼梯而非电梯的用户在10次测试中有3次满足了他们的偏好。最后一项实验高失败率的主要原因是到达目的地的路线选择过多，可能超过了最大允许的提示数量，导致失败。如图[3](https://arxiv.org/html/2410.20666v1#S1.F3 "图 3 ‣ 引言 ‣ Guide-LLM：基于LLM的视觉障碍人士机器人引导框架及文本拓扑地图")所示，地图的左上角呈矩形，导致路径规划模块生成了8条路线选项，导致代理未能反映用户的偏好。

## V 结论与未来工作

我们介绍了Guide-LLM，这是一个创新框架，利用大型语言模型（LLM）和新颖的基于文本的拓扑地图，帮助视力受限人士（PVI）在大型室内环境中进行导航。我们的系统成功展示了提供高效、适应性强且个性化导航的能力，显著减少了对详细用户指令的需求。未来的工作将专注于扩展系统的能力，包括自主探索和地图生成，并解决如避障等实时挑战。与PVI的现实场景测试将是未来工作的重点，旨在改善和完善系统。这些进展将使我们更接近提供一种全面的辅助解决方案，帮助PVI在复杂环境中实现更大的独立性和信心。

## 参考文献

+   [1] D. L. Rudman 和 M. Durdle，“与恐惧共生：视力受限老年人社区出行的生活体验，”*老龄化与身体活动杂志*，第17卷，第1期，106-122页，2008年。

+   [2] N. A. Giudice 和 G. E. Legge，“盲人导航与技术的作用”，*智能技术在老龄化、残障与独立性中的应用工程手册*，页码 479–500，2008年。

+   [3] M. Y. Wang, J. Rousseau, H. Boisjoly, H. Schmaltz, M.-J. Kergoat, S. Moghadaszadeh, F. Djafari, 和 E. E. Freeman，“由于怕摔倒导致的老年人活动限制”，发表于*眼科与视觉科学研究*，第53卷，第13期，页码 7,967–7,972，2012年。

+   [4] W. Jeamwatthanachai, M. Wald, 和 G. Wills，“盲人使用室内导航：在陌生空间和建筑中的行为与挑战”，发表于*英国视觉障碍学报*，第37卷，第2期，页码 140–153，2019年。

+   [5] Z. Başgöze, J. Gualtieri, M. T. Sachs, 和 E. A. Cooper，“视觉障碍人士使用导航辅助工具”，发表于*技术与残障人士期刊：…年度国际技术与残障人士会议*，第8卷。NIH公共访问，2020年，页码22。

+   [6] N. A. Giudice, B. A. Guenther, T. M. Kaplan, S. M. Anderson, R. J. Knuesel, 和 J. F. Cioffi，“有视力和盲人旅行者使用室内导航系统：视觉状态和年龄之间的表现相似性”，发表于*ACM无障碍计算学报（TACCESS）*，第13卷，第3期，页码 1–27，2020年。

+   [7] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat *等*，“GPT-4技术报告”，*arXiv预印本arXiv:2303.08774*，2023年。

+   [8] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan *等*，“Llama 3系列模型”，*arXiv预印本arXiv:2407.21783*，2024年。

+   [9] H. Liu, C. Li, Q. Wu, 和 Y. J. Lee，“视觉指令调优”，发表于*神经信息处理系统进展*，第36卷，2024年。

+   [10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark *等*，“从自然语言监督中学习可迁移的视觉模型”，发表于*国际机器学习会议*。PMLR，2021，页码 8,748–8,763。

+   [11] J. Li, D. Li, S. Savarese, 和 S. Hoi，“Blip-2：使用冻结的图像编码器和大型语言模型进行语言-图像预训练的自举法”，发表于*国际机器学习会议*。PMLR，2023，页码 19,730–19,742。

+   [12] T. Williams, C. Matuszek, R. Mead, 和 N. Depalma，“奥兹的稻草人：大型语言模型在HRI中的应用”，发表于*ACM人机交互学报*，第13卷，第1期，页码 1–11，2024年1月。

+   [13] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, 和 N. Suenderhauf，“Sayplan：使用3D场景图基于大规模语言模型的可扩展机器人任务规划”，发表于*第七届机器人学习年会*，2023年。

+   [14] X. Jiang, Y. Dong, L. Wang, F. Zheng, Q. Shang, G. Li, Z. Jin, 和 W. Jiao，“使用大型语言模型进行自我规划代码生成”，发表于*ACM软件工程与方法学学报*，2024年6月。

+   [15] D. Shah, B. Osiński, S. Levine *等*，“Lm-nav：使用大型预训练语言、视觉和动作模型的机器人导航，”发表于*机器人学习会议*，PMLR，2023年，第492–504页。

+   [16] Y. Cui, S. Huang, J. Zhong, Z. Liu, Y. Wang, C. Sun, B. Li, X. Wang, 和 A. Khajepour，“Drivellm：使用大型语言模型绘制通往完全自动驾驶的路径，”*IEEE智能车辆期刊*，第9卷，第1期，第1450–1464页，2024年1月。

+   [17] S. Liu, A. Hasan, K. Hong, R. Wang, P. Chang, Z. Mizrachi, J. Lin, D. L. McPherson, W. A. Rogers, 和 K. Driggs-Campbell，“Dragon：基于对话的机器人，用于视觉语言基础的辅助导航，”*IEEE机器人与自动化通讯*，2024年。

+   [18] C. Huang, O. Mees, A. Zeng, 和 W. Burgard，“用于机器人导航的视觉语言地图，”发表于*2023年IEEE国际机器人与自动化会议（ICRA）*，IEEE，2023年，第10 608–10 615页。

+   [19] S. Chen, X. Chen, C. Zhang, M. Li, G. Yu, H. Fei, H. Zhu, J. Fan, 和 T. Chen，“Ll3da：用于全方位3D理解推理与规划的视觉交互式指令调优，”发表于*IEEE/CVF计算机视觉与模式识别会议论文集*，2024年，第26 428–26 438页。

+   [20] S. Yang, J. Liu, R. Zhang, M. Pan, Z. Guo, X. Li, Z. Chen, P. Gao, Y. Guo, 和 S. Zhang，“Lidar-llm：探索大型语言模型在3D激光雷达理解中的潜力，”*arXiv预印本arXiv:2312.14074*，2023年。

+   [21] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, 和 C. Gan，“3d-llm：将3D世界注入大型语言模型，”*神经信息处理系统进展*，第36卷，第20 482–20 494页，2023年。

+   [22] R. Xu, X. Wang, T. Wang, Y. Chen, J. Pang, 和 D. Lin，“Pointllm：赋能大型语言模型理解点云，”*arXiv预印本arXiv:2308.16911*，2023年。

+   [23] M. Swobodzinski 和 M. Raubal，“盲人室内路径规划算法：开发并与视力正常者的路径规划算法进行比较，”*国际地理信息科学期刊*，第23卷，第10期，第1315–1343页，2009年。

+   [24] S. Shafique, W. Setti, C. Campus, S. Zanchi, A. Del Bue, 和 M. Gori，“盲人路径积分能力在不同探索条件下的变化，”*神经科学前沿*，第18卷，p. 1375225，2024年。

+   [25] G. Zhou, Y. Hong, 和 Q. Wu，“Navgpt：利用大型语言模型进行视觉与语言导航中的显式推理，”发表于*AAAI人工智能会议论文集*，第38卷，第7期，2024年，第7641–7649页。

+   [26] P. Slade, A. Tambe, 和 M. J. Kochenderfer，“多模态感知和直观引导辅助提高视力障碍者的导航和流动性，”*科学机器人学*，第6卷，第59期，2021年10月。

+   [27] M. M. Islam, M. S. Sadi, 和 T. Bräunl，“自动化步态引导增强视力障碍者的流动性，”*IEEE医学机器人与生物力学期刊*，第2卷，第3期，第485–496页，2020年。

+   [28] L. Jin, H. Zhang, 和 C. Ye, “一款可穿戴机器人设备，用于辅助导航与物体操作，” 见 *2021 IEEE/RSJ 国际智能机器人与系统会议（IROS）*，IEEE，2021年，第765–770页。

+   [29] Y. Bouteraa, “为盲人和视力障碍人士设计并开发一款集成模糊决策支持系统的可穿戴辅助设备，” *Micromachines*，第12卷，第9期，第1082页，2021年。

+   [30] G. Li, J. Xu, Z. Li, C. Chen, 和 Z. Kan, “可穿戴认知辅助系统的感知与导航，帮助视力障碍人士，” *IEEE 认知与发展系统期刊*，第15卷，第1期，第122–133页，2022年。

+   [31] X. Liu, B. Wang, 和 Z. Li, “基于视觉的可穿戴引导辅助系统，用于跑步时视力受损人士，” 见 *2024 IEEE 国际机器人与自动化会议（ICRA）*，IEEE，2024年，第15 270–15 275页。

+   [32] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, 和 Y. Cao, “React: 协同推理与行动在语言模型中的应用，” *arXiv 预印本 arXiv:2210.03629*，2022。

+   [33] P. Sermanet, T. Ding, J. Zhao, F. Xia, D. Dwibedi, K. Gopalakrishnan, C. Chan, G. Dulac-Arnold, S. Maddineni, N. J. Joshi *等*，“Robovqa：机器人学中的多模态长远推理，” 见 *2024 IEEE 国际机器人与自动化会议（ICRA）*，IEEE，2024年，第645–652页。

+   [34] D. Shah, M. R. Equi, B. Osiński, F. Xia, B. Ichter, 和 S. Levine, “使用大型语言模型进行导航：作为规划启发式的语义推测，” 见 *Robot Learning大会*，PMLR，2023年，第2683–2699页。

+   [35] A. Rajvanshi, K. Sikka, X. Lin, B. Lee, H.-P. Chiu, 和 A. Velasquez, “Saynav：为大型语言模型提供基础，进行动态规划并在新环境中进行导航，” 见 *国际自动化规划与调度会议论文集*，第34卷，2024年，第464–474页。

+   [36] W. Huang, P. Abbeel, D. Pathak, 和 I. Mordatch, “语言模型作为零-shot规划者：为具身代理提取可操作知识，” 见 *国际机器学习会议*，PMLR，2022年，第9118–9147页。

+   [37] J. Chen, B. Lin, R. Xu, Z. Chai, X. Liang, 和 K.-Y. Wong, “Mapgpt：通过自适应路径规划的地图引导提示，用于视觉与语言导航，” 见 *第62届计算语言学协会年会（第1卷：长篇论文）*，2024年，第9796–9810页。

+   [38] V. S. Dorbala, J. F. Mullen, 和 D. Manocha, “一个具身代理能找到你的“猫形杯子”吗？基于大型语言模型的零-shot目标导航，” *IEEE Robotics and Automation Letters*，第9卷，第5期，第4083–4090页，2024年5月。

+   [39] D. Fried, R. Hu, V. Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick, K. Saenko, D. Klein, 和 T. Darrell, “视听导航的说话者-跟随者模型，” *神经信息处理系统进展*，第31卷，2018年。

+   [40] C.-Y. Ma, J. Lu, Z. Wu, G. AlRegib, Z. Kira, R. Socher, 和 C. Xiong，“通过辅助进展估计自我监控导航代理，”*arXiv 预印本 arXiv:1901.03035*，2019年。

+   [41] X. Li, A. Yuan, 和 X. Lu，“基于属性和注意力机制的视觉-语言任务，”*IEEE 控制学报*，第51卷，第2期，第913-926页，2019年。

+   [42] A. B. Vasudevan, D. Dai, 和 L. Van Gool，“Talk2nav：具有双重注意力和空间记忆的远程视觉-语言导航，”*计算机视觉国际期刊*，第129卷，第246-266页，2021年。

+   [43] X. Wang, W. Xiong, H. Wang, 和 W. Y. Wang，“三思而后行：桥接无模型和基于模型的强化学习以实现提前规划的视觉-语言导航，” 见于*欧洲计算机视觉会议（ECCV）论文集*，2018年，第37-53页。

+   [44] H. Wang, Q. Wu, 和 C. Shen，“用于视觉-语言导航的软专家奖励学习，”见于*计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23-28日，会议论文集，第IX部分16*，Springer，2020年，第126-141页。

+   [45] S. Garg, K. Rana, M. Hosseinzadeh, L. Mares, N. Sünderhauf, F. Dayoub, 和 I. Reid，“Robohop：基于分段的拓扑地图表示用于开放世界视觉导航，”*arXiv 预印本 arXiv:2405.05792*，2024年。

+   [46] R. Schumann, W. Zhu, W. Feng, T.-J. Fu, S. Riezler, 和 W. Y. Wang，“Velma：用于街景视觉和语言导航的LLM代理的言语化体现，”见于*美国人工智能会议论文集*，第38卷，第17期，2024年，第18,924-18,933页。

+   [47] OpenAI，“Hello gpt-4，”[https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)，访问时间：2024年9月14日。

+   [48] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou *等*，“链式思维提示引发大语言模型的推理，”*神经信息处理系统进展*，第35卷，第24,824–24,837页，2022年。

+   [49] C. Li, F. Xia, R. Martín-Martín, M. Lingelbach, S. Srivastava, B. Shen, K. Vainio, C. Gokmen, G. Dharan, T. Jain *等*，“igibson 2.0：面向机器人学习日常家务任务的物体中心仿真，”*arXiv 预印本 arXiv:2108.03272*，2021年。

+   [50] C. An, S. Gong, M. Zhong, X. Zhao, M. Li, J. Zhang, L. Kong, 和 X. Qiu，“L-eval：为长上下文语言模型建立标准化评估，”*arXiv 预印本 arXiv:2307.11088*，2023年。

+   [51] Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou *等*，“Longbench：用于长上下文理解的双语多任务基准，”*arXiv 预印本 arXiv:2308.14508*，2023年。

+   [52] M. Levy, A. Jacoby, 和 Y. Goldberg，“相同任务，更多令牌：输入长度对大语言模型推理性能的影响，”*arXiv 预印本 arXiv:2402.14848*，2024年。

+   [53] D. Ahmetovic, J. Guerreiro, E. Ohn-Bar, K. M. Kitani, 和 C. Asakawa，“专业知识对视觉障碍人士导航辅助互动偏好的影响，”发表于 *第16届全球无障碍网络会议论文集*，2019年，页码 1-9。
