- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从LLM到对话代理：具有大型语言模型微调的记忆增强架构
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02777](https://ar5iv.labs.arxiv.org/html/2401.02777)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.02777](https://ar5iv.labs.arxiv.org/html/2401.02777)
- en: Na Liu, Liangyu Chen, Xiaoyu Tian,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Na Liu, Liangyu Chen, Xiaoyu Tian,
- en: Wei Zou, Kaijiang Chen, Ming Cui Beike Inc., Beijing, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Zou, Kaijiang Chen, Ming Cui Beike Inc., Beijing, China
- en: '{liuna013, chenliangyu003, tianxiaoyu011,'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{liuna013, chenliangyu003, tianxiaoyu011,'
- en: zouwei026, chenkaijiang001, cuiming001}@ke.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: zouwei026, chenkaijiang001, cuiming001}@ke.com
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples),
    an advanced architecture enhancing the integration of Large Language Models (LLMs)
    like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework,
    incorporates a dual-component memory system, mirroring human short-term and long-term
    memory, to maintain context and continuity in conversations. It entails a comprehensive
    agent construction scenario, including phases like Conversation Selection, Scene
    Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training
    phase. This approach appears to enhance agent controllability and adaptability
    in complex, multi-turn dialogues. Our preliminary evaluations in a real estate
    sales context suggest that RAISE has some advantages over traditional agents,
    indicating its potential for broader applications. This work contributes to the
    AI field by providing a robust framework for developing more context-aware and
    versatile conversational agents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了RAISE（通过便笺和示例进行推理和行动），这是一种先进的架构，增强了像GPT-4这样的**大型语言模型（LLMs）**在对话代理中的集成。RAISE是对ReAct框架的增强，结合了双组件记忆系统，类似于人类的短期和长期记忆，以保持对话中的上下文和连续性。它包含一个全面的代理构建场景，包括对话选择、场景提取、CoT完成和场景增强等阶段，最终进入LLMs训练阶段。这种方法似乎提升了代理在复杂、多轮对话中的可控性和适应性。我们在房地产销售环境中的初步评估表明，RAISE相较于传统代理有一些优势，显示出其更广泛应用的潜力。这项工作通过提供一个强健的框架，致力于开发更具上下文意识和多功能的对话代理，为AI领域做出了贡献。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The landscape of Artificial Intelligence (AI) is continuously evolving, with
    Large Language Models (LLMs) emerging as pivotal components in the advancement
    towards Artificial General Intelligence (AGI)[[Ouyang et al., 2022](#bib.bibx18),
    [Wei et al., 2022a](#bib.bibx29), [Bubeck et al., 2023](#bib.bibx3)]. These models,
    exemplified by GPT-4[[OpenAI, 2023b](#bib.bibx17)] and similar architectures,
    have demonstrated remarkable proficiency in a range of tasks, from conversation
    and reasoning to complex problem-solving in various domains. The versatility of
    LLMs has been further enriched by innovative prompting strategies and the integration
    of external tools, enhancing their capabilities beyond basic language processing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）的格局正在不断演变，大型语言模型（LLMs）作为实现人工通用智能（AGI）的关键组成部分而出现[[Ouyang et al., 2022](#bib.bibx18),
    [Wei et al., 2022a](#bib.bibx29), [Bubeck et al., 2023](#bib.bibx3)]。这些模型以GPT-4[[OpenAI,
    2023b](#bib.bibx17)]及类似架构为代表，在各种任务中展现出卓越的能力，从对话和推理到复杂问题解决。LLMs的多功能性通过创新的提示策略和外部工具的集成得到了进一步增强，超越了基本的语言处理能力。
- en: However, a significant challenge in the realm of LLMs lies in their integration
    into conversational agents[[Weng, 2023](#bib.bibx31), [Wang et al., 2023a](#bib.bibx25),
    [Sumers et al., 2023](#bib.bibx24), [Xi et al., 2023](#bib.bibx32)] . While these
    models exhibit high levels of performance in isolated tasks, creating an agent
    that can sustain coherent, context-aware, and purpose-driven conversations remains
    an intricate endeavor. The need for a more sophisticated framework that leverages
    the strengths of LLMs while addressing their limitations in conversational settings
    has become increasingly apparent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在LLMs领域的一个重大挑战是它们在对话代理中的集成[[Weng, 2023](#bib.bibx31), [Wang et al., 2023a](#bib.bibx25),
    [Sumers et al., 2023](#bib.bibx24), [Xi et al., 2023](#bib.bibx32)]。虽然这些模型在孤立任务中表现出高水平的性能，但创建一个能够维持连贯、上下文敏感且目标驱动的对话的代理仍然是一项复杂的工作。需要一个更复杂的框架来发挥LLMs的优势，同时解决其在对话环境中的局限性，这一点已变得越来越明显。
- en: In response to this need, we introduce the RAISE (Reasoning and Acting through
    Scratchpad and Examples) architecture. RAISE represents a refined enhancement
    of the existing ReAct[[Yao et al., 2023](#bib.bibx35)] framework, specifically
    designed to augment the capabilities of conversational agents. This paper presents
    a detailed exploration of RAISE, highlighting its unique components and the benefits
    it offers in the development of conversational agents.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这种需求，我们引入了RAISE（通过临时笔记和示例进行推理和行动）架构。RAISE代表了现有ReAct[[Yao et al., 2023](#bib.bibx35)]框架的精细化增强版，专门设计用于增强对话智能体的能力。本文详细探讨了RAISE，突出了其独特的组件及其在对话智能体开发中的优势。
- en: The cornerstone of RAISE is its incorporation of a dual-component memory system,
    analogous to the human brain’s short-term and long-term memory functions. The
    Scratchpad component functions as a transient storage, capturing and processing
    key information and conclusions from recent interactions, akin to short-term memory.
    In parallel, the retrieval module operates as the agent’s long-term memory, sourcing
    and incorporating examples relevant to the current conversational context. This
    enhanced memory mechanism can flexibly bolster the capabilities of conversational
    AI, and also provides a convenient interface for humans to customize and control
    the behavior of conversational AI systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE的核心在于其双组件记忆系统的融入，类似于人脑的短期和长期记忆功能。临时笔记组件作为瞬时存储，捕捉和处理最近互动中的关键信息和结论，类似于短期记忆。同时，检索模块作为智能体的长期记忆，提供与当前对话上下文相关的示例。这个增强的记忆机制可以灵活地提升对话AI的能力，同时也为人类提供了一个方便的界面，以定制和控制对话AI系统的行为。
- en: '![Refer to caption](img/673e521c4691947ecf76a3a1eecd968e.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/673e521c4691947ecf76a3a1eecd968e.png)'
- en: 'Figure 1: The overview of RAISE.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：RAISE的概述。
- en: Furthermore, the RAISE architecture is founded on a comprehensive agent construction
    scenario, emphasizing the creation of conversational agents from scratch to ensure
    authenticity and relevance in real-world interactions. This paper delineates the
    RAISE methodology, encompassing a sequence of meticulously orchestrated phases.
    These include Conversation Selection, Scene Extraction, CoT (Chain of Thought)[[Wei
    et al., 2022b](#bib.bibx30)] Completion, and Scene Augmentation, all leading up
    to the pivotal LLMs Training phase. This structured approach is instrumental in
    developing agents that excel not only in language processing but also in contextual
    awareness and adaptability, catering to a spectrum of conversational dynamics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAISE架构基于一个全面的智能体构建场景，强调从头创建对话智能体以确保在真实世界互动中的真实性和相关性。本文描述了RAISE方法，包括一系列精心组织的阶段。这些阶段包括对话选择、场景提取、CoT（思维链）[[Wei
    et al., 2022b](#bib.bibx30)] 完成和场景增强，所有这些都导致了关键的LLMs训练阶段。这种结构化的方法对开发不仅在语言处理方面表现出色，而且在上下文意识和适应性方面卓越的智能体至关重要，以应对各种对话动态。
- en: Our experimental evaluations, conducted on a specialized in-house dataset focused
    on real estate sales, demonstrate the superiority of RAISE over conventional conversational
    agents. The results showcase RAISE’s ability to handle complex, multi-turn conversations
    with enhanced context awareness and adaptability. While our experiments are centered
    on the real estate domain, the principles and methodologies underpinning RAISE
    are universally applicable, making it a versatile framework for various applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在专注于房地产销售的内部数据集上进行的实验评估显示，RAISE优于传统对话代理。结果展示了RAISE处理复杂、多轮对话的能力，并具有增强的上下文意识和适应性。虽然我们的实验集中于房地产领域，但RAISE的原则和方法具有普遍适用性，使其成为各种应用的多功能框架。
- en: 'In summary, this paper presents the following contributions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文提出了以下贡献：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce RAISE, a refined enhancement of the ReAct framework, which utilizes
    scratchpad and retrieved examples to augment the agent’s capabilities.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了RAISE，这是ReAct框架的一个精细化增强版，它利用临时笔记和检索的示例来提升智能体的能力。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a fine-tuning scenario for Large Language Models (LLMs) within RAISE,
    which, compared to the use of prompts alone, not only enhances the controllability
    of the agent but also improves its effectiveness and efficiency.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了RAISE中大型语言模型（LLMs）的微调场景，与单纯使用提示相比，这不仅提高了智能体的可控性，还提升了其效果和效率。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Through experiments conducted on our in-house dataset, we demonstrate RAISE’s
    superiority as a conversational agent. While our experiments are concentrated
    on real estate sales, the underlying principles and methodologies of RAISE have
    wide-ranging applications and can be adapted to various domains, highlighting
    its versatility.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过在我们内部数据集上进行的实验，我们展示了RAISE作为对话代理的优越性。虽然我们的实验集中在房地产销售上，但RAISE的基本原理和方法具有广泛的应用范围，可以适应各种领域，突出其多功能性。
- en: 2 Agent Framework
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 代理框架
- en: 'Inspired by ReAct [[Yao et al., 2022](#bib.bibx34)], we introduce RAISE architecture,
    as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models").
    The architecture primarily encompasses the following components.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 受ReAct [[Yao et al., 2022](#bib.bibx34)]启发，我们引入了RAISE架构，如图[1](#S1.F1 "图 1 ‣
    1 引言 ‣ 从LLM到对话代理：一种增强记忆的架构和大型语言模型的微调")所示。该架构主要包括以下组件。
- en: 2.1 Dialogue
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 对话
- en: The dialogue module serves as the core interface for user-agent communication.
    It handles incoming user queries and delivers tailored responses formulated by
    the agent.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对话模块作为用户-代理通信的核心接口。它处理来自用户的查询，并提供由代理制定的量身定制的响应。
- en: 2.2 LLMs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs
- en: As the agent’s brain, the LLMs requires capabilities for perception, task-specific
    planning, tool usage, and summarization. These skills can be developed on the
    LLMs using either prompt engineering [[Crispino et al., 2023](#bib.bibx8)] or
    fine-tuning methods [[Zeng et al., 2023](#bib.bibx37)]. Our study has conducted
    comparative experiments to stimulate these capabilities, utilizing models such
    as GPT-4 [[OpenAI, 2023b](#bib.bibx17)], GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)],
    and Qwen-14B-Chat [[Bai et al., 2023](#bib.bibx1)]. This paper explores the strengths
    and limitations of each model in handling specific task types and provides concrete
    metrics for evaluating their performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代理的大脑，LLMs需要具备感知、任务特定规划、工具使用和总结能力。这些技能可以通过提示工程[[Crispino et al., 2023](#bib.bibx8)]或微调方法[[Zeng
    et al., 2023](#bib.bibx37)]在LLMs上进行培养。我们的研究进行了比较实验来激发这些能力，利用了如GPT-4 [[OpenAI,
    2023b](#bib.bibx17)]、GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)]和Qwen-14B-Chat [[Bai
    et al., 2023](#bib.bibx1)]等模型。本文探讨了每个模型在处理特定任务类型时的优缺点，并提供了评估其性能的具体指标。
- en: 2.3 Memory
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 记忆
- en: 'The memory module in RAISE framework stores information perceived from its
    environment and facilitates the agent’s future actions. The memory includes the
    following components:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE框架中的记忆模块存储从环境中感知到的信息，并促进代理的未来行动。记忆包括以下组件：
- en: System Prompt  Includes profiles (detailing role identity, objectives, and behaviors),
    task instructions, tool descriptions, and few-shot learning elements for optimizing
    model performance. Flexibly designed, system prompt can either remain static or
    dynamically adjust to accommodate various stages of a dialogue and differing query
    types.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示  包括个人档案（详细说明角色身份、目标和行为）、任务说明、工具描述和用于优化模型性能的少量示例学习元素。系统提示设计灵活，可以保持静态或动态调整，以适应对话的不同阶段和不同的查询类型。
- en: Context  Includes conversation history and task trajectory. Conversation history
    records all query-response pairs within the dialogue, providing a complete context
    for more accurate agent perception. Task trajectory documents the decision-making
    trajectory, including plan designation, tool selection, and execution, guiding
    the agent’s future planning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文  包括对话历史和任务轨迹。对话历史记录对话中的所有查询-响应对，为更准确的代理感知提供完整的背景。任务轨迹记录决策过程，包括计划指定、工具选择和执行，指导代理的未来规划。
- en: Scratchpad  Logs background information, knowledge generated by reasoning and
    observations from previous tool usage, essential for efficiency in multi-turn
    interactions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 便签  记录背景信息、通过推理生成的知识以及之前工具使用中的观察，对于多轮交互的效率至关重要。
- en: Examples  Comprises query-response pairs used for recalling relevant examples
    to supplement the model’s and tools’ knowledge gaps and to customize agent behavior
    and expression.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 示例  包括用于回忆相关示例的查询-响应对，以弥补模型和工具的知识空白，并自定义代理的行为和表达。
- en: These four components collectively form the working memory of RAISE, with conversation
    history and scratchpad being dialogue-level, while examples and task trajectory
    are turn-level.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个组件共同构成了RAISE的工作记忆，其中对话历史和便签属于对话级别，而示例和任务轨迹则是轮次级别的。
- en: 2.4 Tool
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 工具
- en: The tool module enriches LLMs after pretraining and Supervised Fine-Tuning (SFT)
    by integrating external knowledge sources and resources. This module incorporates
    a diverse array of tools, including but not limited to databases for data retrieval,
    APIs for system interactions, sophisticated recommendation systems, and collaborative
    frameworks involving other LLMs or agents. The description file for a tool typically
    needs to include the tool’s name, its function, essential parameters, optional
    parameters, and may also include some usage examples. This descriptive file aids
    agents in better planning, tool selection, parameter generation for tools, and
    execution of those tools.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 工具模块通过整合外部知识源和资源，丰富了经过预训练和监督微调（SFT）的语言模型（LLMs）。该模块包含各种工具，包括但不限于用于数据检索的数据库、用于系统交互的API、复杂的推荐系统以及涉及其他LLMs或代理的协作框架。工具的描述文件通常需要包括工具的名称、功能、必需参数、可选参数，并且可能还包括一些使用示例。这个描述文件帮助代理更好地进行规划、选择工具、生成工具参数和执行这些工具。
- en: '2.5 Controller: Control Agent Loop'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 控制器：控制代理循环
- en: The controller module connects the aforementioned modules through preset trigger
    conditions. Upon receiving a new query, the agent executes the loop of perception,
    planning, tool selection, and tool execution. The specific process is as follows.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器模块通过预设的触发条件连接上述模块。在收到新查询后，代理执行感知、规划、工具选择和工具执行的循环。具体过程如下。
- en: Memory Update  At the beginning of a conversation, the Scratchpad records the
    context of the dialogue, including user and agent roles, date, time, etc.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 内存更新  在对话开始时，Scratchpad记录对话的上下文，包括用户和代理角色、日期、时间等。
- en: 'During the conversation, each time a user query is received, the system will:
    (1) Add the user’s query to the Conversation History; (2) Recall top-$k$ relevant
    examples from the Example Pool for the current task, based on the historical and
    current query, using vector retrieval; (3) Update the current entity information
    in the Scratchpad if the user’s query contains a product link; (4) Update the
    agent’s trajectory in the Task Memory and the results of tool usage in the Scratchpad
    during task execution; (5) Post-task completion, include the agent’s final output
    in the Conversation History.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在对话过程中，每次收到用户查询时，系统将：(1) 将用户的查询添加到对话历史中；(2) 基于历史和当前查询，从示例池中回忆与当前任务相关的前$k$个示例，使用向量检索；(3)
    如果用户的查询包含产品链接，则更新Scratchpad中的当前实体信息；(4) 更新任务执行过程中Scratchpad中的代理轨迹和工具使用结果；(5) 任务完成后，将代理的最终输出包含在对话历史中。
- en: '![Refer to caption](img/923f78c197d1d3e409623f90f0f0151c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/923f78c197d1d3e409623f90f0f0151c.png)'
- en: 'Figure 2: Task Inference Prompt Template.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：任务推理提示模板。
- en: 'Task Planning  After collecting the above information, it is combined into
    a complete task inference prompt according to the designed template, as illustrated
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.5 Controller: Control Agent Loop ‣ 2 Agent
    Framework ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with
    Fine-Tuning of Large Language Models"). An example of the complete prompt is available
    in Table [5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models")
    of the Appendix. The LLM utilizes the information within the prompt for perception
    and planning, subsequently outputting actions in accordance with the format outlined
    in the prompt. If an action involves invoking a tool, it should specify the tool’s
    name and input parameters.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '任务规划  在收集上述信息后，按照设计的模板将其合并为完整的任务推理提示，如图 [2](#S2.F2 "Figure 2 ‣ 2.5 Controller:
    Control Agent Loop ‣ 2 Agent Framework ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models")所示。完整提示的示例可在附录的表
    [5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ From LLM to Conversational Agent:
    A Memory Enhanced Architecture with Fine-Tuning of Large Language Models") 中找到。LLM利用提示中的信息进行感知和规划，然后按照提示中概述的格式输出动作。如果一个动作涉及调用工具，应指定工具的名称和输入参数。'
- en: Tool Execution  This phase involves executing the tool selected in the previous
    step. The command for tool execution may either be directly output by the agent
    or correspond to a manually crafted function specific to each tool. The output
    of the execution is formatted as predetermined.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 工具执行  这个阶段涉及执行在前一步中选择的工具。工具执行的命令可能由代理直接输出，也可能对应于特定于每个工具的手动编写函数。执行的输出格式是预定的。
- en: Summary  The agent, synthesizing all the information gathered from the environment,
    decides whether it can respond to the user’s query. Termination criteria might
    include having gathered sufficient information, exceeding a preset number of loops,
    or encountering a system error. Upon meeting any of these conditions, the agent
    can proceed to summarize its findings and provide a response.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要  代理在综合了从环境中收集的所有信息后，决定是否能够回应用户的查询。终止标准可能包括已收集到足够的信息、超过预设的循环次数或遇到系统错误。一旦满足这些条件中的任何一个，代理可以开始总结其发现并提供回应。
- en: 3 Agent Tuning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 代理微调
- en: 'Section [2](#S2 "2 Agent Framework ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models") presented the
    RAISE architecture, establishing a hardware base for agents in complex dialogues.
    This section shifts focus to software enhancements for RAISE, particularly activating
    LLMs as the agent’s core. Despite the success of open-source LLMs in various tasks,
    studies [[Liu et al., 2023](#bib.bibx13)] reveal their limitations in real-world
    scenarios, especially compared to GPT-3.5 [[OpenAI, 2023a](#bib.bibx16)] and GPT-4
    [[OpenAI, 2023b](#bib.bibx17)]. Addressing this gap, this paper introduces a versatile
    finetuning method suitable for complex agent applications.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 第[2](#S2 "2 代理框架 ‣ 从LLM到对话代理：一种内存增强的架构，结合大型语言模型的微调")节介绍了RAISE架构，为复杂对话中的代理建立了硬件基础。本节转向RAISE的软件增强，特别是将LLM激活为代理的核心。尽管开源LLM在各种任务中取得了成功，但研究[[Liu
    et al., 2023](#bib.bibx13)]揭示了它们在实际应用中的局限性，尤其是与GPT-3.5[[OpenAI, 2023a](#bib.bibx16)]和GPT-4[[OpenAI,
    2023b](#bib.bibx17)]相比。为弥补这一差距，本文介绍了一种适用于复杂代理应用的多功能微调方法。
- en: 3.1 Build Datasets
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 构建数据集
- en: '![Refer to caption](img/e62187d3ee9de3b9c6d4bbedc81a0454.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e62187d3ee9de3b9c6d4bbedc81a0454.png)'
- en: 'Figure 3: Dataset Construction Pipeline for Agent Training.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：代理训练的数据集构建流程。
- en: 'The creation of training data entails significant costs. Our objective is to
    finetune the model efficiently using a compact yet high-quality dataset that precisely
    aligns with specific role-based behavioral logic. The dataset must fulfill these
    criteria:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的创建需要 significant costs。我们的目标是使用 compact yet high-quality 数据集有效地微调模型，该数据集与特定角色的行为逻辑精确对齐。数据集必须满足以下标准：
- en: Authenticity  It should closely mimic real-life scenarios.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 真实性  数据应尽可能接近真实场景。
- en: Diversity  The data should encompass a wide range of scenarios.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性  数据应涵盖广泛的场景。
- en: High Quality  The data must have an accurate Chain of Thought (CoT) process,
    encompassing aspects like planning, tool utilization, and response formulation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量  数据必须具有准确的思维链（CoT）过程，包括规划、工具使用和回应制定等方面。
- en: 'As shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning
    ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models"), our proposed pipeline comprises several stages, including
    Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation.
    The details of each stage are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3](#S3.F3 "图3 ‣ 3.1 构建数据集 ‣ 3 代理微调 ‣ 从LLM到对话代理：一种内存增强的架构，结合大型语言模型的微调")所示，我们提出的流程包括多个阶段，包括对话选择、场景提取、CoT完成和场景增强。每个阶段的详细信息如下：
- en: 3.1.1 Conversation Selection
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 对话选择
- en: 'To emulate specific roles in real scenarios, we start by filtering conversations
    from authentic dialogues based on criteria such as scene completion, a minimum
    number of dialogue turns, high conversation quality, and a threshold for user
    message ratio. These selected dialogues are then anonymized for further processing,
    as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning
    ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models")(a).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟真实场景中的特定角色，我们首先根据场景完成情况、对话轮次、对话质量和用户消息比例阈值等标准过滤真实对话。这些选定的对话随后会被匿名化处理，如图[3](#S3.F3
    "图3 ‣ 3.1 构建数据集 ‣ 3 代理微调 ‣ 从LLM到对话代理：一种内存增强的架构，结合大型语言模型的微调")(a)所示。
- en: 3.1.2 Scene Extraction
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 场景提取
- en: 'Each round of interaction serves as a segmentation point, dividing the previously
    selected dialogues into multiple samples, as shown in Figure [3](#S3.F3 "Figure
    3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to Conversational Agent: A
    Memory Enhanced Architecture with Fine-Tuning of Large Language Models")(b). Each
    sample is an original scene (defined as $Scene_{origin}$ comprises the following
    components:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每轮交互作为一个分割点，将先前选择的对话划分为多个样本，如图[3](#S3.F3 "图 3 ‣ 3.1 构建数据集 ‣ 3 代理调整 ‣ 从LLM到对话代理：具有内存增强的架构与大型语言模型的微调")(b)所示。每个样本是一个原始场景（定义为$Scene_{origin}$，包括以下组件：
- en: '|  | $$\displaystyle Scene^{origin}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle Scene^{origin}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
- en: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (1) |'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (1) |'
- en: Subsequently, to ensure diversity, we perform sampling based on dialogue turn
    counts and the intents behind user queries, resulting in a dataset rich in varied
    scene types.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，为了确保多样性，我们基于对话轮次和用户查询背后的意图进行采样，从而生成一个包含多种场景类型的数据集。
- en: 3.1.3 CoT Completion
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 CoT完成
- en: 'In refining the training data for the RAISE framework, the next phase involves
    enhancing the original scenes with a CoT process, which bridges the gap between
    user queries and character responses. This CoT process encompasses perception,
    planning, tool selection, and execution. Studies[[Nori et al., 2023](#bib.bibx15)]
    have demonstrated GPT-4’s efficacy in generating high-quality CoT prompts for
    intricate scenarios. In this study, we initially utilize GPT-4 for automated generation,
    followed by meticulous manual validation of the output. To assist GPT-4 in consistently
    generating CoT processes, we incorporate additional elements such as predefined
    profiles, tools, and few-shot examples into the original scene, which collectively
    shape the construction of the prompt, as shown in Figure [3](#S3.F3 "Figure 3
    ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models")(c). The refined
    complete scene thus includes the following elements:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在完善RAISE框架的训练数据时，下一阶段涉及通过CoT过程增强原始场景，该过程弥合了用户查询与角色响应之间的差距。这个CoT过程包括感知、规划、工具选择和执行。研究[[Nori
    et al., 2023](#bib.bibx15)]证明了GPT-4在生成高质量CoT提示方面的有效性。在这项研究中，我们最初利用GPT-4进行自动生成，然后对输出进行仔细的人工验证。为了帮助GPT-4持续生成CoT过程，我们将预定义的配置文件、工具和少量示例等额外元素纳入原始场景中，这些元素共同塑造了提示的构建，如图[3](#S3.F3
    "图 3 ‣ 3.1 构建数据集 ‣ 3 代理调整 ‣ 从LLM到对话代理：具有内存增强的架构与大型语言模型的微调")(c)所示。经过改进的完整场景包括以下元素：
- en: '|  | $$\displaystyle Scene^{complete}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle Scene^{complete}_{t}=\left\{\begin{aligned} &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\
    &amp;Query_{t}:Q_{t}\\'
- en: '&amp;CoT_{t}:Thought,Action,Observaton\\'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;CoT_{t}:思维、行动、观察\\'
- en: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (2) |'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (2) |'
- en: 3.1.4 Scene Augmentation
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 场景增强
- en: 'While the Scene Extraction phase ensured diversity through actual data sampling
    and the CoT Completion phase added the necessary CoT intricacies, two critical
    challenges still need addressing:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然场景提取阶段通过实际数据采样确保了多样性，而CoT完成阶段则添加了必要的CoT复杂性，但仍需解决两个关键挑战：
- en: Role Hallucination  LLMs, endowed with vast domain knowledge from pre-training
    and fine-tuning, exhibit extensive capabilities. However, if left unchecked, our
    trained agents might retain these broad skills, which could conflict with their
    intended functional roles. For example, an agent designed to provide sales services
    might erroneously possess skills like coding in Python or offering recipe advice.
    To counter this, we introduce specific scenarios that teach the agent its capability
    limits, essentially making it ’unlearn’ the general abilities of LLMs within these
    defined contexts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 角色幻想   LLMs具备广泛的领域知识，通过预训练和微调展现出强大的能力。然而，如果不加以控制，我们训练的代理可能会保留这些广泛的技能，可能与其预期的功能角色冲突。例如，一个旨在提供销售服务的代理可能会错误地具备如Python编码或提供食谱建议的技能。为应对这一问题，我们引入了特定场景，以教会代理其能力限制，本质上使其在这些定义的上下文中‘遗忘’LLMs的通用能力。
- en: '![Refer to caption](img/19080bce003ca08a3b49c2d15ae6222a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/19080bce003ca08a3b49c2d15ae6222a.png)'
- en: 'Figure 4: Comparison of 5 Agent Frameworks.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 5个代理框架的比较。'
- en: Knowledge Hallucination  This phenomenon involves creating unrealistic or incorrect
    statements due to inadequate or misapplied knowledge acquired during pre-training.
    To mitigate this, we incorporate scenarios where the agent, despite tool utilization,
    still lacks essential factual knowledge, resulting in inability to respond accurately.
    In instances requiring factual accuracy, the agent should base its responses on
    knowledge acquired from its working memory or through tool interaction, rather
    than relying on its pre-trained database.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 知识幻觉   这一现象涉及由于在预训练过程中获得的知识不足或应用不当，导致创建不现实或不正确的陈述。为缓解这一问题，我们引入了代理尽管使用了工具，但仍然缺乏必要事实知识的场景，导致无法准确响应。在需要事实准确性的情况下，代理应根据从工作记忆或通过工具互动获得的知识来回答，而不是依赖于其预训练的数据库。
- en: 'To overcome these issues, we perform data augmentation on these two categories
    of data, which are not included in the real online datasets, as demonstrated in
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From LLM to
    Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large
    Language Models")(d).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这些问题，我们对这两类数据进行了数据增强，这些数据未包含在真实在线数据集中，如图 [3](#S3.F3 "Figure 3 ‣ 3.1 Build
    Datasets ‣ 3 Agent Tuning ‣ From LLM to Conversational Agent: A Memory Enhanced
    Architecture with Fine-Tuning of Large Language Models")(d)所示。'
- en: 3.2 LLMs Training
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLMs 训练
- en: 'Following the previous phase, we have acquired a dataset characterized by both
    high quality and diversity. Each sample in this dataset, identified as $Scene_{complete}$:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一阶段之后，我们获得了一个具有高质量和多样性的 dataset。这个数据集中的每个样本，被标识为 $Scene_{complete}$：
- en: '|  | $$\displaystyle Sample=\left\{\begin{aligned} &amp;SystemPrompt:Profile,Instruction,\dots\\
    &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle Sample=\left\{\begin{aligned} &amp;SystemPrompt:Profile,Instruction,\dots\\
    &amp;History_{t}:Q_{1},A_{1},\dots,Q_{t-1},A_{t-1}\\'
- en: '&amp;Query_{t}:Q_{t}\\'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Query_{t}:Q_{t}\\'
- en: '&amp;CoT_{t}:Thought,Action,Observaton\\'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;CoT_{t}:Thought,Action,Observaton\\'
- en: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (3) |'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;Response_{t}:A_{t}\end{aligned}\right.$$ |  | (3) |'
- en: 'These instances are then processed into a format conducive for full-parameter
    fine-tuning of open-source LLMs. Our experiments have led to an encouraging discovery:
    by constructing a modest amount (¡$1K$) of high-quality, representative data,
    the RAISE framework achieved impressive results.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实例被处理成适合对开源 LLM 进行全参数微调的格式。我们的实验带来了一个令人鼓舞的发现：通过构建少量（¡$1K$）高质量、具有代表性的数据，RAISE
    框架取得了令人印象深刻的结果。
- en: 4 Experiments
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: To demonstrate the effectiveness of the RAISE architecture and the fine-tuning
    method proposed in this paper in complex real-world scenarios, we conducted experiments
    in a real estate online Instant Messaging (IM) dialogue setting. In this scenario,
    the user is a customer inquiring about real estate purchases, and the agent assumes
    the role of a real estate consultant. A detailed introduction to the dataset,
    toolset, and the method of activating agent capabilities in LLMs is provided below.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 RAISE 架构和本文提出的微调方法在复杂现实场景中的有效性，我们在一个房地产在线即时消息（IM）对话环境中进行了实验。在这个场景中，用户是一个咨询房地产购买的客户，而代理则扮演房地产顾问的角色。下面提供了数据集、工具集的详细介绍，以及在
    LLMs 中激活代理能力的方法。
- en: 4.1 Datasets
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: 'To ascertain the effectiveness of the RAISE framework, we conducted comparative
    evaluations with various architectures, including Act-Only, ReAct, ReAct+Scratchpad,
    ReAct+Examples, and RAISE. This comparison aimed to ensure fair evaluation across
    different models, maintaining uniform dialogue scenarios and consistent additional
    knowledge in identical training samples across various datasets. The full trajectories
    for a single scenario under each architecture are depicted in Figure [4](#S3.F4
    "Figure 4 ‣ 3.1.4 Scene Augmentation ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From
    LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of
    Large Language Models").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确定 RAISE 框架的有效性，我们进行了与多种架构的比较评估，包括 Act-Only、ReAct、ReAct+Scratchpad、ReAct+Examples
    和 RAISE。这一比较旨在确保在不同模型之间的公平评估，保持统一的对话场景和一致的额外知识，在各种数据集中的相同训练样本中。每种架构下单一场景的完整轨迹如图
    [4](#S3.F4 "Figure 4 ‣ 3.1.4 Scene Augmentation ‣ 3.1 Build Datasets ‣ 3 Agent
    Tuning ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with
    Fine-Tuning of Large Language Models")所示。'
- en: 'Initially, we generated the ReAct architecture dataset following the procedure
    outlined in Section [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning
    ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning
    of Large Language Models"). We then modified this data to create training sets
    for the other architectures. The methodologies applied were as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '最初，我们按照第 [3](#S3.F3 "Figure 3 ‣ 3.1 Build Datasets ‣ 3 Agent Tuning ‣ From
    LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of
    Large Language Models") 节中描述的程序生成了 ReAct 架构数据集。然后我们修改了这些数据，以创建其他架构的训练集。应用的方法如下：'
- en: Act-Only  This architecture was formed by removing the ’thought’ process from
    ReAct, allowing for straightforward generation through coding.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Act-Only 这种架构通过去除 ReAct 的“思考”过程而形成，使得生成过程通过编码变得直接而简单。
- en: ReAct+Scratchpad  Building upon ReAct, the initial Scratchpad distribution comprised
    20% empty, 30% partially informative, and 50% fully informative content. The ReAct
    data, when combined with varying output requirements, served as prompts for the
    regeneration of the CoT process using GPT-4.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct+Scratchpad 在 ReAct 的基础上，最初的 Scratchpad 分布包含 20% 空白，30% 部分信息，50% 完全信息内容。当
    ReAct 数据与不同的输出要求结合时，作为 GPT-4 驱动的 CoT 过程再生的提示。
- en: ReAct+Examples  Similar to ReAct+Scratchpad, with the distribution of Examples
    set at 20% empty, 30% partially informative, and 50% fully informative. The ReAct
    data, merged with diverse output requirements, were reformulated into prompts
    for GPT-4-driven CoT regeneration.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct+Examples 类似于 ReAct+Scratchpad，其中 Examples 的分布设置为 20% 空白，30% 部分信息，50% 完全信息。将
    ReAct 数据与不同的输出要求合并，重新制定成 GPT-4 驱动的 CoT 再生提示。
- en: RAISE  This model integrated aspects of both ReAct+Scratchpad and ReAct+Examples.
    The CoT process was similarly regenerated using GPT-4.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE 该模型整合了 ReAct+Scratchpad 和 ReAct+Examples 的各个方面。CoT 过程同样使用 GPT-4 进行再生。
- en: This structured approach enabled a thorough evaluation of each architectural
    element within the RAISE framework, clearly demonstrating the incremental benefits
    introduced by each component. Following the outlined procedure, a total of 948
    scenes were generated. Out of these, 100 were randomly selected to serve as the
    evaluation set, while the remaining 848 instances were used for fine-tuning the
    model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构化的方法使得在 RAISE 框架内对每个架构元素进行了全面评估，清楚地展示了每个组件带来的增益。按照所述程序，共生成了 948 个场景。其中特别随机选择了
    100 个作为评估集，其余 848 个实例则用于对模型进行微调。
- en: 4.2 Tools
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 工具
- en: 'Based on real estate online IM conversations, we have abstractly defined the
    following 12 tools, each including the tool name, input parameters, and functions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于房地产在线 IM 对话，我们抽象定义了以下 12 种工具，每种工具包括工具名称、输入参数和功能：
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real Estate Consultant Information $[agent\_ucid]$: Retrieves the consultant’s
    name, contact details, WeChat ID, ranking, performance metrics, and more.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房地产顾问信息 $[agent\_ucid]$: 检索顾问的姓名、联系方式、微信 ID、排名、绩效指标等。'
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'House Information $[house\_id]$: Offers essential details about a property,
    including its size, price, floor level, school district presence, and renovation
    status.'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房屋信息 $[house\_id]$: 提供关于物业的基本信息，包括大小、价格、楼层、学区情况和装修状态。'
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Community Information $[resblock\_id]$: Provides insights into the community,
    covering aspects like green spaces, property management, building specifications,
    proximity to subway stations, schools, and medical facilities.'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '社区信息 $[resblock\_id]$: 提供社区的洞察，包括绿地、物业管理、建筑规格、地铁站、学校和医疗设施的接近程度等方面。'
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'House Layout Analysis $[frame\_id]$: Analyzes the strengths and weaknesses
    of a property’s layout.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房屋布局分析 $[frame\_id]$: 分析物业布局的优缺点。'
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'House Price Changes $[house\_id]$: Tracks price fluctuations for a specific
    property.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '房价变动 $[house\_id]$: 跟踪特定物业的价格波动。'
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Community Price Changes $[resblock\_id]$: Reports on average price trends within
    a particular community.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '社区价格变动 $[resblock\_id]$: 报告特定社区内的平均价格趋势。'
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Community Transactions $[resblock\_id]$: Accesses recent transaction data from
    the same community.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '社区交易 $[resblock\_id]$: 访问同一社区的最新交易数据。'
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tax Policy $[city\_id]$: Updates on the latest tax regulations and implications.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '税收政策 $[city\_id]$: 更新最新的税收法规及其影响。'
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loan Policy $[city\_id]$: Delivers current information on loan policies.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '贷款政策 $[city\_id]$: 提供当前的贷款政策信息。'
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Market Analysis $[city\_id]$: Provides up-to-date real estate market insights.'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '市场分析 $[city\_id]$: 提供最新的房地产市场洞察。'
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recommend Listings $[Conversation\ History]$: Suggests property listings to
    customers based on their conversation history and inferred needs, including rationale
    for each recommendation.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '推荐列表 $[Conversation\ History]$: 根据客户的对话历史和推断需求，向客户推荐房产列表，并提供每个推荐的理由。'
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Value Report $[house\_id]$: Generates a comprehensive value report card for
    a property, aimed at engaging customers and encouraging them to share their contact
    details.'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Value Report $[house\_id]$: 为物业生成全面的价值报告，旨在吸引客户并鼓励他们提供联系信息。'
- en: 4.3 LLMs
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LLMs
- en: 'The models used in this study are as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的模型如下：
- en: OpenAI GPT We utilized GPT-4 for generating all fine-tuning data and employed
    both GPT-3.5 and GPT-4 for prompting purposes. Both models were operated in ChatCompletion
    mode as of November 2023, with the temperature set to 0.5.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT 我们利用 GPT-4 生成了所有微调数据，并使用 GPT-3.5 和 GPT-4 进行提示。两种模型均在 2023 年 11 月以
    ChatCompletion 模式运行，温度设置为 0.5。
- en: 'Qwen-14B-Chat An open-source conversational model from Alibaba Cloud, featuring
    14 billion parameters, which has demonstrated exceptional performance in tool
    utilization [[Chen et al., 2023b](#bib.bibx6)]. Qwen-14B-Chat was used for both
    fine-tuning and prompting. The parameter configuration for the Supervised Fine-Tuning
    (SFT) is detailed in Table [1](#S4.T1 "Table 1 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From
    LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of
    Large Language Models"). The hyperparameter settings for the prompting and fine-tuning
    methods during inference are identical, also shown in Table [1](#S4.T1 "Table
    1 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From LLM to Conversational Agent: A Memory Enhanced
    Architecture with Fine-Tuning of Large Language Models"). In the inference phase,
    we utilized an NVIDIA A100 GPU equipped with 80GB of memory, offering robust computational
    power and substantial memory capacity for efficient processing.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Qwen-14B-Chat 来自阿里巴巴云的开源对话模型，拥有 140 亿参数，在工具使用方面表现出色 [[Chen et al., 2023b](#bib.bibx6)]。Qwen-14B-Chat
    用于微调和提示。监督微调（SFT）的参数配置详见表 [1](#S4.T1 "表 1 ‣ 4.3 LLMs ‣ 4 实验 ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")。提示和微调方法的超参数设置在推理阶段是相同的，也在表
    [1](#S4.T1 "表 1 ‣ 4.3 LLMs ‣ 4 实验 ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构") 中显示。在推理阶段，我们使用了配备
    80GB 内存的 NVIDIA A100 GPU，提供强大的计算能力和充足的内存容量，以实现高效处理。
- en: 'Another distinction between prompting and fine-tuning methods is the use of
    one-shot guidance in prompting for structured output generation, whereas fine-tuning
    omits this step. Complete prompts for various architectures are detailed in the
    appendix [A](#A1 "Appendix A Appendix ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models").'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个提示与微调方法之间的区别是提示方法中使用了单次指导来生成结构化输出，而微调则省略了这一步。各种架构的完整提示在附录[A](#A1 "附录 A 附录
    ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")中详细介绍。
- en: 'Table 1: Hyper-parameter settings for SFT and Inference'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：SFT 和推理的超参数设置
- en: '| SFT Hyper-parameters |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SFT 超参数 |'
- en: '| Hyper parameter | Value |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| precision | bfloat16 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| precision | bfloat16 |'
- en: '| model_max_length | 4096 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| model_max_length | 4096 |'
- en: '| epochs | 3 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| epochs | 3 |'
- en: '| batch size | 64 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| batch size | 64 |'
- en: '| learning rate | 5e-6 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| learning rate | 5e-6 |'
- en: '| warmup ratio | 0.03 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| warmup ratio | 0.03 |'
- en: '| LR scheduler type | cosine |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| LR scheduler type | cosine |'
- en: '| Inference Hyper-parameters |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 推理超参数 |'
- en: '| Hyper parameter | Value |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| max_new_tokens | 300 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| max_new_tokens | 300 |'
- en: '| top_p | 0.85 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| top_p | 0.85 |'
- en: '| temperature | 0.5 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| temperature | 0.5 |'
- en: '| repetition_penalty | 1.1 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| repetition_penalty | 1.1 |'
- en: 'Table 2: The evaluation criteria details of the defined metrics.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：已定义指标的评估标准详细信息。
- en: '| Dimension | Metric | Score | Description |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 指标 | 分数 | 描述 |'
- en: '| Quality | Specificity | 0 | Vague, general answer without specific information
    or details. |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 质量 | 具体性 | 0 | 含糊的、没有具体信息或细节的回答。 |'
- en: '| 1 | Provides some specifics, but lacks detail or full relevance to the question.
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 提供了一些具体信息，但缺乏细节或与问题的全面相关性。 |'
- en: '| 2 | Directly addressing the user’s query with detailed and specific information.
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 直接回应用户的问题，提供详细和具体的信息。 |'
- en: '| Factuality | 0 | Contains false information, clearly contradicts facts. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 事实性 | 0 | 包含虚假信息，明显与事实相矛盾。 |'
- en: '| 1 | Mostly accurate, with minor inaccuracies or oversights. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 大多数情况下准确，但有轻微的不准确或遗漏。 |'
- en: '| 2 | Completely accurate, all information is fact-checked. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 完全准确，所有信息都经过事实核查。 |'
- en: '| Coherence | 0 | Logically disorganized, unrelated to prior content or overall
    topic. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 连贯性 | 0 | 逻辑上混乱，与先前内容或总体主题无关。 |'
- en: '| 1 | Generally coherent, with some logical inconsistencies. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 一般连贯，存在一些逻辑不一致。 |'
- en: '| 2 | Very coherent, logically sound, closely aligned with the conversation
    topic. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 非常连贯，逻辑上合理，与对话主题紧密相关。 |'
- en: '| Naturalness | 0 | Mechanical and unnatural, deviating from human conversational
    norms. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 自然性 | 0 | 机械且不自然，偏离人类对话规范。 |'
- en: '| 1 | Imitates natural dialogue to an extent, but still somewhat stiff or unnatural.
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 在一定程度上模仿自然对话，但仍有些僵硬或不自然。 |'
- en: '| 2 | Smooth and natural, akin to human dialogue, easily understood and accepted.
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 顺畅自然，类似于人类对话，易于理解和接受。 |'
- en: '| Efficiency | Plan Steps | - | Number of planning steps. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 效率 | 计划步骤 | - | 计划步骤的数量。 |'
- en: '| Action Steps | - | Number of action steps. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 行动步骤 | - | 行动步骤的数量。 |'
- en: '| Inference Speed | - | The average time taken to process each user query,
    measured in seconds. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 推理速度 | - | 处理每个用户查询所需的平均时间，以秒为单位。 |'
- en: 4.4 Evaluation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 评估
- en: 'In the challenging landscape of human-computer dialogue systems, the evaluation
    of agent performance necessitates a nuanced approach [[Liu et al., 2023](#bib.bibx13),
    [Wang et al., 2023b](#bib.bibx26)]. This is particularly pertinent when agents
    are tasked with engaging in direct conversations with human users, where the ultimate
    goal is to nurture a trust-based relationship. To achieve this, agents must exhibit
    a spectrum of qualities: they must be not only helpful and trustworthy but also
    responsive in a timely manner, and capable of understanding and articulating responses
    in a variety of contexts, akin to human interaction. This paper delineates seven
    sophisticated metrics designed to rigorously assess both the quality and efficiency
    of agent responses. The specific metrics and their corresponding scoring criteria
    are detailed in Table [2](#S4.T2 "Table 2 ‣ 4.3 LLMs ‣ 4 Experiments ‣ From LLM
    to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large
    Language Models").'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '在人机对话系统的挑战性环境中，评估代理的性能需要一种细致的方式 [[Liu et al., 2023](#bib.bibx13), [Wang et
    al., 2023b](#bib.bibx26)]。这在代理与人类用户直接对话时尤为重要，因为*最终目标*是培养基于信任的关系。为此，代理必须展示一系列品质：不仅要有帮助和可信赖，还要及时响应，并能够在各种情境中理解和表达回应，类似于人类互动。本文
    delineates 七种复杂的指标，旨在严格评估代理响应的质量和效率。具体指标及其评分标准详见表 [2](#S4.T2 "Table 2 ‣ 4.3 LLMs
    ‣ 4 Experiments ‣ From LLM to Conversational Agent: A Memory Enhanced Architecture
    with Fine-Tuning of Large Language Models")。'
- en: For assessing quality, we leverage human-centric annotation methods that closely
    replicate human evaluative standards. Meanwhile, the efficiency metrics are derived
    through a systematic statistical analysis, providing concrete, quantifiable insights.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估质量，我们利用以人为本的注释方法，这些方法 closely replicate 人类评估标准。同时，效率指标通过系统的统计分析得出，提供具体的、可量化的洞察。
- en: 4.5 Ablation Study
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 消融研究
- en: 'To demonstrate the effectiveness of the RAISE framework and fine-tuning method
    proposed in this paper, we conduct several ablation experiments in this section.
    The experiments are divided into two main aspects: (1) Comparative Analysis of
    Different Frameworks: We evaluate the performance of various frameworks under
    the same capability activation method, comparing their results using both the
    prompting and fine-tuning methods. The evaluation results are presented in Table
    [3](#S4.T3 "Table 3 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models").
    (2) Comparative Analysis of Different Capability Activation Methods: We compare
    the performance of the prompting method and fine-tuning method within the same
    framework, with the evaluation results presented in Table [4](#S4.T4 "Table 4
    ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ From LLM to Conversational Agent: A Memory
    Enhanced Architecture with Fine-Tuning of Large Language Models").'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示本论文提出的RAISE框架和微调方法的有效性，我们在本节中进行了一些消融实验。这些实验分为两个主要方面：(1) 不同框架的比较分析：我们评估了在相同能力激活方法下，各种框架的性能，并通过提示和微调方法比较其结果。评估结果见表
    [3](#S4.T3 "表 3 ‣ 4.5 消融研究 ‣ 4 实验 ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")。(2) 不同能力激活方法的比较分析：我们比较了在相同框架内，提示方法和微调方法的性能，评估结果见表
    [4](#S4.T4 "表 4 ‣ 4.5 消融研究 ‣ 4 实验 ‣ 从 LLM 到对话代理：具有大语言模型微调的记忆增强架构")。
- en: It’s important to note that the inference speed of the OpenAI GPT API is subject
    to platform and network variations, so this metric was omitted from our analysis.
    The inference environment for Qwen-14B-Chat was kept consistent, utilizing an
    A100 GPU with 80GB memory.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，OpenAI GPT API 的推理速度会受到平台和网络变化的影响，因此我们在分析中省略了这一指标。Qwen-14B-Chat 的推理环境保持一致，使用了
    80GB 内存的 A100 GPU。
- en: 'Table 3: The evaluation results of different frameworks'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同框架的评估结果
- en: '| Framework | Spec. | Fact. | Coher. | Nat. | Ov. Qual. Score | Plan Steps
    | Act. Steps | Inf. Speed(s) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Framework | Spec. | Fact. | Coher. | Nat. | Ov. Qual. Score | Plan Steps
    | Act. Steps | Inf. Speed(s) |'
- en: '| Prompting (GPT-4) |  |  |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (GPT-4) |  |  |  |'
- en: '| Act-Only | 1.89 | 1.66 | 1.95 | 1.87 | 7.37 | - | 1.29 | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Act-Only | 1.89 | 1.66 | 1.95 | 1.87 | 7.37 | - | 1.29 | - |'
- en: '| ReAct | 1.98 | 1.87 | 1.93 | 1.79 | 7.57 | 2 | 1 | - |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 1.98 | 1.87 | 1.93 | 1.79 | 7.57 | 2 | 1 | - |'
- en: '| ReAct+Scratchpad | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 | - |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+Scratchpad | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 | - |'
- en: '| ReAct+Examples | 1.96 | 1.87 | 1.96 | 1.93 | 7.72 | 2.1 | 1.1 | - |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+Examples | 1.96 | 1.87 | 1.96 | 1.93 | 7.72 | 2.1 | 1.1 | - |'
- en: '| RAISE | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 | - |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| RAISE | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 | - |'
- en: '| Fine-tuning (Qwen-14B-Chat) |  |  |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Fine-tuning (Qwen-14B-Chat) |  |  |  |'
- en: '| Act-Only | 1.66 | 1.71 | 1.82 | 1.92 | 7.11 | - | 0.66 | 1.935 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Act-Only | 1.66 | 1.71 | 1.82 | 1.92 | 7.11 | - | 0.66 | 1.935 |'
- en: '| ReAct | 1.88 | 1.79 | 1.93 | 1.92 | 7.52 | 1.88 | 0.88 | 4.315 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 1.88 | 1.79 | 1.93 | 1.92 | 7.52 | 1.88 | 0.88 | 4.315 |'
- en: '| ReAct+Scratchpad | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61 | 3.833
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+Scratchpad | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61 | 3.833
    |'
- en: '| ReAct+Examples | 1.93 | 1.82 | 1.96 | 1.95 | 7.66 | 1.33 | 0.33 | 3.327 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+Examples | 1.93 | 1.82 | 1.96 | 1.95 | 7.66 | 1.33 | 0.33 | 3.327 |'
- en: '| RAISE | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26 | 3.227 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| RAISE | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26 | 3.227 |'
- en: 'Table 4: The evaluation results for prompting vs. fine-tuning'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：提示与微调的评估结果
- en: '| Method | Spec. | Fact. | Coher. | Nat. | Ov. Qual. Score | Plan Steps | Act.
    Steps |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Method | Spec. | Fact. | Coher. | Nat. | Ov. Qual. Score | Plan Steps | Act.
    Steps |  |'
- en: '| RAISE |  |  |  |  |  |  |  |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| RAISE |  |  |  |  |  |  |  |  |'
- en: '| Prompting (GPT-3.5) | 1.65 | 1.72 | 1.66 | 1.67 | 6.7 | 2.13 | 5 |  |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (GPT-3.5) | 1.65 | 1.72 | 1.66 | 1.67 | 6.7 | 2.13 | 5 |  |'
- en: '| Prompting (Qwen-14B-Chat) | 1.69 | 1.66 | 1.68 | 1.65 | 6.68 | 2.06 | 1.2
    |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (Qwen-14B-Chat) | 1.69 | 1.66 | 1.68 | 1.65 | 6.68 | 2.06 | 1.2
    |  |'
- en: '| Prompting (GPT-4) | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 |  |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (GPT-4) | 1.95 | 1.92 | 1.97 | 1.85 | 7.69 | 1.79 | 0.8 |  |'
- en: '| Fine-tuning (Qwen-14B-Chat) | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26
    |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Fine-tuning (Qwen-14B-Chat) | 1.87 | 1.9 | 1.96 | 1.98 | 7.71 | 1.26 | 0.26
    |  |'
- en: '| ReAct+Scratchpad |  |  |  |  |  |  |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ReAct+Scratchpad |  |  |  |  |  |  |  |  |'
- en: '| Prompting (GPT-3.5) | 1.62 | 1.57 | 1.74 | 1.55 | 6.48 | 2.19 | 1.18 |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (GPT-3.5) | 1.62 | 1.57 | 1.74 | 1.55 | 6.48 | 2.19 | 1.18 |  |'
- en: '| Prompting (Qwen-14B-Chat) | 1.68 | 1.56 | 1.71 | 1.7 | 6.65 | 2.07 | 1.09
    |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (Qwen-14B-Chat) | 1.68 | 1.56 | 1.71 | 1.7 | 6.65 | 2.07 | 1.09
    |  |'
- en: '| Prompting (GPT-4) | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Prompting (GPT-4) | 1.98 | 1.88 | 1.99 | 1.65 | 7.5 | 1.97 | 0.96 |  |'
- en: '| Fine-tuning (Qwen-14B-Chat) | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61
    |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 微调 (Qwen-14B-Chat) | 1.91 | 1.81 | 1.93 | 1.96 | 7.61 | 1.6 | 0.61 |  |'
- en: 'Upon analyzing the experimental outcomes, the following key conclusions emerge
    with respect to the efficacy and efficiency of different agent frameworks and
    methods:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析实验结果后，以下是关于不同代理框架和方法的效果和效率的关键结论：
- en: Framework Performance Ranking within the Same LLM  The RAISE framework demonstrates
    superior performance, followed by ReAct+Examples, ReAct+Scratchpad, ReAct, and
    lastly, the Act-Only approach. This ranking indicates a clear gradient in effectiveness
    and efficiency, highlighting the incremental benefits of integrating additional
    elements like examples and scratchpads into the base ReAct model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 相同LLM中的框架性能排名  RAISE框架表现优异，其次是ReAct+Examples、ReAct+Scratchpad、ReAct，最后是Act-Only方法。这一排名表明了效果和效率的明确梯度，突显了将额外元素如Examples和Scratchpads集成到基础ReAct模型中的递增好处。
- en: Comparative Analysis of Capability Activation Methods within Identical Frameworks
     The fine-tuning approach outperforms the prompting method. This suggests that
    tailored training and customization of models to specific tasks or datasets result
    in more efficient and effective performance compared to using generalized prompt-based
    interactions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 相同框架内能力激活方法的比较分析  微调方法优于提示方法。这表明，相对于使用通用提示交互，针对特定任务或数据集的模型定制训练能带来更高效和有效的性能。
- en: In the following parts, we delve into detailed analyses of these findings, examining
    the implications and potential applications of each framework and methodology.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将*深入*分析这些发现，审视每种框架和方法的意义及潜在应用。
- en: 'Chain of Thought (CoT): A Catalyst for Enhanced Comprehension and Response
    Accurac  CoT significantly boosts AI’s ability to deeply comprehend and precisely
    respond to complex queries. Our experimental findings reaffirm the importance
    of CoT in complex tasks. For instance, in comparative experiments across different
    frameworks, agents employing the Act-Only method showed substantially lower performance
    compared to those incorporating CoT. These findings underscore the critical role
    of CoT in promoting AI models to deliver depth-oriented and logically coherent
    responses, particularly in scenarios requiring complex reasoning.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 思维链（CoT）：提升理解力和响应准确性的催化剂  CoT显著提高了AI对复杂查询的深度理解和精准回应的能力。我们的实验结果重申了CoT在复杂任务中的重要性。例如，在不同框架的比较实验中，使用Act-Only方法的代理表现明显低于那些采用CoT的方法。这些发现强调了CoT在促进AI模型提供深度导向和逻辑连贯回应中的关键作用，尤其是在需要复杂推理的场景中。
- en: 'RAISE Architecture: Dual Benefits of Efficiency from Scratchpad and Examples
     The RAISE architecture, by harmonizing Scratchpad and Example mechanisms, attains
    a dual advantage in processing efficiency and output quality. The application
    of Scratchpad significantly enhances the efficiency in handling complex tasks,
    while the utilization of Examples simultaneously bolsters the response’s naturalness,
    specificity, and efficiency. This dual advantage positions the RAISE architecture
    as a suitable choice for scenarios demanding rapid, accurate, and naturally interactive
    responses.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: RAISE架构：Scratchpad和Examples的双重效率好处  RAISE架构通过将Scratchpad和Example机制相结合，获得了处理效率和输出质量的双重优势。Scratchpad的应用显著提升了处理复杂任务的效率，而Examples的利用同时增强了回应的自然性、特异性和效率。这一双重优势使RAISE架构成为需要快速、准确和自然互动回应的场景中的合适选择。
- en: 'Fine-tuning: A Lever for Enhancing Agent Performance  Utilizing diverse, high-quality
    datasets for fine-tuning helps to better align AI models with human behavioral
    logic, potentially leading to notable improvements in specific application areas.
    This approach excels in specialized tasks, offering a high degree of professionalism
    and customization. For instance, in the RAISE framework under fine-tuning, the
    overall quality score reached 7.71, and inference efficiency was optimized. These
    results validate the effectiveness of fine-tuning in delivering precise, human-like
    and efficient outcomes, particularly suitable for scenarios requiring customized
    solutions, such as online real estate services.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 微调：提升智能体性能的杠杆 利用多样化的高质量数据集进行微调，有助于更好地将 AI 模型与人类行为逻辑对齐，从而在特定应用领域可能带来显著的改进。这种方法在专业化任务中表现卓越，提供了高水平的专业性和定制性。例如，在微调下的
    RAISE 框架中，总体质量评分达到了 7.71，推理效率得到了优化。这些结果验证了微调在提供精准、类人且高效结果方面的有效性，尤其适用于需要定制化解决方案的场景，如在线房地产服务。
- en: 'Fine-tuning: Enhancing Cost-Efficiency and Speed during Agent Inference  Although
    fine-tuning may require higher initial investments, its long-term benefits in
    operational efficiency and precision can offset these costs. In application, fine-tuned
    models often necessitate fewer computational resources, thereby reducing operational
    costs and accelerating response times. This cost-effectiveness, coupled with improved
    performance, makes fine-tuning a prudent choice for specific, resource-intensive
    tasks.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 微调：提升智能体推理过程中的成本效益和速度 尽管微调可能需要较高的初期投入，但其在运营效率和精度上的长期收益可以弥补这些成本。在应用中，微调后的模型通常需要更少的计算资源，从而降低了运营成本并加快了响应时间。这种成本效益，加上改进的性能，使得微调成为特定资源密集型任务的明智选择。
- en: 'Strategic Deployment of Language Agents: When to Choose Fine-tuning Over Prompting
     The decision to opt for fine-tuning or prompting hinges on the specific requirements
    of the application. Fine-tuning offers superior performance and efficiency in
    specialized domains but may involve higher initial costs and training needs. In
    contrast, prompting is more flexible in handling a wide range of queries but may
    slightly lag behind fine-tuned models in specificity and stability. Strategic
    decision-making in this context involves balancing these factors against the specific
    needs of the application, budget constraints, and performance expectations.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 语言智能体的战略部署：何时选择微调而非提示 选择微调还是提示取决于应用的具体需求。微调在专业领域提供更优的性能和效率，但可能涉及较高的初期成本和训练需求。相比之下，提示在处理各种查询时更为灵活，但在特异性和稳定性上可能略逊于微调模型。在这种情况下，战略决策涉及将这些因素与应用的具体需求、预算限制和性能期望进行平衡。
- en: 5 Related Work
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: The exploration and advancements in AI agents have captivated the AI research
    community for some time. Defined as artificial entities capable of perceiving
    their surroundings, making decisions, and executing actions [[Zalta et al., 1995](#bib.bibx36),
    [Barandiaran et al., 2009](#bib.bibx2)], AI agents represent a significant stride
    in artificial intelligence.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能智能体的探索和进展已引起 AI 研究界的广泛关注。人工智能智能体被定义为能够感知其环境、做出决策并执行行动的人工实体[[Zalta et al.,
    1995](#bib.bibx36), [Barandiaran et al., 2009](#bib.bibx2)]，代表了人工智能领域的重要进展。
- en: The advent of Large Language Models (LLMs) has been a pivotal development, often
    regarded as a step towards the realization of Artificial General Intelligence
    (AGI) [[Ouyang et al., 2022](#bib.bibx18), [Wei et al., 2022a](#bib.bibx29), [Bubeck
    et al., 2023](#bib.bibx3)]. In recent years, there has been an influx of studies
    proposing intricate LLM-based architectures for AI agents [[Weng, 2023](#bib.bibx31),
    [Wang et al., 2023a](#bib.bibx25), [Sumers et al., 2023](#bib.bibx24), [Xi et
    al., 2023](#bib.bibx32)]. These architectures are crucial in enabling agents to
    navigate complex dialogue scenarios and effectively apply their acquired knowledge.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现是一个关键的发展，通常被视为实现人工通用智能（AGI）的重要一步[[Ouyang et al., 2022](#bib.bibx18),
    [Wei et al., 2022a](#bib.bibx29), [Bubeck et al., 2023](#bib.bibx3)]。近年来，出现了大量研究提出复杂的基于
    LLM 的智能体架构[[Weng, 2023](#bib.bibx31), [Wang et al., 2023a](#bib.bibx25), [Sumers
    et al., 2023](#bib.bibx24), [Xi et al., 2023](#bib.bibx32)]。这些架构对于使智能体能够应对复杂的对话场景和有效应用其获得的知识至关重要。
- en: 'This body of work primarily revolves around two core aspects:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要围绕两个核心方面：
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Planning: Central to the functionality of dialogue agents is the concept of
    Chain-of-Thought (CoT) reasoning. This involves eliciting logical rationales via
    CoT prompts, as explored in [[Wei et al., 2022b](#bib.bibx30), [Wang et al., 2023c](#bib.bibx27),
    [Zhou et al., 2023](#bib.bibx39)]. However, integrating this reasoning effectively
    into dialogues remains challenging. The ReAct framework [[Yao et al., 2023](#bib.bibx35)]
    presents an approach that guides LLMs in reasoning before planning actions, addressing
    this issue.'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规划：对话代理功能的核心概念是Chain-of-Thought（CoT）推理。这涉及通过CoT提示引发逻辑推理，如[[Wei et al., 2022b](#bib.bibx30),
    [Wang et al., 2023c](#bib.bibx27), [Zhou et al., 2023](#bib.bibx39)]中探讨的。然而，将这种推理有效地融入对话中仍然具有挑战性。ReAct框架[[Yao
    et al., 2023](#bib.bibx35)]提出了一种方法，指导LLMs在规划行动之前进行推理，从而解决这一问题。
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tool Use: Another critical facet is the ability of LLMs to utilize external
    tools and resources. Studies such as [[Schick et al., 2023](#bib.bibx21), [Li
    et al., 2023b](#bib.bibx12), [Shen et al., 2023](#bib.bibx23)] have demonstrated
    the proficiency of LLMs in leveraging external tools and APIs. Moreover, the capacity
    to extract and integrate knowledge from external sources has been further exemplified
    by projects like WebGPT [[Nakano et al., 2022](#bib.bibx14)] and ExpeL [[Zhao
    et al., 2023](#bib.bibx38)].'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具使用：另一个关键方面是LLMs利用外部工具和资源的能力。研究如[[Schick et al., 2023](#bib.bibx21), [Li et
    al., 2023b](#bib.bibx12), [Shen et al., 2023](#bib.bibx23)]已经展示了LLMs在利用外部工具和API方面的能力。此外，像WebGPT
    [[Nakano et al., 2022](#bib.bibx14)]和ExpeL [[Zhao et al., 2023](#bib.bibx38)]这样的项目进一步展示了从外部来源提取和整合知识的能力。
- en: In addition to these areas, several works have focused on broader algorithmic
    frameworks for LLM-based agents.[[Xie et al., 2023](#bib.bibx33), [Pan et al.,
    2023](#bib.bibx19), [Sumers et al., 2023](#bib.bibx24), [Ruan et al., 2023](#bib.bibx20),
    [Kong et al., 2023](#bib.bibx10), [Li et al., 2023a](#bib.bibx11)] On the other
    hand, specific dialogue agents have also been a focal point.[[Shao et al., 2023](#bib.bibx22),
    [Wang et al., 2023d](#bib.bibx28), [Chen et al., 2023c](#bib.bibx7), [Chae et
    al., 2023](#bib.bibx4), [Hong et al., 2023](#bib.bibx9)]. The fine-tuning of LLMs
    within agents is another critical area, with works like [[Zeng et al., 2023](#bib.bibx37),
    [Chen et al., 2023a](#bib.bibx5)] exploring this aspect.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些领域，还有一些工作集中于基于LLM的代理的更广泛算法框架。[[Xie et al., 2023](#bib.bibx33), [Pan et al.,
    2023](#bib.bibx19), [Sumers et al., 2023](#bib.bibx24), [Ruan et al., 2023](#bib.bibx20),
    [Kong et al., 2023](#bib.bibx10), [Li et al., 2023a](#bib.bibx11)] 另一方面，特定对话代理也成为了焦点。[[Shao
    et al., 2023](#bib.bibx22), [Wang et al., 2023d](#bib.bibx28), [Chen et al., 2023c](#bib.bibx7),
    [Chae et al., 2023](#bib.bibx4), [Hong et al., 2023](#bib.bibx9)] LLMs在代理中的微调是另一个关键领域，如[[Zeng
    et al., 2023](#bib.bibx37), [Chen et al., 2023a](#bib.bibx5)]探讨了这一方面。
- en: These developments underscore the growing complexity and capabilities of LLM-based
    AI agents, highlighting both the challenges and the innovations shaping the field.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发展凸显了基于LLM的AI代理的复杂性和能力的增长，突显了塑造该领域的挑战和创新。
- en: 6 Conclusions and Future work
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: This study introduces RAISE, an advanced architecture enhancing Long Language
    Models (LLMs) like GPT-4 for conversational agents. Building on the ReAct framework,
    RAISE integrates a dual-component memory system, improving dialogue context retention
    and continuity. We also propose a fine-tuning method within RAISE, which enhances
    agent controllability and efficiency, particularly in real estate sales, though
    applicable in various domains.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究介绍了RAISE，一种先进的架构，旨在提升类似于GPT-4的长语言模型（LLMs）的对话代理能力。在ReAct框架的基础上，RAISE集成了一个双组件记忆系统，改善了对话上下文的保留和连贯性。我们还提出了一种RAISE内部的微调方法，这种方法提高了代理的可控性和效率，特别是在房地产销售领域，但也适用于各种领域。
- en: However, the study has limitations, including potential hallucination issues
    and challenges in handling complex logic problems, necessitating further research.
    Despite these limitations, RAISE presents a promising advancement in adaptable,
    context-aware conversational agents, offering a foundation for future developments
    in artificial intelligence.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，本研究存在一些局限性，包括潜在的幻觉问题和处理复杂逻辑问题的挑战，需要进一步研究。尽管存在这些局限性，RAISE在适应性和上下文感知的对话代理领域仍展示了有前途的进展，为未来人工智能的发展提供了基础。
- en: References
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Bai et al., 2023] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bai 等，2023] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu。2023。Qwen 技术报告。arXiv 预印本 arXiv:2309.16609。'
- en: '[Barandiaran et al., 2009] Xabier E Barandiaran, Ezequiel Di Paolo, and Marieke
    Rohde. 2009. Defining agency: Individuality, normativity, asymmetry, and spatio-temporality
    in action. Adaptive Behavior, 17(5):367–386.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Barandiaran 等，2009] Xabier E Barandiaran, Ezequiel Di Paolo, 和 Marieke Rohde。2009。Defining
    agency: Individuality, normativity, asymmetry, and spatio-temporality in action。Adaptive
    Behavior，17(5):367–386。'
- en: '[Bubeck et al., 2023] S. Bubeck, V. Chandrasekaran, R. Eldan, et al. 2023.
    Sparks of artificial general intelligence: Early experiments with gpt-4. CoRR.
    arXiv:2303.12712.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bubeck 等，2023] S. Bubeck, V. Chandrasekaran, R. Eldan, 等。2023。Sparks of artificial
    general intelligence: Early experiments with gpt-4。CoRR。arXiv:2303.12712。'
- en: '[Chae et al., 2023] Hyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon,
    Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, and Jinyoung Yeo. 2023. Dialogue
    chain-of-thought distillation for commonsense-aware conversational agents. arXiv
    preprint arXiv:2310.09343.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chae 等，2023] Hyungjoo Chae, Yongho Song, Kai Tzu-iunn Ong, Taeyoon Kwon, Minjin
    Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, 和 Jinyoung Yeo。2023。Dialogue chain-of-thought
    distillation for commonsense-aware conversational agents。arXiv 预印本 arXiv:2310.09343。'
- en: '[Chen et al., 2023a] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,
    Karthik Narasimhan, and Shunyu Yao. 2023a. Fireact: Toward language agent fine-tuning.
    arXiv preprint arXiv:2310.05915.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等，2023a] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, 和 Shunyu Yao。2023a。Fireact: Toward language agent fine-tuning。arXiv
    预印本 arXiv:2310.05915。'
- en: '[Chen et al., 2023b] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning
    Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, and Feng
    Zhao. 2023b. T-eval: Evaluating the tool utilization capability step by step.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等，2023b] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu,
    Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, 和 Feng Zhao。2023b。T-eval:
    Evaluating the tool utilization capability step by step。'
- en: '[Chen et al., 2023c] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin
    Zhao, and Ji-Rong Wen. 2023c. Chatcot: Tool-augmented chain-of-thought reasoning
    on$\backslash$chat-based large language models. arXiv preprint arXiv:2305.14323.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等，2023c] Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin
    Zhao, 和 Ji-Rong Wen。2023c。Chatcot: Tool-augmented chain-of-thought reasoning on$\backslash$chat-based
    large language models。arXiv 预印本 arXiv:2305.14323。'
- en: '[Crispino et al., 2023] Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn
    Song, and Chenguang Wang. 2023. Agent instructs large language models to be general
    zero-shot reasoners. ArXiv, abs/2310.03710.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Crispino 等，2023] Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song,
    和 Chenguang Wang。2023。Agent instructs large language models to be general zero-shot
    reasoners。ArXiv，abs/2310.03710。'
- en: '[Hong et al., 2023] Joey Hong, Sergey Levine, and Anca Dragan. 2023. Zero-shot
    goal-directed dialogue via rl on imagined conversations. arXiv preprint arXiv:2311.05584.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hong 等，2023] Joey Hong, Sergey Levine, 和 Anca Dragan。2023。Zero-shot goal-directed
    dialogue via rl on imagined conversations。arXiv 预印本 arXiv:2311.05584。'
- en: '[Kong et al., 2023] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng
    Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, et al. 2023. Tptu-v2:
    Boosting task planning and tool usage of large language model-based agents in
    real-world systems. arXiv preprint arXiv:2311.11315.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kong 等，2023] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao,
    Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, 等。2023。Tptu-v2: Boosting
    task planning and tool usage of large language model-based agents in real-world
    systems。arXiv 预印本 arXiv:2311.11315。'
- en: '[Li et al., 2023a] Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang
    Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. 2023a.
    Modelscope-agent: Building your customizable agent system with open-source large
    language models. arXiv preprint arXiv:2309.00986.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等，2023a] Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu,
    Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, 等。2023a。Modelscope-agent:
    Building your customizable agent system with open-source large language models。arXiv
    预印本 arXiv:2309.00986。'
- en: '[Li et al., 2023b] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li,
    Fei Huang, and Yongbin Li. 2023b. Apibank: A benchmark for tool-augmented llms.
    arXiv preprint.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[李等, 2023b] 李名浩、宋飞凡、余博文、余海洋、李周俊、黄飞、李永斌。2023b。《Apibank: 用于工具增强大语言模型的基准》。arXiv
    预印本。'
- en: '[Liu et al., 2023] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench:
    Evaluating llms as agents. arXiv preprint arXiv:2308.03688.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[刘等, 2023] 刘晓、余浩、张汉臣、徐一凡、雷轩宇、赖汉宇、谷雨、丁航良、门凯文、杨克娟，等。2023。《Agentbench: 评估大语言模型作为代理的表现》。arXiv
    预印本 arXiv:2308.03688。'
- en: '[Nakano et al., 2022] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. Webgpt: Browser-assisted
    question-answering with human feedback. arXiv preprint.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[中野等, 2022] 中野零一郎、雅各布·希尔顿、苏奇尔·巴拉吉、杰夫·吴、龙欧阳、克里斯蒂娜·金、克里斯托弗·赫斯、香农·简、维尼特·科萨拉朱、威廉·桑德斯、徐江、卡尔·科比、泰娜·埃隆杜、格雷琴·克鲁格、凯文·巴顿、马修·奈特、本杰明·切斯、约翰·舒尔曼。2022。《Webgpt:
    基于浏览器的问答系统与人类反馈》。arXiv 预印本。'
- en: '[Nori et al., 2023] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard
    Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu,
    Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin,
    Naoto Usuyama, Chris White, and Eric Horvitz. 2023. Can generalist foundation
    models outcompete special-purpose tuning? case study in medicine. ArXiv, abs/2311.16452.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[诺里等, 2023] 哈尔莎·诺里、李银达、张胜、迪恩·卡里甘、理查德·埃德加、尼科洛·富西、尼古拉斯·金、乔纳森·拉尔森、李元智、刘伟生、罗人乾、斯科特·迈耶·麦金尼、罗伯特·奥萨祖瓦·内斯、胡芳·彭、秦涛、宇野修山、克里斯·怀特、埃里克·霍维茨。2023。《通用基础模型能否超越专用调整？医学中的案例研究》。ArXiv,
    abs/2311.16452。'
- en: '[OpenAI, 2023a] OpenAI. 2023a. Chatgpt: Optimizing language models for dialogue.
    Blog post.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI, 2023a] OpenAI。2023a。《Chatgpt: 优化对话的语言模型》。博客文章。'
- en: '[OpenAI, 2023b] OpenAI. 2023b. Gpt-4 technical report. Blog post.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[OpenAI, 2023b] OpenAI。2023b。《GPT-4 技术报告》。博客文章。'
- en: '[Ouyang et al., 2022] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[欧阳等, 2022] 欧阳龙、吴杰夫、徐江、迪奥戈·阿尔梅达、卡罗尔·L·温赖特、帕梅拉·米什金、张冲、桑迪尼·阿加瓦尔、卡塔里娜·斯拉马、亚历克斯·雷、约翰·舒尔曼、雅各布·希尔顿、弗雷泽·凯尔顿、卢克·米勒、玛迪·西门斯、阿曼达·阿斯克尔、彼得·韦林德、保罗·克里斯蒂亚诺、简·莱克和瑞安·洛。2022。《训练语言模型以跟随指令并结合人类反馈》。'
- en: '[Pan et al., 2023] Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu,
    Ming Liu, Zhongyuan Wang, and Bing Qin. 2023. Kwaiagents: Generalized information-seeking
    agent system with large language models. arXiv preprint arXiv:2312.04889.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[潘等, 2023] 潘浩杰、翟泽鹏、袁浩、吕耀佳、傅瑞基、刘鸣、王忠远、秦冰。2023。《Kwaiagents: 基于大语言模型的广义信息搜索代理系统》。arXiv
    预印本 arXiv:2312.04889。'
- en: '[Ruan et al., 2023] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng
    Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. 2023. Tptu:
    Task planning and tool usage of large language model-based ai agents. arXiv preprint
    arXiv:2308.03427.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[阮等, 2023] 阮景清、陈一红、张斌、徐志伟、包天鹏、杜国庆、石伟、毛航宇、曾兴宇、赵锐。2023。《Tptu: 基于大语言模型的人工智能代理的任务规划与工具使用》。arXiv
    预印本 arXiv:2308.03427。'
- en: '[Schick et al., 2023] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
    Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[施克等, 2023] 蒂莫·施克、简·德维维迪-余、罗伯托·德西、罗伯塔·雷莱亚努、玛丽亚·洛梅利、卢克·泽特尔莫耶尔、尼科拉·坎切达、托马斯·斯查洛姆。2023。《Toolformer:
    语言模型能够自我学习使用工具》。arXiv 预印本。'
- en: '[Shao et al., 2023] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023.
    Character-llm: A trainable agent for role-playing. arXiv preprint arXiv:2310.10158.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[邵等, 2023] 邵云帆、李林阳、戴俊琪、邱西鹏。2023。《Character-llm: 一种可训练的角色扮演代理》。arXiv 预印本 arXiv:2310.10158。'
- en: '[Shen et al., 2023] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface. arXiv preprint arXiv:2303.17580.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[申等, 2023] 申永亮、宋开涛、谭旭、李冬生、陆伟明、庄越婷。2023。《Hugginggpt: 使用ChatGPT及其在Huggingface中的朋友解决AI任务》。arXiv
    预印本 arXiv:2303.17580。'
- en: '[Sumers et al., 2023] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and
    Thomas L Griffiths. 2023. Cognitive architectures for language agents. arXiv preprint
    arXiv:2309.02427.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sumers et al., 2023] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, 和 Thomas
    L Griffiths. 2023. 语言代理的认知架构. arXiv 预印本 arXiv:2309.02427.'
- en: '[Wang et al., 2023a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,
    Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023a. A
    survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023a] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang,
    Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2023a. 基于大型语言模型的自主代理调查.
    arXiv 预印本 arXiv:2308.11432.'
- en: '[Wang et al., 2023b] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao
    ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin
    Zhao, Zhewei Wei, and Ji rong Wen. 2023b. A survey on large language model based
    autonomous agents. ArXiv, abs/2308.11432.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023b] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao
    ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne
    Xin Zhao, Zhewei Wei, 和 Ji rong Wen. 2023b. 基于大型语言模型的自主代理调查. ArXiv, abs/2308.11432.'
- en: '[Wang et al., 2023c] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-consistency
    improves chain of thought reasoning in language models. In Proceedings of ICLR.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023c] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed
    H. Chi, Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023c. 自一致性提高语言模型中的思维链推理.
    在 ICLR 会议录中.'
- en: '[Wang et al., 2023d] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng
    Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang,
    et al. 2023d. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities
    of large language models. arXiv preprint arXiv:2310.00746.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang et al., 2023d] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng
    Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang,
    等. 2023d. Rolellm: 基准测试、引发和增强大型语言模型的角色扮演能力. arXiv 预印本 arXiv:2310.00746.'
- en: '[Wei et al., 2022a] J. Wei, Y. Tay, R. Bommasani, et al. 2022a. Emergent abilities
    of large language models. Trans. Mach. Learn. Res.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wei et al., 2022a] J. Wei, Y. Tay, R. Bommasani, 等. 2022a. 大型语言模型的涌现能力. 机器学习研究转化.'
- en: '[Wei et al., 2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
    Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022b. Chain of thought
    prompting elicits reasoning in large language models. In Proceedings of NeurIPS.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wei et al., 2022b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma,
    Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, 和 Denny Zhou. 2022b. 思维链提示引发大型语言模型的推理.
    在 NeurIPS 会议录中.'
- en: '[Weng, 2023] L. Weng. 2023. Llm-powered autonomous agents.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Weng, 2023] L. Weng. 2023. 由大型语言模型驱动的自主代理.'
- en: '[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xi et al., 2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, 等. 2023. 基于大型语言模型的代理的崛起与潜力:
    一项调查. arXiv 预印本 arXiv:2309.07864.'
- en: '[Xie et al., 2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan
    Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. 2023. Openagents:
    An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xie et al., 2023] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan
    Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, 等. 2023. Openagents:
    一个用于真实环境中语言代理的开放平台. arXiv 预印本 arXiv:2310.10634.'
- en: '[Yao et al., 2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. arXiv preprint arXiv:2210.03629.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao et al., 2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022. React: 协同推理与行动在语言模型中的应用. arXiv 预印本 arXiv:2210.03629.'
- en: '[Yao et al., 2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting
    in language models. arXiv preprint.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao et al., 2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2023. React: 协同推理与行动在语言模型中的应用. arXiv 预印本.'
- en: '[Zalta et al., 1995] Edward N Zalta, Uri Nodelman, Colin Allen, and John Perry.
    1995. Stanford encyclopedia of philosophy.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zalta et al., 1995] Edward N Zalta, Uri Nodelman, Colin Allen, 和 John Perry.
    1995. 斯坦福哲学百科全书.'
- en: '[Zeng et al., 2023] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu,
    Yuxiao Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities
    for llms. arXiv preprint arXiv:2310.12823.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zeng et al., 2023] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu,
    Yuxiao Dong, 和 Jie Tang. 2023. Agenttuning: 为大型语言模型启用通用代理能力. arXiv 预印本 arXiv:2310.12823.'
- en: '[Zhao et al., 2023] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin
    Liu, and Gao Huang. 2023. Expel: Llm agents are experiential learners. arXiv preprint
    arXiv:2308.10144.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zhao et al., 2023] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin
    Liu, and Gao Huang. 2023. Expel: Llm agents are experiential learners. arXiv预印本arXiv:2308.10144.'
- en: '[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le,
    and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large
    language models. In Proceedings of ICLR.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le,
    and Ed H. Chi. 2023. Least-to-most prompting enables complex reasoning in large
    language models. 在ICLR会议录中。'
- en: Appendix A Appendix
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: 'The complete prompts used by the five agent frameworks during inference are
    shown in Table [5](#A1.T5 "Table 5 ‣ Appendix A Appendix ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models")
    to Table [9](#A1.T9 "Table 9 ‣ Appendix A Appendix ‣ From LLM to Conversational
    Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '五个代理框架在推理过程中使用的完整提示见表[5](#A1.T5 "表5 ‣ 附录A 附录 ‣ 从LLM到对话代理: 基于大语言模型的记忆增强架构")
    到表[9](#A1.T9 "表9 ‣ 附录A 附录 ‣ 从LLM到对话代理: 基于大语言模型的记忆增强架构")。'
- en: 'Table 5: The prompt used for RAISE'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '表5: 用于RAISE的提示'
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 你是一名熟练的房地产顾问，服务于贝壳找房，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的住宅。公司设想成为服务于3亿家庭的优质住宅平台，其使命是成为一个体面服务提供者，贡献于更好的生活体验。在在线聊天互动中，你的目标是回答客户的问题，吸引他们购买房产，并鼓励他们加你微信或面谈。
    |'
- en: '| You need to respond to client queries using the steps of Scratchpad, Examples,
    Thought, Action, Observation, Finish, based on historical conversations and the
    client’s questions. Avoid repeating actions that have been used before. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用Scratchpad、Examples、Thought、Action、Observation、Finish的步骤来回应客户查询。避免重复之前已使用的行动。
    |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中的每个工具定义如下: |'
- en: '| {tool descriptions} |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述} |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个示例: (在微调方法中省略。) |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史记录: 用户: “houseCode”: “1021111”, “houseName”: “华润24城大厦，采光和景观好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: 房子是哪一年建造的？ |'
- en: '| Scratchpad: [Real Estate Consultant Information]: Name: Zhang Hua, WeChat:
    123456, Rank: Intermediate Consultant, Performance: 25 deals closed. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Scratchpad: [房地产顾问信息]: 姓名: 张华，微信: 123456，职级: 中级顾问，业绩: 完成25笔交易。 |'
- en: '| Examples: User: In which year was this house constructed? Agent: This house
    was constructed in 2020, and it’s still relatively new. When would you like to
    come and see it? |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 示例: 用户: 这栋房子是哪一年建造的？代理: 这栋房子建于2020年，仍然相对较新。你想什么时候来看房？ |'
- en: '| Thought: The year of construction of the house in the Examples matches the
    customer’s question, so I can directly use the response method from the Examples
    to answer the customer’s question. Action: Finish [This house was built in 2020,
    making it a relatively new property. When are you available to view the house?
    I can help you schedule an appointment.] |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 思考: 示例中的房屋建造年份与客户的问题匹配，因此我可以直接使用示例中的回应方法来回答客户的问题。行动: 完成[这栋房子建于2020年，是相对较新的房产。你什么时候有空来看房？我可以帮你安排预约。]
    |'
- en: '| Let’s get started: |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 开始吧: |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史记录: {Conversation History} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: {Current Query} |'
- en: 'Table 6: The prompt used for Act-Only'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表6: 用于Act-Only的提示'
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位熟练的房地产顾问，服务于贝壳找房，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的家园。它设想成为一个优质的住宅平台，服务于3亿家庭，其使命是成为一个有尊严的服务提供者，为更好的生活体验做出贡献。在在线聊天互动中，你的目标是回答客户的问题，吸引他们购买房产，并鼓励他们加你微信或面谈。
    |'
- en: '| You need to respond to client queries using the steps of Action, Observation,
    Finish, based on historical conversations and the client’s questions. Avoid repeating
    actions that have been used before. |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题使用“行动、观察、完成”的步骤来回应客户查询。避免重复使用已经使用过的动作。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中每个工具的定义如下： |'
- en: '| (1) Real Estate Consultant Information $[agent\_ucid]$: Retrieves the consultant’s
    name, contact details, WeChat ID, ranking, performance metrics, and more. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| (1) 房产顾问信息 $[agent\_ucid]$: 获取顾问的姓名、联系方式、微信ID、排名、业绩指标等信息。 |'
- en: '| (2) House Information $[house\_id]$: Offers essential details about a property,
    including its size, price, floor level, school district presence, and renovation
    status. |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| (2) 房产信息 $[house\_id]$: 提供关于房产的基本信息，包括其大小、价格、楼层、学区情况和装修状态。 |'
- en: '| (3) Community Information $[resblock\_id]$: Provides insights into the community,
    covering aspects like green spaces, property management, building specifications,
    proximity to subway stations, schools, and medical facilities. |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| (3) 小区信息 $[resblock\_id]$: 提供关于小区的见解，包括绿地、物业管理、建筑规格、靠近地铁站、学校和医疗设施等方面。 |'
- en: '| (4) House Layout Analysis $[frame\_id]$: Analyzes the strengths and weaknesses
    of a property’s layout. |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (4) 房屋布局分析 $[frame\_id]$: 分析房产布局的优缺点。 |'
- en: '| (5) House Price Changes $[house\_id]$: Tracks price fluctuations for a specific
    property. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| (5) 房价变化 $[house\_id]$: 跟踪特定房产的价格波动。 |'
- en: '| (6) Community Price Changes $[resblock\_id]$: Reports on average price trends
    within a particular community. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| (6) 小区价格变化 $[resblock\_id]$: 报告特定小区的平均价格趋势。 |'
- en: '| (7) unity Transactions $[resblock\_id]$: Accesses recent transaction data
    from the same community. |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| (7) 统一交易 $[resblock\_id]$: 访问同一小区的最新交易数据。 |'
- en: '| (8) Tax Policy $[city\_id]$: Updates on the latest tax regulations and implications.
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| (8) 税收政策 $[city\_id]$: 更新最新的税收法规及其影响。 |'
- en: '| (9) Loan Policy $[city\_id]$: Delivers current information on loan policies.
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| (9) 贷款政策 $[city\_id]$: 提供当前的贷款政策信息。 |'
- en: '| (10)Market Analysis $[city\_id]$: Provides up-to-date real estate market
    insights. |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| (10) 市场分析 $[city\_id]$: 提供最新的房地产市场见解。 |'
- en: '| (11) Recommend Listings $[Conversation\ History]$: Suggests property listings
    to customers based on their conversation history and inferred needs, including
    rationale for each recommendation. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| (11) 推荐房源 $[Conversation\ History]$: 根据客户的对话历史和推测的需求向客户推荐房源，包括每个推荐的理由。 |'
- en: '| (12) Value Report $[house\_id]$: Generates a comprehensive value report card
    for a property, aimed at engaging customers and encouraging them to share their
    contact details. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| (12) 价值报告 $[house\_id]$: 为房产生成一份全面的价值报告，旨在吸引客户并鼓励他们分享联系信息。 |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 这里是一个例子：（在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: 用户: “houseCode”: “1021111”, “houseName”: “华润24城大厦，采光好，视野佳，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：房子是哪一年建造的？ |'
- en: '| Action: House Information [house_id: 1021111] |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 行动: 房产信息 [house_id: 1021111] |'
- en: '| Observation: House ID: 1021111; House Name: Huarun 24 City Mansion, good
    lighting and view, quiet; House Status: Active; Type of Property: Resale; Number
    of Bedrooms: 2; Number of Halls: 2; Number of Bathrooms: 2; Area: 88 square meters;
    Orientation: South-North; Floor: 5; Total Floors: 9; Elevator: Yes; Construction
    Year: 2020; Qualifies for ”Two Years”: No; Qualifies for ”Five Years”: No; House
    Price: 1.94 million yuan |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 观察: 房屋ID: 1021111; 房屋名称: 华润24城大厦，采光和视野良好，安静; 房屋状态: 活跃; 物业类型: 二手房; 卧室数量: 2;
    客厅数量: 2; 浴室数量: 2; 面积: 88平方米; 朝向: 南北; 楼层: 5; 总楼层: 9; 电梯: 有; 建造年份: 2020; 满足“两年”条件:
    否; 满足“五年”条件: 否; 房屋价格: 194万元 |'
- en: '| Action: Finish $[$ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 操作: 完成 $[$ |'
- en: '| Let’s get started: |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 开始吧: |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: {对话历史} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: {当前查询} |'
- en: 'Table 7: The prompt used for ReAct'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '表7: 用于ReAct的提示'
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个熟练的房地产顾问，服务于贝壳找房公司，该公司提供房地产中介服务。公司的价值在于帮助买家找到理想的家园。它设想成为服务于3亿家庭的优质住宅平台，其使命是成为一个体面的服务提供者，致力于改善生活体验。你的目标是在在线聊天互动中，回答客户的问题，吸引他们购买房产，并鼓励他们添加你的微信或亲自见面。
    |'
- en: '| You need to respond to client queries using the steps of Thought, Action,
    Observation, Finish, based on historical conversations and the client’s questions.
    Avoid repeating actions that have been used before. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用思考、操作、观察、完成的步骤来回应客户的查询。避免重复使用已经使用过的操作。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中每个工具的定义如下: |'
- en: '| {tool descriptions}over |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述}结束 |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个例子: （在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: 用户: “houseCode”: “1021111”, “houseName”: “华润24城大厦，采光和视野良好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: 房子是哪一年建造的? |'
- en: '| Thought: The client wants to know the year of construction of the house.
    I need to look up the property information to find this out. Action: House Information
    [house_id: 1021111] |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 思考: 客户想知道房子的建造年份。我需要查找房产信息来找出这个信息。 操作: 房产信息 [house_id: 1021111] |'
- en: '| Observation: House ID: 1021111; House Name: Huarun 24 City Mansion, good
    lighting and view, quiet; House Status: Active; Type of Property: Resale; Number
    of Bedrooms: 2; Number of Halls: 2; Number of Bathrooms: 2; Area: 88 square meters;
    Orientation: South-North; Floor: 5; Total Floors: 9; Elevator: Yes; Construction
    Year: 2020; Qualifies for ”Two Years”: No; Qualifies for ”Five Years”: No; House
    Price: 1.94 million yuan Thought: Based on the information I’ve gathered, this
    house was built in 2020, which makes it relatively new. I should ask the client
    when they are available to view the house, and I can help schedule a viewing.
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 观察: 房屋ID: 1021111; 房屋名称: 华润24城大厦，采光和视野良好，安静; 房屋状态: 活跃; 物业类型: 二手房; 卧室数量: 2;
    客厅数量: 2; 浴室数量: 2; 面积: 88平方米; 朝向: 南北; 楼层: 5; 总楼层: 9; 电梯: 有; 建造年份: 2020; 满足“两年”条件:
    否; 满足“五年”条件: 否; 房屋价格: 194万元 思考: 根据我收集到的信息，这栋房子建于2020年，相对较新。我应该询问客户何时有空看房，并可以帮助安排看房。
    |'
- en: '| Action: Finish $[$ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 操作: 完成 $[$ |'
- en: '| Let’s get started: |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 开始吧: |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: {对话历史} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: {当前查询} |'
- en: 'Table 8: The prompt used for ReAct+Scratchpad'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 用于ReAct+Scratchpad的提示'
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位熟练的房地产顾问，服务于贝壳找房，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的家园。它的愿景是成为一个优质的住宅平台，服务于3亿家庭，其使命是成为一个有尊严的服务提供者，为更好的生活体验做出贡献。你的目标是在在线聊天互动中，回答客户的问题，吸引他们购买房产，并鼓励他们添加你的微信或面谈。
    |'
- en: '| You need to respond to client queries using the steps of Scratchpad, Thought,
    Action, Finish, based on historical conversations and the client’s questions.
    Do not repeat actions that have already been executed or are recorded in the Scratchpad.
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用草稿本、思考、操作、完成的步骤来回应客户的查询。不要重复已执行的操作或在草稿本中记录的操作。 |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中的每个工具定义如下: |'
- en: '| {tool descriptions} |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述} |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 这是一个示例: (在微调方法中省略。) |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: 用户: “houseCode”: “1021111”, “houseName”: “华润24城大厦，采光和视野好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: 房子的建造年份是多少？ |'
- en: '| Scratchpad:[Real Estate Consultant Information]: Name: Zhang Hua, WeChat:
    123456, Rank: Intermediate Consultant, Performance: 25 deals closed. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 草稿本:[房地产顾问信息]: 姓名: 张华, 微信: 123456, 职级: 中级顾问, 业绩: 完成了25笔交易。 |'
- en: '| Thought: The customer wants to know the year of construction of the house,
    but there is no relevant information in the Scratchpad. I need to query the house
    information to obtain this. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 思考: 客户想知道房子的建造年份，但在草稿本中没有相关信息。我需要查询房屋信息以获得此信息。 |'
- en: '| Action: House Information [house_id: 1021111] |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 操作: 房屋信息 [house_id: 1021111] |'
- en: '| Scratchpad: [Real Estate Consultant Information]: Name: Zhang Hua, WeChat:
    123456, Rank: Intermediate Consultant, Performance: 25 deals closed. [House Information]:
    House ID: 1021111; House Name: Huarun 24 City Mansion, good lighting and view,
    quiet; House Status: Active; Type of Property: Resale; Number of Bedrooms: 2;
    Number of Halls: 2; Number of Bathrooms: 2; Area: 88 square meters; Orientation:
    South-North; Floor: 5; Total Floors: 9; Elevator: Yes; Construction Year: 2020;
    Qualifies for ”Two Years”: No; Qualifies for ”Five Years”: No; House Price: 1.94
    million yuan. |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 草稿本: [房地产顾问信息]: 姓名: 张华, 微信: 123456, 职级: 中级顾问, 业绩: 完成了25笔交易。 [房屋信息]: 房屋ID:
    1021111; 房屋名称: 华润24城大厦，采光和视野好，安静; 房屋状态: 在售; 房屋类型: 二手房; 卧室数量: 2; 客厅数量: 2; 卫生间数量:
    2; 面积: 88平方米; 朝向: 南北; 楼层: 5; 总楼层: 9; 电梯: 有; 建造年份: 2020年; 是否符合“二年”: 否; 是否符合“五年”:
    否; 房屋价格: 194万元。 |'
- en: '| Thought: Based on the house information in the Scratchpad, I can tell the
    customer that this house was built in 2020, making it relatively new. Now, I should
    ask when the customer is available to view the house, and I can help schedule
    an appointment. |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 思考: 根据草稿本中的房屋信息，我可以告诉客户这套房子建于2020年，相对较新。现在，我应该询问客户何时方便看房，并可以帮助安排预约。 |'
- en: '| Action: Finish $[$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 操作: 完成 $[$ |'
- en: '| Let’s get started: |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 让我们开始吧: |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史: {对话历史} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询: {当前查询} |'
- en: 'Table 9: The prompt used for ReAct+Examples'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '表格9: ReAct+Examples 使用的提示'
- en: '| You are a proficient real estate consultant working for Beike Zhaofang, a
    company that provides real estate brokerage services. The company’s value lies
    in assisting buyers to find their ideal homes. It envisions becoming a quality
    residential platform serving 300 million families, and its mission is to be a
    dignified service provider, contributing to a better living experience. Your objective,
    during online chat interactions, is to answer clients’ questions, attract them
    to purchase properties, and encourage them to add you on WeChat or meet in person.
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位熟练的房地产顾问，工作于贝壳找房，这是一家提供房地产经纪服务的公司。公司的价值在于帮助买家找到理想的家园。公司设想成为一个服务3亿家庭的优质住宅平台，使命是成为一个值得尊敬的服务提供商，为更好的居住体验做出贡献。你在在线聊天互动中的目标是回答客户的问题，吸引他们购买房产，并鼓励他们加你微信或亲自见面。
    |'
- en: '| You need to respond to client queries using the steps of Examples, Thought,
    Action, Observation, Finish, based on historical conversations and the client’s
    questions. Avoid repeating actions that have been used before. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 你需要根据历史对话和客户的问题，使用示例中的步骤：Examples、Thought、Action、Observation、Finish，来回应客户的查询。避免重复之前使用过的行动。
    |'
- en: '| Each tool in the toolset is defined as follows: |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 工具集中中的每个工具定义如下： |'
- en: '| {tool descriptions} |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| {工具描述} |'
- en: '| Here is an example: (Omit in the fine-tuning method.) |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 这里是一个示例：（在微调方法中省略。） |'
- en: '| Conversation History: User: “houseCode”: “1021111”, “houseName”: “Huarun
    24 City Mansion, good lighting and view, quiet” |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：用户：“houseCode”： “1021111”，“houseName”： “华润24城大厦，采光和视野好，安静” |'
- en: '| Current Query: What year was the house constructed? |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：这座房子是什么年份建造的？ |'
- en: '| Examples: User: In which year was this house constructed? Agent: This house
    was constructed in 2020, and it’s still relatively new. When would you like to
    come and see it? |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 示例：用户：这座房子是什么年份建造的？ 代理：这座房子建于2020年，仍然相对较新。您什么时候想来看房？ |'
- en: '| Thought: The construction year of the house in the Examples matches the customer’s
    question, so I can directly use the response method from the Example to answer
    the customer’s question. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 思考：示例中的房子建造年份与客户的问题匹配，因此我可以直接使用示例中的回应方法来回答客户的问题。 |'
- en: '| Action: Finish [This house was built in 2020, so it’s considered relatively
    new. When are you available to view the house? I can help you make an appointment.]
    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 行动：完成 [这座房子建于2020年，因此被认为是相对较新的。您什么时候有空来看房？我可以帮您预约。] |'
- en: '| Let’s get started: |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 让我们开始吧： |'
- en: '| Conversation History: {Conversation History} |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 对话历史：{Conversation History} |'
- en: '| Current Query: {Current Query} |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 当前查询：{Current Query} |'
