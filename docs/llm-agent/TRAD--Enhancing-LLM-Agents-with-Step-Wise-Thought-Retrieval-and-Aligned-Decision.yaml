- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:50:35'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:35
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'TRAD: 通过逐步思维检索和对齐决策增强LLM代理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.06221](https://ar5iv.labs.arxiv.org/html/2403.06221)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.06221](https://ar5iv.labs.arxiv.org/html/2403.06221)
- en: Ruiwen Zhou [skyriver@sjtu.edu.cn](mailto:skyriver@sjtu.edu.cn) Shanghai Jiao
    Tong UniversityShanghaiChina ,  Yingxuan Yang [zoeyyx@sjtu.edu.cn](mailto:zoeyyx@sjtu.edu.cn)
    Shanghai Jiao Tong UniversityShanghaiChina ,  Muning Wen [muningwen@sjtu.edu.cn](mailto:muningwen@sjtu.edu.cn)
    Shanghai Jiao Tong UniversityShanghaiChina ,  Ying Wen [ying.wen@sjtu.edu.cn](mailto:ying.wen@sjtu.edu.cn)
    Shanghai Jiao Tong UniversityShanghaiChina ,  Wenhao Wang [wangwenhao-009@cpic.com.cn](mailto:wangwenhao-009@cpic.com.cn%0A)
    China Pacific InsuranceShanghaiChina ,  Chunling Xi [xichunling@cpic.com.cn](mailto:xichunling@cpic.com.cn)
    China Pacific InsuranceShanghaiChina ,  Guoqiang Xu [xuguoqiang-009@cpic.com.cn](mailto:xuguoqiang-009@cpic.com.cn)
    China Pacific InsuranceShanghaiChina ,  Yong Yu [yyu@apex.sjtu.edu.cn](mailto:yyu@apex.sjtu.edu.cn)
    Shanghai Jiao Tong UniversityShanghaiChina  and  Weinan Zhang [wnzhang@sjtu.edu.cn](mailto:wnzhang@sjtu.edu.cn)
    Shanghai Jiao Tong UniversityShanghaiChina
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruiwen Zhou [skyriver@sjtu.edu.cn](mailto:skyriver@sjtu.edu.cn) 上海交通大学上海中国，Yingxuan
    Yang [zoeyyx@sjtu.edu.cn](mailto:zoeyyx@sjtu.edu.cn) 上海交通大学上海中国，Muning Wen [muningwen@sjtu.edu.cn](mailto:muningwen@sjtu.edu.cn)
    上海交通大学上海中国，Ying Wen [ying.wen@sjtu.edu.cn](mailto:ying.wen@sjtu.edu.cn) 上海交通大学上海中国，Wenhao
    Wang [wangwenhao-009@cpic.com.cn](mailto:wangwenhao-009@cpic.com.cn%0A) 中国太平洋保险上海中国，Chunling
    Xi [xichunling@cpic.com.cn](mailto:xichunling@cpic.com.cn) 中国太平洋保险上海中国，Guoqiang
    Xu [xuguoqiang-009@cpic.com.cn](mailto:xuguoqiang-009@cpic.com.cn) 中国太平洋保险上海中国，Yong
    Yu [yyu@apex.sjtu.edu.cn](mailto:yyu@apex.sjtu.edu.cn) 上海交通大学上海中国，以及Weinan Zhang
    [wnzhang@sjtu.edu.cn](mailto:wnzhang@sjtu.edu.cn) 上海交通大学上海中国
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Numerous large language model (LLM) agents have been built for different tasks
    like web navigation and online shopping due to LLM’s wide knowledge and text-understanding
    ability. Among these works, many of them utilize in-context examples to achieve
    generalization without the need for fine-tuning, while few of them have considered
    the problem of how to select and effectively utilize these examples. Recently,
    methods based on trajectory-level retrieval with task meta-data and using trajectories
    as in-context examples have been proposed to improve the agent’s overall performance
    in some sequential decision making tasks. However, these methods can be problematic
    due to plausible examples retrieved without task-specific state transition dynamics
    and long input with plenty of irrelevant context. In this paper, we propose a
    novel framework (*TRAD*) to address these issues. *TRAD* first conducts *Thought
    Retrieval*, achieving step-level demonstration selection via thought matching,
    leading to more helpful demonstrations and less irrelevant input noise. Then,
    *TRAD* introduces *Aligned Decision*, complementing retrieved demonstration steps
    with their previous or subsequent steps, which enables tolerance for imperfect
    thought and provides a choice for balance between more context and less noise.
    Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only
    outperforms state-of-the-art models but also effectively helps in reducing noise
    and promoting generalization. Furthermore, TRAD has been deployed in real-world
    scenarios of a global business insurance company and improves the success rate
    of robotic process automation. Our codes are available at: [https://github.com/skyriver-2000/TRAD-Official](https://github.com/skyriver-2000/TRAD-Official).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型（LLM）广泛的知识和文本理解能力，已经为不同的任务（如网页导航和在线购物）构建了大量的LLM代理。在这些工作中，许多利用上下文示例来实现泛化，无需微调，但很少考虑如何选择和有效利用这些示例。最近，基于任务元数据的轨迹级检索和使用轨迹作为上下文示例的方法被提出，以提高代理在某些序列决策任务中的整体表现。然而，这些方法可能会遇到问题，例如检索到的示例可能不符合特定任务的状态转换动态，或输入过长且包含大量无关的上下文。本文提出了一种新颖的框架（*TRAD*）来解决这些问题。*TRAD*首先进行*思维检索*，通过思维匹配实现逐步演示选择，从而获得更有帮助的演示和减少无关输入噪声。然后，*TRAD*引入*对齐决策*，用前后步骤来补充检索到的演示步骤，这样能够容忍思维的不完美，并提供在更多上下文和较少噪声之间的平衡选择。对ALFWorld和Mind2Web基准的广泛实验表明，*TRAD*不仅优于最先进的模型，还有效减少了噪声并促进了泛化。此外，*TRAD*已在全球商业保险公司的实际场景中部署，并提高了机器人过程自动化的成功率。我们的代码可在：[https://github.com/skyriver-2000/TRAD-Official](https://github.com/skyriver-2000/TRAD-Official)。
- en: Large Language Model, LLM Agent, Sequential Decision Making, LLM Reasoning,
    Information Retrieval
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型、LLM代理、序列决策、LLM推理、信息检索
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: '![Refer to caption](img/50d4636319b49d0a8ced94f2aa9cd612.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/50d4636319b49d0a8ced94f2aa9cd612.png)'
- en: 'Figure 1. An overall illustration of *TRAD* agent (on ALFWorld (Shridhar et al.,
    [2021](#bib.bib31)) enviroment). *TRAD* first pre-processes expert trajectories,
    labeling each step with high-quality thoughts. At inference time, *TRAD* first
    conducts *thought retrieval*, which generates thought with trajectory-wise retrieved
    demonstrations as the query and keys for a more precise step-wise demonstration
    retrieval. Given the retrieved steps, TRAD employs *aligned decision* module to
    complement their temporally neighboring steps and corresponding position information
    (Fig. [2](#S2.F2 "Figure 2 ‣ 2.2\. In-Context Example Selection ‣ 2\. Related
    Work ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned
    Decision")). Finally, the next action is generated according to the enhanced demonstration.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '图1. *TRAD*代理的整体示意图（在ALFWorld（Shridhar等，[2021](#bib.bib31)）环境中）。*TRAD*首先预处理专家轨迹，为每个步骤标注高质量的思维。在推理时，*TRAD*首先进行*思维检索*，生成以轨迹为基础的检索演示作为查询和关键字，从而更精确地进行逐步演示检索。根据检索到的步骤，*TRAD*使用*对齐决策*模块来补充其时间邻近步骤及相应的位置信息（见图[2](#S2.F2
    "图2 ‣ 2.2\. 上下文示例选择 ‣ 2\. 相关工作 ‣ TRAD: 用逐步思维检索和对齐决策增强LLM代理")）。最后，根据增强的演示生成下一步动作。'
- en: Large Language Models (LLMs) (Brown et al., [2020](#bib.bib4); Touvron et al.,
    [2023](#bib.bib33)) have achieved remarkable success on various tasks like question
    answering (Zheng et al., [2024a](#bib.bib46)), chatbot (Ouyang et al., [2022](#bib.bib21)),
    code synthesis (Roziere et al., [2023](#bib.bib25)), text ranking (Ferraretto
    et al., [2023](#bib.bib8)), table-based reasoning (Ye et al., [2023](#bib.bib44)),
    and retrieval query expansion (Mackie et al., [2023](#bib.bib18)) due to their
    wide knowledge and excellent ability of text understanding and generation. Recently,
    a series of works have attempted to build powerful agents based on LLMs for various
    sequential decision-making tasks, including text-based games (Yao et al., [2023a](#bib.bib42)),
    online shopping (Yao et al., [2022](#bib.bib41)), web navigation (Deng et al.,
    [2023](#bib.bib5)), and information retrieval (Zhu et al., [2023](#bib.bib49)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）（Brown et al., [2020](#bib.bib4); Touvron et al., [2023](#bib.bib33)）在各种任务上取得了显著成功，如问答（Zheng
    et al., [2024a](#bib.bib46)）、聊天机器人（Ouyang et al., [2022](#bib.bib21)）、代码生成（Roziere
    et al., [2023](#bib.bib25)）、文本排序（Ferraretto et al., [2023](#bib.bib8)）、基于表格的推理（Ye
    et al., [2023](#bib.bib44)）和检索查询扩展（Mackie et al., [2023](#bib.bib18)），这归功于它们广泛的知识和出色的文本理解与生成能力。最近，一系列研究尝试基于LLMs构建强大的代理，用于各种顺序决策任务，包括基于文本的游戏（Yao
    et al., [2023a](#bib.bib42)）、在线购物（Yao et al., [2022](#bib.bib41)）、网页导航（Deng et
    al., [2023](#bib.bib5)）和信息检索（Zhu et al., [2023](#bib.bib49)）。
- en: Among existing LLM agents, some are trained with large-scale expert data by
    supervised fine-tuning (SFT) (Nakano et al., [2021](#bib.bib19); Gur et al., [2023](#bib.bib10),
    [2024](#bib.bib9)), while some are tuning-free and utilize in-context learning
    (ICL) with few expert demonstration examples (Yao et al., [2023b](#bib.bib43);
    Kim et al., [2023](#bib.bib14); Wang et al., [2023d](#bib.bib35); Zheng et al.,
    [2024b](#bib.bib47)). In this paper, we focus the scope on tuning-free ICL methods,
    as they are highly cost-effective and can seamlessly generalize to different tasks
    using only a small amount of expert samples. Most existing ICL-based agents are
    prompted with expert trajectories carefully selected by human (Wei et al., [2022](#bib.bib39);
    Yao et al., [2023b](#bib.bib43); Shinn et al., [2023](#bib.bib29)), which work
    well when few expert trajectories are available. However, when we have access
    to a large dataset of expert trajectories or an expert policy, the automatic and
    personalized selection of expert trajectories for each task instruction becomes
    necessary, and can have an essential influence on task performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的LLM代理中，有些是通过监督微调（SFT）（Nakano et al., [2021](#bib.bib19); Gur et al., [2023](#bib.bib10),
    [2024](#bib.bib9)）使用大规模专家数据进行训练的，而有些则是无调优的，利用少量专家示例进行上下文学习（ICL）（Yao et al., [2023b](#bib.bib43);
    Kim et al., [2023](#bib.bib14); Wang et al., [2023d](#bib.bib35); Zheng et al.,
    [2024b](#bib.bib47)）。在本文中，我们将重点关注无调优的ICL方法，因为它们具有高成本效益，并且仅使用少量专家样本就能无缝地推广到不同任务。大多数现有的基于ICL的代理都是通过人类精心选择的专家轨迹来提示的（Wei
    et al., [2022](#bib.bib39); Yao et al., [2023b](#bib.bib43); Shinn et al., [2023](#bib.bib29)），当可用的专家轨迹很少时，这些方法效果良好。然而，当我们能够访问大量专家轨迹数据集或专家策略时，为每个任务指令自动化和个性化地选择专家轨迹变得必要，这对任务性能可能产生重要影响。
- en: Recently, Zheng et al. ([2024b](#bib.bib47)) study the problem of demonstration
    selection and propose *Synapse*, which retrieves relevant expert trajectories
    by task meta-data, and then prompts LLMs with these retrieved trajectories. *Synapse*
    performs well on computer control tasks (MiniWob++ (Shi et al., [2017](#bib.bib28)))
    and web navigation tasks (Mind2Web (Deng et al., [2023](#bib.bib5))). Nevertheless,
    retrieving and prompting with complete trajectories can be problematic in the
    following three aspects.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Zheng et al. ([2024b](#bib.bib47)) 研究了演示选择的问题，并提出了*Synapse*，它通过任务元数据检索相关的专家轨迹，然后用这些检索到的轨迹提示LLMs。*Synapse*
    在计算机控制任务（MiniWob++（Shi et al., [2017](#bib.bib28)））和网页导航任务（Mind2Web（Deng et al.,
    [2023](#bib.bib5)））中表现良好。然而，使用完整轨迹进行检索和提示在以下三个方面可能存在问题。
- en: Plausible examples. Sometimes generalization to data from various domains can
    be critical. For example, in cross-website and cross-domain subsets of Mind2Web,
    agents operate on websites unseen in the training set, i.e., memory. In this case,
    retrieving trajectories with only task meta-data is very likely to provide plausible
    examples, which share similar task instructions to the current one but require
    totally different solutions. As shown by experiments in (Zheng et al., [2024b](#bib.bib47)),
    plausible examples provide no more information than random examples and can usually
    mislead LLM agents to wrong decisions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 合理示例。有时，对来自不同领域的数据进行泛化至关重要。例如，在 Mind2Web 的跨网站和跨领域子集里，代理在训练集未见过的网站上操作，即记忆。在这种情况下，仅用任务元数据检索轨迹很可能会提供合理示例，这些示例与当前任务有相似的任务指令，但需要完全不同的解决方案。如（Zheng
    et al.，[2024b](#bib.bib47)）的实验所示，合理示例提供的信息不比随机示例多，并且通常会误导 LLM 代理做出错误决定。
- en: Context limit of LLMs. When facing tasks with long horizons and complex observations,
    prompting with complete trajectories will result in input sequences longer than
    the allowed length of LLMs. *Synapse* thus has to reduce the number of trajectory
    examples or even fail to complete the task directly. Though some long-context
    LLMs can receive very long prompts, the performance can be harmed due to the issue
    of long-term forgetting (Team, [2023](#bib.bib32)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的上下文限制。在面对具有长远视野和复杂观察的任务时，使用完整的轨迹进行提示会导致输入序列超出 LLMs 允许的长度。因此，*Synapse*
    必须减少轨迹示例的数量，甚至可能无法直接完成任务。尽管一些长上下文 LLMs 可以接收非常长的提示，但由于长期遗忘问题，其性能可能受到影响（Team，[2023](#bib.bib32)）。
- en: Irrelevant information in prompts. LLMs are found sensitive to their prompts,
    and can easily copy their recent input (Radford et al., [2019](#bib.bib23); Holtzman
    et al., [2020](#bib.bib12)). The decision at the current timestep can be related
    to very few steps in a retrieved trajectory, while other steps do not provide
    any helpful information. Therefore, irrelevant steps will have unpredictable effects
    on the decision of LLM agents. As shown by our experiments, they negatively impact
    the performance most of the time.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 提示中的无关信息。发现 LLMs 对其提示非常敏感，并且容易复制其最近的输入（Radford et al., [2019](#bib.bib23); Holtzman
    et al., [2020](#bib.bib12)）。当前时间步的决策可能与检索轨迹中的非常少量步骤相关，而其他步骤则没有提供任何有用的信息。因此，无关步骤对
    LLM 代理的决策会产生不可预测的影响。我们的实验表明，这些无关步骤大多数情况下会对性能产生负面影响。
- en: To address the problems of trajectory-wise retrieval and prompting, we delve
    into step-wise demonstration retrieval and prompting. We discover that, via demonstrating
    with relevant steps, the input context of the LLM agent can be significantly reduced.
    Thus, the issue of context limit and irrelevant information can be alleviated.
    Therefore, the critical part is to retrieve step demonstrations that are truly
    relevant and helpful. To achieve this, we utilize step-by-step reasoning, i.e.
    *Chain-of-Thought* technique (Wei et al., [2022](#bib.bib39)), to abstract the
    state at each timestep as retrieval queries and keys. The generated *thoughts*
    can involve historical information or future plans, which is more specific with
    state transitions and helpful in reducing plausible examples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决轨迹级检索和提示的问题，我们深入探讨了逐步演示检索和提示。我们发现，通过展示相关步骤，可以显著减少 LLM 代理的输入上下文。因此，可以缓解上下文限制和无关信息的问题。因此，关键是检索真正相关和有用的步骤演示。为此，我们利用逐步推理，即
    *Chain-of-Thought* 技术（Wei et al., [2022](#bib.bib39)），将每个时间步的状态抽象为检索查询和键。生成的 *thoughts*
    可以涉及历史信息或未来计划，这在状态转移上更为具体，并有助于减少合理示例。
- en: 'In this paper, we propose *Thought Retrieval* and *Aligned Decision* (*TRAD*),
    a novel framework that achieves step-wise demonstration retrieval via thought
    matching and enhances the context for action prediction with temporally neighboring
    steps and their order information. Our contribution can be summarized in four-folds:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 *Thought Retrieval* 和 *Aligned Decision*（*TRAD*），这是一个通过思维匹配实现逐步演示检索的新框架，并通过时间上相邻的步骤及其顺序信息增强行动预测的上下文。我们的贡献可以总结为四个方面：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a *thought retrieval* method, where we label thoughts for expert
    demonstration steps in advance with an LLM, prompt LLM agents to reason at inference
    time, and achieve step-wise retrieval by a similarity search on thought. To the
    best of our knowledge, this is the first work that enables the LLM agent with
    thought retrieval techniques for sequential decision-making.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种*思维检索*方法，我们预先用LLM标记专家演示步骤中的思维，在推理时提示LLM代理进行推理，并通过对思维的相似性搜索实现逐步检索。根据我们所知，这是首个将思维检索技术应用于顺序决策的LLM代理的工作。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on the thought retrieval operation, we further propose an *aligned decision*
    method, where we supply the retrieved steps with their temporal neighbors to overcome
    imperfect thoughts and enhance task-relevant information.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于思维检索操作，我们进一步提出了一种*对齐决策*方法，其中我们为检索到的步骤提供其时间邻域以克服不完善的思维并增强任务相关信息。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments and analysis on Mind2Web (Deng et al., [2023](#bib.bib5))
    tasks and ALFWorld (Shridhar et al., [2021](#bib.bib31)), showing that TRAD achieves
    state-of-the-art (SoTA) performance compared to existing works. *TRAD* brings
    a 2.99% improvement over the strongest baseline (93.78% $\rightarrow$ 96.77%)
    to the success rate (SR) on ALFWorld. On Mind2Web, *TRAD* improves element accuracy,
    step SR, and SR remarkably over the powerful *Synapse* agent (Zheng et al., [2024b](#bib.bib47))
    by 2.1%, 1.4%, and 0.5%.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在Mind2Web（Deng et al., [2023](#bib.bib5)）任务和ALFWorld（Shridhar et al., [2021](#bib.bib31)）上进行了广泛的实验和分析，显示**TRAD**相比现有工作达到了最先进的（SoTA）性能。*TRAD*在ALFWorld上比最强基线提高了2.99%（93.78%
    $\rightarrow$ 96.77%）成功率（SR）。在Mind2Web上，*TRAD*在元素准确性、步骤成功率和整体成功率方面分别比强大的*Synapse*代理（Zheng
    et al., [2024b](#bib.bib47)）提高了2.1%、1.4%和0.5%。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have deployed TRAD to the real-world robotic process automation scenarios
    of a global business insurance company, where *TRAD* enables the LLM agent to
    significantly improve the success rate in a bunch of practical tasks. In average,
    *TRAD* raises step SR from 90.2% to 98.1% and SR from 65.0% to 92.5%.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经将**TRAD**部署到全球商业保险公司的真实世界机器人流程自动化场景中，其中*TRAD*使LLM代理在一系列实际任务中显著提高了成功率。平均而言，*TRAD*将步骤成功率从90.2%提高到98.1%，将整体成功率从65.0%提高到92.5%。
- en: 2\. Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. LLM Agents
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM代理
- en: 'In recent years, there has been a rapidly growing trend to utilize pre-trained
    LLMs as the central controller to obtain human-level decision-making capabilities
    (Wang et al., [2023b](#bib.bib36)). Among these works: Nakano et al. ([2021](#bib.bib19))
    fine-tune the GPT-3 (Brown et al., [2020](#bib.bib4)) model for question answering
    in a text-based web browsing environment. Yao et al. ([2022](#bib.bib41)) develop
    WebShop, a simulated e-commerce website environment, and fine-tune a BERT (Devlin
    et al., [2018](#bib.bib6)) model with imitation learning and reinforcement learning.
    Yao et al. ([2023b](#bib.bib43)) insert a reasoning section between observation
    input and action output, significantly improving the performance on ALFWorld (Shridhar
    et al., [2021](#bib.bib31)) and WebShop (Yao et al., [2022](#bib.bib41)) tasks.
    Shinn et al. ([2023](#bib.bib29)) further improve over (Yao et al., [2023b](#bib.bib43))
    via verbally reflecting on linguistic task feedback signals. Schick et al. ([2023](#bib.bib27))
    teach LLMs to use external tools via simple APIs in a self-supervised learning
    way. Park et al. ([2023](#bib.bib22)) introduce *Generative Agents*, extending
    LLMs with natural language memories and retrieving them dynamically to plan behavior.
    Wang et al. ([2023a](#bib.bib38)) propose *DEPS*, an interactive planning approach,
    which facilitates better error correction by integrating a description of the
    plan execution process and an explanation of failure feedback. Wang et al. ([2023d](#bib.bib35))
    employ an exploration curriculum, a growing skill library, and a novel iterative
    prompting mechanism, leading to better proficiency in playing Minecraft. Deng
    et al. ([2023](#bib.bib5)) construct the Mind2Web dataset from real-world webpages,
    which consists of three subsets requiring different degrees of generalization,
    and compare the performance of imitation learning and few-shot inference.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，利用预训练的大型语言模型（LLMs）作为核心控制器以获得人类水平的决策能力的趋势迅速增长（Wang 等，[2023b](#bib.bib36)）。在这些研究中：Nakano
    等人（[2021](#bib.bib19)）对 GPT-3（Brown 等，[2020](#bib.bib4)）模型进行微调，用于文本基础的网页浏览环境中的问答。Yao
    等人（[2022](#bib.bib41)）开发了 WebShop，一个模拟的电子商务网站环境，并通过模仿学习和强化学习对 BERT（Devlin 等，[2018](#bib.bib6)）模型进行微调。Yao
    等人（[2023b](#bib.bib43)）在观察输入和行动输出之间插入了一个推理部分，显著提高了 ALFWorld（Shridhar 等，[2021](#bib.bib31)）和
    WebShop（Yao 等，[2022](#bib.bib41)）任务的性能。Shinn 等人（[2023](#bib.bib29)）通过对语言任务反馈信号进行口头反思，进一步改进了（Yao
    等，[2023b](#bib.bib43)）的方法。Schick 等人（[2023](#bib.bib27)）通过自我监督学习的方式，教会 LLMs 使用简单
    API 的外部工具。Park 等人（[2023](#bib.bib22)）介绍了*生成代理*，通过自然语言记忆扩展 LLMs，并动态检索这些记忆以规划行为。Wang
    等人（[2023a](#bib.bib38)）提出了*DEPS*，一种互动规划方法，通过整合计划执行过程的描述和失败反馈的解释，促进了更好的错误修正。Wang
    等人（[2023d](#bib.bib35)）采用了探索课程、不断增长的技能库和一种新颖的迭代提示机制，从而在玩 Minecraft 中提高了能力。Deng
    等人（[2023](#bib.bib5)）构建了 Mind2Web 数据集，该数据集来源于真实的网页，由需要不同程度泛化的三个子集组成，并比较了模仿学习和少量示例推理的性能。
- en: 'As can be seen above, most existing LLM agents focus on: 1) improving task
    performance by direct fine-tuning (Nakano et al., [2021](#bib.bib19); Yao et al.,
    [2022](#bib.bib41); Deng et al., [2023](#bib.bib5)); 2) enhancing planning or
    reasoning by explicitly prompting (Yao et al., [2023b](#bib.bib43); Shinn et al.,
    [2023](#bib.bib29); Wang et al., [2023a](#bib.bib38)); 3) extending the application
    with an external memory or tool library (Schick et al., [2023](#bib.bib27); Park
    et al., [2023](#bib.bib22); Wang et al., [2023d](#bib.bib35)). However, providing
    more relevant information in prompts, as a fundamental way to elicit better task
    understanding, does not receive sufficient attention. When near-optimal demonstrations
    are accessible, selecting few-shot demonstrations properly can be a simple yet
    very effective way to improve task performance, which is investigated in our work.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，大多数现有的 LLM 代理集中于：1）通过直接微调提高任务性能（Nakano 等，[2021](#bib.bib19)；Yao 等，[2022](#bib.bib41)；Deng
    等，[2023](#bib.bib5)）；2）通过显式提示增强规划或推理（Yao 等，[2023b](#bib.bib43)；Shinn 等，[2023](#bib.bib29)；Wang
    等，[2023a](#bib.bib38)）；3）通过外部记忆或工具库扩展应用（Schick 等，[2023](#bib.bib27)；Park 等，[2023](#bib.bib22)；Wang
    等，[2023d](#bib.bib35)）。然而，在提示中提供更多相关信息，作为引发更好任务理解的基础方法，并未受到足够关注。当接近最佳演示可用时，适当地选择少量演示可以是一种简单但非常有效的方法来提高任务性能，这是我们工作的研究内容。
- en: 2.2\. In-Context Example Selection
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 上下文示例选择
- en: LLMs have been shown excellence of few-shot learning (Brown et al., [2020](#bib.bib4)),
    and the selection of in-context examples can yield a significant improvement on
    the overall performance. Liu et al. ([2021](#bib.bib17)) first propose to retrieve
    the $k$-NN search via minimizing the entropy of output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在少量样本学习方面表现出了卓越的能力（Brown 等，[2020](#bib.bib4)），而在上下文中的示例选择可以显著提高整体性能。Liu
    等（[2021](#bib.bib17)）首次提出通过最小化输出的熵来检索 $k$-NN。
- en: '*IRCoT* (Trivedi et al., [2023](#bib.bib34)) should be the most relevant work
    to ours, which retrieves relevant documents with reasoning steps on question-answering
    tasks. However, their method consists of retrieving with a complete historical
    trajectory and accumulating retrieved trajectories over time, which are not transferable
    to complex sequential decision-making tasks, and we propose a method different
    from theirs in that: (i) Our method focuses on both providing more relevant demonstrations
    and reducing irrelevant context for sequential decision-making tasks, while theirs
    is limited to question-answering tasks and only addresses the first issue. (ii)
    Our method retrieves completely different steps across timesteps and complements
    the retrieval results with temporal information, while theirs only accumulates
    relevant documents at every reasoning step and heuristically cuts off the earliest
    ones to fit in the context limit of LLMs. (iii) Our method prepares pseudo-golden
    thoughts for expert trajectories in the memory to enable retrieval with trajectories
    without thoughts, and utilizes single-step thoughts as both queries and keys for
    precise retrieval, while theirs uses thoughts only as queries with raw documents
    as keys.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*IRCoT*（Trivedi 等，[2023](#bib.bib34)）应是与我们工作最相关的研究，它在问答任务中通过推理步骤检索相关文档。然而，他们的方法包括使用完整的历史轨迹进行检索，并随着时间的推移累积检索到的轨迹，这些方法不适用于复杂的顺序决策任务，我们提出了一种与他们不同的方法：（i）我们的方法不仅关注提供更相关的示例，还减少顺序决策任务中的无关上下文，而他们的方法仅限于问答任务，仅解决了第一个问题。（ii）我们的方法在时间步长中检索完全不同的步骤，并利用时间信息补充检索结果，而他们的方法仅在每个推理步骤中累积相关文档，并通过启发式方法剪切最早的文档以适应LLMs的上下文限制。（iii）我们的方法为专家轨迹在内存中准备伪黄金思想，以便在没有思想的情况下进行轨迹检索，并将单步思想作为查询和键进行精确检索，而他们的方法仅将思想作为查询，原始文档作为键。'
- en: '![Refer to caption](img/69fc91bc4d00d3ae108052d9b6e5d6cc.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/69fc91bc4d00d3ae108052d9b6e5d6cc.png)'
- en: Figure 2. An illustration of our *aligned decision* method, where $B=F=1$ previous
    steps to enrich information and align with demonstrations.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 我们的*对齐决策*方法的示意图，其中 $B=F=1$ 代表前一步骤以丰富信息并与示例对齐。
- en: 'The selection of in-context examples has been studied thoroughly for non-sequential
    tasks like question answering and sentiment analysis. However, for sequential
    decision-making tasks, how to select the examples to improve the overall performance
    remains unclear. Zheng et al. ([2024b](#bib.bib47)) propose a trajectory-wise
    retrieval solution, while a more precise step-wise solution is still desired as
    discussed in Section [1](#S1 "1\. Introduction ‣ TRAD: Enhancing LLM Agents with
    Step-Wise Thought Retrieval and Aligned Decision"), which motivates our work.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文示例的选择在非顺序任务（如问答和情感分析）中已经被彻底研究。然而，对于顺序决策任务，如何选择示例以提高整体性能仍然不清楚。Zheng 等（[2024b](#bib.bib47)）提出了一种基于轨迹的检索解决方案，而如第[1](#S1
    "1\. Introduction ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval
    and Aligned Decision")节所讨论的，更精确的逐步解决方案仍然是期望的，这也激励了我们的工作。'
- en: 2.3\. LLM Planning and Reasoning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. LLM 规划与推理
- en: Our work proposes to use thought, which can be viewed as a general abstraction
    of the current state, as queries and keys for retrieval. Nevertheless, plans,
    code comments, and any other text that extracts comprehensive information about
    the current state can serve as an alternative. Therefore, we particularly review
    some remarkable reasoning and planning works based on LLMs, and most of them are
    complementary to our work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作建议使用思想作为检索的查询和键，这可以被视为当前状态的一般抽象。然而，计划、代码注释以及任何其他提取当前状态全面信息的文本也可以作为替代方案。因此，我们特别回顾了一些基于LLMs的显著推理和规划工作，它们大多数与我们的工作是互补的。
- en: Wei et al. ([2022](#bib.bib39)) first introduce the concept of *Chain-of-Thought*
    (CoT) by providing with explicit step-by-step reasoning process in example outputs
    improving performance on arithmetic, commonsense, and symbolic reasoning tasks.
    Wang et al. ([2023c](#bib.bib37)) further find that a single reasoning path can
    be sub-optimal, and propose *self-consistency* to address this problem by sampling
    multiple reasoning paths. For efficient yet flexible search of reasoning paths,
    Yao et al. ([2023a](#bib.bib42)) apply tree search with self-evaluation to find
    globally excellent thoughts. Besta et al. ([2023](#bib.bib3)) later extend the
    tree-search structure to a graph search for even better flexibility and overall
    performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Wei 等人 ([2022](#bib.bib39)) 首次引入了 *Chain-of-Thought*（CoT）的概念，通过在示例输出中提供明确的逐步推理过程来提高算术、常识和符号推理任务的性能。Wang
    等人 ([2023c](#bib.bib37)) 进一步发现单一的推理路径可能并不最优，并提出 *self-consistency* 来解决这个问题，通过采样多个推理路径。为了高效而灵活地搜索推理路径，Yao
    等人 ([2023a](#bib.bib42)) 应用自我评估的树搜索来找到全球优秀的思路。Besta 等人 ([2023](#bib.bib3)) 后来将树搜索结构扩展为图搜索，以获得更好的灵活性和整体性能。
- en: 'The works mentioned above consider problems that are non-sequential or solvable
    by a single complete reasoning path after receiving the input. For harder sequential
    decision-making problems: Zhou et al. ([2023](#bib.bib48)) introduce *least-to-most*
    prompting to solve hard problems by decomposing the problem and solving sub-problems
    sequentially. *ReAct* proposed by Yao et al. ([2023b](#bib.bib43)) interacts with
    the environment in a reason-then-act style, which enriches the context for action
    prediction. *Code-as-Policies* (Liang et al., [2023](#bib.bib15)) writes executable
    codes for embodied control by hierarchically expanding undefined programs, which
    can be viewed as implicit reasoning or CoT process. Liu et al. ([2023](#bib.bib16))
    propose to incorporate the strength of classical planners by translating the original
    problem into a PDDL (Aeronautiques et al., [1998](#bib.bib2)) problem to solve
    by classical planners. Hao et al. ([2023](#bib.bib11)) and Ding et al. ([2023](#bib.bib7))
    share a similar insight that reasoning can be implemented indeed by planning,
    where (Hao et al., [2023](#bib.bib11)) use LLMs as world models and (Ding et al.,
    [2023](#bib.bib7)) conduct MCTS for thought generation with a light-weight extra
    network.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工作考虑的是输入后可以通过单一完整推理路径解决的非顺序问题。对于更困难的顺序决策问题：Zhou 等人 ([2023](#bib.bib48)) 引入了
    *least-to-most* 提示，通过将问题分解并顺序解决子问题来解决困难问题。Yao 等人 ([2023b](#bib.bib43)) 提出的 *ReAct*
    以先推理后行动的方式与环境互动，从而丰富了行动预测的背景。*Code-as-Policies*（Liang 等人，[2023](#bib.bib15)）通过分层扩展未定义的程序来为体现控制编写可执行代码，这可以视为隐性推理或
    CoT 过程。Liu 等人 ([2023](#bib.bib16)) 提议通过将原始问题转换为 PDDL（Aeronautiques 等人，[1998](#bib.bib2)）问题来结合经典规划者的优势以供经典规划者解决。Hao
    等人 ([2023](#bib.bib11)) 和 Ding 等人 ([2023](#bib.bib7)) 分享了一个类似的见解，即推理确实可以通过规划来实现，其中
    (Hao 等人，[2023](#bib.bib11)) 使用 LLM 作为世界模型，而 (Ding 等人，[2023](#bib.bib7)) 使用轻量级的额外网络进行思路生成的
    MCTS。
- en: To summarize, LLM planning and reasoning have continuously received huge attention
    from researchers in recent years. This makes our work flexible and improvable
    with more powerful planning and reasoning methods in the future.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，近年来 LLM 规划和推理持续受到研究人员的广泛关注。这使得我们的工作具有灵活性，并能在未来通过更强大的规划和推理方法进行改进。
- en: 3\. The TRAD Framework
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. TRAD 框架
- en: 'As discussed in Section [1](#S1 "1\. Introduction ‣ TRAD: Enhancing LLM Agents
    with Step-Wise Thought Retrieval and Aligned Decision"), trajectory-wise retrieving
    and prompting lead to issues of plausible examples, LLM context limits, and irrelevant
    information. To resolve these issues, we propose a novel method called *Thought
    Retrieval* and *Aligned Decision* (*TRAD*), as illustrated in Fig. [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought
    Retrieval and Aligned Decision"). Our *TRAD* agent utilizes thought, which is
    obtained by reasoning about its current state, to retrieve similar steps from
    expert trajectories, and is then complemented with steps temporally correlated
    to the retrieved ones and their temporal position information to predict the action.
    Formally, our *TRAD* agent can be summarized in one equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[1](#S1 "1\. 介绍 ‣ TRAD: 用于逐步思维检索和对齐决策的LLM代理的增强")节中讨论的，基于轨迹的检索和提示会导致可能示例、LLM上下文限制和无关信息的问题。为了解决这些问题，我们提出了一种新方法，称为*思维检索*和*对齐决策*（*TRAD*），如图[1](#S1.F1
    "图 1 ‣ 1\. 介绍 ‣ TRAD: 用于逐步思维检索和对齐决策的LLM代理的增强")所示。我们的*TRAD*代理利用通过推理当前状态获得的思维，从专家轨迹中检索相似步骤，并通过与检索步骤时间上相关的步骤及其时间位置信息来预测动作。正式地，我们的*TRAD*代理可以用一个方程来总结：'
- en: '|  | $1$2 |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\xi$ refers to the thought-enhanced memory. We will present each module
    of *TRAD* in the following subsections.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\xi$ 指代增强思维记忆。我们将在以下小节中介绍*TRAD*的每个模块。
- en: 3.1\. Thought Preparation
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 思维准备
- en: Most expert trajectories, collected by either human or other expert agents,
    do not contain their reasoning process. Therefore, before we utilize thoughts
    for retrieval, we should prepare thoughts for each demonstration step in the memory.
    Specifically, we start from a small subset of expert demonstrations and provide
    thoughts written by human experts for each step in it. Given this small subset
    as few-shot examples in prompts, we can query LLMs to label thoughts for a large
    memory. Although ground-truth actions are not accessible at inference time, we
    can prompt LLMs with them to generate thoughts of higher quality. In this way,
    LLMs produce pseudo-golden thoughts consistent with expert actions, and we obtain
    a *thought-enhanced memory* ${\mathcal{M}}$ supporting both trajectory-wise retrieval
    with task meta-data and step-wise retrieval with thoughts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数由人类或其他专家代理收集的专家轨迹不包含其推理过程。因此，在我们利用思维进行检索之前，我们应该为内存中的每个演示步骤准备思维。具体来说，我们从一小部分专家演示开始，并为其中的每一步提供人类专家编写的思维。将这一小部分作为提示中的少量示例，我们可以查询LLMs以标记内存的大量思维。虽然在推理时无法获得真实的动作，但我们可以用它们来提示LLMs，以生成更高质量的思维。通过这种方式，LLMs生成与专家动作一致的伪黄金思维，我们获得一个*增强思维记忆*
    ${\mathcal{M}}$，支持基于轨迹的检索以及基于思维的逐步检索。
- en: 3.2\. Thought Retrieval
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 思维检索
- en: 'Given pseudo-golden thoughts for all steps in the memory, which can serve as
    keys for step-wise similarity search, we now present our *thought retrieval* method
    to select relevant demonstrations at inference time. To be specific, we first
    conduct trajectory-wise demonstration retrieval as in (Zheng et al., [2024b](#bib.bib47))
    for thought generation. With these trajectory demonstrations, at each timestep
    $t$ for step-wise retrieval. Note that this process does not directly effects
    decision-making, hence it can be further simplified if necessary and the issues
    mentioned in Section [1](#S1 "1\. Introduction ‣ TRAD: Enhancing LLM Agents with
    Step-Wise Thought Retrieval and Aligned Decision") will not impact the agent severely.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '给定内存中所有步骤的伪黄金思维，这些思维可以作为逐步相似性搜索的键，我们现在介绍我们的*思维检索*方法，以选择推理时相关的演示。具体来说，我们首先进行轨迹级演示检索，如(Zheng
    et al., [2024b](#bib.bib47))中所述，用于思维生成。利用这些轨迹演示，在每个时间步 $t$ 进行逐步检索。请注意，这一过程不会直接影响决策，因此如果必要的话可以进一步简化，第[1](#S1
    "1\. 介绍 ‣ TRAD: 用于逐步思维检索和对齐决策的LLM代理的增强")节中提到的问题不会对代理造成严重影响。'
- en: With the thought $\tau_{t}$ relevant steps that belong to mutually different
    trajectories and their corresponding task instructions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 结合思维 $\tau_{t}$ 相关的步骤，这些步骤属于相互不同的轨迹及其对应的任务指令。
- en: 3.3\. Aligned Decision
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 对齐决策
- en: 'Now we have relevant demonstration steps from *thought retrieval*. However,
    the query thought can be imperfect due to the lack of expert action information
    at inference time. As we will show by ablation experiments in Section [4.4](#S4.SS4
    "4.4\. Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision"), directly using these steps to form single-step
    demonstrations does not provide satisfactory performance, which is similar to
    the plausible example issue of trajectory-wise retrieval. Therefore, we propose
    an *aligned decision* method to incorporate more information during the decision-making
    process. *Aligned decision* complements LLM agents with steps temporally correlated
    to the retrieved ones and their temporal position information. As illustrated
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.2\. In-Context Example Selection ‣ 2\. Related
    Work ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned
    Decision"), the *aligned decision* method can be decomposed into following three
    sub-processes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们有来自*思维检索*的相关演示步骤。然而，由于推理时缺乏专家行动信息，查询的思维可能是不完善的。正如我们将在第[4.4节](#S4.SS4 "4.4\.
    Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision")的消融实验中展示的那样，直接使用这些步骤形成单步演示并不能提供令人满意的性能，这类似于轨迹检索的合理示例问题。因此，我们提出了一种*对齐决策*方法，在决策过程中纳入更多信息。*对齐决策*通过将步骤与检索到的步骤及其时间位置相关联，来补充LLM代理。如图[2](#S2.F2
    "Figure 2 ‣ 2.2\. In-Context Example Selection ‣ 2\. Related Work ‣ TRAD: Enhancing
    LLM Agents with Step-Wise Thought Retrieval and Aligned Decision")所示，*对齐决策*方法可以分解为以下三个子过程。'
- en: Temporal expansion. For each retrieved step, we first expand it into a step
    sequence involving $B$ temporally successive steps, allowing LLM agents to correct
    their imperfect thoughts by looking at more related steps at decision-making time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 时间扩展。对于每个检索到的步骤，我们首先将其扩展为包含$B$个时间上连续步骤的步骤序列，使LLM代理能够通过查看更多相关步骤来纠正其不完善的思维。
- en: Relative order mark. Given $K$ demonstration steps, and promotes more accurate
    demonstration following.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 相对顺序标记。给定$K$个演示步骤，推动更准确的演示跟随。
- en: History alignment. Sometimes the optimal policy to a task, like ALFWorld, can
    be history-dependent, hence using single-step input for action prediction is unreasonable.
    Since we aim to reduce input content for less forgetting and noise, we should
    neither use all historical observations and actions. Moreover, even if we include
    previous actions as auxiliary information, there exists a mismatch where expert
    demonstrations are given as sequences of length $B+1+F$, transforming current
    input into a similar sequence to demonstrations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 历史对齐。有时，任务的最优策略，如ALFWorld，可能依赖于历史，因此使用单步输入进行动作预测是不合理的。由于我们旨在减少输入内容以减少遗忘和噪声，我们不应使用所有历史观察和行动。此外，即使我们将先前的行动作为辅助信息，也存在一个不匹配的情况，即专家演示被给定为长度为$B+1+F$的序列，将当前输入转化为类似于演示的序列。
- en: Table 1. Success Rate of Different Methods on 6 Types of ALFWorld Tasks. We
    compare *TRAD* with *ReAct* (Yao et al., [2023b](#bib.bib43)), *Synapse* (Zheng
    et al., [2024b](#bib.bib47)), and their strong combination. *TRAD* significantly
    outperforms all baselines in terms of overall performance, achieves the best performance
    in 5 out of 6 types of task, and shows a decent performance on Heat task. The
    improvement of *TRAD* over all baselines on overall performance is statistically
    significant (measured by student’s t-test at $p<0.05$).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 在6种ALFWorld任务上的不同方法的成功率。我们将*TRAD*与*ReAct*（Yao et al., [2023b](#bib.bib43)）、*Synapse*（Zheng
    et al., [2024b](#bib.bib47)）及其强组合进行比较。*TRAD*在整体性能方面显著优于所有基线方法，在6种任务中的5种任务上取得了最佳性能，并且在Heat任务上表现良好。*TRAD*在整体性能上的改进在统计上显著（通过$t$检验测量，$p<0.05$）。
- en: '| Method | Put | Examine | Clean | Heat | Cool | PutTwo | All |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 放置 | 检查 | 清理 | 加热 | 冷却 | 放置两个 | 总计 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ReAct (Random) | 0.8472$\pm$0.0093 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ReAct（随机） | 0.8472$\pm$0.0093 |'
- en: '| ReAct (Fixed) | 0.7778$\pm$0.0186 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ReAct（固定） | 0.7778$\pm$0.0186 |'
- en: '| Synapse | 0.9444$\pm$0.0106 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Synapse | 0.9444$\pm$0.0106 |'
- en: '| Synapse + ReAct | 0.9167$\pm$0.0035 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Synapse + ReAct | 0.9167$\pm$0.0035 |'
- en: '| TRAD (Ours) | 0.9583$\pm$0.0141 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| TRAD（我们的） | 0.9583$\pm$0.0141 |'
- en: 4\. Experiments
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 'In this section, we aim to study the following research questions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在研究以下研究问题：
- en: RQ1
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ1
- en: How does *TRAD* perform against existing SoTA methods?
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*TRAD*在现有的SoTA方法中表现如何？'
- en: RQ2
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ2
- en: Does *thought retrieval* help to reduce irrelevant context and improve the overall
    performance?
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*思维检索*是否有助于减少无关上下文并提高整体性能？'
- en: RQ3
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ3
- en: Does *aligned decision* help to supply information when generalization is important?
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*对齐决策*是否有助于在泛化重要时提供信息？'
- en: RQ4
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ4
- en: Diving into *aligned decision*, are all *temporal expansion* (TE), *relative
    order mark* (ROM), and *history alignment* (HA) necessary for improvement?
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深入探讨*对齐决策*，所有*时间扩展*（TE）、*相对顺序标记*（ROM）和*历史对齐*（HA）是否对改进都是必要的？
- en: RQ5
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RQ5
- en: How will the performance and advantage of *TRAD* be effected by critical hyper-parameters?
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键超参数如何影响*TRAD*的性能和优势？
- en: 4.1\. Experiment Setup
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验设置
- en: To answer the above research questions, we conduct extensive experiments on
    ALFWorld (Shridhar et al., [2021](#bib.bib31)) and Mind2Web (Deng et al., [2023](#bib.bib5))
    tasks. For each task, we introduce the details of evaluation as follows.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答上述研究问题，我们在ALFWorld（Shridhar等人，[2021](#bib.bib31)）和Mind2Web（Deng等人，[2023](#bib.bib5)）任务上进行了广泛的实验。对于每个任务，我们引入以下评估细节。
- en: 'ALFWorld (Shridhar et al., [2021](#bib.bib31)) is a text-based game aligned
    with ALFRED (Shridhar et al., [2020](#bib.bib30)) benchmark. It involves 6 types
    of tasks where an agent must take a series of actions (e.g. *go to shelf 1*, *take
    vase 2 from shelf 1*, *put vase 2 in/on cabinet 5*) to achieve a high-level goal
    given by a natural language instruction (e.g. *put some vase on a cabinet*). This
    environment is challenging in three aspects: 1) Agent should determine likely
    places of a householding object and explore them one by one to find such object;
    2) Agent should understand the usage of some objects like microwaves, fridges,
    and desklamps; 3) Some tasks can take an agent more than 30 steps to solve, requiring
    substantial long-term memorization.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ALFWorld（Shridhar等人，[2021](#bib.bib31)）是一个基于文本的游戏，与ALFRED（Shridhar等人，[2020](#bib.bib30)）基准对齐。它涉及6种类型的任务，其中代理必须采取一系列行动（例如*去架子1*、*从架子1上取下花瓶2*、*将花瓶2放在/放在柜子5上*）以实现自然语言指令给出的高层目标（例如*将一些花瓶放在柜子上*）。这个环境在三个方面具有挑战性：1）代理应确定家用物品的可能位置，并逐一探索以找到该物品；2）代理应理解一些物品的使用方式，如微波炉、冰箱和台灯；3）一些任务可能需要代理超过30步才能解决，需要大量的长期记忆。
- en: 'Following Shridhar et al. ([2021](#bib.bib31)), we evaluate on the subset of
    134 out-of-distribution tasks, comparing the task success rates of *TRAD* to *ReAct*
    (Yao et al., [2023b](#bib.bib43)) and *Synapse* (Zheng et al., [2024b](#bib.bib47))
    (without state abstraction as observations are short). As *ReAct* and *Synapse*
    has provided sufficiently strong performances, we do not include more complex
    reasoning and planning baselines and corresponding variants of *TRAD* due to our
    API cost limit. Note that the original *ReAct* uses fixed but not retrieved trajectories
    as demonstrations, hence we test two *ReAct* baselines to eliminate such an effect:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Shridhar等人（[2021](#bib.bib31)），我们在134个分布外任务的子集上进行评估，将*TRAD*的任务成功率与*ReAct*（Yao等人，[2023b](#bib.bib43)）和*Synapse*（Zheng等人，[2024b](#bib.bib47)）进行比较（由于观察短小而不进行状态抽象）。由于*ReAct*和*Synapse*已提供了足够强的性能，我们没有包括更复杂的推理和规划基线以及*TRAD*的相应变体，因API成本限制。请注意，原始*ReAct*使用固定但未检索的轨迹作为示例，因此我们测试了两个*ReAct*基线以消除这种影响：
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ReAct* (Fixed) uses fixed human-written trajectories as demonstrations;'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ReAct*（固定）使用固定的人写轨迹作为示例；'
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ReAct* (Random) randomly samples trajectories from the memory as demonstrations.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ReAct*（随机）从记忆中随机抽样轨迹作为示例。'
- en: For fair comparison, *TRAD* uses thoughts in exactly the same format as *ReAct*,
    and shares a consistent memory of expert trajectories with *Synapse*. We also
    add a strong baseline (*Synapse*+*ReAct*) combining the trajectory-level retrieval
    in *Synapse* and the reasoning in *ReAct*. On ALFWorld, all methods are built
    with GPT-4 (OpenAI, [2023](#bib.bib20)) and 2 in-context examples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了公平比较，*TRAD*以与*ReAct*完全相同的格式使用思维，并与*Synapse*共享一致的专家轨迹记忆。我们还添加了一个强基线（*Synapse*+*ReAct*），结合了*Synapse*中的轨迹级检索和*ReAct*中的推理。在ALFWorld上，所有方法都基于GPT-4（OpenAI，[2023](#bib.bib20)）和2个上下文示例构建。
- en: 'Mind2Web (Deng et al., [2023](#bib.bib5)) is an HTML-based web navigation benchmark
    collected from real-world webpages, involving various tasks such as searching,
    trip booking, social network subscription, etc. It contains 3 subsets, i.e., cross-task,
    cross-website, cross-domain. This environment is challenging in two aspects: 1)
    Existing LLM agents can hardly understand HTML input well; 2) Unseen tasks and
    websites can require substantial generalization. Deng et al. ([2023](#bib.bib5))
    find that the cross-website and cross-domain subsets are significantly harder
    due to the need for generalization to unseen websites.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Mind2Web（Deng等，[2023](#bib.bib5)）是一个基于HTML的网页导航基准数据集，来自现实网页，涉及搜索、旅行预订、社交网络订阅等多种任务。它包含3个子集，即跨任务、跨网站、跨领域。这个环境在两个方面具有挑战性：1）现有的LLM代理很难很好地理解HTML输入；2）未见过的任务和网站可能需要大量的泛化。Deng等（[2023](#bib.bib5)）发现，跨网站和跨领域子集由于需要对未见过的网站进行泛化，因此要难得多。
- en: 'Since Mind2Web was introduced only about half a year ago, there is a lack of
    suitable baseline algorithms, and thus we compare our *TRAD* agent to *Synapse*
    (Zheng et al., [2024b](#bib.bib47)) and *ReAct* (Yao et al., [2023b](#bib.bib43)).
    Following Zheng et al. ([2024b](#bib.bib47)), we evaluate on all 3 subsets, comparing
    the element accuracy (Ele. Acc), step success rate (Step SR), and trajectory success
    rate (SR). For fair comparison, we follow (Zheng et al., [2024b](#bib.bib47))
    and summarize observations into 5 web elements with the pre-trained element ranker
    provided by (Deng et al., [2023](#bib.bib5)) for all methods. Since the observations
    are still very complex on Mind2Web, including thoughts for every step in trajectories
    is not available, hence: 1) we do not include a *Synapse* + *ReAct* baseline;
    2) *TRAD* generates thoughts and predicts actions by a single-step prompt with
    the current observation and previous actions (without previous observations).
    To eliminate the effect of prompting style and reasoning, we build two *ReAct*
    baselines using the same format of prompt as *TRAD*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Mind2Web在大约半年前才被引入，缺乏合适的基线算法，因此我们将我们的*TRAD*代理与*Synapse*（Zheng等，[2024b](#bib.bib47)）和*ReAct*（Yao等，[2023b](#bib.bib43)）进行比较。根据Zheng等（[2024b](#bib.bib47)），我们在所有3个子集上进行评估，比较元素准确率（Ele.
    Acc）、步骤成功率（Step SR）和轨迹成功率（SR）。为了公平比较，我们遵循（Zheng等，[2024b](#bib.bib47)）并将观察结果总结为5个网页元素，使用（Deng等，[2023](#bib.bib5)）提供的预训练元素排序器。由于Mind2Web上的观察结果仍然非常复杂，无法在轨迹中的每一步中包含思考，因此：1）我们不包括*Synapse*
    + *ReAct*基线；2）*TRAD*通过使用当前观察和先前的动作（没有先前观察）的单步提示生成思考并预测动作。为了消除提示风格和推理的影响，我们使用与*TRAD*相同的提示格式构建了两个*ReAct*基线。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ReAct* (Random), for which we prompt *ReAct* with completely random demonstration
    steps.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ReAct*（随机），我们用完全随机的示例步骤来提示*ReAct*。'
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ReAct* (Relevant), for which we prompt *ReAct* with demonstrate steps randomly
    chosen from trajectories retrieved by *Synapse*.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ReAct*（相关性），我们用从*Synapse*中检索到的轨迹中随机选择的示例步骤来提示*ReAct*。'
- en: We do not include the *ReAct* (Fixed) baseline as it is hard to write or pick
    demonstrations commonly helpful for such diverse test sets. We also provide the
    results of the simplest MindAct (Deng et al., [2023](#bib.bib5)) baseline without
    reasoning and retrieval for completeness. On Mind2Web, all methods are built with
    GPT-3.5-turbo and 3 in-context examples.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不包括*ReAct*（固定）基线，因为很难编写或选择对这种多样化测试集常见有用的示例。为了完整性，我们还提供了最简单的MindAct（Deng等，[2023](#bib.bib5)）基线的结果，该基线没有推理和检索。在Mind2Web上，所有方法都使用GPT-3.5-turbo和3个上下文示例构建。
- en: Table 2. Results (%) of all methods on Mind2Web benchmark. *TRAD* achieves the
    best overall performances and the most improvement on the two harder subsets,
    especially the most out-of-distribution Cross-Domain subset. The improvement of
    *TRAD* over all baselines on three overall metrics is statistically significant
    (measured by student’s t-test with $p<0.01$).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. Mind2Web基准上所有方法的结果（%）。*TRAD*在两个更难的子集上，特别是在最具挑战性的跨领域子集上，取得了最佳的整体表现和最大的改进。*TRAD*在三个总体指标上相较于所有基线的改进在统计上是显著的（通过学生t检验测得，$p<0.01$）。
- en: '| Method | Cross-Task |  | Cross-Website |  | Cross-Domain |  | All |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 跨任务 |  | 跨网站 |  | 跨领域 |  | 全部 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ele. Acc | Step SR | SR |  | Ele. Acc | Step SR | SR |  | Ele. Acc | Step
    SR | SR |  | Ele. Acc | Step SR | SR |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 元素准确率 | 步骤成功率 | 成功率 |  | 元素准确率 | 步骤成功率 | 成功率 |  | 元素准确率 | 步骤成功率 | 成功率 |  |
    元素准确率 | 步骤成功率 | 成功率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| MindAct | 20.3 | 17.4 | 0.8 |  | 19.3 | 16.2 | 0.6 |  | 21.0 | 18.6 | 1.0
    |  | 20.6 | 18.0 | 0.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| MindAct | 20.3 | 17.4 | 0.8 |  | 19.3 | 16.2 | 0.6 |  | 21.0 | 18.6 | 1.0
    |  | 20.6 | 18.0 | 0.9 |'
- en: '| ReAct (Random) | 31.0 | 24.7 | 1.6 |  | 25.7 | 19.1 | 0.6 |  | 27.9 | 22.9
    | 1.8 |  | 28.3 | 22.7 | 1.6 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ReAct（随机） | 31.0 | 24.7 | 1.6 |  | 25.7 | 19.1 | 0.6 |  | 27.9 | 22.9 | 1.8
    |  | 28.3 | 22.7 | 1.6 |'
- en: '| ReAct (Relevant) | 31.3 | 26.0 | 1.2 |  | 26.7 | 20.5 | 0.6 |  | 28.0 | 23.1
    | 1.6 |  | 28.5 | 23.4 | 1.4 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ReAct（相关） | 31.3 | 26.0 | 1.2 |  | 26.7 | 20.5 | 0.6 |  | 28.0 | 23.1 | 1.6
    |  | 28.5 | 23.4 | 1.4 |'
- en: '| Synapse w/o Retrieval | 33.1 | 28.9 | 3.2 |  | 27.8 | 22.1 | 1.1 |  | 30.0
    | 26.5 | 1.4 |  | 30.4 | 26.4 | 1.7 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 无检索的 Synapse | 33.1 | 28.9 | 3.2 |  | 27.8 | 22.1 | 1.1 |  | 30.0 | 26.5
    | 1.4 |  | 30.4 | 26.4 | 1.7 |'
- en: '| Synapse | 34.4 | 30.6 | 2.0 |  | 28.8 | 23.4 | 1.1 |  | 29.4 | 25.9 | 1.6
    |  | 30.4 | 26.6 | 1.6 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Synapse | 34.4 | 30.6 | 2.0 |  | 28.8 | 23.4 | 1.1 |  | 29.4 | 25.9 | 1.6
    |  | 30.4 | 26.6 | 1.6 |'
- en: '| TRAD (Ours) | 35.2 | 30.8 | 3.6 |  | 30.4 | 24.0 | 0.6 |  | 32.0 | 28.0 |
    2.0 |  | 32.5 | 28.0 | 2.1 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| TRAD（我们的） | 35.2 | 30.8 | 3.6 |  | 30.4 | 24.0 | 0.6 |  | 32.0 | 28.0 | 2.0
    |  | 32.5 | 28.0 | 2.1 |'
- en: 4.2\. Evaluation on ALFWorld
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 在 ALFWorld 上的评估
- en: 'The success rate of each method tested on ALFWorld is shown in Tab. [1](#S3.T1
    "Table 1 ‣ 3.3\. Aligned Decision ‣ 3\. The TRAD Framework ‣ TRAD: Enhancing LLM
    Agents with Step-Wise Thought Retrieval and Aligned Decision"). Generally, our
    *TRAD* agent achieves an average success rate of 96.77%, significantly outperforming
    *ReAct* ($\sim$90%), *Synapse* (89.55%), and even their strong combination (93.78%).
    It is also worth noting that the worst trial of *TRAD* among 3 random seeds achieves
    a success rate of 94.8%, outperforming the best trial produced by any other method
    (94.0%).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法在 ALFWorld 上的成功率见表 [1](#S3.T1 "表 1 ‣ 3.3\. 对齐决策 ‣ 3\. TRAD 框架 ‣ TRAD：通过逐步思维检索和对齐决策增强
    LLM 代理")。总体而言，我们的 *TRAD* 代理的平均成功率为 96.77%，显著优于 *ReAct*（约 90%）、*Synapse*（89.55%），甚至它们的强组合（93.78%）。还值得注意的是，在
    3 个随机种子中的最差试验中，*TRAD* 的成功率达到了 94.8%，超越了其他方法中任何一个的最佳试验（94.0%）。
- en: 'Down to the success rate on each type of task, we observe that the success
    rate of each method varies more on the simplest *Put* task and the hardest *PutTwo*
    task. We discuss the results of these two tasks respectively as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种任务类型的成功率，我们观察到最简单的 *Put* 任务和最困难的 *PutTwo* 任务的成功率差异更大。我们分别讨论这两种任务的结果如下：
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: On the simplest *Put* task, *ReAct* performs even more poorly than other harder
    tasks. We find that the two vital reasons for *ReAct*’s failure on *Put* task
    are incorrect location and usage of objects, e.g. trying to put an object in a
    closed safe. As this issue can be alleviated through a combination with *Synapse*,
    the necessity of retrieving relevant demonstrations thus justified.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在最简单的 *Put* 任务中，*ReAct* 的表现甚至比其他更困难的任务还要差。我们发现 *ReAct* 在 *Put* 任务失败的两个重要原因是位置不正确和物体使用不当，例如尝试将物体放入封闭的保险箱。由于这一问题可以通过与
    *Synapse* 的结合得到缓解，因此证明了检索相关演示的必要性。
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*TRAD* achieves the largest improvement on the hardest *PutTwo* task. *PutTwo*
    requires to correct the locations of two objects and a comprehensive understanding
    of its task process. Since *TRAD*’s outstanding performance on this hardest task
    is obtained from a reduced input context at decision-making time, we can conclude
    that step-wise *thought retrieval* is helpful by reducing the noise of irrelevant
    steps and finding relevant examples more precisely.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*TRAD* 在最困难的 *PutTwo* 任务上取得了最大的改进。*PutTwo* 任务需要纠正两个物体的位置，并全面理解任务过程。由于 *TRAD*
    在这个最困难的任务上取得的出色表现是通过在决策时减少输入上下文获得的，我们可以得出结论：逐步的 *思维检索* 通过减少无关步骤的噪声和更精确地找到相关示例是有帮助的。'
- en: 4.3\. Evaluation on Mind2Web
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 在 Mind2Web 上的评估
- en: 'To verify the capability of *TRAD* under more realistic scenarios, we compare
    *TRAD* to *ReAct* and the current SoTA method, *Synapse*, on the Mind2Web benchmark,
    and the results are shown in Tab. [2](#S4.T2 "Table 2 ‣ 4.1\. Experiment Setup
    ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval
    and Aligned Decision"). We also include the results of *Synapse* without retrieval
    here to better illustrate the effect of different retrieval methods.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 *TRAD* 在更现实场景下的能力，我们将 *TRAD* 与 *ReAct* 以及当前的 SoTA 方法 *Synapse* 在 Mind2Web
    基准上进行比较，结果如表 [2](#S4.T2 "表 2 ‣ 4.1\. 实验设置 ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策增强 LLM
    代理") 所示。我们还在此处包括了 *Synapse* 无检索的结果，以更好地说明不同检索方法的效果。
- en: 'Generally, *TRAD* achieves the highest performance in terms of all 3 metrics
    averaged on 3 subsets. Considering that the trajectory-level retrieval of *Synapse*
    only brings marginal boosts on Cross-Task and Cross-Website subsets, and even
    slightly impacts the performance on the Cross-Domain subset, our *TRAD* method
    can be thus justified in two aspects:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，*TRAD*在所有3个子集的3项指标平均值中表现最佳。考虑到*Synapse*的轨迹级检索在跨任务和跨网站子集上仅带来了微小的提升，甚至对跨领域子集的性能有轻微影响，因此我们的*TRAD*方法在两个方面可以得到证明：
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: By reducing input context and utilizing step-wise relevant demonstrations, our
    step-wise *thought retrieval* helps more than the trajectory-wise retrieval with
    task meta-data in *Synapse* to improve on the simplest Cross-Task subset.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过减少输入上下文并利用逐步相关的示例，我们逐步的*思路检索*在最简单的跨任务子集上比*Synapse*中的基于轨迹的检索（具有任务元数据）效果更好。
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: By eliminating plausible examples and complementing temporal correlated steps,
    *aligned decision* helps to improve on the two harder subsets, especially the
    most out-of-distribution Cross-Domain subset.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过消除可能的示例并补充时间相关的步骤，*对齐决策*有助于在两个较难的子集上提高性能，特别是在最具分布外性的跨领域子集上。
- en: 'Furthermore, we observe that the two *ReAct* baselines perform poorly on this
    task, which indicates that:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们观察到两个*ReAct*基线在这个任务上的表现较差，这表明：
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The thoughts generated by GPT-3.5-turbo on Mind2Web tasks are not sufficient
    for LLM agents to infer the correct action.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-3.5-turbo在Mind2Web任务上生成的思路不足以让LLM代理推断出正确的行动。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The single-step prompting style which removes previous observations does not
    benefit overall performance.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去除之前观察的单步提示风格对整体性能没有帮助。
- en: On the contrary, *TRAD* utilizes these imperfect thoughts for retrieval rather
    than direct decision-making, and is complemented with temporally correlated steps
    via *aligned decision*. Therefore, *TRAD* is not negatively impacted by the imperfect
    thoughts, but transforms them into helpful information.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，*TRAD*利用这些不完美的思路进行检索，而不是直接决策，并通过*对齐决策*来补充时间相关的步骤。因此，*TRAD*并未受到不完美思路的负面影响，而是将其转化为有用的信息。
- en: Before we start the study on detailed design and hyper-parameter choices of
    *TRAD*, we can summarize our performance evaluation on ALFWorld and Mind2Web benchmarks
    and answer the first three research questions as follows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始对*TRAD*的详细设计和超参数选择进行研究之前，我们可以总结一下我们在ALFWorld和Mind2Web基准上的性能评估，并回答前三个研究问题如下。
- en: 'Answer to RQ1: On both householding (ALFWorld) and web navigation (Mind2Web)
    tasks, *TRAD* significantly outperforms curernt SoTA methods and becomes the new
    SoTA method.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对RQ1的回答：在家务（ALFWorld）和网页导航（Mind2Web）任务中，*TRAD*显著超越了当前的最先进方法，成为新的最先进方法。
- en: 'Answer to RQ2: On ALFWorld benchmark, *Synapse* + *ReAct* generates thoughts
    in exactly the same way with our *TRAD*, and uses entire relevant trajectories
    (more information than *TRAD*) as demonstrations for action prediction. However,
    *TRAD* shows obvious advantage over this baseline. Therefore, we can conclude
    that *TRAD* benefits from more relevant demonstrations and less irrelevant input
    context brought by *thought retrieval*.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对RQ2的回答：在ALFWorld基准上，*Synapse* + *ReAct* 生成的思路与我们的*TRAD*完全相同，并且使用了整个相关轨迹（比*TRAD*更多的信息）作为行动预测的示例。然而，*TRAD*在这一基线之上显示出了明显的优势。因此，我们可以得出结论，*TRAD*得益于更多相关的示例和*思路检索*带来的更少的无关输入上下文。
- en: 'Answer to RQ3: On Mind2Web benchmark, *TRAD* achieves the most improvement
    over *Synapse* on the Cross-Domain subset which requires the most generalization.
    Therefore, we can tell that the *aligned decision* method complements critical
    information for decision-making on unseen input.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对RQ3的回答：在Mind2Web基准上，*TRAD*在需要最大泛化的跨领域子集上对*Synapse*实现了最大的改进。因此，我们可以说*对齐决策*方法补充了对未见输入决策的关键信息。
- en: 4.4\. Ablation Studies
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 消融研究
- en: We have verified the effectiveness of *TRAD* on two different scenarios, i.e.,
    automatic householding and web navigation. Next, we are to examine the effect
    of each module in *TRAD*. Due to our limited budget for API usage, all ablation
    studies are conducted on the Mind2Web benchmark with GPT-3.5-turbo.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经验证了*TRAD*在自动家务和网页导航这两种不同场景下的有效性。接下来，我们将检查*TRAD*中每个模块的效果。由于我们对API使用的预算有限，所有消融研究都在Mind2Web基准上使用GPT-3.5-turbo进行。
- en: 4.4.1\. The Effect of Aligned Decision
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 对齐决策的效果
- en: 'First, we study the effect of macro building blocks of *TRAD*. Since eliminating
    *thought retrieval* will disable *aligned decision* at the same time and break
    the framework fundamentally, we do not remove the *thought retrieval* module,
    but ablate each component of *aligned decision*, i.e., *temporal expansion* (TE),
    *relative order mark* (ROM), and *history alignment* (HA), and compare the corresponding
    performances. The results are shown in Tab. [3](#S4.T3 "Table 3 ‣ 4.4.1\. The
    Effect of Aligned Decision ‣ 4.4\. Ablation Studies ‣ 4\. Experiments ‣ TRAD:
    Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们研究*TRAD*的宏观构建模块的效果。由于消除*思维检索*会同时禁用*对齐决策*并从根本上破坏框架，因此我们不移除*思维检索*模块，而是对*对齐决策*的每个组件进行消融，即*时间扩展*（TE）、*相对顺序标记*（ROM）和*历史对齐*（HA），并比较相应的性能。结果如表[3](#S4.T3
    "表3 ‣ 4.4.1\. 对齐决策的影响 ‣ 4.4\. 消融研究 ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策增强LLM代理")所示。
- en: Table 3. Results (%) of ablation studies on Mind2Web benchmark. TE builds the
    basic structure of *aligned decision* and is thus critical for performance boost
    on all three subsets. HA and ROM work well to promote generalization on the two
    harder Cross-Website and Cross-Domain subsets but provide little help on the Cross-Task
    subset. The improvement of *TRAD* over all ablation baselines on Ele. Acc and
    Step SR is statistically significant (measured by student’s t-test with $p<0.05$).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表3. Mind2Web基准的消融研究结果（%）。TE构建了*对齐决策*的基本结构，因此对所有三个子集的性能提升至关重要。HA和ROM在两个较难的跨网站和跨域子集上表现良好，但对跨任务子集的帮助有限。*TRAD*在Ele.
    Acc和Step SR上相对于所有消融基线的改进在统计上显著（通过学生t检验测得$p<0.05$）。
- en: '| Method | Cross-Task |  | Cross-Website |  | Cross-Domain |  | All |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 跨任务 |  | 跨网站 |  | 跨域 |  | 全部 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Ele. Acc | Step SR | SR |  | Ele. Acc | Step SR | SR |  | Ele. Acc | Step
    SR | SR |  | Ele. Acc | Step SR | SR |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| Ele. Acc | Step SR | SR |  | Ele. Acc | Step SR | SR |  | Ele. Acc | Step
    SR | SR |  | Ele. Acc | Step SR | SR |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| TRAD w/o TE | 34.2 | 28.4 | 1.2 |  | 27.4 | 20.4 | 0.6 |  | 29.1 | 24.0 |
    1.4 |  | 30.0 | 24.5 | 1.3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| TRAD w/o TE | 34.2 | 28.4 | 1.2 |  | 27.4 | 20.4 | 0.6 |  | 29.1 | 24.0 |
    1.4 |  | 30.0 | 24.5 | 1.3 |'
- en: '| TRAD w/o HA | 36.2 | 31.1 | 4.0 |  | 28.3 | 22.2 | 0.6 |  | 29.4 | 24.9 |
    1.8 |  | 30.8 | 25.9 | 2.1 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| TRAD w/o HA | 36.2 | 31.1 | 4.0 |  | 28.3 | 22.2 | 0.6 |  | 29.4 | 24.9 |
    1.8 |  | 30.8 | 25.9 | 2.1 |'
- en: '| TRAD w/o ROM | 35.7 | 30.5 | 3.6 |  | 28.9 | 22.3 | 0.6 |  | 31.5 | 27.2
    | 1.9 |  | 32.1 | 27.2 | 2.0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| TRAD w/o ROM | 35.7 | 30.5 | 3.6 |  | 28.9 | 22.3 | 0.6 |  | 31.5 | 27.2
    | 1.9 |  | 32.1 | 27.2 | 2.0 |'
- en: '| TRAD (Ours) | 35.2 | 30.8 | 3.6 |  | 30.4 | 24.0 | 0.6 |  | 32.0 | 28.0 |
    2.0 |  | 32.5 | 28.0 | 2.1 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| TRAD（我们的） | 35.2 | 30.8 | 3.6 |  | 30.4 | 24.0 | 0.6 |  | 32.0 | 28.0 | 2.0
    |  | 32.5 | 28.0 | 2.1 |'
- en: 'From Tab. [3](#S4.T3 "Table 3 ‣ 4.4.1\. The Effect of Aligned Decision ‣ 4.4\.
    Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision"), we observe that the performance without
    each component varies differently on the simplest Cross-Task subset and the two
    harder subsets:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[3](#S4.T3 "表3 ‣ 4.4.1\. 对齐决策的影响 ‣ 4.4\. 消融研究 ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策增强LLM代理")中，我们观察到没有每个组件的性能在最简单的跨任务子集和两个较难的子集上表现不同：
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: On the harder Cross-Website and Cross-Domain subsets, the elimination of all
    three modules in *aligned decision* results in a significant performance drop,
    and the effect of *temporal expansion* is the most significant. This is intuitive,
    since only retrieved steps are provided to the agent without TE, and thus the
    agent becomes more vulnerable to imperfect thoughts.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在较难的跨网站和跨域子集上，*对齐决策*中所有三个模块的移除会导致显著的性能下降，其中*时间扩展*的影响最为显著。这是直观的，因为没有TE，代理仅获得检索到的步骤，从而使得代理对不完美的思维变得更加脆弱。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'On the simplest Cross-Task subset, however, *history alignment* and *relative
    order mark* are not that helpful and even cause performance drop. As discussed
    earlier (Section [1](#S1 "1\. Introduction ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision") and Section [3.3](#S3.SS3 "3.3\. Aligned
    Decision ‣ 3\. The TRAD Framework ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision")), when the issue of plausible examples
    is not severe, reducing context and prompting with the most relevant demonstration
    becomes the dominant factor of performance boost. Therefore, only *temporal expansion*
    remains beneficial for recovering from imperfect thoughts, while the other two
    components lead to sub-optimal performance.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '然而，在最简单的跨任务子集上，*历史对齐*和*相对顺序标记*的帮助不大，甚至导致性能下降。如前所述（第[1](#S1 "1\. Introduction
    ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision")节和第[3.3](#S3.SS3
    "3.3\. Aligned Decision ‣ 3\. The TRAD Framework ‣ TRAD: Enhancing LLM Agents
    with Step-Wise Thought Retrieval and Aligned Decision")节），当合理示例的问题不严重时，减少上下文并以最相关的示例进行提示成为性能提升的主导因素。因此，只有*时间扩展*在恢复不完善思维方面仍然有益，而另外两个组件则导致次优表现。'
- en: Generally, the *aligned decision* method provides more information about the
    source trajectories of retrieved steps and the current trajectory, and helps especially
    for scenarios where generalization is essential. We can now summarize these observations
    and answer the fourth research question.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*对齐决策*方法提供了关于检索步骤的源轨迹和当前轨迹的更多信息，特别适用于泛化至关重要的场景。我们现在可以总结这些观察结果，并回答第四个研究问题。
- en: 'Answer to RQ4: Among the sub-processes in *aligned decision*, 1) *temporal
    expansion* provides tolerance for imperfect thoughts and improves the overall
    performance of *TRAD* consistently; 2) *relative order mark* and *history alignment*
    complement *TRAD* with temporal information about the trajectories of retrieved
    steps and the current trajectory, which serve as useful context for out-of-distribution
    decision-making but may become less useful for in-distribution decision-making.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对RQ4的回答：在*对齐决策*的子过程当中，1）*时间扩展*为不完善的思维提供了容错，并持续提升*TRAD*的整体表现；2）*相对顺序标记*和*历史对齐*通过提供检索步骤轨迹和当前轨迹的时间信息来补充*TRAD*，这些信息在分布外决策中作为有用的上下文，但在分布内决策中可能变得不那么有用。
- en: 4.4.2\. The Effect of Expansion Steps $B$
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 扩展步骤 $B$ 的影响
- en: 'Next we vary a critical hyper-parameter, the number of temporal expansion steps,
    and investigate how the overall performance will change accordingly. To avoid
    an expensive grid search on $B$. The results over all 3 subsets are shown in Fig. [3](#S4.F3
    "Figure 3 ‣ 4.4.2\. The Effect of Expansion Steps 𝐵 and 𝐹 ‣ 4.4\. Ablation Studies
    ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval
    and Aligned Decision").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们将变化一个关键超参数，即时间扩展步骤的数量，并调查整体表现如何相应变化。为了避免对$B$进行代价高昂的网格搜索，所有3个子集的结果如图[3](#S4.F3
    "Figure 3 ‣ 4.4.2\. The Effect of Expansion Steps 𝐵 and 𝐹 ‣ 4.4\. Ablation Studies
    ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval
    and Aligned Decision")所示。'
- en: '![Refer to caption](img/e83119ffe6dffcfdbb14edf246be9aac.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e83119ffe6dffcfdbb14edf246be9aac.png)'
- en: (a) Varying $F$
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 变化$F$
- en: '![Refer to caption](img/6c2bf2b5b49ea6d6726fe38c46a83e77.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6c2bf2b5b49ea6d6726fe38c46a83e77.png)'
- en: (b) Varying $B$
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 变化$B$
- en: Figure 3. The effect of varying subsequent steps $F$ does not help more when
    they are sufficiently large.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 当后续步骤$F$足够大时，变化对性能的帮助不大。
- en: 'From Fig. [3](#S4.F3 "Figure 3 ‣ 4.4.2\. The Effect of Expansion Steps 𝐵 and
    𝐹 ‣ 4.4\. Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with
    Step-Wise Thought Retrieval and Aligned Decision"), we can have the following
    observations:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '从图[3](#S4.F3 "Figure 3 ‣ 4.4.2\. The Effect of Expansion Steps 𝐵 and 𝐹 ‣ 4.4\.
    Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision")中，我们可以得到以下观察结果：'
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Both forward expansion ().
    This justifies our design of *aligned decision*.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无论是前向扩展（）。这证明了我们对*对齐决策*的设计合理性。
- en: •
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Either forward expansion or backward expansion does not benefit from increasing
    a large enough $F$ further. This proves our hypothesis that irrelevant context
    too far from the current state is of little value and even noisy.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前向扩展或后向扩展的效果都不会因为进一步增加足够大的$F$而受益。这验证了我们的假设，即与当前状态相距过远的无关上下文价值不大，甚至会造成噪声。
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generally, forward expansion performs better than backward expansion when varying
    $F$. The reason for this phenomenon might be that historical information has been
    incorporated in thoughts and thus future information helps more.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，当$F$变化时，前向扩展比回溯扩展表现更好。这一现象的原因可能是历史信息已被纳入思维中，因此未来的信息帮助更大。
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*TRAD* achieves its best performance when $F=2$, and consistently outperforms
    *Synapse* with forward expansion.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*TRAD*在$F=2$时表现最佳，并且在前向扩展时始终优于*Synapse*。'
- en: 4.4.3\. The Effect of Demonstration Amount $K$
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 演示数量$K$的影响
- en: Finally, we look into a common yet important hyper-parameter, the number of
    retrieved demonstrations $K$, and thus we omit this result.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们研究了一个常见而重要的超参数——检索演示的数量$K$，因此我们省略了这一结果。
- en: '![Refer to caption](img/90a146078d7d5b7bd9a68256723a8e53.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/90a146078d7d5b7bd9a68256723a8e53.png)'
- en: Figure 4. The effect of varying the number of retrieved demonstrations $K$ varies.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图4. 不同检索演示数量$K$的影响变化。
- en: 'From Fig. [4](#S4.F4 "Figure 4 ‣ 4.4.3\. The Effect of Demonstration Amount
    𝐾 ‣ 4.4\. Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with
    Step-Wise Thought Retrieval and Aligned Decision"), we see that $K$.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从图[4](#S4.F4 "图 4 ‣ 4.4.3\. 演示数量 𝐾 的影响 ‣ 4.4\. 消融研究 ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策提升LLM代理")中，我们可以看到$K$。
- en: 'With results in Section [4.4.2](#S4.SS4.SSS2 "4.4.2\. The Effect of Expansion
    Steps 𝐵 and 𝐹 ‣ 4.4\. Ablation Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM
    Agents with Step-Wise Thought Retrieval and Aligned Decision") and Section [4.4.3](#S4.SS4.SSS3
    "4.4.3\. The Effect of Demonstration Amount 𝐾 ‣ 4.4\. Ablation Studies ‣ 4\. Experiments
    ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision"),
    we now respond to our last research question.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 根据第[4.4.2](#S4.SS4.SSS2 "4.4.2\. 扩展步骤 𝐵 和 𝐹 的影响 ‣ 4.4\. 消融研究 ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策提升LLM代理")节和第[4.4.3](#S4.SS4.SSS3
    "4.4.3\. 演示数量 𝐾 的影响 ‣ 4.4\. 消融研究 ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策提升LLM代理")节的结果，我们现在回应我们的最后一个研究问题。
- en: 'Answer to RQ5: The performance and advantage of *TRAD* generally remains stable
    with different hyper-parameter choices, i.e., temporal expansion steps, number
    of retrieved demonstrations. Its performance and advantage only degrade when using
    long backward extension, which is possibly due to the fact that historical information
    has already been incorporated in thoughts and does not provide further help for
    decision-making.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 对RQ5的回答：*TRAD*的性能和优势在不同的超参数选择下通常保持稳定，即时间扩展步骤、检索演示的数量。其性能和优势只有在使用长时间的回溯扩展时才会下降，这可能是由于历史信息已经被纳入思考中，并且不再对决策提供进一步帮助。
- en: 4.5\. Case Studies
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 案例研究
- en: 'At the end of this section, we present some representative trajectories or
    steps, where we can intuitively learn the advantages of *TRAD*. We show two cases
    produced by *Synapse* and our *TRAD* agent on the cross-domain subset of Mind2Web
    in Fig. [5](#S4.F5 "Figure 5 ‣ 4.5\. Case Studies ‣ 4\. Experiments ‣ TRAD: Enhancing
    LLM Agents with Step-Wise Thought Retrieval and Aligned Decision"), to demonstrate:
    1) the difference between task meta-data retrieval and *thought retrieval*; 2)
    the reason for retrieval rather than direct prediction with thought and the tolerance
    for imperfect thoughts.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后，我们展示了一些代表性的轨迹或步骤，从中可以直观地了解到*TRAD*的优势。我们在图[5](#S4.F5 "图 5 ‣ 4.5\. 案例研究
    ‣ 4\. 实验 ‣ TRAD：通过逐步思维检索和对齐决策提升LLM代理")中展示了由*Synapse*和我们的*TRAD*代理生成的两个案例，展示：1)
    任务元数据检索和*思维检索*之间的区别；2) 选择检索而非直接预测思维的原因及对不完美思维的容忍度。
- en: 'In Fig. [5(a)](#S4.F5.sf1 "In Figure 5 ‣ 4.5\. Case Studies ‣ 4\. Experiments
    ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision"),
    the trajectory-wise retrieval of *Synapse* is obviously problematic, which only
    considers “search” in task instructions and the retrieved trajectories are completely
    irrelevant to the current one. However, when we use these irrelevant demonstrations
    for thought production and conduct *thought retrieval* afterwards, the retrieved
    demonstrations become much more relevant as they all relate to baby (toddler)
    and reflect the process of interacting with navigation links or buttons to unfold
    invisible web pages during web browsing. With the demonstrations from *thought
    retrieval*, *TRAD* is capable of making the correct decision.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [5(a)](#S4.F5.sf1 "在图 5 ‣ 4.5\. 案例研究 ‣ 4\. 实验 ‣ TRAD: 通过逐步思维检索和对齐决策来增强 LLM
    代理") 中，*Synapse* 的轨迹检索显然存在问题，它只考虑任务指令中的“搜索”，检索到的轨迹与当前轨迹完全无关。然而，当我们使用这些无关的示例进行思维生成并随后进行*思维检索*时，检索到的示例变得更加相关，因为它们都涉及到婴儿（幼儿），并反映了在浏览网页过程中与导航链接或按钮的交互过程。通过*思维检索*得到的示例，*TRAD*
    能够做出正确的决策。'
- en: 'In Fig. [5(b)](#S4.F5.sf2 "In Figure 5 ‣ 4.5\. Case Studies ‣ 4\. Experiments
    ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision"),
    both *Synapse* and *TRAD* seem to retrieve relevant examples trying to find something
    in New York, but if we examine the trajectories retrieved by task meta-data, 2/3
    of them fulfill the condition “New York” by clicking some link or button rather
    than typing in a text box. Unfortunately, the correct action under the current
    state is typing, not clicking, and thus *Synapse* fails to type the correct content.
    On the contrary, *TRAD* learns to type the correct content “New York” into the
    text box, even if its thought is incorrect. This also validates our hypothesis
    that using thought for retrieval instead of prediction helps to correct imperfect
    thoughts.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [5(b)](#S4.F5.sf2 "在图 5 ‣ 4.5\. 案例研究 ‣ 4\. 实验 ‣ TRAD: 通过逐步思维检索和对齐决策来增强 LLM
    代理") 中，*Synapse* 和 *TRAD* 看起来都检索到了相关的示例，试图在纽约找到一些东西，但如果我们检查任务元数据检索到的轨迹，2/3 的轨迹是通过点击某些链接或按钮来满足“New
    York”这一条件，而不是在文本框中输入。不幸的是，在当前状态下正确的操作是输入，而不是点击，因此 *Synapse* 未能输入正确的内容。相反，*TRAD*
    即使其思维不正确，也学会了在文本框中输入正确的内容“New York”。这也验证了我们的假设，即使用思维进行检索而不是预测有助于纠正不完美的思维。'
- en: '![Refer to caption](img/4e2ac55c38474f72e36fab57bf521e7c.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e2ac55c38474f72e36fab57bf521e7c.png)'
- en: (a) Representative Case 1
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 代表性案例 1
- en: '![Refer to caption](img/fd47f1845572ef6e267e3630276c5a85.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd47f1845572ef6e267e3630276c5a85.png)'
- en: (b) Representative Case 2
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 代表性案例 2
- en: Figure 5. Comparison between Synapse trajectory-wise retrieval with task meta-data
    and TRAD step-wise retrieval with thought. (a) The trajectory-wise retrieval of
    *Synapse* only considers “search” in task instructions and the retrieved trajectories
    are completely irrelevant. However, by generating thoughts with these irrelevant
    trajectories, *thought retrieval* finds more relevant step-wise demonstrations
    related to baby (toddler) and navigation. (b) The trajectory-wise retrieval of
    *Synapse* retrieves plausible examples which do not type in a text box with task
    meta-data. Although thoughts are imperfect, *thought retrieval* finds more relevant
    demonstrations and *TRAD* learns to input “New York”.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. *Synapse* 轨迹检索与任务元数据的比较，以及 TRAD 逐步检索与思维的比较。 (a) *Synapse* 的轨迹检索只考虑任务指令中的“搜索”，检索到的轨迹完全无关。然而，通过生成思维来处理这些无关的轨迹，*思维检索*
    找到了更多相关的逐步示例，涉及到婴儿（幼儿）和导航。(b) *Synapse* 的轨迹检索检索到了一些看似合理的示例，这些示例没有在文本框中输入，而是使用了任务元数据。尽管思维不完美，*思维检索*
    仍然找到更多相关的示例，*TRAD* 学会了在文本框中输入“New York”。
- en: 5\. Real-World Deployment of TRAD
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. TRAD 的现实世界部署
- en: Since Dec. 2023, we have deployed our *TRAD* agent to automate some real-world
    office tasks in a mainstream insurance company, which owns a global business with
    approximately 170 million customers worldwide. We select 4 different websites
    and collect 100 expert trajectories for some representative tasks on each website
    as our memory. For evaluation, we collect 20 unseen tasks on each website, using
    step success rate (Step SR) and trajectory success rate (SR) as evaluation metrics.
    Tasks involve filling in insurance inquiry forms, implementing advanced information
    retrieval, etc. Since the websites are complex and contain thousands of web elements,
    prompting with complete trajectories is not available, hence we only consider
    single-step prompting with historical actions as auxiliary information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 自2023年12月起，我们已将*TRAD*代理部署到一家主流保险公司，以自动化一些现实世界的办公任务，该公司拥有约1.7亿全球客户。我们选择了4个不同的网站，并收集了每个网站上代表性任务的100条专家轨迹作为我们的记忆。为了评估，我们在每个网站上收集了20个未见过的任务，使用步骤成功率（Step
    SR）和轨迹成功率（SR）作为评估指标。任务涉及填写保险查询表单、实施高级信息检索等。由于网站复杂且包含成千上万的网页元素，无法提供完整轨迹的提示，因此我们仅考虑单步提示与历史操作作为辅助信息。
- en: 'To verify the effectiveness of *TRAD*, we use two different *ReAct* agents
    that the company has attempted as our baseline:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证*TRAD*的有效性，我们使用公司尝试过的两种不同的*ReAct*代理作为我们的基准：
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ReAct*-RD: randomly selects expert steps in random trajectories as demonstrations.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ReAct*-RD：随机选择在随机轨迹中的专家步骤作为演示。'
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*ReAct*-RV: randomly selects expert steps in relevant trajectories retrieved
    by task instruction as demonstrations.'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*ReAct*-RV：随机选择在任务指令检索到的相关轨迹中的专家步骤作为演示。'
- en: 'To be specific, the difference between *TRAD* and *ReAct*-RV is using thought
    for a second-time step retrieval and the aligned decision module. To further investigate
    the effect of *thought retrieval* and *aligned decision*, we also deploy a TR
    agent which removes our *aligned decision* method, namely the *TRAD* w/o TE baseline
    in Tab. [3](#S4.T3 "Table 3 ‣ 4.4.1\. The Effect of Aligned Decision ‣ 4.4\. Ablation
    Studies ‣ 4\. Experiments ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought
    Retrieval and Aligned Decision"). We list the results in Tab. [4](#S5.T4 "Table
    4 ‣ 5\. Real-World Deployment of TRAD ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，*TRAD*与*ReAct*-RV之间的区别在于使用思考进行第二次步骤检索和对齐决策模块。为了进一步研究*思考检索*和*对齐决策*的效果，我们还部署了一个TR代理，去除了我们的*对齐决策*方法，即Tab. [3](#S4.T3
    "表 3 ‣ 4.4.1\. 对齐决策的效果 ‣ 4.4\. 消融研究 ‣ 4\. 实验 ‣ TRAD：通过逐步思考检索和对齐决策增强LLM代理")中的*TRAD*
    w/o TE基准。我们在Tab. [4](#S5.T4 "表 4 ‣ 5\. TRAD的真实世界部署 ‣ TRAD：通过逐步思考检索和对齐决策增强LLM代理")中列出了结果。
- en: Table 4. Evaluation results on real-world websites from a mainstream global
    business insurance company.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4. 来自主流全球商业保险公司的真实网站上的评估结果。
- en: '| Method | ReAct-RD | ReAct-RV | TR | TRAD (Ours) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ReAct-RD | ReAct-RV | TR | TRAD（我们的方法） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Website 1 | Step SR | 0.843 | 0.826 | 0.941 | 0.950 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 网站 1 | 步骤 SR | 0.843 | 0.826 | 0.941 | 0.950 |'
- en: '| (form filling) | SR | 0.500 | 0.450 | 0.800 | 0.800 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| (表单填写) | SR | 0.500 | 0.450 | 0.800 | 0.800 |'
- en: '| Website 2 | Step SR | 0.941 | 0.937 | 0.958 | 0.974 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 网站 2 | 步骤 SR | 0.941 | 0.937 | 0.958 | 0.974 |'
- en: '| (advanced IR) | SR | 0.900 | 0.850 | 0.850 | 0.900 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (高级信息检索) | SR | 0.900 | 0.850 | 0.850 | 0.900 |'
- en: '| Website 3 | Step SR | 0.962 | 0.987 | 1.000 | 1.000 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 网站 3 | 步骤 SR | 0.962 | 0.987 | 1.000 | 1.000 |'
- en: '| (advanced IR) | SR | 0.850 | 0.800 | 0.850 | 1.000 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| (高级信息检索) | SR | 0.850 | 0.800 | 0.850 | 1.000 |'
- en: '| Website 4 | Step SR | 0.820 | 0.860 | 0.845 | 1.000 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 网站 4 | 步骤 SR | 0.820 | 0.860 | 0.845 | 1.000 |'
- en: '| (form filling) | SR | 0.350 | 0.350 | 0.400 | 1.000 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| (表单填写) | SR | 0.350 | 0.350 | 0.400 | 1.000 |'
- en: '| Average | Step SR | 0.891 | 0.902 | 0.936 | 0.981 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 步骤 SR | 0.891 | 0.902 | 0.936 | 0.981 |'
- en: '| SR | 0.650 | 0.613 | 0.725 | 0.925 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| SR | 0.650 | 0.613 | 0.725 | 0.925 |'
- en: 'As can be seen in Tab. [4](#S5.T4 "Table 4 ‣ 5\. Real-World Deployment of TRAD
    ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision"),
    *TRAD* achieves the best performance on all 4 websites, showing its advantage
    can remain when deployed to real-world scenarios. Moreover, we observe that *TRAD*
    w/o TE baseline also outperforms both *ReAct* agents, but exhibits noticeable
    disadvantages compared to the complete *TRAD* agents. This justifies our design
    of both *thought retrieval* and *aligned decision*.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [4](#S5.T4 "Table 4 ‣ 5\. Real-World Deployment of TRAD ‣ TRAD: Enhancing
    LLM Agents with Step-Wise Thought Retrieval and Aligned Decision") 所示，*TRAD* 在所有
    4 个网站上都表现最佳，这显示了它在部署到现实世界场景中的优势。此外，我们观察到，*TRAD* 在没有 TE 基线的情况下也优于两个 *ReAct* 代理，但与完整的
    *TRAD* 代理相比，表现出明显的劣势。这证明了我们设计中的 *思维检索* 和 *对齐决策* 的必要性。'
- en: Inference efficiency of *TRAD*. At inference time, our *TRAD* agent only introduces
    little extra time consumption in *thought retrieval* compared to *ReAct*. We profile
    the inference process of *TRAD* and *ReAct* on all websites and tasks, and in
    average *TRAD* takes only 11.7% more time than *ReAct*-RD, which indicates that
    our method achieves improvement without much sacrifice on efficiency.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*TRAD* 的推理效率。在推理时，与 *ReAct* 相比，我们的 *TRAD* 代理仅引入了少量额外的 *思维检索* 时间消耗。我们对 *TRAD*
    和 *ReAct* 在所有网站和任务上的推理过程进行了分析，平均而言，*TRAD* 仅比 *ReAct*-RD 多花费 11.7% 的时间，这表明我们的方法在没有大幅牺牲效率的情况下实现了改进。'
- en: 6\. Discussions
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. 讨论
- en: 6.1\. Limitations of TRAD
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1. *TRAD* 的局限性
- en: Although *TRAD* exhibits excellent performances over a diverse set of tasks,
    it still has limitations like dependence on high-quality thought and trade-off
    between information and noise in *temporal expansion*, and we briefly discuss
    about them here.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 *TRAD* 在各种任务上表现出色，但它仍然存在一些局限性，比如对高质量思维的依赖和 *时间扩展* 中信息与噪声的权衡，我们在此简要讨论。
- en: 6.1.1\. Dependence on high-quality thought.
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1. 对高质量思维的依赖。
- en: '*TRAD* alleviates the issue of imperfect thoughts by its *aligned decision*
    module, but its capability still depends heavily on the quality of thoughts and
    the capability of backbone LLM. To make such a step-wise retrieval-augmented method
    work well, the abstraction of current state is critical since it serves as the
    query and key for retrieval, hence the LLM used to build a *TRAD* agent should
    at least have a decent understanding of the task.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*TRAD* 通过其 *对齐决策* 模块缓解了不完善思维的问题，但其能力仍然严重依赖于思维的质量和基础 LLM 的能力。要使这种逐步检索增强的方法有效，当前状态的抽象至关重要，因为它作为检索的查询和关键字，因此用于构建
    *TRAD* 代理的 LLM 至少应对任务有一定的理解。'
- en: 6.1.2\. Trade-off in temporal expansion.
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2. 时间扩展中的权衡。
- en: '*TRAD* expects to keep relevant information but reduce irrelevant input context
    by step-wise *thought retrieval*, while preserving some chance for correcting
    imperfect thoughts by *temporal expansion*. Here exists a trade-off: a longer
    *temporal expansion* brings not only more tolerance to imperfect thoughts, but
    also more irrelevant noise in demonstrations. This trade-off requires careful
    consideration for different tasks.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*TRAD* 期望通过逐步的 *思维检索* 保留相关信息，同时减少不相关的输入上下文，并保留一些机会以通过 *时间扩展* 纠正不完善的思维。这里存在一个权衡：更长的
    *时间扩展* 不仅带来对不完善思维的更大容忍度，还带来更多示范中的无关噪声。这个权衡需要对不同任务进行仔细考虑。'
- en: 6.2\. Future Directions
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 未来方向
- en: While ablation studies have been conducted to justify our design of *TRAD*,
    there are some promising ideas worth study which can probably improve *TRAD* further.
    We leave them as future works, and discuss them as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已经进行了消融研究以证明我们设计的 *TRAD* 的合理性，但仍有一些值得研究的有前景的想法，这些想法可能会进一步改进 *TRAD*。我们将这些想法留作未来工作，并如下讨论。
- en: 6.2.1\. Better Demonstrations For Reasoning
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 更好的推理示范
- en: '*TRAD* currently employs relevant trajectories or randomly-chosen steps from
    them as demonstrations to generate thoughts, which still suffers from the issues
    discussed in Section [1](#S1 "1\. Introduction ‣ TRAD: Enhancing LLM Agents with
    Step-Wise Thought Retrieval and Aligned Decision") to some extent. Therefore,
    modifications can be made to generate thoughts of higher quality, and thus improve
    the overall performance of *TRAD*.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*TRAD* 目前使用相关轨迹或从中随机选择的步骤作为示范来生成思维，这在一定程度上仍然存在第 [1](#S1 "1\. Introduction ‣
    TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision")
    节中讨论的问题。因此，可以进行修改以生成更高质量的思维，从而提高 *TRAD* 的整体性能。'
- en: 6.2.2\. Better Representations For Retrieval
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2. 更好的检索表示
- en: 'As we have discussed in Section [2.3](#S2.SS3 "2.3\. LLM Planning and Reasoning
    ‣ 2\. Related Work ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval
    and Aligned Decision"), *TRAD* can utilize any other methods to obtain a comprehensive
    abstraction of the current state in a sequential decision-making task, which can
    possibly serve as better queries and keys for the step-wise demonstration retrieval.
    Therefore, *TRAD* can be combined with more powerful LLM planning and reasoning
    methods and even dense abstractions produced by LLMs pre-trained on domain-specific
    data like (Gur et al., [2024](#bib.bib9)).'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们在第[2.3节](#S2.SS3 "2.3\. LLM Planning and Reasoning ‣ 2\. Related Work ‣
    TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision")中讨论的，*TRAD*可以利用任何其他方法来获取当前状态的全面抽象，这可能作为更好的查询和键用于逐步演示检索。因此，*TRAD*可以与更强大的LLM规划和推理方法结合，甚至与在特定领域数据上预训练的LLM产生的密集抽象结合（如Gur等，
    [2024](#bib.bib9)）。'
- en: 7\. Conclusions
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: In this work, we propose a novel LLM agent augmented by step-wise demonstration
    retrieval (*TRAD*) for sequential decision-making tasks. *TRAD* first retrieves
    relevant step demonstrations by its thought about current state, and then complements
    temporally correlated steps for more informative action prediction. Extensive
    experiments are conducted on two different sequential decision-making tasks to
    validate the effectiveness of our solution, and thorough ablation studies justify
    the design choice and stability of our method. We further present the results
    from real-world deployment of our method, showing its value in real-world applications.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的LLM代理，增强了逐步演示检索（*TRAD*）以应对顺序决策任务。*TRAD*首先通过其对当前状态的思考检索相关的步骤演示，然后补充时间相关的步骤以提高行动预测的准确性。我们在两个不同的顺序决策任务上进行了广泛实验，以验证我们解决方案的有效性，全面的消融研究证明了我们方法的设计选择和稳定性。我们进一步展示了我们方法在实际应用中的结果，展示了其在现实世界中的价值。
- en: Acknowledgements.
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: The Shanghai Jiao Tong University team is partially supported by Shanghai Municipal
    Science and Technology Major Project (2021SHZDZX0102) and National Natural Science
    Foundation of China (62322603, 62076161).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上海交通大学团队部分由上海市科技重大专项（2021SHZDZX0102）和国家自然科学基金（62322603，62076161）资助。
- en: References
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: Aeronautiques et al. (1998) Constructions Aeronautiques, Adele Howe, Craig Knoblock,
    ISI Drew McDermott, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins SRI,
    Anthony Barrett, Dave Christianson, et al. 1998. Pddl— the planning domain definition
    language. *Technical Report* (1998).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aeronautiques 等人（1998）Constructions Aeronautiques、Adele Howe、Craig Knoblock、ISI
    Drew McDermott、Ashwin Ram、Manuela Veloso、Daniel Weld、David Wilkins SRI、Anthony
    Barrett、Dave Christianson 等。1998年。《Pddl——规划领域定义语言》。*技术报告*（1998年）。
- en: 'Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, and Torsten Hoefler. 2023. Graph of Thoughts: Solving Elaborate
    Problems with Large Language Models. *arXiv preprint arXiv:2308.09687* (2023).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等人（2023）Maciej Besta、Nils Blach、Ales Kubicek、Robert Gerstenberger、Lukas
    Gianinazzi、Joanna Gajda、Tomasz Lehmann、Michal Podstawski、Hubert Niewiadomski、Piotr
    Nyczyk 和 Torsten Hoefler。2023年。《思想图谱：利用大语言模型解决复杂问题》。*arXiv 预印本 arXiv:2308.09687*（2023年）。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language Models are Few-Shot Learners. In *Proceedings of the 34th
    Advances in Neural Information Processing Systems (NeurIPS)*.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell、Sandhini
    Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon Child、Aditya Ramesh、Daniel
    M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark Chen、Eric Sigler、Mateusz
    Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher Berner、Sam McCandlish、Alec
    Radford、Ilya Sutskever 和 Dario Amodei。2020年。《语言模型是少样本学习者》。载于 *第34届神经信息处理系统大会（NeurIPS）论文集*。
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2Web: Towards a Generalist Agent for
    the Web. In *Proceedings of the 37th Advances in Neural Information Processing
    Systems (NeurIPS)*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, 和 Yu Su。2023。Mind2Web: 朝着通用 Web 代理的方向发展。在 *第37届神经信息处理系统进展会议
    (NeurIPS)*。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova。2018。Bert:
    深度双向变换器的语言理解预训练。*arXiv 预印本 arXiv:1810.04805* (2018)。'
- en: 'Ding et al. (2023) Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma,
    Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023. Everything
    of thoughts: Defying the law of penrose triangle for thought generation. *arXiv
    preprint arXiv:2311.04254* (2023).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma,
    Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, 和 Dongmei Zhang。2023。思想的一切：颠覆潘罗斯三角形的思想生成法则。*arXiv
    预印本 arXiv:2311.04254* (2023)。
- en: 'Ferraretto et al. (2023) Fernando Ferraretto, Thiago Laitz, Roberto Lotufo,
    and Rodrigo Nogueira. 2023. ExaRanker: Synthetic Explanations Improve Neural Rankers.
    In *Proceedings of the 46th International ACM SIGIR Conference on Research and
    Development in Information Retrieval (SIGIR)*. 2409––2414.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ferraretto et al. (2023) Fernando Ferraretto, Thiago Laitz, Roberto Lotufo,
    和 Rodrigo Nogueira。2023。ExaRanker: 合成解释改进神经排序器。在 *第46届国际 ACM SIGIR 信息检索研究与开发会议
    (SIGIR)*。2409–2414。'
- en: Gur et al. (2024) Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. A Real-World WebAgent
    with Planning, Long Context Understanding, and Program Synthesis. In *Proceedings
    of The 12th International Conference on Learning Representations (ICLR)*.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur et al. (2024) Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, 和 Aleksandra Faust。2024。具有规划、长上下文理解和程序合成的现实世界 WebAgent。在
    *第12届国际学习表征会议 (ICLR)*。
- en: Gur et al. (2023) Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari,
    Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra
    Faust. 2023. Understanding HTML with Large Language Models. In *Findings of the
    Association for Computational Linguistics (EMNLP)*. 2803–2821.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur et al. (2023) Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari,
    Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, 和 Aleksandra Faust。2023。理解
    HTML 与大型语言模型。在 *计算语言学协会发现 (EMNLP)*。2803–2821。
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with Language Model is Planning
    with World Model. In *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*. 8154–8173.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, 和 Zhiting Hu。2023。与语言模型推理即用世界模型进行规划。在 *2023年自然语言处理实证方法会议 (EMNLP)*。8154–8173。
- en: Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin
    Choi. 2020. The Curious Case of Neural Text Degeneration. In *Proceedings of the
    8th International Conference on Learning Representations (ICLR)*.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtzman et al. (2020) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, 和 Yejin
    Choi。2020。神经文本退化的奇异案例。在 *第8届国际学习表征会议 (ICLR)*。
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense
    Passage Retrieval for Open-Domain Question Answering. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*. 6769–6781.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, 和 Wen-tau Yih。2020。开放领域问答的密集段落检索。在
    *2020年自然语言处理实证方法会议 (EMNLP)*。6769–6781。
- en: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language
    Models can Solve Computer Tasks. In *Proceedings of the 37th Advances in Neural
    Information Processing Systems (NeurIPS)*.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, 和 Stephen McAleer。2023。语言模型可以解决计算机任务。在
    *第37届神经信息处理系统进展会议 (NeurIPS)*。
- en: 'Liang et al. (2023) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, and Andy Zeng. 2023. Code as Policies: Language Model
    Programs for Embodied Control. In *Proceedings of 2023 IEEE International Conference
    on Robotics and Automation (ICRA)*. 9493–9500.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2023) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, 和 Andy Zeng。2023。代码作为策略：用于具身控制的语言模型程序。在 *2023年IEEE国际机器人与自动化会议
    (ICRA)*。9493–9500。
- en: 'Liu et al. (2023) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023. LLM+P: Empowering large language models
    with optimal planning proficiency. *arXiv preprint arXiv:2304.11477* (2023).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023）刘博、姜玉倩、张晓涵、刘强、张世奇、乔伊迪普·比斯瓦斯、彼得·斯通。2023年。《LLM+P：赋能大型语言模型以最优规划能力》。*arXiv预印本
    arXiv:2304.11477*（2023年）。
- en: Liu et al. (2021) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? *arXiv
    preprint arXiv:2101.06804* (2021).
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2021）刘家常、沈定汉、张一哲、比尔·多兰、劳伦斯·卡林、陈伟柱。2021年。《GPT-3的优质上下文示例是什么？》*arXiv预印本 arXiv:2101.06804*（2021年）。
- en: Mackie et al. (2023) Iain Mackie, Shubham Chatterjee, and Jeffrey Dalton. 2023.
    Generative Relevance Feedback with Large Language Models. In *Proceedings of the
    46th International ACM SIGIR Conference on Research and Development in Information
    Retrieval (SIGIR)*. 2026–2031.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 麦基等（2023）伊恩·麦基、舒布汉·查特吉、杰弗里·道尔顿。2023年。《使用大型语言模型的生成相关反馈》。在*第46届国际ACM SIGIR信息检索研究与开发会议（SIGIR）论文集*中。2026–2031。
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with
    human feedback. *arXiv preprint arXiv:2112.09332* (2021).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中野等（2021）中野零一郎、雅各布·希尔顿、苏奇尔·巴拉吉、杰夫·吴、龙欧阳、克里斯蒂娜·金、克里斯托弗·赫斯、尚塔努·贾因、维尼特·科萨拉朱、威廉·桑德斯等。2021年。《Webgpt：浏览器辅助的人类反馈问答》。*arXiv预印本
    arXiv:2112.09332*（2021年）。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. *arXiv preprint arXiv:2303.08774*
    (2023).
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。《GPT-4技术报告》。*arXiv预印本 arXiv:2303.08774*（2023年）。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    In *Proceedings of the 36th Advances in Neural Information Processing Systems
    (NeurIPS)*. 27730–27744.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等（2022）欧阳龙、杰弗里·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·温赖特、帕梅拉·米什金、张冲、桑迪尼·阿加瓦尔、卡塔里娜·斯拉马、亚历克斯·雷等。2022年。《训练语言模型以遵循人类反馈的指令》。在*第36届神经信息处理系统进展会议（NeurIPS）论文集*中。27730–27744。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th Annual ACM Symposium
    on User Interface Software and Technology (UIST)*. 1–22.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴等（2023）朴俊成、约瑟夫·奥布莱恩、凯瑞·君·蔡、梅雷迪思·林格尔·莫里斯、佩西·梁、迈克尔·S·伯恩斯坦。2023年。《生成代理：人类行为的互动模拟体》。在*第36届ACM用户界面软件与技术年会（UIST）论文集*中。1–22。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
    *OpenAI Blog* (2019).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福等（2019）亚历克·拉德福、杰弗里·吴、雷温·查尔德、大卫·陆安、达里奥·阿莫代伊、伊利亚·苏茨克维尔。2019年。《语言模型是无监督的多任务学习者》。*OpenAI博客*（2019年）。
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT:
    Sentence Embeddings using Siamese BERT-Networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. 3980–3990.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 莱默斯和古雷维奇（2019）尼尔斯·莱默斯和伊琳娜·古雷维奇。2019年。《Sentence-BERT：使用Siamese BERT网络的句子嵌入》。在*2019年自然语言处理经验方法会议和第九届国际自然语言处理联合会议（EMNLP-IJCNLP）论文集*中。3980–3990。
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 罗齐耶等（2023）巴普蒂斯特·罗齐耶、乔纳斯·盖林、法比安·格洛克、斯腾·索特拉、伊泰·加特、肖青·艾伦·谭、约西·阿迪、刘静宇、塔尔·雷梅兹、杰雷米·拉潘等。2023年。《代码驼鹿：开放的代码基础模型》。*arXiv预印本
    arXiv:2308.12950*（2023年）。
- en: 'Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    Learning To Retrieve Prompts for In-Context Learning. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT)*. 2655–2671.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲁宾等（2022）奥哈德·鲁宾、乔纳森·赫尔齐格、乔纳森·贝兰特。2022年。《学习检索上下文学习的提示》。在*2022年北美计算语言学协会人类语言技术会议（NAACL-HLT）论文集*中。2655–2671。
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer:
    Language models can teach themselves to use tools. In *Proceedings of the 37th
    Advances in Neural Information Processing Systems (NeurIPS)*.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick等（2023）Timo Schick、Jane Dwivedi-Yu、Roberto Dessì、Roberta Raileanu、Maria
    Lomeli、Luke Zettlemoyer、Nicola Cancedda 和 Thomas Scialom。2023年。《Toolformer: 语言模型可以自我学习使用工具》。发表于*第37届神经信息处理系统会议（NeurIPS）论文集*。'
- en: 'Shi et al. (2017) Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez,
    and Percy Liang. 2017. World of Bits: An Open-Domain Platform for Web-Based Agents.
    In *Proceedings of the 34th International Conference on Machine Learning (ICML)*,
    Vol. 70\. 3135–3144.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi等（2017）Tianlin Shi、Andrej Karpathy、Linxi Fan、Jonathan Hernandez 和 Percy
    Liang。2017年。《World of Bits: 一个用于基于网络代理的开放域平台》。发表于*第34届国际机器学习会议（ICML）论文集*，第70卷，第3135–3144页。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement
    learning. In *Proceedings of the 37th Advances in Neural Information Processing
    Systems (NeurIPS)*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn等（2023）Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik R Narasimhan
    和 Shunyu Yao。2023年。《Reflexion: 具有语言强化学习的语言代理》。发表于*第37届神经信息处理系统会议（NeurIPS）论文集*。'
- en: 'Shridhar et al. (2020) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. ALFRED:
    A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In *Proceedings
    of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*.
    10737–10746.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar等（2020）Mohit Shridhar、Jesse Thomason、Daniel Gordon、Yonatan Bisk、Winson
    Han、Roozbeh Mottaghi、Luke Zettlemoyer 和 Dieter Fox。2020年。《ALFRED: 一个用于解释日常任务中基础指令的基准》。发表于*2020
    IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集*，第10737–10746页。'
- en: 'Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew J. Hausknecht. 2021. ALFWorld: Aligning Text
    and Embodied Environments for Interactive Learning. In *Proceedings of 9th International
    Conference on Learning Representations (ICLR)*.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar等（2021）Mohit Shridhar、Xingdi Yuan、Marc-Alexandre Côté、Yonatan Bisk、Adam
    Trischler 和 Matthew J. Hausknecht。2021年。《ALFWorld: Aligning Text and Embodied
    Environments for Interactive Learning》。发表于*第九届国际学习表征会议（ICLR）论文集*。'
- en: Team (2023) The LongChat Team. 2023. *How Long Can Open-Source LLMs Truly Promise
    on Context Length?* [https://lmsys.org/blog/2023-06-29-longchat/](https://lmsys.org/blog/2023-06-29-longchat/)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team（2023）LongChat团队。2023年。*开放源代码LLMs在上下文长度上能真正承诺多长时间？* [https://lmsys.org/blog/2023-06-29-longchat/](https://lmsys.org/blog/2023-06-29-longchat/)
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等（2023）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurelien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample。2023年。《LLaMA: Open and
    Efficient Foundation Language Models》。*arXiv预印本 arXiv:2302.13971*（2023年）。'
- en: Trivedi et al. (2023) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
    and Ashish Sabharwal. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning
    for Knowledge-Intensive Multi-Step Questions. In *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (ACL)*. 10014–10037.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trivedi等（2023）Harsh Trivedi、Niranjan Balasubramanian、Tushar Khot 和 Ashish Sabharwal。2023年。《交替检索与链式思维推理用于知识密集型多步骤问题》。发表于*第61届计算语言学协会年会（ACL）论文集*，第10014–10037页。
- en: 'Wang et al. (2023d) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023d. Voyager: An open-ended
    embodied agent with large language models. *arXiv preprint arXiv:2305.16291* (2023).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等（2023d）Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke
    Zhu、Linxi Fan 和 Anima Anandkumar。2023d年。《Voyager: 一个与大型语言模型相结合的开放式具身代理》。*arXiv预印本
    arXiv:2305.16291*（2023年）。'
- en: Wang et al. (2023b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023b. A survey
    on large language model based autonomous agents. *arXiv preprint arXiv:2308.11432*
    (2023).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2023b）Lei Wang、Chen Ma、Xueyang Feng、Zeyu Zhang、Hao Yang、Jingsen Zhang、Zhiyuan
    Chen、Jiakai Tang、Xu Chen、Yankai Lin 等。2023b年。《基于大型语言模型的自主代理调查》。*arXiv预印本 arXiv:2308.11432*（2023年）。
- en: Wang et al. (2023c) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023c. Self-Consistency
    Improves Chain of Thought Reasoning in Language Models. In *The 11th International
    Conference on Learning Representations, (ICLR)*.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023c) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed
    H. Chi, Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023c. 自一致性提高语言模型中的思维链推理。见于
    *第11届国际学习表示大会（ICLR）*。
- en: 'Wang et al. (2023a) Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao
    Liang. 2023a. Describe, explain, plan and select: Interactive planning with large
    language models enables open-world multi-task agents. In *Proceedings of the 37th
    Advances in Neural Information Processing Systems (NeurIPS)*.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, 和 Yitao
    Liang. 2023a. 描述、解释、规划和选择：大型语言模型的互动规划使开放世界多任务代理成为可能。见于 *第37届神经信息处理系统会议（NeurIPS）*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models. In *Proceedings of the 36th
    Advances in Neural Information Processing Systems (NeurIPS)*.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, 和 Denny Zhou. 2022. 思维链提示引发大型语言模型中的推理。见于
    *第36届神经信息处理系统会议（NeurIPS）*。
- en: 'Wu et al. (2023) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong.
    2023. Self-Adaptive In-Context Learning: An Information Compression Perspective
    for In-Context Example Selection and Ordering. In *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (ACL)*. 1423–1436.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, 和 Lingpeng Kong. 2023.
    自适应上下文学习：一种信息压缩视角用于上下文示例选择和排序。见于 *第61届计算语言学协会年会（ACL）*。1423–1436。
- en: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language
    Agents. In *Proceedings of 36th Conference on Neural Information Processing Systems
    (NeurIPS)*.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, 和 Karthik Narasimhan.
    2022. WebShop: 迈向可扩展的真实世界网络交互与基础语言代理。见于 *第36届神经信息处理系统会议（NeurIPS）*。'
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models. In *Proceedings of 37th Conference
    on Neural Information Processing Systems (NeurIPS)*.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas
    L. Griffiths, Yuan Cao, 和 Karthik Narasimhan. 2023a. 思维树：通过大型语言模型进行深思熟虑的问题解决。见于
    *第37届神经信息处理系统会议（NeurIPS）*。
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *Proceedings of The 11th International Conference on Learning
    Representations (ICLR)*.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, 和 Yuan Cao. 2023b. ReAct：在语言模型中协同推理与行动。见于 *第11届国际学习表示大会（ICLR）*。
- en: 'Ye et al. (2023) Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and
    Yongbin Li. 2023. Large Language Models are Versatile Decomposers: Decomposing
    Evidence and Questions for Table-based Reasoning. In *Proceedings of the 46th
    International ACM SIGIR Conference on Research and Development in Information
    Retrieval (SIGIR)*. 174–184.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2023) Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, 和 Yongbin
    Li. 2023. 大型语言模型是多功能的分解器：为基于表格的推理分解证据和问题。见于 *第46届国际ACM SIGIR信息检索研究与开发会议（SIGIR）*。174–184。
- en: Zhang et al. (2022) Yiming Zhang, Shi Feng, and Chenhao Tan. 2022. Active Example
    Selection for In-Context Learning. In *Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*. 9134–9148.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Yiming Zhang, Shi Feng, 和 Chenhao Tan. 2022. 上下文学习中的主动示例选择。见于
    *2022年自然语言处理经验方法会议（EMNLP）*。9134–9148。
- en: Zheng et al. (2024a) Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze
    Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou. 2024a. Step-Back Prompting Enables
    Reasoning Via Abstraction in Large Language Models. In *Proceedings of The 12th
    International Conference on Learning Representations (ICLR)*.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024a) Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze
    Cheng, Ed H. Chi, Quoc V Le, 和 Denny Zhou. 2024a. 退步提示通过抽象使大型语言模型能够推理。见于 *第12届国际学习表示大会（ICLR）*。
- en: 'Zheng et al. (2024b) Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2024b.
    Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. In
    *Proceedings of 12th International Conference on Learning Representations (ICLR)*.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2024b) Longtao Zheng, Rundong Wang, Xinrun Wang, 和 Bo An. 2024b.
    Synapse: 使用记忆的轨迹作为示例提示进行计算机控制。发表于*第12届国际学习表征会议（ICLR）*。'
- en: Zhou et al. (2023) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le,
    and Ed H. Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large
    Language Models. In *The 11th International Conference on Learning Representations
    (ICLR)*.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le,
    和 Ed H. Chi. 2023. 最小到最大提示使大型语言模型能够进行复杂推理。发表于*第11届国际学习表征会议（ICLR）*。
- en: 'Zhu et al. (2023) Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan
    Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models
    for information retrieval: A survey. *arXiv preprint arXiv:2308.07107* (2023).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan
    Liu, Chenlong Deng, Zhicheng Dou, 和 Ji-Rong Wen. 2023. 用于信息检索的大型语言模型：综述。*arXiv预印本
    arXiv:2308.07107*（2023）。
- en: Appendix A Prompt Library
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 提示库
- en: A.1\. Prompts on ALFWorld
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1\. ALFWorld上的提示
- en: ALFWorld includes 6 different types of task, and we only present the prompt
    for the Put task here.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ALFWorld包含6种不同类型的任务，这里仅展示“Put”任务的提示。
- en: A.1.1\. Thought preparation
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.1\. 思路准备
- en: We write thoughts for the same demonstration ($Demo 1 and $Demo 2) as the first
    two in *ReAct* (Yao et al., [2023b](#bib.bib43)) and use them for thought preparation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为*ReAct*（Yao et al., [2023b](#bib.bib43)）的前两个示例（$Demo 1 和 $Demo 2）编写了相同的思路，并用于思路准备。
- en: 'You  are  an  agent  to  interact  with  a  household  to  solve  a  task.  You  will  be  given  a  task  where  you  need  to  put  an  (two)  object(s)  to  a  target  either  directly  or  after  an  operation.  Each  time  you  first  think  about  your  current  situation,  then  output  an  action,  and  wait  for  next  observation.Here  is  your  action  space:*  go  to  target:  Move  to  the  target,  and  you  will  observe  what  is  in/on  the  target  or  know  it  is  closed  or  opened.*  open  target:  Open  the  target  when  it  is  closed,  and  you  will  observe  what  is  in/on  the  target.  Only  cabinets,  drawers,  fridges,  safes,  and  microwaves  can  be  opened.*  take  object  from  target:  Take  the  object  from  the  target  when  the  object  is  in/on  the  target.  You  can  take  only  one  object  at  the  same  time.*  put  object  in/on  target:  Put  an  object  you  have  taken/picked  up  in/on  the  target.  You  should  go  to  the  target  in  your  last  action.  You  can  put  no  matter  there  are  other  objects  in/on  the  target  or  not.*  clean  object  with  target:  Clean  an  object  you  have  taken/picked  up  with  the  target.  The  target  should  be  a  sinkbasin.  You  should  go  to  the  target  in  your  last  action.  You  can  clean  no  matter  there  are  other  objects  in/on  the  target  or  not.*  heat  object  with  target:  Heat  an  object  you  have  taken/picked  up  with  the  target.  The  target  should  be  a  microwave.  You  should  go  to  the  target  in  your  last  action.  You  can  heat  no  matter  there  are  other  objects  in/on  the  target  or  not.*  cool  object  with  target:  Cool  an  object  you  have  taken/picked  up  with  the  target.  The  target  should  be  a  fridge.  You  should  go  to  the  target  in  your  last  action.  You  can  cool  no  matter  there  are  other  objects  in/on  the  target  or  not.*  use  object:  Use  the  object.  The  object  should  be  a  desklamp.  You  should  be  in/on  a  place  where  the  object  appears.*  look:  Look  around  and  see  what  you  are  facing.  Only  look  when  nothing  happens.Here  are  two  examples.$Demo  1$Demo  2Here  is  the  task.$Input'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个代理，与家庭成员互动以完成任务。你将会被分配一个任务，需要将一个（或两个）物体直接放置到目标上，或者经过某些操作后放置到目标上。每次你首先需要考虑当前的情况，然后输出一个行动，并等待下一次观察。以下是你的行动空间：*
    去目标：移动到目标处，你将观察到目标上有什么，或知道目标是关闭还是打开的。* 打开目标：当目标关闭时打开目标，你将观察到目标上有什么。只有橱柜、抽屉、冰箱、保险箱和微波炉可以被打开。*
    从目标中取物体：当物体在目标上时，从目标中取出物体。你一次只能取一个物体。* 将物体放在/放到目标上：将你已经取下/捡起的物体放在/放到目标上。你在上一个行动中应该去过目标。无论目标上是否有其他物体，你都可以放置物体。*
    用目标清洁物体：用目标清洁你已经取下/捡起的物体。目标应该是一个水槽。你在上一个行动中应该去过目标。无论目标上是否有其他物体，你都可以进行清洁。* 用目标加热物体：用目标加热你已经取下/捡起的物体。目标应该是一个微波炉。你在上一个行动中应该去过目标。无论目标上是否有其他物体，你都可以进行加热。*
    用目标冷却物体：用目标冷却你已经取下/捡起的物体。目标应该是一个冰箱。你在上一个行动中应该去过目标。无论目标上是否有其他物体，你都可以进行冷却。* 使用物体：使用物体。物体应该是一个台灯。你应该在物体出现的地方。*
    查看：环顾四周，看看你面临什么情况。只有在没有其他行动发生时才进行查看。这里有两个示例。$Demo 1$Demo 2这里是任务。$Input
- en: For the Put task we show part of $Demo 1 here.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“放置”任务，我们在这里展示了部分 $Demo 1。
- en: 'You  are  in  the  middle  of  a  room.  Looking  quickly  around  you,  you  see  a  cabinet  4,  a  cabinet  3,  a  cabinet  2,  a  cabinet  1,  a  countertop  1,  a  garbagecan  1,  a  handtowelholder  2,  a  handtowelholder  1,  a  sinkbasin  2,  a  sinkbasin  1,  a  toilet  1,  a  toiletpaperhanger  1,  and  a  towelholder  1.Your  task  is  to:  put  some  spraybottle  on  toilet.act:  go  to  cabinet  1think:  I  am  now  in/on:  the  middle  of  a  roomCritical  objects  I  have  found:NoneObjects  I  have  taken:NoneTo  solve  the  task,  I  need  to  find  and  take  a  sparybottle,  then  put  it  on  toilet.  First  I  need  to  find  a  spraybottle.  A  spraybottle  is  more  likely  to  appear  in  cabinet  (1-4),  countertop  (1),  toilet  (1),  sinkbasin  (1-2),  garbagecan  (1).  I  can  check  one  by  one,  starting  with  cabinet  1.obs:  On  the  cabinet  1,  you  see  a  cloth  1,  a  soapbar  1,  a  soapbottle  1.act:  go  to  cabinet  2think:  I  am  now  in/on:  cabinet  1Critical  objects  I  have  found:NoneObjects  I  have  taken:NoneIn  cabinet  1,  there  is  no  spraybottle.  Next,  I  need  to  check  cabinet  2.......obs:  On  the  toilet  1,  you  see  a  soapbottle  2.act:  put  spraybottle  2  in/on  toilet  1think:  I  am  now  in/on:  toilet  1Critical  objects  I  have  found:spraybottle  2  (cabinet  2)Objects  I  have  taken:spraybottle  2Now  I  am  on  the  toilet  1.  Next,  I  need  to  put  spraybottle  2  in/on  it.obs:  You  put  the  spraybottle  2  in/on  the  toilet  1.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在在一个房间的中央。快速环顾四周，你看到一个柜子 4，一个柜子 3，一个柜子 2，一个柜子 1，一个台面 1，一个垃圾桶 1，一个手巾架 2，一个手巾架
    1，一个洗手盆 2，一个洗手盆 1，一个马桶 1，一个厕纸架 1，以及一个毛巾架 1。你的任务是：将一个喷雾瓶放到马桶上。行为：前往柜子 1。思考：我现在在/上：房间的中央。关键物品我发现了：无。已取物品：无。要完成任务，我需要找到并取一个喷雾瓶，然后把它放到马桶上。首先我需要找到一个喷雾瓶。喷雾瓶更可能出现在柜子（1-4），台面（1），马桶（1），洗手盆（1-2），垃圾桶（1）。我可以一个一个检查，从柜子
    1 开始。观察：在柜子 1 上，你看到一个布 1，一个肥皂块 1，一个肥皂瓶 1。行为：前往柜子 2。思考：我现在在/上：柜子 1。关键物品我发现了：无。已取物品：无。在柜子
    1 中，没有喷雾瓶。接下来，我需要检查柜子 2……观察：在马桶 1 上，你看到一个肥皂瓶 2。行为：将喷雾瓶 2 放到/在马桶 1 上。思考：我现在在/上：马桶
    1。关键物品我发现了：喷雾瓶 2（柜子 2）。已取物品：喷雾瓶 2。现在我在马桶 1 上。接下来，我需要将喷雾瓶 2 放到/在上面。观察：你将喷雾瓶 2 放到/在马桶
    1 上。
- en: A.1.2\. Thought generation
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.2\. 思维生成
- en: 'You  are  an  agent  to  interact  with  a  household  to  solve  a  task.  You  will  be  given  a  task  where  you  need  to  put  an  (two)  object(s)  to  a  target  either  directly  or  after  an  operation.  Each  time  you  first  think  about  your  current  situation,  then  output  an  action,  and  wait  for  next  observation.Here  is  your  action  space:*  go  to  target:  Move  to  the  target,  and  you  will  observe  what  is  in/on  the  target  or  know  it  is  closed  or  opened.*  open  target:  Open  the  target  when  it  is  closed,  and  you  will  observe  what  is  in/on  the  target.  Only  cabinets,  drawers,  fridges,  safes,  and  microwaves  can  be  opened.*  take  object  from  target:  Take  the  object  from  the  target  when  the  object  is  in/on  the  target.  You  can  take  only  one  object  at  the  same  time.*  put  object  in/on  target:  Put  an  object  you  have  taken/picked  up  in/on  the  target.  You  should  go  to  the  target  in  your  last  action.  You  can  put  no  matter  there  are  other  objects  in/on  the  target  or  not.*  clean  object  with  target:  Clean  an  object  you  have  taken/picked  up  with  the  target.  The  target  should  be  a  sinkbasin.  You  should  go  to  the  target  in  your  last  action.  You  can  clean  no  matter  there  are  other  objects  in/on  the  target  or  not.*  heat  object  with  target:  Heat  an  object  you  have  taken/picked  up  with  the  target.  The  target  should  be  a  microwave.  You  should  go  to  the  target  in  your  last  action.  You  can  heat  no  matter  there  are  other  objects  in/on  the  target  or  not.*  cool  object  with  target:  Cool  an  object  you  have  taken/picked  up  with  the  target.  The  target  should  be  a  fridge.  You  should  go  to  the  target  in  your  last  action.  You  can  cool  no  matter  there  are  other  objects  in/on  the  target  or  not.*  use  object:  Use  the  object.  The  object  should  be  a  desklamp.  You  should  be  in/on  a  place  where  the  object  appears.*  look:  Look  around  and  see  what  you  are  facing.  Only  look  when  nothing  happens.When  you  cannot  find  some  object,  remember  to  check  less  possible  places  that  you  have  not  visited,  or  even  impossible  places  like  garbagecan.Here  are  two  examples.$Demo  1$Demo  2Here  is  the  task.$Input'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个与家庭互动以完成任务的代理。你将获得一个任务，其中需要将一个（两个）物体直接放到目标上，或在操作后放到目标上。每次你首先考虑你当前的情况，然后输出一个动作，并等待下一个观察。这里是你的动作空间：*
    去目标：移动到目标，你将观察到目标上有什么，或知道它是关闭还是打开的。* 打开目标：当目标关闭时，打开目标，你将观察到目标上有什么。只有橱柜、抽屉、冰箱、保险箱和微波炉可以被打开。*
    从目标中取物体：当物体在目标上时，从目标中取出物体。你一次只能取一个物体。* 将物体放到目标上：将你已取/拾起的物体放到目标上。你应该在你最后的动作中到达目标。不论目标上是否有其他物体，你都可以放置。*
    用目标清洁物体：用目标清洁你已取/拾起的物体。目标应该是水槽。你应该在你最后的动作中到达目标。不论目标上是否有其他物体，你都可以清洁。* 用目标加热物体：用目标加热你已取/拾起的物体。目标应该是微波炉。你应该在你最后的动作中到达目标。不论目标上是否有其他物体，你都可以加热。*
    用目标冷却物体：用目标冷却你已取/拾起的物体。目标应该是冰箱。你应该在你最后的动作中到达目标。不论目标上是否有其他物体，你都可以冷却。* 使用物体：使用物体。物体应该是台灯。你应该在物体出现的地方。*
    查看：四处查看，看看你面临的情况。只有在没有发生其他事情时才查看。当你找不到某个物体时，记得检查你没有访问过的可能性较小的地方，甚至是不可能的地方，如垃圾桶。这里有两个例子。$Demo
    1$Demo 2这里是任务。$Input
- en: For the Put task we show part of a typical $Demo here.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 对于放置任务，我们在这里展示一个典型的$Demo。
- en: 'You  are  in  the  middle  of  a  room.  Looking  quickly  around  you,  you  see  a  cabinet  1,  a  cabinet  2,  a  cabinet  3,  a  cabinet  4,  a  countertop  1,  a  garbagecan  1,  a  handtowelholder  1,  a  handtowelholder  2,  a  sinkbasin  1,  a  sinkbasin  2,  a  toilet  1,  a  toiletpaperhanger  1,  and  a  towelholder  1.Your  task  is  to:  put  some  soapbottle  on  toilet.>  think:  I  am  now  in/on:  the  middle  of  a  roomCritical  objects  I  have  found:NoneObjects  I  have  taken:NoneTo  solve  the  task,  I  need  to  find  and  take  a  soapbottle,  then  put  it  on  the  toilet.  First,  I  need  to  find  a  soapbottle.  A  soapbottle  is  more  likely  to  appear  in  cabinets  (1-4),  countertop  (1),  or  sinkbasin  (1-2).  I  will  start  by  checking  the  garbagecan  1,  although  it’s  less  likely  to  find  a  soapbottle  there,  it’s  still  possible.>  act:  go  to  garbagecan  1On  the  garbagecan  1,  you  see  a  soapbottle  1.>  think:  I  am  now  in/on:  garbagecan  1Critical  objects  I  have  found:soapbottle  1  (garbagecan  1)Objects  I  have  taken:NoneI  have  found  a  soapbottle  (1)  in  the  garbagecan  1.  Next,  I  need  to  take  it.>  act:  take  soapbottle  1  from  garbagecan  1You  pick  up  the  soapbottle  1  from  the  garbagecan  1.>  think:  I  am  now  in/on:  garbagecan  1Critical  objects  I  have  found:soapbottle  1  (garbagecan  1)Objects  I  have  taken:soapbottle  1Now  I  have  taken  the  soapbottle  (1).  Next,  I  need  to  go  to  and  put  it  on  the  toilet  1.>  act:  go  to  toilet  1On  the  toilet  1,  you  see  a  cloth  1.>  think:  I  am  now  in/on:  toilet  1Critical  objects  I  have  found:soapbottle  1  (garbagecan  1)Objects  I  have  taken:soapbottle  1Now  I  am  at  the  toilet  1.  Next,  I  need  to  put  the  soapbottle  1  on  it.>  act:  put  soapbottle  1  in/on  toilet  1'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你在一个房间的中间。快速环顾四周，你看到一个橱柜1，一个橱柜2，一个橱柜3，一个橱柜4，一个台面1，一个垃圾桶1，一个手巾架1，一个手巾架2，一个洗手盆1，一个洗手盆2，一个厕所1，一个厕纸架1，以及一个毛巾架1。你的任务是：把一瓶肥皂放在厕所上。>
    思考：我现在在/在：房间的中间 关键物体：没有 我已经拿过的物体：没有 要完成任务，我需要找到并拿起一瓶肥皂，然后把它放在厕所上。首先，我需要找到一瓶肥皂。一瓶肥皂更可能出现在橱柜（1-4），台面（1），或洗手盆（1-2）中。我将首先检查垃圾桶1，尽管在那里找到肥皂的可能性较小，但仍然有可能。>
    行动：前往垃圾桶1 在垃圾桶1上，你看到一瓶肥皂1。> 思考：我现在在/在：垃圾桶1 关键物体：肥皂1（垃圾桶1） 我已经拿过的物体：没有 我在垃圾桶1中找到了肥皂（1）。接下来，我需要拿起它。>
    行动：从垃圾桶1中拿起肥皂1 你从垃圾桶1中拿起了肥皂1。> 思考：我现在在/在：垃圾桶1 关键物体：肥皂1（垃圾桶1） 我已经拿过的物体：肥皂1 现在我已经拿到了肥皂（1）。接下来，我需要前往厕所1，并将它放在上面。>
    行动：前往厕所1 在厕所1上，你看到一个布1。> 思考：我现在在/在：厕所1 关键物体：肥皂1（垃圾桶1） 我已经拿过的物体：肥皂1 现在我在厕所1。接下来，我需要把肥皂1放在上面。>
    行动：将肥皂1放在/放在厕所1上
- en: A.1.3\. Action prediction
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.1.3. 动作预测
- en: 'For action prediction we use almost the same prompt as thought generation.
    For *TRAD* we only use $B+1+F$ steps in each demonstration, and we add the following
    sentence to tell LLM the meaning of *relative order mark*:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动作预测，我们使用与思维生成几乎相同的提示。对于*TRAD*，我们在每个演示中只使用$B+1+F$步，并且我们添加了以下句子来告诉LLM*相对顺序标记*的含义：
- en: The  mark  [Step  $i]  in  expert  examples  indicates  a  coarse  relative  position  of  expert  demonstration  steps  to  your  situation.  For  example,  [Step  -1]  means  the  last  step,  [Step  0]  means  the  current  step,  and  [Step  1]  means  the  next  step.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 标记[步骤 $i] 在专家示例中表示专家演示步骤相对于你的情况的粗略相对位置。例如，[步骤 -1] 表示最后一步，[步骤 0] 表示当前步骤，[步骤 1]
    表示下一步。
- en: A.2\. Prompts on Mind2Web
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2. Mind2Web 上的提示
- en: On Mind2Web we generally follow prompts in *Synapse* (Zheng et al., [2024b](#bib.bib47)).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mind2Web 上，我们通常遵循*Synapse*（Zheng 等，[2024b](#bib.bib47)）中的提示。
- en: A.2.1\. Thought preparation
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1. 思维准备
- en: 'You  are  a  large  language  model  trained  to  navigate  the  web.  You  will  be  given  a  task,  an  observation,  and  your  previous  actions,  and  each  time  you  should  output  the  next  action  and  wait  for  the  next  observation.  Here  is  the  action  space:1.  ‘CLICK  [id]‘:  Click  on  an  HTML  element  with  its  id.2.  ‘TYPE  [id]  [value]‘:  Type  a  string  into  the  element  with  the  id.3.  ‘SELECT  [id]  [value]‘:  Select  a  value  for  an  HTML  element  by  its  id.Now  you  are  given  some  expert  demonstrations  and  reasons  for  their  actions,  follow  these  examples  and  give  your  reason  for  the  given  action.  Note  that  you  should  take  all  previous  actions  into  reasoning,  and  not  take  the  current  action  as  what  you  have  done.$Demo  1$Demo  2$Demo  3$Input'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个训练有素的大型语言模型，旨在浏览网页。你将会得到一个任务、一项观察以及你之前的行动，每次你应该输出下一个行动并等待下一次观察。这里是行动空间：1.
    ‘CLICK [id]’：点击一个具有指定id的HTML元素。2. ‘TYPE [id] [value]’：在具有指定id的元素中输入一个字符串。3. ‘SELECT
    [id] [value]’：为一个具有指定id的HTML元素选择一个值。现在你将获得一些专家演示和他们行动的理由，按照这些示例并给出你对给定行动的理由。请注意，你应该将所有之前的行动纳入考虑，而不是将当前行动视为你已经完成的行动。$Demo
    1$Demo 2$Demo 3$Input
- en: 'We show one demonstration here:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示一个演示：
- en: 'Task:  Find  JetBlue  press  releases  for  the  year  2020Trajectory:obs:  ‘              From                To                    Investor  Relations        Press  Room                250  characters  remaining      Submit        ‘act:  ‘CLICK  [107]‘  ([link]  Press  RoomExternal  Link  should  open  in  a  new  windo...  ->  CLICK)obs:  ‘        View  all  releases                Print  Core  Overhead  Bins                  Print  Welcome  Kiosk  (JFK)                  Print  Core  Overhead  Bins                  Print  Welcome  Kiosk  (JFK)              ‘act:  ‘CLICK  [4386]‘  ([link]  View  All  Releases  \uedbe  ->  CLICK)obs:  ‘          Select  year:        2023      2022      2021      2020      2019      2018      2017      2016      2015      2014      2013      2012      2011      2010      2009      2008      2007      2006      2005      2004      2003      2002            Category:        All  Latest  News      Press  Releases      Articles            Search:          Search            ‘act:  ‘SELECT  [7685]  [2020]‘  ([combobox]  Select  year:  ->  SELECT:  2020)reason:  I  have  to  find:JetBlue  press  releasesfor  the  year  2020Now  I  have  done:‘CLICK  [107]‘  ([link]  Press  RoomExternal  Link  should  open  in  a  new  windo...  ->  CLICK)‘CLICK  [4386]‘  ([link]  View  All  Releases  \uedbe  ->  CLICK)  to  show  all  releasesTherefore,  next  I  have  to:‘SELECT  [7685]  [2020]‘  ([combobox]  Select  year:  ->  SELECT:  2020)  due  to  the  condition  ‘for  the  year  2020‘'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: The input are presented in the same format as demonstrations without human-written
    reasons.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 输入以与演示相同的格式呈现，没有人工编写的理由。
- en: A.2.2\. Thought generation
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2\. 思维生成
- en: 'You  are  a  large  language  model  trained  to  navigate  the  web.  You  will  be  given  a  task,  an  observation,  and  your  previous  actions.  Each  time  you  should  output  the  next  action  and  wait  for  the  next  observation.  Here  is  the  action  space:1.  ‘CLICK  [id]‘:  Click  on  an  HTML  element  with  its  id.2.  ‘TYPE  [id]  [value]‘:  Type  a  string  into  the  element  with  the  id.3.  ‘SELECT  [id]  [value]‘:  Select  a  value  for  an  HTML  element  by  its  id.Now  you  are  given  some  expert  demonstrations,  follow  these  examples  and  conduct  reasoning  about  your  situation.$Demo  1$Demo  2$Demo  3$Input'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个大型语言模型，经过训练可以在网络上进行导航。你将被分配一个任务、一个观察结果以及你之前的动作。每次你都应该输出下一个动作并等待下一个观察结果。这里是动作空间：1.
    ‘CLICK [id]’：点击一个具有其id的HTML元素。2. ‘TYPE [id] [value]’：在具有id的元素中输入一个字符串。3. ‘SELECT
    [id] [value]’：为一个HTML元素选择一个值，通过其id。现在你会看到一些专家演示，按照这些示例进行推理并处理你的情况。$Demo 1$Demo
    2$Demo 3$Input
- en: We show part of a typical $Demo here.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示了一个典型的$Demo的一部分。
- en: 'Task:  Find  cheapest  cars  available  at  San  Francisco  Airport  for  a  day.Trajectory:obs:  ‘        Pick-up  location                San  Francisco          San  Francisco  Airport            Airports          ‘previous  actions:reason:  I  have  to  find:the  cheapest  carsavailable  at  San  Francisco  Airportfor  a  dayNow  I  have  done:NoneTherefore,  next  I  have  to:‘CLICK  [896]‘  ([link]  San  Francisco  Airport  ->  CLICK)  to  select  the  pick-up  location  as  San  Francisco  Airport.obs:  ‘        Car  rental            Pick-up  location                Pick-up  date      Tue,  Mar  28              Drop-off  date      Fri,  Mar  31            Search            ‘previous  actions:‘CLICK  [896]‘  ([link]  San  Francisco  Airport  ->  CLICK)reason:  I  have  to  find:the  cheapest  carsavailable  at  San  Francisco  Airportfor  a  dayNow  I  have  done:‘CLICK  [896]‘  ([link]  San  Francisco  Airport  ->  CLICK)  due  to  the  condition  ‘at  San  Francisco  Airport‘Therefore,  next  I  have  to:‘CLICK  [2131]‘  ([div]  Fri,  Mar  31  ->  CLICK)  to  select  the  drop-off  date  as  "Fri,  Mar  31"obs:  ‘        Car  rental            Pick-up  location                  29                  Drop-off  date      Fri,  Mar  31            Midnight      12:30  AM      1:00  AM      1:30  AM      2:00  AM      2:30  AM      3:00  AM      3:30  AM      4:00  AM      4:30  AM      5:00  AM      5:30  AM      6:00  AM      6:30  AM      7:00  AM      7:30  AM      8:00  AM      8:30  AM      9:00  AM      9:30  AM      10:00  AM      10:30  AM      11:00  AM      11:30  AM      Noon            Search            ‘previous  actions:‘CLICK  [896]‘  ([link]  San  Francisco  Airport  ->  CLICK)‘CLICK  [2131]‘  ([div]  Fri,  Mar  31  ->  CLICK)reason:  I  have  to  Find  cheapest  cars  available  at  San  Francisco  Airport  for  a  day.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 任务：在旧金山机场查找一天内最便宜的汽车。轨迹：观察：‘    取车地点 
          旧金山     旧金山机场      机场 
       ‘之前的操作：原因：我必须找到：在旧金山机场一天内最便宜的汽车现在我已经完成：无因此，接下来我必须：‘点击
    [896]‘ ([link] 旧金山机场 -> 点击) 选择取车地点为旧金山机场。观察：‘   
    汽车租赁      取车地点        取车日期 
     周二, 3月28日     
     还车日期   周五, 3月31日    
     搜索      ‘之前的操作：‘点击 [896]‘ ([link]
    旧金山机场 -> 点击)原因：我必须找到：在旧金山机场一天内最便宜的汽车现在我已经完成：‘点击 [896]‘ ([link] 旧金山机场 -> 点击) 由于条件‘在旧金山机场’因此，接下来我必须：‘点击
    [2131]‘ ([div] 周五, 3月31日 -> 点击) 选择还车日期为“周五, 3月31日”。观察：‘  
     汽车租赁      取车地点         29         还车日期   周五, 3月31日   
      午夜   12:30
    AM   1:00 AM   1:30 AM 
     2:00 AM   2:30 AM  
    3:00 AM   3:30 AM   4:00 AM 
     4:30 AM   5:00 AM  
    5:30 AM   6:00 AM   6:30 AM 
     7:00 AM   7:30 AM  
    8:00 AM   8:30 AM   9:00 AM 
     9:30 AM   10:00 AM   10:30 AM   11:00 AM   11:30
    AM   中午    
     搜索      ‘之前的操作：‘点击 [896]‘ ([link]
    旧金山机场 -> 点击)‘点击 [2131]‘ ([div] 周五, 3月31日 -> 点击)原因：我必须在旧金山机场找到一天内最便宜的汽车。
- en: A.2.3\. Action prediction
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.3\. 动作预测
- en: We use the same sentence as in ALFWorld to tell LLM about the *relative order
    mark*.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与 ALFWorld 相同的句子来告诉 LLM 关于*相对顺序标记*。
- en: 'You  are  a  large  language  model  trained  to  navigate  the  web.  You  will  be  given  a  task,  an  observation,  and  your  previous  actions.  Each  time  you  should  output  the  next  action  and  wait  for  the  next  observation.  Here  is  the  action  space:1.  ‘CLICK  [id]‘:  Click  on  an  HTML  element  with  its  id.2.  ‘TYPE  [id]  [value]‘:  Type  a  string  into  the  element  with  the  id.3.  ‘SELECT  [id]  [value]‘:  Select  a  value  for  an  HTML  element  by  its  id.Now  you  are  given  some  expert  demonstrations,  follow  these  demonstrations  and  make  your  decision.The  mark  [Step  $i]  indicates  a  coarse  relative  position  of  expert  demonstration  steps  to  your  situation.  For  example,  [Step  -1]  means  the  last  step,  [Step  0]  means  the  current  step,  and  [Step  1]  means  the  next  step.Note  that  you  should  take  all  previous  actions  into  reasoning.  In  your  output,  the  action  should  be  quoted  by  a  pair  of  ’‘’.$Demo  1$Demo  2$Demo  3$Input'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个经过训练的大型语言模型，能够浏览网页。你将会得到一个任务、一项观察和你之前的动作。每次你应该输出下一步动作，并等待下一次观察。以下是动作空间：
- en: 'We show the format of demonstrations here:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示演示的格式：
- en: 'Task:  Look  for  a  job  opening  in  sales  in  San  Fransisco,  and  if  found,  apply  for  the  job.obs:  ‘                Research                      Our  Team      Careers              Our  Team      Careers        ‘previous  actions:‘CLICK  [117]‘  ([link]  Our  Team  ->  CLICK)act:  ‘CLICK  [8015]‘  ([menuitem]  olink  ->  CLICK)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 任务：在旧金山寻找销售职位空缺，如果找到，请申请该职位。观察：‘    
       研究     
         我们的团队   职业    
      我们的团队   职业   
    ‘ 之前的动作：‘CLICK [117]‘ ([link] 我们的团队 -> 点击) 动作：‘CLICK [8015]‘ ([menuitem]
    olink -> 点击)
- en: The input are presented in the same format as demonstrations, except that they
    have no ground-truth actions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 输入以与演示相同的格式呈现，只是它们没有真实动作。
- en: Appendix B Full Experiment Results
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 完整实验结果
- en: B.1\. The Effect of $F$
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. $F$ 的效果
- en: 'We list the results of varying subsequent step number $F$ of temporal expansion
    on each subset and over all 3 subsets of the Mind2Web benchmark in Fig. [6](#A2.F6
    "Figure 6 ‣ B.1\. The Effect of 𝐹 and 𝐵 ‣ Appendix B Full Experiment Results ‣
    TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision").'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [6](#A2.F6 "图 6 ‣ B.1\. $F$ 和 $B$ 的效果 ‣ 附录 B 完整实验结果 ‣ TRAD: 增强 LLM 代理的逐步思考检索和对齐决策")
    中列出了不同的时间扩展后续步骤数 $F$ 对 Mind2Web 基准的每个子集以及所有 3 个子集的结果。'
- en: '![Refer to caption](img/da183bed86b8cae76532615c63066bd3.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/da183bed86b8cae76532615c63066bd3.png)'
- en: (a) Cross-Task ($F$)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 跨任务 ($F$)
- en: '![Refer to caption](img/40edd03c320403a1f374290bb47e45bc.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/40edd03c320403a1f374290bb47e45bc.png)'
- en: (b) Cross-Task ($B$)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 跨任务 ($B$)
- en: '![Refer to caption](img/682a5f1edebe7b5352aa6bffc348e279.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/682a5f1edebe7b5352aa6bffc348e279.png)'
- en: (c) Cross-Website ($F$)
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 跨网站 ($F$)
- en: '![Refer to caption](img/ffa1a122821166caeedd9beffc7eca00.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ffa1a122821166caeedd9beffc7eca00.png)'
- en: (d) Cross-Website ($B$)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 跨网站 ($B$)
- en: '![Refer to caption](img/3cc2f42ae050ad9b364b8f40b4f9f03c.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3cc2f42ae050ad9b364b8f40b4f9f03c.png)'
- en: (e) Cross-Domain ($F$)
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 跨领域 ($F$)
- en: '![Refer to caption](img/dd7add88b917086e6d8df3ef3f27c691.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd7add88b917086e6d8df3ef3f27c691.png)'
- en: (f) Cross-Domain ($B$)
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 跨领域 ($B$)
- en: '![Refer to caption](img/f18cf93fd59d9f566b12e825f7faf3f9.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f18cf93fd59d9f566b12e825f7faf3f9.png)'
- en: (g) All ($F$)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 全部 ($F$)
- en: '![Refer to caption](img/e62d0c938a226b3d84573ba602a16c80.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e62d0c938a226b3d84573ba602a16c80.png)'
- en: (h) All ($B$)
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 全部 ($B$)
- en: Figure 6. The effect of varying subsequent steps $F$ does not help more when
    they are sufficiently large.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. 当后续步骤$F$足够大时，其变化的效果没有额外帮助。
- en: B.2\. The Effect of $K$
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. $K$的效果
- en: 'We list the results of varying retrieval size $K$ on each subset and over all
    3 subsets of the Mind2Web benchmark in Fig. [7](#A2.F7 "Figure 7 ‣ B.2\. The Effect
    of 𝐾 ‣ Appendix B Full Experiment Results ‣ TRAD: Enhancing LLM Agents with Step-Wise
    Thought Retrieval and Aligned Decision").'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图[7](#A2.F7 "Figure 7 ‣ B.2\. The Effect of 𝐾 ‣ Appendix B Full Experiment
    Results ‣ TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned
    Decision")中列出了不同检索大小$K$对每个子集以及对Mind2Web基准测试的所有3个子集的结果。'
- en: '![Refer to caption](img/b01a0e2061d603102dc48a9dd156eb42.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b01a0e2061d603102dc48a9dd156eb42.png)'
- en: (a) Cross-Task
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 跨任务
- en: '![Refer to caption](img/fc27eb800953b4ada354001265e62b19.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc27eb800953b4ada354001265e62b19.png)'
- en: (b) Cross-Website
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 跨网站
- en: '![Refer to caption](img/99e5f3e728f3a8f115ff683faba243e5.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/99e5f3e728f3a8f115ff683faba243e5.png)'
- en: (c) Cross-Domain
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 跨领域
- en: '![Refer to caption](img/b28b1fd48f212abfe2440321dc33b3c7.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b28b1fd48f212abfe2440321dc33b3c7.png)'
- en: (d) All
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 全部
- en: Figure 7. The effect of varying the number of retrieved demonstrations $K$ varies.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 检索演示数量$K$的变化效果有所不同。
