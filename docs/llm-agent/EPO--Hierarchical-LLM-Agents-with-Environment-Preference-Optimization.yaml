- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:38:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EPO: Hierarchical LLM Agents with Environment Preference Optimization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EPO：具有环境偏好优化的层次化LLM代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16090](https://ar5iv.labs.arxiv.org/html/2408.16090)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.16090](https://ar5iv.labs.arxiv.org/html/2408.16090)
- en: Qi Zhao^*, Haotian Fu^*, Chen Sun, George Konidaris
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 齐兆^*，傅昊天^*，陈孙，乔治·科尼达里斯
- en: 'Brown University *: Equal contribution.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 布朗大学 *：平等贡献。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Long-horizon decision-making tasks present significant challenges for LLM-based
    agents due to the need for extensive planning over multiple steps. In this paper,
    we propose a hierarchical framework that decomposes complex tasks into manageable
    subgoals, utilizing separate LLMs for subgoal prediction and low-level action
    generation. To address the challenge of creating training signals for unannotated
    datasets, we develop a reward model that leverages multimodal environment feedback
    to automatically generate reward signals. We introduce Environment Preference
    Optimization (EPO), a novel method that generates preference signals from the
    environment’s feedback and uses them to train LLM-based agents. Extensive experiments
    on ALFRED demonstrate the state-of-the-art performance of our framework, achieving
    first place on the ALFRED public leaderboard and showcasing its potential to improve
    long-horizon decision-making in diverse environments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 长期决策任务对基于LLM的代理提出了重大挑战，因为它们需要在多个步骤中进行广泛规划。在本文中，我们提出了一种层次化框架，将复杂任务分解为可管理的子目标，利用不同的LLM进行子目标预测和低级动作生成。为了应对创建未标注数据集的训练信号的挑战，我们开发了一种奖励模型，利用多模态环境反馈自动生成奖励信号。我们介绍了环境偏好优化（EPO），一种从环境反馈中生成偏好信号并利用这些信号训练基于LLM的代理的新方法。在ALFRED上的大量实验展示了我们框架的最先进性能，获得了ALFRED公共排行榜的第一名，并展示了其在多样环境中提升长期决策能力的潜力。
- en: 'EPO: Hierarchical LLM Agents with Environment Preference Optimization'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: EPO：具有环境偏好优化的层次化LLM代理
- en: Qi Zhao^*, Haotian Fu^*, Chen Sun, George Konidaris Brown University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 齐兆^*，傅昊天^*，陈孙，乔治·科尼达里斯布朗大学
- en: '^†^†footnotetext: *: Equal contribution.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注： *：平等贡献。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Long-horizon decision-making/planning remains a formidable challenge for Large
    Language Model(LLM)-based agents (Valmeekam et al., [2023](#bib.bib43); Liu et al.,
    [2023](#bib.bib23); Silver et al., [2024](#bib.bib35)). These tasks require extensive
    planning over multiple steps, maintaining coherence and goal orientation, which
    is difficult for LLMs that are typically designed for more immediate and localized
    predictions. Moreover, a key issue of finetuning LLMs for embodied agents is the
    need of large scale labeled data (Reed et al., [2022](#bib.bib32)). The same issue
    is reflected in researchers’ effort in building reward models from vision foundation
    models as we might need to obtain “internet-scale” data of task demonstrations
    (Fan et al., [2022](#bib.bib9)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 长期决策/规划仍然是基于大型语言模型（LLM）代理的一个重大挑战（Valmeekam 等，[2023](#bib.bib43)；刘等，[2023](#bib.bib23)；Silver
    等，[2024](#bib.bib35)）。这些任务需要在多个步骤中进行广泛规划，保持连贯性和目标导向，这对于通常设计用于更即时和局部预测的LLM而言非常困难。此外，针对具身代理的LLM微调的一个关键问题是需要大量标记数据（Reed
    等，[2022](#bib.bib32)）。这一问题也反映在研究人员从视觉基础模型中构建奖励模型的努力中，我们可能需要获得“互联网规模”的任务示例数据（Fan
    等，[2022](#bib.bib9)）。
- en: To tackle the first challenge, a straightforward way is to first let the LLM
    decompose the long-horizon task into shorter horizon subtasks, and then use different
    LLMs as the policies at different levels, i.e., use one LLM-based policy to generate
    subgoals, and use another LLM generate low-level actions given the subgoals, both
    of which require significantly fewer planning steps. This decomposition facilitates
    more effective planning and execution by leveraging the predictive power of LLMs
    at both the subgoal and action levels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对第一个挑战，一种简单的方法是首先让LLM将长期任务分解为较短期的子任务，然后在不同层次上使用不同的LLM作为策略，即，使用一个LLM生成子目标，并使用另一个LLM在给定子目标的情况下生成低级动作，这两者都需要显著较少的规划步骤。这种分解通过在子目标和动作层面利用LLM的预测能力，促进了更有效的规划和执行。
- en: However, the problem of how to efficiently train these LLM-based agents remains.
    In this paper, we consider the setting where only part of the dataset are annotated
    with ground-truth actions and subgoals, and we need to find a way to create training
    signals for the unannotated dataset. The common training signals for decision-making
    agents are based on the rewards received during interactions with the environment (Sutton
    and Barto, [1998](#bib.bib41)). But the manual design of reward functions is both
    time-consuming and prone to inaccuracies, which hinders the scalability and adaptability
    of LLM-based agents in dynamic and diverse environments. Consequently, there is
    a growing need for methods that can automatically generate reward signals from
    the environment, thus bypassing the complexities associated with human-engineered
    rewards. This motivation drives us to explore reward modeling approaches that
    can leverage multimodal feedback from the environment, such as visual and interaction
    data, to guide the learning process of LLM-based agents by leveraging the public
    pretrained foundation models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如何高效训练这些基于 LLM 的代理仍然是一个问题。在本文中，我们考虑到数据集中只有部分数据被标注为真实的动作和子目标，我们需要找到一种方法为未标注数据集创建训练信号。决策代理的常见训练信号是基于与环境交互过程中获得的奖励（Sutton
    和 Barto，[1998](#bib.bib41)）。但手动设计奖励函数既耗时又容易出现不准确性，这阻碍了 LLM 基于代理在动态和多样化环境中的可扩展性和适应性。因此，迫切需要能够从环境中自动生成奖励信号的方法，从而绕过与人工设计奖励相关的复杂性。这一动机驱使我们探索能够利用环境中的多模态反馈（如视觉和互动数据）来指导基于
    LLM 的代理学习过程的奖励建模方法，同时利用公开的预训练基础模型。
- en: On the other hand, recent advancements in preference optimization techniques,
    such as Direct Preference Optimization (DPO) (Rafailov et al., [2023](#bib.bib31)),
    have shown that LLMs can be effectively trained using preference-based signals
    rather than explicit reward functions. DPO leverages the inherent capabilities
    of LLMs to model preferences between different outputs, facilitating a more intuitive
    and flexible training paradigm. This insight inspires us to develop a novel method
    that combines the strengths of preference optimization with automatic reward modeling
    to enhance the performance of LLM-based agents in long-horizon decision-making
    tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，最近在偏好优化技术方面的进展，例如直接偏好优化（DPO）（Rafailov 等，[2023](#bib.bib31)），表明可以有效地使用基于偏好的信号来训练大型语言模型（LLMs），而不是使用明确的奖励函数。DPO
    利用 LLMs 在不同输出之间建模偏好的固有能力，从而促进了更直观和灵活的训练范式。这一洞察力激发我们开发一种新方法，将偏好优化的优势与自动奖励建模相结合，以增强基于
    LLM 的代理在长期决策任务中的表现。
- en: In this paper, we propose a hierarchical LLMs-based framework for long-horizon
    decision making problems. Our agent decomposes complex tasks into manageable subtasks
    by training two LLMs to predict the subgoal decomposition and low-level actions
    respectively. To retrieve enough training signals from the unannotated dataset,
    we propose a LLM-based reward model that is able to integrate the multimodal environment
    feedback information and automatically generate reward signals for the unannotated
    dataset. Then, we introduce Environment Preference Optimization (EPO), a method
    that generates preference signals automatically from the environment’s feedback.
    EPO ranks the proposed actions and subgoals based on the estimated rewards and
    constructs a preference dataset that guides the training of LLM-based agents.
    This approach leverages both annotated and unannotated datasets, significantly
    expanding the training data available for improving agent performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于 LLM 的层次化框架，用于长期决策问题。我们的代理通过训练两个 LLM 来预测子目标分解和低级动作，从而将复杂任务分解为可管理的子任务。为了从未标注的数据集中检索到足够的训练信号，我们提出了一种基于
    LLM 的奖励模型，它能够整合多模态环境反馈信息，并自动生成未标注数据集的奖励信号。然后，我们引入环境偏好优化（EPO），一种从环境反馈中自动生成偏好信号的方法。EPO
    根据估计的奖励对提出的动作和子目标进行排序，并构建一个偏好数据集，以指导基于 LLM 的代理的训练。这种方法利用了标注和未标注的数据集，显著扩展了可用于提高代理性能的训练数据。
- en: To validate our framework design, we conduct extensive experiments on ALFRED (Shridhar
    et al., [2020a](#bib.bib33)), a popular household simulation environment for embodied
    agents. Our method achieves the state-of-the-art performance on ALFRED. We also
    find that unified environment feedback significantly help decision-making agents
    in both subgoal decomposition level and environment interaction level. Moreover,
    in the setup where there exists a large dataset of task specifications but only
    a small annotated task and demonstrations, our framework allows agent to benefit
    from the unannotated new tasks while significantly outperforming supervised training,
    indicating the potential of our framework.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的框架设计，我们在 ALFRED (Shridhar et al., [2020a](#bib.bib33)) 这个受欢迎的家庭仿真环境上进行了广泛的实验。我们的方法在
    ALFRED 上达到了最先进的性能。我们还发现，统一的环境反馈显著帮助决策代理在子目标分解层面和环境互动层面上。更重要的是，在任务规格的大型数据集存在但只有少量标注任务和示例的设置中，我们的框架允许代理从未标注的新任务中获益，同时显著超越了监督训练，表明了我们框架的潜力。
- en: 'To sum up, we make the following contributions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们做出了以下贡献：
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose a hierarchical LLMs-based framework for long-horizon decision-making
    problems, where both levels of LLMs can be jointly trained with preference signals
    generated from a LLM-based reward model.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种基于层次化 LLMs 的长期决策框架，其中两个层次的 LLMs 可以与从 LLM 基于奖励模型生成的偏好信号一起联合训练。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We propose Environment Preference Optimization (EPO), a method that first learns
    to automatically generate preference signals for an unannotated dataset from multimodal
    environment feedbacks by learning a reward model, and then use them to train/finetune
    the hierarchical LLMs-based agents.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了环境偏好优化 (EPO)，这是一种方法，通过学习奖励模型，首先自动生成来自多模态环境反馈的未标注数据集的偏好信号，然后使用这些信号训练/微调基于层次化
    LLMs 的代理。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We demonstrate the effectiveness of our framework through extensive experiments
    and achieved state-of-the-art performance on ALFRED (we reached the first place
    on the ALFRED public leaderboard¹¹1[https://leaderboard.allenai.org/alfred/submissions/public](https://leaderboard.allenai.org/alfred/submissions/public).
    EPO has been top of the leaderboard as of the release date of this paper.).
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过广泛的实验展示了我们框架的有效性，并在 ALFRED 上取得了最先进的性能（我们在 ALFRED 公共排行榜上获得了第一名¹¹1[https://leaderboard.allenai.org/alfred/submissions/public](https://leaderboard.allenai.org/alfred/submissions/public)。EPO
    在本文发布时已经位居排行榜榜首。）。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Foundational Models for Embodied Agents. A number of recent works have explored
    foundational models for embodied agents (Driess et al., [2023](#bib.bib6); Stone
    et al., [2023](#bib.bib40); Brohan et al., [2023](#bib.bib3); Zitkovich et al.,
    [2023](#bib.bib47)). Our work is inspired by many previous language grounding
    agents work (Singh et al., [2023](#bib.bib36); Ahn et al., [2022](#bib.bib1);
    Huang et al., [2022](#bib.bib15)) on robotics. These studies work on grounding
    natural language prompt or robotic actions with symbolically represented visual
    or interaction information. Similarly effort in grounding language to visual information
    for embodied agents have been done in (Song et al., [2023a](#bib.bib37)). Among
    works in simulation, Pashevich et al. ([2021](#bib.bib30)) present the end-to-end
    approach for decision-making agents, which directly predicts the agent’s next
    action from task specification and visual input without subgoal alignment and
    map-based navigation. Min et al. ([2021](#bib.bib27)) introduce a hierarchical
    approach, which has dominated due to their superior performance. Fu et al. ([2024](#bib.bib10))
    leverages LLM to help learning skills from demonstrations. Our hierarchical LLMs
    framework is also inspired by many prior hierarchical RL works (Nachum et al.,
    [2018](#bib.bib28); Levy et al., [2019](#bib.bib21); Fu et al., [2023](#bib.bib11)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 具身智能体的基础模型。最近的研究探讨了具身智能体的基础模型（Driess et al., [2023](#bib.bib6); Stone et al.,
    [2023](#bib.bib40); Brohan et al., [2023](#bib.bib3); Zitkovich et al., [2023](#bib.bib47)）。我们的工作受到许多之前关于语言基础智能体工作的启发（Singh
    et al., [2023](#bib.bib36); Ahn et al., [2022](#bib.bib1); Huang et al., [2022](#bib.bib15)）。这些研究致力于将自然语言提示或机器人动作与符号化的视觉或交互信息进行基础对接。同样，针对具身智能体将语言与视觉信息对接的努力也在（Song
    et al., [2023a](#bib.bib37)）中进行。在仿真领域，Pashevich et al. ([2021](#bib.bib30)) 提出了端到端的决策智能体方法，该方法直接从任务规范和视觉输入中预测智能体的下一个动作，而无需子目标对齐和基于地图的导航。Min
    et al. ([2021](#bib.bib27)) 引入了一个层次化的方法，由于其优越的性能而占据主导地位。Fu et al. ([2024](#bib.bib10))
    利用大型语言模型（LLM）从演示中帮助学习技能。我们的层次化LLMs框架也受到许多先前层次化强化学习工作的启发（Nachum et al., [2018](#bib.bib28);
    Levy et al., [2019](#bib.bib21); Fu et al., [2023](#bib.bib11)）。
- en: Reward Modeling with Foundational Models. Foundation models with their capability
    in encoding generic representations of a modality have motivated researchers to
    use them to generate reward signals in order to bypass human reward engineering.
    Among these efforts, Sontakke et al. ([2023](#bib.bib39)); Escontrela et al. ([2023](#bib.bib8));
    Chen et al. ([2021](#bib.bib4)); Fan et al. ([2022](#bib.bib9)); Mahmoudieh et al.
    ([2022](#bib.bib26)) use vision foundation models to estimate the reward by aligning
    visual features with desired actions or state transitions. However, these approaches
    often require large scale data. In contrast, we are interested in using pretrained
    LLMs to generate reward signals (Kwon et al., [2023](#bib.bib19)) from all symbolically
    represented environment feedback. Within this scope, Song et al. ([2023b](#bib.bib38));
    Yu et al. ([2023](#bib.bib45)); Ma et al. ([2023](#bib.bib25)); Huang et al. ([2023](#bib.bib14));
    Wang et al. ([2023](#bib.bib44)) use language models to generate rewards to help
    robot learn skills based on the symbolic states. For embodied agents, ELLM (Du
    et al., [2023](#bib.bib7)) propose a framework to use LLMs to guide agents’ exploration
    and generate reward based on the task goals in 2D games and robotic simulators.
    Compared to existing works, we fill in the blank by proposing a generic framework
    that use LLMs to synthesize reward from multimodal environment feedback.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的奖励建模。基础模型具有编码模态通用表示的能力，激励了研究人员使用它们生成奖励信号，以绕过人工奖励工程。在这些努力中，Sontakke et al.
    ([2023](#bib.bib39)); Escontrela et al. ([2023](#bib.bib8)); Chen et al. ([2021](#bib.bib4));
    Fan et al. ([2022](#bib.bib9)); Mahmoudieh et al. ([2022](#bib.bib26)) 使用视觉基础模型通过将视觉特征与期望的动作或状态转换对齐来估计奖励。然而，这些方法通常需要大规模的数据。相比之下，我们感兴趣的是使用预训练的LLMs从所有符号化环境反馈中生成奖励信号（Kwon
    et al., [2023](#bib.bib19)）。在此范围内，Song et al. ([2023b](#bib.bib38)); Yu et al.
    ([2023](#bib.bib45)); Ma et al. ([2023](#bib.bib25)); Huang et al. ([2023](#bib.bib14));
    Wang et al. ([2023](#bib.bib44)) 使用语言模型生成奖励，帮助机器人基于符号状态学习技能。对于具身智能体，ELLM（Du et
    al., [2023](#bib.bib7)）提出了一个框架，利用LLMs指导智能体的探索，并基于任务目标在2D游戏和机器人模拟器中生成奖励。与现有工作相比，我们通过提出一个通用框架，填补了这一空白，该框架利用LLMs从多模态环境反馈中综合生成奖励。
- en: Preference-Based Learning for Language Models. Aligning language models to human
    preference (Ouyang et al., [2022](#bib.bib29)) has greatly improved language models
    to follow human instructions. Recent development such as Direct Preference Optimization
    (Rafailov et al., [2023](#bib.bib31)), self-rewarding language models Yuan et al.
    ([2024](#bib.bib46)) in preference alignment allows the language model to directly
    learn the preference relation and also learn from its own synthesized data. Inspired
    by these work, we extend the definition of “preference” into the alignment between
    environment feedback and agent actions with respect to the task specification.
    We leverage the algorithmic advantage demonstrated in DPO and the idea of self
    data synthesis (Lee et al., [2023](#bib.bib20)) to train LLM-based embodied agents
    to ground language to environment feedback.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于偏好的语言模型学习。将语言模型与人类偏好对齐（Ouyang et al., [2022](#bib.bib29)）极大地提高了语言模型遵循人类指令的能力。近期的发展，如直接偏好优化（Rafailov
    et al., [2023](#bib.bib31)）、自奖励语言模型（Yuan et al., [2024](#bib.bib46)），在偏好对齐中允许语言模型直接学习偏好关系，并从自身合成的数据中学习。受到这些工作的启发，我们将“偏好”的定义扩展为环境反馈与代理行为之间的对齐，符合任务规格。我们利用DPO展示的算法优势以及自数据合成的思想（Lee
    et al., [2023](#bib.bib20)），训练基于LLM的具身代理，将语言与环境反馈对接。
- en: 3 Method
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: '![Refer to caption](img/4610e34470ffa37b5c1fc39d4b47b4fb.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4610e34470ffa37b5c1fc39d4b47b4fb.png)'
- en: 'Figure 1: An illustration of the hierarchical framework. Our agent first outputs
    the subgoals from human instructions and visual inputs using its high-level subgoal
    decomposition module. Then the interaction module predicts low-level actions autoregressively
    to complete the given subgoals.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：层次结构框架的示意图。我们的代理首先使用其高层次子目标分解模块从人类指令和视觉输入中输出子目标。然后，交互模块自回归地预测低级动作，以完成给定的子目标。
- en: 'We first describe the problem setup in [3.1](#S3.SS1 "3.1 Problem Setup ‣ 3
    Method ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization")
    and then introduce our hierarchical LLMs-based decision-making agent in [3.2](#S3.SS2
    "3.2 Hierarchical LLMs-based Agent ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with
    Environment Preference Optimization"). Then we present our approaches for generating
    reward signals from multimodal environment feedback in [3.3](#S3.SS3 "3.3 Reward
    modeling from Environment Feedback ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with
    Environment Preference Optimization"). Lastly, we explain how we train the hierarchical
    agents with Environment Preference Optimization in [3.4](#S3.SS4 "3.4 Environment
    Preference Optimization ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在 [3.1](#S3.SS1 "3.1 Problem Setup ‣ 3 Method ‣ EPO: Hierarchical LLM Agents
    with Environment Preference Optimization") 描述问题设置，然后在 [3.2](#S3.SS2 "3.2 Hierarchical
    LLMs-based Agent ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment Preference
    Optimization") 介绍我们的基于层次结构的LLMs决策代理。接着，我们在 [3.3](#S3.SS3 "3.3 Reward modeling
    from Environment Feedback ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization") 展示如何从多模态环境反馈中生成奖励信号。最后，我们在 [3.4](#S3.SS4 "3.4 Environment
    Preference Optimization ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization") 解释如何用环境偏好优化训练层次结构代理。'
- en: 3.1 Problem Setup
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题设置
- en: In this paper, we consider the decision-making agents that take in human language
    instructions $G$. The performance of our agent is measured with task success rate,
    which is the percentage of test tasks completed given a set of human task instructions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们考虑接收人类语言指令$G$的决策代理。我们的代理性能通过任务成功率来衡量，即在给定一组人类任务指令的情况下，完成测试任务的百分比。
- en: 3.2 Hierarchical LLMs-based Agent
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于层次结构的LLMs代理
- en: 'LLMs are known for struggling with long-horizon planning tasks. A natural way
    to alleviate this issue is by decomposing the tasks into shorter-horizon subtasks.
    We show our hierarchical LLMs-based agent framework in Figure [1](#S3.F1 "Figure
    1 ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization").
    We finetune pretrained LLMs to output predictions for subgoals given the general
    task goal, and finetune another LLM to output predictions for low-level actions
    given the subgoals. Specifically, we parameterize each subgoal with a high-level
    action type $h$, e.g. “Heat Cup”.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMs 在长远规划任务中常常面临困难。缓解这一问题的自然方法是将任务分解为短期子任务。我们在图 [1](#S3.F1 "Figure 1 ‣ 3 Method
    ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization") 中展示了我们的基于层次结构的LLMs代理框架。我们对预训练的LLMs进行微调，以根据一般任务目标输出子目标的预测，然后对另一个LLM进行微调，以根据子目标输出低级动作的预测。具体而言，我们用高层次的动作类型$h$对每个子目标进行参数化，例如“加热杯子”。'
- en: 'We find this subgoal decomposition design especially beneficial for training
    embodied agents that directly use LLMs as their policies since: 1\. Subgoals with
    a fixed form instead of the free-form language from the dataset enable us to better
    infer the preference signals between two possible responses (see Section [3.4](#S3.SS4
    "3.4 Environment Preference Optimization ‣ 3 Method ‣ EPO: Hierarchical LLM Agents
    with Environment Preference Optimization")). 2\. It functions as a translation
    of the original subgoal instructions described in natural language. E.g., we find
    that in practice, in ALFRED, one of the subgoal instructions given be the dataset
    is “Then, pick up the dog from the desk”. However, there’s no dog in the room
    and the “dog” in the instruction actually refers to the statue that looks like
    a dog. Thus our subgoal decomposition outputs two subgoals “Moveto desk” and “Pickup
    statue”, which correct the mistake in the dataset and also make the subgoals more
    concise for the low-level policy (LLM) to infer the grounding actions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现这种子目标分解设计对于训练直接使用 LLM 作为其策略的具身智能体特别有利，因为：1\. 固定形式的子目标而不是数据集中的自由形式语言使我们能够更好地推断两个可能回应之间的偏好信号（见第
    [3.4](#S3.SS4 "3.4 环境偏好优化 ‣ 3 方法 ‣ EPO：具有环境偏好优化的层级 LLM 智能体") 节）。2\. 它作为原始子目标指令的自然语言描述的翻译。例如，在
    ALFRED 实践中，我们发现数据集中给出的一个子目标指令是“然后，从桌子上拿起狗”。然而，房间里没有狗，“狗”实际上指的是一只看起来像狗的雕像。因此，我们的子目标分解输出两个子目标“移动到桌子”和“捡起雕像”，这纠正了数据集中的错误，并使子目标对低级策略
    (LLM) 来推断基础动作更加简洁。
- en: For the low-level interaction module, the agent is given the subgoal decomposition
    output from the high-level module, and autoregressively outputs a sequence of
    low-level actions $a$ to reach the given subgoal, all in the form of natural language.
    The low-level agent will output a  token if it thinks the subgoal is fulfilled
    - the language-model-based policy outputs a sequence of actions and we switch
    to the next subgoal once these actions are all executed. One can expect the agent
    complete the given task if its subgoal decomposition module can predict the subgoal
    sequence correctly and for each subgoal, the skill module can output the correct
    low-level action sequences.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于低级交互模块，智能体从高级模块中获得子目标分解输出，并自回归地输出一系列低级动作 $a$ 以实现给定的子目标，所有动作形式为自然语言。如果低级智能体认为子目标已完成，它将输出一个
     标记——语言模型基础的策略输出一系列动作，并在这些动作全部执行后切换到下一个子目标。如果子目标分解模块能够正确预测子目标序列，并且每个子目标的技能模块能够输出正确的低级动作序列，预计智能体能够完成给定任务。
- en: '![Refer to caption](img/e5c4b334916b73a682aa613cc6293ddb.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5c4b334916b73a682aa613cc6293ddb.png)'
- en: 'Figure 2: An illustration of our pipeline to train reward model for grounding
    environment feedback with human instructions. We supervisedly train the reward
    model given the annotated data. Then we use the reward model to label unannotated
    data to obtain the preference relations. Then we form the EPO datasets and optimize
    our agent policies using the proposed EPO algorithm.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们用于训练奖励模型的流程图，该模型用于将环境反馈与人工指令对接。我们在给定标注数据的情况下监督训练奖励模型。然后，我们使用奖励模型对未标注数据进行标记，以获取偏好关系。接着，我们形成
    EPO 数据集，并使用所提出的 EPO 算法优化我们的智能体策略。
- en: 3.3 Reward modeling from Environment Feedback
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 从环境反馈中建模奖励
- en: 'One of the key motivations of this paper is to bypass the complex human-based
    reward engineering and learn to automatically generate feedback signals for a
    diverse set of unannotated tasks that can help train the LLM-based agent. To this
    end, we propose an approach to learn a reward model that is able to generate feedback
    signals from the multimodal observations of the environment. We show the proposed
    Reward Modeling and EPO training framework in Figure [2](#S3.F2 "Figure 2 ‣ 3.2
    Hierarchical LLMs-based Agent ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的一个关键动机是绕过复杂的人类奖励工程，并学习自动生成反馈信号，以帮助训练 LLM 基础的智能体。为此，我们提出了一种方法来学习一个能够从环境的多模态观察中生成反馈信号的奖励模型。我们在图
    [2](#S3.F2 "图 2 ‣ 3.2 基于层级 LLM 的智能体 ‣ 3 方法 ‣ EPO：具有环境偏好优化的层级 LLM 智能体") 中展示了所提出的奖励建模和
    EPO 训练框架。
- en: Environment Feedback. We consider two types of environment feedback that an
    embodied agent can typically receive. The first one is visual positional feedback,
    i.e., each timestep the agent will receive a visual observation (image) describing
    the current environment, and we apply pretrained vision models to retrieve visual
    positional feedback $V$ in the form of boolean values or natural language. For
    example, our agent could attempt to “Pick up Cup”, then it will receive a boolean
    value indicating if its action succeeded.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 环境反馈。我们考虑到一个具身智能体通常可以接收到的两种类型的环境反馈。第一种是视觉位置反馈，即每个时间步，智能体会收到描述当前环境的视觉观察（图像），我们应用预训练的视觉模型以布尔值或自然语言的形式获取视觉位置反馈
    $V$。例如，我们的智能体可能尝试“拾起杯子”，然后它将收到一个布尔值，指示其动作是否成功。
- en: Reward Modeling. In order to unify the feedback information, we symbolically
    represent them all in language if they are in the form of labels. We denote the
    language represented feedback information as $F$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励建模。为了统一反馈信息，如果这些信息以标签的形式存在，我们用语言进行符号表示。我们将语言表示的反馈信息记作 $F$。
- en: '|  | $\hat{r}=R_{\rho}(F,T,P)$ |  | (1) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{r}=R_{\rho}(F,T,P)$ |  | (1) |'
- en: To train this reward model, we construct positive pairs based on whether the
    proposed output is correct with respect to the task input and assign them with
    high rewards. Similarly we construct negative pairs with incorrect proposed output
    and low rewards. For instance, if the visual positional feedback we get from the
    environment after symbolic representation $F$, but the proposed answer is randomly
    chosen from possible outputs, it can be “Pick up object cup”. In this way, we
    construct a synthetic dataset that maps the environment feedback, task specifications,
    and proposed answers to reward values. Then we train the reward model using the
    cross-entropy loss.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这个奖励模型，我们基于提出的输出是否与任务输入相符来构造正样本对，并为其分配高奖励。同样，我们构造负样本对，其中包括不正确的提出输出和低奖励。例如，如果我们从环境中获得的视觉位置反馈在符号表示
    $F$ 后，但提出的答案是从可能的输出中随机选择的，比如“拾起物体杯子”。通过这种方式，我们构建了一个合成数据集，将环境反馈、任务规范和提出的答案映射到奖励值。然后，我们使用交叉熵损失训练奖励模型。
- en: 3.4 Environment Preference Optimization
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 环境偏好优化
- en: With the trained reward model, we can leverage the unannotated dataset by evaluating
    our agent’s proposed subgoals or low-actions according to the given environment
    feedback and task specification. We first pretrain the hierarchical LLM modules
    on the annotated dataset. Then on the unannotated dataset, we use our reward model
    to evaluate the LLM modules’ outputs and rank them according to the estimated
    reward. After that, we will have a ranking of the outputs $(p_{1},p_{2},\ldots,p_{n})$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练好的奖励模型，我们可以利用未标注的数据集，通过根据给定的环境反馈和任务规范评估智能体提出的子目标或低级动作。我们首先在标注数据集上预训练分层的
    LLM 模块。然后在未标注数据集上，我们使用我们的奖励模型评估 LLM 模块的输出，并根据估计的奖励对其进行排序。之后，我们将得到一个输出的排名 $(p_{1},p_{2},\ldots,p_{n})$。
- en: 'From the response ranking, we can construct a preference dataset $\mathcal{D}=\{(F_{1},T_{1},p_{w1},p_{l1}),(F_{2},T_{2},p_{w2},p_{l2}),\ldots\}$
    is the less likely one. Given that the environment feedback and our reward model
    labeling might not be perfect, especially under the circumstance of insufficient
    labeled data, we propose Environment Preference Optimization (EPO) which combines
    DPO (Rafailov et al., [2023](#bib.bib31)) training with an token-level alignment
    loss. We provide additional token-level constraint while preserving the learning
    of preference relations. The training objective is as below:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从响应排名中，我们可以构建一个偏好数据集 $\mathcal{D}=\{(F_{1},T_{1},p_{w1},p_{l1}),(F_{2},T_{2},p_{w2},p_{l2}),\ldots\}$
    是不太可能的。鉴于环境反馈和我们的奖励模型标记可能不完美，特别是在标注数据不足的情况下，我们提出了环境偏好优化（EPO），它将 DPO (Rafailov
    et al., [2023](#bib.bib31)) 训练与令牌级别对齐损失相结合。我们在保持偏好关系学习的同时提供了额外的令牌级约束。训练目标如下：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: ', where'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中
- en: '|  |  | $\displaystyle\mathcal{L}_{D}=-\mathbb{E}_{(T,p_{w},p_{l})\sim\mathcal{D}}\Big{[}\log\sigma\Big{(}\beta\log\frac{\pi_{\theta}(p_{w}\mid
    T)}{\pi_{\text{sup}}(p_{w}\mid T)}$ |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathcal{L}_{D}=-\mathbb{E}_{(T,p_{w},p_{l})\sim\mathcal{D}}\Big{[}\log\sigma\Big{(}\beta\log\frac{\pi_{\theta}(p_{w}\mid
    T)}{\pi_{\text{sup}}(p_{w}\mid T)}$ |  | (3) |'
- en: '|  |  | $\displaystyle-\beta\log\frac{\pi_{\theta}(p_{l}\mid T)}{\pi_{\text{sup}}(p_{l}\mid
    T)}\Big{)}\Big{]}.$ |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\beta\log\frac{\pi_{\theta}(p_{l}\mid T)}{\pi_{\text{sup}}(p_{l}\mid
    T)}\Big{)}\Big{]}.$ |  |'
- en: '$\pi_{\theta}$. With the alignment loss (first term in Eqn [2](#S3.E2 "In 3.4
    Environment Preference Optimization ‣ 3 Method ‣ EPO: Hierarchical LLM Agents
    with Environment Preference Optimization")), we guide the optimization process
    to reduce the algorithmic instability rises especially when we train with a large
    amount of unlabeled data. In this way, we let the model learn the preference relation
    between answers but also align towards the most correct outputs with parameters
    in given format since it does the reward modeling and the token level optimization
    at the same time. Note that in practice, we apply EPO to both high- and low-level
    policies’ training process.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '$\pi_{\theta}$。通过对齐损失（公式[2](#S3.E2 "In 3.4 Environment Preference Optimization
    ‣ 3 Method ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization")中的第一个项），我们指导优化过程以减少算法不稳定性，尤其是在使用大量未标记数据进行训练时。这样，我们让模型学习答案之间的偏好关系，同时还对齐最正确的输出，因为它同时进行奖励建模和令牌级别优化。请注意，在实际操作中，我们将EPO应用于高层次和低层次策略的训练过程。'
- en: 4 Experimental Details
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验细节
- en: 4.1 Environment
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 环境
- en: We conduct experiments on ALFRED (Shridhar et al., [2020a](#bib.bib33)), a popular
    household simulation environment based on AI2-THOR (Kolve et al., [2017](#bib.bib18))
    for embodied agents. It consists of 120 indoor simulations of different room types.
    The official expert demonstration dataset consist of 8055 task demonstration annotated
    with 25,743 natural language instructions in English. The entire dataset is split
    into 21023 instructions in training set, 820 in seen validation set whose environment
    scenes are shared with those in the training set, 821 in unseen validation whose
    environment scenes are not available in training set. Only the task instructions
    in training and validation set are paired with the subgoal and low-level action
    annotations. Subgoals and actions annotations are in the form of structured natural
    language. In this environment, our agent receives egocentric visual observation
    in RGB, and render low-level actions to interact with the environment. The low-level
    action space consists of 12 discrete action types and 82 discrete object types.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在ALFRED（Shridhar et al., [2020a](#bib.bib33)）上进行实验，这是一个基于AI2-THOR（Kolve et
    al., [2017](#bib.bib18)）的流行家庭模拟环境，适用于具身智能体。它包含120个不同房间类型的室内模拟。官方专家演示数据集包含8055个任务演示，标注了25,743条英语自然语言指令。整个数据集分为21023条训练集指令，820条已见验证集指令（这些环境场景与训练集中的场景共享），821条未见验证集指令（这些环境场景在训练集中不可用）。只有训练和验证集中的任务指令与子目标和低级动作注释配对。子目标和动作注释以结构化自然语言的形式存在。在这个环境中，我们的智能体接收RGB的自我中心视觉观测，并执行低级动作与环境互动。低级动作空间由12种离散动作类型和82种离散对象类型组成。
- en: 4.2 Implementation details
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实施细节
- en: We use pretrained RCNN as the object detection model and Mask-RCNN as the segmentation
    model (He et al., [2017](#bib.bib12)). For representing visual information, we
    also want to study how visual detail information (e.g. image captions) could contribute
    as a form of environment feedback. Therefore, we use BLIP-2 (Li et al., [2023](#bib.bib22))
    as our image captioning model and we apply it at the view-points where we can
    interact with the objects.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用预训练的RCNN作为目标检测模型，使用Mask-RCNN作为分割模型（He et al., [2017](#bib.bib12)）。为了表示视觉信息，我们还想研究视觉细节信息（例如图像说明）如何作为环境反馈的形式。因此，我们使用BLIP-2（Li
    et al., [2023](#bib.bib22)）作为我们的图像描述模型，并将其应用于我们可以与对象交互的视角。
- en: For both levels of our agent modules, and reward models, we use Llama2-7B (Touvron
    et al., [2023](#bib.bib42)) as the large language model backbone and use LoRA (Hu
    et al., [2022](#bib.bib13)) to efficiently finetune the language models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们智能体模块和奖励模型的两个层级，我们使用Llama2-7B（Touvron et al., [2023](#bib.bib42)）作为大型语言模型骨干，并使用LoRA（Hu
    et al., [2022](#bib.bib13)）来高效地微调语言模型。
- en: Agent Learning. In order to validate the effectiveness of our framework in learning
    from unannotated dataset, we split the annotated trained dataset into a labeled
    dataset for which we have access to the annotated labels and a unlabeled dataset
    for which we have only access to the task specifications without labels, to mimic
    the real world scenario where we have only limited annotated expert demonstrations
    but can access to many new task specifications. On the unlabeled dataset, we use
    our reward model trained on the labeled dataset to inference reward for each possible
    outputs. Then we form the environment preference dataset based on the rewards
    of the outputs. More details about our experimental setting can be found in the
    appendix.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 代理学习。为了验证我们框架在从未标注数据集中学习的有效性，我们将标注的训练数据集分为带标签的数据集（我们可以访问标注标签）和未标注的数据集（我们只能访问任务规范而没有标签），以模拟现实世界场景，在这种场景中，我们只有有限的标注专家示范，但可以访问许多新的任务规范。在未标注的数据集上，我们使用在标注数据集上训练的奖励模型来推断每个可能输出的奖励。然后，我们根据输出的奖励形成环境偏好数据集。有关我们实验设置的更多细节可以在附录中找到。
- en: 5 Results
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: '|  | Success Rate | GC | PLWSR | PLWGC |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 成功率 | GC | PLWSR | PLWGC |'
- en: '|  Model | Unseen | Seen | Unseen | Seen | Unseen | Seen | Unseen | Seen |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 未见 | 已见 | 未见 | 已见 | 未见 | 已见 | 未见 | 已见 |'
- en: '| HLSM Blukis et al. ([2022](#bib.bib2)) | 0.2027 | 0.2994 | 0.3031 | 0.4121
    | 0.0555 | 0.0874 | 0.0999 | 0.1458 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| HLSM Blukis等 ([2022](#bib.bib2)) | 0.2027 | 0.2994 | 0.3031 | 0.4121 | 0.0555
    | 0.0874 | 0.0999 | 0.1458 |'
- en: '| FILM Min et al. ([2021](#bib.bib27)) | 0.2780 | 0.2883 | 0.3852 | 0.3955
    | 0.1132 | 0.1127 | 0.1513 | 0.1559 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| FILM Min等 ([2021](#bib.bib27)) | 0.2780 | 0.2883 | 0.3852 | 0.3955 | 0.1132
    | 0.1127 | 0.1513 | 0.1559 |'
- en: '| EPA Liu et al. ([2022](#bib.bib24)) | 0.3607 | 0.3996 | 0.3954 | 0.4414 |
    0.0292 | 0.0256 | 0.0391 | 0.0347 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| EPA Liu等 ([2022](#bib.bib24)) | 0.3607 | 0.3996 | 0.3954 | 0.4414 | 0.0292
    | 0.0256 | 0.0391 | 0.0347 |'
- en: '| Prompter Inoue and Ohashi ([2022](#bib.bib16)) | 0.4572 | 0.5323 | 0.5876
    | 0.6343 | 0.2076 | 0.2581 | 0.2622 | 0.3072 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Prompter Inoue和Ohashi ([2022](#bib.bib16)) | 0.4572 | 0.5323 | 0.5876 | 0.6343
    | 0.2076 | 0.2581 | 0.2622 | 0.3072 |'
- en: '| CAPEAM Kim et al. ([2023](#bib.bib17)) | 0.5036 | 0.5258 | 0.6140 | 0.6098
    | 0.2159 | 0.2309 | 0.2531 | 0.2710 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| CAPEAM Kim等 ([2023](#bib.bib17)) | 0.5036 | 0.5258 | 0.6140 | 0.6098 | 0.2159
    | 0.2309 | 0.2531 | 0.2710 |'
- en: '| EPO (ours) | 0.6235 | 0.6479 | 0.6752 | 0.7230 | 0.5199 | 0.5692 | 0.6415
    | 0.6620 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| EPO（我们的） | 0.6235 | 0.6479 | 0.6752 | 0.7230 | 0.5199 | 0.5692 | 0.6415 |
    0.6620 |'
- en: 'Table 1: Comparison with SOTA methods on ALFRED test set. GC stands for “goal-conditioned”.
    PLW stands for “path length weighted”. We get the data of the baselines from ALFRED’s
    public leaderboard.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：与ALFRED测试集上的SOTA方法比较。GC代表“目标条件”。PLW代表“路径长度加权”。我们从ALFRED的公开排行榜中获取基线数据。
- en: In this section, we first compare the overall performance of our framework with
    the state-of-the-art methods on ALFRED public leaderboard and then modularly study
    the components of our framework. We obtain all the results following the standard
    setting in ALFRED where we first let the agent learn from the given dataset offline,
    and then test the the online rollout performance of the learned policies (modules)
    on the given set of new test tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将我们框架的整体性能与ALFRED公开排行榜上的最先进方法进行比较，然后模块化地研究我们框架的组件。我们按照ALFRED中的标准设置获得所有结果，其中我们首先让代理从给定的数据集中离线学习，然后在给定的新测试任务集上测试学习到的策略（模块）的在线滚动性能。
- en: 5.1 Comparison with SOTA on ALFRED
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 与ALFRED上的SOTA比较
- en: 'To demonstrate the effectiveness of our framework, we compare the performance
    of the proposed algorithm to existing works on ALFRED public leaderboard on the
    hold out test set. Here we use the best setup for all our module. That means we
    use the subgoal decomposition module and interaction module both trained on environment
    feedback with reward modeling and EPO. In Table [1](#S5.T1 "Table 1 ‣ 5 Results
    ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization"), our
    method significantly outperforms previous work over 12.0% on unseen test tasks
    while achieving SOTA performance on both unseen and seen scenarios in all metrics,
    indicating the effectiveness of our approach. Moreover, our method achieves significant
    superior performance on path length weighted (PLW) metrics, which indicates the
    efficiency of our method in completing the tasks in fewer steps. It is worth mentioning
    that our approach does not use semantic voxel map (Shridhar et al., [2020b](#bib.bib34)),
    which requires the access of environment meta data. Our approach uses agent exploration
    (Appendix [B](#A2 "Appendix B Additional Algorithm Details ‣ EPO: Hierarchical
    LLM Agents with Environment Preference Optimization")) to obtain object location
    information which generalizes better to real world scenarios without the meta
    information defined in simulators.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示我们框架的有效性，我们将提出的算法的表现与 ALFRED 公共排行榜上的现有工作进行了比较。这里我们使用了我们所有模块的最佳设置。这意味着我们使用了基于环境反馈和奖励建模的子目标分解模块和交互模块以及
    EPO。在表 [1](#S5.T1 "表 1 ‣ 5 结果 ‣ EPO: 基于环境偏好的分层 LLM 代理")中，我们的方法在未见测试任务上的表现比以往工作高出
    12.0% 以上，同时在所有指标上在未见和已见场景中都达到了 SOTA 表现，表明了我们方法的有效性。此外，我们的方法在路径长度加权（PLW）指标上也表现出显著的优越性，这表明我们的方法在用更少的步骤完成任务方面更高效。值得一提的是，我们的方法没有使用需要访问环境元数据的语义体素地图（Shridhar
    et al., [2020b](#bib.bib34)），而是使用代理探索（附录 [B](#A2 "附录 B 额外算法细节 ‣ EPO: 基于环境偏好的分层
    LLM 代理")）来获取物体位置信息，这在没有模拟器中定义的元信息的实际场景中表现更好。'
- en: 5.2 How well does EPO learn from unannotated data?
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 EPO 从未标注数据中学习的效果如何？
- en: Environment preference optimization enhances the agent’s performance via training
    on the unannotated data. We compare to Supervised Fine-Tuning (SFT), where we
    directly prepend the environment feedback to task information and train only use
    the annotated dataset. To study whether our proposed framework can further improve
    itself through learning from unannotated dataset, we consider three data split.
    First, full/No-Split means we use the entire annotated ALFRED dataset. Second,
    90/10 means we use 90% of the demonstration with their annotations and 10% of
    the demonstration without annotation. Lastly, 10/90 refers to the split where
    only 10% of the data we use is annotated and 90% is unannotated. We can see that
    in all three setups, our method based on environment preference optimization outperforms
    supervised fine-tuning. As we increase the amount of unannotated data, one can
    observe that our framework start to show more significant superior performance
    than supervised fine-tuning. This trend of our proposed EPO performing better
    when there exists more unannotated data indicates that the data efficiency and
    potential of EPO in real application scenarios, as data efficiency is one of the
    most important problems for learning from demonstrations in practice.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 环境偏好优化通过对未标注数据进行训练来提升代理的表现。我们与监督微调（SFT）进行了比较，SFT 是直接将环境反馈添加到任务信息中，并仅使用标注数据集进行训练。为了研究我们提出的框架是否能够通过学习未标注数据进一步改进自身，我们考虑了三种数据分割方式。首先，full/No-Split
    表示我们使用整个标注的 ALFRED 数据集。其次，90/10 表示我们使用 90% 的有标注演示和 10% 的未标注演示。最后，10/90 指的是只有 10%
    的数据是有标注的，而 90% 是未标注的。在这三种设置中，我们的方法基于环境偏好优化的表现均优于监督微调。随着未标注数据量的增加，我们可以观察到我们的框架开始显示出比监督微调更显著的优越性能。这一趋势表明，当存在更多未标注数据时，EPO
    的数据效率和潜力在实际应用场景中更为显著，因为数据效率是实践中从演示中学习的一个重要问题。
- en: '|  Learning | Data Split | Unseen | Seen |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  学习 | 数据分割 | 未见 | 已见 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SFT | full | 0.5383 | 0.4939 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SFT | full | 0.5383 | 0.4939 |'
- en: '| EPO | full | 0.5481 | 0.5024 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| EPO | full | 0.5481 | 0.5024 |'
- en: '| SFT | 90/10 | 0.5286 | 0.4841 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| SFT | 90/10 | 0.5286 | 0.4841 |'
- en: '| EPO | 90/10 | 0.5445 | 0.4988 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| EPO | 90/10 | 0.5445 | 0.4988 |'
- en: '| SFT | 10/90 | 0.4689 | 0.4305 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SFT | 10/90 | 0.4689 | 0.4305 |'
- en: '| EPO | 10/90 | 0.5091 | 0.4668 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| EPO | 10/90 | 0.5091 | 0.4668 |'
- en: 'Table 2: Comparing different learning paradigms on validation dataset.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：比较不同学习范式在验证数据集上的表现。
- en: '![Refer to caption](img/522b6058a2cbc79df159202fb0f406e0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/522b6058a2cbc79df159202fb0f406e0.png)'
- en: 'Figure 3: An visual illustration of how EPO improved both high-level subgoal
    decomposition policy and the low-level interaction policy. In the left figure,
    we present the difference between a baseline high-level policy and a EPO trained
    counterpart. We observe that the latter one can correctly figure out the subgoal.
    In the right figure, we present the difference between a baseline low-level policy
    and a EPO trained counterpart. We observe that the latter one can conduct post
    adjustment to successfully execute the actions.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：展示了EPO如何改进高层子目标分解策略和低层交互策略的视觉说明。在左图中，我们展示了基线高层策略和EPO训练后的对应策略之间的差异。我们观察到后者能够正确地识别子目标。在右图中，我们展示了基线低层策略和EPO训练后的对应策略之间的差异。我们观察到后者能够进行后期调整以成功执行动作。
- en: 5.3 How well do different environment feedbacks help decision making?
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 不同环境反馈对决策的帮助如何？
- en: 'Reward modeling can help improve low-level interaction module. Previous work
    on ALFRED (Min et al., [2021](#bib.bib27)) makes the hypothesis that, ALFRED’s
    low-level action dynamics to accomplish the interaction subgoals are quite deterministic
    and can potentially be handled with a deterministic program. We consider the comparison
    between our learning-based interaction module (LLM) against the hard-coded deterministic
    program. Here we use the same subgoal decomposition policy which is supervised
    fine-tuned with the environment feedback and only change the interaction module
    for a fair comparison. As shown in Table [3](#S5.T3 "Table 3 ‣ 5.3 How well do
    different environment feedbacks help decision making? ‣ 5 Results ‣ EPO: Hierarchical
    LLM Agents with Environment Preference Optimization"), with reward modeling and
    EPO training, our LLM-based interaction module is able to achieve better performance
    than the hard-coded program. We also observe that without reward modeling, our
    interaction module fails to achieve comparable result with respect to the deterministic
    program due to the inaccuracy in choosing low-level actions. We find that since
    the interaction module in this setup is only trained to imitate previous action
    trajectories, it fails on the test tasks when the setup is different from the
    training settings. For example, we would expect the agent to first “open the drawer"
    when the drawer is closed before attempting to “pickup the pen”. However, in training
    data, the majority of “pickup object” actions do not require to open the receptacle
    object first.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '奖励建模可以帮助改进低层交互模块。之前关于ALFRED（Min et al., [2021](#bib.bib27)）的研究提出了这样的假设：ALFRED实现交互子目标的低层动作动态是相当确定的，并且可以用确定性的程序处理。我们比较了基于学习的交互模块（LLM）与硬编码的确定性程序。在这里，我们使用相同的子目标分解策略，该策略经过环境反馈的监督微调，仅改变交互模块以进行公平比较。如表[3](#S5.T3
    "Table 3 ‣ 5.3 How well do different environment feedbacks help decision making?
    ‣ 5 Results ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization")所示，通过奖励建模和EPO训练，我们的LLM基础交互模块能够比硬编码程序实现更好的性能。我们还观察到，缺少奖励建模时，我们的交互模块无法达到与确定性程序相当的结果，因为在选择低层动作时存在不准确性。我们发现，由于此设置中的交互模块仅被训练以模仿之前的动作轨迹，当设置与训练设置不同时，它在测试任务中失败。例如，我们期望代理在尝试“拿起笔”之前首先“打开抽屉”，但在训练数据中，大多数“拿起物体”动作并不要求先打开容器对象。'
- en: '|  Action Policy | Feedback | Reward | Unseen | Seen |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 行动策略 | 反馈 | 奖励 | 未见 | 已见 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Program | - | - | 0.5383 | 0.4939 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 程序 | - | - | 0.5383 | 0.4939 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Model | No | No | 0.2907 | 0.2707 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 否 | 否 | 0.2907 | 0.2707 |'
- en: '| Model | Yes | No | 0.5116 | 0.4744 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 是 | 否 | 0.5116 | 0.4744 |'
- en: '| Model | Yes | Yes | 0.5542 | 0.5341 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 是 | 是 | 0.5542 | 0.5341 |'
- en: 'Table 3: Comparison between static program and learning-based interaction module.
    Feedback indicates whether we include feedback information. Reward indicates whether
    we use SFT or EPO with data gathered during interaction.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：静态程序与基于学习的交互模块的比较。反馈指示是否包括反馈信息。奖励指示我们是否使用SFT或EPO以及在交互过程中收集的数据。
- en: 'Environment feedback can help subgoal decomposition. We use supervised finetuning
    to fine-tune the subgoal decomposition policy with environment feedback and use
    the static program as the interaction module as a fair comparison. Both the learning
    algorithm and the interaction module are the same as the baseline module. As shown
    in Table [4](#S5.T4 "Table 4 ‣ 5.3 How well do different environment feedbacks
    help decision making? ‣ 5 Results ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization"), with either interaction feedback or visual feedback
    or a combination of both, we obtain performance gain on both seen and unseen tasks.
    We also find that a combination of both types of feedback reaches the best performance
    and that the interaction feedback exhibits more benefit for training than only
    using visual feedback. One possible reason is that our image captioning model
    only gives a scene description while the interaction feedback that is more concrete
    indicator on whether the object is a potential candidate for subgoals.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '环境反馈有助于子目标分解。我们使用监督微调来用环境反馈微调子目标分解策略，并使用静态程序作为交互模块作为公平比较。学习算法和交互模块与基线模块相同。如表[4](#S5.T4
    "Table 4 ‣ 5.3 How well do different environment feedbacks help decision making?
    ‣ 5 Results ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization")所示，无论是交互反馈、视觉反馈还是两者的结合，我们在已知任务和未知任务上都取得了性能提升。我们还发现，两种反馈类型的组合达到最佳性能，且交互反馈对训练的益处大于仅使用视觉反馈。一个可能的原因是我们的图像描述模型仅提供场景描述，而交互反馈则是是否对象是子目标潜在候选者的更具体指标。'
- en: '|  Model | interaction | visual | Unseen | Seen |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  模型 | 交互 | 视觉 | 未见 | 已见 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Baseline | No | No | 0.4397 | 0.4036 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 否 | 否 | 0.4397 | 0.4036 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Augmented | Yes | No | 0.5383 | 0.4939 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 增强 | 是 | 否 | 0.5383 | 0.4939 |'
- en: '| Augmented | No | Yes | 0.4738 | 0.4317 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 增强 | 否 | 是 | 0.4738 | 0.4317 |'
- en: '| Augmented | Yes | Yes | 0.5334 | 0.5036 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 增强 | 是 | 是 | 0.5334 | 0.5036 |'
- en: 'Table 4: Comparing different feedback types on validation set. Interaction
    means whether we include interaction feedback when learning the reward model.
    Visual means whether we include visual feedback.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：比较不同反馈类型在验证集上的表现。交互指的是我们在学习奖励模型时是否包括交互反馈。视觉指的是我们是否包括视觉反馈。
- en: 5.4 Qualitative Analysis
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 定性分析
- en: 'In addition to quantitative experiments, we visualize the performance of our
    policies and investigate their effectiveness. Figure [3](#S5.F3 "Figure 3 ‣ 5.2
    How well does EPO learn from unannotated data? ‣ 5 Results ‣ EPO: Hierarchical
    LLM Agents with Environment Preference Optimization")(a) shows a comparison between
    the baseline policy and the EPO-tuned policy. We see that the baseline policy
    outputs subgoal predictions closely following the language but outputs the wrong
    object “cup” that the low-level interaction module cannot process. However, from
    environment feedback we detected “mug” exists. Our EPO-tuned policy is able to
    output the correct parameterization for the subgoal and complete the task. Figure
    [3](#S5.F3 "Figure 3 ‣ 5.2 How well does EPO learn from unannotated data? ‣ 5
    Results ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization")(b)
    shows a comparison between the hard-coded deterministic program and our learning-based
    low-level interaction module. We find that the deterministic program fails because
    although it outputs the action that is nearly correct but the agent is not close
    enough to the object so the action (Putobject) cannot be executed. On the other
    hand, after EPO-tuning our module learn to first output actions to adjust its
    pose, which leads to success interaction with the environment.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '除了定量实验外，我们还可视化了我们策略的表现并调查其有效性。图[3](#S5.F3 "Figure 3 ‣ 5.2 How well does EPO
    learn from unannotated data? ‣ 5 Results ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization")(a) 显示了基线策略和 EPO 调优策略之间的比较。我们看到基线策略输出的子目标预测紧跟语言，但输出了低级交互模块无法处理的错误对象“cup”。然而，从环境反馈中我们检测到存在“mug”。我们的
    EPO 调优策略能够输出正确的子目标参数化并完成任务。图[3](#S5.F3 "Figure 3 ‣ 5.2 How well does EPO learn
    from unannotated data? ‣ 5 Results ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization")(b) 显示了硬编码确定性程序和我们基于学习的低级交互模块之间的比较。我们发现确定性程序失败，因为尽管它输出的动作几乎正确，但代理离对象不够近，因此动作（Putobject）无法执行。另一方面，在
    EPO 调优后，我们的模块学会了首先输出调整姿势的动作，从而成功与环境互动。'
- en: 6 Conclusion
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we presented a hierarchical LLM-based framework for long-horizon
    decision-making tasks, addressing the inherent challenges of extensive planning
    and the need for scalable training signals. By leveraging a reward model that
    integrates multimodal environment feedback, and introducing Environment Preference
    Optimization (EPO), we successfully generated training signals for unannotated
    datasets. Our framework demonstrated state-of-the-art performance on the ALFRED
    benchmark. Future work will focus on exploring the integration of additional types
    of multimodal feedback to further enhance the agent’s decision-making capabilities,
    as well as extending our framework to real world robotics tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于层次化 LLM 的框架用于长期决策任务，解决了广泛规划和可扩展训练信号的固有挑战。通过利用一个集成了多模态环境反馈的奖励模型，并引入环境偏好优化
    (EPO)，我们成功地为未标注的数据集生成了训练信号。我们的框架在 ALFRED 基准测试中表现出最先进的性能。未来的工作将集中在探索整合更多类型的多模态反馈，以进一步增强智能体的决策能力，以及将我们的框架扩展到现实世界的机器人任务中。
- en: Limitations
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: We evaluate the proposed method on ALFRED, where the low-level action space
    is discrete and annotated with language. For some continuous control tasks, the
    action space can be much larger and hard to interpret. Future work will focus
    on exploring the integration of additional types of multimodal feedback to further
    enhance the agent’s decision-making capabilities, as well as extending our framework
    to real world robotics tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 ALFRED 上评估了提出的方法，其中低层次的动作空间是离散的并且用语言进行了注释。对于一些连续控制任务，动作空间可能要大得多且难以解释。未来的工作将集中在探索整合更多类型的多模态反馈，以进一步增强智能体的决策能力，以及将我们的框架扩展到现实世界的机器人任务中。
- en: Acknowledgement
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was conducted using computational resources and services at the Center
    for Computation and Visualization, Brown University. The project was in part supported
    by the Samsung Global Research Outreach program. The authors would like to thank
    Calvin Luo, Tian Yun, as well as the anonymous reviewers for valuable feedbacks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本项工作使用了布朗大学计算与可视化中心的计算资源和服务。该项目部分得到了三星全球研究推广计划的支持。作者感谢 Calvin Luo, Tian Yun
    以及匿名审稿人提供的宝贵反馈。
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691*.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等人 (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
    Hausman, 等人. 2022. 做我能做的，不要做我说的：将语言基于机器人功能. *arXiv 预印本 arXiv:2204.01691*。
- en: Blukis et al. (2022) Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and
    Yoav Artzi. 2022. A persistent spatial semantic representation for high-level
    natural language instruction execution. In *CoRL*.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blukis 等人 (2022) Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, 和 Yoav
    Artzi. 2022. 用于高层自然语言指令执行的持久空间语义表示. 发表在 *CoRL* 上。
- en: 'Brohan et al. (2023) Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar,
    Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander
    Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally
    Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel
    Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor
    Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch,
    Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi,
    Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong T.
    Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun
    Xu, Tianhe Yu, and Brianna Zitkovich. 2023. RT-1: robotics transformer for real-world
    control at scale. In *Robotics: Science and Systems XIX, Daegu, Republic of Korea,
    July 10-14, 2023*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brohan 等人 (2023) Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar,
    Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander
    Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally
    Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel
    Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor
    Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch,
    Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag R. Sanketi,
    Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong
    T. Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu,
    Sichun Xu, Tianhe Yu, 和 Brianna Zitkovich. 2023. RT-1: 规模化现实世界控制的机器人变换器. 发表在 *Robotics:
    Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023* 上。'
- en: Chen et al. (2021) Annie S Chen, Suraj Nair, and Chelsea Finn. 2021. Learning
    generalizable robotic reward functions from" in-the-wild" human videos.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021) Annie S Chen, Suraj Nair, 和 Chelsea Finn. 2021. 从“在野外”的人类视频中学习可泛化的机器人奖励函数。
- en: 'Chevalier-Boisvert et al. (2019) Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
    Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio.
    2019. Babyai: A platform to study the sample efficiency of grounded language learning.
    In *7th International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019*. OpenReview.net.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chevalier-Boisvert et al. (2019) Maxime Chevalier-Boisvert, Dzmitry Bahdanau,
    Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, 和 Yoshua Bengio.
    2019. Babyai: 一个用于研究有根语言学习样本效率的平台。发表于*第七届国际学习表征会议，ICLR 2019，新奥尔良，路易斯安那州，美国，2019年5月6-9日*。OpenReview.net。'
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,
    Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
    Andy Zeng, Igor Mordatch, and Pete Florence. 2023. Palm-e: An embodied multimodal
    language model. In *International Conference on Machine Learning, ICML 2023, 23-29
    July 2023, Honolulu, Hawaii, USA*, volume 202 of *Proceedings of Machine Learning
    Research*, pages 8469–8488\. PMLR.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,
    Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
    Andy Zeng, Igor Mordatch, 和 Pete Florence. 2023. Palm-e: 一个具身的多模态语言模型。发表于*国际机器学习大会，ICML
    2023，2023年7月23-29日，夏威夷檀香山，美国*，第202卷*机器学习研究论文集*，第8469–8488页。PMLR。'
- en: Du et al. (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor
    Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. 2023. Guiding pretraining
    in reinforcement learning with large language models.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor
    Darrell, Pieter Abbeel, Abhishek Gupta, 和 Jacob Andreas. 2023. 利用大型语言模型指导强化学习中的预训练。
- en: Escontrela et al. (2023) Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay
    Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, and Pieter Abbeel.
    2023. Video prediction models as rewards for reinforcement learning. *arXiv preprint
    arXiv:2305.14343*.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Escontrela et al. (2023) Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay
    Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, 和 Pieter Abbeel.
    2023. 视频预测模型作为强化学习的奖励。*arXiv 预印本 arXiv:2305.14343*。
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    Minedojo: Building open-ended embodied agents with internet-scale knowledge. In
    *Thirty-sixth Conference on Neural Information Processing Systems Datasets and
    Benchmarks Track*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, 和 Anima Anandkumar. 2022.
    Minedojo: 构建具有互联网规模知识的开放式具身智能体。发表于*第36届神经信息处理系统大会数据集与基准跟踪*。'
- en: Fu et al. (2024) Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris,
    Nicolas Le Roux, Marc-Alexandre Côté, and Xingdi Yuan. 2024. Language-guided skill
    learning with temporal variational inference. *CoRR*, abs/2402.16354.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2024) Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, George Konidaris,
    Nicolas Le Roux, Marc-Alexandre Côté, 和 Xingdi Yuan. 2024. 语言引导的技能学习与时间变分推断。*CoRR*，abs/2402.16354。
- en: Fu et al. (2023) Haotian Fu, Shangqun Yu, Saket Tiwari, Michael Littman, and
    George Konidaris. 2023. Meta-learning parameterized skills. In *International
    Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,
    USA*, volume 202 of *Proceedings of Machine Learning Research*, pages 10461–10481\.
    PMLR.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. (2023) Haotian Fu, Shangqun Yu, Saket Tiwari, Michael Littman, 和 George
    Konidaris. 2023. 元学习参数化技能。发表于*国际机器学习大会，ICML 2023，2023年7月23-29日，夏威夷檀香山，美国*，第202卷*机器学习研究论文集*，第10461–10481页。PMLR。
- en: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
    2017. Mask r-cnn. In *Proceedings of the IEEE international conference on computer
    vision*, pages 2961–2969.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017) Kaiming He, Georgia Gkioxari, Piotr Dollár, 和 Ross Girshick.
    2017. Mask R-CNN。发表于*IEEE国际计算机视觉会议论文集*，第2961–2969页。
- en: 'Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation
    of large language models. In *The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022*. OpenReview.net.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等（2022）爱德华·J·胡、沈业龙、菲利普·沃利斯、泽元·艾伦-朱、刘元志、沈旺、卢旺和魏朱·陈。2022年。Lora：大型语言模型的低秩适配。发表于*第十届国际学习表征会议，ICLR
    2022，虚拟活动，2022年4月25-29日*。OpenReview.net。
- en: 'Huang et al. (2023) Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun
    Wu, and Li Fei-Fei. 2023. Voxposer: Composable 3d value maps for robotic manipulation
    with language models.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等（2023）黄文龙、王晨、张若寒、李云竹、吴佳俊和李飞飞。2023年。Voxposer：用于机器人操控的可组合3D价值图与语言模型。
- en: 'Huang et al. (2022) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022. Inner monologue: Embodied reasoning through planning with language models.
    In *Conference on Robot Learning*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等（2022）黄文龙、夏飞、肖特、哈里斯·陈、杰基·梁、皮特·弗洛伦斯、安迪·曾、乔纳森·汤普森、伊戈尔·莫达奇、叶夫根·切博塔尔等。2022年。内心独白：通过规划与语言模型进行体现推理。发表于*机器人学习会议*。
- en: 'Inoue and Ohashi (2022) Yuki Inoue and Hiroki Ohashi. 2022. Prompter: Utilizing
    large language model prompting for a data efficient embodied instruction following.
    *arXiv preprint arXiv:2211.03267*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 井上和大桥（2022）井上幸和大桥广树。2022年。Prompter：利用大型语言模型提示进行数据高效的体现指令跟随。*arXiv 预印本 arXiv:2211.03267*。
- en: Kim et al. (2023) Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, and
    Jonghyun Choi. 2023. Context-aware planning and environment-aware memory for instruction
    following embodied agents. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 10936–10946.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等（2023）金炳辉、金进言、金宥英、闵哲洪和崔钟贤。2023年。面向指令跟随体现体的上下文感知规划与环境感知记忆。发表于*IEEE/CVF国际计算机视觉会议论文集*，第10936–10946页。
- en: 'Kolve et al. (2017) Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu,
    Abhinav Gupta, and Ali Farhadi. 2017. AI2-THOR: an interactive 3d environment
    for visual AI. *CoRR*, abs/1712.05474.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科尔夫等（2017）埃里克·科尔夫、鲁兹贝赫·莫塔吉、丹尼尔·戈登、朱雨科、阿比纳夫·古普塔和阿里·法赫迪。2017年。AI2-THOR：一个用于视觉人工智能的互动3D环境。*CoRR*,
    abs/1712.05474。
- en: Kwon et al. (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa
    Sadigh. 2023. Reward design with language models.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权等（2023）权美娜、桑·迈克尔·谢、凯莱莎·布拉德和多尔萨·萨迪赫。2023年。使用语言模型的奖励设计。
- en: 'Lee et al. (2023) Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
    Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. Rlaif:
    Scaling reinforcement learning from human feedback with ai feedback. *arXiv preprint
    arXiv:2309.00267*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2023）哈里森·李、萨姆拉特·帕塔尔、哈桑·曼苏尔、凯莉·卢、托马斯·梅斯纳德、科尔顿·比肖普、维克托·卡布内和阿比纳夫·拉斯托吉。2023年。Rlaif：通过人工智能反馈扩展从人类反馈中学习的强化学习。*arXiv
    预印本 arXiv:2309.00267*。
- en: Levy et al. (2019) Andrew Levy, George Dimitri Konidaris, Robert Platt Jr.,
    and Kate Saenko. 2019. Learning multi-level hierarchies with hindsight. In *7th
    International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019*. OpenReview.net.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利维等（2019）安德鲁·利维、乔治·迪米特里·科尼达里斯、罗伯特·普拉特和凯特·萨恩科。2019年。通过事后观察学习多级层次。发表于*第七届国际学习表征会议，ICLR
    2019，新奥尔良，美国，2019年5月6-9日*。OpenReview.net。
- en: 'Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.
    Blip-2: Bootstrapping language-image pre-training with frozen image encoders and
    large language models. *arXiv preprint arXiv:2301.12597*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2023）李俊楠、李东旭、席尔维奥·萨瓦雷斯和史蒂文·霍伊。2023年。Blip-2：通过冻结图像编码器和大型语言模型引导语言-图像预训练。*arXiv
    预印本 arXiv:2301.12597*。
- en: 'Liu et al. (2023) Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang,
    Joydeep Biswas, and Peter Stone. 2023. LLM+P: empowering large language models
    with optimal planning proficiency. *CoRR*, abs/2304.11477.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023）刘博、姜宇乾、张晓涵、刘强、张世奇、乔伊迪普·比斯瓦斯和彼得·斯通。2023年。LLM+P：通过优化规划能力赋能大型语言模型。*CoRR*,
    abs/2304.11477。
- en: Liu et al. (2022) Xiaotian Liu, Hector Palacios, and Christian Muise. 2022.
    A planning based neural-symbolic approach for embodied instruction following.
    *Interactions*, 9(8):17.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2022）刘晓天、赫克托·帕拉西奥斯和克里斯蒂安·穆伊斯。2022年。一种基于规划的神经符号方法用于体现指令跟随。*Interactions*,
    9(8):17。
- en: 'Ma et al. (2023) Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023.
    Eureka: Human-level reward design via coding large language models. *arXiv preprint
    arXiv:2310.12931*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马等（2023）马烨成、威廉·梁、管志旺、黄德安、奥斯伯特·巴斯塔尼、迪内什·贾亚拉曼、朱雨科、范林熙和阿尼玛·安南德库马尔。2023年。Eureka：通过编码大型语言模型进行人类级别的奖励设计。*arXiv
    预印本 arXiv:2310.12931*。
- en: Mahmoudieh et al. (2022) Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell.
    2022. Zero-shot reward specification via grounded natural language. In *CoRL*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmoudieh 等人（2022）Parsa Mahmoudieh、Deepak Pathak 和 Trevor Darrell。2022年。通过具身自然语言进行零-shot奖励规范。在
    *CoRL*。
- en: 'Min et al. (2021) So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan
    Bisk, and Ruslan Salakhutdinov. 2021. Film: Following instructions in language
    with modular methods. *arXiv preprint arXiv:2110.07342*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等人（2021）So Yeon Min、Devendra Singh Chaplot、Pradeep Ravikumar、Yonatan Bisk
    和 Ruslan Salakhutdinov。2021年。Film：使用模块化方法在语言中跟随指令。*arXiv 预印本 arXiv:2110.07342*。
- en: 'Nachum et al. (2018) Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine.
    2018. Data-efficient hierarchical reinforcement learning. In *Advances in Neural
    Information Processing Systems 31: Annual Conference on Neural Information Processing
    Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada*, pages 3307–3317.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nachum 等人（2018）Ofir Nachum、Shixiang Gu、Honglak Lee 和 Sergey Levine。2018年。数据高效的层次化强化学习。在
    *神经信息处理系统进展 31：2018年神经信息处理系统年会，NeurIPS 2018，2018年12月3-8日，加拿大蒙特利尔*，第3307–3317页。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等。2022年。训练语言模型以根据人类反馈遵循指令。*神经信息处理系统进展*，35:27730–27744。
- en: Pashevich et al. (2021) Alexander Pashevich, Cordelia Schmid, and Chen Sun.
    2021. Episodic transformer for vision-and-language navigation. In *ICCV*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pashevich 等人（2021）Alexander Pashevich、Cordelia Schmid 和 Chen Sun。2021年。用于视觉与语言导航的情节变换器。在
    *ICCV*。
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. 2023. Direct preference optimization:
    Your language model is secretly a reward model. *NeurIPS*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等人（2023）Rafael Rafailov、Archit Sharma、Eric Mitchell、Stefano Ermon、Christopher
    D Manning 和 Chelsea Finn。2023年。直接偏好优化：你的语言模型实际上是奖励模型。*NeurIPS*。
- en: Reed et al. (2022) Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez
    Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky,
    Jackie Kay, Jost Tobias Springenberg, et al. 2022. A generalist agent. *arXiv
    preprint arXiv:2205.06175*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed 等人（2022）Scott Reed、Konrad Zolna、Emilio Parisotto、Sergio Gomez Colmenarejo、Alexander
    Novikov、Gabriel Barth-Maron、Mai Gimenez、Yury Sulsky、Jackie Kay、Jost Tobias Springenberg
    等。2022年。通用智能体。*arXiv 预印本 arXiv:2205.06175*。
- en: 'Shridhar et al. (2020a) Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan
    Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020a. Alfred:
    A benchmark for interpreting grounded instructions for everyday tasks. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    10740–10749.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等人（2020a）Mohit Shridhar、Jesse Thomason、Daniel Gordon、Yonatan Bisk、Winson
    Han、Roozbeh Mottaghi、Luke Zettlemoyer 和 Dieter Fox。2020a年。Alfred：一个用于解释日常任务中具身指令的基准。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第10740–10749页。
- en: 'Shridhar et al. (2020b) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020b. Alfworld: Aligning text and
    embodied environments for interactive learning. In *ICLR*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar 等人（2020b）Mohit Shridhar、Xingdi Yuan、Marc-Alexandre Cote、Yonatan Bisk、Adam
    Trischler 和 Matthew Hausknecht。2020b年。Alfworld：对齐文本和具身环境以进行交互学习。在 *ICLR*。
- en: Silver et al. (2024) Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum,
    Leslie Pack Kaelbling, and Michael Katz. 2024. Generalized planning in PDDL domains
    with pretrained large language models. In *Thirty-Eighth AAAI Conference on Artificial
    Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of
    Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances
    in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada*,
    pages 20256–20264\. AAAI Press.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver 等人（2024）Tom Silver、Soham Dan、Kavitha Srinivas、Joshua B. Tenenbaum、Leslie
    Pack Kaelbling 和 Michael Katz。2024年。使用预训练的大型语言模型在 PDDL 领域进行广义规划。在 *第38届AAAI人工智能会议，AAAI
    2024，第36届人工智能创新应用会议，IAAI 2024，第14届人工智能教育进展研讨会，EAAI 2014，2024年2月20-27日，加拿大温哥华*，第20256–20264页。AAAI出版社。
- en: 'Singh et al. (2023) Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.
    Progprompt: Generating situated robot task plans using large language models.
    In *ICRA*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2023）Ishika Singh、Valts Blukis、Arsalan Mousavian、Ankit Goyal、Danfei
    Xu、Jonathan Tremblay、Dieter Fox、Jesse Thomason 和 Animesh Garg。2023年。Progprompt：使用大型语言模型生成定位的机器人任务计划。在
    *ICRA*。
- en: 'Song et al. (2023a) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. 2023a. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In *ICCV*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2023a) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. 2023a. Llm-planner: 基于大语言模型的少量示例的具身智能体规划。发表于 *ICCV*。'
- en: Song et al. (2023b) Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan
    Shu, and Lei Ma. 2023b. Self-refined large language model as automated reward
    function designer for deep reinforcement learning in robotics. *arXiv preprint
    arXiv:2309.06687*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2023b) Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan
    Shu 和 Lei Ma. 2023b. 自我精炼的大语言模型作为深度强化学习中的自动奖励函数设计器。*arXiv 预印本 arXiv:2309.06687*。
- en: 'Sontakke et al. (2023) Sumedh Anand Sontakke, Jesse Zhang, Séb Arnold, Karl
    Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, and Laurent Itti. 2023. Roboclip:
    One demonstration is enough to learn robot policies. In *NeurIPS*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sontakke et al. (2023) Sumedh Anand Sontakke, Jesse Zhang, Séb Arnold, Karl
    Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn 和 Laurent Itti. 2023. Roboclip:
    一次演示足以学习机器人策略。发表于 *NeurIPS*。'
- en: Stone et al. (2023) Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan,
    Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei
    Xia, Chelsea Finn, and Karol Hausman. 2023. Open-world object manipulation using
    pre-trained vision-language models. In *Conference on Robot Learning, CoRL 2023,
    6-9 November 2023, Atlanta, GA, USA*, volume 229 of *Proceedings of Machine Learning
    Research*, pages 3397–3417\. PMLR.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stone et al. (2023) Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan,
    Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei
    Xia, Chelsea Finn 和 Karol Hausman. 2023. 使用预训练视觉-语言模型的开放世界物体操作。发表于 *2023年机器人学习大会（CoRL
    2023），2023年11月6-9日，亚特兰大，GA，美国*，*机器学习研究论文集*第229卷，页3397–3417。PMLR。
- en: Sutton and Barto (1998) Richard S. Sutton and Andrew G. Barto. 1998. *Reinforcement
    learning - an introduction*. Adaptive computation and machine learning. MIT Press.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton and Barto (1998) Richard S. Sutton 和 Andrew G. Barto. 1998. *强化学习 - 介绍*。自适应计算与机器学习。MIT出版社。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. 2023. Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Valmeekam et al. (2023) Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan,
    and Subbarao Kambhampati. 2023. On the planning abilities of large language models
    - A critical investigation. In *Advances in Neural Information Processing Systems
    36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,
    New Orleans, LA, USA, December 10 - 16, 2023*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valmeekam et al. (2023) Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan
    和 Subbarao Kambhampati. 2023. 大语言模型的规划能力 - 一项关键调查。发表于 *2023年神经信息处理系统进展第36卷：神经信息处理系统年会2023，NeurIPS
    2023，新奥尔良，LA，美国，2023年12月10 - 16日*。
- en: 'Wang et al. (2023) Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian
    Wang, Zackory Erickson, David Held, and Chuang Gan. 2023. Robogen: Towards unleashing
    infinite data for automated robot learning via generative simulation. *CoRR*,
    abs/2311.01455.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian
    Wang, Zackory Erickson, David Held 和 Chuang Gan. 2023. Robogen: 通过生成模拟释放无限数据用于自动化机器人学习。*CoRR*，abs/2311.01455。'
- en: Yu et al. (2023) Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei
    Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever,
    Jan Humplik, et al. 2023. Language to rewards for robotic skill synthesis. *arXiv
    preprint arXiv:2306.08647*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2023) Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei
    Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever,
    Jan Humplik 等. 2023. 从语言到奖励用于机器人技能合成。*arXiv 预印本 arXiv:2306.08647*。
- en: Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar
    Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. *arXiv
    preprint arXiv:2401.10020*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2024) Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar
    Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. *arXiv
    预印本 arXiv:2401.10020*。
- en: 'Zitkovich et al. (2023) Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted
    Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong,
    Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh,
    Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann,
    Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey
    Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov,
    Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander
    Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea
    Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski,
    Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez
    Arenas, and Kehang Han. 2023. RT-2: vision-language-action models transfer web
    knowledge to robotic control. In *Conference on Robot Learning, CoRL 2023, 6-9
    November 2023, Atlanta, GA, USA*, volume 229 of *Proceedings of Machine Learning
    Research*, pages 2165–2183\. PMLR.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zitkovich 等（2023） Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao,
    Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent
    Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet,
    Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao,
    Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee,
    Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian,
    Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol
    Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar
    Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen,
    Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez
    Arenas, and Kehang Han. 2023. RT-2: 视觉-语言-动作模型将网页知识转移到机器人控制中。*机器人学习会议，CoRL 2023，2023年11月6-9日，美国乔治亚州亚特兰大*，*机器学习研究文集*第229卷，页面2165–2183。PMLR。'
- en: Appendix A Symbolic Representation and Prompt Examples
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 符号表示和提示示例
- en: 'In dealing with multimodal feedback information, it is crucial for us to design
    structure prompt to interact the LLMs. Luckily, the task specifications $\tau$,
    subgoal and low-level action annotations are already in the form of text so we
    do not need to further tune them. The visual and interaction feedback however,
    needs to proper symbolically represented. For example, when our object detector
    finds visible objects, our agent will interact with it. If the attempted interaction
    is successful, our agent will receive a boolean value from the system. We would
    describe this event as “action successful” for our low-level policies. In gathering
    the environment feedback, we would just simply append the name of the object to
    the existing object list. Visual feedback, which is the image captioning data,
    is already in the form of text. Figure [4](#A2.F4 "Figure 4 ‣ Appendix B Additional
    Algorithm Details ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization")
    illustrates prompt examples of our pipeline.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '在处理多模态反馈信息时，为了与 LLMs 进行交互，设计结构化提示是至关重要的。幸运的是，任务规范 $\tau$、子目标和低级动作注释已经是文本形式，因此我们不需要进一步调整它们。然而，视觉和交互反馈需要适当的符号表示。例如，当我们的目标检测器找到可见物体时，我们的代理将与其互动。如果尝试的交互成功，我们的代理将从系统中收到一个布尔值。我们将这一事件描述为“动作成功”以用于我们的低级策略。在收集环境反馈时，我们只需将物体名称附加到现有物体列表中。视觉反馈，即图像描述数据，已经是文本形式。图
    [4](#A2.F4 "Figure 4 ‣ 附录 B 额外算法细节 ‣ EPO: 环境偏好优化的层级 LLM 代理") 展示了我们流程的提示示例。'
- en: Algorithm 1 Environment Preference Dataset Generation
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 环境偏好数据集生成
- en: '1:  Input: Task specification $\tau$}10:     Append preference data point to
    environment preference dataset11:  end for'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  输入: 任务规范 $\tau$}10:  将偏好数据点附加到环境偏好数据集中11:  结束 for'
- en: Appendix B Additional Algorithm Details
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外算法细节
- en: 'In Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Symbolic Representation and
    Prompt Examples ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization"),
    we provide the detailed steps of our environment preference data generation process.
    We first infer reward values from possible outputs from the policy using the reward
    model. Then we rank all the possible outputs based on reward. Then we pick the
    output with the highest reward as the chosen prompt and the rest as the rejected
    output, the prompt is environment feedback $f$.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '在算法 [1](#alg1 "Algorithm 1 ‣ 附录 A 符号表示和提示示例 ‣ EPO: 环境偏好优化的层级 LLM 代理") 中，我们提供了环境偏好数据生成过程的详细步骤。我们首先使用奖励模型从策略的可能输出中推断奖励值。然后，我们根据奖励对所有可能的输出进行排名。接着，我们选择奖励最高的输出作为选定的提示，其余的作为被拒绝的输出，提示是环境反馈
    $f$。'
- en: Language Model Training For all our policies, we use pretrained Llama-7B as
    the backbone LLM. It has around 7 billion parameters. All our experiments are
    conducted on NVIDIA A6000 GPU. We use LoRA to efficiently fine-tune the language
    models with the datasets we design. Specifically, we use $r=8$. In all our training,
    we use a batch size of 32\. To train the reward model, we use Llama2 with a classification
    head instead of casual generation. For BLIP-2, we only use the image as input
    to generate the captions. We did try providing additional text in the prompt but
    did not observe any clear benefits to the results.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型训练 对于我们所有的策略，我们使用预训练的 Llama-7B 作为基础 LLM。它大约有 70 亿个参数。我们所有的实验都在 NVIDIA A6000
    GPU 上进行。我们使用 LoRA 高效地微调我们设计的数据集上的语言模型。具体来说，我们使用 $r=8$。在所有的训练中，我们使用的批量大小为 32。为了训练奖励模型，我们使用
    Llama2 结合分类头而不是随意生成。对于 BLIP-2，我们仅使用图像作为输入生成标题。我们确实尝试过在提示中提供额外的文本，但没有观察到结果的明显改善。
- en: ALFRED There are two categories of subgoals, navigation and interaction. We
    use the deterministic navigator provided by (Shridhar et al., [2020b](#bib.bib34)),
    which needs the view-point location to navigate to. However, we did not use environment
    meta information to obtain the view-points for the objects. Our agent exploration
    process is able to successful record possible view-points for successful interaction.
    The only meta information we use is action success and agent inventory. To determine
    the object to navigate to, we use the target object of the next subgoal as the
    navigation target. To interact with objects in ALFRED, one needs to output a interaction
    mask. We do so using the MaskRCNN model provided by (Pashevich et al., [2021](#bib.bib30)).
    We use the checkpoints from Episodic Transformer (Pashevich et al., [2021](#bib.bib30)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ALFRED 有两个子目标类别，导航和交互。我们使用由 (Shridhar et al., [2020b](#bib.bib34)) 提供的确定性导航器，它需要视点位置来进行导航。然而，我们并没有使用环境元信息来获取物体的视点。我们的代理探索过程能够成功记录潜在的视点以实现成功交互。我们使用的唯一元信息是动作成功和代理库存。为了确定要导航的物体，我们将下一个子目标的目标物体作为导航目标。要与
    ALFRED 中的物体交互，需要输出一个交互掩码。我们使用 (Pashevich et al., [2021](#bib.bib30)) 提供的 MaskRCNN
    模型来实现这一点。我们使用了 Episodic Transformer (Pashevich et al., [2021](#bib.bib30)) 的检查点。
- en: Environment exploration In order to receive feedback from the environment, we
    need an structured process of exploration. First, we define the concept of “view-points",
    which indicates the location, direction and camera angle. A view-point is parameterized
    with four variables $x,y,r,h$ indicates the eye level angle of our agents. We
    consider the height of our agent fixed at all time. We explore the environment
    to let the agent visit as much view-points as possible. We allow agents to explore
    all possible locations and “view-points” to interact with the visible objects.
    Through our exploration, we apply object detector to obtain the visible objects.
    We record the object that our agent successfully interacted with. After exploration,
    we will have a “view-point” point map of all objects the agent has interacted
    with.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 环境探索 为了从环境中获得反馈，我们需要一个结构化的探索过程。首先，我们定义“视点”概念，表示位置、方向和相机角度。一个视点由四个变量 $x,y,r,h$
    参数化，表示我们代理的眼睛水平角度。我们认为代理的高度始终固定。我们探索环境，让代理尽可能多地访问视点。我们允许代理探索所有可能的位置和“视点”以与可见物体互动。通过我们的探索，我们应用物体检测器来获取可见物体。我们记录代理成功交互的物体。探索后，我们将拥有一个代理已交互物体的“视点”点图。
- en: Decomposition module The input of our decomposition module is the task instructions
    and the output is generated text that indicates the subgoal prediction. The generated
    text will be post-processed into high-level actions and target objects in the
    form of texts. One could form this problem as a classification task without the
    intermediate text. But we argue that generating free-form language generalizes
    better to environments and tasks when the possible subgoals of our agent are hard
    to be defined in a closed set.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 分解模块 我们的分解模块的输入是任务指令，输出是生成的文本，指示子目标预测。生成的文本将被后处理为高层次的动作和目标物体的文本形式。可以将这个问题视作一个分类任务，而不需要中间文本。但我们认为，当代理的潜在子目标难以在一个封闭的集合中定义时，生成自由形式的语言对于环境和任务的泛化能力更强。
- en: 'Interaction module. After our agent predicts the subgoals, it uses an interaction
    module to output the low-level actions to complete each subgoal sequentially.
    There exists two types of subgoals: navigation and interaction. For a navigation
    subgoal, we use a view-point-based navigation planner with the object location
    information we gained during agent exploration. For interaction subgoals, as noticed
    by previous work (Min et al., [2021](#bib.bib27)), the action sequences required
    to complete them can be quite deterministic and is possible to solved them with
    a static program. Nevertheless, we propose a learning-based method in which our
    model uses a large language model as its backbone. It takes in the subgoal information,
    the interaction feedback from its previous action, and its historical actions
    in completing this subgoal, all symbolically-represented in text and outputs the
    next low-level action. Our model generalize better to the scenarios which action
    dynamics are less deterministic. It predicts the next action based on interaction
    feedback and previous actions in an auto-regressive manner. Later in experiments,
    we show that this learning-based module can be further improved with environment
    feedback and EPO.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 交互模块 在我们的代理预测子目标后，它使用交互模块输出低级动作，以顺序完成每个子目标。子目标有两种类型：导航和交互。对于导航子目标，我们使用基于视角的导航规划器，结合在代理探索过程中获得的物体位置信息。对于交互子目标，正如之前的工作（Min等，[2021](#bib.bib27)）所指出的，完成这些子目标所需的动作序列可以非常确定，并且可能通过静态程序解决。然而，我们提出了一种基于学习的方法，其中我们的模型使用大型语言模型作为其骨干。它接受子目标信息、来自前一个动作的交互反馈，以及完成该子目标的历史动作，这些都以文本的形式符号化表示，并输出下一个低级动作。我们的模型在动作动态不那么确定的场景中表现得更好。它基于交互反馈和先前动作以自回归的方式预测下一个动作。在实验中，我们展示了这种基于学习的模块可以通过环境反馈和EPO进一步改进。
- en: Reward Modeling Recall that our reward model estimates the likelihood of the
    output is correct and form the environment preference dataset through ranking.
    In training the reward model for the subgoal decomposition module, we use the
    annotated dataset to form input consist of environment feedback $F$. In training
    the reward model for interaction module, we gather online data by allowing our
    agent to attempt various pose changes and interactions until it could succeed
    its intended action. Then we record the actions led to successful interaction
    and other unsuccessful actions to form the positive and negative pairs. Then the
    process to form the preference dataset is similar with that of the subgoal decomposition
    module. We did not any AI assistant in writing this paper.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励建模 记住我们的奖励模型通过排序来估计输出的正确性，并形成环境偏好数据集。在为子目标分解模块训练奖励模型时，我们使用带有注释的数据集来形成由环境反馈$F$构成的输入。在为交互模块训练奖励模型时，我们通过允许代理尝试各种姿势变化和交互，直到成功完成预期动作来收集在线数据。然后，我们记录导致成功交互的动作以及其他不成功的动作，以形成正负对。形成偏好数据集的过程与子目标分解模块类似。我们在撰写本文时没有使用任何AI助手。
- en: Baselines We compare the overall performance of our framework with the state-of-the-art
    methods on ALFRED public leaderboard. We obtain all the results following the
    standard setting in ALFRED where we first let the agent learn from the given dataset
    offline, and then test the online rollout performance of the learned policies
    (modules) on the given set of new test tasks. All baselines have access to the
    same amount of information, as this is the standard setting required by ALFRED
    to get a score on the public leaderboard. Thus we believe the comparison with
    all the baselines is fair. We will add more descriptions for each baseline listed
    in the updated version of our paper as suggested. Specifically, HLSM proposes
    to build a persistent spatial semantic representation from natural language instructions.
    FILM involves the creation of a semantic map of the environment and a semantic
    search policy to navigate and interact based on the instructions provided. EPA
    uses a discrete graph representation enriched with new perceptions during exploration,
    allowing the agent to generate new planning problems and recover from action failures.
    Prompter introduces a method that replaces the traditional semantic search module
    in embodied instruction following systems with language model prompting. CAPEAM
    enhances an agent’s ability to perform household tasks by integrating semantic
    context and maintaining the state of objects within the environment.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 基线 我们将我们的框架的整体性能与 ALFRED 公共排行榜上的最新方法进行比较。我们根据 ALFRED 中的标准设置获得所有结果，其中我们首先让代理从给定的数据集中离线学习，然后测试学习到的策略（模块）在一组新的测试任务上的在线滚动性能。所有基线访问相同的信息量，因为这是
    ALFRED 为了在公共排行榜上获得分数所要求的标准设置。因此，我们认为与所有基线的比较是公平的。根据建议，我们将在我们论文的更新版本中添加更多对每个基线的描述。具体而言，HLSM
    提出了从自然语言指令构建持久空间语义表示的方法。FILM 涉及创建环境的语义图并基于提供的指令进行导航和交互的语义搜索策略。EPA 使用在探索过程中通过新感知丰富的离散图表示，允许代理生成新的规划问题并从行动失败中恢复。Prompter
    引入了一种方法，通过语言模型提示替代了具身指令跟随系统中的传统语义搜索模块。CAPEAM 通过整合语义上下文并保持环境中对象的状态来增强代理执行家庭任务的能力。
- en: '| Methods | Success rates |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 成功率 |'
- en: '| --- | --- |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Baseline (without environment feedback) | 0.7409 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 基线（没有环境反馈） | 0.7409 |'
- en: '| EPO (with 10$\%$ annotated data) | 0.9781 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| EPO（使用 10$\%$ 标注的数据） | 0.9781 |'
- en: '| EPO (with fully annotated data) | 0.9905 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| EPO（使用完全标注的数据） | 0.9905 |'
- en: 'Table 5: Results on BabyAI'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：BabyAI 的结果
- en: 'Results on BabyAI We also conduct a set of experiments on BabyAI (Chevalier-Boisvert
    et al., [2019](#bib.bib5)) minibosslevel, which is an environment where an agent
    navigates and interacts in a grid world to achieve a goal described in language.
    As shown in Table [5](#A2.T5 "Table 5 ‣ Appendix B Additional Algorithm Details
    ‣ EPO: Hierarchical LLM Agents with Environment Preference Optimization"), we
    observe that EPO with environment feedback (object type observed by the agent)
    can boost task success rate from 0.7409 to 0.9905 and with 10% of labeled data
    and EPO, our policy can reach 0.9781 task success rate, which is just 0.0124 less
    than using all labeled training data.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'BabyAI 上的结果 我们还在 BabyAI（Chevalier-Boisvert 等，[2019](#bib.bib5)）的 minibosslevel
    环境中进行了一系列实验，这是一个代理在一个网格世界中导航和交互以实现语言描述的目标的环境。如表 [5](#A2.T5 "Table 5 ‣ Appendix
    B Additional Algorithm Details ‣ EPO: Hierarchical LLM Agents with Environment
    Preference Optimization") 所示，我们观察到，具有环境反馈（代理观察到的对象类型）的 EPO 可以将任务成功率从 0.7409 提升至
    0.9905，并且使用 10% 的标记数据和 EPO 时，我们的策略可以达到 0.9781 的任务成功率，这仅比使用所有标记训练数据少 0.0124。'
- en: '![Refer to caption](img/a4b2b712c0786e1b3c77d76b8f9e9029.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a4b2b712c0786e1b3c77d76b8f9e9029.png)'
- en: 'Figure 4: A illustration of prompt to our LLM policies. From top to bottom:
    example of baseline subgoal policy, example of baseline interaction policy, example
    of interaction feedback , example of visual feedback , example of reward model
    training Data, example of Environment Preference Data'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：展示我们 LLM 策略的提示示例。从上到下：基线子目标策略示例，基线交互策略示例，交互反馈示例，视觉反馈示例，奖励模型训练数据示例，环境偏好数据示例
