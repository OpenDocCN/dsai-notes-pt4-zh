- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:49:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:49:36'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Do LLM Agents Have Regret? A Case Study in Online Learning and Games
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 代理是否会有遗憾？在线学习和游戏中的案例研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.16843](https://ar5iv.labs.arxiv.org/html/2403.16843)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.16843](https://ar5iv.labs.arxiv.org/html/2403.16843)
- en: 'Chanwoo Park    Chanwoo Park^‡^‡‡Equal Contribution    Xiangyu Liu^†^†footnotemark:
       Asuman Ozdaglar    Kaiqing Zhang ^†^†C. Park and A. Ozdaglar are with Massachusetts
    Institute of Technology, Cambridge, MA, 02139. X. Liu and K. Zhang are with the
    University of Maryland, College Park, MD, 20742. E-mails: {cpark97,asuman}@mit.edu,
    {xyliu999,kaiqing}@umd.edu.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chanwoo Park    Chanwoo Park^‡^‡‡平等贡献    Xiangyu Liu^†^†脚注标记:    Asuman Ozdaglar
       Kaiqing Zhang ^†^†C. Park和A. Ozdaglar在麻省理工学院，剑桥，MA，02139。X. Liu和K. Zhang在马里兰大学，College
    Park，MD，20742。电子邮件: {cpark97,asuman}@mit.edu, {xyliu999,kaiqing}@umd.edu.'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large language models (LLMs) have been increasingly employed for (interactive)
    decision-making, via the development of LLM-based autonomous agents. Despite their
    emerging successes, the performance of LLM agents in decision-making has not been
    fully investigated through quantitative metrics, especially in the multi-agent
    setting when they interact with each other, a typical scenario in real-world LLM-agent
    applications. To better understand the limits of LLM agents in these interactive
    environments, we propose to study their interactions in benchmark decision-making
    settings in online learning and game theory, through the performance metric of
    *regret*. We first empirically study the no-regret behaviors of LLMs in canonical
    (non-stationary) online learning problems, as well as the emergence of equilibria
    when LLM agents interact through playing repeated games. We then provide some
    theoretical insights into the no-regret behaviors of LLM agents, under certain
    assumptions on the supervised pre-training and the rationality model of human
    decision-makers who generate the data. Notably, we also identify (simple) cases
    where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret
    behaviors, we propose a novel *unsupervised* training loss of *regret-loss*, which,
    in contrast to the supervised pre-training loss, does not require the labels of
    (optimal) actions. We then establish the statistical guarantee of generalization
    bound for regret-loss minimization, followed by the optimization guarantee that
    minimizing such a loss may automatically lead to known no-regret learning algorithms.
    Our further experiments demonstrate the effectiveness of our regret-loss, especially
    in addressing the above “regrettable” cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）越来越多地用于（交互式）决策，通过开发基于LLM的自主代理。尽管它们在决策中的新兴成功，但LLM代理在决策中的表现尚未通过量化指标充分调查，尤其是在多代理环境中它们相互作用的情况下，这在现实世界的LLM代理应用中是一个典型场景。为了更好地理解LLM代理在这些交互环境中的限制，我们建议通过*遗憾*的表现指标来研究它们在在线学习和博弈论中的基准决策设置中的交互。我们首先实证研究LLM在经典（非平稳）在线学习问题中的无遗憾行为，以及LLM代理通过进行重复游戏时平衡状态的出现。然后，在对监督预训练和生成数据的人类决策者的理性模型进行某些假设下，我们提供了LLM代理无遗憾行为的一些理论见解。值得注意的是，我们还识别出（简单的）情况，其中高级LLM如GPT-4未能达到无遗憾。为了促进无遗憾行为，我们提出了一种新颖的*无监督*训练损失*遗憾损失*，与监督预训练损失不同，它不需要（最佳）行动的标签。然后，我们建立了遗憾损失最小化的统计泛化界限的保证，随后是优化保证，即最小化这种损失可能自动导致已知的无遗憾学习算法。我们的进一步实验展示了我们遗憾损失的有效性，尤其是在解决上述“遗憾”情况方面。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Live Life with No Excuses. Travel with No Regret.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 活出无悔的人生，旅行无悔。
- en: 'Large language models (LLMs) have recently exhibited remarkable reasoning capabilities
    (Bubeck et al., [2023](#bib.bib23); Achiam et al., [2023](#bib.bib3); Wei et al.,
    [2022b](#bib.bib111); Yao et al., [2023a](#bib.bib117)). As a consequence, a burgeoning
    body of work has been investigating the employment of LLMs as central controllers
    for (interactive) decision-making, through the construction of *LLM-based autonomous
    agents* (Hao et al., [2023](#bib.bib46); Shen et al., [2023](#bib.bib94); Yao
    et al., [2023b](#bib.bib118); Shinn et al., [2023](#bib.bib95); Wang et al., [2023c](#bib.bib107);
    Significant Gravitas, [2023](#bib.bib96)). Specifically, the LLM agent interacts
    with the (physical) world in a *dynamic/sequential* way: it uses LLMs as an oracle
    for reasoning, then acts in the environment based on the reasoning and the feedback
    it perceives over time. LLM agent has achieved impressive successes in embodied
    AI (Ahn et al., [2022](#bib.bib6); Huang et al., [2022a](#bib.bib51); Wang et al.,
    [2023a](#bib.bib105)), natural science (Wu et al., [2023](#bib.bib112); Swan et al.,
    [2023](#bib.bib98)), and social science (Park et al., [2022](#bib.bib84), [2023](#bib.bib83))
    applications.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）最近展示了显著的推理能力（Bubeck et al., [2023](#bib.bib23); Achiam et al., [2023](#bib.bib3);
    Wei et al., [2022b](#bib.bib111); Yao et al., [2023a](#bib.bib117)）。因此，越来越多的研究正在探讨将LLMs作为（交互式）决策的核心控制器，通过构建*基于LLM的自主代理*（Hao
    et al., [2023](#bib.bib46); Shen et al., [2023](#bib.bib94); Yao et al., [2023b](#bib.bib118);
    Shinn et al., [2023](#bib.bib95); Wang et al., [2023c](#bib.bib107); Significant
    Gravitas, [2023](#bib.bib96)）。具体来说，LLM代理以*动态/顺序*的方式与（物理）世界互动：它将LLMs用作推理的神谕，然后根据推理和它在时间中感知到的反馈在环境中行动。LLM代理在具身AI（Ahn
    et al., [2022](#bib.bib6); Huang et al., [2022a](#bib.bib51); Wang et al., [2023a](#bib.bib105)）、自然科学（Wu
    et al., [2023](#bib.bib112); Swan et al., [2023](#bib.bib98)）和社会科学（Park et al.,
    [2022](#bib.bib84), [2023](#bib.bib83)）应用中取得了令人印象深刻的成功。
- en: Besides being *dynamic*, another increasingly captivating feature of LLM-based
    decision-making is the involvement of *strategic* interactions, oftentimes among
    multiple LLM agents. For example, it has been continually reported that the reasoning
    capability of LLMs can be improved by interacting with each other through negotiation
    and/or debate games (Fu et al., [2023](#bib.bib40); Du et al., [2023](#bib.bib35));
    LLM agents have now been widely used to *simulate* the strategic behaviors for
    social and economic studies, to understand the emerging behaviors in interactive
    social systems (Aher et al., [2023](#bib.bib4); Park et al., [2023](#bib.bib83)).
    Moreover, LLMs have also exhibited remarkable potential in solving various games
    (Bakhtin et al., [2022](#bib.bib14); Mukobi et al., [2023](#bib.bib79)), and in
    fact, a rapidly expanding literature has employed *repeated games* as a fundamental
    benchmark to understand the strategic behaviors of LLMs (Brookins and DeBacker,
    [2023](#bib.bib20); Akata et al., [2023](#bib.bib8); Fan et al., [2023](#bib.bib38)).
    These exciting empirical successes call for a rigorous examination and understanding
    through a theoretical lens of decision-making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*动态*性，基于LLM的决策的另一个日益引人注目的特征是*战略*互动，通常涉及多个LLM代理。例如，持续有报道指出，通过谈判和/或辩论游戏相互作用可以提高LLMs的推理能力（Fu
    et al., [2023](#bib.bib40); Du et al., [2023](#bib.bib35)）；LLM代理现在已被广泛用于*模拟*社会和经济研究中的战略行为，以理解交互社会系统中的新兴行为（Aher
    et al., [2023](#bib.bib4); Park et al., [2023](#bib.bib83)）。此外，LLMs还在解决各种游戏（Bakhtin
    et al., [2022](#bib.bib14); Mukobi et al., [2023](#bib.bib79)）方面展示了显著的潜力，实际上，迅速扩展的文献已经将*重复游戏*作为理解LLMs战略行为的基本基准（Brookins
    and DeBacker, [2023](#bib.bib20); Akata et al., [2023](#bib.bib8); Fan et al.,
    [2023](#bib.bib38)）。这些令人兴奋的实证成功呼唤通过理论决策视角进行严格的审查和理解。
- en: '*Regret*, on the other hand, has been a core metric in (online) decision-making.
    It measures how “sorry” the decision-maker is, in retrospect, not to have followed
    the best prediction in hindsight (Shalev-Shwartz, [2012](#bib.bib92)). It provides
    not only a sensible way to *evaluate* the intelligence level of online decision-makers,
    but also a quantitative way to measure their *robustness* against arbitrary (and
    possibly adversarial) environments. More importantly, it inherently offers a connection
    to modeling and analyzing strategic behaviors: the long-run interaction of no-regret
    learners leads to certain *equilibria* when they repeatedly play games (Cesa-Bianchi
    and Lugosi, [2006](#bib.bib26)). In fact, *no-regret* learning has served as a
    natural model for predicting and explaining human behaviors in strategic decision-making,
    with experimental evidence (Erev and Roth, [1998](#bib.bib37); Nekipelov et al.,
    [2015](#bib.bib80); Balseiro and Gur, [2019](#bib.bib15)). It has thus been posited
    as an important model of “rational behavior” in playing games (Blum et al., [2008](#bib.bib18);
    Roughgarden, [2015](#bib.bib88); Roughgarden et al., [2017](#bib.bib89)). Thus,
    it is natural to ask:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*遗憾*，另一方面，一直是（在线）决策制定中的核心度量标准。它衡量了决策者在事后“遗憾”没有遵循最佳预测（Shalev-Shwartz，[2012](#bib.bib92)）。它不仅提供了一种*评估*在线决策者智能水平的合理方式，还提供了一种量化测量他们在任意（可能是对抗性）环境下的*鲁棒性*的方式。更重要的是，它本质上提供了建模和分析战略行为的连接：无遗憾学习者的长期互动在重复博弈中导致某些*均衡*（Cesa-Bianchi
    和 Lugosi，[2006](#bib.bib26)）。事实上，*无遗憾*学习已作为预测和解释战略决策中人类行为的自然模型，具有实验证据（Erev 和 Roth，[1998](#bib.bib37)；Nekipelov
    等，[2015](#bib.bib80)；Balseiro 和 Gur，[2019](#bib.bib15)）。因此，它被认为是博弈中“理性行为”的一个重要模型（Blum
    等，[2008](#bib.bib18)；Roughgarden，[2015](#bib.bib88)；Roughgarden 等，[2017](#bib.bib89)）。因此，自然而然地提出：'
- en: '*Can we examine and better understand the online and strategic decision-making'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们能否检验并更好地理解在线和战略决策制定*'
- en: behaviors of LLMs through the lens of *regret*?*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从*遗憾*的角度来看LLM的行为？
- en: 'Acknowledging that LLM(-agents) are extremely complicated to analyze, to gain
    some insights into the question, we focus on benchmark decision-making settings:
    online learning with convex (linear) loss functions, and playing repeated games.
    We summarize our contributions as follows.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到LLM（-agents）分析极为复杂，为了获得对问题的深入见解，我们集中于基准决策制定设置：具有凸（线性）损失函数的在线学习和重复博弈。我们将我们的贡献总结如下。
- en: Contributions.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贡献。
- en: First, we carefully examine the performance of several representative pre-trained
    LLMs in aforementioned benchmark online decision-making settings, in terms of
    *regret*. We observe that oftentimes, LLM agents exhibit no-regret behaviors in
    these (non-stationary) online learning settings, where the loss functions change
    over time either arbitrarily (and even adversarially), or by following some patterns
    with bounded variation, and in playing both representative and randomly generated
    repeated games. For the latter, equilibria will emerge as the long-term behavior
    of the interactions when all LLM agents are no-regret. Second, we provide some
    theoretical insights into the observed no-regret behaviors, based on some hypothetical
    model of the human decision-makers who generate the data, and certain assumptions
    on the *supervised pre-training* procedure, a common practice in training large
    models for decision-making. In particular, we make a connection of pre-trained
    LLMs to the known no-regret algorithm of *follow-the-perturbed-leader* (FTPL)
    under such assumptions. Third, we also identify (simple) cases where advanced
    LLMs as GPT-4 fail to be no-regret. We thus propose a novel *unsupervised* training
    loss, *regret-loss*, which, in contrast to the supervised pre-training loss, does
    not require the *labels* of (optimal) actions. We then establish both statistical
    and optimization guarantees for regret-loss minimization, showing that minimizing
    such a loss may automatically lead to known no-regret learning algorithms. Our
    further experiments demonstrate the effectiveness of regret-loss, especially in
    addressing the above “regrettable” cases.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们仔细检查了几种代表性的预训练LLMs在上述基准在线决策环境中的表现，特别是在*遗憾*方面。我们观察到，LLM代理在这些（非平稳的）在线学习环境中通常表现出无遗憾行为，这些环境中损失函数随时间变化，要么是任意的（甚至是对抗性的），要么是遵循某些具有界限变化的模式，并且在进行代表性和随机生成的重复博弈中。对于后者，当所有LLM代理都是无遗憾时，均衡会作为互动的长期行为出现。其次，我们基于一些假设的人类决策者模型和对*监督预训练*过程的某些假设，提供了一些关于观察到的无遗憾行为的理论见解，这是训练大规模决策模型的常见实践。特别是，我们在这些假设下将预训练LLMs与已知的*跟随扰动领袖*（FTPL）无遗憾算法进行了关联。第三，我们还识别了（简单的）情况下，像GPT-4这样的先进LLMs未能表现出无遗憾。为此，我们提出了一种新颖的*无监督*训练损失，即*遗憾损失*，与监督预训练损失相比，它不需要（最优）动作的*标签*。我们然后建立了遗憾损失最小化的统计和优化保证，表明最小化这种损失可能自动导致已知的无遗憾学习算法。我们的进一步实验展示了遗憾损失的有效性，特别是在解决上述“遗憾”情况方面。
- en: 1.1 Related Work
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 相关工作
- en: LLM(-agent) for decision-making.
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM（-agent）用于决策制定。
- en: The impressive capability of LLMs for *reasoning* (Bubeck et al., [2023](#bib.bib23);
    Achiam et al., [2023](#bib.bib3); Wei et al., [2022b](#bib.bib111), [a](#bib.bib110);
    Srivastava et al., [2023](#bib.bib97); Yao et al., [2023a](#bib.bib117)) has inspired
    a growing line of research on *LLM for (interactive) decision-making*, i.e., an
    LLM-based autonomous agent interacts with the environment by taking actions repeatedly/sequentially,
    based on the feedback it perceives. Some promises have been shown from a *planning*
    perspective (Hao et al., [2023](#bib.bib46); Valmeekam et al., [2023](#bib.bib100);
    Huang et al., [2022b](#bib.bib52); Shen et al., [2023](#bib.bib94)). In particular,
    for embodied AI applications, e.g., robotics, LLMs have achieved impressive performance
    when used as the controller for decision-making (Ahn et al., [2022](#bib.bib6);
    Yao et al., [2023b](#bib.bib118); Shinn et al., [2023](#bib.bib95); Wang et al.,
    [2023c](#bib.bib107); Driess et al., [2023](#bib.bib34); Significant Gravitas,
    [2023](#bib.bib96)). However, the performance of decision-making has not been
    rigorously characterized via the regret metric in these works. Very recently,
    Liu et al. ([2023c](#bib.bib71)) has proposed a principled architecture for LLM-agent,
    with provable regret guarantees in stationary and stochastic decision-making environments,
    under the Bayesian adaptive Markov decision processes framework. In contrast,
    our work focuses on online learning and game-theoretic settings, in potentially
    adversarial and non-stationary environments. Moreover, (first part of) our work
    focuses on *evaluating* the intelligence level of LLM per se in decision-making
    (in terms of the regret metric), while Liu et al. ([2023c](#bib.bib71)) focused
    on *developing* a new architecture that uses LLM as an oracle for reasoning, together
    with memory and specific planning/acting subroutines, *to achieve* sublinear (Bayesian)
    regret, in stationary and stochastic environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在*推理*方面的令人印象深刻的能力（Bubeck等，[2023](#bib.bib23)；Achiam等，[2023](#bib.bib3)；Wei等，[2022b](#bib.bib111)，[a](#bib.bib110)；Srivastava等，[2023](#bib.bib97)；Yao等，[2023a](#bib.bib117)）激发了对*LLM在（交互式）决策制定*方面研究的不断增长，即基于其感知的反馈，LLM驱动的自主代理通过重复/顺序地采取行动与环境进行交互。从*规划*的角度来看，一些承诺已经显现（Hao等，[2023](#bib.bib46)；Valmeekam等，[2023](#bib.bib100)；Huang等，[2022b](#bib.bib52)；Shen等，[2023](#bib.bib94)）。特别是对于具身AI应用，如机器人，当LLM被用作决策控制器时，已取得了令人印象深刻的表现（Ahn等，[2022](#bib.bib6)；Yao等，[2023b](#bib.bib118)；Shinn等，[2023](#bib.bib95)；Wang等，[2023c](#bib.bib107)；Driess等，[2023](#bib.bib34)；Significant
    Gravitas，[2023](#bib.bib96)）。然而，这些研究中决策制定的表现尚未通过遗憾度指标进行严格表征。最近，Liu等（[2023c](#bib.bib71)）在贝叶斯自适应马尔可夫决策过程框架下，提出了一种具有可证明遗憾度保证的LLM-agent的原则性架构。相比之下，我们的工作专注于在线学习和博弈论设置中的潜在对抗性和非平稳环境。此外，我们工作的（第一部分）集中于*评估*LLM在决策制定中的智能水平（以遗憾度指标为标准），而Liu等（[2023c](#bib.bib71)）专注于*开发*一种新的架构，该架构利用LLM作为推理的神谕，并结合记忆和特定的规划/行动子程序，*以实现*在平稳和随机环境下的次线性（贝叶斯）遗憾度。
- en: LLMs in multi-agent environments.
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLMs在多代理环境中的应用。
- en: The interaction of multiple LLM agents has garnered significant attention lately.
    For example, Fu et al. ([2023](#bib.bib40)) showed that LLMs can autonomously
    improve each other in a negotiation game by playing and criticizing each other.
    Similarly, (Du et al., [2023](#bib.bib35); Liang et al., [2023](#bib.bib66); Xiong
    et al., [2023](#bib.bib114); Chan et al., [2024](#bib.bib27); Li et al., [2023c](#bib.bib62))
    showed that multi-LLM *debate* can improve the reasoning and evaluation capabilities
    of the LLMs. Qian et al. ([2023](#bib.bib85)); Schick et al. ([2023](#bib.bib90));
    Wu et al. ([2023](#bib.bib112)) demonstrated the potential of multi-LLM interactions
    and collaboration in software development, writing, and problem-solving, respectively.
    Zhang et al. ([2024](#bib.bib120)) exhibited a similar potential in embodied cooperative
    environments. More formally, multi-LLM interactions have also been investigated
    under a *game-theoretic* framework, to characterize the *strategic* decision-making
    of LLM agents. Bakhtin et al. ([2022](#bib.bib14)); Mukobi et al. ([2023](#bib.bib79))
    and Xu et al. ([2023b](#bib.bib116), [a](#bib.bib115)) have demonstrated the promise
    of LLMs in playing Diplomacy and WereWolf games, respectively, which are both
    language-based games with a mixture of competitive and cooperative agents. Note
    that these works utilized LLM to solve a specific rather than a general game.
    Related to our work, Brookins and DeBacker ([2023](#bib.bib20)); Akata et al.
    ([2023](#bib.bib8)); Lorè and Heydari ([2023](#bib.bib72)); Brookins and DeBacker
    ([2023](#bib.bib20)); Fan et al. ([2023](#bib.bib38)) have also used (repeated)
    matrix games as a benchmark to evaluate the reasoning capability and rationality
    of LLM agents. In contrast to our work, these empirical studies have not formally
    investigated LLM agents using the metric of *regret*, nor through the lenses of
    *online learning* and *equilibrium-computation*, which are all fundamental in
    modeling and analyzing strategic multi-agent interactions. Moreover, our work
    also provides theoretical results to explain and further enhance the no-regret
    property of LLM agents.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 多个LLM代理的互动最近引起了广泛关注。例如，Fu等人（[2023](#bib.bib40)）展示了LLMs如何通过相互对抗和批评在谈判游戏中自主改进彼此。类似地，（Du等人，[2023](#bib.bib35)；Liang等人，[2023](#bib.bib66)；Xiong等人，[2023](#bib.bib114)；Chan等人，[2024](#bib.bib27)；Li等人，[2023c](#bib.bib62)）展示了多LLM*辩论*可以提升LLM的推理和评估能力。Qian等人（[2023](#bib.bib85)）；Schick等人（[2023](#bib.bib90)）；Wu等人（[2023](#bib.bib112)）展示了多LLM互动和协作在软件开发、写作和问题解决中的潜力。Zhang等人（[2024](#bib.bib120)）在体现性合作环境中展示了类似的潜力。更正式地，多LLM互动也在*博弈论*框架下进行了研究，以刻画LLM代理的*战略*决策。Bakhtin等人（[2022](#bib.bib14)）；Mukobi等人（[2023](#bib.bib79)）和Xu等人（[2023b](#bib.bib116)，[a](#bib.bib115)）展示了LLMs在玩《外交》和《狼人杀》游戏中的潜力，这些都是具有竞争和合作代理混合的语言基础游戏。注意，这些研究利用LLM解决的是特定游戏，而非通用游戏。与我们的工作相关，Brookins和DeBacker（[2023](#bib.bib20)）；Akata等人（[2023](#bib.bib8)）；Lorè和Heydari（[2023](#bib.bib72)）；Brookins和DeBacker（[2023](#bib.bib20)）；Fan等人（[2023](#bib.bib38)）也使用（重复的）矩阵游戏作为基准来评估LLM代理的推理能力和理性。与我们的工作不同，这些实证研究没有正式使用*遗憾*指标，也没有通过*在线学习*和*均衡计算*的视角进行研究，这些都是建模和分析战略多代理互动中的基础。我们的工作还提供了理论结果来解释和进一步增强LLM代理的无悔性属性。
- en: LLMs & Human/Social behavior.
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLMs与人类/社会行为。
- en: LLMs have also been used to *simulate* the behavior of human beings, for social
    science and economics studies (Engel et al., [2023](#bib.bib36)). The extent of
    LLMs simulating human behavior has been claimed as a way to evaluate the level
    of its intelligence in a controlled environment (Aher et al., [2023](#bib.bib4);
    Tsai et al., [2023](#bib.bib99)). For example, Li et al. ([2023b](#bib.bib61));
    Hong et al. ([2024](#bib.bib49)); Zhao et al. ([2023](#bib.bib123)) showed that
    by specifying different “roles” to LLM agents, certain collaborative/competitive
    behaviors can emerge. Argyle et al. ([2023](#bib.bib10)) showed that LLMs can
    emulate response distributions from diverse human subgroups, illustrating their
    adaptability. Horton ([2023](#bib.bib50)) argued that an LLM, as a computational
    model of humans, can be used as *homo economicus* when given endowments, information,
    preferences, etc., to gain new economic insights by simulating its interaction
    with other LLMs. Park et al. ([2022](#bib.bib84), [2023](#bib.bib83)) proposed
    scalable simulators that can generate realistic social behaviors emerging in populated
    and interactive social systems, and the emerging behaviors of LLM agents in society
    have also been consistently observed in Chen et al. ([2024](#bib.bib28), [2023](#bib.bib29)).
    Li et al. ([2023d](#bib.bib63), [a](#bib.bib60)) studied the opinion/behavioral
    dynamics of LLM agents on social networks. These empirical results have inspired
    our work, which can be viewed as an initial attempt towards quantitatively understanding
    the *emerging behavior* of LLMs as computational human models, given the well-known
    justification of *equilibrium* being a long-run emerging behavior of *learning
    dynamics* (Fudenberg and Levine, [1998](#bib.bib42)) and strategic interactions
    (Young, [2004](#bib.bib119); Camerer, [2011](#bib.bib24)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs也被用来*模拟*人类行为，用于社会科学和经济学研究（Engel等，[2023](#bib.bib36)）。LLMs模拟人类行为的程度被认为是评估其在受控环境中智能水平的一种方式（Aher等，[2023](#bib.bib4)；Tsai等，[2023](#bib.bib99)）。例如，Li等人（[2023b](#bib.bib61)）；Hong等人（[2024](#bib.bib49)）；Zhao等人（[2023](#bib.bib123)）显示，通过为LLM代理指定不同的“角色”，可以产生某些协作/竞争行为。Argyle等人（[2023](#bib.bib10)）显示，LLMs可以模拟来自不同人群子组的反应分布，展示了它们的适应性。Horton（[2023](#bib.bib50)）认为，LLM作为人类的计算模型，当给定赋予、信息、偏好等时，可以作为*homo
    economicus*使用，通过模拟与其他LLM的互动获得新的经济见解。Park等人（[2022](#bib.bib84)，[2023](#bib.bib83)）提出了可扩展的模拟器，这些模拟器可以生成在拥挤和互动的社会系统中出现的现实社会行为，LLM代理在社会中的新兴行为也在Chen等人（[2024](#bib.bib28)，[2023](#bib.bib29)）的研究中被一致观察到。Li等人（[2023d](#bib.bib63)，[a](#bib.bib60)）研究了LLM代理在社交网络上的意见/行为动态。这些实证结果激发了我们的工作，这可以视为对LLM作为计算人类模型的新兴行为的定量理解的初步尝试，考虑到*均衡*是*学习动态*的长期新兴行为（Fudenberg和Levine，[1998](#bib.bib42)）和战略互动（Young，[2004](#bib.bib119)；Camerer，[2011](#bib.bib24)）的广为人知的理由。
- en: Transformers & In-context-learning.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Transformers & In-context-learning.
- en: LLMs nowadays are predominantly built upon the architecture of Transformers
    (Vaswani et al., [2017](#bib.bib102)). Transformers have exhibited a remarkable
    capacity of *in-context-learning* (ICL), which can construct new predictors from
    sequences of labeled examples as input, without further parameter updates. This
    has enabled the *few-shot learning* capability of Transformers (Brown et al.,
    [2020](#bib.bib21); Garg et al., [2022](#bib.bib44); Min et al., [2022](#bib.bib78)).
    The empirical successes have inspired burgeoning theoretical studies on ICL. Xie
    et al. ([2022](#bib.bib113)) used a Bayesian inference framework to explain how
    ICL works, which has also been adopted in Wang et al. ([2023b](#bib.bib106));
    Jiang ([2023](#bib.bib53)). Akyürek et al. ([2023](#bib.bib9)); Von Oswald et al.
    ([2023](#bib.bib103)); Dai et al. ([2023](#bib.bib30)); Giannou et al. ([2023](#bib.bib45))
    showed (among other results) that ICL comes from the fact that Transformers can
    implement the gradient descent (GD) algorithm. Bai et al. ([2023](#bib.bib13))
    further established that Transformers can implement a broad class of machine learning
    algorithms in context. Moreover, Ahn et al. ([2023](#bib.bib5)); Zhang et al.
    ([2023a](#bib.bib121)); Mahankali et al. ([2023](#bib.bib74)) proved that a *minimizer*
    of the certain training loss among single-layer Transformers is equivalent to
    a single step of GD for linear regression. Li et al. ([2023e](#bib.bib64)) established
    generalization bounds of ICL from a multi-task learning perspective. Zhang et al.
    ([2023b](#bib.bib122)) argued that ICL implicitly implements Bayesian model averaging,
    and can be approximated by the attention mechanism. They also established a result
    on some *regret* metric. However, the regret notion is not defined for (online)
    decision-making, and is fundamentally different from ours that is standard in
    online learning and games. Also, we provide extensive experiments to validate
    the no-regret behavior by our definition. More recently, the ICL property has
    also been generalized to decision-making settings. Laskin et al. ([2023](#bib.bib57));
    Lee et al. ([2023](#bib.bib59)); Lin et al. ([2024](#bib.bib67)) investigated
    the in-context reinforcement learning (RL) property of Transformers under supervised
    pre-training, for solving stochastic bandits and Markov decision processes. In
    contrast, our work focuses on online learning settings with an arbitrary and *potentially
    adversarial* nature, as well as *game-theoretic* settings. We also provide a new
    *unsupervised* loss to promote the no-regret behavior in our settings.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的LLMs主要基于Transformers（Vaswani等，[2017](#bib.bib102)）架构。Transformers展现了*上下文学习*（ICL）的显著能力，这种能力可以从标记示例的序列中构建新的预测器，而无需进一步的参数更新。这使得Transformers具备了*少样本学习*能力（Brown等，[2020](#bib.bib21)；Garg等，[2022](#bib.bib44)；Min等，[2022](#bib.bib78)）。这些经验上的成功激发了对ICL的理论研究。Xie等人（[2022](#bib.bib113)）使用贝叶斯推断框架解释了ICL的工作原理，这一框架也被Wang等人（[2023b](#bib.bib106)）；Jiang（[2023](#bib.bib53)）采用。Akyürek等人（[2023](#bib.bib9)）；Von
    Oswald等人（[2023](#bib.bib103)）；Dai等人（[2023](#bib.bib30)）；Giannou等人（[2023](#bib.bib45)）展示了（其中包括其他结果）ICL源于Transformers可以实现梯度下降（GD）算法。Bai等人（[2023](#bib.bib13)）进一步建立了Transformers能够在上下文中实现广泛的机器学习算法。此外，Ahn等人（[2023](#bib.bib5)）；Zhang等人（[2023a](#bib.bib121)）；Mahankali等人（[2023](#bib.bib74)）证明了某些训练损失在单层Transformers中的*最小化器*等价于线性回归的GD单步。Li等人（[2023e](#bib.bib64)）从多任务学习的角度建立了ICL的泛化界限。Zhang等人（[2023b](#bib.bib122)）认为ICL隐式实现了贝叶斯模型平均，并且可以通过注意机制进行近似。他们还在某些*遗憾*度量上建立了结果。然而，遗憾概念在（在线）决策制定中尚未定义，并且与我们在在线学习和游戏中的标准有根本区别。此外，我们提供了广泛的实验来验证我们定义下的无遗憾行为。最近，ICL属性也被推广到决策制定环境中。Laskin等人（[2023](#bib.bib57)）；Lee等人（[2023](#bib.bib59)）；Lin等人（[2024](#bib.bib67)）调查了Transformers在监督预训练下的上下文强化学习（RL）属性，用于解决随机赌博机和马尔可夫决策过程。相比之下，我们的工作集中于具有任意和*潜在对抗*性质的在线学习环境，以及*博弈论*设置。我们还提供了一种新的*无监督*损失，以促进我们设置中的无遗憾行为。
- en: Online learning and games.
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在线学习和游戏。
- en: Online learning has been extensively studied to model the decision-making of
    an agent who interacts with the environment sequentially, with a potentially arbitrary
    sequence of loss functions (Shalev-Shwartz, [2012](#bib.bib92); Hazan, [2016](#bib.bib47)),
    and has a deep connection to game theory (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)).
    In particular, regret, the difference between the incurred accumulated loss and
    the best-in-hindsight accumulated loss, has been the core performance metric,
    and a good online learning algorithm should have regret at most sublinear in time
    $T$), which is referred to as being *no-regret*. Many well-known algorithms can
    achieve no-regret against *arbitrary* loss sequences, e.g., multiplicative weight
    updates (MWU)/Hedge (Freund and Schapire, [1997](#bib.bib39); Arora et al., [2012](#bib.bib11)),
    EXP3 (Auer et al., [2002](#bib.bib12)), and more generally follow-the-regularized-leader
    (FTRL) (Shalev-Shwartz and Singer, [2007](#bib.bib93)) and follow-the-perturbed-leader
    (FTPL) (Kalai and Vempala, [2005](#bib.bib54)). In the bandit literature (Lattimore
    and Szepesvári, [2020](#bib.bib58); Bubeck et al., [2012](#bib.bib22)), such a
    setting without any statistical assumptions on the losses is also referred to
    as the *adversarial/non-stochastic* setting. Following the conventions in this
    literature, the online settings we focus on shall not be confused with the stationary
    and *stochastic*(-bandit)/(-reinforcement learning) settings that have been explored
    in several other recent works on *Transformers for decision-making* (Lee et al.,
    [2023](#bib.bib59); Lin et al., [2024](#bib.bib67)). Centering around the regret
    metric, our work has also explored the non-stationary bandit setting (Besbes et al.,
    [2014](#bib.bib17)), as well as the repeated game setting where the environment
    itself consists of strategic agents (Cesa-Bianchi and Lugosi, [2006](#bib.bib26)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习已经被广泛研究，用于建模一个与环境顺序交互的代理，其损失函数序列可能是任意的（Shalev-Shwartz，[2012](#bib.bib92)；Hazan，[2016](#bib.bib47)），并且与博弈论有深刻的联系（Cesa-Bianchi和Lugosi，[2006](#bib.bib26)）。特别地，后悔，即实际累计损失与最佳后视累计损失之间的差异，一直是核心性能指标，一个好的在线学习算法应该在时间$T$中具有至多次线性的后悔，这被称为*无后悔*。许多知名算法可以在*任意*损失序列下实现无后悔，例如，乘法权重更新（MWU）/Hedge（Freund和Schapire，[1997](#bib.bib39)；Arora等，[2012](#bib.bib11)），EXP3（Auer等，[2002](#bib.bib12)），以及更一般的跟随正则化领导者（FTRL）（Shalev-Shwartz和Singer，[2007](#bib.bib93)）和跟随扰动领导者（FTPL）（Kalai和Vempala，[2005](#bib.bib54)）。在强盗文献中（Lattimore和Szepesvári，[2020](#bib.bib58)；Bubeck等，[2012](#bib.bib22)），这种没有任何统计假设的设置也称为*对抗性/非随机*设置。根据这一文献中的惯例，我们关注的在线设置不应与在若干近期关于*决策制定的变换器*（Lee等，[2023](#bib.bib59)；Lin等，[2024](#bib.bib67)）中的平稳和*随机*（-强盗）/（-强化学习）设置混淆。围绕后悔指标，我们的工作还探讨了非平稳强盗设置（Besbes等，[2014](#bib.bib17)），以及环境本身由战略代理组成的重复博弈设置（Cesa-Bianchi和Lugosi，[2006](#bib.bib26)）。
- en: 2 Preliminaries
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: Notation.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 符号。
- en: We use $\mathbb{N}$.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$\mathbb{N}$。
- en: 2.1 Online Learning & Games
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 在线学习与博弈
- en: Online learning.
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在线学习。
- en: We first consider the online learning setting where an agent interacts with
    the environment for $T$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑一个在线学习环境，其中一个代理与环境进行$T$步交互。
- en: At time step $t\in[T]$ simply as *online learning*. Moreover, if the loss functions
    change over time (usually with certain bounded variation), we will refer to it
    as *non-stationary online learning* for short, whose bandit-feedback version is
    also referred to as the *non-stationary bandit* problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步$t\in[T]$下，称之为*在线学习*。此外，如果损失函数随时间变化（通常具有一定的有界变化），我们将其称为*非平稳在线学习*，其带有反馈的版本也称为*非平稳强盗*问题。
- en: Repeated games.
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重复博弈。
- en: 'The online learning setting above has an intimate connection to game theory.
    Consider a normal-form game $\mathcal{G}=\langle N,\{\mathcal{A}_{n}\}_{n\in[N]},\{r_{n}\}_{n\in[N]}\rangle$.
    We will refer to it as the *game setting* for short, and use the terms of “agent”
    and “player” interchangeably hereafter. The key difference between online learning
    and repeated games is in their interaction dynamics: online learning involves
    an agent facing a potentially adversarial, changing environment (or sequence of
    loss functions), while in repeated games, agents interact by playing the same
    game repeatedly, which might be less adversarial when they follow specific learning
    algorithms.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 上述在线学习设置与博弈论有着密切的联系。考虑一个标准形式博弈 $\mathcal{G}=\langle N,\{\mathcal{A}_{n}\}_{n\in[N]},\{r_{n}\}_{n\in[N]}\rangle$。我们将其简称为
    *博弈设置*，并在此后将“代理”和“玩家”这两个术语互换使用。在线学习和重复博弈的关键区别在于它们的互动动态：在线学习涉及一个面对潜在敌对、变化环境（或损失函数序列）的代理，而在重复博弈中，代理通过反复进行同一博弈进行互动，当他们遵循特定的学习算法时，这可能会减少敌对性。
- en: '2.2 Performance Metric: Regret'
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 性能指标：悔恨
- en: We now introduce *regret*, the core performance metric used in online learning
    and games. For a given algorithm $\mathscr{A}$ and that incurred by the best-in-hindsight
    fixed decision, can be defined as
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍 *悔恨*，这是在线学习和博弈中使用的核心性能指标。对于给定的算法 $\mathscr{A}$ 和由最优固定决策产生的悔恨，可以定义为
- en: '|  | $1$2 |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: In the Experts Problem, the definition can be instantiated as $\text{Regret}_{\mathscr{A}}(({\ell}_{t})_{t\in[T]}):=\sum_{t=1}^{T}\langle{\ell}_{t},\pi_{\mathscr{A},t}\rangle-\inf_{\pi\in\Pi}\sum_{t=1}^{T}\langle{\ell}_{t},\pi\rangle$.
    Widely-known no-regret algorithms include follow-the-regularized-leader (FTRL)
    (Shalev-Shwartz and Singer, [2007](#bib.bib93)), follow-the-perturbed-leader (Kalai
    and Vempala, [2005](#bib.bib54)) (See [Section A.3](#A1.SS3 "A.3 Online Learning
    Algorithms ‣ Appendix A Deferred Background ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") for a detailed introduction).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在专家问题中，定义可以表示为 $\text{Regret}_{\mathscr{A}}(({\ell}_{t})_{t\in[T]}):=\sum_{t=1}^{T}\langle{\ell}_{t},\pi_{\mathscr{A},t}\rangle-\inf_{\pi\in\Pi}\sum_{t=1}^{T}\langle{\ell}_{t},\pi\rangle$。广为人知的无悔算法包括
    follow-the-regularized-leader (FTRL) (Shalev-Shwartz 和 Singer, [2007](#bib.bib93))，follow-the-perturbed-leader
    (Kalai 和 Vempala, [2005](#bib.bib54))（详见 [A.3节](#A1.SS3 "A.3 在线学习算法 ‣ 附录 A 延迟背景
    ‣ 大型语言模型代理是否有悔恨？在线学习和博弈中的案例研究")）。
- en: In non-stationary online learning, one also uses the metric of *dynamic regret*
    (Zinkevich, [2003](#bib.bib125)), where the *comparator* in the definition also
    changes over time, as the best decision policy at each individual time $t$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在非平稳的在线学习中，还使用 *动态悔恨* 的度量 (Zinkevich, [2003](#bib.bib125))，其中定义中的 *比较者* 随时间变化，作为每个时间点
    $t$ 的最佳决策策略。
- en: 3 Do Pre-Trained LLMs Have Regret? Experimental Validation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 预训练的大型语言模型是否有悔恨？实验验证
- en: In this section, we explore the no-regret behaviors of representative pre-trained
    LLMs (i.e., GPT-4 Turbo, GPT-4, and GPT-3.5 Turbo), in the context of online learning
    and games. All experiments with LLMs are conducted using the public OpenAI Python
    API (Openai, [2023](#bib.bib81)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨代表性的预训练大型语言模型（即 GPT-4 Turbo、GPT-4 和 GPT-3.5 Turbo）在在线学习和博弈中的无悔行为。所有大型语言模型的实验均使用公开的
    OpenAI Python API (Openai, [2023](#bib.bib81))。
- en: Intuition why pre-trained language models may exhibit no-regret behavior.
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 直观地解释为什么预训练语言模型可能表现出无悔行为。
- en: Transformer-based LLMs have demonstrated impressive *in-context-learning* and
    few-/zero-shot learning capabilities (Brown et al., [2020](#bib.bib21); Garg et al.,
    [2022](#bib.bib44); Min et al., [2022](#bib.bib78)). One theoretical explanation
    is that, trained Transformers can implement the *gradient descent algorithm* on
    the testing loss in certain supervised learning problems (Akyürek et al., [2023](#bib.bib9);
    Von Oswald et al., [2023](#bib.bib103); Dai et al., [2023](#bib.bib30); Ahn et al.,
    [2023](#bib.bib5); Zhang et al., [2023a](#bib.bib121); Mahankali et al., [2023](#bib.bib74)),
    which is inherently *adaptive* to the loss function used at test time. On the
    other hand, it is known in online learning that the simple algorithm of *online
    gradient descent* (Zinkevich, [2003](#bib.bib125)) can achieve no-regret. Hence,
    it seems reasonable to envision the no-regret behavior of such meta-learners in
    online learning, due to their fast adaptability. However, it is not straightforward
    due to the fundamental difference between multi-task/meta-learning and online
    learning settings, as well as the difference between *stationary* and *non-stationary/adversarial*
    environments in decision-making. Next, we provide both experimental and theoretical
    studies to validate this intuition.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的LLMs展示了令人印象深刻的*上下文学习*和少量/零样本学习能力（Brown et al., [2020](#bib.bib21);
    Garg et al., [2022](#bib.bib44); Min et al., [2022](#bib.bib78)）。一种理论解释是，训练后的Transformers可以在某些监督学习问题上实现*梯度下降算法*（Akyürek
    et al., [2023](#bib.bib9); Von Oswald et al., [2023](#bib.bib103); Dai et al.,
    [2023](#bib.bib30); Ahn et al., [2023](#bib.bib5); Zhang et al., [2023a](#bib.bib121);
    Mahankali et al., [2023](#bib.bib74)），这本质上是*适应性*的，以适应测试时使用的损失函数。另一方面，已知在在线学习中，简单的*在线梯度下降*算法（Zinkevich,
    [2003](#bib.bib125)）可以实现无遗憾。因此，由于这些元学习者的快速适应性，设想其在在线学习中的无遗憾行为似乎是合理的。然而，由于多任务/元学习和在线学习设置之间的根本差异，以及决策中*静态*和*非静态/对抗*环境之间的差异，这并不简单。接下来，我们提供了实验和理论研究来验证这一直觉。
- en: Interaction protocol.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 互动协议。
- en: 'To enable the sequential interactions with LLMs, we first describe the setup
    and objective of our experimental study. At each round, we incorporate the entire
    history of loss vectors of past interactions into our prompts, as concatenated
    texts, and ask the LLM agent to determine a policy that guides the decision-making
    for the next round. Note that since we hope to *evaluate* the intelligence level
    of pre-trained LLMs through online learning or games, we only provide simple prompts
    that she should utilize the history information, without providing explicit rules
    of *how* to make use of the history information, nor asking her to *minimize regret*
    (in any sense). A detailed description and an ablation study of the prompts are
    deferred to [Section B.1](#A2.SS1 "B.1 Ablation Study on Prompts ‣ Appendix B
    Deferred Results and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"), and an illustration of the protocol in playing
    repeated games is given in [Figure 3](#S3.F3 "In Extension to bandit-feedback
    settings. ‣ 3.2 Results: Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现与LLMs的连续互动，我们首先描述了实验研究的设置和目标。在每一轮中，我们将过去互动的所有损失向量的历史记录整合到我们的提示中，作为连接的文本，并要求LLM代理确定一个指导下一轮决策的策略。请注意，由于我们希望通过在线学习或游戏来*评估*预训练LLMs的智能水平，我们只提供简单的提示，要求她利用历史信息，而不提供如何使用历史信息的明确规则，也不要求她*最小化遗憾*（以任何意义）。关于提示的详细描述和消融研究推迟到[第B.1节](#A2.SS1
    "B.1 Ablation Study on Prompts ‣ Appendix B Deferred Results and Proofs in Section
    3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")，并且重复游戏中的协议说明见[图3](#S3.F3
    "In Extension to bandit-feedback settings. ‣ 3.2 Results: Online Learning ‣ 3
    Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games")。'
- en: 3.1 Framework for No-Regret Behavior Validation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 无遗憾行为验证框架
- en: Before delving into the results, we note that to the best of our knowledge,
    we are not aware of any principled framework for validating no-regret behaviors
    with finite-time experimental data. Therefore, we propose two frameworks to rigorously
    validate the no-regret behavior of algorithms over a *finite* $T$, which might
    be of independent interest.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入结果之前，我们注意到，据我们所知，我们尚未发现任何用于验证有限时间实验数据下的无遗憾行为的原则性框架。因此，我们提出了两个框架来严格验证算法在*有限*
    $T$ 下的无遗憾行为，这可能具有独立的兴趣。
- en: Trend-checking framework.
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 趋势检查框架。
- en: 'We propose the following hypothesis test:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出以下假设检验：
- en: '|  | $\displaystyle H_{0}$ |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{0}$ |  |'
- en: '|  | $\displaystyle H_{1}$ |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{1}$ |  |'
- en: 'with $H_{0}$ by definition, making it challenging to verify directly. As an
    alternative, we propose a more tractable hypothesis test, albeit a weaker one,
    that still captures the essence of our objective:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $H_{0}$ 的定义，使得直接验证具有挑战性。作为替代方案，我们提出了一种更易处理的假设检验，尽管较弱，但仍然捕捉到我们目标的本质：
- en: '|  | $\displaystyle H_{0}$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{0}$ |  |'
- en: '|  | $\displaystyle H_{1}$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{1}$ |  |'
- en: Ideally, one should check if $\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)/t$,
    as the output of this framework.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，应该检查 $\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)/t$，作为该框架的输出。
- en: Proposition 1.
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 1。
- en: '*($p$-value of the null hypothesis).* Define the event'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*（原假设的$p$-值）。* 定义事件'
- en: '|  | $$\displaystyle\mathcal{E}(s,T):=\left\{\text{The~{}number~{}of~{}}\frac{\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)}{t}-\frac{\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t+1]}\right)}{t+1}>
    |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle\mathcal{E}(s,T):=\left\{\text{数~{}量~{}的~{}}\frac{\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t]}\right)}{t}-\frac{\text{Regret}_{\mathscr{A}}\left((f_{\tau})_{\tau\in[t+1]}\right)}{t+1}>
    |  |'
- en: Under the assumption that the null hypothesis $H_{0}$ holds, the probability
    of this event happening is bounded as  |  |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle\sigma^{\star}\in\arg\min_{\sigma<$$ |  |'
- en: 'We solve this single-variable optimization problem by grid search over $[0,10]$
    on another *unseen test set*, and compare it with the behavior of the actual LLM
    agents. We use all the interaction data from [Section 3.2](#S3.SS2 "3.2 Results:
    Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and split
    it in half for training and testing. In [Figure 8](#S4.F8 "In Calibrating the
    degree of bounded rationality of actual LLMs. ‣ 4.3 Case Study: Pre-Training under
    Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"), we show the averaged regret for the LLM agent
    and the calibrated generalized quantal response. It can be seen that calibrated
    generalized quantal response can *very well capture* the behavior of the LLM agent
    for all problem instances in [Section 3.2](#S3.SS2 "3.2 Results: Online Learning
    ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games"), justifying the applicability
    of our hypothetical model and assumptions.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在另一个*未见测试集*上对$[0,10]$进行网格搜索来解决这个单变量优化问题，并将其与实际LLM代理的行为进行比较。我们使用[第3.2节](#S3.SS2
    "3.2 结果：在线学习 ‣ 3 预训练LLM是否有后悔？实验验证 ‣ LLM代理是否有后悔？在线学习和游戏中的案例研究")中的所有交互数据，并将其分成两半进行训练和测试。在[图8](#S4.F8
    "在校准实际LLM的有限理性程度 ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练LLM（无）后悔？一个假设模型和一些理论见解 ‣ LLM代理是否有后悔？在线学习和游戏中的案例研究")中，我们展示了LLM代理的平均后悔和校准的广义量反应。可以看到，校准的广义量反应可以*非常好地捕捉*LLM代理在[第3.2节](#S3.SS2
    "3.2 结果：在线学习 ‣ 3 预训练LLM是否有后悔？实验验证 ‣ LLM代理是否有后悔？在线学习和游戏中的案例研究")中的所有问题实例的行为，这证明了我们假设模型和假设的适用性。
- en: '![Refer to caption](img/a9abb330635fc7f8359929a435a16d21.png)![Refer to caption](img/ee86508d79e597a018a2383e392ff74f.png)![Refer
    to caption](img/f05a16da0d09608231af19738a44c453.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9abb330635fc7f8359929a435a16d21.png)![参见说明](img/ee86508d79e597a018a2383e392ff74f.png)![参见说明](img/f05a16da0d09608231af19738a44c453.png)'
- en: 'Figure 8: (left) Comparison of GPT-4 with a calibrated agent on the test set,
    where the calibrated quantal response can perfectly capture the behavior of the
    GPT-4 agent. (mid, right) The calibrated agent on the less predictable and adaptive
    loss sequences failed to make accurate predictions for the GPT-4 anymore.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8： (左) 比较 GPT-4 和经过校准的代理在测试集上的表现，其中经过校准的量子响应可以完美捕捉 GPT-4 代理的行为。 (中、右) 在较难预测和适应性损失序列上，经过校准的代理未能对
    GPT-4 做出准确预测。
- en: 'We also use the same framework to understand the regrettable behaviors in [Section 3.4](#S3.SS4
    "3.4 Pre-Trained LLM Agents May Still Have Regret ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"). This analysis uses all the data from [Section 3.4](#S3.SS4
    "3.4 Pre-Trained LLM Agents May Still Have Regret ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"). We first find that such fitting procedures do not yield
    good predictions for LLMs on those counter-examples. Therefore, we resort to a
    more expressive model by directly fitting each $\eta_{t}$. Even under the expressive
    model, LLMs fail to follow the generalized quantal response for the counter-examples
    with less predictable or adaptive loss sequences, as [Figure 8](#S4.F8 "In Calibrating
    the degree of bounded rationality of actual LLMs. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") shows the gap between GPT-4 (dynamic)
    regret and the calibrated agent (dynamic) regret.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用相同的框架来理解[第 3.4 节](#S3.SS4 "3.4 预训练 LLM 代理可能仍然有悔恨 ‣ 3 预训练 LLM 是否有悔恨？实验验证
    ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")中的悔恨行为。这一分析使用了[第 3.4 节](#S3.SS4 "3.4 预训练 LLM 代理可能仍然有悔恨
    ‣ 3 预训练 LLM 是否有悔恨？实验验证 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")中的所有数据。我们首先发现这些拟合程序在那些反例上对
    LLM 的预测效果不好。因此，我们诉诸于更具表现力的模型，通过直接拟合每个 $\eta_{t}$。即使在表现力模型下，对于那些具有较少可预测性或适应性损失序列的反例，LLM
    也未能跟随广义量子响应，如[图 8](#S4.F8 "在校准实际 LLM 的有界理性程度中 ‣ 4.3 案例研究：在标准数据分布下预训练 ‣ 4 为什么预训练的
    LLM (无)悔恨？一个假设模型和一些理论见解 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")显示 GPT-4（动态）悔恨和经过校准的代理（动态）悔恨之间的差距。
- en: Finally, we acknowledge that for most existing pre-trained LLMs like GPT-4,
    the canonical assumptions above, though may be further relaxed (c.f. [Remark 4](#Thmremark4
    "Remark 4 (Pre-training with relaxed data assumptions). ‣ C.5.2 Relaxation under
    Decision-Irrelevant Pre-Training Data ‣ C.5 Extending Theorem 1 with Relaxed Assumptions
    ‣ Appendix C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")), may not hold in general. More importantly,
    the *supervision labels*, i.e., the optimal action given $z$, may be sometimes
    imperfect or unavailable during the dataset collection. Hence, it is completely
    possible to observe regrettable behaviors (c.f. LABEL:{sec:yes_regret_example}).
    Motivated by these caveats, we next propose a new training loss that is *unsupervised*,
    and can promote no-regret behavior provably.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们承认，对于大多数现有的预训练 LLM，如 GPT-4，上述标准假设虽然可能会进一步放宽（参见[备注 4](#Thmremark4 "备注 4
    (使用放宽数据假设的预训练) ‣ C.5.2 决策无关预训练数据下的放宽 ‣ C.5 在放宽假设下扩展定理 1 ‣ 附录 C 延迟结果和第 4 节的证明 ‣
    LLM 代理是否有悔恨？在线学习和游戏中的案例研究")），但一般来说可能并不成立。更重要的是，*监督标签*，即给定 $z$ 的最优动作，有时可能在数据集收集期间不完美或不可用。因此，观察到悔恨行为是完全可能的（参见
    LABEL:{sec:yes_regret_example}）。鉴于这些警告，我们接下来提出了一种新的训练损失，这是一种*无监督*的，并且可以证明促进无悔行为。
- en: 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 通过无监督损失证明促进无悔行为
- en: 'In light of the observations in [Section 3](#S3 "3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games"), we ask the question:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于[第 3 节](#S3 "3 预训练 LLM 是否有悔恨？实验验证 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")中的观察，我们提出了以下问题：
- en: '*Is there a way to further enhance the no-regret property of LLM agents,'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*有没有办法进一步增强 LLM 代理的无悔属性，'
- en: hopefully without (optimal) action labels?*
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 希望不需要（最优）动作标签？*
- en: To address this question, we propose to train LLMs with a new *unsupervised
    learning* loss that naturally provides no-regret behaviors. This approach is akin
    to the process of “instruction tuning” (Wei et al., [2021](#bib.bib109)), which
    was shown to have enhanced LLMs’ ability when learning from context, with both
    theoretical (Ahn et al., [2023](#bib.bib5); Mahankali et al., [2023](#bib.bib74);
    Zhang et al., [2023a](#bib.bib121)) and empirical (Lu et al., [2023](#bib.bib73))
    evidence.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们建议使用一种新的*无监督学习*损失来训练LLMs，该方法自然提供无悔行为。这种方法类似于“指令调优”（Wei et al., [2021](#bib.bib109)），已显示在从上下文学习时增强了LLMs的能力，拥有理论（Ahn
    et al., [2023](#bib.bib5); Mahankali et al., [2023](#bib.bib74); Zhang et al.,
    [2023a](#bib.bib121)）和实证（Lu et al., [2023](#bib.bib73)）证据。
- en: '5.1 A New Unsupervised Training Loss: Regret-Loss'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 一种新的无监督训练损失：悔恨损失
- en: Intuitively, our new training loss is designed to enforce the trained LLM to
    minimize the regret under an arbitrary sequence of loss vectors. Specifically,
    we define the training loss as
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，我们的新训练损失旨在强制训练的LLM在任意的损失向量序列下最小化悔恨。具体地，我们将训练损失定义为
- en: '|  | $\displaystyle{\mathcal{L}(\theta)}:=\max_{\ell_{1},\dots,\ell_{T}}~{}~{}~{}\text{Regret}_{\text{LLM}_{\theta}}\left((\ell_{t})_{t\in[T]}\right)$
    |  | (2) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\mathcal{L}(\theta)}:=\max_{\ell_{1},\dots,\ell_{T}}~{}~{}~{}\text{Regret}_{\text{LLM}_{\theta}}\left((\ell_{t})_{t\in[T]}\right)$
    |  | (2) |'
- en: 'where $\|\ell_{t}\|_{\infty}\leq B$ in the definition of regret. Therefore,
    we provide a general framework so that we can approximate [Equation 2](#S5.E2
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") by the following surrogate:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '在悔恨定义中，$\|\ell_{t}\|_{\infty}\leq B$。因此，我们提供了一个通用框架，以便我们可以通过以下代理来近似[方程2](#S5.E2
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")：'
- en: '|  | $1$2 |  | (3) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $k\in\mathbb{N}^{+}$), in contrast to those in [Section 4](#S4 "4 Why
    Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") to *justify*
    the no-regret property of pre-trained LLMs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$k\in\mathbb{N}^{+}$），与[第4节](#S4 "4 Why Are Pre-Trained LLMs (No-)Regret?
    A Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")中的那些相比，*证明*预训练LLMs的无悔特性。
- en: In [Section D.2](#A4.SS2 "D.2 Deferred Proof for the Arguments in Section 5.1
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), we prove that under certain regularity
    conditions of $f$, we have
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第D.2节](#A4.SS2 "D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")，我们证明了在$f$的某些规则性条件下，我们有
- en: '|  | $1$2 |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'as well as the uniform convergence of $\mathcal{L}(\theta,k,N)$ may promote
    the trained LLM to have a small regret value. We will hereafter refer to [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") as the regret-loss. Similarly, we can also define
    dynamic-regret-loss, and the results to be presented next can also generalize
    to this case (c.f. [Remark 5](#Thmremark5 "Remark 5 (Dynamic-regret loss). ‣ D.3
    Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") in [Section D.3](#A4.SS3 "D.3 Deferred Proofs of Theorem 2 and Corollary
    1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '以及$\mathcal{L}(\theta,k,N)$的均匀收敛性可能促进训练的LLM具有较小的悔恨值。我们将[方程3](#S5.E3 "In 5.1
    A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")称为悔恨损失。类似地，我们也可以定义动态悔恨损失，接下来的结果也可以推广到这种情况（参见[备注5](#Thmremark5
    "Remark 5 (Dynamic-regret loss). ‣ D.3 Deferred Proofs of Theorem 2 and Corollary
    1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")在[第D.3节](#A4.SS3 "D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"))。'
- en: 5.2 Guarantees via Regret-Loss Minimization
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 通过悔恨损失最小化的保证
- en: 'We first establish a *statistical* guarantee under general parameterizations
    of $\text{LLM}_{\theta}$, including the Transformer-based models as used in GPT-4
    and most existing LLMs (see [Proposition 2](#Thmproposition2 "Proposition 2\.
    ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") for an example with formal statement). This guarantee focuses on their
    *generalization ability* when trained to minimize the empirical regret loss, which
    is defined as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在$\text{LLM}_{\theta}$的一般参数化下建立了一个*统计*保证，包括用于GPT-4和大多数现有LLM的基于Transformer的模型（见[命题 2](#Thmproposition2
    "Proposition 2\. ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")的形式化陈述示例）。这个保证关注于它们在训练以最小化经验悔恨损失时的*泛化能力*，经验悔恨损失定义如下：
- en: Definition 3  (Empirical loss function).
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3（经验损失函数）。
- en: 'We define the empirical loss $\widehat{\mathcal{L}}$ samples as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将经验损失$\widehat{\mathcal{L}}$样本定义如下：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $(\ell_{s,t}^{(j)})_{j\in[N],t\in[T]}$.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$(\ell_{s,t}^{(j)})_{j\in[N],t\in[T]}$。
- en: We denote $\widehat{\theta}_{k,N,N_{T}}\in\operatorname*{arg\,min}_{\theta\in\Theta}~{}\widehat{\mathcal{L}}(\theta,k,N,N_{T})$,
    and present the generalization guarantee below.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表示$\widehat{\theta}_{k,N,N_{T}}\in\operatorname*{arg\,min}_{\theta\in\Theta}~{}\widehat{\mathcal{L}}(\theta,k,N,N_{T})$，并展示下面的泛化保证。
- en: Theorem 2.
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2。
- en: '*(Generalization gap).* Suppose $\emph{LLM}_{\theta}$, we have'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*(泛化差距).* 假设$\emph{LLM}_{\theta}$，我们有'
- en: '|  | $\displaystyle\mathcal{L}\left(\widehat{\theta}_{k,N,N_{T}},k,N\right)$
    |  | (5) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}\left(\widehat{\theta}_{k,N,N_{T}},k,N\right)$
    |  | (5) |'
- en: for any $N$.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何$N$。
- en: 'Through a careful use of Berge’s Maximum Theorem (Berge, [1877](#bib.bib16)),
    we prove that the right-hand side of [Equation 5](#S5.E5 "In Theorem 2\. ‣ 5.2
    Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") does *not* depend on $k$ (c.f. [Section D.2](#A4.SS2 "D.2 Deferred
    Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")), we further obtain the following corollary on the regret guarantee:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细使用Berge的最大定理（Berge，[1877](#bib.bib16)），我们证明了[方程 5](#S5.E5 "In Theorem 2\.
    ‣ 5.2 Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")的右侧*不*依赖于$k$（参见[第D.2节](#A4.SS2 "D.2 Deferred Proof
    for the Arguments in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")），我们进一步得到关于悔恨保证的以下推论：
- en: Corollary 1.
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 推论 1。
- en: '*(Regret).* Suppose^§^§§Note that these conditions on $h,f$). Then, with high
    probably, we have'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*(悔恨).* 假设^§^§§注意这些条件对$h,f$的影响)。那么，高概率下，我们有'
- en: '|  | $1$2 |  | (6) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: Corollary 2.
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 推论 2。
- en: '*(Emerging behavior: Coarse correlated equilibrium)*. For a sufficiently large
    $N_{T}$, then the time-averaged policy for each agent will constitute an approximate
    coarse correlated equilibrium of the game.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*(新兴行为：粗略相关均衡)*。对于足够大的$N_{T}$，每个代理的时间平均策略将构成游戏的近似粗略相关均衡。'
- en: 'Proofs of [Theorem 2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [Corollary 1](#Thmcorollary1
    "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") are deferred to [Section D.3](#A4.SS3 "D.3
    Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), and [Corollary 2](#Thmcorollary2 "Corollary 2\. ‣ 5.2 Guarantees
    via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    follows from the folklore result discussed in [Remark 1](#Thmremark1 "Remark 1
    (Implication for playing repeated games). ‣ 4.3 Case Study: Pre-Training under
    Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"). Therefore, if additionally, the LLM parameterization
    (i.e., Transformers) can realize a no-regret algorithm (for example, the single-layer
    self-attention model can construct FTRL, as to be shown next in [Section 5.3](#S5.SS3
    "5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")), then [Corollary 1](#Thmcorollary1
    "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") means that with a large enough number of
    samples $N_{T}$.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[定理 2](#Thmtheorem2 "定理 2. ‣ 5.2 通过悔恨损失最小化的保证 ‣ 5 通过无监督损失证明促进无悔行为 ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")
    和 [引理 1](#Thmcorollary1 "引理 1. ‣ 5.2 通过悔恨损失最小化的保证 ‣ 5 通过无监督损失证明促进无悔行为 ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")
    的证明被推迟到[第 D.3 节](#A4.SS3 "D.3 定理 2 和引理 1 的推迟证明 ‣ 附录 D 在第 5 节中的推迟结果和证明 ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")，而
    [引理 2](#Thmcorollary2 "引理 2. ‣ 5.2 通过悔恨损失最小化的保证 ‣ 5 通过无监督损失证明促进无悔行为 ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")
    从[备注 1](#Thmremark1 "备注 1（对重复游戏的影响）。 ‣ 4.3 案例研究：在规范数据分布下的预训练 ‣ 4 为什么预训练的大型语言模型（不）悔恨？一个假设模型和一些理论见解
    ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究") 中讨论的民间结果推导得出。因此，如果另外，LLM 参数化（即 Transformers）能够实现无悔算法（例如，单层自注意力模型可以构造
    FTRL，如 [第 5.3 节](#S5.SS3 "5.3 最小化悔恨损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为 ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")
    中将展示的），那么 [引理 1](#Thmcorollary1 "引理 1. ‣ 5.2 通过悔恨损失最小化的保证 ‣ 5 通过无监督损失证明促进无悔行为
    ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究") 意味着当样本数量 $N_{T}$ 足够大时。'
- en: Despite the power and generality of the previous results, one cannot use an
    *infinitely large* $N$ is finite, for the specific parameterization of the LLMs
    using Transformers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管之前结果的威力和普遍性很强，但对于使用 Transformers 的 LLMs 特定参数化，不能使用 *无限大* 的 $N$，因为 $N$ 是有限的。
- en: 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 最小化悔恨损失可以自动产生已知的在线学习算法
- en: 'We now study the setting of minimizing [Equation 3](#S5.E3 "In 5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    when $\text{LLM}_{\theta}$.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在研究最小化[方程 3](#S5.E3 "在 5.1 节 无监督训练损失：悔恨损失 ‣ 5 通过无监督损失证明促进无悔行为 ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")时的设置，当
    $\text{LLM}_{\theta}$。
- en: 'Firstly, we consider the following structure of single-layer self-attention
    model $g$ (see a formal introduction in [Section A.1](#A1.SS1 "A.1 Additional
    Definitions for Appendix ‣ Appendix A Deferred Background ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games")):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们考虑单层自注意力模型 $g$ 的以下结构（有关正式介绍，请参见[第 A.1 节](#A1.SS1 "A.1 附录的额外定义 ‣ 附录 A 推迟的背景
    ‣ 大型语言模型是否会有悔恨？在线学习和游戏中的案例研究")）：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: where $Z_{t}=(\ell_{1},\dots,\ell_{t},c)$ is a constant vector. We then have
    the following result.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z_{t}=(\ell_{1},\dots,\ell_{t},c)$ 是一个常量向量。我们随后得到以下结果。
- en: Theorem 3.
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3。
- en: Consider the policy space $\Pi=B(0,R_{\Pi},\|\cdot\|)$ and
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑策略空间 $\Pi=B(0,R_{\Pi},\|\cdot\|)$ 和
- en: '|  | $1$2 |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'is a first-order stationary point of [Equation 3](#S5.E3 "In 5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    with $N=1$.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 是[方程 3](#S5.E3 "在 5.1 无监督训练损失：后悔损失 ‣ 5 通过无监督损失证明促进无后悔行为 ‣ LLM 代理是否有后悔？在线学习和博弈中的案例研究")的**一阶**静态点，其中
    $N=1$。
- en: 'In practical training, such stationary points of the loss may be attained by
    first-order optimization algorithms of (stochastic) gradient descent, the workhorse
    in machine learning. Moreover, we also consider the single-layer *linear* self-attention
    model as follows, for which we can strengthen the results above from a stationary-point
    to an *optimal-solution* argument:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际训练中，这样的损失的静态点可以通过（随机）梯度下降的**一阶**优化算法来实现，这是机器学习中的主要工具。此外，我们还考虑如下的单层*线性*自注意力模型，对于该模型，我们可以将上述结果从静态点强化为*最优解*的论证：
- en: '|  | $1$2 |  | (8) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: Theorem 4.
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 4。
- en: Consider the policy space $\Pi=B(0,R_{\Pi},\|\cdot\|)$.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑策略空间 $\Pi=B(0,R_{\Pi},\|\cdot\|)$。
- en: '[Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") shows the capacity of self-attention Transformer models to realize
    online learning algorithms, thanks to the regret-loss we proposed. In particular,
    this can be achieved automatically by optimizing the new loss, *without* hard-coding
    the parameters of the Transformer.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[定理 4](#Thmtheorem4 "定理 4 ‣ 5.3 最小化后悔损失可以自动生成已知的在线学习算法 ‣ 5 通过无监督损失证明促进无后悔行为
    ‣ LLM 代理是否有后悔？在线学习和博弈中的案例研究") 展示了自注意力 Transformer 模型实现在线学习算法的能力，这得益于我们提出的后悔损失。特别是，这可以通过优化新的损失*自动*实现，而无需对
    Transformer 的参数进行硬编码。'
- en: The above results are for the case of FTRL with an $L_{2}$ in the Experts Problem.
    We defer the discussion of this case to [Section D.7](#A4.SS7 "D.7 Discussions
    on the Production of FTRL with Entropy Regularization ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"). Through these results, we can also guarantee in the repeated game
    setting that approximate coarse correlated equilibria would emerge in the long
    run, since each player will exhibit no-regret behavior, using a similar argument
    as that for [Corollary 2](#Thmcorollary2 "Corollary 2\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果适用于具有 $L_{2}$ 的专家问题中的 FTRL 情况。我们将讨论此情况的内容推迟到[第 D.7 节](#A4.SS7 "D.7 讨论具有熵正则化的
    FTRL 的产生 ‣ 附录 D 推迟的结果和第 5 节的证明 ‣ LLM 代理是否有后悔？在线学习和博弈中的案例研究")。通过这些结果，我们还可以在重复博弈的设置中保证长期内会出现近似粗略相关均衡，因为每个玩家将展示无后悔行为，使用与[推论
    2](#Thmcorollary2 "推论 2 ‣ 5.2 通过后悔损失最小化的保证 ‣ 5 通过无监督损失证明促进无后悔行为 ‣ LLM 代理是否有后悔？在线学习和博弈中的案例研究")相似的论证。
- en: Remark 2.
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 2。
- en: The very recent studies by (Ahn et al., [2023](#bib.bib5); Zhang et al., [2023a](#bib.bib121);
    Mahankali et al., [2023](#bib.bib74)) have demonstrated that if $Z_{t}=((x_{1},y_{1}),\dots,(x_{t},y_{t}),(x_{t+1},0))$-labels,
    such an implicit gradient descent update-rule is hard to define. Compared to the
    previous studies, our global optimizer among single-layer linear self-attention
    models is an *explicit* and *online* gradient descent update for online learning.
    With a different loss (regret-loss v.s. instruction-tuning-loss), the techniques
    to obtain the seemingly similar results are also fundamentally different.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究（Ahn 等人，[2023](#bib.bib5)；Zhang 等人，[2023a](#bib.bib121)；Mahankali 等人，[2023](#bib.bib74)）已证明，如果
    $Z_{t}=((x_{1},y_{1}),\dots,(x_{t},y_{t}),(x_{t+1},0))$-标签，这样的隐式梯度下降更新规则难以定义。与之前的研究相比，我们在单层线性自注意力模型中的全局优化器是**显式**和*在线*的梯度下降更新，用于在线学习。由于不同的损失（后悔损失与指令调整损失），获得表面上相似结果的技术也从根本上不同。
- en: 5.4 Experimental Results for Minimizing Regret-Loss
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 最小化后悔损失的实验结果
- en: 'We now provide experimental results for minimizing our *regret-loss*, and evaluate
    in the following environments: 1) randomly-generated loss sequences ([Figure 9](#S5.F9
    "In Randomly generated loss sequences. ‣ 5.4 Experimental Results for Minimizing
    Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")); 2)
    loss sequences with a predictable trend ([Figure 10](#S5.F10 "In Randomly generated
    loss sequences. ‣ 5.4 Experimental Results for Minimizing Regret-Loss ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")); 3) repeated games ([Figure 11](#S5.F11
    "In Repeated games. ‣ 5.4 Experimental Results for Minimizing Regret-Loss ‣ 5
    Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")); and 4) counterexamples
    for pre-trained LLMs to be regrettable ([Figure 7](#S3.F7 "In Randomly generated
    games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do Pre-Trained LLMs Have
    Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")). Details of the training setup can be found in [Section D.8](#A4.SS8
    "D.8 Training Details of Section 5.4 ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"). We also provide an ablation study for the training of the loss [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") in [Section D.9](#A4.SS9 "D.9 Ablation Study on Training
    Equation 3 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在提供了最小化我们*遗憾损失*的实验结果，并在以下环境中进行评估：1) 随机生成的损失序列 ([图 9](#S5.F9 "在随机生成的损失序列中。
    ‣ 5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")); 2) 具有可预测趋势的损失序列
    ([图 10](#S5.F10 "在随机生成的损失序列中。 ‣ 5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究"));
    3) 重复游戏 ([图 11](#S5.F11 "在重复游戏中。 ‣ 5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM
    代理是否有遗憾？在线学习和游戏中的案例研究")); 和 4) 预训练LLMs的反例 ([图 7](#S3.F7 "在随机生成的游戏中。 ‣ 3.3 结果：多玩家重复游戏
    ‣ 3 预训练LLMs是否有遗憾？实验验证 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究"))。训练设置的详细信息可以在[第 D.8 节](#A4.SS8
    "D.8 第 5.4 节的训练细节 ‣ 附录 D 第 5 节的延迟结果和证明 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")中找到。我们还提供了关于损失[方程式
    3](#S5.E3 "在 5.1 新的无监督训练损失：遗憾损失 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")的消融研究，详见[第
    D.9 节](#A4.SS9 "D.9 方程式 3 的消融研究 ‣ 附录 D 第 5 节的延迟结果和证明 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")。
- en: Randomly generated loss sequences.
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机生成的损失序列。
- en: 'We use the same loss vectors as those in [Section 3.2](#S3.SS2 "3.2 Results:
    Online Learning ‣ 3 Do Pre-Trained LLMs Have Regret? Experimental Validation ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") for randomly
    generated loss functions, and compare the results with that using GPT-4\. The
    results show that with regret-loss, both the trained single-layer self-attention
    model and the trained Transformers with multi-layer self-attention structures
    can achieve comparable regrets as FTRL and GPT-4\. The results can be found in
    [Figure 9](#S5.F9 "In Randomly generated loss sequences. ‣ 5.4 Experimental Results
    for Minimizing Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与[第 3.2 节](#S3.SS2 "3.2 结果：在线学习 ‣ 3 预训练LLMs是否有遗憾？实验验证 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")中相同的损失向量进行随机生成的损失函数，并将结果与使用
    GPT-4 的结果进行比较。结果显示，通过遗憾损失，训练后的单层自注意力模型和多层自注意力结构的训练 Transformer 可以实现与 FTRL 和 GPT-4
    相当的遗憾。结果可以在[图 9](#S5.F9 "在随机生成的损失序列中。 ‣ 5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失证明促进无遗憾行为
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")中找到。
- en: '![Refer to caption](img/fc59f0ef2d19b560267403f2b8f4fd2a.png)![Refer to caption](img/075028bc9021be5f83cdbe26c4eb39ea.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc59f0ef2d19b560267403f2b8f4fd2a.png)![参见说明](img/075028bc9021be5f83cdbe26c4eb39ea.png)'
- en: 'Figure 9: Regret performance for the randomly generated loss sequences that
    are generated by Gaussian with truncation and uniform distribution. No-regret
    behaviors of single-layer and multi-layer self-attention models are validated
    by both of our frameworks (low $p$).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：随机生成的损失序列的遗憾性能，这些序列由截断高斯分布和均匀分布生成。单层和多层自注意力模型的无遗憾行为都由我们的两个框架验证（低 $p$）。
- en: '![Refer to caption](img/177c98600d024577c329cc7d64f19c45.png)![Refer to caption](img/85a731c69237aae5bef9e1444b96efa3.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/177c98600d024577c329cc7d64f19c45.png)![参考图注](img/85a731c69237aae5bef9e1444b96efa3.png)'
- en: 'Figure 10: Regret performance for the randomly generated loss sequences that
    are generated by linear-trend and sine-trend. No-regret behaviors of single-layer
    and multi-layer self-attention models are validated by both of our frameworks
    (low $p$).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：随机生成的损失序列的遗憾性能，这些序列由线性趋势和正弦趋势生成。单层和多层自注意力模型的无遗憾行为都由我们的两个框架验证（低 $p$）。
- en: Loss sequences with a predictable trend.
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 具有可预测趋势的损失序列。
- en: 'We investigate the case where the loss sequences have predictable trends such
    as linear-trend or sine-trend. One might expect that the performance of the trained
    Transformer would surpass the performance of traditional no-regret learning algorithms
    such as FTRL, since they may not be an optimal algorithm for the loss sequence
    with a predictable trend. We modify the training distribution by changing the
    distribution of random variable $Z$) to follow two kinds of trends: linear and
    sine functions. The results, as illustrated in [Figure 10](#S5.F10 "In Randomly
    generated loss sequences. ‣ 5.4 Experimental Results for Minimizing Regret-Loss
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games"), show that the trained
    single-layer self-attention model and the trained Transformer with multi-layer
    self-attention structures with regret-loss outperformed GPT-4 and FTRL in terms
    of regret, when the loss sequence is a linear trend. Similarly, [Figure 10](#S5.F10
    "In Randomly generated loss sequences. ‣ 5.4 Experimental Results for Minimizing
    Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") shows
    that the trained Transformer with multi-layer self-attention structures with regret-loss
    is comparable to GPT-4 and outperformed FTRL in terms of regret, when the loss
    sequence is a sine-trend. Note that the training dataset does not contain the
    sequence of losses. Nonetheless, by focusing on the overall trend during training,
    we can attain performance that is either superior to or on par with that of FTRL
    and GPT-4.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了损失序列具有可预测趋势（如线性趋势或正弦趋势）的情况。可以预期，训练后的 Transformer 的性能会超越传统的无遗憾学习算法（如 FTRL），因为这些传统算法可能不是针对具有可预测趋势的损失序列的最佳算法。我们通过将随机变量
    $Z$ 的分布更改为遵循两种趋势：线性和正弦函数，来修改训练分布。如 [图 10](#S5.F10 "在随机生成的损失序列中。 ‣ 5.4 最小化遗憾损失的实验结果
    ‣ 5 通过无监督损失证明无遗憾行为 ‣ 大型语言模型代理是否有遗憾？在线学习和博弈的案例研究") 所示，当损失序列为线性趋势时，训练后的单层自注意力模型和具有多层自注意力结构的
    Transformer 在遗憾损失方面的表现优于 GPT-4 和 FTRL。同样，[图 10](#S5.F10 "在随机生成的损失序列中。 ‣ 5.4 最小化遗憾损失的实验结果
    ‣ 5 通过无监督损失证明无遗憾行为 ‣ 大型语言模型代理是否有遗憾？在线学习和博弈的案例研究") 显示，当损失序列为正弦趋势时，训练后的具有多层自注意力结构的
    Transformer 在遗憾方面与 GPT-4 相当，并且优于 FTRL。注意，训练数据集不包含损失序列。尽管如此，通过在训练期间关注整体趋势，我们可以达到优于或与
    FTRL 和 GPT-4 相当的性能。
- en: Repeated games.
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重复博弈。
- en: 'We then investigate the case of multi-player repeated games. We study 2x2,
    3x3x3, 3x3x3x3 games, where each entry of the payoff matrix is sampled randomly
    from $\operatorname{Unif}([0,10])$. The results, as illustrated in [Figure 11](#S5.F11
    "In Repeated games. ‣ 5.4 Experimental Results for Minimizing Regret-Loss ‣ 5
    Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games"), show that the trained
    single-layer self-attention model and the trained Transformer with multi-layer
    self-attention structures with regret-loss have a similar performance as that
    of FTRL. However, GPT-4 still outperforms the trained single-layer self-attention
    model and the trained Transformer with multi-layer self-attention structures in
    terms of regret. Since for repeated games (in which the environment faced by the
    agent can be less adversarial than that in the online setting), there might be
    a better algorithm than FTRL (see e.g., Daskalakis et al. ([2021](#bib.bib32))),
    while our self-attention models have a similar structure as FTRL ([Theorem 3](#Thmtheorem3
    "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online
    Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    or [Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")). Also, in practical training (with the empirical loss in [Equation 4](#S5.E4
    "In Definition 3 (Empirical loss function). ‣ 5.2 Guarantees via Regret-Loss Minimization
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")), we possibly did not
    find the exact global minimum or stationary point of the *expected* loss in [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games"). Hence, it is possible that GPT-4 may have lower regret
    than our trained models with the regret-loss.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着研究了多玩家重复博弈的情况。我们研究了2x2、3x3x3、3x3x3x3的博弈，每个支付矩阵的条目是从$\operatorname{Unif}([0,10])$中随机采样的。如[图11](#S5.F11
    "在重复博弈中。 ‣ 5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")所示，经过训练的单层自注意力模型和带有多层自注意力结构的Transformer在遗憾损失方面表现与FTRL相似。然而，GPT-4在遗憾方面仍优于经过训练的单层自注意力模型和带有多层自注意力结构的Transformer。由于在重复博弈中（代理面临的环境可能不如在线设置中的对抗性），可能存在比FTRL更好的算法（参见Daskalakis等人（[2021](#bib.bib32)）），而我们的自注意力模型具有与FTRL类似的结构（[定理3](#Thmtheorem3
    "定理3。 ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")或[定理4](#Thmtheorem4
    "定理4。 ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")）。此外，在实际训练中（在[方程4](#S5.E4
    "在定义3（经验损失函数）。 ‣ 5.2 通过遗憾损失最小化的保证 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")中经验损失），我们可能没有找到*期望*损失的准确全局最小值或驻点（[方程3](#S5.E3
    "在5.1 一种新的无监督训练损失：遗憾损失 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")）。因此，GPT-4的遗憾可能低于我们训练的带有遗憾损失的模型。
- en: '![Refer to caption](img/bbe3e58f98005139f25e6b113a111122.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bbe3e58f98005139f25e6b113a111122.png)'
- en: 'Figure 11: Regret performance for the game with two players, three players,
    and four players general-sum games. No-regret behaviors of single-layer and multi-layer
    self-attention models are validated by both of our frameworks (low $p$).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：针对两名玩家、三名玩家和四名玩家的一般总和博弈的遗憾表现。我们两个框架验证了单层和多层自注意力模型的无遗憾行为（低$p$）。
- en: Two scenarios that caused regrettable behaviors of GPT-4.
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT-4导致遗憾行为的两个场景。
- en: 'Finally, we investigate the cases that have caused GPT-4 to have regrettable
    performance in [Section 3.2](#S3.SS2 "3.2 Results: Online Learning ‣ 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"). The results, which can be found in [Figure 7](#S3.F7
    "In Randomly generated games. ‣ 3.3 Results: Multi-Player Repeated Games ‣ 3 Do
    Pre-Trained LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), show that both the trained single-layer
    self-attention model and the trained Transformer with regret-loss can achieve
    comparable no-regret performance as FTRL, and outperforms that of GPT-4\. This
    validates that our new unsupervised training loss can address the regrettable
    cases, as our theory in [Sections 5.2](#S5.SS2 "5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [5.3](#S5.SS3
    "5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") has predicted.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调查了[第3.2节](#S3.SS2 "3.2 结果：在线学习 ‣ 3 预训练LLMs是否有悔恨？实验验证 ‣ LLM代理是否有悔恨？在线学习和博弈中的案例研究")中导致GPT-4表现令人遗憾的情况。结果可以在[图7](#S3.F7
    "在随机生成的游戏中。 ‣ 3.3 结果：多玩家重复博弈 ‣ 3 预训练LLMs是否有悔恨？实验验证 ‣ LLM代理是否有悔恨？在线学习和博弈中的案例研究")中找到，显示训练的单层自注意模型和带有悔恨损失的训练变换器可以实现与FTRL相当的无悔恨性能，并超越GPT-4。这验证了我们的新无监督训练损失可以解决令人遗憾的案例，正如我们在[第5.2节](#S5.SS2
    "5.2 通过悔恨损失最小化的保证 ‣ 5 通过无监督损失证明性促进无悔恨行为 ‣ LLM代理是否有悔恨？在线学习和博弈中的案例研究")和[5.3节](#S5.SS3
    "5.3 最小化悔恨损失可以自动生成已知的在线学习算法 ‣ 5 通过无监督损失证明性促进无悔恨行为 ‣ LLM代理是否有悔恨？在线学习和博弈中的案例研究")中的理论预测。
- en: 6 Concluding Remarks
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 总结
- en: In this paper, we studied the online decision-making and strategic behaviors
    of LLMs quantitatively, through the metric of regret. We first examined and validated
    the no-regret behavior of several representative pre-trained LLMs in benchmark
    settings of online learning and games. As a consequence, (coarse correlated) equilibrium
    can oftentimes emerge as the long-term outcome of multiple LLMs playing repeated
    games. We then provide some theoretical insights into the no-regret behavior,
    by connecting pre-trained LLMs to the follow-the-perturbed-leader algorithm in
    online learning, under certain assumptions. We also identified (simple) cases
    where pre-trained LLMs fail to be no-regret, and thus proposed a new unsupervised
    training loss, *regret-loss*, to provably promote the no-regret behavior of Transformers
    without the labels of (optimal) actions. We established both experimental and
    theoretical evidence for the effectiveness of our regret-loss.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过悔恨的度量定量研究了LLMs的在线决策和战略行为。我们首先检查并验证了几个代表性预训练LLMs在在线学习和博弈基准设置中的无悔恨行为。因此，（粗糙相关的）均衡常常会作为多个LLMs重复博弈的长期结果出现。然后，我们通过将预训练LLMs与在线学习中的扰动跟随者算法联系起来，在特定假设下提供了一些关于无悔恨行为的理论见解。我们还识别了预训练LLMs未能实现无悔恨的（简单）情况，因此提出了一种新的无监督训练损失，*悔恨损失*，以证明性地促进变换器的无悔恨行为，而无需（最优）动作的标签。我们建立了实验和理论证据来证明我们的悔恨损失的有效性。
- en: 'As a first attempt toward rigorously understanding the online and strategic
    decision-making behaviors of LLMs through the metric of regret, our work has opened
    up fruitful directions for future research:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 作为通过悔恨度量来严谨理解LLMs的在线和战略决策行为的首次尝试，我们的工作为未来的研究开辟了富有成效的方向：
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: There are more than one definitions of (dynamic-)regret in the online learning
    literature, and we mainly focused on the so-called *external-regret* in the literature.
    It would be interesting to study the no-regret behavior of LLMs in terms of other
    regret metrics, e.g., swap-regret (Blum and Mansour, [2007](#bib.bib19)), which
    may lead to stronger equilibrium notions in playing repeated games.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在线学习文献中存在多个（动态）悔恨的定义，我们主要关注了文献中的*外部悔恨*。研究LLMs在其他悔恨度量下的无悔恨行为，例如交换悔恨（Blum和Mansour，[2007](#bib.bib19)），将是很有趣的，这可能会在重复博弈中导致更强的均衡概念。
- en: •
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our new regret-loss has exhibited promises in our experiments for training modest-scale
    Transformers. We are currently generalizing it to training other larger-scale
    models, such as Foundation Models, for decision-making.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的新后悔损失在我们对中等规模Transformers的训练实验中展现出了潜力。我们目前正在将其推广到训练其他大规模模型，如基础模型，以用于决策制定。
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No-regret behavior can sometimes lead to better outcomes in terms of social
    efficiency (Blum et al., [2008](#bib.bib18); Roughgarden, [2015](#bib.bib88);
    Nekipelov et al., [2015](#bib.bib80)). It would thus be interesting to further
    validate the efficiency of no-regret LLM agents in these scenarios, as well as
    identifying new prompts and training losses for LLMs to promote the efficiency
    of the outcomes.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无悔行为有时可以在社会效率方面带来更好的结果（Blum et al., [2008](#bib.bib18); Roughgarden, [2015](#bib.bib88);
    Nekipelov et al., [2015](#bib.bib80)）。因此，进一步验证无悔LLM代理在这些情境中的效率，以及识别新的提示和训练损失以提高LLM的结果效率，将是非常有趣的。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To evaluate the performance quantitatively, we focused on online learning and
    games with *numeric valued* payoffs. It would be interesting to connect our no-regret-based
    and game-theoretic framework with existing multi-LLM frameworks, e.g., debate,
    collaborative problem-solving, and human/social behavior simulation, with potentially
    new notions of regret (defined in different spaces) as performance metrics.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了定量评估性能，我们集中于在线学习和具有*数值值*的回报的游戏。将我们的无悔基础和博弈论框架与现有的多LLM框架（例如，辩论、协作问题解决和人类/社会行为模拟）连接起来，结合可能的新后悔概念（在不同空间中定义）作为性能指标，将是很有趣的。
- en: Acknowledgement
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors thank Dingwen Kong for discussing the truncation idea in proving
    [Lemma 8](#Thmlemma8 "Lemma 8\. ‣ Proof. ‣ D.2 Deferred Proof for the Arguments
    in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"). Also, the authors
    thank Kristian Georgiev and Aleksander Madry for the helpful feedback.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢Dingwen Kong讨论在证明[引理8](#Thmlemma8 "Lemma 8\. ‣ Proof. ‣ D.2 Deferred Proof
    for the Arguments in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")中的截断想法。同时，作者感谢Kristian
    Georgiev和Aleksander Madry的宝贵反馈。
- en: References
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abernethy et al. (2014) Abernethy, J., Lee, C., Sinha, A. and Tewari, A. (2014).
    Online linear optimization via smoothing. In Conference on Learning Theory. PMLR.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abernethy et al. (2014) Abernethy, J., Lee, C., Sinha, A. 和 Tewari, A. (2014)。通过平滑进行在线线性优化。学习理论会议。PMLR。
- en: Abernethy et al. (2015) Abernethy, J. D., Lee, C. and Tewari, A. (2015). Fighting
    bandits with a new kind of smoothness. Advances in Neural Information Processing
    Systems, 28.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abernethy et al. (2015) Abernethy, J. D., Lee, C. 和 Tewari, A. (2015)。用一种新型平滑度对抗赌博者。神经信息处理系统进展，28。
- en: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al.
    (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. 等 (2023)。Gpt-4技术报告。arXiv预印本
    arXiv:2303.08774。
- en: Aher et al. (2023) Aher, G. V., Arriaga, R. I. and Kalai, A. T. (2023). Using
    large language models to simulate multiple humans and replicate human subject
    studies. In International Conference on Machine Learning. PMLR.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aher et al. (2023) Aher, G. V., Arriaga, R. I. 和 Kalai, A. T. (2023)。使用大型语言模型模拟多个人类并复制人类研究。国际机器学习大会论文集。PMLR。
- en: Ahn et al. (2023) Ahn, K., Cheng, X., Daneshmand, H. and Sra, S. (2023). Transformers
    learn to implement preconditioned gradient descent for in-context learning. Advanced
    in Neural Information Processing Systems.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn et al. (2023) Ahn, K., Cheng, X., Daneshmand, H. 和 Sra, S. (2023)。Transformers
    学会实现预条件梯度下降用于上下文学习。高级神经信息处理系统。
- en: 'Ahn et al. (2022) Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,
    David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K. et al. (2022). Do
    as i can, not as i say: Grounding language in robotic affordances. arXiv preprint
    arXiv:2204.01691.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn et al. (2022) Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,
    David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K. 等 (2022)。按我所能做，而非按我所说做：将语言基础于机器人能力。arXiv预印本
    arXiv:2204.01691。
- en: Ahsanullah et al. (2013) Ahsanullah, M., Nevzorov, V. B. and Shakil, M. (2013).
    An introduction to order statistics, vol. 8. Springer.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahsanullah et al. (2013) Ahsanullah, M., Nevzorov, V. B. 和 Shakil, M. (2013)。有序统计学导论，第8卷。Springer。
- en: Akata et al. (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge,
    M. and Schulz, E. (2023). Playing repeated games with large language models. arXiv
    preprint arXiv:2305.16867.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata 等 (2023) Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M.
    和 Schulz, E. (2023). 与大型语言模型进行重复游戏。arXiv 预印本 arXiv:2305.16867。
- en: Akyürek et al. (2023) Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. and Zhou,
    D. (2023). What learning algorithm is in-context learning? investigations with
    linear models. International Conference on Learning Representations.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akyürek 等 (2023) Akyürek, E., Schuurmans, D., Andreas, J., Ma, T. 和 Zhou, D.
    (2023). 什么学习算法是上下文学习？基于线性模型的研究。国际学习表示会议。
- en: 'Argyle et al. (2023) Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R.,
    Rytting, C. and Wingate, D. (2023). Out of one, many: Using language models to
    simulate human samples. Political Analysis, 31 337–351.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argyle 等 (2023) Argyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting,
    C. 和 Wingate, D. (2023). 从一中多：使用语言模型模拟人类样本。《政治分析》，31 337–351。
- en: 'Arora et al. (2012) Arora, S., Hazan, E. and Kale, S. (2012). The multiplicative
    weights update method: a meta-algorithm and applications. Theory of computing,
    8 121–164.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 等 (2012) Arora, S., Hazan, E. 和 Kale, S. (2012). 乘法权重更新方法：一种元算法及其应用。《计算理论》，8
    121–164。
- en: Auer et al. (2002) Auer, P., Cesa-Bianchi, N., Freund, Y. and Schapire, R. E.
    (2002). The nonstochastic multiarmed bandit problem. SIAM journal on computing,
    32 48–77.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Auer 等 (2002) Auer, P., Cesa-Bianchi, N., Freund, Y. 和 Schapire, R. E. (2002).
    非随机多臂赌博机问题。《SIAM 计算期刊》，32 48–77。
- en: 'Bai et al. (2023) Bai, Y., Chen, F., Wang, H., Xiong, C. and Mei, S. (2023).
    Transformers as statisticians: Provable in-context learning with in-context algorithm
    selection. Advanced in Neural Information Processing Systems.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等 (2023) Bai, Y., Chen, F., Wang, H., Xiong, C. 和 Mei, S. (2023). 变换器作为统计学家：证明的上下文学习与上下文算法选择。《神经信息处理系统进展》。
- en: Bakhtin et al. (2022) Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty,
    C., Fried, D., Goff, A., Gray, J., Hu, H. et al. (2022). Human-level play in the
    game of diplomacy by combining language models with strategic reasoning. Science,
    378 1067–1074.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bakhtin 等 (2022) Bakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C.,
    Fried, D., Goff, A., Gray, J., Hu, H. 等 (2022). 通过结合语言模型和战略推理在人类级别的外交游戏中进行游戏。《科学》，378
    1067–1074。
- en: 'Balseiro and Gur (2019) Balseiro, S. R. and Gur, Y. (2019). Learning in repeated
    auctions with budgets: Regret minimization and equilibrium. Management Science,
    65 3952–3968.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balseiro 和 Gur (2019) Balseiro, S. R. 和 Gur, Y. (2019). 带预算的重复拍卖中的学习：悔恨最小化与均衡。《管理科学》，65
    3952–3968。
- en: 'Berge (1877) Berge, C. (1877). Topological spaces: Including a treatment of
    multi-valued functions, vector spaces and convexity. Oliver & Boyd.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berge (1877) Berge, C. (1877). 拓扑空间：包括多值函数、向量空间和凸性的处理。Oliver & Boyd。
- en: Besbes et al. (2014) Besbes, O., Gur, Y. and Zeevi, A. (2014). Stochastic multi-armed-bandit
    problem with non-stationary rewards. Advances in neural information processing
    systems, 27.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besbes 等 (2014) Besbes, O., Gur, Y. 和 Zeevi, A. (2014). 非平稳奖励的随机多臂赌博机问题。《神经信息处理系统进展》，27。
- en: Blum et al. (2008) Blum, A., Hajiaghayi, M., Ligett, K. and Roth, A. (2008).
    Regret minimization and the price of total anarchy. In Proceedings of the fortieth
    annual ACM symposium on Theory of computing.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum 等 (2008) Blum, A., Hajiaghayi, M., Ligett, K. 和 Roth, A. (2008). 悔恨最小化与完全混乱的代价。在第四十届年度
    ACM 理论计算机学会研讨会论文集中。
- en: Blum and Mansour (2007) Blum, A. and Mansour, Y. (2007). From external to internal
    regret. Journal of Machine Learning Research, 8.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum 和 Mansour (2007) Blum, A. 和 Mansour, Y. (2007). 从外部悔恨到内部悔恨。《机器学习研究期刊》，8。
- en: 'Brookins and DeBacker (2023) Brookins, P. and DeBacker, J. M. (2023). Playing
    games with GPT: What can we learn about a large language model from canonical
    strategic games? Available at SSRN 4493398.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brookins 和 DeBacker (2023) Brookins, P. 和 DeBacker, J. M. (2023). 与 GPT 一起玩游戏：我们可以从经典战略游戏中学到什么关于大型语言模型的知识？可在
    SSRN 4493398 上获得。
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. et al. (2020).
    Language models are few-shot learners. Advances in neural information processing
    systems, 33 1877–1901.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. 等 (2020). 语言模型是少样本学习者。《神经信息处理系统进展》，33
    1877–1901。
- en: Bubeck et al. (2012) Bubeck, S., Cesa-Bianchi, N. et al. (2012). Regret analysis
    of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends®
    in Machine Learning, 5 1–122.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等 (2012) Bubeck, S., Cesa-Bianchi, N. 等 (2012). 随机和非随机多臂赌博机问题的悔恨分析。《机器学习基础与趋势®》，5
    1–122。
- en: 'Bubeck et al. (2023) Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
    Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. et al. (2023).
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等（2023）Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz,
    E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S. 等（2023）。人工通用智能的火花：GPT-4
    的早期实验。arXiv 预印本 arXiv:2303.12712。
- en: 'Camerer (2011) Camerer, C. F. (2011). Behavioral game theory: Experiments in
    strategic interaction. Princeton University Press.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Camerer（2011）Camerer, C. F.（2011）。行为博弈论：战略互动实验。普林斯顿大学出版社。
- en: Cesa-Bianchi et al. (1996) Cesa-Bianchi, N., Long, P. M. and Warmuth, M. K.
    (1996). Worst-case quadratic loss bounds for prediction using linear functions
    and gradient descent. IEEE Transactions on Neural Networks, 7 604–619.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cesa-Bianchi 等（1996）Cesa-Bianchi, N., Long, P. M. 和 Warmuth, M. K.（1996）。使用线性函数和梯度下降的最坏情况二次损失界限。IEEE
    神经网络交易，7 604–619。
- en: Cesa-Bianchi and Lugosi (2006) Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction,
    Learning, and Games. Cambridge University Press.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cesa-Bianchi 和 Lugosi（2006）Cesa-Bianchi, N. 和 Lugosi, G.（2006）。预测、学习与博弈。剑桥大学出版社。
- en: 'Chan et al. (2024) Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S.,
    Fu, J. and Liu, Z. (2024). Chateval: Towards better llm-based evaluators through
    multi-agent debate. International Conference on Learning Representations.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等（2024）Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J.
    和 Liu, Z.（2024）。Chateval：通过多智能体辩论实现更好的基于 LLM 的评估器。国际学习表征会议。
- en: 'Chen et al. (2024) Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C.,
    Chan, C.-M., Qin, Y., Lu, Y., Xie, R. et al. (2024). Agentverse: Facilitating
    multi-agent collaboration and exploring emergent behaviors in agents. International
    Conference on Learning Representations.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2024）Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M.,
    Qin, Y., Lu, Y., Xie, R. 等（2024）。Agentverse：促进多智能体协作及探索智能体的涌现行为。国际学习表征会议。
- en: Chen et al. (2023) Chen, Y., Liu, T. X., Shan, Y. and Zhong, S. (2023). The
    emergence of economic rationality of gpt. Proceedings of the National Academy
    of Sciences, 120 e2316205120.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023）Chen, Y., Liu, T. X., Shan, Y. 和 Zhong, S.（2023）。GPT 经济理性的出现。美国国家科学院院刊，120
    e2316205120。
- en: 'Dai et al. (2023) Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z. and
    Wei, F. (2023). Why can GPT learn in-context? language models secretly perform
    gradient descent as meta-optimizers. In Findings of the Association for Computational
    Linguistics: ACL 2023 (A. Rogers, J. Boyd-Graber and N. Okazaki, eds.). Association
    for Computational Linguistics, Toronto, Canada.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2023）Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z. 和 Wei, F.（2023）。GPT
    为什么能进行上下文学习？语言模型秘密地作为元优化器执行梯度下降。在《计算语言学协会会议发现：ACL 2023》 （A. Rogers, J. Boyd-Graber
    和 N. Okazaki 编）。计算语言学协会，多伦多，加拿大。
- en: '[https://aclanthology.org/2023.findings-acl.247](https://aclanthology.org/2023.findings-acl.247)'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://aclanthology.org/2023.findings-acl.247](https://aclanthology.org/2023.findings-acl.247)'
- en: Danskin (1966) Danskin, J. M. (1966). The theory of max-min, with applications.
    SIAM Journal on Applied Mathematics, 14 641–664.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danskin（1966）Danskin, J. M.（1966）。最大-最小理论及其应用。SIAM 应用数学杂志，14 641–664。
- en: Daskalakis et al. (2021) Daskalakis, C., Fishelson, M. and Golowich, N. (2021).
    Near-optimal no-regret learning in general games. Advances in Neural Information
    Processing Systems, 34 27604–27616.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daskalakis 等（2021）Daskalakis, C., Fishelson, M. 和 Golowich, N.（2021）。一般博弈中的近似最优无悔学习。神经信息处理系统进展，34
    27604–27616。
- en: 'Ding et al. (2022) Ding, J., Feng, Y. and Rong, Y. (2022). Myopic quantal response
    policy: Thompson sampling meets behavioral economics. arXiv preprint arXiv:2207.01028.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2022）Ding, J., Feng, Y. 和 Rong, Y.（2022）。短视量化响应策略：汤普森采样与行为经济学的交汇。arXiv
    预印本 arXiv:2207.01028。
- en: 'Driess et al. (2023) Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,
    A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T. et al. (2023). Palm-e:
    An embodied multimodal language model. International Conference on Machine Learning.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess 等（2023）Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A.,
    Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T. 等（2023）。Palm-e：一种具身的多模态语言模型。国际机器学习会议。
- en: Du et al. (2023) Du, Y., Li, S., Torralba, A., Tenenbaum, J. B. and Mordatch,
    I. (2023). Improving factuality and reasoning in language models through multiagent
    debate. arXiv preprint arXiv:2305.14325.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2023）Du, Y., Li, S., Torralba, A., Tenenbaum, J. B. 和 Mordatch, I.（2023）。通过多智能体辩论提升语言模型的真实性和推理能力。arXiv
    预印本 arXiv:2305.14325。
- en: 'Engel et al. (2023) Engel, C., Grossmann, M. R. and Ockenfels, A. (2023). Integrating
    machine behavior into human subject experiments: A user-friendly toolkit and illustrations.
    Available at SSRN.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engel 等 (2023) Engel, C., Grossmann, M. R. 和 Ockenfels, A. (2023). 将机器行为整合到人类受试者实验中：一个用户友好的工具包和实例。可在
    SSRN 上获取。
- en: 'Erev and Roth (1998) Erev, I. and Roth, A. E. (1998). Predicting how people
    play games: Reinforcement learning in experimental games with unique, mixed strategy
    equilibria. American Economic Review 848–881.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erev 和 Roth (1998) Erev, I. 和 Roth, A. E. (1998). 预测人们如何进行游戏：在具有唯一混合策略均衡的实验游戏中的强化学习。美国经济评论
    848–881。
- en: Fan et al. (2023) Fan, C., Chen, J., Jin, Y. and He, H. (2023). Can large language
    models serve as rational players in game theory? a systematic analysis. arXiv
    preprint arXiv:2312.05488.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等 (2023) Fan, C., Chen, J., Jin, Y. 和 He, H. (2023). 大型语言模型能否作为博弈论中的理性玩家？系统分析。arXiv
    预印本 arXiv:2312.05488。
- en: Freund and Schapire (1997) Freund, Y. and Schapire, R. E. (1997). A decision-theoretic
    generalization of on-line learning and an application to boosting. Journal of
    computer and system sciences, 55 119–139.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freund 和 Schapire (1997) Freund, Y. 和 Schapire, R. E. (1997). 在线学习的决策理论推广及其在提升中的应用。计算机与系统科学杂志，55
    119–139。
- en: Fu et al. (2023) Fu, Y., Peng, H., Khot, T. and Lapata, M. (2023). Improving
    language model negotiation with self-play and in-context learning from ai feedback.
    arXiv preprint arXiv:2305.10142.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2023) Fu, Y., Peng, H., Khot, T. 和 Lapata, M. (2023). 通过自我对弈和从 AI 反馈中学习提高语言模型谈判能力。arXiv
    预印本 arXiv:2305.10142。
- en: Fudenberg and Kreps (1993) Fudenberg, D. and Kreps, D. M. (1993). Learning mixed
    equilibria. Games and Economic Behavior, 5 320–367.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fudenberg 和 Kreps (1993) Fudenberg, D. 和 Kreps, D. M. (1993). 学习混合均衡。博弈与经济行为，5
    320–367。
- en: Fudenberg and Levine (1998) Fudenberg, D. and Levine, D. K. (1998). The theory
    of learning in games, vol. 2. MIT Press.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fudenberg 和 Levine (1998) Fudenberg, D. 和 Levine, D. K. (1998). 游戏中的学习理论，第 2
    卷。麻省理工学院出版社。
- en: Gao and Pavel (2017) Gao, B. and Pavel, L. (2017). On the properties of the
    softmax function with application in game theory and reinforcement learning. arXiv
    preprint arXiv:1704.00805.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 和 Pavel (2017) Gao, B. 和 Pavel, L. (2017). 软最大函数的性质及其在博弈论和强化学习中的应用。arXiv
    预印本 arXiv:1704.00805。
- en: Garg et al. (2022) Garg, S., Tsipras, D., Liang, P. S. and Valiant, G. (2022).
    What can transformers learn in-context? a case study of simple function classes.
    Advances in Neural Information Processing Systems, 35 30583–30598.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg 等 (2022) Garg, S., Tsipras, D., Liang, P. S. 和 Valiant, G. (2022). 转换器在上下文中能学到什么？简单函数类的案例研究。神经信息处理系统进展，35
    30583–30598。
- en: Giannou et al. (2023) Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D.
    and Papailiopoulos, D. (2023). Looped transformers as programmable computers.
    International Conference on Machine Learning.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giannou 等 (2023) Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D. 和
    Papailiopoulos, D. (2023). 循环转换器作为可编程计算机。国际机器学习会议。
- en: Hao et al. (2023) Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D. and
    Hu, Z. (2023). Reasoning with language model is planning with world model. In
    Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
    (H. Bouamor, J. Pino and K. Bali, eds.). Association for Computational Linguistics,
    Singapore.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等 (2023) Hao, S., Gu, Y., Ma, H., Hong, J., Wang, Z., Wang, D. 和 Hu, Z.
    (2023). 使用语言模型进行推理即为使用世界模型进行规划。发表于2023年自然语言处理实证方法会议（H. Bouamor, J. Pino 和 K. Bali
    编）。计算语言学协会，新加坡。
- en: '[https://aclanthology.org/2023.emnlp-main.507](https://aclanthology.org/2023.emnlp-main.507)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://aclanthology.org/2023.emnlp-main.507](https://aclanthology.org/2023.emnlp-main.507)'
- en: Hazan (2016) Hazan, E. (2016). Introduction to online convex optimization. Foundations
    and Trends® in Optimization, 2 157–325.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hazan (2016) Hazan, E. (2016). 在线凸优化简介。优化基础与趋势®，2 157–325。
- en: Hofbauer and Sandholm (2002) Hofbauer, J. and Sandholm, W. H. (2002). On the
    global convergence of stochastic fictitious play. Econometrica, 70 2265–2294.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofbauer 和 Sandholm (2002) Hofbauer, J. 和 Sandholm, W. H. (2002). 随机虚拟博弈的全局收敛性。计量经济学，70
    2265–2294。
- en: 'Hong et al. (2024) Hong, S., Zheng, X., Chen, J., Cheng, Y., Zhang, C., Wang,
    Z., Yau, S. K. S., Lin, Z., Zhou, L., Ran, C. et al. (2024). Metagpt: Meta programming
    for multi-agent collaborative framework. nternational Conference on Learning Representations.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等 (2024) Hong, S., Zheng, X., Chen, J., Cheng, Y., Zhang, C., Wang, Z.,
    Yau, S. K. S., Lin, Z., Zhou, L., Ran, C. 等 (2024). MetaGPT：用于多智能体协作框架的元编程。国际学习表征会议。
- en: 'Horton (2023) Horton, J. J. (2023). Large language models as simulated economic
    agents: What can we learn from homo silicus? Tech. rep., National Bureau of Economic
    Research.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horton (2023) Horton, J. J. (2023). 大语言模型作为模拟经济体：我们能从 Homo Silicus 中学到什么？技术报告，美国国家经济研究局。
- en: 'Huang et al. (2022a) Huang, W., Abbeel, P., Pathak, D. and Mordatch, I. (2022a).
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. In International Conference on Machine Learning. PMLR.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2022a) Huang, W., Abbeel, P., Pathak, D. 和 Mordatch, I. (2022a). 语言模型作为零-shot
    规划器：为具身智能体提取可操作的知识。在国际机器学习会议。PMLR。
- en: 'Huang et al. (2022b) Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,
    P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y. et al. (2022b). Inner monologue:
    Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2022b) Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,
    P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y. 等 (2022b). 内在独白：通过语言模型的规划进行具身推理。arXiv
    预印本 arXiv:2207.05608。
- en: Jiang (2023) Jiang, H. (2023). A latent space theory for emergent abilities
    in large language models. arXiv preprint arXiv:2304.09960.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang (2023) Jiang, H. (2023). 大语言模型中涌现能力的潜在空间理论。arXiv 预印本 arXiv:2304.09960。
- en: Kalai and Vempala (2005) Kalai, A. and Vempala, S. (2005). Efficient algorithms
    for online decision problems. Journal of Computer and System Sciences, 71 291–307.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalai 和 Vempala (2005) Kalai, A. 和 Vempala, S. (2005). 在线决策问题的高效算法。计算机与系统科学期刊，71
    291–307。
- en: Kasprzak et al. (2022) Kasprzak, M. J., Giordano, R. and Broderick, T. (2022).
    How good is your gaussian approximation of the posterior? finite-sample computable
    error bounds for a variety of useful divergences. arXiv preprint arXiv:2209.14992.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasprzak 等 (2022) Kasprzak, M. J., Giordano, R. 和 Broderick, T. (2022). 你的后验高斯近似有多好？各种有用散度的有限样本可计算误差界限。arXiv
    预印本 arXiv:2209.14992。
- en: Kirschner et al. (2023) Kirschner, J., Bakhtiari, A., Chandak, K., Tkachuk,
    V. and Szepesvari, C. (2023). Regret minimization via saddle point optimization.
    In Thirty-seventh Conference on Neural Information Processing Systems.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirschner 等 (2023) Kirschner, J., Bakhtiari, A., Chandak, K., Tkachuk, V. 和
    Szepesvari, C. (2023). 通过鞍点优化来最小化遗憾。在第37届神经信息处理系统会议上。
- en: Laskin et al. (2023) Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S.,
    Steigerwald, R., Strouse, D., Hansen, S., Filos, A., Brooks, E. et al. (2023).
    In-context reinforcement learning with algorithm distillation. International Conference
    on Learning Representations.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laskin 等 (2023) Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S., Steigerwald,
    R., Strouse, D., Hansen, S., Filos, A., Brooks, E. 等 (2023). 通过算法蒸馏的上下文强化学习。国际学习表示会议。
- en: Lattimore and Szepesvári (2020) Lattimore, T. and Szepesvári, C. (2020). Bandit
    algorithms. Cambridge University Press.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lattimore 和 Szepesvári (2020) Lattimore, T. 和 Szepesvári, C. (2020). 赌博算法。剑桥大学出版社。
- en: Lee et al. (2023) Lee, J. N., Xie, A., Pacchiano, A., Chandak, Y., Finn, C.,
    Nachum, O. and Brunskill, E. (2023). Supervised pretraining can learn in-context
    reinforcement learning. Neural Information Processing Systems.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2023) Lee, J. N., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum,
    O. 和 Brunskill, E. (2023). 监督预训练可以学习上下文强化学习。神经信息处理系统会议。
- en: Li et al. (2023a) Li, C., Su, X., Fan, C., Han, H., Xue, C. and Zheng, C. (2023a).
    Quantifying the impact of large language models on collective opinion dynamics.
    arXiv preprint arXiv:2308.03313.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023a) Li, C., Su, X., Fan, C., Han, H., Xue, C. 和 Zheng, C. (2023a).
    量化大语言模型对集体意见动态的影响。arXiv 预印本 arXiv:2308.03313。
- en: 'Li et al. (2023b) Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D. and
    Ghanem, B. (2023b). Camel: Communicative agents for” mind” exploration of large
    scale language model society. Neural Information Processing Systems.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023b) Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D. 和 Ghanem,
    B. (2023b). Camel: 大规模语言模型社会的“心智”探索的交流型智能体。神经信息处理系统会议。'
- en: 'Li et al. (2023c) Li, R., Patel, T. and Du, X. (2023c). Prd: Peer rank and
    discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023c) Li, R., Patel, T. 和 Du, X. (2023c). Prd: 同行排名和讨论改善基于大语言模型的评估。arXiv
    预印本 arXiv:2307.02762。'
- en: Li et al. (2023d) Li, S., Yang, J. and Zhao, K. (2023d). Are you in a masquerade?
    exploring the behavior and impact of large language model driven social bots in
    online social networks. arXiv preprint arXiv:2307.10337.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023d) Li, S., Yang, J. 和 Zhao, K. (2023d). 你在伪装吗？探索大语言模型驱动的社交机器人在在线社交网络中的行为和影响。arXiv
    预印本 arXiv:2307.10337。
- en: 'Li et al. (2023e) Li, Y., Ildiz, M. E., Papailiopoulos, D. and Oymak, S. (2023e).
    Transformers as algorithms: Generalization and stability in in-context learning.
    International Conference on Machine Learning.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023e）Li, Y., Ildiz, M. E., Papailiopoulos, D. 和 Oymak, S.（2023e）。《变压器作为算法：上下文学习中的泛化与稳定性》。国际机器学习会议。
- en: 'Li and Tewari (2017) Li, Z. and Tewari, A. (2017). Beyond the hazard rate:
    More perturbation algorithms for adversarial multi-armed bandits. J. Mach. Learn.
    Res., 18 183–1.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Tewari（2017）Li, Z. 和 Tewari, A.（2017）。《超越危险率：更多针对对抗性多臂强盗的扰动算法》。机器学习研究期刊,
    18 183–1。
- en: Liang et al. (2023) Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R.,
    Yang, Y., Tu, Z. and Shi, S. (2023). Encouraging divergent thinking in large language
    models through multi-agent debate. arXiv preprint arXiv:2305.19118.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2023）Liang, T., He, Z., Jiao, W., Wang, X., Wang, Y., Wang, R., Yang,
    Y., Tu, Z. 和 Shi, S.（2023）。《通过多智能体辩论鼓励大语言模型中的发散思维》。arXiv 预印本 arXiv:2305.19118。
- en: 'Lin et al. (2024) Lin, L., Bai, Y. and Mei, S. (2024). Transformers as decision
    makers: Provable in-context reinforcement learning via supervised pretraining.
    International Conference on Learning Representations.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2024）Lin, L., Bai, Y. 和 Mei, S.（2024）。《变压器作为决策者：通过监督预训练的可证明的上下文强化学习》。国际学习表征会议。
- en: Littlestone and Warmuth (1994) Littlestone, N. and Warmuth, M. K. (1994). The
    weighted majority algorithm. Information and computation, 108 212–261.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Littlestone 和 Warmuth（1994）Littlestone, N. 和 Warmuth, M. K.（1994）。《加权多数算法》。信息与计算,
    108 212–261。
- en: Liu et al. (2023a) Liu, H., Sferrazza, C. and Abbeel, P. (2023a). Chain of hindsight
    aligns language models with feedback. arXiv preprint arXiv:2302.02676, 3.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023a）Liu, H., Sferrazza, C. 和 Abbeel, P.（2023a）。《回顾链对齐语言模型与反馈》。arXiv
    预印本 arXiv:2302.02676, 3。
- en: Liu et al. (2023b) Liu, Y., Van Roy, B. and Xu, K. (2023b). Nonstationary bandit
    learning via predictive sampling. In International Conference on Artificial Intelligence
    and Statistics. PMLR.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023b）Liu, Y., Van Roy, B. 和 Xu, K.（2023b）。《通过预测采样进行非平稳强盗学习》。在国际人工智能与统计会议中。PMLR。
- en: 'Liu et al. (2023c) Liu, Z., Hu, H., Zhang, S., Guo, H., Ke, S., Liu, B. and
    Wang, Z. (2023c). Reason for future, act for now: A principled architecture for
    autonomous llm agents. In NeurIPS 2023 Foundation Models for Decision Making Workshop.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023c）Liu, Z., Hu, H., Zhang, S., Guo, H., Ke, S., Liu, B. 和 Wang, Z.（2023c）。《为未来而思考，为现在而行动：自主
    LLM 代理的原则性架构》。在 NeurIPS 2023 决策制定基础模型研讨会中。
- en: 'Lorè and Heydari (2023) Lorè, N. and Heydari, B. (2023). Strategic behavior
    of large language models: Game structure vs. contextual framing. arXiv preprint
    arXiv:2309.05898.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorè 和 Heydari（2023）Lorè, N. 和 Heydari, B.（2023）。《大语言模型的战略行为：博弈结构与上下文框架》。arXiv
    预印本 arXiv:2309.05898。
- en: Lu et al. (2023) Lu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H. T. and
    Gurevych, I. (2023). Are emergent abilities in large language models just in-context
    learning? arXiv preprint arXiv:2309.01809.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等（2023）Lu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H. T. 和 Gurevych,
    I.（2023）。《大语言模型中的突现能力是否只是上下文学习？》。arXiv 预印本 arXiv:2309.01809。
- en: Mahankali et al. (2023) Mahankali, A., Hashimoto, T. B. and Ma, T. (2023). One
    step of gradient descent is provably the optimal in-context learner with one layer
    of linear self-attention. International Conference on Learning Representations.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahankali 等（2023）Mahankali, A., Hashimoto, T. B. 和 Ma, T.（2023）。《一层线性自注意力的梯度下降一步在可证明的上下文学习者中是最优的》。国际学习表征会议。
- en: 'Mao et al. (2020) Mao, W., Zhang, K., Zhu, R., Simchi-Levi, D. and Başar, T.
    (2020). Model-free non-stationary RL: Near-optimal regret and applications in
    multi-agent RL and inventory control. arXiv preprint arXiv:2010.03161.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2020）Mao, W., Zhang, K., Zhu, R., Simchi-Levi, D. 和 Başar, T.（2020）。《无模型非平稳强化学习：接近最优的遗憾及其在多智能体强化学习和库存控制中的应用》。arXiv
    预印本 arXiv:2010.03161。
- en: 'McFadden (1976) McFadden, D. L. (1976). Quantal choice analaysis: A survey.
    Annals of Economic and Social Measurement, Volume 5, number 4 363–390.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McFadden（1976）McFadden, D. L.（1976）。《量子选择分析：综述》。经济与社会测量年鉴，第5卷，第4期 363–390。
- en: McKelvey and Palfrey (1995) McKelvey, R. D. and Palfrey, T. R. (1995). Quantal
    response equilibria for normal form games. Games and economic behavior, 10 6–38.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKelvey 和 Palfrey（1995）McKelvey, R. D. 和 Palfrey, T. R.（1995）。《正常形式博弈的量子响应均衡》。博弈与经济行为,
    10 6–38。
- en: 'Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi,
    H. and Zettlemoyer, L. (2022). Rethinking the role of demonstrations: What makes
    in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing (Y. Goldberg, Z. Kozareva and Y. Zhang, eds.).
    Association for Computational Linguistics, Abu Dhabi, United Arab Emirates.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2022) Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi,
    H. 和 Zettlemoyer, L. (2022). 重新思考示例的作用：是什么使得上下文学习有效？在 2022 年自然语言处理实证方法会议论文集中（Y.
    Goldberg, Z. Kozareva 和 Y. Zhang 主编）。计算语言学协会，阿布扎比，阿联酋。
- en: '[https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)'
- en: 'Mukobi et al. (2023) Mukobi, G., Erlebach, H., Lauffer, N., Hammond, L., Chan,
    A. and Clifton, J. (2023). Welfare diplomacy: Benchmarking language model cooperation.
    arXiv preprint arXiv:2310.08901.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukobi et al. (2023) Mukobi, G., Erlebach, H., Lauffer, N., Hammond, L., Chan,
    A. 和 Clifton, J. (2023). 福利外交：语言模型合作的基准测试。arXiv 预印本 arXiv:2310.08901。
- en: Nekipelov et al. (2015) Nekipelov, D., Syrgkanis, V. and Tardos, E. (2015).
    Econometrics for learning agents. In ACM Conference on Economics and Computation.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nekipelov et al. (2015) Nekipelov, D., Syrgkanis, V. 和 Tardos, E. (2015). 学习智能体的计量经济学。在
    ACM 经济与计算会议上。
- en: Openai (2023) Openai (2023). Gpt-4 technical report.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Openai (2023) Openai (2023). GPT-4 技术报告。
- en: Osband et al. (2013) Osband, I., Russo, D. and Van Roy, B. (2013). (more) efficient
    reinforcement learning via posterior sampling. Advances in Neural Information
    Processing Systems, 26.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osband et al. (2013) Osband, I., Russo, D. 和 Van Roy, B. (2013). 通过后验采样提高（更多的）强化学习效率。神经信息处理系统进展，26。
- en: 'Park et al. (2023) Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
    P. and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human
    behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software
    and Technology. UIST ’23, Association for Computing Machinery, New York, NY, USA.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2023) Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
    P. 和 Bernstein, M. S. (2023). 生成智能体：人类行为的交互式模拟体。在第 36 届 ACM 用户界面软件与技术年会论文集中。UIST
    ’23，计算机协会，纽约，NY，USA。
- en: '[https://doi.org/10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763)'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://doi.org/10.1145/3586183.3606763](https://doi.org/10.1145/3586183.3606763)'
- en: 'Park et al. (2022) Park, J. S., Popowski, L., Cai, C., Morris, M. R., Liang,
    P. and Bernstein, M. S. (2022). Social simulacra: Creating populated prototypes
    for social computing systems. In Proceedings of the 35th Annual ACM Symposium
    on User Interface Software and Technology.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2022) Park, J. S., Popowski, L., Cai, C., Morris, M. R., Liang,
    P. 和 Bernstein, M. S. (2022). 社会模拟体：为社会计算系统创建人口原型。在第 35 届 ACM 用户界面软件与技术年会论文集中。
- en: Qian et al. (2023) Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu,
    Z. and Sun, M. (2023). Communicative agents for software development. arXiv preprint
    arXiv:2307.07924.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. (2023) Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu,
    Z. 和 Sun, M. (2023). 用于软件开发的交互智能体。arXiv 预印本 arXiv:2307.07924。
- en: Reed et al. (2022) Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov,
    A., Barth-maron, G., Giménez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles,
    T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals,
    O., Bordbar, M. and de Freitas, N. (2022). A generalist agent. Transactions on
    Machine Learning Research. Featured Certification, Outstanding Certification.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reed et al. (2022) Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov,
    A., Barth-maron, G., Giménez, M., Sulsky, Y., Kay, J., Springenberg, J. T., Eccles,
    T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals,
    O., Bordbar, M. 和 de Freitas, N. (2022). 通用智能体。机器学习研究交易。特色认证，杰出认证。
- en: '[https://openreview.net/forum?id=1ikK0kHjvj](https://openreview.net/forum?id=1ikK0kHjvj)'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://openreview.net/forum?id=1ikK0kHjvj](https://openreview.net/forum?id=1ikK0kHjvj)'
- en: 'Robinson and Goforth (2005) Robinson, D. and Goforth, D. (2005). The topology
    of the 2x2 games: a new periodic table, vol. 3. Psychology Press.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robinson and Goforth (2005) Robinson, D. 和 Goforth, D. (2005). 2x2 博弈的拓扑：一种新的周期表，第
    3 卷。心理学出版社。
- en: Roughgarden (2015) Roughgarden, T. (2015). Intrinsic robustness of the price
    of anarchy. Journal of the ACM (JACM), 62 1–42.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roughgarden (2015) Roughgarden, T. (2015). 无序价格的内在鲁棒性。ACM 期刊（JACM），62 1–42。
- en: Roughgarden et al. (2017) Roughgarden, T., Syrgkanis, V. and Tardos, E. (2017).
    The price of anarchy in auctions. Journal of Artificial Intelligence Research,
    59 59–101.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roughgarden et al. (2017) Roughgarden, T., Syrgkanis, V. 和 Tardos, E. (2017).
    拍卖中的无序价格。人工智能研究期刊，59 59–101。
- en: 'Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis,
    P., Izacard, G., You, Q., Nalmpantis, C., Grave, E. and Riedel, S. (2023). Peer:
    A collaborative language model. International Conference on Learning Representations.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 等 (2023) Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis, P.,
    Izacard, G., You, Q., Nalmpantis, C., Grave, E. 和 Riedel, S. (2023)。Peer：一个协作语言模型。国际学习表征会议。
- en: 'Shalev-Shwartz (2007) Shalev-Shwartz, S. (2007). Online learning: Theory, algorithms,
    and applications. Hebrew University.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shalev-Shwartz (2007) Shalev-Shwartz, S. (2007)。在线学习：理论、算法与应用。希伯来大学。
- en: Shalev-Shwartz (2012) Shalev-Shwartz, S. (2012). Online learning and online
    convex optimization. Foundations and Trends® in Machine Learning, 4 107–194.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shalev-Shwartz (2012) Shalev-Shwartz, S. (2012)。在线学习和在线凸优化。机器学习基础与趋势®，4 107–194。
- en: Shalev-Shwartz and Singer (2007) Shalev-Shwartz, S. and Singer, Y. (2007). A
    primal-dual perspective of online learning algorithms. Machine Learning, 69 115–142.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shalev-Shwartz 和 Singer (2007) Shalev-Shwartz, S. 和 Singer, Y. (2007)。在线学习算法的原始-对偶视角。机器学习，69
    115–142。
- en: 'Shen et al. (2023) Shen, Y., Song, K., Tan, X., Li, D., Lu, W. and Zhuang,
    Y. (2023). Hugginggpt: Solving AI tasks with chatgpt and its friends in huggingface.
    Neural Information Processing Systems.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 (2023) Shen, Y., Song, K., Tan, X., Li, D., Lu, W. 和 Zhuang, Y. (2023)。Hugginggpt：利用
    chatgpt 和 huggingface 解决 AI 任务。神经信息处理系统会议。
- en: 'Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R.
    and Yao, S. (2023). Reflexion: Language agents with verbal reinforcement learning.
    In Thirty-seventh Conference on Neural Information Processing Systems.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等 (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R. 和 Yao,
    S. (2023)。Reflexion：具有语言强化学习的语言代理。第三十七届神经信息处理系统会议。
- en: Significant Gravitas (2023) Significant Gravitas (2023). Autogpt.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Significant Gravitas (2023) Significant Gravitas (2023)。Autogpt。
- en: '[https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)'
- en: 'Srivastava et al. (2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M.,
    Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A.
    et al. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities
    of language models. Transactions on Machine Learning Research.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等 (2023) Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
    A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A. 等 (2023)。超越模仿游戏：量化和推断语言模型的能力。机器学习研究汇刊。
- en: 'Swan et al. (2023) Swan, M., Kido, T., Roland, E. and Santos, R. P. d. (2023).
    Math agents: Computational infrastructure, mathematical embedding, and genomics.
    arXiv preprint arXiv:2307.02502.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swan 等 (2023) Swan, M., Kido, T., Roland, E. 和 Santos, R. P. d. (2023)。数学代理：计算基础设施、数学嵌入和基因组学。arXiv
    预印本 arXiv:2307.02502。
- en: Tsai et al. (2023) Tsai, C. F., Zhou, X., Liu, S. S., Li, J., Yu, M. and Mei,
    H. (2023). Can large language models play text games well? current state-of-the-art
    and open questions. arXiv preprint arXiv:2304.02868.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 等 (2023) Tsai, C. F., Zhou, X., Liu, S. S., Li, J., Yu, M. 和 Mei, H. (2023)。大型语言模型能否很好地玩文字游戏？当前的最先进技术和未解问题。arXiv
    预印本 arXiv:2304.02868。
- en: 'Valmeekam et al. (2023) Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S.
    and Kambhampati, S. (2023). Planbench: An extensible benchmark for evaluating
    large language models on planning and reasoning about change. In Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valmeekam 等 (2023) Valmeekam, K., Marquez, M., Olmo, A., Sreedharan, S. 和 Kambhampati,
    S. (2023)。Planbench：一个可扩展的基准，用于评估大型语言模型在计划和推理方面的能力。在第三十七届神经信息处理系统会议数据集和基准追踪。
- en: Van der Vaart (2000) Van der Vaart, A. W. (2000). Asymptotic statistics, vol. 3.
    Cambridge university press.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van der Vaart (2000) Van der Vaart, A. W. (2000)。渐近统计学，第3卷。剑桥大学出版社。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I. (2017). Attention is all you need.
    Advances in neural information processing systems, 30.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等 (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł. 和 Polosukhin, I. (2017)。注意力机制就是你所需要的一切。神经信息处理系统进展，30。
- en: Von Oswald et al. (2023) Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento,
    J., Mordvintsev, A., Zhmoginov, A. and Vladymyrov, M. (2023). Transformers learn
    in-context by gradient descent. In International Conference on Machine Learning.
    PMLR.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Von Oswald 等 (2023) Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento,
    J., Mordvintsev, A., Zhmoginov, A. 和 Vladymyrov, M. (2023)。变压器通过梯度下降进行上下文学习。在国际机器学习会议。PMLR。
- en: 'Wainwright (2019) Wainwright, M. J. (2019). High-dimensional statistics: A
    non-asymptotic viewpoint, vol. 48. Cambridge university press.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wainwright（2019）Wainwright, M. J.（2019）。高维统计：非渐近视角，第 48 卷。剑桥大学出版社。
- en: 'Wang et al. (2023a) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C.,
    Zhu, Y., Fan, L. and Anandkumar, A. (2023a). Voyager: An open-ended embodied agent
    with large language models. arXiv preprint arXiv:2305.16291.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023a）Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y.,
    Fan, L. 和 Anandkumar, A.（2023a）。Voyager：一个与大型语言模型的开放式具身代理。arXiv 预印本 arXiv:2305.16291。
- en: 'Wang et al. (2023b) Wang, X., Zhu, W. and Wang, W. Y. (2023b). Large language
    models are implicitly topic models: Explaining and finding good demonstrations
    for in-context learning. International Conference on Machine Learning 2023 Workshop
    ES-FoMO.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023b）Wang, X., Zhu, W. 和 Wang, W. Y.（2023b）。大型语言模型隐式地是主题模型：解释和找到上下文学习的良好示例。国际机器学习会议
    2023 工作坊 ES-FoMO。
- en: 'Wang et al. (2023c) Wang, Z., Cai, S., Liu, A., Ma, X. and Liang, Y. (2023c).
    Describe, explain, plan and select: Interactive planning with large language models
    enables open-world multi-task agents. Advances in neural information processing
    systems.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023c）Wang, Z., Cai, S., Liu, A., Ma, X. 和 Liang, Y.（2023c）。描述、解释、计划和选择：与大型语言模型的互动规划使开放世界多任务代理成为可能。神经信息处理系统的进展。
- en: 'Wei and Luo (2021) Wei, C.-Y. and Luo, H. (2021). Non-stationary reinforcement
    learning without prior knowledge: An optimal black-box approach. In Conference
    on learning theory. PMLR.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 和 Luo（2021）Wei, C.-Y. 和 Luo, H.（2021）。无先验知识的非平稳强化学习：一种最优黑箱方法。在学习理论会议。PMLR。
- en: Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M. and Le, Q. V. (2021). Finetuned language models are zero-shot
    learners. International Conference on Learning Representations.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2021）Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B.,
    Du, N., Dai, A. M. 和 Le, Q. V.（2021）。微调语言模型是零样本学习者。国际学习表征会议。
- en: Wei et al. (2022a) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D. et al. (2022a). Emergent abilities
    of large language models. Transactions on Machine Learning Research.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022a）Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D. 等（2022a）。大型语言模型的涌现能力。机器学习研究交易。
- en: Wei et al. (2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D. et al. (2022b). Chain-of-thought prompting elicits reasoning
    in large language models. Advances in Neural Information Processing Systems, 35
    24824–24837.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022b）Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E.,
    Le, Q. V., Zhou, D. 等（2022b）。链式思维提示在大型语言模型中引发推理。神经信息处理系统的进展，35 24824–24837。
- en: 'Wu et al. (2023) Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E.,
    Li, B., Jiang, L., Zhang, X. and Wang, C. (2023). Autogen: Enabling next-gen llm
    applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023）Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B.,
    Jiang, L., Zhang, X. 和 Wang, C.（2023）。Autogen：通过多代理对话框架实现下一代 LLM 应用。arXiv 预印本
    arXiv:2308.08155。
- en: Xie et al. (2022) Xie, S. M., Raghunathan, A., Liang, P. and Ma, T. (2022).
    An explanation of in-context learning as implicit bayesian inference. International
    Conference on Learning Representations.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2022）Xie, S. M., Raghunathan, A., Liang, P. 和 Ma, T.（2022）。将上下文学习解释为隐式贝叶斯推断。国际学习表征会议。
- en: 'Xiong et al. (2023) Xiong, K., Ding, X., Cao, Y., Liu, T. and Qin, B. (2023).
    Examining inter-consistency of large language models collaboration: An in-depth
    analysis via debate. In Findings of the Association for Computational Linguistics:
    EMNLP 2023 (H. Bouamor, J. Pino and K. Bali, eds.). Association for Computational
    Linguistics, Singapore.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等（2023）Xiong, K., Ding, X., Cao, Y., Liu, T. 和 Qin, B.（2023）。通过辩论深入分析大型语言模型协作的一致性。在《计算语言学协会的发现：EMNLP
    2023》（H. Bouamor, J. Pino 和 K. Bali 编辑）。计算语言学协会，新加坡。
- en: '[https://aclanthology.org/2023.findings-emnlp.508](https://aclanthology.org/2023.findings-emnlp.508)'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://aclanthology.org/2023.findings-emnlp.508](https://aclanthology.org/2023.findings-emnlp.508)'
- en: 'Xu et al. (2023a) Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W. and
    Liu, Y. (2023a). Exploring large language models for communication games: An empirical
    study on werewolf. arXiv preprint arXiv:2309.04658.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2023a）Xu, Y., Wang, S., Li, P., Luo, F., Wang, X., Liu, W. 和 Liu, Y.（2023a）。探索大型语言模型在沟通游戏中的应用：对狼人杀的实证研究。arXiv
    预印本 arXiv:2309.04658。
- en: Xu et al. (2023b) Xu, Z., Yu, C., Fang, F., Wang, Y. and Wu, Y. (2023b). Language
    agents with reinforcement learning for strategic play in the werewolf game. arXiv
    preprint arXiv:2310.18940.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等 (2023b) Xu, Z., Yu, C., Fang, F., Wang, Y. 和 Wu, Y. (2023b). 使用强化学习的语言代理在狼人游戏中的战略玩法。arXiv
    预印本 arXiv:2310.18940。
- en: 'Yao et al. (2023a) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L.,
    Cao, Y. and Narasimhan, K. (2023a). Tree of thoughts: Deliberate problem solving
    with large language models. Advances in Neural Information Processing Systems.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2023a) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao,
    Y. 和 Narasimhan, K. (2023a). 思维树：使用大型语言模型进行深思熟虑的问题解决。神经信息处理系统进展。
- en: 'Yao et al. (2023b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
    K. and Cao, Y. (2023b). React: Synergizing reasoning and acting in language models.
    International Conference on Learning Representations.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等 (2023b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.
    和 Cao, Y. (2023b). React：在语言模型中协同推理与行动。国际学习表征大会。
- en: Young (2004) Young, H. P. (2004). Strategic learning and its limits. OUP Oxford.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young (2004) Young, H. P. (2004). 战略学习及其局限性。OUP Oxford。
- en: Zhang et al. (2024) Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum,
    J. B., Shu, T. and Gan, C. (2024). Building cooperative embodied agents modularly
    with large language models. International Conference on Learning Representations.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2024) Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J. B.,
    Shu, T. 和 Gan, C. (2024). 使用大型语言模型模块化构建协作型具身代理。国际学习表征大会。
- en: Zhang et al. (2023a) Zhang, R., Frei, S. and Bartlett, P. L. (2023a). Trained
    transformers learn linear models in-context. arXiv preprint arXiv:2306.09927.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023a) Zhang, R., Frei, S. 和 Bartlett, P. L. (2023a). 训练的变换器在上下文中学习线性模型。arXiv
    预印本 arXiv:2306.09927。
- en: Zhang et al. (2023b) Zhang, Y., Zhang, F., Yang, Z. and Wang, Z. (2023b). What
    and how does in-context learning learn? bayesian model averaging, parameterization,
    and generalization. arXiv preprint arXiv:2305.19420.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023b) Zhang, Y., Zhang, F., Yang, Z. 和 Wang, Z. (2023b). 上下文学习到底学到了什么和如何学习？贝叶斯模型平均、参数化与泛化。arXiv
    预印本 arXiv:2305.19420。
- en: 'Zhao et al. (2023) Zhao, Q., Wang, J., Zhang, Y., Jin, Y., Zhu, K., Chen, H.
    and Xie, X. (2023). Competeai: Understanding the competition behaviors in large
    language model-based agents. arXiv preprint arXiv:2310.17512.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2023) Zhao, Q., Wang, J., Zhang, Y., Jin, Y., Zhu, K., Chen, H. 和 Xie,
    X. (2023). Competeai：理解基于大语言模型的代理的竞争行为。arXiv 预印本 arXiv:2310.17512。
- en: 'Zimmert and Seldin (2021) Zimmert, J. and Seldin, Y. (2021). Tsallis-inf: An
    optimal algorithm for stochastic and adversarial bandits. The Journal of Machine
    Learning Research, 22 1310–1358.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zimmert 和 Seldin (2021) Zimmert, J. 和 Seldin, Y. (2021). Tsallis-inf：一种用于随机和对抗性强盗的最优算法。机器学习研究杂志，22
    1310–1358。
- en: Zinkevich (2003) Zinkevich, M. (2003). Online convex programming and generalized
    infinitesimal gradient ascent. In International Conference on Machine Learning.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zinkevich (2003) Zinkevich, M. (2003). 在线凸编程和广义无穷小梯度上升。发表于国际机器学习大会。
- en: Supplementary Materials for
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 附录材料
- en: “Do LLM Agents Have Regret? A Case Study in Online Learning and Games”
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: “LLM代理是否有遗憾？在线学习与游戏中的案例研究”
- en: Contents
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1 "In Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 引言](#S1 "在《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[1.1 Related Work](#S1.SS1 "In 1 Introduction ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1.1 相关工作](#S1.SS1 "在1 引言 ‣ 《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[2 Preliminaries](#S2 "In Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 前言](#S2 "在《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[2.1 Online Learning & Games](#S2.SS1 "In 2 Preliminaries ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games")'
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.1 在线学习与游戏](#S2.SS1 "在2 前言 ‣ 《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[2.2 Performance Metric: Regret](#S2.SS2 "In 2 Preliminaries ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2.2 性能指标：遗憾](#S2.SS2 "在2 前言 ‣ 《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[3 Do Pre-Trained LLMs Have Regret? Experimental Validation](#S3 "In Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 预训练LLM是否有遗憾？实验验证](#S3 "在《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[3.1 Framework for No-Regret Behavior Validation](#S3.SS1 "In 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 无遗憾行为验证框架](#S3.SS1 "在3 预训练LLM是否有遗憾？实验验证 ‣ 《LLM代理是否有遗憾？在线学习与游戏中的案例研究》中")'
- en: '[3.2 Results: Online Learning](#S3.SS2 "In 3 Do Pre-Trained LLMs Have Regret?
    Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 结果：在线学习](#S3.SS2 "在 3 中预训练的 LLM 是否有悔恨？实验验证 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[3.3 Results: Multi-Player Repeated Games](#S3.SS3 "In 3 Do Pre-Trained LLMs
    Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games")'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.3 结果：多玩家重复游戏](#S3.SS3 "在 3 中预训练的 LLM 是否有悔恨？实验验证 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[3.4 Pre-Trained LLM Agents May Still Have Regret](#S3.SS4 "In 3 Do Pre-Trained
    LLMs Have Regret? Experimental Validation ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.4 预训练的 LLM 代理可能仍然有悔恨](#S3.SS4 "在 3 中预训练的 LLM 是否有悔恨？实验验证 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights](#S4 "In Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 为什么预训练的 LLM 有（或没有）悔恨？一个假设模型和一些理论见解](#S4 "在 LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[4.1 Pre-Trained LLMs Have Similar Regret as Humans (Who Generate Data)](#S4.SS1
    "In 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 预训练的 LLM 有与人类（生成数据者）类似的悔恨](#S4.SS1 "在 4 中为什么预训练的 LLM 有（或没有）悔恨？一个假设模型和一些理论见解
    ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[4.2 A Human Decision-Making Model: Quantal Response](#S4.SS2 "In 4 Why Are
    Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 人类决策模型：量化响应](#S4.SS2 "在 4 中为什么预训练的 LLM 有（或没有）悔恨？一个假设模型和一些理论见解 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[4.3 Case Study: Pre-Training under Canonical Data Distribution](#S4.SS3 "In
    4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 案例研究：在规范数据分布下的预训练](#S4.SS3 "在 4 中为什么预训练的 LLM 有（或没有）悔恨？一个假设模型和一些理论见解 ‣
    LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[5 Provably Promoting No-Regret Behavior by an Unsupervised Loss](#S5 "In Do
    LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 通过无监督损失证明促进无悔恨行为](#S5 "在 LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[5.1 A New Unsupervised Training Loss: Regret-Loss](#S5.SS1 "In 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 一种新的无监督训练损失：悔恨损失](#S5.SS1 "在 5 中通过无监督损失证明促进无悔恨行为 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[5.2 Guarantees via Regret-Loss Minimization](#S5.SS2 "In 5 Provably Promoting
    No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 通过悔恨损失最小化的保证](#S5.SS2 "在 5 中通过无监督损失证明促进无悔恨行为 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning
    Algorithms](#S5.SS3 "In 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 最小化悔恨损失可以自动生成已知的在线学习算法](#S5.SS3 "在 5 中通过无监督损失证明促进无悔恨行为 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[5.4 Experimental Results for Minimizing Regret-Loss](#S5.SS4 "In 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4 最小化悔恨损失的实验结果](#S5.SS4 "在 5 中通过无监督损失证明促进无悔恨行为 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[6 Concluding Remarks](#S6 "In Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 结论性评述](#S6 "在 LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[A Deferred Background](#A1 "In Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 延迟背景](#A1 "在 LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[A.1 Additional Definitions for Appendix](#A1.SS1 "In Appendix A Deferred Background
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.1 附录的额外定义](#A1.SS1 "在附录 A 延迟背景 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[A.2 In-Context Learning](#A1.SS2 "In Appendix A Deferred Background ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.2 上下文学习](#A1.SS2 "在附录 A 延迟背景 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")'
- en: '[A.3 Online Learning Algorithms](#A1.SS3 "In Appendix A Deferred Background
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.3 在线学习算法](#A1.SS3 "在附录 A 延迟的背景 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[A.4 Why Focusing on Linear Loss Function?](#A1.SS4 "In Appendix A Deferred
    Background ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.4 为什么关注线性损失函数？](#A1.SS4 "在附录 A 延迟的背景 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[A.5 Six Representative General-Sum Games](#A1.SS5 "In Appendix A Deferred
    Background ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A.5 六个代表性的非零和博弈](#A1.SS5 "在附录 A 延迟的背景 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[B Deferred Results and Proofs in Section 3](#A2 "In Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 延迟的结果和证明第 3 节](#A2 "在 LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[B.1 Ablation Study on Prompts](#A2.SS1 "In Appendix B Deferred Results and
    Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.1 对提示的消融研究](#A2.SS1 "在附录 B 延迟的结果和证明第 3 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[B.2 Proof for Proposition 1](#A2.SS2 "In Appendix B Deferred Results and Proofs
    in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")'
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.2 命题 1 的证明](#A2.SS2 "在附录 B 延迟的结果和证明第 3 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[B.3 Results of GPT-4 Turbo](#A2.SS3 "In Appendix B Deferred Results and Proofs
    in Section 3 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")'
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.3 GPT-4 Turbo 的结果](#A2.SS3 "在附录 B 延迟的结果和证明第 3 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[B.4 LLM Agents’ Explanation on Their Output Policies](#A2.SS4 "In Appendix
    B Deferred Results and Proofs in Section 3 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B.4 LLM 代理对其输出策略的解释](#A2.SS4 "在附录 B 延迟的结果和证明第 3 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C Deferred Results and Proofs in Section 4](#A3 "In Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 延迟的结果和证明第 4 节](#A3 "在 LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.1 Deferred Proof of Observation 1](#A3.SS1 "In Appendix C Deferred Results
    and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.1 观察 1 的延迟证明](#A3.SS1 "在附录 C 延迟的结果和证明第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.2 Deferred Proof of Lemma 1](#A3.SS2 "In Appendix C Deferred Results and
    Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.2 引理 1 的延迟证明](#A3.SS2 "在附录 C 延迟的结果和证明第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.3 Relationship between FTPL and Definition 2](#A3.SS3 "In Appendix C Deferred
    Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.3 FTPL 与定义 2 的关系](#A3.SS3 "在附录 C 延迟的结果和证明第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.4 Deferred Proof of Theorem 1](#A3.SS4 "In Appendix C Deferred Results and
    Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.4 定理 1 的延迟证明](#A3.SS4 "在附录 C 延迟的结果和证明第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.5 Extending Theorem 1 with Relaxed Assumptions](#A3.SS5 "In Appendix C Deferred
    Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.5 使用放松假设扩展定理 1](#A3.SS5 "在附录 C 延迟的结果和证明第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.5.1 Relaxation under More General Data Distributions](#A3.SS5.SSS1 "In C.5
    Extending Theorem 1 with Relaxed Assumptions ‣ Appendix C Deferred Results and
    Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.5.1 在更一般的数据分布下的放松](#A3.SS5.SSS1 "在 C.5 使用放松假设扩展定理 1 ‣ 附录 C 延迟的结果和证明第 4 节
    ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[C.5.2 Relaxation under Decision-Irrelevant Pre-Training Data](#A3.SS5.SSS2
    "In C.5 Extending Theorem 1 with Relaxed Assumptions ‣ Appendix C Deferred Results
    and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C.5.2 在与决策无关的预训练数据下的放松](#A3.SS5.SSS2 "在 C.5 使用放松假设扩展定理 1 ‣ 附录 C 延迟的结果和证明第
    4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[D Deferred Results and Proofs in Section 5](#A4 "In Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D 延迟的结果和证明第 5 节](#A4 "在 LLM 代理是否有遗憾？在线学习和游戏的案例研究")'
- en: '[D.1 Basic Lemmas](#A4.SS1 "In Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.1 基本引理](#A4.SS1 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.2 Deferred Proof for the Arguments in Section 5.1](#A4.SS2 "In Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.2 第5.1节论点的延迟证明](#A4.SS2 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.3 Deferred Proofs of Theorem 2 and Corollary 1](#A4.SS3 "In Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.3 定理2和推论1的延迟证明](#A4.SS3 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.4 Deferred Proof of Theorem 3](#A4.SS4 "In Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.4 定理3的延迟证明](#A4.SS4 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.5 Deferred Proof of Theorem 4](#A4.SS5 "In Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.5 定理4的延迟证明](#A4.SS5 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.6 Empirical Validation of Theorem 3 and Theorem 4](#A4.SS6 "In Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")'
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.6 定理3和定理4的实证验证](#A4.SS6 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.6.1 Empirical Validation of Theorem 3](#A4.SS6.SSS1 "In D.6 Empirical Validation
    of Theorem 3 and Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.6.1 定理3的实证验证](#A4.SS6.SSS1 "在D.6 定理3和定理4的实证验证 ‣ 附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.6.2 Empirical Validation of Theorem 4](#A4.SS6.SSS2 "In D.6 Empirical Validation
    of Theorem 3 and Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.6.2 定理4的实证验证](#A4.SS6.SSS2 "在D.6 定理3和定理4的实证验证 ‣ 附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.7 Discussions on the Production of FTRL with Entropy Regularization](#A4.SS7
    "In Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.7 FTRL生成与熵正则化的讨论](#A4.SS7 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.7.1 Numerical Analysis of Step 2 and Step 4](#A4.SS7.SSS1 "In D.7 Discussions
    on the Production of FTRL with Entropy Regularization ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.7.1 步骤2和步骤4的数值分析](#A4.SS7.SSS1 "在D.7 FTRL生成与熵正则化的讨论 ‣ 附录D 延迟结果和证明于第5节 ‣
    LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.7.2 Empirical Validation](#A4.SS7.SSS2 "In D.7 Discussions on the Production
    of FTRL with Entropy Regularization ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.7.2 实证验证](#A4.SS7.SSS2 "在D.7 FTRL生成与熵正则化的讨论 ‣ 附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.8 Training Details of Section 5.4](#A4.SS8 "In Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.8 第5.4节的训练细节](#A4.SS8 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: '[D.9 Ablation Study on Training Equation 3](#A4.SS9 "In Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")'
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D.9 关于训练方程3的消融研究](#A4.SS9 "在附录D 延迟结果和证明于第5节 ‣ LLM代理是否有遗憾？在线学习和游戏中的案例研究")'
- en: Appendix A Deferred Background
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 延迟背景
- en: A.1 Additional Definitions for Appendix
  id: totrans-474
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 附录的附加定义
- en: (Linear) Self-attention.
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (线性) 自注意力。
- en: One key component in Transformers (Vaswani et al., [2017](#bib.bib102)), the
    backbone of modern language models, is the *(self-)attention* mechanism. For simplicity,
    we here focus on introducing the *single-layer* self-attention architecture. The
    mechanism takes a sequence of vectors $Z=[z_{1},\dots,z_{t}]\in\mathbb{R}^{d\times
    t}$.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers（Vaswani 等，*[2017](#bib.bib102)*）中的一个关键组件是*(自注意力)*机制。为简单起见，这里我们专注于介绍*单层*自注意力结构。该机制接受一个向量序列
    $Z=[z_{1},\dots,z_{t}]\in\mathbb{R}^{d\times t}$。
- en: Transformers.
  id: totrans-477
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Transformers。
- en: For a multi-layer perceptron (MLP) layer, it takes $Z=[z_{1},\dots,z_{t}]\in\mathbb{R}^{d\times
    t}$ as
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多层感知机（MLP）层，它接受 $Z=[z_{1},\dots,z_{t}]\in\mathbb{R}^{d\times t}$ 作为
- en: '|  | $\displaystyle\texttt{TF}_{\theta}(Z):=Z^{(L)},$ |  |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\texttt{TF}_{\theta}(Z):=Z^{(L)},$ |  |'
- en: where the output $Z^{(L)}$ and
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 其中输出 $Z^{(L)}$ 和
- en: '|  | $1$2 |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: for some  to be sufficiently large such that clip does not
    take effect on any of our approximation results.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 和 $$B_{\texttt{TF}}> 应足够大，以便剪切对我们的任何近似结果不产生影响。
- en: A.2 In-Context Learning
  id: totrans-485
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 上下文学习
- en: In-context learning is an emergent behavior of LLMs (Brown et al., [2020](#bib.bib21)),
    which means that these models can adapt and learn from a limited number of examples
    provided within their immediate input context. In in-context learning, the prompt
    is usually constituted by a length of $T$.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习是 LLMs（Brown 等，*[2020](#bib.bib21)*）的一种新兴行为，这意味着这些模型可以从其立即输入上下文中提供的有限示例中适应和学习。在上下文学习中，提示通常由长度为
    $T$ 的内容构成。
- en: A.3 Online Learning Algorithms
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 在线学习算法
- en: Follow-the-regularized-leader (FTRL).
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跟随正则化领导者（FTRL）。
- en: The Follow-the-Regularized-Leader (FTRL) algorithm (Shalev-Shwartz, [2007](#bib.bib91))
    is an iterative method that updates policy based on the observed data and a regularization
    term. The idea is to choose the next policy that minimizes the sum of the past
    losses and a regularization term.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随正则化领导者（FTRL）算法（Shalev-Shwartz，*[2007](#bib.bib91)*）是一种迭代方法，它基于观察到的数据和正则化项更新策略。其思想是选择下一步策略以最小化过去损失的总和及正则化项。
- en: 'Mathematically, given a sequence of loss vectors ${\ell}_{1},{\ell}_{2},\dots,{\ell}_{t}$
    as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，给定一系列损失向量 ${\ell}_{1},{\ell}_{2},\dots,{\ell}_{t}$ 如下：
- en: '|  | $\pi_{t+1}=\arg\min_{\pi\in\Pi}\left(\sum_{i=1}^{t}\langle{\ell}_{i},\pi\rangle+R(\pi)\right),$
    |  |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{t+1}=\arg\min_{\pi\in\Pi}\left(\sum_{i=1}^{t}\langle{\ell}_{i},\pi\rangle+R(\pi)\right),$
    |  |'
- en: 'where $R(\pi)$:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $R(\pi)$：
- en: '|  | $1$2 |  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: A key property that ensures the convergence and stability of the FTRL algorithm
    is the strong convexity of the regularization term $R(\pi)$ ensures that the optimization
    problem in FTRL has a unique solution. The FTRL algorithm’s flexibility allows
    it to encompass a wide range of online learning algorithms, from gradient-based
    methods like online gradient descent to decision-making algorithms like Hedge
    (Freund and Schapire, [1997](#bib.bib39)).
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 确保 FTRL 算法收敛和稳定性的一个关键属性是正则化项 $R(\pi)$ 的强凸性，这确保了 FTRL 中的优化问题具有唯一解。FTRL 算法的灵活性使其能够涵盖广泛的在线学习算法，从基于梯度的方法如在线梯度下降到决策算法如
    Hedge（Freund 和 Schapire，*[1997](#bib.bib39)*）。
- en: Connection to online gradient descent (OGD).
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与在线梯度下降（OGD）的关联。
- en: 'The Online Gradient Descent (OGD) (Cesa-Bianchi et al., [1996](#bib.bib25))
    algorithm is a special case of the FTRL algorithm when the regularization term
    is the $L_{2}$ is updated using the gradient of the loss function:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在线梯度下降（OGD）（Cesa-Bianchi 等，*[1996](#bib.bib25)*）算法是 FTRL 算法的一种特例，当正则化项为 $L_{2}$
    时，使用损失函数的梯度进行更新：
- en: '|  | $\pi_{t+1}=\pi_{t}-{\ell}_{t}.$ |  |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi_{t+1}=\pi_{t}-{\ell}_{t}.$ |  |'
- en: Therefore, the connection between FTRL and OGD can be seen by observing that
    the update rule for FTRL with $L_{2}$ regularization can be derived from the OGD
    update rule.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过观察 FTRL 在 $L_{2}$ 正则化下的更新规则，可以看出 FTRL 和 OGD 之间的联系。
- en: Connection to the Hedge algorithm.
  id: totrans-499
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与 Hedge 算法的关联。
- en: The Hedge algorithm (Freund and Schapire, [1997](#bib.bib39)) (also referred
    to as the Multiplicative Weight Update algorithm (Arora et al., [2012](#bib.bib11)))
    is an online learning algorithm designed for problems where the learner has to
    choose from a set of actions (denoted as $\mathcal{A}$), then the FTRL update
    rule yields the Hedge algorithm as
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: Hedge 算法（Freund 和 Schapire，[1997](#bib.bib39)）（也称为乘法权重更新算法（Arora 等，[2012](#bib.bib11)））是一种针对学习者需要从一组动作中选择的在线学习算法（记作
    $\mathcal{A}$），然后 FTRL 更新规则产生 Hedge 算法如下：
- en: '|  | $1$2 |  |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: for $j\in[d]$.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $j\in[d]$。
- en: Follow-the-perturbed-leader (FTPL).
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 跟随扰动领导者 (FTPL)。
- en: 'Given a sequence of loss vectors ${\ell}_{1},{\ell}_{2},\dots,{\ell}_{t-1}$
    for the next time step is chosen by solving the following optimization problem:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一系列损失向量 ${\ell}_{1},{\ell}_{2},\dots,{\ell}_{t-1}$，下一个时间步的选择是通过解决以下优化问题来确定的：
- en: '|  | $\displaystyle\pi_{t}=\mathbb{E}\left[\arg\min_{\pi\in\Pi}\langle\epsilon_{t},\pi\rangle+\sum_{i=1}^{t-1}\langle{\ell}_{i},\pi\rangle\right].$
    |  | (10) |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\pi_{t}=\mathbb{E}\left[\arg\min_{\pi\in\Pi}\langle\epsilon_{t},\pi\rangle+\sum_{i=1}^{t-1}\langle{\ell}_{i},\pi\rangle\right].$
    |  | (10) |'
- en: Here $\epsilon_{t}$ introduces randomness to the decision-making.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\epsilon_{t}$ 引入了决策的随机性。
- en: Relationship between FTRL and FTPL.
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: FTRL 和 FTPL 之间的关系。
- en: The FTRL and FTPL algorithms are deeply related. For example, FTPL with perturbations
    of Gumbel distribution and FTRL with Entropy Regularization (i.e., Hedge) are
    equivalent. In general, for the FTPL algorithm with any perturbation distribution,
    one can always find an FTRL algorithm with a particular regularization such that
    their update rule is equivalent. However, this relationship does not hold vice
    versa. For example, Hofbauer and Sandholm ([2002](#bib.bib48)) shows that for
    FTRL with log barrier regularization, there does not exist an equivalent perturbation
    distribution for FTPL.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: FTRL 和 FTPL 算法有着深刻的关系。例如，具有 Gumbel 分布扰动的 FTPL 和具有熵正则化（即 Hedge）的 FTRL 是等价的。一般来说，对于任何扰动分布的
    FTPL 算法，总能找到一个具有特定正则化的 FTRL 算法，使得它们的更新规则是等价的。然而，这种关系并非逆向成立。例如，Hofbauer 和 Sandholm（[2002](#bib.bib48)）表明，对于具有对数障碍正则化的
    FTRL，没有一个等效的扰动分布用于 FTPL。
- en: Restarting techniques for non-stationary online learning.
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非平稳在线学习的重启技术。
- en: 'For non-stationary online learning problems, one common technique is *restarting*:
    one restarts the standard online learning algorithm periodically (Besbes et al.,
    [2014](#bib.bib17)) (see also e.g., Wei and Luo ([2021](#bib.bib108)); Mao et al.
    ([2020](#bib.bib75))). After each restarting operation, the algorithm will ignore
    the previous history and execute as if it is the beginning of the interaction
    with the environment. Since the variation of the loss sequences is bounded, loss
    sequences between two consecutive restarting operations can be regarded as being
    *almost stationary*, which makes achieving an overall sublinear dynamic regret
    guarantee possible.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非平稳的在线学习问题，一种常见的技术是*重启*：定期重新启动标准在线学习算法（Besbes 等，[2014](#bib.bib17)）（另见如 Wei
    和 Luo（[2021](#bib.bib108)）；Mao 等（[2020](#bib.bib75)））。每次重启操作后，算法将忽略之前的历史，仿佛是与环境互动的开始。由于损失序列的变化是有界的，两个连续重启操作之间的损失序列可以被视为*几乎平稳*，这使得实现整体次线性动态遗憾保证成为可能。
- en: A.4 Why Focusing on Linear Loss Function?
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 为什么关注线性损失函数？
- en: We note that focusing on the linear loss function $f_{t}(\pi):=\langle{\ell}_{t},\pi\rangle$,
    which indicates
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，关注线性损失函数 $f_{t}(\pi):=\langle{\ell}_{t},\pi\rangle$，这表明
- en: '|  | $1$2 |  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Therefore, one can regard the loss vector $({\ell}_{t})_{t\in[T]}$ corresponds
    to the gradient of the convex function evaluated at the policy at that round.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以认为损失向量 $({\ell}_{t})_{t\in[T]}$ 对应于在该回合政策下评估的凸函数的梯度。
- en: A.5 Six Representative General-Sum Games
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 六个代表性的一般和游戏
- en: In game theory, there are six representative two-player general-sum games (Robinson
    and Goforth, [2005](#bib.bib87)). Firstly, consider the win-win game represented
    by matrices  |  |'
  id: totrans-733
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle\min_{u> |  |'
- en: where the optimal solution is $u^{\star}=\left(\frac{C^{2}T^{2}\log d}{4V_{T}^{2}}\right)^{1/3}$,
    which results in a regret bound of
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 最优解为 $u^{\star}=\left(\frac{C^{2}T^{2}\log d}{4V_{T}^{2}}\right)^{1/3}$，这导致了一个遗憾界限为
- en: '|  | $\displaystyle\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})\leq\frac{2T}{\sqrt{u^{\star}}}C\sqrt{\log
    d}+4u^{\star}V_{T}=\mathcal{O}\left((\log d\ V_{T})^{1/3}T^{2/3}\right).$ |  |'
  id: totrans-735
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})\leq\frac{2T}{\sqrt{u^{\star}}}C\sqrt{\log
    d}+4u^{\star}V_{T}=\mathcal{O}\left((\log d\ V_{T})^{1/3}T^{2/3}\right).$ |  |'
- en: Now we check the conditions for $u^{\star}\in[1,T]$ is large enough.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查 $u^{\star}\in[1,T]$ 的条件是否足够大。
- en: (3) Combining the above result with [Lemma 3](#Thmlemma3 "Lemma 3 (Regret guarantee
    of FTPL with bandit feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), we can prove a regret guarantee for online
    learning with bandit feedback.
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 结合上述结果与[引理 3](#Thmlemma3 "引理 3（FTPL的带反馈的遗憾保证）。 ‣ 证明。 ‣ C.4 论文 1 的推迟证明 ‣
    附录 C 推迟结果和证明在第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")，我们可以证明带反馈的在线学习的遗憾保证。
- en: (4) Combining this result with [Lemma 3](#Thmlemma3 "Lemma 3 (Regret guarantee
    of FTPL with bandit feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") and [Lemma 4](#Thmlemma4 "Lemma 4\. ‣ Proof.
    ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs in
    Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"),
    it holds that
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 将这一结果与[引理 3](#Thmlemma3 "引理 3（FTPL的带反馈的遗憾保证）。 ‣ 证明。 ‣ C.4 论文 1 的推迟证明 ‣ 附录
    C 推迟结果和证明在第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")和[引理 4](#Thmlemma4 "引理 4。 ‣ 证明。 ‣
    C.4 论文 1 的推迟证明 ‣ 附录 C 推迟结果和证明在第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏的案例研究")结合，我们得到
- en: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})]\leq\min_{\Delta_{T}\in[T]}\frac{2T}{\Delta_{T}}C(\log
    d)^{\frac{1}{2}}d\Delta_{T}^{\frac{1}{2}+\frac{1}{\log T}}\log\Delta_{T}+2\Delta_{T}V_{T},$
    |  |'
  id: totrans-739
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})]\leq\min_{\Delta_{T}\in[T]}\frac{2T}{\Delta_{T}}C(\log
    d)^{\frac{1}{2}}d\Delta_{T}^{\frac{1}{2}+\frac{1}{\log T}}\log\Delta_{T}+2\Delta_{T}V_{T},$
    |  |'
- en: 'for some constant $C$ and derive the following regret:'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 对某个常数 $C$，我们推导出以下遗憾：
- en: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})]\leq\mathcal{O}\left((T^{2}d^{2}V_{T})^{1/3}(\log
    d)^{1/2}T^{1/\log T}\log T\right).$ |  |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\text{LLM}{{}_{\theta^{\star}}}}(({\ell}_{i})_{i\in[T]})]\leq\mathcal{O}\left((T^{2}d^{2}V_{T})^{1/3}(\log
    d)^{1/2}T^{1/\log T}\log T\right).$ |  |'
- en: Now we check the condition of $u^{\star}\in[1,T]$ is large enough.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查 $u^{\star}\in[1,T]$ 的条件是否足够大。
- en: Now, we present [Lemma 2](#Thmlemma2 "Lemma 2 (Regret guarantee of FTPL with
    full-information feedback). ‣ Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") - [Lemma 4](#Thmlemma4 "Lemma 4\. ‣ Proof.
    ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs in
    Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    Before proceeding, we assume $\|\ell_{t}\|_{\infty}\leq B=1$.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们展示 [引理 2](#Thmlemma2 "引理 2（FTPL 完全信息反馈的遗憾保证）。 ‣ 证明。 ‣ C.4 定理 1 的延迟证明 ‣
    附录 C 延迟结果和证明在第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究") - [引理 4](#Thmlemma4 "引理 4。 ‣
    证明。 ‣ C.4 定理 1 的延迟证明 ‣ 附录 C 延迟结果和证明在第 4 节 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")。在继续之前，我们假设
    $\|\ell_{t}\|_{\infty}\leq B=1$。
- en: Lemma 2  (Regret guarantee of FTPL with full-information feedback).
  id: totrans-744
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2（FTPL 完全信息反馈的遗憾保证）。
- en: Suppose the noise distribution of FTPL satisfies that $\epsilon_{t}\sim\mathcal{N}(\boldsymbol{0}_{d},\zeta_{t}^{2}I)$,
    then for online learning with full-information feedback,
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 FTPL 的噪声分布满足 $\epsilon_{t}\sim\mathcal{N}(\boldsymbol{0}_{d},\zeta_{t}^{2}I)$，那么对于完全信息反馈的在线学习，
- en: '|  | $\displaystyle\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})\leq
    4\left(\sigma+\frac{1}{\sigma}\right)\sqrt{T\log d}=\mathcal{O}(\sqrt{T\log d}).$
    |  |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})\leq
    4\left(\sigma+\frac{1}{\sigma}\right)\sqrt{T\log d}=\mathcal{O}(\sqrt{T\log d}).$
    |  |'
- en: Proof.
  id: totrans-747
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: By Theorem 8 of Abernethy et al. ([2014](#bib.bib1)), we have
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Abernethy 等人的定理 8 ([2014](#bib.bib1))，我们有
- en: '|  | $1$2 |  |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Therefore, plugging $\zeta_{t}=\sigma\sqrt{t}$ provides
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，代入 $\zeta_{t}=\sigma\sqrt{t}$ 得到
- en: '|  | $1$2 |  |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: completing the proof. ∎
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 完成证明。∎
- en: Lemma 3  (Regret guarantee of FTPL with bandit feedback).
  id: totrans-753
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3（FTPL 带赌博反馈的遗憾保证）。
- en: Suppose the noise distribution of FTPL satisfies that $\epsilon_{t}\sim\mathcal{N}(\boldsymbol{0}_{d},\zeta_{t}^{2}I)$,
    then for online learning with bandit feedback,
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 FTPL 的噪声分布满足 $\epsilon_{t}\sim\mathcal{N}(\boldsymbol{0}_{d},\zeta_{t}^{2}I)$，那么对于带有赌博反馈的在线学习，
- en: '|  | $1$2 |  |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-756
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: The proof of the bandit problem is more complex. We first define the following
    notation. We denote $G_{t}=\sum_{t^{\prime}=1}^{t}-\ell_{t^{\prime}}$,
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 赌博问题的证明更为复杂。我们首先定义以下符号。我们定义 $G_{t}=\sum_{t^{\prime}=1}^{t}-\ell_{t^{\prime}}$，
- en: '|  | $\displaystyle\Phi(G_{T})=\Phi(\mathbb{E}[\widehat{G}_{T}])\leq\mathbb{E}[\Phi(\widehat{G}_{T})].$
    |  |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Phi(G_{T})=\Phi(\mathbb{E}[\widehat{G}_{T}])\leq\mathbb{E}[\Phi(\widehat{G}_{T})].$
    |  |'
- en: Therefore,
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $\displaystyle\mathbb{E}[\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})]$
    |  |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})]$
    |  |'
- en: By recalling the definition of the Bregman divergence, we have
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回顾 Bregman 发散的定义，我们有
- en: '|  | $\displaystyle-\sum_{t=1}^{T}\langle\pi_{t},-\widehat{\ell}_{t}\rangle$
    |  |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle-\sum_{t=1}^{T}\langle\pi_{t},-\widehat{\ell}_{t}\rangle$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Therefore,
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $\displaystyle\mathbb{E}\left[\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})\right]$
    |  |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[\text{Regret}_{\text{FTPL}}(({\ell}_{i})_{i\in[T]})\right]$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: $(iii)\leq 0$, we use Lemma 10 of Abernethy et al. ([2014](#bib.bib1)) to obtain
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: $(iii)\leq 0$，我们使用 Abernethy 等人的引理 10 ([2014](#bib.bib1)) 得到
- en: '|  | $1$2 |  |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'For $(i)$, the following holds:'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $(i)$，如下所示：
- en: '|  | $1$2 |  |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: By tuning $\alpha=\frac{2}{\log T}$. ∎
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整 $\alpha=\frac{2}{\log T}$。∎
- en: Lemma 4.
  id: totrans-772
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 4。
- en: Denote the variation of loss vectors as $L_{T}=\sum_{t=1}^{T-1}\|\ell_{t+1}-\ell_{t}\|_{\infty}$
    that can achieve
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 将损失向量的变化表示为 $L_{T}=\sum_{t=1}^{T-1}\|\ell_{t+1}-\ell_{t}\|_{\infty}$，可以得到
- en: '|  | $1$2 |  |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Similarly, suppose there exists an algorithm $\mathscr{B}$ that can achieve
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，假设存在一个算法 $\mathscr{B}$ 可以实现
- en: '|  | $1$2 |  |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-777
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: We denote $\mathscr{A}^{\prime}$
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义 $\mathscr{A}^{\prime}$
- en: '|  | $\displaystyle\min_{j\in[d]}\left(\sum_{t\in{\mathcal{T}}_{k}}\ell_{t}\right)_{j}-\sum_{t\in{\mathcal{T}}_{k}}\ell^{\star}_{t}\leq
    2\Delta_{T}L_{k},$ |  |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{j\in[d]}\left(\sum_{t\in{\mathcal{T}}_{k}}\ell_{t}\right)_{j}-\sum_{t\in{\mathcal{T}}_{k}}\ell^{\star}_{t}\leq
    2\Delta_{T}L_{k},$ |  |'
- en: where we define $L_{k}=\sum_{t\in{\mathcal{T}}_{k}}\|\ell_{t+1}-\ell_{t}\|_{\infty}$.
    Therefore, we have
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 $L_{k}=\sum_{t\in{\mathcal{T}}_{k}}\|\ell_{t+1}-\ell_{t}\|_{\infty}$。因此，我们有
- en: '|  | $\displaystyle\text{D-Regret}_{\mathscr{A}^{\prime}}(({\ell}_{i})_{i\in[T]})$
    |  | (13) |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{D-Regret}_{\mathscr{A}^{\prime}}(({\ell}_{i})_{i\in[T]})$
    |  | (13) |'
- en: '|  |  | $\displaystyle\leq 2\Delta_{T}(\sum_{k\in[m]}L_{k})+(T/\Delta_{T}+1)g(\Delta_{T},d).$
    |  |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq 2\Delta_{T}(\sum_{k\in[m]}L_{k})+(T/\Delta_{T}+1)g(\Delta_{T},d).$
    |  |'
- en: By Equation (4) of Besbes et al. ([2014](#bib.bib17)) that $\sum_{k\in[m]}L_{k}\leq
    L_{T}$.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Besbes 等人（[2014](#bib.bib17)）的方程 (4)，$\sum_{k\in[m]}L_{k}\leq L_{T}$。
- en: Similarly, if we take the expectation for [Equation 13](#A3.E13 "In Proof. ‣
    Proof. ‣ C.4 Deferred Proof of Theorem 1 ‣ Appendix C Deferred Results and Proofs
    in Section 4 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"), it holds that
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果我们对[方程 13](#A3.E13 "在证明中 ‣ 证明 ‣ C.4 定理 1 的推迟证明 ‣ 附录 C 推迟的结果和证明 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")取期望，那么
- en: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\mathscr{B}^{\prime}}(({\ell}_{i})_{i\in[T]})]$
    |  |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[\text{D-Regret}_{\mathscr{B}^{\prime}}(({\ell}_{i})_{i\in[T]})]$
    |  |'
- en: '|  |  | $\displaystyle\leq\min_{\Delta_{T}\in[T]}\left(\frac{T}{\Delta_{T}}+1\right)g(\Delta_{T},d)+2\Delta_{T}L_{T},$
    |  |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\min_{\Delta_{T}\in[T]}\left(\frac{T}{\Delta_{T}}+1\right)g(\Delta_{T},d)+2\Delta_{T}L_{T},$
    |  |'
- en: thus completing the proof. ∎
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 从而完成证明。 ∎
- en: 'Combining the results above completes the proof for [Theorem 1](#Thmtheorem1
    "Theorem 1\. ‣ 4.3 Case Study: Pre-Training under Canonical Data Distribution
    ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical Model and Some Theoretical
    Insights ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    ∎'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 综合上述结果，完成了对[定理 1](#Thmtheorem1 "定理 1\. ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练的 LLM
    没有悔恨？一个假设模型和一些理论见解 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")的证明。 ∎
- en: 'C.5 Extending [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") with Relaxed Assumptions'
  id: totrans-789
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5 扩展[定理 1](#Thmtheorem1 "定理 1\. ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练的 LLM 没有悔恨？一个假设模型和一些理论见解
    ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")，以放宽的假设
- en: C.5.1 Relaxation under More General Data Distributions
  id: totrans-790
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.5.1 在更一般的数据分布下的放宽
- en: We first remark on the possibility of relaxing the Gaussian assumptions on the
    data distributions.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论放宽对数据分布的高斯假设的可能性。
- en: Remark 3  (Relaxing the Gaussian distribution assumption).
  id: totrans-792
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释 3（放宽高斯分布假设）。
- en: 'In the proof of [Lemma 1](#Thmlemma1 "Lemma 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), to obtain the result that the action
    is a quantal response w.r.t. $\ell_{1:T}$, as long as its posterior distribution
    satisfies [Equation 11](#A3.E11 "In Proof. ‣ C.2 Deferred Proof of Lemma 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"), it would suffice. It is a combined effect
    of both the prior and the conditional distributions.'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 在[引理 1](#Thmlemma1 "引理 1\. ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练的 LLM 没有悔恨？一个假设模型和一些理论见解
    ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")的证明中，为了得到动作是相对于 $\ell_{1:T}$ 的量化响应，只要其后验分布满足[方程 11](#A3.E11
    "在证明中 ‣ C.2 推迟的引理 1 证明 ‣ 附录 C 推迟的结果和证明 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")即可。这是先验分布和条件分布的共同作用。
- en: 'More formally, we can extend [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case
    Study: Pre-Training under Canonical Data Distribution ‣ 4 Why Are Pre-Trained
    LLMs (No-)Regret? A Hypothetical Model and Some Theoretical Insights ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games") to the case with
    a much more general prior task distribution than the Gaussian one, where the key
    is that [Equation 11](#A3.E11 "In Proof. ‣ C.2 Deferred Proof of Lemma 1 ‣ Appendix
    C Deferred Results and Proofs in Section 4 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") only needs to hold approximately.'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，我们可以将[定理 1](#Thmtheorem1 "定理 1\. ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练的 LLM
    没有悔恨？一个假设模型和一些理论见解 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")扩展到具有比高斯分布更一般的先验任务分布的情况，其中关键是[方程
    11](#A3.E11 "在证明中 ‣ C.2 推迟的引理 1 证明 ‣ 附录 C 推迟的结果和证明 ‣ LLM 代理是否有悔恨？在线学习和游戏中的案例研究")只需大致成立即可。
- en: Theorem 5.
  id: totrans-795
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 5。
- en: 'In [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training under
    Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A Hypothetical
    Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret? A Case Study
    in Online Learning and Games"), we can relax the assumption on $\mathbb{P}(z)$.'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 在[定理 1](#Thmtheorem1 "定理 1\. ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练的 LLM 没有（或有）遗憾？一个假设模型和一些理论见解
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")中，我们可以放宽对$\mathbb{P}(z)$的假设。
- en: The key idea of the proof is that when $t$ is large enough, the prior distribution
    does not affect the posterior distribution, which is also referred to as the *Bernstein–von
    Mises theorem* (Van der Vaart, [2000](#bib.bib101)).
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 证明的关键思想是，当$t$足够大时，先验分布不会影响后验分布，这也称为*伯恩斯坦–冯·米塞斯定理*（Van der Vaart，[2000](#bib.bib101)）。
- en: Proof.
  id: totrans-798
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Since we extend [Theorem 1](#Thmtheorem1 "Theorem 1\. ‣ 4.3 Case Study: Pre-Training
    under Canonical Data Distribution ‣ 4 Why Are Pre-Trained LLMs (No-)Regret? A
    Hypothetical Model and Some Theoretical Insights ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") to settings with general task prior
    distribution only requiring the coordinates to be i.i.d, from now on, we consider
    the $j$, we define the log-likelihood of the posterior as'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将[定理 1](#Thmtheorem1 "定理 1\. ‣ 4.3 案例研究：在标准数据分布下的预训练 ‣ 4 为什么预训练的 LLM 没有（或有）遗憾？一个假设模型和一些理论见解
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")扩展到具有一般任务先验分布的设置，只要求坐标为i.i.d，因此从现在开始，我们考虑$j$，定义后验的对数似然为
- en: '|  | $1$2 |  |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then, the MLE estimator $\widehat{z}_{j,t}$ is defined as
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，MLE 估计量 $\widehat{z}_{j,t}$ 定义为
- en: '|  | $\displaystyle\widehat{z}_{j,t}:=\arg\max_{z_{j}\in\mathbb{R}}L_{t}(z_{j})=\frac{1}{t}\sum_{i=1}^{t}\ell_{ij}.$
    |  |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{z}_{j,t}:=\arg\max_{z_{j}\in\mathbb{R}}L_{t}(z_{j})=\frac{1}{t}\sum_{i=1}^{t}\ell_{ij}.$
    |  |'
- en: 'We also define $\widehat{J}_{t}:\mathbb{R}\to\mathbb{R}$ as:'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义 $\widehat{J}_{t}:\mathbb{R}\to\mathbb{R}$ 为：
- en: '|  | $\displaystyle\widehat{J}_{t}(z_{j}):=-\frac{\nabla^{2}L_{t}(z_{j})}{t}=\frac{1}{\sigma^{2}}.$
    |  |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{J}_{t}(z_{j}):=-\frac{\nabla^{2}L_{t}(z_{j})}{t}=\frac{1}{\sigma^{2}}.$
    |  |'
- en: For Assumption 1 of Kasprzak et al. ([2022](#bib.bib55)) to hold, any 
    |  |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle\frac{&#124;\{i\in[N]\mid(H(X_{i})
    |  |'
- en: as $N\to\infty$. Therefore, we have
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $N\to\infty$ 时。因此，我们有
- en: '|  | 
    |  |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\displaystyle\frac{\sum_{i=1}^{N}f(X_{i},k)\mathbbm{1}(H(X_{i})}
    |  |'
- en: ∎
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Note that $\lim_{k\to\infty}A(k,H,\epsilon)=0$, we have
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到$\lim_{k\to\infty}A(k,H,\epsilon)=0$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-919
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-920
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where we recall the shorthand notation of $R_{\text{LLM}_{\theta}}=\text{Regret}_{\text{LLM}_{\theta}}$
    in [Lemma 9](#Thmlemma9 "Lemma 9\. ‣ Proof. ‣ D.2 Deferred Proof for the Arguments
    in Section 5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM
    Agents Have Regret? A Case Study in Online Learning and Games"). Therefore,
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们回顾了$R_{\text{LLM}_{\theta}}=\text{Regret}_{\text{LLM}_{\theta}}$在[引理 9](#Thmlemma9
    "引理 9。 ‣ 证明。 ‣ D.2 第 5.1 节中论点的延迟证明 ‣ 附录 D 第 5 节中的延迟结果和证明 ‣ LLM 代理是否有悔恨？ 在线学习和游戏中的案例研究")中的简写符号。因此，
- en: '|  | $\displaystyle 1$ |  |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 1$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-924
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-925
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: with probability at least $1-\delta$.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 以至少$1-\delta$的概率。
- en: Now, for any <math id=$$, we have
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于任何<math id=$$, 我们有
- en: '|  | $\displaystyle 0$ |  |'
  id: totrans-928
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle 0$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Note that
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到
- en: '|  | $1$2 |  |'
  id: totrans-931
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: is a continuous function of $\theta$. Therefore,
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 是$\theta$的连续函数。因此，
- en: '|  | $1$2 |  | (15) |'
  id: totrans-933
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (15) |'
- en: and we know that $\lim_{N,k\to\infty}1+A(k,h,\epsilon)(\frac{1}{p(\epsilon)}+\widetilde{\mathcal{O}}(\sqrt{1/N}))=1$.
    ∎
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们知道$\lim_{N,k\to\infty}1+A(k,h,\epsilon)(\frac{1}{p(\epsilon)}+\widetilde{\mathcal{O}}(\sqrt{1/N}))=1$。∎
- en: Claim 3  (Double iterated limit of supremum).
  id: totrans-935
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 主张 3（上确界的双重迭代极限）。
- en: 'It holds that:'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 它成立：
- en: '|  | $1$2 |  |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-938
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Since $1$2, we will prove
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$1$2，我们将证明
- en: '|  | $1$2 |  |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Lemma 10.
  id: totrans-941
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 10。
- en: $\frac{\sum_{i=1}^{N}f(X_{i},k_{1})h(X_{i})}{\sum_{i=1}^{N}f(X_{i},k_{1})}\leq\frac{\sum_{i=1}^{N}f(X_{i},k_{2})h(X_{i})}{\sum_{i=1}^{N}f(X_{i},k_{2})}$.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: $\frac{\sum_{i=1}^{N}f(X_{i},k_{1})h(X_{i})}{\sum_{i=1}^{N}f(X_{i},k_{1})}\leq\frac{\sum_{i=1}^{N}f(X_{i},k_{2})h(X_{i})}{\sum_{i=1}^{N}f(X_{i},k_{2})}$。
- en: Proof.
  id: totrans-943
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: By multiplying $({\sum_{i=1}^{N}f(X_{i},k_{1})})({\sum_{i=1}^{N}f(X_{i},k_{2})})$.
    This is equivalent to
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: 通过乘以$({\sum_{i=1}^{N}f(X_{i},k_{1})})({\sum_{i=1}^{N}f(X_{i},k_{2})})$。这等于
- en: '|  | $1$2 |  |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: which is true since if $X_{i}\geq X_{j}$. ∎
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正确的，因为如果$X_{i}\geq X_{j}$。∎
- en: Therefore, $\mathcal{L}(\theta,k,N)$ is fixed, which indicates that
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\mathcal{L}(\theta,k,N)$是固定的，这表明
- en: '|  | $1$2 |  |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: exists, as $\mathcal{L}(\theta,k,N)$ is also bounded. Therefore, by [Lemma 5](#Thmlemma5
    "Lemma 5 (Double iterated limit). ‣ D.1 Basic Lemmas ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") and [2](#Thmclaim2 "Claim 2 (Uniform convergence of ℒ⁢(𝜃,𝑘,𝑁) (with
    respect to 𝑘 and 𝑁)). ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣
    Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games"), we know that
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 存在，因为$\mathcal{L}(\theta,k,N)$也是有界的。因此，根据[引理 5](#Thmlemma5 "引理 5 (双重迭代极限)。 ‣
    D.1 基本引理 ‣ 附录 D 第 5 节中的延迟结果和证明 ‣ LLM 代理是否有悔恨？ 在线学习和游戏中的案例研究")和[2](#Thmclaim2 "主张
    2 (ℒ⁢(𝜃,𝑘,𝑁) 的均匀收敛（关于𝑘和𝑁））。 ‣ D.2 第 5.1 节中论点的延迟证明 ‣ 附录 D 第 5 节中的延迟结果和证明 ‣ LLM
    代理是否有悔恨？ 在线学习和游戏中的案例研究")，我们知道
- en: '|  | $1$2 |  |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: exists and this value should be 0. ∎
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 存在，这个值应该是0。∎
- en: Claim 4.
  id: totrans-952
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 主张 4。
- en: It holds that
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 它成立
- en: '|  | $1$2 |  |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-955
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Firstly, by [Lemma 7](#Thmlemma7 "Lemma 7 (Uniform convergence ⟹ Interchanging
    limit and infimum). ‣ D.1 Basic Lemmas ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games"), we have $\lim_{N,k\to\infty}\inf_{\theta\in\Theta}\mathcal{L}(\theta,k,N)=\inf_{\theta\in\Theta}h(\max_{\ell_{1},\dots,\ell_{T}}\text{Regret}_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]}))$.
    ∎
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过 [引理 7](#Thmlemma7 "引理 7（均匀收敛 ⟹ 极限与下确界互换）。 ‣ D.1 基本引理 ‣ 附录 D 延迟结果和第 5 节的证明
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")，我们有 $\lim_{N,k\to\infty}\inf_{\theta\in\Theta}\mathcal{L}(\theta,k,N)=\inf_{\theta\in\Theta}h(\max_{\ell_{1},\dots,\ell_{T}}\text{Regret}_{\text{LLM}_{\theta}}((\ell_{t})_{t\in[T]}))$。∎
- en: D.3 Deferred Proofs of [Theorem 2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees
    via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    and [Corollary 1](#Thmcorollary1 "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  id: totrans-957
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[定理 2](#Thmtheorem2 "定理 2。 ‣ 5.2 通过遗憾-损失最小化的保证 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
    和 [推论 1](#Thmcorollary1 "推论 1。 ‣ 5.2 通过遗憾-损失最小化的保证 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM
    代理是否有遗憾？在线学习和游戏中的案例研究") 的延迟证明'
- en: See [2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees via Regret-Loss Minimization
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 见 [2](#Thmtheorem2 "定理 2。 ‣ 5.2 通过遗憾-损失最小化的保证 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
- en: Before proving the theorem, we remark on what LLM structure enjoys the Lipschitz-continuity.
    We provide two auxiliary results in the following proposition. The first result
    is from (Bai et al., [2023](#bib.bib13), Section J.1), which is about the Lipschitzness
    of Transformers. The second result is regarding processing the output of Transformers.
    In particular, the output of Transformers is usually not directly used, but passed
    through some matrix multiplication (by some matrix $A$), followed by some projection
    Operator (to be specified later).
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 在证明定理之前，我们对 LLm 结构的 Lipschitz 连续性做一些说明。我们在以下命题中提供了两个辅助结果。第一个结果来自（Bai 等，[2023](#bib.bib13)，第
    J.1 节），关于 Transformers 的 Lipschitz 性。第二个结果涉及到 Transformers 的输出处理。特别是，Transformers
    的输出通常不会直接使用，而是通过一些矩阵乘法（由某个矩阵 $A$ 完成），然后经过某些投影算子（稍后说明）。
- en: Proposition 2.
  id: totrans-960
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 2。
- en: The $L$, i.e.,
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: $L$，即，
- en: '|  | $\displaystyle\&#124;\texttt{TF}_{\theta_{1}}(Z)-\texttt{TF}_{\theta_{2}}(Z)\&#124;_{2,\infty}\leq
    C_{\texttt{TF}}\&#124;\theta_{1}-\theta_{2}\&#124;_{\texttt{TF}}$ |  |'
  id: totrans-962
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\&#124;\texttt{TF}_{\theta_{1}}(Z)-\texttt{TF}_{\theta_{2}}(Z)\&#124;_{2,\infty}\leq
    C_{\texttt{TF}}\&#124;\theta_{1}-\theta_{2}\&#124;_{\texttt{TF}}$ |  |'
- en: where $\|\cdot\|_{\texttt{TF}}$, i.e.,
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\|\cdot\|_{\texttt{TF}}$，即，
- en: '|  | $1$2 |  |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Here, Operator is either the projection operator onto some convex set, or the
    Softmax function.
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Operator 是对某些凸集的投影算子或 Softmax 函数。
- en: Proof.
  id: totrans-966
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: The first result is from (Bai et al., [2023](#bib.bib13), Section J.1). The
    second result comes from
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个结果来自（Bai 等，[2023](#bib.bib13)，第 J.1 节）。第二个结果来自
- en: •
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If Operator is a projection onto the convex set, then $\|\texttt{Operator}(x)-\texttt{Operator}(y)\|_{2}\leq\|x-y\|_{2}$;
  id: totrans-969
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 Operator 是对凸集的投影，那么 $\|\texttt{Operator}(x)-\texttt{Operator}(y)\|_{2}\leq\|x-y\|_{2}$；
- en: •
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If Operator is Softmax, then $\|\texttt{Softmax}(x)-\texttt{Softmax}(y)\|_{2}\leq\|x-y\|_{2}$
    (Gao and Pavel, [2017](#bib.bib43), Corollary 3).
  id: totrans-971
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果 Operator 是 Softmax，那么 $\|\texttt{Softmax}(x)-\texttt{Softmax}(y)\|_{2}\leq\|x-y\|_{2}$（Gao
    和 Pavel，[2017](#bib.bib43)，推论 3）。
- en: Note that the only condition that we require for Operator is its non-expansiveness.
    ∎
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们对 Operator 的唯一要求是它的非扩展性。∎
- en: Proof of [Theorem 2](#Thmtheorem2 "Theorem 2\. ‣ 5.2 Guarantees via Regret-Loss
    Minimization ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss
    ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  id: totrans-973
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '[定理 2](#Thmtheorem2 "定理 2。 ‣ 5.2 通过遗憾-损失最小化的保证 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
    的证明。'
- en: Let $C_{\text{LLM}}$ above in [Proposition 2](#Thmproposition2 "Proposition
    2\. ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")). Now, we prove that regret is also a Lipschitz-continuous function
    with respect to the LLM’s parameter.
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Proposition 2](#Thmproposition2 "Proposition 2\. ‣ D.3 Deferred Proofs of
    Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")中定义了$C_{\text{LLM}}$。现在，我们证明了遗憾也是相对于LLM参数的Lipschitz连续函数。
- en: Lemma 11  (Lipschitzness of regret).
  id: totrans-975
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 11 (遗憾的Lipschitz性)。
- en: The function $\text{Regret}_{\text{LLM}_{\theta}}$, i.e.,
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 函数$\text{Regret}_{\text{LLM}_{\theta}}$，即，
- en: '|  | $\displaystyle\Big{&#124;}\emph{Regret}_{\text{LLM}_{\theta_{1}}}$ |  |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Big{&#124;}\emph{Regret}_{\text{LLM}_{\theta_{1}}}$ |  |'
- en: Proof.
  id: totrans-978
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: By definition, we have
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，我们有
- en: '|  | $\displaystyle\Big{&#124;}\text{Regret}_{\text{LLM}_{\theta_{1}}}$ |  |'
  id: totrans-980
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Big{&#124;}\text{Regret}_{\text{LLM}_{\theta_{1}}}$ |  |'
- en: '|  |  | $\displaystyle=B\sum_{t=1}^{T}\&#124;\text{LLM}_{\theta_{1}}(Z_{t-1})-\text{LLM}_{\theta_{2}}(Z_{t-1})\&#124;$
    |  |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=B\sum_{t=1}^{T}\&#124;\text{LLM}_{\theta_{1}}(Z_{t-1})-\text{LLM}_{\theta_{2}}(Z_{t-1})\&#124;$
    |  |'
- en: '|  |  | $\displaystyle\leq BC_{\text{LLM}}T\&#124;\theta_{1}-\theta_{2}\&#124;_{\text{LLM}}$
    |  |'
  id: totrans-982
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq BC_{\text{LLM}}T\&#124;\theta_{1}-\theta_{2}\&#124;_{\text{LLM}}$
    |  |'
- en: where $Z_{t}:=(\ell_{1},\dots,\ell_{t},c)$-dimensional vector. ∎
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z_{t}:=(\ell_{1},\dots,\ell_{t},c)$-维向量。∎
- en: Now, we will prove the Lipschitzness of
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将证明
- en: '|  | $1$2 |  | (16) |'
  id: totrans-985
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (16) |'
- en: with respect to the model parameter $\theta$.
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于模型参数$\theta$。
- en: Claim 5.
  id: totrans-987
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Claim 5。
- en: For any $$R>, we have
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意$$R>，我们有
- en: '|  | $1$2 |  |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: for every $x,y\in\mathbb{R}^{n}$.
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个$x,y\in\mathbb{R}^{n}$。
- en: Proof.
  id: totrans-991
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: If $\beta=\infty$, we have
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\beta=\infty$，我们有
- en: '|  | $\displaystyle\lim_{\beta\to\infty}$ |  |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\lim_{\beta\to\infty}$ |  |'
- en: 'holds. Moreover, consider the following constrained optimization problem:'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 证明成立。此外，考虑以下约束优化问题：
- en: '|  | $1$2 |  |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle\qquad\text{subject to }\qquad&#124;x_{i}&#124;\leq R,~{}~{}~{}&#124;y_{i}&#124;\leq
    R~{}~{}~{}\text{ for all }i\in[N],$ |  |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\text{subject to }\qquad&#124;x_{i}&#124;\leq R,~{}~{}~{}&#124;y_{i}&#124;\leq
    R~{}~{}~{}\text{ for all }i\in[N],$ |  |'
- en: whose optimum is denoted as $F(R,\beta)$. ∎
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 其最优解记作$F(R,\beta)$。∎
- en: Note that [5](#Thmclaim5 "Claim 5\. ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") does
    not hold if either $x_{i}$.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，[5](#Thmclaim5 "Claim 5\. ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs of
    Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")如果$x_{i}$成立，则不成立。
- en: Also, note that the domain of $h:\mathbb{R}\to\mathbb{R}^{+}$.
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意$h:\mathbb{R}\to\mathbb{R}^{+}$的定义域。
- en: Lemma 12  (Lipschitzness of $C$ in [Equation 16](#A4.E16 "In Proof of Theorem
    2\. ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")).
  id: totrans-1000
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 12 (在[Equation 16](#A4.E16 "In Proof of Theorem 2\. ‣ D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")中的$C$的Lipschitz性)。
- en: The function $C$, i.e.,
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 函数$C$，即，
- en: '|  | $1$2 |  |'
  id: totrans-1002
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Proof.
  id: totrans-1003
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: '|  | $\displaystyle\big{&#124;}C(($ |  |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\big{&#124;}C(($ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1005
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1006
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Here, (i) holds due to [5](#Thmclaim5 "Claim 5\. ‣ Proof of Theorem 2\. ‣ D.3
    Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games"), (ii) holds since $h$, and (iii) holds due to [Lemma 11](#Thmlemma11
    "Lemma 11 (Lipschitzness of regret). ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"). ∎
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，(i) 由于[5](#Thmclaim5 "Claim 5\. ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs
    of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")成立，(ii)
    由于$h$成立，(iii) 由于[Lemma 11](#Thmlemma11 "Lemma 11 (Lipschitzness of regret). ‣
    Proof of Theorem 2\. ‣ D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games")成立。∎
- en: For completeness of the paper, we provide the definition of covering set and
    covering number.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 为了论文的完整性，我们提供了覆盖集和覆盖数的定义。
- en: Definition 4  (Covering set and covering number).
  id: totrans-1010
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4 (覆盖集和覆盖数)。
- en: For  can be
    bounded by
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 根据（Wainwright，[2019](#bib.bib104)，例5.8），对于任意$$r>可以被界定为
- en: '|  | $\displaystyle\log N(\delta;B(0,r,\&#124;\cdot\&#124;_{\text{LLM}}),\&#124;\cdot\&#124;_{\text{LLM}})\leq
    d_{\theta}\log(1+2r/\delta),$ |  |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\log N(\delta;B(0,r,\&#124;\cdot\&#124;_{\text{LLM}}),\&#124;\cdot\&#124;_{\text{LLM}})\leq
    d_{\theta}\log(1+2r/\delta),$ |  |'
- en: where $d_{\theta}$,
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$d_{\theta}$，
- en: '|  | $\displaystyle\log N(\delta;B(0,r,\&#124;\cdot\&#124;_{\text{LLM}}),\&#124;\cdot\&#124;_{\text{LLM}})\leq
    L(3Md^{2}+2d(dd^{\prime}+3md^{2}))\log(1+2r/\delta).$ |  |'
  id: totrans-1015
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\log N(\delta;B(0,r,\&#124;\cdot\&#124;_{\text{LLM}}),\&#124;\cdot\&#124;_{\text{LLM}})\leq
    L(3Md^{2}+2d(dd^{\prime}+3md^{2}))\log(1+2r/\delta).$ |  |'
- en: Since we consider a compact $\Theta$ with
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们考虑一个紧凑的$\Theta$，
- en: '|  | $1$2 |  |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then, by the standard result from statistical learning theory (Wainwright, [2019](#bib.bib104),
    Chapter 5), when trained with $N_{T}$, we have
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据统计学习理论的标准结果（Wainwright，[2019](#bib.bib104)，第5章），当训练次数为$N_{T}$时，我们有
- en: '|  | $\displaystyle\mathcal{L}(\widehat{\theta}_{k,N,N_{T}},k,N)$ |  |'
  id: totrans-1019
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(\widehat{\theta}_{k,N,N_{T}},k,N)$ |  |'
- en: Setting $\delta=\Omega(\sqrt{\log(\epsilon)/N_{T}})$, we further obtain
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: 设定$\delta=\Omega(\sqrt{\log(\epsilon)/N_{T}})$，我们进一步得到
- en: '|  | $\displaystyle\mathcal{L}(\widehat{\theta}_{k,N,N_{T}},k,N)$ |  |'
  id: totrans-1021
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}(\widehat{\theta}_{k,N,N_{T}},k,N)$ |  |'
- en: with probability at least $1-\epsilon$, completing the proof. ∎
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 以至少$1-\epsilon$的概率完成证明。∎
- en: See [1](#Thmcorollary1 "Corollary 1\. ‣ 5.2 Guarantees via Regret-Loss Minimization
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games")
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 见[1](#Thmcorollary1 "推论1。 ‣ 5.2 通过遗憾-损失最小化的保证 ‣ 5 通过无监督损失证明无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")
- en: Proof.
  id: totrans-1024
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: The limit on the right-hand side of [Equation 5](#S5.E5 "In Theorem 2\. ‣ 5.2
    Guarantees via Regret-Loss Minimization ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") remains as $\widetilde{\mathcal{O}}\left(\sqrt{\frac{d_{\theta}+\log(1/\epsilon)}{N_{T}}}\right)$.
    Next, we have
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程5](#S5.E5 "在定理2。 ‣ 5.2 通过遗憾-损失最小化的保证 ‣ 5 通过无监督损失证明无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")右侧的极限保持为$\widetilde{\mathcal{O}}\left(\sqrt{\frac{d_{\theta}+\log(1/\epsilon)}{N_{T}}}\right)$。接下来，我们有'
- en: '|  | $1$2 |  |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1028
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1029
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: due to the continuity of $h$ and [3](#Thmclaim3 "Claim 3 (Double iterated limit
    of supremum). ‣ D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games"). Finally, we have
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 由于$h$的连续性和[3](#Thmclaim3 "声明3（双重极限的极限）。 ‣ D.2 延迟证明在第5.1节的论点 ‣ 附录D 延迟结果和第5节中的证明
    ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究"）。最后，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: due to [4](#Thmclaim4 "Claim 4\. ‣ D.2 Deferred Proof for the Arguments in Section
    5.1 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have
    Regret? A Case Study in Online Learning and Games"), which, combined with the
    fact that $h$ is non-decreasing, completes the proof. ∎
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[4](#Thmclaim4 "声明4。 ‣ D.2 延迟证明在第5.1节的论点 ‣ 附录D 延迟结果和第5节中的证明 ‣ LLM代理是否有遗憾？在线学习和博弈中的案例研究")，结合$h$是单调递增的事实，完成了证明。∎
- en: Remark 5  (Dynamic-regret loss).
  id: totrans-1033
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注释5（动态遗憾损失）。
- en: 'So far, we have focused on the canonical online learning setting with regret
    being the metric. One can also generalize the results to the non-stationary setting,
    with dynamic regret being the metric. Specifically, one can define the *dynamic-regret-loss*
    function as follows:'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们专注于以遗憾作为度量的标准在线学习设置。结果也可以推广到非平稳设置，其中动态遗憾作为度量。具体而言，可以定义*动态遗憾损失*函数如下：
- en: '|  | $1$2 |  |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Then, one can also establish similar results as before, since the analysis does
    not utilize other properties of the regret except its boundedness, and the Lipschitz-continuity
    of LLM with respect to $\theta$. To be specific, [Lemma 11](#Thmlemma11 "Lemma
    11 (Lipschitzness of regret). ‣ Proof of Theorem 2\. ‣ D.3 Deferred Proofs of
    Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games") holds
    due to the reason that we can bound the difference of the regret with the term
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由于分析没有利用除了悔恨的有界性和 LLM 关于 $\theta$ 的 Lipschitz 连续性之外的其他性质，因此可以建立类似的结果。具体来说，[引理
    11](#Thmlemma11 "Lemma 11 (Lipschitzness of regret). ‣ Proof of Theorem 2\. ‣
    D.3 Deferred Proofs of Theorem 2 and Corollary 1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") 成立是因为我们可以将悔恨的差异界定在该项内
- en: '|  | $1$2 |  |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: as well as the fact that $\inf_{\pi_{i}\in\Pi}\langle\ell_{i},\pi_{i}\rangle$
    will be canceled. One can verify that all the arguments in [Section D.2](#A4.SS2
    "D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") also hold for similar reasons.
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 以及 $\inf_{\pi_{i}\in\Pi}\langle\ell_{i},\pi_{i}\rangle$ 将被取消的事实。可以验证，[第 D.2
    节](#A4.SS2 "D.2 Deferred Proof for the Arguments in Section 5.1 ‣ Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games")中的所有论点也由于类似的原因成立。
- en: D.4 Deferred Proof of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  id: totrans-1039
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 [定理 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") 的延期证明
- en: See [3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见 [3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")
- en: Proof.
  id: totrans-1041
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We will locally use $\mathcal{A}=[d]$, the loss function ([Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games")) can be written as follows:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将局部使用 $\mathcal{A}=[d]$，损失函数 ([公式 3](#S5.E3 "In 5.1 A New Unsupervised Training
    Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"))
    可以写成如下形式：'
- en: '|  | $1$2 |  |'
  id: totrans-1043
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where for $t=1$.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于 $t=1$。
- en: Step 1. Calculating $\frac{\partial f}{\partial a}$.
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1. 计算 $\frac{\partial f}{\partial a}$。
- en: 'For $x\in[d]$:'
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $x\in[d]$：
- en: '|  | $1$2 |  |'
  id: totrans-1047
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1048
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1050
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Plugging $a=\boldsymbol{0}_{d}$ provides
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 代入 $a=\boldsymbol{0}_{d}$ 得到
- en: '|  | $\displaystyle\frac{\partial}{\partial a_{x}}$ |  |'
  id: totrans-1052
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial a_{x}}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1053
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'For $t=1$ as follows:'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $t=1$ 如下：
- en: '|  | $1$2 |  |'
  id: totrans-1055
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1056
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
- en: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial a_{x}}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
  id: totrans-1058
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial a_{x}}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
- en: '|  | $1$2 |  | (17) |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (17) |'
- en: '|  | $\displaystyle=0,$ |  |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=0,$ |  |'
- en: 'where we used the fact that $\ell_{i}$ yields the same distribution, which
    leads to the following:'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 $\ell_{i}$ 产生相同分布的事实，这导致了以下结果：
- en: '|  | $1$2 |  |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1063
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This yields [Equation 17](#A4.E17 "In Proof. ‣ D.4 Deferred Proof of Theorem
    3 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")=0.
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 这得到了[方程 17](#A4.E17 "在证明中。 ‣ D.4 推迟证明定理 3 ‣ 附录 D 推迟的结果和证明在第 5 节 ‣ LLM 代理是否有悔恨？在线学习和博弈中的案例研究")=0。
- en: Step 2. Calculating $\frac{\partial f}{\partial v_{c}}$.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步。计算 $\frac{\partial f}{\partial v_{c}}$。
- en: 'We will use the following equation for $t\geq 2$:'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对 $t\geq 2$ 使用以下方程：
- en: '|  | $\displaystyle\frac{\partial}{\partial v_{c}}$ |  |'
  id: totrans-1067
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial v_{c}}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'For $t=1$ as follows:'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $t=1$ 如下：
- en: '|  | $1$2 |  |'
  id: totrans-1070
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1071
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
  id: totrans-1072
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
- en: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial v_{c}}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial v_{c}}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1074
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The last line is due to the same reason as the last part of Step 1.
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行与第 1 步的最后部分有相同的原因。
- en: Step 3. Calculating $\frac{\partial f}{\partial V}$.
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步。计算 $\frac{\partial f}{\partial V}$。
- en: 'We calculate the following equation, which will be used to calculate $\frac{\partial
    f}{\partial V}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$:'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算以下方程，这将用于计算 $\frac{\partial f}{\partial V}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$：
- en: '|  | $\displaystyle\frac{\partial}{\partial V}$ |  |'
  id: totrans-1078
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial V}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1079
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: For $t=1$.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $t=1$。
- en: Therefore, we have
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1083
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1084
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}$
    |  |'
- en: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial V}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
  id: totrans-1086
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\qquad\frac{\partial}{\partial V}\left(\sum_{t=1}^{T}\ell_{t}^{\intercal}(V\ell_{1:t-1}+v_{c}\boldsymbol{1}_{t-1}^{\intercal})\texttt{Softmax}(\ell_{1:t-1}^{\intercal}a+b_{t-1})+R_{\Pi}\&#124;\sum_{t=1}^{T}\ell_{t}\&#124;_{2}\right)\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=\boldsymbol{0}_{d},(b_{t}=\beta\boldsymbol{1}_{t})_{t\in[T-1]}}\Bigg{]}$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1087
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1090
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1091
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Therefore, if $V^{\star}=R_{\Pi}\frac{T}{\sum_{t=1}^{T-1}1/t}\Sigma^{-1}\mathbb{E}\Bigg{[}\|\sum_{t=1}^{T}\ell_{t}\|_{2}\ell_{t}\ell_{i}^{\intercal}\Bigg{]}\Sigma^{-1}$.
    Lastly, we have
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 $V^{\star}=R_{\Pi}\frac{T}{\sum_{t=1}^{T-1}1/t}\Sigma^{-1}\mathbb{E}\Bigg{[}\|\sum_{t=1}^{T}\ell_{t}\|_{2}\ell_{t}\ell_{i}^{\intercal}\Bigg{]}\Sigma^{-1}$。最后，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1093
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1094
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1095
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'which means that such configurations are first-order stationary points of [Equation 3](#S5.E3
    "In 5.1 A New Unsupervised Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret
    Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in
    Online Learning and Games") with $N=1$. ∎'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这些配置是[公式 3](#S5.E3 "在 5.1 新的无监督训练损失：遗憾损失 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
    中的一级驻点，$N=1$。∎
- en: D.5 Deferred Proof of [Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  id: totrans-1097
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.5 延迟证明[定理 4](#Thmtheorem4 "定理 4\. ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无遗憾行为
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
- en: See [4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games")
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [4](#Thmtheorem4 "定理 4\. ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无遗憾行为
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
- en: Proof.
  id: totrans-1099
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'The output of the single-layer linear self-attention structure is as follows:'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: 单层线性自注意力结构的输出如下：
- en: '|  | $\displaystyle g($ |  | (18) |'
  id: totrans-1101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g($ |  | (18) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1102
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: which can be expressed with a larger class
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用更大类来表示
- en: '|  | $\displaystyle g(Z_{t},\mathbb{A},\beta,\mathbb{C},\delta):=\sum_{i=1}^{t}(\mathbb{A}\ell_{i}\ell_{i}^{\intercal}\beta+\mathbb{C}\ell_{i}+\delta),$
    |  | (19) |'
  id: totrans-1104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g(Z_{t},\mathbb{A},\beta,\mathbb{C},\delta):=\sum_{i=1}^{t}(\mathbb{A}\ell_{i}\ell_{i}^{\intercal}\beta+\mathbb{C}\ell_{i}+\delta),$
    |  | (19) |'
- en: where $\mathbb{A}\in\mathbb{R}^{d\times d}$. Then, if a minimizer of
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{A}\in\mathbb{R}^{d\times d}$。然后，如果一个最小化者
- en: '|  | $\displaystyle f(\mathbb{A},\beta,\mathbb{C},\delta):$ |  |'
  id: totrans-1106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathbb{A},\beta,\mathbb{C},\delta):$ |  |'
- en: can be expressed as $\mathbb{A}=V,\beta=K^{\intercal}(Qc+q_{c}),\mathbb{C}=Vk_{c}^{\intercal}(Qc+q_{c})+v_{c}(Qc+q_{c})^{\intercal}K,\beta=v_{c}k_{c}^{\intercal}(Qc+q_{c})$
    are also a minimizer of
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表示为 $\mathbb{A}=V,\beta=K^{\intercal}(Qc+q_{c}),\mathbb{C}=Vk_{c}^{\intercal}(Qc+q_{c})+v_{c}(Qc+q_{c})^{\intercal}K,\beta=v_{c}k_{c}^{\intercal}(Qc+q_{c})$
    也是一个最小化者
- en: '|  | $1$2 |  |'
  id: totrans-1108
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: since the corresponding $V,Q,K,v_{c},q_{c},k_{c}$ as
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 因为对应的 $V,Q,K,v_{c},q_{c},k_{c}$ 如下
- en: '|  | $1$2 |  | (20) |'
  id: totrans-1110
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (20) |'
- en: Step 1. Finding condition for $\frac{\partial f}{\partial\delta}=0$.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步。寻找 $\frac{\partial f}{\partial\delta}=0$ 的条件。
- en: Due to the Leibniz rule, if we calculate the partial derivative of [Equation 20](#A4.E20
    "In Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix D Deferred Results and
    Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") w.r.t. $\delta$, we have
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于莱布尼茨法则，如果我们计算[公式 20](#A4.E20 "在证明中。 ‣ D.5 定理 4 的延迟证明 ‣ 附录 D 第 5 节的延迟结果和证明
    ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究") 对 $\delta$ 的偏导数，我们得到
- en: '|  | $1$2 |  |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1114
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  | (21) |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (21) |'
- en: Since the expectation of either odd-order polynomial or even-order polynomial
    times $\|\cdot\|_{2}$ follows a symmetric distribution, we have
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任意奇数阶多项式或偶数阶多项式乘以 $\|\cdot\|_{2}$ 的期望遵循对称分布，我们得到
- en: '|  | $1$2 |  |'
  id: totrans-1117
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Now, we calculate
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算
- en: '|  | $\displaystyle\mathbb{E}\sum_{t=1}^{T}$ |  |'
  id: totrans-1119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\sum_{t=1}^{T}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where $(i)$. Lastly,
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(i)$。最后，
- en: '|  | $\displaystyle\mathbb{E}\sum_{t=1}^{T}$ |  |'
  id: totrans-1122
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\sum_{t=1}^{T}$ |  |'
- en: Plugging the above equations into [Equation 21](#A4.E21 "In Proof. ‣ D.5 Deferred
    Proof of Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do
    LLM Agents Have Regret? A Case Study in Online Learning and Games"), we have
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述方程代入[公式 21](#A4.E21 "在证明中。 ‣ D.5 定理 4 的延迟证明 ‣ 附录 D 第 5 节的延迟结果和证明 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")，我们得到
- en: '|  | $\displaystyle\frac{\partial f(\mathbb{A},\beta,\mathbb{C},\delta)}{\partial\delta}=\frac{1}{6}T(2T^{2}-3T+1)(\Sigma\mathbb{A}\Sigma\beta+\Sigma\delta).$
    |  |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial f(\mathbb{A},\beta,\mathbb{C},\delta)}{\partial\delta}=\frac{1}{6}T(2T^{2}-3T+1)(\Sigma\mathbb{A}\Sigma\beta+\Sigma\delta).$
    |  |'
- en: Due to the optimality condition, we have
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最优性条件，我们得到
- en: '|  | $\displaystyle\mathbb{A}\Sigma\beta+\delta=0.$ |  | (22) |'
  id: totrans-1126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{A}\Sigma\beta+\delta=0.$ |  | (22) |'
- en: Step 2. Plugging the optimality condition for $\frac{\partial f}{\partial\delta}$
    into [Equation 20](#A4.E20 "In Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games").
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步。将最优性条件 $\frac{\partial f}{\partial\delta}$ 代入 [方程 20](#A4.E20 "在证明中 ‣ D.5
    推迟证明定理 4 ‣ 附录 D 推迟结果与证明在第5节 ‣ LLM 代理是否有遗憾？一个在线学习和游戏的案例研究")。
- en: Plugging [Equation 22](#A4.E22 "In Proof. ‣ D.5 Deferred Proof of Theorem 4
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") to [Equation 20](#A4.E20 "In Proof.
    ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games"),
    $f$ can be written as
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 将 [方程 22](#A4.E22 "在证明中 ‣ D.5 推迟证明定理 4 ‣ 附录 D 推迟结果与证明在第5节 ‣ LLM 代理是否有遗憾？一个在线学习和游戏的案例研究")
    代入 [方程 20](#A4.E20 "在证明中 ‣ D.5 推迟证明定理 4 ‣ 附录 D 推迟结果与证明在第5节 ‣ LLM 代理是否有遗憾？一个在线学习和游戏的案例研究")，$f$
    可以表示为
- en: '|  | $\displaystyle f(\mathbb{A},$ |  |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathbb{A},$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1131
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1132
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1133
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: For the part $(i)$, we have
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于部分 $(i)$，我们有
- en: '|  | $\displaystyle\mathbb{E}$ |  |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1136
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1137
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  | (23) |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (23) |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Here, $(1)$.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$(1)$。
- en: If $\mathbb{A}\neq\bm{O}_{d\times d}$, and is a non-zero vector.
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $\mathbb{A}\neq\bm{O}_{d\times d}$，并且是一个非零向量。
- en: Now, we will generally consider $a_{x,y}(v):=vv^{\intercal}x-y$ can be calculated
    as
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们一般考虑 $a_{x,y}(v):=vv^{\intercal}x-y$ 可以计算为
- en: '|  | <math id=$$ |  |'
  id: totrans-1143
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id=$$ |  |'
- en: Therefore, $\text{Volume}(V_{1}(vv^{\intercal}-\Sigma))$ should hold. In both
    cases, [Equation 19](#A4.E19 "In Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") can be re-written as
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，$\text{Volume}(V_{1}(vv^{\intercal}-\Sigma))$ 应该成立。在这两种情况下，[方程 19](#A4.E19
    "在证明中 ‣ D.5 推迟证明定理 4 ‣ 附录 D 推迟结果与证明在第5节 ‣ LLM 代理是否有遗憾？一个在线学习和游戏的案例研究") 可以重新写为
- en: '|  | $\displaystyle g(Z_{t};\mathbb{A},\beta,\mathbb{C},\delta):=\sum_{i=1}^{t}\mathbb{C}\ell_{i},$
    |  |'
  id: totrans-1145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g(Z_{t};\mathbb{A},\beta,\mathbb{C},\delta):=\sum_{i=1}^{t}\mathbb{C}\ell_{i},$
    |  |'
- en: and this is covered by the original parametrization ([Equation 18](#A4.E18 "In
    Proof. ‣ D.5 Deferred Proof of Theorem 4 ‣ Appendix D Deferred Results and Proofs
    in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and
    Games")) with $K^{\intercal}(Qc+q_{c})=v_{c}=\boldsymbol{0}_{d}$.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 这由原始参数化覆盖 ([方程 18](#A4.E18 "在证明中 ‣ D.5 推迟证明定理 4 ‣ 附录 D 推迟结果与证明在第5节 ‣ LLM 代理是否有遗憾？一个在线学习和游戏的案例研究"))，其中
    $K^{\intercal}(Qc+q_{c})=v_{c}=\boldsymbol{0}_{d}$。
- en: Step 3. Calculating $\frac{\partial f}{\partial\mathbb{C}}$.
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步。计算 $\frac{\partial f}{\partial\mathbb{C}}$。
- en: 'Now, we optimize over $\mathbb{C}$, by minimizing the following objective:'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对 $\mathbb{C}$ 进行优化，通过最小化以下目标：
- en: '|  | $\displaystyle f(\mathbb{C}):$ |  |'
  id: totrans-1149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathbb{C}):$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1150
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1151
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'Here, $(i)$ can be calculated as follows:'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$(i)$ 可以按如下方式计算：
- en: '|  | $\displaystyle\mathbb{E}$ |  |'
  id: totrans-1153
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1154
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: since (1) holds because if $t_{1}\neq t$.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 (1) 成立是因为如果 $t_{1}\neq t$。
- en: 'We calculate $\frac{\partial f(\mathbb{C})}{\partial\mathbb{C}}$:'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 $\frac{\partial f(\mathbb{C})}{\partial\mathbb{C}}$：
- en: '|  | $1$2 |  |'
  id: totrans-1158
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Hence, the optimal $1$2
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最优的 $1$2
- en: Now, we see that for the special case of $\Sigma=I$, we need to calculate
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们看到在特例 $\Sigma=I$ 下，我们需要计算
- en: '|  | $\displaystyle\mathbb{E}_{\ell}\left[\sqrt{\sum_{o=1}^{d}(\sum_{s=1}^{T}\ell_{so})^{2}}\ell_{ia}\ell_{kb}\right].$
    |  |'
  id: totrans-1161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\ell}\left[\sqrt{\sum_{o=1}^{d}(\sum_{s=1}^{T}\ell_{so})^{2}}\ell_{ia}\ell_{kb}\right].$
    |  |'
- en: If $a\neq b$’s coordinates are independent.
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $a\neq b$ 的坐标是独立的。
- en: Now, we calculate the scale of $\mathbb{E}_{\ell}\left[\sqrt{\sum_{o=1}^{d}(\sum_{s=1}^{T}\ell_{so})^{2}}\ell_{i1}\ell_{k1}\right]$.
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算 $\mathbb{E}_{\ell}\left[\sqrt{\sum_{o=1}^{d}(\sum_{s=1}^{T}\ell_{so})^{2}}\ell_{i1}\ell_{k1}\right]$
    的规模。
- en: '|  | $\displaystyle\mathbb{E}_{\ell}$ |  |'
  id: totrans-1164
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}_{\ell}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1165
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1166
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Taking $d\to\infty$, we have
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 取 $d\to\infty$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1168
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: which further implies
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 这进一步意味着
- en: '|  | $1$2 |  |'
  id: totrans-1170
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1171
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: as $d\to\infty$. Therefore,
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 $d\to\infty$。因此，
- en: '|  | $1$2 |  |'
  id: totrans-1173
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1174
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: which is a constant. The last equality came from the fact that $W$ (Hazan, [2016](#bib.bib47),
    Theorem 3.1), which is consistent with the result above. ∎
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个常量。最后一个等式来源于$W$ (Hazan, [2016](#bib.bib47), 定理3.1)，这与上述结果一致。∎
- en: D.6 Empirical Validation of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") and [Theorem 4](#Thmtheorem4 "Theorem
    4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning
    Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  id: totrans-1176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.6 《定理3》的经验验证见[定理3](#Thmtheorem3 "定理3 ‣ 5.3 最小化悔恨损失可以自动生成已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为
    ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")和[定理4](#Thmtheorem4 "定理4 ‣ 5.3 最小化悔恨损失可以自动生成已知的在线学习算法
    ‣ 5 通过无监督损失证明促进无悔行为 ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")
- en: We now provide empirical validations for [Theorem 3](#Thmtheorem3 "Theorem 3\.
    ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") and [Theorem 4](#Thmtheorem4
    "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online
    Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    We provide the training details and the results as follows.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为[定理3](#Thmtheorem3 "定理3 ‣ 5.3 最小化悔恨损失可以自动生成已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为
    ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")和[定理4](#Thmtheorem4 "定理4 ‣ 5.3 最小化悔恨损失可以自动生成已知的在线学习算法
    ‣ 5 通过无监督损失证明促进无悔行为 ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")提供经验验证。我们提供训练细节和结果如下。
- en: D.6.1 Empirical Validation of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  id: totrans-1178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.6.1 《定理3》的经验验证见[定理3](#Thmtheorem3 "定理3 ‣ 5.3 最小化悔恨损失可以自动生成已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为
    ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")
- en: 'Our model architecture is defined as follows: the number of layers $T$) as
    zero vectors.'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型架构定义如下：层数为$T$的向量。
- en: 'Our empirical analysis aims to demonstrate that the optimized model inherently
    emulates online gradient descent. To illustrate this, we will focus on two key
    convergence properties: $K^{\intercal}Q$. The results are demonstrated in the
    first plot of [Figure 15](#A4.F15 "In D.6.2 Empirical Validation of Theorem 4
    ‣ D.6 Empirical Validation of Theorem 3 and Theorem 4 ‣ Appendix D Deferred Results
    and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games").'
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的经验分析旨在展示优化模型本质上模拟了在线梯度下降。为了说明这一点，我们将关注两个关键的收敛性质：$K^{\intercal}Q$。结果展示在[图
    15](#A4.F15 "在D.6.2《定理4的经验验证》 ‣ D.6《定理3和定理4的经验验证》 ‣ 附录D 延迟结果与证明 ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")的第一个图中。
- en: D.6.2 Empirical Validation of [Theorem 4](#Thmtheorem4 "Theorem 4\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")
  id: totrans-1181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.6.2 《定理4》的经验验证见[定理4](#Thmtheorem4 "定理4 ‣ 5.3 最小化悔恨损失可以自动生成已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为
    ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")
- en: 'We now focus on two key convergence properties: $K^{\intercal}(Q\boldsymbol{1}_{d}+q_{c})$.
    The results are demonstrated in the second plot of [Figure 15](#A4.F15 "In D.6.2
    Empirical Validation of Theorem 4 ‣ D.6 Empirical Validation of Theorem 3 and
    Theorem 4 ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games").'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在关注两个关键的收敛性质：$K^{\intercal}(Q\boldsymbol{1}_{d}+q_{c})$。结果展示在[图 15](#A4.F15
    "在D.6.2《定理4的经验验证》 ‣ D.6《定理3和定理4的经验验证》 ‣ 附录D 延迟结果与证明 ‣ LLM代理是否有悔恨？在线学习和游戏中的案例研究")的第二个图中。
- en: '![Refer to caption](img/8a9731f09c265329c7c42de3203dd694.png)![Refer to caption](img/195e130ed05eaf1b85b007dd662def5c.png)![Refer
    to caption](img/7c4ecbd4c3c8185a1017a5f7a81d8e65.png)'
  id: totrans-1183
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8a9731f09c265329c7c42de3203dd694.png)![参见标题](img/195e130ed05eaf1b85b007dd662def5c.png)![参见标题](img/7c4ecbd4c3c8185a1017a5f7a81d8e65.png)'
- en: 'Figure 15: Empirical validation of [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣
    5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") (top), [Theorem 4](#Thmtheorem4
    "Theorem 4\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online
    Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    (middle), and [6](#Thmtheorem6 "Conjecture 6\. ‣ D.7 Discussions on the Production
    of FTRL with Entropy Regularization ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    (bottom). The observed convergence in [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣
    5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning Algorithms
    ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents
    Have Regret? A Case Study in Online Learning and Games") and [6](#Thmtheorem6
    "Conjecture 6\. ‣ D.7 Discussions on the Production of FTRL with Entropy Regularization
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games")’s result suggests that configuration
    in [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically
    Produce Known Online Learning Algorithms ‣ 5 Provably Promoting No-Regret Behavior
    by an Unsupervised Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning
    and Games") and [6](#Thmtheorem6 "Conjecture 6\. ‣ D.7 Discussions on the Production
    of FTRL with Entropy Regularization ‣ Appendix D Deferred Results and Proofs in
    Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
    are not only the local optimal point, but it has the potential as being the global
    optimizer.'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：对[定理 3](#Thmtheorem3 "定理 3\. ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无遗憾行为
    ‣ LLM代理是否有遗憾？在线学习和博弈的案例研究")（顶部）、[定理 4](#Thmtheorem4 "定理 4\. ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法
    ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈的案例研究")（中部）和[6](#Thmtheorem6 "猜想 6\.
    ‣ D.7 关于带有熵正则化的FTRL生产的讨论 ‣ 附录 D 第 5 节延迟结果和证明 ‣ LLM代理是否有遗憾？在线学习和博弈的案例研究")（底部）的经验验证。观察到[定理
    3](#Thmtheorem3 "定理 3\. ‣ 5.3 最小化遗憾损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无遗憾行为 ‣ LLM代理是否有遗憾？在线学习和博弈的案例研究")和[6](#Thmtheorem6
    "猜想 6\. ‣ D.7 关于带有熵正则化的FTRL生产的讨论 ‣ 附录 D 第 5 节延迟结果和证明 ‣ LLM代理是否有遗憾？在线学习和博弈的案例研究")的结果表明，这些配置不仅是局部最优点，而且有作为全局优化器的潜力。
- en: D.7 Discussions on the Production of FTRL with Entropy Regularization
  id: totrans-1185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.7 关于带有熵正则化的FTRL生产的讨论
- en: 'Now, we will consider projecting a single-layer linear self-attention model
    into a constrained domain such as a simplex, which is more amenable to the Experts
    Problem setting. To this end, we consider the following parameterization by adding
    an additional *non-linear* structure for the single-layer linear self-attention:'
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将考虑将单层线性自注意力模型投射到约束领域，如单纯形，这更适合专家问题设置。为此，我们考虑通过为单层线性自注意力添加一个额外的*非线性*结构来进行参数化：
- en: '|  | $\displaystyle g(Z_{t};V,K,Q,v_{c},k_{c},q_{c})=\texttt{Operator}\left(\sum_{i=1}^{t}(V\ell_{i}+v_{c})((K\ell_{i}+k_{c}))^{\intercal}\cdot(Qc+q_{c}))\right),$
    |  | (24) |'
  id: totrans-1187
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g(Z_{t};V,K,Q,v_{c},k_{c},q_{c})=\texttt{Operator}\left(\sum_{i=1}^{t}(V\ell_{i}+v_{c})((K\ell_{i}+k_{c}))^{\intercal}\cdot(Qc+q_{c}))\right),$
    |  | (24) |'
- en: where the Operator denotes projection to the convex set.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Operator表示投射到凸集。
- en: Conjecture 6.
  id: totrans-1189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 猜想 6。
- en: Assume $\Sigma=I$. This configuration performs FTRL with an entropy regularizer
    which is a no-regret algorithm.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $\Sigma=I$。该配置执行带有熵正则化的FTRL，这是一个无遗憾算法。
- en: We provide an idea for proving the conjecture, together with its numerical validation.
    Also, we have observed in [Figure 15](#A4.F15 "In D.6.2 Empirical Validation of
    Theorem 4 ‣ D.6 Empirical Validation of Theorem 3 and Theorem 4 ‣ Appendix D Deferred
    Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case Study in Online
    Learning and Games") that [Theorem 3](#Thmtheorem3 "Theorem 3\. ‣ 5.3 Minimizing
    Regret-Loss Can Automatically Produce Known Online Learning Algorithms ‣ 5 Provably
    Promoting No-Regret Behavior by an Unsupervised Loss ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") and [6](#Thmtheorem6 "Conjecture 6\.
    ‣ D.7 Discussions on the Production of FTRL with Entropy Regularization ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") might also be a global optimizer, as training
    results have provided the configuration that [Theorem 3](#Thmtheorem3 "Theorem
    3\. ‣ 5.3 Minimizing Regret-Loss Can Automatically Produce Known Online Learning
    Algorithms ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised Loss ‣
    Do LLM Agents Have Regret? A Case Study in Online Learning and Games") and [6](#Thmtheorem6
    "Conjecture 6\. ‣ D.7 Discussions on the Production of FTRL with Entropy Regularization
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games") have suggested.
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个证明猜想的思路，并进行了数值验证。此外，我们在[图15](#A4.F15 "在 D.6.2 定理 4 的经验验证 ‣ D.6 定理 3 和定理
    4 的经验验证 ‣ 附录 D 延迟结果与证明在第 5 节 ‣ LLM 代理是否有悔恨？在线学习与博弈中的案例研究")中观察到，[定理3](#Thmtheorem3
    "定理 3. ‣ 5.3 最小化悔恨损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为 ‣ LLM 代理是否有悔恨？在线学习与博弈中的案例研究")和[6](#Thmtheorem6
    "猜想 6. ‣ D.7 关于具有熵正则化的 FTRL 生产的讨论 ‣ 附录 D 延迟结果与证明在第 5 节 ‣ LLM 代理是否有悔恨？在线学习与博弈中的案例研究")也可能是全局优化器，因为训练结果提供了[定理3](#Thmtheorem3
    "定理 3. ‣ 5.3 最小化悔恨损失可以自动产生已知的在线学习算法 ‣ 5 通过无监督损失证明促进无悔行为 ‣ LLM 代理是否有悔恨？在线学习与博弈中的案例研究")和[6](#Thmtheorem6
    "猜想 6. ‣ D.7 关于具有熵正则化的 FTRL 生产的讨论 ‣ 附录 D 延迟结果与证明在第 5 节 ‣ LLM 代理是否有悔恨？在线学习与博弈中的案例研究")建议的配置。
- en: To be specific, we will consider
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将考虑
- en: '|  | $\displaystyle f(V,a,\beta,v_{c})$ |  |'
  id: totrans-1193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(V,a,\beta,v_{c})$ |  |'
- en: and will try to prove that $a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI$
    is a first-order stationary point.
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 并将尝试证明 $a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI$ 是一阶驻点。
- en: Step 1. Calculating $\frac{\partial f}{\partial v_{c}}$.
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步. 计算 $\frac{\partial f}{\partial v_{c}}$。
- en: 'We use the following formula: for $x\in[d]$, we have'
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下公式：对于 $x\in[d]$，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1197
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1198
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1199
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=t\beta\exp(v\beta)\exp(\beta k\sum_{i=1}^{t}\ell_{iy}),$
    |  |'
  id: totrans-1200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=t\beta\exp(v\beta)\exp(\beta k\sum_{i=1}^{t}\ell_{iy}),$
    |  |'
- en: and for $t=1$. Thus, we have
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 并且对于 $t=1$。因此，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1202
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\beta\exp(v\beta)$ |  |'
  id: totrans-1203
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\beta\exp(v\beta)$ |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1204
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=0.$ |  |'
  id: totrans-1205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=0.$ |  |'
- en: Therefore,
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial v_{cx}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
  id: totrans-1207
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial v_{cx}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1208
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1209
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=0.$ |  |'
  id: totrans-1210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=0.$ |  |'
- en: Step 2. Calculating $\frac{\partial f}{\partial V}$.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步. 计算 $\frac{\partial f}{\partial V}$。
- en: The following formula will be used for calculating $\frac{\partial f}{\partial
    V}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$, we have
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式将用于计算 $\frac{\partial f}{\partial V}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$，我们有
- en: '|  | $\displaystyle\frac{\partial}{\partial V_{rc}}$ |  |'
  id: totrans-1213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial V_{rc}}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1214
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Therefore,
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial V_{rc}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
  id: totrans-1217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial V_{rc}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1219
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}+v\beta\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta V\ell_{jy}+v\beta\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  id: totrans-1220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}+v\beta\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta V\ell_{jy}+v\beta\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1221
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1222
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\beta\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta V\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  id: totrans-1223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\beta\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta V\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1224
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1225
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'We can observe the followings: 1) if $r_{1}\neq c_{1}$.'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到以下情况：1) 如果 $r_{1}\neq c_{1}$。
- en: Step 3. Calculating $\frac{\partial f}{\partial\beta}$.
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3. 计算 $\frac{\partial f}{\partial\beta}$。
- en: 'The following formula will be used for calculating $\frac{\partial f}{\partial\beta}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$:'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式将用于计算 $\frac{\partial f}{\partial\beta}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$：
- en: '|  | $\displaystyle\frac{\partial}{\partial\beta}$ |  |'
  id: totrans-1229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial\beta}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1230
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=tv\beta\exp\left(\sum_{i=1}^{t}k\beta\ell_{iy}+v\beta\right).$
    |  |'
  id: totrans-1231
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=tv\beta\exp\left(\sum_{i=1}^{t}k\beta\ell_{iy}+v\beta\right).$
    |  |'
- en: Further, we have
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步，我们有
- en: '|  | $1$2 |  |'
  id: totrans-1233
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=v\beta\exp(v\beta)$ |  |'
  id: totrans-1234
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=v\beta\exp(v\beta)$ |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1235
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=0.$ |  |'
  id: totrans-1236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=0.$ |  |'
- en: Step 4. Calculating $\frac{\partial f}{\partial a}$.
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4. 计算 $\frac{\partial f}{\partial a}$。
- en: Note that
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到
- en: '|  | $\displaystyle\frac{\partial}{\partial a_{x}}$ |  |'
  id: totrans-1239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial}{\partial a_{x}}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1240
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1241
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-1242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: Therefore,
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，
- en: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial a_{x}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
  id: totrans-1244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial f(V,a,\beta,v_{c})}{\partial a_{x}}\bigg{&#124;}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1245
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1246
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  id: totrans-1247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}\Bigg{[}\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
- en: '|  | $\displaystyle\qquad\Biggl{(}\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\sum_{j=1}^{t-1}(k\ell_{js}\ell_{jx}+v\ell_{jx})\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}$ |  |'
  id: totrans-1248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\Biggl{(}\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\sum_{j=1}^{t-1}(k\ell_{js}\ell_{jx}+v\ell_{jx})\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}$ |  |'
- en: '|  | $1$2 |  |'
  id: totrans-1249
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $\displaystyle=\mathbb{E}\Bigg{[}k\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
  id: totrans-1250
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\mathbb{E}\Bigg{[}k\left(\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)}{\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}-\min_{s}\sum_{t=1}^{T}\ell_{ts}\right)$
    |  |'
- en: '|  | $\displaystyle\qquad\Biggl{(}\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\sum_{j=1}^{t-1}\ell_{js}\ell_{jx}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}$ |  |'
  id: totrans-1251
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\Biggl{(}\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\sum_{j=1}^{t-1}\ell_{js}\ell_{jx}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta k\ell_{jy}\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}$ |  |'
- en: '|  | $\displaystyle\qquad\qquad-\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\left(\sum_{j=1}^{t-1}\ell_{jy}\ell_{jx}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}\Biggr{)}\Bigg{]}$ |  |'
  id: totrans-1252
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\qquad\qquad-\sum_{t=1}^{T}\sum_{s=1}^{d}\ell_{ts}\frac{\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{js}\right)\sum_{y=1}^{d}\left(\sum_{j=1}^{t-1}\ell_{jy}\ell_{jx}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)}{\left(\sum_{y=1}^{d}\exp\left(\sum_{j=1}^{t-1}\beta
    k\ell_{jy}\right)\right)^{2}}\Biggr{)}\Bigg{]}$ |  |'
- en: Note that the value does not depend on $x$.
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意该值不依赖于 $x$。
- en: D.7.1 Numerical Analysis of Step 2 and Step 4
  id: totrans-1254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.7.1 步骤 2 和步骤 4 的数值分析
- en: In Steps 2 and 4 above, we were not able to show that a $k$ times.
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述步骤 2 和 4 中，我们未能展示 $k$ 的 $k$ 次。
- en: '![Refer to caption](img/3b96f11c4ffae829dd34e68270baa95c.png)'
  id: totrans-1256
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3b96f11c4ffae829dd34e68270baa95c.png)'
- en: 'Figure 16: Calculation of $20\frac{\partial f}{\partial V_{rc}}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    would coincide.'
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：$20\frac{\partial f}{\partial V_{rc}}\bigg{|}_{a=\boldsymbol{0}_{d},v_{c}=v\boldsymbol{1}_{d},V=kI}$
    的计算将一致。
- en: D.7.2 Empirical Validation
  id: totrans-1258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.7.2 实证验证
- en: 'Our model architecture is defined as follows: the number of layers $T$. The
    results are demonstrated in the third plot of [Figure 15](#A4.F15 "In D.6.2 Empirical
    Validation of Theorem 4 ‣ D.6 Empirical Validation of Theorem 3 and Theorem 4
    ‣ Appendix D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret?
    A Case Study in Online Learning and Games").'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型架构定义如下：层数为 $T$。结果展示在[图 15](#A4.F15 "在 D.6.2 定理 4 的实证验证 ‣ D.6 定理 3 和定理 4
    的实证验证 ‣ 附录 D 延迟结果和证明在第 5 节 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")的第三个图中。
- en: D.8 Training Details of [Section 5.4](#S5.SS4 "5.4 Experimental Results for
    Minimizing Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")
  id: totrans-1260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.8 [第 5.4 节](#S5.SS4 "5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失可证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
    的训练细节
- en: We provide the training details of [Section 5.4](#S5.SS4 "5.4 Experimental Results
    for Minimizing Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
    For the multi-layer Transformer training, we used 4 layers, 1 head Transformer.
    For both single-layer and multi-layer, we employed the Adam optimizer, setting
    the learning rate to 0.001\. During training, we conducted 2,000 epochs with a
    batch size 512\. Moreover, when we trained for the loss sequences with the predictable
    trend, we used 4 layers, 1 head Transformer. For both single-layer and multi-layer,
    we employed the Adam optimizer, setting the learning rate to 0.001\. During training,
    we conducted 9,000 epochs with a batch size of 512.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了[第 5.4 节](#S5.SS4 "5.4 最小化遗憾损失的实验结果 ‣ 5 通过无监督损失可证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
    的训练细节。对于多层 Transformer 训练，我们使用了 4 层、1 头 Transformer。对于单层和多层模型，我们使用了 Adam 优化器，将学习率设置为
    0.001。在训练过程中，我们进行了 2,000 次迭代，批量大小为 512。此外，当我们训练具有可预测趋势的损失序列时，我们使用了 4 层、1 头 Transformer。对于单层和多层模型，我们使用了
    Adam 优化器，将学习率设置为 0.001。在训练过程中，我们进行了 9,000 次迭代，批量大小为 512。
- en: 'D.9 Ablation Study on Training [Equation 3](#S5.E3 "In 5.1 A New Unsupervised
    Training Loss: Regret-Loss ‣ 5 Provably Promoting No-Regret Behavior by an Unsupervised
    Loss ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games")'
  id: totrans-1262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.9 对[方程 3](#S5.E3 "在 5.1 一种新的无监督训练损失：遗憾损失 ‣ 5 通过无监督损失可证明促进无遗憾行为 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")
    的消融研究
- en: In this section, we provide an ablation study that changes $N$), with the results
    in [Figure 19](#A4.F19 "In D.9 Ablation Study on Training Equation 3 ‣ Appendix
    D Deferred Results and Proofs in Section 5 ‣ Do LLM Agents Have Regret? A Case
    Study in Online Learning and Games") and [Figure 20](#A4.F20 "In D.9 Ablation
    Study on Training Equation 3 ‣ Appendix D Deferred Results and Proofs in Section
    5 ‣ Do LLM Agents Have Regret? A Case Study in Online Learning and Games").
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个消融研究，改变 $N$，结果见[图 19](#A4.F19 "在 D.9 对训练方程 3 的消融研究 ‣ 附录 D 延迟结果和证明在第
    5 节 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")和[图 20](#A4.F20 "在 D.9 对训练方程 3 的消融研究 ‣ 附录 D 延迟结果和证明在第
    5 节 ‣ LLM 代理是否有遗憾？在线学习和游戏中的案例研究")。
- en: '![Refer to caption](img/29e1e28a803a4b538d9fbee1ab75f114.png)'
  id: totrans-1264
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/29e1e28a803a4b538d9fbee1ab75f114.png)'
- en: 'Figure 17: Ablation study for the uniform loss sequence trained with single-layer
    self-attention layer and Softmax projection.'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：使用单层自注意力层和 Softmax 投影训练的均匀损失序列的消融研究。
- en: '![Refer to caption](img/81f98a938ccb3ae51367bcd5ae307e6f.png)'
  id: totrans-1266
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/81f98a938ccb3ae51367bcd5ae307e6f.png)'
- en: 'Figure 18: Ablation study for the uniform loss sequence trained with multi-layer
    self-attention layer and Softmax projection.'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：使用多层自注意力层和 Softmax 投影训练的均匀损失序列的消融研究。
- en: '![Refer to caption](img/995169a8bf6fc98adc6f2c3d1019bce2.png)'
  id: totrans-1268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/995169a8bf6fc98adc6f2c3d1019bce2.png)'
- en: 'Figure 19: Ablation study for the Gaussian loss sequence trained with single-layer
    self-attention layer and Softmax projection.'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：使用单层自注意力层和 Softmax 投影训练的高斯损失序列的消融研究。
- en: '![Refer to caption](img/4d93e662abcfd45b917400d3677a2068.png)'
  id: totrans-1270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4d93e662abcfd45b917400d3677a2068.png)'
- en: 'Figure 20: Ablation study for the Gaussian loss sequence trained with single-layer
    self-attention layer and Softmax projection.'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：对使用单层自注意力层和 Softmax 投影训练的高斯损失序列的消融研究。
