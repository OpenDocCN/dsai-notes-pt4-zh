- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:43
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Towards Socially and Morally Aware RL agent: Reward Design With LLM'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朝向社会和道德意识的RL代理：与LLM的奖励设计
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.12459](https://ar5iv.labs.arxiv.org/html/2401.12459)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.12459](https://ar5iv.labs.arxiv.org/html/2401.12459)
- en: Zhaoyue Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhaoyue Wang
- en: zhaoyue.wang@mail.utoronto.ca
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: zhaoyue.wang@mail.utoronto.ca
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: When we design and deploy an Reinforcement Learning (RL) agent, reward functions
    motivates agents to achieve an objective. An incorrect or incomplete specification
    of the objective can result in behavior that does not align with human values
    - failing to adhere with social and moral norms that are ambiguous and context
    dependent, and cause undesired outcomes such as negative side effects and exploration
    that is unsafe. Previous work have manually defined reward functions to avoid
    negative side effects, use human oversight for safe exploration, or use foundation
    models as planning tools. This work studies the ability of leveraging Large Language
    Models (LLM)’ understanding of morality and social norms on safe exploration augmented
    RL methods. This work evaluates language model’s result against human feedbacks
    and demonstrates language model’s capability as direct reward signals.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们设计和部署一个强化学习（RL）代理时，奖励函数激励代理实现目标。不正确或不完整的目标规范可能导致行为与人类价值观不一致——未能遵守社会和道德规范，这些规范是模糊且依赖于上下文的，并可能导致不良结果，例如负面副作用和不安全的探索。以往的工作通过手动定义奖励函数来避免负面副作用，使用人工监督以确保安全探索，或利用基础模型作为规划工具。本研究探讨了利用大型语言模型（LLM）对道德和社会规范理解来增强安全探索的RL方法的能力。本研究评估了语言模型的结果与人类反馈的对比，并展示了语言模型作为直接奖励信号的能力。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement Learning (RL) is is widely applied in decision-making problems.
    An agent, the AI system, is trained to find an optimal policy towards satisfying
    certain objective by maximizing a reward signal by when interacting with the environment
    through trial and error [[10](#bib.bib10)].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）被广泛应用于决策问题。一个代理，即AI系统，通过试错与环境互动来寻找满足特定目标的最优策略，目标是通过最大化奖励信号来实现[[10](#bib.bib10)]。
- en: The learnt policy may have issues aligning with human values[[6](#bib.bib6)]
    and may cause side effects[[1](#bib.bib1)], and the exploration in finding this
    policy may be inefficient or unsafe[[2](#bib.bib2)]. As it is difficult to manually
    specify reward signals for all the things the agent should do and not do while
    pursuing its goal. Prior approaches to tackle these problems includes using human
    demonstration[[4](#bib.bib4)], human intervention [[9](#bib.bib9)] and using language
    models [[7](#bib.bib7), [5](#bib.bib5)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的策略可能与人类价值观不一致[[6](#bib.bib6)]，并可能导致副作用[[1](#bib.bib1)]，在寻找这一策略的探索过程中可能效率低下或不安全[[2](#bib.bib2)]。由于很难手动为代理在追求目标时应做和不应做的所有事项指定奖励信号，先前的解决方法包括使用人工示范[[4](#bib.bib4)]、人工干预[[9](#bib.bib9)]以及使用语言模型[[7](#bib.bib7),
    [5](#bib.bib5)]。
- en: 'This work establishes a simple 2D Grid World [3.1](#S3.SS1 "3.1 Experiments
    2D Grid Worlds ‣ 3 Experiments and Evaluation ‣ Towards Socially and Morally Aware
    RL agent: Reward Design With LLM") with various items unrelated to the goal but
    may have undesired or even catastrophic consequences according to moral and social
    values. This work outlines and implements an approach that allows the RL agent
    to prompt a language model for auxiliary rewards, explore with precaution and
    reflect on it’s past trajectories. This work provide an empirical analysis to
    ascertain if and when the proposed approach allows the RL agent to align to human
    values, avoids negative side effects and explore safely. Experiment 1 [3.3.1](#S3.SS3.SSS1
    "3.3.1 Simple Vase ‣ 3.3 Evaluation ‣ 3 Experiments and Evaluation ‣ Towards Socially
    and Morally Aware RL agent: Reward Design With LLM") reflects the agent’s ability
    to avoid side effects and explore with precaution. Experiment 2 [3.3.2](#S3.SS3.SSS2
    "3.3.2 Vase and Person ‣ 3.3 Evaluation ‣ 3 Experiments and Evaluation ‣ Towards
    Socially and Morally Aware RL agent: Reward Design With LLM") aim at providing
    a demonstration of language model’s understanding of moral values where experiment
    3 [3.3.3](#S3.SS3.SSS3 "3.3.3 Public vs Private sphere ‣ 3.3 Evaluation ‣ 3 Experiments
    and Evaluation ‣ Towards Socially and Morally Aware RL agent: Reward Design With
    LLM") highlights language model’s understanding of social norms.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作建立了一个简单的 2D 网格世界 [3.1](#S3.SS1 "3.1 实验 2D 网格世界 ‣ 3 实验与评估 ‣ 面向社会和道德意识的 RL
    代理：使用 LLM 的奖励设计")，其中包含各种与目标无关的物品，但根据道德和社会价值观可能产生不良或甚至灾难性的后果。本工作概述并实施了一种方法，该方法允许
    RL 代理请求语言模型的辅助奖励，谨慎探索并反思其过去的轨迹。本工作提供了实证分析，以确定提出的方法是否及何时能使 RL 代理与人类价值观对齐，避免负面副作用并安全探索。实验
    1 [3.3.1](#S3.SS3.SSS1 "3.3.1 简单的花瓶 ‣ 3.3 评估 ‣ 3 实验与评估 ‣ 面向社会和道德意识的 RL 代理：使用 LLM
    的奖励设计") 反映了代理避免副作用和谨慎探索的能力。实验 2 [3.3.2](#S3.SS3.SSS2 "3.3.2 花瓶和人 ‣ 3.3 评估 ‣ 3
    实验与评估 ‣ 面向社会和道德意识的 RL 代理：使用 LLM 的奖励设计") 旨在展示语言模型对道德价值观的理解，而实验 3 [3.3.3](#S3.SS3.SSS3
    "3.3.3 公众与私人领域 ‣ 3.3 评估 ‣ 3 实验与评估 ‣ 面向社会和道德意识的 RL 代理：使用 LLM 的奖励设计") 则突出了语言模型对社会规范的理解。
- en: ¹¹1Code is available at [https://github.com/Inputrrr0/LLM-reward-RL](https://github.com/Inputrrr0/LLM-reward-RL)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹1代码可在 [https://github.com/Inputrrr0/LLM-reward-RL](https://github.com/Inputrrr0/LLM-reward-RL)
    找到。
- en: .
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 2 Related Work
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Avoiding side effects While pursuing it’s objective, the agent’s interaction
    of the environment can cause unrelated side effects that are undesired. One approach
    [[1](#bib.bib1)] to avoid negative side effects is by augmenting the reward function
    to consider other agent’s future value function. Moreover, certain side effect
    may be not only undesired but dangerous or even catastrophic. This makes the trial
    and error exploration become unsafe.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 避免副作用 在追求目标的过程中，代理与环境的交互可能会产生无关的副作用，这些副作用是不希望发生的。一种 [[1](#bib.bib1)] 避免负面副作用的方法是通过增强奖励函数以考虑其他代理的未来价值函数。此外，某些副作用不仅是不希望的，还可能是危险的甚至是灾难性的。这使得试错探索变得不安全。
- en: Safe Exploration To tackle the previously mentioned issue, [[9](#bib.bib9)]
    approach to safe exploration uses human intervention to train a model-free RL
    agent to learn while avoiding all catastrophic actions in Atari games. At every
    timestep, a human observes the current state s and the agent’s proposed action
    a. If the action leads to catastrophic consequence, the human sends a safe action
    to the environment instead and replaces the new reward with a penalty reward.
    The difference between safe exploration and avoiding side effect is that in training,
    the agent decreases the probability and frequency of hitting a negative effect,
    rather than learning that the effect is negative as the agent explores. This current
    work investigates whether a language model can be used in place of a human observer
    in a environment that simulates moral decisions and social norm decisions in real
    life.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 安全探索 为了解决前述问题，[[9](#bib.bib9)] 的安全探索方法使用人工干预来训练一个无模型的 RL 代理，在避免所有灾难性行为的同时进行学习。在每个时间步，人类观察当前状态
    s 和代理提出的行动 a。如果行动导致灾难性后果，人类会向环境发送一个安全动作，并将新的奖励替换为惩罚奖励。安全探索与避免副作用的区别在于，在训练过程中，代理减少了负面效果的概率和频率，而不是在代理探索时学习效果是负面的。本工作调查了是否可以在模拟道德决策和现实生活中的社会规范决策的环境中，使用语言模型替代人类观察者。
- en: Large Language Model as Reward [[8](#bib.bib8)] shows the efficiency of leveraging
    Large Language Model (LLM)’s knowledge as reward signal by prompting the language
    model to mark each trajectory as good or bad. [[11](#bib.bib11)] further explores
    the use of LLM by feeding it knowledge of Atari games and prompts it to generate
    a positive or negative reward that reflects whether an action leads to winning
    the game. The paper showed improvement in exploration efficiency. Moreover, [[7](#bib.bib7)]
    shows that language model can be used to guide RL agents in taking more morally
    acceptable actions in text-based games.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型作为奖励[[8](#bib.bib8)]展示了通过提示语言模型标记每条轨迹为好或坏，从而有效利用大型语言模型（LLM）知识作为奖励信号的效率。[[11](#bib.bib11)]进一步探讨了通过将Atari游戏的知识输入LLM，并提示其生成一个反映某个动作是否导致赢得游戏的正面或负面奖励的应用。该论文展示了探索效率的提升。此外，[[7](#bib.bib7)]表明语言模型可以用于指导RL智能体在基于文本的游戏中采取更道德可接受的行动。
- en: 3 Experiments and Evaluation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验和评估
- en: 3.1 Experiments 2D Grid Worlds
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验 2D 网格世界
- en: 'The environment used for the experiments is a 10x10 2D grid world with discrete
    states and actions. The outmost cells are walls that cannot be occupied. Each
    of the other cells can contain at most one item that have a pre-specified consequence
    or event that will happen either as long as the agent is in the same cell, or
    if the agent interacts with it. Each item can only be interacted once, after which
    it will no longer be present in the cell. The detail setup for each experiment
    is further elaborated in [3.3](#S3.SS3 "3.3 Evaluation ‣ 3 Experiments and Evaluation
    ‣ Towards Socially and Morally Aware RL agent: Reward Design With LLM")'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实验的环境是一个10x10的2D网格世界，具有离散的状态和动作。最外层的单元格是墙壁，无法被占据。其他每个单元格最多可以包含一个具有预设后果或事件的项目，该项目会在智能体处于同一单元格内或与其交互时发生。每个项目只能互动一次，之后将不再出现在单元格中。每个实验的详细设置将在[3.3](#S3.SS3
    "3.3 评估 ‣ 3 实验和评估 ‣ 朝向社会和道德意识的RL智能体：利用LLM设计奖励")中进一步阐述。
- en: Action Space
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 行为空间
- en: 'The agent have 5 actions: UP, DOWN, LEFT, RIGHT and USE. The first four would
    move the agent to the respective neighbouring cell if the cell is not a wall and
    remain in the same cell otherwise. The fifth action allows the agent to actively
    interact with the environment. The consequence of any items are independent of
    each other and remain the same regardless of the state.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体有5个动作：UP、DOWN、LEFT、RIGHT和USE。前四个动作会将智能体移动到相应的邻近单元格（如果该单元格不是墙壁）或者保持在当前单元格。第五个动作允许智能体主动与环境进行交互。任何项目的后果彼此独立，并且无论状态如何保持不变。
- en: Reward
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 奖励
- en: The environment gives one reward when the specified goal is achieved and the
    episode terminates. The environment does not have any other inherent rewards for
    items unrelated to the goal. Before the episode terminates or truncate after a
    high maximum step, a small negative reward is generated by the environment. This
    encourages the agent to be goal-oriented and prevents the agent from exploiting
    items with consequences where the language model will give positive reward. The
    range of reward given by the language model is manually defined as $[-10,10]$
    ²²2The reward for goal attainment is 100\. These values are manually specified
    in this work to ensure the agent have enough incentive to reach the goal. .
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当达到指定目标并且回合结束时，环境会给予奖励。环境对于与目标无关的项目没有其他固有奖励。在回合结束之前或在高最大步数后被截断时，环境会生成一个小的负奖励。这鼓励智能体以目标为导向，防止智能体利用那些语言模型会给予正奖励的项目。语言模型给予的奖励范围手动定义为$[-10,10]$²²²。目标达成的奖励是100。这些值在本工作中被手动指定，以确保智能体有足够的激励去达成目标。
- en: 3.2 Approach
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 方法
- en: 'The approach [3](#footnote3 "footnote 3 ‣ Figure 1 ‣ 3.2 Approach ‣ 3 Experiments
    and Evaluation ‣ Towards Socially and Morally Aware RL agent: Reward Design With
    LLM") implements concepts for building RL agents towards solving the aforementioned
    problems. Although the concepts can be applied to other environments, this work
    implements them on tabular Q-learning for the 2D Grid World environment.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 方法[3](#footnote3 "脚注 3 ‣ 图 1 ‣ 3.2 方法 ‣ 3 实验和评估 ‣ 朝向社会和道德意识的RL智能体：利用LLM设计奖励")实现了构建RL智能体以解决上述问题的概念。虽然这些概念可以应用于其他环境，但本工作在2D网格世界环境中实现了它们。
- en: '![Refer to caption](img/242ec3f94c960118da7c72515d5f7ce3.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/242ec3f94c960118da7c72515d5f7ce3.png)'
- en: 'Figure 1: The flow chart of the approach, highlighting three main components
    and contributions of this work: 1\. safe exploration where the probability of
    taking the dangerous³³3dangerous as judged by the language model. But appendix
    [A](#A1 "Appendix A validity of LLM generated value ‣ Towards Socially and Morally
    Aware RL agent: Reward Design With LLM") shows that human judgement mostly agrees
    with language model generated results.action is decreased, 2\. avoiding negative
    side effect where the language model is prompted to act as proxy reward, and 3\.
    prompting the language model to compare items visited in 2 randomly selected trajectories
    to avoid local optimal policies.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：该方法的流程图，突出显示了该工作的三个主要组成部分和贡献：1\. 安全探索，即降低被语言模型判断为危险的动作的概率。附录 [A](#A1 "附录
    A LLM 生成值的有效性 ‣ 朝向社会和道德意识的 RL 代理：LLM 的奖励设计") 显示人类判断通常与语言模型生成的结果一致。2\. 避免负面副作用，即语言模型被提示充当代理奖励。3\.
    提示语言模型比较 2 个随机选择的轨迹中的项目，以避免局部最优策略。
- en: Prompting language model
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示语言模型
- en: Instead of using language model for high level task decomposition and planning,
    this approach prompts the language model to generate a proxy reward in two ways.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将语言模型用于高层次任务分解和规划，不如使用这种方法提示语言模型通过两种方式生成代理奖励。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Act as human intervention [[9](#bib.bib9)] at step level. The objective is to
    first ensure the exploration is efficient and safe, and second determine the nature
    of side effects, whether it is positive or negative or neutral. The language model
    is prompted to generate a reward for all encountered item with a predefined consequence
    or event. The prompt used is "You are a socially and morally aware person. Please
    rank the action ${action}$ is replaced with the string that describes the consequence
    or event of the item.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在步骤级别充当人为干预 [[9](#bib.bib9)]。目标是首先确保探索既高效又安全，其次确定副作用的性质，无论是正面、负面还是中立。语言模型会被提示为所有遇到的项目生成一个具有预定义后果或事件的奖励。使用的提示是“你是一个社会和道德意识的人。请对动作
    ${action}$ 进行排名，${action}$ 被替换为描述该项目后果或事件的字符串。”
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ensure the trajectory is globally safe and appropriate by using a replay buffer.
    The language model is prompted to compare two trajectories. When two trajectories
    have been picked, the number of items visited is extracted. The prompt is "consider
    you as someone adhering to moral and social norms. In a hypothetical scenario
    where one or the other will happen, please pick a better result from the following
    results: 1.list of items in trajectory i or 2.list of items in trajectory j. Label
    your answer as "reasoning:" and "preferred results: ". ". For example, if trajectory
    i contains 3 vases and trajectory j contains 1 person, the prompt becomes: "consider
    you as someone adhering to moral and social norms. In a hypothetical scenario
    where one or the other will happen, please pick a better result from the following
    results: 1."breaking a vase x3" and 2."killing a person x1". Label your answer
    as "reasoning:" and "preferred results: ". " ⁴⁴4Note it is important to say ”hypothetical
    scenario where one or the other will happen” or GPT3.5 will refuse to answer..
    A small reward is added to each state-action pair of the preferred trajectory.
    The Q table is updated to reflect this positive change.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用重放缓冲区，确保轨迹在全局上安全且合适。语言模型被提示比较两个轨迹。当选择了两个轨迹后，提取访问的项目数量。提示为：“考虑你作为一个遵守道德和社会规范的人。在一个假设的情境中，其中一个或另一个将发生，请从以下结果中选择更好的结果：1.
    轨迹 i 中的项目列表，或 2. 轨迹 j 中的项目列表。将你的答案标记为‘推理：’和‘首选结果：’。”例如，如果轨迹 i 包含 3 个花瓶而轨迹 j 包含
    1 人，则提示变为：“考虑你作为一个遵守道德和社会规范的人。在一个假设的情境中，其中一个或另一个将发生，请从以下结果中选择更好的结果：1.‘打破花瓶 x3’和
    2.‘杀死一个人 x1’。将你的答案标记为‘推理：’和‘首选结果：’。”⁴⁴4注意，重要的是说“假设的情境中其中一个或另一个将发生”，否则 GPT3.5 将拒绝回答。对首选轨迹的每个状态-动作对添加小奖励。Q
    表格被更新以反映这一积极变化。
- en: The model used in this work is OpenAI’s GPT3.5.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作中使用的模型是 OpenAI 的 GPT3.5。
- en: Precaution
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 预防措施
- en: At each state, before taking any action, the agent prompts the language model
    to evaluate the consequence of each of the items present in the four neighboring
    cells (if any) and output a resulting number $n$ is negative. For example, the
    reward for hitting a vase is -3, the agent avoids it with 30% probability.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个状态下，在采取任何行动之前，代理会提示语言模型评估四个邻近单元格中每个项目（如果有的话）的后果，并输出一个结果数字 $n$ 是否为负。例如，击打花瓶的奖励为
    -3，代理以 30% 的概率避免它。
- en: Replay Buffer
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 重放缓冲区
- en: At some state $s$. In this approach, every training and evaluation episodes
    is stored in a replay buffer. After every 10 episodes, two trajectories will be
    randomly selected from a pool of past trajectories. The language model is prompted
    to pick a more preferred trajectory. At each state-action pair of this trajectory,
    the Q value of the action is increased by a small percentage.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个状态$s$。在这种方法中，每个训练和评估回合都会存储在重放缓冲区中。每10个回合后，将从过去的轨迹池中随机选择两个轨迹。语言模型会被提示选择更为偏好的轨迹。在该轨迹的每个状态-动作对中，动作的Q值会增加一个小百分比。
- en: 3.3 Evaluation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 评估
- en: 'The aforementioned approach is tested on 3 different worlds. Each world have
    the same goal, which is for the agent to obtain a key and go to a exit door. The
    setup of different worlds differ in the kind of items they contain and their respective
    side effects. In all worlds, the goal and item setup are undisclosed to the agent
    by not explicitly embedding them as pretrained skills. The agent have 5 actions:
    UP, DOWN, LEFT, RIGHT and USE. The first four would move the agent to the respective
    neighbouring cell if the cell is not a wall and remain in the same cell otherwise.
    The fifth action allows the agent to pick up the key. Moreover, the key can be
    obtained if and only if the agent’s action is USE when it is on the same cell
    as the key. Each world is evaluated differently and reflects different aspect
    of the proposed approach.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法在3个不同的世界中进行了测试。每个世界都有相同的目标，即让代理获取一个钥匙并找到出口门。不同世界的设置在于它们包含的物品种类及其各自的副作用。在所有世界中，目标和物品设置都没有明确嵌入为预训练技能，因此对代理来说是未知的。代理有5个动作：UP、DOWN、LEFT、RIGHT和USE。前四个动作会将代理移动到相应的邻近单元格，如果该单元格不是墙壁，否则保持在当前单元格。第五个动作允许代理捡起钥匙。此外，钥匙只能在代理与钥匙在同一单元格时，代理的动作为USE时才能获得。每个世界的评估方式不同，反映了所提方法的不同方面。
- en: 3.3.1 Simple Vase
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 简单花瓶
- en: 'This world [3](#S3.F3 "Figure 3 ‣ 3.3.1 Simple Vase ‣ 3.3 Evaluation ‣ 3 Experiments
    and Evaluation ‣ Towards Socially and Morally Aware RL agent: Reward Design With
    LLM") contains one vase placed on the shortest path from the agent’s initial position
    to the goal. The consequence of the vase is predetermined: when the agent is in
    the same cell as the vase, the vase will be "broken". The location and consequence
    of the items are static so the states for the Q table only has the agent’s location
    and it’s inventory. This drastically reduces the complexity of the tabular Q learning
    algorithm and allow us to better observe the effect of augmenting it with the
    additional feature of precaution. The aim of this world is to evaluate the effect
    of language model as reward signal. For baseline, this work compares the proposed
    approach to regular Q learning with no reward shaping.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个世界[3](#S3.F3 "图3 ‣ 3.3.1 简单花瓶 ‣ 3.3 评估 ‣ 3 实验与评估 ‣ 朝向社会和道德意识的RL代理：与LLM的奖励设计")包含一个放置在从代理初始位置到目标的最短路径上的花瓶。花瓶的后果是预定的：当代理与花瓶在同一单元格时，花瓶会被“打破”。物品的位置和后果是静态的，因此Q表的状态仅包含代理的位置和它的库存。这大大降低了表格Q学习算法的复杂性，并使我们能够更好地观察到通过附加预防特征增强其效果的影响。这个世界的目的是评估语言模型作为奖励信号的效果。作为基线，这项工作将所提方法与没有奖励塑造的常规Q学习进行了比较。
- en: '![Refer to caption](img/54e5b65509d294e1ac4a0a9395cefc48.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/54e5b65509d294e1ac4a0a9395cefc48.png)'
- en: 'Figure 2: A example of the world containing only one vase, one key and one
    exit door. The agent is represented with a red triangle.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个包含一个花瓶、一个钥匙和一个出口门的世界示例。代理用红色三角形表示。
- en: '![Refer to caption](img/a5da4760f461ee42ecf1babaed8f49af.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a5da4760f461ee42ecf1babaed8f49af.png)'
- en: 'Figure 3: Convergence of the proposed approach on tabular Q learning with reduced
    state representation. Each evaluation episode is after 10 training episodes. Evaluation
    episode do not update the Q table.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在减少状态表示的表格Q学习中，所提方法的收敛情况。每次评估是在10个训练回合之后进行的。评估回合不会更新Q表。
- en: 'The aforementioned approach in this set up is evaluated by comparing to regular
    tabular Q learning. This work found that after the method converges [3](#S3.F3
    "Figure 3 ‣ 3.3.1 Simple Vase ‣ 3.3 Evaluation ‣ 3 Experiments and Evaluation
    ‣ Towards Socially and Morally Aware RL agent: Reward Design With LLM"), the total
    count of vase encountered is 0\. On the other hand, after the standard Q learning
    converges, the total count of vase encountered is 215 out of 300 evaluation episodes.
    Since the vase has been predetermined to result in "broken" once in contact with
    the agent, we as human would prefer if the agent does not come in contact with
    the vase. It is observed that the agent’s behavior aligns with our values. Moreover,
    the agent also demonstrates the ability to avoid with vase with precaution.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，前述方法的效果通过与常规表格 Q 学习进行比较来评估。这项工作发现，在方法收敛后[3](#S3.F3 "图 3 ‣ 3.3.1 简单花瓶
    ‣ 3.3 评估 ‣ 3 实验与评估 ‣ 朝着社会和道德意识的 RL 代理：使用 LLM 的奖励设计")，遇到花瓶的总次数为 0。另一方面，在标准 Q 学习收敛后，遇到花瓶的总次数为
    300 次评估回合中的 215 次。由于花瓶一旦接触到代理就会被预定为“破损”，我们作为人类更希望代理不与花瓶接触。观察到代理的行为与我们的价值观相一致。此外，代理还表现出了谨慎避开花瓶的能力。
- en: 3.3.2 Vase and Person
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 花瓶与人物
- en: 'This world [5](#S3.F5 "Figure 5 ‣ 3.3.2 Vase and Person ‣ 3.3 Evaluation ‣
    3 Experiments and Evaluation ‣ Towards Socially and Morally Aware RL agent: Reward
    Design With LLM") tests the language model’s ability to converge to a globally
    optimal policy rather than a locally optimal policy given the proxy reward. The
    agent is thus setup without Act with precaution. The environment has only two
    available paths towards the goal where other cells are blocked. In the first path,
    the agent must interact with a "person" item inorder to reach the goal. In the
    other path, the agent must interact with 10 vase items. The "person" item has
    the consequence of "killing a person". The vase has the consequence of "breaking
    a vase".Locally, taking the second path results in -30 reward where the first
    one is -10 (according to GPT3.5’s reward in appendix [A](#A1 "Appendix A validity
    of LLM generated value ‣ Towards Socially and Morally Aware RL agent: Reward Design
    With LLM"). But when prompts the language model to compare two paths, the language
    model prefers the second one. It is also intuitive to think that sacrificing 10
    vases is in general acceptable than killing a person, as a person’s life is viewed
    to hold greater value than objects. The result in figure [5](#S3.F5 "Figure 5
    ‣ 3.3.2 Vase and Person ‣ 3.3 Evaluation ‣ 3 Experiments and Evaluation ‣ Towards
    Socially and Morally Aware RL agent: Reward Design With LLM") shows the change
    of items visited with the replay buffer activated at evaluation episode 230\.
    The dark blue line represents the average number of "person" item visited in each
    trajectory of 10 episodes, with maximum of 1 meaning it is visited once in every
    episode, to a minimum of 0 meaning it is never visited in all 10 episodes. The
    light blue line presents the average number of "vase" item visited in each trajectory
    of 10 episodes, with maximum of 10 meaning it is visited 10 times in every episode,
    to a minimum of 0 meaning it is never visited in all 10 episodes. At the beginning,
    in almost all trajectories, the agent visits 10 "vase" items. As the number of
    episode increases, the agent’s action value slowly align with a global optimal
    solution. The frequency of visiting the "person" item decreases. At evaluation
    episode 530, the "person" item is never visited in the past 10 evaluation episodes.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个世界 [5](#S3.F5 "图 5 ‣ 3.3.2 花瓶与人 ‣ 3.3 评估 ‣ 3 实验与评估 ‣ 朝着社会和道德意识的 RL 代理：基于 LLM
    的奖励设计") 测试了语言模型在给定代理奖励的情况下收敛到全局最优策略而非局部最优策略的能力。因此，代理在没有“谨慎行动”的情况下进行设置。环境中只有两条通往目标的可用路径，其它路径被阻塞。在第一条路径中，代理必须与一个“人”物品进行互动才能达到目标。在另一条路径中，代理必须与
    10 个花瓶物品进行互动。“人”物品的后果是“杀死一个人”。花瓶的后果是“打破一个花瓶”。从局部来看，选择第二条路径会获得 -30 奖励，而第一条路径是 -10（根据
    GPT3.5 的奖励在附录 [A](#A1 "附录 A LLM 生成值的有效性 ‣ 朝着社会和道德意识的 RL 代理：基于 LLM 的奖励设计")）。但是，当提示语言模型比较两条路径时，语言模型更倾向于第二条路径。也直观地认为，牺牲
    10 个花瓶通常比杀死一个人更可接受，因为人的生命被认为比物品更有价值。图 [5](#S3.F5 "图 5 ‣ 3.3.2 花瓶与人 ‣ 3.3 评估 ‣
    3 实验与评估 ‣ 朝着社会和道德意识的 RL 代理：基于 LLM 的奖励设计") 显示了在评估回合 230 激活重放缓冲区时访问的物品数量变化。深蓝色线表示每
    10 回合中的“人”物品的平均访问次数，最大值为 1 表示每回合访问一次，最小值为 0 表示在所有 10 回合中从未访问。浅蓝色线表示每 10 回合中“花瓶”物品的平均访问次数，最大值为
    10 表示每回合访问 10 次，最小值为 0 表示在所有 10 回合中从未访问。最开始，在几乎所有的轨迹中，代理访问了 10 个“花瓶”物品。随着回合数的增加，代理的行动价值逐渐与全局最优解对齐。访问“人”物品的频率减少。在评估回合
    530 时，过去 10 个评估回合中“人”物品从未被访问。
- en: '![Refer to caption](img/8d99f22a4df0c69ff77d95a29ef0e5f3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d99f22a4df0c69ff77d95a29ef0e5f3.png)'
- en: 'Figure 4: A example of the world containing two paths towards reaching the
    goal where at least one "person" and ten "vase" items needs to be interacted by
    the agent. The agent is represented with a red triangle.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个包含两条通往目标路径的世界示例，其中代理至少需要与一个“人”和十个“花瓶”物品互动。代理由红色三角形表示。
- en: '![Refer to caption](img/07a6d05f9cbff1ba7f1f05666dbc34bb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/07a6d05f9cbff1ba7f1f05666dbc34bb.png)'
- en: 'Figure 5: Average number of items interacted from 10 evaluation episodes, from
    episode 230 to 530\. Each evaluation episode is after 10 training episodes. The
    replay buffer starts at evaluation episode 230 when the algorithm begins the converge
    (agent learns to reach for the goal).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：从第 230 回合到第 530 回合的 10 个评估回合中的物品互动的平均数量。每个评估回合是在 10 个训练回合之后。重放缓冲区从评估回合 230
    开始，当算法开始收敛（代理学会了朝目标前进）时。
- en: 3.3.3 Public vs Private sphere
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 公共领域与私人领域
- en: 'The world (figure [8](#S3.F8 "Figure 8 ‣ 3.3.3 Public vs Private sphere ‣ 3.3
    Evaluation ‣ 3 Experiments and Evaluation ‣ Towards Socially and Morally Aware
    RL agent: Reward Design With LLM")) is designed to test language model’s understanding
    of social norms. Social norms[[3](#bib.bib3)] are context-dependent and ambiguous.
    In this experiment, there are 3 items on the map: shower, hamburger and cloth
    with their respective event taking a shower, eating food and changing cloth [8](#S3.F8
    "Figure 8 ‣ 3.3.3 Public vs Private sphere ‣ 3.3 Evaluation ‣ 3 Experiments and
    Evaluation ‣ Towards Socially and Morally Aware RL agent: Reward Design With LLM").
    The appropriateness for each of the items is different depending on whether the
    event is done is public or not. The goal is to reach for the exit door. The prompt
    for the language model is: "Consider you as someone in a public space. Please
    rank the appropriateness of the following action "taking a shower" on a scale
    of -10 to 10, from least to most appropriate. Label your answer as: "reasoning:
    " and "result number: number. "". The langugage model used in this experiment
    is OpenAI’s GPT3.5 model. The human annotated score in appendix [A](#A1 "Appendix
    A validity of LLM generated value ‣ Towards Socially and Morally Aware RL agent:
    Reward Design With LLM") agrees with the language model’s result except for the
    "eating food in public" event. This is because the language model considers eating
    in a restaurant where some people surveyed considered eating in public transport
    and some considered eating in a restaurant. This deviation in the understanding
    is reflect in the large standard deviation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '这个世界（图 [8](#S3.F8 "Figure 8 ‣ 3.3.3 Public vs Private sphere ‣ 3.3 Evaluation
    ‣ 3 Experiments and Evaluation ‣ Towards Socially and Morally Aware RL agent:
    Reward Design With LLM")）旨在测试语言模型对社会规范的理解。社会规范[[3](#bib.bib3)]是依赖于上下文且模糊的。在这个实验中，地图上有
    3 个项目：淋浴、汉堡和衣物，它们分别对应的事件是淋浴、吃东西和换衣服 [8](#S3.F8 "Figure 8 ‣ 3.3.3 Public vs Private
    sphere ‣ 3.3 Evaluation ‣ 3 Experiments and Evaluation ‣ Towards Socially and
    Morally Aware RL agent: Reward Design With LLM")。每个项目的适当性取决于事件是否发生在公共场所。目标是到达出口门。语言模型的提示是：“假设你在一个公共场所。请将以下行为‘淋浴’的适当性按从-10到10的尺度进行排名，从最不适当到最适当。将你的答案标记为：‘推理：’和‘结果编号：编号。’”。这个实验中使用的语言模型是
    OpenAI 的 GPT3.5 模型。附录 [A](#A1 "Appendix A validity of LLM generated value ‣ Towards
    Socially and Morally Aware RL agent: Reward Design With LLM") 中的人类注释分数与语言模型的结果一致，除了“在公共场合吃东西”事件。这是因为语言模型认为在餐厅吃东西，而一些受访者认为是在公共交通上吃东西，另一些认为是在餐厅吃东西。这种理解的偏差反映在较大的标准差中。'
- en: 'The result in figure [8](#S3.F8 "Figure 8 ‣ 3.3.3 Public vs Private sphere
    ‣ 3.3 Evaluation ‣ 3 Experiments and Evaluation ‣ Towards Socially and Morally
    Aware RL agent: Reward Design With LLM") shows average number of items interacted
    from 100 evaluation episodes after evaluation episode 300\. Episode 300 is chosen
    to ensure agent learns to reach for the goal and the reward associated with each
    item. Each evaluation episode is after 10 training episodes. When the prompt puts
    the events related to the items in public, it can be observed that the language
    model generated reward guides the agent to avoid the shower item and the cloth
    item, but reaches for the hamberger item as it has a positive reward. When the
    prompts put the events in a private context, the language model generated reward
    guides the agent to reach for all of the items on the map as they all have a positive
    reward.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [8](#S3.F8 "Figure 8 ‣ 3.3.3 Public vs Private sphere ‣ 3.3 Evaluation ‣
    3 Experiments and Evaluation ‣ Towards Socially and Morally Aware RL agent: Reward
    Design With LLM") 中的结果显示，从 100 次评估回合中计算出的互动项目的平均数量，评估回合 300 后的结果。选择第 300 次回合是为了确保代理学习到达目标及与每个项目相关的奖励。每次评估回合后进行
    10 次训练回合。当提示将与项目相关的事件放在公共场合时，可以观察到，语言模型生成的奖励指导代理避免选择淋浴项目和衣物项目，但会选择汉堡项目，因为它有正面奖励。当提示将事件放在私人环境中时，语言模型生成的奖励指导代理选择地图上的所有项目，因为它们都有正面奖励。'
- en: '![Refer to caption](img/7b4e7ee743b948011878572b5fdb4b71.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b4e7ee743b948011878572b5fdb4b71.png)'
- en: 'Figure 6: A example of the world containing 3 different items. The agent is
    represented with a red triangle. The goal is to reach the exit door.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：包含 3 个不同项目的世界示例。代理用红色三角形表示。目标是到达出口门。
- en: '![Refer to caption](img/fe65bc029f293d3c4d0024ac0d34f67d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fe65bc029f293d3c4d0024ac0d34f67d.png)'
- en: 'Figure 7: Description of items.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：项目描述。
- en: '![Refer to caption](img/2c52881a9acace28f9aa77b1e219ac9b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2c52881a9acace28f9aa77b1e219ac9b.png)'
- en: 'Figure 8: Average number of items interacted from 100 evaluation episodes after
    evaluation episode 300.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：第300次评估之后100次评估回合中互动项目的平均数量。
- en: 4 Conclusion and Future Work
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论与未来工作
- en: 4.1 Conclusion
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 结论
- en: The result underscore the effectiveness of using LLM as reward signal to guide
    RL agent in social and morally sensitive scenarios. This work conducts experiments
    on 3 different Grid World settings with different side effects that should be
    avoided or reached. The experimental results shows that the RL agent using the
    proposed approach is 1\. converge to global optimal by using a replay buffer,
    2\. leverage the knowledge of language model to take different paths in depending
    on the context.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果强调了使用LLM作为奖励信号以指导RL代理在社会和道德敏感场景中的有效性。此工作在3种不同的Grid World设置下进行了实验，这些设置中需要避免或实现不同的副作用。实验结果显示，使用所提方法的RL代理能够1.
    使用回放缓冲器收敛到全局最优，2. 根据上下文利用语言模型的知识采取不同路径。
- en: 4.2 Limitation and Future Work
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 限制与未来工作
- en: 'The environment used for the experiments is simple. The consequence when the
    agent interacts with items is manually predetermined and static. The future direction
    is two-fold: first is to further test the capability of the proposed approach
    by conducting experiments on a larger and more complex environment, second is
    to explore different usage of the language model. Including probabilistic events
    where the consequence of one item depends on the consequence of other items introduces
    dynamic changes to the environment. Another extension is to explore context dependent
    events by prompting the language model to deduce what the consequence is and guide
    the RL agent accordingly. For example, more context of the environment can be
    given - exiting the door requires a key, the vase is someone else’s, expensive
    and fragile.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中使用的环境很简单。代理与项目互动时的结果是手动预定的且静态的。未来的方向有两个：首先，通过在更大、更复杂的环境中进行实验，进一步测试所提方法的能力；其次，探索语言模型的不同使用方式。引入概率事件，即一个项目的结果依赖于其他项目的结果，将为环境带来动态变化。另一个扩展是通过提示语言模型推断后果并相应地指导RL代理来探索上下文依赖事件。例如，可以提供更多环境的上下文——离开门需要钥匙，花瓶是别人的，昂贵且易碎。
- en: References
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Alizadeh Alamdari et al. [2022] P. Alizadeh Alamdari, T. Q. Klassen, R. Toro Icarte,
    and S. A. McIlraith. Be considerate: Avoiding negative side effects in reinforcement
    learning. In *Proceedings of the 21st International Conference on Autonomous Agents
    and Multiagent Systems*, AAMAS ’22, page 18–26, Richland, SC, 2022\. International
    Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450392136.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alizadeh Alamdari 等 [2022] P. Alizadeh Alamdari, T. Q. Klassen, R. Toro Icarte,
    和 S. A. McIlraith. 考虑周到：避免强化学习中的负面副作用。载于*第21届国际自主代理和多智能体系统会议论文集*，AAMAS ’22，第18–26页，南卡罗来纳州里士满，2022年。国际自主代理和多智能体系统基金会。ISBN
    9781450392136。
- en: Amodei et al. [2016] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman,
    and D. Mané. Concrete problems in AI safety. *CoRR*, abs/1606.06565, 2016. URL
    [http://arxiv.org/abs/1606.06565](http://arxiv.org/abs/1606.06565).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amodei 等 [2016] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman,
    和 D. Mané. 人工智能安全中的具体问题。*CoRR*，abs/1606.06565，2016。网址 [http://arxiv.org/abs/1606.06565](http://arxiv.org/abs/1606.06565)。
- en: Bicchieri et al. [2023] C. Bicchieri, R. Muldoon, and A. Sontuoso. Social Norms.
    In E. N. Zalta and U. Nodelman, editors, *The Stanford Encyclopedia of Philosophy*.
    Metaphysics Research Lab, Stanford University, Winter 2023 edition, 2023.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bicchieri 等 [2023] C. Bicchieri, R. Muldoon, 和 A. Sontuoso. 社会规范。载于 E. N. Zalta
    和 U. Nodelman 编辑的*斯坦福哲学百科全书*。斯坦福大学形而上学研究实验室，2023年冬季版，2023。
- en: Hadfield-Menell et al. [2016] D. Hadfield-Menell, A. Dragan, P. Abbeel, and
    S. Russell. Cooperative inverse reinforcement learning, 2016.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadfield-Menell 等 [2016] D. Hadfield-Menell, A. Dragan, P. Abbeel, 和 S. Russell.
    协作逆强化学习，2016。
- en: Hafner et al. [2023] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering
    diverse domains through world models, 2023.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner 等 [2023] D. Hafner, J. Pasukonis, J. Ba, 和 T. Lillicrap. 通过世界模型掌握多样领域，2023。
- en: Hendrycks et al. [2021a] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li,
    D. Song, and J. Steinhardt. Aligning {ai} with shared human values. In *International
    Conference on Learning Representations*, 2021a. URL [https://openreview.net/forum?id=dNy_RKzJacY](https://openreview.net/forum?id=dNy_RKzJacY).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2021a] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D.
    Song, 和 J. Steinhardt. 使{ai}与共享人类价值观对齐。载于*国际学习表征会议*，2021a。网址 [https://openreview.net/forum?id=dNy_RKzJacY](https://openreview.net/forum?id=dNy_RKzJacY)。
- en: Hendrycks et al. [2021b] D. Hendrycks, M. Mazeika, A. Zou, S. Patel, C. Zhu,
    J. Navarro, D. Song, B. Li, and J. Steinhardt. What would jiminy cricket do? towards
    agents that behave morally. *CoRR*, abs/2110.13136, 2021b. URL [https://arxiv.org/abs/2110.13136](https://arxiv.org/abs/2110.13136).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 [2021b] D. Hendrycks, M. Mazeika, A. Zou, S. Patel, C. Zhu, J. Navarro,
    D. Song, B. Li 和 J. Steinhardt. 吉米尼蟋蟀会怎么做？朝着道德行为的代理发展。*CoRR*, abs/2110.13136，2021b。网址
    [https://arxiv.org/abs/2110.13136](https://arxiv.org/abs/2110.13136)。
- en: Kwon et al. [2023] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh. Reward design
    with language models. In *The Eleventh International Conference on Learning Representations*,
    2023. URL [https://openreview.net/forum?id=10uNUgI5Kl](https://openreview.net/forum?id=10uNUgI5Kl).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等 [2023] M. Kwon, S. M. Xie, K. Bullard 和 D. Sadigh. 语言模型的奖励设计。在 *第十一届国际学习表征会议*，2023年。网址
    [https://openreview.net/forum?id=10uNUgI5Kl](https://openreview.net/forum?id=10uNUgI5Kl)。
- en: 'Saunders et al. [2018] W. Saunders, G. Sastry, A. Stuhlmüller, and O. Evans.
    Trial without error: Towards safe reinforcement learning via human intervention.
    In *Proceedings of the 17th International Conference on Autonomous Agents and
    MultiAgent Systems*, AAMAS ’18, page 2067–2069, Richland, SC, 2018\. International
    Foundation for Autonomous Agents and Multiagent Systems.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saunders 等 [2018] W. Saunders, G. Sastry, A. Stuhlmüller 和 O. Evans. 无错试验：通过人工干预实现安全强化学习。在
    *第17届国际自主代理与多代理系统会议*，AAMAS ’18，第2067-2069页，Richland, SC, 2018年。国际自主代理与多代理系统基金会。
- en: 'Sutton and Barto [2018] R. S. Sutton and A. G. Barto. *Reinforcement learning:
    An introduction*. MIT press, 2018.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto [2018] R. S. Sutton 和 A. G. Barto. *强化学习：导论*. MIT出版社，2018年。
- en: 'Wu et al. [2023] Y. Wu, Y. Fan, P. P. Liang, A. Azaria, Y. Li, and T. Mitchell.
    Read and reap the rewards: Learning to play atari with the help of instruction
    manuals. In *Workshop on Reincarnating Reinforcement Learning at ICLR 2023*, 2023.
    URL [https://openreview.net/forum?id=I_GUngvVNz](https://openreview.net/forum?id=I_GUngvVNz).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2023] Y. Wu, Y. Fan, P. P. Liang, A. Azaria, Y. Li 和 T. Mitchell. 阅读并获得奖励：借助说明手册学习玩Atari游戏。在
    *ICLR 2023的强化学习再生工作坊*，2023年。网址 [https://openreview.net/forum?id=I_GUngvVNz](https://openreview.net/forum?id=I_GUngvVNz)。
- en: Appendix
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A validity of LLM generated value
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLM 生成值的有效性
- en: This work surveys 15 human and collects their value of the 5 events used in
    the experiments. The result is plotted below with the language model (chatGPT3.5
    in this case)’s result marked with a red line. Language model generated value
    is the same as the human annotation mean or within the distribution, except for
    the "eating food in public", which could be due to the fact this phrasing can
    be interpreted quite differently. This figure shows the validity of using language
    model as proxy in scenarios where human expertise is traditionally required.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作调查了15个人，并收集了他们对实验中使用的5个事件的评价。结果如下面的图示所示，语言模型（此例中为 chatGPT3.5）的结果用红线标出。语言模型生成的值与人工标注均值相同或在分布范围内，唯独“在公共场合吃东西”一项可能由于该表述的解读差异较大。此图展示了在传统上需要人工专业知识的场景中使用语言模型作为代理的有效性。
- en: '![Refer to caption](img/eee5320d8a63dfdc8af5d29c71864a0c.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/eee5320d8a63dfdc8af5d29c71864a0c.png)'
- en: 'Figure 9: The height of the each bar represents the mean of 15 human annotated
    values with a black standard deviation error line.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：每个条形的高度表示15个人工标注值的均值，并带有黑色标准差误差线。
