- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:42:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:42:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越数值奖励：带有 LLM 代理的上下文决斗老虎机
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01887](https://ar5iv.labs.arxiv.org/html/2407.01887)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.01887](https://ar5iv.labs.arxiv.org/html/2407.01887)
- en: \pdfcolInitStack
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfcolInitStack
- en: tcb@breakable
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: tcb@breakable
- en: Fanzeng Xia fanzengxia@link.cuhk.edu.cn, litongxin@cuhk.edu.cn Hao Liu {hliu3,
    yyue}@caltech.edu Yisong Yue {hliu3, yyue}@caltech.edu Tongxin Li fanzengxia@link.cuhk.edu.cn,
    litongxin@cuhk.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Fanzeng Xia fanzengxia@link.cuhk.edu.cn, litongxin@cuhk.edu.cn Hao Liu {hliu3,
    yyue}@caltech.edu Yisong Yue {hliu3, yyue}@caltech.edu Tongxin Li fanzengxia@link.cuhk.edu.cn,
    litongxin@cuhk.edu.cn
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In-context decision-making is an important capability of artificial general
    intelligence, which Large Language Models (LLMs) have effectively demonstrated
    in various scenarios. However, LLMs often face challenges when dealing with numerical
    contexts, and limited attention has been paid to evaluating their performance
    through preference feedback generated by the environment. This paper investigates
    the performance of LLMs as decision-makers in the context of Dueling Bandits (DB).
    We first evaluate the performance of LLMs by comparing GPT-3.5 Turbo, GPT-4, and
    GPT-4 Turbo against established DB algorithms. Our results reveal that LLMs, particularly
    GPT-4 Turbo, quickly identify the Condorcet winner, thus outperforming existing
    state-of-the-art algorithms in terms of weak regret. Nevertheless, LLMs struggle
    to converge even when explicitly prompted to do so, and are sensitive to prompt
    variations. To overcome these issues, we introduce an LLM-augmented algorithm,
    IF-Enhanced LLM, which takes advantage of both in-context decision-making capabilities
    of LLMs and theoretical guarantees inherited from classic DB algorithms. The design
    of such an algorithm sheds light on how to enhance trustworthiness for LLMs used
    in decision-making tasks where performance robustness matters. We show that IF-Enhanced
    LLM has theoretical guarantees on both weak and strong regret. Our experimental
    results validate that IF-Enhanced LLM is robust even with noisy and adversarial
    prompts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文决策能力是人工通用智能的一个重要能力，大型语言模型（LLMs）已经在各种场景中有效地展示了这一能力。然而，LLMs 在处理数值上下文时往往面临挑战，而且对通过环境生成的偏好反馈来评估其性能的关注有限。本文研究了
    LLMs 在决斗老虎机（DB）背景下作为决策者的表现。我们首先通过将 GPT-3.5 Turbo、GPT-4 和 GPT-4 Turbo 与已建立的 DB
    算法进行比较，来评估 LLMs 的表现。我们的结果显示，LLMs，特别是 GPT-4 Turbo，能够快速识别 Condorcet 胜者，从而在**弱悔恨度**方面超越现有的最先进算法。然而，LLMs
    即使在明确提示下也难以收敛，并且对提示变化非常敏感。为解决这些问题，我们引入了一种增强型 LLM 算法 IF-Enhanced LLM，该算法利用了 LLMs
    的上下文决策能力以及经典 DB 算法继承的理论保证。这种算法的设计为提高在性能鲁棒性重要的决策任务中 LLMs 的**可信赖性**提供了启示。我们证明了 IF-Enhanced
    LLM 在**弱悔恨度**和**强悔恨度**上具有理论保证。我们的实验结果验证了 IF-Enhanced LLM 即使在噪声和对抗性提示下也具有鲁棒性。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In this paper, we are particularly interested in studying the ability of Large
    Language Model (LLM) agent to behave optimally in online decision-making settings,
    i.e., have low regret. In such settings, notable failure cases have been reported [[1](#bib.bib1),
    [2](#bib.bib2)], e.g., LLM agents being vulnerable to adversarial loss functions
    and suffer from high regret compared to classic algorithms such as Follow-The-Regularized-Leader
    (FTRL) [[1](#bib.bib1)], and failures in exploration for classic Multi-Armed Bandit
    (MAB) problems [[2](#bib.bib2)]. These results suggest that carefully designed
    prompts (particularly with external summarization) and non-trivial algorithmic
    interventions are needed to elicit desirable behavior of LLM agents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文特别关注研究大型语言模型（LLM）代理在在线决策设置中表现出的**最佳行为**能力，即具有低悔恨度。在这种设置下，已经报告了显著的失败案例[[1](#bib.bib1),
    [2](#bib.bib2)]，例如，LLM 代理容易受到对抗性损失函数的影响，与经典算法如 Follow-The-Regularized-Leader (FTRL)
    [[1](#bib.bib1)] 相比悔恨度较高，以及经典的多臂老虎机（MAB）问题中的探索失败[[2](#bib.bib2)]。这些结果表明，需要精心设计的提示（特别是带有外部总结）和非平凡的算法干预来引出
    LLM 代理的**理想行为**。
- en: 'The failure cases encountered by LLMs may be attributed to intrinsic difficulties
    of handling numerical rewards, and there has been a notable lack of emphasis on
    evaluating the relative comparisons among the decisions they generate. To disentangle
    the complexities introduced by numerical rewards, this paper focuses on the problem
    of dueling bandits (DB) [[3](#bib.bib3)] as an important variant of the MAB problem.
    The problem of DB[[3](#bib.bib3), [4](#bib.bib4)] extends the classic MAB model[[5](#bib.bib5)]
    by querying for preference feedback between selected pairs of arms to identify
    the best one. In DB, a decision-maker chooses a pair of arms every time, but only
    observes the binary outcome (win or lose) of a noisy comparison between the two
    selected arms. Figure[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") shows a toy example illustrating
    the in-context interaction between an LLM agent and an environment. As a variant
    of the standard MAB problem, DB has attracted significant attention in recent
    years due to its applicability in information retrieval[[6](#bib.bib6)], recommendation
    systems [[7](#bib.bib7)], and online ranker evaluation [[4](#bib.bib4)]. This
    setup is particularly useful when eliciting explicit feedback is challenging or
    when the feedback is inherently comparative, like taste of food and product attractiveness [[3](#bib.bib3)].
    We frame our investigation with the following question:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMs遇到的失败案例可能归因于处理数值奖励的固有困难，并且在评估它们生成的决策之间的相对比较时明显缺乏重视。为了理清数值奖励引入的复杂性，本文关注于对决赌博（DB）[[3](#bib.bib3)]作为MAB问题的一个重要变体。DB问题[[3](#bib.bib3),
    [4](#bib.bib4)]通过查询所选手臂对之间的偏好反馈来扩展经典MAB模型[[5](#bib.bib5)]，以识别最佳手臂。在DB中，决策者每次选择一对手臂，但仅观察到两个选定手臂之间的噪声比较的二元结果（赢或输）。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")展示了一个玩具示例，说明了LLM代理与环境之间的上下文交互。作为标准MAB问题的变体，DB由于其在信息检索[[6](#bib.bib6)]、推荐系统[[7](#bib.bib7)]和在线排名评估[[4](#bib.bib4)]中的应用而近年来引起了广泛关注。当引导明确的反馈具有挑战性或反馈本质上是比较性的，如食物的口味和产品的吸引力[[3](#bib.bib3)]时，这种设置特别有用。我们以以下问题来框定我们的研究：'
- en: Are LLMs effective in-context agents for solving the problem of dueling bandits?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在解决对决赌博问题时作为上下文中的代理是否有效？
- en: We highlight two aspects where the capabilities and limitations of LLMs intersect
    with the challenges of the dueling bandit problem. First, dueling bandits pose
    distinctive challenges as a decision-making instance, notably due to the sparse
    nature of the relative rewards they provide. This sparsity complicates the in-context
    decision-making process, as it restricts the feedback obtained from interactions,
    introducing a level of difficulty not typically seen in conventional bandit problems.
    Even though reduction from DB to standard MAB exists [[8](#bib.bib8), [9](#bib.bib9)],
    it is unclear how LLMs would perform in DB with relative comparison feedback rather
    than numerical rewards. There are conceptual differences between them, similar
    to those for Reinforcement Learning from Human Feedback (RLHF) [[10](#bib.bib10)]
    and standard RL, where impossibility results can be found in[[11](#bib.bib11)].
    Additionally, LLMs often struggle with numerical contexts, and there has been
    insufficient focus on assessing their performance using relative comparison feedback.
    Besides these aspects, it is worth noting that dueling bandits have broad applications,
    notably in areas such as healthcare decision support[[12](#bib.bib12)], legal
    judgment prediction [[13](#bib.bib13)], and stock movement prediction [[14](#bib.bib14)].
    We summarize our main results below.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调了LLMs的能力和局限性与对决赌博问题挑战相交的两个方面。首先，对决赌博作为一种决策实例，具有独特的挑战，特别是由于它们提供的相对奖励的稀疏性。这种稀疏性使得上下文中的决策过程变得复杂，因为它限制了从交互中获得的反馈，引入了在传统赌博问题中通常不见的难度。尽管存在从DB到标准MAB的简化[[8](#bib.bib8),
    [9](#bib.bib9)]，但尚不清楚LLMs在处理相对比较反馈而非数值奖励的DB问题中表现如何。它们之间存在概念上的差异，类似于从人类反馈强化学习（RLHF）[[10](#bib.bib10)]到标准RL的差异，在[[11](#bib.bib11)]中可以找到不可能的结果。此外，LLMs在处理数值上下文时常常面临困难，并且对使用相对比较反馈评估它们的性能关注不足。除了这些方面外，值得注意的是，对决赌博有广泛的应用，特别是在医疗决策支持[[12](#bib.bib12)]、法律判断预测[[13](#bib.bib13)]和股票运动预测[[14](#bib.bib14)]等领域。我们在下面总结了主要结果。
- en: 'Evaluation of LLMs’ capabilities for preference feedback. First, going beyond
    numeric rewards, in Section[4](#S4 "4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), we thoroughly
    evaluate the performance of LLM agents in terms of both strong and weak regret
    for making decisions in DB by comparing against various baseline DB algorithms
    via a case study. LLMs exhibit behaviors that significantly differ from those
    in classic MAB settings [[2](#bib.bib2)], as summarized in Table [1](#S4.T1 "Table
    1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents"). Notably, GPT-4 Turbo can serve as an effective decision-maker
    for dueling bandits in the short term, quickly identifying and exploiting the
    best arm with low variance across different instances. However, the long-term
    performance is hindered by over-estimation bias in the exploration stage and lack
    of convergence criterion in the exploitation stage. This highlights the need for
    more robust and effective strategies to use LLMs.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '评估LLMs的偏好反馈能力。首先，超越数字奖励，在第[4](#S4 "4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")节中，我们通过案例研究，彻底评估LLM代理在对抗赌博中做决策的强后悔和弱后悔表现，并与各种基线DB算法进行比较。LLMs展现出与经典MAB设置显著不同的行为[[2](#bib.bib2)]，如表[1](#S4.T1
    "Table 1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs
    as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")所总结。值得注意的是，GPT-4 Turbo可以在短期内作为对抗赌博的有效决策者，快速识别并利用不同实例中的最佳臂，但长期表现受到探索阶段的过度估计偏差和利用阶段缺乏收敛标准的阻碍。这突显了对LLMs使用更稳健有效策略的需求。'
- en: 'Effective and Robust LLM-augmented DB algorithm. To enhance the trustworthiness
    of integrating LLMs in decision-making problems where performance robustness is
    critical, in Section[5.1](#S5.SS1 "5.1 Algorithm Design of IF-Enhanced LLM ‣ 5
    Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), we propose a design of LLM-augmented algorithm,
    IF-Enhanced LLM that combines a classic DB algorithm, Interleaved Filter2 (IF2),
    with LLM agents. We show that the proposed algorithm has theoretical guarantees,
    with experiments demonstrating its efficacy and robustness with various prompting
    scenarios.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '有效且稳健的LLM增强DB算法。为了增强将LLMs整合到决策问题中的可信度，尤其是在性能稳健性至关重要的情况下，在第[5.1](#S5.SS1 "5.1
    Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")节中，我们提出了一种LLM增强算法IF-Enhanced
    LLM，它将经典的DB算法Interleaved Filter2 (IF2)与LLM代理相结合。我们展示了该算法具有理论保障，并通过实验证明了其在各种提示场景下的有效性和稳健性。'
- en: '![Refer to caption](img/494de5e57126f1567c46d23cad991ee5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/494de5e57126f1567c46d23cad991ee5.png)'
- en: 'Figure 1: In-context interaction at time $t$ between an LLM agent and a dueling
    bandit environment.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：时间$t$时，LLM代理与对抗赌博环境之间的上下文交互。
- en: 2 Preliminaries
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基础知识
- en: 'In this section, we briefly introduce the problem of dueling bandits (DB) and
    establish the necessary notation for this paper. Additional useful definitions
    can be found in Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas
    for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and
    Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们简要介绍对抗赌博（DB）问题，并建立本文所需的符号。更多有用的定义可以在附录[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful
    Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix
    A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")中找到。'
- en: Dueling Bandits. In a $K$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗赌博。在$K$。
- en: 'LLM Agents for Dueling Bandits. We consider an LLM agent with policy $\pi_{\mathrm{LLM}}$
    (see Figure[6](#A2.F6 "Figure 6 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")), consisting of three parts:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗赌博的LLM代理。我们考虑一个具有策略$\pi_{\mathrm{LLM}}$的LLM代理（见图[6](#A2.F6 "Figure 6 ‣ B.1.1
    Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")），它由三部分组成：'
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Context $C$, and the task objective.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文$C$和任务目标。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: History $H_{t}$, which includes a sequence of pairwise dueling results and the
    empirical probabilities.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 历史$H_{t}$，包括一系列成对对抗结果和经验概率。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning $R$: the zero-shot chain-of-thought (CoT) reasoning [[15](#bib.bib15)]
    that encourages the LLM agent to reason about the problem in a structured manner.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '推理 $R$: 零-shot 思维链（CoT）推理 [[15](#bib.bib15)]，鼓励LLM代理以结构化方式推理问题。'
- en: 'The LLM agent’s policy can be represented as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理的策略可以表示为：
- en: '|  | $\displaystyle\left(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t)\right)=\pi_{\mathrm{LLM}}\left(\mathtt{Prompt}(C,H_{t},R)\right).$
    |  | (1) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t)\right)=\pi_{\mathrm{LLM}}\left(\mathtt{Prompt}(C,H_{t},R)\right).$
    |  | (1) |'
- en: 'The goal is to maximize the cumulative reward over some time horizon $T$, where
    the reward is the sum of the unknown probabilities of the two chosen arms beating
    the best arm (Condorcet winner). We can quantify performance as minimizing the
    cumulative regret, either in the strong or weak sense (see Eq.([2](#S2.E2 "In
    2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"))
    and Eq.([3](#S2.E3 "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents"))).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在某个时间范围 $T$ 内最大化累计奖励，其中奖励是两个选择的臂击败最佳臂（Condorcet赢家）的未知概率之和。我们可以通过最小化累计后悔来量化性能，无论是强后悔还是弱后悔（见
    Eq.([2](#S2.E2 "在 2 初步 ‣ 超越数值奖励：LLM代理的上下文对决")) 和 Eq.([3](#S2.E3 "在 2 初步 ‣ 超越数值奖励：LLM代理的上下文对决"))）。
- en: Strong and Weak Regret. We assume the standard setting that a Condorcet winner
    exists[[7](#bib.bib7), [16](#bib.bib16), [4](#bib.bib4), [3](#bib.bib3)]. The
    Condorcet winner $b^{*}$. Throughout this paper, we consider two important performance
    metrics. The first is the strong regret of a given algorithm ALG, defined as
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 强后悔与弱后悔。我们假设存在Condorcet赢家[[7](#bib.bib7), [16](#bib.bib16), [4](#bib.bib4),
    [3](#bib.bib3)]。Condorcet赢家为 $b^{*}$。在本文中，我们考虑两个重要的性能指标。第一个是给定算法ALG的强后悔，定义为
- en: '|  | $1$2 |  | (2) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $T$ is the time horizon. The second is the weak regret of ALG, defined
    as
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T$ 是时间范围。第二个是ALG的弱后悔，定义为
- en: '|  | $1$2 |  | (3) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'which only compares $b^{*}$. It is worth highlighting that LLM agents exhibit
    significantly different behaviors with respect to the two defined notions of regret,
    as detailed in Section [4.2](#S4.SS2 "4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents"). More useful definitions and settings are provided in Appendix [A.2.1](#A1.SS2.SSS1
    "A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 仅比较 $b^{*}$。值得注意的是，LLM代理在这两种定义的后悔概念下表现出显著不同的行为，详见第 [4.2](#S4.SS2 "4.2 实验结果 ‣
    4 LLMs作为独立的上下文决策者 ‣ 超越数值奖励：LLM代理的上下文对决") 节。附录 [A.2.1](#A1.SS2.SSS1 "A.2.1 对决抢带的有用假设和引理
    ‣ A.2 理论分析 ‣ 附录 A IF增强LLM的算法设计与分析 ‣ 超越数值奖励：LLM代理的上下文对决") 提供了更多有用的定义和设置。
- en: 3 Related Works
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: Dueling Bandits. The problem of dueling bandits was initially introduced in
    [[3](#bib.bib3)]. Various methods have been proposed to tackle the task since
    then. These methods can be broadly classified into two categories as Explore-Then-Exploit
    methods and Ongoing Regret Minimization methods according to [[4](#bib.bib4)].
    Explore-Then-Exploit methods focus on identifying the best arm with high confidence
    before exploiting it, such as Interleaved Filter (IF) [[3](#bib.bib3)] and Beat
    the Mean (BTM) [[17](#bib.bib17)], etc. In contrast, Ongoing Regret Minimization
    methods explicitly target the objective of minimizing cumulative regret, including
    Relative Upper Confidence Bound (RUCB) [[18](#bib.bib18)] and Self-Sparring[[7](#bib.bib7)],
    etc. Dueling bandit problem and preference feedback in general has a wide variety
    of applications, including recommendation systems [[3](#bib.bib3)], robotics [[19](#bib.bib19)],
    and most recently, the training algorithm of large language models, such as Reinforcement
    Learning from Human Feedback (RLHF) [[20](#bib.bib20)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗赌博者问题。对抗赌博者问题最初在[[3](#bib.bib3)]中被提出。自那时以来，已经提出了各种方法来应对这个任务。这些方法可以大致分为两类：探索-然后利用方法和持续遗憾最小化方法，根据[[4](#bib.bib4)]。探索-然后利用方法关注于在利用之前以较高的置信度识别最佳臂，例如交替过滤器（IF）[[3](#bib.bib3)]和击败均值（BTM）[[17](#bib.bib17)]等。相反，持续遗憾最小化方法明确以最小化累计遗憾为目标，包括相对上界置信度（RUCB）[[18](#bib.bib18)]和自我对抗[[7](#bib.bib7)]等。对抗赌博者问题和偏好反馈通常具有广泛的应用，包括推荐系统[[3](#bib.bib3)]、机器人技术[[19](#bib.bib19)]，以及最近的大型语言模型训练算法，例如人类反馈强化学习（RLHF）[[20](#bib.bib20)]。
- en: LLM Agents for Multi-Armed Bandits. Several recent works have explored evaluating
    the capabilities of LLMs in bandit problems. For example, [[21](#bib.bib21)] proposed
    an approach to enhance contextual bandits by integrating LLMs as encoders. The
    LLMs’ ability to capture rich semantic and syntactic information from textual
    contexts is leveraged to provide the algorithm with a more informative representation
    of the context. The LLM-augmented algorithm transforms the raw context into a
    latent space vector using the LLM’s encoding capabilities. This encoded context
    is then used to guide the decision-making process. [[2](#bib.bib2)] investigates
    whether LLMs can engage in exploration in simple MAB environments without additional
    training. They compared various prompt designs and found that GPT-4 with zero-shot
    chain-of-thought (CoT) reasoning and an externally summarized interaction history
    performed the best, while other configurations failed in exploration, either by
    never selecting the best arm after initial rounds or by selecting all arms nearly
    equally often[[2](#bib.bib2)]. Different from the previous results, in this work
    we go beyond the settings of numeric rewards and investigate the capabilities
    of LLMs under preference feedback.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博问题中的LLM代理。最近的一些研究探讨了评估LLM在赌博问题中的能力。例如，[[21](#bib.bib21)]提出了一种通过将LLM作为编码器来增强上下文赌博问题的方法。利用LLM从文本上下文中捕捉丰富的语义和句法信息的能力，为算法提供了更具信息量的上下文表示。LLM增强的算法通过LLM的编码能力将原始上下文转换为潜在空间向量。然后，利用这个编码上下文来指导决策过程。[[2](#bib.bib2)]探讨了LLM是否可以在简单的多臂赌博环境中进行探索而无需额外训练。他们比较了各种提示设计，发现GPT-4在零-shot链式推理（CoT）和外部总结的交互历史中表现最佳，而其他配置在探索中失败，要么在初始轮次后从未选择最佳臂，要么几乎同等频繁地选择所有臂[[2](#bib.bib2)]。与之前的结果不同，在本研究中，我们超越了数字奖励的设置，探讨了LLM在偏好反馈下的能力。
- en: In-Context LLMs for Decision-Making. Beyond bandit problems, LLM agents have
    demonstrated strong capabilities in complex reasoning across a wide range of in-context
    decision-making tasks[[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    Various existing works aim to understand LLM agents’ capabilities for in-context
    decision-making, with notable examples including planning [[26](#bib.bib26), [27](#bib.bib27)].
    Additionally, LLM agents have been shown to enhance embodied agents in various
    robotic applications by providing advanced task planning abilities [[28](#bib.bib28)]
    and reward designing[[29](#bib.bib29)], further enabling the development of lifelong
    learning agents [[30](#bib.bib30)]. Besides these empirical successes, the authors
    of [[1](#bib.bib1)] analyzed LLMs’ interactions in online learning and game theory
    settings through the lens of the regret metrics. They identified simple cases
    where LLMs fail to be no-regret. Another line of research incorporates LLMs into
    classic decision-making frameworks to create LLM-augmented online decision-makers.
    For instance, Liu et al.[[31](#bib.bib31)] utilized LLMs to enhance the components
    of warm starting, sampling candidates, and surrogate modeling in Bayesian optimization.
    Our work contributes to this broad area by integrating LLM agents with the Interleaved
    Filter algorithm to enhance the utilization of preference feedback.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文中的大规模语言模型（LLMs）用于决策制定。在超越赌博问题的范围内，LLM 代理在各种上下文决策任务中展示了强大的复杂推理能力[[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。现有的各种研究旨在理解 LLM 代理在上下文决策中的能力，其中一些显著的例子包括规划[[26](#bib.bib26),
    [27](#bib.bib27)]。此外，LLM 代理已经被证明通过提供先进的任务规划能力[[28](#bib.bib28)]和奖励设计[[29](#bib.bib29)]，在各种机器人应用中增强了具身代理，从而进一步促进了终身学习代理的发展[[30](#bib.bib30)]。除了这些实证成功，[[1](#bib.bib1)]的作者通过后悔度度量分析了
    LLM 在在线学习和博弈论环境中的互动。他们识别出 LLM 在某些简单情况下未能实现无悔策略。另一类研究将 LLM 融入经典决策框架中，创造了增强型在线决策者。例如，Liu
    等人[[31](#bib.bib31)]利用 LLM 提升了贝叶斯优化中的温暖启动、候选样本选择和代理建模等组件。我们的工作通过将 LLM 代理与交错滤波算法结合，增强了偏好反馈的利用，进一步贡献于这一广泛领域。
- en: 4 LLMs as Standalone In-Context Decision-Makers
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LLM 作为独立的上下文决策者
- en: 'To evaluate the efficacy of LLMs for solving DB problems, in this section,
    we use LLMs as standalone in-context decision-making agents and compare them with
    classic baseline algorithms. Our evaluation is two-fold: First, in Figures [2](#S4.F2
    "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"), we compare the performance
    of LLM agents and classic algorithms in terms of the strong and weak regret (see
    Eq. ([2](#S2.E2 "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")) and Eq. ([3](#S2.E3 "In 2 Preliminaries ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")), with standard deviation).
    Second, we delve into the experimental results and analyze the advantages and
    disadvantages of LLM agents.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 LLM 解决 DB 问题的有效性，在本节中，我们将 LLM 作为独立的上下文决策代理，并将其与经典基线算法进行比较。我们的评估分为两部分：首先，在图[2](#S4.F2
    "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数值奖励：上下文对决赌博与 LLM 代理")和[8](#A2.F8 "图
    8 ‣ B.2.1 与不同度量标准的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数值奖励：上下文对决赌博与 LLM 代理")中，我们比较了
    LLM 代理和经典算法在强后悔和弱后悔方面的表现（见公式([2](#S2.E2 "在 2 基础知识 ‣ 超越数值奖励：上下文对决赌博与 LLM 代理")）和公式([3](#S2.E3
    "在 2 基础知识 ‣ 超越数值奖励：上下文对决赌博与 LLM 代理")），包括标准差）。其次，我们深入探讨实验结果，分析 LLM 代理的优点和缺点。
- en: 4.1 Implementation Details of Experiments
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验实施细节
- en: 'Prompts and Configurations of LLMs. We employ an interactive zero-shot chain-of-thought
    (CoT) prompt $\mathtt{Prompt}(C,H_{t},R)$. We adopt the prompting template that
    leads to the best performance among all prompt variations explored in a recent
    study [[2](#bib.bib2)] for the classic MAB problem. The LLM agents interact with
    dueling bandit environments in a round-based manner, with the prompt guiding their
    decision-making process. We conduct experiments with three LLMs: GPT-3.5 Turbo,
    GPT-4, and GPT-4 Turbo through the OpenAI API with temperature = 0\. The detailed
    prompt is provided in Appendix[B.1.1](#A2.SS1.SSS1 "B.1.1 Design of Prompts ‣
    B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM的提示和配置。我们采用交互式零-shot链式思维（CoT）提示$\mathtt{Prompt}(C,H_{t},R)$。我们采用了最近研究中所有提示变体中表现最佳的提示模板[[2](#bib.bib2)]，用于经典的MAB问题。LLM代理以回合为基础的方式与对抗赌博环境进行互动，提示指导他们的决策过程。我们通过OpenAI
    API以温度=0对三种LLMs进行实验：GPT-3.5 Turbo、GPT-4和GPT-4 Turbo。详细提示见附录[B.1.1](#A2.SS1.SSS1
    "B.1.1 Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")。'
- en: 'Baselines. We compare LLMs against eight well-established baseline algorithms
    to evaluate their efficacy. The baselines include Interleaved Filter (IF2) [[3](#bib.bib3)],
    Beat the Mean (BTM) [[17](#bib.bib17)], Sensitivity Analysis of VAriables for
    Generic Exploration (SAVAGE) [[32](#bib.bib32)], Relative Upper Confidence Bound
    (RUCB) [[18](#bib.bib18)], Relative Confidence Sampling (RCS) [[4](#bib.bib4)],
    Relative Minimum Empirical Divergence (RMED) [[33](#bib.bib33)], Self-Sparring [[7](#bib.bib7)],
    and Double Thompson Sampling (DTS) [[16](#bib.bib16)]. Each of these algorithms
    employs distinct strategies for selecting arms and estimating preferences, with
    the ultimate goal of efficiently identifying the Condorcet winner. We assess the
    performance of LLMs and baseline algorithms using strong regret and weak regret
    metrics defined in Section [2](#S2 "2 Preliminaries ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '基准。我们将LLMs与八种成熟的基准算法进行比较，以评估其效果。这些基准包括交替过滤器（IF2）[[3](#bib.bib3)]、击败均值（BTM）[[17](#bib.bib17)]、通用探索变量敏感性分析（SAVAGE）[[32](#bib.bib32)]、相对上置信界（RUCB）[[18](#bib.bib18)]、相对置信采样（RCS）[[4](#bib.bib4)]、相对最小经验差异（RMED）[[33](#bib.bib33)]、自我对抗（Self-Sparring）[[7](#bib.bib7)]和双重汤普森采样（DTS）[[16](#bib.bib16)]。这些算法采用不同的策略来选择臂并估计偏好，*最终*目标是高效地识别康多塞胜者。我们使用第[2](#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")节中定义的强后悔和弱后悔度量来评估LLMs和基准算法的性能。'
- en: 'Environments. We evaluate the regret performance of LLMs and baselines across
    two stochastic environments, each characterized by a distinct preference matrix
    $P$ parameters given by:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 环境。我们在两个随机环境中评估LLMs和基准的后悔表现，每个环境都由不同的偏好矩阵$P$参数给出：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Easy instance: $\theta(1)=1,\ \theta(i)=0.5-(i-1)/2K$.'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单实例：$\theta(1)=1,\ \theta(i)=0.5-(i-1)/2K$。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hard instance: $\theta(i)=1-(i-1)/K$.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 难实例：$\theta(i)=1-(i-1)/K$。
- en: 'Note that the datasets generated in this way satisfy the Strong Stochastic
    Transitivity (SST) and Stochastic Triangle Inequality (STI) properties [[3](#bib.bib3)]
    (see Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") for more details). The settings of the BTL model used in our experiments
    also imply the existence of a Condorcet winner.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，以这种方式生成的数据集满足强随机传递性（SST）和随机三角不等式（STI）属性[[3](#bib.bib3)]（更多细节请参见附录[A.2.1](#A1.SS2.SSS1
    "A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")）。我们实验中使用的BTL模型设置也暗示了存在一个康多塞胜者。'
- en: Random Tests. The scale of our experiments is chosen to balance computational
    feasibility while preserving the ability of obtaining meaningful conclusions.
    We set the time horizon to $T=2000$ times for the baseline algorithms, enabling
    an understanding of their average behaviors and reliable performance estimates.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 随机测试。我们选择实验规模以平衡计算可行性，同时保留得出有意义结论的能力。我们将时间范围设置为基准算法的$T=2000$次，以便了解其平均行为和可靠的性能估计。
- en: '![Refer to caption](img/097221bb53277accfd3f1731acb7a9a0.png)![Refer to caption](img/ca9563161504d0368d178e7b32888c74.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/097221bb53277accfd3f1731acb7a9a0.png)![请参见说明](img/ca9563161504d0368d178e7b32888c74.png)'
- en: 'Figure 2: Comparisons between LLM agents and various classic DB algorithms.
    Left and Right: strong and weak regret for the Easy instance. Results for the
    Hard instance are presented in Figure [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLM代理与各种经典DB算法的比较。左侧和右侧：简单实例的强悔和弱悔。困难实例的结果见图[8](#A2.F8 "图 8 ‣ B.2.1 不同指标的比较
    ‣ B.2 补充实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")。
- en: 4.2 Experimental results
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'For brevity, we present our initial analysis focused on the Easy instance (Figure [2](#S4.F2
    "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).
    The analysis is qualitatively similar for the Hard instance (Figure [8](#A2.F8
    "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") in Appendix [B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")). We use $\gamma=0.5$ for
    RUCB, RCS and DTS. We analyze the results in terms of the strong and weak regret
    defined in Section [2](#S2 "2 Preliminaries ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简明起见，我们在这里展示了初步分析，重点关注了简单实例（图[2](#S4.F2 "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLMs作为独立的上下文决策者
    ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")）。对于困难实例（图[8](#A2.F8 "图 8 ‣ B.2.1 不同指标的比较 ‣ B.2 补充实验
    ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")）的分析在定性上是类似的（见附录[B.2](#A2.SS2 "B.2
    补充实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")）。我们对RUCB、RCS和DTS使用了$\gamma=0.5$。我们根据第[2](#S2
    "2 前言 ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")节中定义的强悔和弱悔分析结果。
- en: 'Strong Regret. The strong regret results in Figure[2](#S4.F2 "Figure 2 ‣ 4.1
    Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") (Left) reveal
    that among the LLMs, GPT-4 Turbo demonstrates the best performance in both the
    Easy and Hard instances. Comparing GPT-4 Turbo with the state-of-the-art baseline
    algorithms (DTS and Self-Sparring), we observe the following: (i) In the first
    $20$ time steps, GPT-4 Turbo maintains a performance level comparable to the state-of-the-art
    baselines in the Hard instance and slightly underperforms compared to the state-of-the-art
    baselines in the Easy instance, yet still outperforms all the other baseline algorithms.
    (iii) Throughout the entire time horizon, the strong regret of LLMs grows approximately
    linearly in both instances, indicating a non-converging behavior of the LLM agents.
    The results for the Hard instance (Figure[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")) are qualitatively similar.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 强悔。图[2](#S4.F2 "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLMs作为独立的上下文决策者 ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")（左）显示，在LLM中，GPT-4
    Turbo在简单和困难实例中都表现最佳。将GPT-4 Turbo与最先进的基准算法（DTS和自我对抗）进行比较，我们观察到以下几点：（i）在前$20$个时间步骤中，GPT-4
    Turbo在困难实例中保持了与最先进基准算法相当的性能，而在简单实例中略逊于最先进基准算法，但仍然优于所有其他基准算法。（iii）在整个时间范围内，LLM的强悔在两个实例中都大致呈线性增长，表明LLM代理存在非收敛行为。困难实例的结果（图[8](#A2.F8
    "图 8 ‣ B.2.1 不同指标的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖励：LLM代理的上下文对决多臂老虎机")）在定性上是类似的。
- en: 'Weak Regret. In terms of the weak regret metric, Figure[2](#S4.F2 "Figure 2
    ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    (Right) indicates the exceptional performance of GPT-4 Turbo. It consistently
    outperforms all other LLMs and state-of-the-art baseline algorithms throughout
    the entire time horizon in both instances, exhibiting the lowest mean weak regret
    with small variance. The results for the Hard instance (Figure [8](#A2.F8 "Figure
    8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣
    Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")) are qualitatively similar.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 弱遗憾。在弱遗憾指标方面，图 [2](#S4.F2 "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数值奖励：与 LLM
    代理的上下文对决匪徒")（右）显示了 GPT-4 Turbo 的卓越表现。它在整个时间范围内始终优于所有其他 LLM 和最先进的基线算法，在两个实例中都表现出最低的平均弱遗憾和小的方差。对于
    Hard 实例的结果（图 [8](#A2.F8 "图 8 ‣ B.2.1 不同指标的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数值奖励：与
    LLM 代理的上下文对决匪徒")）在质量上也相似。
- en: 'To more thoroughly evaluate the capabilities of LLMs, we delve into the success
    and failure cases found in our experiments, with more results presented in Appendix [B.2](#A2.SS2
    "B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). In the
    following analyses, we focus on GPT-4 Turbo, which is the top-performing LLM as
    shown in Figures [2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更彻底地评估大语言模型（LLMs）的能力，我们深入探讨了实验中的成功与失败案例，更多结果见附录 [B.2](#A2.SS2 "B.2 补充实验 ‣
    附录 B 提示设计与补充结果 ‣ 超越数值奖励：与 LLM 代理的上下文对决匪徒")。在以下分析中，我们重点关注 GPT-4 Turbo，这在图 [2](#S4.F2
    "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数值奖励：与 LLM 代理的上下文对决匪徒") 和 [8](#A2.F8
    "图 8 ‣ B.2.1 不同指标的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数值奖励：与 LLM 代理的上下文对决匪徒") 中显示为表现最好的
    LLM。
- en: 4.2.1 Advantages of LLM Agents
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 LLM 代理的优势
- en: 'Efficient Best Arm Identification. As evident in Figures[2](#S4.F2 "Figure
    2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") (Right), GPT-4 Turbo demonstrates
    strong zero-shot chain-of-thought (CoT) reasoning capabilities in terms of the
    weak regret. This can be attributed to its ability to quickly identify and consistently
    include the best arm in its duels. Figure [9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") in Appendix[B.2](#A2.SS2 "B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") illustrates the fraction of duels including
    the best arm across different time intervals. GPT-4 Turbo outperforms other LLMs
    and the state-of-the-art baselines throughout the entire timeline in both instances.
    These findings suggest that GPT-4 Turbo is able to effectively process the feedback
    obtained from duels and make informed decisions to identify and select the best
    arm, leading to superior weak regret performance.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '高效的最佳臂识别。正如图[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") 和图[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")（右）所示，GPT-4 Turbo 在弱遗憾方面展示了强大的零-shot 思维链（CoT）推理能力。这归因于其能够快速识别并始终在对决中包含最佳臂。附录[B.2](#A2.SS2
    "B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")中的图[9](#A2.F9
    "Figure 9 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") 展示了不同时间间隔内包括最佳臂的对决比例。GPT-4 Turbo
    在整个时间线中超越了其他 LLM 和最先进的基线。研究结果表明，GPT-4 Turbo 能够有效处理从对决中获得的反馈，并做出明智的决策以识别和选择最佳臂，从而实现卓越的弱遗憾表现。'
- en: 'Low Variance of Regret.  GPT-4 Turbo demonstrates low variance compared to
    other LLMs and baseline algorithms. As shown in Figure [10](#A2.F10 "Figure 10
    ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") (Left) in Appendix[B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"), GPT-4 Turbo exhibits the
    lowest average generalized variance of strong and weak regret in the Easy instance,
    suggesting that it can make consistent decisions when the preference structure
    is simple. For the Hard instance shown in Figure [10](#A2.F10 "Figure 10 ‣ B.2.1
    Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") (Right), GPT-4 Turbo has a variance level comparable
    to the state-of-the-art baseline algorithms (except BTM and SAVAGE, which are
    still in an early stage of exploration), indicating that its performance stability
    remains competitive even when faced with a more complex preference structure.
    This low variance result highlights GPT-4 Turbo’s ability to adapt to varying
    levels of difficulty and maintain a stable decision-making process in DB setting.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '低遗憾方差。GPT-4 Turbo 相较于其他 LLM 和基线算法表现出低方差。如附录[B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")中的图[10](#A2.F10 "Figure 10
    ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")（左）所示，GPT-4 Turbo 在简单实例中表现出最低的强遗憾和弱遗憾的平均广义方差，表明它能够在偏好结构简单时做出一致的决策。对于图[10](#A2.F10
    "Figure 10 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")（右）所示的复杂实例，GPT-4 Turbo 的方差水平与最先进的基线算法相当（除
    BTM 和 SAVAGE 外，这些算法仍处于探索初期），表明其性能稳定性在面对更复杂的偏好结构时仍然具有竞争力。这一低方差结果突显了 GPT-4 Turbo
    在 DB 设置中适应不同难度水平和维持稳定决策过程的能力。'
- en: 4.2.2 Disadvantages of LLM Agents
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 LLM 代理的缺点
- en: 'Figures [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2
    Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") and[12](#A2.F12
    "Figure 12 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") in Appendix[B.2.2](#A2.SS2.SSS2 "B.2.2
    Duel Selection Trajectory ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") present the arm selection traces of GPT-4 Turbo and state-of-the-art
    baselines on the Easy and Hard instances to analyze the failure cases. The trace
    plots suggest that GPT-4 Turbo exhibits a two-stage Explore-Then-Exploit pattern[[4](#bib.bib4)]
    with occasional robustness checks. Analysis of the two stages reveals two failure
    modes of LLMs:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 附录中的图[11](#A2.F11 "图 11 ‣ LLM 代理与基线算法 ‣ B.2 附加实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数值奖项：LLM
    代理的上下文对决劫掠者")和[12](#A2.F12 "图 12 ‣ LLM 代理与基线算法 ‣ B.2 附加实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数值奖项：LLM
    代理的上下文对决劫掠者")展示了 GPT-4 Turbo 和最先进的基线在简单和困难实例中的臂选择轨迹，以分析失败情况。轨迹图表明 GPT-4 Turbo
    展示了一个两阶段的探索-再利用模式[[4](#bib.bib4)]，偶尔进行鲁棒性检查。对这两个阶段的分析揭示了 LLM 的两种失败模式：
- en: 'Exploration Vulnerability. In the exploration stage, we observe that GPT-4
    Turbo tends to quickly narrow down to a small subset of arms (usually containing
    the Condorcet winner) and repeatedly compare these arms. In contrast, the baselines
    exhibit more diverse and extended exploration patterns. This behavior suggests
    that GPT-4 Turbo may overestimate the quality of arms that win their initial comparisons
    based on limited historical data. Unlike the baselines, which have explicit exploration
    mechanisms, LLMs rely on their inherent randomness (via sampling from their output
    distribution) to explore. Based on these observations, we hypothesize that if
    GPT-4 Turbo happens to sample a sequence of comparisons that favors suboptimal
    arms early on, it can get stuck comparing these arms indefinitely. To test this
    hypothesis, we conducted experiments using noisy prompts with biased history.
    Our results in Figure[12](#A2.F12 "Figure 12 ‣ LLM Agents and Baseline Algorithms
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") confirm
    that GPT-4 Turbo’s exploration strategy is indeed vulnerable to biased history
    initialization and can converge to local optima.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 探索脆弱性。在探索阶段，我们观察到 GPT-4 Turbo 倾向于快速缩小到一个较小的臂子集（通常包含 Condorcet 胜者）并反复比较这些臂。相比之下，基线展示了更具多样性和扩展性的探索模式。这种行为表明
    GPT-4 Turbo 可能会高估在有限历史数据下赢得初次比较的臂的质量。与具有明确探索机制的基线不同，LLM 依赖于其固有的随机性（通过从其输出分布中采样）来进行探索。根据这些观察，我们假设如果
    GPT-4 Turbo 偶然采样了一系列倾向于早期劣质臂的比较，它可能会无限期地陷入对这些臂的比较。为验证这一假设，我们使用带有偏置历史的噪声提示进行了实验。图[12](#A2.F12
    "图 12 ‣ LLM 代理与基线算法 ‣ B.2 附加实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数值奖项：LLM 代理的上下文对决劫掠者")中的结果确认了
    GPT-4 Turbo 的探索策略确实容易受到偏置历史初始化的影响，并可能收敛到局部最优解。
- en: 'Convergence Instability. Despite GPT-4 Turbo’s outstanding weak regret performance,
    it fails to consistently converge to a single best arm to duel against itself,
    even when the prompt setting explicitly calls for it. This behavior highlights
    a fundamental limitation of LLMs: they are primarily designed and trained for
    word token prediction rather than decision-making. Unlike baselines with explicit
    stopping conditions, GPT-4 Turbo relies on its inherent language modeling capabilities
    to determine when to stop exploring. Consequently, in the later exploitation stage,
    GPT-4 Turbo keeps comparing the same top arms without committing to a single winner
    (see Figure [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2
    Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")), reflecting
    an intrinsic uncertainty in its decision-making process for DB. The patterns learned
    during pre-training do not lead to converged decisions, even if the prompt includes
    emphatic commands, exclamation marks, quotation marks, or other forms of emphasis.
    This suggests that the language modeling objective alone may not be sufficient
    for LLMs to achieve optimal control in complex decision-making tasks like DB.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '收敛不稳定性。尽管 GPT-4 Turbo 的弱后悔表现非常出色，但它仍然无法始终如一地收敛到一个最佳臂来与自己对决，即使在提示设置明确要求时也是如此。这种行为突显了大型语言模型（LLMs）的一个基本限制：它们主要是为了词汇预测而设计和训练的，而不是决策制定。与具有明确停止条件的基准方法不同，GPT-4
    Turbo 依赖其固有的语言建模能力来决定何时停止探索。因此，在后期利用阶段，GPT-4 Turbo 不断比较相同的顶级臂，而没有承诺一个单一的赢家（参见图
    [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")），这反映了其在 DB 决策过程中的内在不确定性。即使提示中包含了强调命令、感叹号、引号或其他强调形式，预训练期间学习到的模式也不会导致收敛的决策。这表明，仅凭语言建模目标可能不足以使
    LLMs 在像 DB 这样的复杂决策任务中实现最佳控制。'
- en: Type
                                                  Experimental Results
    Pros Best Arm Identification Condorcet Winner is identified and
    consistently selected
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类型
                                                  实验结果 优点 最佳臂识别
    Condorcet 胜者被识别并且一致选择
- en: 'Results: Figure[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons with Different Metrics
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") Low Variance
    Low variance across instances of varying difficulty'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：图 [2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs
    as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")、[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different
    Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    和 [9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") 显示了低方差，难度变化的实例间方差较小。'
- en: 'Results: Figure[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [10](#A2.F10 "Figure 10 ‣ B.2.1 Comparisons with Different Metrics
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") Cons Exploration
    Vulnerability Overestimates the quality of initially winning arms'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：图[2](#S4.F2 "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数字奖励：具有 LLM 代理的上下文对抗贼")、[8](#A2.F8
    "图 8 ‣ B.2.1 与不同指标的比较 ‣ B.2 附加实验 ‣ 附录 B 提示设计与附加结果 ‣ 超越数字奖励：具有 LLM 代理的上下文对抗贼")
    和 [10](#A2.F10 "图 10 ‣ B.2.1 与不同指标的比较 ‣ B.2 附加实验 ‣ 附录 B 提示设计与附加结果 ‣ 超越数字奖励：具有
    LLM 代理的上下文对抗贼") 结果探讨了 Cons 探索的脆弱性，过高估计了最初获胜臂的质量。
- en: 'Results: Figure[12](#A2.F12 "Figure 12 ‣ LLM Agents and Baseline Algorithms
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") Convergence
    Instability Cannot guarantee convergence as a black-box language model'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：图[12](#A2.F12 "图 12 ‣ LLM 代理与基线算法 ‣ B.2 附加实验 ‣ 附录 B 提示设计与附加结果 ‣ 超越数字奖励：具有
    LLM 代理的上下文对抗贼") 收敛不稳定，作为黑箱语言模型无法保证收敛。
- en: 'Results: Figure[2](#S4.F2 "Figure 2 ‣ 4.1 Implementation Details of Experiments
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with
    Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：图[2](#S4.F2 "图 2 ‣ 4.1 实验实施细节 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数字奖励：具有 LLM 代理的上下文对抗贼")、[8](#A2.F8
    "图 8 ‣ B.2.1 与不同指标的比较 ‣ B.2 附加实验 ‣ 附录 B 提示设计与附加结果 ‣ 超越数字奖励：具有 LLM 代理的上下文对抗贼")
    和 [11](#A2.F11 "图 11 ‣ LLM 代理与基线算法 ‣ B.2 附加实验 ‣ 附录 B 提示设计与附加结果 ‣ 超越数字奖励：具有 LLM
    代理的上下文对抗贼")
- en: 'Table 1: Summary of GPT-4 Turbo’s pros and cons in our experimental results.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：总结了 GPT-4 Turbo 在我们实验结果中的优缺点。
- en: 'Summary of Experimental Results. Table [1](#S4.T1 "Table 1 ‣ 4.2.2 Disadvantages
    of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") summarizes
    the success and failure cases of using GPT-4 Turbo as an in-context decision-maker
    for the DB problem. GPT-4 Turbo can serve as an effective decision-maker in the
    short term, by quickly identifying and exploiting the best arm with low variance
    across different instances. However, the long-term performance is hindered by
    over-estimation bias in exploration stage and lack of convergence criterion in
    exploitation stage. This highlights the need for more robust and effective strategies
    to use LLMs.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果总结。表[1](#S4.T1 "表 1 ‣ 4.2.2 LLM 代理的缺点 ‣ 4.2 实验结果 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数字奖励：具有
    LLM 代理的上下文对抗贼") 总结了使用 GPT-4 Turbo 作为上下文决策者在对抗贼问题上的成功与失败案例。GPT-4 Turbo 可以在短期内作为有效的决策者，通过快速识别和利用在不同实例中具有低方差的最佳臂。然而，长期表现受到探索阶段的过度估计偏差和利用阶段缺乏收敛标准的影响。这突显了使用
    LLM 需要更强大有效的策略。
- en: 'To further investigate the algorithmic behavior of LLMs in DB and develop more
    robust and effective strategies, we seek to answer the following questions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探究 LLM 在对抗贼中的算法行为并制定更强大有效的策略，我们寻求回答以下问题：
- en: '[Q1]  Can we develop an LLM-augmented approach for DB with a theoretical guarantee?'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[Q1] 我们能否为对抗贼开发一种具有理论保证的 LLM 增强方法？'
- en: '[Q2]  How does it perform compared to standalone LLM agents and classic DB
    algorithms?'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[Q2] 与独立的 LLM 代理和经典对抗贼算法相比，其表现如何？'
- en: 5 Algorithm-Enhanced LLMs for Dueling Bandits
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 算法增强型 LLM 适用于对抗贼
- en: 'Classic DB algorithms such as Interleaved Filter 2 (IF2) [[3](#bib.bib3)] are
    known to be near-optimal, with matching regret upper and lower bounds up to multiplicative
    constants. To address the challenges identified in Table[1](#S4.T1 "Table 1 ‣
    4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") of using LLM agents as standalone decision-makers for DB, we
    propose an LLM-augmented approach to demonstrate the possibility of taking advantage
    of both LLM agents and classic DB algorithms. Our algorithm, IF-Enhanced LLM,
    enjoys both a regret guarantee and strong empirical performance.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的数据库算法如交错滤波器2 (IF2) [[3](#bib.bib3)] 被认为是接近最优的，其匹配的遗憾上界和下界具有乘法常数。为了应对在表[1](#S4.T1
    "表 1 ‣ 4.2.2 LLM 代理的缺点 ‣ 4.2 实验结果 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博")中识别的将LLM代理作为独立决策者使用数据库所面临的挑战，我们提出了一种增强
    LLM 的方法，以展示充分利用 LLM 代理和经典数据库算法的可能性。我们的算法，IF-Enhanced LLM，具有遗憾保证和强大的实证性能。
- en: 5.1 Algorithm Design of IF-Enhanced LLM
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 IF-Enhanced LLM 的算法设计
- en: '![Refer to caption](img/d4c5f9a2601dbbe54f173d0b5e714e35.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d4c5f9a2601dbbe54f173d0b5e714e35.png)'
- en: 'Figure 3: Main components of the proposed IF-Enhanced LLM agent in Algorithm [1](#algorithm1
    "Algorithm 1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") are illustrated: (i) The blue-colored part represents the LLM phase.
    (ii) The grey-colored part indicates the IF2 phase. (iii) The Match Arms Procedure
    and IF2 Procedure are detailed in Appendix [A.1](#A1.SS1 "A.1 Detailed Procedure
    Description ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents"). (iv) The black arrows
    denote shared interactions between components. (v) The dotted arrows represent
    the input and output.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：所提出的 IF-Enhanced LLM 代理在算法 [1](#algorithm1 "算法 1 ‣ 5.1 IF-Enhanced LLM 的算法设计
    ‣ 5 增强 LLM 的对抗赌博 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博") 中的主要组件如图所示：(i) 蓝色部分代表 LLM 阶段。(ii)
    灰色部分表示 IF2 阶段。(iii) 匹配臂程序和 IF2 程序的详细说明见附录 [A.1](#A1.SS1 "A.1 详细程序描述 ‣ 附录 A IF-Enhanced
    LLM 的算法设计与分析 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博")。(iv) 黑色箭头表示组件之间的共享交互。(v) 虚线箭头表示输入和输出。
- en: In this section, we present the design intuitions of IF-Enhanced LLM. We begin
    by discussing the limitations of a naive intervention approach and the desirable
    properties for an effective LLM augmentation framework. Based on these considerations,
    we identify IF2 [[3](#bib.bib3)] as the ideal candidate algorithm to serve as
    the foundation for IF-Enhanced LLM. Finally, we illustrate the components of IF-Enhanced
    LLM and provide a detailed algorithm description.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍 IF-Enhanced LLM 的设计直觉。我们首先讨论了简单干预方法的局限性以及有效 LLM 增强框架所需的特性。基于这些考虑，我们确定
    IF2 [[3](#bib.bib3)] 是作为 IF-Enhanced LLM 基础的理想候选算法。最后，我们展示了 IF-Enhanced LLM 的组成部分，并提供了详细的算法描述。
- en: 'Limitations of Naive Intervention. A straightforward approach to addressing
    the limitations of LLMs is to use a simple if-else condition that forces the LLMs
    to converge when they appear to select two identical arms, which we call the Convergence-Triggered
    (CT) intervention strategy. However, CT fails to guarantee the selection of the
    true Condorcet winner and can reinforce local optima (see Figure[13](#A2.F13 "Figure
    13 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") in Appendix[B.2](#A2.SS2 "B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") for a failure example). This
    suggests that relying on the LLM’s internal convergence behavior for such a mechanism
    is not robust, as the LLM’s exploration is largely driven by its inherent sampling
    noise rather than a structured exploration policy. Thus, handling this limitation
    with theoretical guarantees remains challenging.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '朴素干预的局限性。解决LLM限制的一个直接方法是使用简单的if-else条件，强制LLM在它们选择两个相同臂时收敛，我们称之为收敛触发（CT）干预策略。然而，CT无法保证选择真正的康多塞赢家，并且可能会强化局部最优（见附录[B.2](#A2.SS2
    "B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")中的失败示例）。这表明，依赖LLM的内部收敛行为来实现这种机制并不稳健，因为LLM的探索主要受其固有的采样噪声驱动，而不是结构化的探索策略。因此，用理论保证来处理这一限制仍然具有挑战性。'
- en: 'Desirable Properties for LLM Augmentation. To address [Q1], we seek an algorithmic
    framework with the following properties: (i) A clear, symbolic logical structure
    that allows for easy integration with LLM suggestions. (ii) A well-defined exploration-exploitation
    trade-off that leverages the LLMs’ exploration behavior while ensuring convergence.
    (iii) Strong theoretical guarantees to maintain robustness with various prompting
    scenarios.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: LLM增强的期望属性。为了解决[Q1]，我们寻求具有以下属性的算法框架：（i）明确的符号逻辑结构，便于与LLM建议集成。（ii）明确定义的探索-开发权衡，利用LLM的探索行为同时确保收敛。（iii）强大的理论保证，以维持在各种提示场景下的稳健性。
- en: 'IF2 as an Ideal Candidate. Among classic DB algorithms, IF2 stands out as particularly
    well-suited for LLM augmentation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: IF2作为理想候选者。在经典的DB算法中，IF2尤为适合LLM增强：
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Its Explore-Then-Exploit [[4](#bib.bib4)] structure naturally aligns with the
    LLMs’ tendency to keep exploring without converging (see Figure[11](#A2.F11 "Figure
    11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")), allowing for effective leveraging the LLMs’
    exploration behavior while mitigating their exploration vulnerability and convergence
    instability (see Table [1](#S4.T1 "Table 1 ‣ 4.2.2 Disadvantages of LLM Agents
    ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '它的Explore-Then-Exploit [[4](#bib.bib4)]结构自然地与LLM不断探索而不收敛的倾向相一致（见图[11](#A2.F11
    "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")），有效地利用LLM的探索行为，同时缓解其探索脆弱性和收敛不稳定性（见表[1](#S4.T1
    "Table 1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs
    as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")）。'
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Its symbolic representation of the algorithm’s logic enables clear integration
    of LLM suggestions at specific points without disrupting the overall structure
    and theoretical guarantees. In contrast, algorithms like Self-Sparring [[7](#bib.bib7)]
    are less symbolic, making them less suitable for direct LLM augmentation.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它的算法逻辑符号表示使得在特定点上清晰地集成LLM建议而不会破坏整体结构和理论保证。相比之下，像Self-Sparring [[7](#bib.bib7)]这样的算法符号性较差，使其不适合直接进行LLM增强。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Its strong theoretical guarantees, with an expected regret bound of $O((K/\epsilon_{\mathrm{bad}})\log
    T)$ up to constants (see Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions
    and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")), and empirical performance (see Figures [2](#S4.F2
    "Figure 2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")) provide a robust foundation,
    ensuring convergence and bounded regret.'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '它具有强大的理论保证，期望的遗憾界限为 $O((K/\epsilon_{\mathrm{bad}})\log T)$，常数项除外（见附录[A.2.1](#A1.SS2.SSS1
    "A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")），以及实证表现（见图 [2](#S4.F2 "Figure
    2 ‣ 4.1 Implementation Details of Experiments ‣ 4 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    和 [8](#A2.F8 "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")），提供了一个强大的基础，确保了收敛性和有限的遗憾。'
- en: 'By selecting IF2 as the base algorithm and leveraging its Explore-Then-Exploit
    pattern, we address [Q1]. IF2’s alignment with the observed behavior of LLMs (see
    Figure [11](#A2.F11 "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary
    Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")) allows for a natural integration
    that takes advantage of the LLMs’ exploration capabilities while maintaining theoretical
    guarantees.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '通过选择 IF2 作为基础算法并利用其探索-再利用模式，我们解决了 [Q1]。IF2 与 LLMs 的观察行为的对齐（见图 [11](#A2.F11
    "Figure 11 ‣ LLM Agents and Baseline Algorithms ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")）允许自然整合，充分利用 LLMs 的探索能力，同时保持理论保证。'
- en: 'Algorithm Description. The procedures of the IF2-enhanced LLM agent are illustrated
    in Figure [3](#S5.F3 "Figure 3 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), presented in Algorithm [1](#algorithm1 "Algorithm 1 ‣ 5.1 Algorithm
    Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents") (see more details
    in Appendix[A.1](#A1.SS1 "A.1 Detailed Procedure Description ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents")). IF-Enhanced LLM maintains a confidence parameter $\delta$
    that control the algorithm’s confidence of matches between arms. The key components
    of IF-Enhanced LLM are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '算法描述。IF2 增强的 LLM 代理的过程如图 [3](#S5.F3 "Figure 3 ‣ 5.1 Algorithm Design of IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") 所示，算法 [1](#algorithm1 "Algorithm 1 ‣ 5.1 Algorithm
    Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")（详细信息见附录[A.1](#A1.SS1
    "A.1 Detailed Procedure Description ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")）。IF-增强的 LLM 维护一个置信参数 $\delta$，用于控制算法对臂之间匹配的置信度。IF-增强 LLM 的关键组成部分如下：'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Phase 1 (LLM Phase):  *Utilization of LLM recommended arms*: The algorithm
    maintains a set of candidate arms $B$ is defeated by another arm, indicating that
    the LLM’s suggestions are no longer trusted.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阶段 1（LLM 阶段）：*LLM 推荐臂的利用*：算法维护一组候选臂 $B$，如果某个臂被其他臂击败，则表示不再信任 LLM 的建议。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Phase 2 (IF2 Phase):  *Roll back to IF2*: If $b_{\mathrm{LLM}}$.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阶段 2（IF2 阶段）：*回滚到 IF2*：如果 $b_{\mathrm{LLM}}$。
- en: 'After Phase 2, the LLM-augmented agent repeats Phase 1 until $B$ only contains
    the best arm. Algorithm[1](#algorithm1 "Algorithm 1 ‣ 5.1 Algorithm Design of
    IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") and Figure[3](#S5.F3 "Figure
    3 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    summarize the phases above, with detailed procedures defined in Appendix[A.1](#A1.SS1
    "A.1 Detailed Procedure Description ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2阶段之后，增强型 LLM 代理重复第1阶段，直到 $B$ 仅包含最佳臂。算法[1](#algorithm1 "算法 1 ‣ 5.1 IF 增强型
    LLM 的算法设计 ‣ 5 对抗强盗的算法增强型 LLM ‣ 超越数字奖项：在上下文对抗强盗与 LLM 代理") 和图[3](#S5.F3 "图 3 ‣ 5.1
    IF 增强型 LLM 的算法设计 ‣ 5 对抗强盗的算法增强型 LLM ‣ 超越数字奖项：在上下文对抗强盗与 LLM 代理") 总结了上述阶段，详细程序定义在附录[A.1](#A1.SS1
    "A.1 详细程序描述 ‣ 附录 A IF 增强型 LLM 的算法设计与分析 ‣ 超越数字奖项：在上下文对抗强盗与 LLM 代理")中。
- en: Algorithm 1 LLM-augmented Agent: IF-Enhanced LLM
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 增强型 LLM 代理：IF 增强型 LLM
- en: 'Initialize :'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：
- en: $T,B=\{b_{1},\ldots,b_{K}\}$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: $T,B=\{b_{1},\ldots,b_{K}\}$
- en: while *$|B|\geq 2$* do
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当*$|B|\geq 2$*时
- en: $\textit{TrustLLM}\leftarrow\text{True}$
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: $\textit{TrustLLM}\leftarrow\text{True}$
- en: 5.2 Theoretical Guarantees for IF-Enhanced LLM
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 IF 增强型 LLM 的理论保证
- en: 'In this section, we present two main theoretical results. First, we characterize
    the vulnerability of using standalone LLM agents for dueling bandits in Assumption [1](#Thmassumption1
    "Assumption 1 (Worst-Case Behavior). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and Theorem[5.1](#S5.Thmtheorem1 "Theorem 5.1
    (Vulnerability). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"). Then, we provide the theoretical guarantees of IF-Enhanced LLM in
    Theorem[5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical
    Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), demonstrating
    its robustness and efficacy.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们呈现两个主要的理论结果。首先，我们在假设[1](#Thmassumption1 "假设 1（最坏情况行为）。 ‣ 5.2 IF 增强型
    LLM 的理论保证 ‣ 5 对抗强盗的算法增强型 LLM ‣ 超越数字奖项：在上下文对抗强盗与 LLM 代理") 和定理[5.1](#S5.Thmtheorem1
    "定理 5.1（脆弱性）。 ‣ 5.2 IF 增强型 LLM 的理论保证 ‣ 5 对抗强盗的算法增强型 LLM ‣ 超越数字奖项：在上下文对抗强盗与 LLM
    代理") 中描述了使用独立 LLM 代理进行对抗强盗的脆弱性。然后，我们提供了在定理[5.2](#S5.Thmtheorem2 "定理 5.2（预期遗憾界限）。
    ‣ 5.2 IF 增强型 LLM 的理论保证 ‣ 5 对抗强盗的算法增强型 LLM ‣ 超越数字奖项：在上下文对抗强盗与 LLM 代理") 中的 IF 增强型
    LLM 的理论保证，展示了其鲁棒性和有效性。
- en: Assumption 1  (Worst-Case Behavior).
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 1（最坏情况行为）。
- en: 'Under the original prompt (see Figure [6](#A2.F6 "Figure 6 ‣ B.1.1 Design of
    Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")), the worst-case
    behavior of an LLM agent in the dueling bandit setting is equivalent to a randomizer
    that selects actions uniformly at random.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始提示下（见图[6](#A2.F6 "图 6 ‣ B.1.1 提示设计 ‣ B.1 LLM 实验 ‣ 附录 B 提示设计与补充结果 ‣ 超越数字奖项：在上下文对抗强盗与
    LLM 代理")），LLM 代理在对抗强盗设置中的最坏情况行为相当于一个均匀随机选择动作的随机器。
- en: 'Vulnerability of Standalone LLM Agents. Inspired by the adversarial corruptions
    framework introduced in [[36](#bib.bib36)] for the classic MAB problem, we investigate
    the vulnerability of standalone LLM agents in the DB setting under adversarial
    prompts. We consider an attacker with a budget $\Phi(T)$ rounds. This adversarial
    strategy compels the LLM agent to select suboptimal arms, resulting in poor performance,
    as formalized in the following theorem with Assumption[1](#Thmassumption1 "Assumption
    1 (Worst-Case Behavior). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5
    Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 独立 LLM 代理的脆弱性。受到为经典 MAB 问题引入的对抗性破坏框架[[36](#bib.bib36)]的启发，我们研究了在对抗性提示下，独立 LLM
    代理在 DB 设置中的脆弱性。我们考虑了一个具有预算 $\Phi(T)$ 回合的攻击者。这种对抗性策略迫使 LLM 代理选择次优臂，导致性能不佳，如以下定理中与假设[1](#Thmassumption1
    "假设 1（最坏情况行为）。 ‣ 5.2 IF-Enhanced LLM 的理论保证 ‣ 5 算法增强的 LLMs 用于对抗赌博 ‣ 超越数值奖励：使用 LLM
    代理的上下文对抗赌博")中正式化的那样。
- en: Theorem 5.1  (Vulnerability).
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 5.1 （脆弱性）。
- en: For the dueling bandits problem with $K$.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 $K$ 的对抗赌博问题。
- en: 'The proof of Theorem [5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣
    5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    is provided in Appendix[A.2.2](#A1.SS2.SSS2 "A.2.2 Theoretical Guarantees of IF-Enhanced
    LLM ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced
    LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The
    theorem underscores the suboptimality of standalone LLM agents in DB problems,
    particularly when input prompts are subjected to adversarial attacks. This vulnerability
    highlights the need for a more robust approach that harnesses the power of LLMs
    while offering theoretical guarantees under diverse prompting scenarios.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 [5.1](#S5.Thmtheorem1 "定理 5.1 （脆弱性）。 ‣ 5.2 IF-Enhanced LLM 的理论保证 ‣ 5 算法增强的
    LLMs 用于对抗赌博 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博") 的证明见附录[A.2.2](#A1.SS2.SSS2 "A.2.2 IF-Enhanced
    LLM 的理论保证 ‣ A.2 理论分析 ‣ 附录 A IF-Enhanced LLM 的算法设计与分析 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博")。该定理强调了在
    DB 问题中独立 LLM 代理的次优性，尤其是当输入提示遭受对抗性攻击时。这种脆弱性突显了需要一种更强健的方法，该方法既利用 LLM 的力量，又在多种提示场景下提供理论保证。
- en: 'Expected Regret Bounds of IF-Enhanced LLM. Following the algorithm design of
    IF-Enhanced LLM introduced in Section[5.1](#S5.SS1 "5.1 Algorithm Design of IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), IF-Enhanced LLM inherits the theoretical guarantees
    of IF2 (see Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas
    for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and
    Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")), while leveraging the benefits of LLM-enhanced exploration.
    Specifically, IF-Enhanced LLM offers the following theoretical guarantee:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: IF-Enhanced LLM 的期望悔恨界限。根据第[5.1](#S5.SS1 "5.1 IF-Enhanced LLM 算法设计 ‣ 5 算法增强的
    LLMs 用于对抗赌博 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博")节中介绍的 IF-Enhanced LLM 算法设计，IF-Enhanced
    LLM 继承了 IF2 的理论保证（见附录[A.2.1](#A1.SS2.SSS1 "A.2.1 对抗赌博的有用假设和引理 ‣ A.2 理论分析 ‣ 附录
    A IF-Enhanced LLM 的算法设计与分析 ‣ 超越数值奖励：使用 LLM 代理的上下文对抗赌博")），同时利用了 LLM 增强探索的好处。具体而言，IF-Enhanced
    LLM 提供了以下理论保证：
- en: Theorem 5.2  (Expected Regret Bounds).
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 5.2 （期望悔恨界限）。
- en: Suppose after $T_{\mathrm{LLM}}$, the expected strong regret of IF-Enhanced
    LLM satisfies that
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在 $T_{\mathrm{LLM}}$ 之后，IF-Enhanced LLM 的期望强悔恨满足
- en: '|  | $1$2 |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: and the expected weak regret of IF-Enhanced LLM is bounded from above by
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: IF-Enhanced LLM 的期望弱悔恨上界为
- en: '|  | $\displaystyle\mathbb{E}\left[\mathsf{WR}(\textsc{IF-Enhanced LLM})\right]\leq\min\left\{O(T_{\mathrm{LLM}}),\widetilde{O}\left(\frac{K\log
    K}{\epsilon_{1,2}}\log T\right)\right\},$ |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[\mathsf{WR}(\textsc{IF-Enhanced LLM})\right]\leq\min\left\{O(T_{\mathrm{LLM}}),\widetilde{O}\left(\frac{K\log
    K}{\epsilon_{1,2}}\log T\right)\right\},$ |  |'
- en: where $\widetilde{O}(\cdot)$.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{O}(\cdot)$。
- en: 'The proof of Theorem [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds).
    ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    is provided in Appendix[A.2.2](#A1.SS2.SSS2 "A.2.2 Theoretical Guarantees of IF-Enhanced
    LLM ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced
    LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The
    required assumptions are precisely stated in Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1
    Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣
    Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"). Theorem [5.2](#S5.Thmtheorem2 "Theorem
    5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM
    ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") establishes the efficacy and robustness of IF-Enhanced
    LLM:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 定理[5.2](#S5.Thmtheorem2 "定理 5.2 (期望遗憾界限)。 ‣ 5.2 IF-Enhanced LLM的理论保证 ‣ 5 算法增强的LLM用于对抗性赌博
    ‣ 超越数值奖励：与LLM代理的上下文对抗性赌博")的证明在附录[A.2.2](#A1.SS2.SSS2 "A.2.2 IF-Enhanced LLM的理论保证
    ‣ A.2 理论分析 ‣ 附录 A IF-Enhanced LLM的算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗性赌博")中提供。所需假设在附录[A.2.1](#A1.SS2.SSS1
    "A.2.1 对抗性赌博的有用假设和引理 ‣ A.2 理论分析 ‣ 附录 A IF-Enhanced LLM的算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗性赌博")中精确陈述。定理[5.2](#S5.Thmtheorem2
    "定理 5.2 (期望遗憾界限)。 ‣ 5.2 IF-Enhanced LLM的理论保证 ‣ 5 算法增强的LLM用于对抗性赌博 ‣ 超越数值奖励：与LLM代理的上下文对抗性赌博")确立了IF-Enhanced
    LLM的效果和稳健性。
- en: 'Efficacy: Both the strong and weak regret for IF-Enhanced LLM are guaranteed
    to satisfy a worst-case upper bound of $\widetilde{O}\left(({K\log K}/{\epsilon_{1,2}})\log
    T\right)$.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 效果：IF-Enhanced LLM的强遗憾和弱遗憾都保证满足最坏情况上界为$\widetilde{O}\left(({K\log K}/{\epsilon_{1,2}})\log
    T\right)$。
- en: 'Robustness: The worst-case upper bounds on the strong and weak regret hold
    regardless of the specific prompting scenario, ensuring that IF-Enhanced LLM maintains
    its theoretical guarantees even in the presence of noisy or adversarial prompts,
    as considered in Theorem[5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣
    5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").
    This robustness is particularly important in practical applications, where the
    prompts provided to the LLM agent may be subject to various uncertainties or manipulations.
    By leveraging the strengths of LLMs while incorporating a robust algorithmic framework,
    IF-Enhanced LLM offers a reliable and theoretically grounded solution for DB problems
    in real-world settings, where the prompting quality may not always be optimal.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健性：强遗憾和弱遗憾的最坏情况上界在特定提示场景下保持不变，确保IF-Enhanced LLM即使在存在噪声或对抗性提示的情况下也能保持其理论保证，这在定理[5.1](#S5.Thmtheorem1
    "定理 5.1 (脆弱性)。 ‣ 5.2 IF-Enhanced LLM的理论保证 ‣ 5 算法增强的LLM用于对抗性赌博 ‣ 超越数值奖励：与LLM代理的上下文对抗性赌博")中有所考虑。这种稳健性在实际应用中尤为重要，因为提供给LLM代理的提示可能会受到各种不确定性或操控。通过利用LLMs的优势并结合稳健的算法框架，IF-Enhanced
    LLM在现实世界设置中为DB问题提供了一个可靠且理论基础扎实的解决方案，在这些设置中，提示质量可能并不总是最优的。
- en: 5.3 Empirical Evaluation of IF-Enhanced LLM
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 IF-Enhanced LLM的实证评估
- en: Regarding [Q2], we design a two-fold evaluation to assess efficacy and robustness.
    The evaluation is conducted on the Easy instance, which provides higher distinguishability,
    allowing us to observe convergence and regret differences within a practical number
    of steps. First, we compare the strong regret of IF-Enhanced LLM against state-of-the-art
    baseline algorithms to validate its efficacy. Second, we investigate the robustness
    of IF-Enhanced LLM with noisy and adversarial prompts.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[Q2]，我们设计了一个双重评估来评估效果和稳健性。评估在Easy实例上进行，该实例提供了更高的可区分性，使我们能够观察到实际步数中的收敛和遗憾差异。首先，我们将IF-Enhanced
    LLM的强遗憾与最先进的基线算法进行比较，以验证其效果。其次，我们研究了IF-Enhanced LLM在噪声和对抗性提示下的稳健性。
- en: '5.3.1 Efficacy Evaluation: Strong Regret and Weak Regret'
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 效果评估：强遗憾和弱遗憾
- en: '![Refer to caption](img/f37b49fb96bdba081c63143b3279ceda.png)![Refer to caption](img/b133b5d77bb7123a073a72d40a37d598.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f37b49fb96bdba081c63143b3279ceda.png)![参见说明](img/b133b5d77bb7123a073a72d40a37d598.png)'
- en: 'Figure 4: Comparisons between IF-Enhanced LLM, GPT-4 Turbo, IF2, and state-of-the-art
    baseline algorithms (Self-Sparring and DTS). Left and Right: strong and weak regret
    on the Easy instance.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：IF增强型LLM、GPT-4 Turbo、IF2以及最先进基线算法（Self-Sparring和DTS）之间的比较。左侧和右侧：简单实例上的强遗憾和弱遗憾。
- en: 'Hyper-parameters. In our implementation of IF-Enhanced LLM (see Algorithm[1](#algorithm1
    "Algorithm 1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")), there are two hyper-parameters: the threshold parameter $t$ provided
    the best performance in terms of cumulative regret. These hyper-parameter values
    strike a balance between the number of comparisons required to identify the best
    arm and the confidence level for pruning suboptimal arms, enabling IF-Enhanced
    LLM to efficiently explore and exploit the available arms in the dueling bandits
    setting.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数。在我们对IF增强型LLM的实现中（参见算法[1](#algorithm1 "算法 1 ‣ 5.1 IF增强型LLM的算法设计 ‣ 5 算法增强型LLM用于对抗赌博机
    ‣ 超越数值奖励：具有LLM代理的上下文对抗赌博机")），有两个超参数：阈值参数$t$在累积遗憾方面表现最佳。这些超参数值在识别最佳臂所需的比较次数和修剪次优臂的置信水平之间取得了平衡，使得IF增强型LLM能够在对抗赌博机设置中高效地探索和利用可用的臂。
- en: 'We evaluate the cumulative strong and weak regret performance of the proposed
    IF-Enhanced LLM with different confidence parameter settings ($\delta=0.1,0.2,0.4$:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了在不同置信参数设置（$\delta=0.1,0.2,0.4$）下提出的IF增强型LLM的累积强遗憾和弱遗憾表现。
- en: 'Strong Regret. Figure[4](#S5.F4 "Figure 4 ‣ 5.3.1 Efficacy Evaluation: Strong
    Regret and Weak Regret ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") (Left) demonstrates that IF-Enhanced LLM exhibits competitive performance
    across different $\delta$ allows for more aggressive exploration, enabling IF-Enhanced
    LLM to identify the optimal arm sooner, leading to reduced cumulative strong regret.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 强遗憾。图[4](#S5.F4 "图 4 ‣ 5.3.1 效能评估：强遗憾和弱遗憾 ‣ 5.3 IF增强型LLM的实证评估 ‣ 5 算法增强型LLM用于对抗赌博机
    ‣ 超越数值奖励：具有LLM代理的上下文对抗赌博机")（左侧）展示了IF增强型LLM在不同的$\delta$下表现出竞争力，允许更具攻击性的探索，使得IF增强型LLM能够更早地识别最佳臂，从而减少累积强遗憾。
- en: 'Weak Regret. Figure[4](#S5.F4 "Figure 4 ‣ 5.3.1 Efficacy Evaluation: Strong
    Regret and Weak Regret ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") (Right) showcases the superior weak regret performance of IF-Enhanced
    LLM. When $\delta=0.2$, IF-Enhanced LLM consistently achieves lower cumulative
    weak regret, demonstrating its ability to effectively identify and include the
    optimal arm in its comparisons. As emphasized in Table[1](#S4.T1 "Table 1 ‣ 4.2.2
    Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents"), this can be attributed to GPT-4 Turbo’s ability to consistently
    identify the best arm.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 弱遗憾。图[4](#S5.F4 "图 4 ‣ 5.3.1 效能评估：强遗憾和弱遗憾 ‣ 5.3 IF增强型LLM的实证评估 ‣ 5 算法增强型LLM用于对抗赌博机
    ‣ 超越数值奖励：具有LLM代理的上下文对抗赌博机")（右侧）展示了IF增强型LLM在弱遗憾方面的优越表现。当$\delta=0.2$时，IF增强型LLM始终实现较低的累积弱遗憾，证明其能够有效地识别并包含最佳臂进行比较。正如表[1](#S4.T1
    "表 1 ‣ 4.2.2 LLM代理的缺点 ‣ 4.2 实验结果 ‣ 4 作为独立上下文决策者的LLM ‣ 超越数值奖励：具有LLM代理的上下文对抗赌博机")所强调的，这归因于GPT-4
    Turbo能够始终如一地识别最佳臂。
- en: '5.3.2 Robustness Evaluation: Noisy and Adversarial Prompts'
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 稳健性评估：噪声和对抗性提示
- en: '![Refer to caption](img/0054e53e36684dc1a53e1da4ec3efd27.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0054e53e36684dc1a53e1da4ec3efd27.png)'
- en: 'Figure 5: Robustness evaluation under prompt perturbations. The specific noisy
    (biased history) and adversarial (reversed goal) prompts are shown in Figure [7](#A2.F7
    "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在提示扰动下的稳健性评估。具体的噪声（偏见历史）和对抗性（反向目标）提示如图[7](#A2.F7 "图 7 ‣ B.1.1 提示设计 ‣ B.1
    LLM实验 ‣ 附录B 提示设计与补充结果 ‣ 超越数值奖励：具有LLM代理的上下文对抗赌博机")所示。
- en: 'Recent studies [[2](#bib.bib2), [37](#bib.bib37)] have emphasized the importance
    of varying prompts to elicit the desired behavior from LLMs in decision-making
    tasks, highlighting the potential limitations of prompt quality. Results obtained
    from a single prompt template may lead to unreliable conclusions that cannot generalize
    to real-world situations where optimal prompts are often unavailable. Thus, we
    evaluate the robustness of IF-Enhanced LLM in DB tasks by employing two types
    of prompt perturbations (see Figure[7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts
    ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")) along with the original
    prompt (see Figure [6](#A2.F6 "Figure 6 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的研究 [[2](#bib.bib2), [37](#bib.bib37)] 强调了变更提示以引发 LLM 在决策任务中期望行为的重要性，突出提示质量的潜在局限性。单一提示模板获得的结果可能导致不可靠的结论，这些结论无法推广到现实世界中最优提示通常不可用的情况。因此，我们通过使用两种类型的提示扰动（见图
    [7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")）以及原始提示（见图 [6](#A2.F6 "Figure 6 ‣ B.1.1 Design
    of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")）来评估
    IF-Enhanced LLM 在 DB 任务中的稳健性。'
- en: 'Original Prompt. Under the initial prompt, IF-Enhanced LLM leverages the LLM’s
    ability to quickly identify the best arm through exploration. As shown in Figures
    [14](#A2.F14 "Figure 14 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and [15](#A2.F15 "Figure 15 ‣ IF-Enhanced LLM
    ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") (Top Row),
    we observe that: (i) Compared to IF2, IF-Enhanced LLM benefits from the LLM’s
    exploration ability by initializing with the best arm as the incumbent when entering
    the IF2 phase. (ii) Compared to GPT-4 Turbo, convergence to the Condorcet winner
    is guaranteed for IF-Enhanced LLM with high probability.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '原始提示。在初始提示下，IF-Enhanced LLM 利用 LLM 通过探索快速识别最佳臂的能力。如图 [14](#A2.F14 "Figure 14
    ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") 和 [15](#A2.F15 "Figure 15 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")（顶部行）所示，我们观察到： (i) 与 IF2 相比，IF-Enhanced
    LLM 通过在进入 IF2 阶段时用最佳臂初始化来利用 LLM 的探索能力。 (ii) 与 GPT-4 Turbo 相比，IF-Enhanced LLM 以高概率确保收敛到
    Condorcet 获胜者。'
- en: 'Biased History. We inject an incorrect history into the prompt, where each
    non-optimal arm initially wins against the best arm 10 times, while keeping the
    underlying preference matrix unchanged. In this scenario, standalone LLM agents
    are observed to get trapped in local optima for extended periods. In contrast,
    IF-Enhanced LLM overcomes this limitation by employing uniform comparisons in
    the IF2 phase to escape such suboptimal exploration modes (see Figure [15](#A2.F15
    "Figure 15 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") (Middle Row)). This effectively addresses the local optima problem
    highlighted in the disadvantages of standalone LLMs in Table[1](#S4.T1 "Table
    1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results ‣ 4 LLMs as Standalone
    In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '偏倚历史。我们向提示中注入了不正确的历史，其中每个非最优臂最初击败最佳臂 10 次，同时保持底层偏好矩阵不变。在这种情况下，观察到独立的 LLM 代理被困在局部最优解中很长时间。相比之下，IF-Enhanced
    LLM 通过在 IF2 阶段采用均匀比较来克服这一局限性，从而摆脱这种次优探索模式（见图 [15](#A2.F15 "Figure 15 ‣ IF-Enhanced
    LLM ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")（中间行））。这有效解决了表
    [1](#S4.T1 "Table 1 ‣ 4.2.2 Disadvantages of LLM Agents ‣ 4.2 Experimental results
    ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") 中指出的独立 LLM 的局部最优问题。'
- en: 'Reversed Goal. When the prompt is adversarially modified from maximizing reward
    to minimizing, the LLM consistently recommends non-optimal arms after its exploration
    stage. Nevertheless, even with adversarial prompts, IF-Enhanced LLM still maintains
    a robust behavior and achieves near-optimal cumulative strong regret, as shown
    in Figure [15](#A2.F15 "Figure 15 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") (Bottom Row). Since the LLM’s exploration
    capability is only utilized within the bounded length of the Match Arms procedure,
    the impact of the reversed goal on the exploitation phase is eliminated.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '反向目标。当提示从最大化奖励调整为最小化时，LLM 在探索阶段后持续推荐次优的臂。然而，即便在对抗性提示下，**IF-Enhanced LLM** 仍保持稳健的行为，并实现接近最优的累积强后悔，如图
    [15](#A2.F15 "Figure 15 ‣ IF-Enhanced LLM ‣ B.2 Supplementary Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")（底部行）所示。由于 LLM 的探索能力仅在 Match Arms 过程的有限长度内得到利用，反向目标对利用阶段的影响被消除。'
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.3.2 Robustness Evaluation: Noisy and Adversarial
    Prompts ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") presents the cumulative strong regret results comparing IF-Enhanced LLM
    against standalone LLM agents and the IF2 algorithm across three prompt designs.
    Notably, IF-Enhanced LLM with $\delta=1/(TK^{2})$ (consistent with IF2 to showcase
    the robust behavior) achieves near-optimal cumulative regret with low variance
    even with noisy and adversarial prompts, validating the regret bounds stated in
    Theorem [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical
    Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The experiment
    in this section is extended from 2000 steps to 3000 steps to illustrate GPT-4
    Turbo’s non-convergence and growing regret. IF-Enhanced LLM and IF2 converge to
    the best arm within 2000 steps, while GPT-4 Turbo’s cumulative expected regret
    continues to increase, indicating the instability of standalone LLM agents.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#S5.F5 "Figure 5 ‣ 5.3.2 Robustness Evaluation: Noisy and Adversarial
    Prompts ‣ 5.3 Empirical Evaluation of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") 展示了 IF-Enhanced LLM 与独立 LLM 代理以及 IF2 算法在三种提示设计下的累积强后悔结果。值得注意的是，**IF-Enhanced
    LLM** 在 $\delta=1/(TK^{2})$（与 IF2 一致以展示稳健行为）下，即便在有噪声和对抗性提示下，也能实现接近最优的累积后悔且方差低，这验证了定理
    [5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical
    Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") 中所述的后悔界限。本节实验从
    2000 步扩展到 3000 步，以说明 GPT-4 Turbo 的非收敛性和不断增长的后悔。**IF-Enhanced LLM** 和 IF2 在 2000
    步内收敛到最佳臂，而 GPT-4 Turbo 的累积期望后悔持续增加，表明独立 LLM 代理的不稳定性。'
- en: 6 Conclusion
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This work evaluates LLM agents as in-context decision-makers for the problem
    of dueling bandits (DB) through case studies. We find that the top-performing
    LLM (GPT-4 Turbo) exhibits strong capabilities in identifying the Condorcet winner
    and achieving low weak regret while maintaining minimal variance. Yet, stemming
    from the fact that LLMs are primarily designed and trained for word token prediction
    rather than optimizing decision-making objectives, GPT-4 Turbo struggles to consistently
    converge to a single optimal solution in the long run and is vulnerable to prompt
    design variations. To address these limitations, we propose IF-Enhanced LLM, an
    LLM-augmented approach that takes advantages of both LLMs and classic DB algorithms,
    providing theoretical guarantees and robust empirical performance even with noisy
    and adversarial prompts. Our findings shed light on the capabilities and challenges
    of LLMs as in-context decision-makers, and pave the way for the employment of
    LLMs in complex decision-making tasks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究通过案例研究评估 LLM 代理作为上下文决策者在对抗性 bandits（DB）问题中的表现。我们发现，表现最佳的 LLM（GPT-4 Turbo）在识别
    Condorcet 胜者和实现低弱后悔方面表现出强大的能力，同时保持最低的方差。然而，由于 LLM 主要设计和训练用于词汇预测而非优化决策目标，GPT-4
    Turbo 在长期内难以持续收敛到单一最优解，并且容易受到提示设计变化的影响。为了解决这些限制，我们提出了**IF-Enhanced LLM**，这是一种增强的
    LLM 方法，结合了 LLM 和经典 DB 算法的优点，即使在有噪声和对抗性提示下也能提供理论保证和稳健的实证表现。我们的发现阐明了 LLM 作为上下文决策者的能力和挑战，并为在复杂决策任务中使用
    LLM 铺平了道路。
- en: 'Limitations and Future Works. Due to computational constraints and financial
    costs, our case studies focus on a relatively small number of arms ($K=5$). To
    address this limitation, it is worth investigating the scalability of the proposed
    approach to larger decision spaces with more arms as certified by Theorem [5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"). In addition, while IF2 is employed as a representative
    Explore-Then-Exploit[[18](#bib.bib18)] algorithm to design an LLM-augmented algorithm
    in this work, utilizing other ongoing regret-minimization algorithms, such as
    RUCB [[18](#bib.bib18)], would be an interesting future direction. Moreover, considering
    more complex preference structures, such as non-transitive preferences, would
    provide insights on algorithm design. Investigating other variants of dueling
    bandits, including contextual dueling bandits, multi-dueling bandits, and adversarial
    dueling bandits, would further expand the applicability of this work. Finally,
    we have only experimented with some of the models available to us as of May 2024,
    selecting high-profile closed-source models over open-source models. Exploring
    the performance of a wider range of LLMs, including open-source models, would
    provide a more comprehensive understanding of the capabilities and limitations
    of LLMs in the dueling bandits setting.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 限制与未来工作。由于计算约束和财务成本，我们的案例研究集中在相对少量的臂上（$K=5$）。为了解决这一限制，值得研究所提出方法在具有更多臂的大型决策空间中的可扩展性，如定理[5.2](#S5.Thmtheorem2
    "定理 5.2 (预期遗憾界限)。 ‣ 5.2 IF增强型 LLM 的理论保证 ‣ 5 算法增强型 LLM 适用于对抗性拉霸 ‣ 超越数值奖励：具有 LLM
    代理的上下文对抗性拉霸")所证实。此外，尽管 IF2 被用作代表性 Explore-Then-Exploit[[18](#bib.bib18)] 算法以设计本文中的
    LLM 增强算法，使用其他正在进行的遗憾最小化算法，例如 RUCB [[18](#bib.bib18)]，将是一个有趣的未来方向。此外，考虑更复杂的偏好结构，例如非传递性偏好，将为算法设计提供见解。研究其他变体的对抗性拉霸，包括上下文对抗性拉霸、多对抗性拉霸和对抗性对抗性拉霸，将进一步扩展这项工作的适用性。最后，我们仅对截至
    2024 年 5 月时可用的一些模型进行了实验，选择了高知名度的闭源模型而非开源模型。探索更广泛的 LLM 性能，包括开源模型，将提供对 LLM 在对抗性拉霸环境中的能力和局限性的更全面的理解。
- en: Broader Impacts. The ability of LLMs to encode complex contextual information
    enables their application in various decision-making scenarios beyond the scope
    of traditional DB algorithms. This could lead to more informed and context-aware
    decision-making in domains such as healthcare, finance, and recommender systems,
    where incorporating user feedback and adaptivity is crucial for handling dynamic
    relative preferences. However, the substantial computational resources required
    to train and deploy large-scale LLMs for decision-making tasks raise concerns
    about energy consumption and environmental impact, especially when dealing with
    a large set of arms.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛的影响。LLM 编码复杂上下文信息的能力使其可以应用于超出传统 DB 算法范围的各种决策场景。这可能会导致在医疗、金融和推荐系统等领域的决策更加信息化和上下文敏感，其中纳入用户反馈和适应性对于处理动态相对偏好至关重要。然而，训练和部署大规模
    LLM 进行决策任务所需的巨大的计算资源引发了关于能源消耗和环境影响的担忧，特别是在处理大量臂时。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang. Do llm agents
    have regret? a case study in online learning and games. arXiv preprint arXiv:2403.16843,
    2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, 和 Kaiqing Zhang。**LLM** 代理是否有遗憾？一个在线学习和游戏的案例研究。arXiv
    预印本 arXiv:2403.16843, 2024。'
- en: '[2] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, and Aleksandrs
    Slivkins. Can large language models explore in-context? arXiv preprint arXiv:2403.15371,
    2024.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, 和 Aleksandrs
    Slivkins。大语言模型能在上下文中探索吗？arXiv 预印本 arXiv:2403.15371, 2024。'
- en: '[3] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The
    k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):1538--1556,
    2012.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yisong Yue, Josef Broder, Robert Kleinberg, 和 Thorsten Joachims。k-臂对抗性拉霸问题。计算机与系统科学杂志，78(5):1538--1556,
    2012。'
- en: '[4] Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and Remi Munos. Relative
    confidence sampling for efficient on-line ranker evaluation. In Proceedings of
    the 7th ACM international conference on Web search and data mining, pages 73--82,
    2014.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, 和 Remi Munos。用于高效在线排序评估的相对置信度采样。发表于第七届
    ACM 国际网络搜索与数据挖掘会议论文集，页 73--82, 2014。'
- en: '[5] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University
    Press, 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 托尔·拉特莫尔和查巴·谢佩斯瓦里。《赌博算法》。剑桥大学出版社，2020。'
- en: '[6] Yisong Yue and Thorsten Joachims. Interactively optimizing information
    retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual
    International Conference on Machine Learning, pages 1201--1208, 2009.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 岳一松和托斯滕·乔希姆斯。《将信息检索系统的交互优化视作对战赌博问题》。在第26届国际机器学习大会会议论文集中，第1201--1208页，2009。'
- en: '[7] Yanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue. Multi-dueling
    bandits with dependent arms. arXiv preprint arXiv:1705.00253, 2017.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 隋亚楠、庄文杰、乔尔·W·伯迪克和岳一松。《多对战赌博者与相关臂》。arXiv 预印本 arXiv:1705.00253，2017。'
- en: '[8] Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing dueling bandits
    to cardinal bandits. In International Conference on Machine Learning, pages 856--864\.
    PMLR, 2014.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 尼尔·艾隆、佐哈尔·卡尼和托斯滕·乔希姆斯。《将对战赌博问题简化为基数赌博问题》。在国际机器学习大会上，第856--864页。PMLR，2014。'
- en: '[9] Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both-world
    analyses for online learning from preferences. In ICML 2022-39th International
    Conference on Machine Learning, pages 1--25, 2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 阿迪鲁帕·萨哈和皮埃尔·盖雅尔。《多功能对战赌博者：在线学习偏好的最佳双重分析》。在 ICML 2022-第39届国际机器学习大会上，第1--25页，2022。'
- en: '[10] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
    Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize
    with human feedback. Advances in Neural Information Processing Systems, 33:3008--3021,
    2020.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 尼桑·斯提农、龙欧阳、杰弗里·吴、丹尼尔·齐格勒、瑞安·洛、切尔西·沃斯、亚历克·拉德福德、达里奥·阿莫代和保罗·F·克里斯蒂亚诺。《通过人类反馈学习总结》。神经信息处理系统进展，33:3008--3021，2020。'
- en: '[11] Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult than standard
    rl? a theoretical perspective. Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 汪元浩、刘青华和金驰。《RLHF 比标准 RL 更困难吗？一种理论视角》。神经信息处理系统进展，36，2024。'
- en: '[12] Roy H Perlis, Joseph F Goldberg, Michael J Ostacher, and Christopher D
    Schneck. Clinical decision support for bipolar depression using large language
    models. Neuropsychopharmacology, pages 1--5, 2024.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 罗伊·H·佩尔利斯、约瑟夫·F·戈德伯格、迈克尔·J·奥斯塔赫和克里斯托弗·D·施耐克。《使用大语言模型的双相抑郁症临床决策支持》。神经心理药理学，第1--5页，2024。'
- en: '[13] Cong Jiang and Xiaolei Yang. Legal syllogism prompting: Teaching large
    language models for legal judgment prediction. In Proceedings of the Nineteenth
    International Conference on Artificial Intelligence and Law, pages 417--421, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 江聪和杨小磊。《法律三段论提示：教大型语言模型进行法律判决预测》。在第十九届国际人工智能与法律大会会议论文集中，第417--421页，2023。'
- en: '[14] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. The
    wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement
    prediction challenges. arXiv preprint arXiv:2304.05351, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 谢前前、韩伟光、赖彦昭、彭敏和黄季敏。《华尔街新手：对 ChatGPT 在多模态股票走势预测挑战中的零-shot 分析》。arXiv 预印本
    arXiv:2304.05351，2023。'
- en: '[15] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
    Iwasawa. Large language models are zero-shot reasoners. Advances in neural information
    processing systems, 35:22199--22213, 2022.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 小岛武、郭士祥·肖恩、麦克尔·里德、松尾丰和岩泽祐介。《大型语言模型是零-shot 推理者》。神经信息处理系统进展，35:22199--22213，2022。'
- en: '[16] Huasen Wu and Xin Liu. Double thompson sampling for dueling bandits. Advances
    in neural information processing systems, 29, 2016.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 吴华森和刘欣。《对战赌博问题的双重汤普森抽样》。神经信息处理系统进展，29，2016。'
- en: '[17] Yisong Yue and Thorsten Joachims. Beat the mean bandit. In Proceedings
    of the 28th international conference on machine learning (ICML-11), pages 241--248\.
    Citeseer, 2011.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 岳一松和托斯滕·乔希姆斯。《战胜平均赌博者》。在第28届国际机器学习大会（ICML-11）上，第241--248页。Citeseer，2011。'
- en: '[18] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative
    upper confidence bound for the k-armed dueling bandit problem. In International
    conference on machine learning, pages 10--18\. PMLR, 2014.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 马斯鲁尔·佐赫、希蒙·怀特森、雷米·穆诺斯和马滕·瑞克。《k-臂对战赌博问题的相对上置信界限》。在国际机器学习大会上，第10--18页。PMLR，2014。'
- en: '[19] Maegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui, Yisong Yue,
    Joel W Burdick, and Aaron D Ames. Preference-based learning for exoskeleton gait
    optimization. In 2020 IEEE international conference on robotics and automation
    (ICRA), pages 2351--2357\. IEEE, 2020.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 梅根·塔克、艾伦·诺沃塞勒、克劳迪亚·坎、隋亚楠、岳一松、乔尔·W·伯迪克和亚伦·D·艾姆斯。《基于偏好的学习用于外骨骼步态优化》。在2020
    IEEE 国际机器人与自动化会议（ICRA）上，第2351--2357页。IEEE，2020。'
- en: '[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in neural information processing systems, 35:27730--27744, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人。训练语言模型以遵循人类反馈的指令。神经信息处理系统进展，35:27730--27744，2022年。'
- en: '[21] Ali Baheri and Cecilia O Alm. Llms-augmented contextual bandit. arXiv
    preprint arXiv:2311.02268, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Ali Baheri 和 Cecilia O Alm。大型语言模型增强的上下文赌博机。arXiv 预印本 arXiv:2311.02268，2023年。'
- en: '[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824--24837,
    2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou 等人。思维链提示引发大型语言模型的推理。神经信息处理系统进展，35:24824--24837，2022年。'
- en: '[23] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
    reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery 和 Denny Zhou。自我一致性改善语言模型中的思维链推理。arXiv 预印本 arXiv:2203.11171，2022年。'
- en: '[24] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi
    Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most
    prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625,
    2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi
    Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le 等人。最少到最多提示使大型语言模型能够进行复杂推理。arXiv
    预印本 arXiv:2205.10625，2022年。'
- en: '[25] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan
    Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with
    large language models. Advances in Neural Information Processing Systems, 36,
    2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan
    Cao 和 Karthik Narasimhan。思想树：使用大型语言模型进行深思熟虑的问题解决。神经信息处理系统进展，36，2024年。'
- en: '[26] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,
    Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue:
    Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608,
    2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence,
    Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar 等人。内心独白：通过语言模型进行具身推理与规划。arXiv
    预印本 arXiv:2207.05608，2022年。'
- en: '[27] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    and Zhiting Hu. Reasoning with language model is planning with world model. arXiv
    preprint arXiv:2305.14992, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang
    和 Zhiting Hu。用语言模型进行推理就是用世界模型进行规划。arXiv 预印本 arXiv:2305.14992，2023年。'
- en: '[28] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander
    Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do
    as i can, not as i say: Grounding language in robotic affordances. In Conference
    on robot learning, pages 287--318\. PMLR, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander
    Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian 等人。按我能做的，不是我说的做：将语言与机器人能力结合。在机器人学习会议中，第287--318页。PMLR，2023年。'
- en: '[29] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani,
    Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level
    reward design via coding large language models. arXiv preprint arXiv:2310.12931,
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani,
    Dinesh Jayaraman, Yuke Zhu, Linxi Fan 和 Anima Anandkumar。Eureka：通过编码大型语言模型实现人类级奖励设计。arXiv
    预印本 arXiv:2310.12931，2023年。'
- en: '[30] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan 和 Anima Anandkumar。探险者：一个基于大型语言模型的开放式具身体智能体。arXiv 预印本 arXiv:2305.16291，2023年。'
- en: '[31] Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela van der Schaar.
    Large language models to enhance bayesian optimization. arXiv preprint arXiv:2402.03921,
    2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Tennison Liu, Nicolás Astorga, Nabeel Seedat 和 Mihaela van der Schaar。利用大型语言模型提升贝叶斯优化。arXiv
    预印本 arXiv:2402.03921，2024年。'
- en: '[32] Tanguy Urvoy, Fabrice Clerot, Raphael Féraud, and Sami Naamane. Generic
    exploration and k-armed voting bandits. In International conference on machine
    learning, pages 91--99\. PMLR, 2013.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Tanguy Urvoy, Fabrice Clerot, Raphael Féraud 和 Sami Naamane。通用探索与 k 臂投票强盗。在国际机器学习会议中，第91--99页。PMLR，2013年。'
- en: '[33] Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret
    lower bound and optimal algorithm in dueling bandit problem. In Conference on
    learning theory, pages 1141--1154\. PMLR, 2015.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Junpei Komiyama, Junya Honda, Hisashi Kashima, 和 Hiroshi Nakagawa. 对抗性赌博问题中的遗憾下界与最优算法。在学习理论会议上，页码1141--1154。PMLR，2015年。'
- en: '[34] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block
    designs: I. the method of paired comparisons. Biometrika, 39(3/4):324--345, 1952.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Ralph Allan Bradley 和 Milton E Terry. 不完全区组设计的等级分析：I. 配对比较方法。《生物统计学》，39(3/4):324--345，1952年。'
- en: '[35] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical
    Society Series C: Applied Statistics, 24(2):193--202, 1975.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Robin L Plackett. 排列分析。《皇家统计学会C系列期刊：应用统计》，24(2):193--202，1975年。'
- en: '[36] Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John Lui, Wing Shing Wong,
    et al. Adversarial bandits with corruptions: Regret lower bound and no-regret
    algorithm. Advances in Neural Information Processing Systems, 33:19943--19952,
    2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John Lui, Wing Shing Wong
    等. 带有干扰的对抗性赌博：遗憾下界和无遗憾算法。神经信息处理系统进展，33:19943--19952，2020年。'
- en: '[37] Manikanta Loya, Divya Anand Sinha, and Richard Futrell. Exploring the
    sensitivity of llms’ decision-making capabilities: Insights from prompt variation
    and hyperparameters. arXiv preprint arXiv:2312.17476, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Manikanta Loya, Divya Anand Sinha, 和 Richard Futrell. 探索 LLM 决策能力的敏感性：来自提示变化和超参数的见解。arXiv
    预印本 arXiv:2312.17476，2023年。'
- en: Appendix
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'This appendix provides supplementary information and additional experimental
    results to support the main text. The content is organized into two main parts:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录提供了补充信息和额外的实验结果，以支持正文内容。内容分为两个主要部分：
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Theoretical Part: Algorithm Design and Analysis of IF-Enhanced LLM'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论部分：IF-Enhanced LLM 的算法设计与分析
- en: –
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Appendix [A.1](#A1.SS1 "A.1 Detailed Procedure Description ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents") describes the IF-Enhanced LLM algorithm stated in Section[5.1](#S5.SS1
    "5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),
    detailing its key features and implementation remarks.'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [A.1](#A1.SS1 "A.1 详细程序描述 ‣ 附录 A IF-Enhanced LLM 的算法设计与分析 ‣ 超越数字奖励：LLM 代理的上下文对抗性赌博")
    详细描述了第[5.1](#S5.SS1 "5.1 IF-Enhanced LLM 的算法设计 ‣ 5 对抗性赌博的算法增强 LLM ‣ 超越数字奖励：LLM
    代理的上下文对抗性赌博")节中所述的 IF-Enhanced LLM 算法，详细说明了其关键特性和实施备注。
- en: –
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") presents the necessary assumptions and lemmas for the theoretical analysis
    of IF-Enhanced LLM in Section[5.2](#S5.SS2 "5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [A.2.1](#A1.SS2.SSS1 "A.2.1 对抗性赌博的有用假设和引理 ‣ A.2 理论分析 ‣ 附录 A IF-Enhanced LLM
    的算法设计与分析 ‣ 超越数字奖励：LLM 代理的上下文对抗性赌博") 提出了第[5.2](#S5.SS2 "5.2 IF-Enhanced LLM 的理论保证
    ‣ 5 对抗性赌博的算法增强 LLM ‣ 超越数字奖励：LLM 代理的上下文对抗性赌博")节中 IF-Enhanced LLM 理论分析所需的假设和引理。
- en: –
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Appendix [A.2.2](#A1.SS2.SSS2 "A.2.2 Theoretical Guarantees of IF-Enhanced
    LLM ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced
    LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") proves
    Theorem[5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣ 5.2 Theoretical Guarantees
    for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") and[5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), establishing IF-Enhanced LLM’s regret bounds.'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [A.2.2](#A1.SS2.SSS2 "A.2.2 IF-Enhanced LLM 的理论保证 ‣ A.2 理论分析 ‣ 附录 A IF-Enhanced
    LLM 的算法设计与分析 ‣ 超越数字奖励：LLM 代理的上下文对抗性赌博") 证明了定理[5.1](#S5.Thmtheorem1 "定理 5.1 (脆弱性)。
    ‣ 5.2 IF-Enhanced LLM 的理论保证 ‣ 5 对抗性赌博的算法增强 LLM ‣ 超越数字奖励：LLM 代理的上下文对抗性赌博")和[5.2](#S5.Thmtheorem2
    "定理 5.2 (期望遗憾界限)。 ‣ 5.2 IF-Enhanced LLM 的理论保证 ‣ 5 对抗性赌博的算法增强 LLM ‣ 超越数字奖励：LLM
    代理的上下文对抗性赌博")，建立了 IF-Enhanced LLM 的遗憾界限。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Experimental Part: Prompt Design and Supplementary Results'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验部分：提示设计和补充结果
- en: –
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Appendix [B.1.1](#A2.SS1.SSS1 "B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") illustrates the prompt design and
    prompt perturbations logic.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [B.1.1](#A2.SS1.SSS1 "B.1.1 提示设计 ‣ B.1 LLM实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")说明了提示设计和提示扰动逻辑。
- en: –
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Appendix [B.1.2](#A2.SS1.SSS2 "B.1.2 Exemplars of GPT-4 Turbo ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") provides a five consecutive time
    steps example of GPT-4 Turbo to showcase its behavior.'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [B.1.2](#A2.SS1.SSS2 "B.1.2 GPT-4 Turbo的五个连续时间步骤示例 ‣ B.1 LLM实验 ‣ 附录 B 提示设计和补充结果
    ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")提供了GPT-4 Turbo在五个连续时间步骤中的示例，以展示其行为。
- en: –
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Appendix [B.2](#A2.SS2 "B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") presents supplementary experimental results, providing further
    insights into the performance and behavior of the algorithms in Sections[4](#S4
    "4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and [5](#S5 "5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [B.2](#A2.SS2 "B.2 补充实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")提供了补充实验结果，进一步揭示了第[4](#S4
    "4 LLM作为独立的上下文决策者 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")和第[5](#S5 "5 算法增强的LLM用于对抗性困境 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")节中算法的性能和行为。
- en: Appendix A Algorithm Design and Analysis of IF-Enhanced LLM
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A IF增强LLM的算法设计和分析
- en: 'In this section, we detail the design principles and implementation of the
    IF-Enhanced LLM algorithm. We also provide a rigorous proof of Theorem[5.1](#S5.Thmtheorem1
    "Theorem 5.1 (Vulnerability). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM
    ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") and[5.2](#S5.Thmtheorem2 "Theorem 5.2 (Expected
    Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), establishing the theoretical guarantees of IF-Enhanced LLM under
    the assumptions outlined in Appendix[A.2.1](#A1.SS2.SSS1 "A.2.1 Useful Assumptions
    and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm
    Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents").'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细描述了IF增强LLM算法的设计原则和实现方法。我们还提供了定理[5.1](#S5.Thmtheorem1 "定理 5.1（脆弱性）。
    ‣ 5.2 IF增强LLM的理论保证 ‣ 5 算法增强的LLM用于对抗性困境 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")和[5.2](#S5.Thmtheorem2
    "定理 5.2（期望遗憾界限）。 ‣ 5.2 IF增强LLM的理论保证 ‣ 5 算法增强的LLM用于对抗性困境 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")的严格证明，确立了在附录[A.2.1](#A1.SS2.SSS1
    "A.2.1 对抗性困境的有用假设和引理 ‣ A.2 理论分析 ‣ 附录 A IF增强LLM的算法设计和分析 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")中概述的假设下IF增强LLM的理论保证。
- en: A.1 Detailed Procedure Description
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 详细过程描述
- en: 'In Procedure [1](#algorithm1a "Procedure 1 ‣ A.1 Detailed Procedure Description
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") below, we describe the Match
    Arms procedure used in IF-Enhanced LLM (see Algorithm[1](#algorithm1 "Algorithm
    1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and Figure[3](#S5.F3 "Figure 3 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的过程 [1](#algorithm1a "过程 1 ‣ A.1 详细过程描述 ‣ 附录 A IF增强LLM的算法设计和分析 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")中，我们描述了IF增强LLM中使用的匹配臂过程（参见算法[1](#algorithm1
    "算法 1 ‣ 5.1 IF增强LLM的算法设计 ‣ 5 算法增强的LLM用于对抗性困境 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境")和图[3](#S5.F3
    "图 3 ‣ 5.1 IF增强LLM的算法设计 ‣ 5 算法增强的LLM用于对抗性困境 ‣ 超越数值奖励：与LLM代理的上下文对抗性困境"))。
- en: Algorithm 1 Match Arms
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 匹配臂
- en: 'Input: Two arms $a,a^{\prime}$'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：两个臂 $a,a^{\prime}$
- en: if *$a\neq a^{\prime}$* then
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *$a\neq a^{\prime}$* 则
- en: while *$\nexists\ (b,b^{\prime})\in B$*
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *$\nexists\ (b,b^{\prime})\in B$* 时
- en: We reprise the IF2 procedure in[[3](#bib.bib3)] below to complement the presentation
    of IF-Enhanced LLM.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[[3](#bib.bib3)]中重述 IF2 程序，以补充对 IF-Enhanced LLM 的介绍。
- en: Algorithm 2 IF2 Procedure
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 IF2 程序
- en: 'Input: Incumbent arm $a$'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：现有臂 $a$
- en: if *$t\leq(8K/\epsilon_{1,2}^{2})\log(TK)$* then
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *$t\leq(8K/\epsilon_{1,2}^{2})\log(TK)$* 则
- en: while *$\nexists\ (b,b^{\prime})\in B$*
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *$\nexists\ (b,b^{\prime})\in B$* 时
- en: 'It is worth noting the following features of Algorithm [1](#algorithm1 "Algorithm
    1 ‣ 5.1 Algorithm Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    in its practical implementation.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在实际实施中，算法 [1](#algorithm1 "算法 1 ‣ 5.1 IF-Enhanced LLM 的算法设计 ‣ 5 对抗赌博者的算法增强
    LLM ‣ 超越数值奖励：与 LLM 代理的上下文对抗赌博者") 的以下特点。
- en: Remark 1.
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 1。
- en: The LLM Phase allows for flexible exploration design within the bounded length
    of the Match Arms procedure, not limiting the number of prompts and comparisons
    performed by the LLM to identify an empirically best arm.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 阶段允许在 Match Arms 过程的有界长度内灵活设计探索，不限制 LLM 识别经验最佳臂时进行的提示和比较次数。
- en: Remark 2.
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 2。
- en: The bound length in the Match Arms procedure can be adjusted based on empirical
    requirements. Modifying the confidence parameter $\delta$ will affect the regret
    bound and the algorithm’s performance. These parameters can be tuned to balance
    exploration and exploitation, depending on the specific application and desired
    level of confidence.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Match Arms 过程中的边界长度可以根据经验要求进行调整。修改置信参数 $\delta$ 会影响遗憾边界和算法的性能。这些参数可以调整以平衡探索与利用，具体取决于特定应用和期望的置信水平。
- en: A.2 Theoretical Analysis
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 理论分析
- en: A.2.1 Useful Assumptions and Lemmas for Dueling Bandits
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 对于对抗赌博者有用的假设和引理
- en: We introduce the useful assumptions and lemmas for Dueling Bandits that are
    necessary for the theoretical analysis of our proposed algorithm.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了对抗赌博者的有用假设和引理，这些假设和引理对我们提出的算法的理论分析是必要的。
- en: Assumption 2  (Total Ordering).
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 2（全序）。
- en: The preference matrix $P=(\epsilon_{ij})$.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好矩阵 $P=(\epsilon_{ij})$。
- en: With the TO property satisfied, we assume the preference matrix $P$ further
    satisfies the following two standard properties[[6](#bib.bib6), [17](#bib.bib17),
    [3](#bib.bib3)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在满足 TO 属性的情况下，我们假设偏好矩阵 $P$ 进一步满足以下两个标准属性[[6](#bib.bib6), [17](#bib.bib17), [3](#bib.bib3)]。
- en: Assumption 3  (Strong Stochastic Transitivity).
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 3（强随机传递性）。
- en: The preference matrix $P=(\epsilon_{ij})$.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好矩阵 $P=(\epsilon_{ij})$。
- en: Assumption 4  (Stochastic Triangle Inequality).
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 4（随机三角不等式）。
- en: The preference matrix $P=(\epsilon_{ij})$.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好矩阵 $P=(\epsilon_{ij})$。
- en: 'Note that the Bradley-Terry-Luce (BTL) model [[34](#bib.bib34)] used in our
    experiments[4.1](#S4.SS1 "4.1 Implementation Details of Experiments ‣ 4 LLMs as
    Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents") satisfies Assumption[3](#Thmassumption3 "Assumption
    3 (Strong Stochastic Transitivity). ‣ A.2.1 Useful Assumptions and Lemmas for
    Dueling Bandits ‣ A.2 Theoretical Analysis ‣ Appendix A Algorithm Design and Analysis
    of IF-Enhanced LLM ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") and[4](#Thmassumption4 "Assumption 4 (Stochastic Triangle Inequality).
    ‣ A.2.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ A.2 Theoretical Analysis
    ‣ Appendix A Algorithm Design and Analysis of IF-Enhanced LLM ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"). We restate the following
    theoretical guarantees for IF2 that is useful in the proof of Theorem[5.2](#S5.Thmtheorem2
    "Theorem 5.2 (Expected Regret Bounds). ‣ 5.2 Theoretical Guarantees for IF-Enhanced
    LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"). Let $\epsilon_{\mathrm{bad}}\coloneq\min_{b\neq
    b^{*}}\epsilon(b,b^{*})$.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们实验中使用的Bradley-Terry-Luce (BTL) 模型 [[34](#bib.bib34)] 满足假设[3](#Thmassumption3
    "假设 3 (强随机传递性)。 ‣ A.2.1 对抗性赌博的有用假设和引理 ‣ A.2 理论分析 ‣ 附录 A IF增强LLM的算法设计与分析 ‣ 超越数值奖励：带有LLM代理的上下文对抗性赌博")
    和[4](#Thmassumption4 "假设 4 (随机三角不等式)。 ‣ A.2.1 对抗性赌博的有用假设和引理 ‣ A.2 理论分析 ‣ 附录 A
    IF增强LLM的算法设计与分析 ‣ 超越数值奖励：带有LLM代理的上下文对抗性赌博")。 我们重述以下理论保证，对于IF2，这在定理[5.2](#S5.Thmtheorem2
    "定理 5.2 (期望遗憾界限)。 ‣ 5.2 IF增强LLM的理论保证 ‣ 5 算法增强LLM用于对抗性赌博 ‣ 超越数值奖励：带有LLM代理的上下文对抗性赌博")的证明中很有用。
    设 $\epsilon_{\mathrm{bad}}\coloneq\min_{b\neq b^{*}}\epsilon(b,b^{*})$。
- en: Lemma 1  (Theorem 2 in [[3](#bib.bib3)]).
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1  (定理 2 在 [[3](#bib.bib3)])。
- en: Assuming the preference matrix $P$ satisfies the SST and STI, then IF2 has its
    expected regret (both weak and strong) bounded from above by
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 假设偏好矩阵 $P$ 满足SST和STI，则IF2的期望遗憾（包括弱和强）上界为
- en: '|  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{IF2})]\leq O\left(\frac{K}{\epsilon_{\mathrm{bad}}}\log
    T\right).$ |  | (4) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{IF2})]\leq O\left(\frac{K}{\epsilon_{\mathrm{bad}}}\log
    T\right).$ |  | (4) |'
- en: The following expected regret bound achieved by IF2 is tight up to multiplicative
    constants, as indicated by the lower bound (Theorem 4) in[[3](#bib.bib3)] such
    that any algorithm Alg for DB satisfies $\mathbb{E}[\mathsf{SR}(\textsc{Alg})]=\Omega\left((K/\epsilon_{\mathrm{bad}})\log
    T\right)$.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: IF2实现的以下期望遗憾界限在乘法常数的范围内是紧的，如[[3](#bib.bib3)] 中的下界（定理 4）所示，使得任何算法 Alg 对于DB满足
    $\mathbb{E}[\mathsf{SR}(\textsc{Alg})]=\Omega\left((K/\epsilon_{\mathrm{bad}})\log
    T\right)$。
- en: A.2.2 Theoretical Guarantees of IF-Enhanced LLM
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2 IF增强LLM的理论保证
- en: 'Part I: Vulnerability of Standalone LLM Agents'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分：独立LLM代理的脆弱性
- en: 'Proof of Theorem [5.1](#S5.Thmtheorem1 "Theorem 5.1 (Vulnerability). ‣ 5.2
    Theoretical Guarantees for IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-235
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理的证明 [5.1](#S5.Thmtheorem1 "定理 5.1 (脆弱性)。 ‣ 5.2 IF增强LLM的理论保证 ‣ 5 算法增强LLM用于对抗性赌博
    ‣ 超越数值奖励：带有LLM代理的上下文对抗性赌博")。
- en: 'Consider the following Dueling Bandit instance with $K\geq 3$:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下具有 $K\geq 3$ 的对抗性赌博实例：
- en: '|  | $$P_{i,j}=\begin{cases}0.5+\epsilon,&amp;\text{if }b_{i}=b^{*}\text{ and
    }b_{j}\neq b^{*},\\ 0.5-\epsilon,&amp;\text{if }b_{i}\neq b^{*}\text{ and }b_{j}=b^{*},\\'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$P_{i,j}=\begin{cases}0.5+\epsilon,&amp;\text{如果 }b_{i}=b^{*}\text{ 且
    }b_{j}\neq b^{*},\\ 0.5-\epsilon,&amp;\text{如果 }b_{i}\neq b^{*}\text{ 且 }b_{j}=b^{*},\\'
- en: 0.5,&amp;\text{otherwise.}\end{cases}$$ |  |
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 0.5,&amp;\text{否则。}\end{cases}$$ |  |
- en: where $0<\epsilonAnswer at Time Step 0: 1\.
    Analysis: Since no duels have been conducted yet, all empirical probabilities
    between arms are at their initial value of 0.50, indicating no preference or historical
    data to suggest one arm is better than another. To begin identifying the Condorcet
    Winner, we need to start collecting data by conducting duels between different
    arms. A balanced approach would be to start dueling pairs that have not yet competed,
    but since all pairs are at the same starting point, we can choose any pair. 2\.
    Next Pair: (1, 2)Answer
    at Time Step 1: 1\. Analysis: From the results provided, the only conclusive duel
    so far is between Arm 1 and Arm 2, where Arm 1 won, giving P(1 > 2) = 1.00\. This
    suggests that Arm 1 might be stronger than Arm 2, but more data is needed to confirm
    its strength relative to other arms. All other pairwise comparisons have not yet
    resulted in any duels, so their probabilities remain at the initial estimate of
    0.50, indicating no preference between the arms based on the data. To better understand
    the relative strengths of the arms and potentially identify the Condorcet Winner,
    it is crucial to increase the number of duels among arms with less information.
    2\. Next Pair: Given that Arm 1 has shown some potential by winning against Arm
    2, it would be informative to test Arm 1 against another arm. Since Arm 1 has
    not yet been tested against Arm 3, Arm 4, or Arm 5 in a conclusive duel, any of
    these could be a good choice. However, to balance the exploration among other
    arms and gather more data on their performance, let’s choose a pair that has not
    been tested yet. Arm 3 and Arm 4 have not dueled conclusively, and understanding
    their relative strength could provide insights into the overall hierarchy of the
    arms. Choose: (Arm 3, Arm 4)Answer
    at Time Step 2: 1\. Analysis: - Arm 1 vs Arm 2: Arm 1 has a clear advantage over
    Arm 2 with a win probability of 1.00. - Arm 3 vs Arm 4: Arm 3 has a clear advantage
    over Arm 4 with a win probability of 1.00. - Other Duels: All other duels have
    resulted in a 50% win probability for each arm, indicating no clear advantage
    based on the limited data available. Given these results, Arm 1 and Arm 3 appear
    to be stronger contenders in their respective duels. However, more data is needed
    to confidently identify the Condorcet Winner. To gather more information, it would
    be strategic to pit arms against each other where the win probabilities are still
    uncertain (i.e., 0.50), especially focusing on those arms that have shown potential
    strength. 2\. Next Pair: (Arm 1, Arm 3)Answer
    at Time Step 3: 1\. Analysis: - Arm 1 has shown strong performance against Arm
    2 and Arm 3, winning all duels against them. However, it has not yet won against
    Arm 4 and Arm 5, with the results still uncertain (P=0.50). - Arm 3 has won against
    Arm 4, suggesting it might be stronger than Arm 4. - Arms 4 and 5 have not won
    any duels yet, and their probabilities against other arms are still at 0.50, indicating
    uncertainty and lack of data. - Arms 2, 4, and 5 have not won any duels, and their
    probabilities against other arms are mostly at 0.50, indicating a need for more
    data to determine their strengths. Given this analysis, it seems prudent to focus
    on gathering more information about the arms that have uncertain outcomes and
    have not shown strong winning probabilities. Specifically, more data is needed
    for Arm 1 against Arms 4 and 5, as well as between Arms 4 and 5 themselves, to
    better understand their relative strengths. 2\. Next Pair: (Arm 1, Arm 4)Answer at Time Step 4: 1\.
    Analysis: - Arm 1 has shown strong performance against Arms 2, 3, and 4, winning
    all duels against them. However, it has not yet won against Arm 5, with the probability
    of winning currently estimated at 0.50. - Arm 3 has also shown some strength by
    winning against Arm 4. - The other arms (2, 4, 5) have not won any duels yet,
    and their probabilities of winning against each other are all estimated at 0.50,
    indicating no clear evidence of superiority among them. - Given the current data,
    Arm 1 appears to be a strong candidate for the Condorcet Winner, but more evidence
    is needed, particularly in its performance against Arm 5. To further clarify the
    potential of Arm 1 and to explore the capabilities of Arm 5, which has not lost
    yet but also has not won (except in an undecided duel against Arm 1), it would
    be strategic to have them duel again. This will help in determining if Arm 1 can
    consistently outperform Arm 5 or if Arm 5 has potential that hasn’t been fully
    revealed yet. 2\. Next Pair: (Arm 1, Arm 5)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步长 0 的答案：1\. 分析：由于尚未进行任何对决，各臂之间的所有经验概率均为初始值
    0.50，表示没有偏好或历史数据表明某一臂比另一臂更好。为了开始识别 Condorcet 胜者，我们需要通过进行不同臂之间的对决来开始收集数据。一个平衡的方法是开始对决尚未竞争的对，然而，由于所有对都处于相同的起始点，我们可以选择任何一对。2\.
    下一对： (1, 2)时间步长
    1 的答案：1\. 分析：根据提供的结果，到目前为止唯一确定的对决是臂 1 和臂 2 的对决，其中臂 1 胜出，给出了 P(1 > 2) = 1.00。这表明臂
    1 可能比臂 2 更强，但需要更多数据来确认其相对于其他臂的强度。所有其他对比尚未产生任何对决，因此它们的概率保持在初始估计的 0.50，表示基于数据对臂之间没有偏好。为了更好地了解臂的相对强度，并可能识别
    Condorcet 胜者，增加在信息较少的臂之间的对决数量至关重要。2\. 下一对：鉴于臂 1 在与臂 2 的对决中表现出一些潜力，测试臂 1 对其他臂将是有益的。由于臂
    1 尚未在决定性对决中与臂 3、臂 4 或臂 5 进行测试，因此这些中的任何一个都可以是一个不错的选择。然而，为了在其他臂之间平衡探索并收集更多关于它们表现的数据，我们选择一对尚未测试的对。臂
    3 和臂 4 尚未决定性对决，了解它们的相对强度可以提供有关臂的整体层次的见解。选择： (臂 3, 臂 4)时间步长 2 的答案：1\. 分析：- 臂 1 对臂 2：臂 1 对臂 2 的胜算为 1.00，显示出明显的优势。- 臂 3
    对臂 4：臂 3 对臂 4 的胜算为 1.00，显示出明显的优势。- 其他对决：所有其他对决的胜算为 50%，表明基于有限的数据没有明显的优势。根据这些结果，臂
    1 和臂 3 在各自的对决中似乎是更强的竞争者。然而，仍需更多数据来自信地识别 Condorcet 胜者。为了收集更多信息，战略上应将臂对臂之间的对决集中在胜算仍不确定的地方（即
    0.50），特别是关注那些已经显示出潜在强度的臂。2\. 下一对： (臂 1, 臂 3)时间步长
    3 的答案：1\. 分析：- 臂 1 在与臂 2 和臂 3 的对决中表现出强劲的表现，赢得了所有与它们的对决。然而，它尚未赢得对臂 4 和臂 5 的对决，结果仍不确定
    (P=0.50)。- 臂 3 战胜了臂 4，表明它可能比臂 4 更强。- 臂 4 和臂 5 尚未赢得任何对决，它们对其他臂的胜算仍为 0.50，表明不确定性和数据不足。-
    臂 2、臂 4 和臂 5 尚未赢得任何对决，它们对其他臂的胜算大多为 0.50，表明需要更多的数据来确定它们的强度。根据这一分析，集中于收集关于那些结果不确定且未显示出强劲胜算的臂的信息似乎是明智的。特别是需要更多关于臂
    1 对臂 4 和臂 5 的数据，以及臂 4 和臂 5 之间的对决，以便更好地了解它们的相对强度。2\. 下一对： (臂 1, 臂 4)时间步长 4 的答案：1\. 分析：- 臂 1 在对臂 2、臂 3 和臂 4 的对决中表现强劲，赢得了所有对决。然而，它尚未赢得对臂
    5 的对决，目前的胜算估计为 0.50。- 臂 3 也通过战胜臂 4 显示出一些
- en: B.2 Supplementary Experiments
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 补充实验
- en: B.2.1 Comparisons with Different Metrics
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.1 不同指标的比较
- en: 'We present supplementary results to complement our case studies. (i) Figure [8](#A2.F8
    "Figure 8 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") shows the strong and weak regret
    comparisons for the Hard instance. (ii) Figure[9](#A2.F9 "Figure 9 ‣ B.2.1 Comparisons
    with Different Metrics ‣ B.2 Supplementary Experiments ‣ Appendix B Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") introduces the Best Arm Ratio, measuring how often the algorithms
    select the best arm (Condorcet winner) in a duel for both instances. (iii) Figure[10](#A2.F10
    "Figure 10 ‣ B.2.1 Comparisons with Different Metrics ‣ B.2 Supplementary Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") examines the generalized variance
    of the strong and weak regret for both instances.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了补充结果以补充我们的案例研究。(i) 图[8](#A2.F8 "图 8 ‣ B.2.1 不同指标的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计和补充结果
    ‣ 超越数字奖项：与 LLM 代理的上下文对战困境") 显示了 Hard 实例中的强后悔和弱后悔比较。(ii) 图[9](#A2.F9 "图 9 ‣ B.2.1
    不同指标的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖项：与 LLM 代理的上下文对战困境") 介绍了最佳臂比率，该比率衡量算法在对战中选择最佳臂（Condorcet
    胜者）的频率。(iii) 图[10](#A2.F10 "图 10 ‣ B.2.1 不同指标的比较 ‣ B.2 补充实验 ‣ 附录 B 提示设计和补充结果 ‣
    超越数字奖项：与 LLM 代理的上下文对战困境") 检查了强后悔和弱后悔在两个实例中的广义方差。
- en: '![Refer to caption](img/39d84c5958672e46020c4be2d5e6a223.png)![Refer to caption](img/17ef2c5d735c9eb02482faa5ad558cec.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/39d84c5958672e46020c4be2d5e6a223.png)![参见说明](img/17ef2c5d735c9eb02482faa5ad558cec.png)'
- en: 'Figure 8: Comparisons between LLM agents and various classic DB algorithms.
    Left and Right: strong and weak regret for the Hard instance. Results for the
    Easy instance is presented in Figure [2](#S4.F2 "Figure 2 ‣ 4.1 Implementation
    Details of Experiments ‣ 4 LLMs as Standalone In-Context Decision-Makers ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: LLM 代理与各种经典 DB 算法的比较。左侧和右侧：Hard 实例中的强后悔和弱后悔。Easy 实例的结果见图[2](#S4.F2 "图
    2 ‣ 4.1 实验实施细节 ‣ 4 LLM 作为独立的上下文决策者 ‣ 超越数字奖项：与 LLM 代理的上下文对战困境")。'
- en: '![Refer to caption](img/e79459b60cbc4b5f77508b9222633ad2.png)![Refer to caption](img/9145a6d3edcc28dbfb533dfef85b49dd.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e79459b60cbc4b5f77508b9222633ad2.png)![参见说明](img/9145a6d3edcc28dbfb533dfef85b49dd.png)'
- en: 'Figure 9: The Best Arm Ratio represents the fraction of duels that include
    the best arm (Condorcet winner) over different time intervals in the Easy instance
    (Left) and the Hard instance (Right). This metric measures how often an algorithm
    selects the best arm to participate in a duel. A higher Best Arm Ratio indicates
    that the algorithm is more effective at identifying and exploiting the best arm.
    Three LLMs (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo) and two state-of-the-art baselines
    (Self-Sparring and DTS) are compared against each other.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 最佳臂比率表示在 Easy 实例（左）和 Hard 实例（右）中，不同时间间隔内包含最佳臂（Condorcet 胜者）的对战比例。该指标衡量算法选择最佳臂参与对战的频率。较高的最佳臂比率表明算法在识别和利用最佳臂方面更有效。比较了三种
    LLM（GPT-3.5 Turbo、GPT-4、GPT-4 Turbo）和两种最先进的基线（Self-Sparring 和 DTS）。'
- en: '![Refer to caption](img/5a78d9f88ceb8d532bb44ab1592f8808.png)![Refer to caption](img/e7b3d8f8d51d56881c050271860542a1.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a78d9f88ceb8d532bb44ab1592f8808.png)![参见说明](img/e7b3d8f8d51d56881c050271860542a1.png)'
- en: 'Figure 10: Comparison of the generalized variance of strong and weak regret
    between LLMs and baseline algorithms on the Easy (Left) and Hard (Right) instances.
    In the Easy instance, GPT-4 Turbo exhibits the lowest average generalized variance.
    For the Hard instance, GPT-4 Turbo maintains a variance level comparable to state-of-the-art
    baseline algorithms (except BTM and SAVAGE, which are in an early exploration
    stage).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: LLM 和基线算法在 Easy（左）和 Hard（右）实例中强后悔和弱后悔的广义方差比较。在 Easy 实例中，GPT-4 Turbo 显示出最低的平均广义方差。对于
    Hard 实例，GPT-4 Turbo 维持了与最先进的基线算法（除了 BTM 和 SAVAGE，它们处于早期探索阶段）相当的方差水平。'
- en: B.2.2 Duel Selection Trajectory
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2.2 对战选择轨迹
- en: We visualize the duel selection trajectory in representative experiments to
    better understand the behavior of LLM agents and baseline algorithms.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在具有代表性的实验中可视化对决选择轨迹，以更好地理解 LLM 代理和基线算法的行为。
- en: 'Duel Selection Trajectory Explanation: The reshuffled arm order is $b_{5}\succ
    b_{3}\succ b_{2}\succ b_{1}\succ b_{4}$, with arm indices from bottom to top:
    5, 4, 3, 2, 1\. Each filled black cell represents a selected arm at that time
    step. For instance, black lines in arms 5 and 3 indicate the selection of the
    duel between (arm 5, arm 3) at that particular time step.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对决选择轨迹解释：重新排列的臂顺序是 $b_{5}\succ b_{3}\succ b_{2}\succ b_{1}\succ b_{4}$，臂的索引从下到上为：5、4、3、2、1。每个填充的黑色单元格代表在该时间步选择的臂。例如，臂
    5 和 3 的黑色线条表示在该时间步选择了（臂 5，臂 3）进行对决。
- en: LLM Agents and Baseline Algorithms
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 代理和基线算法
- en: '![Refer to caption](img/ed42d5de3ab2e7c58eb96536d2c9516e.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed42d5de3ab2e7c58eb96536d2c9516e.png)'
- en: 'Figure 11: Comparison of duel selection trajectories among GPT-4 Turbo, Self-Sparring,
    and DTS on the Easy (Top Row) and Hard (Bottom Row) instances. The decision trajectories
    of GPT-4 Turbo exhibit a clear pattern of continuous exploration without converging
    to the best arm. In contrast, Self-Sparring and DTS demonstrate more structured
    exploration patterns and convergence properties on both instances.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: GPT-4 Turbo、Self-Sparring 和 DTS 在简单（上排）和困难（下排）实例中的决策轨迹比较。GPT-4 Turbo
    的决策轨迹表现出连续探索的明显模式，而未能收敛到最佳臂。相比之下，Self-Sparring 和 DTS 在这两种实例中展示了更有结构的探索模式和收敛特性。'
- en: '![Refer to caption](img/9b066c3c8835e363d4c6dbcfbad3b724.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b066c3c8835e363d4c6dbcfbad3b724.png)'
- en: 'Figure 12: Local optima trajectories of GPT-3.5 Turbo (Left), GPT-4 (Middle),
    and GPT-4 Turbo (Right, with noisy prompt) on the Hard instance. Less capable
    LLMs, such as GPT-3.5 Turbo and GPT-4, could get stuck comparing suboptimal arms
    on hard preference structure. Even for GPT-4 Turbo, noisy prompts with biased
    history (see Figure [7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments
    ‣ Appendix B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")) can lead it to be trapped in bad
    tournaments.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: GPT-3.5 Turbo（左）、GPT-4（中）和 GPT-4 Turbo（右，带有噪声提示）在困难实例上的局部最优轨迹。能力较弱的 LLM，如
    GPT-3.5 Turbo 和 GPT-4，可能会在困难的偏好结构中困于比较次优臂。即便是 GPT-4 Turbo，带有偏见历史的噪声提示（见图 [7](#A2.F7
    "图 7 ‣ B.1.1 提示设计 ‣ B.1 LLM 实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数值奖励：LLM 代理的上下文对决赌博")) 也可能使其陷入糟糕的竞赛中。'
- en: '![Refer to caption](img/4f7690d0e459b3dc6b3c76f25d527d61.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f7690d0e459b3dc6b3c76f25d527d61.png)'
- en: 'Figure 13: Comparison of success (Left) and failure (Right) cases for the Convergence-Triggered
    GPT-4 Turbo intervention strategy discussed in Section [5.1](#S5.SS1 "5.1 Algorithm
    Design of IF-Enhanced LLM ‣ 5 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents"). While it works for
    most cases due to GPT-4 Turbo’s strong capability (Left), sometimes this naive
    intervention can reinforce suboptimal choices (Right) on the Hard instance.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 针对第 [5.1](#S5.SS1 "5.1 IF增强型 LLM 算法设计 ‣ 5 算法增强型 LLMs for Dueling Bandits
    ‣ 超越数值奖励：LLM 代理的上下文对决赌博") 节讨论的收敛触发的 GPT-4 Turbo 干预策略的成功（左）和失败（右）案例比较。尽管由于 GPT-4
    Turbo 的强大能力，这种方法在大多数情况下有效（左），但有时这种简单的干预会在困难实例中强化次优选择（右）。'
- en: IF-Enhanced LLM
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IF增强型 LLM
- en: '![Refer to caption](img/6abdd423fcee43d588c9ff02a16370ce.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6abdd423fcee43d588c9ff02a16370ce.png)'
- en: 'Figure 14: Trajectory of the classic IF2 algorithm with different initial incumbent
    arm selections. The left plot shows the behavior of IF2 when a good initial incumbent
    arm is randomly selected, leading to faster convergence and better overall performance.
    In contrast, the right plot illustrates the case where a bad initial incumbent
    arm is chosen, resulting in slower convergence and suboptimal performance.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: 经典 IF2 算法在不同初始选择的表现轨迹。左侧图显示了当随机选择一个好的初始臂时，IF2 的行为，导致更快的收敛和更好的整体性能。相反，右侧图展示了选择一个不好的初始臂的情况，导致较慢的收敛和次优的性能。'
- en: '![Refer to caption](img/3830a04acf5915464e4514a7e1980d21.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3830a04acf5915464e4514a7e1980d21.png)'
- en: 'Figure 15: Duel selection trajectory comparison between GPT-4 Turbo and IF-Enhanced
    LLM under different prompt settings (see Figures [6](#A2.F6 "Figure 6 ‣ B.1.1
    Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix B Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and [7](#A2.F7 "Figure 7 ‣ B.1.1 Design of Prompts ‣ B.1 LLM Experiments ‣ Appendix
    B Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")). (i) Top Row: With the original prompt, IF-Enhanced
    LLM leverages GPT-4 Turbo’s exploration ability and guarantees convergence through
    the IF2 phase. (ii) Middle Row: With a noisy prompt (biased history), IF-Enhanced
    LLM overcomes the limitation of standalone GPT-4 Turbo getting trapped in local
    optima by employing uniform comparisons in the IF2 phase. (iii) Bottom Row: With
    an adversarial prompt (reversed goal), IF-Enhanced LLM maintains robust behavior
    despite the adversarial modification. Across all scenarios, IF-Enhanced LLM demonstrates
    superior performance and robustness compared to standalone GPT-4 Turbo.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：在不同提示设置下，GPT-4 Turbo 和 IF-Enhanced LLM 的对决选择轨迹比较（见图 [6](#A2.F6 "图 6 ‣ B.1.1
    提示设计 ‣ B.1 LLM 实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖励：与 LLM 代理的上下文对决强盗") 和 [7](#A2.F7 "图
    7 ‣ B.1.1 提示设计 ‣ B.1 LLM 实验 ‣ 附录 B 提示设计和补充结果 ‣ 超越数字奖励：与 LLM 代理的上下文对决强盗")）。 (i)
    顶部行：在原始提示下，IF-Enhanced LLM 利用 GPT-4 Turbo 的探索能力，并通过 IF2 阶段保证收敛。 (ii) 中间行：在有噪声的提示（有偏见的历史）下，IF-Enhanced
    LLM 通过在 IF2 阶段采用均匀比较，克服了单独 GPT-4 Turbo 陷入局部最优的限制。 (iii) 底部行：在对抗性提示（逆向目标）下，IF-Enhanced
    LLM 尽管经过了对抗性修改，但仍保持了强健的行为。在所有场景中，IF-Enhanced LLM 相较于单独的 GPT-4 Turbo 展现了更优越的性能和鲁棒性。
