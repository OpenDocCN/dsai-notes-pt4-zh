- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning
    and Multi-Branch Reasoning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过微调和多分支推理增强低参数 LLMs 的通用代理能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.19962](https://ar5iv.labs.arxiv.org/html/2403.19962)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.19962](https://ar5iv.labs.arxiv.org/html/2403.19962)
- en: Qinhao Zhou¹ Zihan Zhang¹ Xiang Xiang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qinhao Zhou¹ Zihan Zhang¹ Xiang Xiang¹
- en: Ke Wang² Yuchuan Wu² Yongbin Li ²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ke Wang² Yuchuan Wu² Yongbin Li ²
- en: ¹ National Key Lab of MSIIPT, School of Artificial Intelligence and Automation,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 国家重点实验室，人工智能与自动化学院，
- en: Huazhong University of Science and Technology, Wuhan, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 华中科技大学，武汉，中国
- en: '² DAMO Academy, Alibaba Group, Beijing, China ^∗Corresponding author (e-mail:
    [xex@hust.edu.cn](xex@hust.edu.cn)); also with Peng Cheng Laboratory, Shenzhen,
    China.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ² 达摩院，阿里巴巴集团，北京，中国 ^∗通讯作者（电子邮件：[xex@hust.edu.cn](xex@hust.edu.cn)）；同时在深圳，彭城实验室。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Open-source pre-trained Large Language Models (LLMs) exhibit strong language
    understanding and generation capabilities, making them highly successful in a
    variety of tasks. However, when used as agents for dealing with complex problems
    in the real world, their performance is far inferior to large commercial models
    such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities
    of task planning, long-term memory, and the ability to leverage external tools
    to achieve satisfactory performance. Various methods have been proposed to enhance
    the agent capabilities of LLMs. On the one hand, methods involve constructing
    agent-specific data and fine-tuning the models. On the other hand, some methods
    focus on designing prompts that effectively activate the reasoning abilities of
    the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive
    method for constructing agent-specific data using GPT-4. Through supervised fine-tuning
    with constructed data, we find that for these models with a relatively small number
    of parameters, supervised fine-tuning can significantly reduce hallucination outputs
    and formatting errors in agent tasks. Furthermore, techniques such as multi-path
    reasoning and task decomposition can effectively decrease problem complexity and
    enhance the performance of LLMs as agents. We evaluate our method on five agent
    tasks of AgentBench and achieve satisfactory results.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 开源预训练的大型语言模型（LLMs）展现出强大的语言理解和生成能力，使其在各种任务中表现出色。然而，当作为处理复杂现实问题的代理时，它们的表现远不如 ChatGPT
    和 GPT-4 等大型商业模型。作为智能代理，LLMs 需要具备任务规划、长期记忆和利用外部工具的能力，以实现令人满意的性能。各种方法已被提出以提升 LLMs
    的代理能力。一方面，这些方法涉及构建特定于代理的数据和微调模型。另一方面，一些方法则专注于设计能够有效激活 LLMs 推理能力的提示。我们在 7B 和 13B
    模型上探索了这两种策略。我们提出了一种使用 GPT-4 构建代理特定数据的综合方法。通过对构建的数据进行监督微调，我们发现对于这些参数较少的模型，监督微调可以显著减少幻觉输出和格式错误。此外，多路径推理和任务分解等技术可以有效降低问题复杂性，并提升
    LLMs 作为代理的性能。我们在 AgentBench 的五个代理任务上评估了我们的方法，并取得了令人满意的结果。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/86fffc204bf8b6400d14f068510464d5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86fffc204bf8b6400d14f068510464d5.png)'
- en: 'Figure 1: The agent performance of open-source LLMs and commercial LLMs. Agent
    Overall Score is the average accuracy of several agent tasks.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：开源 LLMs 与商业 LLMs 的代理性能。代理总体评分是多个代理任务的平均准确率。
- en: Large Language Models (LLMs) have been extensively employed in a wide range
    of natural language processing tasks, yielding groundbreaking achievements. Furthermore,
    LLMs have demonstrated their capability to undertake more challenging tasks, such
    as functioning as AI agents. Unlike conventional reasoning tasks, an AI agent
    is an entity that needs to interact with the human or external environment, draw
    inferences, and judge subsequent actions based on feedback. Each single task typically
    involves multiple rounds of dialogue to accomplish. For instance, in a home environment,
    an agent may be tasked with various household tasks that require continuous interaction
    with the environment. The agent needs to evaluate its actions based on the feedback
    from the environment and make timely adjustments to its strategies. Traditional
    AI agents are usually effective in specific domains or environments, but their
    generalization and adaptability are obviously insufficient Liu et al. ([2023](#bib.bib8)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已被广泛应用于各种自然语言处理任务，取得了突破性的成就。此外，LLMs 已展示出承担更具挑战性的任务的能力，例如作为 AI 代理。与传统的推理任务不同，AI
    代理是一个需要与人类或外部环境互动、进行推断，并根据反馈判断后续行动的实体。每个单独的任务通常涉及多轮对话才能完成。例如，在家庭环境中，代理可能会被分配执行各种家务任务，这些任务需要持续与环境互动。代理需要根据环境反馈评估其行动，并及时调整其策略。传统的
    AI 代理通常在特定领域或环境中有效，但其泛化能力和适应性明显不足 Liu et al. ([2023](#bib.bib8))。
- en: In recent years, an increasing number of work Brown et al. ([2020](#bib.bib1));
    OpenAI ([2023](#bib.bib11)); Qin et al. ([2023](#bib.bib16)); Shinn et al. ([2023](#bib.bib18));
    Zhu et al. ([2023](#bib.bib36)) have demonstrated that LLMs possess strong capabilities
    in reasoning, planning, memory, and utilizing external tools. This has propelled
    LLMs towards becoming more generalized and adaptive agents. Recently, AgentBench
    Liu et al. ([2023](#bib.bib8)) conducts extensive evaluations of both commercial
    and open-source LLMs on eight different agent tasks. The results reveal that commercial
    API models show superior agent capabilities. In addition, work such as AutoGPT
    Gravitas ([2023](#bib.bib4)) and GPT-Engineer Osika et al. ([2023](#bib.bib12))
    also use LLMs as agents to build a complete framework for solving complex real-world
    problems. However, open-source models, especially those with smaller parameter
    sizes, still have substantial potential for enhancement. As shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Enhancing the General Agent Capabilities of Low-Parameter
    LLMs through Tuning and Multi-Branch Reasoning"), the average performance of 7B
    and 13B LLMs on each agent task is significantly lower than the commercial models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，越来越多的研究 Brown et al. ([2020](#bib.bib1)); OpenAI ([2023](#bib.bib11)); Qin
    et al. ([2023](#bib.bib16)); Shinn et al. ([2023](#bib.bib18)); Zhu et al. ([2023](#bib.bib36))
    表明 LLMs 在推理、规划、记忆和利用外部工具方面具有强大的能力。这推动了 LLMs 向更通用和适应性强的代理发展。最近，AgentBench Liu et
    al. ([2023](#bib.bib8)) 对商业和开源 LLMs 在八种不同代理任务上的表现进行了广泛评估。结果显示，商业 API 模型表现出更优越的代理能力。此外，如
    AutoGPT Gravitas ([2023](#bib.bib4)) 和 GPT-Engineer Osika et al. ([2023](#bib.bib12))
    的工作也使用 LLMs 作为代理来构建解决复杂现实世界问题的完整框架。然而，开源模型，特别是那些参数规模较小的模型，仍有很大的提升潜力。如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Enhancing the General Agent Capabilities of Low-Parameter
    LLMs through Tuning and Multi-Branch Reasoning") 所示，7B 和 13B LLMs 在每个代理任务上的平均表现明显低于商业模型。
- en: Unlike commercial LLMs, small-scale open-source LLMs are relatively inefficient
    in general knowledge Peters et al. ([2019](#bib.bib14)). Besides, lower parameter
    sizes limit reasoning and memory capacity, often leading to hallucinations in
    the agent dialogue process Zhang et al. ([2023b](#bib.bib33)). However, in practical
    applications, LLMs with 7B and 13B parameters are the most widely used due to
    their relative ease of deployment and fine-tuning. Therefore, enhancing the capabilities
    of such LLMs is of great practical significance. Currently, studies on LLMs agents
    or enhancing model reasoning capabilities Xi et al. ([2023a](#bib.bib25)); Wang
    et al. ([2023](#bib.bib20)) primarily focus on large-scale models. The investigation
    of agent capabilities on 7B and 13B LLMs is still in its early stages of exploration.
    As explained, a proficient agent requires task-planning abilities, proficiency
    in utilizing external tools, and long-term memory capabilities. Task planning
    refers to the ability of the model to decompose large-scale tasks into manageable
    sub-goals, facilitating efficient handling of complex tasks. Long-term memory
    capabilities reflect the ability of the LLMs to retain and recall historical information
    during their interactive processes with the environment. Considering these abilities,
    we propose a method to enhance the performance of 7B and 13B LLMs on agent tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与商业 LLM 不同，小规模的开源 LLM 通常在一般知识方面相对低效（Peters et al. ([2019](#bib.bib14))）。此外，较小的参数规模限制了推理和记忆能力，通常会导致代理对话过程中出现幻觉（Zhang
    et al. ([2023b](#bib.bib33))）。然而，在实际应用中，拥有 7B 和 13B 参数的 LLM 由于相对易于部署和微调而被广泛使用。因此，提高这些
    LLM 的能力具有重要的实际意义。目前，关于 LLM 代理或提升模型推理能力的研究（Xi et al. ([2023a](#bib.bib25))；Wang
    et al. ([2023](#bib.bib20))）主要集中在大规模模型上。对 7B 和 13B LLM 的代理能力的研究仍处于探索的早期阶段。如前所述，一个熟练的代理需要任务规划能力、使用外部工具的熟练程度和长期记忆能力。任务规划指的是模型将大规模任务分解成可管理的子目标，从而有效处理复杂任务的能力。长期记忆能力反映了
    LLM 在与环境互动过程中保持和回忆历史信息的能力。考虑到这些能力，我们提出了一种提高 7B 和 13B LLM 在代理任务中的表现的方法。
- en: In our proposed approach, We focus on enhancing the agent capabilities of LLMs
    from two key aspects. First, improving the agent capabilities through Supervised
    Fine-Tuning (SFT). This approach fundamentally enhances the LLMs themselves. Unlike
    general reasoning tasks, an agent’s role goes beyond planning and reasoning. It
    also involves continuous interaction with the environment or humans to execute
    subsequent actions until a desired outcome is achieved. To improve the agent abilities
    of LLMs, it is essential to train them on diverse datasets that reflect the full
    range of interactive behaviors between the agent and the environment. This involves
    constructing data that not only records the actions taken by the agent but also
    captures the internal thought processes and decision-making. Additionally, the
    environment should provide meaningful feedback to guide the learning of the agent.
    We propose to use GPT-4 OpenAI ([2023](#bib.bib11)) to construct data. By designing
    a framework that involves GPT-4 engaging the multi-turn dialogues, we can generate
    conversational data that captures the interaction between different roles. During
    these conversations, GPT-4 can take on different roles, such as playing the part
    of an agent, a user, or the environment, and actively participate in dynamic exchanges.
    In addition, we incorporate a significant amount of general instruction tuning
    data into the constructed dataset to preserve the general capabilities of the
    LLMs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的方法中，我们重点从两个关键方面提升 LLM 的代理能力。首先，通过**监督微调**（SFT）来提高代理能力。这种方法从根本上提升了 LLM
    自身。与一般推理任务不同，代理的角色不仅涉及规划和推理，还包括与环境或人类的持续互动，以执行后续行动直到达到期望结果。为了提高 LLM 的代理能力，必须在反映代理与环境之间全范围互动行为的多样化数据集上进行训练。这包括构建记录代理所采取的行动、捕捉内部思考过程和决策的数据。此外，环境还应提供有意义的反馈以指导代理的学习。我们提议使用
    GPT-4 OpenAI ([2023](#bib.bib11)) 来构建数据。通过设计一个涉及 GPT-4 进行多轮对话的框架，我们可以生成捕捉不同角色之间互动的对话数据。在这些对话中，GPT-4
    可以扮演不同的角色，例如代理、用户或环境，并积极参与动态交流。此外，我们还将大量通用指令调优数据纳入构建的数据集中，以保持 LLM 的通用能力。
- en: Besides, we optimize the reasoning path through task decomposition and backtracking.
    Inspired by Chain of Thought Wei et al. ([2022](#bib.bib23)), significant efforts
    have been dedicated to activating the reasoning ability of the LLMs. For instance,
    ReAct Yao et al. ([2022b](#bib.bib30)) integrates the thinking process into the
    task of multi-step reasoning. ToT Yao et al. ([2023](#bib.bib29)) uses depth-first
    and breadth-first traversal of reasoning nodes, which is more conducive to finding
    the optimal solution. We migrate the idea of ToT to the agent tasks and combine
    it with task decomposition and backtracking. Task decomposition leverages the
    task planning capability of the LLMs to decompose complex and lengthy tasks into
    several smaller subtasks. Considering that it is difficult for LLMs to find optimal
    answers or complete tasks through a single reasoning path, we introduce a judgment
    process where the reasoning process goes back to the starting point, termed backtracking.
    Through the integration of task decomposition and backtracking, we aim to enhance
    LLMs’ ability to handle complex tasks effectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通过任务分解和回溯优化推理路径。受到 Chain of Thought Wei 等人（[2022](#bib.bib23)）的启发，已经付出了大量努力来激发
    LLM 的推理能力。例如，ReAct Yao 等人（[2022b](#bib.bib30)）将思维过程整合到多步骤推理任务中。ToT Yao 等人（[2023](#bib.bib29)）使用深度优先和广度优先遍历推理节点，这更有利于找到最优解。我们将
    ToT 的思想迁移到代理任务中，并结合任务分解和回溯。任务分解利用 LLM 的任务规划能力，将复杂且冗长的任务分解为几个较小的子任务。考虑到 LLM 难以通过单一推理路径找到最优答案或完成任务，我们引入了一个判断过程，即推理过程回到起点，这被称为回溯。通过任务分解和回溯的结合，我们旨在增强
    LLM 有效处理复杂任务的能力。
- en: 'The main contributions of this paper are: 1) We explore the capabilities of
    7B and 13B open-source LLMs as agents, exploring their potential in performing
    agent tasks. 2) We propose supervised fine-tuning with specific agent data as
    a fundamental approach to improving the capability of open-source LLMs as agents.
    To achieve this, we develop a method for constructing agent data. 3) We find that
    task decomposition and backtracking are effective approaches for addressing complex
    agent tasks. We conduct experiments on AgentBench and achieve promising results.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献是：1）我们探索了 7B 和 13B 开源 LLM 作为代理的能力，探讨了它们在执行代理任务中的潜力。2）我们提出了使用特定代理数据的监督微调作为提升开源
    LLM 作为代理能力的基本方法。为此，我们开发了一种构建代理数据的方法。3）我们发现任务分解和回溯是解决复杂代理任务的有效方法。我们在 AgentBench
    上进行了实验，并取得了令人满意的结果。
- en: 2 Related Works
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Planning and Reasoning. Planning and reasoning are crucial capacities for agents
    to solve complex tasks. Through the in-context of the thinking chain, Chain-of-Thought
    Wei et al. ([2022](#bib.bib23)) activates the reasoning capabilities of LLMs and
    enables the generation of intermediate thought processes before producing answers.
    Some other strategies have also been proposed to further enhance the thinking
    process of models. For example, SC Wang et al. ([2022](#bib.bib21)) leverages
    the self-consistency of LLMs by generating multiple thinking chains and determining
    the final answer through voting. Reconcile Chen et al. ([2023](#bib.bib2)) enhances
    the reasoning capabilities of LLMs through multiple rounds of discussions and
    using confidence-weighted voting. Besides, self-polish Xi et al. ([2023b](#bib.bib26)),
    and self-refine Madaan et al. ([2023](#bib.bib9)) augment the thinking process
    of LLMs from other perspectives. Furthermore, ToT Yao et al. ([2023](#bib.bib29))
    explores the abstracting reasoning process into deep tree search. In addition,
    there are some works Zhang et al. ([2023c](#bib.bib34)) that apply the idea of
    chain thinking to multi-modal tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 规划和推理。规划和推理是代理解决复杂任务的重要能力。通过 Chain-of-Thought Wei 等人（[2022](#bib.bib23)）中的思维链上下文，激发了
    LLM 的推理能力，并在产生答案之前生成中间思考过程。还提出了一些其他策略，以进一步增强模型的思维过程。例如，SC Wang 等人（[2022](#bib.bib21)）通过生成多个思维链并通过投票确定最终答案，利用了
    LLM 的自一致性。Reconcile Chen 等人（[2023](#bib.bib2)）通过多轮讨论和使用信心加权投票增强了 LLM 的推理能力。此外，self-polish
    Xi 等人（[2023b](#bib.bib26)）和 self-refine Madaan 等人（[2023](#bib.bib9)）从其他角度增强了 LLM
    的思维过程。此外，ToT Yao 等人（[2023](#bib.bib29)）探索了将推理过程抽象化为深度树搜索的方式。此外，还有一些工作 Zhang 等人（[2023c](#bib.bib34)）将链式思维的理念应用于多模态任务。
- en: Large Language Model as Agent. With the rapid advancement of LLMs, extensive
    research has been conducted to explore their powerful capabilities in planning
    and reasoning Xi et al. ([2023a](#bib.bib25)); Wang et al. ([2023](#bib.bib20)).
    This has opened up the possibility of employing LLMs as agents. On the one hand,
    there have been several efforts to apply LLMs to various agent tasks and construct
    agent simulation frameworks. On the other hand, several works Xu et al. ([2023](#bib.bib27));
    Kim et al. ([2023](#bib.bib7)), such as ReAct Yao et al. ([2022b](#bib.bib30)),
    have focused on incorporating reasoning and deliberation into the agent process
    for LLMs. In addition, some works apply the reasoning methods to the agent interaction
    process. PET Wu et al. ([2023](#bib.bib24)) applies task decomposition to the
    household agent environment, which is helpful for LLMs to complete complex tasks.
    LATS Zhou et al. ([2023](#bib.bib35)) and RAP Hao et al. ([2023](#bib.bib5)) apply
    Monte Carlo tree search to the agent reasoning process. It is advantageous to
    find better answers compared with ToT. In addition, research works such as AutoGPT
    Gravitas ([2023](#bib.bib4)) and GPT-Engineer Osika et al. ([2023](#bib.bib12))
    utilize commercial LLMs as agent core of their frameworks, enabling the development
    of comprehensive agent architectures to tackle complex real-world problems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型作为代理。随着 LLMs 的快速发展，广泛的研究已探索其在规划和推理方面的强大能力 Xi 等人（[2023a](#bib.bib25)）；Wang
    等人（[2023](#bib.bib20)）。这开辟了将 LLMs 用作代理的可能性。一方面，已经有多个努力将 LLMs 应用于各种代理任务，并构建代理仿真框架。另一方面，一些工作
    Xu 等人（[2023](#bib.bib27)）；Kim 等人（[2023](#bib.bib7)），如 ReAct Yao 等人（[2022b](#bib.bib30)），专注于将推理和审议融入
    LLMs 的代理过程。此外，一些工作将推理方法应用于代理交互过程。PET Wu 等人（[2023](#bib.bib24)）将任务分解应用于家庭代理环境，这有助于
    LLMs 完成复杂任务。LATS Zhou 等人（[2023](#bib.bib35)）和 RAP Hao 等人（[2023](#bib.bib5)）将蒙特卡罗树搜索应用于代理推理过程。与
    ToT 相比，这有助于找到更好的答案。此外，像 AutoGPT Gravitas（[2023](#bib.bib4)）和 GPT-Engineer Osika
    等人（[2023](#bib.bib12)）这样的研究工作利用商业 LLMs 作为其框架的核心代理，使得开发全面的代理架构以解决复杂的现实世界问题成为可能。
- en: Instruction Tuning for Language Model. Instruction tuning plays a crucial role
    in training LLMs. After pre-training with massive unsupervised data, LLMs acquire
    a substantial amount of knowledge and process language understanding and generation
    capabilities. Further supervised instruction fine-tuning Zhang et al. ([2023a](#bib.bib32));
    Dong et al. ([2022](#bib.bib3)) is conducted to align the model with human instructions
    and generate outputs that better align with human preferences. Instruction tuning
    mainly focuses on constructing complex and diverse general-purpose tasks to train
    LLMs to answer questions in a human manner. For example, FLAN Wei et al. ([2021](#bib.bib22))
    and T0 Sanh et al. ([2021](#bib.bib17)) construct a multi-task instruction tuning
    dataset using massive publicly available datasets. The fine-tuned model shows
    strong zero-shot generalizability. In addition to utilizing existing datasets,
    another common approach is to generate data using commercial LLMs. Self-Instruct
    Wang et al. ([2022](#bib.bib21)); Peng et al. ([2023](#bib.bib13)) leverages GPT-4
    to generate a large amount of diverse data, given a few seed tasks. These data
    are used for fine-tuning open-source LLMs and get significant improvements in
    various tasks. To enhance the agent capability of LLMs, AgentTuning Zeng et al.
    ([2023](#bib.bib31)) utilizes commercial LLMs to construct data in specific agent
    environments containing multi-turn dialogues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的指令调优。指令调优在训练大型语言模型（LLMs）中扮演着至关重要的角色。在经过大量无监督数据的预训练后，LLMs获得了大量知识，并具备了语言理解和生成的能力。进一步的监督指令微调
    Zhang 等人（[2023a](#bib.bib32)）；Dong 等人（[2022](#bib.bib3)）旨在使模型与人类指令对齐，生成更符合人类偏好的输出。指令调优主要集中在构建复杂而多样化的通用任务，以训练
    LLMs 以人类的方式回答问题。例如，FLAN Wei 等人（[2021](#bib.bib22)）和 T0 Sanh 等人（[2021](#bib.bib17)）利用大量公开可用的数据集构建了一个多任务指令调优数据集。经过微调的模型展现了强大的零-shot
    泛化能力。除了利用现有数据集，另一种常见方法是使用商业 LLMs 生成数据。Self-Instruct Wang 等人（[2022](#bib.bib21)）；Peng
    等人（[2023](#bib.bib13)）利用 GPT-4 生成大量多样化的数据，给定一些种子任务。这些数据用于微调开源 LLMs，并在各种任务中获得了显著提升。为了增强
    LLMs 的代理能力，AgentTuning Zeng 等人（[2023](#bib.bib31)）利用商业 LLMs 在包含多轮对话的特定代理环境中构建数据。
- en: 3 Methodology
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: In this section, we first give a formal definition of LLMs as agents. Then,
    we introduce the two components of our approach. In the first part, we construct
    agent-tuning data to fine-tune LLMs with parameter-efficient tuning methods. This
    is a way to fundamentally improve the capabilities of LLMs. In the second part,
    we propose enhancing the reasoning capabilities of LLMs through task decomposition
    and backtracking.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先给出LLMs作为代理的正式定义。然后，我们介绍我们方法的两个组成部分。在第一部分，我们构建代理微调数据，以参数高效的微调方法来微调LLMs。这是一种从根本上提高LLMs能力的方法。在第二部分，我们提出通过任务分解和回溯来增强LLMs的推理能力。
- en: 3.1 Problem Formulation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: For a given agent task, the interaction trajectory of LLMs as agents can be
    represented as a dialogue history $(e_{1},a_{1},...,e_{n},a_{n})$, which reflects
    the completion of the task.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的代理任务，LLMs 作为代理的交互轨迹可以表示为对话历史 $(e_{1},a_{1},...,e_{n},a_{n})$，这反映了任务的完成情况。
- en: 3.2 Supervised Tuning with General and Constructed Agent Data
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用通用和构建的代理数据进行监督微调
- en: We observe a significant disparity in the agent capabilities between the open-source
    7B and 13B LLMs and the commercial models. In the dialogue process, open-source
    models often exhibit issues such as formatting errors, getting stuck in infinite
    loops, and generating hallucinatory outputs. To reduce the occurrence of the above
    issues, a fundamental approach is to fine-tune the LLMs with appropriate data.
    However, the agent is engaged in multi-turn dialogues and interacts with specific
    environments, which is different from currently available open-source general-purpose
    instruction data. To solve this challenge, we leverage commercial models API to
    construct agent-specific data and merge them with general instruction datasets
    to fine-tune the low-parameter LLMs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到开源7B和13B LLM与商业模型之间的代理能力存在显著差异。在对话过程中，开源模型经常出现格式错误、陷入无限循环和生成虚假输出等问题。为减少这些问题的发生，一个基本的方法是用适当的数据对LLMs进行微调。然而，代理涉及多轮对话并与特定环境互动，这与当前可用的开源通用指令数据不同。为了解决这一挑战，我们利用商业模型API构建特定于代理的数据，并将其与通用指令数据集合并，以微调低参数LLMs。
- en: '![Refer to caption](img/d4b31aef5816ca2c4a8618ec1ed7ca62.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4b31aef5816ca2c4a8618ec1ed7ca62.png)'
- en: 'Figure 2: The process of constructing agent data. For task planning and external
    tool usage capabilities, we use two strategies, respectively.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：构建代理数据的过程。对于任务规划和外部工具使用能力，我们分别使用两种策略。
- en: 'As agents, LLMs need to possess three fundamental capabilities: task planning,
    long-term memory, and tool usage. To enhance the task planning capabilities of
    LLMs, we take ALFworld Shridhar et al. ([2020](#bib.bib19)) as an example to construct
    data with interactive trajectories. Unlike current methods of constructing data
    using models like GPT-3.5 OpenAI ([2022](#bib.bib10)), data for agents should
    not only involve multi-turn dialogues but also need to reflect task planning and
    trajectory. Therefore, we meticulously design the construction process of the
    dataset, dividing the process of each piece of data into three steps. It includes
    task construction, trajectory interaction, and manual filtering. This approach
    ensures that each piece of data captures the necessary elements for training agents
    effectively. We utilize GPT-3.5 or GPT-4 to generate questions and interaction
    trajectories and this process can be easily extended to other agent tasks. As
    illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Supervised Tuning with General
    and Constructed Agent Data ‣ 3 Methodology ‣ Enhancing the General Agent Capabilities
    of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning") right, to generate
    a complete interaction trajectory, we simulate GPT playing three distinct roles
    in a household environment. These roles are named as question generator, action
    maker, and environmental agent.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代理，LLM 需要具备三种基本能力：任务规划、长期记忆和工具使用。为了增强 LLM 的任务规划能力，我们以 ALFworld Shridhar 等人
    ([2020](#bib.bib19)) 为例，构建具有交互轨迹的数据。与当前使用 GPT-3.5 OpenAI ([2022](#bib.bib10))
    等模型构建数据的方法不同，代理的数据不仅应包含多轮对话，还需要体现任务规划和轨迹。因此，我们精心设计数据集的构建过程，将每个数据的过程分为三个步骤：任务构建、轨迹互动和人工筛选。这种方法确保每个数据片段都捕捉到有效训练代理所需的元素。我们利用
    GPT-3.5 或 GPT-4 生成问题和互动轨迹，并且这一过程可以轻松扩展到其他代理任务。如图[2](#S3.F2 "图 2 ‣ 3.2 使用通用和构建的代理数据进行监督调优
    ‣ 3 方法论 ‣ 通过调优和多分支推理提升低参数 LLM 的通用代理能力")所示，为了生成完整的互动轨迹，我们模拟 GPT 在家庭环境中扮演三个不同的角色。这些角色被命名为问题生成器、行动制定者和环境代理。
- en: First, we randomly initialize a specific room environment, determining the number
    and placement of household items. The question generator role is then responsible
    for generating intelligent household-related questions based on the provided environment.
    Subsequently, the action maker role continuously offers its thoughts and actions
    based on the environment feedback, simultaneously, the environment agent role
    provides reasonable feedback and cues corresponding to the actions taken in each
    step. These two roles continue to interact until the problem is completed or the
    maximum number of interactions is reached, thus generating a complete trajectory.
    However, as there is no assurance of the logical consistency of the environment
    agent’s feedback and the action maker’s actions, manual screening is required
    after the data is generated.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们随机初始化一个特定的房间环境，确定家庭物品的数量和摆放位置。然后，问题生成器角色负责根据提供的环境生成智能的家庭相关问题。随后，行动制定者角色根据环境反馈持续提供其思路和行动，同时，环境代理角色提供与每一步所采取的行动相对应的合理反馈和提示。这两个角色继续互动，直到问题完成或达到最大交互次数，从而生成完整的轨迹。然而，由于无法保证环境代理的反馈和行动制定者的行动在逻辑上的一致性，因此在数据生成后需要进行人工筛选。
- en: In addition to agent tasks that focus on task planning, there are also agent
    tasks such as Operating System, and WebShop Yao et al. ([2022a](#bib.bib28)) that
    have fewer dialogue rounds and prioritize the use of external tools. For this
    type of task, we draw on the idea of in-context learning. Specifically, as shown
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Supervised Tuning with General and Constructed
    Agent Data ‣ 3 Methodology ‣ Enhancing the General Agent Capabilities of Low-Parameter
    LLMs through Tuning and Multi-Branch Reasoning") left, we provide GPT with examples
    with complete reasoning trajectories to enable it to imitate. Subsequently, we
    manually filter and select logically consistent data from generated outputs. We
    expect to use this type of data to improve the retrieval capabilities and tool
    usage capabilities of LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了专注于任务规划的代理任务，还有如操作系统和WebShop Yao等人（[2022a](#bib.bib28)）这样的代理任务，这些任务对话轮次较少，并优先使用外部工具。对于这种类型的任务，我们借鉴了上下文学习的思想。具体来说，如图[2](#S3.F2
    "图2 ‣ 3.2 使用通用和构建的代理数据进行监督调整 ‣ 3 方法 ‣ 通过调整和多分支推理增强低参数LLMs的通用代理能力")所示，我们向GPT提供具有完整推理轨迹的示例以使其模仿。随后，我们手动过滤并选择逻辑上一致的数据。我们期望利用这种数据来提高LLMs的检索能力和工具使用能力。
- en: 'Existing work on agent fine-tuning Zeng et al. ([2023](#bib.bib31)) shows that
    using only agent data to fine-tune LLMs compromises their generalizability. Therefore,
    we mix some general instruction tunning data into our agent data when fine-tuning
    LLMs. Suppose $M_{\theta}$. We optimize the loss function as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Zeng等人（[2023](#bib.bib31)）在代理微调方面的现有工作表明，仅使用代理数据来微调LLMs会影响其泛化能力。因此，我们在微调LLMs时，将一些通用指令调整数据混入我们的代理数据中。假设$M_{\theta}$。我们优化损失函数如下：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: Where $\lambda\in[0,1]$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\lambda\in[0,1]$。
- en: In the context of fine-tuning strategiy, we adopt Low-Rank Adaptation (LORA)
    Hu et al. ([2021](#bib.bib6)) fine-tuning which is based on making low-rank modifications
    to the weight matrices in LLMs. For each linear layer in the model, the original
    weight matrix $W$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调策略的背景下，我们采用了低秩适配（LORA）Hu等人（[2021](#bib.bib6)）微调方法，该方法基于对LLMs中的权重矩阵进行低秩修改。对于模型中的每个线性层，原始权重矩阵$W$。
- en: 3.3 Multi-Path Reasoning under Task Decomposition
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 任务分解下的多路径推理
- en: Recently, because it is difficult for a single agent to complete complex multi-step
    tasks, more and more work tends to involve multi-agent collaboration, allowing
    models to play different roles to jointly advance tasks Qiao et al. ([2024](#bib.bib15)).
    We take a similar approach. On the one hand, we we instruct LLMs to generate multiple
    available actions in each reasoning step. On the other hand, we employ a judge
    model to select one action from the provided set and continue the reasoning process
    until a final output is obtained.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，由于单一代理难以完成复杂的多步骤任务，越来越多的工作倾向于多代理协作，使模型扮演不同角色共同推进任务 Qiao等人（[2024](#bib.bib15)）。我们采用类似的方法。一方面，我们指示LLMs在每个推理步骤中生成多个可用的行动。另一方面，我们使用一个判断模型从提供的集合中选择一个行动，并继续推理过程，直到得到最终输出。
- en: '![Refer to caption](img/13f721ff25a1a1591f02282201c17ec3.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13f721ff25a1a1591f02282201c17ec3.png)'
- en: 'Figure 3: The process of task decomposition. The planning model breaks the
    entire task into several small subtasks.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：任务分解的过程。规划模型将整个任务分解为几个小的子任务。
- en: For LLMs with small parameter sizes, due to their limited long-term memory capacity,
    it is challenging for them to handle complex long dialogue tasks. To address this
    issue, we employ a task decomposition strategy, where complex tasks that require
    multiple steps are broken down into simpler subtasks. We use another LLM with
    the same number of parameters as our planning module and we name it as $M_{p}$
    to a LLM and get the output of "Yes" or "No" until the subtask is completed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数较小的LLMs，由于其长期记忆容量有限，处理复杂的长对话任务具有挑战性。为了解决这个问题，我们采用任务分解策略，将需要多个步骤的复杂任务分解为更简单的子任务。我们使用另一个参数数量相同的LLM作为规划模块，并将其命名为$M_{p}$，直到子任务完成，我们从LLM获得“Yes”或“No”的输出。
- en: Agent tasks in the real world are often complex and one single reasoning path
    may not yield the optimal answer. Inspired by the reflective ability in human
    thinking processes, we propose to take multi-path reasoning with LLMs. We call
    this method backtracking. When a particular reasoning path yields a suboptimal
    output, we compose a backtracking prompt as "it was observed that the answer was
    not the optimal choice for task $\mathcal{T}$…". We also prompt the LLMs to eschew
    reasoning paths that have been previously deduced. To this end, we compose the
    prompt as "it is important to note that actions should be adjusted appropriately
    based on the historical information" and we splice this prompt behind the backtracking
    prompt. Furthermore, backtracking and task decomposition are not mutually exclusive
    and can be applied together in the reasoning process of LLMs. We find that task
    decomposition is more effective for agent tasks that emphasize planning abilities,
    while backtracking is more effective for agent tasks that emphasize API invocation
    capabilities.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的代理任务通常是复杂的，单一的推理路径可能无法提供**最佳答案**。受到人类思维过程中的反思能力的启发，我们建议使用LLMs进行多路径推理。我们将这种方法称为回溯。当特定的推理路径产生次优输出时，我们会编写一个回溯提示，内容为“观察到答案对任务$\mathcal{T}$来说不是最佳选择...”。我们还会提示LLMs避免之前已推导出的推理路径。为此，我们编写提示为“重要的是，根据历史信息适当地调整行动”，并将此提示添加到回溯提示之后。此外，回溯和任务分解并不是互相排斥的，可以在LLMs的推理过程中一起应用。我们发现，任务分解在强调规划能力的代理任务中更为有效，而回溯在强调API调用能力的代理任务中更为有效。
- en: '![Refer to caption](img/c0928b5036780fa36917a89c33ea4e81.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c0928b5036780fa36917a89c33ea4e81.png)'
- en: 'Figure 4: The comparison of different reasoning methods. From the left to right
    are Input Output (IO), ToT and our method.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不同推理方法的比较。从左到右分别是输入输出（IO）、ToT和我们的方法。
- en: Overall, our method is divided into two parts. The first part uses commercial
    LLMs to construct agent data and employs SFT to fundamentally enhance the agent
    capabilities of low-parameter LLMs. In the second part, while keeping the LLMs
    unchanged, it maximizes the activation of the agent capabilities by incorporating
    multi-path reasoning and task decomposition. For 7B and 13B LLMs, common issues
    such as hallucinatory outputs and forgetting errors often occur. By fine-tuning
    the LLMs on domain-specific data that adheres to the desired format, these issues
    can be significantly mitigated. For reasoning problems with vast search spaces,
    finding the optimal solution through a single inference path is challenging. This
    issue cannot be effectively addressed through supervised fine-tuning alone. However,
    by introducing techniques such as multi-path reasoning and task decomposition,
    the complexity of the problem can be reduced, facilitating the identification
    of the optimal solution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的方法分为两个部分。第一部分使用商业LLMs构建代理数据，并利用SFT从根本上提升低参数LLMs的代理能力。在第二部分，保持LLMs不变，通过引入多路径推理和任务分解来最大化激活代理能力。对于7B和13B的LLMs，常见问题如幻觉输出和遗忘错误时有发生。通过在符合所需格式的领域特定数据上对LLMs进行微调，可以显著缓解这些问题。对于搜索空间广阔的推理问题，通过单一推理路径找到最佳解决方案是具有挑战性的。仅通过监督微调无法有效解决这一问题。然而，通过引入多路径推理和任务分解等技术，可以降低问题的复杂性，便于找到最佳解决方案。
- en: '|  | Data type | Operating System | DataBase | Webshop | ALFWorld | Mind2web
    | Avg. $\uparrow$ |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据类型 | 操作系统 | 数据库 | 网上商店 | ALFWorld | Mind2web | 平均 $\uparrow$ |'
- en: '| GPT-4 |  | 42.4 | 32 | 61.1 | 78 | 29 | 48.50 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 |  | 42.4 | 32 | 61.1 | 78 | 29 | 48.50 |'
- en: '| GPT-3.5-turbo |  | 32.6 | 36.7 | 64.1 | 16 | 20 | 33.88 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo |  | 32.6 | 36.7 | 64.1 | 16 | 20 | 33.88 |'
- en: '| claude |  | 9.7 | 22 | 55.7 | 58 | 25 | 34.08 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| claude |  | 9.7 | 22 | 55.7 | 58 | 25 | 34.08 |'
- en: '| llama2-chat w/o sft |  | 3.8 | 2.66 | 0 | 0 | 5.68 | 2.43 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| llama2-chat w/o sft |  | 3.8 | 2.66 | 0 | 0 | 5.68 | 2.43 |'
- en: '| codegen-struct | code | 3.8 | 1.3 | 0 | 0 | 0 | 1.27 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| codegen-struct | 代码 | 3.8 | 1.3 | 0 | 0 | 0 | 1.27 |'
- en: '| alpaca-code | 3.8 | 1.3 | 4.20 | 0 | 5.68 | 2.99 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| alpaca-code | 3.8 | 1.3 | 4.20 | 0 | 5.68 | 2.99 |'
- en: '| open-assistant | dialog | 0 | 2.67 | 2.70 | 0 | 3.41 | 1.76 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| open-assistant | 对话 | 0 | 2.67 | 2.70 | 0 | 3.41 | 1.76 |'
- en: '| alpaca | instro+agent | 15.38 | 3.33 | 31.10 | 0 | 8.52 | 11.67 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| alpaca | instro+agent | 15.38 | 3.33 | 31.10 | 0 | 8.52 | 11.67 |'
- en: '| agenttuning | 15.38 | 38.30 | 32.60 | 10 | 7.38 | 20.73 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| agenttuning | 15.38 | 38.30 | 32.60 | 10 | 7.38 | 20.73 |'
- en: '| ours | 11.54 | 27.0 | 34.53 | 10 | 9.66 | 18.33 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ours | 11.54 | 27.0 | 34.53 | 10 | 9.66 | 18.33 |'
- en: 'Table 1: The experimental results of fine-tuning LLMs with different instruction
    tuning datasets on AgentBench tasks. We use llama2-7b-chat as the base model.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 AgentBench 任务上使用不同指令调优数据集对 LLM 进行微调的实验结果。我们使用 llama2-7b-chat 作为基础模型。
- en: 4 Experiments
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Agent Datasets: We select five tasks from AgentBench benchmark Liu et al. ([2023](#bib.bib8)):
    ALFWorld, WebShop, Mind2Web, Operating System, and Database. Next, we will introduce
    each agent task one by one in detail.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Agent 数据集：我们从 AgentBench 基准 Liu 等人 ([2023](#bib.bib8)) 中选择了五个任务：ALFWorld、WebShop、Mind2Web、Operating
    System 和 Database。接下来，我们将逐一详细介绍每个代理任务。
- en: ALFWorld is designed to evaluate the planning ability of LLMs in a simulated
    home environment. The model needs to make decisions and execute actions through
    a text interface based on the environment description and target instructions,
    and dynamically adjust the plan to complete the task.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ALFWorld 旨在评估 LLM 在模拟家庭环境中的规划能力。该模型需要根据环境描述和目标指令，通过文本界面做出决策并执行动作，并动态调整计划以完成任务。
- en: WebShop aims to evaluate the performance of LLMs in a simulated online shopping
    environment that mimics a real e-commerce website.The goal of the evaluation is
    to require LLMs to shop in a virtual shopping environment according to instructions
    and select products that meet desired attributes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: WebShop 的目标是评估 LLM 在模拟的在线购物环境中的表现，该环境模拟了真实的电子商务网站。评估的目标是要求 LLM 根据指令在虚拟购物环境中购物，并选择符合期望属性的产品。
- en: Mind2Web is a general web agent evaluation benchmark designed to evaluate the
    ability of LLMs to perform complex tasks on websites in different domains. The
    dataset covers a cross-domain test set across multiple websites. Each task includes
    a task description, a reference action sequence, and web page information and
    is designed to test the performance of LLMs in web browsing and interactive environments.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Mind2Web 是一个通用的网页代理评估基准，旨在评估 LLM 在不同领域的网站上执行复杂任务的能力。该数据集覆盖了多个网站的跨领域测试集。每个任务包括任务描述、参考动作序列以及网页信息，并旨在测试
    LLM 在网页浏览和交互环境中的表现。
- en: Operating System is designed to evaluate the ability of LLMs to perform tasks
    in the Bash environment of a real operating system. Tasks includes question answering
    and action, where the model needs to generate commands to solve a problem or perform
    an action.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Operating System 旨在评估 LLM 在真实操作系统的 Bash 环境中执行任务的能力。任务包括回答问题和执行操作，其中模型需要生成命令以解决问题或执行操作。
- en: DataBase is designed to evaluate the ability of LLMs to operate via SQL on real
    databases. The dataset contains a diverse set of instructions and databases, created
    by combining multiple existing datasets and performing data augmentation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DataBase 旨在评估 LLM 在真实数据库上通过 SQL 操作的能力。数据集包含多样化的指令和数据库，通过结合多个现有数据集并进行数据增强而创建。
- en: 'Implementation details: We use AgentBench as our benchmark and conduct experiments
    based on it. For 13B models, we choose OpenChat. OpenChat is a series of open-source
    LLMs fine-tuned on diverse and high-quality datasets of multi-round conversations.
    We select two models, openchat-v3.2 and openchat-v3.2-super for experiments. For
    the 7B models, we select llama2 and agentlm Zeng et al. ([2023](#bib.bib31)) for
    experiments. We use the fastchat framework to deploy LLMs and we use four RTX
    4090 NVIDIA GPUs. See also the project page¹¹1[https://github.com/HAIV-Lab/LLM-TMBR](https://github.com/HAIV-Lab/LLM-TMBR).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节：我们使用 AgentBench 作为基准并在此基础上进行实验。对于 13B 模型，我们选择 OpenChat。OpenChat 是一系列基于多轮对话的多样化高质量数据集微调的开源
    LLM。我们选择了两个模型，openchat-v3.2 和 openchat-v3.2-super 进行实验。对于 7B 模型，我们选择了 llama2 和
    agentlm Zeng 等人 ([2023](#bib.bib31)) 进行实验。我们使用 fastchat 框架来部署 LLM，并使用四个 RTX 4090
    NVIDIA GPU。更多信息请参见项目页面¹¹1[https://github.com/HAIV-Lab/LLM-TMBR](https://github.com/HAIV-Lab/LLM-TMBR)。
- en: 4.1 Experimental Results
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验结果
- en: Supervised fine-tuning with constructed dataset. The experiments of supervised
    fine-tuning are shown in Tab. [1](#S3.T1 "Table 1 ‣ 3.3 Multi-Path Reasoning under
    Task Decomposition ‣ 3 Methodology ‣ Enhancing the General Agent Capabilities
    of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning"). We fine-tune
    the 7B model on various instruction-tuning datasets and test it on five agent
    tasks. It can be seen that fine-tuning on various instruction datasets has a positive
    effect on improving the capabilities of agents. Among them, we find that fine-tuning
    the LLMs using code-type instructions has shown relatively limited effectiveness
    in improving agent capabilities. For example, after fine-tuning on alpaca-code
    dataset, the performance of llama2 on operating system task does not improve,
    and its performance on database tasks actually declined by $1.33\%$. We analyze
    that although code-type data can enhance the understanding of the code of LLMs,
    it lacks dialogue processes and the decomposition of complex problems. Similar
    to code-type data, fine-tuning LLMs on regular dialog data alone is not an appropriate
    choice for enhancing its agent capabilities. For instance, after fine-tuning on
    Open-Assistant, llama2 exhibited a decrease in performance on operating system
    task and a lower improvement on the webshop task compared to other datasets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用构建数据集的监督微调。监督微调的实验结果见表 [1](#S3.T1 "表 1 ‣ 3.3 任务分解下的多路径推理 ‣ 3 方法论 ‣ 通过调优和多分支推理提升低参数LLMs的通用能力")。我们对7B模型进行了各种指令微调数据集的微调，并在五个代理任务上进行了测试。可以看出，在各种指令数据集上的微调对提升代理的能力有积极效果。其中，我们发现使用代码类型指令进行LLM微调的效果相对有限。例如，在alpaca-code数据集上进行微调后，llama2在操作系统任务上的表现没有改善，其在数据库任务上的表现实际上下降了$1.33\%$。我们分析认为，尽管代码类型数据可以增强LLMs对代码的理解，但它缺乏对话过程和复杂问题的分解。类似于代码类型数据，单独在常规对话数据上对LLMs进行微调也不是提升其代理能力的合适选择。例如，在Open-Assistant上进行微调后，llama2在操作系统任务上的表现下降，且在webshop任务上的提升低于其他数据集。
- en: '|    Size | LLMs | Methods |    Webshop | ALFWorld | Operate System | Avg.
    $\uparrow$ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|    Size | LLMs | Methods |    Webshop | ALFWorld | Operate System | Avg.
    $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 13B | openchat_v3.2 | IO | 1 | 0 | 0 | 0.33 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 13B | openchat_v3.2 | IO | 1 | 0 | 0 | 0.33 |'
- en: '| CoT | 19 | 0 | 0 | 6.33 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 19 | 0 | 0 | 6.33 |'
- en: '| ReAct | 26 | 5 | 7.6 | 12.86 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 26 | 5 | 7.6 | 12.86 |'
- en: '| Ours | 27 | 10 | 7.6 | 14.86 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 27 | 10 | 7.6 | 14.86 |'
- en: '| openchat_v3.2_super | IO | 5 | 0 | 0 | 1.66 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| openchat_v3.2_super | IO | 5 | 0 | 0 | 1.66 |'
- en: '| CoT | 23 | 0 | 0 | 7.66 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 23 | 0 | 0 | 7.66 |'
- en: '| ReAct | 30 | 5 | 3.8 | 12.93 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 30 | 5 | 3.8 | 12.93 |'
- en: '| Ours | 31 | 11 | 3.8 | 15.26 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 31 | 11 | 3.8 | 15.26 |'
- en: '| 7B | AgentLM-7B | IO | 50 | 5 | 3.8 | 20.86 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 7B | AgentLM-7B | IO | 50 | 5 | 3.8 | 20.86 |'
- en: '| CoT | 34 | 5 | 7.6 | 19.50 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 34 | 5 | 7.6 | 19.50 |'
- en: '| ReAct | 33 | 0 | 7.6 | 13.53 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 33 | 0 | 7.6 | 13.53 |'
- en: '| Ours | 51 | 0 | 7.6 | 19.53 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 51 | 0 | 7.6 | 19.53 |'
- en: '| llama2-7B | IO | 0 | 0 | 0 | 0 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| llama2-7B | IO | 0 | 0 | 0 | 0 |'
- en: '| CoT | 4 | 0 | 0 | 1.33 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 4 | 0 | 0 | 1.33 |'
- en: '| ReAct | 13.35 | 0 | 7.6 | 6.98 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 13.35 | 0 | 7.6 | 6.98 |'
- en: '| Ours | 13.40 | 0 | 7.6 | 7.00 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Ours | 13.40 | 0 | 7.6 | 7.00 |'
- en: 'Table 2: Experimental results of different reasoning methods on three agent
    benchmarks.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 不同推理方法在三个代理基准上的实验结果。'
- en: Besides, we find that fine-tuning LLMs on high-quality general instruction tuning
    datasets can significantly improve its agent capabilities. For example, after
    fine-tuning with alpaca instruction tuning data, llama2 exhibit significant improvements
    across multiple agent tasks. In the operating system tasks and webshop tasks,
    llama2 tuning with alpaca data achieves nearly comparable results to those obtained
    through agenttuning. Agenttuning is the most effective tuning dataset. It combines
    GPT-4 assisted trajectory-labeled agent data with general instruction tuning data,
    resulting in significant improvements for llama2 across different agent tasks.
    Its performance in the database even exceeds that of the commercial model. Fine-tuning
    the model using our constructed data can also improve the performance of LLMs
    on agent tasks. Although we construct limited and easy-to-collect data, the performance
    of LLMs fine-tuned with our data exceeds other datasets on some agent tasks. For
    example, on operating system tasks, our results are $7.74\%$ higher than dialog-type
    datasets. Compared with agenttuning, our results are still far behind, which can
    be attributed to the limited amount of data. In addition, there are fewer complex
    tasks involving long conversations in our data, which is also one of the reasons.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现，在高质量的一般指令调整数据集上微调 LLM 可以显著提升其代理能力。例如，在使用 alpaca 指令调整数据微调后，llama2 在多个代理任务中表现出显著的提升。在操作系统任务和网页商店任务中，llama2
    使用 alpaca 数据的调整结果几乎与通过 agenttuning 获得的结果相当。Agenttuning 是最有效的调整数据集。它将 GPT-4 辅助的轨迹标注代理数据与一般指令调整数据相结合，从而在不同代理任务中显著提升
    llama2 的表现。其在数据库中的表现甚至超过了商业模型。使用我们构建的数据微调模型也能提升 LLM 在代理任务上的表现。尽管我们构建的数据有限且易于收集，但使用我们数据微调的
    LLM 在一些代理任务上的表现超过了其他数据集。例如，在操作系统任务上，我们的结果比对话型数据集高出 $7.74\%$。与 agenttuning 相比，我们的结果仍然远远落后，这可以归因于数据量有限。此外，我们的数据中涉及长对话的复杂任务较少，这也是原因之一。
- en: Reasoning with task decomposition and backtracking. We compare different reasoning
    methods on 7B and 13B LLMs, and the results are shown in Tab. [2](#S4.T2 "Table
    2 ‣ 4.1 Experimental Results ‣ 4 Experiments ‣ Enhancing the General Agent Capabilities
    of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning"). The 7B LLMs
    we evaluated are fine-tuned with agent data. AgentLM is fine-tuned with agenttuning
    data, and llama2 is fine-tuned with the data we constructed. We mainly conduct
    evaluations on webshop, household and operating system tasks. It can be seen that
    applying ReAct to various tasks is usually better than direct input and output
    (IO). For example, on the openchat-v3.2 model, ReAct is $18\%$, respectively,
    on the 13B LLMs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分解和回溯推理。我们在 7B 和 13B LLM 上比较了不同的推理方法，结果见表[2](#S4.T2 "表 2 ‣ 4.1 实验结果 ‣ 4 实验
    ‣ 通过调整和多分支推理增强低参数 LLM 的通用代理能力")。我们评估的 7B LLM 经过了代理数据的微调。AgentLM 使用 agenttuning
    数据进行微调，而 llama2 使用我们构建的数据进行微调。我们主要在网页商店、家庭和操作系统任务上进行评估。可以看出，将 ReAct 应用于各种任务通常比直接的输入和输出（IO）更好。例如，在
    openchat-v3.2 模型上，ReAct 在 13B LLM 上的改进幅度为 $18\%$。
- en: '![Refer to caption](img/5ee65e9092d8bd10deb37531d2fbca2b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ee65e9092d8bd10deb37531d2fbca2b.png)'
- en: 'Figure 5: Comparison of ReAct and our method in agent task reasoning. We show
    the action and observation in webshop and household tasks.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：ReAct 和我们方法在代理任务推理中的比较。我们展示了网页商店和家庭任务中的行动和观察。
- en: To delve into the impact of different reasoning methods on the results, we compare
    ReAct and our reasoning process as shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1 Experimental
    Results ‣ 4 Experiments ‣ Enhancing the General Agent Capabilities of Low-Parameter
    LLMs through Tuning and Multi-Branch Reasoning"). It can be seen that ReAct can
    prompt LLMs to think in each reasoning step, the models can still experience issues
    such as getting stuck in infinite loops and suffering from memory confusion. In
    contrast, on household tasks, since we break down complex tasks into several smaller
    tasks, model thinking is less error-prone than ReAct.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为深入探讨不同推理方法对结果的影响，我们比较了 ReAct 和我们的推理过程，如图[5](#S4.F5 "图 5 ‣ 4.1 实验结果 ‣ 4 实验 ‣
    通过调整和多分支推理增强低参数 LLM 的通用代理能力")所示。可以看出，ReAct 可以促使 LLM 在每个推理步骤中思考，但模型仍然会遇到如陷入无限循环和记忆混乱等问题。相比之下，在家庭任务中，由于我们将复杂任务拆解为几个较小的任务，模型的思考比
    ReAct 更不容易出错。
- en: 4.2 Ablation Study
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融研究
- en: The experiments of num path and branch. "num path" refers to the number of backtracking
    iterations conducted, with a higher value indicating an increase in the number
    of reasoning paths explored. We conduct experiments of "num path" shown in Tab. [3](#S4.T3
    "Table 3 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ Enhancing the General Agent Capabilities
    of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning") left. It can
    be seen that appropriately increasing "num path" can improve performance, but
    when "num path" is greater than $2$, performance decreases.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: num path和branch的实验。“num path”指的是进行的回溯迭代次数，值越高表示探索的推理路径增加。我们在表[3](#S4.T3 "Table
    3 ‣ 4.2 Ablation Study ‣ 4 Experiments ‣ Enhancing the General Agent Capabilities
    of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning")中进行了“num path”的实验。可以看出，适当地增加“num
    path”可以提高性能，但当“num path”大于$2$时，性能会下降。
- en: We conduct experiments on the mixing ratio of different general data and agent
    data as shown in Fig.[4](#S4.T4 "Table 4 ‣ 4.2 Ablation Study ‣ 4 Experiments
    ‣ Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning
    and Multi-Branch Reasoning"). We find that too much agent data will not bring
    huge improvements, and general data is equally important.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对不同的通用数据和代理数据的混合比进行实验，如图[4](#S4.T4 "Table 4 ‣ 4.2 Ablation Study ‣ 4 Experiments
    ‣ Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning
    and Multi-Branch Reasoning")所示。我们发现，过多的代理数据不会带来巨大的改进，通用数据同样重要。
- en: '| num path | Webshop | num branch | Webshop |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| num path | Webshop | num branch | Webshop |'
- en: '| --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 20.29 | 1 | 26.00 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 20.29 | 1 | 26.00 |'
- en: '| 2 | 27.00 | 2 | 27.00 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 27.00 | 2 | 27.00 |'
- en: '| 3 | 17.84 | 3 | 6.80 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 17.84 | 3 | 6.80 |'
- en: '| 4 | 16.67 | 4 | 15.80 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 16.67 | 4 | 15.80 |'
- en: 'Table 3: The experimental results of the effect of num path and num branch
    in our reasoning method.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：我们推理方法中num path和num branch的实验结果。
- en: '| $\lambda$ | Alfworld | Webshop | Mind2web | OS |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda$ | Alfworld | Webshop | Mind2web | OS |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0.1 | 0.0 | 38.13 | 6.81 | 0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 0.1 | 0.0 | 38.13 | 6.81 | 0 |'
- en: '| 0.3 | 0.0 | 30.06 | 7.95 | 0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 0.3 | 0.0 | 30.06 | 7.95 | 0 |'
- en: '| 0.5 | 0.0 | 36.42 | 7.95 | 3.8 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.0 | 36.42 | 7.95 | 3.8 |'
- en: '| 0.8 | 5 | 23.35 | 3.97 | 0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 0.8 | 5 | 23.35 | 3.97 | 0 |'
- en: 'Table 4: Experimental results after mixing different general data and agent
    data.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：混合不同通用数据和代理数据后的实验结果。
- en: 5 Conclusion
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: LLMs as intelligent agents have demonstrated powerful agent capabilities. In
    this work, we explore the 7B and 13B LLMs as agents, and propose to enhance the
    agent performance of these open-source models by supervised fine-tuning through
    agent data as well as multi-branch reasoning. SFT can effectively reduce format
    errors and hallucination output of the LLMs, which not only improves the agent
    performance but also facilitates the application of various reasoning methods
    to agent tasks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 作为智能代理，LLMs展示了强大的代理能力。在这项工作中，我们探讨了7B和13B LLM作为代理的情况，并提出通过代理数据和多分支推理来增强这些开源模型的代理性能。SFT可以有效减少LLMs的格式错误和幻觉输出，这不仅提高了代理性能，还促进了各种推理方法在代理任务中的应用。
- en: 6 Limitations
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 局限性
- en: This study presents several limitations. First, our experiments are limited
    to 7B and 13B LLMs, and thus, the applicability of our findings to models of different
    sizes is not verified. The methods we propose may also not be feasible for all
    researchers due to the computational demands of fine-tuning larger models. Additionally,
    measuring reductions in hallucinations and formatting errors is inherently subjective,
    and the performance metrics used may not fully capture the agent capabilities
    in complex real-world tasks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一些局限性。首先，我们的实验仅限于7B和13B LLM，因此我们发现的结果在不同规模的模型上的适用性尚未验证。我们提出的方法可能由于微调更大模型的计算需求而不适合所有研究人员。此外，减少幻觉和格式错误的测量本质上是主观的，使用的性能指标可能无法完全捕捉代理在复杂真实任务中的能力。
- en: The constructed data for SFT could introduce biases and the potential for model
    overfitting, limiting the performance of LLMs on unencountered tasks. Moreover,
    while we implement multi-path reasoning and task decomposition, the strategies
    for optimizing these techniques are not definitive. Our evaluation on a limited
    set of tasks does not account for the full range of an agent capabilities, necessitating
    broader evaluations in future research.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为SFT构建的数据可能引入偏差，并可能导致模型过拟合，从而限制LLMs在未遇到的任务上的表现。此外，虽然我们实现了多路径推理和任务分解，但优化这些技术的策略并不确定。我们对有限任务的评估未能涵盖代理能力的全貌，未来研究需要更广泛的评估。
- en: Acknowledgement
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Acknowledgement
- en: 'The funding for this research was generously provided by the Alibaba Innovation
    Research program under Grant Contract # CRAQ7WHZ11220001-20978282, Natural Science
    Fund of Hubei Province (Grant # 2022CFB823), and HUST Independent Innovation Research
    Fund (Grant # 2021XXJS096).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究的资助得到了阿里巴巴创新研究计划（资助合同 # CRAQ7WHZ11220001-20978282）、湖北省自然科学基金（资助 # 2022CFB823）和华中科技大学自主创新研究基金（资助
    # 2021XXJS096）的慷慨资助。'
- en: References
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等. 2020. 语言模型是少样本学习者。*神经信息处理系统进展*, 33:1877–1901。
- en: 'Chen et al. (2023) Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal.
    2023. Reconcile: Round-table conference improves reasoning via consensus among
    diverse llms. *arXiv preprint arXiv:2309.13007*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2023) Justin Chih-Yao Chen, Swarnadeep Saha, 和 Mohit Bansal. 2023.
    Reconcile: 圆桌会议通过多样化大型语言模型的共识提升推理能力。*arXiv 预印本 arXiv:2309.13007*。'
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, 和 Zhifang Sui. 2022. 语境学习综述。*arXiv 预印本 arXiv:2301.00234*。
- en: 'Gravitas (2023) Significant Gravitas. 2023. Auto-gpt: An autonomous gpt-4 experiment.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gravitas (2023) Significant Gravitas. 2023. Auto-gpt: 一个自主 gpt-4 实验。'
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning
    with world model. *arXiv preprint arXiv:2305.14992*.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, 和 Zhiting Hu. 2023. 用语言模型进行推理即用世界模型进行规划。*arXiv 预印本 arXiv:2305.14992*。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2021. Lora: 大型语言模型的低秩适应。*arXiv
    预印本 arXiv:2106.09685*。'
- en: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language
    models can solve computer tasks. *arXiv preprint arXiv:2303.17491*.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Geunwoo Kim, Pierre Baldi, 和 Stephen McAleer. 2023. 语言模型可以解决计算机任务。*arXiv
    预印本 arXiv:2303.17491*。
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. Agentbench:
    Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang 等. 2023. Agentbench: 将大型语言模型作为智能体进行评估。*arXiv
    预印本 arXiv:2308.03688*。'
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2023. Self-refine: Iterative refinement with self-feedback. *arXiv preprint
    arXiv:2303.17651*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang
    等. 2023. Self-refine: 自我反馈的迭代精炼。*arXiv 预印本 arXiv:2303.17651*。'
- en: OpenAI (2022) OpenAI. 2022. Introducing chatgpt.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 2022. 介绍 chatgpt。
- en: OpenAI (2023) R OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. *View
    in Article*, 2:3.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) R OpenAI. 2023. Gpt-4 技术报告。arxiv 2303.08774。*文章中查看*, 2:3。
- en: Osika et al. (2023) Anton Osika et al. 2023. Gpt engineer.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Osika et al. (2023) Anton Osika 等. 2023. Gpt 工程师。
- en: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. 2023. Instruction tuning with gpt-4. *arXiv preprint arXiv:2304.03277*.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, 和
    Jianfeng Gao. 2023. 使用 gpt-4 进行指令调整。*arXiv 预印本 arXiv:2304.03277*。
- en: Peters et al. (2019) Matthew E Peters, Mark Neumann, Robert L Logan IV, Roy
    Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge enhanced
    contextual word representations. *arXiv preprint arXiv:1909.04164*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters et al. (2019) Matthew E Peters, Mark Neumann, Robert L Logan IV, Roy
    Schwartz, Vidur Joshi, Sameer Singh, 和 Noah A Smith. 2019. 知识增强的上下文词表示。*arXiv
    预印本 arXiv:1909.04164*。
- en: 'Qiao et al. (2024) Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu
    Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. Autoact: Automatic
    agent learning from scratch via self-planning. *arXiv preprint arXiv:2401.05268*.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiao et al. (2024) Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu
    Zhou, Yuchen Eleanor Jiang, Chengfei Lv, 和 Huajun Chen. 2024. Autoact: 通过自我规划从头学习自动智能体。*arXiv
    预印本 arXiv:2401.05268*。'
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, 等. 2023. Toolllm: 促进大型语言模型掌握
    16000+ 真实世界 API。*arXiv 预印本 arXiv:2307.16789*。'
- en: Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,
    Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
    Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization.
    *arXiv preprint arXiv:2110.08207*.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,
    Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
    Arun Raja, 等. 2021. 多任务提示训练实现零样本任务泛化。*arXiv 预印本 arXiv:2110.08207*。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    R Narasimhan, 和 Shunyu Yao. 2023. Reflexion: 带有语言强化学习的语言代理。在 *第37届神经信息处理系统会议*。'
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and
    embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2020. Alfworld: 对齐文本和具身环境以实现互动学习。*arXiv
    预印本 arXiv:2010.03768*。'
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. A survey on
    large language model based autonomous agents. *arXiv preprint arXiv:2308.11432*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2023. 基于大型语言模型的自主代理综述。*arXiv
    预印本 arXiv:2308.11432*。
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022. 自我一致性提升语言模型中的思维链推理。*arXiv
    预印本 arXiv:2203.11171*。
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language
    models are zero-shot learners. *arXiv preprint arXiv:2109.01652*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams
    Wei Yu, Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le. 2021. 微调语言模型是零样本学习者。*arXiv
    预印本 arXiv:2109.01652*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. 思维链提示引发大型语言模型中的推理。*神经信息处理系统进展*，35:24824–24837。
- en: Wu et al. (2023) Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos
    Azaria, Yuanzhi Li, Tom Mitchell, and Shrimai Prabhumoye. 2023. Plan, eliminate,
    and track–language models are good teachers for embodied agents. *arXiv preprint
    arXiv:2305.02412*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos
    Azaria, Yuanzhi Li, Tom Mitchell, 和 Shrimai Prabhumoye. 2023. 计划、消除和跟踪——语言模型是具身代理的优秀教师。*arXiv
    预印本 arXiv:2305.02412*。
- en: 'Xi et al. (2023a) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023a. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2023a) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, 等. 2023a. 大型语言模型基础代理的崛起与潜力：一项综述。*arXiv
    预印本 arXiv:2309.07864*。
- en: 'Xi et al. (2023b) Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao,
    Tao Gui, Qi Zhang, and Xuanjing Huang. 2023b. Self-polish: Enhance reasoning in
    large language models via problem refinement. *arXiv preprint arXiv:2305.14497*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2023b) Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao,
    Tao Gui, Qi Zhang, 和 Xuanjing Huang. 2023b. 自我打磨：通过问题优化提升大型语言模型的推理能力。*arXiv 预印本
    arXiv:2305.14497*。
- en: 'Xu et al. (2023) Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee,
    Yuchen Liu, and Dongkuan Xu. 2023. Rewoo: Decoupling reasoning from observations
    for efficient augmented language models. *arXiv preprint arXiv:2305.18323*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023) Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee,
    Yuchen Liu, 和 Dongkuan Xu. 2023. Rewoo: 将推理与观察解耦以提高增强语言模型的效率。*arXiv 预印本 arXiv:2305.18323*。'
- en: 'Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022a. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, 和 Karthik Narasimhan.
    2022a. Webshop: 朝着可扩展的真实世界网页交互与有根语言代理迈进。*神经信息处理系统进展*，35:20744–20757。'
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate
    problem solving with large language models. *arXiv preprint arXiv:2305.10601*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas
    L Griffiths, Yuan Cao, 和 Karthik Narasimhan. 2023. 思维之树: 使用大型语言模型进行深思熟虑的问题解决。*arXiv
    预印本 arXiv:2305.10601*。'
- en: 'Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022b. React: 协同推理与行动在语言模型中的应用。*arXiv 预印本 arXiv:2210.03629*。'
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, 和 Jie Tang. 2023. Agenttuning: 赋能大型语言模型的通用代理能力。*arXiv 预印本 arXiv:2310.12823*。'
- en: 'Zhang et al. (2023a) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei
    Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023a. Instruction
    tuning for large language models: A survey. *arXiv preprint arXiv:2308.10792*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei
    Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, 等. 2023a. 大型语言模型的指令调整:
    一项调查。*arXiv 预印本 arXiv:2308.10792*。'
- en: 'Zhang et al. (2023b) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023b. Siren’s song
    in the ai ocean: A survey on hallucination in large language models. *arXiv preprint
    arXiv:2309.01219*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023b) Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen
    Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, 等. 2023b. 人工智能海洋中的海妖之歌: 大型语言模型中的幻觉调查。*arXiv
    预印本 arXiv:2309.01219*。'
- en: Zhang et al. (2023c) Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis,
    and Alex Smola. 2023c. Multimodal chain-of-thought reasoning in language models.
    *arXiv preprint arXiv:2302.00923*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023c) Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis,
    和 Alex Smola. 2023c. 语言模型中的多模态链式推理。*arXiv 预印本 arXiv:2302.00923*。
- en: Zhou et al. (2023) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. 2023. Language agent tree search unifies reasoning acting and
    planning in language models. *arXiv preprint arXiv:2310.04406*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    和 Yu-Xiong Wang. 2023. 语言代理树搜索统一了语言模型中的推理、行动和规划。*arXiv 预印本 arXiv:2310.04406*。
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. 2023. Ghost in
    the minecraft: Generally capable agents for open-world enviroments via large language
    models with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, 等. 2023. Minecraft中的幽灵:
    通过具有文本基础知识和记忆的大型语言模型用于开放世界环境的一般性能力代理。*arXiv 预印本 arXiv:2305.17144*。'
