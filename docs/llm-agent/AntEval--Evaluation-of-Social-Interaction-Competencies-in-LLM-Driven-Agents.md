<!--yml
category: 未分类
date: 2025-01-11 12:57:48
-->

# AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents

> 来源：[https://arxiv.org/html/2401.06509/](https://arxiv.org/html/2401.06509/)

Yuanzhi Liang
University of Technology Sydney
liangyzh18@outlook.com
&Linchao Zhu
Zhejiang University
zhulinchao7@gmail.com
&Yi Yang
Zhejiang University
yangyics@zju.edu.cn 

###### Abstract

Large Language Models (LLMs) have demonstrated their ability to replicate human behaviors across a wide range of scenarios. However, their capability in handling complex, multi-character social interactions has yet to be fully explored, primarily due to the absence of robust, quantitative evaluation methods. This gap has slowed the development of agents proficient in more nuanced interactions beyond simple exchanges, for example, small talk. To address this challenge, we introduce the Multi-Agent Interaction Evaluation Framework (AntEval), encompassing a novel interaction framework and evaluation methods. The interaction framework aims to foster an complex interaction environment that bolsters information exchange and intention expression within social interactions. Furthermore, we introduce evaluation methods, including two metrics: Information Exchanging Precision (IEP) and Interaction Expressiveness Gap (IEG), designed for the quantitative and objective assessment of agents’ interaction competencies. Our findings highlight the utility of these evaluative methods and show significant potential for improving LLMs’ ability to construct agents that interact in a more natural manner with human-like intricacy.

## 1 Introduction

Advancements in Large Language Models (LLMs) have significantly impacted Artificial Intelligence (AI) research and applications, showcasing remarkable proficiency in understanding Chen et al. ([2017](https://arxiv.org/html/2401.06509v3#bib.bib9)); Hendrycks et al. ([2020](https://arxiv.org/html/2401.06509v3#bib.bib20)) and reasoning Zellers et al. ([2019](https://arxiv.org/html/2401.06509v3#bib.bib48)); Huang and Chang ([2022](https://arxiv.org/html/2401.06509v3#bib.bib21)). LLMs have been deployed to construct agents that excel in various domains such as translation Fan et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib16)); Yang et al. ([2020](https://arxiv.org/html/2401.06509v3#bib.bib46)), question answering Zhu et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib52)); Lehnert ([2022](https://arxiv.org/html/2401.06509v3#bib.bib24)), and more specialized tasks like SAT solving Ye et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib47)) and law examinations Bommarito II and Katz ([2022](https://arxiv.org/html/2401.06509v3#bib.bib7)).

Distinct from logic and reasoning abilities, the capacity for social interaction emerges as equally vital. Sociologically and anthropologically, human beings are inherently social, with interactions forming the backbone of societal structures and personal development. Studies Batson ([1990](https://arxiv.org/html/2401.06509v3#bib.bib5)); Dijksterhuis ([2005](https://arxiv.org/html/2401.06509v3#bib.bib14)); Goody ([1995](https://arxiv.org/html/2401.06509v3#bib.bib17)); Sterelny ([2007](https://arxiv.org/html/2401.06509v3#bib.bib40)); Lopes et al. ([2004](https://arxiv.org/html/2401.06509v3#bib.bib31)) have linked intelligence to social interactions, including dialogues, actions, emotion, etc., highlighting the importance of assessing agents’ social interaction competencies. Recent efforts Liu et al. ([2023b](https://arxiv.org/html/2401.06509v3#bib.bib30)); Park et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib35)); Chen et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib11)) have begun to investigate LLM-driven agents in social interactions. However, the rapid advancement of LLMs and agents might outpace the progress in developing evaluation methodologies. Existing evaluation metrics primarily focus on domain-specific knowledge and cognitive skills Hendrycks et al. ([2020](https://arxiv.org/html/2401.06509v3#bib.bib20)); Zellers et al. ([2019](https://arxiv.org/html/2401.06509v3#bib.bib48)), which are hard to assess flexible and diverse social interactions. While subjective evaluations exist Park et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib35)); Liu et al. ([2023b](https://arxiv.org/html/2401.06509v3#bib.bib30)), the field still lacks comprehensive, objective, and quantitative evaluation methods, highlighting a significant area for further investigation.

The challenge of designing an evaluation method for interactions lies in their inherent complexity and variability. Defining, collecting, and annotating human interactions for analysis are difficult, as everyday interactions often lack clear intentions or detailed information Wang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib42)); Rawte et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib36)), such as in small talk Lipenkova ([2023](https://arxiv.org/html/2401.06509v3#bib.bib28)); Ahmed et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib1)). Moreover, interactions rich in nuance, like business negotiations Heikkinen et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib19)); Beauregard ([2020](https://arxiv.org/html/2401.06509v3#bib.bib6)), are seldom documented due to privacy issues. The limited availability of complex scenarios for agent interactions presents a significant challenge, making it difficult for LLM-driven agents to engage in sophisticated interactions. Furthermore, the absence of comprehensive evaluation benchmarks critically hampers the agents’ ability to strive for more informative and expressive interactions. This dual-level deficiency highlights an urgent need for both diverse interaction environments and objective, quantitative evaluation methods to improve the competencies of agent interaction.

![Refer to caption](img/3b41c54e7b83cd9157b3d3c3b2ed8ae6.png)

Figure 1: Real human interactions are marked by their efficient exchange of information and the clarity of their intentions, showcasing both complexity and depth. In contrast, LLM-driven agents’ interactions Chen et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib11)); Park et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib35)), as depicted in results (b), typically exhibit a lack of substantial content, resembling mere superficial interactions. AntEval framework encourages agents to partake in interactions that are both intricate and significant. Importantly, AntEval further introduces evaluation methods, specifically crafted to quantitatively evaluate the interactions based on informativeness and expressiveness. Our framework aims to provide an evaluation framework, guiding the enhancement of LLMs’ abilities close to genuine human interaction.

In this paper, we introduce Multi-Agent Interaction Evaluation Framework (AntEval), a novel evaluation framework specifically designed for assessing multi-agent interactions. First, to provide a complex interaction environment and prevent the agents from engaging in meaningless small talk, AntEval incorporates Tabletop Role-Playing Games (TRPG) Gygax and Arneson ([1974](https://arxiv.org/html/2401.06509v3#bib.bib18)) as a platform for interaction generation. TRPGs offer a richly narrative environment, replete with characters endowed with diverse settings (e.g., personalities, ideals, bonds, etc.). These diverse settings lay the base for complex interactions, circumventing the privacy concerns tied to collecting real-world data. In the games, players are required to engage effectively with one another to exchange valuable information and articulate their intentions with clarity and vividness for cooperative purposes. Moreover, the game’s mechanics provide the standardization and explicit expression of player intentions within the narrative framework. A key aspect of TRPGs is the Dungeon Master (DM) Gygax and Arneson ([1974](https://arxiv.org/html/2401.06509v3#bib.bib18)), who oversees gameplay and implements necessary skill checks. This, coupled with the game’s special rules, ensures detailed and accurate records of players’ intentions in the game logs. This distinct characteristic of TRPGs offers a valuable opportunity to analyze and evaluate the complexity and depth of interactions in ways that were previously inaccessible Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)).

Leveraging the settings of TRPG, AntEval introduces an interaction framework that encourages agents to interact informatively and expressively. Specifically, we create a variety of characters with detailed settings based on TRPG rules. Agents are then prompted to interact in two distinct scenarios: information exchange and intention expression. To quantitatively assess the quality of these interactions, AntEval introduces two evaluation metrics: informativeness in information exchange and expressiveness in intention. For information exchange, we propose the Information Exchange Precision (IEP) metric, assessing the accuracy of information communication and reflecting the agents’ capability for informative interactions. For intention expression, we introduce the Intention Expressiveness Gap (IEG). In this metric, we incorporate virtual DMs, fine-tuned by both real interactions from human players and virtual interactions generated by agents, to evaluate performance disparities in the intention estimation task Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)). If agent-generated interactions closely mirror human expressiveness, a virtual DM trained on such data should close to human capability in distinguish intentions. Consequently, the IEG metric offers a quantifiable measure to identify the expressiveness gap between synthetic and real interactions, serving as a reliable gauge for LLMs’ effectiveness in creating authentic social exchanges.

Our contributions are outlined as follows:

1\. We introduce AntEval, a novel framework tailored for the evaluation of interaction capabilities in LLM-driven agents. This framework introduces an interaction framework and evaluation methods, enabling the quantitative and objective assessment of interaction abilities within complex scenarios.

2\. AntEval presents two key metrics: Information Exchange Precision (IEP) and Intention Expressiveness Gap (IEG). These metrics facilitate the quantitative evaluation of informativeness and expressiveness within multi-agent interactions, specifically designed for scenarios of information exchange and intention expression within AntEval’s interaction framework.

3\. We implemented the AntEval framework to conduct thorough experiments across various LLMs. Our research yields several important insights: a). Social Interaction as a Distinct Challenge: Beyond logic and reasoning, the ability to navigate social interactions poses a unique challenge for LLMs. They must generate grounded language for complex interactions, striving for a level of informativeness and expressiveness that mirrors human interaction. While natural for humans, even advanced models like GPT-4 find this difficult, indicating a need for further research. b). The Importance of Alignment: Alignment is a critical issue for all LLMs, especially open-source models, which show significant potential for improvement. Furthermore, the capability to anthropomorphically operate interactions without hallucinations remains an area ripe for exploration. c). Complexities of Long-Context Interactions: Understanding and maintaining coherence in long-context interactions remains a hurdle. While LLMs can handle individual turns effectively, the cumulative quality over several turns often lacks the informativeness and expressiveness characteristic of human dialogue. Developing methods to retain valuable content and maintain the natural flexibility observed in human interactions is a challenging problem.

## 2 Related Work

Evaluation of LLMs. The advancements in Large Language Models (LLMs) have eclipsed the scope of traditional benchmarks, necessitating a refined evaluation methodology that encompasses a broader spectrum of tasks, including language Zellers et al. ([2019](https://arxiv.org/html/2401.06509v3#bib.bib48)); Huang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib22)); Hendrycks et al. ([2020](https://arxiv.org/html/2401.06509v3#bib.bib20)), multi-modal Lu et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib33)), etc. Existing benchmarks primarily bifurcate into: 1) multifaceted evaluations such as MMLU Hendrycks et al. ([2020](https://arxiv.org/html/2401.06509v3#bib.bib20)) with its 57 diverse tasks, Big-bench Srivastava et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib39)) offering over 200 tasks for a comprehensive LLM assessment, and HELM Liang et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib26)) which proposes a structured taxonomy for thorough evaluations; 2) real-world application assessments, including standardized tests for human-like understanding Zhong et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib49)); Clark et al. ([2018](https://arxiv.org/html/2401.06509v3#bib.bib12)); Sakaguchi et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib37)); Zellers et al. ([2019](https://arxiv.org/html/2401.06509v3#bib.bib48)), programming capabilities Chen et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib10)); Babe et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib4)); Austin et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib3)), mathematical problem-solving Cobbe et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib13)); Imani et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib23)), and practical interactions Xu et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib45)); Li et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib25)); Valmeekam et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib41)); Ahn et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib2)). Our work shifts focus to quantitatively evaluating social interactions, a domain where humans excel but LLM-based agents struggle.

TRPG in LLM Research. Tabletop Role-Playing Games (TRPGs) offer an intricate setting for Natural Language Processing (NLP) research, featuring players navigating fictional universes guided by a Game Master, enabling complex, natural language interactions Weir et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib43)); Louis and Sutton ([2018](https://arxiv.org/html/2401.06509v3#bib.bib32)); Callison-Burch et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib8)). Investigations into TRPG data have addressed NLP tasks like predicting actions Louis and Sutton ([2018](https://arxiv.org/html/2401.06509v3#bib.bib32)) and generating context-aware dialogues Si et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib38)); Newman and Liu ([2022](https://arxiv.org/html/2401.06509v3#bib.bib34)). Additionally, recent studies have utilized data from Dungeons & Dragons (DND) forums, compiling comprehensive datasets Callison-Burch et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib8)); Zhou et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib50)); Zhu et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib51)) for generating in-game guidance and commands.

![Refer to caption](img/8c3f6a0009949733acda975676affb5a.png)

Figure 2: Framework illustration for AntEval, showcasing the use of TRPG rules to create an interactive environment for agents. Agents engage in role-playing, aiming to participate in high-quality interactions for information exchange and intention expression, with the goal of completing game adventures. The framework involves detailed and diverse character settings based on the DND rulebook. Agents are involved in two types of scenarios: interacting based on intentions and exchanging knowledge, highlighting their capabilities in informative and expressive interactions.

## 3 Agent Interaction Framework

To evaluate the social interaction capabilities of LLM-based agents, our methodology leverages TRPG settings, focusing on: (1) creating complex character settings to mirror real-world interactions, with detailed character descriptions for sophisticated interactions; and (2) establishing an interaction environment where information that needs to be exchanged and intentions that need to be expressed are clearly defined. This addresses data annotation and privacy challenges in complex interaction data collection (e.g., business negotiation), facilitating nuanced interactions.

As shown in Fig. [2](https://arxiv.org/html/2401.06509v3#S2.F2 "Figure 2 ‣ 2 Related Work ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), the implementation of our framework is divided into two main components: character generation and agent interaction generation. In the first phase, character generation, we focus on creating detailed character profiles that include both the settings and descriptions of each character. Following this, LLMs are given these character descriptions and are tasked with role-playing as player agents within the game. Subsequently, we introduce multiple agents to facilitate interactions. All detailed settings are given in the supplementary LABEL:settings.

### 3.1 Character Generation

To construct characters with detailed attributes, we integrate elements from TRPG, concentrating on essential traits such as name, race, and background. Attributes like race, background, etc., are predefined in DND rules, as in Fig. [2](https://arxiv.org/html/2401.06509v3#S2.F2 "Figure 2 ‣ 2 Related Work ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), and are randomly assigned to each character. We then utilize the GPT-4 model to generate names and detailed descriptions that reflect these chosen attributes, in line with the game’s rulebook. With these descriptions, LLMs are prompted to role-play within the game, acting as game agents Liu et al. ([2023a](https://arxiv.org/html/2401.06509v3#bib.bib29)); Park et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib35)); Chen et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib11)).

### 3.2 Agent Interaction Generation

We introduce two scenarios, information exchange and intention expression, to evaluate agent interactions focused on informativeness and expressiveness. These scenarios involve $T$ player agents and one non-player character (NPC), each with distinct settings. Agents interact based on their unique character descriptions, incorporating either the assigned knowledge or intentions.

Information Exchange: An NPC is endowed with exclusive knowledge (magic items, weapons, etc.) generated by the GPT-4 model. This setup requires player agents to discover this knowledge through interaction. Their success is measured against the NPC’s undisclosed information after $N$ turns. For example in Fig. [3](https://arxiv.org/html/2401.06509v3#S3.F3 "Figure 3 ‣ 3.2 Agent Interaction Generation ‣ 3 Agent Interaction Framework ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), agent Keyleth knows about the maze, unknown to player agents Orisik, Adrie, and Valna. These agents must effectively communicate to uncover this hidden knowledge and progress in their adventure.

![Refer to caption](img/43a2b687ca23e3120858964386e46609.png)

Figure 3: Our AntEval evaluates informativeness and expressiveness through specific scenarios: information exchange and intention expression. We fine-tune virtual DMs with agent-generated and real interactions to assess expressiveness, and gauge informativeness by comparing agents’ responses to the predefined knowledge.

Intention Expression: Mirroring DND’s skill check system, we assign skill checks to characters as representations of their intentions. These pre-determined intentions are integrated into character descriptions, guiding agents to express these intentions during interactions. This scenario encourages agents with predefined intentions engaging in role-play over $N$ turns, aiming to convey their intentions through actions and dialogue that align with their character settings. As in Fig. [3](https://arxiv.org/html/2401.06509v3#S3.F3 "Figure 3 ‣ 3.2 Agent Interaction Generation ‣ 3 Agent Interaction Framework ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), upon entering a dungeon room, each agent, guided by specific intentions, interacts based on their character’s goals. For instance, Valna searches the room for clues to navigate the maze, while Keyleth, lacking a specific goal, passively observes, demonstrating the diverse interaction dynamics driven by their intentions.

## 4 Evaluation of Multi-Agent Interactions

Our evaluation framework assesses LLM-based multi-agent interactions through two targeted scenarios: information exchange and intention expression, to examine informativeness and expressiveness, respectively.

### 4.1 Informativeness Evaluation

To move beyond superficial exchanges and assess the efficiency of information exchanging, we introduce the Information Exchange Precision (IEP) metric. This evaluates how effectively agents share and gather information that is pivotal to advancing the quality of interactions. The process starts by querying player agents about the information they have collected from their interactions. We then summarize these responses using GPT-4 into a set of $k$ key points. Similarly, the external knowledge pre-loaded into the NPC agent are also summarized into $k$ key points. Then, we further prompt GPT-4 model to compare two sets of key points and identify the number of overlaps ($s$) to quantify the effectiveness of the information exchange. Examples for summarized key points are present in Supplementary LABEL:supp_IEP_case. IEP for each agent is calculated by $p={s}/{k}$, providing a direct measure of the informativeness. This evaluation is repeated across $H$ times, each with unique character setups and knowledge. The final score of IEP $P$ is the average of precision calculated as $P=\sum_{i}^{H\times T}p_{i}$.

### 4.2 Expressiveness Evaluation

The unique mechanism of TRPGs allows character intentions to be annotated through skill checks during games, as recorded in real game logs. In our work, expressiveness in agent interactions is gauged through the lens of intention understanding, employing a virtual DM Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)) to estimate the intentions. Specifically, we utilize two virtual DM agents, $D_{r}$ and $D_{g}$, fine-tuned on real interactions and agent-generated interactions, respectively. Moreover, the intention prediction is a tuple (character name, skill name), as in Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)). We assess the F-score $f^{c}$ of predicted character names, indicating who is acting, and the F-score $f^{o}$ of the overall tuple, indicating both who and how they act. The final expressiveness gap, $G$, is calculated as following equations:

|  | $\displaystyle G^{c}=\left&#124;\frac{f_{r}^{c}-f_{g}^{c}}{f_{r}^{c}+f_{g}^{c}}% \right&#124;,\quad G^{o}=\left&#124;\frac{f_{r}^{o}-f_{g}^{o}}{f_{r}^{o}+f_{g}^{o}}\right&#124;$ |  | (1) |

where $f_{r}$ and $f_{g}$ indicates F-scores achieved by $D_{r}$ and $D_{g}$, respectively. This gap measures the ability discrepancy in understanding intentions between agents and humans. A smaller gap indicates agent-generated interactions closely resemble the complexity and expressiveness of human interactions.

To ensure a fair comparison and isolate the impact of the finetuning model, we exclusively fine-tune the GPT-3.5 model with interactions generated by different LLMs. This standardizes the virtual DM’s capability, focusing our evaluation on the quality of the interactions rather than the model’s intrinsic understanding capacity.

Additionally, relying on a single virtual DM to evaluate both real and generated interactions might not effectively gauge the quality of these interactions. This is because generated interactions could be overly simplistic, with agents directly stating their intentions. In such cases, the virtual DM might easily interpret these low-quality interactions, yet struggle to understand the more complex and nuanced interactions typical of real human players. Moreover, there is a possibility that generated interactions could veer towards trivial small talk, lacking in intention expressiveness. These less informative and unproductive interactions would likely diminish the virtual DM’s performance. Therefore, directly comparing the performance gap between generated and real data may not yield a valuable assessment.

Moreover, we fine-tune the LLMs separately with generated and real data. We then evaluate the performance gap using only real data. By focusing the evaluation on real data, we ensure a more robust and realistic assessment of how well the generated interactions approximate the complexity of actual human interactions.

## 5 Experiment

### 5.1 Implementation Details

Our experimental setup for AntEval, drawing upon the framework established by Chen et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib11)), orchestrates multi-agent interactions where each agent, following a sequential order, has the option to perform an action, communicate verbally, or do both. The agents can also choose to pass their current turn without interaction. Aligning with most game logs in the DND games, our sessions include four player agents ($T=3$) and one NPC agent.

To assess informativeness, we design prompts that allow each player agent to respond to questions after $30$ interaction turns ($M=30$ and $N=30$), aligning with the average turn count observed in the dataset by Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)). For equitable analysis, GPT-4 is deployed to summarize all agents’ responses and pre-defined knowledge.

For intention expressiveness, we fine-tune GPT-3.5 to act as the virtual DM, with either real or generated interactions. The real interactions derive from Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)), while the generated interactions are produced by diverse LLM-based agents under uniform settings, as outlined in Sec. [3.1](https://arxiv.org/html/2401.06509v3#S3.SS1 "3.1 Character Generation ‣ 3 Agent Interaction Framework ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents") and generated by GPT-4\. It should be noted that the only variable in our experiment is the generated interactions used to train different virtual DMs, ensuring a fair comparison by maintaining consistency across all other variables, such as character settings, prompts, the virtual DM model, etc. For model training, real player interactions and generated interactions are uploaded to the OpenAI website for fine-tuning GPT models.

Moreover, for IEG evaluation, we generate agent interactions by different LLMs across $600$ different sessions, each consisting of $30$ turns, to reduce biases from size differences between generated data and real data. More details and case studies are presented in the supplementary.

 | Model | IEP | IEG |
| $P(\uparrow)$ | $f_{g}^{c}(\uparrow)$ | $G^{c}(\downarrow)$ | $f_{g}^{o}(\uparrow)$ | $G^{o}(\downarrow)$ |
| Alpaca-13b | 0.63 | 4.96 | 77.80 | 0.08 | 99.48 |
| ChatGLM2-6b | 0.70 | 5.16 | 77.01 | 0.02 | 99.87 |
| Vicuna-7b | 1.67 | 12.41 | 52.40 | 0.49 | 96.85 |
| Vicuna-13b | 3.92 | 15.59 | 43.64 | 1.52 | 90.54 |
| LLaMA-2-7b | 1.60 | 13.51 | 49.25 | 0.90 | 94.29 |
| LLaMA-2-13b | 3.83 | 17.74 | 38.26 | 3.59 | 79.01 |
| LLaMA-2-70b | 9.64 | 22.10 | 28.51 | 4.12 | 76.27 |
| Mistral-7B | 3.09 | 16.21 | 42.04 | 2.93 | 82.52 |
| Mixtral-8x7B | 8.68 | 20.72 | 31.45 | 3.71 | 78.38 |
| GPT-3.5 | 55.93 | 29.28 | 15.14 | 10.12 | 50.31 |
| GPT-4 | 60.40 | 33.92 | 12.94 | 20.22 | 24.44 | 

Table 1: Comparison for IEP and IEG between different LLMs.

### 5.2 Informativeness Evaluation

 | Interaction Agent | Virtual DM | Character Prediction | Overall Prediction |
| --- | --- | --- | --- |
| $f_{0}^{c}(\uparrow)$ | $f_{g}^{c}(\uparrow)$ | $f_{r}^{c}(\uparrow)$ | $G^{c}(\downarrow)$ | $f_{0}^{o}(\uparrow)$ | $f_{g}^{o}(\uparrow)$ | $f_{r}^{o}(\uparrow)$ | $G^{o}(\downarrow)$ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| GPT-3.5 | GPT-3.5 | 26.48 | 29.28 | 39.73 | 15.14 | 8.32 | 10.12 | 30.61 | 50.31 |
| GPT-3.5-lc | 29.89 | 33.92 | 44.01 | 12.94 | 12.22 | 20.33 | 33.48 | 24.44 |
| GPT-3.5-lc | GPT-3.5 | 26.48 | 29.97 | 39.73 | 14.00 | 8.32 | 12.16 | 30.61 | 43.14 |
| GPT-3.5-lc | 29.89 | 35.41 | 44.01 | 10.83 | 12.22 | 23.51 | 33.48 | 17.49 |
| GPT-4 | GPT-3.5 | 26.48 | 32.65 | 39.73 | 9.78 | 8.32 | 14.89 | 30.61 | 34.55 |
| GPT-3.5-lc | 29.89 | 37.27 | 44.01 | 8.29 | 12.22 | 27.09 | 33.48 | 10.55 | 

Table 2: Comparison for different virtual DM models. The metric $f_{0}$ reflects results obtained using LLMs without fine-tuning. GPT-3.5-lc represents the recently released long-context version.

In our comparative analysis presented in Tab. [1](https://arxiv.org/html/2401.06509v3#S5.T1 "Table 1 ‣ 5.1 Implementation Details ‣ 5 Experiment ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), we identify a significant discrepancy between GPT models and open-source models. Notably, LLaMA-2-70b, the highest-performing open-source model, exhibits a substantial gap $46.29\%$ in IEP. The results reveal two critical shortcomings in open-source models: 1) Significant hallucination issues, characterized by repetitive inputs and irrelevant content generation, or failure to adhere to specific interaction formats, a problem more prevalent in smaller models; 2) A propensity to overlook essential character settings and DND rules, despite demonstrating the ability to correctly answer DND rule-related questions. This suggests that while the models possess the requisite knowledge, they struggle to effectively apply it in practice.

Furthermore, although GPT models significantly outperform their open-source counterparts, their performance remains considerably below expectations, especially when compared to real human interactions. In real settings, humans effortlessly engage in information exchange with a level of flexibility and spontaneity that current LLMs fail to replicate. This gap underscores a fundamental limitation in LLMs, manifesting as a lack of genuine informativeness in interactions generated by GPT models, which often tend to result in ‘safe’ and trivial interactions. While this issue may not be as pronounced in short-context interactions Hendrycks et al. ([2020](https://arxiv.org/html/2401.06509v3#bib.bib20)); Liang et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib26)); Zellers et al. ([2019](https://arxiv.org/html/2401.06509v3#bib.bib48)) or standard interaction environments Park et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib35)); Chen et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib11)), it becomes evident in complex scenarios rich with information. For instance, GPT-4 achieves only a $60.40\%$ accuracy rate in information exchange, a task that typically poses little challenge to human participants.

### 5.3 Expressiveness Evaluation

In expressiveness evaluation, we fine-tune LLMs using both real and generated interaction data. These models then construct virtual DMs and engage in the intention estimation task as in Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)). As shown in Tab [1](https://arxiv.org/html/2401.06509v3#S5.T1 "Table 1 ‣ 5.1 Implementation Details ‣ 5 Experiment ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), we observe significant gaps $G$ in all settings, with values exceeding about $12\%$. These high values of IEG indicate a significant difference between generated and real interactions, suggesting that real data provide more substantial insights than generated interactions. The F-score results offer additional detail, supporting this conclusion. Notably, models fine-tuned with real data consistently outperform those tuned with generated data. For example, GPT-3.5, when fine-tuned with real interactions, exceeds the performance of those fine-tuned with generated data by $10.45\%$ and $20.49\%$ in character and skill checks, respectively. This disparity underscores the importance of high-quality interactions for effective intention estimation. LLMs appear to learn more meaningful information from real data than from generated interactions.

To establish a baseline for comparison, we introduce an additional evaluation metric, $f_{0}$, which gauges the performance of LLMs in constructing virtual agents without any fine-tuning, as detailed in Liang et al. ([2023](https://arxiv.org/html/2401.06509v3#bib.bib27)). As illustrated in Tab [2](https://arxiv.org/html/2401.06509v3#S5.T2 "Table 2 ‣ 5.2 Informativeness Evaluation ‣ 5 Experiment ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents"), $f_{0}$ shows lower performance scores than both $f_{v}$ and $f_{r}$, underscoring the benefits of fine-tuning with interaction data. Notably, the analysis reveals that learning from real human interactions is significantly more beneficial than relying solely on agent-generated data.

Furthermore, our evaluation extends to comparing virtual DM models, specifically GPT-3.5 and its long-context variant, denoted as GPT-3.5-lc. This comparison demonstrates that models with enhanced long-context capabilities, such as GPT-3.5-lc, consistently surpass their standard counterparts in performance. Nonetheless, due to API cost considerations, GPT-3.5 is selected as the fine-tuning model for virtual DMs, balancing performance with efficiency.

### 5.4 Case Study for Generated Interactions

In our examination of the IEP evaluation’s failure cases, we sought to identify the factors limiting LLM performance. Given the pronounced disparity between open-source models and GPT models, with some failing to produce coherent responses consistently, our analysis focused on the GPT-4 model, the most advanced model available. The shortcomings of GPT-4 can provide valuable insights for steering future research directions. We selected interactions where agents scored below $50\%$ in IEP following the interaction and randomly chose $20$ for further analysis. Five volunteers were then asked to evaluate these interactions, summarizing their feelings in a single word. These $100$ words of responses were aggregated and summarized to create a word cloud as shown in Fig. [4](https://arxiv.org/html/2401.06509v3#S5.F4 "Figure 4 ‣ 5.4 Case Study for Generated Interactions ‣ 5 Experiment ‣ AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents").

![Refer to caption](img/e6faaefcd1f4ece815307fd1bdf820cb.png)

Figure 4: Word cloud representing common descriptors for interactions by GPT-4 that underperformed in IEP evaluation.

This analysis revealed ‘boring’ as the predominant feedback, indicating that the interactions generated were often deemed uninformative and lacking the vividness expected by human participants. Detailed cases are provided in the supplementary LABEL:case_study.

### 5.5 Insights into Improving LLMs

Our exploration through AntEval has unveiled insights that current LLM research has overlooked, offering directions for future work aimed at refining LLMs’ performance in real-human contexts. These insights are summarized as follows:

1\. Interaction capabilities, beyond logic and reasoning, need further investigation in LLM research. AntEval demonstrates that interactions do not always hinge on complex mathematical reasoning or logical puzzles but rather on generating grounded language and actions for engaging with others. Notably, many young children can navigate social interactions or excel in environments like DND games without formal mathematical or logical training. This observation underscores a pronounced disparity between LLMs and human interaction abilities, highlighting the challenge of enabling LLMs to respond with human-like spontaneity as an open and enduring research question, beyond the scope of training by pre-defined datasets or learning to program.

2\. Alignment remains a pivotal concern across all LLMs, with substantial room for improvement. Despite their superior performance in AntEval, GPT models often generate overly cautious and polite interactions, prioritizing safety over informativeness and expressiveness, leading to unproductive small talk. Conversely, models like LLaMA and Mixtral, including their largest variants, tend to replicate content or exhibit hallucinations, impairing interaction quality. Furthermore, smaller models frequently struggle to adhere to instructions or generate responses in a specific format, let alone hallucination issues. Addressing alignment to foster more human-like performance across all LLMs presents a formidable challenge.

3\. The understanding of long-context scenarios by LLMs presents several daunting challenges. Accurately understanding intentions embedded within nuanced and flexible language expressions and summarizing characters’ emotions, positions, etc., remains a complex task. Beyond summarizing content such as articles El-Kassas et al. ([2021](https://arxiv.org/html/2401.06509v3#bib.bib15)); Widyassari et al. ([2022](https://arxiv.org/html/2401.06509v3#bib.bib44)), grappling with the intricacies of long-context interactions characterized by their flexibility and complexity poses significant hurdles. Not all real human interactions carry consequential meanings or necessitate that need to be summarized and recalled. Yet, some meaningless and trivial interactions may be expressive, conveying individual opinions, stances, or personalities. The essence of human interaction lies in its adaptability and groundedness, presenting substantial difficulties in developing specific methodologies for processing, understanding, and generation.

## 6 Conclusion

We present AntEval, a framework for evaluating agent interactions, introducing the evaluation benchmark, and Information Exchange Precision (IEP) and Intention Expression Granularity (IEG) to measure informativeness and expressiveness. These metrics are designed to assess the informativeness and expressiveness of agent interactions. AntEval navigates the intricacies of interaction complexity and privacy concerns, showcasing its efficacy in steering AI agents towards interactions that closely mirror human social behavior. By using these evaluation metrics, AntEval provides new insights into LLMs’ social interaction capabilities and establishes a refined benchmark for the development of better AI systems.

## References

*   Ahmed et al. (2023) Imtiaz Ahmed, Ayon Roy, Mashrafi Kajol, Uzma Hasan, Partha Protim Datta, and Md Rokonuzzaman Reza. 2023. Chatgpt vs. bard: a comparative study. *Authorea Preprints*.
*   Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic affordances. *arXiv preprint arXiv:2204.01691*.
*   Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*.
*   Babe et al. (2023) Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, and Carolyn Jane Anderson. 2023. Studenteval: A benchmark of student-written prompts for large language models of code. *arXiv preprint arXiv:2306.04556*.
*   Batson (1990) C Daniel Batson. 1990. How social an animal? the human capacity for caring. *American psychologist*, 45(3):336.
*   Beauregard (2020) Robert A Beauregard. 2020. From place to site: Negotiating narrative complexity. In *Site matters*, pages 226–238\. Routledge.
*   Bommarito II and Katz (2022) Michael Bommarito II and Daniel Martin Katz. 2022. Gpt takes the bar exam. *arXiv preprint arXiv:2212.14402*.
*   Callison-Burch et al. (2022) Chris Callison-Burch, Gaurav Singh Tomar, Lara Martin, Daphne Ippolito, Suma Bailis, and David Reitter. 2022. [Dungeons and dragons as a dialog challenge for artificial intelligence](https://aclanthology.org/2022.emnlp-main.637). In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 9379–9393, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
*   Chen et al. (2017) Lang Chen, Matthew A Lambon Ralph, and Timothy T Rogers. 2017. A unified model of human semantic knowledge and its disorders. *Nature human behaviour*, 1(3):0039.
*   Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.
*   Chen et al. (2023) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. *arXiv preprint arXiv:2308.10848*.
*   Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
*   Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*.
*   Dijksterhuis (2005) Ap Dijksterhuis. 2005. Why we are social animals: The high road to imitation as social glue. *Perspectives on imitation: From neuroscience to social science*, 2:207–220.
*   El-Kassas et al. (2021) Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey. *Expert systems with applications*, 165:113679.
*   Fan et al. (2021) Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation. *Journal of Machine Learning Research*, 22(107):1–48.
*   Goody (1995) Esther N Goody. 1995. *Social intelligence and interaction: Expressions and implications of the social bias in human intelligence*. Cambridge University Press.
*   Gygax and Arneson (1974) Gary Gygax and Dave Arneson. 1974. *dungeons & dragons*, volume 19. Tactical Studies Rules Lake Geneva, WI.
*   Heikkinen et al. (2021) Suvi Heikkinen, Anna-Maija Lämsä, and Charlotta Niemistö. 2021. Work–family practices and complexity of their usage: a discourse analysis towards socially responsible human resource management. *Journal of Business Ethics*, 171:815–831.
*   Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. *arXiv preprint arXiv:2009.03300*.
*   Huang and Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. *arXiv preprint arXiv:2212.10403*.
*   Huang et al. (2023) Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. *arXiv preprint arXiv:2305.08322*.
*   Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. 2023. Mathprompter: Mathematical reasoning using large language models. *arXiv preprint arXiv:2303.05398*.
*   Lehnert (2022) Wendy G Lehnert. 2022. *The process of question answering: A computer simulation of cognition*. Taylor & Francis.
*   Li et al. (2023) Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: A benchmark for tool-augmented llms. *arXiv preprint arXiv:2304.08244*.
*   Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
*   Liang et al. (2023) Yuanzhi Liang, Linchao Zhu, and Yi Yang. 2023. Tachikuma: Understading complex interactions with multi-character and novel objects by large language models. *arXiv preprint arXiv:2307.12573*.
*   Lipenkova (2023) Janna Lipenkova. 2023. Overcoming the limitations of large language models how to enhance llms with human-like cognitive skills.
*   Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. *ACM Computing Surveys*, 55(9):1–35.
*   Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b. Agentbench: Evaluating llms as agents. *arXiv preprint arXiv:2308.03688*.
*   Lopes et al. (2004) Paulo N Lopes, Marc A Brackett, John B Nezlek, Astrid Schütz, Ina Sellin, and Peter Salovey. 2004. Emotional intelligence and social interaction. *Personality and social psychology bulletin*, 30(8):1018–1034.
*   Louis and Sutton (2018) Annie Louis and Charles Sutton. 2018. Deep dungeons and dragons: Learning character-action interactions from role-playing game transcripts. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, pages 708–713.
*   Lu et al. (2022) Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. Unified-io: A unified model for vision, language, and multi-modal tasks. *arXiv preprint arXiv:2206.08916*.
*   Newman and Liu (2022) Pax Newman and Yudong Liu. 2022. [Generating descriptive and rules-adhering spells for dungeons & dragons fifth edition](https://aclanthology.org/2022.games-1.7). In *Proceedings of the 9th Workshop on Games and Natural Language Processing within the 13th Language Resources and Evaluation Conference*, pages 54–60, Marseille, France. European Language Resources Association.
*   Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. *arXiv preprint arXiv:2304.03442*.
*   Rawte et al. (2023) Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. *arXiv preprint arXiv:2309.05922*.
*   Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. *Communications of the ACM*, 64(9):99–106.
*   Si et al. (2021) Wai Man Si, Prithviraj Ammanabrolu, and Mark Riedl. 2021. [Telling stories through multi-user dialogue by modeling character relations](https://aclanthology.org/2021.sigdial-1.30). In *Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue*, pages 269–275, Singapore and Online. Association for Computational Linguistics.
*   Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.
*   Sterelny (2007) Kim Sterelny. 2007. Social intelligence, human intelligence and niche construction. *Philosophical Transactions of the Royal Society B: Biological Sciences*, 362(1480):719–730.
*   Valmeekam et al. (2022) Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large language models still can’t plan (a benchmark for llms on planning and reasoning about change). *arXiv preprint arXiv:2206.10498*.
*   Wang et al. (2023) Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. 2023. Evaluation and analysis of hallucination in large vision-language models. *arXiv preprint arXiv:2308.15126*.
*   Weir et al. (2022) Nathaniel Weir, Ryan Thomas, Randolph D’Amore, Kellie Hill, Benjamin Van Durme, and Harsh Jhamtani. 2022. Ontologically faithful generation of non-player character dialogues. *arXiv preprint arXiv:2212.10618*.
*   Widyassari et al. (2022) Adhika Pramita Widyassari, Supriadi Rustad, Guruh Fajar Shidik, Edi Noersasongko, Abdul Syukur, Affandy Affandy, et al. 2022. Review of automatic text summarization techniques & methods. *Journal of King Saud University-Computer and Information Sciences*, 34(4):1029–1046.
*   Xu et al. (2023) Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, and Zhenzhong Lan. 2023. Superclue: A comprehensive chinese large language model benchmark. *arXiv preprint arXiv:2307.15020*.
*   Yang et al. (2020) Shuoheng Yang, Yuxin Wang, and Xiaowen Chu. 2020. A survey of deep learning techniques for neural machine translation. *arXiv preprint arXiv:2002.07526*.
*   Ye et al. (2023) Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. 2023. Satlm: Satisfiability-aided language models using declarative prompting. *Proceedings of NeurIPS*, pages 1–33.
*   Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv preprint arXiv:1905.07830*.
*   Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. *arXiv preprint arXiv:2304.06364*.
*   Zhou et al. (2022) Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, Chris Callison-Burch, Yejin Choi, and Prithviraj Ammanabrolu. 2022. An ai dungeon master’s guide: Learning to converse and guide with intents and theory-of-mind in dungeons and dragons. *arXiv preprint arXiv:2212.10060*.
*   Zhu et al. (2023) Andrew Zhu, Karmanya Aggarwal, Alexander Feng, Lara J Martin, and Chris Callison-Burch. 2023. Fireball: A dataset of dungeons and dragons actual-play with structured game state information. *arXiv preprint arXiv:2305.01528*.
*   Zhu et al. (2021) Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. *arXiv preprint arXiv:2101.00774*.