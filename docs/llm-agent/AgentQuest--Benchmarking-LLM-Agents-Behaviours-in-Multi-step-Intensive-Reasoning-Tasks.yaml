- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:48:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AgentQuest：在多步骤密集推理任务中对 LLM 代理行为进行基准测试
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06411](https://ar5iv.labs.arxiv.org/html/2404.06411)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06411](https://ar5iv.labs.arxiv.org/html/2404.06411)
- en: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini^(1,2)、Giuseppe Siracusano¹、Davide Sanvito¹、Kiril Gashteovski^(1,3)、
- en: David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: David Friede¹、Roberto Bifulco¹、Carolin Lawrence¹
- en: ¹ NEC Laboratories Europe, Heidelberg, Germany
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ NEC 实验室欧洲，德国海德堡
- en: ² Politecnico di Torino, Turin, Italy
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 都灵理工大学，意大利都灵
- en: ³ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³ CAIR，圣西里尔和圣美索迪乌斯大学，北马其顿斯科普里
- en: 'AgentQuest: A Modular Benchmark Framework'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AgentQuest：一个模块化的基准框架
- en: to Measure Progress and Improve LLM Agents
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以测量进展和改进 LLM 代理
- en: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini^(1,2)、Giuseppe Siracusano¹、Davide Sanvito¹、Kiril Gashteovski^(1,3)、
- en: David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: David Friede¹、Roberto Bifulco¹、Carolin Lawrence¹
- en: ¹ NEC Laboratories Europe, Heidelberg, Germany
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ NEC 实验室欧洲，德国海德堡
- en: ² Politecnico di Torino, Turin, Italy
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ² 都灵理工大学，意大利都灵
- en: ³ CAIR, Ss. Cyril and Methodius University, Skopje, North Macedonia
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ³ CAIR，圣西里尔和圣美索迪乌斯大学，北马其顿斯科普里
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advances made by Large Language Models (LLMs) have led to the pursuit of
    LLM agents that can solve intricate, multi-step reasoning tasks. As with any research
    pursuit, benchmarking and evaluation are key corner stones to efficient and reliable
    progress. However, existing benchmarks are often narrow and simply compute overall
    task success. To face these issues, we propose AgentQuest ¹¹1Demo provided at
    [https://youtu.be/0JNkIfwnoak](https://youtu.be/0JNkIfwnoak). – a framework where
    (i) both benchmarks and metrics are modular and easily extensible through well
    documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that
    can reliably track LLM agent progress while solving a task. We exemplify the utility
    of the metrics on two use cases wherein we identify common failure points and
    refine the agent architecture to obtain a significant performance increase. Together
    with the research community, we hope to extend AgentQuest further and therefore
    we make it available under [https://github.com/nec-research/agentquest](https://github.com/nec-research/agentquest).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进展推动了对能够解决复杂、多步骤推理任务的 LLM 代理的追求。像任何研究一样，基准测试和评估是高效可靠进展的关键基石。然而，现有的基准测试往往过于狭窄，仅计算总体任务成功率。为应对这些问题，我们提出了
    AgentQuest ¹¹1Demo，详见 [https://youtu.be/0JNkIfwnoak](https://youtu.be/0JNkIfwnoak)。–
    一个框架，其中（i）基准测试和指标都是模块化的，并且通过良好的文档和易于使用的 API 可以轻松扩展；（ii）我们提供了两个新的评估指标，可以在解决任务时可靠地跟踪
    LLM 代理的进展。我们在两个用例上示范了这些指标的实用性，在这些用例中我们识别了常见的失败点，并优化了代理架构，从而显著提高了性能。我们希望与研究社区一起进一步扩展
    AgentQuest，因此我们将其公开于 [https://github.com/nec-research/agentquest](https://github.com/nec-research/agentquest)。
- en: 'AgentQuest: A Modular Benchmark Framework'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest：一个模块化的基准框架
- en: to Measure Progress and Improve LLM Agents
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以测量进展和改进 LLM 代理
- en: Luca Gioacchini^(1,2), Giuseppe Siracusano¹, Davide Sanvito¹, Kiril Gashteovski^(1,3),
    David Friede¹, Roberto Bifulco¹, Carolin Lawrence¹ ¹ NEC Laboratories Europe,
    Heidelberg, Germany ² Politecnico di Torino, Turin, Italy ³ CAIR, Ss. Cyril and
    Methodius University, Skopje, North Macedonia
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Luca Gioacchini^(1,2)、Giuseppe Siracusano¹、Davide Sanvito¹、Kiril Gashteovski^(1,3)、David
    Friede¹、Roberto Bifulco¹、Carolin Lawrence¹ ¹ NEC 实验室欧洲，德国海德堡 ² 都灵理工大学，意大利都灵 ³
    CAIR，圣西里尔和圣美索迪乌斯大学，北马其顿斯科普里
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative Agents Kiela et al. ([2023](#bib.bib5)) are software systems that
    leverage foundation models like Large Language Models (LLMs) to perform complex
    tasks, take decisions, devise multi-steps plans and use tools (API calls, coding,
    etc.) to build solutions in heterogeneous contexts Wang et al. ([2023](#bib.bib19));
    Weng ([2023](#bib.bib20)). The potential ability to solve heterogeneous tasks
    with high degrees of autonomy has catalysed the interest of both research and
    industrial communities. Nonetheless, it is still unclear to which extent current
    systems are successfully able to fulfil their promises. In fact, methodologies
    to benchmark, evaluate and advance these systems are still in their early days.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 生成代理 Kiela 等 ([2023](#bib.bib5)) 是利用基础模型如大型语言模型（LLMs）执行复杂任务、做出决策、制定多步骤计划并使用工具（API
    调用、编码等）在异构环境中构建解决方案的软件系统 Wang 等 ([2023](#bib.bib19)); Weng ([2023](#bib.bib20))。解决异构任务的潜在能力，具有高度自主性，激发了研究和工业界的兴趣。然而，目前尚不清楚当前系统在多大程度上能够成功实现其承诺。实际上，评估、基准测试和推进这些系统的方法仍处于起步阶段。
- en: '![Refer to caption](img/89401429d32b07819b1859c506c373e0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89401429d32b07819b1859c506c373e0.png)'
- en: (a) Existing
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 现有
- en: '![Refer to caption](img/38b594fdec62729fadbce466dd94b4b3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38b594fdec62729fadbce466dd94b4b3.png)'
- en: (b) AgentQuest
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AgentQuest
- en: 'Figure 1: Overview of agent-benchmark interactions in existing frameworks and
    in AgentQuest. AgentQuest defines a common interface to interact with the benchmarks
    and to compute progress metrics, easing the addition of new benchmarks and allowing
    researchers to evaluate and debug their agent architectures.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：现有框架和 AgentQuest 中代理与基准测试交互的概述。AgentQuest 定义了与基准测试交互并计算进展指标的通用接口，简化了新基准测试的添加，并允许研究人员评估和调试他们的代理架构。
- en: We identify a couple of gaps. Firstly, benchmarking agents requires combining
    different benchmark types Liu et al. ([2023](#bib.bib8)); Chalamalasetti et al.
    ([2023](#bib.bib1)). For example, some benchmarks focus on specific capabilities
    and provide gaming environments, which we refer to as “closed-box” – i.e. with
    a finite set of actions Liu et al. ([2023](#bib.bib8)); Patil et al. ([2023](#bib.bib14));
    Chalamalasetti et al. ([2023](#bib.bib1)) – whereas other benchmarks provide open-ended
    tasks and access to general tools, like web browsing Zhuang et al. ([2023](#bib.bib25));
    Zheng et al. ([2023](#bib.bib24)); Mialon et al. ([2023](#bib.bib9)). As benchmarks
    are developed independently, significant effort goes into custom integration of
    new agent architectures with each benchmark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现了几个不足之处。首先，基准测试代理需要结合不同的基准类型 Liu 等 ([2023](#bib.bib8)); Chalamalasetti 等
    ([2023](#bib.bib1))。例如，一些基准测试专注于特定能力并提供游戏环境，我们称之为“封闭式”——即具有有限动作集 Liu 等 ([2023](#bib.bib8));
    Patil 等 ([2023](#bib.bib14)); Chalamalasetti 等 ([2023](#bib.bib1))——而其他基准测试提供开放式任务和对通用工具的访问，如网页浏览
    Zhuang 等 ([2023](#bib.bib25)); Zheng 等 ([2023](#bib.bib24)); Mialon 等 ([2023](#bib.bib9))。由于基准测试是独立开发的，因此需要付出大量努力将新的代理架构与每个基准测试进行自定义集成。
- en: Secondly, and more critically, existing benchmarks mostly focus on providing
    a *success rate* measure, i.e. a binary success/fail evaluation for each of the
    proposed tasks. While success rate is helpful to measure overall advances of an
    agent technology, it has limited use in guiding improvements for new generative
    agent architectures. Here, it is important to consider that generative agents
    often combine foundation models with multiple other components, such as memory
    and tools. Developers can reason about these individual components in terms of
    architecture and their inter-dependence, and could actively change and evolve
    them using deeper insights about how an agent performs in a benchmark. That is,
    developers need benchmarks to both evaluate and *debug* agents.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，更为关键的是，现有的基准测试大多侧重于提供*成功率*衡量标准，即对每项提议任务的二元成功/失败评估。虽然成功率有助于衡量代理技术的整体进展，但在指导新生成代理架构的改进方面作用有限。在这里，重要的是要考虑到生成代理通常将基础模型与多个其他组件结合在一起，如记忆和工具。开发人员可以根据架构及其相互依赖性来推理这些单独的组件，并利用有关代理在基准测试中表现的更深入见解来主动改变和发展它们。也就是说，开发人员需要基准测试来评估和*调试*代理。
- en: For example, current benchmarks make it hard to answer questions like does the
    agent fail completely the tasks or does it partially solve them? Does the agent
    fail consistently at a certain step? Would extra run time lead to finding a solution?
    Answering these questions would require tracing and inspecting the execution of
    the agent. We argue that providing a more efficient approach that is consistent
    over multiple benchmarks is a stepping stone towards evolving generative agents.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，目前的基准测试很难回答诸如代理是否完全失败了任务或者是否部分解决了任务的问题？代理是否在某个步骤上持续失败？额外的运行时间是否会找到解决方案？回答这些问题需要跟踪和检查代理的执行情况。我们认为，提供一种在多个基准测试中一致的更高效的方法是推动生成代理发展的一个重要步骤。
- en: 'We address these gaps introducing AgentQuest, a modular framework to support
    multiple diverse benchmarks and agent architectures (See Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks")), alongside with two new metrics – i.e. progress rate
    and repetition rate – to debug an agent architecture behaviour. AgentQuest defines
    a standard interface to connect an arbitrary agent architecture with diverse benchmarks,
    and to compute progress and repetition rates from them.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过引入AgentQuest来解决这些差距，这是一种模块化框架，支持多种不同的基准测试和代理架构（见图[1](#S1.F1 "图 1 ‣ 1 介绍
    ‣ AgentQuest：在多步骤密集推理任务中基准测试LLM代理行为")），以及两个新指标——即进展率和重复率——以调试代理架构行为。AgentQuest定义了一个标准接口，用于将任意代理架构与各种基准测试连接，并计算它们的进展率和重复率。
- en: 'We showcase the framework, implementing 4 benchmarks in AgentQuest: ALFWorld Shridhar
    et al. ([2020](#bib.bib15)), Lateral Thinking Puzzles Sloane ([1992](#bib.bib16)),
    Mastermind and Sudoku. The latter two are newly introduced with AgentQuest. Additional
    benchmarks can be easily added, while requiring no changes to the tested agents.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了该框架，在AgentQuest中实现了4个基准测试：ALFWorld Shridhar et al. ([2020](#bib.bib15))，侧向思维谜题
    Sloane ([1992](#bib.bib16))，Mastermind 和数独。后两个基准测试是AgentQuest中新引入的。可以轻松添加额外的基准测试，而无需更改测试的代理。
- en: Our final contribution is to present our experience leveraging the proposed
    metrics to debug and improve existing agent architectures as implemented in LangChain Chase
    ([2022](#bib.bib2)). In particular, we show that in the Mastermind benchmark the
    combination of progress rate and repetition rate identifies a limitation in the
    ability of the agent to explore the full space of potential solutions. Guided
    by this insight we could improve the success rate in this benchmark by up to $\approx$20%.
    In Lateral Thinking Puzzles we show that partially repeating actions is part of
    the agent strategy, whereas in ALFWorld, we show that monitoring the progress
    rate makes it possible to identify that the final success rate is limited by the
    allowed runtime of the agent, and that more steps lead to a better performance.
    Finally, in the Sudoku benchmark, we show that the low success rate is actually
    paired with low progress rate, making clear that the tested agent is unable to
    solve this type of tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终贡献是展示利用提出的指标调试和改进现有代理架构的经验，这些架构在LangChain Chase ([2022](#bib.bib2))中实现。特别地，我们展示了在Mastermind基准测试中，进展率和重复率的结合识别出代理在探索潜在解决方案的完整空间中的能力限制。在这一见解的指导下，我们能够将该基准测试的成功率提高到$\approx$20%。在侧向思维谜题中，我们展示了部分重复动作是代理策略的一部分，而在ALFWorld中，我们展示了监控进展率可以识别出最终的成功率受限于代理允许的运行时间，并且更多的步骤会带来更好的表现。最后，在数独基准测试中，我们展示了低成功率实际上与低进展率相关，这明确了被测试的代理无法解决这类任务。
- en: 2 Generative AI Agents in a Nutshell
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 生成AI代理概述
- en: 'Generative AI agents are automated systems relying on software components integrated
    with LLMs pre-trained on large amount of data for language understanding and processing.
    When assigned a task, an agent engages in a systematic process: it iteratively
    formulates self-generated instructions, executes them, and observes the outcomes
    until the ultimate objective is achieved. Next, we showcase the basic interaction
    between agents and the environment in which they operate and describe the standard
    benchmarking techniques.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI代理是依赖于与大型数据进行语言理解和处理的LLMs集成的软件组件的自动化系统。当分配任务时，代理会参与一个系统化的过程：它反复制定自生成的指令，执行这些指令，并观察结果，直到达到*最终目标*。接下来，我们展示代理与其操作环境之间的基本交互，并描述标准基准测试技术。
- en: 2.1 Agent-Environment interaction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代理-环境交互
- en: 'Closely following the terminology in Reinforcement Learning (RL)²²2Unlike RL
    scenarios, the agent does not need a further training process. It relies on the
    pre-trained LLM and does not perform an action under the influence of any reward. Sutton
    and Barto ([2018](#bib.bib18)), the core elements defining the agent-environment
    interaction are *environment*, *state*, *observation* and *action* (see [Figure 1(a)](#S1.F1.sf1
    "In Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 紧跟强化学习（RL）中的术语²²2与RL场景不同的是，代理无需进一步的训练过程。它依赖于预训练的LLM，不会在任何奖励的影响下执行动作。Sutton 和
    Barto ([2018](#bib.bib18))定义了代理-环境交互的核心要素，包括*环境*、*状态*、*观察*和*动作*（见[图 1(a)](#S1.F1.sf1
    "在图 1 ‣ 1 介绍 ‣ AgentQuest：在多步骤密集推理任务中基准测试LLM代理行为")）。
- en: Environment and states.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 环境和状态。
- en: The environment refers to the external system the agent interacts with. In this
    context, we treat the benchmark and the environment as synonyms. It is typically
    described through a finite set of hidden *states*, which are not directly observable
    by the agent and represent the benchmark configuration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 环境指的是代理与之交互的外部系统。在这个上下文中，我们将基准测试和环境视为同义词。它通常通过一组有限的隐藏*状态*来描述，这些状态代理无法直接观察到，并且代表基准测试的配置。
- en: Observations and actions.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察和动作。
- en: The agent interacts with the environment for multiple execution steps. At each
    step, the environment produces an *observation* providing information about its
    current hidden state. The agent uses the internal LLM to process the received
    observation. Being pre-trained on general knowledge data, the LLM engages a reasoning
    process generating a *thought* on the observation (e.g. the planned strategy to
    follow in the current step or the usage of a tool). According to this thought,
    the agent provides the environment an *action* to modify the current hidden state.
    ³³3Unlike RL, the LLM outputs are unconstrained, and any provided action is considered
    valid.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在多个执行步骤中与环境交互。在每一步，环境会生成一个*观察*，提供有关其当前隐藏状态的信息。代理使用内部LLM处理收到的观察。由于LLM在通用知识数据上进行过预训练，它会进行推理过程，生成对观察的*思考*（例如，在当前步骤中要遵循的计划策略或工具的使用）。根据这一思考，代理向环境提供一个*动作*以修改当前隐藏状态。³³3与RL不同的是，LLM的输出没有约束，任何提供的动作都被认为是有效的。
- en: The following is an example of an agent interacting with a closed-box environment
    to solve a code-breaking challenge, i.e. the Mastermind game Stuckman and Zhang
    ([2005](#bib.bib17)). The agent is tasked to guess a four-digit code. At each
    step, the agent devises a strategy and acts accordingly to guess the code. The
    environment provides a feedback on the number of guessed digits in correct and
    wrong positions. The process continues until a stopping condition is met.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代理与封闭箱环境交互以解决代码破解挑战的示例，即 Mastermind 游戏 Stuckman 和 Zhang ([2005](#bib.bib17))。代理的任务是猜测一个四位数的代码。在每一步，代理制定策略并据此行动以猜测代码。环境提供关于猜测的数字在正确和错误位置上的反馈。该过程持续进行，直到满足停止条件。
- en: 'ENVIRONMENT:
    Initializing a new environment...OBSERVATION: Start guessing the 4 digits code.THOUGHT:
    I’ll start by guessing 1234 to get an idea of whichof these numbers are in the
    secret number and if any are inthe correct position.ACTION: 1234ENVIRONMENT: Comparing
    the guessed code with the truth...OBSERVATION: Your guess has 1 correct numbers
    in the wrongposition and 0 correct numbers in the correct position.Keep guessing...THOUGHT:
    One of the digits 1, 2, 3, or 4 is correct but inthe wrong position. I’ll try
    to move each digit to adifferent position to see if I can get a correct position.ACTION:
    2143...'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '环境：初始化一个新环境...观察：开始猜测4位数字代码。思考：我将开始猜测1234，以了解这些数字中哪些在秘密数字中以及是否有任何在正确的位置。行动：1234环境：将猜测的代码与真实情况进行比较...观察：你的猜测有1个数字在错误的位置和0个数字在正确的位置。继续猜测...思考：数字1、2、3或4中的一个是正确的，但在错误的位置。我将尝试将每个数字移动到不同的位置，看看是否能找到正确的位置。行动：2143...'
- en: 2.2 Benchmarking an agent
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 对代理进行基准测试
- en: 'When evaluating agents performance on a benchmark, the following two metrics
    are commonly used Liu et al. ([2023](#bib.bib8)): (i) Success Rate (SR), i.e.
    the ratio of successful tasks to the total instances; (ii) Time to Success, i.e.
    the average time required to obtain a solution. While important and trending metrics Chalamalasetti
    et al. ([2023](#bib.bib1)); Hessel et al. ([2022](#bib.bib4)); Zhang et al. ([2020a](#bib.bib22)),
    they exclusively address the final success. They cannot measure intermediate success
    or failure and therefore make it difficult to understand why agents might systematically
    fail and how they can be improved. In contrast, we want to define intermediate
    metrics that allow us to easily assess and compare the performance of agents across
    a wide range of tasks.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估代理在基准测试中的表现时，通常使用以下两个指标：刘等人（[2023](#bib.bib8)）：（i）成功率（SR），即成功任务与总实例的比率；（ii）成功时间，即获得解决方案所需的平均时间。虽然这些指标很重要且有趋势（查拉马拉塞蒂等人（[2023](#bib.bib1)）；赫瑟尔等人（[2022](#bib.bib4)）；张等人（[2020a](#bib.bib22)）），但它们仅专注于最终的成功。它们无法衡量中间成功或失败，因此很难理解代理为何可能会系统性失败以及如何改进。相比之下，我们希望定义中间指标，以便轻松评估和比较代理在各种任务中的表现。
- en: 3 AgentQuest Overview
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 AgentQuest 概述
- en: 'We designed AgentQuest as a separation layer between agent and environment
    (see [Figure 1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). Essentially,
    it offers (i) a unified interface (i.e. the *driver*) ensuring compatibility between
    different agent architectures and benchmarks with minimal programming efforts
    (Section [3.1](#S3.SS1 "3.1 Benchmarks common interface ‣ 3 AgentQuest Overview
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks")); (ii) the implementation of two metrics beyond task success (i.e. *progress
    rate* and *repetition rate*) aimed at monitoring the agent advancement toward
    the final goal and allowing us to understand the reasons behind failures (Section
    [3.2](#S3.SS2 "3.2 Understanding agent advancements ‣ 3 AgentQuest Overview ‣
    AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks")); (iii) a unique vantage point and interface for implementing new metrics
    to monitoring and measuring the execution (Section [3.3](#S3.SS3 "3.3 Adding new
    metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks")).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设计了 AgentQuest 作为代理和环境之间的分离层（见 [图 1(b)](#S1.F1.sf2 "在图 1 ‣ 1 引言 ‣ AgentQuest:
    基准测试 LLM 代理在多步骤深度推理任务中的行为")）。本质上，它提供了（i）一个统一的接口（即 *驱动程序*），确保不同代理架构和基准之间的兼容性，且编程工作量最小（第
    [3.1](#S3.SS1 "3.1 基准测试通用接口 ‣ 3 AgentQuest 概述 ‣ AgentQuest: 基准测试 LLM 代理在多步骤深度推理任务中的行为")
    节）；（ii）超越任务成功的两个度量（即 *进展率* 和 *重复率*），旨在监控代理朝最终目标的进展，并帮助我们理解失败的原因（第 [3.2](#S3.SS2
    "3.2 理解代理进展 ‣ 3 AgentQuest 概述 ‣ AgentQuest: 基准测试 LLM 代理在多步骤深度推理任务中的行为") 节）；（iii）一个独特的视角和接口，用于实现新的度量标准以监控和衡量执行情况（第
    [3.3](#S3.SS3 "3.3 添加新度量 ‣ 3 AgentQuest 概述 ‣ AgentQuest: 基准测试 LLM 代理在多步骤深度推理任务中的行为")
    节）。'
- en: 3.1 Benchmarks common interface
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基准测试通用接口
- en: Different benchmarks require invoking distinct functions, using specific formats,
    and performing parsing and post-processing of observations and agent actions.
    To integrate different agent architectures, the common trend is hardcoding such
    benchmark-specific requirements directly in the framework (Liu et al. [2023](#bib.bib8);
    Chalamalasetti et al. [2023](#bib.bib1), inter alia). This results in many custom
    interfaces tailored on each environment, making it difficult to easily move to
    other benchmarks and agent architectures.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的基准测试需要调用不同的函数，使用特定的格式，并执行对观察和代理动作的解析及后处理。为了整合不同的代理架构，常见的做法是将这些基准特定的要求硬编码到框架中（Liu
    等 [2023](#bib.bib8); Chalamalasetti 等 [2023](#bib.bib1) 等）。这导致了许多针对每个环境定制的接口，使得在不同的基准测试和代理架构之间迁移变得困难。
- en: Instead, AgentQuest exposes a single unified Python interface, i.e. the Driver
    and two classes reflecting the agent-environment interaction components (i.e.
    Observation, Action).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，AgentQuest 提供了一个统一的 Python 接口，即 Driver 和两个反映代理-环境交互组件的类（即 Observation 和 Action）。
- en: Observations and actions.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察和动作。
- en: 'We provide two simple classes: Observation and Action. The first has two required
    attributes: (i) output, a string reporting information about the environment state;
    (ii) done, a Boolean variable indicating if the final task is currently accomplished
    or not. The Action class has one required attribute, action_value. It is a string
    directly output by the agent. Once processed and provided to the environment,
    it triggers the environment change. To customise the interactions, developers
    can define optional attributes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了两个简单的类：Observation 和 Action。Observation 类有两个必需的属性：（i）output，一个字符串，用于报告关于环境状态的信息；（ii）done，一个布尔变量，指示最终任务是否已完成。Action
    类有一个必需的属性 action_value。它是代理直接输出的字符串。一旦处理并提供给环境，它将触发环境的变化。为了自定义交互，开发人员可以定义可选属性。
- en: Driver.
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 驱动程序。
- en: 'We provide the Driver class with two mandatory methods: (i) the reset method
    initialises a new instance of the environment and returns the first observation;
    (ii) the step method performs one single execution step. It accepts one instance
    of the Action class from the agent, processes the action (e.g. parses the action_value
    string) and uses it to modify the environment state. It always returns an observation.
    The driver supports also the benchmark-specific state attribute, acting as a simple
    API. It exposes the environment state at step $t$, useful to compute the progress
    rate.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 Driver 类提供了两个必需的方法：(i) `reset` 方法初始化环境的新实例并返回第一次观察；(ii) `step` 方法执行单个执行步骤。它接受来自代理的一个
    `Action` 类实例，处理该动作（例如解析 `action_value` 字符串），并用它来修改环境状态。它总是返回一个观察结果。该驱动程序还支持特定基准的状态属性，作为一个简单的
    API。它公开在步骤 $t$ 的环境状态，这对于计算进度率很有用。
- en: 'We here provide an example of the implemented interaction for Mastermind:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了 Mastermind 实现交互的示例：
- en: 'from  agentquest.drivers  import  MasterMindDriverfrom  agentquest.utils  import  Actionfrom  agentquest.metrics  import  get_progress,  get_repetitionagent  =  ...  #  Initialize  your  agentactions,  progress,  repetitions  =  [],  [],  []#  Initialize  the  environment  and  reset  rounddriver  =  MasterMindDriver(truth=’5618’)obs  =  driver.reset()#  Agent  loopwhile  not  obs.done:guess  =  agent(obs.output)  #  Get  the  agent  outputaction  =  Action(action_value=guess)  #  Create  actionactions.append(action.action_value)  #  Store  actionobs  =  driver.step(action)  #  Execute  step#  Compute  current  progress  and  repetitionprogress.append(get_progress(driver.state,  ’5618’))repetitions.append(get_repetitions(actions))#  Extend  with  your  custom  metrics  here  ...#  Compute  final  metricsPR  =  [x/len(’5618’)  for  x  in  progress]RR  =  [x/(len(actions)-1)  for  x  in  repetitions]'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '从
    agentquest.drivers 导入 MasterMindDriver 从 agentquest.utils 导入 Action 从 agentquest.metrics
    导入 get_progress, get_repetition agent = ...  # 初始化你的代理 actions, progress, repetitions
    = [], [], [] # 初始化环境并重置轮次 driver = MasterMindDriver(truth=’5618’) obs = driver.reset()
    # 代理循环 while not obs.done: guess = agent(obs.output)  # 获取代理输出 action = Action(action_value=guess)  #
    创建动作 actions.append(action.action_value)  # 存储动作 obs = driver.step(action)  #
    执行步骤 # 计算当前进度和重复次数 progress.append(get_progress(driver.state, ’5618’)) repetitions.append(get_repetitions(actions))
    # 在这里扩展你的自定义指标 ... # 计算最终指标 PR = [x/len(’5618’) for x in progress] RR = [x/(len(actions)-1)
    for x in repetitions]'
- en: 3.2 Understanding agent advancements
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 了解代理的进展
- en: Getting insights on how they tackle a specific task is key to comprehend agent
    behaviours, capabilities and limitations. Furthermore, identifying systematic
    agent failures allows to pinpoint necessary adjustments within the architecture
    to effectively address the underlying issues.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 了解他们如何处理特定任务是理解代理行为、能力和局限性的关键。此外，识别系统性代理失败可以准确找出架构中需要调整的部分，以有效解决潜在问题。
- en: AgentQuest contributes towards this direction introducing two cross-benchmark
    metrics, the *progress rate* and the *repetition rate*. While the first expresses
    *how much* the agent is advancing towards the final goal, the latter indicates
    *how* it is reaching it, with a specific focus on the amount of repeated (i.e.
    similar) actions the agent performs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest 朝着这个方向迈进，引入了两个跨基准的指标，即*进度率*和*重复率*。前者表示*代理人*向最终目标的推进程度，后者则表明*代理人*如何达到这个目标，特别关注代理人执行的重复（即相似）动作的数量。
- en: Milestones and progress rate.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 里程碑和进度率。
- en: To quantify the agent advancement towards the final goal, AgentQuest uses a
    set of *milestones* $\mathcal{M}$ the evaluation coincides with the success rate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化代理人向最终目标的推进，AgentQuest 使用一组*里程碑* $\mathcal{M}$，评估与成功率一致。
- en: We assign a score to all the states included in $\mathcal{M}$ dependant of such
    scoring function, as an indication of how far the agent is from the goal, allowing
    to track agent progress over time. Depending on the benchmark, the progress rate
    might also decrease during the execution. Milestones can either be manually annotated,
    or internally computed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 $\mathcal{M}$ 中的所有状态分配一个分数，作为代理人距离目标的指示，这样可以跟踪代理人随时间的进展。根据基准测试，进度率在执行过程中也可能会下降。里程碑可以手动标注，也可以内部计算。
- en: Repetition rate.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重复率。
- en: 'The repetition rate $\text{RR}_{t}$ is a measure of the agent tendency of repeating
    actions. Depending on the benchmark, we do not consider repetitions as a limitation
    – e.g. solving a maze requires repetitions, such as going left repeatedly. See
    also [Section 4](#S4 "4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM
    Agents Behaviours in Multi-step Intensive Reasoning Tasks") for a positive and
    negative example of repetitions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重复率 $\text{RR}_{t}$ 是代理人重复动作的倾向的度量。根据基准测试，我们不将重复视为限制——例如，解决迷宫需要重复的动作，如反复向左走。有关重复的正面和负面示例，请参见
    [第 4 节](#S4 "4 通过 AgentQuest 获取洞察 ‣ AgentQuest：多步骤密集推理任务中的 LLM 代理行为基准测试")。
- en: At execution step $t$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行步骤 $t$。
- en: Based on this, we define the repetition rate at step $t$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们定义步骤 $t$ 的重复率。
- en: 3.3 Adding new metrics
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 添加新指标
- en: 'Table 1: Attributes exposing components of the agent-environment interaction
    useful to define new metrics.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：暴露代理人-环境交互组件的属性，有助于定义新指标。
- en: '| Class | Attribute | Access to |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 属性 | 访问权限 |'
- en: '| Driver | state | Hidden states |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 驱动程序 | 状态 | 隐藏状态 |'
- en: '| Observation | output | Observations |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 观察 | 输出 | 观察结果 |'
- en: '| Action | action$\_$value | Agent actions |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | action$\_$value | 代理行动 |'
- en: We rely on the progress and repetition rates to show how AgentQuest can be extended
    with new metrics through a simple function template. We then show the implementations
    of the functions adapted to the considered benchmark.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依靠进度率和重复率来展示 AgentQuest 如何通过一个简单的函数模板扩展新的指标。接着，我们展示了适应于考虑的基准测试的函数实现。
- en: Metric function template.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标函数模板。
- en: 'We use a Python function template to easily define the elements of the agent-environment
    interactions required for computing a given metric. [Table 1](#S3.T1 "In 3.3 Adding
    new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks") provides a recap of the main attributes
    and reference classes that can be used as input for the custom metrics. Additionally,
    users can provide external data, like milestones or action history.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Python 函数模板来轻松定义计算给定指标所需的代理人-环境交互元素。[表 1](#S3.T1 "在 3.3 添加新指标 ‣ 3 AgentQuest
    概述 ‣ AgentQuest：多步骤密集推理任务中的 LLM 代理行为基准测试") 提供了可以作为自定义指标输入的主要属性和参考类的回顾。此外，用户可以提供外部数据，如里程碑或行动历史。
- en: 'Table 2: Overview of the benchmarks provided in AgentQuest.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：AgentQuest 提供的基准测试概述。
- en: '| Benchmark | Description | Milestones |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 描述 | 里程碑 |'
- en: '| Mastermind |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 主谋 |'
- en: '&#124; Guessing a numeric code with feedback on guessed digits and positions.
    &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测一个数字代码，并反馈猜测的数字和位置。 &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Digits of the code to guess. &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要猜测的代码位数。 &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LTP |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LTP |'
- en: '&#124; Solving riddles by asking Yes/No questions. &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过询问是/否问题来解决谜题。 &#124;'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Guessed riddle key aspects. &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 猜测谜题的关键方面。 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ALFWorld |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld |'
- en: '&#124; Finding an object in a textual world and using it. &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在文本世界中寻找对象并使用它。 &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Sequence of actions. &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行动序列。 &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Sudoku |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 数独 |'
- en: '&#124; 9x9 grid puzzle. Digits 1-9 fill each column, row, and 3x3 sub-grid
    &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9x9 网格谜题。数字 1-9 填充每列、每行和 3x3 子网格 &#124;'
- en: '&#124; without repetition. &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无重复。 &#124;'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total number of correct &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确的总数量 &#124;'
- en: '&#124; inserted digits. &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 插入的数字。 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Implement progress rate.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现进度率。
- en: 'Depending on the benchmark, developers need to implement the custom scoring
    function $f$. Milestones can either be user-defined or internally computed within
    get_progress. Here, we show the definition of get_progress to quantify the achieved
    milestones for Mastermind. The milestones are the digits of the final solution
    and the progress indicates the count of correctly guessed digits in their positions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 根据基准，开发者需要实现自定义评分函数 $f$。里程碑可以是用户定义的或在 get_progress 内部计算的。在这里，我们展示了 get_progress
    的定义，以量化 Mastermind 的已实现里程碑。里程碑是最终解的数字，进度表示正确猜测的数字在其位置的数量：
- en: 'def  get_progress(state,  milestones):reached_milestones  =  0  #  Digits  in  correct  positionfor  i,  j  in  zip(state,  milestones):if  i  ==  j:  reached_milestones  +=  1return  reached_milestones#  Usage  example.  The  code  to  guess  is  ’5618’progress  =  get_progress(’2318’,  ’5618’)  #  Reached  milestones>>>  2progress/len(’5618’)  #  Compute  Progress  Rate>>>  0.5'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'def  get_progress(state,  milestones):reached_milestones  =  0  #  正确位置的数字for  i,  j  in  zip(state,  milestones):if  i  ==  j:  reached_milestones  +=  1return  reached_milestones#  使用示例。猜测的代码是
    ’5618’progress  =  get_progress(’2318’,  ’5618’)  #  已达成的里程碑>>>  2progress/len(’5618’)  #  计算进度率>>>  0.5'
- en: Implement repetition rate.
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现重复率。
- en: To determine if an action is repeated, the end user must define the similarity
    function $g$ is the Levenshtein similarity Levenshtein ([1966](#bib.bib6)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定一个动作是否被重复，最终用户必须定义相似性函数 $g$，该函数为 Levenshtein 相似性 Levenshtein ([1966](#bib.bib6))。
- en: 'from  Levenshtein  import  ratio  as  gdef  get_repetitions(actions,  THETA_A):unique_act  =  set()  #  Initialise  unique  actionsfor  i,a  in  enumerate(actions):#  Check  for  repetitionsif  all([g(a,actions[x])>>  1  repeated  action#  Compute  Repetition  Raterepetitions/(len(actions)-1)>>>  0.33'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '
    从 Levenshtein import ratio 作为 gdef get_repetitions(actions, THETA_A): unique_act
    = set()  # 初始化唯一动作 for i, a in enumerate(actions): # 检查重复项 if all([g(a, actions[x])
    < THETA_A for x in range(i)]): unique_act.add(a) return len(actions) - len(unique_act)
    # 使用示例。要猜的代码是 ''5618'' actions = [''1234'', ''2143'', ''1234'', ''5618'']  # 动作历史
    repetitions = get_repetitions(actions, 1.0) >>> 1 次重复动作 # 计算重复率 repetitions /
    (len(actions) - 1) >>> 0.33'
- en: In other cases, where $a$ can be any text string, we can use standard metrics,
    such as BLEU Papineni et al. ([2002](#bib.bib12)), ROUGE Lin ([2004](#bib.bib7))
    or BERTScore Zhang et al. ([2020b](#bib.bib23)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，当 $a$ 可以是任何文本字符串时，我们可以使用标准度量，例如 BLEU Papineni et al. ([2002](#bib.bib12))、ROUGE
    Lin ([2004](#bib.bib7)) 或 BERTScore Zhang et al. ([2020b](#bib.bib23))。
- en: 4 Insights via AgentQuest
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 AgentQuest 的 4 个见解
- en: 'We investigate agent behaviours in different reasoning scenarios by proposing
    a starting set of four benchmarks. We implemented from scratch Sudoku Felgenhauer
    and Jarvis ([2006](#bib.bib3)) and Mastermind Stuckman and Zhang ([2005](#bib.bib17))
    environments, while ALFWorld Shridhar et al. ([2020](#bib.bib15)) and Lateral
    Thinking Puzzles (LTP)Sloane ([1992](#bib.bib16)) are existing implementations Liu
    et al. ([2023](#bib.bib8)). [Table 2](#S3.T2 "In Metric function template. ‣ 3.3
    Adding new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM Agents
    Behaviours in Multi-step Intensive Reasoning Tasks") provides an overview of the
    benchmarks and their respective milestones used to measure progress.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过提出一组四个基准来调查在不同推理场景下的代理行为。我们从零实现了 Sudoku Felgenhauer 和 Jarvis ([2006](#bib.bib3))
    和 Mastermind Stuckman 和 Zhang ([2005](#bib.bib17)) 环境，而 ALFWorld Shridhar et al.
    ([2020](#bib.bib15)) 和 Lateral Thinking Puzzles (LTP) Sloane ([1992](#bib.bib16))
    是现有实现 Liu et al. ([2023](#bib.bib8))。 [表 2](#S3.T2 "In Metric function template.
    ‣ 3.3 Adding new metrics ‣ 3 AgentQuest Overview ‣ AgentQuest: Benchmarking LLM
    Agents Behaviours in Multi-step Intensive Reasoning Tasks") 提供了基准和其各自里程碑的概述，用于测量进展。'
- en: We emphasise that this evaluation is not aimed at providing a thorough evaluation
    and comparison of agent architectures, but rather to show how to use AgentQuest
    and how monitoring progress and action repetition can provide relevant insights
    to developers, even after a few executions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调，这一评估并非旨在提供对代理架构的全面评估和比较，而是展示如何使用 AgentQuest，以及如何通过监控进展和动作重复提供对开发者相关的见解，即使在几次执行之后。
- en: Experimental setup.
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: 'We use as reference architecture the off-the-shelf chat agent provided by LangChain Chase
    ([2022](#bib.bib2)) powered by GPT-4 OpenAI ([2023b](#bib.bib11)) as LLM because
    it is intuitive, easy to extend and open source. We run 15 instances of the four
    benchmarks within AgentQuest, setting the maximum number of execution steps as
    60⁵⁵5We limit the number of instances in our experiments for two main reasons:
    (i) the work primarily serves as a demonstration of the developed framework itself,
    rather than an extensive evaluation of the agent performance; (ii) extensive tests
    could have significantly impacted the ability to reproduce the experiments due
    to the expensive nature of API calls.. In Appendix [B](#A2 "Appendix B Appendix:
    Additional agents architectures and benchmarks ‣ AgentQuest: Benchmarking LLM
    Agents Behaviours in Multi-step Intensive Reasoning Tasks") we provide examples
    on how to use AgentQuest with two additional agent architectures and GAIA Mialon
    et al. ([2023](#bib.bib9)) as open-ended environment.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 LangChain Chase 提供的现成聊天代理作为参考架构（[2022](#bib.bib2)），该代理由 GPT-4 OpenAI（[2023b](#bib.bib11)）驱动，作为
    LLM，因为它直观、易于扩展且开源。我们在 AgentQuest 中运行了四个基准的 15 个实例，将最大执行步骤数设置为 60⁵⁵5。我们限制实验中的实例数量有两个主要原因：（i）这项工作主要作为已开发框架本身的演示，而非对代理性能的广泛评估；（ii）广泛的测试可能会由于
    API 调用的昂贵性质而显著影响实验的重现性。在附录 [B](#A2 "附录 B 附录：附加代理架构和基准 ‣ AgentQuest：在多步骤密集推理任务中基准测试
    LLM 代理行为")中，我们提供了如何使用 AgentQuest 进行两种额外代理架构和 GAIA Mialon 等（[2023](#bib.bib9)）作为开放式环境的示例。
- en: 'Table 3: Average existing and proposed metrics for the tested benchmarks. We
    report the metrics, Success Rate (SR), Steps, Progress Rate at step 60 (PR[60])
    and Repetition Rate at final step 60 (RR[60]). We denote with ^∗ the improved
    results after modifying the agent architecture.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：测试基准的平均现有和提议指标。我们报告了指标、成功率 (SR)、步骤、步骤 60 的进展率 (PR[60]) 和最终步骤 60 的重复率 (RR[60])。我们用
    ^∗ 标记修改代理架构后改进的结果。
- en: '|  | Existing Metrics | AgentQuest |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 现有指标 | AgentQuest |'
- en: '|  | SR | Steps | PR[60] | RR[60] |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | SR | 步骤 | PR[60] | RR[60] |'
- en: '| Mastermind | 0.47 | 41.87 | 0.62 | 0.32 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind | 0.47 | 41.87 | 0.62 | 0.32 |'
- en: '| LTP | 0.20 | 52.00 | 0.46 | 0.81 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LTP | 0.20 | 52.00 | 0.46 | 0.81 |'
- en: '| ALFWorld | 0.86 | 21.00 | 0.74 | 0.06 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld | 0.86 | 21.00 | 0.74 | 0.06 |'
- en: '| Sudoku | 0.00 | 59.67 | 0.08 | 0.22 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 数独 | 0.00 | 59.67 | 0.08 | 0.22 |'
- en: '| Mastermind^∗ | 0.60 | 39.73 | 0.73 | 0.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Mastermind^∗ | 0.60 | 39.73 | 0.73 | 0.00 |'
- en: '| ALFWorld^∗ | 0.93 | 25.86 | 0.80^† | 0.07^† |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld^∗ | 0.93 | 25.86 | 0.80^† | 0.07^† |'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ^†Metrics referred to the extended runtime up to 120 &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ^†指标指扩展运行时间到 120 &#124;'
- en: '&#124; steps, hence PR[120] and RR[120]. &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 步骤，因此 PR[120] 和 RR[120]。 &#124;'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Experimental results.
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验结果。
- en: 'For Mastermind, [Figure 2(a)](#S4.F2.sf1 "In Figure 2 ‣ Experimental results.
    ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks") shows the progress rate PR[t] and repetition
    rate RR[t]. In the first 22 steps, the agent explores different solutions (RR${}_{[0,22]}<5\%$.
    Hence, AgentQuest offered us a crucial insights on why the current agent cannot
    solve the Mastermind game.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mastermind，[图 2(a)](#S4.F2.sf1 "在图 2 中 ‣ 实验结果 ‣ 4 通过 AgentQuest 的洞察 ‣ AgentQuest：在多步骤密集推理任务中基准测试
    LLM 代理行为") 显示了进展率 PR[t] 和重复率 RR[t]。在前 22 步中，代理探索了不同的解决方案（RR${}_{[0,22]}<5\%$）。因此，AgentQuest
    为我们提供了有关当前代理无法解决 Mastermind 游戏的重要见解。
- en: 'To overcome this agent limitation we incorporate a memory component Park et al.
    ([2023](#bib.bib13)) into the agent architecture. The agent stores the past guesses
    in a local buffer. Then, at each step, if the agent outputs an action already
    in the buffer, it is prompted to provide a new one. [Table 3](#S4.T3 "In Experimental
    setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks") (Mastermind^∗) shows that this simple
    change in agent architecture has a big impact: the agent can now solve more instances,
    increasing the final SR from 47% to 60% and preventing repetitions (RR${}_{60}=0\%$).
    This highlights how studying the interplay between progress and repetition rates
    can allow us to improve agent architecture, sometimes even with simple remedies.
    We support our intuition extending the evaluation to more instances of Mastermind
    from 15 to 60 achieving comparable results – i.e. 43% of SR with the standard
    architecture and 62% with the simple memory (19% of improvement).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一代理局限性，我们将一个记忆组件 Park et al. ([2023](#bib.bib13)) 集成到代理架构中。代理将过去的猜测存储在本地缓冲区中。然后，在每一步，如果代理输出的行动已存在于缓冲区中，则会提示提供一个新的行动。[表
    3](#S4.T3 "在实验设置中。 ‣ 4 通过 AgentQuest 获取见解 ‣ AgentQuest：在多步骤密集推理任务中基准测试 LLM 代理行为")
    (Mastermind^∗) 显示，这一简单的代理架构变更产生了巨大影响：代理现在能够解决更多的实例，将最终 SR 从 47% 提升到 60%，并防止了重复（RR${}_{60}=0\%$）。这突显了研究进展率和重复率之间的相互作用如何帮助我们改进代理架构，有时甚至可以通过简单的修正来实现。我们通过将评估扩展到更多的
    Mastermind 实例，从 15 个增加到 60 个，并取得了相似的结果——即标准架构下 SR 为 43%，而简单记忆下 SR 为 62%（提升了 19%）。
- en: '![Refer to caption](img/b20ba5b73ec0042b2e99ab2dd15c2f0c.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b20ba5b73ec0042b2e99ab2dd15c2f0c.png)'
- en: (a) Mastermind
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mastermind
- en: '![Refer to caption](img/a1b8e48e5effd4a6aec9771ca534f737.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a1b8e48e5effd4a6aec9771ca534f737.png)'
- en: (b) LTP
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LTP
- en: 'Figure 2: Average Progress rate PR[t] and the repetition rate RR[t] on Mastermind
    and LTP. Mastermind: It starts out with a low RR[t] but this increases after step
    22 while the progress rate also stall at 55%. LTP: at first a higher RR[t] allows
    the agent to progress by making small variations that lead to success, but later
    this plateaus.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: Mastermind 和 LTP 中的平均进展率 PR[t] 和重复率 RR[t]。Mastermind: 起初 RR[t] 较低，但在第
    22 步后增加，同时进展率也停滞在 55%。LTP: 起初较高的 RR[t] 允许代理通过进行小的变动来取得成功，但之后进展趋于平稳。'
- en: '![Refer to caption](img/57f89da4c4982e92db9e5f22a6851ae2.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/57f89da4c4982e92db9e5f22a6851ae2.png)'
- en: (a) Mastermind
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Mastermind
- en: '![Refer to caption](img/777fd447306968fed1abafca68031cb9.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/777fd447306968fed1abafca68031cb9.png)'
- en: (b) LTP
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LTP
- en: 'Figure 3: Examples of repeated actions in Mastermind and LTP. Mastermind: there
    is a set of unique actions at first, but then gets stuck repeating the same actions
    over and over. LTP: repeated actions are small variations of the same question
    that lead to progress.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: Mastermind 和 LTP 中重复行动的示例。Mastermind: 起初有一组独特的行动，但随后不断重复相同的行动。LTP: 重复的行动是相同问题的小变动，这些变动带来了进展。'
- en: 'For LTP, the AgentQuest metrics reveal a different agent behaviour, where repetitions
    are part of the agent reasoning strategy, enhancing the progress rate ([Figure 2(b)](#S4.F2.sf2
    "In Figure 2 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")).
    From the initial steps, the agent changes aspects of the same questions until
    a local solution emerges. This leads to horizontal indicators in [Figure 3(b)](#S4.F3.sf2
    "In Figure 3 ‣ Experimental results. ‣ 4 Insights via AgentQuest ‣ AgentQuest:
    Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks") and
    RR${}_{20}\approx 30\%.$. This shows us how the interplay of progress and repetition
    rates provides an insight on how agents behave across the different time steps.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LTP，AgentQuest 指标揭示了不同的代理行为，其中重复是代理推理策略的一部分，增强了进展率（[图 2(b)](#S4.F2.sf2 "在图
    2 中 ‣ 实验结果。 ‣ 4 通过 AgentQuest 获取见解 ‣ AgentQuest：在多步骤密集推理任务中基准测试 LLM 代理行为")）。从最初步骤开始，代理改变相同问题的各个方面，直到出现局部解决方案。这在
    [图 3(b)](#S4.F3.sf2 "在图 3 中 ‣ 实验结果。 ‣ 4 通过 AgentQuest 获取见解 ‣ AgentQuest：在多步骤密集推理任务中基准测试
    LLM 代理行为") 中导致了水平指示器，并且 RR${}_{20}\approx 30\%$。这向我们展示了进展率和重复率的相互作用如何提供对代理在不同时间步骤下行为的洞察。
- en: 'Consider the benchmark ALFWorld in [Table 3](#S4.T3 "In Experimental setup.
    ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours in
    Multi-step Intensive Reasoning Tasks") (we report the metrics trend in Appendix
    [A](#A1 "Appendix A Appendix: ALFWorld and Sudoku benchmarks ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). It requires
    the exploration of a textual world to locate an object. While the agent explores
    the solution space and limits action repetitions (RR${}_{60}=6\%$). This discrepancy
    may arise from the more exploration steps required to discover the object. We
    support this intuition extending the benchmark runtime to 120 steps resulting
    in a success and progress rates increase by 6% (ALFWorld^∗ in [Table 3](#S4.T3
    "In Experimental setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking
    LLM Agents Behaviours in Multi-step Intensive Reasoning Tasks")). This confirms
    the usefulness of AgentQuest in understanding the agent failures. We support our
    intuition also extending the evaluation to more instances of ALFWorld from 15
    to 60 achieving comparable results – i.e. 83% of SR with 60 steps as limit and
    87% with 120 steps as limit (4% of improvement).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑基准 ALFWorld 在[表 3](#S4.T3 "在实验设置中 ‣ 4 通过 AgentQuest 获得的见解 ‣ AgentQuest：基准测试
    LLM 代理在多步骤密集推理任务中的行为")（我们在附录[A](#A1 "附录 A 附录：ALFWorld 和数独基准 ‣ AgentQuest：基准测试
    LLM 代理在多步骤密集推理任务中的行为")中报告了这些指标的趋势）。它要求探索一个文本世界以定位一个对象。虽然代理在探索解空间并限制行动重复（RR${}_{60}=6\%$）。这种差异可能源于发现对象所需的更多探索步骤。我们通过将基准测试运行时间延长至
    120 步来支持这一直觉，这导致成功率和进展率增加了 6%（ALFWorld^∗ 在[表 3](#S4.T3 "在实验设置中 ‣ 4 通过 AgentQuest
    获得的见解 ‣ AgentQuest：基准测试 LLM 代理在多步骤密集推理任务中的行为")）。这确认了 AgentQuest 在理解代理失败中的有用性。我们还通过将评估扩展到
    ALFWorld 更多实例，从 15 个扩展到 60 个，取得了类似结果——即限制为 60 步的成功率为 83%，限制为 120 步的成功率为 87%（提升
    4%）。
- en: 'Finally, we look at Sudoku, known for its high level of difficulty Felgenhauer
    and Jarvis ([2006](#bib.bib3)). The low progress and repetition rates achieved
    after 60 steps (PR${}_{60}=8\%$) indicate that the current agent architecture
    struggles in finding correct solutions solving this task. We report the metrics
    trend in Appendix [A](#A1 "Appendix A Appendix: ALFWorld and Sudoku benchmarks
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看数独，因其高难度而著名（Felgenhauer 和 Jarvis ([2006](#bib.bib3))）。在 60 步之后取得的低进展和重复率（PR${}_{60}=8\%$）表明当前的代理架构在找到正确解决方案方面存在困难。我们在附录[A](#A1
    "附录 A 附录：ALFWorld 和数独基准 ‣ AgentQuest：基准测试 LLM 代理在多步骤密集推理任务中的行为")中报告了这些指标的趋势。
- en: 5 Conclusions
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: AgentQuest allows the research community to keep track of agent progress in
    a holistic manner. Starting out with a first set of four benchmarks and two new
    metrics, AgentQuest is easily extendable. Furthermore, the two proposed metrics,
    progress and repetition rates, have the great advantage of allowing to track how
    agents advance toward the final goal over time. Especially studying their interplay
    can lead to important insights that will allow the research community to improve
    agent performance. Finally, we believe that promptly sharing AgentQuest with the
    research community will facilitate benchmarking and debugging agents, and will
    foster the creation and use of new benchmarks and metrics.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: AgentQuest 允许研究社区全面跟踪代理的进展。AgentQuest 从一组四个基准和两个新指标开始，易于扩展。此外，两个提出的指标——进展率和重复率——具有跟踪代理随时间推进到最终目标的显著优势。特别是研究它们的相互作用可以带来重要的见解，帮助研究社区提升代理的表现。最后，我们相信及时与研究社区共享
    AgentQuest 将有助于基准测试和调试代理，并促进新基准和指标的创建和使用。
- en: Ethical Considerations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: The complexity of LLM agents poses challenges in comprehending their decision-making
    processes. Ethical guidelines must demand transparency in such systems, ensuring
    that developers and end-users comprehend how decisions are reached.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理的复杂性使得理解其决策过程成为挑战。伦理指南必须要求这些系统具有透明性，确保开发者和最终用户理解决策是如何做出的。
- en: We are not aware of any direct ethical impact generated by our work. However,
    we hope that insights into Generative AI agents’ decision-making processes will
    be applied to improve and promote transparency and fairness.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有意识到我们的工作会产生直接的伦理影响。然而，我们希望对生成式 AI 代理决策过程的见解能被应用于提升和促进透明度与公平性。
- en: Acknowledgements
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This project has received funding from the European Union’s Horizon Europe research
    and innovation programme (SNS-JU) under the Grant Agreement No 101139285 (“NATWORK”).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目已获得欧盟地平线欧洲研究和创新计划（SNS-JU）资助，资助协议编号 101139285（“NATWORK”）。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chalamalasetti et al. (2023) Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov,
    Brielen Madureira, Philipp Sadler, and David Schlangen. 2023. [Clembench: Using
    Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents](http://arxiv.org/abs/2305.13455).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chalamalasetti et al. (2023) Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov,
    Brielen Madureira, Philipp Sadler, 和 David Schlangen. 2023. [Clembench：使用游戏玩法评估聊天优化语言模型作为对话代理](http://arxiv.org/abs/2305.13455)。
- en: Chase (2022) Harrison Chase. 2022. [LangChain - Building applications with LLMs
    through composability](https://github.com/langchain-ai/langchain).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase (2022) Harrison Chase. 2022. [LangChain - 通过可组合性构建LLM应用](https://github.com/langchain-ai/langchain)。
- en: Felgenhauer and Jarvis (2006) Bertram Felgenhauer and Frazer Jarvis. 2006. Mathematics
    of Sudoku I. *Mathematical Spectrum*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Felgenhauer 和 Jarvis (2006) Bertram Felgenhauer 和 Frazer Jarvis. 2006. 数独的数学
    I. *数学光谱*。
- en: 'Hessel et al. (2022) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2022. [CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning](http://arxiv.org/abs/2104.08718).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel et al. (2022) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    和 Yejin Choi. 2022. [CLIPScore：一种无参考图像字幕评估指标](http://arxiv.org/abs/2104.08718)。
- en: Kiela et al. (2023) Douwe Kiela, Tristan Thrush, Kawin Ethayarajh, and Amanpreet
    Singh. 2023. [Plotting Progress in AI](https://contextual.ai/plotting-progress-in-ai/).
    *Contextual AI Blog*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiela et al. (2023) Douwe Kiela, Tristan Thrush, Kawin Ethayarajh, 和 Amanpreet
    Singh. 2023. [AI的进展图谱](https://contextual.ai/plotting-progress-in-ai/)。 *Contextual
    AI 博客*。
- en: Levenshtein (1966) Vladimir I. Levenshtein. 1966. Binary codes capable of correcting
    deletions, insertions, and reversals. In *Soviet Physics Doklady*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levenshtein (1966) Vladimir I. Levenshtein. 1966. 纠正删除、插入和反转的二进制编码。发表于 *苏联物理学公报*。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A Package for Automatic Evaluation of
    Summaries](https://aclanthology.org/W04-1013).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2004) Chin-Yew Lin. 2004. [ROUGE：自动评估摘要的软件包](https://aclanthology.org/W04-1013)。
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023. [AgentBench:
    Evaluating LLMs as Agents](http://arxiv.org/abs/2308.03688).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, 等. 2023. [AgentBench：评估LLM作为代理](http://arxiv.org/abs/2308.03688)。
- en: 'Mialon et al. (2023) Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, and Thomas Scialom. 2023. [GAIA: a benchmark for General AI
    Assistants](http://arxiv.org/abs/2311.12983).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon et al. (2023) Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas
    Wolf, Yann LeCun, 和 Thomas Scialom. 2023. [GAIA：通用AI助理基准](http://arxiv.org/abs/2311.12983)。
- en: OpenAI (2023a) OpenAI. 2023a. [Assistants API](https://platform.openai.com/docs/assistants/overview).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023a) OpenAI. 2023a. [助理API](https://platform.openai.com/docs/assistants/overview)。
- en: OpenAI (2023b) OpenAI. 2023b. [GPT-4 Technical Report](http://arxiv.org/abs/2303.08774).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI. 2023b. [GPT-4技术报告](http://arxiv.org/abs/2303.08774)。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu: a Method for Automatic Evaluation of Machine Translation](https://doi.org/10.3115/1073083.1073135).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. 2002. [Bleu：一种自动评估机器翻译的方法](https://doi.org/10.3115/1073083.1073135)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. [Generative Agents: Interactive
    Simulacra of Human Behavior](https://doi.org/10.1145/3586183.3606763).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith
    Ringel Morris, Percy Liang, 和 Michael S Bernstein. 2023. [生成代理：人类行为的互动模拟](https://doi.org/10.1145/3586183.3606763)。
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. 2023. [Gorilla: Large Language Model Connected with Massive APIs](http://arxiv.org/abs/2305.15334).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, 和 Joseph E. Gonzalez.
    2023. [Gorilla：与大量API连接的大型语言模型](http://arxiv.org/abs/2305.15334)。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](http://arxiv.org/abs/2010.03768).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2020. [ALFWorld：对齐文本和具身环境以实现互动学习](http://arxiv.org/abs/2010.03768)。
- en: Sloane (1992) Paul Sloane. 1992. *Lateral Thinking Puzzlers*. Sterling Publishing
    Company, Inc.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sloane (1992) Paul Sloane. 1992. *横向思维谜题*。Sterling Publishing Company, Inc.
- en: Stuckman and Zhang (2005) Jeff Stuckman and Guo-Qiang Zhang. 2005. [Mastermind
    is NP-complete](http://arxiv.org/abs/cs/0512049).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stuckman 和 Zhang（2005）Jeff Stuckman 和 Guo-Qiang Zhang。2005。[Mastermind 是 NP
    完全的](http://arxiv.org/abs/cs/0512049)。
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    Learning: An Introduction*. MIT press.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto（2018）Richard S Sutton 和 Andrew G Barto。2018。*强化学习：导论*。MIT 出版社。
- en: Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2023. [A Survey
    on Large Language Model based Autonomous Agents](http://arxiv.org/abs/2308.11432).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin 等。2023。[基于大型语言模型的自主代理调研](http://arxiv.org/abs/2308.11432)。
- en: Weng (2023) Lilian Weng. 2023. [LLM-powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng（2023）Lilian Weng。2023。[LLM 驱动的自主代理](https://lilianweng.github.io/posts/2023-06-23-agent/)。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. [ReAct: Synergizing Reasoning and Acting
    in Language Models](http://arxiv.org/abs/2210.03629).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2022）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan 和 Yuan Cao。2022。[ReAct：在语言模型中协同推理与行动](http://arxiv.org/abs/2210.03629)。
- en: 'Zhang et al. (2020a) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020a. [BERTScore: Evaluating Text Generation with BERT](http://arxiv.org/abs/1904.09675).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020a）Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger 和
    Yoav Artzi。2020a。[BERTScore：使用 BERT 评估文本生成](http://arxiv.org/abs/1904.09675)。
- en: 'Zhang et al. (2020b) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger,
    and Yoav Artzi. 2020b. [BERTScore: Evaluating Text Generation with BERT](https://openreview.net/forum?id=SkeHuCVFDr).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020b）Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger 和
    Yoav Artzi。2020b。[BERTScore：使用 BERT 评估文本生成](https://openreview.net/forum?id=SkeHuCVFDr)。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. [Judging LLM-as-a-judge with MT-Bench and Chatbot Arena](http://arxiv.org/abs/2306.05685).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023）Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等。2023。[评估 LLM 作为评审的
    MT-Bench 和 Chatbot Arena](http://arxiv.org/abs/2306.05685)。
- en: 'Zhuang et al. (2023) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao
    Zhang. 2023. [ToolQA: A Dataset for LLM Question Answering with External Tools](http://arxiv.org/abs/2306.13304).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang 等（2023）Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun 和 Chao Zhang。2023。[ToolQA：用于
    LLM 问答的外部工具数据集](http://arxiv.org/abs/2306.13304)。
- en: 'Appendix A Appendix: ALFWorld and Sudoku benchmarks'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录：ALFWorld 和 Sudoku 基准测试
- en: In this section we report the detailed metrics for each step for the ALFWorld
    and Sudoku benchmarks, omitted for the sake of brevity from the main paper.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们报告了 ALFWorld 和 Sudoku 基准测试每个步骤的详细指标，因篇幅限制从主论文中省略。
- en: '![Refer to caption](img/a846aa40dd019d161ef4ee6a0a1f4abe.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a846aa40dd019d161ef4ee6a0a1f4abe.png)'
- en: (a) ALFWorld
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (a) ALFWorld
- en: '![Refer to caption](img/a6d46b5a28c844aabb6df2cfa923badb.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6d46b5a28c844aabb6df2cfa923badb.png)'
- en: (b) Sudoku
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Sudoku
- en: 'Figure 4: Progress rate PR[t] and the repetition rate RR[t] on ALFWorld and
    Sudoku averaged over 15 runs. ALFWorld: It starts out with a low repetition rate
    and quick increase of the progress rate. Then a slow increase of the repetition
    rate enables to further increase the progress rate although less quickly. Sudoku:
    The progress rate quickly reaches 8%. The repetition rate then slowly increases
    without any positive change in the progress rate.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：ALFWorld 和 Sudoku 上的进度率 PR[t] 和重复率 RR[t]，平均 15 次运行。ALFWorld：起初重复率较低，进度率迅速上升。随后，重复率缓慢上升使得进度率进一步提高，尽管提高速度较慢。Sudoku：进度率迅速达到
    8%。随后，重复率缓慢上升，进度率没有任何积极变化。
- en: 'Figure [4(a)](#A1.F4.sf1 "Figure 4(a) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") reports the progress rate and repetition rate for
    ALFWorld. The repetition rate is close to 0% for the first 20 steps, then it slowly
    increases up to 6% after 60 steps. The progress rate quickly reaches over 50%
    in 10 steps, then keeps increasing, although slowly, up to 74%. The consistent
    improvement of the progress rate even for steps close to 60 together with the
    low repetition rate suggests that higher values may be reached by increasing the
    maximum number of steps. We validate this hypothesis by extending the benchmark
    runtime to 120 steps. As previously reported in Table [3](#S4.T3 "Table 3 ‣ Experimental
    setup. ‣ 4 Insights via AgentQuest ‣ AgentQuest: Benchmarking LLM Agents Behaviours
    in Multi-step Intensive Reasoning Tasks"), this results in an improvement of 6
    percentage points for both the success rate the progress rate, i.e. SR$=93\%$.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4(a)](#A1.F4.sf1 "Figure 4(a) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") 报告了 ALFWorld 的进展率和重复率。前 20 步的重复率接近 0%，然后在 60 步后缓慢增加至
    6%。进展率在 10 步内迅速超过 50%，然后继续缓慢增加，达到 74%。即使在接近 60 步时进展率仍然一致提高，加上低重复率，表明通过增加最大步数可能会达到更高的值。我们通过将基准测试运行时间延长至
    120 步来验证这一假设。正如表 [3](#S4.T3 "Table 3 ‣ Experimental setup. ‣ 4 Insights via AgentQuest
    ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step Intensive Reasoning
    Tasks") 中先前报告的，这使得成功率和进展率均提高了 6 个百分点，即 SR$=93\%$。'
- en: 'Figure [4(b)](#A1.F4.sf2 "Figure 4(b) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") includes the two metrics for the Sudoku benchmark.
    We can observe that the progress rate quickly reaches a plateau at 8% in very
    few steps. The repetition rate is close to 0% for the first 10 steps, then it
    slowly increases up to 22% after 60 steps without any improvement of the progress
    rate.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4(b)](#A1.F4.sf2 "Figure 4(b) ‣ Figure 4 ‣ Appendix A Appendix: ALFWorld
    and Sudoku benchmarks ‣ AgentQuest: Benchmarking LLM Agents Behaviours in Multi-step
    Intensive Reasoning Tasks") 包含了数独基准测试的两个指标。我们可以观察到，进展率在非常少的步骤中迅速达到 8% 的平台期。前 10
    步的重复率接近 0%，然后它在 60 步后缓慢增加至 22%，而进展率没有任何改善。'
- en: 'Appendix B Appendix: Additional agents architectures and benchmarks'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附录：额外的代理架构和基准测试
- en: In this section we highlight the plug-and-play aspect of AgentQuest showing
    the implementation of Mastermind with two additional agents architectures, i.e.
    ReAct Yao et al. ([2022](#bib.bib21)) as the most used architecture in literature
    and OpenAI Assistant OpenAI ([2023a](#bib.bib10)), as the most recent proprietary
    architecture. Additionally, we show how to implement the open-ended benchmark
    GAIA Mialon et al. ([2023](#bib.bib9)) requiring the usage of external tools.
    For brevity, in the following snippets we omit details, like error handling or
    full agent definition. The complete code is available in the [GitHub repository](https://github.com/nec-research/agentquest).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们突出展示了 AgentQuest 的即插即用特性，展示了使用两个额外代理架构的 Mastermind 实现，即 ReAct Yao 等人（[2022](#bib.bib21)），这是文献中使用最广泛的架构，以及
    OpenAI Assistant OpenAI（[2023a](#bib.bib10)），这是最新的专有架构。此外，我们展示了如何实现开放式基准测试 GAIA
    Mialon 等人（[2023](#bib.bib9)），这需要使用外部工具。为了简洁起见，在以下代码片段中，我们省略了一些细节，如错误处理或完整的代理定义。完整代码可在
    [GitHub 仓库](https://github.com/nec-research/agentquest)中找到。
- en: B.1 ReAct for Closed-box Environments
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 ReAct 在封闭环境中的应用
- en: We show an example of how to execute a closed-box benchmark (i.e. ALFWorld)
    with an agent based on the ReAct architecture Yao et al. ([2022](#bib.bib21)).
    Such architecture forces the agent decision making process to generate both textual
    reasoning traces and actions pertaining to a task in an interleaved manner. Common
    implementations Chase ([2022](#bib.bib2)); Yao et al. ([2022](#bib.bib21)) rely
    on external tools to perform actions. Here, we ensure compatibility with existing
    implementations providing a single tool (i.e. ProxyTool) that forwards the actions
    to the driver. In a nutshell, the agent reflects on the action to take and invokes
    the tool. Then, we feed the tool input to the driver to perform the interaction
    with the environment. At each step, we provide the agent the updated history of
    the actions and observations through the intermediate_steps variable.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何使用基于 ReAct 架构的智能体执行封闭箱基准测试（即 ALFWorld）的示例，Yao 等人（[2022](#bib.bib21)）。这种架构强制智能体的决策过程以交错的方式生成任务相关的文本推理痕迹和动作。常见的实现方式，如
    Chase（[2022](#bib.bib2)）；Yao 等人（[2022](#bib.bib21)），依赖外部工具来执行动作。在这里，我们通过提供一个单一工具（即
    ProxyTool）来确保与现有实现的兼容性，该工具将动作转发给驱动程序。简而言之，智能体考虑采取的行动并调用工具。然后，我们将工具输入提供给驱动程序，以便与环境进行交互。在每一步中，我们通过
    intermediate_steps 变量向智能体提供更新后的动作和观察历史。
- en: 'from  agentquest.drivers  import  MasterMindDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  a  dummy  tool  for  closed-box  environmentsclass  ProxyTool(BaseTool):name  =  "proxytool"description  =  "Provide  the  action  you  want  to  perform"def  _run(self):pass#  Instantiate  custom  promptprompt  =  CustomPromptTemplate(template=...,  #  LLM  prompttools=[ProxyTool()],input_variables=["intermediate_steps",  ...])#  Initialise  the  agentagent  =  create_react_agent(llm,  [ProxyTool()],  prompt)intermediate_steps  =  []#  Initialise  the  driverdriver  =  MasterMindDriver(game)#  Get  the  first  observationobs  =  driver.reset()#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputagent_choice  =  agent.invoke({’input’:obs.output,’intermediate_steps’:intermediate_steps})action  =  Action(action_value=agent_choice.tool_input)#  Perform  the  stepobs  =  driver.step(action)#  Update  intermediate  stepsintermediate_steps.append((agent_choice,  obs.output))#  Get  current  metrics  ...'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '来自  agentquest.drivers  导入  MasterMindDriver来自  agentquest.metrics  导入  ...来自  agentquest.utils  导入  Action...#  定义一个用于封闭式环境的虚拟工具类  ProxyTool(BaseTool):name  =  "proxytool"description  =  "提供你想执行的操作"def  _run(self):pass#  实例化自定义提示prompt  =  CustomPromptTemplate(template=...,  #  LLM  提示工具=[ProxyTool()],input_variables=["intermediate_steps",  ...])#  初始化代理agent  =  create_react_agent(llm,  [ProxyTool()],  prompt)intermediate_steps  =  []#  初始化驱动程序driver  =  MasterMindDriver(game)#  获取第一次观察obs  =  driver.reset()#  代理循环while  not  obs.done:#  检索代理输出agent_choice  =  agent.invoke({’input’:obs.output,’intermediate_steps’:intermediate_steps})action  =  Action(action_value=agent_choice.tool_input)#  执行步骤obs  =  driver.step(action)#  更新中间步骤intermediate_steps.append((agent_choice,  obs.output))#  获取当前指标  ...'
- en: B.2 OpenAI Assistant for Closed-box Environments
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 OpenAI 助手用于封闭式环境
- en: The OpenAI Assistant OpenAI ([2023a](#bib.bib10)) is a proprietary architecture.
    It allows users to define custom agents by specifying the tasks to accomplish
    and the set of tools the agent can use. While the decision-making process is not
    directly accessible by the end-users (the agent and the LLM are hosted on the
    proprietary cloud environment), the tools can be invoked both remotely or locally.
    In the latter, users have control on the tool invocation managing the agent loop.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Assistant OpenAI ([2023a](#bib.bib10)) 是一个专有架构。它允许用户通过指定要完成的任务和代理可以使用的工具集来定义自定义代理。虽然决策过程对最终用户不可直接访问（代理和
    LLM 托管在专有云环境中），但工具可以远程或本地调用。在后者情况下，用户可以控制工具调用，管理代理循环。
- en: Similarly to ReAct, we here rely on the ProxyTool, acting as a proxy between
    the agent and the environment. We invoke the remote agent with the initial task
    (e.g. first ALFWorld observation) and process the output of its decision making
    process, i.e. the action to perform provided as tool input. Then, we bypass the
    tool invocation, directly forwarding the action to the driver to perform the execution
    step and retrieve the next observation. Finally, we invoke the agent with the
    new observation concluding the execution step.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ReAct，我们在这里依赖于 ProxyTool，它充当代理，连接代理与环境之间。我们用初始任务（例如，第一次 ALFWorld 观察）调用远程代理，并处理其决策过程的输出，即作为工具输入提供的执行动作。然后，我们绕过工具调用，直接将动作转发给驱动程序以执行步骤并获取下一个观察。最后，我们用新的观察结果调用代理，完成执行步骤。
- en: 'from  agentquest.drivers  import  MasterMindDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  a  dummy  tool  for  closed-box  environmentsclass  ProxyTool(BaseTool):name  =  "proxytool"description  =  "Provide  the  action  you  want  to  perform"def  _run(self):pass#  Initialise  the  agentagent  =  OpenAIAssistantRunnable.create_assistant(instructions=...  #  LLM  prompttools=[ProxyTool()],model=...  #  Chosen  LLMas_agent=True)#  Initialise  the  driverdriver  =  MasterMindDriver(game)#  Get  the  first  observationobs  =  driver.reset()#  Get  the  first  actionresponse  =  agent.invoke({"content":  obs.output})#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputagent_guess  =  response[0].tool_inputaction  =  Action(action_value=agent_guess)#  Perform  the  stepobs  =  driver.step(action)#  Get  current  metrics  ...#  Manage  Proxy  Tool  outputtool_outputs  =  [{"output":  obs.output,"tool_call_id":  response[0].tool_call_id}]#  Invoke  the  agent  to  get  the  next  actionresponse  =  agent.invoke({"tool_outputs":  tool_outputs,"run_id":  response[0].run_id,"thread_id":  response[0].thread_id})'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '从  agentquest.drivers  导入  MasterMindDriver从  agentquest.metrics  导入  ...从  agentquest.utils  导入  Action...#  定义  一个  用于  封闭环境的  虚拟工具类  ProxyTool(BaseTool):名称  =  "proxytool"描述  =  "提供  您  想要  执行的  操作"def  _run(self):pass#  初始化  代理agent  =  OpenAIAssistantRunnable.create_assistant(指令=...  #  LLM  提示工具=[ProxyTool()],模型=...  #  选择的  LLM_as_agent=True)#  初始化  驱动程序driver  =  MasterMindDriver(游戏)#  获取  第一次  观察观察  =  driver.reset()#  获取  第一次  动作响应  =  agent.invoke({"content":  观察.output})#  代理循环while  not  观察.done:#  获取  代理  输出代理猜测  =  响应[0].tool_input动作  =  Action(action_value=代理猜测)#  执行  步骤观察  =  driver.step(动作)#  获取  当前  指标  ...#  管理  代理工具  输出工具_输出  =  [{"output":  观察.output,"tool_call_id":  响应[0].tool_call_id}]#  调用  代理  获取  下一步  动作响应  =  agent.invoke({"tool_outputs":  工具_输出,"run_id":  响应[0].run_id,"thread_id":  响应[0].thread_id})'
- en: B.3 OpenAI Assistant for Open-ended Environments
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 OpenAI 助手用于开放式环境
- en: When interacting with an open-ended environment, the agent is not restricted
    to the pre-defined actions of the closed-box environment and it is allowed to
    select any user-defined tool (e.g. retrieving information online or executing
    code). Hence, we provide the agent the list of tools via the tool variable. The
    agent relies on its reasoning process to choose which tool to invoke. Omitted
    here for the sake of brevity, we rely of the manual annotations of the GAIA questions Mialon
    et al. ([2023](#bib.bib9)) as milestones to compute the progress rate.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当与开放环境交互时，代理不受限于封闭环境中的预定义操作，而是可以选择任何用户定义的工具（例如，在线检索信息或执行代码）。因此，我们通过工具变量提供给代理一个工具列表。代理依赖其推理过程来选择调用哪个工具。为了简洁起见，此处省略了，我们依赖GAIA问题的人工注释（Mialon
    et al. ([2023](#bib.bib9))）作为计算进度率的里程碑。
- en: 'from  agentquest.drivers  import  GaiaDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  the  toolstools=[OnlineSearch(),  #  Retrieve  a  web  page  linkWebContentParser(),  #  Read  the  web  pageFinalAnswerRetriever(),  #  Provide  the  final  answer...]#  Initialise  the  agentagent  =  OpenAIAssistantRunnable.create_assistant(instructions=...  #  LLM  prompttools=tools,model=...  #  Chosen  LLMas_agent=True)#  Initialise  the  driverdriver  =  GaiaDriver(question,  tools)#  Get  the  first  observationobs  =  driver.reset()#  Get  the  first  actionresponse  =  agent.invoke({"content":  obs.output})#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputact  =  f’{response[0].tool}:{response[0].tool_input}’action  =  Action(action_value=act)#  Perform  the  step  invoking  the  local  toolobs  =  driver.step(action)#  Get  current  metrics  ...#  Manage  tool  output  as  observationtool_outputs  =  [{"output":  obs.output,"tool_call_id":  response[0].tool_call_id}]#  Invoke  the  agent  to  get  the  next  actionresponse  =  agent.invoke({"tool_outputs":  tool_outputs,"run_id":  response[0].run_id,"thread_id":  response[0].thread_id})'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 'from  agentquest.drivers  import  GaiaDriverfrom  agentquest.metrics  import  ...from  agentquest.utils  import  Action...#  Define  the  toolstools=[OnlineSearch(),  #  Retrieve  a  web  page  linkWebContentParser(),  #  Read  the  web  pageFinalAnswerRetriever(),  #  Provide  the  final  answer...]#  Initialise  the  agentagent  =  OpenAIAssistantRunnable.create_assistant(instructions=...  #  LLM  prompttools=tools,model=...  #  Chosen  LLMas_agent=True)#  Initialise  the  driverdriver  =  GaiaDriver(question,  tools)#  Get  the  first  observationobs  =  driver.reset()#  Get  the  first  actionresponse  =  agent.invoke({"content":  obs.output})#  Agent  Loopwhile  not  obs.done:#  Retrieve  the  agent  outputact  =  f’{response[0].tool}:{response[0].tool_input}’action  =  Action(action_value=act)#  Perform  the  step  invoking  the  local  toolobs  =  driver.step(action)#  Get  current  metrics  ...#  Manage  tool  output  as  observationtool_outputs  =  [{"output":  obs.output,"tool_call_id":  response[0].tool_call_id}]#  Invoke  the  agent  to  get  the  next  actionresponse  =  agent.invoke({"tool_outputs":  tool_outputs,"run_id":  response[0].run_id,"thread_id":  response[0].thread_id})'
- en: 'Here, the driver acts as a wrapper, executing the tool with the parameters
    provided by the agent (tool_input) and forwards the output to the agent in the
    correct format:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，驱动程序充当封装器，执行工具并使用代理提供的参数（tool_input），然后将输出以正确的格式转发给代理：
- en: 'class  GaiaDriver():def  __init__(self,  question,  tools,  ...):#  Initialise  the  tool  lookupself.tool_lookup  =  {x.name:x  for  x  in  tools}...def  step(self,  action):#  Parse  the  actiontool,  tool_input  =  action.action_value.split(’:’)#  Invoke  the  tooltool_out  =  self.tool_lookup[tool]._run(tool_input)#  Parse  the  tool  output  here  ...return  Observation(output=tool_out)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '类  GaiaDriver():def  __init__(self,  question,  tools,  ...):#  初始化  工具  查找self.tool_lookup  =  {x.name:x  for  x  in  tools}...def  step(self,  action):#  解析  操作tool,  tool_input  =  action.action_value.split(’:’)#  调用  工具tool_out  =  self.tool_lookup[tool]._run(tool_input)#  在这里  解析  工具  输出  ...return  Observation(output=tool_out)'
