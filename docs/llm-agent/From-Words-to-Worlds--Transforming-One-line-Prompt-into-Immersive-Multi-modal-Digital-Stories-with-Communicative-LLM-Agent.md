<!--yml
category: 未分类
date: 2025-01-11 12:33:02
-->

# From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent

> 来源：[https://arxiv.org/html/2406.10478/](https://arxiv.org/html/2406.10478/)

Samuel S. Sohn Rutgers University, 08854, New Jersey, USA Equal contribution Danrui Li Rutgers University, 08854, New Jersey, USA Equal contribution Sen Zhang Rutgers University, 08854, New Jersey, USA Equal contribution Che-Jui Chang Rutgers University, 08854, New Jersey, USA Mubbasir Kapadia Rutgers University, 08854, New Jersey, USA

###### Abstract

Digital storytelling, essential in entertainment, education, and marketing, faces challenges in production scalability and flexibility. The StoryAgent framework, introduced in this paper, utilizes Large Language Models and generative tools to automate and refine digital storytelling. Employing a top-down story drafting and bottom-up asset generation approach, StoryAgent tackles key issues such as manual intervention, interactive scene orchestration, and narrative consistency. This framework enables efficient production of interactive and consistent narratives across multiple modalities, democratizing content creation and enhancing engagement. Our results demonstrate the framework’s capability to produce coherent digital stories without reference videos, marking a significant advancement in automated digital storytelling.

Key Words: Pedestrian Model, Visual Attention, Retail Environment, Transportation hubs

![Refer to caption](img/3eca99fc3718225be947c2b05eb2287c.png)

Figure 1: StoryAgent is a digital storytelling generation framework that integrates communicative Large Language Model agents with state-of-the-art generative models and tools. Taking one-line text instruction as input, it produces digital storytelling content with scene interactivity, long-duration consistency, and intervention flexibility.

## 1 Introduction

Digital storytelling has emerged as a powerful medium across various domains, including entertainment, education, and marketing [[1](https://arxiv.org/html/2406.10478v2#bib.bib1), [2](https://arxiv.org/html/2406.10478v2#bib.bib2), [3](https://arxiv.org/html/2406.10478v2#bib.bib3)] due to its ability to combine multimedia elements such as text, images, audio, and video to create immersive and interactive digital narratives. Its versatile applications, ranging from interactive narratives in video games to immersive experiences in virtual reality, make it an indispensable tool for conveying information and eliciting emotions in today’s digital age. Traditionally the production processes of digital storytelling narratives are often time-consuming and resource-intensive, limiting the speed and volume of content output.

Hence, there arises a critical need to streamline and automate the production pipeline to meet the growing demands for engaging and dynamic storytelling content. Recent advancements in text-based generative models[[4](https://arxiv.org/html/2406.10478v2#bib.bib4), [5](https://arxiv.org/html/2406.10478v2#bib.bib5), [6](https://arxiv.org/html/2406.10478v2#bib.bib6), [7](https://arxiv.org/html/2406.10478v2#bib.bib7), [8](https://arxiv.org/html/2406.10478v2#bib.bib8), [9](https://arxiv.org/html/2406.10478v2#bib.bib9), [10](https://arxiv.org/html/2406.10478v2#bib.bib10)], which enable synthesizing assets in text, image, sound, and motion, have facilitated a hands-free digital storytelling production. It can democratize the creation process and potentially enable anyone who lacks the artistic skills to produce complex digital narratives easily. Several previous works [[11](https://arxiv.org/html/2406.10478v2#bib.bib11), [12](https://arxiv.org/html/2406.10478v2#bib.bib12), [13](https://arxiv.org/html/2406.10478v2#bib.bib13), [14](https://arxiv.org/html/2406.10478v2#bib.bib14)] tried to consolidate an automation procedure for part of the production pipeline for different use cases such as immersive interactive storytelling, storyboard making, staging, and script writing and functioning as authoring tools. With the help of multimodal generative models, we could ease the procedure of producing assets for digital storytelling. However, significant challenges persist in leveraging those multimodal generative models.

One notable challenge is the need for flexible intervention (Challenge 1), as human creators often require the ability to modify initial generation results according to their preferences. Text-based generative models[[7](https://arxiv.org/html/2406.10478v2#bib.bib7), [9](https://arxiv.org/html/2406.10478v2#bib.bib9)] are proficient at creating high-quality short clips but offer limited fine-grained control over outcomes, such as modifying characters while maintaining the same storyline. Conversely, procedural methods [[12](https://arxiv.org/html/2406.10478v2#bib.bib12), [15](https://arxiv.org/html/2406.10478v2#bib.bib15)] enable fine-grained control but typically require a specialized interaction interface with the framework, which often lacks a universal and convenient approach for human intervention. Moreover, orchestrating the interactions between characters, objects, and scenes remains a difficult task (Challenge 2), yet indispensable to improving the visual fidelity and elevating the digital storytelling experience [[16](https://arxiv.org/html/2406.10478v2#bib.bib16)]. Finally, consistency is needed for audience engagement. For instance, character appearances and voice tones should remain consistent with the narrative context throughout the story. In addition, consistency should also include the synchronization of the textual plots and downstream modalities, including audio, speech, and visuals [[17](https://arxiv.org/html/2406.10478v2#bib.bib17), [18](https://arxiv.org/html/2406.10478v2#bib.bib18), [19](https://arxiv.org/html/2406.10478v2#bib.bib19), [20](https://arxiv.org/html/2406.10478v2#bib.bib20), [21](https://arxiv.org/html/2406.10478v2#bib.bib21), [22](https://arxiv.org/html/2406.10478v2#bib.bib22)]. Despite advancements in diffusion-based animation generation [[23](https://arxiv.org/html/2406.10478v2#bib.bib23), [24](https://arxiv.org/html/2406.10478v2#bib.bib24), [25](https://arxiv.org/html/2406.10478v2#bib.bib25)], existing methods struggle to ensure long-term consistency or require additional inputs like reference videos or skeletons (Challenge 3).

We propose a novel StoryAgent framework, which integrates communicative Large Language Model agents [[26](https://arxiv.org/html/2406.10478v2#bib.bib26), [27](https://arxiv.org/html/2406.10478v2#bib.bib27)] with generative models and tools. Our framework operates by initially drafting the story by a top-down approach, using communicative LLM agents to decompose text instructions into a hierarchical textual representation of the digital storytelling content, where the leaf nodes are descriptions of a single modality for a snippet of the timeline. Subsequently, it employs generative models and tools in a bottom-up fashion to create and assemble the corresponding assets from text descriptions.

The framework addresses the three aforementioned challenges. First, its textual representation and generation pipeline facilitate fine-grained control and intervention for human developers through simple natural language instructions (Challenge 1). Leveraging the reasoning capabilities of Large Language Models, the framework can comprehend instructions to identify and modify the corresponding leaf nodes in the hierarchy, while keeping other components unchanged. This process allows for targeted adjustments without disrupting the rest of the content. Moreover, by combining the bottom-up idea in the procedural generation pipeline with the top-down hierarchical textual representation, our framework handles the issue of consistency and scene interactivity (Challenge 2). For instance, a character’s appearance across time frames is linked to the same costume asset ID within the textual representation. During video rendering, this asset is consistently reused, ensuring visual uniformity across all scenes. Similarly, semantic and spatial information from the generated images is captured and integrated into the textual hierarchy, paving the way to the scene interactivity for downstream components. Inherently, with the design of our framework, it can generate videos that require no reference videos as inputs, and the temporal limitation does not exist(Challenge 3).

Additionally, by leveraging text as the intermediate product to decouple story drafting and asset generation, our framework facilitates a plug-and-play structure. It not only allows for unprecedented coverage of modalities (see Tab. [1](https://arxiv.org/html/2406.10478v2#S1.T1 "Table 1 ‣ 1 Introduction ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")), but also easy integration of the latest generative models, which ensures that our framework performance can continuously benefit from ongoing research developments.

|  |  | World | Character | Audio | Others |
|  | Plot | Semantic | Visual | Appearance | Animation | Music | Speech | SFX | Cinematography |
| [[28](https://arxiv.org/html/2406.10478v2#bib.bib28)] | ✓ |  |  |  |  |  |  |  |  |
| [[29](https://arxiv.org/html/2406.10478v2#bib.bib29)] |  | ✓ |  |  |  |  |  |  |  |
| [[30](https://arxiv.org/html/2406.10478v2#bib.bib30)] |  | ✓ | $\triangle$ |  |  |  |  |  |  |
| [[13](https://arxiv.org/html/2406.10478v2#bib.bib13)] |  |  |  |  |  |  |  |  | ✓ |
| [[31](https://arxiv.org/html/2406.10478v2#bib.bib31)] |  |  | $\triangle$ | $\triangle$ | ✓+ $\triangle$ |  |  |  |  |
| [[32](https://arxiv.org/html/2406.10478v2#bib.bib32)] | ✓ |  | $\triangle$ | $\triangle$ | $\triangle$ |  |  |  |  |
| [[8](https://arxiv.org/html/2406.10478v2#bib.bib8)] |  |  |  |  |  | ✓ | ✓+ $\triangle$ | ✓ |  |
| Ours | ✓ | ✓ | ✓ | ✓+ $\triangle$ | $\triangle$ | ✓ | ✓ | ✓+ $\triangle$ | ✓ |

Table 1: Digital storytelling components covered in prior works and ours. ✓= generative models. $\triangle$ = retrieval methods.

## 2 Background

### 2.1 Digital storytelling as authoring tools

Digital storytelling has traditionally focused on providing authoring tools for human developers, typically adopting a procedural generation approach rather than end-to-end solutions like [[9](https://arxiv.org/html/2406.10478v2#bib.bib9)]. This methodology incorporates a wide range of storytelling components. Following the definition of [[17](https://arxiv.org/html/2406.10478v2#bib.bib17)], the components can be categorized into plots (textual contents such as story arc and events) and space (world settings, characters, scene props, etc.).

Previous storytelling authoring tools like [[11](https://arxiv.org/html/2406.10478v2#bib.bib11), [12](https://arxiv.org/html/2406.10478v2#bib.bib12), [13](https://arxiv.org/html/2406.10478v2#bib.bib13), [14](https://arxiv.org/html/2406.10478v2#bib.bib14)] are made for a specific stage in the film production pipeline, to involve human invention and refinement, they require domain knowledge for users. Our StoryAgent framework provides agent-wise human intervention both on the high and low levels for amateurs and professionals.

### 2.2 Enable scene interactivity

Recent studies have advanced the dynamics of character interactions within 3D environments, significantly aided by the explicit spatial representations of 3D objects [[31](https://arxiv.org/html/2406.10478v2#bib.bib31), [16](https://arxiv.org/html/2406.10478v2#bib.bib16), [33](https://arxiv.org/html/2406.10478v2#bib.bib33), [34](https://arxiv.org/html/2406.10478v2#bib.bib34), [18](https://arxiv.org/html/2406.10478v2#bib.bib18), [35](https://arxiv.org/html/2406.10478v2#bib.bib35), [36](https://arxiv.org/html/2406.10478v2#bib.bib36), [37](https://arxiv.org/html/2406.10478v2#bib.bib37)]. In contrast, for 2D art styles, despite diffusion-based models delivering superior visual quality, the lack of inherent spatial scene information within 2D images poses challenges for implementing interactivity.

Nevertheless, existing research in image understanding offers many tools for deducing spatial data from images, including techniques like segmentation [[38](https://arxiv.org/html/2406.10478v2#bib.bib38)] and depth estimation [[39](https://arxiv.org/html/2406.10478v2#bib.bib39)]. These methodologies can underpin a procedural pipeline designed to facilitate interactivity in 2D contexts. This paper represents an initial effort to enable such scene interactivity for 2D art styles.

### 2.3 Consistency in digital storytelling

Prior works have explored various approaches to ensure coherence and consistency within the plot generation, such as event-driven [[15](https://arxiv.org/html/2406.10478v2#bib.bib15)], persona-driven [[40](https://arxiv.org/html/2406.10478v2#bib.bib40), [41](https://arxiv.org/html/2406.10478v2#bib.bib41)], state-space planning[[42](https://arxiv.org/html/2406.10478v2#bib.bib42)], and top-down decomposition [[43](https://arxiv.org/html/2406.10478v2#bib.bib43)]. However, for the consistent joint generation between plot and space components, previous studies have been limited, generally focusing on one or two areas such as crowd motion [[44](https://arxiv.org/html/2406.10478v2#bib.bib44)], cinematography [[13](https://arxiv.org/html/2406.10478v2#bib.bib13)], world settings [[30](https://arxiv.org/html/2406.10478v2#bib.bib30), [45](https://arxiv.org/html/2406.10478v2#bib.bib45)] etc. (see Tab. [1](https://arxiv.org/html/2406.10478v2#S1.T1 "Table 1 ‣ 1 Introduction ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")). While some approaches have aimed to address the alignment of all visual components simultaneously, typically using latent diffusion text-to-image models [[46](https://arxiv.org/html/2406.10478v2#bib.bib46), [47](https://arxiv.org/html/2406.10478v2#bib.bib47), [48](https://arxiv.org/html/2406.10478v2#bib.bib48)], their effectiveness is still constrained by the time length, rendering them unsuitable for long-term digital storytelling scenarios. In this work, we aim to achieve a wide range of consistency between various components from a procedural approach.

### 2.4 Generative digital storytelling with LLM agents

Prior works have explored LLM-assisted generation frameworks covering several components such as text[[28](https://arxiv.org/html/2406.10478v2#bib.bib28), [29](https://arxiv.org/html/2406.10478v2#bib.bib29)], audio[[8](https://arxiv.org/html/2406.10478v2#bib.bib8)], and visual[[32](https://arxiv.org/html/2406.10478v2#bib.bib32)], where LLMs have shown their capabilities in assisting single-modal asset generation in the following aspects:

Firstly, they can extract information from natural language descriptions and convert it into formatted parameters [[49](https://arxiv.org/html/2406.10478v2#bib.bib49), [32](https://arxiv.org/html/2406.10478v2#bib.bib32)]. Leveraging their world knowledge, LLMs can also break down complicated concepts into several simpler components [[8](https://arxiv.org/html/2406.10478v2#bib.bib8)], which lowers the difficulties for downstream generative models. In addition, the organization of components can be stored and updated in explicit formats [[50](https://arxiv.org/html/2406.10478v2#bib.bib50)]. Finally, with reasoning capabilities [[51](https://arxiv.org/html/2406.10478v2#bib.bib51), [52](https://arxiv.org/html/2406.10478v2#bib.bib52)], LLMs can plan for the generation tasks with predefined toolsets and real-time feedback. But prompt-based narrative scene generation tools like [[32](https://arxiv.org/html/2406.10478v2#bib.bib32)] are limited in the ability to generate multiple coherent and compelling narrative scenes in sequence.

LLM agent systems are widely used in complex generation tasks such as game world narratives [[53](https://arxiv.org/html/2406.10478v2#bib.bib53)] and computer programs [[54](https://arxiv.org/html/2406.10478v2#bib.bib54)]. In these cases, the consistencies are achieved by predefined hierarchical memories and reflection procedures. In this paper, we apply an LLM agent system to ensure consistency in both temporal and modal dimensions.

## 3 Method

![Refer to caption](img/a9b533e9f62ea516b615e1a31be2ff72.png)

Figure 2: The framework of StoryAgent. Beginning with a text instruction, the framework builds the story with task decomposition, specifying all asset files for each modality in the textual description. Then generative models and tools are organized to create and compose tangible assets of the story.

The entire pipeline (see Fig. [2](https://arxiv.org/html/2406.10478v2#S3.F2 "Figure 2 ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")) begins with a text instruction, then develops an intricate storyline by an LLM-agent-based story cluster (see [3.1](https://arxiv.org/html/2406.10478v2#S3.SS1 "3.1 Story cluster ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")) with scene understanding capabilities (see [3.2](https://arxiv.org/html/2406.10478v2#S3.SS2 "3.2 Image-based scene understanding and interaction ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")). The story cluster deconstructs the digital storytelling task into multiple subtasks, each targeting a specific modality. For each modality, the story cluster specifies all output asset files of the story through the textual descriptions. Based on these descriptions, generative models and tools are organized into asset generation teams (see [3.3](https://arxiv.org/html/2406.10478v2#S3.SS3 "3.3 Asset generation teams ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")) to create tangible assets of the story. Finally, approaches for potential interventions are introduced in [3.4](https://arxiv.org/html/2406.10478v2#S3.SS4 "3.4 Intervention ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent").

### 3.1 Story cluster

The story cluster drafts the storyline through a network of LLM agent teams. Inspired by the pipeline workflow in the film industry, the agent teams handle story arcs, characters, settings, story beats, setting affordances, story scenes, and the screenplay respectively.

The network begins with a story arc team, which composes a blueprint of the narrative in text, instructing the entire production at the highest level. Then, the blueprint is distributed to numerous downstream teams to handle different narrative components such as character settings, settings, scenes, screenplays, and so on. The intermediate products that are circulated between teams are always in text. Although the cluster is built upon AutoGen[[27](https://arxiv.org/html/2406.10478v2#bib.bib27)], the circulation follows a predefined procedure (see Fig. [1](https://arxiv.org/html/2406.10478v2#S0.F1 "Figure 1 ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")) to ensure generation stability.

![Refer to caption](img/048bf0548cbed0a6fe85d7f2f71ca735.png)

Figure 3: Structure of a two-stage LLM agent team. It takes upstream JSON as inputs and uses two expert-critic LLM agent pairs to process. Finally, another JSON string will be generated for downstream teams.

Based on upstream content, each team’s generated outcome is incrementally optimized through single-stage or multi-stage dialogues (see Fig.[3](https://arxiv.org/html/2406.10478v2#S3.F3 "Figure 3 ‣ 3.1 Story cluster ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")). All multi-stage configurations are manually designed to further decompose the generation process, as some agent teams require complex reflections that the current LLM backbone cannot handle. In each stage, the dialogue takes place between two LLM agents. Inspired by the role-playing method in [[26](https://arxiv.org/html/2406.10478v2#bib.bib26)], each agent is initialized with a system prompt, where dialogue history and upstream content are attached to an agent-specific template.

Among the two, one “expert agent” is responsible for creating component specifications based on predefined requirements and upstream inputs. The other “critic agent”, evaluates and scores them against predefined criteria. The roles of the agents, their constraints, and the output formats are shared between the expert and the critic within individual team stages but differ across different agent teams and stages. For comprehensive prompts for all agents, please refer to the appendix.

Starting from the expert agent, the two respond to each others’ outputs in a round-robin way, where the circulation ends when the critic is satisfied with the expert’s outcome.

Utilizing GPT-4 as the LLM agent backbone, this expert-critic design facilitates a robust capacity for self-correction, which ensures consistency between the upstream commands and downstream outcomes. To illustrate, consider the following example where the critic agent helps the expert agent refine a character profile:

Admin

<svg class="ltx_picture" height="24.26" id="S3.SS1.p8.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">(provide a story arc in JSON format)</foreignobject></g></g></svg>

Expert in Character

<svg class="ltx_picture" height="24.26" id="S3.SS1.p10.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">{”name”: ”Village elders”…}</foreignobject></g></g></svg>

Critic in Character

<svg class="ltx_picture" height="66.99" id="S3.SS1.p12.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,66.99) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 29.13)"><foreignobject color="#000000" height="55.18" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">…The character name ”Village elders” is plural and should be singular…The expert should break down ”Village elders” into individual characters with singular names. For example, they could create characters for a few prominent elders and villagers.</foreignobject></g></g></svg>

Expert in Character

<svg class="ltx_picture" height="24.26" id="S3.SS1.p14.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">{”name”: ”Elder John”…},{”name”: ”Elder Mary”…}…</foreignobject></g></g></svg>

Critic in Character

<svg class="ltx_picture" height="24.26" id="S3.SS1.p16.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">(pass the check)</foreignobject></g></g></svg>

### 3.2 Image-based scene understanding and interaction

Simply putting the characters in the foreground and the story setting images in the background could result in unreasonable arrangements like a character standing on the water or up in the air. To enhance the fidelity of each scene and give audiences a better immersion in the story, the pipeline conducts several measures: first fusing image semantic segmentation and depth estimation explicitly and implicitly as the visuospatial information and then letting setting affordance agents to enable foreground characters to interact with background images, which would otherwise be disjoint. The visuospatial information could also benefit cinematography by providing the object’s estimated position and distance to the camera.

#### 3.2.1 Scene understanding

The scene understanding process is structured in three key stages, designed to equip setting agents with comprehensive knowledge of the narrative environment, enabling them to provide relevant affordances for dynamic story interaction.

Initially, story setting agents construct a hierarchical graph of all narrative settings. Each node within this graph represents a distinct setting and includes essential details like the setting’s name, visual prompts, and its relationship with other settings (parent and children). From these visual prompts, background images are generated using [[5](https://arxiv.org/html/2406.10478v2#bib.bib5)], forming the visual foundation of the story’s environments.

The generated images undergo semantic segmentation to identify and categorize objects within each setting. This segmentation ([[38](https://arxiv.org/html/2406.10478v2#bib.bib38)]), paired with depth estimation ([[55](https://arxiv.org/html/2406.10478v2#bib.bib55)]), provides a three-dimensional understanding of each scene. Objects are encapsulated within bounding boxes, which highlight their spatial position and median depth, aiding in the precise placement within the narrative space. This crucial spatial data informs how characters and cameras will interact with these objects, ensuring accurate and realistic scene compositions(see [3.2.2](https://arxiv.org/html/2406.10478v2#S3.SS2.SSS2 "3.2.2 Character and camera interaction ‣ 3.2 Image-based scene understanding and interaction ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")).

![Refer to caption](img/5c91a13f8e35473ae520f48c647ca3b1.png)

Figure 4: Semantic scene understanding example

In the final stage, setting affordance agents utilize the detailed object and spatial data (see appendix [6.1.5](https://arxiv.org/html/2406.10478v2#S6.SS1.SSS5.Px2 "Stage 2 ‣ 6.1.5 Setting affordance team ‣ 6.1 Prompts in story cluster ‣ 6 Appendix ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")) to create a rich layer of interaction possibilities through affordances. These affordances are meticulously documented with object relations, narrative relevance, and evidence of the object’s existence in the image. This structural information is then passed to story scene agents, who use it to script interactions and narrative events, ensuring that characters can interact naturally with their environment.

#### 3.2.2 Character and camera interaction

Our pipeline processes and utilizes object location information to enhance storytelling through precise cinematography. Once this data is captured, screenplay agents integrate it to orchestrate scene dynamics effectively. This involves using the object centers as focal points in animation and cinematography—characters interact with key objects via targeted movements, and cameras adjust focus and framing based on the object’s position and estimated distance. This approach allows for the strategic selection of shot types (close, medium, or wide) to best capture the narrative moment.

### 3.3 Asset generation teams

Each agent generation team represents a hybrid of LLM agents and generative models. Generally, the LLM agents here serve as the bridge between the upstream textual descriptions and the downstream generative models and tools. Specifically, LLM agents aim to convert specifications articulated in natural language into model parameters and reflect on feedback from generative models and tools.

In our study, we employ the rigging framework, CC2D[[56](https://arxiv.org/html/2406.10478v2#bib.bib56)], to construct and animate human avatars. To generate image assets for different parts of the character, such as pants and clothing, we input appearance descriptions into a text-to-image generative model [[5](https://arxiv.org/html/2406.10478v2#bib.bib5)]. These assets are produced using a supplemental prompt—such as “detailed, cartoon, 8k”—to enhance the initial description. Since diffusion-based generative models are hard to generate meaningful 2D textures directly, the generative procedure goes through a composite and decompose process. First We composite the 2D texture of the tunic and pants into a meaningful shape of the asset (see Fig. [5](https://arxiv.org/html/2406.10478v2#S3.F5 "Figure 5 ‣ 3.3 Asset generation teams ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")), then generate assets using its assembled mask. After the generation is done, we decompose the whole assets back with their texture mask and then integrate them within the CC2D framework (see Fig. [6](https://arxiv.org/html/2406.10478v2#S3.F6 "Figure 6 ‣ 3.3 Asset generation teams ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")). Textures are loaded during runtime (see Fig. [7](https://arxiv.org/html/2406.10478v2#S3.F7 "Figure 7 ‣ 3.3 Asset generation teams ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")).

![Refer to caption](img/f30f72e4d1dec7a6b374027bec32306d.png)  ![Refer to caption](img/58d1eebd8ef1167e5e1d1a110736541d.png)  ![Refer to caption](img/8c717046926d1656972e8f8a403ea79a.png)

Figure 5: Character pants asset examples

![Refer to caption](img/03da578dd1a41009580d42519f2a1afb.png)  ![Refer to caption](img/97ab24764fd02a071237ca19bc932fcc.png)

Figure 6: Character parts mask. Left is the original texture format and right is the assembled mask for better generation quality

![Refer to caption](img/99cf1172526b84c8522ef2fc589c26fc.png)

Figure 7: Pants asset in the runtime

For background image creation, we utilize the same generative model to ensure style consistency. We use [[57](https://arxiv.org/html/2406.10478v2#bib.bib57)] to lift the token limitation and generate backgrounds by visual prompts provided by the setting configuration agents. Furthermore, these images undergo further processing in a scene understanding cluster (see [3.2](https://arxiv.org/html/2406.10478v2#S3.SS2 "3.2 Image-based scene understanding and interaction ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")) to enable interactivity. This additional step allows for dynamic interaction within the generated environments (detailed in [3.2](https://arxiv.org/html/2406.10478v2#S3.SS2 "3.2 Image-based scene understanding and interaction ‣ 3 Method ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")).

In speech generation, a character description simplified by LLMs is firstly fed to AudioGen[[10](https://arxiv.org/html/2406.10478v2#bib.bib10)] to create a voice fingerprint, containing the voice feature of that character. Then it is smoothed by XTTS [[58](https://arxiv.org/html/2406.10478v2#bib.bib58)], producing a reference speech audio. Next, the reference audio, speech in text, and the emotion state in text are fed to ElevenLabs API [[59](https://arxiv.org/html/2406.10478v2#bib.bib59)] to create the final speech audio.

Inspired from [[8](https://arxiv.org/html/2406.10478v2#bib.bib8)], the sound description is firstly decomposed to components by LLMs, where different decomposition methods are tailored for sound effect types (looping or one-time). Then, for each component, an LLM agent is applied to search corresponding assets on FreeSound [[60](https://arxiv.org/html/2406.10478v2#bib.bib60)], where the assets are evaluated by feeding their descriptions to the agent. The search is repeatedly triggered with keywords that are optimized by the agent using previous search results. If there are no suitable assets, AudioGen[[10](https://arxiv.org/html/2406.10478v2#bib.bib10)] is called to create one. All music clips are created by MusicGen [[10](https://arxiv.org/html/2406.10478v2#bib.bib10)], a text-to-music generative model.

Consistency is optimized by communication between LLM agents and other generative models and tools. For instance, in sound retrieval, after the initial description is broken down into simpler components, each component’s description is transformed into a query string, which is iteratively refined based on the results of previous searches. This method underscores the dynamic use of LLMs to enhance the consistency between the text description and the retrieved assets.

Admin

<svg class="ltx_picture" height="36.73" id="S3.SS3.p8.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,36.73) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 14.01)"><foreignobject color="#000000" height="24.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">The sound of townsfolk chatting, children playing, and the gentle splash of the fountain water.</foreignobject></g></g></svg>

Decomposition Agent

<svg class="ltx_picture" height="74.08" id="S3.SS3.p10.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,74.08) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 32.68)"><foreignobject color="#000000" height="62.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">{ ”name”: ”townsfolk chatting”, ”desc”: ”the sound of townsfolk chatting in a town square”, ”volume”: -25 }, { ”name”: ”children playing”, ”desc”: ”the sound of children playing in a town square”, ”volume”: -20 }, { ”name”: ”fountain water”, ”desc”: ”the gentle splash of fountain water in a town square”, ”volume”: -30 }</foreignobject></g></g></svg>

Decomposition Agent

<svg class="ltx_picture" height="22.88" id="S3.SS3.p12.pic1" overflow="visible" version="1.1" width="473.84"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,22.88) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.08)"><foreignobject color="#000000" height="11.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="454.15">@Query Agent the gentle splash of fountain water in a town square</foreignobject></g></g></svg>

Query Agent

<svg class="ltx_picture" height="22.88" id="S3.SS3.p14.pic1" overflow="visible" version="1.1" width="473.84"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,22.88) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.08)"><foreignobject color="#000000" height="11.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="454.15">@Search API water +fountain +splash +gentle -ocean -river</foreignobject></g></g></svg>

Search API

<svg class="ltx_picture" height="24.26" id="S3.SS3.p16.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">(a list of sound with descriptions)</foreignobject></g></g></svg>

Query Agent

<svg class="ltx_picture" height="22.88" id="S3.SS3.p18.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,22.88) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.08)"><foreignobject color="#000000" height="11.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">water +fountain +gentle +town -ocean -river -greenway</foreignobject></g></g></svg>

Search API

<svg class="ltx_picture" height="24.26" id="S3.SS3.p20.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">(a list of sound with descriptions)</foreignobject></g></g></svg>

Query Agent

<svg class="ltx_picture" height="22.88" id="S3.SS3.p22.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,22.88) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.08)"><foreignobject color="#000000" height="11.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">fountain +gentle +town</foreignobject></g></g></svg>

Search API

<svg class="ltx_picture" height="24.26" id="S3.SS3.p24.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">(a list of sound with descriptions)</foreignobject></g></g></svg>

Query Agent

<svg class="ltx_picture" height="24.26" id="S3.SS3.p26.pic1" overflow="visible" version="1.1" width="469.68"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,24.26) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 7.77)"><foreignobject color="#000000" height="12.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="450">(The index of the best sound asset)</foreignobject></g></g></svg>

### 3.4 Intervention

While our framework facilitates a hands-free generation process, it also offers flexible intervention approaches for human creators. The framework allows for the following intervention approach:

*   •

    Full regeneration from the beginning: by feeding prior generation results and extra instructions to the framework inputs, the framework updates all contents with maximum consistency ensured. We apply this approach in Section [4.3](https://arxiv.org/html/2406.10478v2#S4.SS3 "4.3 Agent-wise human intervention ‣ 4 Storytelling Experience ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent"). Please refer to the instruction prompt in the appendix.

*   •

    Intermediate the process: by fixing upstream results and altering agent system prompt, the framework updates all downstream contents with finer control over selected modalities.

*   •

    Replacement: by replacing generated results with human-crafted ones, human creators get their maximum control over the outcome. However, replacing downstream assets may cause inconsistency issues since the framework cannot reflect on them.

![Refer to caption](img/f7e0c923b546ffd8612e9daf7bd46dc9.png)

(a) A curious boy named Tim and his adventurous friend, Sam, decide to explore the woods near their homes, despite the rumors of it being haunted.

![Refer to caption](img/85844a0b0ed4001d14a9c55d8dec7cce.png)

(b) As they venture deeper into the woods, they find cryptic messages carved into the trees. They decide to decipher these, believing they might lead to a treasure.

![Refer to caption](img/35ac43a7d20b02539dae3de24d55e3a4.png)

(c) They finally decipher the messages that lead them to a specific tree. They find an old, weathered box buried at the base of the tree.

![Refer to caption](img/04f33a67452e345685a2d57551ec4581.png)

(d) They open the box to find a map of their town from decades ago, with a path marked leading back to their homes.

![Refer to caption](img/5660c3184f2e9fab4415899b84be31c6.png)

(e) They follow the path on the map and realize that the treasure was the journey and the memories they made. They return to Sam’s home with friendship strengthened.

Figure 8: Screenshots of a story generation, with one frame selected from each stage of a five-stage storytelling arc.

## 4 Storytelling Experience

In this section, we present the qualitative results of our pipeline, highlighting three benefits of our text-based pipeline. We first demonstrate how the reasoning capabilities of StoryAgent facilitate consistency between plots and downstream components. Then we illustrate how the coordination of various agents and generative tools ensures scene interactivity in 2D art styles. Finally, we show its capability to adapt human intervention during the generation process, thus providing story alternatives for human creators. Here is the story arc for our baseline story:

In the story, Tim and his friend Sam explore the woods near their homes, discovering cryptic messages on trees that they believe might lead to treasure. Following the clues, they uncover an old box containing an antique map of their town. The map guides them back home, revealing that the real treasure was the adventure and the bond they strengthened along the way.

### 4.1 Story consistency

Our framework keeps a hierarchical organization of all asset files, which enables asset re-usages. Consequently, this structure ensures appearance consistency across all visual elements, both within neighboring frames and across different scenes (see Fig.[9](https://arxiv.org/html/2406.10478v2#S4.F9 "Figure 9 ‣ 4.1 Story consistency ‣ 4 Storytelling Experience ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent")).

![Refer to caption](img/f2435f6076615ce9a13ce4f50df0e0f6.png)  ![Refer to caption](img/51a8cbea1a446de347d7102708d05597.png)  ![Refer to caption](img/5660c3184f2e9fab4415899b84be31c6.png)

Figure 9: Character consistency across scenes. Based on the textual descriptions of the generative narrative, the character assets are reused across scenes, ensuring visual consistency.

Additionally, our framework ensures consistency across modalities, as visual and audio elements together create an emotive storytelling experience. In this manuscript, the generated auditory descriptions of scenes are paired with corresponding visualizations in Fig.[10](https://arxiv.org/html/2406.10478v2#S4.F10 "Figure 10 ‣ 4.1 Story consistency ‣ 4 Storytelling Experience ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent"), illustrating how our framework maintains alignment between visual and audio styles.

![Refer to caption](img/fcd7151c5c0e7a28451f98b5952298f9.png)   <svg class="ltx_picture" height="71.84" id="S4.F10.2.2.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,71.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 13.78)"><foreignobject color="#000000" height="44.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.32">[Visual] A small room with a single bed, a wooden desk, a bookshelf filled with books, a window overlooking the yard, and a blue rug. [Audio] Background sounds of a quiet suburban neighborhood, with distant sounds of children playing and birds chirping.</foreignobject></g></g></svg>   ![Refer to caption](img/83a15abbc79443420ab1b99f152592fc.png)  <svg class="ltx_picture" height="71.84" id="S4.F10.4.4.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,71.84) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.84 13.78)"><foreignobject color="#000000" height="44.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.32">[Visual] A dense forest with towering trees, a carpet of fallen leaves, a narrow trail, bird nests in branches, and a quiet stream. [Audio] Background sounds of rustling leaves, chirping birds, and distant animal sounds create an atmosphere of being deep in the woods.</foreignobject></g></g></svg> 

Figure 10: Consistency across modalities. Our framework ensure the coherence between visual and audio elements by aligning their text descriptions.

### 4.2 Scene interactivity

StoryAgent can integrate detailed scene information into digital storytelling, creating scene interactivity. We focus on two common practices in digital storytelling for children, effectively engaging young audiences by emphasizing narrative elements.

Firstly, the interactions take place at the semantic level. When the story unfolds, characters will refer to the scene objects in their dialogue, maintaining the story’s coherence at the same time. In Fig.[11](https://arxiv.org/html/2406.10478v2#S4.F11 "Figure 11 ‣ 4.2 Scene interactivity ‣ 4 Storytelling Experience ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent"), the characters are talking about their childhood memories, while raising a fence in the background as a reference.

![Refer to caption](img/d39ecdd3bdd4e38a49d1aae6b4532f0c.png)  ![Refer to caption](img/7862ad181d17366a47c71aef1c456fad.png)

Figure 11: Character dialogue with scene understanding. (left) ”Look at our house Sam. It is not the treasure we expected, but it’s a symbol of our adventure today” (right) ”You are right Tim. And remember the times we used to sit on that fence, dreaming about finding treasure?”

The interactivity is also enhanced by the synchronous interplay between the dialogue, character gestures, and cinematography. As characters discuss an object within the scene, not only does the camera shift focus to and zoom in on the object for detailed visualization, but the characters also direct their gazes and gestures toward it. This coordination makes digital storytelling more accessible and engaging for children, as detailed in Fig. [12](https://arxiv.org/html/2406.10478v2#S4.F12 "Figure 12 ‣ 4.2 Scene interactivity ‣ 4 Storytelling Experience ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent").

![Refer to caption](img/c2e8476b2a685ac0e1f578fc7ec3e18c.png)  ![Refer to caption](img/f52b5690eba82ab0bf0ee6ea5b837f66.png)

Figure 12: Camera shoot and character gesture with scene understanding. (left) The camera shifts to focus on the trees while simultaneously, the character points at them as he discusses them. (right) The camera zooms in on the character’s face while the narrator describes his eyes.

 Base Story Settings Intervened Character and Settings Intervened Event 1: Decipher the code ![Refer to caption](img/85844a0b0ed4001d14a9c55d8dec7cce.png) ![Refer to caption](img/853f2a407229a00f5329954bc293630e.png) ![Refer to caption](img/be4470750030db8e7a7b0646d407ac91.png) These carvings..they might be a code…   Look at that graffiti, Sam. It’s so cryptic!  Guys, look at these messages. They are like a puzzle…    Event 2: Navigate back to origin  ![Refer to caption](img/ee7422ac6be292a78e2c9255ba131066.png) ![Refer to caption](img/c16bcc95b6e15869c789468651714c02.png) ![Refer to caption](img/2a34be837df491d1b1bab295cca0c71d.png) Tim and Sam…begin their journey back through the woods   Tim and Sam…navigate through the bustling cityscape  They continue their journey…    Event 3: Treasure is the adventure  ![Refer to caption](img/101e634a9e7879cfdab1ad9650c4a5c0.png) ![Refer to caption](img/42c6fd0e6f490e6de044a7c704fd9044.png) ![Refer to caption](img/f6fe68ccbdac31db29e52e4f58d35384.png)    Indeed, the real treasure was the journey…   And the adventure, Sam. The adventure is a treasure, too.  …And the treasure was the memories we made. 

Figure 13: Story outcomes (screenshots and speech) with agent-wise intervention. The original base story is shown in the first column, while a variant on world settings is shown in the second. The last column is the variant where both world settings and character settings are intervened. The results show a strong complaint to the original plot, as all outcomes show the same key events (shown in rows). However, the diversity of the intervention modality is ensured at the same time.

### 4.3 Agent-wise human intervention

Our framework’s coarse-to-fine approach to generating digital narratives lends itself to flexible human intervention because all agents that are affected by the intervention are strictly downstream. Below, we demonstrate this flexibility by altering a base story generation outcome at various locations within the pipeline, each corresponding to a different modality in the storytelling process.

First, our framework demonstrates its capability to adapt writing styles while preserving the story’s outline. Here we directly instruct a downstream agent team to compose the dialogue in the screenplay as rhyming couplets. This allowed us to re-use the upstream content of the base story for an easier comparison.

Base Story Script:

<svg class="ltx_picture" height="88.44" id="S4.SS3.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,88.44) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="60.88" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">NARRATOR: In a quiet neighborhood, we meet Tim, a curious boy with a thirst for adventure. TIM: I’ve heard the rumors about the woods, but I can’t help but want to explore it. NARRATOR: Despite the rumors, Tim’s curiosity is not deterred. TIM: No matter how far I go, I know this house will always be my safe haven.</foreignobject></g></g></svg>

Screenplay-Intervened Story Script:

<svg class="ltx_picture" height="102.62" id="S4.SS3.p6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,102.62) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="75.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">NARRATOR: In a cozy home, lived a boy named Tim, his spirit was adventurous, his curiosity never dim. TIM: I’ve heard of the woods, so haunted and deep, I must explore, while the world’s asleep. NARRATOR: With a heart full of courage, and a mind full of wonder, into the woods, Tim decided to wander. TIM: No matter how far, into the woods I delve, this house will always be my safe haven, my safe shelf.</foreignobject></g></g></svg>

Next, we demonstrate more complex interventions with larger-scale impacts on the narrative generation process (i.e., downstream agents). This was achieved by using the story arc of the base story as input in the framework with an additional instruction to preserve some elements of the story while changing others. Unlike the screenplay intervention, where character appearances were directly copied, this intervention fully regenerated the characters, resulting in different-colored clothing.

Figure [9](https://arxiv.org/html/2406.10478v2#S4.F9 "Figure 9 ‣ 4.1 Story consistency ‣ 4 Storytelling Experience ‣ From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent") shows two such interventions: one where the characters and plot are fixed, but the setting is changed (S-intervention), and another where the plot is fixed, but the characters and setting are changed (CS-intervention). Between both interventions, we observe that the cryptic carvings in the forest of the base story is reflected in the cryptic graffiti in the S-intervention and the cryptic etchings in the CS-intervention (Event 1). Each image corresponding to Event 1 also shows that the story is making use of affordances to have characters look and point at the cryptic imagery. In Events 2 and 3, we see that each story’s plot leads the characters on a circuitous adventure back to where they started and they share the same moral of appreciating the adventure. In the CS-intervention, the dialogue that was originally distributed between two characters has been distributed among four characters, meaning that the characters have not only been added but are being fully utilized. Unedited videos of the original story and the three interventions can be found in the Supplementary Materials.

## 5 Discussions

In this paper, we introduce the ”StoryAgent” framework, which utilizes text as a central medium to organize existing generative models to produce long-duration digital storytelling. While the state-of-the-art prior works excel in the quality of short animation clips, our work is designed to address two principal hurdles in storytelling: long-duration alignment between modalities and scene interactivity.

In addition, we argue such a framework offers more flexibility in real-world digital storytelling production: First, the rapid emergence of new generative models and tools for various modalities necessitates their swift applications into a larger pipeline at minimal cost. Our model-independent framework facilitates a plug-and-play approach, allowing the latest research advancements to be easily incorporated. This ensures that the performance of our framework continuously benefits from ongoing research developments. Second, by replacing generative assets with human-crafted ones, our framework addresses the specific limitations faced by indie developers in certain modalities. For example, graphic artists can use our pipeline to integrate music into their digital stories, whereas musicians can employ it to generate visual representations of their audio works. Finally, the text-based nature of our framework permits human artists to readily adjust intermediate products, thereby enhancing the controllability of the production process.

However, we identify several limitations in our study as follows: Firstly, while our approach has achieved broad coverage of story components, the narrative presentation—encompassing duration, order, and style of storytelling (collectively referred to as ’discourse’ in [[17](https://arxiv.org/html/2406.10478v2#bib.bib17)])—is only minimally explored. Secondly, we acknowledge that the current generation results are constrained by the performance and capabilities of state-of-the-art singular-modality models. This limitation affects the alignment between the plots and the downstream components, potentially impacting the overall coherence and immersion of the generated results. Lastly, our experimentation with scene interactivity is limited to a small number of scenarios. Future research should explore more complex interaction modes, such as enabling characters to pick up items or alter the status of objects within the environment, to enhance the interactive experience in 2D art styles.

## References

*   De Jager et al. [2017] Adele De Jager, Andrea Fogarty, Anna Tewson, Caroline Lenette, and Katherine M Boydell. Digital storytelling in research: A systematic review. *The Qualitative Report*, 22(10):2548–2582, 2017.
*   Lambert [2013] Joe Lambert. *Digital storytelling: Capturing lives, creating community*. Routledge, 2013.
*   Wu and Chen [2020] Jing Wu and Der-Thanq Victor Chen. A systematic review of educational digital storytelling. *Computers & Education*, 147:103786, 2020. ISSN 0360-1315. doi: https://doi.org/10.1016/j.compedu.2019.103786. URL [https://www.sciencedirect.com/science/article/pii/S0360131519303367](https://www.sciencedirect.com/science/article/pii/S0360131519303367).
*   Betker et al. [2023] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions, 2023. URL [https://cdn.openai.com/papers/dall-e-3.pdf](https://cdn.openai.com/papers/dall-e-3.pdf).
*   Li et al. [2024] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.
*   Esser et al. [2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024.
*   Ho et al. [2022] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models, 2022.
*   Liu et al. [2023] Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, and Wenwu Wang. Wavjourney: Compositional audio creation with large language models, 2023.
*   Brooks et al. [2024] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL [https://openai.com/research/video-generation-models-as-world-simulators](https://openai.com/research/video-generation-models-as-world-simulators).
*   Kreuk et al. [2023] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation, 2023.
*   Cavazza et al. [2007] Marc Cavazza, Jean-Luc Lugrin, David Pizzi, and Fred Charles. Madame bovary on the holodeck: immersive interactive storytelling. In *Proceedings of the 15th ACM International Conference on Multimedia*, MM ’07, page 651–660, New York, NY, USA, 2007\. Association for Computing Machinery. ISBN 9781595937025. doi: 10.1145/1291233.1291387. URL [https://doi.org/10.1145/1291233.1291387](https://doi.org/10.1145/1291233.1291387).
*   Kapadia et al. [2016a] Mubbasir Kapadia, Seth Frey, Alexander Shoulson, Robert W. Sumner, and Markus Gross. Canvas: computer-assisted narrative animation synthesis. In *Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation*, SCA ’16, page 199–209, Goslar, DEU, 2016a. Eurographics Association. ISBN 9783905674613.
*   Louarn et al. [2018] Amaury Louarn, Marc Christie, and Fabrice Lamarche. Automated staging for virtual cinematography. In *Proceedings of the 11th ACM SIGGRAPH Conference on Motion, Interaction and Games*, MIG ’18, New York, NY, USA, 2018\. Association for Computing Machinery. ISBN 9781450360159. doi: 10.1145/3274247.3274500. URL [https://doi.org/10.1145/3274247.3274500](https://doi.org/10.1145/3274247.3274500).
*   Marti et al. [2018] Marcel Marti, Jodok Vieli, Wojciech Witoń, Rushit Sanghrajka, Daniel Inversini, Diana Wotruba, Isabel Simo, Sasha Schriber, Mubbasir Kapadia, and Markus Gross. Cardinal: Computer assisted authoring of movie scripts. In *Proceedings of the 23rd International Conference on Intelligent User Interfaces*, IUI ’18, page 509–519, New York, NY, USA, 2018\. Association for Computing Machinery. ISBN 9781450349451. doi: 10.1145/3172944.3172972. URL [https://doi.org/10.1145/3172944.3172972](https://doi.org/10.1145/3172944.3172972).
*   Kapadia et al. [2016b] Mubbasir Kapadia, Alexander Shoulson, Cyril Steimer, Samuel Oberholzer, Robert W. Sumner, and Markus Gross. An event-centric approach to authoring stories in crowds. In *Proceedings of the 9th International Conference on Motion in Games*, MIG ’16, page 15–24, New York, NY, USA, 2016b. Association for Computing Machinery. ISBN 9781450345927. doi: 10.1145/2994258.2994265. URL [https://doi.org/10.1145/2994258.2994265](https://doi.org/10.1145/2994258.2994265).
*   Yi et al. [2024] Hongwei Yi, Justus Thies, Michael J. Black, Xue Bin Peng, and Davis Rempe. Generating human interaction motions in scenes with text control, 2024.
*   Kybartas and Bidarra [2017] Ben Kybartas and Rafael Bidarra. A survey on story generation techniques for authoring computational narratives. *IEEE Transactions on Computational Intelligence and AI in Games*, 9(3):239–253, 2017. doi: 10.1109/TCIAIG.2016.2546063.
*   Chang et al. [2023] Che-Jui Chang, Samuel S Sohn, Sen Zhang, Rajath Jayashankar, Muhammad Usman, and Mubbasir Kapadia. The importance of multimodal emotion conditioning and affect consistency for embodied conversational agents. In *Proceedings of the 28th International Conference on Intelligent User Interfaces*, pages 790–801, 2023.
*   Chang et al. [2022a] Che-Jui Chang, Long Zhao, Sen Zhang, and Mubbasir Kapadia. Disentangling audio content and emotion with adaptive instance normalization for expressive facial animation synthesis. *Computer Animation and Virtual Worlds*, 33(3-4):e2076, 2022a.
*   Chang et al. [2022b] Che-Jui Chang, Sen Zhang, and Mubbasir Kapadia. The ivi lab entry to the genea challenge 2022–a tacotron2 based method for co-speech gesture generation with locality-constraint attention mechanism. In *Proceedings of the 2022 International Conference on Multimodal Interaction*, pages 784–789, 2022b.
*   Salselas and Penha [2019] Inês Salselas and Rui Penha. The role of sound in inducing storytelling in immersive environments. In *Proceedings of the 14th International Audio Mostly Conference: A Journey in Sound*, AM ’19, page 191–198, New York, NY, USA, 2019\. Association for Computing Machinery. ISBN 9781450372978. doi: 10.1145/3356590.3356619. URL [https://doi.org/10.1145/3356590.3356619](https://doi.org/10.1145/3356590.3356619).
*   Cummings and Bailenson [2016] James J. Cummings and Jeremy N. Bailenson. How immersive is enough? a meta-analysis of the effect of immersive technology on user presence. *Media Psychology*, 19(2):272–309, 2016. doi: 10.1080/15213269.2015.1015740. URL [https://doi.org/10.1080/15213269.2015.1015740](https://doi.org/10.1080/15213269.2015.1015740).
*   Feng et al. [2023] Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, Aojie Li, Xiaoyang Kang, Biwen Lei, Miaomiao Cui, Peiran Ren, and Xuansong Xie. Dreamoving: A human video generation framework based on diffusion models, 2023.
*   Guo et al. [2023] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, 2023.
*   Liew et al. [2023] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity and temporally coherent video editing, 2023.
*   Li et al. [2023] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for ”mind” exploration of large language model society. In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023.
*   Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.
*   Ammanabrolu et al. [2020] Prithviraj Ammanabrolu, Ethan Tien, Wesley Cheung, Zhaochen Luo, William Ma, Lara J. Martin, and Mark O. Riedl. Story realization: Expanding plot events into sentences. *Proceedings of the AAAI Conference on Artificial Intelligence*, 34(05):7375–7382, April 2020. ISSN 2159-5399. doi: 10.1609/aaai.v34i05.6232. URL [http://dx.doi.org/10.1609/aaai.v34i05.6232](http://dx.doi.org/10.1609/aaai.v34i05.6232).
*   Balint and Bidarra [2023] J. Timothy Balint and Rafael Bidarra. Procedural generation of narrative worlds. *IEEE Transactions on Games*, 15(2):262–272, 2023. ISSN 2475-1502. doi: 10.1109/TG.2022.3216582. Green Open Access added to TU Delft Institutional Repository ‘You share, we take care!’ – Taverne project https://www.openaccess.nl/en/you-share-we-take-care Otherwise as indicated in the copyright section: the publisher is the copyright holder of this work and the author uses the Dutch legislation to make this work public.
*   Hartsook et al. [2011] Ken Hartsook, Alexander Zook, Sauvik Das, and Mark O. Riedl. Toward supporting stories with procedurally generated game worlds. In *2011 IEEE Conference on Computational Intelligence and Games (CIG’11)*, pages 297–304, 2011. doi: 10.1109/CIG.2011.6032020.
*   Zhang et al. [2021] Jia-Qi Zhang, Xiang Xu, Zhi-Meng Shen, Ze-Huan Huang, Yang Zhao, Yan-Pei Cao, Pengfei Wan, and Miao Wang. Write-an-animation: High-level text-based animation editing with character-scene interaction. *Computer Graphics Forum*, 40(7):217–228, 2021. doi: https://doi.org/10.1111/cgf.14415.
*   Kumaran et al. [2023] Vikram Kumaran, Jonathan Rowe, Bradford Mott, and James Lester. Scenecraft: Automating interactive narrative scene generation in digital games with large language models. *Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment*, 19(1):86–96, Oct. 2023. doi: 10.1609/aiide.v19i1.27504. URL [https://ojs.aaai.org/index.php/AIIDE/article/view/27504](https://ojs.aaai.org/index.php/AIIDE/article/view/27504).
*   Chang et al. [2024a] Che-Jui Chang, Danrui Li, Deep Patel, Parth Goel, Honglu Zhou, Seonghyeon Moon, Samuel S Sohn, Sejong Yoon, Vladimir Pavlovic, and Mubbasir Kapadia. Learning from synthetic human group activities. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 21922–21932, 2024a.
*   Chang et al. [2024b] Che-Jui Chang, Danrui Li, Seonghyeon Moon, and Mubbasir Kapadia. On the equivalency, substitutability, and flexibility of synthetic data. *arXiv preprint arXiv:2403.16244*, 2024b.
*   Xu et al. [2023] Pei Xu, Kaixiang Xie, Sheldon Andrews, Paul G. Kry, Michael Neff, Morgan Mcguire, Ioannis Karamouzas, and Victor Zordan. Adaptnet: Policy adaptation for physics-based character control. *ACM Trans. Graph.*, 42(6), dec 2023. ISSN 0730-0301. doi: 10.1145/3618375. URL [https://doi.org/10.1145/3618375](https://doi.org/10.1145/3618375).
*   Starke et al. [2019] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for character-scene interactions. *ACM Trans. Graph.*, 38(6), nov 2019. ISSN 0730-0301. doi: 10.1145/3355089.3356505. URL [https://doi.org/10.1145/3355089.3356505](https://doi.org/10.1145/3355089.3356505).
*   Hassan et al. [2023] Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, and Xue Bin Peng. Synthesizing physical character-scene interactions. In *ACM SIGGRAPH 2023 Conference Proceedings*, SIGGRAPH ’23, New York, NY, USA, 2023\. Association for Computing Machinery. ISBN 9798400701597. doi: 10.1145/3588432.3591525. URL [https://doi.org/10.1145/3588432.3591525](https://doi.org/10.1145/3588432.3591525).
*   Xie et al. [2021] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In *Neural Information Processing Systems (NeurIPS)*, 2021.
*   Bhat et al. [2023a] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. *arXiv preprint arXiv:2302.12288*, 2023a.
*   Zhang et al. [2022] Zhexin Zhang, Jiaxin Wen, Jian Guan, and Minlie Huang. Persona-guided planning for controlling the protagonist’s persona in story generation, 2022.
*   Xu et al. [2020] Feifei Xu, Xinpeng Wang, Yunpu Ma, Volker Tresp, Yuyi Wang, Shanlin Zhou, and Haizhou Du. Controllable multi-character psychology-oriented story generation. In *Proceedings of the 29th ACM International Conference on Information & Knowledge Management*, CIKM ’20, page 1675–1684, New York, NY, USA, 2020\. Association for Computing Machinery. ISBN 9781450368599. doi: 10.1145/3340531.3411937. URL [https://doi.org/10.1145/3340531.3411937](https://doi.org/10.1145/3340531.3411937).
*   Miller et al. [2019] Chris Miller, Mayank Dighe, Chris Martens, and Arnav Jhala. Stories of the town: balancing character autonomy and coherent narrative in procedurally generated worlds. In *Proceedings of the 14th International Conference on the Foundations of Digital Games*, FDG ’19, New York, NY, USA, 2019\. Association for Computing Machinery. ISBN 9781450372176. doi: 10.1145/3337722.3341850. URL [https://doi.org/10.1145/3337722.3341850](https://doi.org/10.1145/3337722.3341850).
*   Kim et al. [2023] Juntae Kim, Yoonseok Heo, Hogeon Yu, and Jongho Nang. A multi-modal story generation framework with ai-driven storyline guidance. *Electronics*, 12(6), 2023. ISSN 2079-9292. doi: 10.3390/electronics12061289. URL [https://www.mdpi.com/2079-9292/12/6/1289](https://www.mdpi.com/2079-9292/12/6/1289).
*   Chen et al. [2020] Chien-Yuan Chen, Sai-Keung Wong, and Wen-Yun Liu. Generation of small groups with rich behaviors from natural language interface. *Computer Animation and Virtual Worlds*, 31(4-5):e1960, 2020. doi: https://doi.org/10.1002/cav.1960. URL [https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.1960](https://onlinelibrary.wiley.com/doi/abs/10.1002/cav.1960).
*   Merino et al. [2023] Timothy Merino, Roman Negri, Dipika Rajesh, M Charity, and Julian Togelius. The five-dollar model: generating game maps and sprites from sentence embeddings. In *Proceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment*, AIIDE ’23\. AAAI Press, 2023. ISBN 1-57735-883-X. doi: 10.1609/aiide.v19i1.27506. URL [https://doi.org/10.1609/aiide.v19i1.27506](https://doi.org/10.1609/aiide.v19i1.27506).
*   Maharana et al. [2022] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-image transformers for story continuation. In *Computer Vision – ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVII*, page 70–87, Berlin, Heidelberg, 2022\. Springer-Verlag. ISBN 978-3-031-19835-9. doi: 10.1007/978-3-031-19836-6˙5.
*   Shen and Elhoseiny [2023] Xiaoqian Shen and Mohamed Elhoseiny. Storygpt-v: Large language models as consistent story visualizers, 2023.
*   Liu et al. [2024] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm – open-ended visual storytelling via latent diffusion models, 2024.
*   Qing et al. [2023] Zhongfei Qing, Zhongang Cai, Zhitao Yang, and Lei Yang. Story-to-motion: Synthesizing infinite and controllable character animation from long text. In *SIGGRAPH Asia 2023 Technical Communications*, SA ’23, New York, NY, USA, 2023\. Association for Computing Machinery. ISBN 9798400703140. doi: 10.1145/3610543.3626176. URL [https://doi.org/10.1145/3610543.3626176](https://doi.org/10.1145/3610543.3626176).
*   Torre et al. [2024] Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language models, 2024.
*   Yao et al. [2023] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023.
*   Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
*   Park et al. [2023] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior, 2023.
*   Qian et al. [2023] Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023.
*   Bhat et al. [2023b] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth, 2023b. URL [https://arxiv.org/abs/2302.12288](https://arxiv.org/abs/2302.12288).
*   cc [2] CC2D Essential Bundle | 2D Characters | Unity Asset Store. URL [https://assetstore.unity.com/packages/2d/characters/cc2d-essential-bundle-187410](https://assetstore.unity.com/packages/2d/characters/cc2d-essential-bundle-187410).
*   [57] Damian Stewart. Compel: A prompting enhancement library for transformers-type text embedding systems. URL [https://github.com/damian0815/compel](https://github.com/damian0815/compel).
*   [58] CoquiAI. coqui-ai/TTS: a deep learning toolkit for Text-to-Speech, battle-tested in research and production. URL [https://github.com/coqui-ai/TTS?tab=readme-ov-file](https://github.com/coqui-ai/TTS?tab=readme-ov-file).
*   ele [2024] elevenlabs/elevenlabs-python, April 2024. URL [https://github.com/elevenlabs/elevenlabs-python](https://github.com/elevenlabs/elevenlabs-python). original-date: 2023-03-26T11:59:52Z.
*   [60] Freesound. URL [https://freesound.org/](https://freesound.org/).

## 6 Appendix

### 6.1 Prompts in story cluster

For their system prompts, all LLM agents within the story cluster utilize one of two templates provided below. Each template is tailored to a specific agent by filling in details for the agent role, constraint, and output format. Initially, we present the templates in full and subsequently, we focus exclusively on the aforementioned three components, which are illustrated as placeholders within each template.

Expert Agent

[⬇](data:text/plain;base64,e3Vwc3RyZWFtX2NvbnRlbnRzfQoKRXhwZXJ0IHthZ2VudF9uYW1lfS4KUm9sZToKe2FnZW50X3JvbGV9CgpDb25zdHJhaW50czoKe2FnZW50X2NvbnN0cmFpbnRzfQoKT05MWSBPVVRQVVQgQSBKU09OIEZJTEUuClRoZSBleHBlcnQgbXVzdCBpbmNsdWRlIGFuIGV4cGxhbmF0aW9uIG9mIGhvdyBlYWNoIGNvbnN0cmFpbnQgaXMgZm9sbG93ZWQuCgpGb3JtYXQ6CnthZ2VudF9vdXRwdXR9){upstream_contents}Expert  {agent_name}.Role:{agent_role}Constraints:{agent_constraints}ONLY  OUTPUT  A  JSON  FILE.The  expert  must  include  an  explanation  of  how  each  constraint  is  followed.Format:{agent_output}

Critic Agent

[⬇](data:text/plain;base64,e3Vwc3RyZWFtX2NvbnRlbnRzfQoKQ3JpdGljIHthZ2VudF9uYW1lfS4KQ2hlY2sgdGhhdCB0aGUgZXhwZXJ0IGlzIHBlcmZvcm1pbmcgaXRzIHJvbGUgYW5kIGFkaGVyaW5nIHRvIHRoZSBmb2xsb3dpbmcgY29uc3RyYWludHMuCgpSb2xlOgp7YWdlbnRfcm9sZX0KCkNvbnN0cmFpbnRzOgp7YWdlbnRfY29uc3RyYWludHN9CgpGb3IgZXZlcnkgc2luZ2xlIGNvbnN0cmFpbnQsIG91dHB1dCBhIGNvbnN0cmFpbnQgc2NvcmUgYmV0d2VlbiAwLjAgYW5kIDEuMCAoaW5jbHVzaXZlKSB0aGF0IGp1ZGdlcyB0aGUgcXVhbGl0eSBvZiBFeHBlcnQncyBvdXRwdXQgYW5kIGV4cGxhaW4geW91ciByZWFzb25pbmcuIERvIG5vdCBvbWl0IGFueSBjb25zdHJhaW50cy4gQmUgdmVyeSBzdHJpY3QuCgpBbHNvIGVuc3VyZSB0aGF0IHRoZSBleHBlcnQncyBvdXRwdXQgYWRoZXJlcyBwZXJmZWN0bHkgdG8gdGhlIGZvbGxvd2luZyBmb3JtYXQgYW5kIGFkZHMgbm90aGluZyBlbHNlOgoKe2FnZW50X291dHB1dH0KCk91dHB1dCBhIGZvcm1hdCBzY29yZSBiZXR3ZWVuIDAuMCBhbmQgMS4wIChpbmNsdXNpdmUpIGZvciB3aGV0aGVyIHRoZSBvdXRwdXQgaXMgc3RyaWN0bHkgaW4gSlNPTiBmb3JtYXQuIDEuMCBtZWFucyB0aGF0IHRoZSBmb3JtYXQgaXMgcGVyZmVjdC4gQmUgdmVyeSBzdHJpY3QuCkRPIE5PVCBGT1JHRVQgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCIuCkRPIE5PVCBDUkVBVEUgdW50ZXJtaW5hdGVkIHN0cmluZ3MuCgpJZiB0aGUgZXhwZXJ0IGhhcyBub3QgY29tcGxldGVkIGFsbCBvZiBpdHMgdHVybnMsIGl0cyB0dXJuIGNvbnN0cmFpbnQgc2NvcmUgbXVzdCBiZSAwLjAuCgpJZiBhbmQgb25seSBpZiB0aGUgY29uc3RyYWludCBhbmQgZm9ybWF0IHNjb3JlcyBhcmUgYWxsIGVxdWFsIHRvIDEuMCAobWVhbmluZyB0aGVyZSBhcmUgbm8gc3VnZ2VzdGlvbnMpLCB5b3UgbXVzdCBwcmludCBURVJNSU5BVEUuCk90aGVyd2lzZSwgZXhwbGFpbiB0byB0aGUgRXhwZXJ0IGV4YWN0bHkgd2h5IHRoZSBzY29yZSBpcyBsZXNzIHRoYW4gMS4wLCBtYWtlIHN1Z2dlc3Rpb25zIG9uIGhvdyB0aGV5IGNhbiBpbXByb3ZlIHRoZWlyIHNjb3JlIGFuZCBETyBOT1QgUFJJTlQgVEVSTUlOQVRFIElOIEFOWSBXQVkuCkRPIE5PVCBTQVkgIndlIGNhbm5vdCBwcmludCBURVJNSU5BVEUiLgoKRE8gTk9UIEFTSyBRVUVTVElPTlMu){upstream_contents}Critic  {agent_name}.Check  that  the  expert  is  performing  its  role  and  adhering  to  the  following  constraints.Role:{agent_role}Constraints:{agent_constraints}For  every  single  constraint,  output  a  constraint  score  between  0.0  and  1.0  (inclusive)  that  judges  the  quality  of  Expert’s  output  and  explain  your  reasoning.  Do  not  omit  any  constraints.  Be  very  strict.Also  ensure  that  the  expert’s  output  adheres  perfectly  to  the  following  format  and  adds  nothing  else:{agent_output}Output  a  format  score  between  0.0  and  1.0  (inclusive)  for  whether  the  output  is  strictly  in  JSON  format.  1.0  means  that  the  format  is  perfect.  Be  very  strict.DO  NOT  FORGET  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘".DO  NOT  CREATE  unterminated  strings.If  the  expert  has  not  completed  all  of  its  turns,  its  turn  constraint  score  must  be  0.0.If  and  only  if  the  constraint  and  format  scores  are  all  equal  to  1.0  (meaning  there  are  no  suggestions),  you  must  print  TERMINATE.Otherwise,  explain  to  the  Expert  exactly  why  the  score  is  less  than  1.0,  make  suggestions  on  how  they  can  improve  their  score  and  DO  NOT  PRINT  TERMINATE  IN  ANY  WAY.DO  NOT  SAY  "we  cannot  print  TERMINATE".DO  NOT  ASK  QUESTIONS.

#### 6.1.1 Story arc team

Agent role

[⬇](data:text/plain;base64,SW4geW91ciBmaXJzdCB0dXJuLCB1c2UgdGhlIHN0b3J5IGlucHV0IHRvIG91dHB1dCBhIGdlbmVyYWwgc3RvcnkgYXJjIHRoYXQgZGVzY3JpYmVzIHRoZSBleHBvc2l0aW9uLCByaXNpbmcgYWN0aW9uLCBjbGltYXgsIGZhbGxpbmcgYWN0aW9uLCBhbmQgcmVzb2x1dGlvbi4KSW4geW91ciBzZWNvbmQgdHVybiwgZXhwYW5kIHVwb24gYW55dGhpbmcgdmFndWUuIEUuZy4sIGlmIGNoYWxsZW5nZXMvcHV6emxlcy9teXN0ZXJpZXMgYXJlIG1lbnRpb25lZCwgZXhwbGFpbiB3aGF0IHRoZXkgYXJlIHNwZWNpZmljYWxseS4=)In  your  first  turn,  use  the  story  input  to  output  a  general  story  arc  that  describes  the  exposition,  rising  action,  climax,  falling  action,  and  resolution.In  your  second  turn,  expand  upon  anything  vague.  E.g.,  if  challenges/puzzles/mysteries  are  mentioned,  explain  what  they  are  specifically.

Agent constraint

[⬇](data:text/plain;base64,MS4gRWFjaCBjaGFyYWN0ZXIgbXVzdCBiZSBhIHNpbmd1bGFyIGh1bWFuLgoyLiBUaGVyZSBtdXN0IGJlIG5vIG5vbi1odW1hbiBjaGFyYWN0ZXJzLgozLiBDaGFyYWN0ZXJzIGNhbm5vdCBpbnRlcmFjdCB3aXRoIG5vbi1odW1hbiBjaGFyYWN0ZXJzIGluIHRoZSBzdG9yeS4KNC4gVGhlcmUgbXVzdCBiZSBtdWx0aXBsZSBtYWluIGNoYXJhY3RlcnMgdGhhdCB0YWxrIHRvIGVhY2ggb3RoZXIgdGhyb3VnaG91dCB0aGUgc3RvcnkuCjUuIEtlZXAgdGhlIHN0b3J5IHNpbXBsZS4KNi4gRG8gbm90IGJlIHZhZ3VlIChlLmcuLCAidGhleSBzb2x2ZSBhIHB1enpsZSIsICJ0aGV5IGVuY291bnRlciBhIG15c3RlcnkiKSwgYmUgc3BlY2lmaWMhCjcuIEl0IG11c3QgYmUgcG9zc2libGUgdG8gYW5pbWF0ZSB0aGUgc3Rvcnkgd2l0aCBjaGFyYWN0ZXJzIG9ubHkgc3BlYWtpbmcgYW5kIHdhbGtpbmcuIFRoZXkgY2Fubm90IGNsaW1iLCBqdW1wLCBvciBob2xkIGFueXRoaW5nLgo4LiBNYWtlIHN1cmUgdGhhdCB3aXRoIGVhY2ggY2hhbmdlLCB0aGUgbW90aWZzIGluIHRoZSBzdG9yeSBzdGF5IGNvbnNpc3RlbnQu)1.  Each  character  must  be  a  singular  human.2.  There  must  be  no  non-human  characters.3.  Characters  cannot  interact  with  non-human  characters  in  the  story.4.  There  must  be  multiple  main  characters  that  talk  to  each  other  throughout  the  story.5.  Keep  the  story  simple.6.  Do  not  be  vague  (e.g.,  "they  solve  a  puzzle",  "they  encounter  a  mystery"),  be  specific!7.  It  must  be  possible  to  animate  the  story  with  characters  only  speaking  and  walking.  They  cannot  climb,  jump,  or  hold  anything.8.  Make  sure  that  with  each  change,  the  motifs  in  the  story  stay  consistent.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInN0b3J5X2FyYyI6IGRpY3QgIC8vIFN0b3J5IGFyYwp7CiAgICAiZXhwb3NpdGlvbiI6IHN0cgogICAgInJpc2luZ19hY3Rpb24iOiBzdHIKICAgICJjbGltYXgiOiBzdHIKICAgICJmYWxsaW5nX2FjdGlvbiI6IHN0cgogICAgInJlc29sdXRpb24iOiBzdHIKfQoiY29uc3RyYWludHMiOiBsaXN0W3N0cl0gIC8vIEV4cGxhbmF0aW9uIGZvciBlYWNoIGNvbnN0cmFpbnQKfQpgYGA=)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"story_arc":  dict  //  Story  arc{"exposition":  str"rising_action":  str"climax":  str"falling_action":  str"resolution":  str}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

#### 6.1.2 Characters team

Agent role

[⬇](data:text/plain;base64,Rm9yIGVhY2ggY2hhcmFjdGVyLCBkZXNjcmliZSB0aGVpciBuYW1lLCBnZW5kZXIsIGFnZSwgcGVyc29uYWxpdHksIGJlbGllZnMsIG1vdGl2YXRpb25zLCBkZXZlbG9wbWVudCwgYW5kIHBoeXNpY2FsIGRlc2NyaXB0aW9uLgpUaGUgcGh5c2ljYWwgZGVzY3JpcHRpb24gbXVzdCBkZXNjcmliZSB0aGUgZmFicmljLCBjb2xvciwgdGV4dHVyZSwgYW5kIGFjY2Vzc29yaWVzIG9mIHRoZSB0dW5pYywgcGFudHMsIGFuZCBib290cyB0aGF0IHRoZSBjaGFyYWN0ZXIgaXMgd2VhcmluZy4KCkluIHlvdXIgZmlyc3QgdHVybiwgZm9yIGVhY2ggcGFydCBvZiB0aGUgc3RvcnkgYXJjIChleHBvc2l0aW9uLCByaXNpbmcgYWN0aW9uLCBjbGltYXgsIGZhbGxpbmcgYWN0aW9uLCBhbmQgcmVzb2x1dGlvbikgY3JlYXRlIGEgbGlzdCBvZiBldmVyeSBzaW5nbGUgaHVtYW4gbWVudGlvbmVkIGRpcmVjdGx5IG9yIGluZGlyZWN0bHkgd2l0aCBlbXB0eSBhdHRyaWJ1dGVzLgpJbiB5b3VyIHNlY29uZCB0dXJuLCBjb2xsYXRlIHRoZXNlIGxpc3RzIGFuZCBwb3B1bGF0ZSBldmVyeSBjaGFyYWN0ZXIncyBhdHRyaWJ1dGVzIHdpdGggYnJvYWQgZGV0YWlscyB0aGF0IGFyZSBjb25zaXN0ZW50IHdpdGggdGhlIHN0b3J5IGFyYy4KSW4geW91ciB0aGlyZCB0dXJuLCBtYWtlIGF0dHJpYnV0ZXMgbXVjaCBtb3JlIGRldGFpbGVkLgo=)For  each  character,  describe  their  name,  gender,  age,  personality,  beliefs,  motivations,  development,  and  physical  description.The  physical  description  must  describe  the  fabric,  color,  texture,  and  accessories  of  the  tunic,  pants,  and  boots  that  the  character  is  wearing.In  your  first  turn,  for  each  part  of  the  story  arc  (exposition,  rising  action,  climax,  falling  action,  and  resolution)  create  a  list  of  every  single  human  mentioned  directly  or  indirectly  with  empty  attributes.In  your  second  turn,  collate  these  lists  and  populate  every  character’s  attributes  with  broad  details  that  are  consistent  with  the  story  arc.In  your  third  turn,  make  attributes  much  more  detailed.

Agent constraint

[⬇](data:text/plain;base64,MS4gRXZlcnkgaHVtYW4gbWVudGlvbmVkIGluIHRoZSBzdG9yeSBhcmMgbXVzdCBiZSBpbiB0aGUgY2hhcmFjdGVyIGxpc3QuCjIuIFRoZXJlIG11c3QgYmUgbW9yZSB0aGFuIDEgY2hhcmFjdGVyIGxpc3RlZC4KCjMuIEV2ZXJ5IGNoYXJhY3RlciBtdXN0IGJlIGh1bWFuLgo0LiBFdmVyeSBjaGFyYWN0ZXIgbmFtZSBtdXN0IGJlIHNpbmd1bGFyLCBlLmcuLCBhbiBlbnRyeSBjYW5ub3QgYmUgbmFtZWQgInZpbGxhZ2VycyIgb3IgInNpYmxpbmdzIiBiZWNhdXNlIHRoZXNlIGFyZSBwbHVyYWwuCgo1LiBUaGVyZSBtdXN0IGJlIGEgTmFycmF0b3Igd2l0aCBlbXB0eSBhdHRyaWJ1dGVzIGluIGl0cyBwaHlzaWNhbCBkZXNjcmlwdGlvbi4KCjYuIEVhY2ggY2hhcmFjdGVyJ3MgYXR0cmlidXRlcyBtdXN0IGJlIG5vbi1lbXB0eSwgdW5hbWJpZ3VvdXMsIGFuZCBkZXRhaWxlZC4K)1.  Every  human  mentioned  in  the  story  arc  must  be  in  the  character  list.2.  There  must  be  more  than  1  character  listed.3.  Every  character  must  be  human.4.  Every  character  name  must  be  singular,  e.g.,  an  entry  cannot  be  named  "villagers"  or  "siblings"  because  these  are  plural.5.  There  must  be  a  Narrator  with  empty  attributes  in  its  physical  description.6.  Each  character’s  attributes  must  be  non-empty,  unambiguous,  and  detailed.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInN0b3J5X2NoYXJhY3RlcnMiOiBsaXN0W2RpY3RdICAvLyBMaXN0IG9mIHN0b3J5IGNoYXJhY3RlcnMKewoibmFtZSI6IHN0ciAgLy8gQ2hhcmFjdGVyIG5hbWUKImdlbmRlciI6IHN0ciAgLy8gQ2hhcmFjdGVyIGdlbmRlcjogIk1hbGUiIG9yICJGZW1hbGUiCiJhZ2UiOiBpbnQgIC8vIENoYXJhY3RlciBhZ2UKInBlcnNvbmFsaXR5Ijogc3RyICAvLyBDaGFyYWN0ZXIncyBwZXJzb25hbGl0eQoiYmVsaWVmcyI6IHN0ciAgLy8gQ2hhcmFjdGVyJ3MgYmVsaWVmcwoibW90aXZhdGlvbnMiOiBzdHIgIC8vIENoYXJhY3RlcidzIG1vdGl2YXRpb25zCiJkZXZlbG9wbWVudCI6IHN0ciAgLy8gSG93IHRoZSBjaGFyYWN0ZXIgZGV2ZWxvcHMgdGhyb3VnaG91dCB0aGUgc3RvcnkKInBoeXNpY2FsX2Rlc2NyaXB0aW9uIjogZGljdFtzdHJdICAvLyBDaGFyYWN0ZXIncyBwaHlzaWNhbCBkZXNjcmlwdGlvbgogICAgewogICAgICAgICJ0dW5pYyI6IHN0ciAgLy8gRGVzY3JpcHRpb24gb2YgdHVuaWMKICAgICAgICAicGFudHMiOiBzdHIgIC8vIERlc2NyaXB0aW9uIG9mIHBhbnRzCiAgICAgICAgImJvb3RzIjogc3RyICAvLyBEZXNjcmlwdGlvbiBvZiBib290cwogICAgfQp9CiJjb25zdHJhaW50cyI6IGxpc3Rbc3RyXSAgLy8gRXhwbGFuYXRpb24gZm9yIGVhY2ggY29uc3RyYWludAp9CmBgYA==)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"story_characters":  list[dict]  //  List  of  story  characters{"name":  str  //  Character  name"gender":  str  //  Character  gender:  "Male"  or  "Female""age":  int  //  Character  age"personality":  str  //  Character’s  personality"beliefs":  str  //  Character’s  beliefs"motivations":  str  //  Character’s  motivations"development":  str  //  How  the  character  develops  throughout  the  story"physical_description":  dict[str]  //  Character’s  physical  description{"tunic":  str  //  Description  of  tunic"pants":  str  //  Description  of  pants"boots":  str  //  Description  of  boots}}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

#### 6.1.3 Settings team

##### Stage 1

Agent role

[⬇](data:text/plain;base64,Q3JlYXRlIGEgaGllcmFyY2hpY2FsIGdyYXBoIG9mIHNldHRpbmdzIGRpcmVjdGx5IG9yIGluZGlyZWN0bHkgbWVudGlvbmVkIGluIHRoZSBzdG9yeSBhcmMsIHdoZXJlIGVhY2ggbm9kZSBpcyBhIHNldHRpbmcgY29uc2lzdHMgb2YgaXRzIG5hbWUsIGl0cyBwYXJlbnQgc2V0dGluZywgYSBsaXN0IG9mIGl0cyBjaGlsZHJlbiBzZXR0aW5ncywgYW5kIHN0b3J5LXJlbGF0ZWQgaW5mb3JtYXRpb24uClRoZSBwYXJlbnQgb2YgdGhlIGN1cnJlbnQgc2V0dGluZyBtdXN0IGJlIGEgbGFyZ2VyIGFyZWEgdGhhdCB0aGUgY3VycmVudCBzZXR0aW5nIG11c3QgYmUgZnVsbHkgY29udGFpbmVkIHdpdGhpbi4KQSBjaGlsZCBvZiB0aGUgY3VycmVudCBzZXR0aW5nIG11c3QgYmUgYSBzbWFsbGVyIGFyZWEgdGhhdCBpcyBmdWxseSBjb250YWluZWQgd2l0aGluIHRoZSBjdXJyZW50IHNldHRpbmcuCkFsbCBjaGlsZHJlbiB3aXRoaW4gYSBzZXR0aW5nIHNob3VsZCBiZSBuYXZpZ2FibGUgYmV0d2VlbiBlYWNoIG90aGVyIHdpdGhvdXQgbGVhdmluZyB0aGUgc2V0dGluZy4KClRoZSBmaXJzdCBzZXR0aW5nIHNob3VsZCBiZSBuYW1lZCBXb3JsZCBhbmQgYWxsIG90aGVyIHNldHRpbmdzIGFyZSBjb250YWluZWQgd2l0aGluIHRoaXMgc2V0dGluZy4KVGhlIFdvcmxkIHNldHRpbmcncyBwYXJlbnQgbXVzdCBiZSBhbiBlbXB0eSBzdHJpbmcuCgpJbiB5b3VyIGZpcnN0IHR1cm4sIGNyZWF0ZSB0aGUgc2V0dGluZyBoaWVyYXJjaHkgYW5kIG9taXQgImlzX291dHNpZGUiIGF0dHJpYnV0ZXMuCkluIHlvdXIgc2Vjb25kIHR1cm4sIGZvciBlYWNoIHNldHRpbmcgdGhhdCBoYXMgYW4gaW50ZXJpb3IgZW50ZXIgKGUuZy4sIGEgaG91c2UpLCBjcmVhdGUgYXBwcm9wcmlhdGUgY2hpbGRyZW4gc2V0dGluZ3MgZm9yIHRoZSBpbnNpZGUgdGhhdCBhcmUgcmVsZXZhbnQgdG8gdGhlIHN0b3J5IGFyYy4KSW4geW91ciB0aGlyZCB0dXJuLCBmaWxsIGluIHRoZSAiaXNfb3V0c2lkZSIgYXR0cmlidXRlIGZvciBlYWNoIHNldHRpbmcuCg==)Create  a  hierarchical  graph  of  settings  directly  or  indirectly  mentioned  in  the  story  arc,  where  each  node  is  a  setting  consists  of  its  name,  its  parent  setting,  a  list  of  its  children  settings,  and  story-related  information.The  parent  of  the  current  setting  must  be  a  larger  area  that  the  current  setting  must  be  fully  contained  within.A  child  of  the  current  setting  must  be  a  smaller  area  that  is  fully  contained  within  the  current  setting.All  children  within  a  setting  should  be  navigable  between  each  other  without  leaving  the  setting.The  first  setting  should  be  named  World  and  all  other  settings  are  contained  within  this  setting.The  World  setting’s  parent  must  be  an  empty  string.In  your  first  turn,  create  the  setting  hierarchy  and  omit  "is_outside"  attributes.In  your  second  turn,  for  each  setting  that  has  an  interior  enter  (e.g.,  a  house),  create  appropriate  children  settings  for  the  inside  that  are  relevant  to  the  story  arc.In  your  third  turn,  fill  in  the  "is_outside"  attribute  for  each  setting.

Agent constraint

[⬇](data:text/plain;base64,MS4gRXZlcnkgc2luZ2xlIHNldHRpbmcgbWVudGlvbmVkIGRpcmVjdGx5IG9yIGluZGlyZWN0bHkgaW4gdGhlIHN0b3J5IGFyYyAoYm90aCBsYXJnZSBhbmQgc21hbGwpIG11c3QgYmUgaW5jbHVkZWQuCjIuIEZvciBldmVyeSBzZXR0aW5nIHRoYXQgaGFzIGFuIGludGVyaW9yIChlLmcuLCBhIGhvdXNlKSwgdGhlcmUgbXVzdCBiZSBhdCBsZWFzdCBvbmUgY2hpbGQgc2V0dGluZyBzbyB0aGF0IGNoYXJhY3RlcnMgY2FuIGVudGVyLgozLiBUaGUgZGlmZmVyZW5jZSBpbiBzY2FsZSBiZXR3ZWVuIGEgcGFyZW50IHNldHRpbmcgYW5kIGNoaWxkIHNldHRpbmcgbXVzdCBiZSBncmFkdWFsLiBDcmVhdGUgaW50ZXJtZWRpYXRlIHNldHRpbmdzIGlzIHRoaXMgaXMgbm90IHRoZSBjYXNlLgo0LiBJZiBhIHNldHRpbmcgaXMgYSBidWlsZGluZyAoZS5nLiwgYSBob3VzZSwgbWFuc2lvbiwgc2hlZCwgZXRjLiksIGl0cyBpc19vdXRzaWRlIGF0dHJpYnV0ZSBtdXN0IGJlIHRydWUsIGJ1dCBpdHMgY2hpbGRyZW4gc2V0dGluZ3MgKHJvb21zKSBtdXN0IGhhdmUgaXNfb3V0c2lkZSA9IGZhbHNlLgo1LiBBbGwgc2V0dGluZyBuYW1lcyBtdXN0IGJlIHVuaXF1ZS4KNi4gTWFrZSBzdXJlIHRoZSBleHBlcnQgY29tcGxldGVzIDMgdHVybnMu)1.  Every  single  setting  mentioned  directly  or  indirectly  in  the  story  arc  (both  large  and  small)  must  be  included.2.  For  every  setting  that  has  an  interior  (e.g.,  a  house),  there  must  be  at  least  one  child  setting  so  that  characters  can  enter.3.  The  difference  in  scale  between  a  parent  setting  and  child  setting  must  be  gradual.  Create  intermediate  settings  is  this  is  not  the  case.4.  If  a  setting  is  a  building  (e.g.,  a  house,  mansion,  shed,  etc.),  its  is_outside  attribute  must  be  true,  but  its  children  settings  (rooms)  must  have  is_outside  =  false.5.  All  setting  names  must  be  unique.6.  Make  sure  the  expert  completes  3  turns.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInN0b3J5X3NldHRpbmdzIjogbGlzdFtkaWN0XSAgLy8gTGlzdCBvZiBzZXR0aW5ncwp7CiAgICAibmFtZSI6IHN0ciAgLy8gTmFtZSBvZiB0aGUgc2V0dGluZwogICAgImlzX291dHNpZGUiOiBib29sICAvLyBJbmRpY2F0ZXMgd2hldGhlciB0aGUgc2V0dGluZyBpcyBpbnNpZGUgb3Igb3V0c2lkZS4KICAgICJwYXJlbnQiOiBzdHIgIC8vIE5hbWUgb2YgcGFyZW50IHNldHRpbmcgb3IgZW1wdHkgc3RyaW5nIGlmIG5vbmUKICAgICJjaGlsZHJlbiI6IGxpc3Rbc3RyXSAgLy8gTGlzdCBvZiBjaGlsZHJlbiBzZXR0aW5ncwp9CiJjb25zdHJhaW50cyI6IGxpc3Rbc3RyXSAgLy8gRXhwbGFuYXRpb24gZm9yIGVhY2ggY29uc3RyYWludAp9CmBgYA==)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"story_settings":  list[dict]  //  List  of  settings{"name":  str  //  Name  of  the  setting"is_outside":  bool  //  Indicates  whether  the  setting  is  inside  or  outside."parent":  str  //  Name  of  parent  setting  or  empty  string  if  none"children":  list[str]  //  List  of  children  settings}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

##### Stage 2

Agent role

[⬇](data:text/plain;base64,Rm9yIGVhY2ggc2V0dGluZywgYWRkIGEgdmlzdWFsIHByb21wdCB0aGF0IHN1bW1hcml6ZXMgaXRzIHBoeXNpY2FsIHN0YXRlLiBJbmNsdWRlIHRoZSB2aXNpYmxlIHNxdWFyZSBmb290YWdlIG9mIHdhbGthYmxlIGFyZWEgYW5kIHRoZSB2aXN1YWwgc3R5bGUgdGhhdCBhbGwgc2V0dGluZ3Mgc2hvdWxkIGhhdmUgKGUuZy4sIHRpbWUgcGVyaW9kKS4=)For  each  setting,  add  a  visual  prompt  that  summarizes  its  physical  state.  Include  the  visible  square  footage  of  walkable  area  and  the  visual  style  that  all  settings  should  have  (e.g.,  time  period).

Agent constraint

[⬇](data:text/plain;base64,MS4gVGhlIHZpc3VhbCBwcm9tcHQgc2hvdWxkIG5vdCBpbmNsdWRlIGFueSBpbmZvcm1hdGlvbiBhYm91dCB0aGUgY2hhcmFjdGVycy4KMi4gVGhlIHZpc3VhbCBwcm9tcHQgbXVzdCBkZXNjcmliZSBhdCBsZWFzdCA1IGludGVyZXN0aW5nIG9iamVjdHMgKGJpZyBvciBzbWFsbCkgaW4gdGhlIHNldHRpbmcuCjMuIFRoZSB2aXN1YWwgcHJvbXB0IHNob3VsZCBiZSB3cml0dGVuIGluIHByb3NlIGFuZCBiZSBjb25jaXNlIGFuZCBsaXRlcmFsIGFuZCBub3QgZmlndXJhdGl2ZS4KNC4gVGhlIHZpc3VhbCBwcm9tcHQgbXVzdCBiZSBVTkRFUiA2MCB3b3JkcyBsb25nLiBEbyBub3Qgd2FzdGUgYW55IHdvcmRzIQo1LiBJZiBhIHNldHRpbmcgaGFzIGlzX291dGRvb3JzID0gdHJ1ZSwgdGhlIHNxdWFyZSBmb290YWdlIHNob3VsZCBhbHNvIGNvbnNpZGVyIHRoZSBzdXJyb3VuZGluZyBhcmVhLiBJZiB0aGUgc2V0dGluZyBoYXMgaXNfb3V0ZG9vcnMgPSBmYWxzZSwgdGhlIHNxdWFyZSBmb290YWdlIHNob3VsZCBiZSBsaW1pdGVkIHRvIHRoZSByb29tLgo2LiBJZiBhIHNldHRpbmcgaGFzIGlzX291dGRvb3JzID0gdHJ1ZSwgT05MWSBERVNDUklCRSBUSEUgT1VUU0lERSBvZiB0aGUgc2V0dGluZy4gSWYgdGhlIHNldHRpbmcgaGFzIGlzX291dGRvb3JzID0gZmFsc2UsIE9OTFkgREVTQ1JJQkUgVEhFIElOU0lERS4KNy4gVGhlIHNxdWFyZV9mb290YWdlIG9mIGVhY2ggc2V0dGluZyBtdXN0IGJlIGF0IGxlYXN0IDEwMC4gT3RoZXJ3aXNlLCBhZGQgZGV0YWlscyBhYm91dCB0aGUgcGFyZW50IHNldHRpbmcgaW50byB0aGUgdmlzdWFsIHByb21wdCBhbmQgdXBkYXRlIHNxdWFyZV9mb290YWdlIGFjY29yZGluZ2x5Lgo4LiBETyBOT1QgT01JVCBBTlkgU0VUVElOR1MuCg==)1.  The  visual  prompt  should  not  include  any  information  about  the  characters.2.  The  visual  prompt  must  describe  at  least  5  interesting  objects  (big  or  small)  in  the  setting.3.  The  visual  prompt  should  be  written  in  prose  and  be  concise  and  literal  and  not  figurative.4.  The  visual  prompt  must  be  UNDER  60  words  long.  Do  not  waste  any  words!5.  If  a  setting  has  is_outdoors  =  true,  the  square  footage  should  also  consider  the  surrounding  area.  If  the  setting  has  is_outdoors  =  false,  the  square  footage  should  be  limited  to  the  room.6.  If  a  setting  has  is_outdoors  =  true,  ONLY  DESCRIBE  THE  OUTSIDE  of  the  setting.  If  the  setting  has  is_outdoors  =  false,  ONLY  DESCRIBE  THE  INSIDE.7.  The  square_footage  of  each  setting  must  be  at  least  100.  Otherwise,  add  details  about  the  parent  setting  into  the  visual  prompt  and  update  square_footage  accordingly.8.  DO  NOT  OMIT  ANY  SETTINGS.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInN0b3J5X3NldHRpbmdzIjogbGlzdFtkaWN0XSAgLy8gTGlzdCBvZiBzZXR0aW5ncwp7CiAgICAibmFtZSI6IHN0ciAgLy8gTmFtZSBvZiB0aGUgc2V0dGluZwogICAgImlzX291dHNpZGUiOiBib29sICAvLyBJbmRpY2F0ZXMgd2hldGhlciB0aGUgc2V0dGluZyBpcyBpbnNpZGUgb3Igb3V0c2lkZS4KICAgICJwYXJlbnQiOiBzdHIgIC8vIE5hbWUgb2YgcGFyZW50IHNldHRpbmcgb3IgZW1wdHkgc3RyaW5nIGlmIG5vbmUKICAgICJjaGlsZHJlbiI6IGxpc3Rbc3RyXSAgLy8gTGlzdCBvZiBjaGlsZHJlbiBzZXR0aW5ncwogICAgInZpc3VhbF9wcm9tcHQiOiBzdHIgIC8vIERlc2NyaXB0aW9uIG9mIHBoeXNpY2FsIHN0YXRlIGZvciBzdGFibGUgZGlmZnVzaW9uIHByb21wdAogICAgInNxdWFyZV9mb290YWdlIjogaW50ICAvLyBWaXNpYmxlIHNxdWFyZSBmb290YWdlIG9mIHNldHRpbmcKfQoidmlzdWFsX3N0eWxlIjogc3RyICAvLyBWaXN1YWwgc3R5bGUgb2YgYWxsIHNldHRpbmdzCiJjb25zdHJhaW50cyI6IGxpc3Rbc3RyXSAgLy8gRXhwbGFuYXRpb24gZm9yIGVhY2ggY29uc3RyYWludAp9CmBgYA==)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"story_settings":  list[dict]  //  List  of  settings{"name":  str  //  Name  of  the  setting"is_outside":  bool  //  Indicates  whether  the  setting  is  inside  or  outside."parent":  str  //  Name  of  parent  setting  or  empty  string  if  none"children":  list[str]  //  List  of  children  settings"visual_prompt":  str  //  Description  of  physical  state  for  stable  diffusion  prompt"square_footage":  int  //  Visible  square  footage  of  setting}"visual_style":  str  //  Visual  style  of  all  settings"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

#### 6.1.4 Storybeat team

Agent role

[⬇](data:text/plain;base64,RWFjaCBzdG9yeSBiZWF0IGhhcyBhIGRlc2NyaXB0aW9uLCBhIGxpc3Qgb2YgY2hhcmFjdGVycyBpbnZvbHZlZCwgYSBsaXN0IG9mIHNldHRpbmdzIGludm9sdmVkLCB0aW1lIG9mIGRheSwgYW5kIGhvdyBjaGFyYWN0ZXJzIGFuZCB0aGUgYXVkaWVuY2Ugc2hvdWxkIGZlZWwuCgpJbiB5b3VyIGZpcnN0IHR1cm4sIG91dHB1dCBhIGRldGFpbGVkIGxpc3Qgb2Ygc3RvcnkgYmVhdHMuCkluIHlvdXIgc2Vjb25kIHR1cm4sIHNwbGl0IHN0b3J5IGJlYXRzIHRoYXQgaGF2ZSBtdWx0aXBsZSBzZXR0aW5ncyBpbnRvIG11bHRpcGxlIHN0b3J5IGJlYXRzIHdpdGggc2luZ2xlIHNldHRpbmdzLgo=)Each  story  beat  has  a  description,  a  list  of  characters  involved,  a  list  of  settings  involved,  time  of  day,  and  how  characters  and  the  audience  should  feel.In  your  first  turn,  output  a  detailed  list  of  story  beats.In  your  second  turn,  split  story  beats  that  have  multiple  settings  into  multiple  story  beats  with  single  settings.

Agent constraint

[⬇](data:text/plain;base64,MS4gVGhlcmUgbXVzdCBiZSBhdCBsZWFzdCAxIHN0b3J5IGJlYXQgZm9yIGVhY2ggcGFydCBvZiB0aGUgc3RvcnkgYXJjIChleHBvc2l0aW9uLCByaXNpbmcgYWN0aW9uLCBjbGltYXgsIGZhbGxpbmcgYWN0aW9uLCByZXNvbHV0aW9uKS4KMi4gRG8gbm90IGludHJvZHVjZSBuZXcgc2V0dGluZ3Mgb3IgY2hhcmFjdGVycyB0aGF0IGFyZSBub3QgaW4gc3RvcnlfY2hhcmFjdGVycyBhbmQgc3Rvcnlfc2V0dGluZ3MuCjMuIEVhY2ggc3RvcnkgYmVhdCdzIGNoYXJhY3RlcnMgbGlzdCBtdXN0IGluY2x1ZGUgTmFycmF0b3IuCjQuIFRoZSBwYXNzYWdlIG9mIHRpbWUgZHVyaW5nIGEgc3RvcnkgYmVhdCBzaG91bGQgYmUgcmVhc29uYWJseSBzbWFsbCB0byBtaW5pbWl6ZSB0aGUgYW1vdW50IG9mIGp1bXBzIGluIHRpbWUu)1.  There  must  be  at  least  1  story  beat  for  each  part  of  the  story  arc  (exposition,  rising  action,  climax,  falling  action,  resolution).2.  Do  not  introduce  new  settings  or  characters  that  are  not  in  story_characters  and  story_settings.3.  Each  story  beat’s  characters  list  must  include  Narrator.4.  The  passage  of  time  during  a  story  beat  should  be  reasonably  small  to  minimize  the  amount  of  jumps  in  time.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInN0b3J5X2JlYXRzIjogbGlzdFtkaWN0XSAgLy8gU3RvcnkgYmVhdHMKewogICAgImRlc2NyaXB0aW9uIjogc3RyICAvLyBEZXNjcmlwdGlvbiBvZiB3aGF0IGhhcHBlbnMgaW4gdGhlIHN0b3J5IGJlYXQKICAgICJzZXR0aW5ncyI6IGxpc3Rbc3RyXSAgLy8gTGlzdCBvZiBzZXR0aW5nIHVzZWQgaW4gdGhlIHN0b3J5IGJlYXQKICAgICJjaGFyYWN0ZXJzIjogbGlzdFtzdHJdICAvLyBMaXN0IG9mIGNoYXJhY3RlcnMgcHJlc2VudCBpbiB0aGUgc3RvcnkgYmVhdAogICAgInRpbWVfcGFzc2FnZSI6IHN0ciAgLy8gSG93IG11Y2ggdGltZSBwYXNzZXMgZHVyaW5nIHRoZSBzdG9yeSBiZWF0CiAgICAiY2hhcmFjdGVyX2ZlZWxpbmdzIjogc3RyICAvLyBEZXNjcmliZSBob3cgY2hhcmFjdGVycyBmZWVsIGR1cmluZyB0aGUgc3RvcnkgYmVhdAogICAgImF1ZGllbmNlX2ZlZWxpbmdzIjogc3RyICAvLyBEZXNjcmliZSBob3cgdGhlIGF1ZGllbmNlIHNob3VsZCBmZWVsIGR1cmluZyB0aGUgc3RvcnkgYmVhdAp9CiJjb25zdHJhaW50cyI6IGxpc3Rbc3RyXSAgLy8gRXhwbGFuYXRpb24gZm9yIGVhY2ggY29uc3RyYWludAp9CmBgYA==)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"story_beats":  list[dict]  //  Story  beats{"description":  str  //  Description  of  what  happens  in  the  story  beat"settings":  list[str]  //  List  of  setting  used  in  the  story  beat"characters":  list[str]  //  List  of  characters  present  in  the  story  beat"time_passage":  str  //  How  much  time  passes  during  the  story  beat"character_feelings":  str  //  Describe  how  characters  feel  during  the  story  beat"audience_feelings":  str  //  Describe  how  the  audience  should  feel  during  the  story  beat}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

#### 6.1.5 Setting affordance team

##### Stage 1

Agent role

[⬇](data:text/plain;base64,c2V0dGluZ19vYmplY3RzIGNvbnRhaW5zIGEgbGlzdCBvZiBvYmplY3RzIGRldGVjdGVkIGluIHRoYXQgaW1hZ2UsIHRoZWlyIGJvdW5kaW5nIGJveGVzLCBhbmQgdGhlaXIgZGVwdGhzLiBLRUVQIElOIE1JTkQgdGhhdCBib3VuZGluZyBib3hlcyBjYW4gYmUgbWlzbGFiZWxlZCBvciBmYWxzZSBwb3NpdGl2ZXMgYW5kIDEgb2JqZWN0IGNhbiBiZSBkZXRlY3RlZCBhcyBtdWx0aXBsZSBib3VuZGluZyBib3hlcy4KCkluIHlvdXIgZmlyc3QgdHVybiwgYWRkIGVhY2ggZW50cnkgaW4gc2V0dGluZ19vYmplY3RzIHRvIG9iamVjdF9saXN0IGFuZCB1c2UgdGhlIGJib3hlcyAoZGV0ZWN0ZWQgYm91bmRpbmcgYm94ZXMpIGFuZCBiYm94X2RlcHRocyAoYXZlcmFnZSBkZXB0aHMgb2YgdGhlIGJvdW5kaW5nIGJveGVzKSB0byBkZXNjcmliZSB0aGUgY29uZmlndXJhdGlvbiBvZiB0aGUgb2JqZWN0cyBpbiBoaWdoIGRldGFpbC4gT3V0cHV0IGVtcHR5IHN0cmluZ3MgZm9yIHNwYXRpYWxfcmVsYXRpb25zIGFuZCBldmlkZW5jZS4KSW4geW91ciBzZWNvbmQgdHVybiwgYWRkIHRoZSBzcGF0aWFsX3JlbGF0aW9ucyBkZXNjcmliaW5nIGhvdyB0aGUgY29uZmlndXJhdGlvbnMgYXJlIHJlbGF0ZWQgQkVUV0VFTiBkaWZmZXJlbnQgdHlwZXMgb2Ygb2JqZWN0cyBpbiBoaWdoIGRldGFpbC4gT3V0cHV0IGVtcHR5IHN0cmluZ3MgZm9yIGV2aWRlbmNlLgo=)setting_objects  contains  a  list  of  objects  detected  in  that  image,  their  bounding  boxes,  and  their  depths.  KEEP  IN  MIND  that  bounding  boxes  can  be  mislabeled  or  false  positives  and  1  object  can  be  detected  as  multiple  bounding  boxes.In  your  first  turn,  add  each  entry  in  setting_objects  to  object_list  and  use  the  bboxes  (detected  bounding  boxes)  and  bbox_depths  (average  depths  of  the  bounding  boxes)  to  describe  the  configuration  of  the  objects  in  high  detail.  Output  empty  strings  for  spatial_relations  and  evidence.In  your  second  turn,  add  the  spatial_relations  describing  how  the  configurations  are  related  BETWEEN  different  types  of  objects  in  high  detail.  Output  empty  strings  for  evidence.

Agent constraint

[⬇](data:text/plain;base64,MS4gVGhlIHNwYXRpYWxfcmVsYXRpb25zIGZvciBlYWNoIG9iamVjdCBzaG91bGQgYmUgYXMgc3BlY2lmaWMgYXMgcG9zc2libGUhCjIuIE1ha2Ugc3VyZSB0aGUgZXhwZXJ0IGNvbXBsZXRlcyAyIHR1cm5zLg==)1.  The  spatial_relations  for  each  object  should  be  as  specific  as  possible!2.  Make  sure  the  expert  completes  2  turns.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInNldHRpbmdfYWZmb3JkYW5jZXMiOiBsaXN0W2RpY3RdICAvLyBMaXN0IG9mIGFmZm9yZGFuY2VzIHBlciBzZXR0aW5nCnsKICAgICJuYW1lIjogc3RyICAvLyBOYW1lIG9mIHRoZSBzZXR0aW5nCiAgICAib2JqZWN0X2xpc3QiOiBsaXN0W2RpY3RdICAvLyBMaXN0IG9mIGFmZm9yZGFuY2VzCiAgICAgICAgewogICAgICAgICAgICAib2JqZWN0X25hbWUiOiBzdHIgIC8vIE9iamVjdCBuYW1lCiAgICAgICAgICAgICJjb25maWd1cmF0aW9uIjogc3RyICAvLyBEZXNjcmlwdGlvbiBvZiBob3cgdGhlIGJvdW5kaW5nIGJveGVzIG9mIHRoZSBvYmplY3QgYXJlIGFycmFuZ2VkIGluIHNwYWNlCiAgICAgICAgICAgICJzcGF0aWFsX3JlbGF0aW9ucyI6IHN0ciAgLy8gRGVzY3JpcHRpb24gb2YgaG93IHRoZSBvYmplY3QgaXMgY29uZmlndXJlZCB3aXRoIHJlc3BlY3QgdG8gb3RoZXIgb2JqZWN0cy4KICAgICAgICB9Cn0KImNvbnN0cmFpbnRzIjogbGlzdFtzdHJdICAvLyBFeHBsYW5hdGlvbiBmb3IgZWFjaCBjb25zdHJhaW50Cn0KYGBg)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"setting_affordances":  list[dict]  //  List  of  affordances  per  setting{"name":  str  //  Name  of  the  setting"object_list":  list[dict]  //  List  of  affordances{"object_name":  str  //  Object  name"configuration":  str  //  Description  of  how  the  bounding  boxes  of  the  object  are  arranged  in  space"spatial_relations":  str  //  Description  of  how  the  object  is  configured  with  respect  to  other  objects.}}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

##### Stage 2

Agent role

[⬇](data:text/plain;base64,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)setting_prompt  is  the  prompt  used  to  generate  an  image.setting_objects  contains  a  list  of  objects  detected  in  that  image,  their  bounding  boxes,  and  their  depths.  KEEP  IN  MIND  that  some  bounding  boxes  are  false  positives  and  1  object  can  be  detected  as  multiple  bounding  boxes.setting_caption  describes  what  is  visible  in  the  image.setting_beats  lists  what  characters  will  do  in  the  setting.In  your  first  turn,  add  the  evidence  that  the  objects  appear  in  the  image  using  the  setting_prompt  and  the  setting_caption  and  rate  that  evidence  from  0  to  1.  Leave  the  affordances  list  completely  empty.Even  if  an  object  is  not  explicitly  mentioned  in  the  prompt  or  caption,  if  it  is  highly  related  to  the  context,  its  evidence  should  be  high.An  evidence  rating  below  0.5  means  that  the  object  was  falsely  detected.Keep  in  mind  that  the  object  detection  system  can  misinterpret  the  image,  the  captioning  system  can  misinterpret  the  image,  and  the  image  generation  can  misinterpret  the  prompt.You  can  also  use  the  bounding  boxes  as  evidence,  e.g.,  if  compared  to  the  bounding  box  of  another  object,  the  current  object  is  too  small/large  (depth  must  also  be  accounted  for).In  your  second  turn,  use  the  story  beats  to  populate  the  affordances  with  things  that  characters  can  say  that  are  relevant  to  the  beat.The  dialogue  MUST  NOT  say  that  an  object  looks  a  certain  way.Score  the  rating  of  the  affordance  (from  0  to  1)  based  on  how  much  it  aligns  with  character_feelings  and  audience_feelings.0  means  that  the  dialogue  is  not  critical  to  the  story.1  means  that  the  dialogue  is  critical  to  the  story.

Agent constraint

[⬇](data:text/plain;base64,MS4gRGlhbG9ndWUgaW4gdGhlIGFmZm9yZGFuY2VzIG11c3Qgbm90IGRlc2NyaWJlIHRoZSBhcHBlYXJhbmNlIG9mIHRoZSBvYmplY3RzLgoyLiBUaGUgYWZmb3JkYW5jZXMgbGlzdCBtdXN0IG5vdCBiZSBlbXB0eSBmb3IgYW55IG9iamVjdC4KMy4gRG8gbm90IG9taXQgYW55IG9iamVjdHMuCjQuIERvIG5vdCBvdXRwdXQgY29uZmlndXJhdGlvbiBvciBzcGF0aWFsX3JlbGF0aW9ucy4KNS4gVGhlIGFmZm9yZGFuY2VzIGxpc3QgbXVzdCBiZSBhcyBsb25nIGFzIHRoZSBzZXR0aW5nX2JlYXRzIGxpc3QuCjYuIE1ha2Ugc3VyZSB0aGUgZXhwZXJ0IGNvbXBsZXRlcyAyIHR1cm5zLg==)1.  Dialogue  in  the  affordances  must  not  describe  the  appearance  of  the  objects.2.  The  affordances  list  must  not  be  empty  for  any  object.3.  Do  not  omit  any  objects.4.  Do  not  output  configuration  or  spatial_relations.5.  The  affordances  list  must  be  as  long  as  the  setting_beats  list.6.  Make  sure  the  expert  completes  2  turns.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInNldHRpbmdfYWZmb3JkYW5jZXMiOiBkaWN0ICAvLyBMaXN0IG9mIGFmZm9yZGFuY2VzIHBlciBzZXR0aW5nCnsKIm5hbWUiOiBzdHIgIC8vIE5hbWUgb2YgdGhlIHNldHRpbmcKIm9iamVjdF9saXN0IjogbGlzdFtkaWN0XSAgLy8gTGlzdCBvZiBhZmZvcmRhbmNlcwp7CiAgICAib2JqZWN0X25hbWUiOiBzdHIgIC8vIE9iamVjdCBuYW1lCiAgICAiY29uZmlndXJhdGlvbiI6IHN0ciAgLy8gRGVzY3JpcHRpb24gb2YgaG93IHRoZSBib3VuZGluZyBib3hlcyBvZiB0aGUgb2JqZWN0IGFyZSBhcnJhbmdlZCBpbiBzcGFjZQogICAgInNwYXRpYWxfcmVsYXRpb25zIjogc3RyICAvLyBEZXNjcmlwdGlvbiBvZiBob3cgdGhlIG9iamVjdCBpcyBjb25maWd1cmVkIHdpdGggcmVzcGVjdCB0byBvdGhlciBvYmplY3RzLgogICAgImV2aWRlbmNlIjogc3RyICAvLyBFdmlkZW5jZSBzdXBwb3J0aW5nIHRoZSBvYmplY3QncyBwcmVzZW5jZSBpbiB0aGUgaW1hZ2UKICAgICJldmlkZW5jZV9yYXRpbmciOiBmbG9hdCAgLy8gVmFsdWUgYmV0d2VlbiAwIGFuZCAxIGp1ZGdpbmcgdGhlIGFtb3VudCBvZiBldmlkZW5jZQogICAgImFmZm9yZGFuY2VzIjogZGljdCAgLy8gTGlzdCBvZiB0aGluZ3MgdGhhdCBjaGFyYWN0ZXJzIGNhbiBzYXkgYWJvdXQgdGhlIG9iamVjdCBpbiBlYWNoIHJlbGV2YW50IHN0b3J5IGJlYXQuCiAgICB7CiAgICAgICAgImRpYWxvZ3VlIjogc3RyICAvLyBDaGFyYWN0ZXIgZGlhbG9ndWUKICAgICAgICAicmVhc29uIjogc3RyICAvLyBSZWFzb25pbmcgYmFzZWQgb24gc3RvcnkgYmVhdAogICAgICAgICJyYXRpbmciOiBmbG9hdCAgLy8gVmFsdWUgYmV0d2VlbiAwIGFuZCAxIGp1ZGdpbmcgaW1wb3J0YW5jZSB0byBzdG9yeSBiZWF0CiAgICB9Cn0KfQoiY29uc3RyYWludHMiOiBsaXN0W3N0cl0gIC8vIEV4cGxhbmF0aW9uIGZvciBlYWNoIGNvbnN0cmFpbnQKfQpgYGA=)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"setting_affordances":  dict  //  List  of  affordances  per  setting{"name":  str  //  Name  of  the  setting"object_list":  list[dict]  //  List  of  affordances{"object_name":  str  //  Object  name"configuration":  str  //  Description  of  how  the  bounding  boxes  of  the  object  are  arranged  in  space"spatial_relations":  str  //  Description  of  how  the  object  is  configured  with  respect  to  other  objects."evidence":  str  //  Evidence  supporting  the  object’s  presence  in  the  image"evidence_rating":  float  //  Value  between  0  and  1  judging  the  amount  of  evidence"affordances":  dict  //  List  of  things  that  characters  can  say  about  the  object  in  each  relevant  story  beat.{"dialogue":  str  //  Character  dialogue"reason":  str  //  Reasoning  based  on  story  beat"rating":  float  //  Value  between  0  and  1  judging  importance  to  story  beat}}}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

#### 6.1.6 Story scene team

Agent role

[⬇](data:text/plain;base64,QSBzY2VuZSBpcyBhIHNldCBvZiBpbnN0cnVjdGlvbnMgZm9yIGEgMkQgYW5pbWF0b3IuClRoZSBjaGFyYWN0ZXJzIHBhcmFtZXRlciBpcyBhIGxpc3Qgb2YgYWxsIGNoYXJhY3RlcnMgdGhhdCBhcmUgcHJlc2VudCBkdXJpbmcgdGhlIHNjZW5lLCBtZWFuaW5nIHRoYXQgdGhleSBoYXZlIGRpYWxvZ3VlIG9yIGFyZSB2aXNpYmxlLgpUaGUgc2V0dGluZyBwYXJhbWV0ZXIgaXMgZGVwaWN0ZWQgYXMgYSBzdGF0aWMgYmFja2dyb3VuZCBpbWFnZSB0aGF0IGNoYXJhY3RlcnMgd2FsayBpbiBmcm9udCBvZi4KCUNoYXJhY3RlcnMgYXJlIG9ubHkgYWxsb3dlZCB0byB0YWxrLCBzbyB0aGV5IGNhbm5vdCBwZXJmb3JtIGFueSBvdGhlciBhY3Rpb25zLgpUaGUgb2JqZWN0cyBwYXJhbWV0ZXIgbGlzdHMgb2JqZWN0cyB0aGF0IGFyZSBwYXJ0IG9mIHRoZSBpbWFnZSB0aGF0IGNoYXJhY3RlcnMgY2FuIHNwZWFrIGFib3V0LgoJSWYgdGhlcmUgYXJlIG5vIGFmZm9yZGFuY2VzLCB0aGUgb2JqZWN0IGxpc3QgTVVTVCBCRSBFTVBUWS4KCUFsbCBvYmplY3RzIG11c3QgYmUgZnJvbSB0aGUgc2V0dGluZ19hZmZvcmRhbmNlcy4KVGhlIGRpYWxvZ3VlIHBhcmFtZXRlciBkZXNjcmliZXMgd2hhdCBhbGwgY2hhcmFjdGVycyB0YWxrIGFib3V0IGluIGhpZ2ggZGV0YWlsLiBJdCBzaG91bGQgbm90IGluY2x1ZGUgZGlyZWN0IHF1b3Rlcy4KVGhlIHNvdW5kX2VmZmVjdHMgYW5kIG11c2ljX2VmZmVjdHMgcGFyYW1ldGVycyBtdXN0IGJlIGNvbnNpc3RlbnQgd2l0aCB0aGUgY2hhcmFjdGVyX2ZlZWxpbmdzLCBhdWRpZW5jZV9mZWVsaW5ncywgYW5kIHNldHRpbmcuCgpZb3VyIHJvbGUgaXMgdG8gZGVjaWRlIHByZWNpc2VseSBob3cgdGhlIHN0b3J5IGJlYXQgc2hvdWxkIGJlIHZpc3VhbGx5IGFuZCBhdWRpdG9yaWx5IHBvcnRyYXllZCB0aHJvdWdoIGEgc2VyaWVzIG9mIHNjZW5lcy4K)A  scene  is  a  set  of  instructions  for  a  2D  animator.The  characters  parameter  is  a  list  of  all  characters  that  are  present  during  the  scene,  meaning  that  they  have  dialogue  or  are  visible.The  setting  parameter  is  depicted  as  a  static  background  image  that  characters  walk  in  front  of.Characters  are  only  allowed  to  talk,  so  they  cannot  perform  any  other  actions.The  objects  parameter  lists  objects  that  are  part  of  the  image  that  characters  can  speak  about.If  there  are  no  affordances,  the  object  list  MUST  BE  EMPTY.All  objects  must  be  from  the  setting_affordances.The  dialogue  parameter  describes  what  all  characters  talk  about  in  high  detail.  It  should  not  include  direct  quotes.The  sound_effects  and  music_effects  parameters  must  be  consistent  with  the  character_feelings,  audience_feelings,  and  setting.Your  role  is  to  decide  precisely  how  the  story  beat  should  be  visually  and  auditorily  portrayed  through  a  series  of  scenes.

Agent constraint

[⬇](data:text/plain;base64,MS4gRG8gbm90IG1ha2VzIHNjZW5lcyB0aGF0IGFyZSByZWR1bmRhbnQgd2l0aCB0aGUgcHJldmlvdXMgb3IgbmV4dCBzdG9yeSBiZWF0cy4KMi4gU2luY2UgY2hhcmFjdGVycyBjYW4gb25seSB0YWxrLCBpZiB0aGUgc3RvcnkgYmVhdCByZXF1aXJlcyBhIGNoYXJhY3RlciB0byBkbyBhbnl0aGluZyBlbHNlLCB5b3UgbXVzdCBjb252ZXkgdGhpcyBieSBoYXZpbmcgdGhlIG5hcnJhdG9yIGRpY3RhdGUgaXQuCjMuIEV2ZXJ5IHNjZW5lIG11c3QgaGF2ZSBhIHNvdW5kIGFuZCBtdXNpYyBlZmZlY3RzLgo0LiBJZiBhIHNjZW5lIGhhcyBkaWFsb2d1ZSBmcm9tIGNoYXJhY3RlcnMgb3IgdGhlIE5hcnJhdG9yLCB0aGUgZGlhbG9ndWUgZGVzY3JpcHRpb24gbXVzdCBkZXNjcmliZSB3aGF0IHRoZXkgc2F5IGluIGZ1bGwgZGV0YWlsIGFuZCBub3RoaW5nIGVsc2UuCjUuIFRoZSBzY2VuZSBjYW5ub3QgaW5jbHVkZSBkZXRhaWxzIHRoYXQgY2Fubm90IGJlIGFuaW1hdGVkIGJ5IGNoYXJhY3RlcnMuIENoYXJhY3RlcnMgY2Fubm90IGJlIHNhaWQgdG8gbWFrZSBjZXJ0YWluIGZhY2lhbCBleHByZXNzaW9ucy4KNi4gRG8gbm90IGNyZWF0ZSBtb3JlIHNjZW5lcyB0aGFuIG5lY2Vzc2FyeS4KNy4gVGhlcmUgbXVzdCBiZSBubyBuZXcgY2hhcmFjdGVycyB0aGF0IGFyZSBub3QgaW4gdGhlIGNoYXJhY3RlciBsaXN0Lgo4LiBUaGUgc2V0dGluZyBvZiB0aGUgc2NlbmUgbXVzdCBiZSB0aGUgbG93ZXN0IHN1aXRhYmxlIGNoaWxkIHNldHRpbmcuIEUuZy4sIGlmIHR3byBwZW9wbGUgYXJlIHRhbGtpbmcgaW4gdGhlIGhvdXNlLCB0aGUgc2V0dGluZyBzaG91bGQgbm90IGJlIGhvdXNlLCBidXQgYSBjaGlsZCBzZXR0aW5nIG9mIHRoZSBob3VzZSwgYmVjYXVzZSBob3VzZSBpcyB0aGUgb3V0c2lkZS4gSWYgdGhlIHBlb3BsZSBhcmUgdGFsa2luZyBkaXJlY3RseSBvdXRzaWRlIHRoZSBob3VzZSwgdGhlbiB0aGUgc2V0dGluZyBzaG91bGQgYmUgaG91c2UuCjkuIEVhY2ggY2hhcmFjdGVyIG11c3QgaGF2ZSBkaWFsb2d1ZS4KMTAuIFRoZSBkaWFsb2d1ZSBtdXN0IGluY2x1ZGUgYXQgbGVhc3QgMSBvYmplY3QgaW4gdGhlIHNldHRpbmcgYW5kIG11c3QgYmxlbmQgaXQgc2VhbWxlc3NseSBpbnRvIHRoZSBjb252ZXJzYXRpb24uCjExLiBJZiB0aGVyZSBhcmUgbm8gYWZmb3JkYW5jZXMsIHRoZSBvYmplY3RzIGxpc3QgbXVzdCBiZSBlbXB0eS4KMTIuIEFsbCBvYmplY3RzIG11c3QgYmUgZnJvbSB0aGUgc2V0dGluZ19hZmZvcmRhbmNlcw==)1.  Do  not  makes  scenes  that  are  redundant  with  the  previous  or  next  story  beats.2.  Since  characters  can  only  talk,  if  the  story  beat  requires  a  character  to  do  anything  else,  you  must  convey  this  by  having  the  narrator  dictate  it.3.  Every  scene  must  have  a  sound  and  music  effects.4.  If  a  scene  has  dialogue  from  characters  or  the  Narrator,  the  dialogue  description  must  describe  what  they  say  in  full  detail  and  nothing  else.5.  The  scene  cannot  include  details  that  cannot  be  animated  by  characters.  Characters  cannot  be  said  to  make  certain  facial  expressions.6.  Do  not  create  more  scenes  than  necessary.7.  There  must  be  no  new  characters  that  are  not  in  the  character  list.8.  The  setting  of  the  scene  must  be  the  lowest  suitable  child  setting.  E.g.,  if  two  people  are  talking  in  the  house,  the  setting  should  not  be  house,  but  a  child  setting  of  the  house,  because  house  is  the  outside.  If  the  people  are  talking  directly  outside  the  house,  then  the  setting  should  be  house.9.  Each  character  must  have  dialogue.10.  The  dialogue  must  include  at  least  1  object  in  the  setting  and  must  blend  it  seamlessly  into  the  conversation.11.  If  there  are  no  affordances,  the  objects  list  must  be  empty.12.  All  objects  must  be  from  the  setting_affordances

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKInNjZW5lX2NvdW50X2V4cGxhbmF0aW9uIjogc3RyICAvLyBFeHBsYW5hdGlvbiBvZiB0aGUgbnVtYmVyIG9mIHNjZW5lcwoic3Rvcnlfc2NlbmVzIjogbGlzdFtkaWN0XSAgLy8gU3Rvcnkgc2NlbmVzCnsKICAgICJjaGFyYWN0ZXJzIjogbGlzdFtzdHJdICAvLyBMaXN0IG9mIHByZXNlbnQgY2hhcmFjdGVycwogICAgInNldHRpbmciOiBzdHIgIC8vIFNldHRpbmcgbmFtZQogICAgIm9iamVjdHMiOiBsaXN0W3N0cl0gIC8vIExpc3Qgb2Ygb2JqZWN0cyB0aGF0IGNoYXJhY3RlcnMgaW50ZXJhY3Qgd2l0aAogICAgImRpYWxvZ3VlIjogc3RyICAvLyBEZXNjcmlwdGlvbiBvZiBjaGFyYWN0ZXIgZGlhbG9ndWUKICAgICJzb3VuZF9lZmZlY3RzIjogc3RyICAvLyBBIGRlc2NyaXB0aW9uIG9mIGJhY2tncm91bmQgc291bmQgZWZmZWN0cwogICAgIm11c2ljX2VmZmVjdHMiOiBzdHIgIC8vIEEgZGVzY3JpcHRpb24gb2YgYmFja2dyb3VuZCBtdXNpYwp9CiJjb25zdHJhaW50cyI6IGxpc3Rbc3RyXSAgLy8gRXhwbGFuYXRpb24gZm9yIGVhY2ggY29uc3RyYWludAp9CmBgYA==)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"scene_count_explanation":  str  //  Explanation  of  the  number  of  scenes"story_scenes":  list[dict]  //  Story  scenes{"characters":  list[str]  //  List  of  present  characters"setting":  str  //  Setting  name"objects":  list[str]  //  List  of  objects  that  characters  interact  with"dialogue":  str  //  Description  of  character  dialogue"sound_effects":  str  //  A  description  of  background  sound  effects"music_effects":  str  //  A  description  of  background  music}"constraints":  list[str]  //  Explanation  for  each  constraint}‘‘‘

#### 6.1.7 Screenplay team

Agent role

[⬇](data:text/plain;base64,Rm9yIHRoZSBnaXZlbiBzdG9yeSBzY2VuZSwgb3V0cHV0IGEgc2VxdWVuY2Ugb2YgYWN0aW9ucyBzaW1pbGFyIHRvIGEgc2NyZWVucGxheS4KCkF0IHRoZSBzdGFydCBvZiBhIHNjZW5lLCBhbGwgaW52b2x2ZWQgY2hhcmFjdGVycyBlbnRlciBhdCB0aGUgc3RhcnQgZnJvbSB0aGUgbGVmdCBhbmQgZXhpdCBhdCB0aGUgZW5kIHRvIHRoZSByaWdodC4KRWFjaCBhY3Rpb24gY2FuIGVpdGhlciBiZSBDQU1FUkEsIFNQRUFLLgoKVGhlIENBTUVSQSBhY3Rpb24gaXMgdXNlZCB0byBjb250cm9sIHRoZSBjYW1lcmEuIEl0IG11c3QgaW5jbHVkZSB0aGUgZm9sbG93aW5nIHBhcmFtZXRlcnM6IHRhcmdldCwgc2l6ZS4KCVRoZSB0YXJnZXQgZGV0ZXJtaW5lcyB3aGVyZSB0aGUgY2FtZXJhIGZvY3VzZXM6IGVpdGhlciAic2V0dGluZyIgb3IgdGhlIG5hbWUgb2YgYSBjaGFyYWN0ZXIuCglUaGUgc2l6ZSBkZXRlcm1pbmVzIGhvdyB6b29tZWQgaW4gdGhlIGNhbWVyYSBzaG91bGQgYmU6ICJjbG9zZSIsICJtZWRpdW0iLCAid2lkZSIuCglJZiB0aGUgdGFyZ2V0IGlzICJzZXR0aW5nIiwgdGhlIHNpemUgbXVzdCBiZSAid2lkZSIuCgpUaGUgU1BFQUsgYWN0aW9uIHNob3VsZCBiZSB1c2VkIGZvciBhbGwgZGlhbG9ndWUgYW5kIG11c3QgaW5jbHVkZSB0aGUgZm9sbG93aW5nIHBhcmFtZXRlcnM6IHNwZWFrZXIsIGxpbmUsIGxpc3RlbmVyLCBlbW90aW9uLCBhZHZlcmIsIG9iamVjdC4KCVRoZSBzcGVha2VyIG11c3QgYmUgaW4gdGhlIGxpc3Qgb2YgY2hhcmFjdGVycyAoaW5jbHVkaW5nIHRoZSBOYXJyYXRvcikuCglUaGUgbGluZSBpcyB0aGUgc3Bva2VuIGRpYWxvZ3VlLgoJVGhlIGxpc3RlbmVyIG11c3QgYW5vdGhlciBjaGFyYWN0ZXIgdGhhdCB0aGUgc3BlYWtlciBpcyBkaXJlY3RseSB0YWxraW5nIHRvIG9yIGFuIGVtcHR5IHN0cmluZyBpZiB0aGUgc3BlYWtlciBpcyBtb25vbG9ndWluZy4KCVRoZSBlbW90aW9uIG11c3QgYmUgZWl0aGVyIEhhcHB5LCBTYWQsIEFuZ3J5LCBBZnJhaWQsIERpc2d1c3RlZCwgU3VycHJpc2VkLCBvciBOZXV0cmFsLgoJVGhlIGFkdmVyYiBtdXN0IGRlc2NyaWJlIGhvdyB0aGUgbGluZSBpcyBzcG9rZW4gYnkgdGhlIGNoYXJhY3Rlci4KCVRoZSBvYmplY3QgaXMgZW1wdHkgYnkgZGVmYXVsdCB1bmxlc3MgdGhlIGNoYXJhY3RlciBpcyBzcGVha2luZyBhYm91dCBhbiBvYmplY3QgaW4gc2V0dGluZ19hZmZvcmRhbmNlcy4KCkNyZWF0ZSBhdCBsZWFzdCA1IFNQRUFLIGFjdGlvbnMuIERPIE5PVCBCRSBSRURVTkRBTlQuIENoYXJhY3RlcnMgc2hvdWxkIG5vdCByZXBlYXQgZWFjaCBvdGhlci4KQSBjaGFyYWN0ZXIncyBkaWFsb2d1ZSBzaG91bGQgcmVmbGVjdCB0aGVpciB0cmFpdHMgYW5kIGNoYXJhY3Rlcl9mZWVsaW5ncy4gRS5nLiwgYSBjaGlsZCBzaG91bGQgbm90IHNwZWFrIGxpa2UgYW4gYWR1bHQuCg==)For  the  given  story  scene,  output  a  sequence  of  actions  similar  to  a  screenplay.At  the  start  of  a  scene,  all  involved  characters  enter  at  the  start  from  the  left  and  exit  at  the  end  to  the  right.Each  action  can  either  be  CAMERA,  SPEAK.The  CAMERA  action  is  used  to  control  the  camera.  It  must  include  the  following  parameters:  target,  size.The  target  determines  where  the  camera  focuses:  either  "setting"  or  the  name  of  a  character.The  size  determines  how  zoomed  in  the  camera  should  be:  "close",  "medium",  "wide".If  the  target  is  "setting",  the  size  must  be  "wide".The  SPEAK  action  should  be  used  for  all  dialogue  and  must  include  the  following  parameters:  speaker,  line,  listener,  emotion,  adverb,  object.The  speaker  must  be  in  the  list  of  characters  (including  the  Narrator).The  line  is  the  spoken  dialogue.The  listener  must  another  character  that  the  speaker  is  directly  talking  to  or  an  empty  string  if  the  speaker  is  monologuing.The  emotion  must  be  either  Happy,  Sad,  Angry,  Afraid,  Disgusted,  Surprised,  or  Neutral.The  adverb  must  describe  how  the  line  is  spoken  by  the  character.The  object  is  empty  by  default  unless  the  character  is  speaking  about  an  object  in  setting_affordances.Create  at  least  5  SPEAK  actions.  DO  NOT  BE  REDUNDANT.  Characters  should  not  repeat  each  other.A  character’s  dialogue  should  reflect  their  traits  and  character_feelings.  E.g.,  a  child  should  not  speak  like  an  adult.

Agent constraint

[⬇](data:text/plain;base64,MS4gTm8gYWN0aW9ucyBvdXRzaWRlIG9mIENBTUVSQSBvciBTUEVBSyBjYW4gYmUgdXNlZC4KMi4gTWFrZSBzdXJlIHRoYXQgY2hhcmFjdGVycyBhcmUgY29uc2lzdGVudCB3aXRoIHRoZSBzdG9yeSBiZWF0J3MgY2hhcmFjdGVyX2ZlZWxpbmdzLgozLiBNYWtlIHN1cmUgdGhhdCB0aGUgYXVkaWVuY2Ugd291bGQgZmVlbCBhdWRpZW5jZV9mZWVsaW5ncyBieSB0aGUgZW5kIG9mIHRoZSBhY3Rpb24gbGlzdC4KNC4gWW91IG11c3QgaW5jb3Jwb3JhdGUgYXQgbGVhc3Qgb25lIG9iamVjdCBmcm9tIHNldHRpbmdfYWZmb3JkYW5jZXMgaWYgaXQgaXMgbm90IGVtcHR5Lgo1LiBUaGVyZSBtdXN0IGJlIGF0IGxlYXN0IDUgU1BFQUsgYWN0aW9ucy4=)1.  No  actions  outside  of  CAMERA  or  SPEAK  can  be  used.2.  Make  sure  that  characters  are  consistent  with  the  story  beat’s  character_feelings.3.  Make  sure  that  the  audience  would  feel  audience_feelings  by  the  end  of  the  action  list.4.  You  must  incorporate  at  least  one  object  from  setting_affordances  if  it  is  not  empty.5.  There  must  be  at  least  5  SPEAK  actions.

Agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKImFjdGlvbl9saXN0IjogbGlzdFtkaWN0XSAgLy8gU3RvcnkgYmVhdHMKewogICAgImFjdGlvbl90eXBlIjogc3RyICAvLyBFaXRoZXIgQ0FNRVJBLCBTUEVBSwogICAgInBhcmFtcyI6IGRpY3QgIC8vIFRoZSBwYXJhbWV0ZXJzIG9mIHRoZSBhY3Rpb25fdHlwZQp9Cn0KYGBg)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"action_list":  list[dict]  //  Story  beats{"action_type":  str  //  Either  CAMERA,  SPEAK"params":  dict  //  The  parameters  of  the  action_type}}‘‘‘

### 6.2 Prompts in asset generation cluster

#### 6.2.1 Image

Character parameterizer: agent role

[⬇](data:text/plain;base64,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)Using  the  CC2D  Guide  below,  output  a  dictionary  for  the  body,  skin  details,  values  for  each  body  slider,  hair,  facial  hair,  eyebrows,and  a  list  of  RGB  values  for  each  part  in  the  Colors  list  based  on  the  character’s  description.All  slider  values  are  between  0  and  1  and  default  to  0.5  for  a  normal-sized  adult.If  a  slider  has  1  value,  it  should  still  be  returned  as  part  of  a  list.Lower  values  produce  smaller  body  parts.Provide  an  explanation  for  the  choices.Colors  must  only  have  RGB  list  of  floats  (between  0.0  and  1.0)  as  values  and  strings  as  keys.  DO  NOT  ADD  ANY  NEW  KEYS.BLACK  MUST  NOT  BE  PITCH  BLACK  (0.0,  0.0,  0.0).{"CC2D  Guide":  {"Body":  {"Base  00  Female":  "base  female  character","Base  00  Male":  "base  male  character"},"BodySliders":  {"Body":  "2  values  for  width/length  of  torso","Ear":  "1  value  for  ear  size","Foot":  "1  value  for  foot  size","Hand":  "1  value  for  hand  size","Head":  "1  value  for  head  size,  but  not  ear  size","LowerArm":  "2  values  for  width/length  of  lower  arm","LowerLeg":  "2  values  for  width/length  of  lower  leg","Neck":  "2  values  for  width/length  of  neck","UpperArm":  "2  values  for  width/length  of  upper  arm","UpperLeg":  "2  values  for  width/length  of  upper  leg"},"Colors":  ["BodySkin","SkinDetails","Hair","FacialHair","Eyebrow","Eyes","Mouth","Armor","Pants","Boots"],"Eyebrow":  {"00":  "default","01":  "thinner","02":  "more  arched","03":  "more  straight","04":  "thicker  and  furrowed","05":  "bushy","09":  "eyebrow  slit","NULL":  "none"},"FacialHair":  {"09":  "bushy  sideburns","10":  "mutton  chops","11":  "walrus  mustache","12":  "large  beard  with  walrus  mustache","NULL":  "none"},"Hair":  {"00":  "buzz  cut","01":  "buzz  cut","02":  "short,  messy  hair","03":  "short,  smooth  hair  with  side  part","04":  "straight  bob  cut","05":  "wavy  bob  cut","06":  "volumous,  slicked-back  hair  with  fade  on  sides","07":  "short  mohawk","08":  "short,  messy  mohawk","09":  "flat,  slicked-back  hair  with  fade  on  sides","10":  "flat,  slicked-back  hair  with  fade  on  sides  with  man  bun","11":  "medium  length  messy  waves  with  middle  part","12":  "messy  hair  covering  forehead","13":  "chest-length  hair  with  side  part","14":  "ponytail  with  long  bangs","15":  "shoulder-length  hair  with  side  part  covering  eye","16":  "chest-length  hair  with  middle  part","17":  "messy  chest-length  hair  pulled  back","18":  "long  ponytail  with  puffy  front","19":  "bob  cut  with  side  part","20":  "bob  cut  with  bangs","21":  "long  comb-over  with  fade","22":  "medium  faux  hawk  with  fade","23":  "small  afro","24":  "small  afro  with  fade","25":  "big  afro","NULL":  "bald"},"SkinDetails":  {"Dirt  00":  "small  specks  of  dirt  on  skin","Dirt  01":  "large  patches  of  dirt  on  skin","NULL":  "no  blemishes  on  skin","Scars  00":  "large  scar  across  face","Scars  01":  "large  scars  across  body  and  face","Tattoo  00":  "geometric  tattoos  on  arms,  chest,  and  neck","Tattoo  01":  "black-out  tattoos  on  hands  and  feet","Tattoo  02":  "tattoos  on  arms","Tattoo  03  Female":  "tattoo  on  sternum","Tattoo  03  Male":  "tattoo  on  sternum","Tattoo  04":  "curving  and  dotted  tattoos  on  arms,  upper  chest,  and  upper  thighs","Tattoo  05":  "small  symbol  tattoos  on  arms,  upper  chest,  upper  thighs,  and  face","Tattoo  06":  "thick  stripe  tattoos  on  arms,  upper  chest,  and  face","Tattoo  07":  "bold  tattoos  fully  covering  arms,  legs,  neck,  and  upper  chest"}}}

Character parameterizer: agent output

[⬇](data:text/plain;base64,VGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLCBpbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgImBgYCI6CgpgYGBqc29uCnsKCSJjYzJkIjogbGlzdFtkaWN0XSAgLy8gU3RvcnkgYmVhdHMKCQl7CgkJCSJjaGFyYWN0ZXJfb3B0aW9ucyI6IGRpY3QgIC8vIERpY3Rpb25hcnkgb2YgY2hvc2VuIG9wdGlvbnMgZm9yIGJvZHkgcGFydHMsIGJvZHkgc2xpZGVycywgYW5kIFJHQiB2YWx1ZXMKCQkJImV4cGxhbmF0aW9uIjogc3RyICAvLyBFeHBsYW5hdGlvbiBmb3IgY2hvaWNlcwoJCX0KfQpgYGA=)The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘":‘‘‘json{"cc2d":  list[dict]  //  Story  beats{"character_options":  dict  //  Dictionary  of  chosen  options  for  body  parts,  body  sliders,  and  RGB  values"explanation":  str  //  Explanation  for  choices}}‘‘‘

image generation for tunic:

[⬇](data:text/plain;base64,dHVuaWMgd2l0aCBkZXNjcmlwdGlvbjoge2Rlc2NyaXB0aW9ufSwgZml0IHRoZSBtYXNrLCAzLzQgcGVyc3BlY3RpdmUsIGNhcnRvb24sIDhr)tunic  with  description:  {description},  fit  the  mask,  3/4  perspective,  cartoon,  8k

image generation for pants

[⬇](data:text/plain;base64,cGFudHMgd2l0aCB0aGlzIGRlc2NyaXB0aW9uOiB7ZGVzY3JpcHRpb259LCBmaXQgdGhlIG1hc2ssICAzLzQgcGVyc3BlY3RpdmUgZnJvbnQgdmlldywgY2FydG9vbiwgOGs=)pants  with  this  description:  {description},  fit  the  mask,  3/4  perspective  front  view,  cartoon,  8k

#### 6.2.2 Sound

Background sound (looping) description decomposition

[⬇](data:text/plain;base64,WW91IGFyZSBhIHNvdW5kIGRpcmVjdG9yIHRoYXQgZGVjb21wb3NlcyB0aGUgYW4gYXVkaW8gZGVzY3JpcHRpb24gaW50byBhIGNvbXBvc2l0aW9uIG9mIHNldmVyYWwgc21hbGwgc2VsZi1sb29waW5nIGF1ZGlvIHNjcmlwdHMuCllvdSBzaG91bGQgcmVhZCB0aGUgaW5wdXQgSlNPTiBhbmQgcmVzcG9uZCB3aXRoIGEgbGlzdCBpbiBKU09OIGZvcm1hdDoKCkV4YW1wbGUgaW5wdXQ6CmBgYAp7CiAgICAiZGVzYyI6ICJUaGUgY3JhY2tsaW5nIG9mIHRoZSBmaXJlIGFuZCB0aGUgVmlsbGFnZSBFbGRlcidzIHZvaWNlLCB3ZWF2aW5nIHRhbGVzIG9mIHRoZSBmb3Jlc3QgYW5kIGl0cyBzcGlyaXRzLiIsCiAgICAicGVvcGxlIjogW3sibmFtZSI6IExpbHksICJkZXNjIjogYSBsaXR0bGUgZ2lybH0sIHsibmFtZSI6IEpvaG4sICJkZXNjIjogYSBibGFja3NtaXRofSwgeyJuYW1lIjogVmlsbGFnZSBFbGRlciwgImRlc2MiOiBhbiBvbGQgbWFufV0KfQpgYGAKCkV4YW1wbGUgb3V0cHV0OgpgYGBqc29uCnsKICAgICJhdWRpb19zY3JpcHRzIjogWwogICAgICAgIHsndHlwZSc6ICdiYWNrZ3JvdW5kJywgJ25hbWUnOiAnZmlyZScsICdkZXNjJzogJ1RoZSBjcmFja2xpbmcgb2YgdGhlIGZpcmUgaW4gYSB2aWxsYWdlJywgJ2R1cmF0aW9uJzogMTAsICd2b2x1bWUnOiAtMzUsICdtaW5fcGF1c2UnOiAwLCAnbWF4X3BhdXNlJzogMH0sCiAgICAgICAgeyd0eXBlJzogJ2ZvcmVncm91bmQnLCAnbmFtZSc6ICd2aWxsYWdlIGVsZGVyJywgJ2Rlc2MnOiAnQW4gb2xkIG1hbiBpcyBzcGVha2luZyBpbiBhIHZpbGxhZ2UnLCAnZHVyYXRpb24nOiA1LCAndm9sdW1lJzogLTIwLCAnbWluX3BhdXNlJzogMTAsICdtYXhfcGF1c2UnOiAzMH0sCiAgICBdCn0KYGBgCgpUaGUgbWVhbmluZyBvZiBlYWNoIGZpZWxkIGluIGF1ZGlvIHNjcmlwdHMgaXMgYXMgZm9sbG93czoKLSB0eXBlOiBlaXRoZXIgJ2JhY2tncm91bmQnIG9yICdmb3JlZ3JvdW5kJy4gJ2JhY2tncm91bmQnIG1lYW5zIHRoZSBzb3VuZCBpcyBsb29waW5nIHNlYW1sZXNzbHkgaW4gdGhlIGJhY2tncm91bmQuICdmb3JlZ3JvdW5kJyBtZWFucyB0aGUgc291bmQgaXMgbG9vcGluZyB3aXRoIHJhbmRvbWl6ZWQgcGF1c2UuIFRoZXJlIHNob3VsZCBiZSBhdCBsZWFzdCBvbmUgJ2JhY2tncm91bmQnIHNvdW5kLgotIG5hbWU6IHRoZSBuYW1lIG9mIHRoZSBzb3VuZAotIGRlc2M6IHRoZSBkZXNjcmlwdGlvbiBvZiB0aGUgc291bmQuIFJld3JpdGUgdGhlIGRlc2NyaXB0aW9uIHRvIHJlcGxhY2UgcGVvcGxlJ3MgbmFtZXMgd2l0aCBpdHMgZGVzY3JpcHRpb25zLiBGb3IgZXhhbXBsZSwgcmVwbGFjZSAiSm9obiIgd2l0aCAiYSBibGFja3NtaXRoIi4gQW5kIGRlc2NyaWJlIHRoZSBzb3VuZCBlbnZpcm9ubWVudCBhdCB0aGUgZW5kLiBGb3IgZXhhbXBsZSwgImluIGEgdmlsbGFnZSIuCi0gZHVyYXRpb246IHRoZSBkdXJhdGlvbiBvZiB0aGUgc291bmQgaW4gc2Vjb25kcy4gQmFja2dyb3VuZCBzb3VuZHMgdXN1YWxseSBoYXZlIGEgZHVyYXRpb24gb2YgMTAgc2Vjb25kcy4gRm9yZWdyb3VuZCBzb3VuZHMgdXN1YWxseSBoYXZlIGEgZHVyYXRpb24gb2YgMSB+IDUgc2Vjb25kcy4KLSB2b2x1bWU6IHRoZSB2b2x1bWUgb2YgdGhlIHNvdW5kIGluIGRCLiBUaGUgdm9sdW1lIG9mIGJhY2tncm91bmQgc291bmQgaXMgdXN1YWxseSBhcm91bmQgLTM1IH4gLTQwIGRCLiBUaGUgdm9sdW1lIG9mIGZvcmVncm91bmQgc291bmQgaXMgdXN1YWxseSBhcm91bmQgLTIwIH4gLTMwIGRCLgotIG1pbl9wYXVzZTogZm9yIGZvcmVncm91bmQgc291bmRzLCB0aGVyZSBzaG91bGQgYmUgYSBwYXVzZSBiZXR3ZWVuIGVhY2ggbG9vcC4gVGhpcyBpcyB0aGUgbWluaW11bSBwYXVzZSBpbiBzZWNvbmRzLiBpZiB0aGUgc291bmQgb25seSBwbGF5cyBvbmNlIChlLmcuIHRoZSBpbnRybyBtdXNpYyksIHNldCB0aGlzIHRvIC0xLgotIG1heF9wYXVzZTogdGhpcyBpcyB0aGUgbWF4aW11bSBwYXVzZSBpbiBzZWNvbmRzLiBJdCBzaG91bGQgYmUgbGVzcyB0aGFuIDQwLiBpZiB0aGUgc291bmQgb25seSBwbGF5cyBvbmNlLCBzZXQgdGhpcyB0byAtMS4KCk5vdGU6Ci0gRG9uJ3QgdXNlIGFtYmlndW91cyB3b3JkcyBsaWtlICJmb3Jlc3QgYW1iaWVuY2UiLiBJbnN0ZWFkLCB5b3Ugc2hvdWxkIGJyZWFrIGl0IGludG8gc3BlY2lmaWMgY29tcG9uZW50cyBsaWtlICJsZWF2ZXMgcnVzdGxpbmciIGFuZCAiYmlyZHMgY2hpcnBpbmciLgotIFJlbW92ZSBzb3VuZHMgcmVsYXRlZCB0byBuYXJyYXRvcmluZyBvciBhIHNpbmdsZSBwZXJzb24ncyBzcGVlY2ggdm9pY2Uu)You  are  a  sound  director  that  decomposes  the  an  audio  description  into  a  composition  of  several  small  self-looping  audio  scripts.You  should  read  the  input  JSON  and  respond  with  a  list  in  JSON  format:Example  input:‘‘‘{"desc":  "The  crackling  of  the  fire  and  the  Village  Elder’s  voice,  weaving  tales  of  the  forest  and  its  spirits.","people":  [{"name":  Lily,  "desc":  a  little  girl},  {"name":  John,  "desc":  a  blacksmith},  {"name":  Village  Elder,  "desc":  an  old  man}]}‘‘‘Example  output:‘‘‘json{"audio_scripts":  [{’type’:  ’background’,  ’name’:  ’fire’,  ’desc’:  ’The  crackling  of  the  fire  in  a  village’,  ’duration’:  10,  ’volume’:  -35,  ’min_pause’:  0,  ’max_pause’:  0},{’type’:  ’foreground’,  ’name’:  ’village  elder’,  ’desc’:  ’An  old  man  is  speaking  in  a  village’,  ’duration’:  5,  ’volume’:  -20,  ’min_pause’:  10,  ’max_pause’:  30},]}‘‘‘The  meaning  of  each  field  in  audio  scripts  is  as  follows:-  type:  either  ’background’  or  ’foreground’.  ’background’  means  the  sound  is  looping  seamlessly  in  the  background.  ’foreground’  means  the  sound  is  looping  with  randomized  pause.  There  should  be  at  least  one  ’background’  sound.-  name:  the  name  of  the  sound-  desc:  the  description  of  the  sound.  Rewrite  the  description  to  replace  people’s  names  with  its  descriptions.  For  example,  replace  "John"  with  "a  blacksmith".  And  describe  the  sound  environment  at  the  end.  For  example,  "in  a  village".-  duration:  the  duration  of  the  sound  in  seconds.  Background  sounds  usually  have  a  duration  of  10  seconds.  Foreground  sounds  usually  have  a  duration  of  1  ~  5  seconds.-  volume:  the  volume  of  the  sound  in  dB.  The  volume  of  background  sound  is  usually  around  -35  ~  -40  dB.  The  volume  of  foreground  sound  is  usually  around  -20  ~  -30  dB.-  min_pause:  for  foreground  sounds,  there  should  be  a  pause  between  each  loop.  This  is  the  minimum  pause  in  seconds.  if  the  sound  only  plays  once  (e.g.  the  intro  music),  set  this  to  -1.-  max_pause:  this  is  the  maximum  pause  in  seconds.  It  should  be  less  than  40.  if  the  sound  only  plays  once,  set  this  to  -1.Note:-  Don’t  use  ambiguous  words  like  "forest  ambience".  Instead,  you  should  break  it  into  specific  components  like  "leaves  rustling"  and  "birds  chirping".-  Remove  sounds  related  to  narratoring  or  a  single  person’s  speech  voice.

Sound effects (one-time) description decomposition

[⬇](data:text/plain;base64,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)You  are  a  sound  director  that  modify  an  audio  description  using  a  given  instruction,  then  decomposes  the  new  description  into  a  composition  of  several  small  audio  scripts.You  should  read  the  input  JSON  and  respond  with  a  list  in  JSON  format:Example  input:‘‘‘{"description":  "The  sound  of  pebbles  crunching  under  Sammy’s  feet  as  he  walks  along  the  river  bank.","people":  [{"name":  Sammy,  "desc":  a  little  boy},  {"name":  John,  "desc":  a  blacksmith},  {"name":  Village  Elder,  "desc":  an  old  man}]}‘‘‘Example  output:‘‘‘json{"duration":  3,"audio_scripts":  [{’name’:  ’pebbles’,  ’desc’:  ’pebbles  crunching  at  a  river  bank’,  ’volume’:  -20},]}‘‘‘Example  input  2:‘‘‘{"description":  "The  hoot  of  an  owl  echoes  through  the  woods,  followed  by  the  rustling  of  leaves.","people":  [{"name":  Sammy,  "desc":  a  little  boy},  {"name":  John,  "desc":  a  blacksmith},  {"name":  Village  Elder,  "desc":  an  old  man}]}‘‘‘Example  output  2:‘‘‘json{"duration":  5,"audio_scripts":  [{’name’:  ’owl’,  ’desc’:  ’the  hoot  of  an  owl  at  a  river  bank’,  ’volume’:  -20},{’name’:  ’leaves  rustling’,  ’desc’:  ’the  rustling  of  leaves  at  a  river  bank’,  ’volume’:  -30},]}‘‘‘The  meaning  of  each  field  is  as  follows:-  duration:  the  duration  of  all  the  sound  in  seconds,  usually  around  1  ~  10.-  name:  the  name  of  the  sound.-  desc:  the  description  of  the  sound.  Simplify  the  description  by  removing  text  that  are  unrelated  to  the  sound  feature.  Rewrite  the  description  to  replace  people’s  names  with  its  descriptions.  For  example,  replace  "John"  with  "a  blacksmith".  And  describe  the  sound  environment  at  the  end.  For  example,  "in  a  village".-  volume:  the  volume  of  the  sound  in  dB.  The  volume  of  sound  effects  is  usually  around  -40  ~  -20  dB.Note:-  Don’t  use  ambiguous  words  like  "forest  ambience".  Instead,  you  should  break  it  into  specific  components  like  "leaves  rustling"  and  "birds  chirping".-  Remove  sounds  related  to  narratoring  or  a  single  person’s  speech  voice.

Sound query and rank (first run)

[⬇](data:text/plain;base64,R2l2ZW4gYSB0ZXh0IGRlc2NyaXB0aW9uIGFib3V0IGEgc291bmQgZWZmZWN0LCBwbGVhc2UgZ2VuZXJhdGUgYSBxdWVyeSBzdHJpbmcgdGhhdCBjYW4gYmUgdXNlZCBpbiBhIHNlYXJjaCBlbmdpbmUuIFlvdSBzaG91bGQgb25seSByZXNwb25kIGluIHF1ZXJ5IHN0cmluZy4KCiMgUXVlcnkgZm9ybWF0CgpRdWVyaWVzIGNhbiBiZSBjb21wb3NlZCBvZiBwb3NpdGl2ZSB0YWdzIGFuZCBuZWdhdGl2ZSB0YWdzIHN1Y2ggYXM6CgpgYGAKZG9nIGJhcmsgK2NpdHkgLWNhdApgYGAKYW5kCmBgYAp3YXRlciArcml2ZXIgLXNlYQpgYGAKd2hlcmUgcG9zaXRpdmUgdGFncyBhcmUgcHJlY2VkZWQgd2l0aCAiKyIgYW5kIG5lZ2F0aXZlIG9uZXMgd2l0aCAiLSIKCiMgWW91ciB0YXNrCntkZXNjcmlwdGlvbn0=)Given  a  text  description  about  a  sound  effect,  please  generate  a  query  string  that  can  be  used  in  a  search  engine.  You  should  only  respond  in  query  string.#  Query  formatQueries  can  be  composed  of  positive  tags  and  negative  tags  such  as:‘‘‘dog  bark  +city  -cat‘‘‘and‘‘‘water  +river  -sea‘‘‘where  positive  tags  are  preceded  with  "+"  and  negative  ones  with  "-"#  Your  task{description}

Sound query and rank (subsequent run)

[⬇](data:text/plain;base64,WW91IGFyZSBhbiBhc3Npc3RhbnQgZm9yIHNvdW5kIGVmZmVjdCBzZWFyY2hpbmcuIEJhc2VkIG9uIGN1cnJlbnQgc2VhcmNoIHJlc3VsdHMsIHBsZWFzZSBzZWxlY3QgdGhlIGJlc3QgcmVzdWx0IGFuZCByZWZpbmUgdGhlIHNlYXJjaCBxdWVyeSBzdHJpbmcgdG8gbWVldCB0aGUgc2VhcmNoIGdvYWwuCgojIFF1ZXJ5IGZvcm1hdAoKUXVlcmllcyBjYW4gYmUgY29tcG9zZWQgb2YgcG9zaXRpdmUgdGFncyBhbmQgbmVnYXRpdmUgdGFncyBzdWNoIGFzOgpgYGAKZG9nIGJhcmsgK2NpdHkgLWNhdApgYGAKd2hlcmUgcG9zaXRpdmUgdGFncyBhcmUgcHJlY2VkZWQgd2l0aCAiKyIgYW5kIG5lZ2F0aXZlIG9uZXMgd2l0aCAiLSIKCiMgTm90ZQoKSWYgbm8gcmVzdWx0cyBhcmUgZm91bmQsIHRoYXQgbWVhbnMgdGhlIGN1cnJlbnQgcXVlcnkgaXMgdG9vIHN0cmljdC4KWW91IHNob3VsZCBzaG9ydGVuIHRoZSBxdWVyeSBkb3duIHRvIG9uZSBvciB0d28gdGFncy4KCiMgUmVzcG9uc2UgZm9ybWF0CgpZb3Ugc2hvdWxkIG9ubHkgcmVzcG9uZCBpbiBKU09OIGxpa2UgdGhpczoKYGBganNvbgp7CiAgICAiYmVzdF9pbmRleCI6ICJ0aGUgaW5kZXggb2YgdGhlIGJlc3QgcmVzdWx0IiwKICAgICJxdWVyeSI6ICJyZWZpbmVkIHF1ZXJ5IiwKfQpgYGAKCiMgU2VhcmNoIGdvYWwKe2dvYWx9CgojIFNlYXJjaCBoaXN0b3J5CntoaXN0b3J5fQoKIyBDdXJyZW50IHF1ZXJ5CntxdWVyeX0KCiMgQ3VycmVudCByZXN1bHRzCntyZXN1bHRzfQ==)You  are  an  assistant  for  sound  effect  searching.  Based  on  current  search  results,  please  select  the  best  result  and  refine  the  search  query  string  to  meet  the  search  goal.#  Query  formatQueries  can  be  composed  of  positive  tags  and  negative  tags  such  as:‘‘‘dog  bark  +city  -cat‘‘‘where  positive  tags  are  preceded  with  "+"  and  negative  ones  with  "-"#  NoteIf  no  results  are  found,  that  means  the  current  query  is  too  strict.You  should  shorten  the  query  down  to  one  or  two  tags.#  Response  formatYou  should  only  respond  in  JSON  like  this:‘‘‘json{"best_index":  "the  index  of  the  best  result","query":  "refined  query",}‘‘‘#  Search  goal{goal}#  Search  history{history}#  Current  query{query}#  Current  results{results}

Sound selection

[⬇](data:text/plain;base64,R2l2ZW4gYSB0ZXh0IGRlc2NyaXB0aW9uIGFib3V0IGEgc291bmQgZWZmZWN0LCBwbGVhc2UgY2hvb3NlIGEgc291bmQgZmlsZSBmcm9tIHRoZSBmb2xsb3dpbmcgbGlzdC4gWW91IHNob3VsZCBvbmx5IHJlc3BvbmQgaW4gaW5kZXggbnVtYmVyLiBJZiBub25lIG9mIHRoZSBmaWxlcyBpcyBnb29kIGVub3VnaCwgcmVzcG9uZCBpbiAtMS4Ke3N5c3RlbV9wcm9tcHRfZW5kfQoKIyBTb3VuZCBkZXNjcmlwdGlvbgp7ZGVzY3JpcHRpb259CgojIFNlYXJjaGVkIHNvdW5kIGZpbGVzCntyZXN1bHRzfQ==)Given  a  text  description  about  a  sound  effect,  please  choose  a  sound  file  from  the  following  list.  You  should  only  respond  in  index  number.  If  none  of  the  files  is  good  enough,  respond  in  -1.{system_prompt_end}#  Sound  description{description}#  Searched  sound  files{results}

#### 6.2.3 Speech

Character simplification

[⬇](data:text/plain;base64,UGxlYXNlIHNpbXBsaWZ5IHRoZSBjaGFyYWN0ZXIgZGVzY3JpcHRpb24gdXNpbmcgb25lIG9yIHR3byBzaW1wbGUgYWRqZWN0aXZlcyBmb2xsb3dlZCBieSBhIG5vdW4uCllvdSBzaG91bGQgcmVhZCB0aGUgaW5wdXQganNvbiBhbmQgcmVzcG9uZCB3aXRoIGEganNvbiB0aGF0IGhhcyB0aGUgc2FtZSBmb3JtYXQuCgpFeGFtcGxlIGlucHV0OgpgYGBqc29uClsKICAgIHsnbmFtZSc6ICdUb20nLCAnYWdlJzogNiwgJ2dlbmRlcic6ICdtYWxlJ30sCiAgICB7J25hbWUnOiAnQ2Fyb2wnLCAnYWdlJzogNDUsICdnZW5kZXInOiAnZmVtYWxlJ30sCiAgICB7J25hbWUnOiAnTWF4JywgJ2Rlc2MnOiAnV2VhcmluZyBhIGJsYWNrIHQtc2hpcnQgYW5kIGplYW5zLCBNYXggaXMgYSB5b3VuZyBtYWxlIHdpdGggc2hvcnQsIGJyb3duIGhhaXIgYW5kIGJsdWUgZXllcyd9LApdCmBgYAoKRXhhbXBsZSBvdXRwdXQ6CmBgYGpzb24KWwogICAgeyduYW1lJzogJ1RvbScsICdkZXNjJzogJ0EgeW91bmcgYm95J30sCiAgICB7J25hbWUnOiAnQ2Fyb2wnLCAnZGVzYyc6ICdBIG1pZGRsZSBhZ2VkIHdvbWFuJ30sCiAgICB7J25hbWUnOiAnTWF4JywgJ2Rlc2MnOiAnQSB5b3VuZyBtYWxlJ30sCl0KYGBg)Please  simplify  the  character  description  using  one  or  two  simple  adjectives  followed  by  a  noun.You  should  read  the  input  json  and  respond  with  a  json  that  has  the  same  format.Example  input:‘‘‘json[{’name’:  ’Tom’,  ’age’:  6,  ’gender’:  ’male’},{’name’:  ’Carol’,  ’age’:  45,  ’gender’:  ’female’},{’name’:  ’Max’,  ’desc’:  ’Wearing  a  black  t-shirt  and  jeans,  Max  is  a  young  male  with  short,  brown  hair  and  blue  eyes’},]‘‘‘Example  output:‘‘‘json[{’name’:  ’Tom’,  ’desc’:  ’A  young  boy’},{’name’:  ’Carol’,  ’desc’:  ’A  middle  aged  woman’},{’name’:  ’Max’,  ’desc’:  ’A  young  male’},]‘‘‘

Prompts for voice fingerprint (using AudioGen)

[⬇](data:text/plain;base64,e2NoYXJhY3Rlcl9kZXNjfSBpcyBzcGVha2luZyBjbGVhcmx5IGFuZCBzbG93bHku){character_desc}  is  speaking  clearly  and  slowly.

Prompts for speech reference (using XTTS)

[⬇](data:text/plain;base64,VGhlcmUgYXJlIGEgbG90IG9mIHRoaW5ncyB0aGF0IEkgY291bGQgdGFsayBhYm91dC4gQnV0IGl0IHdpbGwgcHJvYmFibHkgc291bmQgc2ltaWxhciB0byB0aGlzLg==)There  are  a  lot  of  things  that  I  could  talk  about.  But  it  will  probably  sound  similar  to  this.

Prompts for text-to-speech API.

[⬇](data:text/plain;base64,IntsaW5lfSIge2NoYXJhY3Rlcn0gc2FpZCB7ZW1vdGlvbl9hZHZlcmJ9Lg==)"{line}"  {character}  said  {emotion_adverb}.