- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:41:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered
    Autonomous Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汇聚的范式：符号AI与联结主义AI在LLM赋能自主代理中的协同作用
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08516](https://ar5iv.labs.arxiv.org/html/2407.08516)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08516](https://ar5iv.labs.arxiv.org/html/2407.08516)
- en: 1]Baidu Inc 2]University of Virginia 3]Hong Kong University of Science and Technology
    (GZ) 4]Nottingham Trent University, Nottingham, UK 5]Silesian University of Technology,
    Gliwice, Poland
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 1]百度公司 2]弗吉尼亚大学 3]香港科技大学（广州） 4]诺丁汉特伦特大学，英国诺丁汉 5]西里西亚科技大学，波兰格利维采
- en: \fnmHaoyi \surXiong [haoyi.xiong.fr@ieee.org](mailto:haoyi.xiong.fr@ieee.org)
       \fnmZhiyuan \surWang [vmf9pr@virginia.edu](mailto:vmf9pr@virginia.edu)    \fnmXuhong
    \surLi [jacqueslixuhong@gmail.com](mailto:jacqueslixuhong@gmail.com)    \fnmJiang
    \surBian [jiangbian03@gmail.com](mailto:jiangbian03@gmail.com)    \fnmZeke \surXie
    [zekexie@hkust-gz.edu.cn](mailto:zekexie@hkust-gz.edu.cn)    \fnmShahid \surMumtaz
    [dr.shahid.mumtaz@ieee.org](mailto:dr.shahid.mumtaz@ieee.org)    \fnmLaura E.
    \surBarnes [lb3dp@virginia.edu](mailto:lb3dp@virginia.edu) [ [ [ [ [
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \fnmHaoyi \surXiong [haoyi.xiong.fr@ieee.org](mailto:haoyi.xiong.fr@ieee.org)
       \fnmZhiyuan \surWang [vmf9pr@virginia.edu](mailto:vmf9pr@virginia.edu)    \fnmXuhong
    \surLi [jacqueslixuhong@gmail.com](mailto:jacqueslixuhong@gmail.com)    \fnmJiang
    \surBian [jiangbian03@gmail.com](mailto:jiangbian03@gmail.com)    \fnmZeke \surXie
    [zekexie@hkust-gz.edu.cn](mailto:zekexie@hkust-gz.edu.cn)    \fnmShahid \surMumtaz
    [dr.shahid.mumtaz@ieee.org](mailto:dr.shahid.mumtaz@ieee.org)    \fnmLaura E.
    \surBarnes [lb3dp@virginia.edu](mailto:lb3dp@virginia.edu) [ [ [ [ [
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This article explores the convergence of connectionist and symbolic artificial
    intelligence (AI), from historical debates to contemporary advancements. Traditionally
    considered distinct paradigms, connectionist AI focuses on neural networks, while
    symbolic AI emphasizes symbolic representation and logic. Recent advancements
    in large language models (LLMs), exemplified by ChatGPT and GPT-4, highlight the
    potential of connectionist architectures in handling human language as a form
    of symbols. The study argues that LLM-empowered Autonomous Agents (LAAs) embody
    this paradigm convergence. By utilizing LLMs for text-based knowledge modeling
    and representation, LAAs integrate neuro-symbolic AI principles, showcasing enhanced
    reasoning and decision-making capabilities. Comparing LAAs with Knowledge Graphs
    within the neuro-symbolic AI theme highlights the unique strengths of LAAs in
    mimicking human-like reasoning processes, scaling effectively with large datasets,
    and leveraging in-context samples without explicit re-training. The research underscores
    promising avenues in neuro-vector-symbolic integration, instructional encoding,
    and implicit reasoning, aimed at further enhancing LAA capabilities. By exploring
    the progression of neuro-symbolic AI and proposing future research trajectories,
    this work advances the understanding and development of AI technologies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了联结主义与符号人工智能（AI）的汇聚，从历史争论到当代进展。传统上被视为不同的范式，联结主义AI专注于神经网络，而符号AI则强调符号表示和逻辑。近年来的大型语言模型（LLMs），如ChatGPT和GPT-4，突显了联结主义架构在处理人类语言作为符号形式中的潜力。研究认为，LLM赋能的自主代理（LAAs）体现了这种范式汇聚。通过利用LLMs进行基于文本的知识建模和表示，LAAs整合了神经符号AI原则，展示了增强的推理和决策能力。将LAAs与神经符号AI主题中的知识图谱进行比较，突显了LAAs在模拟类人推理过程、有效地处理大规模数据集以及在没有显式再训练的情况下利用上下文样本的独特优势。研究强调了神经-向量-符号集成、指令编码和隐式推理中的有前景的方向，旨在进一步提升LAA的能力。通过探索神经符号AI的发展并提出未来研究方向，本研究推动了AI技术的理解和发展。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Large Language Models (LLMs), LLM-Empowered Autonomous Agents (LAAs), Neuro-symbolic
    AI, Program-of-Thoughts (PoT) prompting
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）、LLM赋能的自主代理（LAAs）、神经符号AI、思维计划（PoT）提示
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Artificial Intelligence (AI) has historically navigated the fascinating duality
    of two foundational paradigms: connectionism and symbolism. Connectionism, deeply
    influenced by cognitive science and computational neuroscience, delves into neural
    networks and machine learning algorithms that echo the deep neural architecture
    and functions of the human brain [[1](#bib.bib1)]. Imagine a sprawling network
    of neurons firing in electric synchrony, mirroring how advanced AI systems identify
    patterns and glean insights from vast datasets. Conversely, symbolism is the epitome
    of conceptual and logical clarity. It anchors itself in the high-level abstractions
    and representations of knowledge, flourishing through rule-based systems that
    excel in reasoning and decision-making [[2](#bib.bib2)]. Picture a grand library
    where every book is a rule, and every chapter a pathway to logical deduction–symbolic
    AI analogising the thought processes of human reasoning.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）历史上一直在连接主义和符号主义这两个基础范式的迷人双重性中导航。连接主义深受认知科学和计算神经科学的影响，深入探讨了神经网络和机器学习算法，这些算法回响了人脑的深层神经架构和功能[[1](#bib.bib1)]。想象一下，一个广阔的神经网络以电同步的方式运作，类似于先进的人工智能系统如何从大量数据集中识别模式和提取见解。相反，符号主义是概念和逻辑清晰的典范。它依托于知识的高层次抽象和表示，通过基于规则的系统在推理和决策方面表现出色[[2](#bib.bib2)]。想象一个宏大的图书馆，每本书都是一条规则，每章都是通往逻辑推理的路径——符号人工智能模拟了人类推理过程的思维。
- en: The dynamic interplay between these two paradigms has sculpted the continuous
    evolution of AI, like a grand philosophical debate, resulting in shifts in dominance
    and application across various research domains. Think of this dialectic as a
    dance through time—the elegant waltz of connectionism and symbolism, sometimes
    leading, sometimes following, yet always in a harmonious exchange that propels
    the boundaries of what AI can achieve. For instance, in the domain of image recognition,
    connectionist models driven by deep neural networks demonstrate their prowess
    by identifying subtle patterns in pixel data, akin to how our brains recognize
    faces in a crowd [[3](#bib.bib3)]. Meanwhile, in expert systems used for medical
    diagnostics, symbolism shines by methodically applying predefined rules to diagnose
    diseases, mimicking the logical flow of a doctor’s thought process [[4](#bib.bib4)].
    This storied dance of paradigms has not just shaped, but revitalized AI, continuing
    to impact its trajectory as it ventures into increasingly sophisticated applications.
    The oscillation of dominance between these approaches resembles the ebb and flow
    of tides, each rise and retreat bringing new insights and innovations to the fore.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种范式之间的动态交互塑造了人工智能的持续演变，犹如一场宏大的哲学辩论，导致了在各个研究领域中主导地位和应用的变化。把这种辩证法想象成一场穿越时间的舞蹈——连接主义与符号主义的优雅华尔兹，有时领先，有时跟随，但始终在和谐的交流中推动人工智能能够达到的边界。例如，在图像识别领域，由深度神经网络驱动的连接主义模型通过识别像素数据中的微妙模式展示其强大能力，类似于我们的大脑在拥挤的人群中识别面孔的方式[[3](#bib.bib3)]。与此同时，在用于医学诊断的专家系统中，符号主义通过系统地应用预定义规则来诊断疾病，模拟了医生思考过程的逻辑流[[4](#bib.bib4)]。这种范式的历史舞蹈不仅塑造了人工智能，还使其焕发活力，继续影响其轨迹，探索日益复杂的应用。这些方法主导地位的波动类似于潮汐的涨落，每一次的起伏都带来新的见解和创新。
- en: 'In recent years, the advancements in Large Language Models (LLMs) and foundation
    models have catalyzed the integration of connectionist and symbolic AI paradigms,
    realizing new levels of computational intelligence and versatility [[5](#bib.bib5)].
    These models, exemplified by systems such as OpenAI’s GPT-4, have demonstrated
    unprecedented capabilities in natural language understanding and generation, exhibiting
    robust performance across a range of complex tasks [[6](#bib.bib6)]. LLMs themselves
    are a triumph of connectionism, empowered by vast amounts of data and sophisticated
    neural architectures to produce coherent and contextually relevant texts. Moreover,
    the emergence of LLM-empowered Autonomous Agents (LAAs) signifies a pivotal juncture
    in the development of AI, embodying the convergence of symbolic and connectionist
    AI. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Converging Paradigms:
    The Synergy of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents"),
    LAAs combine a symbolic subsystem, utilizing language-based knowledge, rules,
    and workflows intrinsic to symbolic AI, with the generative capabilities of LLMs [[7](#bib.bib7)].
    This symbolic subsystem works seamlessly with the neural subsystem and incorporates
    external tools for perceptions and actions [[8](#bib.bib8)]. LAAs demonstrate
    advanced reasoning, planning, and decision-making abilities, marking a new era
    in AI. The dual subsystems align with dual-process theories of reasoning [[9](#bib.bib9)]
    and Systems I and II proposed by Yoshua Bengio [[10](#bib.bib10)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，大型语言模型（LLMs）和基础模型的进步促进了连接主义和符号主义AI范式的融合，实现了新的计算智能和多功能性水平[[5](#bib.bib5)]。以OpenAI的GPT-4为代表的这些模型在自然语言理解和生成方面展示了前所未有的能力，在各种复杂任务中表现出强大的性能[[6](#bib.bib6)]。LLMs本身是连接主义的胜利，通过海量的数据和复杂的神经网络结构生成连贯且上下文相关的文本。此外，LLM驱动的自主智能体（LAAs）的出现标志着AI发展的一个关键时刻，体现了符号主义和连接主义AI的融合。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Converging Paradigms: The Synergy of Symbolic and
    Connectionist AI in LLM-Empowered Autonomous Agents")所示，LAAs将利用符号AI固有的基于语言的知识、规则和工作流的符号子系统，与LLMs的生成能力结合[[7](#bib.bib7)]。这个符号子系统与神经子系统无缝协作，并结合了外部工具用于感知和行动[[8](#bib.bib8)]。LAAs展示了先进的推理、规划和决策能力，标志着AI新时代的到来。这两个子系统与推理的双过程理论[[9](#bib.bib9)]和Yoshua
    Bengio提出的系统I和II[[10](#bib.bib10)]相一致。'
- en: '![Refer to caption](img/71c73d5c4b1bab271bfc7be2456a624f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/71c73d5c4b1bab271bfc7be2456a624f.png)'
- en: 'Figure 1: Elements of LLM-empowered Autonomous Agents (LAAs): Large Language
    Models (Neural Sub-System), Agentic Workflows (Symbolic Sub-System), and External
    Tools'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM驱动的自主智能体（LAAs）的元素：大型语言模型（神经子系统）、代理工作流（符号子系统）和外部工具
- en: In this work, we aim at examining the historical evolution and current state
    of AI by exploring the enduring debate between connectionism and symbolism and
    their convergence in modern technologies, particularly in the theme of neuro-symbolic
    approaches, including Knowledge Graphs, LLMs, and LAAs. This review aims to illustrate
    how the integration of these paradigms has led to groundbreaking advancements,
    offering new perspectives on the capabilities and future directions of AI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在通过探讨连接主义和符号主义之间持久的辩论及其在现代技术中的融合，特别是在神经符号方法的主题下，包括知识图谱、LLMs和LAAs，来检视AI的历史演变和当前状态。此综述旨在阐明这些范式的整合如何带来突破性的进展，为AI的能力和未来方向提供新的视角。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Historical Context of Technology: This article provides an in-depth examination
    of the historical debate between connectionism and symbolism, contextualizing
    modern AI developments and highlighting the strengths of each approach. We present
    recent advancements in LLMs with Knowledge Graphs (KGs) [[11](#bib.bib11)] as
    references, discussing these techniques from the perspectives of symbolic, connectionist,
    and neuro-symbolic AI. The article also showcases the transformative impact of
    these techniques on knowledge modeling, acquisition, representation, and reasoning.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 技术的历史背景：本文深入探讨了连接主义与符号主义之间的历史辩论，将现代AI发展置于背景中，并突出每种方法的优势。我们以包含知识图谱（KGs）的LLMs的最新进展[[11](#bib.bib11)]作为参考，从符号主义、连接主义和神经符号AI的角度讨论这些技术。文章还展示了这些技术对知识建模、获取、表示和推理的变革性影响。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Convergence of Paradigms: This article highlights the convergence of symbolic
    and connectionist approaches in developing LAAs, emphasizing their enhanced reasoning,
    decision-making, and efficiency. By contrasting LAAs with Knowledge Graphs (KGs)
    within neuro-symbolic AI, we examine distinct patterns and functionalities. While
    both integrate symbolic and neural methodologies, LAAs demonstrate unique advantages
    over KGs: (1) analogizing human reasoning with agentic workflows and various prompting
    techniques [[12](#bib.bib12), [13](#bib.bib13)], (2) scaling effectively on large
    datasets, adapting to in-context samples, and leveraging the emergent abilities
    of LLMs. These strengths drive the surge of a new wave of neuro-symbolic AI [[14](#bib.bib14)].'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 范式融合：本文突出符号方法与连接主义方法在开发LAA中的融合，强调它们在推理、决策和效率方面的增强。通过将LAA与神经-符号AI中的知识图谱（KGs）进行对比，我们考察了不同的模式和功能。尽管两者都整合了符号和神经方法，但LAA相较于KGs展现了独特的优势：（1）将人类推理与代理工作流及各种提示技术类比[[12](#bib.bib12),
    [13](#bib.bib13)]，（2）在大数据集上有效扩展，适应上下文样本，并利用LLMs的突现能力。这些优势推动了新一波神经-符号AI的兴起[[14](#bib.bib14)]。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Future Directions: By highlighting the trend of converging paradigms and current
    limitations of LAAs, the article underscores two promising directions: (1) *neuro-vector-symbolic
    architectures*, which incorporate vector manipulation to enhance agentic reasoning
    capabilities, and (2) *generative encoding*, embedding agentic logical steps into
    text vectorization for advanced sample selection for in-context learning of LAAs
    through instructing LLMs.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未来方向：通过强调范式的融合趋势和当前LAA的局限性，本文突出了两个有前景的方向：（1）*神经-向量-符号架构*，其通过向量操作来增强代理推理能力，以及（2）*生成编码*，将代理逻辑步骤嵌入文本向量化中，以便通过指导LLMs进行上下文学习的高级样本选择。
- en: These contributions are crucial as they provide a comprehensive understanding
    of the evolution of AI, highlight the significance of paradigm convergence, and
    offer insights into future research and application potentials in the rapidly
    evolving field of AI.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些贡献至关重要，因为它们提供了对AI演变的全面理解，突出范式融合的重要性，并提供了对迅速发展的AI领域未来研究和应用潜力的见解。
- en: 2 Preliminaries
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言
- en: This section begins by summarizing the historical debate between connectionist
    AI and symbolic AI. We then explore knowledge graphs (KGs) as an early effort
    to synergize these two paradigms through neuro-symbolic AI. Lastly, we examine
    LLMs as the latest advancements in connectionist AI.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先总结了连接主义AI与符号AI之间的历史辩论。接着，我们探讨了知识图谱（KGs）作为一种早期努力，通过神经-符号AI来协同这两种范式。最后，我们考察了LLMs作为连接主义AI的最新进展。
- en: '2.1 Connectionism vs. Symbolism: a Historical Debate on AI'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 连接主义与符号主义：AI的历史辩论
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Connectionism vs. Symbolism:
    a Historical Debate on AI ‣ 2 Preliminaries ‣ Converging Paradigms: The Synergy
    of Symbolic and Connectionist AI in LLM-Empowered Autonomous Agents"), the discourse
    of AI has long revolved around the dichotomy between connectionism and symbolism,
    two paradigms integral to the field. Connectionism models cognitive processes
    through artificial neural networks that emulate the brain’s neuron structures,
    emphasizing learning through algorithms and pattern recognition. This began with
    Frank Rosenblatt’s Perceptron in 1958 [[15](#bib.bib15)] and advanced significantly
    with the backpropagation algorithm developed by David Rumelhart, Geoffrey Hinton,
    and Ronald J. Williams in the 1980s [[16](#bib.bib16)], setting the stage for
    modern deep learning [[1](#bib.bib1)]. Conversely, symbolism focuses on high-level
    knowledge representations and symbolic manipulation to mimic human reasoning,
    gaining prominence with systems like the Logic Theorist by Allen Newell and Herbert
    A. Simon in 1956 [[17](#bib.bib17)]. Symbolic AI thrived with expert systems such
    as MYCIN [[4](#bib.bib4)] and DENDRAL [[18](#bib.bib18)] in the 1970s and 1980s,
    excelling in specific domains through predefined rules.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](#S2.F2 "图2 ‣ 2.1 联结主义与符号主义：关于AI的历史辩论 ‣ 2 基础知识 ‣ 收敛范式：符号主义和联结主义AI在LLM驱动的自主智能体中的协同作用")所示，AI的讨论长期围绕联结主义和符号主义这两种对立范式展开，这两种范式对该领域至关重要。联结主义通过模仿大脑神经结构的人工神经网络来建模认知过程，强调通过算法和模式识别来学习。这始于1958年的Frank
    Rosenblatt的感知器[[15](#bib.bib15)]，并在1980年代由David Rumelhart、Geoffrey Hinton和Ronald
    J. Williams开发的反向传播算法[[16](#bib.bib16)]中取得了重大进展，为现代深度学习奠定了基础[[1](#bib.bib1)]。相反，符号主义关注于高级知识表示和符号操作以模仿人类推理，1956年Allen
    Newell和Herbert A. Simon的逻辑理论家系统[[17](#bib.bib17)]使其获得了关注。符号AI在1970年代和1980年代通过如MYCIN[[4](#bib.bib4)]和DENDRAL[[18](#bib.bib18)]等专家系统得到了发展，通过预定义规则在特定领域表现优异。
- en: 'In the 1980s, as Ashok Goel noted, debates often involved criticisms that attacked
    caricatures of the opposing methods [[19](#bib.bib19)]. Each approach has its
    limitations: connectionist AI is criticized for its black-box nature and lack
    of interpretability [[20](#bib.bib20)], while symbolic AI faced challenges with
    the labor-intensive knowledge acquisition process [[21](#bib.bib21)] and its limited
    adaptability [[22](#bib.bib22)]. Historical debates between figures, such as Yann
    LeCun, Yoshua Bengio, and Gary Marcus, have underscored these limitations [[23](#bib.bib23)].
    However, the integration of both paradigms has led to robust hybrid models, combining
    neural networks’ pattern recognition with symbolic systems’ interpretability and
    logical reasoning [[24](#bib.bib24)]. Contemporary research exemplifies this convergence,
    seen in neuro-symbolic AI and large-scale pre-trained models like BERT [[25](#bib.bib25)],
    GPT [[5](#bib.bib5)], and hybrid reinforcement learning models [[26](#bib.bib26)],
    reflecting the ongoing evolution inspired by the historical debate.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在1980年代，正如Ashok Goel所指出的，辩论常常涉及对对方方法的刻画性批评[[19](#bib.bib19)]。每种方法都有其局限性：联结主义AI因其黑箱性质和缺乏可解释性而受到批评[[20](#bib.bib20)]，而符号AI面临知识获取过程劳动密集[[21](#bib.bib21)]和适应性有限[[22](#bib.bib22)]等挑战。历史上的辩论人物，如Yann
    LeCun、Yoshua Bengio和Gary Marcus，强调了这些局限性[[23](#bib.bib23)]。然而，两种范式的整合带来了强大的混合模型，将神经网络的模式识别与符号系统的可解释性和逻辑推理相结合[[24](#bib.bib24)]。当代研究体现了这一收敛，见于神经符号AI和像BERT[[25](#bib.bib25)]、GPT[[5](#bib.bib5)]等大规模预训练模型，以及混合强化学习模型[[26](#bib.bib26)]，反映了历史辩论激发的持续演变。
- en: '![Refer to caption](img/bc586196b737518e14e4158a9255c821.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bc586196b737518e14e4158a9255c821.png)'
- en: 'Figure 2: Exploring the Evolution of Artificial Intelligence: A Timeline of
    Key Innovations and Milestones. It starts from the birth of symbolic and connectionist
    AI in the 1950s, through key milestones like the AI debates of the 1980s and the
    advancement in machine learning in the 1990s. This figure highlights significant
    developments such as the impact of AlexNet on image recognition, the transformation
    in NLP by models like BERT and GPT, and the rise of generative AI, culminating
    in the use of LLMs and Agents for autonomous decision-making in the 2020s.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：探索人工智能的演变：关键创新和里程碑的时间轴。它从1950年代符号主义和联结主义AI的诞生开始，通过1980年代的AI辩论和1990年代机器学习的进展等关键里程碑。此图突出显示了重要的发展，如AlexNet对图像识别的影响，BERT和GPT等模型对自然语言处理的变革，以及生成式AI的兴起，最终在2020年代以LLMs和智能体用于自主决策的应用为高潮。
- en: '2.2 Knowledge Graphs: An Early Neuro-symbolic Attempt'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 知识图谱：早期的神经符号尝试
- en: Knowledge graphs have a foundation rooted in the evolution of semantic web technologies
    and the Resource Description Framework (RDF). Proposed by the W3C in the 1990s,
    RDF standardized data interchange on the web using triples (subject, predicate,
    object) for seamless data integration and interoperability [[27](#bib.bib27)].
    This movement established the Semantic Web, aiming for a more intelligent and
    interconnected web [[28](#bib.bib28)]. Early adopters used RDF to build schemas
    and taxonomies, forming the basics of modern knowledge graphs [[29](#bib.bib29)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱的基础源于语义网技术和资源描述框架（RDF）的发展。RDF由W3C在1990年代提出，标准化了网络上的数据交换，使用三元组（主题、谓词、对象）实现无缝的数据集成和互操作性[[27](#bib.bib27)]。这一运动建立了语义网，旨在实现更智能和互联的网络[[28](#bib.bib28)]。早期采用者使用RDF构建模式和分类法，奠定了现代知识图谱的基础[[29](#bib.bib29)]。
- en: As the field matured, the focus shifted towards capturing complex relationships
    and domain-specific knowledge. Ontologies, formal specifications of concepts and
    relationships, provided a framework for annotating and interlinking data, enabling
    semantic reasoning at a certain level [[30](#bib.bib30)]. Markov-logic networks
    introduced probabilistic reasoning to knowledge graphs, allowing for handling
    uncertainty and inconsistency in data [[31](#bib.bib31)]. The synergy of Ontologies
    and Markov-logic networks advanced the ability of symbolic AI to perform robust
    reasoning over large datasets [[32](#bib.bib32)].
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着领域的发展，重点转向捕捉复杂的关系和领域特定的知识。本体论，作为概念和关系的正式规范，提供了一个注释和互联数据的框架，实现了某种程度上的语义推理[[30](#bib.bib30)]。马尔可夫逻辑网络将概率推理引入知识图谱，允许处理数据中的不确定性和不一致性[[31](#bib.bib31)]。本体论和马尔可夫逻辑网络的协同作用提升了符号AI在大数据集上的强大推理能力[[32](#bib.bib32)]。
- en: In recent years, the use of graph neural networks (GNNs) has further revolutionized
    the landscape of knowledge graphs. GNNs adeptly leverage the graph structure for
    advanced pattern recognition and complex predictions. They excel in tasks such
    as node classification, link prediction, and the extraction of hidden patterns
    from graph-structured data [[33](#bib.bib33)]. This paradigm shift towards neural
    networks marks a convergence with modern machine learning techniques, enabling
    more nuanced and scalable interpretations of often massive and intricate datasets.
    The ability of GNNs to embed nodes and entire graphs numerically has significantly
    enhanced the computational handling of knowledge graphs [[34](#bib.bib34)]. In
    conclusion, the integration of graph neural networks with rule-based reasoning
    has positioned knowledge graphs at the core of the neuro-symbolic AI approach [[11](#bib.bib11)]
    prior to the surge of LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，图神经网络（GNNs）的使用进一步革新了知识图谱的领域。GNNs巧妙地利用图结构进行高级模式识别和复杂预测。它们在节点分类、链接预测以及从图结构数据中提取隐藏模式等任务中表现出色[[33](#bib.bib33)]。这种向神经网络的范式转变标志着与现代机器学习技术的融合，使得对通常庞大而复杂的数据集的解释更加细致和可扩展。GNNs对节点和整个图进行数值嵌入的能力显著提升了知识图谱的计算处理能力[[34](#bib.bib34)]。总之，图神经网络与基于规则的推理的整合，使得知识图谱在神经符号AI方法的核心地位得到巩固[[11](#bib.bib11)]，在LLMs的涌现之前。
- en: '2.3 LLMs: Recent Connectionist AI Advancements'
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLMs：近期的连接主义AI进展
- en: The field of connectionist AI has undergone substantial evolution, beginning
    with the invention of the perceptron [[15](#bib.bib15)], kicking off the neural
    network research in the late 1950s. In the following decades, the development
    of Multi-Layer Perceptrons (MLPs) introduced hidden layers and non-linear activation
    functions, enabling the modeling of more complex functions [[16](#bib.bib16)].
    In the 1990s, Long Short-Term Memory (LSTM) networks were developed to address
    the limitations of traditional recurrent neural networks (RNNs) by introducing
    gating mechanisms to handle long-term dependencies in sequential data [[35](#bib.bib35)].
    Self-attention mechanisms and transformer architectures proposed in the late 2010s
    further revolutionized sequence modeling, such as texts for natural language processing,
    by allowing models to focus on different parts of the input sequence when generating
    each part of the output sequence [[36](#bib.bib36)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 连接主义人工智能领域经历了重大的演变，始于感知器的发明 [[15](#bib.bib15)]，开启了20世纪50年代末的神经网络研究。在接下来的几十年里，多层感知器（MLP）的发展引入了隐藏层和非线性激活函数，使得建模更复杂的函数成为可能 [[16](#bib.bib16)]。在1990年代，长短期记忆（LSTM）网络的出现解决了传统递归神经网络（RNN）在处理序列数据的长期依赖问题，通过引入门控机制 [[35](#bib.bib35)]。2010年代末提出的自注意力机制和变换器架构进一步革新了序列建模，如自然语言处理中的文本，使得模型在生成每个输出序列的部分时能够关注输入序列的不同部分 [[36](#bib.bib36)]。
- en: 'The development of transformer-based pre-trained language models has significantly
    advanced natural language processing (NLP). These architectures include encoder-only
    models, e.g., BERT [[25](#bib.bib25)], which excel at understanding and classifying
    text; decoder-only models, e.g., GPT [[6](#bib.bib6)], which generate coherent
    and contextually relevant text; and encoder-decoder models, e.g., T5 [[37](#bib.bib37)],
    which are effective in tasks requiring both comprehension and generation. Transformer-based
    language models, such as OpenAI’s GPT-4 [[38](#bib.bib38)], Google’s Gemini [[39](#bib.bib39)]
    and PaLM [[40](#bib.bib40)], Microsoft’s Phi-3 [[41](#bib.bib41)], and Meta’s
    LLaMA [[42](#bib.bib42)], are termed Large Language Models (LLMs). These models,
    illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.3 LLMs: Recent Connectionist AI
    Advancements ‣ 2 Preliminaries ‣ Converging Paradigms: The Synergy of Symbolic
    and Connectionist AI in LLM-Empowered Autonomous Agents"), are trained on large-scale
    transformers comprising billions of learnable parameters to support various abilities
    to enable agents, including perception, reasoning, planning, and action [[12](#bib.bib12)].
    As the central component of an agent’s neural sub-system, the larger the model,
    the stronger the agent’s capability.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '基于变换器的预训练语言模型显著推动了自然语言处理（NLP）的发展。这些架构包括仅编码器模型，如BERT [[25](#bib.bib25)]，擅长于理解和分类文本；仅解码器模型，如GPT [[6](#bib.bib6)]，能够生成连贯且上下文相关的文本；以及编码器-解码器模型，如T5 [[37](#bib.bib37)]，在需要理解和生成的任务中表现出色。基于变换器的语言模型，如OpenAI的GPT-4 [[38](#bib.bib38)]、谷歌的Gemini [[39](#bib.bib39)]和PaLM [[40](#bib.bib40)]、微软的Phi-3 [[41](#bib.bib41)]和Meta的LLaMA [[42](#bib.bib42)]，被称为大型语言模型（LLM）。这些模型如图 [3](#S2.F3
    "图 3 ‣ 2.3 LLMs: 最近的连接主义AI进展 ‣ 2 初步 ‣ 汇聚范式: 符号AI与连接主义AI在LLM驱动的自主代理中的协同") 所示，训练于拥有数十亿可学习参数的大规模变换器上，以支持各种能力，包括感知、推理、规划和行动 [[12](#bib.bib12)]。作为代理神经子系统的核心组件，模型越大，代理的能力就越强。'
- en: '![Refer to caption](img/a30312077b78de0b384345576f682af1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a30312077b78de0b384345576f682af1.png)'
- en: 'Figure 3: Large Language Models and Their Agentic Abilities. The X-axis shows
    the release dates, and the Y-axis represents the LLM Agent Benchmark Score [[43](#bib.bib43)].
    Bubble size indicates the number of parameters (in billions). An asterisk (*)
    denotes estimated parameter counts when the official release is not available.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：大型语言模型及其代理能力。X轴显示发布日期，Y轴表示LLM代理基准评分 [[43](#bib.bib43)]。气泡的大小表示参数数量（以十亿为单位）。星号（*）表示在官方发布不可用时的估算参数数量。
- en: 'In general, every LLM undergos a two-stage training process: *pre-training*
    and *fine-tuning*. Pre-training involves adjusting model parameters based on the
    statistical properties of a large text corpus, enabling an understanding of syntax,
    semantics, and linguistic nuances [[25](#bib.bib25)]. Fine-tuning then adapts
    the pre-trained model to specific tasks or domains using a smaller, task-specific
    dataset, optimizing performance for particular applications [[44](#bib.bib44)].
    To ensure LLMs follow human’s instructions, align with human values and exhibit
    desired behaviors, instruction tuning and reinforcement learning from human feedback
    (RLHF) have been proposed on top of fine-tuning [[45](#bib.bib45)].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，每个LLM都经历两个阶段的训练过程：*预训练*和*微调*。预训练涉及根据大量文本语料库的统计属性调整模型参数，从而实现对语法、语义和语言细微差别的理解[[25](#bib.bib25)]。随后，微调利用较小的特定任务数据集对预训练模型进行调整，以优化特定应用的性能[[44](#bib.bib44)]。为了确保LLM能够遵循人类的指令、符合人类价值观并表现出期望的行为，除了微调之外，还提出了指令调优和基于人类反馈的强化学习（RLHF）[[45](#bib.bib45)]。
- en: As the size of LLMs increases, they exhibit a range of emerging capabilities,
    such as writing computer code, playing chess, diagnosing medical conditions, and
    translating languages. These capabilities often develop suddenly and dramatically
    at certain scales due to scaling laws, which describe how task performance can
    surge unexpectedly when a model reaches a particular threshold size [[46](#bib.bib46)].
    This phenomenon is particularly observable in tasks requiring multi-step reasoning,
    where success probabilities compound multiplicatively, leading to rapid performance
    jumps [[47](#bib.bib47)]. However, these advancements come with *“hallucination”
    challenges* [[48](#bib.bib48)], such as producing false or nonsensical information
    that appears convincing but is inaccurate or not based on reality. These issues
    underline the importance of continued research and engineering to harness the
    benefits of LLMs while mitigating their drawbacks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM规模的增加，它们展现出一系列新兴能力，如编写计算机代码、下棋、诊断医学状况和翻译语言。这些能力往往会在某些规模下突然且显著地发展，这种现象可以通过缩放定律来解释，该定律描述了当模型达到特定阈值规模时，任务性能如何意外地激增[[46](#bib.bib46)]。这种现象在需要多步骤推理的任务中尤为明显，其中成功概率会呈乘法增长，导致性能的快速跃升[[47](#bib.bib47)]。然而，这些进展也伴随着*“幻觉”挑战*[[48](#bib.bib48)]，例如生成看似可信但实际上不准确或不基于现实的虚假或无意义信息。这些问题突显了持续研究和工程的重要性，以在利用LLM的好处的同时减轻其缺陷。
- en: '3 LLM-empowered Autonomous Agents: The Convergence of Symbolism and Connectionism'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM驱动的自主智能体：符号主义与联结主义的融合
- en: This section reviews the definition of both traditional and LLM-based agents,
    introduces core techniques for designing and implementing LAAs, and rethinks these
    innovations through the lens of symbolic AI.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了传统和基于LLM的智能体的定义，介绍了设计和实施LLA的核心技术，并通过符号AI的视角重新审视这些创新。
- en: '3.1 Autonomous Agents: Classic and LLM-empowered'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自主智能体：经典的与LLM驱动的
- en: An autonomous agent is an artificially intelligent entity designed to achieve
    specific goals independently, acquiring contextual factors to perceive the environmental
    state and undertaking context-relevant actions [[49](#bib.bib49)]. These agents,
    equipped with reasoning, learning, and adaptability, thrive in dynamic and complex
    contexts. Unlike traditional software programs that follow predetermined rules,
    autonomous agents operate with self-governing attributes, allowing them to function
    under varying conditions [[50](#bib.bib50)]. Leveraging these capabilities, they
    facilitate automation by performing tasks that typically require human intervention,
    enhancing efficiency, and reducing operational costs across fields such as robotics,
    communication, financial trading, and healthcare [[50](#bib.bib50)]. For instance,
    in robotic applications, autonomous agents can navigate tasks with minimal supervision,
    continuously monitor their surroundings, and adapt to new situations, making them
    robust solutions for long-term automation [[51](#bib.bib51), [52](#bib.bib52)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理是一个人工智能实体，旨在独立实现特定目标，获取上下文因素以感知环境状态并采取相关的行动[[49](#bib.bib49)]。这些代理具备推理、学习和适应能力，能够在动态和复杂的环境中发挥作用。与遵循预定规则的传统软件程序不同，自主代理具有自我管理属性，使它们能够在变化的条件下运行[[50](#bib.bib50)]。利用这些能力，它们通过执行通常需要人类干预的任务来促进自动化，提高效率，并降低在机器人、通信、金融交易和医疗保健等领域的运营成本[[50](#bib.bib50)]。例如，在机器人应用中，自主代理可以在最小监督下导航任务，持续监控周围环境，并适应新情况，使其成为长期自动化的强大解决方案[[51](#bib.bib51),
    [52](#bib.bib52)]。
- en: The foundational techniques of autonomous agent design originate from classic
    AI approaches, such as Probabilistic Graphical Models [[53](#bib.bib53)], Reinforcement
    Learning [[54](#bib.bib54)], and Multi-Agent Systems [[55](#bib.bib55)], which
    manage uncertainty and learn optimal behaviors in dynamic environments or enable
    agents to interact and share information efficiently. However, the advent of LAAs
    marks a significant evolution beyond traditional AI for both symbolic and neural
    sub-systems. These agents use extensive pre-training on vast textual corpora to
    acquire broad knowledge, performing human reasoning tasks by generating contextually
    appropriate text [[56](#bib.bib56)]. This capability not only simulates understanding
    and decision-making but also allows the generation of code and other communicative
    texts, enhancing their practical utility [[57](#bib.bib57)]. By integrating pre-trained
    language models with natural language understanding, LAAs adapt flexibly to diverse
    scenarios, expanding AI’s potential in autonomous operations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理设计的基础技术源于经典AI方法，如概率图模型[[53](#bib.bib53)]、强化学习[[54](#bib.bib54)]和多智能体系统[[55](#bib.bib55)]，这些方法管理不确定性并学习动态环境中的最佳行为，或者使代理能够高效地互动和共享信息。然而，LAAs的出现标志着超越传统AI的显著演变，无论是符号子系统还是神经子系统。这些代理通过对大量文本语料库的广泛预训练来获取广泛的知识，通过生成语境适当的文本[[56](#bib.bib56)]执行人类推理任务。这种能力不仅模拟了理解和决策，还允许生成代码和其他交流文本，从而增强了它们的实际实用性[[57](#bib.bib57)]。通过将预训练语言模型与自然语言理解相结合，LAAs灵活地适应各种场景，扩展了AI在自主操作中的潜力。
- en: 3.2 Design and Implementation of LAAs
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LAAs的设计与实现
- en: Central to the design of an agent is its neural sub-system–an LLM, which functions
    as the core controller or coordinator. The LLM orchestrates with the agent’s symbolic
    sub-system and external tools, including a planning and reasoning component for
    task decomposition and self-reflection, memory (both short-term and long-term),
    and a tool-use component that allows access to external information and functionalities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 设计代理的核心是其神经子系统——LLM，作为核心控制器或协调者。LLM与代理的符号子系统和外部工具进行协调，包括用于任务分解和自我反思的规划和推理组件、记忆（包括短期和长期记忆）以及允许访问外部信息和功能的工具使用组件。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Agentic Workflow: An agentic workflow combines planning, reasoning, memory
    management, tool integration, and user interfaces with LLMs. Frameworks, such
    as LangChain [[58](#bib.bib58)] and LlamaIndex [[59](#bib.bib59)], help design
    these workflows.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Agentic Workflow: Agentic workflow将规划、推理、记忆管理、工具集成和用户界面与LLMs结合起来。框架，如LangChain[[58](#bib.bib58)]和LlamaIndex[[59](#bib.bib59)]，帮助设计这些工作流程。'
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Planner and Reasoner: Advanced techniques such as chain-of-thought and tree-of-thought
    prompting [[60](#bib.bib60)] break down tasks into sub-tasks, with self-reflection
    allowing agents to critique and refine outputs [[61](#bib.bib61)].'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计划者与推理者：高级技术，如链式思维和树状思维提示[[60](#bib.bib60)]，将任务分解为子任务，自我反思使代理能够批判和完善输出[[61](#bib.bib61)]。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Memory Management: Incorporates short-term memory for context and long-term
    memory using external storage, such as vector databases, enabling efficient information
    retrieval and enhanced reasoning [[62](#bib.bib62), [63](#bib.bib63)].'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内存管理：结合了用于上下文的短期记忆和使用外部存储（如向量数据库）的长期记忆，从而实现高效的信息检索和增强的推理能力[[62](#bib.bib62),
    [63](#bib.bib63)]。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tool-Use & Natural Language Interface (NLI) Integration: Agents can access
    external tools, APIs, and models, deciding when and how to utilize them based
    on task goals [[64](#bib.bib64), [65](#bib.bib65)]. In addition, An effective
    NLI interprets user requests and communicates actions [[66](#bib.bib66)]. Techniques,
    such as ReAct and MRKL, provide structured interaction steps (thought, action,
    action input, observation) [[67](#bib.bib67), [68](#bib.bib68)].'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具使用与自然语言接口（NLI）集成：代理可以访问外部工具、API和模型，根据任务目标决定何时及如何使用它们[[64](#bib.bib64), [65](#bib.bib65)]。此外，有效的NLI可以解释用户请求并传达行动[[66](#bib.bib66)]。如ReAct和MRKL等技术提供了结构化的交互步骤（思考、行动、行动输入、观察）[[67](#bib.bib67),
    [68](#bib.bib68)]。
- en: By integrating these components, LAAs can tackle complex tasks. However, challenges
    like limited context windows, long-term planning, and reliable interfaces remain,
    necessitating ongoing research and development.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合这些组件，LAA能够处理复杂任务。然而，有限的上下文窗口、长期规划和可靠的接口等挑战仍然存在，需要持续的研究和发展。
- en: 3.3 Rethink LAAs from the Perspective of Neuro-symbolic AI
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 从神经符号人工智能的角度重新思考LAA
- en: Neuro-symbolic AI combines the strengths of neural networks and symbolic reasoning,
    producing decision-making processes that are both explicit and interpretable.
    In autonomous agents enhanced by LLMs, the latest advancements in deep neural
    networks are harnessed, while task decomposition and planning are guided by symbolic
    AI principles — breaking complex tasks into discrete, logical steps that can be
    systematically analyzed and reasoned through [[69](#bib.bib69)]. This fusion of
    symbolic structures and deep neural networks creates a powerful synergy, significantly
    boosting the capabilities of these agents.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 神经符号人工智能结合了神经网络和符号推理的优点，产生了既明确又可解释的决策过程。在由LLM增强的自主代理中，利用了深度神经网络的最新进展，同时任务分解和规划则遵循符号人工智能的原则——将复杂任务分解为离散的逻辑步骤，以便系统地分析和推理[[69](#bib.bib69)]。这种符号结构和深度神经网络的融合创造了强大的协同效应，显著提升了这些代理的能力。
- en: Symbolic Modeling and Neural Representation
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 符号建模与神经表征
- en: Classic symbolic AI represents knowledge using abstractions and symbols, utilizing
    explicit symbolic modeling such as rules and relationships to perform reasoning [[70](#bib.bib70)].
    This approach typically involves well-defined logic and structured knowledge bases,
    enabling systems to behave based on pre-defined rules. In contrast, LAAs, driven
    by language models, represent knowledge in a more distributed and implicit manner.
    Instead of relying on explicit symbols and rules, these agents leverage vast amounts
    of corpus and self-supervised pre-training on language models to infer patterns
    and relationships from raw text [[25](#bib.bib25)]. The knowledge is embedded
    within the weights of LLMs, allowing for more flexible and context-driven reasoning.
    This advantage fundamentally contrasts with the rigidity of symbolic AI, providing
    LAAs with the ability to handle ambiguity and generate more human-like responses [[5](#bib.bib5)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的符号人工智能通过抽象和符号来表示知识，利用明确的符号建模，如规则和关系，进行推理[[70](#bib.bib70)]。这种方法通常涉及明确的逻辑和结构化知识库，使系统能够基于预定义的规则进行行为。相比之下，LAA，由语言模型驱动，以更分散和隐式的方式表示知识。它们不依赖于明确的符号和规则，而是利用大量的语料库和语言模型的自监督预训练，从原始文本中推断模式和关系[[25](#bib.bib25)]。知识嵌入在LLM的权重中，使推理更加灵活和基于上下文。这种优势与符号人工智能的刚性形成了根本对比，使LAA能够处理模糊性并生成更像人类的响应[[5](#bib.bib5)]。
- en: Search-based Decision Making by Generation
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于生成的搜索决策
- en: Given a complex goal requiring multiple steps to achieve, existing agent technologies
    either harness symbolic AI to systematically explore the space of potential actions
    or employ reinforcement learning to optimize the trajectory of these actions,
    efficiently partitioning complex tasks into manageable subtasks [[54](#bib.bib54)].
    Within a LLM-empowered agent, the Chain-of-Thought (CoT) method guides LLMs to
    generate texts about intermediate reasoning steps, enhancing their cognitive task
    performance [[47](#bib.bib47)]. By breaking tasks into logical sequences, CoT
    prompts encourage LLMs to structure their reasoning systematically. This method
    overcomes LLM limitations at the token level by enabling coherent, step-by-step
    elaboration of thought processes, improving problem-solving accuracy and reliability.
    More recently, Tree-of-Thought (ToT) prompting extends this approach by allowing
    LLMs to explore multiple reasoning paths simultaneously in a tree structure [[71](#bib.bib71)]
    and the proposal of functional search over program generation, leveraging large
    language models (LLMs), successfully facilitates mathematical discoveries [[72](#bib.bib72)].
    These methods enhance LLM problem-solving abilities by promoting dynamic and reflective
    reasoning processes, closely mirroring symbolic reasoning techniques, on top of
    a neural basis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 针对需要多个步骤才能实现的复杂目标，现有的代理技术要么利用符号AI系统性地探索潜在行动的空间，要么利用强化学习优化这些行动的轨迹，从而高效地将复杂任务分解为可管理的子任务[[54](#bib.bib54)]。在一个由LLM赋能的代理中，思维链（CoT）方法引导LLMs生成关于中间推理步骤的文本，从而提升它们的认知任务表现[[47](#bib.bib47)]。通过将任务分解为逻辑序列，CoT提示促使LLMs系统性地组织推理。这种方法通过实现连贯的、逐步展开的思维过程，克服了LLM在token层面的限制，从而提高了解决问题的准确性和可靠性。最近，思维树（ToT）提示扩展了这一方法，允许LLMs在树状结构中同时探索多个推理路径[[71](#bib.bib71)]，以及利用大型语言模型（LLMs）在程序生成上的功能搜索提案，成功促进了数学发现[[72](#bib.bib72)]。这些方法通过促进动态和反思性推理过程，在神经基础上紧密地模拟了符号推理技术，从而提升了LLM的解决问题能力。
- en: Case-based Reasoning through In-context Learning
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过上下文学习进行的案例推理
- en: An agent must adapt to new situations, while traditional methods rely on either
    re-training neural networks or deducing examples of new situations into rules
    for better reasoning. Within a LLM-empowered agent, few-shot in-context learning
    (ICL) has been proposed to utilize given examples into a prompt to generate appropriate
    responses that solve problems without explicit re-training the LLM [[73](#bib.bib73)].
    This approach mimics the *case-based reasoning*, a fundamental concept in symbolic
    AI, by leveraging explicit knowledge and experiences to tackle new problems. This
    enhances the model’s ability to generalize from specific examples, effectively
    creating a neuro-symbolic mapping from presented examples to desired outcomes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 代理必须适应新情况，而传统方法则依赖于重新训练神经网络或将新情况的示例推导为规则以便更好地推理。在一个由LLM赋能的代理中，提出了少量示例上下文学习（ICL）的方法，通过将给定示例纳入提示来生成适当的响应，从而在不显式重新训练LLM的情况下解决问题[[73](#bib.bib73)]。这种方法通过利用显性知识和经验来解决新问题，模仿了*案例推理*，这是符号AI中的一个基本概念。这增强了模型从特定示例中进行概括的能力，有效地创建了从呈现示例到期望结果的神经符号映射。
- en: Neuro-symbolic Integration Driven by Emergent Abilities
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 由突现能力驱动的神经符号集成
- en: The emergent abilities of LLMs, such as contextual understanding, sequential
    reasoning, goal reformulation, and task decomposition, are surged by over-parameterized
    architectures and extensive pre-training corpora [[46](#bib.bib46)]. Combining
    well-designed rules with the emergent abilities of LLMs enables agents to create
    and follow complex workflows, known as agentic workflows. By prompting large language
    models with instructions like “let’s think step by step”, these models analogise
    human’s reasoning processes and can exhibit logical and mathematical reasoning,
    thereby enhancing their structured reasoning skills [[12](#bib.bib12), [13](#bib.bib13)].
    This agentic approach allows LLMs to not only process but also proactively generate
    structured, logical, and adaptive reasoning pathways [[56](#bib.bib56)], significantly
    improving their problem-solving and decision-making capabilities, marking a pivotal
    evolution in neuro-symbolic AI technologies.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的出现能力，如上下文理解、序列推理、目标重构和任务分解，得益于过度参数化的架构和广泛的预训练语料库[[46](#bib.bib46)]。将设计良好的规则与LLM的出现能力相结合，使得智能体能够创建和遵循复杂的工作流，即代理工作流。通过用“让我们一步步思考”这样的指令来提示大型语言模型，这些模型模拟了人类的推理过程，并可以展示逻辑和数学推理，从而增强其结构化推理能力[[12](#bib.bib12),
    [13](#bib.bib13)]。这种代理方法使得LLM不仅能够处理，还能主动生成结构化、逻辑化和自适应的推理路径[[56](#bib.bib56)]，显著提高了它们的问题解决和决策能力，标志着神经符号AI技术的关键进展。
- en: 4 Discussions and Future Directions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论与未来方向
- en: In this section, we discuss the LLM-empowered autonomous agent by comparing
    it with an alternative neuro-symbolic approach—the Knowledge Graph—and then highlight
    future directions for this technology.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过与另一种神经符号方法——知识图谱——进行比较，讨论了由LLM驱动的自主智能体，并强调了这一技术的未来方向。
- en: '4.1 Comparative Analysis: LAAs versus KGs'
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 比较分析：LAAs与KGs
- en: Previous sections have presented LAAs and KGs, both of which exemplify neuro-symbolic
    approaches to AI. We here compare these two methodologies to highlight the superior
    positioning of LAAs in the current wave of AI advancements.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的章节介绍了LAAs和KGs，这两者都体现了神经符号方法在人工智能中的应用。在这里，我们比较这两种方法，以突出LAAs在当前人工智能发展浪潮中的优越定位。
- en: KGs harness the power of symbolic AI, organizing domain-specific knowledge through
    explicit relationships and rules. This design makes them highly effective in static
    environments where precision, interpretability, and predefined schemas are crucial.
    Their logical reasoning capabilities ensure that outputs are consistent and verifiable,
    which is paramount for applications needing clarity and exactitude in knowledge
    modeling [[74](#bib.bib74)]. In addition, the scalability of KGs is inherently
    limited by their requirements of explicit schema definitions and manual updates [[75](#bib.bib75)].
    As the volume of data grows, the complexity of managing and querying the graph
    increases significantly. The maintenance of a large-scale KG demands substantial
    computational resources and human expertise, affecting efficiency and agility
    in evolving environments.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱利用符号AI的力量，通过显式的关系和规则组织领域特定的知识。这种设计使它们在静态环境中非常有效，在这些环境中，精确性、可解释性和预定义模式至关重要。它们的逻辑推理能力确保了输出的一致性和可验证性，这对需要知识建模清晰性和准确性的应用程序至关重要[[74](#bib.bib74)]。此外，知识图谱的可扩展性本质上受到显式模式定义和手动更新的限制[[75](#bib.bib75)]。随着数据量的增加，管理和查询图的复杂性显著增加。维护大规模知识图谱需要大量计算资源和人力专业知识，影响了在不断变化的环境中的效率和灵活性。
- en: On the other hand, LAAs are designed with a more dynamic and flexible approach.
    By combining the language comprehension and generation abilities of neural networks
    with the structured reasoning of symbolic AI, these agents are equipped to tackle
    a wide range of complex tasks. The implicit knowledge stored in neural networks
    enables context-sensitive responses and seamless adaptation to changing environments [[76](#bib.bib76)].
    Additionally, LLMs efficiently compress vast corpora into a learnable network,
    making these agents highly scalable. Once trained, the models can be fine-tuned
    with additional data at a fraction of the cost and effort required for updating
    knowledge graphs, and can even support in-context learning without fine-tuning.
    As a result, LLM-powered agents can handle larger datasets with ease and even
    process online data to respond to real-time changes effectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LAA采用了更具动态性和灵活性的方法。通过结合神经网络的语言理解与生成能力以及符号AI的结构化推理，这些代理能够处理各种复杂任务。神经网络中存储的隐性知识使得它们能够提供上下文敏感的响应，并能无缝适应变化的环境[[76](#bib.bib76)]。此外，LLM能够高效地将庞大的语料库压缩成一个可学习的网络，使这些代理具备高度的可扩展性。一旦训练完成，这些模型可以以极低的成本和努力来用额外的数据进行微调，甚至可以在不微调的情况下支持上下文学习。因此，基于LLM的代理可以轻松处理更大的数据集，甚至可以处理在线数据以有效响应实时变化。
- en: Furthermore, the advanced reasoning mechanisms employed by LAAs, such as CoT [[47](#bib.bib47)]
    and ToT [[71](#bib.bib71)], enable them to break down and solve complex problems
    effectively through analogising human reasoning steps [[12](#bib.bib12)]. These
    methods mitigate the limitations of token-level constraints in LLMs, fostering
    a more robust and contextually aware decision-making process. As a result, LAAs
    are poised to drive future innovations in AI, offering more versatile and intelligent
    solutions than their knowledge graph counterparts.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LAA使用的先进推理机制，如CoT[[47](#bib.bib47)]和ToT[[71](#bib.bib71)]，使它们能够通过类比人类推理步骤有效地分解和解决复杂问题[[12](#bib.bib12)]。这些方法缓解了LLM在令牌级约束方面的局限性，促进了更强健且具备上下文意识的决策过程。因此，LAA有望推动AI领域的未来创新，提供比知识图谱解决方案更为多样化和智能的解决方案。
- en: 4.2 Future Directions
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 未来方向
- en: Following prior discussions, we propose several future research directions aimed
    at enhancing the current landscape of LAAs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前的讨论，我们提出了几个未来的研究方向，旨在增强当前LAA的应用格局。
- en: Neuro-vector-symbolic Integrative Intelligence
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经-矢量-符号集成智能
- en: Current agentic reasoning approaches emulate human reasoning steps explicitly
    [[12](#bib.bib12)]. For instance, when an agent receives a user’s request, it
    retrieves similar cases and enhances its actions through in-context learning,
    and for ambiguous requests, the agent prompts the LLM to clarify and rewrite the
    request in various forms [[77](#bib.bib77)]. This process involve extracting vectors
    for each rewritten request and performing multi-vector retrieval, improving context
    understanding and generative performance but increasing computational load. A
    vector-centric perspective, utilizing encoder-decoder architectures such as GritLM
    [[78](#bib.bib78)] that prompt generative models for instructional text encoding/vectorization,
    implicit neural reasoners that extend transformers with causal relation graphs
    for enhanced long-range reasoning [[79](#bib.bib79)] with latent vectors and attention
    matrices, and vector-symbolic architectures (VSAs) [[80](#bib.bib80)], could significantly
    address this problem. Specifically, the VSA employs high-dimensional vectors to
    encode and manipulate information, allowing the representation of complex structures
    and relationships compactly and contextually [[80](#bib.bib80)]. It models the
    cognitive and reasoning processes as *algebraic operations* in the vector space.
    Combining VSAs with LLMs could enhance cognitive capabilities, enabling precise
    multi-step decision-making, with applications in scientific discovery, such as
    solving Raven’s progressive matrices [[81](#bib.bib81)], thus accelerating the
    convergence between connectionist and symbolic paradigms through computable vectorization.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的代理推理方法明确模拟人类推理步骤[[12](#bib.bib12)]。例如，当代理接收到用户的请求时，它会检索类似的案例，并通过上下文学习增强其行动，对于模糊的请求，代理会提示LLM澄清并以不同形式重新编写请求[[77](#bib.bib77)]。这一过程涉及为每个重写的请求提取向量并执行多向量检索，提高了上下文理解和生成性能，但也增加了计算负担。一个以向量为中心的视角，利用如GritLM[[78](#bib.bib78)]的编码器-解码器架构，提示生成模型进行指令文本编码/向量化，隐式神经推理器将变换器扩展为因果关系图以增强远程推理[[79](#bib.bib79)]，以及向量-符号架构（VSAs）[[80](#bib.bib80)]，可能显著解决这一问题。具体来说，VSA使用高维向量来编码和操控信息，允许以紧凑和上下文相关的方式表示复杂结构和关系[[80](#bib.bib80)]。它将认知和推理过程建模为向量空间中的*代数运算*。将VSA与LLM结合可以增强认知能力，使多步骤决策更为精确，具有科学发现的应用，如解决Raven的渐进矩阵[[81](#bib.bib81)]，从而通过可计算的向量化加速联结主义和符号范式之间的融合。
- en: '![Refer to caption](img/f9152975f749ae37c882fcaeb32f3120.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/f9152975f749ae37c882fcaeb32f3120.png)'
- en: 'Figure 4: An Illustrative Example of Program-of-Thoughts (PoT) for Mathematical
    Proof Verification'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：数学证明验证的思维程序（PoT）示例
- en: Program-of-Thoughts Reasoning
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维程序推理
- en: 'We here illustrate the proposal of Program-of-Thoughts (PoT) for agentic reasoning
    in a rigorous manner, building on the methodologies of CoT and ToT prompting.
    Specifically, PoT decomposes complex reasoning processes into a series of propositions
    organized in linear or tree structures. It leverages the programming language
    for program proofs, such as Dafny [[82](#bib.bib82)] or Lean [[83](#bib.bib83)],
    to model and verify these propositions. Future research should focus on refining
    proposition modeling and verification by prompting LLMs for code generation [[84](#bib.bib84)],
    improving integration with external theorem provers and assertion verifiers (e.g.,
    Dafny and Lean), and scaling PoT to handle multi-modal data for advanced reasoning.
    Further, automating code generation, optimizing hybrid PoT/CoT/ToT models, incorporating
    self-verification and self-correction, and adopting PoT into domain-specific applications,
    including logical deduction and scientific discovery can significantly advance
    its capabilities [[72](#bib.bib72)]. Figure [4](#S4.F4 "Figure 4 ‣ Neuro-vector-symbolic
    Integrative Intelligence ‣ 4.2 Future Directions ‣ 4 Discussions and Future Directions
    ‣ Converging Paradigms: The Synergy of Symbolic and Connectionist AI in LLM-Empowered
    Autonomous Agents") demonstrates the use of the PoT framework to verify a basic
    mathematical proof that for any even integer $n$ positive integers is an even
    number. By decomposing the problem into distinct, verifiable propositions and
    using Dafny for formal verification, this example highlights the structured and
    rigorous approach of PoT in logical reasoning and verification.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在这里以严谨的方式阐述了程序化思维（PoT）的提案，用于代理推理，基于CoT和ToT提示的方法论。具体来说，PoT将复杂的推理过程分解为一系列线性或树状结构组织的命题。它利用编程语言进行程序证明，如Dafny [[82](#bib.bib82)]或Lean [[83](#bib.bib83)]，来建模和验证这些命题。未来的研究应着重于通过提示LLMs进行代码生成 [[84](#bib.bib84)]来完善命题建模和验证，提高与外部定理证明器和断言验证器（如Dafny和Lean）的集成，并扩展PoT以处理多模态数据以支持高级推理。此外，自动化代码生成、优化混合PoT/CoT/ToT模型、引入自我验证和自我修正，并将PoT应用于领域特定的应用，包括逻辑推理和科学发现，将显著提升其能力 [[72](#bib.bib72)]。图 [4](#S4.F4
    "Figure 4 ‣ Neuro-vector-symbolic Integrative Intelligence ‣ 4.2 Future Directions
    ‣ 4 Discussions and Future Directions ‣ Converging Paradigms: The Synergy of Symbolic
    and Connectionist AI in LLM-Empowered Autonomous Agents") 展示了PoT框架用于验证一个基本的数学证明，即对于任何偶整数
    $n$，正整数是偶数。通过将问题分解为明确的、可验证的命题，并使用Dafny进行形式验证，这个例子突显了PoT在逻辑推理和验证中的结构化和严谨方法。'
- en: 5 Conclusions
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In conclusion, the synthesis of connectionist and symbolic paradigms, particularly
    through the rise of LLM-empowered Autonomous Agents (LAAs), marks a pivotal evolution
    in the field of AI, especially the neuro-symbolic AI. This paper has highlighted
    the historical context and the ongoing convergence of symbolic reasoning and neural
    network-based methods, underscoring how LAAs leverage the text-based knowledge
    representation and generative capabilities of LLMs to achieve logical reasoning
    and decision-making. By contrasting LAAs with Knowledge Graphs (KGs), we have
    demonstrated the unique advantages of LAAs in mimicking human-like reasoning processes,
    scaling effectively with large datasets, and leveraging in-context learning without
    extensive re-training. Promising directions such as neuro-vector-symbolic architectures
    and program-of-thoughts (PoT) prompting are on the horizon, potentially enhancing
    the agentic reasoning capabilities of AI further. These insights not only encapsulate
    the transformative potential of current AI technologies but also provide a clear
    trajectory for future research, fostering a deeper understanding and more advanced
    applications of neuro-symbolic AI.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，连接主义和符号主义范式的综合，特别是通过LLM赋能的自主代理（LAA）的兴起，标志着AI领域，尤其是神经符号AI的一个重要演变。本文强调了历史背景和符号推理与基于神经网络的方法的持续融合，强调了LAA如何利用LLM的文本知识表示和生成能力来实现逻辑推理和决策。通过将LAA与知识图谱（KG）进行对比，我们展示了LAA在模拟类人推理过程、有效扩展大数据集以及利用上下文学习而无需广泛再训练方面的独特优势。前景广阔的方向，如神经-向量-符号架构和程序化思维（PoT）提示，正在到来，可能进一步提升AI的代理推理能力。这些见解不仅概括了当前AI技术的变革潜力，还为未来研究提供了明确的方向，促进了对神经符号AI的更深理解和更高级应用。
- en: References
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature,
    521(7553):436–444, 2015.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 深度学习. Nature, 521(7553):436–444,
    2015.'
- en: '[2] Allen Newell and Herbert A Simon. Computer science as empirical inquiry:
    Symbols and search. In ACM Turing award lectures, page 1975\. 2007.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 艾伦·纽厄尔 和 赫伯特·A·西蒙. 计算机科学作为经验性研究：符号与搜索. 在 ACM 图灵奖讲座中, 页码 1975\. 2007.'
- en: '[3] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. Advances in neural information processing
    systems, 25, 2012.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 亚历克斯·克里日夫斯基, 伊利亚·苏茨克维尔, 和 杰弗里·E·辛顿. 使用深度卷积神经网络进行 ImageNet 分类. 神经信息处理系统进展,
    25, 2012.'
- en: '[4] Edward Shortliffe. Computer-based medical consultations: MYCIN, volume 2.
    Elsevier, 2012.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 爱德华·肖特利夫. 基于计算机的医学咨询：MYCIN, 第 2 卷. 爱思唯尔, 2012.'
- en: '[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 汤姆·布朗, 本杰明·曼恩, 尼克·瑞德, 梅拉妮·萨比亚, 贾瑞德·D·卡普兰, 普拉夫拉·达里瓦尔, 阿尔文·尼拉坎坦, 普拉纳夫·夏姆,
    吉里什·萨斯特里, 阿曼达·阿斯克尔, 等. 语言模型是少样本学习者. 神经信息处理系统进展, 33:1877–1901, 2020.'
- en: '[6] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
    et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,
    2019.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 亚历克·拉德福德, 杰弗里·吴, 瑞温·柴尔德, 大卫·罗安, 达里奥·阿莫代, 伊利亚·苏茨克维, 等. 语言模型是无监督的多任务学习者.
    OpenAI 博客, 1(8):9, 2019.'
- en: '[7] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 塞巴斯蒂安·布贝克, 瓦伦·钱德拉塞卡兰, 罗南·艾尔丹, 约翰内斯·格尔克, 埃里克·霍维茨, 埃切·卡马尔, 彼得·李, 尹·塔特·李,
    袁志丽, 斯科特·伦德伯格, 等. 人工通用智能的火花：与 GPT-4 的早期实验. arXiv 预印本 arXiv:2303.12712, 2023.'
- en: '[8] Haoyi Xiong, Jiang Bian, Sijia Yang, Xiaofei Zhang, Linghe Kong, and Daqing
    Zhang. Natural language based context modeling and reasoning with llms: A tutorial.
    arXiv preprint arXiv:2309.15074, 2023.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 熊浩毅, 边江, 杨思佳, 张晓飞, 孔玲赫, 张大庆. 基于自然语言的上下文建模与推理使用大型语言模型：教程. arXiv 预印本 arXiv:2309.15074,
    2023.'
- en: '[9] Jonathan St BT Evans. In two minds: dual-process accounts of reasoning.
    Trends in cognitive sciences, 7(10):454–459, 2003.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 乔纳森·斯特 BT 埃文斯. 两种思维：双重过程推理的解释. 认知科学趋势, 7(10):454–459, 2003.'
- en: '[10] Yoshua Bengio. Deep learning for system 2 processing. AAAI 2020, 2020.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 约书亚·本吉奥. 系统 2 处理的深度学习. AAAI 2020, 2020.'
- en: '[11] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip.
    A survey on knowledge graphs: Representation, acquisition, and applications. IEEE
    Transactions on Neural Networks and Learning Systems, 33(2):494–514, 2021.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 姚绍雄, 潘世睿, 埃里克·坎布里亚, 佩卡·马尔蒂宁, 和 S Yu Philip. 知识图谱调查：表示、获取与应用. IEEE 神经网络与学习系统汇刊,
    33(2):494–514, 2021.'
- en: '[12] Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning
    in large language models. Nature Human Behaviour, 7(9):1526–1541, 2023.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 泰勒·韦布, 基思·J·霍利奥克, 和 洪晶·陆. 大型语言模型中的类比推理的出现. 《自然·人类行为》, 7(9):1526–1541,
    2023.'
- en: '[13] James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio
    Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido
    Manzi, et al. Testing theory of mind in large language models and humans. Nature
    Human Behaviour, pages 1–11, 2024.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 詹姆斯·WA·斯特拉坎, 达莉拉·阿尔贝尔戈, 朱莉娅·博尔吉尼, 奥里亚娜·潘萨尔迪, 尤金尼奥·斯卡利蒂, 索拉布·古普塔, 克拉提·萨克塞纳,
    亚历山德罗·鲁福, 斯特凡诺·潘泽里, 圭多·曼齐, 等. 在大型语言模型和人类中测试心智理论. 《自然·人类行为》, 页码 1–11, 2024.'
- en: '[14] Artur d’Avila Garcez and Luis C Lamb. Neurosymbolic ai: The 3 rd wave.
    Artificial Intelligence Review, 56(11):12387–12406, 2023.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 阿图尔·d’阿维拉·加尔塞斯 和 路易斯·C·兰姆. 神经符号 AI：第三波. 人工智能评论, 56(11):12387–12406, 2023.'
- en: '[15] Frank Rosenblatt. The perceptron: a probabilistic model for information
    storage and organization in the brain. Psychological review, 65(6):386, 1958.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 弗兰克·罗森布拉特. 感知机：一种脑中信息存储与组织的概率模型. 心理学评论, 65(6):386, 1958.'
- en: '[16] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning
    representations by back-propagating errors. Nature, 323(6088):533–536, 1986.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 大卫·E·鲁梅尔哈特, 杰弗里·E·辛顿, 和 罗纳德·J·威廉姆斯. 通过反向传播误差学习表示. 《自然》, 323(6088):533–536,
    1986.'
- en: '[17] Allen Newell and Herbert Simon. The logic theory machine–a complex information
    processing system. IRE Transactions on information theory, 2(3):61–79, 1956.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 艾伦·纽厄尔 和 赫伯特·西蒙. 逻辑理论机器——一种复杂的信息处理系统. IRE 信息论汇刊, 2(3):61–79, 1956.'
- en: '[18] Bruce G Buchanan and Edward A Feigenbaum. Dendral and meta-dendral: Their
    applications dimension. Artificial intelligence, 11(1-2):5–24, 1978.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 布鲁斯·G·布坎南 和 爱德华·A·费根鲍姆. Dendral 和 Meta-Dendral：它们的应用维度. 人工智能, 11(1-2):5–24,
    1978.'
- en: '[19] Ashok Kumar Goel. Integration of case-based reasoning and model-based
    reasoning for adaptive design problem-solving. The Ohio State University, 1989.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Ashok Kumar Goel。《案例推理与模型推理在自适应设计问题解决中的整合》。俄亥俄州立大学，1989 年。'
- en: '[20] Zachary C Lipton. The mythos of model interpretability: In machine learning,
    the concept of interpretability is both important and slippery. Queue, 16(3):31–57,
    2018.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Zachary C Lipton。《模型可解释性的神话：在机器学习中，可解释性的概念既重要又难以捉摸》。Queue，16(3):31–57，2018
    年。'
- en: '[21] Edward A Feigenbaum et al. The art of artificial intelligence: Themes
    and case studies of knowledge engineering. 1977.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Edward A Feigenbaum 等。《人工智能的艺术：知识工程的主题与案例研究》。1977 年。'
- en: '[22] Charles Elkan and Russell Greiner. Building large knowledge-based systems:
    Representation and inference in the cyc project: Db lenat and rv guha, 1993.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Charles Elkan 和 Russell Greiner。《构建大型知识基础系统：Cyc 项目的表示与推理：Db lenat 和 rv
    guha》，1993 年。'
- en: '[23] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai.
    Communications of the ACM, 64(7):58–65, 2021.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yoshua Bengio、Yann Lecun 和 Geoffrey Hinton。《人工智能的深度学习》。ACM 通讯，64(7):58–65，2021
    年。'
- en: '[24] Artur SD’Avila Garcez, Luis C Lamb, and Dov M Gabbay. Neural-symbolic
    cognitive reasoning. Springer Science & Business Media, 2008.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Artur SD’Avila Garcez、Luis C Lamb 和 Dov M Gabbay。《神经-符号认知推理》。Springer
    科学与商业媒体，2008 年。'
- en: '[25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。Bert: 语言理解的深度双向变换器的预训练。arXiv
    预印本 arXiv:1810.04805，2018 年。'
- en: '[26] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou,
    Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
    et al. Mastering the game of go without human knowledge. Nature, 550(7676):354–359,
    2017.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] David Silver、Julian Schrittwieser、Karen Simonyan、Ioannis Antonoglou、Aja
    Huang、Arthur Guez、Thomas Hubert、Lucas Baker、Matthew Lai、Adrian Bolton 等。《无需人类知识的围棋大师》。自然，550(7676):354–359，2017
    年。'
- en: '[27] World Wide Web Consortium (W3C). Resource description framework (rdf)
    model and syntax specification, 1999. Retrieved from [https://www.w3.org/TR/1999/REC-rdf-syntax-19990222/](https://www.w3.org/TR/1999/REC-rdf-syntax-19990222/).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] World Wide Web Consortium (W3C)。资源描述框架（rdf）模型和语法规范，1999 年。取自 [https://www.w3.org/TR/1999/REC-rdf-syntax-19990222/](https://www.w3.org/TR/1999/REC-rdf-syntax-19990222/)。'
- en: '[28] Tim Berners-Lee, James Hendler, and Ora Lassila. The semantic web: A new
    form of web content that is meaningful to computers will unleash a revolution
    of new possibilities. In Linking the World’s Information: Essays on Tim Berners-Lee’s
    Invention of the World Wide Web, pages 91–103\. 2023.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Tim Berners-Lee、James Hendler 和 Ora Lassila。《语义网：一种对计算机有意义的新型网页内容将引发新可能性的革命》。在《链接世界信息：关于
    Tim Berners-Lee 发明的世界范围网的文章》中，第 91–103 页，2023 年。'
- en: '[29] Grigoris Antoniou and Frank Van Harmelen. A semantic web primer. MIT press,
    2004.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Grigoris Antoniou 和 Frank Van Harmelen。《语义网入门》。MIT出版社，2004 年。'
- en: '[30] Thomas R Gruber. A translation approach to portable ontology specifications.
    Knowledge Acquisition, 5(2):199–220, 1993.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Thomas R Gruber。《便携本体规范的翻译方法》。知识获取，5(2):199–220，1993 年。'
- en: '[31] Stanley Kok and Pedro Domingos. Learning the structure of markov logic
    networks. In Proceedings of the 22nd International Conference on Machine Learning,
    pages 441–448, 2005.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Stanley Kok 和 Pedro Domingos。《学习马尔可夫逻辑网络的结构》。在第 22 届国际机器学习会议论文集中，第 441–448
    页，2005 年。'
- en: '[32] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich.
    A review of relational machine learning for knowledge graphs. Proceedings of the
    IEEE, 104(1):11–33, 2015.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Maximilian Nickel、Kevin Murphy、Volker Tresp 和 Evgeniy Gabrilovich。《关系机器学习在知识图谱中的回顾》。IEEE
    期刊，104(1):11–33，2015 年。'
- en: '[33] Thomas N Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. In International Conference on Learning Representations,
    2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Thomas N Kipf 和 Max Welling。《图卷积网络的半监督分类》。在国际学习表征会议上，2022 年。'
- en: '[34] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding:
    A survey of approaches and applications. IEEE Transactions on Knowledge and Data
    Engineering, 29(12):2724–2743, 2017.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Quan Wang、Zhendong Mao、Bin Wang 和 Li Guo。《知识图谱嵌入：方法与应用综述》。IEEE 知识与数据工程学报，29(12):2724–2743，2017
    年。'
- en: '[35] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Sepp Hochreiter 和 Jürgen Schmidhuber。《长短期记忆》。神经计算，9(8):1735–1780，1997
    年。'
- en: '[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan
    N Gomez、Łukasz Kaiser 和 Illia Polosukhin。《Attention is all you need》。神经信息处理系统进展，30，2017。'
- en: '[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。《Exploring the limits of transfer learning
    with a unified text-to-text transformer》。机器学习研究杂志，21(140):1–67，2020。'
- en: '[38] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge Akkaya、Florencia
    Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal Anadkat 等人。《Gpt-4
    technical report》。arXiv 预印本 arXiv:2303.08774，2023。'
- en: '[39] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Gemini Team、Rohan Anil、Sebastian Borgeaud、Yonghui Wu、Jean-Baptiste Alayrac、Jiahui
    Yu、Radu Soricut、Johan Schalkwyk、Andrew M Dai、Anja Hauth 等人。《Gemini: a family of
    highly capable multimodal models》。arXiv 预印本 arXiv:2312.11805，2023。'
- en: '[40] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine
    Learning Research, 24(240):1–113, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav Mishra、Adam
    Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann 等人。《Palm:
    Scaling language modeling with pathways》。机器学习研究杂志，24(240):1–113，2023。'
- en: '[41] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,
    Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al.
    Phi-3 technical report: A highly capable language model locally on your phone.
    arXiv preprint arXiv:2404.14219, 2024.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Marah Abdin、Sam Ade Jacobs、Ammar Ahmad Awan、Jyoti Aneja、Ahmed Awadallah、Hany
    Awadalla、Nguyen Bach、Amit Bahree、Arash Bakhtiari、Harkirat Behl 等人。《Phi-3 technical
    report: A highly capable language model locally on your phone》。arXiv 预印本 arXiv:2404.14219，2024。'
- en: '[42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等人。《Llama: Open and efficient foundation language models》。arXiv 预印本 arXiv:2302.13971，2023。'
- en: '[43] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu,
    Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as
    agents. arXiv preprint arXiv:2308.03688, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Xiao Liu、Hao Yu、Hanchen Zhang、Yifan Xu、Xuanyu Lei、Hanyu Lai、Yu Gu、Hangliang
    Ding、Kaiwen Men、Kejuan Yang 等人。《Agentbench: Evaluating llms as agents》。arXiv 预印本
    arXiv:2308.03688，2023。'
- en: '[44] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,
    Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient
    fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence,
    5(3):220–235, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Ning Ding、Yujia Qin、Guang Yang、Fuchao Wei、Zonghan Yang、Yusheng Su、Shengding
    Hu、Yulin Chen、Chi-Min Chan、Weize Chen 等人。《Parameter-efficient fine-tuning of large-scale
    pre-trained language models》。自然机器智能，5(3):220–235，2023。'
- en: '[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in neural information processing systems, 35:27730–27744, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等人。《Training language
    models to follow instructions with human feedback》。神经信息处理系统进展，35:27730–27744，2022。'
- en: '[46] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities
    of large language models a mirage? Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Rylan Schaeffer、Brando Miranda 和 Sanmi Koyejo。《Are emergent abilities
    of large language models a mirage?》神经信息处理系统进展，36，2024。'
- en: '[47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed Chi、Quoc
    V Le、Denny Zhou 等人。《Chain-of-thought prompting elicits reasoning in large language
    models》。神经信息处理系统进展，35:24824–24837，2022。'
- en: '[48] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
    Halueval: A large-scale hallucination evaluation benchmark for large language
    models. In Proceedings of the 2023 Conference on Empirical Methods in Natural
    Language Processing, pages 6449–6464, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Junyi Li、Xiaoxue Cheng、Wayne Xin Zhao、Jian-Yun Nie 和 Ji-Rong Wen。Halueval：大型语言模型的规模化幻觉评估基准。发表于《2023年自然语言处理经验方法会议论文集》，第6449–6464页，2023年。'
- en: '[49] Pattie Maes. Modeling adaptive autonomous agents. Artificial life, 1(1_2):135–162,
    1993.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Pattie Maes。建模自适应自主代理。《人工生命》，1(1_2):135–162，1993年。'
- en: '[50] Stefano V Albrecht and Peter Stone. Autonomous agents modelling other
    agents: A comprehensive survey and open problems. Artificial Intelligence, 258:66–95,
    2018.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Stefano V Albrecht 和 Peter Stone。自主代理建模其他代理：综合调查与开放问题。《人工智能》，258:66–95，2018年。'
- en: '[51] Ronald C Arkin. Behavior-based robotics. MIT press, 1998.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Ronald C Arkin。基于行为的机器人技术。MIT出版社，1998年。'
- en: '[52] Yara Rizk, Mariette Awad, and Edward W Tunstel. Cooperative heterogeneous
    multi-robot systems: A survey. ACM Computing Surveys (CSUR), 52(2):1–31, 2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Yara Rizk、Mariette Awad 和 Edward W Tunstel。合作异质多机器人系统：一项调查。《ACM计算机调查》（CSUR），52(2):1–31，2019年。'
- en: '[53] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles
    and techniques. MIT press, 2009.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Daphne Koller 和 Nir Friedman。《概率图模型：原理与技术》。MIT出版社，2009年。'
- en: '[54] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Richard S Sutton 和 Andrew G Barto。《强化学习：导论》。MIT出版社，2018年。'
- en: '[55] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language
    model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Lei Wang、Chen Ma、Xueyang Feng、Zeyu Zhang、Hao Yang、Jingsen Zhang、Zhiyuan
    Chen、Jiakai Tang、Xu Chen、Yankai Lin 等。基于大型语言模型的自主代理调查。《计算机科学前沿》，18(6):186345，2024年。'
- en: '[56] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
    language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Zhiheng Xi、Wenxiang Chen、Xin Guo、Wei He、Yiwen Ding、Boyang Hong、Ming Zhang、Junzhe
    Wang、Senjie Jin、Enyu Zhou 等。基于大型语言模型代理的兴起与潜力：一项调查。arXiv预印本 arXiv:2309.07864，2023年。'
- en: '[57] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen
    llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,
    2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Qingyun Wu、Gagan Bansal、Jieyu Zhang、Yiran Wu、Shaokun Zhang、Erkang Zhu、Beibin
    Li、Li Jiang、Xiaoyun Zhang 和 Chi Wang。Autogen：通过多代理对话框架实现下一代LLM应用。arXiv预印本 arXiv:2308.08155，2023年。'
- en: '[58] Oguzhan Topsakal and Tahir Cetin Akinci. Creating large language model
    applications utilizing langchain: A primer on developing llm apps fast. In International
    Conference on Applied Engineering and Natural Sciences, volume 1, pages 1050–1056,
    2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Oguzhan Topsakal 和 Tahir Cetin Akinci。利用LangChain创建大型语言模型应用：快速开发LLM应用的入门指南。发表于《应用工程与自然科学国际会议》，第1卷，第1050–1056页，2023年。'
- en: '[59] Jerry Liu. LlamaIndex, 11 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Jerry Liu。LlamaIndex，2022年11月。'
- en: '[60] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun
    Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with
    large language models. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pages 2998–3009, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Chan Hee Song、Jiaman Wu、Clayton Washington、Brian M Sadler、Wei-Lun Chao
    和 Yu Su。Llm-planner：基于大语言模型的少样本嵌入规划。发表于《IEEE/CVF国际计算机视觉会议论文集》，第2998–3009页，2023年。'
- en: '[61] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang,
    Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. Swiftsage:
    A generative agent with fast and slow thinking for complex interactive tasks.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Bill Yuchen Lin、Yicheng Fu、Karina Yang、Faeze Brahman、Shiyu Huang、Chandra
    Bhagavatula、Prithviraj Ammanabrolu、Yejin Choi 和 Xiang Ren。Swiftsage：一种用于复杂互动任务的快速与慢速思维生成代理。《神经信息处理系统进展》，36，2024年。'
- en: '[62] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity
    search with gpus. IEEE Transactions on Big Data, 7(3):535–547, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Jeff Johnson、Matthijs Douze 和 Hervé Jégou。基于GPU的十亿规模相似性搜索。《IEEE大数据汇刊》，7(3):535–547，2019年。'
- en: '[63] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
    Retrieval augmented language model pre-training. In International conference on
    machine learning, pages 3929–3938\. PMLR, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Kelvin Guu、Kenton Lee、Zora Tung、Panupong Pasupat 和 Mingwei Chang。检索增强语言模型预训练。发表于《国际机器学习会议》，第3929–3938页。PMLR，2020年。'
- en: '[64] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo,
    Alexander Novikov, Gabriel Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay,
    Jost Tobias Springenberg, et al. A generalist agent. Transactions on Machine Learning
    Research.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo,
    Alexander Novikov, Gabriel Barth-maron, Mai Giménez, Yury Sulsky, Jackie Kay,
    Jost Tobias Springenberg 等人。一种通用型代理。机器学习研究交易。'
- en: '[65] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu 和 Yueting
    Zhuang。Hugginggpt：利用chatgpt及其在Hugging Face中的朋友解决ai任务。Neural Information Processing
    Systems进展，36, 2024。'
- en: '[66] Jianfeng Gao, Michel Galley, and Lihong Li. Neural approaches to conversational
    ai. In The 41st international ACM SIGIR conference on research & development in
    information retrieval, pages 1371–1374, 2018.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Jianfeng Gao, Michel Galley 和 Lihong Li。对话AI的神经方法。第41届国际ACM SIGIR信息检索研究与开发会议，页1371–1374,
    2018。'
- en: '[67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv
    preprint arXiv:2210.03629, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan
    和 Yuan Cao。React：语言模型中推理与行动的协同。arXiv 预印本 arXiv:2210.03629, 2022。'
- en: '[68] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir
    Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl
    systems: A modular, neuro-symbolic architecture that combines large language models,
    external knowledge sources and discrete reasoning. arXiv preprint arXiv:2205.00445,
    2022.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir
    Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown 等人。MRKL系统：一个模块化的神经符号架构，结合了大型语言模型、外部知识源和离散推理。arXiv
    预印本 arXiv:2205.00445, 2022。'
- en: '[69] Artur d’Avila Garcez, Tarek R Besold, Luc De Raedt, Peter Földiak, Pascal
    Hitzler, Thomas Icard, Kai-Uwe Kühnberger, Luis C Lamb, Risto Miikkulainen, and
    Daniel L Silver. Neural-symbolic learning and reasoning: contributions and challenges.
    In 2015 AAAI Spring Symposium Series, 2015.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Artur d’Avila Garcez, Tarek R Besold, Luc De Raedt, Peter Földiak, Pascal
    Hitzler, Thomas Icard, Kai-Uwe Kühnberger, Luis C Lamb, Risto Miikkulainen 和 Daniel
    L Silver。神经符号学习与推理：贡献与挑战。2015 AAAI春季研讨会系列，2015。'
- en: '[70] Ronald Brachman and Hector Levesque. Knowledge representation and reasoning.
    Morgan Kaufmann, 2004.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Ronald Brachman 和 Hector Levesque。知识表示与推理。Morgan Kaufmann, 2004。'
- en: '[71] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan
    Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with
    large language models. Advances in Neural Information Processing Systems, 36,
    2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan
    Cao 和 Karthik Narasimhan。思维树：利用大型语言模型进行深思熟虑的问题解决。Neural Information Processing
    Systems进展，36, 2024。'
- en: '[72] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov,
    Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg,
    Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search
    with large language models. Nature, 625(7995):468–475, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov,
    Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg,
    Pengming Wang, Omar Fawzi 等人。利用大型语言模型的程序搜索中的数学发现。Nature, 625(7995):468–475, 2024。'
- en: '[73] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What
    makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
    Hajishirzi 和 Luke Zettlemoyer. 重新思考演示的角色：是什么让上下文学习有效？arXiv 预印本 arXiv:2202.12837,
    2022。'
- en: '[74] Lauren Nicole DeLong, Ramon Fernández Mir, and Jacques D Fleuriot. Neurosymbolic
    ai for reasoning over knowledge graphs: A survey. arXiv preprint arXiv:2302.07200,
    2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Lauren Nicole DeLong, Ramon Fernández Mir 和 Jacques D Fleuriot。神经符号人工智能在知识图谱上的推理：综述。arXiv
    预印本 arXiv:2302.07200, 2023。'
- en: '[75] Ciyuan Peng, Feng Xia, Mehdi Naseriparsa, and Francesco Osborne. Knowledge
    graphs: Opportunities and challenges. Artificial Intelligence Review, 56(11):13071–13102,
    2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Ciyuan Peng, Feng Xia, Mehdi Naseriparsa 和 Francesco Osborne。知识图谱：机遇与挑战。人工智能评论，56(11):13071–13102,
    2023。'
- en: '[76] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation
    of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080,
    2021.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Sang Michael Xie, Aditi Raghunathan, Percy Liang 和 Tengyu Ma。将上下文学习解释为隐式贝叶斯推断。arXiv
    预印本 arXiv:2111.02080, 2021。'
- en: '[77] Arian Askari, Roxana Petcu, Chuan Meng, Mohammad Aliannejadi, Amin Abolghasemi,
    Evangelos Kanoulas, and Suzan Verberne. Self-seeding and multi-intent self-instructing
    llms for generating intent-aware information-seeking dialogs. arXiv preprint arXiv:2402.11633,
    2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Arian Askari、Roxana Petcu、Chuan Meng、Mohammad Aliannejadi、Amin Abolghasemi、Evangelos
    Kanoulas 和 Suzan Verberne. 自我种子和多意图自我指导 LLMs 用于生成意图感知的信息检索对话。arXiv 预印本 arXiv:2402.11633，2024年。'
- en: '[78] Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu,
    Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning.
    arXiv preprint arXiv:2402.09906, 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Niklas Muennighoff、Hongjin Su、Liang Wang、Nan Yang、Furu Wei、Tao Yu、Amanpreet
    Singh 和 Douwe Kiela. 生成性表征指令调整。arXiv 预印本 arXiv:2402.09906，2024年。'
- en: '[79] Petar Veličković and Charles Blundell. Neural algorithmic reasoning. Patterns,
    2(7), 2021.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Petar Veličković 和 Charles Blundell. 神经算法推理。Patterns，2(7)，2021年。'
- en: '[80] Pentti Kanerva. Hyperdimensional computing: An introduction to computing
    in distributed representation with high-dimensional random vectors. Cognitive
    computation, 1:139–159, 2009.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Pentti Kanerva. 高维计算: 高维随机向量分布表示计算简介。认知计算，1:139–159，2009年。'
- en: '[81] Michael Hersche, Mustafa Zeqiri, Luca Benini, Abu Sebastian, and Abbas
    Rahimi. A neuro-vector-symbolic architecture for solving raven’s progressive matrices.
    Nature Machine Intelligence, 5(4):363–375, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Michael Hersche、Mustafa Zeqiri、Luca Benini、Abu Sebastian 和 Abbas Rahimi.
    用于解决乌鸦渐进矩阵的神经-向量-符号架构。自然机器智能，5(4):363–375，2023年。'
- en: '[82] K Rustan M Leino. Dafny: An automatic program verifier for functional
    correctness. In International conference on logic for programming artificial intelligence
    and reasoning, pages 348–370\. Springer, 2010.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] K Rustan M Leino. Dafny: 一种用于功能正确性的自动程序验证器。发表于国际逻辑编程、人工智能和推理会议，页面348–370。Springer，2010年。'
- en: '[83] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and
    programming language. In Automated Deduction–CADE 28: 28th International Conference
    on Automated Deduction, Virtual Event, July 12–15, 2021, Proceedings 28, pages
    625–635\. Springer, 2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Leonardo de Moura 和 Sebastian Ullrich. Lean 4 定理证明器和编程语言。发表于自动推理–CADE
    28: 第28届自动推理国际会议，虚拟会议，2021年7月12–15日，论文集28，页面625–635。Springer，2021年。'
- en: '[84] Eric Mugnier, Emmanuel Anaya Gonzalez, Ranjit Jhala, Nadia Polikarpova,
    and Yuanyuan Zhou. Laurel: Generating dafny assertions using large language models.
    arXiv preprint arXiv:2405.16792, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Eric Mugnier、Emmanuel Anaya Gonzalez、Ranjit Jhala、Nadia Polikarpova 和
    Yuanyuan Zhou. Laurel: 使用大型语言模型生成 Dafny 断言。arXiv 预印本 arXiv:2405.16792，2024年。'
