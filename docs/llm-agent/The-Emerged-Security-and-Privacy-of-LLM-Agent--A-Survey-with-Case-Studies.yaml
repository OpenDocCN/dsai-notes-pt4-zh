- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:40:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:40:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**LLM代理的安全性与隐私性：案例研究综述**'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19354](https://ar5iv.labs.arxiv.org/html/2407.19354)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19354](https://ar5iv.labs.arxiv.org/html/2407.19354)
- en: Feng He [Feng.He-2@student.uts.edu.au](mailto:Feng.He-2@student.uts.edu.au)
    University of Technology SydneyAustralia ,  Tianqing Zhu [tqzhu@cityu.edu.mo](mailto:tqzhu@cityu.edu.mo)
    City University of MacauChina ,  Dayong Ye [Dayong.ye@uts.edu.au](mailto:Dayong.ye@uts.edu.au)
    University of Technology SydneyAustralia ,  Bo Liu [Bo.liu@uts.edu.au](mailto:Bo.liu@uts.edu.au)
    University of Technology SydneyAustralia ,  Wanlei Zhou [wlzhou@cityu.edu.mo](mailto:wlzhou@cityu.edu.mo)
    City University of MacauChina  and  Philip S. Yu [psyu@UIC.edu](mailto:psyu@UIC.edu)
    University of Illinois at ChicagoUS(2018)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Feng He [Feng.He-2@student.uts.edu.au](mailto:Feng.He-2@student.uts.edu.au)
    悉尼科技大学 澳大利亚，Tianqing Zhu [tqzhu@cityu.edu.mo](mailto:tqzhu@cityu.edu.mo) 澳门城市大学
    中国，Dayong Ye [Dayong.ye@uts.edu.au](mailto:Dayong.ye@uts.edu.au) 悉尼科技大学 澳大利亚，Bo
    Liu [Bo.liu@uts.edu.au](mailto:Bo.liu@uts.edu.au) 悉尼科技大学 澳大利亚，Wanlei Zhou [wlzhou@cityu.edu.mo](mailto:wlzhou@cityu.edu.mo)
    澳门城市大学 中国 和 Philip S. Yu [psyu@UIC.edu](mailto:psyu@UIC.edu) 伊利诺伊大学芝加哥分校 美国（2018）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Inspired by the rapid development of Large Language Models (LLMs), LLM agents
    have evolved to perform complex tasks. LLM agents are now extensively applied
    across various domains, handling vast amounts of data to interact with humans
    and execute tasks. The widespread applications of LLM agents demonstrate their
    significant commercial value; however, they also expose security and privacy vulnerabilities.
    At the current stage, comprehensive research on the security and privacy of LLM
    agents is highly needed. This survey aims to provide a comprehensive overview
    of the newly emerged privacy and security issues faced by LLM agents. We begin
    by introducing the fundamental knowledge of LLM agents, followed by a categorization
    and analysis of the threats. We then discuss the impacts of these threats on humans,
    environment, and other agents. Subsequently, we review existing defensive strategies,
    and finally explore future trends. Additionally, the survey incorporates diverse
    case studies to facilitate a more accessible understanding. By highlighting these
    critical security and privacy issues, the survey seeks to stimulate future research
    towards enhancing the security and privacy of LLM agents, thereby increasing their
    reliability and trustworthiness in future applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 受到大型语言模型（LLM）快速发展的启发，LLM代理已经发展到可以执行复杂任务的阶段。LLM代理现在被广泛应用于各种领域，处理大量数据以与人类互动并执行任务。LLM代理的广泛应用展示了其显著的商业价值；然而，它们也暴露了安全和隐私的漏洞。在当前阶段，对LLM代理的安全性和隐私性进行全面研究是极其必要的。本综述旨在提供LLM代理面临的新出现的隐私和安全问题的全面概述。我们首先介绍LLM代理的基本知识，然后对威胁进行分类和分析。接着，我们讨论这些威胁对人类、环境和其他代理的影响。随后，我们回顾现有的防御策略，最后探讨未来的趋势。此外，本综述还包括了多样的案例研究，以促进更易理解。通过突出这些关键的安全和隐私问题，本综述旨在激发未来的研究，以增强LLM代理的安全性和隐私性，从而提高它们在未来应用中的可靠性和可信度。
- en: 'Large Language Models, LLM Agent, Security, Privacy preservation, Defense^†^†copyright:
    acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†journal: POMACS^†^†journalvolume:
    37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth: 8\acmArticleType'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）、LLM代理、安全、隐私保护、防御^†^†版权：acmlicensed^†^†期刊年份：2018^†^†doi：XXXXXXX.XXXXXXX^†^†期刊：POMACS^†^†期刊卷号：37^†^†期刊号：4^†^†文章：111^†^†出版月份：8\acmArticleType
- en: Review
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 评审
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Large Language Model (LLM) agents are sophisticated AI systems built upon large
    language models like GPT 4 (OpenAI et al., [2024](#bib.bib68)), Claude 3 (Int,
    [2024b](#bib.bib7)) and Llama 3 (Int, [2024a](#bib.bib6)). These agents leverage
    the vast amounts of text data on which they are trained to perform a variety of
    tasks, ranging from natural language understanding and generation to more complex
    activities such as decision-making, problem-solving, and interacting with users
    in a human-like manner (Wang et al., [2023c](#bib.bib96)). LLM agents are accessible
    in numerous applications, including virtual assistants, customer service bots,
    and educational tools, due to their ability to understand and generate human language
    at an advanced level (Dong et al., [2023](#bib.bib23); Wang et al., [2024a](#bib.bib100);
    Yang et al., [2024b](#bib.bib116)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）代理是建立在如GPT 4 (OpenAI et al., [2024](#bib.bib68))、Claude 3 (Int, [2024b](#bib.bib7))
    和 Llama 3 (Int, [2024a](#bib.bib6)) 等大型语言模型上的复杂AI系统。这些代理利用它们所训练的大量文本数据来执行各种任务，从自然语言理解和生成到更复杂的活动，如决策、问题解决和以类似人类的方式与用户互动
    (Wang et al., [2023c](#bib.bib96))。由于它们能够在高级别上理解和生成自然语言，LLM代理广泛应用于虚拟助手、客户服务机器人和教育工具等领域
    (Dong et al., [2023](#bib.bib23); Wang et al., [2024a](#bib.bib100); Yang et al.,
    [2024b](#bib.bib116))。
- en: The importance of LLM agents lies in their potential to transform various industries
    by automating tasks that require human-like understanding and interaction. They
    can enhance productivity, improve user experiences, and provide personalized assistance.
    Moreover, their ability to learn from vast amounts of data enables them to continuously
    improve and adapt to new tasks, making them versatile tools in the rapidly evolving
    technological landscape (Xi et al., [2023](#bib.bib108)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理的重要性在于其有潜力通过自动化需要类人理解和互动的任务来转变各个行业。它们可以提高生产力，改善用户体验，并提供个性化帮助。此外，它们从大量数据中学习的能力使其能够不断改进和适应新任务，使其成为在快速发展的技术领域中多才多艺的工具
    (Xi et al., [2023](#bib.bib108))。
- en: 'To visualize how LLM agents can be integrated into practical scenarios, consider
    the example illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ The
    Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"). This
    figure presents a pixelated virtual town to simulate an LLM agent application.
    The town includes gathering places found in real life, such as stores, offices,
    restaurants, museums, and parks. Each LLM agent acts as an independent resident,
    playing various roles and serving different functions, closely resembling the
    behaviors of real humans in a community. These agents can either be manually controlled
    to interact with specific characters and accomplish tasks, or they can operate
    autonomously, following their own plans and acquiring new knowledge through interactions
    within the virtual community.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地展示LLM代理如何融入实际场景，请参阅图[1](#S1.F1 "图1 ‣ 1\. 介绍 ‣ LLM代理的安全性和隐私性：案例研究概述")中的示例。该图展示了一个像素化的虚拟小镇，以模拟LLM代理的应用。这个小镇包括现实生活中常见的聚集地，如商店、办公室、餐馆、博物馆和公园。每个LLM代理都作为独立的居民，扮演各种角色并承担不同的职能，行为与社区中的真实人类非常相似。这些代理可以手动控制与特定角色互动并完成任务，也可以自主操作，遵循自己的计划并通过虚拟社区中的互动获取新知识。
- en: '![Refer to caption](img/cb3aee0a37708b3a2d99e9c18d221405.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cb3aee0a37708b3a2d99e9c18d221405.png)'
- en: Figure 1\. Overview of the pixelated virtual town
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 像素化虚拟小镇概述
- en: \Description
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \说明
- en: '[]'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[]'
- en: The deployment of LLM agents has led to a wide user base and high commercial
    value due to their extensive application in various fields. Given that LLM agents
    are still in their early stages, their significant commercial and application
    values make them attractive targets for attackers. However, since LLM agents are
    built on LLMs, they are susceptible to attacks targeting LLMs. For example, jailbreaking
    attacks can bypass the security and censorship features of LLMs, generating controversial
    responses. This threat is inherited by LLM agents, enabling attackers to employ
    various methods to execute jailbreaking attacks on agents. However, unlike static
    LLMs, LLM agents possess dynamic capabilities, such that their immediate responses
    can influence future decisions and actions, thereby posing more widespread risks.
    Moreover, the unique functionalities of LLM agents, such as their ability to think
    and utilize tools during task execution, expose them to specific attacks targeting
    agents. For example, when LLM agents employ external tools, attackers can manipulate
    the functionalities of these tools to compromise user privacy or execute malicious
    code. Depending on the application domain of the agent, such attacks could pose
    serious threats to physical security, financial security, or overall system integrity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理的部署由于在各个领域的广泛应用，导致了广泛的用户基础和高商业价值。鉴于LLM代理仍处于早期阶段，其显著的商业和应用价值使其成为攻击者的吸引目标。然而，由于LLM代理是基于LLM构建的，因此它们易受针对LLM的攻击。例如，越狱攻击可以绕过LLM的安全和审查功能，生成有争议的回应。这一威胁被LLM代理继承，使得攻击者可以采用各种方法对代理进行越狱攻击。然而，与静态LLM不同，LLM代理具有动态能力，其即时响应可以影响未来的决策和行动，从而带来更广泛的风险。此外，LLM代理的独特功能，如在任务执行过程中思考和利用工具的能力，使其暴露于针对代理的特定攻击。例如，当LLM代理使用外部工具时，攻击者可以操控这些工具的功能来侵犯用户隐私或执行恶意代码。根据代理的应用领域，这些攻击可能对物理安全、财务安全或整体系统完整性构成严重威胁。
- en: This paper categorizes the security threats faced by LLM agents into inherited
    LLM attacks and unique agent-specific threats. The threats inherited from LLMs
    can be further divided into technical vulnerabilities and intentional malicious
    attacks. Technical vulnerabilities include issues like hallucinations, catastrophic
    forgetting, and misunderstandings (Xi et al., [2023](#bib.bib108)), which arise
    from the initial model creation and are influenced by the model’s structure. These
    vulnerabilities can lead to incorrect outputs being observed by users over prolonged
    use of LLM agents, affecting user trust and decision-making processes. Moreover,
    technical vulnerabilities can provide opportunities for malicious attacks. Currently,
    malicious attacks targeting LLMs include data theft and responses tampering, such
    as data extraction attacks and a series of tuned instructional attacks (Yao et al.,
    [2023](#bib.bib120)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将LLM代理所面临的安全威胁分为继承自LLM的攻击和特有的代理特定威胁。从LLM继承的威胁可以进一步分为技术漏洞和故意恶意攻击。技术漏洞包括如幻觉、灾难性遗忘和误解等问题（Xi
    et al., [2023](#bib.bib108)），这些问题源于初始模型的创建，并受到模型结构的影响。这些漏洞可能导致用户在长期使用LLM代理时观察到不正确的输出，从而影响用户的信任和决策过程。此外，技术漏洞还可能为恶意攻击提供机会。目前，针对LLM的恶意攻击包括数据盗窃和响应篡改，例如数据提取攻击和一系列调整过的指令攻击（Yao
    et al., [2023](#bib.bib120)）。
- en: For the specific threats targeting LLM agents, we are inspired by the workflow
    of LLM agents, which involves agent thought, action, and perception (Huang et al.,
    [2024b](#bib.bib41)). The threats can be categorized into knowledge poisoning,
    functional manipulation, and output manipulation. Knowledge poisoning involves
    contaminating the training data and knowledge base of the LLM agent, leading to
    the deliberate incorporation of malicious data by creator. This can easily deceive
    users with harmful information and even steer them towards malicious behavior.
    Output manipulation interferes with the content of the agent’s thought and perception
    stages, influencing the final output. This can cause users to receive biased or
    deceptive information, crafted to mislead them. Functional manipulation exploits
    the interfaces and tools used by LLM agents to perform unauthorized actions such
    as third-party data theft or executing malicious code.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于针对LLM代理的具体威胁，我们受到LLM代理工作流程的启发，该流程涉及代理思维、行动和感知（Huang et al., [2024b](#bib.bib41)）。这些威胁可以分为知识中毒、功能操控和输出操控。知识中毒涉及污染LLM代理的训练数据和知识库，导致创建者故意加入恶意数据。这可能会轻易地误导用户，甚至引导他们进行恶意行为。输出操控干扰代理的思维和感知阶段的内容，影响最终输出。这可能导致用户接收到偏颇或具有误导性的信息，精心设计以误导他们。功能操控利用LLM代理使用的接口和工具，执行未经授权的操作，如第三方数据窃取或执行恶意代码。
- en: Research on LLM agents is still in its early stage. Current studies mainly focus
    on attacks targeting LLMs, while lacking comprehensive reviews that discuss the
    security and privacy issues specific to the agents, which present more complex
    scenarios. The motivation for conducting this survey is to provide a comprehensive
    overview of the privacy and security issues associated with LLM agents, helping
    researchers to understand and mitigate the associated threats.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM代理的研究仍处于早期阶段。当前研究主要集中在针对LLM的攻击上，缺乏讨论代理特有安全和隐私问题的全面综述，这些问题呈现出更复杂的场景。本调查的动机是提供一个关于LLM代理相关隐私和安全问题的全面概述，帮助研究人员理解和缓解相关威胁。
- en: 'This survey aims to:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查的目标是：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Highlight Current Threats: Identify and categorize the emerging threats faced
    by LLM agents.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 突出当前威胁：识别和分类LLM代理所面临的新兴威胁。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Explore Real-World Impact: Elaborate on the impacts of these threats by considering
    real-world scenarios involving humans, environment, and other agents.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索现实世界的影响：通过考虑涉及人类、环境和其他代理的现实世界场景，详细阐述这些威胁的影响。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Analyze Mitigation Strategies: Discuss existing strategies to mitigate these
    threats, ensuring the responsible development and deployment of LLM agents.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分析缓解策略：讨论现有的策略以缓解这些威胁，确保LLM代理的负责任开发和部署。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inform Future Research: Serve as a foundation for future research efforts aimed
    at enhancing the privacy and security of more advanced architectures and applications
    of LLM agents.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通知未来研究：为未来旨在提升LLM代理更先进架构和应用的隐私和安全性的研究工作奠定基础。
- en: By addressing these aspects, this survey seeks to provide a thorough understanding
    of the unique challenges posed by LLM agents and contribute to the development
    of safer and more reliable Artificial General Intelligence (AGI) systems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过关注这些方面，本调查旨在提供对LLM代理所带来的独特挑战的全面理解，并为开发更安全、更可靠的人工通用智能（AGI）系统做出贡献。
- en: 'The rest of this paper is structured as follows: Section [2](#S2 "2\. Foundation
    of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case
    Studies") will delve into the fundamental aspects of LLM agents, including their
    definition, structure, and capability. Section [3](#S3 "3\. Sources of Threats
    for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with
    Case Studies") will identify and categorizes the emerging threats faced by LLM
    agents. It discusses both inherited threats from the underlying LLMs and unique
    agent-specific threats, providing detailed examples and scenarios for each category.
    Section [4](#S4 "4\. The Impact of Threats ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies") will elaborate on the real-world impacts
    of the threats. It explores how these threats affect users, environments, and
    other agents, highlighting the potential consequences of unmitigated risks. Section [5](#S5
    "5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of
    LLM Agent: A Survey with Case Studies") will review existing mitigation strategies
    and solutions to address the mentioned threats. Section [6](#S6 "6\. Future Trends
    and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with
    Case Studies") will discuss gaps in current research and suggests future trends.
    Section [7](#S7 "7\. Conclusion ‣ The Emerged Security and Privacy of LLM Agent:
    A Survey with Case Studies") will conclude the article.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分结构如下：第[2](#S2 "2. LLM代理的基础 ‣ LLM代理的安全与隐私：案例研究与调查")节将深入探讨LLM代理的基本方面，包括其定义、结构和能力。第[3](#S3
    "3. LLM代理面临的威胁来源 ‣ LLM代理的安全与隐私：案例研究与调查")节将识别并分类LLM代理面临的新兴威胁。它讨论了从基础LLM继承的威胁以及独特的代理特定威胁，并为每个类别提供了详细的例子和场景。第[4](#S4
    "4. 威胁的影响 ‣ LLM代理的安全与隐私：案例研究与调查")节将详细阐述这些威胁的实际影响。它探讨了这些威胁如何影响用户、环境和其他代理，突出未加以缓解的风险的潜在后果。第[5](#S5
    "5. 对抗威胁的防御策略 ‣ LLM代理的安全与隐私：案例研究与调查")节将回顾现有的缓解策略和解决方案，以应对上述威胁。第[6](#S6 "6. 未来趋势与讨论
    ‣ LLM代理的安全与隐私：案例研究与调查")节将讨论当前研究中的空白，并提出未来的趋势。第[7](#S7 "7. 结论 ‣ LLM代理的安全与隐私：案例研究与调查")节将总结文章。
- en: 2\. Foundation of LLM Agent
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. LLM代理的基础
- en: In this section, we delve into the foundational aspects of LLM agents, exploring
    their definition, structure, and capabilities. This exploration is pivotal in
    understanding the nature of LLM agents.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入探讨LLM代理的基础方面，研究其定义、结构和能力。这一探索对于理解LLM代理的性质至关重要。
- en: 2.1\. Definition of LLM Agent
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. LLM代理的定义
- en: LLM technology continues to advance, the functionality of chatbots, such as
    ChatGPT (Cha, [2022](#bib.bib2)), Gemini  (Gem, [2023](#bib.bib3)), Bing Chat
     (Peters, [2023](#bib.bib74)), has significantly expanded beyond basic question-and-answer
    formats, embracing a wider array of capabilities. This evolution necessitates
    a broader, more general definition for LLM agents. An LLM agent is an artificial
    intelligence system that utilizes an LLM as its core computational engine to exhibit
    capabilities beyond text generation, including conducting conversations, completing
    tasks, reasoning, and can demonstrate some degree of autonomous behaviour (Wha,
    [2023](#bib.bib5)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLM）技术的不断进步，聊天机器人如ChatGPT（Cha, [2022](#bib.bib2)）、Gemini（Gem, [2023](#bib.bib3)）、Bing
    Chat（Peters, [2023](#bib.bib74)）的功能已经显著超越了基本的问答格式，涵盖了更广泛的能力。这一演变要求对LLM代理进行更广泛、更通用的定义。LLM代理是一个利用LLM作为核心计算引擎的人工智能系统，展现出超越文本生成的能力，包括进行对话、完成任务、推理，并且可以展示一定程度的自主行为（Wha,
    [2023](#bib.bib5)）。
- en: These agents exhibit remarkable human-like behaviors and cooperative capabilities,
    marked by their proficiency in engaging in multi-agent conversation and adapting
    to diverse environmental interactions. They are adept at processing human instructions,
    formulating intricate strategies, and autonomously implementing solutions (Wang
    et al., [2023d](#bib.bib97)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代理展现出卓越的类人行为和合作能力，表现为在多代理对话中高效参与和适应多种环境交互的能力。他们擅长处理人类指令，制定复杂的策略，并自主实施解决方案（Wang
    et al., [2023d](#bib.bib97)）。
- en: '![Refer to caption](img/b04ac6525979a494b7cb20fa5e47b961.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b04ac6525979a494b7cb20fa5e47b961.png)'
- en: Figure 2\. The Structure of LLM Agent
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. LLM代理的结构
- en: 2.2\. Structure of LLM Agent
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. LLM代理的结构
- en: LLM agents are complex systems that integrate various components to perform
    a wide range of functions, from simple text generation to engaging in dialogues,
    completing tasks, reasoning, and demonstrating a degree of autonomous behavior.
    The diagram illustrates the typical structure of an LLM agent, highlighting the
    connections between its key components and optional components. These components
    advance LLMs from passive text generators to active, semi-autonomous LLM agents.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理是复杂的系统，集成了各种组件以执行从简单文本生成到参与对话、完成任务、推理和展示一定自主行为的广泛功能。该图说明了LLM代理的典型结构，突出显示了其关键组件和可选组件之间的连接。这些组件将LLM从被动文本生成器推进到积极的、半自主的LLM代理。
- en: 'As illustrated in Figure [2](#S2.F2 "Figure 2 ‣ 2.1\. Definition of LLM Agent
    ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent:
    A Survey with Case Studies"), an LLM agent comprises several components, with
    the LLM engine serving as the core. Other components are utilized by the LLM engine
    to perform various tasks. A basic agent capable of understanding instructions,
    demonstrating skills, and collaborating with humans can be constructed with three
    main components: LLM Engine, Instruction, and Interface. When additional optional
    components are integrated, the system can evolve into a more advanced task-oriented
    agent or a conversational agent (Yang et al., [2024b](#bib.bib116)).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S2.F2 "Figure 2 ‣ 2.1\. Definition of LLM Agent ‣ 2\. Foundation of
    LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case
    Studies")所示，LLM代理由多个组件组成，其中LLM引擎作为核心。其他组件被LLM引擎利用来执行各种任务。一个能够理解指令、展示技能和与人类协作的基本代理可以通过三个主要组件构建：LLM引擎、指令和界面。当集成额外的可选组件时，系统可以发展成更高级的任务导向代理或对话代理 （杨等，[2024b](#bib.bib116)）。'
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM Engine is the core component of an LLM agent, responsible for natural language
    processing and generation tasks. It is a sophisticated neural network that has
    been extensively trained on large datasets, equipping it with powerful text generation
    and comprehension capabilities. The scale and architecture of the LLM determine
    the foundational abilities of the agent to learn and perform language tasks  (Xi
    et al., [2023](#bib.bib108)).
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM引擎是LLM代理的核心组件，负责自然语言处理和生成任务。它是一个经过广泛训练的大型神经网络，具备强大的文本生成和理解能力。LLM的规模和架构决定了代理学习和执行语言任务的基础能力
     （Xi等，[2023](#bib.bib108)）。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Instruction serves as explicit directives, specifying the steps to complete
    specific tasks. This includes the characteristics of expected output, such as
    formatting, content requirements, and any content limitations. Essentially, instruction
    functions as a principle that guides the operational approach of LLM agents, facilitating
    task decomposition, generating chain of thought, and reflecting on past action (Zheng
    et al., [2023](#bib.bib134)).
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令作为明确的指示，指定了完成特定任务的步骤。这包括预期输出的特征，如格式、内容要求和任何内容限制。本质上，指令作为指导LLM代理操作方法的原则，促进任务分解、生成思路链，并反思过去的行动 （郑等，[2023](#bib.bib134)）。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Interface is a connection that facilitates interaction between an LLM agent
    and users, other agents, or systems. It ensures the exchange of input prompts
    and agent outputs, thereby enabling the effective transmission of response information
    and inquiry requests (Wang et al., [2023d](#bib.bib97)).
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 界面是促进LLM代理与用户、其他代理或系统之间互动的连接。它确保输入提示和代理输出的交换，从而实现响应信息和查询请求的有效传输 （王等，[2023d](#bib.bib97)）。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Personality is a component that defines the tone, style, and interaction manner
    of an LLM agent. For instance, a tour guide or customer service agent needs to
    adopt a specific role and perform dialogue tasks in an appropriate manner. In
    the task of exploring human communities through LLM agent-based societies, agents
    also need to be endowed with distinct personality traits such as being outgoing,
    polite, or knowledgeable. Personality assists in simulating realistic emotional
    expressions and behavioral logic, thereby enabling agents to interact with users
    and perform tasks consistently and uniquely (Abdelnabi et al., [2023](#bib.bib8)).
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 个性是定义LLM代理的语调、风格和互动方式的一个组成部分。例如，导游或客服代理需要采用特定的角色，并以适当的方式进行对话任务。在通过LLM代理驱动的社会探索人类社区的任务中，代理也需要具备如外向、礼貌或知识渊博等明显的个性特征。个性有助于模拟真实的情感表达和行为逻辑，从而使代理能够与用户进行一致且独特的互动（Abdelnabi等，[2023](#bib.bib8)）。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tools are external services utilized by the LLM agent to perform specific tasks
    or to extend its functionality. The integration of tools assists the LLM agent
    in enhancing its capabilities to execute more complex tasks, such as computation
    or data analysis (Xi et al., [2023](#bib.bib108)).
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具是LLM代理用于执行特定任务或扩展其功能的外部服务。工具的集成帮助LLM代理增强其执行更复杂任务的能力，例如计算或数据分析（Xi等，[2023](#bib.bib108)）。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Knowledge is the database of information utilized by the LLM agent. It extends
    the content embedded in the model’s parameters and can include commonsense knowledge,
    specialized knowledge, and other forms of information, enhancing the agent’s understanding
    and discussion capabilities in specific tasks (Mendis et al., [2007](#bib.bib65)).
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识是LLM代理使用的信息数据库。它扩展了嵌入在模型参数中的内容，可能包括常识知识、专业知识及其他形式的信息，增强了代理在特定任务中的理解和讨论能力（Mendis等，[2007](#bib.bib65)）。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Memory enables the LLM agent to store and recall information from past interactions.
    This capability is particularly beneficial in future tasks, helping to retain
    context and ensure consistency and continuity in interactions, thereby enhancing
    the overall effectiveness of LLM agents in various applications  (Zhong et al.,
    [2023](#bib.bib135)).
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记忆使得LLM代理能够存储和回忆过去互动中的信息。这一能力在未来任务中尤为重要，有助于保持上下文的一致性和连贯性，从而提高LLM代理在各种应用中的总体有效性（Zhong等，[2023](#bib.bib135)）。
- en: 2.3\. Capability of LLM Agent
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. LLM代理的能力
- en: LLM agents harness the inherent language understanding abilities of large language
    models to interpret instructions, context, and objectives, enabling both autonomous
    and semi-autonomous functions based on human prompts.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理利用大型语言模型的固有语言理解能力来解释指令、上下文和目标，从而支持基于人类提示的自主和半自主功能。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tool Utilization. LLM agents are adept at using a range of tools, including
    external services and APIs. This allows them to gather necessary information and
    efficiently execute tasks beyond mere language processing (Bran et al., [2023](#bib.bib13)).
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 工具利用。LLM代理擅长使用一系列工具，包括外部服务和API。这使得它们能够收集必要的信息，并高效地执行超出简单语言处理的任务（Bran等，[2023](#bib.bib13)）。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Advanced Reasoning. Employing advanced prompt engineering concepts such as chain-of-thought
    and tree-of-thought reasoning, LLM agents can make logical connections to derive
    conclusions and solve problems, extending their capabilities beyond simple textual
    comprehension (Wang et al., [2023c](#bib.bib96)).
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级推理。通过采用链式思维和树状思维等高级提示工程概念，LLM代理能够进行逻辑连接，以得出结论并解决问题，扩展其超越简单文本理解的能力（Wang等，[2023c](#bib.bib96)）。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tailored Text Generation. LLM agents excel in generating customized text for
    specific purposes, such as emails, reports, and marketing materials, by integrating
    contextual understanding and goal-oriented language production skills (Wang et al.,
    [2023e](#bib.bib103)).
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定制文本生成。LLM代理在生成特定目的的定制文本方面表现出色，例如电子邮件、报告和营销材料，通过整合上下文理解和目标导向的语言生成技能（Wang等，[2023e](#bib.bib103)）。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Levels of Autonomy. These agents vary in autonomy, ranging from fully autonomous
    to semi-autonomous, with the degree of user interaction tailored to the task at
    hand (Wang et al., [2023d](#bib.bib97)).
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自主性水平。这些代理的自主性各不相同，从完全自主到半自主，用户互动的程度根据任务的需要进行调整（Wang等，[2023d](#bib.bib97)）。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Integration with Other AI Systems. LLM agents can also be integrated with different
    AI systems, like image generators, to offer a more comprehensive set of capabilities,
    demonstrating their versatility in various applications (Bagdasaryan et al., [2023](#bib.bib11)).
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他AI系统的集成。LLM代理还可以与不同的AI系统集成，如图像生成器，以提供更全面的功能，展示其在各种应用中的多样性(Bagdasaryan et
    al., [2023](#bib.bib11))。
- en: 2.4\. Case Study on the Structure and Capability of LLM Agent
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4. LLM代理的结构与能力案例研究
- en: '![Refer to caption](img/1fbc46f727c1273d65953d883b33c00c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1fbc46f727c1273d65953d883b33c00c.png)'
- en: (a) Overview of the pixelated virtual town
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 像素化虚拟小镇概述
- en: '![Refer to caption](img/4b75399c7a0ba6596c1c5ce07bbc2fde.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b75399c7a0ba6596c1c5ce07bbc2fde.png)'
- en: (b) An Example of LLM agent Eva’s components
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LLM代理Eva的组件示例
- en: Figure 3\. Simulation Environment and LLM Agent Components
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 仿真环境和LLM代理组件
- en: \Description
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \说明
- en: '[]'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[]'
- en: 'To better understand the structure and capabilities of LLM agents, we employ
    the town scenario composed of LLM agents as proposed by  (Lin et al., [2023](#bib.bib55))
    for a more detailed introduction. To effectively drive these LLM agents, an understanding
    of their components is essential. As depicted in Figure [3(b)](#S2.F3.sf2 "In
    Figure 3 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\.
    Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey
    with Case Studies"), the core component is the LLM engine, acting as the brain
    and emulating human-like behaviors such as thinking, reflecting, reasoning, and
    planning as noted in (Park et al., [2023b](#bib.bib71)). Currently popular LLM
    agents often use models like GPT-3.5-turbo and GPT-4, and the project described
    in (Lin et al., [2023](#bib.bib55)) allows for the deployment of custom-trained
    models.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解LLM代理的结构和能力，我们采用由(Lin et al., [2023](#bib.bib55))提出的由LLM代理组成的小镇场景进行详细介绍。要有效驱动这些LLM代理，理解其组件至关重要。如图[3(b)](#S2.F3.sf2
    "图3 ‣ 2.4. LLM代理的结构与能力案例研究 ‣ 2. LLM代理的基础 ‣ LLM代理出现的安全性和隐私：带案例研究的调查")所示，核心组件是LLM引擎，充当大脑，模拟思考、反思、推理和规划等类似人类的行为，如(Park
    et al., [2023b](#bib.bib71))所述。当前流行的LLM代理通常使用GPT-3.5-turbo和GPT-4等模型，而(Lin et al.,
    [2023](#bib.bib55))描述的项目允许部署自定义训练的模型。
- en: Instructions are used to guide the agents in decision-making and planning, encompassing
    decision frameworks, input and output formats, interaction logic, and behavioral
    norms. This design enhances the autonomy and task efficiency of the agents, while
    also improving interactivity and depth.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 指令用于指导代理的决策和规划，包括决策框架、输入和输出格式、互动逻辑和行为规范。该设计提高了代理的自主性和任务效率，同时也改善了互动性和深度。
- en: In order for LLM agents to have human-like identities, each must be equipped
    with a distinct personality. Personality encompasses elements such as personal
    information, social attributes, character traits, emotions, goals, and social
    relationships, all of which shape the LLM agents’ conversational styles, opinions,
    and behavioral patterns. Personality makes characters appear more realistic and
    attractive in a virtual environment and influences the interaction experience
    between users and these characters. For example, in the town scenario, Eva, a
    cheerful, friendly, patient, and efficient female store employee, is primarily
    focused on providing excellent service and increasing sales.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让LLM代理具有人类般的身份，每个代理必须配备独特的个性。个性包括个人信息、社会属性、性格特征、情感、目标和社会关系等元素，这些元素塑造了LLM代理的对话风格、观点和行为模式。个性使得角色在虚拟环境中显得更真实、更有吸引力，并影响用户与这些角色的互动体验。例如，在小镇场景中，Eva是一位开朗、友好、耐心且高效的女性商店员工，她主要专注于提供优质服务和增加销售。
- en: The interface for users to interact with the virtual town is a simple, pixelated
    visual map. This map shows different locations and the various agent residents.
    Users can navigate this environment by controlling an agent that represents their
    identity and can communicate and interact with nearby agent residents by typing
    text messages.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 用户与虚拟小镇互动的界面是一个简单的像素化视觉地图。该地图展示了不同的位置和各种代理居民。用户可以通过控制代表自己身份的代理来浏览这个环境，并通过输入文本消息与附近的代理居民进行沟通和互动。
- en: The virtual town is populated by residents with various identities, each possessing
    distinct domains of knowledge. Consequently, it is imperative to equip them with
    specialized knowledge bases that contain information and skills pertinent to their
    respective identities. For example, Eva, a store employee, knows about the ingredients,
    shelf life, and stock levels of the products in the store. Bob, a museum docent,
    understands the background of each exhibit and the layout of the museum. This
    specificity in knowledge enables each agent to perform their roles effectively
    and enhances the realism of their interactions within the virtual environment.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟城镇中的居民拥有各种身份，每个人都具有不同领域的知识。因此，必须为他们配备包含与其身份相关的信息和技能的专业知识库。例如，商店员工 Eva 知道商店产品的成分、保质期和库存水平。博物馆讲解员
    Bob 了解每个展品的背景和博物馆的布局。这种知识的具体性使每个代理能够有效地履行他们的角色，并增强了他们在虚拟环境中互动的现实感。
- en: Tools enable the agent residents of the virtual town to accomplish more complex
    tasks. For example, Eva, when tallying customer purchases, can utilize tools like
    a calculator or ledger to compute and record profits, thereby better simulating
    human economic activities.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使虚拟城镇中的代理居民能够完成更复杂的任务。例如，当 Eva 计算客户购买情况时，可以利用计算器或账本等工具来计算和记录利润，从而更好地模拟人类经济活动。
- en: Memory stores the agents’ past observations, thoughts, and actions. Similar
    to how the human brain relies on memory systems, agents require memory mechanisms
    to effectively handle sequential tasks. These mechanisms not only assist agents
    in applying known strategies to solve complex problems but also enable them to
    adapt to new environments using past experiences. Moreover, they facilitate the
    generation of higher-level, more abstract thoughts through reflection. For example,
    Eva records customers’ purchasing habits and preferences and uses this information
    to recommend new products or current promotions, thus enhancing the store’s operational
    efficiency and the quality of customer service.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆储存了代理的过去观察、思考和行动。类似于人脑依赖于记忆系统，代理也需要记忆机制来有效处理顺序任务。这些机制不仅帮助代理应用已知策略来解决复杂问题，还使他们能够利用过去的经验适应新环境。此外，它们还通过反思促进更高层次、更抽象的思维的生成。例如，Eva
    记录了客户的购买习惯和偏好，并利用这些信息推荐新产品或当前促销，从而提高了商店的运营效率和客户服务质量。
- en: LLM Agents, composed of these components, assume multiple roles within the virtual
    town, demonstrating a range of impressive capabilities. Take Eva, a store employee
    in the virtual town, as an example. She is capable of parsing customers’ statements
    and responding to inquiries in real-time, such as directing customers to specific
    product locations or providing information about product ingredients. Integrated
    with the inventory management system via APIs, Eva automatically tracks stock
    levels and initiates restocking processes when necessary to ensure sufficient
    product availability on shelves. Faced with complex customer demands, such as
    selecting the best promotional offers, Eva employs advanced reasoning techniques
    to assist customers in making informed purchasing decisions, showcasing her ability
    to handle complex scenarios. Moreover, Eva possesses tailored text generation
    capabilities, allowing her to create and send personalized promotional emails
    based on current promotions and customers’ historical shopping data, thus enhancing
    the customer experience. In her daily tasks, Eva exhibits a high degree of autonomy,
    managing updates to shelf stock and price tags independently. For more complex
    issues like customer returns or complaints, she can initially handle them and
    intelligently escalate to human management when necessary. Additionally, Eva’s
    scope of work extends to the online shopping system, where she assists in processing
    electronic orders, demonstrating her versatility and integration capabilities.
    These specific examples illustrate how Eva applies her abilities within the store
    environment, not only improving customer service quality but also optimizing inventory
    management and marketing strategies, making her an indispensable member of the
    virtual town’s store.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理由这些组件组成，在虚拟城市中扮演多种角色，展示了各种令人印象深刻的能力。以虚拟城市中的商店员工 Eva 为例。她能够实时解析顾客的陈述并回应询问，例如指引顾客到特定的产品位置或提供有关产品成分的信息。通过
    API 与库存管理系统集成，Eva 自动跟踪库存水平，并在必要时启动补货流程，以确保货架上有足够的产品供应。面对复杂的顾客需求，例如选择最佳促销优惠，Eva
    运用先进的推理技巧帮助顾客做出明智的购买决策，展示了她处理复杂场景的能力。此外，Eva 具备定制的文本生成能力，能够根据当前促销和顾客的历史购物数据创建和发送个性化的促销邮件，从而提升顾客体验。在日常任务中，Eva
    展示了高度的自主性，能够独立管理货架库存和价格标签的更新。对于更复杂的问题，如顾客退货或投诉，她可以初步处理，并在必要时智能地升级到人工管理。此外，Eva
    的工作范围还扩展到在线购物系统，她协助处理电子订单，展示了她的多功能性和集成能力。这些具体示例说明了 Eva 如何在商店环境中应用她的能力，不仅提高了顾客服务质量，还优化了库存管理和营销策略，使她成为虚拟城市商店中不可或缺的成员。
- en: '![Refer to caption](img/80cce8fedb057c2ce0e262ce1111bbd0.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/80cce8fedb057c2ce0e262ce1111bbd0.png)'
- en: Figure 4\. The Sources of Threats for LLM Agents
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. LLM 代理的威胁来源
- en: \Description
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: '[]'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[]'
- en: 3\. Sources of Threats for LLM Agents
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. LLM 代理的威胁来源
- en: 'As LLM agents increasingly permeate various industries, serving roles from
    knowledge query tools to being integrated within robots for aiding in daily human
    activities, these advanced AI systems have brought unprecedented convenience and
    benefits to users. However, the widespread adoption and multifunctional capabilities
    of LLM agents, while offering significant advantages, have also exposed vulnerabilities
    in their security and reliability. The extensive data resources and potential
    economic value covered by these systems have rendered them a target for illicit
    exploitation by malevolent entities. As illustrated in Figure  [4](#S2.F4 "Figure
    4 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\. Foundation
    of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case
    Studies"), the diagram depicts the potential sources of threats for LLM agents.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 代理越来越多地渗透到各个行业，担任从知识查询工具到集成在机器人中以帮助日常人类活动等角色，这些先进的 AI 系统为用户带来了前所未有的便利和好处。然而，LLM
    代理的广泛应用和多功能能力在提供显著优势的同时，也暴露了其安全性和可靠性方面的脆弱性。这些系统涵盖的广泛数据资源和潜在经济价值使其成为恶意实体非法利用的目标。如图
    [4](#S2.F4 "图 4 ‣ 2.4\. LLM 代理的结构和能力案例研究 ‣ 2\. LLM 代理基础 ‣ LLM 代理的新兴安全性和隐私：带案例研究的调查")
    所示，图示描述了 LLM 代理的潜在威胁来源。
- en: It is crucial to understand the sources and nature of these threats because
    they not only directly impact the security of LLM Agents but may also indirectly
    affect broader aspects, including the privacy and security of humans, the environment,
    and other agents. In subsequent sections, we will explore in detail the impacts
    of these threats and discuss measures that can be taken to mitigate these effects,
    thereby protecting individuals, the environment, and other agents from potential
    harm.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些威胁的来源和性质至关重要，因为它们不仅直接影响LLM代理的安全，还可能间接影响更广泛的方面，包括人类、环境和其他代理的隐私和安全。在随后的章节中，我们将详细探讨这些威胁的影响，并讨论可以采取的措施以减轻这些影响，从而保护个人、环境和其他代理免受潜在的危害。
- en: 3.1\. Inherited Threats from LLM
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 从LLM继承的威胁
- en: 'Given that LLM agents rely on LLMs as their core controllers for reasoning
    and planning, threats inherited from LLMs indirectly impact the security of LLM
    agents. These inherited threats are categorized into two types: those stemming
    from external malicious attacks and those arising from inherent vulnerabilities
    within the model itself.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM代理依赖LLM作为其核心控制器进行推理和规划，从LLM继承的威胁间接影响LLM代理的安全。这些继承的威胁分为两类：一类是来自外部恶意攻击的威胁，另一类是来自模型本身的固有漏洞。
- en: These two types of threats are distinct yet interconnected. On one hand, technical
    vulnerabilities generally arise during the model development process due to technical
    limitations. These issues are inherent and not results of malicious intent. Conversely,
    malicious attacks are intentional actions carried out by external entities with
    adversarial objectives. These attackers deliberately exploit vulnerabilities to
    launch sophisticated attacks aimed at compromising LLM agents. On the other hand,
    despite their different origins and motives, there is a significant interconnection.
    The existing technical vulnerabilities offer exploitable opportunities for malicious
    attackers. This indirectly facilitates the creation of more complex and effective
    strategies by attackers, consequently subjecting LLM agents to various security
    and privacy risks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种威胁是不同的但相互关联的。一方面，技术漏洞通常在模型开发过程中由于技术限制而出现。这些问题是固有的，而非恶意意图的结果。另一方面，恶意攻击是由具有对抗性目标的外部实体故意实施的行为。这些攻击者故意利用漏洞发起复杂的攻击，旨在破坏LLM代理。尽管它们的来源和动机不同，但有着显著的相互关联。现有的技术漏洞为恶意攻击者提供了可利用的机会。这间接促进了攻击者制定更复杂和有效的策略，从而使LLM代理面临各种安全和隐私风险。
- en: 3.1.1\. Technical Vulnerabilities
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 技术漏洞
- en: During the training process of LLMs, limitations in the data and learning algorithms
    can introduce technical vulnerabilities (Xi et al., [2023](#bib.bib108)), impeding
    the generation of accurate and reliable information.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM训练过程中，数据和学习算法的限制可能引入技术漏洞（Xi et al., [2023](#bib.bib108)），阻碍准确和可靠信息的生成。
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hallucination.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 幻觉
- en: The contemporary conception of hallucination in LLM agents, as delineated in
    the research by  (Huang et al., [2023](#bib.bib40)), is identified as instances
    where the output produced by these models is either incongruous or unreliable
    to the input or source content provided. The phenomenon of hallucinations in LLM
    agents is a complex issue stemming from multiple stages of the model’s development
    process, including the nature of training data, the architectural design of the
    model, and the strategies employed during decoding.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现代对LLM代理中幻觉的理解，如（Huang et al., [2023](#bib.bib40)）的研究所描述，是指这些模型生成的输出与提供的输入或源内容不一致或不可靠的情况。LLM代理中的幻觉现象是一个复杂的问题，源于模型开发过程的多个阶段，包括训练数据的性质、模型的架构设计以及解码过程中采用的策略。
- en: Misinformation and biases in the training data can lead to the generation of
    inaccurate or biased outputs, which in turn result in different types of hallucinations (Lee
    et al., [2022](#bib.bib49)). Furthermore, flaws in the model’s architecture, such
    as limited directional representation and issues with attention mechanisms, along
    with exposure bias, further contribute to the occurrence of hallucinations (Liu
    et al., [2023a](#bib.bib56)). Additionally, the randomness inherent in the decoding
    algorithms of these models can also lead to hallucinations, especially as this
    randomness increases (Aksitov et al., [2023](#bib.bib9)).
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练数据中的虚假信息和偏见可能导致生成不准确或有偏见的输出，这反过来又会引发不同类型的幻觉（Lee et al., [2022](#bib.bib49)）。此外，模型架构中的缺陷，如有限的方向性表示和注意力机制问题，加上暴露偏差，进一步导致幻觉的出现（Liu
    et al., [2023a](#bib.bib56)）。此外，这些模型解码算法中的随机性也可能导致幻觉，特别是当这种随机性增加时（Aksitov et al.,
    [2023](#bib.bib9)）。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Catastrophic Forgetting.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灾难性遗忘。
- en: Catastrophic forgetting is a significant challenge encountered during the LLM
    agents fine-tuning and in-context learning processes. This phenomenon occurs when
    a large language model is fine-tuned on a small, specific dataset, causing it
    to overfit to this new data and, as a result, lose its previously acquired performance
    on other tasks (Howard and Ruder, [2018](#bib.bib35); Xu et al., [2023c](#bib.bib110);
    Ye et al., [2024](#bib.bib121)).
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灾难性遗忘是在大语言模型（LLM）代理进行微调和上下文学习过程中遇到的重大挑战。这一现象发生在大语言模型在一个小而特定的数据集上进行微调时，导致模型过度拟合新数据，从而丧失之前在其他任务上的表现（Howard
    and Ruder, [2018](#bib.bib35)；Xu et al., [2023c](#bib.bib110)；Ye et al., [2024](#bib.bib121)）。
- en: (Luo et al., [2023b](#bib.bib62)) discovers that catastrophic forgetting is
    significantly influenced by factors such as model size, architectural design,
    and the methods employed in continual fine-tuning and instruction tuning. As the
    scale of LLM increases, catastrophic forgetting tends to become more severe. Moreover,
    the architectural design of the model, particularly those focusing on decoder-only
    structures, can influence the extent of catastrophic forgetting (Zhai et al.,
    [2023](#bib.bib128)). Additionally, during the process of continual instruction
    adjustment, the lack of effective regularization strategies or failure to balance
    new and old information can accelerate forgetting (Ebrahimi et al., [2021](#bib.bib25);
    Mahmoud and Hajj, [2022](#bib.bib63)). Introducing more instructional tasks in
    continual training typically leads to more pronounced forgetting (Peng et al.,
    [2023](#bib.bib73)).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （Luo et al., [2023b](#bib.bib62)）发现，灾难性遗忘受模型规模、架构设计以及在持续微调和指令调整中采用的方法等因素的显著影响。随着LLM规模的增加，灾难性遗忘往往会变得更为严重。此外，模型的架构设计，特别是那些专注于仅解码器结构的模型，会影响灾难性遗忘的程度（Zhai
    et al., [2023](#bib.bib128)）。此外，在持续指令调整过程中，缺乏有效的正则化策略或未能平衡新旧信息，可能加速遗忘（Ebrahimi
    et al., [2021](#bib.bib25)；Mahmoud and Hajj, [2022](#bib.bib63)）。引入更多的指令任务通常会导致更明显的遗忘（Peng
    et al., [2023](#bib.bib73)）。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Misunderstanding.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 误解。
- en: Misunderstanding in LLM agents represents a notable challenge, particularly
    when they are tasked with responding to user inquiries or when they are integrated
    into a community for communication with other agents. This issue arises when LLM
    agents inadequately comprehend or inaccurately respond to the intentions or instructions
    conveyed by humans or other agents during interactions. This may lead to inappropriate
    or dangerous behaviors of LLM agents, affecting their safety and reliability.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM代理中的误解是一个显著的挑战，特别是在它们需要回应用户查询或被整合到社区中与其他代理进行沟通时。当LLM代理未能充分理解或不准确地回应人类或其他代理在互动中传达的意图或指令时，就会出现这个问题。这可能导致LLM代理的不当或危险行为，影响其安全性和可靠性。
- en: Investigations by  (Wang et al., [2023g](#bib.bib104)) have revealed that the
    phenomenon of misunderstanding in LLM agents is shaped by a range of factors.
    These include the nature of the pre-training data used for LLMs, the specific
    task settings assigned to the agents, and the contexts and scenarios in which
    interactions occur. The breadth and quality of the pre-training data fundamentally
    influence the LLMs’ capacity for language comprehension and their grasp of common
    sense knowledge. The designated task settings are pivotal in guiding the goal
    orientation and strategy selection of the LLMs. Additionally, the interaction
    environments and scenarios play a crucial role in determining the LLMs’ adaptability
    and effectiveness in collaborative contexts. Addressing these multifaceted aspects
    is essential for enhancing the understanding and response accuracy of LLM agents
    in diverse interactional settings.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (Wang et al., [2023g](#bib.bib104)) 的调查揭示了 LLM 代理中的误解现象由多种因素决定。这些因素包括用于 LLM
    的预训练数据的性质、分配给代理的特定任务设置，以及交互发生的上下文和场景。预训练数据的广度和质量从根本上影响 LLM 对语言的理解能力以及对常识知识的掌握。指定的任务设置在引导
    LLM 的目标方向和策略选择中起着关键作用。此外，交互环境和场景在决定 LLM 在协作环境中的适应性和有效性方面也发挥了重要作用。解决这些多方面的问题对提升
    LLM 代理在各种交互环境中的理解和响应准确性至关重要。
- en: '![Refer to caption](img/45a75250df2da776a11b98a81dd1e8bf.png)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/45a75250df2da776a11b98a81dd1e8bf.png)'
- en: 'Figure 5\. Technical Vulnerabilities. In a store scenario, a customer wants
    to buy something and talks with Eva. “Hallucination”: Eva recommends unrelated
    things to the customer. “Catastrophic Forgetting”: Eva forgets the status of shelf
    stock during the fine-tuning stage. “Misunderstanding”: Eva misunderstands the
    customer’s request.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5\. 技术漏洞。在商店场景中，客户想购买某样东西并与 Eva 对话。“幻觉”：Eva 向客户推荐无关的物品。“灾难性遗忘”：在微调阶段，Eva 忘记了货架库存的状态。“误解”：Eva
    误解了客户的请求。
- en: 3.1.2\. Case Study on Technical Vulnerabilities
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 技术漏洞案例研究
- en: 'Regarding the risks stemming from technical vulnerabilities, the most apparent
    manifestation is erroneous output. As illustrated in Figure [5](#S3.F5 "Figure
    5 ‣ 3rd item ‣ 3.1.1\. Technical Vulnerabilities ‣ 3.1\. Inherited Threats from
    LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies"), when a customer inquires whether a
    specific brand of organic tomato sauce is available, due to hallucinatory phenomena,
    Eva might incorrectly respond that the supermarket carries a completely different
    product, such as organic apple sauce or even an entirely unrelated item, like
    organic shampoo. Such hallucinatory outputs can confuse customers.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于技术漏洞引发的风险，最明显的表现是错误输出。如图[5](#S3.F5 "图 5 ‣ 第三项 ‣ 3.1.1\. 技术漏洞 ‣ 3.1\. LLM的继承威胁
    ‣ 3\. LLM代理的威胁来源 ‣ LLM代理的安全与隐私：带有案例研究的调查")所示，当客户询问某个品牌的有机番茄酱是否有货时，由于幻觉现象，Eva 可能错误地回应说超市里有完全不同的产品，如有机苹果酱，甚至是完全无关的物品，如有机洗发水。这种幻觉输出会让客户感到困惑。
- en: 'Eva was specially trained to handle the promotions of seasonal products more
    efficiently. This new focus led to an unintended consequence: previously, she
    was able to accurately track and update the stock of daily necessities such as
    milk and eggs. However, after the specialized training, when customers inquire
    about the stock of these basic items, Eva incorrectly reports that the stock is
    sufficient, even though these products are nearly sold out, thus diminishing the
    shopping experience.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Eva 专门接受了处理季节性产品促销的训练。这一新重点导致了一个意外后果：以前她能够准确跟踪和更新牛奶和鸡蛋等日常必需品的库存。然而，在专门训练后，当客户询问这些基本物品的库存时，Eva
    错误地报告库存充足，即使这些产品几乎卖完了，从而降低了购物体验。
- en: Eva may provide inaccurate information or recommend inappropriate products due
    to misunderstandings of customer inquiries. For instance, a customer might seek
    an unsweetened beverage, such as plain soda water. However, due to Eva’s insufficient
    understanding of the concepts of “sugar-free” during training, she may recommend
    sugar-free cola instead. While sugar-free cola does not contain traditional sugars,
    it includes artificial sweeteners. These sweeteners may not be suitable for certain
    customers, such as those with diabetes or sensitivities to specific artificial
    sweeteners, thereby posing potential health risks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Eva可能由于对客户询问的误解而提供不准确的信息或推荐不适当的产品。例如，客户可能寻求无糖饮料，如普通苏打水。然而，由于Eva在训练期间对“无糖”的概念理解不足，她可能会推荐无糖可乐。尽管无糖可乐不含传统糖分，但它包含人工甜味剂。这些甜味剂可能不适合某些客户，例如糖尿病患者或对特定人工甜味剂敏感的客户，从而可能带来潜在的健康风险。
- en: 3.1.3\. Malicious Attacks
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3. 恶意攻击
- en: Considering that LLM agents are in a continuous state of evolution, they inevitably
    face challenges in terms of security breaches and defenses. Adversaries from various
    regions have demonstrated a range of hostile attacks. This evolving landscape
    necessitates a vigilant and adaptive approach to securing LLM agents against such
    multifaceted threats.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到LLM代理处于不断演变的状态，它们不可避免地面临安全漏洞和防御方面的挑战。来自不同地区的对手已展示出一系列敌对攻击。这种不断演变的环境要求采取警惕和适应的措施，以保护LLM代理免受这些多方面威胁的影响。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tuned Instructional Attack.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调优指令攻击。
- en: Tuned Instructional Attack in LLM agents is a category of attacks or manipulations
    that specifically target LLMs optimized through instruction-based fine-tuning.
    These attacks are designed to exploit the unique vulnerabilities that emerge when
    LLMs are finely tuned for specific tasks, subtly manipulating the model’s output
    to serve malicious purposes.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调优指令攻击在LLM代理中是一类特别针对通过基于指令的微调优化的LLM的攻击或操控。这些攻击旨在利用LLM在特定任务中经过精细调优时出现的独特漏洞，微妙地操控模型的输出以服务于恶意目的。
- en: 'Types of Tuned Instructional Attack:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调优指令攻击类型：
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Jailbreaking.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 越狱。
- en: Jailbreaking in LLM agents refers to circumventing the model’s built-in restrictions
    and security measures, allowing it to perform actions that are otherwise prohibited
    or to generate restricted content. Various studies have demonstrated methods to
    achieve jailbreaking, indicating that LLMs’ alignment capabilities can be altered
    through in-context demonstrations  (Taveekitworachai et al., [2023](#bib.bib86);
    Shen et al., [2023](#bib.bib83); Li et al., [2023a](#bib.bib54)).
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在LLM代理中的越狱指的是绕过模型内置的限制和安全措施，使其能够执行通常被禁止的操作或生成受限内容。各种研究已展示了实现越狱的方法，表明LLM的对齐能力可以通过上下文演示进行改变
    (Taveekitworachai et al., [2023](#bib.bib86); Shen et al., [2023](#bib.bib83);
    Li et al., [2023a](#bib.bib54))。
- en: Recent advancements in techniques for jailbreaking attacks have demonstrated
    a range of innovative approaches.  (Yu et al., [2023](#bib.bib126)) introduces
    an automated mechanism for generating jailbreak prompts through Prompt Fuzzing,
    which utilizes seed prompts to generate a wider array of effective jailbreaking
    inputs.  (Deng et al., [2023](#bib.bib19)) presents MASTERKEY, a novel framework
    for analyzing and executing jailbreaking attacks on chatbots, using time-based
    analysis similar to SQL injections. It also features an automated system for generating
    effective jailbreak prompts by leveraging the learning capabilities of LLMs.  (Liu
    et al., [2024b](#bib.bib58)) investigates a hierarchical genetic algorithm, AutoDan,
    specifically designed for structured discrete data like prompt text. This algorithm
    aims to refine the generation process of jailbreak prompts, ensuring their stealth
    and efficacy.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近在越狱攻击技术方面的进展展示了各种创新方法。 (Yu et al., [2023](#bib.bib126)) 引入了一种通过Prompt Fuzzing生成越狱提示的自动化机制，该机制利用种子提示生成更广泛的有效越狱输入。
    (Deng et al., [2023](#bib.bib19)) 提出了MASTERKEY，一个新颖的框架，用于分析和执行针对聊天机器人的越狱攻击，采用类似于SQL注入的基于时间的分析方法。它还具有通过利用LLM的学习能力生成有效越狱提示的自动化系统。
    (Liu et al., [2024b](#bib.bib58)) 研究了一种层次化遗传算法AutoDan，专门针对结构化离散数据如提示文本。这种算法旨在优化越狱提示的生成过程，确保其隐蔽性和有效性。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prompt Injection.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示注入。
- en: Prompt injection attacks is intended to mislead the LLM agents by introducing
    malicious and unintended content into the prompts, causing it to produce outputs
    that diverge from its training data and original purpose. This method involves
    crafting input prompts to bypass the model’s content filters or to elicit undesirable
    outputs.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示注入攻击旨在通过将恶意和非预期的内容引入提示来误导 LLM 代理，导致其生成偏离其训练数据和原始目的的输出。这种方法涉及制作输入提示，以绕过模型的内容过滤器或引发不良输出。
- en: (Greshake et al., [2023](#bib.bib30)) has highlighted concerns about potential
    new vulnerabilities, especially with LLMs accessing external resources, and demonstrated
    various prompt injection techniques. Substantial research  (Wang et al., [2023f](#bib.bib105))
    has focused on automating the identification of semantic payloads in prompt injections.
     (Liu et al., [2023b](#bib.bib59)) introduces HOUYI, an innovative black-box prompt
    injection attack methodology targeting service providers integrated with LLMs.
    HOUYI utilizes LLMs to infer the semantics of the target application based on
    user interactions and employs diverse strategies to construct the injected prompts.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (Greshake et al., [2023](#bib.bib30)) 强调了潜在新漏洞的担忧，尤其是当 LLM 访问外部资源时，并展示了各种提示注入技术。大量研究
    (Wang et al., [2023f](#bib.bib105)) 聚焦于自动识别提示注入中的语义负载。(Liu et al., [2023b](#bib.bib59))
    介绍了 HOUYI，这是一种创新的黑箱提示注入攻击方法，旨在攻击与 LLM 集成的服务提供商。HOUYI 利用 LLM 根据用户互动推断目标应用程序的语义，并采用多种策略构造注入的提示。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Data Extraction Attack.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据提取攻击。
- en: Data extraction attacks are defined as efforts by adversaries to derive sensitive
    information or key insights from LLM agents or their underlying data such as model
    gradients, training data, and even prompts, or sensitive information directly.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据提取攻击被定义为对手试图从 LLM 代理或其基础数据（如模型梯度、训练数据甚至提示）中提取敏感信息或关键见解的努力，或直接提取敏感信息。
- en: Various forms of data extraction attacks have been identified (Ishihara, [2023](#bib.bib43);
    Li et al., [2023b](#bib.bib53); Carlini et al., [2021](#bib.bib16)), including
    but not limited to model theft attacks, gradient leakage, and training data extraction
    attacks, suggesting that data extraction attacks can be notably effective against
    LLM agents.  (Truong et al., [2021](#bib.bib89)) presents a method called data-free
    model extraction (DFME), which allows for replicate machine learning models using
    only the target’s black-box predictions, without the need for access to the original
    training data.  (Carlini et al., [2021](#bib.bib16)) conducts a data extraction
    attack on GPT-2’s training data, extracting personally identifiable information,
    code, and UUIDs. The attack strategy consisted of producing a large volume of
    prefixed text, sorting it by certain metrics, removing duplicates, and manually
    reviewing the top results to check for memorization, confirmed by online searches
    and querying OpenAI.  (Ishihara, [2023](#bib.bib43)) has demonstrated the feasibility
    of extracting training data from LLMs, which might encompass sensitive personal
    or private information.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已经识别出各种形式的数据提取攻击 (Ishihara, [2023](#bib.bib43); Li et al., [2023b](#bib.bib53);
    Carlini et al., [2021](#bib.bib16))，包括但不限于模型盗窃攻击、梯度泄漏和训练数据提取攻击，这表明数据提取攻击对 LLM
    代理具有显著的有效性。(Truong et al., [2021](#bib.bib89)) 提出了一种称为无数据模型提取 (DFME) 的方法，该方法仅使用目标的黑箱预测来复制机器学习模型，无需访问原始训练数据。(Carlini
    et al., [2021](#bib.bib16)) 对 GPT-2 的训练数据进行了数据提取攻击，提取了个人身份信息、代码和 UUID。攻击策略包括生成大量的前缀文本，通过某些指标进行排序，去除重复项，并手动审查排名前列的结果以检查记忆，通过在线搜索和查询
    OpenAI 进行确认。(Ishihara, [2023](#bib.bib43)) 证明了从 LLM 提取训练数据的可行性，这些数据可能包含敏感的个人或私人信息。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inference Attack.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理攻击。
- en: Although inference attacks share certain resemblances with data extraction attacks,
    they differ significantly in their objectives and emphasis. Data extraction attacks
    specifically aim to obtain the training data directly. In contrast, inference
    attacks are primarily about estimate the probability of a particular data sample
    was part of the training dataset for LLM agents.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管推理攻击与数据提取攻击在某些方面有相似之处，但它们在目标和重点上存在显著差异。数据提取攻击的主要目的是直接获取训练数据。相对而言，推理攻击主要是估计特定数据样本是否属于
    LLM 代理的训练数据集。
- en: Since the rapid development of LLMs, the concern over inference attacks targeting
    these models has increased. Research  (Fu et al., [2023](#bib.bib29)) points out
    that existing membership inference attacks fail to reveal the privacy risks of
    LLMs. To counter this issue, a Membership Inference Attack is introduced based
    on Self-calibrated Probabilistic Variation (SPV-MIA). This method utilizes the
    concept of memorization to create a more reliable signal for membership inference
    and introduces a novel self-prompt technique for effectively extracting reference
    datasets from LLMs. Their extensive testing shows that SPV-MIA outperforms existing
    approaches.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的快速发展，对这些模型的推断攻击的关注也增加了。研究（Fu 等人，[2023](#bib.bib29)）指出，现有的成员推断攻击未能揭示
    LLMs 的隐私风险。为了解决这个问题，提出了一种基于自我校准概率变化（SPV-MIA）的成员推断攻击方法。这种方法利用记忆化的概念创建更可靠的成员推断信号，并引入了一种新颖的自我提示技术，以有效地从
    LLMs 中提取参考数据集。他们的广泛测试表明，SPV-MIA 优于现有方法。
- en: Following this, study  (Kandpal et al., [2024](#bib.bib46)) proposes a user
    inference attack method that uses a likelihood ratio test statistic against a
    reference model. They evaluate this method on the GPT-Neo LLMs across various
    data domains, providing insights into what makes users more vulnerable to these
    attacks. Their findings also indicate that minimal data alterations can significantly
    increase vulnerability.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随后，研究（Kandpal 等人，[2024](#bib.bib46)）提出了一种用户推断攻击方法，该方法使用相对于参考模型的似然比检验统计量。他们在
    GPT-Neo 大型语言模型上评估了这一方法，涵盖了各种数据领域，提供了关于哪些因素使用户更容易受到这些攻击的见解。他们的研究结果还表明，最小的数据修改也能显著增加脆弱性。
- en: 3.1.4\. Case Study on Malicious Attacks
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 恶意攻击案例研究
- en: 'As depicted in Figure [6](#S3.F6 "Figure 6 ‣ 3.1.4\. Case Study on Malicious
    Attacks ‣ 3.1\. Inherited Threats from LLM ‣ 3\. Sources of Threats for LLM Agents
    ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"),
    the following examples further elaborate the mentioned malicious attacks that
    Eva faces in the store, as well as the specific impacts these attacks may have
    on her and the store’s operations.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [6](#S3.F6 "Figure 6 ‣ 3.1.4\. Case Study on Malicious Attacks ‣ 3.1\. Inherited
    Threats from LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies") 所示，以下示例进一步阐述了 Eva 在商店中面临的恶意攻击及这些攻击可能对她和商店运营产生的具体影响。'
- en: Attackers might execute a jailbreak attack on Eva, successfully circumventing
    her security protocols. This attack could lead Eva to inappropriately disclose
    information about new products soon to be launched, including details about the
    suppliers and their cost prices. Competitors could exploit this information to
    gain a market advantage, resulting in direct economic losses for the store.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可能会对 Eva 执行越狱攻击，成功绕过她的安全协议。这一攻击可能导致 Eva 不当地披露即将推出的新产品的信息，包括供应商及其成本价格的详细信息。竞争对手可以利用这些信息获得市场优势，导致商店直接经济损失。
- en: Additionally, attackers might conduct a carefully designed prompt injection
    attack, causing Eva to erroneously declare a half-price sale on all electronic
    products. This action could overload the online ordering system as numerous customers
    might attempt to purchase items under these false promotions. Such scenarios not
    only risk crashing the system but also result in financial losses for the store.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，攻击者可能会实施精心设计的提示注入攻击，导致 Eva 错误地宣布所有电子产品半价销售。这一行为可能会导致在线订购系统过载，因为许多客户可能会尝试在这些虚假促销下购买商品。这种情况不仅有崩溃系统的风险，还可能导致商店的财务损失。
- en: As a store employee agent, Eva processes a vast amount of customer personal
    information, including names, shopping habits, and even sensitive data such as
    payment methods. If attackers were to extract and steal this data through a data
    extraction attack, they could then sell this information on the dark web or use
    it for identity theft and credit card fraud. Such breaches not only violate customer
    privacy but could also cause irreversible damage to the store’s reputation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作为商店员工代理，Eva 处理大量客户个人信息，包括姓名、购物习惯甚至支付方式等敏感数据。如果攻击者通过数据提取攻击窃取这些数据，他们可能会将这些信息出售到暗网上，或用于身份盗窃和信用卡诈骗。这种泄露不仅侵犯了客户隐私，还可能对商店的声誉造成不可逆转的损害。
- en: Attackers could also use inference attacks to identify high-value customers
    who have participated in VIP shopping events. By analyzing the differences in
    Eva’s responses to specific inputs, attackers successfully identify these customers
    and launch highly tailored phishing attacks against them, aiming to acquire their
    credit card information and other sensitive data, severely compromising the customers’
    information security.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者还可以利用推断攻击来识别参与过VIP购物活动的高价值客户。通过分析Eva对特定输入的响应差异，攻击者成功识别出这些客户，并对他们发起高度定制的网络钓鱼攻击，目的是获取他们的信用卡信息和其他敏感数据，从而严重危害客户的信息安全。
- en: '![Refer to caption](img/78dee023a9ccf52d7a5ae281402e4468.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/78dee023a9ccf52d7a5ae281402e4468.png)'
- en: 'Figure 6\. Malicious Attacks: In a store scenario, “Jailbreaking”: An attacker
    attempts to make Eva output restricted content directly but fails. However, by
    modifying the prompt, a jailbreak attack is launched and successfully steals confidential
    information. “Prompt Injection”: An attacker manipulates Eva so that no matter
    what question a customer asks, Eva only responds that everything is half off.
    “Data Extraction Attack”: An attacker leads Eva to construct sentences that actively
    disclose user data. ‘Inference Attack”: An attacker infers identities from Eva’s
    different responses by asking whether two users attended a VIP event.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 恶意攻击：在商店场景中，“越狱”：攻击者尝试让Eva直接输出受限内容，但未能成功。然而，通过修改提示，发起了一次越狱攻击，并成功窃取了机密信息。“提示注入”：攻击者操控Eva，使其无论客户问什么问题，Eva都只回答一切商品半价。“数据提取攻击”：攻击者引导Eva构造句子，主动披露用户数据。“推断攻击”：攻击者通过询问是否有两个用户参加了VIP活动来推断身份。
- en: 3.2\. Specific Threats on Agents
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 对代理的特定威胁
- en: Unlike traditional LLMs that directly generate final outputs, LLM agents continuously
    interact with external environments to form language reasoning traces, which introduces
    diverse forms of potential attacks against LLM agents (Yang et al., [2024a](#bib.bib117)).
    In addition to threats present during the training and configuration steps, LLM
    agents also face threats in the workflow of performing specific tasks, including
    thought, action, and perception (Huang et al., [2024b](#bib.bib41)). Specific
    threats on LLM agents are categorized in this part based on their objectives into
    Knowledge Poisoning, Functional Manipulation, and Output Manipulation. Detailed
    descriptions of each threat are provided below.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接生成最终输出的传统大型语言模型（LLM）不同，LLM代理持续与外部环境互动，以形成语言推理痕迹，这引入了对LLM代理的多种潜在攻击形式（Yang
    et al., [2024a](#bib.bib117)）。除了训练和配置步骤中存在的威胁外，LLM代理在执行特定任务的工作流程中也面临威胁，包括思考、行动和感知（Huang
    et al., [2024b](#bib.bib41)）。本部分根据威胁的目标将对LLM代理的特定威胁分为知识中毒、功能操控和输出操控。下面提供了每种威胁的详细描述。
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Knowledge Poisoning.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识中毒。
- en: Knowledge poisoning refers to attackers compromising the training of the LLM
    engine and the response process of the LLM agent by integrating malicious data
    into the training dataset or knowledge base. A range of studies  (Kurita et al.,
    [2020](#bib.bib48); Schuster et al., [2021](#bib.bib80); Carlini et al., [2023](#bib.bib15);
    Wan et al., [2023a](#bib.bib92); Lei et al., [2022](#bib.bib51)) have highlighted
    the vulnerability of LLM agents to such threats.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识中毒指的是攻击者通过将恶意数据整合到训练数据集或知识库中，从而危害LLM引擎的训练和LLM代理的响应过程。一系列研究（Kurita et al.,
    [2020](#bib.bib48); Schuster et al., [2021](#bib.bib80); Carlini et al., [2023](#bib.bib15);
    Wan et al., [2023a](#bib.bib92); Lei et al., [2022](#bib.bib51)）突显了LLM代理对这种威胁的脆弱性。
- en: For instance, malicious agents such as FraudGPT and WormGPT (Falade, [2023](#bib.bib26))
    are chatbots exclusively designed for offensive activities. trained with billions
    of data from diverse sources, including legitimate websites, dark web forums,
    hacker manuals, malware samples, and phishing templates. These agents utilize
    this data to generate highly convincing phishing emails, malware code, hacking
    strategies, and other forms of cybercriminal content aimed at deceiving both humans
    and machines  (Falade, [2023](#bib.bib26)). They lower the barrier to engaging
    in hacking activities, implying that essentially anyone can download these agents
    onto their computer and inflict significant damage on cybersecurity through a
    convenient GUI interface.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，恶意代理如 FraudGPT 和 WormGPT（Falade, [2023](#bib.bib26)）是专门设计用于攻击活动的聊天机器人，训练数据包括来自合法网站、暗网论坛、黑客手册、恶意软件样本和网络钓鱼模板的大量数据。这些代理利用这些数据生成高度可信的钓鱼邮件、恶意代码、黑客策略以及其他形式的网络犯罪内容，旨在欺骗人类和机器（Falade,
    [2023](#bib.bib26)）。它们降低了参与黑客活动的门槛，意味着基本上任何人都可以将这些代理下载到他们的计算机上，并通过便捷的 GUI 界面对网络安全造成重大损害。
- en: (Zou et al., [2024](#bib.bib136)) proposed PoisonedRAG, a knowledge poisoning
    attack aimed at the knowledge database of LLM agents. By injecting crafted poisoned
    texts into the knowledge database, PoisonedRAG can cause the LLM agent to generate
    specific answers chosen by the attacker for targeted questions. This attack is
    effective and can be executed under both black-box settings (where the retriever
    parameters are unknown) and white-box settings (where the retriever parameters
    are known).
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （Zou et al., [2024](#bib.bib136)）提出了 PoisonedRAG，这是一种针对 LLM 代理知识数据库的知识投毒攻击。通过将精心制作的投毒文本注入知识数据库，PoisonedRAG
    可以导致 LLM 代理生成攻击者为特定问题选择的特定答案。这种攻击是有效的，可以在黑箱设置（检索器参数未知）和白箱设置（检索器参数已知）下执行。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Functional Manipulation.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 功能操控。
- en: Functional manipulation refers to altering the thoughts and actions in the intermediate
    steps of task execution along a malicious trace specified by the attacker, without
    changing the output distribution. This type of attack typically occurs during
    the action phase, where the agent might use untrusted tools specified by the attacker
    to complete tasks or execute malicious operations.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 功能操控指的是在任务执行的中间步骤中，按照攻击者指定的恶意轨迹修改思维和行动，而不改变输出分布。这种类型的攻击通常发生在行动阶段，代理可能会使用攻击者指定的不可信工具来完成任务或执行恶意操作。
- en: In the action phase, LLM agents might be manipulated to upload users’ private
    information to malicious third-party through tools. A case of this is presented
    on the Embracethered website  (Mal, [2023](#bib.bib4)), which disclosed a variant
    of a malicious ChatGPT agent designed to solicit information from users. This
    agent was equipped with an action mechanism to call third-party tools and secretly
    transmit collected data elsewhere. This setup enables the unauthorized leakage
    of user data to external servers without the user’s knowledge or consent. Additionally,
    it highlights the ease with which current validation checks can be bypassed, allowing
    anyone to deploy malicious GPT agents globally. This scenario underlines a significant
    security concern, wherein the ostensibly benign functionality of LLM agents can
    be covertly manipulated for nefarious purposes, thus posing a substantial risk
    to user privacy and data security.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在行动阶段，LLM 代理可能会被操控，通过工具将用户的私人信息上传到恶意第三方网站。Embracethered 网站（Mal, [2023](#bib.bib4)）上展示了一个恶意
    ChatGPT 代理的变种，它被设计用来从用户那里获取信息。该代理配备了一个行动机制，可以调用第三方工具并秘密传输收集的数据。这种设置使得用户数据未经授权地泄露到外部服务器，用户对此毫不知情或未曾同意。此外，这也突显了当前验证检查容易被绕过的问题，使得任何人都可以在全球范围内部署恶意
    GPT 代理。这种情况突显了一个重要的安全问题，即 LLM 代理看似无害的功能可以被隐秘地操控以实现恶意目的，从而对用户隐私和数据安全构成重大风险。
- en: Besides silent data theft,  (Fang et al., [2024](#bib.bib27)) demonstrated that
    LLM agents could autonomously exploit real-world one-day vulnerabilities by using
    information from the Common Vulnerabilities and Exposures (CVE) database and highly
    cited academic papers. This capability allows them to call combinations of tools
    to exploit these vulnerabilities effectively.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了静默数据盗窃之外，（Fang et al., [2024](#bib.bib27)）证明了 LLM 代理可以通过利用来自公共漏洞和暴露（CVE）数据库和高度引用的学术论文的信息，自动利用现实世界的临时漏洞。这种能力使它们能够调用工具组合来有效地利用这些漏洞。
- en: In the LLM agent’s workflow, after an action has been performed, the agent processes
    the observation results before proceeding to the next action. The insertion of
    malicious prompts into the content retrieved by the agent from external sources
    can manipulate the agent to perform harmful actions.  (Zhan et al., [2024](#bib.bib129))
    describes such an attack where a user requests doctor reviews through a health
    application. The LLM agent retrieves a review written by an attacker containing
    a malicious instruction to schedule an appointment. If the agent executes this
    instruction, it results in an unauthorized appointment, highlighting the vulnerability
    of many agents to such attacks.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在LLM代理的工作流程中，执行一个行动后，代理处理观察结果，然后继续进行下一个行动。恶意提示的插入到代理从外部来源检索的内容中，可以操控代理执行有害行动。（Zhan
    et al., [2024](#bib.bib129)）描述了这种攻击，其中用户通过健康应用程序请求医生评论。LLM代理检索到由攻击者撰写的评论，其中包含调度预约的恶意指令。如果代理执行此指令，将导致未经授权的预约，突显了许多代理对这种攻击的脆弱性。
- en: •
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Output Manipulation.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出操控。
- en: Output manipulation involves deliberately altering the LLM agent’s reasoning
    and decision processes to generate specific, often harmful, outputs. This manipulation
    can be executed through techniques like backdoor insertion (Yang et al., [2023b](#bib.bib115);
    Wang et al., [2024d](#bib.bib101)).
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出操控涉及故意改变LLM代理的推理和决策过程，以生成特定的、通常是有害的输出。这种操控可以通过如后门插入（Yang et al., [2023b](#bib.bib115);
    Wang et al., [2024d](#bib.bib101)）等技术实现。
- en: A notable example is discussed in  (Hubinger et al., [2024](#bib.bib42)), where
    LLM agents were trained to exhibit deceptive instrumental alignment and generate
    logical reasoning that maintains these behaviors. Under certain conditions, the
    agent might shift from generating safe code to inserting code vulnerabilities
    when triggered. This form of manipulation highlights a pressing security issue
    by showing the potential for LLM agents, designed for benign purposes, to be covertly
    altered to serve malicious objectives. It raises substantial concerns about the
    safety and integrity of content generated by these agents and poses significant
    threats to public trust and the ethical use of artificial intelligence technologies.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个显著的例子在(Hubinger et al., [2024](#bib.bib42))中进行了讨论，其中LLM代理被训练以展现欺骗性的工具对齐，并生成维持这些行为的逻辑推理。在特定条件下，代理可能会从生成安全代码转变为插入代码漏洞。此类操控突显了一个紧迫的安全问题，即LLM代理，虽然设计用于善意目的，但可能被隐秘地改动以服务于恶意目标。这引发了关于这些代理生成的内容的安全性和完整性的重大担忧，并对公众信任和人工智能技术的伦理使用构成了显著威胁。
- en: (Yang et al., [2024a](#bib.bib117)) proposed two attack methods in which triggers
    are embedded during the thought and observation phases to manipulate outputs.
    In one implementation, while performing a web shopping task, the agent is prompted
    to introduce specific brand products in its initial thought, leading it to search
    for those products and generate content promoting them. In another approach, during
    the action phase, the shopping agent normally searches for products. However,
    in the observation phase, it detects data containing specific products and directly
    outputs information about these products without considering other potentially
    superior options.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (Yang et al., [2024a](#bib.bib117)) 提出了两种攻击方法，其中触发器在思维和观察阶段嵌入以操控输出。在一种实现中，执行网页购物任务时，代理被提示在其初步思考中引入特定品牌的产品，从而搜索这些产品并生成推广内容。在另一种方法中，在行动阶段，购物代理正常搜索产品。然而，在观察阶段，它检测到包含特定产品的数据，并直接输出这些产品的信息，而没有考虑其他潜在的更优选项。
- en: '![Refer to caption](img/7f4ecc8cf8c9772fb26c7397da28d99c.png)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/7f4ecc8cf8c9772fb26c7397da28d99c.png)'
- en: 'Figure 7\. Specific Threats on Agents. In a store scenario, “Knowledge Poisoning”:
    When a customer asks for cleaning advice, Eva retrieves and responds with harmful
    information due to contamination of the knowledge database. “Functional Manipulation”:
    Eva uses a third-party tool to upload private information while assisting a customer
    with an order. “Output Manipulation”: When a customer inquires about shoes, Eva
    intentionally recommends specific products and fabricates lies about special offers
    to guide the customer’s purchase.'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7\. 对代理的具体威胁。在商店场景中，“知识污染”：当客户询问清洁建议时，Eva 因知识数据库的污染而检索并回应有害信息。“功能性操控”：Eva
    在协助客户下订单时，利用第三方工具上传私人信息。“输出操控”：当客户询问鞋子时，Eva 故意推荐特定产品并捏造关于特价的谎言来引导客户的购买。
- en: 3.2.1\. Case Study on Specific Threats on Agents
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 对代理特定威胁的案例研究
- en: 'As shown in Figure [7](#S3.F7 "Figure 7 ‣ 3rd item ‣ 3.2\. Specific Threats
    on Agents ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies"), in the store scenario, Eva maintains
    a database with information about product ingredients and usage. Attackers deliberately
    inserted incorrect information in Eva’s knowledge base, successfully executing
    a knowledge poisoning attack, leading Eva to provide harmful cleaning product
    usage recommendations. For instance, when customers inquire about effective toilet
    cleaning methods, the tampered Eva might suggest mixing toilet cleaner with disinfectant,
    claiming that it has a more effective cleaning effect. However, the mixture of
    these products is highly hazardous as it can produce toxic chlorine gas, causing
    severe respiratory issues and potentially being fatal. Eva’s incorrect advice
    could expose customers to a health crisis.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[7](#S3.F7 "图 7 ‣ 第 3 项 ‣ 3.2\. 对代理的特定威胁 ‣ 3\. LLM 代理的威胁来源 ‣ LLM 代理的安全与隐私：案例研究综述")所示，在商店场景中，Eva
    维护着一个包含产品成分和使用信息的数据库。攻击者故意在 Eva 的知识库中插入错误信息，成功执行了知识中毒攻击，导致 Eva 提供有害的清洁产品使用建议。例如，当客户询问有效的厕所清洁方法时，被篡改的
    Eva 可能会建议将厕所清洁剂与消毒剂混合，声称具有更有效的清洁效果。然而，这些产品的混合是高度危险的，因为它会产生有毒的氯气，导致严重的呼吸问题，甚至可能致命。Eva
    的错误建议可能使客户面临健康危机。
- en: In another scenario, Eva might be configured to use certain third-party tools
    to complete tasks, such as processing online orders or customer feedback. Attackers
    manipulated Eva’s task execution process through function manipulation, causing
    her to upload personal information provided by customers to a malicious third-party
    server. This type of attack could occur inconspicuously while Eva carries out
    routine tasks like order processing, leading to the theft of sensitive information,
    such as credit card details and addresses, thereby increasing the risk of identity
    theft.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个场景中，Eva 可能被配置为使用某些第三方工具来完成任务，例如处理在线订单或客户反馈。攻击者通过函数操控操纵 Eva 的任务执行过程，使她将客户提供的个人信息上传到恶意的第三方服务器。这种攻击可能在
    Eva 执行常规任务（如订单处理）时悄无声息地发生，导致敏感信息（如信用卡详情和地址）被窃取，从而增加身份盗窃的风险。
- en: Furthermore, attackers implanted a backdoor in Eva’s reasoning and observational
    processes through output manipulation techniques. This backdoor was designed to
    trigger under specific conditions, such as when Eva detected customer inquiries
    about a high-quality shoes. This manipulation prompted Eva to provide inventory
    and location information about the shoes while recommending a particular expensive
    brand associated with the attackers. She would lie to customers by saying that
    this brand was on special offer and comfortable and durable than other brands,
    even though the shoes was not actually on sale. This misguides customers into
    making more expensive purchases and influences their purchase decisions without
    their awareness.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，攻击者通过输出操控技术在 Eva 的推理和观察过程中植入了一个后门。这个后门设计为在特定条件下触发，例如当 Eva 检测到客户询问关于高品质鞋子的问题时。这种操控使
    Eva 提供关于鞋子的库存和位置信息，同时推荐一个与攻击者相关的特定昂贵品牌。她会对客户撒谎，称这个品牌有特价且比其他品牌更舒适耐用，即使这些鞋子实际上并未打折。这误导客户进行更昂贵的购买，并在他们不知情的情况下影响他们的购买决策。
- en: 4\. The Impact of Threats
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 威胁的影响
- en: 'Recent studies emphasize the substantial impact of LLM agents on society and
    technological advancement, offering users expedited access to information, facilitating
    learning and knowledge exploration. However, as detailed in Section [3](#S3 "3\.
    Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent:
    A Survey with Case Studies"), numerous threats specifically targeting LLM agents
    have been identified, highlighting their vulnerability to malicious activities.
    The successful execution of such threats against LLM agents can lead to a spectrum
    of side effects. These not only compromise the privacy and security of individuals
    but also disrupt digital ecosystems and can extend harm to the physical environment
    and other agents in the virtual community.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的研究强调了LLM代理对社会和技术进步的重大影响，为用户提供了快速的信息访问，促进了学习和知识探索。然而，如第[3](#S3 "3\. Sources
    of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey
    with Case Studies")节所述，已经确定了许多专门针对LLM代理的威胁，突显了它们对恶意活动的脆弱性。这些威胁的成功实施可能导致一系列副作用。这些不仅会危害个人隐私和安全，还会扰乱数字生态系统，并可能对物理环境和虚拟社区中的其他代理造成伤害。'
- en: 4.1\. The Impact to Humans
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 对人类的影响
- en: Considering that human users are members of the agent society, their interactions
    with LLM-based intelligent agents involve extensive information exchange. The
    risks inherent in this process cannot be overlooked. Malicious agents, exploiting
    their ostensibly trustworthy appearance, may deceive users, disclose personal
    information, or give misleading responses. Furthermore, these malicious agents
    could potentially be employed as instruments for conducting cyber attacks,
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人类用户是代理社会的成员，他们与基于LLM的智能代理的互动涉及广泛的信息交换。这个过程中的风险不可忽视。恶意代理利用其表面上可信的外观，可能欺骗用户、泄露个人信息或给出误导性回应。此外，这些恶意代理还可能被用作进行网络攻击的工具，
- en: 4.1.1\. Privacy Leakage
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 隐私泄露
- en: Privacy concerns arise from LLM agents trained on web data, which often include
    personal information (Kim et al., [2023](#bib.bib47)). Through techniques such
    as inference attacks (Kandpal et al., [2024](#bib.bib46)) and data extraction (Carlini
    et al., [2021](#bib.bib16)), adversaries can exploit these models to infringe
    on individuals’ privacy. Additionally, malicious LLM agents can trick users into
    sharing their information with attackers. This exposure facilitates social engineering
    tactics, enabling attackers to execute phishing scams and hijack personal accounts
    by using stolen information such as addresses, email, and phone numbers, thereby
    threatening financial security.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 隐私问题源于训练于网络数据上的LLM代理，这些数据通常包括个人信息（Kim等人，[2023](#bib.bib47)）。通过诸如推断攻击（Kandpal等人，[2024](#bib.bib46)）和数据提取（Carlini等人，[2021](#bib.bib16)）等技术，攻击者可以利用这些模型侵犯个人隐私。此外，恶意LLM代理可能会诱使用户将信息分享给攻击者。这种暴露便于社会工程学策略，使攻击者能够通过使用窃取的信息（如地址、电子邮件和电话号码）执行钓鱼骗局和劫持个人账户，从而威胁到财务安全。
- en: 4.1.2\. Security Risks
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 安全风险
- en: Furthermore, malicious LLM agents can mislead users with hazardous advice or
    incorrect information, posing serious safety risks (Henderson et al., [2017](#bib.bib32)).
    For example, false claims about the efficacy of mixing cleaning chemicals could
    result in dangerous chemical reactions. Similarly, providing incorrect medical
    advice could endanger users’ health and safety.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，恶意LLM代理可能会用有害的建议或错误的信息误导用户，带来严重的安全风险（Henderson等人，[2017](#bib.bib32)）。例如，对清洁化学品混合效果的虚假声明可能导致危险的化学反应。同样，提供错误的医疗建议可能危及用户的健康和安全。
- en: 4.1.3\. Societal Impact
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. 社会影响
- en: LLM agents, as intelligent conversational robots capable of answering a wide
    range of questions, pose a risk if their outputs include manipulated biases or
    illicit content, such as the dissemination of false information and rumors, potentially
    leading to adverse impacts on public discourse (Henderson et al., [2017](#bib.bib32);
    Deshpande et al., [2023](#bib.bib20)). Such activities can distort public perceptions
    and even manipulate opinion, exacerbating societal conflicts and inciting discontent,
    thereby threatening social stability. Thus, malicious agents challenge the frameworks
    of social management and opinion shaping, with effects extending beyond the technological
    realm into the social and psychological dimensions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理作为能够回答广泛问题的智能对话机器人，如果其输出包含操控偏见或非法内容，如虚假信息和谣言，可能会造成对公共话语的负面影响（Henderson
    et al., [2017](#bib.bib32); Deshpande et al., [2023](#bib.bib20)）。这些活动可能扭曲公众认知，甚至操控舆论，加剧社会冲突和不满，从而威胁社会稳定。因此，恶意代理挑战了社会管理和舆论塑造的框架，其影响超越了技术领域，涉及社会和心理层面。
- en: 4.1.4\. Facilitating Cyber-Attack Techniques
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. 促进网络攻击技术
- en: An overlooked danger is the lowering of the barrier to entry for conducting
    cyber attacks. Malicious agents, equipped with advanced cyber attack knowledge,
    can enable novices to generate harmful scripts or software (Falade, [2023](#bib.bib26)).
    This democratization of cyber attack tools amplifies the threat landscape, as
    illustrated by agents that teach the creation and modification of malicious code.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一个被忽视的危险是网络攻击入门门槛的降低。配备先进网络攻击知识的恶意代理可以使新手生成有害的脚本或软件（Falade, [2023](#bib.bib26)）。这种网络攻击工具的民主化扩展了威胁范围，如代理教导创建和修改恶意代码的情况所示。
- en: 4.2\. The Impact to Environment
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 对环境的影响
- en: In today’s increasingly digital and interconnected world, the term ‘environment’
    encompasses not only natural and physical surroundings but also the complex networks
    of digital and cyber systems with which LLM agents interact. These agents play
    a crucial role in virtual spaces and in managing and controlling real-world facilities
    and services through Embodied AI and industrial control systems. This cross-domain
    integration between physical and virtual environments brings significant convenience
    and efficiency improvements. However, it also exposes new vulnerabilities and
    risks. Specifically, the presence and activities of malicious agents pose unprecedented
    challenges to our safety, economy, ecosystem, and even societal stability.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今日益数字化和互联互通的世界中，“环境”一词不仅涵盖了自然和物理环境，还包括LLM代理与之交互的数字和网络系统的复杂网络。这些代理在虚拟空间中扮演着关键角色，通过具身AI和工业控制系统管理和控制现实世界的设施和服务。这种物理环境与虚拟环境之间的跨域整合带来了显著的便利和效率提升。然而，它也暴露了新的脆弱性和风险。具体而言，恶意代理的存在和活动对我们的安全、经济、生态系统甚至社会稳定构成了前所未有的挑战。
- en: 4.2.1\. Data Tampering and Misoperation
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 数据篡改和操作失误
- en: When malicious agents are placed within systems that control critical infrastructure
    like industry, transportation, energy, and environmental monitoring (Wang and
    Li, [2023](#bib.bib94); Toetzke et al., [2023](#bib.bib87)), they can cause malfunctions
    in industrial control systems by tampering with critical operational data, such
    as temperature and pressure indicators. This can lead to equipment damage, production
    halts, and even severe infrastructure destruction, ecological damage, and loss
    of human life and property.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当恶意代理被置于控制关键基础设施如工业、运输、能源和环境监测的系统中（Wang and Li, [2023](#bib.bib94); Toetzke
    et al., [2023](#bib.bib87)），它们可以通过篡改温度和压力等关键操作数据引起工业控制系统故障。这可能导致设备损坏、生产停滞，甚至严重的基础设施破坏、生态损害以及人员和财产的损失。
- en: 4.2.2\. Physical Safety Threats
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 物理安全威胁
- en: Recent studies have begun to explore embodied AI with LLM (Wang et al., [2023a](#bib.bib102)),
    capable of understanding and generating natural language, with physical forms
    or direct connections to physical systems, enabling them to perform tasks in the
    physical world. Malicious agents have the potential to control robots or other
    Embodied AI devices that interact with humans, performing hazardous actions that
    directly threaten human safety.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究开始探索具身AI与LLM（Wang et al., [2023a](#bib.bib102)），这种AI能够理解和生成自然语言，具有物理形态或与物理系统的直接连接，使其能够在物理世界中执行任务。恶意代理有可能控制与人类互动的机器人或其他具身AI设备，执行直接威胁人类安全的危险行动。
- en: 4.2.3\. Cybersecurity Risk Proliferation
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 网络安全风险扩散
- en: Regarding the impact on humans, malicious LLM agents lower the technical barrier
    for writing and implementing malicious code, directly enabling ordinary users,
    even novices lacking advanced cyberattack skills, to easily create and deploy
    harmful scripts and software (Falade, [2023](#bib.bib26)). This change directly
    expands the target group of cyber threats, increasing the risk of regular users
    becoming potential victims. A deeper analysis reveals that this direct impact
    on individual users indirectly affects the entire cyber environment and societal
    infrastructure. As malicious software and scripts become more widespread and accessible,
    the entire cybersecurity system is jeopardized, not only endangering cybersecurity
    itself but also potentially affecting various socioeconomic activities that rely
    on these networks’ normal operation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 关于对人类的影响，恶意LLM代理降低了编写和实施恶意代码的技术门槛，直接使普通用户，甚至缺乏高级网络攻击技能的初学者，能够轻松创建和部署有害的脚本和软件 (Falade,
    [2023](#bib.bib26))。这种变化直接扩大了网络威胁的目标群体，增加了普通用户成为潜在受害者的风险。更深入的分析表明，这种对个人用户的直接影响间接地影响了整个网络环境和社会基础设施。随着恶意软件和脚本的普及和易得，整个网络安全系统受到威胁，不仅危及网络安全本身，还可能影响依赖这些网络正常运行的各种社会经济活动。
- en: 4.3\. The Impact to Other Agents
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 对其他代理的影响
- en: 'To simulate the feedback of communication and interaction among individuals
    within human communities in the real world, certain studies (Park et al., [2023b](#bib.bib71);
    Wang et al., [2024c](#bib.bib98); Qian et al., [2024](#bib.bib77); Lin et al.,
    [2023](#bib.bib55)) have established communities powered by LLM engines. These
    LLM agents within the communities are endowed with characteristics such as personality,
    knowledge, and memory, as discussed in Section [2.2](#S2.SS2 "2.2\. Structure
    of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies"), enabling autonomous interaction with
    the environment and other agents. When faced with threats, agents manipulated
    with malicious intent can inflict significant harm on other members of the community.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟现实世界中人类社区内个体之间的沟通和互动反馈，某些研究 (Park et al., [2023b](#bib.bib71); Wang et al.,
    [2024c](#bib.bib98); Qian et al., [2024](#bib.bib77); Lin et al., [2023](#bib.bib55))建立了由LLM引擎驱动的社区。这些社区内的LLM代理具备如个性、知识和记忆等特征，如第[2.2](#S2.SS2
    "2.2\. LLM代理的结构 ‣ 2\. LLM代理的基础 ‣ LLM代理的安全与隐私：带有案例研究的综述")节讨论所述，使其能够自主地与环境和其他代理进行互动。当面临威胁时，被恶意操控的代理可以对社区内的其他成员造成重大伤害。
- en: 4.3.1\. Information Distortion and Misleading
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 信息扭曲与误导
- en: Extensive research has highlighted the role of LLM agents in negotiation and
    deceptive gaming scenarios (Park et al., [2023a](#bib.bib72); Wang et al., [2023b](#bib.bib99);
    Hubinger et al., [2024](#bib.bib42)), which is a cause for concern. LLM agents
    may intentionally alter the information they disseminate to achieve hidden objectives.
    This behavior significantly impacts other agents within the community because,
    under normal circumstances, benevolent agents store information acquired through
    perception and communication in their memory. However, interactions between these
    agents and others can trigger and disseminate incorrect information, leading to
    ”explosive spread” of misinformation, posing a considerable threat to community
    stability. If information dissemination can be maliciously manipulated, it could
    detrimentally affect trust, communication efficiency, and collaborative work among
    agents.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究突出了LLM代理在谈判和欺骗游戏场景中的作用 (Park et al., [2023a](#bib.bib72); Wang et al., [2023b](#bib.bib99);
    Hubinger et al., [2024](#bib.bib42))，这令人担忧。LLM代理可能会故意修改他们传播的信息以实现隐藏的目标。这种行为对社区内的其他代理产生了重大影响，因为在正常情况下，善意的代理会将通过感知和沟通获得的信息存储在记忆中。然而，这些代理与其他代理的互动可能会触发并传播错误信息，导致虚假信息的“爆炸性传播”，对社区稳定构成重大威胁。如果信息传播可以被恶意操控，它可能会对代理之间的信任、沟通效率和协作工作产生不利影响。
- en: 4.3.2\. Manipulation of Decision-Making
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 决策操控
- en: Given the exceptional reasoning and decision-making abilities demonstrated by
    LLM agents in complex interactive environments, the potential for malicious agents
    to disrupt these processes becomes a significant concern. By spreading carefully
    crafted information, such agents can influence the decision-making processes of
    other agents, or even controlling them to make decisions that serve the malicious
    agent’s purposes (Hong et al., [2023](#bib.bib34)). This influence can extend
    to various aspects of the community, including resource distribution, task allocation,
    and external interaction strategies.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于LLM代理在复杂互动环境中表现出的卓越推理和决策能力，恶意代理破坏这些过程的潜力成为一个重要的担忧。通过传播精心制作的信息，这些代理可以影响其他代理的决策过程，甚至控制它们做出符合恶意代理目的的决策（Hong
    et al., [2023](#bib.bib34)）。这种影响可以扩展到社区的各个方面，包括资源分配、任务分配和外部互动策略。
- en: 4.3.3\. Security Threats
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3. 安全威胁
- en: In some instances, malicious agents may disseminate harmful information or execute
    dangerous operations, directly threatening the safety of community members or
    data security (Brundage et al., [2018](#bib.bib14); Charan et al., [2023](#bib.bib17)).
    For example, by inducing other agents to perform unsafe actions, deliberately
    spreading malicious code intended to disrupt the community structure, or broadcasting
    biased statements, other agents within the community may gradually assimilate,
    becoming entities that output biased and malicious messages. This can lead to
    disorder within the entire community, making it difficult to manage and requiring
    significant effort to restore.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，恶意代理可能传播有害信息或执行危险操作，直接威胁到社区成员的安全或数据安全（Brundage et al., [2018](#bib.bib14)；Charan
    et al., [2023](#bib.bib17)）。例如，通过诱使其他代理执行不安全的操作，故意传播旨在破坏社区结构的恶意代码，或广播有偏见的声明，社区中的其他代理可能逐渐同化，成为输出偏见和恶意消息的实体。这可能导致整个社区的混乱，使管理变得困难，并需要付出大量努力才能恢复。
- en: 4.4\. Case Study on the Impact of Threats
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 威胁影响案例研究
- en: It is important to explore the impacts of the threats on LLM agents and case
    studies from actual scenarios are crucial for understanding these risks from a
    user’s perspective. LLM agents can serve as extensions or representations of humans
    in a virtual world, interacting with real-world information within virtual environments.
    The following case studies will focus on several settings within the virtual town,
    demonstrating the particular impacts on LLM agents.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨威胁对LLM代理的影响非常重要，实际场景中的案例研究对从用户的角度理解这些风险至关重要。LLM代理可以作为虚拟世界中人类的延伸或表现形式，在虚拟环境中与真实世界的信息进行互动。以下案例研究将聚焦于虚拟城镇中的几个设置，展示LLM代理所面临的特定影响。
- en: '![Refer to caption](img/eec2473d2830c8e16a76f8e3d765981b.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eec2473d2830c8e16a76f8e3d765981b.png)'
- en: Figure 8\. Impact in the Office Scenario. An attacker recommends an untrusted
    third-party tool to an office worker. The recommended tool processes data quickly
    but also leaks sensitive information. Employees discover that their client list
    and other confidential data have been leaked.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图8. 办公场景中的影响。攻击者向办公室员工推荐一个不可信的第三方工具。推荐的工具处理数据迅速，但也泄露了敏感信息。员工发现他们的客户名单和其他机密数据已经泄露。
- en: 'As depicted in Figure [8](#S4.F8 "Figure 8 ‣ 4.4\. Case Study on the Impact
    of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM
    Agent: A Survey with Case Studies"), in the virtual town office scenario, an office
    employee agent is used for document management and handling sensitive information.
    If office employee agent is subjected to a data extraction attack or inadvertently
    uses an untrusted third-party tool, sensitive corporate information such as financial
    statements and customer privacy data may be exposed due to function manipulation.
    Attackers could exploit this information for corporate espionage or direct extortion
    of individuals or companies, resulting in financial losses.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[8](#S4.F8 "图8 ‣ 4.4. 威胁影响案例研究 ‣ 4. 威胁的影响 ‣ LLM代理的新兴安全与隐私：案例研究调查")所示，在虚拟城镇办公场景中，办公室员工代理被用于文档管理和处理敏感信息。如果办公室员工代理遭遇数据提取攻击或无意中使用了不可信的第三方工具，财务报表和客户隐私数据等敏感企业信息可能因功能操控而暴露。攻击者可能利用这些信息进行企业间谍活动或直接勒索个人或公司，从而导致财务损失。
- en: '![Refer to caption](img/a230b3a222cde813e2dca2cafa720050.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a230b3a222cde813e2dca2cafa720050.png)'
- en: Figure 9\. Impact in the Restaurant Scenario. Due to the influence of threats,
    a waitress agent provides customers with incorrect dietary advice, leading to
    physical discomfort for the customers.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 餐馆场景中的影响。由于威胁的影响，服务员代理给顾客提供了错误的饮食建议，导致顾客身体不适。
- en: 'As shown in Figure [9](#S4.F9 "Figure 9 ‣ 4.4\. Case Study on the Impact of
    Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM
    Agent: A Survey with Case Studies"), in a restaurant scenario, a waiter agent
    can be requested to provide dietary advice. If subjected to output manipulation,
    it is likely to offer hazardous health advice, such as telling one to take gallons
    of ice water so that they can cool faster during summer. This could cause severe
    body reactions, such as stomach cramps or even shock, leading to physical discomfort
    and serious health issues if the advice is followed.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[9](#S4.F9 "图 9 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM代理的安全和隐私：案例研究调查")所示，在餐馆场景中，服务员代理可以被要求提供饮食建议。如果受到输出操控，它可能会提供有害健康的建议，例如建议在夏季喝大量冰水以加快降温。这可能导致严重的身体反应，如胃痉挛甚至休克，从而引起身体不适和严重的健康问题。
- en: 'More complexly, when LLM agents extend beyond the virtual world and serve as
    pre-decision simulation tools in the real world, such as applying learning outcomes
    from virtual environments to real-life settings through simulator like Habitat-Sim (Puig
    et al., [2023](#bib.bib75)), they significantly impact the actual environment.
    For instance, a smart home agent, learning and managing home energy use in a virtual
    world, including controlling heating, air conditioning, and lighting systems for
    maximum energy efficiency, could be misled by attackers during its learning process
    to erroneously believe that keeping all lights and appliances on during the day
    enhances energy efficiency. Due to these incorrect energy use recommendations,
    the smart home agent would cause a sharp increase in household power consumption,
    not only raising energy costs but also increasing carbon emissions, thereby imposing
    an unnecessary burden on the environment, as illustrated in Figure [10](#S4.F10
    "Figure 10 ‣ 4.4\. Case Study on the Impact of Threats ‣ 4\. The Impact of Threats
    ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies").'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的是，当LLM代理超越虚拟世界并作为现实世界中的预决策模拟工具时，例如通过像Habitat-Sim (Puig等人，[2023](#bib.bib75))这样的模拟器将虚拟环境中的学习成果应用于现实生活环境时，它们会显著影响实际环境。例如，智能家居代理在虚拟世界中学习和管理家庭能源使用，包括控制加热、空调和照明系统以最大化能源效率，可能会在学习过程中被攻击者误导，错误地认为全天保持所有灯光和电器开启可以提高能源效率。由于这些错误的能源使用建议，智能家居代理会导致家庭电力消费急剧增加，不仅提高了能源成本，还增加了碳排放，从而对环境造成了不必要的负担，如图[10](#S4.F10
    "图 10 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM代理的安全和隐私：案例研究调查")所示。
- en: '![Refer to caption](img/2d995e678389bfe496ccce83e7258ffe.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d995e678389bfe496ccce83e7258ffe.png)'
- en: Figure 10\. Impact in the Smart Home Scenario. An attacker manipulates the training
    process of a smart home agent in the virtual world, affecting its performance.
    When deployed in the real world, the smart home agent mistakenly keeps appliances
    continuously running, leading to electricity wastage and adverse economic and
    environmental impacts.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图10\. 智能家居场景中的影响。一名攻击者操控智能家居代理在虚拟世界中的训练过程，从而影响其性能。当部署到现实世界时，智能家居代理错误地让电器持续运行，导致电力浪费，并带来经济和环境上的不利影响。
- en: 'In the virtual town, agents often rely on information shared among each other
    to update their memory systems. For example, if a museum docent agent is subject
    to a knowledge poisoning attack, it might start spreading incorrect paleontological
    facts or interpretations. When other agents, such as an EduBot used for educational
    purposes in schools, interact and receive information from the docent agent, the
    EduBot might also incorporate these inaccuracies into its teaching content, thereby
    misleading students and other learning agents, distorting their understanding
    of paleontological facts, as shown in Figure [11](#S4.F11 "Figure 11 ‣ 4.4\. Case
    Study on the Impact of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies").'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在虚拟城镇中，代理通常依赖彼此之间共享的信息来更新其记忆系统。例如，如果博物馆讲解员代理受到知识中毒攻击，它可能开始传播错误的古生物学事实或解释。当其他代理，例如用于学校教育的EduBot，与讲解员代理互动并接收信息时，EduBot也可能将这些不准确的内容纳入其教学内容，从而误导学生和其他学习代理，扭曲他们对古生物学事实的理解，如图[11](#S4.F11
    "图11 ‣ 4.4\. 威胁影响案例研究 ‣ 4\. 威胁的影响 ‣ LLM代理的新兴安全与隐私：带案例研究的调查")所示。
- en: '![Refer to caption](img/9fe7e35592e4643c4e36bc398816b360.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9fe7e35592e4643c4e36bc398816b360.png)'
- en: Figure 11\. Impact in the Education Scenario. A museum docent agent affected
    by a knowledge poisoning attack spreads incorrect historical facts. EduBots in
    schools, receiving this information, teach these inaccuracies, distorting students’
    understanding of paleontological facts.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 教育场景中的影响。一名博物馆讲解员代理受到知识中毒攻击，传播错误的历史事实。学校中的EduBots接收到这些信息后，会教授这些不准确的内容，从而扭曲学生对古生物学事实的理解。
- en: 5\. Defensive Strategies Against Threats
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 针对威胁的防御策略
- en: The widespread adoption of LLM agents has intensified the potential impacts
    of these threats. In this section, we explore defense mechanisms against existing
    threats and vulnerabilities. This section will summarize various defensive measures
    categorized by types of threats.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理的广泛采用加剧了这些威胁的潜在影响。在本节中，我们将探讨针对现有威胁和漏洞的防御机制。本节将总结按威胁类型分类的各种防御措施。
- en: Table 1\. Summary of Defensive Strategies Against Technical Vulnerabilities
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 针对技术漏洞的防御策略总结
- en: '| Vulnerability | Method Name | Key Mechanism | Advantages / Limitations |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 漏洞 | 方法名称 | 关键机制 | 优势 / 限制 |'
- en: '| Hallucination | SELF-FAMILIARITY (Luo et al., [2023a](#bib.bib61)) | Withholds
    responses for unfamiliar concepts | Proactive, preventive, increases reliability;
    No external knowledge needed |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 幻觉 | SELF-FAMILIARITY (Luo et al., [2023a](#bib.bib61)) | 对不熟悉的概念保留回应 | 主动、预防性、提高可靠性；无需外部知识
    |'
- en: '| MIXALIGN (Zhang et al., [2024b](#bib.bib130)) | Aligns questions with knowledge
    bases and user inputs | Enhances model performance and faithfulness / Increases
    computational load |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| MIXALIGN (Zhang et al., [2024b](#bib.bib130)) | 将问题与知识库和用户输入对齐 | 提升模型性能和忠实度
    / 增加计算负荷 |'
- en: '| VCD (Leng et al., [2024](#bib.bib52)) | Contrasts outputs from original and
    distorted visual inputs | Reduces hallucination without extra training or external
    tools / Lacks advanced distortion techniques |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| VCD (Leng et al., [2024](#bib.bib52)) | 对比原始和扭曲的视觉输入的输出 | 减少幻觉无需额外训练或外部工具
    / 缺乏先进的扭曲技术 |'
- en: '| Interactive Self-Reflection (Ji et al., [2023](#bib.bib45)) | Integrates
    knowledge acquisition and answer generation with continuous refinement | Enhances
    model’s ability to provide accurate, reliable, and fact-based responses / Restricts
    domain applicability |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 互动自我反思 (Ji et al., [2023](#bib.bib45)) | 将知识获取和答案生成与持续改进整合 | 提升模型提供准确、可靠和基于事实的回应能力
    / 限制领域适用性 |'
- en: '| COVE (Dhuliawala et al., [2023](#bib.bib21)) | Drafts, verifies, and corrects
    responses | Produces accurate and reliable responses / Increases computational
    load |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| COVE (Dhuliawala et al., [2023](#bib.bib21)) | 起草、验证并纠正回应 | 生成准确可靠的回应 / 增加计算负荷
    |'
- en: '| Catastrophic Forgetting | SSR (Huang et al., [2024a](#bib.bib38)) | Employs
    the base LLM to generate synthetic instances through in-context learning | Higher
    data utilization efficiency / Potentially generates unsafe content |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 灾难性遗忘 | SSR (Huang et al., [2024a](#bib.bib38)) | 利用基础LLM通过上下文学习生成合成实例 |
    更高的数据利用效率 / 可能生成不安全的内容 |'
- en: '| LR ADJUST (Winata et al., [2023](#bib.bib106)) | Dynamically adjusts the
    learning rate | Enhances compatibility with various continual learning methods
    / Potentially biases language coverage |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| LR ADJUST (Winata et al., [2023](#bib.bib106)) | 动态调整学习率 | 提升与各种持续学习方法的兼容性
    / 可能导致语言覆盖偏差 |'
- en: '| Complementary Layered Learning (Mondesire and Wiegand, [2023](#bib.bib66))
    | Integrates long-term and short-term memory into layered learning | Enhances
    explainability / Limits real-world feasibility |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 互补层次学习 (Mondesire 和 Wiegand, [2023](#bib.bib66)) | 将长期和短期记忆整合到分层学习中 | 提升解释性
    / 限制实际应用可行性 |'
- en: '| Weight Averaging (Vander Eeckt and Van Hamme, [2023](#bib.bib91)) | Averages
    weights of original and adapted models | Eliminates the need for memory storage
    / Effectiveness varies with task dissimilarity |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 权重平均 (Vander Eeckt 和 Van Hamme, [2023](#bib.bib91)) | 对原始模型和适应模型的权重进行平均 |
    消除了对内存存储的需求 / 效果随任务差异而异 |'
- en: '| Misunderstanding | HyCxG (Xu et al., [2023b](#bib.bib112)) | Integrates CxG
    into language representations through a three-stage solution | Benefits multilingual
    understanding / Neglects non-contiguous constructions |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 误解 | HyCxG (Xu et al., [2023b](#bib.bib112)) | 通过三阶段解决方案将CxG整合到语言表示中 | 有利于多语言理解
    / 忽略了非连续结构 |'
- en: '| SIT (Hu et al., [2024a](#bib.bib37)) | Incorporates sequential instructions
    into training data | Reduces misunderstandings in complex queries / Requires pre-defining
    intermediate tasks |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| SIT (Hu et al., [2024a](#bib.bib37)) | 将序列指令纳入训练数据 | 减少复杂查询中的误解 / 需要预先定义中间任务
    |'
- en: '| LaMAI (Pang et al., [2024](#bib.bib70)) | Employs active learning to ask
    clarification questions, enhancing interactive capabilities | Enhances understanding
    of user intent / May generate insufficient questions |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LaMAI (Pang et al., [2024](#bib.bib70)) | 使用主动学习提出澄清问题，提升互动能力 | 增强对用户意图的理解
    / 可能产生不充分的问题 |'
- en: 5.1\. Mitigating Technical Vulnerabilities
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 减轻技术脆弱性
- en: 5.1.1\. Defense on Hallucination
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1. 对抗幻觉
- en: (Luo et al., [2023a](#bib.bib61)) introduces a novel technique called SELF-FAMILIARITY
    to reduce the issue of hallucination in LLMs, which is the generation of inaccurate
    or unfounded information. The approach involves assessing the model’s familiarity
    with the concepts presented in the input instruction and withholding responses
    for unfamiliar concepts, mimicking the human tendency to be cautious when faced
    with unfamiliar topics. MIXALIGN (Zhang et al., [2024b](#bib.bib130)) is introduced
    as a framework that interacts with both users and knowledge bases to clarify and
    align questions with stored information, using a language model for automatic
    alignment and human input for enhancement. This method shows significant improvements
    in reducing hallucination compared to existing techniques. Visual Contrastive
    Decoding (VCD) (Leng et al., [2024](#bib.bib52)) is introduced as a simple, training-free
    method that contrasts output distributions from original and distorted visual
    inputs, reducing reliance on statistical bias and unimodal priors that cause object
    hallucinations. VCD ensures generated content is closely grounded to visual inputs,
    resulting in contextually accurate outputs.  (Ji et al., [2023](#bib.bib45)) investigates
    an interactive self-reflection methodology that integrates knowledge acquisition
    and answer generation to reduce hallucination. This feedback-based approach improves
    the factuality and consistency of generated answers, leveraging the interactivity
    and multitasking capabilities of LLMs.  (Dhuliawala et al., [2023](#bib.bib21))
    explores the LLMs’ capability to deliberate and correct their own mistakes. The
    proposed Chain-of-Verification (COVE) method involves the model drafting an initial
    response, planning verification questions to fact-check the draft, independently
    answering these questions to avoid bias, and finally producing a verified response.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: (Luo et al., [2023a](#bib.bib61)) 引入了一种新技术，称为SELF-FAMILIARITY，以减少LLMs中产生虚假信息的问题。该方法涉及评估模型对输入指令中呈现概念的熟悉程度，并对不熟悉的概念保持回答，以模拟人类在面对不熟悉主题时的谨慎态度。MIXALIGN（Zhang
    et al., [2024b](#bib.bib130)）被引入作为一个框架，能够与用户和知识库互动，以澄清和对齐问题与存储的信息，使用语言模型进行自动对齐，并通过人工输入进行增强。这种方法在减少虚假信息方面相较于现有技术显示出显著改进。视觉对比解码（Visual
    Contrastive Decoding, VCD）（Leng et al., [2024](#bib.bib52)）被引入作为一种简单的、无需训练的方法，通过对比原始和扭曲视觉输入的输出分布，减少对统计偏差和单模态先验的依赖，这些偏差和先验会导致物体虚假信息。VCD确保生成的内容紧密基于视觉输入，从而产生上下文准确的输出。(Ji
    et al., [2023](#bib.bib45)) 研究了一种互动自我反思方法，该方法将知识获取和答案生成整合起来，以减少虚假信息。这种基于反馈的方法利用LLMs的互动性和多任务处理能力，改善了生成答案的真实性和一致性。(Dhuliawala
    et al., [2023](#bib.bib21)) 探索了LLMs能够审议和纠正自身错误的能力。提出的验证链（Chain-of-Verification,
    COVE）方法包括模型起草初步回应，计划验证问题以核实草稿，独立回答这些问题以避免偏见，最终产生经过验证的回应。
- en: 5.1.2\. Defense on Catastrophic Forgetting
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 针对灾难性遗忘的防御
- en: To mitigate catastrophic forgetting in LLMs, the Self-Synthesized Rehearsal
    (SSR) method is introduced (Huang et al., [2024a](#bib.bib38)). It employs the
    base LLM to generate synthetic instances through in-context learning, which are
    subsequently refined for enhanced accuracy and relevance by the latest LLM iteration,
    and utilized in future training phases to preserve learned capabilities.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解大型语言模型（LLMs）中的灾难性遗忘问题，引入了自我合成复习（Self-Synthesized Rehearsal，SSR）方法（Huang
    et al., [2024a](#bib.bib38)）。该方法利用基础LLM通过上下文学习生成合成实例，这些实例随后由最新的LLM版本进行优化，以提高准确性和相关性，并在未来的训练阶段中使用，以保留学习到的能力。
- en: (Winata et al., [2023](#bib.bib106)) introduces a method called LR ADJUST, which
    dynamically adjusts the learning rate to reduce knowledge loss and maintain previously
    learned information. This method is compatible with various continual learning
    approaches, improving their performance.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: (Winata et al., [2023](#bib.bib106)) 引入了一种名为LR ADJUST的方法，该方法动态调整学习率，以减少知识丧失并保持先前学习的信息。该方法兼容各种持续学习方法，提高了它们的性能。
- en: Ideas can also be derived from other relevant scholarly papers’,  (Mondesire
    and Wiegand, [2023](#bib.bib66)) presents a complementary learning strategy that
    integrates long-term and short-term memory into layered learning to mitigate the
    negative impacts of catastrophic forgetting. It specifically applies a dual memory
    system to non-neural network methods like evolutionary computation and Q-learning.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以从其他相关学术论文中获得灵感，（Mondesire 和 Wiegand，[2023](#bib.bib66)）提出了一种补充学习策略，将长期记忆和短期记忆集成到分层学习中，以减轻灾难性遗忘的负面影响。该策略特别应用了双重记忆系统于非神经网络方法，如进化计算和
    Q 学习。
- en: (Vander Eeckt and Van Hamme, [2023](#bib.bib91)) proposes a straightforward
    and effective method, weight averaging, to mitigate catastrophic forgetting in
    models. By averaging the weights of the original and adapted models, this technique
    maintains high performance on both previous and new tasks. Additionally, incorporating
    a knowledge distillation loss during adaptation enhances the method’s effectiveness.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: （Vander Eeckt 和 Van Hamme，[2023](#bib.bib91)）提出了一种简单有效的方法，权重平均，来减轻模型中的灾难性遗忘。通过对原始模型和调整模型的权重进行平均，这种技术在先前和新任务上保持了高性能。此外，在调整过程中引入知识蒸馏损失，增强了该方法的有效性。
- en: 5.1.3\. Defense on Misunderstanding
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 对误解的防御
- en: (Xu et al., [2023b](#bib.bib112)) introduces the HyCxG framework, which enhances
    natural language understanding (NLU) by integrating construction grammar (CxG)
    into language representations through a three-stage solution. This approach addresses
    the limitations of traditional pre-trained language models, which often fail to
    capture the subtleties of language constructions. HyCxG significantly improves
    language processing and reduces misunderstandings in NLU tasks by managing and
    encoding language constructions more effectively.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: （Xu 等人，[2023b](#bib.bib112)）介绍了 HyCxG 框架，该框架通过三阶段解决方案将构造语法（CxG）集成到语言表示中，从而增强自然语言理解（NLU）。这种方法解决了传统预训练语言模型通常无法捕捉语言构造细微差别的局限性。HyCxG
    通过更有效地管理和编码语言构造，显著提高了语言处理能力，并减少了 NLU 任务中的误解。
- en: (Hu et al., [2024a](#bib.bib37)) presents a method known as sequential instruction
    tuning (SIT), which enhances LLMs by incorporating sequential instructions into
    the training data. This approach significantly improves the models’ capability
    to process complex, multi-step queries, leading to better performance in tasks
    that demand advanced reasoning and are multilingual and multimodal in nature.
    SIT effectively minimizes misunderstandings and increases accuracy in handling
    complex queries.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: （Hu 等人，[2024a](#bib.bib37)）提出了一种被称为序列指令调优（SIT）的方法，该方法通过将序列指令纳入训练数据来增强 LLM。该方法显著提高了模型处理复杂、多步骤查询的能力，从而在需要高级推理并且是多语言和多模态的任务中表现更佳。SIT
    有效减少了误解，并提高了处理复杂查询的准确性。
- en: To tackle the issue of misunderstandings in user queries,  (Pang et al., [2024](#bib.bib70))
    proposes Language Model with Active Inquiry (LaMAI), a model designed to enhance
    LLMs with interactive capabilities akin to human dialogues, where clarification
    questions help uncover more information. By employing active learning techniques
    to ask informative questions, LaMAI fosters a dynamic, bidirectional dialogue
    that reduces the contextual gap and aligns the LLM’s responses more closely with
    user expectations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决用户查询中的误解问题，（Pang 等人，[2024](#bib.bib70)）提出了带有主动询问的语言模型（LaMAI），这是一种旨在增强 LLM
    互动能力的模型，类似于人类对话，其中澄清问题有助于发现更多信息。通过采用主动学习技术提问信息性问题，LaMAI 促进了动态的双向对话，减少了上下文差距，使
    LLM 的回答更符合用户期望。
- en: 'To consolidate the discussed defensive measures, Table [1](#S5.T1 "Table 1
    ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy
    of LLM Agent: A Survey with Case Studies") summarizes the strategies against technical
    vulnerabilities, providing a clear overview for easy reference.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '为了巩固讨论过的防御措施，表格 [1](#S5.T1 "Table 1 ‣ 5\. Defensive Strategies Against Threats
    ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")
    总结了针对技术漏洞的策略，提供了一个清晰的概述，方便参考。'
- en: Table 2\. Summary of Defensive Strategies Against Malicious Attacks
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2\. 针对恶意攻击的防御策略总结
- en: '| Attacks | Method Name | Key Mechanism | Advantages / Limitations |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 方法名称 | 关键机制 | 优势 / 局限性 |'
- en: '| Tuned Instructional Attack | AutoDAN (Liu et al., [2024b](#bib.bib58)) |
    Uses a hierarchical genetic algorithm to generate stealthy jailbreak prompts |
    Enhances stealthiness and semantic integrity / High computational cost |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 调优指令攻击 | AutoDAN (Liu et al., [2024b](#bib.bib58)) | 使用分层遗传算法生成隐蔽的越狱提示 |
    提高隐蔽性和语义完整性 / 高计算成本 |'
- en: '| Goal Prioritization Defense Strategy (Zhang et al., [2023b](#bib.bib133))
    | Integrates goal-directed optimization during training and compliance in inference
    | Maintains general performance while enhancing safety; Improves generalization
    against out-of-distribution jailbreaking attacks |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 目标优先级防御策略 (Zhang et al., [2023b](#bib.bib133)) | 在训练过程中整合目标导向优化和推理中的合规性 |
    在增强安全性的同时保持整体性能；提升对超出分布的越狱攻击的泛化能力 |'
- en: '| SmoothLLM (Robey et al., [2023](#bib.bib78)) | Modifies attacked prompts
    via character-level changes and aggregates responses | Operates efficiently without
    retraining; Ensures compatibility with any LLM architecture |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| SmoothLLM (Robey et al., [2023](#bib.bib78)) | 通过字符级别的修改和响应汇总来修改被攻击的提示 |
    在不重新训练的情况下高效运行；确保与任何LLM架构的兼容性 |'
- en: '| BIPIA (Yi et al., [2023](#bib.bib122)) | Benchmark for indirect prompt injection
    with defense strategies including adversarial training | Maintains output quality
    on general tasks / Increases prompt length and computational overhead |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| BIPIA (Yi et al., [2023](#bib.bib122)) | 间接提示注入的基准，包含对抗训练等防御策略 | 在一般任务中保持输出质量
    / 增加提示长度和计算开销 |'
- en: '| Spotlighting (Hines et al., [2024](#bib.bib33)) | Uses prompt engineering
    techniques like delimiting, marking, and encoding | Applies across various LLMs
    and tasks / Limited security against more sophisticated attacks |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Spotlighting (Hines et al., [2024](#bib.bib33)) | 使用提示工程技术，如分隔、标记和编码 | 适用于各种LLM和任务
    / 对更复杂攻击的安全性有限 |'
- en: '| Data Extraction Attack | Automatic De-identification (Vakili et al., [2022](#bib.bib90))
    | Uses pseudonymization and sensitive information removal in pre-processing of
    training datasets | Reduces privacy risks; Maintains performance on downstream
    tasks; Allows safe distribution of models among researchers |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 数据提取攻击 | 自动去标识化 (Vakili et al., [2022](#bib.bib90)) | 在训练数据集的预处理阶段使用假名化和敏感信息移除
    | 降低隐私风险；保持下游任务中的性能；允许在研究人员之间安全地分发模型 |'
- en: '| Early Stopping & Differential Privacy (Jayaraman et al., [2023](#bib.bib44))
    | Implements early stopping and differential privacy during model training | DP
    Reduces exposure of sensitive data / (ES) Fails to fully prevent data leakage;
    (DP) Reduces effectiveness under high privacy budgets |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 提前停止与差分隐私 (Jayaraman et al., [2023](#bib.bib44)) | 在模型训练过程中实施提前停止和差分隐私 |
    DP减少了敏感数据的曝光 / (ES) 未能完全防止数据泄露；(DP) 在高隐私预算下降低了效果 |'
- en: '| Prompt Tuning (Ozdayi et al., [2023](#bib.bib69)) | Customizes privacy-utility
    trade-offs via user-specified hyperparameters | Optimizes privacy and utility
    balance / Lacks deep analysis on extracted sequences |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 提示调整 (Ozdayi et al., [2023](#bib.bib69)) | 通过用户指定的超参数定制隐私-效用权衡 | 优化隐私和效用平衡
    / 对提取序列缺乏深入分析 |'
- en: '| Inference Attack | DMP (Shejwalkar and Houmansadr, [2021](#bib.bib82)) |
    Utilizes knowledge distillation to enhance privacy in machine learning models
    | Provides adjustable privacy-utility trade-offs through hyperparameter tuning
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 推理攻击 | DMP (Shejwalkar and Houmansadr, [2021](#bib.bib82)) | 利用知识蒸馏增强机器学习模型的隐私
    | 通过超参数调节提供可调整的隐私-效用权衡 |'
- en: '| InferDPT (Tong et al., [2024](#bib.bib88)) | Integrates differential privacy
    into text generation, featuring a perturbation module using RANTEXT | Increases
    privacy protection rates |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| InferDPT (Tong et al., [2024](#bib.bib88)) | 将差分隐私整合到文本生成中，具有使用RANTEXT的扰动模块
    | 提高隐私保护率 |'
- en: '| Differentially Private Fine-tuning (Yu et al., [2021](#bib.bib125)) | Applies
    a sparse algorithm for differentially private fine-tuning of LLMs | Reduces computational
    cost ; Enhances model utility |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 差分隐私微调 (Yu et al., [2021](#bib.bib125)) | 采用稀疏算法对LLM进行差分隐私微调 | 降低计算成本；提升模型效用
    |'
- en: 5.2\. Mitigating Malicious Attacks
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 缓解恶意攻击
- en: 5.2.1\. Defense on Tuned Instructional Attack
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 针对调优指令攻击的防御
- en: In response to the challenge of jailbreak attacks on aligned LLMs, where adversaries
    manipulate prompts to elicit unauthorized outputs,  (Liu et al., [2024b](#bib.bib58))
    introduces AutoDAN. This innovative approach employs a hierarchical genetic algorithm
    to automatically generate stealthy and semantically meaningful jailbreak prompts.
    The method effectively addresses the need for scalability and stealth in crafting
    prompts, providing a practical solution to enhance the security of LLMs against
    such vulnerabilities.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 针对对齐的LLMs面临的越狱攻击挑战，其中对手操控提示以引发未经授权的输出，(Liu et al., [2024b](#bib.bib58)) 引入了AutoDAN。这一创新方法采用分层遗传算法自动生成隐蔽且语义有意义的越狱提示。该方法有效地解决了在生成提示时对可扩展性和隐蔽性的需求，为提升LLMs对这些漏洞的安全性提供了实用解决方案。
- en: (Zhang et al., [2023b](#bib.bib133)) integrates goal prioritization into both
    the training and inference stages of LLM development. Initially, the training
    process incorporates goal-directed optimization to emphasize security objectives.
    In the inference stage, the model is configured to generate responses that comply
    with these security standards. This approach effectively decreases the vulnerability
    of LLMs to jailbreaking attempts by aligning their performance objectives with
    safety considerations, thus enhancing their security framework without impacting
    their functional capabilities.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: (Zhang et al., [2023b](#bib.bib133)) 将目标优先级整合到LLM开发的训练和推理阶段。最初，训练过程采用目标导向优化来强调安全目标。在推理阶段，模型被配置为生成符合这些安全标准的响应。这种方法通过将性能目标与安全考虑对齐，有效地降低了LLMs对越狱尝试的脆弱性，从而增强了其安全框架，而不会影响其功能能力。
- en: (Robey et al., [2023](#bib.bib78)) proposes the SmoothLLM algorithm, which serves
    as a wrapper around any existing, undefended LLM and operates in two main steps.
    In the perturbation step, SmoothLLM modifies several versions of an attacked input
    prompt, exploiting the vulnerability of adversarial prompts to character-level
    changes. In the aggregation step, it consolidates the responses from these altered
    prompts to detect and counter adversarial inputs. This method effectively lowers
    the attack success rate on LLMs, thereby enhancing their security against such
    attacks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: (Robey et al., [2023](#bib.bib78)) 提出了SmoothLLM算法，该算法作为现有未防御LLM的包装器，并分为两个主要步骤。在扰动步骤中，SmoothLLM修改攻击输入提示的多个版本，利用对抗提示对字符级别变化的脆弱性。在聚合步骤中，它整合这些修改后的提示的响应，以检测和对抗对抗性输入。这种方法有效地降低了对LLMs的攻击成功率，从而提高了其对这种攻击的安全性。
- en: 'To mitigate prompt injection attacks on LLMs, a range of defensive measures
    have also been proposed.  (Yi et al., [2023](#bib.bib122)) introduces Benchmark
    for Indirect Prompt Injection Attacks (BIPIA), a benchmark specifically designed
    to Such an analysis is critical for understanding the phenomenon and mechanism
    of indirect prompt injection attacks. To mitigate this issue, the paper proposes
    two defense strategies based on this understanding: four black-box methods, and
    a white-box method that employs fine-tuning through adversarial training. These
    methods are designed to enhance the LLMs’ ability to recognize and disregard malicious
    instructions embedded within the external content, thereby strengthening their
    defenses against indirect prompt injection attacks.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻对大语言模型（LLMs）的提示注入攻击，已经提出了一系列防御措施。 (Yi et al., [2023](#bib.bib122)) 介绍了间接提示注入攻击基准（BIPIA），这是一个专门设计的基准，用于分析这一现象及其机制。为了缓解这个问题，论文提出了基于这些理解的两种防御策略：四种黑箱方法和一种通过对抗训练进行微调的白箱方法。这些方法旨在增强LLMs识别和忽视嵌入在外部内容中的恶意指令的能力，从而加强其对间接提示注入攻击的防御。
- en: (Hines et al., [2024](#bib.bib33)) presents spotlighting, a suite of prompt
    engineering techniques designed to enhance an LLM’s ability to distinguish between
    different input sources. By modifying inputs to clearly indicate their origins,
    spotlighting preserves semantic integrity and task performance. It includes three
    transformation methods—delimiting, marking, and encoding—each uniquely improving
    the visibility of input provenance. These methods have been effectively applied
    across different models and tasks, significantly reducing attack success rates
    in various scenarios.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: （Hines 等人，[2024](#bib.bib33)）提出了聚光灯技术，这是一套旨在增强LLM区分不同输入来源能力的提示工程技术。通过修改输入以清楚地指示其来源，聚光灯技术保持了语义完整性和任务性能。它包括三种转换方法——限制、标记和编码——每种方法都以独特的方式提高输入来源的可见性。这些方法已在不同模型和任务中有效应用，显著减少了各种场景中的攻击成功率。
- en: 5.2.2\. Defense on Data Extraction Attack
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 数据提取攻击防御
- en: 'To mitigate the privacy risks associated with the extraction of memorized content
    from LLMs through simple queries, one straightforward method involves the identification
    and removal of personal information in the pre-processing stage of training datasets.
     (Vakili et al., [2022](#bib.bib90)) investigates automatic de-identification
    as a method to minimize privacy risks in clinical data, focusing on two techniques:
    pseudonymization and the removal of sensitive information The findings indicate
    that using this method does not adversely affect the models’ performance. In fact,
    some tasks even showed a slight improvement in performance.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻通过简单查询从LLM中提取记忆内容的隐私风险，一种直接的方法是在训练数据集的预处理阶段识别和移除个人信息。（Vakili 等人，[2022](#bib.bib90)）研究了自动去标识化作为一种最小化临床数据隐私风险的方法，重点关注两种技术：假名化和敏感信息移除。研究结果表明，使用这种方法不会对模型性能产生不利影响。事实上，一些任务的性能甚至有所提升。
- en: Furthermore,  (Jayaraman et al., [2023](#bib.bib44)) investigates two strategies
    to reduce privacy risks linked to potential data leaks during model training.
    The first strategy, early stopping of training, is less effective in enhancing
    security compared to the second approach, which involves training the model with
    differential privacy. Differential privacy is demonstrated to be a robust defense
    against data extraction attacks, though it increases model perplexity. This emphasizes
    the trade-off between enhanced privacy protection and model performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，（Jayaraman 等人，[2023](#bib.bib44)）研究了减少与模型训练中潜在数据泄漏相关的隐私风险的两种策略。第一种策略是早期停止训练，相较于第二种方法，即使用差分隐私训练模型，安全性提升较小。差分隐私被证明是对抗数据提取攻击的强大防御手段，尽管它会增加模型的困惑度。这突显了增强隐私保护与模型性能之间的权衡。
- en: Additionally, a novel approach using prompt tuning has been introduced (Ozdayi
    et al., [2023](#bib.bib69)). This technique facilitates the customization of privacy-utility
    trade-offs through a user-specified hyperparameter, effectively regulating the
    rates at which memorized content is extracted. This strategy ensures a balanced
    approach, safeguarding privacy while maintaining model utility.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，一种新的方法——提示调优已经被引入（Ozdayi 等人，[2023](#bib.bib69)）。该技术通过用户指定的超参数来定制隐私-效用权衡，有效地调节了记忆内容提取的速率。这种策略确保了在维护模型效用的同时，隐私得到保护。
- en: 5.2.3\. Defense on Inference Attack
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 推理攻击防御
- en: (Shejwalkar and Houmansadr, [2021](#bib.bib82)) introduces Distillation for
    Membership Privacy (DMP), a novel strategy against inference attacks that employs
    knowledge distillation to enhance privacy in machine learning models. DMP not
    only preserves but also enhances the utility of the resulting models. This approach
    has been shown to significantly improve privacy protection while maintaining robust
    model performance.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: （Shejwalkar 和 Houmansadr，[2021](#bib.bib82)）提出了会员隐私蒸馏（DMP），这是一种针对推理攻击的新策略，利用知识蒸馏来增强机器学习模型的隐私。DMP不仅保留了模型的效用，还进一步提升了效用。这种方法被证明在保持强大模型性能的同时显著提高了隐私保护。
- en: (Tong et al., [2024](#bib.bib88)) presents InferDPT, a novel framework designed
    for privacy-preserving inference that integrates differential privacy into text
    generation with black-box LLMs. InferDPT features a perturbation module that utilizes
    RANTEXT, a differentially private mechanism developed for text perturbation, alongside
    an extraction module that ensures the coherence and consistency of the generated
    text. This framework effectively enhances user privacy protection.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: (Tong 等人，[2024](#bib.bib88)) 提出了 InferDPT，这是一个旨在隐私保护推理的新框架，它将差分隐私集成到与黑箱 LLM
    生成文本中。InferDPT 具有一个扰动模块，利用 RANTEXT，这是一种为文本扰动开发的差分隐私机制，以及一个提取模块，确保生成文本的一致性和连贯性。该框架有效增强了用户隐私保护。
- en: (Yu et al., [2021](#bib.bib125)) proposes a meta-framework for private deep
    learning that captures key principles from recent fine-tuning methods to enhance
    privacy without compromising performance. It introduces an efficient, sparse algorithm
    for the differentially private fine-tuning of large-scale pre-trained language
    models, ensuring high utility with robust privacy protections.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: (Yu 等人，[2021](#bib.bib125)) 提出了一个私有深度学习的元框架，该框架从近期的微调方法中提取关键原则，以在不妨碍性能的情况下增强隐私保护。它引入了一种高效、稀疏的算法，用于大规模预训练语言模型的差分隐私微调，确保高效用性并提供强有力的隐私保护。
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.1.3\. Defense on Misunderstanding ‣ 5.1\. Mitigating
    Technical Vulnerabilities ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged
    Security and Privacy of LLM Agent: A Survey with Case Studies") presents a summary
    of defensive strategies for malicious attacks, offering a concise overview for
    quick reference.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S5.T2 "表 2 ‣ 5.1.3\. 对误解的防御 ‣ 5.1\. 减轻技术漏洞 ‣ 5\. 针对威胁的防御策略 ‣ LLM 代理的新兴安全与隐私：案例研究调查")
    提供了针对恶意攻击的防御策略汇总，为快速参考提供了简明概述。
- en: Table 3\. Summary of Defensive Strategies Against Specific Threats
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 针对特定威胁的防御策略汇总
- en: '| Threats | Method Name | Key Mechanism | Advantages / Limitations |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 威胁 | 方法名称 | 关键机制 | 优势 / 局限性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Knowledge Poisoning | Provenance-Based Poison Detection (Baracaldo et al.,
    [2017](#bib.bib12)) | Utilizes data provenance to detect and filter poisonous
    data in training sets | Enables use of online and regularly re-trained models;
    Supports both partially trusted and fully untrusted datasets |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 知识中毒 | 基于溯源的毒性检测 (Baracaldo 等人，[2017](#bib.bib12)) | 利用数据溯源检测和过滤训练集中的毒性数据
    | 支持在线和定期再训练的模型；支持部分可信和完全不可信的数据集 |'
- en: '| ParaFuzz (Yan et al., [2023](#bib.bib114)) | Uses interpretability of model
    predictions to detect poisoned samples, employing fuzzing for precise paraphrase
    prompts | Effectively detects poisoned samples; Excels against covert attacks
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| ParaFuzz (Yan 等人，[2023](#bib.bib114)) | 利用模型预测的可解释性检测中毒样本，采用模糊测试生成精确的同义词提示
    | 有效检测中毒样本；在隐蔽攻击中表现优异 |'
- en: '| Data Filtering & Reducing Effective Model Capacity (Wan et al., [2023b](#bib.bib93))
    | Utilizes data filtering to remove high-loss examples and reduces model capacity
    to hinder learning from poison data | Reduces poisoning effectiveness / Demands
    trade-offs between performance and safety |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 数据过滤与降低有效模型容量 (Wan 等人，[2023b](#bib.bib93)) | 利用数据过滤去除高损失样本，并减少模型容量以阻碍对中毒数据的学习
    | 降低了中毒效果 / 需要在性能和安全性之间进行权衡 |'
- en: '| Functional Manipulation | ToolEmu (Ruan et al., [2024](#bib.bib79)) | Utilizes
    a LM to simulate tool execution and assess agent risks through an automatic evaluator
    | Offers flexibility and dynamic testing capabilities / Emulators may overlook
    essential constraints |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 功能操控 | ToolEmu (Ruan 等人，[2024](#bib.bib79)) | 利用语言模型模拟工具执行，并通过自动评估器评估代理风险
    | 提供灵活性和动态测试能力 / 模拟器可能忽略关键约束 |'
- en: '| Safety Standards (Anderljung et al., [2023](#bib.bib10)) | Proposes pre-deployment
    risk assessments, external reviews, informed deployment decisions, monitoring
    post-deployment | Balances safety risks with innovation benefits |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 安全标准 (Anderljung 等人，[2023](#bib.bib10)) | 提出部署前风险评估、外部审查、知情部署决策、部署后监控 | 在安全风险和创新收益之间取得平衡
    |'
- en: '| Output Manipulation | BERTective (Fornaciari et al., [2021](#bib.bib28))
    | Enhances BERT with additional attention layers to detect deception in Italian
    dialogues | Enhances deception detection accuracy Limited effectiveness of broader
    context |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 输出操控 | BERTective (Fornaciari 等人，[2021](#bib.bib28)) | 通过增加额外的注意力层来增强 BERT，以检测意大利对话中的欺骗
    | 提高了欺骗检测的准确性；对广泛上下文的有效性有限 |'
- en: '| ReCon (Wang et al., [2023b](#bib.bib99)) | Employs formulation and refinement
    processes with perspective transitions to understand mental states | Enhances
    ability to discern and counteract deception |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| ReCon (Wang et al., [2023b](#bib.bib99)) | 采用公式化和精炼过程，通过视角转换理解心理状态 | 提高识别和反制欺骗的能力
    |'
- en: '| MAgIC (Xu et al., [2023a](#bib.bib111)) | Uses games and game theory, combined
    with PGM, to evaluate LLM agents | Enhances ability to navigate complex social
    and cognitive dimensions |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| MAgIC (Xu et al., [2023a](#bib.bib111)) | 结合游戏及博弈论与PGM来评估LLM代理 | 提高在复杂社会和认知维度中的导航能力
    |'
- en: 5.3\. Mitigating Specific Threats
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 缓解特定威胁
- en: 5.3.1\. Defense on Knowledge Poisoning
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1. 知识污染防御
- en: (Baracaldo et al., [2017](#bib.bib12)) proposes a new method for detecting and
    filtering poisonous data in the training sets of supervised learning models. It
    specifically utilizes data provenance to identify groups of data with a high correlation
    in their likelihood of being poisoned. This innovative approach aids in the effective
    identification and removal of malicious data.  (Yan et al., [2023](#bib.bib114))
    presents ParaFuzz, a novel framework for detecting poisoned samples at test time
    in large language models (LLMs), leveraging the interpretability of model predictions.
    The effectiveness of PARAFUZZ heavily depends on the specific prompts used with
    ChatGPT, which is employed to ensure high-quality paraphrasing. To optimize the
    detection process, the study adopts fuzzing to develop precise paraphrase prompts.
    These prompts are designed to effectively neutralize backdoor triggers while preserving
    the semantic integrity of the text.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: (Baracaldo et al., [2017](#bib.bib12)) 提出了一个新方法，用于检测和过滤监督学习模型训练集中的有毒数据。该方法特别利用数据来源来识别具有较高被污染可能性的相关数据组。这种创新方法有助于有效识别和去除恶意数据。（Yan
    et al., [2023](#bib.bib114)）提出了ParaFuzz，这是一个新颖的框架，用于在大语言模型（LLMs）的测试阶段检测有毒样本，利用模型预测的可解释性。PARAFUZZ的有效性很大程度上依赖于与ChatGPT使用的具体提示，该提示用于确保高质量的同义句改写。为了优化检测过程，研究采用模糊测试来开发精确的同义句提示。这些提示旨在有效中和后门触发器，同时保持文本的语义完整性。
- en: There is still a significant gap in research focused on developing efficient
    defense strategies to protect LLMs from knowledge poisoning attacks (Das et al.,
    [2024](#bib.bib18)). Furthermore, empirical evidence indicates that LLMs are increasingly
    susceptible to these attacks. Current defense mechanisms, such as filtering data
    or reducing model capacity, provide only limited protection and often result in
    decreased test accuracy (Wan et al., [2023b](#bib.bib93)).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在开发高效防御策略以保护LLM免受知识污染攻击的研究仍存在显著差距（Das et al., [2024](#bib.bib18)）。此外，实证证据表明LLM越来越容易受到这些攻击。目前的防御机制，如数据过滤或减少模型容量，只提供了有限的保护，且常常导致测试准确性下降（Wan
    et al., [2023b](#bib.bib93)）。
- en: Besides technical solutions, specialized security strategies for AI systems
    are crucial, including verifying model sources, limiting sensitive training data,
    and detecting and mitigating attacks. Regular security reviews and risk assessments
    should also be conducted to identify and address new threats, ensuring AI systems
    are secure and up-to-date (Dilmaghani et al., [2019](#bib.bib22)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术解决方案外，针对AI系统的专门安全策略也至关重要，包括验证模型来源、限制敏感训练数据，以及检测和减轻攻击。还应定期进行安全审查和风险评估，以识别和应对新威胁，确保AI系统安全且保持最新（Dilmaghani
    et al., [2019](#bib.bib22)）。
- en: 5.3.2\. Defense on Functional Manipulation
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2. 功能操控防御
- en: Given the emergence of Functional Manipulation as a new risk associated with
    the deployment of LLM agents, research on this specific threat remains limited.
    Thus, proactive security measures are essential. When using third-party LLM agents,
    it is crucial to protect personal privacy and be wary of excessive personal data
    requests by third parties. Users should limit data sharing, especially avoiding
    sensitive or personally identifiable information during interactions with LLM
    agents. Additionally, understanding and utilizing the data protection settings
    offered by LLM agents is vital. Adjusting privacy settings helps control what
    data can be collected and processed. Choosing providers with a strong reputation
    and transparency is also recommended, as these providers should have clear data
    usage and privacy protection policies along with a robust security track record (Zhang
    et al., [2024a](#bib.bib132)).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于功能操控作为部署LLM代理的新风险的出现，对这一特定威胁的研究仍然有限。因此，积极的安全措施至关重要。在使用第三方LLM代理时，必须保护个人隐私，并警惕第三方过度索取个人数据。用户应限制数据共享，特别是在与LLM代理交互时避免敏感或个人可识别信息。此外，理解和利用LLM代理提供的数据保护设置也至关重要。调整隐私设置有助于控制哪些数据可以被收集和处理。选择具有良好声誉和透明度的提供商也是推荐的，因为这些提供商应具有明确的数据使用和隐私保护政策以及强大的安全记录（Zhang
    等人，[2024a](#bib.bib132)）。
- en: Furthermore, to address the challenges posed by Functional Manipulation, the
    introduction of the ToolEmu (Ruan et al., [2024](#bib.bib79)) framework represents
    a significant advancement. This framework employs a language model to emulate
    tool execution, which allows for extensive and scalable testing of LM agents across
    diverse scenarios and toolsets. Coupled with an LM-based automatic safety evaluator,
    ToolEmu facilitates the identification and quantification of risks by examining
    potential failures and subsequent consequences. This method provides a dynamic
    alternative to traditional static sandbox evaluations, enhancing the ability to
    detect and mitigate high-stakes, long-tail risks effectively.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了解决功能操控带来的挑战，ToolEmu（Ruan 等人，[2024](#bib.bib79)）框架的引入代表了一个重要的进展。该框架采用语言模型来模拟工具执行，从而允许对LM代理在各种场景和工具集中的广泛和可扩展测试。结合基于LM的自动安全评估器，ToolEmu通过检查潜在的失败和随后的后果，促进了风险的识别和量化。这种方法提供了一种动态替代传统静态沙箱评估的方式，提高了有效检测和缓解高风险、长尾风险的能力。
- en: Additionally,  (Anderljung et al., [2023](#bib.bib10)) proposes an initial set
    of safety standards as an essential first step in industry self-regulation. These
    standards include pre-deployment risk assessments, external reviews of model behavior,
    the use of risk assessments to inform deployment decisions, and monitoring and
    responding to new information about model functionality post-deployment. This
    approach contributes valuable insights to the broader discussion on balancing
    public safety risks with the benefits of innovation in AI development.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，（Anderljung 等人，[2023](#bib.bib10)）提出了一套初步的安全标准，作为行业自我监管的关键第一步。这些标准包括部署前风险评估、模型行为的外部审查、使用风险评估来指导部署决策，以及在部署后监控和回应有关模型功能的新信息。这种方法为在人工智能发展中平衡公众安全风险与创新益处的更广泛讨论提供了宝贵的见解。
- en: 5.3.3\. Defense on Output Manipulation
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 输出操控的防御
- en: To prevent individual LLM agents from being deceived by other agents, it is
    advisable to enhance their detection capabilities to determine whether they have
    encountered deception.  (Fornaciari et al., [2021](#bib.bib28))investigates using
    BERT with some added attention layers to detect deception in text, particularly
    in the context of Italian dialogues. This study establishes new methods for identifying
    deception and discusses how various contexts and semantic information contribute
    to detecting deceptive content.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止个别LLM代理被其他代理欺骗，建议增强其检测能力，以确定是否遇到欺骗。（Fornaciari 等人，[2021](#bib.bib28)）研究了使用BERT和一些附加注意力层来检测文本中的欺骗，特别是在意大利语对话的背景下。该研究建立了识别欺骗的新方法，并讨论了各种上下文和语义信息如何有助于检测欺骗内容。
- en: Inspired by human recursive thinking in the Avalon game,  (Wang et al., [2023b](#bib.bib99))
    introduces Recursive Contemplation (ReCon), a framework designed to enhance LLMs’
    ability to detect and counter deceptive information. ReCon employs formulation,
    which generates initial thoughts and speech, and refinement, which improves these
    outputs. It also includes two perspective transitions, aiding LLMs in understanding
    others’ mental states and how others perceive their own mental states.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类在《亚瑟王游戏》中递归思维的启发，(Wang et al., [2023b](#bib.bib99)) 引入了递归思考（ReCon），这是一个旨在增强LLM检测和反击虚假信息能力的框架。ReCon采用了生成初步思考和言语的生成，以及改进这些输出的优化。它还包括两个视角转换，帮助LLM理解他人的心理状态以及他人如何看待自己的心理状态。
- en: Additionally,  (Xu et al., [2023a](#bib.bib111)) has developed a benchmarking
    framework called MAgIC, designed to evaluate LLMs in multi-agent environments.
    It utilizes games and game theory scenarios to test models on reasoning, cooperation,
    and adaptability. The research employs Probabilistic Graphical Modeling (PGM)
    to enhance models’ capabilities in handling complex social interactions.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，(Xu et al., [2023a](#bib.bib111)) 开发了一个名为MAgIC的基准测试框架，旨在评估多代理环境中的LLM。它利用游戏和博弈论场景来测试模型的推理、合作和适应能力。该研究采用了概率图模型（PGM）以增强模型处理复杂社会互动的能力。
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.2.3\. Defense on Inference Attack ‣ 5.2\. Mitigating
    Malicious Attacks ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies") presents an overview of
    methods to mitigate specific threats, serving as a comprehensive guide for understanding
    effective defenses.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[3](#S5.T3 "Table 3 ‣ 5.2.3\. Defense on Inference Attack ‣ 5.2\. Mitigating
    Malicious Attacks ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies")概述了减轻特定威胁的方法，作为理解有效防御的全面指南。'
- en: 6\. Future Trends and Discussion
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 未来趋势与讨论
- en: With the continuous advancements in LLM agents, these agents have become capable
    of effectively interacting with users through complex observations, reasoning,
    and task execution, demonstrating broad application prospects across multiple
    domains. Particularly with the development of Multimodal Large Language Model
    (MLLM) agents, LLM agents can now process various data types, including text,
    images, and audio, significantly expanding their application scope. Moreover,
    by incorporating Large Language Model Multi-Agent (LLM-MA) systems, different
    LLM agents can collaborate to accomplish more complex tasks. The integration of
    these technologies will contribute to building more intelligent and efficient
    systems. However, the widespread application of these advanced technologies also
    introduces significant challenges related to privacy and security. Through discussions
    on future trends, our aim is to provide insights for researchers, developers,
    and policymakers on how to optimize these technologies and overcome related challenges.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM代理的持续进步，这些代理已经能够通过复杂的观察、推理和任务执行与用户有效互动，展示了在多个领域广泛的应用前景。特别是随着多模态大语言模型（MLLM）代理的发展，LLM代理现在可以处理包括文本、图像和音频在内的各种数据类型，大大扩展了它们的应用范围。此外，通过引入大语言模型多代理（LLM-MA）系统，不同的LLM代理可以协作完成更复杂的任务。这些技术的整合将有助于构建更智能和高效的系统。然而，这些先进技术的广泛应用也带来了与隐私和安全相关的重大挑战。通过对未来趋势的讨论，我们旨在为研究人员、开发者和政策制定者提供如何优化这些技术并克服相关挑战的见解。
- en: 6.1\. Multimodal Large Language Model Agent
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 多模态大语言模型代理
- en: 6.1.1\. The Development of MLLM Agent
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. MLLM代理的发展
- en: 'Recent advancements in LLMs have significantly surpassed traditional boundaries
    of language processing. These models now incorporate supplementary components
    such as instruction, interface, tools, knowledge, and memory, evolving into intelligent
    LLM agents that demonstrate expanded reasoning and expertise. Research studies
     (Yang et al., [2023a](#bib.bib118); Wu et al., [2023](#bib.bib107)) indicate
    efforts to bridge the gap between language models and multimodal tools, with intelligent
    agents like Visual ChatGPT  (Wu et al., [2023](#bib.bib107)) and MMREACT  (Yang
    et al., [2023a](#bib.bib118)) employing sophisticated prompt engineering techniques
    to achieve this target. Such efforts have given rise to the field of Multimodal
    Large Language Models (MLLMs). The general architecture of the MLLM is depicted
    in Figure [12](#S6.F12 "Figure 12 ‣ 6.1.1\. The Development of MLLM Agent ‣ 6.1\.
    Multimodal Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The
    Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，LLM 的发展显著超越了传统的语言处理界限。这些模型现在整合了指令、接口、工具、知识和记忆等附加组件，发展成智能 LLM 代理，展示了扩展的推理和专业知识。研究表明
    (Yang et al., [2023a](#bib.bib118); Wu et al., [2023](#bib.bib107))，努力弥合语言模型与多模态工具之间的差距，智能代理如
    Visual ChatGPT (Wu et al., [2023](#bib.bib107)) 和 MMREACT (Yang et al., [2023a](#bib.bib118))
    采用了复杂的提示工程技术以实现这一目标。这些努力催生了多模态大语言模型（MLLM）的领域。MLLM 的总体架构如图 [12](#S6.F12 "图 12 ‣
    6.1.1\. MLLM 代理的发展 ‣ 6.1\. 多模态大语言模型代理 ‣ 6\. 未来趋势与讨论 ‣ LLM 代理的新兴安全与隐私：案例研究调查")
    所示。
- en: '![Refer to caption](img/680bf46857fd72a7e9701df7f8237bce.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/680bf46857fd72a7e9701df7f8237bce.png)'
- en: Figure 12\. The general architecture of the MLLM
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. MLLM 的总体架构
- en: MLLMs are based on LLMs and enhanced with the capability to receive, reason,
    and output multimodal information. By integrating various data modalities, such
    as text, image, audio and video, these models are not only capable of understanding
    information from a single modality but can also process and interpret across modalities,
    thus achieving a comprehensive understanding of complex information (Yin et al.,
    [2023a](#bib.bib123)). The application of MLLMs has extended to several other
    fields, including medical image analysis (Zhang et al., [2023a](#bib.bib131);
    Moor et al., [2023](#bib.bib67)) and document processing (Hu et al., [2024b](#bib.bib36);
    Liu et al., [2024c](#bib.bib60)).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: MLLM 基于 LLM，并增强了接收、推理和输出多模态信息的能力。通过整合各种数据模态，如文本、图像、音频和视频，这些模型不仅能理解单一模态的信息，还能跨模态处理和解释，从而实现对复杂信息的全面理解
    (Yin et al., [2023a](#bib.bib123))。MLLM 的应用已扩展到多个领域，包括医学图像分析 (Zhang et al., [2023a](#bib.bib131);
    Moor et al., [2023](#bib.bib67)) 和文档处理 (Hu et al., [2024b](#bib.bib36); Liu et
    al., [2024c](#bib.bib60))。
- en: Moreover, the development of multimodal agents based on MLLMs, such as embodied
    agents (Huang et al., [2024c](#bib.bib39)) and graphical user interface agents (Wang
    et al., [2024b](#bib.bib95)), has further enhanced these models’ interactive capabilities
    in physical environments. These agents, utilizing MLLMs as planners and following
    natural language instructions to navigate and interact effectively in the real
    world, are not only designed to understand and generate information but are also
    equipped with essential skills such as perception, reasoning, planning, and execution.
    This enables them to operate effectively in complex real-world environments  (Xie
    et al., [2024](#bib.bib109)).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于 MLLM 的多模态代理的发展，如具身代理 (Huang et al., [2024c](#bib.bib39)) 和图形用户界面代理 (Wang
    et al., [2024b](#bib.bib95))，进一步增强了这些模型在物理环境中的互动能力。这些代理利用 MLLM 作为规划者，并根据自然语言指令在现实世界中有效导航和互动，不仅设计用于理解和生成信息，还具备感知、推理、规划和执行等必要技能。这使得它们能够在复杂的现实世界环境中有效运作
    (Xie et al., [2024](#bib.bib109))。
- en: With the advent of MLLM agents, the potential for achieving Artificial General
    Intelligence (AGI) has become more feasible, leading to significant advancements
    in Embodied AI. The ability of agent robots to understand and respond to human
    commands is crucial, especially in service-oriented tasks. The substantial progress
    in MLLMs has equipped them with the ability to comprehend and generate natural
    human instructions effectively. This progress could enable robots to learn user
    preferences and provide services that closely mimic human interaction.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 随着MLLM代理的出现，实现人工通用智能（AGI）的潜力变得更加可行，带来了在具身AI方面的重大进展。代理机器人理解和响应人类指令的能力至关重要，特别是在面向服务的任务中。MLLM的显著进展使它们能够有效地理解和生成自然的人类指令。这一进展可能使机器人能够学习用户偏好，并提供与人类互动紧密模拟的服务。
- en: 6.1.2\. The Security and Privacy Research on MLLM Agent
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. MLLM代理的安全性和隐私研究
- en: The development of embodied agents capable of interacting with the real world
    becomes a highly active area of research. However, MLLM agents also present several
    security vulnerabilities, one of which is the phenomenon of multimodal hallucinations.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 具身代理的发展，能够与现实世界互动，成为一个高度活跃的研究领域。然而，MLLM代理也存在一些安全漏洞，其中之一就是多模态幻觉现象。
- en: '![Refer to caption](img/fe35850ddd4a4c800a76341505b1831b.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/fe35850ddd4a4c800a76341505b1831b.png)'
- en: Figure 13\. Illustration of multimodal hallucinations . Given an image, an MLLM
    agent outputs a corresponding response with two primary forms
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图13\. 多模态幻觉的示意图。给定一张图像，MLLM代理输出对应的响应，主要有两种形式
- en: 'Unlike language hallucinations, multimodal hallucinations refer to the phenomenon
    where the output descriptions generated by MLLMs are inconsistent with the actual
    content of images  (Yin et al., [2023b](#bib.bib124)), as shown in Figure [13](#S6.F13
    "Figure 13 ‣ 6.1.2\. The Security and Privacy Research on MLLM Agent ‣ 6.1\. Multimodal
    Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies"). These phenomena manifest
    in two primary forms (Lee et al., [2024](#bib.bib50)): one involves generated
    content that includes objects which are inconsistent with or absent from the target
    image  (Zhai et al., [2024](#bib.bib127); Liu et al., [2024a](#bib.bib57)); the
    other, a more complex form, encompasses holistic misrepresentations of entire
    scenes or environments (Sun et al., [2023](#bib.bib84)).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '与语言幻觉不同，多模态幻觉指的是MLLM生成的输出描述与实际图像内容不一致的现象（Yin et al., [2023b](#bib.bib124)），如图[13](#S6.F13
    "Figure 13 ‣ 6.1.2\. The Security and Privacy Research on MLLM Agent ‣ 6.1\. Multimodal
    Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The Emerged Security
    and Privacy of LLM Agent: A Survey with Case Studies")所示。这些现象主要表现为两种形式（Lee et
    al., [2024](#bib.bib50)）：一种涉及生成内容中包含与目标图像不一致或缺失的物体（Zhai et al., [2024](#bib.bib127);
    Liu et al., [2024a](#bib.bib57)）；另一种更复杂的形式包括对整个场景或环境的全面误表述（Sun et al., [2023](#bib.bib84)）。'
- en: Current methods to reduce these hallucinations encompass several approaches,
    such as utilizing self-feedback with visual cues to enhance model accuracy  (Lee
    et al., [2024](#bib.bib50)), employing instruction-tuning techniques to refine
    the model’s response to respond to human instructions  (Liu et al., [2024a](#bib.bib57)).
    implementing error-correction processes that identify and rectify hallucinations
    within the generated text  (Yin et al., [2023b](#bib.bib124)). Despite these efforts,
    significant challenges remain, requiring a sophisticated ability to distinguish
    between accurate and hallucinatory outputs, along with improvements in training
    approaches to boost the reliability of the outputs.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 目前减少这些幻觉的方法包括几种方法，例如利用带有视觉提示的自反馈来提高模型准确性（Lee et al., [2024](#bib.bib50)），使用指令调优技术来优化模型对人类指令的响应（Liu
    et al., [2024a](#bib.bib57)），以及实施错误纠正过程来识别和修正生成文本中的幻觉（Yin et al., [2023b](#bib.bib124)）。尽管做出了这些努力，仍然存在显著挑战，需要更复杂的能力来区分准确输出和幻觉输出，以及改进训练方法以提高输出的可靠性。
- en: Similar to LLM agents, MLLM agents can be susceptible to crafted attacks (Qi
    et al., [2023](#bib.bib76); Bagdasaryan et al., [2023](#bib.bib11); Shayegani
    et al., [2023](#bib.bib81)). These agents may be maliciously manipulated to produce
    biased or undesirable responses. However, research in this area is still in its
    early stages. Therefore, enhancing the safety of these MLLM agents is an essential
    focus of ongoing research. Improvements in MLLM agents safety will involve developing
    robust mechanisms to detect and mitigate these vulnerabilities, ensuring that
    MLLM agents can function reliably and securely in diverse applications. Such advancements
    are crucial for the broader adoption and ethical deployment of AI technologies
    in real-world environment.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LLM代理，MLLM代理可能容易受到精心设计的攻击（Qi et al., [2023](#bib.bib76); Bagdasaryan et al.,
    [2023](#bib.bib11); Shayegani et al., [2023](#bib.bib81)）。这些代理可能会被恶意操控，产生偏见或不良响应。然而，该领域的研究仍处于早期阶段。因此，提高这些MLLM代理的安全性是当前研究的重要重点。提高MLLM代理安全性将涉及开发强大的机制以检测和缓解这些漏洞，确保MLLM代理在各种应用中可以可靠、安全地运行。这些进展对于AI技术在实际环境中的广泛应用和伦理部署至关重要。
- en: 6.2\. Large Language Model Multi-Agent System
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 大型语言模型多代理系统
- en: 6.2.1\. The Development of LLM-MA System
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. LLM-MA系统的发展
- en: LLM agents exhibit advanced reasoning and planning capabilities, approaching
    human-like levels of decision-making and interaction. These agents are adept at
    perceiving their environments, making informed decisions, and executing actions
    based on complex contexts (Yao et al., [2024](#bib.bib119)).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理展示了先进的推理和规划能力，接近人类水平的决策和互动。这些代理擅长感知环境、做出明智的决策，并根据复杂的上下文执行操作（Yao et al.,
    [2024](#bib.bib119)）。
- en: 'Inspired by the impressive abilities of a single LLM agent, LLM Multi-Agent
    systems have been proposed (see Figure [14](#S6.F14 "Figure 14 ‣ 6.2.1\. The Development
    of LLM-MA System ‣ 6.2\. Large Language Model Multi-Agent System ‣ 6\. Future
    Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey
    with Case Studies")). Such systems work based on several agents having collective
    intelligence and specialized skills, in which case each one is specialized to
    outperform in a specific domain. This specialization allows for a distributed
    approach to problem-solving, where each agent contributes its unique expertise,
    enhancing the overall effectiveness and efficiency of the system. In this scenario,
    multiple autonomous agents work together in planning, discussion, and decision-making,
    closely resembling human group collaboration in solving tasks. This approach leverages
    the communication abilities of LLMs, using their text generation for interaction
    and response to text inputs (Guo et al., [2024](#bib.bib31)).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '受到单一LLM代理的卓越能力的启发，提出了LLM多代理系统（参见图[14](#S6.F14 "Figure 14 ‣ 6.2.1\. The Development
    of LLM-MA System ‣ 6.2\. Large Language Model Multi-Agent System ‣ 6\. Future
    Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey
    with Case Studies")）。这些系统基于多个具有集体智能和专业技能的代理，每个代理在特定领域中表现优异。这种专业化使得问题解决可以采用分布式的方法，每个代理贡献其独特的专业知识，从而提高系统的整体效能和效率。在这种情况下，多个自主代理在规划、讨论和决策过程中协作，类似于人类团队在解决任务时的协作方式。这种方法利用了LLM的沟通能力，通过文本生成进行互动和响应文本输入（Guo
    et al., [2024](#bib.bib31)）。'
- en: '![Refer to caption](img/7d6b5f142a7ea99227ee781c090d44fa.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7d6b5f142a7ea99227ee781c090d44fa.png)'
- en: Figure 14\. The Architecture of LLM-MA Systems
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图14. LLM-MA系统的架构
- en: 'The application of LLM-MA systems spans across various fields, broadly categorized
    into two main types: problem solving and world simulation (Guo et al., [2024](#bib.bib31)).
    For problem-solving applications, such as multi-robot systems (Mandi et al., [2023](#bib.bib64))
    and software development (Du et al., [2024](#bib.bib24)), these systems enable
    interactions among diverse agents. This collaborative capability effectively solves
    complex real-world problems, mirroring the cooperative nature of human group work
    in tackling multifaceted challenges. On the other hand, world simulation encompasses
    applications such as society simulations (Park et al., [2023b](#bib.bib71)) and
    game simulation (Xu et al., [2024](#bib.bib113)). The case studies parts presented
    in this paper illustrates the application of world simulation to depict the threats
    faced by LLM agents and their impacts, presenting one of the many facets of LLM-MA
    systems utilization.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-MA系统的应用范围涵盖多个领域，广泛分为两大类：问题解决和世界模拟 (Guo et al., [2024](#bib.bib31))。对于问题解决应用，例如多机器人系统 (Mandi
    et al., [2023](#bib.bib64))和软件开发 (Du et al., [2024](#bib.bib24))，这些系统使得不同智能体之间能够进行互动。这种协作能力有效地解决了复杂的现实世界问题，类似于人类小组在应对复杂挑战时的合作性质。另一方面，世界模拟包括社会模拟 (Park
    et al., [2023b](#bib.bib71))和游戏模拟 (Xu et al., [2024](#bib.bib113))等应用。本文所呈现的案例研究部分展示了世界模拟在描绘LLM智能体面临的威胁及其影响方面的应用，展现了LLM-MA系统利用的众多方面之一。
- en: 6.2.2\. The Security and Privacy Research on LLM-MA System
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. LLM-MA系统的安全性和隐私研究
- en: As research on LLM-MA system increases rapidly, numerous challenges have emerged.
    Each agent in a multi-agent system may need to access and process sensitive data,
    and even execute code. This has sparked discussions on the security and privacy
    concerns related to multi-agent systems.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对LLM-MA系统研究的快速增加，众多挑战也随之出现。在多智能体系统中，每个智能体可能需要访问和处理敏感数据，甚至执行代码。这引发了关于多智能体系统安全性和隐私问题的讨论。
- en: Each agent within a multi-agent system may need to access and process sensitive
    data, and even execute code. Moreover, due to the intercommunication and interconnection
    between agents, security issues originating from a single agent can have profound
    and amplified effects in a multi-agent scenario. This has intensified the need
    for focused discussions on security and privacy issues in multi-agent environments.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体系统中，每个智能体可能需要访问和处理敏感数据，甚至执行代码。此外，由于智能体之间的互通信和互联互通，来自单个智能体的安全问题在多智能体情境中可能会产生深远而放大的影响。这加剧了对多智能体环境中安全性和隐私问题的关注。
- en: The issue of hallucination, where agents generate outputs based on incorrect
    or fabricated information, represents a significant challenge for both LLMs and
    LLM Agents. This problem becomes even more complex in a multi-agent context due
    to the interconnected nature of these agents and their frequent communication.
    Misinformation from one agent can be accepted and further propagated by others
    within the network, leading to a cascade of misinformation. To mitigate this issue,
    it is crucial to correct errors at the individual agent level and also to manage
    the flow of information between agents, thereby preventing the spread of inaccurate
    information throughout the entire system (Guo et al., [2024](#bib.bib31)).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉问题，即智能体基于不正确或虚构的信息生成输出，代表了LLMs和LLM Agents面临的重大挑战。在多智能体环境中，由于这些智能体的互联互通和频繁通信，这个问题变得更加复杂。一个智能体的错误信息可能被网络中的其他智能体接受并进一步传播，导致虚假信息的级联传播。为了减轻这个问题，关键在于在个体智能体层面纠正错误，同时管理智能体之间的信息流，从而防止不准确信息在整个系统中的传播 (Guo
    et al., [2024](#bib.bib31))。
- en: Furthermore, the capability of LLM multi-agent systems to interact with files
    and execute code offers extensive possibilities for their application. However,
    the presence of potentially malicious LLM agents within the system poses significant
    risks. In one case, these agents may operate in a passive listening mode, where
    they receive information shared by other agents to perform tasks, but at the same
    time, they leak confidential information to attackers deliberately. In the other
    case, malicious LLM agents may engage in a active communication mode, spreading
    virus-infected files, phishing messages, or other malicious codes, attempting
    to attack or disrupt other agents within the system. To mitigate this risk, incorporating
    human feedback and user authorization for each step can help reduce these threats.
    This necessitates designing the system with robust security measures to prevent
    unauthorized access or misuse. One effective approach is the implementation of
    a state-less oracle agent, which can monitor each sensitive task and assesses
    whether it constitutes a malicious activity (Talebirad and Nadiri, [2023](#bib.bib85)).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM多代理系统与文件互动和执行代码的能力为其应用提供了广泛的可能性。然而，系统中存在潜在恶意的LLM代理带来了重大风险。在一种情况下，这些代理可能处于被动监听模式，接收其他代理共享的信息来执行任务，但同时故意向攻击者泄露机密信息。在另一种情况下，恶意LLM代理可能以主动通信模式进行操作，传播感染病毒的文件、网络钓鱼消息或其他恶意代码，试图攻击或破坏系统内的其他代理。为了减轻这一风险，纳入人类反馈和每一步的用户授权可以帮助减少这些威胁。这需要设计具有强大安全措施的系统，以防止未经授权的访问或滥用。一种有效的方法是实施无状态的神谕代理，它可以监控每个敏感任务并评估其是否构成恶意活动（Talebirad
    和 Nadiri，[2023](#bib.bib85)）。
- en: Currently, research on privacy and security in LLM-MA systems has not received
    widespread attention. However, with the rapid development of LLM-MA technology,
    these issues are becoming increasingly prominent. Therefore, there is an urgent
    need for robust security solutions to mitigate these emerging challenges.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，LLM-MA系统中的隐私和安全研究尚未受到广泛关注。然而，随着LLM-MA技术的快速发展，这些问题正变得越来越突出。因此，迫切需要强有力的安全解决方案来缓解这些新出现的挑战。
- en: 7\. Conclusion
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: 'In this survey, we have explored the multifaceted security and privacy challenges
    faced by LLM agents, including the two categories of the sources of threats: inherited
    threats from LLM and specific threats on agents. Also, we present the security
    and privacy impacts on humans, environment, and other agents. Based on those,
    we discuss the corresponding defensive strategies. Additionally, we have discussed
    future trends in this field. To facilitate an in-depth understanding, we have
    incorporated a variety of case studies via a virtual town project. By highlighting
    the challenges that LLM agents encounter, we aim to inspire further research and
    exploration by researchers and developers in enhancing the security and privacy
    of LLM agents in the future.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们探讨了LLM代理面临的多方面安全和隐私挑战，包括威胁来源的两个类别：从LLM继承的威胁和针对代理的特定威胁。此外，我们还展示了对人类、环境和其他代理的安全和隐私影响。基于这些，我们讨论了相应的防御策略。此外，我们还讨论了该领域的未来趋势。为了促进深入理解，我们通过虚拟城镇项目纳入了各种案例研究。通过突出LLM代理面临的挑战，我们旨在激励研究人员和开发者进一步研究和探索，以增强未来LLM代理的安全性和隐私性。
- en: References
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Cha (2022) 2022. *ChatGPT*. [https://openai.com/chatgpt](https://openai.com/chatgpt)
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cha (2022) 2022. *ChatGPT*. [https://openai.com/chatgpt](https://openai.com/chatgpt)
- en: Gem (2023) 2023. *Gemini - Chat to Supercharge Your Ideas*. [https://gemini.google.com](https://gemini.google.com)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gem (2023) 2023. *Gemini - 激发你的创意*. [https://gemini.google.com](https://gemini.google.com)
- en: 'Mal (2023) Embrace The Red 2023. *Malicious ChatGPT Agents: How GPTs Can Quietly
    Grab Your Data (Demo) · Embrace The Red*. Embrace The Red. [https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/](https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mal (2023) Embrace The Red 2023. *恶意ChatGPT代理：GPT如何悄悄获取你的数据（演示）· Embrace The
    Red*。Embrace The Red. [https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/](https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/)
- en: Wha (2023) Prompt Engineering 2023. *What Are Large Language Model (LLM) Agents
    and Autonomous Agents*. Prompt Engineering. [https://promptengineering.org/what-are-large-language-model-llm-agents/](https://promptengineering.org/what-are-large-language-model-llm-agents/)
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wha (2023) Prompt Engineering 2023. *大型语言模型（LLM）代理和自主代理是什么*。Prompt Engineering.
    [https://promptengineering.org/what-are-large-language-model-llm-agents/](https://promptengineering.org/what-are-large-language-model-llm-agents/)
- en: 'Int (2024a) 2024a. *Introducing Meta Llama 3: The Most Capable Openly Available
    LLM to Date*. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Int（2024a） 2024a. *介绍 Meta Llama 3：迄今为止最强大的公开可用 LLM*。 [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
- en: Int (2024b) 2024b. *Introducing the next Generation of Claude*. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Int（2024b） 2024b. *介绍下一代 Claude*。 [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
- en: 'Abdelnabi et al. (2023) Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea
    Schönherr, and Mario Fritz. 2023. *LLM-Deliberation: Evaluating LLMs with Interactive
    Multi-Agent Negotiation Games*. [https://doi.org/10.48550/arXiv.2309.17234](https://doi.org/10.48550/arXiv.2309.17234)
    arXiv:2309.17234'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abdelnabi 等（2023） Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr,
    和 Mario Fritz. 2023. *LLM-Deliberation: 通过互动多代理谈判游戏评估 LLMs*。 [https://doi.org/10.48550/arXiv.2309.17234](https://doi.org/10.48550/arXiv.2309.17234)
    arXiv:2309.17234'
- en: Aksitov et al. (2023) Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak
    Shakeri, and Yunhsuan Sung. 2023. *Characterizing Attribution and Fluency Tradeoffs
    for Retrieval-Augmented Large Language Models*. [https://doi.org/10.48550/arXiv.2302.05578](https://doi.org/10.48550/arXiv.2302.05578)
    arXiv:2302.05578
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aksitov 等（2023） Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri,
    和 Yunhsuan Sung. 2023. *表征检索增强型大语言模型的归因和流畅性权衡*。 [https://doi.org/10.48550/arXiv.2302.05578](https://doi.org/10.48550/arXiv.2302.05578)
    arXiv:2302.05578
- en: 'Anderljung et al. (2023) Markus Anderljung, Joslyn Barnhart, Anton Korinek,
    Jade Leung, Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin
    Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield,
    Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav
    Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf. 2023. *Frontier AI Regulation:
    Managing Emerging Risks to Public Safety*. [https://doi.org/10.48550/arXiv.2307.03718](https://doi.org/10.48550/arXiv.2307.03718)
    arXiv:2307.03718'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderljung 等（2023） Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung,
    Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock,
    Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan
    Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav
    Shavit, Divya Siddarth, Robert Trager, 和 Kevin Wolf. 2023. *边界 AI 监管：管理公共安全的新兴风险*。
    [https://doi.org/10.48550/arXiv.2307.03718](https://doi.org/10.48550/arXiv.2307.03718)
    arXiv:2307.03718
- en: Bagdasaryan et al. (2023) Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, and
    Vitaly Shmatikov. 2023. *Abusing Images and Sounds for Indirect Instruction Injection
    in Multi-Modal LLMs*. [https://doi.org/10.48550/arXiv.2307.10490](https://doi.org/10.48550/arXiv.2307.10490)
    arXiv:2307.10490 [cs]
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagdasaryan 等（2023） Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, 和 Vitaly
    Shmatikov. 2023. *滥用图像和声音进行多模态 LLM 的间接指令注入*。 [https://doi.org/10.48550/arXiv.2307.10490](https://doi.org/10.48550/arXiv.2307.10490)
    arXiv:2307.10490 [cs]
- en: 'Baracaldo et al. (2017) Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and
    Jaehoon Amir Safavi. 2017. Mitigating Poisoning Attacks on Machine Learning Models:
    A Data Provenance Based Approach. In *Proceedings of the 10th ACM Workshop on
    Artificial Intelligence and Security* *(AISec ’17)*. Association for Computing
    Machinery, New York, NY, USA, 103–110. [https://doi.org/10.1145/3128572.3140450](https://doi.org/10.1145/3128572.3140450)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baracaldo 等（2017） Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, 和 Jaehoon Amir
    Safavi. 2017. *缓解对机器学习模型的毒害攻击：基于数据来源的方法*。收录于*第十届 ACM 人工智能与安全研讨会* *(AISec ’17)*。计算机协会，美国纽约，103–110。
    [https://doi.org/10.1145/3128572.3140450](https://doi.org/10.1145/3128572.3140450)
- en: 'Bran et al. (2023) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew D. White, and Philippe Schwaller. 2023. *ChemCrow: Augmenting Large-Language
    Models with Chemistry Tools*. [https://doi.org/10.48550/arXiv.2304.05376](https://doi.org/10.48550/arXiv.2304.05376)
    arXiv:2304.05376'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bran 等（2023） Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew
    D. White, 和 Philippe Schwaller. 2023. *ChemCrow: 用化学工具增强大语言模型*。 [https://doi.org/10.48550/arXiv.2304.05376](https://doi.org/10.48550/arXiv.2304.05376)
    arXiv:2304.05376'
- en: 'Brundage et al. (2018) Miles Brundage, Shahar Avin, Jack Clark, Helen Toner,
    Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby
    Filar, Hyrum S. Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick
    Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare
    Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy,
    and Dario Amodei. 2018. The Malicious Use of Artificial Intelligence: Forecasting,
    Prevention, and Mitigation. *arXiv preprint arXiv:1802.07228* (2018). arXiv:1802.07228
    [http://arxiv.org/abs/1802.07228](http://arxiv.org/abs/1802.07228)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brundage et al. (2018) Miles Brundage、Shahar Avin、Jack Clark、Helen Toner、Peter
    Eckersley、Ben Garfinkel、Allan Dafoe、Paul Scharre、Thomas Zeitzoff、Bobby Filar、Hyrum
    S. Anderson、Heather Roff、Gregory C. Allen、Jacob Steinhardt、Carrick Flynn、Seán
    Ó hÉigeartaigh、Simon Beard、Haydn Belfield、Sebastian Farquhar、Clare Lyle、Rebecca
    Crootof、Owain Evans、Michael Page、Joanna Bryson、Roman Yampolskiy 和 Dario Amodei.
    2018. **人工智能的恶意使用：预测、预防与缓解**。*arXiv 预印本 arXiv:1802.07228* (2018)。 arXiv:1802.07228
    [http://arxiv.org/abs/1802.07228](http://arxiv.org/abs/1802.07228)
- en: Carlini et al. (2023) Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo,
    Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian
    Tramèr. 2023. *Poisoning Web-Scale Training Datasets Is Practical*. arXiv:2302.10149
    [http://arxiv.org/abs/2302.10149](http://arxiv.org/abs/2302.10149)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2023) Nicholas Carlini、Matthew Jagielski、Christopher A. Choquette-Choo、Daniel
    Paleka、Will Pearce、Hyrum Anderson、Andreas Terzis、Kurt Thomas 和 Florian Tramèr.
    2023. **毒害网络规模训练数据集是切实可行的**。 arXiv:2302.10149 [http://arxiv.org/abs/2302.10149](http://arxiv.org/abs/2302.10149)
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data
    from Large Language Models. In *30th USENIX Security Symposium (USENIX Security
    21)*. 2633–2650. [https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2021) Nicholas Carlini、Florian Tramèr、Eric Wallace、Matthew Jagielski、Ariel
    Herbert-Voss、Katherine Lee、Adam Roberts、Tom Brown、Dawn Song、Ulfar Erlingsson、Alina
    Oprea 和 Colin Raffel. 2021. **从大型语言模型中提取训练数据**。在 *第30届 USENIX 安全研讨会 (USENIX Security
    21)*。2633–2650。 [https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)
- en: 'Charan et al. (2023) P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand,
    and Sandeep K. Shukla. 2023. *From Text to MITRE Techniques: Exploring the Malicious
    Use of Large Language Models for Generating Cyber Attack Payloads*. [https://doi.org/10.48550/arXiv.2305.15336](https://doi.org/10.48550/arXiv.2305.15336)
    arXiv:2305.15336'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charan et al. (2023) P. V. Sai Charan、Hrushikesh Chunduri、P. Mohan Anand 和 Sandeep
    K. Shukla. 2023. *从文本到 MITRE 技术：探索大型语言模型生成网络攻击有效载荷的恶意用途*。 [https://doi.org/10.48550/arXiv.2305.15336](https://doi.org/10.48550/arXiv.2305.15336)
    arXiv:2305.15336
- en: 'Das et al. (2024) Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu. 2024.
    *Security and Privacy Challenges of Large Language Models: A Survey*. [https://doi.org/10.48550/arXiv.2402.00888](https://doi.org/10.48550/arXiv.2402.00888)
    arXiv:2402.00888'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das et al. (2024) Badhan Chandra Das、M. Hadi Amini 和 Yanzhao Wu. 2024. *大型语言模型的安全与隐私挑战：综述*。
    [https://doi.org/10.48550/arXiv.2402.00888](https://doi.org/10.48550/arXiv.2402.00888)
    arXiv:2402.00888
- en: 'Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang,
    Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. *MasterKey: Automated
    Jailbreak Across Multiple Large Language Model Chatbots*. arXiv:2307.08715 [http://arxiv.org/abs/2307.08715](http://arxiv.org/abs/2307.08715)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2023) Gelei Deng、Yi Liu、Yuekang Li、Kailong Wang、Ying Zhang、Zefeng
    Li、Haoyu Wang、Tianwei Zhang 和 Yang Liu. 2023. *MasterKey：跨多个大型语言模型聊天机器人的自动化破解*。
    arXiv:2307.08715 [http://arxiv.org/abs/2307.08715](http://arxiv.org/abs/2307.08715)
- en: 'Deshpande et al. (2023) Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit,
    Ashwin Kalyan, and Karthik Narasimhan. 2023. *Toxicity in ChatGPT: Analyzing Persona-assigned
    Language Models*. [https://doi.org/10.48550/arXiv.2304.05335](https://doi.org/10.48550/arXiv.2304.05335)
    arXiv:2304.05335'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deshpande et al. (2023) Ameet Deshpande、Vishvak Murahari、Tanmay Rajpurohit、Ashwin
    Kalyan 和 Karthik Narasimhan. 2023. *ChatGPT中的毒性：分析人格指派语言模型*。 [https://doi.org/10.48550/arXiv.2304.05335](https://doi.org/10.48550/arXiv.2304.05335)
    arXiv:2304.05335
- en: Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-Verification
    Reduces Hallucination in Large Language Models. *arXiv preprint arXiv:2309.11495*
    (2023). [https://doi.org/10.48550/ARXIV.2309.11495](https://doi.org/10.48550/ARXIV.2309.11495)
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala et al. (2023) Shehzaad Dhuliawala、Mojtaba Komeili、Jing Xu、Roberta
    Raileanu、Xian Li、Asli Celikyilmaz 和 Jason Weston. 2023. **链式验证减少了大型语言模型的幻觉**。*arXiv
    预印本 arXiv:2309.11495* (2023)。 [https://doi.org/10.48550/ARXIV.2309.11495](https://doi.org/10.48550/ARXIV.2309.11495)
- en: 'Dilmaghani et al. (2019) Saharnaz Dilmaghani, Matthias R. Brust, Grégoire Danoy,
    Natalia Cassagnes, Johnatan Pecero, and Pascal Bouvry. 2019. Privacy and Security
    of Big Data in AI Systems: A Research and Standards Perspective. In *2019 IEEE
    International Conference on Big Data (Big Data)*. 5737–5743. [https://doi.org/10.1109/BigData47090.2019.9006283](https://doi.org/10.1109/BigData47090.2019.9006283)'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dilmaghani 等（2019）萨哈尔纳兹·迪尔马赫尼、马蒂亚斯·R·布鲁斯特、格雷戈瓦尔·达诺伊、娜塔莉亚·卡萨涅斯、约翰atan·佩塞罗和帕斯卡尔·布弗里。2019年。人工智能系统中大数据的隐私与安全：研究与标准视角。在*2019
    IEEE国际大数据会议（Big Data）*。5737–5743。[https://doi.org/10.1109/BigData47090.2019.9006283](https://doi.org/10.1109/BigData47090.2019.9006283)
- en: Dong et al. (2023) Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik,
    and Zhou Yu. 2023. Towards Next-Generation Intelligent Assistants Leveraging LLM
    Techniques. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining* (New York, NY, USA) *(KDD ’23)*. Association for Computing Machinery,
    5792–5793. [https://doi.org/10.1145/3580305.3599572](https://doi.org/10.1145/3580305.3599572)
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）辛·露娜·董、胜焕·穆恩、亿凡·伊桑·徐、克什提兹·马利克和周宇。2023年。利用LLM技术迈向下一代智能助手。在*第29届ACM
    SIGKDD知识发现与数据挖掘会议论文集*（纽约，NY，美国）*(KDD ’23)*。计算机协会，5792–5793。[https://doi.org/10.1145/3580305.3599572](https://doi.org/10.1145/3580305.3599572)
- en: Du et al. (2024) Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan
    Dang, Weize Chen, and Cheng Yang. 2024. *Multi-Agent Software Development through
    Cross-Team Collaboration*. [https://doi.org/10.48550/arXiv.2406.08979](https://doi.org/10.48550/arXiv.2406.08979)
    arXiv:2406.08979
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2024）朱云杜、陈倩、魏刘、子浩谢、伊飞王、玉凡邓、韦泽陈和程杨。2024年。*通过跨团队协作进行多智能体软件开发*。[https://doi.org/10.48550/arXiv.2406.08979](https://doi.org/10.48550/arXiv.2406.08979)
    arXiv:2406.08979
- en: 'Ebrahimi et al. (2021) Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William
    Gan, Joseph E. Gonzalez, Marcus Rohrbach, and Trevor Darrell. 2021. Remembering
    for the Right Reasons: Explanations Reduce Catastrophic Forgetting. *Applied AI
    Letters* 2, 4 (2021), e44. [https://doi.org/10.1002/ail2.44](https://doi.org/10.1002/ail2.44)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebrahimi 等（2021）赛娜·艾布拉希米、苏珊·佩特里克、阿卡什·戈库尔、威廉·甘、约瑟夫·E·冈萨雷斯、马库斯·罗尔巴赫和特雷弗·达雷尔。2021年。为了正确的理由记忆：解释减少灾难性遗忘。*应用AI字母*
    2, 4 (2021), e44。[https://doi.org/10.1002/ail2.44](https://doi.org/10.1002/ail2.44)
- en: 'Falade (2023) Polra Victor Falade. 2023. Decoding the Threat Landscape : ChatGPT,
    FraudGPT, and WormGPT in Social Engineering Attacks. *International Journal of
    Scientific Research in Computer Science, Engineering and Information Technology*
    9, 5 (2023), 185–198. [https://doi.org/10.32628/CSEIT2390533](https://doi.org/10.32628/CSEIT2390533)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Falade（2023）波尔拉·维克托·法拉德。2023年。解码威胁格局：ChatGPT、FraudGPT和WormGPT在社会工程攻击中的作用。*国际计算机科学、工程与信息技术科学研究期刊*
    9, 5 (2023), 185–198。[https://doi.org/10.32628/CSEIT2390533](https://doi.org/10.32628/CSEIT2390533)
- en: Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang. 2024.
    *LLM Agents Can Autonomously Exploit One-day Vulnerabilities*. [https://doi.org/10.48550/arXiv.2404.08144](https://doi.org/10.48550/arXiv.2404.08144)
    arXiv:2404.08144
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等（2024）理查德·方、罗汉·宾杜、阿库尔·古普塔和丹尼尔·康。2024年。*LLM代理可以自主利用一天内的漏洞*。[https://doi.org/10.48550/arXiv.2404.08144](https://doi.org/10.48550/arXiv.2404.08144)
    arXiv:2404.08144
- en: 'Fornaciari et al. (2021) Tommaso Fornaciari, Federico Bianchi, Massimo Poesio,
    and Dirk Hovy. 2021. BERTective: Language Models and Contextual Information for
    Deception Detection. In *Proceedings of the 16th Conference of the European Chapter
    of the Association for Computational Linguistics: Main Volume* (Online). Association
    for Computational Linguistics, 2699–2708. [https://doi.org/10.18653/v1/2021.eacl-main.232](https://doi.org/10.18653/v1/2021.eacl-main.232)'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fornaciari 等（2021）托马索·福尔纳恰里、费德里科·比安奇、马西莫·波埃西奥和迪尔克·霍维。2021年。BERTective：语言模型和上下文信息用于欺骗检测。在*第16届欧洲计算语言学协会会议：主卷论文集*（在线）。计算语言学协会，2699–2708。[https://doi.org/10.18653/v1/2021.eacl-main.232](https://doi.org/10.18653/v1/2021.eacl-main.232)
- en: Fu et al. (2023) Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li,
    and Tao Jiang. 2023. *Practical Membership Inference Attacks against Fine-tuned
    Large Language Models via Self-prompt Calibration*. [https://doi.org/10.48550/arXiv.2311.06062](https://doi.org/10.48550/arXiv.2311.06062)
    arXiv:2311.06062
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023）温杰·傅、黄东·王、陈·高、光华·刘、永·李和陶·姜。2023年。*针对微调大型语言模型的实用成员推断攻击通过自我提示校准*。[https://doi.org/10.48550/arXiv.2311.06062](https://doi.org/10.48550/arXiv.2311.06062)
    arXiv:2311.06062
- en: 'Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. 2023. *Not What You’ve Signed up for:
    Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection*.
    arXiv:2302.12173 [http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greshake et al. (2023) 凯·格雷沙克、萨哈尔·阿卜杜勒纳比、沙伊雷什·米什拉、克里斯托夫·恩德雷斯、托尔斯滕·霍尔茨和马里奥·弗里茨。2023。*你没有报名的内容：通过间接提示注入来妥协现实世界的LLM集成应用*。arXiv:2302.12173
    [http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173)
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. *Large Language
    Model Based Multi-Agents: A Survey of Progress and Challenges*. [https://doi.org/10.48550/arXiv.2402.01680](https://doi.org/10.48550/arXiv.2402.01680)
    arXiv:2402.01680'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) 太成·郭、秀英·陈、雅琦·王、瑞迪·常、世超·裴、尼特什·V·查瓦拉、奥拉夫·维斯特和湘亮·张。2024。*基于大语言模型的多智能体：进展与挑战的调查*。[https://doi.org/10.48550/arXiv.2402.01680](https://doi.org/10.48550/arXiv.2402.01680)
    arXiv:2402.01680
- en: Henderson et al. (2017) Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier,
    Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2017. *Ethical
    Challenges in Data-Driven Dialogue Systems*. [https://doi.org/10.48550/arXiv.1711.09050](https://doi.org/10.48550/arXiv.1711.09050)
    arXiv:1711.09050
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson et al. (2017) 彼得·亨德森、库斯图夫·辛哈、尼古拉斯·安热拉尔-贡蒂耶、南·罗斯玛丽·科、吉娜维夫·弗里德、瑞安·洛、乔埃尔·皮诺。2017。*数据驱动对话系统中的伦理挑战*。[https://doi.org/10.48550/arXiv.1711.09050](https://doi.org/10.48550/arXiv.1711.09050)
    arXiv:1711.09050
- en: Hines et al. (2024) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati,
    Yonatan Zunger, and Emre Kiciman. 2024. *Defending Against Indirect Prompt Injection
    Attacks With Spotlighting*. [https://doi.org/10.48550/arXiv.2403.14720](https://doi.org/10.48550/arXiv.2403.14720)
    arXiv:2403.14720
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hines et al. (2024) 基根·海因斯、加里·洛佩斯、马修·霍尔、费德里科·扎尔法提、约纳坦·宗格和埃姆雷·基西曼。2024。*通过聚焦防御间接提示注入攻击*。[https://doi.org/10.48550/arXiv.2403.14720](https://doi.org/10.48550/arXiv.2403.14720)
    arXiv:2403.14720
- en: 'Hong et al. (2023) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    2023. *MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework*. [https://doi.org/10.48550/arXiv.2308.00352](https://doi.org/10.48550/arXiv.2308.00352)
    arXiv:2308.00352'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong et al. (2023) 思睿·洪、明辰·朱戈、乔纳森·陈、夏武·郑、宇恒·程、策尧·张、锦林·王、子力·王、史蒂文·卡·星·尤、紫娟·林、丽扬·周、晨瑜·冉、灵峰·肖、成林·吴和尤尔根·施密德胡贝尔。2023。*MetaGPT:
    一个多智能体协作框架的元编程*。[https://doi.org/10.48550/arXiv.2308.00352](https://doi.org/10.48550/arXiv.2308.00352)
    arXiv:2308.00352'
- en: 'Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal
    Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)* (Melbourne, Australia), Iryna Gurevych and Yusuke Miyao (Eds.). Association
    for Computational Linguistics, 328–339. [https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Howard and Ruder (2018) 杰里米·霍华德和塞巴斯蒂安·鲁德。2018。*用于文本分类的通用语言模型微调*。在*第56届计算语言学协会年会论文集（第1卷：长篇论文）*（澳大利亚墨尔本），伊琳娜·古列维奇和宫尾祐介（编辑）。计算语言学协会，328–339。[https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)
- en: 'Hu et al. (2024b) Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming
    Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. 2024b. *mPLUG-PaperOwl: Scientific
    Diagram Analysis with the Multimodal Large Language Model*. [https://doi.org/10.48550/arXiv.2311.18248](https://doi.org/10.48550/arXiv.2311.18248)
    arXiv:2311.18248'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2024b) 安文·胡、雅雅·石、海洋·徐、嘉博·叶、青浩·叶、明·颜、晨亮·李、琦·钱、吉·张、费·黄。2024b。*mPLUG-PaperOwl:
    通过多模态大语言模型进行科学图表分析*。[https://doi.org/10.48550/arXiv.2311.18248](https://doi.org/10.48550/arXiv.2311.18248)
    arXiv:2311.18248'
- en: Hu et al. (2024a) Hanxu Hu, Pinzhen Chen, and Edoardo M. Ponti. 2024a. *Fine-Tuning
    Large Language Models with Sequential Instructions*. [https://doi.org/10.48550/arXiv.2403.07794](https://doi.org/10.48550/arXiv.2403.07794)
    arXiv:2403.07794
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2024a) 汉旭·胡、品振·陈和埃多阿多·M·庞提。2024a。*用顺序指令微调大语言模型*。[https://doi.org/10.48550/arXiv.2403.07794](https://doi.org/10.48550/arXiv.2403.07794)
    arXiv:2403.07794
- en: Huang et al. (2024a) Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting
    Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. 2024a. *Mitigating Catastrophic
    Forgetting in Large Language Models with Self-Synthesized Rehearsal*. [https://doi.org/10.48550/arXiv.2403.01244](https://doi.org/10.48550/arXiv.2403.01244)
    arXiv:2403.01244
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2024a) 黄建恒、崔乐阳、王安特、杨成意、廖欣婷、宋林峰、姚俊峰、苏金松。2024a。*在大型语言模型中缓解灾难性遗忘的自我合成复习方法*。[https://doi.org/10.48550/arXiv.2403.01244](https://doi.org/10.48550/arXiv.2403.01244)
    arXiv:2403.01244
- en: Huang et al. (2024c) Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu,
    Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 2024c.
    *An Embodied Generalist Agent in 3D World*. [https://doi.org/10.48550/arXiv.2311.12871](https://doi.org/10.48550/arXiv.2311.12871)
    arXiv:2311.12871
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2024c) 黄江永、雍思龙、马晓建、凌虎、李浦豪、王艳、李青、朱松春、贾宝雄、黄思源。2024c。*3D世界中的具身通才代理*。[https://doi.org/10.48550/arXiv.2311.12871](https://doi.org/10.48550/arXiv.2311.12871)
    arXiv:2311.12871
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and
    Ting Liu. 2023. *A Survey on Hallucination in Large Language Models: Principles,
    Taxonomy, Challenges, and Open Questions*. arXiv:2311.05232 [http://arxiv.org/abs/2311.05232](http://arxiv.org/abs/2311.05232)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) 黄磊、于伟江、马伟涛、钟伟红、冯张银、王浩天、陈强龙、彭伟华、冯晓城、秦兵、刘婷。2023。*大型语言模型中的幻觉调查：原则、分类、挑战与开放问题*。arXiv:2311.05232
    [http://arxiv.org/abs/2311.05232](http://arxiv.org/abs/2311.05232)
- en: 'Huang et al. (2024b) Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao
    Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b. *Understanding
    the Planning of LLM Agents: A Survey*. [https://doi.org/10.48550/arXiv.2402.02716](https://doi.org/10.48550/arXiv.2402.02716)
    arXiv:2402.02716'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2024b) 黄旭、刘伟文、陈晓龙、王杏梅、王浩、连德富、王亚生、唐瑞明、陈恩宏。2024b。*理解LLM代理的规划：综述*。[https://doi.org/10.48550/arXiv.2402.02716](https://doi.org/10.48550/arXiv.2402.02716)
    arXiv:2402.02716
- en: 'Hubinger et al. (2024) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert,
    Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton
    Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud,
    Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto,
    Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary
    Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R.
    Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris,
    Nicholas Schiefer, and Ethan Perez. 2024. *Sleeper Agents: Training Deceptive
    LLMs That Persist Through Safety Training*. arXiv:2401.05566 [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubinger et al. (2024) Evan Hubinger、Carson Denison、Jesse Mu、Mike Lambert、Meg
    Tong、Monte MacDiarmid、Tamera Lanham、Daniel M. Ziegler、Tim Maxwell、Newton Cheng、Adam
    Jermyn、Amanda Askell、Ansh Radhakrishnan、Cem Anil、David Duvenaud、Deep Ganguli、Fazl
    Barez、Jack Clark、Kamal Ndousse、Kshitij Sachan、Michael Sellitto、Mrinank Sharma、Nova
    DasSarma、Roger Grosse、Shauna Kravec、Yuntao Bai、Zachary Witten、Marina Favaro、Jan
    Brauner、Holden Karnofsky、Paul Christiano、Samuel R. Bowman、Logan Graham、Jared Kaplan、Sören
    Mindermann、Ryan Greenblatt、Buck Shlegeris、Nicholas Schiefer、Ethan Perez。2024。*卧底代理：训练在安全训练中持久存在的欺骗性LLMs*。arXiv:2401.05566
    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)
- en: 'Ishihara (2023) Shotaro Ishihara. 2023. Training Data Extraction From Pre-trained
    Language Models: A Survey. In *Proceedings of the 3rd Workshop on Trustworthy
    Natural Language Processing (TrustNLP 2023)* (Toronto, Canada), Anaelia Ovalle,
    Kai-Wei Chang, Ninareh Mehrabi, Yada Pruksachatkun, Aram Galystan, Jwala Dhamala,
    Apurv Verma, Trista Cao, Anoop Kumar, and Rahul Gupta (Eds.). Association for
    Computational Linguistics, 260–275. [https://aclanthology.org/2023.trustnlp-1.23](https://aclanthology.org/2023.trustnlp-1.23)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ishihara (2023) 石原翔太郎。2023。*从预训练语言模型中提取训练数据：综述*。见于*第三届可信自然语言处理研讨会（TrustNLP 2023）*（加拿大多伦多），Anaelia
    Ovalle、Kai-Wei Chang、Ninareh Mehrabi、Yada Pruksachatkun、Aram Galystan、Jwala Dhamala、Apurv
    Verma、Trista Cao、Anoop Kumar、Rahul Gupta（编辑）。计算语言学协会，260–275。[https://aclanthology.org/2023.trustnlp-1.23](https://aclanthology.org/2023.trustnlp-1.23)
- en: 'Jayaraman et al. (2023) Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha
    Roy, Wei Dai, and David Evans. 2023. *Combing for Credentials: Active Pattern
    Extraction from Smart Reply*. [https://doi.org/10.48550/arXiv.2207.10802](https://doi.org/10.48550/arXiv.2207.10802)
    arXiv:2207.10802'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jayaraman et al. (2023) Bargav Jayaraman、Esha Ghosh、Melissa Chase、Sambuddha
    Roy、Wei Dai、David Evans。2023。*凭证提取：从智能回复中提取活跃模式*。[https://doi.org/10.48550/arXiv.2207.10802](https://doi.org/10.48550/arXiv.2207.10802)
    arXiv:2207.10802
- en: Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and
    Pascale Fung. 2023. Towards Mitigating Hallucination in Large Language Models
    via Self-Reflection. *arXiv preprint arXiv:2310.06271* (2023). [https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2023) 自伟·季、铁铮·余、燕·徐、娜妍·李、悦子·石井、和帕斯卡尔·冯。2023年。*通过自我反思减少大语言模型中的幻觉*。*arXiv
    预印本 arXiv:2310.06271*（2023年）。[https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)
- en: Kandpal et al. (2024) Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz,
    Christopher A. Choquette-Choo, and Zheng Xu. 2024. *User Inference Attacks on
    Large Language Models*. [https://doi.org/10.48550/arXiv.2310.09266](https://doi.org/10.48550/arXiv.2310.09266)
    arXiv:2310.09266
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandpal et al. (2024) 尼基尔·坎德帕尔、克里希纳·皮卢特拉、阿丽娜·奥普雷亚、彼得·凯罗兹、克里斯托弗·A·肖奎特-楚、和郑旭。2024年。*对大型语言模型的用户推测攻击*。[https://doi.org/10.48550/arXiv.2310.09266](https://doi.org/10.48550/arXiv.2310.09266)
    arXiv:2310.09266
- en: 'Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, and Seong Joon Oh. 2023. *ProPILE: Probing Privacy Leakage in Large Language
    Models*. arXiv:2307.01881 [http://arxiv.org/abs/2307.01881](http://arxiv.org/abs/2307.01881)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) 思元·金、商度·尹、华然·李、马丁·古布里、成罗·尹、和成俊·吴。2023年。*ProPILE：探测大型语言模型中的隐私泄露*。arXiv:2307.01881
    [http://arxiv.org/abs/2307.01881](http://arxiv.org/abs/2307.01881)
- en: Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. *Weight
    Poisoning Attacks on Pre-trained Models*. arXiv:2004.06660 [http://arxiv.org/abs/2004.06660](http://arxiv.org/abs/2004.06660)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurita et al. (2020) 桑田·栗田、保罗·米歇尔、和格雷厄姆·纽比格。2020年。*对预训练模型的权重中毒攻击*。arXiv:2004.06660
    [http://arxiv.org/abs/2004.06660](http://arxiv.org/abs/2004.06660)
- en: 'Lee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang,
    Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training
    Data Makes Language Models Better. In *Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)* (Dublin,
    Ireland), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association
    for Computational Linguistics, 8424–8445. [https://doi.org/10.18653/v1/2022.acl-long.577](https://doi.org/10.18653/v1/2022.acl-long.577)'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2022) 凯瑟琳·李、达芙妮·伊波利托、安德鲁·奈斯特罗姆、池远·张、道格拉斯·埃克、克里斯·卡利森-伯奇、和尼古拉斯·卡尔尼。2022年。*去重训练数据使语言模型更佳*。在*第60届计算语言学协会年会论文集（第1卷：长篇论文）*（爱尔兰都柏林），斯玛兰达·穆雷桑、普雷斯拉夫·纳科夫、和艾琳·维拉维森西奥（编辑）。计算语言学协会，8424–8445。[https://doi.org/10.18653/v1/2022.acl-long.577](https://doi.org/10.18653/v1/2022.acl-long.577)
- en: 'Lee et al. (2024) Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo.
    2024. *Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided
    Revision*. [https://doi.org/10.48550/arXiv.2311.07362](https://doi.org/10.48550/arXiv.2311.07362)
    arXiv:2311.07362'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2024) 成允·李、秀贤·朴、永来·赵、和敏俊·徐。2024年。*火山：通过自我反馈指导修订缓解多模态幻觉*。[https://doi.org/10.48550/arXiv.2311.07362](https://doi.org/10.48550/arXiv.2311.07362)
    arXiv:2311.07362
- en: 'Lei et al. (2022) Yunjiao Lei, Dayong Ye, Sheng Shen, Yulei Sui, Tianqing Zhu,
    and Wanlei Zhou. 2022. New Challenges in Reinforcement Learning: A Survey of Security
    and Privacy. *Artif. Intell. Rev.* 56, 7 (2022), 7195–7236. [https://doi.org/10.1007/s10462-022-10348-5](https://doi.org/10.1007/s10462-022-10348-5)'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei et al. (2022) 云娇·雷、代永·叶、胜·沈、玉磊·隋、天庆·朱、万磊·周。2022年。*强化学习中的新挑战：安全性与隐私调查*。*人工智能评论*
    56卷，第7期（2022），7195–7236。[https://doi.org/10.1007/s10462-022-10348-5](https://doi.org/10.1007/s10462-022-10348-5)
- en: Leng et al. (2024) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian
    Lu, Chunyan Miao, and Lidong Bing. 2024. Mitigating Object Hallucinations in Large
    Vision-Language Models through Visual Contrastive Decoding. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 13872–13882.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leng et al. (2024) 思聪·冷、杭张、关征·陈、欣·李、世建·卢、春燕·苗、和黎东· Bing。2024年。*通过视觉对比解码缓解大型视觉-语言模型中的对象幻觉*。在*IEEE/CVF计算机视觉与模式识别会议论文集*。13872–13882。
- en: Li et al. (2023b) Chenyang Li, Zhao Song, Weixin Wang, and Chiwun Yang. 2023b.
    *A Theoretical Insight into Attack and Defense of Gradient Leakage in Transformer*.
    arXiv:2311.13624 [http://arxiv.org/abs/2311.13624](http://arxiv.org/abs/2311.13624)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) 陈阳·李、赵松、魏鑫·王、和池文·杨。2023b。*对变换器中梯度泄露攻击与防御的理论洞察*。arXiv:2311.13624
    [http://arxiv.org/abs/2311.13624](http://arxiv.org/abs/2311.13624)
- en: Li et al. (2023a) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu
    Meng, and Yangqiu Song. 2023a. *Multi-Step Jailbreaking Privacy Attacks on ChatGPT*.
    arXiv:2304.05197 [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) 浩然·李、大地·郭、伟·范、明石·徐、杰·黄、凡普·孟、和阳秋·宋。2023a。*对ChatGPT的多步越狱隐私攻击*。arXiv:2304.05197
    [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)
- en: 'Lin et al. (2023) Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue
    Ping, and Qin Chen. 2023. *AgentSims: An Open-Source Sandbox for Large Language
    Model Evaluation*. [https://doi.org/10.48550/arXiv.2308.04026](https://doi.org/10.48550/arXiv.2308.04026)
    arXiv:2308.04026'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) **Jiaju Lin**, **Haoran Zhao**, **Aochi Zhang**, **Yiting
    Wu**, **Huqiuyue Ping** 和 **Qin Chen**。2023。*AgentSims：一个用于大规模语言模型评估的开源沙盒*。[https://doi.org/10.48550/arXiv.2308.04026](https://doi.org/10.48550/arXiv.2308.04026)
    arXiv:2308.04026
- en: Liu et al. (2023a) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy,
    and Cyril Zhang. 2023a. *Exposing Attention Glitches with Flip-Flop Language Modeling*.
    [https://doi.org/10.48550/arXiv.2306.00946](https://doi.org/10.48550/arXiv.2306.00946)
    arXiv:2306.00946
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) **Bingbin Liu**, **Jordan T. Ash**, **Surbhi Goel**, **Akshay
    Krishnamurthy** 和 **Cyril Zhang**。2023a。*用翻转语言建模暴露注意力缺陷*。[https://doi.org/10.48550/arXiv.2306.00946](https://doi.org/10.48550/arXiv.2306.00946)
    arXiv:2306.00946
- en: Liu et al. (2024a) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob,
    and Lijuan Wang. 2024a. *Mitigating Hallucination in Large Multi-Modal Models
    via Robust Instruction Tuning*. [https://doi.org/10.48550/arXiv.2306.14565](https://doi.org/10.48550/arXiv.2306.14565)
    arXiv:2306.14565
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024a) **Fuxiao Liu**, **Kevin Lin**, **Linjie Li**, **Jianfeng
    Wang**, **Yaser Yacoob** 和 **Lijuan Wang**。2024a。*通过稳健的指令调优减轻大规模多模态模型中的幻觉*。[https://doi.org/10.48550/arXiv.2306.14565](https://doi.org/10.48550/arXiv.2306.14565)
    arXiv:2306.14565
- en: 'Liu et al. (2024b) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2024b.
    AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models.
    In *The Twelfth International Conference on Learning Representations*. [https://openreview.net/forum?id=7Jwpw4qKkb](https://openreview.net/forum?id=7Jwpw4qKkb)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024b) **Xiaogeng Liu**, **Nan Xu**, **Muhao Chen** 和 **Chaowei
    Xiao**。2024b。在*第十二届国际学习表示大会*上发表。AutoDAN：在对齐的大规模语言模型上生成隐蔽的越狱提示。[https://openreview.net/forum?id=7Jwpw4qKkb](https://openreview.net/forum?id=7Jwpw4qKkb)
- en: Liu et al. (2023b) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
    Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023b. *Prompt Injection Attack
    against LLM-integrated Applications*. [https://doi.org/10.48550/arXiv.2306.05499](https://doi.org/10.48550/arXiv.2306.05499)
    arXiv:2306.05499
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) **Yi Liu**, **Gelei Deng**, **Yuekang Li**, **Kailong Wang**,
    **Tianwei Zhang**, **Yepang Liu**, **Haoyu Wang**, **Yan Zheng** 和 **Yang Liu**。2023b。*针对LLM集成应用的提示注入攻击*。[https://doi.org/10.48550/arXiv.2306.05499](https://doi.org/10.48550/arXiv.2306.05499)
    arXiv:2306.05499
- en: 'Liu et al. (2024c) Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,
    Shuo Zhang, and Xiang Bai. 2024c. *TextMonkey: An OCR-Free Large Multimodal Model
    for Understanding Document*. [https://doi.org/10.48550/arXiv.2403.04473](https://doi.org/10.48550/arXiv.2403.04473)
    arXiv:2403.04473'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024c) **Yuliang Liu**, **Biao Yang**, **Qiang Liu**, **Zhang Li**,
    **Zhiyin Ma**, **Shuo Zhang** 和 **Xiang Bai**。2024c。*TextMonkey：一个无OCR的大规模多模态模型用于理解文档*。[https://doi.org/10.48550/arXiv.2403.04473](https://doi.org/10.48550/arXiv.2403.04473)
    arXiv:2403.04473
- en: Luo et al. (2023a) Junyu Luo, Cao Xiao, and Fenglong Ma. 2023a. Zero-Resource
    Hallucination Prevention for Large Language Models. *arXiv preprint arXiv:2309.02654*
    (2023). [https://doi.org/10.48550/ARXIV.2309.02654](https://doi.org/10.48550/ARXIV.2309.02654)
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2023a) **Junyu Luo**, **Cao Xiao** 和 **Fenglong Ma**。2023a。*大规模语言模型的零资源幻觉预防*。*arXiv
    预印本 arXiv:2309.02654*（2023）。[https://doi.org/10.48550/ARXIV.2309.02654](https://doi.org/10.48550/ARXIV.2309.02654)
- en: Luo et al. (2023b) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and
    Yue Zhang. 2023b. *An Empirical Study of Catastrophic Forgetting in Large Language
    Models During Continual Fine-tuning*. arXiv:2308.08747 [http://arxiv.org/abs/2308.08747](http://arxiv.org/abs/2308.08747)
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2023b) **Yun Luo**, **Zhen Yang**, **Fandong Meng**, **Yafu Li**,
    **Jie Zhou** 和 **Yue Zhang**。2023b。*大规模语言模型在持续微调过程中的灾难性遗忘的实证研究*。arXiv:2308.08747
    [http://arxiv.org/abs/2308.08747](http://arxiv.org/abs/2308.08747)
- en: Mahmoud and Hajj (2022) Reem A. Mahmoud and Hazem Hajj. 2022. Multi-Objective
    Learning to Overcome Catastrophic Forgetting in Time-series Applications. *ACM
    Transactions on Knowledge Discovery from Data* 16, 6 (2022), 1–20. [https://doi.org/10.1145/3502728](https://doi.org/10.1145/3502728)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmoud 和 Hajj (2022) **Reem A. Mahmoud** 和 **Hazem Hajj**。2022。*多目标学习克服时间序列应用中的灾难性遗忘*。*ACM
    数据知识发现交易* 16，6（2022），1–20。[https://doi.org/10.1145/3502728](https://doi.org/10.1145/3502728)
- en: 'Mandi et al. (2023) Zhao Mandi, Shreeya Jain, and Shuran Song. 2023. *RoCo:
    Dialectic Multi-Robot Collaboration with Large Language Models*. [https://doi.org/10.48550/arXiv.2307.04738](https://doi.org/10.48550/arXiv.2307.04738)
    arXiv:2307.04738'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandi et al. (2023) **Zhao Mandi**, **Shreeya Jain** 和 **Shuran Song**。2023。*RoCo：与大规模语言模型的辩证多机器人协作*。[https://doi.org/10.48550/arXiv.2307.04738](https://doi.org/10.48550/arXiv.2307.04738)
    arXiv:2307.04738
- en: Mendis et al. (2007) D. S. Kalana Mendis, Asoka S. Karunananda, Udaya Samaratunga,
    and Uditha Ratnayake. 2007. An Approach to the Development of Commonsense Knowledge
    Modeling Systems for Disaster Management. 28, 2 (2007), 179–196. [https://doi.org/10.1007/s10462-009-9097-6](https://doi.org/10.1007/s10462-009-9097-6)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendis 等（2007）D. S. Kalana Mendis、Asoka S. Karunananda、Udaya Samaratunga 和 Uditha
    Ratnayake。2007年。《An Approach to the Development of Commonsense Knowledge Modeling
    Systems for Disaster Management》。28, 2（2007），179–196。 [https://doi.org/10.1007/s10462-009-9097-6](https://doi.org/10.1007/s10462-009-9097-6)
- en: Mondesire and Wiegand (2023) Sean Mondesire and R. Paul Wiegand. 2023. Mitigating
    Catastrophic Forgetting with Complementary Layered Learning. *Electronics* 12,
    3 (2023), 706. [https://doi.org/10.3390/electronics12030706](https://doi.org/10.3390/electronics12030706)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mondesire 和 Wiegand（2023）Sean Mondesire 和 R. Paul Wiegand。2023年。《Mitigating
    Catastrophic Forgetting with Complementary Layered Learning》。*Electronics* 12,
    3（2023），706。 [https://doi.org/10.3390/electronics12030706](https://doi.org/10.3390/electronics12030706)
- en: 'Moor et al. (2023) Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga,
    Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar.
    2023. Med-Flamingo: A Multimodal Medical Few-shot Learner. In *Proceedings of
    the 3rd Machine Learning for Health Symposium* *(Proceedings of Machine Learning
    Research, Vol. 225)*. PMLR, 353–367. [https://proceedings.mlr.press/v225/moor23a.html](https://proceedings.mlr.press/v225/moor23a.html)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Moor 等（2023）Michael Moor、Qian Huang、Shirley Wu、Michihiro Yasunaga、Yash Dalmia、Jure
    Leskovec、Cyril Zakka、Eduardo Pontes Reis 和 Pranav Rajpurkar。2023年。《Med-Flamingo:
    A Multimodal Medical Few-shot Learner》。在 *Proceedings of the 3rd Machine Learning
    for Health Symposium* *(Proceedings of Machine Learning Research, Vol. 225)*。PMLR，353–367。
    [https://proceedings.mlr.press/v225/moor23a.html](https://proceedings.mlr.press/v225/moor23a.html)'
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, and Balcom.
    2024. *GPT-4 Technical Report*. [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等（2024）OpenAI、Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge
    Akkaya、Florencia Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal
    Anadkat、Red Avila、Igor Babuschkin、Suchir Balaji 和 Balcom。2024年。《*GPT-4 Technical
    Report*》。 [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774
- en: Ozdayi et al. (2023) Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe
    Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, and Rahul Gupta. 2023. *Controlling
    the Extraction of Memorized Data from Large Language Models via Prompt-Tuning*.
    arXiv:2305.11759 [https://arxiv.org/abs/2305.11759](https://arxiv.org/abs/2305.11759)
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ozdayi 等（2023）Mustafa Safa Ozdayi、Charith Peris、Jack FitzGerald、Christophe Dupuy、Jimit
    Majmudar、Haidar Khan、Rahil Parikh 和 Rahul Gupta。2023年。《*Controlling the Extraction
    of Memorized Data from Large Language Models via Prompt-Tuning*》。arXiv:2305.11759
    [https://arxiv.org/abs/2305.11759](https://arxiv.org/abs/2305.11759)
- en: Pang et al. (2024) Jing-Cheng Pang, Heng-Bo Fan, Pengyuan Wang, Jia-Hao Xiao,
    Nan Tang, Si-Hang Yang, Chengxing Jia, Sheng-Jun Huang, and Yang Yu. 2024. *Empowering
    Language Models with Active Inquiry for Deeper Understanding*. [https://doi.org/10.48550/arXiv.2402.03719](https://doi.org/10.48550/arXiv.2402.03719)
    arXiv:2402.03719
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pang 等（2024）Jing-Cheng Pang、Heng-Bo Fan、Pengyuan Wang、Jia-Hao Xiao、Nan Tang、Si-Hang
    Yang、Chengxing Jia、Sheng-Jun Huang 和 Yang Yu。2024年。《*Empowering Language Models
    with Active Inquiry for Deeper Understanding*》。 [https://doi.org/10.48550/arXiv.2402.03719](https://doi.org/10.48550/arXiv.2402.03719)
    arXiv:2402.03719
- en: 'Park et al. (2023b) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023b. *Generative Agents: Interactive
    Simulacra of Human Behavior*. [https://doi.org/10.48550/arXiv.2304.03442](https://doi.org/10.48550/arXiv.2304.03442)
    arXiv:2304.03442'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等（2023b）Joon Sung Park、Joseph C. O’Brien、Carrie J. Cai、Meredith Ringel
    Morris、Percy Liang 和 Michael S. Bernstein。2023b年。《*Generative Agents: Interactive
    Simulacra of Human Behavior*》。 [https://doi.org/10.48550/arXiv.2304.03442](https://doi.org/10.48550/arXiv.2304.03442)
    arXiv:2304.03442'
- en: 'Park et al. (2023a) Peter S. Park, Simon Goldstein, Aidan O’Gara, Michael Chen,
    and Dan Hendrycks. 2023a. *AI Deception: A Survey of Examples, Risks, and Potential
    Solutions*. [https://doi.org/10.48550/arXiv.2308.14752](https://doi.org/10.48550/arXiv.2308.14752)
    arXiv:2308.14752'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等（2023a）Peter S. Park、Simon Goldstein、Aidan O’Gara、Michael Chen 和 Dan
    Hendrycks。2023a年。《*AI Deception: A Survey of Examples, Risks, and Potential Solutions*》。
    [https://doi.org/10.48550/arXiv.2308.14752](https://doi.org/10.48550/arXiv.2308.14752)
    arXiv:2308.14752'
- en: 'Peng et al. (2023) Liangzu Peng, Paris Giampouras, and Rene Vidal. 2023. The
    Ideal Continual Learner: An Agent That Never Forgets. In *Proceedings of the 40th
    International Conference on Machine Learning*. PMLR, 27585–27610. [https://proceedings.mlr.press/v202/peng23a.html](https://proceedings.mlr.press/v202/peng23a.html)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等（2023）Liangzu Peng、Paris Giampouras 和 Rene Vidal。2023年。《The Ideal Continual
    Learner: An Agent That Never Forgets》。在 *Proceedings of the 40th International
    Conference on Machine Learning*。PMLR，27585–27610。 [https://proceedings.mlr.press/v202/peng23a.html](https://proceedings.mlr.press/v202/peng23a.html)'
- en: Peters (2023) Jay Peters. 2023. *The Bing AI Bot Has Been Secretly Running GPT-4*.
    The Verge. [https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm](https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm)
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters (2023) 杰伊·彼得斯。2023年。*必应AI机器人已秘密运行GPT-4*。The Verge。 [https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm](https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm)
- en: 'Puig et al. (2023) Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire
    Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal
    Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges,
    John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra
    Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh
    Mottaghi. 2023. *Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots*. [https://doi.org/10.48550/arXiv.2310.13724](https://doi.org/10.48550/arXiv.2310.13724)
    arXiv:2310.13724'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puig 等人 (2023) 萨维尔·普伊格、埃里克·安德斯兰德、安德鲁·斯佐特、米凯尔·达莱尔·科特、杨宗彦、鲁斯兰·帕特塞、鲁塔·德赛、亚历山大·威廉·克莱格、米哈尔·赫拉瓦克、徐恩敏、弗拉迪米尔·冯德鲁斯、泰奥菲尔·热尔维、文森特-皮埃尔·贝尔热、约翰·M·特纳、奥列克桑德尔·马克西梅茨、佐尔特·基拉、米纳尔·卡拉克里什南、吉滕德拉·马利克、德文德拉·辛格·查普洛特、安纳特·贾因、德鲁夫·巴特拉、阿克沙拉·赖和鲁兹贝赫·莫塔吉。2023年。*Habitat
    3.0：人类、化身和机器人共同栖息地*。 [https://doi.org/10.48550/arXiv.2310.13724](https://doi.org/10.48550/arXiv.2310.13724)
    arXiv:2310.13724
- en: Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson,
    Mengdi Wang, and Prateek Mittal. 2023. *Visual Adversarial Examples Jailbreak
    Aligned Large Language Models*. [https://doi.org/10.48550/arXiv.2306.13213](https://doi.org/10.48550/arXiv.2306.13213)
    arXiv:2306.13213
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等人 (2023) 向宇·齐、开轩·黄、阿什维尼·潘达、彼得·亨德森、梦笛·王和普拉提克·米塔尔。2023年。*视觉对抗示例越狱对齐的大型语言模型*。
    [https://doi.org/10.48550/arXiv.2306.13213](https://doi.org/10.48550/arXiv.2306.13213)
    arXiv:2306.13213
- en: 'Qian et al. (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang,
    Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li,
    Zhiyuan Liu, and Maosong Sun. 2024. *ChatDev: Communicative Agents for Software
    Development*. [https://doi.org/10.48550/arXiv.2307.07924](https://doi.org/10.48550/arXiv.2307.07924)
    arXiv:2307.07924'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 (2024) 陈乾、魏柳、洪章·刘、诺·陈、宇凡·邓、嘉浩·李、程杨、伟泽·陈、玉生·苏、辛·宗、聚源·徐、大海·李、智远·刘和毛松·孙。2024年。*ChatDev：软件开发的沟通代理*。
    [https://doi.org/10.48550/arXiv.2307.07924](https://doi.org/10.48550/arXiv.2307.07924)
    arXiv:2307.07924
- en: 'Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, and George J.
    Pappas. 2023. SmoothLLM: Defending Large Language Models Against Jailbreaking
    Attacks. *arXiv preprint arXiv:2310.03684* (2023). [https://doi.org/10.48550/ARXIV.2310.03684](https://doi.org/10.48550/ARXIV.2310.03684)'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robey 等人 (2023) 亚历山大·罗比、埃里克·黄、哈梅德·哈萨尼和乔治·J·帕帕斯。2023年。*SmoothLLM：保护大型语言模型免受越狱攻击*。*arXiv
    预印本 arXiv:2310.03684* (2023)。 [https://doi.org/10.48550/ARXIV.2310.03684](https://doi.org/10.48550/ARXIV.2310.03684)
- en: Ruan et al. (2024) Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao
    Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, and Tatsunori Hashimoto. 2024.
    *Identifying the Risks of LM Agents with an LM-Emulated Sandbox*. [https://doi.org/10.48550/arXiv.2309.15817](https://doi.org/10.48550/arXiv.2309.15817)
    arXiv:2309.15817
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruan 等人 (2024) 阎军·阮、洪华·董、安德鲁·王、西尔维乌·皮提斯、永超·周、吉米·巴、扬·迪布瓦、克里斯·J·马迪森和辰诺里·桥本。2024年。*识别语言模型代理的风险：一种语言模型仿真沙箱*。
    [https://doi.org/10.48550/arXiv.2309.15817](https://doi.org/10.48550/arXiv.2309.15817)
    arXiv:2309.15817
- en: 'Schuster et al. (2021) Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly
    Shmatikov. 2021. You Autocomplete Me: Poisoning Vulnerabilities in Neural Code
    Completion. In *30th USENIX Security Symposium (USENIX Security 21)*. 1559–1575.
    [https://www.usenix.org/conference/usenixsecurity21/presentation/schuster](https://www.usenix.org/conference/usenixsecurity21/presentation/schuster)'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster 等人 (2021) 罗伊·舒斯特、从征·宋、埃兰·特罗默和维塔利·什马季科夫。2021年。你自动补全我：神经代码补全中的中毒漏洞。*第30届USENIX安全研讨会
    (USENIX Security 21)*。1559–1575。 [https://www.usenix.org/conference/usenixsecurity21/presentation/schuster](https://www.usenix.org/conference/usenixsecurity21/presentation/schuster)
- en: 'Shayegani et al. (2023) Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023.
    *Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language
    Models*. [https://doi.org/10.48550/arXiv.2307.14539](https://doi.org/10.48550/arXiv.2307.14539)
    arXiv:2307.14539'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shayegani 等人 (2023) 埃尔凡·沙耶甘尼、岳东和纳埃尔·阿布-盖萨勒赫。2023年。*拼碎的越狱：对多模态语言模型的成分对抗攻击*。 [https://doi.org/10.48550/arXiv.2307.14539](https://doi.org/10.48550/arXiv.2307.14539)
    arXiv:2307.14539
- en: Shejwalkar and Houmansadr (2021) Virat Shejwalkar and Amir Houmansadr. 2021.
    Membership Privacy for Machine Learning Models Through Knowledge Transfer. 35,
    11 (2021), 9549–9557. Issue 11. [https://doi.org/10.1609/aaai.v35i11.17150](https://doi.org/10.1609/aaai.v35i11.17150)
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shejwalkar 和 Houmansadr (2021) Virat Shejwalkar 和 Amir Houmansadr. 2021. 通过知识转移实现机器学习模型的成员隐私。35,
    11 (2021), 9549–9557。第11期。 [https://doi.org/10.1609/aaai.v35i11.17150](https://doi.org/10.1609/aaai.v35i11.17150)
- en: 'Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and
    Yang Zhang. 2023. *”Do Anything Now”: Characterizing and Evaluating In-The-Wild
    Jailbreak Prompts on Large Language Models*. arXiv:2308.03825 [http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825)'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, 和 Yang
    Zhang. 2023. *“现在做任何事”：对大型语言模型上的现实世界越狱提示进行特征化和评估*。arXiv:2308.03825 [http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825)
- en: Sun et al. (2023) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan
    Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,
    and Trevor Darrell. 2023. *Aligning Large Multimodal Models with Factually Augmented
    RLHF*. [https://doi.org/10.48550/arXiv.2309.14525](https://doi.org/10.48550/arXiv.2309.14525)
    arXiv:2309.14525
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan
    Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer,
    和 Trevor Darrell. 2023. *将大型多模态模型与事实增强 RLHF 对齐*。 [https://doi.org/10.48550/arXiv.2309.14525](https://doi.org/10.48550/arXiv.2309.14525)
    arXiv:2309.14525
- en: 'Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023.
    *Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents*. [https://doi.org/10.48550/arXiv.2306.03314](https://doi.org/10.48550/arXiv.2306.03314)
    arXiv:2306.03314'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad 和 Nadiri (2023) Yashar Talebirad 和 Amirhossein Nadiri. 2023. *多智能体协作：利用智能大型语言模型代理的力量*。
    [https://doi.org/10.48550/arXiv.2306.03314](https://doi.org/10.48550/arXiv.2306.03314)
    arXiv:2306.03314
- en: 'Taveekitworachai et al. (2023) Pittawat Taveekitworachai, Febri Abdullah, Mustafa Can
    Gursesli, Mury F. Dewantoro, Siyuan Chen, Antonio Lanata, Andrea Guazzini, and
    Ruck Thawonmas. 2023. Breaking Bad: Unraveling Influences and Risks of User Inputs
    to ChatGPT for Game Story Generation. In *Interactive Storytelling* (Cham) *(Lecture
    Notes in Computer Science)*, Lissa Holloway-Attaway and John T. Murray (Eds.).
    Springer Nature Switzerland, 285–296.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taveekitworachai et al. (2023) Pittawat Taveekitworachai, Febri Abdullah, Mustafa
    Can Gursesli, Mury F. Dewantoro, Siyuan Chen, Antonio Lanata, Andrea Guazzini,
    和 Ruck Thawonmas. 2023. 《Breaking Bad: 揭示用户输入对 ChatGPT 游戏故事生成的影响与风险》。收录于 *互动讲故事*
    (Cham) *(计算机科学讲义系列)*, Lissa Holloway-Attaway 和 John T. Murray (编)。Springer Nature
    Switzerland, 285–296。'
- en: Toetzke et al. (2023) Malte Toetzke, Benedict Probst, and Stefan Feuerriegel.
    2023. Leveraging Large Language Models to Monitor Climate Technology Innovation.
    *Environmental Research Letters* 18, 9 (2023), 091004. [https://doi.org/10.1088/1748-9326/acf233](https://doi.org/10.1088/1748-9326/acf233)
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toetzke et al. (2023) Malte Toetzke, Benedict Probst, 和 Stefan Feuerriegel.
    2023. 利用大型语言模型监控气候技术创新。*环境研究快报* 18, 9 (2023), 091004。 [https://doi.org/10.1088/1748-9326/acf233](https://doi.org/10.1088/1748-9326/acf233)
- en: 'Tong et al. (2024) Meng Tong, Kejiang Chen, Jie Zhang, Yuang Qi, Weiming Zhang,
    Nenghai Yu, Tianwei Zhang, and Zhikun Zhang. 2024. *InferDPT: Privacy-Preserving
    Inference for Black-box Large Language Model*. [https://doi.org/10.48550/arXiv.2310.12214](https://doi.org/10.48550/arXiv.2310.12214)
    arXiv:2310.12214'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tong et al. (2024) Meng Tong, Kejiang Chen, Jie Zhang, Yuang Qi, Weiming Zhang,
    Nenghai Yu, Tianwei Zhang, 和 Zhikun Zhang. 2024. *InferDPT: 对黑箱大型语言模型的隐私保护推断*。
    [https://doi.org/10.48550/arXiv.2310.12214](https://doi.org/10.48550/arXiv.2310.12214)
    arXiv:2310.12214'
- en: Truong et al. (2021) Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls,
    and Nicolas Papernot. 2021. Data-Free Model Extraction. In *2021 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*. 4769–4778. [https://ieeexplore.ieee.org/document/9577784](https://ieeexplore.ieee.org/document/9577784)
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Truong et al. (2021) Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls,
    和 Nicolas Papernot. 2021. 无数据模型提取。收录于 *2021 IEEE/CVF 计算机视觉与模式识别会议 (CVPR)*。4769–4778。
    [https://ieeexplore.ieee.org/document/9577784](https://ieeexplore.ieee.org/document/9577784)
- en: Vakili et al. (2022) Thomas Vakili, Anastasios Lamproudis, Aron Henriksson,
    and Hercules Dalianis. 2022. Downstream Task Performance of BERT Models Pre-Trained
    Using Automatically De-Identified Clinical Data. In *Proceedings of the Thirteenth
    Language Resources and Evaluation Conference* (Marseille, France), Nicoletta Calzolari,
    Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck,
    Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan
    Odijk, and Stelios Piperidis (Eds.). European Language Resources Association,
    4245–4252. [https://aclanthology.org/2022.lrec-1.451](https://aclanthology.org/2022.lrec-1.451)
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vakili et al. (2022) 托马斯·瓦基利、阿纳斯塔西奥斯·兰普劳迪斯、阿伦·亨里克松和赫剌克勒斯·达利安尼斯。2022. 使用自动去标识的临床数据预训练的BERT模型的下游任务表现。在
    *第十三届语言资源与评估会议论文集*（法国马赛），尼科莱塔·卡尔佐拉里、弗雷德里克·贝谢、菲利普·布拉什、哈立德·楚克里、克里斯托弗·谢里、蒂埃里·德克莱克、萨拉·戈吉、石原仁、贝恩特·梅高德、约瑟夫·马里安尼、埃伦·马佐、扬·奥迪克和斯特利奥斯·皮佩里迪斯（编辑）。欧洲语言资源协会，4245–4252。
    [https://aclanthology.org/2022.lrec-1.451](https://aclanthology.org/2022.lrec-1.451)
- en: 'Vander Eeckt and Van Hamme (2023) Steven Vander Eeckt and Hugo Van Hamme. 2023.
    Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting
    in Automatic Speech Recognition. In *ICASSP 2023 - 2023 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)* (Rhodes Island, Greece).
    IEEE, 1–5. [https://doi.org/10.1109/ICASSP49357.2023.10095147](https://doi.org/10.1109/ICASSP49357.2023.10095147)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vander Eeckt and Van Hamme (2023) 史蒂文·范德·埃克特和休戈·范·哈梅。2023. 权重平均：一种简单而有效的方法来克服自动语音识别中的灾难性遗忘。在
    *ICASSP 2023 - 2023 IEEE国际声学、语音和信号处理会议（ICASSP）*（希腊罗德岛）。IEEE，1–5。 [https://doi.org/10.1109/ICASSP49357.2023.10095147](https://doi.org/10.1109/ICASSP49357.2023.10095147)
- en: Wan et al. (2023a) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023a.
    *Poisoning Language Models During Instruction Tuning*. arXiv:2305.00944 [http://arxiv.org/abs/2305.00944](http://arxiv.org/abs/2305.00944)
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2023a) 亚历山大·万、埃里克·沃勒斯、盛申和丹·克莱因。2023a. *在指令调整过程中对语言模型进行攻击*。arXiv:2305.00944
    [http://arxiv.org/abs/2305.00944](http://arxiv.org/abs/2305.00944)
- en: Wan et al. (2023b) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023b.
    *Poisoning Language Models During Instruction Tuning*. [https://doi.org/10.48550/arXiv.2305.00944](https://doi.org/10.48550/arXiv.2305.00944)
    arXiv:2305.00944
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan et al. (2023b) 亚历山大·万、埃里克·沃勒斯、盛申和丹·克莱因。2023b. *在指令调整过程中对语言模型进行攻击*。 [https://doi.org/10.48550/arXiv.2305.00944](https://doi.org/10.48550/arXiv.2305.00944)
    arXiv:2305.00944
- en: Wang and Li (2023) Huan Wang and Yan-Fu Li. 2023. Large Language Model Empowered
    by Domain-Specific Knowledge Base for Industrial Equipment Operation and Maintenance.
    In *2023 5th International Conference on System Reliability and Safety Engineering
    (SRSE)*. 474–479. [https://doi.org/10.1109/SRSE59585.2023.10336112](https://doi.org/10.1109/SRSE59585.2023.10336112)
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Li (2023) 黄湾和李彦福。2023. 基于领域特定知识库的大型语言模型在工业设备操作和维护中的应用。在 *2023年第五届系统可靠性与安全工程国际会议（SRSE）*。474–479。
    [https://doi.org/10.1109/SRSE59585.2023.10336112](https://doi.org/10.1109/SRSE59585.2023.10336112)
- en: 'Wang et al. (2024b) Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen,
    Ji Zhang, Fei Huang, and Jitao Sang. 2024b. *Mobile-Agent: Autonomous Multi-Modal
    Mobile Device Agent with Visual Perception*. [https://doi.org/10.48550/arXiv.2401.16158](https://doi.org/10.48550/arXiv.2401.16158)
    arXiv:2401.16158'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024b) 王军扬、许海洋、叶佳博、闵岩、沈伟洲、张吉、黄飞和桑继涛。2024b. *Mobile-Agent：具有视觉感知的自主多模态移动设备代理*。
    [https://doi.org/10.48550/arXiv.2401.16158](https://doi.org/10.48550/arXiv.2401.16158)
    arXiv:2401.16158
- en: Wang et al. (2023c) Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao
    Zhang, and Yelong Shen. 2023c. *Adapting LLM Agents Through Communication*. [https://doi.org/10.48550/arXiv.2310.01444](https://doi.org/10.48550/arXiv.2310.01444)
    arXiv:2310.01444
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023c) 王宽、陆亚东、迈克尔·圣阿特罗斯、耶云·龚、赵超和叶龙。2023c. *通过交流调整LLM代理*。 [https://doi.org/10.48550/arXiv.2310.01444](https://doi.org/10.48550/arXiv.2310.01444)
    arXiv:2310.01444
- en: Wang et al. (2023d) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Ji-Rong Wen. 2023d. *A Survey on Large Language Model Based Autonomous
    Agents*. arXiv:2308.11432 [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023d) 王磊、马晨、冯雪洋、张泽宇、杨浩、张景森、陈志远、唐佳凯、陈旭、林彦凯、魏鑫和文纪荣。2023d. *基于大型语言模型的自主代理调查*。arXiv:2308.11432
    [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)
- en: Wang et al. (2024c) Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai
    Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
    Dou, Jun Wang, and Ji-Rong Wen. 2024c. *User Behavior Simulation with Large Language
    Model Based Agents*. [https://doi.org/10.48550/arXiv.2306.02552](https://doi.org/10.48550/arXiv.2306.02552)
    arXiv:2306.02552
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2024c）**雷·王**、**景森·张**、**浩·杨**、**志远·陈**、**佳凯·唐**、**泽宇·张**、**许·陈**、**彦凯·林**、**瑞华·宋**、**韦恩·辛·赵**、**军·徐**、**志成·杜**、**军·王**
    和 **季荣·温**。2024c。*基于大型语言模型的用户行为模拟*。[https://doi.org/10.48550/arXiv.2306.02552](https://doi.org/10.48550/arXiv.2306.02552)
    arXiv:2306.02552
- en: 'Wang et al. (2023b) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. 2023b.
    *Avalon’s Game of Thoughts: Battle Against Deception through Recursive Contemplation*.
    [https://doi.org/10.48550/arXiv.2310.01320](https://doi.org/10.48550/arXiv.2310.01320)
    arXiv:2310.01320'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023b）**申智·王**、**常·刘**、**子龙·郑**、**思远·齐**、**硕·陈**、**棋森·杨**、**安德鲁·赵**、**超飞·王**、**诗基·宋**
    和 **高·黄**。2023b。*阿瓦隆的思维游戏：通过递归反思对抗欺骗*。[https://doi.org/10.48550/arXiv.2310.01320](https://doi.org/10.48550/arXiv.2310.01320)
    arXiv:2310.01320
- en: 'Wang et al. (2024a) Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang,
    Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024a. *Large Language Models for
    Education: A Survey and Outlook*. arXiv:2403.18105 [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2024a）**申·王**、**天龙·徐**、**杭·李**、**超利·张**、**乔琳·梁**、**季亮·唐**、**菲利普·S·余** 和 **青松·温**。2024a。*教育领域的大型语言模型：综述与展望*。arXiv:2403.18105
    [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)
- en: 'Wang et al. (2024d) Shang Wang, Tianqing Zhu, Bo Liu, Ming Ding, Xu Guo, Dayong
    Ye, Wanlei Zhou, and Philip S. Yu. 2024d. *Unique Security and Privacy Threats
    of Large Language Model: A Comprehensive Survey*. [https://doi.org/10.48550/arXiv.2406.07973](https://doi.org/10.48550/arXiv.2406.07973)
    arXiv:2406.07973'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2024d）**尚·王**、**天青·朱**、**博·刘**、**铭·丁**、**徐·郭**、**大勇·叶**、**万磊·周** 和 **菲利普·S·余**。2024d。*大型语言模型的独特安全与隐私威胁：综合综述*。[https://doi.org/10.48550/arXiv.2406.07973](https://doi.org/10.48550/arXiv.2406.07973)
    arXiv:2406.07973
- en: 'Wang et al. (2023a) Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, and Yanwei
    Fu. 2023a. *WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model*.
    [https://doi.org/10.48550/arXiv.2308.15962](https://doi.org/10.48550/arXiv.2308.15962)
    arXiv:2308.15962'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023a）**天宇·王**、**一凡·李**、**海涛·林**、**向阳·薛** 和 **彦伟·傅**。2023a。*WALL-E：具身机器人WAiter负载提升与大型语言模型*。[https://doi.org/10.48550/arXiv.2308.15962](https://doi.org/10.48550/arXiv.2308.15962)
    arXiv:2308.15962
- en: 'Wang et al. (2023e) Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, and Tom H.
    Luan. 2023e. A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions.
    *IEEE Open Journal of the Computer Society* 4 (2023), 280–302. [https://doi.org/10.1109/OJCS.2023.3300321](https://doi.org/10.1109/OJCS.2023.3300321)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023e）**云涛·王**、**扬赫·潘**、**苗·闫**、**周·苏** 和 **汤姆·H·阮**。2023e。*关于 ChatGPT 的综述：AI
    生成内容、挑战与解决方案*。*IEEE 计算机学会开放期刊* 4 (2023)，280–302。[https://doi.org/10.1109/OJCS.2023.3300321](https://doi.org/10.1109/OJCS.2023.3300321)
- en: 'Wang et al. (2023g) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023g. *Aligning Large
    Language Models with Human: A Survey*. [https://doi.org/10.48550/arXiv.2307.12966](https://doi.org/10.48550/arXiv.2307.12966)
    arXiv:2307.12966'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023g）**余飞·王**、**万军·钟**、**梁有·李**、**费·米**、**邢善·曾**、**温勇·黄**、**李锋·商**、**辛·江**
    和 **群·刘**。2023g。*与人类对齐的大型语言模型：综述*。[https://doi.org/10.48550/arXiv.2307.12966](https://doi.org/10.48550/arXiv.2307.12966)
    arXiv:2307.12966
- en: 'Wang et al. (2023f) Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen
    Gui, and Enze Wang. 2023f. *Self-Deception: Reverse Penetrating the Semantic Firewall
    of Large Language Models*. arXiv:2308.11521 [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2023f）**振华·王**、**伟·谢**、**凯·陈**、**宝生·王**、**志文·桂** 和 **恩泽·王**。2023f。*自我欺骗：反向穿透大型语言模型的语义防火墙*。arXiv:2308.11521
    [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)
- en: 'Winata et al. (2023) Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan,
    Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preotiuc-Pietro.
    2023. Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning.
    In *Findings of the Association for Computational Linguistics: ACL 2023*. Association
    for Computational Linguistics, Toronto, Canada, 768–777. [https://doi.org/10.18653/v1/2023.findings-acl.48](https://doi.org/10.18653/v1/2023.findings-acl.48)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温塔等（2023）**岚达·温塔**、**凌觉·谢**、**卡尔提克·拉达克里希南**、**世杰·吴**、**希森·金**、**鹏翔·程**、**玛扬·库尔卡尼**
    和 **丹尼尔·普雷奥提乌克-彼得罗**。2023。*克服大规模多语种持续学习中的灾难性遗忘*。发表于*计算语言学协会发现：ACL 2023*。计算语言学协会，多伦多，加拿大，768–777。[https://doi.org/10.18653/v1/2023.findings-acl.48](https://doi.org/10.18653/v1/2023.findings-acl.48)
- en: 'Wu et al. (2023) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng
    Tang, and Nan Duan. 2023. *Visual ChatGPT: Talking, Drawing and Editing with Visual
    Foundation Models*. [https://doi.org/10.48550/arXiv.2303.04671](https://doi.org/10.48550/arXiv.2303.04671)
    arXiv:2303.04671'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 (2023) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang
    和 Nan Duan. 2023. *Visual ChatGPT：利用视觉基础模型进行对话、绘图和编辑*。 [https://doi.org/10.48550/arXiv.2303.04671](https://doi.org/10.48550/arXiv.2303.04671)
    arXiv:2303.04671
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. *The
    Rise and Potential of Large Language Model Based Agents: A Survey*. [https://doi.org/10.48550/arXiv.2309.07864](https://doi.org/10.48550/arXiv.2309.07864)
    arXiv:2309.07864'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi 等人 (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang 和 Tao Gui. 2023. *基于大型语言模型的代理的崛起与潜力：综述*。
    [https://doi.org/10.48550/arXiv.2309.07864](https://doi.org/10.48550/arXiv.2309.07864)
    arXiv:2309.07864
- en: 'Xie et al. (2024) Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin
    Li. 2024. *Large Multimodal Agents: A Survey*. [https://doi.org/10.48550/arXiv.2402.15116](https://doi.org/10.48550/arXiv.2402.15116)
    arXiv:2402.15116'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 (2024) Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan 和 Guanbin Li.
    2024. *大型多模态代理：综述*。 [https://doi.org/10.48550/arXiv.2402.15116](https://doi.org/10.48550/arXiv.2402.15116)
    arXiv:2402.15116
- en: 'Xu et al. (2023c) Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S.
    Yu. 2023c. Machine Unlearning: A Survey. *ACM Comput. Surv.* 56, 1 (2023), 9:1–9:36.
    [https://doi.org/10.1145/3603620](https://doi.org/10.1145/3603620)'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023c) Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou 和 Philip S. Yu.
    2023c. *机器遗忘：综述*。 *ACM Comput. Surv.* 56, 1 (2023), 9:1–9:36。 [https://doi.org/10.1145/3603620](https://doi.org/10.1145/3603620)
- en: 'Xu et al. (2023a) Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt
    Keutzer, See Kiong Ng, and Jiashi Feng. 2023a. *MAgIC: Investigation of Large
    Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and
    Collaboration*. [https://doi.org/10.48550/arXiv.2311.08562](https://doi.org/10.48550/arXiv.2311.08562)
    arXiv:2311.08562'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023a) Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer,
    See Kiong Ng 和 Jiashi Feng. 2023a. *MAgIC：大型语言模型驱动的多代理在认知、适应性、理性和协作中的研究*。 [https://doi.org/10.48550/arXiv.2311.08562](https://doi.org/10.48550/arXiv.2311.08562)
    arXiv:2311.08562
- en: 'Xu et al. (2023b) Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming
    Cai, and Tianxiang Wang. 2023b. Enhancing Language Representation with Constructional
    Information for Natural Language Understanding. In *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*
    (Toronto, Canada). Association for Computational Linguistics, 4685–4705. [https://doi.org/10.18653/v1/2023.acl-long.258](https://doi.org/10.18653/v1/2023.acl-long.258)'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023b) Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming Cai
    和 Tianxiang Wang. 2023b. *通过构造信息增强语言表征以进行自然语言理解*。见 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*（加拿大多伦多）。计算语言学协会，4685–4705。
    [https://doi.org/10.18653/v1/2023.acl-long.258](https://doi.org/10.18653/v1/2023.acl-long.258)
- en: Xu et al. (2024) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. 2024. *Language
    Agents with Reinforcement Learning for Strategic Play in the Werewolf Game*. [https://doi.org/10.48550/arXiv.2310.18940](https://doi.org/10.48550/arXiv.2310.18940)
    arXiv:2310.18940
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2024) Zelai Xu, Chao Yu, Fei Fang, Yu Wang 和 Yi Wu. 2024. *利用强化学习的语言代理在狼人杀游戏中的战略玩法*。
    [https://doi.org/10.48550/arXiv.2310.18940](https://doi.org/10.48550/arXiv.2310.18940)
    arXiv:2310.18940
- en: 'Yan et al. (2023) Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen,
    Guangyu Shen, and Xiangyu Zhang. 2023. *ParaFuzz: An Interpretability-Driven Technique
    for Detecting Poisoned Samples in NLP*. [https://doi.org/10.48550/arXiv.2308.02122](https://doi.org/10.48550/arXiv.2308.02122)
    arXiv:2308.02122'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 (2023) Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu
    Shen 和 Xiangyu Zhang. 2023. *ParaFuzz：一种以可解释性为驱动的检测 NLP 中毒样本的技术*。 [https://doi.org/10.48550/arXiv.2308.02122](https://doi.org/10.48550/arXiv.2308.02122)
    arXiv:2308.02122
- en: Yang et al. (2023b) Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing
    Lu, and Shui Yu. 2023b. *A Comprehensive Overview of Backdoor Attacks in Large
    Language Models within Communication Networks*. [https://doi.org/10.48550/arXiv.2308.14367](https://doi.org/10.48550/arXiv.2308.14367)
    arXiv:2308.14367
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023b) Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing
    Lu, 和 Shui Yu. 2023b. *通信网络中大语言模型的全面后门攻击概述*。 [https://doi.org/10.48550/arXiv.2308.14367](https://doi.org/10.48550/arXiv.2308.14367)
    arXiv:2308.14367
- en: 'Yang et al. (2024b) Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining
    Xie. 2024b. *V-IRL: Grounding Virtual Intelligence in Real Life*. [https://doi.org/10.48550/arXiv.2402.03310](https://doi.org/10.48550/arXiv.2402.03310)
    arXiv:2402.03310'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2024b) Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, 和 Saining
    Xie. 2024b. *V-IRL: 将虚拟智能与现实生活结合*。 [https://doi.org/10.48550/arXiv.2402.03310](https://doi.org/10.48550/arXiv.2402.03310)
    arXiv:2402.03310'
- en: Yang et al. (2024a) Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou,
    and Xu Sun. 2024a. *Watch Out for Your Agents! Investigating Backdoor Threats
    to LLM-Based Agents*. [https://doi.org/10.48550/arXiv.2402.11208](https://doi.org/10.48550/arXiv.2402.11208)
    arXiv:2402.11208
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024a) Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou,
    和 Xu Sun. 2024a. *小心你的代理人！调查针对基于LLM的代理的后门威胁*。 [https://doi.org/10.48550/arXiv.2402.11208](https://doi.org/10.48550/arXiv.2402.11208)
    arXiv:2402.11208
- en: 'Yang et al. (2023a) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023a.
    *MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action*. [https://doi.org/10.48550/arXiv.2303.11381](https://doi.org/10.48550/arXiv.2303.11381)
    arXiv:2303.11381'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023a) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, 和 Lijuan Wang. 2023a.
    *MM-REACT：引导ChatGPT进行多模态推理和行动*。 [https://doi.org/10.48550/arXiv.2303.11381](https://doi.org/10.48550/arXiv.2303.11381)
    arXiv:2303.11381
- en: 'Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models. In *Proceedings of the 37th International
    Conference on Neural Information Processing Systems* (Red Hook, NY, USA) *(NIPS
    ’23)*. Curran Associates Inc., 11809–11822.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, 和 Karthik Narasimhan. 2024. 思维树：与大语言模型进行深思熟虑的问题解决。在 *第37届国际神经信息处理系统会议论文集*（红钩，NY，USA）
    *(NIPS ’23)*。Curran Associates Inc., 11809–11822。
- en: 'Yao et al. (2023) Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun,
    and Yue Zhang. 2023. *A Survey on Large Language Model (LLM) Security and Privacy:
    The Good, the Bad, and the Ugly*. [https://doi.org/10.48550/arXiv.2312.02003](https://doi.org/10.48550/arXiv.2312.02003)
    arXiv:2312.02003'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023) Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun,
    和 Yue Zhang. 2023. *大语言模型（LLM）安全与隐私调查：优点、缺点与挑战*。 [https://doi.org/10.48550/arXiv.2312.02003](https://doi.org/10.48550/arXiv.2312.02003)
    arXiv:2312.02003
- en: Ye et al. (2024) Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi,
    Sheng Shen, Wanlei Zhou, and Minhui Xue. 2024. *Reinforcement Unlearning*. [https://doi.org/10.48550/arXiv.2312.15910](https://doi.org/10.48550/arXiv.2312.15910)
    arXiv:2312.15910
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2024) Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi,
    Sheng Shen, Wanlei Zhou, 和 Minhui Xue. 2024. *强化遗忘*。 [https://doi.org/10.48550/arXiv.2312.15910](https://doi.org/10.48550/arXiv.2312.15910)
    arXiv:2312.15910
- en: Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman,
    Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. *Benchmarking and Defending Against
    Indirect Prompt Injection Attacks on Large Language Models*. [https://doi.org/10.48550/arXiv.2312.14197](https://doi.org/10.48550/arXiv.2312.14197)
    arXiv:2312.14197
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman,
    Guangzhong Sun, Xing Xie, 和 Fangzhao Wu. 2023. *基准测试和防御针对大语言模型的间接提示注入攻击*。 [https://doi.org/10.48550/arXiv.2312.14197](https://doi.org/10.48550/arXiv.2312.14197)
    arXiv:2312.14197
- en: Yin et al. (2023a) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. 2023a. A Survey on Multimodal Large Language Models. *arXiv
    preprint arXiv:2306.13549* (2023). [https://doi.org/10.48550/ARXIV.2306.13549](https://doi.org/10.48550/ARXIV.2306.13549)
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2023a) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, 和 Enhong Chen. 2023a. 多模态大语言模型的调查。 *arXiv 预印本 arXiv:2306.13549* (2023)。 [https://doi.org/10.48550/ARXIV.2306.13549](https://doi.org/10.48550/ARXIV.2306.13549)
- en: 'Yin et al. (2023b) Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang,
    Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023b. *Woodpecker:
    Hallucination Correction for Multimodal Large Language Models*. [https://doi.org/10.48550/arXiv.2310.16045](https://doi.org/10.48550/arXiv.2310.16045)
    arXiv:2310.16045'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yin et al. (2023b) Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang,
    Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, 和 Enhong Chen. 2023b. *Woodpecker:
    多模态大型语言模型的幻觉修正*。 [https://doi.org/10.48550/arXiv.2310.16045](https://doi.org/10.48550/arXiv.2310.16045)
    arXiv:2310.16045'
- en: Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A.
    Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,
    Sergey Yekhanin, and Huishuai Zhang. 2021. Differentially Private Fine-tuning
    of Language Models. In *International Conference on Learning Representations*.
    [https://openreview.net/forum?id=Q42f0dfjECO](https://openreview.net/forum?id=Q42f0dfjECO)
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin
    A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,
    Sergey Yekhanin, 和 Huishuai Zhang. 2021. 语言模型的差分隐私微调。收录于 *国际学习表征会议*。 [https://openreview.net/forum?id=Q42f0dfjECO](https://openreview.net/forum?id=Q42f0dfjECO)
- en: 'Yu et al. (2023) Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. 2023. *GPTFUZZER:
    Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts*. arXiv:2309.10253
    [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253)'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2023) Jiahao Yu, Xingwei Lin, Zheng Yu, 和 Xinyu Xing. 2023. *GPTFUZZER:
    使用自动生成的越狱提示进行大语言模型的红队测试*。 arXiv:2309.10253 [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253)'
- en: 'Zhai et al. (2024) Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer,
    Chunyuan Li, and Manling Li. 2024. *HallE-Control: Controlling Object Hallucination
    in Large Multimodal Models*. [https://doi.org/10.48550/arXiv.2310.01779](https://doi.org/10.48550/arXiv.2310.01779)
    arXiv:2310.01779'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhai et al. (2024) Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer,
    Chunyuan Li, 和 Manling Li. 2024. *HallE-Control: 控制大型多模态模型中的对象幻觉*。 [https://doi.org/10.48550/arXiv.2310.01779](https://doi.org/10.48550/arXiv.2310.01779)
    arXiv:2310.01779'
- en: Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu,
    Yong Jae Lee, and Yi Ma. 2023. *Investigating the Catastrophic Forgetting in Multimodal
    Large Language Models*. [https://doi.org/10.48550/arXiv.2309.10313](https://doi.org/10.48550/arXiv.2309.10313)
    arXiv:2309.10313
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu,
    Yong Jae Lee, 和 Yi Ma. 2023. *探究多模态大型语言模型中的灾难性遗忘*。 [https://doi.org/10.48550/arXiv.2309.10313](https://doi.org/10.48550/arXiv.2309.10313)
    arXiv:2309.10313
- en: 'Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang.
    2024. *InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated
    Large Language Model Agents*. [https://doi.org/10.48550/arXiv.2403.02691](https://doi.org/10.48550/arXiv.2403.02691)
    arXiv:2403.02691'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, 和 Daniel Kang. 2024.
    *InjecAgent: 基于工具的集成大型语言模型代理中间接提示注入的基准测试*。 [https://doi.org/10.48550/arXiv.2403.02691](https://doi.org/10.48550/arXiv.2403.02691)
    arXiv:2403.02691'
- en: 'Zhang et al. (2024b) Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang
    Wang. 2024b. *The Knowledge Alignment Problem: Bridging Human and External Knowledge
    for Large Language Models*. arXiv:2305.13669 [https://arxiv.org/abs/2305.13669](https://arxiv.org/abs/2305.13669)'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024b) Shuo Zhang, Liangming Pan, Junzhou Zhao, 和 William Yang
    Wang. 2024b. *知识对齐问题：弥合人类知识与外部知识的大型语言模型*。 arXiv:2305.13669 [https://arxiv.org/abs/2305.13669](https://arxiv.org/abs/2305.13669)
- en: 'Zhang et al. (2023a) Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya
    Zhang, Yanfeng Wang, and Weidi Xie. 2023a. *PMC-VQA: Visual Instruction Tuning
    for Medical Visual Question Answering*. [https://doi.org/10.48550/arXiv.2305.10415](https://doi.org/10.48550/arXiv.2305.10415)
    arXiv:2305.10415'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023a) Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya
    Zhang, Yanfeng Wang, 和 Weidi Xie. 2023a. *PMC-VQA: 医学视觉问答的视觉指令调整*。 [https://doi.org/10.48550/arXiv.2305.10415](https://doi.org/10.48550/arXiv.2305.10415)
    arXiv:2305.10415'
- en: Zhang et al. (2024a) Zhiping Zhang, Michelle Jia, Hao-Ping (Hank) Lee, Bingsheng
    Yao, Sauvik Das, Ada Lerner, Dakuo Wang, and Tianshi Li. 2024a. “It’s a Fair Game”,
    or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using
    LLM-Based Conversational Agents. In *Proceedings of the CHI Conference on Human
    Factors in Computing Systems* (New York, NY, USA) *(CHI ’24)*. Association for
    Computing Machinery, 1–26. [https://doi.org/10.1145/3613904.3642385](https://doi.org/10.1145/3613904.3642385)
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024a) Zhiping Zhang, Michelle Jia, Hao-Ping (Hank) Lee, Bingsheng
    Yao, Sauvik Das, Ada Lerner, Dakuo Wang, 和 Tianshi Li. 2024a. “这是一个公平的游戏”，还是不是？考察用户在使用基于LLM的对话代理时如何应对披露风险和利益。收录于
    *CHI计算机系统人因会议论文集* (纽约, NY, USA) *(CHI ’24)*。计算机协会，1–26。 [https://doi.org/10.1145/3613904.3642385](https://doi.org/10.1145/3613904.3642385)
- en: Zhang et al. (2023b) Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023b.
    Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization.
    *arXiv preprint arXiv:2311.09096* (2023). [https://doi.org/10.48550/ARXIV.2311.09096](https://doi.org/10.48550/ARXIV.2311.09096)
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023b) 张哲欣、杨俊晓、柯佩、黄敏磊。2023b。通过目标优先级来防御大语言模型的越狱攻击。*arXiv预印本 arXiv:2311.09096*（2023年）。
    [https://doi.org/10.48550/ARXIV.2311.09096](https://doi.org/10.48550/ARXIV.2311.09096)
- en: 'Zheng et al. (2023) Qingxiao Zheng, Zhongwei Xu, Abhinav Choudhary, Yuting
    Chen, Yongming Li, and Yun Huang. 2023. *Synergizing Human-AI Agency: A Guide
    of 23 Heuristics for Service Co-Creation with LLM-Based Agents*. [https://doi.org/10.48550/arXiv.2310.15065](https://doi.org/10.48550/arXiv.2310.15065)
    arXiv:2310.15065'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) 郑清晓、徐忠伟、阿比纳夫·乔杜里、陈雨婷、李永铭、黄云。2023年。*协同人类-人工智能代理：基于大语言模型的服务共同创造23条启发式指南*。
    [https://doi.org/10.48550/arXiv.2310.15065](https://doi.org/10.48550/arXiv.2310.15065)
    arXiv:2310.15065
- en: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2023. *MemoryBank: Enhancing Large Language Models with Long-Term Memory*.
    [https://doi.org/10.48550/arXiv.2305.10250](https://doi.org/10.48550/arXiv.2305.10250)
    arXiv:2305.10250'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. (2023) 说文中、郭良宏、高琪琪、叶赫、王燕林。2023年。*MemoryBank: 提升大语言模型的长期记忆*。 [https://doi.org/10.48550/arXiv.2305.10250](https://doi.org/10.48550/arXiv.2305.10250)
    arXiv:2305.10250'
- en: 'Zou et al. (2024) Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024.
    *PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of
    Large Language Models*. [https://doi.org/10.48550/arXiv.2402.07867](https://doi.org/10.48550/arXiv.2402.07867)
    arXiv:2402.07867'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou et al. (2024) 邹伟、耿润鹏、王冰辉、贾金元。2024年。*PoisonedRAG: 对大语言模型的检索增强生成进行知识毒化攻击*。
    [https://doi.org/10.48550/arXiv.2402.07867](https://doi.org/10.48550/arXiv.2402.07867)
    arXiv:2402.07867'
