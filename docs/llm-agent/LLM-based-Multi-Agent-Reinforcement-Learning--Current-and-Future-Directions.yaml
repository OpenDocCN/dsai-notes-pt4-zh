- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:46:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:46:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 LLM 的多智能体强化学习：当前与未来方向
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.11106](https://ar5iv.labs.arxiv.org/html/2405.11106)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.11106](https://ar5iv.labs.arxiv.org/html/2405.11106)
- en: Chuanneng Sun, , Songjun Huang, ,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 孙川能、黄松君、
- en: 'and Dario Pompili The authors are with the Department of Electrical and Computer
    Engineering, Rutgers University–New Brunswick, NJ, USA. Emails: *{chuanneng.sun,
    songjun.huang, pompili}@rutgers.edu* This work was supported by the NSF RTML Award
    No. CCF-1937403.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 和达里奥·蓬皮利 作者来自美国新泽西州罗格斯大学电气与计算机工程系。电子邮件：*{chuanneng.sun, songjun.huang, pompili}@rutgers.edu*
    本工作得到 NSF RTML 奖项 No. CCF-1937403 的支持。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent years, Large Language Models (LLMs) have shown great abilities in
    various tasks, including question answering, arithmetic problem solving, and poem
    writing, among others. Although research on LLM-as-an-agent has shown that LLM
    can be applied to Reinforcement Learning (RL) and achieve decent results, the
    extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many
    aspects, such as coordination and communication between agents, are not considered
    in the RL frameworks of a single agent. To inspire more research on LLM-based
    MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent
    RL frameworks and provide potential research directions for future research. In
    particular, we focus on the cooperative tasks of multiple agents with a common
    goal and communication among them. We also consider human-in/on-the-loop scenarios
    enabled by the language component in the framework.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）在各种任务中展示了出色的能力，包括问答、算术问题解决和诗歌创作等。尽管对 LLM 作为代理的研究表明 LLM 可以应用于强化学习（RL）并取得不错的结果，但将
    LLM 基础的 RL 扩展到多智能体系统（MAS）并非易事，因为许多方面，如智能体之间的协调和通信，在单智能体的 RL 框架中并未考虑。为了激发更多关于 LLM
    基础的 MARL 研究，在这封信中，我们调查了现有的 LLM 基础的单智能体和多智能体 RL 框架，并提供了未来研究的潜在方向。特别是，我们关注于多个智能体共同目标的合作任务及其间的通信。我们还考虑了通过框架中的语言组件实现的人机交互场景。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Multi-Agent Reinforcement Learning, Language Models, Multi-Agent Systems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体强化学习、语言模型、多智能体系统。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Multi-Agent Reinforcement Learning (MARL) has emerged as a popular approach
    to address the coordination problem in Multi-Agent Systems (MAS). As opposed to
    Individual Reinforcement Learning (IRL)-based or traditional optimization-based
    solutions, MARL has shown a significant improvement in scalability and robustness
    to uncertainty and dynamicity [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]. This improvement is largely attributed to the communication and
    coordination among agents inherent in MARL, where multiple agents learn and adapt
    their policies simultaneously while interacting within a shared environment and
    communicating with others. However, how and what to communicate among the agents
    in the MAS remains to be explored. Representative examples include MARL frameworks
    that learn to generate numerical messages using neural networks, formulate neural
    communication protocols, and learn targeted ad hoc communications. Despite the
    decent performance of the MARL frameworks achieved in various applications, they
    still underperform human experts. As a result, it is reasonable to think *why
    not leveraging human knowledge and human languages in MARL?*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体强化学习（MARL）作为一种流行的方法，已被用于解决多智能体系统（MAS）中的协调问题。与基于个体强化学习（IRL）或传统优化方法的解决方案不同，MARL
    在可扩展性和对不确定性及动态性的鲁棒性方面显示出显著的改善[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4)]。这一改善主要归功于 MARL 中智能体之间的通信和协调，其中多个智能体在共享环境中相互作用并同时学习和调整其策略。然而，在
    MAS 中，智能体之间如何及沟通内容仍待探索。代表性的例子包括使用神经网络生成数值消息、制定神经通信协议和学习针对性的临时通信的 MARL 框架。尽管这些
    MARL 框架在各种应用中表现良好，但仍不及人类专家。因此，*为什么不在 MARL 中利用人类知识和语言呢？*
- en: '![Refer to caption](img/39d534d6850a9901e939116f5cd37625.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39d534d6850a9901e939116f5cd37625.png)'
- en: 'Figure 1: Well-known Large Language Models (LLMs) over the past three years.
    Among them, only PaLM-E from Google is trained specifically for embodied applications,
    e.g., robot control.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：过去三年内著名的大型语言模型（LLMs）。其中，只有 Google 的 PaLM-E 是专门为具身应用训练的，例如机器人控制。
- en: As recent advances in Natural Language Processing (NLP) demonstrate great abilities
    in multi-modal tasks, language-conditioned MARL becomes a promising research problem.
    NLP has been an active research topic for decades and many famous models have
    been proposed for language modeling such as Recurrent Neural Network (RNN) [[5](#bib.bib5),
    [6](#bib.bib6)], Long-Short Term Memory networks (LSTM) [[7](#bib.bib7)], and
    transformers [[8](#bib.bib8)]. These foundational models have greatly improved
    the ability of machines to understand and generate human language, setting the
    stage for more complex applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言处理（NLP）的最新进展在多模态任务中表现出色，语言条件 MARL 成为一个有前景的研究问题。NLP 已经成为一个活跃的研究领域，提出了许多著名的语言建模模型，如递归神经网络（RNN）[[5](#bib.bib5),
    [6](#bib.bib6)]，长短期记忆网络（LSTM）[[7](#bib.bib7)]，以及变压器模型[[8](#bib.bib8)]。这些基础模型大大提高了机器理解和生成自然语言的能力，为更复杂的应用奠定了基础。
- en: 'In recent years, the integration of NLP with single-agent RL has led to the
    development of language-conditioned RL frameworks [[9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)], especially as Large Language Models (LLMs) [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] emerged as the rising star
    in the artificial intelligence community (see Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions"))
    and has been successfully applied in various fields [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18)]. Pre-trained LLMs contain general human knowledge about the
    world and can easily adapt to RL problems without the need for retraining. This
    integration not only leverages the semantic richness of language but also allows
    for the dynamic adjustment of agent behaviors based on linguistic input. In particular,
    LLM is able to generate new information that it has not seen before on the basis
    of a few examples. For example, in Reflexion [[19](#bib.bib19)], the authors showed
    that the LLM agent could generate decent reflections on its decisions without
    any reward/feedback from the environment. Such capabilities are particularly valuable
    in multi-agent systems, where agents must coordinate and cooperate based on shared
    goals communicated through language.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，NLP 与单智能体 RL 的集成导致了语言条件 RL 框架的开发[[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]，特别是大型语言模型（LLMs）[[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] 作为人工智能领域的明星应运而生（见图 [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ LLM-based Multi-Agent Reinforcement Learning: Current
    and Future Directions)"))，并在各个领域取得了成功应用[[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]。预训练的
    LLM 包含关于世界的通用知识，并可以轻松适应 RL 问题，无需重新训练。这种集成不仅利用了语言的语义丰富性，还允许基于语言输入动态调整智能体行为。特别是，LLM
    能够在少量示例的基础上生成其之前未见过的新信息。例如，在 Reflexion [[19](#bib.bib19)] 中，作者展示了 LLM 智能体能够在没有来自环境的奖励/反馈的情况下对其决策生成合理的反思。这种能力在多智能体系统中特别有价值，在这些系统中，智能体必须根据通过语言传达的共享目标进行协调和合作。'
- en: Due to the need for communication and coordination, the problem of MARL becomes
    more complex than simply multiplying the RL of a single agent by the number of
    agents. As opposed to conventional MARL, LLMs-based MARL can leverage linguistic
    cues to facilitate inter-agent communication and collaboration, further boosting
    system performance. For example, agents can use shared language to negotiate roles,
    coordinate actions, or exchange information about the environment or their internal
    states, thereby aligning their objectives more effectively. This language-enhanced
    coordination becomes critical in complex scenarios where agents must handle ambiguous
    or evolving tasks that require continual communication and mutual understanding.
    The exploration of these capabilities opens up new possibilities for designing
    more intelligent and flexible multi-agent systems capable of operating in unpredictable,
    real-world environments.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要沟通和协调，MARL 问题变得比简单地将单个智能体的 RL 乘以智能体数量更复杂。与传统的 MARL 相对，基于 LLM 的 MARL 可以利用语言线索来促进智能体之间的沟通与协作，进一步提升系统性能。例如，智能体可以使用共享语言来协商角色、协调行动或交换关于环境或内部状态的信息，从而更有效地对齐其目标。在复杂的场景中，这种语言增强的协调变得至关重要，因为智能体必须处理需要持续沟通和相互理解的模糊或不断变化的任务。这些能力的探索为设计更智能、更灵活的多智能体系统提供了新的可能性，使其能够在不可预测的现实环境中操作。
- en: Guo et al. [[20](#bib.bib20)] reviewed LLM-based multi-agent frameworks, but
    the emphasis of that paper was not on MARL. Unlike their paper, this letter focuses
    more on the MAS that tries to accomplish a task cooperatively. In addition to
    that, there are several surveys on the topic of MARL [[21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23)] and single agent LLM-based RL [[24](#bib.bib24), [25](#bib.bib25)],
    but none of them is dedicated to LLM-based MARL. Therefore, *we claim that we
    are among the first to provide a systematic overview of the LLM-based MARL problem
    and provide potential future research directions.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Guo 等人[[20](#bib.bib20)] 综述了 LLM 基础的多代理框架，但该论文的重点不在于 MARL。与他们的论文不同，本信函更侧重于那些尝试协作完成任务的
    MAS。除此之外，还有若干关于 MARL [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)] 和单代理
    LLM 基础 RL [[24](#bib.bib24), [25](#bib.bib25)] 的综述，但没有一篇专注于 LLM 基础的 MARL。因此，*我们声称我们是首批提供
    LLM 基础的 MARL 问题的系统性概述并提供潜在未来研究方向的文献之一。*
- en: 'The remainder of this letter is organized as follows. We first introduce the
    problem of MARL and provide a brief overview of conventional, i.e., non-LLM-based,
    MARL, and single-agent LLM-based RL, in Sect. [II](#S2 "II Preliminaries ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions"). Then, we
    will survey the existing LLM-based MARL frameworks in Sect. [III](#S3 "III Existing
    LLM-based MARL ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future
    Directions"). After that, we will discuss the challenges and future research directions
    for this field in Sect. [IV](#S4 "IV Open Research Problems ‣ LLM-based Multi-Agent
    Reinforcement Learning: Current and Future Directions"). Finally, we will conclude
    the letter in Sect. [V](#S5 "V Conclusion ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '本信函的其余部分组织如下。我们首先介绍 MARL 的问题，并在 Sect. [II](#S2 "II Preliminaries ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions") 中简要概述传统的，即非
    LLM 基础的 MARL 和单代理 LLM 基础的 RL。接着，我们将在 Sect. [III](#S3 "III Existing LLM-based MARL
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")
    中调查现有的 LLM 基础的 MARL 框架。之后，我们将在 Sect. [IV](#S4 "IV Open Research Problems ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions") 讨论该领域的挑战和未来研究方向。最后，我们将在
    Sect. [V](#S5 "V Conclusion ‣ LLM-based Multi-Agent Reinforcement Learning: Current
    and Future Directions") 总结本信函。'
- en: II Preliminaries
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 预备知识
- en: 'In this section, we will first introduce the problem of MARL (Sect. [II-A](#S2.SS1
    "II-A MARL Problem Definition ‣ II Preliminaries ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions")). Then, we will briefly discuss conventional
    non-LLM-based MARL in Sect. [II-B](#S2.SS2 "II-B Traditional MARL ‣ II Preliminaries
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions").
    To prepare the ground for LLM-based MARL, we will introduce LLM-based single-agent
    RL in Sect. [II-C](#S2.SS3 "II-C LLM-based Single-Agent RL ‣ II Preliminaries
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将首先介绍 MARL 的问题（Sect. [II-A](#S2.SS1 "II-A MARL Problem Definition ‣
    II Preliminaries ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future
    Directions")）。然后，我们将在 Sect. [II-B](#S2.SS2 "II-B Traditional MARL ‣ II Preliminaries
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")
    中简要讨论传统的非 LLM 基础 MARL。为了为 LLM 基础的 MARL 做准备，我们将在 Sect. [II-C](#S2.SS3 "II-C LLM-based
    Single-Agent RL ‣ II Preliminaries ‣ LLM-based Multi-Agent Reinforcement Learning:
    Current and Future Directions") 中介绍 LLM 基础的单代理 RL。'
- en: II-A MARL Problem Definition
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A MARL 问题定义
- en: MARL can be modeled with the Decentralized Partially Observable Markov Decision
    Process (Dec-POMDP) [[26](#bib.bib26)], an extension to a multi-agent manner of
    the Markov Decision Process (MDP). An MDP for $N$ is the total time length. A
    key difference between Dec-POMDP and normal MDP is the partial observability,
    i.e., for one agent, the actions of other agents and the subsequent outcomes are
    not directly observable, thereby increasing the difficulty of solving the problem.
    Due to this partial observability, individual uncoordinated learning frameworks
    will not work well. Typical deep MARL frameworks adopt the actor-critic structure,
    where actors are trained to output the action given the observation, and the critics
    output a score to judge whether these actions are good in the long-term horizon.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: MARL 可以通过去中心化部分可观察马尔可夫决策过程（Dec-POMDP）[[26](#bib.bib26)]来建模，这是对马尔可夫决策过程（MDP）的一种多智能体扩展。一个
    N 的 MDP 是总时间长度。Dec-POMDP 与普通 MDP 的一个主要区别是部分可观察性，即对于一个智能体，其他智能体的动作及其随后的结果不可直接观察，从而增加了解决问题的难度。由于这种部分可观察性，个体无协调学习框架效果不好。典型的深度
    MARL 框架采用演员-评论家结构，其中演员被训练在给定观察的情况下输出动作，评论家则输出一个分数来判断这些动作在长期视角下是否良好。
- en: II-B Traditional MARL
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 传统 MARL
- en: 'To solve the problem of Dec-POMDP, many frameworks have been proposed. These
    frameworks can be roughly categorized into two classes: learning-to-cooperate
    and learning-to-communicate.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 Dec-POMDP 的问题，已经提出了许多框架。这些框架大致可以分为两类：学习合作和学习通信。
- en: 'Learning to coordinate: The first kind of approach, such as QMIX [[27](#bib.bib27)],
    QTRAN [[28](#bib.bib28)], MADDPG [[29](#bib.bib29)], MAPPO [[30](#bib.bib30)],
    and many others [[31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)], assumes that through centralized training
    with ideal communication, agents can learn to work with each other during the
    centralized training; therefore, communication is not needed during execution.
    In other words, these approaches expect the agents to learn to adapt to other
    agents’ behavior patterns. These approaches can also be classified as policy-based
    and value-based approaches. Policy-based approaches typically adopt the actor-critic
    architecture where actors are trained to make decisions, and critics approximate
    the long-term return and provide feedback to the actors. Value-based approaches
    learn optimized joint Q values given the team’s observations and actions. A problem
    that often happens in this situation is the credit assignment problem, where the
    critic needs to determine the contribution of each agent to the performance.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 协调学习：第一种方法，如 QMIX [[27](#bib.bib27)]、QTRAN [[28](#bib.bib28)]、MADDPG [[29](#bib.bib29)]、MAPPO
    [[30](#bib.bib30)] 及其他 [[31](#bib.bib31)、[32](#bib.bib32)、[33](#bib.bib33)、[34](#bib.bib34)、[35](#bib.bib35)、[36](#bib.bib36)]，假设通过理想通信的集中训练，智能体可以在集中训练过程中学会彼此协作；因此，在执行过程中不需要通信。换句话说，这些方法期望智能体学会适应其他智能体的行为模式。这些方法也可以分为基于策略和基于价值的方法。基于策略的方法通常采用演员-评论家架构，其中演员被训练以做出决策，评论家则近似长期回报并向演员提供反馈。基于价值的方法则学习在给定团队的观察和行动的情况下优化联合
    Q 值。在这种情况下，通常会出现信用分配问题，即评论家需要确定每个智能体对性能的贡献。
- en: 'Learning to communicate: In communication-based approaches, agents are equipped
    with the capability to share information through various means, such as adjusting
    the content of the shared messages [[37](#bib.bib37)] or optimizing the structure
    of the communication network [[38](#bib.bib38)]. This explicit inter-agent communication
    facilitates coordinated strategies and is crucial in dynamic environments where
    conditions and objectives may frequently change [[39](#bib.bib39), [40](#bib.bib40)].
    Effective communication enables agents to form coalitions to achieve common goals,
    adapt to peers’ actions, and optimize collective outcomes, improving system performance
    in tasks ranging from cooperative manipulation to competitive strategic games [[37](#bib.bib37)].
    Protocols for communication, often learned during training, leverage advanced
    techniques such as differentiable interagent learning algorithms, which refine
    communication patterns based on environmental feedback [[41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43)]. In addition, frameworks for learning emergent communication
    protocols/languages have also been proposed [[44](#bib.bib44), [45](#bib.bib45)].
    These frameworks encourage the agents to learn a certain “language” that is understandable
    by other agents and encodes certain information.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 学习沟通：在基于沟通的方法中，代理被赋予通过各种方式共享信息的能力，例如调整共享信息的内容[[37](#bib.bib37)]或优化通信网络的结构[[38](#bib.bib38)]。这种明确的代理间沟通促进了协调策略，并在条件和目标可能频繁变化的动态环境中至关重要[[39](#bib.bib39),
    [40](#bib.bib40)]。有效的沟通使代理能够形成联盟以实现共同目标，适应同行的行动，并优化集体结果，从而提高系统在从合作操控到竞争性战略游戏等任务中的表现[[37](#bib.bib37)]。沟通协议，通常在训练期间学习，利用先进的技术如可微分的代理间学习算法，这些算法根据环境反馈改进沟通模式[[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]。此外，也提出了学习突现沟通协议/语言的框架[[44](#bib.bib44), [45](#bib.bib45)]。这些框架鼓励代理学习一种其他代理可以理解并编码某些信息的“语言”。
- en: II-C LLM-based Single-Agent RL
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 基于LLM的单代理RL
- en: As LLMs demonstrated their abilities in various tasks, several LLM-based decision-making
    frameworks have been proposed. These frameworks are not necessarily RL frameworks
    because many of them are open-loop, meaning that the feedback/reward from the
    environment is not used during the decision-making process. Instead, many frameworks
    simply leverage the generalizability of LLMs and the general knowledge they contain
    to solve problems. Typically, in these works, a few examples of how the LLMs are
    expected to solve the problem are provided, and the LLMs can generalize from these
    examples to new problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）在各种任务中展示其能力，已经提出了几种基于LLM的决策框架。这些框架不一定是强化学习（RL）框架，因为其中许多是开环的，这意味着在决策过程中不会使用来自环境的反馈/奖励。相反，许多框架只是利用LLM的泛化能力及其包含的一般知识来解决问题。通常，在这些工作中，会提供一些LLM预期如何解决问题的示例，并且LLM可以从这些示例中泛化到新问题。
- en: 'Open-loop LLM-based RL: Among these frameworks, we will summarize some significant
    contributions. Yao et al. [[46](#bib.bib46)] proposed ReAct, in which the LLM
    is prompted to generate “thoughts” to solve the problem given the observation,
    allowing the model to dynamically adjust and refine its strategies in response
    to changing environmental cues and task demands. Based on ReAct, Shinn et al. [[19](#bib.bib19)]
    proposed Reflexion, which uses a few-shot verbal feedback to enhance decision-making
    capabilities. Reflexion processes feedback from interactions within task environments
    into textual summaries, which are then used to augment the model’s episodic memory.
    Prasad et al. [[47](#bib.bib47)] proposed ADaPT, where LLMs learn to decompose
    the task into subtasks through short examples. Although these approaches can achieve
    decent performances in reasoning or word-based games, they are constrained by
    the knowledge the LLMs have and could be biased for certain problems. More importantly,
    the reward, one of the most important signals from the environment, is not considered.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 开环LLM基础的RL：在这些框架中，我们将总结一些重要贡献。Yao等人 [[46](#bib.bib46)] 提出了ReAct，其中LLM被提示生成“思考”以解决给定观察到的问题，使模型能够根据环境线索和任务需求的变化动态调整和改进其策略。基于ReAct，Shinn等人 [[19](#bib.bib19)]
    提出了Reflexion，该方法使用少量样本的语言反馈来增强决策能力。Reflexion将任务环境中的交互反馈处理为文本摘要，然后用来增强模型的情节记忆。Prasad等人 [[47](#bib.bib47)]
    提出了ADaPT，LLMs通过短例子学习将任务分解为子任务。尽管这些方法在推理或基于词语的游戏中能实现良好的性能，但它们受限于LLMs拥有的知识，并且可能对某些问题存在偏见。更重要的是，它们没有考虑奖励，这是来自环境的最重要信号之一。
- en: 'Closed-loop LLM-based RL: There are also LLM-based RL frameworks that incorporate
    feedback for closed-loop control. Paul et al. [[48](#bib.bib48)] proposed Refiner,
    in which a fine-tuned LLM is used to provide feedback on policy decisions. Zhang
    et al. [[49](#bib.bib49)] introduced a framework that uses feedback from LLMs
    to enhance credit assignment in RL tasks. Their work targeted sparse reward environments
    and leveraged the rich domain knowledge available in LLMs to dynamically generate
    and refine reward functions. To improve sample efficiency, the authors proposed
    sequential, tree-based, and moving target feedback, facilitating more targeted
    exploration and reducing redundancy in state exploration. Yao et al. [[50](#bib.bib50)]
    proposed Retroformer, where a frozen LLM is used as the policy, while another
    smaller LM is trained to provide verbal feedback on the decisions based on the
    reward. Murthy et al. [[51](#bib.bib51)] proposed REX, adopting the Monte-Carlo
    Tree Search (MCTS) algorithm as the basis to solve problems. The Upper Confidence
    Bound (UCB) technique is adopted to guide the agent’s exploration.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 闭环LLM基础的RL：也有一些LLM基础的RL框架结合了反馈进行闭环控制。Paul等人 [[48](#bib.bib48)] 提出了Refiner，其中一个经过微调的LLM用于对策略决策提供反馈。Zhang等人 [[49](#bib.bib49)]
    介绍了一个框架，使用来自LLMs的反馈来增强RL任务中的信用分配。他们的工作针对稀疏奖励环境，并利用LLMs中丰富的领域知识动态生成和改进奖励函数。为了提高样本效率，作者提出了顺序、基于树的和移动目标反馈，以促进更有针对性的探索并减少状态探索中的冗余。Yao等人 [[50](#bib.bib50)]
    提出了Retroformer，其中一个冻结的LLM被用作策略，而另一个较小的LM被训练以根据奖励对决策提供语言反馈。Murthy等人 [[51](#bib.bib51)]
    提出了REX，采用Monte-Carlo树搜索（MCTS）算法作为解决问题的基础。使用上置信界限（UCB）技术来指导智能体的探索。
- en: Besides the aforementioned work that uses LLMs as RL policies, multi-modal LLMs
    that are trained on RL tasks such as robot control (e.g., PaLM-E [[52](#bib.bib52)])
    and models for grounding languages to actions [[53](#bib.bib53), [54](#bib.bib54)]
    have also been proposed. These models can achieve decent zero-shot performances
    in several robotic tasks because of their parameter scale.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的将LLMs用作RL策略的工作之外，还提出了在机器人控制（例如，PaLM-E [[52](#bib.bib52)]）等RL任务上训练的多模态LLMs，以及将语言与动作相结合的模型 [[53](#bib.bib53),
    [54](#bib.bib54)]。由于这些模型的参数规模，它们可以在多个机器人任务中实现良好的零-shot性能。
- en: III Existing LLM-based MARL
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 现有的LLM基础的MARL
- en: 'TABLE I: Existing LLM for MARL frameworks with an emphasis on multi-agent coordination.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：现有LLM用于MARL框架，重点关注多智能体协调。
- en: '| Framework | Application | Dataset/Simulator | Training | LLM Role |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 应用 | 数据集/模拟器 | 训练 | LLM角色 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| DyLAN [[55](#bib.bib55)] | Reasoning, Coding | [MATH](https://github.com/hendrycks/math/),
    [MMLU](https://github.com/hendrycks/test) [[56](#bib.bib56), [57](#bib.bib57)];
    [HumanEval](https://github.com/openai/human-eval) [[58](#bib.bib58)] | ✗ | Decision,
    Communication |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| DyLAN [[55](#bib.bib55)] | 推理、编码 | [MATH](https://github.com/hendrycks/math/),
    [MMLU](https://github.com/hendrycks/test) [[56](#bib.bib56), [57](#bib.bib57)];
    [HumanEval](https://github.com/openai/human-eval) [[58](#bib.bib58)] | ✗ | 决策、沟通
    |'
- en: '| FAMA [[59](#bib.bib59)] | Text Game, Driving | [BabyAI-Text](https://github.com/flowersteam/Grounding_LLMs_with_online_RL/tree/main/babyai-text),
    Traffic Junction [[39](#bib.bib39)] | ✓ | Decision, Communication |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| FAMA [[59](#bib.bib59)] | 文本游戏、驾驶 | [BabyAI-Text](https://github.com/flowersteam/Grounding_LLMs_with_online_RL/tree/main/babyai-text),
    交通交汇 [[39](#bib.bib39)] | ✓ | 决策、沟通 |'
- en: '| Chen et al. [[60](#bib.bib60)] | Consensus Seeking | [Generated Data](https://github.com/WestlakeIntelligentRobotics/ConsensusLLM-code/releases/tag/v1.0.1)
    | ✗ | Decision |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Chen 等人 [[60](#bib.bib60)] | 共识寻求 | [生成的数据](https://github.com/WestlakeIntelligentRobotics/ConsensusLLM-code/releases/tag/v1.0.1)
    | ✗ | 决策 |'
- en: '| Li et al. [[61](#bib.bib61)] | Path Planning | Close-source simulator | ✗
    | Decision, Communication, Theory of Mind |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Li 等人 [[61](#bib.bib61)] | 路径规划 | 闭源模拟器 | ✗ | 决策、沟通、心智理论 |'
- en: '| CoELA [[62](#bib.bib62)] | Multi-Agent Planning | [TDW-MAT](https://github.com/threedworld-mit/tdw),
    [C-WAH](https://github.com/xavierpuigf/watch_and_help) [[63](#bib.bib63)] | ✓
    | Decision, Communication, Memory |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| CoELA [[62](#bib.bib62)] | 多智能体规划 | [TDW-MAT](https://github.com/threedworld-mit/tdw),
    [C-WAH](https://github.com/xavierpuigf/watch_and_help) [[63](#bib.bib63)] | ✓
    | 决策、沟通、记忆 |'
- en: '| SMART-LLM [[64](#bib.bib64)] | Multi-Agent Planning | [Proposed Benchmark
    Dataset](https://github.com/SMARTlab-Purdue/SMART-LLM/tree/master/data) | ✗ |
    Decision, Planning |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SMART-LLM [[64](#bib.bib64)] | 多智能体规划 | [建议的基准数据集](https://github.com/SMARTlab-Purdue/SMART-LLM/tree/master/data)
    | ✗ | 决策、规划 |'
- en: '| RoCo [[65](#bib.bib65)] | Motion Planning | [RoCoBench](https://github.com/MandiZhao/robot-collab/tree/main/rocobench)
    | ✗ | Decision, Planning |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| RoCo [[65](#bib.bib65)] | 运动规划 | [RoCoBench](https://github.com/MandiZhao/robot-collab/tree/main/rocobench)
    | ✗ | 决策、规划 |'
- en: '| Co-NavGPT [[66](#bib.bib66)] | Semantic Navigation | [Habitat-Matterport
    3D](https://aihabitat.org/datasets/hm3d/) [[67](#bib.bib67)] | ✗ | Planning |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Co-NavGPT [[66](#bib.bib66)] | 语义导航 | [Habitat-Matterport 3D](https://aihabitat.org/datasets/hm3d/) [[67](#bib.bib67)]
    | ✗ | 规划 |'
- en: '| Guo et al. [[68](#bib.bib68)] | Multi-Agent Cooperation | [VirtualHome-Social](http://virtual-home.org/)
    | ✗ | Decision, Communication |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Guo 等人 [[68](#bib.bib68)] | 多智能体合作 | [VirtualHome-Social](http://virtual-home.org/)
    | ✗ | 决策、沟通 |'
- en: '| MetaGPT [[69](#bib.bib69)] | Coding | [HumanEval](https://github.com/openai/human-eval) [[58](#bib.bib58)],
    [MBPP](https://github.com/google-research/google-research/tree/master/mbpp) [[70](#bib.bib70)]
    | ✗ | Code Generation, Communication |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MetaGPT [[69](#bib.bib69)] | 编码 | [HumanEval](https://github.com/openai/human-eval) [[58](#bib.bib58)],
    [MBPP](https://github.com/google-research/google-research/tree/master/mbpp) [[70](#bib.bib70)]
    | ✗ | 代码生成、沟通 |'
- en: Although LLM-based MARL frameworks have not been widely studied, there is still
    some work focused on this topic.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于 LLM 的 MARL 框架尚未被广泛研究，但仍有一些工作专注于这一主题。
- en: 'MARL for problem solving: Huang et al. [[71](#bib.bib71)] introduced $\gamma$-Bench,
    which encompasses a variety of multi-agent games to assess these models. Their
    work included a detailed analysis of different versions of the GPT models, which
    demonstrated a systematic improvement in their game ability. This framework demonstrated
    the enhanced performance of newer LLM versions, such as GPT-4, and the potential
    to augment these models with reasoning techniques such as CoT. Liu et al. [[55](#bib.bib55)]
    proposed Dynamic LLM-Agent Network (DyLAN), a framework that studied the capabilities
    of LLM-agent collaborations for complex reasoning and code generation tasks. Unlike
    previous methods that used static architectures, DyLAN dynamically adjusted agent
    interactions based on real-time performance and task demands, incorporating features
    such as inference-time agent selection and an early stopping mechanism. This allowed
    DyLAN to enhance computational efficiency and optimize the contribution of individual
    agents through an unsupervised scoring metric, the agent importance score. Slumbers
    et al. [[59](#bib.bib59)] introduced the Functionally-Aligned Multi-Agents (FAMA)
    framework by integrating a centralized critic architecture and allowing natural
    language communication between agents. The framework aligns LLMs to the functional
    needs of the environment through an online fine-tuning process, which adjusts
    the LLM’s pre-trained knowledge to better fit the specific task requirements.
    Additionally, FAMA allows for intuitive inter-agent communication in natural language,
    making the coordination more efficient and human-interpretable. Chen et al. [[60](#bib.bib60)]
    present a study on the dynamics of consensus seeking in multi-agent systems driven
    by LLMs. The authors focused on the inter-agent negotiation processes, where each
    agent starts with a unique numerical state and negotiates to reach a unified consensus.
    They also provided insights on how different factors, such as agent personality
    (stubborn vs. suggestible), agent number, and network topology, influence the
    negotiation and consensus process. Li et al. [[61](#bib.bib61)] explored Theory
    of Mind (ToM) modeling with LLMs generating communication messages and beliefs
    about the environment and other agents. Hong et al. [[69](#bib.bib69)] proposed
    MetaGPT, where agents share messages with all other agents in a message pool and
    agents can subscribe to messages related to their task.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: MARL 用于问题解决：黄等人[[71](#bib.bib71)] 介绍了 $\gamma$-Bench，这是一种涵盖各种多智能体游戏以评估这些模型的工具。他们的工作包括对不同版本
    GPT 模型的详细分析，显示了它们在游戏能力上的系统性改进。这个框架展示了较新 LLM 版本（如 GPT-4）的性能提升，以及通过推理技术如 CoT 增强这些模型的潜力。刘等人[[55](#bib.bib55)]
    提出了动态 LLM-智能体网络（DyLAN），这是一个研究 LLM-智能体协作在复杂推理和代码生成任务中能力的框架。与使用静态架构的先前方法不同，DyLAN
    根据实时性能和任务需求动态调整智能体交互，融入了如推理时智能体选择和早期停止机制等特性。这使得 DyLAN 能够通过无监督评分指标（智能体重要性评分）提高计算效率和优化单个智能体的贡献。Slumbers
    等人[[59](#bib.bib59)] 通过集成中心化的评论架构并允许智能体之间的自然语言通信，引入了功能对齐多智能体（FAMA）框架。该框架通过在线微调过程将
    LLM 对齐到环境的功能需求，调整 LLM 的预训练知识以更好地适应特定任务需求。此外，FAMA 允许自然语言中的直观智能体间通信，使协调更加高效且易于理解。陈等人[[60](#bib.bib60)]
    研究了由 LLM 驱动的多智能体系统中共识寻求的动态。作者重点关注智能体间的谈判过程，每个智能体从一个独特的数值状态开始，并进行谈判以达成统一的共识。他们还提供了不同因素（如智能体个性（固执与易变）、智能体数量和网络拓扑）如何影响谈判和共识过程的见解。李等人[[61](#bib.bib61)]
    探索了具有 LLM 生成通信信息和对环境及其他智能体的信念的心智理论（ToM）建模。洪等人[[69](#bib.bib69)] 提出了 MetaGPT，其中智能体将消息共享到所有其他智能体的消息池中，智能体可以订阅与其任务相关的消息。
- en: 'MARL for embodied applications: Other than the aforementioned MARL frameworks
    for problem solving, there are also LLM-based MARL frameworks for embodied application.
    Zhang et al. [[62](#bib.bib62)] proposed a Cooperative Embodied Language Agent (CoELA),
    a modular framework that integrates LLM to improve communication and collaborative
    decision-making among multiple agents. The modular structure includes a perception
    module for interpreting sensory data, a memory module for retaining and recalling
    environmental and task-related information, a communication module to facilitate
    inter-agent dialogue, a planning module for strategic decision making, and an
    execution module for carrying out planned actions. By incorporating LLMs into
    the memory, communication, and planning modules, the framework enables agents
    to utilize natural language to improve both understanding and execution of cooperative
    tasks. Kannan et al. [[64](#bib.bib64)] introduced SMART-LLM, a framework that
    integrated LLM with multi-agent robot task planning to translate high-level instructions
    into executable strategies for robot teams. By structuring task planning into
    sequential phases of decomposition, coalition formation, and allocation, SMART-LLM
    generates robot actions to achieve complex objectives. Their approach leveraged
    the cognitive processing power of LLMs to enhance the comprehension and execution
    capabilities of robot systems. Mandi et al. [[65](#bib.bib65)] introduced RoCo,
    a multi-robot arm collaboration framework with each arm equipped with an LLM agent.
    The LLM agents are responsible for coordination among agents by communicating
    with other LLM agents and path planning. Yu et al. [[66](#bib.bib66)] introduced
    Co-NavGPT, an LLM-based multi-agent navigation framework. However, unlike other
    frameworks where multiple LLMs are employed, in Co-NavGPT, only one LLM is used
    to assign frontiers to agents globally. Guo et al. [[68](#bib.bib68)] studied
    the collaboration of multiple LLM-based agents on various tasks with a focus on
    communication and coordination among multiple agents. They proposed the Criticize-Reflect
    method with an LLM critic and an LLM coordinator. Table [I](#S3.T1 "TABLE I ‣
    III Existing LLM-based MARL ‣ LLM-based Multi-Agent Reinforcement Learning: Current
    and Future Directions") provides more details on these works.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'MARL 在体现应用中的应用：除了上述用于问题解决的 MARL 框架，还有基于 LLM 的 MARL 框架用于体现应用。张等人 [[62](#bib.bib62)]
    提出了合作体现语言代理（CoELA），这是一个模块化框架，集成了 LLM 以改善多个代理之间的沟通和协作决策。该模块化结构包括用于解释感知数据的感知模块、用于保留和回忆环境及任务相关信息的记忆模块、促进代理之间对话的通信模块、用于战略决策的规划模块和执行规划行动的执行模块。通过将
    LLM 融入记忆、通信和规划模块，该框架使代理能够利用自然语言提高合作任务的理解和执行能力。坎南等人 [[64](#bib.bib64)] 引入了 SMART-LLM，这是一个将
    LLM 与多代理机器人任务规划集成的框架，用于将高层指令转换为机器人团队可以执行的策略。通过将任务规划结构化为分解、联盟形成和分配的顺序阶段，SMART-LLM
    生成机器人动作以实现复杂目标。他们的方法利用了 LLM 的认知处理能力，增强了机器人系统的理解和执行能力。曼迪等人 [[65](#bib.bib65)] 引入了
    RoCo，一个多机器人臂协作框架，每个臂都配备了 LLM 代理。LLM 代理负责通过与其他 LLM 代理沟通和路径规划来协调代理之间的工作。于等人 [[66](#bib.bib66)]
    引入了 Co-NavGPT，这是一个基于 LLM 的多代理导航框架。然而，与其他使用多个 LLM 的框架不同，在 Co-NavGPT 中，仅使用一个 LLM
    全球分配代理的前沿。郭等人 [[68](#bib.bib68)] 研究了多个基于 LLM 的代理在各种任务中的协作，重点关注多个代理之间的沟通和协调。他们提出了带有
    LLM 批评者和 LLM 协调员的批评-反思方法。表格 [I](#S3.T1 "TABLE I ‣ III Existing LLM-based MARL
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")
    提供了更多关于这些工作的细节。'
- en: In addition to LLM-based MARL, several works explored multi-agent interaction [[72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74)], e.g., multi-agent conversation and gaming.
    However, these works fall out of the MARL scope; we will not use too much space
    on them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于 LLM 的 MARL，一些研究探讨了多代理交互 [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)]，例如多代理对话和游戏。然而，这些研究超出了
    MARL 的范围；我们将不会在这些方面占用过多篇幅。
- en: Overall, these studies illustrated that while the exploration into language-conditioned
    MARL is still nascent, it holds considerable promise for advancing the capabilities
    of MAS. Using natural language, these systems can achieve higher levels of coordination
    and understanding, which is essential for complex environments.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些研究表明，虽然对语言条件 MARL 的探索仍处于初期阶段，但它在提升 MAS 能力方面具有相当大的潜力。通过使用自然语言，这些系统可以实现更高水平的协调和理解，这对于复杂环境至关重要。
- en: IV Open Research Problems
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 开放研究问题
- en: 'Despite the research efforts mentioned above, language-conditioned MARL is
    still an unexplored field with many unexplored aspects. To inspire more research
    in this field, we provide several research directions in this section. Specifically,
    we discuss four potential research directions: i) *personality-enabled cooperation*
    (Sect. [IV-A](#S4.SS1 "IV-A Personality-enabled Cooperation ‣ IV Open Research
    Problems ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")),
    ii) *language-enabled human-in/on-the-loop frameworks* (Sect. [IV-B](#S4.SS2 "IV-B
    Language-enabled Human-in/on-the-Loop Frameworks ‣ IV Open Research Problems ‣
    LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")),
    iii) *traditional MARL and LLM co-design* (Sect. [IV-C](#S4.SS3 "IV-C Traditional
    MARL and LLM Co-Design ‣ IV Open Research Problems ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions")), and iv) *safety and security in MAS*
    (Sect. [IV-D](#S4.SS4 "IV-D Safety and Security in MAS ‣ IV Open Research Problems
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")).
    Fig. [2](#S4.F2 "Figure 2 ‣ IV Open Research Problems ‣ LLM-based Multi-Agent
    Reinforcement Learning: Current and Future Directions") also provides a more vivid
    demonstration of these research ideas.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管上述研究工作有所进展，语言条件下的MARL仍然是一个未被充分探索的领域，存在许多未被探讨的方面。为了激发更多的研究，我们在本节提供了若干研究方向。具体来说，我们讨论了四个潜在的研究方向：i）*个性化协作*（第[IV-A](#S4.SS1
    "IV-A Personality-enabled Cooperation ‣ IV Open Research Problems ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions")节），ii）*语言增强的人类在环/人类操作框架*（第[IV-B](#S4.SS2
    "IV-B Language-enabled Human-in/on-the-Loop Frameworks ‣ IV Open Research Problems
    ‣ LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions")节），iii）*传统MARL与LLM协同设计*（第[IV-C](#S4.SS3
    "IV-C Traditional MARL and LLM Co-Design ‣ IV Open Research Problems ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions")节），以及iv）*MAS中的安全性和保密性*（第[IV-D](#S4.SS4
    "IV-D Safety and Security in MAS ‣ IV Open Research Problems ‣ LLM-based Multi-Agent
    Reinforcement Learning: Current and Future Directions")节）。图[2](#S4.F2 "Figure
    2 ‣ IV Open Research Problems ‣ LLM-based Multi-Agent Reinforcement Learning:
    Current and Future Directions")还提供了这些研究想法的更生动的展示。'
- en: '|  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| (a) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| (a) |'
- en: '![Refer to caption](img/04148aa91d97033ea8babb243035547f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04148aa91d97033ea8babb243035547f.png)'
- en: (a)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '|  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| (b) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| (b) |'
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| (c) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| (c) |'
- en: '![Refer to caption](img/1db16954559667b117927c94d625013f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1db16954559667b117927c94d625013f.png)'
- en: (b)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/23bf7362fee5493a133e742777a6b601.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/23bf7362fee5493a133e742777a6b601.png)'
- en: (c)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 2: Potential research directions for language-conditioned Multi-Agent
    Reinforcement Learning (MARL). (a) Personality-enabled cooperation, where different
    robots have different personalities defined by the commands. (b) Language-enabled
    human-on-the-loop frameworks, where humans supervise robots and provide feedback.
    (c) Traditional co-design of MARL and LLM, where knowledge about different aspects
    of LLM is distilled into smaller models that can be executed on board.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：语言条件下的多智能体强化学习（MARL）潜在研究方向。（a）个性化协作，其中不同的机器人具有由命令定义的不同个性。（b）语言增强的人类参与框架，人类监督机器人并提供反馈。（c）传统的MARL与LLM协同设计，其中关于LLM不同方面的知识被提炼成较小的模型，这些模型可以在设备上执行。
- en: IV-A Personality-enabled Cooperation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 个性化协作
- en: Previous work [[75](#bib.bib75), [60](#bib.bib60)] has shown that different
    personalities in MARL frameworks can produce promising results. This idea can
    be naturally extended to language-conditioned MARL frameworks. In these frameworks,
    agents are distinguished by their assigned personalities. For example, an agent
    with a “curious” personality will tend to explore the environment, while an agent
    with a “conservative” personality will tend to stay in the safe areas. A team
    of agents with a combination of different personalities can often achieve better
    performance than those with the same personality. In traditional MARL frameworks,
    these personalities are encoded in the agents’ model parameters, i.e., the weights
    of their models. However, with LLMs as agents, personalities can be assigned to
    agents by prompts, in which narratives about the agent’s personality will be provided.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的工作[[75](#bib.bib75), [60](#bib.bib60)]已经表明，在MARL框架中，不同的个性可以产生有前景的结果。这个想法可以自然地扩展到语言条件下的MARL框架中。在这些框架中，智能体通过其分配的个性来区分。例如，具有“好奇”个性的智能体往往会探索环境，而具有“保守”个性的智能体则倾向于待在安全区域。具有不同个性的智能体团队通常能比具有相同个性的团队获得更好的表现。在传统的MARL框架中，这些个性被编码在智能体的模型参数中，即其模型的权重。然而，使用LLM作为智能体时，可以通过提示将个性分配给智能体，其中将提供关于智能体个性的叙述。
- en: Another potential advantage of language-conditioned MARL with personalized agents
    is the ability to handle conflicts and negotiate solutions more effectively. Agents
    can be trained to understand and generate language-based responses that consider
    the perspectives and goals of other agents, facilitating a negotiation process
    that mirrors human interaction. This capability is particularly useful in scenarios
    where agents must share resources or decide on joint actions that impact the collective
    outcome.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化代理的语言条件下MARL的另一个潜在优势是能够更有效地处理冲突和谈判解决方案。代理可以被训练以理解和生成基于语言的响应，考虑其他代理的观点和目标，从而促进类似于人类互动的谈判过程。这一能力在代理必须共享资源或决定影响集体结果的联合行动的情境中特别有用。
- en: However, implementing these personalized language behaviors in agents presents
    several challenges. The primary concern is ensuring that language models do not
    perpetuate or amplify undesirable biases that could lead to unfair or inefficient
    outcomes. Additionally, the complexity of training such models increases as they
    must not only understand and generate appropriate responses, but also adapt their
    linguistic style based on the evolving context of the interaction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在代理中实施这些个性化语言行为存在若干挑战。主要的问题是确保语言模型不会延续或放大可能导致不公平或低效结果的偏见。此外，训练这些模型的复杂性增加，因为它们不仅必须理解和生成适当的响应，还必须根据互动的不断变化的背景调整其语言风格。
- en: Future research could focus on developing frameworks that can effectively integrate
    personality-driven language models into MARL systems. This integration involves
    creating robust prompts with memories that encode the information from past experiences
    in a wide range of interactive scenarios, allowing agents to learn from both their
    successes and failures. Furthermore, evaluating these systems will require new
    metrics that can assess not just the efficacy of task performance but also the
    appropriateness and effectiveness of communication between agents.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究可以集中在开发能够有效整合个性驱动的语言模型到MARL系统中的框架。这种整合涉及创建具有记忆的稳健提示，这些记忆编码了来自广泛互动场景的过往经验信息，使代理能够从成功和失败中学习。此外，评估这些系统将需要新的指标，这些指标不仅可以评估任务执行的有效性，还可以评估代理之间沟通的适当性和有效性。
- en: Another direction of research is to explore competitive agents instead of cooperative
    agents. However, the competition here should be benign, which means that the agents
    compete to achieve the same goal. By addressing these challenges, language-conditioned
    MARL with diverse agent personalities has the potential to advance the field of
    artificial intelligence.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个研究方向是探索竞争型代理而非合作型代理。然而，这里的竞争应该是善意的，这意味着代理竞争以实现相同的目标。通过解决这些挑战，具有多样化代理个性的语言条件下的MARL有可能推动人工智能领域的发展。
- en: IV-B Language-enabled Human-in/on-the-Loop Frameworks
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 语言支持的人类在环/环上框架
- en: One of the direct advantages of language-conditioned MARL frameworks is the
    possibility of involving humans in or on the loop. To illustrate, human-in-the-loop
    frameworks [[76](#bib.bib76), [77](#bib.bib77), [78](#bib.bib78)] involve humans
    as agents that can generate actions to affect the environment, while human-on-the-loop
    frameworks [[79](#bib.bib79)] regard humans as supervisors without directly being
    involved in the decision-making process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 语言条件下的多智能体强化学习（MARL）框架的直接优势之一是可能涉及人类参与其中或在其周围。举例来说，人类在环框架 [[76](#bib.bib76),
    [77](#bib.bib77), [78](#bib.bib78)] 将人类视为可以生成行动以影响环境的代理，而人类在环框架 [[79](#bib.bib79)]
    则将人类视为监督者，但不直接参与决策过程。
- en: In human-in-the-loop setups, humans actively participate in the learning process,
    often providing corrective feedback or rewards to shape agent behaviors in real
    time. This direct interaction helps in refining the agent’s actions and strategies,
    making them more aligned with human-like reasoning and ethical standards. For
    example, a human could guide an agent away from potential pitfalls in its learning
    process that might not be immediately apparent through algorithmic reinforcement
    signals alone. On the other hand, human-on-the-loop frameworks play a crucial
    oversight role. Here, humans monitor the system’s performance and intervene only
    when necessary. This approach is particularly valuable in applications where autonomous
    operations are preferable, but human oversight is necessary to ensure safety and
    compliance with regulatory standards. For example, in autonomous driving, while
    the system can handle most driving tasks, a human supervisor may only need to
    intervene in complex or hazardous road conditions, ensuring that the system operates
    within safe limits without requiring constant human control.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在人机协作的设置中，人类积极参与学习过程，通常提供纠正反馈或奖励，以实时塑造智能体的行为。这种直接互动有助于细化智能体的行动和策略，使其更符合人类的推理方式和伦理标准。例如，人类可以引导智能体远离在学习过程中可能出现的潜在陷阱，这些陷阱通过算法强化信号可能不易察觉。另一方面，人机监控框架则扮演着关键的监督角色。在这里，人类监控系统的性能，仅在必要时进行干预。这种方法在需要自动操作但又需要人类监督以确保安全和符合监管标准的应用中尤其有价值。例如，在自动驾驶中，虽然系统可以处理大多数驾驶任务，但在复杂或危险的道路条件下，人类监督员可能只需在必要时干预，以确保系统在安全限度内操作，而无需持续的人类控制。
- en: Both of these human roles within language-conditioned MARL can benefit significantly
    from the integration of natural language. Language serves as a versatile interface
    that enables clearer and more intuitive communication between humans and agents.
    Agents can report their status, explain their decisions, or even ask for clarification
    in human-understandable language, improving the effectiveness of human interventions.
    Furthermore, the use of language can facilitate the transfer of knowledge between
    agents by allowing them to share insights or strategies in a comprehensible format.
    In scenarios involving multiple agents with varying roles, language can help maintain
    coherence and unity of purpose across the team, guiding less experienced agents
    through complex tasks or strategies articulated by more experienced ones or even
    by human supervisors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言条件的MARL中，这两种人类角色都可以从自然语言的整合中显著受益。语言作为一种多功能界面，使得人类与智能体之间的沟通更加清晰和直观。智能体可以用人类可理解的语言报告其状态、解释决策，甚至请求澄清，从而提高人类干预的有效性。此外，语言的使用可以通过允许智能体以易于理解的格式共享见解或策略，促进智能体之间知识的传递。在涉及多个角色的智能体场景中，语言可以帮助维持团队的连贯性和目标一致性，引导经验较少的智能体通过由经验丰富的智能体或甚至人类监督员阐述的复杂任务或策略。
- en: Future research could explore optimizing these interactions between human supervisors
    and agents, possibly by developing advanced language models that can understand
    and generate more context-aware, situation-specific dialogue. Furthermore, ensuring
    that language-based communications are not only informative, but also prompt and
    actionable will be crucial for the practical deployment of such systems in real-world
    applications. This balance between automation and human oversight, facilitated
    by natural language, promises to enhance the robustness and reliability of multi-agent
    systems, pushing the boundaries of what automated systems can achieve while ensuring
    they operate under safe and ethical guidelines.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究可以探索优化这些人类监督者与智能体之间的互动，可能通过开发先进的语言模型，这些模型能够理解并生成更具上下文感知和情况特定的对话。此外，确保基于语言的沟通不仅信息丰富，还能够迅速并具有可操作性，对实际部署这些系统在现实世界应用中至关重要。这种通过自然语言促进的自动化与人类监督之间的平衡，有望增强多智能体系统的稳健性和可靠性，推动自动化系统所能实现的边界，同时确保它们在安全和伦理的指导下运行。
- en: IV-C Traditional MARL and LLM Co-Design
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 传统MARL与LLM协同设计
- en: 'Since LLMs tend to have large sizes, especially those pre-trained models, performing
    inference on-board on robot hardware is not practical. A popular way towards resource-efficient
    computing is through Parameter-Efficient Fine-Tuning (PEFT) techniques [[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)] combined with quantization.
    However, this kind of approach still requires inference through the large LLM
    network, which is impractical for small robots. To make this happen, we envision
    a co-design framework of traditional MARL policies and the LM models. A typical
    design for such systems could be to use the LLM model as a centralized critic
    to guide the training of the actors. This design follows the CTDE scheme introduced
    in Sect. [II-B](#S2.SS2 "II-B Traditional MARL ‣ II Preliminaries ‣ LLM-based
    Multi-Agent Reinforcement Learning: Current and Future Directions"), where the
    critic will be removed during execution. To leverage communication during execution,
    we can distill the knowledge from the LLMs about communication into smaller models
    that can be executed onboard.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '由于LLMs往往体积庞大，尤其是那些预训练模型，在机器人硬件上进行推理是不现实的。实现资源高效计算的一种流行方式是通过参数高效微调（PEFT）技术[[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83)]结合量化。然而，这种方法仍然需要通过大型LLM网络进行推理，这对小型机器人来说不切实际。为了实现这一目标，我们设想了一个传统MARL政策与LLM模型的协同设计框架。这类系统的典型设计可以是将LLM模型用作集中式评论员来指导演员的训练。这种设计遵循在第[II-B](#S2.SS2
    "II-B Traditional MARL ‣ II Preliminaries ‣ LLM-based Multi-Agent Reinforcement
    Learning: Current and Future Directions")节中介绍的CTDE方案，其中评论员将在执行过程中被移除。为了在执行期间利用通信，我们可以将LLM中关于通信的知识蒸馏到可以在板上执行的小型模型中。'
- en: One potential development is the refinement of the distillation process, which
    aims to transfer knowledge from LLMs to more compact models suitable for deployment
    on less powerful hardware, such as robots or Internet of Things (IoT) devices.
    A promising direction in this direction would be in-context distillation [[84](#bib.bib84),
    [85](#bib.bib85)], where the teacher model is an LLM with a pre-defined context.
    For example, for controlling warehouse robots, the context can be refined to tell
    the LLM to avoid people and collisions. By focusing on the essential features
    necessary for the communication and decision-making learned by the LLM, smaller
    models can execute complex tasks effectively with a fraction of the computational
    overhead. In addition, to facilitate effective communication between agents during
    execution, specialized communication protocols could be designed. These protocols
    would utilize the distilled models to ensure that critical information, as understood
    and processed by the LLM during the training phase, is efficiently conveyed between
    agents. This approach not only conserves bandwidth, but also optimizes the real-time
    decision-making process, allowing for dynamic adjustments based on the operational
    environment and agent states.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一种潜在的发展是改进蒸馏过程，其目的是将知识从大型语言模型（LLMs）转移到更为精简的模型，以适用于在功率较低的硬件上进行部署，如机器人或物联网（IoT）设备。在这方面一个有前景的方向是上下文蒸馏[[84](#bib.bib84),
    [85](#bib.bib85)]，其中教师模型是一个具有预定义上下文的LLM。例如，对于控制仓库机器人，可以将上下文精炼为指示LLM避免人员和碰撞。通过专注于LLM在沟通和决策中学到的关键特征，更小的模型可以以较少的计算开销有效地执行复杂任务。此外，为了在执行期间促进代理之间的有效沟通，可以设计专门的通信协议。这些协议将利用蒸馏后的模型，确保在训练阶段由LLM理解和处理的关键信息在代理之间高效传达。这种方法不仅节省了带宽，还优化了实时决策过程，使其能够根据操作环境和代理状态进行动态调整。
- en: Additionally, the co-design framework can be enhanced by integrating adaptive
    mechanisms that allow the MARL system to recalibrate its strategies based on feedback
    from the operational environment. Such adaptive systems could dynamically adjust
    the compression level of the distilled models or modify the communication protocols
    based on the complexity of the tasks and the computational capabilities available
    at that time. This flexibility would be particularly useful in environments where
    conditions change rapidly or unpredictably, requiring swift responses from the
    agent collective. Furthermore, the implementation of this co-design framework
    would benefit significantly from the development of specialized hardware tailored
    to the execution of compressed models. This hardware could optimize the execution
    of neural network operations, potentially in a power-efficient manner, which is
    critical for mobile or embedded systems.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过整合适应性机制以允许MARL系统根据操作环境的反馈重新校准其策略，可以增强协同设计框架。这种适应性系统可以根据任务的复杂性和当时可用的计算能力，动态调整蒸馏模型的压缩级别或修改通信协议。这种灵活性在环境条件变化迅速或不可预测的情况下尤为有用，要求代理集体做出快速响应。此外，实施这一协同设计框架将显著受益于专门为压缩模型执行而量身定制的硬件开发。这种硬件可以优化神经网络操作的执行，可能以节能的方式进行，这对于移动或嵌入式系统至关重要。
- en: IV-D Safety and Security in MAS
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D MAS中的安全性和保障
- en: Ensuring the safety and security of MAS is critical, especially as these systems
    are increasingly deployed in diverse and potentially high-stakes environments.
    The integration of language models into MARL introduces unique challenges and
    vulnerabilities, from the manipulation of agent communication to the exploitation
    of model biases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 确保MAS的安全性和保障至关重要，特别是随着这些系统在多样化和潜在高风险环境中的部署越来越多。将语言模型集成到MARL中引入了独特的挑战和脆弱性，从代理通信的操控到模型偏见的利用。
- en: Many robotic operations have continuous action spaces, where the output of each
    agent’s policy is a set of continuous values. Unlike discrete action spaces, which
    can be reformulated as multi-choice problems and solved by prompting the multi-choice
    question to the LLM, continuous action space is more tricky, especially in high-stake
    environments, for example, operation robots. Existing methods replace the last
    few layers of the LLMs with new layers that map the observation in languages to
    continuous action spaces. However, this kind of approach requires training the
    new layers in the desired environment, which might be inaccessible. Therefore,
    exploring alternative methods for integrating LLMs into the control loop of robots
    operating in continuous action spaces without the need for substantial retraining
    or modification of the LLMs is promising.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器人操作具有连续动作空间，其中每个代理的策略输出是一组连续值。与可以重构为多选问题并通过向LLM提示多选问题来解决的离散动作空间不同，连续动作空间更加棘手，尤其是在高风险环境中，例如操作机器人。现有方法用新的层替换LLM的最后几层，将观察语言映射到连续动作空间。然而，这种方法需要在所需环境中训练新层，而这些环境可能无法接触。因此，探索将LLM集成到操作在连续动作空间中的机器人控制环中的替代方法，而无需大量重新训练或修改LLM，是一种有前景的方向。
- en: In addition to safety in actions, safety and security against potential attacks
    are also crucial in MAS. One way towards safety is through proactive measures.
    This includes the development of secure communication protocols between agents
    to prevent eavesdropping or the injection of malicious data that could lead to
    compromised decision-making. Communications encryption can be a fundamental aspect
    of this, ensuring that even if data transmissions are intercepted, the information
    remains protected. In addition, securing the language model training process against
    adversarial attacks is crucial. Adversarial training, which involves exposing
    the system to a wide range of attack vectors during the training phase, can help
    models learn to resist or mitigate these attacks in deployment. In addition, input
    validation techniques can be employed to filter out potentially harmful or misleading
    inputs that could cause the system to behave unpredictably. This is particularly
    important in scenarios where agents interact with humans or systems outside the
    controlled environment and are exposed to a broader range of language inputs and
    behaviors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除了行动中的安全性外，对潜在攻击的安全性和保障在MAS中也至关重要。通向安全的一种方法是采取主动措施。这包括开发代理之间的安全通信协议，以防止窃听或注入恶意数据，这些数据可能导致决策受损。通信加密可以成为其中的一个基本方面，确保即使数据传输被拦截，信息仍然得到保护。此外，保护语言模型训练过程免受对抗攻击也是至关重要的。对抗训练涉及在训练阶段将系统暴露于广泛的攻击向量中，这可以帮助模型在部署时学习抵抗或减轻这些攻击。此外，可以采用输入验证技术过滤掉潜在的有害或误导性输入，这些输入可能导致系统行为不可预测。这在代理与人类或受控环境外的系统互动，并接触到更广泛的语言输入和行为时尤为重要。
- en: Despite the best proactive defenses, systems may still encounter unforeseen
    vulnerabilities post-deployment. Thus, reactive strategies are necessary to quickly
    address any breaches or failures. This can involve real-time monitoring of agent
    behaviors and communications to detect anomalies that may indicate a security
    breach or a failure in safety protocols. Once an anomaly is detected, the systems
    should be able to isolate affected agents and roll back their states to secure
    configurations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有最好的主动防御，系统在部署后仍可能遇到意想不到的漏洞。因此，反应策略是必要的，以便迅速应对任何安全漏洞或失败。这可能涉及对代理行为和通信的实时监控，以检测可能表明安全漏洞或安全协议失败的异常。一旦检测到异常，系统应能够隔离受影响的代理，并将其状态回滚到安全配置。
- en: V Conclusion
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: In this letter, we provide a brief overview of Multi-Agent Reinforcement Learning
    (MARL) based on conventional non-Large Language Model (LLM)-based Multi-Agent
    Reinforcement Learning (MARL), LLM-based single-agent RL, and existing LLM-based
    MARL frameworks. These works paved the way for new ideas that we discuss in later
    sections. Specifically, we discussed potential research directions ranging from
    multi-agent personality to safety and security in the LLM-based Multi-Agent System (MAS).
    Although works are studying LLM-based MARL, the field is still to be explored
    and has significant potential because of the great ability of LLMs and their in-context
    and interpretable nature. With LLMs, designing MARL frameworks becomes more analogous
    to modeling the group learning process of animals or even humans, where knowledge
    is transferred or exchanged via natural languages. We hope, with this letter,
    that more research works can be enlightened and the boundary of multi-agent intelligence
    could be pushed further.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这封信中，我们对基于传统非大语言模型（LLM）的多智能体强化学习（MARL）、基于LLM的单智能体RL，以及现有的基于LLM的MARL框架进行了简要概述。这些工作为我们在后续章节讨论的新想法奠定了基础。具体来说，我们讨论了从多智能体个性到基于LLM的多智能体系统（MAS）中的安全和保障等潜在研究方向。尽管已有研究在探索基于LLM的MARL，但该领域仍待深入开发，因LLM的巨大能力及其上下文和可解释性自然具有重要潜力。借助LLM，设计MARL框架变得更类似于模拟动物甚至人类的群体学习过程，其中知识通过自然语言传递或交流。我们希望通过这封信，能激发更多的研究工作，并推动多智能体智能的边界进一步拓展。
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] C. Sun, S. Huang, and D. Pompili, “Hmaac: Hierarchical multi-agent actor-critic
    for aerial search with explicit coordination modeling,” in 2023 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 7728–7734, IEEE, 2023.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] C. Sun, S. Huang, 和 D. Pompili，“Hmaac: Hierarchical multi-agent actor-critic
    for aerial search with explicit coordination modeling，” 见于2023 IEEE国际机器人与自动化会议（ICRA），第7728–7734页，IEEE，2023年。'
- en: '[2] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, multi-agent, reinforcement
    learning for autonomous driving,” arXiv preprint arXiv:1610.03295, 2016.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Shalev-Shwartz, S. Shammah, 和 A. Shashua, “安全的多智能体强化学习用于自主驾驶，” arXiv
    预印本 arXiv:1610.03295，2016年。'
- en: '[3] V. Sadhu, C. Sun, A. Karimian, R. Tron, and D. Pompili, “Aerial-deepsearch:
    Distributed multi-agent deep reinforcement learning for search missions,” in 2020
    IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),
    pp. 165–173, IEEE, 2020.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] V. Sadhu, C. Sun, A. Karimian, R. Tron, 和 D. Pompili, “Aerial-deepsearch：分布式多智能体深度强化学习用于搜索任务，”
    在 2020 IEEE 第 17 届国际移动自组网与传感器系统会议 (MASS) 上，第 165–173 页，IEEE，2020年。'
- en: '[4] J. A. Calvo and I. Dusparic, “Heterogeneous multi-agent deep reinforcement
    learning for traffic lights control.,” in AICS, pp. 2–13, 2018.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. A. Calvo 和 I. Dusparic, “异质多智能体深度强化学习用于交通灯控制。” 在 AICS，第 2–13 页，2018年。'
- en: '[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal representations
    by error propagation, parallel distributed processing, explorations in the microstructure
    of cognition, ed. de rumelhart and j. mcclelland. vol. 1\. 1986,” Biometrika,
    vol. 71, pp. 599–607, 1986.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. E. Rumelhart, G. E. Hinton, 和 R. J. Williams, “通过错误传播学习内部表征，平行分布处理，认知微观结构探索，编者：de
    rumelhart 和 j. mcclelland，第 1 卷，1986年，” Biometrika，第 71 卷，第 599–607 页，1986年。'
- en: '[6] M. I. Jordan, “Serial order: A parallel distributed processing approach,”
    in Advances in psychology, vol. 121, pp. 471–495, Elsevier, 1997.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. I. Jordan, “序列顺序：一种平行分布处理方法，” 在《心理学进展》，第 121 卷，第 471–495 页，Elsevier，1997年。'
- en: '[7] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] S. Hochreiter 和 J. Schmidhuber, “长短期记忆，” 神经计算，第 9 卷，第 8 期，第 1735–1780 页，1997年。'
- en: '[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural
    information processing systems, vol. 30, 2017.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, “注意力机制全靠你，” 神经信息处理系统进展，第 30 卷，2017年。'
- en: '[9] S. Peng, X. Hu, R. Zhang, J. Guo, Q. Yi, R. Chen, Z. Du, L. Li, Q. Guo,
    and Y. Chen, “Conceptual reinforcement learning for language-conditioned tasks,”
    in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 9426–9434,
    2023.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Peng, X. Hu, R. Zhang, J. Guo, Q. Yi, R. Chen, Z. Du, L. Li, Q. Guo,
    和 Y. Chen, “面向语言条件任务的概念强化学习，” 在 AAAI 人工智能会议论文集中，第 37 卷，第 9426–9434 页，2023年。'
- en: '[10] Y. Jiang, S. S. Gu, K. P. Murphy, and C. Finn, “Language as an abstraction
    for hierarchical deep reinforcement learning,” Advances in Neural Information
    Processing Systems, vol. 32, 2019.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Jiang, S. S. Gu, K. P. Murphy, 和 C. Finn, “语言作为层次深度强化学习的抽象，” 神经信息处理系统进展，第
    32 卷，2019年。'
- en: '[11] L. Zhou and K. Small, “Inverse reinforcement learning with natural language
    goals,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35,
    pp. 11116–11124, 2021.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Zhou 和 K. Small, “带有自然语言目标的逆向强化学习，” 在 AAAI 人工智能会议论文集中，第 35 卷，第 11116–11124
    页，2021年。'
- en: '[12] OpenAI, “ChatGPT: Optimizing Language Models for Dialogue.” [https://www.openai.com/chatgpt](https://www.openai.com/chatgpt),
    2023. Accessed: 2024-04-22.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] OpenAI, “ChatGPT: 优化对话的语言模型。” [https://www.openai.com/chatgpt](https://www.openai.com/chatgpt)，2023年。访问时间：2024-04-22。'
- en: '[13] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等，“Llama 2：开放基础和微调的聊天模型，” arXiv 预印本
    arXiv:2307.09288，2023年。'
- en: '[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham,
    H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling language modeling
    with pathways,” Journal of Machine Learning Research, vol. 24, no. 240, pp. 1–113,
    2023.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P.
    Barham, H. W. Chung, C. Sutton, S. Gehrmann 等，“Palm：通过路径扩展语言建模，” 机器学习研究杂志，第 24
    卷，第 240 期，第 1–113 页，2023年。'
- en: '[15] R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk,
    A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable multimodal models,”
    arXiv preprint arXiv:2312.11805, 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk,
    A. M. Dai, A. Hauth 等，“Gemini：一系列高效的多模态模型，” arXiv 预印本 arXiv:2312.11805，2023年。'
- en: '[16] J. Wu, Z. Lai, S. Chen, R. Tao, P. Zhao, and N. Hovakimyan, “The new agronomists:
    Language models are experts in crop management,” arXiv preprint arXiv:2403.19839,
    2024.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Wu, Z. Lai, S. Chen, R. Tao, P. Zhao, 和 N. Hovakimyan, “新农业专家：语言模型在作物管理中的专家，”
    arXiv 预印本 arXiv:2403.19839，2024年。'
- en: '[17] Z. Lai, J. Wu, S. Chen, Y. Zhou, A. Hovakimyan, and N. Hovakimyan, “Language
    models are free boosters for biomedical imaging tasks,” arXiv preprint arXiv:2403.17343,
    2024.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Z. Lai, J. Wu, S. Chen, Y. Zhou, A. Hovakimyan, 和 N. Hovakimyan, “语言模型是生物医学影像任务的免费助推器，”
    arXiv 预印本 arXiv:2403.17343, 2024。'
- en: '[18] G. Han, W. Liu, X. Huang, and B. Borsari, “Chain-of-interaction: Enhancing
    large language models for psychiatric behavior understanding by dyadic contexts,”
    arXiv preprint arXiv:2403.13786, 2024.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] G. Han, W. Liu, X. Huang, 和 B. Borsari, “交互链：通过双向上下文增强大规模语言模型在精神行为理解中的效果，”
    arXiv 预印本 arXiv:2403.13786, 2024。'
- en: '[19] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion:
    Language agents with verbal reinforcement learning,” Advances in Neural Information
    Processing Systems, vol. 36, 2024.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, 和 S. Yao, “Reflexion：具有语言强化学习的语言代理，”
    神经信息处理系统进展，vol. 36, 2024。'
- en: '[20] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and
    X. Zhang, “Large language model based multi-agents: A survey of progress and challenges,”
    arXiv preprint arXiv:2402.01680, 2024.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, 和
    X. Zhang, “基于大规模语言模型的多智能体：进展与挑战的调查，” arXiv 预印本 arXiv:2402.01680, 2024。'
- en: '[21] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multiagent systems: A review of challenges, solutions, and applications,”
    IEEE transactions on cybernetics, vol. 50, no. 9, pp. 3826–3839, 2020.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] T. T. Nguyen, N. D. Nguyen, 和 S. Nahavandi, “多智能体系统的深度强化学习：挑战、解决方案和应用的综述，”
    IEEE 网络控制学报，vol. 50, no. 9, pp. 3826–3839, 2020。'
- en: '[22] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
    of multiagent deep reinforcement learning,” Autonomous Agents and Multi-Agent
    Systems, vol. 33, no. 6, pp. 750–797, 2019.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] P. Hernandez-Leal, B. Kartal, 和 M. E. Taylor, “多智能体深度强化学习的调查与批评，” 自主代理与多智能体系统，vol.
    33, no. 6, pp. 750–797, 2019。'
- en: '[23] S. Gronauer and K. Diepold, “Multi-agent deep reinforcement learning:
    a survey,” Artificial Intelligence Review, pp. 1–49, 2022.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Gronauer 和 K. Diepold, “多智能体深度强化学习：综述，” 人工智能评论，pp. 1–49, 2022。'
- en: '[24] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas, E. Grefenstette,
    S. Whiteson, and T. Rocktäschel, “A survey of reinforcement learning informed
    by natural language,” arXiv preprint arXiv:1906.03926, 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Luketina, N. Nardelli, G. Farquhar, J. Foerster, J. Andreas, E. Grefenstette,
    S. Whiteson, 和 T. Rocktäschel, “自然语言启发的强化学习调查，” arXiv 预印本 arXiv:1906.03926, 2019。'
- en: '[25] Y. Cao, H. Zhao, Y. Cheng, T. Shu, G. Liu, G. Liang, J. Zhao, and Y. Li,
    “Survey on large language model-enhanced reinforcement learning: Concept, taxonomy,
    and methods,” arXiv preprint arXiv:2404.00282, 2024.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Cao, H. Zhao, Y. Cheng, T. Shu, G. Liu, G. Liang, J. Zhao, 和 Y. Li,
    “关于大规模语言模型增强强化学习的调查：概念、分类和方法，” arXiv 预印本 arXiv:2404.00282, 2024。'
- en: '[26] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized
    POMDPs, vol. 1. Springer, 2016.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] F. A. Oliehoek, C. Amato, 等，简明的去中心化 POMDPs 介绍，vol. 1. Springer, 2016。'
- en: '[27] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and
    S. Whiteson, “Monotonic value function factorisation for deep multi-agent reinforcement
    learning,” The Journal of Machine Learning Research, vol. 21, no. 1, pp. 7234–7284,
    2020.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, 和 S.
    Whiteson, “深度多智能体强化学习的单调值函数因式分解，” 机器学习研究杂志，vol. 21, no. 1, pp. 7234–7284, 2020。'
- en: '[28] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi, “Qtran: Learning
    to factorize with transformation for cooperative multi-agent reinforcement learning,”
    in International conference on machine learning, pp. 5887–5896, PMLR, 2019.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] K. Son, D. Kim, W. J. Kang, D. E. Hostallero, 和 Y. Yi, “Qtran: 通过变换学习因式分解以实现合作多智能体强化学习，”
    在国际机器学习会议上，pp. 5887–5896, PMLR, 2019。'
- en: '[29] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch,
    “Multi-agent actor-critic for mixed cooperative-competitive environments,” Advances
    in neural information processing systems, vol. 30, 2017.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, 和 I. Mordatch,
    “混合合作-竞争环境下的多智能体演员-评论家方法，” 神经信息处理系统进展，vol. 30, 2017。'
- en: '[30] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu, “The
    surprising effectiveness of ppo in cooperative multi-agent games,” Advances in
    Neural Information Processing Systems, vol. 35, pp. 24611–24624, 2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, 和 Y. Wu, “PPO
    在合作多智能体游戏中的惊人效果，” 神经信息处理系统进展，vol. 35, pp. 24611–24624, 2022。'
- en: '[31] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg,
    M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al., “Value-decomposition networks
    for cooperative multi-agent learning based on team reward,” in Proceedings of
    the 17th International Conference on Autonomous Agents and MultiAgent Systems,
    pp. 2085–2087, 2018.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg,
    M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls 等人，“基于团队奖励的合作多智能体学习的价值分解网络，”发表于第17届国际自主代理与多智能体系统会议论文集，第2085-2087页，2018年。'
- en: '[32] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted qmix: Expanding
    monotonic value function factorisation for deep multi-agent reinforcement learning,”
    Advances in neural information processing systems, vol. 33, pp. 10199–10210, 2020.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] T. Rashid, G. Farquhar, B. Peng 和 S. Whiteson，“加权qmix：扩展深度多智能体强化学习的单调值函数分解，”《神经信息处理系统进展》，第33卷，第10199-10210页，2020年。'
- en: '[33] J. Wang, Z. Ren, T. Liu, Y. Yu, and C. Zhang, “Qplex: Duplex dueling multi-agent
    q-learning,” in International Conference on Learning Representations, 2021.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Wang, Z. Ren, T. Liu, Y. Yu 和 C. Zhang，“Qplex：双重对抗多智能体q学习，”发表于国际学习表征会议，2021年。'
- en: '[34] J. Ackermann, V. Gabler, T. Osa, and M. Sugiyama, “Reducing overestimation
    bias in multi-agent domains using double centralized critics,” arXiv preprint
    arXiv:1910.01465, 2019.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Ackermann, V. Gabler, T. Osa 和 M. Sugiyama，“使用双重集中式评论家减少多智能体领域中的过度估计偏差，”arXiv预印本arXiv:1910.01465，2019年。'
- en: '[35] Y. Wang, B. Han, T. Wang, H. Dong, and C. Zhang, “Dop: Off-policy multi-agent
    decomposed policy gradients,” in International conference on learning representations,
    2020.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Wang, B. Han, T. Wang, H. Dong 和 C. Zhang，“Dop：离线策略多智能体分解策略梯度，”发表于国际学习表征会议，2020年。'
- en: '[36] T. Zhang, Y. Li, C. Wang, G. Xie, and Z. Lu, “Fop: Factorizing optimal
    joint policy of maximum-entropy multi-agent reinforcement learning,” in International
    Conference on Machine Learning, pp. 12491–12500, PMLR, 2021.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Zhang, Y. Li, C. Wang, G. Xie 和 Z. Lu，“Fop：最大熵多智能体强化学习的最优联合策略因子分解，”发表于国际机器学习会议，第12491-12500页，PMLR，2021年。'
- en: '[37] J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson, “Learning to
    communicate with deep multi-agent reinforcement learning,” Advances in neural
    information processing systems, vol. 29, 2016.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Foerster, I. A. Assael, N. De Freitas 和 S. Whiteson，“使用深度多智能体强化学习进行沟通学习，”《神经信息处理系统进展》，第29卷，2016年。'
- en: '[38] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat, and J. Pineau,
    “Tarmac: Targeted multi-agent communication,” in International Conference on machine
    learning, pp. 1538–1546, PMLR, 2019.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Das, T. Gervet, J. Romoff, D. Batra, D. Parikh, M. Rabbat 和 J. Pineau，“Tarmac：目标多智能体通信，”发表于国际机器学习会议，第1538-1546页，PMLR，2019年。'
- en: '[39] S. Sukhbaatar, R. Fergus, et al., “Learning multiagent communication with
    backpropagation,” Advances in neural information processing systems, vol. 29,
    2016.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Sukhbaatar, R. Fergus 等人，“通过反向传播学习多智能体通信，”《神经信息处理系统进展》，第29卷，2016年。'
- en: '[40] Y. Hoshen, “Vain: Attentional multi-agent predictive modeling,” Advances
    in neural information processing systems, vol. 30, 2017.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Y. Hoshen，“Vain：关注机制的多智能体预测建模，”《神经信息处理系统进展》，第30卷，2017年。'
- en: '[41] J. Jiang and Z. Lu, “Learning attentional communication for multi-agent
    cooperation,” Advances in neural information processing systems, vol. 31, 2018.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Jiang 和 Z. Lu，“学习多智能体合作的关注通信，”《神经信息处理系统进展》，第31卷，2018年。'
- en: '[42] I. Mordatch and P. Abbeel, “Emergence of grounded compositional language
    in multi-agent populations,” in Proceedings of the AAAI conference on artificial
    intelligence, vol. 32, 2018.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] I. Mordatch 和 P. Abbeel，“多智能体群体中基础组成语言的出现，”发表于AAAI人工智能会议论文集，第32卷，2018年。'
- en: '[43] S. Shen, Y. Fu, H. Su, H. Pan, P. Qiao, Y. Dou, and C. Wang, “Graphcomm:
    A graph neural network based method for multi-agent reinforcement learning,” in
    ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pp. 3510–3514, IEEE, 2021.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] S. Shen, Y. Fu, H. Su, H. Pan, P. Qiao, Y. Dou 和 C. Wang，“Graphcomm：一种基于图神经网络的多智能体强化学习方法，”发表于ICASSP
    2021-2021 IEEE国际声学、语音与信号处理会议（ICASSP），第3510-3514页，IEEE，2021年。'
- en: '[44] S. Gupta, R. Hazra, and A. Dukkipati, “Networked multi-agent reinforcement
    learning with emergent communication,” arXiv preprint arXiv:2004.02780, 2020.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Gupta, R. Hazra 和 A. Dukkipati，“带有紧急通信的网络化多智能体强化学习，”arXiv预印本arXiv:2004.02780，2020年。'
- en: '[45] A. Lazaridou and M. Baroni, “Emergent multi-agent communication in the
    deep learning era,” arXiv preprint arXiv:2006.02419, 2020.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Lazaridou 和 M. Baroni，“深度学习时代的紧急多智能体通信，”arXiv预印本arXiv:2006.02419，2020年。'
- en: '[46] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao,
    “React: Synergizing reasoning and acting in language models,” in The Eleventh
    International Conference on Learning Representations, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, 和 Y. Cao，“React:
    在语言模型中协同推理与行动”，在第十一届国际学习表征会议中，2023。'
- en: '[47] A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal, M. Bansal,
    and T. Khot, “Adapt: As-needed decomposition and planning with language models,”
    arXiv preprint arXiv:2311.05772, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal, M. Bansal,
    和 T. Khot，“Adapt: 按需分解与语言模型规划”，arXiv 预印本 arXiv:2311.05772, 2023。'
- en: '[48] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West,
    and B. Faltings, “Refiner: Reasoning feedback on intermediate representations,”
    arXiv preprint arXiv:2304.01904, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West,
    和 B. Faltings，“Refiner: 中间表示的推理反馈”，arXiv 预印本 arXiv:2304.01904, 2023。'
- en: '[49] A. Zhang, A. Parashar, and D. Saha, “A simple framework for intrinsic
    reward-shaping for rl using llm feedback,”'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Zhang, A. Parashar, 和 D. Saha，“一种简单的框架用于通过 LLM 反馈进行 RL 的内在奖励塑造”。'
- en: '[50] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. R. N,
    Z. Chen, J. Zhang, D. Arpit, R. Xu, P. L. Mui, H. Wang, C. Xiong, and S. Savarese,
    “Retroformer: Retrospective large language agents with policy gradient optimization,”
    in The Twelfth International Conference on Learning Representations, 2024.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] W. Yao, S. Heinecke, J. C. Niebles, Z. Liu, Y. Feng, L. Xue, R. R. N,
    Z. Chen, J. Zhang, D. Arpit, R. Xu, P. L. Mui, H. Wang, C. Xiong, 和 S. Savarese，“Retroformer:
    使用策略梯度优化的回顾性大语言模型代理”，在第十二届国际学习表征会议中，2024。'
- en: '[51] R. Murthy, S. Heinecke, J. C. Niebles, Z. Liu, L. Xue, W. Yao, Y. Feng,
    Z. Chen, A. Gokul, D. Arpit, et al., “Rex: Rapid exploration and exploitation
    for ai agents,” arXiv preprint arXiv:2307.08962, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] R. Murthy, S. Heinecke, J. C. Niebles, Z. Liu, L. Xue, W. Yao, Y. Feng,
    Z. Chen, A. Gokul, D. Arpit 等，“Rex: AI 代理的快速探索与利用”，arXiv 预印本 arXiv:2307.08962,
    2023。'
- en: '[52] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid,
    J. Tompson, Q. Vuong, T. Yu, et al., “Palm-e: An embodied multimodal language
    model,” in International Conference on Machine Learning, pp. 8469–8488, PMLR,
    2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A.
    Wahid, J. Tompson, Q. Vuong, T. Yu 等，“Palm-e: 一种具身的多模态语言模型”，在国际机器学习大会中，pp. 8469–8488,
    PMLR, 2023。'
- en: '[53] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in International
    Conference on Machine Learning, pp. 9118–9147, PMLR, 2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] W. Huang, P. Abbeel, D. Pathak, 和 I. Mordatch，“语言模型作为零-shot 规划者: 提取具身代理的可操作知识”，在国际机器学习大会中，pp.
    9118–9147, PMLR, 2022。'
- en: '[54] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz,
    A. Irpan, E. Jang, R. Julian, et al., “Do as i can, not as i say: Grounding language
    in robotic affordances,” in Conference on robot learning, pp. 287–318, PMLR, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz,
    A. Irpan, E. Jang, R. Julian 等，“做我能做的，而不是我说的: 将语言与机器人能力结合”，在机器人学习会议中，pp. 287–318,
    PMLR, 2023。'
- en: '[55] Z. Liu, Y. Zhang, P. Li, Y. Liu, and D. Yang, “Dynamic llm-agent network:
    An llm-agent collaboration framework with agent team optimization,” arXiv preprint
    arXiv:2310.02170, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. Liu, Y. Zhang, P. Li, Y. Liu, 和 D. Yang，“动态 LLM-agent 网络: 一种与代理团队优化的
    LLM-agent 协作框架”，arXiv 预印本 arXiv:2310.02170, 2023。'
- en: '[56] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song,
    and J. Steinhardt, “Measuring mathematical problem solving with the math dataset,”
    NeurIPS, 2021.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D.
    Song, 和 J. Steinhardt，“使用数学数据集测量数学问题解决能力”，NeurIPS, 2021。'
- en: '[57] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, and J. Steinhardt,
    “Aligning ai with shared human values,” Proceedings of the International Conference
    on Learning Representations (ICLR), 2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song, 和 J. Steinhardt，“将人工智能与共享的人类价值观对齐”，国际学习表征会议（ICLR）论文集,
    2021。'
- en: '[58] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” 2021.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, 和 W. Zaremba，“评估训练有素的代码大语言模型”，2021年。'
- en: '[59] O. Slumbers, D. H. Mguni, K. Shao, and J. Wang, “Leveraging large language
    models for optimised coordination in textual multi-agent reinforcement learning,”
    2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] O. Slumbers, D. H. Mguni, K. Shao, 和 J. Wang，“利用大语言模型优化文本多智能体强化学习中的协调”，2023年。'
- en: '[60] H. Chen, W. Ji, L. Xu, and S. Zhao, “Multi-agent consensus seeking via
    large language models,” arXiv preprint arXiv:2310.20151, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Chen, W. Ji, L. Xu, 和 S. Zhao，“通过大语言模型实现多智能体共识”，arXiv 预印本 arXiv:2310.20151，2023年。'
- en: '[61] H. Li, Y. Chong, S. Stepputtis, J. P. Campbell, D. Hughes, C. Lewis, and
    K. Sycara, “Theory of mind for multi-agent collaboration via large language models,”
    in Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing, pp. 180–192, 2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] H. Li, Y. Chong, S. Stepputtis, J. P. Campbell, D. Hughes, C. Lewis, 和
    K. Sycara，“通过大语言模型实现多智能体协作的心智理论”，发表于2023年自然语言处理实证方法会议论文集，第180–192页，2023年。'
- en: '[62] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and
    C. Gan, “Building cooperative embodied agents modularly with large language models,”
    in The Twelfth International Conference on Learning Representations, 2024.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, 和 C.
    Gan，“用大语言模型模块化构建合作的具身智能体”，在第十二届国际学习表征会议上，2024年。'
- en: '[63] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler,
    and A. Torralba, “Watch-and-help: A challenge for social perception and human-ai
    collaboration,” arXiv preprint arXiv:2010.09890, 2020.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Puig, T. Shu, S. Li, Z. Wang, Y.-H. Liao, J. B. Tenenbaum, S. Fidler,
    和 A. Torralba，“Watch-and-help: 社会感知和人类-AI协作的挑战”，arXiv 预印本 arXiv:2010.09890，2020年。'
- en: '[64] S. S. Kannan, V. L. Venkatesh, and B.-C. Min, “Smart-llm: Smart multi-agent
    robot task planning using large language models,” arXiv preprint arXiv:2309.10062,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. S. Kannan, V. L. Venkatesh, 和 B.-C. Min，“Smart-llm: 利用大语言模型进行智能多智能体机器人任务规划”，arXiv
    预印本 arXiv:2309.10062，2023年。'
- en: '[65] Z. Mandi, S. Jain, and S. Song, “Roco: Dialectic multi-robot collaboration
    with large language models,” arXiv preprint arXiv:2307.04738, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Z. Mandi, S. Jain, 和 S. Song，“Roco: 用大语言模型进行辩证的多机器人协作”，arXiv 预印本 arXiv:2307.04738，2023年。'
- en: '[66] B. Yu, H. Kasaei, and M. Cao, “Co-navgpt: Multi-robot cooperative visual
    semantic navigation using large language models,” arXiv preprint arXiv:2310.07937,
    2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] B. Yu, H. Kasaei, 和 M. Cao，“Co-navgpt: 使用大语言模型进行多机器人合作视觉语义导航”，arXiv 预印本
    arXiv:2310.07937，2023年。'
- en: '[67] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J. M.
    Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao,
    and D. Batra, “Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments
    for embodied AI,” in Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track, 2021.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets, A. Clegg, J.
    M. Turner, E. Undersander, W. Galuba, A. Westbury, A. X. Chang, M. Savva, Y. Zhao,
    和 D. Batra，“Habitat-matterport 3d 数据集（HM3d）：1000个大规模3D环境用于具身AI”，发表于第35届神经信息处理系统会议数据集与基准轨道，2021年。'
- en: '[68] X. Guo, K. Huang, J. Liu, W. Fan, N. Vélez, Q. Wu, H. Wang, T. L. Griffiths,
    and M. Wang, “Embodied llm agents learn to cooperate in organized teams,” arXiv
    preprint arXiv:2403.12482, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] X. Guo, K. Huang, J. Liu, W. Fan, N. Vélez, Q. Wu, H. Wang, T. L. Griffiths,
    和 M. Wang，“具身的大语言模型智能体在有组织的团队中学习合作”，arXiv 预印本 arXiv:2403.12482，2024年。'
- en: '[69] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z. Wang,
    S. K. S. Yau, Z. Lin, et al., “Metagpt: Meta programming for multi-agent collaborative
    framework,” in The Twelfth International Conference on Learning Representations,
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z.
    Wang, S. K. S. Yau, Z. Lin, 等, “Metagpt：用于多智能体协作框架的元编程”，发表于第十二届国际学习表征会议，2023年。'
- en: '[70] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang,
    C. Cai, M. Terry, Q. Le, et al., “Program synthesis with large language models,”
    arXiv preprint arXiv:2108.07732, 2021.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang,
    C. Cai, M. Terry, Q. Le, 等, “使用大型语言模型的程序合成”，arXiv预印本 arXiv:2108.07732，2021年。'
- en: '[71] J.-t. Huang, E. J. Li, M. H. Lam, T. Liang, W. Wang, Y. Yuan, W. Jiao,
    X. Wang, Z. Tu, and M. R. Lyu, “How far are we on the decision-making of llms?
    evaluating llms’ gaming ability in multi-agent environments,” arXiv preprint arXiv:2403.11807,
    2024.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J.-t. Huang, E. J. Li, M. H. Lam, T. Liang, W. Wang, Y. Yuan, W. Jiao,
    X. Wang, Z. Tu, 和 M. R. Lyu, “我们在大型语言模型决策能力上走了多远？评估大型语言模型在多智能体环境中的游戏能力”，arXiv预印本
    arXiv:2403.11807，2024年。'
- en: '[72] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, and C. Wang, “Autogen: Enabling next-gen llm applications via multi-agent
    conversation framework,” arXiv preprint arXiv:2308.08155, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang,
    X. Zhang, 和 C. Wang, “Autogen：通过多智能体对话框架实现下一代大型语言模型应用”，arXiv预印本 arXiv:2308.08155，2023年。'
- en: '[73] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein,
    “Generative agents: Interactive simulacra of human behavior,” in Proceedings of
    the 36th Annual ACM Symposium on User Interface Software and Technology, pp. 1–22,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, 和 M. S. Bernstein,
    “生成型智能体：人类行为的交互式模拟”，发表于第36届年度ACM用户界面软件与技术研讨会论文集，第1–22页，2023年。'
- en: '[74] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel: Communicative
    agents for “mind” exploration of large language model society,” Advances in Neural
    Information Processing Systems, vol. 36, 2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] G. Li, H. Hammoud, H. Itani, D. Khizbullin, 和 B. Ghanem, “Camel：用于大型语言模型社会“心智”探索的交流型智能体”，神经信息处理系统进展，第36卷，2024年。'
- en: '[75] A. Szot, U. Jain, D. Batra, Z. Kira, R. Desai, and A. Rai, “Adaptive coordination
    in social embodied rearrangement,” in International Conference on Machine Learning,
    pp. 33365–33380, PMLR, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] A. Szot, U. Jain, D. Batra, Z. Kira, R. Desai, 和 A. Rai, “社会化体态重组中的自适应协调”，发表于国际机器学习会议，第33365–33380页，PMLR，2023年。'
- en: '[76] D. Abel, J. Salvatier, A. Stuhlmüller, and O. Evans, “Agent-agnostic human-in-the-loop
    reinforcement learning,” arXiv preprint arXiv:1701.04079, 2017.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] D. Abel, J. Salvatier, A. Stuhlmüller, 和 O. Evans, “与智能体无关的人机交互强化学习”，arXiv预印本
    arXiv:1701.04079，2017年。'
- en: '[77] H. Liang, L. Yang, H. Cheng, W. Tu, and M. Xu, “Human-in-the-loop reinforcement
    learning,” in 2017 Chinese Automation Congress (CAC), pp. 4511–4518, IEEE, 2017.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] H. Liang, L. Yang, H. Cheng, W. Tu, 和 M. Xu, “人机交互强化学习”，发表于2017年中国自动化大会（CAC），第4511–4518页，IEEE，2017年。'
- en: '[78] B. Luo, Z. Wu, F. Zhou, and B.-C. Wang, “Human-in-the-loop reinforcement
    learning in continuous-action space,” IEEE Transactions on Neural Networks and
    Learning Systems, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] B. Luo, Z. Wu, F. Zhou, 和 B.-C. Wang, “在连续动作空间中的人机交互强化学习”，IEEE神经网络与学习系统汇刊，2023年。'
- en: '[79] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei,
    “Deep reinforcement learning from human preferences,” Advances in neural information
    processing systems, vol. 30, 2017.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, 和 D. Amodei,
    “基于人类偏好的深度强化学习”，神经信息处理系统进展，第30卷，2017年。'
- en: '[80] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint
    arXiv:2106.09685, 2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen, “Lora：大型语言模型的低秩适配”，arXiv预印本 arXiv:2106.09685，2021年。'
- en: '[81] Y. Xin, J. Du, Q. Wang, K. Yan, and S. Ding, “Mmap: Multi-modal alignment
    prompt for cross-domain multi-task learning,” in Proceedings of the AAAI Conference
    on Artificial Intelligence, vol. 38, pp. 16076–16084, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Xin, J. Du, Q. Wang, K. Yan, 和 S. Ding, “Mmap：用于跨领域多任务学习的多模态对齐提示”，发表于AAAI人工智能会议论文集，第38卷，第16076–16084页，2024年。'
- en: '[82] Y. Xin, J. Du, Q. Wang, Z. Lin, and K. Yan, “Vmt-adapter: Parameter-efficient
    transfer learning for multi-task dense scene understanding,” in Proceedings of
    the AAAI Conference on Artificial Intelligence, vol. 38, pp. 16085–16093, 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Xin, J. Du, Q. Wang, Z. Lin, 和 K. Yan, “Vmt-adapter：用于多任务密集场景理解的参数高效迁移学习”，发表于AAAI人工智能会议论文集，第38卷，第16085–16093页，2024年。'
- en: '[83] Y. Xin, S. Luo, H. Zhou, J. Du, X. Liu, Y. Fan, Q. Li, and Y. Du, “Parameter-efficient
    fine-tuning for pre-trained vision models: A survey,” arXiv preprint arXiv:2402.02242,
    2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Xin, S. Luo, H. Zhou, J. Du, X. Liu, Y. Fan, Q. Li 和 Y. Du，"预训练视觉模型的参数高效微调：一项综述"，arXiv
    预印本 arXiv:2402.02242，2024。'
- en: '[84] Y. Huang, Y. Chen, Z. Yu, and K. McKeown, “In-context learning distillation:
    Transferring few-shot learning ability of pre-trained language models,” arXiv
    preprint arXiv:2212.10670, 2022.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Y. Huang, Y. Chen, Z. Yu 和 K. McKeown，"上下文学习提炼：迁移预训练语言模型的少样本学习能力"，arXiv
    预印本 arXiv:2212.10670，2022。'
- en: '[85] C. Snell, D. Klein, and R. Zhong, “Learning by distilling context,” arXiv
    preprint arXiv:2209.15189, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] C. Snell, D. Klein 和 R. Zhong，"通过提炼上下文学习"，arXiv 预印本 arXiv:2209.15189，2022。'
