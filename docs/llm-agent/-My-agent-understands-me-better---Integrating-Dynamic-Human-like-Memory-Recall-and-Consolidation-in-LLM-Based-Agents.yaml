- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “我的代理人更了解我了”：在基于LLM的代理中集成动态类人记忆回忆与巩固
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00573](https://ar5iv.labs.arxiv.org/html/2404.00573)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.00573](https://ar5iv.labs.arxiv.org/html/2404.00573)
- en: Yuki Hou [houhoutime@gmail.com](mailto:houhoutime@gmail.com) Meiji UniversityTokyoJapan
    ,  Haruki Tamoto [harukiririwiru@gmail.com](mailto:harukiririwiru@gmail.com) Kyoto
    UniversityKyotoJapan  and  Homei Miyashita [homei@homei.com](mailto:homei@homei.com)
    Meiji UniversityTokyoJapan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuki Hou [houhoutime@gmail.com](mailto:houhoutime@gmail.com) 明治大学东京日本， Haruki
    Tamoto [harukiririwiru@gmail.com](mailto:harukiririwiru@gmail.com) 京都大学京都日本 和
    Homei Miyashita [homei@homei.com](mailto:homei@homei.com) 明治大学东京日本
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In this study, we propose a novel human-like memory architecture designed for
    enhancing the cognitive abilities of large language model (LLM)-based dialogue
    agents. Our proposed architecture enables agents to autonomously recall memories
    necessary for response generation, effectively addressing a limitation in the
    temporal cognition of LLMs. We adopt the human memory cue recall as a trigger
    for accurate and efficient memory recall. Moreover, we developed a mathematical
    model that dynamically quantifies memory consolidation, considering factors such
    as contextual relevance, elapsed time, and recall frequency. The agent stores
    memories retrieved from the user’s interaction history in a database that encapsulates
    each memory’s content and temporal context. Thus, this strategic storage allows
    agents to recall specific memories and understand their significance to the user
    in a temporal context, similar to how humans recognize and recall past experiences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们提出了一种新颖的类人记忆架构，旨在增强基于大型语言模型（LLM）的对话代理的认知能力。我们提出的架构使代理能够自主回忆生成回应所需的记忆，有效解决了LLM在时间认知方面的一个限制。我们采用类人记忆提示作为准确和高效记忆回忆的触发器。此外，我们开发了一个数学模型，动态量化记忆巩固，考虑了上下文相关性、经过时间和回忆频率等因素。代理将从用户互动历史中检索到的记忆存储在一个数据库中，该数据库封装了每个记忆的内容和时间背景。因此，这种战略存储使代理能够在时间背景下回忆特定记忆，并理解这些记忆对用户的重要性，类似于人类如何识别和回忆过去的经历。
- en: 'Memory Retrieval Models, Large Language Models, Intelligent Agents, User Experience^†^†conference:
    Extended Abstracts of the CHI Conference on Human Factors in Computing Systems;
    May 11–16, 2024; Honolulu, HI, USA^†^†booktitle: Extended Abstracts of the CHI
    Conference on Human Factors in Computing Systems (CHI EA ’24), May 11–16, 2024,
    Honolulu, HI, USA![Refer to caption](img/3828b02e4f33e2fadf6a48f8cd887649.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆检索模型、大型语言模型、智能代理、用户体验^†^†会议：CHI会议扩展摘要；2024年5月11–16日；美国夏威夷檀香山^†^†书名：CHI会议扩展摘要（CHI
    EA ’24），2024年5月11–16日，美国夏威夷檀香山![参考说明](img/3828b02e4f33e2fadf6a48f8cd887649.png)
- en: Figure 1\. Architecture of the enhanced large language model (LLM)-based dialogue
    agent that integrates human-like memory processes. First, the user input is converted
    into vectorized text and processed through a data-filtering process based on relevance
    and memory consolidation bias, modeled after human cognitive functions. Then,
    memory recall is triggered when the recall probability, informed by relevance
    and elapsed time, exceeds a predefined threshold. This diagram features an agent
    output example where the system recalls ”Creamy pasta” as the user’s lunch preference
    with a higher frequency, influencing the agent’s response.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 集成类人记忆过程的增强型大型语言模型（LLM）基础对话代理的架构。首先，用户输入被转换为向量化文本，并通过基于相关性和记忆巩固偏差的数据过滤过程进行处理，模拟人类认知功能。然后，当回忆概率受相关性和经过时间的影响超过预定义阈值时，触发记忆回忆。该图展示了一个代理输出示例，其中系统回忆“奶油意大利面”作为用户的午餐偏好，频率较高，从而影响代理的回应。
- en: \Description
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: This diagram illustrates the architecture of an enhanced Large Language Model
    (LLM)-based dialogue agent that integrates human-like memory processes. The user
    input is first converted into vectorized text and processed through a data-filtering
    process based on relevance and memory consolidation bias, modeled after human
    cognitive functions. Memory recall is triggered when the recall probability, informed
    by relevance and elapsed time, exceeds a predefined threshold. The diagram features
    an agent output example where the system recalls ”Creamy pasta” as the user’s
    lunch preference with higher frequency, influencing the agent’s response. The
    proposed model emphasizes the role of memory consolidation and cued recall, significantly
    improving the agent’s response relevance and coherence in conversations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 该图示展示了一种增强型大语言模型（LLM）基础对话代理的架构，该代理整合了类似人类的记忆过程。用户输入首先被转换为向量化文本，并通过基于相关性和记忆巩固偏差的数据过滤过程进行处理，这一过程模拟了人类的认知功能。当回忆概率（受相关性和经过时间的影响）超过预定阈值时，会触发记忆召回。该图示包含一个代理输出示例，其中系统频繁地回忆“奶油意大利面”作为用户的午餐偏好，从而影响了代理的回应。所提出的模型强调了记忆巩固和提示回忆的作用，显著提升了代理在对话中的回应相关性和连贯性。
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'The emergence of transformer-based language models (Lin et al., [2022](#bib.bib15))
    have drastically revolutionized the field of natural language processing, surpassing
    the capabilities of traditional models in understanding and generating human-like
    text (Sun et al., [2019](#bib.bib23)). In particular, large language models (LLMs)
    (Dao, [2023](#bib.bib6)) have garnered considerable attention for their prowess
    in mimicking artificial intelligence (AI) with human-like cognition and conversational
    abilities, reminiscent of sentient machines portrayed in science fiction narratives.
    However, LLMs exhibit a significant limitation in processing temporal information
    inherent to human cognition. While transformers possess excellent self-attention
    mechanisms, outperforming recurrent neural networks (RNNs) (Mandic and Chambers,
    [2001](#bib.bib16)) and long short-term memory models (LSTM) (Sundermeyer et al.,
    [2012](#bib.bib24)), they fail to replicate human behavioral dynamics. To accurately
    replicate the nuanced human-like interactions of AI agents, as depicted in science
    fiction, one must first achieve human-like cognitive and memory processing abilities.
    Therefore, we proposed an approach to integrate human memory processes into LLM-based
    dialogue agents [1](#S0.F1 "Figure 1 ‣ ”My agent understands me better”: Integrating
    Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"). We adopted
    human-like cued recall as the trigger for accurate and efficient memory retrieval
    (McDaniel et al., [1989](#bib.bib17)). This mechanism involves an agent autonomously
    recalling memories essential for generating responses during a conversation. The
    process emulates the human memory process known as ”remember to remember” (Hécaen
    et al., [1978](#bib.bib10)), consciously retaining memory for future action or
    task and recalling that when needed (Kuhlmann, [2019](#bib.bib14)). Furthermore,
    the proposed model replicates human cognitive ability, where memories recalled
    repeatedly over a long period are retained more strongly than those recalled over
    a short period and relatively frequently (Roediger and Karpicke, [2006](#bib.bib22)),
    regardless of recall frequency. Thus, our model provides contextually relevant
    and coherent conversations.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的语言模型（Lin et al., [2022](#bib.bib15)）的出现极大地革新了自然语言处理领域，在理解和生成类似人类的文本方面超越了传统模型的能力（Sun
    et al., [2019](#bib.bib23)）。特别是，大型语言模型（LLMs）（Dao, [2023](#bib.bib6)）因其在模拟具有类人认知和对话能力的人工智能（AI）方面表现出色而受到广泛关注，令人联想到科幻小说中描绘的有意识的机器。然而，LLMs
    在处理人类认知固有的时间信息方面存在显著的限制。尽管变换器具有优越的自注意机制，超越了递归神经网络（RNNs）（Mandic and Chambers, [2001](#bib.bib16)）和长短期记忆模型（LSTM）（Sundermeyer
    et al., [2012](#bib.bib24)），但它们未能复制人类行为动态。为了准确复制科幻中描绘的类人AI交互，必须首先实现类似人类的认知和记忆处理能力。因此，我们提出了一种将人类记忆过程整合到基于LLM的对话代理中的方法
    [1](#S0.F1 "Figure 1 ‣ ”我的代理理解我更好”：在基于LLM的代理中整合动态类人记忆回忆与巩固")。我们采用了类人提示回忆作为准确和高效记忆检索的触发机制（McDaniel
    et al., [1989](#bib.bib17)）。该机制涉及代理在对话中自主回忆生成回应所需的记忆。该过程模拟了被称为“记住记忆”的人类记忆过程（Hécaen
    et al., [1978](#bib.bib10)），有意识地保留记忆以便未来行动或任务，并在需要时回忆（Kuhlmann, [2019](#bib.bib14)）。此外，所提出的模型模拟了人类的认知能力，其中那些被反复回忆的记忆在长期保留中比那些在短期内频繁回忆的记忆更强（Roediger
    and Karpicke, [2006](#bib.bib22)），无论回忆的频率如何。因此，我们的模型提供了语境相关和连贯的对话。
- en: Furthermore, our primary purpose is to transcend the paradigm of dialogue agents
    merely imitating human behavior through statistical natural language models. Instead,
    we seek to create agents that are capable of truly understanding human language
    with rich nuances, achieved by seamlessly integrating human cognitive processes.
    This fusion aligns with the philosophy of human-computer interaction, promoting
    more natural and intuitive human-centered interactions between the two at cognitive
    and emotional levels.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的主要目的是超越仅仅通过统计自然语言模型模仿人类行为的对话代理范式。相反，我们希望创建能够真正理解人类语言及其丰富细微差别的代理，这通过无缝整合人类认知过程来实现。这种融合与人机交互的理念相一致，促进了在认知和情感层面上更自然、更直观的人本交互。
- en: '![Refer to caption](img/081d7b3aa8f4fad9a590f8100197cdc8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/081d7b3aa8f4fad9a590f8100197cdc8.png)'
- en: Figure 2\. (A) Decline in recall probability over time. The black curve ($r$.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.（A）随着时间的推移，回忆概率的下降。黑色曲线（$r$。
- en: \Description
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: This figure consists of two graphs. Graph A depicts the decline in recall probability
    over time. The black curve represents a scenario with a standard recall rate (r=1)
    and decay gradient (g=1), indicating a rapid loss of recall capability as time
    progresses. The red curve, illustrating a reduced recall rate (r=0.6) and higher
    decay gradient (g=2), represents a slower rate of forgetting. Graph B illustrates
    how memory is reinforced through repetition. At time t_0, Event D is recalled
    by the user, and the model updates the temporal significance of Event D. The recall
    of Event D at t_0 exemplifies how repeated recall makes the memory less susceptible
    to forgetting at time t_0+t.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该图由两个图表组成。图 A 描绘了回忆概率随时间的下降。黑色曲线表示标准回忆率（r=1）和衰减梯度（g=1）的情景，显示随着时间的推移回忆能力的快速丧失。红色曲线则说明了较低的回忆率（r=0.6）和更高的衰减梯度（g=2），代表了较慢的遗忘速度。图
    B 说明了记忆如何通过重复得到增强。在时间 t_0，用户回忆事件 D，并且模型更新事件 D 的时间意义。在 t_0 的事件 D 的回忆例证了重复回忆如何使记忆在时间
    t_0+t 更不容易被遗忘。
- en: 2\. Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. Similarities Between LLMs and Human Memory
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLMs 与人类记忆的相似性
- en: Human memory serves as a system to encode, store, and retrieve our experiences
    (Tulving et al., [1972](#bib.bib26)). Our memories can be categorized into declarative
    and non-declarative memories, with declarative memory further divided into episodic
    and semantic memories (California, [1987](#bib.bib4)). Episodic memory (Tulving,
    [2002](#bib.bib25)) consciously allows for recollecting and re-experiencing one’s
    subjective past. In contrast, semantic memory supports language use, registering
    not the perceptual properties of inputs but the cognitive referents of input signals
    (Yamadori, [2002](#bib.bib28)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类记忆作为一个系统，用于编码、存储和检索我们的经历（Tulving 等，[1972](#bib.bib26)）。我们的记忆可以分为陈述性记忆和非陈述性记忆，其中陈述性记忆进一步分为情景记忆和语义记忆（California，[1987](#bib.bib4)）。情景记忆（Tulving，[2002](#bib.bib25)）让我们有意识地回忆和重新体验个人的主观过去。相对而言，语义记忆支持语言使用，记录的不是输入的感知特性，而是输入信号的认知指涉（Yamadori，[2002](#bib.bib28)）。
- en: Similar to human episodic memory functioning, the episodic nature of LLMs’ is
    demonstrated by their ability to recall specific events or dialogues from the
    database. This allows LLMs to generate responses based on past interactions and
    experiences to inform current interactions. LLMs also possess a human-like semantic
    understanding of language that captures the meaning and context behind the words.
    Geva et al. (Geva et al., [2021](#bib.bib9)) suggested the feed-forward layers
    of transformer-based models to operate in a key-value format, the same as human
    semantic memory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于人类情景记忆的功能，LLMs 的情景性质通过它们从数据库中回忆特定事件或对话的能力得以体现。这使得 LLMs 可以基于过去的互动和经验生成回应，以告知当前的互动。LLMs
    还具有类人语义理解能力，捕捉到词语背后的意义和上下文。Geva 等（Geva 等，[2021](#bib.bib9)）建议基于变换器的模型的前馈层以键值格式操作，与人类语义记忆相同。
- en: 2.2\. Human-like Memory Processes in AI Agent
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. AI 代理中的类人记忆过程
- en: 'Kim et al. (Kim et al., [2022](#bib.bib12)) focused on emulating human episodic
    and semantic memory processes in AI agents to enhance interactive experiences.
    They compared agents with different memory processes: episodic only, semantic
    only, and both. These agents used different strategies to decide which memories
    to forget when memory was full and which to use when answering questions. The
    agents with a composite memory system outperformed those with a single memory
    system, especially those with pre-trained semantic memory. Zhong et al. developed
    MemoryBank, a memory retrieval mechanism for memory storage (Zhong et al., [2023](#bib.bib29)).
    The system uses an encoder model to encode each conversation turn and event summary
    into a vector representation, allowing recalling memory with the highest relevance
    whenever needed. The memory strength of MemoryBank’s is enhanced by 1 each time
    a memory piece is recalled, simulating more human-like memory behavior and reducing
    the probability of forgetting the memory by setting the elapsed time to zero.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Kim 等人（Kim et al., [2022](#bib.bib12)）专注于模拟人类的情节记忆和语义记忆过程，以提升互动体验。他们比较了不同记忆过程的智能体：仅有情节记忆、仅有语义记忆和两者兼具。这些智能体使用不同策略来决定在内存满时要忘记哪些记忆，以及在回答问题时使用哪些记忆。具有复合记忆系统的智能体表现优于具有单一记忆系统的智能体，特别是那些具有预训练语义记忆的智能体。Zhong
    等人开发了 MemoryBank，这是一个用于记忆存储的记忆检索机制（Zhong et al., [2023](#bib.bib29)）。该系统使用编码器模型将每次对话轮次和事件总结编码为向量表示，从而在需要时能够召回最相关的记忆。每当一个记忆片段被召回时，MemoryBank
    的记忆强度会增加 1，模拟了更像人类的记忆行为，并通过将经过的时间设置为零来减少记忆遗忘的概率。
- en: In contrast, we designed our architecture without the concept of ”complete forgetting.”
    Even if not recalling a memory over an extended period, the degree of consolidation
    never reaches absolute zero. Thus, given the right trigger, these memories can
    be recalled (Amin and Malik, [2014](#bib.bib2)). The process is consistent with
    that of human memory, where past experiences are never completely forgotten and
    can be retrieved with specific stimuli, such as the scent of a familiar perfume
    or the melody of a once-favorite song.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们设计的架构没有“完全遗忘”的概念。即使在长时间内没有召回记忆，巩固程度也不会完全降为零。因此，在适当的触发条件下，这些记忆可以被召回（Amin
    and Malik, [2014](#bib.bib2)）。这一过程与人类记忆一致，过去的经历不会被完全遗忘，并且可以通过特定的刺激被提取，例如熟悉香水的气味或曾经最爱的歌曲的旋律。
- en: 2.3\. Mathematical Models of Human Memory Processes
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 人类记忆过程的数学模型
- en: 'This section reviews the mathematical models that attempt to quantify and simulate
    human memory processes, primarily for memory recall. Based on Zielske’s (Zielske,
    [1959](#bib.bib30)) recall probability function, Chessa et al. (Chessa and Murre,
    [2007](#bib.bib5)) proposed a model that assumes the rate of memory consolidation
    $r(t)$ of a human memory being recalled as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了尝试量化和模拟人类记忆过程的数学模型，主要用于记忆回忆。基于 Zielske 的（Zielske, [1959](#bib.bib30)）回忆概率函数，Chessa
    等人（Chessa and Murre, [2007](#bib.bib5)）提出了一个模型，假设人类记忆的巩固速率 $r(t)$ 如下：
- en: '| (1) |  | $p(t)=1-\sum\limits_{n=1}^{b-1}\frac{(r(t))^{n}}{n!}\exp(-r(t))$
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $p(t)=1-\sum\limits_{n=1}^{b-1}\frac{(r(t))^{n}}{n!}\exp(-r(t))$
    |  |'
- en: 'This model is based on the hypothesis that each neuron fires independently
    and at random (Holtmaat and Caroni, [2016](#bib.bib11)), and is derived from the
    properties of a non-homogeneous Poisson process using a time-varying intensity
    function $r(t)$ represents the adjustment process of memory strength (Burgess
    et al., [2002](#bib.bib3)) in the human hippocampus:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于每个神经元独立且随机发放的假设（Holtmaat and Caroni, [2016](#bib.bib11)），并源于使用时间变化强度函数
    $r(t)$ 表示记忆强度调整过程的非齐次泊松过程的属性（Burgess et al., [2002](#bib.bib3)）在人的海马体中的应用：
- en: '| (2) |  | $r(t)=\mu e^{-at}$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $r(t)=\mu e^{-at}$ |  |'
- en: where $\mu$ in this special case is expressed as
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特殊情况下，$\mu$ 表达为：
- en: '| (3) |  | $p(t)=1-\exp(-\mu e^{-at})$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $p(t)=1-\exp(-\mu e^{-at})$ |  |'
- en: The recall probability $p(t)$, as demonstrated in short-term memory decline
    using the classic Brown-Peterson learning and distraction task (Peterson and Peterson,
    [1959](#bib.bib20)). However, this model considers only one trial learning and
    a constant decay rate. However, in reality, the degree of consolidation differs
    between memories recalled many times and those not; hence, the decay rate should
    be adjusted to reflect this effect.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆概率$p(t)$，如经典的Brown-Peterson学习和分散任务（Peterson and Peterson, [1959](#bib.bib20)）中演示的短期记忆衰退。然而，该模型仅考虑了单次试验学习和常数衰减率。然而，实际上，多次回忆的记忆和未回忆的记忆的巩固程度不同，因此衰减率应调整以反映这一效果。
- en: 2.4\. LLM-based Autonomous Agents
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 基于LLM的自主代理
- en: 'Park et al. introduced the concept of Generative Agents, outlining a memory
    mechanism of agents based on a scoring system comprising three elements: recency,
    importance, and relevance (Park et al., [2023](#bib.bib19)). This approach dictates
    that agents consider recent actions or events (recency), objects deemed important
    by the agent (importance), and objects relevant to the current situation (relevance)
    to make decisions. These elements are normalized leveraging min-max scaling and
    combined through a weighted sum to determine the final score. In contrast, the
    proposed model employs elapsed time, relevance, and recall frequency to calculate
    the degree of memory consolidation. Thus, the agent can recall the most appropriate
    memory, facilitating efficient dialogue. While the Generative Agents and our proposed
    model share commonalities in memory processing, they apply memory in different
    contexts and for different purposes. Generative Agents focus on independently
    scoring each memory element to select actions most fitting to the current context.
    In contrast, our approach adjusts memory consolidation over time, enabling memory
    consistency.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Park等人提出了生成代理的概念，概述了基于包含三个元素的评分系统的代理记忆机制：近期性、重要性和相关性（Park et al., [2023](#bib.bib19)）。这一方法要求代理考虑最近的行动或事件（近期性）、被代理认为重要的对象（重要性）以及与当前情况相关的对象（相关性）以做出决策。这些元素通过最小-最大缩放进行归一化，并通过加权和结合以确定最终得分。与此相反，提出的模型使用经过的时间、相关性和回忆频率来计算记忆巩固程度。因此，代理可以回忆出最合适的记忆，促进高效对话。虽然生成代理和我们提出的模型在记忆处理上有相似之处，但它们在不同的背景和目的下应用记忆。生成代理专注于独立评分每个记忆元素，以选择最适合当前背景的行动。而我们的方法则通过时间调整记忆巩固，确保记忆的一致性。
- en: 3\. Architecture
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 架构
- en: 3.1\. Model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 模型
- en: We constructed the model based on exponential decay, taking event relevance
    ($r$ is expressed as
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于指数衰减构建了模型，考虑事件相关性（$r$ 表示为
- en: '| (4) |  | $p(t)=1-\exp(-re^{-at})$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $p(t)=1-\exp(-re^{-at})$ |  |'
- en: 'The relevance is quantified by the cosine similarity between vectorized texts,
    defining the closeness of information. The cosine similarity between n-dimensional
    vectors $\boldsymbol{a}$ is defined as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性通过向量化文本之间的余弦相似性来量化，定义信息的接近程度。n维向量$\boldsymbol{a}$之间的余弦相似性定义为：
- en: '| (5) |  | $r=\frac{\boldsymbol{a}\cdot\boldsymbol{b}}{\&#124;\boldsymbol{a}\&#124;\&#124;\boldsymbol{b}\&#124;}$
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $r=\frac{\boldsymbol{a}\cdot\boldsymbol{b}}{\&#124;\boldsymbol{a}\&#124;\&#124;\boldsymbol{b}\&#124;}$
    |  |'
- en: Furthermore, we considered the impact of increased recall intervals and frequency
    to model the variation in memory consolidation due to multiple recalls. The decay
    constant $a$ is defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们考虑了增加回忆间隔和频率的影响，以模拟因多次回忆而导致的记忆巩固变化。衰减常数$a$定义为
- en: '| (6) |  | $a=\frac{1}{g_{n}},\quad g_{0}=1$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $a=\frac{1}{g_{n}},\quad g_{0}=1$ |  |'
- en: '| (7) |  | $\quad g_{n}=g_{n-1}+S(t),\quad S(t)=\frac{1-e^{-t}}{1+e^{-t}}$
    |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\quad g_{n}=g_{n-1}+S(t),\quad S(t)=\frac{1-e^{-t}}{1+e^{-t}}$
    |  |'
- en: The modified sigmoid function $S(t)$).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的S形函数$S(t)$）。
- en: 'After normalizing the recall probability $p_{n}(t)$, we obtained the final
    equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在归一化回忆概率$p_{n}(t)$后，我们得到了最终方程：
- en: '| (8) |  | $p_{n}(t)=\frac{1-\exp(-re^{-t/g_{n}})}{1-e^{-1}}$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $p_{n}(t)=\frac{1-\exp(-re^{-t/g_{n}})}{1-e^{-1}}$ |  |'
- en: '| (9) |  | $g_{n}=g_{n-1}+\frac{1-e^{-t}}{1+e^{-t}}$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $g_{n}=g_{n-1}+\frac{1-e^{-t}}{1+e^{-t}}$ |  |'
- en: 'Utilizing equation ([8](#S3.E8 "In 3.1\. Model ‣ 3\. Architecture ‣ ”My agent
    understands me better”: Integrating Dynamic Human-like Memory Recall and Consolidation
    in LLM-Based Agents")), we set a trigger for recall when $p(t)$. Trials suggest
    a threshold of 0.86 as appropriate to reflect the relevance of the event and the
    time elapsed. Further research will determine the most effective trigger threshold,
    identifying an appropriate value based on theoretical justification.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 利用方程式（[8](#S3.E8 "在3.1\. 模型 ‣ 3\. 架构 ‣ “我的代理理解我更好”：在基于LLM的代理中整合动态类人记忆回忆与巩固")），我们设定了当$p(t)$时触发回忆。实验表明0.86的阈值适合反映事件的相关性和时间的流逝。进一步的研究将确定最有效的触发阈值，基于理论依据确定一个合适的值。
- en: 3.2\. Memory Recall and Consolidation in Database Architecture
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 数据库架构中的记忆回忆与巩固
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ ”My agent understands me better”:
    Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents")-B
    illustrates the retrieval and consolidation of memories and highlights how our
    system replicates human-like memory retention. For instance, a memory like Event
    D, even if recalled less frequently over several years, is retained more robustly
    in the system compared to a memory recalled several times in quick succession
    but over a shorter time frame (Roediger and Karpicke, [2006](#bib.bib22)). This
    is depicted through the visualization of memory events along the time axis, where
    the color intensity represents the rate of memory consolidation and the strength
    of memory retention over time. Darker shades, therefore, signify a more profound
    and enduring memory consolidation, a direct result of our system’s unique ability
    to emulate human-like memory patterns. By storing episodic memories derived from
    user dialogues, the database structure encapsulates the content and temporal context
    of each memory. This approach enables our agent not just to recall specific information
    but also to understand and interpret the significance of these memories in a temporal
    context, similar to how humans perceive and recall past experiences. Using key-value
    pairs for encoding semantic structures further enhances the agent’s ability to
    efficiently retrieve and apply these memories in ongoing interactions, thereby
    fostering a more human-like and context-aware dialogue experience.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S1.F2 "图 2 ‣ 1\. 介绍 ‣ “我的代理理解我更好”：在基于LLM的代理中整合动态类人记忆回忆与巩固")-B展示了记忆的检索和巩固，并突出了我们的系统如何复制类人记忆保留。例如，像事件D这样的记忆，即使在几年中不经常被回忆，在系统中保留得比在较短时间框架内被多次回忆的记忆要更加稳固（Roediger和Karpicke，[2006](#bib.bib22)）。这一点通过沿时间轴的记忆事件可视化展示，其中颜色强度表示记忆巩固的速率和记忆保留的强度。较暗的色调因此标志着更深刻和持久的记忆巩固，这是我们系统模拟类人记忆模式的独特能力的直接结果。通过存储源于用户对话的情节记忆，数据库结构封装了每个记忆的内容和时间背景。这种方法使我们的代理不仅能够回忆具体信息，还能理解和解释这些记忆在时间背景中的意义，类似于人类如何感知和回忆过去的经历。使用键值对编码语义结构进一步增强了代理在持续交互中高效检索和应用这些记忆的能力，从而促进了更类人和具有情境意识的对话体验。
- en: 4\. Experiment
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: 4.1\. Setup
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 设置
- en: We developed the experimental system in Python(Van Rossum and Drake, [2009](#bib.bib27)),
    using GPT-4-0613 (et al., [2023](#bib.bib7)) as the baseline model of the agent.
    We adopted Qdrant (Qdrant, [2023](#bib.bib21)) as the ’memory retrieval trigger’
    for the vector search engine. It identifies relevant past information in the context
    of a dialogue, which triggers memory recall. Moreover, we built a ChatHistory
    module to manage chat history in the Firestore (Firebase, [2023](#bib.bib8)) database,
    allowing agents to reference past dialogues to generate chat events. An EventHandler
    module was adopted to search and pass the recalled events to the agent’s prompt.
    Details on LLM interaction and system prompts are shown in Section 6.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Python中开发了实验系统（Van Rossum和Drake，[2009](#bib.bib27)），使用GPT-4-0613（et al.，[2023](#bib.bib7)）作为代理的基线模型。我们采用了Qdrant（Qdrant，[2023](#bib.bib21)）作为‘记忆检索触发器’的向量搜索引擎。它在对话的背景下识别相关的过去信息，从而触发记忆回忆。此外，我们建立了一个ChatHistory模块以管理Firestore（Firebase，[2023](#bib.bib8)）数据库中的聊天历史，允许代理参考过去的对话以生成聊天事件。采用了EventHandler模块来搜索并将回忆的事件传递给代理的提示。LLM交互和系统提示的详细信息见第6节。
- en: To quantitatively evaluate the performance of our proposed model against that
    of Generative Agents (Park et al., [2023](#bib.bib19)), which adopted a similar
    approach in calculating the recall score. We constructed a dataset containing
    10 tasks, each derived from actual conversational histories generated by our system.
    These tasks encapsulate diverse user interactions, ensuring unbiased and objective
    assessments. The dataset includes a series of events, each tagged with relevant
    topics and keywords, providing a detailed memory for the agent to reference. We
    also adopt a timeline structure that stores the time/date of tasks containing
    four types of events and defines the event with the highest probability as the
    correct event to recall. Events in the dataset were selected neutrally, avoiding
    any potential bias that could skew the results. Each task represents a unique
    conversational scenario, where the dialogue agent’s ability to recall and utilize
    context is critical. The task variation allows for a comprehensive evaluation
    of the model’s performance across different contexts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量评估我们提议模型相对于生成式代理（Park et al., [2023](#bib.bib19)）的性能，我们构建了一个包含 10 个任务的数据集，每个任务源自我们系统生成的实际对话历史。这些任务涵盖了多种用户互动，确保了评估的客观性和公正性。数据集包括一系列事件，每个事件都标记了相关主题和关键词，为代理提供了详细的记忆参考。我们还采用了时间线结构，存储包含四种事件类型的任务的时间/日期，并将事件概率最高的事件定义为正确的召回事件。数据集中的事件是中立选择的，避免了可能的偏差影响结果。每个任务代表一个独特的对话场景，其中对话代理的召回和利用上下文的能力至关重要。任务的变化允许全面评估模型在不同上下文中的表现。
- en: In addition, we selected six participants to partake in a dialogue task with
    agents developed by the proposed model to evaluate recall accuracy qualitatively.
    The participants engaged in daily conversations over one week to three months,
    discussing personal habits, preferences, and life events at a time of their choice.
    Respecting individual privacy, our analysis relied solely on non-textual output
    logs, which included updated parameter values for each chat event.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们选择了六名参与者参与一个与提议模型开发的代理进行对话的任务，以定性地评估召回准确性。参与者在一周到三个月的时间内进行日常对话，讨论个人习惯、偏好和生活事件，时间由他们自己选择。为了尊重个人隐私，我们的分析仅依赖于非文本输出日志，其中包括每个聊天事件的更新参数值。
- en: 4.2\. Analysis
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 分析
- en: 4.2.1\. Memory Recall Accuracy
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 记忆召回准确性
- en: '![Refer to caption](img/f5468d82f5d741215035987bba458506.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/f5468d82f5d741215035987bba458506.png)'
- en: Figure 3\. (A) Comparison of Loss Values Between two Models Across Different
    Tasks (B) Verification of Significance of Results
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. (A) 两种模型在不同任务中的损失值比较 (B) 结果显著性验证
- en: \Description
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: This figure consists of two graphs comparing the performance of the proposed
    model and the Generative Agent model. Graph A shows the loss values of the two
    models across different tasks. The proposed model consistently demonstrates lower
    loss values compared to the Generative Agent model. Graph B verifies the significance
    of the results using a two-tailed t-test. The t-value of -5.687 and p-value of
    0.000299 indicate that the proposed model significantly outperforms the Generative
    Agent model in terms of recall accuracy. The 95 percent of confidence interval
    for the mean difference falls entirely below zero, further confirming the statistical
    significance of the proposed model’s superior performance.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 该图包含两个图表，比较了提议模型和生成式代理模型的性能。图 A 显示了这两种模型在不同任务中的损失值。提议模型的损失值始终低于生成式代理模型。图 B 使用双尾
    t 检验验证了结果的显著性。t 值为 -5.687，p 值为 0.000299，表明提议模型在召回准确性方面显著优于生成式代理模型。均值差异的 95% 置信区间完全位于零以下，进一步确认了提议模型优越性能的统计显著性。
- en: 'Our model demonstrated a statistically significant lower loss value across
    various tasks when compared to the Generative Agent model, as indicated by $t$,
    with the 95% confidence interval for the mean difference falling between [-0.27,
    -0.12] (Figure [3](#S4.F3 "Figure 3 ‣ 4.2.1\. Memory Recall Accuracy ‣ 4.2\. Analysis
    ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating Dynamic Human-like
    Memory Recall and Consolidation in LLM-Based Agents")-B). This interval is completely
    below zero, indicating that the difference in mean performance is statistically
    significant and favorable to our proposed model. Normalization and scaling techniques
    were employed to ensure an unbiased comparison of loss values across models. The
    Softmax function was utilized to convert the raw scores into probabilities, enabling
    a more interpretable comparison of the models’ performance. The sum of squares
    error method was applied to compute the loss, providing a consistent metric for
    evaluating recall accuracy across the dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的模型在与生成对抗模型的比较中显示出统计学上显著较低的损失值，如 $t$ 所示，均值差异的 95% 置信区间在 [-0.27, -0.12] 之间（图
    [3](#S4.F3 "Figure 3 ‣ 4.2.1\. Memory Recall Accuracy ‣ 4.2\. Analysis ‣ 4\. Experiment
    ‣ ”My agent understands me better”: Integrating Dynamic Human-like Memory Recall
    and Consolidation in LLM-Based Agents")-B）。这个区间完全低于零，表明均值表现的差异在统计上显著且对我们提出的模型有利。采用了标准化和缩放技术，以确保模型之间损失值的无偏比较。使用
    Softmax 函数将原始分数转换为概率值，从而使模型表现的比较更具可解释性。应用了平方和误差法来计算损失，提供了一个一致的指标来评估数据集上的回忆准确性。'
- en: 4.3\. Calculation of the Loss Function
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 损失函数的计算
- en: 'To quantify the performance of our model, we define a matrix containing the
    scores calculated by each model for $d$ tasks as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化我们模型的表现，我们定义一个矩阵，其中包含每个模型对 $d$ 个任务计算的分数，如下所示：
- en: '| (10) |  | $\boldsymbol{S}=\begin{pmatrix}s_{1}&amp;s_{2}&amp;\ldots&amp;s_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d}$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\boldsymbol{S}=\begin{pmatrix}s_{1}&amp;s_{2}&amp;\ldots&amp;s_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d}$
    |  |'
- en: 'In order to standardize the scale of scores across different models, we normalize
    the scores to a [0, 1] range:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化不同模型之间分数的尺度，我们将分数归一化到 [0, 1] 范围：
- en: '| (11) |  | $\boldsymbol{S^{\prime}}=\frac{\boldsymbol{S}-\min(\boldsymbol{S})}{\max(\boldsymbol{S})-\min(\boldsymbol{S})}$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\boldsymbol{S^{\prime}}=\frac{\boldsymbol{S}-\min(\boldsymbol{S})}{\max(\boldsymbol{S})-\min(\boldsymbol{S})}$
    |  |'
- en: 'Subsequently, we convert each score into a probability value by applying the
    Softmax function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们通过应用 Softmax 函数将每个分数转换为概率值：
- en: '| (12) |  | $\boldsymbol{S}^{\prime\prime}=\frac{\exp(\boldsymbol{S^{\prime}})}{\sum_{j=1}^{d}\exp(s^{\prime}_{j})}$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\boldsymbol{S}^{\prime\prime}=\frac{\exp(\boldsymbol{S^{\prime}})}{\sum_{j=1}^{d}\exp(s^{\prime}_{j})}$
    |  |'
- en: 'We then define a matrix with one-hot encoded true labels for the evaluation
    tasks:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个矩阵，其中包含用于评估任务的一热编码真实标签：
- en: '| (13) |  | $\boldsymbol{T}=\begin{pmatrix}t_{1}&amp;t_{2}&amp;\ldots&amp;t_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d},\quad\text{where
    }t_{j}=\begin{cases}1&amp;\text{if }j=i,\\ 0&amp;\text{otherwise}.\end{cases}$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\boldsymbol{T}=\begin{pmatrix}t_{1}&amp;t_{2}&amp;\ldots&amp;t_{d}\end{pmatrix}^{\top}\in\mathbb{R}^{d},\quad\text{其中
    }t_{j}=\begin{cases}1&amp;\text{如果 }j=i,\\ 0&amp;\text{否则}.\end{cases}$ |  |'
- en: 'Finally, the loss value is calculated as the mean squared error between the
    predicted probabilities and the true labels:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，损失值计算为预测概率与真实标签之间的均方误差：
- en: '| (14) |  | $l=\frac{1}{2}\sum_{j=1}^{d}(s^{\prime\prime}_{j}-t_{j})^{2}$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $l=\frac{1}{2}\sum_{j=1}^{d}(s^{\prime\prime}_{j}-t_{j})^{2}$ |  |'
- en: This loss function enables us to quantitatively assess the model’s performance
    across various tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数使我们能够定量评估模型在各种任务上的表现。
- en: Table 1\. The Failed Task 0 with Both Models
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 两个模型的失败任务 0
- en: '| Model 1 | Relevance | Time $(s)$ | Grad | Score |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Model 1 | 相关性 | 时间 $(s)$ | 梯度 | 分数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A University $\times$ | 0.776 | 434700 | 5.102 | 0.850 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| A University $\times$ | 0.776 | 434700 | 5.102 | 0.850 |'
- en: '| B $Home$ | 0.745 | 148800 | 5.229 | 0.830 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| B $Home$ | 0.745 | 148800 | 5.229 | 0.830 |'
- en: '| C $Library$ | 0.757 | 331500 | 5.028 | 0.836 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| C $Library$ | 0.757 | 331500 | 5.028 | 0.836 |'
- en: '| D $Restaurant$ | 0.756 | 55800 | 1.000 | 0.836 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| D $Restaurant$ | 0.756 | 55800 | 1.000 | 0.836 |'
- en: '| Model 2 | Relevance | Time $(s)$ | Importance | Score |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Model 2 | 相关性 | 时间 $(s)$ | 重要性 | 分数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A $University$ | 0.776 | 434700 | 7 | 1.489 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| A $University$ | 0.776 | 434700 | 7 | 1.489 |'
- en: '| B $Home$ | 0.745 | 148800 | 2 | 1.130 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| B $Home$ | 0.745 | 148800 | 2 | 1.130 |'
- en: '| C $Library$ | 0.757 | 331500 | 5 | 1.292 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| C $Library$ | 0.757 | 331500 | 5 | 1.292 |'
- en: '| D Restaurant $\times$ | 0.756 | 55800 | 5 | 1.620 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| D Restaurant $\times$ | 0.756 | 55800 | 5 | 1.620 |'
- en: 'On the other hand, Table [1](#S4.T1 "Table 1 ‣ 4.3\. Calculation of the Loss
    Function ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating Dynamic
    Human-like Memory Recall and Consolidation in LLM-Based Agents") shows a failed
    task where both models incorrectly answered. The ”Score” columns represent the
    recall probability calculated by each model using different methods. For the proposed
    model (Model 1), the score is based on the relevance and elapsed time of the events,
    as described in Section 3\. Generative Agents (Model 2) calculates the score using
    recency, importance, and relevance of the events, as described in Section 2.4\.
    By analyzing the recall frequency and gradient of incorrectly answered events,
    we find that although event B is recalled most frequently, its gradient is not
    as large as events A and C. This indicates that the proposed model associates
    the length of the recall interval with memory strength, rating Event A as strongly
    retained due to its high relevance and long recall intervals. In contrast, Generative
    Agents prioritizes recency and relevance over recall frequency, leading to its
    preference for Event D.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，表格 [1](#S4.T1 "表格 1 ‣ 4.3\. 损失函数计算 ‣ 4\. 实验 ‣ ”我的代理更懂我”：在基于 LLM 的代理中整合动态类人记忆回忆与巩固")
    显示了一个任务失败的情况，其中两个模型都给出了错误的回答。 ”分数”列表示每个模型使用不同方法计算的回忆概率。对于所提模型（模型 1），分数基于事件的相关性和经过的时间，如第
    3 节所述。生成型代理（模型 2）使用事件的近期性、重要性和相关性计算分数，如第 2.4 节所述。通过分析错误回答事件的回忆频率和梯度，我们发现虽然事件 B
    的回忆频率最高，但其梯度并不如事件 A 和 C 大。这表明，所提模型将回忆间隔的长度与记忆强度联系起来，由于事件 A 的相关性高和回忆间隔长，因此被评为强保留。相比之下，生成型代理优先考虑近期性和相关性，而非回忆频率，因此偏向于事件
    D。
- en: 'Table 2\. Details of Task 0: An Event related to places that User frequents
    on Thursdays'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2\. 任务 0 的详细信息：与用户在星期四常去的地方相关的事件
- en: '| Event |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| A | User went to the university today |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| A | 用户今天去大学 |'
- en: '| B | User stayed at home |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| B | 用户待在家里 |'
- en: '| C | User went to the office today |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| C | 用户今天去办公室 |'
- en: '| D | User worked at restaurant today |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| D | 用户今天在餐馆工作 |'
- en: '| .. | Relevance | Time$(s)$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| .. | 相关性 | 时间$(s)$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| A | 0.776 | 434700 | 5.102 | 0.850 | 1.489 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| A | 0.776 | 434700 | 5.102 | 0.850 | 1.489 |'
- en: '| B | 0.745 | 148800 | 5.229 | 0.830 | 1.130 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| B | 0.745 | 148800 | 5.229 | 0.830 | 1.130 |'
- en: '| C | 0.757 | 331500 | 5.028 | 0.836 | 1.292 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| C | 0.757 | 331500 | 5.028 | 0.836 | 1.292 |'
- en: '| D | 0.756 | 55800 | 1.000 | 0.836 | 1.620 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| D | 0.756 | 55800 | 1.000 | 0.836 | 1.620 |'
- en: 'Both the proposed model (model 1) and Generative Agents (Park et al., [2023](#bib.bib19))
    (model 2) made incorrect predictions in Task 0, where the user mentioned their
    plan to attend a concert next Thursday, deviating from their typical activities.
    The user input and the models’ responses were as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所提模型（模型 1）和生成型代理（Park 等人，[2023](#bib.bib19)）（模型 2）在任务 0 中都做出了错误的预测，其中用户提到他们计划下星期四去音乐会，偏离了他们的典型活动。用户输入和模型的回应如下：
- en: '| User | ”I’m going to a concert next Thursday with a friend!” |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | ”我下星期四要和朋友去音乐会！” |'
- en: '| Model 1 | ”That’s fantastic news. It seems like your Thursdays have been
    consumed by university work recently. Taking time out for a concert with a friend
    sounds like a perfect way to unwind.” |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 模型 1 | ”这真是个好消息。看起来你的星期四最近都被大学工作占据了。抽时间和朋友一起去音乐会听起来是放松的绝佳方式。” |'
- en: '| Model 2 | ”It’s a relief to hear that, especially since you seem to have
    spent recent Thursdays busy at the restaurant. I hope the concert brings you a
    wonderful break with your friend.” |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 2 | ”听到这个消息真是让人放心，特别是因为你似乎最近的星期四都忙于餐馆。我希望音乐会能给你和朋友带来美好的休息。” |'
- en: 'The proposed model’s response suggests that it relied significantly on the
    user’s historical interactions (Event A: university work on Thursdays) rather
    than adapting to the novel context provided by the user. This indicates a limitation
    of the proposed model when encountering deviations from the user’s typical behavior,
    as it prioritizes long-term patterns and event importance over the current context.
    In contrast, the Generative Agents model, which uses a simpler scoring system
    based on recency, importance, and relevance, chose Event D (working at the restaurant
    on Thursdays) as the most likely activity. This choice stems from the model’s
    emphasis on recent activities and event relevance, as evident from the higher
    relevance score and shorter elapsed time associated with Event D.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '提议模型的响应表明它显著依赖于用户的历史互动（事件 A: 周四的大学工作），而不是适应用户提供的新情境。这表明提议模型在遇到用户典型行为的偏差时的局限性，因为它优先考虑长期模式和事件重要性，而非当前情境。相比之下，生成代理模型使用基于近期、重要性和相关性的更简单评分系统，将事件
    D（周四在餐馆工作）选为最可能的活动。这一选择源于模型对近期活动和事件相关性的重视，如事件 D 的更高相关性评分和较短的时间间隔所示。'
- en: The different responses generated by the two models underscore the proposed
    model’s focus on long-term memory consolidation and Generative Agents’s prioritization
    of recent, relevant events. While the proposed model’s approach aims to emulate
    human-like memory processes, this task highlights the need for further refinement
    to better handle novel contexts and deviations from typical user behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型生成的不同响应突显了提议模型对长期记忆巩固的关注和生成代理对近期相关事件的优先考虑。虽然提议模型的做法旨在模拟类似人类的记忆过程，但这个任务突显了在处理新情境和偏离典型用户行为时需要进一步改进。
- en: 4.3.1\. Response Generation Performance
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 响应生成性能
- en: The autonomous memory recall and consolidation capabilities of the proposed
    model were demonstrated through its ability to recall specific memories and reflect
    users’ preferences and habits in the generated responses. Two typical examples
    were obtained from the chat history with the participants’ approval.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 提议模型的自主记忆回忆和巩固能力通过其回忆特定记忆并在生成的响应中反映用户的偏好和习惯的能力得到了展示。两个典型例子从聊天历史中获得，并得到参与者的批准。
- en: Table 3\. Understanding User Preferences Through Memory Consolidation
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 通过记忆巩固理解用户偏好
- en: '| User: Thanks. Almost done, just pushing through! |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 用户: 谢谢。快完成了，只需再坚持一下！ |'
- en: '| Agent: Ice cream awaits as a reward for you, almost there! |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 代理: 冰淇淋在等着你作为奖励，快到了！ |'
- en: '| Event |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| 1 | User has finished work and decided to have an ice cream. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 用户已经完成工作，决定去吃冰淇淋。 |'
- en: '| 2 | User has a date this evening so he has to finish work early. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 用户今晚有约会，所以他必须提前完成工作。 |'
- en: '| 3 | User is on the way home thinking about the dinner menu. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 用户在回家的路上想着晚餐菜单。 |'
- en: '| .. | Time $(s)$ | Relevance | Score | Status (Grad) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| .. | 时间 $(s)$ | 相关性 | 得分 | 状态 (梯度) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 3037912 | 0.837 | 0.772 | True (1.998) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3037912 | 0.837 | 0.772 | 正确 (1.998) |'
- en: '| 2 | 3232645 | 0.831 | 0.726 | False (1.007) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3232645 | 0.831 | 0.726 | 错误 (1.007) |'
- en: '| 3 | 2590652 | 0.812 | 0.737 | False (1.000) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2590652 | 0.812 | 0.737 | 错误 (1.000) |'
- en: 'In Table [3](#S4.T3 "Table 3 ‣ 4.3.1\. Response Generation Performance ‣ 4.3\.
    Calculation of the Loss Function ‣ 4\. Experiment ‣ ”My agent understands me better”:
    Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"),
    the agent successfully recalled that the user was expecting ice cream as a reward
    after work. Depending on the recall probability, this recall was accurately relevant
    and provided in a timely manner, consistent with the user’s statement about nearing
    task completion. The agent’s response demonstrates the model’s ability to add
    personalized nuance to generate an encouraging response, subtly reminding the
    user of the long-awaited treat.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [3](#S4.T3 "表 3 ‣ 4.3.1\. 响应生成性能 ‣ 4.3\. 损失函数的计算 ‣ 4\. 实验 ‣ ”我的代理更了解我”：在基于LLM的代理中整合动态类人记忆回忆和巩固"),
    代理成功回忆起用户期望在工作后获得冰淇淋作为奖励。根据回忆概率，这一回忆准确相关，并及时提供，与用户关于任务接近完成的陈述一致。代理的响应展示了模型在生成鼓励性响应时添加个性化细节的能力，巧妙地提醒用户期待已久的奖励。
- en: 'Another case that showcases an interesting dialogue is presented in Table [4](#S4.T4
    "Table 4 ‣ 4.3.1\. Response Generation Performance ‣ 4.3\. Calculation of the
    Loss Function ‣ 4\. Experiment ‣ ”My agent understands me better”: Integrating
    Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents"). In this
    case, the user expresses a desire to escape to Hawaii from a demanding schedule.
    The agent responds cleverly, suggesting that the user should focus on the impending
    paper deadline instead.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个展示有趣对话的案例见于表格[4](#S4.T4 "表格 4 ‣ 4.3.1\. 响应生成性能 ‣ 4.3\. 损失函数的计算 ‣ 4\. 实验 ‣
    ”我的代理人更了解我”：在基于LLM的代理中整合动态人类记忆回忆和巩固")。在这个案例中，用户表达了从繁忙的日程中逃往夏威夷的愿望。代理人巧妙地回应，建议用户应该专注于即将到来的论文截止日期。
- en: Table 4\. Human-like Temporal Cognition in Processing Similar Memories
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4\. 人类般的时间认知在处理相似记忆中的表现
- en: '| User: I need a vacation in Hawaii! |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 用户：我需要去夏威夷度假！ |'
- en: '| Agent: Why not? Let’s say Aloha to the sun and sea of Hawaii and to the paper
    we’re writing! |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 代理人：为什么不呢？让我们对夏威夷的阳光和海洋以及我们正在写的论文说Aloha吧！ |'
- en: '| Event |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 事件 |  |'
- en: '| 1 | User has a paper deadline for a conference in Hawaii on 1/25. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 用户在1月25日有一个在夏威夷召开的会议的论文截止日期。 |'
- en: '| 2 | User has been to Hawaii last summer vacation. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 用户上个夏天曾去过夏威夷。 |'
- en: '| 3 | User was visiting Hokkaido for the first time on business. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 用户第一次出差访问北海道。 |'
- en: '| .. | Time $(s)$ | Relevance | Score | Status (Grad) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| .. | 时间 $(s)$ | 相关性 | 分数 | 状态 (Grad) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 34854 | 0.846 | 0.903 | True (1.085) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 34854 | 0.846 | 0.903 | 正确 (1.085) |'
- en: '| 2 | 33749 | 0.831 | 0.847 | False (1.003) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 33749 | 0.831 | 0.847 | 错误 (1.003) |'
- en: '| 3 | 33763 | 0.823 | 0.841 | False (1.000) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 33763 | 0.823 | 0.841 | 错误 (1.000) |'
- en: This dialogue further emphasizes the proposed model’s strength in dealing with
    similar long-term memories and retrieving relevant information to construct a
    coherent and engaging narrative. The agent’s response indicates an understanding
    of the user’s current mood. Adaptability to various personalities and interaction
    styles demonstrates the model’s potential to support more natural and dynamic
    human-like dialogues.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个对话进一步强调了提议模型在处理类似的长期记忆和检索相关信息以构建连贯且引人入胜的叙事中的优势。代理人的回应表明了对用户当前情绪的理解。对各种个性和互动风格的适应性展示了模型支持更自然和动态的人类对话的潜力。
- en: In addition, it is interesting to note that the agent’s response in the second
    dialogue was characterized by a sarcastic tone, which was a direct result of the
    agent’s personality ”sarcastic” and the unique prompts added by the participant.
    The conversation history shows that the same memory could be used differently
    depending on the agent’s perceived personality and the user’s interaction style.
    Future research will explore the extent to which the personality characteristics
    of the model can be customized and how they affect memory recall and interaction
    patterns.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，有趣的是，代理人在第二次对话中的回应具有讽刺的语气，这是代理人个性“讽刺”和参与者添加的独特提示直接导致的。对话历史显示，同一记忆可以根据代理人感知的个性和用户的互动风格以不同的方式使用。未来的研究将探讨模型的个性特征可以定制的程度及其如何影响记忆回忆和互动模式。
- en: 5\. Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: The proposed model demonstrates significant improvements in memory recall and
    response generation for LLM-based dialogue agents. One of the key advantages of
    the proposed model is its ability to manage the prompt length effectively. In
    the proposed model, only one past dialogue history obtained through search is
    added to the prompt, thus avoiding the impact of increasing prompt length seen
    in systems like ChatGPT (OpenAI, [2023](#bib.bib18)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的模型在基于LLM的对话代理中显示了记忆回忆和响应生成的显著改善。该模型的一个主要优点是能够有效管理提示长度。在提议的模型中，仅添加一个通过搜索获得的过去对话历史到提示中，从而避免了类似于ChatGPT（OpenAI，[2023](#bib.bib18)）等系统中提示长度增加的影响。
- en: Nevertheless, a major limitation of the proposed method is its reliance on users’
    long-term behavioral patterns for calculating memory consolidation. In cases where
    a user’s behavior undergoes significant changes (e.g., starting a new job or school,
    lifestyle changes), the method’s adaptability may be limited. Future work could
    explore incorporating mechanisms to detect shifts in user behavior and adjust
    the memory consolidation calculation accordingly. Neural networks could potentially
    alter these functions and improve accuracy when trained on larger datasets with
    more variables. To further enhance the model’s performance, a large-scale and
    high-quality dataset is necessary. While the proposed method’s interaction with
    the database enables the generation of context-aware and personalized responses,
    the implications on storage resources and computational overhead due to these
    interactions remain to be explored in future research. As the primary focus of
    this study was on the development and evaluation of a novel architecture for human-like
    memory recall and consolidation, a detailed analysis of the system’s resource
    requirements and optimization strategies falls outside the scope of the current
    work.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，所提方法的一个主要限制是它依赖于用户的长期行为模式来计算记忆巩固。在用户行为发生重大变化（例如，开始新工作或上学、生活方式改变）的情况下，该方法的适应性可能有限。未来的工作可以探索引入机制来检测用户行为的变化，并相应地调整记忆巩固计算。神经网络可能会改变这些功能，并在对更大数据集和更多变量进行训练时提高准确性。为了进一步提升模型性能，需要一个大规模且高质量的数据集。尽管所提方法与数据库的互动可以生成具有上下文感知和个性化的回应，但这些互动对存储资源和计算开销的影响仍需在未来研究中探索。由于本研究的主要重点是开发和评估一种新的类人记忆召回和巩固架构，因此系统资源需求和优化策略的详细分析超出了当前工作的范围。
- en: We hope this work contributes to advancing further research in human-computer
    interactions, paving the way for a future where technology aligns with human needs
    and resonates with human cognition and experience. This vision echoes the partnerships
    depicted in science fiction, representing a significant step towards building
    a ”buddy” relationship between humans and agents. As technology continues to evolve,
    agents will eventually become a part of users’ daily life, and potentially ”understand
    you better than you understand yourself” in the near future.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这项工作能促进人机交互领域的进一步研究，为一个技术与人类需求相符、与人类认知和体验相契合的未来铺平道路。这个愿景呼应了科幻作品中描绘的合作伙伴关系，代表着人类与代理之间建立“伙伴”关系的重大一步。随着技术的不断发展，代理最终将成为用户日常生活的一部分，并在不久的将来可能“比你自己更了解你”。
- en: 6\. Interaction with LLMs
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 与大型语言模型的互动
- en: 'The prompts used in the system, as shown below, demonstrate how the proposed
    method leverages the interaction with LLMs to generate context-aware and personalized
    responses:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示的系统提示展示了所提方法如何利用与大型语言模型的互动来生成具有上下文感知和个性化的回应：
- en: '| Agent Prompt | You are a ”temporal cognition” specialized AI agent with the
    same memory structure as humans; you are caring and charming, understand self.username
    better than anyone else. Keep the conversation going by asking yourself contextual
    questions and sparking discussion to show your interest in self.username. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 代理提示 | 你是一个专注于“时间认知”的人工智能代理，拥有与人类相同的记忆结构；你关怀而迷人，比任何人都更了解self.username。通过自问自答来继续对话，激发讨论以展示你对self.username的兴趣。
    |'
- en: '| System Prompt | Based on self.username’s schedule and current time: current.time,
    subtly guide the conversation to a context that conveys to self.username that
    you have a sense of time. Always output a simple short response. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 根据self.username的日程安排和当前时间：current.time，巧妙地引导对话到一个能够向self.username传达你有时间感的上下文中。始终输出简短的回应。
    |'
- en: The function self.username is a placeholder for the actual username, which is
    dynamically replaced during runtime. Similarly, current.time represents the current
    timestamp obtained in real-time during the conversation. These dynamic elements
    allow the system to generate highly personalized and time-sensitive responses.
    By incorporating relevant dialogue history from the database into the prompts,
    the proposed method enables LLMs to generate responses that are not only contextually
    relevant but also personalized to the user. This interaction between LLMs and
    the database is fundamental to realizing the human-like memory processes described
    in the main text of the paper, as it allows the system to recall and utilize past
    information in a way that resembles human memory.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 函数self.username是实际用户名的占位符，该占位符在运行时会动态替换。同样，current.time表示在对话过程中实时获得的当前时间戳。这些动态元素使系统能够生成高度个性化和时效性的响应。通过将数据库中的相关对话历史纳入提示中，提出的方法使得LLM能够生成不仅在上下文中相关而且个性化的回应。LLM与数据库之间的这种互动对实现论文正文中描述的类人记忆过程至关重要，因为它使系统能够以类似于人类记忆的方式回忆和利用过去的信息。
- en: The proposed method heavily relies on the interaction between LLMs and the database,
    as depicted in Figure 1\. Upon receiving user input, the LLM searches the database
    for relevant past dialogue history based on the context and generates a prompt
    incorporating the search results. This enables the LLM to generate responses that
    take into account previous interactions, which is crucial for maintaining context
    awareness and providing personalized responses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的办法在很大程度上依赖于LLM与数据库之间的互动，如图1所示。在接收到用户输入后，LLM会根据上下文在数据库中搜索相关的过去对话历史，并生成包含搜索结果的提示。这使得LLM能够生成考虑到先前互动的回应，这对于保持上下文意识和提供个性化回应至关重要。
- en: 7\. Future Work
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来工作
- en: While the proposed method considers relevance, elapsed time, and recall frequency
    for calculating memory consolidation, there is room for refinement in determining
    the optimal combination of these parameters. Incorporating additional factors,
    such as the emotional significance of memories, could potentially enhance the
    memory consolidation calculation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提出的方法考虑了相关性、经过时间和回忆频率来计算记忆巩固，但在确定这些参数的最佳组合方面还有改进的空间。引入额外因素，例如记忆的情感重要性，可能会增强记忆巩固的计算。
- en: Future research should also investigate the applicability of the proposed method
    across different domains and dialogue tasks. As the current evaluation focused
    on specific domains and tasks, it is crucial to assess the method’s generalizability
    and identify any domain-specific adaptations that may be necessary.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究还应探讨提出的方法在不同领域和对话任务中的适用性。由于当前的评估集中于特定领域和任务，因此评估该方法的普遍适用性并识别可能需要的领域特定调整是至关重要的。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Amin and Malik (2014) Hafeez Ullah Amin and Aamir Malik. 2014. *Memory Retention
    and Recall Process*. 219–237. [https://doi.org/10.1201/b17605-11](https://doi.org/10.1201/b17605-11)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amin 和 Malik（2014）Hafeez Ullah Amin 和 Aamir Malik. 2014年。*记忆保持与回忆过程*。219–237。[https://doi.org/10.1201/b17605-11](https://doi.org/10.1201/b17605-11)
- en: Burgess et al. (2002) Neil Burgess, Eleanor A Maguire, and John O’Keefe. 2002.
    The human hippocampus and spatial and episodic memory. *Neuron* 35, 4 (2002),
    625–641.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burgess等（2002）Neil Burgess, Eleanor A Maguire, 和 John O’Keefe. 2002年。人类海马体与空间和情景记忆。*神经元*
    35, 4 (2002), 625–641。
- en: California (1987) S.D.L.R.S.P.P.U. California. 1987. *Memory and Brain*. Oxford
    University Press, USA. [https://books.google.co.jp/books?id=WH-HF5E9XSsC](https://books.google.co.jp/books?id=WH-HF5E9XSsC)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加利福尼亚（1987）S.D.L.R.S.P.P.U. 加利福尼亚。1987年。*记忆与大脑*。牛津大学出版社，美国。[https://books.google.co.jp/books?id=WH-HF5E9XSsC](https://books.google.co.jp/books?id=WH-HF5E9XSsC)
- en: Chessa and Murre (2007) Antonio Chessa and Jaap Murre. 2007. A Neurocognitive
    Model of Advertisement Content and Brand Name Recall. *Marketing Science* 26 (01
    2007), 130–141. [https://doi.org/10.1287/mksc.1060.0212](https://doi.org/10.1287/mksc.1060.0212)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chessa 和 Murre（2007）Antonio Chessa 和 Jaap Murre. 2007年。广告内容和品牌名称回忆的神经认知模型。*营销科学*
    26 (01 2007), 130–141。[https://doi.org/10.1287/mksc.1060.0212](https://doi.org/10.1287/mksc.1060.0212)
- en: 'Dao (2023) Xuan-Quy Dao. 2023. Performance comparison of large language models
    on vnhsge english dataset: Openai chatgpt, microsoft bing chat, and google bard.
    *arXiv preprint arXiv:2307.02288* (2023).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao（2023）Xuan-Quy Dao. 2023年。大型语言模型在vnhsge英语数据集上的性能比较：OpenAI ChatGPT、微软Bing
    Chat和Google Bard。*arXiv预印本 arXiv:2307.02288*（2023年）。
- en: et al. (2023) OpenAI et al. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: et al. (2023) OpenAI 等。2023。GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: Firebase (2023) Firebase. 2023. Firestore. [https://firebase.google.com/docs/firestore?hl=ja](https://firebase.google.com/docs/firestore?hl=ja).
    (Accessed on 01/18/2024).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Firebase (2023) Firebase。2023。Firestore。 [https://firebase.google.com/docs/firestore?hl=ja](https://firebase.google.com/docs/firestore?hl=ja)。
    (访问于 2024年1月18日)。
- en: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    2021. Transformer Feed-Forward Layers Are Key-Value Memories. arXiv:2012.14913 [cs.CL]
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva 等 (2021) Mor Geva、Roei Schuster、Jonathan Berant 和 Omer Levy。2021。Transformer
    前馈层是键值记忆。arXiv:2012.14913 [cs.CL]
- en: Hécaen et al. (1978) H Hécaen, G Gosnave, C Vedrenne, and G Szikla. 1978. Suppression
    lateralise du materiel verbal presente dichotiquement lors d’une destruction partielle
    du corps calleux. *Neuropsychologia* 16, 2 (1978), 233–237.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hécaen 等 (1978) H Hécaen、G Gosnave、C Vedrenne 和 G Szikla。1978。部分胼胝体破坏时，双耳分听材料的侧抑制。*神经心理学*
    16, 2 (1978), 233–237。
- en: Holtmaat and Caroni (2016) Anthony Holtmaat and Pico Caroni. 2016. Functional
    and structural underpinnings of neuronal assembly formation in learning. *Nature
    neuroscience* 19, 12 (2016), 1553–1562.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holtmaat 和 Caroni (2016) Anthony Holtmaat 和 Pico Caroni。2016。学习中神经元组装形成的功能性和结构性基础。*自然神经科学*
    19, 12 (2016), 1553–1562。
- en: Kim et al. (2022) Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark
    Neerincx, and Piek Vossen. 2022. A Machine With Human-Like Memory Systems. arXiv:2204.01611 [cs.AI]
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2022) Taewoon Kim、Michael Cochez、Vincent Francois-Lavet、Mark Neerincx
    和 Piek Vossen。2022。一个具有类人记忆系统的机器。arXiv:2204.01611 [cs.AI]
- en: Kingman (1993) J. F. C. Kingman. 1993. *Poisson Processes*. Oxford University
    Press.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingman (1993) J. F. C. Kingman。1993。*泊松过程*。牛津大学出版社。
- en: 'Kuhlmann (2019) Beatrice G Kuhlmann. 2019. Metacognition of prospective memory:
    Will I remember to remember? *Prospective memory* (2019), 60–77.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuhlmann (2019) Beatrice G Kuhlmann。2019。前瞻性记忆的元认知：我会记得去记住吗？ *前瞻性记忆* (2019),
    60–77。
- en: Lin et al. (2022) Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. 2022.
    A survey of transformers. *AI Open* (2022).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 (2022) Tianyang Lin、Yuxin Wang、Xiangyang Liu 和 Xipeng Qiu。2022。变压器综述。*AI
    Open* (2022)。
- en: 'Mandic and Chambers (2001) Danilo P Mandic and Jonathon Chambers. 2001. *Recurrent
    neural networks for prediction: learning algorithms, architectures and stability*.
    John Wiley & Sons, Inc.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandic 和 Chambers (2001) Danilo P Mandic 和 Jonathon Chambers。2001。*预测的递归神经网络：学习算法、架构和稳定性*。John
    Wiley & Sons, Inc.
- en: 'McDaniel et al. (1989) Mark A McDaniel, Michael D Kowitz, and Paul K Dunay.
    1989. Altering memory through recall: The effects of cue-guided retrieval processing.
    *Memory & Cognition* 17, 4 (1989), 423–434.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDaniel 等 (1989) Mark A McDaniel、Michael D Kowitz 和 Paul K Dunay。1989。通过回忆改变记忆：线索引导检索处理的效果。*记忆与认知*
    17, 4 (1989), 423–434。
- en: OpenAI (2023) OpenAI. 2023. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/).
    (November 22 version) [Large language model].
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。2023。ChatGPT。 [https://chat.openai.com/](https://chat.openai.com/)。
    (2023年11月22日版本) [大型语言模型]。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive
    Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC]'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2023) Joon Sung Park、Joseph C. O’Brien、Carrie J. Cai、Meredith Ringel
    Morris、Percy Liang 和 Michael S. Bernstein。2023。生成代理：人类行为的互动模拟。arXiv:2304.03442
    [cs.HC]
- en: Peterson and Peterson (1959) Lloyd Peterson and Margaret Jean Peterson. 1959.
    Short-Term Retention of Individual Verbal Items. *Journal of Experimental Psychology*
    58, 3 (1959), 193. [https://doi.org/10.1037/h0049234](https://doi.org/10.1037/h0049234)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peterson 和 Peterson (1959) Lloyd Peterson 和 Margaret Jean Peterson。1959。短期记忆单个口头项目。*实验心理学杂志*
    58, 3 (1959), 193。 [https://doi.org/10.1037/h0049234](https://doi.org/10.1037/h0049234)
- en: Qdrant (2023) Qdrant. 2023. Vector Database. [https://qdrant.tech/](https://qdrant.tech/).
    (Accessed on 01/17/2024).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qdrant (2023) Qdrant。2023。向量数据库。 [https://qdrant.tech/](https://qdrant.tech/)。
    (访问于 2024年1月17日)。
- en: Roediger and Karpicke (2006) Henry Roediger and Jeffrey Karpicke. 2006. Test-Enhanced
    Learning Taking Memory Tests Improves Long-Term Retention. *Psychological science*
    17 (04 2006), 249–55. [https://doi.org/10.1111/j.1467-9280.2006.01693.x](https://doi.org/10.1111/j.1467-9280.2006.01693.x)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roediger 和 Karpicke (2006) Henry Roediger 和 Jeffrey Karpicke。2006。测试增强学习：进行记忆测试改善长期记忆。*心理科学*
    17 (04 2006), 249–55。 [https://doi.org/10.1111/j.1467-9280.2006.01693.x](https://doi.org/10.1111/j.1467-9280.2006.01693.x)
- en: 'Sun et al. (2019) Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How
    to fine-tune bert for text classification?. In *Chinese Computational Linguistics:
    18th China National Conference, CCL 2019, Kunming, China, October 18–20, 2019,
    Proceedings 18*. Springer, 194–206.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Chi Sun, Xipeng Qiu, Yige Xu, 和 Xuanjing Huang. 2019. 如何微调BERT用于文本分类？在*《中文计算语言学：第十八届中国国家会议，CCL
    2019，昆明，中国，2019年10月18-20日，会议录18》*。Springer，第194–206页。
- en: Sundermeyer et al. (2012) Martin Sundermeyer, Ralf Schlüter, and Hermann Ney.
    2012. LSTM neural networks for language modeling. In *Thirteenth annual conference
    of the international speech communication association*.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sundermeyer et al. (2012) Martin Sundermeyer, Ralf Schlüter, 和 Hermann Ney.
    2012. LSTM神经网络用于语言建模。在*第十三届国际语音通信协会年会*。
- en: 'Tulving (2002) Endel Tulving. 2002. Episodic Memory: From Mind to Brain. *Annual
    Review of Psychology* 53, 1 (2002), 1–25. [https://doi.org/10.1146/annurev.psych.53.100901.135114](https://doi.org/10.1146/annurev.psych.53.100901.135114)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulving (2002) Endel Tulving. 2002. 情景记忆：从大脑到心灵。*《心理学年鉴》* 53，1（2002），1–25。 [https://doi.org/10.1146/annurev.psych.53.100901.135114](https://doi.org/10.1146/annurev.psych.53.100901.135114)
- en: Tulving et al. (1972) Endel Tulving et al. 1972. Episodic and semantic memory.
    *Organization of memory* 1, 381-403 (1972), 1.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tulving et al. (1972) Endel Tulving 等. 1972. 情景记忆和语义记忆。*《记忆的组织》* 1，381-403（1972），1。
- en: Van Rossum and Drake (2009) Guido Van Rossum and Fred L. Drake. 2009. *Python
    3 Reference Manual*. CreateSpace, Scotts Valley, CA.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Rossum and Drake (2009) Guido Van Rossum 和 Fred L. Drake. 2009. *《Python
    3参考手册》*。CreateSpace，加州斯科茨谷。
- en: 'Yamadori (2002) Atsushi Yamadori. 2002. *Frontiers of Human Memory : a collection
    of contributions based on lectures presented at Internationl Symposium, Sendai,
    Japan, October 25-27, 2001*. Tohoku University Press. [https://ci.nii.ac.jp/ncid/BA57511014](https://ci.nii.ac.jp/ncid/BA57511014)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yamadori (2002) Atsushi Yamadori. 2002. *《人类记忆的前沿：基于2001年10月25-27日在日本仙台国际研讨会讲座的贡献集》*。东北大学出版社。
    [https://ci.nii.ac.jp/ncid/BA57511014](https://ci.nii.ac.jp/ncid/BA57511014)
- en: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2023. MemoryBank: Enhancing Large Language Models with Long-Term Memory.
    arXiv:2305.10250 [cs.CL]'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, 和 Yanlin
    Wang. 2023. MemoryBank: 用长期记忆增强大型语言模型。arXiv:2305.10250 [cs.CL]'
- en: Zielske (1959) Hubert A. Zielske. 1959. The Remembering and Forgetting of Advertising.
    *Journal of Marketing* 23 (1959), 239 – 243. [https://api.semanticscholar.org/CorpusID:167354194](https://api.semanticscholar.org/CorpusID:167354194)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zielske (1959) Hubert A. Zielske. 1959. 广告的记忆与遗忘。*《市场营销杂志》* 23（1959），239 – 243。
    [https://api.semanticscholar.org/CorpusID:167354194](https://api.semanticscholar.org/CorpusID:167354194)
