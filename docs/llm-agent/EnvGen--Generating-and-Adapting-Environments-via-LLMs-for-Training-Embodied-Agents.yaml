- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EnvGen: 通过LLM生成和适应环境以训练具身代理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.12014](https://ar5iv.labs.arxiv.org/html/2403.12014)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.12014](https://ar5iv.labs.arxiv.org/html/2403.12014)
- en: 'Abhay Zala  Jaemin Cho^†^†footnotemark:  Han Lin  Jaehong Yoon  Mohit Bansal'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Abhay Zala  Jaemin Cho^†^†footnotemark:  Han Lin  Jaehong Yoon  Mohit Bansal'
- en: UNC Chapel Hill
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: UNC Chapel Hill
- en: '{aszala, jmincho, hanlincs, jhyoon, mbansal}@cs.unc.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{aszala, jmincho, hanlincs, jhyoon, mbansal}@cs.unc.edu'
- en: '[https://envgen-llm.github.io](https://envgen-llm.github.io) equal contribution'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://envgen-llm.github.io](https://envgen-llm.github.io) 等同贡献'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Recent state-of-the-art approaches for embodied learning via interaction directly
    employ large language models (LLMs) as agents to determine the next steps in an
    environment. Due to their world knowledge and reasoning capabilities, LLM agents
    achieve stronger performance than previous smaller agents based on reinforcement
    learning (RL); however, frequently calling LLMs is slow and expensive. This begs
    an interesting question: Instead of directly employing LLMs as embodied agents,
    can we use LLMs’ reasoning capabilities to adaptively create training environments
    to help smaller embodied RL agents learn useful skills that they are weak at?
    In this work, we propose EnvGen, a novel framework to address this question. First,
    we prompt an LLM to generate training environments that allow agents to quickly
    learn different tasks in parallel. Concretely, the LLM is given the task description
    and environment simulator objectives that the agents should learn and is then
    asked to generate a set of environment configurations (*e.g*., different terrains,
    items initially given to agents, chance of finding certain objects, *etc*.). Next,
    we train a small RL agent in a mixture of the original and LLM-generated environments.
    Then, we enable the LLM to *continuously adapt* the generated environments to
    progressively improve the skills that the agent is weak at, by providing feedback
    to the LLM in the form of the agent’s performance. We demonstrate the usefulness
    of EnvGen with comprehensive experiments in Crafter and Heist game environments.
    We find that a small RL agent trained with EnvGen can outperform SOTA methods,
    including a GPT-4 agent, and learns long-horizon tasks significantly faster. We
    also show qualitatively how the LLM adapts training environments to help improve
    RL agents’ weaker skills over time. Additionally, EnvGen is substantially more
    efficient as it only uses a small number of LLM calls (*e.g*., 4 in total), whereas
    LLM agents require one or more LLM calls per step (resulting in thousands of LLM
    calls per episode). Lastly, we present detailed ablation studies for EnvGen’s
    design choices.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的前沿方法通过交互进行具身学习，直接将大型语言模型（LLMs）作为代理，以决定环境中的下一步。由于其世界知识和推理能力，LLM代理的表现优于以往基于强化学习（RL）的较小代理；然而，频繁调用LLM速度慢且成本高。这引出了一个有趣的问题：我们是否可以利用LLM的推理能力，适应性地创建训练环境，帮助较小的具身RL代理学习它们较弱的有用技能，而不是直接将LLM用作具身代理？在这项工作中，我们提出了EnvGen，一个解决这个问题的创新框架。首先，我们提示LLM生成训练环境，使代理能够快速并行学习不同任务。具体来说，LLM接收到任务描述和代理应学习的环境模拟器目标，然后被要求生成一组环境配置（*例如*，不同的地形、最初给予代理的物品、发现特定物体的机会，*等等*）。接下来，我们在原始环境和LLM生成的环境的混合中训练一个小型RL代理。然后，我们使LLM*持续适应*生成的环境，通过提供代理表现的反馈来逐步改善代理的弱点。我们通过在Crafter和Heist游戏环境中的全面实验展示了EnvGen的有用性。我们发现，用EnvGen训练的小型RL代理可以超越SOTA方法，包括GPT-4代理，并显著更快地学习长期任务。我们还定性展示了LLM如何随时间适应训练环境，帮助改善RL代理的较弱技能。此外，EnvGen效率显著更高，因为它仅使用少量LLM调用（*例如*，总共4次），而LLM代理每一步需要一次或多次LLM调用（导致每集数千次LLM调用）。最后，我们展示了EnvGen设计选择的详细消融研究。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'There has been growing interest in embodied AI, where agents learn through
    interactions with environments instead of static datasets (Ahn et al., [2022](#bib.bib2);
    Duan et al., [2022](#bib.bib14); Wang et al., [2023a](#bib.bib51); Yao et al.,
    [2023](#bib.bib60); Driess et al., [2023](#bib.bib12)). Open-world games such
    as Minecraft (Mojang Studios, [2009](#bib.bib35)) and Crafter (Hafner, [2022](#bib.bib19))
    have been widely used as research environments for embodied agents, where the
    agents visually perceive their surroundings, traverse large terrains, and learn
    to unlock various achievements (*e.g*., collecting resources, building tools,
    defeating monsters, *etc*.). Some achievements can be easily unlocked within a
    few steps, whereas others are more challenging as they only become accessible
    after the agent completes a series of prerequisite achievements, requiring hundreds
    of steps (*i.e*., long-horizon tasks). As illustrated in [Fig. 1](#S1.F1 "In 1
    Introduction ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents") (a), traditional embodied agents are based on reinforcement
    learning (RL) (Hafner et al., [2020](#bib.bib20); [2021](#bib.bib21); [2023](#bib.bib22);
    Schulman et al., [2017](#bib.bib41); Burda et al., [2018](#bib.bib8); Hessel et al.,
    [2018](#bib.bib24); Sekar et al., [2020](#bib.bib42); Moon et al., [2023](#bib.bib36)).
    However, these RL agents usually struggle when learning such long-horizon tasks
    since the reward is sparsely given only after the correct execution of successive
    actions, and it is very expensive to automatically find many action sequences
    which lead to the reward (Aytar et al., [2018](#bib.bib4); Li et al., [2022a](#bib.bib29);
    Yuan et al., [2023](#bib.bib61)), even after long pretraining with curiosity-driven
    intrinsic reward (Walker et al., [2023](#bib.bib50)).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具身 AI 的兴趣日益增长，这种 AI 通过与环境的互动而非静态数据集来学习（Ahn 等人，[2022](#bib.bib2); Duan 等人，[2022](#bib.bib14);
    Wang 等人，[2023a](#bib.bib51); Yao 等人，[2023](#bib.bib60); Driess 等人，[2023](#bib.bib12)）。像
    Minecraft（Mojang Studios，[2009](#bib.bib35)）和 Crafter（Hafner，[2022](#bib.bib19)）这样的开放世界游戏被广泛用作具身代理的研究环境，在这些环境中，代理可以直观地感知周围环境，穿越大面积的地形，并学习解锁各种成就（*例如*，收集资源、建造工具、击败怪物、*等等*）。一些成就可以在几步之内轻松解锁，而另一些则更具挑战性，因为它们只有在代理完成一系列先决条件之后才能获得，这通常需要数百步（*即*，长期任务）。如[图
    1](#S1.F1 "在 1 引言 ‣ EnvGen：通过 LLM 为训练具身代理生成和调整环境")（a）所示，传统的具身代理基于强化学习（RL）（Hafner
    等人，[2020](#bib.bib20); [2021](#bib.bib21); [2023](#bib.bib22); Schulman 等人，[2017](#bib.bib41);
    Burda 等人，[2018](#bib.bib8); Hessel 等人，[2018](#bib.bib24); Sekar 等人，[2020](#bib.bib42);
    Moon 等人，[2023](#bib.bib36)）。然而，这些 RL 代理在学习这种长期任务时通常会遇到困难，因为奖励通常在正确执行连续动作之后才会稀疏地给出，并且自动找到许多能够获得奖励的动作序列非常昂贵（Aytar
    等人，[2018](#bib.bib4); Li 等人，[2022a](#bib.bib29); Yuan 等人，[2023](#bib.bib61)），即使在经过长期的好奇驱动的内在奖励预训练之后（Walker
    等人，[2023](#bib.bib50)）。
- en: '![Refer to caption](img/15148b9cd1a2b3c0c15be2ef93cccd1e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/15148b9cd1a2b3c0c15be2ef93cccd1e.png)'
- en: 'Figure 1: Comparison of different methods for creating embodied agents. Previous
    works commonly use (a) small RL agents or (b) LLM agents to explore skills. In
    (c) EnvGen, we train a small RL agent with diverse LLM-generated environments
    that train different skills in parallel and can be adapted via feedback to help
    the agents progressively improve skills that they are weaker at. Our method benefits
    from using the world knowledge from LLMs while maintaining efficient training
    through a lightweight RL agent.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：创建具身代理的不同方法的比较。以往的工作通常使用（a）小型 RL 代理或（b）LLM 代理来探索技能。在（c）EnvGen 中，我们训练一个小型
    RL 代理，使用多样的 LLM 生成的环境，这些环境可以并行地训练不同的技能，并且可以通过反馈进行调整，以帮助代理逐步提高他们较弱的技能。我们的方法利用了
    LLM 的世界知识，同时通过轻量级 RL 代理保持高效的训练。
- en: 'As large language models (LLMs) have shown remarkable progress in various tasks
    that require complex reasoning (Brown et al., [2020](#bib.bib7); OpenAI, [2023a](#bib.bib38);
    Touvron et al., [2023a](#bib.bib48); [b](#bib.bib49); Chowdhery et al., [2023](#bib.bib10);
    Anil et al., [2023](#bib.bib3)), recent works study implementing embodied agents
    based on LLMs. As illustrated in [Fig. 1](#S1.F1 "In 1 Introduction ‣ EnvGen:
    Generating and Adapting Environments via LLMs for Training Embodied Agents") (b),
    these methods leverage LLMs’ world knowledge with chain-of-thought reasoning (Nye
    et al., [2021](#bib.bib37); Kojima et al., [2022](#bib.bib25); Wei et al., [2022](#bib.bib57))
    by creating action plans, giving feedback, and obtaining rewards throughout the
    episode (Yuan et al., [2023](#bib.bib61); Wang et al., [2023c](#bib.bib53); Wu
    et al., [2023](#bib.bib59); Wang et al., [2023a](#bib.bib51); [d](#bib.bib54);
    Zhao et al., [2023](#bib.bib62); Du et al., [2023](#bib.bib13)). While these LLM-based
    agents that verbalize their knowledge in reasoning steps have seen success in
    achieving better performance over previous approaches, iteratively calling LLMs
    throughout the episode is prohibitively slow and expensive (*e.g*., SPRING (Wu
    et al., [2023](#bib.bib59)) calls GPT-4 (OpenAI, [2023a](#bib.bib38)) 9 times
    to take any action step, which results in $270 USD to complete an episode). Du
    et al. ([2023](#bib.bib13)) use LLMs to create rewards to train smaller agents,
    but the training is still costly, as it requires many interactions between the
    LLMs and student agents. This begs the question: Instead of directly employing
    LLMs as embodied agents, can we use LLMs’ reasoning capability to adaptively create
    training environments to help smaller embodied RL agents learn useful skills that
    they are weak at?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '随着大型语言模型（LLMs）在需要复杂推理的各种任务中取得显著进展（Brown et al., [2020](#bib.bib7); OpenAI,
    [2023a](#bib.bib38); Touvron et al., [2023a](#bib.bib48); [b](#bib.bib49); Chowdhery
    et al., [2023](#bib.bib10); Anil et al., [2023](#bib.bib3)），近期的研究探讨了基于LLMs实现具身智能体的应用。如[图1](#S1.F1
    "In 1 Introduction ‣ EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents")（b）所示，这些方法通过创建行动计划、提供反馈和在整个过程中获得奖励，利用LLMs的世界知识进行链式推理（Nye
    et al., [2021](#bib.bib37); Kojima et al., [2022](#bib.bib25); Wei et al., [2022](#bib.bib57)）（Yuan
    et al., [2023](#bib.bib61); Wang et al., [2023c](#bib.bib53); Wu et al., [2023](#bib.bib59);
    Wang et al., [2023a](#bib.bib51); [d](#bib.bib54); Zhao et al., [2023](#bib.bib62);
    Du et al., [2023](#bib.bib13)）。虽然这些基于LLM的智能体通过推理步骤表述其知识在实现优于以往方法的表现上取得了成功，但在整个过程中迭代调用LLM是极其缓慢且昂贵的（*例如*，SPRING（Wu
    et al., [2023](#bib.bib59)）调用GPT-4（OpenAI, [2023a](#bib.bib38)）9次来执行任何一个行动步骤，这导致完成一个回合的费用高达270美元）。Du
    et al.（[2023](#bib.bib13)）利用LLMs生成奖励来训练较小的智能体，但训练成本仍然很高，因为这需要LLMs与学生智能体之间的多次互动。这就提出了一个问题：我们能否利用LLMs的推理能力来自适应地创建训练环境，以帮助较小的具身强化学习智能体学习它们较弱的有用技能，而不是直接将LLMs用作具身智能体？'
- en: 'To address this question, we propose EnvGen, a novel framework where an LLM
    adaptively generates training environments to teach smaller embodied RL agents.
    We aim to generate environments that can create various conditions (*e.g*., have
    different terrains or some subgoals are already achieved) so that agents can learn
    different skills in parallel and obtain more frequent rewards for challenging
    long-horizon tasks than in the original environment. As shown in [Fig. 1](#S1.F1
    "In 1 Introduction ‣ EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents") (c), EnvGen iterates over multiple training cycles,
    each consisting of the following four steps.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，我们提出了EnvGen，一个新颖的框架，其中LLM自适应地生成训练环境以教授较小的具身强化学习智能体。我们的目标是生成能够创造各种条件的环境（*例如*，具有不同的地形或某些子目标已达成），以便智能体可以并行学习不同的技能，并为具有挑战性的长期任务提供比原始环境更多的奖励。如[图1](#S1.F1
    "In 1 Introduction ‣ EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents")（c）所示，EnvGen在多个训练周期中迭代，每个周期包括以下四个步骤。'
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 1: We generate configurations for custom training environments (*i.e*.,
    specifically created to train an RL agent on certain skills) by providing an LLM
    with a prompt including task description, controllable simulator settings, and
    simulator constraints (see [Fig. 2](#S2.F2 "In 2 EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents") and [Sec. 2](#S2 "2 EnvGen:
    Generating and Adapting Environments via LLMs for Training Embodied Agents ‣ EnvGen:
    Generating and Adapting Environments via LLMs for Training Embodied Agents") for
    details). Then we use the generated configurations to create different custom
    environments (*e.g*., different terrains, items initially given to agents, and
    chance of finding certain objects) that can teach multiple skills in parallel.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '步骤 1：我们通过给大型语言模型（LLM）提供一个包含任务描述、可控的模拟器设置和模拟器约束的提示，生成自定义训练环境的配置（例如，专门为训练 RL
    代理在某些技能上创建的环境）（见 [图 2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents") 和 [第 2 节](#S2 "2 EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents") 了解详情）。然后，我们利用生成的配置创建不同的自定义环境（例如，不同的地形、最初给代理的物品和找到某些物体的机会），这些环境可以同时教授多种技能。'
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 2: We train the RL agent in multiple LLM-generated environments (*i.e*.,
    LLM environments), so that it can learn different useful skills in parallel.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 2：我们在多个 LLM 生成的环境（即 LLM 环境）中训练 RL 代理，以便它能够并行学习不同的有用技能。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 3: We first train the RL agent in the original environment to mitigate
    overfitting to the LLM environments. Then we measure the current RL agent’s performance
    in different tasks in the original environment to check which skills/tasks the
    agent is still weak at.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 3：我们首先在原始环境中训练 RL 代理，以减轻对 LLM 环境的过拟合。然后我们测量当前 RL 代理在原始环境中不同任务的表现，以检查代理在哪些技能/任务上仍然较弱。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Step 4: We provide the RL agent’s successes/failures in different tasks (from
    step 3) as feedback to the LLM, so that the LLM can adapt the custom training
    environments to focus on progressively improving the skills that the agent is
    weak at.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 步骤 4：我们将 RL 代理在不同任务（来自步骤 3）的成功/失败情况反馈给 LLM，以便 LLM 可以调整自定义训练环境，专注于逐步提高代理较弱的技能。
- en: Note that EnvGen only requires a few LLM calls (*e.g*., 4) for environment generation/updating
    during the entire RL agent training process, whereas other works based on LLM
    agents query an LLM once or multiple times every step (resulting in thousands
    of LLM calls for a single episode).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，EnvGen 在整个 RL 代理训练过程中只需要少量的 LLM 调用（例如，4 次）来生成/更新环境，而其他基于 LLM 的工作每一步都需要查询
    LLM 一次或多次（导致单个回合中 LLM 调用数达数千次）。
- en: 'We study the usefulness of EnvGen in different game environments: Crafter (Hafner,
    [2022](#bib.bib19)) and Heist (Cobbe et al., [2020](#bib.bib11)). In the Crafter
    environment, a simple PPO-based (Schulman et al., [2017](#bib.bib41)) lightweight
    ($<5$M parameters) RL agent trained with our LLM-generated environments outperforms
    strong baselines including a GPT-4 based agent that queries an LLM multiple times
    at every step, and RL agents that use extensive pretraining (*e.g*., 150M steps
    *vs*. less than 1M steps for us). When compared to just training longer in the
    original Crafter environment, an RL agent trained with EnvGen achieves significant
    improvements on the overall score and long-horizon tasks. In Heist, we also show
    that our LLM-generated environments can improve overall agent performance and
    training stability. We also show a qualitative study on how the LLM adapts training
    environments to help improve RL agents’ weaker skills over time. Finally, we provide
    comprehensive analysis and ablation studies of the design choices of EnvGen, including
    EnvGen *vs*. longer training in the original environment, adaptively updating
    LLM-generated environments *vs*. a fixed environment, different LLMs for generating
    environments, frequency of environment updates, the number of LLM-generated environments,
    and the mixture ratio between the original and LLM environment during training.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了 EnvGen 在不同游戏环境中的有效性：Crafter（Hafner，[2022](#bib.bib19)）和 Heist（Cobbe 等，[2020](#bib.bib11)）。在
    Crafter 环境中，一个基于 PPO（Schulman 等，[2017](#bib.bib41)）的简单轻量级（<$<5$M 参数）RL 智能体在我们的
    LLM 生成的环境中训练后表现优于强基线，包括一个基于 GPT-4 的智能体，该智能体在每一步都多次查询 LLM，以及使用 extensive pretraining（*例如*，150M
    步 *vs*. 少于 1M 步）的 RL 智能体。与在原始 Crafter 环境中进行更长时间的训练相比，使用 EnvGen 训练的 RL 智能体在总体评分和长期任务上取得了显著的改进。在
    Heist 环境中，我们还展示了我们的 LLM 生成的环境可以提高整体智能体表现和训练稳定性。我们还展示了一个定性研究，说明 LLM 如何适应训练环境，以帮助提高
    RL 智能体的弱技能。最后，我们提供了对 EnvGen 设计选择的全面分析和消融研究，包括 EnvGen *vs*. 在原始环境中更长时间的训练、适应性更新
    LLM 生成的环境 *vs*. 固定环境、生成环境的不同 LLM、环境更新的频率、LLM 生成环境的数量以及训练过程中原始环境和 LLM 环境的混合比例。
- en: '2 EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2 EnvGen: 通过 LLM 生成和调整环境以训练具身智能体'
- en: 'We propose EnvGen, a novel framework where an LLM adaptively generates training
    environments to train smaller embodied RL agents, enabling them to accomplish
    various tasks within an environment, particularly long-horizon tasks. During the
    training process, the LLM is given feedback (in the form of the agent’s performance)
    and can adaptively update the training environments to progressively focus on
    improving the tasks that the agent is weak at. In the following, we first explain
    why it is challenging to explore long-horizon tasks in open-world games ([Sec. 2.1](#S2.SS1
    "2.1 Preliminary: Exploration is Hard for Long-Horizon Tasks ‣ 2 EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents")). Then we explain
    our method details, including how we generate environments and how agents are
    trained in the generated and original environments ([Sec. 2.2](#S2.SS2 "2.2 EnvGen
    Method Details ‣ 2 EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents")).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出了 EnvGen，一个新颖的框架，通过 LLM 自适应地生成训练环境以训练较小的具身 RL 智能体，使它们能够在环境中完成各种任务，特别是长期任务。在训练过程中，LLM
    会接收反馈（以智能体表现的形式），并可以自适应地更新训练环境，以逐步集中改善智能体薄弱的任务。接下来，我们首先解释了在开放世界游戏中探索长期任务的挑战（[第
    2.1 节](#S2.SS1 "2.1 初步：探索对长期任务来说很难 ‣ 2 EnvGen: 通过 LLM 生成和调整环境以训练具身智能体 ‣ EnvGen:
    通过 LLM 生成和调整环境以训练具身智能体")）。然后我们详细说明了我们的方法，包括我们如何生成环境以及智能体如何在生成环境和原始环境中进行训练（[第 2.2
    节](#S2.SS2 "2.2 EnvGen 方法细节 ‣ 2 EnvGen: 通过 LLM 生成和调整环境以训练具身智能体 ‣ EnvGen: 通过 LLM
    生成和调整环境以训练具身智能体")）。'
- en: '![Refer to caption](img/bf0033debb3bee1567a2827b6698b9d2.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bf0033debb3bee1567a2827b6698b9d2.png)'
- en: 'Figure 2: In EnvGen framework, we generate multiple environments with an LLM
    to let the agent learn different skills effectively, with the $N^{\text{Cycle}}$
    training cycles, each consisting of the following four steps. Step 1: we provide
    an LLM with a prompt composed of four components (*i.e*., task description, environment
    details, output template, and feedback from the previous cycle), and ask the LLM
    to fill the template and output various environment configurations that can be
    used to train agents on different skills. Step 2: we train a small RL agent in
    the LLM-generated environments. Step 3: we train the agent in the original environment
    to allow for better generalization and then measure the RL agent’s training progress
    by letting it explore the original environment. Step 4: we provide the LLM with
    the agent performance from the original environment (measured in step 3) as feedback
    for adapting the LLM environments in the next cycle to focus on the weaker performing
    skills.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在 EnvGen 框架中，我们利用 LLM 生成多个环境，以便让代理有效地学习不同的技能，通过 $N^{\text{Cycle}}$ 个训练周期，每个周期包括以下四个步骤。步骤
    1：我们提供给 LLM 一个由四个组件组成的提示（*即*，任务描述、环境细节、输出模板和来自上一个周期的反馈），并要求 LLM 填写模板，输出各种环境配置，以用于训练代理的不同技能。步骤
    2：我们在 LLM 生成的环境中训练一个小型 RL 代理。步骤 3：我们在原始环境中训练代理，以便于更好的泛化，然后通过让代理探索原始环境来测量 RL 代理的训练进展。步骤
    4：我们将原始环境中代理的表现（在步骤 3 中测量）作为反馈，提供给 LLM，以便在下一个周期中调整 LLM 环境，专注于表现较弱的技能。
- en: '2.1 Preliminary: Exploration is Hard for Long-Horizon Tasks'
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 初步：长远任务的探索很困难
- en: 'In the RL framework, agents explore various states along a trajectory and amplify
    policies based on the rewards received from those trajectories. However, exploration
    for long-horizon tasks is slow and computationally expensive, as rewards for such
    tasks are sparsely given only after a sequence of successful actions that often
    involve achieving multiple subgoals. For example, the goal in Crafter (Hafner,
    [2022](#bib.bib19)) is to unlock 22 achievements, where some achievements can
    be unlocked quickly through several simple actions and others require long chains
    of prerequisites (*e.g*., collect iron requires make stone pickaxe, which must
    be preceded by collect stone, … *etc*.); see [Sec. 3.1](#S3.SS1 "3.1 Evaluated
    Benchmarks and Training Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents") for details. As
    shown in Hafner ([2022](#bib.bib19)), existing agents in Crafter spend most exploration
    steps learning low-level achievements but fail to unlock high-order achievements
    with many prerequisites.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '在 RL 框架中，代理在轨迹中探索各种状态，并根据从这些轨迹中获得的奖励来放大策略。然而，长远任务的探索速度慢且计算成本高，因为此类任务的奖励是稀疏给出的，仅在一系列成功行动之后才会出现，这些行动通常涉及实现多个子目标。例如，Crafter（Hafner，[2022](#bib.bib19)）中的目标是解锁
    22 项成就，其中一些成就可以通过几个简单的动作快速解锁，而其他则需要长链的前提条件（*例如*，收集铁需要制作石镐，这必须在收集石头之后进行，…… *等等*）；详细信息请参见
    [Sec. 3.1](#S3.SS1 "3.1 Evaluated Benchmarks and Training Details ‣ 3 Experimental
    Setup ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")。正如 Hafner ([2022](#bib.bib19)) 所示，Crafter 中的现有代理大多数探索步骤用于学习低级成就，但未能解锁具有许多前提条件的高级成就。'
- en: 2.2 EnvGen Method Details
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 EnvGen 方法细节
- en: 'We introduce EnvGen, where we train an embodied RL agent in multiple LLM-generated
    environments (we refer to these as ‘LLM environments’ in the paper) that progressively
    adapt to improve agent performance in multiple skills. The generated environments
    can provide various conditions (*e.g*., different terrains, or some subgoals are
    already achieved) so that agents can learn different skills in parallel and obtain
    more frequent rewards for long-horizon tasks. As shown in [Fig. 2](#S2.F2 "In
    2 EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents"), EnvGen iterates $N^{\text{Cycle}}$ training cycles, each consisting
    of the following four steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '我们引入了 EnvGen，在多个 LLM 生成的环境中训练具身 RL 代理（我们在论文中称这些为“LLM 环境”），这些环境逐步适应以提高代理在多个技能中的表现。生成的环境可以提供各种条件（*例如*，不同的地形，或某些子目标已被实现），以便代理能够并行学习不同的技能，并在长远任务中获得更频繁的奖励。正如
    [图 2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents") 所示，EnvGen 迭代 $N^{\text{Cycle}}$ 个训练周期，每个周期包括以下四个步骤：'
- en: 'Step 1: Generate training environments with an LLM.'
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：利用 LLM 生成训练环境。
- en: 'As illustrated in step 1 of [Fig. 2](#S2.F2 "In 2 EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents"), we use an LLM (*e.g*., GPT-4 (OpenAI,
    [2023a](#bib.bib38))¹¹1We use GPT-4-1106-Preview (*i.e*., GPT-4 Turbo).) to first
    generate $N^{\text{LLM-Env}}$ custom training environment configurations²²2We
    find that N=4 works well; see [Table 7](#S4.T7 "In Frequency of LLM feedback /
    environment updates. ‣ 4.5 Additional Analysis and Ablation Studies ‣ 4 Results
    and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents") for details. that can cover various objectives and skills that
    are required in the original environment. The following describes the LLM input
    prompt components used to create environment configurations.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")第1步所示，我们使用LLM（*例如*，GPT-4 (OpenAI, [2023a](#bib.bib38))¹¹1我们使用GPT-4-1106-Preview（*即*，GPT-4
    Turbo）。）首先生成$N^{\text{LLM-Env}}$自定义训练环境配置²²2我们发现N=4效果很好；详见[表7](#S4.T7 "In Frequency
    of LLM feedback / environment updates. ‣ 4.5 Additional Analysis and Ablation
    Studies ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents")。这些配置可以覆盖原始环境中所需的各种目标和技能。以下描述了用于创建环境配置的LLM输入提示组件。'
- en: '1.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Task description: We provide a brief description of the environment and what
    the LLM should do (*e.g*., “generate a set of training environments…”).'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务描述：我们提供对环境的简要描述以及LLM应该做什么（*例如*，“生成一组训练环境…”）。
- en: '2.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Game/simulator details: We provide a list of objectives that need to be achieved
    in the environment (*e.g*., “collect coal, collect iron, *etc*.” for Crafter);
    a list of which simulator settings can be controlled (*e.g*., terrain, agent inventory);
    and a list of constraints/rules that the simulator has (*e.g*., “skeletons only
    spawn in mountains; cows only spawn in grass; …” for Crafter).'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 游戏/模拟器详细信息：我们提供一个需要在环境中完成的目标列表（*例如*，“收集煤炭、收集铁矿、*等等*”用于Crafter）；可以控制的模拟器设置列表（*例如*，地形、代理库存）；以及模拟器的约束/规则列表（*例如*，“骷髅只在山脉中生成；牛只在草地中生成；…”用于Crafter）。
- en: '3.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Output environment configuration template: We provide a blank output configuration
    template (*i.e*., a JSON object where the environment settings are empty) to the
    LLM, and request it to fill in the values, creating $N^{\text{LLM-Env}}$ environment
    configurations. Along with filling the templates, we also ask the LLM to verbally
    explain the purpose for each environment (*e.g*., what the environment would teach
    the agent); this would help users easily understand the environment generation
    process.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出环境配置模板：我们向LLM提供一个空白的输出配置模板（*即*，一个环境设置为空的JSON对象），并要求它填写这些值，创建$N^{\text{LLM-Env}}$环境配置。在填写模板的同时，我们还要求LLM口头解释每个环境的目的（*例如*，环境会教会代理什么）；这将帮助用户更容易理解环境生成过程。
- en: '4.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Adaptation feedback based on the RL agent’s performance: We provide the LLM
    with the performance of the RL agent from the original environment (measured in
    step 3 and summarized in step 4), as feedback for adapting LLM environments to
    focus on skills that the RL agent is weak at. The feedback is given at the end
    of each cycle, so it is only provided to LLM from the second cycle onwards.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于RL代理的表现进行适应性反馈：我们将RL代理在原始环境中的表现（在第3步中测量并在第4步中总结）提供给LLM，作为反馈用于调整LLM环境，以集中于RL代理较弱的技能。反馈在每个周期结束时提供，因此仅从第二个周期开始才会提供给LLM。
- en: 'The obtained environment configurations are then rendered in the game’s simulator.
    [Fig. 2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents") presents the summary of input prompt and output
    environments from the GPT-4 model. We provide more prompt details in the appendix.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '获得的环境配置随后会在游戏的模拟器中渲染。[图2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents")展示了来自GPT-4模型的输入提示和输出环境的摘要。我们在附录中提供了更多提示细节。'
- en: 'Step 2: Train a small RL agent in the LLM-generated environments.'
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：在LLM生成的环境中训练一个小型RL代理。
- en: 'As shown in step 2 of [Fig. 2](#S2.F2 "In 2 EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents"), we first train the small
    RL agent in the LLM-generated environments. Concretely, we train the agent in
    the $N^{\text{LLM-Env}}$ total steps in parallel.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")第2步所示，我们首先在LLM生成的环境中训练小型RL代理。具体而言，我们在$N^{\text{LLM-Env}}$个总步骤中并行训练代理。'
- en: 'Step 3: Train and measure the RL agent’s performance in the original environment.'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：在原始环境中训练并测量RL代理的表现。
- en: 'It is important to note that the goal of EnvGen is to improve the RL agent’s
    performance in the original environment, instead of the performance only in the
    LLM environments. To help the RL agent effectively adapt to the original environment
    and provide the LLM with the current agent’s performance as feedback, we train
    the agent and measure its performance in the original environment, as shown in
    step 3 of [Fig. 2](#S2.F2 "In 2 EnvGen: Generating and Adapting Environments via
    LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents"). First, to mitigate the overfitting to
    LLM environments, we train the agent in the original environment for $T^{\text{Orig-Env}}$
    works well; see [Table 8](#S4.T8 "In Number of LLM environments. ‣ 4.5 Additional
    Analysis and Ablation Studies ‣ 4 Results and Analysis ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents") for details. Next,
    to find the skills that the RL agent needs to improve at, we test the agent in
    the original environment, without any parameter updates. Concretely, we measure
    individual success rates for each environment task (*e.g*., Crafter achievements).
    The agent performance is summarized (in step 4) and is provided to LLM as feedback
    (in step 1) to adapt training environments in the next cycle. Moreover, importantly,
    to obtain a more calibrated estimation of agent performance, we calculate the
    average and variance of the task-specific scores by testing agents with multiple
    random seeds (*i.e*., 12). Note this performance measuring takes minimal time
    and computation compared to the actual training steps (*e.g*., 30x faster in our
    experiments) as it does not involve backpropagation.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '需要注意的是，EnvGen的目标是提升RL代理在原始环境中的表现，而不仅仅是在LLM环境中的表现。为了帮助RL代理有效适应原始环境，并向LLM提供当前代理的表现作为反馈，我们在原始环境中训练代理并测量其表现，如[图2](#S2.F2
    "In 2 EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")第3步所示。首先，为了减轻对LLM环境的过拟合，我们在原始环境中训练代理$T^{\text{Orig-Env}}$效果良好；详情请参见[表8](#S4.T8
    "In Number of LLM environments. ‣ 4.5 Additional Analysis and Ablation Studies
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")。接下来，为了找出RL代理需要改进的技能，我们在原始环境中测试代理，而不进行任何参数更新。具体而言，我们测量每个环境任务的个体成功率（*例如*，Crafter成就）。代理的表现总结（在第4步）并作为反馈提供给LLM（在第1步），以便在下一个周期中调整训练环境。此外，重要的是，为了获得更准确的代理表现估计，我们通过测试多个随机种子的代理来计算任务特定分数的平均值和方差（*即*，12）。请注意，这种性能测量所需的时间和计算量相比于实际训练步骤要少得多（*例如*，在我们的实验中快30倍），因为它不涉及反向传播。'
- en: 'Step 4: Send feedback to LLM to adapt environments (to focus on weak skills).'
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：向LLM发送反馈以调整环境（以关注弱技能）。
- en: We provide the LLM with the agent’s performance from the original environment
    (measured in step 3), as feedback for updating LLM environments. Concretely, we
    list the agent’s average task-specific success rate in percentages along with
    one standard deviation (*e.g*., “$\dots$ times.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向LLM提供来自原始环境的代理性能（在第3步测量），作为更新LLM环境的反馈。具体而言，我们列出代理在每个任务中的平均成功率（以百分比表示）以及一个标准偏差（*例如*，“$\dots$
    次”）。
- en: 3 Experimental Setup
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 'In the following subsections, we present the benchmarks in which we evaluate
    EnvGen framework on ([Sec. 3.1](#S3.SS1 "3.1 Evaluated Benchmarks and Training
    Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents")) and the agent architectures that we use
    for experiments ([Sec. 3.2](#S3.SS2 "3.2 Agent Architectures ‣ 3 Experimental
    Setup ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们展示了对EnvGen框架进行评估的基准测试（见[第3.1节](#S3.SS1 "3.1 评估的基准和训练细节 ‣ 3 实验设置 ‣
    EnvGen：通过LLM生成和调整环境以训练具身体代理")）以及我们用于实验的代理架构（见[第3.2节](#S3.SS2 "3.2 代理架构 ‣ 3 实验设置
    ‣ EnvGen：通过LLM生成和调整环境以训练具身体代理")）。
- en: 3.1 Evaluated Benchmarks and Training Details
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 评估的基准和训练细节
- en: '![Refer to caption](img/9fc56bda8bf921b9ce16aae9ce097600.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9fc56bda8bf921b9ce16aae9ce097600.png)'
- en: 'Figure 3: (a): Crafter gameplay screenshot. An agent explores a 2D world and
    completes 22 achievements. (b): Crafter achievement hierarchy. Some achievements
    can be completed right away; others require previous achievements to be unlocked
    first (*i.e*., in a hierarchical order following the arrows).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：（a）：工匠游戏截图。一个代理在2D世界中探索并完成22个成就。（b）：工匠成就层级。某些成就可以立即完成；其他成就需要先解锁之前的成就（*即*，按照箭头的层级顺序）。
- en: Crafter.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工匠。
- en: 'Crafter (Hafner, [2022](#bib.bib19)) is an open-world 2D survival game focused
    on evaluating a broad range of agent capabilities (see [Fig. 3](#S3.F3 "In 3.1
    Evaluated Benchmarks and Training Details ‣ 3 Experimental Setup ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents")). Crafter features
    22 achievements that an agent can unlock during an episode of play. Some achievements
    can be unlocked in a few steps (*e.g*., collect wood, collect sapling, *etc*.),
    but other achievements, such as make iron pickaxe or collect diamond, require
    many training/exploration steps and several prerequisite achievements to be unlocked
    (see [Fig. 3](#S3.F3 "In 3.1 Evaluated Benchmarks and Training Details ‣ 3 Experimental
    Setup ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents") b). For example, to make a stone pickaxe, an agent must first collect
    enough wood to make a table and a wooden pickaxe, then go collect stone and return
    to the table (or collect more wood to make a new one) and then construct the stone
    pickaxe. As an agent progresses, these prerequisite achievements become increasingly
    compounded (*e.g*., an iron pickaxe requires a table + a stone pickaxe + a furnace
    + some coal + some wood), making it incredibly difficult to complete all achievements.
    The score for Crafter is computed as the geometric mean of individual success
    rates of each achievement for each episode it is completed within 1M training
    steps: $S=exp(\frac{1}{22}\sum^{22}_{i=1}ln(1+s_{i}))-1$th achievement across
    all episodes that occurred during training.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 工匠（Hafner，[2022](#bib.bib19)）是一个开放世界的2D生存游戏，专注于评估广泛的代理能力（见[图3](#S3.F3 "在3.1
    评估的基准和训练细节 ‣ 3 实验设置 ‣ EnvGen：通过LLM生成和调整环境以训练具身体代理")）。工匠有22个成就，代理可以在游戏过程中解锁。某些成就可以通过几个步骤解锁（*例如*，收集木材、收集幼苗，*等等*），但其他成就，如制作铁镐或收集钻石，则需要许多训练/探索步骤和几个前提成就才能解锁（见[图3](#S3.F3
    "在3.1 评估的基准和训练细节 ‣ 3 实验设置 ‣ EnvGen：通过LLM生成和调整环境以训练具身体代理") b）。例如，要制作石镐，代理必须首先收集足够的木材制作一个桌子和木镐，然后去收集石头并返回桌子（或收集更多木材制作一个新的桌子），然后构造石镐。随着代理的进展，这些前提成就变得越来越复杂（*例如*，铁镐需要一个桌子
    + 一个石镐 + 一个熔炉 + 一些煤 + 一些木材），使得完成所有成就变得极其困难。工匠的得分是每集完成的各成就成功率的几何平均数，在1M训练步骤内计算：$S=exp(\frac{1}{22}\sum^{22}_{i=1}ln(1+s_{i}))-1$。
- en: For EnvGen setup, we use $N^{\text{Cycle}}=4$ 10 different random seeds).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于EnvGen设置，我们使用$N^{\text{Cycle}}=4$ 10个不同的随机种子。
- en: '![Refer to caption](img/422f6d30cc9639b4094ea1db9e3bcb52.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/422f6d30cc9639b4094ea1db9e3bcb52.png)'
- en: 'Figure 4: (a): Heist gameplay screenshot. An agent aims to steal a gem (colored
    yellow), navigating a maze and colored opening locks. (b): Heist achievement hierarchy.
    The agent can only reach the gem after successively unlocking all locks in order.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：（a）：抢劫游戏截图。一个代理的目标是偷取一颗宝石（黄色），在迷宫中导航并打开上色的锁。（b）：抢劫成就层级。代理只能在按顺序成功解锁所有锁后才能到达宝石。
- en: Heist.
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 抢劫。
- en: 'Heist is part of the OpenAI Procgen (Cobbe et al., [2020](#bib.bib11)) benchmark.
    In this environment, agents must successfully ‘steal’ the gem after navigating
    a maze and opening all locks (see [Fig. 4](#S3.F4 "In Crafter. ‣ 3.1 Evaluated
    Benchmarks and Training Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents")). The gem is behind
    three layers of color-coded locks, each requiring that the previous lock be unlocked
    first (*e.g*., to unlock the green lock, the blue lock must first be unlocked).
    Following Moon et al. ([2023](#bib.bib36)), the final score is calculated as the
    average success of the agent in stealing the gem in 100 test episodes in 10 different
    seeds (*i.e*., 1,000 runs in total). For agent training, we use a total of 5M
    steps in the LLM-generated environments (*i.e*., 5M Heist${}^{\text{EnvGen}}$
    training cycle.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'Heist是OpenAI Procgen（Cobbe等人，[2020](#bib.bib11)）基准的一部分。在这个环境中，代理必须在导航迷宫并打开所有锁之后成功“偷走”宝石（见[图4](#S3.F4
    "在Crafter. ‣ 3.1 评估基准和训练细节 ‣ 3 实验设置 ‣ EnvGen: 通过LLMs生成和调整环境以训练具身代理")）。宝石被三层颜色编码的锁阻挡，每一层锁都要求先解开前一层锁（*例如*，要解开绿色锁，必须先解开蓝色锁）。按照Moon等人（[2023](#bib.bib36)）的方法，最终得分是通过计算代理在10个不同种子中100个测试回合中成功偷取宝石的平均成功率来得出的（*即*，总共1,000次运行）。对于代理训练，我们在LLM生成的环境中使用了总共5M步（*即*，5M
    Heist${}^{\text{EnvGen}}$训练周期）。'
- en: 3.2 Agent Architectures
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 代理架构
- en: Our base RL agent.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的基本RL代理。
- en: 'For both Crafter and Heist, we test the EnvGen framework with a simple (CNN
    + linear layer) and lightweight ($<$5M) agent used in Moon et al. ([2023](#bib.bib36)),
    which is slightly modified from the agent architecture used in IMPALA (Espeholt
    et al., [2018](#bib.bib15)). Following Moon et al. ([2023](#bib.bib36)), we train
    the agent with a PPO (Schulman et al., [2017](#bib.bib41)) objective. At every
    step, the agent takes an RGB image (surroundings for Crafter, entire maze for
    Heist) as input and outputs the value estimates and policy (action probability).
    See [Fig. 3](#S3.F3 "In 3.1 Evaluated Benchmarks and Training Details ‣ 3 Experimental
    Setup ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents") (a) and [Fig. 4](#S3.F4 "In Crafter. ‣ 3.1 Evaluated Benchmarks and Training
    Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents") (a) for example visual inputs for agents.
    We provide additional model implementation details in the appendix.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '对于Crafter和Heist，我们使用Moon等人（[2023](#bib.bib36)）中使用的一个简单（CNN + 线性层）且轻量级（$<$5M）的代理来测试EnvGen框架，该代理略微修改自IMPALA（Espeholt等人，[2018](#bib.bib15)）中使用的代理架构。按照Moon等人（[2023](#bib.bib36)）的方法，我们使用PPO（Schulman等人，[2017](#bib.bib41)）目标来训练代理。在每一步，代理接收一个RGB图像（Crafter的环境或Heist的整个迷宫）作为输入，并输出价值估计和策略（动作概率）。请参见[图3](#S3.F3
    "在3.1 评估基准和训练细节 ‣ 3 实验设置 ‣ EnvGen: 通过LLMs生成和调整环境以训练具身代理")（a）和[图4](#S3.F4 "在Crafter.
    ‣ 3.1 评估基准和训练细节 ‣ 3 实验设置 ‣ EnvGen: 通过LLMs生成和调整环境以训练具身代理")（a）了解代理的示例视觉输入。附录中提供了更多模型实现细节。'
- en: Baseline methods.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线方法。
- en: 'For Crafter, we compare our method to two groups of recent baselines – (1)
    methods that use frequent (*i.e*., more than thousands of) LLM calls during training
    or inference: SPRING (Wu et al., [2023](#bib.bib59)) (based on GPT-4 (OpenAI,
    [2023a](#bib.bib38))) and ELLM (Du et al., [2023](#bib.bib13)) (based on Codex (Chen
    et al., [2021](#bib.bib9))) and (2) methods that do not use an LLM during training
    or inference: DreamerV3 (Hafner et al., [2023](#bib.bib22)), MuZero + SPR (Walker
    et al., [2023](#bib.bib50)), LSTM-SPCNN (Stanić et al., [2023](#bib.bib44)), PPO (Schulman
    et al., [2017](#bib.bib41)), and Achievement Distillation (AD) (Moon et al., [2023](#bib.bib36)).
    For Heist, we compare against the PPO agent. For the PPO and AD agents, we follow
    the implementation of Moon et al. ([2023](#bib.bib36)). See appendix for the PPO/AD
    agent details.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Crafter，我们将我们的方法与两组最近的基线进行比较：（1）在训练或推理过程中使用频繁（*即*，超过千次）的LLM调用的方法：SPRING（Wu等人，[2023](#bib.bib59)）（基于GPT-4（OpenAI，[2023a](#bib.bib38)））和ELLM（Du等人，[2023](#bib.bib13)）（基于Codex（Chen等人，[2021](#bib.bib9)）），以及（2）在训练或推理过程中不使用LLM的方法：DreamerV3（Hafner等人，[2023](#bib.bib22)），MuZero
    + SPR（Walker等人，[2023](#bib.bib50)），LSTM-SPCNN（Stanić等人，[2023](#bib.bib44)），PPO（Schulman等人，[2017](#bib.bib41)）和成就蒸馏（AD）（Moon等人，[2023](#bib.bib36)）。对于Heist，我们与PPO代理进行比较。对于PPO和AD代理，我们遵循Moon等人（[2023](#bib.bib36)）的实现。有关PPO/AD代理的详细信息，请参见附录。
- en: 4 Results and Analysis
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与分析
- en: 'We demonstrate the usefulness of the EnvGen method with comprehensive experiments
    and analysis. We first compare RL agents trained with EnvGen to different baseline
    methods on Crafter, an open-world game with 22 hierarchical achievements ([Sec. 4.1](#S4.SS1
    "4.1 Comparison with State-of-the-art Methods on Crafter Environment ‣ 4 Results
    and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents")). Next, we present a detailed analysis of the improvements that
    training with EnvGen environments can give RL agents on long-horizon tasks ([Sec. 4.2](#S4.SS2
    "4.2 Detailed Achievement Analysis on Crafter Environment ‣ 4 Results and Analysis
    ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")). Then, we analyze how the LLM-based environment adaptation can help
    an RL agent progressively improve the skills that the agent is weak at ([Sec. 4.3](#S4.SS3
    "4.3 Adaptation of Training Environments Helps the Agent Improve Weaker Skills
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")). Moreover, we also compare an RL agent’s performance
    on Heist (a maze navigation game), with and without EnvGen environments ([Sec. 4.4](#S4.SS4
    "4.4 Evaluation on Heist Environment ‣ 4 Results and Analysis ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents")). Lastly, we
    present various ablation studies on EnvGen design choices ([Sec. 4.5](#S4.SS5
    "4.5 Additional Analysis and Ablation Studies ‣ 4 Results and Analysis ‣ EnvGen:
    Generating and Adapting Environments via LLMs for Training Embodied Agents")).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过全面的实验和分析展示了 EnvGen 方法的实用性。我们首先将使用 EnvGen 训练的 RL 代理与在 Crafter（一个拥有 22 个分层成就的开放世界游戏）上不同基准方法进行比较（[第
    4.1 节](#S4.SS1 "4.1 Comparison with State-of-the-art Methods on Crafter Environment
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")）。接下来，我们详细分析了使用 EnvGen 环境训练对 RL 代理在长期任务中带来的改进（[第
    4.2 节](#S4.SS2 "4.2 Detailed Achievement Analysis on Crafter Environment ‣ 4 Results
    and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents")）。然后，我们分析了基于 LLM 的环境适应如何帮助 RL 代理逐步提高代理在某些方面的不足（[第 4.3 节](#S4.SS3
    "4.3 Adaptation of Training Environments Helps the Agent Improve Weaker Skills
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")）。此外，我们还比较了 RL 代理在 Heist（一个迷宫导航游戏）中的表现，包含和不包含 EnvGen
    环境（[第 4.4 节](#S4.SS4 "4.4 Evaluation on Heist Environment ‣ 4 Results and Analysis
    ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")）。最后，我们展示了对 EnvGen 设计选择的各种消融研究（[第 4.5 节](#S4.SS5 "4.5 Additional Analysis
    and Ablation Studies ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents")）。'
- en: '| Models | Description | # LLM calls | # Agent Params | Score (%) | Reward
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 描述 | # LLM 调用次数 | # 代理参数 | 得分（%） | 奖励 |'
- en: '| Human^∗ |  |  |  | 50.5 $\pm$ 2.3 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能^∗ |  |  |  | 50.5 $\pm$ 2.3 |'
- en: '| Random^∗ |  |  |  | 1.6 $\pm$ 1.3 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 随机^∗ |  |  |  | 1.6 $\pm$ 1.3 |'
- en: '| ELLM* (Du et al., [2023](#bib.bib13)) | 5M step PT in Crafter w/ Codex reward
    | 5M | 62M | - | 6.0 $\pm$ 0.4 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| ELLM*（Du 等， [2023](#bib.bib13)） | 5M 步 PT 使用 Crafter 和 Codex 奖励 | 5M | 62M
    | - | 6.0 $\pm$ 0.4 |'
- en: '| LSTM-SPCNN^∗ (Stanić et al., [2023](#bib.bib44)) |  |  | 135M | 11.7 $\pm$
    0.2 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| LSTM-SPCNN^∗（Stanić 等， [2023](#bib.bib44)） |  |  | 135M | 11.7 $\pm$ 0.2
    |'
- en: '| DreamerV3^∗ (Hafner et al., [2023](#bib.bib22)) |  |  | 201M | 14.8 $\pm$
    0.5 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| DreamerV3^∗（Hafner 等， [2023](#bib.bib22)） |  |  | 201M | 14.8 $\pm$ 0.5 |'
- en: '| MuZero + SPR^∗ (Walker et al., [2023](#bib.bib50)) | 150M step PT in Crafter
    w/ RND reward |  | 54M | 16.4 $\pm$ 0.4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| MuZero + SPR^∗（Walker 等， [2023](#bib.bib50)） | 150M 步 PT 使用 Crafter 和 RND
    奖励 |  | 54M | 16.4 $\pm$ 0.4 |'
- en: '| SPRING* (Wu et al., [2023](#bib.bib59)) | 9 queries to call GPT-4 per step
    | 2.7K^† | Unknown | 27.3 $\pm$ 0.7 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SPRING*（Wu 等， [2023](#bib.bib59)） | 每步调用 GPT-4 9 次 | 2.7K^† | 未知 | 27.3 $\pm$
    0.7 |'
- en: '| PPO (Moon et al., [2023](#bib.bib36)) |  |  | 4M | 15.5 $\pm$ 0.6 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| PPO（Moon 等， [2023](#bib.bib36)） |  |  | 4M | 15.5 $\pm$ 0.6 |'
- en: '| PPO (Moon et al., [2023](#bib.bib36)) | 0.96M step PT in Crafter |  | 4M
    | 26.4 $\pm$ 1.0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| PPO（Moon 等， [2023](#bib.bib36)） | 0.96M 步 PT 使用 Crafter |  | 4M | 26.4 $\pm$
    1.0 |'
- en: '| AD* (Moon et al., [2023](#bib.bib36)) |  |  | 9M | 21.8 $\pm$ 0.3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| AD*（Moon 等， [2023](#bib.bib36)） |  |  | 9M | 21.8 $\pm$ 0.3 |'
- en: '| AD (Moon et al., [2023](#bib.bib36)) | 0.96M step PT in Crafter |  | 9M |
    31.8 $\pm$ 1.2 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| AD（Moon 等， [2023](#bib.bib36)） | 0.96M 步 PT 使用 Crafter |  | 9M | 31.8 $\pm$
    1.2 |'
- en: '| PPO + EnvGen (Ours) | 0.96M step PT w/ Crafter${}^{\text{EnvGen}}$ 0.6 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| PPO + EnvGen（我们的方法） | 0.96M 步 PT 使用 Crafter${}^{\text{EnvGen}}$ 0.6 |'
- en: '| AD + EnvGen (Ours) | 0.96M step PT w/ Crafter${}^{\text{EnvGen}}$ 0.8 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| AD + EnvGen（我们的方法） | 0.96M 步 PT 使用 Crafter${}^{\text{EnvGen}}$ 0.8 |'
- en: 'Table 1: Comparison of different agents in the Crafter (Hafner, [2022](#bib.bib19))
    environment. Following previous works, we report the geometric mean of success
    rates across its 22 achievements and rewards for 1M Crafter steps. We experiment
    with EnvGen on two models, PPO and Achievement Distillation. *: scores from the
    Crafter Scoreboard (Hafner, [2022](#bib.bib19)) and Moon et al. ([2023](#bib.bib36)).
    $\dagger$: one standard deviation.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 1：Crafter（Hafner，[2022](#bib.bib19)）环境中不同代理的比较。遵循之前的研究，我们报告了在 1M Crafter
    步骤中其 22 项成就和奖励的成功率的几何平均值。我们在两个模型 PPO 和 Achievement Distillation 上实验了 EnvGen。*:
    来自 Crafter Scoreboard（Hafner，[2022](#bib.bib19)）和 Moon 等（[2023](#bib.bib36)）的分数。$\dagger$:
    一个标准差。'
- en: 4.1 Comparison with State-of-the-art Methods on Crafter Environment
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 在 Crafter 环境中与最先进方法的比较
- en: Small RL agent trained with EnvGen outperforms state-of-the-art baselines.
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 EnvGen 训练的小型 RL 代理超越了最先进的基线。
- en: 'On the Crafter environment (described in [Sec. 3.1](#S3.SS1 "3.1 Evaluated
    Benchmarks and Training Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents")), we compare a small
    PPO agent trained in Crafter${}^{\text{EnvGen}}$ (*i.e*., Crafter environments
    generated with EnvGen) to state-of-the-art baseline methods. As shown in [Table 1](#S4.T1
    "In 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via
    LLMs for Training Embodied Agents"), we find that a small (4M parameters) PPO
    agent with EnvGen achieves an average score of 32.2% and significantly outperforms
    the baselines (and also in terms of the average reward). Note that some baseline
    agents have many more parameters or pretraining steps such as SPRING (27.3%) that
    directly employs GPT-4 as agent, and MuZero + SPR (16.4%) that uses 150M pretraining
    steps. Also, note that our method only uses orders of magnitude fewer LLM calls
    (only 4) than works like SPRING (2.7K on average) and ELLM (5M). Due to the large
    number of LLM calls, SPRING costs around $270 USD to run an agent in a single
    episode, whereas EnvGen only costs a couple of cents total for any number of episodes
    (see appendix for cost details). Moreover, we find that EnvGen can help train
    another small RL agent – Achievement Distillation (AD) (Moon et al., [2023](#bib.bib36))
    – to achieve an even higher score (35.3%).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Crafter 环境中（见[第 3.1 节](#S3.SS1 "3.1 评估基准和训练细节 ‣ 3 实验设置 ‣ EnvGen: 通过 LLM 生成和调整环境以训练体现智能体")），我们将使用
    EnvGen 生成的 Crafter${}^{\text{EnvGen}}$（*即*，使用 EnvGen 生成的 Crafter 环境）训练的小型 PPO
    代理与最先进的基线方法进行了比较。如[表格 1](#S4.T1 "在 4 结果和分析 ‣ EnvGen: 通过 LLM 生成和调整环境以训练体现智能体")所示，我们发现，使用
    EnvGen 的小型（4M 参数）PPO 代理平均得分为 32.2%，显著优于基线（在平均奖励方面也是如此）。注意，一些基线代理有更多的参数或预训练步骤，例如直接使用
    GPT-4 作为代理的 SPRING（27.3%），以及使用 150M 预训练步骤的 MuZero + SPR（16.4%）。另外，我们的方法只使用了数量级上少得多的
    LLM 调用（仅 4 次），而像 SPRING（平均 2.7K 次）和 ELLM（5M 次）这样的工作需要大量的 LLM 调用。由于大量的 LLM 调用，SPRING
    在单次实验中运行一个代理的费用约为 270 美元，而 EnvGen 对任何数量的实验的总费用仅为几美分（详见附录中的费用细节）。此外，我们发现 EnvGen
    可以帮助训练另一个小型 RL 代理——Achievement Distillation（AD）（Moon 等，[2023](#bib.bib36)）——以获得更高的得分（35.3%）。'
- en: 4.2 Detailed Achievement Analysis on Crafter Environment
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 Crafter 环境中的详细成就分析
- en: 'Next, we analyze where EnvGen improves the overall score by checking individual
    achievement success rates in detail. For this, we compare the same PPO agent architecture
    trained with different setups: (1) an agent trained on Crafter for 1.96M steps
    and (2) an agent trained on Crafter${}^{\text{EnvGen}}$ 4 training cycles, see
    [Sec. 2.2](#S2.SS2 "2.2 EnvGen Method Details ‣ 2 EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents")) and then trained on Crafter
    for 1M steps. We measure the success rate ([Fig. 5](#S4.F5 "In 4.2 Detailed Achievement
    Analysis on Crafter Environment ‣ 4 Results and Analysis ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents")) of each achievement
    and unlocking speed ([Fig. 6](#S4.F6 "In EnvGen helps RL agents to tackle challenging
    long-horizon achievements. ‣ 4.2 Detailed Achievement Analysis on Crafter Environment
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")) of iron tools in the last 1M training steps, and
    discuss the results below.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过详细检查各个成就的成功率来分析EnvGen在整体得分上的改进。为此，我们比较了使用不同设置训练的相同PPO代理架构：（1）在Crafter上训练了1.96M步的代理和（2）在Crafter${}^{\text{EnvGen}}$上训练了4个训练周期的代理，然后在Crafter上训练了1M步。我们测量了每个成就的成功率（见[图5](#S4.F5
    "在Crafter环境中的详细成就分析 ‣ 结果与分析 ‣ EnvGen：通过LLMs生成和调整环境以训练体现智能体")）和在最后1M训练步骤中铁工具的解锁速度（见[图6](#S4.F6
    "EnvGen帮助RL智能体应对具有挑战性的长期成就 ‣ 在Crafter环境中的详细成就分析 ‣ 结果与分析 ‣ EnvGen：通过LLMs生成和调整环境以训练体现智能体")），并在下文讨论结果。
- en: '![Refer to caption](img/ae140f7aaa8203de688990203b80bad2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/ae140f7aaa8203de688990203b80bad2.png)'
- en: 'Figure 5: Success rates for all the Crafter achievements of two PPO agents (Moon
    et al., [2023](#bib.bib36)) – (1) Baseline: trained in Crafter for 1.96M steps,
    and (2) Ours: trained in 0.96M steps in Crafter${}^{\text{EnvGen}}$ and 1M in
    Crafter.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：两个PPO代理的Crafter所有成就的成功率（Moon等，[2023](#bib.bib36)）– （1）基线：在Crafter上训练了1.96M步，以及（2）我们的：在Crafter${}^{\text{EnvGen}}$上训练了0.96M步，再在Crafter上训练了1M步。
- en: EnvGen helps RL agents to tackle challenging long-horizon achievements.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EnvGen帮助RL智能体应对具有挑战性的长期成就。
- en: '[Fig. 5](#S4.F5 "In 4.2 Detailed Achievement Analysis on Crafter Environment
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents") shows that training in Crafter${}^{\text{EnvGen}}$
    can also improve the success rate of another challenging long-horizon achievement
    – ‘collect diamond’.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5](#S4.F5 "在Crafter环境中的详细成就分析 ‣ 结果与分析 ‣ EnvGen：通过LLMs生成和调整环境以训练体现智能体")显示，在Crafter${}^{\text{EnvGen}}$上训练也可以提高另一个具有挑战性的长期成就——‘收集钻石’的成功率。'
- en: '![Refer to caption](img/8ecbf5dd04ce892d4a8a8ad8d2db769f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/8ecbf5dd04ce892d4a8a8ad8d2db769f.png)'
- en: 'Figure 6: Unlock times (the first moment when the agent completed an achievement)
    for three long-horizon achievements (‘make stone pickaxe’, ‘make iron pickaxe’,
    and ‘make iron sword’) of two PPO agents (Moon et al., [2023](#bib.bib36)) – (1)
    Baseline: trained in Crafter for 1.96M steps, and (2) Ours: trained for 0.96M
    steps in Crafter${}^{\text{EnvGen}}$ environments unlocks the achievements much
    quicker than the baseline agent that was only trained in the Crafter environment.
    As shown in [Fig. 3](#S3.F3 "In 3.1 Evaluated Benchmarks and Training Details
    ‣ 3 Experimental Setup ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents"), these achievements have many prerequisites and
    require long-term planning.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：两个PPO代理在三个长期成就（‘制作石镐’，‘制作铁镐’，和‘制作铁剑’）上的解锁时间（代理首次完成成就的时刻）（Moon等，[2023](#bib.bib36)）–
    （1）基线：在Crafter上训练了1.96M步，以及（2）我们的：在Crafter${}^{\text{EnvGen}}$环境中训练了0.96M步，而在Crafter环境中训练了1M步。结果显示，相较于仅在Crafter环境中训练的基线代理，训练在Crafter${}^{\text{EnvGen}}$环境中的代理能够更快解锁成就。如[图3](#S3.F3
    "在评估的基准和训练细节 ‣ 实验设置 ‣ EnvGen：通过LLMs生成和调整环境以训练体现智能体")所示，这些成就有许多前提条件，需要长期规划。
- en: '![Refer to caption](img/779a1054de7377bf1c31685d1a2f0ef7.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/779a1054de7377bf1c31685d1a2f0ef7.png)'
- en: 'Figure 7: Adaptation of training environments based on agent performance over
    EnvGen cycles. At the end of each cycle, the RL agent’s performance is given to
    the LLM as feedback (*e.g*., ‘Collect coal is 2%’). The LLM uses the feedback
    to adaptively generate new environments that can help the agent progressively
    tackle skills it was previously weak at. As the training proceeds, our RL agent
    trained with EnvGen shows more rapid improvements than the baseline agent trained
    only in Crafter, by adaptively focusing the learning on previously weaker skills
    (*i.e*., ‘collect coal’ and ‘make stone pickaxe’).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：基于代理表现的训练环境适应性调整，经过 EnvGen 循环。在每个周期结束时，将 RL 代理的表现反馈给 LLM（*例如*，“收集煤矿的成功率为
    2%”）。LLM 使用反馈自适应生成新的环境，帮助代理逐步应对之前较弱的技能。随着训练的进行，我们使用 EnvGen 训练的 RL 代理比仅在 Crafter
    中训练的基准代理显示出更快的改进，通过自适应地将学习重点放在之前较弱的技能上（*即*，“收集煤矿”和“制作石镐”）。
- en: 4.3 Adaptation of Training Environments Helps the Agent Improve Weaker Skills
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 训练环境的适应性帮助代理提高较弱的技能
- en: '[Fig. 7](#S4.F7 "In EnvGen helps RL agents to tackle challenging long-horizon
    achievements. ‣ 4.2 Detailed Achievement Analysis on Crafter Environment ‣ 4 Results
    and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents") shows how the LLM adaptively generates new training environments
    based on the intermediate performance of our PPO-based RL agent. In the intermediate
    performance plots, we compare the baseline agent trained only in Crafter and our
    RL agent trained in Crafter${}^{\text{EnvGen}}$. In the cycle 2, given the feedback
    that the current RL agent is not good at collecting coal, the LLM could generate
    an environment to help the agent focus on the skill, improving the agent performance
    in ‘collect coal’. Likewise, in the cycle 3, given the feedback that the agent
    is weak at making stone pickaxe, the LLM could generate an environment to help
    the agent more easily craft the stone pickaxe, helping the agent improving the
    score in ‘make stone pickaxe’. Powered by the adaptive LLM environment generation
    of EnvGen, our agent learns to unlock these two achievements significantly faster
    than the baseline agent.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7](#S4.F7 "在 EnvGen 中帮助 RL 代理应对具有挑战性的长期成就。 ‣ 4.2 Crafter 环境的详细成就分析 ‣ 4 结果与分析
    ‣ EnvGen：通过 LLM 为训练具身代理生成和调整环境") 展示了 LLM 如何根据我们基于 PPO 的 RL 代理的中期表现自适应生成新的训练环境。在中期表现图中，我们比较了仅在
    Crafter 中训练的基准代理和在 Crafter${}^{\text{EnvGen}}$ 中训练的 RL 代理。在周期 2 中，鉴于当前 RL 代理在收集煤矿方面表现不佳，LLM
    可以生成一个环境来帮助代理专注于该技能，提高代理在“收集煤矿”方面的表现。同样，在周期 3 中，鉴于代理在制作石镐方面较弱，LLM 可以生成一个环境来帮助代理更容易制作石镐，从而帮助代理提高在“制作石镐”方面的分数。得益于
    EnvGen 的自适应 LLM 环境生成，我们的代理比仅在 Crafter 中训练的基准代理显著更快地解锁了这两个成就。'
- en: '| Model | # Training Steps in Heist${}^{\text{EnvGen}}$ | # Training Steps
    in Heist | Score (%) | Reward |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Heist${}^{\text{EnvGen}}$ 中的训练步数 | Heist 中的训练步数 | 分数 (%) | 奖励 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPO | - | 25M | 25.9 $\pm$ 1.8 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| PPO | - | 25M | 25.9 $\pm$ 1.8 |'
- en: '| PPO + EnvGen (Ours) | 5M | 20M | 37.7 $\pm$ 0.9 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| PPO + EnvGen (我们) | 5M | 20M | 37.7 $\pm$ 0.9 |'
- en: 'Table 2: Evaluation results on Heist. Scores are computed as the average success
    rate over 100 test episodes over 10 different seeds.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 Heist 上的评估结果。分数是基于 10 个不同种子的 100 个测试回合的成功率平均值。
- en: 4.4 Evaluation on Heist Environment
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 在 Heist 环境中的评估
- en: EnvGen can generalize to Heist.
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EnvGen 可以推广到 Heist。
- en: 'We also evaluate the effectiveness of EnvGen framework with another game environment
    – Heist, a maze navigation game described in [Sec. 3.1](#S3.SS1 "3.1 Evaluated
    Benchmarks and Training Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents"). We compare the
    PPO-based agent trained with and without EnvGen (*i.e*., Heist${}^{\text{EnvGen}}$
    7.5%).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还用另一种游戏环境——Heist，进行了 EnvGen 框架的有效性评估。Heist 是一种迷宫导航游戏，描述见 [Sec. 3.1](#S3.SS1
    "3.1 评估的基准和训练细节 ‣ 3 实验设置 ‣ EnvGen：通过 LLM 为训练具身代理生成和调整环境")。我们比较了使用和不使用 EnvGen 的
    PPO 基础代理（*即*，Heist${}^{\text{EnvGen}}$ 7.5%）。
- en: 4.5 Additional Analysis and Ablation Studies
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 额外分析和消融研究
- en: 'In the following, we show comprehensive ablation studies of EnvGen method:
    EnvGen *vs*. longer training in the original environment, dynamically updating
    LLM environments (*i.e*., using adaptive environments) *vs*. using a fixed LLM
    environment, different LLMs for generating environments, frequency of environment
    updates, the number of LLM environments, and the ratio of training steps in the
    LLM environments to the original environment. Unless otherwise noted, we use the
    PPO-based agent (Moon et al., [2023](#bib.bib36)) (described in [Sec. 3.2](#S3.SS2
    "3.2 Agent Architectures ‣ 3 Experimental Setup ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents")) on the Crafter (Hafner,
    [2022](#bib.bib19)) benchmark (described in [Sec. 3.1](#S3.SS1 "3.1 Evaluated
    Benchmarks and Training Details ‣ 3 Experimental Setup ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents")) with 0.96M steps
    in Crafter${}^{\text{EnvGen}}$ and average results for 30 runs (10 different seeds,
    3 different initial environments).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '以下，我们展示了 EnvGen 方法的全面消融研究：EnvGen *vs*. 原环境中的较长训练，动态更新 LLM 环境（*即*，使用自适应环境） *vs*.
    使用固定 LLM 环境，生成环境的不同 LLM，环境更新的频率，LLM 环境的数量，以及 LLM 环境与原环境的训练步骤比例。除非另有说明，我们使用基于 PPO
    的代理（Moon 等，[2023](#bib.bib36)）（描述见 [Sec. 3.2](#S3.SS2 "3.2 Agent Architectures
    ‣ 3 Experimental Setup ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")）在 Crafter（Hafner，[2022](#bib.bib19)）基准上（描述见 [Sec.
    3.1](#S3.SS1 "3.1 Evaluated Benchmarks and Training Details ‣ 3 Experimental Setup
    ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")）进行训练，使用 0.96M 步骤在 Crafter${}^{\text{EnvGen}}$ 中，并对 30 次运行的平均结果进行评估（10
    个不同种子，3 个不同初始环境）。'
- en: '| # Training Steps in Crafter${}^{\text{EnvGen}}$ | # Training Steps in Crafter
    | Score (%) | Reward |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| # 在 Crafter${}^{\text{EnvGen}}$ 中的训练步骤 | # 在 Crafter 中的训练步骤 | 分数 (%) | 奖励
    |'
- en: '| (Total 1.24M steps) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| （总计 1.24M 步骤） |'
- en: '| - | 1.24M | 21.1 $\pm$ 0.9 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| - | 1.24M | 21.1 $\pm$ 0.9 |'
- en: '| 0.12M | 1.12M | 22.3 $\pm$ 0.8 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 0.12M | 1.12M | 22.3 $\pm$ 0.8 |'
- en: '| (Total 1.48M steps) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| （总计 1.48M 步骤） |'
- en: '| - | 1.48M | 21.9 $\pm$ 0.9 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| - | 1.48M | 21.9 $\pm$ 0.9 |'
- en: '| 0.24M | 1.24M | 27.9 $\pm$ 0.7 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 0.24M | 1.24M | 27.9 $\pm$ 0.7 |'
- en: '| (Total 1.96M steps) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| （总计 1.96M 步骤） |'
- en: '| - | 1.96M | 26.4 $\pm$ 1.0 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| - | 1.96M | 26.4 $\pm$ 1.0 |'
- en: '| 0.48M | 1.48M | 32.2 $\pm$ 0.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 0.48M | 1.48M | 32.2 $\pm$ 0.6 |'
- en: 'Table 3: RL agents trained in Crafter${}^{\text{EnvGen}}$ environments *vs*.
    agents trained only in the Crafter environment. We calculate the scores based
    on the last 1M training steps in Crafter.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在 Crafter${}^{\text{EnvGen}}$ 环境中训练的 RL 代理 *vs*. 仅在 Crafter 环境中训练的代理。我们根据
    Crafter 中最后 1M 训练步骤的得分进行计算。'
- en: EnvGen *vs*. longer training in the original environment.
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EnvGen *vs*. 原环境中的较长训练。
- en: '[Table 3](#S4.T3 "In 4.5 Additional Analysis and Ablation Studies ‣ 4 Results
    and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents") shows that when given an equivalent # of total training steps,
    the agents trained with Crafter${}^{\text{EnvGen}}$ environments outperform the
    agents only trained with Crafter (*e.g*., 22.3% *vs*. 21.1% for 1.24M total steps).
    Although the agent performances tend to improve with longer training steps in
    both settings, training with EnvGen shows stronger performance gains than only
    training longer in Crafter (*e.g*., 32.2% *vs*. 26.4% for 1.96M total steps).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3](#S4.T3 "在 4.5 附加分析和消融研究 ‣ 4 结果和分析 ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents") 显示，当给定等量的总训练步骤时，使用 Crafter${}^{\text{EnvGen}}$
    环境训练的代理在表现上优于仅在 Crafter 中训练的代理（*例如*，1.24M 总步骤下 22.3% *vs*. 21.1%）。尽管代理的表现倾向于随着训练步骤的增加而提高，但使用
    EnvGen 进行训练显示出比仅在 Crafter 中进行较长训练更强的性能提升（*例如*，1.96M 总步骤下 32.2% *vs*. 26.4%）。'
- en: '| Crafter${}^{\text{EnvGen}}$ environments during training | Score (%) | Reward
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Crafter${}^{\text{EnvGen}}$ 环境中的训练 | 分数 (%) | 奖励 |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Fixed | 29.9 $\pm$ 0.8 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 固定 | 29.9 $\pm$ 0.8 |'
- en: '| Updated based on RL agent performance (default) | 32.2 $\pm$ 0.6 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 基于 RL 代理表现更新（默认） | 32.2 $\pm$ 0.6 |'
- en: 'Table 4: RL agents trained in Crafter${}^{\text{EnvGen}}$ times during training.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: RL 代理在训练期间在 Crafter${}^{\text{EnvGen}}$ 中的训练次数。'
- en: 'Adaptive environments: Fixed. vs. Updated based on RL agent performance.'
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自适应环境：固定的 vs. 根据 RL 代理表现更新的。
- en: '[Table 4](#S4.T4 "In EnvGen vs. longer training in the original environment.
    ‣ 4.5 Additional Analysis and Ablation Studies ‣ 4 Results and Analysis ‣ EnvGen:
    Generating and Adapting Environments via LLMs for Training Embodied Agents") shows
    that using LLM environments that are adaptively updated based on intermediate
    agent performance to improve weaker skills results in overall higher scoring agents
    than just using the initial LLM environments for the whole training (32.2% *vs*.
    29.9%). These results indicate the effectiveness of the agent feedback and environment
    updating (step 4 described in [Sec. 2](#S2 "2 EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents")).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4](#S4.T4 "在EnvGen与原始环境中的更长训练。 ‣ 4.5 额外分析和消融研究 ‣ 4 结果与分析 ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents")显示，使用基于中间代理表现自适应更新的LLM环境来提升较弱技能，结果产生的代理得分总体上高于仅使用初始LLM环境进行整个训练（32.2%
    *vs*. 29.9%）。这些结果表明了代理反馈和环境更新的有效性（第4步描述在[第2节](#S2 "2 EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents")）。'
- en: '| LLM | Score (%) | Reward |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 得分（%） | 奖励 |'
- en: '| --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Deepseek Coder 33B Instruct | 26.3 $\pm$ 0.8 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek Coder 33B Instruct | 26.3 $\pm$ 0.8 |'
- en: '| GPT-3.5-Turbo | 21.5 $\pm$ 1.0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | 21.5 $\pm$ 1.0 |'
- en: '| GPT-4-Turbo (default) | 29.9 $\pm$ 0.8 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (默认) | 29.9 $\pm$ 0.8 |'
- en: 'Table 5: Ablation of employing different LLMs to generate the environments.
    Agents are trained with 0.96M steps in Crafter${}^{\text{EnvGen}}$ and 1M steps
    in the Crafter environment.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：使用不同LLM生成环境的消融研究。代理在Crafter${}^{\text{EnvGen}}$中经过0.96M步训练，在Crafter环境中经过1M步训练。
- en: Different LLMs to generate environments.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成环境的不同LLM。
- en: 'To figure out which LLM can generate more useful training environments, we
    experiment with three different LLMs (GPT-4-Turbo, GPT-3.5-Turbo (OpenAI, [2023b](#bib.bib39)),
    and Deepseek Coder 33B Instruct (Guo et al., [2024](#bib.bib18))) and use $N^{\text{Cycle}}=1$
    (*i.e*., fixed environment). [Table 5](#S4.T5 "In Adaptive environments: Fixed.
    vs. Updated based on RL agent performance. ‣ 4.5 Additional Analysis and Ablation
    Studies ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents") shows that environments generated by GPT-4-Turbo
    outperform that of other LLMs including GPT-3.5-Turbo and Deepseek Coder 33B Instruct.
    We see that GPT-3.5-Turbo performs the worst with only a score of 21.5%, while
    Deepseek 33B Instruct is able to get several points higher (26.3%) and GPT-4-Turbo,
    our default LLM, gets a few extra points (29.9%).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '为了找出哪个LLM能生成更有用的训练环境，我们尝试了三种不同的LLM（GPT-4-Turbo、GPT-3.5-Turbo（OpenAI，[2023b](#bib.bib39)）和Deepseek
    Coder 33B Instruct（Guo et al.，[2024](#bib.bib18)）），并使用$N^{\text{Cycle}}=1$（*即*，固定环境）。[表5](#S4.T5
    "在自适应环境中：固定。与基于RL代理性能的更新。 ‣ 4.5 额外分析和消融研究 ‣ 4 结果与分析 ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents")显示，GPT-4-Turbo生成的环境优于其他LLM，包括GPT-3.5-Turbo和Deepseek
    Coder 33B Instruct。我们发现GPT-3.5-Turbo的表现最差，仅得21.5%的分数，而Deepseek 33B Instruct能得到更高的分数（26.3%），GPT-4-Turbo，即我们的默认LLM，则获得了额外的几分（29.9%）。'
- en: '| Environment Update Frequency | # Training cycles $N^{\text{Cycle}}$ | Score
    (%) | Reward |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 环境更新频率 | # 训练周期 $N^{\text{Cycle}}$ | 得分（%） | 奖励 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Every 0.012M steps | 40 cycles | 30.8 $\pm$ 0.6 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 每0.012M步 | 40个周期 | 30.8 $\pm$ 0.6 |'
- en: '| Every 0.06M steps | 8 cycles | 32.1 $\pm$ 0.8 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 每0.06M步 | 8个周期 | 32.1 $\pm$ 0.8 |'
- en: '| Every 0.12M steps (default) | 4 cycles | 32.2 $\pm$ 0.6 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 每0.12M步（默认） | 4个周期 | 32.2 $\pm$ 0.6 |'
- en: 'Table 6: Different frequencies to give feedback to the LLM and update the environments
    (see [Sec. 2](#S2 "2 EnvGen: Generating and Adapting Environments via LLMs for
    Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents") for details). Agents are trained with 0.96M steps
    in Crafter${}^{\text{EnvGen}}$ and 1M steps in Crafter environment.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '表6：不同频率下向LLM提供反馈并更新环境（详见[第2节](#S2 "2 EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents")）。代理在Crafter${}^{\text{EnvGen}}$中经过0.96M步训练，在Crafter环境中经过1M步训练。'
- en: Frequency of LLM feedback / environment updates.
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM反馈/环境更新的频率。
- en: '[Table 6](#S4.T6 "In Different LLMs to generate environments. ‣ 4.5 Additional
    Analysis and Ablation Studies ‣ 4 Results and Analysis ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents") shows that updating
    the LLM environments at every 0.12M steps results in the best agent performance.
    While increasing the cycles of environment feedback beyond 4 does not improve
    further, we find that updating environments with feedback always helps improve
    the RL agent’s performance compared to training only with the original Crafter
    environment (26.4%) or the fixed LLM environment (29.9%).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[表6](#S4.T6 "在不同LLMs中生成环境。 ‣ 4.5 附加分析和消融研究 ‣ 4 结果与分析 ‣ EnvGen: 通过LLMs生成和调整环境以训练具身智能体")显示，每0.12M步骤更新LLM环境能取得最佳智能体表现。尽管增加超过4个周期的环境反馈不会进一步提高，但我们发现与仅在原始Crafter环境（26.4%）或固定LLM环境（29.9%）中训练相比，更新环境的反馈始终有助于提高RL智能体的表现。'
- en: '| # LLM environments | Score (%) | Reward |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| # LLM环境 | 得分 (%) | 奖励 |'
- en: '| --- | --- | --- |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 30.8 $\pm$ 0.8 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 30.8 $\pm$ 0.8 |'
- en: '| 2 | 29.1 $\pm$ 0.6 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 29.1 $\pm$ 0.6 |'
- en: '| 4 (default) | 32.2 $\pm$ 0.6 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 4（默认） | 32.2 $\pm$ 0.6 |'
- en: '| 8 | 31.0 $\pm$ 0.8 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 31.0 $\pm$ 0.8 |'
- en: 'Table 7: Different number of LLM environments being generated by the LLM per
    cycle. Agents are trained with 0.96M steps in Crafter${}^{\text{EnvGen}}$ and
    1M steps in the real Crafter environment.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表7: 每个周期中LLM生成的LLM环境数量。智能体在Crafter${}^{\text{EnvGen}}$中接受0.96M步骤的训练，在真实Crafter环境中接受1M步骤的训练。'
- en: Number of LLM environments.
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM环境的数量。
- en: '[Table 7](#S4.T7 "In Frequency of LLM feedback / environment updates. ‣ 4.5
    Additional Analysis and Ablation Studies ‣ 4 Results and Analysis ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents") shows that changing
    the number of environments generated by the LLM at each cycle (*i.e*., 1, 2, 4,
    and 8) can slightly affect agent performance. While training with four environments
    produces the highest result, training with environments generated with any of
    the tested configurations improves performance over training only with the original
    Crafter environment (26.4%).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7](#S4.T7 "在LLM反馈/环境更新的频率中。 ‣ 4.5 附加分析和消融研究 ‣ 4 结果与分析 ‣ EnvGen: 通过LLMs生成和调整环境以训练具身智能体")显示，改变每个周期中LLM生成的环境数量（*即*，1、2、4和8）会稍微影响智能体的表现。虽然使用四个环境的训练结果最佳，但使用任何测试配置生成的环境的训练都比仅使用原始Crafter环境（26.4%）的训练效果更好。'
- en: '| Ratio of Training Steps in Crafter${}^{\text{EnvGen}}$ : Crafter | Score
    (%) | Reward |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Crafter${}^{\text{EnvGen}}$中的训练步骤比例 : Crafter | 得分 (%) | 奖励 |'
- en: '| --- | --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 5:1 | 30.3 $\pm$ 0.9 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 5:1 | 30.3 $\pm$ 0.9 |'
- en: '| 2:1 | 30.1 $\pm$ 0.7 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 2:1 | 30.1 $\pm$ 0.7 |'
- en: '| 1:1 (default) | 32.2 $\pm$ 0.6 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 1:1（默认） | 32.2 $\pm$ 0.6 |'
- en: 'Table 8: Different ratios of training steps in LLM-generated environments (Crafter${}^{\text{EnvGen}}$,
    the RL agent gets one training step in Crafter). We keep the total number of training
    steps constant at 1.96M.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '表8: 在LLM生成的环境中训练步骤的不同比例（Crafter${}^{\text{EnvGen}}$，RL智能体在Crafter中获得一次训练步骤）。我们保持训练步骤总数为1.96M不变。'
- en: 'Ratio of training steps: LLM environments *vs*. original environment.'
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练步骤比例：LLM环境 *vs*. 原始环境。
- en: 'As mentioned in [Sec. 2.2](#S2.SS2 "2.2 EnvGen Method Details ‣ 2 EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents"), in EnvGen,
    we train the RL agent in LLM environments (step 2) and then in the original environment
    (step 3) to mitigate the agent from overfitting to the LLM environments. We experiment
    with different ratios of training steps in LLM environments (*i.e*., Crafter${}^{\text{EnvGen}}$,
    the RL agent gets one training step in Crafter). As shown in [Table 8](#S4.T8
    "In Number of LLM environments. ‣ 4.5 Additional Analysis and Ablation Studies
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents"), while different ratios do not result in big differences,
    the default 1:1 ratio provides the highest scores.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '如在[第2.2节](#S2.SS2 "2.2 EnvGen 方法详情 ‣ 2 EnvGen: 通过 LLMs 生成和调整环境以训练具身智能体 ‣ EnvGen:
    通过 LLMs 生成和调整环境以训练具身智能体")中提到，在EnvGen中，我们在LLM环境（第2步）和原始环境（第3步）中训练RL智能体，以减少智能体对LLM环境的过拟合。我们实验了LLM环境中不同的训练步骤比例（*即*，Crafter${}^{\text{EnvGen}}$，RL智能体在Crafter中获得一次训练步骤）。如[表8](#S4.T8
    "LLM环境的数量。 ‣ 4.5 附加分析和消融研究 ‣ 4 结果与分析 ‣ EnvGen: 通过LLMs生成和调整环境以训练具身智能体")所示，尽管不同的比例没有产生显著差异，但默认的1:1比例提供了最高的得分。'
- en: 5 Related Works
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: LLMs as open-world game agents.
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大型语言模型作为开放世界游戏代理。
- en: 'As LLMs have shown rapid progress in various domains (Brown et al., [2020](#bib.bib7);
    OpenAI, [2023a](#bib.bib38); Touvron et al., [2023a](#bib.bib48); [b](#bib.bib49);
    Chowdhery et al., [2023](#bib.bib10); Anil et al., [2023](#bib.bib3); Gemini Team,
    [2023](#bib.bib16)), recent works study using LLMs to create action plans (*i.e*.,
    a list of subgoals or skills to target) for embodied agents in open-world games
    like Minecraft and Crafter (Hafner, [2022](#bib.bib19)). Most of these methods
    require calling LLMs frequently (*e.g*., at every step) for planning the next
    steps (Yuan et al., [2023](#bib.bib61); Wang et al., [2023c](#bib.bib53); Wu et al.,
    [2023](#bib.bib59); Wang et al., [2023a](#bib.bib51); [d](#bib.bib54); Zhao et al.,
    [2023](#bib.bib62)). Other methods, such as Li et al. ([2024](#bib.bib30)); Kwon
    et al. ([2023](#bib.bib28)); Ma et al. ([2023](#bib.bib34)); Du et al. ([2023](#bib.bib13)),
    have used LLMs to create/adjust rewards to train agents. Although these works
    show initial promising results leveraging the world knowledge of LLMs to tackle
    long-horizon tasks, iteratively calling LLMs throughout episodes is prohibitively
    slow and expensive (*e.g*., running a single episode in the Crafter environment
    with SPRING (Wu et al., [2023](#bib.bib59)) costs around $270 USD as they have
    2.7K LLM calls on average). EnvGen framework proposes an alternative: calling
    LLMs only a few times (*e.g*., 4) throughout the learning process to create training
    environments that focus on helping the RL agent progressively improve the skills
    that the agent is weak at.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型在各个领域显示出快速进展（Brown 等人，[2020](#bib.bib7)；OpenAI，[2023a](#bib.bib38)；Touvron
    等人，[2023a](#bib.bib48)；[b](#bib.bib49)；Chowdhery 等人，[2023](#bib.bib10)；Anil 等人，[2023](#bib.bib3)；Gemini
    团队，[2023](#bib.bib16)），近期的研究探索了使用大型语言模型为开放世界游戏中的具身代理（如 Minecraft 和 Crafter）创建行动计划（*即*，一系列子目标或技能）的方法（Hafner，[2022](#bib.bib19)）。这些方法中的大多数需要频繁调用大型语言模型（*例如*，每一步都调用）来规划下一步（Yuan
    等人，[2023](#bib.bib61)；Wang 等人，[2023c](#bib.bib53)；Wu 等人，[2023](#bib.bib59)；Wang
    等人，[2023a](#bib.bib51)；[d](#bib.bib54)；Zhao 等人，[2023](#bib.bib62)）。其他方法，例如 Li
    等人（[2024](#bib.bib30)）；Kwon 等人（[2023](#bib.bib28)）；Ma 等人（[2023](#bib.bib34)）；Du
    等人（[2023](#bib.bib13)），已经使用大型语言模型来创建/调整奖励以训练代理。尽管这些工作显示了利用大型语言模型的世界知识来解决长时间跨度任务的初步有希望的结果，但在整个过程中迭代调用大型语言模型的速度和成本都非常高（*例如*，在
    Crafter 环境中运行一个单独的 SPRING 任务（Wu 等人，[2023](#bib.bib59)）的成本大约为 270 美元，因为他们平均需要 2.7K
    次大型语言模型调用）。EnvGen 框架提出了一种替代方案：在学习过程中只调用大型语言模型几次（*例如*，4 次），以创建专注于帮助强化学习代理逐步提升其薄弱技能的训练环境。
- en: Reward designs in reinforcement learning.
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强化学习中的奖励设计。
- en: Finding good action trajectories is critical in reinforcement learning (RL) (Sutton
    & Barto, [2018](#bib.bib46)). While classic random exploration algorithms such
    as epsilon-greedy (Watkins, [1989](#bib.bib56)) work well in simple settings such
    as multi-armed bandit, it is not the case for hard exploration problems where
    the environment gives very sparse rewards (Weng, [2020](#bib.bib58)). A line of
    work studies how to augment the original (extrinsic) rewards from the environment
    with intrinsic rewards that encourage exploration (Bellemare et al., [2016](#bib.bib6);
    Burda et al., [2018](#bib.bib8)). While such intrinsic rewards can help RL agents
    discover novel states and improve their knowledge about the environment, it often
    requires long pretraining and does not guarantee that the intrinsic reward can
    help the target task. Another recent line of work studies using LLMs to adjust
    reward functions to help RL agents progressively learn certain tasks (Li et al.,
    [2024](#bib.bib30); Kwon et al., [2023](#bib.bib28); Ma et al., [2023](#bib.bib34);
    Du et al., [2023](#bib.bib13)). Instead of designing new rewards, in EnvGen, an
    LLM adaptively generates training environments that can help the RL agent learn
    multiple skills it is weak at with fewer training steps than in the original environment;
    reward design could be complementary to our method.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，寻找好的行动轨迹至关重要（Sutton & Barto，[2018](#bib.bib46)）。虽然经典的随机探索算法，如epsilon-greedy（Watkins，[1989](#bib.bib56)），在简单环境下如多臂老虎机中表现良好，但在环境给予非常稀疏奖励的困难探索问题中却不然（Weng，[2020](#bib.bib58)）。一类研究探讨了如何通过引入鼓励探索的内在奖励来增强来自环境的原始（外在）奖励（Bellemare
    et al., [2016](#bib.bib6)；Burda et al., [2018](#bib.bib8)）。虽然这些内在奖励可以帮助RL代理发现新状态并提高对环境的了解，但通常需要长时间的预训练，并且不能保证内在奖励能帮助目标任务。另一类最新研究探讨了使用LLMs来调整奖励函数，以帮助RL代理逐步学习某些任务（Li
    et al., [2024](#bib.bib30)；Kwon et al., [2023](#bib.bib28)；Ma et al., [2023](#bib.bib34)；Du
    et al., [2023](#bib.bib13)）。在EnvGen中，LLM自适应地生成训练环境，这些环境可以帮助RL代理在比原始环境更少的训练步骤下学习多个其薄弱的技能；奖励设计可以作为我们方法的补充。
- en: Deep learning-based game/simulator content generation.
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于深度学习的游戏/模拟器内容生成。
- en: Procedural content generation (PCG) for games is about the automatic generation
    of levels, landscapes, items, rules, quests, or other types of game contents (Shaker
    et al., [2016](#bib.bib43)). While traditional PCG methods are based on search/solver/rule/grammar-based
    methods, recent works study applying deep learning methods such as GAN (Goodfellow
    et al., [2014](#bib.bib17)) for PCG (Liu et al., [2021](#bib.bib33); Kumaran et al.,
    [2020](#bib.bib26); Schubert et al., [2022](#bib.bib40)). Several works have recently
    explored using LLMs to generate game content such as difficulty levels (Sudhakaran
    et al., [2023](#bib.bib45); Todd et al., [2023](#bib.bib47)) and scenes/environments (Kumaran
    et al., [2023](#bib.bib27); Wang et al., [2023b](#bib.bib52); Afshar & Li, [2024](#bib.bib1)).
    While these works study how to help developers create extra/new game content in
    simulators, generated environments may not be good for teaching agents how to
    play/be better in the original environment. Our work studies using LLMs to adaptively
    generate training environments that teach multiple useful skills to produce better-performing
    embodied RL agents. Beyond game content generation, several recent works investigate
    visually augmenting vision-and-language navigation (VLN) simulators (*e.g*., rendering
    the environments with different styles) using image generation models (Li et al.,
    [2022b](#bib.bib32); Wang et al., [2023e](#bib.bib55); Li & Bansal, [2023](#bib.bib31)).
    Such works could complement our LLM-based environment generation (*e.g*., rendering
    our LLM-generated environments with diverse colors and textures).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏中的过程生成内容（PCG）涉及自动生成关卡、景观、物品、规则、任务或其他类型的游戏内容（Shaker 等，[2016](#bib.bib43)）。虽然传统的
    PCG 方法基于搜索/求解器/规则/语法方法，但近期研究探讨了应用深度学习方法，如 GAN（Goodfellow 等，[2014](#bib.bib17)），来进行
    PCG（Liu 等，[2021](#bib.bib33)；Kumaran 等，[2020](#bib.bib26)；Schubert 等，[2022](#bib.bib40)）。最近一些工作探讨了使用
    LLM 生成游戏内容，如难度等级（Sudhakaran 等，[2023](#bib.bib45)；Todd 等，[2023](#bib.bib47)）和场景/环境（Kumaran
    等，[2023](#bib.bib27)；Wang 等，[2023b](#bib.bib52)；Afshar & Li，[2024](#bib.bib1)）。虽然这些研究探讨了如何帮助开发者在模拟器中创建额外/新的游戏内容，但生成的环境可能不适合教导代理如何在原始环境中表现得更好。我们的工作研究了使用
    LLM 自适应地生成训练环境，以教导多个有用的技能，从而提高表现更好的具身强化学习代理。除了游戏内容生成，最近几项工作还研究了使用图像生成模型（Li 等，[2022b](#bib.bib32)；Wang
    等，[2023e](#bib.bib55)；Li & Bansal，[2023](#bib.bib31)）在视觉与语言导航（VLN）模拟器中进行视觉增强（例如，用不同风格渲染环境）。这些工作可以补充我们基于
    LLM 的环境生成（例如，用多样的颜色和纹理渲染我们生成的环境）。
- en: 6 Conclusion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We propose EnvGen, a novel framework to improve embodied agent performance by
    utilizing the world knowledge of LLMs to adaptively generate customized training
    environments that progressively teach agents different skills more effectively.
    In EnvGen, we give an LLM a prompt describing a game/simulator and ask the LLM
    to generate the configurations to create new environments that can teach different
    skills. Next, we train the agents in the LLM-generated environments and give feedback
    to the LLM by testing the agent performance in the original environments, and
    then ask the LLM to update the environments to teach agents skills they are weaker
    at. We experiment with two different games, Crafter and Heist, and find that our
    EnvGen framework effectively can increase agent performance. We also demonstrate
    that training in LLM-generated environments can be more effective than just training
    longer in the original environments when learning long-horizon tasks. Moreover,
    we show that a lightweight model ($<$ 5M parameters) trained with LLM-generated
    environments can even outperform an LLM agent, with significantly fewer LLM calls.
    We also show comprehensive analyses of our results and ablation studies validating
    the design choices of EnvGen framework. We hope our work can guide future works
    in leveraging LLMs for training embodied agents.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 EnvGen，这是一个新颖的框架，通过利用 LLM 的世界知识自适应地生成定制化的训练环境，以更有效地逐步教导代理不同技能，从而提高具身代理的表现。在
    EnvGen 中，我们给 LLM 提供一个描述游戏/模拟器的提示，并要求 LLM 生成配置以创建新的环境，这些环境可以教导不同的技能。接下来，我们在 LLM
    生成的环境中训练代理，并通过在原始环境中测试代理的表现来反馈给 LLM，然后要求 LLM 更新环境，以教导代理它们较弱的技能。我们在两个不同的游戏，Crafter
    和 Heist 中进行了实验，发现我们的 EnvGen 框架可以有效提高代理的表现。我们还展示了在 LLM 生成的环境中训练在学习长远任务时比仅在原始环境中训练更有效。此外，我们还展示了一个轻量级模型（<$
    5M 参数）在用 LLM 生成的环境中训练可以超越 LLM 代理，且调用 LLM 的次数显著减少。我们还对结果进行了全面分析和消融研究，验证了 EnvGen
    框架的设计选择。我们希望我们的工作能为未来利用 LLM 训练具身代理提供指导。
- en: Acknowledgments
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Elias Stengel-Eskin for the thoughtful discussion. This work was supported
    by DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL-2112635,
    DARPA Machine Commonsense (MCS) Grant N66001-19-2-4031, ARO Award W911NF2110220,
    ONR Grant N00014-23-1-2356, and a Bloomberg Data Science Ph.D. Fellowship. The
    views contained in this article are those of the authors and not of the funding
    agency.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Elias Stengel-Eskin 进行的深思熟虑的讨论。本工作得到了DARPA ECOLE计划编号 HR00112390060，NSF-AI
    Engage Institute DRL-2112635，DARPA机器常识 (MCS) 资助编号 N66001-19-2-4031，ARO奖 W911NF2110220，ONR资助编号
    N00014-23-1-2356，以及Bloomberg数据科学博士生奖学金的支持。本文观点仅代表作者个人，与资助机构无关。
- en: References
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Afshar & Li (2024) Aida Afshar and Wenchao Li. Delf: Designing learning environments
    with foundation models. In *AAAI Workshop*, 2024.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Afshar & Li (2024) Aida Afshar 和 Wenchao Li。Delf: 使用基础模型设计学习环境。在*AAAI Workshop*，2024年。'
- en: 'Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter,
    Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J.
    Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang Huei Lee, Sergey Levine,
    Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,
    Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,
    Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan
    Yan, and Andy Zeng. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.
    In *CoRL*, 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahn 等 (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar
    Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
    Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex
    Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil
    J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang Huei Lee, Sergey
    Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka
    Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton
    Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
    Mengyuan Yan, 和 Andy Zeng。Do As I Can, Not As I Say: 将语言基础于机器人功能。在*CoRL*，2022年。'
- en: Anil et al. (2023) Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
    Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy
    Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
    Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
    Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee,
    Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao
    Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez,
    Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom,
    Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan
    Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan
    Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R.
    So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,
    Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin
    Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
    Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical
    report, 2023.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等人（2023）Rohan Anil、Andrew M. Dai、Orhan Firat、Melvin Johnson、Dmitry Lepikhin、Alexandre
    Passos、Siamak Shakeri、Emanuel Taropa、Paige Bailey、Zhifeng Chen、Eric Chu、Jonathan
    H. Clark、Laurent El Shafey、Yanping Huang、Kathy Meier-Hellstern、Gaurav Mishra、Erica
    Moreira、Mark Omernick、Kevin Robinson、Sebastian Ruder、Yi Tay、Kefan Xiao、Yuanzhong
    Xu、Yujing Zhang、Gustavo Hernandez Abrego、Junwhan Ahn、Jacob Austin、Paul Barham、Jan
    Botha、James Bradbury、Siddhartha Brahma、Kevin Brooks、Michele Catasta、Yong Cheng、Colin
    Cherry、Christopher A. Choquette-Choo、Aakanksha Chowdhery、Clément Crepy、Shachi
    Dave、Mostafa Dehghani、Sunipa Dev、Jacob Devlin、Mark Díaz、Nan Du、Ethan Dyer、Vlad
    Feinberg、Fangxiaoyu Feng、Vlad Fienber、Markus Freitag、Xavier Garcia、Sebastian Gehrmann、Lucas
    Gonzalez、Guy Gur-Ari、Steven Hand、Hadi Hashemi、Le Hou、Joshua Howland、Andrea Hu、Jeffrey
    Hui、Jeremy Hurwitz、Michael Isard、Abe Ittycheriah、Matthew Jagielski、Wenhao Jia、Kathleen
    Kenealy、Maxim Krikun、Sneha Kudugunta、Chang Lan、Katherine Lee、Benjamin Lee、Eric
    Li、Music Li、Wei Li、YaGuang Li、Jian Li、Hyeontaek Lim、Hanzhao Lin、Zhongtao Liu、Frederick
    Liu、Marcello Maggioni、Aroma Mahendru、Joshua Maynez、Vedant Misra、Maysam Moussalem、Zachary
    Nado、John Nham、Eric Ni、Andrew Nystrom、Alicia Parrish、Marie Pellat、Martin Polacek、Alex
    Polozov、Reiner Pope、Siyuan Qiao、Emily Reif、Bryan Richter、Parker Riley、Alex Castro
    Ros、Aurko Roy、Brennan Saeta、Rajkumar Samuel、Renee Shelby、Ambrose Slone、Daniel
    Smilkov、David R. So、Daniel Sohn、Simon Tokumine、Dasha Valter、Vijay Vasudevan、Kiran
    Vodrahalli、Xuezhi Wang、Pidong Wang、Zirui Wang、Tao Wang、John Wieting、Yuhuai Wu、Kelvin
    Xu、Yunhan Xu、Linting Xue、Pengcheng Yin、Jiahui Yu、Qiao Zhang、Steven Zheng、Ce Zheng、Weikang
    Zhou、Denny Zhou、Slav Petrov 和 Yonghui Wu。Palm 2 技术报告，2023。
- en: Aytar et al. (2018) Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu
    Wang, and Nando de Freitas. Playing hard exploration games by watching YouTube.
    In *NeurIPS*, 2018. URL [http://arxiv.org/abs/1805.11592](http://arxiv.org/abs/1805.11592).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aytar 等人（2018）Yusuf Aytar、Tobias Pfaff、David Budden、Tom Le Paine、Ziyu Wang 和
    Nando de Freitas。通过观看 YouTube 玩难度探索游戏。发表于 *NeurIPS*，2018。网址 [http://arxiv.org/abs/1805.11592](http://arxiv.org/abs/1805.11592)。
- en: Ba et al. (2016) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
    Normalization. In *NIPS 2016 Deep Learning Symposium*, 2016. URL [http://arxiv.org/abs/1607.06450](http://arxiv.org/abs/1607.06450).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ba 等人（2016）Jimmy Lei Ba、Jamie Ryan Kiros 和 Geoffrey E. Hinton。层归一化。发表于 *NIPS
    2016 深度学习研讨会*，2016。网址 [http://arxiv.org/abs/1607.06450](http://arxiv.org/abs/1607.06450)。
- en: Bellemare et al. (2016) Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski,
    Tom Schaul, David Saxton, and Rémi Munos. Unifying count-based exploration and
    intrinsic motivation. In *NIPS*, 2016.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare 等人（2016）Marc G. Bellemare、Sriram Srinivasan、Georg Ostrovski、Tom Schaul、David
    Saxton 和 Rémi Munos。统一基于计数的探索与内在动机。发表于 *NIPS*，2016。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language Models are Few-Shot Learners. In *NeurIPS*, 2020. URL [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell、Sandhini
    Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon Child、Aditya Ramesh、Daniel
    M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark Chen、Eric Sigler、Mateusz
    Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher Berner、Sam McCandlish、Alec
    Radford、Ilya Sutskever 和 Dario Amodei。语言模型是少量学习者。在*NeurIPS*，2020年。URL [http://arxiv.org/abs/2005.14165](http://arxiv.org/abs/2005.14165)。
- en: Burda et al. (2018) Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
    Exploration by Random Network Distillation. In *ICLR*, 2018.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burda 等（2018）Yuri Burda、Harrison Edwards、Amos Storkey 和 Oleg Klimov。通过随机网络蒸馏进行探索。在*ICLR*，2018年。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code, 2021.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021）Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde de
    Oliveira Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman、Alex
    Ray、Raul Puri、Gretchen Krueger、Michael Petrov、Heidy Khlaaf、Girish Sastry、Pamela
    Mishkin、Brooke Chan、Scott Gray、Nick Ryder、Mikhail Pavlov、Alethea Power、Lukasz
    Kaiser、Mohammad Bavarian、Clemens Winter、Philippe Tillet、Felipe Petroski Such、Dave
    Cummings、Matthias Plappert、Fotios Chantzis、Elizabeth Barnes、Ariel Herbert-Voss、William
    Hebgen Guss、Alex Nichol、Alex Paino、Nikolas Tezak、Jie Tang、Igor Babuschkin、Suchir
    Balaji、Shantanu Jain、William Saunders、Christopher Hesse、Andrew N. Carr、Jan Leike、Josh
    Achiam、Vedant Misra、Evan Morikawa、Alec Radford、Matthew Knight、Miles Brundage、Mira
    Murati、Katie Mayer、Peter Welinder、Bob McGrew、Dario Amodei、Sam McCandlish、Ilya
    Sutskever 和 Wojciech Zaremba。评估基于代码训练的大型语言模型，2021年。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
    Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily
    Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
    Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
    Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam
    Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander
    Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
    Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
    Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
    Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,
    Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling
    with Pathways. *JMLR*, pp.  1–83, 2023. URL [http://arxiv.org/abs/2204.02311](http://arxiv.org/abs/2204.02311).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2023）Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav
    Mishra、Adam Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann、Parker
    Schuh、Kensen Shi、Sasha Tsvyashchenko、Joshua Maynez、Abhishek Rao、Parker Barnes、Yi
    Tay、Noam Shazeer、Vinodkumar Prabhakaran、Emily Reif、Nan Du、Ben Hutchinson、Reiner
    Pope、James Bradbury、Jacob Austin、Michael Isard、Guy Gur-Ari、Pengcheng Yin、Toju
    Duke、Anselm Levskaya、Sanjay Ghemawat、Sunipa Dev、Henryk Michalewski、Xavier Garcia、Vedant
    Misra、Kevin Robinson、Liam Fedus、Denny Zhou、Daphne Ippolito、David Luan、Hyeontaek
    Lim、Barret Zoph、Alexander Spiridonov、Ryan Sepassi、David Dohan、Shivani Agrawal、Mark
    Omernick、Andrew M. Dai、Thanumalayan Sankaranarayana Pillai、Marie Pellat、Aitor
    Lewkowycz、Erica Moreira、Rewon Child、Oleksandr Polozov、Katherine Lee、Zongwei Zhou、Xuezhi
    Wang、Brennan Saeta、Mark Diaz、Orhan Firat、Michele Catasta、Jason Wei、Kathy Meier-Hellstern、Douglas
    Eck、Jeff Dean、Slav Petrov 和 Noah Fiedel。PaLM: 扩展语言建模与路径方法。*JMLR*，第1–83页，2023年。URL
    [http://arxiv.org/abs/2204.02311](http://arxiv.org/abs/2204.02311)。'
- en: Cobbe et al. (2020) Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
    Leveraging procedural generation to benchmark reinforcement learning. In Hal Daumé
    III and Aarti Singh (eds.), *Proceedings of the 37th International Conference
    on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*,
    pp.  2048–2056\. PMLR, 13–18 Jul 2020. URL [https://proceedings.mlr.press/v119/cobbe20a.html](https://proceedings.mlr.press/v119/cobbe20a.html).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等 (2020) Karl Cobbe, Chris Hesse, Jacob Hilton, 和 John Schulman. 利用程序生成来基准测试强化学习.
    发表在 Hal Daumé III 和 Aarti Singh (编)，*第37届国际机器学习大会论文集*，第119卷，*机器学习研究论文集*，第2048–2056页。PMLR，2020年7月13–18日。网址
    [https://proceedings.mlr.press/v119/cobbe20a.html](https://proceedings.mlr.press/v119/cobbe20a.html)。
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,
    Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
    Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An Embodied Multimodal Language
    Model. In *ICML 2023*, 2023. URL [http://arxiv.org/abs/2303.03378](http://arxiv.org/abs/2303.03378).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess 等 (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,
    Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
    Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor
    Mordatch, 和 Pete Florence. PaLM-E：一个具身的多模态语言模型. 发表在 *ICML 2023*，2023。网址 [http://arxiv.org/abs/2303.03378](http://arxiv.org/abs/2303.03378)。
- en: Du et al. (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor
    Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding Pretraining
    in Reinforcement Learning with Large Language Models. In *ICML*, 2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell,
    Pieter Abbeel, Abhishek Gupta, 和 Jacob Andreas. 通过大语言模型引导强化学习中的预训练. 发表在 *ICML*，2023。
- en: 'Duan et al. (2022) Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston
    Tan. A survey of embodied ai: From simulators to research tasks. *IEEE Transactions
    on Emerging Topics in Computational Intelligence*, 6(2):230–244, 2022. doi: 10.1109/TETCI.2022.3141105.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan 等 (2022) Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, 和 Cheston Tan.
    具身 AI 调查：从模拟器到研究任务. *IEEE Transactions on Emerging Topics in Computational Intelligence*,
    6(2):230–244, 2022。doi: 10.1109/TETCI.2022.3141105。'
- en: 'Espeholt et al. (2018) Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,
    Volodymyr Mnih, Tom Ward, Boron Yotam, Firoiu Vlad, Harley Tim, Iain Dunning,
    Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance
    Weighted Actor-Learner Architectures. In *ICML*, 2018. ISBN 9781510867963.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Espeholt 等 (2018) Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,
    Volodymyr Mnih, Tom Ward, Boron Yotam, Firoiu Vlad, Harley Tim, Iain Dunning,
    Shane Legg, 和 Koray Kavukcuoglu. IMPALA：可扩展的分布式深度强化学习与重要性加权的演员-学习者架构. 发表在 *ICML*，2018。ISBN
    9781510867963。
- en: 'Gemini Team (2023) Gemini Team. Gemini: A family of highly capable multimodal
    models, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini Team (2023) Gemini Team. Gemini：一系列高性能的多模态模型，2023。
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
    Generative Adversarial Networks. In *NIPS*, 2014. ISBN 1406.2661. URL [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 生成对抗网络.
    发表在 *NIPS*，2014。ISBN 1406.2661。网址 [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661)。
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. Deepseek-coder: When the large language model meets programming – the rise
    of code intelligence, 2024. URL [https://arxiv.org/abs/2401.14196](https://arxiv.org/abs/2401.14196).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, 和 Wenfeng
    Liang. Deepseek-coder：大型语言模型与编程相遇——代码智能的崛起，2024。网址 [https://arxiv.org/abs/2401.14196](https://arxiv.org/abs/2401.14196)。
- en: Hafner (2022) Danijar Hafner. Benchmarking the spectrum of agent capabilities.
    In *ICLR*, 2022. URL [https://github.com/danijar/crafter](https://github.com/danijar/crafter).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner (2022) Danijar Hafner. 代理能力谱的基准测试. 发表在 *ICLR*，2022。网址 [https://github.com/danijar/crafter](https://github.com/danijar/crafter)。
- en: 'Hafner et al. (2020) Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad
    Norouzi. Dream to Control: Learning Behaviors by Latent Imagination. In *ICLR*,
    2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner 等 (2020) Danijar Hafner, Timothy Lillicrap, Jimmy Ba, 和 Mohammad Norouzi.
    梦想控制：通过潜在想象学习行为. 发表在 *ICLR*，2020。
- en: Hafner et al. (2021) Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and
    Jimmy Ba. Mastering Atari with Discrete World Models. In *ICLR*, 2021.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner et al. (2021) Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, 和
    Jimmy Ba. 使用离散世界模型掌握Atari游戏。在*ICLR*中，2021。
- en: Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy
    Lillicrap. Mastering Diverse Domains through World Models, 2023. URL [http://arxiv.org/abs/2301.04104](http://arxiv.org/abs/2301.04104).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, 和 Timothy Lillicrap.
    通过世界模型掌握多样化领域，2023。URL [http://arxiv.org/abs/2301.04104](http://arxiv.org/abs/2301.04104)。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    Residual Learning for Image Recognition. In *CVPR*, 2016.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 用于图像识别的深度残差学习。在*CVPR*中，2016。
- en: 'Hessel et al. (2018) Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul,
    Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David
    Silver. Rainbow: Combining improvements in deep reinforcement learning. In *AAAI*,
    2018. ISBN 9781577358008. doi: 10.1609/aaai.v32i1.11796.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hessel et al. (2018) Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul,
    Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, 和 David Silver.
    Rainbow: 结合深度强化学习的改进。*AAAI*，2018。ISBN 9781577358008。doi: 10.1609/aaai.v32i1.11796。'
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large Language Models are Zero-Shot Reasoners. In
    *NeurIPS*, 2022. URL [http://arxiv.org/abs/2205.11916](http://arxiv.org/abs/2205.11916).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, 和 Yusuke Iwasawa. 大型语言模型是零样本推理器。在*NeurIPS*中，2022。URL [http://arxiv.org/abs/2205.11916](http://arxiv.org/abs/2205.11916)。
- en: 'Kumaran et al. (2020) Vikram Kumaran, Bradford W. Mott, and James C. Lester.
    Generating game levels for multiple distinct games with a common latent space.
    In *AIIDE*, pp.  109–115, 2020. ISBN 9781577358497. doi: 10.1609/aiide.v16i1.7485.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumaran et al. (2020) Vikram Kumaran, Bradford W. Mott, 和 James C. Lester.
    为多个不同游戏生成游戏关卡，利用公共潜在空间。在*AIIDE*中，pp. 109–115，2020。ISBN 9781577358497。doi: 10.1609/aiide.v16i1.7485。'
- en: 'Kumaran et al. (2023) Vikram Kumaran, Jonathan Rowe, Bradford Mott, and James
    Lester. SCENECRAFT: Automating Interactive Narrative Scene Generation in Digital
    Games with Large Language Models. In *AIIDE*, pp.  86–96, 2023. ISBN 157735883X.
    doi: 10.1609/aiide.v19i1.27504.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumaran et al. (2023) Vikram Kumaran, Jonathan Rowe, Bradford Mott, 和 James
    Lester. SCENECRAFT: 使用大型语言模型自动生成数字游戏中的互动叙事场景。在*AIIDE*中，pp. 86–96，2023。ISBN 157735883X。doi:
    10.1609/aiide.v19i1.27504。'
- en: Kwon et al. (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa
    Sadigh. Reward design with language models. In *International Conference on Learning
    Representations*, 2023.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard, 和 Dorsa Sadigh.
    使用语言模型进行奖励设计。在*国际学习表示会议*中，2023。
- en: Li et al. (2022a) Andrew C Li, Pashootan Vaezipoor, Rodrigo Toro Icarte, and
    Sheila A. McIlraith. Exploring long-horizon reasoning with deep RL in combinatorially
    hard tasks. In *Decision Awareness in Reinforcement Learning Workshop at ICML
    2022*, 2022a. URL [https://openreview.net/forum?id=7vPSZASOF0o](https://openreview.net/forum?id=7vPSZASOF0o).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022a) Andrew C Li, Pashootan Vaezipoor, Rodrigo Toro Icarte, 和 Sheila
    A. McIlraith. 在*决策意识与强化学习研讨会，ICML 2022*中探索深度强化学习中的长期推理。2022a。URL [https://openreview.net/forum?id=7vPSZASOF0o](https://openreview.net/forum?id=7vPSZASOF0o)。
- en: 'Li et al. (2024) Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao,
    Xiaogang Wang, Hongsheng Li, Lewei Lu, and Jifeng Dai. Auto mc-reward: Automated
    dense reward design with large language models for minecraft. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao,
    Xiaogang Wang, Hongsheng Li, Lewei Lu, 和 Jifeng Dai. Auto mc-reward: 利用大型语言模型进行Minecraft的自动化密集奖励设计。在*IEEE/CVF计算机视觉与模式识别会议*中，2024。'
- en: 'Li & Bansal (2023) Jialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic
    environment generation for vision-and-language navigation. *Advances in Neural
    Information Processing Systems*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li & Bansal (2023) Jialu Li 和 Mohit Bansal. Panogen: 基于文本的全景环境生成，用于视觉和语言导航。*神经信息处理系统进展*，2023。'
- en: 'Li et al. (2022b) Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment
    editing for vision-and-language navigation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2022b.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022b) Jialu Li, Hao Tan, 和 Mohit Bansal. Envedit: 视觉和语言导航的环境编辑。在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2022b。'
- en: 'Liu et al. (2021) Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi,
    Georgios N. Yannakakis, and Julian Togelius. Deep learning for procedural content
    generation. *Neural Comput. Appl.*, 33(1):19–37, jan 2021. ISSN 0941-0643. doi:
    10.1007/s00521-020-05383-8. URL [https://doi.org/10.1007/s00521-020-05383-8](https://doi.org/10.1007/s00521-020-05383-8).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2021）Jialin Liu, Sam Snodgrass, Ahmed Khalifa, Sebastian Risi, Georgios
    N. Yannakakis 和 Julian Togelius. 用于程序性内容生成的深度学习。*Neural Comput. Appl.*，33(1):19–37，2021年1月。ISSN
    0941-0643。doi: 10.1007/s00521-020-05383-8。网址 [https://doi.org/10.1007/s00521-020-05383-8](https://doi.org/10.1007/s00521-020-05383-8)。'
- en: 'Ma et al. (2023) Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka:
    Human-level reward design via coding large language models. *ArXiv*, abs/2310.12931,
    2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2023）Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert
    Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan 和 Anima Anandkumar. Eureka：通过编码大型语言模型设计人类级奖励。*ArXiv*，abs/2310.12931，2023年。
- en: Mojang Studios (2009) Mojang Studios. Minecraft, 2009. URL [https://www.minecraft.net/](https://www.minecraft.net/).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mojang Studios（2009）Mojang Studios. Minecraft，2009年。网址 [https://www.minecraft.net/](https://www.minecraft.net/)。
- en: Moon et al. (2023) Seungyong Moon, Junyoung Yeom, Bumsoo Park, and Hyun Oh Song.
    Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive
    Learning. In *NeurIPS*, 2023. URL [http://arxiv.org/abs/2307.03486](http://arxiv.org/abs/2307.03486).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon 等（2023）Seungyong Moon, Junyoung Yeom, Bumsoo Park 和 Hyun Oh Song. 通过对比学习发现强化学习中的层级成就。在
    *NeurIPS*，2023年。网址 [http://arxiv.org/abs/2307.03486](http://arxiv.org/abs/2307.03486)。
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, Charles Sutton, and Augustus Odena. Show Your Work: Scratchpads
    for Intermediate Computation with Language Models, 2021. URL [http://arxiv.org/abs/2112.00114](http://arxiv.org/abs/2112.00114).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye 等（2021）Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski,
    Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
    Luan, Charles Sutton 和 Augustus Odena. 展示你的工作：带有语言模型的中间计算草稿，2021年。网址 [http://arxiv.org/abs/2112.00114](http://arxiv.org/abs/2112.00114)。
- en: OpenAI (2023a) OpenAI. Gpt-4 technical report. *ArXiv*, 2023a. URL [https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI. Gpt-4 技术报告。*ArXiv*，2023a。网址 [https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815)。
- en: OpenAI (2023b) OpenAI. Chatgpt. [https://openai.com/chatgpt](https://openai.com/chatgpt),
    2023b.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023b）OpenAI. Chatgpt。网址 [https://openai.com/chatgpt](https://openai.com/chatgpt)，2023b。
- en: 'Schubert et al. (2022) Frederik Schubert, Maren Awiszus, and Bodo Rosenhahn.
    TOAD-GAN: A Flexible Framework for Few-Shot Level Generation in Token-Based Games.
    *IEEE Transactions on Games*, 14(2):284–293, 2022. ISSN 24751510. doi: 10.1109/TG.2021.3069833.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schubert 等（2022）Frederik Schubert, Maren Awiszus 和 Bodo Rosenhahn. TOAD-GAN：一种用于基于标记的游戏中少量样本关卡生成的灵活框架。*IEEE
    Transactions on Games*，14(2):284–293，2022年。ISSN 24751510。doi: 10.1109/TG.2021.3069833。'
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms, 2017.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等（2017）John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford
    和 Oleg Klimov. Proximal Policy Optimization Algorithms，2017年。
- en: Sekar et al. (2020) Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel,
    Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervisedworld
    models. In *ICML*, 2020. ISBN 9781713821120.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sekar 等（2020）Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar
    Hafner 和 Deepak Pathak. 通过自监督世界模型进行探索计划。在 *ICML*，2020年。ISBN 9781713821120。
- en: Shaker et al. (2016) Noor Shaker, Julian Togelius, and Mark J. Nelson. *Procedural
    Content Generation in Games*. Springer Publishing Company, Incorporated, 1st edition,
    2016. ISBN 3319427148.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shaker 等（2016）Noor Shaker, Julian Togelius 和 Mark J. Nelson. *游戏中的程序性内容生成*.
    Springer Publishing Company, Incorporated，第1版，2016年。ISBN 3319427148。
- en: Stanić et al. (2023) Aleksandar Stanić, Yujin Tang, David Ha, and Jürgen Schmidhuber.
    Learning to generalize with object-centric agents in the open world survival game
    crafter. *IEEE Transactions on Games*, 2023.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stanić 等（2023）Aleksandar Stanić, Yujin Tang, David Ha 和 Jürgen Schmidhuber.
    在开放世界生存游戏 Craft中与以物体为中心的智能体一起学习泛化。*IEEE Transactions on Games*，2023年。
- en: 'Sudhakaran et al. (2023) Shyam Sudhakaran, Miguel González-Duque, Claire Glanois,
    Matthias Freiberger, Elias Najarro, and Sebastian Risi. MarioGPT: Open-Ended Text2Level
    Generation through Large Language Models. In *NeurIPS*, 2023. URL [http://arxiv.org/abs/2302.05981](http://arxiv.org/abs/2302.05981).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudhakaran 等（2023）Shyam Sudhakaran, Miguel González-Duque, Claire Glanois, Matthias
    Freiberger, Elias Najarro 和 Sebastian Risi. MarioGPT：通过大型语言模型进行开放式文本2关生成。在 *NeurIPS*，2023年。网址
    [http://arxiv.org/abs/2302.05981](http://arxiv.org/abs/2302.05981)。
- en: 'Sutton & Barto (2018) Richard S. Sutton and Andrew G. Barto. *Reinforcement
    Learning: An Introduction*. The MIT Press, 2 edition, 2018.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton & Barto (2018) Richard S. Sutton 和 Andrew G. Barto。*强化学习：导论*。麻省理工学院出版社，第
    2 版，2018。
- en: 'Todd et al. (2023) Graham Todd, Sam Earle, Muhammad Umair Nasir, Michael Cerny
    Green, and Julian Togelius. Level Generation Through Large Language Models. In
    *FDG*, 2023. ISBN 9781450398565. doi: 10.1145/3582437.3587211.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Todd et al. (2023) Graham Todd、Sam Earle、Muhammad Umair Nasir、Michael Cerny
    Green 和 Julian Togelius。通过大型语言模型生成关卡。在 *FDG*，2023。ISBN 9781450398565。doi: 10.1145/3582437.3587211。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurelien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample。Llama: 开放且高效的基础语言模型，2023a。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023b.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad
    Almahairi、Yasmine Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti
    Bhosale、Dan Bikel、Lukas Blecher、Cristian Canton Ferrer、Moya Chen、Guillem Cucurull、David
    Esiobu、Jude Fernandes、Jeremy Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman
    Goyal、Anthony Hartshorn、Saghar Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor
    Kerkez、Madian Khabsa、Isabel Kloumann、Artem Korenev、Punit Singh Koura、Marie-Anne
    Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai Lu、Yuning Mao、Xavier
    Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin Nie、Andrew Poulton、Jeremy
    Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan Silva、Eric Michael Smith、Ranjan
    Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross Taylor、Adina Williams、Jian Xiang
    Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela Fan、Melanie Kambadur、Sharan
    Narang、Aurelien Rodriguez、Robert Stojnic、Sergey Edunov 和 Thomas Scialom。Llama
    2: 开放基础和微调聊天模型，2023b。'
- en: Walker et al. (2023) Jacob Walker, Eszter Vértes, Yazhe Li, Gabriel Dulac-Arnold,
    Ankesh Anand, Théophane Weber, and Jessica B. Hamrick. Investigating the Role
    of Model-Based Learning in Exploration and Transfer. In *ICML*, 2023.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walker et al. (2023) Jacob Walker、Eszter Vértes、Yazhe Li、Gabriel Dulac-Arnold、Ankesh
    Anand、Théophane Weber 和 Jessica B. Hamrick。探讨模型基学习在探索和迁移中的作用。载于 *ICML*，2023。
- en: 'Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An Open-Ended Embodied
    Agent with Large Language Models, 2023a. URL [http://arxiv.org/abs/2305.16291](http://arxiv.org/abs/2305.16291).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023a) Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei
    Xiao、Yuke Zhu、Linxi Fan 和 Anima Anandkumar。Voyager: 一种基于大型语言模型的开放式具身智能体，2023a。网址
    [http://arxiv.org/abs/2305.16291](http://arxiv.org/abs/2305.16291)。'
- en: 'Wang et al. (2023b) Ruoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, Marc-Alexandre
    Côté, and Peter Jansen. ByteSized32: A corpus and challenge task for generating
    task-specific world models expressed as text games. In Houda Bouamor, Juan Pino,
    and Kalika Bali (eds.), *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, pp.  13455–13471, Singapore, December 2023b.
    Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.830.
    URL [https://aclanthology.org/2023.emnlp-main.830](https://aclanthology.org/2023.emnlp-main.830).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023b) Ruoyao Wang、Graham Todd、Xingdi Yuan、Ziang Xiao、Marc-Alexandre
    Côté 和 Peter Jansen。ByteSized32: 一种用于生成任务特定世界模型的语料库和挑战任务，表现为文本游戏。载于 Houda Bouamor、Juan
    Pino 和 Kalika Bali（编），*2023年自然语言处理实证方法会议论文集*，第 13455–13471 页，新加坡，2023年12月。计算语言学协会。doi:
    10.18653/v1/2023.emnlp-main.830。网址 [https://aclanthology.org/2023.emnlp-main.830](https://aclanthology.org/2023.emnlp-main.830)。'
- en: 'Wang et al. (2023c) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. Describe, Explain, Plan and Select: Interactive Planning
    with Large Language Models Enables Open-World Multi-Task Agents. In *NeurIPS*,
    2023c. URL [http://arxiv.org/abs/2302.01560](http://arxiv.org/abs/2302.01560).'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023c）Zihao Wang、Shaofei Cai、Guanzhou Chen、Anji Liu、Xiaojian Ma 和 Yitao
    Liang。*Describe, Explain, Plan and Select: Interactive Planning with Large Language
    Models Enables Open-World Multi-Task Agents*。发表于 *NeurIPS*，2023c。网址 [http://arxiv.org/abs/2302.01560](http://arxiv.org/abs/2302.01560)。'
- en: 'Wang et al. (2023d) Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing
    Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian
    Ma, and Yitao Liang. JARVIS-1: Open-World Multi-task Agents with Memory-Augmented
    Multimodal Language Models, 2023d. URL [http://arxiv.org/abs/2311.05997](http://arxiv.org/abs/2311.05997).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023d）Zihao Wang、Shaofei Cai、Anji Liu、Yonggang Jin、Jinbing Hou、Bowei
    Zhang、Haowei Lin、Zhaofeng He、Zilong Zheng、Yaodong Yang、Xiaojian Ma 和 Yitao Liang。*JARVIS-1:
    Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models*，2023d。网址
    [http://arxiv.org/abs/2311.05997](http://arxiv.org/abs/2311.05997)。'
- en: Wang et al. (2023e) Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal,
    Stephen Gould, Hao Tan, and Yu Qiao. Scaling data generation in vision-and-language
    navigation. In *ICCV*, 2023e.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023e）Zun Wang、Jialu Li、Yicong Hong、Yi Wang、Qi Wu、Mohit Bansal、Stephen
    Gould、Hao Tan 和 Yu Qiao。*Scaling data generation in vision-and-language navigation*。发表于
    *ICCV*，2023e。
- en: Watkins (1989) Christopher J.C.H. Watkins. *Learning from Delayed Rewards*.
    PhD thesis, University of Cambridge, England, May 1989.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins（1989）Christopher J.C.H. Watkins。*Learning from Delayed Rewards*。博士学位论文，剑桥大学，英国，1989年5月。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits
    Reasoning in Large Language Models. In *NeurIPS*, pp.  1–43, 2022. URL [http://arxiv.org/abs/2201.11903](http://arxiv.org/abs/2201.11903).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei
    Xia、Ed Chi、Quoc Le 和 Denny Zhou。*Chain-of-Thought Prompting Elicits Reasoning
    in Large Language Models*。发表于 *NeurIPS*，第 1–43 页，2022。网址 [http://arxiv.org/abs/2201.11903](http://arxiv.org/abs/2201.11903)。
- en: Weng (2020) Lilian Weng. Exploration strategies in deep reinforcement learning.
    *lilianweng.github.io*, Jun 2020. URL [https://lilianweng.github.io/posts/2020-06-07-exploration-drl/](https://lilianweng.github.io/posts/2020-06-07-exploration-drl/).
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng（2020）Lilian Weng。*Exploration strategies in deep reinforcement learning*。*lilianweng.github.io*，2020年6月。网址
    [https://lilianweng.github.io/posts/2020-06-07-exploration-drl/](https://lilianweng.github.io/posts/2020-06-07-exploration-drl/)。
- en: 'Wu et al. (2023) Yue Wu, Shrimai Prabhumoye, So Yeon Min, Yonatan Bisk, Ruslan
    Salakhutdinov, Amos Azaria, Tom Mitchell, and Yuanzhi Li. SPRING: Studying the
    Paper and Reasoning to Play Games. In *NeurIPS*, 2023. URL [http://arxiv.org/abs/2305.15486](http://arxiv.org/abs/2305.15486).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等（2023）Yue Wu、Shrimai Prabhumoye、So Yeon Min、Yonatan Bisk、Ruslan Salakhutdinov、Amos
    Azaria、Tom Mitchell 和 Yuanzhi Li。*SPRING: Studying the Paper and Reasoning to
    Play Games*。发表于 *NeurIPS*，2023。网址 [http://arxiv.org/abs/2305.15486](http://arxiv.org/abs/2305.15486)。'
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language
    Models. In *ICLR*, 2023. URL [http://arxiv.org/abs/2210.03629](http://arxiv.org/abs/2210.03629).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等（2023）Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik Narasimhan
    和 Yuan Cao。*ReAct: Synergizing Reasoning and Acting in Language Models*。发表于 *ICLR*，2023。网址
    [http://arxiv.org/abs/2210.03629](http://arxiv.org/abs/2210.03629)。'
- en: Yuan et al. (2023) Haoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin
    Cai, Hao Dong, and Zongqing Lu. Skill Reinforcement Learning and Planning for
    Open-World Long-Horizon Tasks. In *Foundation Models for Decision Making Workshop
    at NeurIPS*, 2023.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等（2023）Haoqi Yuan、Chi Zhang、Hongcheng Wang、Feiyang Xie、Penglin Cai、Hao
    Dong 和 Zongqing Lu。*Skill Reinforcement Learning and Planning for Open-World Long-Horizon
    Tasks*。发表于 *Foundation Models for Decision Making Workshop at NeurIPS*，2023。
- en: 'Zhao et al. (2023) Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu
    Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, and Gaoang Wang. See and Think: Embodied
    Agent in Virtual Environment, 2023. URL [http://arxiv.org/abs/2311.15209](http://arxiv.org/abs/2311.15209).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等（2023）Zhonghan Zhao、Wenhao Chai、Xuan Wang、Li Boyi、Shengyu Hao、Shidong
    Cao、Tian Ye、Jenq-Neng Hwang 和 Gaoang Wang。*See and Think: Embodied Agent in Virtual
    Environment*，2023。网址 [http://arxiv.org/abs/2311.15209](http://arxiv.org/abs/2311.15209)。'
- en: Appendix
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'In this appendix, we present additional experiment results with another RL
    agent ([Appendix A](#A1 "Appendix A Additional Experiment Results with Another
    RL Agent ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents")), RL agent implementation details ([Appendix B](#A2 "Appendix
    B RL Agent Implementation Details ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents")), additional LLM details ([Appendix C](#A3
    "Appendix C Additional LLM Details ‣ EnvGen: Generating and Adapting Environments
    via LLMs for Training Embodied Agents")), and limitations ([Appendix D](#A4 "Appendix
    D Limitations ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents")).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '在本附录中，我们展示了另一个 RL 代理的额外实验结果（[附录 A](#A1 "Appendix A Additional Experiment Results
    with Another RL Agent ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")），RL 代理实现细节（[附录 B](#A2 "Appendix B RL Agent Implementation
    Details ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents")），额外的 LLM 细节（[附录 C](#A3 "Appendix C Additional LLM Details ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents")），以及限制（[附录 D](#A4
    "Appendix D Limitations ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")）。'
- en: Appendix A Additional Experiment Results with Another RL Agent
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 另一个 RL 代理的额外实验结果
- en: Achievement Distillation + EnvGen.
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成就蒸馏 + EnvGen。
- en: 'As mentioned in the [Sec. 4.2](#S4.SS2 "4.2 Detailed Achievement Analysis on
    Crafter Environment ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting
    Environments via LLMs for Training Embodied Agents"), we also experiment using
    EnvGen with the Achievement Distillation (AD) (Moon et al., [2023](#bib.bib36))
    agent. As shown in [Fig. 8](#A1.F8 "In Achievement Distillation + EnvGen. ‣ Appendix
    A Additional Experiment Results with Another RL Agent ‣ EnvGen: Generating and
    Adapting Environments via LLMs for Training Embodied Agents"), similar to our
    results on the PPO-based agent, we find that by applying EnvGen, there is performance
    gain in long-horizon tasks like making iron tools and collecting diamonds.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '如[第 4.2 节](#S4.SS2 "4.2 Detailed Achievement Analysis on Crafter Environment
    ‣ 4 Results and Analysis ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents")中提到，我们还尝试使用 EnvGen 和成就蒸馏（AD）（Moon 等，[2023](#bib.bib36)）代理。如[图
    8](#A1.F8 "In Achievement Distillation + EnvGen. ‣ Appendix A Additional Experiment
    Results with Another RL Agent ‣ EnvGen: Generating and Adapting Environments via
    LLMs for Training Embodied Agents")所示，与我们在基于 PPO 的代理上的结果类似，我们发现通过应用 EnvGen，在长时间跨度任务中如制作铁工具和收集钻石等方面有性能提升。'
- en: '![Refer to caption](img/eb8637a817492174003c59c5230faafc.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb8637a817492174003c59c5230faafc.png)'
- en: 'Figure 8: Success rates for all Crafter achievements of two Achievement Distillation
    (AD) agents (Moon et al., [2023](#bib.bib36)): (1) Baseline: trained in Crafter
    for 1.96M steps, and (2) Ours: trained in Crafter${}^{\text{EnvGen}}$ for 0.96M
    steps and Crafter for 1M steps.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：两个成就蒸馏（AD）代理（Moon 等，[2023](#bib.bib36)）在 Crafter 所有成就的成功率：（1）基线：在 Crafter
    中训练 1.96M 步，（2）我们的：在 Crafter${}^{\text{EnvGen}}$ 中训练 0.96M 步，在 Crafter 中训练 1M
    步。
- en: Appendix B RL Agent Implementation Details
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B RL 代理实现细节
- en: PPO agent.
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PPO 代理。
- en: We use the PPO-based (Schulman et al., [2017](#bib.bib41)) agent used in (Moon
    et al., [2023](#bib.bib36)), which modifies the default ResNet (He et al., [2016](#bib.bib23))
    architecture in IMPALA (Espeholt et al., [2018](#bib.bib15)) by increasing channel
    size and hidden size and adding a layer normalization Ba et al. ([2016](#bib.bib5))
    before each linear/convolutional layer. We slightly modify this architecture further
    to place the layer norm after the final linear layer instead of before. Hyperparameters
    for this model are shown in Table 9.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了基于 PPO 的（Schulman 等，[2017](#bib.bib41)）代理，该代理用于（Moon 等，[2023](#bib.bib36)），它通过增加通道大小和隐藏层大小以及在每个线性/卷积层之前添加一层归一化（Ba
    等，[2016](#bib.bib5)）来修改默认的 ResNet（He 等，[2016](#bib.bib23)）架构。我们进一步稍微修改了这一架构，将层归一化放在最终线性层之后，而不是之前。该模型的超参数如表
    9 所示。
- en: '| Hyperparameter | Value |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| Discount factor | 0.95 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 折扣因子 | 0.95 |'
- en: '| GAE smoothing parameter | 0.65 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| GAE 平滑参数 | 0.65 |'
- en: '| # timesteps per rollout | 4096 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 每次滚动的时间步数 | 4096 |'
- en: '| # epochs per rollout | 3 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 每次滚动的纪元数 | 3 |'
- en: '| # mini-batches per epoch | 8 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 每个纪元的迷你批次数 | 8 |'
- en: '| Entropy bonus | 0.01 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 熵奖励 | 0.01 |'
- en: '| PPO clip range | 0.2 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| PPO 剪辑范围 | 0.2 |'
- en: '| Reward normalization | No |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 奖励归一化 | 否 |'
- en: '| EWMA decay rate | 0.99 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| EWMA 衰减率 | 0.99 |'
- en: '| Learning rate | 3e-4 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 3e-4 |'
- en: '| Max grad norm | 0.5 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 最大梯度范数 | 0.5 |'
- en: '| Value function coefficient | 0.5 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 值函数系数 | 0.5 |'
- en: 'Table 9: PPO agent hyperparameters. Hyperparameters are following Moon et al.
    ([2023](#bib.bib36)).'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：PPO 代理的超参数。超参数参照 Moon 等（[2023](#bib.bib36)）。
- en: '| Hyperparameter | Value |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 值 |'
- en: '| Policy regularizer coefficient | 1.0 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 策略正则化系数 | 1.0 |'
- en: '| Value regularizer coefficient | 1.0 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 值正则化系数 | 1.0 |'
- en: '| Entropic regularizer coefficient | 0.05 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 熵正则化系数 | 0.05 |'
- en: '| # policy phases per auxiliary phase | 8 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| # 每个辅助阶段的策略阶段 | 8 |'
- en: '| # epochs per auxiliary phase | 6 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| # 每个辅助阶段的训练轮数 | 6 |'
- en: 'Table 10: Achievement Distillation hyperparameters. Hyperparameters are following
    Moon et al. ([2023](#bib.bib36)).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：成就蒸馏超参数。超参数参考 Moon 等人 ([2023](#bib.bib36))。
- en: Achievement distillation (AD) agent.
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成就蒸馏（AD）代理。
- en: 'Moon et al. ([2023](#bib.bib36)) builds upon its PPO-based agent model and
    adds auxiliary training steps after the PPO policy updates. Their auxiliary training
    consists of two parts: (1) intra-trajectory achievement prediction and (2) cross-trajectory
    achievement matching. (1) Intra-trajectory achievement prediction maximizes the
    similarity between state-action pairs and the corresponding next achievement that
    needs to be unlocked in the achievement hierarchy within an episode. (2) Cross-trajectory
    achievement matching maximizes the similarity between achievements across episodes.
    Hyperparameters for this model are shown in Table 10.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Moon 等人 ([2023](#bib.bib36)) 在其基于 PPO 的代理模型上进行了改进，并在 PPO 策略更新后添加了辅助训练步骤。他们的辅助训练包括两个部分：（1）轨迹内成就预测和（2）轨迹间成就匹配。（1）轨迹内成就预测最大化状态-动作对与需要在成就层次结构中解锁的相应下一个成就之间的相似度。（2）轨迹间成就匹配最大化跨剧集成就之间的相似度。该模型的超参数见表
    10。
- en: Appendix C Additional LLM Details
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 额外的 LLM 细节
- en: Prompt Template.
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示模板。
- en: 'In [Fig. 9](#A3.F9 "In Prompt Template. ‣ Appendix C Additional LLM Details
    ‣ EnvGen: Generating and Adapting Environments via LLMs for Training Embodied
    Agents") (a), we show the LLM prompt template that is used to generate environments.
    The contents of the prompt can vary slightly between different environments/games
    though generally remain the same. In [Fig. 9](#A3.F9 "In Prompt Template. ‣ Appendix
    C Additional LLM Details ‣ EnvGen: Generating and Adapting Environments via LLMs
    for Training Embodied Agents") (b), we show the additional prompt template that
    is used during the feedback step (step 4 in [Sec. 2.2](#S2.SS2 "2.2 EnvGen Method
    Details ‣ 2 EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents ‣ EnvGen: Generating and Adapting Environments via LLMs for Training
    Embodied Agents")). At each feedback cycle iteration, the additional prompt is
    concatenated to the previous LLM output (*i.e*., maintaining a chat history).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 9](#A3.F9 "在提示模板中。 ‣ 附录 C 额外的 LLM 细节 ‣ EnvGen：通过 LLM 生成和调整环境以训练具身体的代理")
    (a) 中，我们展示了用于生成环境的 LLM 提示模板。虽然提示的内容在不同环境/游戏之间可能略有不同，但通常保持不变。在[图 9](#A3.F9 "在提示模板中。
    ‣ 附录 C 额外的 LLM 细节 ‣ EnvGen：通过 LLM 生成和调整环境以训练具身体的代理") (b) 中，我们展示了在反馈步骤（[第 4 步](#S2.SS2
    "2.2 EnvGen 方法细节 ‣ 2 EnvGen：通过 LLM 生成和调整环境以训练具身体的代理 ‣ EnvGen：通过 LLM 生成和调整环境以训练具身体的代理")）期间使用的额外提示模板。在每次反馈周期迭代中，额外提示会与之前的
    LLM 输出拼接在一起（*即*，保持聊天记录）。
- en: '![Refer to caption](img/165331b1011055fc217823e79796b466.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/165331b1011055fc217823e79796b466.png)'
- en: 'Figure 9: The prompts that are given to the LLM to generate environments in
    step 1 of [Sec. 2.2](#S2.SS2 "2.2 EnvGen Method Details ‣ 2 EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents ‣ EnvGen: Generating
    and Adapting Environments via LLMs for Training Embodied Agents").'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：在 [第 2.2 节](#S2.SS2 "2.2 EnvGen 方法细节 ‣ 2 EnvGen：通过 LLM 生成和调整环境以训练具身体的代理
    ‣ EnvGen：通过 LLM 生成和调整环境以训练具身体的代理") 第 1 步中给 LLM 的生成环境的提示。
- en: API Cost.
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: API 成本。
- en: As we use GPT-4-Turbo (1106-preview version) the API cost is $10.00 per 1M tokens
    and $30.00 per 1M tokens. The initial environment generation cost is $0.03 and
    then each iteration of the feedback cycle adds $0.04. Once the model is trained
    via EnvGen it no longer requires any LLM calls for inference or further training
    on the original environment. Works like SPRING (Wu et al., [2023](#bib.bib59))
    require $270 USD and several thousand LLM calls per episode, which is much more
    expensive than our work.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是 GPT-4-Turbo（1106-preview 版本），API 成本为每 100 万个标记 $10.00 和每 100 万个标记 $30.00。初始环境生成成本为
    $0.03，然后每次反馈周期的迭代增加 $0.04。一旦通过 EnvGen 训练完成，模型就不再需要任何 LLM 调用进行推断或在原始环境上进行进一步训练。像
    SPRING（Wu 等人，[2023](#bib.bib59)）这样的工作每集需要 $270 美元和几千次 LLM 调用，比我们的工作要昂贵得多。
- en: Appendix D Limitations
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 限制
- en: EnvGen relies on strong LLMs (*e.g*., GPT-4). But note that one of the main
    motivations of EnvGen is to more efficiently use LLMs to help train embodied agents,
    and as such EnvGen requires very few LLM calls (*e.g*., 4 calls), which only costs
    less than $1 USD during the entire training. We hope that advances in quantization/distillation
    and open-source models will make strong LLMs more accessible.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: EnvGen 依赖于强大的 LLMs (*例如*，GPT-4)。但请注意，EnvGen 的主要动机之一是更高效地使用 LLMs 来帮助训练具身智能体，因此
    EnvGen 只需非常少量的 LLM 调用 (*例如*，4 次)，这在整个训练过程中花费不到 1 美元。我们希望量化/蒸馏技术和开源模型的进步能使强大的 LLMs
    更加可及。
- en: EnvGen also requires that the environment simulators can (or be easily edited
    to) accept configurations in standard formats (*e.g*., JSON, CSV, YAML, TOML *etc*.),
    and the LLM can correctly generate configurations in such formats. Note that such
    text configuration formats are widely used for managing game simulators. We empirically
    find that environment configurations generated by GPT-4 can be easily parsed without
    error.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: EnvGen 还要求环境模拟器能够（或容易编辑为）接受标准格式的配置 (*例如*，JSON、CSV、YAML、TOML *等*)，并且 LLM 能正确生成这些格式的配置。请注意，这种文本配置格式在管理游戏模拟器中被广泛使用。我们通过经验发现，GPT-4
    生成的环境配置可以轻松解析且不会出错。
