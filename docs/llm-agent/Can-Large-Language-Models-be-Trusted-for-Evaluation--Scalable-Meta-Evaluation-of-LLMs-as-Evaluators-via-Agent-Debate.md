<!--yml
category: 未分类
date: 2025-01-11 12:57:00
-->

# Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate

> 来源：[https://arxiv.org/html/2401.16788/](https://arxiv.org/html/2401.16788/)

Steffi Chern^(2,4)  Ethan Chern^(1,4)  Graham Neubig²  Pengfei Liu^(1,3,4)
¹Shanghai Jiao Tong University  ²Carnegie Mellon University
³Shanghai Artificial Intelligence Laboratory  ⁴Generative AI Research Lab (GAIR)   Corresponding author

###### Abstract

Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: [https://github.com/GAIR-NLP/scaleeval](https://github.com/GAIR-NLP/scaleeval).

## 1 Introduction

![Refer to caption](img/e24a42f1ce74dbd339846ca58bcd7873.png)

Figure 1: We demonstrate ScaleEval, our scalable meta-evaluation framework. This is used in assessing the reliability and robustness of employing LLMs as evaluators for different evaluative purposes.

Large Language Models (LLMs) Bubeck et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib3)); Gemini Team et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib12)) have rapidly evolved to the point where they can tackle a wide range of tasks with impressive performance. While this has unlocked a variety of exciting potential applications, it has also introduced complex challenges in evaluating the generated outputs. Current efforts on LLM evaluation primarily focus on automated evaluation metrics (Fu et al., [2023](https://arxiv.org/html/2401.16788v1#bib.bib10); Li et al., [2023c](https://arxiv.org/html/2401.16788v1#bib.bib19); Zheng et al., [2023](https://arxiv.org/html/2401.16788v1#bib.bib25); Wang et al., [2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)), many of which use LLMs themselves to do evaluation. However, when these LLMs as evaluators are applied to a new task, it begs the question: *can LLMs be trusted for evaluation?* In many cases, the answer is not clear.

On the other hand, there are a few fortunate tasks where meta-evaluation (evaluation of evaluation metrics) has been performed rigorously (§[2](https://arxiv.org/html/2401.16788v1#S2 "2 Related Work ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")). This meta-evaluation typically involves the collection of human-annotated judgements for particular criteria (e.g. fluency of outputs, semantic adherence to the input). For instance, for machine translation quality metrics, there is an extensive meta-evaluation data from the WMT metrics task Freitag et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib9)), and for summarization there are datasets like TAC and RealSum Dang et al. ([2008](https://arxiv.org/html/2401.16788v1#bib.bib8)); Bhandari et al. ([2020](https://arxiv.org/html/2401.16788v1#bib.bib2)). Once such a dataset is collected, meta-evaluation can be performed by measuring the correlation between automatic evaluation metrics and the human gold-standard (§[3](https://arxiv.org/html/2401.16788v1#S3 "3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).

However, these datasets are extremely costly to collect, as they require meticulous annotation by skilled human experts. With the increasing use of LLMs for various purposes such as math problem solving Hendrycks et al. ([2021](https://arxiv.org/html/2401.16788v1#bib.bib14)), reading comprehension Zhong et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib26)), creative writing Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)), multilingual applications Hu et al. ([2020](https://arxiv.org/html/2401.16788v1#bib.bib15)); Bang et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib1)), and many more, it is not feasible to create these human-judged datasets for every new task. As a result, LLMs as evaluators are used without proper vetting, and in many cases the evaluators themselves are highly unreliable (Wang et al., [2023b](https://arxiv.org/html/2401.16788v1#bib.bib24); Huang et al., [2023](https://arxiv.org/html/2401.16788v1#bib.bib16)).

In this paper, we propose ScaleEval, a *scalable meta-evaluation framework* for the era of LLMs, which creates meta-evaluation benchmarks across various tasks and scenarios (§[4](https://arxiv.org/html/2401.16788v1#S4 "4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")). Concretely, ScaleEval relies on debate between multiple LLM agents, followed by minimal human oversight in cases where the agent LLMs do not agree (Fig. [1](https://arxiv.org/html/2401.16788v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")). Since our framework allows users to use their own prompts and responses while applying the framework to any scenario or criterion that they define, it offers flexibility and adaptability in various evaluation contexts.

In experiments, we conduct meta-meta evaluation (§[6](https://arxiv.org/html/2401.16788v1#S6 "6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")) demonstrating that our proposed approach correlates well with when meta-evaluation is performed entirely by human expert annotators. Further, we assess the reliability and cost-performance trade-off of various LLMs as evaluators under a variety of scenarios, and closely examine their specific capabilities and limitations as evaluators (§[7](https://arxiv.org/html/2401.16788v1#S7 "7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")). We also examine the impact that variations in prompts used for evaluation can have on the performance of LLMs as evaluators (§[8](https://arxiv.org/html/2401.16788v1#S8 "8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).

All code from our framework is made available open-source, enabling the community to conduct meta-evaluation on LLMs as evaluators using their own prompts, LLM responses, criteria, and scenarios.

## 2 Related Work

 |  | Meta-Eval | # Scenarios | Custom. | Scala. |
| --- | --- | --- | --- | --- |
| LLM-as-a-Judge | Human | High | ✗ | Low |
| FairEval | Human | Low | ✗ | Low |
| ChatEval | Human | Low | ✗ | Low |
| ScaleEval | Agent Debate | High | ✓ | High | 

Table 1: Comparison of the meta-evaluation processes across different strategies using LLMs as evaluators: LLM-as-a-Judge Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)), FairEval Wang et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib24)), ChatEval Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)), and our own work, ScaleEval. “Custom.” denotes whether the evaluation criterion could be customized. “Scala.” refers to scalability.

### 2.1 Automatic Evaluation of LLM Output

The most common paradigm for evaluating LLMs is to evaluate their capabilities on standard benchmarks for tasks such as reasoning (e.g. BigBench Srivastava et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib22))), common sense QA (e.g. MMLU Hendrycks et al. ([2020](https://arxiv.org/html/2401.16788v1#bib.bib13))), or code generation (e.g. HumanEval Chen et al. ([2021b](https://arxiv.org/html/2401.16788v1#bib.bib6))). These are indicative of the capabilities of the models, but do not measure model abilities for open-ended tasks requiring generation of free-form text.

To adapt to the rapid growth in the capabilities of LLMs for open-ended tasks, LLM evaluation has started to shift towards evaluating generated text directly, often using LLMs themselves as evaluators Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10)); Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)); Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Wang et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)). In addition, there are a few recent works that perform LLM-based multi-agent debate to improve the fidelity of evaluation Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)); Li et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib18)). While these methods take advantage of the instruction-following capabilities and versatility of LLMs, directly using LLMs as evaluators or communicative agents out-of-the-box in diverse, unseen user-defined scenarios provides no guarantees with respect to the accuracy of these methods. We aim to address this issue by introducing scalable meta-evaluation to ensure the reliability of the evaluation protocol under diverse scenarios.

Another widely used evaluation platform, Chatbot Arena Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)) supports a crowd-sourcing method to collect diverse user prompts from various scenarios. However, the process of evaluating LLMs’ performance in Chatbot Arena relies heavily on human evaluations, which may not be readily accessible to everyone interested in assessing LLMs’ abilities for a specific tasks or scenario. In addition, the human evaluators involved are not subject to a uniform set of standards or explicit evaluation guidelines, which could lead to biased or imprecise evaluation assessments.

### 2.2 Meta-Evaluation of LLMs as Evaluators

Previous research proposing methods for LLMs as evaluators usually involves conducting meta-evaluation in 3 different ways: (i) leveraging existing NLP meta-evaluation benchmarks Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10)); Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)), (ii) conducting small-scale meta-evaluations on expert-annotated datasets for specific tasks or scenarios Chiang and Lee ([2023](https://arxiv.org/html/2401.16788v1#bib.bib7)); Wang et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)); Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)), or (iii) using crowd-sourcing platforms to collect human annotations Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)). However, due to the lack of coverage in existing datasets and annotation budgets, both (i) and (ii) are inherently limited in their comprehensiveness. (iii) can provide more comprehensive meta-evaluation via crowd-sourcing, but the amount of human annotation required in the meta-evaluation process limits the scalability of the approach, and crowd workers may not be particularly accurate at more complex tasks. To address these issues, we propose an agent-debate-assisted meta-evaluation approach to mitigate this effort.

## 3 Preliminaries

In this section, we provide an introduction to the concepts of automatic evaluation and meta-evaluation systems, particularly focused on evaluation of LLM-generated outputs in the era of generative AI.

### 3.1 Key Terms

We first define some key terms that will be used throughout our paper.

*   •

    Criterion: A criterion defines a standard that measures the quality of the response generated by LLMs based on the user prompt. Some examples include: helpfulness, fluency, factuality, or creativity, among others.

*   •

    Scenario: A scenario describes the real-world situations in which users are interacting with LLMs. For example, brainstorming, coding, and dialog, among others.

### 3.2 Automatic Evaluation

Automatic evaluation using LLMs measures the quality of LLM-generated responses given prompts under different criteria. Usually, automatic evaluation is conducted with one of two different protocols: single-response evaluation and pairwise response comparison Ouyang et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib21)); Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Li et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib17)). In this paper, we focus on pairwise response comparison. Pairwise response comparison is intuitive for both humans and LLMs as evaluators when conducting assessments. It could be further extended to provide win-rates and Elo scores across models Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)), offering a straightforward leaderboard to understand the relative performance of different models under various scenarios. Formally, given an automatic evaluation metric $E$, a user-defined evaluation criterion $c$ (e.g. helpfulness, reasoning, creativity), a user prompt $p$, and responses generated by two systems $r_{1},r_{2}$, evaluation for pairwise response comparison is done in the following way:

|  | $o=E(c,p,r_{1},r_{2}).$ |  | (1) |

$o\in\{1,0,-1\}$ represents that $r_{1}$ is better, equal, or worse than $r_{2}$, respectively, given the user prompt $p$ under criterion $c$.

### 3.3 Meta-Evaluation

Meta-evaluation assesses the quality of an automatic evaluation metric. Formally, we define a gold-standard evaluation metric $G$ (e.g. human experts) that other automatic metrics should aspire to match. In pairwise response comparison, the meta-evaluation dataset $\mathcal{G}=\{G(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$ contains user prompts and corresponding responses from two systems, annotated with gold-standard evaluations. The meta-evaluation process assesses the performance $\textsc{meta}(E)$ of the automatic evaluation metric $E$ under a certain criterion $c$.

In pairwise response comparison, the meta-evaluation measures the example-level agreement rate or the system-level agreement rate between $E$ and $G$ across the meta-evaluation dataset. A high agreement rate between $E$ and $G$ represents that $E$ is a good automatic evaluation metric.

For the example-level agreement rate, we calculate:

|  | $\textsc{meta}(E)=\frac{1}{n}\sum_{i=1}^{n}\delta_{E(c,p_{i},r_{1,i},r_{2,i}),G% (c,p_{i},r_{1,i},r_{2,i})},$ |  | (2) |

where $0\leq\textsc{meta}(E)\leq 1$, and $\delta_{\cdot,\cdot}$ refers to the Kronecker delta function.

For the system-level agreement rate, given that $\mathcal{E}=\{E(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$ and $\mathcal{G}=\{G(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$, we calculate:

|  | $\textsc{meta}(E)=\delta_{\mathrm{mode}(\mathcal{E}),\mathrm{mode}(\mathcal{G})},$ |  | (3) |

where $\textsc{meta}(E)\in\{0,1\}$, $\delta_{\cdot,\cdot}$ refers to the Kronecker delta function, and $\mathrm{mode(\cdot)}$ refers to the value (either $1,0,-1$ in this case) that appears most often in the set $\mathcal{\mathcal{E}}$ or $\mathcal{\mathcal{G}}$.

## 4 Methodology

In this section, we detail the frameworks that ScaleEval employs for meta-evaluation, evaluation, and human expert meta-meta evaluation. For meta-evaluation, we generally follow the pairwise response comparison setting described in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). Notably, instead of relying solely on human labor to construct the meta-evaluation benchmark $\mathcal{G}$, we use a scalable, agent-debate assisted framework to instantiate the golden metric $G$ and construct the benchmark $\mathcal{G}$. For evaluation, we follow the pairwise response comparison setting outlined in §[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). The meta-meta evaluation process also follows the rules for meta-evaluation, as described in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). The process is included to ensure the reliability of using the agent-debate assisted meta-evaluation framework.

### 4.1 Meta-Evaluation Framework via Multi-Agent Debate

The meta-evaluation framework involves multiple communicative agents $\{A_{j}\}_{j=1}^{m}$ that conduct rounds of discussion $d=0\sim D-1$ with each other. This is less time-consuming and costly compared to traditional methods for meta-evaluation that relies entirely on human effort. With this agent-debate-assisted meta-evaluation framework, we can leverage each LLM agent’s distinct understanding about each query prompt $p_{i}$, LLM responses $r_{1,i},r_{2,i}$, and defined criterion $c$ to make a comprehensive assessment of LLMs under different scenarios and criteria. Each LLM agent is capable of providing an evaluation result regarding which response is better, along with its corresponding justifications. Note that each LLM agent can also review other agents’ evaluation results and justifications after the initial round of discussion.

In the initial round of discussion $d=0$, each LLM agent independently provides an evaluation result and justification:

|  | $\mathcal{A}_{0}=[A_{1}(c,p_{i},r_{1,i},r_{2,i},\varnothing),\ldots,\\ A_{m}(c,p_{i},r_{1,i},r_{2,i},\varnothing)],$ |  | (4) |

where

|  | $\mathcal{A}_{0}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$ |  | (5) |

indicates whether $r_{1,i}$ is better, equal, or worse than $r_{2,i}$, respectively, along with its justification. Note that the $\varnothing$ in the last argument of $A_{j}$ represents that in the initial round of discussion, each agent doesn’t have access to previous rounds of discussion. In subsequent discussion rounds $d=1\sim D-1$, agents are allowed to look at other agents’ previous assessments and conduct re-evaluations, in which each agent is prompted to stick with or change their original evaluation result. Specifically, given $\mathcal{A}_{d-1}(d\geq 1)$, which represents the evaluation results and justifications of agents after $(d-1)^{th}$ rounds of discussions, we conduct the $d^{th}$ round of discussion:

|  | $\mathcal{A}_{d}=[A_{1}(c,p_{i},r_{1,i},r_{2,i},\mathcal{A}_{d-1}),\ldots,\\ A_{m}(c,p_{i},r_{1,i},r_{2,i},\mathcal{A}_{d-1})]$ |  | (6) |

where similarly to $\mathcal{A}_{0}$,

|  | $\mathcal{A}_{d}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$ |  | (7) |

The detailed prompt template for meta-evaluation can be found in Table [6](https://arxiv.org/html/2401.16788v1#A1.T6 "Table 6 ‣ Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate") under Appendix.

In cases where agents fail to reach a consensus after $d=D-1$ rounds of discussions, a human evaluator intervenes. The human evaluator reviews the assessment reports provided by the agents and makes a final decision. Through this process, we incorporate an element of human oversight, thereby increasing the reliability of the final decision. This approach strikes a balance between efficiency and the need for human judgment, ensuring that evaluations are done in a timely and accurate manner. An example of the multi-agent debate process during meta-evaluation is demonstrated in Fig. [2](https://arxiv.org/html/2401.16788v1#S4.F2 "Figure 2 ‣ 4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").

![Refer to caption](img/b0599204bb08edc76b3a6e97c3fe7f0c.png)

Figure 2: An example of the multi-agent debate process during meta-evaluation.

### 4.2 Evaluation Framework

We follow the pairwise response comparison setting outlined in §[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). Note that in the LLM era, the automatic evaluation metric $E$ is often instantiated through single LLMs Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10)); Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)); Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Wang et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)), or multi-agent debate Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)); Li et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib18)). In ScaleEval, we focus on instantiating $E$ through single LLMs (e.g., gpt-3.5-turbo). However, it is important to note that our framework can be further generalized to other instantiations of $E$.

### 4.3 Human Expert Meta-Meta Evaluation

To test the reliability of our proposed meta-evaluation framework, we apply meta-meta evaluation. The meta-meta evaluation process also follows the meta-evaluation process described in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"), where $E$ is instantiated as the agent-debated assisted protocol as described in §[4.1](https://arxiv.org/html/2401.16788v1#S4.SS1 "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"), and $G$ is instantiated as the human expert annotation protocol.

## 5 Examined Scenarios

Establishing real-life scenarios that reflect individuals’ daily usage is key to assess the performance and limitations of LLMs in a comprehensive manner. In the current instantiation of ScaleEval, we include 8 different scenarios that are closely related to everyday situations and tasks Liang et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib20)); Li et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib17)). Some example prompts for each defined scenario is shown in Table [2](https://arxiv.org/html/2401.16788v1#S5.T2 "Table 2 ‣ 5 Examined Scenarios ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). We describe more about exactly how we collect data for each of these scenarios below. Individuals interested in evaluating LLMs with our framework can supplement their assessment with additional scenarios.

 | Scenario | Examples |
| --- | --- |
| Brainstorming | - Can you tell me how to make chocolate chip cookies? |
| - Make a list of snacks and foods to serve as party snacks on a game day! |
| Coding | - What is the difference between HTML and JavaScript? |
| - Implement a binary search algorithm to find a specific element in a sorted array. |
| Dialog | - Act as the Norse Goddess Freyja. |
| - Can you think and feel like a human? |
| Judgement | - What if the Aztecs had successfully repelled the Spanish conquistadors? |
| - How can you determine if a person is genuinely interested in a conversation or simply being polite? |
| Math | - Given that f(x) = 5$x^{3}$ - 2$x$ + 3, find the value of f(2). |
| - If the endpoints of a line segment are (2, -2) and (10, 4), what is the length of the segment? |
| ODG | - Is there a meaning for Christmas wreaths? |
| - What are some of the best universities for studying robotics? |
| ODS | - What causes the northern lights? |
| - What do the different octane values of gasoline mean? |
| Writing | - Can you help me write a formal email to a potential business partner proposing a joint venture? |
| - Take MLK speech "I had a dream" but turn it into a top 100 rap song. | 

Table 2: Examined scenarios and corresponding selected examples.

#### Brainstorming

The brainstorming scenario is designed to test the LLMs’ ability to engage in problem-solving, creative ideation, and generation of insightful responses, especially in situations that require critical thinking and detailed, step-by-step reasoning.

#### Coding

The code scenario evaluates LLMs’ ability to comprehend, produce, and debug code, as well as answering coding-related questions.

#### Dialog

The dialog scenario measures LLMs’ ability to engage with users in a manner that is intuitive, human-like, and dynamic, testing their proficiency through context-sensitive conversations and role-playing that require maintaining a consistent persona throughout a series of interactions.

#### Judgement

The judgement scenario assesses LLMs‘ ability to make inferences and formulate opinions, including soliciting insights on diverse situations or emotions, and posing questions that require logical thinking or reasoning.

#### Math

The math scenario evaluates the LLMs’ proficiency in understanding and solving mathematical problems, emphasizing their accuracy in tasks ranging from simple calculations to complex reasoning.

#### Open-Domain General (ODG)

The ODG scenario measures LLMs’ proficiency in applying diverse knowledge and exercising reasoning across a wide array of topics, such as answering questions with definitive answers.

#### Open-Domain Science (ODS)

The ODS scenario tests the LLMs’ application of scientific knowledge, and gauges their ability to accurately interpret and respond to queries related to scientific disciplines like biology, chemistry, physics, astronomy, and more.

#### Writing

The writing scenario evaluates LLMs’ ability to summarize, translate, and generate various texts, testing their core language processing and production skills.

## 6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate

In this section, we first perform meta-meta-evaluation, examining whether the meta-evaluation results of using ScaleEval match closely to those resulting from meta-evaluation using human evaluators.

#### Setup

For our ScaleEval meta-evaluation framework (as described in §[4.1](https://arxiv.org/html/2401.16788v1#S4.SS1 "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")), we deploy three LLM agents to perform multi-agent debate: gpt-4-turbo, claude-2, and gpt-3.5-turbo.¹¹1Results collected in December 2023\. Specific models used are: gpt-4-1106-preview, claude-2, and gpt-3.5-turbo-1106. In our meta-evaluation experiment, we analyze a total of 160 prompts. This set is comprised 137 prompts from AlpacaEval Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)), 10 coding problem prompts from HumanEval Chen et al. ([2021a](https://arxiv.org/html/2401.16788v1#bib.bib5)), and 13 math problem prompts from GSM-Hard Gao et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib11)). We categorize these prompts into four distinct scenarios: brainstorming, coding, math, and writing, where each scenario contains 40 prompts.

Each scenario is evaluated based on the following criteria, respectively: helpfulness, interpretability, reasoning, and creativity. We evaluate the generated responses from the following three LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro. We select the above LLMs to evaluate due to their rather similar performances according to past research and public user feedback, which can help us establish a more nuanced understanding of their performance in various real-world scenarios, and to identify specific contexts where one may outperform the others.

Our meta-meta evaluation involves having human experts annotate which LLM submission they think is better based on a defined criterion during pairwise comparisons. A total of seven human experts were selected from a pool of Carnegie Mellon University students who have the relevant expertise in answering the queries in each scenario. Different groups of three human experts are responsible for answering the prompts in each scenario, where they are assigned to the scenario that relates to their expertise. Each expert received identical instructions for the task – they were asked to decide which submission is better based on our defined criteria, and for each comparison, label either 0 (neither submission is better), 1 (submission 1 is better), or 2 (submission 2 is better). The label 2 corresponds to the label -1 as denoted in section [3.2](https://arxiv.org/html/2401.16788v1#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). The experts were tasked to conduct 30 comparisons for each of the four different scenarios (brainstorming, coding, math, and writing), based on their corresponding defined criteria (helpfulness, interpretability, reasoning, and creativity). This results in a total of 120 final judgements. The question prompts, LLM responses, and criteria utilized for human expert annotations were consistent with those used during our meta-evaluation experiment. All the details were presented in a google sheet that allowed experts to record their answers.

#### Q1: Can LLM agents with multi-agent debate be used as meta-evaluators in new user-defined scenarios?

To validate the reliability of ScaleEval’s meta-evaluation framework, we perform comparisons between the results from human experts and ScaleEval’s multi-agent debate by two key metrics: the example-level agreement rate and the system-level agreement rate, as mentioned in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). The example-level agreement rate measures the proportion of instances where the multi-agent debate results correspond with the human experts judgements. On the other hand, the system-level agreement rate assesses whether the human experts and multi-agents concur in their overall evaluation of which LLMs produce the best responses for each scenario. A high agreement rate in both metrics would suggest a strong reliability and validity of our meta-evaluation framework, indicating that both human and LLM agents consistently recognize and agree on the quality of responses generated by LLMs.

#### Results

From Table [3](https://arxiv.org/html/2401.16788v1#S6.T3 "Table 3 ‣ Results ‣ 6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we generally observe a higher example-level agreement rate between human experts and ScaleEval, compared to the agreement rate between human experts and individual LLM evaluations. The consistently high agreement rates observed suggest that our meta-evaluation framework aligns well with human expert judgments in these areas, indicating a reliable performance of the collective use of LLMs in meta-evaluating complex scenarios. Across all LLM submission comparisons in our experiment, we observe higher agreement rates in decisions between ScaleEval outcomes and those of human experts, particularly in coding and math scenarios. This observed trend could be attributed to the inherently objective nature of these subjects, which have relatively clear, definitive answers unlike more subjective areas like creative writing.

Based on Fig. [3](https://arxiv.org/html/2401.16788v1#S6.F3 "Figure 3 ‣ Results ‣ 6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we notice a consistent "preference in the same direction" between human experts and multi-agent debates across all LLM pairwise comparisons and scenarios. Notably, gpt-3.5-turbo is favored (higher win rates) in brainstorming, math, and writing scenarios when compared with claude-instant. Similarly, gemini-pro is also preferred over claude-instant in all scenarios. When comparing gpt-3.5-turbo with gemini-pro, a varied pattern in decision outcomes is observed: both human experts and multi-agent systems agree that gpt-3.5-turbo outperforms gemini-pro in scenarios involving math and writing. Conversely, gemini-pro is deemed superior in brainstorming and coding scenarios. The high agreement of multi-agent preferences with human expert judgement results verifies the reliability of using multiple LLMs agents as meta-evaluators in various user-defined scenarios.

| LLM Pairwise Comparisons | Criterion | Scenario | Meta-Evaluation | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo |
| --- | --- | --- | --- | --- | --- | --- |
| GPT-3.5-Turbo vs. Claude-Instant | Helpfulness | Brainstorming | 0.600 | 0.633 | 0.433 | 0.267 |
|  | Interpretability | Coding | 0.733 | 0.700 | 0.533 | 0.567 |
|  | Reasoning | Math | 0.867 | 0.600 | 0.400 | 0.367 |
|  | Creativity | Writing | 0.700 | 0.667 | 0.400 | 0.333 |
| Claude-Instant vs. Gemini-Pro | Helpfulness | Brainstorming | 0.667 | 0.533 | 0.467 | 0.500 |
|  | Interpretability | Coding | 0.833 | 0.600 | 0.500 | 0.567 |
|  | Reasoning | Math | 0.767 | 0.667 | 0.330 | 0.367 |
|  | Creativity | Writing | 0.733 | 0.633 | 0.400 | 0.500 |
| GPT-3.5-Turbo vs. Gemini-Pro | Helpfulness | Brainstorming | 0.733 | 0.600 | 0.467 | 0.467 |
|  | Interpretability | Coding | 0.833 | 0.733 | 0.567 | 0.667 |
|  | Reasoning | Math | 0.867 | 0.767 | 0.500 | 0.433 |
|  | Creativity | Writing | 0.767 | 0.667 | 0.500 | 0.433 |

Table 3: Example-level agreement rate comparison between human expert and ScaleEval’s meta-evaluation vs. human expert and single LLM evaluation across four scenarios and criteria.

![Refer to caption](img/d917865a84db39b38b00a0e85da6c462.png)

(a) GPT-3.5-Turbo vs. Claude-Instant

![Refer to caption](img/25d9b406729d7c32ca2dfd0ea7b15afc.png)

(b) Claude-Instant vs. Gemini-Pro

![Refer to caption](img/0b68ddd2906ef37fb2203868a1bc4548.png)

(c) GPT-3.5-Turbo vs. Gemini-Pro

Figure 3: System-level agreement – win rates for each LLM pairwise comparison. Left bars in each scenario represent human expert results; right bars represent ScaleEval’s meta-evaluation results.

![Refer to caption](img/491218262822d0859c53ca32eae59b62.png)

Figure 4: Human Fleiss Kappa for each LLM pairwise comparison under four scenarios.

## 7 Exp-II: Meta-Evaluation vs. LLM Evaluators

Next, we use the fact that ScaleEval allows for reliable and scalable meta-evaluation to examine the traits of LLMs as evaluators.

#### Q2: What are the capabilities and limitations of each LLM evaluator?

To effectively evaluate the performance of each LLM in its role as an evaluator, we adopt an approach that involves comparing the outcomes from our meta-evaluation process with the evaluations made independently by each LLM evaluator, which uncovers any disagreements or alignments between them. In the process, we aim to shed light on the performance characteristics of each LLM evaluator, which helps us identify which of them demonstrate superior evaluative abilities, thereby contributing to our understanding of their reliability in evaluating responses under each scenario. In addition, we provide a comprehensive cost-performance analysis to decide which LLM evaluator is the most suitable choice in each scenario.

#### Setup

For meta-evaluation, we employed three LLMs (gpt-4-turbo, claude-2, and gpt-3.5-turbo) as evaluators to perform pairwise comparisons of responses from three distinct LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro. Previous studies have highlighted the presence of positional biases when LLMs are used as evaluators Wang et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib24)). In response to these findings, we have implemented a strategy of randomization to mitigate such biases. Specifically, the sequence in which submissions from LLMs are presented to the agent evaluators is randomized. Additionally, we also randomize the order of discussions for each agent evaluator in every case. These approaches ensure that the process is fair and unbiased as much as possible, allowing for a more accurate assessment of the LLM evaluators’ performance. The meta-evaluations were done under the following 8 scenarios: brainstorming, coding, dialog, judgement, open-domain general, open-domain science, and writing, with the same set of 4 criteria used during human expert annotation.

#### Results

Table [4](https://arxiv.org/html/2401.16788v1#S7.T4 "Table 4 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate") compares the agreement rate between ScaleEval’s meta-evaluation and each LLM evaluator across criteria and scenarios. We observe that gpt-4-turbo, when serving as an evaluator, has the highest agreement rates with our meta-evaluation, particularly in the scenarios of brainstorming, dialog, and ODG with the helpfulness criterion. It stands out with the highest overall average score of 0.780\. However, our selected open-source model evaluator, auto-j, outperforms gpt-4-turbo in evaluating coding questions based on the helpfulness criterion. In addition, it exhibits the highest agreement rate with our meta-evaluation in the judgement scenario, according to the helpfulness criterion, indicating it as the most capable evaluator in this setting. It also achieves comparable results with other closed-source models like claude-2 and gpt-3.5-turbo in most of the other scenarios.

While gpt-4-turbo performs the best as an evaluator in a majority of scenarios, it is not necessarily the best choice when we take into consideration its relatively high API costs. In fact, both the more affordable version (gpt-3.5-turbo) and our selected free, open-source model (auto-j) show comparable performance in scenarios like judgement and writing. For coding-related evaluations, the slightly less expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo.

| Criterion | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo | Auto-J |
| Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 | 0.575 |
|  | Coding | 0.600 | 0.725 | 0.675 | 0.675 |
|  | Dialog | 0.800 | 0.700 | 0.700 | 0.625 |
|  | Judgement | 0.725 | 0.625 | 0.725 | 0.750 |
|  | Math | 0.825 | 0.650 | 0.600 | 0.350 |
|  | ODG | 0.850 | 0.525 | 0.575 | 0.700 |
|  | ODS | 0.875 | 0.525 | 0.575 | 0.675 |
|  | Writing | 0.750 | 0.600 | 0.750 | 0.600 |
| Interpretability | Coding | 0.825 | 0.600 | 0.550 | 0.525 |
| Reasoning | Math | 0.650 | 0.525 | 0.475 | 0.450 |
|  | Judgement | 0.750 | 0.650 | 0.700 | 0.675 |
| Creativity | Writing | 0.775 | 0.600 | 0.575 | 0.650 |
|  | Brainstorming | 0.800 | 0.525 | 0.550 | 0.625 |
|  | Dialog | 0.875 | 0.750 | 0.700 | 0.800 |
| Average | Overall | 0.780 | 0.607 | 0.629 | 0.619 |

Table 4: Agreement rate between ScaleEval’s meta-evaluation and each LLM evaluator for comparing GPT3.5-Turbo vs. Claude-Instant.

| Criteria Format | Criteria | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo |
| --- | --- | --- | --- | --- | --- |
| General | Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 |
|  | Interpretability | Coding | 0.825 | 0.600 | 0.550 |
|  | Reasoning | Math | 0.650 | 0.525 | 0.475 |
|  | Creativity | Writing | 0.800 | 0.600 | 0.575 |
| Shortened | Helpfulness | Brainstorming | 0.675 | 0.500 | 0.575 |
|  | Interpretability | Coding | 0.675 | 0.325 | 0.425 |
|  | Reasoning | Math | 0.625 | 0.425 | 0.400 |
|  | Creativity | Writing | 0.675 | 0.250 | 0.525 |
| Gibberish | Helpfulness | Brainstorming | 0.575 | 0.450 | 0.575 |
|  | Interpretability | Coding | 0.700 | 0.275 | 0.525 |
|  | Reasoning | Math | 0.650 | 0.200 | 0.400 |
|  | Creativity | Writing | 0.550 | 0.150 | 0.450 |
| Shuffled | Helpfulness | Brainstorming | 0.625 | 0.550 | 0.500 |
|  | Interpretability | Coding | 0.600 | 0.400 | 0.525 |
|  | Reasoning | Math | 0.625 | 0.225 | 0.600 |
|  | Creativity | Writing | 0.625 | 0.275 | 0.500 |
| Flipped | Helpfulness | Brainstorming | 0.725 | 0.325 | 0.550 |
|  | Interpretability | Coding | 0.725 | 0.425 | 0.300 |
|  | Reasoning | Math | 0.575 | 0.250 | 0.500 |
|  | Creativity | Writing | 0.750 | 0.075 | 0.550 |
| Masked | Helpfulness | Brainstorming | 0.725 | 0.300 | 0.500 |
|  | Interpretability | Coding | 0.650 | 0.225 | 0.475 |
|  | Reasoning | Math | 0.575 | 0.150 | 0.375 |
|  | Creativity | Writing | 0.575 | 0.200 | 0.400 |

Table 5: Agreement rate between ScaleEval’s meta-evaluation results and each LLM evaluator under various criteria prompt formats and scenarios comparing GPT3.5-Turbo vs. Claude-Instant.

## 8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations

#### Q3: How do the qualities of criteria prompts influence the robustness of LLMs as evaluators in different scenarios?

Prior studies have revealed that variations in prompts can substantially affect the behavior of LLMs, particularly with the text they generate. With this in mind, we define various formatted criteria for evaluating LLM responses under each scenario. This approach aims to examine the extent to which different formats of criteria prompts influence both the performance and robustness of LLMs as evaluators.

#### Setup

We define five variations of the same criteria prompts: shortened, gibberish, shuffled, flipped, and masked (see Table [7](https://arxiv.org/html/2401.16788v1#A1.T7 "Table 7 ‣ Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate") under Appendix [A](https://arxiv.org/html/2401.16788v1#A1 "Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate") for detailed format). With these criteria format variations, we intend to observe how the LLMs as evaluators would respond differently when conducting evaluation. We compare the example-level agreement rate between ScaleEval’s meta-evaluation results and each LLM evaluator.

#### Results

Based on Table [5](https://arxiv.org/html/2401.16788v1#S7.T5 "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we observe that the performance of LLMs as evaluators generally deteriorates when certain letters in the criteria prompts are masked. Furthermore, the removal of guiding phrases at the beginning, such as "Not Helpful" or "Highly Helpful", can also diminish their effectiveness as evaluators. Both gpt-4-turbo and gpt-3.5-turbo demonstrate some resilience to these adversarially formatted criteria prompts, maintaining a relatively consistent agreement rates across various criteria formats. In contrast, Claude-2 often showcases confusion and refuses to evaluate, particularly in cases with gibberish and masked criteria prompts, where it rejects answering about half of the questions. It typically responds with statements like, "Unfortunately I do not have enough information here to provide a fair evaluation… The criteria describe different quality levels, but there is no detail on what specific aspects of the responses should be assessed… any judgement risks being arbitrary or biased…". None of the LLMs as evaluators we tested maintained very similar evaluation capabilities when faced with these adversarially formatted criteria prompts, indicating a limitation in these LLMs as evaluators’ current design and application. Despite their advanced capabilities in fulfilling a variety of tasks, they may still struggle with understanding and responding accurately to substituted criteria information, highlighting an area for potential improvement in future iterations of LLM technology. Among all the different formatted criteria, we highlight the cases where the LLMs perform the best as evaluators in Table [5](https://arxiv.org/html/2401.16788v1#S7.T5 "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").

## 9 Conclusion

In this work, we propose ScaleEval, a scalable, agent-debate assisted meta-evaluation framework for assessing the reliability and robustness of LLMs as evaluators. This approach addresses the expensive and time-intensive challenges inherent in traditional meta-evaluation methods, particularly pertinent as the usage of LLMs expands, necessitating a more scalable solution. Through our research, we have not only demonstrated the reliability of our proposed meta-evaluation framework, but also shed light on the capabilities and limitations of LLMs as evaluators in various scenarios. We observe how the results from these LLMs as evaluators vary based on modifications to the same criteria prompts. By open-sourcing our framework, we aim to foster further research in this field and encourage the development of more advanced and reliable LLMs as evaluators in the future.

## Acknowledgements

We thank Chunting Zhou, Weizhe Yuan, Chunpu Xu, Yan Ma, and Binjie Wang for the helpful discussions and feedback.

## References

*   Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale. Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. *arXiv:2302.04023v3*.
*   Bhandari et al. (2020) Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. *arXiv preprint arXiv:2010.07100*.
*   Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. *arXiv preprint arXiv:2303.12712*.
*   Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. *arXiv preprint arXiv:2308.07201*.
*   Chen et al. (2021a) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021a. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
*   Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021b. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.
*   Chiang and Lee (2023) Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*.
*   Dang et al. (2008) Hoa Trang Dang, Karolina Owczarzak, et al. 2008. Overview of the tac 2008 update summarization task. In *TAC*.
*   Freitag et al. (2022) Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André FT Martins. 2022. Results of wmt22 metrics shared task: Stop using bleu–neural metrics are better and more robust. In *Proceedings of the Seventh Conference on Machine Translation (WMT)*, pages 46–68.
*   Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.
*   Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. *arXiv preprint arXiv:2211.10435*.
*   Gemini Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*.
*   Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. *arXiv preprint arXiv:2009.03300*.
*   Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
*   Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin. Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization. *arXiv:2003.11080v5*.
*   Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. *arXiv preprint arXiv:2310.01798*.
*   Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint arXiv:2310.05470*.
*   Li et al. (2023b) Ruosen Li, Teerth Patel, and Xinya Du. 2023b. Prd: Peer rank and discussion improve large language model based evaluations. *arXiv preprint arXiv:2307.02762*.
*   Li et al. (2023c) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023c. Alpacaeval: An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).
*   Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
*   Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems*, 35:27730–27744.
*   Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.
*   Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. *arXiv preprint arXiv:2303.04048*.
*   Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. *ArXiv*, abs/2305.17926.
*   Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*.
*   Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan. Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. *arXiv:2304.06364v2*.

## Appendix A Meta-Evaluation Prompt

| <Initial Evaluation> |
| Compare the two submissions based on the criteria above. Which one is better? First, provide a step-by-step explanation of your evaluation reasoning according to the criteria. Avoid any potential bias. Ensure that the order in which the submissions were presented does not affect your judgement. Keep your explanation strictly under 150 words. Afterwards, choose one of the following options: |
| Submission 1 is better: "1" |
| Submission 2 is better: "2" |
| Neither is better: "0" |
| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds to your reasoning. At the end, repeat just the number again by itself on a new line. |
| [Question]: {question} |
| [Submission 1]: {submission_1} |
| [Submission 2]: {submission_2} |
| [Criteria]: {criteria} |
| [User]: {user_prompt} |
| You are evaluating two submissions for a particular question, using a specific set of criteria. Above is the data. |
| <Discussion Rounds> |
| Always remember you are Speaker 1/2/3. Review again your own previous evaluations/discussions first, then answer user’s request from Speaker 1/2/3’s perspective. |
| [Question]: {question} |
| [Submission 1]: {submission_1} |
| [Submission 2]: {submission_2} |
| [Criteria]: {criteria} |
| [Speaker 1’s Initial Evaluation]: {evaluation_1} |
| [Speaker 2’s Initial Evaluation]: {evaluation_2} |
| [Speaker 3’s Initial Evaluation]: {evaluation_3} |
| [Speaker {speaker_number}’s Discussion -- Round {round_number}]: {discussion_reasoning} |
| ... |
| Read the question, submissions, criteria, and evaluations above. First, explain your thoughts step-by-step about other speakers’ evaluations. Second, explain your reasoning step-by-step regarding whether or not to change your original answer about which submission you think is better after considering other speakers’ perspectives. Keep your reasoning strictly under 150 words. Afterwards, choose one of the following options: |
| Submission 1 is better: "1" |
| Submission 2 is better: "2" |
| Neither is better: "0" |
| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds to your reasoning. At the end, repeat just the number again by itself on a new line. |

Table 6: Prompt template for meta-evaluation via multi-agent debate

| <Type 1: General Format Version> |
| "1": "Not Helpful - The response is completely unrelated, lacks coherence, and fails to provide any meaningful information." |
| "2": "Somewhat Helpful - The response bears some relevance but remains largely superficial and unclear, addressing only the peripheral aspects of the user’s needs." |
| "3": "Moderately Helpful - The response is mostly relevant and clear, covering the basic aspects of the query, but lacks depth and comprehensive elucidation." |
| "4": "Helpful - The response is on-point, detailed, and well-articulated, offering valuable information and clarifications that meet the user’s primary needs and enhance understanding." |
| "5": "Highly Helpful - The response is exceptionally thorough and precise, providing additional insights and valuable supplementary information." |
| <Type 2: Shortened Format Version> |
| "1": "The response is completely unrelated, lacks coherence, and fails to provide any meaningful information." |
| "2": "The response bears some relevance but remains largely superficial and unclear, addressing only the peripheral aspects of the user’s needs." |
| "3": "The response is mostly relevant and clear, covering the basic aspects of the query, but lacks depth and comprehensive elucidation." |
| "4": "The response is on-point, detailed, and well-articulated, offering valuable information and clarifications that meet the user’s primary needs and enhance understanding." |
| "5": "The response is exceptionally thorough and precise, providing additional insights and valuable supplementary information." |
| <Type 3: Gibberish Format Version> |
| "1": "N*t H$l%ful - Th$ r$sp0n$e is c mplt$l? unr€la7$d, la$ks c()h$r$n(€, and f#i/s t# p$o&id$ any m€an*&gful !format$on." |
| "2": "S#m$*ha+ H$%*fu/ - Th$ r#s0!n$ b%ars $o/e re$ev*nc$ b$t r$ma$n$ l#rg$l4 $u/7$r7cial an* !ncl=4r, a6r$ss@n4 o7ly th$ p$r4ph@r$l a5p$cts #f th$ $s*r’s n**ds." |
| "3": "M$!7r$t#ly H$lpfu& - Th$ r@s0*n$@ !s m$%stl€ r$’$van7 an cl$ar, c$%$r$n4 th$ ba$!c a$%cts of th$ qu€ry, b$t l#cks d$pth an cmpr$h$ns$v$ lu$7$dat!on." |
| "4": "H$lpfu& - Th$ r!s0*n$e !s o/7-p$!nt, d$ta$!l$d, an w$l/-a&!u/at$d, #ff$r!n4 v#l$%bl$ #nformat$on and cl*r$!cat!ons th#t m=t th$ u/7$rś pr!/ary n$$ds an* @n7anc$ un#rstand!n4." |
| "5": "H4#h7y H$!p%u& - Th$ r$s&*n!e !s $xc$pt$#nally th#r#7gh an* pr$c$%$, pr#v$d$n# a4*!t$#nal !ns$4hts an* v#lu%bl$ @*pp%$%ntary #n%ormat$on." |
| <Type 4: Shuffled Format Version> |
| "1": "coherence fails provide unrelated, completely response - and the meaningful any to lacks Not Helpful is The information." |
| "2": "superficial response largely addressing unclear, remains only needs. - relevance user’s and the Helpful the peripheral some bears but aspects Somewhat The of" |
| "3": "basic aspects query, lacks Moderately covering clear, - Helpful is depth response and comprehensive elucidation. relevant mostly the The and the of but" |
| "4": "clarifications the is response information needs enhance and Helpful - on-point, valuable well-articulated, offering understanding. The and detailed, primary that user’s meet" |
| "5": "valuable Highly response is providing - the exceptionally Helpful information. insights thorough and additional precise, supplementary and The" |
| <Type 5: Flipped Format Version> |
| "1": "toN lufpleH - ehT esnopser si yletelpmoc detalernu, skcal ecnerehoc, dna sliaf ot edivorp yna lufgninaem noitamrofni." |
| "2": "tamewoS lufpleH - ehT esnopser sraeb emos ecnaveler tub sniamer ylegral laicifrepus dna raelcnu, gnisserdda ylno eht larehpirep stcepsa fo eht s’resu sdeen." |
| "3": "yletaredoM lufpleH - ehT esnopser si yltsom tnaveler dna raelc, gnirevoc eht cisab stcepsa fo eht yreuq, tub skcal htped dna evisneherpmoc noitadicule." |
| "4": "lufpleH - ehT esnopser si tniop-no, deliated, dna detalucitra-llew, gnireffo elbaulav noitamrofni dna snoitacifralc taht teem eht s’resu yramirp sdeen dna ecnahne gnidnatsrednu." |
| "5": "ylhgiH lufpleH - ehT esnopser si yllanoitpecxe hguoroht dna esicerp, gnidivorp lanoitidda sthgisni dna elbaulav yratnemelppus noitamrofni." |
| <Type 6: Masked Format Version> |
| "1": "N__ H_l_ful - The r__pnse is c_m__et__y unr_l_te_, lacks _ohe_en_e, _nd _ai_s to p_ov_de _ny m_a__ngfu_ _nfo_ma_ion." |
| "2": "_om_w_at He_p_ul - T_e re_ponse be_rs _ome rel__a_ce but r__ains la__ely s__erfi__al and u_cle__, ad_res__ng onl_ _he __ri__er_l a_pe_ts of t__ u_e_’s ne_ds." |
| "3": "Mod___tely _elp__l - Th_ _esp__se is mos__y re__va_t an_ _le_r, c_v__ing the ba_ic _spe_ts of the q_e_y, but __cks _e_th and co_preh_ns_ve el_c_d_t_on." |
| "4": "__lpful - _he respo_se is on-p_in_, d___iled, and we_l-ar_icu_ated, of_er_ng val_ab_e __for_ation and cl_r_fi__t_ons t_at mee_ the _se_’s p_im_r_ _eeds and en__nce u_de__tan_ing." |
| "5": "Hi_h_y H__p_ul - The r_spon_e is e_c_p_io_al__ th_r_ugh and p_ec_se, pr_vi_ing a_di__on_l ins_g_ts and va_u_b_e _upp_e_en_a_y inf_rma_io_." |

Table 7: Criteria prompt format variations for helpfulness