- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:11'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:11
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SceneCraft：一个将 3D 场景合成到 Blender 代码的 LLM 代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01248](https://ar5iv.labs.arxiv.org/html/2403.01248)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01248](https://ar5iv.labs.arxiv.org/html/2403.01248)
- en: Ziniu Hu    Ahmet Iscen    Aashi Jain    Thomas Kipf    Yisong Yue    David
    A. Ross    Cordelia Schmid    Alireza Fathi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 祖牛·胡    艾哈迈德·伊斯岑    阿什·贾因    托马斯·基普夫    易松·岳    大卫·A·罗斯    科尔德莉亚·施密德    阿利雷扎·法提
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting
    text descriptions into Blender-executable Python scripts which render complex
    scenes with up to a hundred 3D assets. This process requires complex spatial planning
    and arrangement. We tackle these challenges through a combination of advanced
    abstraction, strategic planning, and library learning. SceneCraft first models
    a scene graph as a blueprint, detailing the spatial relationships among assets
    in the scene. SceneCraft then writes Python scripts based on this graph, translating
    relationships into numerical constraints for asset layout. Next, SceneCraft leverages
    the perceptual strengths of vision-language foundation models like GPT-V to analyze
    rendered images and iteratively refine the scene. On top of this process, SceneCraft
    features a library learning mechanism that compiles common script functions into
    a reusable library, facilitating continuous self-improvement without expensive
    LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing
    LLM-based agents in rendering complex scenes, as shown by its adherence to constraints
    and favorable human assessments. We also showcase the broader application potential
    of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding
    a video generative model with generated scenes as intermediary control signal.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 SceneCraft，一种大型语言模型（LLM）代理，能够将文本描述转换为 Blender 可执行的 Python 脚本，从而渲染包含多达一百个
    3D 资产的复杂场景。这个过程需要复杂的空间规划和安排。我们通过高级抽象、战略规划和库学习的结合来应对这些挑战。SceneCraft 首先将场景图建模为蓝图，详细描述了场景中资产之间的空间关系。然后，SceneCraft
    基于这个图编写 Python 脚本，将关系转换为资产布局的数值约束。接下来，SceneCraft 利用 GPT-V 等视觉语言基础模型的感知优势来分析渲染的图像，并迭代地优化场景。在此基础上，SceneCraft
    具有一个库学习机制，将常见的脚本功能编译成可重用的库，从而实现持续自我改进，而无需昂贵的 LLM 参数调整。我们的评估表明，SceneCraft 在渲染复杂场景方面超越了现有的基于
    LLM 的代理，表现为其对约束的遵守和有利的人类评估。我们还展示了 SceneCraft 的更广泛应用潜力，通过从 Sintel 电影中重建详细的 3D 场景，并以生成的场景作为中介控制信号来引导视频生成模型。
- en: 'Machine Learning, ICML¹¹1^*Correspondence to: Ziniu Hu $<$.![Refer to caption](img/9e88897b3e4f25c043c75a947471a3fa.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML¹¹1^*通讯作者：祖牛·胡 $<$.![参考说明](img/9e88897b3e4f25c043c75a947471a3fa.png)
- en: 'Figure 1: Examples comparing SceneCraft’s output against a BlenderGPT baseline
    for different queries.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：比较 SceneCraft 输出与 BlenderGPT 基线对于不同查询的示例。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transforming natural language descriptions into 3D scenes is a key technology
    for industries like architectural design, game development, virtual reality, and
    cinematic production. Recent 3D generative models like DreamFusion (Poole et al.,
    [2022](#bib.bib19)) and Magic3D (Lin et al., [2023](#bib.bib11)) have made great
    progress in transforming text to a 3D neural representation of an object. However,
    these works fall short of composing entire scenes with multiple assets due to
    dataset scale limitations and domain specificity. In this work, we are inspired
    by how human artists typically adopt a holistic process for designing 3D scenes,
    where they take an iterative, step-by-step approach that includes storyboarding,
    3D modeling, texturing, rigging, layout, animation, and rendering, using professional
    software such as Blender²²2[https://www.blender.org/](https://www.blender.org/).
    This iterative process grants the artists in studios a nuanced control over each
    asset’s placement and movement — a level of control not yet achieved by existing
    models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将自然语言描述转化为3D场景是建筑设计、游戏开发、虚拟现实和电影制作等行业的关键技术。最近的3D生成模型如DreamFusion (Poole et al.,
    [2022](#bib.bib19))和Magic3D (Lin et al., [2023](#bib.bib11))在将文本转化为对象的3D神经表示方面取得了巨大进展。然而，由于数据集规模限制和领域特异性，这些工作在构建包含多个资产的完整场景方面仍有所不足。在这项工作中，我们受到人类艺术家通常采用整体过程设计3D场景的启发，他们采取迭代的逐步方法，包括故事板制作、3D建模、纹理处理、绑定、布局、动画和渲染，使用专业软件如Blender²²2[https://www.blender.org/](https://www.blender.org/)。这一迭代过程赋予了工作室中的艺术家对每个资产的放置和移动具有微妙的控制——这是现有模型尚未实现的控制水平。
- en: Our paper introduces SceneCraft, an LLM-powered agent that is designed to streamline
    this text-to-3D scene conversion process, closely emulating the workflow of studio
    artists. SceneCraft transforms textual descriptions into executable Blender code,
    rendering 3D scenes that are visually cohesive and contextually accurate. This
    task goes beyond mere data processing, demanding a nuanced understanding of spatial
    and semantic relationships, which remains a challenge even for today’s LLMs. While
    earlier systems like WordsEye (Coyne & Sproat, [2001](#bib.bib7)) and SceneSere (Chang
    et al., [2017](#bib.bib5)) have made progress towards using predefined templates
    and rules to extract spatial constraints from linguistic queries, they depend
    on extensive human input, especially in new domains with unique relationship patterns.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文介绍了SceneCraft，这是一种由LLM驱动的代理，旨在简化文本到3D场景的转换过程，紧密模拟工作室艺术家的工作流程。SceneCraft将文本描述转化为可执行的Blender代码，渲染出视觉上连贯且语境准确的3D场景。这项任务超越了单纯的数据处理，需要对空间和语义关系的深刻理解，这仍然是今天的LLMs面临的挑战。虽然早期系统如WordsEye (Coyne
    & Sproat, [2001](#bib.bib7))和SceneSere (Chang et al., [2017](#bib.bib5))在使用预定义模板和规则从语言查询中提取空间约束方面取得了一些进展，但它们仍依赖于大量的人为输入，特别是在具有独特关系模式的新领域中。
- en: '![Refer to caption](img/88336c88615dcdf5a7ce899d3b8663a4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/88336c88615dcdf5a7ce899d3b8663a4.png)'
- en: 'Figure 2: SceneCraft is composed of a dual-loop self-improving pipeline: in
    the inner-loop, per each scene, an LLM autonomously writes a script to interact
    with Blender, receives rendered image, and keeps improving the script until getting
    good scenes; in the outer-loop, SceneCraft summarizes common functions over a
    batch of written scripts to maintain a reusable design skill library.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：SceneCraft由一个双循环自我改进的管道组成：在内循环中，每个场景，LLM自动编写一个脚本与Blender交互，接收渲染图像，并不断改进脚本直到获得良好的场景；在外循环中，SceneCraft总结一批编写的脚本中的常见功能，以维护一个可重用的设计技能库。
- en: 'SceneCraft leverages LLMs to autonomously generate Python code, translating
    spatial relations within scenes into precise numerical constraints. To achieve
    this, the core of SceneCraft is a dual-loop optimization pipeline, illustrated
    in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ SceneCraft: An LLM Agent for
    Synthesizing 3D Scene as Blender Code"). The inner-loop focuses on per-scene layout
    optimization. Here, an LLM-based planner constructs a scene graph outlining the
    spatial constraints for asset arrangement. SceneCraft then writes Python code
    to transform these relations into numerical constraints. These constraints are
    fed to a specialized solver that determines the layout parameters of each asset,
    including location, orientation and sizes. After rendering these scripts into
    images via Blender, we utilize a multimodal LLM (GPT-V (OpenAI, [2023](#bib.bib16)))
    to assess the alignment between the generated image and the textual description.
    If a misalignment is detected, the LLM identifies the problematic semantic relations
    and corresponding constraints, subsequently refining the scripts. This iterative
    process of refinement and feedback is crucial for enhancing the scene’s fidelity,
    ensuring each rendition progressively aligns more closely with the original vision,
    which also matches more the human artists’ designing process.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'SceneCraft 利用 LLMs 自主生成 Python 代码，将场景中的空间关系转化为精确的数值约束。为实现这一点，SceneCraft 的核心是一个双循环优化流程，如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") 所示。内循环专注于每个场景的布局优化。在这里，基于 LLM 的规划器构建了一个场景图，概述了资产布置的空间约束。SceneCraft
    然后编写 Python 代码，将这些关系转化为数值约束。这些约束被送到一个专门的求解器，确定每个资产的布局参数，包括位置、方向和尺寸。在通过 Blender
    渲染这些脚本成图像后，我们利用多模态 LLM (GPT-V (OpenAI, [2023](#bib.bib16))) 评估生成图像与文本描述之间的一致性。如果检测到不一致，LLM
    识别出问题的语义关系和相应的约束，并随即改进脚本。这一迭代的改进和反馈过程对提高场景的逼真度至关重要，确保每次渲染逐渐更接近原始构想，这也更符合人类艺术家的设计过程。'
- en: Following the inner-loop refinement of scene scripts, SceneCraft starts its
    outer loop to dynamically expand its ’spatial skill’ library. Within this procedure,
    it reviews the incremental changes made to the constraint scripts across inner-loop
    iterations for each scene, identifying and integrating common code patterns, thereby
    streamlining the acquisition of new non-parametric skills for self-improvement.
    For instance, if the text query describes a lamp placed on a desk, but the initial
    rendering shows desk lamps floating mid-air, SceneCraft may learn to introduce
    a new ”grounded” constraint between lamp and desk surfaces. By continuously updating
    its library through such outer-loop learning over batches, SceneCraft acquires
    an expanding repertoire of spatial skills over time. SceneCraft is therefore able
    to handle increasingly complex scenes and descriptions without external human
    expertise or LLM parameter tuning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在内循环优化场景脚本之后，SceneCraft 开始其外循环，以动态扩展其“空间技能”库。在这个过程中，它审查内循环迭代中对约束脚本的增量更改，识别并整合常见的代码模式，从而简化了新非参数技能的自我改进。例如，如果文本查询描述了一个放置在桌子上的灯，但初始渲染显示桌灯悬浮在空中，SceneCraft
    可能会学习在灯和桌面之间引入新的“接地”约束。通过这种外循环学习不断更新其库，SceneCraft 随着时间的推移获得了越来越丰富的空间技能。因此，SceneCraft
    能够处理越来越复杂的场景和描述，而无需外部人类专业知识或 LLM 参数调优。
- en: To evaluate SceneCraft, we conduct comprehensive experiments on both synthetic
    and real-world datasets. First, we create our own curated datasets with ground-truth
    spatial constraints to quantify SceneCraft’s fidelity in translating text to constraint
    scripts. Second, we apply SceneCraft to the Sintel movie dataset by finetuning
    a video generative model on the first half of the movie conditioned on ground-truth
    scene images. For the second half, we generate scenes using SceneCraft and other
    baselines as input to the video model. Across datasets, results demonstrate SceneCraft’s
    superior sample efficiency and accuracy in rendering intricate 3D scenes from
    textual descriptions, enabled by its dual-loop optimization. Quantitatively, SceneCraft
    achieves over 45.1% and 40.9% improvement on generated scenes’ CLIP score, compared
    with another popular LLM agent baseline BlenderGPT, over both unseen synthetic
    queries and real-world movies like Sintel. SceneCraft also achieves significantly
    better constraint passing score (88.9 against 5.6). Qualitatively, the scenes
    and videos generated using SceneCraft more accurately encapsulate the narrative
    and artistic nuances described in the text. It receives much higher human preference
    ratings on different perspective, and also benefit a video generative model with
    very light fine-tuning. Together, our comprehensive evaluation validates SceneCraft
    as an adaptable and efficient framework for translating imaginative text to 3D
    reality while continuously improving itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估SceneCraft，我们在合成和真实世界数据集上进行了全面的实验。首先，我们创建了自己策划的数据集，并设置真实的空间约束，以量化SceneCraft在将文本转换为约束脚本方面的准确性。其次，我们通过在电影的前半部分对视频生成模型进行微调，并以真实场景图像为条件，应用SceneCraft于Sintel电影数据集。对于后半部分，我们使用SceneCraft和其他基线生成场景作为视频模型的输入。结果表明，SceneCraft在从文本描述中渲染复杂的3D场景方面表现出优越的样本效率和准确性，这得益于其双循环优化。在量化方面，与另一种流行的LLM代理基线BlenderGPT相比，SceneCraft在生成场景的CLIP分数上取得了超过45.1%和40.9%的改进，无论是在未见过的合成查询还是像Sintel这样的真实世界电影中。SceneCraft在约束通过评分方面也表现出显著的优势（88.9对比5.6）。在定性方面，使用SceneCraft生成的场景和视频更准确地体现了文本中描述的叙事和艺术细微差别。它在不同视角上获得了更高的人类偏好评分，并且对视频生成模型的微调非常轻量。综合评估验证了SceneCraft作为一个适应性强且高效的框架，用于将富有想象力的文本转化为3D现实，同时不断自我改进。
- en: 'This paper’s contributions are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文的贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An LLM Agent that transforms an input text query into a 3D scene by generating
    a Blender script. The script is iteratively improved by a multimodal LLM that
    identifies unsatisfied constraints and fixes them in a feedback loop.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个LLM代理，它通过生成Blender脚本将输入文本查询转换为3D场景。该脚本通过多模态LLM进行迭代改进，LLM识别未满足的约束并在反馈循环中修复这些约束。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A spatial skill library learned given a set of synthetic input queries without
    requiring human involvement and LLM fine-tuning, resulting in improved scene generation
    results.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个空间技能库，该库在给定一组合成输入查询的情况下进行学习，不需要人工干预和LLM微调，从而改进了场景生成结果。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results show that comparing with BlenderGPT, another LLM-based
    agent baseline, SceneCraft achieves 45.1% and 40.9% improvement on generated scenes’
    CLIP score, over both unseen synthetic queries and real-world movies like Sintel.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，与另一种基于LLM的代理基线BlenderGPT相比，SceneCraft在生成场景的CLIP分数上取得了45.1%和40.9%的改进，无论是在未见过的合成查询还是像Sintel这样的真实世界电影中。
- en: '![Refer to caption](img/6eff7cf2032f81f7cc3d49c1a520069b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6eff7cf2032f81f7cc3d49c1a520069b.png)'
- en: 'Figure 3: The workflow of SceneCraft’s inner-loop improvement of each scene.
    1) given query, a LLM writes a list of assets descriptions, then use CLIP retriever
    to fetch assets; 2) then LLM decomposes the full query into a sequence of sub-scene,
    each associated with a subset of assets and a text description; 3) a LLM-Planner
    generate a relational graph linking assets to spatial relationship; 4) Based on
    the graph, LLM-Coder writes python codes to get a list of numerical constraints,
    which can be executed to search optimal layout, and render into image using Blender;
    5) LLM-Reviewer with vision perception capability criticize the rendered image,
    and update the script accordingly. This critic-and-revise procedure can be done
    multiple times to iteratively improve the script and scene.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：SceneCraft 对每个场景的内部循环改进流程。1) 给定查询，LLM 写出一份资产描述列表，然后使用 CLIP 检索器获取资产；2) 然后
    LLM 将完整查询分解为一系列子场景，每个子场景与一部分资产和文本描述相关联；3) LLM 规划器生成一个将资产与空间关系链接的关系图；4) 基于图，LLM
    编码器编写 Python 代码以获取一系列数值约束，这些约束可以被执行以搜索最佳布局，并使用 Blender 渲染为图像；5) LLM 评审员利用视觉感知能力批评渲染图像，并相应地更新脚本。这个批评和修订的过程可以多次进行，以迭代地改进脚本和场景。
- en: 2 Approach
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: Our goal is to transform a text query $q$ that is not only spatially coherent
    but also contextually rich and aesthetically pleasing. This requires (a) identifying
    the correct spatial and contextual relationships between assets, and (b) predicting
    a high fidelity and nice looking arrangement that aligns with these relationships.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将一个文本查询 $q$ 转换为一个不仅空间上连贯而且上下文丰富且美观的结果。这需要 (a) 识别资产之间正确的空间和上下文关系，(b) 预测与这些关系对齐的高保真且美观的布局。
- en: SceneCraft performs this task by building on top of a state-of-the-art multimodal
    LLM (i.e., GPT-4V (OpenAI, [2023](#bib.bib16))) and a professional rendering software
    (Blender). We now describe the key components of our method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft 通过建立在最先进的多模态语言模型 (即 GPT-4V (OpenAI, [2023](#bib.bib16))) 和专业渲染软件
    (Blender) 之上来执行此任务。我们现在描述我们方法的关键组件。
- en: 2.1 Asset Retrieval and Scene Decomposition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 资产检索与场景分解
- en: A scene consists of a set of assets, where each asset $a_{i}$ are retrieved
    from a large repository of 3D objects utilizing a CLIP-based retriever. The retrieval
    process first finds the top-10 assets based on the text description of each asset.
    Then each retrieved asset is rendered as an image and the one with the highest
    text-to-image score is selected.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个场景由一组资产组成，其中每个资产 $a_{i}$ 从一个大型 3D 对象库中检索，使用基于 CLIP 的检索器。检索过程首先根据每个资产的文本描述找到前
    10 个资产。然后，将每个检索到的资产渲染为图像，并选择文本到图像评分最高的那个。
- en: Some scenes might contain up to a hundred assets, making the layout planning
    very difficult. Therefore, SceneCraft agent decomposes the scene into a set of
    sub-scenes, each representing a part of the entire scene. Breaking the problem
    into small pieces is a widely adopted strategy in natural language question answering (Perez
    et al., [2020](#bib.bib18)) and general reasoning (Zhou et al., [2023a](#bib.bib34)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些场景可能包含多达一百个资产，使得布局规划非常困难。因此，SceneCraft 代理将场景分解为一组子场景，每个子场景代表整个场景的一部分。将问题拆解为小块是自然语言问答
    (Perez et al., [2020](#bib.bib18)) 和一般推理 (Zhou et al., [2023a](#bib.bib34)) 中广泛采用的策略。
- en: The agent calls a LLM-empowered decomposer that breaks the input query into
    a sequence of sub-scenes $\hat{s}_{k}$. The scene descriptions are used to guide
    the scene optimization in the later stages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理调用一个 LLM 驱动的分解器，将输入查询分解为一系列子场景 $\hat{s}_{k}$。场景描述用于指导后续阶段的场景优化。
- en: '|  | $\displaystyle(q_{1},\mathcal{A}_{1}),\ldots,(q_{K},\mathcal{A}_{K})\leftarrow\texttt{LLM-decomposer}(q).$
    |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(q_{1},\mathcal{A}_{1}),\ldots,(q_{K},\mathcal{A}_{K})\leftarrow\texttt{LLM-decomposer}(q).$
    |  | (1) |'
- en: 'As an example shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code"), given a query ”a girl
    hunter walking in a slum village with fantasy creatures”, SceneCraft decomposes
    it into three different steps, among which the first step includes the following
    information:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S1.F3 "图 3 ‣ 1 引言 ‣ SceneCraft: 一个用于合成 Blender 代码的 LLM 代理") 所示的示例中，给定查询
    “一个女孩猎人在一个贫民窟村庄中与幻想生物一起走”，SceneCraft 将其分解为三个不同的步骤，其中第一个步骤包括以下信息：'
- en: '![[Uncaptioned image]](img/aea2e0fd99d43257f63fbb20feadad78.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/aea2e0fd99d43257f63fbb20feadad78.png)'
- en: 2.2 Scene Graph Construction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 场景图构建
- en: In order to put the 3D assets together to create a scene $s$ shall position
    the vase, table and window in the scene such that the vase is standing on the
    table and the table is located near the window.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将3D资产组合在一起以创建一个场景$s$，需要将花瓶、桌子和窗户放置在场景中，使得花瓶放在桌子上，桌子靠近窗户。
- en: The key challenge is to correctly put each asset in the right location and orientation
    by predicting the layout matrix $\mathcal{L}(a_{i})$ for each asset. The naive
    approach is to directly predict all the layout matrices directly given the scene
    description. However, this is a highly complex task even for the most advanced
    LLMs, due to the vast combinatorial space of the possible layouts and deep understanding
    the intricate spatial relations between assets. This is why in SceneCraft we use
    a relational scene graph as an intermediate layer of abstraction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关键挑战在于通过预测每个资产的布局矩阵 $\mathcal{L}(a_{i})$ 来将每个资产正确放置在合适的位置和方向。最简单的方法是直接根据场景描述预测所有布局矩阵。然而，由于可能布局的组合空间庞大以及对资产之间复杂空间关系的深入理解，这对于最先进的LLM来说是一个高度复杂的任务。这就是为什么在
    SceneCraft 中我们使用关系场景图作为中间抽象层的原因。
- en: 'To model the spatial relations between assets, SceneCraft utilizes a set of
    spatial and contextual relations, such as proximity, alignment, parallelism, etc.
    Each relation $r$ applies to a specific set of assets within the scene. Full list
    of relations that we consider can be found in Sec [B](#A2 "Appendix B List of
    relationships ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender
    Code") in Appendix.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '为了建模资产之间的空间关系，SceneCraft 利用一组空间和上下文关系，如接近度、对齐度、平行度等。每个关系$r$ 适用于场景中的特定资产集。我们考虑的关系的完整列表可以在附录第[B](#A2
    "Appendix B List of relationships ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")节找到。'
- en: Using these relations, the scene $s$ in the scene that satisfies this relation.³³3For
    each relation type, we can have multiple relation nodes linking to different subsets
    of assets, e.g., Align-1 relation node links table and vase, and Align-2 links
    vase and window.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些关系，场景$s$在满足这些关系的场景中。³³3对于每种关系类型，我们可以有多个关系节点链接到不同的资产子集，例如，Align-1 关系节点链接桌子和花瓶，而
    Align-2 链接花瓶和窗户。
- en: Based on this definition, SceneCraft then uses a LLM-Planner to construct a
    scene graph connecting assets to corresponding spatial relation nodes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此定义，SceneCraft 然后使用LLM-Planner构建一个场景图，将资产连接到相应的空间关系节点。
- en: '|  | $\displaystyle\mathcal{G}(s)=(\mathcal{A},\mathcal{R},\mathcal{E})\leftarrow\texttt{LLM-Planner}(q_{k},\mathcal{A})$
    |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{G}(s)=(\mathcal{A},\mathcal{R},\mathcal{E})\leftarrow\texttt{LLM-Planner}(q_{k},\mathcal{A})$
    |  | (2) |'
- en: 'For example, when we create the outline of slum village, LLM-Planner predicts
    the following edges:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们创建贫民窟村庄的轮廓时，LLM-Planner 预测以下边：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\langle$: all housess are aligned side by side to form a side-street;'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\langle$: 所有房屋并排对齐以形成侧街；'
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\langle$: Duplicate one side of street to form a pathway or road;'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\langle$: 复制街道的一侧形成小径或道路；'
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\langle$: lamps are located in front of each house.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\langle$: 灯位于每栋房子前面。'
- en: The relations between the assets provide soft spatial constraints for the layout
    matrices $\mathcal{L}$ of the assets. Thus, this intermediate graph serves as
    a high-level plan for subsequent code generation and self-improvement, which significantly
    reduces the complexity of arranging the assets in the scene.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 资产之间的关系为资产的布局矩阵 $\mathcal{L}$ 提供了软空间约束。因此，这个中间图作为后续代码生成和自我改进的高级计划，显著降低了在场景中排列资产的复杂性。
- en: 2.3 Scene Layout Optimization in a Feedback Loop
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 反馈循环中的场景布局优化
- en: 'After we obtain the spatial constraints between the assets, we use a set of
    scoring functions (one per relation) to optimize the scene layout. In Sec. [2.4](#S2.SS4
    "2.4 Library Learning ‣ 2 Approach ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") we describe how we learn the library of scoring functions
    automatically. The scoring function $F_{r}(\cdot)$ returns a real number between
    0 and 1 describing how much this relational constraint is satisfied.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在获得资产之间的空间约束后，我们使用一组评分函数（每个关系一个）来优化场景布局。在第[2.4](#S2.SS4 "2.4 Library Learning
    ‣ 2 Approach ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code")节中，我们描述了如何自动学习评分函数库。评分函数
    $F_{r}(\cdot)$ 返回一个介于0和1之间的实数，描述了这个关系约束的满足程度。'
- en: An LLM-Coder then reuses these existing functions stored in skill library to
    synthesize an overall Blender code script code, including loading the assets,
    doing grouping and generating all the numerical constraints, etc. The LLM-Coder
    will also predict all the arguments $\texttt{arg}_{r}$, such as the exact distance
    for Proximity relation, etc.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，LLM-Coder 会重用存储在技能库中的这些现有函数来综合生成整体的 Blender 代码脚本，包括加载资源、进行分组和生成所有的数值约束等。LLM-Coder
    还会预测所有的参数 $\texttt{arg}_{r}$，例如 Proximity 关系的确切距离等。
- en: '|  | $\displaystyle\texttt{code},\texttt{arg}\leftarrow\texttt{LLM-Coder}(\mathcal{G}(s),q_{k})$
    |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\texttt{code},\texttt{arg}\leftarrow\texttt{LLM-Coder}(\mathcal{G}(s),q_{k})$
    |  | (3) |'
- en: 'For each scene $s$ finding an optimal layout could be formalized as the following
    optimization problem:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个场景 $s$，寻找最佳布局可以形式化为以下优化问题：
- en: '|  | $\hat{\mathcal{L}}\leftarrow\underset{\mathcal{L}}{\mathrm{argmax}}\sum_{r\in\mathcal{R}}F_{r}\Big{(}\big{\{}\mathcal{L}(a_{i})\mid
    a_{i}\in\mathcal{E}(r)\big{\}},\texttt{arg}_{r}\Big{)}$ |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathcal{L}}\leftarrow\underset{\mathcal{L}}{\mathrm{argmax}}\sum_{r\in\mathcal{R}}F_{r}\Big{(}\big{\{}\mathcal{L}(a_{i})\mid
    a_{i}\in\mathcal{E}(r)\big{\}},\texttt{arg}_{r}\Big{)}$ |  | (4) |'
- en: 'This enables SceneCraft to simultaneously balance multiple constraints, ensuring
    a comprehensive and contextually accurate scene layout planning. After getting
    the optimal layout $\hat{\mathcal{L}}$, we can render the scene with the Blender
    code script code to get image output. Examples of generated scripts and rendered
    images can be found at Sec. [A](#A1 "Appendix A Examples of SceneCraft’s Generated
    Scripts and Rendered Scenes ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene
    as Blender Code") in Appendix.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '这使得 SceneCraft 能够同时平衡多个约束，确保全面且符合上下文的场景布局规划。在得到最佳布局 $\hat{\mathcal{L}}$ 后，我们可以使用
    Blender 代码脚本代码渲染场景以获得图像输出。生成的脚本和渲染图像的示例可以在附录 [A](#A1 "Appendix A Examples of SceneCraft’s
    Generated Scripts and Rendered Scenes ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") 中找到。'
- en: Self-Improvement of Scene Script
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景脚本的自我改进
- en: However, the agent often does not produce the correct layout outright. This
    is because either (a) the predicted constraints do not reflect the requirements
    in the input query or do not follow common-sense knowledge, which requires updating
    the scene graph edges $\mathcal{E}$).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，代理通常不会直接生成正确的布局。这是因为 (a) 预测的约束未能反映输入查询中的要求或不符合常识知识，这需要更新场景图边缘 $\mathcal{E}$）。
- en: 'We iteratively improve the initially generated scene layout in a visual feedback
    loop by taking advantage of the perception capabilities of a multimodal LLM (GPT-V (OpenAI,
    [2023](#bib.bib16))). We render the generated scene into an image, then feed the
    rendered image and the scene description directly to the LLM+V-Reviewer, asking
    it which constraints are lacking or not correctly satisfied, asking it to revise
    the script to reflect all the mistakes it finds. If LLM+V-Reviewer finds out that
    the error is rooted in the constraint functions, it can either modify existing
    functions or add new sub-functions to improve the layout planning for the current
    scene. This procedure repeats at every iteration of the feedback loop. We denote
    that at the $t$. This shares a similar intuition with recent works that utilize
    foundational models to generate a reward signal (Baumli et al., [2023](#bib.bib1);
    Rocamonde et al., [2023](#bib.bib22); Ma et al., [2023](#bib.bib15); Shinn et al.,
    [2023](#bib.bib25)). The feedback-loop optimization procedure can formally be
    written as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用多模态 LLM (GPT-V (OpenAI, [2023](#bib.bib16))) 的感知能力，在视觉反馈循环中迭代改进初步生成的场景布局。我们将生成的场景渲染为图像，然后将渲染的图像和场景描述直接提供给
    LLM+V-Reviewer，询问缺少哪些约束或未正确满足的约束，并要求其修订脚本以反映发现的所有错误。如果 LLM+V-Reviewer 发现错误根源在于约束函数，它可以修改现有函数或添加新的子函数来改进当前场景的布局规划。这个过程在每次反馈循环的迭代中重复进行。我们在第
    $t$ 次迭代中表示这一点。这与近期利用基础模型生成奖励信号的研究有类似的直觉 (Baumli et al., [2023](#bib.bib1); Rocamonde
    et al., [2023](#bib.bib22); Ma et al., [2023](#bib.bib15); Shinn et al., [2023](#bib.bib25))。反馈循环优化过程可以形式化地写作：
- en: '|  | $\displaystyle\mathcal{E}^{(t+1)},\mathcal{F}^{(t+1)},\texttt{arg}^{(t+1)}\leftarrow\texttt{LLM+V-Reviewer}(\texttt{img},q_{k})$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{E}^{(t+1)},\mathcal{F}^{(t+1)},\texttt{arg}^{(t+1)}\leftarrow\texttt{LLM+V-Reviewer}(\texttt{img},q_{k})$
    |  |'
- en: '|  | $\displaystyle\text{subject to}\ \ \texttt{img}\leftarrow\texttt{Blender-Render}(\mathcal{A},\mathcal{L}^{t},\texttt{code}^{t})$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{subject to}\ \ \texttt{img}\leftarrow\texttt{Blender-Render}(\mathcal{A},\mathcal{L}^{t},\texttt{code}^{t})$
    |  |'
- en: '|  | $1$2 |  | (5) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: '![Refer to caption](img/2348459d291cdcff4bf9b21e04b92513.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2348459d291cdcff4bf9b21e04b92513.png)'
- en: 'Figure 4: Example of function parallelism_score update in outer-loop library
    learning phase. The update adds constraint score forcing the orientation of the
    assets to be similar.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：外部循环库学习阶段中函数 `parallelism_score` 更新的示例。更新增加了约束得分，强制资产的方向相似。
- en: Algorithm 1 Dual-Loop Improvement Workflow
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 双循环改进工作流
- en: 'Data: $L=\{F_{r}\}$: number of iterations for scene refinement and library
    learning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：$L=\{F_{r}\}$：场景优化和库学习的迭代次数。
- en: repeat // outer-loop
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 重复 // 外部循环
- en: for *$q\in\mathcal{Q}$ times*;
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *$q\in\mathcal{Q}$ 次*；
- en: 2.4 Library Learning
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 库学习
- en: In the preceding sections, we described the methodology behind SceneCraft’s
    generation of scenes, which involves the formulation of relations and constraint
    scoring functions, followed by their iterative optimization through a feedback
    loop. In this section, we go over the process by which we learn a comprehensive
    spatial skill library of constraint functions, designed for re-application in
    the scene generation process for new input queries.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们描述了 SceneCraft 生成场景的过程，这涉及到关系和约束评分函数的制定，随后通过反馈循环进行迭代优化。在本节中，我们将介绍学习全面空间技能库的过程，该库设计用于在新的输入查询的场景生成过程中重新应用。
- en: 'The core of SceneCraft’s library learning originates from the aforementioned
    self-refinement procedure. When a specific constraint function is not sufficient
    to cover all cases of a relation, the LLM+V-Reviewer is able to identify the pitfall
    of function implementation, and make corresponding modification. As an example
    shown in Figure [4](#S2.F4 "Figure 4 ‣ Self-Improvement of Scene Script ‣ 2.3
    Scene Layout Optimization in a Feedback Loop ‣ 2 Approach ‣ SceneCraft: An LLM
    Agent for Synthesizing 3D Scene as Blender Code"), the previous implementation
    of parallelism relation only consider the assets’ location. Through feedback-loop
    optimization for scene improvement, GPT-V identifies that it is necessary to consider
    the similarity over orientation. Therefore, the main goal of library learning
    procedure is to review these gradual changes of $F_{r}$, detect common patterns
    in the addition or modification, and merge these changes into the library.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft 的库学习的核心源于上述自我优化过程。当特定约束函数不足以涵盖关系的所有情况时，LLM+V-Reviewer 能够识别函数实现的缺陷，并做出相应的修改。图
    [4](#S2.F4 "图 4 ‣ 场景脚本的自我改进 ‣ 2.3 反馈循环中的场景布局优化 ‣ 2 方法 ‣ SceneCraft：一个将 3D 场景合成到
    Blender 代码的 LLM 代理") 中的示例显示，之前的平行关系实现只考虑了资产的位置。通过反馈循环优化场景改进，GPT-V 识别出需要考虑方向上的相似性。因此，库学习过程的主要目标是审查
    $F_{r}$ 的这些渐进变化，检测添加或修改中的共性模式，并将这些变化合并到库中。
- en: 'Specifically, we denote $\hat{F_{r}}(q)=F_{r}^{T}(q)$. SceneCraft reviews all
    these updates, try to find one that represents the consensus of all, and merge
    it into the global skill library. This procedure shares similar intuition as universal
    self-consistency (Chen et al., [2023](#bib.bib6)). We thus learn the new function
    as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 具体地，我们表示为 $\hat{F_{r}}(q)=F_{r}^{T}(q)$。SceneCraft 审查所有这些更新，尝试找到一个代表所有共识的更新，并将其合并到全球技能库中。这个过程与通用自一致性（Chen
    et al., [2023](#bib.bib6)）的直觉相似。因此，我们将新函数学习为：
- en: '|  | $\displaystyle F_{r}\leftarrow\texttt{Library-Learner}\Big{(}\big{\{}\hat{F_{r}}(q)\mid
    q\in\mathcal{Q}\big{\}}\Big{)}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{r}\leftarrow\texttt{Library-Learner}\Big{(}\big{\{}\hat{F_{r}}(q)\mid
    q\in\mathcal{Q}\big{\}}\Big{)}$ |  | (6) |'
- en: 'This process is conducted over a batch of queries $\mathcal{Q}=\{q_{i}\}$ to
    ensure the universality of the learned skills. Note that: 1) this procedure could
    be regarded as a meta-learning update of the function initialization to facilitate
    the feedback-loop optimization. 2) this procedure does not require any ground-truth
    scenes, explicit reward function, or any human intervention. All the internal
    learning signal is just the LLM+V-reviewer during the feedback-loop to maximize
    the alignment to textual query. 3) For both optimization stages, updates are made
    to non-parametric knowledge represented as Python code, avoiding the computational
    cost and inaccessibility issues associated with back-propagation in large language
    models.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程在一批查询 $\mathcal{Q}=\{q_{i}\}$ 上进行，以确保所学技能的普遍性。注意：1) 这一过程可以视为对函数初始化的元学习更新，以促进反馈循环优化。2)
    这一过程不需要任何真实场景、显式奖励函数或任何人工干预。所有内部学习信号仅为LLM+V-reviewer在反馈循环中的作用，以最大限度地对齐文本查询。3)
    在两个优化阶段中，更新的是作为Python代码表示的非参数知识，避免了与大语言模型中反向传播相关的计算成本和不可及性问题。
- en: 'SceneCraft’s library learning process is also highly sample-efficient. By manually
    creating 20 examples with ground-truth constraints and running dual-stage optimization
    on them, SceneCraft develops a robust skill library. This approach contrasts with
    traditional model fine-tuning, offering efficiency and adaptability in learning
    for complex tasks like 3D scene generation. The pseudo-code of the whole dual-loop
    learning is illustrated in Alg [1](#alg1 "Algorithm 1 ‣ Self-Improvement of Scene
    Script ‣ 2.3 Scene Layout Optimization in a Feedback Loop ‣ 2 Approach ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'SceneCraft的库学习过程也具有很高的样本效率。通过手动创建20个带有真实约束的示例并对其进行双阶段优化，SceneCraft建立了一个强大的技能库。这种方法与传统的模型微调相比，提供了在3D场景生成等复杂任务中学习的效率和适应性。整个双循环学习的伪代码在Alg [1](#alg1
    "算法 1 ‣ 场景脚本的自我改进 ‣ 2.3 反馈循环中的场景布局优化 ‣ 2 方法 ‣ SceneCraft: 一种将3D场景合成到Blender代码的LLM代理")中进行了说明。'
- en: 3 Experiments
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: We evaluate our proposed SceneCraft first on our curated synthetic queries where
    the ground-truth constraints are available. We then show how the generated 3D
    scenes can help video generation on the Sintel movie as a case study.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在我们精心策划的合成查询上评估我们提出的SceneCraft，其中包含真实的约束。然后，我们展示生成的3D场景如何通过以Sintel电影为案例研究来帮助视频生成。
- en: 3.1 Evaluate Scene Synthesis with Given Constraints
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 在给定约束下评估场景合成
- en: 'SceneCraft is an agent for open-domain scene synthesis. Most of the existing
    3D scene datasets with ground-truth focus on a specific domain such as in-door
    scene (Song et al., [2023](#bib.bib26); Wei et al., [2023](#bib.bib30)) or road
    traffic (Savkin et al., [2023](#bib.bib23)). To systematically study and evaluate
    our agent in this task, we manually create 40 synthetic queries with ground-truth
    constraints. The way we generate these queries is by first sampling a subset of
    relation constraints from the full list (shown in Appendix [B](#A2 "Appendix B
    List of relationships ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as
    Blender Code")). Based on this, the human annotators evaluate whether the scene
    satisfies this relational constraint. Assets are retrieved from Turbosquid⁴⁴4[https://www.turbosquid.com/](https://www.turbosquid.com/).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 'SceneCraft是一个用于开放领域场景合成的代理。大多数现有的带有真实数据的3D场景数据集专注于特定领域，如室内场景 (Song et al.,
    [2023](#bib.bib26); Wei et al., [2023](#bib.bib30))或道路交通 (Savkin et al., [2023](#bib.bib23))。为了系统地研究和评估我们的代理在此任务中的表现，我们手动创建了40个带有真实约束的合成查询。我们生成这些查询的方法是首先从完整列表中抽样出一部分关系约束（见附录 [B](#A2
    "附录 B 关系列表 ‣ SceneCraft: 一种将3D场景合成到Blender代码的LLM代理")）。基于此，人工标注者评估场景是否满足这些关系约束。资产从Turbosquid⁴⁴4[https://www.turbosquid.com/](https://www.turbosquid.com/)中检索。'
- en: Evaluation Metric
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'To verify whether a generated scene fulfills the textual requirement, we ask
    human annotators to also write a scoring function to estimate how much the constraint
    is satisfied. Such function is different from the one SceneCraft learns in its
    skill library, because the scoring function only needs to work for this specific
    scene query. We show examples of the queries as well as the implemented scoring
    function in Sec [D](#A4 "Appendix D Examples of annotated queries ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") in Appendix. The output
    of these scoring functions is less than or equal to 1, and only reaches equality
    when all constraints are strictly satisfied.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证生成的场景是否符合文本要求，我们要求人工标注者编写一个评分函数来估计约束的满足程度。这样的函数不同于SceneCraft在其技能库中学习的函数，因为评分函数仅需适用于此特定场景查询。我们在附录的Sec [D](#A4
    "Appendix D Examples of annotated queries ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")中展示了查询示例以及实现的评分函数。这些评分函数的输出值不超过1，只有在所有约束严格满足时才会达到1。'
- en: For this synthetic dataset, as we don’t have the ground-truth scene layout,
    we adopt two metrics for evaluating scene synthesis model’s performance. The first
    is the standard text-to-image CLIP similarity score (Radford et al., [2021](#bib.bib20)),
    which measures how well the generated scene satisfies the textual description;
    we also use the functions human annotators wrote as a more fine-grained evaluation
    on how our generated scene satisfies all the semantic requirements hidden in the
    query. We use 20 of the 40 queries for building the spatial skill library through
    dual-loop optimization, during which the model only sees the query instead of
    ground-truth constraint score. Afterwards, we evaluate the model performance on
    the remaining 20 queries.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个合成数据集，由于我们没有真实的场景布局，我们采用了两种指标来评估场景合成模型的性能。第一种是标准的文本到图像CLIP相似度评分（Radford等，[2021](#bib.bib20)），它衡量生成的场景满足文本描述的程度；我们还使用人工标注者编写的函数作为更细粒度的评估，以了解我们生成的场景满足查询中所有隐藏的语义要求的程度。我们使用40个查询中的20个通过双循环优化构建空间技能库，在此过程中模型仅查看查询而不是真实的约束分数。之后，我们对剩下的20个查询评估模型性能。
- en: Baselines
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'Most of the existing 3D scene synthesis works only focus on a specific domain,
    e.g., indoor scenes. The only prior system that serves similar purpose to SceneCraft
    might be BlenderGPT⁵⁵5[https://github.com/gd3kr/BlenderGPT](https://github.com/gd3kr/BlenderGPT),
    an LLM assistant that also takes text query as input and generates Blender code.
    The main difference of BlenderGPT against SceneCraft is that BlenderGPT is limited
    to only the basic Blender instructions such as moving an asset or changing texture.
    To allow it to solve the text-to-scene synthesis task, we modify their code to:
    1) enable BlenderGPT to use GPT-V to receive the screenshot of Blender as visual
    feedback; 2) asking itself to give the per-step instruction for generating the
    complex scene. We also report results of our own system’s ablation. There are
    three major design choices of SceneCraft: 1) Abstraction of scene as relational
    graph; 2) inner-loop optimization of the scene with visual feedback; 3) outer-loop
    learning of skill library. These three components have some dependencies: constraint
    function grounded by relational graph is the main interface to be updated by inner-loop
    (BlenderGPT can be regarded as a baseline only with inner-loop update but without
    graph grounding); while the inner-loop updates of function is the root for library
    learning. Therefore, we do ablation study by removing one component after the
    other.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数3D场景合成工作仅关注于特定领域，例如室内场景。与SceneCraft具有类似目的的唯一先前系统可能是BlenderGPT⁵⁵5[https://github.com/gd3kr/BlenderGPT](https://github.com/gd3kr/BlenderGPT)，它是一个LLM助手，也接受文本查询作为输入并生成Blender代码。BlenderGPT与SceneCraft的主要区别在于BlenderGPT仅限于基本的Blender指令，例如移动资产或更改纹理。为了使其能够解决文本到场景的合成任务，我们修改了它们的代码，以：1)
    使BlenderGPT能够使用GPT-V接收Blender的截图作为视觉反馈；2) 让其自我提供生成复杂场景的逐步指令。我们还报告了我们自己系统的消融实验结果。SceneCraft有三个主要的设计选择：1)
    将场景抽象为关系图；2) 使用视觉反馈对场景进行内部优化；3) 外部技能库学习。这三个组件之间有一些依赖关系：由关系图约束的函数是由内部循环更新的主要接口（BlenderGPT可以视为仅具有内部循环更新但没有图形约束的基准）；而内部循环函数的更新是库学习的根本。因此，我们通过逐个删除组件进行消融研究。
- en: Experimental Results
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验结果
- en: 'Results are shown in Table [1](#S3.T1 "Table 1 ‣ Experimental Results ‣ 3.1
    Evaluate Scene Synthesis with Given Constraints ‣ 3 Experiments ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code"). We see that our method
    consistently improves over all baselines in terms of both CLIP similarity as well
    as the constraint score. Notably, on the constraint score, the BlenderGPT baseline
    only achieves 5.6 score. We show a few head-to-head comparisons in Figure [1](#S0.F1
    "Figure 1 ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code").
    As example, in the first query that asks three boxes stack one on top of each
    other, BlenderGPT simply lists the three boxes in a line and does not follow the
    instruction of stacking; on the second query that asks three trees in a row, BlenderGPT
    does organize the trees in a line, but perpendicular with the road edge. These
    examples show that BlenderGPT without the relational constraint is not able to
    conduct complex spatial planning.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '结果显示在表 [1](#S3.T1 "Table 1 ‣ Experimental Results ‣ 3.1 Evaluate Scene Synthesis
    with Given Constraints ‣ 3 Experiments ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")中。我们看到我们的方法在CLIP相似度和约束分数方面均持续优于所有基线。特别是在约束分数上，BlenderGPT
    基线仅取得 5.6 的分数。我们在图 [1](#S0.F1 "Figure 1 ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")中展示了一些逐对比较。例如，在第一个查询中要求将三个盒子堆叠在一起，BlenderGPT 只是将三个盒子排成一行，并没有遵循堆叠的指示；在第二个查询中要求三棵树排成一排，BlenderGPT
    确实将树木排成一行，但与道路边缘垂直。这些例子表明，没有关系约束的 BlenderGPT 无法进行复杂的空间规划。'
- en: In the meantime, the ablation studies by removing each component also shows
    that all components are very crucial for Scenecraft. Among these components, inner-loop
    optimization provides the most important leaps; removing it leads to 38.4 drop
    on constraint score, and it’s also the root for library learning that keep the
    system self-improving without human annotation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，通过去除每个组件的消融研究也表明所有组件对于 SceneCraft 都非常重要。在这些组件中，内循环优化提供了最重要的提升；去除它会导致约束分数下降
    38.4，并且它也是库学习的根源，使系统在没有人工注释的情况下自我改进。
- en: '| Metric | CLIP SIM | Constraint Score |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | CLIP SIM | 约束分数 |'
- en: '| BlenderGPT | 24.7 | 5.6 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| BlenderGPT | 24.7 | 5.6 |'
- en: '| SceneCraft | 69.8 | 88.9 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SceneCraft | 69.8 | 88.9 |'
- en: '| (—-Ablation by removing one component after the other—-) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| (——通过逐一去除一个组件的消融研究——) |'
- en: '| – Learned Library | 48.3 | 64.5 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| – 学习库 | 48.3 | 64.5 |'
- en: '|      – Inner-Loop | 32.8 | 26.1 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|      – 内循环 | 32.8 | 26.1 |'
- en: '|        – Relation Graph | 19.4 | 3.2 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|        – 关系图 | 19.4 | 3.2 |'
- en: 'Table 1: Comparison of SceneCraft against BlenderGPT and ablation baselines
    on synthetic queries with annotated constraints.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: SceneCraft 与 BlenderGPT 及去除基线在合成查询中具有注释约束的比较。'
- en: 'We also conduct a qualitative evaluation of SceneCraft’s output versus BlenderGPT
    baseline. We randomly select 10 pairs generated by SceneCraft and BlenderGPT,
    and ask humans to judge which one is better, in terms of three major dimensions:
    1) text fidelity: how much the generated scene aligns with the textual query;
    2) composition & constraint agreement: we tell the raters the ground-truth relations,
    and ask whether the generated scene follows all these constraints; 3) Aesthetics:
    we ask which output has better overall visual quality. The order of our output
    against baseline is completely random. Detailed question and interface is shown
    in Figure [11](#A5.F11 "Figure 11 ‣ Appendix E Prompt Used at each stage ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") in Appendix. Altogether
    we collect 22 responses. Results in Table [2](#S3.T2 "Table 2 ‣ Experimental Results
    ‣ 3.1 Evaluate Scene Synthesis with Given Constraints ‣ 3 Experiments ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") show that our method
    outperforms BlenderGPT in all the three dimensions significantly. Specifically,
    consistent with our results on constraint score, SceneCraft gains more improvement
    over the constraint agreement, making the scene logically correct.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还对SceneCraft的输出与BlenderGPT基线进行定性评估。我们随机选择了SceneCraft和BlenderGPT生成的10对场景，让人类判断哪一个更好，评估的三个主要维度是：1)
    文本保真度：生成的场景与文本查询的对齐程度；2) 组成与约束一致性：我们告知评估者真实的关系，并询问生成的场景是否遵循所有这些约束；3) 美学：我们询问哪一个输出的整体视觉质量更好。我们输出与基线的顺序完全是随机的。详细的问题和界面见附录中的图 [11](#A5.F11
    "图 11 ‣ 附录 E 每个阶段使用的提示 ‣ SceneCraft: 用于生成 3D 场景的 LLM 代理")。总共收集了22个反馈。表 [2](#S3.T2
    "表 2 ‣ 实验结果 ‣ 3.1 根据给定约束评估场景合成 ‣ 3 实验 ‣ SceneCraft: 用于生成 3D 场景的 LLM 代理")中的结果表明，我们的方法在所有三个维度上显著优于BlenderGPT。具体来说，与我们在约束评分上的结果一致，SceneCraft在约束一致性方面有更多提升，使场景在逻辑上更为正确。'
- en: '| Win Rate | Text Fidelity | Composition | Aesthetics |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 胜率 | 文本保真度 | 组成 | 美学 |'
- en: '| SceneCraft | 76.8% | 83.6% | 74.5% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SceneCraft | 76.8% | 83.6% | 74.5% |'
- en: '| BlenderGPT | 12.7% | 11.4% | 14.5% |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| BlenderGPT | 12.7% | 11.4% | 14.5% |'
- en: 'Table 2: Qualitative Human comparison of SceneCraft against BlenderGPT baseline.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：SceneCraft与BlenderGPT基线的定性人工比较。
- en: 3.2 Scene-Guided Video Generation over Sintel Movie
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 基于场景的视频生成在Sintel电影中的应用
- en: In addition to synthetic queries, we also show that SceneCraft’s layout planning
    capability generalizes to real scenes , and has potential to control and benefit
    video generation. As open-domain videos do not always have ground-truth scenes,
    we take the Sintel Movie, which is an animated fantasy short film produced with
    Blender, where scripts and Blender scenes are open sourced⁶⁶6[https://studio.blender.org/films/sintel/](https://studio.blender.org/films/sintel/).
    We download all these scenes, using the first half as the training set and the
    remaining half for testing. For this task, we assume that the model is given the
    ground-truth assets for the scene, and only focuses on layout planning to satisfy
    the textual description. After we recover the scene, we study how it can benefit
    a video generation model to get higher-quality predictions. We thus fine-tune
    the VideoPoet model (Kondratyuk et al., [2023](#bib.bib10)), an autoregressive
    Transformer-based video generation framework, on the training set with one ground-truth
    scene image frame as a conditional input. The image will be converted into image
    tokens, and add as prefix after text prompt. We then take the fine-tuned VideoPoet
    model, taking our model and BlenderGPT’s predicted scene, to generate a 2 second
    video.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了合成查询，我们还展示了SceneCraft的布局规划能力如何推广到真实场景，并且有潜力控制和促进视频生成。由于开放域视频并不总是有真实场景，我们选择了Sintel电影，这是由Blender制作的动画奇幻短片，剧本和Blender场景是开源的⁶⁶6[https://studio.blender.org/films/sintel/](https://studio.blender.org/films/sintel/)。我们下载了所有这些场景，使用前半部分作为训练集，剩余部分作为测试集。对于这项任务，我们假设模型已经获得了场景的真实资产，并且只关注布局规划以满足文本描述。恢复场景后，我们研究如何利用它来提升视频生成模型的预测质量。因此，我们在训练集上微调了VideoPoet模型（Kondratyuk等人，[2023](#bib.bib10)），这是一个基于自回归Transformer的视频生成框架，以一帧真实场景图像作为条件输入。图像将被转换为图像令牌，并在文本提示后作为前缀添加。然后，我们使用微调后的VideoPoet模型，结合我们模型和BlenderGPT预测的场景，生成一个2秒的视频。
- en: '![Refer to caption](img/4824e28246e62de390ebd9780ba24521.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4824e28246e62de390ebd9780ba24521.png)'
- en: 'Figure 5: Predicted 3D Scenes as well as the generated videos by SceneCraft
    against other baselines.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：SceneCraft与其他基线对比的预测3D场景以及生成的视频。
- en: '| Quantitative Metric | Scene Comparison | Video Comparison |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 定量指标 | 场景比较 | 视频比较 |'
- en: '| Layout Matrix SIM | Scene CLIP SIM | CLIP-based RM | FVD $\downarrow$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 布局矩阵相似度 | 场景CLIP相似度 | 基于CLIP的RM | FVD $\downarrow$ |'
- en: '| Text-to-Video (w.o. / finetune) | / | / | 56.8 | 846 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 文本到视频（无微调） | / | / | 56.8 | 846 |'
- en: '| Text-to-Video (w / finetune) | / | / | 64.2 | 531 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 文本到视频（带微调） | / | / | 64.2 | 531 |'
- en: '| Text-to-Scene-to-Video, finetune a videogen model on groundtruth scene, infer
    with scenes generated by: |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 文本到场景到视频，在真实场景上微调视频生成模型，使用以下生成的场景进行推断： |'
- en: '| BlenderGPT | 27.5 | 41.8 | 69.1 | 574 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| BlenderGPT | 27.5 | 41.8 | 69.1 | 574 |'
- en: '| SceneCraft (Dual-Loop) | 69.3 | 82.7 | 46.2 | 317 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| SceneCraft（双循环） | 69.3 | 82.7 | 46.2 | 317 |'
- en: 'Table 3: Comparison of SceneCraft with other ablated baselines on a Sintel
    movie. In this setting, we assume to be given fixed assets for each scene, try
    to recover the scene, and guide a video generative model which is fine-tuned on
    first half of the video. We compare with naiive text-to-video baselines without
    scene guidance.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在Sintel电影上，SceneCraft与其他去除基线的比较。在此设置中，我们假设每个场景都有固定的资产，尝试恢复场景，并指导一个在视频前半部分进行微调的视频生成模型。我们与没有场景指导的简单文本到视频基线进行了比较。
- en: 'We compare the output in terms of both the scene itself as well as how much
    it benefits the overall video generation. For the scene, we use two metrics: the
    layout matrix’s similarity (first calculate mutual similarity between assets,
    then calculate cosine similarity), and the rendered image’s CLIP score. For the
    video, as we use both the standard Frechet Video Distance (FVD) distribution score (Unterthiner
    et al., [2019](#bib.bib28)), as well as CLIP-based Relative Matching (RM) score (Wu
    et al., [2021](#bib.bib31)). The results shown in Table [3](#S3.T3 "Table 3 ‣
    3.2 Scene-Guided Video Generation over Sintel Movie ‣ 3 Experiments ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") illustrate that our method
    consistently improves the BlenderGPT output in terms of scene planning. In addition,
    the generated scene helps the video generation and outperform the vanilla text-to-video
    baseline. From the examples in Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Scene-Guided
    Video Generation over Sintel Movie ‣ 3 Experiments ‣ SceneCraft: An LLM Agent
    for Synthesizing 3D Scene as Blender Code"), we can see that the 3D scene grounding
    help the generated video follow more similar structure as ground-truth ones. This
    shows the potential of SceneCraft in controlling video generation in wider domain.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在场景本身以及它对整体视频生成的贡献两个方面比较了输出。对于场景，我们使用两个指标：布局矩阵的相似度（首先计算资产之间的相似性，然后计算余弦相似度），以及渲染图像的CLIP得分。对于视频，我们使用了标准的Frechet视频距离（FVD）分布得分（Unterthiner等，[2019](#bib.bib28)），以及基于CLIP的相对匹配（RM）得分（Wu等，[2021](#bib.bib31)）。表[3](#S3.T3
    "表3 ‣ 3.2 场景指导的视频生成在Sintel电影上的表现 ‣ 3 实验 ‣ SceneCraft: 作为Blender代码合成3D场景的LLM代理")中显示的结果表明，我们的方法在场景规划方面始终改进了BlenderGPT输出。此外，生成的场景有助于视频生成，并超越了普通文本到视频的基线。从图[5](#S3.F5
    "图5 ‣ 3.2 场景指导的视频生成在Sintel电影上的表现 ‣ 3 实验 ‣ SceneCraft: 作为Blender代码合成3D场景的LLM代理")中的示例可以看出，3D场景对视频生成的帮助使得生成的视频更接近真实结构。这显示了SceneCraft在更广泛领域控制视频生成的潜力。'
- en: 4 Related Works
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Text to 3D-Scene Synthesis
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本到3D场景合成
- en: One of the earliest forays into text-driven 3D scene synthesis is WordsEye (Coyne
    & Sproat, [2001](#bib.bib7)). This system, and its follow-up works (Seversky &
    Yin, [2006](#bib.bib24); Chang et al., [2014](#bib.bib3); Ma et al., [2018](#bib.bib14)),
    can generate 3D scenes from natural language. However, these systems often require
    manual mapping between language and object placement, leading to somewhat unnatural
    commands for scene description. Zitnick et al. ([2013](#bib.bib36)) learns to
    map visual features to semantic phrases extracted from sentences, focusing on
    binary spatial or semantic relationships. Chang et al. ([2014](#bib.bib3)) build
    upon and improve these early systems. The key advancement is the use of spatial
    knowledge, derived from 3D scene data, to more accurately constrain scene generations.
    This approach allows for a more realistic interpretation of unstated facts or
    common sense in scene synthesis. In their subsequent work (Chang et al., [2015](#bib.bib4)),
    they focused on lexical grounding of textual terms to 3D model references, combining
    rule-based models with user annotations to select appropriate objects. Their latest
    paper (Chang et al., [2017](#bib.bib5)) further refines this approach, introducing
    interactive text-based scene editing operations and an improved user interface.
    All these systems are most purely symbolic rule-based and require significant
    human efforts to maintain, and are, therefore, hard to generalize to new domains
    and types of constraints.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本驱动的 3D 场景合成的最早尝试之一是 WordsEye (Coyne & Sproat, [2001](#bib.bib7))。该系统及其后续工作
    (Seversky & Yin, [2006](#bib.bib24); Chang et al., [2014](#bib.bib3); Ma et al.,
    [2018](#bib.bib14)) 能够从自然语言生成 3D 场景。然而，这些系统通常需要在语言和对象位置之间进行手动映射，这导致场景描述命令有些不自然。Zitnick
    et al. ([2013](#bib.bib36)) 学习将视觉特征映射到从句子中提取的语义短语，专注于二元空间或语义关系。Chang et al. ([2014](#bib.bib3))
    在这些早期系统的基础上进行改进。关键的进展是利用从 3D 场景数据中获得的空间知识，更准确地约束场景生成。这种方法允许对未明确说明的事实或场景合成中的常识进行更现实的解释。在他们随后的工作中
    (Chang et al., [2015](#bib.bib4))，他们专注于将文本术语的词汇基础与 3D 模型参考相结合，结合基于规则的模型和用户注释来选择合适的对象。他们的最新论文
    (Chang et al., [2017](#bib.bib5)) 进一步完善了这种方法，引入了交互式基于文本的场景编辑操作和改进的用户界面。所有这些系统都是最纯粹的基于符号的规则系统，需要大量的人力维护，因此很难推广到新的领域和约束类型。
- en: There also exist a line of neural-based 3D scene generation that learns from
    data. Most works in this direction focus on a specific domain, such as in-door
    scenes (Patil et al., [2023](#bib.bib17)). For instance, RoomDreamer (Song et al.,
    [2023](#bib.bib26)) trains a diffusion model to simultaneously generate layout,
    geometry and texture for in-door scenes; LEGO-Net (Wei et al., [2023](#bib.bib30))
    focus on the layout planning, and trains a Transformer model to iteratively cleanup
    the messy room. Despite the impressive performance of these work, they are restricted
    by the available 3D scene data. For most open-domain image and videos, it is very
    hard to collect ground-truth 3D scenes, which is why most works in this domain
    focus on in-door scenes. On the contrary, this paper focus on exploring whether
    we can take advantage of the existing knowledge and reasoning capabilities of
    Large Language Models to directly do layout planning without tuning its parameters,
    and we try to learn general spatial planning skills that can be generalized from
    very small number of synthetic queries.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 还存在一系列基于神经网络的 3D 场景生成方法，这些方法从数据中学习。大多数这方面的工作集中在特定领域，例如室内场景 (Patil et al., [2023](#bib.bib17))。例如，RoomDreamer
    (Song et al., [2023](#bib.bib26)) 训练了一个扩散模型，能够同时生成室内场景的布局、几何和纹理；LEGO-Net (Wei
    et al., [2023](#bib.bib30)) 专注于布局规划，训练了一个 Transformer 模型来迭代清理杂乱的房间。尽管这些工作的表现令人印象深刻，但它们受限于可用的
    3D 场景数据。对于大多数开放域的图像和视频，很难收集真实的 3D 场景，这就是为什么大多数这方面的工作集中在室内场景上的原因。相反，本文专注于探索我们是否可以利用现有知识和大型语言模型的推理能力来直接进行布局规划，而无需调整其参数，并尝试学习可以从极少量的合成查询中推广的通用空间规划技能。
- en: Multimodal LLM Agents
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模态 LLM 代理
- en: 'Leverageing visual perception abilities of recent models like GPT-V, multimodal
    LLM Agents (Liu et al., [2023](#bib.bib12)) are capable of interacting with external
    visual environments, such as web browsing (Deng et al., [2023](#bib.bib8); Zhou
    et al., [2023b](#bib.bib35); Hu et al., [2023](#bib.bib9); Zheng et al., [2024](#bib.bib33)),
    gaming (Wang et al., [2023](#bib.bib29)), robotics (Brohan et al., [2023](#bib.bib2))
    and design (Lv et al., [2023](#bib.bib13); Yang et al., [2024](#bib.bib32)). The
    most related concurrent works is 3D-GPT (Sun et al., [2023](#bib.bib27)), which
    interacts with Infinigen (Raistrick et al., [2023](#bib.bib21)), a high-level
    wrapper on top of Blender, to create high-quality environmental scenes. The main
    difference of our work against 3D-GPT⁷⁷7The code of this work hasn’t released,
    and we plan to compare after they open-source the code. is: 1) Environment-wise,
    we directly interact with Blender and a large-scale asset pool, which provide
    richer assets to construct the scene, while Infinigen for now only supports limited
    number of assets and environment arguments; 2) methodology-wise, SceneCraft features
    a dual-loop self-improvement pipeline, which enables us to learn new design skills
    to handle unseen tasks, which differentiate us to many existing llm-agent works
    that heavily rely on manual prompt design.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 利用最近模型如 GPT-V 的视觉感知能力，多模态 LLM 代理（刘等，[2023](#bib.bib12)）能够与外部视觉环境互动，如网页浏览（邓等，[2023](#bib.bib8)；周等，[2023b](#bib.bib35)；胡等，[2023](#bib.bib9)；郑等，[2024](#bib.bib33)）、游戏（王等，[2023](#bib.bib29)）、机器人（布罗汉等，[2023](#bib.bib2)）和设计（吕等，[2023](#bib.bib13)；杨等，[2024](#bib.bib32)）。与之最相关的并发工作是
    3D-GPT（孙等，[2023](#bib.bib27)），它与 Infinigen（赖斯特里克等，[2023](#bib.bib21)）互动，后者是 Blender
    上的高级包装器，用于创建高质量的环境场景。我们工作的主要区别在于 3D-GPT：1) 在环境方面，我们直接与 Blender 和大规模资产库互动，这提供了更丰富的资产来构建场景，而
    Infinigen 目前仅支持有限数量的资产和环境参数；2) 在方法论方面，SceneCraft 具有双循环自我改进管道，使我们能够学习新的设计技能以处理未见任务，这使我们与许多现有的
    LLM 代理工作区分开来，这些工作严重依赖于手动提示设计。
- en: 5 Conclusion
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this paper, we present SceneCraft, an LLM-powered autonomous agent for transforming
    input text query to a 3D Scene by generating a Blender-executable Python script.
    Scenecraft builds on top of multimodal LLMs for both planning and library learning
    in a dual-loop self-improving framework. In the inner-loop, SceneCraft generates
    Blender-executable Python scripts to render an image of the scene, and use a Self-critiquing
    loop to iteratively refine its output and learn from its performance. The outer-loop
    dynamically expands a ’spatial skill’ library, facilitating continuous self-improvement
    without the need for expensive LLM parameter tuning. In the future, we’d like
    to explore: 1) using our framework for reconstructing the 3D scene corresponding
    to a given open-domain image or video; 2) utilizing the generated dataset to fine-tune
    a video generation conditioned on a 3D scene as control signal.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 SceneCraft，一个由 LLM 驱动的自主代理，用于将输入的文本查询转换为 3D 场景，通过生成一个 Blender 可执行的
    Python 脚本。SceneCraft 基于多模态 LLM，用于在双循环自我改进框架中进行规划和库学习。在内部循环中，SceneCraft 生成 Blender
    可执行的 Python 脚本来渲染场景的图像，并使用自我批评循环来迭代优化其输出并从中学习。外部循环动态扩展“空间技能”库，促进持续自我改进，无需昂贵的 LLM
    参数调整。未来，我们希望探索：1) 使用我们的框架重建与给定开放域图像或视频对应的 3D 场景；2) 利用生成的数据集来微调以 3D 场景作为控制信号的生成视频。
- en: Impact Statements
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance the text-to-3d-scene synthesis.
    The work can have potential to benefit the gaming, cinematic and design industry,
    which are mostly positive impact. We only learn a non-parametric skill library
    from synthetic queries, and not using any private information. There are many
    potential societal consequences of our work, none which we feel must be specifically
    highlighted here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了旨在推进文本到 3D 场景合成的工作。该工作有可能惠及游戏、电影和设计行业，带来积极的影响。我们仅从合成查询中学习了一个非参数化的技能库，并未使用任何私人信息。我们工作的潜在社会影响有很多，但我们认为没有必要在此特别强调。
- en: References
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Baumli et al. (2023) Baumli, K., Baveja, S., Behbahani, F. M. P., Chan, H.,
    Comanici, G., Flennerhag, S., Gazeau, M., Holsheimer, K., Horgan, D., Laskin,
    M., Lyle, C., Masoom, H., McKinney, K., Mnih, V., Neitz, A., Pardo, F., Parker-Holder,
    J., Quan, J., Rocktäschel, T., Sahni, H., Schaul, T., Schroecker, Y., Spencer,
    S., Steigerwald, R., Wang, L., and Zhang, L. Vision-language models as a source
    of rewards. *CoRR*, abs/2312.09187, 2023. doi: 10.48550/ARXIV.2312.09187. URL
    [https://doi.org/10.48550/arXiv.2312.09187](https://doi.org/10.48550/arXiv.2312.09187).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baumli et al. (2023) Baumli, K., Baveja, S., Behbahani, F. M. P., Chan, H.,
    Comanici, G., Flennerhag, S., Gazeau, M., Holsheimer, K., Horgan, D., Laskin,
    M., Lyle, C., Masoom, H., McKinney, K., Mnih, V., Neitz, A., Pardo, F., Parker-Holder,
    J., Quan, J., Rocktäschel, T., Sahni, H., Schaul, T., Schroecker, Y., Spencer,
    S., Steigerwald, R., Wang, L., 和 Zhang, L. 视觉-语言模型作为奖励来源。*CoRR*，abs/2312.09187，2023。doi:
    10.48550/ARXIV.2312.09187。网址 [https://doi.org/10.48550/arXiv.2312.09187](https://doi.org/10.48550/arXiv.2312.09187)。'
- en: 'Brohan et al. (2023) Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen,
    X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P.,
    Fu, C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu,
    J., Ichter, B., Irpan, A., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y.,
    Leal, I., Lee, L., Lee, T. E., Levine, S., Lu, Y., Michalewski, H., Mordatch,
    I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M. S., Salazar, G., Sanketi, P.,
    Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H. T., Vanhoucke, V., Vuong,
    Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu,
    S., Yu, T., and Zitkovich, B. RT-2: vision-language-action models transfer web
    knowledge to robotic control. *CoRR*, abs/2307.15818, 2023. doi: 10.48550/ARXIV.2307.15818.
    URL [https://doi.org/10.48550/arXiv.2307.15818](https://doi.org/10.48550/arXiv.2307.15818).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brohan et al. (2023) Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen,
    X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P.,
    Fu, C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu,
    J., Ichter, B., Irpan, A., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y.,
    Leal, I., Lee, L., Lee, T. E., Levine, S., Lu, Y., Michalewski, H., Mordatch,
    I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M. S., Salazar, G., Sanketi, P.,
    Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H. T., Vanhoucke, V., Vuong,
    Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu,
    S., Yu, T., 和 Zitkovich, B. RT-2: 视觉-语言-行动模型将网络知识转移到机器人控制。*CoRR*，abs/2307.15818，2023。doi:
    10.48550/ARXIV.2307.15818。网址 [https://doi.org/10.48550/arXiv.2307.15818](https://doi.org/10.48550/arXiv.2307.15818)。'
- en: 'Chang et al. (2014) Chang, A. X., Savva, M., and Manning, C. D. Learning spatial
    knowledge for text to 3d scene generation. In Moschitti, A., Pang, B., and Daelemans,
    W. (eds.), *Proceedings of the 2014 Conference on Empirical Methods in Natural
    Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of
    SIGDAT, a Special Interest Group of the ACL*, pp.  2028–2038\. ACL, 2014. doi:
    10.3115/V1/D14-1217. URL [https://doi.org/10.3115/v1/d14-1217](https://doi.org/10.3115/v1/d14-1217).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang et al. (2014) Chang, A. X., Savva, M., 和 Manning, C. D. 学习空间知识以进行文本到3D场景生成。见
    Moschitti, A., Pang, B., 和 Daelemans, W.（编），*2014年自然语言处理经验方法会议论文集，EMNLP 2014，2014年10月25-29日，卡塔尔多哈，ACL的一个特别兴趣小组SIGDAT的会议*，第2028–2038页。ACL，2014。doi:
    10.3115/V1/D14-1217。网址 [https://doi.org/10.3115/v1/d14-1217](https://doi.org/10.3115/v1/d14-1217)。'
- en: 'Chang et al. (2015) Chang, A. X., Monroe, W., Savva, M., Potts, C., and Manning,
    C. D. Text to 3d scene generation with rich lexical grounding. In *Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing of the Asian
    Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing,
    China, Volume 1: Long Papers*, pp.  53–62\. The Association for Computer Linguistics,
    2015. doi: 10.3115/V1/P15-1006. URL [https://doi.org/10.3115/v1/p15-1006](https://doi.org/10.3115/v1/p15-1006).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang et al. (2015) Chang, A. X., Monroe, W., Savva, M., Potts, C., 和 Manning,
    C. D. 基于丰富词汇基础的文本到3D场景生成。见 *第53届计算语言学协会年会和第7届亚洲自然语言处理联合国际会议论文集，ACL 2015，2015年7月26-31日，中国北京，第1卷：长篇论文*，第53–62页。计算语言学协会，2015。doi:
    10.3115/V1/P15-1006。网址 [https://doi.org/10.3115/v1/p15-1006](https://doi.org/10.3115/v1/p15-1006)。'
- en: 'Chang et al. (2017) Chang, A. X., Eric, M., Savva, M., and Manning, C. D. Sceneseer:
    3d scene design with natural language. *CoRR*, abs/1703.00050, 2017. URL [http://arxiv.org/abs/1703.00050](http://arxiv.org/abs/1703.00050).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang et al. (2017) Chang, A. X., Eric, M., Savva, M., 和 Manning, C. D. Sceneseer:
    使用自然语言进行3D场景设计。*CoRR*，abs/1703.00050，2017。网址 [http://arxiv.org/abs/1703.00050](http://arxiv.org/abs/1703.00050)。'
- en: 'Chen et al. (2023) Chen, X., Aksitov, R., Alon, U., Ren, J., Xiao, K., Yin,
    P., Prakash, S., Sutton, C., Wang, X., and Zhou, D. Universal self-consistency
    for large language model generation. *CoRR*, abs/2311.17311, 2023. doi: 10.48550/ARXIV.2311.17311.
    URL [https://doi.org/10.48550/arXiv.2311.17311](https://doi.org/10.48550/arXiv.2311.17311).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人（2023）陈晓，阿克西托夫，阿隆，任杰，小康，尹鹏，普拉卡什，萨顿，王旭，周磊。大型语言模型生成的普遍自洽性。*CoRR*，abs/2311.17311，2023年。doi:
    10.48550/ARXIV.2311.17311。网址 [https://doi.org/10.48550/arXiv.2311.17311](https://doi.org/10.48550/arXiv.2311.17311)。'
- en: 'Coyne & Sproat (2001) Coyne, R. and Sproat, R. Wordseye: an automatic text-to-scene
    conversion system. In Pocock, L. (ed.), *Proceedings of the 28th Annual Conference
    on Computer Graphics and Interactive Techniques, SIGGRAPH 2001, Los Angeles, California,
    USA, August 12-17, 2001*, pp.  487–496\. ACM, 2001. doi: 10.1145/383259.383316.
    URL [https://doi.org/10.1145/383259.383316](https://doi.org/10.1145/383259.383316).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Coyne & Sproat（2001）Coyne，R. 和 Sproat，R. Wordseye：一种自动文本到场景转换系统。见 Pocock，L.（编），*第28届计算机图形学与交互技术年会论文集，SIGGRAPH
    2001，洛杉矶，加利福尼亚州，美国，2001年8月12-17日*，第487-496页。ACM，2001年。doi: 10.1145/383259.383316。网址
    [https://doi.org/10.1145/383259.383316](https://doi.org/10.1145/383259.383316)。'
- en: 'Deng et al. (2023) Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang,
    B., Sun, H., and Su, Y. Mind2web: Towards a generalist agent for the web. *CoRR*,
    abs/2306.06070, 2023. doi: 10.48550/ARXIV.2306.06070. URL [https://doi.org/10.48550/arXiv.2306.06070](https://doi.org/10.48550/arXiv.2306.06070).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '邓等人（2023）邓晓，顾颖，郑博，陈晟，史蒂文斯，王博，孙宏，苏艳。Mind2web：朝着通用智能体迈进。*CoRR*，abs/2306.06070，2023年。doi:
    10.48550/ARXIV.2306.06070。网址 [https://doi.org/10.48550/arXiv.2306.06070](https://doi.org/10.48550/arXiv.2306.06070)。'
- en: 'Hu et al. (2023) Hu, Z., Iscen, A., Sun, C., Chang, K., Sun, Y., Ross, D. A.,
    Schmid, C., and Fathi, A. AVIS: autonomous visual information seeking with large
    language models. *CoRR*, abs/2306.08129, 2023. doi: 10.48550/ARXIV.2306.08129.
    URL [https://doi.org/10.48550/arXiv.2306.08129](https://doi.org/10.48550/arXiv.2306.08129).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '胡等人（2023）胡哲，伊森，孙晨，张凯，孙燕，罗斯，D. A.，施密德，C.，法提。AVIS：使用大型语言模型进行自主视觉信息寻求。*CoRR*，abs/2306.08129，2023年。doi:
    10.48550/ARXIV.2306.08129。网址 [https://doi.org/10.48550/arXiv.2306.08129](https://doi.org/10.48550/arXiv.2306.08129)。'
- en: 'Kondratyuk et al. (2023) Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang,
    J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., Cheng, Y., Chiu,
    M., Dillon, J., Essa, I., Gupta, A., Hahn, M., Hauth, A., Hendon, D., Martinez,
    A., Minnen, D., Ross, D. A., Schindler, G., Sirotenko, M., Sohn, K., Somandepalli,
    K., Wang, H., Yan, J., Yang, M., Yang, X., Seybold, B., and Jiang, L. Videopoet:
    A large language model for zero-shot video generation. *CoRR*, abs/2312.14125,
    2023. doi: 10.48550/ARXIV.2312.14125. URL [https://doi.org/10.48550/arXiv.2312.14125](https://doi.org/10.48550/arXiv.2312.14125).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kondratyuk 等人（2023）Kondratyuk，D.，余玲，顾轩，雷扎马，黄佳，霍农，亚当，阿克巴里，阿隆，毕罗德卡，程洋，邱美，迪龙，艾萨，古普塔，汉，豪斯，亨登，马丁内斯，敏恩，罗斯，G.
    A.，辛德勒，M.，西罗滕科，宋凯，索曼德帕利，王浩，阎杰，杨敏，杨鑫，赛博德，姜雷。Videopoet：一种用于零样本视频生成的大型语言模型。*CoRR*，abs/2312.14125，2023年。doi:
    10.48550/ARXIV.2312.14125。网址 [https://doi.org/10.48550/arXiv.2312.14125](https://doi.org/10.48550/arXiv.2312.14125)。'
- en: 'Lin et al. (2023) Lin, C., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang,
    X., Kreis, K., Fidler, S., Liu, M., and Lin, T. Magic3d: High-resolution text-to-3d
    content creation. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023*, pp.  300–309\. IEEE, 2023.
    doi: 10.1109/CVPR52729.2023.00037. URL [https://doi.org/10.1109/CVPR52729.2023.00037](https://doi.org/10.1109/CVPR52729.2023.00037).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '林等人（2023）林晨，郝骏，唐磊，滝川拓，曾新，黄欣，克雷斯，费德勒，刘敏，林涛。Magic3d：高分辨率文本到3D内容创建。见 *IEEE/CVF计算机视觉与模式识别会议，CVPR
    2023，温哥华，不列颠哥伦比亚，加拿大，2023年6月17-24日*，第300-309页。IEEE，2023年。doi: 10.1109/CVPR52729.2023.00037。网址
    [https://doi.org/10.1109/CVPR52729.2023.00037](https://doi.org/10.1109/CVPR52729.2023.00037)。'
- en: 'Liu et al. (2023) Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu,
    Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang,
    C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench:
    Evaluating llms as agents. *CoRR*, abs/2308.03688, 2023. doi: 10.48550/ARXIV.2308.03688.
    URL [https://doi.org/10.48550/arXiv.2308.03688](https://doi.org/10.48550/arXiv.2308.03688).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人（2023）刘晓，余浩，张赫，徐阳，雷晓，赖华，顾颖，丁赫，门凯，杨凯，张书，邓旭，曾安，杜志，张超，沈尚，张天，苏艳，孙宏，黄茂，董阳，唐骏。Agentbench：将LLMs作为智能体进行评估。*CoRR*，abs/2308.03688，2023年。doi:
    10.48550/ARXIV.2308.03688。网址 [https://doi.org/10.48550/arXiv.2308.03688](https://doi.org/10.48550/arXiv.2308.03688)。'
- en: 'Lv et al. (2023) Lv, J., Huang, Y., Yan, M., Huang, J., Liu, J., Liu, Y., Wen,
    Y., Chen, X., and Chen, S. Gpt4motion: Scripting physical motions in text-to-video
    generation via blender-oriented gpt planning. *arXiv preprint arXiv:2311.12631*,
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lv 等 (2023) Lv, J., Huang, Y., Yan, M., Huang, J., Liu, J., Liu, Y., Wen, Y.,
    Chen, X., 和 Chen, S. Gpt4motion: 在文本到视频生成中通过面向 Blender 的 GPT 规划脚本编写物理运动。*arXiv
    预印本 arXiv:2311.12631*, 2023。'
- en: 'Ma et al. (2018) Ma, R., Patil, A. G., Fisher, M., Li, M., Pirk, S., Hua, B.,
    Yeung, S., Tong, X., Guibas, L. J., and Zhang, H. Language-driven synthesis of
    3d scenes from scene databases. *ACM Trans. Graph.*, 37(6):212, 2018. doi: 10.1145/3272127.3275035.
    URL [https://doi.org/10.1145/3272127.3275035](https://doi.org/10.1145/3272127.3275035).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 (2018) Ma, R., Patil, A. G., Fisher, M., Li, M., Pirk, S., Hua, B., Yeung,
    S., Tong, X., Guibas, L. J., 和 Zhang, H. 基于语言驱动的从场景数据库合成 3D 场景。*ACM Trans. Graph.*,
    37(6):212, 2018. doi: 10.1145/3272127.3275035. URL [https://doi.org/10.1145/3272127.3275035](https://doi.org/10.1145/3272127.3275035)。'
- en: 'Ma et al. (2023) Ma, Y. J., Liang, W., Wang, G., Huang, D., Bastani, O., Jayaraman,
    D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward design via
    coding large language models. *CoRR*, abs/2310.12931, 2023. doi: 10.48550/ARXIV.2310.12931.
    URL [https://doi.org/10.48550/arXiv.2310.12931](https://doi.org/10.48550/arXiv.2310.12931).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 (2023) Ma, Y. J., Liang, W., Wang, G., Huang, D., Bastani, O., Jayaraman,
    D., Zhu, Y., Fan, L., 和 Anandkumar, A. Eureka: 通过编码大型语言模型实现人类级奖励设计。*CoRR*, abs/2310.12931,
    2023. doi: 10.48550/ARXIV.2310.12931. URL [https://doi.org/10.48550/arXiv.2310.12931](https://doi.org/10.48550/arXiv.2310.12931)。'
- en: OpenAI (2023) OpenAI. Gpt-4v(ision) system card. System Card, 2023. URL [URL_of_the_System_Card](URL_of_the_System_Card).
    Version 1.0.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. Gpt-4v(ision) 系统卡。系统卡, 2023. URL [URL_of_the_System_Card](URL_of_the_System_Card).
    版本 1.0。
- en: 'Patil et al. (2023) Patil, A. G., Patil, S. G., Li, M., Fisher, M., Savva,
    M., and Zhang, H. Advances in data-driven analysis and synthesis of 3d indoor
    scenes. *CoRR*, abs/2304.03188, 2023. doi: 10.48550/ARXIV.2304.03188. URL [https://doi.org/10.48550/arXiv.2304.03188](https://doi.org/10.48550/arXiv.2304.03188).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil 等 (2023) Patil, A. G., Patil, S. G., Li, M., Fisher, M., Savva, M., 和
    Zhang, H. 基于数据驱动的 3D 室内场景分析与合成的进展。*CoRR*, abs/2304.03188, 2023. doi: 10.48550/ARXIV.2304.03188.
    URL [https://doi.org/10.48550/arXiv.2304.03188](https://doi.org/10.48550/arXiv.2304.03188)。'
- en: 'Perez et al. (2020) Perez, E., Lewis, P. S. H., Yih, W., Cho, K., and Kiela,
    D. Unsupervised question decomposition for question answering. In Webber, B.,
    Cohn, T., He, Y., and Liu, Y. (eds.), *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*,
    pp.  8864–8880\. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.713.
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.713](https://doi.org/10.18653/v1/2020.emnlp-main.713).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Perez 等 (2020) Perez, E., Lewis, P. S. H., Yih, W., Cho, K., 和 Kiela, D. 无监督的问题分解用于问答。
    在 Webber, B., Cohn, T., He, Y., 和 Liu, Y. (编辑), *2020 年自然语言处理实证方法会议论文集, EMNLP
    2020, 在线, 2020 年 11 月 16-20 日*, 第 8864–8880 页。计算语言学协会, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.713.
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.713](https://doi.org/10.18653/v1/2020.emnlp-main.713)。'
- en: 'Poole et al. (2022) Poole, B., Jain, A., Barron, J. T., and Mildenhall, B.
    Dreamfusion: Text-to-3d using 2d diffusion. *CoRR*, abs/2209.14988, 2022. doi:
    10.48550/ARXIV.2209.14988. URL [https://doi.org/10.48550/arXiv.2209.14988](https://doi.org/10.48550/arXiv.2209.14988).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poole 等 (2022) Poole, B., Jain, A., Barron, J. T., 和 Mildenhall, B. Dreamfusion:
    使用 2D 扩散生成 3D。*CoRR*, abs/2209.14988, 2022. doi: 10.48550/ARXIV.2209.14988. URL
    [https://doi.org/10.48550/arXiv.2209.14988](https://doi.org/10.48550/arXiv.2209.14988)。'
- en: Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G.,
    and Sutskever, I. Learning transferable visual models from natural language supervision.
    In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International Conference
    on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event*, volume 139 of
    *Proceedings of Machine Learning Research*, pp.  8748–8763\. PMLR, 2021. URL [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
    Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., 和 Sutskever,
    I. 从自然语言监督中学习可迁移的视觉模型。在 Meila, M. 和 Zhang, T. (编辑), *第 38 届国际机器学习会议论文集, ICML 2021,
    2021 年 7 月 18-24 日, 虚拟会议*, 第 139 卷 *机器学习研究论文集*, 第 8748–8763 页。PMLR, 2021. URL
    [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html)。
- en: 'Raistrick et al. (2023) Raistrick, A., Lipson, L., Ma, Z., Mei, L., Wang, M.,
    Zuo, Y., Kayan, K., Wen, H., Han, B., Wang, Y., Newell, A., Law, H., Goyal, A.,
    Yang, K., and Deng, J. Infinite photorealistic worlds using procedural generation.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023,
    Vancouver, BC, Canada, June 17-24, 2023*, pp.  12630–12641\. IEEE, 2023. doi:
    10.1109/CVPR52729.2023.01215. URL [https://doi.org/10.1109/CVPR52729.2023.01215](https://doi.org/10.1109/CVPR52729.2023.01215).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raistrick 等（2023）Raistrick, A., Lipson, L., Ma, Z., Mei, L., Wang, M., Zuo,
    Y., Kayan, K., Wen, H., Han, B., Wang, Y., Newell, A., Law, H., Goyal, A., Yang,
    K., 和 Deng, J. 《使用程序生成的无限摄影真实世界》。发表于 *IEEE/CVF计算机视觉与模式识别会议，CVPR 2023，温哥华，加拿大，2023年6月17-24日*，第12630-12641页。IEEE，2023。doi:
    10.1109/CVPR52729.2023.01215。网址 [https://doi.org/10.1109/CVPR52729.2023.01215](https://doi.org/10.1109/CVPR52729.2023.01215)。'
- en: 'Rocamonde et al. (2023) Rocamonde, J., Montesinos, V., Nava, E., Perez, E.,
    and Lindner, D. Vision-language models are zero-shot reward models for reinforcement
    learning. *CoRR*, abs/2310.12921, 2023. doi: 10.48550/ARXIV.2310.12921. URL [https://doi.org/10.48550/arXiv.2310.12921](https://doi.org/10.48550/arXiv.2310.12921).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rocamonde 等（2023）Rocamonde, J., Montesinos, V., Nava, E., Perez, E., 和 Lindner,
    D. 《视觉-语言模型是用于强化学习的零样本奖励模型》。*CoRR*，abs/2310.12921，2023。doi: 10.48550/ARXIV.2310.12921。网址
    [https://doi.org/10.48550/arXiv.2310.12921](https://doi.org/10.48550/arXiv.2310.12921)。'
- en: 'Savkin et al. (2023) Savkin, A., Ellouze, R., Navab, N., and Tombari, F. Unsupervised
    traffic scene generation with synthetic 3d scene graphs. *CoRR*, abs/2303.08473,
    2023. doi: 10.48550/ARXIV.2303.08473. URL [https://doi.org/10.48550/arXiv.2303.08473](https://doi.org/10.48550/arXiv.2303.08473).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Savkin 等（2023）Savkin, A., Ellouze, R., Navab, N., 和 Tombari, F. 《基于合成3D场景图的无监督交通场景生成》。*CoRR*，abs/2303.08473，2023。doi:
    10.48550/ARXIV.2303.08473。网址 [https://doi.org/10.48550/arXiv.2303.08473](https://doi.org/10.48550/arXiv.2303.08473)。'
- en: 'Seversky & Yin (2006) Seversky, L. M. and Yin, L. Real-time automatic 3d scene
    generation from natural language voice and text descriptions. In Nahrstedt, K.,
    Turk, M. A., Rui, Y., Klas, W., and Mayer-Patel, K. (eds.), *Proceedings of the
    14th ACM International Conference on Multimedia, Santa Barbara, CA, USA, October
    23-27, 2006*, pp.  61–64\. ACM, 2006. doi: 10.1145/1180639.1180660. URL [https://doi.org/10.1145/1180639.1180660](https://doi.org/10.1145/1180639.1180660).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Seversky & Yin（2006）Seversky, L. M. 和 Yin, L. 《基于自然语言语音和文本描述的实时自动3D场景生成》。发表于
    Nahrstedt, K., Turk, M. A., Rui, Y., Klas, W., 和 Mayer-Patel, K.（编辑），*第14届ACM国际多媒体会议，圣巴巴拉，美国，加州，2006年10月23-27日*，第61-64页。ACM，2006。doi:
    10.1145/1180639.1180660。网址 [https://doi.org/10.1145/1180639.1180660](https://doi.org/10.1145/1180639.1180660)。'
- en: 'Shinn et al. (2023) Shinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous
    agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*,
    2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等（2023）Shinn, N., Labash, B., 和 Gopinath, A. 《Reflexion: 一种具有动态记忆和自我反思的自主智能体》。*arXiv预印本
    arXiv:2303.11366*，2023。'
- en: 'Song et al. (2023) Song, L., Cao, L., Xu, H., Kang, K., Tang, F., Yuan, J.,
    and Yang, Z. Roomdreamer: Text-driven 3d indoor scene synthesis with coherent
    geometry and texture. In El-Saddik, A., Mei, T., Cucchiara, R., Bertini, M., Vallejo,
    D. P. T., Atrey, P. K., and Hossain, M. S. (eds.), *Proceedings of the 31st ACM
    International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October
    2023- 3 November 2023*, pp.  6898–6906\. ACM, 2023. doi: 10.1145/3581783.3611800.
    URL [https://doi.org/10.1145/3581783.3611800](https://doi.org/10.1145/3581783.3611800).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等（2023）Song, L., Cao, L., Xu, H., Kang, K., Tang, F., Yuan, J., 和 Yang,
    Z. 《Roomdreamer: 基于文本的3D室内场景合成，具备一致的几何体和纹理》。发表于 El-Saddik, A., Mei, T., Cucchiara,
    R., Bertini, M., Vallejo, D. P. T., Atrey, P. K., 和 Hossain, M. S.（编辑），*第31届ACM国际多媒体会议，MM
    2023，渥太华，加拿大，2023年10月29日-11月3日*，第6898-6906页。ACM，2023。doi: 10.1145/3581783.3611800。网址
    [https://doi.org/10.1145/3581783.3611800](https://doi.org/10.1145/3581783.3611800)。'
- en: 'Sun et al. (2023) Sun, C., Han, J., Deng, W., Wang, X., Qin, Z., and Gould,
    S. 3d-gpt: Procedural 3d modeling with large language models. *CoRR*, abs/2310.12945,
    2023. doi: 10.48550/ARXIV.2310.12945. URL [https://doi.org/10.48550/arXiv.2310.12945](https://doi.org/10.48550/arXiv.2310.12945).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等（2023）Sun, C., Han, J., Deng, W., Wang, X., Qin, Z., 和 Gould, S. 《3D-GPT:
    使用大型语言模型进行程序化3D建模》。*CoRR*，abs/2310.12945，2023。doi: 10.48550/ARXIV.2310.12945。网址
    [https://doi.org/10.48550/arXiv.2310.12945](https://doi.org/10.48550/arXiv.2310.12945)。'
- en: 'Unterthiner et al. (2019) Unterthiner, T., van Steenkiste, S., Kurach, K.,
    Marinier, R., Michalski, M., and Gelly, S. FVD: A new metric for video generation.
    In *Deep Generative Models for Highly Structured Data, ICLR 2019 Workshop, New
    Orleans, Louisiana, United States, May 6, 2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=rylgEULtdN](https://openreview.net/forum?id=rylgEULtdN).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unterthiner 等（2019） Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier,
    R., Michalski, M., 和 Gelly, S. FVD：一种用于视频生成的新度量。发表于 *深度生成模型在高度结构化数据上的应用，ICLR 2019研讨会，美国路易斯安那州新奥尔良，2019年5月6日*。OpenReview.net，2019年。URL
    [https://openreview.net/forum?id=rylgEULtdN](https://openreview.net/forum?id=rylgEULtdN)。
- en: 'Wang et al. (2023) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large
    language models. *CoRR*, abs/2305.16291, 2023. doi: 10.48550/ARXIV.2305.16291.
    URL [https://doi.org/10.48550/arXiv.2305.16291](https://doi.org/10.48550/arXiv.2305.16291).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023） Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y.,
    Fan, L., 和 Anandkumar, A. Voyager：一个具有大型语言模型的开放式具身代理。*CoRR*，abs/2305.16291，2023年。doi:
    10.48550/ARXIV.2305.16291。URL [https://doi.org/10.48550/arXiv.2305.16291](https://doi.org/10.48550/arXiv.2305.16291)。'
- en: 'Wei et al. (2023) Wei, Q. A., Ding, S., Park, J. J., Sajnani, R., Poulenard,
    A., Sridhar, S., and Guibas, L. J. Lego-net: Learning regular rearrangements of
    objects in rooms. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023*, pp.  19037–19047\. IEEE,
    2023. doi: 10.1109/CVPR52729.2023.01825. URL [https://doi.org/10.1109/CVPR52729.2023.01825](https://doi.org/10.1109/CVPR52729.2023.01825).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等（2023） Wei, Q. A., Ding, S., Park, J. J., Sajnani, R., Poulenard, A.,
    Sridhar, S., 和 Guibas, L. J. Lego-net：学习房间中物体的规则性重新排列。发表于 *IEEE/CVF计算机视觉与模式识别会议，CVPR
    2023，加拿大温哥华，2023年6月17日至24日*，第19037–19047页。IEEE，2023年。doi: 10.1109/CVPR52729.2023.01825。URL
    [https://doi.org/10.1109/CVPR52729.2023.01825](https://doi.org/10.1109/CVPR52729.2023.01825)。'
- en: 'Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro,
    G., and Duan, N. GODIVA: generating open-domain videos from natural descriptions.
    *CoRR*, abs/2104.14806, 2021. URL [https://arxiv.org/abs/2104.14806](https://arxiv.org/abs/2104.14806).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2021） Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro, G.,
    和 Duan, N. GODIVA：从自然描述生成开放域视频。*CoRR*，abs/2104.14806，2021年。URL [https://arxiv.org/abs/2104.14806](https://arxiv.org/abs/2104.14806)。
- en: 'Yang et al. (2024) Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., and Cui,
    B. Mastering text-to-image diffusion: Recaptioning, planning, and generating with
    multimodal llms. *arXiv preprint arXiv:2401.11708*, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2024） Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., 和 Cui, B. 精通文本到图像的扩散：用多模态LLMs进行重述、规划和生成。*arXiv预印本
    arXiv:2401.11708*，2024年。
- en: Zheng et al. (2024) Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v(ision)
    is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*, 2024.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2024） Zheng, B., Gou, B., Kil, J., Sun, H., 和 Su, Y. Gpt-4v(ision) 是一个通用的网络代理，如果是有基础的。*arXiv预印本
    arXiv:2401.01614*，2024年。
- en: Zhou et al. (2023a) Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang,
    X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V., and Chi, E. H. Least-to-most
    prompting enables complex reasoning in large language models. In *The Eleventh
    International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
    May 1-5, 2023*. OpenReview.net, 2023a. URL [https://openreview.net/pdf?id=WZH7099tgfM](https://openreview.net/pdf?id=WZH7099tgfM).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023a） Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X.,
    Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V., 和 Chi, E. H. 从最少到最多的提示使大型语言模型能够进行复杂推理。发表于
    *第十一届国际学习表征会议，ICLR 2023，卢旺达基加利，2023年5月1日至5日*。OpenReview.net，2023a。URL [https://openreview.net/pdf?id=WZH7099tgfM](https://openreview.net/pdf?id=WZH7099tgfM)。
- en: 'Zhou et al. (2023b) Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar,
    A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena: A realistic web
    environment for building autonomous agents. *arXiv preprint arXiv:2307.13854*,
    2023b. URL [https://webarena.dev](https://webarena.dev).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023b） Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng,
    X., Bisk, Y., Fried, D., Alon, U., 等。Webarena：一个用于构建自主代理的现实网络环境。*arXiv预印本 arXiv:2307.13854*，2023b。URL
    [https://webarena.dev](https://webarena.dev)。
- en: 'Zitnick et al. (2013) Zitnick, C. L., Parikh, D., and Vanderwende, L. Learning
    the visual interpretation of sentences. In *IEEE International Conference on Computer
    Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013*, pp.  1681–1688\. IEEE
    Computer Society, 2013. doi: 10.1109/ICCV.2013.211. URL [https://doi.org/10.1109/ICCV.2013.211](https://doi.org/10.1109/ICCV.2013.211).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zitnick 等（2013） Zitnick, C. L., Parikh, D., 和 Vanderwende, L. 学习句子的视觉解释。发表于
    *IEEE国际计算机视觉会议，ICCV 2013，澳大利亚悉尼，2013年12月1日至8日*，第1681–1688页。IEEE计算机学会，2013年。doi:
    10.1109/ICCV.2013.211。URL [https://doi.org/10.1109/ICCV.2013.211](https://doi.org/10.1109/ICCV.2013.211)。'
- en: Supplementary Material for SceneCraft
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft的补充材料
- en: Appendix A Examples of SceneCraft’s Generated Scripts and Rendered Scenes
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A SceneCraft生成脚本和渲染场景示例
- en: 'Example of SceneCraft’s generated scripts and rendered scene on the synthetic
    datasets are in Figure [6](#A1.F6 "Figure 6 ‣ Appendix A Examples of SceneCraft’s
    Generated Scripts and Rendered Scenes ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") and Figure [7](#A1.F7 "Figure 7 ‣ Appendix A Examples
    of SceneCraft’s Generated Scripts and Rendered Scenes ‣ SceneCraft: An LLM Agent
    for Synthesizing 3D Scene as Blender Code").'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft生成的脚本和合成数据集上的渲染场景示例见图[6](#A1.F6 "图 6 ‣ 附录 A SceneCraft生成脚本和渲染场景示例
    ‣ SceneCraft：一个用于合成3D场景的LLM Agent")和图[7](#A1.F7 "图 7 ‣ 附录 A SceneCraft生成脚本和渲染场景示例
    ‣ SceneCraft：一个用于合成3D场景的LLM Agent")。
- en: '![Refer to caption](img/04a73ec37a7150f31a2134f6789c71ae.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/04a73ec37a7150f31a2134f6789c71ae.png)'
- en: 'Figure 6: Examples of generated code and scenes'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：生成的代码和场景示例
- en: '![Refer to caption](img/be3fff4eeadea8deae445cff767489ab.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be3fff4eeadea8deae445cff767489ab.png)'
- en: 'Figure 7: Examples of generated code and scenes'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：生成的代码和场景示例
- en: Appendix B List of relationships
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 关系列表
- en: 'SceneCraft encapsulates several types of relationships and constraints, including:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft封装了几种类型的关系和约束，包括：
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Proximity: A constraint enforcing the closeness of two objects, e.g., a chair
    near a table.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接近度：一种强制执行两个对象接近的约束，例如，桌子旁边的椅子。
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direction: The angle of one object is targeting at the other.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方向：一个对象的角度指向另一个对象。
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Alignment: Ensuring objects align along a common axis, e.g., paintings aligned
    vertically on a wall.'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对齐：确保对象沿着共同的轴线对齐，例如，墙上垂直对齐的画作。
- en: •
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Symmetry: Mirroring objects along an axis, e.g., symmetrical placement of lamps
    on either side of a bed.'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对称：沿某轴线镜像对象，例如，床两侧对称放置的灯。
- en: •
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Overlap: One object partially covering another, creating depth, e.g., a rug
    under a coffee table.'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重叠：一个对象部分覆盖另一个对象，创造深度，例如，咖啡桌下的地毯。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parallelism: Objects parallel to each other, suggesting direction, e.g., parallel
    rows of seats in a theater.'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平行度：对象彼此平行，提示方向，例如，剧院中的平行座椅排。
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perpendicularity: Objects intersecting at a right angle, e.g., a bookshelf
    perpendicular to a desk.'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 垂直度：对象在直角处相交，例如，垂直于桌子的书架。
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchy: Indicating a list of objects follow a certain order of size / volumns.'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次结构：指示一组对象遵循某种大小/体积顺序。
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rotation: a list of objects rotate a cirtain point, e.g., rotating chairs around
    a meeting table.'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 旋转：一组对象围绕某一点旋转，例如，会议桌周围旋转的椅子。
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Repetition: Repeating patterns for rhythm or emphasis, e.g., a sequence of
    street lights.'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重复：用于节奏或强调的重复模式，例如，一系列街灯。
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scaling: Adjusting object sizes for depth or focus, e.g., smaller background
    trees to create depth perception.'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缩放：调整对象大小以突出深度或焦点，例如，较小的背景树木以创造深度感。
- en: These relationships are vital for creating scenes that are not only visually
    appealing but also contextually coherent. Traditionally the functions $F(\cdot)$,
    using a Large Language Model (LLM) Agent.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关系对于创建不仅视觉上吸引人且具有上下文一致性的场景至关重要。传统上，这些功能$F(\cdot)$，使用大型语言模型（LLM）Agent。
- en: Appendix C Spatial Skill Library
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 空间技能库
- en: 'Below listed all the functions our framework generate. There exist some basic
    fundamental editing functions like import object, add camera, scaling, repetition;
    some functions to get information from the scene, such as calculate shortest distance
    between objects, calculate volumn, etc; as well as functions that calculate constraint
    satisfying score for each relationship. All these functions are autonomously written
    and modified by LLM Agent itself, without ground-truth label or explicit human
    intervention:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 下列是我们框架生成的所有功能。包括一些基本的编辑功能，如导入对象、添加相机、缩放、重复；一些获取场景信息的功能，例如计算物体间的最短距离、计算体积等；以及计算每个关系的约束满足评分的功能。所有这些功能都是由LLM
    Agent自主编写和修改的，无需真实标签或明确的人为干预：
- en: '@dataclassclass  Layout:location:  Tuple[float,  float,  float]min:  Tuple[float,  float,  float]max:  Tuple[float,  float,  float]orientation:  Tuple[float,  float,  float]  #  Euler  angles  (pitch,  yaw,  roll)def  scale_group(objects:  List[bpy.types.Object],  scale_factor:  float)  ->  None:"""Scale  a  group  of  objects  by  a  given  factor.Args:objects  (List[bpy.types.Object]):  List  of  Blender  objects  to  scale.scale_factor  (float):  The  scale  factor  to  apply.Example:scale_group([object1,  object2],  1.5)"""for  obj  in  objects:obj.scale  =  (obj.scale.x  *  scale_factor,obj.scale.y  *  scale_factor,obj.scale.z  *  scale_factor)obj.matrix_world  =  obj.matrix_world  *  scale_factordef  find_highest_vertex_point(objs:  List[bpy.types.Object])  ->  Dict[str,  float]:"""Find  the  highest  vertex  point  among  a  list  of  objects.Args:objs  (List[bpy.types.Object]):  List  of  Blender  objects  to  evaluate.Returns:Dict[str,  float]:  The  lowest  x,  y,  and  z  coordinates.Example:lowest_point  =  find_lowest_vertex_point([object1,  object2])"""bpy.context.view_layer.update()highest_points  =  {’x’:  -float(’inf’),  ’y’:  -float(’inf’),  ’z’:  -float(’inf’)}for  obj  in  objs:#  Apply  the  object’s  current  transformation  to  its  verticesobj_matrix_world  =  obj.matrix_worldif  obj.type  ==  ’MESH’:#  Update  mesh  to  the  latest  dataobj.data.update()for  vertex  in  obj.data.vertices:world_vertex  =  obj_matrix_world  @  vertex.cohighest_points[’x’]  =  max(highest_points[’x’],  world_vertex.x)highest_points[’y’]  =  max(highest_points[’y’],  world_vertex.y)highest_points[’z’]  =  max(highest_points[’z’],  world_vertex.z)return  highest_pointsdef  find_lowest_vertex_point(objs:  List[bpy.types.Object])  ->  Dict[str,  float]:"""Find  the  lowest  vertex  point  among  a  list  of  objects.Args:objs  (List[bpy.types.Object]):  List  of  Blender  objects  to  evaluate.Returns:Dict[str,  float]:  The  lowest  x,  y,  and  z  coordinates.Example:lowest_point  =  find_lowest_vertex_point([object1,  object2])"""bpy.context.view_layer.update()lowest_points  =  {’x’:  float(’inf’),  ’y’:  float(’inf’),  ’z’:  float(’inf’)}for  obj  in  objs:#  Apply  the  object’s  current  transformation  to  its  verticesobj_matrix_world  =  obj.matrix_worldif  obj.type  ==  ’MESH’:#  Update  mesh  to  the  latest  dataobj.data.update()for  vertex  in  obj.data.vertices:world_vertex  =  obj_matrix_world  @  vertex.colowest_points[’x’]  =  min(lowest_points[’x’],  world_vertex.x)lowest_points[’y’]  =  min(lowest_points[’y’],  world_vertex.y)lowest_points[’z’]  =  min(lowest_points[’z’],  world_vertex.z)return  lowest_pointsdef  rotate_objects_z_axis(objects:  List[bpy.types.Object],  angle_degrees:  float)  ->  None:"""Rotate  a  group  of  objects  around  the  Z-axis  by  a  given  angle.Args:objects  (List[bpy.types.Object]):  List  of  objects  to  rotate.angle_degrees  (float):  The  angle  in  degrees  to  rotate.Example:rotate_objects_z_axis([object1,  object2],  45)"""bpy.context.view_layer.update()angle_radians  =  math.radians(angle_degrees)  #  Convert  angle  to  radiansrotation_matrix  =  mathutils.Matrix.Rotation(angle_radians,  4,  ’Y’)lowest_point  =  find_lowest_vertex_point(objects)highest_points  =  find_highest_vertex_point(objects)center_point  =  {’x’:  (lowest_point[’x’]  +  highest_points[’x’])  /  2,’y’:  (lowest_point[’y’]  +  highest_points[’y’])  /  2,’z’:  0}for  obj  in  objects:if  obj.type  ==  ’MESH’:obj.data.update()obj.matrix_world  =  obj.matrix_world  @  rotation_matrixlowest_point  =  find_lowest_vertex_point(objects)highest_points  =  find_highest_vertex_point(objects)center_point[’x’]  -=  (lowest_point[’x’]  +  highest_points[’x’])  /  2center_point[’y’]  -=  (lowest_point[’y’]  +  highest_points[’y’])  /  2shift(objects,  center_point)def  shift(objects:  List[bpy.types.Object],  shift_loc:  Dict[str,  float])  ->  None:"""Shift  a  group  of  objects  with  shift_loc.Args:objects  (List[bpy.types.Object]):  List  of  objects  to  rotate.shift_loc  (float):  The  shift  vector.Example:rotate_objects_z_axis([object1,  object2],  (5,3,1))"""for  obj  in  objects:#  Shift  object  so  the  lowest  point  is  at  (0,0,0)obj.location.x  +=  shift_loc[’x’]obj.location.y  +=  shift_loc[’y’]obj.location.z  +=  shift_loc[’z’]bpy.context.view_layer.update()def  calculate_shortest_distance(vertices1:  Set[Tuple[float,  float,  float]],  vertices2:  Set[Tuple[float,  float,  float]])  ->  float:"""Calculate  the  shortest  distance  between  two  sets  of  vertices.Args:vertices1  (Set[Tuple[float,  float,  float]]):  First  set  of  vertices.vertices2  (Set[Tuple[float,  float,  float]]):  Second  set  of  vertices.Returns:float:  Shortest  distance  over  the  Z-axis."""min_distance  =  float(’inf’)for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)distance  =  (v1  -  v2).lengthmin_distance  =  min(min_distance,  distance)return  min_distancedef  rotate_objects_z_axis(objects:  List[bpy.types.Object],  angle_degrees:  float)  ->  None:"""Rotate  a  group  of  objects  around  the  Z-axis  by  a  given  angle.Args:objects  (List[bpy.types.Object]):  List  of  objects  to  rotate.angle_degrees  (float):  The  angle  in  degrees  to  rotate.Example:rotate_objects_z_axis([object1,  object2],  45)"""bpy.context.view_layer.update()angle_radians  =  math.radians(angle_degrees)  #  Convert  angle  to  radiansrotation_matrix  =  mathutils.Matrix.Rotation(angle_radians,  4,  ’Y’)lowest_point  =  find_lowest_vertex_point(objects)highest_points  =  find_highest_vertex_point(objects)center_point  =  {’x’:  (lowest_point[’x’]  +  highest_points[’x’])  /  2,’y’:  (lowest_point[’y’]  +  highest_points[’y’])  /  2,’z’:  0}for  obj  in  objects:if  obj.type  ==  ’MESH’:obj.data.update()obj.matrix_world  =  obj.matrix_world  @  rotation_matrixlowest_point  =  find_lowest_vertex_point(objects)highest_points  =  find_highest_vertex_point(objects)center_point[’x’]  -=  (lowest_point[’x’]  +  highest_points[’x’])  /  2center_point[’y’]  -=  (lowest_point[’y’]  +  highest_points[’y’])  /  2shift(objects,  center_point)def  shift(objects:  List[bpy.types.Object],  shift_loc:  Dict[str,  float])  ->  None:"""Shift  a  group  of  objects  with  shift_loc.Args:objects  (List[bpy.types.Object]):  List  of  objects  to  rotate.shift_loc  (float):  The  shift  vector.Example:rotate_objects_z_axis([object1,  object2],  (5,3,1))"""for  obj  in  objects:#  Shift  object  so  the  lowest  point  is  at  (0,0,0)obj.location.x  +=  shift_loc[’x’]obj.location.y  +=  shift_loc[’y’]obj.location.z  +=  shift_loc[’z’]bpy.context.view_layer.update()def  calculate_shortest_distance(vertices1:  Set[Tuple[float,  float,  float]],  vertices2:  Set[Tuple[float,  float,  float]])  ->  float:"""Calculate  the  shortest  distance  between  two  sets  of  vertices.Args:vertices1  (Set[Tuple[float,  float,  float]]):  First  set  of  vertices.vertices2  (Set[Tuple[float,  float,  float]]):  Second  set  of  vertices.Returns:float:  Shortest  distance  over  the  Z-axis."""min_distance  =  float(’inf’)for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)distance  =  (v1  -  v2).lengthmin_distance  =  min(min_distance,  distance)return  min_distancedef  check_vertex_overlap(vertices1:  Set[Vector],  vertices2:  Set[Vector],  threshold:  float  =  0.01)  ->  float:"""Check  if  there  is  any  overlap  between  two  sets  of  vertices  within  a  threshold.Args:vertices1  (Set[Vector]):  First  set  of  vertices.vertices2  (Set[Vector]):  Second  set  of  vertices.threshold  (float):  Distance  threshold  to  consider  as  an  overlap.Returns:bool:  True  if  there  is  an  overlap,  False  otherwise."""for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)if  (v1  -  v2).length  <=  threshold:return  1.0return  0.0def  evaluate_constraints(assets,  constraints):"""Evaluate  all  constraints  and  return  the  overall  score."""total_score  =  0for  constraint_func,  involved_assets  in  constraints:#  Assuming  each  constraint  function  takes  involved  assets  and  returns  a  scorescores  =  constraint_func([assets[name]  for  name  in  involved_assets])total_score  +=  sum(scores)  #  Summing  scores  assuming  each  constraint  can  contribute  multiple  scoresreturn  total_scoredef  adjust_positions(assets,  adjustment_step=0.1):"""Randomly  adjust  the  positions  of  assets."""for  asset  in  assets.values():#  Randomly  adjust  position  within  a  small  range  to  explore  the  spaceasset.location  =  (asset.location[0]  +  random.uniform(-adjustment_step,  adjustment_step),asset.location[1]  +  random.uniform(-adjustment_step,  adjustment_step),asset.location[2]  #  Z  position  kept  constant  for  simplicity)def  constraint_solving(assets,  constraints,  max_iterations=100):"""Find  an  optimal  layout  of  assets  to  maximize  the  score  defined  by  constraints."""best_score  =  evaluate_constraints(assets,  constraints)best_layout  =  {name:  asset.copy()  for  name,  asset  in  assets.items()}  #  Assuming  a  copy  method  existsfor  _  in  range(max_iterations):adjust_positions(assets)current_score  =  evaluate_constraints(assets,  constraints)if  current_score  >  best_score:best_score  =  current_scorebest_layout  =  {name:  asset.copy()  for  name,  asset  in  assets.items()}else:#  Revert  to  best  layout  if  no  improvementassets  =  {name:  layout.copy()  for  name,  layout  in  best_layout.items()}return  best_layout,  best_scoredef  normalize_vector(v:  np.ndarray)  ->  np.ndarray:"""Normalize  a  vector."""norm  =  np.linalg.norm(v)return  v  /  norm  if  norm  >  0  else  np.zeros_like(v)def  orientation_similarity(orientation1:  Tuple[float,  float,  float],  orientation2:  Tuple[float,  float,  float])  ->  float:"""Calculate  the  similarity  between  two  orientations,  represented  as  Euler  angles."""#  Convert  Euler  angles  to  vectors  for  simplicity  in  comparisonvector1  =  np.array(orientation1)vector2  =  np.array(orientation2)#  Calculate  the  cosine  similarity  between  the  two  orientation  vectorscos_similarity  =  np.dot(vector1,  vector2)  /  (np.linalg.norm(vector1)  *  np.linalg.norm(vector2))return  cos_similaritydef  parallelism_score(assets:  List[Layout])  ->  float:"""Evaluates  and  returns  a  score  indicating  the  degree  of  parallelism  in  a  list  of  assets’  layouts,  considering  both  position  and  orientation.Args:assets  (List[Layout]):  A  list  of  asset  layouts.Returns:float:  A  score  between  0  and  1  indicating  the  parallelism  of  the  assets."""if  len(assets)  <  2:return  1.0  #  Single  asset  or  no  asset  is  arbitrarily  considered  perfectly  parallel#  Positional  parallelismvectors  =  [calculate_vector(assets[i].location,  assets[i+1].location)  for  i  in  range(len(assets)-1)]normalized_vectors  =  [normalize_vector(v)  for  v  in  vectors]dot_products_position  =  [np.dot(normalized_vectors[i],  normalized_vectors[i+1])  for  i  in  range(len(normalized_vectors)-1)]#  Rotational  similarityorientation_similarities  =  [orientation_similarity(assets[i].orientation,  assets[i+1].orientation)  for  i  in  range(len(assets)-1)]#  Combine  scoresposition_score  =  np.mean([0.5  *  (dot  +  1)  for  dot  in  dot_products_position])orientation_score  =  np.mean([(similarity  +  1)  /  2  for  similarity  in  orientation_similarities])#  Average  the  position  and  orientation  scores  for  the  final  scorefinal_score  =  (position_score  +  orientation_score)  /  2return  final_scoredef  calculate_distance(location1:  Tuple[float,  float,  float],  location2:  Tuple[float,  float,  float])  ->  float:"""Calculate  the  Euclidean  distance  between  two  points."""return  np.linalg.norm(np.array(location1)  -  np.array(location2))def  proximity_score(object1:  Layout,  object2:  Layout,  min_distance:  float  =  1.0,  max_distance:  float  =  5.0)  ->  float:"""Calculates  a  proximity  score  indicating  how  close  two  objects  are,  with  1  being  very  close  and  0  being  far  apart.Args:object1  (Layout):  The  first  object’s  layout.object2  (Layout):  The  second  object’s  layout.min_distance  (float):  The  distance  below  which  objects  are  considered  to  be  at  optimal  closeness.  Scores  1.max_distance  (float):  The  distance  beyond  which  objects  are  considered  too  far  apart.  Scores  0.Returns:float:  A  score  between  0  and  1  indicating  the  proximity  of  the  two  objects."""distance  =  calculate_distance(object1.location,  object2.location)if  distance  <=  min_distance:return  1.0elif  distance  >=  max_distance:return  0.0else:#  Linearly  interpolate  the  score  based  on  the  distancereturn  1  -  (distance  -  min_distance)  /  (max_distance  -  min_distance)def  euler_to_forward_vector(orientation:  Tuple[float,  float,  float])  ->  np.ndarray:"""Convert  Euler  angles  to  a  forward  direction  vector."""pitch,  yaw,  _  =  orientation#  Assuming  the  angles  are  in  radiansx  =  np.cos(yaw)  *  np.cos(pitch)y  =  np.sin(yaw)  *  np.cos(pitch)z  =  np.sin(pitch)return  np.array([x,  y,  z])def  calculate_vector(a:  Tuple[float,  float,  float],  b:  Tuple[float,  float,  float])  ->  np.ndarray:"""Calculate  the  directional  vector  from  point  a  to  b."""return  np.array(b)  -  np.array(a)def  direction_score(object1:  Layout,  object2:  Layout)  ->  float:"""Calculates  a  score  indicating  how  directly  object1  is  targeting  object2.Args:object1  (Layout):  The  first  object’s  layout,  assumed  to  be  the  one  doing  the  targeting.object2  (Layout):  The  second  object’s  layout,  assumed  to  be  the  target.Returns:float:  A  score  between  0  and  1  indicating  the  directionality  of  object1  towards  object2."""forward_vector  =  euler_to_forward_vector(object1.orientation)target_vector  =  calculate_vector(object1.location,  object2.location)#  Normalize  vectors  to  ensure  the  dot  product  calculation  is  based  only  on  directionforward_vector_normalized  =  normalize_vector(forward_vector)target_vector_normalized  =  normalize_vector(target_vector)#  Calculate  the  cosine  of  the  angle  between  the  two  vectorscos_angle  =  np.dot(forward_vector_normalized,  target_vector_normalized)#  Map  the  cosine  range  [-1,  1]  to  a  score  range  [0,  1]score  =  (cos_angle  +  1)  /  2return  scoredef  alignment_score(assets:  List[Layout],  axis:  str)  ->  float:"""Calculates  an  alignment  score  for  a  list  of  assets  along  a  specified  axis.Args:assets  (List[Layout]):  A  list  of  asset  layouts  to  be  evaluated  for  alignment.axis  (str):  The  axis  along  which  to  evaluate  alignment  (’x’,  ’y’,  or  ’z’).Returns:float:  A  score  between  0  and  1  indicating  the  degree  of  alignment  along  the  specified  axis."""if  not  assets  or  axis  not  in  [’x’,  ’y’,  ’z’]:return  0.0  #  Return  a  score  of  0  for  invalid  input#  Axis  index  mapping  to  the  location  tupleaxis_index  =  {’x’:  0,  ’y’:  1,  ’z’:  2}[axis]#  Extract  the  relevant  coordinate  for  each  asset  based  on  the  chosen  axiscoordinates  =  [asset.location[axis_index]  for  asset  in  assets]#  Calculate  the  variance  of  these  coordinatesvariance  =  np.var(coordinates)#  Inverse  the  variance  to  calculate  the  score,  assuming  a  lower  variance  indicates  better  alignment#  Normalize  the  score  to  be  between  0  and  1,  considering  a  reasonable  threshold  for  "perfect"  alignmentthreshold_variance  =  1.0  #  Define  a  threshold  variance  for  "perfect"  alignmentscore  =  1  /  (1  +  variance  /  threshold_variance)#  Clamp  the  score  between  0  and  1score  =  max(0,  min(score,  1))return  scoredef  check_vertex_overlap(vertices1:  Set[Vector],  vertices2:  Set[Vector],  threshold:  float  =  0.01)  ->  float:"""Check  if  there  is  any  overlap  between  two  sets  of  vertices  within  a  threshold.Args:vertices1  (Set[Vector]):  First  set  of  vertices.vertices2  (Set[Vector]):  Second  set  of  vertices.threshold  (float):  Distance  threshold  to  consider  as  an  overlap.Returns:bool:  True  if  there  is  an  overlap,  False  otherwise."""for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)if  (v1  -  v2).length  <=  threshold:return  0.0return  1.0def  symmetry_score(assets:  List[Layout],  axis:  str)  ->  float:"""Calculates  a  symmetry  score  for  a  list  of  assets  along  a  specified  axis.Args:assets  (List[Layout]):  A  list  of  asset  layouts  to  be  evaluated  for  symmetry.axis  (str):  The  axis  along  which  to  evaluate  symmetry  (’x’,  ’y’,  or  ’z’).Returns:float:  A  score  between  0  and  1  indicating  the  degree  of  symmetry  along  the  specified  axis."""if  not  assets  or  axis  not  in  [’x’,  ’y’,  ’z’]:return  0.0  #  Return  a  score  of  0  for  invalid  input#  Axis  index  mapping  to  the  location  tupleaxis_index  =  {’x’:  0,  ’y’:  1,  ’z’:  2}[axis]#  Find  the  median  coordinate  along  the  specified  axis  to  define  the  symmetry  axiscoordinates  =  [asset.location[axis_index]  for  asset  in  assets]symmetry_axis  =  np.median(coordinates)#  Calculate  the  deviation  from  symmetry  for  each  assetdeviations  =  []for  asset  in  assets:#  Find  the  mirrored  coordinate  across  the  symmetry  axismirrored_coordinate  =  2  *  symmetry_axis  -  asset.location[axis_index]#  Find  the  closest  asset  to  this  mirrored  coordinateclosest_distance  =  min(abs(mirrored_coordinate  -  other.location[axis_index])  for  other  in  assets)deviations.append(closest_distance)#  Calculate  the  average  deviation  from  perfect  symmetryavg_deviation  =  np.mean(deviations)#  Convert  the  average  deviation  to  a  score,  assuming  smaller  deviations  indicate  better  symmetry#  The  scoring  formula  can  be  adjusted  based  on  the  specific  requirements  for  symmetry  in  the  applicationmax_deviation  =  10.0  #  Define  a  maximum  deviation  for  which  the  score  would  be  0score  =  max(0,  1  -  avg_deviation  /  max_deviation)return  scoredef  perpendicularity_score(object1:  Layout,  object2:  Layout)  ->  float:"""Calculates  a  score  indicating  how  perpendicular  two  objects  are,  based  on  their  forward  direction  vectors.Args:object1  (Layout):  The  first  object’s  layout,  including  its  orientation  as  Euler  angles.object2  (Layout):  The  second  object’s  layout,  including  its  orientation  as  Euler  angles.Returns:float:  A  score  between  0  and  1  indicating  the  degree  of  perpendicularity."""vector1  =  euler_to_forward_vector(object1.orientation)vector2  =  euler_to_forward_vector(object2.orientation)cos_angle  =  np.dot(vector1,  vector2)  /  (np.linalg.norm(vector1)  *  np.linalg.norm(vector2))score  =  1  -  np.abs(cos_angle)return  scoredef  calculate_volume(layout:  Layout)  ->  float:"""Calculate  the  volume  of  an  object  based  on  its  layout  dimensions."""length  =  abs(layout.max[0]  -  layout.min[0])width  =  abs(layout.max[1]  -  layout.min[1])height  =  abs(layout.max[2]  -  layout.min[2])return  length  *  width  *  heightdef  evaluate_hierarchy(assets:  List[Layout],  expected_order:  List[str])  ->  float:"""Evaluates  how  well  a  list  of  objects  follows  a  specified  hierarchical  order  based  on  size.Args:assets  (List[Layout]):  A  list  of  asset  layouts  to  be  evaluated.expected_order  (List[str]):  A  list  of  identifiers  (names)  for  the  assets,  specifying  the  expected  order  of  sizes.Returns:float:  A  metric  indicating  how  well  the  actual  sizes  of  the  objects  match  the  expected  hierarchical  order."""#  Map  identifiers  to  volumesid_to_volume  =  {asset_id:  calculate_volume(asset)  for  asset_id,  asset  in  zip(expected_order,  assets)}#  Calculate  the  actual  order  based  on  sizesactual_order  =  sorted(id_to_volume.keys(),  key=lambda  x:  id_to_volume[x],  reverse=True)#  Evaluate  the  match  between  the  expected  and  actual  orderscorrect_positions  =  sum(1  for  actual,  expected  in  zip(actual_order,  expected_order)  if  actual  ==  expected)total_positions  =  len(expected_order)#  Calculate  the  match  percentage  as  a  measure  of  hierarchy  adherencematch_percentage  =  correct_positions  /  total_positionsreturn  match_percentagedef  calculate_angle_from_center(center:  Tuple[float,  float,  float],  object_location:  Tuple[float,  float,  float])  ->  float:"""Calculate  the  angle  of  an  object  relative  to  a  central  point."""vector  =  np.array(object_location)  -  np.array(center)angle  =  np.arctan2(vector[1],  vector[0])return  angledef  rotation_uniformity_score(objects:  List[Layout],  center:  Tuple[float,  float,  float])  ->  float:"""Calculates  how  uniformly  objects  are  distributed  around  a  central  point  in  terms  of  rotation.Args:objects  (List[Layout]):  A  list  of  object  layouts  to  be  evaluated.center  (Tuple[float,  float,  float]):  The  central  point  around  which  objects  are  rotating.Returns:float:  A  score  between  0  and  1  indicating  the  uniformity  of  object  distribution  around  the  center."""angles  =  [calculate_angle_from_center(center,  obj.location)  for  obj  in  objects]angles  =  np.sort(np.mod(angles,  2*np.pi))  #  Normalize  angles  to  [0,  2\pi]  and  sort#  Calculate  differences  between  consecutive  angles,  including  wrap-around  differenceangle_diffs  =  np.diff(np.append(angles,  angles[0]  +  2*np.pi))#  Evaluate  uniformity  as  the  variance  of  these  differencesvariance  =  np.var(angle_diffs)uniformity_score  =  1  /  (1  +  variance)  #  Inverse  variance,  higher  score  for  lower  variancereturn  uniformity_scoredef  put_ontop(obj_dict,  moving_set_name,  target_set_name,  threshold,  step):"""Adjust  objects  in  moving_set_name  until  the  shortest  distance  to  target_set_name  is  below  the  threshold.Args:obj_dict  (dict):  Dictionary  of  object  sets.moving_set_name  (str):  The  key  for  the  set  of  objects  to  move.target_set_name  (str):  The  key  for  the  set  of  objects  to  calculate  distance  to.threshold  (float):  The  distance  threshold.step  (float):  The  step  by  which  to  move  objects  in  the  Z  direction."""while  True:vertices_set1  =  get_all_vertices(obj_dict[moving_set_name])vertices_set2  =  get_all_vertices(obj_dict[target_set_name])shortest_distance  =  calculate_shortest_distance(vertices_set1,  vertices_set2)print(shortest_distance)if  shortest_distance  <  threshold:breakfor  obj  in  obj_dict[moving_set_name]:obj.location.z  -=  max(step,  shortest_distance)bpy.context.view_layer.update()def  repeat_object(original:  Layout,  direction:  Tuple[float,  float,  float],  repetitions:  int,  distance:  float)  ->  List[Layout]:"""Creates  a  series  of  duplicated  objects  based  on  the  original,  repeating  them  in  a  specified  direction  at  a  set  distance.Args:original  (Layout):  The  original  object  to  be  repeated.direction  (Tuple[float,  float,  float]):  The  direction  vector  along  which  to  repeat  the  object.repetitions  (int):  The  number  of  times  the  object  should  be  repeated.distance  (float):  The  distance  between  each  object.Returns:List[Layout]:  A  list  of  Layout  objects  representing  the  original  and  its  duplicates."""repeated_objects  =  [original]  #  Include  the  original  object  in  the  output  listfor  i  in  range(1,  repetitions):#  Calculate  the  new  location  for  each  repeated  objectnew_location  =  (original.location[0]  +  direction[0]  *  distance  *  i,original.location[1]  +  direction[1]  *  distance  *  i,original.location[2]  +  direction[2]  *  distance  *  i)#  Create  a  new  Layout  instance  for  each  repetitionnew_object  =  Layout(location=new_location,min=original.min,max=original.max,orientation=original.orientation)repeated_objects.append(new_object)return  repeated_objectsdef  add_camera(location:  Tuple[float,  float,  float],  target_point:  Tuple[float,  float,  float],  lens:  float  =  35)  ->  bpy.types.Object:"""Add  a  camera  to  the  Blender  scene.Args:location  (Vector):  The  location  to  place  the  camera.target_point  (Vector):  The  point  the  camera  should  be  aimed  at.lens  (float,  optional):  The  lens  size.  Defaults  to  35.Returns:bpy.types.Object:  The  created  camera  object.Example:camera  =  add_camera((10,  10,  10),  (0,  0,  0))"""#  Create  a  new  camera  data  objectcam_data  =  bpy.data.cameras.new(name="Camera")cam_data.lens  =  lens  #  Set  the  lens  property#  Create  a  new  camera  object  and  link  it  to  the  scenecam_object  =  bpy.data.objects.new(’Camera’,  cam_data)bpy.context.collection.objects.link(cam_object)#  Set  the  camera  locationcam_object.location  =  location#  Calculate  the  direction  vector  from  the  camera  to  the  target  pointdirection  =  Vector(target_point)  -  Vector(location)#  Orient  the  camera  to  look  at  the  target  pointrot_quat  =  direction.to_track_quat(’-Z’,  ’Y’)cam_object.rotation_euler  =  rot_quat.to_euler()#  Set  the  created  camera  as  the  active  camera  in  the  scenebpy.context.scene.camera  =  cam_objectreturn  cam_object'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '@dataclassclass  布局:位置:  元组[浮点数,  浮点数,  浮点数]最小:  元组[浮点数,  浮点数,  浮点数]最大:  元组[浮点数,  浮点数,  浮点数]方向:  元组[浮点数,  浮点数,  浮点数]  #  欧拉角  (俯仰,  偏航,  滚转)def  缩放组(objects:  列表[bpy.types.Object],  scale_factor:  浮点数)  ->  无:"""按给定因子缩放一组对象。参数:objects  (列表[bpy.types.Object]):  要缩放的Blender对象列表.scale_factor  (浮点数):  要应用的缩放因子.示例:缩放组([object1,  object2],  1.5)"""for  obj  in  objects:obj.scale  =  (obj.scale.x  *  scale_factor,obj.scale.y  *  scale_factor,obj.scale.z  *  scale_factor)obj.matrix_world  =  obj.matrix_world  *  scale_factordef  找到最高顶点点(objs:  列表[bpy.types.Object])  ->  字典[str,  浮点数]:"""在一组对象中找到最高的顶点点。参数:objs  (列表[bpy.types.Object]):  要评估的Blender对象列表.返回:字典[str,  浮点数]:  最低的x,  y,  和  z  坐标.示例:最低点  =  找到最低顶点点([object1,  object2])"""bpy.context.view_layer.update()最高点  =  {’x’:  -浮点数(’inf’),  ’y’:  -浮点数(’inf’),  ’z’:  -浮点数(’inf’)}for  obj  in  objs:#  将对象的当前变换应用于其顶点obj_matrix_world  =  obj.matrix_worldif  obj.type  ==  ’MESH’:#  更新网格到最新数据obj.data.update()for  vertex  in  obj.data.vertices:world_vertex  =  obj_matrix_world  @  vertex.cohighest_points[’x’]  =  max(highest_points[’x’],  world_vertex.x)highest_points[’y’]  =  max(highest_points[’y’],  world_vertex.y)highest_points[’z’]  =  max(highest_points[’z’],  world_vertex.z)return  highest_pointsdef  找到最低顶点点(objs:  列表[bpy.types.Object])  ->  字典[str,  浮点数]:"""在一组对象中找到最低的顶点点。参数:objs  (列表[bpy.types.Object]):  要评估的Blender对象列表.返回:字典[str,  浮点数]:  最低的x,  y,  和  z  坐标.示例:最低点  =  找到最低顶点点([object1,  object2])"""bpy.context.view_layer.update()最低点  =  {’x’:  浮点数(’inf’),  ’y’:  浮点数(’inf’),  ’z’:  浮点数(’inf’)}for  obj  in  objs:#  将对象的当前变换应用于其顶点obj_matrix_world  =  obj.matrix_worldif  obj.type  ==  ’MESH’:#  更新网格到最新数据obj.data.update()for  vertex  in  obj.data.vertices:world_vertex  =  obj_matrix_world  @  vertex.colowest_points[’x’]  =  min(lowest_points[’x’],  world_vertex.x)lowest_points[’y’]  =  min(lowest_points[’y’],  world_vertex.y)lowest_points[’z’]  =  min(lowest_points[’z’],  world_vertex.z)return  lowest_pointsdef  旋转对象z轴(objects:  列表[bpy.types.Object],  angle_degrees:  浮点数)  ->  无:"""按给定角度围绕Z轴旋转一组对象。参数:objects  (列表[bpy.types.Object]):  要旋转的对象列表.angle_degrees  (浮点数):  要旋转的角度（以度为单位）。示例:旋转对象z轴([object1,  object2],  45)"""bpy.context.view_layer.update()angle_radians  =  math.radians(angle_degrees)  #  将角度转换为弧度rotation_matrix  =  mathutils.Matrix.Rotation(angle_radians,  4,  ’Y’)最低点  =  找到最低顶点点(objects)最高点  =  找到最高顶点点(objects)中心点  =  {’x’:  (最低点[’x’]  +  最高点[’x’])  /  2,’y’:  (最低点[’y’]  +  最高点[’y’])  /  2,’z’:  0}for  obj  in  objects:if  obj.type  ==  ’MESH’:obj.data.update()obj.matrix_world  =  obj.matrix_world  @  rotation_matrix最低点  =  找到最低顶点点(objects)最高点  =  找到最高顶点点(objects)中心点[’x’]  -=  (最低点[’x’]  +  最高点[’x’])  /  2中心点[’y’]  -=  (最低点[’y’]  +  最高点[’y’])  /  2移动(objects,  center_point)def  移动(objects:  列表[bpy.types.Object],  shift_loc:  字典[str,  浮点数])  ->  无:"""使用shift_loc移动一组对象。参数:objects  (列表[bpy.types.Object]):  要旋转的对象列表.shift_loc  (浮点数):  移动向量.示例:旋转对象z轴([object1,  object2],  (5,3,1))"""for  obj  in  objects:#  移动对象，使最低点位于(0,0,0)obj.location.x  +=  shift_loc[’x’]obj.location.y  +=  shift_loc[’y’]obj.location.z  +=  shift_loc[’z’]bpy.context.view_layer.update()def  计算最短距离(vertices1:  集合[元组[浮点数,  浮点数,  浮点数]],  vertices2:  集合[元组[浮点数,  浮点数,  浮点数]])  ->  浮点数:"""计算两组顶点之间的最短距离。参数:vertices1  (集合[元组[浮点数,  浮点数,  浮点数]]):  第一组顶点.vertices2  (集合[元组[浮点数,  浮点数,  浮点数]]):  第二组顶点.返回:浮点数:  Z轴上的最短距离。"""min_distance  =  浮点数(’inf’)for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)distance  =  (v1  -  v2).lengthmin_distance  =  min(min_distance,  distance)return  min_distancedef  旋转对象z轴(objects:  列表[bpy.types.Object],  angle_degrees:  浮点数)  ->  无:"""按给定角度围绕Z轴旋转一组对象。参数:objects  (列表[bpy.types.Object]):  要旋转的对象列表.angle_degrees  (浮点数):  要旋转的角度（以度为单位）。示例:旋转对象z轴([object1,  object2],  45)"""bpy.context.view_layer.update()angle_radians  =  math.radians(angle_degrees)  #  将角度转换为弧度rotation_matrix  =  mathutils.Matrix.Rotation(angle_radians,  4,  ’Y’)最低点  =  找到最低顶点点(objects)最高点  =  找到最高顶点点(objects)中心点  =  {’x’:  (最低点[’x’]  +  最高点[’x’])  /  2,’y’:  (最低点[’y’]  +  最高点[’y’])  /  2,’z’:  0}for  obj  in  objects:if  obj.type  ==  ’MESH’:obj.data.update()obj.matrix_world  =  obj.matrix_world  @  rotation_matrix最低点  =  找到最低顶点点(objects)最高点  =  找到最高顶点点(objects)中心点[’x’]  -=  (最低点[’x’]  +  最高点[’x’])  /  2中心点[’y’]  -=  (最低点[’y’]  +  最高点[’y’])  /  2移动(objects,  center_point)def  移动(objects:  列表[bpy.types.Object],  shift_loc:  字典[str,  浮点数])  ->  无:"""使用shift_loc移动一组对象。参数:objects  (列表[bpy.types.Object]):  要旋转的对象列表.shift_loc  (浮点数):  移动向量.示例:旋转对象z轴([object1,  object2],  (5,3,1))"""for  obj  in  objects:#  移动对象，使最低点位于(0,0,0)obj.location.x  +=  shift_loc[’x’]obj.location.y  +=  shift_loc[’y’]obj.location.z  +=  shift_loc[’z’]bpy.context.view_layer.update()def  计算最短距离(vertices1:  集合[元组[浮点数,  浮点数,  浮点数]],  vertices2:  集合[元组[浮点数,  浮点数,  浮点数]])  ->  浮点数:"""计算两组顶点之间的最短距离。参数:vertices1  (集合[元组[浮点数,  浮点数,  浮点数]]):  第一组顶点.vertices2  (集合[元组[浮点数,  浮点数,  浮点数]]):  第二组顶点.返回:浮点数:  Z轴上的最短距离。"""min_distance  =  浮点数(’inf’)for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)distance  =  (v1  -  v2).lengthmin_distance  =  min(min_distance,  distance)return  min_distancedef  检查顶点重叠(vertices1:  集合[Vector],  vertices2:  集合[Vector],  threshold:  浮点数  =  0.01)  ->  浮点数:"""检查两组顶点之间是否存在重叠，重叠范围在阈值内。参数:vertices1  (集合[Vector]):  第一组顶点.vertices2  (集合[Vector]):  第二组顶点.threshold  (浮点数):  被视为重叠的距离阈值.返回:布尔值:  如果存在重叠则返回True，否则返回False。"""for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)if  (v1  -  v2).length  <=  threshold:return  1.0return  0.0def  评估约束(assets,  constraints):"""评估所有约束并返回总体得分。"""total_score  =  0for  constraint_func,  involved_assets  in  constraints:#  假设每个约束函数接受相关资产并返回得分scores  =  constraint_func([assets[name]  for  name  in  involved_assets])total_score  +=  sum(scores)  #  假设每个约束可以贡献多个得分返回  total_scoredef  调整位置(assets,  adjustment_step=0.1):"""随机调整资产的位置。"""for  asset  in  assets.values():#  在小范围内随机调整位置以探索空间asset.location  =  (asset.location[0]  +  random.uniform(-adjustment_step,  adjustment_step),asset.location[1]  +  random.uniform(-adjustment_step,  adjustment_step),asset.location[2]  #  Z  位置保持不变以简化)def  约束求解(assets,  constraints,  max_iterations=100):"""找到资产的最佳布局，以最大化约束定义的得分。"""best_score  =  评估约束(assets,  constraints)best_layout  =  {name:  asset.copy()  for  name,  asset  in  assets.items()}  #  假设存在复制方法for  _  in  range(max_iterations):调整位置(assets)current_score  =  评估约束(assets,  constraints)if  current_score  >  best_score:best_score  =  current_scorebest_layout  =  {name:  asset.copy()  for  name,  asset  in  assets.items()}else:#  如果没有改进，则恢复到最佳布局assets  =  {name:  layout.copy()  for  name,  layout  in  best_layout.items()}return  best_layout,  best_scoredef  归一化向量(v:  np.ndarray)  ->  np.ndarray:"""归一化一个向量。"""norm  =  np.linalg.norm(v)return  v  /  norm  if  norm  >  0  else  np.zeros_like(v)def  方向相似度(orientation1:  元组[浮点数,  浮点数,  浮点数],  orientation2:  元组[浮点数,  浮点数,  浮点数])  ->  浮点数:"""计算两个方向之间的相似度，表示为欧拉角。"""#  为了简化比较，将欧拉角转换为向量vector1  =  np.array(orientation1)vector2  =  np.array(orientation2)#  计算两个方向向量之间的余弦相似度cos_similarity  =  np.dot(vector1,  vector2)  /  (np.linalg.norm(vector1)  *  np.linalg.norm(vector2))return  cos_similaritydef  平行度得分(assets:  列表[布局])  ->  浮点数:"""评估并返回一个得分，指示一组资产布局的平行度，考虑位置和方向。参数:assets  (列表[布局]):  一组资产布局。返回:浮点数:  一个介于0和1之间的得分，指示资产的平行度。"""if  len(assets)  <  2:return  1.0  #  单个资产或没有资产被任意视为完全平行#  位置平行性vectors  =  [计算向量(assets[i].location,  assets[i+1].location)  for  i  in  range(len(assets)-1)]normalized_vectors  =  [归一化向量(v)  for  v  in  vectors]dot_products_position  =  [np.dot(normalized_vectors[i],  normalized_vectors[i+1])  for  i  in  range(len(normalized_vectors)-1)]#  旋转相似度orientation_similarities  =  [方向相似度(assets[i].orientation,  assets[i+1].orientation)  for  i  in  range(len(assets)-1)]#  合并得分position_score  =  np.mean([0.5  *  (dot  +  1)  for  dot  in  dot_products_position])orientation_score  =  np.mean([(similarity  +  1)  /  2  for  similarity  in  orientation_similarities])#  对位置和方向得分进行平均以获得最终得分final_score  =  (position_score  +  orientation_score)  /  2return  final_scoredef  计算距离(location1:  元组[浮点数,  浮点数,  浮点数],  location2:  元组[浮点数,  浮点数,  浮点数])  ->  浮点数:"""计算两点之间的欧几里得距离。"""return  np.linalg.norm(np.array(location1)  -  np.array(location2))def  接近度得分(object1:  布局,  object2:  布局,  min_distance:  浮点数  =  1.0,  max_distance:  浮点数  =  5.0)  ->  浮点数:"""计算一个接近度得分，指示两个对象的接近程度，1表示非常接近，0表示相距较远。参数:object1  (布局):  第一个对象的布局.object2  (布局):  第二个对象的布局.min_distance  (浮点数):  被视为最佳接近度的距离。得分
    1.max_distance  (浮点数):  被视为过于远离的距离。得分 0.返回:浮点数:  一个介于0和1之间的得分，指示两个对象的接近程度。"""distance  =  计算距离(object1.location,  object2.location)if  distance  <=  min_distance:return  1.0elif  distance  >=  max_distance:return  0.0else:#  基于距离线性插值得分return  1  -  (distance  -  min_distance)  /  (max_distance  -  min_distance)def  欧拉到前向向量(orientation:  元组[浮点数,  浮点数,  浮点数])  ->  np.ndarray:"""将欧拉角转换为前向方向向量。"""pitch,  yaw,  _  =  orientation#  假设角度以弧度表示x  =  np.cos(yaw)  *  np.cos(pitch)y  =  np.sin(yaw)  *  np.cos(pitch)z  =  np.sin(pitch)return  np.array([x,  y,  z])def  计算向量(a:  元组[浮点数,  浮点数,  浮点数],  b:  元组[浮点数,  浮点数,  浮点数])  ->  np.ndarray:"""计算从点a到b的方向向量。"""return  np.array(b)  -  np.array(a)def  方向得分(object1:  布局,  object2:  布局)  ->  浮点数:"""计算一个得分，指示object1对object2的目标指向程度。参数:object1  (布局):  第一个对象的布局，假设是进行目标指向的对象.object2  (布局):  第二个对象的布局，假设是目标。返回:浮点数:  一个介于0和1之间的得分，指示object1对object2的方向性。"""forward_vector  =  欧拉到前向向量(object1.orientation)target_vector  =  计算向量(object1.location,  object2.location)#  归一化向量以确保点积计算仅基于方向forward_vector_normalized  =  归一化向量(forward_vector)target_vector_normalized  =  归一化向量(target_vector)#  计算两个向量之间的角度余弦cos_angle  =  np.dot(forward_vector_normalized,  target_vector_normalized)#  将余弦范围[-1,  1]映射到得分范围[0,  1]score  =  (cos_angle  +  1)  /  2return  scoredef  对齐得分(assets:  列表[布局],  axis:  str)  ->  浮点数:"""计算一组资产在指定轴上的对齐得分。参数:assets  (列表[布局]):  一组资产布局，需评估对齐情况.axis  (str):  用于评估对齐的轴（’x’，’y’或’z’）。返回:浮点数:  一个介于0和1之间的得分，指示在指定轴上的对齐程度。"""if  not  assets  or  axis  not  in  [’x’,  ’y’,  ’z’]:return  0.0  #  对于无效输入返回0得分#  轴索引映射到位置元组axis_index  =  {’x’:  0,  ’y’:  1,  ’z’:  2}[axis]#  根据所选轴提取每个资产的相关坐标coordinates  =  [asset.location[axis_index]  for  asset  in  assets]#  计算这些坐标的方差variance  =  np.var(coordinates)#  反转方差以计算得分，假设较低的方差表示更好的对齐#  将得分归一化到0和1之间，考虑“完美”对齐的合理阈值threshold_variance  =  1.0  #  定义“完美”对齐的阈值方差score  =  1  /  (1  +  variance  /  threshold_variance)#  将得分限制在0和1之间score  =  max(0,  min(score,  1))return  scoredef  检查顶点重叠(vertices1:  集合[Vector],  vertices2:  集合[Vector],  threshold:  浮点数  =  0.01)  ->  浮点数:"""检查两组顶点之间是否存在重叠，重叠范围在阈值内。参数:vertices1  (集合[Vector]):  第一组顶点.vertices2  (集合[Vector]):  第二组顶点.threshold  (浮点数):  被视为重叠的距离阈值.返回:布尔值:  如果存在重叠则返回True，否则返回False。"""for  v1_tuple  in  vertices1:v1  =  Vector(v1_tuple)for  v2_tuple  in  vertices2:v2  =  Vector(v2_tuple)if  (v1  -  v2).length  <=  threshold:return  0.0return  1.0def  对称性得分(assets:  列表[布局],  axis:  str)  ->  浮点数:"""计算一组资产在指定轴上的对称性得分。参数:assets  (列表[布局]):  一组资产布局，需评估对称性.axis  (str):  用于评估对称性的轴（’x’，’y’或’z’）。返回:浮点数:  一个介于0和1之间的得分，指示在指定轴上的对称程度。"""if  not  assets  or  axis  not  in  [’x’,  ’y’,  ’z’]:return  0.0  #  对于无效输入返回0得分#  轴索引映射到位置元组axis_index  =  {’x’:  0,  ’y’:  1,  ’z’:  2}[axis]#  找到指定轴上的中位坐标以定义对称轴coordinates  =  [asset.location[axis_index]  for  asset  in  assets]symmetry_axis  =  np.median(coordinates)#  计算每个资产的对称偏差deviations  =  []for  asset  in  assets:#  找到对称轴上的镜像坐标mirrored_coordinate  =  2  *  symmetry_axis  -  asset.location[axis_index]#  找到与此镜像坐标最接近的资产closest_distance  =  min(abs(mirrored_coordinate  -  other.location[axis_index])  for  other  in  assets)deviations.append(closest_distance)#  计算与完美对称的平均偏差avg_deviation  =  np.mean(deviations)#  将平均偏差转换为得分，假设较小的偏差表示更好的对称性#  得分公式可以根据应用中对对称性的具体要求进行调整max_deviation  =  10.0  #  定义得分为0的最大偏差score  =  max(0,  1  -  avg_deviation  /  max_deviation)return  scoredef  垂直度得分(object1:  布局,  object2:  布局)  ->  浮点数:"""计算一个得分，指示两个对象的垂直程度，基于它们的前向方向向量。参数:object1  (布局):  第一个对象的布局，包括其作为欧拉角的方向。object2  (布局):  第二个对象的布局，包括其作为欧拉角的方向。返回:浮点数:  一个介于0和1之间的得分，指示垂直程度。"""vector1  =  欧拉到前向向量(object1.orientation)vector2  =  欧拉到前向向量(object2.orientation)cos_angle  =  np.dot(vector1,  vector2)  /  (np.linalg.norm(vector1)  *  np.linalg.norm(vector2))score  =  1  -  np.abs(cos_angle)return  scoredef  计算体积(layout:  布局)  ->  浮点数:"""根据布局尺寸计算对象的体积。"""length  =  abs(layout.max[0]  -  layout.min[0])width  =  abs(layout.max[1]  -  layout.min[1])height  =  abs(layout.max[2]  -  layout.min[2])return  length  *  width  *  heightdef  评估层级(assets:  列表[布局],  expected_order:  列表[str])  ->  浮点数:"""评估一组对象是否遵循指定的层级顺序，基于大小。参数:assets  (列表[布局]):  一组资产布局，需评估。expected_order  (列表[str]):  一组标识符（名称），指定资产的预期大小顺序。返回:浮点数:  一个指标，指示对象的实际大小与预期层级顺序的匹配程度。"""#  将标识符映射到体积id_to_volume  =  {asset_id:  计算体积(asset)  for  asset_id,  asset  in  zip(expected_order,  assets)}#  根据大小计算实际顺序actual_order  =  sorted(id_to_volume.keys(),  key=lambda  x:  id_to_volume[x],  reverse=True)#  评估预期和实际顺序之间的匹配correct_positions  =  sum(1  for  actual,  expected  in  zip(actual_order,  expected_order)  if  actual  ==  expected)total_positions  =  len(expected_order)#  计算匹配百分比作为层级遵循的度量match_percentage  =  correct_positions  /  total_positionsreturn  match_percentagedef  计算中心角度(center:  元组[浮点数,  浮点数,  浮点数],  object_location:  元组[浮点数,  浮点数,  浮点数])  ->  浮点数:"""计算一个对象相对于中心点的角度。"""vector  =  np.array(object_location)  -  np.array(center)angle  =  np.arctan2(vector[1],  vector[0])return  angledef  旋转均匀性得分(objects:  列表[布局],  center:  元组[浮点数,  浮点数,  浮点数])  ->  浮点数:"""计算对象在旋转方面围绕中心点的均匀分布程度。参数:objects  (列表[布局]):  一组对象布局，需评估.center  (元组[浮点数,  浮点数,  浮点数]):  对象旋转的中心点。返回:浮点数:  一个介于0和1之间的得分，指示对象围绕中心的分布均匀性。"""angles  =  [计算中心角度(center,  obj.location)  for  obj  in  objects]angles  =  np.sort(np.mod(angles,  2*np.pi))  #  将角度归一化到[0,  2\pi]并排序#  计算连续角度之间的差异，包括环绕差异angle_diffs  =  np.diff(np.append(angles,  angles[0]  +  2*np.pi))#  将均匀性评估为这些差异的方差variance  =  np.var(angle_diffs)uniformity_score  =  1  /  (1  +  variance)  #  反转方差，较低方差得分较高return  uniformity_scoredef  放置在顶部(obj_dict,  moving_set_name,  target_set_name,  threshold,  step):"""调整moving_set_name中的对象，直到与target_set_name的最短距离低于阈值。参数:obj_dict  (字典):  对象集的字典.moving_set_name  (str):  要移动的对象集的键.target_set_name  (str):  计算距离的对象集的键.threshold  (浮点数):  距离阈值.step  (浮点数):  在Z方向上移动对象的步长。"""while  True:vertices_set1  =  获取所有顶点(obj_dict[moving_set_name])vertices_set2  =  获取所有顶点(obj_dict[target_set_name])shortest_distance  =  计算最短距离(vertices_set1,  vertices_set2)print(shortest_distance)if  shortest_distance  <  threshold:breakfor  obj  in  obj_dict[moving_set_name]:obj.location.z  -=  max(step,  shortest_distance)bpy.context.view_layer.update()def  重复对象(original:  布局,  direction:  元组[浮点数,  浮点数,  浮点数],  repetitions:  int,  distance:  浮点数)  ->  列表[布局]:"""根据原始对象创建一系列重复对象，按指定方向在设定距离上重复。参数:original  (布局):  要重复的原始对象.direction  (元组[浮点数,  浮点数,  浮点数]):  重复对象的方向向量.repetitions  (int):  对象应重复的次数.distance  (浮点数):  每个对象之间的距离。返回:列表[布局]:  表示原始对象及其重复的布局对象列表。"""repeated_objects  =  [original]  #  将原始对象包含在输出列表中for  i  in  range(1,  repetitions):#  计算每个重复对象的新位置new_location  =  (original.location[0]  +  direction[0]  *  distance  *  i,original.location[1]  +  direction[1]  *  distance  *  i,original.location[2]  +  direction[2]  *  distance  *  i)#  为每个重复创建一个新的布局实例new_object  =  布局(location=new_location,min=original.min,max=original.max,orientation=original.orientation)repeated_objects.append(new_object)return  repeated_objectsdef  添加相机(location:  元组[浮点数,  浮点数,  浮点数],  target_point:  元组[浮点数,  浮点数,  浮点数],  lens:  浮点数  =  35)  ->  bpy.types.Object:"""将相机添加到Blender场景中。参数:location  (向量):  放置相机的位置.target_point  (向量):  相机应瞄准的点.lens  (浮点数,  可选):  镜头大小。默认为35。返回:bpy.types.Object:  创建的相机对象.示例:camera  =  添加相机((10,  10,  10),  (0,  0,  0))"""#  创建一个新的相机数据对象cam_data  =  bpy.data.cameras.new(name="Camera")cam_data.lens  =  lens  #  设置镜头属性#  创建一个新的相机对象并将其链接到场景cam_object  =  bpy.data.objects.new(’Camera’,  cam_data)bpy.context.collection.objects.link(cam_object)#  设置相机位置cam_object.location  =  location#  计算从相机到目标点的方向向量direction  =  Vector(target_point)  -  Vector(location)#  定位相机以瞄准目标点rot_quat  =  direction.to_track_quat(’-Z’,  ’Y’)cam_object.rotation_euler  =  rot_quat.to_euler()#  将创建的相机设置为场景中的活动相机bpy.context.scene.camera  =  cam_objectreturn  cam_object'
- en: Appendix D Examples of annotated queries
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 注释查询示例
- en: 'Examples of the annotated queries as well as the per-scene scoring functions
    are shown in Figure [8](#A4.F8 "Figure 8 ‣ Appendix D Examples of annotated queries
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code") and Figure [10](#A5.F10
    "Figure 10 ‣ Appendix E Prompt Used at each stage ‣ SceneCraft: An LLM Agent for
    Synthesizing 3D Scene as Blender Code").'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '注释查询的示例以及每场景的评分函数显示在图 [8](#A4.F8 "Figure 8 ‣ Appendix D Examples of annotated
    queries ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code")
    和图 [10](#A5.F10 "Figure 10 ‣ Appendix E Prompt Used at each stage ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") 中。'
- en: 'scene_1  =  {"description":  "A  book  lying  flat  on  a  table,  two  chair  on  each  side","assets":  ["book",  "table"],"relationships":  {"relativity":  {"description":  "The  book  should  be  on  top  of  the  table","involved_objects":  ["book",  "table"]},"alignment":  {"description":  "The  book  should  be  aligned  with  the  table  in  the  x  and  y  directions","involved_objects":  ["book",  "table"]}}}def  score_1(locs):#  Extracting  locationsx_book,  y_book,  z_book  =  locs[’book’][’x’],  locs[’book’][’y’],  locs[’book’][’z’]x_table,  y_table,  z_table  =  locs[’table’][’x’],  locs[’table’][’y’],  locs[’table’][’z’]#  Relativity  score  (penalizing  if  book  is  below  table  surface)relativity_score  =  max(0,  z_table  -  z_book)  #  positive  if  book  is  below  table#  Alignment  score  (difference  in  x  and  y  positions,  zero  if  perfectly  aligned)alignment_score_x  =  abs(x_book  -  x_table)alignment_score_y  =  abs(y_book  -  y_table)#  Total  score  (sum  of  individual  scores)total_score  =  relativity_score  +  alignment_score_x  +  alignment_score_yreturn  1  -  total_score  /  100.scene_2  =  {"description":  "A  busy  airport  terminal  with  people,  seating  areas,  and  information  displays","assets":  ["person1",  "person2",  "seating_area",  "information_display"],"relationships":  {"grouping":  {"description":  "People  should  be  grouped  near  the  seating  areas","involved_objects":  ["person1",  "person2",  "seating_area"]},"alignment":  {"description":  "Information  displays  should  be  aligned  above  the  seating  areas","involved_objects":  ["seating_area",  "information_display"]},"proximity":  {"description":  "People  should  be  close  to  information  displays  for  visibility","involved_objects":  ["person1",  "person2",  "information_display"]}}}def  score_2(locs):def  distance(a,  b):return  math.sqrt((a[’x’]  -  b[’x’])**2  +  (a[’y’]  -  b[’y’])**2  +  (a[’z’]  -  b[’z’])**2)#  Grouping  score  (distance  of  people  from  seating  areas)grouping_score  =  sum(distance(locs[p],  locs[’seating_area’])  for  p  in  [’person1’,  ’person2’])#  Alignment  score  (information  display  above  seating  areas)alignment_score  =  abs(locs[’seating_area’][’y’]  -  locs[’information_display’][’y’])#  Proximity  score  (people  close  to  information  displays)proximity_score  =  sum(distance(locs[p],  locs[’information_display’])  for  p  in  [’person1’,  ’person2’])#  Total  scoretotal_score  =  grouping_score  +  alignment_score  +  proximity_scorereturn  1  -  total_score  /  100.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'scene_1  =  {"description":  "一本书平放在桌子上，桌子两边各有一把椅子","assets":  ["book",  "table"],"relationships":  {"relativity":  {"description":  "书应放在桌子上面","involved_objects":  ["book",  "table"]},"alignment":  {"description":  "书应在x轴和y轴方向上与桌子对齐","involved_objects":  ["book",  "table"]}}}def  score_1(locs):#  提取位置x_book,  y_book,  z_book  =  locs[’book’][’x’],  locs[’book’][’y’],  locs[’book’][’z’]x_table,  y_table,  z_table  =  locs[’table’][’x’],  locs[’table’][’y’],  locs[’table’][’z’]#  相对性评分  (如果书在桌面下方则扣分)relativity_score  =  max(0,  z_table  -  z_book)  #  如果书在桌子下方则为正#  对齐评分  (x  和  y  位置的差异，如果对齐则为零)alignment_score_x  =  abs(x_book  -  x_table)alignment_score_y  =  abs(y_book  -  y_table)#  总评分  (各项评分之和)total_score  =  relativity_score  +  alignment_score_x  +  alignment_score_yreturn  1  -  total_score  /  100.scene_2  =  {"description":  "一个繁忙的机场候机厅，里面有人员、座位区和信息显示屏","assets":  ["person1",  "person2",  "seating_area",  "information_display"],"relationships":  {"grouping":  {"description":  "人员应在座位区附近分组","involved_objects":  ["person1",  "person2",  "seating_area"]},"alignment":  {"description":  "信息显示屏应与座位区对齐","involved_objects":  ["seating_area",  "information_display"]},"proximity":  {"description":  "人员应靠近信息显示屏以便于查看","involved_objects":  ["person1",  "person2",  "information_display"]}}}def  score_2(locs):def  distance(a,  b):return  math.sqrt((a[’x’]  -  b[’x’])**2  +  (a[’y’]  -  b[’y’])**2  +  (a[’z’]  -  b[’z’])**2)#  分组评分  (人员距离座位区的距离)grouping_score  =  sum(distance(locs[p],  locs[’seating_area’])  for  p  in  [’person1’,  ’person2’])#  对齐评分  (信息显示屏在座位区上方)alignment_score  =  abs(locs[’seating_area’][’y’]  -  locs[’information_display’][’y’])#  靠近评分  (人员靠近信息显示屏)proximity_score  =  sum(distance(locs[p],  locs[’information_display’])  for  p  in  [’person1’,  ’person2’])#  总评分total_score  =  grouping_score  +  alignment_score  +  proximity_scorereturn  1  -  total_score  /  100.'
- en: 'Figure 8: Example of annotated queries and scoring function'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：带注释的查询和评分函数示例
- en: 'scene_3  =  {"description":  "Three  boxes  of  different  sizes,  stacked  on  top  of  each  other","assets":  ["box1",  "box2",  "box3"],"relationships":  {"hierarchy":  {"description":  "The  boxes  should  be  in  descending  order  of  size  from  bottom  to  top","involved_objects":  ["box1",  "box2",  "box3"]},"layering":  {"description":  "The  boxes  should  be  placed  one  above  the  other","involved_objects":  ["box1",  "box2",  "box3"]}},}def  score_3(locs):#  Extracting  locationsz_box1,  z_box2,  z_box3  =  locs[’box1’][’z’],  locs[’box2’][’z’],  locs[’box3’][’z’]w_box1,  w_box2,  w_box3  =  locs[’box1’][’w’],  locs[’box2’][’w’],  locs[’box3’][’w’]#  Hierarchy  score  (sizes)hierarchy_score  =  0if  not  (w_box1  >  w_box2  >  w_box3):hierarchy_score  =  abs(w_box1  -  w_box2)  +  abs(w_box2  -  w_box3)#  Layering  score  (z-axis  positioning)layering_score  =  0if  not  (z_box1  <  z_box2  <  z_box3):layering_score  =  abs(z_box1  -  z_box2)  +  abs(z_box2  -  z_box3)#  Total  scoretotal_score  =  hierarchy_score  +  layering_scorereturn  1  -  total_score  /  100scene_4  =  {"description":  "A  new  solar  system  with  planets  orbiting  around  a  small  star","assets":  ["sun",  "planet1",  "planet2",  "planet3"],"relationships":  {"rotation":  {"description":  "Planets  should  orbit  around  the  sun","involved_objects":  ["planet1",  "planet2",  "planet3",  "sun"]},"scaling":  {"description":  "Planets  should  vary  in  size","involved_objects":  ["planet1",  "planet2",  "planet3"]}}}def  score_4(locs):import  mathdef  distance(a,  b):return  math.sqrt((a[’x’]  -  b[’x’])**2  +  (a[’y’]  -  b[’y’])**2  +  (a[’z’]  -  b[’z’])**2)#  Rotation  score  (distance  from  sun)rotation_score  =  sum(distance(locs[p],  locs[’sun’])  for  p  in  [’planet1’,  ’planet2’,  ’planet3’])#  Scaling  score  (size  of  planets)scaling_score  =  abs(locs[’planet1’][’size’]  -  locs[’planet2’][’size’])  +  abs(locs[’planet2’][’size’]  -  locs[’planet3’][’size’])#  Total  scoretotal_score  =  rotation_score  +  scaling_scorereturn  1  -  total_score  /  100'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'scene_3 = {"description": "三个不同大小的盒子，依次堆叠在一起", "assets": ["box1", "box2", "box3"],
    "relationships": {"hierarchy": {"description": "盒子应从底部到顶部按大小递减排序", "involved_objects":
    ["box1", "box2", "box3"]}, "layering": {"description": "盒子应一个接一个地放置", "involved_objects":
    ["box1", "box2", "box3"]}},}def score_3(locs):# 提取位置z_box1, z_box2, z_box3 = locs[’box1’][’z’],
    locs[’box2’][’z’], locs[’box3’][’z’]w_box1, w_box2, w_box3 = locs[’box1’][’w’],
    locs[’box2’][’w’], locs[’box3’][’w’]# 层级评分 (尺寸)hierarchy_score = 0if not (w_box1
    > w_box2 > w_box3):hierarchy_score = abs(w_box1 - w_box2) + abs(w_box2 - w_box3)#
    分层评分 (z轴定位)layering_score = 0if not (z_box1 < z_box2 < z_box3):layering_score
    = abs(z_box1 - z_box2) + abs(z_box2 - z_box3)# 总评分total_score = hierarchy_score
    + layering_scorereturn 1 - total_score / 100scene_4 = {"description": "一个新的太阳系，行星绕小恒星轨道运行",
    "assets": ["sun", "planet1", "planet2", "planet3"], "relationships": {"rotation":
    {"description": "行星应绕太阳运行", "involved_objects": ["planet1", "planet2", "planet3",
    "sun"]}, "scaling": {"description": "行星的大小应有所不同", "involved_objects": ["planet1",
    "planet2", "planet3"]}}}def score_4(locs):import mathdef distance(a, b):return
    math.sqrt((a[’x’] - b[’x’])**2 + (a[’y’] - b[’y’])**2 + (a[’z’] - b[’z’])**2)#
    旋转评分 (距离太阳)rotation_score = sum(distance(locs[p], locs[’sun’]) for p in [’planet1’,
    ’planet2’, ’planet3’])# 缩放评分 (行星大小)scaling_score = abs(locs[’planet1’][’size’]
    - locs[’planet2’][’size’]) + abs(locs[’planet2’][’size’] - locs[’planet3’][’size’])#
    总评分total_score = rotation_score + scaling_scorereturn 1 - total_score / 100'
- en: 'Figure 9: Example of annotated queries and scoring function'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：注释查询和评分函数的示例
- en: Appendix E Prompt Used at each stage
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 各阶段使用的提示
- en: 'The prompt used in SceneCraft is shown in Figure [10](#A5.F10 "Figure 10 ‣
    Appendix E Prompt Used at each stage ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '在 SceneCraft 中使用的提示如图[10](#A5.F10 "Figure 10 ‣ Appendix E Prompt Used at each
    stage ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code")所示。'
- en: 'query_find_assets  =  """I  am  writing  several  blender  scripts  to  generate  scenes  for:  %s.Please  think  step  by  step  and  then  give  me  the  assets  (each  is  a  single  unit,  avoid  a  composite  set  that  contains  multiple  objects)  that  shall  appear  in  these  scenes.After  explanation,  structured  in:  Output:  1)  x1:  y1;  2)  x2:  y2;  3)  ...  Each  with  a  general  descriptive  name  (x)  and  a  very  detailed  visual  description  (y)."""query_height_assets  =  """I  am  writing  several  blender  scripts  to  generate  scenes  for  %s.Below  are  the  assets  we’d  like  to  use.  Now  we  need  to  scale  them  to  correct  height,  please  generate  a  python  dictionary  called  height_dict,  where  key  is  each  asset’s  name,  and  value  is  a  number  representing  the  height  (measured  in  metre)%sOutput  the  complete  python  dict  via  height_dict  =  {asset_name:  height,  ...},  also  give  detailed  explanation  as  comment  before  the  value  in  the  dict."""query_plan_assets  =  """I  am  writing  several  blender  scripts  to  generate  a  scene  for  %s.Below  are  the  assets  I’d  like  to  use:%sNow  I  want  a  concrete  plan  to  put  them  into  the  scene.  Please  think  step  by  step,  and  give  me  a  multi-step  plan  to  put  assets  into  the  scene.For  each  step,  structure  your  output  as:layout_plan_i  =  {"title":  title_i,"asset_list"  :[asset_name_1,  asset_name_2],"description":  desc_i}where  title_i  is  the  high-level  name  for  this  step,  and  desc  is  detailed  visual  text  description  of  what  it  shall  look  like  after  layout.  asset_list  is  the  non-empty  list  of  assets  to  be  added  in  this  step.Please  think  step  by  step,  place  assets  from  environmental  ones  to  more  details  assets.  Return  me  a  list  of  python  dictonaries  layout_plan_1,  layout_plan_2,  ..."""prompt_graph  =  """You  are  tasked  with  constructing  a  relational  bipartite  graph  for  a  3D  scene  based  on  the  provided  description  and  asset  list.  Your  goal  is  to  identify  the  spatial  and  contextual  relationships  between  assets  and  represent  these  relationships  in  a  structured  format.  Follow  these  steps:1.  Review  the  scene  description  and  the  list  of  assets.2.  Determine  the  spatial  and  contextual  relationships  needed  to  accurately  represent  the  scene’s  layout.  Consider  relationships  like  proximity,  alignment,  parallelism,  etc.3.  Construct  the  relational  bipartite  graph  ‘G(s)  =  (A,  R,  E)‘  where:-  ‘A‘  represents  the  set  of  assets.-  ‘R‘  represents  the  set  of  relations  as  nodes.-  ‘E‘  represents  the  edges  connecting  a  relation  node  to  a  subset  of  assets  ‘E(r)‘  in  the  scene  that  satisfies  this  relation.4.  For  each  identified  relationship,  create  a  relation  node  and  link  it  to  the  appropriate  assets  through  edges  in  the  graph.Output  your  findings  in  a  structured  format:-  List  of  relation  nodes  ‘R‘  with  their  types  and  descriptions.-  Edges  ‘E‘  that  link  assets  to  their  corresponding  relation  nodes.This  process  will  guide  the  arrangement  of  assets  in  the  3D  scene,  ensuring  they  are  positioned,  scaled,  and  oriented  correctly  according  to  the  scene’s  requirements  and  the  relationships  between  objects."""'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Example of prompts being used'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：使用提示的示例
- en: '![Refer to caption](img/5f932a708a1a5d658f2522b32edb5b1e.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5f932a708a1a5d658f2522b32edb5b1e.png)'
- en: 'Figure 11: Questionnaire Interface, with three questions, about 1) Text Fidelity;
    2) Composition; 3) Aesthetics'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：问卷界面，包含三个问题，分别是 1) 文本忠实度；2) 组成；3) 美学
