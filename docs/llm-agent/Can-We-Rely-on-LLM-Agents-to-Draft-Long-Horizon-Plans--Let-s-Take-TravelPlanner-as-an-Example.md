<!--yml
category: 未分类
date: 2025-01-11 12:19:21
-->

# Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example

> 来源：[https://arxiv.org/html/2408.06318/](https://arxiv.org/html/2408.06318/)

Yanan Chen, Ali Pesaranghader, Tanmana Sadhu    Dong Hoon Yi
LG Electronics, Toronto AI Lab, Toronto, Canada
{yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com

###### Abstract

Large language models (LLMs) have brought autonomous agents closer to artificial general intelligence (AGI) due to their promising generalization and emergent capabilities. There is, however, a lack of studies on how LLM-based agents behave, why they could potentially fail, and how to improve them, particularly in demanding real-world planning tasks. In this paper, as an effort to fill the gap, we present our study using a realistic benchmark, TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)), where an agent must meet multiple constraints to generate accurate plans. We leverage this benchmark to address four key research questions: (1) are LLM agents robust enough to lengthy and noisy contexts when it comes to reasoning and planning? (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios with long context? (3) can we rely on refinement to improve plans, and (4) can fine-tuning LLMs with both positive and negative feedback lead to further improvement? Our comprehensive experiments indicate that, firstly, LLMs often fail to attend to crucial parts of a long context, despite their ability to handle extensive reference information and few-shot examples; secondly, they still struggle with analyzing the long plans and cannot provide accurate feedback for refinement; thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and negative feedback, resulting in substantial gains over Supervised Fine-Tuning (SFT). Our findings offer in-depth insights to the community on various aspects related to real-world planning applications.

Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example

Yanan Chen, Ali Pesaranghader, Tanmana Sadhu,  and Dong Hoon Yi LG Electronics, Toronto AI Lab, Toronto, Canada {yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com

## 1 Introduction

LLMs have shown significant reasoning and planning results against various benchmarks such as WebArena Zhou et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib58)), WebShop Yao et al. ([2022a](https://arxiv.org/html/2408.06318v1#bib.bib48)), AgentBench Liu et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib17)) and AgentGym Xi et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib44)) where they act as agents to finish a given task on behalf of humans. In this vein, the community considers two main directions for developing LLM-based agents: (1) prompting LLMs for reasoning, planning, and execution Qin et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib25)); Wei et al. ([2022](https://arxiv.org/html/2408.06318v1#bib.bib38)); Yao et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib49)); Wang et al. ([2022](https://arxiv.org/html/2408.06318v1#bib.bib37)), and (2) fine-tuning LLMs for a given task Chen et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib3)); Zeng et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib51)); Zhang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib53)); Chen et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib4)); Song et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib33)). Despite promising contributions in each direction, it is seen that LLMs still fall short in more complex scenarios. TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)), as an example, is a benchmark where an agent should generate a plan which must meet multiple constraints with respect to input queries. The authors showed that GPT-4-Turbo OpenAI ([2023](https://arxiv.org/html/2408.06318v1#bib.bib21)) could only reach to Final Pass Rate of 4.4%. This indicates that LLM agents cannot handle long-horizon reasoning and planning. In this paper, we investigate these challenges further with four research questions using TravelPlanner as the benchmark, and we trust that our promising and negative findings will benefit the community.

Our extensive experiments indicate that (1) lengthy and noisy context can adversely impact planning ability of the LLM agent, (2) more shots do not necessarily guarantee performance improvement, (3) refinement may not be effective when LLMs are employed as feedback generators; however, it is more likely to work if the feedback generator is based on heuristic rules, and (4) feedback-aware fine-tuning (FAFT), our proposed approach, inspired by negative aware training (NAT) Wang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)), can show remarkable improvement in planning.

![Refer to caption](img/240e04fcecb0b74444b245ef999cbb3a.png)

Figure 1: Four LLM agents interact to generate a plan. (Fig. [A.1](https://arxiv.org/html/2408.06318v1#A1.F1 "Figure A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") is an example for the refinement module.)

## 2 Methodology

Our framework, built upon TravelPlanner, consists of five main components: Scrubber, Planner, Feedback Generator, Refiner, and the Evaluation module (as shown in Fig. [1](https://arxiv.org/html/2408.06318v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). The scrubber provides clean reference information¹¹1This is a terminology that TravelPlanner uses to refer to necessary information for generating a plan. and few-shot examples to the Planner for generating a plan. Then, the Feedback Generator provides feedback to the Refiner for improving the plan if required. The interaction continues until the pre-defined settings are met. The Planner is the core of the framework which can be based on either (1) in-context learning (ICL), or (2) supervised fine-tuning (SFT), e.g., FAFT as proposed in Section [4](https://arxiv.org/html/2408.06318v1#S4 "4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")-RQ4\. Appx. [A.3](https://arxiv.org/html/2408.06318v1#A1.SS3 "A.3 Framework ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") describes each agent in detail.

## 3 Experimental Settings

Basic Setting. Since the focus of our work is on agents’ capabilities in drafting plans, we only rely on the Sole Planning setting from TravelPlanner. That is, all comprehensive and necessary information, which are human annotations, is directly provided to the planner agent. We also consider the Direct²²2the query is input directly into the model along with instructions detailing the task and relevant information gathered. planning strategy for its simplicity because it performs at a similar level to other reasoning techniques such as ZS-CoT Wei et al. ([2022](https://arxiv.org/html/2408.06318v1#bib.bib38)), ReAct Yao et al. ([2022b](https://arxiv.org/html/2408.06318v1#bib.bib50)) and Reflexion Shinn et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib30)).

Dataset. (See Appx. [A.1](https://arxiv.org/html/2408.06318v1#A1.SS1 "A.1 Dataset ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")) We use the training set for both few-shot prompting and fine-tuning because it provides annotated plans. We evaluate the agent against both validation and test sets for RQ1 and RQ2 in Section [4](https://arxiv.org/html/2408.06318v1#S4 "4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"). As for RQ3, we consider only the validation set because we do not have access to the system feedback offline. Regarding RQ4, we use the training set for fine-tuning the (Open-LLM) planner agent.

Metrics. We utilize the original evaluation metrics from TravelPlanner, which evaluate performance based on the pass rates of multiple constraints. Additional details are available in Appx. [A.2](https://arxiv.org/html/2408.06318v1#A1.SS2 "A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example").

 | GPT-3.5-Turbo as Planner |
|  |  | Validation Set (#180) | Test Set (#1,000) |
| Reference Scrubbed? | Num. Shots | Delivery Rate |  

&#124; Commonsense &#124;
&#124; Pass Rate &#124;

 |  

&#124; Hard Constraint &#124;
&#124; Pass Rate &#124;

 | Final Pass Rate | Halluc. Rate | Delivery Rate |  

&#124; Commonsense &#124;
&#124; Pass Rate &#124;

 |  

&#124; Hard Constraint &#124;
&#124; Pass Rate &#124;

 | Final Pass Rate | Halluc. Rate |
| (RQ1) | (RQ2) | Micro | Macro | Micro | Macro | Micro | Macro | Micro | Macro |
| No | 0 | 100 | 60.2 | 4.4 | 11.0 | 2.8 | 0.0 | 57.4 | 100 | 60.8 | 3.5 | 13.6 | 4.9 | 0.6 | 61.1 |
| No | 1 | 100 | 65.4 | 11.0 | 17.5 | 5.1 | 1.0 | 52.3 | 100 | 64.0 | 10.1 | 16.1 | 6.4 | 1.2 | 59.4 |
| Yes | 0 | 100 | 74.4 | 18.9 | 29.0 | 14.4 | 4.4 | 41.6 | 100 | 70.3 | 12.3 | 25.0 | 10.7 | 2.7 | 49.8 |
| Yes | 1 | 100 | 80.6 | 24.4 | 40.2 | 17.8 | 7.2 | 35.5 | 100 | 78.0 | 18.6 | 36.1 | 17.7 | 4.9 | 40.8 |
| Yes | 2 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | 38.8 | 100 | 80.9 | 22.4 | 34.3 | 16.7 | 6.5 | 44.3 |
| Yes | 4 | 100 | 81.5 | 29.4 | 35.5 | 12.2 | 5.8 | 46.6 | 100 | 80.3 | 21.1 | 31.5 | 15.0 | 5.2 | 50.8 |
| Yes | 5 | 100 | 81.1 | 26.3 | 32.6 | 12.4 | 4.8 | 49.8 | 100 | 79.5 | 20.4 | 30.4 | 13.2 | 5.6 | 53.1 | 

Table 1: Performance of GPT-3.5-Turbo as the Planner agent for different settings for RQ1 and RQ2

 |  |  | GPT-3.5-Turbo as Planner and GPT-4-Turbo as Refiner |
|  |  | vs. Validation Set (#180) |
| Feedback Generator (RQ3) | Refinement Iteration | Delivery Rate |  

&#124; Commonsense &#124;
&#124; Pass Rate &#124;

 |  

&#124; Hard Constraint &#124;
&#124; Pass Rate &#124;

 | Final Pass Rate | Uplift Ratio ($\uparrow$) | Flat Ratio ($\downarrow$) | Downgrade Ratio ($\downarrow$) |
| Micro | Macro | Micro | Macro |
| None | 0 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | – | – | – |
| Oracle (Heuristic Rules) | 1 | 100 | 89.7 | 51.1 | 50.0 | 22.2 | 11.7 | 46.1 | 52.8 | 1.1 |
| 2 | 100 | 89.0 | 54.4 | 50.5 | 18.3 | 12.8 | 8.9 | 76.7 | 14.4 |
| 3 | 100 | 89.9 | 56.1 | 50.7 | 21.1 | 13.3 | 13.9 | 78.9 | 7.2 |
| 4 | 100 | 89.1 | 59.4 | 49.8 | 21.7 | 13.9 | 5.0 | 86.7 | 8.3 |
| Random | 1 | 100 | 82.3 | 31.1 | 47.1 | 21.1 | 7.2 | 21.7 | 53.3 | 25.0 |
| 2 | 100 | 82.3 | 32.2 | 46.2 | 20.0 | 8.3 | 18.3 | 63.9 | 17.8 |
| 3 | 100 | 82.0 | 30.6 | 45.5 | 20.0 | 7.2 | 19.4 | 61.7 | 18.9 |
| 4 | 100 | 82.6 | 30.6 | 44.8 | 18.3 | 7.2 | 18.9 | 62.8 | 18.3 |
| GPT-3.5-Turbo (0125) | 1 | 100 | 82.0 | 24.4 | 41.4 | 21.7 | 8.9 | 22.2 | 52.8 | 25.0 |
| 2 | 100 | 82.9 | 28.9 | 40.9 | 18.3 | 8.9 | 24.4 | 55.0 | 20.6 |
| 3 | 100 | 83.8 | 27.8 | 41.7 | 20.0 | 8.9 | 25.0 | 56.1 | 18.9 |
| 4 | 100 | 82.4 | 26.7 | 40.7 | 18.9 | 7.8 | 19.4 | 57.8 | 22.8 |
| GPT-4-Turbo (1106-preview) | 1 | 100 | 86.9 | 32.8 | 39.3 | 20.0 | 9.4 | 34.4 | 40.6 | 25.0 |
| 2 | 100 | 84.3 | 29.4 | 37.9 | 15.6 | 7.2 | 20.0 | 59.4 | 20.6 |
| 3 | 100 | 84.6 | 30.0 | 40.5 | 18.3 | 7.2 | 18.3 | 67.2 | 14.4 |
| 4 | 100 | 86.4 | 28.3 | 37.9 | 20.0 | 6.7 | 19.4 | 58.3 | 22.2 | 

Table 2: Performance of different Feedback Generators. Uplift Ratio ($\uparrow$), Flat Ratio ($\downarrow$), and Downgrade Ratio ($\downarrow$) show what percentage of plans has improved, not changed, and deteriorated, respectively.

## 4 Findings

RQ1: Are LLM agents robust enough to noisy information for reasoning and planning? Table [1](https://arxiv.org/html/2408.06318v1#S3.T1 "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") shows that GPT-3.5-Turbo has a better performance when it receives *shrunk* reference information. This indicates that GPT-3.5-Turbo still struggles to attend to the most important parts of a given context for reasoning, prone to excessive irrelevant (context) chunks. Therefore, it is worth considering an external intelligent context-cleaning agent.

RQ2: Can more shots help with the planning task, or does it worsen hallucination? It is commonly accepted that having more few-shots is helpful in ICL, but does it apply to TravelPlanner? As Table [1](https://arxiv.org/html/2408.06318v1#S3.T1 "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") shows, the Final Pass rate reaches its highest value when there are $2$ shots, while having more shots may not improve if not hurt more. We presume that more shots in the context window may distract the LLM and lead to hallucination (e.g., using entities that do not exist in the given reference information). The results of the Hallucination Rate attest to this assumption. That is, giving more shots may potentially cause severer hallucination in tasks where the context of the reference information is complex tabular texts. Another finding is that at least one in-context example is beneficial Xie and Min ([2022](https://arxiv.org/html/2408.06318v1#bib.bib46)). Finally, we conclude that as we have more shots, the pass rate and hallucination rate results worsen.

Our RQ1 and RQ2 observations align with the existing theoretical and experimental works, e.g., Han et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib10)); Levy et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib13)), which identify the potential causes underlying current LLMs’ failure in length generalization, that when they encounter a much longer context, the attention scores are diluted, and thus the score distribution becomes flat leading to information loss. That is, the entropy of the attention score will explode with increasing context. In other words, LLMs become lost in how to focus on the right information, especially when pre-training is done on shorter text segments.

RQ3: Can we rely on refinement to improve plans? To address this, we require feedback that highlights what went wrong, accompanied by explanations of the reasons behind the issues. For that, we examine the reliability of GPT-3.5-Turbo and GPT-4-Turbo as LLM-based feedback generators. In addition, as an ablative point of view, we also consider Random and Oracle feedback generators. The former refers to the setting where we fabricate the feedback using random content in a valid format, and the latter uses heuristic hard-coded rules³³3Rules from TravelPlanner: [https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation](https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation) to check whether the plan complies with the constraints. Furthermore, we do not consider any weaker language models because they have shown to be incapable of handling this kind of task in previous studies Madaan et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib20)). We only focus on commonsense constraints in this part due to the frequent absence of hard constraints in the queries and the feedback⁴⁴4 Consistent with the original setting of TravelPlanner, i.e., plans that fail to satisfy all commonsense constraints will not proceed to receive feedback regarding hard constraints. This decision is rooted in the dependency of hard constraint computation on commonsense criteria.. We present the results for RQ3 in Table [2](https://arxiv.org/html/2408.06318v1#S3.T2 "Table 2 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"). The feedback generator and the Refiner agent interact iteratively; at each iteration, the previously generated plans are reviewed by the feedback generator to draft feedback subjectively. Considering the feedback, if any refinement is needed, i.e., any constraint is not met, the Refiner agent is triggered to modify the plan. *This design simulates the production-level environment where no Oracle feedback generator is available to check whether a plan needs refinement.* We summarize our findings as follows:

*   •

    Refinement can help improve the plans if the feedback is of high quality and precise – We see that the refinement helps with improving the plans if the feedback is accurate and well-organized as in the Oracle setting. The table shows, in the first iteration, $46.1\%$ of the plans are improved and the final pass rate is lifted from $7.2\%$ to $11.7\%$. From the second iteration, we do not see any significant improvement and the pass rates saturate.

*   •

    LLM feedback generators are not reliable – The LLM-based feedback generators, equipped with meticulously designed prompts with two-shots, still struggle with writing unerring feedback. Specifically, for faulty plans, these feedback generators cannot identify where the violation is or write excessive (baseless) feedback. Additionally, for qualified plans, they may generate false negative feedback which triggers the refinement module and causes unnecessary modification potentially leading to an invalid plan. As a result, the overall performance becomes stagnant, i.e., the Flat Ratio dominates, and modifications in a negative direction (downgrade ratio) have counteracted the positive changes (uplift ratio).

RQ4: Can we enhance the development of a superior planner by employing our feedback-aware fine-tuning (FAFT) technique, as opposed to relying on off-the-shelf proprietary LLMs? For plan generation, we can use the Oracle feedback for in-context learning (as shown in RQ3) or fine-tuning an (open-source) LLM. The focus of this experiment lies in the latter aspect, where we examine the performance of SFT and FAFT in building the Planner agent.

SFT vs. FAFT – In our proposed approach, i.e., FAFT, we extend beyond the considerations of query, reference information, and annotated plan as in SFT, by also incorporating feedback into the fine-tuning process (Appx. [A.4](https://arxiv.org/html/2408.06318v1#A1.SS4 "A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). To generate feedback, we initially use the queries from the training set and prompt the Planner agent to generate plans⁵⁵5Under the same setting as in RQ1 and RQ2.. Subsequently, we gather feedback by evaluating the generated plans using the Oracle (i.e., the system). We set `temperature` to $1.0$ for plan generation to enhance the diversity of the generated plans, consequently introducing a broader range of positive and negative feedback samples. We collected $14,800$ samples including $45$ original annotated plans from the training set together with their `all-success` feedback⁶⁶6It is noteworthy that more samples could be collected. (Appx. [A.6.2](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS2 "A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). During inference, in the prompt, the feedback will be set to `all-success`, aiming to encourage the model to generate a correct plan. Appx. [A.4.3](https://arxiv.org/html/2408.06318v1#A1.SS4.SSS3 "A.4.3 Inference Example Template for FAFT ‣ A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") provides more information.

Table [3](https://arxiv.org/html/2408.06318v1#S4.T3 "Table 3 ‣ 4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") presents the impact of FAFT where a significant improvement is witnessed across all pass rates, compared to Vanilla Llama-3-8B and its SFT version. This observation aligns with the previous studies, e.g., Negative-Aware Training (NAT) Wang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)), that the performance can be boosted by increasing the diversity of prompts. In FAFT, elaborative and rich feedback acts as thought chains to improve the agent’s planning. Further, our results validate the recent works Lee et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib12)); Wei et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib39)) by suggesting that (1) injecting auxiliary information in conventional SFT data can markedly improve the performance, and (2) a CoT-style training set and detailed scratchpads can significantly improve learning by reducing sample complexity.

Our findings advocate that when annotation is scarce while interaction with the system is affordable, collecting samples with comprehensive and rich feedback (either positive or negative), can be worthwhile. This approach can be seen as a promising alternative to RL-based solutions, such as PPO Schulman et al. ([2017](https://arxiv.org/html/2408.06318v1#bib.bib28)), which has been criticized for instability.

 | Llama-3-8B as Planner |
| Planner (RQ4) | Delivery Rate |  

&#124; Commonsense &#124;
&#124; Pass Rate &#124;

 |  

&#124; Hard Constraint &#124;
&#124; Pass Rate &#124;

 | Final Pass Rate |
| Micro | Macro | Micro | Macro |
| Vanilla | 94.4 | 49.5 | 1.1 | 7.9 | 0.0 | 0.0 |
| + SFT | 97.8 | 64.2 | 11.1 | 12.4 | 6.1 | 3.9 |
| + FAFT | 98.9 | 81.7 | 28.9 | 36.9 | 15.0 | 8.3 | 

Table 3: Performance of Llama-3-8B +SFT and +FAFT.

## 5 Conclusion

In this paper, we studied the impacts of context, the number of shots, and the utilization of feedback on a complex long-horizon planning task known as TravelPlanner. Our findings aim to advance a broader spectrum of agentic frameworks and strategies within the research community.

For future work, we plan to explore methods that incorporate annotated shots in SFT and post-training. This approach can address the bottleneck where LLMs’ knowledge and skills are predominantly acquired during pre-training, while alignment SFT teaches the model which sub-distribution of formats to use when interacting with users Zhou et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib57)). Finally, we will explore the interplay between RLHF and FAFT.

## Limitations

Due to budget constraints, we were only able to use GPT-3.5-Turbo as the Planner agent for RQ1 and RQ2\. For RQ4, further investigations are needed to explore the relationship between the magnitude of gains and the size of the FAFT training set, as well as the impact of the ratio of positive to negative samples on the final performance. Additionally, enhancing the feedback expressions could further improve the performance of FAFT. It would also be interesting to investigate RLHF techniques, such as DPO Rafailov et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib26)) and PRO Song et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib32)), to better utilize feedback.

## Ethics Statement

Our work is founded upon TravelPlanner, a benchmark designed for complex planning tasks. We adhere to the original work’s specifications, utilizing their data, evaluation scripts, and definitions of commonsense. Acknowledging the foundational concepts and designs of the original benchmark, we strictly adhere to TravelPlanner’s guidelines, ensuring the integrity of the evaluation process by prohibiting any form of cheating in the validation and test sets. This commitment upholds the fairness and reliability of this work.

As for environmental cost, we acknowledge that our work necessitated extensive experiments to derive robust conclusions. However, future endeavours can leverage these insights, potentially reducing the need for numerous large-scale comparisons. Models intended for production could undergo training once, utilizing the most promising settings identified through our research.

## References

*   Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. *ACM Transactions on Intelligent Systems and Technology*, 15(3):1–45.
*   Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. 2023a. Improving code generation by training with natural language feedback. *arXiv preprint arXiv:2303.16749*.
*   Chen et al. (2023b) Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023b. Fireact: Toward language agent fine-tuning. *arXiv preprint arXiv:2310.05915*.
*   Chen et al. (2024) Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and methods of effective agent tuning for large language models. *arXiv preprint arXiv:2403.12881*.
*   Christianos et al. (2023) Filippos Christianos, Georgios Papoudakis, Matthieu Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran, Xidong Feng, Jiacheng Liu, et al. 2023. Pangu-agent: A fine-tunable generalist agent with structured reasoning. *arXiv preprint arXiv:2312.14878*.
*   Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: Towards a generalist agent for the web](http://arxiv.org/abs/2306.06070).
*   Fei et al. (2023) Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, and Wei Han. 2023. Extending context window of large language models via semantic compression. *arXiv preprint arXiv:2312.09571*.
*   gkamradt (2023) gkamradt. 2023. Llmtest needle in a haystack - pressure testing llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
*   Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*.
*   Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large language models. *arXiv preprint arXiv:2308.16137*.
*   Kim et al. (2024) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language models can solve computer tasks. *Advances in Neural Information Processing Systems*, 36.
*   Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. 2023. Teaching arithmetic to small transformers. *arXiv preprint arXiv:2307.03381*.
*   Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task, more tokens: the impact of input length on the reasoning performance of large language models. *arXiv preprint arXiv:2402.14848*.
*   Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2024. Camel: Communicative agents for" mind" exploration of large language model society. *Advances in Neural Information Processing Systems*, 36.
*   Li et al. (2023) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun, Xinglin Wang, Heda Wang, and Kan Li. 2023. [Turning dust into gold: Distilling complex reasoning capabilities from llms by leveraging negative data](https://api.semanticscholar.org/CorpusID:266375154). In *AAAI Conference on Artificial Intelligence*.
*   Liu et al. (2023a) Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian. 2023a. Tcra-llm: Token compression retrieval augmented large language model for inference cost reduction. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 9796–9810.
*   Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: Evaluating llms as agents. *arXiv preprint arXiv: 2308.03688*.
*   Liu et al. (2024) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. 2024. Agentlite: A lightweight library for building and advancing task-oriented llm agent system. *arXiv preprint arXiv:2402.15538*.
*   Ma et al. (2024) Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents. *arXiv preprint arXiv:2401.13178*.
*   Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in Neural Information Processing Systems*, 36.
*   OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774). *arXiv preprint arXiv:2303.08774*.
*   Pan et al. (2024) Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 2024. Autonomous evaluation and refinement of digital agents. *arXiv preprint arXiv:2404.06474*.
*   Paul et al. (2023) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback on intermediate representations. *arXiv preprint arXiv:2304.01904*.
*   Qian et al. (2024) Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia Zhou, Xu Chen, and Zhicheng Dou. 2024. Are long-llms a necessity for long-context tasks? *arXiv preprint arXiv:2405.15318*.
*   Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. *arXiv preprint arXiv:2307.16789*.
*   Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. *Advances in Neural Information Processing Systems*, 36.
*   Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. [Parallel context windows for large language models](https://doi.org/10.18653/v1/2023.acl-long.352). In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 6383–6402, Toronto, Canada. Association for Computational Linguistics.
*   Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*.
*   Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In *International Conference on Machine Learning*, pages 31210–31227\. PMLR.
*   Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. *Advances in Neural Information Processing Systems*, 36.
*   Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.
*   Song et al. (2024a) Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2024a. Preference ranking optimization for human alignment. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, pages 18990–18998.
*   Song et al. (2024b) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024b. Trial and error: Exploration-based trajectory optimization for llm agents. *arXiv preprint arXiv:2403.02502*.
*   Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023. Multi-agent collaboration: Harnessing the power of intelligent llm agents. *arXiv preprint arXiv:2306.03314*.
*   Wang et al. (2024a) Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024a. Mixture-of-agents enhances large language model capabilities. *arXiv preprint arXiv:2406.04692*.
*   Wang et al. (2024b) Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy Baldwin. 2024b. Learning from failure: Integrating negative examples when fine-tuning large language models as agents. *arXiv preprint arXiv:2402.11651*.
*   Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
*   Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. *Advances in neural information processing systems*, 35:24824–24837.
*   Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120*.
*   Wu et al. (2023a) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. *arXiv preprint arXiv:2308.08155*.
*   Wu et al. (2024) Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. 2024. How easily do irrelevant inputs skew the responses of large language models? *arXiv preprint arXiv:2404.03302*.
*   Wu et al. (2023b) Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b. An empirical study on challenging math problem solving with gpt-4. In *ArXiv preprint arXiv:2306.01337*.
*   Xi et al. (2024a) Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. 2024a. Training large language models for reasoning through reverse curriculum reinforcement learning. *arXiv preprint arXiv:2402.05808*.
*   Xi et al. (2024b) Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, and Yu-Gang Jiang. 2024b. [Agentgym: Evolving large language model-based agents across diverse environments](http://arxiv.org/abs/2406.04151).
*   Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world planning with language agents. *arXiv preprint arXiv:2402.01622*.
*   Xie and Min (2022) Sang Michael Xie and Sewon Min. 2022. How does in-context learning work? a framework for understanding the differences from traditional supervised learning.
*   Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. *arXiv preprint arXiv:2312.13771*.
*   Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022a. Webshop: Towards scalable real-world web interaction with grounded language agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.
*   Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. *Advances in Neural Information Processing Systems*, 36.
*   Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*.
*   Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for llms. *arXiv preprint arXiv:2310.12823*.
*   Zhang et al. (2024a) Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang, and Yong Liu. 2024a. Meta-task planning for language agents. *arXiv preprint arXiv:2405.16510*.
*   Zhang et al. (2024b) Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. 2024b. Agentohana: Design unified data and training pipeline for effective agent learning. *arXiv preprint arXiv:2402.15506*.
*   Zhang et al. (2024c) Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. 2024c. Training language model agents without modifying language models. *ICML’24*.
*   Zhao et al. (2024) Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Longagent: Scaling language models to 128k context through multi-agent collaboration. *arXiv preprint arXiv:2402.11550*.
*   Zheng et al. (2024) Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. Gpt-4v (ision) is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*.
*   Zhou et al. (2024a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024a. Lima: Less is more for alignment. *Advances in Neural Information Processing Systems*, 36.
*   Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023. [Webarena: A realistic web environment for building autonomous agents](https://webarena.dev). *arXiv preprint arXiv:2307.13854*.
*   Zhou et al. (2024b) Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. 2024b. Archer: Training language model agents via hierarchical multi-turn rl. *arXiv preprint arXiv:2402.19446*.
*   Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. *arXiv preprint arXiv:2306.04528*.

## Appendix A Appendix

### A.1 Dataset

The TravelPlanner dataset⁷⁷7TravelPlanner Dataset: [https://huggingface.co/datasets/osunlp/TravelPlanner](https://huggingface.co/datasets/osunlp/TravelPlanner) consists of three splits of training, validation, and test sets as follows:

*   •

    The Training Set consists of $45$ triplets of query, reference, and human annotated plan. The annotations are used as demonstrations for in-context learning or supervised fine-tuning in our paper. Please note that these annotated plans are merely a subset of many feasible plans. As expected, the Oracle (i.e., system) returns the feedback for the annotations where no issue is raised (Appx. [A.6.2](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS2 "A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")).

*   •

    The Validation Set comes with $180$ pairs of query and reference, with no annotated plans.

*   •

    The Test Set holds $1,000$ queries together with their references, without any annotated plans.

For a given query, agents are expected to formulate a (comprehensive) plan which includes transportation, restaurants, attractions, and accommodation for each day (Appx. [A.6.1](https://arxiv.org/html/2408.06318v1#A1.SS6.SSS1 "A.6.1 Query Example with its Travel Plan ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") shows an example).

### A.2 Evaluation metrics

Following TravelPlanner, we use automatic evaluation metrics to assess whether a plan generated by the agent meets the (correct) format condition as well as all the constraints.

*   •

    Delivery Rate measures whether the agent could successfully generate a plan within a limited number of steps. Falling into any dead loops or invalid plan formats leads to failure. In the sole-planning setting, any failure in drafting a plan negatively impacts the delivery rate.

*   •

    Commonsense Constraint Pass Rate assesses whether the agent can incorporate commonsense while drafting plans without explicit instructions. For example, the agent has to pick valid entities (incl. restaurants, hotels, etc.) from the reference information and not hallucinate.

*   •

    Hard Constraint Pass Rate measures whether a plan meets all hard constraints mentioned in the query, e.g., budget limit, cuisine preference, or accommodation type.
    N.B. For Commonsense and Hard Constraint Pass Rates, the evaluation is done in two ways, Micro and Macro, which evaluate the agent’s capability of following individual constraints vs. all the constraints holistically Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)).

*   •

    Final Pass Rate measures whether a plan satisfies all hard and commonsense constraints.

*   •

    Hallucination Rate measures whether a plan contains entities that cannot be found in the reference information.

TravelPlanner’s Leaderboard⁸⁸8TravelPlanner Leaderboard: [https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard](https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard) let us evaluate the performance of agents against both validation and test sets online. This creates a stage for fair evaluation for all researchers. We use this leaderboard to calculate the figures for the validation and test sets for our experiments. We run each five times, with a different random seed, and report the average scores.

![Refer to caption](img/d4c14f06212d1f05298fde7b04852e6e.png)

Figure A.1: Toy Example: The Planner initially generates a plan w.r.t. query and reference, then the feedback generator generates feedback considering the commonsense constraints. Then, the Refiner modifies the plan to meet the requirements for all constraints.

### A.3 Framework

In Fig. [1](https://arxiv.org/html/2408.06318v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"), we show that the Planner agent generates a plan for a given query and (cleaned) reference information. In TravelPlanner’s Two-Staging setting, the reference information is collected by an upstream tool agent which gathers valid information related to transportation, dining, attractions, and accommodation from their corresponding source files. The original benchmark also particularly creates valid reference information for the Sole Planning setting where the focus is on the Planner agent. Hence, we evaluate our solution only in the Sole Planning setting since our focus is on planning.

#### A.3.1 The Scrubber Agent

Since the reference information is massive and lengthy (i.e., $10,000$ tokens on average), we propose the Scrubber, a filtration agent, which infers the hard constraints from the query. There are $5$ hard constraints: `Room Rule`, `Room Type`, `Cuisine`, `Budget` and `Transportation`. We let the Scrubber to predict the exact constraint value based on the query, for example, one or several cuisine preferences from the set: {`American`, `Chinese`, `French`, `Indian`, `Italian`, `Mediterranean`, `Mexican`}. Internally within the Scrubber, we inject the whole training set as few-shot examples on top of the test query, to improve the accuracy⁹⁹9We utilize GPT-4-Turbo and achieve nearly $100\%$ accuracy.. Then, during inference, with the Scrubber agent, each predicted hard constraint is used to remove the rows (from the tables in the reference information) that are not used to produce the final plan. For example, if the predicted cuisine preferences are `Italian`, `Mediterranean`, then any restaurants that cannot provide these two cuisine types are removed. So, after removal, the length of reference information becomes shorter. Besides, we manually removed several irrelevant columns to the final planning task, such as ratings, phone numbers, and websites, from the tables in the reference. These two efforts massively shorten the long reference information by around $60\%$. It is noteworthy that there are still many choices available to the Planner agent for drafting a correct plan. For example, if the user’s budget for a trip is $\$8,000$, after removing the hotels whose prices are above this limit, there are still other choices left for the Planner agent to reason and draft a plan to meet the budget and other constraints. The prompt for the Scrubber agent is found in Appx. [A.5.1](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS1 "A.5.1 The Scrubber’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example").

#### A.3.2 The Feedback Generator and Refiner

Once the original plan has been drafted, refinement is conducted in an iterative manner. For this, we follow previous works where two agents are separately created with natural language communication capabilities.

The Feedback Generator which is responsible for generating nuanced task-dependent feedback that addresses multiple constraints. We tailor a prompt, as shown in Appx. [A.5.2](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS2 "A.5.2 Feedback Generator’s Prompt ‣ A.5 Prompt Templates for Agents ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"), to ask LLMs to write feedback with regard to commonsense constraints. In the instructions, we provide a list of constraints with their descriptions. Here two-shots are used to help with feedback generation. The shots are randomly selected from the training set.

The Refiner Agent refines the generated plan based on the feedback received from the Feedback Generator towards a better version (see the prompt in Appx. [A.5.3](https://arxiv.org/html/2408.06318v1#A1.SS5.SSS3 "A.5.3 The Refiner’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")).

Fig. [A.1](https://arxiv.org/html/2408.06318v1#A1.F1 "Figure A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") illustrates the entire refinement phase. The feedback points out that there is a repeated attraction for Days $1$ and $2$, and the accommodation does not satisfy the minimum number of nights requirement. Then, the Refiner agent refines this draft plan into a new plan where the attraction for the first day is replaced to avoid repetition, and another hotel is chosen which allows a two-night stay. Finally, based on the system assessment, the refined plan meets all commonsense constraints.

### A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning

#### A.4.1 Training Example Template for SFT

The TravelPlanner training set consists of $45$ samples with annotated plans. We use reference information, queries, and annotated plans for general SFT (which is a baseline).

[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDoge3JlZn0KcXVlcnk6IHtxdWVyeX0KZHJhZnQgdHJhdmVsIHBsYW46IHtwbGFufQ==)reference  information  box:  {ref}query:  {query}draft  travel  plan:  {plan}

#### A.4.2 Training Example Template for FAFT

[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KZHJhZnQgdHJhdmVsIHBsYW46e3BsYW59)reference  information  box:{ref}query:{query}feedback:{feedback}draft  travel  plan:{plan}

#### A.4.3 Inference Example Template for FAFT

[⬇](data:text/plain;base64,cmVmZXJlbmNlIGluZm9ybWF0aW9uIGJveDp7cmVmfQpxdWVyeTp7cXVlcnl9CmZlZWRiYWNrOntmZWVkYmFja30KaXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2VzcwpkcmFmdCB0cmF2ZWwgcGxhbjo=)reference  information  box:{ref}query:{query}feedback:{feedback}is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  successdraft  travel  plan:

#### A.4.4 Fine-tuning Setup

In RQ4, for the Planner agent, we fine-tune Llama3-8B for $3$ epochs with a batch size of $4$ for both SFT and FAFT. We use a constant scheduler learning rate of $5\times 10^{-5}$ and no warm-up, and we disable packing among training samples to avoid cross-contamination. We train the model in $4$-bit. The maximum sequence length is set to $7000$ to allow the training context to cover all samples. For computation and memory efficiency, we also use Low-Rank Adaptation with $r=16$ and $alpha=16$.

### A.5 Prompt Templates for Agents

#### A.5.1 The Scrubber’s Prompt Template

[⬇](data:text/plain;base64,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)Can  you  assist  in  creating  a  5-day  travel  itinerary  starting  in  Sacramento  and  covering  2  cities  in  Washington  state  from  March  22nd  to  March  26th,  2022?  The  journey  will  be  for  a  group  of  three  with  a  budget  of  $3,600.  We  require  accommodations  that  provide  entire  rooms  and  do  not  plan  to  travel  by  flight.  As  far  as  cuisines  are  concerned,  we’d  love  to  experience  American,  Mediterranean,  Italian,  and  French  during  our  trip.===>  [’American’,  ’Mediterranean’,  ’Italian’,  ’French’]Can  you  help  with  generating  a  7-day  travel  plan  for  a  party  of  5?  We’re  setting  off  from  Indianapolis  and  planning  to  explore  3  cities  in  Colorado  from  March  11th  to  March  17th,  2022.  We  have  a  budget  of  $15,100  for  this  trip.  We’ll  be  bringing  our  pets,  so  pet-friendly  accommodations  are  a  must.  We’re  also  hoping  to  find  places  that  offer  Mexican,  Italian,  Mediterranean,  and  Indian  cuisines.  Entire  rooms  for  accommodations  would  be  ideal.===>  [’Mexican’,  ’Italian’,  ’Mediterranean’,  ’Indian’]Can  you  assist  in  creating  a  travel  itinerary  for  a  group  of  4,  starting  in  Seattle  and  visiting  3  unique  cities  across  Texas?  This  trip  will  span  over  7  days  from  March  10th  through  March  16th,  2022.  We  have  a  budget  of  $11,000.  Regarding  our  accommodations,  we  would  like  to  rent  entire  rooms,  and  it’s  important  that  our  lodgings  allow  parties.  As  for  transportation,  we  do  not  plan  to  drive  ourselves  around.===>  []...{45  shots  from  trainset}...I  need  your  help  to  plan  a  5-day  vacation  for  a  group  of  4  people.  We’re  departing  from  Honolulu  and  planning  to  visit  2  cities  in  California  from  March  19th  to  March  23rd,  2022.  The  budget  for  our  trip  is  $11,200.  For  food  preferences,  we  enjoy  Mediterranean  and  Mexican  dishes.===>{inference  for  the  cuisine  preference}

#### A.5.2 Feedback Generator’s Prompt

[⬇](data:text/plain;base64,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)Now  You  are  an  advanced  reasoning,  analyzing  and  advisory  agent  who  can  write  feedback  and  insights  for  a  given  draft  travel  plan,  based  on  the  given  query  and  reference  information  box.The  feedback  you  write  should  check  and  judge  if  the  given  draft  travel  plan  violates  one  or  several  following  constraints:*  is_reasonalbe_visiting_city:  {success  or  fail}.  This  refers  to  Reasonable  City  Route:  Changes  in  cities  during  the  trip  must  be  reasonable.*  is_valid_restaurants:  {success  or  fail}.  This  refers  to  Diverse  Restaurants:  Restaurant  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_attractions:  {success  or  fail}.  This  refers  to  Diverse  Attractions:  Attraction  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_accommodation:  {success  or  fail}.  This  refers  to  Minimum  Nights  Stay:  The  number  of  consecutive  days  spent  in  a  specific  accommodation  during  the  trip  must  meet  the  corresponding  required  minimum  number  of  nights’  stay.*  is_valid_transportation:  {success  or  fail}.  This  refers  to  No  conflict  Transportation:  Transportation  choices  within  the  trip  must  be  reasonable.  For  example,  having  both  "self-driving"  and  "flight"  would  be  considered  a  conflict.*  is_valid_information_in_current_city:  {success  or  fail}.  This  refers  to  Within  Current  City:  All  scheduled  activities  for  the  day  must  be  located  within  that  day’s  city(s).*  is_valid_information_in_sandbox:  {success  or  fail}.  This  refers  to  Within  Sandbox:  All  information,  such  as  restaurants,  attractions,  accommodations  and  transportation,  in  the  plan,  must  be  within  the  closed  sandbox  (reference  information  box);  otherwise,  it  will  be  considered  a  hallucination.*  is_not_absent:  {success  or  fail}.  This  refers  to  Complete  Information:  No  key  information  should  be  left  out  of  the  plan,  such  as  the  lack  of  accommodation  during  travel.Here  are  some  examples  for  your  information  as  demonstrations:*****  Example  Starts  *****reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}-------------------------------------reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}*****  Example  Ends  *****Now,  You  should  write  the  feedback  with  regard  to  the  aspect  of  constraints  shown  above.  Follow  the  formats  shown  in  the  examples  above.reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:

#### A.5.3 The Refiner’s Prompt Template

[⬇](data:text/plain;base64,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)You  are  a  proficient  planner.  Based  on  the  provided  information  and  query,  please  give  me  a  detailed  plan,  including  specifics  such  as  flight  numbers  (e.g.,  F0123456),  restaurant  names,  and  accommodation  names.  Note  that  all  the  information  in  your  plan  should  be  derived  from  the  provided  data.  You  must  adhere  to  the  format  given  in  the  example.  Additionally,  all  details  should  align  with  commonsense.  The  symbol  ’-’  indicates  that  information  is  unnecessary.  For  example,  in  the  provided  sample,  you  do  not  need  to  plan  after  returning  to  the  departure  city.  When  you  travel  to  two  cities  in  one  day,  you  should  note  it  in  the  ’Current  City’  section  as  in  the  example  (i.e.,  from  A  to  B).*****  Example  *****Query:  Could  you  create  a  travel  plan  for  7  people  from  Ithaca  to  Charlotte  spanning  3  days,  from  March  8th  to  March  14th,  2022,  with  a  budget  of  $30,200?Travel  Plan:Day  1:Current  City:  from  Ithaca  to  CharlotteTransportation:  Flight  Number:  F3633413,  from  Ithaca  to  Charlotte,  Departure  Time:  05:38,  Arrival  Time:  07:46Breakfast:  Nagaland’s  Kitchen,  CharlotteAttraction:  The  Charlotte  Museum  of  History,  CharlotteLunch:  Cafe  Maple  Street,  CharlotteDinner:  Bombay  Vada  Pav,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  2:Current  City:  CharlotteTransportation:  -Breakfast:  Olive  Tree  Cafe,  CharlotteAttraction:  The  Mint  Museum,  Charlotte;  Romare  Bearden  Park,  Charlotte.Lunch:  Birbal  Ji  Dhaba,  CharlotteDinner:  Pind  Balluchi,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  3:Current  City:  from  Charlotte  to  IthacaTransportation:  Flight  Number:  F3786167,  from  Charlotte  to  Ithaca,  Departure  Time:  21:42,  Arrival  Time:  23:26Breakfast:  Subway,  CharlotteAttraction:  Books  Monument,  Charlotte.Lunch:  Olive  Tree  Cafe,  CharlotteDinner:  Kylin  Skybar,  CharlotteAccommodation:  -*****  Example  Ends  *****Given  information:  {reference  information  box}Query:  {query}Travel  Plan:  {original  draft  travel  plan}Now  You  are  an  advanced  reasoning  and  self-corrective  agent  that  can  improve  based  on  self  refection  and  the  feedback.Based  on  given  query,  reference  information  box,  and  proposed  Travel  Plan,  above,  you  are  now  given  the  feedback  which  includes  the  reason  why  it  fails.Try  to  write  a  new  plan  in  which  the  errors  are  fixed.Keep  in  mind  that  you  only  make  changes  or  replace  the  item  which  causes  the  issue.If  it  appears  at  multiple  places,  correct  them  all  at  once.Try  to  avoid  making  unnecessary  changes  on  the  previous  proposed  plan.Always  make  sure  that  your  generation,  such  as  the  names  of  resuturants,  attractions,  accommodations,  transportations,  can  be  found  in  the  given  reference  information  box  above.For  attraction,  breakfast,  dinner  and  lunch,  do  not  give  repetition  within  each  day  and  among  the  days  in  the  plan,  i.e.  each  of  them  should  NOT  appear  more  than  once  in  the  whole  travel  plan.Feel  free  to  ignore  irrelevant  information  in  reference  information  box.*  If  the  feedback  is  about  repeated  restaurant,  for  example,  "The  restaurant  in  day  4  dinner  is  repeated.",  then  you  need  to  take  another  restuartant  from  reference  infobox,  which  is  different  from  the  previous  one  and  all  other  chosen  ones  in  the  plan;*  If  the  feedback  is  about  "The  breakfast/lunch/dinner/attraction/accommodation  in  day  X  is  invalid  in  the  sandbox",  for  example,  "The  lunch  in  day  3  is  invalid  in  the  sandbox.",  this  means  that  the  choice  cannot  be  found  in  reference  infobox.  Then  you  should  take  another  one  which  is  definitely  within  the  inference  infobox.*  If  the  feedback  is  about  "The  accommodation  X  do  not  obey  the  minumum  nights  rule",  this  means  that  the  total  days/nights  spent  in  the  accommodation  place  chosen  in  the  plan,  does  not  obey  the  minumum  nights  rule.For  example,  if  the  days  spent  in  that  accommodation  in  the  plan  are  2  days,  but  the  ’minumum  nights’  of  that  accommodation  is  greater  than  2,  then  the  plan  violates  the  rule.Therefore,  you  should  review  and  examine  the  number  of  ’minumum  nights’  of  each  accommodation  in  the  reference  information  box  and  make  sure  the  days  spent  in  that  accommodation  is  equal  or  greater  than  that  number.*  If  the  feedback  is  about  "No  accommodation/transportation/attaction/meal  in  day  X  is  not  allowed",  this  means  that  on  that  day,  you  should  arrange  the  corresponding  activity  rather  than  leave  it  blank(denoted  as  ’-’).*  If  the  feedback  is  about  "The  transportation  is  conflicting.",  this  means  that  you  cannot  select  neither  the  combination  of  Taxi  and  Self-driving  nor  the  combination  of  Flight  and  Self-driving,  at  the  same  time,  in  terms  of  transportation.Feedback:  {feedback}Write  a  new  plan:

### A.6 Case Presentation

#### A.6.1 Query Example with its Travel Plan

[⬇](data:text/plain;base64,UVVFUlk6CkNhbiB5b3UgY3JlYXRlIGEgdHJhdmVsIHBsYW4gZm9yIGEgZ3JvdXAgb2YgNCBkZXBhcnRpbmcgZnJvbSBTZWF0dGxlIDIgYW5kIGhlYWRpbmcgdG8gU2FuIEZyYW5jaXNjbyBmb3IgMyBkYXlzLCBmcm9tIE1hcmNoIDYgdGggdG8gTWFyY2ggOHRoLDIwMjI/IE91ciBidWRnZXQgaXMgJDIsOTAwLiBXZSBhcmUgYnJpbmdpbmcgcGV0cywgc28gYWNjb21tb2RhdGlvbnMgbmVlZCB0byBiZSBwZXQtZnJpZW5kbHkuIFdlIGFyZSBpbnRlcmVzdGVkIGluIHRyeWluZyBNZXhpY2FuLCBGcmVuY2gsIEFtZXJpY2FuLCBhbmQgTWVkaXRlcnJhbmVhbiBjdWlzaW5lcyBkdXJpbmcgb3VyIHZpc2l0LiBXZSB3b3VsZCBhbHNvIHByZWZlciB0byBhdm9pZCBmbHlpbmcgZm9yIHRyYW5zcG9ydGF0aW9uLgoKVFJBVkVMIFBMQU46CkRheSAxOgpDdXJyZW50IENpdHk6IGZyb20gU2VhdHRsZSB0byBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiBTZWxmLURyaXZpbmcgZnJvbSBTZWF0dGxlIHRvIFNhbiBGcmFuY2lzY28sIER1cmF0aW9uOiAxMiBob3VycyAyOCBtaW5zLCBDb3N0OiAkNjUKQnJlYWtmYXN0OiAtCkF0dHJhY3Rpb246IC0KTHVuY2g6IC0KRGlubmVyOiBBbnVwYW0gRWF0aW5nIFBvaW50LCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMjoKQ3VycmVudCBDaXR5OiBTYW4gRnJhbmNpc2NvClRyYW5zcG9ydGF0aW9uOiAtCkJyZWFrZmFzdDogQ29mZmVlICYgQ2hhaSBDby4sIFNhbiBGcmFuY2lzY28KQXR0cmFjdGlvbjogR29sZGVuIEdhdGUgQnJpZGdlLCBTYW4gRnJhbmNpc2NvOyBHb2xkZW4gR2F0ZSBQYXJrLCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBCb25uZSBCb3VjaGUsIFNhbiBGcmFuY2lzY28KRGlubmVyOiBFbXByZXNzLCBTYW4gRnJhbmNpc2NvCkFjY29tbW9kYXRpb246IFJvb20gaW4gRG93biB0b3duIEJyb29rbHluIFBhcmtzbG9wLCBTYW4gRnJhbmNpc2NvCgpEYXkgMzoKQ3VycmVudCBDaXR5OiBmcm9tIFNhbiBGcmFuY2lzY28gdG8gU2VhdHRsZQpUcmFuc3BvcnRhdGlvbjogU2VsZi1Ecml2aW5nIGZyb20gU2FuIEZyYW5jaXNjbyB0byBTZWF0dGxlLCBEdXJhdGlvbiA6MTIgaG91cnMgMjUgbWlucywgQ29zdDogJDY1CkJyZWFrZmFzdDogR3VwdGEncyBSYXNvaSwgU2FuIEZyYW5jaXNjbwpBdHRyYWN0aW9uOiBQSUVSIDM5LCBTYW4gRnJhbmNpc2NvCkx1bmNoOiBTaGFtbWkgQmhhaSBMYXNzaSBXYWxhLCBTYW4gRnJhbmNpc2NvCkRpbm5lcjogLQpBY2NvbW1vZGF0aW9uOiAt)QUERY:Can  you  create  a  travel  plan  for  a  group  of  4  departing  from  Seattle  2  and  heading  to  San  Francisco  for  3  days,  from  March  6  th  to  March  8th,2022?  Our  budget  is  $2,900.  We  are  bringing  pets,  so  accommodations  need  to  be  pet-friendly.  We  are  interested  in  trying  Mexican,  French,  American,  and  Mediterranean  cuisines  during  our  visit.  We  would  also  prefer  to  avoid  flying  for  transportation.TRAVEL  PLAN:Day  1:Current  City:  from  Seattle  to  San  FranciscoTransportation:  Self-Driving  from  Seattle  to  San  Francisco,  Duration:  12  hours  28  mins,  Cost:  $65Breakfast:  -Attraction:  -Lunch:  -Dinner:  Anupam  Eating  Point,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  2:Current  City:  San  FranciscoTransportation:  -Breakfast:  Coffee  &  Chai  Co.,  San  FranciscoAttraction:  Golden  Gate  Bridge,  San  Francisco;  Golden  Gate  Park,  San  FranciscoLunch:  Bonne  Bouche,  San  FranciscoDinner:  Empress,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  3:Current  City:  from  San  Francisco  to  SeattleTransportation:  Self-Driving  from  San  Francisco  to  Seattle,  Duration  :12  hours  25  mins,  Cost:  $65Breakfast:  Gupta’s  Rasoi,  San  FranciscoAttraction:  PIER  39,  San  FranciscoLunch:  Shammi  Bhai  Lassi  Wala,  San  FranciscoDinner:  -Accommodation:  -

#### A.6.2 Feedback Examples Generated by LLMs

The feedback generated by LLMs is in the same format of the system feedback.

[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBmYWlsLCByZWFzb246VGhlIHRyaXAgc2hvdWxkIGJlIGEgY2xvc2VkIGNpcmNsZS4KaXNfdmFsaWRfcmVzdGF1cmFudHM6IHN1Y2Nlc3MKaXNfdmFsaWRfYXR0cmFjdGlvbnM6IHN1Y2Nlc3MKaXNfdmFsaWRfYWNjb21tb2RhdGlvbjogZmFpbCwgcmVhc29uOlRoZSBhY2NvbW1vZGF0aW9uIEhhcmxlbSBjb3p5IG5pZ2h0cywgRGVudmVyKENvbG9yYWRvKSBkbyBub3Qgb2JleSB0aGUgbWludW11bSBuaWdodHMgcnVsZS4KaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IGZhaWwsIHJlYXNvbjpUaGUgdHJhbnNwb3J0YXRpb24gaXMgY29uZmxpY3RpbmcuCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX2N1cnJlbnRfY2l0eTogc3VjY2Vzcwppc192YWxpZF9pbmZvcm1hdGlvbl9pbl9zYW5kYm94OiBmYWlsLCByZWFzb246VGhlIGFjY29tbW9kYXRpb24gaW4gZGF5IDMgaXMgaW52YWxpZCBpbiB0aGUgc2FuZGJveC4KaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  fail,  reason:The  trip  should  be  a  closed  circle.is_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  fail,  reason:The  accommodation  Harlem  cozy  nights,  Denver(Colorado)  do  not  obey  the  minumum  nights  rule.is_valid_transportation:  fail,  reason:The  transportation  is  conflicting.is_valid_information_in_current_city:  successis_valid_information_in_sandbox:  fail,  reason:The  accommodation  in  day  3  is  invalid  in  the  sandbox.is_not_absent:  success

Recall that, there are $45$ annotated plans in the training set. For each plan, without any exception, the generated feedback from the Oracle system is `all-success`:

[⬇](data:text/plain;base64,aXNfcmVhc29uYWxiZV92aXNpdGluZ19jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX3Jlc3RhdXJhbnRzOiBzdWNjZXNzCmlzX3ZhbGlkX2F0dHJhY3Rpb25zOiBzdWNjZXNzCmlzX3ZhbGlkX2FjY29tbW9kYXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfdHJhbnNwb3J0YXRpb246IHN1Y2Nlc3MKaXNfdmFsaWRfaW5mb3JtYXRpb25faW5fY3VycmVudF9jaXR5OiBzdWNjZXNzCmlzX3ZhbGlkX2luZm9ybWF0aW9uX2luX3NhbmRib3g6IHN1Y2Nlc3MKaXNfbm90X2Fic2VudDogc3VjY2Vzcw==)is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  success | Benchmark | Task |  

&#124; Feedback &#124;
&#124; Provided? &#124;

 |  

&#124; Trajectory &#124;
&#124; Released? &#124;

 | Baseline |  

&#124; Realistic &#124;
&#124; Interface? &#124;

 |
| WebShop Yao et al. ([2022a](https://arxiv.org/html/2408.06318v1#bib.bib48)) | Web | No | Expert | Rule, IL, RL, IL+RL | Yes |
| WebArena Zhou et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib58)) | Web | No | Expert, Agent | Direct, CoT | Yes |
| AgentBench Liu et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib17)) |  

&#124; Web, Code, &#124;
&#124; Game, Embodiment &#124;

 | No | Not Found | CoT | Yes |
| TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)) | Tool, Planning | Yes | Expert |  

&#124; Direct, CoT, &#124;
&#124; ReAct, Reflexion &#124;

 | No |
| AgentBoard Ma et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib19)) |  

&#124; Web, Game, Tool, &#124;
&#124; Embodiment &#124;

 | Partially | Not Found | Direct | Partially |
| AgentGym Xi et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib44)) |  

&#124; Web, Code, Game, &#124;
&#124; Tool, Embodiment &#124;

 | Partially | Expert, Agent |  

&#124; BC (SFT), &#124;
&#124; ReAct, AGENTEVOL &#124;

 | Yes | 

Table A.1: Popular Benchmarks for LLM Agents

### A.7 Related Works

#### A.7.1 Benchmarks for LLM-based Generalist Agents

It has been anticipated that generalist agents can handle diverse tasks and evolve across different (cyber) environments at the human level which is a long-term goal in the AGI community. LLMs can be used as experts, which mimic humans, that have a strong generalization capability that not only suits conventional NLP but also agentic tasks. Recently, plenty of benchmarks have been proposed to evaluate the agents across various tasks and environments comprehensively and fairly. We provide an overview of popular benchmarks in the community in Table [A.1](https://arxiv.org/html/2408.06318v1#A1.T1 "Table A.1 ‣ A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"). Some benchmarks such as ALFWorld Shridhar et al. ([2020](https://arxiv.org/html/2408.06318v1#bib.bib31)) and Mind2Web Deng et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib6)), which are already included in larger benchmarks, are not listed in the table. Although the recent progress in multi-modal LLMs has spurred research into multi-modal LLM agents Yang et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib47)); Zheng et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib56)), we only list benchmarks that focus exclusively on text-based environments which assess LLM agents’ abilities via textual reasoning and taking actions in-depth.

The listed benchmarks support agents powered by both API-based proprietary and open-weight LLMs with convenient drop-in replacement interfaces. It is also free to add few-shots or use other prompting strategies to generate actions.

#### A.7.2 Long Contexts Challenge for LLMs

Besides the fact that more and more LLMs offer long-context capabilities Fei et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib7)); Ratner et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib27)); Liu et al. ([2023a](https://arxiv.org/html/2408.06318v1#bib.bib16)); Zhao et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib55)); Qian et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib24)), recent studies question LLMs’ ability to find needles in a haystack because they face challenges in discriminating highly semantically related information, and can be easily distracted by irrelevant and misleading contents in long contexts Wu et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib41)); Zhu et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib60)); Chang et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib1)); Shi et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib29)); gkamradt ([2023](https://arxiv.org/html/2408.06318v1#bib.bib8)). The TravelPlanner Xie et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib45)) is a benchmark to provide insightful answers to this problem, wherein lengthy context information, noise, and relevant snippets are deeply intertwined.

#### A.7.3 Multi-Agent Collaboration

Recent studies have borrowed the multiple-agent methodology for collaboration on cyber tasks, gaming, coding, math reasoning, conversation responding, and question answering Guo et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib9)); Wu et al. ([2023a](https://arxiv.org/html/2408.06318v1#bib.bib40), [b](https://arxiv.org/html/2408.06318v1#bib.bib42)); Zhang et al. ([2024c](https://arxiv.org/html/2408.06318v1#bib.bib54)); Li et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib14)); Liu et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib18)); Talebirad and Nadiri ([2023](https://arxiv.org/html/2408.06318v1#bib.bib34)); Zhang et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib52)); Wang et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib35)). Under the hood, these works assign role-specific prompts to the LLM to build multiple agents for synergy and collaboration. The self-refinement works can be classified into this realm, where the advisor and refiner agents can troubleshoot and modify the response in a few rounds Madaan et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib20)); Paul et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib23)); Kim et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib11)); Pan et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib22)); Chen et al. ([2023a](https://arxiv.org/html/2408.06318v1#bib.bib2)). However, few works study the reliability and robustness of multi-agent collaboration in more complex and practical tasks. Compared to the previous testbeds where generation errors are easily noticeable and unambiguous, it is questionable whether refinement can work on TravelPlanner, where the glitches are hard to find due to implicit commonsense constraints. Multi-agent collaboration also places higher demands on the capabilities of individual agents since a failure at any stage from any agent can lead to a collapse, such as a dead loop or deviation from the goal.

#### A.7.4 Reinforcement Learning via Feedback

On top of works that only use successful trajectories for behavioural cloning Zeng et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib51)); Chen et al. ([2023b](https://arxiv.org/html/2408.06318v1#bib.bib3)); Zhang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib53)); Chen et al. ([2024](https://arxiv.org/html/2408.06318v1#bib.bib4)), another line of work trains LLM-based agents based on environmental feedback, referred to as interactive learning methods Song et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib33)); Zhou et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib59)); Christianos et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib5)); Xi et al. ([2024a](https://arxiv.org/html/2408.06318v1#bib.bib43)). Specifically, they train the agents via reinforcement learning. However, poor transferability among scenarios, reward inconsistency, off-policy shift, step-level reward sparsity, and training stability and expenses are the main roots of performance bottlenecks. Possible alternative approaches such as Negative Aware Training (NAT) Wang et al. ([2024b](https://arxiv.org/html/2408.06318v1#bib.bib36)); Li et al. ([2023](https://arxiv.org/html/2408.06318v1#bib.bib15)) can be a more robust solution. Our FAFT approach is motivated by NAT, and it can be seamlessly migrated to other agentic tasks.