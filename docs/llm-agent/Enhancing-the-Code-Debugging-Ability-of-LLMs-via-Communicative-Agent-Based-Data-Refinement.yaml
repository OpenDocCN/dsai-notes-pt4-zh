- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:39:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data
    Refinement
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升 LLM 的代码调试能力通过基于交互式代理的数据精炼
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.05006](https://ar5iv.labs.arxiv.org/html/2408.05006)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.05006](https://ar5iv.labs.arxiv.org/html/2408.05006)
- en: Weiqing Yang *indicates both authors contributed equally to this work. Northeastern
    University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Weiqing Yang *表示两位作者对这项工作贡献相等。东北大学
- en: Shenyang, China    Hanbin Wang Peking University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 沈阳，中国    Hanbin Wang 北京大学
- en: Beijing, China    Zhenghao Liu^† ^†indicates corresponding author. Northeastern
    University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国    Zhenghao Liu^† ^†表示通讯作者。东北大学
- en: Shenyang, China    Xinze Li Northeastern University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 沈阳，中国    Xinze Li 东北大学
- en: Shenyang, China    Yukun Yan Tsinghua University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 沈阳，中国    Yukun Yan 清华大学
- en: Beijing, China    Shuo Wang Tsinghua University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国    Shuo Wang 清华大学
- en: Beijing, China    Yu Gu Northeastern University
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国    Yu Gu 东北大学
- en: Shenyang, China    Minghe Yu Northeastern University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 沈阳，中国    Minghe Yu 东北大学
- en: Shenyang, China    Zhiyuan Liu Tsinghua University
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 沈阳，中国    Zhiyuan Liu 清华大学
- en: Beijing, China    Ge Yu Northeastern University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 北京，中国    Ge Yu 东北大学
- en: Shenyang, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 沈阳，中国
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Debugging is a vital aspect of software development, yet the debugging capabilities
    of Large Language Models (LLMs) remain largely unexplored. This paper first introduces
    DebugEval, a comprehensive benchmark designed to evaluate the debugging capabilities
    of LLMs. DebugEval collects data from existing high-quality datasets and designs
    four different tasks to evaluate the debugging effectiveness, including BUG Localization,
    BUG Identification, Code Review, and Code Repair. Additionally, to enhance the
    code debugging ability of LLMs, this paper proposes a CoMmunicative Agent BaSed
    DaTa REfinement FRamework (MASTER), which generates the refined code debugging
    data for supervised finetuning. Specifically, MASTER employs the Code Quizzer
    to generate refined data according to the defined tasks of DebugEval. Then the
    Code Learner acts as a critic and reserves the generated problems that it can
    not solve. Finally, the Code Teacher provides a detailed Chain-of-Thought based
    solution to deal with the generated problem. We collect the synthesized data and
    finetune the Code Learner to enhance the debugging ability and conduct the NeuDebugger
    model. Our experiments evaluate various LLMs and NeuDebugger in the zero-shot
    setting on DebugEval. Experimental results demonstrate that these 7B-scale LLMs
    have weaker debugging capabilities, even these code-oriented LLMs. On the contrary,
    these larger models (over 70B) show convincing debugging ability. Our further
    analyses illustrate that MASTER is an effective method to enhance the code debugging
    ability by synthesizing data for Supervised Fine-Tuning (SFT) LLMs. All data and
    codes are available at https://github.com/NEUIR/DebugEval.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 调试是软件开发的一个重要方面，但大型语言模型（LLMs）的调试能力仍然 largely 未被探索。本文首先介绍了 DebugEval，一个全面的基准，用于评估
    LLMs 的调试能力。DebugEval 从现有的高质量数据集中收集数据，并设计了四种不同的任务来评估调试效果，包括 BUG 定位、BUG 识别、代码审查和代码修复。此外，为了提升
    LLM 的代码调试能力，本文提出了一个 CoMmunicative Agent BaSed DaTa REfinement FRamework（MASTER），该框架生成精炼的代码调试数据以用于监督细化。具体而言，MASTER
    利用 Code Quizzer 根据 DebugEval 的定义任务生成精炼数据。然后，Code Learner 作为批评者保留无法解决的生成问题。最后，Code
    Teacher 提供详细的基于思路链的解决方案来处理生成的问题。我们收集合成的数据并细化 Code Learner 以增强调试能力，并进行 NeuDebugger
    模型的实验。我们的实验评估了各种 LLM 和 NeuDebugger 在 DebugEval 上的零-shot 设置。实验结果表明，这些 7B 规模的 LLM
    在调试能力上较弱，即使是这些面向代码的 LLM。相反，这些较大的模型（超过 70B）显示出令人信服的调试能力。我们的进一步分析表明，MASTER 是通过为监督细化（SFT）LLM
    合成数据来增强代码调试能力的有效方法。所有数据和代码可在 [https://github.com/NEUIR/DebugEval](https://github.com/NEUIR/DebugEval)
    获得。
- en: 'Index Terms:'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Code Debugging, Large Language Models, DebugEval Benchmark, Data Refinement,
    Agents
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代码调试、大型语言模型、DebugEval 基准、数据精炼、代理
- en: I Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In the realm of software development, code debugging is an indispensable process
    for ensuring the functionality and reliability of applications [[1](#bib.bib1),
    [2](#bib.bib2)]. With the complexity of software systems grows, traditional debugging
    methods, which often rely on heuristics [[3](#bib.bib3), [4](#bib.bib4)] and predefined
    patterns [[5](#bib.bib5), [6](#bib.bib6)], are reaching their limitations. The
    emergent ability of Large Language Models (LLMs) [[7](#bib.bib7), [8](#bib.bib8)]
    has opened up new horizons in automated debugging, offering a more flexible and
    comprehensive approach to identifying and rectifying code errors [[9](#bib.bib9)].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发领域，代码调试是确保应用程序功能性和可靠性的不可或缺的过程 [[1](#bib.bib1), [2](#bib.bib2)]。随着软件系统的复杂性增加，传统的调试方法，通常依赖于启发式 [[3](#bib.bib3),
    [4](#bib.bib4)] 和预定义模式 [[5](#bib.bib5), [6](#bib.bib6)]，正达到其局限性。大型语言模型（LLM）的新兴能力 [[7](#bib.bib7),
    [8](#bib.bib8)] 开辟了自动调试的新领域，提供了一种更灵活和全面的方法来识别和纠正代码错误 [[9](#bib.bib9)]。
- en: 'The capabilities of LLMs have been extensively explored for code-related tasks
    such as code generation and translation [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]. However, their debugging capabilities remain
    relatively underexplored. Recently, researchers have begun to focus on using LLMs
    for self-debugging to repair buggy code iteratively [[9](#bib.bib9), [15](#bib.bib15),
    [16](#bib.bib16)]. To better evaluate the code debugging ability, researchers
    are now building benchmarks to assess the code debugging capabilities of LLMs [[17](#bib.bib17),
    [18](#bib.bib18)]. Nevertheless, existing code debugging benchmarks face two main
    issues: 1) They primarily design tasks around code repair, which is insufficient
    for a comprehensive evaluation of code debugging ability. 2) Constructing buggy
    code using GPT-4 [[19](#bib.bib19)] fails to capture the complexity and diversity
    of code errors encountered in real development environments.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的能力已经在与代码相关的任务中得到了广泛探索，例如代码生成和翻译 [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14)]。然而，它们在调试方面的能力仍然相对未被充分探讨。最近，研究人员开始关注使用 LLM
    进行自我调试，以迭代方式修复有缺陷的代码 [[9](#bib.bib9), [15](#bib.bib15), [16](#bib.bib16)]。为了更好地评估代码调试能力，研究人员现在正在构建基准测试来评估
    LLM 的代码调试能力 [[17](#bib.bib17), [18](#bib.bib18)]。然而，现有的代码调试基准测试面临两个主要问题：1）它们主要围绕代码修复设计任务，这不足以全面评估代码调试能力。2）使用
    GPT-4 [[19](#bib.bib19)] 构造的有缺陷代码未能捕捉到实际开发环境中遇到的代码错误的复杂性和多样性。
- en: 'To address these challenges, this paper introduces DebugEval, a comprehensive
    benchmark designed to evaluate the debugging capabilities of LLMs. DebugEval introduces
    four tasks: BUG Localization, BUG Identification, Code Review, and Code Repair,
    which are designed to test the LLMs’ ability not only to identify and classify
    errors but also to provide correct code solutions. These tasks are of different
    difficulty levels and represent common debugging scenarios in real software development
    environments, making the evaluation both representative and challenging. Besides,
    each task in DebugEval includes Python, C++, and Java. Additionally, DebugEval
    incorporates the real user-written buggy codes and GPT-4 generated buggy codes
    to better simulate real-world software development. While collecting user-written
    buggy codes, we also implement strict quality control to prevent data leakage
    in DebugEval.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，本文介绍了 DebugEval，这是一个综合性的基准测试，旨在评估 LLM 的调试能力。DebugEval 引入了四个任务：BUG 定位、BUG
    识别、代码审查和代码修复，这些任务旨在测试 LLM 不仅能够识别和分类错误，还能提供正确的代码解决方案。这些任务具有不同的难度级别，代表了实际软件开发环境中的常见调试场景，使评估既具有代表性又具有挑战性。此外，DebugEval
    中的每个任务都包括 Python、C++ 和 Java。同时，DebugEval 融入了真实用户编写的有缺陷代码和 GPT-4 生成的有缺陷代码，以更好地模拟现实世界的软件开发。在收集用户编写的有缺陷代码时，我们还实施了严格的质量控制，以防止
    DebugEval 中的数据泄露。
- en: 'Additionally, this paper proposes a coMmunicative Agent baSed daTa rEfinement
    fRamework (MASTER), which focuses on improving the debugging ability of Large
    Language Models (LLMs). To ensure the quality of the Supervised Fine-Tuning (SFT)
    data, MASTER defines three agents: Code Quizzer, Code Learner, and Code Teacher,
    which collaborate to synthesize high-quality code debugging data for finetuning
    the Code Learner. Specifically, the Code Quizzer generates a diverse set of code
    debugging problems, guiding the LLM to acquire debugging knowledge during SFT.
    The Code Learner then acts as a critic, evaluating the educational value of the
    synthesized problems. Problems that the Code Learner answers incorrectly are collected
    as SFT data, and the Code Teacher provides detailed solutions and explanations
    for these problems. Finally, we finetune the Code Learner using the synthesized
    SFT data and develop our NeuDebugger model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文提出了一种称为 coMmunicative Agent baSed daTa rEfinement fRamework (MASTER) 的框架，专注于提升大语言模型
    (LLMs) 的调试能力。为了确保监督微调 (SFT) 数据的质量，MASTER 定义了三个代理：Code Quizzer、Code Learner 和 Code
    Teacher，它们协作合成高质量的代码调试数据用于微调 Code Learner。具体来说，Code Quizzer 生成多样的代码调试问题，指导 LLM
    在 SFT 过程中获取调试知识。随后，Code Learner 作为批评者，评估合成问题的教育价值。Code Learner 回答错误的问题将被收集作为 SFT
    数据，而 Code Teacher 为这些问题提供详细的解答和解释。最后，我们使用合成的 SFT 数据对 Code Learner 进行微调，并开发了我们的
    NeuDebugger 模型。
- en: '![Refer to caption](img/e16a6a9d1393c29a292ce1faea4f62a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e16a6a9d1393c29a292ce1faea4f62a2.png)'
- en: 'Figure 1: The performance of LLMs on DebugEval Benchmark. Left: We evaluate
    the code debugging capability of LLMs on four tasks: BUG Localization, BUG Identification,
    Code Review, and Code Repair. The radar chart shows the distribution of performance
    on each task, reflecting the strengths and weaknesses of different LLMs for these
    debugging tasks. Right: The average performance of open-source and closed-source
    models on four tasks. The same color indicates that the number of parameters belongs
    to the same level.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM 在 DebugEval 基准测试中的表现。左侧：我们在四项任务上评估了 LLM 的代码调试能力：BUG 定位、BUG 识别、代码审查和代码修复。雷达图显示了每项任务的性能分布，反映了不同
    LLM 在这些调试任务中的优缺点。右侧：开源和闭源模型在四项任务上的平均表现。同色表示参数数量属于同一水平。
- en: 'We benchmark 13 open-source and closed-source LLMs on DebugEval and evaluate
    the overall performance of NeuDebugger. The results are shown in Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Enhancing the Code Debugging Ability of LLMs via
    Communicative Agent Based Data Refinement"), indicating that: 1) Models with 7
    billion parameters exhibit relatively weaker debugging capabilities, whereas 70
    billion parameter models and closed-source models perform better. 2) The DeepSeek
    series models demonstrate superior performance, with the open-source model DeepSeek-Coder-V2-0724
    outperforming the closed-source model GPT-4o-mini. 3) NeuDebugger-DS-6.7B and
    NeuDebugger-Llama3-8B, based on DeepSeek-Coder-6.7B-Ins and Llama3-8B-Ins respectively,
    achieve improvements of 27.7% and 4.1% when trained on data synthesized by MASTER,
    indicating that MASTER can significantly refine SFT data to enhance model performance
    on debugging tasks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 DebugEval 上对 13 种开源和闭源的 LLM 进行了基准测试，并评估了 NeuDebugger 的整体性能。结果如图 [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Enhancing the Code Debugging Ability of LLMs via
    Communicative Agent Based Data Refinement") 所示，表明：1) 拥有 70 亿参数的模型表现出相对较弱的调试能力，而拥有
    700 亿参数的模型和闭源模型表现更好。2) DeepSeek 系列模型表现优越，其中开源模型 DeepSeek-Coder-V2-0724 的表现超过了闭源模型
    GPT-4o-mini。3) 基于 DeepSeek-Coder-6.7B-Ins 和 Llama3-8B-Ins 的 NeuDebugger-DS-6.7B
    和 NeuDebugger-Llama3-8B，分别在 MASTER 合成的数据上实现了 27.7% 和 4.1% 的改进，这表明 MASTER 可以显著优化
    SFT 数据，从而提高模型在调试任务上的表现。
- en: 'Further analysis reveals that collecting data solely for SFT does not enhance
    the debugging ability of LLMs [[20](#bib.bib20)]. The Code Teacher effectively
    teaches the Code Student in three tasks: BUG Localization, BUG Identification,
    and Code Review, by generating the Chain-of-Thought (CoT) [[21](#bib.bib21)].
    However, CoT outcomes decrease the performance of LLMs in the Code Repair task,
    as they introduce additional noise and disrupt the code structure. During SFT,
    different models exhibit distinct learning behaviors. We find that synthesized
    data can significantly improve the performance of code-oriented LLMs, such as
    DeepSeek-Coder-6.7B-Ins, than general LLMs, such as Llama3-8B-Int. These experimental
    findings provide important insights and directions for future research on enhancing
    the debugging capabilities of LLMs.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的分析表明，仅仅为SFT收集数据并不会增强LLM的调试能力[[20](#bib.bib20)]。代码教师通过生成思维链（CoT）有效地在三个任务中教导代码学生：BUG定位、BUG识别和代码审查[[21](#bib.bib21)]。然而，CoT的结果会降低LLM在代码修复任务中的表现，因为它们引入了额外的噪音并扰乱了代码结构。在SFT过程中，不同的模型表现出不同的学习行为。我们发现，合成数据能够显著提高代码导向的LLM（如DeepSeek-Coder-6.7B-Ins）的表现，优于一般的LLM（如Llama3-8B-Int）。这些实验结果为未来提升LLM调试能力的研究提供了重要的见解和方向。
- en: II Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: This section first introduces some backbone models for code understating and
    generation and then introduces the related work of code debugging and repair.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先介绍了一些用于代码理解和生成的骨干模型，然后介绍了代码调试和修复的相关工作。
- en: Code-Oriented Language Models. To tailor language models for code understanding,
    related work mainly focuses on pretrained language models and guides them to learn
    the code semantics of syntax, and idiomatic [[22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24)]. CodeBERT masks tokens of Natural Language (NL) and Program
    Language (PL) and then asks the pretrained language models to fill-in the masked
    spans. Then CodeBERT follows the ELECTRA method [[25](#bib.bib25)] and pretrains
    language models to detect whether the tokens are replaced, which helps models
    to better capture the code semantics [[26](#bib.bib26)]. DOBF [[27](#bib.bib27)]
    goes a step further by taking into account the unique attributes of code-related
    tasks, which focuses more on masking and generating the class, function, and variable
    names of code segments. CodeT5 [[28](#bib.bib28)] continuously pretrains T5 models [[29](#bib.bib29)]
    using the span masking strategy and also refines the masking strategy by focusing
    more on the identifiers within code. Such a pretraining method asks the T5 [[29](#bib.bib29)]
    model to generate these identifiers, thereby enhancing its ability to identify
    and understand identifier information in code-related tasks. Furthermore, some
    researchers also incorporate multi-modal data sources, such as code, comments,
    and abstract syntax trees (AST), to pretrain language models, which also helps
    to improve the code understanding ability by aligning the semantic between code
    semantics and natural language [[30](#bib.bib30), [31](#bib.bib31)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 针对代码的语言模型。为了调整语言模型以适应代码理解，相关工作主要集中在预训练语言模型上，并引导它们学习代码的语法和惯用法[[22](#bib.bib22),
    [23](#bib.bib23), [24](#bib.bib24)]。CodeBERT通过遮盖自然语言（NL）和程序语言（PL）的标记，然后要求预训练的语言模型填补这些被遮盖的部分。随后，CodeBERT遵循ELECTRA方法[[25](#bib.bib25)]，并预训练语言模型以检测标记是否被替换，这有助于模型更好地捕捉代码语义[[26](#bib.bib26)]。DOBF[[27](#bib.bib27)]更进一步，考虑到代码相关任务的独特属性，更加关注代码段中类、函数和变量名的遮盖和生成。CodeT5[[28](#bib.bib28)]持续使用跨度遮盖策略对T5模型[[29](#bib.bib29)]进行预训练，并通过更关注代码中的标识符来改进遮盖策略。这种预训练方法要求T5[[29](#bib.bib29)]模型生成这些标识符，从而增强了其在代码相关任务中识别和理解标识符信息的能力。此外，一些研究人员还结合了多模态数据源，如代码、评论和抽象语法树（AST），以预训练语言模型，这也有助于通过对齐代码语义与自然语言之间的语义来提升代码理解能力[[30](#bib.bib30),
    [31](#bib.bib31)]。
- en: Recently, Large Language Models (LLMs), such as ChatGPT [[7](#bib.bib7)] and
    Llama [[8](#bib.bib8)], have demonstrated their emergency ability in dealing with
    different tasks, especially for code understanding and generation tasks. To enhance
    the code generation ability, some widely used LLMs, such as ChatGPT [[7](#bib.bib7)],
    also mix some code data in the pretraining corpus, which has proven its advantage
    in enhancing the reasoning ability of LLMs [[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34)]. Some typical code-based LLMs also collect some instruction
    data of different code-related tasks to supervised finetune LLMs, which significantly
    improves the code generation ability of LLMs [[35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)]. Even though LLMs have strong effectiveness in generating code
    segments, the code segments usually contain bugs [[9](#bib.bib9)], decreasing
    the pass rate of generated codes. To alleviate these problems, the existing efforts
    primarily concentrate on employing an iterative code repair approach to continuously
    refine generated code segments [[9](#bib.bib9), [16](#bib.bib16), [15](#bib.bib15)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs），如ChatGPT[[7](#bib.bib7)]和Llama[[8](#bib.bib8)]，在处理各种任务，尤其是代码理解和生成任务方面展示了其应急能力。为了提升代码生成能力，一些广泛使用的LLM，如ChatGPT[[7](#bib.bib7)]，还在预训练语料中混合了一些代码数据，这已证明在提升LLM推理能力方面具有优势[[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)]。一些典型的基于代码的LLM还收集了不同代码相关任务的指令数据，以监督微调LLM，这显著提高了LLM的代码生成能力[[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37)]。尽管LLM在生成代码片段方面具有强大的有效性，但这些代码片段通常包含错误[[9](#bib.bib9)]，降低了生成代码的通过率。为了缓解这些问题，现有的努力主要集中在采用迭代代码修复方法上，以不断完善生成的代码片段[[9](#bib.bib9),
    [16](#bib.bib16), [15](#bib.bib15)]。
- en: Code Debugging and Repair. Early debugging models primarily rely on feature-based
    methods, such as using templates [[5](#bib.bib5), [6](#bib.bib6)], heuristic rules [[3](#bib.bib3),
    [4](#bib.bib4)], or constraints [[38](#bib.bib38), [39](#bib.bib39)] to correct
    the buggy codes. However, the effectiveness of these feature-based debugging methods
    is hard to broaden to correcting different bug errors and dealing with more complex
    code bugs, because of the limited patterns or rules that need predefinition by
    researchers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 代码调试与修复。早期的调试模型主要依赖于基于特征的方法，例如使用模板[[5](#bib.bib5), [6](#bib.bib6)]、启发式规则[[3](#bib.bib3),
    [4](#bib.bib4)]或约束[[38](#bib.bib38), [39](#bib.bib39)]来修正错误的代码。然而，由于这些特征基础的调试方法需要研究人员预定义的有限模式或规则，因此很难扩大其在纠正不同错误和处理更复杂代码问题上的有效性。
- en: With the development of Pre-trained Language Models (PLMs), the work also follows
    the pretraining and then finetuning strategy to build the debugging model and
    deal with various code bugs occurred in real life. For example, Xia et al. [[40](#bib.bib40)]
    use the code-oriented pretrained model, CodeX [[41](#bib.bib41)], to explore the
    capabilities of PLMs in debugging. It shows that CodeX [[41](#bib.bib41)] achieves
    convinced code repair performance, especially on both Python and Java program
    languages. Kolak et al. [[42](#bib.bib42)] also use GPT-2 [[43](#bib.bib43)] and
    CodeX [[41](#bib.bib41)] to evaluate their effectiveness in generating the correct
    patch line when given corresponding code prefix. All the research has proven that
    the effectiveness of PLM based debugging models mainly thrives on the code understanding
    ability obtained during pretraining.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练语言模型（PLMs）的发展，这项工作也遵循预训练然后微调的策略来构建调试模型并处理实际生活中出现的各种代码错误。例如，Xia等人[[40](#bib.bib40)]使用以代码为导向的预训练模型CodeX[[41](#bib.bib41)]来探索PLMs在调试中的能力。结果表明，CodeX[[41](#bib.bib41)]在Python和Java编程语言中都取得了令人信服的代码修复性能。Kolak等人[[42](#bib.bib42)]也使用GPT-2[[43](#bib.bib43)]和CodeX[[41](#bib.bib41)]来评估其在给定相应代码前缀时生成正确补丁行的有效性。所有研究已证明，基于PLM的调试模型的有效性主要依赖于在预训练期间获得的代码理解能力。
- en: Different from previous debugging work, recent research focuses more on correcting
    the bugs generated by LLMs. Self-Debug [[9](#bib.bib9)] prompts LLMs to generate
    the code reviews to aid themselves to refine generated codes, while Self-Repair [[44](#bib.bib44)]
    incorporates human-provided feedback for repairing the buggy code. Furthermore,
    Self-Edit [[15](#bib.bib15)] trains an additional fault-aware editor to repair
    codes by leveraging the error messages from the test case evaluation and generated
    code segments. Wang et al. [[16](#bib.bib16)] further explore the effectiveness
    of interactive Chain-of-Repair (CoR), which uses LLMs to generate the guidelines
    for repairing codes by incorporating the generated codes and error messages from
    the code compiler. It is evident that the effectiveness of these code repair models
    mainly relies on the debugging ability of LLMs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的调试工作不同，近期的研究更加关注于纠正 LLMs 生成的错误。Self-Debug [[9](#bib.bib9)] 促使 LLMs 生成代码审查，以帮助其自行改进生成的代码，而
    Self-Repair [[44](#bib.bib44)] 则融入了人工提供的反馈以修复有缺陷的代码。此外，Self-Edit [[15](#bib.bib15)]
    训练了一个额外的故障意识编辑器，通过利用测试用例评估和生成的代码段中的错误信息来修复代码。Wang 等人 [[16](#bib.bib16)] 进一步探索了交互式
    Chain-of-Repair (CoR) 的效果，该方法利用 LLMs 生成修复代码的指南，并结合生成的代码和代码编译器的错误信息。显然，这些代码修复模型的有效性主要依赖于
    LLMs 的调试能力。
- en: To improve the debugging ability of LLMs, recent work focuses more on generating
    data for supervised fine-tuning. InstructCoder [[45](#bib.bib45)] employs the
    Self-Instruct method [[46](#bib.bib46)] to build an instruction-tuning dataset
    to improve the effectiveness of LLMs in code debugging. Li et al. [[47](#bib.bib47)]
    further construct the APR-INSTRUCTION dataset and utilize the dataset to finetune
    LLMs using four different Parameter-Efficient Fine-Tuning (PEFT) methods LoRA [[48](#bib.bib48)],
    p-tuning [[49](#bib.bib49)], prefix-tuning [[50](#bib.bib50)] and $(IA)^{3}$ [[51](#bib.bib51)].
    To further evaluate the debugging capabilities of LLMs, some researchers focus
    on building benchmarks to evaluate LLMs. Wang et al. [[16](#bib.bib16)] collect
    real user-submitted buggy code from the Atcoder website to construct CodeError,
    which is used to evaluate the repair abilities of LLMs on Python codes. Tian et
    al. [[17](#bib.bib17)] further propose DebugBench to explore the debugging capabilities
    of LLMs in Python, C++, and Java. They use GPT-4 [[19](#bib.bib19)] to insert
    some code errors into code segments to synthesize the buggy codes and ask LLMs
    to repair codes. Nevertheless, only code repair task is insufficient to evaluate
    the debugging ability of LLMs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为提高大型语言模型（LLMs）的调试能力，近期的研究更多地集中在生成用于监督微调的数据上。InstructCoder [[45](#bib.bib45)]
    采用了 Self-Instruct 方法 [[46](#bib.bib46)] 来构建一个指令调整数据集，以提高 LLMs 在代码调试中的效果。Li 等人 [[47](#bib.bib47)]
    进一步构建了 APR-INSTRUCTION 数据集，并利用该数据集通过四种不同的参数高效微调（PEFT）方法 LoRA [[48](#bib.bib48)],
    p-tuning [[49](#bib.bib49)], prefix-tuning [[50](#bib.bib50)] 和 $(IA)^{3}$ [[51](#bib.bib51)]
    对 LLMs 进行微调。为了进一步评估 LLMs 的调试能力，一些研究者集中于构建基准来评估 LLMs。Wang 等人 [[16](#bib.bib16)]
    从 Atcoder 网站收集真实用户提交的有缺陷代码，构建了 CodeError，用于评估 LLMs 在 Python 代码上的修复能力。Tian 等人 [[17](#bib.bib17)]
    进一步提出了 DebugBench，探索 LLMs 在 Python、C++ 和 Java 中的调试能力。他们使用 GPT-4 [[19](#bib.bib19)]
    向代码片段中插入一些代码错误，以合成有缺陷的代码，并要求 LLMs 修复这些代码。然而，单一的代码修复任务不足以评估 LLMs 的调试能力。
- en: '![Refer to caption](img/436e1cf4321746d42e0fb8b8fa8a9c2a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/436e1cf4321746d42e0fb8b8fa8a9c2a.png)'
- en: 'Figure 2: DebugEval Benchmark. DebugEval includes four tasks: BUG Localization,
    BUG Identification, Code Review, and Code Repair. The first three tasks are multiple-choice
    questions and are evaluated with Accuracy. In Code Review task, we swap the contents
    of options A and B to avoid any potential bias. The Code Repair task is evaluated
    using Pass@$1$.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: DebugEval 基准。DebugEval 包括四个任务：BUG 定位、BUG 识别、代码审查和代码修复。前三个任务是选择题，通过准确性进行评估。在代码审查任务中，我们交换了选项
    A 和 B 的内容，以避免潜在的偏差。代码修复任务通过 Pass@$1$ 进行评估。'
- en: III Evaluating the Debugging Ability of LLMs with DebugEval
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 使用 DebugEval 评估 LLMs 的调试能力
- en: In this section, we introduce the benchmark DebugEval, which is built to evaluate
    the debugging capabilities of Large Language Models (LLMs) from different aspects.
    We first describe the task definition of the designed task in DebugEval (Sec. [III-A](#S3.SS1
    "III-A Task Definition ‣ III Evaluating the Debugging Ability of LLMs with DebugEval
    ‣ Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data
    Refinement")). Then we detail the process of constructing the DebugEval benchmark
    (Sec. [III-B](#S3.SS2 "III-B Details of Data Construction ‣ III Evaluating the
    Debugging Ability of LLMs with DebugEval ‣ Enhancing the Code Debugging Ability
    of LLMs via Communicative Agent Based Data Refinement")).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了基准测试 DebugEval，它旨在从不同方面评估大型语言模型（LLM）的调试能力。我们首先描述了 DebugEval 中设计任务的任务定义（第
    [III-A](#S3.SS1 "III-A Task Definition ‣ III Evaluating the Debugging Ability
    of LLMs with DebugEval ‣ Enhancing the Code Debugging Ability of LLMs via Communicative
    Agent Based Data Refinement") 节）。然后，我们详细说明了构建 DebugEval 基准测试的过程（第 [III-B](#S3.SS2
    "III-B Details of Data Construction ‣ III Evaluating the Debugging Ability of
    LLMs with DebugEval ‣ Enhancing the Code Debugging Ability of LLMs via Communicative
    Agent Based Data Refinement") 节）。
- en: III-A Task Definition
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 任务定义
- en: DebugEval introduces four different tasks to evaluate the debugging ability
    of LLMs. The evaluation tasks include BUG Localization, BUG Identification, Code
    Review, and Code Repair.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DebugEval 引入了四个不同的任务来评估 LLM 的调试能力。这些评估任务包括 BUG 定位、BUG 识别、代码审查和代码修复。
- en: The data statistics of DebugEval are shown in Table [I](#S3.T1 "TABLE I ‣ III-A
    Task Definition ‣ III Evaluating the Debugging Ability of LLMs with DebugEval
    ‣ Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data
    Refinement"). For each task, there are questions in Python, C++, and Java to evaluate
    LLMs’ debugging performance on different programming languages. There are 578,
    2320, 2400, and 414 test instances respectively for BUG Localization, BUG Identification,
    Code Review, and Code Repair tasks, nearly evenly distributed among the three
    programming languages. In the rest of this subsection, we present the illustrations
    of these four evaluation tasks in Figure [2](#S2.F2 "Figure 2 ‣ II Related Work
    ‣ Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data
    Refinement") and delve deeper to describe each evaluation task.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DebugEval 的数据统计如表 [I](#S3.T1 "TABLE I ‣ III-A Task Definition ‣ III Evaluating
    the Debugging Ability of LLMs with DebugEval ‣ Enhancing the Code Debugging Ability
    of LLMs via Communicative Agent Based Data Refinement") 所示。对于每个任务，我们提供了 Python、C++
    和 Java 中的问题，以评估 LLM 在不同编程语言中的调试表现。BUG 定位、BUG 识别、代码审查和代码修复任务分别有 578、2320、2400 和
    414 个测试实例，这些实例在三种编程语言中几乎均匀分布。在本小节的其余部分，我们展示了图 [2](#S2.F2 "Figure 2 ‣ II Related
    Work ‣ Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based
    Data Refinement") 中这四个评估任务的示例，并深入描述每个评估任务。
- en: 'TABLE I: Data Statistics of DebugEval.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：DebugEval 数据统计。
- en: '| Evaluation Tasks | Language | #Test |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 评估任务 | 语言 | 测试数量 |'
- en: '| BUG Localization: Identify which code fragm- ent in the buggy code causing
    the error. | Python | 178 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| BUG 定位：确定在有错误的代码中哪个代码片段导致了错误。 | Python | 178 |'
- en: '| C++ | 195 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 195 |'
- en: '| Java | 205 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Java | 205 |'
- en: '| BUG Identification: Identify the type of error in your code. | Python | 760
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| BUG 识别：识别代码中的错误类型。 | Python | 760 |'
- en: '| C++ | 800 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 800 |'
- en: '| Java | 760 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Java | 760 |'
- en: '| Code Review: Determine which code is incor- rect. | Python | 800 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 代码审查：确定哪个代码是错误的。 | Python | 800 |'
- en: '| C++ | 800 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 800 |'
- en: '| Java | 800 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Java | 800 |'
- en: '| Code Repair: Fixing the buggy code to passes all the test cases. | Python
    | 138 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 代码修复：修复有错误的代码，使其通过所有测试用例。 | Python | 138 |'
- en: '| C++ | 138 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| C++ | 138 |'
- en: '| Java | 138 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Java | 138 |'
- en: BUG Localization. The BUG Localization task focuses on identifying the specific
    line(s) of code that contains the error. It evaluates the ability of LLMs to point
    out the exact location where the error occurs within a code snippet, which is
    usually regarded as the first step in the debugging process. For each test instance
    of the BUG Localization task, we give a buggy code $P$, which contains error.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: BUG 定位。BUG 定位任务侧重于识别包含错误的特定代码行。它评估 LLM 指出代码片段中错误发生的确切位置的能力，这通常被视为调试过程中的第一步。对于每个
    BUG 定位任务的测试实例，我们提供了一个包含错误的有缺陷代码 $P$。
- en: BUG Identification. In this task, the LLMs should classify the type of error
    that occurred in the code. Specifically, given a program $P$ from four choices,
    including SyntaxError, ReferenceError, LogicError, and MultiErrors. SyntaxError
    indicates that the code contains a syntax error. ReferenceError in programming
    typically occurs when code attempts to access a variable, function, or object
    that has not been declared or is out of scope. The LogicError represents that
    the code is usually syntactically correct but contains logical error, not getting
    the expected output. MultiErrors indicates that the code segment contains various
    errors of SyntaxError, ReferenceError, and LogicError.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 错误识别。在此任务中，LLMs应分类代码中出现的错误类型。具体来说，给定一个程序$P$，从四个选项中选择，包括SyntaxError、ReferenceError、LogicError和MultiErrors。SyntaxError表示代码包含语法错误。ReferenceError在编程中通常发生在代码试图访问未声明或超出作用域的变量、函数或对象时。LogicError表示代码通常在语法上是正确的，但包含逻辑错误，没有得到预期的输出。MultiErrors表示代码段包含多种错误，包括SyntaxError、ReferenceError和LogicError。
- en: Code Review. For the Code Review task, we give the correct code $C_{i}$ in the
    experiment to avoid any potential bias.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码审查。对于代码审查任务，我们在实验中提供正确的代码$C_{i}$，以避免任何潜在的偏差。
- en: Code Repair. The Code Repair [[16](#bib.bib16), [17](#bib.bib17)] task requires
    to generate the corrected version $P^{\prime}$).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 代码修复。代码修复 [[16](#bib.bib16), [17](#bib.bib17)] 任务要求生成修正版本$P^{\prime}$。
- en: III-B Details of Data Construction
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 数据构建细节
- en: In this subsection, we elaborate on the source data collection and construction
    method for the DebugEval dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们详细说明了DebugEval数据集的源数据收集和构建方法。
- en: To ensure the quality of DebugEval, we collect high-quality data from reliable
    data sources, such as DebugBench [[17](#bib.bib17)] and LiveCodeBench [[52](#bib.bib52)],
    and the AtCoder website¹¹1[https://atcoder.jp](https://atcoder.jp). The DebugBench
    focuses on the code repair task, which needs to call LeetCode API to evaluate
    the correctness of the generated code. Each test instance of DebugBench consists
    of a buggy code and corresponding error type. The LiveCodeBench is a code generation
    dataset, each test instance contains a task description, correct code, and test
    cases to evaluate the correctness of generated codes. In this paper, we only use
    the questions from LiveCodeBench and construct one correct code and buggy code
    pair for each question from the AtCoder website.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保DebugEval的质量，我们从可靠的数据源收集高质量的数据，例如DebugBench [[17](#bib.bib17)] 和 LiveCodeBench [[52](#bib.bib52)]，以及AtCoder网站¹¹1[https://atcoder.jp](https://atcoder.jp)。DebugBench专注于代码修复任务，需要调用LeetCode
    API来评估生成代码的正确性。DebugBench的每个测试实例包含一段有错误的代码和相应的错误类型。LiveCodeBench是一个代码生成数据集，每个测试实例包含一个任务描述、正确代码和测试用例，以评估生成代码的正确性。在本文中，我们仅使用LiveCodeBench中的问题，并从AtCoder网站构建每个问题的正确代码和有错误代码对。
- en: Bug Localization. For the Bug Localization task, we sample test instances from
    DebugBench. In detail, we built the dataset for the Bug Localization task by sampling
    up to 20 instances of each of the 15 single code error types for each programming
    language. We compare the buggy code with the correct code to find the code snippet
    that contains errors as the golden choice $S_{E}$. To construct the other confusing
    choices, we discard the error code snippet and randomly sample one code line as
    one of the confusing choices.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 错误定位。对于错误定位任务，我们从DebugBench中抽取测试实例。具体来说，我们通过为每种编程语言抽取最多20个单一代码错误类型的实例来构建错误定位任务的数据集。我们将有错误的代码与正确代码进行比较，以找到包含错误的代码片段作为黄金选择$S_{E}$。为了构建其他混淆选项，我们丢弃错误代码片段，并随机抽取一行代码作为混淆选项之一。
- en: 'Bug Identification. To conduct the evaluation on the Bug Identification task,
    we also sample the test instances from DebugBench. Specifically, the choices of
    the task are divided into four types: SyntaxError, ReferenceError, LogicError,
    and MultiErrors, thus it is important to ensure the balance of each choice (code
    error type). For each programming language, we compare the sizes of the test sample
    sets for different types of code errors and then select the smallest set size
    as the sampling number. Finally, we sample test instances from the different code
    error test sets according to this sampling number.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Bug 识别。为了在 Bug 识别任务上进行评估，我们还从 DebugBench 中抽取测试实例。具体来说，任务的选择分为四种类型：SyntaxError、ReferenceError、LogicError
    和 MultiErrors，因此确保每种选择（代码错误类型）的平衡很重要。对于每种编程语言，我们比较不同类型代码错误的测试样本集的大小，然后选择最小的集大小作为抽样数量。最后，我们根据这个抽样数量从不同代码错误测试集中抽取测试实例。
- en: 'TABLE II: A Comparison between DebugEval and Other Code Repair Benchmarks.
    Size only represents the size of the test set. CE indicates compilation errors
    (e.g., SyntaxError). Source of Buggy Code indicates how the buggy code was constructed.
    Against Data Leakage indicates whether there is a data leakage. Part of the data
    of DebugEval is from the DebugBench, and there is no data leakage. The data collected
    by us is all after 2023-09-01, which also avoids data leakage.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: DebugEval 与其他代码修复基准的比较。大小仅表示测试集的规模。CE 表示编译错误（例如，语法错误）。Buggy 代码来源指示了有缺陷代码的构建方式。数据泄露指示是否存在数据泄露。DebugEval
    的部分数据来自 DebugBench，并且没有数据泄露。我们收集的数据均为 2023-09-01 之后的数据，这也避免了数据泄露。'
- en: '| Dataset | Language | Task | Size | Error Type | Source of Bugs | Against
    Data Leakage |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 语言 | 任务 | 大小 | 错误类型 | 错误来源 | 数据泄露 |'
- en: '| DeepFix [[53](#bib.bib53)] | C | Code Repair | 6,971 | CE Only | User Submission
    | ✗ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| DeepFix [[53](#bib.bib53)] | C | 代码修复 | 6,971 | 仅限 CE | 用户提交 | ✗ |'
- en: '| Review4Repair [[54](#bib.bib54)] | Java | Code Repair | 2,961 | All | User
    Submission | ✗ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Review4Repair [[54](#bib.bib54)] | Java | 代码修复 | 2,961 | 所有 | 用户提交 | ✗ |'
- en: '| Bug2Fix [[55](#bib.bib55)] | Java | Code Repair | 5,835 | All | User Submission
    | ✗ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Bug2Fix [[55](#bib.bib55)] | Java | 代码修复 | 5,835 | 所有 | 用户提交 | ✗ |'
- en: '| Github-Python [[53](#bib.bib53)] | Python | Code Repair | 15K | CE Only |
    User Submission | ✗ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Github-Python [[53](#bib.bib53)] | Python | 代码修复 | 15K | 仅限 CE | 用户提交 | ✗
    |'
- en: '| FixEval [[56](#bib.bib56)] | Java/Python | Code Repair | 43k/243k | All |
    User Submission | ✗ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| FixEval [[56](#bib.bib56)] | Java/Python | 代码修复 | 43k/243k | 所有 | 用户提交 |
    ✗ |'
- en: '| CodeError [[16](#bib.bib16)] | Python | Code Repair | 4,463 | All | User
    Submission | ✗ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| CodeError [[16](#bib.bib16)] | Python | 代码修复 | 4,463 | 所有 | 用户提交 | ✗ |'
- en: '| DebugBench [[17](#bib.bib17)] | C++/Java/Python | Code Repair | 1,438/1,401/1,414
    | All | GPT-4 Generation | ✓ |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| DebugBench [[17](#bib.bib17)] | C++/Java/Python | 代码修复 | 1,438/1,401/1,414
    | 所有 | GPT-4 生成 | ✓ |'
- en: '| CodeEditorBench [[18](#bib.bib18)] | C++/Java/Python | Code Repair | 676/515/716
    | All | GPT-4 Generation | ✓ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| CodeEditorBench [[18](#bib.bib18)] | C++/Java/Python | 代码修复 | 676/515/716
    | 所有 | GPT-4 生成 | ✓ |'
- en: '| DebugEval | C++/Java/Python |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| DebugEval | C++/Java/Python |'
- en: '&#124; BUG Localization &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BUG 定位 &#124;'
- en: '&#124; BUG Identification &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BUG 识别 &#124;'
- en: '&#124; Code Review &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码评审 &#124;'
- en: '&#124; Code Repair &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码修复 &#124;'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 195/205/178 &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 195/205/178 &#124;'
- en: '&#124; 800/760/760 &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 800/760/760 &#124;'
- en: '&#124; 800/800/800 &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 800/800/800 &#124;'
- en: '&#124; 138/138/138 &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 138/138/138 &#124;'
- en: '| All |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 所有 |'
- en: '&#124; User Submission &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户提交 &#124;'
- en: '&#124; GPT-4 Generation &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPT-4 生成 &#124;'
- en: '| ✓ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |'
- en: Code Review. We first mix the test instances from both DebugBench and LiveCodeBench.
    Then we collect the test pairs from the mixed dataset by randomly sampling 800
    test instances from each programming language. Each test instance consists of
    the buggy code and the correct code to ask LLMs to choose the buggy one.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 代码评审。我们首先将来自 DebugBench 和 LiveCodeBench 的测试实例混合。然后，我们通过从每种编程语言中随机抽取 800 个测试实例，从混合数据集中收集测试对。每个测试实例包含有缺陷的代码和正确的代码，让
    LLMs 选择有缺陷的代码。
- en: Code Repair. For code repair, we first collect 138 newly released programming
    competition problems from Atcoder from September 1, 2023, to April 1, 2024, which
    reduces the risk of data leakage. Then we collect buggy code submissions from
    real users and respectively choose one buggy code of Python, C++, and Java. Lots
    of buggy codes (82.7%) have the following submissions that correct the buggy codes
    and pass all test cases, which are regarded as the golden answer of the buggy
    code. For the rest of these buggy codes (17.3%), we use different LLMs to correct
    the buggy codes until the corrected code passes all test cases.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 代码修复。为了进行代码修复，我们首先收集了2023年9月1日至2024年4月1日期间，从Atcoder发布的138个新编程竞赛问题，这样可以降低数据泄露的风险。然后，我们从真实用户那里收集了有缺陷的代码提交，并分别选择了Python、C++和Java的一个有缺陷的代码。许多有缺陷的代码（82.7%）具有以下提交，这些提交修正了有缺陷的代码并通过了所有测试用例，被认为是有缺陷代码的黄金答案。对于其余这些有缺陷的代码（17.3%），我们使用不同的LLM修正这些有缺陷的代码，直到修正后的代码通过所有测试用例。
- en: Summary. As shown in Table [II](#S3.T2 "TABLE II ‣ III-B Details of Data Construction
    ‣ III Evaluating the Debugging Ability of LLMs with DebugEval ‣ Enhancing the
    Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement"),
    different from other debugging benchmarks, DebugEval is a multi-lingual and multi-task
    debugging benchmark, which conducts more complete evaluations on the debugging
    ability of LLMs. Besides, DebugEval contains the buggy codes that are generated
    by GPT-4 and humans, making the evaluation more convincing, realistic, and authentic.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。如表[II](#S3.T2 "TABLE II ‣ III-B Details of Data Construction ‣ III Evaluating
    the Debugging Ability of LLMs with DebugEval ‣ Enhancing the Code Debugging Ability
    of LLMs via Communicative Agent Based Data Refinement")所示，不同于其他调试基准，DebugEval是一个多语言和多任务的调试基准，它对LLMs的调试能力进行更全面的评估。此外，DebugEval包含由GPT-4和人类生成的有缺陷代码，使评估更具说服力、现实性和真实性。
- en: '![Refer to caption](img/0c4cd2a8157146d546d7f2e5a13e6663.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0c4cd2a8157146d546d7f2e5a13e6663.png)'
- en: 'Figure 3: Illustration of coMmunicative Agent baSed daTa rEfinement fRamework
    (MASTER).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: coMmunicative Agent baSed daTa rEfinement fRamework (MASTER) 的示意图。'
- en: IV CoMmunicative Agent BaSed DaTa REfinement FRamework
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV CoMmunicative Agent BaSed DaTa REfinement FRamework
- en: Supervised Fine-Tuning (SFT) has been widely adopted to enhance the performance
    of LLMs in specific domains [[37](#bib.bib37), [57](#bib.bib57)] using human-labeled
    datasets or LLM-generated data [[15](#bib.bib15)]. However, SFT’s reliance on
    the availability and quality of labeled data limits its overall effectiveness.
    In this case, this paper introduces the coMmunicative Agent baSed daTa rEfinement
    fRamework (MASTER), which automatically refines code debugging data for Supervised
    Fine-Tuning. As shown in Figure [3](#S3.F3 "Figure 3 ‣ III-B Details of Data Construction
    ‣ III Evaluating the Debugging Ability of LLMs with DebugEval ‣ Enhancing the
    Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement"),
    we give the task examples to LLMs and then prompt LLMs to play different roles
    to synthesize the SFT data (Sec. [IV-A](#S4.SS1 "IV-A Agent Building ‣ IV CoMmunicative
    Agent BaSed DaTa REfinement FRamework ‣ Enhancing the Code Debugging Ability of
    LLMs via Communicative Agent Based Data Refinement")). Finally, MASTER employs
    different agents to refine the synthesized data to guarantee the quality of the
    SFT data (Sec. [IV-B](#S4.SS2 "IV-B SFT Data Refinement with Communicative Multi-Agents
    ‣ IV CoMmunicative Agent BaSed DaTa REfinement FRamework ‣ Enhancing the Code
    Debugging Ability of LLMs via Communicative Agent Based Data Refinement")).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调（SFT）已被广泛应用于提升LLMs在特定领域的性能[[37](#bib.bib37), [57](#bib.bib57)]，使用人工标注的数据集或LLM生成的数据[[15](#bib.bib15)]。然而，SFT对标注数据的可用性和质量的依赖限制了其总体效果。在这种情况下，本文介绍了coMmunicative
    Agent baSed daTa rEfinement fRamework (MASTER)，它自动精炼用于监督微调的代码调试数据。如图[3](#S3.F3
    "Figure 3 ‣ III-B Details of Data Construction ‣ III Evaluating the Debugging
    Ability of LLMs with DebugEval ‣ Enhancing the Code Debugging Ability of LLMs
    via Communicative Agent Based Data Refinement")所示，我们将任务示例提供给LLMs，然后提示LLMs扮演不同角色以合成SFT数据（第[IV-A](#S4.SS1
    "IV-A Agent Building ‣ IV CoMmunicative Agent BaSed DaTa REfinement FRamework
    ‣ Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data
    Refinement")节）。最后，MASTER利用不同的代理来精炼合成的数据，以保证SFT数据的质量（第[IV-B](#S4.SS2 "IV-B SFT
    Data Refinement with Communicative Multi-Agents ‣ IV CoMmunicative Agent BaSed
    DaTa REfinement FRamework ‣ Enhancing the Code Debugging Ability of LLMs via Communicative
    Agent Based Data Refinement")节）。
- en: IV-A Agent Building
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 代理构建
- en: To conduct the data refinement, MASTER constructs three agents that collaboratively
    generate and refine the debugging problem data to synthesize high-quality SFT
    datasets. As illustrated in Figure [4](#S4.F4 "Figure 4 ‣ IV-A Agent Building
    ‣ IV CoMmunicative Agent BaSed DaTa REfinement FRamework ‣ Enhancing the Code
    Debugging Ability of LLMs via Communicative Agent Based Data Refinement"), we
    employ different prompts to guide LLMs to play the roles of Code Quizzer, Code
    Learner, and Code Teacher. The details of each agent are described below.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行数据精炼，MASTER构建了三个智能体，这些智能体协作生成和精炼调试问题数据，以合成高质量的SFT数据集。如图[4](#S4.F4 "图4 ‣
    IV-A 智能体构建 ‣ IV 沟通型智能体数据精炼框架 ‣ 通过沟通型智能体数据精炼提升LLMs的代码调试能力")所示，我们使用不同的提示来指导LLMs扮演代码测试器、代码学习者和代码教师的角色。各个智能体的详细信息如下。
- en: 'Code Quizzer. The Code Quizzer is designed to generate high-quality problems
    for the SFT data. It uses a stronger LLM as the backbone model and provides the
    instruction: “You are a code debugging expert, skilled in generating code debugging
    problems to challenge programmers”. This setup enables the Code Quizzer to generate
    tailored problems by analyzing examples of debugging tasks. These problems are
    intended to evaluate the Code Learner’s ability to solve the corresponding debugging
    tasks.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代码测试器。代码测试器旨在为SFT数据生成高质量的问题。它使用更强大的LLM作为骨干模型，并提供指令：“你是一个代码调试专家，擅长生成代码调试问题以挑战程序员”。这一设置使得代码测试器能够通过分析调试任务示例生成量身定制的问题。这些问题旨在评估代码学习者解决相应调试任务的能力。
- en: 'Code Learner. The Code Learner shares the same backbone model as the SFT model
    and serves as the critic to evaluate the educational value of the problems generated
    by the Code Quizzer. Using the prompt: “You are a student, please provide an answer
    to the following code debugging question using your own knowledge”, the Code Learner
    is tasked with solving the problem based on its memorized knowledge. The educational
    value of the problem is then assessed by determining whether the Code Learner
    can correctly solve it, thereby contributing to the finetuning process.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 代码学习者。代码学习者与SFT模型共享相同的骨干模型，充当评审员，评估代码测试器生成的问题的教育价值。使用提示：“你是一个学生，请使用你的知识回答以下代码调试问题”，代码学习者的任务是根据其记忆中的知识解决问题。然后通过确定代码学习者是否能正确解决问题来评估问题的教育价值，从而有助于微调过程。
- en: 'Code Teacher. Inspired by previous work [[16](#bib.bib16)], we also develop
    a Code Teacher by prompting the same LLM used for the Code Quizzer with the instruction:
    “You are an experienced and insightful debugger”. This prompt directs the LLM
    to act as a proficient code debugger, generating chain-of-thought outcomes [[58](#bib.bib58)]
    as detailed solutions to the problems, which can better guide LLMs during the
    SFT process.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 代码教师。受到之前工作的启发[[16](#bib.bib16)]，我们还通过对代码测试器使用相同的LLM进行提示，开发了一个代码教师，指令为：“你是一个经验丰富且有洞察力的调试员”。该提示引导LLM作为一个熟练的代码调试员，生成链式思考结果[[58](#bib.bib58)]作为问题的详细解决方案，这可以更好地指导LLMs在SFT过程中的操作。
- en: '![Refer to caption](img/48e826f1c4ef5726a05c3d1124305b1e.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/48e826f1c4ef5726a05c3d1124305b1e.png)'
- en: 'Figure 4: Illustrations of Prompts Used in MASTER to Build the Agents. Within
    MASTER, there are three LLM-based agents Code Quizzer, Code Learner, and Code
    Teacher. We utilize specific instructions to ensure that they play the correct
    roles and carry out the intended tasks.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：MASTER中用于构建智能体的提示示意图。在MASTER中，有三个基于LLM的智能体——代码测试器、代码学习者和代码教师。我们利用特定的指令确保它们扮演正确的角色并完成预期的任务。
- en: IV-B SFT Data Refinement with Communicative Multi-Agents
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B SFT数据精炼与沟通型多智能体
- en: MASTER achieves automated data refinement through multi-agent collaboration,
    leveraging the expertise of stronger models to enhance the capabilities of weaker
    ones. The data refinement process consists of three main steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: MASTER通过多智能体协作实现自动化数据精炼，利用更强大模型的专长来提升较弱模型的能力。数据精炼过程包括三个主要步骤。
- en: In the initial step (Step A), the Code Quizzer synthesizes various code debugging
    problems based on examples from the debugging task. To ensure diversity in the
    synthesized data, we instruct the Code Quizzer to generate different debugging
    problems aligned with the tasks defined in DebugEval, including Bug Localization,
    Bug Identification, Code Review, and Code Repair. The data synthesis process of
    each debugging task is guided by a single example, which serves as the demonstrations [[59](#bib.bib59)].
    This approach ensures that the synthesized data encompasses a range of error types
    and difficulty levels, which is crucial for distilling debugging knowledge from
    the Code Quizzer/Teacher model during SFT Code Learner.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始步骤 (步骤 A) 中，代码测试者基于调试任务中的示例综合各种代码调试问题。为了确保综合数据的多样性，我们指示代码测试者生成与 DebugEval
    中定义的任务一致的不同调试问题，包括 BUG 定位、BUG 识别、代码审查和代码修复。每个调试任务的数据综合过程由一个示例指导，该示例作为演示 [[59](#bib.bib59)]。这种方法确保综合数据涵盖了各种错误类型和难度级别，这对于在
    SFT 代码学习者期间从代码测试者/教师模型中提炼调试知识至关重要。
- en: After synthesizing the debugging problems, we proceed to Step B. At this step,
    the Code Learner attempts to solve the problems provided by the Code Quizzer.
    In this case, the Code Learner acts as a critic, assessing the educational value
    of each synthesized problem for the Code Learner. If the Code Learner solves the
    problem correctly, it indicates that the learner already possesses the necessary
    knowledge to solve the problem, and thereby the problem is discarded. On the other
    hand, if the Code Learner provides an incorrect solution, the problem is reserved
    as the SFT data, due to its educational value for guiding the Code Learner.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在综合调试问题后，我们进入步骤 B。在此步骤中，代码学习者尝试解决代码测试者提供的问题。在这种情况下，代码学习者充当评论员，评估每个综合问题对代码学习者的教育价值。如果代码学习者正确解决了问题，这表明学习者已经具备解决该问题所需的知识，因此该问题将被丢弃。另一方面，如果代码学习者提供了错误的解决方案，该问题将保留为
    SFT 数据，因为它具有指导代码学习者的教育价值。
- en: Finally, in Step C, the Code Teacher reviews the reserved problems and generates
    detailed explanations and solutions. These Chain-of-Thought based explanations
    may include error type identification, error explanations, and the correct solutions
    to solve the problems. This feedback is essential for the Code Learner to comprehend
    the problems and refine their solutions. The responses generated by the Code Teacher
    are treated as the final outputs for the synthesized problems, forming the SFT
    data to fine-tune our NeuDebugger model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在步骤 C 中，代码教师审查保留的问题，并生成详细的解释和解决方案。这些基于 Chain-of-Thought 的解释可能包括错误类型识别、错误解释和解决问题的正确方案。这些反馈对代码学习者理解问题和完善他们的解决方案至关重要。代码教师生成的响应被视为综合问题的最终输出，形成
    SFT 数据，用于微调我们的 NeuDebugger 模型。
- en: 'TABLE III: Data Statistics of Different Supervised Fine-Tuning Strategies.
    We collect high-quality instruction tuning data from UltraInteract, InstructCoder
    and RepairLlama to conduct the Vanilla SFT setting.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：不同监督微调策略的数据统计。我们从 UltraInteract、InstructCoder 和 RepairLlama 收集高质量的指令调整数据，以进行
    Vanilla SFT 设置。
- en: '| SFT Data | Data Source | #Instance |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| SFT 数据 | 数据来源 | #实例 |'
- en: '| Human/GPT-4 | UltraInteract [[35](#bib.bib35)] | 154,347 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 人类/GPT-4 | UltraInteract [[35](#bib.bib35)] | 154,347 |'
- en: '| InstructCoder [[60](#bib.bib60)] | 6,913 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| InstructCoder [[60](#bib.bib60)] | 6,913 |'
- en: '| RepairLlama [[61](#bib.bib61)] | 64,643 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| RepairLlama [[61](#bib.bib61)] | 64,643 |'
- en: '| MASTER | BUG Localization data | 4,681 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| MASTER | BUG 定位数据 | 4,681 |'
- en: '| BUG Identification data | 4,474 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| BUG 识别数据 | 4,474 |'
- en: '| Code Review data | 4,420 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 代码审查数据 | 4,420 |'
- en: '| Code Repair data | 11,317 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 代码修复数据 | 11,317 |'
- en: V Experimental Methodology
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验方法
- en: In this section, we describe the Supervised Fine-Tuning (SFT) strategies, evaluation
    metrics, details of evaluated foundation models, and implementation details of
    our experiments.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了监督微调 (SFT) 策略、评估指标、评估基础模型的详细信息以及我们实验的实施细节。
- en: Supervised Fine-Tuning Strategies. As shown in Table [III](#S4.T3 "TABLE III
    ‣ IV-B SFT Data Refinement with Communicative Multi-Agents ‣ IV CoMmunicative
    Agent BaSed DaTa REfinement FRamework ‣ Enhancing the Code Debugging Ability of
    LLMs via Communicative Agent Based Data Refinement"), we describe the experimental
    details of different supervised fine-tuning strategies, including Vanilla SFT
    and MASTER. For Vanilla SFT, we collect SFT data from UltraInteract [[35](#bib.bib35)],
    InstructCoder [[60](#bib.bib60)], and RepairLlama [[61](#bib.bib61)] to finetune
    LLMs. These datasets are generated by GPT-4 or annotated by humans, which are
    of high-quality. For MASTER, we synthesize the SFT data by employing multi-agents.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调策略。如表[III](#S4.T3 "TABLE III ‣ IV-B SFT Data Refinement with Communicative
    Multi-Agents ‣ IV CoMmunicative Agent BaSed DaTa REfinement FRamework ‣ Enhancing
    the Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement")所示，我们描述了不同监督微调策略的实验细节，包括Vanilla
    SFT和MASTER。对于Vanilla SFT，我们从UltraInteract [[35](#bib.bib35)]、InstructCoder [[60](#bib.bib60)]和RepairLlama [[61](#bib.bib61)]收集SFT数据来微调LLMs。这些数据集由GPT-4生成或由人工标注，质量很高。对于MASTER，我们通过使用多智能体合成SFT数据。
- en: Evaluation Metrics. We first introduce the evaluation metrics used to evaluate
    different models on DebugEval.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们首先介绍用于评估不同模型在DebugEval上的评估指标。
- en: 'For BUG Localization, BUG Identification, and Code Review tasks, LLMs need
    to give one answer from multiple choices of the given question. Thus, we use Accuracy
    as the evaluation metric for these three tasks, which is the same as the previous
    work [[62](#bib.bib62)]. In particular, for the Code Review task, since the model
    is asked to choose between two options, we scrambled the order of the options
    to avoid potential Bias. For each piece of test data, the model can only be considered
    correct if both orders are answered correctly. For the Code Repair task, we follow
    previous work [[63](#bib.bib63), [12](#bib.bib12), [11](#bib.bib11), [64](#bib.bib64)]
    and we use Pass@$k$ [[63](#bib.bib63)] to evaluate the effectiveness of different
    LLMs:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BUG定位、BUG识别和代码审查任务，LLMs需要从给定问题的多个选项中选择一个答案。因此，我们使用准确率作为这三项任务的评估指标，这与以前的工作[[62](#bib.bib62)]相同。特别是，对于代码审查任务，由于模型需要在两个选项之间进行选择，我们打乱了选项的顺序以避免潜在的偏见。对于每个测试数据，模型只有在两个顺序均回答正确时才能被视为正确。对于代码修复任务，我们遵循以前的工作[[63](#bib.bib63),
    [12](#bib.bib12), [11](#bib.bib11), [64](#bib.bib64)]，使用Pass@$k$ [[63](#bib.bib63)]来评估不同LLMs的有效性：
- en: '|  | $\text{Pass@}k:=\underset{\text{Problems}}{\operatorname*{\mathbb{E}}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right],$
    |  | (1) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Pass@}k:=\underset{\text{Problems}}{\operatorname*{\mathbb{E}}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right],$
    |  | (1) |'
- en: where $n$.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$。
- en: 'TABLE IV: Evaluation Results of Different LLMs on DebugEval. The three tasks,
    including BUG Localization, BUG Identification, and Code Review, are evaluated
    using Accuracy. Code Repair is evaluated with Pass$@1$. Here, we use DS to represent
    the DeepSeek model and PY to denote Python.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表IV：不同LLMs在DebugEval上的评估结果。包括BUG定位、BUG识别和代码审查的三项任务使用准确率进行评估。代码修复任务使用Pass$@1$进行评估。这里，我们用DS表示DeepSeek模型，用PY表示Python。
- en: '| Model | BUG Localization | BUG Identification | Code Review | Code Repair
    | Avg. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | BUG定位 | BUG识别 | 代码审查 | 代码修复 | 平均 |'
- en: '| PY | C++ | JAVA | Avg. | PY | C++ | JAVA | Avg. | PY | C++ | JAVA | Avg.
    | PY | C++ | JAVA | Avg. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| PY | C++ | JAVA | 平均 | PY | C++ | JAVA | 平均 | PY | C++ | JAVA | 平均 | PY |
    C++ | JAVA | 平均 |'
- en: '| GPT-4o-mini-0718 [[65](#bib.bib65)] | 84.8 | 81.0 | 81.5 | 82.4 | 53.3 |
    48.5 | 48.9 | 50.2 | 85.4 | 90.9 | 91.0 | 89.1 | 65.2 | 67.2 | 67.4 | 66.6 | 72.1
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini-0718 [[65](#bib.bib65)] | 84.8 | 81.0 | 81.5 | 82.4 | 53.3 |
    48.5 | 48.9 | 50.2 | 85.4 | 90.9 | 91.0 | 89.1 | 65.2 | 67.2 | 67.4 | 66.6 | 72.1
    |'
- en: '| GPT-3.5-Turbo-0125 [[7](#bib.bib7)] | 40.4 | 47.2 | 52.2 | 46.9 | 35.5 |
    33.3 | 34.1 | 34.3 | 79.4 | 82.4 | 84.0 | 81.9 | 57.2 | 52.9 | 61.6 | 57.2 | 55.1
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-0125 [[7](#bib.bib7)] | 40.4 | 47.2 | 52.2 | 46.9 | 35.5 |
    33.3 | 34.1 | 34.3 | 79.4 | 82.4 | 84.0 | 81.9 | 57.2 | 52.9 | 61.6 | 57.2 | 55.1
    |'
- en: '| DeepSeek-V2-0628 [[66](#bib.bib66)] | 82.0 | 81.0 | 85.9 | 83.0 | 62.0 |
    61.0 | 61.3 | 61.4 | 77.4 | 83.9 | 80.5 | 80.6 | 65.2 | 63.0 | 63.5 | 63.9 | 72.2
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-V2-0628 [[66](#bib.bib66)] | 82.0 | 81.0 | 85.9 | 83.0 | 62.0 |
    61.0 | 61.3 | 61.4 | 77.4 | 83.9 | 80.5 | 80.6 | 65.2 | 63.0 | 63.5 | 63.9 | 72.2
    |'
- en: '| DeepSeek-Coder-V2-0724 [[67](#bib.bib67)] | 88.8 | 83.1 | 89.8 | 87.2 | 58.7
    | 58.9 | 60.8 | 59.4 | 87.9 | 94.9 | 93.3 | 92.0 | 66.7 | 63.1 | 62.3 | 64.0 |
    75.7 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder-V2-0724 [[67](#bib.bib67)] | 88.8 | 83.1 | 89.8 | 87.2 | 58.7
    | 58.9 | 60.8 | 59.4 | 87.9 | 94.9 | 93.3 | 92.0 | 66.7 | 63.1 | 62.3 | 64.0 |
    75.7 |'
- en: '| Llama3-70B-Ins [[68](#bib.bib68)] | 74.2 | 75.9 | 82.0 | 77.5 | 42.8 | 42.3
    | 44.9 | 43.3 | 73.9 | 61.6 | 63.3 | 66.3 | 44.9 | 44.2 | 45.7 | 44.9 | 58.0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-70B-Ins [[68](#bib.bib68)] | 74.2 | 75.9 | 82.0 | 77.5 | 42.8 | 42.3
    | 44.9 | 43.3 | 73.9 | 61.6 | 63.3 | 66.3 | 44.9 | 44.2 | 45.7 | 44.9 | 58.0 |'
- en: '| Qwen2-72B-Ins [[69](#bib.bib69)] | 79.8 | 69.2 | 74.6 | 74.4 | 45.8 | 45.0
    | 41.3 | 44.1 | 61.5 | 75.8 | 70.4 | 69.2 | 43.5 | 42.0 | 42.8 | 42.8 | 57.6 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-72B-Ins [[69](#bib.bib69)] | 79.8 | 69.2 | 74.6 | 74.4 | 45.8 | 45.0
    | 41.3 | 44.1 | 61.5 | 75.8 | 70.4 | 69.2 | 43.5 | 42.0 | 42.8 | 42.8 | 57.6 |'
- en: '| DSCoder-33B-Ins [[64](#bib.bib64)] | 52.2 | 50.3 | 51.7 | 51.4 | 24.9 | 26.0
    | 30.9 | 27.2 | 24.8 | 27.0 | 30.5 | 27.4 | 46.4 | 50.7 | 54.3 | 50.5 | 39.1 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| DSCoder-33B-Ins [[64](#bib.bib64)] | 52.2 | 50.3 | 51.7 | 51.4 | 24.9 | 26.0
    | 30.9 | 27.2 | 24.8 | 27.0 | 30.5 | 27.4 | 46.4 | 50.7 | 54.3 | 50.5 | 39.1 |'
- en: '| Llama2-7B-Ins [[8](#bib.bib8)] | 18.0 | 20.0 | 22.4 | 20.2 | 24.9 | 27.0
    | 25.8 | 25.9 | 2.3 | 0.6 | 2.0 | 1.6 | 4.3 | 11.7 | 19.6 | 11.9 | 14.9 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-Ins [[8](#bib.bib8)] | 18.0 | 20.0 | 22.4 | 20.2 | 24.9 | 27.0
    | 25.8 | 25.9 | 2.3 | 0.6 | 2.0 | 1.6 | 4.3 | 11.7 | 19.6 | 11.9 | 14.9 |'
- en: '| CodeLlama-7B-Ins [[37](#bib.bib37)] | 27.0 | 20.0 | 23.9 | 23.5 | 26.1 |
    23.0 | 23.8 | 24.3 | 48.1 | 60.5 | 65.6 | 58.1 | 18.8 | 23.2 | 23.2 | 21.7 | 31.9
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-Ins [[37](#bib.bib37)] | 27.0 | 20.0 | 23.9 | 23.5 | 26.1 |
    23.0 | 23.8 | 24.3 | 48.1 | 60.5 | 65.6 | 58.1 | 18.8 | 23.2 | 23.2 | 21.7 | 31.9
    |'
- en: '| CodeQwen1.5-7B-Ins [[36](#bib.bib36)] | 29.2 | 30.8 | 38.0 | 32.9 | 27.6
    | 25.9 | 28.8 | 27.4 | 26.9 | 34.4 | 37.1 | 32.8 | 39.1 | 49.3 | 52.9 | 47.1 |
    35.1 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5-7B-Ins [[36](#bib.bib36)] | 29.2 | 30.8 | 38.0 | 32.9 | 27.6
    | 25.9 | 28.8 | 27.4 | 26.9 | 34.4 | 37.1 | 32.8 | 39.1 | 49.3 | 52.9 | 47.1 |
    35.1 |'
- en: '| DeepSeek-LLM-7B-Ins [[70](#bib.bib70)] | 27.0 | 19.0 | 25.9 | 23.9 | 30.5
    | 28.5 | 30.9 | 29.9 | 35.9 | 36.6 | 46.0 | 39.5 | 21.0 | 24.1 | 14.5 | 19.9 |
    28.3 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-LLM-7B-Ins [[70](#bib.bib70)] | 27.0 | 19.0 | 25.9 | 23.9 | 30.5
    | 28.5 | 30.9 | 29.9 | 35.9 | 36.6 | 46.0 | 39.5 | 21.0 | 24.1 | 14.5 | 19.9 |
    28.3 |'
- en: '| DSCoder-6.7B-Ins [[64](#bib.bib64)] | 22.5 | 25.6 | 33.7 | 27.5 | 26.6 |
    26.0 | 25.9 | 26.2 | 15.5 | 17.8 | 27.8 | 20.3 | 31.9 | 43.5 | 46.4 | 40.6 | 28.7
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| DSCoder-6.7B-Ins [[64](#bib.bib64)] | 22.5 | 25.6 | 33.7 | 27.5 | 26.6 |
    26.0 | 25.9 | 26.2 | 15.5 | 17.8 | 27.8 | 20.3 | 31.9 | 43.5 | 46.4 | 40.6 | 28.7
    |'
- en: '| Llama3-8B-Ins [[68](#bib.bib68)] | 55.6 | 55.9 | 61.0 | 57.6 | 36.8 | 38.1
    | 34.6 | 36.6 | 69.1 | 77.4 | 78.1 | 74.9 | 26.1 | 34.3 | 28.3 | 29.6 | 49.7 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Ins [[68](#bib.bib68)] | 55.6 | 55.9 | 61.0 | 57.6 | 36.8 | 38.1
    | 34.6 | 36.6 | 69.1 | 77.4 | 78.1 | 74.9 | 26.1 | 34.3 | 28.3 | 29.6 | 49.7 |'
- en: '| NeuDebugger-DS-6.7B | 62.4 | 55.4 | 59.0 | 58.8 | 42.6 | 46.9 | 47.8 | 45.8
    | 71.0 | 71.4 | 71.4 | 71.3 | 43.5 | 48.6 | 56.5 | 49.5 | 56.4 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger-DS-6.7B | 62.4 | 55.4 | 59.0 | 58.8 | 42.6 | 46.9 | 47.8 | 45.8
    | 71.0 | 71.4 | 71.4 | 71.3 | 43.5 | 48.6 | 56.5 | 49.5 | 56.4 |'
- en: '| NeuDebugger-Llama3-8B | 64.6 | 57.9 | 61.0 | 61.1 | 38.6 | 29.9 | 33.3 |
    33.8 | 75.3 | 78.0 | 82.4 | 78.5 | 38.4 | 41.3 | 45.7 | 41.8 | 53.8 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger-Llama3-8B | 64.6 | 57.9 | 61.0 | 61.1 | 38.6 | 29.9 | 33.3 |
    33.8 | 75.3 | 78.0 | 82.4 | 78.5 | 38.4 | 41.3 | 45.7 | 41.8 | 53.8 |'
- en: Foundation Models. We evaluated 13 LLMs on DebugEval, including both closed-source
    and open-source models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型。我们在 DebugEval 上评估了 13 款 LLM，包括闭源模型和开源模型。
- en: OpenAI GPTs. GPT-4o-mini-0718 [[65](#bib.bib65)] and GPT-3.5-Turbo-0125 [[7](#bib.bib7)]
    are two popular and powerful LLMs, which belong to different variants of the GPT
    family, developed by OpenAI. GPT-4o-mini-0718 is a lightweight version of the
    GPT-4o model, but it still inherits the core advantages of the GPT-4o, including
    powerful text generation, logical reasoning, and code generation. Both models
    are black-box models, which supply commercial APIs for usage.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPTs。GPT-4o-mini-0718 [[65](#bib.bib65)] 和 GPT-3.5-Turbo-0125 [[7](#bib.bib7)]
    是两款流行且强大的 LLM，它们属于不同的 GPT 系列变体，由 OpenAI 开发。GPT-4o-mini-0718 是 GPT-4o 模型的轻量版本，但它仍然继承了
    GPT-4o 的核心优势，包括强大的文本生成、逻辑推理和代码生成。两款模型都是黑箱模型，提供商业 API 供使用。
- en: Meta Llama. Llama2-7B-Ins [[8](#bib.bib8)] is an open-sourced LLM. It is trained
    with up to 1.4 trillion tokens, where 4.5% of them are code tokens from Github.
    CodeLlama-7B-Ins [[37](#bib.bib37)] conducts additional instruction-tuning stage
    to adapt Llama2 [[8](#bib.bib8)] to improve the effectiveness in code-related
    tasks. Recently, Llama3 [[68](#bib.bib68)] models are released, which is a major
    leap over Llama2 models and establish a new state-of-the-art.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Meta Llama。Llama2-7B-Ins [[8](#bib.bib8)] 是一个开源的 LLM。它使用了多达 1.4 万亿个标记进行训练，其中
    4.5% 是来自 Github 的代码标记。CodeLlama-7B-Ins [[37](#bib.bib37)] 进行了额外的指令调整阶段，以使 Llama2 [[8](#bib.bib8)]
    更好地适应代码相关任务，提高其有效性。最近，Llama3 [[68](#bib.bib68)] 模型发布了，这相较于 Llama2 模型是一次重大飞跃，并建立了新的最先进技术标准。
- en: Aliyun Qwen. Qwen2-72B-Ins is a 72 billion parameter scale LLM. Qwen2-72B employs
    a variety of automated methods for obtaining high-quality instruction and preference
    data, making it perform well on code and maths tasks. CodeQwen1.5-7B-Ins [[36](#bib.bib36)]
    is the code-oriented version of Qwen1.5-7B [[71](#bib.bib71)]. CodeQwen1.5-7B-Ins
    has been tuned with around 3 trillion tokens of code-related data. It supports
    92 programming languages and supports long context understanding and generation
    with a context length of 64K tokens.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 阿里云Qwen。Qwen2-72B-Ins是一个72亿参数规模的LLM。Qwen2-72B采用多种自动化方法获取高质量的指令和偏好数据，使其在代码和数学任务上表现良好。CodeQwen1.5-7B-Ins [[36](#bib.bib36)]是Qwen1.5-7B [[71](#bib.bib71)]的代码导向版本。CodeQwen1.5-7B-Ins已经使用约3万亿个与代码相关的数据进行了调整。它支持92种编程语言，并支持长上下文理解和生成，支持64K标记的上下文长度。
- en: DeepSeek. The DeepSeek series models are released by High-Flyer. DeepSeek-LLM-7B [[70](#bib.bib70)]
    is trained from scratch with 2 trillion tokens in both English and Chinese. DeepSeek-LLM-7B-Ins [[70](#bib.bib70)]
    is initialized by DeepSeek-LLM-7B and tuned with an additional 1 million instruction
    data. DSCoder-6.7B-Ins [[64](#bib.bib64)] and DSCoder-33B-Ins [[64](#bib.bib64)]
    are trained from scratch on 2T tokens, which consist of 87% code and 13% natural
    language. DeepSeek-V2-0628 [[66](#bib.bib66)] contains 236B parameters and employs
    the Mixture-of-Experts (MoE) [[72](#bib.bib72)] architecture to conduct efficient
    training and inference. It is trained on a high-quality corpus comprising 8.1
    trillion tokens. DeepSeek-Coder-V2-0724 [[67](#bib.bib67)] is also an open-sourced
    MoE based LLM, which achieves comparable performance with GPT4-Turbo in code-related
    tasks. DeepSeek-Coder-V2 starts from an intermediate checkpoint of DeepSeek-V2
    and is tuned using 6 trillion tokens.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSeek。DeepSeek系列模型由High-Flyer发布。DeepSeek-LLM-7B [[70](#bib.bib70)]从头开始训练，使用了2万亿个标记，包括英语和中文。DeepSeek-LLM-7B-Ins [[70](#bib.bib70)]由DeepSeek-LLM-7B初始化，并通过额外的100万条指令数据进行调整。DSCoder-6.7B-Ins [[64](#bib.bib64)]和DSCoder-33B-Ins [[64](#bib.bib64)]从头开始在2T标记上进行训练，其中87%是代码，13%是自然语言。DeepSeek-V2-0628 [[66](#bib.bib66)]包含236B参数，并采用了Mixture-of-Experts
    (MoE) [[72](#bib.bib72)]架构，以进行高效训练和推断。它在包含8.1万亿标记的高质量语料库上进行训练。DeepSeek-Coder-V2-0724 [[67](#bib.bib67)]也是一个开源的MoE基础LLM，其在代码相关任务中表现与GPT4-Turbo相当。DeepSeek-Coder-V2从DeepSeek-V2的一个中间检查点开始，并使用6万亿标记进行调整。
- en: Implementation Details. For all LLMs, we set the generation temperature to 0.2
    and the maximum generation length to 1024 tokens. For the closed-source models,
    we use the API endpoints provided by the respective vendors and for the open-source
    models, we use vLLM [[73](#bib.bib73)] framework for inference. For all tasks
    in DebugEval, we use the zero-shot setting in our experiments. To finetune the
    NeuDebugger model, we use DeepSeek-Coder-6.7B-Inst [[64](#bib.bib64)] and Llama3-8B-Inst
    [[68](#bib.bib68)] as our backbone models and leverage the same data synthesized
    by MASTER as the SFT data. During SFT, all models are trained with Llama-Factory [[74](#bib.bib74)]
    and we use LoRA [[75](#bib.bib75)] for efficient fine-tuning. In our experiments,
    we set the learning rate to 2e-5 and the training epoch to 1\. We optimize the
    models using the AdamW optimizer, with the batch size of 8 and the gradient accumulation
    steps of 4.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。对于所有的LLM，我们将生成温度设置为0.2，最大生成长度设置为1024个标记。对于闭源模型，我们使用相应供应商提供的API端点，对于开源模型，我们使用vLLM [[73](#bib.bib73)]框架进行推断。对于DebugEval中的所有任务，我们在实验中使用零样本设置。为了微调NeuDebugger模型，我们使用DeepSeek-Coder-6.7B-Inst
    [[64](#bib.bib64)]和Llama3-8B-Inst [[68](#bib.bib68)]作为我们的基础模型，并利用MASTER合成的相同数据作为SFT数据。在SFT期间，所有模型均使用Llama-Factory [[74](#bib.bib74)]进行训练，我们使用LoRA [[75](#bib.bib75)]进行高效微调。在我们的实验中，我们将学习率设置为2e-5，训练周期设置为1。我们使用AdamW优化器优化模型，批量大小为8，梯度累积步骤为4。
- en: VI Evaluation Results
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI评估结果
- en: In this section, we benchmark LLMs on DebugEval and evaluate the overall performance
    of NeuDebugger. Then we conduct ablation studies and discuss the influence of
    different SFT data amounts on the model performance. The next experiment explores
    the effectiveness of NeuDebugger in handling the problems of different code error
    types. Finally, case studies are presented.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们基准测试了LLMs在DebugEval上的表现，并评估了NeuDebugger的整体性能。然后，我们进行消融研究，并讨论不同SFT数据量对模型性能的影响。接下来的实验探讨了NeuDebugger在处理不同代码错误类型问题上的有效性。最后，呈现了案例研究。
- en: 'TABLE V: The Effectiveness of the MASTER Framework and the Impact of CoT in
    the Training Data on Model Performance. SFT represents Supervised Fine-Tuning
    and CoT represents Chain-of-Thought. “Vanilla SFT” indicates that collected existing
    data was used to train the model. “MASTER (Answer)” indicates that the training
    data for all four tasks does not include CoT in the output. “MASTER (CoT)” means
    that the training data for all four tasks include CoT. “NeuDebugger” adopts mixed
    strategy, means that the other three tasks include CoT, excluding the Code Repair
    task.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：MASTER 框架的有效性及 CoT 在训练数据中对模型性能的影响。SFT 代表监督微调，CoT 代表思维链。 “Vanilla SFT”表示使用收集的现有数据进行模型训练。
    “MASTER (答案)”表示所有四个任务的训练数据中不包括 CoT 输出。 “MASTER (CoT)”意味着所有四个任务的训练数据都包括 CoT。 “NeuDebugger”采用混合策略，表示其他三个任务包括
    CoT，排除了代码修复任务。
- en: '| Method |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |'
- en: '&#124; BUG &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误 &#124;'
- en: '&#124; Loc. &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 位置 &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BUG &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误 &#124;'
- en: '&#124; Iden. &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别 &#124;'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码 &#124;'
- en: '&#124; Rev. &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修正 &#124;'
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码 &#124;'
- en: '&#124; Rep. &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修复 &#124;'
- en: '| Avg. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 |'
- en: '| DSCoder-6.7B-Ins |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DSCoder-6.7B-Ins |'
- en: '| zero-shot | 27.5 | 26.2 | 20.3 | 40.6 | 28.7 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| zero-shot | 27.5 | 26.2 | 20.3 | 40.6 | 28.7 |'
- en: '| w/ Vanilla SFT | 21.8 | 23.1 | 9.4 | 40.1 | 23.6 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Vanilla SFT | 21.8 | 23.1 | 9.4 | 40.1 | 23.6 |'
- en: '| w/ MASTER (Answer) | 43.8 | 35.8 | 32.7 | 43.5 | 39.0 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 使用 MASTER (答案) | 43.8 | 35.8 | 32.7 | 43.5 | 39.0 |'
- en: '| w/ MASTER (CoT) | 60.7 | 45.0 | 34.7 | 38.7 | 44.8 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 使用 MASTER (CoT) | 60.7 | 45.0 | 34.7 | 38.7 | 44.8 |'
- en: '| NeuDebugger | 58.8 | 45.8 | 71.3 | 49.5 | 56.4 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger | 58.8 | 45.8 | 71.3 | 49.5 | 56.4 |'
- en: '| Llama3-8B-Ins |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B-Ins |'
- en: '| zero-shot | 57.6 | 36.6 | 74.9 | 29.6 | 49.7 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| zero-shot | 57.6 | 36.6 | 74.9 | 29.6 | 49.7 |'
- en: '| w/ Vanilla SFT | 53.6 | 34.0 | 26.0 | 28.7 | 35.6 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 使用 Vanilla SFT | 53.6 | 34.0 | 26.0 | 28.7 | 35.6 |'
- en: '| w/ MASTER (Answer) | 58.1 | 34.8 | 32.1 | 42.5 | 41.9 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 使用 MASTER (答案) | 58.1 | 34.8 | 32.1 | 42.5 | 41.9 |'
- en: '| w/ MASTER (CoT) | 64.4 | 34.6 | 78.1 | 32.6 | 52.4 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 使用 MASTER (CoT) | 64.4 | 34.6 | 78.1 | 32.6 | 52.4 |'
- en: '| NeuDebugger | 61.1 | 33.8 | 78.5 | 41.8 | 53.8 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger | 61.1 | 33.8 | 78.5 | 41.8 | 53.8 |'
- en: 'TABLE VI: The Performance of Base Model and NeuDebugger on Different Bug Types.
    Syntax, Ref, Logic, and Multi represent SyntaxError, ReferenceError, LogicError,
    and MultipleErrors respectively.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：基础模型和 NeuDebugger 在不同错误类型上的表现。Syntax、Ref、Logic 和 Multi 分别代表语法错误、引用错误、逻辑错误和多重错误。
- en: '| Task | Model | Python | C++ | Java |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 模型 | Python | C++ | Java |'
- en: '| Syntax | Ref | Logic | Multi | Syntax | Ref | Logic | Multi | Syntax | Ref
    | Logic | Multi |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 语法 | 引用 | 逻辑 | 多重 | 语法 | 引用 | 逻辑 | 多重 | 语法 | 引用 | 逻辑 | 多重 |'
- en: '| Bug Iden. | Llama3-8B-Ins | 0.0 | 3.7 | 97.9 | 45.8 | 0.0 | 3.0 | 87.5 |
    62.0 | 0.0 | 3.7 | 87.4 | 47.4 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 错误识别 | Llama3-8B-Ins | 0.0 | 3.7 | 97.9 | 45.8 | 0.0 | 3.0 | 87.5 | 62.0
    | 0.0 | 3.7 | 87.4 | 47.4 |'
- en: '| NeuDebugger-Llama3-8B | 34.2 | 16.8 | 27.9 | 75.3 | 17.0 | 3.0 | 15.0 | 84.5
    | 25.3 | 4.2 | 23.2 | 80.5 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger-Llama3-8B | 34.2 | 16.8 | 27.9 | 75.3 | 17.0 | 3.0 | 15.0 | 84.5
    | 25.3 | 4.2 | 23.2 | 80.5 |'
- en: '| DSCoder-6.7B-Ins | 3.2 | 3.2 | 99.5 | 0.5 | 2.0 | 0.0 | 100.0 | 2.0 | 1.6
    | 1.6 | 100.0 | 0.5 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| DSCoder-6.7B-Ins | 3.2 | 3.2 | 99.5 | 0.5 | 2.0 | 0.0 | 100.0 | 2.0 | 1.6
    | 1.6 | 100.0 | 0.5 |'
- en: '| NeuDebugger-DS-6.7B | 54.2 | 81.1 | 32.6 | 2.6 | 45.0 | 45.0 | 77.5 | 20.0
    | 50.0 | 55.8 | 72.6 | 12.6 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger-DS-6.7B | 54.2 | 81.1 | 32.6 | 2.6 | 45.0 | 45.0 | 77.5 | 20.0
    | 50.0 | 55.8 | 72.6 | 12.6 |'
- en: '| Code Rep. | Llama3-8B-Ins | 40.0 | 20.0 | 35.6 | 7.5 | 60.9 | 45.8 | 29.6
    | 16.7 | 40.0 | 52.4 | 28.3 | 7.7 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 代码修复 | Llama3-8B-Ins | 40.0 | 20.0 | 35.6 | 7.5 | 60.9 | 45.8 | 29.6 | 16.7
    | 40.0 | 52.4 | 28.3 | 7.7 |'
- en: '| NeuDebugger-Llama3-8B | 90.0 | 46.7 | 39.7 | 20.0 | 65.2 | 66.7 | 40.7 |
    10.8 | 80.0 | 76.2 | 39.6 | 15.4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger-Llama3-8B | 90.0 | 46.7 | 39.7 | 20.0 | 65.2 | 66.7 | 40.7 |
    10.8 | 80.0 | 76.2 | 39.6 | 15.4 |'
- en: '| DSCoder-6.7B-Ins | 40.0 | 33.3 | 38.4 | 17.5 | 69.6 | 58.3 | 40.7 | 21.6
    | 60.0 | 71.4 | 47.2 | 23.1 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| DSCoder-6.7B-Ins | 40.0 | 33.3 | 38.4 | 17.5 | 69.6 | 58.3 | 40.7 | 21.6
    | 60.0 | 71.4 | 47.2 | 23.1 |'
- en: '| NeuDebugger-DS-6.7B | 90.0 | 46.7 | 45.2 | 27.5 | 87.0 | 62.5 | 40.7 | 27.0
    | 76.0 | 85.7 | 54.7 | 30.8 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| NeuDebugger-DS-6.7B | 90.0 | 46.7 | 45.2 | 27.5 | 87.0 | 62.5 | 40.7 | 27.0
    | 76.0 | 85.7 | 54.7 | 30.8 |'
- en: VI-A Overall Performance
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 整体表现
- en: The evaluation results of different LLMs and our NeuDebugger on DebugEval are
    presented in Table [IV](#S5.T4 "TABLE IV ‣ V Experimental Methodology ‣ Enhancing
    the Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement").
    We compared LLMs of varying scales to assess their code debugging effectiveness.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 LLM 和我们 NeuDebugger 在 DebugEval 上的评估结果见表 [IV](#S5.T4 "表 IV ‣ V 实验方法 ‣ 通过交互代理数据优化提高
    LLM 的代码调试能力")。我们比较了不同规模的 LLM，以评估其代码调试效果。
- en: Overall, larger-scale LLMs exhibit stronger code debugging ability. As indicated
    by the evaluation results, LLMs exceeding 70B parameters generally demonstrate
    consistent performance across various debugging tasks. Both the BUG Localization
    and BUG Identification tasks are multiple-choice questions with four options,
    where the accuracy of random guessing is approximately 25%. Unfortunately, most
    7B-scale LLMs achieve less than 30% accuracy on both tasks. This phenomenon underscores
    the importance of model scale in maintaining emergent abilities and acquiring
    critical knowledge through supervised fine-tuning (SFT) on code data [[35](#bib.bib35),
    [20](#bib.bib20)].
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，大规模的LLM展示了更强的代码调试能力。根据评估结果，超过70B参数的LLM通常在各种调试任务中表现一致。BUG定位和BUG识别任务均为四选一的多项选择题，随机猜测的准确率大约为25%。不幸的是，大多数7B规模的LLM在这两个任务中的准确率均低于30%。这一现象突显了模型规模在保持突现能力和通过监督微调（SFT）在代码数据上获取关键知识方面的重要性[[35](#bib.bib35),
    [20](#bib.bib20)]。
- en: In our experiments, we choose DSCoder-6.7B-Ins and Llama3-8B-Ins as the backbone
    models and then finetuning these two LLMs using the synthesized data generated
    by MASTER to conduct NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B models, respectively.
    In contrast to other 7B-scale LLMs, our NeuDebugger significantly enhances the
    code debugging effectiveness of foundation models and achieves competitive performance
    comparable to the 70B models. This demonstrates that building high-quality SFT
    data is essential for ensuring the code understanding and code debugging ability
    of these 7B models. Besides, both NeuDebugger-DS-6.7B and NeuDebugger-Llama3-8B
    perform better than the foundation model DSCoder-6.7B-Ins and Llama3-8B-Ins on
    the four tasks in DebugEval, bringing improvements of 27.7% and 4.1%, respectively.
    These improvements demonstrate that MASTER can refine code debugging data to significantly
    improve model performance on debugging tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们选择了DSCoder-6.7B-Ins和Llama3-8B-Ins作为主干模型，然后使用MASTER生成的合成数据对这两个LLM进行微调，从而分别构建了NeuDebugger-DS-6.7B和NeuDebugger-Llama3-8B模型。与其他7B规模的LLM相比，我们的NeuDebugger显著提高了基础模型的代码调试效果，并实现了与70B模型相当的竞争力。这表明，构建高质量的SFT数据对于确保这些7B模型的代码理解和代码调试能力至关重要。此外，NeuDebugger-DS-6.7B和NeuDebugger-Llama3-8B在DebugEval的四个任务中表现优于基础模型DSCoder-6.7B-Ins和Llama3-8B-Ins，分别提升了27.7%和4.1%。这些改进表明，MASTER可以通过精炼代码调试数据来显著提升模型在调试任务中的表现。
- en: Among the four tasks defined in DebugEval, LLMs typically produce better results
    in both BUG Localization and Code Review tasks. For example, GPT-4o-mini-0718
    achieves accuracy scores of 82.4 and 89.1 on these tasks, respectively. This indicates
    that these LLMs have strong code understanding capabilities by finetuning with
    code generation tasks, allowing them to effectively identify buggy code snippets
    and exhibit better code execution abilities. On the contrary, all LLMs demonstrate
    less effectiveness in both BUG Identification and Code Repair tasks, which focus
    more on assessing the code debugging ability of LLMs. For the BUG Identification
    task, LLMs are required to identify the cause of bugs. The reduced effectiveness
    of LLMs in this task illustrates the difficulty current LLMs have in deriving
    bug causes. The Code Repair task is even more complex, requiring LLMs to locate
    buggy snippets, determine the error type, and then fix the code. The suboptimal
    performance of these 70B LLMs further indicates the challenges they face in self-debugging [[9](#bib.bib9)].
    This phenomenon has also been observed in previous work [[16](#bib.bib16)]. The
    researchers repair codes by incorporating additional feedback from code compilers,
    which aims to enhance the bug identification ability of LLMs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在DebugEval定义的四个任务中，LLM通常在BUG定位和代码审查任务中表现更好。例如，GPT-4o-mini-0718在这些任务中的准确率分别为82.4和89.1。这表明这些LLM通过微调代码生成任务具有强大的代码理解能力，使它们能够有效地识别有缺陷的代码片段，并展现出更好的代码执行能力。相反，所有LLM在BUG识别和代码修复任务中的效果较差，这些任务更侧重于评估LLM的代码调试能力。在BUG识别任务中，LLM需要识别错误的原因。LLM在这一任务中的效果较差说明当前LLM在推导错误原因方面存在困难。代码修复任务更为复杂，需要LLM定位有缺陷的代码片段，确定错误类型，然后修复代码。这些70B
    LLM的表现不佳进一步表明它们在自我调试中面临的挑战[[9](#bib.bib9)]。这一现象在以往的研究中也有所观察[[16](#bib.bib16)]。研究人员通过结合代码编译器的额外反馈来修复代码，旨在提升LLM的错误识别能力。
- en: The model performance across various programming languages reveals the effectiveness
    and robustness of different LLMs. For instance, the GPT-4o-mini-0718 and DeepSeek-Coder-V2-0724
    models exhibit consistent performance across all languages, highlighting their
    robustness in handling diverse tasks. In contrast, some LLMs demonstrate inconsistent
    performance across different programming languages. For example, DSCoder-6.7B-Ins
    excels in Java but performs poorly in Python and C++. These findings underscore
    the necessity of developing a benchmark to evaluate debugging effectiveness across
    various programming languages, further supporting the motivation of our paper
    in building the DebugEval benchmark.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 各种编程语言的模型性能揭示了不同 LLMs 的有效性和鲁棒性。例如，GPT-4o-mini-0718 和 DeepSeek-Coder-V2-0724
    模型在所有语言中的表现一致，突显了它们处理多样任务的鲁棒性。相比之下，一些 LLMs 在不同编程语言中的表现不一致。例如，DSCoder-6.7B-Ins
    在 Java 中表现优异，但在 Python 和 C++ 中表现较差。这些发现强调了开发一个基准来评估不同编程语言调试效果的必要性，进一步支持了我们论文中构建
    DebugEval 基准的动机。
- en: '![Refer to caption](img/1a46edec5a08769cb68b883d08f791eb.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1a46edec5a08769cb68b883d08f791eb.png)'
- en: (a) BUG Localization.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: (a) BUG 定位。
- en: '![Refer to caption](img/66139dccbd61665594be339bb3864537.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/66139dccbd61665594be339bb3864537.png)'
- en: (b) BUG Identification.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: (b) BUG 识别。
- en: '![Refer to caption](img/9c7ef504c67a482bf1d17161d781dd06.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c7ef504c67a482bf1d17161d781dd06.png)'
- en: (c) Code Review.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 代码审查。
- en: '![Refer to caption](img/126435736c07f2ba495112cc1590d0fb.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/126435736c07f2ba495112cc1590d0fb.png)'
- en: (d) Code Repair.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 代码修复。
- en: 'Figure 5: Impact of the Amount of Training Data on Model Performance. Overall,
    the performance gradually rises as the amount of training data increase.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：训练数据量对模型性能的影响。总体而言，随着训练数据量的增加，性能逐渐提升。
- en: '![Refer to caption](img/8d1afbb1c9945b803a4d8c91922d4d8d.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d1afbb1c9945b803a4d8c91922d4d8d.png)'
- en: (a) DSCoder-6.7B-Ins.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: (a) DSCoder-6.7B-Ins。
- en: '![Refer to caption](img/9c351ff9802e5578d9f4c4946de6b952.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c351ff9802e5578d9f4c4946de6b952.png)'
- en: (b) NeuDebugger-DS-6.7B.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: (b) NeuDebugger-DS-6.7B。
- en: 'Figure 6: The Answer Distribution of DSCoder-6.7B-Ins and NeuDebugger-DS-6.7B
    in BUG Identification Task.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：DSCoder-6.7B-Ins 和 NeuDebugger-DS-6.7B 在 BUG 识别任务中的答案分布。
- en: '![Refer to caption](img/626bd92cbfdccb2badc3e76d08192fe7.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/626bd92cbfdccb2badc3e76d08192fe7.png)'
- en: 'Figure 7: Case Studies. We provide two cases from BUG Localization and Code
    Review to show the effectiveness of NeuDebugger.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：案例研究。我们提供了来自 BUG 定位和代码审查的两个案例，以展示 NeuDebugger 的有效性。
- en: VI-B Ablation Studies
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 消融研究
- en: The ablation studies are conducted to explore the effectiveness of the MASTER
    model in finetuning LLMs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 进行消融研究以探讨 MASTER 模型在微调 LLMs 中的有效性。
- en: 'We compare different SFT strategies, including Vanilla SFT, MASTER (Answer),
    MASTER (CoT), and NeuDebugger. The Vanilla SFT strategy gathers high-quality SFT
    data from UltraInteract [[35](#bib.bib35)], InstructCoder [[60](#bib.bib60)],
    and RepairLlama [[61](#bib.bib61)] for fine-tuning large language models (LLMs).
    Then, we use the MASTER framework to construct the SFT data and explore three
    different SFT strategies: MASTER (Answer), MASTER (CoT), and NeuDebugger. MASTER
    (Answer) indicates that we remove the Code Teacher in MASTER and ask LLMs to directly
    give the correct choice during SFT. MASTER (CoT) asks the Code Student to mimic
    the thought of problem solving of the Code Teacher. The NeuDebugger method combines
    the SFT strategies from both MASTER (Answer) and MASTER (CoT) by mixing the datasets
    from both SFT strategies for finetuning backbone models, except for the code repair
    data of MASTER (CoT).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了不同的 SFT 策略，包括 Vanilla SFT、MASTER (Answer)、MASTER (CoT) 和 NeuDebugger。Vanilla
    SFT 策略从 UltraInteract [[35](#bib.bib35)]、InstructCoder [[60](#bib.bib60)] 和 RepairLlama [[61](#bib.bib61)]
    收集高质量的 SFT 数据，用于微调大型语言模型（LLMs）。然后，我们使用 MASTER 框架构建 SFT 数据，并探讨三种不同的 SFT 策略：MASTER
    (Answer)、MASTER (CoT) 和 NeuDebugger。MASTER (Answer) 表示我们移除 MASTER 中的代码教师，并要求 LLMs
    在 SFT 期间直接给出正确的选择。MASTER (CoT) 要求代码学生模仿代码教师的问题解决思路。NeuDebugger 方法结合了 MASTER (Answer)
    和 MASTER (CoT) 的 SFT 策略，通过混合这两种 SFT 策略的数据集来微调主干模型，但不包括 MASTER (CoT) 的代码修复数据。
- en: As shown in Table [V](#S6.T5 "TABLE V ‣ VI Evaluation Results ‣ Enhancing the
    Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement"),
    both DSCoder-6.7B-Ins and Llama3-8B-Ins perform worse with the Vanilla SFT method
    compared to the baseline, indicating that the data quality for SFT remains a challenge
    in fine-tuning LLMs. When the MASTER synthesized data is used for SFT, the code
    debugging ability of these models is significantly enhanced. This illustrates
    that both DSCoder-6.7B-Ins and Llama3-8B-Ins are less effective at learning debugging
    knowledge from human/GPT-4 annotated data [[20](#bib.bib20)]. Furthermore, the
    MASTER (CoT) method generally achieves much better performance than MASTER (Answer),
    except for the code repair task. This may be because the Chain-of-Thought outcomes
    generated by Code Teacher can better explain the reasons behind answer choices
    but might incorporate additional noise in code repair tasks. By combining SFT
    data from both MASTER (CoT) and MASTER (Answer), NeuDebugger achieves the best
    performance among all SFT strategies. All these experimental results demonstrate
    the effectiveness of the MASTER model, which employs multi-agents to synthesize
    and refine SFT data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[V](#S6.T5 "TABLE V ‣ VI Evaluation Results ‣ Enhancing the Code Debugging
    Ability of LLMs via Communicative Agent Based Data Refinement")所示，DSCoder-6.7B-Ins和Llama3-8B-Ins在使用Vanilla
    SFT方法时，表现比基准更差，表明SFT的数据质量在微调LLMs时仍然是一个挑战。当使用MASTER合成数据进行SFT时，这些模型的代码调试能力显著增强。这说明，DSCoder-6.7B-Ins和Llama3-8B-Ins在从人类/GPT-4注释的数据中学习调试知识时效果较差[[20](#bib.bib20)]。此外，MASTER
    (CoT)方法通常比MASTER (Answer)表现更好，除了代码修复任务。这可能是因为由代码教师生成的思维链结果能更好地解释答案选择的原因，但在代码修复任务中可能会引入额外的噪音。通过结合来自MASTER
    (CoT)和MASTER (Answer)的SFT数据，NeuDebugger在所有SFT策略中取得了最佳性能。所有这些实验结果展示了MASTER模型的有效性，该模型采用多代理合成和优化SFT数据。
- en: VI-C The Impact of Data Quantity
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 数据量的影响
- en: This subsection explores the impact of data quantity when finetuning LLMs using
    the synthesized data from MASTER. As shown in Figure [5](#S6.F5 "Figure 5 ‣ VI-A
    Overall Performance ‣ VI Evaluation Results ‣ Enhancing the Code Debugging Ability
    of LLMs via Communicative Agent Based Data Refinement"), we fine-tuned the DSCoder-6.7B-Ins
    and Llama3-8B-Ins models using varying amounts of SFT data points. We then assessed
    their debugging abilities by evaluating their performance on the DebugEval benchmark
    and visualizing the results.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节探讨了使用MASTER合成数据微调LLMs时数据量的影响。如图[5](#S6.F5 "Figure 5 ‣ VI-A Overall Performance
    ‣ VI Evaluation Results ‣ Enhancing the Code Debugging Ability of LLMs via Communicative
    Agent Based Data Refinement")所示，我们使用不同数量的SFT数据点微调了DSCoder-6.7B-Ins和Llama3-8B-Ins模型。然后通过评估它们在DebugEval基准上的性能并可视化结果来评估其调试能力。
- en: Compared to Llama3-8B-Ins, DSCoder-6.7B-Ins shows a significant performance
    decrease when more SFT data points are fed. This indicates that code-oriented
    LLMs are better at learning from debugging data, whereas a standard language model
    struggles to enhance its debugging capabilities without an essential understanding
    of code. Among all debugging tasks, DSCoder-6.7B-Ins exhibits significant improvements
    in BUG Localization, BUG Identification, and Code Review, while only showing slight
    improvements in the code repair task. This suggests that these debugging tasks
    do indeed contribute to the better code repair ability of LLMs, though the task
    remains challenging to improve significantly.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与Llama3-8B-Ins相比，当提供更多SFT数据点时，DSCoder-6.7B-Ins的性能显著下降。这表明，面向代码的LLMs在从调试数据中学习方面表现更好，而标准语言模型在没有对代码的基本理解时难以提升其调试能力。在所有调试任务中，DSCoder-6.7B-Ins在BUG定位、BUG识别和代码审查方面表现出显著改进，而在代码修复任务中仅表现出轻微改进。这表明，这些调试任务确实有助于LLMs更好的代码修复能力，尽管该任务仍然具有挑战性。
- en: VI-D Effectiveness of NeuDebugger on Different Bug Types
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D NeuDebugger在不同Bug类型上的有效性
- en: As shown in Table [VI](#S6.T6 "TABLE VI ‣ VI Evaluation Results ‣ Enhancing
    the Code Debugging Ability of LLMs via Communicative Agent Based Data Refinement"),
    we show the effectiveness of NeuDebugger on more difficult tasks, including Bug
    Identification and Code Repair. The evaluation results on different bug types
    are also shown.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[VI](#S6.T6 "TABLE VI ‣ VI Evaluation Results ‣ Enhancing the Code Debugging
    Ability of LLMs via Communicative Agent Based Data Refinement")所示，我们展示了NeuDebugger在更难任务上的有效性，包括BUG识别和代码修复。不同bug类型的评估结果也展示了出来。
- en: For the BUG Identification task, the evaluation results demonstrate that existing
    LLMs are still suboptimal for debugging tasks. Both DSCoder-6.7B-Ins and Llama3-8B-Ins
    achieve over 97% accuracy on logic errors, but perform poorly on other coder error
    types. Furthermore, Figure [6](#S6.F6 "Figure 6 ‣ VI-A Overall Performance ‣ VI
    Evaluation Results ‣ Enhancing the Code Debugging Ability of LLMs via Communicative
    Agent Based Data Refinement") illustrates the answer distribution of LLMs. The
    evaluation results indicate that DSCoder-6.7B-Ins lacks the ability to identify
    bugs and defaults to selecting logic errors, resulting in high accuracy for this
    specific error type. NeuDebugger shows its effectiveness by conducting a more
    balanced answer choice distribution and achieving significant improvements on
    the Bug Identification task.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BUG识别任务，评估结果表明现有的LLM在调试任务中仍然不够理想。DSCoder-6.7B-Ins和Llama3-8B-Ins在逻辑错误上实现了超过97%的准确率，但在其他编码错误类型上表现较差。此外，图[6](#S6.F6
    "Figure 6 ‣ VI-A Overall Performance ‣ VI Evaluation Results ‣ Enhancing the Code
    Debugging Ability of LLMs via Communicative Agent Based Data Refinement")展示了LLM的答案分布。评估结果表明，DSCoder-6.7B-Ins缺乏识别bug的能力，默认选择逻辑错误，从而在该特定错误类型上具有较高的准确性。NeuDebugger通过进行更平衡的答案选择分布并在BUG识别任务上取得显著改进，展示了其有效性。
- en: For the Code Repair task, we observe that NeuDebugger achieves improvements
    across almost all types of code errors, especially for syntax errors, showing
    its effectiveness for code debugging. This also illustrates that syntax errors
    are relatively simpler and easier for the model to learn when we compare them
    to the other three types of code errors. Besides, NeuDebugger also struggles with
    repairing code containing logical or multiple errors.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码修复任务，我们观察到NeuDebugger在几乎所有类型的代码错误中都取得了改进，特别是在语法错误方面，显示了其在代码调试中的有效性。这也说明，与其他三种类型的代码错误相比，语法错误相对简单，模型更容易学习。此外，NeuDebugger在修复包含逻辑错误或多重错误的代码时也存在困难。
- en: VI-E Case Studies
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 案例研究
- en: Finally, we show two cases in Figure [7](#S6.F7 "Figure 7 ‣ VI-A Overall Performance
    ‣ VI Evaluation Results ‣ Enhancing the Code Debugging Ability of LLMs via Communicative
    Agent Based Data Refinement") to demonstrate the effectiveness of NeuDebugger.
    NeuDebugger is trained on the code debugging data constructed by our proposed
    MASTER framework. And we compare the performance of the model before and after
    training through cases.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在图[7](#S6.F7 "Figure 7 ‣ VI-A Overall Performance ‣ VI Evaluation Results
    ‣ Enhancing the Code Debugging Ability of LLMs via Communicative Agent Based Data
    Refinement")中展示了两个案例，以展示NeuDebugger的有效性。NeuDebugger在我们提出的MASTER框架构建的代码调试数据上进行训练。我们通过案例比较了模型训练前后的性能。
- en: 'For the first case of the BUG Localization task, the code error is caused by
    the line f = min(f, Cost(s[:x]) + A(s[x:], k-1’)), which has an incorrect index
    k-1’. Thus, the correct answer is (C). DSCoder-6.7B-Ins considers the code fragment
    if s[i]!=s[j]: c+=1 as erroneous, stating “This is not the correct way to check
    if a string is a palindrome”. On the contrary, NeuDebugger-DS-6.7B accurately
    analyzes the reason of bugs “contains a reference error in the line f = min(f,
    Cost(s[:x]) + A(s[x:], k-1’)), the error is in the reference to k-1’, which should
    be k-1”, demonstrating its effectiveness in BUG Localization.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '对于BUG定位任务的第一个案例，代码错误由行f = min(f, Cost(s[:x]) + A(s[x:], k-1’))引起，该行中的索引k-1’不正确。因此，正确答案是(C)。DSCoder-6.7B-Ins将代码片段if
    s[i]!=s[j]: c+=1视为错误，表示“这不是检查字符串是否是回文的正确方法”。相反，NeuDebugger-DS-6.7B准确分析了错误的原因“包含对行f
    = min(f, Cost(s[:x]) + A(s[x:], k-1’))中的引用错误，错误在于对k-1’的引用，应该是k-1”，展示了其在BUG定位中的有效性。'
- en: In the second case of the Code Repair task, the error involves the misuse of
    continue, which leads to a logical mistake. The DSCoder-6.7B-Ins model fails to
    identify this error and instead suggests changing the line String s = jc.next().toLowerCase()
    to String s = jc.nextLine().toLowerCase(). This modification introduces a new
    error, as it does not handle input correctly. NeuDebugger-DS-6.7B accurately recognizes
    that the problem lies in the use of continue and changes continue to break, successfully
    resolving the bug.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码修复任务的第二种情况下，错误涉及到`continue`的误用，导致逻辑错误。DSCoder-6.7B-Ins模型未能识别该错误，而是建议将代码行String
    s = jc.next().toLowerCase()更改为String s = jc.nextLine().toLowerCase()。这种修改引入了一个新的错误，因为它没有正确处理输入。NeuDebugger-DS-6.7B准确识别了问题在于`continue`的使用，并将`continue`更改为`break`，成功解决了这个bug。
- en: VII Conclusion
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: This paper presents DebugEval, an innovative benchmark designed to assess the
    debugging capabilities of Large Language Models (LLMs) from multiple perspectives.
    We introduce MASTER, a method that utilizes LLMs to generate high-quality supervised
    fine-tuning (SFT) datasets specifically for debugging tasks, thereby improving
    the performance of smaller models. Our experiments indicate that LLMs with 7B
    parameters are less effective in these debugging tasks and MASTER effectively
    enhances code debugging capabilities by refining data for SFT.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 DebugEval，一种旨在从多角度评估大型语言模型（LLMs）调试能力的创新基准。我们介绍了 MASTER，一种利用 LLMs 生成高质量监督微调（SFT）数据集的方法，专门用于调试任务，从而提高小型模型的性能。我们的实验表明，具有
    7B 参数的 LLMs 在这些调试任务中效果较差，而 MASTER 通过精炼 SFT 数据有效增强了代码调试能力。
- en: References
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] B. Hailpern and P. Santhanam, “Software debugging, testing, and verification,”
    *IBM Systems Journal*, vol. 41, no. 1, pp. 4–12, 2002.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] B. Hailpern 和 P. Santhanam，“软件调试、测试和验证，” *IBM 系统期刊*，卷 41，第 1 期，第 4–12 页，2002年。'
- en: '[2] L. Kirschner, E. Soremekun, and A. Zeller, “Debugging inputs,” in *Proceedings
    of the ACM/IEEE 42nd International Conference on Software Engineering*, 2020,
    pp. 75–86.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] L. Kirschner, E. Soremekun, 和 A. Zeller，“调试输入，”在 *ACM/IEEE 第 42 届国际软件工程会议论文集*，2020年，第
    75–86 页。'
- en: '[3] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A generic
    method for automatic software repair,” *IEEE Transactions on Software Engineering*,
    p. 54–72, 2012.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. Le Goues, T. Nguyen, S. Forrest, 和 W. Weimer，“Genprog：一种通用的自动软件修复方法，”
    *IEEE 软件工程学报*，第 54–72 页，2012年。'
- en: '[4] M. Wen, J. Chen, R. Wu, D. Hao, and S.-C. Cheung, “Context-aware patch
    generation for better automated program repair,” in *Proceedings of the 40th International
    Conference on Software Engineering*, 2018.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Wen, J. Chen, R. Wu, D. Hao, 和 S.-C. Cheung，“上下文感知补丁生成以更好地自动程序修复，”在
    *第 40 届国际软件工程会议论文集*，2018年。'
- en: '[5] J. Hua, M. Zhang, K. Wang, and S. Khurshid, “Sketchfix: a tool for automated
    program repair approach using lazy candidate generation,” in *Proceedings of the
    2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium
    on the Foundations of Software Engineering*, 2018.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Hua, M. Zhang, K. Wang, 和 S. Khurshid，“Sketchfix：一种使用惰性候选生成的自动程序修复工具，”在
    *2018 年第 26 届 ACM 欧洲软件工程会议与软件工程基础研讨会联合会议论文集*，2018年。'
- en: '[6] K. Liu, A. Koyuncu, D. Kim, and T. F. Bissyandé, “Tbar: revisiting template-based
    automated program repair,” in *Proceedings of the 28th ACM SIGSOFT International
    Symposium on Software Testing and Analysis*, 2019.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] K. Liu, A. Koyuncu, D. Kim, 和 T. F. Bissyandé，“Tbar：重新审视基于模板的自动程序修复，”在
    *第 28 届 ACM SIGSOFT 国际软件测试与分析研讨会论文集*，2019年。'
- en: '[7] OpenAI. (2022) Chatgpt: Optimizing language models for dialogue.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] OpenAI. (2022) Chatgpt：优化对话模型的语言模型。'
- en: '[8] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *ArXiv preprint*, vol. abs/2307.09288, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2：开放基础和微调聊天模型，” *ArXiv
    预印本*，卷 abs/2307.09288，2023年。'
- en: '[9] X. Chen, M. Lin, N. Schärli, and D. Zhou, “Teaching large language models
    to self-debug,” *ArXiv preprint*, vol. abs/2304.05128, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Chen, M. Lin, N. Schärli, 和 D. Zhou，“教导大型语言模型自我调试，” *ArXiv 预印本*，卷 abs/2304.05128，2023年。'
- en: '[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards,
    Y. Burda, N. Joseph, G. Brockman *et al.*, “Evaluating large language models trained
    on code,” *ArXiv preprint*, vol. abs/2107.03374, 2021.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H.
    Edwards, Y. Burda, N. Joseph, G. Brockman *等*，“评估训练于代码的大型语言模型，” *ArXiv 预印本*，卷
    abs/2107.03374，2021年。'
- en: '[11] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
    and D. Jiang, “Wizardcoder: Empowering code large language models with evol-instruct,”
    *ArXiv preprint*, vol. abs/2306.08568, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Z. Luo, C. Xu, P. Zhao, Q. Sun, X. Geng, W. Hu, C. Tao, J. Ma, Q. Lin,
    和 D. Jiang，“Wizardcoder：赋能代码大型语言模型与 evol-instruct，” *ArXiv 预印本*，卷 abs/2306.08568，2023年。'
- en: '[12] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, L. Shen, Z. Wang,
    A. Wang, Y. Li *et al.*, “Codegeex: A pre-trained model for code generation with
    multilingual benchmarking on humaneval-x,” in *Proceedings of the 29th ACM SIGKDD
    Conference on Knowledge Discovery and Data Mining*, 2023, pp. 5673–5684.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, L. Shen, Z. Wang,
    A. Wang, Y. Li *等*，“Codegeex：一种用于代码生成的预训练模型，在 humaneval-x 上进行多语言基准测试，”在 *第 29
    届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，2023年，第 5673–5684 页。'
- en: '[13] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,
    C. Akiki, J. Li, J. Chim *et al.*, “Starcoder: May the source be with you!” *ArXiv
    preprint*, vol. abs/2305.06161, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,
    C. Akiki, J. Li, J. Chim *等*，“Starcoder: 愿源代码与你同在！” *ArXiv预印本*，卷abs/2305.06161，2023年。'
- en: '[14] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu,
    Y. Li *et al.*, “Deepseek-coder: When the large language model meets programming–the
    rise of code intelligence,” *ArXiv preprint*, vol. abs/2401.14196, 2024.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y.
    Wu, Y. Li *等*，“Deepseek-coder: 当大语言模型遇上编程——代码智能的崛起，” *ArXiv预印本*，卷abs/2401.14196，2024年。'
- en: '[15] K. Zhang, Z. Li, J. Li, G. Li, and Z. Jin, “Self-edit: Fault-aware code
    editor for code generation,” *ArXiv preprint*, vol. abs/2305.04087, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] K. Zhang, Z. Li, J. Li, G. Li, 和 Z. Jin, “Self-edit: 具备故障感知的代码生成编辑器，”
    *ArXiv预印本*，卷abs/2305.04087，2023年。'
- en: '[16] H. Wang, Z. Liu, S. Wang, G. Cui, N. Ding, Z. Liu, and G. Yu, “Intervenor:
    Prompt the coding ability of large language models with the interactive chain
    of repair,” in *Proceedings of the 62nd Annual Meeting of the Association for
    Computational Linguistics*, 2024.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Wang, Z. Liu, S. Wang, G. Cui, N. Ding, Z. Liu, 和 G. Yu, “Intervenor:
    通过交互链修复来提升大语言模型的编码能力，” 见 *第62届计算语言学协会年会论文集*，2024年。'
- en: '[17] R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Z. Liu, and M. Sun, “Debugbench:
    Evaluating debugging capability of large language models,” 2024.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Z. Liu, 和 M. Sun, “Debugbench:
    评估大语言模型的调试能力，” 2024年。'
- en: '[18] J. Guo, Z. Li, X. Liu, K. Ma, T. Zheng, Z. Yu, D. Pan, Y. LI, R. Liu,
    Y. Wang, S. Guo, X. Qu, X. Yue, G. Zhang, W. Chen, and J. Fu, “Codeeditorbench:
    Evaluating code editing capability of large language models,” 2024.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Guo, Z. Li, X. Liu, K. Ma, T. Zheng, Z. Yu, D. Pan, Y. LI, R. Liu,
    Y. Wang, S. Guo, X. Qu, X. Yue, G. Zhang, W. Chen, 和 J. Fu, “Codeeditorbench:
    评估大语言模型的代码编辑能力，” 2024年。'
- en: '[19] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *ArXiv
    preprint*, vol. abs/2303.08774, 2023.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat *等*，“Gpt-4技术报告，” *ArXiv预印本*，卷abs/2303.08774，2023年。'
- en: '[20] A. Gudibande, E. Wallace, C. V. Snell, X. Geng, H. Liu, P. Abbeel, S. Levine,
    and D. Song, “The false promise of imitating proprietary language models,” in
    *The Twelfth International Conference on Learning Representations*, 2024.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Gudibande, E. Wallace, C. V. Snell, X. Geng, H. Liu, P. Abbeel, S.
    Levine, 和 D. Song, “模仿专有语言模型的虚假承诺，” 见 *第十二届国际学习表示会议*，2024年。'
- en: '[21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le,
    and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,”
    2022.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q.
    Le, 和 D. Zhou, “链式思考提示在大语言模型中引发推理，” 2022年。'
- en: '[22] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
    T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained model for programming
    and natural languages,” in *Findings of the Association for Computational Linguistics:
    EMNLP 2020*.   Online: Association for Computational Linguistics, 2020, pp. 1536–1547.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
    T. Liu, D. Jiang, 和 M. Zhou, “CodeBERT: 一种用于编程和自然语言的预训练模型，” 见 *计算语言学协会年会论文集：EMNLP
    2020*。在线：计算语言学协会，2020年，第1536–1547页。'
- en: '[23] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified pre-training
    for program understanding and generation,” in *Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*.   Online: Association for Computational Linguistics,
    2021, pp. 2655–2668.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] W. Ahmad, S. Chakraborty, B. Ray, 和 K.-W. Chang, “程序理解与生成的统一预训练，” 见 *2021年北美计算语言学协会会议论文集：人类语言技术*。在线：计算语言学协会，2021年，第2655–2668页。'
- en: '[24] D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y. Wang, W. Chen, and
    J.-G. Lou, “Cert: Continual pre-training on sketches for library-oriented code
    generation,” in *International Joint Conference on Artificial Intelligence*, 2022.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] D. Zan, B. Chen, D. Yang, Z. Lin, M. Kim, B. Guan, Y. Wang, W. Chen, 和
    J.-G. Lou, “Cert: 在草图上进行持续预训练以进行库导向的代码生成，” 见 *国际人工智能联合会议*，2022年。'
- en: '[25] K. Clark, M. Luong, Q. V. Le, and C. D. Manning, “ELECTRA: pre-training
    text encoders as discriminators rather than generators,” in *8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020*.   OpenReview.net, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. Clark, M. Luong, Q. V. Le, 和 C. D. Manning，“ELECTRA: 预训练文本编码器作为判别器而非生成器，”
    见于 *第8届国际学习表示会议，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26-30日*。OpenReview.net，2020。'
- en: '[26] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement,
    D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong,
    M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu, “Codexglue: A
    machine learning benchmark dataset for code understanding and generation,” in
    *Proceedings of NeurIPS*, 2021.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement,
    D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong,
    M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, 和 S. Liu，“Codexglue: 一个机器学习基准数据集用于代码理解和生成，”
    见于 *NeurIPS会议论文集*，2021。'
- en: '[27] M. Lachaux, B. Rozière, M. Szafraniec, and G. Lample, “DOBF: A deobfuscation
    pre-training objective for programming languages,” in *Advances in Neural Information
    Processing Systems 34: Annual Conference on Neural Information Processing Systems
    2021, NeurIPS 2021, December 6-14, 2021, virtual*, 2021, pp. 14 967–14 979.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Lachaux, B. Rozière, M. Szafraniec, 和 G. Lample，“DOBF: 编程语言的去混淆预训练目标，”
    见于 *神经信息处理系统进展34：2021年神经信息处理系统年会，NeurIPS 2021，2021年12月6-14日，虚拟*，2021，第14,967–14,979页。'
- en: '[28] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “CodeT5: Identifier-aware unified
    pre-trained encoder-decoder models for code understanding and generation,” in
    *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*.   Online
    and Punta Cana, Dominican Republic: Association for Computational Linguistics,
    2021, pp. 8696–8708.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Wang, W. Wang, S. Joty, 和 S. C. Hoi， “CodeT5: 识别符感知统一预训练编码器-解码器模型用于代码理解和生成，”
    见于 *2021年自然语言处理实证方法会议论文集*。在线和多米尼加共和国蓬塔卡纳：计算语言学协会，2021，第8696–8708页。'
- en: '[29] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified
    text-to-text transformer,” *J. Mach. Learn. Res.*, vol. 21, pp. 140:1–140:67,
    2020.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
    W. Li, 和 P. J. Liu，“探索统一文本到文本变换器的迁移学习极限，” *J. Mach. Learn. Res.*，第21卷，第140:1–140:67页，2020。'
- en: '[30] X. Li, Y. Gong, Y. Shen, X. Qiu, H. Zhang, B. Yao, W. Qi, D. Jiang, W. Chen,
    and N. Duan, “CodeRetriever: A large scale contrastive pre-training method for
    code search,” in *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*.   Abu Dhabi, United Arab Emirates: Association for Computational
    Linguistics, 2022, pp. 2898–2910.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. Li, Y. Gong, Y. Shen, X. Qiu, H. Zhang, B. Yao, W. Qi, D. Jiang, W.
    Chen, 和 N. Duan，“CodeRetriever: 大规模对比预训练方法用于代码搜索，” 见于 *2022年自然语言处理实证方法会议论文集*。阿布扎比，阿联酋：计算语言学协会，2022，第2898–2910页。'
- en: '[31] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, “UniXcoder: Unified
    cross-modal pre-training for code representation,” in *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*.   Dublin, Ireland: Association for Computational Linguistics, 2022,
    pp. 7212–7225.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, 和 J. Yin，“UniXcoder: 统一跨模态预训练用于代码表示，”
    见于 *计算语言学协会第60届年会会议论文集（第1卷：长篇论文）*。都柏林，爱尔兰：计算语言学协会，2022，第7212–7225页。'
- en: '[32] Y. MA, Y. Liu, Y. Yu, Y. Zhang, Y. Jiang, C. Wang, and S. Li, “At which
    training stage does code data help llms reasoning?” in *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. MA, Y. Liu, Y. Yu, Y. Zhang, Y. Jiang, C. Wang, 和 S. Li，“代码数据在何种训练阶段帮助LLMs推理？”
    见于 *第十二届国际学习表示会议*。'
- en: '[33] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang,
    D. Narayanan, Y. Wu, A. Kumar *et al.*, “Holistic evaluation of language models,”
    *Transactions on Machine Learning Research*.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y.
    Zhang, D. Narayanan, Y. Wu, A. Kumar *等*，“语言模型的整体评估，” *机器学习研究汇刊*。'
- en: '[34] Y. Fu, H. Peng, and T. Khot, “How does gpt obtain its ability? tracing
    emergent abilities of language models to their sources,” *Yao Fu’s Notion*, 2022.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. Fu, H. Peng, 和 T. Khot，“GPT如何获得其能力？追踪语言模型的新兴能力及其来源，” *Yao Fu’s Notion*，2022。'
- en: '[35] L. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, J. Deng, B. Shan, H. Chen,
    R. Xie, Y. Lin *et al.*, “Advancing llm reasoning generalists with preference
    trees,” *ArXiv preprint*, vol. abs/2404.02078, 2024.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] L. Yuan, G. Cui, H. Wang, N. Ding, X. Wang, J. Deng, B. Shan, H. Chen,
    R. Xie, Y. Lin *等*，“通过偏好树推进 LLM 推理通才，” *ArXiv 预印本*，第 abs/2404.02078 号，2024。'
- en: '[36] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han,
    F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu,
    J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang,
    S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan,
    Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and
    T. Zhu, “Qwen technical report,” *ArXiv preprint*, vol. abs/2309.16609, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han,
    F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu,
    J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang,
    S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan,
    Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, 和
    T. Zhu, “Qwen 技术报告，” *ArXiv 预印本*，第 abs/2309.16609 号，2023。'
- en: '[37] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,
    J. Liu, T. Remez, J. Rapin *et al.*, “Code llama: Open foundation models for code,”
    *ArXiv preprint*, vol. abs/2308.12950, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y.
    Adi, J. Liu, T. Remez, J. Rapin *等*，“Code llama：开放的基础模型用于代码，” *ArXiv 预印本*，第 abs/2308.12950
    号，2023。'
- en: '[38] S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix,” in *Proceedings of
    the 38th International Conference on Software Engineering*, 2016.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Mechtaev, J. Yi, 和 A. Roychoudhury, “Angelix，” 在 *第38届国际软件工程会议论文集*，2016。'
- en: '[39] F. DeMarco, J. Xuan, D. Le Berre, and M. Monperrus, “Automatic repair
    of buggy if conditions and missing preconditions with smt,” in *Proceedings of
    the 6th International Workshop on Constraints in Software Testing, Verification,
    and Analysis*, 2014.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] F. DeMarco, J. Xuan, D. Le Berre, 和 M. Monperrus, “使用 SMT 自动修复有缺陷的 if
    条件和缺失的前置条件，” 在 *第六届国际软件测试、验证与分析约束研讨会论文集*，2014。'
- en: '[40] C. S. Xia, Y. Wei, and L. Zhang, “Practical program repair in the era
    of large pre-trained language models,” *ArXiv preprint*, vol. abs/2210.14179,
    2022.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] C. S. Xia, Y. Wei, 和 L. Zhang, “大规模预训练语言模型时代的实际程序修复，” *ArXiv 预印本*，第 abs/2210.14179
    号，2022。'
- en: '[41] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, 和 W. Zaremba, “评估训练有素的大型语言模型，” 2021。'
- en: '[42] S. D. Kolak, R. Martins, C. Le Goues, and V. J. Hellendoorn, “Patch generation
    with language models: Feasibility and scaling behavior,” in *Deep Learning for
    Code Workshop*, 2022.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] S. D. Kolak, R. Martins, C. Le Goues, 和 V. J. Hellendoorn, “使用语言模型生成补丁：可行性和扩展性行为，”
    在 *代码深度学习研讨会*，2022。'
- en: '[43] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
    models are unsupervised multitask learners,” 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, 和 I. Sutskever, “语言模型是无监督的多任务学习者，”
    2019。'
- en: '[44] T. X. Olausson, J. P. Inala, C. Wang, J. Gao, and A. Solar-Lezama, “Is
    self-repair a silver bullet for code generation?” in *The Twelfth International
    Conference on Learning Representations*, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] T. X. Olausson, J. P. Inala, C. Wang, J. Gao, 和 A. Solar-Lezama, “自我修复是代码生成的灵丹妙药吗？”
    在 *第十二届国际学习表示会议*，2023。'
- en: '[45] K. Li, Q. Hu, X. Zhao, H. Chen, Y. Xie, T. Liu, Q. Xie, and J. He, “Instructcoder:
    Instruction tuning large language models for code editing,” 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] K. Li, Q. Hu, X. Zhao, H. Chen, Y. Xie, T. Liu, Q. Xie, 和 J. He, “Instructcoder：指令调优大型语言模型以进行代码编辑，”
    2023。'
- en: '[46] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi,
    “Self-instruct: Aligning language models with self-generated instructions,” 2022.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, 和 H. Hajishirzi,
    “Self-instruct: 使语言模型与自生成指令对齐，” 2022年。'
- en: '[47] G. Li, C. Zhi, J. Chen, J. Han, and S. Deng, “A comprehensive evaluation
    of parameter-efficient fine-tuning on automated program repair,” 2024.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G. Li, C. Zhi, J. Chen, J. Han, 和 S. Deng, “关于自动程序修复的参数高效微调的综合评估，” 2024年。'
- en: '[48] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” in *The Tenth International
    Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
    2022*.   OpenReview.net, 2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen, “Lora: 大型语言模型的低秩适应，” 在*第十届国际学习表征会议，ICLR 2022，虚拟活动，2022年4月25-29日*。OpenReview.net,
    2022年。'
- en: '[49] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for
    generation,” in *Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*.   Online: Association for Computational
    Linguistics, 2021, pp. 4582–4597.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. L. Li 和 P. Liang, “Prefix-tuning: 优化生成的连续提示，” 在*第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长论文）*。在线：计算语言学协会，2021年，第4582–4597页。'
- en: '[50] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, “P-tuning v2:
    Prompt tuning can be comparable to fine-tuning universally across scales and tasks,”
    *Cornell University - arXiv,Cornell University - arXiv*, 2021.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, 和 J. Tang, “P-tuning v2:
    提示微调在不同规模和任务上的普遍可比性，” *康奈尔大学 - arXiv*，2021年。'
- en: '[51] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. Raffel,
    “Few-shot parameter-efficient fine-tuning is better and cheaper than in-context
    learning.”'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, 和 C. Raffel,
    “少量样本参数高效微调优于上下文学习，且成本更低。”'
- en: '[52] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama,
    K. Sen, and I. Stoica, “Livecodebench: Holistic and contamination free evaluation
    of large language models for code,” *ArXiv preprint*, vol. abs/2403.07974, 2024.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama,
    K. Sen, 和 I. Stoica, “Livecodebench: 对代码的大型语言模型的整体且无污染评估，” *ArXiv预印本*，卷 abs/2403.07974，2024年。'
- en: '[53] M. Yasunaga and P. Liang, “Break-it-fix-it: Unsupervised learning for
    program repair,” in *Proceedings of the 38th International Conference on Machine
    Learning, ICML 2021, 18-24 July 2021, Virtual Event*, ser. Proceedings of Machine
    Learning Research, vol. 139.   PMLR, 2021, pp. 11 941–11 952.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Yasunaga 和 P. Liang, “Break-it-fix-it: 用于程序修复的无监督学习，” 在*第38届国际机器学习会议，ICML
    2021，2021年7月18-24日，虚拟活动*，系列：机器学习研究论文集，卷 139。PMLR，2021年，第11 941–11 952页。'
- en: '[54] F. Huq, M. Hasan, M. M. A. Haque, S. Mahbub, A. Iqbal, and T. Ahmed, “Review4repair:
    Code review aided automatic program repairing,” *Information and Software Technology*,
    p. 106765, 2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] F. Huq, M. Hasan, M. M. A. Haque, S. Mahbub, A. Iqbal, 和 T. Ahmed, “Review4repair:
    代码审查辅助的自动程序修复，” *信息与软件技术*，第106765页，2022年。'
- en: '[55] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement,
    D. Drain, D. Jiang, D. Tang *et al.*, “Codexglue: A machine learning benchmark
    dataset for code understanding and generation,” *ArXiv preprint*, vol. abs/2102.04664,
    2021.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement,
    D. Drain, D. Jiang, D. Tang *等*，“Codexglue: 代码理解和生成的机器学习基准数据集，” *ArXiv预印本*，卷 abs/2102.04664，2021年。'
- en: '[56] M. M. A. Haque, W. U. Ahmad, I. Lourentzou, and C. Brown, “Fixeval: Execution-based
    evaluation of program fixes for programming problems,” in *2023 IEEE/ACM International
    Workshop on Automated Program Repair (APR)*.   IEEE, 2023, pp. 11–18.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. M. A. Haque, W. U. Ahmad, I. Lourentzou, 和 C. Brown, “Fixeval: 基于执行的程序修复评估，”
    在*2023 IEEE/ACM国际自动程序修复研讨会（APR）*。IEEE，2023年，第11–18页。'
- en: '[57] S. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou, Y. Xiao, S. Yun,
    W. Lin *et al.*, “Disc-lawllm: Fine-tuning large language models for intelligent
    legal services,” *ArXiv preprint*, vol. abs/2309.11325, 2023.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou, Y. Xiao, S.
    Yun, W. Lin *等*，“Disc-lawllm: 微调大型语言模型以提供智能法律服务，” *ArXiv预印本*，卷 abs/2309.11325，2023年。'
- en: '[58] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou
    *et al.*, “Chain-of-thought prompting elicits reasoning in large language models,”
    *Advances in neural information processing systems*, vol. 35, pp. 24 824–24 837,
    2022.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou *等*, “Chain-of-thought 提示在大型语言模型中引发推理，” *神经信息处理系统进展*，第35卷，第24,824–24,837页，2022年。'
- en: '[59] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
    R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
    M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M.
    Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
    A. Radford, I. Sutskever, 和 D. Amodei, “语言模型是少量学习者，” 见 *神经信息处理系统进展 33: 神经信息处理系统年度会议
    2020, NeurIPS 2020, 2020年12月6-12日, 虚拟*，2020年。'
- en: '[60] Q. Hu, K. Li, X. Zhao, Y. Xie, T. Liu, H. Chen, Q. Xie, and J. He, “Instructcoder:
    Empowering language models for code editing,” *ArXiv preprint*, vol. abs/2310.20329,
    2023.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Q. Hu, K. Li, X. Zhao, Y. Xie, T. Liu, H. Chen, Q. Xie, 和 J. He, “Instructcoder:
    赋能语言模型进行代码编辑，” *ArXiv 预印本*，第abs/2310.20329卷，2023年。'
- en: '[61] A. Silva, S. Fang, and M. Monperrus, “Repairllama: Efficient representations
    and fine-tuned adapters for program repair,” Tech. Rep., 2023.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Silva, S. Fang, 和 M. Monperrus, “Repairllama: 高效的表示和针对程序修复的微调适配器，”
    技术报告, 2023。'
- en: '[62] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery,
    Q. V. Le, E. H. Chi, D. Zhou, and J. Wei, “Challenging big-bench tasks and whether
    chain-of-thought can solve them,” 2022.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A.
    Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, 和 J. Wei, “挑战大型基准任务及链式思维是否能解决这些任务，” 2022年。'
- en: '[63] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards,
    Y. Burda, N. Joseph, G. Brockman *et al.*, “Evaluating large language models trained
    on code,” *ArXiv preprint*, vol. abs/2107.03374, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H.
    Edwards, Y. Burda, N. Joseph, G. Brockman *等*, “评估在代码上训练的大型语言模型，” *ArXiv 预印本*，第abs/2107.03374卷，2021年。'
- en: '[64] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu,
    Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder: When the large language
    model meets programming – the rise of code intelligence,” 2024.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y.
    Wu, Y. K. Li, F. Luo, Y. Xiong, 和 W. Liang, “Deepseek-coder: 当大型语言模型遇见编程 —— 代码智能的崛起，”
    2024。'
- en: '[65] OpenAI, “Gpt-4o mini: advancing cost-efficient intelligence,” 2024.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] OpenAI, “Gpt-4o mini: 推进成本效益智能，” 2024。'
- en: '[66] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient mixture-of-experts
    language model,” 2024.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] DeepSeek-AI, “Deepseek-v2: 一种强大、经济且高效的混合专家语言模型，” 2024。'
- en: '[67] Q. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao,
    S. Ma *et al.*, “Deepseek-coder-v2: Breaking the barrier of closed-source models
    in code intelligence,” *ArXiv preprint*, vol. abs/2406.11931, 2024.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Q. Zhu, D. Guo, Z. Shao, D. Yang, P. Wang, R. Xu, Y. Wu, Y. Li, H. Gao,
    S. Ma *等*, “Deepseek-coder-v2: 打破代码智能中闭源模型的障碍，” *ArXiv 预印本*，第abs/2406.11931卷，2024年。'
- en: '[68] AI@Meta, “Llama 3 model card,” 2024.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] AI@Meta, “Llama 3 模型卡，” 2024。'
- en: '[69] “Qwen2 technical report,” 2024.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] “Qwen2 技术报告，” 2024。'
- en: '[70] DeepSeek-AI, “Deepseek llm: Scaling open-source language models with longtermism,”
    *ArXiv preprint*, vol. abs/2401.02954, 2024.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] DeepSeek-AI, “Deepseek llm: 用长期主义扩展开源语言模型，” *ArXiv 预印本*，第abs/2401.02954卷，2024年。'
- en: '[71] Q. Team, “Introducing qwen1.5,” 2024.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Q. Team, “介绍 qwen1.5，” 2024。'
- en: '[72] W. Cai, J. Jiang, F. Wang, J. Tang, S. Kim, and J. Huang, “A survey on
    mixture of experts,” 2024.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] W. Cai, J. Jiang, F. Wang, J. Tang, S. Kim, 和 J. Huang, “混合专家模型综述，” 2024。'
- en: '[73] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez,
    H. Zhang, and I. Stoica, “Efficient memory management for large language model
    serving with pagedattention,” in *Proceedings of the ACM SIGOPS 29th Symposium
    on Operating Systems Principles*, 2023.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez,
    H. Zhang, 和 I. Stoica, “使用分页注意力进行大型语言模型服务的高效内存管理，” 见 *ACM SIGOPS 第29届操作系统原理研讨会论文集*，2023年。'
- en: '[74] hiyouga, “Llama factory,” 2023.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] hiyouga, “Llama factory,” 2023.'
- en: '[75] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” in *The Tenth International
    Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
    2022*.   OpenReview.net, 2022.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, 和
    W. Chen，“Lora：大型语言模型的低秩适应”，在*第十届国际学习表征会议（ICLR 2022），虚拟会议，2022年4月25-29日*。OpenReview.net,
    2022.'
