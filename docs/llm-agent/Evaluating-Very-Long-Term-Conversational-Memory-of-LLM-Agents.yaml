- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Evaluating Very Long-Term Conversational Memory of LLM Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LLM代理的非常长期对话记忆
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.17753](https://ar5iv.labs.arxiv.org/html/2402.17753)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.17753](https://ar5iv.labs.arxiv.org/html/2402.17753)
- en: Adyasha Maharana¹        Dong-Ho Lee²      Sergey Tulyakov³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Adyasha Maharana¹        Dong-Ho Lee²      Sergey Tulyakov³
- en: Mohit Bansal^(1$\dagger$)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Mohit Bansal^(1$\dagger$)
- en: University of North Carolina, Chapel Hill¹ University of Southern California² Snap
    Inc.³
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 北卡罗来纳大学教堂山分校¹ 南加州大学² Snap Inc.³
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Existing works on long-term open-domain dialogues focus on evaluating model
    responses within contexts spanning no more than five chat sessions. Despite advancements
    in long-context large language models (LLMs) and retrieval augmented generation
    (RAG) techniques, their efficacy in very long-term dialogues remains unexplored.
    To address this research gap, we introduce a machine-human pipeline to generate
    high-quality, very long-term dialogues by leveraging LLM-based agent architectures
    and grounding their dialogues on personas and temporal event graphs. Moreover,
    we equip each agent with the capability of sharing and reacting to images. The
    generated conversations are verified and edited by human annotators for long-range
    consistency and grounding to the event graphs. Using this pipeline, we collect
    LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns
    and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive
    evaluation benchmark to measure long-term memory in models, encompassing question
    answering, event summarization, and multi-modal dialogue generation tasks. Our
    experimental results indicate that LLMs exhibit challenges in understanding lengthy
    conversations and comprehending long-range temporal and causal dynamics within
    dialogues. Employing strategies like long-context LLMs or RAG can offer improvements
    but these models still substantially lag behind human performance.¹¹1Code and
    data to be available at
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现有关于长期开放域对话的研究集中在评估模型在不超过五次聊天会话中的响应。尽管在长上下文大语言模型（LLMs）和检索增强生成（RAG）技术方面取得了进展，但它们在非常长期对话中的有效性仍未得到探索。为了解决这一研究空白，我们引入了一个机器-人类管道，通过利用基于LLM的代理架构并将对话建立在角色和时间事件图上，生成高质量的非常长期对话。此外，我们为每个代理提供了分享和响应图像的能力。生成的对话经过人类标注员的验证和编辑，以确保长期一致性和事件图的基础。通过这个管道，我们收集了LoCoMo，一个包含300轮对话和平均9K令牌的数据集，覆盖了最多35次会话。基于LoCoMo，我们提出了一个全面的评估基准，以衡量模型中的长期记忆，包括问答、事件总结和多模态对话生成任务。我们的实验结果表明，LLMs在理解长对话和理解对话中的长期时间和因果动态方面面临挑战。采用长上下文LLMs或RAG等策略可以提供改进，但这些模型仍显著落后于人类表现。¹¹1代码和数据将提供在
- en: '[https://snap-research.github.io/locomo](https://snap-research.github.io/locomo)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://snap-research.github.io/locomo](https://snap-research.github.io/locomo)'
- en: Evaluating Very Long-Term Conversational Memory of LLM Agents
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM代理的非常长期对话记忆
- en: Adyasha Maharana¹        Dong-Ho Lee²      Sergey Tulyakov³ Mohit Bansal^(1$\dagger$)
    University of North Carolina, Chapel Hill¹ University of Southern California² Snap
    Inc.³
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Adyasha Maharana¹        Dong-Ho Lee²      Sergey Tulyakov³ Mohit Bansal^(1$\dagger$)
    北卡罗来纳大学教堂山分校¹ 南加州大学² Snap Inc.³
- en: '¹¹footnotetext: ^†Equal advising.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹脚注：^†平等指导。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/b01ca881a135cf8831678f1a64bf8362.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b01ca881a135cf8831678f1a64bf8362.png)'
- en: 'Figure 1: An example in LoCoMo. Dialogs are steered by the speakers’ personas
    and corresponding events e.g., Joanna’s responses are consistent with her pet
    allergies. For Nate, the event got a new dog is followed by a playdate with neighbor’s
    dog, showcasing long-term memory. Multimodal dialog is enabled with image-sharing
    and image-response behaviors.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LoCoMo中的一个示例。对话由说话者的角色和相应的事件引导，例如，乔安娜的回应与她的宠物过敏情况一致。对于内特，事件“得到了一只新狗”后，跟随的是与邻居的狗的玩耍，展示了长期记忆。多模态对话通过图像分享和图像回应行为实现。
- en: '| Dataset | Avg. turns per conv. | Avg. sessions per conv. | Avg. tokens per
    conv. | Time Interval | Multimodal | Collection |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 平均对话轮次 | 平均会话数 | 平均令牌数 | 时间间隔 | 多模态 | 收集来源 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| MPChat Ahn et al. ([2023](#bib.bib1)) | 2.8 | 1 | 53.3 | - | ✓ | Reddit |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| MPChat Ahn et al. ([2023](#bib.bib1)) | 2.8 | 1 | 53.3 | - | ✓ | Reddit |'
- en: '| MMDialog Feng et al. ([2023](#bib.bib12)) | 4.6 | 1 | 72.5 | - | ✓ | Social
    media |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| MMDialog Feng et al. ([2023](#bib.bib12)) | 4.6 | 1 | 72.5 | - | ✓ | 社交媒体
    |'
- en: '| Daily Dialog Li et al. ([2017](#bib.bib32)) | 7.9 | 1 | 114.7 | - | ✗ | Crowdsourcing
    |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Daily Dialog Li等（[2017](#bib.bib32)） | 7.9 | 1 | 114.7 | - | ✗ | 众包 |'
- en: '| SODA Kim et al. ([2023](#bib.bib23)) | 7.6 | 1 | 122.4 | - | ✗ | LLM-generated
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| SODA Kim等（[2023](#bib.bib23)） | 7.6 | 1 | 122.4 | - | ✗ | LLM生成 |'
- en: '| MSC Xu et al. ([2022](#bib.bib58)) (train; 1-4 sessions) | 53.3 | 4 | 1,225.9
    | few days | ✗ | Crowdsourcing |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| MSC Xu等（[2022](#bib.bib58)）（训练；1-4次会话） | 53.3 | 4 | 1,225.9 | 几天 | ✗ | 众包
    |'
- en: '| Conversation Chronicles Jang et al. ([2023](#bib.bib21)) | 58.5 | 5 | 1,054.7
    | few hours - years | ✗ | LLM-generated |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Conversation Chronicles Jang等（[2023](#bib.bib21)） | 58.5 | 5 | 1,054.7 |
    几小时 - 几年 | ✗ | LLM生成 |'
- en: '| LoCoMo (ours) | 304.9 | 19.3 | 9,209.2 | few months | ✓ | LLM-gen. + crowdsourc.
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| LoCoMo (我们) | 304.9 | 19.3 | 9,209.2 | 几个月 | ✓ | LLM生成 + 众包 |'
- en: 'Table 1: Statistics of LoCoMo compared to existing dialog datasets. The average
    length of a conversation in LoCoMo is 9x that of MSC Xu et al. ([2022](#bib.bib58)),
    distributed over 6x more turns and 4x more sessions (on average).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LoCoMo与现有对话数据集的统计比较。LoCoMo中对话的平均长度是MSC Xu等（[2022](#bib.bib58)）的9倍，分布在6倍多的轮次和4倍多的会话中（平均）。
- en: '![Refer to caption](img/f274f6fdc545cd12f7ea667d2a5ba5da.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f274f6fdc545cd12f7ea667d2a5ba5da.png)'
- en: 'Figure 2: Overview of our evaluation framework. We propose three tasks: question
    answering, event summarization and multimodal dialog generation to evaluate models’
    comprehension in very long-term dialogues.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们评估框架的概述。我们提出了三个任务：问答、事件总结和多模态对话生成，以评估模型在长期对话中的理解能力。
- en: Despite recent advancements in dialogue models based on LLMs for extended contexts Bertsch
    et al. ([2024](#bib.bib5)); Xiao et al. ([2023](#bib.bib56)), as well as the integration
    of retrieval augmented generation (RAG) techniques Shuster et al. ([2021](#bib.bib51));
    Ram et al. ([2023](#bib.bib47)); Shi et al. ([2023](#bib.bib48)), there is still
    a need for thorough evaluation of their efficacy in handling very long conversations.
    Indeed, studies in long-term open-domain dialogues have concentrated on assessing
    model responses within limited contexts e.g., $\sim$1K tokens over five chat sessions Xu
    et al. ([2022](#bib.bib58)); Jang et al. ([2023](#bib.bib21)); Zhang et al. ([2023](#bib.bib62)).
    This long term evaluation is crucial for refining engaging chatbots capable of
    remembering key information from past interactions, to generate empathetic, consistent,
    and useful responses.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最近在基于LLMs的对话模型方面取得了进展，例如Bertsch等（[2024](#bib.bib5)）；Xiao等（[2023](#bib.bib56)），以及检索增强生成（RAG）技术的整合Shuster等（[2021](#bib.bib51)）；Ram等（[2023](#bib.bib47)）；Shi等（[2023](#bib.bib48)），但仍然需要对其处理非常长对话的效果进行彻底评估。实际上，长期开放领域对话的研究集中在评估模型在有限上下文中的响应，例如，$\sim$1K
    tokens在五次聊天会话中的Xu等（[2022](#bib.bib58)）；Jang等（[2023](#bib.bib21)）；Zhang等（[2023](#bib.bib62)）。这种长期评估对于完善能够记住过去互动中的关键信息的吸引人的聊天机器人至关重要，以生成富有同理心、一致且有用的响应。
- en: 'To this end, we present the first study of very long-term open-domain multi-modal
    dialogues, closely mirroring real-world online interactions, collected via a human-machine
    pipeline where we first use LLM-based generative agents to generate conversations
    and then ask human annotators to fix any long-term inconsistencies in the conversations.
    Specifically, drawing on the understanding that real-world conversations are a
    complex blend of collective memories Assmann and Czaplicka ([1995](#bib.bib4));
    Hirst and Manier ([2008](#bib.bib18)), individual viewpoints Hirst et al. ([2018](#bib.bib19)),
    external influences Hirst and Echterhoff ([2012](#bib.bib17)), and the unique
    persona of the speakers Pruitt and Grudin ([2003](#bib.bib46)); Cooper ([1999](#bib.bib9));
    Zhou et al. ([2020](#bib.bib68)); Shum et al. ([2020](#bib.bib49)), we create
    very long-term dialogues based on LLM agent with the following features: (1) a
    unique persona (§[3.1](#S3.SS1 "3.1 Persona ‣ 3 Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")); (2) a timeline
    of causally interlinked events in their lives (§[3.2](#S3.SS2 "3.2 Temporal Event
    Graph ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")); and (3) reflect & response mechanism to respond based
    on dialogue history (like in  Park et al. ([2023](#bib.bib45))) and image sharing
    & image reaction behavior which sends or reacts to images (§[3.3](#S3.SS3 "3.3
    Virtual Agent Architecture ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating Very
    Long-Term Conversational Memory of LLM Agents")). Finally, human annotators fix
    long-range inconsistencies in dialogues, remove irrelevant images, and verify
    the grounding of dialogs to events (§[3.4](#S3.SS4 "3.4 Human Verification & Editing
    ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")). With this pipeline, we create LoCoMo, a dataset of 50
    very long-term dialogues, each consisting of 300 turns and 9K tokens on avg.,
    over up to 35 sessions (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents") and Table [1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们首次研究了非常长期的开放领域多模态对话，这些对话紧密地模拟了真实世界的在线互动，通过一个人机流程收集，其中我们首先使用基于LLM的生成代理生成对话，然后请人工标注者修正对话中的长期不一致性。具体来说，基于对现实世界对话是集体记忆
    Assmann 和 Czaplicka ([1995](#bib.bib4)); Hirst 和 Manier ([2008](#bib.bib18))、个体观点
    Hirst et al. ([2018](#bib.bib19))、外部影响 Hirst 和 Echterhoff ([2012](#bib.bib17))
    和说话者独特个性 Pruitt 和 Grudin ([2003](#bib.bib46)); Cooper ([1999](#bib.bib9)); Zhou
    et al. ([2020](#bib.bib68)); Shum et al. ([2020](#bib.bib49)) 的理解，我们基于LLM代理创建了非常长期的对话，具有以下特征：(1)
    独特个性 (§[3.1](#S3.SS1 "3.1 Persona ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")); (2) 生活中的因果关联事件时间线 (§[3.2](#S3.SS2
    "3.2 Temporal Event Graph ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating Very
    Long-Term Conversational Memory of LLM Agents")); 和 (3) 基于对话历史的反思与回应机制（如在 Park
    et al. ([2023](#bib.bib45)) 中）以及图像分享与图像反应行为，发送或对图像作出反应 (§[3.3](#S3.SS3 "3.3 Virtual
    Agent Architecture ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents")）。最后，人工标注者修正对话中的长程不一致性，移除无关图像，并验证对话与事件的关联
    (§[3.4](#S3.SS4 "3.4 Human Verification & Editing ‣ 3 Generative Pipeline for
    LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")）。通过这个流程，我们创建了LoCoMo，一个包含50个非常长期对话的数据集，每个对话平均有300轮和9K个标记，最多覆盖35个会话（见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents") 和表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Evaluating Very
    Long-Term Conversational Memory of LLM Agents")）。
- en: Conventional approaches for evaluating conversational agents in open-domain
    dialogues involves directly evaluating the agent response based on past dialogue
    history. It often employs lexical overlap Papineni et al. ([2002](#bib.bib44))
    and semantic overlap Zhang et al. ([2019](#bib.bib64)) between ground truth and
    the agent response, or consistency Ghazarian et al. ([2022](#bib.bib15)), contradiction Nie
    et al. ([2021](#bib.bib43)); Welleck et al. ([2019](#bib.bib54)), and empathy Zhang
    et al. ([2021a](#bib.bib60), [2022](#bib.bib61)) of the agent response. However,
    these evaluation metrics are not well-suited for directly assessing the agent’s
    comprehension of long-term contexts.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的开放领域对话体评估方法涉及基于过往对话历史直接评估代理的回应。它通常采用词汇重叠 Papineni et al. ([2002](#bib.bib44))
    和语义重叠 Zhang et al. ([2019](#bib.bib64))，或一致性 Ghazarian et al. ([2022](#bib.bib15))、矛盾
    Nie et al. ([2021](#bib.bib43)); Welleck et al. ([2019](#bib.bib54)) 和同理心 Zhang
    et al. ([2021a](#bib.bib60), [2022](#bib.bib61)) 来评估代理回应。然而，这些评估指标不适合直接评估代理对长期背景的理解。
- en: 'In this study, we present a holistic evaluation framework to assess an agent’s
    proficiency in managing and responding within long-term contexts (see Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Evaluating Very Long-Term Conversational Memory of
    LLM Agents")). First, agents need to “recall” past context correctly to integrate
    relevant information into future responses. We present a direct examination of
    their memory via a question answering (QA) task (§[4.1](#S4.SS1 "4.1 Question
    Answering Task ‣ 4 LoCoMo Evaluation Benchmark ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")). We classify questions into five distinct reasoning types
    to evaluate memory from multiple perspectives: single-hop, multi-hop, temporal,
    commonsense or world knowledge, and adversarial. Second, agents also need to recognize
    long-range causal and temporal connections in the dialogues to generate empathetic
    and relevant responses. We propose a measurement of their causal and temporal
    understanding with an event graph summarization task (§[4.2](#S4.SS2 "4.2 Event
    Summarization Task ‣ 4 LoCoMo Evaluation Benchmark ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents")). In this task, the event graphs linked
    to each LLM speaker serve as the correct answers, and models are tasked with extracting
    this information from the conversation history. Third, conversational agents need
    to utilize relevant context recalled from past conversations to generate responses
    that are consistent with the ongoing narrative. We assess this ability via the
    multi-modal dialog generation task (§[4.3](#S4.SS3 "4.3 Multi-Modal Dialogue Generation
    Task ‣ 4 LoCoMo Evaluation Benchmark ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一个全面的评估框架来评估代理在长期上下文中的管理和响应能力（参见图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")）。首先，代理需要正确“回忆”过去的上下文，以将相关信息整合到未来的响应中。我们通过问答（QA）任务直接检查它们的记忆能力（§[4.1](#S4.SS1
    "4.1 Question Answering Task ‣ 4 LoCoMo Evaluation Benchmark ‣ Evaluating Very
    Long-Term Conversational Memory of LLM Agents")）。我们将问题分类为五种不同的推理类型，以从多个角度评估记忆：单跳、多跳、时间推理、常识或世界知识，以及对抗性问题。其次，代理还需要识别对话中的长程因果和时间连接，以生成富有同情心和相关的响应。我们通过事件图总结任务（§[4.2](#S4.SS2
    "4.2 Event Summarization Task ‣ 4 LoCoMo Evaluation Benchmark ‣ Evaluating Very
    Long-Term Conversational Memory of LLM Agents")）来测量它们的因果和时间理解。在此任务中，关联到每个 LLM
    说话者的事件图作为正确答案，模型需从对话历史中提取这些信息。第三，对话代理需要利用从过去对话中回忆的相关上下文，以生成与正在进行的叙事一致的响应。我们通过多模态对话生成任务（§[4.3](#S4.SS3
    "4.3 Multi-Modal Dialogue Generation Task ‣ 4 LoCoMo Evaluation Benchmark ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")）评估这一能力。
- en: 'We present extensive experimental results on the LoCoMo benchmark using instruction-based
    LLMs, long-context LLMs, and RAG techniques (§[5](#S5 "5 Experimental Setup ‣
    Evaluating Very Long-Term Conversational Memory of LLM Agents")). Our findings
    include:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在 LoCoMo 基准上使用基于指令的 LLMs、长上下文 LLMs 和 RAG 技术的广泛实验结果（§[5](#S5 "5 Experimental
    Setup ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")）。我们的发现包括：
- en: (1) Long-context LLMs and RAG demonstrate effectiveness in QA tasks, improving
    ‘memory’ capabilities of LLMs (with improvements ranging from 22-66%), but still
    significantly lag behind human levels (by 56%), especially in temporal reasoning,
    (by 73%);
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 长上下文 LLMs 和 RAG 在 QA 任务中表现出有效性，提高了 LLMs 的“记忆”能力（提升幅度在 22-66% 之间），但仍显著落后于人类水平（落后
    56%），尤其是在时间推理方面（落后 73%）；
- en: (2) long-context LLMs demonstrate significant difficulty with adversarial questions
    in the QA task, showing a performance that is 83% lower than the base model. They
    are especially prone to misassigning dialogs or events to the wrong speaker. Moreover,
    they show poor performance on event graph summarization, lagging behind the base
    model by 14%, indicating that they may grasp the factual elements within the entire
    conversation but do not accurately comprehend the context; and
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 长上下文 LLMs 在 QA 任务中的对抗性问题上表现出显著困难，表现比基础模型低 83%。它们特别容易将对话或事件错误地分配给错误的发言者。此外，它们在事件图总结上的表现也很差，比基础模型低
    14%，这表明它们可能掌握了整个对话中的事实元素，但未能准确理解上下文；
- en: (3) RAG offers a balanced compromise, combining the accuracy of short-context
    LLMs with the extensive comprehension of wide-context LLMs, and does particularly
    well when dialogues are transformed into a database of assertions (observations)
    about each speaker’s life and persona.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (3) RAG 提供了一个平衡的折衷方案，结合了短上下文 LLM 的准确性和宽上下文 LLM 的广泛理解，并在将对话转换为关于每个说话者的生活和个性陈述（观察）的数据库时表现特别出色。
- en: 2 Related Work
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Long-term Dialogue.
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长期对话。
- en: 'Recent approaches involve retrieving historical context from a range of previous
    dialogues and reasoning over retrieved segments in a temporal order Lee et al.
    ([2023b](#bib.bib28)); Lu et al. ([2023](#bib.bib38)); Zhong et al. ([2023](#bib.bib67));
    Liang et al. ([2023](#bib.bib33)) and/or using events to scaffold the dialogues
    Jang et al. ([2023](#bib.bib21)); Zhang et al. ([2023](#bib.bib62)) to enable
    consistency in long-term conversations. Some limitations of such frameworks are:
    (1) The accuracy of retrieval can be compromised, as the retrieval model is generally
    trained on tasks focusing on semantic similarity rather than specifically on such
    dialogues. Additionally, real-world dialogues often feature co-references and
    missing content (i.e., anaphora) Anantha et al. ([2021](#bib.bib2)), which further
    complicate the retrieval process Mallen et al. ([2023](#bib.bib39)); Gao et al.
    ([2023b](#bib.bib14)); Liu et al. ([2023](#bib.bib36)); (2) Challenges arise in
    reasoning over retrieved documents, especially when the model struggles to identify
    the correct context among the retrieved data Liu et al. ([2024](#bib.bib37));
    (3) Reasoning over time intervals presents challenges. For example, the way a
    system responds about past events can vary depending on the amount of time that
    has passed since the last conversation Zhang et al. ([2023](#bib.bib62)); Jang
    et al. ([2023](#bib.bib21)). Therefore, it is essential to have conversations
    of considerable length, as well as a systematic evaluation framework, to accurately
    assess the effectiveness of approaches to long-term dialogue generation. We design
    a long-term conversation generation pipeline based on retrieval augmentation and
    events graphs and propose a framework for evaluating long-term dialog agents.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的方法涉及从一系列先前对话中检索历史背景，并按时间顺序对检索到的片段进行推理 Lee et al. ([2023b](#bib.bib28)); Lu
    et al. ([2023](#bib.bib38)); Zhong et al. ([2023](#bib.bib67)); Liang et al. ([2023](#bib.bib33))，和/或使用事件来支撑对话
    Jang et al. ([2023](#bib.bib21)); Zhang et al. ([2023](#bib.bib62))，以实现长期对话中的一致性。这些框架的一些局限性包括：(1)
    检索的准确性可能会受到影响，因为检索模型通常在关注语义相似性的任务上进行训练，而不是专门针对这些对话。此外，现实世界中的对话经常出现共指和缺失内容（即指代） Anantha
    et al. ([2021](#bib.bib2))，这进一步复杂化了检索过程 Mallen et al. ([2023](#bib.bib39)); Gao
    et al. ([2023b](#bib.bib14)); Liu et al. ([2023](#bib.bib36)); (2) 在检索文档上进行推理时会遇到挑战，特别是当模型难以在检索数据中识别正确的背景时 Liu
    et al. ([2024](#bib.bib37)); (3) 推理时间间隔存在挑战。例如，系统对过去事件的响应方式可能会因自上次对话以来经过的时间长度而有所不同 Zhang
    et al. ([2023](#bib.bib62)); Jang et al. ([2023](#bib.bib21))。因此，进行较长时间的对话以及拥有系统化的评估框架是至关重要的，以准确评估长期对话生成方法的有效性。我们设计了一个基于检索增强和事件图的长期对话生成流程，并提出了一个用于评估长期对话代理的框架。
- en: Multi-modal Dialogue.
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态对话。
- en: 'Multi-modal dialogue primarily consists of two types of tasks: image-grounded
    dialogue and image-sharing dialogue. The image-grounded dialogue task is centered
    around responding to questions Antol et al. ([2015](#bib.bib3)); Das et al. ([2017](#bib.bib11));
    Kottur et al. ([2019](#bib.bib24)) or creating natural conversations related to
    specific images Mostafazadeh et al. ([2017](#bib.bib42)); Shuster et al. ([2020](#bib.bib50));
    Meng et al. ([2020](#bib.bib40)); Zheng et al. ([2022](#bib.bib66)). Conversely,
    the image-sharing dialogue task focuses on selecting images that semantically
    align with the provided dialogue context Zang et al. ([2021](#bib.bib59)); Feng
    et al. ([2023](#bib.bib12)); Lee et al. ([2023c](#bib.bib29)). We use a method
    from the image-sharing dialogue task to create multimodal dialogs which are then
    evaluated as an image-grounded dialogue task.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态对话主要包括两类任务：图像引导对话和图像共享对话。图像引导对话任务侧重于回答问题 Antol et al. ([2015](#bib.bib3));
    Das et al. ([2017](#bib.bib11)); Kottur et al. ([2019](#bib.bib24))，或创建与特定图像相关的自然对话 Mostafazadeh
    et al. ([2017](#bib.bib42)); Shuster et al. ([2020](#bib.bib50)); Meng et al.
    ([2020](#bib.bib40)); Zheng et al. ([2022](#bib.bib66))。相反，图像共享对话任务则侧重于选择与提供的对话背景在语义上对齐的图像 Zang
    et al. ([2021](#bib.bib59)); Feng et al. ([2023](#bib.bib12)); Lee et al. ([2023c](#bib.bib29))。我们使用了来自图像共享对话任务的方法来创建多模态对话，这些对话随后作为图像引导对话任务进行评估。
- en: Synthetic Evaluation Benchmark.
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 合成评估基准。
- en: Faced with a shortage of human-generated data and observing that LLMs are approaching
    the quality of human-level annotations He et al. ([2023](#bib.bib16)); Lee et al.
    ([2023a](#bib.bib27)), there has been a surge in research drawing inspiration
    from this development. Consequently, numerous studies have started utilizing LLMs
    to augment or synthesize large-scale dialogue benchmarks for assessing responses
    in everyday social interactions Kim et al. ([2023](#bib.bib23)), examining responses
    in multi-modal environment Feng et al. ([2023](#bib.bib12)), and evaluating responses
    that align with specific persona Jandaghi et al. ([2023](#bib.bib20)). We leverage
    LLMs to create data but ensure its high quality with human verification and editing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 面对人类生成数据的短缺，并观察到LLM（大规模语言模型）逐渐接近人类水平注释的质量，He 等人（[2023](#bib.bib16)）；Lee 等人（[2023a](#bib.bib27)），研究人员对这一发展趋势的灵感激增。因此，许多研究开始利用LLM来增强或合成大规模对话基准，以评估日常社交互动中的回应 Kim
    等人（[2023](#bib.bib23)），检查多模态环境中的回应 Feng 等人（[2023](#bib.bib12)），以及评估符合特定角色的回应 Jandaghi
    等人（[2023](#bib.bib20)）。我们利用LLM生成数据，但通过人工验证和编辑来确保其高质量。
- en: 3 Generative Pipeline for LoCoMo
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LoCoMo的生成流程
- en: '![Refer to caption](img/7afe56741026586b8e21fb673a023a08.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7afe56741026586b8e21fb673a023a08.png)'
- en: 'Figure 3: Overview of the generative pipeline for LoCoMo. Each LLM agent is
    assigned a distinct persona and a timeline of causally connected events in their
    file. The agent is equipped with a memory and reflection module to retrieve relevant
    history for dialog generation and is also enabled for image-sharing and image-reaction
    behaviors (left). The generated conversations are edited by human annotators to
    maintain long-range consistency (right).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：LoCoMo生成流程概述。每个LLM代理被分配一个独特的角色以及其文件中因果关联事件的时间线。该代理配备了一个记忆和反思模块，以检索相关的历史数据用于对话生成，同时支持图像共享和图像反应行为（左图）。生成的对话由人工注释员编辑，以保持长期一致性（右图）。
- en: An overview of our generative pipeline for LoCoMo is shown in Figure [3](#S3.F3
    "Figure 3 ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents"). We create two virtual agents, named $\mathcal{L}_{1}$
    can share coherent images, thereby enhancing the multi-modal dialogue aspect.
    Finally, human annotators are tasked with manually filtering and refining the
    generated data (§[3.4](#S3.SS4 "3.4 Human Verification & Editing ‣ 3 Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LoCoMo生成流程概述如图 [3](#S3.F3 "Figure 3 ‣ 3 Generative Pipeline for LoCoMo ‣
    Evaluating Very Long-Term Conversational Memory of LLM Agents")所示。我们创建了两个虚拟代理，名为$\mathcal{L}_{1}$，可以共享连贯的图像，从而增强多模态对话的方面。最后，人工注释员负责手动筛选和精炼生成的数据（§[3.4](#S3.SS4
    "3.4 Human Verification & Editing ‣ 3 Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")）。
- en: 3.1 Persona
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 角色
- en: 'We select an initial persona statement $p_{c}$ (See examples and prompt details
    in Appendix [A.1](#A1.SS1 "A.1 Persona ‣ Appendix A Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")). The generated
    statements typically include details about one or more of the following elements Gao
    et al. ([2023a](#bib.bib13)): objectives, past experiences, daily habits, and
    interpersonal relationships, as well as name, age, and gender of the individual.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择一个初始角色陈述$p_{c}$（见附录 [A.1](#A1.SS1 "A.1 Persona ‣ Appendix A Generative Pipeline
    for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")中的示例和提示详情）。生成的陈述通常包括关于以下一个或多个元素的详细信息 Gao
    等人（[2023a](#bib.bib13)）：目标、过去经历、日常习惯和人际关系，以及个人的姓名、年龄和性别。
- en: 3.2 Temporal Event Graph
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 时间事件图
- en: To utilize the real-life experiences of each agent in the conversation, we construct
    a temporal event graph, labeled as $\mathcal{G}$ events. See details in Appendix [A.2](#A1.SS2
    "A.2 Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents").
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用每个代理在对话中的现实经历，我们构建了一个时间事件图，标记为$\mathcal{G}$事件。详细信息见附录 [A.2](#A1.SS2 "A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")。
- en: 3.3 Virtual Agent Architecture
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 虚拟代理架构
- en: 'Every agent $\mathcal{L}_{i}$ incorporates modules from generative agent architecture Park
    et al. ([2023](#bib.bib45)). The agent has two functions: (1) reflect & respond;
    and (2) image sharing & image reaction. The agent is asked to primarily use the
    reflect & respond function while employing image sharing & image reaction function
    judiciously and appropriately within the context of the conversation.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理$\mathcal{L}_{i}$包含了Park等人（[2023](#bib.bib45)）的生成代理架构中的模块。代理有两个功能：（1）反映与回应；（2）图像共享与图像反应。代理被要求主要使用反映与回应功能，同时在对话的上下文中明智而恰当地使用图像共享与图像反应功能。
- en: Reflect & Respond.
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反映与回应。
- en: The fundamental process for each agent to reflect and respond involves the concept
    of short-term and long-term memory. During inference, agent $\mathcal{L}_{i}$.
    See details in Appendix [A.2.1](#A1.SS2.SSS1 "A.2.1 Virtual Agent Architecture
    ‣ A.2 Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents").
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理反映和响应的基本过程涉及短期和长期记忆的概念。在推理过程中，代理$\mathcal{L}_{i}$。详见附录[A.2.1](#A1.SS2.SSS1
    "A.2.1 Virtual Agent Architecture ‣ A.2 Temporal Event Graph ‣ Appendix A Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")。
- en: Image Sharing & Image Reaction.
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像共享与图像反应。
- en: 'The image sharing & image reaction functions are integrated to add a multi-modal
    dimension to the long-term dialogues.²²2Image captions are also saved to long-term
    memory. The image sharing function is called when the agent decides to send an
    image. This process includes: (1) Generate a caption $c$ (See Appendix [A.2.1](#A1.SS2.SSS1
    "A.2.1 Virtual Agent Architecture ‣ A.2 Temporal Event Graph ‣ Appendix A Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图像共享与图像反应功能集成在一起，为长期对话添加了多模态维度。²²2 图像标题也被保存到长期记忆中。图像共享功能在代理决定发送图像时被调用。这个过程包括：（1）生成标题$c$（见附录[A.2.1](#A1.SS2.SSS1
    "A.2.1 Virtual Agent Architecture ‣ A.2 Temporal Event Graph ‣ Appendix A Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")）。
- en: 3.4 Human Verification & Editing
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 人工验证与编辑
- en: In the concluding phase, human annotators are tasked with (1) editing the dialogue
    to eliminate long-term inconsistencies, (2) removing or substituting irrelevant
    images, and (3) verifying and editing for alignment between event graphs and the
    content of the conversations. Overall, we observed that annotators edited nearly
    15% of the dialog turns and removed or substituted approx. 19% images present
    in the LLM-generated dataset. See examples of some edits in Appendix [A.3](#A1.SS3
    "A.3 Human Filtering ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents").
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终阶段，人类注释者的任务是（1）编辑对话以消除长期不一致，（2）删除或替换不相关的图像，以及（3）验证和编辑事件图与对话内容之间的一致性。总体而言，我们观察到注释者编辑了近15%的对话轮次，并删除或替换了大约19%的图像。有关一些编辑的示例，请参见附录[A.3](#A1.SS3
    "A.3 Human Filtering ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")。
- en: 4 LoCoMo Evaluation Benchmark
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LoCoMo 评估基准
- en: Based on the dialogues generated in section [3](#S3 "3 Generative Pipeline for
    LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"), we introduce
    an evaluation benchmark (see Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")) composed of three tasks
    to assess the accuracy of long-term memory. See statistics of the dataset and
    evaluation benchmark in Table [5](#A2.T5 "Table 5 ‣ B.1 Dataset Statistics ‣ Appendix
    B Dataset ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents") in
    the Appendix.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于在第[3](#S3 "3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")节生成的对话，我们引入了一个评估基准（见图[2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")），该基准由三个任务组成，用于评估长期记忆的准确性。有关数据集和评估基准的统计信息，请参见附录中的表[5](#A2.T5
    "Table 5 ‣ B.1 Dataset Statistics ‣ Appendix B Dataset ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents")。
- en: 4.1 Question Answering Task
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 问答任务
- en: 'A conversational agent is expected to possess a memory to remember previous
    dialogues, reflecting it to create more engaging responses in future conversations.
    For a comprehensive assessment of this memory, we introduce a question-answering
    task divided into five distinct reasoning categories: (1) Single-hop questions
    require answers based on a single session; (2) Multi-hop questions require synthesizing
    information from multiple different sessions; (3) Temporal reasoning questions
    can be answered through temporal reasoning and capturing time-related data cues
    within the conversation; (4) Open-domain knowledge questions can be answered by
    integrating a speaker’s provided information with external knowledge such as commonsense
    or world facts; (5) Adversarial questions are designed to trick the agent into
    providing wrong answers, with the expectation that the agent will correctly identify
    them as unanswerable.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对话代理应具备记忆能力，以记住之前的对话，并在未来的对话中反映出来，从而创建更具吸引力的回应。为了全面评估这种记忆，我们引入了一个问答任务，该任务分为五个不同的推理类别：(1)
    单跳问题需要基于单一会话回答；(2) 多跳问题需要综合来自多个不同会话的信息；(3) 时间推理问题可以通过时间推理和捕捉对话中的时间相关数据线索来回答；(4)
    开放域知识问题可以通过将发言者提供的信息与外部知识（如常识或世界事实）整合来回答；(5) 对抗性问题旨在欺骗代理提供错误答案，期望代理能够正确识别这些问题为无法回答。
- en: For each category, we calculate the F1 score for exact matches, following the
    normalization of both the predicted and the actual ground truth answers. However,
    evaluating long-form answers with automated metrics often presents challenges Xu
    et al. ([2023](#bib.bib57)). LLMs tend to produce paraphrased responses in varied
    formats, complicating exact match evaluation. To simplify evaluation in our task,
    we ensure that answers in our QA annotations are directly taken from the conversations
    as much as possible. We instruct the LLMs to replicate the exact wording in the
    conversation when feasible and employ the F1 partial match metric for evaluating
    the predictions. Each QA sample is also annotated with the turn IDs in the conversation
    logs that contain the answer. We report the accuracy of retrieving the correct
    context for RAG models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个类别，我们计算确切匹配的 F1 分数，经过预测答案和实际答案的标准化。然而，使用自动化指标评估长文本答案常常存在挑战 Xu 等 ([2023](#bib.bib57))。大语言模型往往以各种格式生成释义化的响应，复杂了确切匹配的评估。为了简化我们的任务中的评估，我们确保
    QA 注释中的答案尽可能直接来自对话。我们指示大语言模型在可行时复制对话中的确切措辞，并使用 F1 部分匹配指标来评估预测结果。每个 QA 样本还附有包含答案的对话日志中的轮次
    ID。我们报告 RAG 模型正确检索上下文的准确性。
- en: 4.2 Event Summarization Task
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 事件总结任务
- en: The conversation is generated based on a temporal event graph $\mathcal{G}$.
    The events in LoCoMo are densely annotated lists of life events that are hard
    to summarize due to temporal and causal coreferences present in the dialogues,
    in contrast to existing summarization benchmarks of research papers Li et al.
    ([2023a](#bib.bib30)), movie scripts Chen et al. ([2022](#bib.bib7)), books Kryściński
    et al. ([2022](#bib.bib26)), emails Zhang et al. ([2021b](#bib.bib63)) etc.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对话是基于时间事件图 $\mathcal{G}$ 生成的。LoCoMo 中的事件是密集注释的生活事件列表，由于对话中存在时间和因果指代，难以总结，这与现有的研究论文摘要
    Li 等 ([2023a](#bib.bib30))、电影剧本 Chen 等 ([2022](#bib.bib7))、书籍 Kryściński 等 ([2022](#bib.bib26))、电子邮件
    Zhang 等 ([2021b](#bib.bib63)) 等摘要基准形成对比。
- en: Traditional metrics like BLEU Papineni et al. ([2002](#bib.bib44)) and ROGUE Lin
    ([2004](#bib.bib34)) focus on lexical similarity between the reference and generated
    summaries, not meeting our needs as we emphasize factual accuracy in summarization.
    In this context, we employ FactScore Min et al. ([2023](#bib.bib41)), a method
    that evaluates the factuality of generated text by decomposing both the reference
    and hypothesis into atomic facts. We adapt the metric to measure (1) precision
    of the summarized content by counting the number of atomic facts within the content
    that correspond with those in $\mathcal{G}$ are represented within the content.
    We present the F1 score, derived from the calculated precision and recall.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的评价指标如 BLEU Papineni 等 ([2002](#bib.bib44)) 和 ROGUE Lin ([2004](#bib.bib34))
    主要关注参考摘要与生成摘要之间的词汇相似度，这并不能满足我们的需求，因为我们强调摘要中的事实准确性。在这种情况下，我们采用 FactScore Min 等
    ([2023](#bib.bib41))，这是一种通过将参考和假设文本分解为原子事实来评估生成文本的事实性的的方法。我们调整该指标以测量 (1) 摘要内容的精确度，通过计算摘要内容中与
    $\mathcal{G}$ 中的原子事实相对应的原子事实数量来实现。我们展示了从计算出的精确度和召回率中得出的 F1 分数。
- en: 4.3 Multi-Modal Dialogue Generation Task
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 多模态对话生成任务
- en: The conversations in our dataset are anchored to specific personas $p$. The
    topics in conversations evolve from events that were introduced in earlier dialogues,
    spanning weeks or months. This structure allows for an assessment of whether conversational
    agents can sustain a coherent persona and a continuous narrative over time. For
    example, if a speaker recently had an injury, the next conversations would likely
    focus on them recuperating, rather than engaging in adventurous activities. We
    assess such consistency by measuring how closely the predicted multi-modal dialogues
    align with the ground truth multi-modal dialogues in our dataset, quantifying
    this alignment through MMRelevance Feng et al. ([2023](#bib.bib12)), in addition
    to other NLG metrics.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的对话以特定的角色 $p$ 为基础。对话中的话题从早期对话中引入的事件中演变而来，时间跨度从几周到几个月。这种结构可以评估对话代理是否能够在时间上保持一致的角色和连贯的叙事。例如，如果某个发言者最近受了伤，那么接下来的对话可能会集中在他们的恢复上，而不是参与冒险活动。我们通过测量预测的多模态对话与我们数据集中实际多模态对话的匹配程度来评估这种一致性，通过
    MMRelevance Feng 等人 ([2023](#bib.bib12)) 来量化这种对齐程度，此外还使用其他 NLG 指标。
- en: 5 Experimental Setup
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验设置
- en: For the question-answering and event summarization tasks, we replace images
    in LoCoMo with their captions Li et al. ([2023b](#bib.bib31)), and use state-of-art
    LLMs to reason over text-only dialogues interleaved with image captions. We use
    images directly for the multimodal dialog generation task only. See additional
    details in Appendix [C](#A3 "Appendix C Experimental Setup ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于问答和事件摘要任务，我们用它们的说明 Li 等人 ([2023b](#bib.bib31)) 替换了 LoCoMo 中的图像，并使用最先进的 LLM
    对仅包含文本的对话和图像说明进行推理。我们仅在多模态对话生成任务中直接使用图像。有关更多详细信息，请参见附录 [C](#A3 "附录 C 实验设置 ‣ 评估
    LLM 代理的非常长期对话记忆")。
- en: Question Answering.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问答。
- en: 'We evaluate three types of models: (1) Base LLMs operating with constrained
    context lengths where earlier dialogues are omitted i.e., Mistral-7B Jiang et al.
    ([2023](#bib.bib22)), LLama-70B-chat Touvron et al. ([2023](#bib.bib52)), gpt-3.5-turbo ⁵⁵5https://platform.openai.com/docs/models/gpt-3-5,
    and gpt-4-turbo ⁶⁶6https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo;
    (2) Long-context LLMs with an extended context window i.e., gpt-3.5-turbo-16k;
    (3) Retrieval-augmented Generation (RAG) involves retrieving relevant context
    from a database of dialog history, observations (assertions about speakers; see
    §[3.3](#S3.SS3 "3.3 Virtual Agent Architecture ‣ 3 Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"), Figure [9](#A1.F9
    "Figure 9 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")), or session-level summaries
    (see §[3.3](#S3.SS3 "3.3 Virtual Agent Architecture ‣ 3 Generative Pipeline for
    LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"), Figure [8](#A1.F8
    "Figure 8 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")). We employ DRAGON Lin et al.
    ([2023](#bib.bib35)) as retriever and gpt-3.5-turbo-16k as reader.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估三种类型的模型：（1）基础 LLM，操作时上下文长度受限，即，Mistral-7B Jiang 等人 ([2023](#bib.bib22))，LLama-70B-chat
    Touvron 等人 ([2023](#bib.bib52))，gpt-3.5-turbo [5](https://platform.openai.com/docs/models/gpt-3-5)，和
    gpt-4-turbo [6](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo)；（2）长上下文
    LLM，具有扩展的上下文窗口，即 gpt-3.5-turbo-16k；（3）检索增强生成（RAG），涉及从对话历史的数据库、观察（关于发言者的断言；见 §[3.3](#S3.SS3
    "3.3 虚拟代理架构 ‣ 3 LoCoMo 的生成管道 ‣ 评估 LLM 代理的非常长期对话记忆")，图 [9](#A1.F9 "图 9 ‣ 图像共享与响应。
    ‣ A.2.1 虚拟代理架构 ‣ A.2 时间事件图 ‣ 附录 A LoCoMo 的生成管道 ‣ 评估 LLM 代理的非常长期对话记忆")）或会话级总结（见
    §[3.3](#S3.SS3 "3.3 虚拟代理架构 ‣ 3 LoCoMo 的生成管道 ‣ 评估 LLM 代理的非常长期对话记忆")，图 [8](#A1.F8
    "图 8 ‣ 图像共享与响应。 ‣ A.2.1 虚拟代理架构 ‣ A.2 时间事件图 ‣ 附录 A LoCoMo 的生成管道 ‣ 评估 LLM 代理的非常长期对话记忆")）中检索相关上下文。我们使用
    DRAGON Lin 等人 ([2023](#bib.bib35)) 作为检索器，gpt-3.5-turbo-16k 作为阅读器。
- en: Event Summarization.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事件摘要。
- en: We present experiments using Base and Long-context setups from the question-answering
    task, but refrain from including RAG since summarization requires a comprehensive
    understanding of the entire dialogue, rather than just retrieving a specific portion.
    We implement incremental summarization i.e., iteratively create a summary of a
    preceding sessions and then use that summary as a basis to summarize the subsequent
    sessions Chang et al. ([2023](#bib.bib6)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了使用基础和长上下文设置的问答任务实验，但不包括RAG，因为摘要需要对整个对话进行全面理解，而不仅仅是检索特定部分。我们实现了增量摘要，即迭代地创建先前会话的摘要，然后使用该摘要作为总结后续会话的基础
    Chang等人（[2023](#bib.bib6)）。
- en: Multi-modal Dialogue Generation.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态对话生成。
- en: 'We generate 50 conversations using our automated pipeline (without human filtering;
    §[3](#S3 "3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")) for training data and train three versions of MiniGPT-5 Zheng
    et al. ([2023](#bib.bib65)): (1) Base trains on prior dialogue turns only; (2)
    + summary trains on prior dialogue turns and a global summary of the ongoing conversation;
    (3) + observation trains on prior dialogue turns and observations retrieved from
    conversation history. Each run is initialized with a MiniGPT-5 checkpoint finetuned
    on MMDialog Feng et al. ([2023](#bib.bib12)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自动化管道（没有人工过滤；§[3](#S3 "3 Generative Pipeline for LoCoMo ‣ Evaluating Very
    Long-Term Conversational Memory of LLM Agents")）生成了50个对话作为训练数据，并训练了三个版本的MiniGPT-5
    Zheng等人（[2023](#bib.bib65)）：（1）基础版仅基于之前的对话回合进行训练；（2）+ 摘要版基于之前的对话回合和正在进行对话的全球摘要进行训练；（3）+
    观察版基于之前的对话回合和从对话历史中检索到的观察进行训练。每次训练都是从一个MiniGPT-5检查点开始，该检查点在MMDialog Feng等人（[2023](#bib.bib12)）上进行了微调。
- en: '| Category | Model | Context Length | Answer Prediction (F1) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 模型 | 上下文长度 | 答案预测（F1） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Single Hop | Multi Hop | Temporal | Open Domain | Adversarial | Overall |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 单跳 | 多跳 | 时间序列 | 开放领域 | 对抗性 | 总体 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Human | Human | - | 95.1 | 85.8 | 92.6 | 75.4 | 89.4 | 87.9 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 人工 | 人工 | - | 95.1 | 85.8 | 92.6 | 75.4 | 89.4 | 87.9 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Base | Mistral-Instruct-7B | 8K | 10.2 | 12.8 | 16.1 | 19.5 | 17.0 | 13.9
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | Mistral-Instruct-7B | 8K | 10.2 | 12.8 | 16.1 | 19.5 | 17.0 | 13.9 |'
- en: '| Llama-2-Chat-70B | 4,096 | 19.7 | 14.4 | 13.3 | 15.9 | 22.1 | 17.9 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-Chat-70B | 4,096 | 19.7 | 14.4 | 13.3 | 15.9 | 22.1 | 17.9 |'
- en: '| GPT-3.5-turbo | 4,096 | 29.9 | 23.3 | 17.5 | 29.5 | 12.8 | 22.4 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | 4,096 | 29.9 | 23.3 | 17.5 | 29.5 | 12.8 | 22.4 |'
- en: '| GPT-4-turbo | 4,096 | 23.4 | 23.4 | 10.4 | 24.6 | 70.2 | 32.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-turbo | 4,096 | 23.4 | 23.4 | 10.4 | 24.6 | 70.2 | 32.1 |'
- en: '| Long context | GPT-3.5-turbo-16K | 4K | 31.7 | 25.4 | 16.8 | 27.6 | 13.1
    | 24.1 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | GPT-3.5-turbo-16K | 4K | 31.7 | 25.4 | 16.8 | 27.6 | 13.1 | 24.1 |'
- en: '| 8K | 38.8 | 31.2 | 21.0 | 35.0 | 8.4 | 25.2 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 8K | 38.8 | 31.2 | 21.0 | 35.0 | 8.4 | 25.2 |'
- en: '| 12K | 51.1 | 40.4 | 25.0 | 36.5 | 6.4 | 33.5 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 12K | 51.1 | 40.4 | 25.0 | 36.5 | 6.4 | 33.5 |'
- en: '| 16K | 56.4 | 42.0 | 20.3 | 37.2 | 2.1 | 37.8 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 16K | 56.4 | 42.0 | 20.3 | 37.2 | 2.1 | 37.8 |'
- en: 'Table 2: Question answering performance of Base and Long-context models. Optimal
    performance is in bold. Results are based on F1-score for answer prediction; higher
    is better.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：基础和长上下文模型的问答性能。最佳性能以**粗体**显示。结果基于答案预测的F1得分；得分越高越好。
- en: '|  |  | Answer Prediction (F1 score) |  | Recall Accuracy (R@$k$) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 答案预测（F1得分） |  | 召回准确率（R@$k$） |'
- en: '| Retrieval Unit | top-$k$ | Single Hop | Multi Hop | Temporal | Open Domain
    | Adver- -sarial | Overall | Single Hop | Multi Hop | Temporal | Open Domain |
    Adver- -sarial | Overall |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 检索单元 | top-$k$ | 单跳 | 多跳 | 时间序列 | 开放领域 | 对抗性 | 总体 | 单跳 | 多跳 | 时间序列 | 开放领域
    | 对抗性 | 总体 |'
- en: '| None | - | 29.9 | 23.3 | 17.5 | 29.5 | 12.8 | 22.4 | - | - | - | - | - |
    - |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 无 | - | 29.9 | 23.3 | 17.5 | 29.5 | 12.8 | 22.4 | - | - | - | - | - | - |'
- en: '| Dialog | 5 | 42.9 | 19.4 | 21.3 | 35.8 | 31.9 | 31.7 | 66.2 | 34.4 | 89.2
    | 38.5 | 45.7 | 58.8 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | 5 | 42.9 | 19.4 | 21.3 | 35.8 | 31.9 | 31.7 | 66.2 | 34.4 | 89.2 | 38.5
    | 45.7 | 58.8 |'
- en: '|  | 10 | 46.3 | 26.8 | 24.8 | 37.5 | 29.8 | 34.6 | 72.8 | 247.4 | 97.3 | 53.8
    | 54.3 | 67.5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 10 | 46.3 | 26.8 | 24.8 | 37.5 | 29.8 | 34.6 | 72.8 | 247.4 | 97.3 | 53.8
    | 54.3 | 67.5 |'
- en: '|  | 25 | 48.1 | 36.1 | 26.2 | 43.4 | 23.4 | 35.8 | 87.5 | 64.1 | 97.3 | 67.9
    | 69.1 | 79.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | 25 | 48.1 | 36.1 | 26.2 | 43.4 | 23.4 | 35.8 | 87.5 | 64.1 | 97.3 | 67.9
    | 69.1 | 79.9 |'
- en: '|  | 50 | 50.9 | 37.2 | 24.6 | 38.3 | 17.0 | 34.8 | 90.4 | 75.5 | 97.3 | 67.9
    | 77.7 | 84.8 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 50 | 50.9 | 37.2 | 24.6 | 38.3 | 17.0 | 34.8 | 90.4 | 75.5 | 97.3 | 67.9
    | 77.7 | 84.8 |'
- en: '| Observation | 5 | 44.3 | 30.6 | 41.9 | 40.2 | 44.7 | 41.4 | 52.9 | 40.1 |
    81.1 | 38.5 | 29.8 | 49.6 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 观察 | 5 | 44.3 | 30.6 | 41.9 | 40.2 | 44.7 | 41.4 | 52.9 | 40.1 | 81.1 | 38.5
    | 29.8 | 49.6 |'
- en: '|  | 10 | 42.2 | 30.5 | 42.1 | 41.9 | 36.2 | 38.8 | 57.4 | 53.1 | 83.8 | 46.2
    | 41.5 | 57.1 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 10 | 42.2 | 30.5 | 42.1 | 41.9 | 36.2 | 38.8 | 57.4 | 53.1 | 83.8 | 46.2
    | 41.5 | 57.1 |'
- en: '|  | 25 | 44.6 | 33.2 | 41.8 | 41.9 | 27.7 | 38.0 | 71.3 | 63.8 | 83.8 | 66.7
    | 45.7 | 66.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 25 | 44.6 | 33.2 | 41.8 | 41.9 | 27.7 | 38.0 | 71.3 | 63.8 | 83.8 | 66.7
    | 45.7 | 66.0 |'
- en: '|  | 50 | 44.0 | 34.5 | 41.1 | 41.9 | 27.7 | 37.8 | 72.8 | 73.2 | 83.8 | 74.4
    | 56.4 | 71.1 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 50 | 44.0 | 34.5 | 41.1 | 41.9 | 27.7 | 37.8 | 72.8 | 73.2 | 83.8 | 74.4
    | 56.4 | 71.1 |'
- en: '| Summary | 2 | 34.6 | 15.7 | 26.9 | 26.5 | 36.2 | 29.9 | 68.4 | 39.6 | 56.8
    | 50.0 | 73.4 | 61.5 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 总结 | 2 | 34.6 | 15.7 | 26.9 | 26.5 | 36.2 | 29.9 | 68.4 | 39.6 | 56.8 | 50.0
    | 73.4 | 61.5 |'
- en: '|  | 5 | 36.6 | 16.6 | 31.0 | 34.7 | 38.3 | 32.5 | 81.6 | 57.0 | 70.3 | 60.3
    | 86.2 | 75.1 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | 5 | 36.6 | 16.6 | 31.0 | 34.7 | 38.3 | 32.5 | 81.6 | 57.0 | 70.3 | 60.3
    | 86.2 | 75.1 |'
- en: '|  | 10 | 34.5 | 14.7 | 29.3 | 31.6 | 40.4 | 31.5 | 93.4 | 82.3 | 91.9 | 80.8
    | 94.7 | 90.7 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 10 | 34.5 | 14.7 | 29.3 | 31.6 | 40.4 | 31.5 | 93.4 | 82.3 | 91.9 | 80.8
    | 94.7 | 90.7 |'
- en: 'Table 3: Question answering performance of RAG-based GPT-3.5-turbo-16k. Optimal
    performance is in bold. Results are based on F1-score metric for answer prediction
    and recall@$k$ for recall accuracy; higher is better.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于 RAG 的 GPT-3.5-turbo-16k 问题回答性能。最佳性能以**粗体**显示。结果基于 F1-score 指标用于答案预测和
    recall@$k$ 用于召回准确率；越高越好。
- en: 6 Experimental Results
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验结果
- en: We evaluate and analyze the comprehensive performance of all baseline methods
    for question answering (§[6.1](#S6.SS1 "6.1 Question Answering Task ‣ 6 Experimental
    Results ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")), event
    graph summarization (§[6.2](#S6.SS2 "6.2 Event Summarization Task ‣ 6 Experimental
    Results ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")), and
    multi-modal dialogue generation (§[6.3](#S6.SS3 "6.3 Multi-Modal Dialog Generation
    Task ‣ 6 Experimental Results ‣ Evaluating Very Long-Term Conversational Memory
    of LLM Agents")).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估并分析了所有基线方法在问题回答 (§[6.1](#S6.SS1 "6.1 问题回答任务 ‣ 6 实验结果 ‣ 评估长时间对话记忆能力"))、事件图摘要
    (§[6.2](#S6.SS2 "6.2 事件摘要任务 ‣ 6 实验结果 ‣ 评估长时间对话记忆能力")) 和多模态对话生成 (§[6.3](#S6.SS3
    "6.3 多模态对话生成任务 ‣ 6 实验结果 ‣ 评估长时间对话记忆能力")) 的综合性能。
- en: 6.1 Question Answering Task
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 问题回答任务
- en: 'Tables [2](#S5.T2 "Table 2 ‣ Multi-modal Dialogue Generation. ‣ 5 Experimental
    Setup ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents") and [3](#S5.T3
    "Table 3 ‣ Multi-modal Dialogue Generation. ‣ 5 Experimental Setup ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents") present the performance results
    for the question answering task. We find that: (1) LLMs with limited context length
    face challenges in understanding extremely long conversations due to truncated
    context windows. Despite gpt-4-turbo emerging as the top-performing model with
    an overall score of 32.4, it notably lags behind the human benchmark of 87.9;
    (2) long-context LLMs can comprehend longer narratives, yet they are prone to
    generating hallucinations. gpt-3.5-turbo-16k outperforms other approaches, but
    its performance on adversarial questions drops to a mere 2.1%, as compared to
    22.1% using Llama-2-Chat and 70.2% using GPT-4-turbo with 4K context windows.
    This indicates that LLMs can be easily misled into generating hallucinations when
    they are subjected to long contexts; (3) RAG is effective when conversations are
    stored as observations. There is a noticeable 5% improvement with gpt-3.5-turbo
    when the input is top 5 relevant observations instead of pure conversation logs.
    This improvement falters with an increase in the number of retrieved observations,
    suggesting that it is important to reduce the signal-to-noise (SNR) ratio in retrieved
    contexts for models to utilize the context accurately. Conversely, using session
    summaries as context does not significantly improve the performance despite high
    recall accuracies⁷⁷7For summary-based RAG models, the recall accuracy is based
    on retrieving the summary of the relevant session(s)., likely due to loss of information
    during the conversion of dialogs to summaries.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S5.T2 "Table 2 ‣ Multi-modal Dialogue Generation. ‣ 5 Experimental Setup
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents") 和 [3](#S5.T3
    "Table 3 ‣ Multi-modal Dialogue Generation. ‣ 5 Experimental Setup ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents") 展示了问答任务的性能结果。我们发现：（1）由于上下文窗口截断，具有有限上下文长度的
    LLMs 在理解极长对话时面临挑战。尽管 gpt-4-turbo 成为表现最好的模型，整体得分为 32.4，但显著落后于 87.9 的人类基准；（2）长上下文
    LLMs 可以理解更长的叙事，但容易生成虚假信息。gpt-3.5-turbo-16k 优于其他方法，但在对抗性问题上的表现降至仅 2.1%，而 Llama-2-Chat
    为 22.1%，GPT-4-turbo 4K 上下文窗口为 70.2%。这表明，当 LLMs 面对长上下文时，很容易被误导生成虚假信息；（3）当对话作为观察结果存储时，RAG
    是有效的。当输入为前 5 个相关观察结果而非纯对话日志时，gpt-3.5-turbo 有明显的 5% 改进。随着检索到的观察结果数量增加，这种改进会减弱，表明减少检索上下文中的信噪比
    (SNR) 对模型准确利用上下文很重要。相反，使用会话摘要作为上下文并没有显著提高性能，尽管召回准确率较高⁷⁷7 对于基于摘要的 RAG 模型，召回准确率基于检索相关会话的摘要。，这可能是由于在将对话转换为摘要的过程中信息丢失。
- en: The interesting finding is that time reasoning and open-domain knowledge questions
    are the most challenging scenarios.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的发现是，时间推理和开放域知识问题是最具挑战性的场景。
- en: (1) LLMs face challenges in understanding time concepts within dialogues, which
    is consistent with findings from other single-turn-based benchmarks focused on
    temporal reasoning capabilities for LLMs Wang and Zhao ([2023](#bib.bib53)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (1) LLMs 在对话中理解时间概念时面临挑战，这与其他专注于 LLM 时间推理能力的单轮基准测试的发现一致 Wang 和 Zhao ([2023](#bib.bib53))。
- en: (2) LLMs struggle with open-domain knowledge and degrade in the RAG setting.
    This suggests that while certain open-domain knowledge may be embedded within
    the model’s parameters, introducing improper context from inaccurate retrieval
    can lead to a decline in performance Mallen et al. ([2023](#bib.bib39)).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (2) LLMs 在开放域知识处理方面存在困难，并且在 RAG 设置下性能下降。这表明，虽然某些开放域知识可能嵌入在模型参数中，但不准确的检索引入不适当的上下文会导致性能下降 Mallen
    等 ([2023](#bib.bib39))。
- en: '| Category | Model | Context Length | ROGUE | FactScore |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Category | Model | Context Length | ROGUE | FactScore |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| ROGUE-1 | ROGUE-2 | ROGUE-L | Precision | Recall | F1 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| ROGUE-1 | ROGUE-2 | ROGUE-L | Precision | Recall | F1 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Base | Mistral-Instruct-7B | 8K | 29.4 | 7.2 | 14.1 | 27.1 | 19.8 | 23.0
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Base | Mistral-Instruct-7B | 8K | 29.4 | 7.2 | 14.1 | 27.1 | 19.8 | 23.0
    |'
- en: '| Llama-2-Chat-70B | 4,096 | 28.1 | 9.3 | 14.8 | 36.3 | 22.7 | 28.3 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-Chat-70B | 4,096 | 28.1 | 9.3 | 14.8 | 36.3 | 22.7 | 28.3 |'
- en: '| GPT-4-turbo | 4,096 | 38.8 | 11.4 | 20.6 | 51.6 | 41.8 | 45.1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-turbo | 4,096 | 38.8 | 11.4 | 20.6 | 51.6 | 41.8 | 45.1 |'
- en: '| GPT-3.5-turbo | 4,096 | 41.1 | 13.5 | 20.9 | 45.3 | 46.5 | 45.9 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | 4,096 | 41.1 | 13.5 | 20.9 | 45.3 | 46.5 | 45.9 |'
- en: '| Long context | GPT-3.5-turbo-16K | 16K | 36.2 | 8.5 | 16.4 | 42.3 | 37.8
    | 39.9 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 长上下文 | GPT-3.5-turbo-16K | 16K | 36.2 | 8.5 | 16.4 | 42.3 | 37.8 | 39.9 |'
- en: 'Table 4: Event summarization performance of Base and Long-context models. The
    optimal performance is shown in bold. Results are based on ROUGE and FactScore
    Min et al. ([2023](#bib.bib41)) metrics; higher is better.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：Base 和长上下文模型的事件摘要性能。最佳性能用粗体表示。结果基于 ROUGE 和 FactScore Min 等人 ([2023](#bib.bib41))
    指标；数值越高越好。
- en: '![Refer to caption](img/912572b7222c884519dfb83b3f8647a6.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/912572b7222c884519dfb83b3f8647a6.png)'
- en: 'Figure 4: Multimodal dialog generation performance of MiniGPT-5. (A) an example
    of multimodal dialog predicted using MiniGPT5 with and without observation as
    retrieved context, (B) Variation of MM-Relevance score with length of dialog history,
    and (C) comparison of RAG-based MiniGPT-5 methods.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MiniGPT-5 的多模态对话生成性能。 (A) 使用 MiniGPT5 进行的多模态对话预测示例，分别在有和没有观察作为检索上下文的情况下，
    (B) MM-Relevance 分数随对话历史长度的变化，以及 (C) 基于 RAG 的 MiniGPT-5 方法的比较。
- en: 6.2 Event Summarization Task
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 事件摘要任务
- en: Table [4](#S6.T4 "Table 4 ‣ 6.1 Question Answering Task ‣ 6 Experimental Results
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents") presents results
    for the event summarization task. The use of incremental summarization with gpt-3.5-turbo
    leads to the highest performance in both recall and F1 score. While gpt-4-turbo
    records a 5.3% improvement in precision over with gpt-3.5-turbo, it does not fare
    as well in terms of recall. The event summarization task requires long-range dependency
    to understand the temporal and causal connections between the events discussed
    by the speaker in multiple sessions (see Figure [7](#A1.F7 "Figure 7 ‣ A.2 Temporal
    Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents")). Contrary to expectations, the long-context
    model does not surpass the base model, despite its capability for extended-range
    reasoning facilitated by a larger context window. gpt-3.5-turbo-16k exhibits a
    decline in both precision (by 3.0%) and recall (by 8.7%) compared to gpt-3.5-turbo
    which has a 4K context window. This suggests that long-context models may not
    be proficient at utilizing their context appropriately, which also aligns with
    similar findings in Li et al. ([2023a](#bib.bib30)) as well as the QA task in
    LoCoMo. In terms of both the ROUGE and FactScore metrics, commercial models (gpt-4-turbo,
    gpt-3.5-turbo) significantly outshine their open-source counterparts. Nonetheless,
    there remains considerable scope for improving performance on this task.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S6.T4 "表 4 ‣ 6.1 问答任务 ‣ 6 实验结果 ‣ 评估 LLM 代理的长期对话记忆") 展示了事件摘要任务的结果。使用 gpt-3.5-turbo
    进行增量摘要在召回率和 F1 分数方面都取得了最佳性能。虽然 gpt-4-turbo 在精度上比 gpt-3.5-turbo 提高了 5.3%，但在召回率方面表现不如
    gpt-3.5-turbo。事件摘要任务需要长期依赖，以理解说话者在多个会话中讨论的事件之间的时间和因果关系（见图 [7](#A1.F7 "图 7 ‣ A.2
    时间事件图 ‣ 附录 A LoCoMo 生成管道 ‣ 评估 LLM 代理的长期对话记忆")）。与预期相反，尽管长上下文模型具备通过更大的上下文窗口进行扩展范围推理的能力，但其表现并未超越基础模型。gpt-3.5-turbo-16k
    相比于具有 4K 上下文窗口的 gpt-3.5-turbo，精度下降了 3.0%，召回率下降了 8.7%。这表明长上下文模型可能未能有效利用其上下文，这也与
    Li 等人 ([2023a](#bib.bib30)) 的类似发现以及 LoCoMo 中的 QA 任务一致。在 ROUGE 和 FactScore 指标方面，商业模型（gpt-4-turbo、gpt-3.5-turbo）显著优于开源模型。然而，仍有较大的提升空间。
- en: 'From a manual analysis of predicted summaries, we identify five broad categories
    of event summarization errors made by LLMs: (1) missing information in events
    because the model fails to make temporal and/or causal connections over a lengthy
    conversation; (2) hallucinations i.e., models pad extra details that are either
    not present in the conversation or are part of a different event in the same session;
    (3) errors from misunderstanding of dialog cues such as humor or sarcasm is a
    distinctive issue with comprehension of dialogs; (4) inaccurate speaker attributions;
    and (5) insignificant dialogs that are wrongly considered as salient events. See
    examples in Table [7](#A4.T7 "Table 7 ‣ D.1 Event Summarization Task ‣ Appendix
    D Results ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents") in
    the Appendix.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对预测摘要的人工分析，我们确定了LLMs在事件总结中犯的五类广泛错误：(1) 由于模型未能在长对话中建立时间或因果关系，事件中缺少信息；(2) 幻觉，即模型添加了对话中不存在或属于同一会话中不同事件的额外细节；(3)
    对对话线索如幽默或讽刺的误解，这在对话理解中是一个显著问题；(4) 说话者归属不准确；(5) 被错误视为重要事件的无关对话。见附录中的表[7](#A4.T7
    "表 7 ‣ D.1 事件总结任务 ‣ 附录D结果 ‣ 评估LLM代理的长期对话记忆")中的示例。
- en: 6.3 Multi-Modal Dialog Generation Task
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 多模态对话生成任务
- en: Figure [4](#S6.F4 "Figure 4 ‣ 6.1 Question Answering Task ‣ 6 Experimental Results
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents") illustrates
    the effectiveness of various MiniGPT-5 training variants in multi-modal dialogue
    generation. Incorporating context into training enhances performance, with the
    inclusion of observation as context yielding significantly improved results. For
    instance, in Figure [4](#S6.F4 "Figure 4 ‣ 6.1 Question Answering Task ‣ 6 Experimental
    Results ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")A, the
    retrieved observations contain information about the speaker’s experience in video
    game tournaments, which leads to the prediction of dialog and images that are
    more faithful to the speaker’s persona. This observation is consistent with earlier
    findings from the QA task as well (see Table [3](#S5.T3 "Table 3 ‣ Multi-modal
    Dialogue Generation. ‣ 5 Experimental Setup ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents")). Also, we observe that the MM-Relevance score drops with
    an increase in the length of dialog history (see Figure [4](#S6.F4 "Figure 4 ‣
    6.1 Question Answering Task ‣ 6 Experimental Results ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents")B). Retrieval-augmented generation alleviates
    the drop in MM-Relevance to some extent.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S6.F4 "图 4 ‣ 6.1 问答任务 ‣ 6 实验结果 ‣ 评估LLM代理的长期对话记忆")展示了各种MiniGPT-5训练变体在多模态对话生成中的有效性。将上下文纳入训练可以提升性能，其中将观察作为上下文的加入显著改善了结果。例如，在图[4](#S6.F4
    "图 4 ‣ 6.1 问答任务 ‣ 6 实验结果 ‣ 评估LLM代理的长期对话记忆")A中，检索到的观察包含关于讲者在视频游戏比赛中的经历的信息，这导致对话和图像的预测更符合讲者的个性。这一观察结果与早期QA任务的发现一致（见表[3](#S5.T3
    "表 3 ‣ 多模态对话生成 ‣ 5 实验设置 ‣ 评估LLM代理的长期对话记忆")）。此外，我们观察到随着对话历史长度的增加，MM-Relevance分数会下降（见图[4](#S6.F4
    "图 4 ‣ 6.1 问答任务 ‣ 6 实验结果 ‣ 评估LLM代理的长期对话记忆")B）。检索增强生成在一定程度上缓解了MM-Relevance的下降。
- en: 7 Conclusion
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We develop a human-machine pipeline to collect LoCoMo, a datset of 50 high-quality
    very long conversations, each encompassing 300 turns and 9K tokens on avg., over
    up to 35 sessions, and propose an evaluation framework consisting of three tasks
    that evaluate models’ proficiency in long conversations. Our experiments show
    that LLMs struggle to comprehend long-term narratives within the dialog and fail
    to draw temporal and causal connections between events discussed by speakers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个人机管道来收集LoCoMo，一个包含50个高质量非常长对话的数据集，每个对话涵盖300轮，平均9K个令牌，最多35个会话，并提出了一个评估框架，包括三个任务来评估模型在长对话中的能力。我们的实验显示，LLMs在理解对话中的长期叙事方面存在困难，并且未能在讲者讨论的事件之间建立时间和因果关系。
- en: 8 Limitations
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: Hybrid human-machine generated data.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 混合人机生成的数据。
- en: Our dataset is sourced primarily from text generated by LLMs. We pursued this
    method, which has quickly emerged as a popular alternative to time-intensive manual
    data collection Kim et al. ([2023](#bib.bib23)); Jang et al. ([2023](#bib.bib21)),
    to avoid the logistical and legal complexities of collecting very long-term real-world
    conversations at scale. We ensure that the dataset mirrors real-world interactions
    as much as possible by having human annotators verify and edit the generated conversations.
    However, we acknowledge that this dataset may not fully reflect the nuances of
    real-world online conversations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集主要来源于LLMs生成的文本。我们选择这种方法，因为它已迅速成为一种流行的替代方案，相比于耗时的人工数据收集方法 Kim et al. ([2023](#bib.bib23));
    Jang et al. ([2023](#bib.bib21))，可以避免大规模收集长期真实对话的后勤和法律复杂性。我们通过让人工标注者验证和编辑生成的对话，确保数据集尽可能反映真实世界的互动。然而，我们承认该数据集可能未能完全反映真实在线对话的细微差别。
- en: Limited exploration of multimodal behavior.
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对多模态行为的探索有限。
- en: Since the images in our dataset are sourced from the web, they do not demonstrate
    the visual long-term consistencies that are usually exhibited in personal photos
    (e.g., appearance, home environment, people and pets, etc.). Consequently, we
    find that the images in our dataset can be replaced with their captions without
    much loss of information, except for cases where OCR is required. Nevertheless,
    our work is a first step toward research into the multimodal aspect of very long-term
    conversations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们数据集中的图像来源于网络，它们未能展示个人照片中通常表现出的长期视觉一致性（例如，外貌、家庭环境、人物和宠物等）。因此，我们发现数据集中的图像可以用其标题替代，而信息损失很小，除非需要OCR。不过，我们的工作是研究非常长期对话的多模态方面的第一步。
- en: Language.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言。
- en: Our LLM-based pipeline for generating long-term conversations has been developed
    for the English language only. However, our pipeline can be made to work with
    any other language using an LLM that is proficient at that language and appropriate
    translations of our prompts.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于LLMs的长期对话生成管道目前仅为英语语言开发。然而，我们的管道可以与任何其他语言的LLM配合使用，只要该LLM擅长该语言，并且适当地翻译我们的提示。
- en: Closed-source LLMs.
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 闭源的大型语言模型（LLMs）。
- en: We use state-of-the-art LLMs in our dialog generation pipeline to create a dialog
    dataset that is as realistic as possible. Unfortunately, this meant employing
    the strongest commercial LLMs available through a paid API, similar to many concurrent
    works that generate synthetic conversations Zhong et al. ([2023](#bib.bib67));
    Lu et al. ([2023](#bib.bib38)). We will make the code for our generative pipeline
    publicly available in the hope that it can be made to work effectively with state-of-the-art
    open-source LLMs in the future.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在对话生成管道中使用最先进的LLMs，以创建尽可能真实的对话数据集。不幸的是，这意味着需要通过付费API使用最强大的商业LLMs，这与许多同时进行的生成合成对话的工作类似
    Zhong et al. ([2023](#bib.bib67)); Lu et al. ([2023](#bib.bib38))。我们将公开我们的生成管道代码，希望它未来能够与最先进的开源LLMs有效配合使用。
- en: Evaluation of long-form NLG.
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长文本自然语言生成的评估。
- en: LLMs are prone to generating verbose answers even when prompted to answer in
    short phrases. This creates challenges in evaluating the correctness of answers
    provided by LLMs and has been widely documented in NLP literature Chang et al.
    ([2023](#bib.bib6)); Xu et al. ([2023](#bib.bib57)); Krishna et al. ([2023](#bib.bib25)).
    Our evaluation framework suffers from the same challenges when used for experimenting
    with LLMs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 即使被要求用简短的短语回答，LLMs也容易生成冗长的答案。这给评估LLMs提供的答案的正确性带来了挑战，并在NLP文献中被广泛记录 Chang et al.
    ([2023](#bib.bib6)); Xu et al. ([2023](#bib.bib57)); Krishna et al. ([2023](#bib.bib25))。我们的评估框架在用于实验LLMs时也面临同样的挑战。
- en: 9 Broader Impacts
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 更广泛的影响
- en: We adopt and improve a framework of generative agents introduced in Park et al.
    ([2023](#bib.bib45)) for the generation of long-term conversations. Consequently,
    the ethical concerns of generative agents outlined by Park et al. ([2023](#bib.bib45))
    apply to our work as well, especially since the goal of our framework is to make
    the conversations as realistic as possible.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用并改进了 Park et al. ([2023](#bib.bib45)) 引入的生成代理框架，用于生成长期对话。因此，Park et al.
    ([2023](#bib.bib45)) 所概述的生成代理的伦理问题同样适用于我们的工作，特别是因为我们框架的目标是尽可能使对话真实。
- en: Specifically, conversational agents that can pose as human beings with a realistic
    life, as enabled by the temporal event graphs in our framework, pose the risk
    that users may form parasocial relationships with such agents that may affect
    their lives adversely. We recommend that any practical deployment of the generative
    frameworks mentioned in our work be always prefaced with a disclaimer about the
    source of the dialogs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，能够伪装成具有真实生活的人类的对话代理，如我们框架中的时间事件图所实现的那样，存在用户可能与这些代理建立寄生社交关系的风险，这可能会对他们的生活产生负面影响。我们建议在实际部署我们工作中提到的生成框架时，应始终附上有关对话来源的免责声明。
- en: Second, the use of multimodal LLMs Zheng et al. ([2023](#bib.bib65)) to generate
    images conditioned on dialog can lead to the propagation of misinformation and
    social biases, especially if the conversational agent can be coerced into parroting
    false information or dangerous opinions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，使用多模态 LLMs Zheng 等 ([2023](#bib.bib65)) 根据对话生成图像可能导致虚假信息和社会偏见的传播，特别是当对话代理被迫重复虚假信息或危险意见时。
- en: Third, it is tempting to use generative agents to substitute real humans for
    a process, especially when there are significant challenges in working with humans
    for a particular goal e.g., collecting real-world interactions between humans
    over a year or more. Care must be taken to ensure that such substitutes are not
    made in studies whose outcomes may be used to make real-world decisions with tangible
    impacts on humans. Our work is merely a study of model comprehension in very long-term
    conversations. We do not make any recommendations for real-world policies based
    on this study and advise potential users of our framework to avoid making such
    recommendations as well.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，利用生成代理来替代真实的人类进行某个过程是非常诱人的，特别是当在实现特定目标时与人类合作面临重大挑战时，例如，收集人类在一年或更长时间内的真实世界互动。必须小心确保这些替代品不会被用于那些结果可能影响到现实世界、对人类产生实际影响的研究中。我们的工作仅仅是对模型在长期对话中的理解进行研究。我们不根据这项研究对现实世界政策提出任何建议，并且建议潜在的框架用户也避免做出此类建议。
- en: References
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahn et al. (2023) Jaewoo Ahn, Yeda Song, Sangdoo Yun, and Gunhee Kim. 2023.
    [MPCHAT: Towards multimodal persona-grounded conversation](https://doi.org/10.18653/v1/2023.acl-long.189).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3354–3377, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahn 等 (2023) Jaewoo Ahn, Yeda Song, Sangdoo Yun, 和 Gunhee Kim. 2023. [MPCHAT:
    迈向多模态个性化对话](https://doi.org/10.18653/v1/2023.acl-long.189)。在 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，第3354–3377页，加拿大多伦多。计算语言学协会。'
- en: 'Anantha et al. (2021) Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne
    Longpre, Stephen Pulman, and Srinivas Chappidi. 2021. Open-domain question answering
    goes conversational via question rewriting. In *Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies*, pages 520–534.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anantha 等 (2021) Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre,
    Stephen Pulman, 和 Srinivas Chappidi. 2021. 开放领域问答通过问题重写转向对话形式。在 *2021年北美计算语言学协会：人类语言技术会议论文集*，第520–534页。
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual
    question answering. In *Proceedings of the IEEE international conference on computer
    vision*, pages 2425–2433.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Antol 等 (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell,
    Dhruv Batra, C Lawrence Zitnick, 和 Devi Parikh. 2015. Vqa: 视觉问答。在 *IEEE国际计算机视觉会议论文集*，第2425–2433页。'
- en: Assmann and Czaplicka (1995) Jan Assmann and John Czaplicka. 1995. Collective
    memory and cultural identity. *New german critique*, (65):125–133.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Assmann 和 Czaplicka (1995) Jan Assmann 和 John Czaplicka. 1995. 集体记忆与文化认同。*新德国批评*，(65):125–133。
- en: 'Bertsch et al. (2024) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew
    Gormley. 2024. Unlimiformer: Long-range transformers with unlimited length input.
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bertsch 等 (2024) Amanda Bertsch, Uri Alon, Graham Neubig, 和 Matthew Gormley.
    2024. Unlimiformer: 长范围变换器与无限长度输入。*神经信息处理系统进展*，36。'
- en: 'Chang et al. (2023) Yapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2023.
    Booookscore: A systematic exploration of book-length summarization in the era
    of llms. In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等（2023）Yapei Chang, Kyle Lo, Tanya Goyal 和 Mohit Iyyer. 2023. Booookscore:
    在 llms 时代对书籍长度摘要的系统探索。在 *第十二届国际学习表示大会*。'
- en: 'Chen et al. (2022) Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022.
    Summscreen: A dataset for abstractive screenplay summarization. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 8602–8615.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2022）Mingda Chen, Zewei Chu, Sam Wiseman 和 Kevin Gimpel. 2022. Summscreen:
    用于抽象剧本摘要的数据集。在 *第60届计算语言学协会年会（第1卷：长篇论文）*，第 8602–8615 页。'
- en: 'Chen et al. (2023) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. 2023. Longlora: Efficient fine-tuning of long-context
    large language models. In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2023）Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu,
    Song Han 和 Jiaya Jia. 2023. Longlora: 高效微调长上下文大语言模型。在 *第十二届国际学习表示大会*。'
- en: Cooper (1999) Alan Cooper. 1999. *The inmates are running the asylum*. Springer.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cooper（1999）Alan Cooper. 1999. *囚犯在管理疯人院*。Springer。
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao 等（2022）Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra 和 Christopher Ré. 2022.
    Flashattention: 快速且内存高效的精确注意力机制，具有 IO 感知。在 *神经信息处理系统进展*，35:16344–16359。'
- en: Das et al. (2017) Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj
    Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 326–335.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等（2017）Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,
    José MF Moura, Devi Parikh 和 Dhruv Batra. 2017. 视觉对话。在 *IEEE计算机视觉与模式识别会议论文集*，第
    326–335 页。
- en: 'Feng et al. (2023) Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang,
    Chongyang Tao, Dongyan Zhao, and Qingwei Lin. 2023. [MMDialog: A large-scale multi-turn
    dialogue dataset towards multi-modal open-domain conversation](https://doi.org/10.18653/v1/2023.acl-long.405).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 7348–7363, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Feng 等（2023）Jiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang
    Tao, Dongyan Zhao 和 Qingwei Lin. 2023. [MMDialog: 面向多模态开放领域对话的大规模多轮对话数据集](https://doi.org/10.18653/v1/2023.acl-long.405)。在
    *第61届计算语言学协会年会（第1卷：长篇论文）*，第 7348–7363 页，多伦多，加拿大。计算语言学协会。'
- en: 'Gao et al. (2023a) Silin Gao, Beatriz Borges, Soyoung Oh, Deniz Bayazit, Saya
    Kanno, Hiromi Wakaki, Yuki Mitsufuji, and Antoine Bosselut. 2023a. [PeaCoK: Persona
    commonsense knowledge for consistent and engaging narratives](https://doi.org/10.18653/v1/2023.acl-long.362).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6569–6591, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等（2023a）Silin Gao, Beatriz Borges, Soyoung Oh, Deniz Bayazit, Saya Kanno,
    Hiromi Wakaki, Yuki Mitsufuji 和 Antoine Bosselut. 2023a. [PeaCoK: 一致且吸引人的叙事的角色常识知识](https://doi.org/10.18653/v1/2023.acl-long.362)。在
    *第61届计算语言学协会年会（第1卷：长篇论文）*，第 6569–6591 页，多伦多，加拿大。计算语言学协会。'
- en: Gao et al. (2023b) Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023b.
    [Enabling large language models to generate text with citations](https://doi.org/10.18653/v1/2023.emnlp-main.398).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 6465–6488, Singapore. Association for Computational Linguistics.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2023b）Tianyu Gao, Howard Yen, Jiatong Yu 和 Danqi Chen. 2023b. [使大语言模型生成带有引用的文本](https://doi.org/10.18653/v1/2023.emnlp-main.398)。在
    *2023年自然语言处理实证方法会议论文集*，第 6465–6488 页，新加坡。计算语言学协会。
- en: 'Ghazarian et al. (2022) Sarik Ghazarian, Nuan Wen, Aram Galstyan, and Nanyun
    Peng. 2022. Deam: Dialogue coherence evaluation using amr-based semantic manipulations.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 771–785.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ghazarian 等（2022）Sarik Ghazarian, Nuan Wen, Aram Galstyan 和 Nanyun Peng. 2022.
    Deam: 使用基于 amr 的语义操作评估对话连贯性。在 *第60届计算语言学协会年会（第1卷：长篇论文）*，第 771–785 页。'
- en: 'He et al. (2023) Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang,
    Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al. 2023. Annollm:
    Making large language models to be better crowdsourced annotators. *arXiv preprint
    arXiv:2303.16854*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2023) Xingwei He、Zhenghao Lin、Yeyun Gong、Alex Jin、Hang Zhang、Chen Lin、Jian
    Jiao、Siu Ming Yiu、Nan Duan、Weizhu Chen 等. 2023. Annollm：使大型语言模型成为更好的众包注释者。*arXiv
    预印本 arXiv:2303.16854*。
- en: 'Hirst and Echterhoff (2012) William Hirst and Gerald Echterhoff. 2012. Remembering
    in conversations: The social sharing and reshaping of memories. *Annual review
    of psychology*, 63:55–79.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirst 和 Echterhoff (2012) William Hirst 和 Gerald Echterhoff. 2012. 对话中的记忆：记忆的社会共享与重塑。*心理学年鉴*，63:55–79。
- en: Hirst and Manier (2008) William Hirst and David Manier. 2008. Towards a psychology
    of collective memory. *Memory*, 16(3):183–200.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirst 和 Manier (2008) William Hirst 和 David Manier. 2008. 朝向集体记忆的心理学。*记忆*，16(3):183–200。
- en: Hirst et al. (2018) William Hirst, Jeremy K Yamashiro, and Alin Coman. 2018.
    Collective memory from a psychological perspective. *Trends in cognitive sciences*,
    22(5):438–451.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hirst 等 (2018) William Hirst、Jeremy K Yamashiro 和 Alin Coman. 2018. 从心理学角度看集体记忆。*认知科学趋势*，22(5):438–451。
- en: Jandaghi et al. (2023) Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara,
    and Hakim Sidahmed. 2023. Faithful persona-based conversational dataset generation
    with large language models. *arXiv preprint arXiv:2312.10007*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jandaghi 等 (2023) Pegah Jandaghi、XiangHai Sheng、Xinyi Bai、Jay Pujara 和 Hakim
    Sidahmed. 2023. 基于人物的对话数据集生成的忠实性与大型语言模型。*arXiv 预印本 arXiv:2312.10007*。
- en: 'Jang et al. (2023) Jihyoung Jang, Minseong Boo, and Hyounghun Kim. 2023. [Conversation
    chronicles: Towards diverse temporal and relational dynamics in multi-session
    conversations](https://doi.org/10.18653/v1/2023.emnlp-main.838). In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    13584–13606, Singapore. Association for Computational Linguistics.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等 (2023) Jihyoung Jang、Minseong Boo 和 Hyounghun Kim. 2023. [对话编年史：朝向多会话中的多样化时间和关系动态](https://doi.org/10.18653/v1/2023.emnlp-main.838)。见于*2023年自然语言处理实证方法会议论文集*，第13584–13606页，新加坡。计算语言学协会。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Albert Q Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier 等. 2023. Mistral 7b。*arXiv 预印本 arXiv:2310.06825*。
- en: 'Kim et al. (2023) Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing
    Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap,
    and Yejin Choi. 2023. [SODA: Million-scale dialogue distillation with social commonsense
    contextualization](https://doi.org/10.18653/v1/2023.emnlp-main.799). In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    12930–12949, Singapore. Association for Computational Linguistics.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 (2023) Hyunwoo Kim、Jack Hessel、Liwei Jiang、Peter West、Ximing Lu、Youngjae
    Yu、Pei Zhou、Ronan Bras、Malihe Alikhani、Gunhee Kim、Maarten Sap 和 Yejin Choi. 2023.
    [SODA：具有社会常识上下文化的大规模对话蒸馏](https://doi.org/10.18653/v1/2023.emnlp-main.799)。见于*2023年自然语言处理实证方法会议论文集*，第12930–12949页，新加坡。计算语言学协会。
- en: 'Kottur et al. (2019) Satwik Kottur, José M. F. Moura, Devi Parikh, Dhruv Batra,
    and Marcus Rohrbach. 2019. [CLEVR-dialog: A diagnostic dataset for multi-round
    reasoning in visual dialog](https://doi.org/10.18653/v1/N19-1058). In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages
    582–595, Minneapolis, Minnesota. Association for Computational Linguistics.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kottur 等 (2019) Satwik Kottur、José M. F. Moura、Devi Parikh、Dhruv Batra 和 Marcus
    Rohrbach. 2019. [CLEVR-dialog：用于视觉对话的多轮推理诊断数据集](https://doi.org/10.18653/v1/N19-1058)。见于*2019年北美计算语言学协会会议：人类语言技术，第1卷（长篇和短篇论文）*，第582–595页，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: 'Krishna et al. (2023) Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer,
    Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. Longeval: Guidelines for human
    evaluation of faithfulness in long-form summarization. In *Proceedings of the
    17th Conference of the European Chapter of the Association for Computational Linguistics*,
    pages 1642–1661.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等 (2023) Kalpesh Krishna、Erin Bransom、Bailey Kuehl、Mohit Iyyer、Pradeep
    Dasigi、Arman Cohan 和 Kyle Lo. 2023. Longeval：长期摘要中忠实性的人工评估指南。见于*第17届欧洲计算语言学协会会议论文集*，第1642–1661页。
- en: 'Kryściński et al. (2022) Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal,
    Caiming Xiong, and Dragomir Radev. 2022. Booksum: A collection of datasets for
    long-form narrative summarization. In *Findings of the Association for Computational
    Linguistics: EMNLP 2022*, pages 6536–6558.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kryściński 等人（2022） Wojciech Kryściński、Nazneen Rajani、Divyansh Agarwal、Caiming
    Xiong 和 Dragomir Radev。2022。《Booksum：用于长篇叙事总结的数据集集合》。在 *计算语言学协会会议成果：EMNLP 2022*，第6536–6558页。
- en: Lee et al. (2023a) Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen White, and Sujay
    Jauhar. 2023a. [Making large language models better data creators](https://doi.org/10.18653/v1/2023.emnlp-main.948).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 15349–15360, Singapore. Association for Computational Linguistics.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2023a） Dong-Ho Lee、Jay Pujara、Mohit Sewak、Ryen White 和 Sujay Jauhar。2023a。《[让大型语言模型成为更好的数据创造者](https://doi.org/10.18653/v1/2023.emnlp-main.948)》。在
    *2023年自然语言处理实证方法会议论文集*，第15349–15360页，新加坡。计算语言学协会。
- en: 'Lee et al. (2023b) Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos,
    and Kangwook Lee. 2023b. [Prompted LLMs as chatbot modules for long open-domain
    conversation](https://doi.org/10.18653/v1/2023.findings-acl.277). In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 4536–4554,
    Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2023b） Gibbeum Lee、Volker Hartmann、Jongho Park、Dimitris Papailiopoulos
    和 Kangwook Lee。2023b。《[用提示的LLMs作为长开放领域对话的聊天模块](https://doi.org/10.18653/v1/2023.findings-acl.277)》。在
    *计算语言学协会会议成果：ACL 2023*，第4536–4554页，多伦多，加拿大。计算语言学协会。
- en: 'Lee et al. (2023c) Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jonghwan Hyeon,
    and Ho-Jin Choi. 2023c. Dialogcc: An automated pipeline for creating high-quality
    multi-modal dialogue datasets. In *NeurIPS 2023 Workshop on Instruction Tuning
    and Instruction Following*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2023c） Young-Jun Lee、Byungsoo Ko、Han-Gyu Kim、Jonghwan Hyeon 和 Ho-Jin
    Choi。2023c。《Dialogcc：创建高质量多模态对话数据集的自动化管道》。在 *NeurIPS 2023 指令调优与指令跟随研讨会*。
- en: 'Li et al. (2023a) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023a.
    Loogle: Can long-context language models understand long contexts? *arXiv preprint
    arXiv:2311.04939*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023a） Jiaqi Li、Mengmeng Wang、Zilong Zheng 和 Muhan Zhang。2023a。《Loogle：长期上下文语言模型能否理解长上下文？》*arXiv
    预印本 arXiv:2311.04939*。
- en: 'Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b.
    Blip-2: Bootstrapping language-image pre-training with frozen image encoders and
    large language models. In *International Conference on Machine Learning*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023b） Junnan Li、Dongxu Li、Silvio Savarese 和 Steven Hoi。2023b。《Blip-2：利用冻结图像编码器和大型语言模型的语言-图像预训练》。在
    *国际机器学习会议*。
- en: 'Li et al. (2017) Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and
    Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset.
    In *Proceedings of the Eighth International Joint Conference on Natural Language
    Processing (Volume 1: Long Papers)*, pages 986–995.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2017） Yanran Li、Hui Su、Xiaoyu Shen、Wenjie Li、Ziqiang Cao 和 Shuzi Niu。2017。《Dailydialog：一个手动标注的多轮对话数据集》。在
    *第八届国际联合自然语言处理会议（第一卷：长篇论文）*，第986–995页。
- en: Liang et al. (2023) Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao
    Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing infinite-length input capacity
    for large-scale language models with self-controlled memory system. *arXiv preprint
    arXiv:2304.13343*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2023） Xinnian Liang、Bing Wang、Hui Huang、Shuangzhi Wu、Peihao Wu、Lu Lu、Zejun
    Ma 和 Zhoujun Li。2023。《释放大规模语言模型的无限长度输入能力，采用自控记忆系统》。*arXiv 预印本 arXiv:2304.13343*。
- en: 'Lin (2004) Chin-Yew Lin. 2004. [ROUGE: A package for automatic evaluation of
    summaries](https://aclanthology.org/W04-1013). In *Text Summarization Branches
    Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004） Chin-Yew Lin。2004。《[ROUGE：用于自动评估摘要的包](https://aclanthology.org/W04-1013)》。在
    *文本总结的分支扩展*，第74–81页，巴塞罗那，西班牙。计算语言学协会。
- en: 'Lin et al. (2023) Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy
    Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen. 2023. [How to train your dragon:
    Diverse augmentation towards generalizable dense retrieval](https://doi.org/10.18653/v1/2023.findings-emnlp.423).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    6385–6400, Singapore. Association for Computational Linguistics.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人（2023） Sheng-Chieh Lin、Akari Asai、Minghan Li、Barlas Oguz、Jimmy Lin、Yashar
    Mehdad、Wen-tau Yih 和 Xilun Chen。2023。《[如何训练你的龙：面向可推广的密集检索的多样化增强](https://doi.org/10.18653/v1/2023.findings-emnlp.423)》。在
    *计算语言学协会会议成果：EMNLP 2023*，第6385–6400页，新加坡。计算语言学协会。
- en: 'Liu et al. (2023) Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. [Evaluating
    verifiability in generative search engines](https://doi.org/10.18653/v1/2023.findings-emnlp.467).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    7001–7025, Singapore. Association for Computational Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. [评估生成搜索引擎的可验证性](https://doi.org/10.18653/v1/2023.findings-emnlp.467)。在*计算语言学协会发现：EMNLP
    2023*，页面7001–7025，新加坡。计算语言学协会。
- en: 'Liu et al. (2024) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. [Lost in the Middle:
    How Language Models Use Long Contexts](https://doi.org/10.1162/tacl_a_00638).
    *Transactions of the Association for Computational Linguistics*, 12:157–173.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. 2024. [迷失在中间：语言模型如何使用长上下文](https://doi.org/10.1162/tacl_a_00638)。*计算语言学协会会刊*，12:157–173。
- en: 'Lu et al. (2023) Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He,
    Di Yin, Xing Sun, and Yunsheng Wu. 2023. Memochat: Tuning llms to use memos for
    consistent long-range open-domain conversation. *arXiv preprint arXiv:2308.08239*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu et al. (2023) Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He,
    Di Yin, Xing Sun, and Yunsheng Wu. 2023. Memochat: 调整大型语言模型以使用备忘录进行一致的长篇开放领域对话。*arXiv
    预印本 arXiv:2308.08239*。'
- en: 'Mallen et al. (2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel
    Khashabi, and Hannaneh Hajishirzi. 2023. [When not to trust language models: Investigating
    effectiveness of parametric and non-parametric memories](https://doi.org/10.18653/v1/2023.acl-long.546).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 9802–9822, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallen et al. (2023) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel
    Khashabi, and Hannaneh Hajishirzi. 2023. [何时不信任语言模型：研究参数和非参数记忆的有效性](https://doi.org/10.18653/v1/2023.acl-long.546)。在*第61届计算语言学协会年会（第1卷：长论文）论文集*，页面9802–9822，多伦多，加拿大。计算语言学协会。
- en: 'Meng et al. (2020) Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei
    Wu, Rui Yan, and Jiwei Li. 2020. Openvidial: A large-scale, open-domain dialogue
    dataset with visual contexts. *arXiv preprint arXiv:2012.15015*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng et al. (2020) Yuxian Meng, Shuhe Wang, Qinghong Han, Xiaofei Sun, Fei Wu,
    Rui Yan, and Jiwei Li. 2020. Openvidial：一个具有视觉上下文的大规模开放领域对话数据集。*arXiv 预印本 arXiv:2012.15015*。
- en: 'Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau
    Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. [FActScore:
    Fine-grained atomic evaluation of factual precision in long form text generation](https://doi.org/10.18653/v1/2023.emnlp-main.741).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 12076–12100, Singapore. Association for Computational Linguistics.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min et al. (2023) Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau
    Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. [FActScore：长文本生成中的事实精确度的细粒度原子评估](https://doi.org/10.18653/v1/2023.emnlp-main.741)。在*2023年自然语言处理实证方法会议论文集*，页面12076–12100，新加坡。计算语言学协会。
- en: 'Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Chris Brockett, Bill Dolan,
    Michel Galley, Jianfeng Gao, Georgios Spithourakis, and Lucy Vanderwende. 2017.
    [Image-grounded conversations: Multimodal context for natural question and response
    generation](https://aclanthology.org/I17-1047). In *Proceedings of the Eighth
    International Joint Conference on Natural Language Processing (Volume 1: Long
    Papers)*, pages 462–472, Taipei, Taiwan. Asian Federation of Natural Language
    Processing.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Chris Brockett, Bill Dolan,
    Michel Galley, Jianfeng Gao, Georgios Spithourakis, and Lucy Vanderwende. 2017.
    [图像基础对话：自然问题和回答生成的多模态上下文](https://aclanthology.org/I17-1047)。在*第八届国际联合自然语言处理大会（第1卷：长论文）论文集*，页面462–472，台北，台湾。亚洲自然语言处理联合会。
- en: 'Nie et al. (2021) Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, and
    Jason Weston. 2021. I like fish, especially dolphins: Addressing contradictions
    in dialogue modeling. In *Proceedings of the 59th Annual Meeting of the Association
    for Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, pages 1699–1713.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie et al. (2021) Yixin Nie, Mary Williamson, Mohit Bansal, Douwe Kiela, and
    Jason Weston. 2021. 我喜欢鱼，尤其是海豚：解决对话建模中的矛盾。在*第59届计算语言学协会年会和第11届国际联合自然语言处理大会（第1卷：长论文）论文集*，页面1699–1713。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu: a method for automatic evaluation of machine translation](https://doi.org/10.3115/1073083.1073135).
    In *Proceedings of the 40th Annual Meeting of the Association for Computational
    Linguistics*, pages 311–318, Philadelphia, Pennsylvania, USA. Association for
    Computational Linguistics.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni 等 (2002) Kishore Papineni、Salim Roukos、Todd Ward 和 Wei-Jing Zhu. 2002.
    [Bleu: 机器翻译自动评估方法](https://doi.org/10.3115/1073083.1073135)。见于 *第40届计算语言学协会年会论文集*，第
    311–318 页，费城，宾夕法尼亚州，美国。计算语言学协会。'
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://doi.org/10.1145/3586183.3606763). In *Proceedings
    of the 36th Annual ACM Symposium on User Interface Software and Technology*, UIST
    ’23, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2023) Joon Sung Park、Joseph O’Brien、Carrie Jun Cai、Meredith Ringel Morris、Percy
    Liang 和 Michael S. Bernstein. 2023. [生成代理：人类行为的互动模拟](https://doi.org/10.1145/3586183.3606763)。见于
    *第36届ACM用户界面软件与技术年会论文集*，UIST ’23，纽约，NY，美国。计算机协会。
- en: 'Pruitt and Grudin (2003) John Pruitt and Jonathan Grudin. 2003. Personas: practice
    and theory. In *Proceedings of the 2003 conference on Designing for user experiences*,
    pages 1–15.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pruitt 和 Grudin (2003) John Pruitt 和 Jonathan Grudin. 2003. 人物角色：实践与理论。见于 *2003年用户体验设计会议论文集*，第
    1–15 页。
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. [In-context retrieval-augmented
    language models](https://doi.org/10.1162/tacl_a_00605). *Transactions of the Association
    for Computational Linguistics*, 11:1316–1331.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram 等 (2023) Ori Ram、Yoav Levine、Itay Dalmedigos、Dor Muhlgay、Amnon Shashua、Kevin
    Leyton-Brown 和 Yoav Shoham. 2023. [上下文检索增强语言模型](https://doi.org/10.1162/tacl_a_00605)。*计算语言学协会会刊*，11:1316–1331。
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrieval-augmented
    black-box language models. *arXiv preprint arXiv:2301.12652*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等 (2023) Weijia Shi、Sewon Min、Michihiro Yasunaga、Minjoon Seo、Rich James、Mike
    Lewis、Luke Zettlemoyer 和 Wen-tau Yih. 2023. Replug: 检索增强的黑箱语言模型。*arXiv 预印本 arXiv:2301.12652*。'
- en: 'Shum et al. (2020) Michael Shum, Stephan Zheng, Wojciech Kryscinski, Caiming
    Xiong, and Richard Socher. 2020. [Sketch-fill-a-R: A persona-grounded chit-chat
    generation framework](https://doi.org/10.18653/v1/2020.nlp4convai-1.14). In *Proceedings
    of the 2nd Workshop on Natural Language Processing for Conversational AI*, pages
    118–131, Online. Association for Computational Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shum 等 (2020) Michael Shum、Stephan Zheng、Wojciech Kryscinski、Caiming Xiong
    和 Richard Socher. 2020. [Sketch-fill-a-R: 一种基于人物角色的闲聊生成框架](https://doi.org/10.18653/v1/2020.nlp4convai-1.14)。见于
    *第2届自然语言处理对话AI研讨会论文集*，第 118–131 页，在线。计算语言学协会。'
- en: 'Shuster et al. (2020) Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason
    Weston. 2020. [Image-chat: Engaging grounded conversations](https://doi.org/10.18653/v1/2020.acl-main.219).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 2414–2429, Online. Association for Computational Linguistics.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shuster 等 (2020) Kurt Shuster、Samuel Humeau、Antoine Bordes 和 Jason Weston.
    2020. [Image-chat: 参与性基础对话](https://doi.org/10.18653/v1/2020.acl-main.219)。见于
    *第58届计算语言学协会年会论文集*，第 2414–2429 页，在线。计算语言学协会。'
- en: 'Shuster et al. (2021) Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and
    Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation.
    In *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages
    3784–3803.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuster 等 (2021) Kurt Shuster、Spencer Poff、Moya Chen、Douwe Kiela 和 Jason Weston.
    2021. 检索增强减少对话中的幻觉。见于 *计算语言学协会发现：EMNLP 2021*，第 3784–3803 页。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等. 2023.
    Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Wang and Zhao (2023) Yuqing Wang and Yun Zhao. 2023. Tram: Benchmarking temporal
    reasoning for large language models. *arXiv preprint arXiv:2310.00835*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 和 Zhao (2023) Yuqing Wang 和 Yun Zhao. 2023. Tram: 大型语言模型的时间推理基准。*arXiv
    预印本 arXiv:2310.00835*。'
- en: Welleck et al. (2019) Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun
    Cho. 2019. [Dialogue natural language inference](https://doi.org/10.18653/v1/P19-1363).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3731–3741, Florence, Italy. Association for Computational
    Linguistics.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck等（2019）肖恩·韦莱克、贾森·韦斯顿、亚瑟·斯拉姆和庆贤·乔。2019年。[对话自然语言推理](https://doi.org/10.18653/v1/P19-1363)。载于*第57届计算语言学协会年会论文集*，第3731–3741页，意大利佛罗伦萨。计算语言学协会。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander Rush. 2020. [Transformers: State-of-the-art natural language processing](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 38–45, Online. Association for Computational
    Linguistics.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf等（2020）托马斯·沃尔夫、莉桑德·德布、维克托·桑赫、朱利安·肖蒙德、克莱门特·德朗格、安东尼·莫伊、皮埃里克·西斯塔克、蒂姆·劳尔特、雷米·卢夫、摩根·芬托维茨、乔·戴维森、萨姆·施莱弗、帕特里克·冯·普拉滕、克拉拉·马、雅辛·耶尔尼特、朱利安·普吕、陈文·徐、特文·勒·斯卡奥、希尔万·古格、玛丽亚马·德拉梅、昆廷·洛赫斯特和亚历山大·拉什。2020年。[Transformers:
    先进的自然语言处理技术](https://doi.org/10.18653/v1/2020.emnlp-demos.6)。载于*2020年自然语言处理实证方法会议：系统演示论文集*，第38–45页，在线。计算语言学协会。'
- en: Xiao et al. (2023) Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and
    Mike Lewis. 2023. Efficient streaming language models with attention sinks. *arXiv
    preprint arXiv:2309.17453*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao等（2023）光轩·肖、远东·田、贝迪·陈、宋·汉和迈克·刘易斯。2023年。具有注意力机制的高效流式语言模型。*arXiv预印本 arXiv:2309.17453*。
- en: 'Xu et al. (2023) Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023.
    [A critical evaluation of evaluations for long-form question answering](https://doi.org/10.18653/v1/2023.acl-long.181).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3225–3245, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2023）方远·徐、易晓·宋、莫希特·伊耶尔和恩索尔·崔。2023年。[对长篇问答评估的批判性评估](https://doi.org/10.18653/v1/2023.acl-long.181)。载于*第61届计算语言学协会年会论文集（第1卷：长篇论文）*，第3225–3245页，加拿大多伦多。计算语言学协会。
- en: 'Xu et al. (2022) Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish
    memory: Long-term open-domain conversation. In *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 5180–5197.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等（2022）静·徐、亚瑟·斯拉姆和贾森·韦斯顿。2022年。超越金鱼记忆：长期开放领域对话。载于*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，第5180–5197页。
- en: 'Zang et al. (2021) Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang,
    and Jindong Chen. 2021. [PhotoChat: A human-human dialogue dataset with photo
    sharing behavior for joint image-text modeling](https://doi.org/10.18653/v1/2021.acl-long.479).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 6142–6152, Online. Association for Computational
    Linguistics.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zang等（2021）晓雪·臧、丽娟·刘、玛丽亚·王、杨·宋、浩·张和金东·陈。2021年。[PhotoChat：一个包含照片共享行为的人际对话数据集，用于联合图像-文本建模](https://doi.org/10.18653/v1/2021.acl-long.479)。载于*第59届计算语言学协会年会与第11届国际联合自然语言处理会议论文集（第1卷：长篇论文）*，第6142–6152页，在线。计算语言学协会。
- en: 'Zhang et al. (2021a) Chen Zhang, Yiming Chen, Luis Fernando D’Haro, Yan Zhang,
    Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021a. Dynaeval: Unifying turn
    and dialogue level evaluation. In *Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics and the 11th International Joint Conference
    on Natural Language Processing (Volume 1: Long Papers)*, pages 5676–5689.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等（2021a）陈·张、依明·陈、路易斯·费尔南多·D’Haro、燕·张、托马斯·弗里德里希斯、格兰迪·李和海洲·李。2021a。Dynaeval：统一回合和对话级评估。载于*第59届计算语言学协会年会与第11届国际联合自然语言处理会议论文集（第1卷：长篇论文）*，第5676–5689页。
- en: 'Zhang et al. (2022) Chen Zhang, Luis Fernando D’Haro, Qiquan Zhang, Thomas
    Friedrichs, and Haizhou Li. 2022. Fined-eval: Fine-grained automatic dialogue-level
    evaluation. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*, pages 3336–3355.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2022）Chen Zhang、Luis Fernando D’Haro、Qiquan Zhang、Thomas Friedrichs
    和 Haizhou Li。2022。**Fined-eval: Fine-grained automatic dialogue-level evaluation**。在*2022年自然语言处理实证方法会议论文集*，第3336–3355页。'
- en: 'Zhang et al. (2023) Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. 2023.
    [Mind the gap between conversations for improved long-term dialogue generation](https://doi.org/10.18653/v1/2023.findings-emnlp.720).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    10735–10762, Singapore. Association for Computational Linguistics.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023）Qiang Zhang、Jason Naradowsky 和 Yusuke Miyao。2023。[关注对话间的差距以改进长期对话生成](https://doi.org/10.18653/v1/2023.findings-emnlp.720)。在*计算语言学协会：EMNLP
    2023 会议论文集*，第10735–10762页，新加坡。计算语言学协会。
- en: 'Zhang et al. (2021b) Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and Mohit
    Bansal. 2021b. Emailsum: Abstractive email thread summarization. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*, pages 6895–6909.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2021b）Shiyue Zhang、Asli Celikyilmaz、Jianfeng Gao 和 Mohit Bansal。2021b。**Emailsum:
    Abstractive email thread summarization**。在*第59届计算语言学协会年会和第11届国际联合自然语言处理会议（第一卷：长篇论文）*，第6895–6909页。'
- en: 'Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. In *International
    Conference on Learning Representations*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2019）Tianyi Zhang、Varsha Kishore、Felix Wu、Kilian Q Weinberger 和 Yoav
    Artzi。2019。**Bertscore: Evaluating text generation with bert**。在*国际学习表示会议*上发表。'
- en: 'Zheng et al. (2023) Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023. Minigpt-5:
    Interleaved vision-and-language generation via generative vokens. *arXiv preprint
    arXiv:2310.02239*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等人（2023）Kaizhi Zheng、Xuehai He 和 Xin Eric Wang。2023。**Minigpt-5: Interleaved
    vision-and-language generation via generative vokens**。*arXiv 预印本 arXiv:2310.02239*。'
- en: 'Zheng et al. (2022) Yinhe Zheng, Guanyi Chen, Xin Liu, and Jian Sun. 2022.
    [MMChat: Multi-modal chat dataset on social media](https://aclanthology.org/2022.lrec-1.621).
    In *Proceedings of the Thirteenth Language Resources and Evaluation Conference*,
    pages 5778–5786, Marseille, France. European Language Resources Association.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng 等人（2022）Yinhe Zheng、Guanyi Chen、Xin Liu 和 Jian Sun。2022。[MMChat: 社交媒体上的多模态聊天数据集](https://aclanthology.org/2022.lrec-1.621)。在*第十三届语言资源与评估会议论文集*，第5778–5786页，马赛，法国。欧洲语言资源协会。'
- en: 'Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang.
    2023. Memorybank: Enhancing large language models with long-term memory. *arXiv
    preprint arXiv:2305.10250*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等人（2023）Wanjun Zhong、Lianghong Guo、Qiqi Gao 和 Yanlin Wang。2023。**Memorybank:
    Enhancing large language models with long-term memory**。*arXiv 预印本 arXiv:2305.10250*。'
- en: Zhou et al. (2020) Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020.
    The design and implementation of xiaoice, an empathetic social chatbot. *Computational
    Linguistics*, 46(1):53–93.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2020）Li Zhou、Jianfeng Gao、Di Li 和 Heung-Yeung Shum。2020。**The design
    and implementation of xiaoice, an empathetic social chatbot**。*计算语言学*，46(1)：53–93。
- en: Appendix Overview
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录概述
- en: 'The appendix is organized as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 附录组织如下：
- en: 'Section A: Details of generative pipeline for the LoCoMo dataset.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: A节：LoCoMo数据集生成流程的详细信息。
- en: 'Section B: Statistics of LoCoMo dataset, license for data release and annotator
    details.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: B节：LoCoMo数据集的统计数据、数据发布许可和标注员详情。
- en: 'Section C: Experimental setup and implementation details.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: C节：实验设置和实施细节。
- en: 'Section D: Additional results from evaluation on the LoCoMo benchmark.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: D节：LoCoMo基准测试的额外结果。
- en: '![Refer to caption](img/a000a2aa854470a422ebba53cd119c04.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a000a2aa854470a422ebba53cd119c04.png)'
- en: 'Figure 5: Prompt for persona statement ($p$) for the virtual agents in our
    conversation generation pipeline (top) and select examples of persona statements
    present in the LoCoMo dataset.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：我们对话生成流程中的虚拟代理的个性声明（$p$）的提示（上图）以及LoCoMo数据集中存在的个性声明的选择示例。
- en: Appendix A Generative Pipeline for LoCoMo
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A LoCoMo的生成流程
- en: A.1 Persona
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 个性
- en: We assign unique persona statement $p$ persona summary as an in-context demonstration
    along with the prompt. A small selection of personas showcasing the diversity
    of speakers in the LoCoMo dataset is demonstrated in Fig. [5](#Ax1.F5 "Figure
    5 ‣ Appendix Overview ‣ Evaluating Very Long-Term Conversational Memory of LLM
    Agents").
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将唯一的角色声明$p$角色总结作为上下文演示，并附带提示。图[5](#Ax1.F5 "Figure 5 ‣ Appendix Overview ‣
    Evaluating Very Long-Term Conversational Memory of LLM Agents")展示了LoCoMo数据集中讲者的多样性的小部分角色样本。
- en: A.2 Temporal Event Graph
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 时间事件图
- en: '![Refer to caption](img/4b3af03c5e6d219db3aefcbd15712a51.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b3af03c5e6d219db3aefcbd15712a51.png)'
- en: 'Figure 6: Prompts for temporal event graph generation. The prompt used to generate
    complete personas for the LLMs in our conversation generation pipeline (top) and
    examples of personas present in the LoCoMo dataset.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：时间事件图生成提示。用于生成对话生成管道中LLMs完整角色的提示（顶部）以及LoCoMo数据集中存在的角色示例。
- en: '![Refer to caption](img/d9b28e23ed989fe782ebad4651973365.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9b28e23ed989fe782ebad4651973365.png)'
- en: 'Figure 7: Temporal Event Graph $\mathcal{G}$ between events are depicted to
    illustrate the casual relationships among them.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：展示了事件之间的时间事件图$\mathcal{G}$，以说明它们之间的因果关系。
- en: As outlined in Sec. [3.2](#S3.SS2 "3.2 Temporal Event Graph ‣ 3 Generative Pipeline
    for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"),
    we use an iterative process for generating event graphs consisting of causally
    connected events based on a given persona summary. The base prompt for describing
    the constitution of the event graph, the nature of events and causal connections
    between events is shown in Fig. [6](#A1.F6 "Figure 6 ‣ A.2 Temporal Event Graph
    ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents"). First, the base prompt is used along with the prompt for
    event graph initialization to generate three independent events relevant to a
    given personality. Then, the base prompt is combined with the prompt for the iterative
    generation of events to continue generating events that are caused by one or more
    of the events that are already present in the graph. See an example of a persona
    and the corresponding temporal event graph in Fig. [7](#A1.F7 "Figure 7 ‣ A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents"). In the example, Jack aspires
    to be a hotel manager. Consequently, he enrolls in a hotel management course in
    July, and after three months, he expresses his excitement about the course on
    social media. In a similar vein, his passion for gaming results in an invitation
    from a well-known gaming company.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[3.2](#S3.SS2 "3.2 Temporal Event Graph ‣ 3 Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")节所述，我们使用迭代过程生成由因果连接的事件组成的事件图，基于给定的角色总结。描述事件图构成、事件性质和事件之间因果连接的基本提示见图[6](#A1.F6
    "Figure 6 ‣ A.2 Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")。首先，使用基本提示和事件图初始化提示生成与给定个性相关的三个独立事件。然后，将基本提示与事件的迭代生成提示结合，继续生成由图中已有的一个或多个事件引起的事件。请参见图[7](#A1.F7
    "Figure 7 ‣ A.2 Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")中的角色示例及其对应的时间事件图。在该示例中，Jack希望成为酒店经理。因此，他在七月报名参加酒店管理课程，三个月后，他在社交媒体上表达了对课程的兴奋之情。类似地，他对游戏的热情导致他受到了知名游戏公司的邀请。
- en: A.2.1 Virtual Agent Architecture
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 虚拟代理架构
- en: As outlined in Section [3.3](#S3.SS3 "3.3 Virtual Agent Architecture ‣ 3 Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"),
    the virtual agents in our generative pipelines are composed of two mechanisms,
    Reflect & respond Park et al. ([2023](#bib.bib45)) and Image sharing & response.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[3.3](#S3.SS3 "3.3 Virtual Agent Architecture ‣ 3 Generative Pipeline for
    LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")节所述，我们生成管道中的虚拟代理由两种机制组成，即Reflect
    & respond Park等([2023](#bib.bib45))和Image sharing & response。
- en: Reflect & respond.
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Reflect & respond.
- en: This mechanism operates over a combination of short-term and long-term memory.
    The short-term memory is a summary of a session that is conditioned on the summary
    from a previous session. See the prompt given to LLMs in our pipeline for generating
    summaries, and an example of a generated summary, in Fig. [8](#A1.F8 "Figure 8
    ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2 Temporal
    Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents"). The long-term memory is a database of observations
    about each speaker, that are essentially assertive statements about the speaker’s
    persona and life. See the prompt given to LLMs in our pipeline for generating
    observations, and an example of observations extracted from a conversation, in
    Fig. [9](#A1.F9 "Figure 9 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture
    ‣ A.2 Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents"). In practice, the conversation
    is annotated with turn IDs for each turn, and the model is also instructed to
    indicate the turn IDs that directly contribute to each observation. This allows
    us to keep track of the evidence when using observations as the context for RAG-based
    models used in our experiments (see Section [5](#S5 "5 Experimental Setup ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该机制在短期记忆和长期记忆的组合下运作。短期记忆是对某次会话的总结，该总结以之前会话的总结为条件。有关生成总结的提示以及生成总结的示例，请参见图 [8](#A1.F8
    "Figure 8 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")。长期记忆是关于每个发言者的观察数据库，这些观察本质上是关于发言者的个性和生活的陈述。有关生成观察的提示以及从对话中提取的观察示例，请参见图 [9](#A1.F9
    "Figure 9 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")。实际上，对话会被标注上每轮的 ID，并且模型还会被指示标出直接贡献于每个观察的轮次
    ID。这使我们能够在使用观察作为我们实验中基于 RAG 的模型的上下文时跟踪证据（见第 [5](#S5 "5 Experimental Setup ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents)节")。
- en: Image sharing & response.
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像共享与回应。
- en: See prompts for implementing image-sharing and image-response behaviors in Figure [10](#A1.F10
    "Figure 10 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2
    Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents").
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见图 [10](#A1.F10 "Figure 10 ‣ Image sharing & response. ‣ A.2.1 Virtual Agent
    Architecture ‣ A.2 Temporal Event Graph ‣ Appendix A Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")中实施图像共享和图像回应行为的提示。
- en: '![Refer to caption](img/939177b3288a01e4d1d98206e7a5e2aa.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/939177b3288a01e4d1d98206e7a5e2aa.png)'
- en: 'Figure 8: Prompt for generating conversation summaries. The prompt used to
    iteratively generate a summary for the current session by conditioning on summary
    from preceding sessions and the raw conversation logs of the current session (top);
    and an example of inputs for the prompt and corresponding output summary of a
    session from the LoCoMo dataset.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：生成对话总结的提示。用于通过以先前会话的总结和当前会话的原始对话记录为条件，迭代生成当前会话总结的提示（顶部）；以及来自 LoCoMo 数据集中对话的提示输入和对应的输出总结示例。
- en: '![Refer to caption](img/3ed7562beb1e776002611a39a71b8503.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3ed7562beb1e776002611a39a71b8503.png)'
- en: 'Figure 9: Prompts for generating observations from conversations. The prompt
    used to generate observations from a conversation (top); and an example of inputs
    for the prompt and corresponding output observations for a session from the LoCoMo
    dataset.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：从对话中生成观察的提示。用于从对话中生成观察的提示（顶部）；以及从 LoCoMo 数据集中提取的对话示例的提示输入和对应的输出观察。
- en: '![Refer to caption](img/5d66a1eb67a473031bf90eb5c86ced6b.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d66a1eb67a473031bf90eb5c86ced6b.png)'
- en: 'Figure 10: Prompts for image-sharing and image-response behavior. The prompt
    used to convert a caption generated by the virtual agent into an image query for
    the web-based image crawler in our pipeline (top), and the prompt used to generate
    a response grounded in the image shared by a virtual agent during a conversation
    as well as the personas of the respective speakers (bottom).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：图像共享和图像回应行为的提示。用于将虚拟代理生成的标题转换为我们管道中基于网页的图像爬虫的图像查询的提示（顶部），以及用于生成基于虚拟代理在对话中共享的图像以及相关发言者的个性的回应的提示（底部）。
- en: A.3 Human Filtering
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 人工筛选
- en: '![Refer to caption](img/6c3543fe536dfa9eba8b6af4c4471771.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6c3543fe536dfa9eba8b6af4c4471771.png)'
- en: 'Figure 11: Example of edits made by annotators. Human annotators are instructed
    to make edits in the LLM-generated conversations to remove irrelevant The prompt
    used to generate complete personas for the LLMs in our conversation generation
    pipeline (top) and examples of personas present in the LoCoMo dataset.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：注释员所做编辑的示例。人类注释员被指示对LLM生成的对话进行编辑，以移除无关内容。用于生成完整角色的提示在我们的对话生成管道中（上图）以及LoCoMo数据集中存在的角色示例。
- en: 'Human annotators are instructed to edit the LLM-generated conversations in
    the following scenarios:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 人类注释员被指示在以下情况下编辑LLM生成的对话：
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Remove an image if it is not relevant to the current dialog or the conversation.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果图像与当前对话或对话无关，则将其移除。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Add context about an image to the current speaker’s dialog if it is not discussed
    by them but the subsequent speaker has reacted to the image.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果当前发言者未讨论图像，但后续发言者对图像作出了反应，则为图像添加上下文到当前发言者的对话中。
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Replace an image if it does not match the caption that was used to query for
    images.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果图像与用于查询图像的标题不匹配，则替换图像。
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Edit the dialog when the information present in the dialog is inconsistent with
    something said (or shared through an image) in earlier or later turns.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当对话中的信息与之前或之后回合中说的（或通过图像共享的）内容不一致时，编辑对话。
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Edit the dialog to ensure that the details in the conversation are consistent
    with those given in the event for the session.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编辑对话以确保对话中的细节与会话事件中的细节一致。
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Remove any events from the event graph if they do not appear in the conversation.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果事件图中的事件未出现在对话中，则将其移除。
- en: See an example of some edits in Fig. [11](#A1.F11 "Figure 11 ‣ A.3 Human Filtering
    ‣ Appendix A Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational
    Memory of LLM Agents").
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见图[11](#A1.F11 "Figure 11 ‣ A.3 Human Filtering ‣ Appendix A Generative Pipeline
    for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")中的一些编辑示例。
- en: Appendix B Dataset
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 数据集
- en: B.1 Dataset Statistics
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 数据集统计
- en: See a breakdown of the statistics of the conversations in the LoCoMo dataset
    in the top panel of Table [5](#A2.T5 "Table 5 ‣ B.1 Dataset Statistics ‣ Appendix
    B Dataset ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"). Also,
    see a breakdown of the statistics of the annotations in the evaluation benchmark
    in the bottom panel of Table [5](#A2.T5 "Table 5 ‣ B.1 Dataset Statistics ‣ Appendix
    B Dataset ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents").
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看表[5](#A2.T5 "Table 5 ‣ B.1 Dataset Statistics ‣ Appendix B Dataset ‣ Evaluating
    Very Long-Term Conversational Memory of LLM Agents")顶部面板中的LoCoMo数据集中对话统计数据的详细信息。此外，请查看表[5](#A2.T5
    "Table 5 ‣ B.1 Dataset Statistics ‣ Appendix B Dataset ‣ Evaluating Very Long-Term
    Conversational Memory of LLM Agents")底部面板中的评估基准中的注释统计数据的详细信息。
- en: '| Conversation Statistics | # Counts |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 对话统计 | 数量 |'
- en: '| --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Total. # conversations $h$. | 50 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 总对话数 $h$ | 50 |'
- en: '| Avg. # sessions $k$ | 19.3 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 平均会话数 $k$ | 19.3 |'
- en: '| Avg. # turns $j$ | 15.8 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 平均回合数 $j$ | 15.8 |'
- en: '| Avg. # tokens. conversation $h$ | 9,209.2 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 平均对话中标记数 $h$ | 9,209.2 |'
- en: '| Avg. # tokens. dialogue $h_{k_{j}}$ | 30.2 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 平均对话标记数 $h_{k_{j}}$ | 30.2 |'
- en: '| Avg. # tokens. observation $o_{k_{j}}$ | 18.2 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 平均观测标记数 $o_{k_{j}}$ | 18.2 |'
- en: '| Avg. # tokens. summary $w_{k}$ | 127.4 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 平均总结标记数 $w_{k}$ | 127.4 |'
- en: '| QA Benchmark Statistics |  |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| QA基准统计 |  |'
- en: '| # questions. single-hop retrieval | 2,705 (36%) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 单跳检索问题数 | 2,705 (36%) |'
- en: '| # questions. multi-hop retrieval | 1,104 (14.6%) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 多跳检索问题数 | 1,104 (14.6%) |'
- en: '| # questions. temporal reasoning | 1,547 (20.6%) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 临时推理问题数 | 1,547 (20.6%) |'
- en: '| # questions. open domain knowledge | 285 (3.9%) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 开放领域知识问题数 | 285 (3.9%) |'
- en: '| # questions. adversarial | 1,871 (24.9%) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性问题数 | 1,871 (24.9%) |'
- en: '| Total. # questions. | 7,512 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 总问题数 | 7,512 |'
- en: '| Event Summarization Statistics |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 事件总结统计 |  |'
- en: '| Avg. # ground truth events. in conversation $h$ | 24.2 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 平均对话中真实事件数 $h$ | 24.2 |'
- en: '| Avg. # tokens. event summary | 896.5 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 平均事件总结标记数 | 896.5 |'
- en: '| Multi-modal Dialogue Generation Statistics |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 多模态对话生成统计 |  |'
- en: '| Avg. # images. in conversation $h$ | 32.3 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 平均对话中图像数 $h$ | 32.3 |'
- en: 'Table 5: Dataset Statistics of conversation and corresponding benchmark'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：对话数据集统计及相应基准
- en: B.2 Dataset License
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 数据集许可
- en: The LoCoMo dataset will be released under the CC BY-NC 4.0 DEED license.⁸⁸8[https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: LoCoMo 数据集将根据 CC BY-NC 4.0 DEED 许可证发布。⁸⁸8[https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)
- en: B.3 Annotator Details
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 注释者详情
- en: The annotators who worked on the LoCoMo dataset were in-house annotators and
    we were unable to obtain their demographics due to the confidential nature of
    such information.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 从事 LoCoMo 数据集工作的注释者是内部注释者，由于信息的机密性，我们无法获得他们的人口统计数据。
- en: Appendix C Experimental Setup
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 实验设置
- en: C.1 Baselines
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 基准
- en: The conversations in the LoCoMo dataset are composed of natural language dialogs
    and images that require higher-order reasoning and multimodal coreference resolution,
    respectively. From initial studies, we observed that multimodal coreference resolution
    can be performed effectively by replacing images in LoCoMo with their captions
    generated using BLIP-2 Li et al. ([2023b](#bib.bib31)), and using state-of-art
    LLMs to reason over natural language text interleaved with image captions. Hence,
    our experiments for the question answering and event summarization tasks are conducted
    using LLMs. We use the images directly only for experiments on the multimodal
    dialog generation task.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: LoCoMo 数据集中的对话由自然语言对话和需要更高阶推理和多模态共指解析的图像组成。从初步研究中，我们观察到，通过用 BLIP-2 Li 等人 ([2023b](#bib.bib31))
    生成的标题替换 LoCoMo 中的图像，可以有效地进行多模态共指解析，并使用最先进的 LLM 对自然语言文本和图像标题进行推理。因此，我们的问答和事件摘要任务实验是使用
    LLM 进行的。我们仅在多模态对话生成任务的实验中直接使用图像。
- en: Question Answering.
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问答。
- en: 'We carry out experiments using three distinct methodologies: (1) Base involves
    utilizing LLMs to directly conduct the task within a constrained context. The
    task description comes after the dialogue history. To accommodate the restricted
    context window size, earlier dialogues are omitted; (2) Long-context employs LLMs
    with an extended context window to expose the models to as much dialogue context
    as possible; (3) Retrieval-augmented Generation (RAG) involves retrieving relevant
    context from a database of dialog history, observations, or session-level summaries.
    Observations are assertions about each speaker extracted from the dialog history
    as described in §[3.3](#S3.SS3 "3.3 Virtual Agent Architecture ‣ 3 Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"),
    see an example in Figure [9](#A1.F9 "Figure 9 ‣ Image sharing & response. ‣ A.2.1
    Virtual Agent Architecture ‣ A.2 Temporal Event Graph ‣ Appendix A Generative
    Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents").
    Session-level summaries are concise summaries of the conversation that takes place
    in each session, see an example in Figure [8](#A1.F8 "Figure 8 ‣ Image sharing
    & response. ‣ A.2.1 Virtual Agent Architecture ‣ A.2 Temporal Event Graph ‣ Appendix
    A Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory
    of LLM Agents").'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用三种不同的方法进行实验：（1）基础方法涉及利用 LLM 直接在有限的上下文中进行任务。任务描述在对话历史之后出现。为了适应受限的上下文窗口大小，早期对话被省略；（2）长上下文方法使用具有扩展上下文窗口的
    LLM，以使模型能够接触到尽可能多的对话上下文；（3）检索增强生成（RAG）方法涉及从对话历史、观察或会话级摘要的数据库中检索相关上下文。观察是关于每个发言者的陈述，这些陈述从对话历史中提取，如
    §[3.3](#S3.SS3 "3.3 虚拟代理架构 ‣ LoCoMo 的生成管道 ‣ 评估 LLM 代理的长期对话记忆") 中所述，见图 [9](#A1.F9
    "图 9 ‣ 图像共享与响应 ‣ A.2.1 虚拟代理架构 ‣ A.2 时间事件图 ‣ 附录 A LoCoMo 的生成管道 ‣ 评估 LLM 代理的长期对话记忆")
    中的示例。会话级摘要是对每个会话中发生的对话的简明总结，见图 [8](#A1.F8 "图 8 ‣ 图像共享与响应 ‣ A.2.1 虚拟代理架构 ‣ A.2
    时间事件图 ‣ 附录 A LoCoMo 的生成管道 ‣ 评估 LLM 代理的长期对话记忆") 中的示例。
- en: For the retrieval model, we employ DRAGON Lin et al. ([2023](#bib.bib35)). In
    the Base, we utilize Mistral-7B Jiang et al. ([2023](#bib.bib22)), LLama-70B-chat Touvron
    et al. ([2023](#bib.bib52)), gpt-3.5-turbo ⁹⁹9https://platform.openai.com/docs/models/gpt-3-5,
    and gpt-4-turbo ^(10)^(10)10https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo.
    To assess the effectiveness in practical scenarios for Long-context and RAG, we
    draw comparisons using variants of gpt-3.5-turbo. We do not report the performance
    of long-context fine-tuned open-source models Chen et al. ([2023](#bib.bib8))
    or those utilizing sliding window Bertsch et al. ([2024](#bib.bib5)); Dao et al.
    ([2022](#bib.bib10)) due to the variability inherent across different open-source
    models and the potential reduction in their capability on shorter context.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 对于检索模型，我们使用了DRAGON Lin et al. ([2023](#bib.bib35))。在Base配置中，我们利用了Mistral-7B
    Jiang et al. ([2023](#bib.bib22))、LLama-70B-chat Touvron et al. ([2023](#bib.bib52))、gpt-3.5-turbo ⁹⁹9https://platform.openai.com/docs/models/gpt-3-5
    和 gpt-4-turbo ^(10)^(10)10https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo。为了评估在实际场景中的有效性，我们通过gpt-3.5-turbo的变体进行比较。我们没有报告长上下文微调的开源模型
    Chen et al. ([2023](#bib.bib8)) 或那些使用滑动窗口 Bertsch et al. ([2024](#bib.bib5));
    Dao et al. ([2022](#bib.bib10)) 的性能，因为不同开源模型之间的变异性以及它们在较短上下文中的能力可能会降低。
- en: Event Summarization.
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事件摘要。
- en: We present experiments conducted in two distinct configurations. We use both
    the Base and Long-context setups from the question answering task, but we refrained
    from including RAG since summarization requires a comprehensive understanding
    of the entire dialogue, rather than just retrieving a specific portion. A notable
    distinction in our approach, compared to the question-answering task, lies in
    our handling of the context. Specifically, we employ an iterative process of creating
    a summary of a preceding session and then use that summary as a basis to generate
    the summary for the subsequent session Chang et al. ([2023](#bib.bib6)). Further,
    we use a single in-context demonstration of input and output to guide the model
    toward selecting only significant life events for the summary.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在两种不同配置下进行的实验。我们使用了来自问题回答任务的基本和长上下文设置，但由于摘要需要对整个对话有全面理解，而不仅仅是检索特定部分，我们避免了使用RAG。与问题回答任务相比，我们的方法在处理上下文方面有显著区别。具体来说，我们采用了一个迭代过程，先创建前一个会话的摘要，然后将该摘要作为基础来生成后续会话的摘要
    Chang et al. ([2023](#bib.bib6))。此外，我们使用了一个单一的上下文示例来引导模型仅选择重要的生活事件进行摘要。
- en: Multi-modal Dialogue Generation.
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态对话生成。
- en: 'For evaluating multi-modal dialogue generation, we train MiniGPT-5 Zheng et al.
    ([2023](#bib.bib65)) on 50 conversations generated using our automated pipeline
    (without human filtering) as detailed in §[3](#S3 "3 Generative Pipeline for LoCoMo
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"). Three distinct
    versions of the model were developed, each with varying training data: (1) Base
    trains on preceding dialogue turns; (2) + summary trains on both prior dialogue
    turns and a global summary of the ongoing conversation; (3) + observation trains
    on both preceding dialogue turns and relevant observations retrieved from the
    conversation history. For each of these models, we started with a MiniGPT-5 checkpoint
    pretrained on the MMDialog dataset Feng et al. ([2023](#bib.bib12)).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多模态对话生成的评估，我们训练了MiniGPT-5 Zheng et al. ([2023](#bib.bib65))，使用我们自动化管道生成的50个对话（未经过人工筛选），如§[3](#S3
    "3 Generative Pipeline for LoCoMo ‣ Evaluating Very Long-Term Conversational Memory
    of LLM Agents")中详细说明。开发了三种不同版本的模型，每种模型的训练数据各异：（1）Base基于前面的对话轮次进行训练；（2）+ summary基于之前的对话轮次和正在进行对话的全局摘要进行训练；（3）+
    observation基于前面的对话轮次和从对话历史中检索的相关观察进行训练。对于这些模型中的每一个，我们都从MiniGPT-5检查点开始，该检查点在MMDialog数据集上进行了预训练
    Feng et al. ([2023](#bib.bib12))。
- en: C.2 Implementation Details
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 实施细节
- en: We use OpenAI API and Huggingface Wolf et al. ([2020](#bib.bib55)), as of January
    2024, with specific settings of $temperature$ set to 1 for evaluation of the LoCoMo
    benchmark. All experiments, including those for RAG-based models, MiniGPT-5 training,
    and inference, are conducted on an Nvidia A6000 server with FP32\. We report results
    from a single inference run for each model in our experiments. For MiniGPT-5,
    we used the hyperparameters recommended in the original codebase and trained our
    models for 10 epochs, which took approximately 30 hours on a single A6000 GPU.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 OpenAI API 和 Huggingface Wolf 等人 ([2020](#bib.bib55))，截至2024年1月，具体设置为
    $temperature$ 设定为 1 以评估 LoCoMo 基准。所有实验，包括基于 RAG 的模型、MiniGPT-5 训练和推理，都在带有 FP32
    的 Nvidia A6000 服务器上进行。我们报告了每个模型在实验中的单次推理结果。对于 MiniGPT-5，我们使用了原始代码库中推荐的超参数，并将模型训练了
    10 个周期，这在单个 A6000 GPU 上大约需要 30 小时。
- en: We use the default implementations of BLEU^(11)^(11)11[https://www.nltk.org/_modules/nltk/translate/bleu_score.html](https://www.nltk.org/_modules/nltk/translate/bleu_score.html),
    ROUGE^(12)^(12)12[https://pypi.org/project/rouge/](https://pypi.org/project/rouge/),
    BertScore^(13)^(13)13[https://pypi.org/project/bert-score/](https://pypi.org/project/bert-score/),
    FactScore^(14)^(14)14[https://github.com/shmsw25/FActScore](https://github.com/shmsw25/FActScore)
    metrics in their respective Python packages in our evaluation protocol.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估协议中使用了 BLEU^(11)^(11)11[https://www.nltk.org/_modules/nltk/translate/bleu_score.html](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)、ROUGE^(12)^(12)12[https://pypi.org/project/rouge/](https://pypi.org/project/rouge/)、BertScore^(13)^(13)13[https://pypi.org/project/bert-score/](https://pypi.org/project/bert-score/)
    和 FactScore^(14)^(14)14[https://github.com/shmsw25/FActScore](https://github.com/shmsw25/FActScore)
    指标的默认实现。
- en: Appendix D Results
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 结果
- en: '| Category | top-$k$ | BLEU-1/2 | Rouge-L | MM-R |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | top-$k$ | BLEU-1/2 | Rouge-L | MM-R |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Base | - | 57.1 / 34.2 | 12.4 | 56.1 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 基本 | - | 57.1 / 34.2 | 12.4 | 56.1 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| + summary | 1 | 58.2 / 34.1 | 12.8 | 56.9 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| + 总结 | 1 | 58.2 / 34.1 | 12.8 | 56.9 |'
- en: '| + summary | 2 | 56.5 / 32.8 | 12.1 | 55.1 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| + 总结 | 2 | 56.5 / 32.8 | 12.1 | 55.1 |'
- en: '| + summary | 5 | 56.1 / 32.5 | 12.0 | 55.2 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| + 总结 | 5 | 56.1 / 32.5 | 12.0 | 55.2 |'
- en: '| + observation | 5 | 59.7 / 35.1 | 13.6 | 57.8 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| + 观察 | 5 | 59.7 / 35.1 | 13.6 | 57.8 |'
- en: '| + observation | 10 | 59.1 / 34.9 | 12.8 | 57.1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| + 观察 | 10 | 59.1 / 34.9 | 12.8 | 57.1 |'
- en: '| + observation | 25 | 58.5 / 34.2 | 12.0 | 56.5 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| + 观察 | 25 | 58.5 / 34.2 | 12.0 | 56.5 |'
- en: 'Table 6: Multi-modal dialogue generation performance comparison between different
    training variants of MiniGPT-5. The optimal performance is shown in bold.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: MiniGPT-5 不同训练变体的多模态对话生成性能比较。最佳性能以**粗体**显示。'
- en: D.1 Event Summarization Task
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 事件总结任务
- en: See an example of the five broad categories of event summarization errors made
    by LLMs, outlined in Section [6.2](#S6.SS2 "6.2 Event Summarization Task ‣ 6 Experimental
    Results ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents"), in
    Table [7](#A4.T7 "Table 7 ‣ D.1 Event Summarization Task ‣ Appendix D Results
    ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents").
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 LLMs 在事件总结错误的五大类别示例，详见第[6.2节](#S6.SS2 "6.2 事件总结任务 ‣ 6 实验结果 ‣ 评估 LLM 代理的长期对话记忆")，表格[7](#A4.T7
    "表 7 ‣ D.1 事件总结任务 ‣ 附录 D 结果 ‣ 评估 LLM 代理的长期对话记忆")。
- en: '| Error Type | Explanation | Ground truth event or relevant dialogs | Predicted
    event |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 错误类型 | 解释 | 真实事件或相关对话 | 预测事件 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Missing information | Key details about event are omitted because the model
    fails to make causal and temporal connections over a long conversation. | Joanna
    submits her third screenplay on loss, identity, and connection to a film contest
    | Joanna submits her recent screenplay to a film contest. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 信息缺失 | 事件的关键细节被省略，因为模型无法在长对话中建立因果和时间上的联系。 | Joanna 提交了她第三部关于失落、身份和联系的剧本到电影比赛
    | Joanna 提交了她最近的剧本到电影比赛。 |'
- en: '| Hallucination | Non-existent details or details from a different event are
    padded onto an event | N: ‘The gaming party was a great success!’ N: ‘… said they’d
    want to do it again next month!’'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '| 幻觉 | 事件中填充了不存在的细节或来自不同事件的细节 | N: ‘游戏派对非常成功！’ N: ‘……说他们想下个月再做一次！’'
- en: 'N: ‘On another note, I made vegan ice cream …’ | Nate’s vegan ice cream is
    a huge success and people want to do it again next month. |'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 'N: ‘另一个话题，我做了素食冰淇淋……’ | Nate 的素食冰淇淋大获成功，人们希望下个月再做一次。 |'
- en: '| Misunder- -standing of dialog cues | e.g., model confuses a light-hearted
    statement from a speaker as a serious statement | J: ‘.. these trails that made
    me feel like writing a drama.’ N: ‘.. go together .. Maybe I’ll start to think
    of a drama myself and write a screenplay …’'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '| 对话提示的误解 | 例如，模型将演讲者的轻松声明误认为是严肃声明 | J: ‘..这些使我想写一部戏剧的经历。’ N: ‘..一起吧..也许我会开始考虑写一部戏剧，编写剧本…’'
- en: 'J: ‘Haha, now that would be something! …’ | Nate considers writing his own
    drama screenplay. |'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 'J: ‘哈哈，那可真是太棒了！…’ | Nate考虑自己编写戏剧剧本。 |'
- en: '| Speaker attribution | Event is attributed to the wrong speaker | Nate invites
    Joanna to try his homemade lactose-free ice cream. | Joanna invites Nate to her
    home to try her dairy-free ice cream recipe. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 说话者归属 | 事件被归因于错误的说话者 | Nate邀请Joanna尝试他自制的无乳糖冰淇淋。 | Joanna邀请Nate到她家尝试她的无乳制品冰淇淋食谱。
    |'
- en: '| Saliency | Unimportant interactions in the conversation are considered significant
    by model | N: Hey Joanna, what’s been up since we last chatted? How’s it going?
    | Nate asks Joanna how she has been she they last talked. |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 重要性 | 模型将对话中的不重要互动视为重要 | N: 嗨，Joanna，自从我们上次聊天以来怎么样？ | Nate问Joanna自上次聊天以来她怎么样。
    |'
- en: 'Table 7: Taxonomy of errors in LLM-generated event summaries. Five types of
    errors predominantly occur in the event summaries generated by LLMs. Examples
    are based on predictions from gpt-3.5-turbo.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：LLM生成的事件摘要中的错误分类。LLM生成的事件摘要中主要出现五种类型的错误。示例基于gpt-3.5-turbo的预测。
- en: D.2 Multimodal Dialog Generation Task
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 多模态对话生成任务
- en: Results from evaluation of various version of MiniGPT-5 model on the multimodal
    dialog generation task in the LoCoMo benchmark is in Table [6](#A4.T6 "Table 6
    ‣ Appendix D Results ‣ Evaluating Very Long-Term Conversational Memory of LLM
    Agents").
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 各种版本MiniGPT-5模型在LoCoMo基准测试中的多模态对话生成任务的评估结果见表[6](#A4.T6 "Table 6 ‣ Appendix D
    Results ‣ Evaluating Very Long-Term Conversational Memory of LLM Agents")。
