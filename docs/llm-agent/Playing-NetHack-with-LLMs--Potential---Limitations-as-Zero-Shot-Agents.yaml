- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:51:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:14'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '使用LLMs玩NetHack: 零样本代理的潜力与局限'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00690](https://ar5iv.labs.arxiv.org/html/2403.00690)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00690](https://ar5iv.labs.arxiv.org/html/2403.00690)
- en: Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow Queen Mary University of
    London
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow 伦敦大学玛丽女王学院
- en: '{d.jeurissen, diego.perez, jeremy.gow}@qmul.ac.uk    Duygu Çakmak, James Kwan
    Creative Assembly'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '{d.jeurissen, diego.perez, jeremy.gow}@qmul.ac.uk    Duygu Çakmak, James Kwan
    Creative Assembly'
- en: '{duygu.cakmak, james.kwan}@creative-assembly.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{duygu.cakmak, james.kwan}@creative-assembly.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have shown great success as high-level planners
    for zero-shot game-playing agents. However, these agents are primarily evaluated
    on Minecraft, where long-term planning is relatively straightforward. In contrast,
    agents tested in dynamic robot environments face limitations due to simplistic
    environments with only a few objects and interactions. To fill this gap in the
    literature, we present NetPlay, the first LLM-powered zero-shot agent for the
    challenging roguelike NetHack. NetHack is a particularly challenging environment
    due to its diverse set of items and monsters, complex interactions, and many ways
    to die.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）作为高层次规划者在零样本游戏代理中表现出色。然而，这些代理主要在Minecraft上进行评估，在Minecraft中长期规划相对简单。相比之下，在动态机器人环境中测试的代理由于环境简化且只有少量物体和交互而面临局限。为了填补文献中的这一空白，我们提出了NetPlay，首个为具有挑战性的roguelike游戏NetHack提供LLM支持的零样本代理。NetHack是一个特别具有挑战性的环境，因为它有多种物品和怪物、复杂的交互以及许多死亡方式。
- en: NetPlay uses an architecture designed for dynamic robot environments, modified
    for NetHack. Like previous approaches, it prompts the LLM to choose from predefined
    skills and tracks past interactions to enhance decision-making. Given NetHack’s
    unpredictable nature, NetPlay detects important game events to interrupt running
    skills, enabling it to react to unforeseen circumstances. While NetPlay demonstrates
    considerable flexibility and proficiency in interacting with NetHack’s mechanics,
    it struggles with ambiguous task descriptions and a lack of explicit feedback.
    Our findings demonstrate that NetPlay performs best with detailed context information,
    indicating the necessity for dynamic methods in supplying context information
    for complex games such as NetHack.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: NetPlay使用了为动态机器人环境设计的架构，并针对NetHack进行了修改。像以前的方法一样，它提示LLM从预定义的技能中进行选择，并跟踪过去的互动以增强决策能力。鉴于NetHack的不确定性，NetPlay会检测重要的游戏事件以中断正在运行的技能，使其能够应对意外情况。虽然NetPlay在与NetHack机制互动方面展示了相当大的灵活性和熟练度，但在处理模糊的任务描述和缺乏明确反馈方面存在困难。我们的研究结果表明，NetPlay在拥有详细上下文信息时表现最佳，这表明在复杂游戏如NetHack中，动态提供上下文信息的方法是必要的。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: NetHack, Large Language Models, Zero-Shot Agent.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack, 大型语言模型, 零样本代理。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Recently, agents based on Large Language Models (LLMs) [[1](#bib.bib1)] have
    been successfully applied to robot environments [[2](#bib.bib2), [3](#bib.bib3)]
    and Minecraft [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)], among others.
    These agents do not require pre-training and typically involve prompting an LLM
    to solve tasks by choosing from predefined skills. They have proven effective
    for tasks demanding extensive knowledge, like crafting a diamond pickaxe in Minecraft.
    Additionally, they can understand a wide range of task descriptions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大型语言模型（LLMs）的代理 [[1](#bib.bib1)] 已被成功应用于机器人环境 [[2](#bib.bib2), [3](#bib.bib3)]
    和Minecraft [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] 等领域。这些代理不需要预训练，通常涉及提示LLM通过选择预定义的技能来解决任务。它们在需要广泛知识的任务中已被证明有效，例如在Minecraft中制作钻石镐。此外，它们能够理解各种任务描述。
- en: LLM agents utilizing predefined skills are particularly promising for game development
    as developing a set of simple skills is often more feasible than designing an
    entire agent. However, existing studies predominantly focus on the capabilities
    of LLMs for game-playing, neglecting to address their limitations. Evaluations
    typically focus on predictable tasks, for example, finding a diamond in Minecraft,
    which can consistently be achieved through strip mining. Many games require more
    dynamic decision-making, where long-term planning is challenging, and the correct
    course of action is more ambiguous. While evaluations have been done on more dynamic
    robot environments, these environments often contain only a handful of objects
    and lack complex interactions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 利用预定义技能的LLM代理在游戏开发中尤为有前景，因为开发一套简单的技能往往比设计一个完整的代理更为可行。然而，现有研究主要集中在LLM在游戏中的能力上，忽略了它们的局限性。评估通常集中在可预测的任务上，例如在Minecraft中找到钻石，这可以通过带状采矿来稳定实现。许多游戏需要更动态的决策制定，其中长期规划具有挑战性，正确的行动路线更为模糊。尽管已有对更动态的机器人环境进行评估，但这些环境通常仅包含少量对象且缺乏复杂的互动。
- en: We build upon existing literature by evaluating an LLM agent in the context
    of the complex and unpredictable roguelike NetHack [[7](#bib.bib7)]. NetHack is
    a challenging game with many monsters, items, interactions, partial observability,
    and an intricate goal condition. The sheer size of NetHack, paired with the many
    sub-systems the player has to understand, make it an excellent candidate for evaluating
    the limitations of LLM agents. NetHack’s description files also allow us to define
    levels, enabling us to evaluate the agent’s abilities in isolation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在复杂且不可预测的roguelike游戏NetHack [[7](#bib.bib7)] 的背景下评估LLM代理，建立在现有文献的基础上。NetHack是一款具有许多怪物、物品、互动、部分可观察性和复杂目标条件的挑战性游戏。NetHack的庞大规模，加上玩家必须理解的多个子系统，使其成为评估LLM代理局限性的绝佳候选者。NetHack的描述文件还允许我们定义关卡，使我们能够在隔离的情况下评估代理的能力。
- en: In the following, we present (NetPlay), a GPT-4 powered agent designed to tackle
    a wide range of tasks in NetHack. NetPlay is inspired by autoascend [[8](#bib.bib8)]
    a handcrafted agent that won the NetHack Challenge 2021 [[9](#bib.bib9)]. While
    autoascend relied on a large network of handcrafted rules to handle the complexity
    of NetHack, NetPlay only requires a set of isolated skills. Our experiments show
    that NetPlay can interact with most of NetHack’s game mechanics and that it excels
    in following detailed instructions. Additionally, the agent exhibits creative
    behavior when focusing its attention on a specific problem. However, when tasked
    to play autonomously, NetPlay is far outperformed by autoascend. Consequently,
    this paper delves into reasons for this, such as the agent’s struggles to handle
    ambiguous instructions, confusing observations, and a lack of explicit feedback.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了(NetPlay)，一个基于GPT-4的代理，旨在应对NetHack中的广泛任务。NetPlay的灵感来源于autoascend [[8](#bib.bib8)]，这是一个手工制作的代理，赢得了2021年NetHack挑战赛
    [[9](#bib.bib9)]。虽然autoascend依赖于大量手工制作的规则来处理NetHack的复杂性，但NetPlay只需要一套孤立的技能。我们的实验表明，NetPlay可以与NetHack的大多数游戏机制互动，并且在执行详细指令方面表现出色。此外，该代理在将注意力集中于特定问题时表现出创造性行为。然而，当被要求自主游戏时，NetPlay远远不如autoascend。因此，本文深入探讨了原因，例如代理处理模糊指令、混乱观察和缺乏明确反馈的困难。
- en: 'We begin in [section II](#S2 "II Background ‣ Playing NetHack with LLMs: Potential
    & Limitations as Zero-Shot Agents") with an overview of NetHack and a review of
    existing work on LLM-powered agents. [Section III](#S3 "III NetPlay ‣ Playing
    NetHack with LLMs: Potential & Limitations as Zero-Shot Agents") discusses the
    architecture of NetPlay, including many of the design decisions we had to make
    due to limitations caused by the LLM. In [section IV](#S4 "IV Experiments ‣ Playing
    NetHack with LLMs: Potential & Limitations as Zero-Shot Agents"), we first evaluate
    NetPlay’s ability to autonomously play the game and compare its performance with
    a simple handcrafted agent and autoascend. We follow this up with an in-depth
    analysis of the agent’s behavior across various isolated scenarios. Subsequently,
    we analyze the experiment results in [section V](#S5 "V Potential and Limitations
    ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents") and
    conclude this study in [section VI](#S6 "VI Conclusion ‣ Playing NetHack with
    LLMs: Potential & Limitations as Zero-Shot Agents"). The source code can be found
    on GitHub¹¹1[https://github.com/CommanderCero/NetPlay](https://github.com/CommanderCero/NetPlay).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[第 II 节](#S2 "II Background ‣ Playing NetHack with LLMs: Potential & Limitations
    as Zero-Shot Agents")中开始概述 NetHack，并回顾现有的 LLM 驱动代理的研究。[第 III 节](#S3 "III NetPlay
    ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents")讨论了
    NetPlay 的架构，包括我们由于 LLM 的限制而必须做出的许多设计决策。在[第 IV 节](#S4 "IV Experiments ‣ Playing
    NetHack with LLMs: Potential & Limitations as Zero-Shot Agents")中，我们首先评估了 NetPlay
    自主玩游戏的能力，并将其表现与简单的手工制作代理和自动升天进行比较。随后，我们对代理在各种孤立场景下的行为进行了深入分析。接着，我们在[第 V 节](#S5
    "V Potential and Limitations ‣ Playing NetHack with LLMs: Potential & Limitations
    as Zero-Shot Agents")中分析实验结果，并在[第 VI 节](#S6 "VI Conclusion ‣ Playing NetHack with
    LLMs: Potential & Limitations as Zero-Shot Agents")中总结本研究。源代码可以在 GitHub¹¹1[https://github.com/CommanderCero/NetPlay](https://github.com/CommanderCero/NetPlay)找到。'
- en: II Background
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A NetHack
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A NetHack
- en: '![Refer to caption](img/2398892313e607f8be6fec824512b4ab.png)![Refer to caption](img/750d01c5ec97172e5dba636e783a266f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2398892313e607f8be6fec824512b4ab.png)![参考说明](img/750d01c5ec97172e5dba636e783a266f.png)'
- en: 'Figure 1: The terminal view of the game NetHack. The left image presents an
    annotated view of the in-game screen, featuring the game’s map, an example of
    a game message, and the agent’s stats. The right image showcases a menu for picking
    up items from a tile containing multiple objects. Image source: [alt.org/nethack](https://alt.org/nethack/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：游戏 NetHack 的终端视图。左侧图像展示了游戏屏幕的注释视图，包括游戏地图、游戏消息示例和代理的统计信息。右侧图像展示了一个用于从包含多个对象的瓷砖中拾取物品的菜单。图片来源：[alt.org/nethack](https://alt.org/nethack/)
- en: 'NetHack [[7](#bib.bib7)], released in 1987, is an extremely challenging turn-based
    roguelike that continues to receive updates to this date. The objective is to
    traverse 50 procedurally generated levels, retrieving the Amulet of Yendor and
    successfully returning to the surface. Doing so unlocks the final challenge of
    the game: the four elemental planes, followed by the astral plane, where players
    must present the Amulet to their deity. See [fig. 1](#S2.F1 "In II-A NetHack ‣
    II Background ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents") for a screenshot of the game.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'NetHack [[7](#bib.bib7)]，发布于 1987 年，是一个极具挑战性的回合制 roguelike 游戏，至今仍在不断更新。目标是穿越
    50 个程序生成的关卡，获取 Yendor 项链并成功返回地表。完成这一目标将解锁游戏的最终挑战：四个元素位面，接着是星界位面，玩家必须向他们的神祇展示项链。请参见[图
    1](#S2.F1 "In II-A NetHack ‣ II Background ‣ Playing NetHack with LLMs: Potential
    & Limitations as Zero-Shot Agents")获取游戏的截图。'
- en: Most aspects of the game are generated, such as level layouts, the player’s
    starting class, and the inventory. The levels follow a somewhat linear structure
    with many branches and sub-dungeons in between. For instance, the entrance to
    the gnomish mines always spawns somewhere between depth 2 and 4, giving the player
    the option to explore them immediately or to postpone exploration until they are
    stronger.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的大多数方面都是生成的，例如关卡布局、玩家的起始职业和库存。关卡遵循一种略微线性的结构，中间有许多分支和子地牢。例如，侏儒矿井的入口总是生成在深度
    2 到 4 之间的某个地方，玩家可以选择立即探索或等到变得更强再进行探索。
- en: NetHack encompasses a diverse array of monsters, items, and interactions. Players
    must skillfully utilize their resources while avoiding many of the game’s lethal
    threats. Even for seasoned players possessing extensive knowledge of the game,
    victory is far from guaranteed. The game’s inherent complexity requires players
    to continuously re-assess their situation to adapt to the unpredictability of
    the elements at play.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack包含了各种各样的怪物、物品和互动。玩家必须熟练地利用资源，同时避免许多致命的威胁。即使是对游戏有丰富知识的老玩家，也无法保证胜利。游戏的复杂性要求玩家不断重新评估自己的情况，以适应游戏中不断变化的元素。
- en: Nethack uses description files (des-files) to describe special levels like the
    oracle level that always contains a room with an oracle monster, centaur statues,
    and four fountains. Des-files offer extensive control over the level-generation
    process, allowing entirely handcrafted levels or a slightly constrained level-generation
    process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack使用描述文件（des-files）来描述特殊的关卡，如总是包含一个神谕怪物房间、半人马雕像和四个喷泉的神谕关卡。描述文件提供了对关卡生成过程的广泛控制，允许完全手工制作的关卡或稍微受限的关卡生成过程。
- en: II-B NetHack Learning Environment
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B NetHack学习环境
- en: The NetHack Learning Environment NLE [[10](#bib.bib10)] serves as a reinforcement
    learning environment for playing NetHack 3.6.6\. NLE offers easy access to most
    aspects of the game, such as the map, the agent’s inventory, game messages, and
    the player’s stats. While NLE provides simplified environments for learning purposes,
    it also allows users to play the entire game without any restrictions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack学习环境NLE [[10](#bib.bib10)]作为一个强化学习环境，用于玩NetHack 3.6.6。NLE提供了对游戏大多数方面的便捷访问，例如地图、代理的库存、游戏消息和玩家的统计数据。虽然NLE提供了简化的学习环境，但它也允许用户在没有任何限制的情况下玩完整的游戏。
- en: MiniHack [[11](#bib.bib11)] utilizes NLE alongside des-files to construct small-scale
    environments that isolate specific challenges that agents will encounter in NetHack.
    Although MiniHack provides a list of challenges, its primary purpose is to streamline
    the process of designing new challenges.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: MiniHack [[11](#bib.bib11)]结合NLE和描述文件，构建小规模环境，隔离代理在NetHack中遇到的特定挑战。尽管MiniHack提供了挑战列表，但其主要目的是简化新挑战的设计过程。
- en: II-C autoascend
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C autoascend
- en: In the 2021 NeurIPS NetHack Challenge [[9](#bib.bib9)], participants tackled
    the symbolic and neural tracks, where solutions were either handcrafted or designed
    using machine learning. Notably, the top-performing agents were exclusively symbolic,
    with the autoascend agent emerging as the frontrunner [[12](#bib.bib12)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在2021年NeurIPS NetHack挑战赛[[9](#bib.bib9)]中，参与者解决了符号化和神经网络两个方向的问题，解决方案要么是手工制作的，要么是通过机器学习设计的。值得注意的是，表现最好的代理均为符号化的，autoascend代理脱颖而出[[12](#bib.bib12)]。
- en: The autoascend agent [[13](#bib.bib13)] succeeded by meticulously parsing observations
    and creating an internal state representation to track essential information.
    The agent utilized the enriched data to implement a behavior tree by hierarchically
    combining strategies representing specific behaviors, like fighting, picking up
    objects, or exploring levels. Overall, autoascend’s strategy consists of staying
    on the first dungeon level until reaching experience level 8, after which it will
    rapidly progress deeper into the dungeon. While following this general strategy,
    autoascend uses many sub-strategies to improve its chance of success, such as
    a solver for solving the Sokoban levels, using altars for farming or identifying
    items, or dipping a long sword into a fountain to gain Excalibur.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: autoascend代理[[13](#bib.bib13)]通过仔细解析观察数据并创建内部状态表示来跟踪关键信息，从而取得了成功。该代理利用丰富的数据来实现行为树，通过分层组合代表特定行为的策略，如战斗、捡取物品或探索关卡。总体而言，autoascend的策略包括在第一个地牢层保持直到达到经验等级8，然后快速深入地牢。在遵循这一总体策略的同时，autoascend使用许多子策略来提高成功的机会，例如解决Sokoban关卡的解算器、使用祭坛进行物品收集或识别，或将长剑浸入喷泉中以获得Excalibur。
- en: Despite its victory, autoascend’s performance depended heavily on its starting
    class, demonstrating optimal results with the Valkyrie class. The agent occasionally
    descended to depth 10 and reached experience level 10\. However, it is crucial
    to highlight that reaching around depth 50 is only one of the objectives to beat
    NetHack, emphasizing how challenging the environment still is.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了胜利，autoascend的表现严重依赖于其起始职业，展示了在Valkyrie职业下的最佳结果。该代理程序偶尔会降到深度10并达到经验等级10。然而，必须强调的是，达到大约深度50只是击败NetHack的目标之一，这突显了环境依然具有挑战性。
- en: II-D LLM Agents
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D LLM代理程序
- en: Recently, a plethora of LLM-based agents have emerged, aiming to leverage the
    planning capabilities of these models. A prominent testbed for these agents is
    Minecraft [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)], primarily focusing
    on the agent’s ability to obtain the various items in the game. While the details
    vary, most approaches implement a closed-loop planning system in which the LLM
    generates a plan consisting of a sequence of predefined skills. The plan is then
    executed and, in case of failure, the agent will re-plan using only feedback from
    the previous plan. A noteworthy aspect of these agents is the storage and reuse
    of successful plans, significantly enhancing overall performance due to the hierarchical
    nature of obtaining items like a diamond pickaxe. The agents primarily utilize
    an LLM for their knowledge of how to acquire items. However, one agent has demonstrated
    the ability to construct structures with human feedback [[6](#bib.bib6)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于LLM的代理程序层出不穷，旨在利用这些模型的规划能力。这些代理程序的一个重要测试平台是Minecraft [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)]，主要关注于代理程序获取游戏中各种物品的能力。虽然细节各异，但大多数方法实现了一个闭环规划系统，其中LLM生成一个包含一系列预定义技能的计划。然后执行该计划，如果失败，代理程序将仅使用来自之前计划的反馈重新进行规划。这些代理程序的一个值得注意的方面是存储和重用成功的计划，由于获取诸如钻石镐等物品的层次性，显著提高了整体性能。代理程序主要利用LLM来获取有关如何获得物品的知识。然而，有一个代理程序展示了在人的反馈下构建结构的能力
    [[6](#bib.bib6)]。
- en: Other popular applications are robot environments, where tasks include rearranging
    objects on a tabletop, interacting within a kitchen, or engaging in simulated
    household activities [[14](#bib.bib14), [15](#bib.bib15), [2](#bib.bib2), [3](#bib.bib3)].
    Because these environments require more dynamic decision-making compared to acquiring
    items in Minecraft, agents like DEPS [[14](#bib.bib14)] and Inner Monologue [[2](#bib.bib2)]
    adopt a distinctive approach. Instead of relying solely on feedback from the last
    failed plan, they re-plan by considering a substantial portion of their recent
    interaction history. Similar to our approach, Inner Monologue models the interaction
    history as a chat containing the LLM’s actions and thoughts, human feedback, and
    feedback from the environment, such as scene descriptions and if an action was
    successful. While the robot environments require more dynamic decision-making,
    the complexity of the observations is limited, usually consisting of a list of
    visible objects with spatial information being omitted as the low-level skills
    are handling it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其他热门应用包括机器人环境，其中任务包括在桌面上重新排列物体、在厨房中互动或参与模拟家庭活动 [[14](#bib.bib14), [15](#bib.bib15),
    [2](#bib.bib2), [3](#bib.bib3)]。由于这些环境相比于在Minecraft中获取物品需要更动态的决策，因此像DEPS [[14](#bib.bib14)]
    和Inner Monologue [[2](#bib.bib2)]这样的代理程序采取了独特的方法。它们不仅仅依赖于上一个失败计划的反馈，而是通过考虑大量最近的互动历史来重新规划。类似于我们的方法，Inner
    Monologue将互动历史建模为包含LLM的行动和思维、人类反馈以及来自环境的反馈（如场景描述和行动是否成功）的对话。虽然机器人环境需要更动态的决策，但观察的复杂性是有限的，通常由可见物体的列表组成，空间信息被省略，因为低级技能在处理这些信息。
- en: An alternative use of LLMs involves employing them to design reward functions,
    which are then used to train reinforcement learning agents [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18)]. Most relevant to our work, Motif employs
    an LLM to learn various playstyles in NetHack. It achieves this by tasking the
    LLM to decide which of NetHack’s game messages it prefers. Motif can leverage
    these preferences to learn reward functions for different playstyles by conditioning
    the LLM to prefer game messages associated with a specific playstyle, such as
    fighting monsters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的另一种使用方式是利用它们设计奖励函数，然后用来训练强化学习代理 [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18)]。与我们的工作最相关的是，Motif
    使用 LLM 学习 NetHack 中的各种游戏风格。它通过任务 LLM 决定它更喜欢 NetHack 的哪种游戏消息来实现这一点。Motif 可以利用这些偏好来学习不同游戏风格的奖励函数，通过让
    LLM 更喜欢与特定游戏风格相关的游戏消息，例如打怪。
- en: III NetPlay
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III NetPlay
- en: '![Refer to caption](img/78e69dc5a80ec80b1a619aa45c3f25e1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78e69dc5a80ec80b1a619aa45c3f25e1.png)'
- en: 'Figure 2: Illustration of NetPlay playing NetHack. The process involves constructing
    a prompt using messages representing past events, the current observation, and
    a task description containing available skills and the desired output format.
    The response is parsed to retrieve the next skill. While executing the selected
    skill, a tracker enriches the given observations and detects important events,
    such as when a new monster appears. When the skill is done, or events interrupt
    the skill execution, the agent will restart the prompting process.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：NetPlay 玩 NetHack 的示意图。该过程包括使用表示过去事件的消息、当前观察和包含可用技能及期望输出格式的任务描述来构建提示。响应被解析以检索下一个技能。在执行所选技能时，跟踪器丰富给定的观察结果并检测重要事件，例如出现新怪物。完成技能或事件中断技能执行时，代理将重新启动提示过程。
- en: 'This section discusses our LLM-powered Nethack agent NetPlay. See [fig. 2](#S3.F2
    "In III NetPlay ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents") for an overview of the architecture. Long-term planning in NetHack proves
    challenging due to its unpredictability, as we cannot know when, where, or what
    will appear as we explore. Consequently, our agent shares many similarities with
    Inner Monologue, which is designed for dynamic environments. It implements a closed-loop
    system where the LLM selects skills sequentially while accumulating feedback in
    the form of game messages, errors, or manually detected events. Although we avoid
    constructing entire plans, the LLM’s thoughts are included for future prompts,
    allowing for strategic planning if deemed necessary by the LLM.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了我们基于 LLM 的 Nethack 代理 NetPlay。有关架构的概述，请参见 [fig. 2](#S3.F2 "在 III NetPlay
    ‣ 使用 LLM 玩 NetHack 的潜力与局限性作为零样本代理")。由于 NetHack 的不可预测性，长期规划变得具有挑战性，因为我们无法知道在探索时何时、何地或什么会出现。因此，我们的代理与为动态环境设计的
    Inner Monologue 有许多相似之处。它实现了一个闭环系统，其中 LLM 顺序选择技能，同时通过游戏消息、错误或手动检测到的事件积累反馈。尽管我们避免构建整个计划，但
    LLM 的思维会被纳入未来的提示中，从而允许 LLM 认为必要时进行战略规划。
- en: III-A Prompting
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 提示
- en: 'We prompt the LLM to choose a skill from a predefined list. The prompt comprises
    three components: $(a)$ a task description alongside the output format.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提示 LLM 从预定义列表中选择一个技能。提示包括三个部分：$(a)$ 任务描述和输出格式。
- en: $(a)$ The observation description primarily focuses on the current level alongside
    additional data like context, inventory, and the agent’s stats. Because we do
    not use a multi-modal LLM, we attempt to convey spatial information by dividing
    the level into structures like rooms and corridors. Each structure is described
    using a unique identifier, the number of steps to reach it, the objects it contains
    with their respective positions, and the number of steps to reach each object.
    Monsters are described separately from the structures by categorizing them as
    close or distant, indicating their potential threat level. Each monster is described
    using its name, position, and number of steps to reach it. For close monsters,
    we also include compass coordinates. The LLM is also informed about which structures
    can be further explored alongside the positions of boulders and doors that block
    exploration progress. Note that despite our emphasis on providing spatial information,
    navigating the environment proved challenging for the LLM. Consequently, we automated
    a large portion of the exploration process using a single skill, potentially rendering
    certain aspects of this observation description obsolete.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $(a)$ 观察描述主要关注当前级别以及附加数据，如上下文、库存和代理的统计数据。由于我们不使用多模态 LLM，我们尝试通过将级别划分为房间和走廊等结构来传达空间信息。每个结构使用唯一标识符进行描述，包括到达该结构的步数、其中包含的对象及其位置，以及到达每个对象的步数。怪物与结构分开描述，通过将其分类为近距离或远距离，表示其潜在的威胁级别。每个怪物都通过其名称、位置和到达它的步数进行描述。对于近距离怪物，我们还包括了罗盘坐标。LLM
    还会被告知哪些结构可以进一步探索，以及阻碍探索进展的石头和门的位置。请注意，尽管我们强调提供空间信息，但对环境的导航对 LLM 来说仍然具有挑战性。因此，我们使用单一技能自动化了探索过程的大部分，可能使观察描述的某些方面变得过时。
- en: $(b)$ The short-term memory is implemented using a list of messages representing
    the timeline of events. Each message is either categorized as system, AI, or human.
    System messages convey feedback from the environment like game messages or errors,
    AI messages capture the LLM’s responses, and new tasks are indicated by human
    messages. Note that while it is possible for a human to provide continual feedback,
    we only study the case where the agent is given a task at the start of the game.
    The memory size is capped at 500 tokens, with older messages being deleted first.
    Observation descriptions are not stored in the memory due to their size.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $(b)$ 短期记忆是通过一系列消息来实现的，这些消息代表事件的时间线。每条消息都被分类为系统、AI 或人类。系统消息传达来自环境的反馈，如游戏消息或错误，AI
    消息捕捉 LLM 的响应，新任务由人类消息表示。请注意，虽然人类可以提供持续的反馈，但我们只研究在游戏开始时给代理分配任务的情况。记忆大小限制为 500 个标记，较旧的消息会被优先删除。观察描述由于其大小而不存储在记忆中。
- en: $(c)$ The task description includes details about the current task, available
    skills, and a JSON output format. We employ chain-of-thought prompting [[19](#bib.bib19)]
    to guide the LLM to a skill choice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $(c)$ 任务描述包括有关当前任务、可用技能和 JSON 输出格式的详细信息。我们使用思维链提示 [[19](#bib.bib19)] 来引导 LLM
    选择技能。
- en: III-B Skills
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 技能
- en: 'TABLE I: Skill Examples: Skills represent parametrizable behaviors that the
    LLM uses to play the game. The name, parameters, and descriptions help to understand
    what each skill does. For some skills, the LLM can omit optional parameters marked
    in [square brackets]. Note that the skill type is only used internally and does
    not matter for the final agent.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 技能示例: 技能代表 LLM 用于游戏的可参数化行为。名称、参数和描述有助于理解每个技能的作用。对于某些技能，LLM 可以省略标记为[方括号]中的可选参数。请注意，技能类型仅在内部使用，对于最终的代理来说并不重要。'
- en: '| Type | Name | Parameters | Description |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 名称 | 参数 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Special | explore_level |  | Explores the level to find new rooms, as well
    as hidden doors and corridors. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 特殊 | explore_level |  | 探索级别以寻找新房间以及隐藏的门和走廊。 |'
- en: '| Special | set_avoid_monster_flag | value: bool | If set to true skills will
    try to avoid monsters. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 特殊 | set_avoid_monster_flag | value: bool | 如果设置为 true，技能将尝试避开怪物。 |'
- en: '| Special | press_key | key: string | Presses the given letter. For special
    keys only ESC, SPACE, and ENTER are supported. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 特殊 | press_key | key: string | 按下给定的字母。仅支持特殊键 ESC、SPACE 和 ENTER。 |'
- en: '| Position | pickup | [x: int, y: int] | Pickup things at your location or
    specify where you want to pickup an item. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | pickup | [x: int, y: int] | 拾取当前位置的物品或指定您想要拾取物品的位置。 |'
- en: '| Position | up | [x: int, y: int] | Go up a staircase at your location or
    specify the position of the staircase you want to use. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Position | up | [x: int, y: int] | 向上走一段楼梯，或者指定你想使用的楼梯的位置。 |'
- en: '| Inventory | drop | item_letter: string | Drop an item. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Inventory | drop | item_letter: string | 丢弃一个物品。 |'
- en: '| Inventory | wield | item_letter: string | Wield a weapon. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Inventory | wield | item_letter: string | 挥舞武器。 |'
- en: '| Direction | kick | x: int, y: int | Kick something. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Direction | kick | x: int, y: int | 踢某物。 |'
- en: '| Basic | cast |  | Opens your spellbook to cast a spell. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Basic | cast |  | 打开你的魔法书以施放法术。 |'
- en: '| Basic | pay |  | Pay your shopping bill. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Basic | pay |  | 支付你的购物账单。 |'
- en: 'Skills, similar to strategies in autoascend, implement specific behaviors by
    returning a sequence of actions. They accept both mandatory and optional parameters
    as input. Skills can generate messages as feedback, which are stored in the agent’s
    memory. Messages are often used, for example, to report why a skill failed. An
    excerpt of skills can be found in [table I](#S3.T1 "In III-B Skills ‣ III NetPlay
    ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '技能类似于 autoascend 中的策略，通过返回一系列动作来实现特定行为。它们接受必需的和可选的参数作为输入。技能可以生成反馈消息，这些消息存储在代理的记忆中。消息通常用于报告例如技能失败的原因。技能的摘录可以在
    [表 I](#S3.T1 "In III-B Skills ‣ III NetPlay ‣ Playing NetHack with LLMs: Potential
    & Limitations as Zero-Shot Agents") 中找到。'
- en: Navigation is automated through skills like “move_to x y” or “go_to room_id”.
    However, exploring levels with only these skills proved challenging for the LLM.
    To address this, we introduced the “explore_level” skill, which uses the exploration
    strategy from autoascend. This skill explores the current level by uncovering
    tiles, opening doors, and searching for hidden corridors. We removed the ability
    to kick open doors to avoid potential issues such as aggravating shopkeepers.
    Note that the agent can still decide to kick open doors using a separate “kick”
    skill. All movement-related skills will attack monsters that are in the way. The
    LLM can turn off this behavior using the “set_avoid_monster_flag” skill.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 导航通过像“move_to x y”或“go_to room_id”这样的技能自动化。然而，仅用这些技能探索关卡对 LLM 来说是具有挑战性的。为了解决这个问题，我们引入了“explore_level”技能，它使用来自
    autoascend 的探索策略。此技能通过揭示瓷砖、打开门和寻找隐藏的走廊来探索当前的关卡。我们移除了踢开门的能力，以避免可能的问题，如激怒商店老板。请注意，代理仍然可以决定使用单独的“kick”技能踢开门。所有与移动相关的技能都会攻击路上的怪物。LLM
    可以使用“set_avoid_monster_flag”技能关闭此行为。
- en: To indicate when the agent is done with a given task, it has access to the “finish_task”
    skill. Additionally, the LLM is equipped with the “press_key” and “type_text”
    skills for navigating NetHack’s various game menus. While a menu is open, only
    the “finish_task” and text input skills remain available.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指示代理完成给定任务，它可以使用“finish_task”技能。此外，LLM 配备了“press_key”和“type_text”技能，用于导航 NetHack
    的各种游戏菜单。当菜单打开时，只有“finish_task”和文本输入技能仍然可用。
- en: The remaining skills are thin wrappers around NetHack commands, such as drink
    or pickup. However, these commands often involve multiple steps, such as confirming
    which item to drink or first positioning the agent correctly to then pick up an
    item. Consequently, the LLM often assumed that the “drink” command accepts an
    item parameter or that “pickup” works seamlessly regardless of the agent’s current
    position. To mitigate these issues, we implemented four types of command skills.
    Base commands only invoke the command. Position commands offer the option to first
    move to the desired location. Inventory commands accept an item parameter to resolve
    the following popup menu. Finally, direction commands like “kick” move the agent
    close to a desired position before executing the command in the correct direction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的技能是 NetHack 命令的薄包装器，如饮用或捡拾。然而，这些命令通常涉及多个步骤，例如确认饮用哪个物品，或首先正确定位代理以便捡拾物品。因此，LLM
    通常认为“drink”命令接受一个物品参数，或者“pickup”在不考虑代理当前的位置的情况下可以顺利工作。为了解决这些问题，我们实现了四种类型的命令技能。基础命令只调用命令。位置命令提供了首先移动到所需位置的选项。库存命令接受物品参数以解决以下弹出菜单。最后，像“kick”这样的方向命令在执行命令前将代理移动到接近目标位置的位置。
- en: III-C Agent Loop
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 代理循环
- en: Upon receiving a new task, the agent prompts the LLM to select the first skill
    to execute. The LLM’s thoughts and the selected skill are stored in the agent’s
    memory as feedback. While executing the chosen skill, a data tracker observes
    and records details such as found structures, features hidden by monsters or items,
    which tiles the agent has already seen or searched, and events. The information
    collected by the data tracker is used by skills to make decisions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在收到新任务后，代理会提示LLM选择第一个要执行的技能。LLM的想法和所选技能会作为反馈存储在代理的记忆中。在执行所选技能时，数据追踪器会观察并记录细节，如发现的结构、被怪物或物品隐藏的特征、代理已经看到或搜索过的瓷砖，以及事件。数据追踪器收集的信息被技能用来做决策。
- en: The data tracker also looks for specific events in the game to provide additional
    feedback to the LLM. Events include new in-game messages, newly discovered structures,
    level changes or teleports, stat changes, low health, and the discovery of new
    monsters, items, and some map features such as fountains or altars.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据追踪器还会寻找游戏中的特定事件，以向LLM提供额外反馈。这些事件包括新的游戏内消息、新发现的结构、等级变化或传送、状态变化、低生命值，以及发现新的怪物、物品和一些地图特征，如喷泉或祭坛。
- en: A skill continues to run until completion or interruption. Skills are interrupted
    when specific events occur, such as changing the level, teleporting, discovering
    new objects, and reaching low health. In addition to events, many skills are interrupted
    when a menu shows up due to their inability to handle them. Regardless of why
    a skill stopped, the agent then prompts the LLM to select the next skill. The
    sole exception is when the “finish_task” skill is selected, or the game has ended,
    at which point the agent will stop until it receives a new task.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 技能会持续运行直到完成或被中断。技能会在特定事件发生时中断，例如更换等级、传送、发现新物体和生命值低。除了事件外，许多技能在菜单出现时也会中断，因为它们无法处理这些菜单。无论技能停止的原因是什么，代理都会提示LLM选择下一个技能。唯一的例外是当选择了“finish_task”技能或游戏结束时，此时代理会停止，直到收到新任务。
- en: III-D Handcrafted Agent
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 手工制作代理
- en: To assess the impact of the LLM in contrast to the predefined skills, we implemented
    a handcrafted agent that aims to replicate the behavior of NetPlay with the task
    set to “Win the Game”. The following list shows a breakdown of the agent’s decision-making
    process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM的影响与预定义技能的对比，我们实现了一个手工制作的代理，旨在复制NetPlay的行为，任务设定为“赢得游戏”。以下列表展示了代理决策过程的详细信息。
- en: '1.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Abort any open menu, as we did not implement a way to navigate them.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 取消任何打开的菜单，因为我们没有实现导航菜单的方式。
- en: '2.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: If there are hostile monsters nearby, fight them.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果附近有敌对怪物，进行战斗。
- en: '3.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: If health is below 60%, try healing with potions or by praying.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果生命值低于60%，尝试使用药水或祈祷来治疗。
- en: '4.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Eat food from the inventory when hungry.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当感到饥饿时，从库存中吃食物。
- en: '5.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Pick up items, which in this case are potions and food.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 捡起物品，在这种情况下是药水和食物。
- en: '6.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: If nothing to explore, move to the next level if possible.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有可探索的地方，尽可能移动到下一个关卡。
- en: '7.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: If nothing else to do, explore the level and try kicking open doors.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有其他事情可做，探索关卡并尝试踢开门。
- en: All the conditions are evaluated in sequence. Once a condition is met, a corresponding
    skill is executed. The selected skill will be interrupted in the same way as NetPlay.
    Once a skill is interrupted, the agent will choose the next skill by again checking
    all conditions in order starting from the first. Note that although we aimed to
    imitate NetPlay’s behavior, the provided rules are too simplistic to capture all
    the nuances.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所有条件按顺序进行评估。一旦满足某个条件，就会执行相应的技能。所选技能会像NetPlay一样被中断。一旦技能被中断，代理会重新检查所有条件，按顺序选择下一个技能。请注意，尽管我们旨在模仿NetPlay的行为，但提供的规则过于简单，无法捕捉所有细微之处。
- en: IV Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: Our goals for the experiments were two-fold. First, to evaluate the ability
    of NetPlay to play NetHack. Second, to provide an analysis of the agent’s strengths
    and weaknesses, focusing on identifying which aspects are influenced by the LLM.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的目标有两个。首先，评估NetPlay在玩NetHack方面的能力。其次，提供对代理的优缺点的分析，重点识别哪些方面受到LLM的影响。
- en: IV-A Setup
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 设置
- en: All of our experiments used OpenAI’s GPT-4-Turbo (gpt-4-1106-preview) API as
    LLM with the temperature set to 0 and the response format set to JSON. Other models
    were not considered as initial tests revealed that models like GPT-3.5 and a 70B
    parameter instruct version of LLAMA 2 [[20](#bib.bib20)] could not correctly utilize
    our skills. The agent’s memory size was set to 500 tokens.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的实验都使用了 OpenAI 的 GPT-4-Turbo（gpt-4-1106-preview）API 作为 LLM，温度设置为 0，响应格式设置为
    JSON。其他模型未被考虑，因为初步测试显示 GPT-3.5 和一个 70B 参数的 LLAMA 2 指令版本 [[20](#bib.bib20)] 无法正确利用我们的技能。代理的内存大小设置为
    500 个 token。
- en: The agent had access to most commands that interact with the game directly,
    except for some rarely relevant commands, like turning undead or using a monster’s
    special ability. All control and system commands, like opening the help menu or
    hiding icons on the map, were excluded. We also implemented a time limit of 10
    LLM calls, at which point the experiment would terminate if the in-game time did
    not advance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理可以访问大多数与游戏直接交互的命令，但有些不常用的命令（如驱邪或使用怪物的特殊能力）被排除在外。所有控制和系统命令，如打开帮助菜单或隐藏地图上的图标，也被排除在外。我们还设置了10次LLM调用的时间限制，如果游戏内时间没有推进，实验将终止。
- en: IV-B Full Runs
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 完整运行
- en: 'TABLE II: Results summary of the mean and standard error for the agents achieved
    score, depth, experience level, and game time.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：代理的得分、深度、经验水平和游戏时间的均值和标准误差结果汇总。
- en: '| Metric | NetPlay (Unguided) | NetPlay (Guided) | autoascend | handcrafted
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | NetPlay（未指导） | NetPlay（指导） | autoascend | 手工制作 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Score | 284.85 $\pm$ 159.17 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | 284.85 $\pm$ 159.17 |'
- en: '| Depth | 2.60 $\pm$ 0.93 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 深度 | 2.60 $\pm$ 0.93 |'
- en: '| Level | 2.40 $\pm$ 1.05 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 级别 | 2.40 $\pm$ 1.05 |'
- en: '| Time | 1292.10 $\pm$ 924.17 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 1292.10 $\pm$ 924.17 |'
- en: We started evaluating NetPlay by letting it play NetHack without any constraints,
    tasking it to win the game. We will refer to this agent as the “unguided agent.”
    Although the task was to play the entire game, the agent occasionally confused
    its own objectives with the assigned task, resulting in the agent marking the
    task as done too early. To address this issue, we disabled the “finish_task” skill
    for this experiment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始评估 NetPlay，让它在没有任何限制的情况下玩 NetHack，并要求它赢得游戏。我们将这个代理称为“未指导代理”。尽管任务是玩完整个游戏，但代理偶尔会将自己的目标与分配的任务混淆，导致代理过早地将任务标记为完成。为了解决这个问题，我们在这次实验中禁用了“完成任务”技能。
- en: 'Due to budget limitations, we evaluated all agents using only the Valkyrie
    role, as most agents performed best with this class during the NetHack 2021 challenge.
    We conducted 20 runs with the unguided agent. Additionally, we performed 100 runs
    each with autoascend and the handcrafted agent for comparison. After evaluating
    the unguided agent, we carried out an additional 10 runs employing a “guided agent”
    who was informed on how to play better. A detailed description of the guided agent
    will be provided below. For now, a summary of the results can be found in [table II](#S4.T2
    "In IV-B Full Runs ‣ IV Experiments ‣ Playing NetHack with LLMs: Potential & Limitations
    as Zero-Shot Agents").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预算限制，我们仅使用了 Valkyrie 角色来评估所有代理，因为大多数代理在 NetHack 2021 挑战中使用此角色表现最佳。我们使用未指导的代理进行了
    20 次运行。此外，我们还进行了各 100 次运行的 autoascend 和手工代理以进行比较。在评估了未指导的代理之后，我们进行了额外的 10 次运行，采用了一个“指导代理”，该代理被告知如何更好地进行游戏。关于指导代理的详细描述将在下文提供。目前，可以在
    [表 II](#S4.T2 "在 IV-B 完整运行 ‣ IV 实验 ‣ 使用 LLM 玩 NetHack：零样本代理的潜力与局限性") 中找到结果的总结。
- en: '[Table II](#S4.T2 "In IV-B Full Runs ‣ IV Experiments ‣ Playing NetHack with
    LLMs: Potential & Limitations as Zero-Shot Agents") shows that autoascend far
    outperforms both NetPlay and the handcrafted agent. While NetPlay managed to beat
    the handcrafted agent by a small margin, it is likely that with a few tweaks,
    the handcrafted agent can also outperform NetPlay.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 II](#S4.T2 "在 IV-B 完整运行 ‣ IV 实验 ‣ 使用 LLM 玩 NetHack：零样本代理的潜力与局限性") 显示 autoascend
    远远优于 NetPlay 和手工制作的代理。虽然 NetPlay 以小幅度击败了手工代理，但可能只需进行一些调整，手工代理也能超过 NetPlay。'
- en: The unguided agent primarily failed due to timeouts, followed by deaths caused
    by eating rotten corpses, fighting with low health, or being overwhelmed by enemies.
    Many timeouts were caused by the agent attempting to move past friendly monsters,
    such as a shopkeeper. By default, bumping into monsters attacks them, but for
    passive monsters, the game prompts the player before initiating an attack. The
    agent’s refusal to attack these monsters often leads to a loop of canceling the
    prompt and moving, resulting in eventual timeouts. A similar loop took place when
    the agent attempted to pick up an item with a generic name on the map but a detailed
    name in the game’s menu. This confusion led the agent to repeatedly close and
    reopen the menu, unable to locate the desired item.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 无指导代理主要因超时而失败，其次是因吃到腐烂尸体、健康低下时战斗或被敌人压倒而导致的死亡。许多超时是由于代理试图穿过友好的怪物（如商人）而造成的。默认情况下，碰到怪物会攻击它们，但对于被动怪物，游戏会在开始攻击前提示玩家。代理拒绝攻击这些怪物通常会导致取消提示并移动的循环，最终导致超时。当代理尝试捡起地图上有通用名称但在游戏菜单中有详细名称的物品时，也出现了类似的循环。这种混乱导致代理反复关闭和重新打开菜单，无法找到所需的物品。
- en: Based on the results of the unguided agent, we constructed a guide that included
    strategies from autoascend, such as staying on the first two dungeon levels until
    reaching experience level 8, consuming only freshly slain corpses to avoid eating
    rotten ones, and leveraging altars to acquire items. Furthermore, we provided
    tips for common mistakes by the unguided agent, such as avoiding getting stuck
    behind passive monsters and informing the agent about the time limit to avoid
    timeouts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基于无指导代理的结果，我们构建了一个包括 autoascend 策略的指南，例如在前两个地下城层保持直到达到经验等级 8，只消耗新鲜的尸体以避免吃到腐烂的尸体，并利用祭坛获取物品。此外，我们还提供了无指导代理常见错误的提示，例如避免被被动怪物卡住，并提醒代理时间限制以避免超时。
- en: The guided agent often managed to stay alive longer by consuming freshly killed
    corpses and praying when hungry or at low health. Its causes of death have been
    a mixture of timeouts, starvation, and dying in combat. Most of the timeouts stemmed
    from a bug with our tracker, which fails to detect when an object disappears while
    being obscured by a monster. For example, the agent repeatedly attempted to pick
    up a dagger already taken by its pet due to the tracker’s misleading observation.
    Despite receiving game messages indicating the absence of the item, the agent
    failed to recognize the situation accurately.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有指导的代理通常通过消耗新鲜杀死的尸体以及在饥饿或健康低下时祈祷来延长生存时间。其死亡原因包括超时、饥饿和战斗中死亡。大多数超时源于我们跟踪器的一个错误，该错误无法检测到在怪物遮挡下物体的消失。例如，由于跟踪器的误导观察，代理重复尝试捡起已经被其宠物拿走的匕首。尽管收到了指示物品缺失的游戏消息，但代理未能准确识别情况。
- en: Because we tasked the guided agent to stay on the first two dungeon levels,
    its average depth is lower than that of the unguided agent. However, because monsters
    keep spawning over time, staying on the first levels is an excellent way to grind
    experience. This results in the guided agent gaining more experience than the
    unguided agent. Nevertheless, the agent’s tendency to stay on the first dungeon
    levels frequently caused it to die of starvation due to not finding enough monster
    corpses to eat. Note that autoascend had a similar starvation issue.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们要求有指导的代理保持在前两个地下城层，其平均深度低于无指导代理。然而，由于怪物会随着时间不断刷新，停留在前几层是获得经验的绝佳方式。这使得有指导的代理获得的经验比无指导代理更多。然而，代理频繁停留在前几个地下城层导致其因找不到足够的怪物尸体而死于饥饿。请注意，autoascend
    也有类似的饥饿问题。
- en: IV-C Scenarios
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 场景
- en: After conducting the full runs, we hypothesized that although NetPlay can be
    creative and interact with most mechanics in the game, it tends to fixate on the
    most straightforward approach for a given task. To confirm this hypothesis, we
    constructed various small-scale scenarios using des-files and a corresponding
    task description. Note that we excluded the handcrafted agent and autoascend for
    this experiment as they cannot easily alter their behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行完整的实验后，我们假设尽管 NetPlay 可以进行创造性操作并与游戏中的大多数机制互动，但它往往会专注于完成任务的最直接方法。为了验证这一假设，我们使用
    des-files 和相应的任务描述构建了各种小规模场景。请注意，我们在此实验中排除了手工制作的代理和 autoascend，因为它们不能轻易改变其行为。
- en: 'The tested scenarios evaluated NetPlay’s ability to interact with game mechanics,
    follow instructions, and its creativity. We conducted five runs for each scenario,
    with all roles and the “finish_task” skill enabled. We also repeated some scenarios
    where the agent performed poorly with additional guidelines. We censored the word
    NetHack for the scenarios to evaluate the agent’s ability independently of its
    knowledge about the game. To avoid the agent never using the “finish_task” skill,
    we set a time limit of 500 timesteps for creative scenarios and 200 for the others.
    See [table III](#S4.T3 "In IV-C Scenarios ‣ IV Experiments ‣ Playing NetHack with
    LLMs: Potential & Limitations as Zero-Shot Agents") for a summary of the tested
    scenarios and their results.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 测试场景评估了 NetPlay 与游戏机制互动、遵循指令和创造力的能力。我们为每个场景进行了五次运行，所有角色和“finish_task”技能均启用。我们还重复了一些代理表现不佳的场景，并提供了额外的指导。为了独立评估代理的能力，我们将“NetHack”这个词进行了审查，以避免代理对游戏的知识影响评估。为了避免代理从未使用“finish_task”技能，我们为创造性场景设置了500时间步的时间限制，而其他场景为200时间步。有关测试场景及其结果的摘要，请参见[表
    III](#S4.T3 "在 IV-C 场景 ‣ IV 实验 ‣ 与 LLMs 一起玩 NetHack：作为零样本代理的潜力与局限性")。
- en: The tested scenarios show that NetPlay performs best when provided with concrete
    instructions. The focused boulder task and both escape tasks, in particular, highlight
    how the agent can act creative if we focus its attention on a specific problem.
    However, without very detailed instructions, the agent often fails to do what
    it wants due to incorrect actions and a lack of explicit feedback.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 测试场景表明，当提供具体指令时，NetPlay 表现最佳。特别是专注的巨石任务和两个逃脱任务突显了当我们将代理的注意力集中在特定问题上时，它如何能够发挥创造力。然而，没有非常详细的指令时，代理经常因操作不正确和缺乏明确反馈而无法完成任务。
- en: The agent’s struggle with explicit feedback is particularly evident in the bag
    and multipickup scenarios, where the agent often failed to navigate the menus
    correctly. While it understood the menus and often chose the correct course of
    action, it often failed by forgetting a crucial step, such as closing the menu.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在显式反馈方面的困难在背包和多次拾取场景中尤为明显，代理经常无法正确导航菜单。虽然它理解菜单并且通常选择正确的行动步骤，但它经常因忘记一个关键步骤（例如关闭菜单）而失败。
- en: 'TABLE III: Scenarios: A detailed description of all the tested scenarios, their
    results, and the agent’s success rate. Note that in some scenarios, the agent
    did not use the “finish_task” skill, even after completing it. We still count
    these as success.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：场景：所有测试场景的详细描述、结果和代理的成功率。请注意，在某些场景中，代理在完成任务后没有使用“finish_task”技能。我们仍然将这些情况视为成功。
- en: '| Scenario | Success | Description | Results |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 成功 | 描述 | 结果 |'
- en: '| Game Mechanics |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 游戏机制 |'
- en: '| bag | 1/5 | A room with four random objects and a bag of holding with the
    task of stuffing all objects into the bag. | The bag of holding menu is quite
    complex. The agent was only successful when using the option that automatically
    stuffs all items into the bag. In the other cases, the agent forgot to mark an
    item or to confirm its selection. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 背包 | 1/5 | 一个有四个随机物体的房间和一个可以容纳所有物体的背包，任务是将所有物体塞入背包中。 | 背包菜单相当复杂。代理只有在使用自动将所有物品塞入背包的选项时才成功。在其他情况下，代理忘记了标记物品或确认选择。
    |'
- en: '| guided bag | 3/5 | Same as bag, but we told the agent the quickest way to
    pick up items and to navigate the bag’s menu. | The agent used the automatic option
    three times. In the other cases, the agent marked the task done too early, stating
    that it would pick up the remaining items next. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 引导背包 | 3/5 | 与背包相同，但我们告诉代理选择物品的最快方法并导航背包的菜单。 | 代理使用了自动选项三次。在其他情况下，代理在任务完成得太早时表示它将接下来拾取剩余的物品。
    |'
- en: '| multipickup | 3/5 | A room with 2-5 objects on the same spot, challenging
    the agent to navigate the multipickup menu. | The agent often picked up items
    inefficiently by opening the pickup menu multiple times. It failed twice by forgetting
    to confirm its item selection. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 多次拾取 | 3/5 | 一个有 2-5 个物体在同一个位置的房间，挑战代理导航多次拾取菜单。 | 代理经常通过多次打开拾取菜单来低效地拾取物品。它因忘记确认物品选择而失败两次。'
- en: '| wand | 1/5 | A room with a statue and a wand with the task of hitting the
    statue with the wand. | The agent often failed by standing atop the statue and
    casting the wand onto itself. Only once did the wand spawn next to the statue,
    causing the agent to cast the wand towards the statue. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 法杖 | 1/5 | 一个房间有一座雕像和一根法杖，任务是用法杖击打雕像。 | 代理经常失败于站在雕像顶部并将法杖施放到自己身上。只有一次法杖生成在雕像旁边，导致代理将法杖施放到雕像上。
    |'
- en: '| guided wand | 5/5 | Same task as wand, but we asked the agent to stand next
    to the statue instead of on top of it and fire in the statue’s direction. | Most
    of the time, the agent succeeded on the first try, except once when he got it
    on the second try after repositioning himself. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 指导性法杖 | 5/5 | 与法杖任务相同，但我们要求代理站在雕像旁边而不是雕像上，并朝雕像方向施放。 | 大多数情况下，代理在第一次尝试时成功，只有一次因为重新定位后在第二次尝试时成功。
    |'
- en: '| Instructions |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 指令 |'
- en: '| ordered | 5/5 | A room with the task to pick up two wands, then a scroll
    of identification, and finally to identify one wand. | The agent executed the
    tasks accurately in the given order. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 有序 | 5/5 | 一个房间的任务是先拿到两个法杖，再拿到一个鉴定卷轴，最后鉴定一个法杖。 | 代理准确地按照给定顺序执行了任务。 |'
- en: '| unordered | 3/5 | A room with the task to drink from a fountain, open a locked
    and a closed door, and kill a monster in any order. | The agent completed the
    tasks in no particular order. One fail stemmed from high-level mobs spawning from
    the fountain, and one from incorrectly using the lockpick. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 无序 | 3/5 | 一个房间的任务是以任何顺序从喷泉饮水、打开一个锁门和一个封闭的门，并杀死一个怪物。 | 代理以无特定顺序完成任务。一项失败源于喷泉生成了高级怪物，另一项因错误使用锁匠工具而失败。
    |'
- en: '| alternative | 5/5 | Three rooms with a fountain and a potion somewhere. The
    task was to drink from a fountain or a potion. | The agent always drank from the
    fountain, which in all cases was found first or was closest to the agent. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 替代 | 5/5 | 三个房间里有一个喷泉和一个药水。任务是从喷泉或药水中饮用。 | 代理总是从喷泉中饮用，喷泉在所有情况下都是首先找到或离代理最近的。
    |'
- en: '| conditional | 4/5 | Three rooms, with only a single potion hidden in one
    of the rooms. The task was to drink from a fountain, or if unavailable a potion.
    | The agent always drinks the first potion it finds without exploring further.
    In one case, it deemed the task impossible due to spawning with no fountain or
    potion in sight. |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 条件性 | 4/5 | 三个房间，只有一个房间里隐藏着一个药水。任务是从喷泉饮水，或者如果没有喷泉则饮用药水。 | 代理总是喝到它找到的第一个药水，而不再进一步探索。有一次，它因为没有喷泉或药水而认为任务不可完成。
    |'
- en: '| Creativity |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 创造力 |'
- en: '| carry | 1/5 | The agent has to carry two very heavy objects through a monster-filled
    room. We also provided tools such as a bag of holding, a teleportation wand, and
    an invisibility cloak. | The agent often refused to play because it could not
    see the required items or it dropped them in the wrong room. |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 携带 | 1/5 | 代理需要在充满怪物的房间中携带两个非常重的物体。我们还提供了持物袋、传送法杖和隐形斗篷等工具。 | 代理经常拒绝执行任务，因为它无法看到所需的物品，或将物品丢在错误的房间。
    |'
- en: '| guided carry | 4/5 | Same task as carry, but we told the agent to prioritize
    killing monsters first, to carry only one of the heavy items at a time, and to
    use the teleportation wand for easier travel. | Most of the time, the agent carried
    only one item, and it often used the wand to teleport. It failed once by dropping
    one item in the incorrect room. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 指导性携带 | 4/5 | 与携带任务相同，但我们告诉代理优先击杀怪物，每次只携带一个重物，并使用传送法杖以便于移动。 | 大多数时候，代理只携带一个物品，且经常使用法杖进行传送。由于在错误的房间丢下一个物品，它曾失败过一次。
    |'
- en: '| boulder | 1/5 | Two rooms connected by a corridor with a boulder. The agent
    starts either with pickaxes or wands to remove the boulder. | When given only
    wands, the agent only used explore_level and ignored the boulder. Only once did
    it start with a pickaxe that it used to mine the boulder. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 巨石 | 1/5 | 两个房间通过走廊连接，里面有一块巨石。代理开始时使用镐子或法杖去移除巨石。 | 当只给法杖时，代理仅使用 explore_level
    而忽略巨石。只有一次它开始时用镐子去挖巨石。 |'
- en: '| focused boulder | 3/5 | Same task as boulder, but the agent was told to remove
    any boulders blocking its path. | The agent often tried kicking the boulder, which
    failed, after which it then used a pickaxe or a wand. It failed twice due to not
    correctly utilizing the available tools. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 集中巨石 | 3/5 | 与巨石任务相同，但要求代理移除任何阻挡路径的巨石。 | 代理经常尝试踢巨石，但失败了，然后才使用镐子或法杖。因未正确利用可用工具，它失败过两次。
    |'
- en: '| guided boulder | 5/5 | Same task as focused boulder, but the agent was told
    explicitly to remove the boulder with the wands or pickaxes. We also provided
    directions on how to utilize the tools. | In all cases, the agent quickly used
    the pickaxe or a wand to remove the boulder. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 引导岩石 | 5/5 | 与集中岩石的任务相同，但代理被明确告知要使用魔杖或镐子移除岩石。我们还提供了如何使用工具的指示。 | 在所有情况下，代理都迅速使用了镐子或魔杖来移除岩石。
    |'
- en: '| escape | 3/5 | The agent must escape from a stone-walled room. Escape methods:
    Digging with a wand through a wall, teleporting with a wand, or morphing into
    a wall-phasing monster using a polymorph control ring with a polymorph wand. |
    The agent escaped twice by teleporting, despite initial teleport failure. It also
    experimented with the wand of digging, casting it in all directions to find an
    exit. It failed twice due to incorrectly using the wands. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 逃脱 | 3/5 | 代理必须从一个石墙房间中逃脱。逃脱方法：用魔杖挖掘墙壁、用魔杖传送，或使用变形控制环和变形魔杖变成墙壁穿透怪物。 | 代理通过传送逃脱了两次，尽管初始传送失败。它还尝试了挖掘魔杖，四处施法寻找出口。由于魔杖使用不当，它失败了两次。
    |'
- en: '| hint escape | 5/5 | Same as escape, with a hint engraved on the floor. The
    hint either reveals which wall is brittle and leads to an escape or hints at the
    name of the wall-phasing monster. | After finding the hint, the agent often used
    the suggested escape method, except for one occasion when it teleported instead.
    In one instance, the initial attempts to dig through the wall failed, so it resorted
    to exploring other methods. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 提示逃脱 | 5/5 | 与逃脱相同，但地板上刻有提示。提示要么揭示哪面墙脆弱并通向逃脱，要么暗示墙壁穿透怪物的名字。 | 在找到提示后，代理通常使用建议的逃脱方法，除了有一次它选择了传送。在一个实例中，最初的挖掘尝试失败了，所以它转而尝试其他方法。
    |'
- en: V Potential and Limitations
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 潜力与局限性
- en: NetPlay uses a similar architecture to Inner Monologue and DEPS, which have
    shown promising results for simple dynamic environments. Our experiments show
    that despite the immense complexity of NetHack, the agent can fulfill a wide range
    of tasks given enough context information. To our knowledge, this is the first
    NetHack agent to exhibit such flexible behavior. However, the benefits of the
    presented approach seem to diminish the more ambiguous a given task is, making
    tasks such as “Win the Game” impossible.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: NetPlay 使用了类似于 Inner Monologue 和 DEPS 的架构，这些架构在简单的动态环境中已显示出有希望的结果。我们的实验表明，尽管
    NetHack 极其复杂，但只要提供足够的上下文信息，代理可以完成广泛的任务。据我们了解，这是首个表现出如此灵活行为的 NetHack 代理。然而，所展示方法的好处似乎随着任务的模糊性增加而减少，使得像“赢得游戏”这样的任务变得不可能。
- en: A promising use case of the presented architecture is regression testing during
    game development. Game developers could test specific aspects of their game by
    providing NetPlay with detailed instructions on what to test. This approach could
    not only streamline the testing process, but it would also benefit from NetPlay’s
    flexibility, enabling the tests to adapt dynamically as the game evolves.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所展示架构的一个有前景的应用场景是游戏开发过程中的回归测试。游戏开发者可以通过向 NetPlay 提供详细的测试指令来测试游戏的特定方面。这种方法不仅可以简化测试过程，而且还将受益于
    NetPlay 的灵活性，使测试能够随着游戏的发展而动态适应。
- en: Given NetPlay’s proficiency when given detailed context information, an obvious
    extension to our approach would be granting the agent access to the NetHack Wikipedia.
    This could be done using a skill that accepts a query and adds the resulting information
    to the agent’s short-term memory. While we think this can improve the results
    at the cost of more LLM calls, finding the most relevant information for a given
    situation would be tricky. Instead, we recommend investing future research into
    automated methods for finding relevant context information, with a particular
    focus on finding the most successful past interactions as guidelines on how to
    play.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 NetPlay 在提供详细上下文信息时的高效，我们的方法的一个明显扩展是允许代理访问 NetHack 维基百科。这可以通过使用接受查询的技能并将结果信息添加到代理的短期记忆中来实现。虽然我们认为这可以改善结果，但代价是更多的
    LLM 调用，找到给定情况的最相关信息将是棘手的。相反，我们建议将未来的研究投资于自动化方法，以找到相关的上下文信息，特别是关注寻找最成功的过去互动作为游戏指导。
- en: A significant limitation of our approach lies in the predefined skills and the
    observation descriptions, which struggle to encompass the vast complexity of NetHack.
    Designing the agent to handle all potential edge cases proved challenging, as
    it is difficult to anticipate every scenario. While the premise of this approach
    is that the LLM can handle these edge cases, this is only true as long as we have
    a comprehensive description of the environment and flexible skills. In practice,
    achieving such a well-designed agent requires an ever-growing repertoire of skills
    and an observation description that grows infinitely. As such, another promising
    research direction is to use machine learning to replace the handcrafted components
    of the agent.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的一个显著限制在于预定义的技能和观察描述，它们难以涵盖NetHack的广泛复杂性。设计一个能够处理所有潜在边缘情况的智能体是具有挑战性的，因为很难预见到每种情况。尽管这一方法的前提是LLM能够处理这些边缘情况，但这只有在我们拥有全面的环境描述和灵活的技能时才成立。在实践中，达到这样的良好设计的智能体需要一个不断增长的技能库和一个无限增长的观察描述。因此，另一个有前景的研究方向是使用机器学习替代智能体的手工组件。
- en: VI Conclusion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this work, we introduce NetPlay, the first LLM-powered zero-shot agent for
    the challenging roguelike NetHack. Building upon an existing approach tailored
    for simpler dynamic environments, we extended its capabilities to address the
    complexities of NetHack. We evaluated the agent’s performance on the whole game
    and analyzed its behavior using various isolated scenarios.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了NetPlay，这是第一个用于具有挑战性的roguelike游戏NetHack的LLM驱动的零样本智能体。基于为更简单的动态环境量身定制的现有方法，我们扩展了其能力，以应对NetHack的复杂性。我们在整个游戏上评估了智能体的表现，并使用各种孤立场景分析了其行为。
- en: NetPlay demonstrates proficiency in executing detailed instructions but struggles
    with more ambiguous tasks, such as winning the game. Notably, a simple rule-based
    agent can achieve comparable performance in playing the game. NetPlay’s strength
    lies in its flexibility and creativity. Our experiments show that, given enough
    context information, NetPlay can perform a wide range of tasks. Moreover, by focusing
    its attention on a particular problem, NetPlay is adept at exploring a wide range
    of potential solutions but often with limited success due to a lack of explicit
    feedback guiding it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: NetPlay在执行详细指令方面表现出色，但在处理更模糊的任务时（如赢得游戏）则表现不佳。值得注意的是，简单的基于规则的智能体在玩游戏时可以达到类似的性能。NetPlay的优势在于其灵活性和创造力。我们的实验表明，给定足够的上下文信息，NetPlay可以执行广泛的任务。此外，通过将注意力集中在特定问题上，NetPlay擅长探索各种潜在解决方案，但由于缺乏明确的反馈指导，往往成功有限。
- en: VII Acknowledgements
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 致谢
- en: This work was supported by the EPSRC Centre for Doctoral Training in Intelligent
    Games & Games Intelligence (IGGI) (EP/S022325/1).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了EPSRC智能游戏与游戏智能（IGGI）（EP/S022325/1）博士培训中心的支持。
- en: This work was supported by Creative Assembly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了Creative Assembly的支持。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar,
    N. Barnes, and A. Mian, “A comprehensive overview of large language models,” 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar,
    N. Barnes, 和 A. Mian，“大型语言模型的综合概述”，2023年。'
- en: '[2] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine,
    K. Hausman, and B. Ichter, “Inner monologue: Embodied reasoning through planning
    with language models,” 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J.
    Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu,
    S. Levine, K. Hausman, 和 B. Ichter，“内心独白：通过语言模型进行具身推理和规划”，2022年。'
- en: '[3] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su,
    “Llm-planner: Few-shot grounded planning for embodied agents with large language
    models,” 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, 和 Y. Su，“Llm-planner:
    基于大型语言模型的少样本地面规划用于具身智能体”，2023年。'
- en: '[4] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng,
    Y. Yang, X. Ma, and Y. Liang, “Jarvis-1: Open-world multi-task agents with memory-augmented
    multimodal language models,” 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng,
    Y. Yang, X. Ma, 和 Y. Liang，“Jarvis-1: 具有记忆增强的多模态语言模型的开放世界多任务智能体”，2023年。'
- en: '[5] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu,
    X. Wang, Y. Qiao, Z. Zhang, and J. Dai, “Ghost in the minecraft: Generally capable
    agents for open-world environments via large language models with text-based knowledge
    and memory,” 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu,
    X. Wang, Y. Qiao, Z. Zhang, 和 J. Dai, “Minecraft中的幽灵：通过大型语言模型与文本知识和记忆的开放世界环境中的通用代理，”
    2023年。'
- en: '[6] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar,
    “Voyager: An open-ended embodied agent with large language models,” 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, 和 A.
    Anandkumar, “Voyager: 一个开放式的具身代理，配备大型语言模型，” 2023年。'
- en: '[7] K. Lorber, “Nethack home page.” [Online]. Available: [https://nethack.org/](https://nethack.org/)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] K. Lorber, “Nethack主页。” [在线]. 可用: [https://nethack.org/](https://nethack.org/)'
- en: '[8] “autoascend,” GitHub, 10 2023\. [Online]. Available: [https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] “autoascend，” GitHub, 2023年10月。 [在线]. 可用: [https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
- en: '[9] “Neurips 2021 - nethack challenge,” 10 2023\. [Online]. Available: [https://nethackchallenge.com/report.html](https://nethackchallenge.com/report.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] “Neurips 2021 - nethack挑战，” 2023年10月。 [在线]. 可用: [https://nethackchallenge.com/report.html](https://nethackchallenge.com/report.html)'
- en: '[10] H. Küttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette,
    and T. Rocktäschel, “The nethack learning environment,” *CoRR*, vol. abs/2006.13760,
    2020\. [Online]. Available: [https://arxiv.org/abs/2006.13760](https://arxiv.org/abs/2006.13760)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] H. Küttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette,
    和 T. Rocktäschel, “Nethack学习环境，” *CoRR*, vol. abs/2006.13760, 2020年。 [在线]. 可用:
    [https://arxiv.org/abs/2006.13760](https://arxiv.org/abs/2006.13760)'
- en: '[11] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro,
    F. Petroni, H. Küttler, E. Grefenstette, and T. Rocktäschel, “Minihack the planet:
    A sandbox for open-ended reinforcement learning research,” *CoRR*, vol. abs/2109.13202,
    2021\. [Online]. Available: [https://arxiv.org/abs/2109.13202](https://arxiv.org/abs/2109.13202)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro,
    F. Petroni, H. Küttler, E. Grefenstette, 和 T. Rocktäschel, “Minihack the planet:
    一个开放式强化学习研究的沙盒，” *CoRR*, vol. abs/2109.13202, 2021年。 [在线]. 可用: [https://arxiv.org/abs/2109.13202](https://arxiv.org/abs/2109.13202)'
- en: '[12] E. Hambro, S. Mohanty, D. Babaev, M. Byeon, D. Chakraborty, E. Grefenstette,
    M. Jiang, D. Jo, A. Kanervisto, J. Kim, S. Kim, R. Kirk, V. Kurin, H. Küttler,
    T. Kwon, D. Lee, V. Mella, N. Nardelli, I. Nazarov, N. Ovsov, J. Parker-Holder,
    R. Raileanu, K. Ramanauskas, T. Rocktäschel, D. Rothermel, M. Samvelyan, D. Sorokin,
    M. Sypetkowski, and M. Sypetkowski, “Insights from the neurips 2021 nethack challenge,”
    2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] E. Hambro, S. Mohanty, D. Babaev, M. Byeon, D. Chakraborty, E. Grefenstette,
    M. Jiang, D. Jo, A. Kanervisto, J. Kim, S. Kim, R. Kirk, V. Kurin, H. Küttler,
    T. Kwon, D. Lee, V. Mella, N. Nardelli, I. Nazarov, N. Ovsov, J. Parker-Holder,
    R. Raileanu, K. Ramanauskas, T. Rocktäschel, D. Rothermel, M. Samvelyan, D. Sorokin,
    M. Sypetkowski, 和 M. Sypetkowski, “来自 neurips 2021 nethack 挑战的见解，” 2022年。'
- en: '[13] maciej sypetkowski, “Autoascend – 1st place nethack agent for the nethack
    challenge at neurips 2021,” GitHub, 01 2024\. [Online]. Available: [https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] maciej sypetkowski, “Autoascend – neurips 2021 nethack挑战中获得第一名的nethack代理，”
    GitHub, 2024年1月。 [在线]. 可用: [https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
- en: '[14] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, “Describe, explain, plan
    and select: Interactive planning with large language models enables open-world
    multi-task agents,” 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Wang, S. Cai, A. Liu, X. Ma, 和 Y. Liang, “描述、解释、计划和选择：与大型语言模型进行互动规划使开放世界多任务代理成为可能，”
    2023年。'
- en: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    and A. Zeng, “Code as policies: Language model programs for embodied control,”
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    和 A. Zeng, “代码作为策略：用于具身控制的语言模型程序，” 2023年。'
- en: '[16] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu,
    L. Fan, and A. Anandkumar, “Eureka: Human-level reward design via coding large
    language models,” 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y.
    Zhu, L. Fan, 和 A. Anandkumar, “Eureka: 通过编程大型语言模型进行人类级别的奖励设计，” 2023年。'
- en: '[17] M. Klissarov, P. D’Oro, S. Sodhani, R. Raileanu, P.-L. Bacon, P. Vincent,
    A. Zhang, and M. Henaff, “Motif: Intrinsic motivation from artificial intelligence
    feedback,” 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Klissarov, P. D’Oro, S. Sodhani, R. Raileanu, P.-L. Bacon, P. Vincent,
    A. Zhang, 和 M. Henaff, “Motif: 来自人工智能反馈的内在动机，” 2023年。'
- en: '[18] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, “Reward design with language
    models,” 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Kwon, S. M. Xie, K. Bullard, 和 D. Sadigh, “使用语言模型进行奖励设计，” 2023年。'
- en: '[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou,
    “Chain of thought prompting elicits reasoning in large language models,” *CoRR*,
    vol. abs/2201.11903, 2022\. [Online]. Available: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le 和 D. Zhou，“思维链提示引发大语言模型的推理，”
    *CoRR*，卷 abs/2201.11903，2022 年。 [在线]. 可用: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
- en: '[20] H. Touvron and et al., “Llama 2: Open foundation and fine-tuned chat models,”
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. Touvron 等，“Llama 2: 开放基础和微调聊天模型，” 2023 年。'
