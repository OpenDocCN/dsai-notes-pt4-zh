<!--yml

category: 未分类

date: 2025-01-11 12:05:51

-->

# 通过可能性和通过率优先经验回放增强LLM代理的代码生成能力

> 来源：[https://arxiv.org/html/2410.12236/](https://arxiv.org/html/2410.12236/)

Yuyang Chen^($\dagger$1,2), Kaiyan Zhao^($\dagger$1), Yiming Wang³, Ming Yang³, Jian Zhang¹, Xiaoguang Niu¹

###### Abstract

当今，用于代码生成任务的基于Transformer的大型语言模型（LLM）通常应用抽样和过滤管道。由于代码生成任务中由于一个令牌错误引起的稀疏奖励问题，基于Transformer的模型会抽样冗余程序直到找到正确的程序，导致效率低下。为了克服这一挑战，我们在微调阶段引入了经验回放（ER），其中生成的代码和程序将被存储并将被重放，以便LLM代理有机会从过去的经验中学习。基于ER的精神，我们引入了一种称为BTP管线的新方法，该方法包括三个阶段：波束搜索抽样阶段、测试阶段和优先经验回放阶段。该方法利用代码模型收集的失败程序，并从重放缓冲区中重播具有高可能性和通过率优先值（P2Value）的程序，以提高效率。P2Value全面考虑了Transformer输出的可能性和通过率，并可以利用LLM收集的大多数程序无法通过任何测试的问题所造成的冗余资源。我们在多个LLM中经验性地应用了我们的方法，表明它提升了它们在代码生成任务中的性能，并超过了现有的基准线。

## 1 Introduction

近年来，大型语言模型（LLM）的发展取得了显著进展，像Transformer（Vaswani et al. [2017](https://arxiv.org/html/2410.12236v1#bib.bib27)）和Llama（Touvron et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib26)）等模型在多个领域得到了应用。一个特别的趋势是将LLM用于自动代码生成任务。像WizardCode（Luo et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib18)）和StarCode（Li et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib14)）等模型被开发出来以应对这些任务。为了评估LLM在代码生成中的有效性和表现，已经建立了各种基准测试。例如，APPS（Hendrycks et al. [2021](https://arxiv.org/html/2410.12236v1#bib.bib11)）广泛用于代码模型的评估，而Code-Contests（Li et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib15)）已成为竞赛级编程任务的标准。在所有模型中，变换器在代码翻译、代码补全和挑战性问题解决等基准任务中表现出显著的成功（Svyatkovskiy et al. [2020](https://arxiv.org/html/2410.12236v1#bib.bib25)）。一些基于变换器的管道甚至在困难任务中取得了显著成果（Zhang et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib34)）。

然而，传统的基于变换器（transformer）的管道，由采样和过滤阶段组成，由于其结构原因存在明显的缺点。在代码生成任务中，一个突出的问题是低效率导致的冗余资源的显著浪费。具体来说，当任务作为输入提供时，代码代理从预训练的基于变换器的LLM中采样大量程序，并通过公共测试集进行测试，在那里根据通过率进行测试和过滤。低效率发生在大多数程序由于单个错误的标记而未能通过测试的情况下（Zhang et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib34)）。因此，模型必须采样许多错误的程序，才能找到一个完全准确的程序，这导致冗余资源的浪费，包括未被重用的不成功程序。

然而，通过某些测试失败的程序并不一定没有价值。相反，大多数预训练的大型语言模型（LLM）在大规模语料库上得到了很好的训练，这意味着它们生成的程序几乎是准确的，但可能因为一些小错误而失败。因此，如果我们能够减少这些宝贵资源的浪费，将节省时间并提高效率。

为了利用这些冗余资源中的价值并提高效率，我们引入了经验重放（Experience Replay，ER），这是一种存储LLMs采样程序的缓冲区，同时记录每个程序的P2Value（可能性和通过率值），我们将其视为程序的价值。P2Value综合考虑了变换器输出的可能性和通过率。一方面，在公共测试集中通过率较高的程序在特定任务中表现良好；另一方面，输出可能性较高的程序根据预训练变换器的计算结果被认为具有更高的价值。基于ER，我们提出了一种名为BTP管道的新方法，它由三个阶段组成：束搜索采样、测试和优先经验重放。核心算法PPER（P2Value优先经验重放）利用束搜索采样并将程序存储在ER中，然后根据程序的P2Value依赖的概率在ER中进行重放。

我们通过实验证明，我们的管道在代码生成任务中提升了大语言模型（LLMs）的性能，无论训练数据是自生成的还是由更高质量的模型生成的，均优于原始模型。更具体地说，我们的贡献如下：

+   •

    首先，我们提出了一种新型算法——BTP管道，它由束搜索采样阶段、测试阶段和经验重放阶段组成，用于微调LLMs。

+   •

    我们通过实验证明，我们的算法不仅在更好的LLMs生成数据以微调普通LLMs的场景下表现良好，在LLMs自行采样程序进行自我微调的场景下也同样有效。也就是说，我们的BTP管道是通用的。

+   •

    最后，我们引入了一种名为经验重放缓冲区（experience replay buffer）的新型缓冲区，它在微调过程中非常高效。未来，我们可以将其与其他算法结合，找到更高效的微调方式。

## 2 相关工作

考虑到我们管道中ER和LLMs的关联，接下来我们将分别介绍它们。

代码生成的LLM：我们的工作与代码生成的LLM密切相关。近年来，像GPT-4（OpenAI [2023](https://arxiv.org/html/2410.12236v1#bib.bib19)）、Llama（Touvron et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib26)）、PaLM（Chowdhery et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib6)）、Chinchilla（Hoffmann et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib12)）等高性能LLM在不同领域相继出现。特别是，我们的工作基于用于代码生成任务的transformer（Vaswani et al. [2017](https://arxiv.org/html/2410.12236v1#bib.bib27)）（Roziere et al. [2020](https://arxiv.org/html/2410.12236v1#bib.bib22)）。像BERT（Devlin et al. [2019](https://arxiv.org/html/2410.12236v1#bib.bib7); Feng et al. [2020](https://arxiv.org/html/2410.12236v1#bib.bib9); Guo et al. [2020](https://arxiv.org/html/2410.12236v1#bib.bib10)）、T5（Raffel et al. [2020](https://arxiv.org/html/2410.12236v1#bib.bib21)）、GPT-2（Radford et al. [2019](https://arxiv.org/html/2410.12236v1#bib.bib20)）、Codex（Chen et al. [2021a](https://arxiv.org/html/2410.12236v1#bib.bib4)）、CodeT5（Wang et al. [2021](https://arxiv.org/html/2410.12236v1#bib.bib29)）、StarCode（Li et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib14)）和WizardCoder（Luo et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib18)）等代码模型已成为代码理解与生成的核心工具。同时，为了评估代码模型的性能，创建了不同的基准测试，如APPS（Hendrycks et al. [2021](https://arxiv.org/html/2410.12236v1#bib.bib11)）、CODE-CONTESTS（Li et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib15)）、OpenOrca（Lian et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib16)）和HumanEval（Chen et al. [2021b](https://arxiv.org/html/2410.12236v1#bib.bib5)）。此外，（Roziere et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib23)）为无监督代码翻译任务构建了训练数据集，（Ellis et al. [2019](https://arxiv.org/html/2410.12236v1#bib.bib8)）为训练RL代理构建了来自不同特定领域的测试用例。此外，许多新方法已经在代码生成中提出。例如，（Chen et al. [2021b](https://arxiv.org/html/2410.12236v1#bib.bib5)）通过微调强大的预训练LLM来索引知识并优化代码完成的性能。（Austin et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib2)）总结了LLM可以应用于代码生成任务，（Wei et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib30)）引入了Chain-of-Thought（CoT）提示，鼓励LLM逐步思考并减少错误率。

基于强化学习的代码生成： (Bunel et al. [2018](https://arxiv.org/html/2410.12236v1#bib.bib3)) 声称，代码生成任务可以分解为一系列决策问题，这些问题类似于强化学习中的问题定义。这意味着可以将强化学习算法应用于变换器，充分利用其序列决策能力。例如，(Zhang et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib34)) 在代码生成任务中将蒙特卡洛树搜索（MCTS）与变换器结合。在这种方法中，MCTS 用于通过模拟不同的代码路径来探索潜在的代码序列，并基于一个衡量代码正确性和效率的奖励函数评估它们。这使得模型在推理过程中能够选择最有前景的代码序列（Yang et al. [2024a](https://arxiv.org/html/2410.12236v1#bib.bib32)）。类似地，(Le et al. [2022](https://arxiv.org/html/2410.12236v1#bib.bib13)) 通过将生成程序的正确性框架化为奖励最大化问题（这是强化学习中的常见目标），优化生成程序的正确性（Yang et al. [2024b](https://arxiv.org/html/2410.12236v1#bib.bib33)）。他们采用策略梯度方法，通过采样代码序列、估算奖励（例如，程序正确性）（Wang et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib28)）并更新模型，从而逐步改进变换器模型的策略，以增加未来迭代中生成正确代码序列的概率。这些方法展示了强化学习的探索-利用（Yang et al. [2023](https://arxiv.org/html/2410.12236v1#bib.bib31)）机制如何与变换器相结合，通过不仅仅预测下一个标记，而是基于累积奖励优化整个序列，从而增强代码生成模型的性能。

经验重放： (Lin [1992](https://arxiv.org/html/2410.12236v1#bib.bib17)) 首次提出了经验重放的概念，建议代理可以将其经验存储在缓冲区中，然后从这个缓冲区中进行抽样，以打破连续观察之间的时间相关性，从而稳定学习过程。通过多次重放这些经验，代理可以提高样本效率，因为它可以从在初始训练阶段可能被遗漏的过去经验中学习。

回顾经验重放（HER）(Andrychowicz et al. [2017](https://arxiv.org/html/2410.12236v1#bib.bib1))通过解决目标条件强化学习（RL）中的稀疏奖励问题进行了扩展。在HER中，失败的回合之后，导致失败的过渡会存储在重放缓冲区。在训练过程中，这些过渡会被重新标记为与原本目标不同的目标，特别是那些在失败的回合中实际实现的目标。通过为这些新目标分配新的奖励，HER使得智能体能够从原本未达到目标的回合中学习，从而有效地将失败转化为学习机会。  

与此同时，优先经验重放（PER）(Schaul et al. [2016](https://arxiv.org/html/2410.12236v1#bib.bib24))提出了并非所有过渡对学习同等重要的观点。PER通过按时间差分（TD）误差的大小抽样过渡来修改经验重放机制，TD误差表示过渡的惊讶性或意外性。高TD误差的过渡，即智能体的预测与实际结果差异较大的过渡，更具信息性，因此会被更频繁地重放。这种方法确保了智能体集中精力从最有信息量的经历中学习，通过减少在不太有用的过渡上浪费的时间，加速了学习过程的收敛。  

## 3 方法论  

![参考标题](img/f04ae86dc49c9284bf9b21c08af76e4d.png)  

图1：在不同$\alpha$值下，使用BTP的GPT2-Wizard的通过率  

本节首先简要介绍BTP管道的框架，该框架由束搜索采样、测试和PPER组成。接着，我们将详细说明所提框架的细节。图1提供了BTP管道的完整过程。  

### 3.1 BTP管道  

如图2所示，大多数以往的工作采用传统框架，其中变换器模型利用采样和过滤管道。在采样阶段，LLM仅根据

|  | $\displaystyle P(Y | X)$ | $\displaystyle=P(y_{1} | X)P(y_{2} | X,y_{1})\dots$ |  | (1) |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  | $\displaystyle\quad P(y_{T} | X,y_{1},y_{2},\dots,y_{T-1})$ |  |   |

其中X是输入，$y_{i}$是第i个时间步生成的token，$P(y_{i}|X,y_{1},y_{2},\dots,y_{i-1})$是给定X，$y_{1}$,…,$y_{i-1}$的条件概率。  

特别是在代码生成任务中，X是提示语的代码任务。传统的LLM仅使用贪心算法，在每个时间步i最大化$P(y_{i}|X,y_{1},y_{2},\dots,y_{i-1})$，直到生成完整的代码序列$y_{1}y_{2}\dots y_{T}$  

在过滤阶段，代码序列被发送到公共测试集。然而，由于某些令牌错误，可能会导致失败。一个主要原因是贪心算法忽略了其他概率较低的序列中隐藏的值。因此，我们在我们的管道中引入了束搜索采样。

### 3.2 束搜索采样阶段

如图 4 所示，在每个时间步，代码模型会找到最有可能的前 k 个候选序列，并将它们保存在一个名为“beams”的容器中。在每一步 i，所有候选序列会从词汇表中探索所有可能的令牌，生成不同的新序列。然后，LLM 会根据组合概率选择前 k 个序列，并将它们添加到新束中以供下一时间步使用。

|  | $P(y_{1}y_{2}\dots y_{i})=P(y_{1}y_{2}\dots y_{i-1})P(y_{i})$ |  | (2) |
| --- | --- | --- | --- |

重复该过程，直到模型找到一个完整的程序 $y_{1}y_{2}\dots y_{T}$，其中 T 是结束时间步。将前 k 个程序 $t_{1}t_{2}\dots t_{k}$ 及其可能性 $P(t_{1})P(t_{2})\dots P(t_{k})$、代码任务 X 及其对应的测试集 S 存储在经验回放缓冲区中，格式如下：

|  | $T=(X,S,t_{i},P(t_{i}))$ |  | (3) |
| --- | --- | --- | --- |

### 3.3 测试阶段

在测试阶段，模型将顺序地从 T 中取出每个元组，并在测试集 S 的每个测试用例中测试 $t_{i}$，并计算通过率 $pass\_rate_{i}$：

|  | $\text{pass\_rate}_{i}=\sum_{forS_{k}\in S}\mathbf{1}(\text{if }t_{i}\text{ % pass }S_{k})$ |  | (4) |
| --- | --- | --- | --- |

通过率以及组合概率将存储在经验回放缓冲区（ER）中。

|  | $ER_{i}=(X,S,t_{i},P(t_{i}),pass\_rate_{i})$ |  | (5) |
| --- | --- | --- | --- |

### 3.4 PPER 阶段

在可能性和通过率优先经验回放（PPER）阶段，代码模型将使用我们称之为 PPER 的方法进行微调。具体来说，存储在回放缓冲区中的程序将根据其可能性和通过率的概率进行采样。然后，采样的程序将用于构建一个小批量，用于微调代码模型。

#### P2Value

在我们的 PPER 方法中，最重要的因素是建立一个标准来定义 ER 中每个程序的优先级。虽然很难确定一个准确的优先级测量标准，但一个合理的替代方案是考虑 P2Value，它结合了变换器的输出概率和在测试集上的通过率。特别是，对于从 ER 中采样的任何元组 $ER_{i}=(X,S,t_{i},P(t_{i}),pass\_rate_{i})$，P2Value 的计算方式如下：

|  | $\text{P2Value}=\alpha\cdot P(t_{i})+(1-\alpha)\cdot pass\_rate_{i}$ |  | (6) |
| --- | --- | --- | --- |

其中$\alpha$是一个参数，用于确定可能性和通过率的权重。它越接近1，可能性就越重要。相应地，原始代码模型更偏好的采样程序将在微调过程中产生更大的影响。相反，通过最多测试集的程序，但原始代码模型的偏好较低，将在微调过程中承载更多的权重。

我们考虑这种公式的原因是，具有更高通过率的程序更适合且对特定代码任务更有价值。然而，由于可能存在低通过率，甚至接近零的情况，我们考虑将可能性应用于程序的价值评估。显然，一个被预训练LLM偏好的程序在LLM的语料库中具有更高的价值。而如何平衡它们的权重正是我们设定参数$\alpha$的原因。

#### 随机优先级采样

从ER中均匀地采样程序或使用整个ER对LLM进行微调是简单的。然而，给予较高值的程序更大的被选中概率更为高效。因此，我们引入了一种随机采样方法，以确保ER中存储的每个程序都按其优先级以严格单调的方式进行采样。这种方法增加了采样高优先级程序的可能性，同时仍保持固定的非零概率来采样最低优先级的程序，确保ER中的每个轨迹都能得到利用。

具体来说，我们在公式5中定义了转换i的采样概率。

|  | $P(i)=\frac{p_{i}^{\alpha}}{\sum_{k}p_{k}^{\alpha}}$ |  | (7) |
| --- | --- | --- | --- |

其中pi是程序$t_{i}$的优先级。索引$\alpha$决定了优先级的级别，当$\alpha$ = 0时，我们考虑两种方式来定义pi。在第一种情况下，我们直接将$p_{i}$定义为P2Value。这直观地描述了采样可能性与优先级之间的关系。

然而，这种方法对与平均值显著偏离的点非常敏感。例如，P2Value值远高的轨迹将被过于频繁地采样。

为了解决这个问题，我们引入了第二个定义

|  | $p_{i}=\frac{1}{\text{rank}(i)}$ |  | (8) |
| --- | --- | --- | --- |

其中rank(i)表示程序在所有轨迹中的优先级排名。与之前的方法相比，这种方法有几个优势。首先，它遵循幂律分布，意味着大部分数据集中在重心附近，而少部分数据分布在极大或极小值周围。此外，它更加稳健，对显著偏离平均值的点不那么敏感。例如，即使轨迹的P2Value大幅下降，最低排名轨迹的P(i)也不会有显著变化。

值得注意的是，代码模型的输出可能性非零，这意味着P2Value也非零，因此不需要常数来防止零概率。

算法1 BTP管道

1:$T$: 代码模型；$beam$: 在束搜索采样阶段存储程序的缓冲区；$k$: 束的大小；$X_{\text{set}}$: 任务集，包含测试集；$ER$: 经验回放缓冲区；$batch$: 用于微调$T$的程序小批量；$n$: 小批量的大小2:束搜索采样阶段3:对每一个$(X,S)$在$X_{\text{set}}$中执行4:     $beam\leftarrow\text{Beam\_search}(X,k)$5:     对每一个$(t,P)$在$beam$中执行6:         将$(X,S,t,P)$存入$ER$7:     结束循环8:结束循环9:测试阶段10:对每一个$(X,S,t,P)$在$ER$中执行11:     在$S$中测试$t$并获得$pass\_rate$12:     将$(X,S,t,P)$替换为$(X,S,t,P,pass\_rate)$13:结束循环14:PPER阶段15:对$i\leftarrow 1$到$n$执行16:     以P2Value的概率从$ER$中抽取$(X,S,t,P,pass\_rate)$17:     将$(X,S,t)$存入$batch$18:结束循环19:用$batch$微调$T$

## 4 实验

|  | 通过率 (%) | 准确率 (%) |
| --- | --- | --- |
|  | APPS 初级 | APPS 中级 | APPS 复杂 | APPS 混合 | APPS 初级 | APPS 中级 | APPS 复杂 | APPS 混合 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| APPS GPT-2 |  |  |  |  |  |  |  |  |
| GPT-2 | 12.37 | 10.67 | 4.33 | 7.30 | 5.30 | 3.30 | 1.32 | 2.72 |
| GPT-2-GPT4 | 50.79 | 43.27 | 37.68 | 40.91 | 25.84 | 19.87 | 15.34 | 20.21 |
| GPT-2-GPT3.5 | 41.68 | 37.62 | 28.59 | 32.25 | 19.42 | 16.25 | 12.21 | 15.63 |
| GPT-2-Llama | 35.82 | 31.14 | 24.37 | 27.60 | 14.35 | 11.40 | 8.27 | 11.69 |
| GPT-2-Wizard | 33.76 | 29.08 | 22.31 | 25.54 | 12.29 | 9.34 | 6.21 | 9.63 |
| APPS GPT-Neo |  |  |  |  |  |  |  |  |
| GPT-Neo | 14.32 | 9.80 | 6.39 | 5.73 | 6.70 | 2.00 | 2.10 | 3.31 |
| GPT-Neo-GPT4 | 51.23 | 46.39 | 38.89 | 42.12 | 26.88 | 23.34 | 17.12 | 20.54 |
| GPT-Neo-GPT3.5 | 39.23 | 34.39 | 26.89 | 30.12 | 14.88 | 11.34 | 5.12 | 8.54 |
| GPT-Neo-Llama | 38.24 | 33.40 | 25.90 | 29.13 | 13.89 | 10.35 | 4.13 | 7.55 |
| GPT-Neo-Wizard | 35.83 | 30.99 | 23.49 | 26.72 | 11.48 | 8.54 | 2.32 | 5.74 |

表1：”更好的模型帮助微调普通模型“实验结果。在表的上下部分，我们展示了GPT-2和GPT-Neo的表现，以及它们在被更好的模型（包括GPT-4-turbo, GPT-3.5-turbo, CodeLlama-34B, WizardCoder-34B）采样的程序微调后表现如何。

在本节中，我们通过实验测量了我们的BTP管道的有效性。我们顺序进行实验，以验证以下猜想。

|  | 通过率 (%) | 准确率 (%) |
| --- | --- | --- |
|  | APPS 初级 | APPS 中级 | APPS 复杂 | APPS 混合 | APPS 初级 | APPS 中级 | APPS 复杂 | APPS 混合 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GPT-2 | 12.37 | 10.67 | 4.33 | 7.30 | 5.30 | 3.30 | 1.32 | 2.72 |
| GPT-2-2 | 15.36 | 11.23 | 5.11 | 8.12 | 4.25 | 2.78 | 2.25 | 3.92 |
| GPT-Neo | 14.32 | 9.80 | 6.39 | 5.73 | 6.70 | 2.00 | 2.10 | 3.31 |
| GPT-Neo-Neo | 14.02 | 9.23 | 7.25 | 5.39 | 6.32 | 2.13 | 2.92 | 3.72 |
| WizardCoder | 45.24 | 41.56 | 37.46 | 40.24 | 20.13 | 17.92 | 14.92 | 16.29 |
| WizardCoder-Wizard | 45.25 | 40.23 | 35.36 | 41.25 | 22.25 | 15.31 | 13.92 | 17.55 |

表 2： ”模型自我微调”实验结果。我们展示了GPT-2、GPT-Neo、WizardCoder的性能，以及它们在通过自己采样的程序微调后的表现。

1：我们的BTP管道帮助代码模型在使用从更好的模型中采样的程序微调标准模型的情况下生成更好的程序。

2：我们的BTP管道帮助代码模型在使用从代码模型本身采样的程序微调模型的情况下生成更好的程序。

3：通过我们的BTP管道微调的最佳代码模型，与基线方法相比具有竞争力。

4：有没有更好的方式来最大化我们的BTP管道的效果？（例如，在ER中混合采样的程序）

### 4.1 实验设置

#### 数据集

近年来，各种开源编程数据集涌现，为评估代码模型提供了坚实的基础。为了确保我们提出的BTP管道的鲁棒性和普适性，我们将其应用于对多个最先进的代码模型进行微调，并在一系列流行的基准数据集上对其进行评估，包括来自AlphaCode的CodeContests (Li等人[2022](https://arxiv.org/html/2410.12236v1#bib.bib15))、APPS (Hendrycks等人[2021](https://arxiv.org/html/2410.12236v1#bib.bib11))和HumanEval (Chen等人[2021b](https://arxiv.org/html/2410.12236v1#bib.bib5))。对于HumanEval，它包含164个编程问题，带有函数签名、文档字符串、主体和单元测试，我们将每个问题的所有单元测试作为测试集，而剩余的描述则作为代码生成任务。对于CodeContests，我们类似地将问题描述视为代码生成任务，并将所有公共和私有测试用例合并为统一的测试集。对于APPS，由于公共和私有测试用例没有区分，我们将所有测试用例聚合为每个代码生成任务的综合测试集。

#### 模型

我们将实验中使用的模型分为两个不同的组。第一组包括进行微调的模型，如GPT-2和GPT-Neo。第二组包括用于生成代码样本的模型。在这一类别中，我们探讨了两种场景：第一种，我们使用先进的代码模型，如GPT-4-turbo（OpenAI [2023](https://arxiv.org/html/2410.12236v1#bib.bib19)）、GPT-3.5-turbo、CodeLlama-34B（Roziere等人[2022](https://arxiv.org/html/2410.12236v1#bib.bib23)）和WizardCoder-34B（Luo等人[2023](https://arxiv.org/html/2410.12236v1#bib.bib18)）；第二种，我们使用与微调时相同的代码模型。

|  | 通过率 (%) | 准确率 (%) |
| --- | --- | --- |
|  | APPS Intro. | APPS Inter. | APPS Comp. | APPS Mixed | APPS Intro. | APPS Inter. | APPS Comp. | APPS Mixed |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GPT-Neo-GPT4 | 51.23 | 46.39 | 38.89 | 42.12 | 26.88 | 23.34 | 17.12 | 20.54 |
| WizardCoder | 45.24 | 41.56 | 37.46 | 40.24 | 29.73 | 25.36 | 21.25 | 23.34 |
| GPT-4-turbo | 84.24 | 80.36 | 78.46 | 81.25 | 65.25 | 60.45 | 53.59 | 59.27 |
| GPT-3.5-turbo | 55.75 | 52.26 | 51.37 | 52.64 | 45.47 | 37.41 | 30.29 | 38.25 |
| CodeLlama | 53.45 | 51.26 | 49.24 | 52.27 | 28.35 | 25.92 | 23.25 | 26.78 |

表3：在不同难度级别下，对比微调代码模型与基准模型在APPS数据集上的表现。

|  | 通过率 (%) | 准确率 (%) |
| --- | --- | --- |
|  | APPS Mixed | CodeContests | HumanEval | APPS Mixed | CodeContests | HumanEval |
| --- | --- | --- | --- | --- | --- | --- |
| GPT-2 GPT4 | 43.27 | 35.38 | 29.25 | 5.39 | 4.25 | 4.11 |
| CodeContests | 36.32 | 46.93 | 38.25 | 4.82 | 6.10 | 5.25 |
| HumanEval | 31.92 | 25.21 | 47.93 | 2.62 | 4.25 | 8.25 |
| Mixed | 40.25 | 42.98 | 41.52 | 5.21 | 5.87 | 6.23 |
| GPT-Neo GPT4 | 45.22 | 38.18 | 30.92 | 6.48 | 5.25 | 5.59 |
| CodeContests | 39.11 | 48.52 | 39.26 | 5.29 | 7.20 | 6.23 |
| HumanEval | 33.74 | 27.93 | 50.92 | 4.92 | 6.24 | 10.29 |
| Mixed | 42.85 | 45.21 | 43.58 | 9.24 | 8.53 | 8.39 |

表4：在不同数据集上，比较微调过的GPT-4-turbo与基准模型的表现。

#### 超参数优化

我们进行了一系列实验，以确定最有效的超参数。最初，我们研究了束搜索采样是否在效果上优于简单采样。结果证实了束搜索采样的优越性，这促使我们进一步实验，寻找束搜索参数$k$的最优值。在平衡效果和资源消耗后，我们选择了$k=3$作为主要实验的参数，基于程序的概率采样排名前3的程序。详细的结果和分析已在附录A中提供。

此外，我们还探讨了在PPER阶段超参数$\alpha$的影响。然而，我们的研究结果表明，最优的$\alpha$值在不同的模型和数据集之间存在差异。为了解决这个问题，我们在多个数据集上进行了针对性的实验，以确定每种场景下表现最好的$\alpha$值。这些实验结果及其综合分析已在附录B和附录C中展示。

### 4.2 使用BTP管道对代码模型进行微调

在本节中，我们通过将实验分为四个不同的部分，系统地回答了之前提出的四个关键问题。

#### 利用先进模型提升基准模型

为了验证我们的第一个假设，记作C1，我们进行了一项实验，测试通过利用我们提出的BTP（更好的变压器编程）管道，基线模型的表现是否能得到显著提升。具体而言，我们使用了APPS数据集，该数据集分为三个难度级别：入门级、中级和竞赛级任务。这些任务旨在评估模型在不同编程挑战中的能力。

如表[1](https://arxiv.org/html/2410.12236v1#S4.T1 "表 1 ‣ 4 实验 ‣ 通过可能性和通过率优先的经验回放增强LLM代理的代码生成能力")所述，我们将APPS数据集的三个部分合并成一个单一的、综合的数据集，称为APPS混合数据集。这个合并后的数据集用于训练和评估模型，确保它们接触到多样化的任务难度，从而提供对其泛化能力的稳健评估。

在本次实验中，我们选择了四个最先进的基于变压器的模型：GPT-4-turbo、GPT-3.5-turbo、CodeLlama-34B和WizardCoder-34B。这些模型的任务是生成示例程序，随后这些程序被用来微调两个基线模型：GPT-2和GPT-Neo。微调过程涉及使用每个先进模型生成的示例程序，创建基线模型的八个微调版本，具体命名如下：

我们使用了四个先进的变压器模型来生成示例程序，随后这些程序被用来微调两个基线模型。具体来说，GPT-4-turbo、GPT-3.5-turbo、CodeLlama-34B和WizardCoder-34B模型被用来生成样本，这些样本随后用于微调GPT-2和GPT-Neo。这个过程产生了八个微调后的模型：分别为使用GPT-4-turbo、GPT-3.5-turbo、CodeLlama-34B和WizardCoder-34B样本微调的GPT-2，命名为GPT-2-GPT4、GPT-2-GPT3.5、GPT-2-Llama和GPT-2-Wizard；类似地，GPT-Neo也使用这四个模型的样本进行了微调，结果得到了GPT-Neo-GPT4、GPT-Neo-GPT3.5、GPT-Neo-Llama和GPT-Neo-Wizard。

在微调过程后，我们评估了这些模型在APPS数据集的三个不同部分以及组合后的APPS混合数据集上的表现。目标是评估微调模型在不同复杂度的代码生成任务中的表现改进程度。

结果如表[2](https://arxiv.org/html/2410.12236v1#S4.T2 "Table 2 ‣ 4 Experiments ‣ Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay")所示，经过微调的模型在性能上相比原始未修改版本有了显著提升。这个改进在APPS数据集的所有部分中都有一致的表现，强调了我们BTP流程的有效性。通过引入用于程序采样的高级模型，我们显著增强了基础变换器模型（如GPT-2和GPT-Neo）的能力。

这些发现强烈表明，即使是相对简单的模型，在训练过程中接触到更先进的模型后，也能取得显著的性能提升，从而使它们在复杂的代码生成任务中表现更好。

#### 通过模型自我微调实现自我提升

在我们的第二个实验中，我们旨在验证第二个假设C2，该假设认为模型可以通过BTP流程中的自我采样方法提高自身性能。在此实验中，我们再次使用了APPS数据集，按照入门级、中级和竞赛级任务进行划分，以便全面评估模型在不同难度级别上的表现。

与我们的第一个实验一样，我们将APPS数据集的三部分合并为一个统一的数据集，命名为APPS混合数据集。这确保了每个模型都在多样化的任务集上进行训练和评估，为自我微调方法提供了严格的测试。

我们为此实验选择了三种模型：GPT-2、GPT-Neo和WizardCoder-34B。每个模型都从APPS混合数据集中采样程序，并将采样的程序用于微调生成这些程序的同一模型。这个过程有效地创建了一个反馈循环，允许模型通过自己的生成输出来优化自身能力。

由此产生的自我微调模型命名如下：

GPT-2、GPT-Neo和WizardCoder-34B都使用它们自己采样的程序进行了微调。这个自我微调过程产生了以下模型：GPT-2-2，即使用自己采样的程序微调的GPT-2；GPT-Neo-Neo，即使用自己采样的程序微调的GPT-Neo；以及WizardCoder-Wizard，即使用自己采样的程序微调的WizardCoder-34B。

然后，我们在APPS数据集的不同部分以及APPS混合数据集上评估了这些自我微调模型，以确定这种自我采样和微调方法的有效性。

结果如表1所示，尽管性能提升是适度的，但大多数任务上都有一致的正向趋势。这表明，即使模型使用自身生成的程序进行微调，BTP管道仍然能够提升模型性能。这项实验支持了模型可以通过自我引导学习逐步提高能力的观点，突显了基于变换器的模型中自我改进机制的潜力。

#### 最佳BTP生成模型与基准模型的对比分析

在所有通过BTP管道生成的微调代码模型中，GPT-Neo-GPT4展示了最佳的整体性能，如表[3](https://arxiv.org/html/2410.12236v1#S4.T3 "Table 3 ‣ Models ‣ 4.1 Experiment Settings ‣ 4 Experiments ‣ Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay")所示。为了进一步评估我们方法的有效性，我们进行了GPT-Neo-GPT4与其他基准模型之间的对比分析，结果如表[4](https://arxiv.org/html/2410.12236v1#S4.T4 "Table 4 ‣ Models ‣ 4.1 Experiment Settings ‣ 4 Experiments ‣ Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay")所示。尽管GPT-Neo-GPT4的表现未超过最先进的基准模型，但它相较于原始表现有显著提升，并缩小了与基准模型之间的性能差距。

这项对比分析强调了BTP管道能够提升基准模型性能的能力，使它们更接近最先进模型，尽管仍存在一些性能差距。

#### 优化BTP管道以实现最大效果

为了回答我们的第四个问题（Q4），我们探索了不同的策略，以最大化BTP管道的效果。具体来说，我们进行了实验，使用GPT-4-turbo从四个不同的数据集中采样程序：仅APPS、仅CodeContests、仅HumanEval，以及这些数据集的混合体。然后，我们在BTP管道内使用这些采样程序对GPT-2和GPT-Neo进行了微调，并随后在这三个数据集上测试了这些微调后的模型。

结果如表[4](https://arxiv.org/html/2410.12236v1#S4.T4 "Table 4 ‣ Models ‣ 4.1 Experiment Settings ‣ 4 Experiments ‣ Enhancing LLM Agents for Code Generation with Possibility and Pass-rate Prioritized Experience Replay")所示，当使用混合数据集对BTP管道中的程序进行采样时，最终微调的模型在更多任务上显示出比单一数据集微调的模型更好的表现。然而，这些模型在特定数据集上的性能可能无法达到那些仅在该数据集上进行微调的模型的水平。

这些发现表明，改变用于程序采样的数据集是一种可行的策略，可以增强 BTP 流水线的整体效果。通过在微调过程中加入多样化的任务，模型可以在不同的代码生成任务中实现更好的泛化能力和性能。

## 5 结论

在代码生成任务中，大型语言模型（LLMs）通常需要生成大量的程序以找到一个完全正确的程序，因为即便是一个错误的符号也可能导致测试失败。因此，许多生成的程序都被浪费了。

为了利用这些资源并提高效率，在这项工作中，我们提出了一种新颖的算法——BTP 流水线，它将束搜索采样与优先经验回放相结合，用于微调 LLM。我们通过实验证明了该算法对多种 LLM 的微调效果，并发现其相较于以前的模型有所提升。我们还展示了该算法不仅在使用更好的代码模型采样程序来增强标准代码模型的场景中有效，而且在代码模型利用自己采样的程序来增强自身的场景中也同样有效。

除了在代码生成任务中提升 LLM 性能外，我们认为我们的 BTP 流水线对于增强通用 LLM 也具有潜力，特别是在 LLM 生成的结果难以通过测试的情况下。这项工作的一个关键限制是其依赖于代码任务及相应的测试用例。测试用例较少的任务通常会导致接近零的通过率，这可能会影响我们算法的有效性。在未来的工作中，我们计划探索类似的测试集，并扩展可用的测试集以解决这一限制。

## 参考文献

+   Andrychowicz 等（2017）Andrychowicz, M.; 等. 2017. 事后经验回放. 在 *第31届国际神经信息处理系统会议（NeurIPS）论文集* 中, 5048–5058. NeurIPS.

+   Austin 等（2022）Austin, J.; 等. 2022. 使用大型语言模型进行程序合成. 在 *2022年国际学习表征会议（ICLR）论文集* 中. ICLR.

+   Bunel 等（2018）Bunel, R.; 等. 2018. 利用语法和强化学习进行神经程序合成. 在 *2018年国际学习表征会议（ICLR）论文集* 中. ICLR.

+   Chen 等（2021a）Chen, M.; 等. 2021a. Codex: 评估训练在代码上的大型语言模型. 在 *2021年自然语言处理实证方法会议（EMNLP）论文集* 中, 6730–6736. 计算语言学协会.

+   Chen 等（2021b）Chen, M.; 等. 2021b. 评估训练在代码上的大型语言模型. 在 *2021年自然语言处理实证方法会议（EMNLP）论文集* 中, 6730–6736. 计算语言学协会.

+   Chowdhery et al. (2022) Chowdhery, A.; et al. 2022. PaLM：通过Pathways扩展语言建模. 载于*神经信息处理系统进展（NeurIPS）论文集*，NeurIPS。

+   Devlin et al. (2019) Devlin, J.; et al. 2019. BERT：用于语言理解的深度双向变换器预训练. 载于*2019年北美计算语言学协会会议（NAACL）论文集*，4171–4186. 计算语言学协会出版。

+   Ellis et al. (2019) Ellis, K.; et al. 2019. 合成程序输入语法. 载于*2019年国际学习表示会议（ICLR）论文集*，ICLR。

+   Feng et al. (2020) Feng, Z.; et al. 2020. CodeBERT: 一种用于编程和自然语言的预训练模型. 载于*2020年自然语言处理实证方法会议（EMNLP）论文集*，1536–1547. 计算语言学协会出版。

+   Guo et al. (2020) Guo, D.; et al. 2020. GraphCodeBERT: 使用数据流进行代码表示的预训练. 载于*2020年自然语言处理实证方法会议（EMNLP）论文集*，1548–1559. 计算语言学协会出版。

+   Hendrycks et al. (2021) Hendrycks, D.; et al. 2021. 使用APPS衡量编程挑战能力. 载于*第35届神经信息处理系统会议（NeurIPS）论文集*，NeurIPS。

+   Hoffmann et al. (2022) Hoffmann, J.; et al. 2022. Chinchilla：训练计算最优的大型语言模型. 载于*神经信息处理系统进展论文集*，NeurIPS。

+   Le et al. (2022) Le, H.; et al. 2022. 基于增强数据的程序合成强化学习. 载于*2022年国际学习表示会议（ICLR）论文集*，ICLR。

+   Li et al. (2023) Li, R.; et al. 2023. StarCoder：愿源代码与你同在！ 载于*2023年国际学习表示会议（ICLR）论文集*，ICLR。

+   Li et al. (2022) Li, Y.; et al. 2022. AlphaCode：竞争级代码生成. https://www.deepmind.com/publications/alphacode.

+   Lian et al. (2023) Lian, Z. X.; et al. 2023. OpenOrca：一个开放的GPT增强FLAN推理轨迹数据集. https://github.com/OpenOrca.

+   Lin (1992) Lin, L.-J. 1992. 基于强化学习、规划与教学的自我改善反应代理. *机器学习*，8(3-4)：293–321。

+   Luo et al. (2023) Luo, Z.; et al. 2023. WizardCoder：通过Evol-Instruct赋能代码大型语言模型. 载于*2023年国际学习表示会议论文集*，ICLR。

+   OpenAI (2023) OpenAI. 2023. GPT-4技术报告. https://openai.com/research/gpt-4.

+   Radford et al. (2019) Radford, A.; et al. 2019. 语言模型是无监督的多任务学习者. https://openai.com/blog/better-language-models.

+   Raffel 等（2020）Raffel, C.; 等. 2020. 使用统一的文本到文本变压器探索迁移学习的极限. 见于 *2020年国际学习表征会议（ICLR）论文集*。ICLR。

+   Roziere 等（2020）Roziere, B.; 等. 2020. 用于程序合成的变压器. 见于 *2020年神经信息处理系统会议（NeurIPS）论文集*。NeurIPS。

+   Roziere 等（2022）Roziere, B.; 等. 2022. 利用自动生成的单元测试进行无监督代码翻译. 见于 *2022年国际学习表征会议（ICLR）论文集*。ICLR。

+   Schaul 等（2016）Schaul, T.; Quan, J.; Antonoglou, I.; 和 Silver, D. 2016. 优先经验回放. 见于 *第4届国际学习表征会议（ICLR）论文集*。ICLR。

+   Svyatkovskiy 等（2020）Svyatkovskiy, A.; 等. 2020. 使用变压器进行代码补全. 见于 *第25届ACM SIGKDD国际知识发现与数据挖掘会议论文集*，1803–1813。ACM。

+   Touvron 等（2023）Touvron, H.; 等. 2023. Llama: 开放且高效的基础语言模型. https://arxiv.org/abs/2302.13971.

+   Vaswani 等（2017）Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; 和 Polosukhin, I. 2017. 注意力即一切. 见于 *神经信息处理系统进展*，5998–6008。NeurIPS。

+   Wang 等（2023）Wang, Y.; Yang, M.; Dong, R.; Sun, B.; Liu, F.; 和 U, L. H. 2023. 使用逆动态双重仿真度量在强化学习中的高效潜力基础探索. 见于 Oh, A.; Naumann, T.; Globerson, A.; Saenko, K.; Hardt, M.; 和 Levine, S., 编辑，*神经信息处理系统进展*，第36卷，38786–38797。Curran Associates, Inc.

+   Wang 等（2021）Wang, Y.; 等. 2021. CodeT5: 面向代码理解与生成的标识符感知统一预训练编码器-解码器模型. 见于 *2021年自然语言处理实证方法会议（EMNLP）论文集*，8697–8708。计算语言学协会。

+   Wei 等（2022）Wei, J.; 等. 2022. 思维链（CoT）提示. 见于 *神经信息处理系统进展（NeurIPS）*。NeurIPS。

+   Yang 等（2023）Yang, M.; Dong, R.; Wang, Y.; Liu, F.; Du, Y.; Zhou, M.; 和 U, L. H. 2023. TieComm: 基于纽带理论学习层次化通信拓扑. 见于 *高级应用数据库系统*，604–613。瑞士：施普林格自然出版集团。ISBN 978-3-031-30637-2。

+   Yang 等（2024a）Yang, M.; Wang, Y.; Yu, Y.; Zhou, M.; 和 U, L. H. 2024a. MixLight: 基于混合代理的协作强化学习用于交通信号灯控制. *IEEE工业信息学报*，20(2)：2653–2661。

+   Yang 等人（2024b）Yang, M.; Zhao, K.; Wang, Y.; Dong, R.; Du, Y.; Liu, F.; Zhou, M.; 和 U, L. H. 2024b. 多智能体强化学习中的团队有效沟通. *自主智能体与多智能体系统*，38(2): 36。

+   Zhang 等人（2023）Zhang, K.; 等人. 2023. 使用大型语言模型进行代码生成的规划. 收录于 *2023年国际学习表征会议（ICLR）论文集*。ICLR。
