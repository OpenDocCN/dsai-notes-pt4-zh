<!--yml
category: 未分类
date: 2025-01-11 11:41:12
-->

# Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects

> 来源：[https://arxiv.org/html/2501.01205/](https://arxiv.org/html/2501.01205/)

Abdullah Mushtaq1, Muhammad Rafay Naeem1, Ibrahim Ghaznavi1, Muhammad Imran Taj2, Imran Hashmi3, Junaid Qadir4 1Department of Computer Science, Information Technology University, Lahore, Pakistan
{bscs20078, bscs20004, ibrahim.ghaznavi}@itu.edu.pk 2College of Interdisciplinary Studies, Zayed University, Dubai, UAE
MuhammadImran.Taj@zu.ac.ae 3Department of Computer Science, University of Oxford, Oxford, UK
imran.hashmi@cs.ox.ac.uk 4Department of Computer Science and Engineering, Qatar University, Doha, Qatar
jqadir@qu.edu.qa

###### Abstract

Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of typical senior capstone projects and evaluated the performance of Multi-Agent and single-agent LLMs using both custom-designed metrics developed in consultation with engineering faculty and some widely used NLP-based metrics. These metrics cover technical quality, ethical considerations, social impact, and feasibility, ensuring that our evaluation aligns with the educational objectives of engineering design. Our findings suggest that Multi-Agent LLMs can provide a richer, more inclusive problem-solving environment compared to single-agent systems, offering a promising tool for enhancing the educational experience of engineering and computer science students by simulating the complexity and collaboration of real-world engineering and computer science practice. By supporting senior design projects, this tool not only aids in achieving academic excellence but also prepares students for the multifaceted challenges they will face in their professional engineering careers.

###### Index Terms:

Large Language Models, Gen AI, LLM Agents, LLM-Based Multi-Agent Systems, Multi-Agent Collaboration, Agentic AI, Autonomous LLM Agents, LLM in Engineering Applications.

## I Introduction

The senior design project (SDP), also known as the capstone or final year project, is a vital component of engineering and computer science education [[1](https://arxiv.org/html/2501.01205v1#bib.bib1)]. It offers students an opportunity to apply their theoretical knowledge in tackling complex, real-world engineering problems, providing an authentic learning experience that integrates the essential competencies required for 21st-century engineers [[2](https://arxiv.org/html/2501.01205v1#bib.bib2), [3](https://arxiv.org/html/2501.01205v1#bib.bib3)]. Accrediting bodies, such as the US Accreditation Board for Engineering and Technology (ABET) emphasize the importance of SDPs as key opportunities for students to engage with the kinds of complex, multidisciplinary challenges they will encounter in professional practice. The problems addressed in SDPs typically involve no straightforward solutions; instead, they require students to balance competing objectives, such as optimizing technical performance while addressing ethical, environmental, and social concerns.

A hallmark of modern engineering practice is the increasingly globalized context in which engineers operate. Engineering products must be designed to meet the needs of a global audience, often requiring the integration of diverse perspectives. This necessitates cultural intelligence—the ability to navigate different cultural contexts and work effectively with stakeholders from various backgrounds. Generative AI, particularly LLMs, offers an innovative way to simulate these diverse perspectives, providing students with an environment to engage in interdisciplinary problem-solving [[4](https://arxiv.org/html/2501.01205v1#bib.bib4), [5](https://arxiv.org/html/2501.01205v1#bib.bib5)]. These LLM agents represent different expert viewpoints and foster a collaborative approach that reflects real-world engineering practices.

The ability to solve complex problems is an essential competence in both education and professional life, as individuals increasingly face challenges arising from globalization and digitalization. A complex problem occurs when a person seeks to achieve a goal for which no clear or straightforward solution is available. Unlike non-complex problems, complex problem-solving (CPS) involves dynamic and opaque barriers, where the initial information is incomplete or subject to change. This definition, as articulated by Fischer, Greiff, and Funke, emphasizes that complex problems require adaptive thinking and flexibility in problem-solving approaches [[6](https://arxiv.org/html/2501.01205v1#bib.bib6), [7](https://arxiv.org/html/2501.01205v1#bib.bib7)]. In the context of engineering, the complexity is further amplified by the need to balance multiple, often conflicting objectives. Systems thinking, as introduced by Peter Senge in The Fifth Discipline, highlights the necessity of understanding the interconnections within complex systems, encouraging engineers to consider multiple perspectives and the broader implications of their solutions [[8](https://arxiv.org/html/2501.01205v1#bib.bib8)].

By leveraging the principle of the “wisdom of crowds”—the idea that large groups of diverse, independent individuals can collectively arrive at better decisions, solutions, and predictions than any single expert, as outlined by Surowiecki in The Wisdom of Crowds [[9](https://arxiv.org/html/2501.01205v1#bib.bib9)]—MAS has the potential to enable collective intelligence through emergent behavior. While intelligence is not inherent in MAS by design, structured interactions, and coordination mechanisms among agents allow complex and intelligent behaviors to develop collectively, leading to the emergence of intelligence [[10](https://arxiv.org/html/2501.01205v1#bib.bib10)]. The framework we propose builds on this idea, using LLM agents to simulate the interactions between diverse expert perspectives, such as problem formulation agents, systems complexity agents, and ethical and societal agents. In this way, LLM agents in the proposed MAS can help students develop the critical thinking and collaboration skills they need to succeed in globalized, multidisciplinary engineering environments.

The contributions of this paper are both pedagogical and technical. From a pedagogical perspective, this work introduces a novel framework for assisting supervisors and training engineering students in complex problem-solving using multi-agent LLMs within the context of SDPs. This approach leverages the capabilities of diverse LLM agents that represent different expert perspectives, simulating real-world interdisciplinary collaboration and enhancing critical thinking. Technically, the paper advances the state of the art by developing a framework that integrates MAS with LLMs to simulate multi-faceted engineering scenarios. This approach makes our framework capable of more effective technical problem-solving while incorporating ethical, social, and environmental dimensions into the decision-making process, offering a holistic solution to training engineers in globalized and complex environments.

The remainder of this paper is organized as follows. Section [II](https://arxiv.org/html/2501.01205v1#S2 "II Background and Related Work ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") discusses background and related Work, introducing foundational concepts in Agent-Based Modeling and multi-agent systems, and highlighting previous efforts in applying AI tools for complex problem-solving in engineering education. Section [III](https://arxiv.org/html/2501.01205v1#S3 "III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") outlines the Methodology, detailing the use of Multi-Agent LLMs to support SDPs and the evaluation criteria. In Section [IV](https://arxiv.org/html/2501.01205v1#S4 "IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), we present the results, followed by a discussion in Section [V](https://arxiv.org/html/2501.01205v1#S5 "V Discussions ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), reflecting on the pedagogical and technical implications. Finally, Section [VI](https://arxiv.org/html/2501.01205v1#S6 "VI Conclusions ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") concludes the paper, summarizing contributions and suggesting future research directions.

## II Background and Related Work

### II-A The Role of SDPs in Engineering Education

SDPs, also referred to as capstone or final-year projects, are a crucial component of computing and engineering education. As required by ABET and other accrediting bodies, SDPs serve as a culminating experience that integrates the knowledge and skills students have acquired over their academic careers. These projects require students to solve complex, real-world engineering problems that involve conflicting objectives, trade-offs, and ethical considerations. For instance, engineers must often balance cost efficiency with environmental sustainability, or optimize technical performance while adhering to regulatory constraints.

In these settings, teamwork and collaboration across diverse skills and perspectives are essential. Given the globalized nature of modern engineering practice, students are also required to consider cultural and social factors when designing engineering solutions. This is especially important as engineers increasingly work in global teams and must develop products that meet the needs of an international audience. Therefore, engaging with multiple stakeholders and considering the broader societal and cultural contexts of engineering solutions are vital competencies that SDPs aim to cultivate.

### II-B Complex Problem Solving and Diversity Dividend

Complex engineering problems, as per the definition of ABET¹¹1[https://www.abet.org/accreditation/accreditation-criteria/criteria-for-accrediting-engineering-programs-2022-2023/](https://www.abet.org/accreditation/accreditation-criteria/criteria-for-accrediting-engineering-programs-2022-2023/), have the following attributes, “involving wide-ranging or conflicting technical issues, having no obvious solution, addressing problems not encompassed by current standards and codes, involving diverse groups of stakeholders, including many component parts or sub-problems, involving multiple disciplines, or having significant consequences in a range of contexts.”

Several frameworks have been proposed that leverage diversity to foster effective problem-solving. One well-known approach is Edward de Bono’s Six Thinking Hats framework, which encourages individuals to look at problems from multiple perspectives—logical, emotional, and creative, among others. This structured, yet flexible, approach is particularly effective when integrated with MAS, as it mirrors the interdisciplinary thinking necessary to solve complex engineering problems. Scott Page’s book The Difference [[11](https://arxiv.org/html/2501.01205v1#bib.bib11)] emphasizes the value of diversity in problem-solving, particularly in complex systems. Page argues that teams composed of individuals with different skills, experiences, and perspectives are better equipped to solve complex problems than homogenous teams. This aligns with the wisdom of crowds principle, which suggests that collective intelligence can outperform individual expertise, particularly when the group is diverse and composed of independent thinkers. Similarly, Minsky’s The Society of Mind [[12](https://arxiv.org/html/2501.01205v1#bib.bib12)] underscores the importance of diverse cognitive processes in problem-solving, positing that complex thought emerges from the interaction of simpler, specialized processes.

### II-C Multi-Agent Systems and Agent-Based Modeling

MAS is widely applied in fields like robotics, artificial intelligence, and complex systems modeling to simulate autonomous agents acting within dynamic environments. While each agent operates independently, their collective behavior can lead to emergent properties—patterns or outcomes that are challenging to predict solely from individual actions. In engineering education, MAS can be used to simulate stakeholder interactions in complex projects, enabling students to experience realistic decision-making, collaboration, and problem-solving scenarios.

Agent-Based Modeling (ABM) extends MAS by offering a detailed computational framework for simulating interactions between agents and their environments. ABM is especially valuable for examining systems where individual behaviors directly influence collective outcomes, making it ideal for complex social or engineering contexts. In SDPs, ABM can simulate interactions among engineers, project managers, regulators, and other stakeholders, giving students practical insight into the collaborative and often unpredictable nature of professional practice [[13](https://arxiv.org/html/2501.01205v1#bib.bib13), [14](https://arxiv.org/html/2501.01205v1#bib.bib14)]. These simulations provide a flexible and dynamic approach for modeling complex systems, allowing students to explore the consequences of various decisions within a controlled environment

![Refer to caption](img/ca7af0b9658c1486ab0df69b9f8b7093.png)

Figure 1: Workflow Overview: 1) Input Submission: The student enters the project title and proposal PDF. 2) Proposal Processing: Coordinator Agent forwards details to Tasks Agent to generate focused tasks. 3) Task Generation: Tasks Agent returns a list of tasks. 4) Task Distribution: Tasks are sent to the Tasks Channel. 5) Task Assignment: Tasks are assigned to relevant agents. 6) Output Generation: Agents produce outputs and send them to the Tasks Channel and Coordinator Agent. 7) Input Linking: Outputs from one agent can serve as inputs for others if needed. 8) User Output: Final results are displayed in the interface, with summaries and detailed analysis.

### II-D MAS and LLMs in Educational Contexts

The integration of MAS and LLMs offers significant promise for enhancing engineering education, especially in tackling complex, real-world problems. By simulating interactions between diverse autonomous agents, MAS allows educators and students to explore the trade-offs, dilemmas, and decision-making processes typical in multidisciplinary projects. These systems mirror real-world engineering dynamics, where professionals must collaborate across domains, negotiate conflicting objectives, and optimize under constraints.

LLMs further enhance this framework by representing expert personas—such as problem formulation agents, project managers, and ethical and societal agents—enabling students to engage in interdisciplinary dialogue. Studies have shown that this approach leads to more innovative and robust solutions by leveraging the “wisdom of crowds” effect, as described by Surowiecki [[9](https://arxiv.org/html/2501.01205v1#bib.bib9)]. By fostering diverse perspectives, students gain deeper insights into the complexities of engineering challenges [[15](https://arxiv.org/html/2501.01205v1#bib.bib15), [16](https://arxiv.org/html/2501.01205v1#bib.bib16), [17](https://arxiv.org/html/2501.01205v1#bib.bib17)].

Existing research highlights the potential of LLM agents in simulating collaborative problem-solving environments, where students engage with virtual experts on issues such as ethical dilemmas and environmental trade-offs. These interactions help students develop critical thinking skills and a holistic approach to problem-solving, essential for SDPs. Additionally, MAS and LLMs support adaptive learning by providing real-time feedback, allowing students to experiment and learn from their decisions in a risk-free environment. This active learning approach enhances their ability to navigate complex, globalized engineering challenges [[15](https://arxiv.org/html/2501.01205v1#bib.bib15), [16](https://arxiv.org/html/2501.01205v1#bib.bib16)].

### II-E Existing LLM Multi-Agent Frameworks

Several LLM-based multi-agent frameworks have emerged, offering advanced capabilities for addressing complex, interdisciplinary challenges. In educational settings, these frameworks are particularly valuable for their ability to simulate real-world scenarios, fostering critical thinking, collaboration, and problem-solving skills. Below are some examples of such frameworks and their key features:

1.  1.

    Camel-AI [[15](https://arxiv.org/html/2501.01205v1#bib.bib15)] is a communicative agent framework designed to simulate a “society” of LLM agents representing different personas. It excels in fostering interdisciplinary dialogue among agents, making it ideal for tasks that require negotiation and the integration of diverse viewpoints. Camel’s capacity to explore various facets of “mind” interactions makes it a strong candidate for projects where multiple perspectives are needed, such as senior design projects.

2.  2.

    Crew.AI [[18](https://arxiv.org/html/2501.01205v1#bib.bib18)] is another prominent open-source multi-agent orchestration framework. This Python-based platform allows the orchestration of role-playing AI agents, working together as a cohesive assembly or “crew” to complete tasks. The framework’s strength lies in automating multi-agent workflows, making it particularly useful in scenarios requiring the coordination of diverse agent roles for collaborative decision-making processes.

3.  3.

    MegaAgent [[16](https://arxiv.org/html/2501.01205v1#bib.bib16)] is designed to handle large-scale cooperation in MAS. Its primary strength lies in its scalability, enabling a vast number of agents to work together in a coordinated fashion. This makes it suitable for simulating large, complex systems, providing students with insights into how large teams or systems operate in engineering contexts.

4.  4.

    AgentScope [[19](https://arxiv.org/html/2501.01205v1#bib.bib19)] focuses on multi-agent simulation at a very large scale, providing a highly detailed simulation environment. Its capacity to handle complex, dynamic interactions between agents makes it particularly useful for simulations that require a high degree of realism, such as real-time problem-solving.

5.  5.

    OpenAgents [[20](https://arxiv.org/html/2501.01205v1#bib.bib20)] and Agent Lumos [[21](https://arxiv.org/html/2501.01205v1#bib.bib21)] are modular frameworks that enable flexible training of LLM agents. OpenAgents provides a platform for building and deploying autonomous language agents in diverse environments, while Agent Lumos unifies and simplifies the training process for these agents. These frameworks are well-suited contexts where the focus is on developing customized agents that can be tuned for specific tasks, such as providing real-time feedback or conducting collaborative discussions.

Together, these frameworks demonstrate the transformative potential of LLM-based multi-agent systems in education. They enable the creation of highly interactive, collaborative environments where students can engage with diverse expert perspectives and gain hands-on experience solving complex engineering problems. These systems provide the flexibility, scalability, and adaptability necessary for real-world problem-solving, making them invaluable tools for enhancing engineering education. For our proposed MAS, we selected Camel AI [[15](https://arxiv.org/html/2501.01205v1#bib.bib15)] due to its use of role-playing and inception prompting, which facilitate agent collaboration with minimal intervention, effectively simulating interdisciplinary teamwork. Additionally, its capability to tackle complex problems by breaking them down into focused, manageable subtasks for each agent makes it an ideal choice.

## III Methodology

This section presents the methodology used to construct and evaluate our Multi-Agent LLM (MAS LLM) framework for supporting complex engineering problem-solving in SDPs. The methodology is divided into two primary subsections:

### III-A MAS LLM Framework Construction

The first subsection outlines the technical details of how the MAS LLM framework was developed. This includes the LLM model used, the integration of the model into a multi-agent system, and the customization of agent roles to simulate diverse expert perspectives such as project managers, breadth and depth agents, and societal and ethical agents. We describe how the system was designed to promote interdisciplinary collaboration and support real-time feedback for students.

Concretely, we designed a copilot-style LLM-powered MAS for engineering and computing students’ senior design projects. Fig. [1](https://arxiv.org/html/2501.01205v1#S2.F1 "Figure 1 ‣ II-C Multi-Agent Systems and Agent-Based Modeling ‣ II Background and Related Work ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") illustrates the system design of our proposed MAS. This system comprises eight LLM agents:

1.  1.

    Problem Formulation Agent

2.  2.

    Breadth and Depth Agent

3.  3.

    Ambiguity and Uncertainty Agent

4.  4.

    System Complexity Agent

5.  5.

    Technical Innovation and Risk Management Agent

6.  6.

    Societal and Ethical Consideration Agent

7.  7.

    Methodology and Approach Agent

8.  8.

    Comprehensive Evaluation Agent

Each agent focuses on a different aspect of the SDP based on its role. These agents are designed to act as experts by crafting personas aligned with their respective roles. Box [III-A](https://arxiv.org/html/2501.01205v1#S3.SS1 "III-A MAS LLM Framework Construction ‣ III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") shows a sample persona crafted for the “Problem Formulation Agent.” Each agent receives its persona, project title, and detailed methodology (extracted from PDF) of the proposed SDP (provided by the students). Figure [2](https://arxiv.org/html/2501.01205v1#S3.F2 "Figure 2 ‣ III-A MAS LLM Framework Construction ‣ III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") shows the detailed personas for each agent used in our proposed MAS. OpenAI’s GPT-4o serves as the primary backend LLM for these agents, as it currently outperforms other LLMs [[22](https://arxiv.org/html/2501.01205v1#bib.bib22)].

The proposed LLM-based MAS operates under a centralized control mechanism managed by a Coordinator Agent, which is responsible for overseeing task execution and ensuring efficient collaboration between agents. Coupled with the Coordinator Agent is a Task Agent, which decomposes each SDP into a series of well-defined tasks for agents to handle. These tasks are then assigned to agents according to their specialized roles through a dedicated Task Channel—a feature in Camel AI designed to streamline task distribution and monitoring [[23](https://arxiv.org/html/2501.01205v1#bib.bib23)].

![Refer to caption](img/e7b141d75b90e01577aff07b2ac7881a.png)

Figure 2: Personas for each agent in the proposed MAS, detailing their tasks, objectives, and evaluation points for evaluating engineering SDPs. The specialized agents—covering problem formulation, breadth and depth, ambiguity, complexity, innovation, ethics, and methodology—enable systematic and holistic assessments across technical and non-technical dimensions.

Upon receiving their respective tasks, agents process them sequentially (one by one) and return their outputs to the Task Channel, which then routes them back to the Coordinator Agent. The Coordinator Agent evaluates each agent’s output to verify that it meets predefined standards, leveraging a specialized feature provided by Camel AI. As an LLM-based agent, the Coordinator automatically generates a requirements list based on the tasks provided by the user or students. With assistance from the Task Agent, the Coordinator outlines what to expect from the agents, considering the context of the overall task and the sub-tasks assigned to them. If an agent’s output does not meet expectations, the Coordinator may reassign the task—either to a different agent with relevant expertise or back to the original agent, augmented with added sub-goals for refinement. When a task from one agent requires input from another agent, the Coordinator efficiently routes the necessary outputs, ensuring both consistency and quality control across the system.

<svg class="ltx_picture" height="276.42" id="S3.SS1.p7.pic1" overflow="visible" version="1.1" width="300"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,276.42) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.28 259.15)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="273.45">Box 1: Problem Formulation Agent Persona</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.28 13.28)"><foreignobject color="#000000" height="228.66" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="273.45">Role: AI assistant specializing in evaluating the Problem Formulation part of Engineering Senior Year Design Projects. Responsibilities: • Task: Assess the clarity, depth, and scope of the problem statement. • Objective: Determine if the problem is appropriately complex and well-defined. Evaluation Points: • Does the proposal clearly articulate the engineering problem? • Is the complexity level of the problem aligned with advanced engineering challenges? • Does it highlight any interdisciplinary aspects or requirements that add to the complexity? • Are the societal, environmental, or ethical implications of the problem considered in the formulation? Evaluation Criteria: • 1 = Not Addressed: The criterion is entirely absent. • 2 = Minimally Addressed: The criterion is mentioned, but lacks depth. • 3 = Partially Addressed: Criterion covered but lacks completeness. • 4 = Adequately Addressed: Criterion is reasonably addressed. • 5 = Thoroughly Addressed: Criterion covered comprehensively with insightful analysis. Expected Output: A final score (1-5) based on Evaluation Criteria and Points, with Strengths, Weaknesses, and Suggestions.

Once all tasks are completed, the Coordinator Agent synthesizes the agents’ outputs into the final specified format. Each agent operates as a Critic Agent, a Camel AI class designed to provide constructive feedback and facilitate decision-making for complex tasks. Each agent also maintains a message history window of the last 10 responses, enabling it to generate informed outputs. Additionally, agents are equipped with essential tools such as the Internet Search Toolkit and Mathematics Toolkit, provided by Camel AI, to assist in generating accurate and well-rounded responses.

A snapshot of the user interface for our proposed LLM-based MAS co-pilot is shown in Figure [7](https://arxiv.org/html/2501.01205v1#S5.F7 "Figure 7 ‣ V Discussions ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"). To enhance interactivity, we provide a follow-up questioning feature where users can ask questions based on the MAS responses. The Coordinator Agent analyzes the follow-up question and previous agent responses to determine which agents are best suited to answer, making the system dynamic and context-aware.

<svg class="ltx_picture" height="175.07" id="S3.SS1.p10.pic1" overflow="visible" version="1.1" width="300"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,175.07) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.28 157.79)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="273.45">Box 2: Tree of Thoughts Prompting Template</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 13.28 13.28)"><foreignobject color="#000000" height="127.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="273.45">“ Imagine X different experts answering this question.
All experts will write down 1 step of their thinking, and then share it with the group.
Then all experts will go on to the next step, etc.
If any expert realizes they’re wrong at any point then they leave.
The question is… ”</foreignobject></g></g></svg>

Along with a MAS, we designed a single agent LLM using GPT-4o to evaluate the SDPs on different engineering and computing aspects and to do a systematic and fair evaluation. To make the single agent perform better from its vanilla settings, we utilized the Tree of Thoughts (TOT) prompting technique [[24](https://arxiv.org/html/2501.01205v1#bib.bib24), [25](https://arxiv.org/html/2501.01205v1#bib.bib25)] to instruct this single agent to simulate the behavior of multiple experts within its response, each covering and evaluating different aspects of the SDP. Box [III-A](https://arxiv.org/html/2501.01205v1#S3.SS1 "III-A MAS LLM Framework Construction ‣ III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") shows the template we used to incorporate TOT in our prompting. We used this template as a blueprint for creating a complete prompt for the single agent.

### III-B Evaluation Methodology

The second subsection explains how the framework was evaluated in the context of SDPs. We detail the metrics used to assess the effectiveness of the MAS LLM framework in enhancing student learning, collaboration, and problem-solving skills. This includes both qualitative and quantitative data collection methods. We also explain how the data was analyzed to assess the pedagogical and technical impact of the framework.

#### III-B1 Comparing Faculty and Agent Scores

We designed a methodology to systematically evaluate the performance of this proposed MAS. In a typical SDP progression cycle, students write the initial draft of their proposal and share it with their supervisor/advisor for feedback. Students then incorporate the suggested changes into the proposal, repeating this process multiple times to ensure the proposal meets the standards set by the faculty at their institution. This process takes a significant amount of time for both students and supervisors to prepare the proposal before moving on to the actual development phase of the project. We advocate for the engineering and computing community to adopt this co-pilot system to enhance workflows and facilitate the development of more advanced and effective co-pilot-style MAS for SDP assistance.

![Refer to caption](img/2bcf1e51254e5bdd0243901188bf3e9b.png)

Figure 3: Visual representation of scores to evaluate the performance of both of our proposed multi-agent and single-agent systems across each SDP (represented in numbers on the x-axis) for each aspect of engineering and computing. It can be seen that multi-agent system scores are more aligned with faculty evaluation scores as compared to single-agent scores.

For this evaluation, we asked four faculty members from the Engineering and Computer Science departments at different universities to provide feedback, which we used as a reference standard. We recognize that these evaluations are subjective and naturally vary among evaluators, so they are not an absolute ground truth but serve as a comparative benchmark for our system. The variability in these faculty scores is also shown in Figure [6](https://arxiv.org/html/2501.01205v1#S4.F6 "Figure 6 ‣ IV-B NLP-based Evaluation Results ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"). This method uses three primary evaluation scores to assess the effectiveness of our multi-agent system-powered copilot in guiding engineering and computer science students.

1.  1.

    The Faculty Evaluation Score reflects faculty assessments of key engineering and computing aspects within each SDP proposal, providing a baseline measure of project quality.

2.  2.

    The Multi-Agent System Score represents scores generated by individual agents themselves, each focusing on specific engineering and computing criteria aligned with their programmed expertise.

3.  3.

    The Single Agent Score represents scores generated by a single agent itself using TOT to simulate different experts to evaluate different aspects of the SDP.

Different engineering and computing aspects we selected for this system are Problem Formulation, Breadth and Depth, Ambiguity and Uncertainty, System Complexity, Technical Innovation and Risk Management, Societal and Ethical Considerations, and Methodology and Approach. Together, these scores across each aspect offer a comprehensive view of both the technical quality of student work and the impact of agent-driven feedback in supporting educational outcomes whether it is a MAS or single agent. We collected six SDP proposals from students in the Engineering and Computer Science departments at X University. All students had completed their SDPs (2023-2024). To ensure anonymity, each proposal was renamed with a randomly assigned number (between 1 & 6). In Section [IV](https://arxiv.org/html/2501.01205v1#S4 "IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), we will further discuss the evaluation results and performance of our proposed methodology.

#### III-B2 NLP-based Evaluation

To evaluate the performance of both MAS and single-agent systems, we used four NLP-based scoring metrics: Lexical Cohesion, Average Sentence Length, Clause Density, and Flesch-Kincaid Score. These criteria provide insights into thematic consistency, readability, and structural complexity in system responses.

1.  1.

    Clause Density [[26](https://arxiv.org/html/2501.01205v1#bib.bib26)] captures sentence complexity by counting clauses per sentence, reflecting layered perspectives. Scores range from 1 (simple, single-idea sentences) to 3+ (highly complex, multi-idea sentences).

2.  2.

    Lexical Cohesion [[27](https://arxiv.org/html/2501.01205v1#bib.bib27)] measures thematic consistency by analyzing word repetition or related terms, indicating how well the content is built on multiple ideas. Scores range from 0 (no cohesion) to 1 (full thematic consistency).

3.  3.

    Flesch-Kincaid Score [[28](https://arxiv.org/html/2501.01205v1#bib.bib28)] estimates readability, indicating the U.S. grade level needed to understand the text. A higher score suggests advanced content suitable for expert readers, with an ideal range balancing accessibility and sophistication (0–16 scale) for academic purposes.

4.  4.

    Average Sentence Length indicates structural complexity and content depth, with typical ranges from 10 to 40 words. Shorter sentences enhance readability, while longer ones may reflect richer, nuanced perspectives but can be harder to follow.

These metrics collectively assess the depth and accessibility of each system’s response. Using these scores, we can evaluate the performance of these systems from a NLP perspective.

## IV Results

In this section, a comprehensive analysis of the results is presented, focusing on faculty evaluations, error rates, and NLP-based performance metrics. The evaluation based on faculty scores is detailed in Figure [3](https://arxiv.org/html/2501.01205v1#S3.F3 "Figure 3 ‣ III-B1 Comparing Faculty and Agent Scores ‣ III-B Evaluation Methodology ‣ III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"). Error rates for each system, reflecting their relative accuracy, are shown in Figure [4](https://arxiv.org/html/2501.01205v1#S4.F4 "Figure 4 ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"). Finally, the performance of each system, assessed using NLP-based metrics, is illustrated in Figure [5](https://arxiv.org/html/2501.01205v1#S4.F5 "Figure 5 ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects").

![Refer to caption](img/b46b15294a29ee326d0b75134a1a4c48.png)

Figure 4: Mean Absolute Error comparison between multi-agent and single-agent systems against faculty evaluations. Lower bars indicate closer alignment with faculty scores, with the multi-agent system generally showing better accuracy.

![Refer to caption](img/2c370c6dc01892f6bb5321fbd8bea6e1.png)

Figure 5: NLP-based performance evaluation of original student proposals, MAS responses, and single-agent system responses. The metrics include Clause Density (complexity), Lexical Cohesion (thematic unity), Flesch-Kincaid Score (readability), and Average Sentence Length (structural depth), designed to balance accessibility with scholarly depth. Our results show that the MAS approach consistently outperforms the single-agent system across all evaluated metrics.

### IV-A Faculty Guided Evaluation Results

As outlined in Section [III-B](https://arxiv.org/html/2501.01205v1#S3.SS2 "III-B Evaluation Methodology ‣ III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), we designed three distinct evaluation scores to assess the performance of our proposed MAS for SDPs with respect to evaluation from faculty members. Scores comparison for each project across faculty evaluations, MAS scores, and single-agent scores. Green bars represent faculty scores, blue bars represent MAS scores and red bars indicate single-agent scores. This section further discusses key insights from the observed scoring patterns and analysis.

The detailed results in Figure [3](https://arxiv.org/html/2501.01205v1#S3.F3 "Figure 3 ‣ III-B1 Comparing Faculty and Agent Scores ‣ III-B Evaluation Methodology ‣ III Methodology ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") show the performance of each system across each aspect and SDP proposals. The graph shows that the MAS consistently matches or outperforms the single-agent system across all aspects, except for an isolated project for the Breadth and Depth aspect. This can be seen by the MAS score in each aspect being much closer to the faculty evaluation scores and following the same trend.

Figure [4](https://arxiv.org/html/2501.01205v1#S4.F4 "Figure 4 ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") presents the results showcasing the effectiveness of the MAS in evaluating SDPs compared to the single-agent system, with both systems benchmarked against faculty evaluation scores. The MAS demonstrates greater alignment with faculty evaluations, with a Mean Absolute Error (MAE) of 0.205 compared to 0.388 for the single-agent system, an 89.3% accuracy improvement. Lower bars in the figure represent closer alignment with faculty scores. The MAS excels in technical categories such as Technical Innovation and Risk Management (MAE 0.345 vs. 0.855) and System Complexity (MAE 0.272 vs. 0.355). However, in Breadth and Depth, the single-agent system performs better (MAE 0.208 vs. 0.292), suggesting certain holistic aspects may favor a unified approach. The MAS outperforms in Ambiguity and Uncertainty (MAE 0.440 vs. 0.857) and Societal and Ethical Considerations (MAE 0.355 vs. 0.772), demonstrating its broader effectiveness. Both systems perform equally in Methodology and Approach (MAE 0.293).

These results demonstrate the effectiveness of the MAS-based approach in evaluating SDPs, with superior accuracy across most assessment criteria due to its use of specialized agents. While the MAS consistently excels in key areas, including technical depth and ethical considerations, it lags behind the single-agent system in the Breadth and Depth category. This highlights an area for further refinement, reinforcing the MAS’s potential as a robust and reliable tool for academic project evaluation with targeted improvements.

### IV-B NLP-based Evaluation Results

From an NLP perspective, we designed a mechanism to evaluate the responses of the MAS and single-agent systems. These metrics assess how each system performed in terms of complex ideation, structural coherence, and readability for both students and supervisors.

As shown in Figure [5](https://arxiv.org/html/2501.01205v1#S4.F5 "Figure 5 ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), the clause density of responses generated by the MAS is more closely aligned with the original proposals compared to those produced by the single-agent system. This indicates that the MAS outputs are more detail-rich, conveying a greater amount of information per sentence, which enhances their effectiveness and suitability in this context. Examining the lexical cohesion graph in Figure [5](https://arxiv.org/html/2501.01205v1#S4.F5 "Figure 5 ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), we see that MAS responses exhibit stronger thematic consistency. Unlike the original proposals written by students and the single-agent responses, MAS outputs offer more collaborative feedback reflecting the interconnectedness of ideas within the text. With different agents covering specific aspects and building upon each other’s responses, MAS produces nuanced, detailed outputs that better support both students and supervisors.

The Flesch-Kincaid readability score, which indicates the U.S. grade level needed to comprehend the responses on a first read, is a crucial metric for evaluating academic suitability. As grade level increases, writing typically becomes more organized and detail-rich—a pattern common in academic documents. For senior-year students engaged in final-year SDP projects, an ideal readability score lies between 14 and 16\. As shown in Figure [5](https://arxiv.org/html/2501.01205v1#S4.F5 "Figure 5 ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects"), the MAS responses in the Flesch-Kincaid graph, predominantly fall within this ideal range, suggesting they are well-suited to the academic level of senior students. The Average Sentence Length metric in this figure indicates that sentence lengths in both MAS and SA responses are similar to those in the original proposals in most of the cases. While typical values for this metric range from 0 to 40, responses from both the MAS and the single-agent system are predominantly concentrated in the mid-range, often aligning more closely with the original proposals. This effect is primarily attributed to the training methodology of the LLMs. Given that both the MAS and single-agent systems rely on the same backend LLM, this behavior is unsurprising.

The results in both faculty-based and NLP-based evaluations (§[IV-A](https://arxiv.org/html/2501.01205v1#S4.SS1 "IV-A Faculty Guided Evaluation Results ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") and §[IV-B](https://arxiv.org/html/2501.01205v1#S4.SS2 "IV-B NLP-based Evaluation Results ‣ IV Results ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects")) highlight the superior performance characteristics of the MAS compared to the single-agent system in evaluating SDPs, with each system displaying distinct strengths. MAS responses demonstrated broader coverage, not only in technical aspects, clause density, and thematic consistency but also in providing more detailed, cohesive feedback. Readability scores indicate that MAS responses align well with senior-year academic expectations, although slightly longer sentence lengths may impact accessibility. Additionally, the MAS exhibited impressive performance in addressing ethical and societal considerations. The single-agent system, on the other hand, displayed higher error rates across most metrics, except in areas like Breadth and Depth, where its error rate is lower than MAS, aligning it closely with faculty scores. This systematic evaluation concretely shows that MAS-based LLMs are more suited for complex problem-solving in engineering and computer science fields because of the multifaceted nature of the requirements for SDPs.

![Refer to caption](img/93ccb5d6ddfbbb31f4cc62329df63cc3.png)

Figure 6: Mean Faculty Evaluation Scores with standard deviations for SDP proposals. The variability illustrates the subjective nature of assessments and the diversity in faculty interpretations of evaluation criteria.

## V Discussions

![Refer to caption](img/3ac2e919d1f8edd49a389562f52cc598.png)

Figure 7: User interface for the proposed LLM-based MAS co-pilot, enabling students to interact with a graphical interface. Students can upload their SDP title and proposal, view summaries and agent feedback, and submit follow-up questions via the interface. This interface streamlines the feedback process, promoting iterative learning and comprehensive project assessment.

### V-A Situating our Initial Approach in the Agentic Landscape

The landscape of LLM agents and MAS is diverse, with various frameworks actively developed to leverage large language models for complex, collaborative tasks. MAS, a longstanding area of research since the 1980s, provides a foundation for decentralized, autonomous problem-solving across domains like artificial intelligence and robotics [[10](https://arxiv.org/html/2501.01205v1#bib.bib10)]. MAS frameworks facilitate agent interactions through mechanisms such as cooperation, coordination, and negotiation, often yielding emergent behaviors beyond the capabilities of individual agents.

While powerful, LLMs alone lack the sophistication to fully function as autonomous agents, as they typically need plugins or external tools to interact meaningfully with other systems. In MAS, agents are traditionally defined as systems capable of autonomous action and interaction within an environment to achieve specific goals [[10](https://arxiv.org/html/2501.01205v1#bib.bib10)]. MAS frameworks allow agents to coordinate, negotiate, and cooperate, often resulting in emergent behaviors beyond the reach of individual agents.

In GenAI, however, an agent is defined as a GenAI system, usually powered by an LLM, that serves a user’s goals by engaging with external systems and executing actions outside the LLM itself [[29](https://arxiv.org/html/2501.01205v1#bib.bib29)]. For example, ChatGPT’s code interpreter, integrated into GPT-4, combines language processing with Python to autonomously handle tasks like data analysis and plotting based on user prompts, improving accuracy by performing direct calculations. Yet, this setup still requires human oversight and does not fully align with the traditional concept of an agent.

There is now rising interest in combining the power of agents and LLMs²²2[https://llmagents-learning.org/f24](https://llmagents-learning.org/f24). Advanced LLM Agent frameworks, such as the MRKL System [[30](https://arxiv.org/html/2501.01205v1#bib.bib30)], extend LLMs into more sophisticated roles by employing a central LLM router to access various tools for complex problem-solving. Further enhancing agent autonomy, systems like LangChain [[31](https://arxiv.org/html/2501.01205v1#bib.bib31)], AutoGen [[32](https://arxiv.org/html/2501.01205v1#bib.bib32)], and ReAct [[33](https://arxiv.org/html/2501.01205v1#bib.bib33)] minimize continuous human input, and Camel AI [[15](https://arxiv.org/html/2501.01205v1#bib.bib15)] leverages role-playing and inception prompting to enable agents to collaborate with minimal intervention, simulating interdisciplinary teamwork. Building on the established use of role-playing in engineering education as a pedagogical tool [[34](https://arxiv.org/html/2501.01205v1#bib.bib34)], and the importance of formative feedback and assessment [[35](https://arxiv.org/html/2501.01205v1#bib.bib35)], we extend this concept by harnessing the capabilities of multi-agent LLMs.

In this work, we introduce a Camel AI-inspired framework for SDPs that provides a structured, role-based problem-solving environment, simulating interdisciplinary collaboration to help students tackle complex engineering tasks. Our system marks an initial exploration into LLM-based multi-agent systems, and unlike more advanced frameworks, it lacks advanced external tool integration and sophisticated multi-agent techniques like coordination and negotiation, which could further enhance collaboration. Despite not having all the advanced techniques our proposed MAS still had 89% more accuracy compared to single-agent systems. Emerging methods for optimizing LLM agent interactions, including advanced prompting techniques [[36](https://arxiv.org/html/2501.01205v1#bib.bib36)], are also beyond our current scope. Future work will explore these areas, to develop a more autonomous multi-agent system that leverages complex coordination and external tools to meet the demands of increasingly sophisticated engineering challenges.

### V-B Pedagogical Implications

Our work impacts key educational stakeholders: students, instructors, and administrators. For students, the multi-agent LLM system provides structured guidance through a web-based co-pilot, helping them tackle complex, interdisciplinary projects while fostering critical thinking without requiring full subject expertise. Figure [7](https://arxiv.org/html/2501.01205v1#S5.F7 "Figure 7 ‣ V Discussions ‣ Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects") illustrates a snapshot of the user interface for the proposed MAS co-pilot system. Given that students are often engaging with complex problem-solving for the first time, our framework offers a practical tool that helps bridge their knowledge gaps and guides them in areas like ethical considerations, technical complexity, and societal impact. However, as current LLMs still rely heavily on human input and precise prompt engineering, students who lack experience in both prompt design and advanced subject nuances may face challenges. This system addresses some of these gaps by offering structured pathways for problem-solving, thus enhancing their ability to work on large-scale projects. For instructors, the framework improves instructional efficiency by automating guidance in key project areas, allowing them to focus on high-level mentorship rather than technical troubleshooting. For administrators, the system can support accreditation efforts by providing a scalable tool that aligns with goals for multidisciplinary and real-world education.

To encourage further development and adaptation, we have shared the code for our SDP complex problem-solving co-pilot as an open-source resource (Link will be released after paper acceptance), allowing educators and researchers to experiment with and extend the system. By making our framework accessible, we aim to contribute to the wider adoption of advanced multi-agent LLM systems in engineering education, fostering a collaborative movement toward practical tools that support complex problem-solving in educational contexts.

## VI Conclusions

In this paper, we explored the use of Multi-Agent Large Language Models (MAS LLMs) as a novel framework for enhancing complex problem-solving in engineering senior design projects. By leveraging the collaborative capabilities of MAS and the generative power of LLMs, we provided students with a dynamic, interdisciplinary environment that mirrors real-world engineering challenges. This approach enabled students to engage with multiple perspectives, simulate trade-offs, and explore the complexity of decision-making in globalized engineering contexts. We evaluated our MAS LLMs and a single-agent system based on some custom-designed metrics that are more inclined toward faculty members of universities and some widely adopted NLP-based metrics. Our findings suggest that the MAS LLM framework can enhance student learning by providing details-rich responses, ideation of complex ideas, fostering collaboration, improving problem-solving skills, and allowing for real-time feedback through follow-up questions. With MAS achieving an overall accuracy of 89%. Our analysis also reveals that single-agent systems are not usually aligned with faculty-assigned scores in most of the engineering and computing aspects. This is due to the fact that the single-agent system tries to oversimplify the responses, and the response window of LLMs is much smaller for one agent as compared to multiple responses from each agent in MAS. The ability of MAS to simulate expert personas, representing diverse stakeholders and viewpoints, and responding with higher attention to details and complexity of senior design projects, offers students a deeper understanding of the ethical, technical, and social dimensions of their projects. Moreover, the framework’s adaptability to various project types and disciplines makes it a promising tool to be used and evaluated by the engineering and computing education community. While our study demonstrates the potential of MAS LLMs in educational settings, future work could explore further customizations of agent behavior and extend the framework to other fields beyond engineering and computer science. Additionally, the integration of more advanced evaluation metrics and longitudinal studies could provide deeper insights into the long-term educational benefits of this approach.

## References

*   [1] A. R. Fernando, J. G. U. Vergara, and C. A. D. Canlapan, “Work in progress: Perception of complex engineering problem among capstone design students,” in *2022 IEEE Global Engineering Education Conference (EDUCON)*.   IEEE, 2022, pp. 14–16.
*   [2] J. Qadir, K.-L. A. Yau, M. A. Imran, and A. Al-Fuqaha, “Engineering education, moving into 2020s: Essential competencies for effective 21st century electrical & computer engineers,” in *2020 IEEE Frontiers in Education Conference (FIE)*.   IEEE, 2020, pp. 1–9.
*   [3] J. Qadir and A. Al-Fuqaha, “A student primer on how to thrive in engineering education during and beyond COVID-19,” *Education Sciences*, vol. 10, no. 9, p. 236, 2020.
*   [4] J. Qadir, “Engineering education in the era of ChatGPT: Promise and pitfalls of generative AI for education,” in *2023 IEEE Global Engineering Education Conference (EDUCON)*.   IEEE, 2023, pp. 1–9.
*   [5] A. Johri, A. S. Katz, J. Qadir, and A. Hingle, “Generative artificial intelligence and engineering education.” *Journal of Engineering Education*, vol. 112, no. 3, 2023.
*   [6] A. Fischer, S. Greiff, and J. Funke, “The process of solving complex problems,” *The Journal of Problem Solving*, vol. 4, no. 1, pp. 19–42, 2012.
*   [7] P. A. Frensch and J. Funke, “Definitions, traditions, and a general framework for understanding complex problem solving,” in *Complex problem solving: The European perspective*.   Lawrence Erlbaum Associates, Hillsdale, NJ, 1995, pp. 24–43.
*   [8] P. M. Senge, *The Fifth Discipline: The Art and Practice of the Learning Organization*.   Doubleday, 1990.
*   [9] J. Surowiecki, *The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies, and Nations*.   Anchor, 2004.
*   [10] M. Wooldridge, *An introduction to multiagent systems*.   John wiley & sons, 2009.
*   [11] S. E. Page, *The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies*.   Princeton University Press, 2007.
*   [12] M. Minsky, *The Society of Mind*.   Simon and Schuster, 1986.
*   [13] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin *et al.*, “A survey on large language model based autonomous agents,” *Frontiers of Computer Science*, vol. 18, no. 6, p. 186345, 2024.
*   [14] T. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla, O. Wiest, and X. Zhang, “Large language model based multi-agents: A survey of progress and challenges,” *arXiv preprint arXiv:2402.01680*, 2024.
*   [15] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel: Communicative agents for “mind”’ exploration of large language model society,” *Advances in Neural Information Processing Systems*, vol. 36, pp. 51 991–52 008, 2023.
*   [16] Q. Wang, T. Wang, Q. Li, J. Liang, and B. He, “Megaagent: A practical framework for autonomous cooperation in large-scale LLM agent systems,” *arXiv preprint arXiv:2408.09955*, 2024.
*   [17] Z. Wang, S. Mao, W. Wu, T. Ge, F. Wei, and H. Ji, “Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration,” *arXiv preprint arXiv:2307.05300*, 2023.
*   [18] J. Moura, “Crew.ai: An open-source multi-agent orchestration framework,” [https://www.crewai.com/](https://www.crewai.com/), 2024, accessed: October 12, 2024.
*   [19] X. Pan, D. Gao, Y. Xie, Z. Wei, Y. Li, B. Ding, J.-R. Wen, and J. Zhou, “Very large-scale multi-agent simulation in agentscope,” *arXiv preprint arXiv:2407.17789*, 2024.
*   [20] T. Xie, F. Zhou, Z. Cheng, P. Shi, L. Weng, Y. Liu, T. J. Hua, J. Zhao, Q. Liu, C. Liu *et al.*, “Openagents: An open platform for language agents in the wild,” *arXiv preprint arXiv:2310.10634*, 2023.
*   [21] D. Yin, F. Brahman, A. Ravichander, K. Chandu, K.-W. Chang, Y. Choi, and B. Y. Lin, “Agent lumos: Unified and modular training for open-source language agents,” in *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 2024, pp. 12 380–12 403.
*   [22] OpenAI. [Online]. Available: [https://openai.com/index/hello-gpt-4o](https://openai.com/index/hello-gpt-4o)
*   [23] CamelAI-Documentation. [Online]. Available: [https://docs.camel-ai.org/camel.html](https://docs.camel-ai.org/camel.html)
*   [24] D. Hulbert, “Using tree-of-thought prompting to boost chatgpt’s reasoning,” [https://github.com/dave1010/tree-of-thought-prompting](https://github.com/dave1010/tree-of-thought-prompting), May 2023\. [Online]. Available: [https://doi.org/10.5281/zenodo.10323452](https://doi.org/10.5281/zenodo.10323452)
*   [25] E. Saravia, “Prompt Engineering Guide,” *https://github.com/dair-ai/Prompt-Engineering-Guide*, 12 2022.
*   [26] D. Biber, B. Gray, and K. Poonpon, “Should we use characteristics of conversation to measure grammatical complexity in L2 writing development?” *Tesol Quarterly*, vol. 45, no. 1, pp. 5–35, 2011.
*   [27] M. A. K. Halliday and R. Hasan, *Cohesion in english*.   Routledge, 2014.
*   [28] J. Kincaid, “Derivation of new readability formulas (automated readability index, fog count and Flesch reading ease formula) for navy enlisted personnel,” *Chief of Naval Technical Training*, 1975.
*   [29] S. Schulhoff, M. Ilie, N. Balepur, K. Kahadze, A. Liu, C. Si, Y. Li, A. Gupta, H. Han, S. Schulhoff *et al.*, “The prompt report: A systematic survey of prompting techniques,” *arXiv preprint arXiv:2406.06608*, 2024.
*   [30] E. Karpas, O. Abend, Y. Belinkov, B. Lenz, O. Lieber, N. Ratner, Y. Shoham, H. Bata, Y. Levine, K. Leyton-Brown *et al.*, “Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning,” *arXiv preprint arXiv:2205.00445*, 2022.
*   [31] O. Topsakal and T. C. Akinci, “Creating large language model applications utilizing LangChain: A primer on developing LLM apps fast,” in *International Conference on Applied Engineering and Natural Sciences*, vol. 1, no. 1, 2023, pp. 1050–1056.
*   [32] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang, X. Zhang, and C. Wang, “AutoGen: Enabling next-gen LLM applications via multi-agent conversation framework,” *arXiv preprint arXiv:2308.08155*, 2023.
*   [33] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, “ReAct: Synergizing reasoning and acting in language models,” *arXiv preprint arXiv:2210.03629*, 2022.
*   [34] A. Hingle and A. Johri, “A framework to develop and implement role-play case studies to teach responsible technology use,” *IEEE Transactions on Technology and Society*, 2024.
*   [35] J. Qadir, A.-E. M. Taha, K.-L. A. Yau, J. Ponciano, S. Hussain, A. Al-Fuqaha, and M. A. Imran, “Leveraging the force of formative assessment & feedback for effective engineering education,” 2020.
*   [36] Prompting Guide, “The prompting guide: LLM agents research,” 2024, accessed: 2024-11-13\. [Online]. Available: [https://www.promptingguide.ai/research/llm-agents](https://www.promptingguide.ai/research/llm-agents)</foreignobject></g></g></svg>