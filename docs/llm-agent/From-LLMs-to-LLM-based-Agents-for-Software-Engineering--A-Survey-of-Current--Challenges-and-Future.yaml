- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:39:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:39:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From LLMs to LLM-based Agents for Software Engineering: A Survey of Current,
    Challenges and Future'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从LLMs到基于LLM的代理在软件工程中的应用：现状、挑战与未来的调查
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02479](https://ar5iv.labs.arxiv.org/html/2408.02479)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.02479](https://ar5iv.labs.arxiv.org/html/2408.02479)
- en: 'Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen Haolin
    Jin, Linghan Huang and Huaming Chen are with the School of Electrical and Computer
    Engineering, The University of Sydney, Sydney, 2006, Australia. (email: huaming.chen@sydney.edu.au)Haipeng
    Cai is with the School of Electrical Engineering and Computer Science at Washington
    State University, USJun Yan is with the School of Computing and Information Technology
    at University of Wollongong, AustraliaBo Li is with the Computer Science Department
    at the University of Chicago, US'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Haolin Jin、Linghan Huang、Haipeng Cai、Jun Yan、Bo Li、Huaming Chen，Haolin Jin、Linghan
    Huang和Huaming Chen在悉尼大学电气与计算机工程学院工作，悉尼，2006年，澳大利亚。（电子邮件：huaming.chen@sydney.edu.au）Haipeng
    Cai在华盛顿州立大学电气工程与计算机科学学院工作，USJun Yan在卧龙岗大学计算与信息技术学院工作，澳大利亚Bo Li在芝加哥大学计算机科学系工作，US
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'With the rise of large language models (LLMs), researchers are increasingly
    exploring their applications in various vertical domains, such as software engineering.
    LLMs have achieved remarkable success in areas including code generation and vulnerability
    detection. However, they also exhibit numerous limitations and shortcomings. LLM-based
    agents, a novel technology with the potential for Artificial General Intelligence
    (AGI), combine LLMs as the core for decision-making and action-taking, addressing
    some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.
    Despite numerous studies and surveys exploring the possibility of using LLMs in
    software engineering, it lacks a clear distinction between LLMs and LLM-based
    agents. It is still in its early stage for a unified standard and benchmarking
    to qualify an LLM solution as an LLM-based agent in its domain. In this survey,
    we broadly investigate the current practice and solutions for LLMs and LLM-based
    agents for software engineering. In particular we summarise six key topics: requirement
    engineering, code generation, autonomous decision-making, software design, test
    generation, and software maintenance. We review and differentiate the work of
    LLMs and LLM-based agents from these six topics, examining their differences and
    similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss
    the models and benchmarks used, providing a comprehensive analysis of their applications
    and effectiveness in software engineering. We anticipate this work will shed some
    lights on pushing the boundaries of LLM-based agents in software engineering for
    future research.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的兴起，研究人员越来越多地探索其在各种垂直领域的应用，如软件工程。LLMs在代码生成和漏洞检测等领域取得了显著成功。然而，它们也展示了许多局限性和缺陷。基于LLM的代理作为一种具有人工通用智能（AGI）潜力的新技术，将LLMs作为决策和行动的核心，解决了LLMs的一些固有局限性，如缺乏自主性和自我改进。尽管已有大量研究和调查探讨了在软件工程中使用LLMs的可能性，但它们与基于LLM的代理之间缺乏明确的区分。在为LLM解决方案制定统一标准和基准测试以将其定性为领域内的基于LLM的代理方面仍处于早期阶段。在本调查中，我们广泛调查了LLMs和基于LLM的代理在软件工程中的当前实践和解决方案。特别是我们总结了六个关键主题：需求工程、代码生成、自治决策、软件设计、测试生成和软件维护。我们回顾并区分了LLMs和基于LLM的代理在这六个主题中的工作，检查了它们在任务、基准测试和评估指标方面的差异和相似性。最后，我们讨论了使用的模型和基准，提供了它们在软件工程中的应用和效果的全面分析。我们预期这项工作将为推动基于LLM的代理在软件工程中的未来研究提供一些启示。
- en: 'Index Terms:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Large Language Models, LLM-based Agents, Software Engineering, Benchmark, Software
    Security, AI System Development
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型、基于LLM的代理、软件工程、基准测试、软件安全、AI系统开发
- en: I Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Software engineering (SE) has seen its booming research and development with
    the aid of artificial intelligence techniques. Traditional approaches leveraging
    neural networks and machine learning have facilitated various SE topics such as
    bug detection, code synthesis, and requirements analysis [[1](#bib.bib1)][[2](#bib.bib2)].
    However, they often present limitations, including the need for exclusive feature
    engineering, scalability issues, and the adaptability across diverse codebases.
    The rise of Large Language Models (LLMs) has embarked on new solutions and findings
    in this landscape. LLMs, such as GPT [[3](#bib.bib3)] and Codex [[4](#bib.bib4)],
    have demonstrated remarkable capabilities in handling downstream tasks in SE,
    including code generation, debugging, and documentation. These models leverage
    vast amounts of training data to generate human-like text, offering unprecedented
    levels of fluency and coherence. Studies have shown that LLMs can enhance productivity
    in software projects by providing intelligent code suggestions, automating repetitive
    tasks, even generating entire code snippets from natural language descriptions [[5](#bib.bib5)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程（SE）在人工智能技术的帮助下经历了蓬勃的发展。传统的方法利用神经网络和机器学习促进了各种SE主题，如错误检测、代码合成和需求分析 [[1](#bib.bib1)][[2](#bib.bib2)]。然而，它们通常存在局限性，包括对特定特征工程的需求、可扩展性问题以及在不同代码库中的适应性。大型语言模型（LLMs）的兴起在这一领域引发了新的解决方案和发现。LLMs，如**GPT** [[3](#bib.bib3)]和**Codex** [[4](#bib.bib4)]，在处理SE中的下游任务，如代码生成、调试和文档编写方面表现出了显著的能力。这些模型利用大量训练数据生成类似人类的文本，提供了前所未有的流畅性和连贯性。研究表明，LLMs通过提供智能代码建议、自动化重复任务，甚至从自然语言描述中生成完整的代码片段，可以提高软件项目的生产力 [[5](#bib.bib5)]。
- en: Despite their potential, there are significant challenges in applying LLMs to
    SE. One major issue is their limited context length [[6](#bib.bib6)], which restricts
    the model’s ability to comprehend and manage extensive codebases, making it challenging
    to maintain coherence over prolonged interactions. Hallucinations is another main
    concern, where the model generates code that appears plausible but is actually
    incorrect or nonsensical [[7](#bib.bib7)], potentially introducing bugs or vulnerabilities
    if not carefully reviewed by experienced developers. Additionally, the inability
    of LLMs to use external tools restricts their access to real-time data and prevent
    them from performing tasks outside their training scope. It diminishes their effectiveness
    in dynamic environments. These limitations significantly impact the application
    of LLMs in SE, and also highlight the need for expert developeers to critically
    refine and validate LLM-generated code for accuracy and security [[8](#bib.bib8)].
    In complex projects, the static nature of LLMs can hinder their ability to adapt
    to changing requirements or efficiently incorporate new information. Moreover,
    LLMs typically cannot interact with external tools or databases, further limits
    their utility in dynamic and evolving SE contexts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有潜力，但将LLMs应用于SE存在显著挑战。其中一个主要问题是它们有限的上下文长度 [[6](#bib.bib6)]，这限制了模型理解和管理大规模代码库的能力，使得在长时间的互动中保持连贯性变得具有挑战性。另一个主要问题是“幻觉”，即模型生成看似可信但实际上不正确或无意义的代码 [[7](#bib.bib7)]，如果不由经验丰富的开发者仔细审查，可能会引入错误或漏洞。此外，LLMs无法使用外部工具，限制了它们访问实时数据的能力，也阻止了它们执行超出训练范围的任务。这降低了它们在动态环境中的有效性。这些局限性显著影响了LLMs在SE中的应用，也突显了需要专家开发者对LLM生成的代码进行准确性和安全性审查 [[8](#bib.bib8)]。在复杂项目中，LLMs的静态特性可能阻碍其适应变化要求或有效整合新信息。此外，LLMs通常无法与外部工具或数据库互动，这进一步限制了它们在动态和不断发展的SE环境中的效用。
- en: To address these challenges, LLM-based agents have emerged [[9](#bib.bib9)][[10](#bib.bib10)],
    combining the strengths of LLMs with external tools and resources to enable more
    dynamic and autonomous operations. These agents leverage recent advancements in
    AI, such as Retrieval-Augmented Generation (RAG) and tool utilization, to perform
    more complex and contextually aware tasks [[11](#bib.bib11)]. For instance, OpenAI’s
    Codex has been integrated into GitHub Copilot [[12](#bib.bib12)], enabling real-time
    code suggestions and completion within development environments. Unlike static
    LLMs, LLM-based agents can perform a wide range of tasks, such as autonomously
    debugging code by identifying and fixing errors, proactively refactoring code
    to enhance efficiency or readability, and generating adaptive test cases that
    evolve alongside the codebase. These features make LLM-based agents a powerful
    tool for SE, capable of handling more complex and dynamic workflows than traditional
    LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，基于LLM的代理应运而生[[9](#bib.bib9)][[10](#bib.bib10)]，将LLM的优势与外部工具和资源结合起来，实现更动态和自主的操作。这些代理利用了最近在AI领域的进展，例如检索增强生成（RAG）和工具利用，以执行更复杂和具有上下文意识的任务[[11](#bib.bib11)]。例如，OpenAI
    的 Codex 已集成到 GitHub Copilot[[12](#bib.bib12)]，实现了开发环境中的实时代码建议和完成。与静态LLM不同，基于LLM的代理可以执行多种任务，例如通过识别和修复错误自主调试代码，主动重构代码以提高效率或可读性，以及生成随代码库演变的自适应测试用例。这些特性使基于LLM的代理成为一个强大的SE工具，能够处理比传统LLM更复杂和动态的工作流程。
- en: Historically, AI agents focused on autonomous actions based on predefined rules
    or learning from interactions  [[13](#bib.bib13)][[14](#bib.bib14)]. The integration
    of LLMs has presented new opportunities in this area, providing the language understanding
    and generative capabilities needed for more sophisticated agent behaviors. [[10](#bib.bib10)]
    shows that LLM-based agents are capable of autonomous reasoning and decision-making,
    achieving the third and fourth levels of WS (World Scope) [[15](#bib.bib15)],
    which outlines the progression from natural language processing (NLP) to general
    AI. In software engineering, LLM-based agents show promise in areas such as autonomous
    debugging, code refactoring, and adaptive test generation, demonstrating capabilities
    that approach artificial general intelligence(AGI).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，AI 代理主要集中在基于预定义规则的自主行动或通过互动学习[[13](#bib.bib13)][[14](#bib.bib14)]。大语言模型（LLMs）的整合在这一领域带来了新的机会，提供了更复杂的代理行为所需的语言理解和生成能力。[[10](#bib.bib10)]
    显示，基于LLM的代理能够进行自主推理和决策，达到WS（世界范围）[[15](#bib.bib15)]的第三和第四级，这描述了从自然语言处理（NLP）到通用人工智能（AGI）的发展。在软件工程中，基于LLM的代理在自主调试、代码重构和自适应测试生成等领域显示出了前景，展现了接近人工通用智能（AGI）的能力。
- en: '![Refer to caption](img/a9437da8d7ae90738b5eca143d11fc83.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a9437da8d7ae90738b5eca143d11fc83.png)'
- en: 'Figure 1: Paper Number for LLMs and LLM-based Agent between 2020-2024'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：2020-2024年LLM和LLM基于代理的论文数量
- en: 'In this work, we present, to the best of our knowledge, a first survey outlining
    the integration and transformation of LLMs to LLM-based agents in the domain of
    SE. Our survey covers six key themes in SE:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们呈现了我们所知的首次调查，概述了LLM到LLM基于代理在软件工程（SE）领域的整合和转变。我们的调查涵盖了SE中的六个关键主题：
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Requirement Engineering and Documentation: Capturing, analyzing, and documenting
    software requirements, as well as generating user manuals and technical documentation.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需求工程与文档：捕捉、分析和记录软件需求，以及生成用户手册和技术文档。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Code Generation and Software Development: Automating code generation, assisting
    in the development lifecycle, refactoring code, and providing intelligent code
    recommendations.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码生成与软件开发：自动化代码生成，协助开发生命周期，重构代码，并提供智能代码建议。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Autonomous Learning and Decision Making: Highlighting the capabilities of LLM-based
    agents in autonomous learning, decision-making, and adaptive planning within SE
    contexts.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自主学习与决策：强调LLM基于代理在SE环境中的自主学习、决策和自适应规划能力。
- en: '4.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Software Design and Evaluation: Contributing to design processes, architecture
    validation, performance evaluation, and code quality assessment.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件设计与评估：参与设计过程、架构验证、性能评估和代码质量评估。
- en: '5.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Software Test Generation: Generating, optimizing, and maintaining software
    tests, including unit tests, integration tests, and system tests.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件测试生成：生成、优化和维护软件测试，包括单元测试、集成测试和系统测试。
- en: '6.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Software Security & Maintenance: Enhancing security protocols, facilitating
    maintenance tasks, and aiding in vulnerability detection and patching.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件安全与维护：增强安全协议，促进维护任务，并协助漏洞检测和修补。
- en: 'In detail, we aim to address following research questions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 详细而言，我们旨在解决以下研究问题：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: What are the state-of-the-art techniques and practices in LLMs and LLM-based
    agents for SE? (Section. [IV](#S4 "IV Requirement Engineering and and Documentation
    ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey of Current,
    Challenges and Future")- [IX](#S9 "IX Software Security and Maintenance ‣ VIII-E
    Evaluation Metrics ‣ VIII Software Test Generation ‣ VII-E Evaluation Metrics
    ‣ VII Software Design and Evaluation ‣ VI-E Evaluation Metrics ‣ VI Autonomous
    Learning and Decision Making ‣ V-E Evaluation Metrics ‣ V Code Generation and
    Software Development ‣ IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement
    Engineering and and Documentation ‣ From LLMs to LLM-based Agents for Software
    Engineering: A Survey of Current, Challenges and Future"))'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ1：LLM和基于LLM的代理在软件工程中的最先进技术和实践是什么？（章节. [IV](#S4 "IV 需求工程与文档 ‣ 从LLM到LLM基础的代理：当前状况、挑战与未来的调查")-
    [IX](#S9 "IX 软件安全与维护 ‣ VIII-E 评价指标 ‣ VIII 软件测试生成 ‣ VII-E 评价指标 ‣ VII 软件设计与评估 ‣
    VI-E 评价指标 ‣ VI 自主学习与决策 ‣ V-E 评价指标 ‣ V 代码生成与软件开发 ‣ IV-E 评价指标 ‣ IV-D 基准 ‣ IV 需求工程与文档
    ‣ 从LLM到LLM基础的代理：当前状况、挑战与未来的调查"))
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: What are the key differences in task performance between LLMs and LLM-based
    agents in SE applications? (Section. [IV](#S4 "IV Requirement Engineering and
    and Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A
    Survey of Current, Challenges and Future")- [IX](#S9 "IX Software Security and
    Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software Test Generation ‣ VII-E
    Evaluation Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation Metrics
    ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation Metrics ‣ V Code
    Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D Benchmarks
    ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based Agents
    for Software Engineering: A Survey of Current, Challenges and Future"))'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ1：LLM和基于LLM的代理在软件工程应用中的任务表现有什么关键差异？（章节. [IV](#S4 "IV 需求工程与文档 ‣ 从LLM到LLM基础的代理：当前状况、挑战与未来的调查")-
    [IX](#S9 "IX 软件安全与维护 ‣ VIII-E 评价指标 ‣ VIII 软件测试生成 ‣ VII-E 评价指标 ‣ VII 软件设计与评估 ‣
    VI-E 评价指标 ‣ VI 自主学习与决策 ‣ V-E 评价指标 ‣ V 代码生成与软件开发 ‣ IV-E 评价指标 ‣ IV-D 基准 ‣ IV 需求工程与文档
    ‣ 从LLM到LLM基础的代理：当前状况、挑战与未来的调查"))
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: Which benchmark datasets and evaluation metrics are most commonly used
    for assessing the performance of LLMs and LLM-based agents in SE tasks? (Section. [IV](#S4
    "IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based Agents
    for Software Engineering: A Survey of Current, Challenges and Future")- [IX](#S9
    "IX Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future") and Section. [X](#S10 "X Discussion ‣ IX-E Evaluation Metrics ‣ IX
    Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"))'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2：评估 LLM 和基于 LLM 的代理在 SE 任务中性能时，最常用的基准数据集和评估指标有哪些？（第 [IV](#S4 "IV 需求工程和文档
    ‣ 从 LLM 到基于 LLM 的软件工程代理：当前、挑战和未来的调查")- [IX](#S9 "IX 软件安全和维护 ‣ VIII-E 评估指标 ‣ VIII
    软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计和评估 ‣ VI-E 评估指标 ‣ VI 自主学习和决策 ‣ V-E 评估指标 ‣ V 代码生成和软件开发
    ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程和文档 ‣ 从 LLM 到基于 LLM 的软件工程代理：当前、挑战和未来的调查") 和第 [X](#S10
    "X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全和维护 ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII
    软件设计和评估 ‣ VI-E 评估指标 ‣ VI 自主学习和决策 ‣ V-E 评估指标 ‣ V 代码生成和软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准
    ‣ IV 需求工程和文档 ‣ 从 LLM 到基于 LLM 的软件工程代理：当前、挑战和未来的调查")
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3: What are the predominant experimental models and methodologies employed
    when utilizing LLMs in SE? (Section. [X](#S10 "X Discussion ‣ IX-E Evaluation
    Metrics ‣ IX Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII
    Software Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and
    Evaluation ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making
    ‣ V-E Evaluation Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation
    Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"))'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3：在使用 LLM 进行 SE 时，主要采用的实验模型和方法是什么？（第 [X](#S10 "X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全和维护
    ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计和评估 ‣ VI-E 评估指标 ‣ VI 自主学习和决策
    ‣ V-E 评估指标 ‣ V 代码生成和软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程和文档 ‣ 从 LLM 到基于 LLM 的软件工程代理：当前、挑战和未来的调查")
- en: II EXISTING WORKS AND THE SURVEY STRUCTURE
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 现有工作和调查结构
- en: II-A Existing works
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 现有工作
- en: 'In recent years, large language models have been primarily applied to help
    programmers generate code and fix bugs. These models understand and complete code
    or text based on the user’s input, leveraging their training data and reasoning
    capabilities. In previous survey papers, such as Angela Fan’s research [[8](#bib.bib8)],
    there has not been much elaboration on requirement engineering. As mentioned in
    the paper, software engineers are generally reluctant to rely on LLMs for higher-level
    design goals. However, with LLMs achieving remarkable improvements in contextual
    analysis and reasoning abilities through various methods like prompt engineering
    and Chain-of-Thought (COT) [[16](#bib.bib16)], their applications in requirement
    engineering are gradually increasing. Table [I](#S2.T1 "TABLE I ‣ II-A Existing
    works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣ From LLMs to LLM-based Agents
    for Software Engineering: A Survey of Current, Challenges and Future") summarizes
    and categorizes the tasks in requirement engineering. Many studies utilize models
    for requirement classification and generation. Since the collection primarily
    focuses on the latter half of 2023 and before April 2024, and some papers address
    multiple tasks, the table does not reflect the exact number of papers we have
    collected.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '近年来，大型语言模型主要被应用于帮助程序员生成代码和修复错误。这些模型基于用户的输入，利用其训练数据和推理能力来理解和完成代码或文本。在以往的调查论文中，如Angela
    Fan的研究[[8](#bib.bib8)]，对于需求工程的阐述并不多。正如论文中提到的，软件工程师通常不愿意依赖大型语言模型（LLMs）来进行高级设计目标。然而，通过各种方法，如提示工程和链式思维（Chain-of-Thought，COT）[[16](#bib.bib16)]，LLMs在上下文分析和推理能力方面取得了显著进展，其在需求工程中的应用逐渐增加。表[I](#S2.T1
    "TABLE I ‣ II-A Existing works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")总结并分类了需求工程中的任务。许多研究利用模型进行需求分类和生成。由于收集主要集中在2023年下半年和2024年4月之前，并且一些论文涉及多个任务，表格未能反映我们所收集的论文的确切数量。'
- en: 'TABLE I: Distribution of SE tasks'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：SE任务分布
- en: '| Category | LLMs | LLM-based agents | Total |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | LLMs | 基于LLMs的代理 | 总计 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Requirement &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需求 &#124;'
- en: '&#124; Engineering and &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工程 &#124;'
- en: '&#124; Documentation &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档 &#124;'
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Requirement Classification and Extraction (3) &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需求分类和提取 (3) &#124;'
- en: '&#124; Requirement Generation and Description (4) &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需求生成和描述 (4) &#124;'
- en: '&#124; Requirements Satisfaction Assessment (1) &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需求满足评估 (1) &#124;'
- en: '&#124; Specification Generation (3) &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 规范生成 (3) &#124;'
- en: '&#124; Quality Evaluation (2) &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 质量评估 (2) &#124;'
- en: '&#124; Ambiguity Detection (2) &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模糊性检测 (2) &#124;'
- en: '|'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Generation of Semi-structured Documents (1) &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 半结构化文档生成 (1) &#124;'
- en: '&#124; Generate safety requirements (1) &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成安全需求 (1) &#124;'
- en: '&#124; Automatically generating use cases based on &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于生成的用例 &#124;'
- en: '&#124; high-level requirements (1) &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高级需求 (1) &#124;'
- en: '&#124; Automated User Story Quality Enhancement (2) &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化用户故事质量提升 (2) &#124;'
- en: '| 19 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 19 |'
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code Generation &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码生成 &#124;'
- en: '&#124; and &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和 &#124;'
- en: '&#124; software &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软件 &#124;'
- en: '&#124; development &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发 &#124;'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code Generation Debugging (3) &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码生成调试 (3) &#124;'
- en: '&#124; Code Evaluation (2) &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码评估 (2) &#124;'
- en: '&#124; Implement HTTP server (1) &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 实现HTTP服务器 (1) &#124;'
- en: '&#124; Enhancing Code Generation Capabilities (3) &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提升代码生成能力 (3) &#124;'
- en: '&#124; Specialized Code Generation (2) &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 专用代码生成 (2) &#124;'
- en: '&#124; Human Feedback Preference Simulation (1) &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人工反馈偏好模拟 (1) &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Automating the Software Development Process (5) &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化软件开发过程 (5) &#124;'
- en: '&#124; Large-Scale Code and Document Generation (1) &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大规模代码和文档生成 (1) &#124;'
- en: '&#124; Tool and External API Usage (2) &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具和外部API使用 (2) &#124;'
- en: '&#124; Multi-Agent Collaboration and Code Refine (2) &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多代理协作和代码优化 (2) &#124;'
- en: '&#124; Improving Code Generation Quality (2) &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提高代码生成质量 (2) &#124;'
- en: '| 23 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 23 |'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Autonomous &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自主 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '&#124; and Decision &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和决策 &#124;'
- en: '&#124; Making &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 制作 &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-LLMs Decision-Making (1) &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多LLMs决策 (1) &#124;'
- en: '&#124; Creativity Evaluation (1) &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 创造力评估 (1) &#124;'
- en: '&#124; Self-Identify and Correct Code (1) &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自我识别和修正代码 (1) &#124;'
- en: '&#124; Judge Chatbot Response (1) &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评判聊天机器人响应 (1) &#124;'
- en: '&#124; Mimics Human Scientific Debugging (1) &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模仿人类科学调试 (1) &#124;'
- en: '&#124; Deliberate Problem Solving(1) &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深思熟虑的问题解决 (1) &#124;'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Collaborative Decision-Making and Multi-Agent &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协作决策与多智能体 &#124;'
- en: '&#124; Systems (5) &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统 (5) &#124;'
- en: '&#124; Autonomous Reasoning and Decision-Making (7) &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自主推理与决策 (7) &#124;'
- en: '&#124; Learning and Adaptation Through Feedback (4) &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过反馈学习与适应 (4) &#124;'
- en: '&#124; Simulation and Evaluation of Human-like &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类人行为的模拟与评估 &#124;'
- en: '&#124; Behaviors (2) &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行为 (2) &#124;'
- en: '| 24 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 24 |'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Software Design &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软件设计 &#124;'
- en: '&#124; and Evaluation &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和评估 &#124;'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Creative Capabilities Evaluation (1) &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 创造能力评估 (1) &#124;'
- en: '&#124; Performance in SE Tasks (1) &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SE任务中的表现 (1) &#124;'
- en: '&#124; Educational Utility and Assessment (1) &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 教育实用性与评估 (1) &#124;'
- en: '&#124; Efficiency Optimization (2) &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 效率优化 (2) &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Automation of Software Engineering Processes (3) &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软件工程过程自动化 (3) &#124;'
- en: '&#124; Enhancing Problem Solving and Reasoning (4) &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强问题解决与推理能力 (4) &#124;'
- en: '&#124; Integration and Management of AI Models and &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AI模型的集成与管理 &#124;'
- en: '&#124; Tools (3) &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具 (3) &#124;'
- en: '&#124; Optimization and Efficiency Improvement (2) &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 优化与效率提升 (2) &#124;'
- en: '&#124; Performance Assessment in Dynamic Environments &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 动态环境下的性能评估 &#124;'
- en: '&#124; (2) &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (2) &#124;'
- en: '| 19 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 19 |'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Software Test &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软件测试 &#124;'
- en: '&#124; Generation &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成 &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Bug Reproduction and Debugging (2) &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误重现与调试 (2) &#124;'
- en: '&#124; Security Test (2) &#124;'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全测试 (2) &#124;'
- en: '&#124; Test Coverage (2) &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试覆盖率 (2) &#124;'
- en: '&#124; Universal Fuzzing (1) &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通用模糊测试 (1) &#124;'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-agent Collaborative Test Generation (2) &#124;'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多智能体协作测试生成 (2) &#124;'
- en: '&#124; Autonomous Testing and Conversational Interfaces &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自主测试与对话接口 &#124;'
- en: '&#124; (3) &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (3) &#124;'
- en: '| 11 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 11 |'
- en: '|'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Software Security &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软件安全 &#124;'
- en: '&#124; & &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; & &#124;'
- en: '&#124; Maintainance &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 维护 &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Vulnerability Detection (6) &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 漏洞检测 (6) &#124;'
- en: '&#124; Vulnerability Repair (2) &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 漏洞修复 (2) &#124;'
- en: '&#124; Program Repair (4) &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 程序修复 (4) &#124;'
- en: '&#124; Robustness Testing (1) &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稳健性测试 (1) &#124;'
- en: '&#124; Requirements Analysis (1) &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需求分析 (1) &#124;'
- en: '&#124; Fuzzing (1) &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模糊测试 (1) &#124;'
- en: '&#124; Duplicate Entry (1) &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重复条目 (1) &#124;'
- en: '&#124; Code Generation and Debugging (4) &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码生成与调试 (4) &#124;'
- en: '&#124; Penetration Testing and Security Assessment (2) &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 渗透测试与安全评估 (2) &#124;'
- en: '&#124; Program Analysis and Debugging (1) &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 程序分析与调试 (1) &#124;'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Autonomous Software Development and &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自主软件开发与 &#124;'
- en: '&#124; Maintenance (4) &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 维护 (4) &#124;'
- en: '&#124; Debugging and Fault Localization (4) &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调试与故障定位 (4) &#124;'
- en: '&#124; Vulnerability Detection and Penetration &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 漏洞检测与渗透 &#124;'
- en: '&#124; Testing (3) &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试 (3) &#124;'
- en: '&#124; Smart Contract Auditing and Repair (2) &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 智能合约审计与修复 (2) &#124;'
- en: '&#124; Safety and Risk Analysis (2) &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 安全与风险分析 (2) &#124;'
- en: '&#124; Adaptive and Communicative Agents (1) &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应与交互型智能体 (1) &#124;'
- en: '| 39 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 39 |'
- en: '![Refer to caption](img/cb7e952bb385cfa2b158960b6fd329dc.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cb7e952bb385cfa2b158960b6fd329dc.png)'
- en: 'Figure 2: Paper Distribution'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 论文分布'
- en: 'While other works have surveyed LLMs applications in some SE tasks [[17](#bib.bib17)] [[8](#bib.bib8)] [[18](#bib.bib18)],
    they lack a wider coverage of the general SE area to incorporate recent research
    developments. More importantly, a focus of LLMs is the main contributions of these
    works, but there is no distinguish the capabilities between LLMs and LLM-based
    agents. We summarize the difference between our work and others in Table [II](#S2.T2
    "TABLE II ‣ II-A Existing works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"), this survey addresses these limitations by distinctly analyzing
    LLMs and LLM-based agents applications across six SE domains, providing a thorough
    and up-to-date review. From previous research, it is evident that the performance
    of LLMs in various applications and tasks heavily depends on the model’s inherent
    capabilities  [[10](#bib.bib10)]. More importantly, earlier surveys often present
    findings from papers spanning in a wide range of publication dates, leading to
    significant content disparities for LLMs in different SE tasks. For instance,
    research in requirement engineering was relatively nascent, resulting in sparse
    content in this area in previous surveys. The recent rise of LLM-based agents,
    with their enhanced capabilities and autonomy, fills these gaps. By focusing on
    the latest researches and clearly differentiating between LLMs and LLM-based agents,
    our survey provides a thorough and in-depth overview of how these technologies
    are applied and new opportunities they bring to SE.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管其他研究已调查了LLMs在一些SE任务中的应用[[17](#bib.bib17)] [[8](#bib.bib8)] [[18](#bib.bib18)]，但它们缺乏对整个SE领域的广泛覆盖，以融入近期的研究进展。更重要的是，LLMs的主要贡献是这些工作的焦点，但没有区分LLMs和LLM-based
    agents之间的能力差异。我们在表[II](#S2.T2 "TABLE II ‣ II-A Existing works ‣ II EXISTING WORKS
    AND THE SURVEY STRUCTURE ‣ From LLMs to LLM-based Agents for Software Engineering:
    A Survey of Current, Challenges and Future")中总结了我们工作与其他工作的区别，本次调查通过明确分析LLMs和LLM-based
    agents在六个SE领域中的应用，提供了一个全面且最新的评审。从以往研究中可以看出，LLMs在各种应用和任务中的表现高度依赖于模型的固有能力[[10](#bib.bib10)]。更重要的是，早期的调查通常呈现了跨越广泛出版日期的论文的发现，导致不同SE任务中的LLMs内容差异显著。例如，需求工程领域的研究相对新兴，导致之前的调查中这一领域的内容稀少。LLM-based
    agents的最新兴起及其增强的能力和自主性填补了这些空白。通过聚焦于最新的研究，并明确区分LLMs和LLM-based agents，我们的调查提供了对这些技术如何应用以及它们为SE带来的新机会的全面而深入的概述。'
- en: 'In summary, we have collected a total of 117 papers directly relevant to this
    topic, covering the six SE domains mentioned earlier as shown in Figure.[1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ From LLMs to LLM-based Agents for Software Engineering:
    A Survey of Current, Challenges and Future"). Our analysis distinguishes between
    LLM and LLM-based agent contributions, offering a comparative overview and addressing
    the limitations of previous surveys. Considering the novelty nature of the LLM-based
    agents field and the lack of standardized benchmarks, this work seeks to offer
    a detailed review that can guide future research and provide a clearer view of
    the potential of these technologies in SE.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，我们收集了总共117篇直接相关的论文，涵盖了前面提到的六个SE领域，如图[1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey of Current,
    Challenges and Future")所示。我们的分析区分了LLM和LLM-based agent的贡献，提供了对比概述，并讨论了之前调查的局限性。考虑到LLM-based
    agents领域的创新性和缺乏标准化基准，这项工作旨在提供详细的评审，以指导未来的研究，并为这些技术在SE领域的潜力提供更清晰的视角。'
- en: 'TABLE II: Comparison between Our Work and the Existing Work for LLM in SE'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：我们的工作与现有工作在SE中LLM的比较
- en: '| Paper | Year | Domain | Benchmarks | Metrics |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 年份 | 领域 | 基准 | 指标 |'
- en: '&#124; Agent &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Agent &#124;'
- en: '&#124; in SE &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在SE中 &#124;'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Agent &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Agent &#124;'
- en: '&#124; Distinction &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区别 &#124;'
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|  [[19](#bib.bib19)] | 2023 | GenAI in SE | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  [[19](#bib.bib19)] | 2023 | SE中的GenAI | ✓ | ✓ | ✓ | ✗ |'
- en: '|  [[8](#bib.bib8)] | 2023 | LLM in SE | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  [[8](#bib.bib8)] | 2023 | SE中的LLM | ✓ | ✓ | ✓ | ✗ |'
- en: '|  [[18](#bib.bib18)] | 2023 | Generation task by LLM in SE | ✓ | ✓ | ✗ | ✗
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  [[18](#bib.bib18)] | 2023 | SE中LLM的生成任务 | ✓ | ✓ | ✗ | ✗ |'
- en: '|  [[20](#bib.bib20)] | 2023 | LLM in syntax comprehension | ✓ | ✓ | ✗ | ✗
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  [[20](#bib.bib20)] | 2023 | LLM在语法理解中的应用 | ✓ | ✓ | ✗ | ✗ |'
- en: '|  [[21](#bib.bib21)] | 2024 | LLM4Code in SE | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  [[21](#bib.bib21)] | 2024 | SE中的LLM4Code | ✓ | ✓ | ✗ | ✗ |'
- en: '|  [[17](#bib.bib17)] | 2024 | LLM for process optimization in SE | ✓ | ✓ |
    ✗ | ✗ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  [[17](#bib.bib17)] | 2024 | 过程优化中的LLM | ✓ | ✓ | ✗ | ✗ |'
- en: '|  [[22](#bib.bib22)] | 2024 | Generation task by LLM in SE | ✓ | ✓ | ✗ | ✗
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  [[22](#bib.bib22)] | 2024 | SE中的LLM生成任务 | ✓ | ✓ | ✗ | ✗ |'
- en: '| Ours | 2024 | LLM & LLM-based Agent in SE | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 2024 | SE中的LLM & 基于LLM的代理 | ✓ | ✓ | ✓ | ✓ |'
- en: II-B Methodology
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 方法论
- en: 'The paper collection process primarily involved searching DBLP and arXiv databases,
    focusing on recent studies from the latter half of 2023 to May 2024\. This approach
    ensured the inclusion of the latest research. We filtered out non-LLM-related
    papers and those with fewer than seven pages. To further refine our selection,
    we used keywords in Table [III](#S2.T3 "TABLE III ‣ II-C Overall Structure of
    the Work ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future")
    to search SE-related works. We then manually screened the remaining papers to
    remove any with formatting errors or student projects. Additionally, we employed
    a snowballing search technique to capture significant works that might have been
    missed initially. Overall, we identified 117 relevant papers. Figure. [2](#S2.F2
    "Figure 2 ‣ II-A Existing works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future") presents the distribution of these papers across the six SE domains
    and the proportion of LLM-based agents studies. However, some papers can be counted
    as multi-class fields so the literature review in the figure is more than 117
    in total.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '论文收集过程主要涉及搜索DBLP和arXiv数据库，重点关注2023年下半年至2024年5月的最新研究。这种方法确保了包含最新研究成果。我们筛选出了与LLM无关的论文以及少于七页的论文。为了进一步优化选择，我们使用了表[III](#S2.T3
    "TABLE III ‣ II-C Overall Structure of the Work ‣ II EXISTING WORKS AND THE SURVEY
    STRUCTURE ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey of
    Current, Challenges and Future")中的关键词搜索SE相关的作品。然后，我们手动筛选剩余的论文，剔除格式错误或学生项目的论文。此外，我们还采用了滚雪球搜索技术，以捕捉可能被最初遗漏的重要作品。总体而言，我们确定了117篇相关论文。图[2](#S2.F2
    "Figure 2 ‣ II-A Existing works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")展示了这些论文在六个SE领域中的分布以及基于LLM的代理研究的比例。然而，一些论文可以被计为多类领域，因此图中的文献综述总数超过117篇。'
- en: II-C Overall Structure of the Work
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 工作的总体结构
- en: The remainder of this paper is organized as follows Section 2 Introduces the
    architectures and background of LLMs and LLM-based agents, including an overview
    of RAG, tool utilization, and their implications for SE. Section 3-8 is the comparative
    analysis which Summarizes and compares the datasets, tasks, benchmarks, and metrics
    used in LLM and LLM-based agents studies across the six SE domains. Section 9
    is the general discussion, section 10 is the final conclusion.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下：第2节介绍了LLMs和基于LLM的代理的架构和背景，包括RAG的概述、工具的使用及其对SE的影响。第3-8节是比较分析，总结和比较了LLM及基于LLM的代理在六个SE领域中使用的数据集、任务、基准和指标。第9节是一般讨论，第10节是最终结论。
- en: 'TABLE III: Keywords for Software Engineering Topics'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：软件工程主题的关键词
- en: '| Topic | Keywords |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 关键词 |'
- en: '| --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Software Security & Maintenance | Software security, Vulnerability detection,
    Automated Program repair, Self-debugging, Vulnerability reproduction |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 软件安全与维护 | 软件安全，漏洞检测，自动化程序修复，自我调试，漏洞重现 |'
- en: '| Code Generation and Software Development | Code generation, Automatic code
    synthesis, Code refactoring, Programming language translation, Software development
    automation, Code completion, AI-assisted coding, Development lifecycle automation
    |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成与软件开发 | 代码生成，自动代码合成，代码重构，编程语言翻译，软件开发自动化，代码补全，AI辅助编码，开发生命周期自动化 |'
- en: '| Requirement Engineering and Documentation | Requirement engineering, Software
    requirements analysis, Automated requirement documentation, Technical documentation
    generation, User manual generation, Documentation maintenance, Requirements modeling,
    Requirements elicitation |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 需求工程与文档 | 需求工程，软件需求分析，自动化需求文档，技术文档生成，用户手册生成，文档维护，需求建模，需求获取 |'
- en: '| Software Design and Evaluation | Software design automation, Architectural
    validation, Design optimization, Performance evaluation, Code quality assessment,
    Software metrics, Design pattern recognition, Architectural analysis, Code structure
    analysis |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 软件设计与评估 | 软件设计自动化、架构验证、设计优化、性能评估、代码质量评估、软件度量、设计模式识别、架构分析、代码结构分析 |'
- en: '| Software Test Generation | Test case generation, Automated testing, Unit
    test generation, Integration test generation, System test generation, Test suite
    optimization, Fault localization, Test maintenance, Regression testing, Adaptive
    testing |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 软件测试生成 | 测试用例生成、自动化测试、单元测试生成、集成测试生成、系统测试生成、测试套件优化、故障定位、测试维护、回归测试、适应性测试 |'
- en: '| Autonomous Learning and Decision Making | Autonomous learning systems, Decision
    making, Adaptive planning, Project management automation, Self-improving software,
    Autonomous software agents |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 自主学习与决策 | 自主学习系统、决策制定、适应性规划、项目管理自动化、自我改进的软件、自主软件代理 |'
- en: III Preliminaries
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 初步介绍
- en: In this section, we introduce the foundational concepts of large language models,
    including the evolution of their frameworks and an overview of their architectures.
    Subsequent to this, we will discuss LLM-based agents, exploring both single-agent
    and multi-agent systems. We will also covers the background of these systems and
    their applications and distinctions in the field of software engineering.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍大型语言模型的基础概念，包括其框架的演变以及架构概述。随后，我们将讨论基于LLM的代理，探讨单代理和多代理系统。我们还将涵盖这些系统的背景及其在软件工程领域的应用和区别。
- en: III-A Large Language Model
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 大型语言模型
- en: There is an inherent connection between large language models and natural language
    processing (NLP), with the historical development of natural language technologies
    tracing back to the 1950s. The earliest attempts to generate language dialogues
    through machines using specific rules can be traced to the period between 1950
    and 1970\. The advent of machine learning technologies in the 1980s and the groundbreaking
    introduction of neural networks in the 1990s indicated a new era for NLP  [[23](#bib.bib23)].
    These advancements facilitated significant progress in the NLP field, especially
    in the development of technologies for text translation and generation. The development
    of Long Short-Term Memory (LSTM) and Recurrent Neural Networks (RNN) during this
    period enabled more effective handling of the sequential nature of language data [[24](#bib.bib24)] [[25](#bib.bib25)].
    These models addressed challenges associated with the lack of dependency in context,
    thereby enhancing the application of NLP in various domains.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型与自然语言处理（NLP）之间存在内在的联系，自然语言技术的历史发展可以追溯到1950年代。最早通过机器使用特定规则生成语言对话的尝试可以追溯到1950年至1970年间。1980年代机器学习技术的出现以及1990年代神经网络的突破性引入标志着NLP的新纪元[[23](#bib.bib23)]。这些进展推动了NLP领域的显著发展，尤其是在文本翻译和生成技术的开发上。这一时期长短期记忆（LSTM）和递归神经网络（RNN）的发展使得对语言数据的顺序性质的处理更为有效[[24](#bib.bib24)]
    [[25](#bib.bib25)]。这些模型解决了上下文依赖性不足的问题，从而增强了NLP在各种领域的应用。
- en: In 2017 the new framework called ”Transformer” introduced by Google’s research
    team [[26](#bib.bib26)]. The transformer model based on the self-attention mechanism
    which significantly improved the effectiveness of language models. The inclusion
    of positional encoding not only solved the long-sequence dependency issue but
    also enabled parallel computation, which was a considerable improvement over previous
    models. In 2018, OpenAI developed the Generative Pre-trained Transformer (GPT) [[3](#bib.bib3)],
    a model based on the transformer architecture. The core idea behind GPT-1 was
    to utilize a large corpus of unlabelled text for pre-training to learn the patterns
    and structures of language, followed by fine-tuning for specific tasks. Over the
    next two years, OpenAI released GPT-2 and GPT-3 which increased the parameter
    count to 175 billions and also demonstrated strong capabilities in context understanding
    and text generation [[27](#bib.bib27)]. GPT-4 launched by OpenAI in 2023, represents
    a milestone following GPT-3.5\. Although GPT-4 maintains a similar parameter count
    of approximately 175 billion, its performance and diversity have seen considerable
    improvements. Through more refined training techniques and algorithm optimizations,
    GPT-4 enhanced the capability of language understanding and generation, particularly
    outperformed in handling complex texts and special contexts. Compared to other
    contemporary models like Google’s PaLM or Meta’s OPT, GPT-4 continues to stand
    out in multi-task learning and logical consistency in the text generation. While
    Google’s PaLM model boasts up to 54 billion parameters, GPT-4 shows superior generalization
    abilities across a broader range of NLP tasks [[28](#bib.bib28)]. On the open-source
    large models, Meta’s OPT model with a parameter size similar to the GPT-4 offers
    direct competition. Despite OPT’s advantages in openness and accessibility, GPT-4
    still maintains a lead in specific application areas such as creative writing
    and complex problem solving [[29](#bib.bib29)].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，谷歌研究团队引入了一个新的框架叫做“Transformer”[[26](#bib.bib26)]。基于自注意力机制的transformer模型显著提高了语言模型的效果。位置编码的引入不仅解决了长序列依赖问题，还实现了并行计算，这相比于之前的模型是一个重要的改进。在2018年，OpenAI开发了基于transformer架构的生成预训练变换器（GPT）[[3](#bib.bib3)]。GPT-1的核心思想是利用大量未标记的文本进行预训练，以学习语言的模式和结构，然后进行特定任务的微调。在接下来的两年里，OpenAI发布了GPT-2和GPT-3，这些模型将参数数量增加到1750亿，并且在上下文理解和文本生成方面表现出强大的能力[[27](#bib.bib27)]。OpenAI在2023年推出的GPT-4代表了继GPT-3.5之后的一个里程碑。虽然GPT-4保持了约1750亿的参数数量，但其性能和多样性有了显著改善。通过更精细的训练技术和算法优化，GPT-4提升了语言理解和生成的能力，特别是在处理复杂文本和特殊上下文方面表现出色。与谷歌的PaLM或Meta的OPT等当代模型相比，GPT-4在多任务学习和文本生成的逻辑一致性方面继续突出。尽管谷歌的PaLM模型拥有多达540亿个参数，GPT-4在更广泛的自然语言处理任务中展现了更强的泛化能力[[28](#bib.bib28)]。在开源大型模型中，Meta的OPT模型与GPT-4参数规模相似，直接竞争。尽管OPT在开放性和可访问性方面具有优势，但GPT-4在创意写作和复杂问题解决等特定应用领域仍保持领先[[29](#bib.bib29)]。
- en: III-B Model Architecture
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 模型架构
- en: There are three common LLM architectures, the Encoder-Decoder architecture,
    exemplified by the traditional transformer model. This architecture comprises
    six encoders and six decoders, data input into the system will first passes through
    the encoder, where it undergoes sequential feature extraction via the model’s
    self-attention mechanism. Subsequently, the decoders utilize the word vectors
    produced by the encoders to generate outputs, this technique is common to see
    in machine translation tasks, where the encoder processes word vectors from one
    language through several attention layers and feed-forward networks, thereby creating
    representations of the context. The decoder then uses this information to incrementally
    construct the correct translated text. A recent example of this architecture is
    the CodeT5+ model, launched by Salesforce AI Research in 2023 [[30](#bib.bib30)].
    This model is an enhancement of the original T5 architecture, which designed to
    improve performance in code understanding and generation tasks. It incorporates
    a flexible architecture and diversified pre-training objectives to optimize its
    effectiveness in these specialized areas. This development highlights the competency
    of Encoder-Decoder architectures in tackling increasingly complex NLP challenges.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 三种常见的LLM架构包括编码器-解码器架构，以传统的transformer模型为例。这种架构由六个编码器和六个解码器组成，数据输入系统后首先经过编码器，在这里通过模型的自注意力机制进行序列特征提取。随后，解码器利用编码器生成的词向量来生成输出，这种技术常见于机器翻译任务中，编码器通过多个注意力层和前馈网络处理来自一种语言的词向量，从而创建上下文表示。然后，解码器使用这些信息逐步构建正确的翻译文本。最近的一个例子是2023年由Salesforce
    AI Research推出的CodeT5+模型[[30](#bib.bib30)]。该模型是对原始T5架构的增强，旨在提高代码理解和生成任务的性能。它结合了灵活的架构和多样化的预训练目标，以优化在这些专业领域的效果。这一发展突显了编码器-解码器架构在应对日益复杂的NLP挑战中的能力。
- en: The Encoder-only architecture, as the name suggests it eliminates the decoder
    from the entire structure making the data more compact. Unlike RNNs, this architecture
    is stateless and uses a masking mechanism that allows input processing without
    relying on hidden states, and also accelerating parallel processing speeds and
    providing excellent contextual awareness. BERT (Bidirectional Encoder Representations
    from Transformers) is a representative model of this architecture, this model
    is a large language model built solely on the encoder architecture. BERT leverages
    the encoder’s powerful feature extraction capabilities and pre-training techniques
    to learn bidirectional representations of text, achieving outstanding results
    in sentiment analysis and contextual analysis  [[31](#bib.bib31)].
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-only架构，顾名思义，它从整个结构中去除了解码器，使数据更为紧凑。与RNN不同，这种架构是无状态的，使用掩码机制允许在不依赖于隐藏状态的情况下进行输入处理，同时加快了并行处理速度，并提供了出色的上下文感知。BERT（Bidirectional
    Encoder Representations from Transformers）是这种架构的代表模型，该模型完全基于编码器架构。BERT利用编码器强大的特征提取能力和预训练技术来学习文本的双向表示，在情感分析和上下文分析中取得了出色的结果[[31](#bib.bib31)]。
- en: The Decoder-only archiecture, in the transformer framework primarily involves
    the decoder receiving processed word vectors and generating output. Utilizing
    the decoder to directly generate text accelerates tasks such as text generation
    and sequence prediction. This characteristic with high scalability is known as
    auto-regressiveness, which is why popular models like GPT use this architecture.
    In 2020, the exceptional performance of GPT-3 and its remarkable few-shot learning
    capabilities demonstrated the vast potential of the decoder-only architecture [[32](#bib.bib32)].
    Given the enormous computational cost and time required to train a model from
    scratch, and the exponential increase in the number of parameters, many researchers
    now prefer to leverage pre-trained models for further research. The most popular
    open-source pre-trained language model LLaMA, developed by Meta AI also employs
    the decoder-only architecture [[33](#bib.bib33)], as mentioned earlier, the autoregressiveness
    and simplicity of this structure make the model easier to train and fine-tune.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器框架中，Decoder-only 架构主要涉及解码器接收处理过的词向量并生成输出。利用解码器直接生成文本加速了文本生成和序列预测等任务。具有高可扩展性的这一特征被称为自回归性，这就是为什么像
    GPT 这样的流行模型使用这种架构。2020 年，GPT-3 的卓越性能及其显著的少量学习能力展示了 Decoder-only 架构的巨大潜力[[32](#bib.bib32)]。考虑到从头训练模型所需的巨大的计算成本和时间，以及参数数量的指数级增加，许多研究人员现在更愿意利用预训练模型进行进一步研究。最受欢迎的开源预训练语言模型
    LLaMA，由 Meta AI 开发，也采用了 Decoder-only 架构[[33](#bib.bib33)]，如前所述，这种结构的自回归性和简洁性使得模型更易于训练和微调。
- en: III-C Large Language Model Based Agent
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 大语言模型基于的代理
- en: The concept of agents even trace back to the 19th century and is often referred
    to as intelligent agents, envisioned to possess intelligence comparable to humans.
    Over the past few decades, as AI technology has evolved, the capabilities of AI
    agents have significantly advanced, particularly with the reinforcement learning.
    This development has enabled AI agents to autonomously handle tasks and learn
    and improve based on specified reward/punishment rules. Notable milestones include
    AlphaGo [[34](#bib.bib34)], which leveraged reinforcement learning to defeat the
    world champion in Go competition.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的概念甚至可以追溯到 19 世纪，通常被称为智能代理，设想具有与人类相当的智能。在过去几十年中，随着 AI 技术的发展，AI 代理的能力有了显著的提升，特别是在强化学习方面。这一发展使得
    AI 代理能够自主处理任务，并根据指定的奖励/惩罚规则进行学习和改进。值得注意的里程碑包括 AlphaGo[[34](#bib.bib34)]，它利用强化学习击败了围棋世界冠军。
- en: The success of GPT has further propelled the field, with researchers exploring
    the use of large language models as the ”brain” of AI agents, thanks to GPT’s
    powerful text understanding and reasoning capabilities. In 2023, a research team
    from Fudan University [[10](#bib.bib10)] conducted a comprehensive survey on LLM-based
    agents, examining their perception, behavior, and cognition. Traditional LLMs
    typically generate responses based solely on given natural language descriptions,
    lacking the ability for independent thinking and judgment. LLM-based agents able
    to employ multiple rounds of interaction and customized prompts to gather more
    information, which enable the model to think and make decisions autonomously.
    In 2023, Andrew Zhao proposed the ExpeL framework [[35](#bib.bib35)], which utilizes
    ReAct as the planning framework combined with an experience pool [[36](#bib.bib36)].
    This allows the LLM to extract insights from past records to aid in subsequent
    related queries, by letting the LLM analyze why previous answers were incorrect,
    it learns from experience to identify the problems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 的成功进一步推动了这一领域的发展，研究人员探索将大语言模型作为 AI 代理的“脑”，这得益于 GPT 强大的文本理解和推理能力。2023 年，复旦大学的一个研究团队[[10](#bib.bib10)]对基于
    LLM 的代理进行了全面调查，研究了它们的感知、行为和认知。传统的 LLM 通常仅根据给定的自然语言描述生成响应，缺乏独立思考和判断的能力。基于 LLM 的代理能够通过多轮互动和定制提示来收集更多信息，从而使模型能够自主思考和决策。2023
    年，Andrew Zhao 提出了 ExpeL 框架[[35](#bib.bib35)]，该框架利用 ReAct 作为规划框架，并结合经验池[[36](#bib.bib36]]。这使得
    LLM 可以从过去的记录中提取见解，以帮助处理后续相关查询，通过让 LLM 分析之前答案错误的原因，从经验中学习以识别问题。
- en: At the same time, the application of LLM-based embodied agents has also become
    a hot research area in recent years. LLM-based Embodied Agents are intelligent
    systems that integrate LLMs with embodied agents [[37](#bib.bib37)]. These systems
    can not only process natural language but also complete tasks through perception
    and actions in physical or virtual environments. By combining language understanding
    with actual actions, these agents can perform tasks in more complex environments.
    This integration often involves using visual domain technologies to process and
    understand visual data and reinforcement learning algorithms to train agents to
    take optimal actions in the environment. These algorithms guide the agent through
    reward mechanisms to learn how to make optimal decisions in different situations,
    while the LLM acts as the brain to understand user instructions and generate appropriate
    feedback. In 2023, Guanzhi Wang introduced VOYAGER, an open-ended embodied agent
    with large language models [[38](#bib.bib38)]. It uses GPT-4 combined with input
    prompts, an iterative prompting mechanism, and a skill library enabling the LLM-based
    agents to autonomously learn and play the game Minecraft, becoming the first lifelong
    learning agent in the game.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，基于LLM的具身体代理的应用近年来也成为了热门研究领域。基于LLM的具身体代理是将LLM与具身体代理结合的智能系统[[37](#bib.bib37)]。这些系统不仅能够处理自然语言，还可以通过感知和在物理或虚拟环境中的行动完成任务。通过将语言理解与实际行动相结合，这些代理可以在更复杂的环境中执行任务。这种集成通常涉及使用视觉领域技术来处理和理解视觉数据，并使用强化学习算法来训练代理在环境中采取最佳行动。这些算法通过奖励机制引导代理学习如何在不同情况下做出最佳决策，而LLM则充当大脑，理解用户指令并生成适当的反馈。2023年，Guanzhi
    Wang 引入了VOYAGER，一个具有大型语言模型的开放式具身体代理[[38](#bib.bib38)]。它使用GPT-4结合输入提示、迭代提示机制和技能库，使得基于LLM的代理能够自主学习和玩Minecraft，成为游戏中第一个终身学习的代理。
- en: '![Refer to caption](img/5b8382c30b9e5f08a03b5fcc3a0373cb.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5b8382c30b9e5f08a03b5fcc3a0373cb.png)'
- en: 'Figure 3: Illustration of Common Data Augmentation Methods'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：常见数据增强方法的示意图
- en: 'Nowadays, various agent systems are emerging and they relies on large language
    models to make judgments, combined with techniques such as few-shot learning and
    multi-turn dialogue for model fine-tuning. However, due to the lack of datasets
    and the novelty of LLM-based agents, many researchers employ different methods
    for data augmentation. Common approaches include synonym replacement, where words
    in the text are replaced with synonyms from the same domain to increase textual
    diversity; back-translation, where the text is translated into another language
    and then back to the original language to generate new texts with slightly different
    grammatical structures and word choices; Paraphrasing refers to the new dialogue
    which similar in context but slightly different in expression created through
    manual or automated means; Synthetic Data Generation refers to use pretrained
    model to make a synthetic data generation as shown in Figure.[3](#S3.F3 "Figure
    3 ‣ III-C Large Language Model Based Agent ‣ III Preliminaries ‣ From LLMs to
    LLM-based Agents for Software Engineering: A Survey of Current, Challenges and
    Future"). In 2023, Chenxi Whitehouse explored using LLMs for data augmentation
    to enhance the performance of multilingual commonsense reasoning datasets, especially
    under conditions of extremely limited training data [[39](#bib.bib39)]. The study
    employed various LLMs (such as Dolly-v2, StableVicuna, ChatGPT, and GPT-4) to
    generate new data. These models were prompted to create new examples similar to
    the original data, thereby increasing the diversity and quantity of training data.
    Prompt engineering was mentioned as an essential skill for effectively interacting
    with LLMs. By applying these prompt patterns, users can efficiently customize
    their dialogues with LLMs ensuring the generation of high-quality outputs and
    achieving complex automated tasks. In 2023, Jules White introduced a set of methods
    and patterns to enhance prompt engineering, optimizing interactions with LLMs
    such as ChatGPT [[40](#bib.bib40)]. This study categorizes prompt engineering
    into five main areas: Input Semantics, Output Customization, Error Identification,
    Prompt Improvement, and Interaction, to address a wide range of problems and adapt
    to different fields.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，各种代理系统正在涌现，它们依赖于大型语言模型来进行判断，结合了少样本学习和多轮对话等技术以对模型进行微调。然而，由于数据集的缺乏以及基于LLM的代理系统的创新性，许多研究人员采用不同的方法进行数据增强。常见的方法包括同义词替换，即用同一领域的同义词替换文本中的单词以增加文本的多样性；反向翻译，即将文本翻译成另一种语言，然后再翻译回原语言，以生成具有稍微不同语法结构和词汇选择的新文本；**同义改写**是指通过手动或自动化手段创建与上下文相似但表达略有不同的新对话；**合成数据生成**指的是使用预训练模型生成合成数据，如图所示[3](#S3.F3
    "图 3 ‣ III-C 基于大型语言模型的代理 ‣ III 基础知识 ‣ 从LLMs到基于LLM的代理的现状、挑战与未来")。在2023年，**Chenxi
    Whitehouse** 探索了使用LLMs进行数据增强，以提升多语言常识推理数据集的性能，特别是在极度有限的训练数据条件下[[39](#bib.bib39)]。该研究使用了多种LLMs（如Dolly-v2、StableVicuna、ChatGPT和GPT-4）生成新数据。这些模型被提示创建与原始数据相似的新示例，从而增加了训练数据的多样性和数量。**提示工程**被提及为与LLMs有效互动的关键技能。通过应用这些提示模式，用户可以高效地定制与LLMs的对话，确保生成高质量的输出并实现复杂的自动化任务。在2023年，**Jules
    White** 介绍了一系列方法和模式，以增强提示工程，优化与LLMs如ChatGPT的互动[[40](#bib.bib40)]。该研究将提示工程分为五个主要领域：输入语义、输出定制、错误识别、提示改进和互动，以解决广泛的问题并适应不同的领域。
- en: One notable technique is Retrieval-Augmented Generation (RAG), the input question
    undergoes similarity matching with documents in an index library, attempting to
    find relevant results. If similar documents are retrieved, they are organized
    in conjunction with the input question to generate a new prompt, which is then
    fed into the large language model. Currently, large language models possess long-text
    memory capabilities, numerous studies have tested Gemini v1.5 against the Needle
    In A Haystack (NIAH) evaluation to explore whether RAG has become obsolete [[41](#bib.bib41)].
    However, from various perspectives such as cost and expense, RAG still holds significant
    advantages (RAG could be 99 percent cheaper than utilizing all tokens). Additionally,
    long texts can negatively impact response performance, causing LLM to respond
    more slowly when the input text is too long. Therefore, the advancements in context
    length for LLM will not fully replace the role of RAG but treated as complements
    between each other.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 一种显著的技术是检索增强生成（RAG），输入问题通过与索引库中的文档进行相似性匹配，尝试找到相关结果。如果检索到类似文档，它们将与输入问题一起组织生成新的提示，然后输入到大型语言模型中。目前，大型语言模型具有长文本记忆能力，许多研究已经测试了Gemini
    v1.5在针尖中的针（NIAH）评估中的表现，以探索RAG是否已经过时[[41](#bib.bib41)]。然而，从成本和费用等不同角度来看，RAG仍然具有显著优势（RAG可能比使用所有令牌便宜99%）。此外，长文本会对响应性能产生负面影响，当输入文本过长时，LLM的响应速度会变慢。因此，LLM在上下文长度上的进展不会完全取代RAG的作用，而是作为彼此的补充。
- en: III-D Single and Multi-Agents
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 单代理和多代理
- en: Single agent systems leverage the capabilities of a LLM to perform various tasks,
    these agents typically use a single LLM to understand and respond to user queries,
    generate content, or execute automated tasks based on predefined instructions.
    Single agents are commonly used in scenarios where tasks accept a general answer
    and do not require complex decision-making. Examples include customer service
    chatbots, virtual assistants for scheduling, and automated content generation
    tools. However, single agents may struggle with dealing long context inputs, leading
    to inconsistent or irrelevant responses. The scalability of these systems is also
    limited when dealing with tasks that require extensive knowledge or context, this
    issue is often exacerbated by long texts as large language models cannot fully
    comprehend and analyze overly lengthy information in one turn. One of the primary
    issues with large language models is hallucination [[7](#bib.bib7)]. Hallucination
    refers to the generation of fabricated information or definitions by LLMs, presented
    in seemingly logical and reasonable language to the user. Most research papers
    on LLMs have stated this problem, while prompt engineering or tool interventions
    can mitigate the affect caused by hallucination, but it cannot be entirely eliminated.
    In 2023, Ziwei Ji conducted an in-depth study on hallucination in natural language
    generation [[42](#bib.bib42)]. This survey reviewed the progress and challenges
    in addressing hallucinations in NLG, providing a comprehensive analysis of hallucination
    phenomena across different tasks, including their definitions and classifications,
    causes, evaluation metrics, and mitigation methods.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 单代理系统利用LLM的能力执行各种任务，这些代理通常使用单个LLM来理解和响应用户查询、生成内容或根据预定义指令执行自动化任务。单代理通常用于接受一般答案且不需要复杂决策的场景。例如，客户服务聊天机器人、日程安排的虚拟助手以及自动内容生成工具。然而，单代理在处理长上下文输入时可能会遇到困难，导致响应不一致或不相关。这些系统在处理需要广泛知识或上下文的任务时，扩展性也有限，这一问题通常由于长文本而加剧，因为大型语言模型无法在一次处理中过度理解和分析过于冗长的信息。大型语言模型的主要问题之一是幻觉[[7](#bib.bib7)]。幻觉指的是LLM生成虚构的信息或定义，并以看似合逻辑和合理的语言呈现给用户。大多数关于LLM的研究论文都指出了这个问题，虽然提示工程或工具干预可以缓解幻觉造成的影响，但无法完全消除。2023年，Ziwei
    Ji对自然语言生成中的幻觉进行了深入研究[[42](#bib.bib42)]。该调查回顾了处理NLG中的幻觉的进展和挑战，提供了对不同任务中幻觉现象的全面分析，包括其定义和分类、原因、评估指标和缓解方法。
- en: Multi-agent systems involve the collaboration of multiple LLMs or agents to
    tackle complex tasks effectively. These systems fully utilize the advantages of
    multiple models, with each model specializing in specific aspects of the task
    to reduce overhead caused by multi-processes in single agents, the collaboration
    among agents allows for more sophisticated and robust problem-solving capabilities.
    Due to their exceptional capabilities, more researchers are beginning to explore
    the field of Multi-LLM based agents and start applying into software engineering
    domains. In 2024, a lot of researchers adopt the multi-agent system into the practical
    experiments [[43](#bib.bib43)] [[44](#bib.bib44)].
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理系统涉及多个LLM或代理的合作，以有效地解决复杂任务。这些系统充分利用了多个模型的优势，每个模型专注于任务的特定方面，从而减少了单个代理中多进程造成的开销，代理之间的协作使得问题解决能力更加复杂和可靠。由于其卓越的能力，越来越多的研究人员开始探索基于Multi-LLM的代理领域，并开始将其应用于软件工程领域。在2024年，许多研究人员将多代理系统应用于实际实验[[43](#bib.bib43)]
    [[44](#bib.bib44)]。
- en: 'Multi-agent systems address the limitations of single-agent systems in the
    following ways:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理系统以以下方式解决了单代理系统的局限性：
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhanced Context Management: Multiple agents can maintain and share context,
    generating more coherent and relevant responses over long interactions.'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 增强的上下文管理：多个代理可以保持和共享上下文，在长时间交互中生成更连贯和相关的回应。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Specialization and Division of Labor: Different agents can focus on specific
    tasks or aspects of a problem, improving efficiency and effectiveness.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专业化和劳动分工：不同的代理可以专注于特定任务或问题的方面，提高效率和效果。
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robustness and Error Correction: Collaborative agents can cross-check and validate
    each other’s outputs, reducing the likelihood of errors and improving overall
    reliability.'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可靠性和错误修正：协作代理可以互相检查和验证对方的输出，减少错误的可能性，提高整体可靠性。
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Contextual Consistency: Multi-agent systems can better manage context over
    long dialogues. The collaboration of multiple agents improves the efficiency of
    incident mitigation.'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上下文一致性：多代理系统能够更好地管理长对话中的上下文。多个代理的协作提高了事件缓解的效率。
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scalability and Flexibility: These systems can integrate specialized agents
    to scale and handle more complex tasks. Through the division of labor among multiple
    agents, the quality of code generation is improved.'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可扩展性和灵活性：这些系统可以整合专业化的代理来扩展和处理更复杂的任务。通过多个代理的劳动分工，代码生成的质量得以提高。
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dynamic Problem Solving: By integrating agents with different expertise, multi-agent
    systems can adapt to a wider range of problems and provide more accurate solutions.'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动态问题解决：通过整合不同专长的代理，多代理系统能够适应更广泛的问题范围，并提供更准确的解决方案。
- en: III-E LLM in Software Engineering
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E LLM在软件工程中的应用
- en: Recently, there has been a shift towards applying general AI models to specific
    vertical domains such as medical and finance. In software engineering, new AI
    agents are emerging that are more flexible and intelligent compared to previous
    applications of LLMs, although they utilize different data and experiments. This
    continuous innovation underscores the transformative potential of AI agents across
    various fields, these models excel in text understanding and generation, promoting
    innovative applications in software development and maintenance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的应用将通用AI模型转向特定的垂直领域，如医疗和金融。在软件工程中，新兴的AI代理相比于之前的LLM应用具有更高的灵活性和智能，尽管它们使用不同的数据和实验。这种持续的创新强调了AI代理在各个领域的变革潜力，这些模型在文本理解和生成方面表现优异，推动了软件开发和维护中的创新应用。
- en: LLMs profoundly impact software engineering by facilitating tasks such as code
    generation, defect prediction, and automated documentation. Integrating these
    models into development workflows not only simplifies the coding process but also
    reduces human errors. LLM-based agents enhance basic LLM capabilities by integrating
    decision-making and interactive problem-solving functions. These agents can understand
    and generate result by interacting with other software tools which optimize workflows,
    and make autonomous decisions to improve software development practices. In 2023,
    Yann Dubois introduced the AlpacaFarm framework [[45](#bib.bib45)], where LLMs
    are used to simulate the behavior of software agents in complex environments.
    Moreover, significant research has been conducted in the field of automated program
    repair (APR). In 2024, Islem Bouzenia introduced RepairAgent [[46](#bib.bib46)],
    another LLM-based tool designed for automatic software repair, this tool reduced
    the time developers spent on fixing issues. Additionally, in 2023, Emanuele Musumeci
    demonstrated a multi-agent LLM system [[47](#bib.bib47)], which involved a multi-agent
    architecture where each agent had a specific role in the generation of documents.
    This system significantly improved handling complex document structures without
    extensive human supervision. Besides these, LLMs have made outstanding contributions
    in software testing, software design, and emerging fields such as software security
    and maintenance.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 对软件工程产生了深远的影响，通过促进代码生成、缺陷预测和自动化文档编制等任务。将这些模型集成到开发工作流程中不仅简化了编码过程，还减少了人为错误。基于
    LLM 的代理通过整合决策制定和互动问题解决功能来增强基本的 LLM 能力。这些代理可以通过与其他软件工具互动来理解和生成结果，从而优化工作流程，并自主做出决策以改善软件开发实践。2023
    年，Yann Dubois 介绍了 AlpacaFarm 框架 [[45](#bib.bib45)]，其中 LLM 被用于模拟复杂环境中软件代理的行为。此外，在自动程序修复
    (APR) 领域已进行了重要的研究。2024 年，Islem Bouzenia 介绍了 RepairAgent [[46](#bib.bib46)]，这是一种用于自动软件修复的
    LLM 基础工具，该工具减少了开发人员修复问题所花费的时间。此外，在 2023 年，Emanuele Musumeci 展示了一个多代理 LLM 系统 [[47](#bib.bib47)]，该系统涉及一个多代理架构，每个代理在文档生成中都有特定的角色。这个系统在处理复杂文档结构时显著提高了效率，而无需大量的人力监督。除此之外，LLMs
    在软件测试、软件设计以及软件安全和维护等新兴领域也做出了杰出贡献。
- en: 'Currently, there is no comprehensive and accurate definition of the capabilities
    an LLM must exhibit to be considered an llm-based agent. Since the application
    of LLMs in software engineering is relatively broad and some frameworks already
    behave certain levels of autonomy and intelligent, this study defines the distinction
    between LLM and LLM-based agents based on mainstream definitions and literature
    from the first half of 2024\. In this survey, an LLM architecture can be called
    an agent when it satisfy the Table [IV](#S3.T4 "TABLE IV ‣ III-E LLM in Software
    Engineering ‣ III Preliminaries ‣ From LLMs to LLM-based Agents for Software Engineering:
    A Survey of Current, Challenges and Future").'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '目前，还没有全面而准确的定义来描述 LLM 必须展示的能力，以便被视为 LLM 基础的代理。由于 LLM 在软件工程中的应用相对广泛，并且一些框架已经表现出一定程度的自主性和智能，本研究基于主流定义和
    2024 年上半年的文献来定义 LLM 和基于 LLM 的代理之间的区别。在这项调查中，当 LLM 架构满足表格 [IV](#S3.T4 "TABLE IV
    ‣ III-E LLM in Software Engineering ‣ III Preliminaries ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future")
    的条件时，可以称其为代理。'
- en: 'TABLE IV: Criteria for LLM-based agent'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE IV: 基于 LLM 的代理标准'
- en: '| Criteria |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Criteria |'
- en: '| 1)  The LLM serves as the brain (the center of information processing and
    generation of thought). 2)  The framework not only relies on the language understanding
    and generation capabilities of LLMs but also possesses decision-making and planning
    abilities. 3)  If tools are available, the model can autonomously decide when
    and which tools to use and integrate the results into its predictions to enhance
    task completion efficiency and accuracy. 4)  The model can select the optimal
    solution from multiple homogeneous results (the ability to evaluate and choose
    among various possible solutions). 5)  The model can handle multiple interactions
    and maintain contextual understanding. 6)  The model has autonomous learning capabilities
    and adaptability. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 1) LLM作为大脑（信息处理和思维生成的中心）。2) 该框架不仅依赖于LLM的语言理解和生成能力，还具备决策和规划能力。3) 如果有工具可用，模型可以自主决定何时使用哪些工具，并将结果整合到预测中，以提高任务完成的效率和准确性。4)
    模型可以从多个同质结果中选择最佳解决方案（评估和选择各种可能解决方案的能力）。5) 模型可以处理多个交互并保持上下文理解。6) 模型具有自主学习能力和适应能力。|'
- en: IV Requirement Engineering and and Documentation
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 需求工程与文档编制
- en: Requirement Engineering is a critical field within software engineering and
    plays an essential role in the software development process, its primary task
    is to ensure that the software system meets the needs of all relevant stakeholders.
    Typically, requirement engineering in project development involves many steps,
    where developers need to fully understand the users’ needs and expectations to
    ensure that the development direction of the software system aligns with actual
    requirements. The collected requirements are then organized and evaluated by the
    development group. Requirements Specification is the process of formally documenting
    the analyzed requirements, the specification must be accurate and concise, and
    the requirement verification must be conducted to ensure that developers are building
    what users need and that it aligns with the specifications. Requirement engineering
    also includes requirement management, a task that spans the entire software development
    life-cycle, developers need to continuously track, control, and respond to any
    changes occurring during development, ensuring that these changes do not negatively
    impact the project’s progress and overall quality.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 需求工程是软件工程中的一个关键领域，在软件开发过程中起着至关重要的作用，其主要任务是确保软件系统满足所有相关利益相关者的需求。通常，项目开发中的需求工程涉及多个步骤，开发人员需要充分理解用户的需求和期望，以确保软件系统的开发方向符合实际需求。收集到的需求随后由开发团队进行整理和评估。需求规格说明是正式记录分析过的需求的过程，规格说明必须准确且简洁，并且需要进行需求验证，以确保开发人员正在构建用户所需的内容，并且符合规格说明。需求工程还包括需求管理，这是一个贯穿整个软件开发生命周期的任务，开发人员需要持续跟踪、控制并响应开发过程中发生的任何变化，确保这些变化不会对项目进展和整体质量产生负面影响。
- en: IV-A LLMs Tasks
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A LLM任务
- en: 'In the field of requirement engineering, LLMs have demonstrated significant
    potential in automating and enhancing tasks such as requirement elicitation, classification,
    generation, specification generation, and quality assessment. Requirement classification
    and extraction is a crucial task in requirement engineering during the development
    process. It is common to encounter situations where clients present multiple requirements
    at once, necessitating manual classification by developers. By categorizing requirements
    into functional and non-functional requirements, developer can better understand
    and manage them, thanks to the strong performance of LLMs in classification tasks,
    many relevant frameworks have been developed. The PRCBERT framework, utilizing
    the BERT pre-trained language model, transforms classification problems into a
    series of binary classification tasks through flexible prompt templates, significantly
    improving classification performance [[48](#bib.bib48)]. Studies have shown that
    the PRCBERT achieved an F1 score of 96.13% on the PROMISE dataset which outperform
    the previous state-of-arts NoRBERT [[49](#bib.bib49)] and BERT-MLM models [[31](#bib.bib31)].
    Additionally, the application of ChatGPT in requirement information retrieval
    has shown promising results, by classifying and extracting information from requirement
    documents, ChatGPT achieved comparable or even better $F\beta$ scores under zero-shot
    settings, particularly in feature extraction tasks, where its performance surpassed
    baseline models [[50](#bib.bib50)]. As seen in Table [I](#S2.T1 "TABLE I ‣ II-A
    Existing works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future"),
    there is also substantial literature and research on using LLMs to automatically
    generate requirements and descriptions in requirement engineering.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '在需求工程领域，LLMs（大语言模型）展示了在自动化和增强任务方面的显著潜力，例如需求引导、分类、生成、规格生成和质量评估。需求分类和提取是在开发过程中需求工程中的关键任务。在实际中，常常会遇到客户一次性提出多个需求的情况，这需要开发者进行手动分类。通过将需求分类为功能需求和非功能需求，开发者可以更好地理解和管理这些需求。得益于LLMs在分类任务中的强大性能，许多相关框架已经被开发出来。PRCBERT框架利用BERT预训练语言模型，通过灵活的提示模板将分类问题转化为一系列二分类任务，从而显著提高了分类性能[[48](#bib.bib48)]。研究表明，PRCBERT在PROMISE数据集上的F1得分达到了96.13%，超越了之前的NoRBERT[[49](#bib.bib49)]和BERT-MLM模型[[31](#bib.bib31)]。此外，ChatGPT在需求信息检索中的应用也显示出了有希望的结果，通过对需求文档进行分类和信息提取，ChatGPT在零样本设置下达到了可比甚至更高的$F\beta$得分，特别是在特征提取任务中，其表现超越了基线模型[[50](#bib.bib50)]。如表[I](#S2.T1
    "TABLE I ‣ II-A Existing works ‣ II EXISTING WORKS AND THE SURVEY STRUCTURE ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")所示，关于使用LLMs自动生成需求和描述的文献和研究也非常丰富。'
- en: By automating the generation and description of requirements, the efficiency
    and accuracy of requirement elicitation can be improved. Research indicates that
    LLMs hold significant potential in requirements generation task. For example,
    using ChatGPT to generate and gather user requirements, studies found that participants
    with professional knowledge could use ChatGPT more effectively, indicating the
    influence of domain expertise on the effectiveness of LLM-assisted requirement
    elicitation [[51](#bib.bib51)]. The study employed qualitative assessments of
    the LLMs’ output against predefined criteria for requirements matches, including
    full matches, partial matches, and the relevancy of the elicited requirements,
    although their success varied depending on the complexity of the task and the
    experience of the users, the result showing that LLMs could effectively assist
    in eliciting requirements, and its particularly useful in identifying, and suggesting
    requirements based on the large corpus of training data they provided. The SRS
    (Software Requirement Specification) generation is an important task which the
    developer normally spent a lot of time to refine and verified. In [[52](#bib.bib52)],
    researchers use both iterative prompting and a single comprehensive prompt to
    assess the performance of LLMs to generate SRS. The experiment conducted on GPT-4
    and CodeLlama-34b one close-source LLM and one open-source LLM for comprehensive
    evaluation, the generated SRS will compare with human-crafted SRS and finally
    scored by the likert scale. The result indicate that, the human-generated SRS
    was overall superior, but CodeLlama often came close, sometimes outperforming
    in specific categories. The CodeLlama scored higher in completeness and internal
    consistency than GPT-4 but less concise, so this stuy demonstrated the potential
    of using fine-tuned LLMs to generate SRS and increase the overall project productivity.
    Another paper also explores using LLMs for generating specifications. In [[53](#bib.bib53)],
    the authors introduce a framework called SpecGen for generating program specifications.
    The framework primarily uses GPT-3.5-turbo as the base model and employs prompt
    engineering combined with multi-turn dialogues to generate the specifications.
    SpecGen applies four mutation operators to modify these specifications and finally
    uses a heuristic selection strategy to choose the optimal variant. The results
    show that SpecGen can generate 70% of the program specifications, outperforming
    traditional tools like Houdini [[54](#bib.bib54)] and Daikon¹¹1[https://github.com/codespecs/daikon](https://github.com/codespecs/daikon).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自动化需求的生成和描述，可以提高需求引导的效率和准确性。研究表明，LLM在需求生成任务中具有显著的潜力。例如，使用ChatGPT生成和收集用户需求的研究发现，具有专业知识的参与者能够更有效地使用ChatGPT，这表明领域专业知识对LLM辅助需求引导的有效性有影响[[51](#bib.bib51)]。该研究对LLM的输出进行了定性评估，依据预定义的需求匹配标准，包括完全匹配、部分匹配和引导需求的相关性，尽管成功率因任务复杂性和用户经验而异，但结果表明LLM可以有效辅助需求引导，特别是在识别和建议需求方面，这些LLM基于其提供的大量训练数据。SRS（软件需求规格说明）的生成是一个重要任务，开发人员通常花费大量时间进行完善和验证。在[[52](#bib.bib52)]中，研究人员使用了迭代提示和单一综合提示来评估LLM生成SRS的性能。实验在GPT-4和CodeLlama-34b这两个LLM上进行，一个是闭源LLM，一个是开源LLM，生成的SRS与人工制作的SRS进行比较，并最终通过李克特量表评分。结果表明，人工生成的SRS总体上更优，但CodeLlama经常接近，有时在特定类别中表现更好。CodeLlama在完整性和内部一致性方面评分高于GPT-4，但简洁性较差，因此这项研究展示了使用微调的LLM生成SRS并提高整体项目生产力的潜力。另一篇论文还探讨了使用LLM生成规格的方式。在[[53](#bib.bib53)]中，作者介绍了一种名为SpecGen的框架，用于生成程序规格。该框架主要使用GPT-3.5-turbo作为基础模型，并结合提示工程和多轮对话来生成规格。SpecGen应用了四种变异操作符来修改这些规格，最终使用启发式选择策略来选择最佳变体。结果显示，SpecGen能够生成70%的程序规格，表现优于传统工具如Houdini[[54](#bib.bib54)]和Daikon¹¹1[https://github.com/codespecs/daikon](https://github.com/codespecs/daikon)。
- en: Furthermore, designing prompt patterns can significantly enhance LLMs’ capabilities
    in tasks such as requirement elicitation and system design. The paper provides
    a catalog of 13 prompt patterns, each aimed at addressing specific challenges
    in software development [[55](#bib.bib55)]. The experiments test the efficacy
    of these patterns in real world scenarios to validate their usefulness. By applying
    different prompt patterns, the study found that these patterns could help generate
    more structured and modular results and reduce common errors. Automated requirement
    completeness enhancement is another important benefit brought by the LLMs in requirement
    generation. The study [[56](#bib.bib56)] use BERT’s Masked Language Model (MLM)
    can detect and fill in missing parts in natural language requirements, significantly
    improving the completeness of requirements. BERT’s MLM achieved a precision of
    82%, indicating that 82% of the predicted missing terms were correct.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，设计提示模式可以显著提升大语言模型（LLMs）在需求获取和系统设计等任务中的能力。论文提供了13种提示模式的目录，每种模式旨在解决软件开发中的特定挑战[[55](#bib.bib55)]。实验测试了这些模式在实际场景中的有效性，以验证其有用性。通过应用不同的提示模式，研究发现这些模式能够帮助生成更结构化和模块化的结果，并减少常见错误。自动化需求完整性增强是LLMs在需求生成中带来的另一个重要好处。研究[[56](#bib.bib56)]使用BERT的掩码语言模型（MLM）可以检测并填补自然语言需求中的缺失部分，显著提高了需求的完整性。BERT的MLM达到了82%的准确率，表明82%的预测缺失项是正确的。
- en: There is also the application of LLMs in ambiguity detection tasks, aimed at
    detecting ambiguities in natural language requirement documents to improve clarity
    and reduce misunderstandings. This study primarily aims to address the issue of
    detecting term ambiguities within the same application domain (where the same
    term has different meanings in different domains). Although current models generally
    possess excellent contextual understanding capabilities, this was a common problem
    in machine learning at that time. This study provides an excellent paradigm for
    the subsequent application of LLMs in requirements engineering, study demonstrated
    the transformer-based machine learning models can effectively detect and identify
    ambiguities in requirement documents, thereby enhancing document clarity and consistency.
    The framework utilizes BERT and K-means clustering to identify terms used in different
    contexts within the same application domain or interdisciplinary project requirements
    documents [[57](#bib.bib57)]. In recent two years, more and more researchers use
    LLMs to help them to evaluate the requirement documentations, quality assessment
    tasks ensure that the generated requirements and code meet expected quality standards.
    The application of ChatGPT in user story quality evaluation has shown potential
    in identifying quality issues, but it requires further optimization and improvement [[58](#bib.bib58)].
    A similar study use LLM to automatically process the requirement satisfaction
    assessment, and evaluate whether design elements fully covered by the given requirements,
    but the the researcher indicated the necessity of further verification and optimization
    in practical applications [[59](#bib.bib59)].
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在模糊性检测任务中的应用也是一个重要领域，旨在检测自然语言需求文档中的模糊性，以提高清晰度并减少误解。本研究主要旨在解决在相同应用领域内检测术语模糊性的问题（即同一术语在不同领域具有不同含义）。尽管当前模型通常具有出色的上下文理解能力，但这是当时机器学习中的一个常见问题。本研究为LLMs在需求工程中的后续应用提供了一个优秀的范例，研究表明基于变压器的机器学习模型能够有效地检测和识别需求文档中的模糊性，从而提高文档的清晰度和一致性。该框架利用BERT和K-means聚类来识别在相同应用领域或跨学科项目需求文档中使用的不同上下文中的术语[[57](#bib.bib57)]。近年来，越来越多的研究人员使用LLMs来帮助评估需求文档，质量评估任务确保生成的需求和代码符合预期的质量标准。ChatGPT在用户故事质量评估中的应用已显示出识别质量问题的潜力，但仍需要进一步优化和改进[[58](#bib.bib58)]。类似的研究使用LLM自动处理需求满足评估，并评估设计元素是否完全覆盖了给定需求，但研究者指出了在实际应用中进一步验证和优化的必要性[[59](#bib.bib59)]。
- en: IV-B LLM-based Agents Tasks
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 基于LLM的代理任务
- en: 'Currently the application of LLM-based agents in the requirement engineering
    is till quite nascent, but there are some useful researches to help us to see
    the potential possibility. LLM-based agents bring both efficiency and accuracy
    for tasks like requirement elicitation, classification, generation, and verification.
    Compared to traditional LLMs, these systems exhibit higher levels of automation
    and precision through task division and collaboration. The application of multi-agent
    systems in semi-structured document generation has shown significant effectiveness.
    In [[60](#bib.bib60)], a multi-agent framework is introduced that combines semantic
    recognition, information retrieval, and content generation tasks to streamline
    the creation and management of semi-structured documents in the public administration
    domain. The proposed framework involves three main types of agents: Semantics
    Identification Agent, Information Retrieval Agent, and Content Generation Agent.
    By avoiding the overhead of a single model, each agent is assigned a specific
    task with minimal user intervention, following the designed framework and workflow.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基于LLM的代理在需求工程中的应用仍然相当初期，但已有一些有用的研究帮助我们看到潜在的可能性。基于LLM的代理在需求引导、分类、生成和验证等任务中带来了效率和准确性。与传统的LLM相比，这些系统通过任务分配和协作展现了更高的自动化和精确度。在半结构化文档生成中，多代理系统的应用已经显示出显著的效果。在[[60](#bib.bib60)]中，介绍了一种结合语义识别、信息检索和内容生成任务的多代理框架，以简化公共行政领域中半结构化文档的创建和管理。提出的框架涉及三种主要类型的代理：语义识别代理、信息检索代理和内容生成代理。通过避免单一模型的开销，每个代理被分配一个特定任务，用户干预最小化，遵循设计的框架和工作流程。
- en: Additionally, the AI-assisted software development framework (AISD) also showcases
    the autonomy brought by the LLM-based agents in requirement engineering. [[61](#bib.bib61)]
    proposes the AISD framework, which continuously improves and optimizes generated
    use cases and code through ongoing user feedback and interaction. In the process
    of the experiment, humans need to first give a fuzzy requirement definition, and
    then LLM-based agent will improve the requirement case according to this information,
    and then design the model and generate the system according to the case, and then
    the generated results will let humans judge whether the requirements are met or
    not. The study results indicate that AISD significantly increased use case pass
    rates to 75.2%, compared to only 24.1% without human involvement. AISD demonstrates
    the agents’ autonomous learning ability by allowing LLMs to generate all code
    files in a single session, continually refining and modifying based on user feedback.
    This also ensures code dependency and consistency, further proving the importance
    of human involvement in the requirement analysis and system testing stages.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，AI辅助的软件开发框架（AISD）也展示了基于LLM的代理在需求工程中带来的自主性。[[61](#bib.bib61)] 提出了AISD框架，该框架通过持续的用户反馈和互动不断改进和优化生成的用例和代码。在实验过程中，人类首先需要给出一个模糊的需求定义，然后LLM-based代理会根据这些信息改进需求案例，再设计模型并根据案例生成系统，然后生成的结果将让人类判断需求是否满足。研究结果表明，与没有人工参与的24.1%相比，AISD显著将用例通过率提高到75.2%。AISD通过允许LLM在一个会话中生成所有代码文件，并根据用户反馈持续进行精炼和修改，展示了代理的自主学习能力。这也确保了代码的依赖性和一致性，进一步证明了人类在需求分析和系统测试阶段的重要性。
- en: Furthermore, in generating safety requirements for autonomous driving, LLM-based
    agents have shown unique advantages by introducing multimodal capabilities. The
    system employs LLMs as automated agents to generate and refine safety requirements
    with minimal human intervention until the verification stage, which is unattainable
    with only LLMs. [[62](#bib.bib62)] describes an LLM prototype integrated into
    the existing Hazard Analysis and Risk Assessment (HARA) process, significantly
    enhancing efficiency by automatically generating specific safety-related requirements.
    The study through three design iterations progressively improved the LLM prototype’s
    efficiency by completing within a day compared to months manually. In agile software
    development, the quality of user stories directly impacts the development cycle
    and the realization of customer expectations. [[63](#bib.bib63)] demonstrates
    the successful application of the ALAS system in six agile teams at the Austrian
    Post Group IT. The ALAS system significantly improved the clarity, comprehensibility,
    and alignment with business objectives of user stories through automated analysis
    and enhancement. The entire agent framework allows the model to perform specific
    roles in the Agile development process, the study results indicated that the ALAS-improved
    user stories received high satisfaction ratings from team members.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在为自动驾驶生成安全需求时，LLM基础代理通过引入多模态能力展示了独特的优势。该系统利用LLM作为自动化代理，生成和精炼安全需求，直到验证阶段，只有LLM无法完成这一点。[[62](#bib.bib62)]
    描述了一个集成到现有危险分析和风险评估（HARA）流程中的LLM原型，通过自动生成具体的安全相关需求显著提高了效率。该研究通过三次设计迭代，逐步提高了LLM原型的效率，与人工处理的几个月相比，能够在一天内完成。在敏捷软件开发中，用户故事的质量直接影响开发周期和客户期望的实现。[[63](#bib.bib63)]
    展示了ALAS系统在奥地利邮政集团IT的六个敏捷团队中的成功应用。ALAS系统通过自动分析和增强显著提高了用户故事的清晰度、可理解性和与业务目标的一致性。整个代理框架使模型能够在敏捷开发过程中执行特定角色，研究结果表明，ALAS改进的用户故事获得了团队成员的高满意度评价。
- en: IV-C Analysis
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 分析
- en: The application of LLM-based agents in requirement engineering has demonstrated
    significant efficiency improvements and quality assurance. Through multi-agent
    collaboration and automated processing, these systems not only reduce manual intervention
    but also enhance the accuracy and consistency of requirement generation and verification.
    We can see that the tasks of LLM-based agents are no longer limited to simply
    generating requirements or filling in the gaps in descriptions. Instead, they
    involve the implementation of an automated process, with the generation of requirement
    documents being just one part of it, integrating LLM into agents enhances the
    overall system’s natural language processing and reasoning capabilities. In the
    real-world application, many tasks can no longer be accomplished by simple LLMs
    alone, especially for high-level software design. The emergence of LLM-based agents
    addresses this issue through a multi-agent collaborative system centered around
    LLMs, these agents continuously analyze and refine the deficiencies in the requirement
    documents, this is might be the main application trend of LLM-based agents in
    requirements engineering in the future.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: LLM基础代理在需求工程中的应用展示了显著的效率提升和质量保证。通过多代理协作和自动化处理，这些系统不仅减少了人工干预，还提高了需求生成和验证的准确性和一致性。我们可以看到，LLM基础代理的任务已不再仅限于生成需求或填补描述中的空白。相反，它们涉及到自动化流程的实施，需求文档的生成只是其中的一部分，将LLM整合到代理中提升了整体系统的自然语言处理和推理能力。在实际应用中，许多任务已无法仅靠简单的LLM完成，特别是在高层次的软件设计中。LLM基础代理的出现通过一个以LLM为核心的多代理协作系统解决了这一问题，这些代理持续分析和精炼需求文档中的不足，这可能是LLM基础代理在需求工程中的主要应用趋势。
- en: '![Refer to caption](img/92b54f2823b41378a8891136f1f3ac96.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/92b54f2823b41378a8891136f1f3ac96.png)'
- en: 'Figure 4: Illustration of Comparison Framework Between LLM-based Agent and
    LLM in User Story Refinement'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：LLM基础代理与LLM在用户故事精炼中的比较框架示意图
- en: 'The application of LLM-based agents in requirements engineering is still relatively
    limited, with most efforts focusing on leveraging the collaborative advantage
    of multi-agent systems to generate and refine requirements engineering documents.
    As illustrated in Figure.[4](#S4.F4 "Figure 4 ‣ IV-C Analysis ‣ IV Requirement
    Engineering and and Documentation ‣ From LLMs to LLM-based Agents for Software
    Engineering: A Survey of Current, Challenges and Future"), which roughly simulates
    the architectures presented in [[58](#bib.bib58)] and [[63](#bib.bib63)], both
    applied to the generation and refinement of user stories, we can clearly compare
    the differences between the two architectures. On the left is the architecture
    of the LLM-based agent, while on the right is the approach of using prompt engineering
    and LLMs alone to refine user stories. The figure omits more detailed and complex
    aspects of the architecture to highlight the core differences between the two
    approaches. LLM-based agents can continuously improve from different professional
    perspectives by utilizing a shared database. Although there are not many papers
    on LLM-based agents, we can observe the trend and benefits of transitioning from
    LLMs to LLM-based agents.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '基于LLM的代理在需求工程中的应用仍然相对有限，大多数努力集中在利用多代理系统的协作优势来生成和完善需求工程文档。如图中所示[4](#S4.F4 "Figure
    4 ‣ IV-C Analysis ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")，大致模拟了[[58](#bib.bib58)]和[[63](#bib.bib63)]中呈现的架构，这些架构都应用于用户故事的生成和完善，我们可以清楚地比较这两种架构之间的差异。左侧是LLM-based代理的架构，而右侧是单独使用提示工程和LLMs来完善用户故事的方法。该图省略了架构的更详细和复杂的方面，以突出两种方法之间的核心差异。LLM-based代理可以通过利用共享数据库从不同的专业角度不断改进。尽管关于LLM-based代理的论文不多，但我们可以观察到从LLMs过渡到LLM-based代理的趋势和好处。'
- en: IV-D Benchmarks
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 基准
- en: Requirement engineering, unlike tasks such as bug fixing and code generation,
    does not have an abundance of public datasets available, such as HumanEval which
    commonly used for code generation assessment. Most training datasets for models
    in requirement engineering are self-collected by the authors and not all of them
    are open-sourced on Huggingface, resulting in a limited amount of dataset in requirement
    engineering. For instance, some papers do not mention a specific benchmark dataset
    but instead focus on practical examples and case studies to demonstrate the effectiveness
    of proposed prompt patterns [[55](#bib.bib55)]. The researcher Let actual developers
    and requirements engineers use the generated requirements documents and code to
    evaluate its accuracy, usability, and completeness. User feedback will be collected
    to further improve and optimize the prompt mode.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 与修复漏洞和生成代码等任务不同，需求工程没有大量的公开数据集，例如常用于代码生成评估的HumanEval。大多数需求工程模型的训练数据集是由作者自行收集的，并非所有数据集都在Huggingface上开源，导致需求工程领域的数据集有限。例如，一些论文没有提到具体的基准数据集，而是专注于实际示例和案例研究来演示所提出的提示模式的有效性[[55](#bib.bib55)]。研究人员让实际开发人员和需求工程师使用生成的需求文档和代码来评估其准确性、可用性和完整性。用户反馈将被收集以进一步改进和优化提示模式。
- en: 'In [[50](#bib.bib50)], four datasets are primarily used, characterized by average
    length, type-token ratio (TTR), and lexical density (LD). The NFR Multi-class
    Classification dataset includes 249 non-functional requirements (NFRs) across
    15 projects from the PROMISE NFR dataset. The App Review NFR Multi-label Classification
    dataset comprises 1800 app reviews from Google Play and Apple App Store, labeled
    with various NFRs. The Term Extraction dataset contains 100 smart home user stories
    with 250 manually extracted domain terms. Lastly, the Feature Extraction dataset
    consists of 50 app descriptions across 10 application categories with manually
    identified feature phrases. In [[56](#bib.bib56)], the PURE dataset consisting
    of 40 requirements specifications totaling over 23,000 sentences, is used to test
    BERT’s ability to complete requirements. In [[64](#bib.bib64)], the benchmark
    dataset comprised 36 responses to six questions: 6 responses generated by ChatGPT
    and 30 responses from five human RE experts (each expert provided 6 responses).
    These datasets serve as evaluation metrics for the models. Combining these papers,
    we can see that benchmark datasets for LLMs in requirement engineering mainly
    include various classifications of software requirements and functional and non-functional
    requirements to aid and assist models in learning this domain, the dataset utilization
    are quite flexible and diversifies.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[50](#bib.bib50)]中，主要使用了四个数据集，这些数据集的特点包括平均长度、类型-词汇比（TTR）和词汇密度（LD）。NFR多分类数据集包含了来自PROMISE
    NFR数据集的15个项目中的249个非功能性需求（NFRs）。App Review NFR多标签分类数据集包括来自Google Play和Apple App
    Store的1800条应用评价，并标记了各种NFR。术语提取数据集包含了100个智能家居用户故事，其中手动提取了250个领域术语。最后，特征提取数据集包括了10个应用类别中的50个应用描述，其中手动识别了特征短语。在[[56](#bib.bib56)]中，PURE数据集包含了40个需求规范，总计超过23,000句，用于测试BERT在完成需求上的能力。在[[64](#bib.bib64)]中，基准数据集包括了对六个问题的36个回答：6个由ChatGPT生成的回答和30个由五名人类需求工程专家提供的回答（每位专家提供6个回答）。这些数据集作为模型的评估指标。综合这些论文，我们可以看到，需求工程中的LLMs基准数据集主要包括各种软件需求的分类以及功能性和非功能性需求，以帮助和辅助模型学习该领域，数据集的使用相当灵活且多样化。
- en: In LLM-based agents’ research in requirement engineering, the selection and
    construction of datasets are also important. In [[47](#bib.bib47)], the dataset
    mainly consists of semantic templates from the public administration domain. These
    templates cover various semi-structured forms of administrative documents, such
    as official certificates and public service forms. Although the detailed composition
    of the dataset is not specified, it can be inferred that these templates include
    a large number of practical cases and contextual information to ensure that the
    documents generated by the multi-agent system meet actual needs.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于LLM的需求工程研究中，数据集的选择和构建也非常重要。在[[47](#bib.bib47)]中，数据集主要由公共行政领域的语义模板组成。这些模板涵盖了各种半结构化形式的行政文件，如官方证书和公共服务表单。虽然数据集的详细组成没有说明，但可以推断，这些模板包括了大量实际案例和上下文信息，以确保由多代理系统生成的文件符合实际需求。
- en: Additionally, in [[61](#bib.bib61)], the CAASD (Capability Assessment of Automatic
    Software Development) dataset is introduced. This specially constructed benchmark
    dataset is used to evaluate the capabilities of AI-assisted software development
    systems. The CAASD dataset contains 72 tasks from various domains, such as small
    games and personal websites, each with reference use cases to define system requirements.
    The purpose of constructing this dataset is to provide a comprehensive evaluation
    benchmark that covers different types of development tasks, testing the performance
    of LLM-based agents in diverse tasks. In [[62](#bib.bib62)], the study mainly
    uses Design Science Methodology to design and evaluate the LLM prototype but does
    not mention a specific dataset, focusing on validating the model’s effectiveness
    through practical application and case studies. Despite the lack of detailed dataset
    descriptions, this approach emphasizes iterative improvement and practical application
    to ensure that the safety requirements generated by LLM-based agents meet high
    safety standards. Finally, in [[63](#bib.bib63)], 25 synthetic user stories are
    used, derived from a mobile delivery application project. The study evaluates
    the ALAS system’s effectiveness by testing it in six agile teams at the Austrian
    Post Group IT. Although these user stories are synthetic data designed for the
    experiment, they realistically reflect the requirements in actual projects, providing
    a valuable testing benchmark.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[[61](#bib.bib61)]中，介绍了CAASD（自动软件开发能力评估）数据集。这个特别构建的基准数据集用于评估AI辅助软件开发系统的能力。CAASD数据集包含72个来自不同领域的任务，如小型游戏和个人网站，每个任务都有参考用例来定义系统需求。构建这个数据集的目的是提供一个全面的评估基准，覆盖不同类型的开发任务，测试基于LLM的代理在各种任务中的表现。在[[62](#bib.bib62)]中，研究主要使用设计科学方法来设计和评估LLM原型，但没有提及具体数据集，重点通过实际应用和案例研究验证模型的有效性。尽管缺乏详细的数据集描述，但这种方法强调迭代改进和实际应用，以确保基于LLM的代理生成的安全要求符合高安全标准。最后，在[[63](#bib.bib63)]中，使用了25个合成用户故事，这些故事源于一个移动交付应用项目。该研究通过在奥地利邮政集团IT的六个敏捷团队中测试ALAS系统的有效性。尽管这些用户故事是为实验设计的合成数据，但它们真实地反映了实际项目中的需求，提供了有价值的测试基准。
- en: From these papers, it can be seen that the selection and construction of datasets
    in LLM-based agents’ research in requirement engineering often rely on practical
    projects and case studies, lacking standardization and large-scale datasets. Compared
    to LLM literature, the datasets used are broader and in a higher level like an
    actual system’s files, not limited to the classification of non-functional requirements
    and pure software requirement specifications. Researchers focus more on validating
    the model’s effectiveness through practical application and iterative improvement
    to enhance model performance. While this approach is flexible and targeted, it
    also highlights the field’s shortcomings in dataset standardization and scaling.
    In the future, with more public datasets being constructed and shared, the application
    of LLM-based agents in requirement engineering is expected to achieve broader
    and deeper development.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些论文中可以看出，LLM基础的需求工程研究中的数据集选择和构建通常依赖于实际项目和案例研究，缺乏标准化和大规模的数据集。与LLM文献相比，使用的数据集范围更广，处于实际系统文件的更高层次，而不仅限于非功能需求和纯软件需求规范的分类。研究人员更多地关注通过实际应用和迭代改进来验证模型的有效性，以提高模型性能。虽然这种方法灵活且有针对性，但也突显了该领域在数据集标准化和扩展方面的不足。未来，随着更多公共数据集的构建和共享，基于LLM的代理在需求工程中的应用有望实现更广泛和更深入的发展。
- en: 'TABLE V: Evaluation Metrics in Requirement Engineering and Documentation'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 需求工程和文档中的评估指标'
- en: '| Reference Paper | Benchmarks | Evaluation Metrics | Agent |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 基准 | 评估指标 | 代理 |'
- en: '|  [[51](#bib.bib51)] | Evaluation on ActApp | Precision and Recall for Elicitation
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  [[51](#bib.bib51)] | ActApp的评估 | 提取的精度和召回率 |  |'
- en: '| Clarity, Consistency, and Compliance |  |  |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度、一致性和合规性 |  |  |  |'
- en: '| Completeness and accuracy of acceptance |  |  |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 完整性和准确性的接受 |  |  |  |'
- en: '| criteria | No |  |  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 否 |  |  |'
- en: '|  [[50](#bib.bib50)] | NFR, Smarthome user stories | Precision, recall, and
    F\beta (F1 or F2) | No |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  [[50](#bib.bib50)] | NFR, 智能家居用户故事 | 精度、召回率和F\beta (F1或F2) | 否 |'
- en: '|  [[56](#bib.bib56)] | PURE | Precision, F1 Score, Recall | No |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  [[56](#bib.bib56)] | PURE | 精度、F1分数、召回率 | 否 |'
- en: '|  [[52](#bib.bib52)] | Not Specified | Likert Scale | No |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  [[52](#bib.bib52)] | 未指定 | 李克特量表 | 否 |'
- en: '|  [[55](#bib.bib55)] | Case studies | Accuracy in identifying missing requirements.
    |  |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  [[55](#bib.bib55)] | 案例研究 | 识别缺失需求的准确性 |  |'
- en: '| Quality and Modularity of generated code. |  |  |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 生成代码的质量与模块化 |  |  |  |'
- en: '| Correctness of refactoring suggestions. |  |  |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 重构建议的正确性 |  |  |  |'
- en: '| Efficiency in automating software engineering |  |  |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 自动化软件工程的效率 |  |  |  |'
- en: '| tasks | No |  |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 否 |  |  |'
- en: '|  [[64](#bib.bib64)] | 36 responses to the six questions | Abstraction, Atomicity,
    Consistency, Correctness |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  [[64](#bib.bib64)] | 对六个问题的36个回答 | 抽象性、原子性、一致性、正确性 |  |'
- en: '| Unambiguity, Understandability, Feasibility | No |  |  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 明确性、可理解性、可行性 | 否 |  |  |'
- en: '|  [[48](#bib.bib48)] | PROMISE NFR-Review, NFR-SO | F1 Score, Weighted F1
    Score (w-F)) | No |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  [[48](#bib.bib48)] | PROMISE NFR-Review、NFR-SO | F1分数、加权F1分数（w-F） | 否 |'
- en: '|  [[53](#bib.bib53)] | SV-COMP, SpecGenBench | Number of Passes, Success Probability
    |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  [[53](#bib.bib53)] | SV-COMP, SpecGenBench | 通过次数、成功概率 |  |'
- en: '| Number of Verifier Calls, User Rating | No |  |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 验证者调用次数、用户评分 | 否 |  |  |'
- en: '|  [[65](#bib.bib65)] | Jdoctor-data, DocTer-data, |  |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  [[65](#bib.bib65)] | Jdoctor-data、DocTer-data |  |  |'
- en: '| SpecGenBench, SV-COMP | Accuracy, Precision, Recall, F1 Score | No |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| SpecGenBench, SV-COMP | 准确性、精确度、召回率、F1分数 | 否 |  |'
- en: '|  [[58](#bib.bib58)] | Benchmark evaluations of user |  |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  [[58](#bib.bib58)] | 用户的基准评估 |  |  |'
- en: '| stories using the AQUSA tool | Agreement Rate, Precision, Recall, Specificity,
    |  |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 使用AQUSA工具的故事 | 一致率、精确度、召回率、特异性 |  |  |'
- en: '| F1 Score | No |  |  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| F1分数 | 否 |  |  |'
- en: '|  [[57](#bib.bib57)] | Crawled Documents from Wikipedia | Manual validation
    | No |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  [[57](#bib.bib57)] | 从维基百科抓取的文档 | 手动验证 | 否 |'
- en: '|  [[59](#bib.bib59)] | CM1, CCHIT, Dronology, PTC-A, |  |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  [[59](#bib.bib59)] | CM1、CCHIT、Dronology、PTC-A |  |  |'
- en: '| PTC-B | F\beta score, Mean Average Precision (MAP) | No |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| PTC-B | F\beta 分数、平均精度均值（MAP） | 否 |  |'
- en: '|  [[49](#bib.bib49)]] | PROMISE NFR | Precision (P), Recall (R), F1-score
    (F1), |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  [[49](#bib.bib49)]] | PROMISE NFR | 精确度（P）、召回率（R）、F1分数（F1） |  |'
- en: '| Weighted average F1-score (A) | No |  |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 加权平均F1分数（A） | 否 |  |  |'
- en: '|  [[66](#bib.bib66)] | CS-specific corpora, PURE | Contextual Clarity, User
    Feedback | No |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  [[66](#bib.bib66)] | CS特定语料库、PURE | 上下文清晰度、用户反馈 | 否 |'
- en: '|  [[60](#bib.bib60)] | Semantic Templates from Public |  |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  [[60](#bib.bib60)] | 从公共语义模板 |  |  |'
- en: '| Administration | Accuracy, Prompt Conformity, User Intervention |  |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 管理 | 准确性、及时符合性、用户干预 |  |  |'
- en: '| Frequency, Hallucination Rate | Yes |  |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 频率、幻觉率 | 是 |  |  |'
- en: '|  [[61](#bib.bib61)] | CAASD | Pass Rate, Token Consumption | Yes |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  [[61](#bib.bib61)] | CAASD | 通过率、标记消耗 | 是 |'
- en: '|  [[62](#bib.bib62)] | AEB, CAEM | Performance Accuracy and Relevance, Efficiency,
    |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|  [[62](#bib.bib62)] | AEB, CAEM | 性能准确性与相关性、效率 |  |'
- en: '| Feedback from industry | Yes |  |  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 行业反馈 | 是 |  |  |'
- en: '|  [[63](#bib.bib63)] | 25 Synthetic User Stories for a |  |  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '|  [[63](#bib.bib63)] | 25个合成用户故事 |  |  |'
- en: '| Mobile Delivery Application | Independence, Negotiability, Value, Estimability,
    |  |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 移动交付应用程序 | 独立性、可协商性、价值、可估计性 |  |  |'
- en: '| Smallness, Testability. |  |  |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 小型化、可测试性 |  |  |  |'
- en: '| Survey among professionals | Yes |  |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 专业人士调查 | 是 |  |  |'
- en: IV-E Evaluation Metrics
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 评估指标
- en: 'In the field of requirement engineering, LLMs and LLM-based agents are evaluated
    using various metrics. These metrics not only include traditional indicators such
    as precision, recall, and F1 score but also more specific indicators tailored
    to the unique nature of requirement engineering. Through these evaluations, we
    can see how these models are assessed and how they are changing the practice of
    requirement engineering. The specific evaluation metrics are detailed in Table [IV-D](#S4.SS4
    "IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"). In [[51](#bib.bib51)], while precision and recall are fundamental
    for evaluating the effectiveness of information retrieval, additional evaluations
    of clarity, consistency, and compliance are included, which are crucial quality
    indicators in requirement engineering. This multidimensional evaluation method
    not only measures the operational performance of LLMs but also examines their
    ability to maintain the quality of requirement specifications. Through this approach,
    LLMs have demonstrated their value in automating and optimizing the requirement
    elicitation process, enhancing both efficiency and the reliability of results.
    The paper  [[52](#bib.bib52)] use the Likert Scale to measure the quality of generated
    specifications, the specification will be scored by its Unambiguous, Understandable,
    Conciseness, etc. The Likert Scale will be scored from 1 to 5 of agreements.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '在需求工程领域，LLMs和基于LLM的代理使用各种指标进行评估。这些指标不仅包括传统的精度、召回率和F1分数，还包括针对需求工程独特性质的更具体指标。通过这些评估，我们可以看到这些模型如何被评估以及它们如何改变需求工程的实践。具体的评估指标详见表[IV-D](#S4.SS4
    "IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")。在[[51](#bib.bib51)]中，尽管精度和召回率是评估信息检索效果的基本指标，但还包括了清晰度、一致性和合规性等附加评估，这些都是需求工程中的关键质量指标。这种多维评估方法不仅测量了LLMs的操作性能，还检查了它们维持需求规格质量的能力。通过这种方法，LLMs展示了其在自动化和优化需求提取过程中的价值，提高了效率和结果的可靠性。论文[[52](#bib.bib52)]使用Likert量表来衡量生成规格的质量，规格将根据其明确性、可理解性、简洁性等进行评分。Likert量表的评分范围从1到5。'
- en: For agent-based LLMs, as demonstrated in [[63](#bib.bib63)], the evaluation
    extends to assessing the independence and negotiability of the agents, elevating
    their functionality to a new level. These agents provide technical solutions and
    also interact with users, autonomously adjusting to meet specific project needs,
    thus resembling collaborative partners. This capability makes LLM-based agents
    valuable in requirement engineering in the requirement management and decision
    optimization, also highlight that LLMs typically focus on improving the accuracy
    and efficiency of specific tasks, while LLM-based agents exhibit higher capabilities
    in autonomy and adaptability.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于代理的LLMs，如[[63](#bib.bib63)]中所示，评估不仅包括对代理的独立性和可协商性的评估，还将其功能提升到一个新的水平。这些代理提供技术解决方案，并与用户互动，自主调整以满足特定项目需求，因此类似于协作伙伴。这种能力使得基于LLM的代理在需求工程中非常有价值，尤其是在需求管理和决策优化方面，同时也突显了LLMs通常专注于提高特定任务的准确性和效率，而基于LLM的代理则表现出更高的自主性和适应性。
- en: 'In Table [IV-D](#S4.SS4 "IV-D Benchmarks ‣ IV Requirement Engineering and and
    Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future"), we can see that the application of LLMs in
    requirements engineering typically requires common metrics such as F1 Score to
    evaluate model’s performance. However, for LLM-based agents, the evaluation focus
    shifts from the performance of requirement document generation to the quality
    of the final product. Therefore, evaluation metrics emphasize user satisfaction,
    such as pass rate, feedback, etc. Essentially, LLM-based agents still leverage
    LLMs themselves to achieve higher-level development, and depends pretty much on
    the nature of the task. In summary we can conclude that, the characteristics of
    agent models both reflect their complex decision-making and learning abilities
    also reveal their potential advantages in collaboration with human or other tools
    to provide higher scalability and flexibility design. This phenomenon implies
    the potential opportunities that the methods of requirement elicitation and processing
    in future software development will become more efficient, precise, and continuous
    refine to better align with stakeholder’s needs by using LLM-based agents.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [IV-D](#S4.SS4 "IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation
    ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey of Current,
    Challenges and Future")中，我们可以看到，LLMs在需求工程中的应用通常需要使用如F1 Score这样的常见指标来评估模型性能。然而，对于基于LLMs的代理，评估重点从需求文档生成的性能转移到了最终产品的质量。因此，评估指标强调用户满意度，例如通过率、反馈等。实际上，基于LLMs的代理仍然利用LLMs自身来实现更高层次的发展，并且很大程度上依赖于任务的性质。总的来说，我们可以得出结论，代理模型的特征既反映了其复杂的决策和学习能力，也揭示了其在与人类或其他工具协作时提供更高可扩展性和灵活性设计的潜在优势。这一现象暗示了未来软件开发中需求引导和处理方法有可能变得更加高效、精准，并通过使用基于LLMs的代理不断优化，更好地满足利益相关者的需求。'
- en: V Code Generation and Software Development
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V代码生成和软件开发
- en: Code generation and software development are core areas within software engineering
    which plays a crucial role in the software development process. The primary objective
    of using LLMs in code generation is to enhance development efficiency and code
    quality through automation processes, thereby meeting the needs of both developers
    and users.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成和软件开发是软件工程中的核心领域，这些领域在软件开发过程中扮演着至关重要的角色。使用**大语言模型（LLMs）**进行代码生成的主要目标是通过自动化过程提高开发效率和代码质量，从而满足开发人员和用户的需求。
- en: In recent years, the application of LLMs in code generation and software development
    has made significant progress, this has changed the way developers work and revealed
    a shift in automated development processes. Compared to requirement engineering,
    research on the application of LLMs and LLM-based agents in code generation and
    software development is more extensive and in-depth. Using natural language processing
    and generation technologies, LLMs can understand and generate complex code snippets,
    assisting developers in automating various stages from code writing and debugging
    to software optimization. The decoder-based large language models such as GPT-4
    have shown significant potential in code generation by providing accurate code
    suggestions and automated debugging, greatly improving development efficiency.
    Recently, the application of LLM-based agents in software development is also
    gaining attention, these intelligent agents can not only perform complex code
    generation tasks but also engage in autonomous learning and continuous refinement,
    thereby offering flexible assist in dynamic development environments. Tools like
    GitHub Copilot [[12](#bib.bib12)], which integrate LLMs, have already demonstrated
    their advantages in enhancing programming efficiency and code quality.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，**大语言模型（LLMs）**在代码生成和软件开发中的应用取得了显著进展，这改变了开发人员的工作方式，并揭示了自动化开发过程的转变。与需求工程相比，关于LLMs和基于LLMs的代理在代码生成和软件开发中的应用研究更为广泛和深入。利用自然语言处理和生成技术，LLMs能够理解和生成复杂的代码片段，帮助开发人员自动化从代码编写和调试到软件优化的各个阶段。基于解码器的大语言模型，如GPT-4，通过提供准确的代码建议和自动化调试，展现了在代码生成中的显著潜力，极大地提高了开发效率。最近，基于LLMs的代理在软件开发中的应用也受到关注，这些智能代理不仅可以执行复杂的代码生成任务，还可以进行自主学习和持续优化，从而在动态开发环境中提供灵活的支持。像GitHub
    Copilot [[12](#bib.bib12)]这样的工具，已经展示了在提高编程效率和代码质量方面的优势。
- en: V-A LLMs Tasks
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A LLMs任务
- en: Large language models have optimized various tasks in code generation and software
    development through automation and reasoning, covering areas such as code generation,
    debugging, code comprehension, code completion, code transformation, and multi-turn
    interactive code generation. The primary method is generating executable code
    from natural language descriptions, where models utilize previously learnt code
    snippets or apply few-shot learning to better understand user requirements. Nowadays
    the AI tools integrates deeply with IDEs like Visual Studio Code²²2[https://code.visualstudio.com/](https://code.visualstudio.com/)
    and JetBrains³³3[https://www.jetbrains.com/](https://www.jetbrains.com/) to enhance
    code writing and translation tasks such as OpenAI’s Codex model[[67](#bib.bib67)].
    Codex fine-tuned on public code from GitHub, demonstrate the capability to generate
    Python functions from doc-strings also outperformed other similar models on the
    HumanEval benchmark.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型通过自动化和推理优化了代码生成和软件开发的各种任务，包括代码生成、调试、代码理解、代码补全、代码转换和多轮互动代码生成。主要方法是从自然语言描述中生成可执行代码，模型利用之前学习的代码片段或应用少量学习以更好地理解用户需求。现在，AI
    工具与 IDE 如 Visual Studio Code²²2[https://code.visualstudio.com/](https://code.visualstudio.com/)
    和 JetBrains³³3[https://www.jetbrains.com/](https://www.jetbrains.com/) 深度集成，提升了代码编写和翻译任务，例如
    OpenAI 的 Codex 模型[[67](#bib.bib67)]。Codex 在 GitHub 上的公共代码上进行了微调，展示了从文档字符串生成 Python
    函数的能力，并在 HumanEval 基准测试中优于其他类似模型。
- en: In [[68](#bib.bib68)], researchers comprehensively evaluated the performance
    of multiple LLMs on L2C(language to code) tasks. The results showed that GPT-4
    demonstrates strong capability in tasks such as semantic parsing, mathematical
    reasoning, and Python programming. With instruction tuning and support from large-scale
    training data, the model can understand and generate code that aligns with user
    intent, achieving high-precision code generation. Applying LLMs to text-to-database
    management and query optimization is also a novel research direction in natural
    language to code generation task. By converting natural language queries into
    SQL statements, LLMs help developers quickly generate efficient database query
    code. In [[69](#bib.bib69)], proposed the SQL-PaLM framework which significantly
    enhances the execution accuracy and exact match rate for text-to-SQL tasks through
    a few-shot prompt and instruction fine-tuning, providing an effective solution
    for complex cross-domain SQL generation tasks. The improvements in accuracy and
    exact match achieved in the SQL-PaLM model are considered state-of-the-art (SOTA)
    in tested benchmarks, the SQL-PaLM performed promise results comparing with existing
    methods such as T5-3B + PICARD, RASAT + PICARD, and even GPT-4, achieving the
    highest test accuracy of 77.3% and an execution accuracy of 82.7%. Multilingual
    code generation is another important application of LLMs, particularly suited
    to the transformer architecture. In [[70](#bib.bib70)], researchers introduced
    the CodeGeeX model, which was pre-trained on multiple programming languages and
    performed well in multilingual code generation and translation tasks. Experimental
    results showed that CodeGeeX outperformed other multilingual models on the HumanEval-X
    benchmark.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[68](#bib.bib68)]中，研究人员全面评估了多个 LLM 在语言到代码（L2C）任务中的表现。结果显示 GPT-4 在语义解析、数学推理和
    Python 编程等任务中表现出强大的能力。通过指令调优和大规模训练数据的支持，该模型能够理解并生成符合用户意图的代码，实现高精度的代码生成。将 LLM 应用于文本到数据库管理和查询优化也是自然语言到代码生成任务中的一个新研究方向。通过将自然语言查询转换为
    SQL 语句，LLM 帮助开发者快速生成高效的数据库查询代码。在[[69](#bib.bib69)]中，提出了 SQL-PaLM 框架，通过少量提示和指令微调显著提高了文本到
    SQL 任务的执行准确性和完全匹配率，为复杂跨领域 SQL 生成任务提供了有效解决方案。SQL-PaLM 模型在准确性和完全匹配度上的改进被认为是测试基准中的先进水平（SOTA），与现有方法如
    T5-3B + PICARD、RASAT + PICARD 甚至 GPT-4 相比，SQL-PaLM 在测试准确性上达到了 77.3% 的最高水平，执行准确性达到了
    82.7%。多语言代码生成是 LLM 另一个重要应用，尤其适合于变换器架构。在[[70](#bib.bib70)]中，研究人员介绍了 CodeGeeX 模型，该模型在多种编程语言上进行了预训练，在多语言代码生成和翻译任务中表现良好。实验结果显示
    CodeGeeX 在 HumanEval-X 基准测试中优于其他多语言模型。
- en: Although current LLMs possess excellent code generation capabilities, with accuracy
    and compile rates reaching usable levels, the quality of generated code often
    depends on the user’s prompts. If the prompts are too vague or general, the LLM
    typically struggles to understand the user’s true requirements, making it difficult
    to generate the desired code in a single attempt. In [[71](#bib.bib71)], researchers
    introduced ”print debugging” technique, using GPT-4 to track variable values and
    execution flows, which enhancing the efficiency and accuracy by using in-context
    learning techniques. This method is particularly suitable for medium-difficulty
    problems on Leetcode, compared to the rubber duck debugging method, print debugging
    improved performance by 1.5% on simple Leetcode problems and by 17.9% on medium-difficulty
    problems.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当前的大型语言模型（LLMs）具有出色的代码生成能力，准确性和编译率都达到了可用水平，但生成代码的质量往往依赖于用户的提示。如果提示过于模糊或笼统，LLM
    通常很难理解用户的真实需求，从而难以在一次尝试中生成所需的代码。在 [[71](#bib.bib71)] 中，研究人员介绍了“打印调试”技术，使用 GPT-4
    跟踪变量值和执行流程，通过使用上下文学习技术提高了效率和准确性。这种方法特别适用于 Leetcode 上的中等难度问题，与橡皮鸭调试方法相比，打印调试在简单的
    Leetcode 问题上的性能提高了 1.5%，在中等难度问题上的性能提高了 17.9%。
- en: Additionally, the application of LLMs in improving programming efficiency has
    garnered widespread attention, the tools like GitHub Copilot which integrating
    OpenAI’s Codex model, provide real-time code completion and suggestions during
    coding. According to [[72](#bib.bib72)], researchers present a controlled experiment
    with the Github Copilot, the result demonstrated that with developers completing
    HTTP server tasks 55.8% faster when using Copilot. Another similar study also
    using LLM to be the programmer tools, in [[73](#bib.bib73)], researchers introduced
    the INCODER model which capable of both program synthesis and editing. By leveraging
    bidirectional context, the model performs well in both single-line and multi-line
    code filling tasks, providing developers with smarter code editing tools. This
    real-time code generation and completion functionality not only improves programming
    efficiency but also reduce the burden on developers, allowing them to focus on
    higher-level design which is a common problem in software development where substantial
    workforce and time are wasted on tedious coding tasks.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLM 在提高编程效率方面的应用引起了广泛关注，像 GitHub Copilot 这样的工具集成了 OpenAI 的 Codex 模型，提供实时代码补全和建议。根据
    [[72](#bib.bib72)]，研究人员进行了一个对照实验，结果表明，使用 Copilot 时，开发者完成 HTTP 服务器任务的速度提高了 55.8%。另一项类似的研究也使用
    LLM 作为编程工具，在 [[73](#bib.bib73)] 中，研究人员介绍了 INCODER 模型，该模型能够进行程序合成和编辑。通过利用双向上下文，该模型在单行和多行代码填充任务中表现出色，为开发者提供了更智能的代码编辑工具。这种实时代码生成和补全功能不仅提高了编程效率，还减轻了开发者的负担，使他们能够专注于更高级的设计，这在软件开发中是一个常见问题，因为大量人力和时间被浪费在繁琐的编码任务上。
- en: The multi-turn program synthesis tasks represent a significant breakthrough
    for LLMs in handling complex programming tasks, in  [[74](#bib.bib74)], researchers
    introduced the CODEGEN model, which iteratively generates programs through multiple
    interactions, significantly improving program synthesis quality and making the
    development process more efficient and accurate. By gradually generating and continuously
    optimizing code at each interaction, LLMs can better understand user intent and
    generate more precise and optimized code. In the experiments, comparisons were
    made with the Codex model, which was considered state-of-the-art in code generation
    at the time. CODEGEN-MONO 2.7B outperformed the Codex model of equivalent outcome
    in pass@k metrics for both k=1 and k=10\. Furthermore, CODEGEN-MONO 16.1B exhibited
    performance that was comparable to or better than the best Codex model on certain
    metrics, further demonstrating its SOTA performance in the code generation. By
    iteratively generating and optimizing code, LLMs continuously improve their output
    quality. In [[75](#bib.bib75)], researchers proposed the Cycle framework, which
    enhances the self-improvement capability of code language models by learning from
    execution feedback, improving code generation performance by 63.5% on multiple
    benchmark datasets. Although Cycle has a certain degree of autonomy, its decision-making
    and planning capabilities are mainly limited to code generation and improvement
    tasks without overall planning, and the execution sequence is completely followed
    a fixed pattern, so it’s better to classified as an advanced LLM application.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 多轮程序合成任务标志着LLM在处理复杂编程任务方面的重大突破。在[[74](#bib.bib74)]中，研究人员介绍了CODEGEN模型，该模型通过多次交互迭代生成程序，显著提高了程序合成的质量，使开发过程更高效、准确。通过在每次交互中逐步生成和不断优化代码，LLM能够更好地理解用户意图，生成更精确和优化的代码。在实验中，与当时被认为是最先进的代码生成模型Codex进行了比较。CODEGEN-MONO
    2.7B在k=1和k=10的pass@k指标上超越了相同结果的Codex模型。此外，CODEGEN-MONO 16.1B在某些指标上表现出与最佳Codex模型相当或更好的性能，进一步展示了其在代码生成中的SOTA表现。通过迭代生成和优化代码，LLM不断提高其输出质量。在[[75](#bib.bib75)]中，研究人员提出了Cycle框架，通过学习执行反馈来增强代码语言模型的自我改进能力，在多个基准数据集上提高了63.5%的代码生成性能。尽管Cycle具有一定程度的自主性，但其决策和规划能力主要限于代码生成和改进任务，没有整体规划，执行顺序完全遵循固定模式，因此更适合被归类为高级LLM应用。
- en: V-B LLM-based Agents Tasks
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 基于LLM的智能体任务
- en: LLM-based agents have shown significant potential and advantages by substantially
    improving task efficiency and effectiveness through multi-agent collaboration.
    Unlike traditional LLMs, LLM-based agents adopt a division of labor approach,
    breaking down complex tasks into multiple subtasks handled by specialized agents,
    this method can enhance task efficiency and improves the quality and accuracy
    of generated code to mitigate the hallucination from the single LLM.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的智能体通过多智能体协作显著提高了任务效率和效果，展现出巨大的潜力和优势。与传统的LLM不同，基于LLM的智能体采用了分工合作的方法，将复杂任务分解为多个由专业智能体处理的子任务，这种方法可以提高任务效率，并改善生成代码的质量和准确性，以减轻单一LLM的幻觉现象。
- en: 'In [[76](#bib.bib76)], researchers proposed a self-collaboration framework
    where multiple ChatGPT (GPT-3.5-turbo) agents act as different roles to collaboratively
    handle complex code generation tasks. Specifically, the introduction of Software
    Development Methodology (SDM) divides the development process into three stages:
    analysis, coding, and testing. Each stage is managed by specific roles, and after
    completing their tasks, each role provides feedback and collaborates with others
    to improve the quality of the generated code. Experiment shows that this self-collaboration
    framework significantly improves performance on both the HumanEval and MBPP benchmarks,
    with the highest improvement reaching 29.9% in HummanEval compared to the SOTA
    model GPT-4\. This result demonstrating the potential of collaborative teams in
    complex code generation tasks. Although it lacks external tool integration and
    dynamic adjustment capabilities, this framework exhibits common characteristics
    of LLM-based agents, such as role distribution, self-improvement ability, and
    excellent autonomous decision-making, these combined capabilities qualify it to
    be considered an LLM-based agent. Similarly, In [[77](#bib.bib77)], the LCG framework
    improved code generation quality also through multi-agent collaboration and chain-of-thought
    techniques, once again demonstrating the effectiveness of multi-agent collaboration
    in the software development process.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[76](#bib.bib76)]中，研究人员提出了一种自我协作框架，其中多个ChatGPT（GPT-3.5-turbo）代理扮演不同角色，以协作处理复杂的代码生成任务。具体而言，引入了软件开发方法论（SDM），将开发过程分为分析、编码和测试三个阶段。每个阶段由特定角色管理，完成任务后，每个角色提供反馈并与其他角色合作以提高生成代码的质量。实验表明，这一自我协作框架在HumanEval和MBPP基准上显著提高了性能，在HumanEval中相比于SOTA模型GPT-4，最高提升达29.9%。这一结果展示了协作团队在复杂代码生成任务中的潜力。尽管缺乏外部工具集成和动态调整能力，但该框架展现了LLM基础代理的共同特征，如角色分配、自我改进能力和出色的自主决策能力，这些综合能力使其符合LLM基础代理的标准。同样，在[[77](#bib.bib77)]中，LCG框架通过多代理协作和链式思维技术也提高了代码生成质量，再次展示了多代理协作在软件开发过程中的有效性。
- en: 'The limitations of context windows was not discussed in previous studies, this
    has been thoroughly explored in a 2024 by University of Cambridge team. In [[78](#bib.bib78)],
    researchers introduced the L2MAC framework, which dynamically manages memory and
    execution context through a multi-agent system to generate large codebases, and
    achieved SOTA performance in generating large codebases for system design tasks.
    The framework is primarily divided into the following components: the processor,
    which is responsible for the actual generation of task outputs; the Instruction
    Registry, which stores program prompts to solve user tasks; and the File Storage,
    which contains both final and intermediate outputs. The Control Unit periodically
    checks the outputs to ensure that the generated content is both syntactically
    and functionally correct. The researchers conducted multiple experiments and compared
    with many novel methods like GPT-4, Reflexion, and AutoGPT, achieving a Pass@1
    score of 90.2% on the HumanEval benchmark, showcasing its superior performance
    in generating large-scale codebases.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 上述文本未在之前的研究中讨论上下文窗口的限制，这一问题已在2024年由剑桥大学团队进行深入探讨。在[[78](#bib.bib78)]中，研究人员介绍了L2MAC框架，该框架通过多代理系统动态管理记忆和执行上下文，以生成大型代码库，并在系统设计任务中实现了**SOTA**性能。该框架主要分为以下组件：处理器，负责实际生成任务输出；指令注册表，存储程序提示以解决用户任务；文件存储，包含最终和中间输出。控制单元定期检查输出，以确保生成内容在语法和功能上都是正确的。研究人员进行了多次实验，并与许多新方法如GPT-4、Reflexion和AutoGPT进行比较，在HumanEval基准上取得了90.2%的Pass@1评分，展示了其在生成大规模代码库方面的优越性能。
- en: Recently, many studies have begun to use LLM-based agents to simulate real software
    development processes, the paper  [[79](#bib.bib79)] introduced the MetaGPT framework,
    which enhanced problem-solving capabilities through standard operating procedures
    (SOPs) encoded in multi-agent collaboration. The entire process of the multi-collaboration
    framework simulates the waterfall life-cycle of software development, with each
    agent playing different roles and collaborating to achieve the goal of automating
    software development. LLM-based agents have also shown strong ability in automated
    software development,[[80](#bib.bib80)] proposed a multi-GPT agent framework that
    automates tasks such as project planning, requirement engineering, software design,
    and debugging, illustrating the potential for automated software development.
    Similarly[[81](#bib.bib81)] introduced the model called CodePori, which is a novel
    model designed to automate code generation for extensive and complex software
    projects based on natural language prompts. In[[82](#bib.bib82)] the AgentCoder
    framework collaborates with programmer agents, test design agents, and test execution
    agents to generate and optimize code, outperforming existing methods, achieved
    SOTA performance on the HumanEval-ET benchmark with pass@1 of 77.4% compared to
    the previous state-of-the-art result of 69.5%, this result showcasing the advantages
    of multi-agent systems in code generation and testing.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多研究开始使用基于LLM的代理来模拟真实的软件开发过程。论文[[79](#bib.bib79)]介绍了MetaGPT框架，该框架通过在多代理协作中编码的标准操作程序（SOPs）增强了问题解决能力。这个多协作框架的整个过程模拟了软件开发的瀑布生命周期，每个代理扮演不同的角色并协作实现自动化软件开发的目标。基于LLM的代理在自动化软件开发中也显示了强大的能力，[[80](#bib.bib80)]提出了一个多-GPT代理框架，该框架自动化了项目规划、需求工程、软件设计和调试等任务，展示了自动化软件开发的潜力。类似地，[[81](#bib.bib81)]介绍了一个名为CodePori的模型，这是一个旨在基于自然语言提示自动生成大量复杂软件项目代码的创新模型。在[[82](#bib.bib82)]中，AgentCoder框架与程序员代理、测试设计代理和测试执行代理协作生成和优化代码，优于现有方法，在HumanEval-ET基准测试中达到了SOTA表现，pass@1为77.4%，而之前的最新成果为69.5%，这一结果展示了多代理系统在代码生成和测试中的优势。
- en: The purpose of integrating LLMs into agents from many framework is to enhance
    the self-feedback and reflection capabilities of the entire agent system. Because
    the current open-source LLMsgenerally have much lower capabilities in this aspect
    compared to proprietary models, the emergence of LLM-based agents can help bridge
    the gap between open-source models and the advanced capabilities of proprietary
    systems like GPT-4. [[83](#bib.bib83)] introduced the OpenCodeInterpreter framework,
    which improved the accuracy of code generation models by integrating code generation,
    execution, and human feedback. Based on CodeLlama and DeepSeekCoder, this framework
    performed close to the GPT-4 Code Interpreter on the HumanEval and MBPP benchmarks.
    The abbility of using external tools or APIs is another significant advantage
    of LLM-based agents,[[84](#bib.bib84)] proposed the Toolformer model, which significantly
    enhanced task performance by learning to call APIs through self-supervision. The
    framework Based on GPT-J (6.7B parameters) achieved significant performance improvements
    across multiple benchmark tasks, demonstrating the possibility of LLM-based agent
    brought by the external tool, the diverse choice of tools and architectures, allowing
    LLMs to continuously learn new things and improve themselves. Similarly,[[85](#bib.bib85)]
    enhanced LLMs’ interaction with external APIs through the ToolLLM framework, outperforming
    Text-Davinci-003 and Claude-2 on the ToolBench and APIBench benchmarks and excelling
    in multi-tool instruction processing.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 将大语言模型（LLMs）集成到许多框架中的代理系统的目的是增强整个代理系统的自我反馈和反思能力。由于当前的开源LLMs在这方面的能力通常远低于专有模型，基于LLM的代理的出现可以帮助弥补开源模型与GPT-4等专有系统的高级能力之间的差距。[[83](#bib.bib83)]介绍了OpenCodeInterpreter框架，通过集成代码生成、执行和人类反馈，提高了代码生成模型的准确性。基于CodeLlama和DeepSeekCoder，该框架在HumanEval和MBPP基准测试中表现接近GPT-4代码解释器。使用外部工具或API的能力是基于LLM的代理的另一个重要优势，[[84](#bib.bib84)]提出了Toolformer模型，通过自我监督学习调用API，从而显著提升了任务性能。基于GPT-J（6.7B参数）的框架在多个基准任务上实现了显著的性能提升，展示了外部工具带来的LLM-based代理的可能性，包括工具和架构的多样选择，使LLMs能够不断学习新事物并自我改进。同样，[[85](#bib.bib85)]通过ToolLLM框架增强了LLMs与外部API的互动，在ToolBench和APIBench基准测试中超越了Text-Davinci-003和Claude-2，并在多工具指令处理上表现优异。
- en: V-C Analysis
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C分析
- en: The main differences between LLM-based agents and traditional LLMs in software
    development applications mainly focus on the efficiency and autonomy, particularly
    in task division and collaboration. Traditional LLMs typically use a single model
    to handle specific tasks, such as generating code from text and code completion.
    However, this approach has limitations when dealing with complex tasks, especially
    regarding context window restrictions and the need for continuous feedback. LLM-based
    agents handle different subtasks through collaboration with clear division of
    labor, thereby enhancing task efficiency and quality. For example, in a code generation
    task, one agent generates the initial code, another designs test cases, and a
    third executes tests and provides feedback, thus achieving iterative optimization.
    Through task division, multi-agent systems, and tool integration, LLM-based agents
    can tackle more complex and broader tasks, improving the quality and efficiency
    of code generation. This approach overcomes the limitations of traditional LLMs
    also provides new directions and ideas for future software development research
    and applications, to frees programmers from the boring test suite generation.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理和传统LLMs在软件开发应用中的主要区别集中在效率和自主性，特别是在任务分配和协作方面。传统LLMs通常使用单一模型来处理特定任务，例如从文本生成代码和代码补全。然而，这种方法在处理复杂任务时存在限制，尤其是在上下文窗口限制和需要持续反馈方面。基于LLM的代理通过明确的劳动分工来处理不同的子任务，从而提高任务的效率和质量。例如，在代码生成任务中，一个代理生成初始代码，另一个设计测试用例，第三个执行测试并提供反馈，从而实现迭代优化。通过任务分配、多代理系统和工具集成，基于LLM的代理可以处理更复杂和更广泛的任务，提高代码生成的质量和效率。这种方法克服了传统LLMs的限制，也为未来的软件开发研究和应用提供了新的方向和思路，使程序员从枯燥的测试用例生成中解放出来。
- en: '![Refer to caption](img/0ebb325034ada11081d8bcd77a4733ad.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0ebb325034ada11081d8bcd77a4733ad.png)'
- en: 'Figure 5: Illustration of Comparison Framework Between LLM-based Agent and
    LLM in Code Generation and Software Development'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：基于LLM的代理与LLM在代码生成和软件开发中的比较框架示意图
- en: 'In software engineering task handling, there are subtle differences between
    LLMs and LLM-based agents in terms of task focus, approach, complexity and scalability,
    automation level, and task management. LLMs primarily focus on enhancing the code
    generation capabilities of a single LLM, including debugging, precision, evaluation.
    These methods typically improve specific aspects of code generation or evaluation
    through a single model, concentrating on performance enhancement within existing
    constraints, such as context windows and single-task execution. In contrast, LLM-based
    agents emphasize handling more complex and broader tasks through the collaboration
    of multiple specialized LLMs or frameworks, integrating tool usage, iterative
    testing, and multi-agent coordination to optimize the whole development process
    and easily surpass the state-of-art model in common benchmarks. The emeergence
    of multi-agent systems also brings more possibilities, this system can imitate
    the real software developer to perform the scrum development. Figure. [5](#S5.F5
    "Figure 5 ‣ V-C Analysis ‣ V Code Generation and Software Development ‣ IV-E Evaluation
    Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future") utilize studies [[77](#bib.bib77)] and [[75](#bib.bib75)] showcase
    the differences between LLM-based agents and LLMs on the same code generation
    task. The LLM-based agents system are able to perform multi-agent collaboration
    and simulate the real scrum development team in the industry. In contrast the
    LLMs on the right are normally use multi-LLMs to analysis mistakes from the test
    cases, and refine the initial generated code, but they lack autonomy and efficiency,
    as the test cases are manually generated by humans.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程任务处理方面，LLMs 和基于 LLM 的代理在任务关注点、方法、复杂性和可扩展性、自动化水平以及任务管理等方面存在细微差别。LLMs 主要关注提升单个
    LLM 的代码生成能力，包括调试、精准度、评估。这些方法通常通过单一模型改善代码生成或评估的特定方面，集中于在现有约束条件下（如上下文窗口和单任务执行）提升性能。相比之下，基于
    LLM 的代理强调通过多个专业 LLM 或框架的协作来处理更复杂、更广泛的任务，整合工具使用、迭代测试和多代理协调，以优化整个开发过程，并轻松超越常见基准中的最先进模型。多代理系统的出现也带来了更多可能性，这些系统可以模拟真实的软件开发人员来执行敏捷开发。图。[5](#S5.F5
    "图 5 ‣ V-C 分析 ‣ V 代码生成与软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程与文档 ‣ 从 LLM 到基于 LLM
    的软件工程代理：当前挑战与未来展望") 利用研究 [[77](#bib.bib77)] 和 [[75](#bib.bib75)] 展示了基于 LLM 的代理与
    LLM 在相同代码生成任务上的差异。基于 LLM 的代理系统能够执行多代理协作，并模拟行业中的真实敏捷开发团队。相比之下，右侧的 LLM 通常使用多 LLM
    来分析测试用例中的错误，并完善初始生成的代码，但它们缺乏自主性和效率，因为测试用例是由人工手动生成的。
- en: V-D Benchmarks
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 基准
- en: In the field of code generation and software development, there are notable
    differences and commonalities in the dataset used for research on LLMs and LLM-based
    agents. These datasets provide important benchmarks for evaluating model performance,
    the HumanEval dataset, widely used for assessing code generation models, is handcrafted
    by OpenAI and contains 164 programming problems, each including a function signature,
    problem description, function body, and unit tests. This dataset is primarily
    used to evaluate a model’s ability to generate correct code, particularly in tasks
    that involve converting natural language descriptions into executable code. Many
    studies have utilized HumanEval to test the performance of code generation models [[76](#bib.bib76)].
    The MBPP (Mostly Basic Python Programming) dataset is another common benchmark,
    comprising 427 Python programming problems that cover basic concepts and standard
    library functions, this dataset is used to evaluate model performance across various
    programming scenarios. In [[82](#bib.bib82)], researchers used the MBPP dataset
    to test the performance of multi-agent systems in code generation and optimization,
    improving the accuracy and robustness of generated code through agent collaboration.
    The HumanEval-ET and MBPP-ET datasets are extensions of the original HumanEval
    and MBPP datasets, adding more test cases and more complex problems for a comprehensive
    evaluation of model performance [[86](#bib.bib86)]. The Spider and BIRD datasets
    focus on converting natural language to SQL queries, evaluating the model’s ability
    to handle complex query generation tasks. In [[69](#bib.bib69)], researchers used
    these datasets to test the SQL-PaLM framework, which evaluating the execution
    accuracy and exact match rate for SQL generation tasks through few-shot prompt
    and instruction fine-tuning. ToolBench and APIBench datasets are used to evaluate
    a model’s capability in using tools and APIs, ToolBench contains 16,464 real-world
    RESTful API instructions, and APIBench normally tests a model’s generalization
    ability to unseen API instructions [[85](#bib.bib85)]. The CAASD (Capability Assessment
    of Automatic Software Development) dataset is a newly developed benchmark comprising
    72 software development tasks from various domains, each with a set of reference
    use cases to evaluate AI-assisted software development systems [[61](#bib.bib61)].
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码生成和软件开发领域，LLM和基于LLM的代理研究所用的数据集存在显著的差异和共性。这些数据集提供了评估模型性能的重要基准，HumanEval 数据集广泛用于评估代码生成模型，由
    OpenAI 手工制作，包含 164 个编程问题，每个问题包括函数签名、问题描述、函数体和单元测试。这个数据集主要用于评估模型生成正确代码的能力，特别是在将自然语言描述转换为可执行代码的任务中。许多研究利用
    HumanEval 来测试代码生成模型的表现 [[76](#bib.bib76)]。MBPP（Mostly Basic Python Programming）数据集是另一个常见的基准，包含
    427 个 Python 编程问题，涵盖基本概念和标准库函数，这个数据集用于评估模型在各种编程场景中的表现。在 [[82](#bib.bib82)]中，研究人员使用
    MBPP 数据集来测试多代理系统在代码生成和优化中的表现，通过代理协作提高生成代码的准确性和鲁棒性。HumanEval-ET 和 MBPP-ET 数据集是
    HumanEval 和 MBPP 数据集的扩展，增加了更多的测试用例和更复杂的问题，以全面评估模型性能 [[86](#bib.bib86)]。Spider
    和 BIRD 数据集侧重于将自然语言转换为 SQL 查询，评估模型处理复杂查询生成任务的能力。在 [[69](#bib.bib69)]中，研究人员使用这些数据集测试
    SQL-PaLM 框架，通过少量示例提示和指令微调来评估 SQL 生成任务的执行准确性和精确匹配率。ToolBench 和 APIBench 数据集用于评估模型在使用工具和
    API 方面的能力，ToolBench 包含 16,464 条真实世界的 RESTful API 指令，而 APIBench 通常测试模型对未见 API 指令的泛化能力 [[85](#bib.bib85)]。CAASD（Capability
    Assessment of Automatic Software Development）数据集是一个新开发的基准，包含来自各个领域的 72 个软件开发任务，每个任务都有一组参考用例，用于评估
    AI 辅助的软件开发系统 [[61](#bib.bib61)]。
- en: There are some obvious commonalities in dataset selection for LLMs and LLM-based
    agents, the HumanEval and MBPP datasets are widely used to assess code generation
    capabilities, covering a variety of programming tasks and languages. Moreover,
    many studies have adopted multilingual and cross-domain datasets, such as HumanEval-X
    and CodeSearchNet, to evaluate model performance across different languages and
    tasks. For the differences, LLM-based agents tend to use multi-agent collaboration
    frameworks to handle complex tasks, thus favoring benchmark datasets that emphasize
    multi-turn interactions and iterative optimization, also focus on tool usage and
    API integration capabilities, the framework TOOLLLM used ToolBench and APIBench
    to assess its tool usage capabilities, while Toolformer demonstrated its ability
    to autonomously learn to use tools. These differences primarily from the different
    approaches to task handling between LLMs and LLM-based agents, LLMs typically
    optimize a single model’s performance by fine-tuning on relevant datasets.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs和基于LLM的智能体，数据集选择有一些明显的共同点，HumanEval和MBPP数据集被广泛用于评估代码生成能力，涵盖了各种编程任务和语言。此外，许多研究还采用了多语言和跨领域的数据集，如HumanEval-X和CodeSearchNet，以评估模型在不同语言和任务上的表现。至于差异，基于LLM的智能体倾向于使用多智能体协作框架来处理复杂任务，因此更青睐于强调多轮交互和迭代优化的基准数据集，同时关注工具使用和API集成能力，TOOLLLM框架使用了ToolBench和APIBench来评估其工具使用能力，而Toolformer展示了其自主学习使用工具的能力。这些差异主要来自LLMs和基于LLM的智能体在任务处理方法上的不同，LLMs通常通过在相关数据集上微调来优化单一模型的表现。
- en: V-E Evaluation Metrics
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 评估指标
- en: 'Various evaluation metrics are used to assess the performance of LLMs and LLM-based
    agents in code generation and software development. These metrics measures the
    models’ performance in specific tasks and how they improve the code generation
    and software development process. Table [V-E](#S5.SS5 "V-E Evaluation Metrics
    ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D
    Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future")
    includes the distribution of evaluation metrics cited in this paper, encompassing
    both LLMs and LLM-based agents.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '各种评估指标用于评估LLMs和基于LLM的智能体在代码生成和软件开发中的表现。这些指标衡量模型在特定任务中的表现以及它们如何改善代码生成和软件开发过程。表格
    [V-E](#S5.SS5 "V-E Evaluation Metrics ‣ V Code Generation and Software Development
    ‣ IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and
    Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future") 包括了本文引用的评估指标的分布，涵盖了LLMs和基于LLM的智能体。'
- en: In research on LLMs and LLM-based agents, Pass@k is a common evaluation metric
    used to measure the proportion of generated code that passes all test cases within
    the first k attempts, this metric is widely applied across various datasets. In [[86](#bib.bib86)],
    Pass@k was used to evaluate the quality of code generation in multi-turn interactions,
    showing that the model’s Pass@k significantly improved by introducing a planning
    phase. Besides Pass@k, BLEU score is another common evaluation metric, mainly
    used to measure the syntactic similarity and correctness between generated code
    and reference code. In [[73](#bib.bib73)], BLEU score was used to evaluate the
    quality of generated code. Complete Time and Success Rate are other important
    evaluation metrics, particularly when assessing the productivity impact of AI-assisted
    development tools, these metrics are crucial as we expect LLMs to generate accurate
    code while maintaining expected speed. Confidence Calibration and Execution Rate
    are metrics used to evaluate the confidence level and execution success rate of
    the model when generating code. Researchers often use these metrics to assess
    various LLMs’ performance in understanding user intent and generating correct
    code with high precision.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在对LLMs及LLM-based代理的研究中，Pass@k是一个常见的评估指标，用于测量在前k次尝试中，生成的代码通过所有测试用例的比例，该指标广泛应用于各种数据集。在[[86](#bib.bib86)]中，Pass@k用于评估多轮交互中的代码生成质量，结果表明通过引入规划阶段，模型的Pass@k显著提高。除了Pass@k，BLEU分数是另一个常见的评估指标，主要用于测量生成代码与参考代码之间的语法相似性和正确性。在[[73](#bib.bib73)]中，BLEU分数被用于评估生成代码的质量。Complete
    Time和Success Rate是其他重要的评估指标，尤其在评估AI辅助开发工具的生产力影响时，这些指标至关重要，因为我们希望LLMs在保持预期速度的同时生成准确的代码。Confidence
    Calibration和Execution Rate是用于评估模型在生成代码时的信心水平和执行成功率的指标。研究人员经常使用这些指标来评估各种LLMs在理解用户意图和生成高精度正确代码方面的表现。
- en: Compared to the evaluation metrics for LLMs in software development, LLM-based
    agents also use Pass@k but more diverse to reflect their multi-agent collaboration
    characteristics. Win Rate and Agreement Rate are important metrics for evaluating
    the effectiveness of multi-agent collaboration. Additionally, LLM-based agents
    often use metrics like Execution Effectiveness and Cost Efficiency to evaluate
    their performance in real-world applications. For instance, in MetaGPT [[79](#bib.bib79)],
    researchers evaluated not only the correctness of code generation but also analyzed
    the execution effectiveness, development costs, and productivity. Results indicated
    that MetaGPT significantly improved development efficiency and reduced development
    costs while generating high-quality code. Overall both are using traditional metrics
    such as Pass@k, Win Rate, and task completion time to evaluate their code generation
    capabilities, these metrics directly reflect the accuracy and efficiency of the
    model in generating code. But LLM-based agents normally requiring more comprehensive
    and diverse metrics for evaluation to help assess the performance of multiple
    agents and the whole development process, that’s why we can see the human revision
    cost, qualitative feedback in the evaluation metrics. Researchers consider user
    or developer satisfaction metrics, as agent applications often involve extensive
    projects rather than isolated small-scale development, these metrics focus on
    the correctness of code generation and also resource utilization efficiency of
    the agent system.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 与软件开发中LLMs的评估指标相比，LLM-based代理也使用Pass@k，但更多样化以反映它们的多代理协作特征。Win Rate和Agreement
    Rate是评估多代理协作效果的重要指标。此外，LLM-based代理通常使用Execution Effectiveness和Cost Efficiency等指标来评估其在实际应用中的表现。例如，在MetaGPT[[79](#bib.bib79)]中，研究人员不仅评估了代码生成的正确性，还分析了执行效果、开发成本和生产力。结果表明，MetaGPT显著提高了开发效率，降低了开发成本，同时生成了高质量的代码。总体而言，两者都使用传统指标如Pass@k、Win
    Rate和任务完成时间来评估它们的代码生成能力，这些指标直接反映了模型在生成代码方面的准确性和效率。但LLM-based代理通常需要更全面和多样化的指标进行评估，以帮助评估多个代理和整个开发过程的表现，这也是为什么我们会看到人工修订成本、定性反馈等评估指标。研究人员考虑用户或开发者满意度指标，因为代理应用通常涉及大规模项目而非孤立的小规模开发，这些指标关注代码生成的正确性以及代理系统的资源利用效率。
- en: 'TABLE VI: Evaluation Metrics in Code Generation and Software Development'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表VI：代码生成和软件开发中的评估指标
- en: '| Reference Paper | Benchmarks | Evaluation Metrics | Agent |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 基准测试 | 评估指标 | 代理 |'
- en: '|  [[71](#bib.bib71)] | Leetcode problem | Accuracy | No |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  [[71](#bib.bib71)] | Leetcode 问题 | 准确性 | 否 |'
- en: '|  [[72](#bib.bib72)] | HTTP server in JavaScript by |  |  |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|  [[72](#bib.bib72)] | JavaScript 中的 HTTP 服务器 |  |  |'
- en: '| 95 programmer | Task Completion Time, |  |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 95 程序员 | 任务完成时间, |  |  |'
- en: '| Task Success | No |  |  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 任务成功 | 否 |  |  |'
- en: '|  [[68](#bib.bib68)] | Spider, WikiTQ, GSM8k, |  |  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '|  [[68](#bib.bib68)] | Spider, WikiTQ, GSM8k, |  |  |'
- en: '| SVAMP, MBPP, MBPP, DS-1000 | Execution Accuracy, |  |  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| SVAMP, MBPP, MBPP, DS-1000 | 执行准确性, |  |  |'
- en: '| Confidence Calibration |  |  |  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 信心校准 |  |  |  |'
- en: '| Execution Rate | No |  |  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 执行率 | 否 |  |  |'
- en: '|  [[45](#bib.bib45)] | Alpaca Data | Win-Rate, Agreement |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  [[45](#bib.bib45)] | Alpaca 数据 | 胜率, 一致性 |  |'
- en: '| Rate | No |  |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 评分 | 无 |  |  |'
- en: '|  [[86](#bib.bib86)] | HumanEval/-X/-ET, |  |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  [[86](#bib.bib86)] | HumanEval/-X/-ET, |  |  |'
- en: '| MBPP-sanitized/-ET | Pass@k, AvgPassRatio, |  |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| MBPP-sanitized/-ET | Pass@k, 平均通过比率, |  |  |'
- en: '| CodeBLEU | No |  |  |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| CodeBLEU | 否 |  |  |'
- en: '|  [[73](#bib.bib73)] | HumanEval, CodeXGLUE | Pass rate, Exact Match |  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|  [[73](#bib.bib73)] | HumanEval, CodeXGLUE | 通过率, 精确匹配 |  |'
- en: '| BLEU Score | No |  |  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| BLEU 分数 | 否 |  |  |'
- en: '|  [[69](#bib.bib69)] | Spider | Accuracy, |  |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  [[69](#bib.bib69)] | Spider | 准确性, |  |'
- en: '| Exact Match | No |  |  |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 精确匹配 | 否 |  |  |'
- en: '|  [[74](#bib.bib74)] | HumanEval, MTPB | Pass@k, Pass rate | No |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  [[74](#bib.bib74)] | HumanEval, MTPB | Pass@k, 通过率 | 否 |'
- en: '|  [[30](#bib.bib30)] | HumanEval, MathQA-Python, |  |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  [[30](#bib.bib30)] | HumanEval, MathQA-Python, |  |  |'
- en: '| GSM8K-Python, CodeSearchNet, |  |  |  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K-Python, CodeSearchNet, |  |  |  |'
- en: '| CosQA, AdvTest | Pass@k, BLEU-4, |  |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| CosQA, AdvTest | Pass@k, BLEU-4, |  |  |'
- en: '| Exact Matcha |  |  |  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 精确匹配 |  |  |  |'
- en: '| Edit Similarity, |  |  |  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 编辑相似度, |  |  |  |'
- en: '| Mean Reciprocal Rank (MRR) | No |  |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 平均倒数排名 (MRR) | 否 |  |  |'
- en: '|  [[70](#bib.bib70)] | HumanEval/-X | Pass@k | No |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  [[70](#bib.bib70)] | HumanEval/-X | Pass@k | 否 |'
- en: '|  [[67](#bib.bib67)] | HumanEval | Pass@k, BLEU Score | No |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  [[67](#bib.bib67)] | HumanEval | Pass@k, BLEU 分数 | 否 |'
- en: '|  [[75](#bib.bib75)] | HumanEval, MBPP-S, APPS | Pass Rate, Token Edit Distance,
    |  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  [[75](#bib.bib75)] | HumanEval, MBPP-S, APPS | 通过率, Token 编辑距离, |  |'
- en: '|  Exact Copy Rate | No |  |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  精确匹配率 | 否 |  |  |'
- en: '|  [[76](#bib.bib76)] | MBPP/-ET, HumanEval/-ET | Pass@k | Yes |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '|  [[76](#bib.bib76)] | MBPP/-ET, HumanEval/-ET | Pass@k | 是 |'
- en: '|  [[78](#bib.bib78)] | HumanEval | Pass@1 | Yes |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  [[78](#bib.bib78)] | HumanEval | Pass@1 | 是 |'
- en: '|  [[87](#bib.bib87)] | CAASD | Pass Rate, Token Consumption | Yes |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  [[87](#bib.bib87)] | CAASD | 通过率, Token 消耗 | 是 |'
- en: '|  [[84](#bib.bib84)] | CCNet, SQuAD, Google-RE, T-REx, |  |  |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  [[84](#bib.bib84)] | CCNet, SQuAD, Google-RE, T-REx, |  |  |'
- en: '| ASDiv, SVAMP, MAWPS, |  |  |  |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| ASDiv, SVAMP, MAWPS, |  |  |  |'
- en: '| Web Questions, Natural Questions, |  |  |  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 网络问题, 自然问题, |  |  |  |'
- en: '| TriviaQA, MLQA, TEMPLAMA | Zero-shot performance, |  |  |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA, MLQA, TEMPLAMA | 零-shot 性能, |  |  |'
- en: '| Perplexity, Tool usage effectiveness | Yes |  |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 困惑度, 工具使用效果 | 是 |  |  |'
- en: '|  [[82](#bib.bib82)] | MBPP/-ET, HumanEval/-ET | Pass@1 | Yes |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  [[82](#bib.bib82)] | MBPP/-ET, HumanEval/-ET | Pass@1 | 是 |'
- en: '|  [[79](#bib.bib79)] | HumanEval, HumanEval, |  |  |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  [[79](#bib.bib79)] | HumanEval, HumanEval, |  |  |'
- en: '| SoftwareDev | Pass@k, Executability, Cost, |  |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| SoftwareDev | Pass@k, 可执行性, 成本, |  |  |'
- en: '| Code Statistics, Productivity, |  |  |  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 代码统计, 生产力, |  |  |  |'
- en: '| Human Revision Cost | Yes |  |  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 人工修订成本 | 是 |  |  |'
- en: '|  [[81](#bib.bib81)] | HumanEval, MBPP | Pass@k, Practitioner-Based |  |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  [[81](#bib.bib81)] | HumanEval, MBPP | Pass@k, 实践者基础 |  |'
- en: '| Assessment | Yes |  |  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 是 |  |  |'
- en: '|  [[85](#bib.bib85)] | ToolBench, APIBench | Pass Rate, Win Rate | Yes |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  [[85](#bib.bib85)] | ToolBench, APIBench | 通过率, 胜率 | 是 |'
- en: '|  [[80](#bib.bib80)] | No Specificed | Pass Rate, Win Rate | Yes |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  [[80](#bib.bib80)] | 无特定 | 通过率, 胜率 | 是 |'
- en: '|  [[77](#bib.bib77)] | MBPP/-ET, HumanEval/-ET | Pass@1 | Yes |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  [[77](#bib.bib77)] | MBPP/-ET, HumanEval/-ET | Pass@1 | 是 |'
- en: '|  [[83](#bib.bib83)] | HumanEval, MBPP, EvalPlus | Pass@1 | Yes |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  [[83](#bib.bib83)] | HumanEval, MBPP, EvalPlus | Pass@1 | 是 |'
- en: '|  [[88](#bib.bib88)] | First-party data from Meta’s |  |  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  [[88](#bib.bib88)] | Meta 的第一方数据 |  |  |'
- en: '| code repositories and notebooks | Acceptance Rate, P |  |  |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 代码库和笔记本 | 接受率, P |  |  |'
- en: '| ercentage of Code Typed, |  |  |  |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 输入代码百分比, |  |  |  |'
- en: '| Qualitative Feedback | Yes |  |  |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 定性反馈 | 是 |  |  |'
- en: VI Autonomous Learning and Decision Making
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 自主学习与决策
- en: Autonomous Learning and Decision Making is a critical and evolving field in
    modern software engineering, especially under the influence of artificial intelligence
    and big data. The core task of autonomous learning and decision making is to achieve
    automated data analysis, model building, and decision optimization through machine
    learning algorithms and intelligent systems, thereby enhancing the autonomy and
    intelligence of systems.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 自主学习和决策是现代软件工程中一个关键而不断发展的领域，尤其是在人工智能和大数据的影响下。自主学习和决策的核心任务是通过机器学习算法和智能系统实现数据分析、模型构建和决策优化，从而增强系统的自主性和智能性。
- en: In this process, LLMs and LLM-based agents bring numerous possibilities, following
    the development of NLP technology, a lot of achievements have been made in the
    application of LLMs in this field. These models can handle complex language tasks
    and also demonstrate powerful reasoning and decision-making abilities, the research
    on voting inference using multiple LLMs calls has revealed new methods for optimizing
    performance, with the frequently used method called majority vote [[89](#bib.bib89)],
    this improves the accuracy of inference systems and ensures the selection of the
    optimal possibility. Additionally, the performance of LLMs in tasks such as automated
    debugging and self-correction has enhanced the system’s autonomous learning capabilities,
    achieving efficient error identification and correction. At the same time, the
    application of LLM-based agents in autonomous learning and decision-making is
    also a novel but popular topic, these agents can perform complex reasoning and
    decision-making tasks with the help from the LLM, and also improve their adaptability
    in dynamic environments through continuous learning and optimization. In this
    context, we have collected nineteen research papers on LLM-based agents in this
    field. This survey will provides a general review of these studies, analyzing
    the specific applications and technical implementations in autonomous learning
    and decision making.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一过程中，LLMs和基于LLM的智能体带来了众多可能性，随着NLP技术的发展，这一领域的LLMs应用取得了许多成就。这些模型可以处理复杂的语言任务，并且展示了强大的推理和决策能力，使用多个LLM调用的投票推理研究揭示了优化性能的新方法，其中一种常用的方法是多数投票[[89](#bib.bib89)]，这提高了推理系统的准确性，并确保选择最佳可能性。此外，LLMs在自动调试和自我修正等任务中的表现增强了系统的自主学习能力，实现了高效的错误识别和修正。同时，基于LLM的智能体在自主学习和决策中的应用也是一个新颖但受欢迎的话题，这些智能体可以借助LLM执行复杂的推理和决策任务，并通过持续学习和优化提高其在动态环境中的适应性。在这种背景下，我们收集了十九篇关于该领域LLM基于智能体的研究论文。本次调查将对这些研究进行总体回顾，分析在自主学习和决策中的具体应用和技术实现。
- en: VI-A LLMs Tasks
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A LLMs任务
- en: The API call for the LLMs is one common applications, often requiring continuous
    calls to enable the model to make judgments and inferences, but does continuously
    increasing the number of calls always improve performance? In [[90](#bib.bib90)],
    researchers explored the impact of increasing LLM calls on the performance of
    composite reasoning systems. Paper analyze the voting inference design systems,
    the result showed that there is a non-linear relationship between the number of
    LLM calls and system performance; performance improves initially with more calls
    but declines after reaching a certain threshold. This research provides a theoretical
    basis for optimizing LLM calls, helping to allocate resources reasonably in practical
    applications to achieve optimal performance. However, the performance of Voting
    Inference Systems shows a non-monotonic trend due to the diversity of query difficulties,
    and the continuously increasing cost also needs to be considered.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLMs的API调用是一个常见的应用，通常需要持续调用以使模型能够进行判断和推理，但是，持续增加调用次数是否总能提高性能？在[[90](#bib.bib90)]中，研究人员探讨了增加LLM调用次数对复合推理系统性能的影响。论文分析了投票推理设计系统，结果显示LLM调用次数与系统性能之间存在非线性关系；性能在初期随着调用次数的增加而提高，但在达到某个阈值后会下降。这项研究为优化LLM调用提供了理论基础，有助于在实际应用中合理配置资源，以实现最佳性能。然而，投票推理系统的性能显示出非单调的趋势，由于查询难度的多样性，持续增加的成本也需要考虑。
- en: 'Autonomous learning is also applied in bug fixing, where researchers hope LLMs
    can continuously learn to fix bugs and eventually identify human oversights or
    common errors. In [[91](#bib.bib91)], the SELF-DEBUGGING method was proposed,
    enabling LLMs to debug code by analyzing execution results and natural language
    explanations. This method significantly improved the accuracy and sample efficiency
    of code generation tasks especially for complex problems. Experimental results
    on the Spider and TransCoder benchmarks showed that the SELF-DEBUGGING method
    increase the model’s accuracy by 2-12% which demonstrates the potential of LLMs
    in autonomous learning to debug and correct any erros. Another similar study introduced
    the AutoSD (Automated Scientific Debugging) technique [[92](#bib.bib92)], which
    simulates the scientific debugging process through LLMs, generating explainable
    patched code. Researchers evaluated AutoSD’s capabilities from six aspects: feasibility,
    debugger ablation, language model change, developer benefit, developer acceptance,
    and qualitative analysis. Result have shown that AutoSD can generate effective
    patches and also improve developers’ accuracy in evaluating patched code by providing
    explanations, its explainability function makes it easier for developers to understand
    and accept automatically generated patches. Although the above two studies primarily
    focus on automated debugging techniques, the frameworks designed in these studies
    automatically determine the optimal repair solution based on the debugging results
    after collecting sufficient information, and provide specific code implementations,
    which demonstrated the capability of autonomous decision-making and learning.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 自主学习同样应用于错误修复领域，研究人员希望LLMs能够持续学习修复错误，并最终识别出人工疏忽或常见错误。在[[91](#bib.bib91)]中，提出了SELF-DEBUGGING方法，使LLMs能够通过分析执行结果和自然语言解释来调试代码。这种方法显著提高了代码生成任务的准确性和样本效率，尤其是在复杂问题上。Spider和TransCoder基准测试的实验结果显示，SELF-DEBUGGING方法将模型的准确率提高了2-12%，这表明LLMs在自主学习以调试和纠正错误方面的潜力。另一项类似研究介绍了AutoSD（自动化科学调试）技术[[92](#bib.bib92)]，该技术通过LLMs模拟科学调试过程，生成可解释的修补代码。研究人员从可行性、调试器消融、语言模型变化、开发者受益、开发者接受度和定性分析六个方面评估了AutoSD的能力。结果表明，AutoSD能够生成有效的补丁，并通过提供解释提高开发者评估修补代码的准确性，其解释功能使开发者更容易理解和接受自动生成的补丁。尽管上述两个研究主要关注自动化调试技术，但这些研究中设计的框架根据调试结果自动确定最佳修复方案，并提供具体的代码实现，展示了自主决策和学习的能力。
- en: Since the rise of LLMs applied to various fields, one research direction has
    been the rational analysis of their creativity and the exploration of their potential
    for continuous learning, this creativity also highly determined by the decision
    making capability of the models. [[93](#bib.bib93)] analyzed the outputs of LLMs
    from the perspective of creativity theory, exploring their ability to generate
    creative content, the study used metrics such as value, novelty, and surprise,
    finding that current LLMs have limitations in generating combinatorial, exploratory,
    and transformative creativity. Although LLMs can generate high-quality creative
    content, further research and improvement are needed to achieve true creative
    breakthroughs. Additionally, innovative responses generated by LLMs may come with
    the possibility of hallucination, a long-standing issue for large language models.
    Despite many techniques to mitigate its downsides, it still cannot be entirely
    prevented. There are many interesting experiments in decision making, such as
    having LLMs act as judges to determine whether a person has committed a crime[[94](#bib.bib94)].
    A familiar attempt is to have a primary LLM interact with other LLMs. [[95](#bib.bib95)]
    explored the effectiveness of using LLMs as judges to evaluate other LLM-driven
    chat assistants. The study validated the consistency of LLM judgements with human
    preferences through the MT-Bench and Chatbot Arena benchmarks, with results showing
    that GPT-4’s judgments were highly consistent with human judgments across various
    tasks. This research demonstrates the potential of LLMs in simulating human evaluation,
    providing new ideas for automated evaluation and optimization.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 自从LLM应用于各个领域以来，一个研究方向是对其创造力的理性分析以及探索其持续学习的潜力，这种创造力也在很大程度上由模型的决策能力决定。[ [93](#bib.bib93)]
    从创造力理论的角度分析了LLM的输出，探讨了其生成创造性内容的能力，研究使用了价值、新颖性和惊奇等指标，发现当前的LLM在生成组合性、探索性和变革性创造力方面存在局限性。虽然LLM可以生成高质量的创造性内容，但仍需要进一步研究和改进以实现真正的创造性突破。此外，LLM生成的创新响应可能伴随幻觉的可能性，这是大型语言模型的一个长期问题。尽管有许多技术来减轻其负面影响，但仍然无法完全避免。在决策中有许多有趣的实验，例如让LLM充当法官来判断一个人是否犯了罪[[94](#bib.bib94)]。一个熟悉的尝试是让一个主LLM与其他LLM互动。[
    [95](#bib.bib95)] 探讨了使用LLM作为法官评估其他LLM驱动的聊天助手的有效性。研究通过MT-Bench和Chatbot Arena基准验证了LLM判断与人类偏好的高度一致性，结果显示GPT-4的判断在各种任务中与人类判断高度一致。这项研究展示了LLM在模拟人类评估方面的潜力，为自动化评估和优化提供了新的思路。
- en: VI-B LLM-based Agents Tasks
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 基于LLM的代理任务
- en: Multi-agent collaboration and dialogue frameworks also demonstrated strong capabilities
    in both decision making and autonomous learning. [[96](#bib.bib96)] explores whether
    multi-agent discussions can enhance the reasoning abilities of LLMs. The proposed
    CMD framework simulates human group discussion processes, showing that multi-agent
    discussions can improve performance in commonsense knowledge and mathematical
    reasoning tasks without task-specific examples. Additionally, the study found
    that multi-agent discussions also correct common errors in single agents, such
    as judgment errors and the propagation of incorrect answers, thereby enhancing
    overall reasoning accuracy.[[97](#bib.bib97)] researchers explored the potential
    of multi-modal large language models (MLLMs) like GPT4-Vision in enhancing agents’
    autonomous decision-making processes. The paper introduce the PCA-EVAL benchmark,
    and evaluated multi-modal decision-making capabilities in areas such as autonomous
    driving, home assistants, and gaming. The results showed that GPT4-Vision exhibiting
    outstanding performance across the dimensions of perception, cognition, and action.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理协作和对话框架在决策和自主学习方面也展示了强大的能力。[[96](#bib.bib96)] 探讨了多代理讨论是否能增强LLM的推理能力。提出的CMD框架模拟了人类群体讨论过程，显示多代理讨论可以在没有任务特定示例的情况下提升常识知识和数学推理任务的表现。此外，研究发现多代理讨论还可以纠正单一代理中的常见错误，如判断错误和错误答案的传播，从而提高整体推理准确性。[
    [97](#bib.bib97)] 研究人员探索了像GPT4-Vision这样的多模态大型语言模型（MLLMs）在提升代理自主决策过程中的潜力。论文介绍了PCA-EVAL基准，并评估了在自主驾驶、家庭助手和游戏等领域的多模态决策能力。结果显示，GPT4-Vision在感知、认知和行动方面表现出色。
- en: '[[98](#bib.bib98)] proposes the Reflexion framework, a novel approach that
    strengthens learning through language feedback rather than traditional weight
    updates to avoid expensive re-train costs. The framework uses self-reflection
    and language feedback to help language agents learn from mistakes, significantly
    improving performance in decision-making, reasoning, and programming tasks. The
    Reflexion’s first-pass success rate on the HumanEval Python programming task increased
    from 80.1% to 91.0%, success rates in the ALFWorld decision-making task improved
    by 22%, and performance in the HotPotQA reasoning task increased by 14%. These
    results indicate that the Reflexion framework demonstrate the state-of-art performance
    in various tasks through self-reflection and language feedback.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[[98](#bib.bib98)] 提出了 Reflexion 框架，这是一种新颖的方法，通过语言反馈来强化学习，而不是传统的权重更新，从而避免昂贵的再训练成本。该框架利用自我反思和语言反馈，帮助语言代理从错误中学习，显著提升决策、推理和编程任务的性能。Reflexion
    在 HumanEval Python 编程任务中的首轮成功率从 80.1% 提升到 91.0%，在 ALFWorld 决策任务中的成功率提高了 22%，而在
    HotPotQA 推理任务中的性能提高了 14%。这些结果表明，Reflexion 框架通过自我反思和语言反馈展示了在各种任务中的最先进性能。'
- en: 'Another agent framework [[35](#bib.bib35)] introduces the ExpeL agent framework
    which enhances decision-making capabilities by autonomously collecting experiences
    and extracting knowledge from a series of training tasks using natural language,
    this experience collection process is similar to how humans gain insights through
    practice and apply them in exams. By accessing internal databases, ExpeL also
    reduces hallucinations, employing the RAG technique discussed in [III](#S3 "III
    Preliminaries ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future"). The ExpeL framework doesn’t require parameter
    updates it enhances decision-making capabilities by recalling past successes and
    failures which fully leveraging the advantages of ReAct framework [[36](#bib.bib36)].
    Experimental showed that ExpeL can continuous improvement across tasks in multiple
    domains and exhibited cross-task transfer learning capabilities. The combination
    of ExpeL and Reflexion even further enhance the performance in iterative task
    attempts, highlighting the importance of autonomous learning and experiential
    accumulation in developing intelligent agents. The ExpeL framework demonstrates
    its potential as a state-of-the-art (SOTA) LLM-based agents in several aspects,
    particularly in cross-task learning, self-improvement, and memory mechanisms.
    By comparing ExpeL with existing SOTA agents like Reflexion [[98](#bib.bib98)],
    ExpeL outperforms baseline methods in various task environments. These studies
    collectively indicate the importance of autonomous learning and improvement in
    LLM-based agents, agent systems continuously optimize and improve decision-making
    processes through self-feedback, self-reflection, and experiential accumulation
    which shows higher autonomy and flexibility in handling dynamic and complex tasks
    compared to traditional LLMs. Unlike traditional LLMs, which mainly rely on pre-training
    data and parameter updates, LLM-based agents adapt and improve their performance
    in real-time through continuous self-learning and feedback mechanisms, thus demonstrating
    outstanding performance in various tasks.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个代理框架 [[35](#bib.bib35)] 介绍了 ExpeL 代理框架，通过自主收集经验和利用自然语言从一系列训练任务中提取知识，增强了决策能力，这一经验收集过程类似于人类通过实践获得洞察并在考试中应用它们。通过访问内部数据库，ExpeL
    还减少了幻觉现象，采用了在 [III](#S3 "III Preliminaries ‣ From LLMs to LLM-based Agents for
    Software Engineering: A Survey of Current, Challenges and Future") 中讨论的 RAG 技术。ExpeL
    框架不需要参数更新，通过回忆过去的成功与失败，利用 ReAct 框架 [[36](#bib.bib36)] 的优势，提升了决策能力。实验表明，ExpeL 可以在多个领域的任务中持续改进，并展现了跨任务迁移学习能力。ExpeL
    和 Reflexion 的结合进一步提升了迭代任务尝试中的表现，突显了自主学习和经验积累在开发智能代理中的重要性。ExpeL 框架在多个方面展现了作为最先进
    (SOTA) LLM 代理的潜力，特别是在跨任务学习、自我改进和记忆机制方面。通过将 ExpeL 与现有的 SOTA 代理如 Reflexion [[98](#bib.bib98)]
    进行比较，ExpeL 在各种任务环境中超越了基准方法。这些研究共同表明，自主学习和改进在 LLM 代理中的重要性，代理系统通过自我反馈、自我反思和经验积累持续优化和改进决策过程，表现出比传统
    LLM 更高的自主性和灵活性。与主要依赖预训练数据和参数更新的传统 LLM 不同，LLM 代理通过持续自我学习和反馈机制实时适应和提高性能，从而在各种任务中表现出色。'
- en: '[[99](#bib.bib99)] proposes the AGENTVERSE multi-agent framework, designed
    to improve task completion efficiency and effectiveness through collaboration.
    The framework draws on human group dynamics by designing a collaborative system
    of expert agents that exhibit outstanding performance in tasks such as text understanding,
    reasoning, coding, and tool usage. Experiments showed that the AGENTVERSE framework
    performed well not only in independent task completion but also significantly
    improved performance through group collaboration especially in coding tasks where
    the framework use GPT-4 to be the brain of the agent groups. The framework also
    observed emergent behaviors in agents during collaboration, such as voluntary
    actions, conformity, and destructive behaviors, providing valuable insights for
    understanding and optimizing multi-agent systems.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '[[99](#bib.bib99)]提出了AGENTVERSE多智能体框架，旨在通过协作提高任务完成效率和效果。该框架借鉴了人类群体动态，设计了一个专家智能体的协作系统，这些智能体在文本理解、推理、编码和工具使用等任务中表现出色。实验显示，AGENTVERSE框架不仅在独立任务完成中表现良好，还通过群体协作显著提升了性能，尤其是在编码任务中，该框架使用GPT-4作为智能体组的“大脑”。该框架还观察到了智能体在协作过程中出现的突现行为，如自发行为、从众行为和破坏行为，为理解和优化多智能体系统提供了宝贵的见解。'
- en: Another multi-agents study  [[100](#bib.bib100)] Introducing the CAMEL framework,
    this is a well known agent framework, which explores building scalable techniques
    to facilitate autonomous collaborative agent frameworks. The study proposes a
    role-playing collaborative agent framework that guides dialogue agents to complete
    tasks through embedded prompts while maintaining alignment with human intentions.
    The CAMEL framework generates dialogue data to study behaviors and capabilities
    within the agent society, the study further enhanced agent performance by fine-tuning
    the LLaMA-7B model, validating the effectiveness of generated datasets in enhancing
    LLM capabilities.[[101](#bib.bib101)] investigates the comprehensive comparison
    of LLM-augmented autonomous agents and proposes a new multi-agent coordination
    strategy for solving complex tasks through efficient communication and coordination
    called BOLAA. The experiment showed that the BOLAA outperforms other agent architectures
    in the WebShop environment especially in high-performance LLMs The above three
    studies focus on achieving a multi-agent collaboration architecture by increasing
    the number of agents. This trend indicates that more frameworks are beginning
    to explore the potential of multi-agent systems.[[44](#bib.bib44)] explores methods
    to enhance LLMs performance by increasing the number of agents. Using sampling
    and voting methods, the study showed that as the number of agents increased, LLM
    performance in arithmetic reasoning, general reasoning, and code generation tasks
    improved significantly. This method proves the effectiveness of multi-agent collaboration
    in enhancing model performance. These studies collectively indicate the importance
    of multi-agent collaboration and dialogue frameworks in autonomous learning and
    decision-making tasks. Compared to traditional LLMs, these multi-agent frameworks
    enhance reasoning accuracy under zero-shot learning and demonstrate higher autonomy
    and flexibility which reduce the burden on developers.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项多智能体研究[[100](#bib.bib100)]介绍了CAMEL框架，这是一个著名的智能体框架，旨在探索构建可扩展技术以促进自主协作智能体框架。该研究提出了一个角色扮演协作智能体框架，通过嵌入式提示指导对话智能体完成任务，同时保持与人类意图的一致性。CAMEL框架生成对话数据以研究智能体社会中的行为和能力，该研究通过微调LLaMA-7B模型进一步提升了智能体的性能，验证了生成数据集在提升LLM能力方面的有效性。[[101](#bib.bib101)]调查了LLM增强的自主智能体的全面比较，并提出了一种新的多智能体协调策略，用于通过高效的沟通和协调解决复杂任务，称为BOLAA。实验显示，BOLAA在WebShop环境中表现优于其他智能体架构，尤其是在高性能LLM中。以上三项研究集中于通过增加智能体数量来实现多智能体协作架构。这一趋势表明，越来越多的框架开始探索多智能体系统的潜力。[[44](#bib.bib44)]探讨了通过增加智能体数量来增强LLM性能的方法。使用抽样和投票方法，研究表明随着智能体数量的增加，LLM在算术推理、一般推理和代码生成任务中的表现显著提升。这种方法证明了多智能体协作在提升模型性能方面的有效性。这些研究共同表明了多智能体协作和对话框架在自主学习和决策任务中的重要性。与传统的LLM相比，这些多智能体框架在零样本学习下提高了推理准确性，并展现出更高的自主性和灵活性，从而减少了开发者的负担。
- en: LLM-based agents not only perform complex data analysis tasks but also demonstrate
    potential in simulating and understanding human trust behaviors. [[102](#bib.bib102)]
    introduces a framework named SELF, designed to achieve self-evolution of LLMs
    through language feedback, use RLHF to train agent behavior to meet the human
    alignment. The framework enhances model capabilities through iterative processes
    of self-feedback and self-improvement without human intervention. In experiments,
    the test accuracy on GSM8K and SVAMP datasets increasing by 6.82% and 4.9%, respectively
    and the overall task win rates on the Vicuna test set and Evol-Instruct test set
    also increased by 10% and 6.9%. Another similar study exploring the potential
    of LLM-based agents to simulate the human trust behaviors.[[103](#bib.bib103)]
    also examines whether LLM-based agents can simulate human trust behaviors. The
    study aims to determine if LLM-based agents exhibit trust behaviors similar to
    humans and explore whether these behaviors can align with human trust. Through
    a series of trust game variants such as initial fund allocation and return trust
    games, the research analyzes LLM-based agents’ trust decisions and behaviors in
    different contexts. Results show that particularly for GPT-4, LLM-based agents
    exhibit trust behaviors consistent with human expectations in these trust games,
    validating the potential of LLM-based agents in simulating human trust behaviors.
    The efficient and accurate handling of diverse datasets highlights the broad application
    prospects in fields such as software engineering. In terms of simulating trust
    behaviors, LLM-based agents demonstrate human-like behavior patterns through complex
    trust decisions and behavior analysis providing an important theoretical foundation
    for future human-machine collaboration and human behavior simulation.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理不仅执行复杂的数据分析任务，还展示了在模拟和理解人类信任行为方面的潜力。[[102](#bib.bib102)] 引入了一个名为SELF的框架，该框架旨在通过语言反馈实现LLM的自我进化，使用RLHF训练代理行为以符合人类对齐。该框架通过自我反馈和自我改进的迭代过程来增强模型能力，无需人工干预。在实验中，GSM8K和SVAMP数据集上的测试准确率分别提高了6.82%和4.9%，而Vicuna测试集和Evol-Instruct测试集上的整体任务胜率也分别增加了10%和6.9%。另一项类似的研究探讨了基于LLM的代理模拟人类信任行为的潜力。[[103](#bib.bib103)]
    还考察了基于LLM的代理是否能够模拟人类信任行为。该研究旨在确定基于LLM的代理是否展现出类似于人类的信任行为，并探讨这些行为是否能够与人类信任对齐。通过一系列信任游戏变体，如初始资金分配和回报信任游戏，研究分析了基于LLM的代理在不同背景下的信任决策和行为。结果表明，特别是对于GPT-4，基于LLM的代理在这些信任游戏中展现出的信任行为与人类期望一致，验证了基于LLM的代理在模拟人类信任行为方面的潜力。高效而准确地处理各种数据集突出了其在软件工程等领域的广泛应用前景。在模拟信任行为方面，基于LLM的代理通过复杂的信任决策和行为分析展示了类似人类的行为模式，为未来的人机协作和人类行为模拟提供了重要的理论基础。
- en: Integrating LLMs into agents allows for more complex task processing. [[104](#bib.bib104)]
    proposes a lightweight user-friendly library named AgentLite whic designed to
    simplify the development, prototyping and evaluation of task-oriented LLM-based
    agents systems. The main goal of the study is to enhance the capabilities and
    flexibility of LLM-based agents in various applications by introducing a flexible
    framework. This framework enhances task-solving capabilities through task decomposition
    and multi-agent coordination, using a hierarchical multi-agent coordination approach
    where a managing agent supervises the task execution of each agent.[[105](#bib.bib105)]
    introduces a framework, GPTSwarm, that represents LLM-based agents as computational
    graphs to unify existing prompt engineering techniques and introduces methods
    for optimizing these graphs to enhance agent performance. The study verifies the
    effectiveness of the framework through various benchmarks such as MMLU, Mini Crosswords,
    and HumanEval. The framework demonstrated significant performance improvements
    on the GAIA benchmark with an improvement margin of up to 90.2% compared to the
    best existing methods. Additionally, agents have shown strong capabilities in
    autonomous learning and decision-making in software engineering and security,
    which will be introduced in the subsequent Software Security section[[106](#bib.bib106)] [[107](#bib.bib107)] [[108](#bib.bib108)] [[109](#bib.bib109)].
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLMs（大型语言模型）集成到代理中可以处理更复杂的任务。[[104](#bib.bib104)] 提出了一个名为AgentLite的轻量级用户友好型库，旨在简化任务导向的LLM基础代理系统的开发、原型制作和评估。该研究的主要目标是通过引入灵活的框架来增强LLM基础代理在各种应用中的能力和灵活性。这个框架通过任务分解和多代理协调来增强任务解决能力，采用一种分层的多代理协调方法，其中一个管理代理监督每个代理的任务执行。[
    [105](#bib.bib105)] 介绍了一个框架GPTSwarm，它将LLM基础代理表示为计算图，以统一现有的提示工程技术，并引入了优化这些图的方法以提升代理性能。研究通过各种基准测试验证了该框架的有效性，如MMLU、Mini
    Crosswords和HumanEval。该框架在GAIA基准测试中表现出了显著的性能提升，与现有最佳方法相比，改进幅度高达90.2%。此外，代理在软件工程和安全领域表现出了强大的自主学习和决策能力，这将在后续的软件安全部分中介绍[[106](#bib.bib106)]
    [[107](#bib.bib107)] [[108](#bib.bib108)] [[109](#bib.bib109)]。
- en: VI-C Analysis
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 分析
- en: Overall, LLMs and LLM-based agents exhibit strong capability on the autonomous
    learning and decision making but slightly different view. These differences are
    reflected in the focus of task execution and also in autonomy, interactivity,
    learning and adaptation mechanisms, and the integration with other systems and
    modalities. From the perspective of task execution focus, LLMs primarily concentrate
    on enhancing specific functions in software engineering, such as debugging, problem-solving
    and automated reasoning. The tasks they perform are usually static and well-defined,
    such as automatic debugging, enhancing debugging capabilities to autonomously
    identify and correct errors, evaluating creativity and judging responses from
    other chatbots. In contrast, LLM-based agents not only focus on specific tasks
    but also manage multiple tasks simultaneously, often involving dynamic decision-making
    and interaction with other agents or systems. Examples of these agents’ tasks
    include enhancing reasoning through multi-agent discussions, continuous learning
    from experiences, requiring real-time dynamic decision-making, and also LLM-based
    agents can get in touch with the multimodal task in the visual environment.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LLMs和LLM基础代理在自主学习和决策方面表现出强大的能力，但视角略有不同。这些差异体现在任务执行的重点以及自主性、交互性、学习和适应机制，以及与其他系统和模式的集成。从任务执行重点的角度来看，LLMs主要集中在增强软件工程中的特定功能，如调试、问题解决和自动推理。它们执行的任务通常是静态且定义明确的，如自动调试、提升调试能力以自主识别和纠正错误、评估创造力以及判断其他聊天机器人的回应。相比之下，LLM基础代理不仅关注特定任务，还同时管理多个任务，通常涉及动态决策和与其他代理或系统的互动。这些代理任务的例子包括通过多代理讨论增强推理、从经验中持续学习、需要实时动态决策，以及LLM基础代理还可以接触到视觉环境中的多模态任务。
- en: We can conclude that, the application of LLM-based agents in the topic of autonomous
    learning and decision-making primarily involves exploring their performance in
    specific tasks through various framework designs. These studies evaluate the agents’
    autonomy and decision-making capabilities to determine whether they align with
    human behavior and decision-making processes. If we dive into the specific task
    deigns, in terms of autonomy and interactivity LLMs are usually designed to perform
    highly specific tasks without needing to adapt to external input or environmental
    changes, they mainly operate as single models focusing on processing and responding
    within predefined boundaries, this also applied to all LLM applications. On the
    other hand, LLM-based agents exhibit higher autonomy which are typically designed
    to interact with or adapt to the environment in real-time, they are often part
    of multi-agent systems where collaboration and communication are key components,
    for example use extra model or tools to further help with the planning phases.
    In terms of integration with other systems and modalities, LLMs typically operate
    in text input-output scenarios and even in multi-modal settings, their role is
    usually limited to processing and generating text-based content. Also, LLM-based
    agents are more likely to integrate with other systems and modalities such as
    visual input or real-world perception data, enabling them to perform more complex
    and context-based decision-making tasks.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，LLM（大规模语言模型）基础的智能体在自主学习和决策主题中的应用主要涉及通过各种框架设计探索其在特定任务中的表现。这些研究评估了智能体的自主性和决策能力，以确定它们是否与人类行为和决策过程一致。如果我们深入探讨具体任务设计，从自主性和互动性角度来看，LLM
    通常被设计为执行高度特定的任务，而无需适应外部输入或环境变化，它们主要作为单一模型运作，专注于在预定义的边界内进行处理和响应，这一点适用于所有LLM应用。另一方面，基于LLM的智能体表现出更高的自主性，它们通常被设计为实时与环境互动或适应，它们往往是多智能体系统的一部分，其中协作和通信是关键组件，例如使用额外的模型或工具进一步帮助规划阶段。在与其他系统和模态的集成方面，LLM通常在文本输入输出场景中运作，即使在多模态设置中，它们的角色通常也仅限于处理和生成基于文本的内容。同时，基于LLM的智能体更可能与其他系统和模态如视觉输入或现实世界感知数据集成，使它们能够执行更复杂和基于上下文的决策任务。
- en: '![Refer to caption](img/f606d99981f0cf17ed91629c0deab9a9.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f606d99981f0cf17ed91629c0deab9a9.png)'
- en: 'Figure 6: Expel[[35](#bib.bib35)] Framework with Reflexion[[98](#bib.bib98)]
    in Experience Gathering'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在经验收集中的Expel[[35](#bib.bib35)]框架与Reflexion[[98](#bib.bib98)]
- en: 'Regarding learning and adaptation mechanisms, LLMs’ adaptation and learning
    are usually confined to the model’s training data and parameter range, although
    they can adapt through new data updates, they lack the ability to continuously
    learn from real-time feedback, they are more focused on using existing knowledge
    to solve problems and generate responses. In contrast, LLM-based agents are often
    equipped with experiential learning and real-time feedback adaptation mechanisms,
    allowing them to optimize strategies and responses based on continuous interactions.
    One good example of LLm-based agents framework is Expel [[35](#bib.bib35)], which
    utilize the previous researches ReAct [[36](#bib.bib36)] and Reflexion[[98](#bib.bib98)]
    as shown in Figure.[6](#S6.F6 "Figure 6 ‣ VI-C Analysis ‣ VI Autonomous Learning
    and Decision Making ‣ V-E Evaluation Metrics ‣ V Code Generation and Software
    Development ‣ IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering
    and and Documentation ‣ From LLMs to LLM-based Agents for Software Engineering:
    A Survey of Current, Challenges and Future"). This framework utilizes a memory
    pool and insights pool to enable the LLM to learn from past knowledge, thereby
    aiding subsequent decision-making. This autonomous decision-making capability
    is something that traditional LLM frameworks cannot achieve.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '关于学习和适应机制，LLMs的适应和学习通常局限于模型的训练数据和参数范围，尽管它们可以通过新数据更新来适应，但缺乏从实时反馈中持续学习的能力，它们更专注于利用现有知识解决问题并生成响应。相比之下，基于LLM的智能体通常配备了经验学习和实时反馈适应机制，使它们能够基于持续互动优化策略和响应。一个很好的LLM基于智能体框架的例子是Expel
    [[35](#bib.bib35)]，它利用了之前的研究ReAct [[36](#bib.bib36)] 和Reflexion[[98](#bib.bib98)]，如图所示。[6](#S6.F6
    "Figure 6 ‣ VI-C Analysis ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")。该框架利用记忆池和见解池，使LLM能够从过去的知识中学习，从而帮助后续决策。这种自主决策能力是传统LLM框架无法实现的。'
- en: VI-D Benchmarks
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 基准
- en: 'In the field of autonomous learning and decision-making, the benchmark datasets
    used by LLMs and LLM-based agents are quite similarly in the task handling and
    application requirements. We can gain a deeper understanding of the strengths
    and weaknesses of both approaches in different tasks and their application contexts.
    The specific dataset references, please see Table [VII](#S6.T7 "TABLE VII ‣ VI-E
    Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future").'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '在自主学习和决策领域，LLMs和基于LLM的智能体使用的基准数据集在任务处理和应用要求方面相似。我们可以更深入地了解两种方法在不同任务及其应用背景下的优缺点。具体的数据集参考，请见表[VII](#S6.T7
    "TABLE VII ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making
    ‣ V-E Evaluation Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation
    Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣
    From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future")。'
- en: In the research on LLMs, the main datasets include Defects4J, MMLU, TransCoder,
    and MBPP. These datasets are primarily used to evaluate model performance in specific
    domains and tasks. Defects4J is a widely used in the software engineering, this
    software defect dataset containing 525 real defects from 17 Java projects. It’s
    designed to test the effectiveness of automated program repair and defect detection
    tools by providing a standardized benchmark that allows researchers to compare
    the performance of different methods. MMLU (Massive Multitask Language Understanding)
    is a large-scale benchmark dataset covering 57 subjects, testing models on a broad
    spectrum of knowledge and reasoning abilities in multitask language understanding.
    It includes questions ranging from elementary education to professional level
    such as College Mathematics, Business Ethics, and College Chemistry, challenging
    the models’ diverse knowledge base and reasoning capabilities. The TransCoder
    dataset focuses on code translation across programming languages which evaluate
    the model’s ability to automatically translate code from one programming language
    to another. This is crucial for multilingual software development and maintenance,
    as it can greatly enhance development efficiency. MBPP (Mostly Basic Python Programming)
    has been introduced in previous section, it’s a dataset containing 427 Python
    programming problems, covering basic concepts and standard library functions,
    it’s widely used to test the model’s performance in different programming scenarios,
    evaluating its ability to generate correct and efficient code.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在对大规模语言模型（LLMs）的研究中，主要的数据集包括 Defects4J、MMLU、TransCoder 和 MBPP。这些数据集主要用于评估模型在特定领域和任务中的性能。Defects4J
    是一个广泛使用的软件工程数据集，包含来自 17 个 Java 项目的 525 个真实缺陷。它旨在通过提供一个标准化的基准，测试自动程序修复和缺陷检测工具的有效性，从而让研究人员能够比较不同方法的性能。MMLU（大规模多任务语言理解）是一个大规模基准数据集，涵盖
    57 个学科，测试模型在多任务语言理解中对知识和推理能力的广泛范围。它包括从基础教育到专业水平的问题，如大学数学、商业伦理和大学化学，挑战模型的多样化知识基础和推理能力。TransCoder
    数据集专注于跨编程语言的代码翻译，评估模型自动将代码从一种编程语言翻译成另一种编程语言的能力。这对于多语言软件开发和维护至关重要，因为它可以极大地提高开发效率。MBPP（主要是基础
    Python 编程）在前面的部分已经介绍过，它是一个包含 427 道 Python 编程问题的数据集，涵盖基础概念和标准库函数，广泛用于测试模型在不同编程场景中的表现，评估其生成正确和高效代码的能力。
- en: In contrast, LLM-based agents use datasets that emphasize multitasking and decision-making
    capabilities in complex scenarios. The main datasets include HotpotQA, ALFWorld,
    FEVER, WebShop, and MGSM. HotpotQA is a multi-hop question-answering dataset that
    requires models to reference content from multiple documents when answering questions,
    evaluating their information synthesis and reasoning abilities, this dataset challenges
    the model’s performance in complex reasoning tasks. ALFWorld is a text-based environment
    simulation dataset requiring multi-step decision-making where the model completes
    tasks in a virtual home environment. The dataset combines natural language processing
    and decision-making, evaluating the model’s performance in dynamic and interactive
    tasks. The FEVER (Fact Extraction and VERification) dataset is used for fact verification
    tasks, where the model needs to verify the truthfulness of given statements and
    provide evidence, it assesses the model’s capabilities in information retrieval
    and logical reasoning. WebShop is an online shopping environment simulation dataset
    containing 1.18 million real-world products and human instructions, it used to
    test the model’s performance in complex decision-making tasks such as completing
    shopping tasks and attribute matching. MGSM (Multimodal Generalized Sequence Modeling)
    is a multimodal dataset containing tasks related to dialogue, creative writing,
    mathematical reasoning, and logical reasoning, evaluating the model’s comprehensive
    abilities in multimodal tasks.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于LLM的智能体使用强调多任务和复杂场景决策能力的数据集。主要数据集包括 HotpotQA、ALFWorld、FEVER、WebShop 和
    MGSM。HotpotQA 是一个多跳问答数据集，要求模型在回答问题时参考多个文档的内容，以评估其信息综合和推理能力，这个数据集挑战了模型在复杂推理任务中的表现。ALFWorld
    是一个基于文本的环境模拟数据集，需要进行多步决策，模型在虚拟家庭环境中完成任务。该数据集结合了自然语言处理和决策制定，评估模型在动态和互动任务中的表现。FEVER（事实提取与验证）数据集用于事实验证任务，模型需要验证给定陈述的真实性并提供证据，它评估了模型在信息检索和逻辑推理方面的能力。WebShop
    是一个在线购物环境模拟数据集，包含 118 万个现实世界的产品和人类指令，用于测试模型在完成购物任务和属性匹配等复杂决策任务中的表现。MGSM（多模态广义序列建模）是一个多模态数据集，包含与对话、创意写作、数学推理和逻辑推理相关的任务，评估模型在多模态任务中的综合能力。
- en: 'Comparatively, LLM datasets typically focus on single, static tasks such as
    code generation, mathematical reasoning and creative writing, which suitable for
    models working within predefined task scopes. Datasets like Defects4J, MMLU, and
    MBPP help evaluate model capabilities in specific domains. LLM-based agents are
    more suited for complex, multitasking, and dynamic environments where datasets
    require models to handle multimodal inputs and real-time decision-making, it can
    showcase their advantages in handling complex interactions and multitasking scenarios.
    Datasets like HotpotQA, ALFWorld, FEVER, and WebShop challenge the models’ performance
    in information synthesis, dynamic decision-making/interaction and multimodal tasks.
    This difference arises from the distinct design goals of the two: LLMs aim to
    optimize performance on single tasks, while LLM-based agents are designed to handle
    complex or multi-modal task, this require higher autonomy and adaptability. It’s
    also reflects modern applications’ demand for highly interactive, adaptive, and
    multifunctional AI systems, driving the development from single LLM models to
    multi-agent systems. Through these analyses, we can identify the different application
    of LLMs and LLM-based agents in autonomous learning and decision-making, it’s
    important to choose the appropriate framework to meet different task requirements
    in the real world applications.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 相较而言，LLM 数据集通常关注单一静态任务，如代码生成、数学推理和创意写作，这些任务适合在预定义任务范围内工作的模型。像 Defects4J、MMLU
    和 MBPP 这样的数据集帮助评估模型在特定领域的能力。基于LLM的智能体更适合于复杂、多任务和动态环境，其中数据集要求模型处理多模态输入和实时决策，这可以展示它们在处理复杂交互和多任务场景中的优势。数据集如
    HotpotQA、ALFWorld、FEVER 和 WebShop 挑战模型在信息综合、动态决策/交互和多模态任务中的表现。这种差异源于两者的不同设计目标：LLM
    旨在优化单一任务的性能，而基于LLM的智能体则设计用于处理复杂或多模态任务，这需要更高的自主性和适应性。这也反映了现代应用对高度互动、适应性和多功能 AI
    系统的需求，推动了从单一LLM模型到多智能体系统的发展。通过这些分析，我们可以识别LLM和基于LLM的智能体在自主学习和决策中的不同应用，选择适当的框架以满足现实世界应用中的不同任务需求至关重要。
- en: VI-E Evaluation Metrics
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-E 评估指标
- en: various evaluation metrics are used in the research on LLMs and LLM-based agents,
    these metrics used to evaluate the models’ performance in specific tasks and analyze
    their application effectiveness in this domain. Below, we discuss several representative
    studies analyzing the evaluation metrics they employed and exploring the differences
    between LLMs and LLM-based agents in this field.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在对LLMs和基于LLM的代理的研究中使用了各种评估指标，这些指标用于评估模型在特定任务中的表现，并分析其在该领域的应用效果。以下，我们将讨论几项具有代表性的研究，分析它们所使用的评估指标，并探讨LLMs与基于LLM的代理在这一领域的差异。
- en: In research on LLMs, evaluation metrics primarily focus on model accuracy and
    task completion. In [[90](#bib.bib90)], researchers used the accuracy of a voting
    inference system which measured by the expected 0/1 loss (the proportion of correct
    responses) to assess model performance. This metric evaluates the accuracy of
    models through multiple calls, reflecting the ability of LLMs to improve result
    accuracy via iterative reasoning. Common evaluation metrics in the literature
    include accuracy and sample efficiency, accuracy refers to the proportion of correct
    predictions made by the model, while sample efficiency measures the number of
    samples required to achieve a certain accuracy level. These metrics assess both
    the predictive and decision making ability of the model and its data utilization
    efficiency during training. In [[92](#bib.bib92)], evaluation metrics include
    possible patches, correct patches, precision, and developer accuracy. Possible
    patches refer to patches that pass all tests, while correct patches are semantically
    equivalent to the original developer patches. Precision measures the proportion
    of correct patches among the possible patches, and developer accuracy assesses
    the correctness of patches with and without explanations through human evaluation.
    These metrics emphasize the model’s explanatory capability and practical effectiveness
    in automated code repair, increasing reliance on human evaluation. To assess model
    creativity, value, novelty and surprise are used as creativity dimensions. Quality,
    social acceptability, and similarity of generated works, as well as the ability
    to generate creative product, are also included in the evaluation. [[110](#bib.bib110)]
    used the success rate in the Game of 24 and the coherence of generated paragraphs
    in creative writing as evaluation metrics. These metrics assess the model’s performance
    in problem-solving and text generation, showcasing LLMs’ potential in solving
    complex problems and generating coherent text. In[[95](#bib.bib95)], consistency
    and success rate were used as evaluation metrics, the consistency calculates the
    probability of agreement between two judges on randomly selected questions which
    measures the alignment of LLM judges with human preferences. Success rate is used
    for specific tasks (such as the Game of 24) to measure the correct response rate.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在对LLMs的研究中，评估指标主要关注模型的准确性和任务完成度。在[[90](#bib.bib90)]中，研究人员使用了通过期望0/1损失（正确响应的比例）来测量的投票推理系统的准确性来评估模型性能。该指标通过多次调用评估模型的准确性，反映了LLMs通过迭代推理提高结果准确性的能力。文献中常见的评估指标包括准确性和样本效率，其中准确性指模型做出正确预测的比例，而样本效率则衡量达到一定准确性水平所需的样本数量。这些指标评估了模型的预测能力、决策能力以及在训练过程中的数据利用效率。在[[92](#bib.bib92)]中，评估指标包括可能的补丁、正确的补丁、精准度和开发者准确性。可能的补丁指通过所有测试的补丁，而正确的补丁与原开发者补丁在语义上等价。精准度衡量可能的补丁中正确补丁的比例，而开发者准确性通过人工评估来评估带有和不带解释的补丁的正确性。这些指标强调了模型在自动代码修复中的解释能力和实际效果，越来越依赖人工评估。为了评估模型的创造力，使用了价值、创新性和惊奇作为创造力维度。质量、社会接受度和生成作品的相似性，以及生成创造性产品的能力也包括在评估中。[[110](#bib.bib110)]使用了24点游戏的成功率和创意写作中生成段落的一致性作为评估指标。这些指标评估了模型在解决问题和文本生成中的表现，展示了LLMs在解决复杂问题和生成连贯文本方面的潜力。在[[95](#bib.bib95)]中，使用了一致性和成功率作为评估指标，其中一致性计算了两个评审在随机选择的问题上达成一致的概率，衡量了LLM评审与人类偏好的对齐程度。成功率用于特定任务（如24点游戏）以测量正确响应的比例。
- en: 'In contrast, LLM-based agents use more diverse evaluation metrics to reflect
    their multi-agent collaboration characteristics. In [[97](#bib.bib97)], evaluation
    metrics include Perception Score (P-Score), Cognition Score (C-Score), and Action
    Score (A-Score). These metrics comprehensively assess the model’s perception,
    cognition and action capabilities, demonstrating the comprehensive performance
    of LLM-based agents in handling multimodal tasks. In multimodal applications,
    success rate (SR) is often used as a primary metric, evaluated through tasks such
    as HotpotQA and FEVER to assess precise matching success. These metrics focus
    on task completion success and accuracy, showcasing the practical execution capabilities
    of LLM-based agents in different task environments. In [[111](#bib.bib111)], evaluation
    metrics include practitioner feedback, efficiency, and accuracy. Practitioner
    feedback uses the Likert scale to collect satisfaction and performance feedback,
    the Likert scale is a commonly used psychometric tool designed to measure an individual’s
    attitude or opinion toward a particular statement. The scale typically consists
    of the following five options: Strongly Disagree, Disagree, Neutral, Agree, Strongly
    Agree. While efficiency and accuracy are measured through the effectiveness of
    model-executed qualitative data analysis validated by practitioners. These metrics
    assess the agents’ performance in qualitative data analysis, demonstrating their
    utility and accuracy in practical applications.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于 LLM 的代理使用更多样化的评估指标来反映其多智能体协作特性。在 [[97](#bib.bib97)] 中，评估指标包括感知评分（P-Score）、认知评分（C-Score）和行动评分（A-Score）。这些指标全面评估模型的感知、认知和行动能力，展示了基于
    LLM 的代理在处理多模态任务中的综合表现。在多模态应用中，成功率（SR）通常作为主要指标，通过 HotpotQA 和 FEVER 等任务评估精准匹配的成功率。这些指标关注任务完成成功和准确性，展示了基于
    LLM 的代理在不同任务环境中的实际执行能力。在 [[111](#bib.bib111)] 中，评估指标包括从业者反馈、效率和准确性。从业者反馈使用 Likert
    量表收集满意度和表现反馈，Likert 量表是一个常用的心理测量工具，用于测量个人对特定陈述的态度或意见。该量表通常包括以下五个选项：强烈不同意、不同意、中立、同意、强烈同意。效率和准确性则通过从业者验证的模型执行定性数据分析的效果进行测量。这些指标评估代理在定性数据分析中的表现，展示了其在实际应用中的实用性和准确性。
- en: By comparing these metrics, we find that LLMs using traditional metrics such
    as accuracy and sample efficiency to assess their capabilities. In contrast, LLM-based
    agents handle more complex algorithm through multi-agents, which requires more
    comprehensive and diverse metrics to evaluate their performance from multiple
    directions. LLM-based agents in multimodal tasks and self-evolution tasks emphasize
    the integrated performance of perception, cognition, and action capabilities.
    This difference reflects LLMs’ strengths in single-task optimization and LLM-based
    agents’ potential in collaborative handling of complex tasks with higher capability
    of autonomous learning. Additionally, practical application evaluation metrics
    for LLM-based agents, such as practitioner feedback, efficiency, and accuracy,
    demonstrate their utility and user satisfaction in real-world scenarios. This
    evaluation approach assesses task completion but also consider a comprehensive
    evaluation of user experience, which can also evaluate the human alignment of
    their decision making capabilities.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较这些指标，我们发现 LLMs 使用传统的指标如准确性和样本效率来评估其能力。相反，基于 LLM 的代理通过多智能体处理更复杂的算法，这需要更全面和多样化的指标，从多个方向评估其表现。基于
    LLM 的代理在多模态任务和自我进化任务中强调感知、认知和行动能力的综合表现。这种差异反映了 LLMs 在单任务优化中的优势以及基于 LLM 的代理在协同处理复杂任务中的潜力，具有更高的自主学习能力。此外，基于
    LLM 的代理的实际应用评估指标，如从业者反馈、效率和准确性，展示了它们在现实场景中的实用性和用户满意度。这种评估方法不仅评估任务完成情况，还考虑了用户体验的全面评估，也可以评估其决策能力的人类对齐程度。
- en: 'TABLE VII: Evaluation Metrics in Autonomous Learning and Decision Making'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：自主学习和决策中的评估指标
- en: '| Reference Paper | Benchmarks | Evaluation Metrics | Agent |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 参考论文 | 基准 | 评估指标 | 代理 |'
- en: '|  [[90](#bib.bib90)] | MMLU | Accuracy | No |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  [[90](#bib.bib90)] | MMLU | 准确性 | 否 |'
- en: '|  [[91](#bib.bib91)] | Spider, TransCoder, MBPP | Accuracy, Sample Efficiency
    | No |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  [[91](#bib.bib91)] | Spider, TransCoder, MBPP | 准确性, 样本效率 | 否 |'
- en: '|  [[92](#bib.bib92)] | Defects4J v1.2, Defects4J v2.0, |  |  |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  [[92](#bib.bib92)] | Defects4J v1.2, Defects4J v2.0, |  |  |'
- en: '| Almost-Right HumanEval | Plausible Patches, |  |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Almost-Right HumanEval | 可行的补丁, |  |  |'
- en: '| Correct Patches, |  |  |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 正确修补， |  |  |  |'
- en: '| Precision, Accuracy | No |  |  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 精确度，准确性 | 否 |  |  |'
- en: '|  [[93](#bib.bib93)] | No Specific | Quality, Acceptance Rate | No |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  [[93](#bib.bib93)] | 无特定 | 质量，接受率 | 否 |'
- en: '|  [[110](#bib.bib110)] | Game of 24, Creative Writing, |  |  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  [[110](#bib.bib110)] | 24点游戏，创意写作， |  |  |'
- en: '| 5x5 Crosswords | Success rate, Coherency | No |  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 5x5 填字游戏 | 成功率，一致性 | 否 |  |'
- en: '|  [[95](#bib.bib95)] | MT-Bench, Chatbot Arena | Agreement Rate, Success Rate
    |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  [[95](#bib.bib95)] | MT-Bench，聊天机器人竞技场 | 一致率，成功率 |  |'
- en: '| Human Judgement | No |  |  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 人类判断 | 否 |  |  |'
- en: '|  [[96](#bib.bib96)] | ECQA, GSM8K, FOLIO-wiki | Accuracy | Yes |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  [[96](#bib.bib96)] | ECQA，GSM8K，FOLIO-wiki | 准确性 | 是 |'
- en: '|  [[97](#bib.bib97)] | PCA-EVAL | Accuracy, P/C/A-Score | Yes |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  [[97](#bib.bib97)] | PCA-EVAL | 准确性，P/C/A-得分 | 是 |'
- en: '|  [[35](#bib.bib35)] | HotpotQA, ALFWorld, WebShop, FEVER | Success Rate |
    Yes |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  [[35](#bib.bib35)] | HotpotQA，ALFWorld，WebShop，FEVER | 成功率 | 是 |'
- en: '|  [[106](#bib.bib106)] | Not specified | Success Rate, Autonomy Leve | Yes
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  [[106](#bib.bib106)] | 未指定 | 成功率，自治水平 | 是 |'
- en: '|  [[44](#bib.bib44)] | GSM8K, MATH, MMLU, Chess, HumanEval | Accuracy | Yes
    |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  [[44](#bib.bib44)] | GSM8K，数学，MMLU，棋类，人类评估 | 准确性 | 是 |'
- en: '|  [[107](#bib.bib107)] | MITRE ATTCK framework | Ability Identify Vulnerabilities
    | Yes |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  [[107](#bib.bib107)] | MITRE ATTCK 框架 | 能力识别漏洞 | 是 |'
- en: '|  [[102](#bib.bib102)] | GSM8K, SVAMP, Vicuna testset, |  |  |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  [[102](#bib.bib102)] | GSM8K，SVAMP，Vicuna 测试集， |  |  |'
- en: '| Evol-Instruct testset | Accuracy, Feedback Accuracy | Yes |  |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| Evol-Instruct 测试集 | 准确性，反馈准确性 | 是 |  |'
- en: '|  [[98](#bib.bib98)] | HotPotQA, ALFWorld, HumanEval,MBPP, |  |  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  [[98](#bib.bib98)] | HotPotQA，ALFWorld，人类评估，MBPP， |  |  |'
- en: '| LeetcodeHardGym | Pass@1, Success Rate | Yes |  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| LeetcodeHardGym | Pass@1，成功率 | 是 |  |'
- en: '|  [[111](#bib.bib111)] | Github Developer Discussions,BBC News, |  |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  [[111](#bib.bib111)] | Github 开发者讨论，BBC 新闻， |  |  |'
- en: '| Social MediaConversations, |  |  |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 社交媒体对话， |  |  |  |'
- en: '| In-depth Interviews | Practitioner Feedback, |  |  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 深度访谈 | 实践者反馈， |  |  |'
- en: '| Efficiency and Accuracy | Yes |  |  |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 效率和准确性 | 是 |  |  |'
- en: '|  [[100](#bib.bib100)] | AI Society, Code, Math,Science, |  |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  [[100](#bib.bib100)] | AI 社会，代码，数学，科学， |  |  |'
- en: '| Misalignment | Human Evaluation, |  |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 不一致 | 人类评估， |  |  |'
- en: '| GPT-4 Evaluation | Yes |  |  |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 评估 | 是 |  |  |'
- en: '|  [[99](#bib.bib99)] | FED, Commongen Challenge, |  |  |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  [[99](#bib.bib99)] | FED，Commongen 挑战， |  |  |'
- en: '| MGSM, Logic Grid Puzzles, |  |  |  |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| MGSM，逻辑网格谜题， |  |  |  |'
- en: '| HumanEval | Pass@1, Task completion rate | Yes |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 人类评估 | Pass@1，任务完成率 | 是 |  |'
- en: '|  [[36](#bib.bib36)] | HotpotQA, FEVER, ALFWorld, WebShop | Exact Match, Accuracy,
    |  |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '|  [[36](#bib.bib36)] | HotpotQA，FEVER，ALFWorld，WebShop | 精确匹配，准确性， |  |'
- en: '| Success rate, Average Score | Yes |  |  |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 成功率，平均得分 | 是 |  |  |'
- en: '|  [[103](#bib.bib103)] | Trust Game, Dictator Game, |  |  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|  [[103](#bib.bib103)] | 信任游戏，独裁者游戏， |  |  |'
- en: '| MAP Trust Game, |  |  |  |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| MAP 信任游戏， |  |  |  |'
- en: '| Risky Dictator Game, |  |  |  |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 危险独裁者游戏， |  |  |  |'
- en: '| Lottery Game, Repeated Trust Game | Valid Response Rate, |  |  |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 彩票游戏，重复信任游戏 | 有效响应率， |  |  |'
- en: '| Alignment | Yes |  |  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 一致性 | 是 |  |  |'
- en: '|  [[104](#bib.bib104)] | HotPotQA, WebShop | F1-Score, Average Reward | Yes
    |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '|  [[104](#bib.bib104)] | HotPotQA，WebShop | F1-得分，平均奖励 | 是 |'
- en: '|  [[108](#bib.bib108)] | 263 real smart contract vulnerabilities | F1 Score,
    Accuracy |  |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  [[108](#bib.bib108)] | 263 个真实智能合约漏洞 | F1 得分，准确性 |  |'
- en: '| Precision, Recall |  |  |  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 精确度，召回率 |  |  |  |'
- en: '| Consistency Rate. | Yes |  |  |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 一致性率。 | 是 |  |  |'
- en: '|  [[109](#bib.bib109)] | 15 real-world one-day vulnerabilities |  |  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|  [[109](#bib.bib109)] | 15 个真实世界的一日漏洞 |  |  |'
- en: '| from CVE database | Success Rate, Cost | Yes |  |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 来自 CVE 数据库 | 成功率，成本 | 是 |  |'
- en: '|  [[101](#bib.bib101)] | WebShop, HotPotQA with Wikipedia AP | Reward Score,
    Recall | Yes |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  [[101](#bib.bib101)] | WebShop，HotPotQA 与 Wikipedia AP | 奖励得分，召回率 | 是 |'
- en: '|  [[105](#bib.bib105)] | MMLU, Mini Crosswords, HumanEval, |  |  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  [[105](#bib.bib105)] | MMLU，迷你填字游戏，人类评估， |  |  |'
- en: '| GAIA | Accuracy, Pass@1 | Yes |  |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| GAIA | 准确性，Pass@1 | 是 |  |'
- en: VII Software Design and Evaluation
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 软件设计与评估
- en: The application of LLMs to software design and evaluation has very similar overlaps
    with previous topics, software design is an early phase of software development,
    and the quality of the design directly impacts the quality of furture development.
    Modern software engineering methodologies emphasize the integration of design
    and development to ensure that decisions made during the design phase seamlessly
    translate into high-quality code. Consequently, the research on software design
    often explores aspects related to code generation and development by utilizing
    LLMs for software development with a certain framework and special architecture
    design. Software design frameworks often involve multiple stages of continuous
    refinement to achieve optimal results, which can be considered part of LLM applications
    in software development  [[83](#bib.bib83)]. Similarly, [[85](#bib.bib85)] and[[84](#bib.bib84)]
    highlight the frequent use of tools or API interfaces when using LLMs to assist
    in development and design, demonstrating an overlap with the topic of code generation
    and software development.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在软件设计和评估中的应用与之前的主题有很大的重叠，软件设计是软件开发的早期阶段，设计的质量直接影响未来开发的质量。现代软件工程方法强调设计与开发的融合，以确保在设计阶段做出的决策能够无缝转化为高质量的代码。因此，关于软件设计的研究通常探讨与代码生成和开发相关的方面，利用
    LLMs 在具有一定框架和特殊架构设计的软件开发中。软件设计框架通常涉及多个阶段的持续改进，以实现最佳结果，这可以视为 LLM 在软件开发中的应用的一部分
    [[83](#bib.bib83)]。同样，[[85](#bib.bib85)] 和 [[84](#bib.bib84)] 强调在使用 LLMs 协助开发和设计时频繁使用工具或
    API 接口，展示了与代码生成和软件开发主题的重叠。
- en: LLMs in software design and evaluation also intersect extensively with autonomous
    learning and decision making, these two topics are interrelated fields. Software
    design needs to consider system adaptability and learning capabilities to handle
    dynamic environments, therefore design evaluations involving autonomous learning
    and decision making naturally become a focal point of intersection for these two
    topics. Many LLM techniques and methods find similar applications in both fields,
    for example LLMs based on reinforcement learning can be used for automated design
    decisions and evaluations, as well as for self-learning and optimization. Common
    applications of LLMs in software engineering involve fine-tuning models with prompt
    engineering techniques to continuously enhance performance particularly in software
    design and evaluation, more sample learning is often required to ensure that the
    model outputs align with user expectations [[93](#bib.bib93)] [[102](#bib.bib102)] [[44](#bib.bib44)] [[111](#bib.bib111)] [[105](#bib.bib105)] [[96](#bib.bib96)].
    Additionally, requirement elicitation and specification in requirement engineering
    can also be considered part of software design and evaluation [[51](#bib.bib51)] [[112](#bib.bib112)].
    This section reviews the main research achievements of LLMs in software design
    and evaluation in recent years, discussing their application scenarios and practical
    effects.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在软件设计和评估中还与自主学习和决策制定广泛交叉，这两个主题是相互关联的领域。软件设计需要考虑系统的适应性和学习能力，以处理动态环境，因此涉及自主学习和决策制定的设计评估自然成为这两个主题交集的焦点。许多
    LLM 技术和方法在这两个领域中有类似的应用，例如基于强化学习的 LLMs 可以用于自动化设计决策和评估，以及自我学习和优化。LLMs 在软件工程中的常见应用涉及使用提示工程技术对模型进行微调，以持续提升性能，特别是在软件设计和评估中，通常需要更多的样本学习来确保模型输出符合用户期望
    [[93](#bib.bib93)] [[102](#bib.bib102)] [[44](#bib.bib44)] [[111](#bib.bib111)]
    [[105](#bib.bib105)] [[96](#bib.bib96)]。此外，需求工程中的需求获取和规范也可以视为软件设计和评估的一部分 [[51](#bib.bib51)]
    [[112](#bib.bib112)]。本节回顾了近年来 LLMs 在软件设计和评估方面的主要研究成果，讨论了它们的应用场景和实际效果。
- en: VII-A LLMs Tasks
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A LLMs 任务
- en: 'In recent years, there has been extensive research on the use of LLMs in tasks
    such as automation, optimization, and code understanding. ChatGPT has been widely
    utilized for various software engineering tasks and demonstrated excellent performance
    in tasks like log summarization, pronoun resolution, and code summarization, achieving
    a 100% success rate in both log summarization and pronoun resolution tasks [[113](#bib.bib113)].
    However, its performance on tasks such as code review and vulnerability detection
    is relatively poor, which shows that it needs further improvement for more complex
    tasks. Another framework EvaluLLM addresses the limitations of traditional reference-based
    evaluation metrics (such as BLEU and ROUGE) by using LLMs to assess the quality
    of natural language generation (NLG) outputs [[114](#bib.bib114)]. The EvaluLLM
    introduces a new evaluation method that compares generative outputs in pairs and
    uses win rate metrics to measure model performance, this approach can simplifies
    the evaluation process also ensures consistency with human assessments, showcasing
    the broad application prospects of LLMs in generative tasks. Similarly, in the
    LLMs evaluation domain, LLM-based NLG Evaluation provides a review and classification
    of current LLMs used for NLG evaluation, the paper summarizes four main evaluation
    methods: LLM-derived metrics, prompt-based LLMs, fine-tuned LLMs, and human-LLM
    collaborative evaluations [[115](#bib.bib115)]. These methods demonstrate the
    potential of LLMs in evaluating generative outputs which also mention challenges
    such as the need for improved evaluation metrics and further exploration of human-LLM
    collaboration.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，关于大语言模型（LLMs）在自动化、优化和代码理解等任务中的应用进行了广泛研究。ChatGPT 已被广泛应用于各种软件工程任务，并在日志总结、代词消解和代码总结等任务中表现出色，在日志总结和代词消解任务中实现了100%的成功率[[113](#bib.bib113)]。然而，它在代码审查和漏洞检测等任务中的表现相对较差，这表明它在处理更复杂的任务时仍需进一步改进。另一个框架
    EvaluLLM 通过使用 LLMs 来评估自然语言生成（NLG）输出的质量，解决了传统基于参考的评估指标（如 BLEU 和 ROUGE）的局限性[[114](#bib.bib114)]。EvaluLLM
    引入了一种新的评估方法，该方法通过对生成输出进行配对比较，并使用胜率指标来衡量模型性能，这种方法简化了评估过程，同时确保与人类评估的一致性，展示了 LLMs
    在生成任务中的广泛应用前景。类似地，在 LLMs 评估领域，基于 LLM 的 NLG 评估提供了对当前用于 NLG 评估的 LLMs 的回顾和分类，论文总结了四种主要的评估方法：LLM
    派生指标、基于提示的 LLMs、微调的 LLMs 和人类-LLM 协作评估[[115](#bib.bib115)]。这些方法展示了 LLMs 在评估生成输出方面的潜力，同时提到了需要改进评估指标和进一步探索人类-LLM
    协作等挑战。
- en: There are also many novel application design with the LLMs which applied in
    the engineering design, one study explores strategies for software/hardware co-design
    to optimize LLMs and applies these strategies to design verification [[116](#bib.bib116)].
    Through quantization, pruning, and operation-level optimization, this research
    demonstrates applications in high-level synthesis (HLS) design functionality verification,
    GPT-4 was used to generate high-level synthesis (HLS) designs containing predefined
    errors to create a dataset called Chrysalis, this dataset provides a valuable
    resource for evaluating and optimizing LLM-based HLS debugging assistants. The
    optimized LLM significantly improves inference performance, providing new possibilities
    for error detection and correction in the electronic design automation (EDA) field.
    In [[117](#bib.bib117)], the researchers introduces RaWi, a data-driven GUI prototyping
    approach. The framework allows users to retrieve GUIs from this repository, edit
    them, and create new high-fidelity prototypes quickly. The experiment conducted
    by comparing RaWi with a traditional GUI prototyping tool (Mockplus) to measure
    how quickly and effectively users can create prototypes. The result demonstrated
    that RaWi outperformed on multiple benchmarks, with 40% improvement on precision@k
    metric. This study proves the possibility of LLMs to improve the efficiency during
    prototyping phase of software design, which allows designers to quickly iterate
    on GUI designs, facilitating early detection of design flaws. With the new possibility
    brought by the LLMs, there has been much discussion in the education field, with
    researchers exploring the implications of the prevalence of large language models
    for education [[118](#bib.bib118)]. Study indicates that ChatGPT shows significant
    potential but some limitations in answering questions from software testing courses [[119](#bib.bib119)].
    ChatGPT was able to answer about 77.5% of the questions and provided correct or
    partially correct answers 55.6% of the time. However, the correctness of its explanations
    was only 53.0%, indicating the need for further improvement in educational applications.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的工程设计中也有许多新颖的应用设计，一项研究探讨了优化LLM的软硬件协同设计策略，并将这些策略应用于设计验证[[116](#bib.bib116)]。通过量化、剪枝和操作级优化，这项研究展示了在高层次综合（HLS）设计功能验证中的应用，GPT-4用于生成包含预定义错误的高层次综合（HLS）设计，从而创建了一个名为Chrysalis的数据集，该数据集为评估和优化基于LLM的HLS调试助手提供了宝贵的资源。优化后的LLM显著提高了推理性能，为电子设计自动化（EDA）领域中的错误检测和修正提供了新的可能性。在[[117](#bib.bib117)]中，研究人员引入了RaWi，一个数据驱动的GUI原型设计方法。该框架允许用户从这个库中检索GUI，编辑它们，并快速创建新的高保真原型。通过将RaWi与传统的GUI原型设计工具（Mockplus）进行比较实验，测量用户创建原型的速度和效果。结果显示，RaWi在多个基准测试中表现优异，在precision@k指标上提高了40%。这项研究证明了LLM在软件设计原型阶段提高效率的可能性，使设计师能够快速迭代GUI设计，促进早期发现设计缺陷。随着LLM带来的新可能性，教育领域也展开了广泛讨论，研究人员探索了大语言模型在教育中的影响[[118](#bib.bib118)]。研究表明，ChatGPT在回答软件测试课程中的问题时表现出显著潜力，但也存在一些局限性[[119](#bib.bib119)]。ChatGPT能够回答约77.5%的问题，并且55.6%的时间提供了正确或部分正确的答案。然而，其解释的正确性只有53.0%，表明在教育应用中仍需进一步改进。
- en: VII-B LLM-based Agents Tasks
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 基于LLM的智能体任务
- en: The application of LLM-based agents in software design and evaluation enhance
    the development efficiency and code quality, as well as showcase the broad applicability
    and immense potential of LLM-based agents in practical software engineering tasks. [[120](#bib.bib120)]
    explores the current capabilities, challenges, and opportunities of autonomous
    agents in software engineering. Study evaluate Auto-GPT’s performance across different
    stages of the software development lifecycle (SDLC), including software design,
    testing, and integration with GitHub, the paper finds that detailed contextual
    prompts significantly enhance agent performance in complex software engineering
    tasks which mentions the importance of context-rich prompts in reducing errors
    and improving efficiency, underscoring the potential of LLM-based agents to automate
    and optimize various SDLC tasks, thereby enhancing development efficiency. This
    paper also evaluate the limitation of the Auto-GPT, includes task or goal skipping,
    generating unnecessary code or files (hallucinations), repetitive or looping responses,
    lack of task completion verification mechanisms. These limitations can lead to
    incomplete workflows, inaccurate outputs, and unstable performance in practical
    applications.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理在软件设计和评估中的应用提高了开发效率和代码质量，并展示了基于LLM的代理在实际软件工程任务中的广泛适用性和巨大的潜力。[[120](#bib.bib120)]
    探讨了自主代理在软件工程中的当前能力、挑战和机遇。研究评估了Auto-GPT在软件开发生命周期（SDLC）的不同阶段，包括软件设计、测试和与GitHub的集成的表现，发现详细的上下文提示显著提升了代理在复杂软件工程任务中的表现，强调了上下文丰富的提示在减少错误和提高效率方面的重要性，凸显了基于LLM的代理在自动化和优化各种SDLC任务中的潜力，从而提高开发效率。本文还评估了Auto-GPT的局限性，包括任务或目标跳过、生成不必要的代码或文件（幻觉）、重复或循环响应、缺乏任务完成验证机制。这些局限性可能导致不完整的工作流程、不准确的输出以及在实际应用中的不稳定性能。
- en: '[[121](#bib.bib121)] introduces ChatDev, the first virtual chat-driven software
    development company, a concept of using LLMs not just for specific tasks but as
    central coordinators in a chat-based, multi-agent framework. this approach allows
    for more structured, efficient, and collaborative software development processes,
    exploring how chat-driven multi-agent systems can achieve efficient software design
    and evaluation, reduce code vulnerabilities, and enhance development efficiency
    and quality. Experiments show that ChatDev can design and generate software in
    an average of 409.84 seconds at a cost of only $0.2967 while significantly reducing
    code vulnerabilities. This indicates that chat-based multi-agent frameworks capable
    to improve software development efficiency and quality. Another similar collaboration
    framework introduced by Microsoft research team,[[122](#bib.bib122)] demonstrates
    the effectiveness of using LLMs, particularly ChatGPT as agent’s controllers to
    manage and execute various AI tasks. The HuggingGPT system that uses ChatGPT to
    orchestrate the execution of tasks by various AI models available in Hugging Face,
    the purpose is to test how effectively the system can handle complex AI tasks,
    including language, vision, and speech tasks, by executing appropriate models
    based on user requests. The innovation lies in using LLMs not just as tools for
    direct task execution but as central orchestrators that leverage existing AI models
    to fulfill complex tasks, This approach expands the practical applicability of
    LLMs beyond typical language tasks.[[123](#bib.bib123)] proposes the LLMARENA
    benchmark framework to evaluate LLMs’ capabilities in dynamic multi-agent environments,
    the idea is similar to the ChatDev but innovates by shifting the focus from single-agent
    static tasks to dynamic and interactive multi-agent environments, providing a
    more realistic and challenging setting to assess the practical utility of LLMs,
    this approach mirrors real-world conditions where multiple agents (either AI or
    human) interact and collaborate. Experiments show that this framework can test
    LLMs’ spatial designing, strategic planning, and teamwork abilities in gaming
    environments, offering new possibilities and tools for designing and evaluating
    LLMs in multi-agent systems.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '[[121](#bib.bib121)] 介绍了 ChatDev，这是首个虚拟聊天驱动的软件开发公司，其概念是将大型语言模型（LLMs）不仅用于特定任务，还作为聊天基础的多代理框架中的中央协调者。这种方法允许更结构化、高效和协作的软件开发过程，探索了聊天驱动的多代理系统如何实现高效的软件设计和评估、减少代码漏洞，并提高开发效率和质量。实验表明，ChatDev
    平均在 409.84 秒内设计并生成软件，成本仅为 $0.2967，同时显著减少了代码漏洞。这表明基于聊天的多代理框架能够提高软件开发效率和质量。微软研究团队介绍的另一个类似的协作框架
    [[122](#bib.bib122)] 展示了使用 LLMs，特别是 ChatGPT 作为代理控制器来管理和执行各种 AI 任务的有效性。HuggingGPT
    系统利用 ChatGPT 协调 Hugging Face 中各种 AI 模型的任务执行，目的是测试系统处理复杂 AI 任务（包括语言、视觉和语音任务）的有效性，通过根据用户请求执行适当的模型。创新之处在于将
    LLMs 不仅作为直接任务执行的工具，还作为中央协调者，利用现有的 AI 模型完成复杂任务。这种方法扩展了 LLMs 在典型语言任务之外的实际应用性。[[123](#bib.bib123)]
    提出了 LLMARENA 基准框架，以评估 LLMs 在动态多代理环境中的能力，其理念类似于 ChatDev，但通过将重点从单代理静态任务转移到动态和互动的多代理环境中来进行创新，为评估
    LLMs 的实际效用提供了更现实和具有挑战性的环境。这种方法反映了现实世界中多个代理（无论是 AI 还是人类）互动和协作的情况。实验表明，该框架可以测试 LLMs
    在游戏环境中的空间设计、战略规划和团队合作能力，为在多代理系统中设计和评估 LLMs 提供了新的可能性和工具。'
- en: '[[124](#bib.bib124)] introduces the ”Flows” conceptual framework for structuring
    interactions between AI models and humans to improve reasoning and collaboration
    capabilities. The study present the idea of conceptualizing processes as independent,
    goal-driven entities that interact through standardized message-based interfaces,
    enabling a modular and extensible design. This approach is inherently concurrency-friendly
    and supports the development of complex nested AI interactions without having
    to manage complex dependencies. Experiments in competitive coding tasks show that
    the ”Flows” framework increases the AI model’s problem-solving rate by 21 percentage
    points and the human-AI collaboration rate by 54 percentage points. This demonstrates
    how modular design can enhance AI and human collaboration, thereby improving the
    software design and evaluation process.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '[[124](#bib.bib124)] 引入了“Flows”概念框架，用于构建 AI 模型与人类之间的交互，以提升推理和协作能力。该研究提出了将过程概念化为独立的、以目标驱动的实体，通过标准化的基于消息的接口进行交互，从而实现模块化和可扩展的设计。这种方法天生支持并发，并且支持复杂的嵌套
    AI 交互的开发，无需管理复杂的依赖关系。竞争编码任务中的实验表明，“Flows”框架使 AI 模型的解决问题的速度提高了 21 个百分点，人机协作率提高了
    54 个百分点。这证明了模块化设计如何提升 AI 和人类的协作，从而改善软件设计和评估过程。'
- en: '[[125](#bib.bib125)] presents a new taxonomy to structurally understand and
    analyze LLM-integrated applications, providing new theories and methods for software
    design and evaluation. This taxonomy helps in understanding the integration of
    LLM components in software systems, laying a theoretical foundation for developing
    more effective and efficient LLM-integrated applications. Similarly,[[126](#bib.bib126)]
    explores the application of LLM-based agents in software maintenance tasks, improving
    code quality and reliability through a collaborative framework. This study should
    origin be categorized under the software maintenance domain but exhibit the iterative
    manner of the design structure. The framework utilize the task decomposition and
    multi-agent strategies to tackle complex engineering tasks that traditional one-shot
    methods cannot handle effectively, multiple agents can learn from each other,
    leading to improved software maintenance outcomes. Experiments show that multi-agent
    systems outperform single-agent systems in complex debugging tasks, indicating
    that this new framework can be applied in software design to provide safer architectures.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '[[125](#bib.bib125)] 提出了一个新的分类法，以结构性地理解和分析 LLM 集成应用，为软件设计和评估提供了新的理论和方法。该分类法有助于理解
    LLM 组件在软件系统中的集成，为开发更有效和高效的 LLM 集成应用奠定了理论基础。同样，[[126](#bib.bib126)] 探讨了基于 LLM 的代理在软件维护任务中的应用，通过协作框架提升代码质量和可靠性。该研究应归类于软件维护领域，但展现了设计结构的迭代方式。该框架利用任务分解和多代理策略来处理传统的一次性方法无法有效应对的复杂工程任务，多个代理可以互相学习，从而改善软件维护结果。实验表明，在复杂的调试任务中，多代理系统优于单代理系统，表明这一新框架可以应用于软件设计中，以提供更安全的架构。'
- en: VII-C Analysis
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 分析
- en: Overall, LLM applications in software design and evaluation typically focus
    on the automation of specific tasks, such as code generation and log summarization,
    with a tendency towards evaluation the capability rather than implementation during
    the design phases. The process of software design is largely intertwined with
    software development and requirements engineering. As previously mentioned, the
    use of LLMs to assist in software development often includes aspects of the software
    design process, particularly in generating related design documentation. Therefore,
    there is relatively limited research focused on using LLMs for higher-level software
    design tasks.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，LLM 在软件设计和评估中的应用通常集中在特定任务的自动化上，例如代码生成和日志总结，倾向于在设计阶段评估能力而非实现。软件设计过程与软件开发和需求工程紧密交织。如前所述，利用
    LLM 协助软件开发通常包括软件设计过程的相关方面，特别是在生成相关设计文档方面。因此，专注于使用 LLM 进行更高层次的软件设计任务的研究相对有限。
- en: LLM-based agents expand the capabilities of LLMs by handling more complex workflows
    through intelligent decision-making and task execution, these agents can collaborate,
    dynamically adjust tasks and gather and utilize external information. In software
    design and evaluation, a single model often cannot comprehensively consider both
    design and evaluation aspects, which is why more software developers are reluctant
    to entrust high-level tasks to AI. LLM-based agents, through collaborative work
    and more refined role division, can efficiently complete design tasks and adapt
    to various application scenarios. However, the application of LLM-based agents
    in software design is commonly included in the software development, like previously
    discussed, the self-reflection and reasoning before action occurs during the software
    design phases. The Chatdev[[121](#bib.bib121)] framework uses role distribution
    to create a separate software design phase which significantly increases the flexibility
    and accuracy in the later development phases. In terms of efficiency and cost,
    LLMs are still slightly superior to LLM-based agents in text generation and vulnerability
    detection. However, handling tasks similar to software maintenance and root cause
    analysis requires more complex architectures, such as multi-turn dialogues, knowledge
    graphs, and RAG techniques, which can further benefit the design and evaluation
    phases.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理通过智能决策和任务执行扩展了LLM的能力，能够处理更复杂的工作流程，这些代理可以协作、动态调整任务，并收集和利用外部信息。在软件设计和评估中，单一模型往往无法全面考虑设计和评估两个方面，这也是为什么越来越多的软件开发者不愿意将高级任务委托给AI的原因。基于LLM的代理通过协作工作和更精细的角色分工，能够高效完成设计任务并适应各种应用场景。然而，基于LLM的代理在软件设计中的应用通常包括在软件开发中，如前所述，自我反思和行动前的推理发生在软件设计阶段。Chatdev[[121](#bib.bib121)]
    框架通过角色分配创建了一个独立的软件设计阶段，这显著提高了后续开发阶段的灵活性和准确性。在效率和成本方面，LLM在文本生成和漏洞检测上仍略优于基于LLM的代理。然而，处理类似软件维护和根本原因分析的任务需要更复杂的架构，如多轮对话、知识图谱和RAG技术，这可以进一步促进设计和评估阶段的工作。
- en: VII-D Benchmarks
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 基准
- en: 'The benchmarks include public datasets and datasets self-crafted by the authors
    themselves, and the application scenarios are also quite differently as shown
    in the Table [VIII](#S7.T8 "TABLE VIII ‣ VII-E Evaluation Metrics ‣ VII Software
    Design and Evaluation ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision
    Making ‣ V-E Evaluation Metrics ‣ V Code Generation and Software Development ‣
    IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and
    Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future"). BigCloneBench is a benchmark dataset for
    code clone detection, containing a large number of Java function pairs. These
    pairs are classified as clones and non-clones, used for training and evaluating
    clone detection models, with the main evaluation metric being the correct identification
    rate. The Chrysalis dataset created by [[116](#bib.bib116)], it contains over
    1000 function-level designs from 11 open-source synthesizable HLS datasets, primarily
    used to evaluate the effectiveness of LLM debugging tools in detecting and correcting
    injected errors in HLS designs, with the main evaluation metric being the effectiveness
    of error detection and correction. The CodexGLUE dataset is a comprehensive benchmark
    dataset covering various code generation and understanding tasks such as code
    completion, code repair, and code translation, used to evaluate the performance
    of code generation models in practical programming tasks. In addition to these
    public datasets, some artificially simulated datasets are used, such as a simulated
    job fair environment dataset. This dataset simulates a virtual job fair environment
    containing multiple task scenarios such as interviews, recruitment, and team project
    coordination. The dataset used to evaluate the coordination capabilities of generative
    agents in complex social tasks, with the main evaluation metrics being task coordination
    success rate and role matching accuracy.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '基准测试包括公共数据集和作者自行创建的数据集，应用场景也各不相同，如表格 [VIII](#S7.T8 "TABLE VIII ‣ VII-E Evaluation
    Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation Metrics ‣ VI Autonomous
    Learning and Decision Making ‣ V-E Evaluation Metrics ‣ V Code Generation and
    Software Development ‣ IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement
    Engineering and and Documentation ‣ From LLMs to LLM-based Agents for Software
    Engineering: A Survey of Current, Challenges and Future") 所示。BigCloneBench 是一个用于代码克隆检测的基准数据集，包含大量的
    Java 函数对。这些函数对被分类为克隆和非克隆，用于训练和评估克隆检测模型，主要评估指标为正确识别率。由 [[116](#bib.bib116)] 创建的
    Chrysalis 数据集，包含来自 11 个开源可综合 HLS 数据集的 1000 多个函数级设计，主要用于评估 LLM 调试工具在检测和修正 HLS 设计中注入的错误的有效性，主要评估指标为错误检测和修正的有效性。CodexGLUE
    数据集是一个综合基准数据集，涵盖各种代码生成和理解任务，如代码补全、代码修复和代码翻译，用于评估代码生成模型在实际编程任务中的表现。除了这些公共数据集外，还使用了一些人工模拟的数据集，例如模拟的招聘会环境数据集。该数据集模拟了一个虚拟招聘会环境，包含多种任务场景，如面试、招聘和团队项目协调。该数据集用于评估生成代理在复杂社会任务中的协调能力，主要评估指标为任务协调成功率和角色匹配准确性。'
- en: Comparatively, LLMs research tends to use specific and publicly available datasets,
    such as BigCloneBench. These datasets provide standardized evaluation benchmarks,
    aiding in the reproducibility and comparability of results. Researches on LLM-based
    agents tends to use customized experimental settings or unspecified datasets,
    such as requirement documentations, without specifying particular datasets but
    emphasizing that the experiments involve 70 user requirements. This choice is
    usually because the research needs to evaluate the performance from multiple angles,
    and it is difficult to perfectly adapt to the vertical application scenarios if
    some general datasets are used. Both LLM and LLM-based agents use a variety of
    datasets to evaluate the performance of the model, these datasets cover tasks
    ranging from code generation, code understanding, to natural language generation
    and task management, due to the topic of software design and evaluation is relatively
    inter-related with others. However, because the LLM-based agents can be expanded
    to application scenarios such as videos and pictures, the agents like Auto-GPT
    and HuggingGPT also use multimodal datasets. These datasets not only contain code
    and text, but also involve multiple data types such as images and speech. Moreover,
    compared with a single LLM framework, LLM-based agents need to evaluate more areas,
    so benchmarks also need to be considered separately. For example, LLMARENA is
    specially designed to test the performance of LLM in dynamic, multi-agent environments,
    covering complex tasks such as spatial reasoning, strategic planning, and risk
    assessment.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，LLM的研究通常使用特定且公开可用的数据集，例如BigCloneBench。这些数据集提供了标准化的评估基准，帮助结果的可重复性和可比性。基于LLM的代理的研究倾向于使用定制的实验设置或未指定的数据集，例如需求文档，而不具体说明数据集，但强调实验涉及70个用户需求。这种选择通常是因为研究需要从多个角度评估性能，如果使用一些通用数据集，很难完全适应垂直应用场景。LLM和基于LLM的代理都使用多种数据集来评估模型的性能，这些数据集涵盖从代码生成、代码理解到自然语言生成和任务管理的任务，因为软件设计和评估的话题与其他话题相对相关。然而，由于基于LLM的代理可以扩展到如视频和图片等应用场景，因此像Auto-GPT和HuggingGPT这样的代理也使用多模态数据集。这些数据集不仅包含代码和文本，还涉及图像和语音等多种数据类型。此外，与单一的LLM框架相比，基于LLM的代理需要评估更多领域，因此基准也需要单独考虑。例如，LLMARENA专门设计用于测试LLM在动态、多代理环境中的性能，涵盖空间推理、战略规划和风险评估等复杂任务。
- en: VII-E Evaluation Metrics
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-E 评估指标
- en: In Software Design and Evaluation, various studies have employed different evaluation
    metrics to measure the performance of LLMs and LLM-based agents across a range
    of tasks. Both LLM and LLM-based agent research use more than one metrics to comprehensively
    assess model performance, LLMs research tends to focus on traditional metrics
    such as accuracy, win rate, and consistency, while LLM-based agent research still
    consider those fundamental metrics but further introduces complex evaluation methods,
    such as task coordination success rate and role matching accuracy. However, it
    cannot be definitively stated that future LLM-based agent research will always
    use more flexible evaluation metrics considering multiple dimensions, but more
    dependent on the specific task and dataset being used. The reason for this phenomenon,
    as observed in this survey, is primarily that tasks in LLMs research are relatively
    single-tasked, mainly focusing on static tasks such as log summarization with
    traditional evaluation methods. On the other hand, LLM-based agent research involves
    more general multi-agent tasks, and its evaluation methods emphasize interactivity
    and dynamics. LLM-based agent research focuses more on the model’s collaboration
    and decision-making capabilities by using multi-dimensional evaluation metrics
    to comprehensively assess their potential in practical applications consider not
    only the accuracy. This explains why, despite the similarity in evaluation metrics
    such as accuracy and completion time, LLM-based agents use flexible evaluation
    metrics, including metrics like mutual exclusiveness and appropriateness.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件设计和评估中，各种研究采用了不同的评估指标来衡量 LLM 和基于 LLM 的代理在各种任务中的性能。LLM 和基于 LLM 的代理研究使用不止一个指标来全面评估模型性能，LLM
    研究倾向于关注传统指标，如准确率、胜率和一致性，而基于 LLM 的代理研究则考虑了这些基本指标，但进一步引入了复杂的评估方法，如任务协调成功率和角色匹配准确率。然而，无法明确指出未来的基于
    LLM 的代理研究是否总是会使用考虑多个维度的更灵活的评估指标，这更多依赖于具体的任务和使用的数据集。根据本调查观察到的现象，主要原因是 LLM 研究中的任务相对单一，主要关注静态任务，如日志摘要，并使用传统的评估方法。另一方面，基于
    LLM 的代理研究涉及更多的一般性多代理任务，其评估方法强调互动性和动态性。基于 LLM 的代理研究通过使用多维度评估指标，更加关注模型的协作和决策能力，以全面评估其在实际应用中的潜力，而不仅仅是准确性。这解释了为什么尽管评估指标如准确性和完成时间相似，基于
    LLM 的代理使用灵活的评估指标，包括互斥性和适用性等指标。
- en: 'TABLE VIII: Evaluation Metrics in Software Design and Evaluation'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE VIII: 软件设计和评估中的评估指标'
- en: '| Reference Paper | Benchmarks | Evaluation Metrics | Agent |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 基准 | 评估指标 | 代理 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[113](#bib.bib113)] &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[113](#bib.bib113)] &#124;'
- en: '|'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BigCloneBench, &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BigCloneBench, &#124;'
- en: '&#124; Python functions, &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Python 函数, &#124;'
- en: '&#124; Java methods, &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Java 方法, &#124;'
- en: '&#124; Random logs, &#124;'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 随机日志, &#124;'
- en: '&#124; Bug reports, &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误报告, &#124;'
- en: '&#124; Requirement specifications &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需求规格 &#124;'
- en: '| Accuracy | No |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| 准确性 | 否 |'
- en: '|'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[114](#bib.bib114)] &#124;'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[114](#bib.bib114)] &#124;'
- en: '| Not Specified | Win rate, Agreement score | No |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 未指定 | 胜率、一致性评分 | 否 |'
- en: '|'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[115](#bib.bib115)] &#124;'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[115](#bib.bib115)] &#124;'
- en: '| Not Specified |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 未指定 |'
- en: '&#124; Embedding-based metrics, &#124;'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于嵌入的指标, &#124;'
- en: '&#124; probability-based metrics, &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于概率的指标, &#124;'
- en: '&#124; Comparison, Ranking &#124;'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 比较、排名 &#124;'
- en: '| No |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[116](#bib.bib116)] &#124;'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[116](#bib.bib116)] &#124;'
- en: '| Chrysalis | Effectiveness | No |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 蝶蛹 | 有效性 | 否 |'
- en: '|'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[127](#bib.bib127)] &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[127](#bib.bib127)] &#124;'
- en: '|'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CommonsenseQA, &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CommonsenseQA, &#124;'
- en: '&#124; StrategyQA, GSM8K &#124;'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; StrategyQA, GSM8K &#124;'
- en: '|'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Accuracy, &#124;'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性, &#124;'
- en: '&#124; Token, Time costs &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Token, 时间成本 &#124;'
- en: '| No |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[119](#bib.bib119)] &#124;'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[119](#bib.bib119)] &#124;'
- en: '|'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 31 Questions from &#124;'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 31 个问题来自 &#124;'
- en: '&#124; software testing textbook. &#124;'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 软件测试教科书。 &#124;'
- en: '| Correctness, Effectiveness | No |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 正确性, 有效性 | 否 |'
- en: '|'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[128](#bib.bib128)] &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[128](#bib.bib128)] &#124;'
- en: '|'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Medical transcripts, &#124;'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 医学记录, &#124;'
- en: '&#124; Amazon Product &#124;'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 亚马逊产品 &#124;'
- en: '&#124; Descriptions &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 描述 &#124;'
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Coverage, &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 覆盖率, &#124;'
- en: '&#124; False Failure Rate &#124;'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 虚假失败率 &#124;'
- en: '&#124; Alignment. &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对齐。 &#124;'
- en: '| No |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[117](#bib.bib117)] &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[117](#bib.bib117)] &#124;'
- en: '| Rico |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| Rico |'
- en: '&#124; Precision@k, &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Precision@k, &#124;'
- en: '&#124; NDCG@k, &#124;'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NDCG@k, &#124;'
- en: '&#124; Mean Reciprocal Rank, &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均倒数排名， &#124;'
- en: '&#124; Average Precision, HITS@k &#124;'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 平均精确度， HITS@k &#124;'
- en: '| No |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[120](#bib.bib120)] &#124;'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[120](#bib.bib120)] &#124;'
- en: '| Not Specified |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 未指定 |'
- en: '&#124; Accuracy, Success rate, &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性， 成功率， &#124;'
- en: '&#124; Consistency, Effectiveness &#124;'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一致性， 效果性 &#124;'
- en: '| Yes |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[122](#bib.bib122)] &#124;'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[122](#bib.bib122)] &#124;'
- en: '|'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hugging Face’s &#124;'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Hugging Face的 &#124;'
- en: '&#124; Model Repository. &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型库。 &#124;'
- en: '|'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Accuracy, &#124;'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性， &#124;'
- en: '&#124; Precision, &#124;'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度， &#124;'
- en: '&#124; Recall, &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 召回率， &#124;'
- en: '&#124; F1-Score, &#124;'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1分数， &#124;'
- en: '&#124; Edit Distance, &#124;'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编辑距离， &#124;'
- en: '&#124; GPT-4 Score, &#124;'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GPT-4 分数， &#124;'
- en: '&#124; Passing Rate, &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率， &#124;'
- en: '&#124; Rationality, &#124;'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 理性， &#124;'
- en: '&#124; Success Rate. &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成功率。 &#124;'
- en: '| Yes |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[124](#bib.bib124)] &#124;'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[124](#bib.bib124)] &#124;'
- en: '| Codeforces, LeetCode | Pass@1 | Yes |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| Codeforces, LeetCode | Pass@1 | 是 |'
- en: '|'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[121](#bib.bib121)] &#124;'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[121](#bib.bib121)] &#124;'
- en: '| 70 User Requirements. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 70 用户需求。 |'
- en: '&#124; Number of files generated, &#124;'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成的文件数量， &#124;'
- en: '&#124; Time taken, Cost &#124;'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 所需时间， 成本 &#124;'
- en: '| Yes |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[121](#bib.bib121)] &#124;'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[121](#bib.bib121)] &#124;'
- en: '| Codeforces |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| Codeforces |'
- en: '&#124; Comprehensiveness, &#124;'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全面性， &#124;'
- en: '&#124; Robustness, Conciseness, &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 稳健性， 简洁性， &#124;'
- en: '&#124; Mutual exclusiveness, &#124;'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互斥性， &#124;'
- en: '&#124; Explanatory power, &#124;'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 解释力， &#124;'
- en: '&#124; Extensibility. &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可扩展性。 &#124;'
- en: '| Yes |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[125](#bib.bib125)] &#124;'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[125](#bib.bib125)] &#124;'
- en: '| Sample Applications. | BERTScore, BLEU | Yes |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| 示例应用程序。 | BERTScore, BLEU | 是 |'
- en: '|'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[126](#bib.bib126)] &#124;'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[126](#bib.bib126)] &#124;'
- en: '| CodexGLUE |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| CodexGLUE |'
- en: '&#124; BLEU, METEOR, &#124;'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BLEU, METEOR, &#124;'
- en: '&#124; ROUGE-L, BERTScore &#124;'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ROUGE-L, BERTScore &#124;'
- en: '| Yes |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[129](#bib.bib129)] &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[129](#bib.bib129)] &#124;'
- en: '| Production Incidents |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 生产事故 |'
- en: '&#124; Success rate, &#124;'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成功率， &#124;'
- en: '&#124; Accuracy, Alignment, &#124;'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性， 对齐， &#124;'
- en: '&#124; Appropriateness &#124;'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适用性 &#124;'
- en: '| Yes |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[130](#bib.bib130)] &#124;'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[130](#bib.bib130)] &#124;'
- en: '|'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Simulated Job Fair &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模拟招聘会 &#124;'
- en: '&#124; Environment &#124;'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 环境 &#124;'
- en: '|'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Completion time, &#124;'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成时间， &#124;'
- en: '&#124; Task Progress, &#124;'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务进展， &#124;'
- en: '&#124; Understanding Level &#124;'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 理解水平 &#124;'
- en: '| Yes |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: VIII Software Test Generation
  id: totrans-603
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 软件测试生成
- en: In software development, a crucial component is software testing, which need
    to continuously been conducted from the initial system development to the final
    deployment. In industry, agile development is commonly used which test system
    continuously at every stage to ensure the robustness of the entire system, whenever
    new code is committed to the GitHub, tests are conducted to ensure the usability
    of the updated version. A common approach is to use Jenkins⁴⁴4[https://www.jenkins.io/](https://www.jenkins.io/)
    to achieve continuous integration and continuous deployment. Jenkins automatically
    hooks into the developer’s action of pushing code to GitHub and runs a test suite
    against the new version. Although the entire process leans towards automated development,
    creating and refining test cases still requires large human effort.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发中，一个关键组成部分是软件测试，这需要从系统初始开发到最终部署的整个过程中持续进行。在行业中，通常使用敏捷开发方法，这种方法在每个阶段都持续进行系统测试，以确保整个系统的稳健性。每当新代码提交到GitHub时，就会进行测试，以确保更新版本的可用性。一种常见的方法是使用Jenkins⁴⁴4[https://www.jenkins.io/](https://www.jenkins.io/)来实现持续集成和持续部署。Jenkins会自动连接到开发者推送代码到GitHub的操作，并对新版本运行测试套件。尽管整个过程趋向于自动化开发，但创建和完善测试用例仍然需要大量的人力。
- en: Typical roles in development involve software testing, such as writing unit
    tests, integration tests, and fuzz tests. Researchers have been attempting to
    use AI to help generate test cases since before the 2000\. Initial implementations
    typically involved simpler forms of AI and machine learning to automate parts
    of the test case generation process. Over time, more sophisticated methods such
    as natural language processing and machine learning models have been applied to
    improve the precision and scope of test case generation. Online tools like Sofy⁵⁵5[https://sofy.ai/](https://sofy.ai/),
    which use machine learning to generate context-based paths in applications, also
    exist to aid in generating test suites. Using large language models to generate
    test cases is a relatively new attempt but has been developing rapidly. In 2020,
    researchers utilized pre-trained language models fine-tuned on labeled data to
    generate test cases. They developed a sequence-to-sequence transformer-based model
    called ”ATHENATEST” and compared its generated results with EvoSuite and GPT-3,
    demonstrating better test coverage [[131](#bib.bib131)]. More research and models
    are being dedicated to test suite generation experiments, for instance, the Codex
    model [[67](#bib.bib67)], mentioned earlier in the code generation section, combined
    with chain-of-thought prompting, achieved high-quality test suite generation with
    CodeCoT, even in zero-shot scenarios. The introduction of LLMs aims to automate
    and streamline the testing process, making it more rigorous and capable of addressing
    aspects that humans might easily overlook.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 开发中的典型角色包括软件测试，例如编写单元测试、集成测试和模糊测试。研究人员早在2000年之前就开始尝试利用AI帮助生成测试用例。最初的实现通常涉及较简单的AI和机器学习形式，以自动化部分测试用例生成过程。随着时间的推移，更复杂的方法，如自然语言处理和机器学习模型，被应用于提高测试用例生成的准确性和范围。像Sofy⁵⁵5[https://sofy.ai/](https://sofy.ai/)这样的在线工具，利用机器学习生成应用程序中的基于上下文的路径，也存在于生成测试套件中。使用大型语言模型生成测试用例是一个相对较新的尝试，但发展迅速。在2020年，研究人员利用在标记数据上微调的预训练语言模型来生成测试用例。他们开发了一种基于序列到序列的变换器模型，称为“ATHENATEST”，并将其生成的结果与EvoSuite和GPT-3进行了比较，展示了更好的测试覆盖率[[131](#bib.bib131)]。更多的研究和模型正致力于测试套件生成实验，例如，前面提到的代码生成部分中的Codex模型[[67](#bib.bib67)]，结合链式思维提示，实现了高质量的测试套件生成，即使在零样本场景中。LLM的引入旨在自动化和简化测试过程，使其更加严谨，并能够处理人类可能容易忽视的方面。
- en: VIII-A LLMs Tasks
  id: totrans-606
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A LLMs任务
- en: The application of LLMs in software test generation is extensive and encompasses
    more than just test suite generation. The reviewed paper included in this survey
    covers several aspects, including security test generation, bug reproduction,
    general bug reproduction, fuzz testing, and coverage-driven test generation. These
    tasks are achieved through various models and techniques, significantly improving
    software quality and reducing developers’ workload.  [[132](#bib.bib132)] aims
    to evaluate the effectiveness of using GPT-4 to generate security tests, demonstrating
    how to conduct supply chain attacks by exploiting dependency vulnerabilities.
    The study experimented with different prompt styles and templates to explore the
    effectiveness of varying information inputs on test generation quality, the results
    showed that tests generated by ChatGPT successfully discovered 24 proof-of-concept
    vulnerabilities in 55 applications, outperforming existing tools TRANSFER[[133](#bib.bib133)]
    and SIEGE⁶⁶6[https://siegecyber.com.au/services/penetration-testing/](https://siegecyber.com.au/services/penetration-testing/).
    This research introduces a new method for generating security tests using LLMs
    and provides empirical evidence of LLM’s potential in the security testing domain,
    offering developers a novel approach to handling library vulnerabilities in applications.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在软件测试生成中的应用非常广泛，涵盖了不仅仅是测试套件生成。本文综述的论文涉及多个方面，包括安全测试生成、缺陷复现、通用缺陷复现、模糊测试和覆盖驱动的测试生成。这些任务通过各种模型和技术实现，显著提高了软件质量并减少了开发人员的工作负担。[[132](#bib.bib132)]旨在评估使用GPT-4生成安全测试的有效性，展示了如何通过利用依赖关系漏洞进行供应链攻击。该研究试验了不同的提示风格和模板，以探索不同信息输入对测试生成质量的影响，结果显示，由ChatGPT生成的测试成功发现了55个应用程序中的24个概念验证漏洞，表现优于现有工具TRANSFER[[133](#bib.bib133)]和SIEGE⁶⁶6[https://siegecyber.com.au/services/penetration-testing/](https://siegecyber.com.au/services/penetration-testing/)。这项研究引入了一种新的使用LLMs生成安全测试的方法，并提供了LLM在安全测试领域潜力的实证证据，为开发人员提供了一种处理应用程序库漏洞的新方法。
- en: Another application is bug reproduction, which allows testers to locate and
    fix bugs more quickly and efficiently. [[134](#bib.bib134)] addresses the limitations
    of current bug reproduction methods, which are constrained by the quality and
    clarity of handcrafted patterns and predefined vocabularies. The paper proposes
    and evaluates a new method framework called AdbGPT, which uses a large language
    model to automatically reproduce errors from Android bug reports. AdbGPT is described
    as outperforming current SOTA approaches in the context of automated bug replay
    for only Android system. The experimental results show that AdbGPT achieved accuracies
    of 90.4% and 90.8% in S2R entity extraction and a success rate of 81.3% in error
    reproduction, significantly outperforming the baseline ReCDroid and ablation study
    versions. By introducing prompt engineering, few-shot learning, and chain-of-thought
    reasoning, AdbGPT demonstrates the powerful capabilities of LLMs in automated
    error reproduction. It also uses GUI encoding to convert the GUI view hierarchy
    into HTML-like syntax, providing LLMs with a clear understanding of the current
    GUI state. While AdbGPT is specialized for Android systems,[[135](#bib.bib135)]
    proposes the LIBRO framework, which uses LLMs to generate bug reproduction tests
    from bug reports. The experimental results show that LIBRO successfully reproduced
    33.5% of bugs in the Defects4J dataset and 32.2% in the GHRB dataset. By combining
    advanced prompt engineering and post-processing techniques, LIBRO demonstrates
    the effectiveness and efficiency of LLMs in generating bug reproduction tests.
    Although LIBRO has a lower absolute effectiveness compared to AdbGPT, it was tested
    across a more diverse set of Java applications and not limited to Android. Therefore,
    while AdbGPT excels in specialized bug replay for Android, LIBRO provides a wider
    range of bug reproduction for Java applications. The extensive application of
    LLMs in test generation tasks such as security test generation, bug reproduction,
    fuzz testing, program repair, and coverage-driven test generation highlights their
    significant potential in improving software quality and reducing the burden on
    developers. Through various models and techniques, these tasks demonstrate how
    LLMs can automate and enhance the software testing process, addressing aspects
    that are often overlooked by humans.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个应用是错误重现，这可以使测试人员更快速、更高效地定位和修复错误。[[134](#bib.bib134)] 讨论了当前错误重现方法的局限性，这些方法受到手工制作模式和预定义词汇表的质量和清晰度的限制。本文提出并评估了一种新的方法框架，称为
    AdbGPT，它利用大型语言模型自动从 Android 错误报告中重现错误。AdbGPT 在仅针对 Android 系统的自动化错误重放方面被描述为优于当前的
    SOTA 方法。实验结果表明，AdbGPT 在 S2R 实体提取中达到了 90.4% 和 90.8% 的准确率，并且在错误重现中的成功率为 81.3%，显著优于基线
    ReCDroid 和消融研究版本。通过引入提示工程、少样本学习和链式思维推理，AdbGPT 展示了 LLM 在自动化错误重现中的强大能力。它还使用 GUI
    编码将 GUI 视图层次结构转换为类似 HTML 的语法，提供了 LLM 对当前 GUI 状态的清晰理解。尽管 AdbGPT 专门针对 Android 系统，[[135](#bib.bib135)]
    提出了 LIBRO 框架，该框架利用 LLM 从错误报告中生成错误重现测试。实验结果表明，LIBRO 成功重现了 Defects4J 数据集中 33.5%
    的错误和 GHRB 数据集中 32.2% 的错误。通过结合先进的提示工程和后处理技术，LIBRO 展示了 LLM 在生成错误重现测试中的有效性和效率。虽然
    LIBRO 的绝对有效性低于 AdbGPT，但它在更多样化的 Java 应用程序中进行了测试，而不仅限于 Android。因此，虽然 AdbGPT 在 Android
    专用错误重放方面表现出色，但 LIBRO 为 Java 应用程序提供了更广泛的错误重现。LLM 在测试生成任务中的广泛应用，如安全测试生成、错误重现、模糊测试、程序修复和覆盖驱动的测试生成，凸显了它们在提高软件质量和减轻开发人员负担方面的重要潜力。通过各种模型和技术，这些任务展示了
    LLM 如何自动化和增强软件测试过程，解决了人类往往忽视的方面。
- en: Similarly, in fuzz testing, LLMs have shown promise potential. [[136](#bib.bib136)]
    developed a universal fuzzing tool, Fuzz4All, which uses LLMs to generate and
    mutate inputs for various software systems. This tool addresses the issues of
    traditional fuzzers being tightly coupled with specific languages or systems and
    lacking support for evolving language features. The study conducted various experiments
    to test the tool’s capabilities, including coverage comparison, bug finding, and
    targeted fuzzing. The results showed that Fuzz4All achieved the highest code coverage
    in all tested languages, with an average increase of 36.8%, and discovered 98
    bugs across nine systems, which considered as state-of-art technique in universal
    fuzzing with LLMs at that time. Through self-prompting and LLM-driven fuzzing
    loops, Fuzz4All demonstrated the effectiveness of LLMs in fuzz testing and showcased
    their capability across multiple languages and systems under test (SUTs) through
    comprehensive evaluations.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在模糊测试中，LLM 显示出了潜力。[ [136](#bib.bib136)] 开发了一个通用模糊测试工具 Fuzz4All，该工具使用 LLM
    生成和变异各种软件系统的输入。该工具解决了传统模糊测试工具与特定语言或系统紧密耦合并缺乏对语言特性演变的支持的问题。研究进行了多项实验以测试工具的能力，包括覆盖率比较、漏洞发现和定向模糊测试。结果表明，Fuzz4All
    在所有测试的语言中实现了最高的代码覆盖率，平均增加了 36.8%，并在九个系统中发现了 98 个漏洞，这被认为是当时通用模糊测试中 LLM 的最先进技术。通过自我提示和
    LLM 驱动的模糊测试循环，Fuzz4All 展示了 LLM 在模糊测试中的有效性，并通过全面评估展示了其在多种语言和待测系统（SUTs）中的能力。
- en: '[[137](#bib.bib137)] introduced SymPrompt, a new code-aware prompting strategy
    aimed at addressing the limitations of existing Search-Based Software Testing
    (SBST) methods and traditional LLM prompting strategies in generating high-coverage
    test cases. By decomposing the original test generation process into a multi-stage
    sequence aligned with the execution paths of the method under test, SymPrompt
    generated high-coverage test cases. Experimental results indicated that SymPrompt
    increased coverage on CodeGen2 and GPT-4 by 26% and 105% respectively. Through
    path constraint prompting and context construction techniques, SymPrompt demonstrated
    the potential of LLMs in generating high-coverage test cases.[[138](#bib.bib138)]
    also focused on test suite coverage, this study introduced the COVERUP system
    which generates high-coverage Python regression tests through coverage analysis
    and interaction with LLMs. The experimental results showed that COVERUP increased
    code coverage from 62% to 81% and branch coverage from 35% to 53% through iterative
    prompting and coverage-driven methods.[[139](#bib.bib139)] proposed the AID method,
    which combines LLMs with differential testing to improve fault detection in ”plausibly
    correct” software. By comparing the effectiveness of AID in generating fault-revealing
    test inputs and oracles, the experiments showed that AID improved recall and precision
    by 1.80 times and 2.65 times respectively, and increased the F1 score by 1.66
    times. By integrating LLMs with differential testing, AID showcased the powerful
    capability of LLMs in detecting complex bugs.'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '[[137](#bib.bib137)] 引入了 SymPrompt，这是一种新的代码感知提示策略，旨在解决现有基于搜索的软件测试（SBST）方法和传统
    LLM 提示策略在生成高覆盖率测试用例方面的局限性。通过将原始测试生成过程分解为与待测方法执行路径对齐的多阶段序列，SymPrompt 生成了高覆盖率的测试用例。实验结果表明，SymPrompt
    在 CodeGen2 和 GPT-4 上分别提高了 26% 和 105% 的覆盖率。通过路径约束提示和上下文构建技术，SymPrompt 展示了 LLM 在生成高覆盖率测试用例方面的潜力。[
    [138](#bib.bib138)] 也关注测试套件覆盖率，这项研究介绍了 COVERUP 系统，该系统通过覆盖分析和与 LLM 的互动生成高覆盖率的 Python
    回归测试。实验结果表明，COVERUP 通过迭代提示和基于覆盖率的方法将代码覆盖率从 62% 提高到 81%，分支覆盖率从 35% 提高到 53%。[ [139](#bib.bib139)]
    提出了 AID 方法，该方法将 LLM 与差分测试结合起来，以提高“看似正确”软件中的故障检测。通过比较 AID 在生成故障揭示测试输入和预言器方面的有效性，实验表明
    AID 将召回率和精确率分别提高了 1.80 倍和 2.65 倍，并将 F1 分数提高了 1.66 倍。通过将 LLM 与差分测试结合，AID 展示了 LLM
    在检测复杂漏洞方面的强大能力。'
- en: VIII-B LLM-based Agents Tasks
  id: totrans-611
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 基于 LLM 的代理任务
- en: In the field of software test generation, the application of LLM-based agents
    demonstrates their potential in automated test generation. While relying on LLM-based
    agents for software test generation might seem excessive, more research is directed
    towards vulnerability detection and system maintenance. LLM-based agents can enhance
    test reliability and quality by distributing tasks such as test generation, execution,
    and optimization through a multi-agent collaborative system. These multi-agent
    systems offer obvious improvements in error detection and repair, and coverage
    testing. An example of such a system is AgentCoder’s multi-agent framework, as
    discussed in the code generation and software development section [[82](#bib.bib82)].
    The primary goal of this system is to leverage multiple specialized agents to
    iteratively optimize code generation, overcoming the limitations of a single agent
    model in generating effective code and test cases. The paper introduce the test
    design agent, which creates diverse and comprehensive test cases; and the test
    execution agent, which executes the tests and provides feedback, it reached an
    89.9% pass rate on the MBPP dataset. Similarly, the SocraTest framework falls
    under the Autonomous Learning and Decision Making topic [[106](#bib.bib106)].
    This framework automates the testing process through conversational interactions,
    the paper presents detailed examples of generating and optimizing test cases using
    GPT-4, emphasizing how multi-step interactions enhance testing methods and generate
    test code. Experimental results show that through conversational LLMs, SocraTest
    can effectively generate and optimize test cases and utilize middleware to facilitate
    interactions between the LLM and various testing tools, achieving more advanced
    automated testing capabilities.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件测试生成领域，基于LLM的代理的应用展示了它们在自动化测试生成中的潜力。虽然依赖基于LLM的代理进行软件测试生成可能看起来过于复杂，但更多的研究还是集中在漏洞检测和系统维护上。基于LLM的代理通过多代理协作系统分配任务，如测试生成、执行和优化，从而增强测试的可靠性和质量。这些多代理系统在错误检测和修复以及覆盖测试方面提供了明显的改进。这样的一个系统例子是AgentCoder的多代理框架，详见代码生成和软件开发部分[[82](#bib.bib82)]。该系统的主要目标是利用多个专门的代理迭代优化代码生成，克服单一代理模型在生成有效代码和测试用例方面的局限性。论文介绍了测试设计代理，它创建了多样化和全面的测试用例；以及测试执行代理，它执行测试并提供反馈，达到了MBPP数据集上89.9%的通过率。同样，SocraTest框架属于自主学习和决策制定主题[[106](#bib.bib106)]。该框架通过对话交互自动化测试过程，论文提供了使用GPT-4生成和优化测试用例的详细示例，强调了多步骤交互如何提升测试方法和生成测试代码。实验结果表明，通过对话LLM，SocraTest能够有效生成和优化测试用例，并利用中间件促进LLM与各种测试工具之间的交互，实现了更先进的自动化测试能力。
- en: The paper collected for the software test generation topic are mostly multiple
    agents based system. The study [[140](#bib.bib140)] evaluates the effectiveness
    of LLMs in generating high-quality test cases and identifies their limitations.
    It proposes a novel multi-agent framework called TestChain. The paper evaluates
    StarChat, CodeLlama, GPT-3.5, and GPT-4 on the HumanEval and LeetCode-hard datasets.
    Experimental results show that the TestChain framework, using GPT-4, achieved
    71.79% accuracy on the LeetCode-hard dataset, an improvement of 13.84% over baseline
    methods. On the HumanEval dataset, TestChain with GPT-4 achieved 90.24% accuracy.
    The TestChain framework designs agents to generate diverse test inputs, maps inputs
    to outputs using ReAct format dialogue chains, and interacts with the Python interpreter
    to obtain accurate test outputs.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 针对软件测试生成主题收集的论文大多是基于多个代理的系统。研究[[140](#bib.bib140)]评估了LLM在生成高质量测试用例中的有效性，并识别了它们的局限性。它提出了一种新型的多代理框架，称为TestChain。论文评估了StarChat、CodeLlama、GPT-3.5和GPT-4在HumanEval和LeetCode-hard数据集上的表现。实验结果表明，使用GPT-4的TestChain框架在LeetCode-hard数据集上达到了71.79%的准确率，比基线方法提高了13.84%。在HumanEval数据集上，TestChain与GPT-4的准确率达到了90.24%。TestChain框架设计了代理以生成多样化的测试输入，使用ReAct格式对话链将输入映射到输出，并与Python解释器交互以获得准确的测试输出。
- en: LLM-based agents can also be applied in user acceptance testing (UAT), [[141](#bib.bib141)]
    aims to enhance the automation of the WeChat Pay UAT process by proposing a multi-agent
    collaborative system named XUAT-Copilot, which uses LLMs to automatically generate
    test scripts. The study evaluates XUAT-Copilot’s performance on 450 test cases
    from the WeChat Pay UAT system, comparing it to a single-agent system and a variant
    without the reflection component. Experimental results show that XUAT-Copilot
    achieved a Pass@1 rate of 88.55%, compared to 22.65% for the single-agent system
    and 81.96% for the variant without the reflection component, with a Complete@1
    rate of 93.03%. XUAT-Copilot employs a multi-agent collaborative framework, including
    action planning, state checking, and parameter selection agents, and uses advanced
    prompting techniques. XUAT-Copilot demonstrates the potential and feasibility
    of LLMs in automating UAT test script generation.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理也可以应用于用户验收测试（UAT），[[141](#bib.bib141)]旨在通过提出一个名为XUAT-Copilot的多代理协作系统来增强微信支付UAT过程的自动化，该系统使用LLM自动生成测试脚本。研究评估了XUAT-Copilot在微信支付UAT系统的450个测试用例上的表现，与单代理系统和没有反射组件的变体进行比较。实验结果表明，XUAT-Copilot的Pass@1率为88.55%，而单代理系统为22.65%，没有反射组件的变体为81.96%，Complete@1率为93.03%。XUAT-Copilot采用了多代理协作框架，包括行动规划、状态检查和参数选择代理，并使用了先进的提示技术。XUAT-Copilot展示了LLM在自动化UAT测试脚本生成中的潜力和可行性。
- en: VIII-C Analysis
  id: totrans-615
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-C 分析
- en: '![Refer to caption](img/8e609fefe0f81916a281f3e07cd98534.png)'
  id: totrans-616
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8e609fefe0f81916a281f3e07cd98534.png)'
- en: 'Figure 7: Illustration of Comparison Framework Between LLM-based Agent[[141](#bib.bib141)]
    and LLM[[136](#bib.bib136)] in Software Test Generation'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：LLM-based Agent[[141](#bib.bib141)]与LLM[[136](#bib.bib136)]在软件测试生成中的比较框架示意图
- en: In comparison, LLMs perform well in single-task implementations, generating
    high-quality test cases through techniques like prompt engineering and few-shot
    learning. The number of related studies is increasing as the capabilities of LLMs
    improve. On the other hand, LLM-Based Agents, through multi-agent collaborative
    systems, decompose tasks for specialized processing, significantly enhancing the
    effectiveness and efficiency of test generation and execution through iterative
    optimization and feedback. Considering the cost, using LLMs for test generation
    only is enough and more cost saving than using LLM-based agents. However, if a
    specific model performs poorly, it can affect the entire system’s performance.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，LLM在单任务实现中表现良好，通过如提示工程和少量学习等技术生成高质量的测试用例。随着LLM能力的提升，相关研究数量也在增加。另一方面，基于LLM的代理通过多代理协作系统分解任务进行专业处理，通过迭代优化和反馈显著提高测试生成和执行的效果和效率。考虑到成本，仅使用LLM进行测试生成已经足够，并且比使用基于LLM的代理更节省成本。然而，如果特定模型表现不佳，可能会影响整个系统的性能。
- en: A single LLM may struggle with complex, multi-step tasks. For example, in high-coverage
    test generation, LLMs may require more complex prompts and post-processing steps
    to achieve the desired results. Additionally, the quality of the generated results
    depends heavily on the prompt design and quality. For tasks requiring fine control
    and continuous optimization, a single LLM may find it challenging to deal with.
    As shown in Figure.LABEL:testGen, the LLM framework uses [[136](#bib.bib136)]
    as an example to demonstrate the usage of LLMs in fuzz testing, the prompt will
    be optimized by given code snippets (fuzz inputs), and re-select by the LLM again
    to choose the best prompt for the future generation. The overall framework lacks
    autonomy, the LLM-based agent [[141](#bib.bib141)] framework on the left fills
    this gap, as well as able to perceive the UI and interact with the skill library
    for the operations. The operation agent will receive any error reported by the
    inspection agent and do the self-reflection to refine the process autonomously.
    However, as previously discussed, build a LLM-based agents framework only for
    the software test generation task are ”overkill”, so the collected paper for LLM-based
    agents system generally focused on program repair by generated test cases or bug
    replay system, like in the Figure.LABEL:testGen, the LLM-based agent framework
    is actually used for automatically test the Wechat Pay system.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 单一的LLM可能难以处理复杂的多步骤任务。例如，在高覆盖率测试生成中，LLM可能需要更复杂的提示和后处理步骤才能实现期望的结果。此外，生成结果的质量在很大程度上依赖于提示设计和质量。对于需要精细控制和持续优化的任务，单一LLM可能难以应对。如图Figure.LABEL:testGen所示，LLM框架使用[[136](#bib.bib136)]作为示例演示LLM在模糊测试中的使用，提示将通过给定的代码片段（模糊输入）进行优化，并由LLM重新选择以选择未来生成的最佳提示。整体框架缺乏自主性，左侧的LLM基础代理框架[[141](#bib.bib141)]填补了这一空白，能够感知用户界面并与技能库互动进行操作。操作代理将接收检查代理报告的任何错误，并进行自我反思以自主改进过程。然而，正如之前讨论的那样，仅为软件测试生成任务构建LLM基础的代理框架是“过度设计”的，因此，针对LLM基础代理系统的收集论文通常集中于通过生成的测试用例或缺陷重放系统进行程序修复，如图Figure.LABEL:testGen所示，LLM基础代理框架实际上用于自动测试微信支付系统。
- en: VIII-D Benchmarks
  id: totrans-620
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-D 基准测试
- en: In the tasks of LLMs in software test generation, the dataset Defects4J used
    to evaluate bug reproduction and program repair techniques. Other public datasets
    such as ReCDroid, ANDROR2+, and Themis are primarily used to evaluate mobile application
    bug reproduction and security test generation, particularly for the Android platform.
    GCC, Clang, Go toolchain, Java compiler (javac), and Qiskit involve fuzz testing
    datasets for various programming languages and toolchains, aimed at assessing
    the effectiveness of fuzz testing in multi-language environments. TrickyBugs and
    EvalPlus are datasets containing complex bug scenarios, used to evaluate the precision
    and recall of generated test cases, the benchmark applications evaluated by CODAMOSA
    are used to assess the effectiveness of coverage-based test generation tools.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件测试生成的LLM任务中，数据集Defects4J用于评估缺陷复现和程序修复技术。其他公共数据集如ReCDroid、ANDROR2+和Themis主要用于评估移动应用程序缺陷复现和安全测试生成，特别是针对Android平台。GCC、Clang、Go工具链、Java编译器（javac）和Qiskit涉及各种编程语言和工具链的模糊测试数据集，旨在评估在多语言环境中模糊测试的有效性。TrickyBugs和EvalPlus是包含复杂缺陷场景的数据集，用于评估生成的测试用例的准确性和召回率，CODAMOSA评估的基准应用程序用于评估基于覆盖率的测试生成工具的有效性。
- en: The datasets used in LLM-Based Agents research are also quite common, HumanEval,
    MBPP, and LeetCode-hard are mainly used to evaluate the accuracy and coverage
    of code generation and test generation, involving various programming problems
    and challenges which frequently appeared in previous sections. Datasets like Codeflaws,
    QuixBugs, and ConDefects are collected to familiarize LLMs with erroneous code
    and programs, containing multiple program errors and defects, and are used to
    evaluate the effectiveness of automated debugging and bug repair. A unique dataset
    is the WeChat Pay UAT system, which includes user acceptance test cases from actual
    applications and is used to evaluate the performance of multi-agent systems in
    user acceptance testing, focusing specifically on WeChat’s security system.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 用于LLM基础代理研究的数据集也相当常见，其中HumanEval、MBPP和LeetCode-hard主要用于评估代码生成和测试生成的准确性和覆盖面，涉及各种编程问题和挑战，这些问题和挑战在前面的部分中经常出现。像Codeflaws、QuixBugs和ConDefects这样的数据集被收集起来，以使LLM熟悉错误的代码和程序，包含多种程序错误和缺陷，用于评估自动化调试和错误修复的有效性。一个独特的数据集是微信支付UAT系统，它包括实际应用中的用户验收测试用例，用于评估多代理系统在用户验收测试中的表现，特别关注微信的安全系统。
- en: Overall, the datasets used in LLM-based agents’ research are broader covering
    a wide range of programming problems and challenges, LLM research is more focused
    on the actual generation tasks, such as bug reproduction on the Android platform
    and fuzz testing in multi-language environments. This because the LLM-Based agents
    not only focus on the quality of generated test cases and code but also evaluate
    the collaborative effects and iterative optimization capabilities of multi-agent
    systems, so the benchmarks also include the dataset used to evaluate performance
    of the framework. For instance, AgentCoder [[82](#bib.bib82)] improves the efficiency
    and accuracy of test generation and execution through multi-agent collaboration
    consider qualitative and quantitative evaluations and using MBPP,HummanEval to
    do the evaluations, researches on LLM-Based agents places more emphasis on verifying
    the effectiveness of the system through qualitative evaluation and user feedback.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，用于LLM基础代理研究的数据集范围更广，涵盖了各种编程问题和挑战，LLM研究更侧重于实际生成任务，例如在Android平台上的错误重现和多语言环境下的模糊测试。这是因为LLM基础代理不仅关注生成的测试用例和代码的质量，还评估多代理系统的协作效果和迭代优化能力，因此基准测试还包括用于评估框架性能的数据集。例如，AgentCoder [[82](#bib.bib82)]通过多代理协作提高测试生成和执行的效率和准确性，考虑了定性和定量评估，并使用MBPP、HummanEval进行评估，LLM基础代理的研究更强调通过定性评估和用户反馈来验证系统的有效性。
- en: VIII-E Evaluation Metrics
  id: totrans-624
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-E 评估指标
- en: 'As seen in Table [IX](#S8.T9 "TABLE IX ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"), LLMs research predominantly utilizes traditional quantitative metrics
    such as bug reproduction rate, code coverage, precision, and recall, these metrics
    directly reflect the effectiveness and quality of test generation. In contrast,
    LLM-Based agents research not only focuses on quantitative metrics but also introduces
    qualitative evaluations, such as improvements through conversational interactions
    and the collaborative effects of multi-agent systems. This diversified evaluation
    approach provides a more comprehensive reflection of the system’s practical application
    effects. From the task perspectives, LLMs are more inclined to single task processing,
    such as generating test sets and considering the coverage of generated test sets.
    However, because of the expansion of agents framework, LLM-based agents often
    tend to use the generated test sets to evaluate whether vulnerabilities can be
    found to achieve a more ideal practicality. From a design perspective, LLM systems
    are relying on prompt engineering and the generative capabilities of the models
    themselves, their evaluation metrics are also mainly focused on the quality and
    effectiveness of the model outputs, also their evaluation metrics include the
    collaborative effects and efficiency within the system, such as improving Pass@1
    and Complete@1 rates through multi-agent collaboration. Overall, LLMs are more
    suited for rapid test generation and evaluation for specific tasks, with evaluation
    metrics directly reflecting the generation’s effectiveness and quality. LLM-Based
    Agents excel in handling complex and diversified tasks, achieving higher system
    efficiency and effectiveness through multi-agent collaboration and iterative optimization.'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '从表[IX](#S8.T9 "TABLE IX ‣ VIII-E Evaluation Metrics ‣ VIII Software Test Generation
    ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation
    Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation Metrics
    ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D
    Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future")中可以看出，LLMs
    研究主要利用传统的定量指标，如错误重现率、代码覆盖率、准确率和召回率，这些指标直接反映了测试生成的效果和质量。相比之下，基于 LLM 的代理研究不仅关注定量指标，还引入了定性评估，例如通过对话互动的改进和多代理系统的协作效果。这种多样化的评估方法提供了对系统实际应用效果的更全面的反映。从任务角度来看，LLMs
    更倾向于单任务处理，例如生成测试集和考虑生成测试集的覆盖率。然而，由于代理框架的扩展，基于 LLM 的代理通常倾向于使用生成的测试集来评估是否能发现漏洞，从而实现更理想的实用性。从设计角度来看，LLM
    系统依赖于提示工程和模型自身的生成能力，它们的评估指标主要集中在模型输出的质量和效果上，还包括系统内部的协作效果和效率，例如通过多代理协作提高 Pass@1
    和 Complete@1 率。总体而言，LLMs 更适合于特定任务的快速测试生成和评估，其评估指标直接反映了生成的效果和质量。基于 LLM 的代理擅长处理复杂和多样化的任务，通过多代理协作和迭代优化实现更高的系统效率和效果。'
- en: 'TABLE IX: Evaluation Metrics in Software Test Generation'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：软件测试生成中的评估指标
- en: '| Reference Paper | Benchmarks | Evaluation Metrics | Agent |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 基准 | 评估指标 | 代理 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[132](#bib.bib132)] &#124;'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[132](#bib.bib132)] &#124;'
- en: '|'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 26 libraries and 55 &#124;'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 26 个库和 55 个 &#124;'
- en: '&#124; applications with &#124;'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带有 &#124;'
- en: '&#124; known vulnerabilities &#124;'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 已知的漏洞 &#124;'
- en: '|'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of applications for &#124;'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用程序数量 &#124;'
- en: '&#124; which security tests were successfully &#124;'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 哪些安全测试成功了 &#124;'
- en: '&#124; generated.Number of tests that could &#124;'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成的测试数量 &#124;'
- en: '&#124; demonstrate exploits. &#124;'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 演示漏洞利用。 &#124;'
- en: '| No |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '| [[134](#bib.bib134)] |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bib134)] |'
- en: '&#124; ReCDroid, ANDROR2+, &#124;'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ReCDroid，ANDROR2+， &#124;'
- en: '&#124; Themis Empirical Study Dataset &#124;'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Themis 实证研究数据集 &#124;'
- en: '|'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Accuracy of S2R Entity Extraction. &#124;'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; S2R 实体提取的准确性。 &#124;'
- en: '&#124; Reproducibility of Bugs. &#124;'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 错误重现性。 &#124;'
- en: '&#124; Runtime Efficiency. &#124;'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运行效率。 &#124;'
- en: '&#124; User Satisfaction. &#124;'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用户满意度。 &#124;'
- en: '| No |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[135](#bib.bib135)] &#124;'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[135](#bib.bib135)] &#124;'
- en: '| Defects4J, GHRB |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| Defects4J, GHRB |'
- en: '&#124; Bug Reproduction Rate. &#124;'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缺陷再现率. &#124;'
- en: '&#124; Precision and Recall. &#124;'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度和召回率. &#124;'
- en: '&#124; Execution Time. &#124;'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 执行时间. &#124;'
- en: '&#124; Developer Effort. &#124;'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 开发者努力. &#124;'
- en: '| No |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[136](#bib.bib136)] &#124;'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[136](#bib.bib136)] &#124;'
- en: '|'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GCC and Clang. &#124;'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GCC 和 Clang 的定性改进. &#124;'
- en: '&#124; CVC5 and Z3. &#124;'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVC5 和 Z3. &#124;'
- en: '&#124; Go Toolchain. &#124;'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Go 工具链. &#124;'
- en: '&#124; Java Compiler (javac). &#124;'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Java 编译器 (javac). &#124;'
- en: '&#124; Qiskit. &#124;'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Qiskit. &#124;'
- en: '|'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Code Coverage. &#124;'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码覆盖率. &#124;'
- en: '&#124; Validity Rate. &#124;'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有效性率. &#124;'
- en: '&#124; Hit Rate. &#124;'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 命中率. &#124;'
- en: '&#124; Bugs Detected. &#124;'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测到的缺陷. &#124;'
- en: '| No |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| 数量 |'
- en: '|'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[137](#bib.bib137)] &#124;'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[137](#bib.bib137)] &#124;'
- en: '|'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 897 focal methods from 26 &#124;'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 26 种方法中的 897 个焦点方法 &#124;'
- en: '&#124; widely used open-source &#124;'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 广泛使用的开源 &#124;'
- en: '&#124; Python projects. &#124;'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Python 项目. &#124;'
- en: '|'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pass@1. &#124;'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Pass@1. &#124;'
- en: '&#124; FM Call@1. &#124;'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FM Call@1. &#124;'
- en: '&#124; Correct@1. &#124;'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确率@1. &#124;'
- en: '&#124; Line & Branch Coverage. &#124;'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行覆盖率 & 分支覆盖率. &#124;'
- en: '| No |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[139](#bib.bib139)] &#124;'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[139](#bib.bib139)] &#124;'
- en: '|'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TrickyBugs &#124;'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TrickyBugs &#124;'
- en: '&#124; EvalPlus datasets. &#124;'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EvalPlus 数据集. &#124;'
- en: '|'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Recall. &#124;'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 召回率. &#124;'
- en: '&#124; Precision. &#124;'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度. &#124;'
- en: '&#124; F1 Score. &#124;'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1 分数. &#124;'
- en: '| No |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[138](#bib.bib138)] &#124;'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[138](#bib.bib138)] &#124;'
- en: '|'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Benchmark applications originally &#124;'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 原始基准应用 &#124;'
- en: '&#124; used to evaluate CODAMOSA. &#124;'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于评估 CODAMOSA. &#124;'
- en: '|'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Line Coverage. &#124;'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行覆盖率. &#124;'
- en: '&#124; Branch Coverage. &#124;'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分支覆盖率. &#124;'
- en: '&#124; Line + Branch Coverage. &#124;'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行 + 分支覆盖率. &#124;'
- en: '| No |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '| 数量 |'
- en: '|'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[82](#bib.bib82)] &#124;'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[82](#bib.bib82)] &#124;'
- en: '|'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HumanEval. &#124;'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HumanEval. &#124;'
- en: '&#124; MBPP. &#124;'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MBPP. &#124;'
- en: '&#124; HumanEval-ET. &#124;'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HumanEval-ET. &#124;'
- en: '&#124; MBPP-ET. &#124;'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MBPP-ET. &#124;'
- en: '| Pass@1 | Yes |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| Pass@1 | 是 |'
- en: '|'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[106](#bib.bib106)] &#124;'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[106](#bib.bib106)] &#124;'
- en: '| Not Specified |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| 未指定 |'
- en: '&#124; Qualitative improvement through &#124;'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过 &#124;'
- en: '&#124; conversational interactions. &#124;'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对话式交互. &#124;'
- en: '| Yes |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[140](#bib.bib140)] &#124;'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[140](#bib.bib140)] &#124;'
- en: '|'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HumanEval. &#124;'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HumanEval. &#124;'
- en: '&#124; LeetCode-hard. &#124;'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LeetCode 难度. &#124;'
- en: '|'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Accuracy. &#124;'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确率. &#124;'
- en: '&#124; Line Coverage (Line Cov). &#124;'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行覆盖率 (Line Cov). &#124;'
- en: '&#124; Code-with-Bugs (CwB). &#124;'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带缺陷代码 (CwB). &#124;'
- en: '| Yes |'
  id: totrans-727
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[142](#bib.bib142)] &#124;'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[142](#bib.bib142)] &#124;'
- en: '|'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Codeflaws. &#124;'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Codeflaws. &#124;'
- en: '&#124; QuixBugs. &#124;'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; QuixBugs. &#124;'
- en: '&#124; ConDefects. &#124;'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ConDefects. &#124;'
- en: '|'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of Correct Patches. &#124;'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确补丁数量. &#124;'
- en: '&#124; Number of Plausible Patches. &#124;'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 合理补丁数量. &#124;'
- en: '&#124; Correctness Rate. &#124;'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 正确率. &#124;'
- en: '| Yes |'
  id: totrans-738
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[141](#bib.bib141)] &#124;'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[141](#bib.bib141)] &#124;'
- en: '|'
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 450 test cases from the &#124;'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 450 个测试用例来自 &#124;'
- en: '&#124; WeChat Pay UAT system &#124;'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 微信支付 UAT 系统 &#124;'
- en: '|'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pass@1. &#124;'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Pass@1. &#124;'
- en: '&#124; Complete@1. &#124;'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完整性@1. &#124;'
- en: '| Yes |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: IX Software Security and Maintenance
  id: totrans-748
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 软件安全与维护
- en: In the software engineering, software security and maintenance is a popular
    area for the application of LLMs, primarily aimed at enhancing the security and
    stability of software systems through existing technologies to meet the needs
    of users and developers. These models provide promise methods of vulnerability
    detection and repair, while also enabling automated security testing and innovative
    maintenance processes. The application of LLMs in software security and maintenance
    encompasses several aspects, including vulnerability detection, automatic repair,
    penetration testing, and system robustness evaluation. Compared to traditional
    methods, LLMs leverage natural language processing and generation technologies
    to understand and generate complex code and security policies, thereby automating
    detection and repair tasks. For example, LLMs can accurately identify potential
    vulnerabilities by analyzing code structures and contextual information and generate
    corresponding repair suggestions which improves the efficiency and accuracy of
    vulnerability recovery.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件工程中，软件安全性和维护是LLMs应用的热门领域，主要旨在通过现有技术增强软件系统的安全性和稳定性，以满足用户和开发者的需求。这些模型提供了有前景的漏洞检测和修复方法，同时还实现了自动化安全测试和创新的维护流程。LLMs在软件安全和维护中的应用涵盖了多个方面，包括漏洞检测、自动修复、渗透测试和系统鲁棒性评估。与传统方法相比，LLMs利用自然语言处理和生成技术来理解和生成复杂的代码和安全策略，从而自动化检测和修复任务。例如，LLMs可以通过分析代码结构和上下文信息准确识别潜在漏洞，并生成相应的修复建议，从而提高漏洞恢复的效率和准确性。
- en: Moreover, LLMs not only exhibit strong capabilities in vulnerability detection
    but also play a role in tasks like penetration testing and security evaluations.
    Automated penetration testing tools, such as PENTESTGPT [[143](#bib.bib143)].
    LLMs also demonstrate significant advantages in evaluating system robustness by
    simulating various attack scenarios to assess system performance under different
    conditions, helping developers better identify and address potential security
    issues. Research on LLM-based agents in software security and maintenance is also
    keep growing, these intelligent agents can execute complex code generation and
    vulnerability repair tasks and possess self-learning and optimization capabilities
    to handle issues encountered in dynamic development environments. Tools like RITFIS [[144](#bib.bib144)]
    and NAVRepair[[145](#bib.bib145)] have shown potential in improving the precision
    and efficiency of program repairs by using LLM-based agents.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs不仅在漏洞检测方面表现出强大的能力，还在渗透测试和安全评估等任务中发挥作用。自动化渗透测试工具，如PENTESTGPT[[143](#bib.bib143)]。LLMs还在通过模拟各种攻击场景来评估系统在不同条件下的性能，从而显著改善系统鲁棒性评估，帮助开发者更好地识别和解决潜在的安全问题。基于LLMs的软件安全和维护研究也在不断增长，这些智能代理能够执行复杂的代码生成和漏洞修复任务，并具备自学习和优化能力，以处理动态开发环境中遇到的问题。像RITFIS[[144](#bib.bib144)]和NAVRepair[[145](#bib.bib145)]这样的工具，通过使用基于LLMs的代理在提高程序修复的精度和效率方面显示出了潜力。
- en: IX-A LLMs Tasks
  id: totrans-751
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A LLMs任务
- en: 'In the field of software security and maintenance, research on LLMs can be
    categorized into three main areas: vulnerability detection, automatic repair,
    and penetration testing, along with some evaluation studies. The collected papers
    reviewed on LLMs in these domain illustrate their diverse applications and potential.'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件安全和维护领域，对LLMs的研究可以分为三个主要领域：漏洞检测、自动修复和渗透测试，以及一些评估研究。对这些领域的LLMs相关文献进行的综述展示了它们的多样化应用和潜力。
- en: IX-A1 Program Vulnerability
  id: totrans-753
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A1 程序漏洞
- en: In the domain of vulnerability detection, researchers have fine-tuned LLMs to
    enhance the accuracy of source code vulnerability detection. [[146](#bib.bib146)]
    aims to investigate the potential of applying LLMs to the task of vulnerability
    detection in source code and to determine if the performance limits of CodeBERT-like
    models are due to their limited capacity and code understanding ability. The study
    fine-tuned the WizardCoder model (an improved version of StarCoder) and compared
    its performance with the ContraBERT model on balanced and unbalanced datasets.
    The experimental results showed that WizardCoder outperformed ContraBERT in both
    ROC AUC and F1 scores, significantly improving Java function vulnerability detection
    performance, which achieved the state-of-art performance at that time by improving
    ROC AUC from 0.66 in CodeBERT to 0.69.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 在漏洞检测领域，研究人员已对 LLMs 进行了微调，以提高源代码漏洞检测的准确性。[[146](#bib.bib146)] 旨在探讨将 LLMs 应用于源代码漏洞检测任务的潜力，并确定
    CodeBERT 类模型的性能限制是否由于其有限的容量和代码理解能力。该研究对 WizardCoder 模型（StarCoder 的改进版）进行了微调，并将其性能与
    ContraBERT 模型在平衡和不平衡数据集上进行了比较。实验结果显示，WizardCoder 在 ROC AUC 和 F1 分数上均优于 ContraBERT，显著提高了
    Java 函数漏洞检测性能，通过将 ROC AUC 从 CodeBERT 的 0.66 提升至 0.69，实现了当时的最先进性能。
- en: There are study mainly explored the applications of pure LLMs without any framework
    architecture in vulnerability detection, uncovering current challenges. [[147](#bib.bib147)]
    evaluated only the performance of ChatGPT and GPT-3 models in detecting vulnerabilities
    in Java code, the study compared text-davinci-003 (GPT-3) and gpt-3.5-turbo against
    a baseline virtual classifier in binary and multi-label classification tasks.
    The experimental results showed that while text-davinci-003 and gpt-3.5-turbo
    had high accuracy and recall rates in binary classification tasks, their AUC (Area
    Under Curve) scores were only 0.51, indicating performance equivalent to random
    guessing. In multi-label classification tasks, GPT-3.5-turbo and text-davinci-003
    did not significantly outperform the baseline virtual classifier in overall accuracy
    and F1 scores. These findings indicate that the earlier model like GPT-3 has limited
    capabilities in practical vulnerability detection tasks, suggesting the need for
    further research and model optimization to improve their performance in real-world
    applications, fine-tuning and optimizing LLMs can significantly enhance their
    performance in source code vulnerability detection, However, these models still
    face many challenges in practical applications, requiring further research and
    technological improvements to enhance their real-world effectiveness and reliability.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 有研究主要探讨了在漏洞检测中应用纯 LLMs 而不使用任何框架架构的问题，揭示了当前的挑战。[[147](#bib.bib147)] 仅评估了 ChatGPT
    和 GPT-3 模型在检测 Java 代码漏洞方面的表现，该研究将 text-davinci-003 (GPT-3) 和 gpt-3.5-turbo 与基线虚拟分类器在二分类和多标签分类任务中进行了比较。实验结果显示，虽然
    text-davinci-003 和 gpt-3.5-turbo 在二分类任务中具有较高的准确率和召回率，但它们的 AUC（曲线下面积）分数仅为 0.51，表明性能相当于随机猜测。在多标签分类任务中，GPT-3.5-turbo
    和 text-davinci-003 在整体准确率和 F1 分数上并未显著优于基线虚拟分类器。这些发现表明，早期的模型如 GPT-3 在实际漏洞检测任务中能力有限，提示需要进一步研究和模型优化，以提高其在实际应用中的表现，微调和优化
    LLMs 可以显著提升它们在源代码漏洞检测中的表现。然而，这些模型在实际应用中仍面临许多挑战，需要进一步研究和技术改进以增强其现实世界的有效性和可靠性。
- en: In the later years, [[148](#bib.bib148)] introduced a method to incorporate
    complex code structures directly into the model learning process, the GRACE framework
    combines graph structure information and in-context learning, using Code Property
    Graphs (CPGs) to represent code structure information. By integrating the semantic,
    syntactic, and lexical similarities of code, the framework GRACE addresses the
    limitations of text-based LLM analysis, improves the precision and recall rates
    of vulnerability detection tasks. The study utilized three vulnerability datasets,
    showing a 28.65% improvement in F1 scores over baseline models, an important aspect
    of vulnerability detection is enhancing LLM performance in code security tasks.[[149](#bib.bib149)]
    fine-tuned LLMs for specific tasks and evaluated their performance against existing
    models such as ContraBERT. The researchers conducted numerous experiments to determine
    the optimal model architecture, training hyperparameters, and loss functions to
    optimize performance in vulnerability detection tasks. The study primarily focused
    on WizardCoder and ContraBERT, validating their performance through comparisons
    on balanced and unbalanced datasets and developing an efficient batch packing
    strategy that improved training speed. Results indicated that with appropriate
    fine-tuning and optimization, LLMs could surpass state-of-the-art models, contributing
    to more robust and secure software development practices.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 在后期，[[148](#bib.bib148)] 引入了一种将复杂代码结构直接纳入模型学习过程的方法，GRACE 框架结合了图结构信息和上下文学习，使用代码属性图（CPGs）来表示代码结构信息。通过整合代码的语义、语法和词汇相似性，GRACE
    框架解决了基于文本的 LLM 分析的局限性，提高了漏洞检测任务的精度和召回率。这项研究利用了三个漏洞数据集，显示出比基线模型提高了 28.65% 的 F1
    分数，提升 LLM 在代码安全任务中的表现是漏洞检测中的一个重要方面。[[149](#bib.bib149)] 对 LLM 进行了特定任务的微调，并将其性能与现有模型如
    ContraBERT 进行评估。研究人员进行了大量实验，以确定优化漏洞检测任务性能的最佳模型架构、训练超参数和损失函数。该研究主要关注 WizardCoder
    和 ContraBERT，通过在平衡和不平衡数据集上的比较验证了它们的性能，并开发了一种提高训练速度的高效批量打包策略。结果表明，通过适当的微调和优化，LLMs
    可以超越最先进的模型，有助于更加稳健和安全的软件开发实践。
- en: Despite the development of numerous models, it is still necessary to investigate
    their practical effectiveness. [[150](#bib.bib150)] explored the effectiveness
    of code language models (code LMs) in detecting software vulnerabilities and identified
    significant flaws in existing vulnerability datasets and benchmarks. The researchers
    developed a new dataset called PRIMEVUL, and conducted experiments using it, they
    compared PRIMEVUL with existing benchmarks such as BigVul to evaluate several
    code LMs, including state-of-the-art base models like GPT-3.5 and GPT-4, using
    various training techniques and evaluation metrics. The results revealed that
    existing benchmarks significantly overestimated the performance of code LMs. For
    example, a state-of-the-art 7B model scored an F1 of 68.26% on BigVul but only
    3.09% on PRIMEVUL, highlighting the gap between current code language models’
    performance and actual requirements for vulnerability detection.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管开发了大量模型，仍有必要调查它们的实际有效性。[[150](#bib.bib150)] 探讨了代码语言模型（code LMs）在检测软件漏洞中的有效性，并发现了现有漏洞数据集和基准中的重大缺陷。研究人员开发了一个新的数据集，称为
    PRIMEVUL，并使用它进行了实验，他们将 PRIMEVUL 与现有基准如 BigVul 进行了比较，以评估多个代码 LMs，包括像 GPT-3.5 和
    GPT-4 这样的最先进的基础模型，使用了各种训练技术和评估指标。结果揭示了现有基准显著高估了代码 LMs 的性能。例如，一个最先进的 7B 模型在 BigVul
    上的 F1 得分为 68.26%，但在 PRIMEVUL 上仅为 3.09%，这突显了当前代码语言模型性能与漏洞检测实际需求之间的差距。
- en: IX-A2 Automating Program Repair
  id: totrans-758
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A2 自动化程序修复
- en: In the domain of software security and maintenance, LLMs have not only been
    applied to vulnerability detection but also extensively used for automating program
    repair. One study proposed using Round-Trip Translation (RTT) for automated program
    repair, researchers translated defective code into another language and then back
    to the original language to generate potential patches. The study used various
    language models and benchmarks to evaluate RTT’s performance in APR. The experiments
    explored how RTT performs when using programming languages as an intermediate
    representation, how RTT performs when using natural language (English) as an intermediate
    representation, and what qualitative trends can be observed in the patches generated
    by RTT. Three measurement standards and eight models were used in the experiments,
    the results showed that the RTT method achieved significant repair effects on
    multiple benchmarks, particularly excelling in terms of compilation and feasibility [[151](#bib.bib151)].
    Similarly, in automated program repair, [[145](#bib.bib145)] introduced several
    innovative methods. For example, NAVRepair specifically targets C/C++ code vulnerabilities
    by combining node type information and error types. Due to the unique pointer
    operations and memory management issues in C/C++, this language poses complexities.
    The framework uses Abstract Syntax Trees (ASTs) to extract node type information
    and combines it with CWE-derived vulnerability templates to generate targeted
    repair suggestions, the study evaluated NAVRepair on several popular LLMs (ChatGPT,
    DeepSeek Coder, and Magicoder) to demonstrate its effectiveness in improving code
    vulnerability repair performance. The results showed that NAVRepair achieved state-of-art
    performance in C/C++ program repair task, which improved repair accuracy by 26%
    compared to existing methods.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件安全与维护领域，LLMs 不仅应用于漏洞检测，还被广泛用于自动化程序修复。一项研究提出使用回译翻译（RTT）进行自动化程序修复，研究人员将有缺陷的代码翻译成另一种语言，再翻译回原始语言，以生成潜在的修复补丁。该研究使用了各种语言模型和基准来评估
    RTT 在自动程序修复（APR）中的表现。实验探讨了在使用编程语言作为中间表示时 RTT 的表现，使用自然语言（英语）作为中间表示时 RTT 的表现，以及在
    RTT 生成的补丁中可以观察到的定性趋势。实验中使用了三种测量标准和八种模型，结果显示 RTT 方法在多个基准上取得了显著的修复效果，特别是在编译和可行性方面表现突出[[151](#bib.bib151)]。类似地，在自动化程序修复领域，[[145](#bib.bib145)]
    介绍了几种创新方法。例如，NAVRepair 专门针对 C/C++ 代码漏洞，通过结合节点类型信息和错误类型来解决问题。由于 C/C++ 中的独特指针操作和内存管理问题，这种语言具有复杂性。该框架使用抽象语法树（ASTs）来提取节点类型信息，并将其与
    CWE 派生的漏洞模板结合，以生成针对性的修复建议。研究评估了在多个流行的 LLMs（ChatGPT、DeepSeek Coder 和 Magicoder）上的
    NAVRepair，以证明其在提高代码漏洞修复性能方面的有效性。结果显示，NAVRepair 在 C/C++ 程序修复任务中达到了最先进的性能，相较于现有方法提高了
    26% 的修复准确率。
- en: In order to address the two main limitations of existing fine-tuning methods
    for LLM-based program repair, which is the lack of reasoning about the logic behind
    code changes and high computational costs associated with fine-tuning large datasets. [[152](#bib.bib152)]
    introduced the MOREPAIR framework, this framework improve the performance of LLMs
    in automated program repair (APR) by simultaneously optimizing syntactic code
    transformations and the logical reasoning behind code changes, the study used
    techniques to enhance fine-tuning efficiency, such as QLoRA (Quantized Low-Rank
    Adaptation)[[153](#bib.bib153)] to reduce memory requirements and NEFTune (Noisy
    Embedding Fine-Tuning)[[154](#bib.bib154)] to prevent overfitting during the fine-tuning
    process. The experiments evaluated MOREPAIR on four open-source LLMs of different
    sizes and architectures (CodeLlama-13B, CodeLlama-7B, StarChat-alpha, and Mistral-7B)
    using two benchmarks, evalrepair-C++ and EvalRepair-Java. The results indicated
    that CodeLlama improved by 11% and 8% on the first 10 repair suggestions for evalrepair-C++
    and EvalRepair-Java respectively. Another study introduced the PyDex system, which
    automatically repairs syntax and semantic errors in introductory Python programming
    assignments using LLMs, the system combines multimodal prompts and iterative querying
    methods to generate repair candidates and uses few-shot learning to improve repair
    accuracy. PyDex was evaluated on 286 real student programs from an introductory
    Python programming course and compared against three baselines. The results showed
    that PyDex significantly improved the repair rate and effectiveness compared to
    existing baselines[[155](#bib.bib155)].
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决现有针对LLM（大规模语言模型）程序修复方法的两个主要限制——即缺乏对代码变更逻辑的推理和与大数据集微调相关的高计算成本，[152](#bib.bib152)引入了MOREPAIR框架。该框架通过同时优化语法代码转换和代码变更背后的逻辑推理，提升了LLM在自动程序修复（APR）中的表现。该研究采用了提高微调效率的技术，例如QLoRA（量化低秩适配）[153](#bib.bib153)以减少内存需求，以及NEFTune（噪声嵌入微调）[154](#bib.bib154)以防止微调过程中出现过拟合。实验对MOREPAIR在四个不同规模和架构的开源LLM（CodeLlama-13B、CodeLlama-7B、StarChat-alpha和Mistral-7B）进行了评估，使用了两个基准：evalrepair-C++和EvalRepair-Java。结果表明，CodeLlama在evalrepair-C++和EvalRepair-Java的前10个修复建议上分别提高了11%和8%。另一项研究引入了PyDex系统，该系统利用LLM自动修复入门Python编程作业中的语法和语义错误。该系统结合了多模态提示和迭代查询方法来生成修复候选，并使用少量学习来提高修复准确性。PyDex在286个真实学生程序的入门Python编程课程中进行了评估，并与三个基准进行了比较。结果显示，与现有基准相比，PyDex显著提高了修复率和效果[155](#bib.bib155)。
- en: '[[156](#bib.bib156)] introduced a new system named RING that leverages large
    language models (LLMCs) to perform multilingual program repair across six programming
    languages. RING employs a prompt strategy that minimizes customization efforts,
    including three stages: fault localization, code transformation, and candidate
    ranking. The results showed that RING was particularly effective in Python, successfully
    repairing 94% of errors on the first attempt. The study also introduced a new
    PowerShell command repair dataset, providing valuable resources for the research
    community, this research demonstrated that AI-driven automation makes program
    repair more efficient and scalable. Another study,[[157](#bib.bib157)] conducted
    a comprehensive investigation into function-level automated program repair, introducing
    a new LLM-based APR technique called SRepair. SRepair utilizes a dual-LLM framework
    to enhance repair performance, the SRepair framework combines a repair suggestion
    model and a patch generation model. It uses chain-of-thought to generate natural
    language repair suggestions based on auxiliary repair-related information and
    then utilizes these suggestions to generate the repaired function. The results
    showed that SRepair outperformed existing APR techniques on the Defects4J dataset,
    repairing 300 single-function errors, with an improvement of at least 85% over
    previous techniques. This study demonstrated the effectiveness of the dual-LLM
    framework in function-level repair and, for the first time achieved multi-function
    error repair, highlighting the significant potential of LLMs in program repair.
    By extending the scope of APR, SRepair paves the way for applying LLMs in practical
    software development and evaluation.'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: '[[156](#bib.bib156)] 引入了一个名为 RING 的新系统，该系统利用大型语言模型（LLMCs）在六种编程语言中执行多语言程序修复。RING
    采用了一种减少定制工作量的提示策略，包括三个阶段：故障定位、代码转换和候选排名。结果显示，RING 在 Python 中特别有效，首次尝试成功修复了 94%
    的错误。该研究还引入了一个新的 PowerShell 命令修复数据集，为研究社区提供了宝贵的资源，这项研究表明，人工智能驱动的自动化使程序修复更高效、可扩展。另一项研究[[157](#bib.bib157)]
    对函数级自动程序修复进行了全面调查，引入了一种新的基于 LLM 的 APR 技术，称为 SRepair。SRepair 利用双 LLM 框架来增强修复性能，该框架结合了修复建议模型和补丁生成模型。它使用思维链生成基于辅助修复相关信息的自然语言修复建议，然后利用这些建议生成修复后的函数。结果表明，SRepair
    在 Defects4J 数据集上的表现优于现有 APR 技术，成功修复了 300 个单函数错误，改进幅度至少达到 85%。这项研究展示了双 LLM 框架在函数级修复中的有效性，并首次实现了多函数错误修复，突显了
    LLM 在程序修复中的巨大潜力。通过扩展 APR 的范围，SRepair 为将 LLM 应用于实际软件开发和评估奠定了基础。'
- en: IX-A3 Penetration Testing
  id: totrans-762
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IX-A3 渗透测试
- en: LLMs can also be applied in the field of penetration testing, where they are
    used to enhance the efficiency and effectiveness of automated penetration testing.
    Although not as frequently studied as vulnerability detection and automated repair,
    this review includes two relevant papers. [[143](#bib.bib143)] investigates the
    development and evaluation of an LLM-driven automatic penetration testing tool
    PENTESTGPT. The main purpose of this study is to evaluate the performance of LLMs
    in practical penetration testing tasks and address the issue of context loss during
    the penetration testing process, the paper introduces three self-interaction modules
    of PENTESTGPT (reasoning, generation, and parsing) and provides empirical research
    based on benchmarks involving 13 targets and 182 sub-tasks. It compares the penetration
    testing performance of GPT-3.5, GPT-4, and Bard. The experimental results show
    that PENTESTGPT’s task completion rate is 228.6% higher than GPT-3.5 and 58.6%
    higher than GPT-4, this study demonstrates the potential of LLMs in automated
    penetration testing, helping to identify and resolve security vulnerabilities,
    thereby enhancing the security and robustness of software systems.
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 也可以应用于渗透测试领域，在该领域中，它们用于提高自动化渗透测试的效率和效果。虽然这个领域的研究不如漏洞检测和自动修复频繁，但这篇综述包含了两篇相关论文。[[143](#bib.bib143)]
    研究了基于 LLM 的自动渗透测试工具 PENTESTGPT 的开发和评估。该研究的主要目的是评估 LLM 在实际渗透测试任务中的表现，并解决渗透测试过程中上下文丢失的问题，论文介绍了
    PENTESTGPT 的三个自我交互模块（推理、生成和解析），并提供了基于涉及 13 个目标和 182 个子任务的基准的实证研究。它比较了 GPT-3.5、GPT-4
    和 Bard 的渗透测试表现。实验结果显示，PENTESTGPT 的任务完成率比 GPT-3.5 高 228.6%，比 GPT-4 高 58.6%，这项研究展示了
    LLM 在自动渗透测试中的潜力，有助于识别和解决安全漏洞，从而提高软件系统的安全性和鲁棒性。
- en: A similar research paper explores the application of generative AI in penetration
    testing. [[158](#bib.bib158)] evaluates the effectiveness, challenges, and potential
    consequences of using generative AI tools (specifically ChatGPT 3.5) in penetration
    testing. Through practical application experiments. The research conducts a five-stage
    penetration test (reconnaissance, scanning, vulnerability assessment, exploitation,
    and reporting) on a vulnerable machine from VulnHub, integrating Shell_GPT (sgpt)
    with ChatGPT’s API to automate guidance in the penetration testing process. The
    experimental results demonstrate that generative AI tools can significantly speed
    up the penetration testing process and provide accurate and useful commands, enhancing
    testing efficiency and effectiveness. This study indicates that the need to consider
    potential risks and unintended consequences, emphasizing the importance of responsible
    use and human oversight. Assessing the robustness of systems is also a crucial
    part of development, LLMs are used to develop and evaluate new testing frameworks
    to detect and improve the robustness of intelligent software systems.[[144](#bib.bib144)]
    introduces a robust input testing framework named RITFIS, designed to evaluate
    the robustness of LLM-based intelligent software against natural language inputs.
    The study adapts 17 existing DNN testing methods to LLM scenarios and empirically
    validates them on multiple datasets to highlight the current robustness deficiencies
    and limitations of LLM software. The study indicate that RITFIS effectively assesses
    the robustness of LLM software and reveals its vulnerabilities in handling complex
    natural language inputs. This research underscores the importance of robustness
    testing for LLM-based intelligent software and provides directions for improving
    testing methods to enhance reliability and security in practical applications.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇类似的研究论文探讨了生成式人工智能在渗透测试中的应用。[[158](#bib.bib158)] 评估了使用生成式人工智能工具（特别是ChatGPT
    3.5）进行渗透测试的有效性、挑战和潜在后果。通过实际应用实验，该研究对VulnHub上的一个易受攻击的机器进行了五阶段渗透测试（侦察、扫描、漏洞评估、利用和报告），将Shell_GPT（sgpt）与ChatGPT的API集成，以自动化渗透测试过程中的指导。实验结果表明，生成式人工智能工具可以显著加快渗透测试过程，并提供准确且有用的命令，从而提高测试效率和有效性。这项研究指出需要考虑潜在风险和意外后果，强调负责任使用和人工监督的重要性。评估系统的鲁棒性也是开发的关键部分，LLM被用于开发和评估新的测试框架，以检测和改善智能软件系统的鲁棒性。[[144](#bib.bib144)]
    介绍了一个名为RITFIS的鲁棒输入测试框架，旨在评估基于LLM的智能软件对自然语言输入的鲁棒性。该研究将17种现有的DNN测试方法适应于LLM场景，并在多个数据集上进行实证验证，以突出当前LLM软件的鲁棒性缺陷和局限性。研究表明，RITFIS有效地评估了LLM软件的鲁棒性，并揭示了其处理复杂自然语言输入的漏洞。这项研究强调了对基于LLM的智能软件进行鲁棒性测试的重要性，并为改进测试方法以增强实际应用中的可靠性和安全性提供了方向。
- en: IX-B LLM-based Agents Task
  id: totrans-765
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-B 基于LLM的代理任务
- en: LLM-based Agents primarily appied in areas such as autonomous decision-making,
    task-specific optimization, and multi-agent collaboration, these frameworks showcasing
    their strong potential in proactive defense.  [[159](#bib.bib159)] aims to address
    the limitations of existing debugging methods that treat the generated program
    as an indivisible entity. By segmenting the program into basic blocks and verifying
    the correctness of each block based on task descriptions, the proposed method
    LDB (Large Language Model Debugger) provide a more detailed and effective debugging
    tool that closely reflects human debugging practices. The study’s experiments
    covered testing LDB on several benchmarks and compared with baseline models without
    a debugger and those using traditional debugging methods (self-debugging with
    explanations and traces). LDB’s accuracy increased from a baseline of 73.8% to
    82.9% on the HumanEval benchmark, an improvement of 9.1%. In the domain of vulnerability
    detection, researchers have enhanced detection accuracy by combining Role-Based
    Access Control (RBAC) practices with deep learning of complex code structures.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的智能体主要应用于自主决策、任务特定优化和多智能体协作等领域，这些框架展示了它们在主动防御方面的强大潜力。[[159](#bib.bib159)]旨在解决现有调试方法的局限性，这些方法将生成的程序视为一个不可分割的实体。通过将程序分段为基本块，并根据任务描述验证每个块的正确性，提出的方法LDB（大型语言模型调试器）提供了一个更详细和有效的调试工具，紧密反映了人类调试的实践。研究中的实验包括在多个基准上测试LDB，并与没有调试器的基线模型和那些使用传统调试方法（自我调试与解释和跟踪）的模型进行了比较。LDB在HumanEval基准上的准确率从基线的73.8%提高到82.9%，提高了9.1%。在漏洞检测领域，研究人员通过将基于角色的访问控制（RBAC）实践与复杂代码结构的深度学习相结合，提升了检测准确率。
- en: '[[160](#bib.bib160)] addresses the challenge of automatically and appropriately
    repairing access control (AC) vulnerabilities in smart contracts. The innovation
    of this paper lies in combining mined RBAC practices with LLMs to create a context-aware
    repair framework for AC vulnerabilities. The model primarily uses GPT-4, enhanced
    by a new method called ACFIX, which mines common RBAC practices from existing
    smart contracts and employs a Multi-Agent Debate (MAD) mechanism to verify the
    generated patches through debates between generator and verifier agents to ensure
    correctness. Experimental results show that ACFIX successfully repaired 94.92%
    of access control vulnerabilities, significantly outperforming the baseline GPT-4’s
    52.54%. Another application in smart contracts[[161](#bib.bib161)], this paper
    introduces a two-stage adversarial framework, GPTLENS, which improves vulnerability
    detection accuracy through generation and discrimination phases. GPTLENS achieved
    a 76.9% success rate in detecting smart contract vulnerabilities, better than
    the 38.5% success rate of traditional methods. Another study, [[109](#bib.bib109)]
    investigates the use of GPT-4 to automatically exploit disclosed but unpatched
    vulnerabilities, the experiments showed that the LLM-based agent achieved an 87%
    success rate in exploiting vulnerabilities when provided with CVE descriptions.
    Finally another LLM-based agent application in the penetration test,[[107](#bib.bib107)]
    employs GPT-3.5 to assist penetration testers by automating high-level task planning
    and low-level vulnerability discovery, thereby enhancing penetration testing capabilities.
    The experiments demonstrated successful automation of multiple stages of penetration
    testing, including high-level strategy formulation and low-level vulnerability
    discovery, showcasing the effectiveness of LLMs in penetration testing.'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '[[160](#bib.bib160)] 解决了在智能合约中自动且恰当地修复访问控制（AC）漏洞的挑战。本文的创新之处在于将挖掘的基于角色的访问控制（RBAC）实践与大型语言模型（LLMs）相结合，以创建一个上下文感知的修复框架来处理
    AC 漏洞。该模型主要使用 GPT-4，通过一种名为 ACFIX 的新方法进行增强，该方法从现有智能合约中挖掘常见的 RBAC 实践，并采用多智能体辩论（MAD）机制，通过生成器和验证器代理之间的辩论来验证生成的补丁以确保其正确性。实验结果表明，ACFIX
    成功修复了 94.92% 的访问控制漏洞，显著优于基线 GPT-4 的 52.54%。在智能合约的另一个应用[[161](#bib.bib161)]中，本文介绍了一种两阶段对抗框架
    GPTLENS，该框架通过生成和判别阶段提高了漏洞检测的准确性。GPTLENS 在检测智能合约漏洞方面取得了 76.9% 的成功率，优于传统方法的 38.5%
    成功率。另一项研究[[109](#bib.bib109)] 探讨了使用 GPT-4 自动利用已披露但未修补的漏洞，实验显示，基于 LLM 的代理在提供 CVE
    描述的情况下，成功利用漏洞的成功率达到了 87%。最后，另一项基于 LLM 的渗透测试应用[[107](#bib.bib107)] 采用 GPT-3.5 协助渗透测试人员，通过自动化高层任务规划和低层漏洞发现，从而增强了渗透测试能力。实验展示了成功自动化渗透测试的多个阶段，包括高层策略制定和低层漏洞发现，体现了
    LLM 在渗透测试中的有效性。'
- en: In the field of software repair by multi-agents collaborations, [[162](#bib.bib162)]
    proposes a dual-agent framework that enhances the automation and accuracy of repairing
    declarative specifications through iterative prompt optimization and multi-agent
    collaboration. The researcher compare the effectiveness of the LLM-based repair
    pipeline with several state-of-the-art Alloy APR techniques (ARepair, ICEBAR,
    BeAFix, and ATR). In the result, framework repaired 231 defects in the Alloy4Fun
    benchmark which surpassing the 278 defects repaired by traditional tools. In[[142](#bib.bib142)],
    developed and evaluated an automated debugging framework named FixAgent, which
    improves fault localization, repair generation, and error analysis through an
    LLM-based multi-agent system. Although this research primarily focuses on automated
    debugging, incorporating elements like fault localization and automated program
    repair (APR), it intersects with test generation, particularly in the validation
    phase for testing bug fixes. The study evaluates FixAgent’s performance on the
    Codeflaws, QuixBugs, and ConDefects datasets, comparing it to 16 baseline methods,
    including state-of-the-art APR tools and LLMs. Experimental results show that
    FixAgent fixed 78 out of 79 bugs in the QuixBugs dataset, including 9 never-before-fixed
    bugs. In the Codeflaws dataset, FixAgent fixed 3982 out of 2780 defects, with
    a correctness rate of 96.5%. The framework includes specialized agents responsible
    for localization, repair, and analysis tasks and uses the rubber duck debugging
    principle. FixAgent demonstrates the powerful capabilities of LLMs in automated
    debugging, improving the performance of existing APR tools and LLMs which can
    be considered as state-of-art framework in APR by LLM-based agent.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 在多代理协作的软件修复领域，[[162](#bib.bib162)] 提出了一个双代理框架，通过迭代提示优化和多代理协作来提高修复声明性规格的自动化和准确性。研究人员将基于LLM的修复管道与几种最先进的Alloy
    APR技术（ARepair、ICEBAR、BeAFix 和 ATR）进行了比较。结果显示，该框架修复了 Alloy4Fun 基准测试中的231个缺陷，超越了传统工具修复的278个缺陷。在[[142](#bib.bib142)]中，开发并评估了一个名为FixAgent的自动化调试框架，通过基于LLM的多代理系统改进了故障定位、修复生成和错误分析。尽管这项研究主要关注自动化调试，包含了故障定位和自动化程序修复（APR）等元素，但在测试生成阶段特别是验证修复的阶段与测试生成交叉。该研究在Codeflaws、QuixBugs和ConDefects数据集上评估了FixAgent的性能，并将其与包括最先进的APR工具和LLMs在内的16种基准方法进行了比较。实验结果表明，FixAgent在QuixBugs数据集中修复了79个中的78个错误，其中包括9个前所未见的错误。在Codeflaws数据集中，FixAgent修复了2780个中的3982个缺陷，正确率达到96.5%。该框架包括负责定位、修复和分析任务的专用代理，并采用了橡皮鸭调试原则。FixAgent展示了LLMs在自动化调试中的强大能力，提升了现有APR工具和LLMs的性能，可以被视为基于LLM的代理的最先进框架。
- en: '[[46](#bib.bib46)] introduces an automated program repair agent named RepairAgent,
    this agent can dynamically generates prompts and integrates tools to automatically
    fix software bugs. This researcher also address the limitations of current LLM-based
    repair techniques, which typically involve fixed prompts or feedback loops that
    do not allow the model to gather comprehensive information about the bug or code.
    RepairAgent is a LLM-based agent designed to alternately collect information about
    the bug, gather repair ingredients, and validate the repairs, similar to how human
    developers fix bugs. RepairAgent achieved impressive result by overall repaired
    186 bugs in the Defects4J benchmark, with 164 being correctly repaired outperforming
    existing repair techniques achieved the state-of-art performances.'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '[[46](#bib.bib46)] 介绍了一个名为RepairAgent的自动化程序修复代理，该代理可以动态生成提示并集成工具以自动修复软件缺陷。研究人员还解决了当前LLM基于修复技术的局限性，这些技术通常涉及固定的提示或反馈循环，不允许模型收集关于缺陷或代码的全面信息。RepairAgent是一个基于LLM的代理，旨在交替收集关于缺陷的信息、收集修复材料和验证修复，类似于人类开发人员修复缺陷的方式。RepairAgent在Defects4J基准测试中取得了令人印象深刻的结果，总共修复了186个缺陷，其中164个被正确修复，超越了现有的修复技术，达到了最先进的表现。'
- en: 'In the realm of software security, researchers have combined LLM and security
    engineering models to improve security analysis and design processes. [[163](#bib.bib163)]
    aims to propose a complex hybrid strategy to ensure the reliability and security
    of software systems, this involves a concept-guided approach where LLM-based agents
    interact with system model diagrams to perform tasks related to safety analysis.[[108](#bib.bib108)]
    introduces the TrustLLM framework which increase the accuracy and interpretability
    of smart contract auditing by customizing LLM capabilities to the specific requirements
    of smart contract code. This paper conducts experiments on a balanced dataset
    comprising 1,734 positive samples and 1,810 negative samples, comparing TrustLLM
    with other models such as CodeBERT, GraphCodeBERT, and several versions of GPT
    and CodeLlama. TrustLLM achieves an F1 score of 91.21% and an accuracy of 91.11%
    which outperforming other models. Beyond software-level security design, LLMs
    can also be integrated into autonomous driving systems.[[164](#bib.bib164)] which
    has already been discussed in the [IV](#S4 "IV Requirement Engineering and and
    Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future").'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '在软件安全领域，研究人员结合了LLM和安全工程模型，以改进安全分析和设计过程。[[163](#bib.bib163)] 旨在提出一种复杂的混合策略，以确保软件系统的可靠性和安全性，这涉及到一种概念引导的方法，其中基于LLM的代理与系统模型图互动以执行与安全分析相关的任务。[
    [108](#bib.bib108)] 介绍了TrustLLM框架，通过将LLM能力定制化以满足智能合约代码的具体要求，提高了智能合约审计的准确性和可解释性。本文在包含1,734个正样本和1,810个负样本的平衡数据集上进行了实验，将TrustLLM与其他模型（如CodeBERT、GraphCodeBERT以及多个版本的GPT和CodeLlama）进行了比较。TrustLLM实现了91.21%的F1得分和91.11%的准确率，优于其他模型。除了软件层面的安全设计，LLM还可以集成到自动驾驶系统中。[[164](#bib.bib164)]
    这一点已经在[IV](#S4 "IV Requirement Engineering and and Documentation ‣ From LLMs to
    LLM-based Agents for Software Engineering: A Survey of Current, Challenges and
    Future")中讨论过。'
- en: IX-C Analysis
  id: totrans-771
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-C 分析
- en: Overall, the direction of LLM-based Agents represents significant innovative
    advancements in software security and maintenance, demonstrating improvements
    across all areas. LLM-based Agents, through multi-agent collaboration and runtime
    information tracking to help with debugging tasks, compared to traditional LLMs
    approaches are often rely on fixed prompts or feedback loops to debug a given
    code snippet or program. In vulnerability detection, LLM-based Agents combine
    RBAC practices and in-depth learning of complex code structures to improve the
    accuracy and efficiency of detecting vulnerabilities, traditional LLMs methods
    normally depend on extensive manual intervention and detailed guidance when handling
    tasks. LLM-based Agents also demonstrate effectiveness in penetration testing
    by automating high-level task planning and low-level vulnerability exploration,
    thereby enhancing penetration testing capabilities. In contrast, traditional LLM
    methods are more suited for passive detection and analysis, lacking proactive
    testing and defense capabilities.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，基于LLM的代理在软件安全性和维护方面代表了显著的创新进展，展示了所有领域的改进。通过多代理协作和运行时信息跟踪来帮助调试任务，相比于传统的LLM方法，LLM基础代理通常依赖于固定的提示或反馈循环来调试给定的代码片段或程序。在漏洞检测中，LLM基础代理结合了RBAC实践和对复杂代码结构的深入学习，提高了检测漏洞的准确性和效率，而传统LLM方法通常依赖于大量的手动干预和详细的指导来处理任务。LLM基础代理在渗透测试中也表现出有效性，通过自动化高级任务规划和低级漏洞探索，从而增强了渗透测试能力。相比之下，传统LLM方法更适合被动检测和分析，缺乏主动测试和防御能力。
- en: '![Refer to caption](img/6919aaf4860ee8cffba9e7c3c2587cf7.png)'
  id: totrans-773
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6919aaf4860ee8cffba9e7c3c2587cf7.png)'
- en: 'Figure 8: Illustration of Comparison Framework Between LLM-based Agent [[46](#bib.bib46)]
    and LLM [[152](#bib.bib152)] in Software Security and Maintenance'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLM基础代理[[46](#bib.bib46)]与LLM[[152](#bib.bib152)]在软件安全性和维护中的比较框架插图
- en: 'From the perspective of automation, LLM-based agents automate the detection
    and repair of software errors through multi-agent frameworks and dynamic analysis
    tools, improving the automation and accuracy of the repair process. Traditional
    LLMs methods also have a good performance on various maintenance or debug tasks,
    but often lack autonomous decision-making and dynamic adjustment capabilities
    during the repair process. In terms of software security, intelligent agent become
    more flexibly by combining LLM and security engineering models to improve security
    analysis and design processes, thereby enhancing the reliability and security
    of software systems. when dealing with security tasks by LLMs only, often rely
    on static analysis lacking adaptability and optimization capabilities. As shown
    in the Figure.[8](#S9.F8 "Figure 8 ‣ IX-C Analysis ‣ IX Software Security and
    Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software Test Generation ‣ VII-E
    Evaluation Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation Metrics
    ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation Metrics ‣ V Code
    Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D Benchmarks
    ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based Agents
    for Software Engineering: A Survey of Current, Challenges and Future"), the comparison
    using the MOREPAIR [[152](#bib.bib152)] for LLMs and RepairAgent[[46](#bib.bib46)]
    for LLM-based agents. The LLM framework utilize the optimization techniques (QLoRA,
    NEFTune) to generate repair advices, the RepairAgent utilize multiple tools during
    the inspection which facilitate the precision and accuracy of the analysis before
    the repair process, the idea is quite similar with ”reasoning before action”.
    Then the agent framework utilize state machine and LLM to refine continuously,
    and if failed during the repair process, the RepairAgent will enter the self-reflection
    phase to understand the reason autonomously.'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 从自动化的角度来看，基于LLM的代理通过多代理框架和动态分析工具自动检测和修复软件错误，提高了修复过程的自动化和准确性。传统的LLM方法在各种维护或调试任务中也表现良好，但在修复过程中往往缺乏自主决策和动态调整能力。在软件安全方面，智能代理通过将LLM与安全工程模型相结合变得更加灵活，以改善安全分析和设计过程，从而增强软件系统的可靠性和安全性。当仅由LLM处理安全任务时，往往依赖于静态分析，缺乏适应性和优化能力。如图所示。[8](#S9.F8
    "图 8 ‣ IX-C 分析 ‣ IX 软件安全与维护 ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计与评估
    ‣ VI-E 评估指标 ‣ VI 自主学习与决策 ‣ V-E 评估指标 ‣ V 代码生成与软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准测试 ‣ IV
    需求工程与文档 ‣ 从LLM到基于LLM的代理在软件工程中的应用：现状、挑战与未来"), 比较使用MOREPAIR [[152](#bib.bib152)]进行LLM评估，以及使用RepairAgent[[46](#bib.bib46)]进行基于LLM的代理评估。LLM框架利用优化技术（QLoRA，NEFTune）生成修复建议，而RepairAgent在检查过程中利用多个工具，有助于在修复过程之前提高分析的精确度和准确性，这个理念与“行动前推理”相似。然后，代理框架利用状态机和LLM不断细化，如果在修复过程中失败，RepairAgent将进入自我反思阶段，以自主理解原因。
- en: Thus, from the review, we can say that LLM-based agents brings more autonomy
    and flexibility in the field of software security and maintenance. These improvements
    can enhance task execution efficiency and accuracy, also extend the application
    scope of LLMs in complex software engineering tasks, demonstrating their strong
    potential in proactive defense, complex task handling, and meeting high reliability
    requirements.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过综述，我们可以说基于LLM的代理在软件安全和维护领域带来了更多的自主性和灵活性。这些改进可以提高任务执行效率和准确性，同时扩展LLM在复杂软件工程任务中的应用范围，展示了其在主动防御、复杂任务处理和满足高可靠性要求方面的强大潜力。
- en: IX-D Benchmarks
  id: totrans-777
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-D 基准测试
- en: When analyzing the benchmarks used in LLM literature, several public datasets
    stand out due to their frequent use and presence across different application
    scenarios. Datasets such as Defects4J, Codeflaws, QuixBugs, and the Common Vulnerability
    and Exposure (CVE) database are commonly employed in the domains of vulnerability
    detection and software security. For instance, Defects4J is widely used in papers
    like [[46](#bib.bib46)] and[[159](#bib.bib159)] to evaluate automated program
    repair tools. Similarly, Codeflaws and QuixBugs are used in papers like[[142](#bib.bib142)]
    to test debugging capabilities, focusing on smaller algorithmic problems typically
    found in competitive programming and educational settings. These datasets effectively
    measure the ability of LLMs to detect vulnerabilities and modify code in specific
    code blocks.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析 LLM 文献中使用的基准时，几个公共数据集因其频繁使用和在不同应用场景中的存在而突出。数据集如 Defects4J、Codeflaws、QuixBugs
    和 Common Vulnerability and Exposure (CVE) 数据库通常用于漏洞检测和软件安全领域。例如，Defects4J 在论文如
    [[46](#bib.bib46)] 和 [[159](#bib.bib159)] 中被广泛使用，以评估自动化程序修复工具。类似地，Codeflaws 和
    QuixBugs 被用于论文如 [[142](#bib.bib142)] 中，以测试调试能力，专注于通常在竞赛编程和教育环境中发现的小型算法问题。这些数据集有效地衡量了
    LLM 在特定代码块中检测漏洞和修改代码的能力。
- en: CVE is a critical benchmark for evaluating the security capabilities of LLMs,
    offering a repository of known vulnerabilities that allow LLMs to assess their
    ability to autonomously detect and exploit security flaws, bridging the gap between
    theoretical research and practical cybersecurity applications. Another notable
    dataset is ARepair, used in [[162](#bib.bib162)]. This dataset consists of defective
    specifications and tests the ability of LLMs to understand and repair formal specifications.
    More common datasets like HumanEval and MBPP are also frequently used to evaluate
    the functional correctness of code generated by LLMs. Similarly, Alloy4Fun is
    used to test the repair of declarative specifications in Alloy framework [[162](#bib.bib162)],
    reflecting the LLM’s performance in understanding and fixing logical errors in
    formal languages.
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: CVE 是评估 LLM 安全能力的关键基准，提供了已知漏洞的存储库，允许 LLM 评估其自主检测和利用安全缺陷的能力，弥合理论研究与实际网络安全应用之间的差距。另一个值得注意的数据集是
    ARepair，使用于 [[162](#bib.bib162)]。该数据集包含缺陷规格，并测试 LLM 理解和修复正式规格的能力。更常见的数据集如 HumanEval
    和 MBPP 也被频繁使用，以评估 LLM 生成代码的功能正确性。同样，Alloy4Fun 被用来测试在 Alloy 框架中修复声明性规格的能力 [[162](#bib.bib162)]，反映了
    LLM 在理解和修复形式语言中的逻辑错误的表现。
- en: Specialized datasets such as VulnHub and HackTheBox are used to evaluate the
    penetration testing capabilities of LLMs. Papers like [[107](#bib.bib107)] utilize
    these environments to simulate real-world hacking scenarios, thereby assessing
    the practical applications of LLMs in cybersecurity. These benchmarks are crucial
    for evaluating the real-world efficacy of LLM-based agents in cyber-security environments,
    bridging the gap between theoretical capabilities and practical applications.
    In the context of smart contract security, datasets extracted from Etherscan and
    those compiled for tools like SmartFix provide benchmarks for evaluating LLMs’
    ability to identify and fix vulnerabilities in blockchain-based applications,
    emphasizing the reliability and security of decentralized applications.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 专门的数据集如 VulnHub 和 HackTheBox 被用来评估 LLM 的渗透测试能力。论文如 [[107](#bib.bib107)] 利用这些环境来模拟现实世界的黑客场景，从而评估
    LLM 在网络安全中的实际应用。这些基准对于评估基于 LLM 的代理在网络安全环境中的实际效果至关重要，弥合理论能力与实际应用之间的差距。在智能合约安全方面，从
    Etherscan 提取的数据集和为如 SmartFix 工具编制的数据集提供了评估 LLM 在区块链应用中识别和修复漏洞能力的基准，强调了去中心化应用的可靠性和安全性。
- en: When comparing the benchmarks used in LLM and LLM-based agent research, several
    key similarities and differences emerge. Both approaches frequently use datasets
    like Defects4J, CVE, and HumanEval, highlighting their foundational role in evaluating
    software engineering tasks. However, LLM-based agent research often combines these
    datasets with specialized benchmarks like VulnHub and HackTheBox to test more
    dynamic and interactive capabilities, especially in the context of cybersecurity.
    LLM-based agent research typically focuses more on real-time autonomous decision-making
    and action, reflected in their choice of benchmarks. These datasets test not only
    the agents’ knowledge but also their ability to autonomously apply this knowledge
    in real-world scenarios. This contrasts with traditional LLMs research, which
    typically focuses on static tasks like vulnerability repair and code generation
    without requiring real-time interaction and further changes or decision-making.
    Moreover, the use of specialized benchmarks like the smart contract datasets from
    Etherscan in LLM-based agent research underscores the importance of blockchain
    technology and the need for robust security measures in decentralized applications,
    this trend highlights the adaptability and diversity of LLM-based agents in addressing
    emerging challenges in software security and maintenance. This distinction reflects
    the broader and more interactive application scenarios of LLM-based agents, also
    the public dataset may not suitable for LLM-based agent in particular structure
    designed, so there are a lot of self-collected benchmark emerged which provide
    more flexibility.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较LLM和基于LLM的代理研究中使用的基准时，几个关键的相似点和差异浮现出来。两种方法都经常使用如Defects4J、CVE和HumanEval等数据集，突显了它们在评估软件工程任务中的基础性作用。然而，基于LLM的代理研究通常将这些数据集与像VulnHub和HackTheBox这样的专业基准结合起来，以测试更具动态性和互动性的能力，尤其是在网络安全的背景下。基于LLM的代理研究通常更加关注实时自主决策和行动，这在其基准选择中有所体现。这些数据集不仅测试代理的知识，还测试它们在实际场景中自主应用这些知识的能力。这与传统的LLM研究形成对比，后者通常关注静态任务，如漏洞修复和代码生成，而不需要实时互动和进一步的更改或决策。此外，基于LLM的代理研究中使用的像Etherscan的智能合约数据集等专业基准，强调了区块链技术的重要性和去中心化应用中对强大安全措施的需求，这一趋势突显了基于LLM的代理在应对软件安全和维护中新兴挑战的适应性和多样性。这一区别反映了基于LLM的代理在更广泛和互动的应用场景中的应用，同时公共数据集可能不适合特别设计的LLM代理，因此出现了许多自收集的基准，以提供更多灵活性。
- en: IX-E Evaluation Metrics
  id: totrans-782
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-E 评估指标
- en: 'The evaluation metrics for the llm in the software security and maintenance
    are quite diverse. Researchers need to consider various factors such as coverage,
    efficiency, and reliability of the model or framework. Evaluation Metrics like
    success rate and pass rate are directly related to the performance of LLMs in
    different scenarios. In Table [X](#S9.T10 "TABLE X ‣ IX-E Evaluation Metrics ‣
    IX Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"), common standards such as success rate and change rate are frequently
    used to evaluate the robustness of models when faced with diverse inputs. Time
    overhead and query number are used to assess the efficiency and resource consumption
    of models when performing specific tasks. Additionally, ROC AUC, F1 score, and
    accuracy are important for evaluating the model’s ability to identify vulnerabilities,
    especially in binary classification tasks. In code repair tasks, metrics such
    as compilability and plausibility are very common, these metrics ensure that the
    generated solutions are correct and deployable. Common standards like BLEU and
    CodeBLEU are used to evaluate the quality and human-likeness of generated code,
    which helps determine if the model’s capabilities and performance are comparable
    to human performance. Furthermore, domain-specific metrics like tree edit distance
    and test pass rate are used to evaluate the effectiveness of LLM applications
    in specialized fields of software engineering, these metrics are used to address
    the limitations posed by software security and maintenance. In contrast, while
    LLM-based agents use evaluation metrics similar to those used for LLMs, such as
    success rate, they also incorporate more subjective metrics for evaluation. These
    include appropriateness, relevance, and adequacy, which are human-judged standards.
    Overall, the evaluation metrics used by agents tend to be simpler and easier to
    understand than those used for LLMs. This is likely because agents handle high-level
    tasks, such as the success rate of generating potential vulnerabilities and the
    frequency of agents calling external tools, so they also need to consider computational
    and time overheads of the overall architecture.'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '在软件安全性和维护中的 LLM 评估指标非常多样。研究人员需要考虑各种因素，如模型或框架的覆盖率、效率和可靠性。像成功率和通过率这样的评估指标直接与
    LLM 在不同场景下的表现相关。在表 [X](#S9.T10 "TABLE X ‣ IX-E Evaluation Metrics ‣ IX Software
    Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software Test Generation
    ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation
    Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation Metrics
    ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D
    Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future")中，像成功率和变化率这样的常见标准常用于评估模型在面对多样化输入时的鲁棒性。时间开销和查询次数用于评估模型在执行特定任务时的效率和资源消耗。此外，ROC
    AUC、F1 分数和准确率对评估模型识别漏洞的能力也非常重要，尤其是在二分类任务中。在代码修复任务中，编译性和合理性等指标非常常见，这些指标确保生成的解决方案是正确的并且可部署。像
    BLEU 和 CodeBLEU 这样的常见标准用于评估生成代码的质量和类人性，这有助于确定模型的能力和表现是否与人类表现相当。此外，领域特定的指标，如树编辑距离和测试通过率，用于评估
    LLM 在软件工程专门领域应用的有效性，这些指标用于解决软件安全性和维护所带来的局限性。相比之下，虽然基于 LLM 的代理使用与 LLM 相似的评估指标，如成功率，但它们还包含更多主观的评估标准。这些标准包括适当性、相关性和充分性，这些都是由人来判断的标准。总体而言，代理使用的评估指标通常比
    LLM 使用的指标更简单易懂。这可能是因为代理处理的是高层次的任务，例如生成潜在漏洞的成功率和代理调用外部工具的频率，因此它们还需要考虑整体架构的计算和时间开销。'
- en: By comparing these metrics, we can see that LLMs emphasising the success rate
    of individual testing methods, LLM-based agents focus more on the overall task
    completion time/cost/effectiveness. LLMs typically use binary classification metrics
    like ROC, AUC, and F1 score, while agents tend to emphasize the success rate and
    accuracy during both the generation and validation phases, providing a comprehensive
    evaluation. For the time cost and performance, LLMs mainly focus on the execution
    time of testing methods and the number of queries to assess their efficiency.
    In contrast, LLM-based agents focus more on the completion time of repair tasks
    and the number of API calls, it will make sure the efficiency and practicality
    of overall architecture.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较这些指标，我们可以看到，LLMs 强调单个测试方法的成功率，而基于LLM的代理更关注整体任务完成时间/成本/效果。LLMs 通常使用像 ROC、AUC
    和 F1 分数这样的二分类指标，而代理更倾向于强调在生成和验证阶段的成功率和准确性，从而提供全面的评估。就时间成本和性能而言，LLMs 主要关注测试方法的执行时间和查询数量，以评估其效率。相比之下，基于LLM的代理更关注修复任务的完成时间和
    API 调用数量，以确保整体架构的效率和实用性。
- en: 'TABLE X: Evaluation Metrics in Software Security and Maintenance'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '表 X: 软件安全性和维护中的评估指标'
- en: '| Reference Paper | Benchmarks | Evaluation Metrics | Agent |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 基准 | 评估指标 | 代理 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[144](#bib.bib144)] &#124;'
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[144](#bib.bib144)] &#124;'
- en: '|'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Financial Sentiment Analysis &#124;'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 财务情感分析 &#124;'
- en: '&#124; Movie Review Analysis &#124;'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 电影评论分析 &#124;'
- en: '&#124; News Classification &#124;'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新闻分类 &#124;'
- en: '|'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Success Rate, Change Rate, Perplexity, Time Overhead, Query Number &#124;'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成功率、变化率、困惑度、时间开销、查询数量 &#124;'
- en: '| No |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[146](#bib.bib146)] &#124;'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[146](#bib.bib146)] &#124;'
- en: '|'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CVEfixes &#124;'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVEfixes &#124;'
- en: '&#124; Manually-Curated Dataset &#124;'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 手工整理的数据集 &#124;'
- en: '&#124; (624 vulnerabilities across &#124;'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (624个漏洞 across &#124;'
- en: '&#124; 205 Java projects) &#124;'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 205 个 Java 项目) &#124;'
- en: '&#124; VCMatch &#124;'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VCMatch &#124;'
- en: '&#124; (10 popular repositories) &#124;'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (10 个热门仓库) &#124;'
- en: '|'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ROC AUC, F1 Score, Accuracy, Optimal Classification, Threshold &#124;'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ROC AUC、F1分数、准确率、最优分类、阈值 &#124;'
- en: '| No |'
  id: totrans-808
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[149](#bib.bib149)] &#124;'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[149](#bib.bib149)] &#124;'
- en: '|'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CVEfixes &#124;'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVEfixes &#124;'
- en: '&#124; Manually-Curated Dataset &#124;'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 手工整理的数据集 &#124;'
- en: '&#124; VCMatch &#124;'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VCMatch &#124;'
- en: '|'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Precision, Recall &#124;'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度、召回率 &#124;'
- en: '| No |'
  id: totrans-817
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '| [[143](#bib.bib143)] |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '| [[143](#bib.bib143)] |'
- en: '&#124; HackTheBox &#124;'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HackTheBox &#124;'
- en: '&#124; VulnHub &#124;'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VulnHub &#124;'
- en: '|'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Overall Task Completion, Sub-task Completion, Task Variety, Challenge
    &#124;'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 整体任务完成度、子任务完成度、任务多样性、挑战 &#124;'
- en: '&#124; Levels, Progress Tracking &#124;'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 层级、进度跟踪 &#124;'
- en: '| No |'
  id: totrans-824
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[151](#bib.bib151)] &#124;'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[151](#bib.bib151)] &#124;'
- en: '|'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Defects4J v1.2 &#124;'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Defects4J v1.2 &#124;'
- en: '&#124; Defects4J v2.0 &#124;'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Defects4J v2.0 &#124;'
- en: '&#124; QuixBugs &#124;'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; QuixBugs &#124;'
- en: '&#124; HumanEval-Java &#124;'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HumanEval-Java &#124;'
- en: '|'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compilability, Plausibility, Test pass rate, Exact Match, BLEU &#124;'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可编译性、合理性、测试通过率、准确匹配、BLEU &#124;'
- en: '| No |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '| [[148](#bib.bib148)] |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] |'
- en: '&#124; Devign &#124;'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Devign &#124;'
- en: '&#124; Reveal &#124;'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Reveal &#124;'
- en: '&#124; Big-Vul &#124;'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Big-Vul &#124;'
- en: '|'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; F1 score, Accuracy, Precision,Recall. &#124;'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1分数、准确率、精确度、召回率。 &#124;'
- en: '| No |'
  id: totrans-841
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[150](#bib.bib150)] &#124;'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[150](#bib.bib150)] &#124;'
- en: '|'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PRIMEVUL &#124;'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PRIMEVUL &#124;'
- en: '&#124; BigVul &#124;'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BigVul &#124;'
- en: '|'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; F1 score, Accuracy, Precision, Recall, VD-S, Pair-wise, evaluation metrics
    &#124;'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1分数、准确率、精确度、召回率、VD-S、成对、评估指标 &#124;'
- en: '| No |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[147](#bib.bib147)] &#124;'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[147](#bib.bib147)] &#124;'
- en: '|'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Customized GitHub dataset &#124;'
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 定制的 GitHub 数据集 &#124;'
- en: '&#124; (308 binary classification and &#124;'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (308 二分类和 &#124;'
- en: '&#124; 120 multi-label classification) &#124;'
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 120 多标签分类) &#124;'
- en: '|'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Precision, Recall, F1-Score, AUC, Accuracy &#124;'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 精确度、召回率、F1分数、AUC、准确率 &#124;'
- en: '| No |'
  id: totrans-858
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[158](#bib.bib158)] &#124;'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[158](#bib.bib158)] &#124;'
- en: '|'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; VulnHub &#124;'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VulnHub &#124;'
- en: '|'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Output’s Description &#124;'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出描述 &#124;'
- en: '| No |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '|'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[165](#bib.bib165)] &#124;'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[165](#bib.bib165)] &#124;'
- en: '|'
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; VulDeePecker &#124;'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VulDeePecker &#124;'
- en: '&#124; SeVC &#124;'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SeVC &#124;'
- en: '|'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; False Positive Rate, False Negative Rate, Precision, Recall, F1-score
    &#124;'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 假阳性率、假阴性率、精确度、召回率、F1分数 &#124;'
- en: '| No |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '| 无 |'
- en: '| [[145](#bib.bib145)] | CVEFixes |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | CVEFixes |'
- en: '&#124; CodeBLEU, Tree Edit Distance, Pass@k &#124;'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CodeBLEU，树编辑距离，Pass@k &#124;'
- en: '| No |'
  id: totrans-876
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[152](#bib.bib152)] &#124;'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[152](#bib.bib152)] &#124;'
- en: '|'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; EvalRepair-C++ &#124;'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EvalRepair-C++ &#124;'
- en: '&#124; EvalRepair-Java &#124;'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EvalRepair-Java &#124;'
- en: '|'
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; TOP-5 and TOP-10, Repair &#124;'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TOP-5 和 TOP-10，修复 &#124;'
- en: '| No |'
  id: totrans-884
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| [[155](#bib.bib155)] |'
  id: totrans-885
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] |'
- en: '&#124; Introductory Python &#124;'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 入门 Python &#124;'
- en: '&#124; Assignments Dataset &#124;'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 作业数据集 &#124;'
- en: '|'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Repair Rate, Token Edit Distance &#124;'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修复率，令牌编辑距离 &#124;'
- en: '| No |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| [[156](#bib.bib156)] |'
  id: totrans-891
  prefs: []
  type: TYPE_TB
  zh: '| [[156](#bib.bib156)] |'
- en: '&#124; Multi-languages dataset &#124;'
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多语言数据集 &#124;'
- en: '&#124; (Excel,Power Fx,Python, &#124;'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (Excel,Power Fx,Python, &#124;'
- en: '&#124; JavaScript,C andPowerShell) &#124;'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; JavaScript,C 和 PowerShell) &#124;'
- en: '| Exact Matches | No |'
  id: totrans-895
  prefs: []
  type: TYPE_TB
  zh: '| 精确匹配 | 否 |'
- en: '|'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[157](#bib.bib157)] &#124;'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[157](#bib.bib157)] &#124;'
- en: '| Defects4J 1.2 and 2.0 |'
  id: totrans-898
  prefs: []
  type: TYPE_TB
  zh: '| Defects4J 1.2 和 2.0 |'
- en: '&#124; Plausible Patches, Correct Fix &#124;'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可能的补丁，正确的修复 &#124;'
- en: '| No |'
  id: totrans-900
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '|'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[107](#bib.bib107)] &#124;'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[107](#bib.bib107)] &#124;'
- en: '|'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Vulnerable Virtual Machine &#124;'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 易受攻击的虚拟机 &#124;'
- en: '&#124; (lin.security Linux VM) &#124;'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (lin.security Linux 虚拟机) &#124;'
- en: '| Success Rate | Yes |'
  id: totrans-906
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 是 |'
- en: '|'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[160](#bib.bib160)] &#124;'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[160](#bib.bib160)] &#124;'
- en: '| 118 AC Vulnerabilities |'
  id: totrans-909
  prefs: []
  type: TYPE_TB
  zh: '| 118 个 AC 漏洞 |'
- en: '&#124; Success Rate, Exploitation based evaluation, Manual inspection of the
    &#124;'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成功率，基于利用的评估，手动检查 &#124;'
- en: '&#124; patches. &#124;'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 补丁。 &#124;'
- en: '| Yes |'
  id: totrans-912
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[161](#bib.bib161)] &#124;'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[161](#bib.bib161)] &#124;'
- en: '| 13 Smart Contract Bugs |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '| 13 个智能合约漏洞 |'
- en: '&#124; Success Rate, Contract level, Trial level &#124;'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 成功率，合同级别，试验级别 &#124;'
- en: '| Yes |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[142](#bib.bib142)] &#124;'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[142](#bib.bib142)] &#124;'
- en: '|'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Codeflaws,QuixBugs, &#124;'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Codeflaws,QuixBugs, &#124;'
- en: '&#124; ConDefects &#124;'
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ConDefects &#124;'
- en: '|'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Number of correctly fixed bugs, Number of plausibly patched bugs, &#124;'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修复的正确漏洞数量，可能修补的漏洞数量，&#124;'
- en: '&#124; Correctness rate of generated patches &#124;'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成补丁的正确率 &#124;'
- en: '| Yes |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[162](#bib.bib162)] &#124;'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[162](#bib.bib162)] &#124;'
- en: '| ARepair,Alloy4Fun |'
  id: totrans-929
  prefs: []
  type: TYPE_TB
  zh: '| ARepair,Alloy4Fun |'
- en: '&#124; Correct@6, Runtime and Token Usage &#124;'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Correct@6，运行时间和令牌使用 &#124;'
- en: '| Yes |'
  id: totrans-931
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[163](#bib.bib163)] &#124;'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[163](#bib.bib163)] &#124;'
- en: '| System Model Graph |'
  id: totrans-934
  prefs: []
  type: TYPE_TB
  zh: '| 系统模型图 |'
- en: '&#124; Accuracy, Effectiveness, Appropriateness &#124;'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性，效果，适当性 &#124;'
- en: '| Yes |'
  id: totrans-936
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[108](#bib.bib108)] &#124;'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[108](#bib.bib108)] &#124;'
- en: '|'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 1734 Positive Samples, &#124;'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1734 个正样本， &#124;'
- en: '&#124; 1810 Negative Samples &#124;'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1810 个负样本 &#124;'
- en: '|'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; F1 score, Accuracy, Consistency &#124;'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; F1 分数，准确性，一致性 &#124;'
- en: '| Yes |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[166](#bib.bib166)] &#124;'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[166](#bib.bib166)] &#124;'
- en: '|'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HumanEval,MBPP, &#124;'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HumanEval,MBPP, &#124;'
- en: '&#124; TransCoder &#124;'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TransCoder &#124;'
- en: '|'
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Accuracy, Pass@1 &#124;'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确性，Pass@1 &#124;'
- en: '| Yes |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[139](#bib.bib139)] &#124;'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[139](#bib.bib139)] &#124;'
- en: '|'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 13 Android Applications &#124;'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 13 个 Android 应用程序 &#124;'
- en: '&#124; from GitHub &#124;'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自 GitHub &#124;'
- en: '|'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Recall, Precision, Correct, Over-fitting, Correct@k &#124;'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 召回率，准确率，正确性，过拟合，Correct@k &#124;'
- en: '| Yes |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[164](#bib.bib164)] &#124;'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[164](#bib.bib164)] &#124;'
- en: '| System Model Graph |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
  zh: '| 系统模型图 |'
- en: '&#124; Relevance, Adequacy &#124;'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相关性，充分性 &#124;'
- en: '| Yes |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: '|'
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[109](#bib.bib109)] &#124;'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[109](#bib.bib109)] &#124;'
- en: '| 15 Vulnerabilities from CVE Lib | Success Rate | Yes |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '| 来自 CVE Lib 的 15 个漏洞 | 成功率 | 是 |'
- en: '|'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[46](#bib.bib46)] &#124;'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[46](#bib.bib46)] &#124;'
- en: '| Defects4J |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '| Defects4J |'
- en: '&#124; Plausible Fixes, Correct Fixes &#124;'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可能的修复，正确的修复 &#124;'
- en: '| Yes |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: X Discussion
  id: totrans-974
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 讨论
- en: X-A Experiment Models
  id: totrans-975
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-A 实验模型
- en: In section 3-8, we reviewed and introduced the research on LLMs and LLM-based
    agent applications in software engineering in recent years. These studies have
    different research directions and we divided them into six subtopics for classification
    and discussion. With the advancement of large language models, thousands of models
    have appeared in the public eye, in order to more intuitively understand the application
    of large language models in various fields and the use of large language models
    as the core of intelligent agents, we summarized a total of 117 papers, mainly
    to discuss the frequency of use of LLMs in the field of software engineering.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3-8节中，我们回顾并介绍了近年来LLM和基于LLM的代理在软件工程中的研究。这些研究具有不同的研究方向，我们将其分为六个子主题进行分类和讨论。随着大型语言模型的进步，成千上万的模型出现在公众视野中。为了更直观地理解大型语言模型在各个领域的应用以及将大型语言模型作为智能代理核心的使用，我们总结了117篇论文，主要讨论LLM在软件工程领域的使用频率。
- en: 'Based on the review of 117 papers, our primary focus is on the models or frameworks
    utilized by the authors in their experiments. This is due to the fact that these
    papers often include tests of model performance in specific domains, such as evaluating
    LLaMA’s performance in code generation. Therefore, during our data collection
    process, we also included models used for comparison purposes, as these models
    often represent the state-of-the-art capabilities in their respective fields at
    the time of the study. In summary, across the 117 papers, we identified a total
    of 79 unique large language models. We visualized the frequency of these model
    names in a word cloud for a more intuitive representation, as shown in Figure.[9](#S10.F9
    "Figure 9 ‣ X-A Experiment Models ‣ X Discussion ‣ IX-E Evaluation Metrics ‣ IX
    Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"). From the figure, we can observe that models such as GPT-3.5, GPT-4,
    LLaMA2, and Codex are frequently used. Although close source LLMs cannot be locally
    deployed or further trained, their exceptional capabilities make them a popular
    choice for comparison in experiments or for data augmentation, where GPT-4 is
    used to generate additional data to support the research model frameworks. For
    instance, researchers might use OpenAI’s API to generate initial text and then
    employ locally deployed models for further processing and optimization [[76](#bib.bib76)] [[122](#bib.bib122)] [[119](#bib.bib119)] [[113](#bib.bib113)].'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对117篇论文的回顾，我们的主要关注点是作者在实验中使用的模型或框架。这是因为这些论文通常包括对特定领域模型性能的测试，例如评估LLaMA在代码生成中的表现。因此，在我们的数据收集过程中，我们还包括了用于比较目的的模型，因为这些模型往往代表了研究时所在领域的最先进能力。总之，在这117篇论文中，我们识别出79个独特的大型语言模型。我们将这些模型名称的频率可视化为词云，以便更直观地展示，如图所示。[9](#S10.F9
    "图9 ‣ X-A实验模型 ‣ X讨论 ‣ IX-E评估指标 ‣ IX软件安全与维护 ‣ VIII-E评估指标 ‣ VIII软件测试生成 ‣ VII-E评估指标
    ‣ VII软件设计与评估 ‣ VI-E评估指标 ‣ VI自主学习与决策 ‣ V-E评估指标 ‣ V代码生成与软件开发 ‣ IV-E评估指标 ‣ IV-D基准
    ‣ IV需求工程与文档 ‣ 从LLMs到基于LLM的软件工程代理：对当前挑战与未来的调查"). 从图中我们可以观察到，GPT-3.5、GPT-4、LLaMA2和Codex等模型被频繁使用。尽管封闭源的LLM无法本地部署或进一步训练，但其卓越的能力使其成为实验中常用的比较对象或数据增强工具，其中GPT-4被用于生成额外的数据以支持研究模型框架。例如，研究人员可能会使用OpenAI的API生成初始文本，然后使用本地部署的模型进行进一步处理和优化[[76](#bib.bib76)]
    [[122](#bib.bib122)] [[119](#bib.bib119)] [[113](#bib.bib113)]。
- en: Therefore, it is not difficult to see that the use of general large models with
    superior performance to assist development or as a measurement standard has been
    increasingly used in the vertical field of software engineering in the past two
    years. In addition, for some fields that have never been touched by LLMs before,
    many researchers first refer to the model ChatGPT and conduct various performance
    experiments on the newer GPT-4 [[55](#bib.bib55)] [[58](#bib.bib58)] [[64](#bib.bib64)].
    Those models can be integrated into larger systems and combining with other machine
    learning models and tools, these models can be used to generate natural language
    responses, while another model handles intent recognition and dialogue management.
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，过去两年中，使用性能优越的一般大型模型来辅助开发或作为衡量标准，在软件工程的垂直领域中已越来越普遍。此外，对于一些以前从未接触过LLMs的领域，许多研究人员首先参考了模型**ChatGPT**并在更新的**GPT-4**上进行各种性能实验[[55](#bib.bib55)]
    [[58](#bib.bib58)] [[64](#bib.bib64)]。这些模型可以集成到更大的系统中，并结合其他机器学习模型和工具，这些模型可以用于生成自然语言响应，而另一个模型则处理意图识别和对话管理。
- en: '![Refer to caption](img/929e7d86fc687057d2812387cc6c2ca2.png)'
  id: totrans-979
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/929e7d86fc687057d2812387cc6c2ca2.png)'
- en: 'Figure 9: Experiment Models Usage WordCloud'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：实验模型使用词云
- en: 'Although the word cloud provides a rough overview of model usage frequency,
    it lacks detailed information. To gain deeper insights, we combined a grouped
    bar chart and a stacked bar chart to further analyze the usage of models in studies
    across different subtopics. The corresponding bar charts are presented in Figure.[10](#S10.F10
    "Figure 10 ‣ X-A Experiment Models ‣ X Discussion ‣ IX-E Evaluation Metrics ‣
    IX Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"). During the analysis, we found that a large number of models appeared
    only once. Including these in the bar chart would have made the overall representation
    cluttered. Therefore, we excluded models that appeared only once and focused on
    the versatility of the remaining models. On the left side of each subtopic, we
    depict the models used in LLM-related studies, with the models used in LLM-based
    agent-related studies highlighted in red-bordered bars. From the figure, it is
    evident that in the Autonomous Learning and Decision Making subtopic, the number
    of models used in LLM-based agent-related studies is quite high. Specifically,
    GPT-4 and GPT-3.5 were used in 10 out of 18 papers and 15 out of 18 papers, respectively.
    In this subtopic, studies commonly utilized GPT-3.5/4 and LLaMA-2 for research
    and evaluation. During our analysis, we found that many studies on LLM-based agents
    evaluated the agents’ ability to mimic human behavior and decision-making or perform
    some reasoning tasks[[103](#bib.bib103)] [[111](#bib.bib111)] [[108](#bib.bib108)].
    Since these studies do not require local deployment, they mainly assess the performance
    of state-of-the-art models in specific directions, leading to the frequent use
    of the GPT-family models. Frameworks like [[98](#bib.bib98)] [[36](#bib.bib36)]
    constructed LLM-based agents by calling the GPT-4 API, using verbal reinforcement
    to help language agents learn from their mistakes. Due to the limitations of GPT
    models, many studies also used LLaMA as the LLM for agents, fine-tuning it on
    the generated datasets to evaluate the emergence of knowledge and capabilities.
    Overall, we found that in the Autonomous Learning and Decision Making subtopic,
    LLM-based agents often use multiple models for testing and performance evaluation
    in a single task, this results in a significantly higher model usage frequency
    in this topic compared to others.'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词云提供了模型使用频率的大致概览，但缺乏详细信息。为了获得更深入的见解，我们结合了分组条形图和堆叠条形图，进一步分析了不同子主题中模型的使用情况。相应的条形图如图中所示。[10](#S10.F10
    "图 10 ‣ X-A 实验模型 ‣ X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全与维护 ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣
    VII-E 评估指标 ‣ VII 软件设计与评估 ‣ VI-E 评估指标 ‣ VI 自主学习与决策 ‣ V-E 评估指标 ‣ V 代码生成与软件开发 ‣ IV-E
    评估指标 ‣ IV-D 基准 ‣ IV 需求工程与文档 ‣ 从 LLM 到 LLM 基础的代理：当前挑战与未来的调查")。在分析过程中，我们发现大量模型仅出现了一次。将这些模型纳入条形图会使整体表示变得混乱。因此，我们排除了仅出现一次的模型，专注于其余模型的多样性。在每个子主题的左侧，我们展示了在
    LLM 相关研究中使用的模型，红色边框的条形图突出显示了在 LLM 基础代理相关研究中使用的模型。从图中可以明显看出，在自主学习与决策子主题中，LLM 基础代理相关研究使用的模型数量相当高。具体来说，GPT-4
    和 GPT-3.5 分别在 18 篇论文中的 10 篇和 15 篇中被使用。在这一子主题中，研究通常使用 GPT-3.5/4 和 LLaMA-2 进行研究和评估。在我们的分析中，我们发现许多关于
    LLM 基础代理的研究评估了代理模仿人类行为和决策的能力或执行一些推理任务[[103](#bib.bib103)] [[111](#bib.bib111)] [[108](#bib.bib108)]。由于这些研究不需要本地部署，它们主要评估最先进模型在特定方向上的表现，这导致
    GPT 系列模型的使用频率较高。像[[98](#bib.bib98)] [[36](#bib.bib36)]这样的框架通过调用 GPT-4 API 构建 LLM
    基础代理，使用语言强化帮助语言代理从错误中学习。由于 GPT 模型的局限性，许多研究还使用了 LLaMA 作为代理的 LLM，在生成的数据集上进行微调，以评估知识和能力的出现。总体而言，我们发现，在自主学习与决策子主题中，LLM
    基础代理经常使用多个模型进行单一任务的测试和性能评估，这使得该主题中的模型使用频率显著高于其他主题。
- en: '![Refer to caption](img/bee54abf0763d401f03f5fb3e6db9923.png)'
  id: totrans-982
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bee54abf0763d401f03f5fb3e6db9923.png)'
- en: 'Figure 10: Experiment Models Usage in Different Subtopics (REQ DENOTES ”Requirement
    Engineering and Documentation”, CODE DENOTES ”Code Generation and Software Development”,
    AUTO DENOTES ”Autonomous Learning and Decision Making”, DES DENOTES ”Software
    Design and Decision Making”, SEC DENOTES ”Software Security and Maintenance”)'
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：不同子主题中的实验模型使用情况（REQ 表示“需求工程和文档”，CODE 表示“代码生成和软件开发”，AUTO 表示“自主学习和决策制定”，DES
    表示“软件设计和决策制定”，SEC 表示“软件安全和维护”）
- en: Not only in the Autonomous Learning and Decision Making subtopic, but also across
    other themes, we observe that the variety of models (represented by the number
    of colors) used by LLM-based agents is relatively limited. For instance, in the
    requirement engineering and documentation subtopic, only GPT-3.5 and GPT-4 models
    were involved in the experiments. To analyze the reasons behind this phenomenon,
    we need to exclude the factors that models appearing only once were not considered
    and that there are inherently fewer studies on intelligent agents. We believe
    this primarily reflects the integration relationship between the agents and the
    large language models. The combination of these two technologies aims to address
    the limitations of large language models in specific tasks or aspects. Intelligent
    agents allow researchers to design a more flexible framework and incorporate the
    large language model into it. These models, having been trained on vast amounts
    of data, possess strong generalizability, making them suitable for a wide range
    of tasks and domains.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅在自主学习和决策制定子主题中，而且在其他主题中，我们观察到由基于大语言模型的智能体使用的模型种类（以颜色数量表示）相对较少。例如，在需求工程和文档子主题中，实验中仅涉及了GPT-3.5和GPT-4模型。为了分析这一现象的原因，我们需要排除模型仅出现一次未被考虑的因素，以及智能体研究本身较少的因素。我们认为这主要反映了智能体与大语言模型之间的整合关系。这两种技术的结合旨在解决大语言模型在特定任务或方面的局限性。智能体允许研究人员设计一个更灵活的框架，并将大语言模型融入其中。这些模型经过大量数据训练，具备强大的泛化能力，使其适用于广泛的任务和领域。
- en: 'Therefore, researchers and developers can use the same model to address multiple
    issues, reducing the need for various models. In code generation [[83](#bib.bib83)] [[79](#bib.bib79)],
    test case generation [[140](#bib.bib140)] [[142](#bib.bib142)], and software security [[167](#bib.bib167)] [[159](#bib.bib159)],
    there are instances of using CodeLlama. This model is fine-tuned and optimized
    based on the LLaMA architecture. At its release, it was considered one of the
    state-of-the-art models for code generation and understanding tasks, showing strong
    performance and potential compared to other models like Codex. Another potential
    reason is the previous successful applications and research outcomes that have
    proven these models’ effectiveness, further enhancing researchers’ trust and reliance
    on them. Compared to models that perform well in specific domains, in intelligent
    agent development, there is a preference for using general-purpose large models
    to ensure that the core of the agent possesses excellent text comprehension abilities,
    allowing for further reasoning, planning, and task execution. From the Figure.[10](#S10.F10
    "Figure 10 ‣ X-A Experiment Models ‣ X Discussion ‣ IX-E Evaluation Metrics ‣
    IX Software Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software
    Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation
    ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation
    Metrics ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics
    ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs
    to LLM-based Agents for Software Engineering: A Survey of Current, Challenges
    and Future"), we can also observe that research in the code generation and software
    development fields adopts a wide variety of models, further indicating the extensive
    attention this area receives and the excellent performance of models in code generation
    task.'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员和开发者可以使用相同的模型来解决多个问题，减少对不同模型的需求。在代码生成 [[83](#bib.bib83)] [[79](#bib.bib79)]、测试用例生成 [[140](#bib.bib140)] [[142](#bib.bib142)]以及软件安全 [[167](#bib.bib167)] [[159](#bib.bib159)]中，都可以看到CodeLlama的应用。这一模型是在LLaMA架构基础上进行微调和优化的。在发布时，它被认为是代码生成和理解任务中的前沿模型之一，相比于Codex等其他模型，展现出了强劲的性能和潜力。另一个潜在原因是之前成功的应用和研究成果证明了这些模型的有效性，进一步增强了研究人员的信任和依赖。与在特定领域表现优异的模型相比，在智能体开发中，更倾向于使用通用的大型模型，以确保智能体的核心具有出色的文本理解能力，从而实现进一步的推理、规划和任务执行。从图[10](#S10.F10
    "图 10 ‣ X-A 实验模型 ‣ X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全与维护 ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣
    VII-E 评估指标 ‣ VII 软件设计与评估 ‣ VI-E 评估指标 ‣ VI 自主学习与决策 ‣ V-E 评估指标 ‣ V 代码生成与软件开发 ‣ IV-E
    评估指标 ‣ IV-D 基准 ‣ IV 需求工程与文档 ‣ 从LLMs到基于LLM的智能体在软件工程中的应用：当前、挑战与未来"), 我们还可以观察到，在代码生成和软件开发领域的研究采用了各种模型，进一步表明这一领域受到广泛关注，以及模型在代码生成任务中的优异表现。
- en: '![Refer to caption](img/6b35cd69bce82d112ffddb88affa9815.png)'
  id: totrans-986
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6b35cd69bce82d112ffddb88affa9815.png)'
- en: 'Figure 11: Distribution of LLMs and Agents across six topics'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：LLMs和智能体在六个主题中的分布
- en: '|  | CODE | REQ | AUTO | DESIGN | SEC | TEST |'
  id: totrans-988
  prefs: []
  type: TYPE_TB
  zh: '|  | 代码 | 需求 | 自动 | 设计 | 安全 | 测试 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-989
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| CODE | X | 1 | 0 | 2 | 3 | 1 |'
  id: totrans-990
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | X | 1 | 0 | 2 | 3 | 1 |'
- en: '| REQ | 1 | X | 1 | 0 | 2 | 0 |'
  id: totrans-991
  prefs: []
  type: TYPE_TB
  zh: '| 需求 | 1 | X | 1 | 0 | 2 | 0 |'
- en: '| AUTO | 0 | 1 | X | 6 | 5 | 1 |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| 自动 | 0 | 1 | X | 6 | 5 | 1 |'
- en: '| DESIGN | 2 | 0 | 6 | X | 1 | 0 |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| 设计 | 2 | 0 | 6 | X | 1 | 0 |'
- en: '| SEC | 3 | 2 | 5 | 1 | X | 2 |'
  id: totrans-994
  prefs: []
  type: TYPE_TB
  zh: '| 安全 | 3 | 2 | 5 | 1 | X | 2 |'
- en: '| TEST | 1 | 0 | 1 | 0 | 2 | X |'
  id: totrans-995
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 1 | 0 | 1 | 0 | 2 | X |'
- en: 'TABLE XI: Overlap of Papers Among Different Topics (REQ DENOTES ”Requirement
    Engineering and Documentation”, CODE DENOTES ”Code Generation and Software Development”,
    AUTO DENOTES ”Autonomous Learning and Decision Making”, DES DENOTES ”Software
    Design and Decision Making”, SEC DENOTES ”Software Security and Maintenance”)'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 表XI：不同主题论文的重叠（需求指“需求工程与文档”，代码指“代码生成与软件开发”，自动指“自主学习与决策”，设计指“软件设计与决策”，安全指“软件安全与维护”）
- en: X-B Topics Overlapping
  id: totrans-997
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-B 主题重叠
- en: 'Figure.[11](#S10.F11 "Figure 11 ‣ X-A Experiment Models ‣ X Discussion ‣ IX-E
    Evaluation Metrics ‣ IX Software Security and Maintenance ‣ VIII-E Evaluation
    Metrics ‣ VIII Software Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software
    Design and Evaluation ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision
    Making ‣ V-E Evaluation Metrics ‣ V Code Generation and Software Development ‣
    IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and
    Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future") shows the distribution of all collected literature
    across six themes. For LLM-type literature, the theme of software security and
    maintenance accounts for nearly 30%, whereas test case generation accounts for
    less than 10%. This trend is similarly reflected in the LLM-based agent literature.
    Research on using LLM-based agents to address requirements engineering and test
    case generation is relatively sparse. Requirements engineering is a new endeavor
    for LLM-based agents, and using the entire agent framework to generate test cases
    might be considered excessive. Therefore, more research tends to evaluate and
    explore the changes LLMs bring within the agent framework, such as autonomous
    decision-making abilities and capabilities in software maintenance and repair.'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 图。[11](#S10.F11 "图 11 ‣ X-A 实验模型 ‣ X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全与维护 ‣ VIII-E 评估指标
    ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计与评估 ‣ VI-E 评估指标 ‣ VI 自主学习与决策 ‣ V-E 评估指标 ‣
    V 代码生成与软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程与文档 ‣ 从 LLM 到基于 LLM 的软件工程代理：当前挑战与未来的调查")
    显示了所有收集到的文献在六个主题中的分布。对于 LLM 类型的文献，软件安全与维护的主题占近 30%，而测试用例生成的占比不到 10%。这一趋势在基于 LLM
    的代理文献中也有所体现。利用基于 LLM 的代理来解决需求工程和测试用例生成的研究相对稀少。需求工程是基于 LLM 的代理的新领域，而使用整个代理框架来生成测试用例可能被认为是过度的。因此，更多的研究倾向于评估和探索
    LLM 在代理框架内带来的变化，例如自主决策能力以及在软件维护和修复中的能力。
- en: 'Table‘[XI](#S10.T11 "TABLE XI ‣ X-A Experiment Models ‣ X Discussion ‣ IX-E
    Evaluation Metrics ‣ IX Software Security and Maintenance ‣ VIII-E Evaluation
    Metrics ‣ VIII Software Test Generation ‣ VII-E Evaluation Metrics ‣ VII Software
    Design and Evaluation ‣ VI-E Evaluation Metrics ‣ VI Autonomous Learning and Decision
    Making ‣ V-E Evaluation Metrics ‣ V Code Generation and Software Development ‣
    IV-E Evaluation Metrics ‣ IV-D Benchmarks ‣ IV Requirement Engineering and and
    Documentation ‣ From LLMs to LLM-based Agents for Software Engineering: A Survey
    of Current, Challenges and Future") presents the number of papers spanning multiple
    themes. For instance, five papers can be classified under both software security
    and maintenance and autonomous learning and decision making. These two themes
    also overlap the most with other themes, indicating that LLMs and LLM-based agent
    research is broad and these tasks often require integrating knowledge and techniques
    from various fields such as code generation, design, and testing. The significant
    overlap reflects the close interrelation between these themes and other areas.
    For example, autonomous learning and decision-making often involve the model’s
    ability to autonomously learn and optimize decision trees, techniques that are
    applied in many specific software engineering tasks. Similarly, software security
    and maintenance typically require a combination of multiple techniques to enhance
    security, such as automatic code generation tools and automated testing frameworks[[71](#bib.bib71)] [[80](#bib.bib80)] [[83](#bib.bib83)] [[102](#bib.bib102)].
    The overlap in literature highlights the increasing need for integrating methods
    and techniques from different research areas within software engineering. For
    instance, ensuring software security relies not only on security measures but
    also on leveraging code generation, automated testing, and design optimization
    technologies. Similarly, autonomous learning and decision-making require a comprehensive
    consideration of requirements engineering, code generation, and system design.
    Moreover, it suggests that certain technologies and methods possess strong commonality.
    For instance, LLM-based agents enhance capabilities in code generation, test automation,
    and security analysis through autonomous learning and decision-making. This sharing
    of technologies promotes knowledge exchange and technological dissemination across
    various fields within software engineering.'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 表‘[XI](#S10.T11 "表 XI ‣ X-A 实验模型 ‣ X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全与维护 ‣ VIII-E 评估指标
    ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计与评估 ‣ VI-E 评估指标 ‣ VI 自主学习与决策 ‣ V-E 评估指标 ‣
    V 代码生成与软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程与文档 ‣ 从 LLM 到基于 LLM 的软件工程代理：现状、挑战与未来调查")
    展示了跨多个主题的论文数量。例如，有五篇论文可以同时归类于软件安全与维护和自主学习与决策。这两个主题也与其他主题的重叠度最高，表明 LLM 和基于 LLM
    的代理研究范围广泛，这些任务通常需要整合来自不同领域的知识和技术，如代码生成、设计和测试。显著的重叠反映了这些主题与其他领域之间的紧密关联。例如，自主学习和决策通常涉及模型自主学习和优化决策树的能力，这些技术在许多具体的软件工程任务中得到应用。类似地，软件安全和维护通常需要多种技术的组合来增强安全性，如自动代码生成工具和自动化测试框架[[71](#bib.bib71)] [[80](#bib.bib80)] [[83](#bib.bib83)] [[102](#bib.bib102)]。文献中的重叠突显了在软件工程不同研究领域之间整合方法和技术的日益需求。例如，确保软件安全不仅依赖于安全措施，还依赖于利用代码生成、自动化测试和设计优化技术。类似地，自主学习和决策需要全面考虑需求工程、代码生成和系统设计。此外，这表明某些技术和方法具有强烈的共性。例如，基于
    LLM 的代理通过自主学习和决策增强了代码生成、测试自动化和安全分析的能力。这种技术共享促进了知识交流和技术传播，贯穿软件工程的各个领域。
- en: X-C Benchmarks and Metrics
  id: totrans-1000
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-C 基准与指标
- en: 'As shown in Figure.[12](#A0.F12 "Figure 12 ‣ -A Benchmarks ‣ XI Conclusion
    ‣ X-C Benchmarks and Metrics ‣ X Discussion ‣ IX-E Evaluation Metrics ‣ IX Software
    Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software Test Generation
    ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation
    Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation Metrics
    ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D
    Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future"),
    it include the distribution of common benchmarks across six topics. In reality,
    the number of benchmark datasets used is far greater than what is shown in the
    figure. Different software engineering tasks use various benchmark datasets for
    evaluation and testing. For instance, in requirements engineering, researchers
    often collect user stories or requirement specifications as datasets [[55](#bib.bib55)] [[63](#bib.bib63)],
    and these datasets are not well-known public datasets, so they were not included
    in the statistics. Alternatively, some studies specify their datasets as “Customized
    GitHub datasets” [[168](#bib.bib168)]. Therefore, the benchmark datasets shown
    in the figure represent commonly used public datasets. For example, MBPP and HumanEval,
    which have been introduced in previous sections, are frequently used. We can also
    observe that the datasets used in LLM and LLM-based agents tasks, apart from common
    public datasets, are different.'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示[12](#A0.F12 "图 12 ‣ - 基准 ‣ XI 结论 ‣ X-C 基准和指标 ‣ X 讨论 ‣ IX-E 评估指标 ‣ IX 软件安全性和维护
    ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计和评估 ‣ VI-E 评估指标 ‣ VI 自主学习和决策
    ‣ V-E 评估指标 ‣ V 代码生成和软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程和文档 ‣ 从 LLM 到 LLM 基于的代理：当前、挑战和未来的调查")，它包含了在六个主题中常见基准的分布。
    实际上，使用的基准数据集数量远远超过图中所示的数量。 不同的软件工程任务使用各种基准数据集进行评估和测试。 例如，在需求工程中，研究人员通常收集用户故事或需求规格作为数据集[[55](#bib.bib55)]
    [[63](#bib.bib63)]，这些数据集并非众所周知的公开数据集，因此未纳入统计。 或者，一些研究将其数据集指定为“定制的 GitHub 数据集”[[168](#bib.bib168)]。
    因此，图中所示的基准数据集代表了常用的公开数据集。 例如，前面部分介绍的 MBPP 和 HumanEval 被频繁使用。 我们还可以观察到，LLM 和 LLM
    基于的代理任务中使用的数据集，除了常见的公开数据集外，还有所不同。
- en: For instance, the FEVER⁷⁷7[https://fever.ai/dataset/fever.html](https://fever.ai/dataset/fever.html)
    dataset is often used in agent-related research. In[[35](#bib.bib35)], the FEVER
    dataset is used to test the ExpeL agent’s performance in fact verification tasks.
    Similarly, the HotpotQA⁸⁸8[https://hotpotqa.github.io/](https://hotpotqa.github.io/)
    dataset is frequently used in agent-related research for knowledge-intensive reasoning
    and question-answering tasks. When handling vulnerability repair tasks, LLMs often
    use the Defects4J⁹⁹9[https://github.com/rjust/defects4j](https://github.com/rjust/defects4j)
    benchmark dataset. This dataset contains 835 real-world defects from multiple
    open-source Java projects, categorized into buggy versions and repaired versions,
    typically used to evaluate the effectiveness of automated program repair techniques.
    Despite its extensive use in LLM research, Defects4J is relatively less used in
    LLM-based agents research. We speculate that this may be because Defects4J primarily
    evaluates single code repair tasks, which do not fully align with the multi-task
    and real-time requirements of LLM-based agents. Additionally, new datasets like
    ConDefects have been introduced[[142](#bib.bib142)], focusing on addressing data
    leakage issues and providing more comprehensive defect localization and repair
    evaluations.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，FEVER⁷⁷7[https://fever.ai/dataset/fever.html](https://fever.ai/dataset/fever.html)
    数据集在与代理相关的研究中经常使用。 在[[35](#bib.bib35)]中，FEVER 数据集用于测试 ExpeL 代理在事实验证任务中的性能。 类似地，HotpotQA⁸⁸8[https://hotpotqa.github.io/](https://hotpotqa.github.io/)
    数据集在与代理相关的研究中经常用于知识密集型推理和问答任务。 在处理漏洞修复任务时，LLM 通常使用 Defects4J⁹⁹9[https://github.com/rjust/defects4j](https://github.com/rjust/defects4j)
    基准数据集。 该数据集包含来自多个开源 Java 项目的 835 个真实缺陷，分为有缺陷版本和修复版本，通常用于评估自动程序修复技术的有效性。 尽管在 LLM
    研究中广泛使用，Defects4J 在 LLM 基于的代理研究中相对较少使用。 我们推测这可能是因为 Defects4J 主要评估单一代码修复任务，这与 LLM
    基于的代理的多任务和实时要求不完全匹配。 此外，像 ConDefects 这样的新数据集已经被引入[[142](#bib.bib142)]，专注于解决数据泄漏问题，并提供更全面的缺陷定位和修复评估。
- en: 'As shown in Figure.[13](#A0.F13 "Figure 13 ‣ -B Evaluation Metrics ‣ XI Conclusion
    ‣ X-C Benchmarks and Metrics ‣ X Discussion ‣ IX-E Evaluation Metrics ‣ IX Software
    Security and Maintenance ‣ VIII-E Evaluation Metrics ‣ VIII Software Test Generation
    ‣ VII-E Evaluation Metrics ‣ VII Software Design and Evaluation ‣ VI-E Evaluation
    Metrics ‣ VI Autonomous Learning and Decision Making ‣ V-E Evaluation Metrics
    ‣ V Code Generation and Software Development ‣ IV-E Evaluation Metrics ‣ IV-D
    Benchmarks ‣ IV Requirement Engineering and and Documentation ‣ From LLMs to LLM-based
    Agents for Software Engineering: A Survey of Current, Challenges and Future"),
    it includes the top ten evaluation metrics for LLMs and LLM-based agents. The
    analysis reveals that the evaluation methods used by both are almost identical.
    In previous sections, we also discussed that for agents, it is necessary to consider
    time and computational resource consumption, which is evident from the pie chart.
    Meanwhile, many studies focus on the code generation capabilities of LLMs, so
    more evaluation metrics pertain to the correctness and Exact Match of the generated
    code [[73](#bib.bib73)] [[69](#bib.bib69)] [[30](#bib.bib30)], but overall, the
    evaluation metrics for LLMs and LLM-based agents in software engineering applications
    are quite similar.'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示[13](#A0.F13 "图 13 ‣ -B 评估指标 ‣ XI 结论 ‣ X-C 基准和指标 ‣ X 讨论 ‣ IX-E 评估指标 ‣ IX
    软件安全与维护 ‣ VIII-E 评估指标 ‣ VIII 软件测试生成 ‣ VII-E 评估指标 ‣ VII 软件设计与评估 ‣ VI-E 评估指标 ‣ VI
    自主学习与决策 ‣ V-E 评估指标 ‣ V 代码生成与软件开发 ‣ IV-E 评估指标 ‣ IV-D 基准 ‣ IV 需求工程与文档 ‣ 从 LLM 到
    LLM 基于代理的软件工程：当前挑战与未来展望")，其中包括 LLM 和 LLM 基于代理的前十名评估指标。分析揭示，两者使用的评估方法几乎相同。在前面的章节中，我们也讨论了对于代理而言，需要考虑时间和计算资源消耗，这从饼图中显而易见。同时，许多研究关注于
    LLM 的代码生成能力，因此更多的评估指标涉及生成代码的正确性和精确匹配[[73](#bib.bib73)] [[69](#bib.bib69)] [[30](#bib.bib30)]，但总体而言，LLM
    和 LLM 基于代理在软件工程应用中的评估指标非常相似。
- en: XI Conclusion
  id: totrans-1004
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XI 结论
- en: 'In this paper, we conducted a comprehensive literature review on the application
    of LLM and LLM-based agents in software engineering. We categorized software engineering
    into six topics: requirement engineering and documentation, code generation and
    software development, autonomous learning and decision making, software design
    and evaluation, software test generation, and software security and maintenance.
    For each topic, we analyzed the tasks, benchmarks, and evaluation metrics, distinguishing
    between LLM and LLM-based agents and discussing the differences and impacts they
    bring. We further analyzed and discussed the models used in the experiments of
    the 117 collected papers. Additionally, we provided statistics and distinctions
    between LLM and LLM-based agents regarding datasets and evaluation metrics. The
    analysis revealed that the emergence of LLM-based agents has led to extensive
    research and applications across various software engineering topics, demonstrating
    different emphases compared to traditional LLMs in terms of tasks, benchmarks,
    and evaluation metrics.'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对 LLM 和 LLM 基于代理在软件工程中的应用进行了全面的文献综述。我们将软件工程分为六个主题：需求工程与文档、代码生成与软件开发、自主学习与决策、软件设计与评估、软件测试生成以及软件安全与维护。针对每个主题，我们分析了任务、基准和评估指标，区分了
    LLM 和 LLM 基于代理，并讨论了它们带来的差异和影响。我们进一步分析了 117 篇收集论文中的实验所用模型。此外，我们提供了 LLM 和 LLM 基于代理在数据集和评估指标方面的统计数据和区别。分析揭示
    LLM 基于代理的出现推动了各种软件工程主题的广泛研究和应用，与传统 LLM 在任务、基准和评估指标上相比表现出了不同的重点。
- en: References
  id: totrans-1006
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Wang, D. Chollak, D. Movshovitz-Attias, and L. Tan, “Bugram: bug detection
    with n-gram language models,” in Proceedings of the 31st IEEE/ACM International
    Conference on Automated Software Engineering, pp. 724–735, 2016.'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Wang, D. Chollak, D. Movshovitz-Attias, 和 L. Tan, “Bugram: 使用 n-gram
    语言模型进行错误检测，” 见第 31 届 IEEE/ACM 国际自动化软件工程会议论文集，页 724–735，2016 年。'
- en: '[2] A. Vogelsang and M. Borg, “Requirements engineering for machine learning:
    Perspectives from data scientists,” in 2019 IEEE 27th International Requirements
    Engineering Conference Workshops (REW), (Jeju, Korea (South)), pp. 245–251, 2019.'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Vogelsang 和 M. Borg, “机器学习的需求工程：数据科学家的视角，” 见 2019 IEEE 第 27 届国际需求工程会议研讨会
    (REW)，（韩国济州岛），页 245–251，2019 年。'
- en: '[3] “Chatgpt: Optimizing language models for dialogue,” 11 2022. [Online; accessed
    17-July-2024].'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] “ChatGPT：对话优化的语言模型”，2022 年 11 月。[在线；访问日期：2024年7月17日]。'
- en: '[4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” arXiv preprint
    arXiv:2107.03374, 2021. arXiv:2107.03374 [cs.LG].'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, 和 W. Zaremba，“评估基于代码训练的大型语言模型”，arXiv 预印本 arXiv:2107.03374，2021 年。arXiv:2107.03374
    [cs.LG]。'
- en: '[5] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani,
    and R. Sharma, “Jigsaw: large language models meet program synthesis,” in Proceedings
    of the 44th International Conference on Software Engineering, ICSE ’22, (New York,
    NY, USA), p. 1219–1231, Association for Computing Machinery, 2022.'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani,
    和 R. Sharma，“Jigsaw：大型语言模型遇见程序合成”，发表于第 44 届国际软件工程大会，ICSE ’22，（美国纽约），第 1219–1231
    页，计算机协会，2022 年。'
- en: '[6] T. Li, G. Zhang, Q. D. Do, X. Yue, and W. Chen, “Long-context llms struggle
    with long in-context learning,” 2024.'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Li, G. Zhang, Q. D. Do, X. Yue, 和 W. Chen，“长上下文 LLM 在长时间上下文学习中的挑战”，2024
    年。'
- en: '[7] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin,
    and X. Hu, “Harnessing the power of llms in practice: A survey on chatgpt and
    beyond,” ACM Trans. Knowl. Discov. Data, vol. 18, apr 2024.'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong, B. Yin,
    和 X. Hu，“实践中利用 LLM 的力量：ChatGPT 及其更远的调查”，《ACM 知识发现数据学报》，第 18 卷，2024 年 4 月。'
- en: '[8] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and
    J. M. Zhang, “Large language models for software engineering: Survey and open
    problems,” in 2023 IEEE/ACM International Conference on Software Engineering:
    Future of Software Engineering (ICSE-FoSE), pp. 31–53, 2023.'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, 和 J.
    M. Zhang，“软件工程中的大型语言模型：调查和开放问题”，发表于 2023 IEEE/ACM 国际软件工程会议：软件工程的未来（ICSE-FoSE），第
    31–53 页，2023 年。'
- en: '[9] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, “A survey on large language model
    based autonomous agents,” Frontiers of Computer Science, vol. 18, no. 6, pp. 186345–,
    2024.'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, 和 J. Wen，“基于大型语言模型的自主体的调查”，《计算机科学前沿》，第 18
    卷，第 6 期，第 186345 页，2024 年。'
- en: '[10] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin,
    E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang, Y. Zou,
    X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng, X. Qiu,
    X. Huang, and T. Gui, “The rise and potential of large language model based agents:
    A survey,” 2023.'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S.
    Jin, E. Zhou, R. Zheng, X. Fan, X. Wang, L. Xiong, Y. Zhou, W. Wang, C. Jiang,
    Y. Zou, X. Liu, Z. Yin, S. Dou, R. Weng, W. Cheng, Q. Zhang, W. Qin, Y. Zheng,
    X. Qiu, X. Huang, 和 T. Gui，“基于大型语言模型的代理的崛起和潜力：调查”，2023 年。'
- en: '[11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler,
    M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented
    generation for knowledge-intensive nlp tasks,” in Advances in Neural Information
    Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,
    eds.), vol. 33, pp. 9459–9474, Curran Associates, Inc., 2020.'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H.
    Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, 和 D. Kiela，“用于知识密集型 NLP
    任务的检索增强生成”，发表于《神经信息处理系统进展》（H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan 和
    H. Lin 编辑），第 33 卷，第 9459–9474 页，Curran Associates, Inc.，2020 年。'
- en: '[12] GitHub, Inc., “Github copilot: Your ai pair programmer.” [https://github.com/features/copilot](https://github.com/features/copilot),
    2024. [Online; accessed 17-July-2024].'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] GitHub, Inc.，“Github copilot: Your ai pair programmer。” [https://github.com/features/copilot](https://github.com/features/copilot)，2024
    年。[在线；访问日期：2024年7月17日]。'
- en: '[13] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach.
    Pearson Education Limited, 2016.'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] S. Russell 和 P. Norvig, 《人工智能：一种现代方法》。Pearson Education Limited，2016。'
- en: '[14] N. R. Jennings, “A survey of agent-oriented software engineering,” Knowledge
    Engineering Review, vol. 15, no. 4, pp. 215–249, 2000.'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] N. R. Jennings，“面向代理的软件工程调查，” 知识工程评论，卷 15，第 4 期，页 215–249，2000。'
- en: '[15] Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata,
    A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian, “Experience grounds
    language,” 2020.'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Bisk, A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M.
    Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, 和 J. Turian，“经验奠定语言基础，”
    2020。'
- en: '[16] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,
    et al., “Chain-of-thought prompting elicits reasoning in large language models,”
    Advances in neural information processing systems, vol. 35, pp. 24824–24837, 2022.'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou 等，“链式思维提示引发大型语言模型的推理，” 神经信息处理系统进展，卷 35，页 24824–24837，2022。'
- en: '[17] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy,
    and H. Wang, “Large language models for software engineering: A systematic literature
    review,” 2024.'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy,
    和 H. Wang，“大型语言模型在软件工程中的应用：系统文献综述，” 2024。'
- en: '[18] Z. Zheng, K. Ning, J. Chen, Y. Wang, W. Chen, L. Guo, and W. Wang, “Towards
    an understanding of large language models in software engineering tasks,” 2023.'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Z. Zheng, K. Ning, J. Chen, Y. Wang, W. Chen, L. Guo, 和 W. Wang，“理解大型语言模型在软件工程任务中的作用，”
    2023。'
- en: '[19] A. Nguyen-Duc, B. Cabrero-Daniel, A. Przybylek, C. Arora, D. Khanna, T. Herda,
    U. Rafiq, J. Melegati, E. Guerra, K.-K. Kemell, M. Saari, Z. Zhang, H. Le, T. Quan,
    and P. Abrahamsson, “Generative artificial intelligence for software engineering
    – a research agenda,” 2023.'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Nguyen-Duc, B. Cabrero-Daniel, A. Przybylek, C. Arora, D. Khanna, T.
    Herda, U. Rafiq, J. Melegati, E. Guerra, K.-K. Kemell, M. Saari, Z. Zhang, H.
    Le, T. Quan, 和 P. Abrahamsson，“生成人工智能在软件工程中的应用——研究议程，” 2023。'
- en: '[20] W. Ma, S. Liu, Z. Lin, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, L. Li,
    and Y. Liu, “Lms: Understanding code syntax and semantics for code analysis,”
    2024.'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] W. Ma, S. Liu, Z. Lin, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, L. Li,
    和 Y. Liu，“Lms: 理解代码语法和语义用于代码分析，” 2024。'
- en: '[21] Z. Yang, Z. Sun, T. Z. Yue, P. Devanbu, and D. Lo, “Robustness, security,
    privacy, explainability, efficiency, and usability of large language models for
    code,” 2024.'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Z. Yang, Z. Sun, T. Z. Yue, P. Devanbu, 和 D. Lo，“大型语言模型的鲁棒性、安全性、隐私性、可解释性、效率和可用性，”
    2024。'
- en: '[22] Y. Huang, Y. Chen, X. Chen, J. Chen, R. Peng, Z. Tang, J. Huang, F. Xu,
    and Z. Zheng, “Generative software engineering,” 2024.'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Huang, Y. Chen, X. Chen, J. Chen, R. Peng, Z. Tang, J. Huang, F. Xu,
    和 Z. Zheng，“生成软件工程，” 2024。'
- en: '[23] C. Manning and H. Schutze, Foundations of statistical natural language
    processing. MIT press, 1999.'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] C. Manning 和 H. Schutze，《统计自然语言处理基础》。MIT Press，1999。'
- en: '[24] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” 神经计算，卷 9，第 8 期，页 1735–1780，1997。'
- en: '[25] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” 神经计算，卷 9，第 8 期，页 1735–1780，1997。'
- en: '[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural
    information processing systems, vol. 30, 2017.'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力即一切，” 神经信息处理系统进展，卷 30，2017。'
- en: '[27] L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and consequences,”
    Minds and Machines, vol. 30, pp. 681–694, 2020.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Floridi 和 M. Chiriatti，“Gpt-3: 其本质、范围、限制和影响，” 思维与机器，卷 30，页 681–694，2020。'
- en: '[28] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham,
    H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling language modeling
    with pathways,” Journal of Machine Learning Research, vol. 24, no. 240, pp. 1–113,
    2023.'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P.
    Barham, H. W. Chung, C. Sutton, S. Gehrmann 等，“Palm: 通过路径扩展语言建模，” 机器学习研究期刊，卷 24，第
    240 期，页 1–113，2023。'
- en: '[29] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,
    P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer, “Opt: Open pre-trained transformer
    language models,” 2022.'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,
    P. S. Koura, A. Sridhar, T. Wang, 和 L. Zettlemoyer，“Opt: 开放预训练变换器语言模型，” 2022。'
- en: '[30] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi, “Codet5+:
    Open code large language models for code understanding and generation,” arXiv
    preprint arXiv:2305.07922, 2023.'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, 和 S. C. Hoi，"Codet5+:
    开放代码大型语言模型用于代码理解和生成"，arXiv 预印本 arXiv:2305.07922，2023。'
- en: '[31] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” arXiv preprint
    arXiv:1810.04805, 2018.'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，"Bert: 深度双向变换器的预训练用于语言理解"，arXiv
    预印本 arXiv:1810.04805，2018。'
- en: '[32] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,”
    Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell 等人，"语言模型是少样本学习者"，《神经信息处理系统进展》，第 33 卷，页 1877–1901，2020。'
- en: '[33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., “Llama: Open and efficient
    foundation language models,” arXiv preprint arXiv:2302.13971, 2023.'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar 等人，"Llama: 开放而高效的基础语言模型"，arXiv 预印本 arXiv:2302.13971，2023。'
- en: '[34] J. X. Chen, “The evolution of computing: Alphago,” Computing in Science
    & Engineering, vol. 18, no. 4, pp. 4–7, 2016.'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. X. Chen，"计算的演变: Alphago"，《计算机科学与工程》，第 18 卷，第 4 期，页 4–7，2016。'
- en: '[35] A. Zhao, D. Huang, Q. Xu, M. Lin, Y.-J. Liu, and G. Huang, “Expel: Llm
    agents are experiential learners,” in Proceedings of the AAAI Conference on Artificial
    Intelligence, vol. 38, pp. 19632–19642, 2024.'
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Zhao, D. Huang, Q. Xu, M. Lin, Y.-J. Liu, 和 G. Huang，"Expel: LLM 代理是经验学习者"，在
    AAAI 人工智能会议论文集，第 38 卷，页 19632–19642，2024。'
- en: '[36] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,
    “React: Synergizing reasoning and acting in language models,” arXiv preprint arXiv:2210.03629,
    2022.'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, 和 Y. Cao，"React:
    在语言模型中协同推理与行动"，arXiv 预印本 arXiv:2210.03629，2022。'
- en: '[37] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models as zero-shot
    planners: Extracting actionable knowledge for embodied agents,” in International
    conference on machine learning, pp. 9118–9147, PMLR, 2022.'
  id: totrans-1043
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] W. Huang, P. Abbeel, D. Pathak, 和 I. Mordatch，"语言模型作为零样本规划者: 为具身代理提取可操作的知识"，在国际机器学习会议，页
    9118–9147，PMLR，2022。'
- en: '[38] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and
    A. Anandkumar, “Voyager: An open-ended embodied agent with large language models,”
    arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-1044
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, 和 A. Anandkumar，"Voyager:
    一个开放式具身代理与大型语言模型"，arXiv 预印本 arXiv:2305.16291，2023。'
- en: '[39] C. Whitehouse, M. Choudhury, and A. F. Aji, “Llm-powered data augmentation
    for enhanced cross-lingual performance,” 2023.'
  id: totrans-1045
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] C. Whitehouse, M. Choudhury, 和 A. F. Aji，"利用 LLM 增强的跨语言性能数据增强"，2023。'
- en: '[40] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar,
    J. Spencer-Smith, and D. C. Schmidt, “A prompt pattern catalog to enhance prompt
    engineering with chatgpt,” arXiv preprint arXiv:2302.11382, 2023.'
  id: totrans-1046
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea, H. Gilbert, A. Elnashar,
    J. Spencer-Smith, 和 D. C. Schmidt，"增强与 ChatGPT 进行提示工程的提示模式目录"，arXiv 预印本 arXiv:2302.11382，2023。'
- en: '[41] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac,
    R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., “Gemini 1.5: Unlocking
    multimodal understanding across millions of tokens of context,” arXiv preprint
    arXiv:2403.05530, 2024.'
  id: totrans-1047
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac,
    R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser 等人，"Gemini 1.5: 解锁跨越数百万标记的多模态理解"，arXiv
    预印本 arXiv:2403.05530，2024。'
- en: '[42] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,”
    ACM Computing Surveys, vol. 55, no. 12, pp. 1–38, 2023.'
  id: totrans-1048
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang,
    A. Madotto, 和 P. Fung，"自然语言生成中的幻觉调查"，《ACM 计算调查》，第 55 卷，第 12 期，页 1–38，2023。'
- en: '[43] K. An, F. Yang, L. Li, Z. Ren, H. Huang, L. Wang, P. Zhao, Y. Kang, H. Ding,
    Q. Lin, et al., “Nissist: An incident mitigation copilot based on troubleshooting
    guides,” arXiv preprint arXiv:2402.17531, 2024.'
  id: totrans-1049
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] K. An, F. Yang, L. Li, Z. Ren, H. Huang, L. Wang, P. Zhao, Y. Kang, H. Ding,
    Q. Lin 等人，"Nissist: 基于故障排除指南的事件缓解副驾驶"，arXiv 预印本 arXiv:2402.17531，2024。'
- en: '[44] J. Li, Q. Zhang, Y. Yu, Q. Fu, and D. Ye, “More agents is all you need,”
    2024.'
  id: totrans-1050
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Li, Q. Zhang, Y. Yu, Q. Fu, 和 D. Ye，"更多的代理就是你所需要的"，2024。'
- en: '[45] Y. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,
    P. S. Liang, and T. B. Hashimoto, “Alpacafarm: A simulation framework for methods
    that learn from human feedback,” Advances in Neural Information Processing Systems,
    vol. 36, 2024.'
  id: totrans-1051
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Dubois, C. X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin,
    P. S. Liang, 和 T. B. Hashimoto, “Alpacafarm：一个从人类反馈中学习的方法模拟框架，” 《神经信息处理系统进展》，第36卷，2024。'
- en: '[46] I. Bouzenia, P. Devanbu, and M. Pradel, “Repairagent: An autonomous, llm-based
    agent for program repair,” arXiv preprint arXiv:2403.17134, 2024.'
  id: totrans-1052
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] I. Bouzenia, P. Devanbu, 和 M. Pradel, “Repairagent：一个基于大型语言模型的自主程序修复代理，”
    arXiv 预印本 arXiv:2403.17134, 2024。'
- en: '[47] E. Musumeci, M. Brienza, V. Suriani, D. Nardi, and D. D. Bloisi, “Llm
    based multi-agent generation of semi-structured documents from semantic templates
    in the public administration domain,” in International Conference on Human-Computer
    Interaction, pp. 98–117, Springer, 2024.'
  id: totrans-1053
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] E. Musumeci, M. Brienza, V. Suriani, D. Nardi, 和 D. D. Bloisi, “基于大型语言模型的多智能体从公共行政领域的语义模板生成半结构化文档，”
    发表在《国际人机交互会议》，pp. 98–117, Springer, 2024。'
- en: '[48] X. Luo, Y. Xue, Z. Xing, and J. Sun, “Prcbert: Prompt learning for requirement
    classification using bert-based pretrained language models,” in Proceedings of
    the 37th IEEE/ACM International Conference on Automated Software Engineering,
    pp. 1–13, 2022.'
  id: totrans-1054
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Luo, Y. Xue, Z. Xing, 和 J. Sun, “Prcbert：使用基于 BERT 的预训练语言模型进行需求分类的提示学习，”
    发表在《第37届 IEEE/ACM 国际自动化软件工程会议论文集》，pp. 1–13, 2022。'
- en: '[49] T. Hey, J. Keim, A. Koziolek, and W. F. Tichy, “Norbert: Transfer learning
    for requirements classification,” in 2020 IEEE 28th International Requirements
    Engineering Conference (RE), pp. 169–179, 2020.'
  id: totrans-1055
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] T. Hey, J. Keim, A. Koziolek, 和 W. F. Tichy, “Norbert：用于需求分类的迁移学习，” 发表在《2020
    IEEE第28届国际需求工程会议（RE）》，pp. 169–179, 2020。'
- en: '[50] J. Zhang, Y. Chen, N. Niu, and C. Liu, “Evaluation of chatgpt on requirements
    information retrieval under zero-shot setting,” Available at SSRN 4450322, 2023.'
  id: totrans-1056
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Zhang, Y. Chen, N. Niu, 和 C. Liu, “在零样本设置下评估 ChatGPT 对需求信息检索的表现，” 可在
    SSRN 4450322 上获取，2023。'
- en: '[51] C. Arora, J. Grundy, and M. Abdelrazek, “Advancing requirements engineering
    through generative ai: Assessing the role of llms,” in Generative AI for Effective
    Software Development, pp. 129–148, Springer, 2024.'
  id: totrans-1057
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] C. Arora, J. Grundy, 和 M. Abdelrazek, “通过生成性 AI 推动需求工程的发展：评估大型语言模型的作用，”
    发表在《有效软件开发的生成性 AI》，pp. 129–148, Springer, 2024。'
- en: '[52] M. Krishna, B. Gaur, A. Verma, and P. Jalote, “Using llms in software
    requirements specifications: An empirical evaluation,” 2024.'
  id: totrans-1058
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] M. Krishna, B. Gaur, A. Verma, 和 P. Jalote, “在软件需求规格说明中使用大型语言模型：一项实证评估，”
    2024。'
- en: '[53] L. Ma, S. Liu, Y. Li, X. Xie, and L. Bu, “Specgen: Automated generation
    of formal program specifications via large language models,” 2024.'
  id: totrans-1059
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] L. Ma, S. Liu, Y. Li, X. Xie, 和 L. Bu, “Specgen：通过大型语言模型自动生成正式程序规范，” 2024。'
- en: '[54] C. Flanagan and K. R. M. Leino, “Houdini, an annotation assistant for
    esc/java,” in FME 2001: Formal Methods for Increasing Software Productivity (J. N.
    Oliveira and P. Zave, eds.), (Berlin, Heidelberg), pp. 500–517, Springer Berlin
    Heidelberg, 2001.'
  id: totrans-1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] C. Flanagan 和 K. R. M. Leino, “Houdini，一个用于 ESC/Java 的注释助手，” 发表在《FME 2001:
    提高软件生产力的形式方法（J. N. Oliveira 和 P. Zave 编）》, (柏林, 海德堡), pp. 500–517, Springer Berlin
    Heidelberg, 2001。'
- en: '[55] J. White, S. Hays, Q. Fu, J. Spencer-Smith, and D. C. Schmidt, ChatGPT
    Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation,
    and Software Design, pp. 71–108. Cham: Springer Nature Switzerland, 2024.'
  id: totrans-1061
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] J. White, S. Hays, Q. Fu, J. Spencer-Smith, 和 D. C. Schmidt, 《ChatGPT
    提示模式用于提升代码质量、重构、需求获取和软件设计》，pp. 71–108. Cham: Springer Nature Switzerland, 2024。'
- en: '[56] D. Luitel, S. Hassani, and M. Sabetzadeh, “Improving requirements completeness:
    Automated assistance through large language models,” Requirements Engineering,
    vol. 29, no. 1, pp. 73–95, 2024.'
  id: totrans-1062
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] D. Luitel, S. Hassani, 和 M. Sabetzadeh, “提高需求完整性：通过大型语言模型的自动化辅助，” 《需求工程》，第29卷，第1期，pp.
    73–95, 2024。'
- en: '[57] A. Moharil and A. Sharma, “Identification of intra-domain ambiguity using
    transformer-based machine learning,” in Proceedings of the 1st International Workshop
    on Natural Language-Based Software Engineering, NLBSE ’22, (New York, NY, USA),
    p. 51–58, Association for Computing Machinery, 2023.'
  id: totrans-1063
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] A. Moharil 和 A. Sharma, “使用基于变换器的机器学习识别领域内模糊性，” 发表在《第1届国际自然语言基础软件工程研讨会论文集》，NLBSE
    ’22, (纽约, NY, USA), p. 51–58, 计算机协会, 2023。'
- en: '[58] K. Ronanki, B. Cabrero-Daniel, and C. Berger, “Chatgpt as a tool for user
    story quality evaluation: Trustworthy out of the box?,” in Agile Processes in
    Software Engineering and Extreme Programming – Workshops (P. Kruchten and P. Gregory,
    eds.), (Cham), pp. 173–181, Springer Nature Switzerland, 2024.'
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] K. Ronanki, B. Cabrero-Daniel, 和 C. Berger, “ChatGPT 作为用户故事质量评估工具：开箱即用的可靠性？”，
    见《敏捷软件工程与极限编程中的过程 – 研讨会》（P. Kruchten 和 P. Gregory 编），（Cham），第173–181页，施普林格自然瑞士，2024年。'
- en: '[59] A. Poudel, J. Lin, and J. Cleland-Huang, “Leveraging transformer-based
    language models to automate requirements satisfaction assessment,” 2023.'
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] A. Poudel, J. Lin, 和 J. Cleland-Huang, “利用基于 Transformer 的语言模型自动化需求满足评估，”
    2023年。'
- en: '[60] E. Musumeci, M. Brienza, V. Suriani, D. Nardi, and D. D. Bloisi, “Llm
    based multi-agent generation of semi-structured documents from semantic templates
    in the public administration domain,” in Artificial Intelligence in HCI (H. Degen
    and S. Ntoa, eds.), (Cham), pp. 98–117, Springer Nature Switzerland, 2024.'
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] E. Musumeci, M. Brienza, V. Suriani, D. Nardi, 和 D. D. Bloisi, “基于 LLM
    的多代理生成来自语义模板的半结构化文档，在公共管理领域，” 见《HCI中的人工智能》（H. Degen 和 S. Ntoa 编），（Cham），第98–117页，施普林格自然瑞士，2024年。'
- en: '[61] S. Zhang, J. Wang, G. Dong, J. Sun, Y. Zhang, and G. Pu, “Experimenting
    a new programming practice with llms,” 2024.'
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Zhang, J. Wang, G. Dong, J. Sun, Y. Zhang, 和 G. Pu, “使用 LLM 实验一种新的编程实践，”
    2024年。'
- en: '[62] A. Nouri, B. Cabrero-Daniel, F. Törner, H. Sivencrona, and C. Berger,
    “Engineering safety requirements for autonomous driving with large language models,”
    2024.'
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] A. Nouri, B. Cabrero-Daniel, F. Törner, H. Sivencrona, 和 C. Berger, “使用大型语言模型工程化自动驾驶的安全需求，”
    2024年。'
- en: '[63] Z. Zhang, M. Rayhan, T. Herda, M. Goisauf, and P. Abrahamsson, “Llm-based
    agents for automating the enhancement of user story quality: An early report,”
    in Agile Processes in Software Engineering and Extreme Programming (D. Šmite,
    E. Guerra, X. Wang, M. Marchesi, and P. Gregory, eds.), (Cham), pp. 117–126, Springer
    Nature Switzerland, 2024.'
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Z. Zhang, M. Rayhan, T. Herda, M. Goisauf, 和 P. Abrahamsson, “基于 LLM 的代理用于自动化提升用户故事质量：初步报告，”
    见《敏捷软件工程与极限编程中的过程》（D. Šmite, E. Guerra, X. Wang, M. Marchesi, 和 P. Gregory 编），（Cham），第117–126页，施普林格自然瑞士，2024年。'
- en: '[64] K. Ronanki, C. Berger, and J. Horkoff, “Investigating chatgpt’s potential
    to assist in requirements elicitation processes,” in 2023 49th Euromicro Conference
    on Software Engineering and Advanced Applications (SEAA), pp. 354–361, 2023.'
  id: totrans-1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. Ronanki, C. Berger, 和 J. Horkoff, “调查 ChatGPT 在需求引出过程中的潜力，” 见2023年第49届欧罗微软件工程与高级应用会议（SEAA），第354–361页，2023年。'
- en: '[65] D. Xie, B. Yoo, N. Jiang, M. Kim, L. Tan, X. Zhang, and J. S. Lee, “Impact
    of large language models on generating software specifications,” 2023.'
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] D. Xie, B. Yoo, N. Jiang, M. Kim, L. Tan, X. Zhang, 和 J. S. Lee, “大型语言模型对生成软件规格的影响，”
    2023年。'
- en: '[66] A. Moharil and A. Sharma, “Tabasco: A transformer based contextualization
    toolkit,” Science of Computer Programming, vol. 230, p. 102994, 2023.'
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] A. Moharil 和 A. Sharma, “Tabasco：一个基于 Transformer 的上下文工具包，” 《计算机编程科学》，第230卷，第102994页，2023年。'
- en: '[67] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” 2021.'
  id: totrans-1073
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, 和 W. Zaremba, “评估在代码上训练的大型语言模型，” 2021年。'
- en: '[68] A. Ni, P. Yin, Y. Zhao, M. Riddell, T. Feng, R. Shen, S. Yin, Y. Liu,
    S. Yavuz, C. Xiong, S. Joty, Y. Zhou, D. Radev, and A. Cohan, “L2ceval: Evaluating
    language-to-code generation capabilities of large language models,” 2023.'
  id: totrans-1074
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] A. Ni, P. Yin, Y. Zhao, M. Riddell, T. Feng, R. Shen, S. Yin, Y. Liu,
    S. Yavuz, C. Xiong, S. Joty, Y. Zhou, D. Radev, 和 A. Cohan, “L2ceval：评估大型语言模型的语言到代码生成能力，”
    2023年。'
- en: '[69] R. Sun, S. Ö. Arik, A. Muzio, L. Miculicich, S. Gundabathula, P. Yin,
    H. Dai, H. Nakhost, R. Sinha, Z. Wang, et al., “Sql-palm: Improved large language
    model adaptation for text-to-sql (extended),” arXiv preprint arXiv:2306.00739,
    2023.'
  id: totrans-1075
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] R. Sun, S. Ö. Arik, A. Muzio, L. Miculicich, S. Gundabathula, P. Yin,
    H. Dai, H. Nakhost, R. Sinha, Z. Wang, 等等, “Sql-palm: 改进的大型语言模型适应于文本到SQL（扩展版）,
    ” arXiv预印本 arXiv:2306.00739, 2023。'
- en: '[70] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen,
    A. Wang, Y. Li, T. Su, Z. Yang, and J. Tang, “Codegeex: A pre-trained model for
    code generation with multilingual benchmarking on humaneval-x,” 2024.'
  id: totrans-1076
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen,
    A. Wang, Y. Li, T. Su, Z. Yang, 和 J. Tang, “Codegeex: 一种预训练的代码生成模型，具有多语言基准测试于humaneval-x,”
    2024。'
- en: '[71] X. Hu, K. Kuang, J. Sun, H. Yang, and F. Wu, “Leveraging print debugging
    to improve code generation in large language models,” 2024.'
  id: totrans-1077
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] X. Hu, K. Kuang, J. Sun, H. Yang, 和 F. Wu, “利用打印调试来改进大型语言模型中的代码生成,” 2024。'
- en: '[72] S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer, “The impact of ai
    on developer productivity: Evidence from github copilot,” 2023.'
  id: totrans-1078
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. Peng, E. Kalliamvakou, P. Cihon, 和 M. Demirer, “AI对开发者生产力的影响：来自GitHub
    Copilot的证据,” 2023。'
- en: '[73] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
    W. tau Yih, L. Zettlemoyer, and M. Lewis, “Incoder: A generative model for code
    infilling and synthesis,” 2023.'
  id: totrans-1079
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong,
    W. tau Yih, L. Zettlemoyer, 和 M. Lewis, “Incoder: 一种用于代码填充和合成的生成模型,” 2023。'
- en: '[74] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    and C. Xiong, “Codegen: An open large language model for code with multi-turn
    program synthesis,” 2023.'
  id: totrans-1080
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    和 C. Xiong, “Codegen: 一种用于代码的开放大型语言模型，具有多轮程序合成,” 2023。'
- en: '[75] Y. Ding, M. J. Min, G. Kaiser, and B. Ray, “Cycle: Learning to self-refine
    the code generation,” Proc. ACM Program. Lang., vol. 8, apr 2024.'
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Ding, M. J. Min, G. Kaiser, 和 B. Ray, “Cycle: 学习自我优化代码生成,” 《ACM程序语言学报》,
    第8卷, 2024年4月。'
- en: '[76] Y. Dong, X. Jiang, Z. Jin, and G. Li, “Self-collaboration code generation
    via chatgpt,” 2024.'
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Dong, X. Jiang, Z. Jin, 和 G. Li, “通过ChatGPT进行自我协作的代码生成,” 2024。'
- en: '[77] F. Lin, D. J. Kim, et al., “When llm-based code generation meets the software
    development process,” arXiv preprint arXiv:2403.15852, 2024.'
  id: totrans-1083
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] F. Lin, D. J. Kim, 等等, “当基于LLM的代码生成遇到软件开发过程时,” arXiv预印本 arXiv:2403.15852,
    2024。'
- en: '[78] S. Holt, M. R. Luyten, and M. van der Schaar, “L2MAC: Large language model
    automatic computer for extensive code generation,” in The Twelfth International
    Conference on Learning Representations, 2024.'
  id: totrans-1084
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] S. Holt, M. R. Luyten, 和 M. van der Schaar, “L2MAC: 大型语言模型自动计算机用于广泛的代码生成,”
    发表在第十二届国际学习表征会议, 2024。'
- en: '[79] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z. Wang,
    S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, and J. Schmidhuber, “Metagpt:
    Meta programming for a multi-agent collaborative framework,” 2023.'
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z.
    Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, 和 J. Schmidhuber,
    “Metagpt: 针对多智能体协作框架的元编程,” 2023。'
- en: '[80] Z. Rasheed, M. Waseem, K.-K. Kemell, W. Xiaofeng, A. N. Duc, K. Systä,
    and P. Abrahamsson, “Autonomous agents in software development: A vision paper,”
    arXiv preprint arXiv:2311.18440, 2023.'
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Z. Rasheed, M. Waseem, K.-K. Kemell, W. Xiaofeng, A. N. Duc, K. Systä,
    和 P. Abrahamsson, “软件开发中的自主智能体：一篇愿景论文,” arXiv预印本 arXiv:2311.18440, 2023。'
- en: '[81] Z. Rasheed, M. Waseem, M. Saari, K. Systä, and P. Abrahamsson, “Codepori:
    Large scale model for autonomous software development by using multi-agents,”
    arXiv preprint arXiv:2402.01411, 2024.'
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Z. Rasheed, M. Waseem, M. Saari, K. Systä, 和 P. Abrahamsson, “Codepori:
    用于自主软件开发的大规模模型，通过使用多智能体,” arXiv预印本 arXiv:2402.01411, 2024。'
- en: '[82] D. Huang, Q. Bu, J. M. Zhang, M. Luck, and H. Cui, “Agentcoder: Multi-agent-based
    code generation with iterative testing and optimisation,” arXiv preprint arXiv:2312.13010,
    2023.'
  id: totrans-1088
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] D. Huang, Q. Bu, J. M. Zhang, M. Luck, 和 H. Cui, “Agentcoder: 基于多智能体的代码生成与迭代测试和优化,”
    arXiv预印本 arXiv:2312.13010, 2023。'
- en: '[83] T. Zheng, G. Zhang, T. Shen, X. Liu, B. Y. Lin, J. Fu, W. Chen, and X. Yue,
    “Opencodeinterpreter: Integrating code generation with execution and refinement,”
    arXiv preprint arXiv:2402.14658, 2024.'
  id: totrans-1089
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] T. Zheng, G. Zhang, T. Shen, X. Liu, B. Y. Lin, J. Fu, W. Chen, 和 X. Yue,
    “Opencodeinterpreter: 将代码生成与执行和优化集成,” arXiv预印本 arXiv:2402.14658, 2024。'
- en: '[84] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro,
    L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can
    teach themselves to use tools,” Advances in Neural Information Processing Systems,
    vol. 36, 2024.'
  id: totrans-1090
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, E. Hambro,
    L. Zettlemoyer, N. Cancedda, 和 T. Scialom, “Toolformer: 语言模型可以自我学习使用工具,” 《神经信息处理系统进展》,
    第36卷, 2024。'
- en: '[85] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,
    B. Qian, et al., “Toolllm: Facilitating large language models to master 16000+
    real-world apis,” arXiv preprint arXiv:2307.16789, 2023.'
  id: totrans-1091
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang,
    B. Qian 等，“Toolllm：帮助大型语言模型掌握 16000+ 现实世界 API”，arXiv 预印本 arXiv:2307.16789, 2023。'
- en: '[86] X. Jiang, Y. Dong, L. Wang, F. Zheng, Q. Shang, G. Li, Z. Jin, and W. Jiao,
    “Self-planning code generation with large language models,” ACM Trans. Softw.
    Eng. Methodol., jun 2024. Just Accepted.'
  id: totrans-1092
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] X. Jiang, Y. Dong, L. Wang, F. Zheng, Q. Shang, G. Li, Z. Jin 和 W. Jiao，“使用大型语言模型进行自我规划的代码生成”，《ACM
    软件工程方法学》, 2024年6月，刚刚接受。'
- en: '[87] S. Zhang, J. Wang, G. Dong, J. Sun, Y. Zhang, and G. Pu, “Experimenting
    a new programming practice with llms,” arXiv preprint arXiv:2401.01062, 2024.'
  id: totrans-1093
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Zhang, J. Wang, G. Dong, J. Sun, Y. Zhang 和 G. Pu，“使用 llms 进行新编程实践的实验”，arXiv
    预印本 arXiv:2401.01062, 2024。'
- en: '[88] V. Murali, C. Maddila, I. Ahmad, M. Bolin, D. Cheng, N. Ghorbani, R. Fernandez,
    and N. Nagappan, “Codecompose: A large-scale industrial deployment of ai-assisted
    code authoring,” arXiv preprint arXiv:2305.12050, 2023.'
  id: totrans-1094
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] V. Murali, C. Maddila, I. Ahmad, M. Bolin, D. Cheng, N. Ghorbani, R. Fernandez
    和 N. Nagappan，“Codecompose：AI 辅助代码创作的大规模工业部署”，arXiv 预印本 arXiv:2305.12050, 2023。'
- en: '[89] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, “Large
    language models can self-improve,” arXiv preprint arXiv:2210.11610, 2022.'
  id: totrans-1095
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu 和 J. Han，“大型语言模型可以自我改进”，arXiv
    预印本 arXiv:2210.11610, 2022。'
- en: '[90] L. Chen, J. Q. Davis, B. Hanin, P. Bailis, I. Stoica, M. Zaharia, and
    J. Zou, “Are more llm calls all you need? towards scaling laws of compound inference
    systems,” arXiv preprint arXiv:2403.02419, 2024.'
  id: totrans-1096
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L. Chen, J. Q. Davis, B. Hanin, P. Bailis, I. Stoica, M. Zaharia 和 J.
    Zou，“更多的 llm 调用是否足够？关于复合推理系统的规模定律”，arXiv 预印本 arXiv:2403.02419, 2024。'
- en: '[91] X. Chen, M. Lin, N. Schärli, and D. Zhou, “Teaching large language models
    to self-debug,” arXiv preprint arXiv:2304.05128, 2023.'
  id: totrans-1097
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] X. Chen, M. Lin, N. Schärli 和 D. Zhou，“教大型语言模型自我调试”，arXiv 预印本 arXiv:2304.05128,
    2023。'
- en: '[92] S. Kang, B. Chen, S. Yoo, and J.-G. Lou, “Explainable automated debugging
    via large language model-driven scientific debugging,” arXiv preprint arXiv:2304.02195,
    2023.'
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] S. Kang, B. Chen, S. Yoo 和 J.-G. Lou，“通过大型语言模型驱动的科学调试进行可解释的自动化调试”，arXiv
    预印本 arXiv:2304.02195, 2023。'
- en: '[93] G. Franceschelli and M. Musolesi, “On the creativity of large language
    models,” arXiv preprint arXiv:2304.00008, 2023.'
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] G. Franceschelli 和 M. Musolesi，“关于大型语言模型的创造力”，arXiv 预印本 arXiv:2304.00008,
    2023。'
- en: '[94] J. Lai, W. Gan, J. Wu, Z. Qi, and P. S. Yu, “Large language models in
    law: A survey,” 2023.'
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] J. Lai, W. Gan, J. Wu, Z. Qi 和 P. S. Yu，“法律中的大型语言模型：调查”，2023。'
- en: '[95] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing, et al., “Judging llm-as-a-judge with mt-bench and chatbot
    arena,” Advances in Neural Information Processing Systems, vol. 36, 2024.'
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing 等，“使用 mt-bench 和 chatbot arena 评判 llm 作为法官”，《神经信息处理系统进展》，第
    36 卷，2024。'
- en: '[96] Q. Wang, Z. Wang, Y. Su, H. Tong, and Y. Song, “Rethinking the bounds
    of llm reasoning: Are multi-agent discussions the key?,” arXiv preprint arXiv:2402.18272,
    2024.'
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Q. Wang, Z. Wang, Y. Su, H. Tong 和 Y. Song，“重新思考 llm 推理的界限：多代理讨论是关键吗？”，arXiv
    预印本 arXiv:2402.18272, 2024。'
- en: '[97] L. Chen, Y. Zhang, S. Ren, H. Zhao, Z. Cai, Y. Wang, P. Wang, T. Liu,
    and B. Chang, “Towards end-to-end embodied decision making via multi-modal large
    language model: Explorations with gpt4-vision and beyond,” arXiv preprint arXiv:2310.02071,
    2023.'
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] L. Chen, Y. Zhang, S. Ren, H. Zhao, Z. Cai, Y. Wang, P. Wang, T. Liu 和
    B. Chang，“通过多模态大型语言模型实现端到端的具身决策：基于 gpt4-vision 等的探索”，arXiv 预印本 arXiv:2310.02071,
    2023。'
- en: '[98] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, “Reflexion:
    Language agents with verbal reinforcement learning,” Advances in Neural Information
    Processing Systems, vol. 36, 2024.'
  id: totrans-1104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan 和 S. Yao，“Reflexion：带有语言代理的言语强化学习”，《神经信息处理系统进展》，第
    36 卷，2024。'
- en: '[99] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan, Y. Qin,
    Y. Lu, R. Xie, et al., “Agentverse: Facilitating multi-agent collaboration and
    exploring emergent behaviors in agents,” arXiv preprint arXiv:2308.10848, 2023.'
  id: totrans-1105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan, Y. Qin,
    Y. Lu, R. Xie 等，“Agentverse：促进多代理协作和探索代理中的突现行为”，arXiv 预印本 arXiv:2308.10848, 2023。'
- en: '[100] G. Li, H. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “Camel: Communicative
    agents for” mind” exploration of large language model society,” Advances in Neural
    Information Processing Systems, vol. 36, pp. 51991–52008, 2023.'
  id: totrans-1106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] G. Li, H. Hammoud, H. Itani, D. Khizbullin 和 B. Ghanem，“Camel：用于大型语言模型社会“思维”探索的交流代理”，《神经信息处理系统进展》，第
    36 卷，第 51991–52008 页，2023。'
- en: '[101] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng, Z. Chen,
    J. C. Niebles, D. Arpit, et al., “Bolaa: Benchmarking and orchestrating llm-augmented
    autonomous agents,” arXiv preprint arXiv:2308.05960, 2023.'
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y. Feng, Z.
    Chen, J. C. Niebles, D. Arpit, 等，“BOLAA: 基准测试和协调 LLM 增强的自主代理，” arXiv 预印本 arXiv:2308.05960，2023。'
- en: '[102] J. Lu, W. Zhong, W. Huang, Y. Wang, Q. Zhu, F. Mi, B. Wang, W. Wang,
    X. Zeng, L. Shang, X. Jiang, and Q. Liu, “Self: Self-evolution with language feedback,”
    2024.'
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Lu, W. Zhong, W. Huang, Y. Wang, Q. Zhu, F. Mi, B. Wang, W. Wang,
    X. Zeng, L. Shang, X. Jiang, 和 Q. Liu，“Self: 通过语言反馈自我进化，” 2024。'
- en: '[103] C. Xie, C. Chen, F. Jia, Z. Ye, K. Shu, A. Bibi, Z. Hu, P. Torr, B. Ghanem,
    and G. Li, “Can large language model agents simulate human trust behaviors?,”
    arXiv preprint arXiv:2402.04559, 2024.'
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] C. Xie, C. Chen, F. Jia, Z. Ye, K. Shu, A. Bibi, Z. Hu, P. Torr, B. Ghanem,
    和 G. Li，“大型语言模型代理能否模拟人类信任行为？”， arXiv 预印本 arXiv:2402.04559，2024。'
- en: '[104] Z. Liu, W. Yao, J. Zhang, L. Yang, Z. Liu, J. Tan, P. K. Choubey, T. Lan,
    J. Wu, H. Wang, et al., “Agentlite: A lightweight library for building and advancing
    task-oriented llm agent system,” arXiv preprint arXiv:2402.15538, 2024.'
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Z. Liu, W. Yao, J. Zhang, L. Yang, Z. Liu, J. Tan, P. K. Choubey, T.
    Lan, J. Wu, H. Wang, 等，“AgentLite: 一个轻量级库，用于构建和推进任务导向的 LLM 代理系统，” arXiv 预印本 arXiv:2402.15538，2024。'
- en: '[105] M. Zhuge, W. Wang, L. Kirsch, F. Faccio, D. Khizbullin, and J. Schmidhuber,
    “Language agents as optimizable graphs,” arXiv preprint arXiv:2402.16823, 2024.'
  id: totrans-1111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] M. Zhuge, W. Wang, L. Kirsch, F. Faccio, D. Khizbullin, 和 J. Schmidhuber，“语言代理作为可优化图，”
    arXiv 预印本 arXiv:2402.16823，2024。'
- en: '[106] R. Feldt, S. Kang, J. Yoon, and S. Yoo, “Towards autonomous testing agents
    via conversational large language models,” in 2023 38th IEEE/ACM International
    Conference on Automated Software Engineering (ASE), pp. 1688–1693, IEEE, 2023.'
  id: totrans-1112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] R. Feldt, S. Kang, J. Yoon, 和 S. Yoo，“通过对话式大型语言模型迈向自主测试代理，” 2023 年第 38
    届 IEEE/ACM 自动化软件工程国际会议（ASE），第 1688–1693 页，IEEE，2023。'
- en: '[107] A. Happe and J. Cito, “Getting pwn’d by ai: Penetration testing with
    large language models,” in Proceedings of the 31st ACM Joint European Software
    Engineering Conference and Symposium on the Foundations of Software Engineering,
    pp. 2082–2086, 2023.'
  id: totrans-1113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] A. Happe 和 J. Cito，“被 AI 侵入：利用大型语言模型进行渗透测试，” 在第 31 届 ACM 联合欧洲软件工程会议暨软件工程基础研讨会论文集中，第
    2082–2086 页，2023。'
- en: '[108] W. Ma, D. Wu, Y. Sun, T. Wang, S. Liu, J. Zhang, Y. Xue, and Y. Liu,
    “Combining fine-tuning and llm-based agents for intuitive smart contract auditing
    with justifications,” arXiv preprint arXiv:2403.16073, 2024.'
  id: totrans-1114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] W. Ma, D. Wu, Y. Sun, T. Wang, S. Liu, J. Zhang, Y. Xue, 和 Y. Liu，“结合微调和基于
    LLM 的代理进行直观的智能合约审计及其理由，” arXiv 预印本 arXiv:2403.16073，2024。'
- en: '[109] R. Fang, R. Bindu, A. Gupta, and D. Kang, “Llm agents can autonomously
    exploit one-day vulnerabilities,” arXiv preprint arXiv:2404.08144, 2024.'
  id: totrans-1115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] R. Fang, R. Bindu, A. Gupta, 和 D. Kang，“LLM 代理可以自主利用一天的漏洞，” arXiv 预印本
    arXiv:2404.08144，2024。'
- en: '[110] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan,
    “Tree of thoughts: Deliberate problem solving with large language models,” Advances
    in Neural Information Processing Systems, vol. 36, 2024.'
  id: totrans-1116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, 和 K. Narasimhan，“思维树：使用大型语言模型进行深思熟虑的问题解决，”
    《神经信息处理系统进展》，第 36 卷，2024。'
- en: '[111] Z. Rasheed, M. Waseem, A. Ahmad, K.-K. Kemell, W. Xiaofeng, A. N. Duc,
    and P. Abrahamsson, “Can large language models serve as data analysts? a multi-agent
    assisted approach for qualitative data analysis,” arXiv preprint arXiv:2402.01386,
    2024.'
  id: totrans-1117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Z. Rasheed, M. Waseem, A. Ahmad, K.-K. Kemell, W. Xiaofeng, A. N. Duc,
    和 P. Abrahamsson，“大型语言模型能否充当数据分析师？一种多代理辅助的定性数据分析方法，” arXiv 预印本 arXiv:2402.01386，2024。'
- en: '[112] M. Ataei, H. Cheong, D. Grandi, Y. Wang, N. Morris, and A. Tessier, “Elicitron:
    An llm agent-based simulation framework for design requirements elicitation,”
    arXiv preprint arXiv:2404.16045, 2024.'
  id: totrans-1118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Ataei, H. Cheong, D. Grandi, Y. Wang, N. Morris, 和 A. Tessier，“Elicitron:
    基于 LLM 的设计需求引导模拟框架，” arXiv 预印本 arXiv:2404.16045，2024。'
- en: '[113] G. Sridhara, S. Mazumdar, et al., “Chatgpt: A study on its utility for
    ubiquitous software engineering tasks,” arXiv preprint arXiv:2305.16837, 2023.'
  id: totrans-1119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] G. Sridhara, S. Mazumdar, 等，“ChatGPT: 对其在无处不在的软件工程任务中的实用性的研究，” arXiv
    预印本 arXiv:2305.16837，2023。'
- en: '[114] M. Desmond, Z. Ashktorab, Q. Pan, C. Dugan, and J. M. Johnson, “Evalullm:
    Llm assisted evaluation of generative outputs,” in Companion Proceedings of the
    29th International Conference on Intelligent User Interfaces, pp. 30–32, 2024.'
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Desmond, Z. Ashktorab, Q. Pan, C. Dugan, 和 J. M. Johnson，“EvalULLM:
    LLM 辅助的生成输出评估，” 在第 29 届国际智能用户界面会议论文集中，第 30–32 页，2024。'
- en: '[115] M. Gao, X. Hu, J. Ruan, X. Pu, and X. Wan, “Llm-based nlg evaluation:
    Current status and challenges,” arXiv preprint arXiv:2402.01383, 2024.'
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] M. Gao, X. Hu, J. Ruan, X. Pu, 和 X. Wan，“基于 LLM 的 NLG 评估：现状与挑战”，arXiv
    预印本 arXiv:2402.01383，2024 年。'
- en: '[116] L. J. Wan, Y. Huang, Y. Li, H. Ye, J. Wang, X. Zhang, and D. Chen, “Software/hardware
    co-design for llm and its application for design verification,” in 2024 29th Asia
    and South Pacific Design Automation Conference (ASP-DAC), pp. 435–441, IEEE, 2024.'
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. J. Wan, Y. Huang, Y. Li, H. Ye, J. Wang, X. Zhang, 和 D. Chen，“LLM
    的软件/硬件协同设计及其在设计验证中的应用”，发表于 2024 年第 29 届亚太设计自动化会议（ASP-DAC），第 435–441 页，IEEE，2024
    年。'
- en: '[117] K. Kolthoff, C. Bartelt, and S. P. Ponzetto, “Data-driven prototyping
    via natural-language-based gui retrieval,” Automated software engineering, vol. 30,
    no. 1, p. 13, 2023.'
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] K. Kolthoff, C. Bartelt, 和 S. P. Ponzetto，“通过自然语言基础的 GUI 检索进行数据驱动的原型设计”，自动化软件工程，第
    30 卷，第 1 期，第 13 页，2023 年。'
- en: '[118] V. D. Kirova, C. S. Ku, J. R. Laracy, and T. J. Marlowe, “Software engineering
    education must adapt and evolve for an llm environment,” in Proceedings of the
    55th ACM Technical Symposium on Computer Science Education V. 1, pp. 666–672,
    2024.'
  id: totrans-1124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] V. D. Kirova, C. S. Ku, J. R. Laracy, 和 T. J. Marlowe，“软件工程教育必须适应并发展以应对
    LLM 环境”，发表于第 55 届 ACM 计算机科学教育技术研讨会第 1 卷，第 666–672 页，2024 年。'
- en: '[119] S. Jalil, S. Rafi, T. D. LaToza, K. Moran, and W. Lam, “Chatgpt and software
    testing education: promises & perils (2023),” arXiv preprint arXiv:2302.03287,
    2023.'
  id: totrans-1125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] S. Jalil, S. Rafi, T. D. LaToza, K. Moran, 和 W. Lam，“ChatGPT 和软件测试教育：承诺与风险（2023）”，arXiv
    预印本 arXiv:2302.03287，2023 年。'
- en: '[120] S. Suri, S. N. Das, K. Singi, K. Dey, V. S. Sharma, and V. Kaulgud, “Software
    engineering using autonomous agents: Are we there yet?,” in 2023 38th IEEE/ACM
    International Conference on Automated Software Engineering (ASE), pp. 1855–1857,
    IEEE, 2023.'
  id: totrans-1126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] S. Suri, S. N. Das, K. Singi, K. Dey, V. S. Sharma, 和 V. Kaulgud，“使用自主代理的软件工程：我们已经达到了吗？”，发表于
    2023 年第 38 届 IEEE/ACM 自动化软件工程国际会议（ASE），第 1855–1857 页，IEEE，2023 年。'
- en: '[121] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, and M. Sun,
    “Communicative agents for software development,” arXiv preprint arXiv:2307.07924,
    2023.'
  id: totrans-1127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, 和 M. Sun，“用于软件开发的沟通代理”，arXiv
    预印本 arXiv:2307.07924，2023 年。'
- en: '[122] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt: Solving
    ai tasks with chatgpt and its friends in hugging face,” Advances in Neural Information
    Processing Systems, vol. 36, 2024.'
  id: totrans-1128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, 和 Y. Zhuang，“HuggingGPT：通过 Hugging
    Face 中的 ChatGPT 和它的朋友解决 AI 任务”，神经信息处理系统进展，第 36 卷，2024 年。'
- en: '[123] J. Chen, X. Hu, S. Liu, S. Huang, W.-W. Tu, Z. He, and L. Wen, “Llmarena:
    Assessing capabilities of large language models in dynamic multi-agent environments,”
    arXiv preprint arXiv:2402.16499, 2024.'
  id: totrans-1129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] J. Chen, X. Hu, S. Liu, S. Huang, W.-W. Tu, Z. He, 和 L. Wen，“LLMArena：评估大型语言模型在动态多代理环境中的能力”，arXiv
    预印本 arXiv:2402.16499，2024 年。'
- en: '[124] M. Josifoski, L. Klein, M. Peyrard, Y. Li, S. Geng, J. P. Schnitzler,
    Y. Yao, J. Wei, D. Paul, and R. West, “Flows: Building blocks of reasoning and
    collaborating ai,” arXiv preprint arXiv:2308.01285, 2023.'
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] M. Josifoski, L. Klein, M. Peyrard, Y. Li, S. Geng, J. P. Schnitzler,
    Y. Yao, J. Wei, D. Paul, 和 R. West，“Flows：推理与协作 AI 的构建块”，arXiv 预印本 arXiv:2308.01285，2023
    年。'
- en: '[125] I. Weber, “Large language models as software components: A taxonomy for
    llm-integrated applications,” arXiv preprint arXiv:2406.10300, 2024.'
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] I. Weber，“大型语言模型作为软件组件：用于 LLM 集成应用的分类”，arXiv 预印本 arXiv:2406.10300，2024
    年。'
- en: '[126] F. Vallecillos Ruiz, “Agent-driven automatic software improvement,” in
    Proceedings of the 28th International Conference on Evaluation and Assessment
    in Software Engineering, pp. 470–475, 2024.'
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] F. Vallecillos Ruiz，“基于代理的自动软件改进”，发表于第 28 届国际软件工程评价与评估会议论文集，第 470–475
    页，2024 年。'
- en: '[127] Z. Cheng, J. Kasai, and T. Yu, “Batch prompting: Efficient inference
    with large language model apis,” arXiv preprint arXiv:2301.08721, 2023.'
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Z. Cheng, J. Kasai, 和 T. Yu，“批量提示：使用大型语言模型 API 的高效推断”，arXiv 预印本 arXiv:2301.08721，2023
    年。'
- en: '[128] S. Shankar, J. Zamfirescu-Pereira, B. Hartmann, A. G. Parameswaran, and
    I. Arawjo, “Who validates the validators? aligning llm-assisted evaluation of
    llm outputs with human preferences,” arXiv preprint arXiv:2404.12272, 2024.'
  id: totrans-1134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] S. Shankar, J. Zamfirescu-Pereira, B. Hartmann, A. G. Parameswaran, 和
    I. Arawjo，“谁来验证验证者？将 LLM 辅助的 LLM 输出评估与人类偏好对齐”，arXiv 预印本 arXiv:2404.12272，2024
    年。'
- en: '[129] D. Roy, X. Zhang, R. Bhave, C. Bansal, P. Las-Casas, R. Fonseca, and
    S. Rajmohan, “Exploring llm-based agents for root cause analysis,” in Companion
    Proceedings of the 32nd ACM International Conference on the Foundations of Software
    Engineering, pp. 208–219, 2024.'
  id: totrans-1135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] D. Roy, X. Zhang, R. Bhave, C. Bansal, P. Las-Casas, R. Fonseca 和 S.
    Rajmohan，"探索基于 llm 的代理进行根本原因分析," 见第32届 ACM 国际软件工程基础会议伴随会议录, 第208–219页, 2024。'
- en: '[130] Y. Li, Y. Zhang, and L. Sun, “Metaagents: Simulating interactions of
    human behaviors for llm-based task-oriented coordination via collaborative generative
    agents,” arXiv preprint arXiv:2310.06500, 2023.'
  id: totrans-1136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Li, Y. Zhang 和 L. Sun，"Metaagents: 利用协作生成代理模拟人类行为的交互以实现基于 llm 的任务导向协调,"
    arXiv 预印本 arXiv:2310.06500, 2023。'
- en: '[131] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, and N. Sundaresan,
    “Unit test case generation with transformers and focal context,” arXiv preprint
    arXiv:2009.05617, 2020.'
  id: totrans-1137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng 和 N. Sundaresan，"利用变换器和焦点上下文生成单元测试用例,"
    arXiv 预印本 arXiv:2009.05617, 2020。'
- en: '[132] Y. Zhang, W. Song, Z. Ji, N. Meng, et al., “How well does llm generate
    security tests?,” arXiv preprint arXiv:2310.00710, 2023.'
  id: totrans-1138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Y. Zhang, W. Song, Z. Ji, N. Meng 等，"llm 生成安全测试的效果如何？," arXiv 预印本 arXiv:2310.00710,
    2023。'
- en: '[133] H. J. Kang, T. G. Nguyen, B. Le, C. S. Păsăreanu, and D. Lo, “Test mimicry
    to assess the exploitability of library vulnerabilities,” in Proceedings of the
    31st ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 276–288,
    2022.'
  id: totrans-1139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] H. J. Kang, T. G. Nguyen, B. Le, C. S. Păsăreanu 和 D. Lo，"测试模拟评估库漏洞的可利用性,"
    见第31届 ACM SIGSOFT 国际软件测试与分析研讨会会议录, 第276–288页, 2022。'
- en: '[134] S. Feng and C. Chen, “Prompting is all you need: Automated android bug
    replay with large language models,” in Proceedings of the 46th IEEE/ACM International
    Conference on Software Engineering, pp. 1–13, 2024.'
  id: totrans-1140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] S. Feng 和 C. Chen，"提示就是你需要的：利用大型语言模型自动重放安卓漏洞," 见第46届 IEEE/ACM 国际软件工程大会会议录,
    第1–13页, 2024。'
- en: '[135] S. Kang, J. Yoon, and S. Yoo, “Large language models are few-shot testers:
    Exploring llm-based general bug reproduction,” in 2023 IEEE/ACM 45th International
    Conference on Software Engineering (ICSE), pp. 2312–2323, IEEE, 2023.'
  id: totrans-1141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] S. Kang, J. Yoon 和 S. Yoo，"大型语言模型是少量样本测试者：探索基于 llm 的通用错误重现," 见 2023 IEEE/ACM
    第45届国际软件工程大会 (ICSE), 第2312–2323页, IEEE, 2023。'
- en: '[136] C. S. Xia, M. Paltenghi, J. Le Tian, M. Pradel, and L. Zhang, “Fuzz4all:
    Universal fuzzing with large language models,” in Proceedings of the IEEE/ACM
    46th International Conference on Software Engineering, pp. 1–13, 2024.'
  id: totrans-1142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] C. S. Xia, M. Paltenghi, J. Le Tian, M. Pradel 和 L. Zhang，"Fuzz4all:
    利用大型语言模型的通用模糊测试," 见 IEEE/ACM 第46届国际软件工程大会会议录, 第1–13页, 2024。'
- en: '[137] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan, and B. Ray,
    “Code-aware prompting: A study of coverage guided test generation in regression
    setting using llm,” arXiv preprint arXiv:2402.00097, 2024.'
  id: totrans-1143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] G. Ryan, S. Jain, M. Shang, S. Wang, X. Ma, M. K. Ramanathan 和 B. Ray，"代码感知提示：使用
    llm 在回归设置中研究覆盖引导测试生成," arXiv 预印本 arXiv:2402.00097, 2024。'
- en: '[138] J. A. Pizzorno and E. D. Berger, “Coverup: Coverage-guided llm-based
    test generation,” arXiv preprint arXiv:2403.16218, 2024.'
  id: totrans-1144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] J. A. Pizzorno 和 E. D. Berger，"Coverup: Coverage-guided llm-based test
    generation," arXiv 预印本 arXiv:2403.16218, 2024。'
- en: '[139] K. Liu, Y. Liu, Z. Chen, J. M. Zhang, Y. Han, Y. Ma, G. Li, and G. Huang,
    “Llm-powered test case generation for detecting tricky bugs,” arXiv preprint arXiv:2404.10304,
    2024.'
  id: totrans-1145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] K. Liu, Y. Liu, Z. Chen, J. M. Zhang, Y. Han, Y. Ma, G. Li 和 G. Huang，"基于
    llm 的测试用例生成用于检测棘手的错误," arXiv 预印本 arXiv:2404.10304, 2024。'
- en: '[140] K. Li and Y. Yuan, “Large language models as test case generators: Performance
    evaluation and enhancement,” arXiv preprint arXiv:2404.13340, 2024.'
  id: totrans-1146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] K. Li 和 Y. Yuan，"大型语言模型作为测试用例生成器：性能评估与提升," arXiv 预印本 arXiv:2404.13340,
    2024。'
- en: '[141] Z. Wang, W. Wang, Z. Li, L. Wang, C. Yi, X. Xu, L. Cao, H. Su, S. Chen,
    and J. Zhou, “Xuat-copilot: Multi-agent collaborative system for automated user
    acceptance testing with large language model,” arXiv preprint arXiv:2401.02705,
    2024.'
  id: totrans-1147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Z. Wang, W. Wang, Z. Li, L. Wang, C. Yi, X. Xu, L. Cao, H. Su, S. Chen
    和 J. Zhou，"Xuat-copilot: 基于大型语言模型的多代理协作系统用于自动化用户验收测试," arXiv 预印本 arXiv:2401.02705,
    2024。'
- en: '[142] C. Lee, C. S. Xia, J.-t. Huang, Z. Zhu, L. Zhang, and M. R. Lyu, “A unified
    debugging approach via llm-based multi-agent synergy,” arXiv preprint arXiv:2404.17153,
    2024.'
  id: totrans-1148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] C. Lee, C. S. Xia, J.-t. Huang, Z. Zhu, L. Zhang 和 M. R. Lyu，"通过 llm
    基于多代理协同的统一调试方法," arXiv 预印本 arXiv:2404.17153, 2024。'
- en: '[143] G. Deng, Y. Liu, V. Mayoral-Vilches, P. Liu, Y. Li, Y. Xu, T. Zhang,
    Y. Liu, M. Pinzger, and S. Rass, “Pentestgpt: An llm-empowered automatic penetration
    testing tool,” arXiv preprint arXiv:2308.06782, 2023.'
  id: totrans-1149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] G. Deng, Y. Liu, V. Mayoral-Vilches, P. Liu, Y. Li, Y. Xu, T. Zhang,
    Y. Liu, M. Pinzger, 和 S. Rass, “Pentestgpt：一种 llm 赋能的自动渗透测试工具，” arXiv 预印本 arXiv:2308.06782,
    2023。'
- en: '[144] M. Xiao, Y. Xiao, H. Dong, S. Ji, and P. Zhang, “Ritfis: Robust input
    testing framework for llms-based intelligent software,” arXiv preprint arXiv:2402.13518,
    2024.'
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] M. Xiao, Y. Xiao, H. Dong, S. Ji, 和 P. Zhang, “Ritfis：针对 llms 基于智能软件的稳健输入测试框架，”
    arXiv 预印本 arXiv:2402.13518, 2024。'
- en: '[145] R. Wang, Z. Li, C. Wang, Y. Xiao, and C. Gao, “Navrepair: Node-type aware
    c/c++ code vulnerability repair,” arXiv preprint arXiv:2405.04994, 2024.'
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] R. Wang, Z. Li, C. Wang, Y. Xiao, 和 C. Gao, “Navrepair：节点类型感知的 C/C++
    代码漏洞修复，” arXiv 预印本 arXiv:2405.04994, 2024。'
- en: '[146] A. Shestov, A. Cheshkov, R. Levichev, R. Mussabayev, P. Zadorozhny, E. Maslov,
    C. Vadim, and E. Bulychev, “Finetuning large language models for vulnerability
    detection,” arXiv preprint arXiv:2401.17010, 2024.'
  id: totrans-1152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] A. Shestov, A. Cheshkov, R. Levichev, R. Mussabayev, P. Zadorozhny, E. Maslov,
    C. Vadim, 和 E. Bulychev, “针对漏洞检测的大型语言模型的微调，” arXiv 预印本 arXiv:2401.17010, 2024。'
- en: '[147] A. Cheshkov, P. Zadorozhny, and R. Levichev, “Evaluation of chatgpt model
    for vulnerability detection,” arXiv preprint arXiv:2304.07232, 2023.'
  id: totrans-1153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] A. Cheshkov, P. Zadorozhny, 和 R. Levichev, “评估 chatgpt 模型在漏洞检测中的表现，”
    arXiv 预印本 arXiv:2304.07232, 2023。'
- en: '[148] G. Lu, X. Ju, X. Chen, W. Pei, and Z. Cai, “Grace: Empowering llm-based
    software vulnerability detection with graph structure and in-context learning,”
    Journal of Systems and Software, vol. 212, p. 112031, 2024.'
  id: totrans-1154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] G. Lu, X. Ju, X. Chen, W. Pei, 和 Z. Cai, “Grace：通过图结构和上下文学习赋能基于 llm 的软件漏洞检测，”
    系统与软件期刊，卷 212，第 112031 页，2024。'
- en: '[149] H. Li, Y. Hao, Y. Zhai, and Z. Qian, “The hitchhiker’s guide to program
    analysis: A journey with large language models,” arXiv preprint arXiv:2308.00245,
    2023.'
  id: totrans-1155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] H. Li, Y. Hao, Y. Zhai, 和 Z. Qian, “程序分析的搭便车指南：与大型语言模型的旅程，” arXiv 预印本
    arXiv:2308.00245, 2023。'
- en: '[150] Y. Ding, Y. Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair, D. Wagner,
    B. Ray, and Y. Chen, “Vulnerability detection with code language models: How far
    are we?,” arXiv preprint arXiv:2403.18624, 2024.'
  id: totrans-1156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Y. Ding, Y. Fu, O. Ibrahim, C. Sitawarin, X. Chen, B. Alomair, D. Wagner,
    B. Ray, 和 Y. Chen, “使用代码语言模型的漏洞检测：我们走多远了？”， arXiv 预印本 arXiv:2403.18624, 2024。'
- en: '[151] F. V. Ruiz, A. Grishina, M. Hort, and L. Moonen, “A novel approach for
    automatic program repair using round-trip translation with large language models,”
    arXiv preprint arXiv:2401.07994, 2024.'
  id: totrans-1157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] F. V. Ruiz, A. Grishina, M. Hort, 和 L. Moonen, “一种使用大型语言模型的往返翻译自动程序修复的新方法，”
    arXiv 预印本 arXiv:2401.07994, 2024。'
- en: '[152] B. Yang, H. Tian, J. Ren, H. Zhang, J. Klein, T. F. Bissyandé, C. L.
    Goues, and S. Jin, “Multi-objective fine-tuning for enhanced program repair with
    llms,” arXiv preprint arXiv:2404.12636, 2024.'
  id: totrans-1158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] B. Yang, H. Tian, J. Ren, H. Zhang, J. Klein, T. F. Bissyandé, C. L.
    Goues, 和 S. Jin, “用于增强程序修复的多目标微调与 llms，” arXiv 预印本 arXiv:2404.12636, 2024。'
- en: '[153] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient
    finetuning of quantized llms,” 2023.'
  id: totrans-1159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer, “Qlora：量化 llms
    的高效微调，” 2023。'
- en: '[154] N. Jain, P. yeh Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli,
    B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha, M. Goldblum, J. Geiping,
    and T. Goldstein, “Neftune: Noisy embeddings improve instruction finetuning,”
    2023.'
  id: totrans-1160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] N. Jain, P. yeh Chiang, Y. Wen, J. Kirchenbauer, H.-M. Chu, G. Somepalli,
    B. R. Bartoldson, B. Kailkhura, A. Schwarzschild, A. Saha, M. Goldblum, J. Geiping,
    和 T. Goldstein, “Neftune：噪声嵌入提升指令微调，” 2023。'
- en: '[155] J. Zhang, J. P. Cambronero, S. Gulwani, V. Le, R. Piskac, G. Soares,
    and G. Verbruggen, “Pydex: Repairing bugs in introductory python assignments using
    llms,” Proceedings of the ACM on Programming Languages, vol. 8, no. OOPSLA1, pp. 1100–1124,
    2024.'
  id: totrans-1161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Zhang, J. P. Cambronero, S. Gulwani, V. Le, R. Piskac, G. Soares,
    和 G. Verbruggen, “Pydex：使用 llms 修复入门 Python 作业中的错误，” ACM 编程语言会议论文集，卷 8，第 OOPSLA1
    期，第 1100–1124 页，2024。'
- en: '[156] H. Joshi, J. C. Sanchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radiček,
    “Repair is nearly generation: Multilingual program repair with llms,” in Proceedings
    of the AAAI Conference on Artificial Intelligence, vol. 37, pp. 5131–5140, 2023.'
  id: totrans-1162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] H. Joshi, J. C. Sanchez, S. Gulwani, V. Le, G. Verbruggen, 和 I. Radiček,
    “修复几乎是生成：多语言程序修复与 llms，” 见于 AAAI 人工智能会议论文集，卷 37，第 5131–5140 页，2023。'
- en: '[157] J. Xiang, X. Xu, F. Kong, M. Wu, H. Zhang, and Y. Zhang, “How far can
    we go with practical function-level program repair?,” arXiv preprint arXiv:2404.12833,
    2024.'
  id: totrans-1163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] J. Xiang, X. Xu, F. Kong, M. Wu, H. Zhang, 和 Y. Zhang, “我们在实际功能级程序修复中能走多远？”，
    arXiv 预印本 arXiv:2404.12833, 2024。'
- en: '[158] E. Hilario, S. Azam, J. Sundaram, K. Imran Mohammed, and B. Shanmugam,
    “Generative ai for pentesting: the good, the bad, the ugly,” International Journal
    of Information Security, vol. 23, no. 3, pp. 2075–2097, 2024.'
  id: totrans-1164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] E. Hilario, S. Azam, J. Sundaram, K. Imran Mohammed, 和 B. Shanmugam,
    “用于渗透测试的生成性 AI: 好的, 坏的, 丑的,” 国际信息安全期刊, 卷 23, 第 3 期, 页 2075–2097, 2024 年。'
- en: '[159] L. Zhong, Z. Wang, and J. Shang, “Ldb: A large language model debugger
    via verifying runtime execution step-by-step,” arXiv preprint arXiv:2402.16906,
    2024.'
  id: totrans-1165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] L. Zhong, Z. Wang, 和 J. Shang, “Ldb: 通过逐步验证运行时执行的大型语言模型调试器,” arXiv 预印本
    arXiv:2402.16906, 2024 年。'
- en: '[160] L. Zhang, K. Li, K. Sun, D. Wu, Y. Liu, H. Tian, and Y. Liu, “Acfix:
    Guiding llms with mined common rbac practices for context-aware repair of access
    control vulnerabilities in smart contracts,” 2024.'
  id: totrans-1166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] L. Zhang, K. Li, K. Sun, D. Wu, Y. Liu, H. Tian, 和 Y. Liu, “Acfix: 通过挖掘常见
    RBAC 实践引导 llm 以进行智能合约中访问控制漏洞的上下文感知修复,” 2024 年。'
- en: '[161] S. Hu, T. Huang, F. İlhan, S. F. Tekin, and L. Liu, “Large language model-powered
    smart contract vulnerability detection: New perspectives,” 2023.'
  id: totrans-1167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Hu, T. Huang, F. İlhan, S. F. Tekin, 和 L. Liu, “基于大型语言模型的智能合约漏洞检测:
    新视角,” 2023 年。'
- en: '[162] M. Alhanahnah, M. R. Hasan, and H. Bagheri, “An empirical evaluation
    of pre-trained large language models for repairing declarative formal specifications,”
    arXiv preprint arXiv:2404.11050, 2024.'
  id: totrans-1168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] M. Alhanahnah, M. R. Hasan, 和 H. Bagheri, “对预训练大型语言模型修复声明性形式规范的实证评估,”
    arXiv 预印本 arXiv:2404.11050, 2024 年。'
- en: '[163] F. Geissler, K. Roscher, and M. Trapp, “Concept-guided llm agents for
    human-ai safety codesign,” in Proceedings of the AAAI Symposium Series, vol. 3,
    pp. 100–104, 2024.'
  id: totrans-1169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] F. Geissler, K. Roscher, 和 M. Trapp, “面向人机安全协同设计的概念引导 llm 代理,” 收录于 AAAI
    研讨会系列, 卷 3, 页 100–104, 2024 年。'
- en: '[164] A. Nouri, B. Cabrero-Daniel, F. Törner, H. Sivencrona, and C. Berger,
    “Engineering safety requirements for autonomous driving with large language models,”
    arXiv preprint arXiv:2403.16289, 2024.'
  id: totrans-1170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] A. Nouri, B. Cabrero-Daniel, F. Törner, H. Sivencrona, 和 C. Berger, “利用大型语言模型为自动驾驶工程安全需求提供支持,”
    arXiv 预印本 arXiv:2403.16289, 2024 年。'
- en: '[165] C. Thapa, S. I. Jang, M. E. Ahmed, S. Camtepe, J. Pieprzyk, and S. Nepal,
    “Transformer-based language models for software vulnerability detection,” in Proceedings
    of the 38th Annual Computer Security Applications Conference, pp. 481–496, 2022.'
  id: totrans-1171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] C. Thapa, S. I. Jang, M. E. Ahmed, S. Camtepe, J. Pieprzyk, 和 S. Nepal,
    “基于变换器的语言模型用于软件漏洞检测,” 收录于第 38 届年度计算机安全应用会议, 页 481–496, 2022 年。'
- en: '[166] L. Zhong, Z. Wang, and J. Shang, “Debug like a human: A large language
    model debugger via verifying runtime execution step-by-step,” 2024.'
  id: totrans-1172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] L. Zhong, Z. Wang, 和 J. Shang, “像人类一样调试: 通过逐步验证运行时执行的大型语言模型调试器,” 2024
    年。'
- en: '[167] N. Alshahwan, M. Harman, I. Harper, A. Marginean, S. Sengupta, and E. Wang,
    “Assured llm-based software engineering,” 2024.'
  id: totrans-1173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] N. Alshahwan, M. Harman, I. Harper, A. Marginean, S. Sengupta, 和 E. Wang,
    “基于 llm 的软件工程保证,” 2024 年。'
- en: '[168] A. Cheshkov, P. Zadorozhny, and R. Levichev, “Evaluation of chatgpt model
    for vulnerability detection,” 2023.'
  id: totrans-1174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] A. Cheshkov, P. Zadorozhny, 和 R. Levichev, “对 chatgpt 模型在漏洞检测中的评估,” 2023
    年。'
- en: -A Benchmarks
  id: totrans-1175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: -A 基准测试
- en: '![Refer to caption](img/404a32097b3c32d477f09acb0be48b7b.png)'
  id: totrans-1176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/404a32097b3c32d477f09acb0be48b7b.png)'
- en: 'Figure 12: Distribution of Benchmarks'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: 基准测试分布'
- en: -B Evaluation Metrics
  id: totrans-1178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: -B 评估指标
- en: '![Refer to caption](img/3d87a68291fb54c10243c283786497f1.png)'
  id: totrans-1179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d87a68291fb54c10243c283786497f1.png)'
- en: 'Figure 13: Top 10 Evaluation Metrics'
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: 前 10 名评估指标'
