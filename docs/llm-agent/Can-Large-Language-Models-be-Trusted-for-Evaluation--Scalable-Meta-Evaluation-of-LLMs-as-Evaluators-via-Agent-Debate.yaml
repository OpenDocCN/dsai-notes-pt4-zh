- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:53:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型是否可以信赖用于评估？通过代理辩论进行 LLMs 作为评估者的可扩展元评估
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.16788](https://ar5iv.labs.arxiv.org/html/2401.16788)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.16788](https://ar5iv.labs.arxiv.org/html/2401.16788)
- en: Steffi Chern^(2,4)  Ethan Chern^(1,4)  Graham Neubig²  Pengfei Liu^(1,3,4)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Steffi Chern^(2,4)  Ethan Chern^(1,4)  Graham Neubig²  Pengfei Liu^(1,3,4)
- en: ¹Shanghai Jiao Tong University  ²Carnegie Mellon University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹上海交通大学  ²卡内基梅隆大学
- en: ³Shanghai Artificial Intelligence Laboratory  ⁴Generative AI Research Lab (GAIR)
      Corresponding author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海人工智能实验室  ⁴生成式 AI 研究实验室 (GAIR)    通讯作者
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Despite the utility of Large Language Models (LLMs) across a wide range of
    tasks and scenarios, developing a method for reliably evaluating LLMs across varied
    contexts continues to be challenging. Modern evaluation approaches often use LLMs
    to assess responses generated by LLMs. However, the meta-evaluation conducted
    to assess the effectiveness of these LLMs as evaluators is typically constrained
    by the coverage of existing benchmarks or requires extensive human annotation.
    This underscores the urgency of methods for scalable meta-evaluation that can
    effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators
    across diverse tasks and scenarios, particularly in potentially new, user-defined
    scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation
    framework that leverages the capabilities of multiple communicative LLM agents.
    This framework supports multi-round discussions to assist human annotators in
    discerning the most capable LLMs as evaluators, which significantly eases their
    workload in cases that used to require large-scale annotations during meta-evaluation.
    We release the code for our framework, which is publicly available at: [https://github.com/GAIR-NLP/scaleeval](https://github.com/GAIR-NLP/scaleeval).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在广泛任务和场景中的实用性非常高，但在各种背景下可靠地评估 LLMs 的方法仍然具有挑战性。现代评估方法通常使用 LLMs
    来评估 LLMs 生成的响应。然而，用于评估这些 LLMs 作为评估者的元评估通常受到现有基准覆盖范围的限制或需要大量人工标注。这突显了需要可扩展元评估方法的紧迫性，这些方法能够有效、可靠和高效地评估
    LLMs 作为评估者在各种任务和场景中的表现，特别是在可能的新用户定义场景中。为填补这一空白，我们提出了 ScaleEval，一种借助多个沟通性 LLM 代理的元评估框架。该框架支持多轮讨论，以帮助人工标注员识别最有能力的
    LLMs 作为评估者，这显著减轻了他们在元评估中需要进行的大规模标注工作。我们发布了我们的框架代码，公开可用，请访问：[https://github.com/GAIR-NLP/scaleeval](https://github.com/GAIR-NLP/scaleeval)。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/de2acb14ffbc24bbfa20e350b47856aa.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/de2acb14ffbc24bbfa20e350b47856aa.png)'
- en: 'Figure 1: We demonstrate ScaleEval, our scalable meta-evaluation framework.
    This is used in assessing the reliability and robustness of employing LLMs as
    evaluators for different evaluative purposes.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们展示了 ScaleEval，我们的可扩展元评估框架。这用于评估将 LLMs 用作不同评估目的的评估者的可靠性和稳健性。
- en: 'Large Language Models (LLMs) Bubeck et al. ([2023](#bib.bib3)); Gemini Team
    et al. ([2023](#bib.bib12)) have rapidly evolved to the point where they can tackle
    a wide range of tasks with impressive performance. While this has unlocked a variety
    of exciting potential applications, it has also introduced complex challenges
    in evaluating the generated outputs. Current efforts on LLM evaluation primarily
    focus on automated evaluation metrics (Fu et al., [2023](#bib.bib10); Li et al.,
    [2023c](#bib.bib19); Zheng et al., [2023](#bib.bib25); Wang et al., [2023a](#bib.bib23)),
    many of which use LLMs themselves to do evaluation. However, when these LLMs as
    evaluators are applied to a new task, it begs the question: *can LLMs be trusted
    for evaluation?* In many cases, the answer is not clear.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs） Bubeck 等人 ([2023](#bib.bib3)); Gemini 团队等人 ([2023](#bib.bib12))
    已迅速发展到可以处理广泛任务并表现出色的程度。虽然这开启了各种令人兴奋的潜在应用，但也带来了评估生成输出的复杂挑战。目前对 LLM 评估的努力主要集中在自动评估指标上（Fu
    等人，[2023](#bib.bib10); Li 等人，[2023c](#bib.bib19); Zheng 等人，[2023](#bib.bib25);
    Wang 等人，[2023a](#bib.bib23)），其中许多使用 LLMs 自身进行评估。然而，当这些 LLMs 作为评估者应用于新任务时，就会提出一个问题：*LLMs
    可以信赖用于评估吗？* 在许多情况下，答案并不明确。
- en: On the other hand, there are a few fortunate tasks where meta-evaluation (evaluation
    of evaluation metrics) has been performed rigorously (§[2](#S2 "2 Related Work
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate")). This meta-evaluation typically involves
    the collection of human-annotated judgements for particular criteria (e.g. fluency
    of outputs, semantic adherence to the input). For instance, for machine translation
    quality metrics, there is an extensive meta-evaluation data from the WMT metrics
    task Freitag et al. ([2022](#bib.bib9)), and for summarization there are datasets
    like TAC and RealSum Dang et al. ([2008](#bib.bib8)); Bhandari et al. ([2020](#bib.bib2)).
    Once such a dataset is collected, meta-evaluation can be performed by measuring
    the correlation between automatic evaluation metrics and the human gold-standard
    (§[3](#S3 "3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，少数幸运的任务已经进行了严格的元评估（评估评估指标） (§[2](#S2 "2 相关工作 ‣ 大语言模型是否值得信赖进行评估？通过代理辩论对
    LLMs 作为评估者的可扩展元评估"))。这种元评估通常涉及对特定标准（例如输出的流畅性、对输入的语义遵循）进行人工标注判断的收集。例如，对于机器翻译质量指标，来自
    WMT 指标任务 Freitag 等人 ([2022](#bib.bib9)) 的数据广泛存在，而对于总结任务，则有 TAC 和 RealSum Dang
    等人 ([2008](#bib.bib8))；Bhandari 等人 ([2020](#bib.bib2)) 这样的数据集。一旦收集到这样的数据集，就可以通过测量自动评估指标与人工标准的相关性来进行元评估
    (§[3](#S3 "3 初步工作 ‣ 大语言模型是否值得信赖进行评估？通过代理辩论对 LLMs 作为评估者的可扩展元评估"))。
- en: However, these datasets are extremely costly to collect, as they require meticulous
    annotation by skilled human experts. With the increasing use of LLMs for various
    purposes such as math problem solving Hendrycks et al. ([2021](#bib.bib14)), reading
    comprehension Zhong et al. ([2023](#bib.bib26)), creative writing Zheng et al.
    ([2023](#bib.bib25)), multilingual applications Hu et al. ([2020](#bib.bib15));
    Bang et al. ([2023](#bib.bib1)), and many more, it is not feasible to create these
    human-judged datasets for every new task. As a result, LLMs as evaluators are
    used without proper vetting, and in many cases the evaluators themselves are highly
    unreliable (Wang et al., [2023b](#bib.bib24); Huang et al., [2023](#bib.bib16)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些数据集的收集成本极高，因为它们需要由熟练的人工专家进行细致的标注。随着大语言模型在各种用途上的使用增加，例如数学问题解决 Hendrycks
    等人 ([2021](#bib.bib14))，阅读理解 Zhong 等人 ([2023](#bib.bib26))，创造性写作 Zheng 等人 ([2023](#bib.bib25))，多语言应用
    Hu 等人 ([2020](#bib.bib15))；Bang 等人 ([2023](#bib.bib1))，以及更多领域，为每个新任务创建这些人工评判的数据集变得不可行。因此，LLMs
    作为评估者在没有适当审查的情况下被使用，在许多情况下，评估者本身也是高度不可靠的（Wang 等人，[2023b](#bib.bib24)；Huang 等人，[2023](#bib.bib16)）。
- en: In this paper, we propose ScaleEval, a *scalable meta-evaluation framework*
    for the era of LLMs, which creates meta-evaluation benchmarks across various tasks
    and scenarios (§[4](#S4 "4 Methodology ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
    Concretely, ScaleEval relies on debate between multiple LLM agents, followed by
    minimal human oversight in cases where the agent LLMs do not agree (Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")). Since our
    framework allows users to use their own prompts and responses while applying the
    framework to any scenario or criterion that they define, it offers flexibility
    and adaptability in various evaluation contexts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 ScaleEval，一个*可扩展的元评估框架*，旨在适应大语言模型（LLMs）的时代，该框架在各种任务和场景中创建了元评估基准 (§[4](#S4
    "4 方法论 ‣ 大语言模型是否值得信赖进行评估？通过代理辩论对 LLMs 作为评估者的可扩展元评估"))。具体而言，ScaleEval 依赖于多个 LLM
    代理之间的辩论，然后在代理 LLM 不达成一致时，进行最少的人工监督 (图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 大语言模型是否值得信赖进行评估？通过代理辩论对
    LLMs 作为评估者的可扩展元评估"))。由于我们的框架允许用户在将其应用于任何他们定义的场景或标准时使用自己的提示和响应，因此它在各种评估环境中提供了灵活性和适应性。
- en: 'In experiments, we conduct meta-meta evaluation (§[6](#S6 "6 Exp-I: Meta-Meta-Evaluation
    of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")) demonstrating that our
    proposed approach correlates well with when meta-evaluation is performed entirely
    by human expert annotators. Further, we assess the reliability and cost-performance
    trade-off of various LLMs as evaluators under a variety of scenarios, and closely
    examine their specific capabilities and limitations as evaluators (§[7](#S7 "7
    Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
    We also examine the impact that variations in prompts used for evaluation can
    have on the performance of LLMs as evaluators (§[8](#S8 "8 Exp-III: Meta-Evaluation
    with Criteria Prompt Format Variations ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在实验中，我们进行了元元评估（§[6](#S6 "6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate")），证明我们提出的方法与完全由人工专家注释的元评估结果具有较好的一致性。此外，我们在各种场景下评估了不同LLMs作为评估者的可靠性和性价比，并详细检查了它们作为评估者的特定能力和局限性（§[7](#S7
    "7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。我们还检查了用于评估的提示变化对LLMs作为评估者的表现可能产生的影响（§[8](#S8
    "8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")）。'
- en: All code from our framework is made available open-source, enabling the community
    to conduct meta-evaluation on LLMs as evaluators using their own prompts, LLM
    responses, criteria, and scenarios.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们框架中的所有代码都以开源方式提供，使社区能够使用自己的提示、LLM响应、标准和场景进行LLMs作为评估者的元评估。
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '|  | Meta-Eval | # Scenarios | Custom. | Scala. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | Meta-Eval | # Scenarios | Custom. | Scala. |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLM-as-a-Judge | Human | High | ✗ | Low |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| LLM-as-a-Judge | Human | High | ✗ | Low |'
- en: '| FairEval | Human | Low | ✗ | Low |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| FairEval | Human | Low | ✗ | Low |'
- en: '| ChatEval | Human | Low | ✗ | Low |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ChatEval | Human | Low | ✗ | Low |'
- en: '| ScaleEval | Agent Debate | High | ✓ | High |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ScaleEval | Agent Debate | High | ✓ | High |'
- en: 'Table 1: Comparison of the meta-evaluation processes across different strategies
    using LLMs as evaluators: LLM-as-a-Judge Zheng et al. ([2023](#bib.bib25)), FairEval
    Wang et al. ([2023b](#bib.bib24)), ChatEval Chan et al. ([2023](#bib.bib4)), and
    our own work, ScaleEval. “Custom.” denotes whether the evaluation criterion could
    be customized. “Scala.” refers to scalability.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用LLMs作为评估者的不同策略的元评估过程比较：LLM-as-a-Judge Zheng et al. ([2023](#bib.bib25))、FairEval
    Wang et al. ([2023b](#bib.bib24))、ChatEval Chan et al. ([2023](#bib.bib4)) 和我们自己的工作ScaleEval。“Custom.”表示评估标准是否可以定制。“Scala.”指可扩展性。
- en: 2.1 Automatic Evaluation of LLM Output
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大语言模型输出的自动评估
- en: The most common paradigm for evaluating LLMs is to evaluate their capabilities
    on standard benchmarks for tasks such as reasoning (e.g. BigBench Srivastava et al.
    ([2022](#bib.bib22))), common sense QA (e.g. MMLU Hendrycks et al. ([2020](#bib.bib13))),
    or code generation (e.g. HumanEval Chen et al. ([2021b](#bib.bib6))). These are
    indicative of the capabilities of the models, but do not measure model abilities
    for open-ended tasks requiring generation of free-form text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 评估大语言模型（LLMs）最常见的范式是通过标准基准测试来评估其在推理（例如BigBench Srivastava et al. ([2022](#bib.bib22))）、常识问答（例如MMLU
    Hendrycks et al. ([2020](#bib.bib13))) 或代码生成（例如HumanEval Chen et al. ([2021b](#bib.bib6)))
    等任务上的能力。这些测试能够反映模型的能力，但不能衡量模型在需要生成自由形式文本的开放性任务中的表现。
- en: To adapt to the rapid growth in the capabilities of LLMs for open-ended tasks,
    LLM evaluation has started to shift towards evaluating generated text directly,
    often using LLMs themselves as evaluators Fu et al. ([2023](#bib.bib10)); Li et al.
    ([2023c](#bib.bib19)); Zheng et al. ([2023](#bib.bib25)); Wang et al. ([2023a](#bib.bib23)).
    In addition, there are a few recent works that perform LLM-based multi-agent debate
    to improve the fidelity of evaluation Chan et al. ([2023](#bib.bib4)); Li et al.
    ([2023b](#bib.bib18)). While these methods take advantage of the instruction-following
    capabilities and versatility of LLMs, directly using LLMs as evaluators or communicative
    agents out-of-the-box in diverse, unseen user-defined scenarios provides no guarantees
    with respect to the accuracy of these methods. We aim to address this issue by
    introducing scalable meta-evaluation to ensure the reliability of the evaluation
    protocol under diverse scenarios.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应 LLM 在开放性任务能力上的快速增长，LLM 评估已经开始转向直接评估生成文本，通常使用 LLM 本身作为评估者 Fu 等人 ([2023](#bib.bib10))；Li
    等人 ([2023c](#bib.bib19))；Zheng 等人 ([2023](#bib.bib25))；Wang 等人 ([2023a](#bib.bib23))。此外，还有一些最近的研究通过
    LLM 基础的多智能体辩论来提高评估的准确性 Chan 等人 ([2023](#bib.bib4))；Li 等人 ([2023b](#bib.bib18))。虽然这些方法利用了
    LLM 的指令跟随能力和多样性，但直接将 LLM 作为评估者或沟通代理在各种未见过的用户定义场景中使用，并不能保证这些方法的准确性。我们旨在通过引入可扩展的元评估来解决这个问题，以确保在不同场景下评估协议的可靠性。
- en: Another widely used evaluation platform, Chatbot Arena Zheng et al. ([2023](#bib.bib25))
    supports a crowd-sourcing method to collect diverse user prompts from various
    scenarios. However, the process of evaluating LLMs’ performance in Chatbot Arena
    relies heavily on human evaluations, which may not be readily accessible to everyone
    interested in assessing LLMs’ abilities for a specific tasks or scenario. In addition,
    the human evaluators involved are not subject to a uniform set of standards or
    explicit evaluation guidelines, which could lead to biased or imprecise evaluation
    assessments.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的评估平台是 Chatbot Arena Zheng 等人 ([2023](#bib.bib25))，它支持一种众包方法来收集来自不同场景的多样化用户提示。然而，在
    Chatbot Arena 中评估 LLM 性能的过程在很大程度上依赖于人工评估，这可能并不容易为所有希望评估 LLM 在特定任务或场景中能力的人所获得。此外，参与评估的人工评估者并没有统一的标准或明确的评估指南，这可能导致评估结果存在偏差或不准确。
- en: 2.2 Meta-Evaluation of LLMs as Evaluators
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM 作为评估者的元评估
- en: 'Previous research proposing methods for LLMs as evaluators usually involves
    conducting meta-evaluation in 3 different ways: (i) leveraging existing NLP meta-evaluation
    benchmarks Fu et al. ([2023](#bib.bib10)); Chan et al. ([2023](#bib.bib4)), (ii)
    conducting small-scale meta-evaluations on expert-annotated datasets for specific
    tasks or scenarios Chiang and Lee ([2023](#bib.bib7)); Wang et al. ([2023a](#bib.bib23));
    Zheng et al. ([2023](#bib.bib25)), or (iii) using crowd-sourcing platforms to
    collect human annotations Zheng et al. ([2023](#bib.bib25)). However, due to the
    lack of coverage in existing datasets and annotation budgets, both (i) and (ii)
    are inherently limited in their comprehensiveness. (iii) can provide more comprehensive
    meta-evaluation via crowd-sourcing, but the amount of human annotation required
    in the meta-evaluation process limits the scalability of the approach, and crowd
    workers may not be particularly accurate at more complex tasks. To address these
    issues, we propose an agent-debate-assisted meta-evaluation approach to mitigate
    this effort.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提出的将 LLM 作为评估者的方法通常涉及以下三种不同的元评估方式：（i）利用现有的 NLP 元评估基准 Fu 等人 ([2023](#bib.bib10))；Chan
    等人 ([2023](#bib.bib4))，（ii）对专家注释的数据集进行小规模的元评估，以针对特定任务或场景 Chiang 和 Lee ([2023](#bib.bib7))；Wang
    等人 ([2023a](#bib.bib23))；Zheng 等人 ([2023](#bib.bib25))，或（iii）使用众包平台收集人工注释 Zheng
    等人 ([2023](#bib.bib25))。然而，由于现有数据集和注释预算的覆盖范围不足，（i）和（ii）在全面性上本质上存在局限性。（iii）通过众包可以提供更全面的元评估，但在元评估过程中所需的人工注释量限制了该方法的可扩展性，且众包工作者在处理更复杂任务时可能并不特别准确。为了解决这些问题，我们提出了一种辅助的智能体辩论元评估方法，以减轻这一工作量。
- en: 3 Preliminaries
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 前言
- en: In this section, we provide an introduction to the concepts of automatic evaluation
    and meta-evaluation systems, particularly focused on evaluation of LLM-generated
    outputs in the era of generative AI.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了自动评估和元评估系统的概念，特别是关注于生成性 AI 时代 LLM 生成输出的评估。
- en: 3.1 Key Terms
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 关键术语
- en: We first define some key terms that will be used throughout our paper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一些将在论文中使用的关键术语。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Criterion: A criterion defines a standard that measures the quality of the
    response generated by LLMs based on the user prompt. Some examples include: helpfulness,
    fluency, factuality, or creativity, among others.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准：标准定义了一个衡量LLM生成响应质量的标准，基于用户提示。一些例子包括：有用性、流畅性、事实性或创造力等。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scenario: A scenario describes the real-world situations in which users are
    interacting with LLMs. For example, brainstorming, coding, and dialog, among others.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景：场景描述了用户与LLM交互的实际情况。例如，头脑风暴、编码和对话等。
- en: 3.2 Automatic Evaluation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自动评估
- en: 'Automatic evaluation using LLMs measures the quality of LLM-generated responses
    given prompts under different criteria. Usually, automatic evaluation is conducted
    with one of two different protocols: single-response evaluation and pairwise response
    comparison Ouyang et al. ([2022](#bib.bib21)); Zheng et al. ([2023](#bib.bib25));
    Li et al. ([2023a](#bib.bib17)). In this paper, we focus on pairwise response
    comparison. Pairwise response comparison is intuitive for both humans and LLMs
    as evaluators when conducting assessments. It could be further extended to provide
    win-rates and Elo scores across models Zheng et al. ([2023](#bib.bib25)), offering
    a straightforward leaderboard to understand the relative performance of different
    models under various scenarios. Formally, given an automatic evaluation metric
    $E$, evaluation for pairwise response comparison is done in the following way:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM的自动评估测量根据不同标准生成的LLM响应的质量。通常，自动评估有两种不同的协议：单响应评估和成对响应比较 Ouyang 等 ([2022](#bib.bib21))；Zheng
    等 ([2023](#bib.bib25))；Li 等 ([2023a](#bib.bib17))。在本文中，我们专注于成对响应比较。成对响应比较对于人类和LLM作为评估者在进行评估时都很直观。它可以进一步扩展为提供模型的胜率和Elo分数
    Zheng 等 ([2023](#bib.bib25))，提供一个简单的排行榜，以了解不同模型在各种场景下的相对性能。形式上，给定一个自动评估指标 $E$，成对响应比较的评估方式如下：
- en: '|  | $o=E(c,p,r_{1},r_{2}).$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $o=E(c,p,r_{1},r_{2})$。 |  | (1) |'
- en: $o\in\{1,0,-1\}$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $o\in\{1,0,-1\}$。
- en: 3.3 Meta-Evaluation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 元评估
- en: Meta-evaluation assesses the quality of an automatic evaluation metric. Formally,
    we define a gold-standard evaluation metric $G$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 元评估评估自动评估指标的质量。形式上，我们定义一个黄金标准评估指标 $G$。
- en: In pairwise response comparison, the meta-evaluation measures the example-level
    agreement rate or the system-level agreement rate between $E$ is a good automatic
    evaluation metric.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在成对响应比较中，元评估测量示例级别的一致率或系统级别的一致率，其中$E$是一个好的自动评估指标。
- en: 'For the example-level agreement rate, we calculate:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于示例级别的一致率，我们计算：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $0\leq\textsc{meta}(E)\leq 1$ refers to the Kronecker delta function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $0\leq\textsc{meta}(E)\leq 1$ 指的是克罗内克 delta 函数。
- en: 'For the system-level agreement rate, given that $\mathcal{E}=\{E(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$,
    we calculate:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于系统级别的一致率，给定 $\mathcal{E}=\{E(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$，我们计算：
- en: '|  | $\textsc{meta}(E)=\delta_{\mathrm{mode}(\mathcal{E}),\mathrm{mode}(\mathcal{G})},$
    |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{meta}(E)=\delta_{\mathrm{mode}(\mathcal{E}),\mathrm{mode}(\mathcal{G})}$，
    |  | (3) |'
- en: where $\textsc{meta}(E)\in\{0,1\}$.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textsc{meta}(E)\in\{0,1\}$。
- en: 4 Methodology
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: In this section, we detail the frameworks that ScaleEval employs for meta-evaluation,
    evaluation, and human expert meta-meta evaluation. For meta-evaluation, we generally
    follow the pairwise response comparison setting described in §[3.3](#S3.SS3 "3.3
    Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). Notably, instead
    of relying solely on human labor to construct the meta-evaluation benchmark $\mathcal{G}$.
    For evaluation, we follow the pairwise response comparison setting outlined in
    §[3.2](#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language
    Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators
    via Agent Debate"). The meta-meta evaluation process also follows the rules for
    meta-evaluation, as described in §[3.3](#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate"). The process is included to ensure the
    reliability of using the agent-debate assisted meta-evaluation framework.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了ScaleEval在元评估、评估和人类专家元元评估中采用的框架。对于元评估，我们通常遵循第§[3.3](#S3.SS3 "3.3
    Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中描述的成对响应比较设置。值得注意的是，我们不仅依赖人工来构建元评估基准$\mathcal{G}$。对于评估，我们遵循第§[3.2](#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中概述的成对响应比较设置。元元评估过程也遵循第§[3.3](#S3.SS3
    "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中描述的元评估规则。此过程的纳入是为了确保使用代理辩论辅助的元评估框架的可靠性。
- en: 4.1 Meta-Evaluation Framework via Multi-Agent Debate
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 通过多代理辩论的元评估框架
- en: The meta-evaluation framework involves multiple communicative agents $\{A_{j}\}_{j=1}^{m}$
    to make a comprehensive assessment of LLMs under different scenarios and criteria.
    Each LLM agent is capable of providing an evaluation result regarding which response
    is better, along with its corresponding justifications. Note that each LLM agent
    can also review other agents’ evaluation results and justifications after the
    initial round of discussion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 元评估框架涉及多个交流代理$\{A_{j}\}_{j=1}^{m}$在不同场景和标准下对LLMs进行全面评估。每个LLM代理能够提供关于哪个响应更好的评估结果，并附上相应的理由。注意，每个LLM代理还可以在初始讨论回合后审查其他代理的评估结果和理由。
- en: 'In the initial round of discussion $d=0$, each LLM agent independently provides
    an evaluation result and justification:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始讨论回合 $d=0$ 中，每个LLM代理独立地提供评估结果和理由：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $\mathcal{A}_{0}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{0}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (5) |'
- en: 'indicates whether $r_{1,i}$ round of discussion:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表示是否 $r_{1,i}$ 讨论回合：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where similarly to $\mathcal{A}_{0}$,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中类似于 $\mathcal{A}_{0}$，
- en: '|  | $\mathcal{A}_{d}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (7) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{d}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (7) |'
- en: The detailed prompt template for meta-evaluation can be found in Table [6](#A1.T6
    "Table 6 ‣ Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")
    under Appendix.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 元评估的详细提示模板可以在附录中的表 [6](#A1.T6 "Table 6 ‣ Appendix A Meta-Evaluation Prompt ‣
    Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") 中找到。
- en: In cases where agents fail to reach a consensus after $d=D-1$ rounds of discussions,
    a human evaluator intervenes. The human evaluator reviews the assessment reports
    provided by the agents and makes a final decision. Through this process, we incorporate
    an element of human oversight, thereby increasing the reliability of the final
    decision. This approach strikes a balance between efficiency and the need for
    human judgment, ensuring that evaluations are done in a timely and accurate manner.
    An example of the multi-agent debate process during meta-evaluation is demonstrated
    in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Meta-Evaluation Framework via Multi-Agent Debate
    ‣ 4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate").
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理商在经过$d=D-1$轮讨论后仍未达成共识的情况下，人类评估员会介入。人类评估员审核代理商提供的评估报告，并作出最终决策。通过这一过程，我们引入了人类监督的元素，从而提高了最终决策的可靠性。这种方法在效率和对人类判断的需求之间取得了平衡，确保评估在及时和准确的方式下完成。图[2](#S4.F2
    "Figure 2 ‣ 4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate")展示了多代理辩论过程的一个示例。
- en: '![Refer to caption](img/27c70a5fa3443a5ef0623e8b0858ded5.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/27c70a5fa3443a5ef0623e8b0858ded5.png)'
- en: 'Figure 2: An example of the multi-agent debate process during meta-evaluation.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 元评估过程中多代理辩论的一个示例。'
- en: 4.2 Evaluation Framework
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估框架
- en: We follow the pairwise response comparison setting outlined in §[3.2](#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    Note that in the LLM era, the automatic evaluation metric $E$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了第§[3.2](#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")节中概述的成对响应比较设置。注意，在LLM时代，自动评估指标$E$。
- en: 4.3 Human Expert Meta-Meta Evaluation
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 人类专家的元-元评估
- en: To test the reliability of our proposed meta-evaluation framework, we apply
    meta-meta evaluation. The meta-meta evaluation process also follows the meta-evaluation
    process described in §[3.3](#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can
    Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs
    as Evaluators via Agent Debate"), where $E$ is instantiated as the human expert
    annotation protocol.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们提出的元评估框架的可靠性，我们应用了元-元评估。元-元评估过程也遵循了第§[3.3](#S3.SS3 "3.3 Meta-Evaluation
    ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中描述的元评估过程，其中$E$被具体化为人类专家注释协议。
- en: 5 Examined Scenarios
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种被检验的场景
- en: Establishing real-life scenarios that reflect individuals’ daily usage is key
    to assess the performance and limitations of LLMs in a comprehensive manner. In
    the current instantiation of ScaleEval, we include 8 different scenarios that
    are closely related to everyday situations and tasks Liang et al. ([2022](#bib.bib20));
    Li et al. ([2023a](#bib.bib17)). Some example prompts for each defined scenario
    is shown in Table [2](#S5.T2 "Table 2 ‣ 5 Examined Scenarios ‣ Can Large Language
    Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators
    via Agent Debate"). We describe more about exactly how we collect data for each
    of these scenarios below. Individuals interested in evaluating LLMs with our framework
    can supplement their assessment with additional scenarios.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 建立反映个人日常使用的现实生活场景是全面评估LLMs性能和局限性的关键。在当前的ScaleEval实例中，我们包括了8种与日常情况和任务密切相关的不同场景
    Liang et al. ([2022](#bib.bib20)); Li et al. ([2023a](#bib.bib17))。每个定义的场景的示例提示如表[2](#S5.T2
    "Table 2 ‣ 5 Examined Scenarios ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")所示。我们将更详细地描述我们如何为每种场景收集数据。对使用我们框架评估LLMs感兴趣的个人可以通过额外场景来补充他们的评估。
- en: '| Scenario | Examples |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 示例 |'
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Brainstorming | - Can you tell me how to make chocolate chip cookies? |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 头脑风暴 | - 你能告诉我怎么做巧克力曲奇吗？ |'
- en: '| - Make a list of snacks and foods to serve as party snacks on a game day!
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| - 制定一个清单，列出在比赛日作为派对小吃的零食和食物！ |'
- en: '| Coding | - What is the difference between HTML and JavaScript? |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 编码 | - HTML和JavaScript之间有什么区别？ |'
- en: '| - Implement a binary search algorithm to find a specific element in a sorted
    array. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| - 实现一个二分查找算法以在排序数组中查找特定元素。 |'
- en: '| Dialog | - Act as the Norse Goddess Freyja. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | - 充当北欧女神弗蕾雅。 |'
- en: '| - Can you think and feel like a human? |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| - 你能像人类一样思考和感受吗？ |'
- en: '| Judgement | - What if the Aztecs had successfully repelled the Spanish conquistadors?
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | - 如果阿兹特克人成功击退了西班牙征服者会怎样？ |'
- en: '| - How can you determine if a person is genuinely interested in a conversation
    or simply being polite? |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| - 如何判断一个人是否真正对谈话感兴趣，还是只是出于礼貌？ |'
- en: '| Math | - Given that f(x) = 5$x^{3}$ + 3, find the value of f(2). |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | - 给定 f(x) = 5$x^{3}$ + 3，求 f(2) 的值。 |'
- en: '| - If the endpoints of a line segment are (2, -2) and (10, 4), what is the
    length of the segment? |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| - 如果线段的端点是 (2, -2) 和 (10, 4)，那么线段的长度是多少？ |'
- en: '| ODG | - Is there a meaning for Christmas wreaths? |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ODG | - 圣诞花环有什么意义？ |'
- en: '| - What are some of the best universities for studying robotics? |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| - 哪些是学习机器人技术的最佳大学？ |'
- en: '| ODS | - What causes the northern lights? |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ODS | - 北极光的形成原因是什么？ |'
- en: '| - What do the different octane values of gasoline mean? |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| - 不同辛烷值的汽油是什么意思？ |'
- en: '| Writing | - Can you help me write a formal email to a potential business
    partner proposing a joint venture? |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 写作 | - 能否帮我写一封正式的邮件给潜在的商业合作伙伴，提议进行合资合作？ |'
- en: '| - Take MLK speech "I had a dream" but turn it into a top 100 rap song. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| - 将马丁·路德·金的演讲《我有一个梦想》改编成一首前100名的说唱歌曲。 |'
- en: 'Table 2: Examined scenarios and corresponding selected examples.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 审查的情境及其对应的示例。'
- en: Brainstorming
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 头脑风暴
- en: The brainstorming scenario is designed to test the LLMs’ ability to engage in
    problem-solving, creative ideation, and generation of insightful responses, especially
    in situations that require critical thinking and detailed, step-by-step reasoning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 头脑风暴情境旨在测试LLMs在问题解决、创意构思和生成有见地的回应方面的能力，特别是在需要批判性思维和详细逐步推理的情况下。
- en: Coding
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编程
- en: The code scenario evaluates LLMs’ ability to comprehend, produce, and debug
    code, as well as answering coding-related questions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 代码情境评估LLMs理解、生成和调试代码的能力，以及回答与编程相关的问题。
- en: Dialog
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对话
- en: The dialog scenario measures LLMs’ ability to engage with users in a manner
    that is intuitive, human-like, and dynamic, testing their proficiency through
    context-sensitive conversations and role-playing that require maintaining a consistent
    persona throughout a series of interactions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对话情境评估LLMs与用户互动的能力，这种互动方式直观、人性化且动态，测试它们通过上下文敏感的对话和角色扮演来保持一致的角色。
- en: Judgement
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 判断
- en: The judgement scenario assesses LLMs‘ ability to make inferences and formulate
    opinions, including soliciting insights on diverse situations or emotions, and
    posing questions that require logical thinking or reasoning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 判断情境评估LLMs进行推理和形成意见的能力，包括对不同情境或情感的见解，并提出需要逻辑思维或推理的问题。
- en: Math
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学
- en: The math scenario evaluates the LLMs’ proficiency in understanding and solving
    mathematical problems, emphasizing their accuracy in tasks ranging from simple
    calculations to complex reasoning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数学情境评估LLMs理解和解决数学问题的能力，强调从简单计算到复杂推理的准确性。
- en: Open-Domain General (ODG)
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开放领域通用（ODG）
- en: The ODG scenario measures LLMs’ proficiency in applying diverse knowledge and
    exercising reasoning across a wide array of topics, such as answering questions
    with definitive answers.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ODG情境衡量LLMs在应用多样知识和跨广泛主题进行推理的能力，例如回答有确定答案的问题。
- en: Open-Domain Science (ODS)
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开放领域科学（ODS）
- en: The ODS scenario tests the LLMs’ application of scientific knowledge, and gauges
    their ability to accurately interpret and respond to queries related to scientific
    disciplines like biology, chemistry, physics, astronomy, and more.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ODS情境测试LLMs应用科学知识的能力，并评估它们准确解读和回应与生物学、化学、物理学、天文学等科学学科相关的查询的能力。
- en: Writing
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写作
- en: The writing scenario evaluates LLMs’ ability to summarize, translate, and generate
    various texts, testing their core language processing and production skills.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 写作情境评估LLMs总结、翻译和生成各种文本的能力，测试它们的核心语言处理和生成技能。
- en: '6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '6 Exp-I: 多代理辩论的元元评估'
- en: In this section, we first perform meta-meta-evaluation, examining whether the
    meta-evaluation results of using ScaleEval match closely to those resulting from
    meta-evaluation using human evaluators.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先进行元元评估，检查使用ScaleEval的元评估结果是否与使用人工评估者的元评估结果相匹配。
- en: Setup
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'For our ScaleEval meta-evaluation framework (as described in §[4.1](#S4.SS1
    "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")), we deploy three LLM agents to perform multi-agent
    debate: gpt-4-turbo, claude-2, and gpt-3.5-turbo.¹¹1Results collected in December
    2023\. Specific models used are: gpt-4-1106-preview, claude-2, and gpt-3.5-turbo-1106.
    In our meta-evaluation experiment, we analyze a total of 160 prompts. This set
    is comprised 137 prompts from AlpacaEval Li et al. ([2023c](#bib.bib19)), 10 coding
    problem prompts from HumanEval Chen et al. ([2021a](#bib.bib5)), and 13 math problem
    prompts from GSM-Hard Gao et al. ([2022](#bib.bib11)). We categorize these prompts
    into four distinct scenarios: brainstorming, coding, math, and writing, where
    each scenario contains 40 prompts.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 ScaleEval 元评估框架（如在 §[4.1](#S4.SS1 "4.1 Meta-Evaluation Framework via Multi-Agent
    Debate ‣ 4 Methodology ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate") 中所述），我们部署了三个
    LLM 代理进行多代理辩论：gpt-4-turbo、claude-2 和 gpt-3.5-turbo。¹¹1 结果收集于 2023 年 12 月。使用的具体模型为：gpt-4-1106-preview、claude-2
    和 gpt-3.5-turbo-1106。在我们的元评估实验中，我们分析了总共 160 个提示。这些提示集包括来自 AlpacaEval Li 等人的 137
    个提示（[2023c](#bib.bib19)）、来自 HumanEval Chen 等人的 10 个编码问题提示（[2021a](#bib.bib5)）以及来自
    GSM-Hard Gao 等人的 13 个数学问题提示（[2022](#bib.bib11)）。我们将这些提示分为四个不同的场景：头脑风暴、编码、数学和写作，每个场景包含
    40 个提示。
- en: 'Each scenario is evaluated based on the following criteria, respectively: helpfulness,
    interpretability, reasoning, and creativity. We evaluate the generated responses
    from the following three LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro.
    We select the above LLMs to evaluate due to their rather similar performances
    according to past research and public user feedback, which can help us establish
    a more nuanced understanding of their performance in various real-world scenarios,
    and to identify specific contexts where one may outperform the others.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个场景根据以下标准进行评估：有用性、可解释性、推理和创造力。我们对以下三种 LLM 生成的响应进行评估：gpt-3.5-turbo、claude-instant
    和 gemini-pro。我们选择上述 LLM 进行评估，是因为根据过去的研究和公众用户反馈，它们的表现相当相似，这可以帮助我们建立对它们在各种真实世界场景中的表现的更细致理解，并确定在特定背景下可能超越其他模型的情况。
- en: Our meta-meta evaluation involves having human experts annotate which LLM submission
    they think is better based on a defined criterion during pairwise comparisons.
    A total of seven human experts were selected from a pool of Carnegie Mellon University
    students who have the relevant expertise in answering the queries in each scenario.
    Different groups of three human experts are responsible for answering the prompts
    in each scenario, where they are assigned to the scenario that relates to their
    expertise. Each expert received identical instructions for the task – they were
    asked to decide which submission is better based on our defined criteria, and
    for each comparison, label either 0 (neither submission is better), 1 (submission
    1 is better), or 2 (submission 2 is better). The label 2 corresponds to the label
    -1 as denoted in section [3.2](#S3.SS2 "3.2 Automatic Evaluation ‣ 3 Preliminaries
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate"). The experts were tasked to conduct 30
    comparisons for each of the four different scenarios (brainstorming, coding, math,
    and writing), based on their corresponding defined criteria (helpfulness, interpretability,
    reasoning, and creativity). This results in a total of 120 final judgements. The
    question prompts, LLM responses, and criteria utilized for human expert annotations
    were consistent with those used during our meta-evaluation experiment. All the
    details were presented in a google sheet that allowed experts to record their
    answers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的元元评估涉及让人类专家在成对比较中根据定义的标准标注他们认为更好的LLM提交。总共从一群具有相关回答场景查询的卡内基梅隆大学学生中选出了七位人类专家。不同的三位专家组负责回答每个场景中的提示，他们被分配到与他们的专业相关的场景。每位专家收到相同的任务说明——他们被要求根据我们的定义标准决定哪个提交更好，对于每个比较，标记为0（两个提交都不更好）、1（提交1更好）或2（提交2更好）。标记2对应于第[3.2](#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中表示的标记-1。专家们被要求针对四个不同场景（头脑风暴、编码、数学和写作）进行30次比较，根据他们对应的定义标准（帮助性、可解释性、推理能力和创造力）。这总共产生了120个最终判断。问题提示、LLM响应和用于人类专家注释的标准与我们在元评估实验中使用的一致。所有细节都在一个谷歌表格中呈现，专家可以在其中记录他们的答案。
- en: 'Q1: Can LLM agents with multi-agent debate be used as meta-evaluators in new
    user-defined scenarios?'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Q1: 多代理辩论中的LLM代理可以在新的用户定义场景中作为元评估者使用吗？'
- en: 'To validate the reliability of ScaleEval’s meta-evaluation framework, we perform
    comparisons between the results from human experts and ScaleEval’s multi-agent
    debate by two key metrics: the example-level agreement rate and the system-level
    agreement rate, as mentioned in §[3.3](#S3.SS3 "3.3 Meta-Evaluation ‣ 3 Preliminaries
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate"). The example-level agreement rate measures
    the proportion of instances where the multi-agent debate results correspond with
    the human experts judgements. On the other hand, the system-level agreement rate
    assesses whether the human experts and multi-agents concur in their overall evaluation
    of which LLMs produce the best responses for each scenario. A high agreement rate
    in both metrics would suggest a strong reliability and validity of our meta-evaluation
    framework, indicating that both human and LLM agents consistently recognize and
    agree on the quality of responses generated by LLMs.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证ScaleEval的元评估框架的可靠性，我们通过两个关键指标进行比较：样本级一致率和系统级一致率，这些指标如§[3.3](#S3.SS3 "3.3
    Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")中所述。样本级一致率衡量多代理辩论结果与人类专家判断的一致程度。另一方面，系统级一致率评估人类专家与多代理是否在总体评价中一致，即哪些LLM在每个场景中产生了最佳的响应。两个指标中一致率高表明我们的元评估框架具有较强的可靠性和有效性，表明人类和LLM代理一致地识别和同意LLM生成的响应的质量。
- en: Results
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'From Table [3](#S6.T3 "Table 3 ‣ Results ‣ 6 Exp-I: Meta-Meta-Evaluation of
    Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we generally observe
    a higher example-level agreement rate between human experts and ScaleEval, compared
    to the agreement rate between human experts and individual LLM evaluations. The
    consistently high agreement rates observed suggest that our meta-evaluation framework
    aligns well with human expert judgments in these areas, indicating a reliable
    performance of the collective use of LLMs in meta-evaluating complex scenarios.
    Across all LLM submission comparisons in our experiment, we observe higher agreement
    rates in decisions between ScaleEval outcomes and those of human experts, particularly
    in coding and math scenarios. This observed trend could be attributed to the inherently
    objective nature of these subjects, which have relatively clear, definitive answers
    unlike more subjective areas like creative writing.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '从表 [3](#S6.T3 "表 3 ‣ 结果 ‣ 6 Exp-I: 多代理辩论的元-元评估 ‣ 大型语言模型是否值得信赖用于评估？通过代理辩论的可扩展元评估")
    中，我们通常观察到人类专家和ScaleEval之间的例子级一致率较高，相比于人类专家和单个LLM评估之间的一致率。观察到的一致率持续较高，表明我们的元评估框架在这些领域与人类专家判断较为一致，说明LLM在复杂场景中的集体使用具有可靠的表现。在我们实验中的所有LLM提交比较中，我们观察到ScaleEval结果和人类专家决策之间的一致率较高，特别是在编码和数学场景中。这种趋势可能归因于这些学科的本质客观性，它们具有相对明确的、确定的答案，不像更主观的领域如创意写作。'
- en: 'Based on Fig. [3](#S6.F3 "Figure 3 ‣ Results ‣ 6 Exp-I: Meta-Meta-Evaluation
    of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we notice a consistent
    "preference in the same direction" between human experts and multi-agent debates
    across all LLM pairwise comparisons and scenarios. Notably, gpt-3.5-turbo is favored
    (higher win rates) in brainstorming, math, and writing scenarios when compared
    with claude-instant. Similarly, gemini-pro is also preferred over claude-instant
    in all scenarios. When comparing gpt-3.5-turbo with gemini-pro, a varied pattern
    in decision outcomes is observed: both human experts and multi-agent systems agree
    that gpt-3.5-turbo outperforms gemini-pro in scenarios involving math and writing.
    Conversely, gemini-pro is deemed superior in brainstorming and coding scenarios.
    The high agreement of multi-agent preferences with human expert judgement results
    verifies the reliability of using multiple LLMs agents as meta-evaluators in various
    user-defined scenarios.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '根据图 [3](#S6.F3 "图 3 ‣ 结果 ‣ 6 Exp-I: 多代理辩论的元-元评估 ‣ 大型语言模型是否值得信赖用于评估？通过代理辩论的可扩展元评估")，我们注意到在人类专家和多代理辩论之间，在所有LLM配对比较和场景中存在一致的“相同方向偏好”。值得注意的是，与claude-instant相比，gpt-3.5-turbo在头脑风暴、数学和写作场景中更受青睐（获胜率更高）。类似地，gemini-pro在所有场景中也优于claude-instant。在比较gpt-3.5-turbo和gemini-pro时，观察到决策结果的多样化模式：人类专家和多代理系统一致认为gpt-3.5-turbo在数学和写作场景中优于gemini-pro。相反，gemini-pro在头脑风暴和编码场景中被认为更优。多代理偏好与人类专家判断的高度一致验证了在各种用户定义场景中使用多个LLM代理作为元评估者的可靠性。'
- en: '| LLM Pairwise Comparisons | Criterion | Scenario | Meta-Evaluation | GPT-4-Turbo
    | Claude-2 | GPT-3.5-Turbo |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM 配对比较 | 标准 | 场景 | 元评估 | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-3.5-Turbo vs. Claude-Instant | Helpfulness | Brainstorming | 0.600 |
    0.633 | 0.433 | 0.267 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo 与 Claude-Instant | 有用性 | 头脑风暴 | 0.600 | 0.633 | 0.433 | 0.267
    |'
- en: '|  | Interpretability | Coding | 0.733 | 0.700 | 0.533 | 0.567 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.733 | 0.700 | 0.533 | 0.567 |'
- en: '|  | Reasoning | Math | 0.867 | 0.600 | 0.400 | 0.367 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.867 | 0.600 | 0.400 | 0.367 |'
- en: '|  | Creativity | Writing | 0.700 | 0.667 | 0.400 | 0.333 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.700 | 0.667 | 0.400 | 0.333 |'
- en: '| Claude-Instant vs. Gemini-Pro | Helpfulness | Brainstorming | 0.667 | 0.533
    | 0.467 | 0.500 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Claude-Instant 与 Gemini-Pro | 有用性 | 头脑风暴 | 0.667 | 0.533 | 0.467 | 0.500
    |'
- en: '|  | Interpretability | Coding | 0.833 | 0.600 | 0.500 | 0.567 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.833 | 0.600 | 0.500 | 0.567 |'
- en: '|  | Reasoning | Math | 0.767 | 0.667 | 0.330 | 0.367 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.767 | 0.667 | 0.330 | 0.367 |'
- en: '|  | Creativity | Writing | 0.733 | 0.633 | 0.400 | 0.500 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.733 | 0.633 | 0.400 | 0.500 |'
- en: '| GPT-3.5-Turbo vs. Gemini-Pro | Helpfulness | Brainstorming | 0.733 | 0.600
    | 0.467 | 0.467 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo 与 Gemini-Pro | 有用性 | 头脑风暴 | 0.733 | 0.600 | 0.467 | 0.467 |'
- en: '|  | Interpretability | Coding | 0.833 | 0.733 | 0.567 | 0.667 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.833 | 0.733 | 0.567 | 0.667 |'
- en: '|  | Reasoning | Math | 0.867 | 0.767 | 0.500 | 0.433 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.867 | 0.767 | 0.500 | 0.433 |'
- en: '|  | Creativity | Writing | 0.767 | 0.667 | 0.500 | 0.433 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.767 | 0.667 | 0.500 | 0.433 |'
- en: 'Table 3: Example-level agreement rate comparison between human expert and ScaleEval’s
    meta-evaluation vs. human expert and single LLM evaluation across four scenarios
    and criteria.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：人类专家与ScaleEval的元评估在四种场景和标准下的示例级别一致性率比较。
- en: '![Refer to caption](img/94ac091abcc90812a9ce178f6dd82fdb.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/94ac091abcc90812a9ce178f6dd82fdb.png)'
- en: (a) GPT-3.5-Turbo vs. Claude-Instant
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPT-3.5-Turbo 与 Claude-Instant
- en: '![Refer to caption](img/58187d6c92a28030224f0235c478094c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/58187d6c92a28030224f0235c478094c.png)'
- en: (b) Claude-Instant vs. Gemini-Pro
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Claude-Instant 与 Gemini-Pro
- en: '![Refer to caption](img/bb0296cc95403bd1c082863792bc87be.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bb0296cc95403bd1c082863792bc87be.png)'
- en: (c) GPT-3.5-Turbo vs. Gemini-Pro
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (c) GPT-3.5-Turbo 与 Gemini-Pro
- en: 'Figure 3: System-level agreement – win rates for each LLM pairwise comparison.
    Left bars in each scenario represent human expert results; right bars represent
    ScaleEval’s meta-evaluation results.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：系统级别一致性——每对LLM比较的胜率。每个场景中的左侧条形图代表人类专家结果；右侧条形图代表ScaleEval的元评估结果。
- en: '![Refer to caption](img/d349af3b522373a9f3d62c9f5827bd11.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d349af3b522373a9f3d62c9f5827bd11.png)'
- en: 'Figure 4: Human Fleiss Kappa for each LLM pairwise comparison under four scenarios.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：每对LLM比较下的四种场景下的人类Fleiss Kappa。
- en: '7 Exp-II: Meta-Evaluation vs. LLM Evaluators'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 Exp-II：元评估与LLM评估者
- en: Next, we use the fact that ScaleEval allows for reliable and scalable meta-evaluation
    to examine the traits of LLMs as evaluators.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们利用ScaleEval提供可靠且可扩展的元评估功能，来考察LLM作为评估者的特征。
- en: 'Q2: What are the capabilities and limitations of each LLM evaluator?'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题2：每个LLM评估者的能力和局限性是什么？
- en: To effectively evaluate the performance of each LLM in its role as an evaluator,
    we adopt an approach that involves comparing the outcomes from our meta-evaluation
    process with the evaluations made independently by each LLM evaluator, which uncovers
    any disagreements or alignments between them. In the process, we aim to shed light
    on the performance characteristics of each LLM evaluator, which helps us identify
    which of them demonstrate superior evaluative abilities, thereby contributing
    to our understanding of their reliability in evaluating responses under each scenario.
    In addition, we provide a comprehensive cost-performance analysis to decide which
    LLM evaluator is the most suitable choice in each scenario.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为有效评估每个LLM在评估者角色中的表现，我们采用了一种方法，将我们的元评估过程中的结果与每个LLM评估者独立进行的评估结果进行比较，从而揭示它们之间的任何分歧或一致性。在此过程中，我们旨在揭示每个LLM评估者的性能特征，帮助我们确定哪些评估者表现出更卓越的评估能力，从而增强我们对它们在各个场景下评估响应可靠性的理解。此外，我们还提供全面的成本性能分析，以决定在每个场景下最合适的LLM评估者。
- en: Setup
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'For meta-evaluation, we employed three LLMs (gpt-4-turbo, claude-2, and gpt-3.5-turbo)
    as evaluators to perform pairwise comparisons of responses from three distinct
    LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro. Previous studies have highlighted
    the presence of positional biases when LLMs are used as evaluators Wang et al.
    ([2023b](#bib.bib24)). In response to these findings, we have implemented a strategy
    of randomization to mitigate such biases. Specifically, the sequence in which
    submissions from LLMs are presented to the agent evaluators is randomized. Additionally,
    we also randomize the order of discussions for each agent evaluator in every case.
    These approaches ensure that the process is fair and unbiased as much as possible,
    allowing for a more accurate assessment of the LLM evaluators’ performance. The
    meta-evaluations were done under the following 8 scenarios: brainstorming, coding,
    dialog, judgement, open-domain general, open-domain science, and writing, with
    the same set of 4 criteria used during human expert annotation.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于元评估，我们采用了三种 LLM（gpt-4-turbo、claude-2 和 gpt-3.5-turbo）作为评估者，对三种不同 LLM（gpt-3.5-turbo、claude-instant
    和 gemini-pro）的响应进行了成对比较。先前的研究指出，当 LLM 作为评估者时存在位置偏见 Wang 等人 ([2023b](#bib.bib24))。对此，我们实施了随机化策略来缓解这些偏见。具体来说，LLM
    提交给代理评估者的顺序是随机的。此外，我们还对每个代理评估者的讨论顺序进行了随机化。这些方法确保了过程尽可能公平和无偏，从而更准确地评估 LLM 评估者的表现。元评估在以下
    8 个场景下进行：头脑风暴、编码、对话、判断、开放域通用、开放域科学和写作，使用相同的 4 项标准进行人工专家注释。
- en: Results
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Table [4](#S7.T4 "Table 4 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators
    ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") compares the agreement rate between ScaleEval’s
    meta-evaluation and each LLM evaluator across criteria and scenarios. We observe
    that gpt-4-turbo, when serving as an evaluator, has the highest agreement rates
    with our meta-evaluation, particularly in the scenarios of brainstorming, dialog,
    and ODG with the helpfulness criterion. It stands out with the highest overall
    average score of 0.780\. However, our selected open-source model evaluator, auto-j,
    outperforms gpt-4-turbo in evaluating coding questions based on the helpfulness
    criterion. In addition, it exhibits the highest agreement rate with our meta-evaluation
    in the judgement scenario, according to the helpfulness criterion, indicating
    it as the most capable evaluator in this setting. It also achieves comparable
    results with other closed-source models like claude-2 and gpt-3.5-turbo in most
    of the other scenarios.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S7.T4 "表 4 ‣ 结果 ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ 大型语言模型是否值得信赖？通过代理辩论对
    LLMs 进行可扩展的元评估") 比较了 ScaleEval 的元评估与各 LLM 评估者在标准和场景中的一致率。我们观察到，当 gpt-4-turbo 担任评估者时，与我们的元评估的一致率最高，尤其是在头脑风暴、对话和以有用性标准为准的
    ODG 场景中。它以 0.780 的最高总体平均分脱颖而出。然而，我们选择的开源模型评估者 auto-j 在以有用性标准评估编码问题方面优于 gpt-4-turbo。此外，它在判断场景中的一致率最高，根据有用性标准，表明它在该设置下是最有能力的评估者。它在大多数其他场景中也与其他封闭源模型如
    claude-2 和 gpt-3.5-turbo 取得了相当的结果。'
- en: While gpt-4-turbo performs the best as an evaluator in a majority of scenarios,
    it is not necessarily the best choice when we take into consideration its relatively
    high API costs. In fact, both the more affordable version (gpt-3.5-turbo) and
    our selected free, open-source model (auto-j) show comparable performance in scenarios
    like judgement and writing. For coding-related evaluations, the slightly less
    expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 gpt-4-turbo 在多数场景下作为评估者表现最佳，但考虑到其相对较高的 API 成本，它不一定是最佳选择。实际上，价格更实惠的版本（gpt-3.5-turbo）和我们选择的免费开源模型（auto-j）在判断和写作等场景中表现相当。对于与编码相关的评估，稍微便宜一点的
    claude-2 可能是 gpt-4-turbo 的更具成本效益的替代方案。
- en: '| Criterion | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo | Auto-J |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 场景 | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo | Auto-J |'
- en: '| Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 | 0.575 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 有用性 | 头脑风暴 | 0.800 | 0.500 | 0.650 | 0.575 |'
- en: '|  | Coding | 0.600 | 0.725 | 0.675 | 0.675 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | 编码 | 0.600 | 0.725 | 0.675 | 0.675 |'
- en: '|  | Dialog | 0.800 | 0.700 | 0.700 | 0.625 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 对话 | 0.800 | 0.700 | 0.700 | 0.625 |'
- en: '|  | Judgement | 0.725 | 0.625 | 0.725 | 0.750 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 判断 | 0.725 | 0.625 | 0.725 | 0.750 |'
- en: '|  | Math | 0.825 | 0.650 | 0.600 | 0.350 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 数学 | 0.825 | 0.650 | 0.600 | 0.350 |'
- en: '|  | ODG | 0.850 | 0.525 | 0.575 | 0.700 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | ODG | 0.850 | 0.525 | 0.575 | 0.700 |'
- en: '|  | ODS | 0.875 | 0.525 | 0.575 | 0.675 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | ODS | 0.875 | 0.525 | 0.575 | 0.675 |'
- en: '|  | Writing | 0.750 | 0.600 | 0.750 | 0.600 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 写作 | 0.750 | 0.600 | 0.750 | 0.600 |'
- en: '| Interpretability | Coding | 0.825 | 0.600 | 0.550 | 0.525 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 可解释性 | 编程 | 0.825 | 0.600 | 0.550 | 0.525 |'
- en: '| Reasoning | Math | 0.650 | 0.525 | 0.475 | 0.450 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 数学 | 0.650 | 0.525 | 0.475 | 0.450 |'
- en: '|  | Judgement | 0.750 | 0.650 | 0.700 | 0.675 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | 判断 | 0.750 | 0.650 | 0.700 | 0.675 |'
- en: '| Creativity | Writing | 0.775 | 0.600 | 0.575 | 0.650 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 创造力 | 写作 | 0.775 | 0.600 | 0.575 | 0.650 |'
- en: '|  | Brainstorming | 0.800 | 0.525 | 0.550 | 0.625 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 头脑风暴 | 0.800 | 0.525 | 0.550 | 0.625 |'
- en: '|  | Dialog | 0.875 | 0.750 | 0.700 | 0.800 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 对话 | 0.875 | 0.750 | 0.700 | 0.800 |'
- en: '| Average | Overall | 0.780 | 0.607 | 0.629 | 0.619 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 总体 | 0.780 | 0.607 | 0.629 | 0.619 |'
- en: 'Table 4: Agreement rate between ScaleEval’s meta-evaluation and each LLM evaluator
    for comparing GPT3.5-Turbo vs. Claude-Instant.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：ScaleEval的元评估与每个LLM评估员在比较GPT3.5-Turbo与Claude-Instant时的一致性率。
- en: '| Criteria Format | Criteria | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 标准格式 | 标准 | 场景 | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| General | Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 有用性 | 头脑风暴 | 0.800 | 0.500 | 0.650 |'
- en: '|  | Interpretability | Coding | 0.825 | 0.600 | 0.550 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.825 | 0.600 | 0.550 |'
- en: '|  | Reasoning | Math | 0.650 | 0.525 | 0.475 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.650 | 0.525 | 0.475 |'
- en: '|  | Creativity | Writing | 0.800 | 0.600 | 0.575 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.800 | 0.600 | 0.575 |'
- en: '| Shortened | Helpfulness | Brainstorming | 0.675 | 0.500 | 0.575 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 缩短 | 有用性 | 头脑风暴 | 0.675 | 0.500 | 0.575 |'
- en: '|  | Interpretability | Coding | 0.675 | 0.325 | 0.425 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.675 | 0.325 | 0.425 |'
- en: '|  | Reasoning | Math | 0.625 | 0.425 | 0.400 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.625 | 0.425 | 0.400 |'
- en: '|  | Creativity | Writing | 0.675 | 0.250 | 0.525 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.675 | 0.250 | 0.525 |'
- en: '| Gibberish | Helpfulness | Brainstorming | 0.575 | 0.450 | 0.575 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 胡言乱语 | 有用性 | 头脑风暴 | 0.575 | 0.450 | 0.575 |'
- en: '|  | Interpretability | Coding | 0.700 | 0.275 | 0.525 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.700 | 0.275 | 0.525 |'
- en: '|  | Reasoning | Math | 0.650 | 0.200 | 0.400 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.650 | 0.200 | 0.400 |'
- en: '|  | Creativity | Writing | 0.550 | 0.150 | 0.450 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.550 | 0.150 | 0.450 |'
- en: '| Shuffled | Helpfulness | Brainstorming | 0.625 | 0.550 | 0.500 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 打乱 | 有用性 | 头脑风暴 | 0.625 | 0.550 | 0.500 |'
- en: '|  | Interpretability | Coding | 0.600 | 0.400 | 0.525 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.600 | 0.400 | 0.525 |'
- en: '|  | Reasoning | Math | 0.625 | 0.225 | 0.600 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.625 | 0.225 | 0.600 |'
- en: '|  | Creativity | Writing | 0.625 | 0.275 | 0.500 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.625 | 0.275 | 0.500 |'
- en: '| Flipped | Helpfulness | Brainstorming | 0.725 | 0.325 | 0.550 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 翻转 | 有用性 | 头脑风暴 | 0.725 | 0.325 | 0.550 |'
- en: '|  | Interpretability | Coding | 0.725 | 0.425 | 0.300 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.725 | 0.425 | 0.300 |'
- en: '|  | Reasoning | Math | 0.575 | 0.250 | 0.500 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.575 | 0.250 | 0.500 |'
- en: '|  | Creativity | Writing | 0.750 | 0.075 | 0.550 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.750 | 0.075 | 0.550 |'
- en: '| Masked | Helpfulness | Brainstorming | 0.725 | 0.300 | 0.500 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 遮罩 | 有用性 | 头脑风暴 | 0.725 | 0.300 | 0.500 |'
- en: '|  | Interpretability | Coding | 0.650 | 0.225 | 0.475 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.650 | 0.225 | 0.475 |'
- en: '|  | Reasoning | Math | 0.575 | 0.150 | 0.375 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.575 | 0.150 | 0.375 |'
- en: '|  | Creativity | Writing | 0.575 | 0.200 | 0.400 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.575 | 0.200 | 0.400 |'
- en: 'Table 5: Agreement rate between ScaleEval’s meta-evaluation results and each
    LLM evaluator under various criteria prompt formats and scenarios comparing GPT3.5-Turbo
    vs. Claude-Instant.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：ScaleEval的元评估结果与每个LLM评估员在不同标准提示格式和场景下的一致性率，比较GPT3.5-Turbo与Claude-Instant。
- en: '8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 Exp-III：具有标准提示格式变化的元评估
- en: 'Q3: How do the qualities of criteria prompts influence the robustness of LLMs
    as evaluators in different scenarios?'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q3：标准提示的质量如何影响LLM在不同场景下作为评估者的稳健性？
- en: Prior studies have revealed that variations in prompts can substantially affect
    the behavior of LLMs, particularly with the text they generate. With this in mind,
    we define various formatted criteria for evaluating LLM responses under each scenario.
    This approach aims to examine the extent to which different formats of criteria
    prompts influence both the performance and robustness of LLMs as evaluators.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，提示的变化可以显著影响LLM的行为，特别是生成的文本。鉴于此，我们定义了各种格式化标准，用于评估LLM在每个场景下的响应。这种方法旨在深入了解不同格式的标准提示在多大程度上影响LLM作为评估者的表现和稳健性。
- en: Setup
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'We define five variations of the same criteria prompts: shortened, gibberish,
    shuffled, flipped, and masked (see Table [7](#A1.T7 "Table 7 ‣ Appendix A Meta-Evaluation
    Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") under Appendix [A](#A1 "Appendix A Meta-Evaluation
    Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") for detailed format). With these criteria
    format variations, we intend to observe how the LLMs as evaluators would respond
    differently when conducting evaluation. We compare the example-level agreement
    rate between ScaleEval’s meta-evaluation results and each LLM evaluator.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了相同标准提示的五种变体：缩短、无意义、打乱、翻转和掩码（详细格式见附录[A](#A1 "附录 A 元评估提示 ‣ 大型语言模型是否值得信赖？通过代理辩论对
    LLM 作为评估者的可扩展元评估")中的表[7](#A1.T7 "表 7 ‣ 附录 A 元评估提示 ‣ 大型语言模型是否值得信赖？通过代理辩论对 LLM 作为评估者的可扩展元评估")）。通过这些标准格式变体，我们旨在观察
    LLM 作为评估者在进行评估时的不同反应。我们比较了 ScaleEval 的元评估结果与每个 LLM 评估者之间的示例级一致性率。
- en: Results
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Based on Table [5](#S7.T5 "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs.
    LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), we observe that the
    performance of LLMs as evaluators generally deteriorates when certain letters
    in the criteria prompts are masked. Furthermore, the removal of guiding phrases
    at the beginning, such as "Not Helpful" or "Highly Helpful", can also diminish
    their effectiveness as evaluators. Both gpt-4-turbo and gpt-3.5-turbo demonstrate
    some resilience to these adversarially formatted criteria prompts, maintaining
    a relatively consistent agreement rates across various criteria formats. In contrast,
    Claude-2 often showcases confusion and refuses to evaluate, particularly in cases
    with gibberish and masked criteria prompts, where it rejects answering about half
    of the questions. It typically responds with statements like, "Unfortunately I
    do not have enough information here to provide a fair evaluation… The criteria
    describe different quality levels, but there is no detail on what specific aspects
    of the responses should be assessed… any judgement risks being arbitrary or biased…".
    None of the LLMs as evaluators we tested maintained very similar evaluation capabilities
    when faced with these adversarially formatted criteria prompts, indicating a limitation
    in these LLMs as evaluators’ current design and application. Despite their advanced
    capabilities in fulfilling a variety of tasks, they may still struggle with understanding
    and responding accurately to substituted criteria information, highlighting an
    area for potential improvement in future iterations of LLM technology. Among all
    the different formatted criteria, we highlight the cases where the LLMs perform
    the best as evaluators in Table [5](#S7.T5 "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation
    vs. LLM Evaluators ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表[5](#S7.T5 "表 5 ‣ 结果 ‣ 7 Exp-II: 元评估与 LLM 评估者 ‣ 大型语言模型是否值得信赖？通过代理辩论对 LLM
    作为评估者的可扩展元评估")，我们观察到当标准提示中的某些字母被掩码时，LLM 作为评估者的性能通常会下降。此外，开头的引导短语如“无帮助”或“非常有帮助”的去除也会降低它们作为评估者的有效性。gpt-4-turbo
    和 gpt-3.5-turbo 对这些敌对格式的标准提示表现出一定的抵抗力，在各种标准格式中保持了相对一致的一致性率。相比之下，Claude-2 经常表现出困惑并拒绝评估，尤其是在无意义和掩码标准提示的情况下，它拒绝回答大约一半的问题。它通常回应诸如“很遗憾，我在这里没有足够的信息来提供公正的评估……标准描述了不同的质量水平，但没有详细说明应评估回应的具体方面……任何判断都有可能是任意或偏见的……”等语句。我们测试的
    LLM 作为评估者在面对这些敌对格式的标准提示时没有保持非常相似的评估能力，表明这些 LLM 作为评估者的当前设计和应用存在局限性。尽管它们在完成各种任务方面能力先进，但仍可能在理解和准确回应替代标准信息方面遇到困难，突显了未来
    LLM 技术迭代的潜在改进空间。在所有不同格式的标准中，我们在表[5](#S7.T5 "表 5 ‣ 结果 ‣ 7 Exp-II: 元评估与 LLM 评估者
    ‣ 大型语言模型是否值得信赖？通过代理辩论对 LLM 作为评估者的可扩展元评估")中突出显示了 LLM 作为评估者表现最好的情况。'
- en: 9 Conclusion
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: In this work, we propose ScaleEval, a scalable, agent-debate assisted meta-evaluation
    framework for assessing the reliability and robustness of LLMs as evaluators.
    This approach addresses the expensive and time-intensive challenges inherent in
    traditional meta-evaluation methods, particularly pertinent as the usage of LLMs
    expands, necessitating a more scalable solution. Through our research, we have
    not only demonstrated the reliability of our proposed meta-evaluation framework,
    but also shed light on the capabilities and limitations of LLMs as evaluators
    in various scenarios. We observe how the results from these LLMs as evaluators
    vary based on modifications to the same criteria prompts. By open-sourcing our
    framework, we aim to foster further research in this field and encourage the development
    of more advanced and reliable LLMs as evaluators in the future.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 ScaleEval，这是一个可扩展的、基于代理辩论的元评价框架，用于评估 LLM 作为评价者的可靠性和稳健性。这种方法解决了传统元评价方法中固有的高成本和耗时挑战，特别是随着
    LLM 使用的扩展，需求一个更具扩展性的解决方案。通过我们的研究，我们不仅证明了我们提出的元评价框架的可靠性，还揭示了 LLM 作为评价者在各种场景中的能力和局限性。我们观察到这些
    LLM 作为评价者的结果如何根据对相同标准提示的修改而变化。通过开源我们的框架，我们旨在促进该领域的进一步研究，并鼓励未来开发更先进、更可靠的 LLM 评价者。
- en: Acknowledgements
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Chunting Zhou, Weizhe Yuan, Chunpu Xu, Yan Ma, and Binjie Wang for
    the helpful discussions and feedback.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Chunting Zhou, Weizhe Yuan, Chunpu Xu, Yan Ma, 和 Binjie Wang 提供的有益讨论和反馈。
- en: References
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale. Fung. 2023. A multitask, multilingual, multimodal evaluation
    of chatgpt on reasoning, hallucination, and interactivity. *arXiv:2302.04023v3*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang 等人 (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan
    Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do,
    Yan Xu, 和 Pascale Fung. 2023. 对 ChatGPT 在推理、幻觉和互动方面的多任务、多语言、多模态评估。*arXiv:2302.04023v3*。
- en: Bhandari et al. (2020) Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu,
    and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. *arXiv
    preprint arXiv:2010.07100*.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhandari 等人 (2020) Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu,
    和 Graham Neubig. 2020. 重新评估文本摘要中的评价。*arXiv 预印本 arXiv:2010.07100*。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人 (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg
    等人. 2023. 人工通用智能的火花：对 GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*。
- en: 'Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based
    evaluators through multi-agent debate. *arXiv preprint arXiv:2308.07201*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等人 (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang
    Zhang, Jie Fu, 和 Zhiyuan Liu. 2023. Chateval：通过多代理辩论迈向更好的 LLM 基于评价者。*arXiv 预印本
    arXiv:2308.07201*。
- en: Chen et al. (2021a) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021a. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2021a) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever 和 Wojciech
    Zaremba. 2021a. [评估训练于代码的大型语言模型](http://arxiv.org/abs/2107.03374)。
- en: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021b. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等. 2021b. 评估训练于代码的大型语言模型。*arXiv 预印本 arXiv:2107.03374*。
- en: Chiang and Lee (2023) Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language
    models be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 和 Lee (2023) Cheng-Han Chiang 和 Hung-yi Lee. 2023. 大型语言模型能否替代人工评估？*arXiv
    预印本 arXiv:2305.01937*。
- en: Dang et al. (2008) Hoa Trang Dang, Karolina Owczarzak, et al. 2008. Overview
    of the tac 2008 update summarization task. In *TAC*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dang 等 (2008) Hoa Trang Dang, Karolina Owczarzak 等. 2008. TAC 2008 更新摘要任务概述。收录于
    *TAC*。
- en: 'Freitag et al. (2022) Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
    Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and
    André FT Martins. 2022. Results of wmt22 metrics shared task: Stop using bleu–neural
    metrics are better and more robust. In *Proceedings of the Seventh Conference
    on Machine Translation (WMT)*, pages 46–68.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag 等 (2022) Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig
    Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie 和 André FT
    Martins. 2022. WMT22 评测共享任务结果：停止使用 BLEU——神经网络指标更好且更稳健。收录于 *第七届机器翻译会议 (WMT) 会议论文集*，第46–68页。
- en: 'Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang 和 Pengfei Liu. 2023. GPTScore：按需评估。*arXiv
    预印本 arXiv:2302.04166*。
- en: 'Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language
    models. *arXiv preprint arXiv:2211.10435*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming
    Yang, Jamie Callan 和 Graham Neubig. 2022. PAL：程序辅助语言模型。*arXiv 预印本 arXiv:2211.10435*。
- en: 'Gemini Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui
    Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M
    Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini Team 等 (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth 等. 2023. Gemini：一系列高能力的多模态模型。*arXiv 预印本 arXiv:2312.11805*。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song 和 Jacob Steinhardt. 2020. 测量大规模多任务语言理解能力。*arXiv 预印本 arXiv:2009.03300*。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, 和 Jacob Steinhardt. 2021. 使用数学数据集测量数学问题解决能力。*arXiv
    预印本 arXiv:2103.03874*。
- en: 'Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig,
    Orhan Firat, and Melvin. Johnson. 2020. Xtreme: A massively multilingual multi-task
    benchmark for evaluating cross-lingual generalization. *arXiv:2003.11080v5*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig,
    Orhan Firat, 和 Melvin Johnson. 2020. Xtreme：一个大规模多语言多任务基准，用于评估跨语言泛化能力。*arXiv:2003.11080v5*。
- en: Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot
    self-correct reasoning yet. *arXiv preprint arXiv:2310.01798*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song, 和 Denny Zhou. 2023. 大语言模型尚无法自我纠正推理。*arXiv 预印本 arXiv:2310.01798*。
- en: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint
    arXiv:2310.05470*.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    和 Pengfei Liu. 2023a. 生成式判断器用于评估对齐。*arXiv 预印本 arXiv:2310.05470*。
- en: 'Li et al. (2023b) Ruosen Li, Teerth Patel, and Xinya Du. 2023b. Prd: Peer rank
    and discussion improve large language model based evaluations. *arXiv preprint
    arXiv:2307.02762*.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Ruosen Li, Teerth Patel, 和 Xinya Du. 2023b. PRD：同行排名和讨论提高了基于大型语言模型的评估。*arXiv
    预印本 arXiv:2307.02762*。
- en: 'Li et al. (2023c) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023c. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023c) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023c. Alpacaeval：一种自动化的指令跟随模型评估工具。
    [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar 等人. 2022. 对语言模型的全面评估。*arXiv 预印本 arXiv:2211.09110*。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray 等人. 2022. 通过人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744。
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu
    Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta,
    Adrià Garriga-Alonso 等人. 2022. 超越模仿游戏：量化和推测语言模型的能力。*arXiv 预印本 arXiv:2206.04615*。
- en: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator?
    a preliminary study. *arXiv preprint arXiv:2303.04048*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, 和 Jie Zhou. 2023a. ChatGPT 是一个好的 NLG 评估器吗？初步研究。*arXiv
    预印本 arXiv:2303.04048*。
- en: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are
    not fair evaluators. *ArXiv*, abs/2305.17926.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, 和 Zhifang Sui. 2023b. 大语言模型不是公平的评估者。*ArXiv*，abs/2305.17926。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing 等人. 2023.
    使用 mt-bench 和 chatbot arena 评估 llm-as-a-judge。*arXiv 预印本 arXiv:2306.05685*。
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan. Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv:2304.06364v2*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '钟等（2023） 万军钟，瑞祥崔，亦多郭，耀博梁，帅陆，彦林王，阿敏赛义德，伟柱陈，和楠·段。2023年。Agieval: 人本基准评估基础模型。*arXiv:2304.06364v2*。'
- en: Appendix A Meta-Evaluation Prompt
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 元评估提示
- en: '|  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Compare the two submissions based on the criteria above. Which one is better?
    First, provide a step-by-step explanation of your evaluation reasoning according
    to the criteria. Avoid any potential bias. Ensure that the order in which the
    submissions were presented does not affect your judgement. Keep your explanation
    strictly under 150 words. Afterwards, choose one of the following options: |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 根据上述标准比较两个提交。哪个更好？首先，按标准逐步解释你的评估推理。避免任何潜在的偏见。确保提交的顺序不会影响你的判断。保持你的解释严格在 150
    字以内。之后，选择以下选项之一： |'
- en: '| Submission 1 is better: "1" |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 提交 1 更好：“1” |'
- en: '| Submission 2 is better: "2" |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 提交 2 更好：“2” |'
- en: '| Neither is better: "0" |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 都不更好：“0” |'
- en: '|  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds
    to your reasoning. At the end, repeat just the number again by itself on a new
    line. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 直接输入“1”或“2”或“0”（不带引号或标点符号）以对应你的推理。最后，在新的一行中仅重复一个数字。 |'
- en: '| [Question]: {question} |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [问题]: {question} |'
- en: '| [Submission 1]: {submission_1} |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| [提交 1]: {submission_1} |'
- en: '| [Submission 2]: {submission_2} |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [提交 2]: {submission_2} |'
- en: '| [Criteria]: {criteria} |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [标准]: {criteria} |'
- en: '| [User]: {user_prompt} |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| [用户]: {user_prompt} |'
- en: '|  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| You are evaluating two submissions for a particular question, using a specific
    set of criteria. Above is the data. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 你正在评估两个提交，针对特定问题使用一组特定的标准。上述为数据。 |'
- en: '|  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Always remember you are Speaker 1/2/3. Review again your own previous evaluations/discussions
    first, then answer user’s request from Speaker 1/2/3’s perspective. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 永远记住你是发言人 1/2/3。首先回顾你自己之前的评估/讨论，然后从发言人 1/2/3 的角度回答用户的请求。 |'
- en: '| [Question]: {question} |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [问题]: {question} |'
- en: '| [Submission 1]: {submission_1} |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| [提交 1]: {submission_1} |'
- en: '| [Submission 2]: {submission_2} |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| [提交 2]: {submission_2} |'
- en: '| [Criteria]: {criteria} |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| [标准]: {criteria} |'
- en: '| [Speaker 1’s Initial Evaluation]: {evaluation_1} |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| [发言人 1 的初步评估]: {evaluation_1} |'
- en: '| [Speaker 2’s Initial Evaluation]: {evaluation_2} |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| [发言人 2 的初步评估]: {evaluation_2} |'
- en: '| [Speaker 3’s Initial Evaluation]: {evaluation_3} |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| [发言人 3 的初步评估]: {evaluation_3} |'
- en: '| [Speaker {speaker_number}’s Discussion -- Round {round_number}]: {discussion_reasoning}
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| [发言人 {speaker_number} 的讨论 -- 第 {round_number} 轮]: {discussion_reasoning}
    |'
- en: '| ... |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| ... |'
- en: '|  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Read the question, submissions, criteria, and evaluations above. First, explain
    your thoughts step-by-step about other speakers’ evaluations. Second, explain
    your reasoning step-by-step regarding whether or not to change your original answer
    about which submission you think is better after considering other speakers’ perspectives.
    Keep your reasoning strictly under 150 words. Afterwards, choose one of the following
    options: |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 阅读上述问题、提交、标准和评估。首先，逐步解释你对其他发言人评估的想法。其次，逐步解释你是否在考虑其他发言人观点后，改变你最初对哪个提交更好的回答。保持你的推理严格在
    150 字以内。之后，选择以下选项之一： |'
- en: '| Submission 1 is better: "1" |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 提交 1 更好：“1” |'
- en: '| Submission 2 is better: "2" |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 提交 2 更好：“2” |'
- en: '| Neither is better: "0" |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 都不更好：“0” |'
- en: '|  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds
    to your reasoning. At the end, repeat just the number again by itself on a new
    line. |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 直接输入“1”或“2”或“0”（不带引号或标点符号）以对应你的推理。最后，在新的一行中仅重复一个数字。 |'
- en: 'Table 6: Prompt template for meta-evaluation via multi-agent debate'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：通过多代理辩论进行的元评估提示模板
- en: '|  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| "1": "Not Helpful - The response is completely unrelated, lacks coherence,
    and fails to provide any meaningful information." |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| “1”： “不有用 - 响应完全无关，缺乏连贯性，未提供任何有意义的信息。” |'
- en: '| "2": "Somewhat Helpful - The response bears some relevance but remains largely
    superficial and unclear, addressing only the peripheral aspects of the user’s
    needs." |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| “2”： “有些有用 - 响应有一定相关性，但仍然相当表面和不清晰，仅解决了用户需求的外围方面。” |'
- en: '| "3": "Moderately Helpful - The response is mostly relevant and clear, covering
    the basic aspects of the query, but lacks depth and comprehensive elucidation."
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| "3": "适度有用 - 响应大部分相关且清晰，涵盖了查询的基本方面，但缺乏深度和全面的阐述。" |'
- en: '| "4": "Helpful - The response is on-point, detailed, and well-articulated,
    offering valuable information and clarifications that meet the user’s primary
    needs and enhance understanding." |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| "4": "有用 - 响应是精准的、详细的，并且表达良好，提供了有价值的信息和澄清，满足了用户的主要需求并增强了理解。" |'
- en: '| "5": "Highly Helpful - The response is exceptionally thorough and precise,
    providing additional insights and valuable supplementary information." |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| "5": "高度有用 - 响应非常彻底且精确，提供了额外的见解和有价值的补充信息。" |'
- en: '|  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| "1": "The response is completely unrelated, lacks coherence, and fails to
    provide any meaningful information." |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| "1": "该响应完全无关，缺乏连贯性，未能提供任何有意义的信息。" |'
- en: '| "2": "The response bears some relevance but remains largely superficial and
    unclear, addressing only the peripheral aspects of the user’s needs." |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| "2": "响应有一定的相关性，但仍然是表面的和不明确的，仅涉及用户需求的外围方面。" |'
- en: '| "3": "The response is mostly relevant and clear, covering the basic aspects
    of the query, but lacks depth and comprehensive elucidation." |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| "3": "响应大部分相关且清晰，涵盖了查询的基本方面，但缺乏深度和全面的阐述。" |'
- en: '| "4": "The response is on-point, detailed, and well-articulated, offering
    valuable information and clarifications that meet the user’s primary needs and
    enhance understanding." |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| "4": "响应是精准的、详细的，并且表达良好，提供了有价值的信息和澄清，满足了用户的主要需求并增强了理解。" |'
- en: '| "5": "The response is exceptionally thorough and precise, providing additional
    insights and valuable supplementary information." |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| "5": "该响应非常彻底且精确，提供了额外的见解和有价值的补充信息。" |'
- en: '|  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| "1": "N*t H$l%ful - Th$ r$sp0n$e is c mplt$l? unr€la7$d, la$ks c()h$r$n(€,
    and f#i/s t# p$o&id$ any m€an*&gful !format$on." |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| "1": "不有用 - 该响应是完全无关的，缺乏连贯性，并且无法提供任何有意义的信息。" |'
- en: '| "2": "S#m$*ha+ H$%*fu/ - Th$ r#s0!n$ b%ars $o/e re$ev*nc$ b$t r$ma$n$ l#rg$l4
    $u/7$r7cial an* !ncl=4r, a6r$ss@n4 o7ly th$ p$r4ph@r$l a5p$cts #f th$ $s*r’s n**ds."
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| "2": "S#m$*ha+ H$%*fu/ - Th$ r#s0!n$ b%ars $o/e re$ev*nc$ b$t r$ma$n$ l#rg$l4
    $u/7$r7cial an* !ncl=4r, a6r$ss@n4 o7ly th$ p$r4ph@r$l a5p$cts #f th$ $s*r’s n**ds."
    |'
- en: '| "3": "M$!7r$t#ly H$lpfu& - Th$ r@s0*n$@ !s m$%stl€ r$’$van7 an cl$ar, c$%$r$n4
    th$ ba$!c a$%cts of th$ qu€ry, b$t l#cks d$pth an cmpr$h$ns$v$ lu$7$dat!on." |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| "3": "适度有用 - 响应大部分相关且清晰，涵盖了查询的基本方面，但缺乏深度和全面的阐述。" |'
- en: '| "4": "H$lpfu& - Th$ r!s0*n$e !s o/7-p$!nt, d$ta$!l$d, an w$l/-a&!u/at$d,
    #ff$r!n4 v#l$%bl$ #nformat$on and cl*r$!cat!ons th#t m=t th$ u/7$rś pr!/ary n$$ds
    an* @n7anc$ un#rstand!n4." |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| "4": "H$lpfu& - Th$ r!s0*n$e !s o/7-p$!nt, d$ta$!l$d, an w$l/-a&!u/at$d,
    #ff$r!n4 v#l$%bl$ #nformat$on and cl*r$!cat!ons th#t m=t th$ u/7$rś pr!/ary n$$ds
    an* @n7anc$ un#rstand!n4." |'
- en: '| "5": "H4#h7y H$!p%u& - Th$ r$s&*n!e !s $xc$pt$#nally th#r#7gh an* pr$c$%$,
    pr#v$d$n# a4*!t$#nal !ns$4hts an* v#lu%bl$ @*pp%$%ntary #n%ormat$on." |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| "5": "H4#h7y H$!p%u& - Th$ r$s&*n!e !s $xc$pt$#nally th#r#7gh an* pr$c$%$,
    pr#v$d$n# a4*!t$#nal !ns$4hts an* v#lu%bl$ @*pp%$%ntary #n%ormat$on." |'
- en: '|  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| "1": "coherence fails provide unrelated, completely response - and the meaningful
    any to lacks Not Helpful is The information." |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| "1": "响应缺乏连贯性，无法提供任何有意义的信息，完全无关 - 不有用的。" |'
- en: '| "2": "superficial response largely addressing unclear, remains only needs.
    - relevance user’s and the Helpful the peripheral some bears but aspects Somewhat
    The of" |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| "2": "响应有一定的相关性，但仍然是表面的和不明确的，仅涉及用户需求的外围方面。" |'
- en: '| "3": "basic aspects query, lacks Moderately covering clear, - Helpful is
    depth response and comprehensive elucidation. relevant mostly the The and the
    of but" |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| "3": "基本方面 查询，缺乏 适度地覆盖 清晰 - 有用的 是深度 响应和全面阐述。相关的 大部分的 和的 但" |'
- en: '| "4": "clarifications the is response information needs enhance and Helpful
    - on-point, valuable well-articulated, offering understanding. The and detailed,
    primary that user’s meet" |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| "4": "响应是精准的、详细的，并且表达良好，提供了有价值的信息和澄清，满足了用户的主要需求并增强了理解。" |'
- en: '| "5": "valuable Highly response is providing - the exceptionally Helpful information.
    insights thorough and additional precise, supplementary and The" |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| "5": "高度 有用 - 响应 提供了 非常彻底的信息。见解 和 补充 精确的 和有价值的。" |'
- en: '|  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| "1": "toN lufpleH - ehT esnopser si yletelpmoc detalernu, skcal ecnerehoc,
    dna sliaf ot edivorp yna lufgninaem noitamrofni." |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| "1": "*不有用* - 响应是完全不相关的，缺乏一致性，并且无法提供任何有意义的信息。" |'
- en: '| "2": "tamewoS lufpleH - ehT esnopser sraeb emos ecnaveler tub sniamer ylegral
    laicifrepus dna raelcnu, gnisserdda ylno eht larehpirep stcepsa fo eht s’resu
    sdeen." |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| "2": "有些有用 - 响应包含一些相关内容，但基本上仍然是表面的和不清晰，只处理了用户需求的基本方面。" |'
- en: '| "3": "yletaredoM lufpleH - ehT esnopser si yltsom tnaveler dna raelc, gnirevoc
    eht cisab stcepsa fo eht yreuq, tub skcal htped dna evisneherpmoc noitadicule."
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| "3": "**中等有用** - *响应大多相关且清晰，覆盖了问题的基础方面，但缺乏深度和全面的解释*。" |'
- en: '| "4": "lufpleH - ehT esnopser si tniop-no, deliated, dna detalucitra-llew,
    gnireffo elbaulav noitamrofni dna snoitacifralc taht teem eht s’resu yramirp sdeen
    dna ecnahne gnidnatsrednu." |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| "4": "**有用** - *响应是非点对点的，详细的，且很好地阐述了可以提供有价值的信息和澄清，以满足用户的主要需求并增强理解*。" |'
- en: '| "5": "ylhgiH lufpleH - ehT esnopser si yllanoitpecxe hguoroht dna esicerp,
    gnidivorp lanoitidda sthgisni dna elbaulav yratnemelppus noitamrofni." |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| "5": "**高度有用** - 响应在整个过程中都是卓越的且准确的，提供了额外的见解和有价值的补充信息。" |'
- en: '|  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| "1": "N__ H_l_ful - The r__pnse is c_m__et__y unr_l_te_, lacks _ohe_en_e,
    _nd _ai_s to p_ov_de _ny m_a__ngfu_ _nfo_ma_ion." |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| "1": "*不太有用* - 响应完全不相关，缺乏连贯性，并且试图提供任何有意义的信息。" |'
- en: '| "2": "_om_w_at He_p_ul - T_e re_ponse be_rs _ome rel__a_ce but r__ains la__ely
    s__erfi__al and u_cle__, ad_res__ng onl_ _he __ri__er_l a_pe_ts of t__ u_e_’s
    ne_ds." |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| "2": "*不太有用* - 响应包含一些相关内容，但基本上仍然是表面的和不清晰，只处理了用户需求的基本方面。" |'
- en: '| "3": "Mod___tely _elp__l - Th_ _esp__se is mos__y re__va_t an_ _le_r, c_v__ing
    the ba_ic _spe_ts of the q_e_y, but __cks _e_th and co_preh_ns_ve el_c_d_t_on."
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| "3": "中___有用 - 响应大___相关且__晰，覆盖了问题的基础__面，但缺乏深度和全面的解释__。" |'
- en: '| "4": "__lpful - _he respo_se is on-p_in_, d___iled, and we_l-ar_icu_ated,
    of_er_ng val_ab_e __for_ation and cl_r_fi__t_ons t_at mee_ the _se_’s p_im_r_
    _eeds and en__nce u_de__tan_ing." |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| "4": "__有用 - _响应是点对点的_，详细的，并且很好地阐述了可以提供有价值的__信息和澄清，以满足用户的主要需求并增强理解__。" |'
- en: '| "5": "Hi_h_y H__p_ul - The r_spon_e is e_c_p_io_al__ th_r_ugh and p_ec_se,
    pr_vi_ing a_di__on_l ins_g_ts and va_u_b_e _upp_e_en_a_y inf_rma_io_." |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| "5": "**高度有用** - *响应在整个过程中都是实用的且准确的，提供了额外的见解和有价值的补充信息*。" |'
- en: '|  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Table 7: Criteria prompt format variations for helpfulness'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表7：有用性的标准提示格式变化 |'
