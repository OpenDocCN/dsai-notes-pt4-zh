- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:31'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'wissNYF: Tool Grounded LLM Agents for Black Box Setting'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SwissNYF: 面向黑箱环境的工具基础 LLM 代理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10051](https://ar5iv.labs.arxiv.org/html/2402.10051)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10051](https://ar5iv.labs.arxiv.org/html/2402.10051)
- en: Somnath Sendhil Kumar¹  Dhruv Jain¹  Eshaan Agarwal¹  Raunak Pandey¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Somnath Sendhil Kumar¹  Dhruv Jain¹  Eshaan Agarwal¹  Raunak Pandey¹
- en: ¹ [Intelligence Group, IIT (BHU), Varanasi](https://cops-iitbhu.github.io/IG-website/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ [智能组，印度理工学院（BHU），瓦拉纳西](https://cops-iitbhu.github.io/IG-website/)
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While Large Language Models (LLMs) have demonstrated enhanced capabilities in
    function-calling, these advancements primarily rely on accessing the functions’
    responses. This methodology is practical for simpler APIs but faces scalability
    issues with irreversible APIs that significantly impact the system, such as a
    database deletion API. Similarly, processes requiring extensive time for each
    API call and those necessitating forward planning, like automated action pipelines,
    present complex challenges. Furthermore, scenarios often arise where a generalized
    approach is needed because algorithms lack direct access to the specific implementations
    of these functions or secrets to use them. Traditional tool planning methods are
    inadequate in these cases, compelling the need to operate within black-box environments.
    Unlike their performance in tool manipulation, LLMs excel in black-box tasks,
    such as program synthesis. Therefore, we harness the program synthesis capabilities
    of LLMs to strategize tool usage in black-box settings, ensuring solutions are
    verified prior to implementation. We introduce TOPGUN, an ingeniously crafted
    approach leveraging program synthesis for black box tool planning. Accompanied
    by SwissNYF, a comprehensive suite that integrates black-box algorithms for planning
    and verification tasks, addressing the aforementioned challenges and enhancing
    the versatility and effectiveness of LLMs in complex API interactions. The public
    code for SwissNYF is available at  [https://github.com/iclr-dummy-user/SwissNYF](https://github.com/iclr-dummy-user/SwissNYF)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在功能调用方面展示了增强的能力，但这些进展主要依赖于访问函数的响应。这种方法对于较简单的 API 是实用的，但在面对不可逆
    API 时会遇到可扩展性问题，例如数据库删除 API。类似地，需要每次 API 调用耗时较长的过程和那些需要前期规划的过程，例如自动化操作管道，也面临复杂的挑战。此外，算法通常缺乏对这些函数或其秘密的直接访问，因此常常需要采用通用的方法。传统的工具规划方法在这些情况下不足，迫使我们在黑箱环境中操作。与在工具操作中的表现不同，LLMs
    在黑箱任务中表现出色，如程序合成。因此，我们利用 LLMs 的程序合成功能来策划在黑箱设置中的工具使用，确保解决方案在实施前经过验证。我们介绍了 TOPGUN，一种巧妙设计的方法，利用程序合成进行黑箱工具规划。它配备了
    SwissNYF，一个综合套件，集成了用于规划和验证任务的黑箱算法，解决了上述挑战，提高了 LLMs 在复杂 API 交互中的多样性和有效性。SwissNYF
    的公开代码可在 [https://github.com/iclr-dummy-user/SwissNYF](https://github.com/iclr-dummy-user/SwissNYF)
    找到
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/88919d8f983c79f36ab95072684db8d8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/88919d8f983c79f36ab95072684db8d8.png)'
- en: 'Figure 1: Illustration of different settings that an LLMs may require to manipulate
    tools.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: LLMs 可能需要操作工具的不同设置示意图。'
- en: Significant advancements in Large Language Models (LLMs) like GPT (Radford et al.
    ([2018](#bib.bib26)); Radford et al. ([2019](#bib.bib27)); Brown et al. ([2020](#bib.bib4));
    Achiam et al. ([2023](#bib.bib1))) and PaLM (Chowdhery et al. ([2023](#bib.bib8));Anil
    et al. ([2023](#bib.bib2));) have demonstrated profound abilities in reasoning
    and following instructions over an extensive array of tasks Huang & Chang ([2023](#bib.bib14)).
    The recent shift towards leveraging LLMs to interact with external tools for addressing
    complex real-world challenges marks a significant area of interest (Hao et al.
    ([2023](#bib.bib13)); Zhang et al. ([2023a](#bib.bib36)); Zhuang et al. ([2023b](#bib.bib42));
    Yang et al. ([2023](#bib.bib33)); Schick et al. ([2023](#bib.bib28));Lu et al.
    ([2023a](#bib.bib17));). In addressing intricate problems, autonomous agents powered
    by LLMs employ an amalgamation of LLMs and various external tools (APIs), crafting
    solutions that necessitate a sequence of intermediate reasoning steps (Schick
    et al. ([2023](#bib.bib28));Lu et al. ([2023a](#bib.bib17));Lu et al. ([2023a](#bib.bib17));Patil
    et al. ([2023](#bib.bib24));Qin et al. ([2023](#bib.bib25))). When presented with
    a problem, These agents’ primary objective is to identify and execute a series
    of API function calls sequentially, leading to a coherent solution. These approaches
    are ineffective when queries lack transparency or when the APIs are irreversible.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT（Radford et al. ([2018](#bib.bib26)); Radford et al. ([2019](#bib.bib27));
    Brown et al. ([2020](#bib.bib4)); Achiam et al. ([2023](#bib.bib1)))和PaLM（Chowdhery
    et al. ([2023](#bib.bib8)); Anil et al. ([2023](#bib.bib2));）等大型语言模型（LLMs）的显著进展展示了在广泛任务中进行推理和遵循指令的深刻能力
    Huang & Chang ([2023](#bib.bib14))。最近转向利用LLMs与外部工具互动以解决复杂的现实世界挑战，标志着一个重要的研究领域（Hao
    et al. ([2023](#bib.bib13)); Zhang et al. ([2023a](#bib.bib36)); Zhuang et al.
    ([2023b](#bib.bib42)); Yang et al. ([2023](#bib.bib33)); Schick et al. ([2023](#bib.bib28));
    Lu et al. ([2023a](#bib.bib17));）。在处理复杂问题时，由LLMs驱动的自主代理结合了LLMs和各种外部工具（API），制定需要一系列中间推理步骤的解决方案（Schick
    et al. ([2023](#bib.bib28)); Lu et al. ([2023a](#bib.bib17)); Lu et al. ([2023a](#bib.bib17));
    Patil et al. ([2023](#bib.bib24)); Qin et al. ([2023](#bib.bib25)))。当面临问题时，这些代理的主要目标是识别并依次执行一系列API函数调用，从而形成连贯的解决方案。当查询缺乏透明性或API不可逆时，这些方法效果不佳。
- en: 'We coin the term "black-box" settings in the context of tool planning as scenarios
    where the outcomes of an API or tool are not observable. This framework is especially
    pertinent in systems where using certain APIs poses risks, such as those causing
    inconsistencies by deleting or updating database entries, canceling jobs, or performing
    similar operations. It’s also relevant where API experimentation incurs high costs
    or when APIs require considerable time to execute, ensuring clarity and comprehensive
    coverage without redundancy, making it challenging to interpret their outcomes.
    We present a taxonomy of such systems Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") into three branches:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在工具规划的背景下创造了“黑箱”设置这个术语，指的是API或工具的结果无法观察的场景。这个框架在使用某些API存在风险的系统中尤为相关，例如那些通过删除或更新数据库条目、取消作业或执行类似操作而导致不一致的API。它也适用于API实验成本高昂或API执行需要相当长时间的情况，确保清晰和全面覆盖而不冗余，这使得解释其结果变得具有挑战性。我们将这种系统的分类呈现为图
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting")，分为三类：'
- en: '1.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'White Box Systems: In these settings, planners can invoke the API, receive
    responses, access the source code and understand its complex logic. This access
    enables the system to navigate complex inputs, intricacies and use cases efficiently.'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 白盒系统：在这些设置中，规划者可以调用API，接收响应，访问源代码并理解其复杂逻辑。这种访问使系统能够高效地处理复杂输入、细节和用例。
- en: '2.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Gray Box Systems: Planners in these environments have descriptions of the tools
    at their disposal and the capability to call the API and receive responses. The
    system’s planning relies solely on the limited descriptions provided and the responses
    for each tool.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灰盒系统：在这些环境中，规划者拥有工具的描述，能够调用API并接收响应。系统的规划完全依赖于提供的有限描述和每个工具的响应。
- en: '3.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Black Box Systems: In the most challenging scenarios, planners are confined
    to tool descriptions without access to actual tool outputs. Here, the planner
    must decipher the dynamics of each tool based solely on its description, making
    it a particularly demanding task to formulate responses to queries.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 黑箱系统：在最具挑战性的场景中，规划者仅限于工具描述而无法访问实际工具输出。在这里，规划者必须仅基于工具的描述来解码每个工具的动态，因此制定回应查询的策略是特别具有挑战性的任务。
- en: The Zhuang et al. ([2023a](#bib.bib41)) and Qin et al. ([2023](#bib.bib25))
    methods excel in straightforward scenarios where an agent can iterate over tools
    to identify the optimal path, yet they lack efficiency and necessitate extensive
    exploration. Approaches like Yao et al. ([2022](#bib.bib34)) and Parisi et al.
    ([2022](#bib.bib22)), subsets of this exploratory paradigm, offer enhanced efficiency
    yet frequently falter due to their constrained directionality in tool search,
    making them suitable predominantly for straightforward API challenges. In contrast,
    the Zhang et al. ([2023b](#bib.bib37)) approach is efficient regarding API execution
    costs by constraining the number of calls. However, it omits any form of verification
    for its proposed trajectory, diminishing its precision in practical applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 庄等人（[2023a](#bib.bib41)）和秦等人（[2023](#bib.bib25)）的方法在代理能够迭代工具以识别最佳路径的直接场景中表现出色，但它们效率较低，需要大量探索。像姚等人（[2022](#bib.bib34)）和帕里西等人（[2022](#bib.bib22)）这样的方法，作为这一探索范式的子集，提供了更高的效率，但由于其在工具搜索中的方向性有限，经常会出现问题，主要适用于简单的API挑战。相比之下，张等人（[2023b](#bib.bib37)）的方法在API执行成本方面更高效，通过限制调用次数来优化。然而，它省略了对其提出轨迹的任何验证，降低了其在实际应用中的精度。
- en: 'These methodologies in tool application present a dichotomy between accuracy
    and computational overhead. While generally unsuitable for black-box settings,
    the Reverse chain approach exhibits potential for adaptation within such frameworks.
    On the other hand, program synthesis-based algorithms have been instrumental in
    exalting reasoning and decision-making capabilities within LLMs, offering a more
    naturally associative decision-making process than that afforded by mere text.
    Works like The Chain of Code Li et al. ([2023](#bib.bib15)) and Program-of-thoughts
    Chen et al. ([2022](#bib.bib7)) are great examples of using code generation to
    improve decision-making for answering general open-domain questions. To this end,
    few works also upheld the reasoning capability of LLMs using code like "TORA:
    A Tool-Integrated Reasoning Agent for Mathematical Problem Solving" Gou et al.
    ([2023](#bib.bib12)), "Solving challenging math word problems using gpt-4 code
    interpreter with code-based self-verification" Zhou et al. ([2023b](#bib.bib40))
    and "PAL: Program-aided Language Models" Gao et al. ([2023](#bib.bib11)) have
    exploited code interpreters for zero-shot verified solving, substantially surpassing
    few-shot learning benchmarks by enabling semi-verification of proposed solutions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '这些工具应用方法在准确性和计算开销之间呈现出二分法。虽然一般不适用于黑箱设置，但逆向链方法在此类框架中展现出适应潜力。另一方面，基于程序合成的算法在提升LLM的推理和决策能力方面发挥了重要作用，比仅依赖文本的决策过程更自然地关联决策。像《代码链》李等人（[2023](#bib.bib15)）和《思维程序》陈等人（[2022](#bib.bib7)）这样的作品是利用代码生成改善决策的极佳例子。为此，也有一些工作通过代码维持了LLM的推理能力，如《TORA:
    用于数学问题求解的工具集成推理代理》勾等人（[2023](#bib.bib12)）、《使用GPT-4代码解释器和基于代码的自我验证解决挑战性数学文字问题》周等人（[2023b](#bib.bib40)）和《PAL:
    程序辅助语言模型》高等人（[2023](#bib.bib11)），这些工作利用代码解释器进行零-shot验证解题，显著超越了few-shot学习基准，通过实现对提出解决方案的半验证。'
- en: However, works like Paranjape et al. ([2023](#bib.bib21)), which employs code
    synthesis for tool usage, are restricted by their limited toolset and the scalability
    challenge posed by the need for extensive human feedback and interventions and
    the need for the human expert to be familiar with the whole toolset. Similarly,
    works such as Xu et al. ([2023](#bib.bib32)), which deploys language models for
    real-time code generation and command execution within controlled environments,
    are limited by their narrow tool range and a deficit in generalizability. The
    state-of-the-art approaches on HumanEval Chen et al. ([2021](#bib.bib6)) and HumanEval-X
    Zheng et al. ([2023](#bib.bib38)) datasets for code generation, like Reflexion
    Shinn et al. ([2023](#bib.bib29)) and LATS Zhou et al. ([2023a](#bib.bib39)),
    which iterate upon code based on interpreter outputs and reflect over them, these
    approaches have yet to be experimented with in other domains associated with LLMs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，像Paranjape等人（[2023](#bib.bib21)）这样使用代码合成进行工具使用的工作，受限于其有限的工具集和需要大量人工反馈与干预的可扩展性挑战，以及人类专家必须熟悉整个工具集的需求。类似地，像Xu等人（[2023](#bib.bib32)）这样的工作，使用语言模型进行实时代码生成和命令执行，但在受控环境中受限于其狭窄的工具范围和泛化能力的不足。最先进的方法如HumanEval
    Chen等人（[2021](#bib.bib6)）和HumanEval-X Zheng等人（[2023](#bib.bib38)）的数据集，用于代码生成，如Reflexion
    Shinn等人（[2023](#bib.bib29)）和LATS Zhou等人（[2023a](#bib.bib39)），这些方法基于解释器输出迭代代码并进行反思，但这些方法尚未在其他与LLMs相关的领域进行实验。
- en: To bridge these gaps, we introduce the TOPGUN (Tool Orchestration and Program
    synthesis for Generalizing over UNknown systems) framework, which unifies code
    generation, reasoning, and strategic tool planning designed for complex tasks.
    TOPGUN also verifies the execution plans and does so with exceptional efficiency
    in API cost, effectively addressing the limitations of preceding models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些不足，我们介绍了TOPGUN（工具编排与程序合成以泛化未知系统）框架，该框架将代码生成、推理和战略工具规划统一起来，旨在应对复杂任务。TOPGUN还验证执行计划，并以卓越的API成本效率完成这一任务，有效解决了前述模型的局限性。
- en: 'Key contributions of our work are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的关键贡献总结如下：
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To the best of our knowledge, we are the First to coin the term Black Box setting
    for API usage and developed a suite to encourage the development of algorithms
    for such scenarios.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们首次提出了“黑箱设置”这一术语用于API使用，并开发了一套工具以鼓励此类场景下算法的发展。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We leverage the program synthesis capabilities of Large Language Models (LLMs)
    to augment their efficacy in tool usage substantially, showcasing a notable enhancement
    in performance.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们利用大型语言模型（LLMs）的程序合成能力，显著增强了它们在工具使用中的效能，展示了性能的显著提升。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We present a robust and cost-efficient framework for scalable solutions across
    a wide array of open-domain queries, even when faced with limited knowledge of
    user data/tools. It is also publically hosted to demonstrate the same.¹¹1[https://swiss-nyf.azurewebsites.net/](https://swiss-nyf.azurewebsites.net/)
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个强大且成本效益高的框架，用于广泛的开放领域查询的可扩展解决方案，即使在用户数据/工具知识有限的情况下也能应对。该框架也公开托管以展示其功能。¹¹1[https://swiss-nyf.azurewebsites.net/](https://swiss-nyf.azurewebsites.net/)
- en: 'This paper details our methodology and its evaluation by first elucidating
    the background on Tool planning [2.1](#S2.SS1 "2.1 Problem Formulation ‣ 2 Preliminaries
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") and Code generation
    using LLM [2.2](#S2.SS2 "2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF: Tool
    Grounded LLM Agents for Black Box Setting") followed by detailing individual components
    of the pipeline [3](#S3 "3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting"). Our evaluation is bifurcated into two segments: initially, we undertake
    a gray box [4.1](#S4.SS1 "4.1 Gray Box Evaluation ‣ 4 Experiments ‣ wissNYF: Tool
    Grounded LLM Agents for Black Box Setting") across principal datasets, and subsequently,
    we delve into a black box setting [4.2](#S4.SS2 "4.2 Black Box Evaluation ‣ 4
    Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"). For the
    latter, we have curated a bespoke dataset employing Toolbench prompts, intentionally
    adjusting the dataset to include only limited documentation of widely used libraries.
    This adjustment aims to validate the generalizability of our approach. Additionally,
    we juxtapose our methodology with a tailored variant of the Reverse Chain method
    to scrutinize performance disparities.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文详细介绍了我们的方法及其评估，首先阐述了工具规划的背景 [2.1](#S2.SS1 "2.1 问题表述 ‣ 2 预备知识 ‣ SwissNYF：面向黑箱环境的工具基础
    LLM 代理") 和使用 LLM 的代码生成 [2.2](#S2.SS2 "2.2 代码生成 ‣ 2 预备知识 ‣ SwissNYF：面向黑箱环境的工具基础
    LLM 代理")，接着详细描述了流程的各个组成部分 [3](#S3 "3 SwissNYF ‣ SwissNYF：面向黑箱环境的工具基础 LLM 代理")。我们的评估分为两个部分：首先，我们在主要数据集上进行灰箱评估
    [4.1](#S4.SS1 "4.1 灰箱评估 ‣ 4 实验 ‣ SwissNYF：面向黑箱环境的工具基础 LLM 代理")，随后，我们深入探讨黑箱环境评估
    [4.2](#S4.SS2 "4.2 黑箱评估 ‣ 4 实验 ‣ SwissNYF：面向黑箱环境的工具基础 LLM 代理")。对于后者，我们专门制作了一个数据集，使用
    Toolbench 提示，并有意调整数据集，仅包括广泛使用库的有限文档。这一调整旨在验证我们方法的普遍性。此外，我们将我们的方法与改进版的反向链方法进行对比，以审查性能差异。
- en: 2 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识
- en: 2.1 Problem Formulation
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题表述
- en: '![Refer to caption](img/8db2eeceb07f2b1d0578e2eac1f5554d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8db2eeceb07f2b1d0578e2eac1f5554d.png)'
- en: 'Figure 2: Illustration of SwissNYF pipeline for tool usage in Black Box setting.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 在黑箱环境中使用 SwissNYF 流程的示意图。'
- en: Tool planning within the context of a Large Language Model (LLM), denoted as
    $\rho$. This process ensures a structured and coherent response strategy, aligning
    the tools’ capabilities with the query’s specific requirements for an effective
    solution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLM）背景下的工具规划，表示为$\rho$。这个过程确保了结构化和连贯的响应策略，将工具的能力与查询的具体要求对齐，从而实现有效的解决方案。
- en: 2.2 Code Generation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 代码生成
- en: 'The integration of Reflexion Shinn et al. ([2023](#bib.bib29)) with Large Language
    Models (LLM) $\rho$. This methodology enhances code quality and aligns with contemporary
    standards, marking a leap in automated code development and verification. This
    process of iterative code generation can be mathematically denoted as Eq. [1](#S2.E1
    "In 2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting")'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Reflexion Shinn 等人（[2023](#bib.bib29)）与大型语言模型（LLM）$\rho$的集成。这一方法提高了代码质量并符合现代标准，标志着自动化代码开发和验证的飞跃。这个迭代代码生成的过程可以用数学公式表示为
    Eq. [1](#S2.E1 "在 2.2 代码生成 ‣ 2 预备知识 ‣ SwissNYF：面向黑箱环境的工具基础 LLM 代理")
- en: '|  | $$\begin{gathered}c_{i}\leftarrow\rho(q,\textit{feedback}_{i-1},c_{i-1})\\
    \textit{output}\leftarrow\mathcal{I}(c_{i})\\'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{gathered}c_{i}\leftarrow\rho(q,\textit{feedback}_{i-1},c_{i-1})\\
    \textit{output}\leftarrow\mathcal{I}(c_{i})\\'
- en: \textit{feedback}_{i},\textit{verified}\leftarrow\mathcal{F}(output)\\
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \textit{feedback}_{i},\textit{verified}\leftarrow\mathcal{F}(output)\\
- en: \end{gathered}$$ |  | (1) |
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \end{gathered}$$ |  | (1) |
- en: 3 SwissNYF
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 SwissNYF
- en: 3.1 Overview
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: '![Refer to caption](img/bca7402a57110e6ec95547c025bb4015.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bca7402a57110e6ec95547c025bb4015.png)'
- en: 'Figure 3: Detailed pipeline of our proposed approach with TOPGUN in SwissNYF'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 我们提出的方法在 SwissNYF 中与 TOPGUN 的详细流程'
- en: 'In this section, we introduce SwissNYF, a suite that enables LLM-based agents
    to efficiently navigate the action space to identify a valid solution for problem-solving
    in a black box scenario. SwissNYF is composed of five major components i.e., Function
    Signature Generation $\mathcal{P}$ as in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Problem
    Formulation ‣ 2 Preliminaries ‣ wissNYF: Tool Grounded LLM Agents for Black Box
    Setting"). We explain individual components of the pipeline in the subsequent
    subsections.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们介绍 SwissNYF，这是一个使基于 LLM 的智能体能够高效地在行动空间中导航，以识别黑箱场景中的有效解决方案的套件。SwissNYF
    由五个主要组件组成，即函数签名生成 $\mathcal{P}$ 如图[2](#S2.F2 "Figure 2 ‣ 2.1 Problem Formulation
    ‣ 2 Preliminaries ‣ SwissNYF: Tool Grounded LLM Agents for Black Box Setting")所示。我们将在后续小节中解释管道的各个组件。'
- en: '![Refer to caption](img/02f5a109af928406bf28955481ddf69c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/02f5a109af928406bf28955481ddf69c.png)'
- en: (a) Example output of CodeSynth $\mathcal{P}$ Algorithm
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CodeSynth $\mathcal{P}$ 算法的示例输出
- en: '![Refer to caption](img/4625cf4859efc3c10db6f3a7df9e67ab.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4625cf4859efc3c10db6f3a7df9e67ab.png)'
- en: (b) Example output of TOPGUN $\mathcal{G}$ Algorithm
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (b) TOPGUN $\mathcal{G}$ 算法的示例输出
- en: 'Figure 4: Illustration of pseudo function and tool planning generated by CodeSynth
    and TOPGUN, respectively.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：分别由 CodeSynth 和 TOPGUN 生成的伪函数和工具规划示意图。
- en: 3.2 Function Signature Generation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 函数签名生成
- en: 'Function signatures, conceptualized as pseudo APIs, serve to emulate the behaviour
    of real API functions based on given tool descriptions. This emulation is crucial
    for two primary reasons in our tool planning methodology: firstly, they act as
    stand-ins for actual API calls, thereby enabling LLMs to plan and execute tasks
    with higher efficiency; secondly, they are treated as pre-defined functions, facilitating
    the transformation of tool augmentation into a task akin to code generation, using
    these pseudo functions. These function signatures are distinguished by their docstrings
    and an example return object that aligns with the tool description, equipping
    the planner with the necessary means to effectively address user queries. In the
    context of our SwissNYF implementation, we have adopted a straightforward yet
    effective method for generating these function signatures, termed CodeSynth. The
    efficacy of this approach is further analyzed in [4.3](#S4.SS3 "4.3 CodeSynth
    Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '函数签名，概念化为伪 API，用于模拟基于给定工具描述的真实 API 函数行为。这种模拟在我们的工具规划方法中至关重要，主要有两个原因：首先，它们充当实际
    API 调用的替代品，从而使 LLM 能以更高效率规划和执行任务；其次，它们被视为预定义函数，使得工具增强的转化变成类似于代码生成的任务，使用这些伪函数。这些函数签名通过其文档字符串和与工具描述一致的示例返回对象来区分，赋予规划者有效处理用户查询所需的手段。在我们的
    SwissNYF 实现中，我们采用了一种简单而有效的方法来生成这些函数签名，称为 CodeSynth。该方法的有效性在[4.3](#S4.SS3 "4.3
    CodeSynth Evaluation ‣ 4 Experiments ‣ SwissNYF: Tool Grounded LLM Agents for
    Black Box Setting")中进一步分析。'
- en: 3.2.1 CodeSynth
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 CodeSynth
- en: 'For a given set of tool descriptions $t\in\mathcal{T}$. Our primary objective
    is to ensure that the arguments and return types of these pseudo-functions remain
    consistent with their descriptions. Additionally, we craft detailed docstrings
    for each pseudo-function to facilitate subsequent processes. A critical aspect
    of CodeSynth is the inclusion of an example return value, which is designed to
    mimic all potential operations the returned object might undergo during the verification
    process. The output generated by CodeSynth is illustrated in Fig. [4(a)](#S3.F4.sf1
    "In Figure 4 ‣ 3.1 Overview ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for
    Black Box Setting"). Moreover, the code generation facilitated by this block benefits
    from validation through Reflexion, as outlined in Eq. [1](#S2.E1 "In 2.2 Code
    Generation ‣ 2 Preliminaries ‣ wissNYF: Tool Grounded LLM Agents for Black Box
    Setting"). Ultimately, the methodologies applied within CodeSynth can be encapsulated
    in Algo. [1](#algorithm1 "In 3.2.1 CodeSynth ‣ 3.2 Function Signature Generation
    ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '对于给定的一组工具描述 $t\in\mathcal{T}$。我们的主要目标是确保这些伪函数的参数和返回类型与其描述保持一致。此外，我们为每个伪函数编写详细的文档字符串，以便于后续过程。CodeSynth
    的一个关键方面是包括一个示例返回值，该返回值旨在模拟返回对象在验证过程中可能经历的所有潜在操作。CodeSynth 生成的输出如图 [4(a)](#S3.F4.sf1
    "图 4 ‣ 3.1 概述 ‣ 3 SwissNYF ‣ wissNYF: 工具基础的 LLM 代理用于黑箱设置") 所示。此外，这一块促进的代码生成通过
    Reflexion 进行验证，如 Eq. [1](#S2.E1 "在 2.2 代码生成 ‣ 2 基本概念 ‣ wissNYF: 工具基础的 LLM 代理用于黑箱设置")
    中概述的那样。最终，CodeSynth 中应用的方法可以在 Algo. [1](#algorithm1 "在 3.2.1 CodeSynth ‣ 3.2 函数签名生成
    ‣ 3 SwissNYF ‣ wissNYF: 工具基础的 LLM 代理用于黑箱设置") 中封装。'
- en: 'Input: $\rho$'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: $\rho$'
- en: 'Algorithm 1 $\mathcal{P}$: CodeSynth'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 1 $\mathcal{P}$: CodeSynth'
- en: 'Utilizing the Function Calling module alongside the Interpreter, we rigorously
    test the pseudo-functions against a wide range of real-world scenarios. This approach
    guarantees that the test cases are comprehensive and reflective of actual function
    usage, allowing us to gather detailed feedback on the pseudo-functions’ performance.
    Such feedback is vital for the iterative improvement of the pseudo-functions,
    significantly enhancing their reliability and applicability in practical settings.
    Prompts for CodeSynth can be documented in [A.1](#A1.SS1 "A.1 Prompts ‣ Appendix
    A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '利用函数调用模块和解释器，我们严格测试伪函数在各种现实场景下的表现。这种方法确保测试用例是全面的，并且能够反映实际函数的使用，从而使我们能够收集关于伪函数性能的详细反馈。这种反馈对于伪函数的迭代改进至关重要，显著提高了其在实际环境中的可靠性和适用性。CodeSynth
    的提示可以记录在 [A.1](#A1.SS1 "A.1 提示 ‣ 附录 A 附录 ‣ wissNYF: 工具基础的 LLM 代理用于黑箱设置") 中。'
- en: 3.3 Corpus and Retriever
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 语料库和检索器
- en: The function signatures, crucial components of our methodology, are systematically
    stored within a corpus for future utilization by any planning system. This corpus
    facilitates the indexing of tool descriptions, enabling the precise retrieval
    of the most appropriate tool based on the index. Notably, the literature documents
    several advanced retrieval systems designed for this purpose, demonstrating exceptional
    accuracy. These include ToolBench IR Qin et al. ([2023](#bib.bib25)), APIRetriever
    Zan et al. ([2022](#bib.bib35)), Instructor-XL Su et al. ([2022](#bib.bib30)),
    and GEAR Lu et al. ([2023b](#bib.bib18)). Our framework incorporates these retrievers,
    with Instructor-XL set as the default option, owing to its proven efficacy. Furthermore,
    we are actively exploring the integration of AnyTool’s Hierarchical API Retriever
    Du et al. ([2024](#bib.bib9)), anticipating significant enhancements to our tool
    retrieval capabilities. This strategic inclusion of multiple retrievers ensures
    our system remains versatile and effective in identifying the most suitable tools
    for a given task, aligning with the latest advancements in retrieval technology.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 函数签名是我们方法论的关键组成部分，系统地存储在一个语料库中，以供任何规划系统将来使用。这个语料库促进了工具描述的索引，能够根据索引精确检索最合适的工具。值得注意的是，文献中记录了几种为此目的设计的高级检索系统，表现出卓越的准确性。这些包括
    ToolBench IR Qin 等人 ([2023](#bib.bib25))、APIRetriever Zan 等人 ([2022](#bib.bib35))、Instructor-XL
    Su 等人 ([2022](#bib.bib30)) 和 GEAR Lu 等人 ([2023b](#bib.bib18))。我们的框架包含这些检索器，其中
    Instructor-XL 被设置为默认选项，因为其 proven efficacy。此外，我们正在积极探索集成 AnyTool 的 Hierarchical
    API Retriever Du 等人 ([2024](#bib.bib9))，预计将显著提升我们的工具检索能力。这种多重检索器的战略性纳入确保了我们的系统在识别最适合的任务工具方面保持多功能和有效，符合最新的检索技术进展。
- en: 3.4 Planner
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 规划器
- en: We have implemented two planning approaches in our framework. The first leverages
    a modified Reverse Chain Zhang et al. ([2023b](#bib.bib37)) to support multiple
    end function calls by decomposing tasks into subtasks and creating sub-trees with
    the original reverse chain technique. The second, TOPGUN, is our proposed code-driven
    planning algorithm, designed for speed, efficiency, consistency, and accuracy,
    especially in black box scenarios. TOPGUN offers a streamlined alternative to
    traditional planning methods, optimizing for complex system navigation and task
    execution with greater reliability and cost-effectiveness.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在框架中实现了两种规划方法。第一种方法利用修改后的 Reverse Chain Zhang 等人 ([2023b](#bib.bib37)) 来支持多个结束函数调用，通过将任务分解为子任务并使用原始的逆链技术创建子树。第二种方法，TOPGUN，是我们提出的基于代码的规划算法，旨在提高速度、效率、一致性和准确性，尤其是在黑箱场景中。TOPGUN
    提供了传统规划方法的简化替代方案，优化了复杂系统导航和任务执行的可靠性和成本效益。
- en: 3.4.1 TOPGUN
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 TOPGUN
- en: 'Input: q: query; $\rho$ code for execution and evaluation'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：q：查询；`$\rho$` 执行和评估的代码
- en: 'Algorithm 2 $\mathcal{G}$: TOPGUN'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 `$\mathcal{G}$`：TOPGUN
- en: 'TOPGUN, an acronym for Tool Orchestration and Program synthesis for Generalizing
    over UNknown systems, redefines the approach to addressing user queries $q$, effectively
    depicted in Fig. [4(b)](#S3.F4.sf2 "In Figure 4 ‣ 3.1 Overview ‣ 3 SwissNYF ‣
    wissNYF: Tool Grounded LLM Agents for Black Box Setting"). Leveraging Reflexion
    detailed in Eq.[1](#S2.E1 "In 2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting"), the framework iteratively refines
    responses to the query. The synthesis of these components into the comprehensive
    algorithm is presented in Algo. [2](#algorithm2 "In 3.4.1 TOPGUN ‣ 3.4 Planner
    ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") showcases
    TOPGUN’s capability to navigate through various solution paths. Unlike traditional
    traversal-based techniques, TOPGUN capitalizes on the inherent code-generation
    capabilities of LLMs, facilitating a more direct and efficient solution process.
    This distinction not only enhances efficacy by pinpointing issues with precision
    but also ensures adaptability in black box scenarios, simultaneously optimizing
    performance in gray box settings. A detailed pipeline overview with TOPGUN in
    place is given in Fig.[4(b)](#S3.F4.sf2 "In Figure 4 ‣ 3.1 Overview ‣ 3 SwissNYF
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"). With prompts documented
    in [A.1](#A1.SS1 "A.1 Prompts ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM
    Agents for Black Box Setting").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'TOPGUN，代表工具编排和程序综合用于对未知系统的泛化，重新定义了处理用户查询$q$的方法，详见图[4(b)](#S3.F4.sf2 "图4 ‣ 3.1
    概述 ‣ 3 SwissNYF ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")。利用公式中详细描述的Reflexion[1](#S2.E1 "在2.2
    代码生成 ‣ 2 基础 ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")，该框架对查询进行迭代式的回应改进。这些组件的综合算法展示在Algo.[2](#algorithm2
    "在3.4.1 TOPGUN ‣ 3.4 规划器 ‣ 3 SwissNYF ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")中，展现了TOPGUN在各种解决路径中导航的能力。与传统的基于遍历的技术不同，TOPGUN利用LLM的内在代码生成能力，提供了更直接和高效的解决过程。这一区别不仅通过精确定位问题提高了效率，还确保了在黑箱场景中的适应性，同时优化了灰箱设置中的性能。图[4(b)](#S3.F4.sf2
    "图4 ‣ 3.1 概述 ‣ 3 SwissNYF ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")中给出了TOPGUN的详细管道概述。提示文档见[A.1](#A1.SS1
    "A.1 提示 ‣ 附录A ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")。'
- en: '![Refer to caption](img/f719685ba53074f6132e75958f1b1db0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f719685ba53074f6132e75958f1b1db0.png)'
- en: 'Figure 5: Illustartion of Self-Reflection Mechanism in TOPGUN'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：TOPGUN中的自我反射机制插图
- en: 3.5 Verifier
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 验证器
- en: Verification is closely linked to the functionality of the Planner $\mathcal{G}$
    can use for subsequent iterations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 验证与规划器 $\mathcal{G}$ 的功能密切相关，可以用于后续的迭代。
- en: 'In our framework, we leverage Reflexion Shinn et al. ([2023](#bib.bib29)),
    detailed in Eq. [1](#S2.E1 "In 2.2 Code Generation ‣ 2 Preliminaries ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting") and depicted in Algo. [2](#algorithm2
    "In 3.4.1 TOPGUN ‣ 3.4 Planner ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting"), to seamlessly integrate verification and feedback within
    the TOPGUN methodology. This eliminates the requirement for an additional function
    call module, concentrating instead on directly executing code pertinent to the
    user query. This approach is illustrated in Fig. [5](#S3.F5 "Figure 5 ‣ 3.4.1
    TOPGUN ‣ 3.4 Planner ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting"), providing a visual representation of the concept.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的框架中，我们利用Reflexion Shinn等人（[2023](#bib.bib29)），详见公式[1](#S2.E1 "在2.2 代码生成
    ‣ 2 基础 ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")和Algo.[2](#algorithm2 "在3.4.1 TOPGUN ‣ 3.4
    规划器 ‣ 3 SwissNYF ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")，将验证和反馈无缝集成到TOPGUN方法中。这消除了对额外功能调用模块的需求，集中在直接执行与用户查询相关的代码上。这种方法在图[5](#S3.F5
    "图5 ‣ 3.4.1 TOPGUN ‣ 3.4 规划器 ‣ 3 SwissNYF ‣ wissNYF: 工具驱动的LLM代理用于黑箱设置")中进行了说明，提供了该概念的视觉表示。'
- en: 3.6 Parser
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 解析器
- en: The Parser $\mathcal{K}$. The process’s efficacy is markedly improved by the
    judicious reuse of elements from the individual trees during their amalgamation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器 $\mathcal{K}$。通过在合并过程中明智地重复使用各个树中的元素，过程的效率得到了显著提升。
- en: Conversely, for the TOPGUN methodology, we adopt the established Abstract Syntax
    Tree (AST) paradigm Fischer et al. ([2007](#bib.bib10)) to segment the program
    into fundamental function calls, alongside specifying their arguments and return
    values. This segmentation is instrumental in constructing a systematic series
    of tool invocations. This meticulously arranged series, denoted as $St$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，对于 TOPGUN 方法，我们采用了已建立的抽象语法树 (AST) 范式 Fischer 等人 ([2007](#bib.bib10)) 将程序分解为基本函数调用，并指定其参数和返回值。这种分解对于构建系统化的工具调用序列至关重要。这一精心安排的序列被称为
    $St$。
- en: 'The entire pipeline, as depicted in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Overview
    ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"), emerges
    from the integration of various components designed to effectively address user
    queries through the strategic orchestration of tools within the SwissNYF framework.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S3.F3 "Figure 3 ‣ 3.1 Overview ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting") 所示，整个流程源于各种组件的集成，这些组件旨在通过在 SwissNYF 框架内战略性地协调工具来有效解决用户查询。'
- en: 'Table 1: Win Rate of different Candidate and Reference model over G1 set'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 不同候选模型和参考模型在 G1 集上的胜率'
- en: '| Candidate | Reference | G1-Instruction | G1-Tool | G1-Category |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 候选 | 参考 | G1-Instruction | G1-Tool | G1-Category |'
- en: '| T.LLaMA ReACT | ChatGPT ReACT | 45.0 | 42.0 | 47.5 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| T.LLaMA ReACT | ChatGPT ReACT | 45.0 | 42.0 | 47.5 |'
- en: '| T.LLaMA DFSDT | ChatGPT ReACT | 55.0 | 55.3 | 54.5 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| T.LLaMA DFSDT | ChatGPT ReACT | 55.0 | 55.3 | 54.5 |'
- en: '| T.LLaMA DFSDT+Ret | ChatGPT ReACT | 62.3 | 59.0 | 55.0 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| T.LLaMA DFSDT+Ret | ChatGPT ReACT | 62.3 | 59.0 | 55.0 |'
- en: '| ChatGPT DFSDT | ChatGPT ReACT | 60.5 | 62.0 | 57.3 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT DFSDT | ChatGPT ReACT | 60.5 | 62.0 | 57.3 |'
- en: '| GPT4 ReACT | ChatGPT ReACT | 60.0 | 58.8 | 63.5 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 ReACT | ChatGPT ReACT | 60.0 | 58.8 | 63.5 |'
- en: '| GPT4 DFSDT | ChatGPT ReACT | 67.5 | 67.8 | 66.5 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 DFSDT | ChatGPT ReACT | 67.5 | 67.8 | 66.5 |'
- en: '| GPT4 TOPGUN | ChatGPT ReACT | 88.192 | 87.46 | 87.15 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | ChatGPT ReACT | 88.192 | 87.46 | 87.15 |'
- en: '| GPT4 TOPGUN | ChatGPT DFSDT | 78.49 | 77.55 | 76.24 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | ChatGPT DFSDT | 78.49 | 77.55 | 76.24 |'
- en: '| GPT4 TOPGUN | T.LLaMA ReACT | 86.72 | 82.94 | 80.80 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | T.LLaMA ReACT | 86.72 | 82.94 | 80.80 |'
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT | 81.75 | 75.51 | 73.81 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | T.LLaMA DFSDT | 81.75 | 75.51 | 73.81 |'
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT+Ret | 80.35 | 77.11 | 75.39 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | T.LLaMA DFSDT+Ret | 80.35 | 77.11 | 75.39 |'
- en: '| GPT4 TOPGUN | GPT4 ReACT | 82.996 | 79.956 | 77.633 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | GPT4 ReACT | 82.996 | 79.956 | 77.633 |'
- en: '| GPT4 TOPGUN | GPT4 DFSDT | 82.065 | 73.69 | 71.14 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | GPT4 DFSDT | 82.065 | 73.69 | 71.14 |'
- en: 4 Experiments
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Tool planning datasets, while diverse, often fall short in supporting multi-turn
    and multi-call dialogues, as seen in works by Schick et al. ([2023](#bib.bib28))
    and Tang et al. ([2023](#bib.bib31)), and lack precise evaluation metrics, complicating
    thorough assessments. Even comprehensive datasets like ToolBench by Qin et al.
    ([2023](#bib.bib25)) struggle with aligning to black-box settings, presenting
    significant challenges for evaluating tool planning in such scenarios.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管工具规划数据集多种多样，但常常无法支持多轮和多次调用的对话，正如 Schick 等人 ([2023](#bib.bib28)) 和 Tang 等人
    ([2023](#bib.bib31)) 的研究所示，并且缺乏精确的评估指标，使得彻底评估变得复杂。即使是如 Qin 等人 ([2023](#bib.bib25))
    所提出的 ToolBench 这样全面的数据集，在与黑盒设置对齐时也面临重大挑战，这给这种场景中的工具规划评估带来了显著困难。
- en: 'Our evaluation employs the ToolBench benchmark Qin et al. ([2023](#bib.bib25))
    and a specially curated dataset for unchar codebases, assessed in both gray ([4.1](#S4.SS1
    "4.1 Gray Box Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for
    Black Box Setting")) and black box ([4.2](#S4.SS2 "4.2 Black Box Evaluation ‣
    4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting")) settings.
    We benchmark our TOPGUN approach against existing methods using win rate, token
    count, and success rate. Additionally, we scrutinize CodeSynth’s ($\mathcal{P}$)
    performance and independently evaluate its ability to generate effective function
    signatures, acting as pseudo functions, detailed in Section [4.3](#S4.SS3 "4.3
    CodeSynth Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的评估采用了 ToolBench 基准 Qin 等人 ([2023](#bib.bib25)) 和专门策划的数据集，针对未公开代码库进行评估，包括灰盒
    ([4.1](#S4.SS1 "4.1 Gray Box Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting")) 和黑盒 ([4.2](#S4.SS2 "4.2 Black Box Evaluation
    ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting")) 设置。我们使用胜率、令牌数和成功率来基准测试我们的
    TOPGUN 方法，并将其与现有方法进行比较。此外，我们还审查了 CodeSynth 的 ($\mathcal{P}$) 性能，并独立评估其生成有效函数签名的能力，这些详细信息在第
    [4.3](#S4.SS3 "4.3 CodeSynth Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting") 节中说明。'
- en: 'Table 2: Win Rate of different Candidate and Reference model over G2, G3 set
    and Average over all sets'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 不同候选模型和参考模型在 G2、G3 数据集上的胜率及所有数据集的平均值'
- en: '| Candidate | Reference | G2-Instruction | G2-Category | G3-Instruction | Average
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 候选模型 | 参考模型 | G2-指令 | G2-类别 | G3-指令 | 平均值 |'
- en: '| T.LLaMA ReACT | ChatGPT ReACT | 50.8 | 41.8 | 55.0 | 47.0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| T.LLaMA ReACT | ChatGPT ReACT | 50.8 | 41.8 | 55.0 | 47.0 |'
- en: '| T.LLaMA DFSDT | ChatGPT ReACT | 68.5 | 58.0 | 69.0 | 60.0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| T.LLaMA DFSDT | ChatGPT ReACT | 68.5 | 58.0 | 69.0 | 60.0 |'
- en: '| T.LLaMA DFSDT+Ret | ChatGPT ReACT | 68.5 | 60.8 | 73.0 | 63.1 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| T.LLaMA DFSDT+Ret | ChatGPT ReACT | 68.5 | 60.8 | 73.0 | 63.1 |'
- en: '| ChatGPT DFSDT | ChatGPT ReACT | 72.0 | 64.8 | 69.0 | 64.3 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT DFSDT | ChatGPT ReACT | 72.0 | 64.8 | 69.0 | 64.3 |'
- en: '| GPT4 ReACT | ChatGPT ReACT | 65.8 | 60.3 | 78.0 | 64.0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 ReACT | ChatGPT ReACT | 65.8 | 60.3 | 78.0 | 64.0 |'
- en: '| GPT4 DFSDT | ChatGPT ReACT | 73.3 | 63.3 | 84.0 | 70.4 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 DFSDT | ChatGPT ReACT | 73.3 | 63.3 | 84.0 | 70.4 |'
- en: '| GPT4 TOPGUN | ChatGPT ReACT | 87.59 | 78.78 | 90.05 | 86.54 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | ChatGPT ReACT | 87.59 | 78.78 | 90.05 | 86.54 |'
- en: '| GPT4 TOPGUN | ChatGPT DFSDT | 81.63 | 73.07 | 85.26 | 78.71 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | ChatGPT DFSDT | 81.63 | 73.07 | 85.26 | 78.71 |'
- en: '| GPT4 TOPGUN | T.LLaMA ReACT | 86.24 | 77.71 | 93.23 | 84.61 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | T.LLaMA ReACT | 86.24 | 77.71 | 93.23 | 84.61 |'
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT | 78.31 | 71.80 | 89.47 | 78.44 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | T.LLaMA DFSDT | 78.31 | 71.80 | 89.47 | 78.44 |'
- en: '| GPT4 TOPGUN | T.LLaMA DFSDT+Ret | 83.07 | 72.92 | 87.82 | 79.44 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | T.LLaMA DFSDT+Ret | 83.07 | 72.92 | 87.82 | 79.44 |'
- en: '| GPT4 TOPGUN | GPT4 ReACT | 78.61 | 73.75 | 93.68 | 80.27 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | GPT4 ReACT | 78.61 | 73.75 | 93.68 | 80.27 |'
- en: '| GPT4 TOPGUN | GPT4 DFSDT | 73.92 | 71.35 | 79.25 | 78.59 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 TOPGUN | GPT4 DFSDT | 73.92 | 71.35 | 79.25 | 78.59 |'
- en: 4.1 Gray Box Evaluation
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 灰盒评估
- en: 'To assess the performance of TOPGUN and compare it with other gray box methodologies
    such as ReACT and DFSDT, we maintain the integrity of our pipeline while adapting
    the evaluation process to incorporate actual functions in place of pseudo functions
    within the output solution trajectory. This approach effectively leaves our black
    box pipeline intact while converting it into a gray box evaluation framework.
    The necessity of responses and Final answers for evaluation purposes has led us
    to adopt this hybrid strategy. In practical scenarios, this mirrors the process
    where a generalist planner delivers a strategy to the client, who then substitutes
    pseudo-function implementations with their real functions for execution. For this
    evaluation, we employ ToolBench, as detailed by Qin et al. ([2023](#bib.bib25)),
    and conduct our analysis across all problem categories provided in the dataset.
    Further elaboration on the precise evaluation methodology and the application
    of ToolBench is documented in [A.2](#A1.SS2 "A.2 ToolBench for Gray Box Evaluation
    ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 TOPGUN 的性能并将其与其他灰盒方法（如 ReACT 和 DFSDT）进行比较，我们在保持管道完整性的同时，调整评估过程以纳入实际函数替代输出解决方案轨迹中的伪函数。这种方法有效地保留了我们的黑盒管道，同时将其转化为灰盒评估框架。评估目的对响应和最终答案的需求促使我们采用这种混合策略。在实际场景中，这类似于一个通才规划师向客户提供策略，客户随后用实际函数替代伪函数进行执行。为了此次评估，我们使用了
    ToolBench，如秦等人（[2023](#bib.bib25)）所述，并在数据集中提供的所有问题类别上进行分析。关于精确评估方法和 ToolBench
    应用的进一步阐述记录在 [A.2](#A1.SS2 "A.2 ToolBench for Gray Box Evaluation ‣ Appendix A
    Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") 中。'
- en: 'Results : Win Rate comparisons for ToolLLaMa-ReACT, ToolLLaMA-DFSDT, ChatGPT-DFSDT,
    GPT4-DFSDT, and GPT4-TOPGUN against ChatGPT-ReACT and GPT4-TOPGUN are summarized,
    with averages taken from 7 runs per model pair, detailed in Tables [1](#S3.T1
    "Table 1 ‣ 3.6 Parser ‣ 3 SwissNYF ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting") and [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting"). TOPGUN significantly surpassed ReAct and DFSDT
    in all categories, achieving win rates of 80.27% versus GPT4-ReACT, 78.59% against
    GPT4-DFSDT, and 86.54% against ChatGPT-ReACT, showing improvements of 22.54% and
    16.14% respectively. These results highlight TOPGUN’s superior ability to create
    tool plans that align with preference evaluation criteria across various conditions.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：ToolLLaMa-ReACT、ToolLLaMA-DFSDT、ChatGPT-DFSDT、GPT4-DFSDT 和 GPT4-TOPGUN 对
    ChatGPT-ReACT 和 GPT4-TOPGUN 的胜率比较总结，如表 [1](#S3.T1 "Table 1 ‣ 3.6 Parser ‣ 3 SwissNYF
    ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting") 和 [2](#S4.T2 "Table
    2 ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting")
    所示，平均值取自每对模型的 7 次运行。TOPGUN 在所有类别中显著超过 ReAct 和 DFSDT，获得了 80.27% 对比 GPT4-ReACT、78.59%
    对比 GPT4-DFSDT 和 86.54% 对比 ChatGPT-ReACT 的胜率，分别提高了 22.54% 和 16.14%。这些结果突显了 TOPGUN
    在各种条件下创建符合偏好评估标准的工具计划的卓越能力。'
- en: 4.2 Black Box Evaluation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 黑箱评估
- en: '![Refer to caption](img/588d2add61cc8c679a8ad46bfb99a8d3.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/588d2add61cc8c679a8ad46bfb99a8d3.png)'
- en: 'Figure 6: Average Token Consumption of individual methodologies in Black Box
    setting.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：黑箱设置中各个方法的平均令牌消耗。
- en: 'Utilizing the Data Generation pipeline from Qin et al. ([2023](#bib.bib25)),
    we constructed a black-box scenario dataset featuring 36 LLaMa-Hub LlamaIndex
    ([2023](#bib.bib16)) tools and unique functions from private libraries. Following
    Zan et al. ([2022](#bib.bib35)), we converted Pandas and Numpy into Monkey and
    BeatNum packages, renaming all internal functions and structures to test planner
    generalizability without LLM prior knowledge. This dataset, detailed at [A.1](#A1.SS1
    "A.1 Prompts ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting"), focuses on accuracy of the solution trajectory, with each query
    designed for a single correct path. After manual annotation, it comprises 100
    queries and 162 tools, with samples and TOPGUN outcomes at [A.3.2](#A1.SS3.SSS2
    "A.3.2 Queries Example ‣ A.3 PrivateEval Dataset ‣ Appendix A Appendix ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting") and [A.5.2](#A1.SS5.SSS2 "A.5.2
    PrivateEval ‣ A.5 TOPGUN Examples ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded
    LLM Agents for Black Box Setting").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '利用秦等人（[2023](#bib.bib25)）的数据生成管道，我们构建了一个黑箱场景数据集，包含36个LLaMa-Hub LlamaIndex（[2023](#bib.bib16)）工具和来自私人库的独特功能。遵循赞等人（[2022](#bib.bib35)）的方法，我们将Pandas和Numpy转换为Monkey和BeatNum包，将所有内部函数和结构重命名，以测试规划器在没有LLM先验知识情况下的泛化能力。该数据集的详细信息见[A.1](#A1.SS1
    "A.1 Prompts ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents for Black
    Box Setting")，重点关注解决方案轨迹的准确性，每个查询设计为单一路径。经过手动注释，它包括100个查询和162个工具，样本和TOPGUN结果见[A.3.2](#A1.SS3.SSS2
    "A.3.2 Queries Example ‣ A.3 PrivateEval Dataset ‣ Appendix A Appendix ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting")和[A.5.2](#A1.SS5.SSS2 "A.5.2 PrivateEval
    ‣ A.5 TOPGUN Examples ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting")。'
- en: 'Results : The black-box evaluation, featuring TOPGUN and a revised Reverse
    Chain, utilizes $\mathcal{P}$ function signatures for a comprehensive black-box
    methodology. TOPGUN surpasses Reverse Chain and undergoes comparison with GPT4-DFSDT
    and GPT4-ReACT within gray box evaluations, emphasizing output trajectories. Success
    rates, derived from exact trajectory matches with the ground truth and averaged
    over ten iterations, are documented in Table [4](#S4.T4 "Table 4 ‣ 4.3 CodeSynth
    Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting").
    Figure [6](#S4.F6 "Figure 6 ‣ 4.2 Black Box Evaluation ‣ 4 Experiments ‣ wissNYF:
    Tool Grounded LLM Agents for Black Box Setting") details the Average Token usage
    for each algorithm per query, underscoring TOPGUN’s effectiveness and efficiency
    in generating precise and resourceful tool plans in black-box scenarios, demonstrating
    its adaptability across diverse datasets.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：黑箱评估中，使用TOPGUN和修订后的Reverse Chain，利用$\mathcal{P}$函数签名进行全面的黑箱方法。TOPGUN优于Reverse
    Chain，并与GPT4-DFSDT和GPT4-ReACT在灰箱评估中进行比较，重点关注输出轨迹。成功率源自与真实轨迹的完全匹配，并在十次迭代中取平均，结果记录在表[4](#S4.T4
    "Table 4 ‣ 4.3 CodeSynth Evaluation ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM
    Agents for Black Box Setting")。图[6](#S4.F6 "Figure 6 ‣ 4.2 Black Box Evaluation
    ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting")详细说明了每个算法每个查询的平均令牌使用情况，突出了TOPGUN在黑箱场景中生成准确且高效工具计划的效果和效率，展示了其在多样数据集中的适应性。'
- en: 'Note: A black-box evaluation using ToolBench is infeasible, as ToolEval’s metrics,
    such as pass rate and win rate, rely on intermediate tool responses and the final
    answer.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于ToolBench的评估依赖于中间工具响应和最终答案，使用ToolBench进行黑箱评估不可行。
- en: 4.3 CodeSynth Evaluation
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 CodeSynth 评估
- en: 'To assess the quality of function signatures produced by CodeSynth, we adopt
    neuro-symbolic representations, as proposed by Parisotto et al. ([2017](#bib.bib23))
    and Nye et al. ([2021](#bib.bib20)). These representations aim to capture the
    abstract semantic essence of a given program, aligning well with our objectives.
    Our evaluation spans the Python subset of HumanEval-X Zheng et al. ([2023](#bib.bib38))
    and MBPP Austin et al. ([2021](#bib.bib3)) dataset. Inspired by the semantic probing
    model introduced by Ma et al. ([2023](#bib.bib19)), we construct semantic representations
    of both synthesized pseudo functions and ground truth code. Utilizing the tree-sitter
    Brunsfeld et al. ([2024](#bib.bib5)) package, we form the Abstract Syntax Tree,
    focusing our computation of the F1 score exclusively on the Function Definition
    block while excluding the body block. Hence, the final metric is precisely representative
    of our objective with CodeSynth. The appendix [A.4.1](#A1.SS4.SSS1 "A.4.1 HumanEval-X
    ‣ A.4 CodeSynth Examples ‣ Appendix A Appendix ‣ wissNYF: Tool Grounded LLM Agents
    for Black Box Setting") can be referred to for function signature examples synthesized
    with the HumanEval-X dataset.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '为评估 CodeSynth 生成的函数签名的质量，我们采用了 Parisotto 等人 ([2017](#bib.bib23)) 和 Nye 等人 ([2021](#bib.bib20))
    提出的神经符号表示。这些表示旨在捕捉给定程序的抽象语义本质，与我们的目标高度一致。我们的评估涵盖了 HumanEval-X Zheng 等人 ([2023](#bib.bib38))
    和 MBPP Austin 等人 ([2021](#bib.bib3)) 数据集的 Python 子集。受 Ma 等人 ([2023](#bib.bib19))
    介绍的语义探测模型启发，我们构建了合成伪函数和真实代码的语义表示。利用 tree-sitter Brunsfeld 等人 ([2024](#bib.bib5))
    包，我们形成了抽象语法树，计算 F1 分数时仅关注函数定义块，排除主体块。因此，最终指标准确代表了 CodeSynth 的目标。有关使用 HumanEval-X
    数据集合成的函数签名示例，请参见附录 [A.4.1](#A1.SS4.SSS1 "A.4.1 HumanEval-X ‣ A.4 CodeSynth 示例
    ‣ 附录 A 附录 ‣ wissNYF: 工具驱动的 LLM 代理黑箱设置")。'
- en: 'Results: We evaluate CodeSynth across multiple reflection cycles, tracking
    the F1 score for each cycle to illustrate consistent enhancements in function
    signature quality, as depicted in Table [4](#S4.T4 "Table 4 ‣ 4.3 CodeSynth Evaluation
    ‣ 4 Experiments ‣ wissNYF: Tool Grounded LLM Agents for Black Box Setting"). CodeSynth
    significantly improved F1-scores on both HumanEval-X and MBPP datasets, achieving
    a perfect score of 1.0 by the fifth iteration from initial scores of 0.844 and
    0.912, respectively. These findings highlight CodeSynth’s ability to produce function
    signatures closely resembling the semantics of the target function.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '结果：我们在多个反射周期中评估了 CodeSynth，跟踪每个周期的 F1 分数，以说明函数签名质量的一致性提升，如表 [4](#S4.T4 "表 4
    ‣ 4.3 CodeSynth 评估 ‣ 4 实验 ‣ wissNYF: 工具驱动的 LLM 代理黑箱设置")所示。CodeSynth 在 HumanEval-X
    和 MBPP 数据集上的 F1 分数显著提高，第五次迭代时达到完美分数 1.0，相较于最初的 0.844 和 0.912。这些发现突显了 CodeSynth
    在生成与目标函数语义相近的函数签名方面的能力。'
- en: '| Method | Success Rate |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 成功率 |'
- en: '| GPT4-TOPGUN | 70.58 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-TOPGUN | 70.58 |'
- en: '| GPT4-DFSDT | 61.45 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-DFSDT | 61.45 |'
- en: '| GPT4-ReAct | 45.45 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-ReAct | 45.45 |'
- en: '| GPT4-ReverseChain | 43.75 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-ReverseChain | 43.75 |'
- en: 'Table 3: Comparison of methodologies in Black Box Setting'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 黑箱设置中方法论的比较'
- en: '| Dataset | F1 Score for max Reflexion Iteration |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 最大反射迭代的 F1 分数 |'
- en: '| @1 | @2 | @3 | @4 | @5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| @1 | @2 | @3 | @4 | @5 |'
- en: '| HumanEval-X | 0.844 | 0.894 | 0.965 | 0.983 | 1.00 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval-X | 0.844 | 0.894 | 0.965 | 0.983 | 1.00 |'
- en: '| MBPP | 0.912 | 0.963 | 0.994 | 1.00 | 1.00 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| MBPP | 0.912 | 0.963 | 0.994 | 1.00 | 1.00 |'
- en: 'Table 4: CodeSynth Evaluation for analyzing Reflexions improvement on Function
    Signature’s AST'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: CodeSynth 评估用于分析函数签名 AST 的反射改进'
- en: 5 Conclusion
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we address the challenge of tool planning in black-box settings,
    where direct access to API calls and their implementations is not feasible, raising
    concerns about cost efficiency and privacy in API interactions. We introduce SwissNYF,
    a comprehensive framework designed to equip Large Language Models (LLMs) with
    the ability to navigate these scenarios effectively. Central to SwissNYF is the
    ingenious function signature generation that allows the planner to rely on tool
    descriptions, circumventing the need for actual API executions. We further introduce
    TOPGUN, a code-driven planning approach leveraging LLMs’ code generation capabilities
    to offer a robust solution for black-box environments. Our extensive evaluation
    across various toolsets and settings demonstrates the superior performance of
    our methodology against traditional tool planning strategies, validating its effectiveness
    and reliability. Through SwissNYF and TOPGUN, we establish an exciting and emerging
    paradigm in tool planning, We envision SwissNYF as a central hub for black-box
    tool usage, encouraging future advancements in developing strategies for black-box
    scenarios, thus making a significant leap towards efficient, privacy-conscious
    tool planning in the realm of LLM-enhanced applications.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们解决了在黑箱设置中工具规划的挑战，其中直接访问API调用及其实现不可行，导致了关于API交互中的成本效益和隐私问题。我们引入了SwissNYF，一个全面的框架，旨在使大型语言模型（LLMs）能够有效地应对这些场景。SwissNYF的核心是巧妙的函数签名生成，它允许规划者依赖于工具描述，绕过实际的API执行需求。我们进一步介绍了TOPGUN，一种基于代码的规划方法，利用LLMs的代码生成能力，为黑箱环境提供了一个强大的解决方案。我们在各种工具集和设置下的广泛评估展示了我们的方法相较于传统工具规划策略的优越性能，验证了其有效性和可靠性。通过SwissNYF和TOPGUN，我们建立了一个令人兴奋的新兴工具规划范式，我们预见SwissNYF将成为黑箱工具使用的核心枢纽，鼓励未来在黑箱场景中开发策略的进展，从而在LLM增强应用领域实现高效、隐私意识强的工具规划的重大飞跃。
- en: References
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人（2023）乔什·阿基亚姆、史蒂文·阿德勒、桑迪尼·阿加瓦尔、拉玛·艾哈迈德、伊尔盖·阿卡亚、弗洛伦西亚·莱奥尼·阿尔曼、迪奥戈·阿尔梅达、扬科·阿尔滕施密特、萨姆·奥特曼、沙亚马尔·阿纳德卡特等。Gpt-4技术报告。*arXiv预印本arXiv:2303.08774*，2023。
- en: Anil et al. (2023) Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry
    Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
    Chen, et al. Palm 2 technical report. *arXiv preprint arXiv:2305.10403*, 2023.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil等人（2023）罗汉·阿尼尔、安德鲁·M·戴、奥尔汗·费拉特、梅尔文·约翰逊、德米特里·列皮欣、亚历山德雷·帕索斯、西亚马克·沙克里、埃曼纽尔·塔罗帕、佩奇·贝利、智峰·陈等。Palm
    2技术报告。*arXiv预印本arXiv:2305.10403*，2023。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin等人（2021）雅各布·奥斯汀、奥古斯都·奥登纳、麦克斯韦·奈、马尔滕·博斯马、亨雷克·米哈维斯基、大卫·多汉、艾伦·江、凯瑞·蔡、迈克尔·特里、阮文安等。利用大型语言模型进行程序合成。*arXiv预印本arXiv:2108.07732*，2021。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）汤姆·布朗、本杰明·曼、尼克·赖德、梅拉妮·苏比亚、贾雷德·D·卡普兰、普拉弗拉·达里瓦尔、阿尔文·内拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。语言模型是少量示例学习者。*《神经信息处理系统进展》*，33:1877–1901，2020。
- en: 'Brunsfeld et al. (2024) Max Brunsfeld, Andrew Hlynskyi, Amaan Qureshi, Patrick
    Thomson, Josh Vera, Phil Turnbull, Timothy Clem, Douglas Creager, Andrew Helwer,
    dundargoc, Rob Rix, Daumantas Kavolis, Hendrik van Antwerpen, Michael Davis, Ika,
    Tuan-Anh Nguyen, Amin Yahyaabadi, Stafford Brunk, Matt Massicotte, and George
    Fraser. tree-sitter/tree-sitter: v0.21.0-pre-release-1, 2024. URL [https://doi.org/10.5281/zenodo.10638807](https://doi.org/10.5281/zenodo.10638807).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brunsfeld等人（2024）马克斯·布伦斯费尔德、安德鲁·赫林斯基、阿曼·库雷希、帕特里克·汤姆森、乔希·维拉、菲尔·特恩布尔、蒂莫西·克莱姆、道格拉斯·克雷格、安德鲁·赫尔维尔、dundargoc、罗布·里克斯、道曼塔斯·卡沃利斯、亨德里克·范安特卫普、迈克尔·戴维斯、伊卡、阮文安、阿敏·亚哈亚巴迪、斯塔福德·布伦克、马特·马西科特和乔治·弗雷泽。tree-sitter/tree-sitter:
    v0.21.0-pre-release-1，2024。网址 [https://doi.org/10.5281/zenodo.10638807](https://doi.org/10.5281/zenodo.10638807)。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021）马克·陈、杰瑞·特沃雷克、赫渥·俊、启明·袁、亨利克·庞德·德·奥利维拉·平托、贾雷德·卡普兰、哈里·爱德华兹、尤里·布尔达、尼古拉斯·约瑟夫、格雷格·布洛克曼等。评估在代码上训练的大型语言模型。*arXiv
    预印本 arXiv:2107.03374*，2021年。
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks. *arXiv preprint arXiv:2211.12588*, 2022.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2022）文华·陈、薛光·马、新怡·王和威廉·W·科恩。思想提示程序：将计算与推理分离用于数值推理任务。*arXiv 预印本 arXiv:2211.12588*，2022年。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 24(240):1–113, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery等（2023）阿坎莎·乔杜赫里、沙兰·纳朗、雅各布·德夫林、马滕·博斯马、高拉夫·米什拉、亚当·罗伯茨、保罗·巴拉姆、兴·温·钟、查尔斯·萨顿、塞巴斯蒂安·格尔曼等。Palm：通过路径扩展语言建模。*机器学习研究杂志*，24(240)：1–113，2023年。
- en: 'Du et al. (2024) Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective,
    hierarchical agents for large-scale api calls. *arXiv preprint arXiv:2402.04253*,
    2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等（2024）余·杜、方云·魏和洪阳·张。Anytool：用于大规模API调用的自我反思层次代理。*arXiv 预印本 arXiv:2402.04253*，2024年。
- en: Fischer et al. (2007) Gregor Fischer, J Lusiardi, and J Wolff Von Gudenberg.
    Abstract syntax trees-and their role in model driven software development. In
    *International Conference on Software Engineering Advances (ICSEA 2007)*, pp. 
    38–38\. IEEE, 2007.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 费舍尔等（2007）格雷戈尔·费舍尔、J·卢西亚尔迪和J·沃尔夫·冯·古登贝格。抽象语法树及其在模型驱动软件开发中的作用。在*国际软件工程进展会议（ICSEA
    2007）*，第38–38页。IEEE，2007年。
- en: 'Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models.
    In *International Conference on Machine Learning*, pp.  10764–10799\. PMLR, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等（2023）吕宇·高、阿曼·马丹、舒燕·周、乌里·阿隆、彭飞·刘、宜明·杨、杰米·卡兰和格雷厄姆·纽比格。PAL：程序辅助语言模型。在*国际机器学习大会*，第10764–10799页。PMLR，2023年。
- en: 'Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie
    Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for
    mathematical problem solving. *arXiv preprint arXiv:2309.17452*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 勾等（2023）志斌·勾、志宏·邵、叶云·龚、余久·杨、敏磊·黄、南段、伟柱·陈等。Tora：用于数学问题解决的工具集成推理代理。*arXiv 预印本
    arXiv:2309.17452*，2023年。
- en: 'Hao et al. (2023) Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt:
    Augmenting frozen language models with massive tools via tool embeddings. *arXiv
    preprint arXiv:2305.11554*, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郝等（2023）世博·郝、天阳·刘、震·王和志廷·胡。Toolkengpt：通过工具嵌入增强冻结语言模型。*arXiv 预印本 arXiv:2305.11554*，2023年。
- en: 'Huang & Chang (2023) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning
    in large language models: A survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki
    Okazaki (eds.), *Findings of the Association for Computational Linguistics: ACL
    2023*, pp.  1049–1065, Toronto, Canada, July 2023\. Association for Computational
    Linguistics. doi: 10.18653/v1/2023.findings-acl.67. URL [https://aclanthology.org/2023.findings-acl.67](https://aclanthology.org/2023.findings-acl.67).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '黄与张（2023）界·黄和凯文·陈-川·张。面向大型语言模型的推理：一项调查。在安娜·罗杰斯、乔丹·博伊德-格雷伯和冈崎直明（编），*计算语言学协会会议论文集：ACL
    2023*，第1049–1065页，加拿大多伦多，2023年7月。计算语言学协会。doi: 10.18653/v1/2023.findings-acl.67。网址
    [https://aclanthology.org/2023.findings-acl.67](https://aclanthology.org/2023.findings-acl.67)。'
- en: 'Li et al. (2023) Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman,
    Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code:
    Reasoning with a language model-augmented code emulator. *arXiv preprint arXiv:2312.04474*,
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2023）程书·李、杰基·梁、安迪·曾、新云·陈、卡罗尔·豪斯曼、多尔萨·萨迪赫、谢尔盖·列文、李·飞飞、费·夏和布莱恩·伊克特。代码链：与语言模型增强的代码模拟器进行推理。*arXiv
    预印本 arXiv:2312.04474*，2023年。
- en: LlamaIndex (2023) LlamaIndex. Llamahub, 2023. URL [https://web.archive.org/web/20231229215448/https://llamahub.ai/](https://web.archive.org/web/20231229215448/https://llamahub.ai/).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LlamaIndex（2023）LlamaIndex。Llamahub，2023年。网址 [https://web.archive.org/web/20231229215448/https://llamahub.ai/](https://web.archive.org/web/20231229215448/https://llamahub.ai/)。
- en: 'Lu et al. (2023a) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional
    reasoning with large language models. *arXiv preprint arXiv:2304.09842*, 2023a.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu等人（2023a）Pan Lu、Baolin Peng、Hao Cheng、Michel Galley、Kai-Wei Chang、Ying Nian
    Wu、Song-Chun Zhu和Jianfeng Gao。Chameleon：具有大型语言模型的即插即用的组合推理。*arXiv预印本 arXiv:2304.09842*，2023年。
- en: 'Lu et al. (2023b) Yining Lu, Haoping Yu, and Daniel Khashabi. Gear: Augmenting
    language models with generalizable and efficient tool resolution. *arXiv preprint
    arXiv:2307.08775*, 2023b.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu等人（2023b）Yining Lu、Haoping Yu和Daniel Khashabi。Gear：通过通用且高效的工具解析增强语言模型。*arXiv预印本
    arXiv:2307.08775*，2023年。
- en: Ma et al. (2023) Wei Ma, Mengjie Zhao, Xiaofei Xie, Qiang Hu, Shangqing Liu,
    Jie Zhang, Wenhan Wang, and Yang Liu. Are code pre-trained models powerful to
    learn code syntax and semantics?, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等人（2023）Wei Ma、Mengjie Zhao、Xiaofei Xie、Qiang Hu、Shangqing Liu、Jie Zhang、Wenhan
    Wang和Yang Liu。代码预训练模型是否能够强大地学习代码语法和语义？2023年。
- en: Nye et al. (2021) Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B.
    Tenenbaum, and Armando Solar-Lezama. Representing partial programs with blended
    abstract semantics. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=mCtadqIxOJ](https://openreview.net/forum?id=mCtadqIxOJ).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye等人（2021）Maxwell Nye、Yewen Pu、Matthew Bowers、Jacob Andreas、Joshua B. Tenenbaum和Armando
    Solar-Lezama。用混合抽象语义表示部分程序。在*国际学习表征会议*上，2021年。URL [https://openreview.net/forum?id=mCtadqIxOJ](https://openreview.net/forum?id=mCtadqIxOJ)。
- en: 'Paranjape et al. (2023) Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh
    Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step
    reasoning and tool-use for large language models. *arXiv preprint arXiv:2303.09014*,
    2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paranjape等人（2023）Bhargavi Paranjape、Scott Lundberg、Sameer Singh、Hannaneh Hajishirzi、Luke
    Zettlemoyer和Marco Tulio Ribeiro。Art：大型语言模型的自动化多步骤推理和工具使用。*arXiv预印本 arXiv:2303.09014*，2023年。
- en: 'Parisi et al. (2022) Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented
    language models. *arXiv preprint arXiv:2205.12255*, 2022.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisi等人（2022）Aaron Parisi、Yao Zhao和Noah Fiedel。Talm：工具增强的语言模型。*arXiv预印本 arXiv:2205.12255*，2022年。
- en: Parisotto et al. (2017) Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh,
    Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis.
    In *International Conference on Learning Representations*, 2017. URL [https://openreview.net/forum?id=rJ0JwFcex](https://openreview.net/forum?id=rJ0JwFcex).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parisotto等人（2017）Emilio Parisotto、Abdel rahman Mohamed、Rishabh Singh、Lihong
    Li、Dengyong Zhou和Pushmeet Kohli。神经符号程序合成。在*国际学习表征会议*上，2017年。URL [https://openreview.net/forum?id=rJ0JwFcex](https://openreview.net/forum?id=rJ0JwFcex)。
- en: 'Patil et al. (2023) Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E
    Gonzalez. Gorilla: Large language model connected with massive apis. *arXiv preprint
    arXiv:2305.15334*, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil等人（2023）Shishir G Patil、Tianjun Zhang、Xin Wang和Joseph E Gonzalez。Gorilla：与大量API连接的大型语言模型。*arXiv预印本
    arXiv:2305.15334*，2023年。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. *arXiv preprint arXiv:2307.16789*,
    2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等人（2023）Yujia Qin、Shihao Liang、Yining Ye、Kunlun Zhu、Lan Yan、Yaxi Lu、Yankai
    Lin、Xin Cong、Xiangru Tang、Bill Qian等。Toolllm：促进大型语言模型掌握16000+实际API。*arXiv预印本 arXiv:2307.16789*，2023年。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2018）Alec Radford、Karthik Narasimhan、Tim Salimans、Ilya Sutskever等。通过生成预训练提高语言理解。2018年。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2019）Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya
    Sutskever等。语言模型是无监督的多任务学习者。*OpenAI博客*，1(8):9，2019年。
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick等人（2023）Timo Schick、Jane Dwivedi-Yu、Roberto Dessì、Roberta Raileanu、Maria
    Lomeli、Luke Zettlemoyer、Nicola Cancedda和Thomas Scialom。Toolformer：语言模型可以自我学习使用工具。*arXiv预印本
    arXiv:2302.04761*，2023年。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn等人（2023）Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik R Narasimhan和Shunyu
    Yao。Reflexion：带有语言强化学习的语言代理。在*第37届神经信息处理系统会议*上，2023年。
- en: 'Su et al. (2022) Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu,
    Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder,
    any task: Instruction-finetuned text embeddings. *arXiv preprint arXiv:2212.09741*,
    2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2022) Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu,
    Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, 和 Tao Yu。一个嵌入器，任何任务：指令微调文本嵌入。*arXiv
    预印本 arXiv:2212.09741*，2022年。
- en: 'Tang et al. (2023) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao
    Liang, and Le Sun. Toolalpaca: Generalized tool learning for language models with
    3000 simulated cases. *arXiv preprint arXiv:2306.05301*, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2023) Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao
    Liang, 和 Le Sun。Toolalpaca：针对语言模型的广义工具学习，使用 3000 个模拟案例。*arXiv 预印本 arXiv:2306.05301*，2023年。
- en: 'Xu et al. (2023) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. Lemur: Harmonizing
    natural language and code for language agents. *arXiv preprint arXiv:2310.06830*,
    2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023) Yiheng Xu, Hongjin Su, Chen Xing, Boyu Mi, Qian Liu, Weijia
    Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, 等。Lemur：为语言代理协调自然语言和代码。*arXiv
    预印本 arXiv:2310.06830*，2023年。
- en: 'Yang et al. (2023) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction,
    2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu
    Li, 和 Ying Shan。Gpt4tools：通过自我指导教导大型语言模型使用工具，2023年。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao。React：在语言模型中协同推理和行动。*arXiv 预印本 arXiv:2210.03629*，2022年。
- en: Zan et al. (2022) Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and
    Jian-Guang Lou. When language model meets private library. *arXiv preprint arXiv:2210.17236*,
    2022.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zan et al. (2022) Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, 和
    Jian-Guang Lou。当语言模型遇上私有库。*arXiv 预印本 arXiv:2210.17236*，2022年。
- en: Zhang et al. (2023a) Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing
    Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive
    math reasoning, 2023a.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023a) Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing
    Sha, Shijin Wang, 和 Ji-Rong Wen。评估和改进工具增强计算密集型数学推理，2023a。
- en: 'Zhang et al. (2023b) Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing
    Zheng. Reverse chain: A generic-rule for llms to master multi-api planning. *arXiv
    preprint arXiv:2310.04474*, 2023b.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023b) Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, 和 Jing Zheng。Reverse
    chain：一个通用规则，帮助 llms 掌握多 api 规划。*arXiv 预印本 arXiv:2310.04474*，2023b。
- en: 'Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. Codegeex: A pre-trained
    model for code generation with multilingual benchmarking on humaneval-x. In *Proceedings
    of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pp. 
    5673–5684, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, 等。Codegeex：一个用于代码生成的预训练模型，具有多语言基准测试在
    humaneval-x 中。在 *第 29 届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第 5673–5684 页，2023年。
- en: Zhou et al. (2023a) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning
    in language models. *arXiv preprint arXiv:2310.04406*, 2023a.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023a) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    和 Yu-Xiong Wang。语言代理树搜索统一了语言模型中的推理、行动和规划。*arXiv 预印本 arXiv:2310.04406*，2023a。
- en: Zhou et al. (2023b) Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng
    Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging
    math word problems using gpt-4 code interpreter with code-based self-verification.
    *arXiv preprint arXiv:2308.07921*, 2023b.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023b) Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng
    Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, 等。使用 gpt-4 代码解释器与基于代码的自我验证解决具有挑战性的数学文字问题。*arXiv
    预印本 arXiv:2308.07921*，2023b。
- en: 'Zhuang et al. (2023a) Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain*: Efficient
    action space navigation in large language models with a* search, 2023a.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2023a) Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, 和 Chao Zhang。Toolchain*：使用 a* 搜索在大型语言模型中高效地导航行动空间，2023a。
- en: 'Zhuang et al. (2023b) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao
    Zhang. Toolqa: A dataset for llm question answering with external tools, 2023b.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2023b) Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, 和 Chao
    Zhang。Toolqa：一个用于 llm 外部工具问答的数据集，2023b。
- en: Appendix A Appendix
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Prompts
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 提示
- en: CodeSynth prompt for function signature generation
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: CodeSynth 用于生成函数签名的提示
- en: '[PRE0]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: TOPGUN prompt for code-based plan generation
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: TOPGUN 用于基于代码的计划生成提示
- en: '[PRE1]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Function Call Prompt for verification
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 用于验证的函数调用提示
- en: '[PRE2]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Self-Reflection Prompt
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思提示
- en: '[PRE3]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: CodeSynth prompt for function signature generation on PrivateEval
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: CodeSynth 用于 PrivateEval 的函数签名生成提示
- en: '[PRE4]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: TOPGUN prompt for code-based plan generation on ToolBench
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: TOPGUN 用于 ToolBench 上的基于代码的计划生成提示
- en: '[PRE5]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Prompt for query generation for PrivateEval
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 PrivateEval 的查询生成提示
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A.2 ToolBench for Gray Box Evaluation
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 ToolBench 用于灰箱评估
- en: 'ToolBench is a diverse benchmark spanning over 16k APIs across 49 categories
    from RapidAPI Hub. It consists of three sets of instructions for tool augmentation
    evaluation: (1) Single-tool instruction (I1), (2) Intra-category multi-tool instruction
    (I2), and (3) Intra-collection multi-tool instructions. Such a rich set of APIs
    and instructions makes it a perfect ground to test our pipeline. ToolBench proposes
    ToolEval containing the evaluation procedure for this set of instructions. ToolEval
    designs two evaluation metrics using ChatGPT: (1) Pass Rate, calculated by the
    proportion of instructions completed within a limited budget; (2) Win Rate, measured
    by asking a ChatGPT evaluator to select its preference for two solution paths.
    We focus on Win Rate for the evaluation metric to draw comparisons between TOPGUN
    and other gray box approaches such as DFSDT and ReAct. ToolEval uses a tree-based
    representation of the responses to generate solution paths, which are then compared
    to calculate the win rate.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ToolBench 是一个多样化的基准，涵盖了来自 RapidAPI Hub 的 49 个类别中的 16k 多个 API。它由三组工具增强评估的指令组成：(1)
    单工具指令 (I1)，(2) 类内多工具指令 (I2)，和 (3) 集合内多工具指令。这种丰富的 API 和指令集使其成为测试我们管道的理想场所。ToolBench
    提出了 ToolEval，包括该指令集的评估程序。ToolEval 使用 ChatGPT 设计了两个评估指标：(1) 通过率，通过在有限预算内完成的指令比例计算；(2)
    胜率，通过要求 ChatGPT 评估者选择对两个解决方案路径的偏好来测量。我们专注于胜率作为评估指标，以比较 TOPGUN 和其他灰箱方法，如 DFSDT
    和 ReAct。ToolEval 使用基于树的响应表示生成解决方案路径，然后进行比较以计算胜率。
- en: ToolEval response representation
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ToolEval 响应表示
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We ensure that the code plan generated by TOPGUN precisely aligns with this
    representation to harness ToolEval for win rate calculation. In our black-box
    inference phase, we lack the final answer and tool responses. However, we retrieve
    these values during gray-box evaluation involving actual API calls and populate
    the representation accordingly.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确保 TOPGUN 生成的代码计划与此表示形式精确对齐，以利用 ToolEval 进行胜率计算。在我们的黑箱推理阶段，我们缺乏最终答案和工具响应。然而，我们会在涉及实际
    API 调用的灰箱评估中检索这些值，并相应地填充表示形式。
- en: Black Box Inference output
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱推理输出
- en: '[PRE8]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Gray Box Evaluation output
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 灰箱评估输出
- en: '[PRE9]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We input the solution path representations from TOPGUN and other approaches
    into ToolEval’s preference test to compute the win rate for each query. These
    win rates are then averaged across different sets of instructions to determine
    the average win rate.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将来自 TOPGUN 和其他方法的解决方案路径表示输入 ToolEval 的偏好测试，以计算每个查询的胜率。这些胜率然后在不同的指令集之间平均，以确定平均胜率。
- en: A.3 PrivateEval Dataset
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 PrivateEval 数据集
- en: Here, we list some examples of tools and queries that we created for PrivateEval.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们列出了一些为 PrivateEval 创建的工具和查询示例。
- en: A.3.1 Tools
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.1 工具
- en: Moneky and BeatNum
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Moneky 和 BeatNum
- en: '[PRE10]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Llama Hub
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Llama Hub
- en: '[PRE11]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A.3.2 Queries Example
  id: totrans-211
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.2 查询示例
- en: '1.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: '[PRE12]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '2.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: '[PRE13]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '3.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: '[PRE14]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '4.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: '[PRE15]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '5.'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: '[PRE16]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '6.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: '[PRE17]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '7.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: '[PRE18]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '8.'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: '[PRE19]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '9.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: '[PRE20]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '10.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: '[PRE21]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '11.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: '[PRE22]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '12.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: '[PRE23]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '13.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: '[PRE24]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '14.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '14.'
- en: '[PRE25]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A.4 CodeSynth Examples
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 CodeSynth 示例
- en: Examples of function signatures and calls generated by CodeSynth while evaluating
    with HumanEval-X and PrivateEval datasets.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由 CodeSynth 生成的函数签名和调用示例，在使用 HumanEval-X 和 PrivateEval 数据集进行评估时。
- en: A.4.1 HumanEval-X
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.1 HumanEval-X
- en: (a)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: '[PRE26]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'def  intersperse(numbers:  List[int],  delimeter:  int)  ->  List[int]:"""Args:numbers  (List[int]):  A  list  of  integersdelimeter  (int):  An  integer  to  be  inserted  between  everytwo  consecutive  elements  of  the  inputlist\parReturns:List[int]:  A  new  list  with  the  delimeter  inserted  betweenevery  two  consecutive  elements  of  the  inputlist"""return  [0]  #  Dummy  return  object\par\par#  Function  Call:from  typing  import  List\parnumbers  =  [1,  2,  3]delimeter  =  4intersperse(numbers,  delimeter)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def intersperse(numbers: List[int], delimeter: int) -> List[int]:"""Args:numbers
    (List[int]): 一个整数列表delimeter (int): 要插入到每两个连续元素之间的整数list\parReturns:List[int]:
    一个新列表，插入了分隔符在输入列表的每两个连续元素之间"""return [0]  # 虚拟返回对象\par\par# 函数调用:from typing import
    List\parnumbers = [1, 2, 3]delimeter = 4intersperse(numbers, delimeter)'
- en: (b)
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: '[PRE27]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'def  pairs_sum_to_zero(l:  List[int])  ->  bool:"""Args:l  (List[int]):  A  list  of  integers  as  an  input.\parReturns:bool:  True  if  there  are  two  distinct  elements  inthe  list  that  sum  to  zero,  and  False  otherwise."""return  False\par\par#  Function  Call:l  =  [2,  4,  -5,  3,  5,  7]pairs_sum_to_zero(l)\par'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  pairs_sum_to_zero(l:  List[int])  ->  bool:"""参数:l  (List[int]):  输入的整数列表。\par返回:bool:  如果列表中有两个不同的元素之和为零，则返回True，否则返回False。"""return  False\par\par#  函数调用:l  =  [2,  4,  -5,  3,  5,  7]pairs_sum_to_zero(l)\par'
- en: (c)
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: '[PRE28]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'def  vowels_count(s:  str)  ->  int:"""Args:s  (str):  A  string  representing  a  word.\parReturns:int:  The  number  of  vowels  in  the  string."""return  0\par\par#  Function  Call:s  =  "example"vowels_count(s)'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  vowels_count(s:  str)  ->  int:"""参数:s  (str):  表示一个单词的字符串。\par返回:int:  字符串中元音字母的数量。"""return  0\par\par#  函数调用:s  =  "example"vowels_count(s)'
- en: (d)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: '[PRE29]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'def  prod_signs(arr:  List[int])  ->  Union[int,  None]:"""Args:arr  (List[int]):  An  array  of  integers.\parReturns:Union[int,  None]:  The  sum  of  magnitudes  of  integersmultiplied  by  the  product  of  all  signsof  each  number  in  the  array,  representedby  1,  -1  or  0.  Returns  None  for  empty  arr.\par"""return  0  #  Dummy  return  object\par\par#  Function  Call:from  typing  import  List,  Union\pararr  =  [1,  2,  2,  -4]prod_signs(arr)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  prod_signs(arr:  List[int])  ->  Union[int,  None]:"""参数:arr  (List[int]):  一个整数数组。\par返回:Union[int,  None]:  整数的绝对值之和乘以数组中每个数字的符号的乘积，符号由1、-1或0表示。对于空数组返回None。\par"""return  0  #  虚拟返回对象\par\par#  函数调用:from  typing  import  List,  Union\pararr  =  [1,  2,  2,  -4]prod_signs(arr)'
- en: (e)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (e)
- en: '[PRE30]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'def  will_it_fly(q:  List[int],  w:  int)  ->  bool:"""Args:q  (List[int]):  A  list  of  integers  representing  theobject’s  weight  distribution.w  (int):  The  maximum  possible  weight  for  the  object  to  fly.\parReturns:bool:  True  if  the  object  will  fly,  False  otherwise."""return  True  #  Dummy  return\par\par#  Function  Call:from  typing  import  List\parq  =  [3,  2,  3]w  =  9will_it_fly(q,  w)'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  will_it_fly(q:  List[int],  w:  int)  ->  bool:"""参数:q  (List[int]):  一个整数列表，表示物体的重量分布。w  (int):  物体飞行的最大可能重量。\par返回:bool:  如果物体会飞行则返回True，否则返回False。"""return  True  #  虚拟返回\par\par#  函数调用:from  typing  import  List\parq  =  [3,  2,  3]w  =  9will_it_fly(q,  w)'
- en: A.4.2 PrivateEval
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.2 PrivateEval
- en: (a)
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: '[PRE31]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'def  stats_analysis(knowledgeframe):"""Performs  various  statistical  analyses  on  a  KnowledgeFrameand  returns  a  new  KnowledgeFrame  containing  the  results.\parArgs:knowledgeframe  (KnowledgeFrame):  The  KnowledgeFrameon  which  statistical  analysis  will  be  performed.\parReturns:KnowledgeFrame:  A  KnowledgeFrame  containing  thestatistical  analysis  results."""return  KnowledgeFrame()  #  Dummy  return  object\par\par#  Function  Call:from  monkey  import  KnowledgeFrame\par#  Dummy  data  for  the  KnowledgeFramedata  =  {’column1’:  [1,  2,  3],’column2’:  [4,  5,  6],’column3’:  [7,  8,  9]}\par#  Create  a  dummy  KnowledgeFrameknowledgeframe  =  KnowledgeFrame(data)\par#  Call  the  stats_analysis  function  with  the  dummy  KnowledgeFrameresult  =  stats_analysis(knowledgeframe)'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  stats_analysis(knowledgeframe):"""对一个KnowledgeFrame进行各种统计分析，并返回一个包含结果的新KnowledgeFrame。\par参数:knowledgeframe  (KnowledgeFrame):  进行统计分析的KnowledgeFrame。\par返回:KnowledgeFrame:  包含统计分析结果的KnowledgeFrame。"""return  KnowledgeFrame()  #  虚拟返回对象\par\par#  函数调用:from  monkey  import  KnowledgeFrame\par#  KnowledgeFrame的虚拟数据data  =  {’column1’:  [1,  2,  3],’column2’:  [4,  5,  6],’column3’:  [7,  8,  9]}\par#  创建一个虚拟的KnowledgeFrameknowledgeframe  =  KnowledgeFrame(data)\par#  使用虚拟的KnowledgeFrame调用stats_analysis函数result  =  stats_analysis(knowledgeframe)'
- en: (b)
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: '[PRE32]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'def  knowledge_summary(knowledgeframe,  columns,  stats_analysis):"""Summarizes  a  KnowledgeFrame  based  on  specified  columns  and  statistical  analysis  results.\parArgs:knowledgeframe  (KnowledgeFrame):  The  KnowledgeFrame  tobe  summarized.\parcolumns  (List[str]):  The  list  of  column  names  to  includein  the  summary.\parstats_analysis  (Dict[str,  Any]):  The  dictionary  containingstatistical  analysis  results  for  the  specified  columns.\parReturns:dict:  A  summary  dictionary  containing  information  aboutthe  specified  columns  and  their  statistical  analysis."""return  {"dummy_key":  "dummy_value"}\par\par#  Function  Call:\par#  Dummy  function  call  for  knowledge_summaryknowledgeframe  =  {"dummy_key":  "dummy_value"}columns  =  ["column1",  "column2"]stats_analysis  =  {"column1":  {"mean":  5,  "median":  4},  "column2":  {"mean":  10,  "median":  8}}\parknowledge_summary(knowledgeframe,  columns,  stats_analysis)'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  知识总结(knowledgeframe,  columns,  stats_analysis):"""根据指定的列和统计分析结果总结KnowledgeFrame。\par参数:knowledgeframe  (KnowledgeFrame):  需要总结的KnowledgeFrame。\parcolumns  (List[str]):  要包括在总结中的列名列表。\parstats_analysis  (Dict[str,  Any]):  包含指定列的统计分析结果的字典。\par返回:dict:  一个包含指定列及其统计分析信息的总结字典。"""return  {"dummy_key":  "dummy_value"}\par\par#  函数调用:\par#  对knowledge_summary的虚拟函数调用knowledgeframe  =  {"dummy_key":  "dummy_value"}columns  =  ["column1",  "column2"]stats_analysis  =  {"column1":  {"mean":  5,  "median":  4},  "column2":  {"mean":  10,  "median":  8}}\parknowledge_summary(knowledgeframe,  columns,  stats_analysis)'
- en: (c)
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: '[PRE33]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'def  to_grayscale(image_array):"""Grayscale  function  takes  an  image  array  as  input  andconverts  it  into  grayscale.\parArgs:image_array  (beatnum.bdnumset):  Input  image  array  tobe  converted  to  grayscale.\parReturns:beatnum.bdnumset:  Grayscale  image  array."""dummy_shape  =  (1,  1)  #  Dummy  shape  for  the  bdnumsetreturn  beatnum.bdnumset(dummy_shape)\par\par#  Function  Call:from  beatnum  import  bdnumset\par#  Dummy  image_arraydummy_shape  =  (1,  1)  #  Dummy  shape  for  the  bdnumsetimage_array  =  bdnumset(dummy_shape)\par\par#  Function  callto_grayscale(image_array)'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  转为灰度图像(image_array):"""灰度函数接收一个图像数组作为输入，并将其转换为灰度图像。\par参数:image_array  (beatnum.bdnumset):  需要转换为灰度图像的输入图像数组。\par返回:beatnum.bdnumset:  灰度图像数组。"""dummy_shape  =  (1,  1)  #  用于bdnumset的虚拟形状return  beatnum.bdnumset(dummy_shape)\par\par#  函数调用:from  beatnum  import  bdnumset\par#  虚拟图像数组dummy_shape  =  (1,  1)  #  用于bdnumset的虚拟形状image_array  =  bdnumset(dummy_shape)\par\par#  函数调用to_grayscale(image_array)'
- en: (d)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: '[PRE34]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'def  flip(image_array,  axis=1):"""Flip  function  takes  an  image  array  as  input  and  flipsit  along  the  specified  axis.\parArgs:image_array  (beatnum.bdnumset):  Input  image  array  to  beflipped.\paraxis  (int,  optional):  Axis  along  which  to  flip  the  imagearray.  Default  is  1  (horizontal  flip).\parReturns:beatnum.bdnumset:  Flipped  image  array."""dummy_shape  =  image_array.shapereturn  beatnum.bdnumset(shape=dummy_shape)\par#  Function  Call:import  beatnum  as  bn\parimage_array  =  bn.bdnumset(shape=(2,  2),  dtype=float,  order=’F’)axis  =  1\parflip(image_array,  axis)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def  翻转(image_array,  axis=1):"""翻转函数接收一个图像数组作为输入，并在指定的轴上翻转图像。\par参数:image_array  (beatnum.bdnumset):  需要翻转的输入图像数组。\paraxis  (int,  optional):  要翻转图像数组的轴。默认值是1（水平翻转）。\par返回:beatnum.bdnumset:  翻转后的图像数组。"""dummy_shape  =  image_array.shapereturn  beatnum.bdnumset(shape=dummy_shape)\par#  函数调用:import  beatnum  as  bn\parimage_array  =  bn.bdnumset(shape=(2,  2),  dtype=float,  order=’F’)axis  =  1\parflip(image_array,  axis)'
- en: (e)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (e)
- en: '[PRE35]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'def  translate(text:  str,  language:  str)  ->  str:"""Translates  text  from  one  language  to  another.  The  sourcelanguage  will  be  automatically  detected.You  need  to  specifythe  target  language  using  a  two  character  language  code.\parArgs:text  (str):  Text  to  be  translated\parlanguage  (str):  Target  translation  language.  One  ofaf,  sq,  am,  ar,  hy,  as,  az,  bn,  ba,  eu,  bs,  bg,  ca,hr,  cs,  da,  dv,  nl,  en,  et,  fo,  fj,  fi,  fr,  gl,  ka,de,  el,  gu,  ht,  he,  hi,  hu,  is,  id,  iu,  ga,  it,  ja,kn,  kk,  km,  ko,  ku,  ky,  lo,  lv,  lt,  mk,  mg,  ms,  ml,mt,  mi,  mr,  my,  ne,  nb,  or,  ps,  fa,  pl,  pt,  pa,  to,ru,  sm,  sk,  sl,  so,  es,  sw,  sv,  ty,  ta,  tt,  te,  the,bo,  ti,  to,  tr,  tk,  uk,  ur,  ug,  uz,  vi,  cy,  zu\parReturns:str:  Translated  text"""return  "dummy_translated_text"\par\par#  Function  Call:text  =  "Hello,  how  are  you?"language  =  "fr"translate(text,  language)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def translate(text: str, language: str) -> str:"""将文本从一种语言翻译成另一种语言。源语言将自动检测。您需要使用两个字符的语言代码指定目标语言。\par参数：text
    (str): 要翻译的文本\parlanguage (str): 目标翻译语言。以下之一：af, sq, am, ar, hy, as, az, bn, ba,
    eu, bs, bg, ca, hr, cs, da, dv, nl, en, et, fo, fj, fi, fr, gl, ka, de, el, gu,
    ht, he, hi, hu, is, id, iu, ga, it, ja, kn, kk, km, ko, ku, ky, lo, lv, lt, mk,
    mg, ms, ml, mt, mi, mr, my, ne, nb, or, ps, fa, pl, pt, pa, to, ru, sm, sk, sl,
    so, es, sw, sv, ty, ta, tt, te, th, bo, ti, to, tr, tk, uk, ur, ug, uz, vi, cy,
    zu\par返回：str: 翻译后的文本"""return "dummy_translated_text"\par\par# 函数调用：text = "Hello,
    how are you?"language = "fr"translate(text, language)'
- en: A.5 TOPGUN Examples
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 TOPGUN 示例
- en: Examples of code-based plans generated by our proposed planning approach TOPGUN,
    as evaluated on ToolBench and PrivateEval datasets.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的规划方法 TOPGUN 生成的基于代码的计划示例，经过 ToolBench 和 PrivateEval 数据集的评估。
- en: A.5.1 ToolBench
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.1 ToolBench
- en: (a)
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: '[PRE36]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '#  Import  the  required  librariesimport  requests\par#  Use  the  api_tracking  function  to  track  the  package  with#  the  Pack  &  Send  reference  numberreference_number  =  ’ReferenceNumberHere’package_tracking_info  =  api_tracking(reference_number)\par#  Use  the  latest  function  to  get  the  latest  status  of  the#  package  with  colis  IDcolis_id  =  ’CA107308006SI’latest_status  =  latest(colis_id)\par#  The  package_tracking_info  and  latest_status  variables#  contain  the  required  information'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 导入所需的库import requests\par# 使用 api_tracking 函数通过# Pack & Send 参考编号追踪包裹reference_number
    = ’ReferenceNumberHere’package_tracking_info = api_tracking(reference_number)\par#
    使用 latest 函数获取包裹的最新状态# 使用 colis IDcolis_id = ’CA107308006SI’latest_status = latest(colis_id)\par#
    package_tracking_info 和 latest_status 变量# 包含所需的信息'
- en: (b)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: '[PRE37]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'import  requests\par#  Get  the  latest  arrivals  from  different  platformsnew_arrivals_data  =  new_arrivals(region=’US’)\par#  Initialize  an  empty  list  to  store  the  movie  detailsmovie_details  =  []\par#  Iterate  through  the  new  arrivals  datafor  movie  in  new_arrivals_data.get(’result’,  []):#  Get  the  IMDb  ID  of  the  movieimdb_id  =  movie.get(’imdbid’,  ’’)\par#  Get  the  basic  information  of  the  movie  using  the  IMDb  IDtitle_data  =  title_details(imdbid=imdb_id)\par#  Extract  the  required  information  from  the  title  datamovie_title  =  title_data.get(’title’,  ’’)streaming_platforms  =  title_data.get(’platforms’,  {})genres  =  title_data.get(’genre’,  ’’)\par#  Append  the  movie  details  to  the  movie_details  listmovie_details.append({’title’:  movie_title,’streaming_platforms’:  streaming_platforms,’genres’:  genres})\par#  The  movie_details  list  now  contains  the  new  arrivals#  along  with  their  streaming  platforms  and  genres'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'import requests\par# 从不同平台获取最新到货new_arrivals_data = new_arrivals(region=’US’)\par#
    初始化一个空列表以存储电影详细信息movie_details = []\par# 遍历最新到货的数据for movie in new_arrivals_data.get(’result’,
    []):# 获取电影的 IMDb IDimdb_id = movie.get(’imdbid’, ’’)\par# 使用 IMDb ID 获取电影的基本信息title_data
    = title_details(imdbid=imdb_id)\par# 从标题数据中提取所需的信息movie_title = title_data.get(’title’,
    ’’)streaming_platforms = title_data.get(’platforms’, {})genres = title_data.get(’genre’,
    ’’)\par# 将电影详细信息添加到 movie_details 列表中movie_details.append({’title’: movie_title,’streaming_platforms’:
    streaming_platforms,’genres’: genres})\par# movie_details 列表现在包含了新到货的电影# 以及它们的流媒体平台和类型'
- en: (c)
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: '[PRE38]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: import  requests\par#  Search  for  videos  related  to  ’action’  on  Vimeoaction_videos  =  searchvideos(format=’json’,  query=’action’,  sort=’relevant’)\par#  Fetch  the  related  people  in  the  ’movies’  categoryrelated_people  =  getrelatedpeople(category=’movies’,  format=’json’)\par#  Provide  a  streaming  link  for  a  YouTube  video  with  the  ID#  ’UxxajLWwzqY’youtube_streaming_link  =  download_stream(is_id=’UxxajLWwzqY’)
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import  requests\par#  在  Vimeo  上  搜索  与  ’动作’  相关的视频action_videos  =  searchvideos(format=’json’,  query=’action’,  sort=’relevant’)\par#  获取  ’电影’  类别  相关  人物related_people  =  getrelatedpeople(category=’movies’,  format=’json’)\par#  提供  YouTube  视频  的  流媒体  链接  其  ID  为  ’UxxajLWwzqY’youtube_streaming_link  =  download_stream(is_id=’UxxajLWwzqY’)
- en: (d)
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: '[PRE39]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'import  requests\par#  Use  the  search  function  to  find  top-rated  fitness  trackers#  on  Amazonsearch_results  =  search(type=’search’,  search_term=’fitness  tracker’,  amazon_domain=’amazon.com’,  sort_by=’average_review’,  exclude_sponsored=True)\par#  Extract  the  top  5  fitness  trackers  from  the  search  resultstop_5_fitness_trackers  =  search_results.get(’results’,  [])[:5]\par#  Get  the  ASINs  of  the  top  5  fitness  trackerstop_5_asins  =  [tracker.get(’asin’,  ’’)  for  tracker  in  top_5_fitness_trackers]\par#  Retrieve  the  product  details  for  each  of  the  top  5  fitness#  trackerstop_5_product_details  =  [product(type=’product’,  asin=asin,  amazon_domain=’amazon.com’)  for  asin  in  top_5_asins]\par#  Extract  the  features  and  prices  of  the  top  5  fitness  trackerstop_5_features_and_prices  =  []for  product_detail  in  top_5_product_details:try:features  =  product_detail.get(’features’,  [])except:features  =  []price  =  product_detail.get(’price’,  {}).get(’value’,  ’N/A’)top_5_features_and_prices.append({’features’:  features,  ’price’:  price})\par#  The  top_5_features_and_prices  variable  contains  the  features#  and  prices  of  the  top  5  fitness  trackers  on  Amazon'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'import  requests\par#  使用  搜索  功能  查找  Amazon  上  顶级  评分  的  健身追踪器search_results  =  search(type=’search’,  search_term=’fitness  tracker’,  amazon_domain=’amazon.com’,  sort_by=’average_review’,  exclude_sponsored=True)\par#  从  搜索  结果中  提取  前  5  款  健身追踪器top_5_fitness_trackers  =  search_results.get(’results’,  [])[:5]\par#  获取  前  5  款  健身追踪器  的  ASINs  码top_5_asins  =  [tracker.get(’asin’,  ’’)  for  tracker  in  top_5_fitness_trackers]\par#  检索  每个  前  5  款  健身追踪器  的  产品  详细信息top_5_product_details  =  [product(type=’product’,  asin=asin,  amazon_domain=’amazon.com’)  for  asin  in  top_5_asins]\par#  提取  前  5  款  健身追踪器  的  特性  和  价格top_5_features_and_prices  =  []for  product_detail  in  top_5_product_details:try:features  =  product_detail.get(’features’,  [])except:features  =  []price  =  product_detail.get(’price’,  {}).get(’value’,  ’N/A’)top_5_features_and_prices.append({’features’:  features,  ’price’:  price})\par#  top_5_features_and_prices  变量  包含  Amazon  上  前  5  款  健身追踪器  的  特性  和  价格'
- en: (e)
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (e)
- en: '[PRE40]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: import  requests\par#  Fetch  cryptocurrency  datacrypto_data  =  prices_and_up_and_down()\par#  Fetch  subregions  of  North  Americasubregions_data  =  get_sub_regions(’North  America’)\par#  Accessing  specific  cryptocurrency  databitcoin_data  =  crypto_data.get(’Bitcoin’,  {})ethereum_data  =  crypto_data.get(’Ethereum’,  {})stellar_data  =  crypto_data.get(’Stellar’,  {})\par#  Accessing  subregions  of  North  Americatry:north_america_subregions  =  subregions_data.get(’subregions’,  [])except:north_america_subregions  =  []\par#  You  can  now  analyze  the  cryptocurrency  data  and  plan  your#  trip  to  North  America  using  the  subregions  information.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: import  requests\par#  获取  加密货币  数据crypto_data  =  prices_and_up_and_down()\par#  获取  北美  的  子区域subregions_data  =  get_sub_regions(’North  America’)\par#  访问  特定  加密货币  数据bitcoin_data  =  crypto_data.get(’Bitcoin’,  {})ethereum_data  =  crypto_data.get(’Ethereum’,  {})stellar_data  =  crypto_data.get(’Stellar’,  {})\par#  访问  北美  的  子区域try:north_america_subregions  =  subregions_data.get(’subregions’,  [])except:north_america_subregions  =  []\par#  现在  你  可以  分析  加密货币  数据  并且  利用  子区域  信息  计划  你的  北美  之行。
- en: (f)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (f)
- en: '[PRE41]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: import  requests\par#  Search  for  landscape  drawing  tutorial  videossearch_result  =  search(’landscape  drawing  tutorial’,  type=’video’,  safesearch=True)\par#  Find  the  most  viewed  videomost_viewed_video  =  Nonemax_views  =  0for  video  in  search_result.get(’items’,  []):views  =  int(video.get(’statistics’,  {}).get(’viewCount’,  ’0’))if  views  >  max_views:max_views  =  viewsmost_viewed_video  =  video\par#  Get  the  video  detailsvideo_id  =  most_viewed_video.get(’id’,  ’video_id_example’)video_details  =  video(video_id)\par#  Get  the  channel  detailschannel_id  =  most_viewed_video.get(’snippet’,  {}).get(’channelId’,  ’channel_id_example’)channel_details  =  video(channel_id)\par#  The  video_details  and  channel_details  variables  contain  the#  required  information
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'import requests \par # 搜索风景绘画教程视频 search_result = search(’landscape drawing
    tutorial’, type=’video’, safesearch=True) \par # 找到观看次数最多的视频 most_viewed_video
    = None max_views = 0 for video in search_result.get(’items’, []): views = int(video.get(’statistics’,
    {}).get(’viewCount’, ’0’)) if views > max_views: max_views = views most_viewed_video
    = video \par # 获取视频详细信息 video_id = most_viewed_video.get(’id’, ’video_id_example’)
    video_details = video(video_id) \par # 获取频道详细信息 channel_id = most_viewed_video.get(’snippet’,
    {}).get(’channelId’, ’channel_id_example’) channel_details = video(channel_id)
    \par # video_details 和 channel_details 变量包含所需的信息'
- en: A.5.2 PrivateEval
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.2 PrivateEval
- en: (a)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: '[PRE42]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '#  Load  the  multilingual  dataset  (assuming  it’s  a  CSV  file)csv_file  =  ’path/to/your/multilingual_dataset.csv’knowledgeframe  =  load_csv(csv_file)\par#  Extract  the  French  column  (assuming  it’s  named  ’french_column’)french_column  =  knowledgeframe(’french_column’)\par#  Translate  the  French  column  to  Englishtranslated_column  =  []for  text  in  french_column:translated_text  =  translate(text,  language=’en’)translated_column.append(translated_text)\par#  Add  the  translated  column  to  the  KnowledgeFrameknowledgeframe[’english_column’]  =  translated_column\par#  Perform  statistical  analysis  on  the  translated  columnanalysis_kf  =  stats_analysis(knowledgeframe)'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 加载多语言数据集（假设为CSV文件）csv_file = ’path/to/your/multilingual_dataset.csv’ knowledgeframe
    = load_csv(csv_file) \par # 提取法语列（假设其名称为 ’french_column’）french_column = knowledgeframe(’french_column’)
    \par # 将法语列翻译成英语 translated_column = [] for text in french_column: translated_text
    = translate(text, language=’en’) translated_column.append(translated_text) \par
    # 将翻译后的列添加到KnowledgeFrame knowledgeframe[’english_column’] = translated_column
    \par # 对翻译后的列进行统计分析 analysis_kf = stats_analysis(knowledgeframe)'
- en: (b)
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: '[PRE43]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '#  Load  the  CSV  file  containing  the  survey  datasurvey_data  =  load_csv(’baby_food_preferences.csv’)\par#  Perform  statistical  analysis  on  the  survey  dataanalysis_results  =  stats_analysis(survey_data)\par#  Generate  a  histogram  to  visualize  the  preferences  across  different  age  groupsbuild_hist(analysis_results)\par#  Summarize  the  most  preferred  food  items  in  a  reportcolumns_to_include  =  [’age_group’,  ’food_item’,  ’preference_score’]summary_report  =  knowledge_summary(survey_data,  columns_to_include,  analysis_results)\par#  The  summary_report  variable  now  contains  the  report  summarizing  the  most  preferred  food  items'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 加载包含调查数据的CSV文件 survey_data = load_csv(’baby_food_preferences.csv’) \par #
    对调查数据进行统计分析 analysis_results = stats_analysis(survey_data) \par # 生成直方图以可视化不同年龄组的偏好
    build_hist(analysis_results) \par # 在报告中总结最受欢迎的食物项目 columns_to_include = [’age_group’,
    ’food_item’, ’preference_score’] summary_report = knowledge_summary(survey_data,
    columns_to_include, analysis_results) \par # summary_report 变量现在包含总结最受欢迎食物项目的报告'
- en: (c)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: '[PRE44]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: from  monkey  import  KnowledgeFramefrom  typing  import  List,  Dict,  Union\par#  Search  for  "Chinchilla  LLM"  paper  on  arXivquery  =  "Chinchilla  LLM"papers  =  arxiv_query(query)\par#  Assuming  first  result  is  relevant  load  the  PDF  datapdf_data  =  load_pdf_data(papers[0])\par#  Search  the  image  of  the  table  in  the  "Chinchilla  LLM"  paperimage_url  =  bing_image_search(’Chinchilla  LLM  paper  table’)\par#  Process  the  imageprocessed_image  =  process_image(image_url,  features=[’objects’])\par#  Convert  the  processed  image  to  a  KnowledgeFrameknowledge_frame  =  read_txt(processed_image.get(’objects’,  ’dummy_objects_text’))\par#  Perform  statistical  analysis  on  the  KnowledgeFrameanalysis_kf  =  stats_analysis(knowledge_frame)\par#  Build  a  histogram  based  on  the  analysisbuild_hist(analysis_kf)
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 monkey 导入 KnowledgeFrame从 typing 导入 List, Dict, Union\par# 在 arXiv 上搜索“Chinchilla
    LLM”论文query = "Chinchilla LLM"papers = arxiv_query(query)\par# 假设第一个结果是相关的，加载
    PDF 数据pdf_data = load_pdf_data(papers[0])\par# 在“Chinchilla LLM”论文中搜索表格的图像image_url
    = bing_image_search(’Chinchilla LLM paper table’)\par# 处理图像processed_image = process_image(image_url,
    features=[’objects’])\par# 将处理后的图像转换为 KnowledgeFrameknowledge_frame = read_txt(processed_image.get(’objects’,
    ’dummy_objects_text’))\par# 对 KnowledgeFrame 进行统计分析analysis_kf = stats_analysis(knowledge_frame)\par#
    根据分析结果构建直方图build_hist(analysis_kf)
- en: (d)
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (d)
- en: '[PRE45]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '#  Import  necessary  librariesfrom  typing  import  List,  Dict,  Unionimport  beatnumfrom  PIL  import  Image\par#  Retrieve  images  of  dogsimage_urls  =  bing_image_search(query=’dogs’)\par#  Load  the  first  imageimage  =  process_image(image_urls[0])image_array  =  beatnum.bdnumset(image)\par#  Perform  data  augmentation  using  simple  image  processing  techniquesrotated_image_array  =  rotate(image_array,  direction=’clockwise’)flipped_image_array  =  flip(image_array,  axis=1)cropped_image_array  =  crop(image_array,  scale_factor=0.5)grayscale_image_array  =  to_grayscale(image_array)\par#  creating  image  object  of  above  arrayrotated_image_data  =  Image.fromarray(rotated_image_array)flipped_image_data  =  Image.fromarray(flipped_image_array)cropped_image_data  =  Image.fromarray(cropped_image_array)grayscale_image_data  =  Image.fromarray(grayscale_image_array)\par#  Save  the  augmented  imagesrotated_image_data.save(’rotated_dog_image.png’)flipped_image_data.save(’flipped_dog_image.png’)cropped_image_data.save(’cropped_dog_image.png’)grayscale_image_data.save(’grayscale_dog_image.png’)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '# 导入必要的库从 typing 导入 List, Dict, Union导入 beatnum从 PIL 导入 Image\par# 获取狗的图像image_urls
    = bing_image_search(query=’dogs’)\par# 加载第一张图像image = process_image(image_urls[0])image_array
    = beatnum.bdnumset(image)\par# 使用简单的图像处理技术进行数据增强rotated_image_array = rotate(image_array,
    direction=’clockwise’)flipped_image_array = flip(image_array, axis=1)cropped_image_array
    = crop(image_array, scale_factor=0.5)grayscale_image_array = to_grayscale(image_array)\par#
    创建上述数组的图像对象rotated_image_data = Image.fromarray(rotated_image_array)flipped_image_data
    = Image.fromarray(flipped_image_array)cropped_image_data = Image.fromarray(cropped_image_array)grayscale_image_data
    = Image.fromarray(grayscale_image_array)\par# 保存增强后的图像rotated_image_data.save(’rotated_dog_image.png’)flipped_image_data.save(’flipped_dog_image.png’)cropped_image_data.save(’cropped_dog_image.png’)grayscale_image_data.save(’grayscale_dog_image.png’)'
