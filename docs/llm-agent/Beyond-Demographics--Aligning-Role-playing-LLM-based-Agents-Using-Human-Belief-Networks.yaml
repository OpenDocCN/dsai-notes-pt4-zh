- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:42:55'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:42:55
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief
    Networks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越人口统计学：利用人类信念网络对基于角色扮演的LLM代理进行对齐
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17232](https://ar5iv.labs.arxiv.org/html/2406.17232)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17232](https://ar5iv.labs.arxiv.org/html/2406.17232)
- en: Yun-Shiuan Chuang, Zach Studdiford^†, Krirk Nirunwiroj^†, Agam Goyal
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 云修安·庄，扎克·斯图迪福德^†，克里克·尼伦维洛伊^†，阿甘·戈亚尔
- en: Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文森特·V·弗里戈，杨斯佳，达万·沙，胡俊杰，蒂莫西·T·罗杰斯
- en: University of Wisconsin-Madison
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星大学麦迪逊分校
- en: '{yunshiuan.chuang,studdiford,nirunwiroj,agoyal25}@wisc.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{yunshiuan.chuang,studdiford,nirunwiroj,agoyal25}@wisc.edu'
- en: '{vfrigo, syang84, dshah, junjie.hu, ttrogers}@wisc.edu'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{vfrigo, syang84, dshah, junjie.hu, ttrogers}@wisc.edu'
- en: ^† joint second author
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^† 联合第二作者
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Creating human-like large language model (LLM) agents is crucial for faithful
    social simulation. Having LLMs role-play based on demographic information sometimes
    improves human likeness but often does not. This study assessed whether LLM alignment
    with human behavior can be improved by integrating information from empirically-derived
    human belief networks. Using data from a human survey, we estimated a belief network
    encompassing 18 topics loading on two non-overlapping latent factors. We then
    seeded LLM-based agents with an opinion on one topic, and assessed the alignment
    of its expressed opinions on remaining test topics with corresponding human data.
    Role-playing based on demographic information alone did not align LLM and human
    opinions, but seeding the agent with a single belief greatly improved alignment
    for topics related in the belief network, and not for topics outside the network.
    These results suggest a novel path for human-LLM belief alignment in work seeking
    to simulate and understand patterns of belief distributions in society.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 创建类似人类的大型语言模型（LLM）代理对忠实的社会模拟至关重要。基于人口统计信息进行LLM角色扮演有时能提高人类相似度，但往往效果有限。本研究评估了是否通过整合来自实证得出的信念网络的信息，可以提高LLM与人类行为的一致性。利用人类调查数据，我们估计了一个包含18个主题的信念网络，这些主题加载在两个不重叠的潜在因素上。然后，我们在一个主题上给LLM代理注入了一个观点，并评估了其在剩余测试主题上表达的观点与对应的人类数据的一致性。仅基于人口统计信息的角色扮演并未对齐LLM与人类的观点，但在信念网络中相关的主题上注入一个单一信念显著提高了对齐度，而对于网络之外的主题则没有。这些结果为模拟和理解社会中信念分布模式的工作提供了一条新途径。
- en: 'Beyond Demographics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 超越人口统计学：
- en: Aligning Role-playing LLM-based Agents Using Human Belief Networks
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 利用人类信念网络对基于角色扮演的LLM代理进行对齐
- en: Yun-Shiuan Chuang, Zach Studdiford^†, Krirk Nirunwiroj^†, Agam Goyal Vincent
    V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers University of
    Wisconsin-Madison {yunshiuan.chuang,studdiford,nirunwiroj,agoyal25}@wisc.edu {vfrigo,
    syang84, dshah, junjie.hu, ttrogers}@wisc.edu ^† joint second author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 云修安·庄，扎克·斯图迪福德^†，克里克·尼伦维洛伊^†，阿甘·戈亚尔 文森特·V·弗里戈，杨斯佳，达万·沙，胡俊杰，蒂莫西·T·罗杰斯 威斯康星大学麦迪逊分校
    {yunshiuan.chuang,studdiford,nirunwiroj,agoyal25}@wisc.edu {vfrigo, syang84, dshah,
    junjie.hu, ttrogers}@wisc.edu ^† 联合第二作者
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With rapid advances in large language models (LLMs), there has grown increasing
    interest in using these technologies to simulate and understand dynamics of human
    communication and persuasion Park et al. ([2023](#bib.bib17), [2022](#bib.bib18));
    Chuang et al. ([2023](#bib.bib7)); Taubenfeld et al. ([2024](#bib.bib23)). Contemporary
    LLMs can be prompted to role-play as individuals with particular demographic traits,
    sometimes then producing patterns of behavior that seem remarkably human-like.
    For instance, when asked to report the US unemployment rate when President Obama
    left office, ChatGPT will provide the exact answer; but if first instructed to
    role-play as a typical Democrat or Republican and asked the same question, the
    model produces incorrect, inflated estimates that mirror patterns of partisan
    bias in analogous human studies Chuang et al. ([2024](#bib.bib8)). Such results
    raise the possibility that, with strategic prompting, LLMs may serve as useful
    proxies for capturing beliefs and attitudes of various socio-demographic groups.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）的快速进展，对使用这些技术模拟和理解人类沟通和劝说动态的兴趣也日益增长（Park et al. ([2023](#bib.bib17),
    [2022](#bib.bib18)); Chuang et al. ([2023](#bib.bib7)); Taubenfeld et al. ([2024](#bib.bib23))）。现代LLMs可以被提示扮演具有特定人口统计特征的个体，有时会产生看似非常类似人类的行为模式。例如，当被要求报告奥巴马总统离任时的美国失业率时，ChatGPT会提供确切的答案；但如果首先被指示扮演一个典型的民主党人或共和党人，并问相同的问题，该模型会产生不正确的、夸大的估计，反映出与类似人类研究中的党派偏见相符的模式（Chuang
    et al. ([2024](#bib.bib8))）。这些结果提出了这样一个可能性，即通过战略性提示，LLMs可能作为捕捉各种社会人口群体的信念和态度的有用代理。
- en: '![Refer to caption](img/832ef8037ac4896115f4e980591ffe03.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/832ef8037ac4896115f4e980591ffe03.png)'
- en: 'Figure 1: An LLM agent $i^{\prime}$).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM代理 $i^{\prime}$)。
- en: Other recent work suggests, however, that the alignment between beliefs expressed
    by role-playing LLMs and matched human participants is unreliable at best. For
    instance, Santurkar et al. ([2023](#bib.bib20)) found that LLMs tuned via human
    feedback generally reflect opinions from liberal and well-educated demographics
    and that having LLMs role-play as humans with different socio-demographic traits
    does not remediate this tendency. Similarly, Sun et al. ([2024](#bib.bib22)) had
    LLMs offer opinions on controversial issues while role-playing as humans with
    varying demographic characteristics, and found that the model only reflected corresponding
    human opinions on one of the ten total topics. Chuang et al. ([2023](#bib.bib7))
    additionally found that, even when seeded with prompts specifying an initial belief
    that runs contrary to social consensus (e.g., "global warming is a hoax"), LLMs
    quickly revert to the accepted ground-truth attitude after repeated interactions
    with other agents. Overall, this work suggests that LLM fine-tuned with human
    feedback tend to adopt progressive stances regardless of the demographic background
    they role-play–a behavior that may aid LLM fairness and value alignment, but limits
    their utility as models of human communicative dynamics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他近期研究表明，角色扮演LLMs所表达的信念与匹配的人工参与者之间的对齐程度充其量是不可靠的。例如，Santurkar et al. ([2023](#bib.bib20))发现，通过人类反馈调整的LLMs通常反映来自自由派和受过良好教育的人群的意见，而让LLMs扮演具有不同社会人口特征的人类并不能改善这种倾向。类似地，Sun
    et al. ([2024](#bib.bib22))让LLMs在扮演具有不同人口统计特征的人类时对有争议的问题提供意见，结果发现该模型仅在十个话题中的一个上反映了相应的人类意见。Chuang
    et al. ([2023](#bib.bib7))还发现，即使在被提示初始信念与社会共识相悖（例如，“全球变暖是个骗局”）的情况下，LLMs在与其他代理反复互动后会迅速回到接受的事实态度。总体而言，这些研究表明，通过人类反馈微调的LLMs往往会采纳进步立场，不论其角色扮演的社会人口背景如何——这种行为可能有助于LLM的公平性和价值对齐，但限制了其作为人类沟通动态模型的实用性。
- en: 'The current paper considers an alternative approach to aligning the attitudes
    expressed by role-playing LLMs and the human groups they are intended to emulate.
    The central idea relies on behavioral studies of human belief networks: the empirical
    observation that beliefs on different topics are not distributed at random across
    the population, but tend to cohere together in patterns of high-order covariation
    Boutyline and Vaisey ([2017](#bib.bib3)); Vlasceanu et al. ([2024](#bib.bib25));
    Keating ([2023](#bib.bib15)); Turner-Zwinkels and Brandt ([2022](#bib.bib24)).
    For instance, people who believe that government should support social welfare
    programs are also more likely to believe in higher taxes on the wealthy, strong
    union protections, and universal health care. Thus, knowing a person’s opinion
    on one topic can carry rich information about their likely views on many others.
    Because LLMs learn from vast amounts of human-generated language, the weights
    they acquire and hence patterns of behaviors they exhibit may implicitly capture
    the tendency for various beliefs to co-occur in human populations, providing novel
    leverage for alignment. Specifically, human-LLM alignment may be guided, not just
    by socio-demographic role-playing, but also by instructing the LLM to hold a specific
    opinion on a representative topic.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当前论文考虑了一种替代方法来对齐角色扮演的LLMs表达的态度与它们旨在模拟的人类群体。核心思想依赖于对人类信念网络的行为研究：一个经验观察是，不同主题的信念在群体中并不是随机分布的，而是倾向于以高阶共变模式聚合在一起
    (Boutyline 和 Vaisey ([2017](#bib.bib3))；Vlasceanu 等 ([2024](#bib.bib25))；Keating
    ([2023](#bib.bib15))；Turner-Zwinkels 和 Brandt ([2022](#bib.bib24)))。例如，相信政府应该支持社会福利项目的人更有可能相信对富人征收更高的税、强有力的工会保护和全民医疗保健。因此，了解一个人在一个主题上的观点可以提供关于他们在许多其他问题上可能观点的丰富信息。由于LLMs从大量人类生成的语言中学习，它们获得的权重以及由此展现的行为模式可能隐含地捕捉了各种信念在人类群体中共现的倾向，从而提供了对齐的新杠杆。具体来说，人类-LLM对齐可能不仅仅通过社会人口角色扮演来指导，还可以通过指示LLM在一个代表性主题上持有特定的观点来实现。
- en: '![Refer to caption](img/e74b4918083bd0504c08b1a3a51ed1fe.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e74b4918083bd0504c08b1a3a51ed1fe.png)'
- en: 'Figure 2: The belief network estimated by factor analysis from human respondents’
    responses on the Belief Survey. (a) Partial factor loading matrix that includes
    the columns for these Ghost (green) and the Partisan (violet) factors and the
    rows for topics that belong to these two factor categories. The full factor loading
    matrix is in Figure [5](#A6.F5 "Figure 5 ‣ Appendix F The Full Factor Analysis
    Results ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human
    Belief Networks") (§[F](#A6 "Appendix F The Full Factor Analysis Results ‣ Beyond
    Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks")).
    Red indicates topics that load positively on a factor, gray indicates near 0 loading,
    and blue indicates loading in the negative direction. The topics in the Ghost
    category has minimal loading on the Partisan factor and vice versa (highlighted
    by the black boxes). The training topics are further highlighted by dark green
    (“Dead Talk”) and purple (“Gun Control”) boxes, respectively. The full statement
    of the each topic is in Table LABEL:tab:list_topic (§[A](#A1 "Appendix A List
    of the 64 Topics in the Belief Survey ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")). (b) The graphical respresentation
    of the belief network, where the central nodes are the two latent factors, and
    the leaves (rectangles) are the individual topics. Red and blue edges indicate
    positive and negative loadings, respectively. The width of each edge encodes the
    strength of the loading. The training topics are highlighted with grey backgrounds.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：由因素分析估计的信念网络，基于人类受访者在信念调查中的回应。(a) 部分因素负荷矩阵，包含这些 Ghost（绿色）和 Partisan（紫色）因素的列，以及属于这两种因素类别的主题的行。完整的因素负荷矩阵见图
    [5](#A6.F5 "图 5 ‣ 附录 F 完整因素分析结果 ‣ 超越人口统计数据：利用人类信念网络对角色扮演的 LLM 基础代理进行对齐") (§[F](#A6
    "附录 F 完整因素分析结果 ‣ 超越人口统计数据：利用人类信念网络对角色扮演的 LLM 基础代理进行对齐"))。红色表示在某个因素上负荷为正，灰色表示负荷接近
    0，蓝色表示负荷为负。Ghost 类别的主题在 Partisan 因素上的负荷最小，反之亦然（由黑框突出显示）。训练主题分别由深绿色（“Dead Talk”）和紫色（“Gun
    Control”）框突出显示。每个主题的完整说明见表 LABEL:tab:list_topic (§[A](#A1 "附录 A 信念调查中的 64 个主题列表
    ‣ 超越人口统计数据：利用人类信念网络对角色扮演的 LLM 基础代理进行对齐"))。(b) 信念网络的图形表示，其中中央节点是两个潜在因素，叶子（矩形）是各个主题。红色和蓝色边缘分别表示正负负荷。每条边的宽度表示负荷的强度。训练主题用灰色背景突出显示。
- en: To test this idea, we considered a simple belief network constructed in prior
    work by applying factor analysis to a dataset measuring human beliefs across a
    diverse array of topics Frigo ([2022](#bib.bib11)). Factor analysis decomposes
    patterns of covariation among expressed beliefs, identifying relationships between
    the beliefs themselves and a set of underlying latent factors. From this analysis
    we identified two orthogonal factors, each receiving high loadings from several
    controversial beliefs, and with no overlap between the beliefs loading highly
    on each. These included a ghost factor grouping beliefs in various supernatural
    phenomena (e.g., talking to the dead) and a partisan factor grouping beliefs that
    are typically politically polarizing in the US (e.g., effectiveness of gun control).
    We then considered how well the opinions of contemporary LLMs align with human
    participants when prompted (a) with no role-playing information, (b) with demographic
    information only, or (c) with demographic information plus a corresponding belief
    on a single topic that aligns strongly with either the ghost factor or the partisan
    factor in the belief network. When seeding each model with such a belief, we additionally
    compared the effects of in-context learning (i.e., prompting) versus supervised
    fine-tuning. The results suggest that attention to empirically-derived human belief
    networks may provide a useful strategy for human-LLM alignment, more so than demographic
    role-playing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一理念，我们考虑了一个简单的信念网络，该网络通过对一个测量人类在各种话题上的信念的数据集进行因子分析而构建 Frigo ([2022](#bib.bib11))。因子分析分解了表达信念之间的协变模式，识别信念本身与一组潜在因素之间的关系。从这一分析中，我们识别出两个正交因素，每个因素都从几个有争议的信念中获得了高负荷，且这些信念在每个因素中没有重叠。这些因素包括将信念归类为各种超自然现象（例如，与死者交谈）的鬼魂因素和归类为通常在美国具有政治极化的信念（例如，枪支管制的有效性）的党派因素。然后，我们考虑了当用以下方式提示时，当前LLM的意见与人类参与者的一致性：
    (a) 没有角色扮演信息，(b) 仅有人口统计信息，或 (c) 具有与信念网络中的鬼魂因素或党派因素强烈对齐的单一话题的对应信念的同时具有人口统计信息。在给每个模型注入这样的信念时，我们还比较了情境学习（即提示）与监督微调的效果。结果表明，关注基于经验的人类信念网络可能提供了比人口统计角色扮演更有用的策略用于人类-LLM对齐。
- en: '![Refer to caption](img/6032867c79f07ef530d70d43a8910f51.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6032867c79f07ef530d70d43a8910f51.png)'
- en: 'Figure 3: LLM agent construction conditions with different levels of respondent’s
    information through in-context learning. (a) “None” condition without role-playing,
    and we directly query the LLM about its opinion on the query topic ($x_{\text{query}}$).
    Everything is in the “system message” except the query topic, which is in the
    “user message”.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：通过情境学习，不同水平的受访者信息下的LLM代理构建条件。（a）“无”条件下没有角色扮演，我们直接询问LLM关于查询主题（`$x_{\text{query}}$`）的意见。除了查询主题在“用户消息”中外，其余内容都在“系统消息”中。
- en: '2 Preliminaries: LLM Agents as Human Digital Twins'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步概念：LLM代理作为人类数字双胞胎
- en: 'As depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Beyond Demographics:
    Aligning Role-playing LLM-based Agents Using Human Belief Networks"), we aim to
    construct an LLM agent $i^{\prime}$. Note that we use the term LLM-based “agent”
    to refer to the digital twin because they are designed to produce a wide range
    of social behaviors that emulate the human individual they role-play Park et al.
    ([2023](#bib.bib17)); Shao et al. ([2023](#bib.bib21)); Zhou et al. ([2023](#bib.bib26)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 超越人口统计学：利用人类信念网络对角色扮演的LLM代理进行对齐")所示，我们旨在构建一个LLM代理`$i^{\prime}$。请注意，我们使用LLM-based“代理”一词来指代数字双胞胎，因为它们被设计用来产生广泛的社会行为，模仿它们所扮演的人类个体
    Park 等 ([2023](#bib.bib17)); Shao 等 ([2023](#bib.bib21)); Zhou 等 ([2023](#bib.bib26))。
- en: 3 Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Controversial Beliefs Survey
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 有争议信念调查
- en: 'The specific opinions we assessed were taken from the Controversial Beliefs
    Survey developed in Frigo ([2022](#bib.bib11)). The survey measures the direction
    and strength of belief across 64 topics spanning broad aspects of human knowledge,
    including history, science, health, religion, the supernatural, economics, politics,
    and conspiracy theories (see Table LABEL:tab:list_topic in §[A](#A1 "Appendix
    A List of the 64 Topics in the Belief Survey ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks") for the full list of topics). Topics
    were selected to elicit a diverse range of opinions about their truthfulness (hence
    “controversial beliefs”). Each belief is stated as a factual proposition (e.g.,
    "States with stricter gun control laws have fewer gun deaths per capita"), and
    participants rate their view about the truth of the statement on a six-point Likert
    scale ranging from "Certainly false" to "Certainly true." Responses with high
    numbers indicate agreement with the rational/consensus ground truth. The dataset
    also contains extensive demographic data from respondents, including age, gender,
    education level, household income, urban versus rural living environment, state
    of residence, and political leaning.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估的具体意见来源于 Frigo 开发的争议性信念调查（[2022](#bib.bib11)）。该调查测量了64个涵盖广泛人类知识方面的话题的信念方向和强度，包括历史、科学、健康、宗教、超自然现象、经济学、政治和阴谋论（完整的话题列表请参见表
    LABEL:tab:list_topic 在 §[A](#A1 "附录 A 争议性信念调查的64个话题列表 ‣ 超越人口统计学：利用人类信念网络对角色扮演的LLM代理进行对齐")）。选择这些话题是为了引出关于其真实性的多样化意见（因此称为“争议性信念”）。每个信念都被陈述为一个事实命题（例如，“拥有更严格枪支管控法律的州每万人中的枪支死亡人数较少”），参与者对该陈述的真实性进行六点李克特量表评分，范围从“肯定错误”到“肯定正确”。高分表示对合理/共识真相的认同。数据集还包含了广泛的受访者人口统计数据，包括年龄、性别、教育水平、家庭收入、城乡生活环境、居住州和政治倾向。
- en: 'The dataset includes ratings for $N=564$: Certainly true. No neutral value
    was provided so participants must minimally lean in one direction or the other.
    The demographic and opinion data together were used to construct and evaluate
    the LLM agents (§[3.3](#S3.SS3 "3.3 LLM Agent Construction ‣ 3 Methods ‣ Beyond
    Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks")).
    The survey dataset can be obtained by contacting its authors Frigo ([2022](#bib.bib11)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包括 $N=564$ 的评分：肯定正确。没有提供中立值，因此参与者必须至少倾向于一个方向。人口统计和意见数据一起用于构建和评估LLM代理 (§[3.3](#S3.SS3
    "3.3 LLM代理构建 ‣ 3 方法 ‣ 超越人口统计学：利用人类信念网络对角色扮演的LLM代理进行对齐")）。调查数据集可以通过联系其作者 Frigo
    ([2022](#bib.bib11)) 获得。
- en: 3.2 Constructing a Belief Network using Factor Analysis
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用因子分析构建信念网络
- en: 'Our objective was to find two independent “belief networks”–that is, two groups
    of topics where expressed beliefs covaried across participants within each group
    but were independent between groups. To this end, we relied on a previous factor
    analysis Frigo ([2022](#bib.bib11)) that first computed correlations in the ratings
    produced across participants for each pair of topics, then decomposed the resulting
    matrix into a set of orthogonal latent factors using principal component analysis
    (PCA) with Varimax rotation Kaiser ([1958](#bib.bib14)). The PCA yielded a factor
    loading matrix that encodes the loading between each topic and each latent factor.
    Nine latent factors were extracted based on the factor scree plot (Cattell, [1966](#bib.bib5),
    see §[D](#A4 "Appendix D The Choice of Number of Factors in Factor Analysis ‣
    Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief
    Networks")), which together accounted for 72% of the variance in the correlation
    matrix. From these, we selected two factors such that topics loading highly on
    the first had loadings near zero on the second and vice versa. These are shown
    in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Beyond Demographics: Aligning
    Role-playing LLM-based Agents Using Human Belief Networks"). The ghost factor
    receives high loadings from 12 topics, all pertaining to supernatural or otherworldly
    beliefs; the partisan factor receives high loadings from 6 topics on highly polarized
    political issues. We referred to these topics as either belonging to the ghost
    topic category or partisan topic category, respectively. We took these 18 topics
    and the corresponding latent factors as the targets for our analysis of LLM alignment.
    The full factor analysis results, including the full factor loading matrix of
    the nine factors, can be found in §[F](#A6 "Appendix F The Full Factor Analysis
    Results ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human
    Belief Networks").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到两个独立的“信念网络”——也就是两个话题组，其中每个组内的表达信念在参与者之间具有协方差，但组间则独立。为此，我们依赖于之前的因子分析
    Frigo ([2022](#bib.bib11))，该分析首先计算了参与者对每对话题的评分之间的相关性，然后使用主成分分析（PCA）和 Varimax 旋转
    Kaiser ([1958](#bib.bib14)) 将结果矩阵分解为一组正交的潜在因子。PCA 产生了一个因子载荷矩阵，编码了每个话题与每个潜在因子之间的载荷。根据因子碎石图
    (Cattell, [1966](#bib.bib5)，见 §[D](#A4 "附录 D 因子分析中因子数量的选择 ‣ 超越人口统计学：通过人类信念网络对角色扮演的
    LLM 基于代理的对齐"))，提取了九个潜在因子，这些因子共同解释了相关矩阵中72%的方差。从中，我们选择了两个因子，使得第一个因子上载荷高的话题在第二个因子上的载荷接近零，反之亦然。这些因子在图
    [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ 超越人口统计学：通过人类信念网络对角色扮演的 LLM 基于代理的对齐") 中展示。鬼魂因子从12个话题中获得高载荷，这些话题都与超自然或异世界信仰有关；党派因子从6个高度两极化的政治问题中获得高载荷。我们将这些话题称为鬼魂话题类别或党派话题类别。我们将这18个话题和相应的潜在因子作为我们分析
    LLM 对齐的目标。完整的因子分析结果，包括九个因子的完整因子载荷矩阵，可在 §[F](#A6 "附录 F 完整因子分析结果 ‣ 超越人口统计学：通过人类信念网络对角色扮演的
    LLM 基于代理的对齐") 中找到。
- en: 3.3 LLM Agent Construction
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM 代理构建
- en: For each factor we designated the topic possessing the highest loading as the
    model training topic ($x_{\text{train}}$ on these topics were used to evaluate
    their alignment with the human respondents. We hypothesized that specifying the
    agent’s opinion on the training topic might elicit shared representation that
    generalize to testing topics close within the belief network (i.e., sharing the
    same latent factor), but not those from the other belief network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个因子，我们指定了载荷最高的话题作为模型训练话题（$x_{\text{train}}$ 用于评估它们与人类受访者的对齐）。我们假设，指定代理在训练话题上的观点可能会引发共享表示，从而推广到信念网络中接近的测试话题（即，具有相同潜在因子的），但不会推广到其他信念网络中的话题。
- en: For each human respondent $i$), and measured how ratings generated by the digital
    twins correlate with the true opinions expressed by corresponding human respondents.
    We then assessed how this measure of human-LLM belief alignment varied with different
    strategies for constructing the digital twin.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个受访者 $i$)，我们测量了数字双胞胎生成的评分与对应人类受访者表达的真实观点之间的相关性。然后我们评估了这种人类-LLM 信念对齐的度量在构建数字双胞胎的不同策略下的变化情况。
- en: In-context Learning (ICL).
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语境学习（ICL）。
- en: 'As shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Beyond Demographics:
    Aligning Role-playing LLM-based Agents Using Human Belief Networks"), these strategies
    involve initializing agents via in-context learning only, with different information
    included in their system message (see §[4.1](#S4.SS1 "4.1 Configuration for LLM
    Agents ‣ 4 Experimental Settings ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks") and Appendix §[B](#A2 "Appendix
    B The Prompts for LLM Agent Construction Through In-context Learning ‣ Beyond
    Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks")
    for the prompts).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S1.F3 "图 3 ‣ 1 引言 ‣ 超越人口统计数据：通过人类信念网络对齐角色扮演 LLM 基础代理") 所示，这些策略涉及仅通过上下文学习初始化代理，系统消息中包含不同的信息（有关提示的信息请参见
    §[4.1](#S4.SS1 "4.1 LLM 代理的配置 ‣ 4 实验设置 ‣ 超越人口统计数据：通过人类信念网络对齐角色扮演 LLM 基础代理") 和附录
    §[B](#A2 "附录 B 通过上下文学习构建 LLM 代理的提示 ‣ 超越人口统计数据：通过人类信念网络对齐角色扮演 LLM 基础代理")）。
- en: a.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a.
- en: 'None: An LLM, without role-playing, is directly queried for its Likert-scale
    opinion on the query topic, providing a performance floor since there is no way
    for the LLM to align with a corresponding human participant. Note that variation
    may still be present due to temperature sampling (§[4.1](#S4.SS1 "4.1 Configuration
    for LLM Agents ‣ 4 Experimental Settings ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")).'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无：一个 LLM，没有角色扮演，直接对查询主题进行 Likert 量表意见评估，提供一个性能底线，因为 LLM 无法与相应的人类参与者对齐。请注意，由于温度采样，仍可能存在变异
    (§[4.1](#S4.SS1 "4.1 LLM 代理的配置 ‣ 4 实验设置 ‣ 超越人口统计数据：通过人类信念网络对齐角色扮演 LLM 基础代理"))。
- en: b.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b.
- en: 'Demo: An LLM agent is constructed to role-play the $i$) in the prompt.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演示：一个 LLM 代理被构建来扮演提示中的 $i$)。
- en: c.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: c.
- en: 'Demo+Train [same category]: In addition to demographic information, the LLM
    receives a respondent’s Likert-scale opinion on the training topic ($x_{\text{train}}$
    within the belief network. This is the condition of interest.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演示+训练 [相同类别]：除了人口统计信息外，LLM 接收受访者对培训主题的 Likert 量表意见（$x_{\text{train}}$ 在信念网络中）。这是感兴趣的条件。
- en: d.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d.
- en: 'Demo+Train [different category]: This control condition is similar to Demo+Train
    [same category], but assesses the LLM on topics from the opposing topic category,
    allowing us to determine whether the cross-topic generalization is restricted
    to adjacent topics in the belief network.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演示+训练 [不同类别]：这个对照条件类似于 演示+训练 [相同类别]，但评估 LLM 对来自对立主题类别的主题的表现，使我们能够确定跨主题的泛化是否仅限于信念网络中的相邻主题。
- en: e.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: e.
- en: 'Demo+Train+Query: This control condition provides the human opinion rating
    on both the training topic ($x_{\text{train}}$) during the agent construction,
    providing an upper bound on generalization behavior.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演示+训练+查询：这个对照条件提供了人类对代理构建期间培训主题 ($x_{\text{train}}$) 的意见评分，提供了对泛化行为的上限。
- en: Supervised Fine-tuning (SFT).
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监督微调（SFT）。
- en: 'We also investigated whether seeding initial beliefs via supervised fine-tuning
    (SFT) can increase human-LLM alignment. Specifically, the correspondence between
    the demographic information $d$. Details of the fine-tuning procedure and the
    corresponding prompts are in §[C](#A3 "Appendix C The Prompts for LLM Agent Construction
    Through Supervised Fine-tuning ‣ Beyond Demographics: Aligning Role-playing LLM-based
    Agents Using Human Belief Networks") and §[E](#A5 "Appendix E Supervised Fine-tuning
    Details ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human
    Belief Networks").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了通过监督微调（SFT）播种初始信念是否可以增加人类-LLM 对齐。具体而言，人口统计信息 $d$ 之间的对应关系。微调过程的详细信息和相应的提示见
    §[C](#A3 "附录 C 通过监督微调构建 LLM 代理的提示 ‣ 超越人口统计数据：通过人类信念网络对齐角色扮演 LLM 基础代理") 和 §[E](#A5
    "附录 E 监督微调细节 ‣ 超越人口统计数据：通过人类信念网络对齐角色扮演 LLM 基础代理")。
- en: 4 Experimental Settings
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: '| Category | Topic | Conditions for LLM Agent Construction (In-context Learning)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 主题 | LLM 代理构建的条件（上下文学习） |'
- en: '| ChatGPT (gpt-3.5-turbo-0125) | Mistral (Mistral-7B-Instruct-v0.2) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT (gpt-3.5-turbo-0125) | Mistral (Mistral-7B-Instruct-v0.2) |'
- en: '| None | Demo | Demo+Train | Demo+Train | Demo+Train | None | Demo | Demo+Train
    | Demo+Train | Demo+Train |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 演示 | 演示+训练 | 演示+训练 | 演示+训练 | 无 | 演示 | 演示+训练 | 演示+训练 | 演示+训练 |'
- en: '|  |  |  |  | [Diff. Cat.] | [Same Cat.] | + Query |  |  | [Diff. Cat.] | [Same
    Cat.] | + Query |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | [不同类别] | [相同类别] | + 查询 |  |  | [不同类别] | [相同类别] | + 查询 |'
- en: '| Ghost |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 幽灵 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '|       Train | Dead Talk | 0.04 | 0.02 | 0.04 | 0.98 | 1.00 | NA | NA | 0.07
    | 0.97 | 0.98 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|       训练 | 死者谈话 | 0.04 | 0.02 | 0.04 | 0.98 | 1.00 | NA | NA | 0.07 | 0.97
    | 0.98 |'
- en: '|       Test | Ghost | 0.03 | 0.05 | -0.07 | 0.53 | 0.75 | NA | NA | NA | 0.59
    | 0.73 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|       测试 | 幽灵 | 0.03 | 0.05 | -0.07 | 0.53 | 0.75 | NA | NA | NA | 0.59 |
    0.73 |'
- en: '|  | Alien Visit | -0.08 | -0.05 | -0.04 | 0.33 | 0.63 | NA | NA | NA | 0.37
    | 0.62 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 外星访问 | -0.08 | -0.05 | -0.04 | 0.33 | 0.63 | NA | NA | NA | 0.37 | 0.62
    |'
- en: '|  | Soul Walk | -0.05 | 0.06 | -0.07 | 0.40 | 0.89 | NA | NA | 0.07 | 0.53
    | 0.63 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 灵魂漫步 | -0.05 | 0.06 | -0.07 | 0.40 | 0.89 | NA | NA | 0.07 | 0.53 | 0.63
    |'
- en: '|  | See Future | -0.03 | 0.07 | -0.03 | 0.34 | 0.80 | NA | NA | -0.08 | 0.38
    | 0.85 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | 见未来 | -0.03 | 0.07 | -0.03 | 0.34 | 0.80 | NA | NA | -0.08 | 0.38 | 0.85
    |'
- en: '|  | Astrology | -0.04 | 0.06 | -0.07 | 0.28 | 0.88 | NA | NA | NA | 0.32 |
    0.71 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | 占星术 | -0.04 | 0.06 | -0.07 | 0.28 | 0.88 | NA | NA | NA | 0.32 | 0.71
    |'
- en: '|  | Roswell | -0.10 | -0.07 | 0.03 | 0.26 | 0.85 | NA | NA | NA | 0.21 | 0.28
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 罗兹威尔 | -0.10 | -0.07 | 0.03 | 0.26 | 0.85 | NA | NA | NA | 0.21 | 0.28
    |'
- en: '|  | Past Life | -0.02 | 0.01 | 0.09 | 0.31 | 0.79 | NA | NA | -0.05 | 0.17
    | 0.61 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | 过去的生活 | -0.02 | 0.01 | 0.09 | 0.31 | 0.79 | NA | NA | -0.05 | 0.17 | 0.61
    |'
- en: '|  | The Secret | -0.01 | 0.05 | 0.02 | 0.32 | 0.66 | NA | NA | NA | 0.07 |
    0.67 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | 秘密 | -0.01 | 0.05 | 0.02 | 0.32 | 0.66 | NA | NA | NA | 0.07 | 0.67 |'
- en: '|  | Aura | 0.03 | 0.02 | -0.02 | 0.25 | 0.80 | NA | NA | NA | 0.35 | 0.62
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | 气场 | 0.03 | 0.02 | -0.02 | 0.25 | 0.80 | NA | NA | NA | 0.35 | 0.62 |'
- en: '|  | Luck | -0.04 | 0.08 | -0.09 | 0.23 | 0.84 | NA | NA | NA | NA | 0.46 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | 运气 | -0.04 | 0.08 | -0.09 | 0.23 | 0.84 | NA | NA | NA | NA | 0.46 |'
- en: '|  | Dousing | -0.02 | 0.03 | 0.00 | 0.19 | 0.71 | NA | NA | 0.01 | 0.23 |
    0.58 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | 占卜 | -0.02 | 0.03 | 0.00 | 0.19 | 0.71 | NA | NA | 0.01 | 0.23 | 0.58
    |'
- en: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [2.42] | [2.54] | [2.31] | [1.29]
    | [0.34] | [1.82] | [1.82] | [1.83] | [1.28] | [0.71] |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [2.42] | [2.54] | [2.31] | [1.29]
    | [0.34] | [1.82] | [1.82] | [1.83] | [1.28] | [0.71] |'
- en: '| Partisan |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 党派 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '|       Train | Gun Control | -0.04 | 0.25 | 0.30 | 0.98 | 1.00 | NA | 0.33
    | 0.12 | 0.90 | 0.90 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|       训练 | 枪支管制 | -0.04 | 0.25 | 0.30 | 0.98 | 1.00 | NA | 0.33 | 0.12 |
    0.90 | 0.90 |'
- en: '|       Test | Globe Warm | -0.09 | 0.27 | 0.27 | 0.27 | 0.94 | NA | 0.32 |
    0.22 | 0.38 | 0.81 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|       测试 | 环球温暖 | -0.09 | 0.27 | 0.27 | 0.27 | 0.94 | NA | 0.32 | 0.22 |
    0.38 | 0.81 |'
- en: '|  | Globe Human | -0.10 | 0.30 | 0.35 | 0.35 | 0.98 | NA | 0.31 | 0.33 | 0.39
    | 0.73 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | 环球人类 | -0.10 | 0.30 | 0.35 | 0.35 | 0.98 | NA | 0.31 | 0.33 | 0.39 | 0.73
    |'
- en: '|  | US Deficit | 0.03 | 0.02 | 0.03 | 0.16 | 0.70 | NA | NA | -0.02 | 0.09
    | 0.70 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 美国赤字 | 0.03 | 0.02 | 0.03 | 0.16 | 0.70 | NA | NA | -0.02 | 0.09 | 0.70
    |'
- en: '|  | Unions | 0.03 | 0.18 | 0.08 | 0.18 | 0.88 | NA | 0.06 | 0.04 | 0.13 |
    0.78 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 工会 | 0.03 | 0.18 | 0.08 | 0.18 | 0.88 | NA | 0.06 | 0.04 | 0.13 | 0.78
    |'
- en: '|  | Death Penalty | -0.14 | 0.00 | 0.00 | 0.00 | 0.32 | NA | NA | NA | NA
    | 0.46 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 死刑 | -0.14 | 0.00 | 0.00 | 0.00 | 0.32 | NA | NA | NA | NA | 0.46 |'
- en: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.42] | [1.32] | [1.35] | [1.25]
    | [0.38] | [2.20] | [1.32] | [1.39] | [1.28] | [0.63] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.42] | [1.32] | [1.35] | [1.25]
    | [0.38] | [2.20] | [1.32] | [1.39] | [1.28] | [0.63] |'
- en: 'Table 1: Kendall’s $\tau_{t}$, the higher the human-LLM alignment. In particular,
    the inclusion of same-category training topic opinions significantly increases
    the alignment. (“Diff. Cat.” : Different Category; “Same Cat.”: Same Category)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：Kendall 的 $\tau_{t}$，人类-LLM 一致性越高。特别是，同类别训练主题意见的纳入显著增加了一致性。 (“Diff. Cat.”：不同类别；“Same
    Cat.”：同类别)
- en: 4.1 Configuration for LLM Agents
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LLM 代理配置
- en: 'We evaluated LLM agents using both ChatGPT (gpt-3.5-turbo-0125; OpenAI, [2022](#bib.bib16))
    and Mistral (Mistral-7B-Instruct-v0.2; Jiang et al., [2023](#bib.bib13)) with
    temperature of $0.7$) were fed to the agent through the model’s “user messages”.
    When using in-context learning (§[3.3](#S3.SS3 "3.3 LLM Agent Construction ‣ 3
    Methods ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human
    Belief Networks")), the training/query topic opinions were also included in the
    model’s “system messages”. The LLM agents were constructed through LangChain Chase
    ([2022](#bib.bib6)). For our compute resources, see §[G](#A7 "Appendix G Compute
    Resources ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using
    Human Belief Networks").'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了 LLM 代理，使用了 ChatGPT（gpt-3.5-turbo-0125；OpenAI，[2022](#bib.bib16)）和 Mistral（Mistral-7B-Instruct-v0.2；Jiang
    等，[2023](#bib.bib13)），温度为 $0.7$）通过模型的“用户消息”输入给代理。在使用上下文学习（§[3.3](#S3.SS3 "3.3
    LLM Agent Construction ‣ 3 Methods ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")）时，训练/查询主题意见也包含在模型的“系统消息”中。LLM 代理通过
    LangChain Chase（[2022](#bib.bib6)）构建。有关我们的计算资源，请参见 §[G](#A7 "Appendix G Compute
    Resources ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using
    Human Belief Networks")。'
- en: 4.2 Supervised Fine-tuning
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 监督微调
- en: 'For LLM agents constructed through supervised fine-tuning (§[3.3](#S3.SS3 "3.3
    LLM Agent Construction ‣ 3 Methods ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")), we used the ChatGPT model gpt-3.5-turbo-0125’s
    fine-tuning API. Critically, because the label (i.e., opinion response $o$. §[E](#A5
    "Appendix E Supervised Fine-tuning Details ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks") lists the hyperparameters for fine-tuning.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '对于通过监督微调构建的LLM代理（§[3.3](#S3.SS3 "3.3 LLM Agent Construction ‣ 3 Methods ‣ Beyond
    Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks")），我们使用了ChatGPT模型gpt-3.5-turbo-0125的微调API。关键是，因为标签（即意见响应
    $o$。§[E](#A5 "Appendix E Supervised Fine-tuning Details ‣ Beyond Demographics:
    Aligning Role-playing LLM-based Agents Using Human Belief Networks") 列出了微调的超参数。'
- en: 4.3 Evaluation Metrics
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估指标
- en: To evaluate the “human-likeness” of the LLM agents’ opinions, we for each topic
    $x$) within the topic category.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM代理意见的“人类相似性”，我们对每个主题$x$)在主题类别内进行评估。
- en: 5 Results
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: The results for in-context learning and supervised fine-tuning were qualitatively
    similar; we discuss the in-context learning results first.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文学习和监督微调的结果在质的方面是类似的；我们首先讨论上下文学习的结果。
- en: Demographic information alone does not align the LLM agent’s opinion.
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单独的**人口统计信息**并不能使LLM代理的意见对齐。
- en: 'As shown in Table [1](#S4.T1 "Table 1 ‣ 4 Experimental Settings ‣ Beyond Demographics:
    Aligning Role-playing LLM-based Agents Using Human Belief Networks"), incorporating
    solely the demographic information (the Demo condition) fails to align LLM agents
    with human respondents. The Kendall’s $\tau$ of the Demo condition is also similar
    to the None baseline condition, indicating that the demographic information alone
    does not help LLM agents align with the human respondents they role-play.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S4.T1 "Table 1 ‣ 4 Experimental Settings ‣ Beyond Demographics: Aligning
    Role-playing LLM-based Agents Using Human Belief Networks")所示，仅仅纳入人口统计信息（Demo条件）未能使LLM代理与人类受访者对齐。Demo条件的Kendall’s
    $\tau$值也类似于None基线条件，表明单独的人口统计信息不能帮助LLM代理与其角色扮演的人类受访者对齐。'
- en: Specifying the agent’s opinion on a training topic aligns other beliefs in the
    same network.
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指定代理在训练主题上的意见能够对齐网络中的其他信念。
- en: 'When the LLM is instructed to adopt the twinned human’s opinion on the training
    topic ($x_{\text{train}}$). This effect is limited to topics within the same belief
    network: expressed beliefs in the other topic category (e.g., about the effectiveness
    of gun control law; Demo+Train [different category] condition) remain uncorrelated
    (unaligned) with the corresponding human opinion opinion. This supports our hypothesis
    – opinions on one topic encourage the LLM agents to align their opinions only
    on topics that are adjacent in the belief network. We additionally note that such
    alignment is not total: human-LLM correlations in the Demo+Train [same category]
    condition do not reach the upper bounds established by the Demo+Train+Query control
    condition, highlighting opportunities for future work to further improve the alignment.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当LLM被指示采纳双胞胎人类在训练主题上的意见 ($x_{\text{train}}$)。这一效果仅限于同一信念网络中的主题：在其他主题类别中的表达信念（例如关于枪支管控法的有效性；Demo+Train
    [不同类别] 条件）与对应的人类意见无关（不对齐）。这支持了我们的假设——一个主题上的意见促使LLM代理仅在信念网络中相邻的主题上对齐其意见。我们还注意到这种对齐并不完全：在Demo+Train
    [相同类别] 条件下的人类-LLM相关性没有达到Demo+Train+Query对照条件所建立的上限，突显了未来进一步改善对齐的机会。
- en: Degree of alignment reflects factor loadings.
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对齐程度反映因子负载。
- en: Different topics showed differing degrees of human-LLM alignment following the
    training-topic prompt, ranging from zero correlation for the death penalty topic
    (“States that have the death penalty have higher rates of violent crime on average”)
    to a correlation of 0.53 (ChatGPT) and 0.59 (Mistral) for belief in ghosts (“After
    people die it is sometimes possible to see their ghost.”). Yet the different topics
    also vary in the strength with which load on their primary factor. To assess whether
    this variation explains alignment patterns, we computed, across all test topics,
    the correlation between the topic’s loading on its primary factor and its degree
    of alignment in the Demo+Train [same category] condition. The result showed a
    tight correlation between these ($r=0.77,p<.001$), suggesting that degree of alignment
    following the training prompt reflects strength of the topic’s participation in
    the corresponding belief network. This relationship does not explain all cross-topic
    variation; at least one topic (death penalty) showed zero alignment even when
    given the correct opinion in the prompt, suggesting some degree of inherent bias
    in model responses for certain topics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 不同话题在训练主题提示后的人工-LLM一致性程度各异，从对死刑话题（“有死刑的州平均暴力犯罪率更高”）的零相关，到对鬼魂信仰（“人死后有时可以看到他们的鬼魂”）的0.53（ChatGPT）和0.59（Mistral）相关性。然而，不同话题在主要因素的负荷强度上也存在差异。为了评估这种变化是否解释了一致性模式，我们计算了所有测试话题中，话题在其主要因素上的负荷与其在Demo+Train
    [同类]条件下的一致性程度之间的相关性。结果显示这些之间有紧密的相关性（$r=0.77,p<.001$），这表明训练提示后的匹配程度反映了话题在相应信念网络中的参与强度。这一关系并未解释所有跨话题的变化；至少一个话题（死刑）即使在提示中给出正确的观点也表现出零一致性，表明模型对某些话题存在固有偏见。
- en: Alignment does not reflect superficial repetition.
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一致性不反映肤浅的重复。
- en: '| Category | Topic | Demo+Train condition [Same Cat.] |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 话题 | Demo+Train条件 [同类] |'
- en: '| ChatGPT | Mistral |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: ChatGPT | Mistral |
- en: '| [Original] | [Balanced] | [Original] | [Balanced] |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [原始] | [平衡] | [原始] | [平衡] |'
- en: '| Ghost |  |  |  |  |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 鬼魂 |  |  |  |  |  |'
- en: '| Train | Dead Talk | 0.98 | 0.99 | 0.97 | 0.97 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 死亡对话 | 0.98 | 0.99 | 0.97 | 0.97 |'
- en: '| Test | Ghost | 0.53 | 0.46 | 0.59 | 0.61 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 鬼魂 | 0.53 | 0.46 | 0.59 | 0.61 |'
- en: '|  | Alien Visit | 0.33 | 0.25 | 0.37 | 0.18 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 外星人访问 | 0.33 | 0.25 | 0.37 | 0.18 |'
- en: '|  | Soul Walk | 0.40 | 0.40 | 0.53 | 0.53 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 灵魂行走 | 0.40 | 0.40 | 0.53 | 0.53 |'
- en: '|  | See Future | 0.34 | 0.16 | 0.38 | 0.52 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 预见未来 | 0.34 | 0.16 | 0.38 | 0.52 |'
- en: '|  | Astrology | 0.28 | 0.13 | 0.32 | 0.32 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | 占星术 | 0.28 | 0.13 | 0.32 | 0.32 |'
- en: '|  | Roswell | 0.26 | 0.31 | 0.21 | 0.12 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | 罗斯威尔 | 0.26 | 0.31 | 0.21 | 0.12 |'
- en: '|  | Past Life | 0.31 | 0.32 | 0.17 | 0.18 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 前生 | 0.31 | 0.32 | 0.17 | 0.18 |'
- en: '|  | The Secret | 0.32 | 0.14 | 0.07 | 0.07 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 秘密 | 0.32 | 0.14 | 0.07 | 0.07 |'
- en: '|  | Aura | 0.25 | 0.15 | 0.35 | 0.32 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | 光环 | 0.25 | 0.15 | 0.35 | 0.32 |'
- en: '|  | Luck | 0.23 | 0.03 | NA | NA |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 运气 | 0.23 | 0.03 | NA | NA |'
- en: '|  | Dousing | 0.19 | 0.24 | 0.23 | 0.32 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | 占卜 | 0.19 | 0.24 | 0.23 | 0.32 |'
- en: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.29] | [1.64] | [1.28] | [1.26]
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.29] | [1.64] | [1.28] | [1.26]
    |'
- en: '| Partisan |  |  |  |  |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 党派 |  |  |  |  |  |'
- en: '| Train | Gun Control | 0.98 | 0.88 | 0.90 | 0.93 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 枪支控制 | 0.98 | 0.88 | 0.90 | 0.93 |'
- en: '| Test | Globe Warm | 0.27 | 0.03 | 0.38 | 0.14 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 环球变暖 | 0.27 | 0.03 | 0.38 | 0.14 |'
- en: '|  | Globe Human | 0.35 | 0.12 | 0.39 | 0.21 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | 环球人类 | 0.35 | 0.12 | 0.39 | 0.21 |'
- en: '|  | US Deficit | 0.16 | 0.01 | 0.09 | 0.10 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 美国赤字 | 0.16 | 0.01 | 0.09 | 0.10 |'
- en: '|  | Union Protection | 0.18 | 0.18 | 0.13 | 0.19 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 工会保护 | 0.18 | 0.18 | 0.13 | 0.19 |'
- en: '|  | Death Penalty | 0.00 | 0.00 | NA | NA |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 死刑 | 0.00 | 0.00 | NA | NA |'
- en: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.25] | [1.24] | [1.28] | [1.23]
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.25] | [1.24] | [1.28] | [1.23]
    |'
- en: 'Table 2: Kendall’s $\tau_{t}$, the higher the human-LLM alignment. Note that
    balancing the label distribution still maintains the superiority of Demo+Train
    [same category] condition when compared with the Demo condition (Table [1](#S4.T1
    "Table 1 ‣ 4 Experimental Settings ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '表2: Kendall的$\tau_{t}$，数值越高表示人工-LLM一致性越强。注意，即使平衡标签分布，Demo+Train [同类]条件在与Demo条件比较时仍保持其优势（表[1](#S4.T1
    "表 1 ‣ 4 实验设置 ‣ 超越人口统计学：利用人类信念网络对角色扮演LLM代理进行对齐")）。'
- en: 'Does increased alignment following the Demo+Train [same category] condition
    arise from a model tendency to simply repeat the opinion providing for the training
    topic? Such a pattern might appear to lead to increased alignment simply because
    the training topic opinion, by definition, correlates with opinions on other topics
    in the same belief network. To address this concern, we conducted an additional
    experiment in which we balanced the label distribution in the prompting contexts
    by constructing reversed framing statements that entail the same semantic meaning.
    We then included both the original and reversed framing statements in the context.
    For example, for the original statement “You believe it is certainly true that
    ‘States with stricter gun control laws have fewer gun deaths per capita”’, the
    reversed frame stated “You believe it is certainly false that ‘States with stricter
    gun control laws have more gun deaths per capita”’. Both statements were included
    in the context in random order so the LLM cannot show increased alignment by merely
    repeating the training topic opinion. Table [2](#S5.T2 "Table 2 ‣ Alignment does
    not reflect superficial repetition. ‣ 5 Results ‣ Beyond Demographics: Aligning
    Role-playing LLM-based Agents Using Human Belief Networks") shows that the LLMs
    continue to show significant alignment with human opinions (high $\tau$ alone.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 增加的对齐是否是因为模型倾向于简单地重复提供训练主题的观点？这种模式可能看起来会导致对齐增加，仅仅因为训练主题的观点与同一信仰网络中其他主题的观点相关。为了解决这个问题，我们进行了一项额外的实验，通过构造具有相同语义的反向框架陈述来平衡提示上下文中的标签分布。然后我们将原始和反向框架陈述都包含在上下文中。例如，对于原始陈述“You
    believe it is certainly true that ‘States with stricter gun control laws have
    fewer gun deaths per capita”’，反向框架陈述为“You believe it is certainly false that ‘States
    with stricter gun control laws have more gun deaths per capita””。两个陈述以随机顺序包含在上下文中，以防止
    LLM 通过简单地重复训练主题观点来表现出增加的对齐。表[2](#S5.T2 "Table 2 ‣ 对齐不反映表面重复。 ‣ 5 结果 ‣ 超越人口统计学：使用人类信仰网络对角色扮演
    LLM 代理进行对齐")显示 LLM 继续显著对齐人类观点（高$\tau$仅仅）。
- en: Supervised fine-tuning yields similar results.
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监督微调产生类似结果。
- en: 'As shown in Table [3](#S5.T3 "Table 3 ‣ Supervised fine-tuning yields similar
    results. ‣ 5 Results ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents
    Using Human Belief Networks"), when the agents are fine-tuned with a training
    topic $x_{\text{train}}$; the Demo+Train [same category] condition), but not on
    those belonging to a different network (Demo+Train [different category] condition)–a
    pattern of results qualitatively similar to in-context learning.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[3](#S5.T3 "Table 3 ‣ 监督微调产生类似结果。 ‣ 5 结果 ‣ 超越人口统计学：使用人类信仰网络对角色扮演 LLM 代理进行对齐")所示，当代理在训练主题$x_{\text{train}}$下进行微调时；在Demo+Train
    [同类别]条件下，但不在属于不同网络的条件下（Demo+Train [不同类别]条件下）——这种结果模式在定性上类似于上下文学习。
- en: '| Cat. | Topic | Conditions for LLM Agent Construction (SFT) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 主题 | LLM 代理构建条件（SFT） |'
- en: '| None | Demo | Demo+Train | Demo+Train |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 演示 | 演示+训练 | 演示+训练 |'
- en: '|  |  | [Diff. Cat.] | [Same Cat.] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [不同类别] | [相同类别] |'
- en: '| Ghost |  |  |  |  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 幽灵 |  |  |  |  |  |'
- en: '| Train | Dead Talk | 0.04 | 0.02 | 0.04 | 0.22 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 死亡谈话 | 0.04 | 0.02 | 0.04 | 0.22 |'
- en: '| Test | Ghost | 0.03 | 0.05 | -0.08 | 0.10 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 幽灵 | 0.03 | 0.05 | -0.08 | 0.10 |'
- en: '|  | Alien Visit | -0.08 | -0.05 | -0.02 | 0.10 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | 外星人访问 | -0.08 | -0.05 | -0.02 | 0.10 |'
- en: '|  | Soul Walk | -0.05 | 0.06 | -0.05 | 0.14 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 灵魂之行 | -0.05 | 0.06 | -0.05 | 0.14 |'
- en: '|  | See Future | -0.03 | 0.07 | 0.07 | 0.12 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 看未来 | -0.03 | 0.07 | 0.07 | 0.12 |'
- en: '|  | Astrology | -0.04 | 0.06 | 0.07 | 0.06 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | 占星术 | -0.04 | 0.06 | 0.07 | 0.06 |'
- en: '|  | Roswell | -0.10 | -0.07 | 0.05 | 0.16 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 罗兹威尔 | -0.10 | -0.07 | 0.05 | 0.16 |'
- en: '|  | Past Life | -0.02 | 0.01 | -0.02 | 0.06 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 过去的生活 | -0.02 | 0.01 | -0.02 | 0.06 |'
- en: '|  | The Secret | -0.01 | 0.05 | 0.05 | 0.14 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | 秘密 | -0.01 | 0.05 | 0.05 | 0.14 |'
- en: '|  | Aura | 0.03 | 0.02 | -0.07 | 0.06 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | 气场 | 0.03 | 0.02 | -0.07 | 0.06 |'
- en: '|  | Luck | -0.04 | 0.08 | -0.06 | 0.17 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | 运气 | -0.04 | 0.08 | -0.06 | 0.17 |'
- en: '|  | Dousing | -0.02 | 0.03 | -0.07 | 0.08 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 灭火 | -0.02 | 0.03 | -0.07 | 0.08 |'
- en: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [2.42] | [2.54] | [2.45] | [1.65]
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [2.42] | [2.54] | [2.45] | [1.65]
    |'
- en: '| Partisan |  |  |  |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 党派 |  |  |  |  |  |'
- en: '| Train | Gun Control | -0.04 | 0.25 | 0.20 | 0.28 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 枪支控制 | -0.04 | 0.25 | 0.20 | 0.28 |'
- en: '| Test | Globe Warm | -0.09 | 0.27 | 0.02 | 0.30 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 全球变暖 | -0.09 | 0.27 | 0.02 | 0.30 |'
- en: '|  | Globe Human | -0.10 | 0.30 | 0.15 | 0.30 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | 全球人类 | -0.10 | 0.30 | 0.15 | 0.30 |'
- en: '|  | US Deficit | 0.03 | 0.02 | 0.01 | 0.09 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | 美国赤字 | 0.03 | 0.02 | 0.01 | 0.09 |'
- en: '|  | Union Protection | 0.03 | 0.18 | 0.07 | 0.18 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | 工会保护 | 0.03 | 0.18 | 0.07 | 0.18 |'
- en: '|  | Death Penalty | -0.14 | 0.00 | 0.00 | 0.05 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | 死刑 | -0.14 | 0.00 | 0.00 | 0.05 |'
- en: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.42] | [1.32] | [1.71] | [1.26]
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MAE}_{\text{test}}\downarrow$ | [1.42] | [1.32] | [1.71] | [1.26]
    |'
- en: 'Table 3: Kendall’s $\tau$, the higher the human-LLM alignment. In particular,
    fine-tuning LLM with same-category training topic opinions significantly increases
    the alignment. The “None” and “Demo” conditions are identical to the ones in Table [1](#S4.T1
    "Table 1 ‣ 4 Experimental Settings ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks") because they are tuning-free baselines.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表3：肯德尔的$\tau$，人类-LLM对齐度越高。特别是，使用相同类别训练主题的意见对LLM进行微调显著提高了对齐度。由于“None”和“Demo”条件与表[1](#S4.T1
    "Table 1 ‣ 4 Experimental Settings ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")中的条件相同，因此它们是无调优基线。'
- en: 6 Related Work
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Aligning human and LLM opinions.
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对齐人类与LLM的意见。
- en: Recent studies highlight both the potential and the limitations of using LLMs
    to emulate human opinions Argyle et al. ([2023](#bib.bib1)); Santurkar et al.
    ([2023](#bib.bib20)); Sun et al. ([2024](#bib.bib22)); Feng et al. ([2023](#bib.bib10));
    Chuang et al. ([2023](#bib.bib7), [2024](#bib.bib8)). Argyle et al. ([2023](#bib.bib1))
    showed that LLMs conditioned on demographic backstories can emulate human voting
    preferences and language use, but did not investigate topic-specific opinions.
    Santurkar et al. ([2023](#bib.bib20)) found that different models have different
    inherent opinions that often align with liberal, high-income, well-educated demographics,
    and that these opinions could not be shifted by providing demographic role-playing
    information. The current paper replicates this finding, but additionally suggests
    that alignment may be shifted via belief networks. To the best of our knowledge
    no prior work has studied such effects.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究突出了使用LLM模拟人类意见的潜力和局限性 Argyle et al. ([2023](#bib.bib1)); Santurkar et al.
    ([2023](#bib.bib20)); Sun et al. ([2024](#bib.bib22)); Feng et al. ([2023](#bib.bib10));
    Chuang et al. ([2023](#bib.bib7), [2024](#bib.bib8))。Argyle et al. ([2023](#bib.bib1))
    发现基于人口统计背景的LLM可以模拟人类的投票偏好和语言使用，但没有调查特定主题的意见。Santurkar et al. ([2023](#bib.bib20))
    发现不同模型具有不同的固有意见，这些意见通常与自由主义、高收入、受过良好教育的人群一致，并且通过提供人口统计角色扮演信息无法改变这些意见。目前的论文重复了这一发现，但额外建议对齐度可能通过信念网络进行调整。据我们所知，尚无先前研究探讨此类效应。
- en: Belief networks.
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 信念网络。
- en: A great deal of prior work has studied human belief networks Boutyline and Vaisey
    ([2017](#bib.bib3)); Vlasceanu et al. ([2024](#bib.bib25)); Keating ([2023](#bib.bib15));
    Turner-Zwinkels and Brandt ([2022](#bib.bib24)); Powell et al. ([2023](#bib.bib19));
    Devine ([2015](#bib.bib9)); Jewitt and Goren ([2016](#bib.bib12)); Baldassarri
    and Goldberg ([2014](#bib.bib2)); Brandt and Sleegers ([2021](#bib.bib4)) and
    has developed a range of approaches beyond factor analysis for characterizing
    these including partial correlation networks Turner-Zwinkels and Brandt ([2022](#bib.bib24))
    or Bayesian networks Powell et al. ([2023](#bib.bib19)). Such networks have been
    shown to predict “spillover effects” of attitude changes across related topics
    Turner-Zwinkels and Brandt ([2022](#bib.bib24)); Powell et al. ([2023](#bib.bib19))
    in human participants, where a change in a given topic can ripple through the
    belief network and influence related topics. In the present study, we investigate
    whether we can leverage the belief network derived from human data to construct
    LLM agents that more accurately reflect human opinions.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 许多先前的研究已经探讨了人类信念网络，包括Boutyline和Vaisey ([2017](#bib.bib3)); Vlasceanu et al.
    ([2024](#bib.bib25)); Keating ([2023](#bib.bib15)); Turner-Zwinkels和Brandt ([2022](#bib.bib24));
    Powell et al. ([2023](#bib.bib19)); Devine ([2015](#bib.bib9)); Jewitt和Goren ([2016](#bib.bib12));
    Baldassarri和Goldberg ([2014](#bib.bib2)); Brandt和Sleegers ([2021](#bib.bib4))，并开发了一系列方法来刻画这些网络，包括部分相关网络
    Turner-Zwinkels和Brandt ([2022](#bib.bib24)) 或贝叶斯网络 Powell et al. ([2023](#bib.bib19))。这些网络已被证明可以预测态度变化对相关主题的“溢出效应”
    Turner-Zwinkels和Brandt ([2022](#bib.bib24)); Powell et al. ([2023](#bib.bib19))，即在给定主题发生变化时，这种变化可以在信念网络中传递并影响相关主题。在本研究中，我们探讨了是否可以利用从人类数据中得出的信念网络来构建更准确反映人类意见的LLM代理。
- en: 7 Conclusion
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We investigated the use of empirically-derived belief networks for promoting
    alignment of expressed beliefs between Large Language Model (LLM) agents and twinned
    human participants. We showed that demographic role-playing alone does not produce
    significant alignment Santurkar et al. ([2023](#bib.bib20)), but that initializing
    an agent with a human opinion on one topic then aligns opinions on nearby topics
    within the belief network. The effect does not extend to distant topics within
    the network, and varies depending the strength of the test-topic’s participation
    in the belief network. We found similar effects for in-context learning and supervised
    fine-tuning, for both a proprietary and an open-source LLM. This work highlights
    a novel and potentially powerful means of enhancing LLM agents’ alignment with
    human opinions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了基于经验推导的信念网络在促进LLM代理与配对人类参与者之间表达的信念对齐方面的应用。我们展示了单独进行人口角色扮演并未显著产生对齐 Santurkar
    et al. ([2023](#bib.bib20))，但在一个话题上以人类意见初始化代理，然后使信念网络中的相邻话题的意见对齐。这种效果并未扩展到网络中的遥远话题，并且根据测试话题在信念网络中的参与强度有所变化。我们发现，对于上下文学习和监督微调，这种效果在专有和开源LLM中都表现类似。这项工作突显了一种新颖且可能强大的方法，用于增强LLM代理与人类意见的一致性。
- en: Limitations
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: The scope of topics
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 话题范围
- en: We considered just 18 topics derived from two orthogonal latent factors identified
    in prior work. While the Partisan topics are of public interest and the Ghost
    topics explore an orthogonal dimension, future research could greatly the scope
    of topics.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅考虑了从先前工作中识别出的两个正交潜在因素中推导出的18个话题。虽然党派话题受到公众关注，幽灵话题探索了一个正交维度，但未来的研究可以大大扩展话题范围。
- en: The structure of the belief network.
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 信念网络的结构。
- en: We considered belief networks based on two highly distinct clusters to facilitate
    evaluation. Other studies have used more sophisticated models, such as Bayesian
    networks Powell et al. ([2023](#bib.bib19)), which allow for precise predictions
    about topic interrelations. Future work could apply such methods to better characterize
    belief networks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了基于两个高度不同的簇的信念网络以促进评估。其他研究使用了更复杂的模型，例如贝叶斯网络 Powell et al. ([2023](#bib.bib19))，这些模型允许对话题之间的关系进行精确预测。未来的工作可以应用这些方法以更好地描述信念网络。
- en: The actions of the LLM agents.
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理的行动。
- en: Our LLM agents expressed their opinions through Likert-scale ratings. This facilitated
    direct comparison with human responses but may not fully capture the expression
    of opinions in real-world settings like social media communication. Future studies
    could explore more complex actions (e.g., writing social media posts) to assess
    their human-likeness in realistic applications.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的LLM代理通过Likert量表评分表达了他们的意见。这使得可以直接与人类反应进行比较，但可能无法完全捕捉到像社交媒体交流这样的现实世界场景中的意见表达。未来的研究可以探索更复杂的行动（例如，撰写社交媒体帖子）以评估其在现实应用中的人类相似度。
- en: Ethics Statement
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: We aim to develop LLM agents capable of simulating realistic human communicative
    dynamics, including the expression of potentially harmful beliefs such as misconception
    about the reality of global warming. Our objective is to facilitate a deeper understanding
    of social phenomena like misinformation spread in order to identify strategies
    that mitigate these challenges effectively. Note that under the current setting,
    the LLM agents only produce Likert-scale ratings from a fixed set of options.
    Therefore, they are not able to produce unexpected harmful responses. We will
    release our code base solely for research purposes, and adhere to the terms of
    use by OpenAI’s API ⁴⁴4[https://openai.com/policies/terms-of-use](https://openai.com/policies/terms-of-use)
    and their MIT license ⁵⁵5[https://github.com/openai/openai-openapi/blob/master/LICENSE](https://github.com/openai/openai-openapi/blob/master/LICENSE),
    as well as Mistral AI’s non-production license (MNPL) ⁶⁶6[https://mistral.ai/licenses/MNPL-0.1.md](https://mistral.ai/licenses/MNPL-0.1.md).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们旨在开发能够模拟真实人类交流动态的LLM代理，包括表达潜在有害信念，如对全球变暖现实的误解。我们的目标是促进对社交现象（如虚假信息传播）的深入理解，以便识别有效缓解这些挑战的策略。请注意，在当前设置下，LLM代理仅从固定选项集中产生Likert量表评分。因此，它们无法产生意外的有害回应。我们将仅为研究目的发布我们的代码库，并遵守OpenAI
    API的使用条款 ⁴⁴4[https://openai.com/policies/terms-of-use](https://openai.com/policies/terms-of-use)及其MIT许可证
    ⁵⁵5[https://github.com/openai/openai-openapi/blob/master/LICENSE](https://github.com/openai/openai-openapi/blob/master/LICENSE)，以及Mistral
    AI的非生产许可证（MNPL） ⁶⁶6[https://mistral.ai/licenses/MNPL-0.1.md](https://mistral.ai/licenses/MNPL-0.1.md)。
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Argyle et al. (2023) Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler,
    Christopher Rytting, and David Wingate. 2023. Out of one, many: Using language
    models to simulate human samples. *Political Analysis*, 31(3):337–351.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argyle 等（2023）Lisa P Argyle、Ethan C Busby、Nancy Fulda、Joshua R Gubler、Christopher
    Rytting 和 David Wingate。2023年。《从一而多：使用语言模型模拟人类样本》。*政治分析*，31（3）：337–351。
- en: 'Baldassarri and Goldberg (2014) Delia Baldassarri and Amir Goldberg. 2014.
    Neither ideologues nor agnostics: Alternative voters’ belief system in an age
    of partisan politics. *American Journal of Sociology*, 120(1):45–95.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baldassarri 和 Goldberg（2014）Delia Baldassarri 和 Amir Goldberg。2014年。《既非意识形态者也非不可知论者：在党派政治时代的替代选民信仰体系》。*美国社会学杂志*，120（1）：45–95。
- en: 'Boutyline and Vaisey (2017) Andrei Boutyline and Stephen Vaisey. 2017. Belief
    network analysis: A relational approach to understanding the structure of attitudes.
    *American journal of sociology*, 122(5):1371–1447.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boutyline 和 Vaisey（2017）Andrei Boutyline 和 Stephen Vaisey。2017年。《信仰网络分析：理解态度结构的关系方法》。*美国社会学杂志*，122（5）：1371–1447。
- en: Brandt and Sleegers (2021) Mark J Brandt and Willem WA Sleegers. 2021. Evaluating
    belief system networks as a theory of political belief system dynamics. *Personality
    and Social Psychology Review*, 25(2):159–185.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandt 和 Sleegers（2021）Mark J Brandt 和 Willem WA Sleegers。2021年。《评估信仰体系网络作为政治信仰体系动态的理论》。*个性与社会心理学评论*，25（2）：159–185。
- en: Cattell (1966) Raymond B Cattell. 1966. The scree test for the number of factors.
    *Multivariate behavioral research*, 1(2):245–276.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cattell（1966）Raymond B Cattell。1966年。《因子数量的碎石图检验》。*多变量行为研究*，1（2）：245–276。
- en: Chase (2022) Harrison Chase. 2022. [Langchain](https://github.com/langchain-ai/langchain).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase（2022）Harrison Chase。2022年。[Langchain](https://github.com/langchain-ai/langchain)。
- en: Chuang et al. (2023) Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth
    Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy T Rogers.
    2023. Simulating opinion dynamics with networks of llm-based agents. *arXiv preprint
    arXiv:2311.09618*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chuang 等（2023）Yun-Shiuan Chuang、Agam Goyal、Nikunj Harlalka、Siddharth Suresh、Robert
    Hawkins、Sijia Yang、Dhavan Shah、Junjie Hu 和 Timothy T Rogers。2023年。《使用基于 LLM 的代理的网络模拟意见动态》。*arXiv
    预印本 arXiv:2311.09618*。
- en: 'Chuang et al. (2024) Yun-Shiuan Chuang, Nikunj Harlalka, Siddharth Suresh,
    Agam Goyal, Robert D Hawkins, Sijia Yang, Dhavan V Shah, Junjie Hu, and Timothy T
    Rogers. 2024. The wisdom of partisan crowds: Comparing collective intelligence
    in humans and llm-based agents. In *ICLR 2024 Workshop on Large Language Model
    (LLM) Agents*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chuang 等（2024）Yun-Shiuan Chuang、Nikunj Harlalka、Siddharth Suresh、Agam Goyal、Robert
    D Hawkins、Sijia Yang、Dhavan V Shah、Junjie Hu 和 Timothy T Rogers。2024年。《党派群众的智慧：比较人类与基于
    LLM 的代理的集体智能》。见 *ICLR 2024 大型语言模型（LLM）代理研讨会*。
- en: 'Devine (2015) Christopher J Devine. 2015. Ideological social identity: Psychological
    attachment to ideological in-groups as a political phenomenon and a behavioral
    influence. *Political Behavior*, 37:509–535.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devine（2015）Christopher J Devine。2015年。《意识形态社会身份：对意识形态内群体的心理依附作为一种政治现象和行为影响》。*政治行为*，37：509–535。
- en: 'Feng et al. (2023) Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov.
    2023. From pretraining data to language models to downstream tasks: Tracking the
    trails of political biases leading to unfair nlp models. In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pages 11737–11762.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2023）Shangbin Feng、Chan Young Park、Yuhan Liu 和 Yulia Tsvetkov。2023年。《从预训练数据到语言模型再到下游任务：追踪导致不公平
    NLP 模型的政治偏见踪迹》。见 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，页11737–11762。
- en: Frigo (2022) Vincent V Frigo. 2022. *An Examination of Non-Normative Belief
    Updating Behavior in Humans (Why Is It so Hard to Change Minds?)*. The University
    of Wisconsin-Madison.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frigo（2022）Vincent V Frigo。2022年。*人类非规范信仰更新行为的研究（为何如此难以改变观点？）*。威斯康星大学麦迪逊分校。
- en: Jewitt and Goren (2016) Caitlin E Jewitt and Paul Goren. 2016. Ideological structure
    and consistency in the age of polarization. *American Politics Research*, 44(1):81–105.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jewitt 和 Goren（2016）Caitlin E Jewitt 和 Paul Goren。2016年。《极化时代的意识形态结构与一致性》。*美国政治研究*，44（1）：81–105。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Albert Q Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier 等。2023年。《Mistral 7b》。*arXiv 预印本 arXiv:2310.06825*。
- en: Kaiser (1958) Henry F Kaiser. 1958. The varimax criterion for analytic rotation
    in factor analysis. *Psychometrika*, 23(3):187–200.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser (1958) Henry F Kaiser. 1958. 因子分析中的 Varimax 旋转准则。*心理计量学*，23(3):187–200。
- en: 'Keating (2023) David M Keating. 2023. Persuasive message effects via activated
    and modified belief clusters: toward a general theory. *Human Communication Research*,
    page hqad035.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keating (2023) David M Keating. 2023. 通过激活和修改的信念群体的劝服信息效应：迈向一般理论。*人类传播研究*，页面
    hqad035。
- en: OpenAI (2022) OpenAI. 2022. Introducing ChatGPT. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
    [Accessed 13-10-2023].
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 2022. 介绍 ChatGPT。 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。
    [访问日期 2023年10月13日]。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. *arXiv preprint arXiv:2304.03442*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S Bernstein. 2023. 生成代理：人类行为的交互模拟。*arXiv 预印本 arXiv:2304.03442*。
- en: 'Park et al. (2022) Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2022. Social simulacra: Creating
    populated prototypes for social computing systems. In *Proceedings of the 35th
    Annual ACM Symposium on User Interface Software and Technology*, pages 1–18.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等 (2022) Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel
    Morris, Percy Liang 和 Michael S Bernstein. 2022. 社会模拟：为社会计算系统创建人口原型。在 *第 35 届
    ACM 用户界面软件与技术年会论文集*，第 1–18 页。
- en: 'Powell et al. (2023) Derek Powell, Kara Weisman, and Ellen M Markman. 2023.
    Modeling and leveraging intuitive theories to improve vaccine attitudes. *Journal
    of Experimental Psychology: General*, 152(5):1379.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Powell 等 (2023) Derek Powell, Kara Weisman 和 Ellen M Markman. 2023. 建模和利用直观理论以改善疫苗态度。*实验心理学杂志：综合*，152(5):1379。
- en: Santurkar et al. (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
    Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models
    reflect? In *International Conference on Machine Learning*, pages 29971–30004\.
    PMLR.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santurkar 等 (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee,
    Percy Liang 和 Tatsunori Hashimoto. 2023. 语言模型反映了谁的观点？在 *国际机器学习大会*，第 29971–30004
    页。PMLR。
- en: 'Shao et al. (2023) Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023.
    Character-llm: A trainable agent for role-playing. *arXiv preprint arXiv:2310.10158*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao 等 (2023) Yunfan Shao, Linyang Li, Junqi Dai 和 Xipeng Qiu. 2023. Character-llm:
    一个可训练的角色扮演代理。*arXiv 预印本 arXiv:2310.10158*。'
- en: 'Sun et al. (2024) Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung
    Lee, Bernard J Jansen, and Jang Hyun Kim. 2024. Random silicon sampling: Simulating
    human sub-population opinion using a large language model based on group-level
    demographic information. *arXiv preprint arXiv:2402.18144*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2024) Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung
    Lee, Bernard J Jansen 和 Jang Hyun Kim. 2024. 随机硅样本：基于群体级人口统计信息的大型语言模型模拟人类子群体意见。*arXiv
    预印本 arXiv:2402.18144*。
- en: Taubenfeld et al. (2024) Amir Taubenfeld, Yaniv Dover, Roi Reichart, and Ariel
    Goldstein. 2024. Systematic biases in llm simulations of debates. *arXiv preprint
    arXiv:2402.04049*.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taubenfeld 等 (2024) Amir Taubenfeld, Yaniv Dover, Roi Reichart 和 Ariel Goldstein.
    2024. LLM 辩论模拟中的系统性偏差。*arXiv 预印本 arXiv:2402.04049*。
- en: Turner-Zwinkels and Brandt (2022) Felicity M Turner-Zwinkels and Mark J Brandt.
    2022. Belief system networks can be used to predict where to expect dynamic constraint.
    *Journal of Experimental Social Psychology*, 100:104279.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turner-Zwinkels 和 Brandt (2022) Felicity M Turner-Zwinkels 和 Mark J Brandt.
    2022. 信念系统网络可以用来预测期望的动态约束。*实验社会心理学杂志*，100:104279。
- en: 'Vlasceanu et al. (2024) Madalina Vlasceanu, Ari M Dyckovsky, and Alin Coman.
    2024. A network approach to investigate the dynamics of individual and collective
    beliefs: Advances and applications of the bending model. *Perspectives on Psychological
    Science*, 19(2):444–453.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vlasceanu 等 (2024) Madalina Vlasceanu, Ari M Dyckovsky 和 Alin Coman. 2024. 网络方法调查个人和集体信念的动态：弯曲模型的进展和应用。*心理科学视角*，19(2):444–453。
- en: 'Zhou et al. (2023) Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei
    Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig,
    et al. 2023. Sotopia: Interactive evaluation for social intelligence in language
    agents. *arXiv preprint arXiv:2310.11667*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 (2023) Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu,
    Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig
    等. 2023. Sotopia: 语言代理中的社会智能交互评估。*arXiv 预印本 arXiv:2310.11667*。'
- en: Appendix A List of the 64 Topics in the Belief Survey
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 信念调查中的 64 个主题列表
- en: Table LABEL:tab:list_topic shows the full stetements of the 64 topics in the
    Belief Survey, including the topic category to which they belong according to
    the factor analysis result, along with whether they belong to the training or
    the test partition.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Table LABEL:tab:list_topic 显示了信仰调查中64个主题的完整陈述，包括根据因子分析结果所属的主题类别，以及它们是否属于训练集或测试集。
- en: 'Table 4: The statements of the 64 topics in the Belief Survey, including the
    topic category to which they belong according to the factor analysis result.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 4: 信仰调查中64个主题的陈述，包括根据因子分析结果所属的主题类别。 |'
- en: '| Topic Category | Topic Name | Topic Statement |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 主题类别 | 主题名称 | 主题陈述 |'
- en: '| Ghost | Dead Talk | No one is able to converse with the dead. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 幽灵 | 死者对话 | 无法与死者交谈。 |'
- en: '|  | Ghost | After someone has died it is not possible to see his or her ghost.
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | 幽灵 | 人死后无法看到其幽灵。 |'
- en: '|  | Alien Visit | Intelligent beings from outer space have not visited the
    Earth via spaceships. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | 外星人访问 | 外太空的智能生物未曾通过飞船访问地球。'
- en: '|  | Soul Walk | It is not possible for anyone to project their soul out of
    their body. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | 灵魂出体 | 没有人可以将自己的灵魂投射到体外。 |'
- en: '|  | See Future | No one is capable of having visions that accurately predict
    future events. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | 未来预见 | 没有人能够准确预测未来事件。 |'
- en: '|  | Astrology | The position of the planets at the time of your birth has
    no influence on your personality. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | 占星术 | 出生时行星的位置对你的性格没有影响。 |'
- en: '|  | Roswell | No alien spacecraft has ever crashed near Roswell, New Mexico.
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | 罗斯威尔 | 从未有外星飞船在新墨西哥州的罗斯威尔附近坠毁。 |'
- en: '|  | Past Life | Nobody can accurately remember living a past life. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | 过去的生活 | 没有人能准确记得自己曾经的过去生活。 |'
- en: '|  | The Secret | Strongly visualizing your fondest wish does not make it more
    likely to become a reality. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | 秘密 | 强烈地想象你最渴望的愿望并不会使其更有可能成为现实。 |'
- en: '|  | Aura | Health cannot be improved by manipulating a person’s aura or electrical
    field. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | 气场 | 通过操控一个人的气场或电场无法改善健康。 |'
- en: '|  | Luck | “Lucky streaks” where random events are more likely to favor a
    person are not real. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | 运气 | “幸运连连”即随机事件更有可能偏向某个人并不真实。 |'
- en: '|  | Dousing | Nobody can sense water using only a forked stick. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | 探测水源 | 无法仅通过一个分叉的棍子感知水源。 |'
- en: '| Psychics | Pyrokinesis | Nobody can start fires just by thinking about it.
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 通灵者 | 火焰操控 | 没有人能仅仅通过思考点燃火焰。 |'
- en: '|  | Thought Control | Nobody can control another’s actions with their mind.
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | 思维控制 | 没有人可以通过思维控制他人的行为。 |'
- en: '|  | Food | Food dropped on the ground for less than five seconds can become
    contaminated. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | 食物 | 食物掉在地上不到五秒钟就可能被污染。 |'
- en: '|  | Palm Reading | It is not possible to predict future life events from markings
    on a person’s palm. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | 手相阅读 | 无法通过手上的纹路预测未来的生活事件。 |'
- en: '|  | Telekinesis | No one is capable of moving objects with his or her mind.
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | 超能力 | 没有人能够通过思维移动物体。 |'
- en: '|  | Witches | Witches cannot influence events by using magic. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | 女巫 | 女巫无法通过魔法影响事件。 |'
- en: '|  | Mind Reading | No one is capable of reading another person’s thoughts.
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | 心灵感应 | 没有人能够读取他人的思想。 |'
- en: '|  | Moon Landing | US astronauts have landed on the moon. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | 登月 | 美国宇航员已经登陆月球。 |'
- en: '|  | Crystals | Crystals do not have unexplained powers. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | 水晶 | 水晶没有未解释的神秘力量。 |'
- en: '|  | Lightning | Lightning can strike twice in the same place. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | 闪电 | 闪电可以在同一地点发生两次。 |'
- en: '|  | Alien Abd | Human beings have not been abducted by aliens from outer space.
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | 外星人绑架 | 人类未曾被外星人从外太空绑架。 |'
- en: '| Religion | God | God does not exist. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 宗教 | 神 | 神并不存在。 |'
- en: '|  | Prayer | Prayer cannot cure illness. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | 祈祷 | 祈祷无法治愈疾病。 |'
- en: '|  | Angels | Angels are not real. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | 天使 | 天使并不存在。 |'
- en: '|  | Religion Explain | Religion does not provide the most accurate explanation
    for how the universe came into existence. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | 宗教解释 | 宗教并未提供最准确的宇宙起源解释。 |'
- en: '|  | Evil Spirit | It is not possible for a person’s actions to be controlled
    by an evil spirit. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | 邪灵 | 个人的行为不可能被邪灵控制。 |'
- en: '|  | Science Expl | Everything that happens can eventually be explained by
    science. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | 科学解释 | 一切发生的事情最终都可以通过科学来解释。 |'
- en: '|  | Miracles | Miracles that defy the laws of nature cannot happen. |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | 奇迹 | 违反自然法则的奇迹是不可能发生的。 |'
- en: '|  | Evolution | Species living on the Earth today have not always existed
    in their present form. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | 进化论 | 今天地球上的物种并非一直以其现有的形式存在。 |'
- en: '| Trump | Homicide | In the US, about 80% of white homicide victims are killed
    by white people. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 特朗普 | 谋杀 | 在美国，大约80%的白人谋杀受害者是被白人杀害的。 |'
- en: '|  | Trump Inaug | More people attended the inauguration of Barack Obama than
    the inauguration of Donald Trump. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | 特朗普就职 | 巴拉克·奥巴马的就职典礼的观众人数多于唐纳德·特朗普的就职典礼。 |'
- en: '|  | Kenya | Barack Obama was born in Hawaii. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | 肯尼亚 | 巴拉克·奥巴马出生在夏威夷。 |'
- en: '|  | US Employment | The US unemployment rate in 2016 was lower than 40%. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | 美国就业 | 2016年美国的失业率低于40%。 |'
- en: '|  | Gov Reg | Government regulations do not always stifle economic growth.
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | 政府监管 | 政府监管并不总是抑制经济增长。'
- en: '|  | Holocaust | The Nazi government in Germany murdered approximately 6 million
    Jewish people during the second world war. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  | 大屠杀 | 纳粹德国政府在第二次世界大战期间谋杀了约600万犹太人。 |'
- en: '|  | Trump Votes | Hilary Clinton received the most overall votes in the 2016
    Presidential election. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | 特朗普选票 | 希拉里·克林顿在2016年总统选举中获得了最多的选票。 |'
- en: '|  | Abortion | Strongly Republican states have higher rates of abortion than
    strongly Democratic states. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | 堕胎 | 强烈支持共和党的州的堕胎率高于强烈支持民主党的州。 |'
- en: '|  | Dem Guns | The official platform of the Democratic Party does not seek
    to repeal the 2nd Amendment. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | 民主党的枪支政策 | 民主党的官方平台并不寻求废除第二修正案。 |'
- en: '|  | Health Insurance | Since the Affordable Care Act (Obamacare) passed, more
    Americans have health insurance. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | 健康保险 | 自《平价医疗法案》（奥巴马医改）通过以来，更多的美国人拥有健康保险。 |'
- en: '| Partisan | Gun Control | States with stricter gun control laws have fewer
    gun deaths per capita. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 党派 | 枪支管控 | 拥有更严格枪支管控法律的州每人枪支死亡人数较少。 |'
- en: '|  | US Deficit | The US deficit decreased after President Obama was elected.
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | 美国赤字 | 奥巴马总统当选后，美国的财政赤字有所减少。 |'
- en: '|  | Globe Human | Human activity is causing the globe to warm. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | 全球变暖 | 人类活动正在导致全球变暖。 |'
- en: '|  | Globe Warm | The global climate is rapidly growing warmer. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | 全球变暖 | 全球气候正在迅速变暖。 |'
- en: '|  | Unions | States with strong union protections have lower unemployment
    than states without such protections. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | 工会 | 拥有强大工会保护的州的失业率低于没有这些保护的州。 |'
- en: '|  | Death Penalty | States that have the death penalty have higher rates of
    violent crime on average. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | 死刑 | 实行死刑的州的暴力犯罪率通常更高。 |'
- en: '| Economic | US Taxes | The United States doesn’t have the highest federal
    income tax rate of any Western country. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 经济 | 美国税收 | 美国的联邦所得税率并不是所有西方国家中最高的。 |'
- en: '|  | Deport | President G. W. Bush deported fewer undocumented immigrants than
    President Obama. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | 驱逐 | 小布什总统驱逐的无证移民数量少于奥巴马总统。 |'
- en: '|  | Low Taxes | Lowering taxes does not always lead to economic growth. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | 低税收 | 降低税收并不总是会导致经济增长。 |'
- en: '|  | Bailout | The rescue of big banks by the federal government aided recovery
    from the 2008 recession. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | 救助 | 联邦政府对大银行的救助有助于从2008年衰退中恢复。 |'
- en: '|  | Gold Stand | Returning to the Gold Standard would make the US more vulnerable
    to a recession. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | 金本位 | 回到金本位将使美国更容易受到经济衰退的影响。 |'
- en: '| LowInfo | Refugees | In 2016 fewer than 100,000 refugees from the Middle
    East were granted permission to live in the United States. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 低信息 | 难民 | 2016年来自中东的难民中，获得在美国居住许可的少于100,000人。 |'
- en: '|  | US Crime | The violent crime rate in the US has declined over the past
    10 years. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | 美国犯罪 | 过去10年中，美国的暴力犯罪率有所下降。 |'
- en: '|  | Earth Age | The Earth is not around 6,000 years old. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | 地球年龄 | 地球的年龄并不是约6,000年。 |'
- en: '|  | Human Trex | The Tyrannosaurus Rex and humans did not live on the Earth
    at the same time. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | 人类与霸王龙 | 霸王龙和人类并没有同时存在于地球上。 |'
- en: '|  | Pub Priv | For a given level of education, private-sector workers typically
    earn more than government workers. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | 公私部门 | 在相同教育水平下，私营部门的员工通常比政府员工赚得更多。 |'
- en: '| Health | Bod Cleanse | A “body cleanse” in which you consume only particular
    kinds of nutrients over 1-3 days does not help your body to eliminate toxins.
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 健康 | 身体排毒 | 进行“身体排毒”，即在1-3天内只摄取特定种类的营养素，并不能帮助身体排除毒素。 |'
- en: '|  | Organic | Organic foods are not healthier to eat than non-organic foods.
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | 有机食品 | 有机食品并不比非有机食品更健康。 |'
- en: '|  | Fasting | Regular fasting will not improve your health. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | 禁食 | 定期禁食不会改善你的健康。 |'
- en: '| Conspiracy | Twin Towers | The twin towers were not brought down from the
    inside by explosives during the 9/11 attack. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 阴谋论 | 双子塔 | 双子塔在9/11袭击中并不是被内部的爆炸物拆毁的。 |'
- en: '|  | JFK | Only one gunman was involved in the assassination of John F. Kennedy.
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | JFK | 只有一名枪手参与了对约翰·F·肯尼迪的刺杀。 |'
- en: '|  | Pearl Harbor | President Roosevelt did not know about the attack on Pearl
    Harbor ahead of time. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | 珍珠港 | 罗斯福总统事先不知道珍珠港袭击的情况。 |'
- en: '|  | Vaccinations | Vaccinations cannot cause Autism. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | 疫苗接种 | 疫苗接种不会导致自闭症。 |'
- en: Appendix B The Prompts for LLM Agent Construction Through In-context Learning
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LLM代理构建的提示通过上下文学习
- en: 'Table [5](#A2.T5 "Table 5 ‣ Appendix B The Prompts for LLM Agent Construction
    Through In-context Learning ‣ Beyond Demographics: Aligning Role-playing LLM-based
    Agents Using Human Belief Networks") shows the prompts we use to construct and
    query the LLM agents in the in-context learning setting (§[3.3](#S3.SS3 "3.3 LLM
    Agent Construction ‣ 3 Methods ‣ Beyond Demographics: Aligning Role-playing LLM-based
    Agents Using Human Belief Networks")). Different LLM agent construction conditions
    include various sets of the prompt types. The parts enclosed in curly brackets
    “$\{\}$), where they are filled with actual information from either the respondents
    or the belief survey. As shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief
    Networks") and §[3.3](#S3.SS3 "3.3 LLM Agent Construction ‣ 3 Methods ‣ Beyond
    Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks"),
    in the None condition, only the “Query” prompt is included. In the Demo condition,
    both the prompt types “Demographics” and “Query” are included. In the Demo + Train
    conditions (both [same category] and [different category]), the prompt types include
    “Demographics”, “Training Topic Opinion”, and “Query”. In the Demo + Train + Query
    condition, the prompt types include “Demographics”, “Training Topic Opinion”,
    “Query Topic Opinion”, and “Query”.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A2.T5 "表 5 ‣ 附录 B LLM代理构建的提示通过上下文学习 ‣ 超越人口统计：使用人类信念网络对角色扮演的LLM代理进行对齐")展示了我们在上下文学习设置中构建和查询LLM代理所使用的提示
    (§[3.3](#S3.SS3 "3.3 LLM代理构建 ‣ 3 方法 ‣ 超越人口统计：使用人类信念网络对角色扮演的LLM代理进行对齐"))。不同的LLM代理构建条件包括各种提示类型的集合。用花括号“$\{\}$”括起来的部分，填入实际信息来自受访者或信念调查。正如图 [3](#S1.F3
    "图 3 ‣ 1 引言 ‣ 超越人口统计：使用人类信念网络对角色扮演的LLM代理进行对齐")和§[3.3](#S3.SS3 "3.3 LLM代理构建 ‣ 3
    方法 ‣ 超越人口统计：使用人类信念网络对角色扮演的LLM代理进行对齐")中所示，在无条件下，仅包含“查询”提示。在演示条件下，包含“人口统计”和“查询”两种提示类型。在演示
    + 培训条件（包括[相同类别]和[不同类别]），提示类型包括“人口统计”、“培训主题观点”和“查询”。在演示 + 培训 + 查询条件下，提示类型包括“人口统计”、“培训主题观点”、“查询主题观点”和“查询”。
    |
- en: '| Prompt Type | Message Type (LangChain) | Prompt Template | Example |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 | 消息类型 (LangChain) | 提示模板 | 示例 |'
- en: '| Demographics | System Message | You are role playing a real person. You are
    a {demo_gender}. You are {demo_age} years old. The highest education You have
    completed is {demo_education}. Your race is {demo_race}. Your household income
    is {demo_income}. The population of your city is {demo_city_pop}. You would characterize
    your hometown as {demo_urban_rural}, and you are from the state of {demo_state}.
    Your political leaning is {demo_party}. | You are role playing a real person.
    You are a {Male}. You are {41} years old. The highest education You have completed
    is {Some college but no degree}. Your race is {White}. Your household income is
    {$40,000-$59,999}. The population of your city is {100,000 - 500,000}. You would
    characterize your hometown as {Urban (City)}, and you are from the state of {Florida}.
    Your political leaning is {Democrat}. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 人口统计 | 系统消息 | 你正在扮演一个真实的人物。你是{demo_gender}。你{demo_age}岁。你完成的最高学历是{demo_education}。你的种族是{demo_race}。你的家庭收入是{demo_income}。你所在城市的人口是{demo_city_pop}。你会将你的家乡描述为{demo_urban_rural}，你来自{demo_state}州。你的政治倾向是{demo_party}。
    | 你正在扮演一个真实的人物。你是{男性}。你{41}岁。你完成的最高学历是{部分大学但没有学位}。你的种族是{白人}。你的家庭收入是{$40,000-$59,999}。你所在城市的人口是{100,000
    - 500,000}。你会将你的家乡描述为{城市}，你来自{佛罗里达}州。你的政治倾向是{民主党}。 |'
- en: '| Training Topic Opinion | System Message | You believe that {training_topic_statement
    ($x_{\text{train}}$)}. | You believe that {States with stricter gun control laws
    have fewer gun deaths per capita.} is {Probably True}. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 培训主题观点 | 系统消息 | 你认为{training_topic_statement ($x_{\text{train}}$)}。 | 你认为{拥有更严格枪支管控法律的州每年枪支死亡人数更少。}是{可能正确的}。
    |'
- en: '| Query Topic Opinion | System Message | You believe that that {query_topic_statement
    ($x_{\text{query}}$)}. | You believe that {The global climate is rapidly growing
    warmer.} is {Certainly True}. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 查询主题意见 | 系统消息 | 你认为 {query_topic_statement ($x_{\text{query}}$)}。| 你认为 {全球气候正在迅速变暖。}
    是 **肯定正确** 的。 |'
- en: '| Query | User Message | Now, what is your opinion on the following statement
    using the following scale of responses?'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '| 查询 | 用户消息 | 现在，请你用以下响应尺度对下面的陈述发表意见？'
- en: '{query_topic_statement ($x_{\text{query}}$)} is Certainly True.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '{query_topic_statement ($x_{\text{query}}$)} 是**肯定正确**的。'
- en: 'Statement: {query_topic_statement ($x_{\text{query}}$)}'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 陈述：{query_topic_statement ($x_{\text{query}}$)}
- en: 'Your opinion on the scale of responses: | Now, what is your opinion on the
    following statement using the following scale of responses?'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你对响应尺度的看法：| 现在，你对以下陈述的看法是使用以下响应尺度？
- en: '{The global climate is rapidly growing warmer.} is Certainly False, {The global
    climate is rapidly growing warmer.} is Probably False, {The global climate is
    rapidly growing warmer.} is Lean False, {The global climate is rapidly growing
    warmer., Probably True that {The global climate is rapidly growing warmer.} is
    Lean True, {The global climate is rapidly growing warmer.} is Certainly True'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '{全球气候正在迅速变暖。} 是 **肯定错误**，{全球气候正在迅速变暖。} 是 **可能错误**，{全球气候正在迅速变暖。} 是 **偏向错误**，{全球气候正在迅速变暖。}
    是 **可能正确**，{全球气候正在迅速变暖。} 是 **偏向正确**，{全球气候正在迅速变暖。} 是 **肯定正确**'
- en: 'Statement: {The global climate is rapidly growing warmer.}'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 陈述：{全球气候正在迅速变暖。}
- en: 'Your opinion on the scale of responses: |'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你对响应尺度的看法：|
- en: 'Table 5: The prompts used for the LLM agent construction and querying in the
    in-context learning setting.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在上下文学习设置中用于 LLM 代理构建和查询的提示。
- en: Appendix C The Prompts for LLM Agent Construction Through Supervised Fine-tuning
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 通过监督微调构建 LLM 代理的提示
- en: 'Table [6](#A3.T6 "Table 6 ‣ Appendix C The Prompts for LLM Agent Construction
    Through Supervised Fine-tuning ‣ Beyond Demographics: Aligning Role-playing LLM-based
    Agents Using Human Belief Networks") shows the prompts we use to construct and
    query the LLM agents in the supervised fine-tuning setting (§[3.3](#S3.SS3 "3.3
    LLM Agent Construction ‣ 3 Methods ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks")). The demographic information is
    included in the system message in the same prompt template as in §[B](#A2 "Appendix
    B The Prompts for LLM Agent Construction Through In-context Learning ‣ Beyond
    Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks").
    For the topic-specific opinions, however, instead of including them in the prompt,
    we formulate them as (prompt, response) pairs for supervised fine-tuning, where
    prompt is the input and response is the output. The prompt templates and examples
    are shown in Table [6](#A3.T6 "Table 6 ‣ Appendix C The Prompts for LLM Agent
    Construction Through Supervised Fine-tuning ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks").'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#A3.T6 "表 6 ‣ 附录 C 通过监督微调构建 LLM 代理的提示 ‣ 超越人口统计学：利用人类信念网络对角色扮演 LLM 代理进行对齐")
    显示了我们在监督微调设置中构建和查询 LLM 代理时使用的提示（§[3.3](#S3.SS3 "3.3 LLM 代理构建 ‣ 3 种方法 ‣ 超越人口统计学：利用人类信念网络对角色扮演
    LLM 代理进行对齐")）。人口统计信息包含在系统消息中，使用与 §[B](#A2 "附录 B 通过上下文学习构建 LLM 代理的提示 ‣ 超越人口统计学：利用人类信念网络对角色扮演
    LLM 代理进行对齐") 相同的提示模板中。然而，对于主题特定的意见，我们不将其包含在提示中，而是将其制定为（提示，响应）对用于监督微调，其中提示是输入，响应是输出。提示模板和示例见表 [6](#A3.T6
    "表 6 ‣ 附录 C 通过监督微调构建 LLM 代理的提示 ‣ 超越人口统计学：利用人类信念网络对角色扮演 LLM 代理进行对齐")。
- en: '| Prompt Template | Example Prompt | Response Template | Example Response |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 提示模板 | 示例提示 | 响应模板 | 示例响应 |'
- en: '| What is your opinion on the following statement using the following scale
    of responses?'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '| 你对以下陈述的看法是使用以下响应尺度？'
- en: Certainly False that {query_topic_statement ($x_{\text{query}}$)}.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '**肯定错误** 那 {query_topic_statement ($x_{\text{query}}$)}。'
- en: 'Please choose your response from the following list of options: Certainly False,
    Probably False, Maybe False, Maybe True, Probably True, Certainly True. | What
    is your opinion on the following statement using the following scale of responses?'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 请从以下选项中选择你的回答：**肯定错误**、**可能错误**、**也许错误**、**也许正确**、**可能正确**、**肯定正确**。 | 你对以下陈述的看法是使用以下响应尺度？
- en: 'Certainly False that {States with stricter gun control laws have fewer gun
    deaths per capita}, Probably False that {States with stricter gun control laws
    have fewer gun deaths per capita}, Maybe False that {States with stricter gun
    control laws have fewer gun deaths per capita}, Maybe True that {States with stricter
    gun control laws have fewer gun deaths per capita}, Probably True that {States
    with stricter gun control laws have fewer gun deaths per capita}, Certainly True
    that {States with stricter gun control laws have fewer gun deaths per capita}
    Statement: {States with stricter gun control laws have fewer gun deaths per capita}'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 确定错误 {那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少}，可能错误 {那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少}，可能错误
    {那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少}，可能正确 {那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少}，可能正确 {那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少}，确定正确
    {那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少} 陈述：{那些拥有更严格枪支管控法律的州每年每千人枪支死亡人数较少}
- en: 'Please choose your response from the following list of options: Certainly False,
    Probably False, Maybe False, Maybe True, Probably True, Certainly True. | My Response:
    {opinion_response} | My Response: {Certainly True} |'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请选择以下选项中的响应：确定错误、可能错误、可能错误、可能正确、可能正确、确定正确。 | 我的响应：{opinion_response} | 我的响应：{确定正确}
    |
- en: 'Table 6: The prompts used for the LLM agent construction and querying in the
    supervised fine-tuning setting.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在监督微调设置中用于 LLM 代理构建和查询的提示。
- en: Appendix D The Choice of Number of Factors in Factor Analysis
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 因子数量的选择
- en: '![Refer to caption](img/606ebf5b25f25d80f32dabdea3ffaa3f.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/606ebf5b25f25d80f32dabdea3ffaa3f.png)'
- en: 'Figure 4: The scree plot of the factor analysis solution.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：因子分析解的碎石图。
- en: 'To determine the number of factors to retain in our factor analysis (FA), we
    visualize the scree plot in Figure [4](#A4.F4 "Figure 4 ‣ Appendix D The Choice
    of Number of Factors in Factor Analysis ‣ Beyond Demographics: Aligning Role-playing
    LLM-based Agents Using Human Belief Networks"). We see that the explained variance
    plateaus after including 9 factors (the “elbow point”). Therefore, we decide to
    retain 9 factors.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为确定在因子分析（FA）中保留的因子数量，我们可视化了图 [4](#A4.F4 "图 4 ‣ 附录 D 因子数量的选择 ‣ 超越人口统计学：通过人类信念网络对角色扮演
    LLM 基于代理的对齐") 中的碎石图。我们看到在包含 9 个因子后解释的方差趋于平稳（即“肘点”）。因此，我们决定保留 9 个因子。
- en: Appendix E Supervised Fine-tuning Details
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 监督微调细节
- en: In this section, we elaborate the different strategies used for constructing
    LLM agents through supervised fine-tuning.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细阐述了通过监督微调构建 LLM 代理所使用的不同策略。
- en: a.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: a.
- en: 'None: Baseline without fine-tuning, (identical to same condition in ICL.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: None：没有微调的基线（与 ICL 中的相同条件一致）。
- en: b.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b.
- en: 'Demo: Baseline without fine-tuning, identical to same condition in ICL.'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Demo：没有微调的基线，与 ICL 中的相同条件一致。
- en: c.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: c.
- en: 'Demo+Train [same category]: For each topic category we constructed the dataset
    $\mathcal{D}_{\text{SFT}}=\{(d_{i},x_{\text{train},i}),o_{\text{train},i}\}_{i=1}^{N}$
    ⁷⁷7For example, we fine-tuned an LLM on the respondents’ opinions on the training
    topic for the Ghost category, then queried its opinion on the test topics in the
    Ghost category.. This is the critical condition of interest that tests cross-topic
    generalization. The verbatim prompts are in §[C](#A3 "Appendix C The Prompts for
    LLM Agent Construction Through Supervised Fine-tuning ‣ Beyond Demographics: Aligning
    Role-playing LLM-based Agents Using Human Belief Networks").'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Demo+Train [相同类别]：对于每个话题类别，我们构建了数据集 $\mathcal{D}_{\text{SFT}}=\{(d_{i},x_{\text{train},i}),o_{\text{train},i}\}_{i=1}^{N}$
    ⁷⁷7例如，我们在 Ghost 类别的受访者意见上对 LLM 进行了微调，然后查询它对 Ghost 类别测试话题的意见。这是测试跨话题泛化的关键条件。逐字提示见
    §[C](#A3 "附录 C 通过监督微调构建 LLM 代理的提示 ‣ 超越人口统计学：通过人类信念网络对角色扮演 LLM 基于代理的对齐")。
- en: d.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d.
- en: 'Demo+Train [different category]: Similar to Demo+Train [same category] condition,
    but the training topic opinion ($x_{\text{train}}^{\dagger}$, allowing us to assess
    whether generalization is restricted to topics in the same belief category.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Demo+Train [不同类别]：类似于 Demo+Train [相同类别] 条件，但训练话题意见（$x_{\text{train}}^{\dagger}$），以评估是否泛化仅限于相同信念类别中的话题。
- en: 'ChatGPT (gpt-3.5-turbo-0125) is fine-tuned through OpenAI’s fine-tuning API
    ⁸⁸8[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning).
    These were the hyper-parameters used in fine-tuning:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT（gpt-3.5-turbo-0125）通过 OpenAI 的微调 API 进行微调 ⁸⁸8[https://platform.openai.com/docs/guides/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)。这些是微调中使用的超参数：
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of Epochs: 3'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练轮数：3
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Batch Size: 1'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量大小：1
- en: •
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning Rate Multiplier: 2'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率乘数：2
- en: Appendix F The Full Factor Analysis Results
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 完整因子分析结果
- en: '![Refer to caption](img/d71f598fcc0278dda314087ccf87d31b.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d71f598fcc0278dda314087ccf87d31b.png)'
- en: 'Figure 5: The factor loading matrix of the Controversial Belief Survey. The
    column indicates the nine factor, and the rows are the 64 topics. Red indicates
    topics that load highly on a factor, gray indicates near 0 loading, and blue indicates
    loading in the negative direction. We focus on the Ghost category and Partisan
    categories, highlighted by the green box and the violet box respectively. The
    topics in the Ghost category has minimal loading on the Partisan factor and vice
    versa (highlighted by the black boxes). The full statement of each topic is in
    Table LABEL:tab:list_topic (§[A](#A1 "Appendix A List of the 64 Topics in the
    Belief Survey ‣ Beyond Demographics: Aligning Role-playing LLM-based Agents Using
    Human Belief Networks")).'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：争议性信念调查的因子载荷矩阵。列表示九个因子，行表示64个主题。红色表示在某因子上载荷高的主题，灰色表示接近 0 的载荷，蓝色表示负载方向的载荷。我们重点关注幽灵类别和党派类别，分别用绿色框和紫色框突出显示。幽灵类别的主题在党派因子上的载荷最小，反之亦然（用黑色框突出显示）。每个主题的完整陈述见表LABEL:tab:list_topic（§[A](#A1
    "附录 A 64 个主题的列表 ‣ 超越人口统计学：使用人类信念网络对角色扮演 LLM 基 Agent 进行对齐")）。
- en: 'In Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Beyond Demographics: Aligning
    Role-playing LLM-based Agents Using Human Belief Networks") in the main text,
    we only show the factor loading matrix of the Ghost and the Partisan factors,
    and the corresponding topics. In this section, we discuss the full factor analysis
    result.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在主文本中的图[2](#S1.F2 "图 2 ‣ 1 引言 ‣ 超越人口统计学：使用人类信念网络对角色扮演 LLM 基 Agent 进行对齐")，我们只展示了幽灵和党派因子的因子载荷矩阵及其相应的主题。在本节中，我们讨论完整的因子分析结果。
- en: 'The factor analysis reveals nine latent factors underlying the 64 topics. Figure [5](#A6.F5
    "Figure 5 ‣ Appendix F The Full Factor Analysis Results ‣ Beyond Demographics:
    Aligning Role-playing LLM-based Agents Using Human Belief Networks") shows the
    full factor loading matrix. The red blocks highlight strong correlations among
    opinions within each factor, indicating that endorsing one conception in a cluster
    often predicts opinion in other conceptions within the same cluster. We assign
    the name of each factor based on its constituent topics: Ghost, Psychics, Religion,
    Trump, Partisan, Economic, LowInfo, Health, and Conspiracy. The 64 topics are
    categorized by which factor they have the highest loadings on. For instance, the
    topic about communication with the dead belongs to the Ghost category because
    it has the highest loading on the Ghost factor (Table LABEL:tab:list_topic shows
    the full list of topics and categories).'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分析揭示了64个主题背后的九个潜在因子。图[5](#A6.F5 "图 5 ‣ 附录 F 完整因子分析结果 ‣ 超越人口统计学：使用人类信念网络对角色扮演
    LLM 基 Agent 进行对齐")展示了完整的因子载荷矩阵。红色块突出显示了每个因子内意见之间的强相关性，表明在一个簇中支持某一观念通常会预测在同一簇中其他观念的意见。我们根据每个因子的组成主题为其命名：幽灵、通灵、宗教、特朗普、党派、经济、低信息、健康和阴谋。64个主题按其在因子上的最高载荷进行分类。例如，关于与死者沟通的主题属于幽灵类别，因为它在幽灵因子上有最高的载荷（表LABEL:tab:list_topic展示了完整的主题和类别列表）。
- en: Appendix G Compute Resources
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 计算资源
- en: We ran all experiments with Mistral on a GPU machine equipped with 1x NVIDIA
    A100\. The experiments with ChatGPT cost about 300 USD.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在配备 1x NVIDIA A100 的 GPU 机器上运行了所有实验。使用 ChatGPT 的实验花费了约 300 美元。
