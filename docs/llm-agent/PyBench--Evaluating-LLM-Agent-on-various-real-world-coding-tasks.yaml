- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:40:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:40:57'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PyBench: Evaluating LLM Agent on various real-world coding tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyBench：评估LLM Agent在各种现实世界编码任务中的表现
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16732](https://ar5iv.labs.arxiv.org/html/2407.16732)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16732](https://ar5iv.labs.arxiv.org/html/2407.16732)
- en: 'Yaolun Zhang¹  , Yinxu Pan²¹¹footnotemark: 1 , Yudong Wang²¹¹footnotemark:
    1 , Jie Cai²'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yaolun Zhang¹  , Yinxu Pan²¹¹脚注: 1 , Yudong Wang²¹¹脚注: 1 , Jie Cai²'
- en: Zhi Zheng², Guoyang Zeng², Zhiyuan Liu³,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhi Zheng², Guoyang Zeng², Zhiyuan Liu³,
- en: ¹School of Statistics, Renmin University of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国人民大学统计学院
- en: ²ModelBest Inc.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²ModelBest Inc.
- en: ³Department of Computer Science and Technology, Tsinghua University.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³清华大学计算机科学与技术系
- en: '{zhangyaolun5}@ruc.edu.cn'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{zhangyaolun5}@ruc.edu.cn'
- en: '{panyinxu, wangyudong, caijie, zhengzhi, zengguoyang}@modelbest.cn'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{panyinxu, wangyudong, caijie, zhengzhi, zengguoyang}@modelbest.cn'
- en: '{liuzy}@tsinghua.edu.cn   Equal contribution.  Work done during internship
    at ModelBest Inc.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{liuzy}@tsinghua.edu.cn   平等贡献。工作在ModelBest Inc.实习期间完成。'
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The LLM Agent, equipped with a code interpreter, is capable of automatically
    solving real-world coding tasks, such as data analysis and image editing. However,
    existing benchmarks primarily focus on either simplistic tasks, such as completing
    a few lines of code, or on extremely complex and specific tasks at the repository
    level, neither of which are representative of various daily coding tasks. To address
    this gap, we introduce PyBench, a benchmark encompassing five main categories
    of real-world tasks, covering more than 10 types of files. Given a high-level
    user query and related files, the LLM Agent needs to reason and execute Python
    code via a code interpreter for a few turns before making a formal response to
    fulfill the user’s requirements. Successfully addressing tasks in PyBench demands
    a robust understanding of various Python packages, superior reasoning capabilities,
    and the ability to incorporate feedback from executed code. Our evaluations indicate
    that current open-source LLMs are struggling with these tasks. Hence, we conduct
    analysis and experiments on four kinds of datasets proving that comprehensive
    abilities are needed for PyBench. Our fine-tuned 8B size model: PyLlama3 achieves
    an exciting performance on PyBench which surpasses many 33B and 70B size models.
    Our Benchmark, Training Dataset, and Model are available at: [https://github.com/Mercury7353/PyBench](https://github.com/Mercury7353/PyBench)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了代码解释器的LLM Agent能够自动解决现实世界中的编码任务，如数据分析和图像编辑。然而，现有的基准测试主要集中在简单的任务上，如完成几行代码，或极其复杂且特定的任务，在代码库层面，这些任务都不能代表各种日常编码任务。为了填补这个空白，我们介绍了PyBench，一个涵盖五大类现实世界任务的基准测试，覆盖了10多种文件类型。在给定高级用户查询和相关文件的情况下，LLM
    Agent需要通过代码解释器推理并执行Python代码若干轮，然后才能做出正式回应以满足用户的要求。成功完成PyBench中的任务需要对各种Python包有深入的了解、出色的推理能力，以及能够结合执行代码的反馈。我们的评估表明，当前的开源LLM在这些任务上表现不佳。因此，我们对四种数据集进行了分析和实验，证明了PyBench需要全面的能力。我们微调的8B模型：PyLlama3在PyBench上取得了令人兴奋的表现，超越了许多33B和70B大小的模型。我们的基准测试、训练数据集和模型可以在以下地址获取：[https://github.com/Mercury7353/PyBench](https://github.com/Mercury7353/PyBench)
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/0ab289289eee717429b45b5f65924cd7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0ab289289eee717429b45b5f65924cd7.png)'
- en: 'Figure 1: An Overview of LLMs’ performance on PyBench'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM在PyBench上的表现概述
- en: The best tool is the one that gets the job done. Enormous real-world tasks like
    data analysis and image & audio processing can be solved by code. Among many programming
    languages, Python stands out for its simplicity, ease of use, extensive libraries,
    and high compatibility, making it a widely used tool for daily tasks. However,
    individuals often need to invest significant time in learning how to use extension
    packages, even if they are already highly proficient in Python itself. Thanks
    to LLM’s powerful code capabilities, it can act as an automatic agent Wang et al.
    ([2024a](#bib.bib30)); Park et al. ([2023](#bib.bib24)); Qin et al. ([2023](#bib.bib26))
    writing and executing code to solve a wide spectrum of real-world tasks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的工具是能够完成任务的工具。巨大的现实世界任务，如数据分析和图像与音频处理，可以通过代码解决。在众多编程语言中，Python因其简单性、易用性、丰富的库和高兼容性而脱颖而出，使其成为日常任务中广泛使用的工具。然而，即使在Python本身已经非常熟练的情况下，个人通常仍需要投入大量时间来学习如何使用扩展包。由于LLM强大的代码能力，它可以作为一个自动化代理
    Wang et al. ([2024a](#bib.bib30)); Park et al. ([2023](#bib.bib24)); Qin et al.
    ([2023](#bib.bib26)) 编写和执行代码来解决广泛的现实世界任务。
- en: However, current LLM code benchmarks have not covered the real-world task area.
    HumanEval Chen et al. ([2021](#bib.bib4)) and MBPP Austin et al. ([2021](#bib.bib1))
    focus on function complement and simple Python problems, which could not evaluate
    the usefulness of LLM Agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当前的LLM代码基准尚未涵盖实际任务领域。HumanEval Chen et al. ([2021](#bib.bib4))和MBPP Austin
    et al. ([2021](#bib.bib1))关注于函数补全和简单的Python问题，这些无法评估LLM代理的实际效用。
- en: Although benchmarks such as DS-1000 Lai et al. ([2023](#bib.bib16)), DevBench
    Li et al. ([2024](#bib.bib17)), and SWE-BenchJimenez et al. ([2023](#bib.bib15))
    focus on repository-level coding issues, they assess the ability of LLMs to use
    and manage specific codebases. These tasks are relatively narrow in scope and
    inherently limited, deviating from practical daily application scenarios and being
    overly complex for routine use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像DS-1000 Lai et al. ([2023](#bib.bib16))、DevBench Li et al. ([2024](#bib.bib17))和SWE-Bench
    Jimenez et al. ([2023](#bib.bib15))等基准关注于代码库级别的编码问题，但它们评估了LLM使用和管理特定代码库的能力。这些任务范围相对狭窄，固有局限，偏离实际日常应用场景，并且对日常使用过于复杂。
- en: 'To address the lack of benchmarks for real-world coding tasks, we introduce
    PyBench, a comprehensive and versatile benchmark designed to evaluate the practical
    coding abilities of LLMs. Specifically, we formulated real-world coding tasks
    into 5 main categories: Chart Analysis, Text Analysis, Image & Audio Editing,
    Complex Math, and Software & Website Development. Each category consists of comprehensive
    subclasses of tasks, reflecting real-world situations. Related files are collected
    for each subclass, and queries are tailored for the content of files. For the
    quantitative evaluation of PyBench tasks, we create a set of unit tests to verify
    whether the tasks are solved successfully. We also employ GPT-4 as a judge to
    evaluate the solutions and calculate the Average Turns as a measure of problem-solving
    efficiency. We evaluate 3 types of models on PyBench: Closed-source LLMs, 70B,
    33B, and 7B size Open-source LLMs, and Code LLMs specifically tailored for coding
    tasks. The evaluation results indicate most LLMs struggle to solve PyBench tasks.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对实际编码任务基准的不足，我们引入了PyBench，这是一个全面且多功能的基准，旨在评估LLM的实际编码能力。具体而言，我们将实际编码任务分为5个主要类别：图表分析、文本分析、图像和音频编辑、复杂数学以及软件和网站开发。每个类别包含综合的子任务，反映实际情况。为每个子任务收集相关文件，并根据文件内容定制查询。为了对PyBench任务进行定量评估，我们创建了一组单元测试来验证任务是否成功解决。我们还使用GPT-4作为评审来评估解决方案，并计算平均轮次作为问题解决效率的度量。我们在PyBench上评估了3种类型的模型：封闭源LLM、70B、33B和7B规模的开源LLM，以及专门针对编码任务的代码LLM。评估结果表明，大多数LLM在解决PyBench任务时表现困难。
- en: 'Consequently, we collect and synthesize four datasets: the homologous training
    dataset, multi-turn code interaction dataset, multi-turn chat dataset, and code-rich
    corpus for continue pre-training. We then conduct a series of analyses and experiments
    to figure out what are necessary abilities to solve the PyBench tasks and how
    to improve the LLM performance on real-world coding tasks. The result demonstrates
    merely homologous data could not help the base model adapt to real-world coding
    tasks. The multi-turn code interaction dataset significantly enhances comprehensive
    capabilities. Specifically, the multi-turn chat dataset boosts performance in
    Chart Analysis, while continued pre-training on a code-rich corpus improves Text
    Analysis. Trained on these datasets, our fine-tuned 8B size model PyLlama3 surpasses
    Llama3-8B-Instruct on PyBench.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们收集并综合了四个数据集：同源训练数据集、多轮代码互动数据集、多轮聊天数据集和用于继续预训练的代码丰富语料库。随后，我们进行了一系列分析和实验，以弄清楚解决PyBench任务所需的能力以及如何提高LLM在实际编码任务中的表现。结果表明，仅有同源数据无法帮助基础模型适应实际编码任务。多轮代码互动数据集显著增强了综合能力。具体而言，多轮聊天数据集提升了图表分析的表现，而在代码丰富语料库上继续预训练则改善了文本分析。在这些数据集上训练后，我们精调的8B规模模型PyLlama3在PyBench上超越了Llama3-8B-Instruct。
- en: 'In summary, our contributions are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We construct PyBench, the first comprehensive benchmark for evaluating LLM Agents
    on real-world coding tasks. PyBench includes real-world files and related queries,
    covering a wide range of daily situations and file types.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们构建了PyBench，这是第一个全面的基准，用于评估LLM代理在实际编码任务中的表现。PyBench包括实际的文件和相关查询，涵盖了各种日常情况和文件类型。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present PyBench tasks that necessitate the agent’s comprehensive capabilities.
    Using only homologous data is inadequate for adapting base models to real-world
    coding tasks. The agent’s ability for multi-turn interaction is crucial, and continued
    pre-training on a code-rich corpus is also beneficial.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了需要代理全面能力的 PyBench 任务。仅使用同质数据不足以将基础模型适应于实际编码任务。代理的多轮互动能力至关重要，并且在丰富代码的语料库上持续预训练也会带来好处。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct continued pre-training of Llama3-8B-Base on a code-rich corpus and
    fine-tune it on homologous, multi-turn code, and multi-turn chat datasets. Our
    PyLlama3 model achieves outstanding performance on PyBench.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在丰富的代码语料库上持续预训练 Llama3-8B-Base，并在同质的、多轮代码和多轮对话数据集上进行微调。我们的 PyLlama3 模型在 PyBench
    上表现出色。
- en: '| Category | Subclass | Relative Python Packages |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 子类 | 相关 Python 包 |'
- en: '| --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Chart Analysis | Data Preprocessing, Data Visualization, Machine Learning…
    | pandas, numpy, sklearn, matplotlib… |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 图表分析 | 数据预处理、数据可视化、机器学习… | pandas, numpy, sklearn, matplotlib… |'
- en: '| Text Analysis | Text Based QA, Theme Analysis, Wordcloud… | jieba, wordcloud,
    PyPDF2… |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 文本分析 | 基于文本的 QA、主题分析、词云… | jieba, wordcloud, PyPDF2… |'
- en: '| Image & Audio Editing | Image Generation, Sound Feature Extraction… | opencv,
    PIL, pydub… |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 图像与音频编辑 | 图像生成、声音特征提取… | opencv, PIL, pydub… |'
- en: '| Complex Math | Large Number Calculation, Calculus… | numpy, scipy… |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 复杂数学 | 大数计算、微积分… | numpy, scipy… |'
- en: '| Software & Website Development | Game Design, Website Design… | pygame, bs4…
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 软件与网站开发 | 游戏设计、网站设计… | pygame, bs4… |'
- en: 'Table 1: Type of tasks and related Python packages of PyBench. We only select
    commonly used Python packages for these tasks, though other Python packages may
    also be used for specific tasks.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：PyBench 的任务类型及相关的 Python 包。我们仅选择了这些任务中常用的 Python 包，尽管其他 Python 包也可以用于特定任务。
- en: 2 Related Works
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Benchmark on coding ability
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 编码能力基准测试
- en: Many existing benchmarks focus on LLM’s code ability. HumanEval Chen et al.
    ([2021](#bib.bib4)) and MBPP Austin et al. ([2021](#bib.bib1)) are two widely
    recognized benchmarks primarily evaluating LLM’s ability to complete functions
    or solve simple Python problems. HumanEval-X, HumanEval+, and MBPP+ Zheng et al.
    ([2024a](#bib.bib38)); Liu et al. ([2023a](#bib.bib19)) extend the benchmarks
    by adding multilingual and plenty of extra tests. APPS Hendrycks et al. ([2021](#bib.bib11))
    focuses on writing code from natural language description. TACO Li et al. ([2023](#bib.bib18))
    builds a more complex benchmark evaluating LLM on algorithmic code tasks. MINT
    Wang et al. ([2023a](#bib.bib32)) and $M^{3}$ToolEval Wang et al. ([2024b](#bib.bib31))
    aim to evaluate models’ multi-turn interaction code ability, with tools or human
    feedback. There are also extremely complex and hard benchmarks evaluating LLMs
    on software development Qian et al. ([2023](#bib.bib25)); Hong et al. ([2023](#bib.bib13)),
    code repository issues Jimenez et al. ([2023](#bib.bib15)); Li et al. ([2024](#bib.bib17)),
    and data science tasksLai et al. ([2023](#bib.bib16)). However, these benchmarks
    are all limited to specific scenarios. To the best of our knowledge, no existing
    benchmark evaluates LLM Agent on real-world coding tasks with various situations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现有基准测试关注于 LLM 的代码能力。HumanEval Chen 等人 ([2021](#bib.bib4)) 和 MBPP Austin 等人
    ([2021](#bib.bib1)) 是两个广泛认可的基准，主要评估 LLM 完成函数或解决简单 Python 问题的能力。HumanEval-X、HumanEval+
    和 MBPP+ Zheng 等人 ([2024a](#bib.bib38)); Liu 等人 ([2023a](#bib.bib19)) 通过增加多语言和大量额外测试扩展了这些基准。APPS
    Hendrycks 等人 ([2021](#bib.bib11)) 关注于从自然语言描述中编写代码。TACO Li 等人 ([2023](#bib.bib18))
    建立了一个更复杂的基准，评估 LLM 在算法代码任务中的表现。MINT Wang 等人 ([2023a](#bib.bib32)) 和 $M^{3}$ToolEval
    Wang 等人 ([2024b](#bib.bib31)) 旨在评估模型的多轮互动代码能力，包括工具或人工反馈。还有一些极其复杂和困难的基准评估 LLM 在软件开发
    Qian 等人 ([2023](#bib.bib25)); Hong 等人 ([2023](#bib.bib13))、代码库问题 Jimenez 等人 ([2023](#bib.bib15));
    Li 等人 ([2024](#bib.bib17)) 以及数据科学任务 Lai 等人 ([2023](#bib.bib16)) 上的表现。然而，这些基准测试都局限于特定场景。根据我们所知，目前还没有现有基准评估
    LLM 代理在多种情况下的实际编码任务中的表现。
- en: 2.2 Code LLMs
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 代码 LLM
- en: Previous works enhance LLM’s coding ability through various methods. OpenCodeInterpreter
    Zheng et al. ([2024b](#bib.bib39)) introduces the CodeFeedback dataset and a code
    execution system with feedback, the fine-tuned model achieves a great performance
    on coding benchmarks. CodeAct Wang et al. ([2024b](#bib.bib31)) uses executable
    Python code to unify LLM agents’ action space, enabling sophisticated task execution
    through multi-turn interactions. NexT Ni et al. ([2024](#bib.bib23)) teaching
    LLM reasoning the execution process of code step by step, effectively improving
    the code quality. WizardCoder Luo et al. ([2023](#bib.bib22)), Magicoder Wei et al.
    ([2024](#bib.bib35)), and AlchemistCoder Song et al. ([2024](#bib.bib28)) build
    effective fine-tuning datasets from massive and multi-source data to train advanced
    code LLMs. Pre-training on code-rich data is also a good method to help LLM coding.
    CodeQwen Bai et al. ([2023](#bib.bib2))and Deepseek-Coder Guo et al. ([2024](#bib.bib10));
    DeepSeek-AI et al. ([2024](#bib.bib7)) develop specialized models for coding by
    continuing to pre-train on code data and employing supervised fine-tuning strategies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的工作通过各种方法提升了 LLM 的编码能力。OpenCodeInterpreter Zheng 等人 ([2024b](#bib.bib39))
    引入了 CodeFeedback 数据集和一个带有反馈的代码执行系统，经过微调的模型在编码基准测试中表现出色。CodeAct Wang 等人 ([2024b](#bib.bib31))
    使用可执行的 Python 代码来统一 LLM agents 的行动空间，通过多轮互动实现复杂任务的执行。NexT Ni 等人 ([2024](#bib.bib23))
    教授 LLM 步骤性地推理代码执行过程，有效提升了代码质量。WizardCoder Luo 等人 ([2023](#bib.bib22))、Magicoder
    Wei 等人 ([2024](#bib.bib35)) 和 AlchemistCoder Song 等人 ([2024](#bib.bib28)) 从大量和多源数据中构建了有效的微调数据集，用于训练高级代码
    LLMs。在丰富代码数据上进行预训练也是帮助 LLM 编码的一个好方法。CodeQwen Bai 等人 ([2023](#bib.bib2)) 和 Deepseek-Coder
    Guo 等人 ([2024](#bib.bib10))；DeepSeek-AI 等人 ([2024](#bib.bib7)) 通过继续在代码数据上预训练和采用监督微调策略开发了专门的编码模型。
- en: 2.3 LLM Agent for real-world tasks
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 适用于现实世界任务的 LLM Agent
- en: LLM as Agent is a great Qian et al. ([2023](#bib.bib25)); Park et al. ([2023](#bib.bib24));
    Chen et al. ([2024](#bib.bib6)) attempt utilizing LLM in real-world tasks. ReAct
    Yao et al. ([2022](#bib.bib37)) first introduced Agent’s Reasoning and Action
    Format. Previous works design many frameworks that build and organize LLM Agents
    to complete real-world coding tasks. MetaGPT Hong et al. ([2023](#bib.bib13))
    ChatDev Qian et al. ([2023](#bib.bib25)), DataInterpreter Hong et al. ([2024](#bib.bib12)),
    and MatplotAgent Yang et al. ([2024](#bib.bib36)) employ agents to complete software
    development or data science tasks. AgentCoder Huang et al. ([2023](#bib.bib14))
    focuses on simple code complement tasks. Furthermore, some general multi-agent
    systems try to adapt agents to various tasks Chen et al. ([2023b](#bib.bib5),
    [a](#bib.bib3)); Wang et al. ([2023b](#bib.bib33)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 作为 Agent 是 Qian 等人 ([2023](#bib.bib25))；Park 等人 ([2023](#bib.bib24))；Chen
    等人 ([2024](#bib.bib6)) 尝试将 LLM 应用于现实世界任务的一个重要尝试。ReAct Yao 等人 ([2022](#bib.bib37))
    首次引入了 Agent 的推理和行动格式。以往的工作设计了许多框架来构建和组织 LLM Agents，以完成现实世界中的编码任务。MetaGPT Hong
    等人 ([2023](#bib.bib13))、ChatDev Qian 等人 ([2023](#bib.bib25))、DataInterpreter Hong
    等人 ([2024](#bib.bib12)) 和 MatplotAgent Yang 等人 ([2024](#bib.bib36)) 使用 agents
    来完成软件开发或数据科学任务。AgentCoder Huang 等人 ([2023](#bib.bib14)) 关注于简单的代码补全任务。此外，一些通用的多-agent
    系统尝试将 agents 适应于各种任务 Chen 等人 ([2023b](#bib.bib5), [a](#bib.bib3))；Wang 等人 ([2023b](#bib.bib33))。
- en: 3 PyBench
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 PyBench
- en: '![Refer to caption](img/17f5f2d69e0a14ee2261978ed00ebea1.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17f5f2d69e0a14ee2261978ed00ebea1.png)'
- en: 'Figure 2: The construction and evaluation workflow of PyBench'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：PyBench 的构建和评估工作流
- en: Coding is a core skill for LLM Agents. When the agent needs to solve real-world
    coding tasks, it should not only write executable code but also utilize the results
    of execution to guide subsequent actions and interact with files. Python is a
    powerful programming language with almost 4 million packages, capable of covering
    almost all real-world coding tasks. Therefore, we propose building PyBench to
    evaluate the LLM agent’s ability in reasoning, writing executable Python code,
    and utilizing code results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 编码是 LLM Agents 的核心技能。当 Agent 需要解决现实世界中的编码任务时，它不仅应该编写可执行代码，还应利用执行结果来指导后续操作并与文件交互。Python
    是一种功能强大的编程语言，拥有近 400 万个包，能够涵盖几乎所有现实世界的编码任务。因此，我们建议构建 PyBench 来评估 LLM agent 在推理、编写可执行
    Python 代码和利用代码结果方面的能力。
- en: 3.1 Task Formulation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务制定
- en: 'We first define what kinds of tasks need to be solved as a truly helpful LLM
    Agent equipped with a Python code interpreter. Given a user query $q$, which fulfill
    the user’s requirement:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义需要解决的任务种类，以作为一个真正有用的 LLM Agent，配备一个 Python 代码解释器。给定一个用户查询 $q$，它满足用户的需求：
- en: '|  | $Ans,F_{out}=A(q,F_{in})$ |  | (1) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $Ans,F_{out}=A(q,F_{in})$ |  | (1) |'
- en: where $A$ contains various types of data such as chart, text, audio, image,
    etc., showcase the LLM Agent should adapt to various real-world coding tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 包含各种类型的数据，如图表、文本、音频、图像等，展示 LLM Agent 应适应各种实际编码任务。
- en: 3.2 Task Categories
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 任务类别
- en: 'In the realm of practical coding applications, the LLM Agent is required to
    autonomously tackle a diverse array of real-world coding challenges. To ensure
    a comprehensive evaluation of its capabilities, we have meticulously curated five
    main categories of real-world coding tasks. Each category is designed to test
    the LLM Agent’s proficiency across a broad spectrum of scenarios, ranging from
    data analysis to software development. Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") demonstrates
    these categories:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际编码应用领域，LLM Agent 需要自主解决各种现实编码挑战。为了全面评估其能力，我们精心策划了五个主要的现实编码任务类别。每个类别旨在测试
    LLM Agent 在广泛场景中的熟练程度，从数据分析到软件开发。表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ PyBench:
    Evaluating LLM Agent on various real-world coding tasks") 展示了这些类别：'
- en: Chart Analysis. In the digital age, the ability to efficiently analyze and interpret
    data is indispensable. This category focuses on tasks that involve handling csv
    or xlsx files for various purposes such as data preprocessing, transformation,
    visualization, and the application of machine learning algorithms Hong et al.
    ([2024](#bib.bib12)); Yang et al. ([2024](#bib.bib36)). The output from the LLM
    Agent includes detailed analytical reports, visual representations, or even trained
    machine learning models in pkl or joblib formats.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图表分析。在数字时代，高效分析和解释数据的能力是不可或缺的。本类别专注于处理 csv 或 xlsx 文件的任务，涉及数据预处理、转换、可视化以及机器学习算法的应用（Hong
    et al. ([2024](#bib.bib12)); Yang et al. ([2024](#bib.bib36))）。LLM Agent 的输出包括详细的分析报告、可视化表示，甚至是
    pkl 或 joblib 格式的训练机器学习模型。
- en: Text Analysis. This category encompasses tasks related to processing txt and
    pdf files, including summarization, keyword extraction, word cloud generation,
    and thematic analysis.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析。本类别包括处理 txt 和 pdf 文件的任务，如摘要生成、关键词提取、词云生成和主题分析。
- en: Image & Audio Editing. As visual content becomes increasingly prevalent, the
    demand for personalized image-processing solutions rises. This category evaluates
    the LLM Agent’s ability to manipulate images in png and jpg formats through various
    techniques such as saturation adjustment, merging, cropping, and generating QR
    codes from scratch. Parallel to image processing, this category also focuses on
    the manipulation of audio files (e.g. mp3 and wav). Tasks include volume control,
    audio trimming, and the creation of audio visualizations, reflecting the diverse
    needs of users in the realm of audio content creation and modification.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图像与音频编辑。随着视觉内容的日益普及，对个性化图像处理解决方案的需求也在上升。本类别评估 LLM Agent 通过各种技术（如饱和度调整、合并、裁剪和从头生成二维码）处理
    png 和 jpg 格式图像的能力。与图像处理相平行，本类别还关注音频文件（如 mp3 和 wav）的处理。任务包括音量控制、音频剪辑以及创建音频可视化，反映了用户在音频内容创建和修改方面的多样化需求。
- en: Complex Math. Beyond the capabilities of basic calculators or the Chain of Thought
    methodology Wei et al. ([2022](#bib.bib34)), this category presents challenges
    involving large-scale computations, polynomial equation solving, and advanced
    calculus. It is designed to test LLM Agents’ ability to navigate and solve intricate
    mathematical problems via a code interpreter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂数学。超越基本计算器或思维链方法（Wei et al. ([2022](#bib.bib34))），本类别涉及大规模计算、多项式方程求解和高级微积分等挑战。旨在测试
    LLM Agents 通过代码解释器导航和解决复杂数学问题的能力。
- en: Website & Software Development. This category is dedicated to the practical
    application of coding skills in the development of personal websites and simple
    software projects, such as a Pac-Man game. It assesses the LLM Agent’s ability
    to translate user requirements into functional and interactive applications, showcasing
    its versatility and creativity in software development Hong et al. ([2023](#bib.bib13));
    Qian et al. ([2023](#bib.bib25)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 网站和软件开发。本类别致力于将编码技能应用于个人网站和简单软件项目的开发，例如一个吃豆人游戏。评估 LLM Agent 将用户需求转化为功能性和互动性应用的能力，展示其在软件开发中的多样性和创造力（Hong
    et al. ([2023](#bib.bib13)); Qian et al. ([2023](#bib.bib25))）。
- en: 3.3 Data Collection
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据收集
- en: We collect and filter the files in PyBench from two main sources.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从两个主要来源收集和筛选PyBench中的文件。
- en: Kaggle Data. [Kaggle](https://www.kaggle.com/) is a great platform for machine
    learning, which contains massive datasets. We obtained csv and xlsx data on Kaggle
    through web crawlers. There are two principles of filtering the files. Firstly,
    the files should not be too large, considering the limited memory in the test
    environment. Secondly, the data tables must contain multiple columns with clear
    meaning, simulating commonly used files.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle数据。[Kaggle](https://www.kaggle.com/)是一个很棒的机器学习平台，包含大量数据集。我们通过网络爬虫在Kaggle上获得了csv和xlsx数据。文件筛选有两个原则。首先，文件不应过大，以考虑测试环境中有限的内存。其次，数据表必须包含多个具有明确意义的列，以模拟常用文件。
- en: arXiv Data. We collect pdf and txt from [arXiv](https://arxiv.org/). The papers
    on arXiv are high-quality text with a clear theme and structure, which is suitable
    for text-based QA, Theme Analysis, and drawing word clouds.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: arXiv数据。我们从[arXiv](https://arxiv.org/)收集pdf和txt文件。arXiv上的论文是高质量的文本，具有清晰的主题和结构，适合用于基于文本的问答、主题分析和绘制词云。
- en: Other Sources Data. For other file types, we responsibly collect files, including
    png, jpeg, gif, mp3, and wav from the internet, ensuring that all content respects
    copyright laws, protects user privacy, and is free from harmful elements.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其他来源数据。对于其他文件类型，我们负责地从互联网收集包括png、jpeg、gif、mp3和wav在内的文件，确保所有内容遵守版权法、保护用户隐私，并且不含有害元素。
- en: 3.4 Task Generation
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 任务生成
- en: 'The queries in PyBench must be precisely related to files and diverse to ensure
    comprehensive coverage. To generate these queries, we designed a multi-agent cooperation
    mechanism with manual checks. Figure [2](#S3.F2 "Figure 2 ‣ 3 PyBench ‣ PyBench:
    Evaluating LLM Agent on various real-world coding tasks") illustrates the PyBench
    construction workflow.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'PyBench中的查询必须与文件精确相关且多样化，以确保全面覆盖。为了生成这些查询，我们设计了一个多代理合作机制，并进行了人工检查。图[2](#S3.F2
    "Figure 2 ‣ 3 PyBench ‣ PyBench: Evaluating LLM Agent on various real-world coding
    tasks")展示了PyBench构建的工作流程。'
- en: First, we prepare lists of keywords for each subclass in a category, forming
    a keywords dictionary for each categoryEldan and Li ([2023](#bib.bib9)). The type
    of file determines which category the task belongs to. Given the initial lines
    of a file and randomly selected keywords from the related category’s dictionary,
    the Query Generator selects appropriate keywords that align with the file content
    and compose a query.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为每个类别中的子类别准备关键词列表，形成每个类别的关键词词典（Eldan 和 Li ([2023](#bib.bib9)））。文件类型决定了任务属于哪个类别。给定文件的初始行和从相关类别词典中随机选择的关键词，查询生成器选择与文件内容一致的合适关键词并编写查询。
- en: Next, a Keywords Checker and a Content Checker verify if the query aligns with
    the file content and the selected keywords. Feedback from these checkers is used
    to refine the query. Once both checkers approve, the query undergoes manual editing
    to clarify the output format and file path. If the Query Generator fails to produce
    a proper query within 5 rounds, these files will be skipped.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，关键词检查器和内容检查器验证查询是否与文件内容和所选关键词一致。这些检查器的反馈用于完善查询。一旦两个检查器都批准，该查询将经过人工编辑，以澄清输出格式和文件路径。如果查询生成器在5轮内未能生成适当的查询，这些文件将被跳过。
- en: Queries that pass all checks are paired with their related files to form a query-file
    pair, which constitutes a task in PyBench.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过所有检查的查询与其相关文件配对，形成查询-文件对，构成PyBench中的一个任务。
- en: 3.5 Evaluation
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 评估
- en: 3.5.1 Trajectory Generation
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 轨迹生成
- en: 'Equip LLM Agent with Code Interpreter. We equipped each LLM with a code interpreter
    to execute Python code written by the LLM and provide feedback on the execution
    results. Previous works on LLM tool usage Qin et al. ([2023](#bib.bib26)) typically
    used tools in a function-calling format. Inspired by Wang et al. ([2024b](#bib.bib31)),
    which uses code as an action and outperforms alternatives, we designed two special
    tokens: <|execute_start|> and <|execute_end|> to help the LLM use the code interpreter
    more effectively. Appendix [D](#A4 "Appendix D Code as Action VS Function Calling
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") shows the
    different performance of the two formats.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '装备 LLM 代理以代码解释器。我们为每个 LLM 配备了代码解释器，以执行由 LLM 编写的 Python 代码，并提供执行结果的反馈。先前关于 LLM
    工具使用的研究 Qin 等人 ([2023](#bib.bib26)) 通常使用函数调用格式的工具。受 Wang 等人 ([2024b](#bib.bib31))
    使用代码作为操作且表现优于其他方法的启发，我们设计了两个特殊的标记： <|execute_start|> 和 <|execute_end|>，以帮助 LLM
    更有效地使用代码解释器。附录 [D](#A4 "附录 D 代码作为操作与函数调用 ‣ PyBench: 评估 LLM 代理在各种实际编码任务中的表现") 展示了这两种格式的不同表现。'
- en: '![Refer to caption](img/4903bedcf84641538bacd1f452b10ae9.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4903bedcf84641538bacd1f452b10ae9.png)'
- en: 'Figure 3: Function Call vs. Our Code Interpreter Format'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 函数调用与我们的代码解释器格式'
- en: 'Reasoning and Action. Given the task description and the uploaded file path,
    the LLM Agent is prompted to analyze the current situation and plan its actions
    before writing executable code Yao et al. ([2022](#bib.bib37)). The code will
    be executed in a pre-defined Python environment (with commonly used packages).
    The result of the code, whether is an output or an error message, well serve as
    feedback to the LLM. The LLM will follow the loop until it fulfills the task or
    reaches a maximum step limit (default set to 10). Figure [4](#S3.F4 "Figure 4
    ‣ 3.5.1 Trajectory Generation ‣ 3.5 Evaluation ‣ 3 PyBench ‣ PyBench: Evaluating
    LLM Agent on various real-world coding tasks") detailed this process.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '推理与行动。给定任务描述和上传的文件路径，LLM 代理会被提示分析当前情况并在编写可执行代码之前规划其操作 Yao 等人 ([2022](#bib.bib37))。代码将在预定义的
    Python 环境中执行（包含常用包）。代码的结果，无论是输出还是错误消息，都将作为对 LLM 的反馈。LLM 将遵循循环，直到完成任务或达到最大步骤限制（默认设置为
    10）。图 [4](#S3.F4 "图 4 ‣ 3.5.1 轨迹生成 ‣ 3.5 评估 ‣ 3 PyBench ‣ PyBench: 评估 LLM 代理在各种实际编码任务中的表现")
    详细描述了这一过程。'
- en: '![Refer to caption](img/6738220323bed60b35908c6695b255db.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6738220323bed60b35908c6695b255db.png)'
- en: 'Figure 4: Generating Trajectory Data by ReAct'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 通过 ReAct 生成轨迹数据'
- en: 3.5.2 Unit Test
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 单元测试
- en: 'To objectively and effectively test whether the LLM Agent has completed a task,
    we have implemented a unit test for each task in PyBench. For tasks with a fixed
    answer, we verify whether the Agent provides a final response that contains the
    correct answer. For tasks requiring a file output, such as cleaned datasets or
    edited images and audio, we check whether the output files meet the specified
    requirements. For tasks without a fixed answer, such as generating a word cloud
    or a website, we verify the existence of the output files. Detailed in Appendix
    [C](#A3 "Appendix C Example Unit Test ‣ PyBench: Evaluating LLM Agent on various
    real-world coding tasks")'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '为了客观有效地测试 LLM 代理是否完成了任务，我们为 PyBench 中的每个任务实施了单元测试。对于有固定答案的任务，我们验证代理是否提供了包含正确答案的最终响应。对于需要文件输出的任务，例如清理的数据集或编辑的图像和音频，我们检查输出文件是否符合指定要求。对于没有固定答案的任务，例如生成词云或网站，我们验证输出文件的存在。详细内容见附录
    [C](#A3 "附录 C 示例单元测试 ‣ PyBench: 评估 LLM 代理在各种实际编码任务中的表现")'
- en: 3.5.3 LLM as Evaluator
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 LLM 作为评估者
- en: 'Although unit tests are convenient and objective, they may fail to comprehensively
    evaluate open-ended tasks, such as assessing the coherence and fluency of text
    output by large models. Therefore, we also employ an LLM (GPT-4o) as an evaluator
    to provide pass-or-fail decisions for each trajectory, serving as an alternative
    to unit tests. Appendix [A.4](#A1.SS4 "A.4 Evaluation Prompt ‣ Appendix A Prompts
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") provides
    the detailed prompt used for the LLM evaluator.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管单元测试既方便又客观，但它们可能无法全面评估开放性任务，例如评估大模型生成文本的连贯性和流畅性。因此，我们还使用 LLM（GPT-4o）作为评估者，为每个轨迹提供合格或不合格的决定，作为单元测试的替代方法。附录
    [A.4](#A1.SS4 "A.4 评估提示 ‣ 附录 A 提示 ‣ PyBench: 评估 LLM 代理在各种实际编码任务中的表现") 提供了用于 LLM
    评估者的详细提示。'
- en: '| Datasets | Instructions | Turns per Traj. | Tokens per Traj. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 指令 | 每个轨迹的轮次 | 每个轨迹的令牌数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | <=3 | 4to6 | 7to9 | >=10 |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  |  | <=3 | 4to6 | 7to9 | >=10 |  |'
- en: '| CodeFeedback | 66383 | 41696 | 21583 | 2923 | 181 | 1503.93 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| CodeFeedback | 66383 | 41696 | 21583 | 2923 | 181 | 1503.93 |'
- en: '| CodeActInstruct | 7139 | 3482 | 3567 | 0 | 0 | 1165.03 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| CodeActInstruct | 7139 | 3482 | 3567 | 0 | 0 | 1165.03 |'
- en: '| PyInstruct | 3091 | 1234 | 1644 | 164 | 49 | 2017.15 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| PyInstruct | 3091 | 1234 | 1644 | 164 | 49 | 2017.15 |'
- en: 'Table 2: Statistics of CodeFeedback, CodaActInstruct, and PyInstruct. Token
    statistics are computed by using Llama-2 tokenizer.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：CodeFeedback、CodeActInstruct和PyInstruct的统计数据。令牌统计数据是使用Llama-2分词器计算的。
- en: 3.5.4 Evaluation Metrics
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 评估指标
- en: There are three evaluation metrics in PyBench.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PyBench中有三个评估指标。
- en: Pass Rate (UT). The percentage of passed tasks evaluated by unit tests.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过率（UT）。由单元测试评估的通过任务的百分比。
- en: Pass Rate (LLM). The percentage of passed tasks evaluated by the LLM Evaluator.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过LLM评估的通过率（LLM）。由LLM评估器评估的通过任务的百分比。
- en: Average Turns. The number of steps taken to complete each task. If a task fails,
    the number of turns is set to the maximum turn limit (10 turns).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 平均轮次。完成每个任务所需的步骤数。如果任务失败，则轮次数设置为最大轮次限制（10轮）。
- en: 4 Fine-tuning LLM Agent for Real World Coding Task
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 微调LLM Agent以应对实际编码任务
- en: In order to figure out what kind of capabilities are required and what training
    data could help enhance the LLM Agent’s performance on PyBench, we collect 4 datasets
    enhancing the Agent’s abilities in planning, coding, and multi-turn interaction.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定所需的能力类型以及哪些训练数据可以帮助提升LLM Agent在PyBench上的表现，我们收集了4个数据集，以增强Agent在规划、编码和多轮交互方面的能力。
- en: Homologous dataset. Intuitively, homologous datasets could enhance performance
    on the same task to be solved. Hence, we employ GPT-3.5-turbo to synthesize trajectories.
    The process is similar to the generation method without manually checking queries
    and using different files. We synthesized 3091 trajectory data covering every
    task class in PyBench to construct PyInstruct.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 同源数据集。从直观上看，同源数据集可以提高在解决相同任务时的表现。因此，我们使用GPT-3.5-turbo来合成轨迹。这个过程类似于生成方法，但无需手动检查查询和使用不同的文件。我们合成了3091个轨迹数据，覆盖了PyBench中的每个任务类别，以构建PyInstruct。
- en: 'Multi-turn of code interaction dataset. Most of the tasks in PyBench need multi-turn
    interaction with the Python code interpreter. It will provide feedback on code
    execution results or error traceback messages, which should be fully leveraged
    by the LLM Agent. There are several existing datasets aiming at enhancing LLM’s
    ability. CodeActInstruct Wang et al. ([2024b](#bib.bib31)) focuses on improving
    LLM’s abilities in various multi-turn tasks such as information seeking, software
    tool usage, external memory access, and robot planning, all executed in the format
    of Python code. CodeFeedback Zheng et al. ([2024b](#bib.bib39)) filters open-source
    code instruction data and converts them into multi-turn code with execution results.
    We repurpose the data by “equipping” our special token for the Python code interpreter.
    These datasets may help enhance LLM’s ability to utilize code feedback. Table
    [2](#S3.T2 "Table 2 ‣ 3.5.3 LLM as Evaluator ‣ 3.5 Evaluation ‣ 3 PyBench ‣ PyBench:
    Evaluating LLM Agent on various real-world coding tasks") shows the statistics
    information of CodeFeedback, CodeActInstruct, and PyInstruct.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '多轮代码交互数据集。PyBench中的大多数任务需要与Python代码解释器进行多轮交互。它将提供代码执行结果或错误回溯信息，这些信息应被LLM Agent充分利用。现有的几个数据集旨在提升LLM的能力。CodeActInstruct
    Wang等人（[2024b](#bib.bib31)）专注于提高LLM在各种多轮任务中的能力，如信息获取、软件工具使用、外部内存访问和机器人规划，这些任务都以Python代码格式执行。CodeFeedback
    Zheng等人（[2024b](#bib.bib39)）筛选开源代码指令数据，并将其转换为多轮代码及执行结果。我们通过“装备”我们特殊的令牌来重新利用这些数据，用于Python代码解释器。这些数据集可能有助于提高LLM在利用代码反馈方面的能力。表[2](#S3.T2
    "Table 2 ‣ 3.5.3 LLM as Evaluator ‣ 3.5 Evaluation ‣ 3 PyBench ‣ PyBench: Evaluating
    LLM Agent on various real-world coding tasks")显示了CodeFeedback、CodeActInstruct和PyInstruct的统计信息。'
- en: Multi-turn chat dataset. Additionally, in PyBench, the LLM Agent is expected
    to comprehend the user’s instructions and provide a formal response upon completing
    the task. High-quality multi-turn chat is also crucial for a Code Agent. UltraChat
    Ding et al. ([2023](#bib.bib8)) is a large-scale dataset comprising 1.5 million
    high-quality multi-turn instructional dialogues, specifically designed to improve
    the performance of open-source conversational models, which is perfectly aligned
    with our requirements.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 多轮对话数据集。此外，在 PyBench 中，LLM 代理需要理解用户的指令并在完成任务后提供正式的回复。高质量的多轮对话对代码代理也至关重要。UltraChat
    Ding 等人 ([2023](#bib.bib8)) 是一个大规模的数据集，包含 150 万个高质量的多轮指令对话，专门设计用于提高开源对话模型的表现，与我们的要求完全契合。
- en: Code-Rich corpus. The foundation ability of a code agent is the quality and
    correctness of its code. We assume that continue pre-train on code-rich corpus
    could contribute to solving PyBench tasks. The-stack-v2 Lozhkov et al. ([2024](#bib.bib21))
    introduces a code-rich corpus of Jupyter notebooks, which contains 11 million
    lines.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 代码丰富的语料库。代码代理的基础能力是代码的质量和正确性。我们假设继续在代码丰富的语料库上进行预训练可能有助于解决 PyBench 任务。The-stack-v2
    Lozhkov 等人 ([2024](#bib.bib21)) 引入了一个包含 1100 万行的 Jupyter 笔记本的代码丰富语料库。
- en: After collecting the four types of datasets, we conducted a series of experiments
    to figure out what are necessary abilities to solve the PyBench tasks and how
    to improve the LLM performance on real-world coding tasks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了四种数据集之后，我们进行了一系列实验，以找出解决 PyBench 任务所需的能力，并了解如何提升 LLM 在实际编码任务中的表现。
- en: 5 Experiment
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: '| Models | Params. | PyBench | HumanEval(+) | MBPP(+) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数 | PyBench | HumanEval(+) | MBPP(+) |'
- en: '|  |  | Pass Rate(LLM) | Pass Rate(UT) | Avg Turns |  |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 通过率(LLM) | 通过率(UT) | 平均轮次 |  |  |'
- en: '|  |  |  | Chart. | Text. | Image. | Math. | Software. | Overall |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 图表 | 文本 | 图片 | 数学 | 软件 | 总体 |  |  |  |'
- en: '| Closed-source Models |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 闭源模型 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| GPT-3.5 Turbo | - | 58.3 | 72.6 | 60.9 | 70.8 | 100.0 | 0.0 | 62.9 | 5.2
    | 78.6(70.7) | 82.5(69.7) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | - | 58.3 | 72.6 | 60.9 | 70.8 | 100.0 | 0.0 | 62.9 | 5.2
    | 78.6(70.7) | 82.5(69.7) |'
- en: '| GPT-4o | - | 81.8 | 80.6 | 82.6 | 77.1 | 100.0 | 100.0 | 79.7 | 4.3 | 91.5(85.4)
    | 91.2(76.6) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | - | 81.8 | 80.6 | 82.6 | 77.1 | 100.0 | 100.0 | 79.7 | 4.3 | 91.5(85.4)
    | 91.2(76.6) |'
- en: '| GPT-4o-mini | - | 67.1 | 77.4 | 69.6 | 87.5 | 100.0 | 100.0 | 81.1 | 4.5
    | 87.2(85.4) | 90.5(76.5) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | - | 67.1 | 77.4 | 69.6 | 87.5 | 100.0 | 100.0 | 81.1 | 4.5
    | 87.2(85.4) | 90.5(76.5) |'
- en: '| 70B size |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 70B 尺寸 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Llama3-Instruct | 70B | 80.2 | 38.7 | 34.8 | 72.9 | 33.3 | 50.0 | 78.3 |
    4.0 | 77.4(72.0) | 82.3(69.0) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-Instruct | 70B | 80.2 | 38.7 | 34.8 | 72.9 | 33.3 | 50.0 | 78.3 |
    4.0 | 77.4(72.0) | 82.3(69.0) |'
- en: '| CodeLlama-Instruct | 70B | 26.6 | 24.2 | 17.4 | 41.7 | 16.7 | 0.0 | 37.8
    | 7.5 | 72.0(65.9) | 62.2(51.2) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-Instruct | 70B | 26.6 | 24.2 | 17.4 | 41.7 | 16.7 | 0.0 | 37.8
    | 7.5 | 72.0(65.9) | 62.2(51.2) |'
- en: '| Deepseek-llm-chat | 67B | 39.4 | 21.0 | 17.4 | 37.5 | 33.3 | 25.0 | 26.6
    | 9.2 | 67.7(58.5) | 62.2(50.5) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-llm-chat | 67B | 39.4 | 21.0 | 17.4 | 37.5 | 33.3 | 25.0 | 26.6
    | 9.2 | 67.7(58.5) | 62.2(50.5) |'
- en: '| 33B size |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 33B 尺寸 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Deepseek-coder-Instruct | 33B | 35.3 | 22.6 | 0.0 | 4.2 | 16.7 | 0.0 | 37.0
    | 6.7 | 81.1(75.0) | 80.4(70.1) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-coder-Instruct | 33B | 35.3 | 22.6 | 0.0 | 4.2 | 16.7 | 0.0 | 37.0
    | 6.7 | 81.1(75.0) | 80.4(70.1) |'
- en: '| CodeLlama-Instruct | 34B | 9.6 | 8.1 | 13.0 | 0.0 | 16.7 | 0.0 | 6.3 | 8.9
    | 51.8(43.9) | 69.3(56.3) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-Instruct | 34B | 9.6 | 8.1 | 13.0 | 0.0 | 16.7 | 0.0 | 6.3 | 8.9
    | 51.8(43.9) | 69.3(56.3) |'
- en: '| Yi-1.5-Chat-16K | 34B | 44.2 | 33.9 | 34.8 | 56.3 | 50.0 | 0.0 | 41.3 | 9.0
    | 58.8(53.4) | 77.5(63.6) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Yi-1.5-Chat-16K | 34B | 44.2 | 33.9 | 34.8 | 56.3 | 50.0 | 0.0 | 41.3 | 9.0
    | 58.8(53.4) | 77.5(63.6) |'
- en: '| Qwen-1.5-Chat | 32B | 43.0 | 12.9 | 39.1 | 10.4 | 50.0 | 75.0 | 19.6 | 9.3
    | 58.5(55.5) | 66.1(56.1) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-1.5-Chat | 32B | 43.0 | 12.9 | 39.1 | 10.4 | 50.0 | 75.0 | 19.6 | 9.3
    | 58.5(55.5) | 66.1(56.1) |'
- en: '| Gemma-2-Instruct | 27B | 66.1 | 58.1 | 56.5 | 87.5 | 83.3 | 75.0 | 69.2 |
    5.0 | 49.4(45.7) | 73.3(61.1) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-Instruct | 27B | 66.1 | 58.1 | 56.5 | 87.5 | 83.3 | 75.0 | 69.2 |
    5.0 | 49.4(45.7) | 73.3(61.1) |'
- en: '| 7B size |  |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 7B 尺寸 |  |  |  |  |  |  |  |  |  |  |  |'
- en: '| Llama3-Instruct | 8B | 49.7 | 43.5 | 26.1 | 72.9 | 66.7 | 50.0 | 49.7 | 6.7
    | 61.6(56.7) | 70.1(59.3) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-Instruct | 8B | 49.7 | 43.5 | 26.1 | 72.9 | 66.7 | 50.0 | 49.7 | 6.7
    | 61.6(56.7) | 70.1(59.3) |'
- en: '| Mistral-Instruct-v0.2 | 7B | 17.5 | 17.7 | 17.4 | 18.8 | 16.7 | 0.0 | 17.5
    | 8.9 | 35.4(30.5) | 31.0(24.1) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-Instruct-v0.2 | 7B | 17.5 | 17.7 | 17.4 | 18.8 | 16.7 | 0.0 | 17.5
    | 8.9 | 35.4(30.5) | 31.0(24.1) |'
- en: '| Gemma-2-Instruct | 9B | 41.7 | 37.1 | 30.4 | 70.1 | 83.3 | 50.0 | 49.7 |
    7.2 | 55.5(48.8) | 66.1(56.1) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-Instruct | 9B | 41.7 | 37.1 | 30.4 | 70.1 | 83.3 | 50.0 | 49.7 |
    7.2 | 55.5(48.8) | 66.1(56.1) |'
- en: '| Qwen2-Instruct | 7B | 57.8 | 59.7 | 47.8 | 58.3 | 83.3 | 75.0 | 58.7 | 5.9
    | 64.6(61.0) | 62.3(52.3) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-Instruct | 7B | 57.8 | 59.7 | 47.8 | 58.3 | 83.3 | 75.0 | 58.7 | 5.9
    | 64.6（61.0） | 62.3（52.3） |'
- en: '| CodeQwen1.5-chat | 7B | 49.2 | 48.4 | 43.5 | 66.7 | 83.3 | 25.0 | 54.5 |
    7.9 | 83.5(78.7) | 79.4(69.0) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| CodeQwen1.5-chat | 7B | 49.2 | 48.4 | 43.5 | 66.7 | 83.3 | 25.0 | 54.5 |
    7.9 | 83.5（78.7） | 79.4（69.0） |'
- en: '| CodeActAgent-Llama2 | 7B | 12.4 | 16.1 | 21.7 | 20.8 | 33.3 | 0.0 | 18.9
    | 8.1 | 18.9(15.2) | 22.8(18.3) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| CodeActAgent-Llama2 | 7B | 12.4 | 16.1 | 21.7 | 20.8 | 33.3 | 0.0 | 18.9
    | 8.1 | 18.9（15.2） | 22.8（18.3） |'
- en: '| CodeActAgent-Mistral-v0.1 | 7B | 18.8 | 17.7 | 4.3 | 20.8 | 16.7 | 0.0 |
    16.1 | 8.8 | 28.7(26.8) | 43.9(35.4) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| CodeActAgent-Mistral-v0.1 | 7B | 18.8 | 17.7 | 4.3 | 20.8 | 16.7 | 0.0 |
    16.1 | 8.8 | 28.7（26.8） | 43.9（35.4） |'
- en: '| OpenCodeInterpreter-DS | 6.7B | 25.0 | 11.3 | 21.7 | 75.0 | 66.7 | 25.0 |
    51.7 | 6.8 | 77.4(73.8) | 80.2(66.4) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| OpenCodeInterpreter-DS | 6.7B | 25.0 | 11.3 | 21.7 | 75.0 | 66.7 | 25.0 |
    51.7 | 6.8 | 77.4（73.8） | 80.2（66.4） |'
- en: '| InternLM2_5-chat | 7B | 17.5 | 32.3 | 13.0 | 4.17 | 50.0 | 25.0 | 20.27 |
    9.84 | 57.3(51.2) | 61.6(51.5) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| InternLM2_5-chat | 7B | 17.5 | 32.3 | 13.0 | 4.17 | 50.0 | 25.0 | 20.27 |
    9.84 | 57.3（51.2） | 61.6（51.5） |'
- en: '| PyLlama3(w/o cpt) | 8B | 56.7 | 58.0 | 52.2 | 73.0 | 50.0 | 50.0 | 58.7 |
    6.3 | 54.3(48.2) | 59.5(50.3) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| PyLlama3（无 cpt） | 8B | 56.7 | 58.0 | 52.2 | 73.0 | 50.0 | 50.0 | 58.7 | 6.3
    | 54.3（48.2） | 59.5（50.3） |'
- en: '| PyLlama3 | 8B | 73.4 | 62.9 | 65.2 | 56.3 | 66.7 | 50.0 | 60.8 | 6.1 | 57.3(48.2)
    | 62.2(51.1) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| PyLlama3 | 8B | 73.4 | 62.9 | 65.2 | 56.3 | 66.7 | 50.0 | 60.8 | 6.1 | 57.3（48.2）
    | 62.2（51.1） |'
- en: 'Table 3: The main result table. We test closed-source models, 70B size models,
    33B size models, and 7B size models. We bold the best results for each size model’s
    results.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：主要结果表。我们测试了闭源模型、70B 模型、33B 模型和 7B 模型。我们对每个尺寸模型的结果进行了加粗以标注最佳结果。
- en: '| PyInst. | Code. | UltraCh. | Jupyter. | Pass Rate(UT) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| PyInst. | Code. | UltraCh. | Jupyter. | 通过率（UT） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  |  |  |  | Chart. | Text. | Image. | Math. | Software. | Over All |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 图表. | 文本. | 图像. | 数学. | 软件. | 总体 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ✓ | ✗ | ✗ | ✗ | 3.2 | 13.0 | 6.3 | 50.0 | 25.0 | 8.4 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✗ | ✗ | ✗ | 3.2 | 13.0 | 6.3 | 50.0 | 25.0 | 8.4 |'
- en: '| ✗ | ✓ | ✗ | ✗ | 40.3 | 26.1 | 58.3 | 83.3 | 25.0 | 45.5 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| ✗ | ✓ | ✗ | ✗ | 40.3 | 26.1 | 58.3 | 83.3 | 25.0 | 45.5 |'
- en: '| ✓ | ✓ | ✗ | ✗ | 51.6 | 56.5 | 60.4 | 100.0 | 25.0 | 56.6 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✗ | ✗ | 51.6 | 56.5 | 60.4 | 100.0 | 25.0 | 56.6 |'
- en: '| ✓ | ✓ | ✓ | ✗ | 62.9 | 52.2 | 56.3 | 83.3 | 25.0 | 58.7 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✗ | 62.9 | 52.2 | 56.3 | 83.3 | 25.0 | 58.7 |'
- en: '| ✓ | ✓ | ✗ | ✓ | 58.0 | 52.2 | 73.0 | 50.0 | 50.0 | 59.4 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✗ | ✓ | 58.0 | 52.2 | 73.0 | 50.0 | 50.0 | 59.4 |'
- en: '| ✓ | ✓ | ✓ | ✓ | 62.9 | 65.2 | 56.3 | 66.7 | 50.0 | 60.8 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✓ | 62.9 | 65.2 | 56.3 | 66.7 | 50.0 | 60.8 |'
- en: 'Table 4: Pass Rate (UT) with different training datasets. PyInst. stands for
    PyInstruct. Code. refers to Codefeedback & CodeActInstruct. UltraCh. means UltraChat,
    and Jupyter. indicates continuing pre-training on the Jupyter notebook corpus.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同训练数据集的通过率（UT）。PyInst. 代表 PyInstruct。Code. 指 Codefeedback & CodeActInstruct。UltraCh.
    意味着 UltraChat，Jupyter. 表示继续在 Jupyter notebook 语料库上进行预训练。
- en: 5.1 Main Result on PyBench
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 PyBench 上的主要结果
- en: 'Testing Evaluation Setup. Firstly, we prepare a conda environment equipped
    with 182 commonly used Python packages (Detailed in Appendix [F](#A6 "Appendix
    F Python Packages in our environment ‣ PyBench: Evaluating LLM Agent on various
    real-world coding tasks")). The max turn is set to $k=10$. We prompt the LLM to
    use code interpreter and follow ReAct Yao et al. ([2022](#bib.bib37)) format.
    (Appendix [A.4](#A1.SS4 "A.4 Evaluation Prompt ‣ Appendix A Prompts ‣ PyBench:
    Evaluating LLM Agent on various real-world coding tasks")) The code in LLM’s response
    will be extracted and the execution result will return to LLM. After getting the
    trajectory and output files, we calculate the pass rate through the unit test
    set (UT) and LLM Evaluator set (LLM). We also adopt other benchmarks to test the
    model’s basic code ability, including HumanEval Chen et al. ([2021](#bib.bib4))
    and HumanEval+ Liu et al. ([2023a](#bib.bib19)) for single-turn code generation,
    and MBPP Austin et al. ([2021](#bib.bib1)) and MBPP+ Liu et al. ([2023a](#bib.bib19))
    for simple Python programming problems.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 测试评估设置。首先，我们准备了一个 conda 环境，其中包含 182 个常用的 Python 包（详见附录 [F](#A6 "附录 F 我们环境中的
    Python 包 ‣ PyBench：评估 LLM 代理在各种实际编码任务中的表现")）。最大轮次设置为 $k=10$。我们提示 LLM 使用代码解释器，并遵循
    ReAct Yao 等（[2022](#bib.bib37)）的格式。（附录 [A.4](#A1.SS4 "A.4 评估提示 ‣ 附录 A 提示 ‣ PyBench：评估
    LLM 代理在各种实际编码任务中的表现")）LLM 响应中的代码将被提取，并将执行结果返回给 LLM。在获取轨迹和输出文件后，我们通过单元测试集（UT）和
    LLM 评估集（LLM）计算通过率。我们还采用其他基准测试模型的基本代码能力，包括 HumanEval Chen 等（[2021](#bib.bib4)）和
    HumanEval+ Liu 等（[2023a](#bib.bib19)）用于单轮代码生成，MBPP Austin 等（[2021](#bib.bib1)）和
    MBPP+ Liu 等（[2023a](#bib.bib19)）用于简单的 Python 编程问题。
- en: 'Results on PyBench. Table [3](#S5.T3 "Table 3 ‣ 5 Experiment ‣ PyBench: Evaluating
    LLM Agent on various real-world coding tasks") shows our main experiment result.
    GPT-4o scored highly in all five tasks, showing its great ability. However, models
    including CodeLlama-Instruct-70B, Deepseek-coder-Instruct-33B, and CodeLlama-Instruct-33B,
    although trained on code corpus and perform well on HumanEval and MBPP, struggle
    to solve PyBench tasks, proving that code ability does not always lead to a good
    score on PyBench. Most of the advanced 7B size models can only solve about 50%
    of tasks in PyBench. Our PyLlama3, with continued pre-train and fine-tuning on
    related datasets, surpasses Llama3-8B-Instruct on Chart Analysis, Text Analysis,
    and Complex Math.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'PyBench 上的结果。表格 [3](#S5.T3 "Table 3 ‣ 5 Experiment ‣ PyBench: Evaluating LLM
    Agent on various real-world coding tasks") 显示了我们的主要实验结果。GPT-4o 在所有五个任务中得分较高，展示了其出色的能力。然而，包括
    CodeLlama-Instruct-70B、Deepseek-coder-Instruct-33B 和 CodeLlama-Instruct-33B 在内的模型，虽然在代码语料上进行了训练并在
    HumanEval 和 MBPP 上表现良好，却难以解决 PyBench 任务，这证明了编码能力并不总是能在 PyBench 上获得高分。大多数先进的 7B
    规模模型只能解决大约 50% 的 PyBench 任务。我们的 PyLlama3 通过在相关数据集上持续的预训练和微调，超越了 Llama3-8B-Instruct
    在图表分析、文本分析和复杂数学问题上的表现。'
- en: 'Coding ability is the foundation. CodeActAgent-Llama-2-7b and CodeActAgent-Mistral-7b-v0.1Wang
    et al. ([2024b](#bib.bib31)), although trained on CodeActInstruct, who teach LLM
    Agent use tools in code format, have the weakest performance on PyBench. The failure
    might be caused by low performance on basic coding ability benchmarks: HumanEval
    and MBPP. The result illustrates basic code ability and the foundation to solve
    real-world coding tasks in PyBench.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 编码能力是基础。CodeActAgent-Llama-2-7b 和 CodeActAgent-Mistral-7b-v0.1Wang 等人 ([2024b](#bib.bib31))
    尽管在 CodeActInstruct 上进行了训练，教授 LLM 代理使用代码格式的工具，但在 PyBench 上的表现最差。这种失败可能是由于在基本编码能力基准测试（HumanEval
    和 MBPP）上的表现较差。结果说明，基础编码能力是解决 PyBench 中实际编码任务的基础。
- en: PyBench evaluates comprehensive abilities of LLM Agent. A great performance
    on basic coding benchmarks does not directly lead to equal performance on PyBench.
    Although Deepseek-coder-Instruct-33B achieved a great score on HumanEval and MBPP,
    it got a weak score on PyBench. Indicating mere coding ability could not lead
    to success in real-world coding tasks. PyBench also evaluates the LLM agent’s
    abilities including planning, multi-turn interaction, leveraging code feedback,
    and generating formal responses to the user.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: PyBench 评估 LLM 代理的综合能力。虽然在基本编码基准测试上表现优异并不能直接导致 PyBench 上的相同表现。尽管 Deepseek-coder-Instruct-33B
    在 HumanEval 和 MBPP 上取得了很高的分数，但在 PyBench 上得分较低。这表明仅有编码能力并不足以在实际编码任务中取得成功。PyBench
    还评估 LLM 代理的能力，包括计划、多轮互动、利用代码反馈以及向用户生成正式响应。
- en: 'Compare LLM Evaluator and Unit Test. Table [3](#S5.T3 "Table 3 ‣ 5 Experiment
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") shows that
    the evaluation results from the LLM and the unit tests are generally consistent,
    with only minor fluctuations. However, there are significant discrepancies for
    several models. To investigate this, we randomly selected 60 trajectories from
    the entire dataset for manual assessment. We found that in over 80% of the cases,
    the LLM evaluation results were consistent with the unit test results. Nevertheless,
    in some instances, the LLM evaluator might only determine whether the code executes
    successfully, without verifying its correctness. In other cases, the tested LLM
    agent might write code or provide answers that coincidentally pass the unit tests.
    A detailed analysis of these scenarios is provided in Appendix [E](#A5 "Appendix
    E Compare LLM Evaluator and Unit Test ‣ PyBench: Evaluating LLM Agent on various
    real-world coding tasks").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '比较 LLM 评估器和单元测试。表格 [3](#S5.T3 "Table 3 ‣ 5 Experiment ‣ PyBench: Evaluating
    LLM Agent on various real-world coding tasks") 显示，LLM 和单元测试的评估结果总体上是一致的，仅有少量波动。然而，对于一些模型存在显著的差异。为调查这一点，我们随机选择了
    60 条轨迹进行人工评估。我们发现，在超过 80% 的情况下，LLM 评估结果与单元测试结果一致。然而，在某些情况下，LLM 评估器可能仅判断代码是否成功执行，而没有验证其正确性。在其他情况下，被测试的
    LLM 代理可能会编写代码或提供答案，恰好通过了单元测试。对这些情况的详细分析见附录 [E](#A5 "Appendix E Compare LLM Evaluator
    and Unit Test ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks")。'
- en: 5.2 Analysis on the training dataset
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 关于训练数据集的分析
- en: In this section, we conduct ablation studies on training datasets to explore
    the necessary capabilities for solving PyBench.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对训练数据集进行消融研究，以探索解决 PyBench 所需的能力。
- en: 5.2.1 Train Setup
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 训练设置
- en: We conduct full-parameter supervised fine-tuning on Llama3-8B with a sequence
    length of 32768 tokens Liu et al. ([2023b](#bib.bib20)); Roziere et al. ([2023](#bib.bib27));
    Touvron et al. ([2023](#bib.bib29)), ensuring it could handle the content of chart
    and text files. For each version of the supervised fine-tuning model, we use 32
    A100 GPU and train 4000 steps. The learning rate is set as 1e-5 with a 0.05 warm-up
    ratio and a cosine scheduler. As for the continue pre-trained model, we add an
    extra 3000 steps to training the corpus.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Llama3-8B上进行全参数监督微调，序列长度为32768个标记 Liu等人 ([2023b](#bib.bib20))；Roziere等人 ([2023](#bib.bib27))；Touvron等人
    ([2023](#bib.bib29))，确保其能处理图表和文本文件的内容。对于每个版本的监督微调模型，我们使用32个A100 GPU，并训练4000步。学习率设置为1e-5，预热比例为0.05，使用余弦调度器。对于继续预训练的模型，我们额外增加了3000步的语料库训练。
- en: 5.2.2 Ablation study
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 消融研究
- en: 'The performance of LLMs on specific tasks can often be enhanced through supervised
    fine-tuning on homologous datasets, where the model learns exactly what to do
    in particular situations. Initially, we hypothesized that training the Llama3-8B-base
    model on PyInstruct, 3k homologous trajectories data, would teach the model to
    handle real-world coding tasks. However, as shown in Table [4](#S5.T4 "Table 4
    ‣ 5 Experiment ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks"),
    this model struggles with PyBench tasks. From the generated trajectories, we observed
    that the model fails to follow human instructions and even struggles to locate
    the correct file paths.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '对特定任务，LLMs的性能往往可以通过在同类数据集上进行监督微调来提升，模型可以准确学习在特定情况下该如何操作。最初，我们假设在PyInstruct、3k同类轨迹数据上训练Llama3-8B-base模型，将教会模型处理现实世界的编码任务。然而，如表[4](#S5.T4
    "Table 4 ‣ 5 Experiment ‣ PyBench: Evaluating LLM Agent on various real-world
    coding tasks")所示，该模型在PyBench任务上表现不佳。从生成的轨迹中，我们观察到模型未能按照人类指示操作，甚至在定位正确文件路径上也存在困难。'
- en: 'To address the drawbacks, we add two multi-turn code interaction datasets:
    CodeActInstruct Wang et al. ([2024b](#bib.bib31)) and CodeFeedback Zheng et al.
    ([2024b](#bib.bib39)). We train the model on these datasets and PyInstruct. The
    result indicated a significant improvement. But when we remove PyInstruct and
    train the model only on CodeActInstruct and CodeFeedback, the model’s performance
    suffers a sharp decline in Chart Analysis and Text Analysis, demonstrating PyInstruct
    can enhance the model’s capability.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些不足，我们添加了两个多轮代码交互数据集：CodeActInstruct Wang等人 ([2024b](#bib.bib31)) 和 CodeFeedback
    Zheng等人 ([2024b](#bib.bib39))。我们在这些数据集和PyInstruct上对模型进行了训练。结果显示，性能有了显著改善。但当我们去除PyInstruct，只用CodeActInstruct和CodeFeedback进行训练时，模型在图表分析和文本分析上的表现急剧下降，证明PyInstruct可以增强模型的能力。
- en: Additionally, to further enhance the model’s capabilities, we added UltraChat
    Ding et al. ([2023](#bib.bib8)) in the training datasets and trained the model
    with the same training settings. However, the result shows adding UltraChat only
    brings improvement to Chart Analysis tasks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了进一步提升模型的能力，我们在训练数据集中加入了UltraChat Ding等人 ([2023](#bib.bib8))，并使用相同的训练设置对模型进行了训练。然而，结果表明，仅添加UltraChat对图表分析任务有所改善。
- en: 'We found that the model still struggles with the usage of some packages and
    specific techniques for real-world coding tasks. Consequently, we conducted continue
    pre-train on a Jupyter Notebook corpus Lozhkov et al. ([2024](#bib.bib21)) before
    fine-tuning on PyInstruct, CodeFeedback, CodeActInstruct, and UltraChat. As shown
    in Table [4](#S5.T4 "Table 4 ‣ 5 Experiment ‣ PyBench: Evaluating LLM Agent on
    various real-world coding tasks"), continue pre-train can also bring great improvement
    to Text Analysis tasks. The continued pretraining model achieves the best performance
    after fine-tuning on PyInstruct, CodeFeedback, CodeActInstruct, and UltraChat,
    which we named to PyLlama3. And the version without continue pre-train is named
    PyLlama3(w/o cpt).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现模型在使用一些包和特定技术处理现实世界编码任务时仍然存在困难。因此，在对PyInstruct、CodeFeedback、CodeActInstruct和UltraChat进行微调之前，我们在Jupyter
    Notebook语料库Lozhkov等人 ([2024](#bib.bib21)) 上进行了继续预训练。如表[4](#S5.T4 "Table 4 ‣ 5
    Experiment ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks")所示，继续预训练也能显著提升文本分析任务的表现。继续预训练的模型在对PyInstruct、CodeFeedback、CodeActInstruct和UltraChat进行微调后表现最佳，我们将其命名为PyLlama3。没有继续预训练的版本命名为PyLlama3(w/o
    cpt)。'
- en: In conclusion, we figured out PyBench not only requires the model’s basic coding
    abilities which instruct them to write Python properly but also multi-turn interaction
    and reasoning abilities to solve real-world tasks.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们发现 PyBench 不仅要求模型具备基本的编码能力以正确编写 Python 代码，还需要具备多轮交互和推理能力以解决实际任务。
- en: 6 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we propose PyBench, a comprehensive benchmark encompassing five
    main categories that reflect real-world coding situations. Through collecting
    files and generating related queries, PyBench could evaluate LLM’s usability and
    efficiency in real-world tasks. After evaluating plenty of LLMs, we find many
    of them are struggling with real-world coding tasks. We collected and synthesized
    four datasets and trained PyLlama3 whose performance surpass many 7B, 33B, and
    70B size models. Our ablation studies prove the effectiveness of the datasets,
    figuring out a way to train a model that could adapt to real-world coding tasks.
    Solving PyBench tasks means the LLM could interact with the file system with Python
    code, which symbolizes a great milestone in developing a really usable LLM Agent
    who can serve as a helpful life assistant for humankind.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 PyBench，这是一个全面的基准测试，涵盖了五个主要类别，以反映实际编码情况。通过收集文件和生成相关查询，PyBench 可以评估
    LLM 在实际任务中的可用性和效率。在评估了大量 LLM 后，我们发现许多 LLM 在实际编码任务中表现不佳。我们收集并综合了四个数据集，并训练了 PyLlama3，其性能超越了许多
    7B、33B 和 70B 大小的模型。我们的消融研究证明了数据集的有效性，找到了训练能够适应实际编码任务的模型的方法。解决 PyBench 任务意味着 LLM
    能够通过 Python 代码与文件系统交互，这标志着在开发真正可用的 LLM Agent 方面迈出了重要的一步，这种 Agent 可以作为人类有用的生活助手。
- en: 7 limitations
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Our work introduces a comprehensive benchmark: PyBench to evaluate LLM Agent
    on real-world coding tasks. Although five main categories are included in PyBench,
    there are many cases in the real world that have not been covered. The coding
    problem that uses other coding languages instead of Python is not covered either.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作引入了一个全面的基准测试：PyBench，用于评估 LLM Agent 在实际编码任务中的表现。虽然 PyBench 包含了五个主要类别，但仍有许多现实世界中的案例未被覆盖。使用其他编程语言而不是
    Python 的编码问题也未被涵盖。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奥斯丁等人（2021年）雅各布·奥斯丁、奥古斯特斯·奥德纳、麦克斯韦·奈、马尔滕·博斯玛、亨里克·米哈维尔斯基、大卫·多汉、艾伦·姜、凯瑞·蔡、迈克尔·特里、阮国等人。2021年。使用大型语言模型进行程序合成。*arXiv
    预印本 arXiv:2108.07732*。
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report.
    *arXiv preprint arXiv:2309.16609*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等人（2023年）金泽·白、帅·白、云飞·楚、泽宇·崔、凯·邓、晓东·邓、杨·范、文斌·戈、余·汉、飞·黄等人。2023年。Qwen 技术报告。*arXiv
    预印本 arXiv:2309.16609*。
- en: 'Chen et al. (2023a) Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay,
    Börje F Karlsson, Jie Fu, and Yemin Shi. 2023a. Autoagents: A framework for automatic
    agent generation. *arXiv preprint arXiv:2309.17288*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023a）光耀·陈、思伟·董、余书、葛章、贾瓦德·塞赛、博尔耶·F·卡尔松、杰·傅和叶敏·石。2023a。Autoagents：一种自动生成智能体的框架。*arXiv
    预印本 arXiv:2309.17288*。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2021年）马克·陈、杰瑞·特沃雷克、徐辉、袁启明、恩里克·庞德·德·奥利维拉·平托、贾雷德·卡普兰、哈里·爱德华兹、尤里·布尔达、尼古拉斯·约瑟夫、格雷格·布罗克曼等人。2021年。评估基于代码训练的大型语言模型。*arXiv
    预印本 arXiv:2107.03374*。
- en: 'Chen et al. (2023b) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023b.
    Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors
    in agents. *arXiv preprint arXiv:2308.10848*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023b）韦泽·陈、俞盛·苏、静伟·左、程杨、陈飞·袁、陈倩、陈志敏、俞佳·秦、雅曦·卢、若冰·谢等人。2023b。Agentverse：促进多智能体协作并探索智能体中的新兴行为。*arXiv
    预印本 arXiv:2308.10848*。
- en: 'Chen et al. (2024) Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning
    Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and
    methods of effective agent tuning for large language models. *arXiv preprint arXiv:2403.12881*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2024年）泽辉·陈、奎昆·刘、秋晨·王、文伟·张、江宁·刘、大华·林、凯·陈和丰·赵。2024年。Agent-flan：为大型语言模型设计有效智能体调整的数据和方法。*arXiv
    预印本 arXiv:2403.12881*。
- en: 'DeepSeek-AI et al. (2024) DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian
    Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding
    Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao,
    Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin
    Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu
    Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo,
    and Wenfeng Liang. 2024. [Deepseek-coder-v2: Breaking the barrier of closed-source
    models in code intelligence](http://arxiv.org/abs/2406.11931).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepSeek-AI 等（2024）DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang,
    Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng,
    Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin
    Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie,
    Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen,
    Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo 和 Wenfeng
    Liang。2024年。[Deepseek-coder-v2: 打破代码智能领域封闭源模型的障碍](http://arxiv.org/abs/2406.11931)。'
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models
    by scaling high-quality instructional conversations. *arXiv preprint arXiv:2305.14233*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2023）Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun 和 Bowen Zhou。2023年。通过扩展高质量指令对话来增强聊天语言模型。*arXiv 预印本
    arXiv:2305.14233*。
- en: 'Eldan and Li (2023) Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small
    can language models be and still speak coherent english? *arXiv preprint arXiv:2305.07759*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eldan 和 Li（2023）Ronen Eldan 和 Yuanzhi Li。2023年。Tinystories: 语言模型可以小到什么程度仍能说出连贯的英语？*arXiv
    预印本 arXiv:2305.07759*。'
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. 2024. Deepseek-coder: When
    the large language model meets programming–the rise of code intelligence. *arXiv
    preprint arXiv:2401.14196*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等（2024）Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,
    Guanting Chen, Xiao Bi, Y Wu, YK Li 等。2024年。Deepseek-coder: 当大语言模型遇上编程——代码智能的崛起。*arXiv
    预印本 arXiv:2401.14196*。'
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    and Jacob Steinhardt. 2021. Measuring coding challenge competence with apps. *NeurIPS*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021）Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika,
    Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song 和 Jacob
    Steinhardt。2021年。通过应用程序测量编码挑战能力。*NeurIPS*。
- en: 'Hong et al. (2024) Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang
    Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, et al.
    2024. Data interpreter: An llm agent for data science. *arXiv preprint arXiv:2402.18679*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等（2024）Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi
    Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge 等。2024年。数据解释器：用于数据科学的
    LLM 代理。*arXiv 预印本 arXiv:2402.18679*。
- en: 'Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al.
    2023. Metagpt: Meta programming for multi-agent collaborative framework. *arXiv
    preprint arXiv:2308.00352*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等（2023）Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang,
    Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou 等。2023年。Metagpt:
    用于多智能体协作框架的元编程。*arXiv 预印本 arXiv:2308.00352*。'
- en: 'Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and
    Heming Cui. 2023. Agentcoder: Multi-agent-based code generation with iterative
    testing and optimisation. *arXiv preprint arXiv:2312.13010*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等（2023）Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck 和 Heming Cui。2023年。Agentcoder:
    基于多智能体的代码生成与迭代测试和优化。*arXiv 预印本 arXiv:2312.13010*。'
- en: 'Jimenez et al. (2023) Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu
    Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language
    models resolve real-world github issues? *arXiv preprint arXiv:2310.06770*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jimenez 等（2023）Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin
    Pei, Ofir Press 和 Karthik Narasimhan。2023年。Swe-bench: 语言模型能否解决现实世界中的 GitHub 问题？*arXiv
    预印本 arXiv:2310.06770*。'
- en: 'Lai et al. (2023) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023.
    Ds-1000: A natural and reliable benchmark for data science code generation. In
    *International Conference on Machine Learning*, pages 18319–18345\. PMLR.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai 等（2023）Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong,
    Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang 和 Tao Yu。2023年。DS-1000:
    一个自然且可靠的数据科学代码生成基准。在 *国际机器学习大会*，第18319–18345页。PMLR。'
- en: 'Li et al. (2024) Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang
    Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, et al. 2024. Devbench:
    A comprehensive benchmark for software development. *arXiv preprint arXiv:2403.08604*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2024) Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li,
    Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, 等人. 2024. Devbench：一个综合的软件开发基准。*arXiv
    预印本 arXiv:2403.08604*。
- en: 'Li et al. (2023) Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen
    Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023. Taco: Topics in algorithmic code generation
    dataset. *arXiv preprint arXiv:2312.14852*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu,
    Guang Liu, Zhi Jin, 和 Ge Li. 2023. Taco：算法代码生成数据集中的主题。*arXiv 预印本 arXiv:2312.14852*。
- en: Liu et al. (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. 2023a. [Is your code generated by chatgpt really correct? rigorous evaluation
    of large language models for code generation](http://arxiv.org/abs/2305.01210).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, 和 Lingming Zhang.
    2023a. [你的代码是由 ChatGPT 生成的，真的正确吗？对大型语言模型代码生成的严格评估](http://arxiv.org/abs/2305.01210)。
- en: Liu et al. (2023b) Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu,
    and Dahua Lin. 2023b. Scaling laws of rope-based extrapolation. *arXiv preprint
    arXiv:2310.05209*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023b) Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, 和
    Dahua Lin. 2023b. 基于绳索的外推规律。*arXiv 预印本 arXiv:2310.05209*。
- en: 'Lozhkov et al. (2024) Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada,
    Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding
    Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii
    Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan
    Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov,
    Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan
    Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian
    McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane
    Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz
    Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra,
    and Harm de Vries. 2024. [Starcoder 2 and the stack v2: The next generation](http://arxiv.org/abs/2402.19173).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lozhkov 等人 (2024) Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano,
    Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang
    Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian
    Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan
    Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae
    Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo
    Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher
    Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao,
    Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu,
    Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas
    Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis,
    Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, 和 Harm
    de Vries. 2024. [Starcoder 2 和 Stack v2：下一代](http://arxiv.org/abs/2402.19173)。
- en: 'Luo et al. (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. [Wizardcoder:
    Empowering code large language models with evol-instruct](http://arxiv.org/abs/2306.08568).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等人 (2023) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, 和 Daxin Jiang. 2023. [Wizardcoder：通过
    evol-instruct 赋能大型语言模型的代码生成](http://arxiv.org/abs/2306.08568)。
- en: 'Ni et al. (2024) Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng,
    Kensen Shi, Charles Sutton, and Pengcheng Yin. 2024. Next: Teaching large language
    models to reason about code execution. *arXiv preprint arXiv:2404.14662*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni 等人 (2024) Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen
    Shi, Charles Sutton, 和 Pengcheng Yin. 2024. 下一步：教导大型语言模型推理代码执行。*arXiv 预印本 arXiv:2404.14662*。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th Annual ACM Symposium
    on User Interface Software and Technology*, pages 1–22.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S Bernstein. 2023. 生成代理：人类行为的交互模拟。见 *第36届年度 ACM
    用户界面软件与技术研讨会论文集*，第 1–22 页。
- en: Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software
    development. *arXiv preprint arXiv:2307.07924*.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan
    Xu, Zhiyuan Liu, 和 Maosong Sun. 2023. 软件开发的沟通代理。*arXiv 预印本 arXiv:2307.07924*。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, 等. 2023. Toolllm: 促进大型语言模型掌握
    16000+ 实际 API。*arXiv 预印本 arXiv:2307.16789*。'
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, 等. 2023. Code llama: 开放基础模型用于代码。*arXiv 预印本 arXiv:2308.12950*。'
- en: 'Song et al. (2024) Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi
    Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, et al. 2024. Alchemistcoder:
    Harmonizing and eliciting code capability by hindsight tuning on multi-source
    data. *arXiv preprint arXiv:2405.19265*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2024) Zifan Song, Yudong Wang, Wenwei Zhang, Kuikun Liu, Chengqi
    Lyu, Demin Song, Qipeng Guo, Hang Yan, Dahua Lin, Kai Chen, 等. 2024. Alchemistcoder:
    通过对多源数据的事后调优来协调和引发代码能力。*arXiv 预印本 arXiv:2405.19265*。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: Wang et al. (2024a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. A survey
    on large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6):186345.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, 等. 2024a. 基于大型语言模型的自主代理综述。*计算机科学前沿*，18(6):186345。
- en: Wang et al. (2024b) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, and Heng Ji. 2024b. Executable code actions elicit better llm agents.
    *arXiv preprint arXiv:2402.01030*.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024b) Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu
    Li, Hao Peng, 和 Heng Ji. 2024b. 可执行代码操作引发更好的 llm 代理。*arXiv 预印本 arXiv:2402.01030*。
- en: 'Wang et al. (2023a) Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan
    Yuan, Hao Peng, and Heng Ji. 2023a. Mint: Evaluating llms in multi-turn interaction
    with tools and language feedback. *arXiv preprint arXiv:2309.10691*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023a) Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan
    Yuan, Hao Peng, 和 Heng Ji. 2023a. Mint: 评估 llms 在与工具和语言反馈的多轮互动中的表现。*arXiv 预印本
    arXiv:2309.10691*。'
- en: 'Wang et al. (2023b) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu
    Wei, and Heng Ji. 2023b. Unleashing cognitive synergy in large language models:
    A task-solving agent through multi-persona self-collaboration. *arXiv preprint
    arXiv:2307.05300*, 1(2):3.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu
    Wei, 和 Heng Ji. 2023b. 在大型语言模型中释放认知协同：通过多角色自我协作解决任务的代理。*arXiv 预印本 arXiv:2307.05300*，1(2):3。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. 思维链提示在大语言模型中引发推理。*神经信息处理系统进展*，35:24824–24837。
- en: 'Wei et al. (2024) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. 2024. [Magicoder: Empowering code generation with oss-instruct](http://arxiv.org/abs/2312.02120).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2024) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, 和 Lingming
    Zhang. 2024. [Magicoder: 用 oss-instruct 增强代码生成](http://arxiv.org/abs/2312.02120)。'
- en: 'Yang et al. (2024) Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun
    Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, et al. 2024. Matplotagent:
    Method and evaluation for llm-based agentic scientific data visualization. *arXiv
    preprint arXiv:2402.11453*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2024) Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun
    Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, 等. 2024. Matplotagent:
    基于 llm 的代理科学数据可视化的方法与评估。*arXiv 预印本 arXiv:2402.11453*。'
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2022. React: 在语言模型中协同推理与行动。*arXiv 预印本 arXiv:2210.03629*。'
- en: 'Zheng et al. (2024a) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang,
    Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and
    Jie Tang. 2024a. [Codegeex: A pre-trained model for code generation with multilingual
    benchmarking on humaneval-x](http://arxiv.org/abs/2303.17568).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '郑等（2024a）郑钦凯，夏晓，邹旭，董玉晓，王杉，薛宇飞，王子涵，沈磊，王安迪，李杨，苏腾，杨志林，唐杰。2024a。 [Codegeex: 一个用于代码生成的预训练模型，具有多语言基准测试，针对
    humaneval-x](http://arxiv.org/abs/2303.17568)。'
- en: 'Zheng et al. (2024b) Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen
    Lin, Jie Fu, Wenhu Chen, and Xiang Yue. 2024b. Opencodeinterpreter: Integrating
    code generation with execution and refinement. *arXiv preprint arXiv:2402.14658*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '郑等（2024b）郑天宇，张戈，沈天豪，刘雪玲，林钰辰，傅洁，陈文虎，岳翔。2024b。Opencodeinterpreter: 将代码生成与执行和优化结合起来。*arXiv
    预印本 arXiv:2402.14658*。'
- en: Appendix A Prompts
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 提示
- en: A.1 Equip LLM with a code interpreter
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 为 LLM 配备代码解释器
- en: 'Fig [5](#A1.F5 "Figure 5 ‣ A.1 Equip LLM with a code interpreter ‣ Appendix
    A Prompts ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks")
    shows how we prompt the LLM to use code interpreter.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#A1.F5 "图 5 ‣ A.1 为 LLM 配备代码解释器 ‣ 附录 A 提示 ‣ PyBench: 评估 LLM 代理在各种实际编程任务中的表现")
    显示了如何提示 LLM 使用代码解释器。'
- en: '![Refer to caption](img/7c27befca707751275386bef97ea1cfc.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c27befca707751275386bef97ea1cfc.png)'
- en: 'Figure 5: Prompt equipping LLM Agent with a code interpreter'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 为 LLM 代理配备代码解释器的提示'
- en: A.2 Query Generation Prompt
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 查询生成提示
- en: 'Fig [6](#A1.F6 "Figure 6 ‣ A.2 Query Generation Prompt ‣ Appendix A Prompts
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") is the prompt
    to generate a related and diverse query from given file content.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#A1.F6 "图 6 ‣ A.2 查询生成提示 ‣ 附录 A 提示 ‣ PyBench: 评估 LLM 代理在各种实际编程任务中的表现")
    是从给定文件内容生成相关且多样化查询的提示。'
- en: '![Refer to caption](img/e8b08f3ae5fbfa231c0575757b2fcaeb.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e8b08f3ae5fbfa231c0575757b2fcaeb.png)'
- en: 'Figure 6: Prompt guide the LLM to generate queries'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 提示引导 LLM 生成查询'
- en: A.3 Checker Prompt
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 检查器提示
- en: 'Fig [7](#A1.F7 "Figure 7 ‣ A.3 Checker Prompt ‣ Appendix A Prompts ‣ PyBench:
    Evaluating LLM Agent on various real-world coding tasks") and Fig [8](#A1.F8 "Figure
    8 ‣ A.3 Checker Prompt ‣ Appendix A Prompts ‣ PyBench: Evaluating LLM Agent on
    various real-world coding tasks") is the prompt for content checker and keywords
    checker.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#A1.F7 "图 7 ‣ A.3 检查器提示 ‣ 附录 A 提示 ‣ PyBench: 评估 LLM 代理在各种实际编程任务中的表现")
    和图 [8](#A1.F8 "图 8 ‣ A.3 检查器提示 ‣ 附录 A 提示 ‣ PyBench: 评估 LLM 代理在各种实际编程任务中的表现") 是内容检查器和关键词检查器的提示。'
- en: '![Refer to caption](img/05c62c9c70eddce3eedd34a7bff7c279.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/05c62c9c70eddce3eedd34a7bff7c279.png)'
- en: 'Figure 7: Prompt of The Content Checker'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 内容检查器的提示'
- en: '![Refer to caption](img/0f1a1908551871c157657d9db81c564d.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f1a1908551871c157657d9db81c564d.png)'
- en: 'Figure 8: Prompt of the Keyword Checker'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 关键词检查器的提示'
- en: A.4 Evaluation Prompt
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 评估提示
- en: '![Refer to caption](img/bf744db4fc1ce54c93abd9a2b75e231c.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bf744db4fc1ce54c93abd9a2b75e231c.png)'
- en: 'Figure 9: Evaluation Prompt'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 评估提示'
- en: Appendix B Key words example for each category
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 每个类别的关键词示例
- en: 'Fig [10](#A2.F10 "Figure 10 ‣ Appendix B Key words example for each category
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") is a keywords
    list example for Chart Analysis tasks'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [10](#A2.F10 "图 10 ‣ 附录 B 每个类别的关键词示例 ‣ PyBench: 评估 LLM 代理在各种实际编程任务中的表现")
    是图表分析任务的关键词列表示例。'
- en: '![Refer to caption](img/ce42e7f360ebb37adb38993c56fd3e0a.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce42e7f360ebb37adb38993c56fd3e0a.png)'
- en: 'Figure 10: Keywords list on Chart Data Analysis'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 图表数据分析中的关键词列表'
- en: Appendix C Example Unit Test
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 示例单元测试
- en: We show three types of Unit Tests in this appendix.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个附录中展示了三种类型的单元测试。
- en: C.1 Directly Verify the Answer
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 直接验证答案
- en: For the task with a fixed answer, we check whether the final response contains
    the answer
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有固定答案的任务，我们检查最终响应是否包含答案。
- en: 1def  test_task_19(trajectory):2  final_answer=trajectory[-1][’content’]3  answer_list=["-346","2504"]4  for  ans  in  answer_list:5  assert  ans  in  final_answer
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 1def  test_task_19(trajectory):2  final_answer=trajectory[-1][’content’]3  answer_list=["-346","2504"]4  for  ans  in  answer_list:5  assert  ans  in  final_answer
- en: C.2 Verify the output file
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 验证输出文件
- en: For task clarify the output file, we check whether the output file fulfills
    the requirements.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任务明确输出文件，我们检查输出文件是否满足要求。
- en: 1def  test_task_88(trajectory):2  image_path  =  "./output/88.png"3  ref_path  =  "./data/88.jpeg"4  image  =  Image.open(image_path)5  gray_image  =  image.convert(’L’)6  gray_array  =  np.array(gray_image)7  black_threshold  =  508  black_pixels  =  np.sum(gray_array  <=  black_threshold)9  total_pixels  =  gray_array.size10  black_ratio  =  black_pixels  /  total_pixels11  assert  black_ratio  >  0.25,  "The  black  pixel  percentage  does  not  exceed  30%."
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 1def  test_task_88(trajectory):2  image_path  =  "./output/88.png"3  ref_path  =  "./data/88.jpeg"4  image  =  Image.open(image_path)5  gray_image  =  image.convert(’L’)6  gray_array  =  np.array(gray_image)7  black_threshold  =  508  black_pixels  =  np.sum(gray_array  <=  black_threshold)9  total_pixels  =  gray_array.size10  black_ratio  =  black_pixels  /  total_pixels11  assert  black_ratio  >  0.25,  "黑色像素百分比不超过
    30%。"
- en: C.3 Verify the output file path
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3 验证输出文件路径
- en: For Open-end tasks like website development, we simply detect whether the output
    file exists.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如网站开发这类开放性任务，我们只需检测输出文件是否存在。
- en: 1def  test_task_141(trajectory):2  output_folder  =  "./output/141"3  html_files_exist  =  any(file.endswith(’.html’)  for  file  in  os.listdir(output_folder))4  assert  html_files_exist
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 1def  test_task_141(trajectory):2  output_folder  =  "./output/141"3  html_files_exist  =  any(file.endswith(’.html’)  for  file  in  os.listdir(output_folder))4  assert  html_files_exist
- en: Appendix D Code as Action VS Function Calling
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 行为代码与函数调用
- en: We conducted an ablation study on the format of calling code interpreter. For
    function calling, we defined an execute_python function where the LLM passes a
    code string as the parameter and uses a special token <|tool_call|> before the
    function.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对调用代码解释器的格式进行了消融研究。对于函数调用，我们定义了一个 execute_python 函数，其中 LLM 传递一个代码字符串作为参数，并在函数前使用特殊标记<|tool_call|>。
- en: 'We repurposed all code snippets in PyInstruct, CodeFeedback, and CodeActInstruct
    to follow the function calling format. We trained the model without continue pre-train
    on code-rich corpus for it exactly aligns with our format. Compare it to PyLlama3(w/o
    cpt), Table [5](#A4.T5 "Table 5 ‣ Appendix D Code as Action VS Function Calling
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") shows that
    the function calling format struggles with real-world code tasks. The model often
    fails to follow the format and frequently neglects to call the execute_python
    tool, leading to failures.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将 PyInstruct、CodeFeedback 和 CodeActInstruct 中的所有代码片段重新调整为符合函数调用格式。我们训练模型时没有继续在代码丰富的语料库上预训练，以便其与我们的格式完全一致。与
    PyLlama3(w/o cpt) 比较，表 [5](#A4.T5 "表 5 ‣ 附录 D 行为代码与函数调用 ‣ PyBench: 评估 LLM 代理在各种实际编码任务中的表现")
    显示函数调用格式在实际代码任务中表现不佳。模型经常无法遵循格式，并且经常忽视调用 execute_python 工具，导致失败。'
- en: '| Format | Pass Rate | Average Turns |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 通过率 | 平均回合数 |'
- en: '| --- | --- | --- |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Function call | 19.6 | 8.3 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 函数调用 | 19.6 | 8.3 |'
- en: '| Ours | 58.7 | 6.3 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 58.7 | 6.3 |'
- en: 'Table 5: Function Call vs Our format'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 函数调用与我们的格式'
- en: Appendix E Compare LLM Evaluator and Unit Test
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 比较 LLM 评估器与单元测试
- en: '| Good | Same | Bad |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 良好 | 相同 | 不良 |'
- en: '| 12 | 43 | 5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 43 | 5 |'
- en: 'Table 6: Compare the LLM Evaluator with the Unit Test. "Good" indicates that
    the LLM Evaluator is correct while the Unit Test is incorrect. "Same" means both
    the LLM Evaluator and the Unit Test provide the same pass-or-fail decision. "Bad"
    signifies that the LLM Evaluator is incorrect while the Unit Test is correct.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 比较 LLM 评估器与单元测试。"良好" 表示 LLM 评估器正确而单元测试不正确。"相同" 表示 LLM 评估器和单元测试提供相同的通过或失败决策。"不良"
    表示 LLM 评估器不正确而单元测试正确。'
- en: 'We randomly select 60 samples from all the generated trajectories and judge
    them manually. Table [6](#A5.T6 "Table 6 ‣ Appendix E Compare LLM Evaluator and
    Unit Test ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks")
    shows in most situations, LLM Evaluator and Unit Test reach a consensus. In other
    situations, LLM Evaluator and Unit Test has their own advantages and drawbacks.
    Figure [11](#A5.F11 "Figure 11 ‣ Appendix E Compare LLM Evaluator and Unit Test
    ‣ PyBench: Evaluating LLM Agent on various real-world coding tasks") show two
    examples.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从所有生成的轨迹中随机选择了 60 个样本并手动判断。表 [6](#A5.T6 "表 6 ‣ 附录 E 比较 LLM 评估器与单元测试 ‣ PyBench:
    评估 LLM 代理在各种实际编码任务中的表现") 显示，在大多数情况下，LLM 评估器和单元测试达成了一致。在其他情况下，LLM 评估器和单元测试各有优缺点。图
    [11](#A5.F11 "图 11 ‣ 附录 E 比较 LLM 评估器与单元测试 ‣ PyBench: 评估 LLM 代理在各种实际编码任务中的表现")
    显示了两个例子。'
- en: '![Refer to caption](img/c23eafc4a1bd47ac17c269fd2912b9b1.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c23eafc4a1bd47ac17c269fd2912b9b1.png)'
- en: 'Figure 11: Compare Unit Test and LLM Evaluator'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 比较单元测试和 LLM 评估器'
- en: Appendix F Python Packages in our environment
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 我们环境中的 Python 包
- en: absl-pyanalytics-pythonattrsaudioreadbeautifulsoup4bokehcachetoolsCairoSVGcairosvgchardetclickclick-pluginscompressed-rtfdebugpydecoratordefusedxmldeprecatdocx2txtebooklibemail-validatorfastapifastjsonschemafbprophetffmpeg-pythonffmpyfirefitzFlaskflaskFlask-CacheBusterflask-cachebusterFlask-Corsflask-corsFlask-Loginflask-loginfonttoolsfrontendfpdffuturefuzzywuzzygensim==3.8.2gradiographvizh5pyhtml5libhttpxIMAPClientimageioimageio-ffmpegimgkitipythonJinja2jinja2jiebajson5jsonpicklejsonschemajupyter-clientjupyter-corejupyter-serverjupyterlabjupyterlab-pygmentsjupyterlab-serverkeraslangchainlangchain-experimentallibrosalogurulxmlmarkdown2markdownifyMarkupSafematplotlibmatplotlib-inlinematplotlib-vennmoviepymurmurhashnbclientnbconvertnbformatnetworkxnltknotebooknumbanumexprnumpynumpy-financialopenaiopencv-pythonopenpyxlorjsonpandaspdf2imagepdfkitpdfminer.sixpdfplumberPillowpillowplotlypsutilPyAudioPyMuPDFpydanticpydubPygmentspygmentspylogPyMuPDFpymupdfpypandocpyparsingPyPDF2pypdf2==2.12.0pytesseractpytestpython-dateutilpython-docxpython-multipartpython-pptxpytzPyWaveletspywaveletspygamePyYAMLpyyamlqrcoderarfilerequestsscikit-imagescikit-learnscipyseabornsentencepieceShapelysoundfileSoundFileSpeechRecognitionspeechrecognitionstarlettestatsmodelssvglibsvgwritesympysumytabulatabulatetextblobtifffiletomltorchtorchaudiotorchtexttorchvisiontornadotqdmtyping-extensionstzdatatzlocalujsonurllib3uvicornWandwandwebsocket-clientwebsocketsWerkzeugwerkzeugwordcloudxgboostxlrdXlsxWriterxlsxwriterxml-pythonzipp
  id: totrans-253
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: absl-pyanalytics-pythonattrs音频读取beautifulsoup4bokeh缓存工具CairoSVGcairosvgchardetclickclick-pluginscompressed-rtfdebugpy装饰器defusedxmldeprecatdocx2txtebooklibemail-validatorfastapifastjsonschemafbprophetffmpeg-pythonffmpyfirefitzFlaskflaskFlask-CacheBusterflask-cachebusterFlask-Corsflask-corsFlask-Loginflask-loginfonttools前端fpdffuturefuzzywuzzygensim==3.8.2gradiographvizh5pyhtml5libhttpxIMAPClientimageioimageio-ffmpegimgkitipythonJinja2jinja2jiebajson5jsonpicklejsonschemajupyter-clientjupyter-corejupyter-serverjupyterlabjupyterlab-pygmentsjupyterlab-serverkeraslangchainlangchain-experimentallibrosalogurulxmlmarkdown2markdownifyMarkupSafematplotlibmatplotlib-inlinematplotlib-vennmoviepymurmurhashnbclientnbconvertnbformatnetworkxnltknotebooknumbanumexprnumpynumpy-financialopenaiopencv-pythonopenpyxlorjsonpandaspdf2imagepdfkitpdfminer.sixpdfplumberPillowpillowplotlypsutilPyAudioPyMuPDFpydanticpydubPygmentspygmentspylogPyMuPDFpymupdfpypandocpyparsingPyPDF2pypdf2==2.12.0pytesseractpytestpython-dateutilpython-docxpython-multipartpython-pptxpytzPyWaveletspywaveletspygamePyYAMLpyyamlqrcoderarfilerequestsscikit-imagescikit-learnscipyseabornsentencepieceShapelysoundfileSoundFileSpeechRecognitionspeechrecognitionstarlettestatsmodelssvglibsvgwritesympysumytabulatabulatetextblobtifffiletomltorchtorchaudiotorchtexttorchvisiontornadotqdmtyping-extensionstzdatatzlocalujsonurllib3uvicornWandwandwebsocket-clientwebsocketsWerkzeugwerkzeugwordcloudxgboostxlrdXlsxWriterxlsxwriterxml-pythonzipp
