- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:53:19'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:19
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Executable Code Actions Elicit Better LLM Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可执行代码动作引发更好的LLM代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01030](https://ar5iv.labs.arxiv.org/html/2402.01030)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.01030](https://ar5iv.labs.arxiv.org/html/2402.01030)
- en: \NewDocumentCommand\hao
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \NewDocumentCommand\hao
- en: mO ^(Hao)[#1] \NewDocumentCommand\heng mO ^(Heng)[#1] \NewDocumentCommand\xingyao
    mO ^(Xingyao)[#1] \NewDocumentCommand\yy mO ^(coolYY)[#1] \NewDocumentCommand\lifan
    mO ^(Lifan)[#1] \NewDocumentCommand\yizhe mO ^(Yizhe)[#1] \NewDocumentCommand\yunzhu
    mO ^(Yunzhu)[#1]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: mO ^(Hao)[#1] \NewDocumentCommand\heng mO ^(Heng)[#1] \NewDocumentCommand\xingyao
    mO ^(Xingyao)[#1] \NewDocumentCommand\yy mO ^(coolYY)[#1] \NewDocumentCommand\lifan
    mO ^(Lifan)[#1] \NewDocumentCommand\yizhe mO ^(Yizhe)[#1] \NewDocumentCommand\yunzhu
    mO ^(Yunzhu)[#1]
- en: Xingyao Wang¹, Yangyi Chen¹, Lifan Yuan¹,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 王星耀¹，陈阳逸¹，袁丽凡¹，
- en: Yizhe Zhang², Yunzhu Li¹, Hao Peng¹, Heng Ji¹
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 张一哲²，李云竹¹，彭昊¹，季恒¹
- en: ¹ University of Illinois Urbana-Champaign, ² Apple
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 伊利诺伊大学厄本那-香槟分校，² 苹果公司
- en: ¹{xingyao6,yangyic3,yunzhuli,haopeng,hengji}@illinois.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ¹{xingyao6,yangyic3,yunzhuli,haopeng,hengji}@illinois.edu
- en: ²yizhe_zhang@apple.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ²yizhe_zhang@apple.com
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Model (LLM) agents, capable of performing a broad range of actions,
    such as invoking tools and controlling robots, show great potential in tackling
    real-world challenges. LLM agents are typically prompted to produce actions by
    generating JSON or text in a pre-defined format, which is usually limited by constrained
    action space (e.g., the scope of pre-defined tools) and restricted flexibility
    (e.g., inability to compose multiple tools). This work proposes to use executable
    Python code to consolidate LLM agents’ actions into a unified action space (CodeAct).
    Integrated with a Python interpreter, CodeAct can execute code actions and dynamically
    revise prior actions or emit new actions upon new observations through multi-turn
    interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated
    benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher
    success rate). The encouraging performance of CodeAct motivates us to build an
    open-source LLM agent that interacts with environments by executing interpretable
    code and collaborates with users using natural language. To this end, we collect
    an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions
    using CodeAct. We show that it can be used with existing data to improve models
    in agent-oriented tasks without compromising their general capability. CodeActAgent,
    finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely
    tailored to perform sophisticated tasks (e.g., model training) using existing
    libraries and autonomously self-debug.¹¹1The code, data, model, and an online
    demo for practitioners to try out are available at [https://github.com/xingyaoww/code-act](https://github.com/xingyaoww/code-act).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）代理能够执行广泛的动作，例如调用工具和控制机器人，在解决现实世界挑战方面展现出巨大潜力。LLM代理通常通过生成JSON或文本的预定义格式来触发动作，这通常受限于受限的动作空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）。本文提出使用可执行的Python代码将LLM代理的动作整合到一个统一的动作空间中（CodeAct）。集成了Python解释器的CodeAct可以执行代码动作，并通过多轮交互动态修正先前的动作或在新的观察结果下发出新动作。我们对17个LLM在API-Bank和一个新创建的基准上的广泛分析表明，CodeAct在成功率上超越了广泛使用的替代方案（高达20%）。CodeAct的鼓舞人心的表现促使我们构建一个开源LLM代理，通过执行可解释代码与环境互动，并使用自然语言与用户合作。为此，我们收集了一个名为CodeActInstruct的指令调优数据集，该数据集包含7千个使用CodeAct的多轮互动。我们展示了它可以与现有数据一起使用，以在不妨碍模型一般能力的情况下改善代理导向任务中的模型。CodeActAgent从Llama2和Mistral中微调，集成了Python解释器，并独特地定制为使用现有库执行复杂任务（例如，模型训练）并自主自我调试。¹¹1代码、数据、模型以及供从业者尝试的在线演示可以在[https://github.com/xingyaoww/code-act](https://github.com/xingyaoww/code-act)上找到。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/8048625f25356c4a6cae40a5b640a85c.png)![Refer to caption](img/b52146bae5e09972df40fbd07df70bcc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8048625f25356c4a6cae40a5b640a85c.png)![参见说明](img/b52146bae5e09972df40fbd07df70bcc.png)'
- en: 'Figure 1: Comparison between CodeAct and Text / JSON as action. (top) Illustrative
    example comparing different actions. (bottom) Quantitative results on M³ToolEval
    (§[2.3](#S2.SS3 "2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：CodeAct与Text / JSON作为动作的比较。（顶部）不同动作的示例比较。（底部）M³ToolEval上的定量结果（§[2.3](#S2.SS3
    "2.3 CodeAct完成更多任务且交互更少 ‣ 2 CodeAct使LLMs成为更好的代理 ‣ 可执行代码动作引发更好的LLM代理")）。
- en: Large Language Models (LLMs) have emerged as a pivotal breakthrough in natural
    language processing (NLP). When augmented with action modules that allow access
    to APIs, their action space expands beyond conventional text processing, allowing
    LLMs to acquire capabilities such as tool invocation and memory management [[30](#bib.bib30),
    [41](#bib.bib41)] and venture into real-world tasks such as controlling robots [[1](#bib.bib1),
    [19](#bib.bib19), [29](#bib.bib29)] and performing scientific experiments [[3](#bib.bib3)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已成为自然语言处理（NLP）领域的关键突破。当结合能够访问API的行动模块时，它们的行动空间超出了传统的文本处理，使LLMs能够获得诸如工具调用和内存管理的能力[[30](#bib.bib30),
    [41](#bib.bib41)]，并涉足控制机器人[[1](#bib.bib1), [19](#bib.bib19), [29](#bib.bib29)]和进行科学实验[[3](#bib.bib3)]等现实世界任务。
- en: 'We inquire: how to effectively expand LLM agents’ action space for solving
    complex real-world problems? Much existing research has examined using text [[62](#bib.bib62),
    [34](#bib.bib34), inter alia] or JSON [[40](#bib.bib40), [5](#bib.bib5), inter
    alia] to produce actions (e.g., tool uses in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Executable Code Actions Elicit Better LLM Agents") top left). However, both
    methods typically suffer from constrained scope of action spaces (actions are
    usually tailored for specific tasks) and restricted flexibility (e.g., inability
    to compose multiple tools in a single action). As an alternative approach, several
    work [[26](#bib.bib26), [44](#bib.bib44), [48](#bib.bib48)] demonstrate the potential
    of using LLMs to generate code to control robots or game characters. However,
    they typically rely on pre-specified control primitives and hand-engineered prompts
    and, more importantly, struggle to dynamically adjust or emit actions based on
    new environmental observation and feedback.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们询问：如何有效扩展LLM代理的行动空间以解决复杂的现实世界问题？许多现有研究已考察了使用文本[[62](#bib.bib62), [34](#bib.bib34),
    其他]或JSON[[40](#bib.bib40), [5](#bib.bib5), 其他]来生成动作（例如图[1](#S1.F1 "图 1 ‣ 1 介绍
    ‣ 可执行代码动作引发更好的LLM代理")左上方）。然而，这两种方法通常面临行动空间受限（动作通常为特定任务量身定制）和灵活性受限（例如，无法在单个动作中组合多个工具）的困境。作为替代方法，一些工作[[26](#bib.bib26),
    [44](#bib.bib44), [48](#bib.bib48)]展示了使用LLMs生成代码来控制机器人或游戏角色的潜力。然而，它们通常依赖于预先指定的控制原语和手工设计的提示，更重要的是，难以根据新的环境观察和反馈动态调整或发出动作。
- en: 'This work proposes CodeAct, a general-purpose framework that allows LLMs to
    generate executable Python code as actions (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Executable Code Actions Elicit Better LLM Agents") top right). CodeAct is designed
    to handle a variety of applications and comes with unique advantages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了CodeAct，一个通用框架，允许LLMs生成可执行的Python代码作为动作（图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 可执行代码动作引发更好的LLM代理")右上方）。CodeAct旨在处理各种应用，并具有独特的优势：
- en: (1)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Integrated with a Python interpreter, CodeAct can execute code actions and dynamically
    adjust prior actions or emit new action based on observations it receives through
    multiple turns of interactions (code execution).
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeAct与Python解释器集成，可以执行代码动作，并根据通过多轮交互（代码执行）接收到的观察结果动态调整先前的动作或发出新的动作。
- en: (2)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: Code actions allow LLM to leverage existing software packages. CodeAct can use
    readily available Python packages for an expanded action space instead of hand-crafted
    task-specific tools [[64](#bib.bib64), [42](#bib.bib42)]. It also allows LLM to
    use automated feedback (e.g., error messages) implemented in most software to
    improve task-solving by self-debugging its generated code [[8](#bib.bib8), [52](#bib.bib52)].
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码动作允许LLM利用现有的软件包。CodeAct可以使用现成的Python包来扩展行动空间，而不是手工制作的特定任务工具[[64](#bib.bib64),
    [42](#bib.bib42)]。它还允许LLM使用大多数软件中实现的自动反馈（例如错误信息）来通过自我调试生成的代码来提高任务解决能力[[8](#bib.bib8),
    [52](#bib.bib52)]。
- en: (3)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: Code data is widely used in pre-training today’s LLMs [[58](#bib.bib58)]. These
    models are already familiar with structured programming languages, allowing cost-effective
    adoption of CodeAct.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码数据在预训练今天的LLMs中被广泛使用[[58](#bib.bib58)]。这些模型已经熟悉结构化编程语言，从而使CodeAct的采用具有成本效益。
- en: (4)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4)
- en: Compared to JSON and text with a pre-defined format, code inherently supports
    control and data flow, allowing for the storage of intermediate results as variables
    for reuse and the composition of multiple tools to perform complex logical operations
    (e.g., if-statements, for-loops) with one piece of code, thereby unlocking LLMs’
    potential to tackle complex tasks by leveraging its pre-trained knowledge of programming.
    In Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit
    Better LLM Agents"), an LLM using with CodeAct (top right) can apply the same
    sequence of tools (e.g., passing one tool’s output as input to another tool using
    the data flow feature) to all inputs through for-loops (i.e., control flow feature)
    with one action; while text or JSON have to take action for every input (top left).
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与具有预定义格式的JSON和文本相比，代码本质上支持控制和数据流，允许将中间结果存储为变量以供重用，并将多个工具组合起来以执行复杂的逻辑操作（例如if语句、for循环），从而利用LLM对编程的预训练知识来解锁其解决复杂任务的潜力。在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents")中，使用CodeAct的LLM（右上角）可以通过for循环（即控制流功能）对所有输入应用相同的工具序列（例如，将一个工具的输出作为输入传递给另一个工具），而文本或JSON则需要对每个输入进行操作（左上角）。
- en: Our extensive experiments with 17 LLMs (including both open-source and proprietary
    ones) confirm the above benefits (3 & 4) of CodeAct. To demonstrate benefit (3),
    our first experiment (§[2.2](#S2.SS2 "2.2 CodeAct Shows the Promise as a Strong
    Tool Use Framework ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents")) compares CodeAct to baselines on basic tasks involving
    atomic tool use (i.e., only one tool is used per action), ablating the control
    and data flow advantage offered by CodeAct. The results show that, for most LLMs,
    CodeAct achieves comparable or better performance than the baselines. CodeAct’s
    performance gains are more prominent on complex tasks, as demonstrated in our
    second experiment (benefit 4). We curate a new benchmark consisting of 82 human-curated
    tasks that typically require multiple calls to multiple tools in multi-turn interactions
    (M³ToolEval; §[2.3](#S2.SS3 "2.3 CodeAct Gets More Done with Fewer Interactions
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents")). Problems in this benchmark often require intricate coordination and
    composition of multiple tools. With its strengths in control and data flow, CodeAct
    achieves up to a 20% absolute improvement over baselines on the success rate of
    solving the problems while requiring up to 30% fewer actions. These performance
    gains widen as the capabilities of the LLMs increase (Fig. [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents") bottom).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对17种LLM（包括开源和专有的）进行的广泛实验证实了CodeAct的上述好处（3和4）。为了展示好处（3），我们的第一次实验（§[2.2](#S2.SS2
    "2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")）将CodeAct与基线在涉及原子工具使用的基本任务上进行了比较（即每个动作仅使用一个工具），并剔除了CodeAct提供的控制和数据流优势。结果表明，对于大多数LLM，CodeAct的性能与基线相当或更好。CodeAct在复杂任务上的性能提升更为显著，如我们第二次实验所示（好处4）。我们策划了一个新的基准测试，包含82个由人工策划的任务，这些任务通常需要在多轮交互中调用多个工具（M³ToolEval；§[2.3](#S2.SS3
    "2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")）。这个基准中的问题通常需要复杂的多工具协调和组合。凭借在控制和数据流方面的优势，CodeAct在解决问题的成功率上比基线提高了最高20%的绝对值，同时需要减少最多30%的操作。这些性能提升随着LLM能力的提高而扩大（图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents")底部）。
- en: 'The promising performance of CodeAct motivates an open-source LLM agent that
    can effectively act through CodeAct, and collaborate with humans through natural
    language. To this end, we collect an instruction-tuning dataset CodeActInstruct
    consisting of 7k high-quality multi-turn interaction trajectories with CodeAct
    (§[3.1](#S3.SS1 "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")). CodeActInstruct is motivated by a general agent framework
    consisting of agent, user, and environments (Fig. [2](#S2.F2 "Figure 2 ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"))
    and focuses on agent-environment interactions with the computer (information seeking,
    software package use, external memory) and the physical world (robot planning).
    On CodeActInstruct, we perform careful data selection to promote the capability
    of improving from multi-turn interaction (e.g., self-debug). We show that CodeActInstruct
    can be used with commonly used instruction tuning data to improve the models’
    performance in agent tasks without compromising their general capabilities (e.g.,
    knowledge-based QA, coding, instruction following, §[3.2](#S3.SS2 "3.2 CodeActAgent
    ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable Code
    Actions Elicit Better LLM Agents")). Our model, dubbed CodeActAgent, is finetuned
    from LLaMA-2 [[47](#bib.bib47)] and Mistral-7B [[20](#bib.bib20)] and improves
    on out-of-domain agent tasks with not only CodeAct, but also text action in a
    pre-defined format (§[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CodeAct 的出色表现激励了一个开源的 LLM 代理，该代理能够通过 CodeAct 有效行动，并通过自然语言与人类合作。为此，我们收集了一个指令调优数据集
    CodeActInstruct，包含 7k 高质量的与 CodeAct 进行的多轮交互轨迹（§[3.1](#S3.SS1 "3.1 CodeActInstruct：代理-环境交互
    ‣ 3 赋能开源 LLM 代理更好地使用 CodeAct ‣ 可执行代码操作引发更好的 LLM 代理")）。CodeActInstruct 的灵感来源于一个包含代理、用户和环境的一般代理框架（图
    [2](#S2.F2 "图 2 ‣ 2 CodeAct 使 LLM 成为更好的代理 ‣ 可执行代码操作引发更好的 LLM 代理")），并专注于与计算机（信息检索、软件包使用、外部记忆）和物理世界（机器人规划）的代理-环境交互。在
    CodeActInstruct 上，我们进行仔细的数据选择，以促进从多轮交互中提升能力（例如，自我调试）。我们展示了 CodeActInstruct 可以与常用的指令调优数据一起使用，以提高模型在代理任务中的表现，而不妨碍其通用能力（例如，基于知识的
    QA、编码、指令跟随，§[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 赋能开源 LLM 代理更好地使用 CodeAct ‣ 可执行代码操作引发更好的
    LLM 代理")）。我们的模型，名为 CodeActAgent，从 LLaMA-2 [[47](#bib.bib47)] 和 Mistral-7B [[20](#bib.bib20)]
    微调而来，并在处理领域外的代理任务时不仅使用 CodeAct，还使用预定义格式的文本操作（§[3.2](#S3.SS2 "3.2 CodeActAgent
    ‣ 3 赋能开源 LLM 代理更好地使用 CodeAct ‣ 可执行代码操作引发更好的 LLM 代理")）。
- en: CodeAct can further benefit from multi-turn interactions and existing software
    (benefit 1 & 2, §[2.4](#S2.SS4 "2.4 CodeAct Benefits from Multi-turn Interactions
    and Existing Software Packages ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable
    Code Actions Elicit Better LLM Agents")). As shown in Fig. [3](#S2.F3 "Figure
    3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs
    Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"), CodeActAgent,
    designed for seamless integration with Python, can carry out sophisticated tasks
    (e.g., model training, data visualization) using existing Python packages. Error
    messages from the environment further enable it to rectify errors autonomously
    through self-debugging in multi-turn interaction. Thanks to LLM’s extensive programming
    knowledge acquired during pre-training, these are achieved without needing in-context
    demonstrations, reducing the human efforts for adapting CodeActAgent to different
    tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: CodeAct 可以进一步从多轮交互和现有软件中受益（优势 1 和 2，§[2.4](#S2.SS4 "2.4 CodeAct 从多轮交互和现有软件包中获益
    ‣ 2 CodeAct 使 LLM 成为更好的代理 ‣ 可执行代码操作引发更好的 LLM 代理")）。如图 [3](#S2.F3 "图 3 ‣ 2.3 CodeAct
    在更少的交互中完成更多任务 ‣ 2 CodeAct 使 LLM 成为更好的代理 ‣ 可执行代码操作引发更好的 LLM 代理") 所示，为了与 Python
    无缝集成而设计的 CodeActAgent 可以使用现有的 Python 包执行复杂任务（例如，模型训练、数据可视化）。来自环境的错误信息进一步使其能够通过在多轮交互中的自我调试自动修正错误。由于
    LLM 在预训练期间获得的广泛编程知识，这些都是在无需上下文演示的情况下实现的，从而减少了将 CodeActAgent 适应不同任务所需的人力。
- en: 'Table 1: The benefit of CodeAct compared to using Text/JSON for LLM action.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：CodeAct 相较于使用 Text/JSON 执行 LLM 操作的优势。
- en: '|  | CodeAct for LLM action | JSON or Text for LLM action |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | CodeAct 执行 LLM 操作 | JSON 或文本执行 LLM 操作 |'
- en: '| Availability of Data | ✔Large quantity of code available¹ for pre-training
    | ✗Data curation required for particular format |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 数据的可用性 | ✔大量代码可用于预训练¹ | ✗特定格式的数据整理 |'
- en: '| Complex Operation (e.g., looping, composition of multiple tools) | ✔Natively
    supported via control and data flow | ✗Requires careful engineering if feasible
    (e.g., define new tools to mimic if-statement) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 复杂操作（例如，循环、多个工具的组合） | ✔通过控制和数据流本地支持 | ✗如果可行则需要仔细工程（例如，定义新的工具以模拟 if 语句） |'
- en: '| Availability of Tools | ✔Can directly use existing software packages² | ✗Requires
    human effort to curate tools from scratch or existing software |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 工具的可用性 | ✔可以直接使用现有的软件包² | ✗需要人工从头开始或从现有软件中整理工具 |'
- en: '| Automated Feedback | ✔Feedback mechanism³ (e.g., traceback) is already implemented
    as an infrastructure for most programming languages | ✗Requires human effort to
    provide feedback or re-route feedback from the underlying programming language
    used to implement the tools |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 自动反馈 | ✔反馈机制³（例如，回溯）已作为大多数编程语言的基础设施实现 | ✗需要人工提供反馈或从底层编程语言中重新路由反馈 |'
- en: '1'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: Including code demonstrating useful behaviors for LLM agents (e.g., task decomposition,
    coordination of multiple function calls to different tools).
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括展示 LLM 代理有用行为的代码（例如，任务分解、协调对不同工具的多个函数调用）。
- en: '2'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: Human-written Python packages covering a wide range of applications are available
    on [https://pypi.org/](https://pypi.org/).
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工编写的 Python 包涵盖了广泛的应用，可以在 [https://pypi.org/](https://pypi.org/) 上找到。
- en: '3'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3'
- en: For example, in Python, errors and exceptions ([https://docs.python.org/3/tutorial/errors.html](https://docs.python.org/3/tutorial/errors.html))
    are available. Most software provides error messages in natural language to help
    human programmers debug their code. CodeAct enables LLM to use them directly.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在 Python 中，错误和异常（[https://docs.python.org/3/tutorial/errors.html](https://docs.python.org/3/tutorial/errors.html)）是可用的。大多数软件提供自然语言的错误信息，以帮助程序员调试代码。CodeAct
    使 LLM 能直接使用这些信息。
- en: 2 CodeAct Makes LLMs Better Agents
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 CodeAct 使 LLM 更好的代理
- en: 'In this section, we first describe CodeAct framework (§[2.1](#S2.SS1 "2.1 What
    is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")) and provide empirical evidence that supports the choice of
    CodeAct. We focus on Python as the programming language for CodeAct due to its
    popularity (ranked top-1 at TIOBE index [[46](#bib.bib46)]) and numerous open-source
    packages. We aim to answer several research questions (RQs) using 17 off-the-shelf
    LLMs. In §[2.2](#S2.SS2 "2.2 CodeAct Shows the Promise as a Strong Tool Use Framework
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents"), we examine RQ1: Does LLMs’ familiarity with code due to a large amount
    of code pre-training data bring CodeAct advantages over text and JSON? We discuss
    RQ2 in §[2.3](#S2.SS3 "2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2
    CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"):
    Does CodeAct benefit from Python’s innate control and data flow feature in complex
    problems? Finally, as an additional benefit, we discuss how using CodeAct further
    enhances LLM agents by enabling multi-turn interactions and allowing them to access
    existing software in §[2.4](#S2.SS4 "2.4 CodeAct Benefits from Multi-turn Interactions
    and Existing Software Packages ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable
    Code Actions Elicit Better LLM Agents") and Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 CodeAct
    Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣
    Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先描述 CodeAct 框架（§[2.1](#S2.SS1 "2.1 What is CodeAct? ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")），并提供支持选择
    CodeAct 的实证证据。我们选择 Python 作为 CodeAct 的编程语言，原因是其受欢迎程度（在 TIOBE 指数中排名第 1 [［46］](#bib.bib46)）以及大量开源包。我们旨在使用
    17 个现成的 LLM 回答几个研究问题（RQ）。在 §[2.2](#S2.SS2 "2.2 CodeAct Shows the Promise as a
    Strong Tool Use Framework ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code
    Actions Elicit Better LLM Agents") 中，我们探讨 RQ1：LLM 对代码的熟悉度由于大量的代码预训练数据是否给 CodeAct
    带来了相对于文本和 JSON 的优势？我们在 §[2.3](#S2.SS3 "2.3 CodeAct Gets More Done with Fewer Interactions
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents") 中讨论 RQ2：CodeAct 是否从 Python 的固有控制和数据流特性中受益于复杂问题？最后，作为额外的好处，我们在 §[2.4](#S2.SS4
    "2.4 CodeAct Benefits from Multi-turn Interactions and Existing Software Packages
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents") 和 Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents") 中讨论如何使用 CodeAct 进一步增强 LLM 代理，通过多轮交互和访问现有软件。
- en: '![Refer to caption](img/8bbf7689f5735116e4582d0913f53b60.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8bbf7689f5735116e4582d0913f53b60.png)'
- en: 'Figure 2: General agent multi-turn interaction framework that describes the
    role of CodeAct and motivates the construction of our data mixture. CodeActInstruct
    focuses on the agent-environment interactions and specifically filters for the
    self-improved planning behavior, while general conversation data we include focuses
    on agent-user interaction (§[3.1](#S3.SS1 "3.1 CodeActInstruct: Agent-Environment
    Interactions ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable
    Code Actions Elicit Better LLM Agents")).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：描述 CodeAct 角色的通用代理多轮交互框架，并激励我们构建数据混合体。CodeActInstruct 侧重于代理与环境的交互，并特别筛选自我改进的规划行为，而我们包含的通用对话数据则侧重于代理与用户的交互
    (§[3.1](#S3.SS1 "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents"))。'
- en: 2.1 What is CodeAct?
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 什么是 CodeAct？
- en: 'In Fig. [2](#S2.F2 "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable
    Code Actions Elicit Better LLM Agents"), we first introduce a general multi-turn
    interaction framework for LLM agents’ real-world usage that considers three roles:
    agent, user, and environment. We define interaction as the information exchange
    between the agent and an external entity (user or environment). For each turn
    of interaction, the agent receives an observation (input) either from the user
    (e.g., natural language instruction) or the environment (e.g., code execution
    result), optionally planning for its action through chain-of-thought [[56](#bib.bib56)],
    and emits an action (output) to either user in natural language or the environment.
    CodeAct employs Python code to consolidate all actions for agent-environment interaction.
    In CodeAct, each emitted action to the environment is a piece of Python code,
    and the agent will receive outputs of code execution (e.g., results, errors) as
    observation. We include an example prompt of CodeAct in §[E](#A5 "Appendix E Example
    Prompt for CodeAct ‣ Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S2.F2 "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code
    Actions Elicit Better LLM Agents") 中，我们首先介绍了一个通用的多轮交互框架，用于 LLM 代理的实际应用，该框架考虑了三个角色：代理、用户和环境。我们将交互定义为代理与外部实体（用户或环境）之间的信息交换。对于每一轮交互，代理接收来自用户（例如，自然语言指令）或环境（例如，代码执行结果）的观察（输入），可选择通过链式思维
    [[56](#bib.bib56)] 规划其行动，并向用户以自然语言或向环境发出行动（输出）。CodeAct 使用 Python 代码来巩固所有代理与环境的交互操作。在
    CodeAct 中，每个发出的环境行动都是一段 Python 代码，代理将接收代码执行的输出（例如，结果、错误）作为观察。我们在 §[E](#A5 "Appendix
    E Example Prompt for CodeAct ‣ Executable Code Actions Elicit Better LLM Agents")
    中包含了 CodeAct 的示例提示。
- en: 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 CodeAct 展现了作为强大工具使用框架的潜力
- en: In this section, we perform a controlled experiment to understand which format
    (text, JSON, CodeAct) is more likely to lead an LLM to generate correct atomic
    tool calls. The performance in this experiment reflects LLM’s familiarity with
    the corresponding format. We hypothesize that using CodeAct to call tools is a
    more natural way to use tools for the models, which typically have extensive exposure
    to code data during their training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们进行了一项控制实验，以了解哪种格式（文本、JSON、CodeAct）更可能使 LLM 生成正确的原子工具调用。该实验的性能反映了 LLM
    对相应格式的熟悉程度。我们假设，使用 CodeAct 调用工具是模型使用工具的更自然的方式，因为模型在训练期间通常接触到大量的代码数据。
- en: Setup. We re-purpose API-Bank [[24](#bib.bib24)] and test LLMs’ API-calling
    performance, comparing CodeAct, JSON, and text actions. For each evaluation instance,
    we instruct LLM to generate one atomic tool call in the format of a Python function
    call, JSON object, or text expression in a pre-defined format. A concrete example
    is shown in Tab. [A.6](#A0.T6 "Table A.6 ‣ Executable Code Actions Elicit Better
    LLM Agents"). We use API-Bank’s level-1 instructions and the provided toolset.
    To evaluate API-calling, we follow their correctness metric, matching the ground-truth
    API outputs with the actual model-generated API’s execution outputs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们重新利用 API-Bank [[24](#bib.bib24)] 并测试 LLM 的 API 调用性能，比较 CodeAct、JSON 和文本操作。对于每个评估实例，我们指示
    LLM 生成一个原子工具调用，格式为 Python 函数调用、JSON 对象或预定义格式的文本表达。具体示例如 Tab. [A.6](#A0.T6 "Table
    A.6 ‣ Executable Code Actions Elicit Better LLM Agents") 中所示。我们使用 API-Bank 的一级指令和提供的工具集。为了评估
    API 调用，我们遵循其正确性指标，将真实的 API 输出与实际模型生成的 API 执行输出进行匹配。
- en: Results. We present results in Tab. [3](#S2.T3 "Table 3 ‣ 2.2 CodeAct Shows
    the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes LLMs Better Agents
    ‣ Executable Code Actions Elicit Better LLM Agents"). For most LLMs, CodeAct achieves
    comparable or better performance even in atomic actions (the simplistic tool use
    scenario) where its control and data flow strengths are ablated. Compared to closed-source
    LLMs, CodeAct’s improvements are more prominent in open-source models. Furthermore,
    code data is usually more accessible for fine-tuning open-source LLMs than the
    specialized JSON or text tool-calling format. Although JSON is consistently weaker
    than other approaches for open-source models, it achieves decent performance with
    closed-source LLMs, indicating that these closed-source models may have gone through
    targeted fine-tuning toward their JSON capabilities. These results suggest optimizing
    for CodeAct is a better route for open-source LLMs than alternatives to improve
    their tool-use capabilities, as they already show good initial CodeAct capability
    due to extensive exposure to code data during pre-training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。我们在表 [3](#S2.T3 "表 3 ‣ 2.2 CodeAct 显示作为强大工具使用框架的潜力 ‣ 2 CodeAct 使 LLM 成为更好的代理
    ‣ 可执行代码操作引发更好的 LLM 代理") 中展示了结果。对于大多数 LLM，CodeAct 即使在原子操作（简单的工具使用场景）中，其控制和数据流优势被削弱时，仍能实现可比或更好的性能。与闭源
    LLM 相比，CodeAct 在开源模型中的改进更加显著。此外，代码数据通常比专用 JSON 或文本工具调用格式更易于获取，因此更适合对开源 LLM 进行微调。尽管
    JSON 在开源模型中始终表现较差，但在闭源 LLM 中表现尚可，表明这些闭源模型可能经过了针对 JSON 能力的专门微调。这些结果表明，针对 CodeAct
    的优化比替代方案更适合开源 LLM，以提升其工具使用能力，因为它们在预训练期间已广泛接触到代码数据，从而展示了良好的初始 CodeAct 能力。
- en: 'Table 2: Atomic API call correctness on API-Bank [[24](#bib.bib24)] with different
    action format. The best performance is bolded, and the second-best is underlined.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同操作格式下在 API-Bank [[24](#bib.bib24)] 上的原子 API 调用正确性。最佳性能已加粗，第二好的已加下划线。
- en: Correctness (%, $\uparrow$
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正确性（%，$\uparrow$
- en: 'Table 3: Success rates (higher the better) and average turns required per instance
    (lower the better) on M³ToolEval. The best results for each model are bolded,
    and the second-best ones are underlined.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 M³ToolEval 上的成功率（越高越好）和每个实例所需的平均回合数（越低越好）。每个模型的最佳结果已加粗，第二好的结果已加下划线。
- en: Success Rate (%, $\uparrow$
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 成功率（%，$\uparrow$
- en: 2.3 CodeAct Gets More Done with Fewer Interactions
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 CodeAct 在更少的互动中完成更多任务
- en: In this section, we investigate whether LLM agents can benefit from the control
    and data flow of code on problems that require complex patterns of tool use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了 LLM 代理是否能够从代码的控制和数据流中受益，特别是对于需要复杂工具使用模式的问题。
- en: M³ToolEval. As shown in Tab. [A.7](#A0.T7 "Table A.7 ‣ Executable Code Actions
    Elicit Better LLM Agents"), to the best of our knowledge, no existing tool-use
    benchmarks contain complex tasks requiring the composition of multiple tools while
    supporting evaluating different action formats. Hence, we curate a benchmark M³ToolEval
    to fill this gap, which evaluates LLMs’ capabilities in solving complex tasks
    that typically require multiple calls to multiple tools in multi-turn interactions.
    It contains 82 human-curated instances, spanning tasks including web browsing,
    finance, travel itinerary planning, science, and information processing. Each
    domain is accompanied by a unique set of manually crafted tools. We intentionally
    keep the prompt simple and avoid providing any task demonstration to test the
    LLM’s zero-shot ability to use tools, similar to how a novice user without domain
    knowledge of few-shot prompting would use the model. Please refer to §[F](#A6
    "Appendix F M3ToolEval Prompt ‣ Executable Code Actions Elicit Better LLM Agents")
    for prompt examples.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: M³ToolEval。如表 [A.7](#A0.T7 "表 A.7 ‣ 可执行代码操作引发更好的 LLM 代理") 所示，据我们所知，目前没有现有的工具使用基准包含需要组合多个工具的复杂任务，同时支持评估不同的操作格式。因此，我们策划了基准
    M³ToolEval 来填补这一空白，评估 LLM 在解决通常需要多次调用多个工具的复杂任务中的能力。它包含 82 个人工策划的实例，涵盖包括网页浏览、金融、旅行计划、科学和信息处理等任务。每个领域配有一套独特的手工制作工具。我们故意保持提示简单，并避免提供任何任务示例，以测试
    LLM 在无领域知识的情况下的零样本能力，类似于没有领域知识的初学者使用模型的方式。有关提示示例，请参见 §[F](#A6 "附录 F M3ToolEval
    提示 ‣ 可执行代码操作引发更好的 LLM 代理")。
- en: Setup. We allow the model to generate fully functional Python code that enables
    control and data flow (e.g., if-statement, for-loop). We follow the action format
    for JSON and text described in Tab. [A.6](#A0.T6 "Table A.6 ‣ Executable Code
    Actions Elicit Better LLM Agents"). Within each turn, the model can either emit
    an action or propose an answer to be verified by an exact match with the ground-truth
    solution. The interaction will terminate when a maximum of 10 interaction turns
    are reached or a correct solution has been submitted, similar to MINT [[53](#bib.bib53)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们允许模型生成完全功能的 Python 代码，以实现控制和数据流（例如 if 语句，for 循环）。我们遵循表 [A.6](#A0.T6 "表
    A.6 ‣ 可执行代码操作引发更好的 LLM 智能体") 中描述的 JSON 和文本的操作格式。在每个轮次中，模型可以发出一个操作或提出一个答案，以通过与真实解决方案的精确匹配进行验证。当达到最大
    10 次交互轮次或提交了正确解决方案时，交互将终止，类似于 MINT [[53](#bib.bib53)]。
- en: 'Metric. We measure the success rate by calculating the percentage of the model
    proposed answers that match the ground-truth solutions. We also include the avg.
    turns metric: the average number of turns on all evaluated instances.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量标准。我们通过计算模型提议的答案与真实解决方案匹配的百分比来衡量成功率。我们还包括平均轮次指标：所有评估实例的平均轮次。
- en: Quantitative Results on M³ToolEval. We include full results in Tab. [3](#S2.T3
    "Table 3 ‣ 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")
    and a subset of results for visualization in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Executable Code Actions Elicit Better LLM Agents"). CodeAct generally has a
    higher task success rate (12 out of 17 evaluated LLMs), similar to the trend in
    §[2.2](#S2.SS2 "2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣
    2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents"). Moreover, using CodeAct requires a lower average number of turns (12
    out of 17 evaluated LLMs). For example, the best model gpt-4-1106-preview achieves
    a $20.7$ fewer interaction turns on average. However, there is still a significant
    gap in terms of absolute CodeAct performance between open- and closed-source LLMs
    as the best open-source model achieving 13.4% while the best closed-source model
    gpt-4-1106-preview 74.4%. This is potentially due to open-source models’ weak
    task-solving capability and inability to follow complex instructions without demonstration,
    suggesting an urgent need to improve open-source LLMs for practical, real-world
    tasks under the zero-shot setting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: M³ToolEval 的定量结果。完整结果见表 [3](#S2.T3 "表 3 ‣ 2.2 CodeAct 显示出作为强大工具使用框架的潜力 ‣ 2 CodeAct
    使 LLMs 成为更好的智能体 ‣ 可执行代码操作引发更好的 LLM 智能体")，可视化结果的子集见图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 可执行代码操作引发更好的
    LLM 智能体")。CodeAct 通常具有更高的任务成功率（17 个评估的 LLM 中的 12 个），与 §[2.2](#S2.SS2 "2.2 CodeAct
    显示出作为强大工具使用框架的潜力 ‣ 2 CodeAct 使 LLMs 成为更好的智能体 ‣ 可执行代码操作引发更好的 LLM 智能体") 中的趋势类似。此外，使用
    CodeAct 需要较少的平均轮次（17 个评估的 LLM 中的 12 个）。例如，最佳模型 gpt-4-1106-preview 平均减少了 $20.7$
    次交互轮次。然而，开源和闭源 LLM 之间的 CodeAct 性能仍存在显著差距，最佳开源模型为 13.4%，而最佳闭源模型 gpt-4-1106-preview
    为 74.4%。这可能是由于开源模型在解决任务方面能力较弱，且无法在没有示例的情况下遵循复杂指令，提示需要紧急改进开源 LLM 以适应实际的零样本任务。
- en: '![Refer to caption](img/a917f811eea2b5217e08088e56270c7f.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a917f811eea2b5217e08088e56270c7f.png)'
- en: 'Figure 3: Example interaction with Python packages in multi-turn interactions
    using CodeActAgent (Mistral-7b-based). No in-context demonstrations are provided
    to the model. Some messages are omitted for space. See [https://chat.xwang.dev/r/Vqn108G](https://chat.xwang.dev/r/Vqn108G)
    for complete interaction.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用 CodeActAgent（基于 Mistral-7b）进行的与 Python 包的多轮交互示例。模型未提供上下文示例。为节省空间，一些消息被省略。完整交互请参见
    [https://chat.xwang.dev/r/Vqn108G](https://chat.xwang.dev/r/Vqn108G)。
- en: 2.4 CodeAct Benefits from Multi-turn Interactions and Existing Software Packages
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 CodeAct 从多轮交互和现有软件包中受益
- en: In Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents"), we show how an LLM agent can integrate with Python (i.e., CodeActAgent
    we trained in §[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM
    Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents"))
    and use existing software to perform complex tasks in multi-turn interactions.
    Thanks to its extensive knowledge of Python learned during pre-training, the LLM
    agent can automatically import the correct Python libraries to solve tasks without
    requiring user-provided tools or demonstrations. As illustrated in Fig. [3](#S2.F3
    "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"), CodeActAgent
    can use Pandas to download and process tabular data, use Scikit-Learn for machine
    learning train-test data split and regression model training, and use Matplotlib
    for data visualization. Furthermore, using the interactive Python interpreter
    for code execution allows automated error messages that help the LLM agent ‘self-debug’
    their actions in a multi-turn interaction and eventually complete the human user’s
    request correctly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[3](#S2.F3 "图 3 ‣ 2.3 CodeAct 通过更少的互动完成更多任务 ‣ 2 CodeAct 使 LLM 更好的代理 ‣ 可执行代码操作促进更好的
    LLM 代理")中，我们展示了一个 LLM 代理如何与 Python 集成（即我们在 §[3.2](#S3.SS2 "3.2 CodeActAgent ‣
    3 赋能开源 LLM 代理以更好地进行 CodeAct ‣ 可执行代码操作促进更好的 LLM 代理") 中训练的 CodeActAgent），并利用现有的软件在多轮互动中执行复杂任务。由于在预训练期间对
    Python 的广泛了解，LLM 代理可以自动导入正确的 Python 库来解决任务，而无需用户提供工具或演示。如图[3](#S2.F3 "图 3 ‣ 2.3
    CodeAct 通过更少的互动完成更多任务 ‣ 2 CodeAct 使 LLM 更好的代理 ‣ 可执行代码操作促进更好的 LLM 代理")所示，CodeActAgent
    可以使用 Pandas 下载和处理表格数据，使用 Scikit-Learn 进行机器学习训练-测试数据划分和回归模型训练，以及使用 Matplotlib 进行数据可视化。此外，使用交互式
    Python 解释器执行代码可以自动生成错误消息，帮助 LLM 代理在多轮互动中“自我调试”其操作，并最终正确完成用户的请求。
- en: 3 Empowering Open-source LLM Agent to be Better at CodeAct
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 赋能开源 LLM 代理以更好地进行 CodeAct
- en: 'The promising results achieved by CodeAct motivate us to build an open-source
    LLM agent that can both interact with environments through CodeAct and communicate
    with humans using language. To improve open-source LLMs’ CodeAct capability, in
    §[3.1](#S3.SS1 "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents"), we introduce CodeActInstruct, an instruction finetuning dataset
    that contains agent-environment interaction trajectories. We discuss data selection
    procedures in §[3.1](#S3.SS1 "3.1 CodeActInstruct: Agent-Environment Interactions
    ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable Code
    Actions Elicit Better LLM Agents") to promote improvement from interaction behavior.
    Additionally, we show that CodeAct can be used together with existing agent-user
    conversation data (§[4](#S3.T4 "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment
    Interactions ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable
    Code Actions Elicit Better LLM Agents")) to balance the dialog capability of the
    resulting LLM. Our model CodeActAgent, finetuned from LLaMA-2 [[47](#bib.bib47)]
    and Mistral-7B [[20](#bib.bib20)] on a mixture of CodeActInstruct and general
    conversations, improves CodeAct performances without hurting LLM’s general performance
    on a diverse suite of tasks (§[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeAct 所取得的有前景的结果激励我们构建一个可以通过 CodeAct 与环境互动并使用语言与人类沟通的开源 LLM 代理。为了提高开源 LLM
    的 CodeAct 能力，在 §[3.1](#S3.SS1 "3.1 CodeActInstruct: 代理-环境互动 ‣ 3 赋能开源 LLM 代理以更好地进行
    CodeAct ‣ 可执行代码操作促进更好的 LLM 代理") 中，我们介绍了 CodeActInstruct，这是一种包含代理-环境互动轨迹的指令微调数据集。我们在
    §[3.1](#S3.SS1 "3.1 CodeActInstruct: 代理-环境互动 ‣ 3 赋能开源 LLM 代理以更好地进行 CodeAct ‣ 可执行代码操作促进更好的
    LLM 代理") 中讨论了数据选择程序，以促进从互动行为中的改进。此外，我们展示了 CodeAct 可以与现有的代理-用户对话数据 (§[4](#S3.T4
    "表 4 ‣ 3.1 CodeActInstruct: 代理-环境互动 ‣ 3 赋能开源 LLM 代理以更好地进行 CodeAct ‣ 可执行代码操作促进更好的
    LLM 代理")) 一起使用，以平衡结果 LLM 的对话能力。我们的模型 CodeActAgent，基于 LLaMA-2 [[47](#bib.bib47)]
    和 Mistral-7B [[20](#bib.bib20)] 在 CodeActInstruct 和一般对话的混合数据上进行微调，提高了 CodeAct
    性能，而没有影响 LLM 在多样任务上的整体性能 (§[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 赋能开源 LLM 代理以更好地进行
    CodeAct ‣ 可执行代码操作促进更好的 LLM 代理"))。'
- en: '3.1 CodeActInstruct: Agent-Environment Interactions'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 CodeActInstruct：代理-环境交互
- en: 'We consider four main use cases in agent-environment interaction and repurpose
    five existing datasets across different domains to generate trajectories:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代理-环境交互中考虑了四个主要使用案例，并重新利用了五个不同领域的现有数据集来生成轨迹：
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Information Seeking: We use a training subset of HotpotQA [[59](#bib.bib59)]
    to generate information-seeking trajectories, where LLMs use the wikipedia_search
    API (provided as a Python function) to search for information to answer questions.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息获取：我们使用 HotpotQA [[59](#bib.bib59)] 的训练子集生成信息获取轨迹，其中 LLM 使用 wikipedia_search
    API（作为 Python 函数提供）搜索信息以回答问题。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Software Package (Tool) Usage: We use the training set of code generation problems
    in APPS [[16](#bib.bib16)] and math problems in MATH [[18](#bib.bib18)]. The code
    generation tasks already involve importing packages and/or creating new tools
    by defining a new Python function. For MATH, we provide an in-context demonstration
    of importing Python packages (e.g., sympy for symbolic math) for problem-solving.'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件包（工具）使用：我们使用 APPS [[16](#bib.bib16)] 中的代码生成问题训练集和 MATH [[18](#bib.bib18)]
    中的数学问题。代码生成任务已经涉及导入包和/或通过定义新的 Python 函数来创建新工具。对于 MATH，我们提供了导入 Python 包（例如用于符号数学的
    sympy）的上下文演示以解决问题。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'External Memory: We repurpose the training subset of WikiTableQuestion [[35](#bib.bib35)]
    and tweak it into two variants of tabular reasoning tasks that require accessing
    external memory: (1) SQL-based, requiring the LLM to interact with an SQL database
    through sqlite3 package to answer the question via SQL execution; (2) Pandas-based,
    requiring the model to interact with pandas tables to perform data operations
    (e.g., select, filter). Examples of instructions can be found in §[G.3.1](#A7.SS3.SSS1
    "G.3.1 Tabular Reasoning (WikiTableQuestion) ‣ G.3 Example of Trajectory Generation
    Prompt ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions Elicit
    Better LLM Agents").'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部记忆：我们重新利用了 WikiTableQuestion [[35](#bib.bib35)] 的训练子集，并将其调整为需要访问外部记忆的两种表格推理任务变体：（1）基于
    SQL 的，需要 LLM 通过 sqlite3 包与 SQL 数据库交互，通过 SQL 执行回答问题；（2）基于 Pandas 的，需要模型与 pandas
    表交互以执行数据操作（例如，选择、过滤）。有关指令的示例可以在 §[G.3.1](#A7.SS3.SSS1 "G.3.1 Tabular Reasoning
    (WikiTableQuestion) ‣ G.3 Example of Trajectory Generation Prompt ‣ Appendix G
    CodeAct Interaction Data ‣ Executable Code Actions Elicit Better LLM Agents")
    中找到。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robot Planning: We use ALFWorld [[43](#bib.bib43)], a text-only embodied environment
    simulator, to generate trajectories that use robot-control APIs (repurposed as
    Python function) to complete household tasks. Following MINT [[53](#bib.bib53)],
    we provide an in-context demonstration to encourage the use of for-loop and if-statement
    code blocks to automate repetitive operations (e.g., searching for items by visiting
    different locations).'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器人规划：我们使用 ALFWorld [[43](#bib.bib43)]，一个仅文本的具身环境模拟器，生成使用机器人控制 API（重新作为 Python
    函数）来完成家庭任务的轨迹。参考 MINT [[53](#bib.bib53)]，我们提供了一个上下文演示，以鼓励使用 for 循环和 if 语句代码块来自动化重复操作（例如，通过访问不同地点来搜索物品）。
- en: Data Down-sampling. We down-sample each dataset by keeping only the most challenging
    instances, aiming to make trajectory generation more efficient and cost-effective.
    Furthermore, it also helps remove simple instances that existing LLMs can already
    solve. The statistics of the filtered dataset can be found in Tab. [A.9](#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents"). Please refer to §[G.1](#A7.SS1 "G.1 Dataset
    Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions Elicit
    Better LLM Agents") for details about the down-sample process.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据下采样。我们通过仅保留最具挑战性的实例来对每个数据集进行下采样，旨在提高轨迹生成的效率和成本效益。此外，这也有助于去除现有LLM已经能够解决的简单实例。过滤后的数据集统计信息可以在表格
    [A.9](#A7.T9 "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction
    Data ‣ Executable Code Actions Elicit Better LLM Agents") 中找到。有关下采样过程的详细信息，请参见
    §[G.1](#A7.SS1 "G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣
    Executable Code Actions Elicit Better LLM Agents")。
- en: Repurpose Data for Multi-turn Interaction. Some datasets (APPS, MATH, WikiTableQuestions)
    are initially single-turn problems that expect one solution per instruction, whereas,
    in a realistic agent use case, we often require multi-turn interaction to complete
    each task (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions
    Elicit Better LLM Agents") top). Following MINT [[53](#bib.bib53)], we repurpose
    single-turn problems into multi-turn ones by allowing LLM to interact with the
    environment for multiple turns before it decides to submit one solution for evaluation.
    Specifically for code generation problems, we provide an in-context example to
    guide LLMs to test their solution on provided test cases before they submit the
    solution. Metrics from the original data will evaluate the submitted solution
    to determine its correctness. We include prompt examples in §[G.3](#A7.SS3 "G.3
    Example of Trajectory Generation Prompt ‣ Appendix G CodeAct Interaction Data
    ‣ Executable Code Actions Elicit Better LLM Agents").
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为多轮交互重新利用数据。某些数据集（APPS、MATH、WikiTableQuestions）最初是单轮问题，每个指令期望一个解决方案，而在实际的代理使用案例中，我们通常需要多轮交互来完成每个任务（见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better
    LLM Agents")）。遵循 MINT [[53](#bib.bib53)]，我们通过允许 LLM 在决定提交一个解决方案进行评估之前与环境进行多轮交互，将单轮问题转化为多轮问题。特别是对于代码生成问题，我们提供一个上下文示例以指导
    LLM 在提交解决方案之前在提供的测试用例上测试其解决方案。来自原始数据的指标将评估提交的解决方案以确定其正确性。我们在 §[G.3](#A7.SS3 "G.3
    Example of Trajectory Generation Prompt ‣ Appendix G CodeAct Interaction Data
    ‣ Executable Code Actions Elicit Better LLM Agents") 包含了提示示例。
- en: Trajectory Generation. We use MINT’s evaluation framework [[53](#bib.bib53)]
    to generate interaction trajectories for the aforementioned datasets and determine
    the correctness of each trajectory. We run gpt-3.5-turbo-0613 from OpenAI, claude-1-instant
    and claude-2 from Anthropic on down-sampled data, except code generation, which
    we use a longer-context version of GPT-3.5 (gpt-3.5-turbo-0613-16k) due to the
    long-context requirement of the self-debugging process. On a subset of problems
    that none of these models can solve, we use gpt-4-0613 to generate trajectories.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹生成。我们使用 MINT 的评估框架 [[53](#bib.bib53)] 来生成上述数据集的交互轨迹，并确定每个轨迹的正确性。我们在下采样数据上运行
    OpenAI 的 gpt-3.5-turbo-0613、Anthropic 的 claude-1-instant 和 claude-2，代码生成除外。由于自我调试过程的长上下文要求，我们使用
    GPT-3.5 的长上下文版本（gpt-3.5-turbo-0613-16k）。在这些模型无法解决的部分问题上，我们使用 gpt-4-0613 生成轨迹。
- en: 'Enhancing Agent’s Capabilities of Improving from Interaction. We select a high-quality
    subset of all the generated trajectories from CodeActInstruct to promote the agent’s
    ability to improve the next action based on prior observations (e.g., self-debugging
    from code execution error message, a planning capability in Fig. [2](#S2.F2 "Figure
    2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better
    LLM Agents")). To achieve this, we selectively preserve those trajectories wherein
    the model initially encounters errors but rectifies these inaccuracies in later
    interactions. For these instances, the LLM typically engages in self-reflection
    following the initial error, thereby proactively enhancing its future actions.
    Other filtering details are discussed in §[G.2](#A7.SS2 "G.2 Data Selection Heuristic
    ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions Elicit Better
    LLM Agents"). On all trajectories generated, we keep 411 trajectories from gpt-4-0613
    and 6728 trajectories from gpt-3.5 and claude. The statistics of the resulting
    dataset CodeActInstruct are shown in Tab. [4](#S3.T4 "Table 4 ‣ 3.1 CodeActInstruct:
    Agent-Environment Interactions ‣ 3 Empowering Open-source LLM Agent to be Better
    at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '提升代理从交互中改进的能力。我们从 CodeActInstruct 生成的所有轨迹中选择了一个高质量子集，以促进代理基于先前观察改进下一步行动的能力（例如，来自代码执行错误信息的自我调试能力，见图
    [2](#S2.F2 "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents")）。为此，我们有选择地保留那些模型最初遇到错误但在后续交互中纠正了这些不准确性的轨迹。在这些情况下，LLM
    通常在初始错误之后进行自我反思，从而积极提升其未来的行动。其他筛选细节在 §[G.2](#A7.SS2 "G.2 Data Selection Heuristic
    ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions Elicit Better
    LLM Agents") 中讨论。对于所有生成的轨迹，我们保留了来自 gpt-4-0613 的 411 条轨迹和来自 gpt-3.5 和 claude 的
    6728 条轨迹。生成数据集 CodeActInstruct 的统计数据如表 [4](#S3.T4 "Table 4 ‣ 3.1 CodeActInstruct:
    Agent-Environment Interactions ‣ 3 Empowering Open-source LLM Agent to be Better
    at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents") 所示。'
- en: 'Table 4: Statistics of our training mixture and comparison with prior work.
    Please refer to §[3.1](#S3.SS1 "3.1 CodeActInstruct: Agent-Environment Interactions
    ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable Code
    Actions Elicit Better LLM Agents") and §[4](#S3.T4 "Table 4 ‣ 3.1 CodeActInstruct:
    Agent-Environment Interactions ‣ 3 Empowering Open-source LLM Agent to be Better
    at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents") for details about
    CodeActInstruct and general conversation data. Token statistics are computed using
    Llama-2 tokenizer.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：我们训练混合的统计数据及与先前工作的比较。有关CodeActInstruct和一般对话数据的详细信息，请参见§[3.1](#S3.SS1 "3.1
    CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source LLM
    Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents")和§[4](#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")。令牌统计使用Llama-2分词器计算。'
- en: '| Data Mixture | Data Type | Data Name | # of Data Instances | # of Total Tokens
    | Avg. Tokens Per Instance |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 数据混合 | 数据类型 | 数据名称 | 数据实例数量 | 总令牌数 | 平均每实例令牌数 |'
- en: '| Prior Work | - | FireAct [[6](#bib.bib6)] | $2,063$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 先前工作 | - | FireAct [[6](#bib.bib6)] | $2,063$ |'
- en: '| - | AgentInstruct [[65](#bib.bib65)] | $1,866$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| - | AgentInstruct [[65](#bib.bib65)] | $1,866$ |'
- en: '| CodeActInstruct (Ours) | Information Seeking | HotpotQA [[59](#bib.bib59)]
    | $1,664$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| CodeActInstruct（我们的） | 信息检索 | HotpotQA [[59](#bib.bib59)] | $1,664$ |'
- en: '| Software Packages (Tool) | MATH (Math, [[18](#bib.bib18)]) | $1,732$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 软件包（工具） | MATH（数学，[[18](#bib.bib18)]) | $1,732$ |'
- en: '| Software Packages (Tool) | APPS (Code, [[16](#bib.bib16)]) | $647$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 软件包（工具） | APPS（代码，[[16](#bib.bib16)]) | $647$ |'
- en: '| External Memory | WikiTableQuestion [[35](#bib.bib35)] | $1,065$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 外部记忆 | WikiTableQuestion [[35](#bib.bib35)] | $1,065$ |'
- en: '| Robot Planning | ALFWorld [[43](#bib.bib43)] | $2,031$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 机器人规划 | ALFWorld [[43](#bib.bib43)] | $2,031$ |'
- en: '| Total | $\mathbf{7,139}$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | $\mathbf{7,139}$ |'
- en: '| General Conversation | Single-Turn Reasoning | OpenOrca (Sub-sampled, [[25](#bib.bib25)])
    | $50,000$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 一般对话 | 单轮推理 | OpenOrca（子采样，[[25](#bib.bib25)]) | $50,000$ |'
- en: '| Multi-Turn Conversations | ShareGPT (Sub-sampled, [[2](#bib.bib2)]) | $10,000$
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 多轮对话 | ShareGPT（子采样，[[2](#bib.bib2)]) | $10,000$ |'
- en: '| Multi-Turn Conversations | ShareGPT (GPT-4, [[32](#bib.bib32)]) | $4,583$
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 多轮对话 | ShareGPT（GPT-4，[[32](#bib.bib32)]) | $4,583$ |'
- en: '| Multi-turn Reasoning | CapyBara [[22](#bib.bib22)] | $4,647$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 多轮推理 | CapyBara [[22](#bib.bib22)] | $4,647$ |'
- en: '| Total | $\mathbf{69,230}$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | $\mathbf{69,230}$ |'
- en: 'Comparing CodeActInstruct with Prior Work. Compared with prior work AgentInstruct
    [[65](#bib.bib65)] and FireAct [[6](#bib.bib6)] that mainly focus using text as
    action, CodeActInstruct results in models that are more practical in real-world
    implementation, as such models using CodeAct can directly interact with Python
    interpreters and open-source toolkits (Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 CodeAct
    Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣
    Executable Code Actions Elicit Better LLM Agents")), reducing the development
    effort for action parsing and tool creations. CodeActInstruct is systematically
    constructed following the general agent framework (Fig. [2](#S2.F2 "Figure 2 ‣
    2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents")). It covers diverse domains (e.g., compared to FireAct that only considers
    QA-task and search API), contains quality data (e.g., promotes agent’s capability
    of self-debug) and of larger size (3.8x / 3.5x more data trajectories and 5x /
    19x more tokens compared to AgentInstruct / FireAct respectively in Tab. [4](#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")). As we empirically show in Tab. [5](#S3.T5 "Table 5 ‣ 3.1
    CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source LLM
    Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents"),
    the resulting model (same backbone) of CodeActInstruct achieves 24% and 119% relative
    improvement compared to AgentInstruct and FireAct.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将 CodeActInstruct 与以往工作进行比较。与主要关注使用文本作为操作的先前工作 AgentInstruct [[65](#bib.bib65)]
    和 FireAct [[6](#bib.bib6)] 相比，CodeActInstruct 产生的模型在实际应用中更为实用，因为这些使用 CodeAct 的模型可以直接与
    Python 解释器和开源工具包交互（图 [3](#S2.F3 "图 3 ‣ 2.3 CodeAct 在更少的交互中完成更多工作 ‣ 2 CodeAct 使
    LLM 成为更好的代理 ‣ 可执行代码操作引出更好的 LLM 代理")），从而减少了操作解析和工具创建的开发工作量。CodeActInstruct 系统地按照通用代理框架构建（图
    [2](#S2.F2 "图 2 ‣ 2 CodeAct 使 LLM 成为更好的代理 ‣ 可执行代码操作引出更好的 LLM 代理")）。它涵盖了不同的领域（例如，相较于仅考虑
    QA 任务和搜索 API 的 FireAct），包含了高质量的数据（例如，提升代理的自我调试能力），并且规模更大（与 AgentInstruct / FireAct
    相比，在表 [4](#S3.T4 "表 4 ‣ 3.1 CodeActInstruct：代理-环境交互 ‣ 3 使开源 LLM 代理在 CodeAct 中表现更好
    ‣ 可执行代码操作引出更好的 LLM 代理") 中分别多出 3.8 倍 / 3.5 倍的数据轨迹和 5 倍 / 19 倍的标记）。正如我们在表 [5](#S3.T5
    "表 5 ‣ 3.1 CodeActInstruct：代理-环境交互 ‣ 3 使开源 LLM 代理在 CodeAct 中表现更好 ‣ 可执行代码操作引出更好的
    LLM 代理") 中实证展示的那样，CodeActInstruct 生成的模型（相同的骨干）与 AgentInstruct 和 FireAct 相比，分别实现了
    24% 和 119% 的相对改进。
- en: 'CodeActInstruct Can Be Used With Existing Agent-User Conversation Data. We
    use a sub-sampled set of OpenOrca [[25](#bib.bib25)] that focuses on single-turn
    chain-of-thought (CoT) reasoning, ShareGPT [[2](#bib.bib2), [32](#bib.bib32)]
    from two sources that contain multi-turn conversations between human and LLM,
    and CapyBara [[22](#bib.bib22)] that focuses on reasoning in multi-turn conversations.
    Details of down-sampling can be found in §[C](#A3 "Appendix C General Data Down-sample
    ‣ Executable Code Actions Elicit Better LLM Agents"). Please refer to Tab. [4](#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents") for statistics of general conversations.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: CodeActInstruct 可以与现有的代理-用户对话数据一起使用。我们使用了一组子抽样的 OpenOrca [[25](#bib.bib25)]
    数据，该数据专注于单回合的链式思维（CoT）推理，ShareGPT [[2](#bib.bib2), [32](#bib.bib32)] 来自两个来源，包含人与
    LLM 之间的多回合对话，以及 CapyBara [[22](#bib.bib22)]，专注于多回合对话中的推理。有关下抽样的详细信息，请参见 §[C](#A3
    "附录 C 一般数据下抽样 ‣ 可执行代码操作引出更好的 LLM 代理")。有关一般对话的统计数据，请参考表 [4](#S3.T4 "表 4 ‣ 3.1 CodeActInstruct：代理-环境交互
    ‣ 3 使开源 LLM 代理在 CodeAct 中表现更好 ‣ 可执行代码操作引出更好的 LLM 代理")。
- en: 'Table 5: Evaluation results for CodeActAgent. The best results among all open-source
    LLMs are bolded, and the second-best results are underlined. ID and OD stand for
    in-domain and out-of-domain evaluation correspondingly. Overall averaged performance
    normalizes the MT-Bench score to be consistent with other tasks and excludes in-domain
    tasks for fair comparison.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：CodeActAgent 的评估结果。所有开源 LLM 中的最佳结果已加粗显示，第二好的结果已下划线标记。ID 和 OD 分别代表领域内和领域外的评估。总体平均性能将
    MT-Bench 分数标准化，以与其他任务一致，并排除领域内任务以便于公平比较。
- en: Agent Tasks Generic Tasks Overall Code as Action Text as Action (OD) (OD) Average
    Model Size MINT (ID) MINT (OD) M³ToolEval (OD) Miniwob++ SciWorld MMLU HumanEval
    GSM8K MTBench Open-source LLMs (LLaMA-2-based) Llama2 Base 7B -^∗ -^∗ -^∗ -^∗
    -^∗ $45.3$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 代理任务 通用任务 总体代码作为操作 文本作为操作（OD） （OD） 平均模型大小 MINT（ID） MINT（OD） M³ToolEval（OD） Miniwob++
    SciWorld MMLU HumanEval GSM8K MTBench 开源 LLM（基于 LLaMA-2） Llama2 Base 7B -^∗ -^∗
    -^∗ -^∗ -^∗ $45.3$
- en: '*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: Some results are only available with instruction-tuned models.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些结果仅在经过指令微调的模型中可用。
- en: 3.2 CodeActAgent
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 CodeActAgent
- en: 'We fine-tune Llama-2 7B [[47](#bib.bib47)] and Mistral 7B [[20](#bib.bib20)]
    on a mixture of CodeActInstruct and general conversations (Tab. [4](#S3.T4 "Table
    4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")) to obtain CodeActAgent.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 CodeActInstruct 和一般对话的混合数据集上对 Llama-2 7B [[47](#bib.bib47)] 和 Mistral 7B
    [[20](#bib.bib20)] 进行了微调（见表[4](#S3.T4 "表 4 ‣ 3.1 CodeActInstruct: 代理-环境交互 ‣ 3
    赋能开源 LLM 代理以更好地执行 CodeAct ‣ 可执行代码操作引发更好的 LLM 代理")），从而得到 CodeActAgent。'
- en: Training Setup. We perform full-parameter supervised fine-tuning with a sequence
    length of 4,096 tokens for Llama-2 and 16,384 for Mistral. Please refer to §[D](#A4
    "Appendix D CodeActAgent Training Details ‣ Executable Code Actions Elicit Better
    LLM Agents") for more details.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 训练设置。我们对 Llama-2 进行全参数监督微调，序列长度为 4,096 个标记，对 Mistral 进行的序列长度为 16,384 个标记。有关更多细节，请参见
    §[D](#A4 "附录 D CodeActAgent 训练细节 ‣ 可执行代码操作引发更好的 LLM 代理")。
- en: 'Evaluation Setup. We use MINT [[53](#bib.bib53)] to evaluate LLMs with CodeAct
    on a diverse range of agent tasks. CodeActAgent has some training domains overlapping
    with MINT’s evaluation (i.e., MINT includes ALFWorld and MATH), hence we report
    separate numbers for MINT’s in- and out-of-domain performance. Unless otherwise
    specified, we measure MINT tasks’ success rates with interaction turn $k=5$. We
    also evaluate out-of-domain agent tasks using text actions from MiniWob++ (computer
    tasks, [[21](#bib.bib21)]) and ScienceWorld (text-based simulator for elementary
    science curriculum, [[50](#bib.bib50)]) to test whether CodeActAgent can generalize
    to different action formats. Finally, we include a suite of general LLM evaluation
    tasks to assess general capability: MMLU [[17](#bib.bib17)] for knowledge-based
    QA, HumanEval [[7](#bib.bib7)] for single-turn code-generation, GSM8K [[12](#bib.bib12)]
    for single-turn tool-free math reasoning, and MTBench [[67](#bib.bib67)] for instruction-following.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 评估设置。我们使用 MINT [[53](#bib.bib53)] 来评估具有 CodeAct 的 LLM 在各种代理任务中的表现。CodeActAgent
    的一些训练领域与 MINT 的评估（即 MINT 包括 ALFWorld 和 MATH）重叠，因此我们报告 MINT 领域内和领域外表现的单独数据。除非另有说明，我们以交互回合
    $k=5$ 测量 MINT 任务的成功率。我们还使用来自 MiniWob++（计算任务，[[21](#bib.bib21)]）和 ScienceWorld（针对小学科学课程的基于文本的模拟器，[[50](#bib.bib50)]）的文本操作评估领域外代理任务，以测试
    CodeActAgent 是否能推广到不同的操作格式。最后，我们包括一套通用 LLM 评估任务，以评估其整体能力：MMLU [[17](#bib.bib17)]
    用于基于知识的 QA，HumanEval [[7](#bib.bib7)] 用于单轮代码生成，GSM8K [[12](#bib.bib12)] 用于单轮无需工具的数学推理，以及
    MTBench [[67](#bib.bib67)] 用于指令遵循。
- en: 'CodeActAgent Excels in CodeAct Task. As shown in Tab. [5](#S3.T5 "Table 5 ‣
    3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents"), CodeActAgent (both variants) perform better than all evaluated open-source
    LLMs on both the in-domain and out-of-domain subsets of MINT. On M³ToolEval, we
    find CodeActAgent (Mistral) outperforms open-source LLMs of similar size (7B and
    13B) and even reaches similar performance to those 70B models (Tab. [3](#S2.T3
    "Table 3 ‣ 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")).
    Surprisingly, no improvement is observed for the Llama-2 variant. We discuss the
    potential reasons in §[H](#A8 "Appendix H CodeActAgent Anomaly on M3ToolEval ‣
    Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeActAgent 在 CodeAct 任务中表现出色。如表[5](#S3.T5 "表 5 ‣ 3.1 CodeActInstruct: 代理-环境交互
    ‣ 3 赋能开源 LLM 代理以更好地执行 CodeAct ‣ 可执行代码操作引发更好的 LLM 代理") 所示，CodeActAgent（两种变体）在 MINT
    的领域内和领域外子集上均优于所有评估的开源 LLM。在 M³ToolEval 上，我们发现 CodeActAgent（Mistral）超越了同规模（7B 和
    13B）的开源 LLM，甚至达到了与 70B 模型类似的性能（见表[3](#S2.T3 "表 3 ‣ 2.2 CodeAct 显示了作为强大工具使用框架的潜力
    ‣ 2 CodeAct 使 LLM 成为更好的代理 ‣ 可执行代码操作引发更好的 LLM 代理")）。令人惊讶的是，Llama-2 变体没有观察到改进。我们在
    §[H](#A8 "附录 H CodeActAgent 在 M3ToolEval 上的异常 ‣ 可执行代码操作引发更好的 LLM 代理") 中讨论了可能的原因。'
- en: CodeActAgent Generalizes to Text Action. When evaluated on out-of-domain text
    actions, CodeActAgent (LLaMA2, 7B), which has never been optimized for text action,
    achieves comparable performance to AgentLM-7B [[65](#bib.bib65)] which has explicit
    tuning for text actions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: CodeActAgent 泛化到文本动作。当在领域外文本动作上评估时，未经过文本动作优化的 CodeActAgent (LLaMA2, 7B) 达到了与专门调优文本动作的
    AgentLM-7B [[65](#bib.bib65)] 相当的性能。
- en: 'CodeActAgent Maintains or Improves the Performance on General LLM Tasks. In
    Tab. [5](#S3.T5 "Table 5 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions
    ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable Code
    Actions Elicit Better LLM Agents"), we find that CodeActAgent (both variants)
    performs better on generic LLM tasks we tested, except for a slight degradation
    on MMLU for CodeActAgent (Mistral, 7B).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeActAgent 在通用 LLM 任务上维持或提高了性能。在表格 [5](#S3.T5 "表 5 ‣ 3.1 CodeActInstruct:
    代理-环境交互 ‣ 3 使开源 LLM 代理在 CodeAct 中表现更好 ‣ 可执行代码动作引发更好的 LLM 代理")中，我们发现 CodeActAgent（两个变体）在我们测试的通用
    LLM 任务中表现更好，除了 CodeActAgent (Mistral, 7B) 在 MMLU 上有轻微退化。'
- en: Ablation Study. Tab. [A.8](#A0.T8 "Table A.8 ‣ Executable Code Actions Elicit
    Better LLM Agents") presents ablation experiments to determine the importance
    of CodeActInstruct and general conversations. Both CodeActInstruct and general
    conversations contribute to agent tasks, while general conversations are essential
    to maintain performance on general tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究。表格 [A.8](#A0.T8 "表 A.8 ‣ 可执行代码动作引发更好的 LLM 代理") 展示了消融实验，以确定 CodeActInstruct
    和一般对话的重要性。CodeActInstruct 和一般对话都对代理任务有贡献，而一般对话对维持通用任务的性能至关重要。
- en: 4 Related Work
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: 4.1 Action Module in LLM Agents
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LLM 代理中的动作模块
- en: 'As detailed in [[49](#bib.bib49)], LLM-based autonomous agents are typically
    structured around four components: customized profiles [[34](#bib.bib34), [37](#bib.bib37)],
    long-term memory capabilities [[68](#bib.bib68), [14](#bib.bib14)], reasoning
    and planning algorithms [[56](#bib.bib56), [10](#bib.bib10)], and, most crucially,
    action modules. The action modules are key to facilitating LLM agents to effectively
    interact with external entities, including humans [[23](#bib.bib23)] and tools [[39](#bib.bib39)]
    in the environment [[53](#bib.bib53)]. In this study, we address the critical
    problem of standardizing the action space for LLM agents. We further discuss the
    difference between CodeAct and the line of work that uses code generation for
    problem-solving in §[A](#A1 "Appendix A Comparison with Prior Work that Uses Code
    Generation for Problem-solving ‣ Executable Code Actions Elicit Better LLM Agents").
    We notice a concurrent study TaskWeaver [[38](#bib.bib38)] similarly endorses
    the use of code. We discuss the principal distinctions in §[B](#A2 "Appendix B
    Comparison with TaskWeaver ‣ Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如[[49](#bib.bib49)]中详细说明的，基于 LLM 的自主代理通常由四个组件组成：定制的个人资料 [[34](#bib.bib34), [37](#bib.bib37)]、长期记忆能力
    [[68](#bib.bib68), [14](#bib.bib14)]、推理和规划算法 [[56](#bib.bib56), [10](#bib.bib10)]，以及最重要的，动作模块。动作模块是使
    LLM 代理有效地与外部实体（包括人类 [[23](#bib.bib23)] 和环境中的工具 [[39](#bib.bib39)]）互动的关键。在这项研究中，我们解决了标准化
    LLM 代理动作空间的关键问题。我们进一步讨论了 CodeAct 与使用代码生成解决问题的工作线之间的区别，见 §[A](#A1 "附录 A 与使用代码生成解决问题的工作比较
    ‣ 可执行代码动作引发更好的 LLM 代理")。我们注意到，另一项同步研究 TaskWeaver [[38](#bib.bib38)] 同样支持使用代码。我们在
    §[B](#A2 "附录 B 与 TaskWeaver 比较 ‣ 可执行代码动作引发更好的 LLM 代理") 中讨论了主要区别。
- en: 4.2 Improving LLM Agents
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 改进 LLM 代理
- en: Two primary methods for enhancing LLM agents are prompt engineering and instruction
    tuning, as surveyed by [[49](#bib.bib49)]. For prompt engineering [[27](#bib.bib27)],
    numerous strategies have been introduced to improve the chain-of-thought reasoning [[56](#bib.bib56)],
    including self-consistency-based reasoning [[54](#bib.bib54), [10](#bib.bib10)]
    and tree-based approaches [[61](#bib.bib61)]. Moreover, LLMs can be strategically
    prompted to reflect on previous plans [[63](#bib.bib63), [55](#bib.bib55), [66](#bib.bib66)],
    enabling them to refine initial actions through trial and error. Contrast to prompt
    engineering, instruction tuning intrinsically enhances LLMs [[11](#bib.bib11)],
    particularly in their agent capabilities [[65](#bib.bib65), [6](#bib.bib6)]. For
    effective training, human annotators can curate expert demonstrations for specific
    agent tasks, such as web browsing [[60](#bib.bib60), [31](#bib.bib31)]. To minimize
    human annotation efforts, prior work creates synthetic datasets using stronger
    LLMs to distill agent capabilities into local models, focusing on tool usage [[40](#bib.bib40)],
    interaction [[9](#bib.bib9)], and social skills [[28](#bib.bib28)]. CodeActInstruct
    aligns with the latter approach and creates datasets using stronger LLMs due to
    limited resources for annotation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 增强LLM代理的两种主要方法是提示工程和指令调优，如[[49](#bib.bib49)]所述。对于提示工程[[27](#bib.bib27)]，已经提出了许多策略来改进链式推理[[56](#bib.bib56)]，包括基于自我一致性的推理[[54](#bib.bib54),
    [10](#bib.bib10)]和基于树的方法[[61](#bib.bib61)]。此外，LLM可以通过战略性提示来反思先前的计划[[63](#bib.bib63),
    [55](#bib.bib55), [66](#bib.bib66)]，使其能够通过反复试验来改进初始行动。与提示工程相比，指令调优从本质上增强了LLM[[11](#bib.bib11)]，特别是在其代理能力方面[[65](#bib.bib65),
    [6](#bib.bib6)]。为了有效训练，人类标注者可以为特定的代理任务（例如网页浏览[[60](#bib.bib60), [31](#bib.bib31)]）策划专家演示。为了减少人类标注工作的努力，先前的工作使用更强大的LLM创建合成数据集，将代理能力提炼到本地模型中，重点关注工具使用[[40](#bib.bib40)]、互动[[9](#bib.bib9)]和社交技能[[28](#bib.bib28)]。CodeActInstruct与后一种方法一致，并使用更强大的LLM创建数据集，因为标注资源有限。
- en: 5 Conclusions
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This work introduces CodeAct that employs executable Python code for the LLM
    agent’s action, which is advantageous over using text or JSON action, especially
    in complex scenarios. We collect CodeAct-focused multi-turn interaction trajectories
    CodeActInstruct for instruction tuning, and train CodeActAgent that is specially
    designed for seamless integration with Python and can execute sophisticated tasks
    (e.g., model training) leveraging existing Python packages and autonomously rectifying
    errors through self-debugging.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作介绍了CodeAct，它采用可执行的Python代码作为LLM代理的动作，这在复杂场景中相较于使用文本或JSON动作具有优势。我们收集了以CodeAct为重点的多轮互动轨迹CodeActInstruct用于指令调优，并训练了特别设计的CodeActAgent，使其能够与Python无缝集成，并利用现有的Python包执行复杂任务（例如，模型训练）并通过自我调试自动纠正错误。
- en: Broader Impacts, Limitations, and Future Work
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响、局限性和未来工作
- en: This paper presents work whose goal is to advance LLM-based autonomous agents
    that can communicate with humans through natural language and assist human users
    by performing tasks in environments on behalf of humans. In this section, we discuss
    potential societal consequences, limitations, and future work related to our work
    and its goal.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的工作旨在推进基于LLM的自主代理，能够通过自然语言与人类沟通，并通过在环境中代表人类执行任务来协助用户。在本节中，我们讨论了与我们的工作及其目标相关的潜在社会影响、局限性和未来工作。
- en: CodeActAgent is an initial prototype of an autonomous agent and still has several
    practical limitations. For example, it may suffer from hallucination commonly
    seen in LLMs (e.g., imagine the content of a variable without actually printing
    it out), suggesting the need for subsequent alignment [[33](#bib.bib33)] for further
    improvements.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: CodeActAgent是一个初步的自主代理原型，仍然存在若干实际局限性。例如，它可能会遭遇LLM中常见的幻觉（例如，想象一个变量的内容而不实际打印出来），这表明需要随后的对齐[[33](#bib.bib33)]以进行进一步改进。
- en: Despite being a prototype, CodeActAgent has already demonstrated limited self-improving
    capability (e.g., self-debug error messages to improve its action) and the ability
    to interact with environments. Future work may build upon CodeActAgent to develop
    better agents by having them perform extensive interactions within a given environment
    and iteratively bootstrap their self-improving capability to learn to improve
    from past mistakes. More powerful agents, as results of such algorithms, are potentially
    beneficial for solving a wide range of real-world problems (e.g., theorem proving,
    drug discovery). As extensively discussed in [[13](#bib.bib13)], a fully autonomous
    agent may transform the current landscape of the labor market and impact the jobs
    of existing workers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CodeActAgent 只是一个原型，但它已经展示了有限的自我改进能力（例如，自我调试错误信息以改进其行动）和与环境互动的能力。未来的工作可能基于
    CodeActAgent 开发更好的代理，通过让它们在给定环境中进行广泛互动，并迭代性地提升其自我改进能力，从过去的错误中学习以进行改进。作为这些算法结果的更强大的代理，可能对解决广泛的现实世界问题（例如定理证明、药物发现）具有潜在的益处。如[[13](#bib.bib13)]中详细讨论，完全自主的代理可能会改变当前的劳动市场格局，并影响现有工人的工作。
- en: Furthermore, since CodeAct directly grants access for the agent to freely execute
    code in a sandbox environment, in the worst scenario (e.g., in Sci-Fi movies),
    such an agent may potentially break free of the sandbox restriction and cause
    harm to the world through cyber-attack, highlighting the need for future work
    to design better safety mechanism to safeguard autonomous agents.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于 CodeAct 直接授予代理在沙箱环境中自由执行代码的权限，在最坏的情况下（例如科幻电影中），这样的代理可能会突破沙箱限制，通过网络攻击对世界造成危害，这突显了未来需要设计更好的安全机制来保护自主代理的必要性。
- en: References
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
    Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
    Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric
    Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan
    Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,
    Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
    Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander
    Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan,
    and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances.
    In arXiv preprint arXiv:2204.01691, 2022.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
    Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
    Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric
    Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan
    Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,
    Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
    Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander
    Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan
    和 Andy Zeng。言行一致：将语言与机器人能力相结合。在 arXiv 预印本 arXiv:2204.01691，2022年。'
- en: '[2] Anonymous. Sharegpt dataset. [https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json](https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json),
    2023. A dataset containing multi-turn conversations between human and LLM assistant.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 匿名。Sharegpt 数据集。 [https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json](https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json)，2023年。一个包含人类与
    LLM 助手之间多轮对话的数据集。'
- en: '[3] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow:
    Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376,
    2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Andres M Bran, Sam Cox, Andrew D White 和 Philippe Schwaller。Chemcrow：用化学工具增强大型语言模型。arXiv
    预印本 arXiv:2304.05376，2023年。'
- en: '[4] Alejandro Hernández Cano, Matteo Pagliardini, Andreas Köpf, Kyle Matoba,
    Amirkeivan Mohtashami, Xingyao Wang, Olivia Simin Fan, Axel Marmet, Deniz Bayazit,
    Igor Krawczuk, Zeming Chen, Francesco Salvi, Antoine Bosselut, and Martin Jaggi.
    epfllm megatron-llm, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Alejandro Hernández Cano, Matteo Pagliardini, Andreas Köpf, Kyle Matoba,
    Amirkeivan Mohtashami, Xingyao Wang, Olivia Simin Fan, Axel Marmet, Deniz Bayazit,
    Igor Krawczuk, Zeming Chen, Francesco Salvi, Antoine Bosselut 和 Martin Jaggi。epfllm
    megatron-llm，2023年。'
- en: '[5] Harrison Chase. LangChain, October 2022.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Harrison Chase。LangChain，2022年10月。'
- en: '[6] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan,
    and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915,
    2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan
    和 Shunyu Yao。Fireact：迈向语言代理微调。arXiv 预印本 arXiv:2310.05915，2023年。'
- en: '[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
    Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,
    et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,
    2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde de Oliveira
    Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman 等。评估训练有素的大型语言模型。arXiv
    预印本 arXiv:2107.03374，2021年。'
- en: '[8] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large
    language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Xinyun Chen、Maxwell Lin、Nathanael Schärli 和 Denny Zhou。教大型语言模型自我调试。arXiv
    预印本 arXiv:2304.05128，2023年。'
- en: '[9] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran.
    Dress: Instructing large vision-language models to align and interact with humans
    via natural language feedback. arXiv preprint arXiv:2311.10081, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Yangyi Chen、Karan Sikka、Michael Cogswell、Heng Ji 和 Ajay Divakaran。Dress：指导大型视觉语言模型通过自然语言反馈与人类对齐和互动。arXiv
    预印本 arXiv:2311.10081，2023年。'
- en: '[10] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran.
    Measuring and improving chain-of-thought reasoning in vision-language models.
    arXiv preprint arXiv:2309.04461, 2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Yangyi Chen、Karan Sikka、Michael Cogswell、Heng Ji 和 Ajay Divakaran。测量和提升视觉语言模型中的链式思维推理。arXiv
    预印本 arXiv:2309.04461，2023年。'
- en: '[11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William
    Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling
    instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Hyung Won Chung、Le Hou、Shayne Longpre、Barret Zoph、Yi Tay、William Fedus、Yunxuan
    Li、Xuezhi Wang、Mostafa Dehghani、Siddhartha Brahma 等。扩展指令微调语言模型。arXiv 预印本 arXiv:2210.11416，2022年。'
- en: '[12] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,
    2021.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Karl Cobbe、Vineet Kosaraju、Mohammad Bavarian、Mark Chen、Heewoo Jun、Lukasz
    Kaiser、Matthias Plappert、Jerry Tworek、Jacob Hilton、Reiichiro Nakano 等。训练验证器解决数学文字问题。arXiv
    预印本 arXiv:2110.14168，2021年。'
- en: '[13] Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are
    gpts: An early look at the labor market impact potential of large language models.
    arXiv preprint arXiv:2303.10130, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tyna Eloundou、Sam Manning、Pamela Mishkin 和 Daniel Rock。GPTs 是 GPTs：大型语言模型对劳动市场潜在影响的早期观察。arXiv
    预印本 arXiv:2303.10130，2023年。'
- en: '[14] Kevin A Fischer. Reflective linguistic programming (rlp): A stepping stone
    in socially-aware agi (socialagi). arXiv preprint arXiv:2305.12647, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Kevin A Fischer。反射性语言编程（rlp）：社会意识 agi（socialagi）中的垫脚石。arXiv 预印本 arXiv:2305.12647，2023年。'
- en: '[15] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang,
    Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International
    Conference on Machine Learning, pages 10764–10799\. PMLR, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Luyu Gao、Aman Madaan、Shuyan Zhou、Uri Alon、Pengfei Liu、Yiming Yang、Jamie
    Callan 和 Graham Neubig。Pal：程序辅助语言模型。在国际机器学习会议，页码10764–10799。PMLR，2023年。'
- en: '[16] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora,
    Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring
    coding challenge competence with apps. In Thirty-fifth Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track (Round 2), 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Dan Hendrycks、Steven Basart、Saurav Kadavath、Mantas Mazeika、Akul Arora、Ethan
    Guo、Collin Burns、Samir Puranik、Horace He、Dawn Song 等。通过应用测量编码挑战能力。在第三十五届神经信息处理系统会议数据集与基准追踪（第二轮），2021年。'
- en: '[17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    In International Conference on Learning Representations, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song 和 Jacob Steinhardt。测量大规模多任务语言理解。在国际学习表示会议，2020年。'
- en: '[18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart,
    Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving
    with the math dataset. In Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track (Round 2), 2021.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Dan Hendrycks、Collin Burns、Saurav Kadavath、Akul Arora、Steven Basart、Eric
    Tang、Dawn Song 和 Jacob Steinhardt。用数学数据集测量数学问题解决能力。在第三十五届神经信息处理系统会议数据集与基准追踪（第二轮），2021年。'
- en: '[19] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei.
    Voxposer: Composable 3d value maps for robotic manipulation with language models.
    arXiv preprint arXiv:2307.05973, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Wenlong Huang、Chen Wang、Ruohan Zhang、Yunzhu Li、Jiajun Wu 和 Li Fei-Fei。Voxposer：用于机器人操作的可组合
    3d 价值图，结合语言模型。arXiv 预印本 arXiv:2307.05973，2023年。'
- en: '[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 阿尔伯特·Q·江，亚历山大·萨布莱罗勒斯，亚瑟·门施，克里斯·班福德，德文德拉·辛格·查普洛特，迭戈·德·拉斯·卡萨斯，弗洛里安·布雷桑，贾娜·伦吉尔，吉约姆·兰普尔，吕西尔·索尔尼耶，等人。Mistral
    7b。arXiv预印本arXiv:2310.06825，2023年。'
- en: '[21] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve
    computer tasks. arXiv preprint arXiv:2303.17491, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 玄宇·金，皮埃尔·巴尔迪，和斯蒂芬·麦卡利尔。语言模型可以解决计算机任务。arXiv预印本arXiv:2303.17491，2023年。'
- en: '[22] LDJnr. Capybara dataset. [https://hf.co/datasets/LDJnr/Verified-Camel](https://hf.co/datasets/LDJnr/Verified-Camel),
    [https://hf.co/datasets/LDJnr/Pure-Dove](https://hf.co/datasets/LDJnr/Pure-Dove),
    [https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct](https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct),
    2023. A dataset focusing on reasoning in multi-turn conversations.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] LDJnr。Capybara数据集。 [https://hf.co/datasets/LDJnr/Verified-Camel](https://hf.co/datasets/LDJnr/Verified-Camel)，
    [https://hf.co/datasets/LDJnr/Pure-Dove](https://hf.co/datasets/LDJnr/Pure-Dove)，
    [https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct](https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct)，2023年。一个关注多轮对话推理的数据集。'
- en: '[23] Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-ai collaborative
    writing dataset for exploring language model capabilities. In Proceedings of the
    2022 CHI conference on human factors in computing systems, pages 1–19, 2022.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 米娜·李，佩西·梁，和钱杨。合著者：设计一个人类与AI协作写作数据集，以探索语言模型能力。在2022年CHI计算系统人因会议论文集中，第1–19页，2022年。'
- en: '[24] Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang,
    and Yongbin Li. Api-bank: A benchmark for tool-augmented llms, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 明浩·李，飞凡·宋，博文·余，海洋·余，周俊·李，飞黄，和永斌·李。API-bank：一个工具增强LLM的基准测试，2023年。'
- en: '[25] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,
    and ”Teknium”. Openorca: An open dataset of gpt augmented flan reasoning traces.
    [https://https://huggingface.co/Open-Orca/OpenOrca](https://https://huggingface.co/Open-Orca/OpenOrca),
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 翼·连，布雷斯·古德森，尤金·彭特兰，奥斯汀·库克，陈维切特·翁，和“Teknium”。Openorca：一个GPT增强FLAN推理痕迹的开放数据集。
    [https://https://huggingface.co/Open-Orca/OpenOrca](https://https://huggingface.co/Open-Orca/OpenOrca)，2023年。'
- en: '[26] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter,
    Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied
    control. In arXiv preprint arXiv:2209.07753, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 杰基·梁，文龙·黄，费·夏，彭·许，卡罗尔·豪斯曼，布赖恩·伊赫特，皮特·弗洛伦斯，和安迪·曾。代码作为策略：用于具身控制的语言模型程序。在arXiv预印本arXiv:2209.07753，2022年。'
- en: '[27] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi,
    and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting
    methods in natural language processing. ACM Computing Surveys, 55(9):1–35, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 彭飞·刘，韦哲·袁，金澜·傅，郑宝·江，广崎·林，和格雷厄姆·纽比。预训练、提示和预测：自然语言处理中的提示方法系统性调查。ACM计算调查，55(9)：1–35，2023年。'
- en: '[28] Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai,
    Diyi Yang, and Soroush Vosoughi. Training socially aligned language models in
    simulated human society. arXiv preprint arXiv:2305.16960, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 瑞博·刘，瑞欣·杨，陈燕·贾，葛·张，丹尼·周，安德鲁·M·戴，迪怡·杨，和索鲁什·沃索基。训练在模拟人类社会中对齐的语言模型。arXiv预印本arXiv:2305.16960，2023年。'
- en: '[29] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani,
    Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level
    reward design via coding large language models. arXiv preprint arXiv:2310.12931,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 叶成·杰森·马，威廉·梁，关志·王，德安·黄，奥斯伯特·巴斯塔尼，迪尼什·贾亚拉曼，于克·朱，林熙·范，和安尼玛·安南德库马尔。Eureka：通过编码大型语言模型实现人类级奖励设计。arXiv预印本arXiv:2310.12931，2023年。'
- en: '[30] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,
    Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
    Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842,
    2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 格雷戈尔·米亚隆，罗伯托·德西，玛利亚·洛梅利，克里斯托弗·纳尔潘蒂斯，拉姆·帕苏努鲁，罗伯塔·雷亚勒努，巴蒂斯特·罗齐耶，蒂莫·希克，简·德维维迪-于，阿斯利·切利基尔马兹，等人。增强语言模型：一项调查。arXiv预印本arXiv:2302.07842，2023年。'
- en: '[31] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
    Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint
    arXiv:2112.09332, 2021.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 纳卡诺·瑞一郎，雅各布·希尔顿，苏希尔·巴拉吉，杰夫·吴，龙·欧阳，克里斯蒂娜·金，克里斯托弗·赫斯，尚坦努·贾因，维尼特·科萨拉朱，威廉·桑德斯，等人。Webgpt：浏览器辅助问答与人工反馈。arXiv预印本arXiv:2112.09332，2021年。'
- en: '[32] OpenChat. Sharegpt dataset. [https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json](https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json),
    2023. A dataset containing multi-turn conversations between human and LLM assistants.
    It is filtered to contain data only from GPT-4.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] OpenChat。Sharegpt 数据集。 [https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json](https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json)，2023年。一个包含人类与LLM助手之间多轮对话的数据集。它被过滤以仅包含来自GPT-4的数据。'
- en: '[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in Neural Information Processing Systems, 35:27730–27744, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 龙欧阳、杰弗里·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·温赖特、帕梅拉·米什金、钟张、桑迪尼·阿格瓦尔、卡塔里娜·斯拉马、亚历克斯·雷等。训练语言模型以通过人工反馈遵循指令。神经信息处理系统进展，35:27730–27744，2022年。'
- en: '[34] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 俊尚·朴、约瑟夫·奥布莱恩、卡里·君·蔡、梅雷迪斯·林戈尔·莫里斯、帕西·梁和迈克尔·S·伯恩斯坦。生成代理：人类行为的交互模拟。在第36届ACM用户界面软件与技术年会论文集中，第1–22页，2023年。'
- en: '[35] Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured
    tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers), pages 1470–1480, 2015.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 帕努蓬·帕苏帕和帕西·梁。在半结构化表格上进行组合语义解析。在第53届计算语言学协会年会暨第7届国际自然语言处理联合会议（第1卷：长篇论文）论文集中，第1470–1480页，2015年。'
- en: '[36] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla:
    Large language model connected with massive apis. ArXiv, abs/2305.15334, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Shishir G. Patil、田俊张、辛王和约瑟夫·E·冈萨雷斯。Gorilla：与大量API连接的大型语言模型。ArXiv，abs/2305.15334，2023年。'
- en: '[37] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 陈乾、辛聪、程杨、魏泽陈、余生苏、居元徐、智远刘和毛松孙。软件开发中的交互代理。arXiv 预印本 arXiv:2307.07924，2023年。'
- en: '[38] Bo Qiao, Liqun Li, Xu Zhang, Shilin He, Yu Kang, Chaoyun Zhang, Fangkai
    Yang, Hang Dong, Jue Zhang, Lu Wang, et al. Taskweaver: A code-first agent framework.
    arXiv preprint arXiv:2311.17541, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 博乔、李群、徐张、时林赫、余康、超云张、方凯杨、杭董、绝张、陆王等。Taskweaver：一个以代码为主的代理框架。arXiv 预印本 arXiv:2311.17541，2023年。'
- en: '[39] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui,
    Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation
    models. arXiv preprint arXiv:2304.08354, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] 于佳秦、盛定胡、彦凯林、魏泽陈、宁丁、甘渠崔、甄妮·曾、余飞黄、超俊肖、迟寒等。使用基础模型进行工具学习。arXiv 预印本 arXiv:2304.08354，2023年。'
- en: '[40] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai
    Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie,
    Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating
    large language models to master 16000+ real-world apis. ArXiv, abs/2307.16789,
    2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] 于佳秦、施亮、易宁叶、昆仑朱、兰燕、雅婷陆、彦凯林、辛聪、项如唐、比尔·钱、思涵赵、润初田、若冰谢、杰周、马克·H·格斯坦、大海李、智远刘和毛松孙。Toolllm：帮助大型语言模型掌握16000+真实世界API。ArXiv，abs/2307.16789，2023年。'
- en: '[41] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models
    can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 蒂莫·希克、简·德维维迪-余、罗伯托·德西、罗伯塔·赖勒努、玛利亚·洛梅利、卢克·泽特尔莫耶、尼古拉·坎切达和托马斯·斯基亚隆。Toolformer：语言模型可以自我学习使用工具。arXiv
    预印本 arXiv:2302.04761，2023年。'
- en: '[42] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    arXiv preprint arXiv:2303.17580, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 永亮申、凯涛宋、徐谭、董胜李、伟铭陆和悦婷庄。Hugginggpt：使用ChatGPT及其在HuggingFace中的朋友解决AI任务。arXiv
    预印本 arXiv:2303.17580，2023年。'
- en: '[43] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler,
    and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
    interactive learning. In International Conference on Learning Representations,
    2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Mohit Shridhar、袁兴迪、Marc-Alexandre Cote、Yonatan Bisk、Adam Trischler 和 Matthew
    Hausknecht。《Alfworld：为互动学习对齐文本和具身环境》。国际学习表征会议，2020年。'
- en: '[44] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu,
    Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating
    situated robot task plans using large language models. In 2023 IEEE International
    Conference on Robotics and Automation (ICRA), pages 11523–11530, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Ishika Singh、Valts Blukis、Arsalan Mousavian、Ankit Goyal、Danfei Xu、Jonathan
    Tremblay、Dieter Fox、Jesse Thomason 和 Animesh Garg。《Progprompt：利用大型语言模型生成具体现实机器人任务计划》。2023
    年 IEEE 国际机器人与自动化会议（ICRA），第 11523-11530 页，2023年。'
- en: '[45] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference
    via python execution for reasoning. Proceedings of IEEE International Conference
    on Computer Vision (ICCV), 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Dídac Surís、Sachit Menon 和 Carl Vondrick。《Vipergpt：通过 Python 执行进行视觉推理》。IEEE
    国际计算机视觉会议（ICCV）论文集，2023年。'
- en: '[46] TIOBE Index. Tiobe index. [https://www.tiobe.com/tiobe-index/](https://www.tiobe.com/tiobe-index/),
    Accessed at Jan 23rd, 2024, 2024. The TIOBE Programming Community index is an
    indicator of the popularity of programming languages. The index is updated once
    a month. The ratings are based on the number of skilled engineers world-wide,
    courses and third party vendors.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] TIOBE Index。《TIOBE 指数》。[https://www.tiobe.com/tiobe-index/](https://www.tiobe.com/tiobe-index/)，访问于
    2024 年 1 月 23 日。TIOBE 编程社区指数是编程语言流行度的指标。该指数每月更新一次。评级基于全球技术工程师数量、课程和第三方供应商。'
- en: '[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。《Llama
    2：开放基础和微调聊天模型》。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[48] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 王冠智、谢雨齐、姜云凡、Ajay Mandlekar、肖超伟、朱煜柯、范林希 和 Anima Anandkumar。《Voyager：一种开放式具身代理与大型语言模型》。arXiv
    预印本 arXiv:2305.16291，2023年。'
- en: '[49] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language
    model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 王磊、马晨、冯雪阳、张泽宇、杨浩、张景森、陈智远、唐佳凯、陈旭、林燕凯等。《基于大型语言模型的自主代理调查》。arXiv 预印本 arXiv:2308.11432，2023年。'
- en: '[50] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference
    on Empirical Methods in Natural Language Processing, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 王若尧、Peter Alexander Jansen、Marc-Alexandre Côté 和 Prithviraj Ammanabrolu。《Scienceworld：你的代理比五年级学生更聪明吗？》。自然语言处理实证方法会议，2022年。'
- en: '[51] Xingyao Wang, Sha Li, and Heng Ji. Code4Struct: Code generation for few-shot
    event structure prediction. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,
    editors, Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 3640–3663, Toronto, Canada, July 2023\.
    Association for Computational Linguistics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 王星耀、李莎 和 Heng Ji。《Code4Struct：用于少量示例事件结构预测的代码生成》。在 Anna Rogers、Jordan
    Boyd-Graber 和 Naoaki Okazaki 编辑的《第 61 届计算语言学协会年会论文集（第 1 卷：长篇论文）》中，第 3640-3663
    页，多伦多，加拿大，2023年 7 月。计算语言学协会。'
- en: '[52] Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. Leti: Learning
    to generate from textual interactions. ArXiv, abs/2305.10314, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 王星耀、彭浩、Reyhaneh Jabbarvand 和 Heng Ji。《Leti：从文本互动中学习生成》。ArXiv，abs/2305.10314，2023年。'
- en: '[53] Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng,
    and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language
    feedback. arXiv preprint arXiv:2309.10691, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 王星耀、王子涵、刘家腾、陈阳义、袁立凡、彭浩 和 Heng Ji。《Mint：在与工具和语言反馈的多轮互动中评估 LLMs》。arXiv 预印本
    arXiv:2309.10691，2023年。'
- en: '[54] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
    reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 薛智王、杰森·韦、戴尔·舒尔曼斯、阮国立、艾德·池、沙兰·纳朗、阿坎克莎·乔杜里和丹尼·周。《自洽性改善了语言模型中的思维链推理》。arXiv
    预印本 arXiv:2203.11171，2022年。'
- en: '[55] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,
    explain, plan and select: Interactive planning with large language models enables
    open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 子豪王、绍飞蔡、安基刘、肖建马和艺涛梁。《描述、解释、计划和选择：与大型语言模型进行交互式规划使开放世界多任务代理成为可能》。arXiv 预印本
    arXiv:2302.01560，2023年。'
- en: '[56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in Neural Information Processing Systems, 35:24824–24837,
    2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 杰森·韦、薛智王、戴尔·舒尔曼斯、马尔滕·博斯马、费晓、艾德·池、阮国立·V·周、丹尼·周等。《思维链提示引发大型语言模型中的推理》。神经信息处理系统进展，第35卷：24824–24837，2022年。'
- en: '[57] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang.
    On the tool manipulation capability of open-source large language models, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 钱童徐、冯璐洪、博李、常然胡、郑宇陈和简张。《关于开源大型语言模型的工具操作能力》，2023年。'
- en: '[58] Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan
    Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai. If llm
    is the wizard, then code is the wand: A survey on how code empowers large language
    models to serve as intelligent agents, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 柯杨、贾腾刘、约翰·吴、超齐杨、易R·冯、沙李、紫轩黄、徐曹、邢瑶王、易全王、恒吉和郑祥翟。《如果大型语言模型是巫师，那么代码就是魔杖：关于代码如何赋能大型语言模型以充当智能代理的调查》，2024年。'
- en: '[59] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
    Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable
    multi-hop question answering. In Proceedings of the 2018 Conference on Empirical
    Methods in Natural Language Processing, pages 2369–2380, 2018.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 直林杨、彭奇、赛政张、约书亚·本吉奥、威廉·科恩、鲁斯兰·萨拉胡丁诺夫和克里斯托弗·D·曼宁。《Hotpotqa: 一个多样化、可解释的多跳问答数据集》。发表于2018年自然语言处理实证方法会议论文集，第2369–2380页，2018年。'
- en: '[60] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards
    scalable real-world web interaction with grounded language agents. Advances in
    Neural Information Processing Systems, 35:20744–20757, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 舒昱姚、霍华德·陈、约翰·杨和卡尔提克·纳拉辛汉。《Webshop: 朝着可扩展的现实世界网页交互与基础语言代理迈进》。神经信息处理系统进展，第35卷：20744–20757，2022年。'
- en: '[61] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. arXiv preprint arXiv:2305.10601, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 舒昱姚、滕余、杰弗里·赵、伊扎克·沙夫兰、托马斯·L·格里菲斯、袁曹和卡尔提克·纳拉辛汉。《思维树：使用大型语言模型进行深思熟虑的问题解决》。arXiv
    预印本 arXiv:2305.10601，2023年。'
- en: '[62] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. In The
    Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 舒昱姚、杰弗里·赵、滕余、南杜、伊扎克·沙夫兰、卡尔提克·R·纳拉辛汉和袁曹。《React: 在语言模型中协同推理和行动》。在第十一届国际学习表示会议上，2022年。'
- en: '[63] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng,
    Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer:
    Retrospective large language agents with policy gradient optimization. arXiv preprint
    arXiv:2308.02151, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 魏然姚、谢尔比·海内克、胡安·卡洛斯·尼布莱斯、智伟刘、易浩冯、乐雪、瑞泰什·穆尔提、泽元陈、建国张、德万什·阿皮特等。《Retroformer:
    具有策略梯度优化的回顾性大型语言代理》。arXiv 预印本 arXiv:2308.02151，2023年。'
- en: '[64] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Ren Fung, Hao Peng, and Heng
    Ji. Craft: Customizing llms by creating and retrieving from specialized toolsets.
    ArXiv, abs/2309.17428, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 李凡元、杨毅陈、邢瑶王、易仁冯、郝鹏和恒吉。《Craft: 通过创建和从专用工具集检索来定制大型语言模型》。ArXiv，abs/2309.17428，2023年。'
- en: '[65] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and
    Jie Tang. Agenttuning: Enabling generalized agent abilities for llms, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 奥汉·曾、明道刘、瑞路、博文王、肖刘、郁晓东和杰唐。《Agenttuning: 使大型语言模型具备通用代理能力》，2023年。'
- en: '[66] Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang,
    and Mingchen Cai. Prefer: Prompt ensemble learning via feedback-reflect-refine.
    arXiv preprint arXiv:2308.12033, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 陈瑞张、林刘、金鹏王、储元王、肖孙、洪宇王和明辰蔡。《Prefer: 通过反馈-反思-优化的提示集合学习》。arXiv 预印本 arXiv:2308.12033，2023年。'
- en: '[67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge
    with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, 等. 使用mt-bench和chatbot
    arena评估llm作为评判者。arXiv预印本 arXiv:2306.05685, 2023.'
- en: '[68] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang,
    Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally
    capable agents for open-world enviroments via large language models with text-based
    knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang,
    Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, 等. Minecraft中的幽灵：通过基于文本的知识和记忆的大型语言模型，为开放世界环境提供通用能力的代理。arXiv预印本
    arXiv:2305.17144, 2023.'
- en: 'Table A.6: Example of actions for re-purposed API-Bank [[24](#bib.bib24)] and
    M³ToolEval.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.6：重新用途API-Bank [[24](#bib.bib24)] 和 M³ToolEval 的操作示例。
- en: '| Format | Action |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | 操作 |'
- en: '| --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CodeAct | AddAgenda(content="Meeting with John", time="2023-10-26 09:00:00")
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| CodeAct | AddAgenda(content="与John的会议", time="2023-10-26 09:00:00") |'
- en: '| JSON | {"action": "AddAgenda", "content": "Meeting with John", "time": "2023-10-26
    09:00:00"} |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| JSON | {"action": "AddAgenda", "content": "与John的会议", "time": "2023-10-26
    09:00:00"} |'
- en: '| Text | Action: AddAgenda, content: Meeting with John, time: 2023-10-26 09:00:00
    |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 操作：AddAgenda，内容：与John的会议，时间：2023-10-26 09:00:00 |'
- en: 'Table A.7: Comparison between M³ToolEval and existing tool-use evaluation benchmark.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.7：M³ToolEval与现有工具使用评估基准的比较。
- en: '| Benchmark | M³ToolEval | ToolBench | APIBench | API-Bank | ToolBench |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 基准测试 | M³ToolEval | ToolBench | APIBench | API-Bank | ToolBench |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | (This work) | [[40](#bib.bib40)] | [[36](#bib.bib36)] | [[24](#bib.bib24)]
    | [[57](#bib.bib57)] |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | (本研究) | [[40](#bib.bib40)] | [[36](#bib.bib36)] | [[24](#bib.bib24)] |
    [[57](#bib.bib57)] |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Requiring multi-turn interaction | ✔ | ✔ | ✗ | ✗ | ✗ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 需要多轮交互 | ✔ | ✔ | ✗ | ✗ | ✗ |'
- en: '| Multiple tools | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 多种工具 | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '| Evaluation | Answer Match | LLM Evaluator | AST Tree Match | API-Call Match
    | Test Case |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 答案匹配 | LLM评估器 | AST树匹配 | API调用匹配 | 测试用例 |'
- en: '| No dependency on external API^∗ | ✔ | ✗ | ✗ | ✔ | ✗ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 不依赖于外部API^∗ | ✔ | ✗ | ✗ | ✔ | ✗ |'
- en: '| Supported API Action Format | CodeAct & JSON & Text | JSON | CodeAct | JSON
    | CodeAct |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 支持的API操作格式 | CodeAct & JSON & 文本 | JSON | CodeAct | JSON | CodeAct |'
- en: '*'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: Whether to rely on external API (e.g., RapidAPI, Google Sheet) hosted by a third
    party. The availability of such third-party APIs can greatly impact evaluation
    results (e.g., low API-calling performance not because the model is bad but rather
    because the API required is not accessible).
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否依赖于第三方托管的外部API（例如，RapidAPI、Google Sheets）。这种第三方API的可用性可能对评估结果产生重大影响（例如，低API调用性能可能不是模型差，而是所需的API不可用）。
- en: 'Table A.8: Ablation study results. The best results are bolded, and the second-best
    results are underlined. ID and OD stand for in-domain and out-of-domain evaluation
    correspondingly. Overall averaged performance normalizes the MT-Bench score to
    be consistent with other tasks and excludes in-domain tasks for fair comparison.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.8：消融研究结果。最佳结果用粗体标出，第二最佳结果用下划线标出。ID和OD分别代表领域内和领域外评估。总体平均性能将MT-Bench分数标准化以与其他任务一致，并排除领域内任务以进行公平比较。
- en: Agent Tasks Generic LLM Tasks Overall Code as Action Text as Action (OD) (OD)
    Average Model Size MINT (ID) MINT (OD) Miniwob++ SciWorld MMLU HumanEval GSM8K
    MTBench CodeActAgent (Llama2-based) 7B $\mathbf{51.3}$
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Agent Tasks Generic LLM Tasks Overall Code as Action Text as Action (OD) (OD)
    平均模型大小 MINT (ID) MINT (OD) Miniwob++ SciWorld MMLU HumanEval GSM8K MTBench CodeActAgent
    (基于Llama2) 7B $\mathbf{51.3}$
- en: Appendix A Comparison with Prior Work that Uses Code Generation for Problem-solving
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A：与使用代码生成进行问题解决的先前工作的比较
- en: In this section, we discuss the fundamental differences between CodeAct and
    prior work that prompt LLM to generate code for problem-solving. Existing work
    have explored using code generation for task-solving in different domains, for
    example, Code4Struct [[51](#bib.bib51)] for structured prediction, PaL [[15](#bib.bib15)]
    for math reasoning, code-as-policy [[26](#bib.bib26)] for robot control, ViperGPT
    [[45](#bib.bib45)] for visual question answering, Voyager [[48](#bib.bib48)] for
    playing games, etc.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了**CodeAct**与以往促使LLM生成代码以解决问题的工作的根本区别。现有工作已经探索了在不同领域使用代码生成来解决任务，例如，用于结构预测的Code4Struct
    [[51](#bib.bib51)]，用于数学推理的PaL [[15](#bib.bib15)]，用于机器人控制的code-as-policy [[26](#bib.bib26)]，用于视觉问答的ViperGPT
    [[45](#bib.bib45)]，以及用于玩游戏的Voyager [[48](#bib.bib48)]等。
- en: 'Most prior work generates code (i.e., a static sequence of actions) in a single-turn
    setting and cannot dynamically readjust action on new observation: It is considered
    a failure when the model-generated code fails to solve a task on the first attempt.
    This setting overlooks the potential of environmental observation (e.g., code
    execution results) that might benefit future action and overall decision (e.g.,
    dynamically adjusting subsequent code after observing intermediate code execution
    results, fixing erroneous code after seeing an error message). That is, the generated
    code is a static sequence of actions that cannot be dynamically re-adjusted on
    the fly by incorporating new observations. Such a single-turn setting makes it
    challenging to scale to more challenging problems since even expert human programmers
    usually cannot write functionally correct code in the first pass. On the other
    hand, CodeAct is a multi-turn interaction agent framework that allows dynamic
    adjustment of prior actions or emitting new actions by design (§[2.1](#S2.SS1
    "2.1 What is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents"), Fig. [2](#S2.F2 "Figure 2 ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")) and is compatible
    with any form of textual observation (e.g., tool execution output, automated feedback)
    from the environment. Beyond being compatible with environmental observation,
    our instruction tuning dataset CodeActInstruct specifically collects data for
    multi-turn self-improving, offering a practical solution to enhance LLM’s multi-turn
    self-improving process.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数以往的工作在单轮设置中生成代码（即，一系列静态动作），无法根据新的观察动态调整动作：当模型生成的代码在第一次尝试时未能解决任务时，这被认为是失败。这种设置忽视了环境观察（例如，代码执行结果）的潜力，这可能对未来的动作和整体决策有利（例如，在观察到中间代码执行结果后动态调整后续代码，看到错误消息后修复错误代码）。也就是说，生成的代码是一系列静态动作，不能通过整合新观察动态重新调整。这种单轮设置使得扩展到更具挑战性的问题变得困难，因为即使是专家程序员通常也无法在第一次尝试时编写功能正确的代码。另一方面，**CodeAct**是一个多轮交互代理框架，设计上允许动态调整之前的动作或发出新动作（§[2.1](#S2.SS1
    "2.1 What is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents")，图[2](#S2.F2 "Figure 2 ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")），并兼容来自环境的任何形式的文本观察（例如，工具执行输出，自动反馈）。除了兼容环境观察外，我们的指令调优数据集CodeActInstruct专门收集用于多轮自我改进的数据，提供了一种实际的解决方案来提升LLM的多轮自我改进过程。
- en: In addition, previous approaches require heavy prompt engineering and crafting
    of few-shot demonstrations to tailor LLMs to a particular domain or task (e.g.,
    robot control [[26](#bib.bib26)]) since the backbone LLMs are not specially optimized
    for dynamic planning and decision making. In contrast, in this work, we propose
    the CodeAct framework that uses executable Python code to consolidate LLM agents’
    actions into unified action space and collect CodeActInstruct on a diverse array
    of tasks (e.g., information seeking, tabular reasoning, robot planning, etc) to
    make the trained model, CodeActAgent, easily scale to diverse tasks and domains
    with minimal human efforts as shown in §[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents").
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以前的方法需要大量的提示工程和少量示例的制作，以使LLM适应特定的领域或任务（例如，机器人控制 [[26](#bib.bib26)]），因为基础LLM并未特别优化用于动态规划和决策。相比之下，在本工作中，我们提出了CodeAct框架，该框架使用可执行的Python代码将LLM代理的动作整合到统一的动作空间中，并在各种任务上收集CodeActInstruct（例如，信息获取、表格推理、机器人规划等），使得训练后的模型CodeActAgent能够以最小的人力成本轻松扩展到多样的任务和领域，如§[3.2](#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")所示。
- en: 'One notable exception among prior work is Voyager [[48](#bib.bib48)], which
    performs iterative prompting in a constrained action space of function definitions
    to fix code errors. Different from CodeAct, such setting disallows dynamic re-adjustment
    of atomic actions on the fly: In CodeAct, for a particular task (e.g., craft stone
    sword in Minecraft), the agent can first execute one line of code (any atomic
    action or composed functions, e.g., move forward, locate stone), and dynamically
    produce different actions based on the observation of the first action. This is
    challenging for Voyager to achieve: Similar to code-as-policy [[26](#bib.bib26)],
    they generate action (a skill, e.g., craft stone sword) as a Python function definition
    that outlines the entire plan for a task (e.g., multi-step code outlining how
    you should craft a stone sword and handles for different potential cases, which
    requires strong domain knowledge). This imposes significant constraints on the
    agent’s action space and disallows dynamic re-adjustment of atomic actions on
    the fly: That is, the agent can only generate one complete function first (e.g.,
    by imaging all possible cases that might happen when you try to locate stones),
    execute the entire function, observe the feedback, and update the entire function
    as action in the subsequent move. Besides the constrained ability to re-adjust
    action from environmental observation, they also rely on heavy prompting engineering
    (a typical drawback discussed above) to provide relevant information (e.g., current
    state, additional self-critics via prompting) to generate revised code, whereas
    CodeAct is situated in a setting that requires no prompt engineering efforts:
    the context window of LLM only contains its past actions and observations and
    does not require human efforts to filter for relevant information.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个显著的例外是 Voyager [[48](#bib.bib48)]，它在受限的函数定义动作空间中执行迭代提示以修复代码错误。与 CodeAct
    不同，这种设置不允许动态重新调整原子动作：在 CodeAct 中，对于特定任务（例如，在 Minecraft 中制作石剑），代理可以先执行一行代码（任何原子动作或组合函数，例如，向前移动，定位石头），并根据第一次行动的观察动态生成不同的动作。这对于
    Voyager 来说具有挑战性：类似于代码即策略 [[26](#bib.bib26)]，他们生成的动作（一个技能，例如，制作石剑）作为 Python 函数定义，概述了任务的整个计划（例如，多步骤代码概述了如何制作石剑，并处理不同的潜在情况，这需要强大的领域知识）。这对代理的动作空间施加了重大限制，并且不允许动态重新调整原子动作：即代理只能首先生成一个完整的函数（例如，通过设想尝试定位石头时可能发生的所有情况），执行整个函数，观察反馈，并在随后的动作中将整个函数更新为行动。除了从环境观察中调整动作的受限能力外，他们还依赖于大量的提示工程（上述讨论的典型缺点）来提供相关信息（例如，当前状态，通过提示提供的额外自我批评）以生成修订代码，而
    CodeAct 处于不需要提示工程的设置中：LLM 的上下文窗口仅包含其过去的动作和观察，不需要人工努力来筛选相关信息。
- en: Appendix B Comparison with TaskWeaver
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 与 TaskWeaver 的比较
- en: In the landscape of unifying the action space of LLM agents, our work represents
    a leap over the previous initiative, TaskWeaver [[38](#bib.bib38)]. While TaskWeaver
    deserves acknowledgment for initially integrating code into the action space of
    LLM agents, its exploration remains limited. This work, primarily characterized
    by its reliance on a limited set of qualitative examples with close-sourced models
    as the backbones, fails to harness the full potential of this integration, remaining
    merely conceptual demonstrations. Our work transcends mere conceptualization by
    conducting an extensive and rigorous analysis, clearly quantifying the benefits
    of code action within LLM agents. Beyond this, we introduce a unique instruction-tuning
    dataset CodeActInstruct specifically designed to amplify the agent’s capabilities
    in executing code-based actions and an open-source LLM agent CodeActAgent. These
    contributions not only extend the work of TaskWeaver but also pave the way for
    future explorations, offering valuable resources to the open-source community
    and redefining the potential of LLM agents in practical applications.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在统一 LLM 代理的动作空间的背景下，我们的工作代表了相对于之前的举措 TaskWeaver [[38](#bib.bib38)] 的飞跃。虽然 TaskWeaver
    应当因为最初将代码整合进 LLM 代理的动作空间而获得认可，但其探索仍然有限。这项工作主要依赖于一组有限的定性示例和闭源模型作为基础，未能充分发挥这种整合的潜力，仅停留在概念展示上。我们的工作超越了单纯的概念化，通过进行广泛且严格的分析，明确量化了代码动作在
    LLM 代理中的好处。除此之外，我们引入了一个独特的指令调优数据集 CodeActInstruct，专门设计用于提升代理在执行基于代码的动作中的能力，以及一个开源
    LLM 代理 CodeActAgent。这些贡献不仅扩展了 TaskWeaver 的工作，还为未来的探索铺平了道路，为开源社区提供了宝贵资源，并重新定义了
    LLM 代理在实际应用中的潜力。
- en: Appendix C General Data Down-sample
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 一般数据下采样
- en: •
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ShareGPT [[2](#bib.bib2)]: We remove all single-turn conversations, then perform
    random sub-sample to a desired final number.'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ShareGPT [[2](#bib.bib2)]：我们删除所有单轮对话，然后进行随机子样本，以达到所需的最终数量。
- en: •
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ShareGPT (GPT-4) [[32](#bib.bib32)]: We do not perform sub-sampling on this
    dataset.'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ShareGPT (GPT-4) [[32](#bib.bib32)]：我们对这个数据集不进行子采样。
- en: •
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'OpenOrca [[25](#bib.bib25)]: We select the CoT subset of OpenOrca, then perform
    a random sub-sample to a desired final number.'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: OpenOrca [[25](#bib.bib25)]：我们选择 OpenOrca 的 CoT 子集，然后进行随机子样本，以达到所需的最终数量。
- en: •
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CapyBara [[22](#bib.bib22)]: We do not perform sub-sampling on this dataset.'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CapyBara [[22](#bib.bib22)]：我们对这个数据集不进行子采样。
- en: Appendix D CodeActAgent Training Details
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D CodeActAgent 训练细节
- en: All SFT experiments are performed on one 4xA100 40GB SXM node using a fork of
    Megatron-LLM [[4](#bib.bib4)] with a training throughput of around 9k tokens per
    second. We use chatML format²²2[https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)
    for all multi-turn data, and we only calculate and optimize for loss on the assistant
    response. We pack short instances into longer ones and apply flash attention for
    training efficiency.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 SFT 实验都在一个 4xA100 40GB SXM 节点上进行，使用 Megatron-LLM [[4](#bib.bib4)] 的一个分支，训练吞吐量约为每秒
    9k 个 token。我们对所有多轮数据使用 chatML 格式²²2[https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)，并且我们只计算和优化助理响应的损失。我们将短实例打包成较长的实例，并应用闪电注意力以提高训练效率。
- en: We train both LLaMA-2 and Mistral LLMs with Tensor Parallel of 4, the learning
    rate of 1e-5 with 50 warmup steps and cosine decay (end learning rate of 1e-6).
    We train for five epochs with a batch size of 32\. We use the 3rd epoch checkpoint
    for all our experiments.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Tensor Parallel 4 对 LLaMA-2 和 Mistral LLM 进行训练，学习率为 1e-5，经过 50 次预热步骤和余弦衰减（结束学习率为
    1e-6）。我们训练了五个 epoch，批次大小为 32。我们使用第 3 个 epoch 的检查点进行所有实验。
- en: Appendix E Example Prompt for CodeAct
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E CodeAct 示例提示
- en: This is an example (zero-shot) system prompt used in a deploy instance of CodeAct
    where we used chatML format.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例（零样本）系统提示，使用了 chatML 格式的 CodeAct 部署实例。
- en: The users may optionally include tools descriptions similar to §[F](#A6 "Appendix
    F M3ToolEval Prompt ‣ Executable Code Actions Elicit Better LLM Agents") or including
    extra in-context examples similar to §[G.3](#A7.SS3 "G.3 Example of Trajectory
    Generation Prompt ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions
    Elicit Better LLM Agents").
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以选择包含类似于§[F](#A6 "Appendix F M3ToolEval Prompt ‣ Executable Code Actions
    Elicit Better LLM Agents")的工具描述，或包括类似于§[G.3](#A7.SS3 "G.3 Example of Trajectory
    Generation Prompt ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions
    Elicit Better LLM Agents")的额外上下文示例。
- en: '<|im_start|>systemA  chat  between  a  curious  user  and  an  artificial  intelligence  assistant.  The  assistant  gives  helpful,  detailed,  and  polite  answers  to  the  user’s  questions.The  assistant  can  interact  with  an  interactive  Python  (Jupyter  Notebook)  environment  and  receive  the  corresponding  output  when  needed.  The  code  should  be  enclosed  using  ""  tag,  for  example:    print("Hello  World!")  .The  assistant  should  attempt  fewer  things  at  a  time  instead  of  putting  too  much  code  in  one    block.  The  assistant  can  install  packages  through  PIP  by    !pip  install  [package  needed]    and  should  always  import  packages  and  define  variables  before  starting  to  use  them.The  assistant  should  stop    and  provide  an  answer  when  they  have  already  obtained  the  answer  from  the  execution  result.  Whenever  possible,  execute  the  code  for  the  user  using    instead  of  providing  it.The  assistant’s  response  should  be  concise,  but  do  express  their  thoughts.<|im_end|>'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: <|im_start|>systemA 这是一个好奇用户与人工智能助手之间的聊天。助手对用户的问题提供有帮助、详细且礼貌的回答。助手可以与互动式 Python（Jupyter
    Notebook）环境互动，并在需要时接收相应的输出。代码应使用""标签括起来，例如： print("Hello World!")
    。助手应该尽量减少每次尝试的内容，而不是将过多的代码放在一个块中。助手可以通过 !pip install
    [所需包] 安装软件包，并且应该在开始使用之前总是导入包和定义变量。助手应该在从执行结果中获得答案后停止并提供答案。尽可能使用为用户执行代码，而不是提供代码。助手的回答应简洁，但要表达其思路。<|im_end|>
- en: Appendix F M³ToolEval Prompt
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F M³ToolEval 提示
- en: 'You  have  access  to  the  following  tools:{{Tool  Definition}}{{Formatting  Instruction}}Now,  let’s  get  started!Instruction:  {{Example:  Find  the  current  price  of  Legendary  Wand.}}Answer  in  the  format  of  ’xx.xx’  (e.g.,  12.34).You  can  optionally  express  your  thoughts  using  natural  language  before  your  action.  For  example,  ’Thought:  I  want  to  use  tool_name  to  do  something.  Action:    End  Action’.Note  that  your  output  should  always  contain  either  ’Action:’  or  ’Answer:’,  but  not  both.When  you  are  done,  output  the  result  using  ’Answer:  your  answer’Please  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下工具：{{工具定义}}{{格式说明}}现在，我们开始吧！指令：{{示例：查找传奇法杖的当前价格。}}答案格式为’xx.xx’（例如，12.34）。你可以在行动前用自然语言表达你的想法。例如，’想法：我想使用
    tool_name 来做某事。行动： 结束行动’。请注意，你的输出应始终包含’行动：’或’答案：’，但不能同时包含。当你完成时，用’答案：你的答案’输出结果。请仅输出答案（例如，单一数字），不要包含其他文本。
- en: Each {{...}} component above will be substitute with corresponding information.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 上述每个 {{...}} 组件将替换为相应的信息。
- en: F.1 Example of {{Tool Definition}}
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 {{工具定义}} 示例
- en: The following is an example tool definition for web-browsing.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个用于网页浏览的示例工具定义。
- en: '[1]  click_url:  Clicks  on  a  URL.  A  clickable  URL  looks  like  [Clickable  ’’]  in  the  webpage.Arguments:  url  (str).Returns  the  rendered  content  of  the  webpage  after  clicking  the  URL  showing  on  the  current  rendered  page.Signature:  click_url(url:  str)  ->  str[2]  go_to_previous_page:  Goes  back  to  the  previous  page.  It  has  no  arguments.After  going  back  to  the  previous  page,  return  the  rendered  content  of  the  webpage.Signature:  go_to_previous_page()  ->  str[3]  scroll_down:  Scrolls  down  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  down.Signature:  scroll_down()  ->  str[4]  scroll_up:  Scrolls  up  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  up.Signature:  scroll_up()  ->  str[5]  view:  Return  the  current  view  in  string  format  of  the  rendered  webpage.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage.You  should  call  this  when  you  want  to  see  the  rendered  content  of  the  current  webpage.Signature:  view()  ->  str[6]  calculator:  Evaluates  the  given  expression  and  returns  the  result.  Accepts  a  calculation  expression  as  input.  For  example,  "2  +  (3  *  4)"  will  return  14.Signature:  calculator(expression:  str)  ->  float'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]  click_url:  点击一个 URL。一个可点击的 URL 在网页中看起来像 [Clickable  ’’]。参数：url  (str)。返回点击当前渲染页面上
    URL 后的网页渲染内容。签名：click_url(url: str) -> str[2]  go_to_previous_page:  返回到上一页。没有参数。返回到上一页后，返回网页的渲染内容。签名：go_to_previous_page()
    -> str[3]  scroll_down:  向下滚动视图。没有参数。返回向下滚动后的网页渲染内容。签名：scroll_down() -> str[4]  scroll_up:  向上滚动视图。没有参数。返回向上滚动后的网页渲染内容。签名：scroll_up()
    -> str[5]  view:  以字符串格式返回当前渲染网页的视图。没有参数。返回网页的渲染内容。你应该在想要查看当前网页的渲染内容时调用此功能。签名：view()
    -> str[6]  calculator:  计算给定表达式并返回结果。接受一个计算表达式作为输入。例如，"2 + (3 * 4)" 将返回 14。签名：calculator(expression:
    str) -> float'
- en: F.2 Example of {{Formatting Instruction}}
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 {{格式指令}} 示例
- en: Different action format has different formatting instructions.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的操作格式有不同的格式指令。
- en: F.3 Formatting Instruction for Code as Action
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 代码作为操作的格式指令
- en: 'You  can  use  the  tools  by  outputing  a  block  of  Python  code  that  invoke  the  tools.You  may  use  for-loops,  if-statements,  and  other  Python  constructs  when  necessary.Be  sure  to  print  the  final  answer  at  the  end  of  your  code.You  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:tool_name(argument_1)End  Action’'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过输出一块调用工具的 Python 代码来使用这些工具。你可以在必要时使用 for 循环、if 语句和其他 Python 结构。确保在代码的最后打印最终答案。你应该以
    ’Action:’ 开始工具调用，并以 ’End Action’ 结束。例如：’Action:tool_name(argument_1)End Action’
- en: F.4 Formatting Instruction for Json as Action
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 Json 的格式指令作为操作
- en: 'You  can  use  the  tools  by  outputing  a  JSON  object  with  the  following  fields:-  ’tool’:  the  name  of  the  tool-  ’args’:  a  list  of  arguments  to  the  toolYou  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:  {"tool":  "tool_name",  "args":  ["argument_1"]}  End  Action’You  can  only  invoke  one  tool  at  a  time.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以通过输出一个 JSON 对象来使用这些工具，JSON 对象包含以下字段：-  ’tool’: 工具的名称 -  ’args’: 工具的参数列表你应该以
    ’Action:’ 开始工具调用，并以 ’End Action’ 结束。例如：’Action: {"tool": "tool_name", "args":
    ["argument_1"]} End Action’你一次只能调用一个工具。'
- en: F.5 Formatting Instruction for Text as Action
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 文本作为操作的格式指令
- en: 'You  can  use  the  tools  by  outputing  the  tool  name  followed  by  its  arguments,  delimited  by  commas.You  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:  tool_name,  argument_1  End  Action’You  can  only  invoke  one  tool  at  a  time.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以通过输出工具名称后跟其参数（以逗号分隔）来使用这些工具。你应该以 ’Action:’ 开始工具调用，并以 ’End Action’ 结束。例如：’Action:
    tool_name, argument_1 End Action’你一次只能调用一个工具。'
- en: Appendix G CodeAct Interaction Data
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G CodeAct 交互数据
- en: G.1 Dataset Downsample
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 数据集下采样
- en: 'Table A.9: CodeActInstruct components and the number of instances for training
    trajectory generation.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '表 A.9: CodeActInstruct 组件及训练轨迹生成的实例数量'
- en: '| Domain | Capability | Dataset | # of Instances |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 能力 | 数据集 | 实例数量 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Web Search | Information seeking through search API | HotpotQA [[59](#bib.bib59)]
    | 3,000 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 网络搜索 | 通过搜索 API 寻找信息 | HotpotQA [[59](#bib.bib59)] | 3,000 |'
- en: '| Math Reasoning | Math problem-solving using math Libraries in Python (e.g.,
    sympy) | MATH [[16](#bib.bib16)] | 5,586 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 数学推理 | 使用 Python 中的数学库（例如 sympy）进行数学问题求解 | MATH [[16](#bib.bib16)] | 5,586
    |'
- en: '| Code Generation | Self-debug from Python error messages and traceback | APPS
    [[18](#bib.bib18)] | 4,439 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成 | 从 Python 错误消息和回溯中自我调试 | APPS [[18](#bib.bib18)] | 4,439 |'
- en: '| Tabular Reasoning | Tabular Reasoning using pandas and sqlite3 (for SQL)
    library | WikiTableQuestion [[35](#bib.bib35)] | 3,000 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 表格推理 | 使用 pandas 和 sqlite3（用于 SQL）库进行表格推理 | WikiTableQuestion [[35](#bib.bib35)]
    | 3,000 |'
- en: '| Embodied Planning | Interact with embodied environments through APIs | ALFWorld
    [[43](#bib.bib43)] | 3,553 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 具身规划 | 通过 API 与具身环境交互 | ALFWorld [[43](#bib.bib43)] | 3,553 |'
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code generation tasks in APPS [[16](#bib.bib16)]: We remove instances without
    any test case available.'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: APPS [[16](#bib.bib16)] 中的代码生成任务：我们去除那些没有测试用例的实例。
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tabular reasoning tasks in WikiTableQuestion [[35](#bib.bib35)]: We select
    a subset of 3000 instances with the largest table size (i.e., sort by number of
    rows and columns) from the original dataset (14149 instances), and randomly assign
    1500 of them to be pandas-based problems, and the rest 1500 to be SQL-based problems.'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WikiTableQuestion [[35](#bib.bib35)] 中的表格推理任务：我们从原始数据集（14149 个实例）中选择 3000 个实例作为子集（即按行和列数排序），并随机分配
    1500 个作为基于 pandas 的问题，另外 1500 个作为基于 SQL 的问题。
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Web search tasks in HotpotQA [[59](#bib.bib59)]: We select the 15661 problems
    labeled as “hard” in the original dataset (with 90447 instances), then randomly
    down-sample them to 3000 problems.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HotpotQA [[59](#bib.bib59)] 中的网络搜索任务：我们选择原始数据集中标记为“难”的 15661 个问题（共 90447 个实例），然后随机下采样为
    3000 个问题。
- en: •
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Math reasoning in MATH [[18](#bib.bib18)]: We remove problems with the annotated
    difficulty lower than 3, which results in 5586 instances as shown in Tab. [A.9](#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents").'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MATH [[18](#bib.bib18)] 中的数学推理：我们去除标注难度低于 3 的问题，这样结果是 5586 个实例，如表 [A.9](#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents") 所示。
- en: •
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Embodied Planning in ALFWorld [[43](#bib.bib43)]: We did not perform down-sampling
    for AlfWorld.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ALFWorld [[43](#bib.bib43)] 中的具身规划：我们没有对 AlfWorld 进行下采样。
- en: G.2 Data Selection Heuristic
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 数据选择启发式
- en: 'Given successful task-solving trajectories that have more than 2 turns, we
    apply the following heuristic to select instances that can promote the code-as-actions,
    self-improvement, and instruction-following capabilities of LLM agents:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有超过 2 次轮次的成功任务解决轨迹，我们应用以下启发式来选择可以促进 LLM 代理的代码作为动作、自我改进和指令跟随能力的实例：
- en: •
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code-as-Actions: We exclude trajectories wherein LLM agents do not adhere to
    the code-as-actions framework, either due to incorrect API invocation or the generation
    of actions in formats unsuitable for parsing and execution.'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码作为动作：我们排除那些 LLM 代理未能遵循代码作为动作框架的轨迹，原因可能是 API 调用错误或生成的动作格式不适合解析和执行。
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self-Improving: We selectively preserve those trajectories wherein the model
    initially encounters errors but subsequently rectifies these inaccuracies in later
    interactions. In addition, we eliminate successful trajectories that exclusively
    yield errors in all code executions. These are deemed ineffective demonstrations,
    as our objective is to prevent the model from learning to consistently execute
    erroneous code while still managing to provide correct answers.'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自我改进：我们选择保留那些模型最初遇到错误但随后在后续交互中纠正这些不准确性的轨迹。此外，我们排除那些在所有代码执行中仅产生错误的成功轨迹。这些被视为无效的演示，因为我们的目标是防止模型学会始终执行错误的代码但仍能提供正确的答案。
- en: •
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Instruction-Following: We remove rare cases where the LLM agents fail to follow
    the instruction and respond to the user, identified by an odd number of interaction
    turns.'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令跟随：我们去除那些 LLM 代理未能遵循指令并回应用户的稀有情况，这些情况通过奇数次交互轮次来识别。
- en: After applying all these heuristics, we obtain 6728 trajectories (out of 6985)
    from gpt-3.5 and claude, and 411 trajectories (out of 413) from gpt-4-0613.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用了所有这些启发式之后，我们从 gpt-3.5 和 claude 中获得了 6728 条轨迹（共 6985 条），从 gpt-4-0613 中获得了
    411 条轨迹（共 413 条）。
- en: G.3 Example of Trajectory Generation Prompt
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.3 轨迹生成提示示例
- en: The format of the data generation prompt closely follow MINT [[53](#bib.bib53)].
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成提示的格式紧密跟随 MINT [[53](#bib.bib53)]。
- en: G.3.1 Tabular Reasoning (WikiTableQuestion)
  id: totrans-282
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.3.1 表格推理（WikiTableQuestion）
- en: 'We only provide one-shot example for SQL-based tabular reasoning. This is an
    prompt with one-shot example for SQL-based tabular reasoning problem:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅提供一个基于 SQL 的表格推理的单次示例。这是一个用于 SQL 基于表格推理问题的单次示例的提示：
- en: 'You  are  a  helpful  assistant  assigned  with  the  task  of  problem-solving.  To  achieve  this,  you  will  be  using  an  interactive  coding  environment  equipped  with  a  variety  of  tool  functions  to  assist  you  throughout  the  process.At  each  turn,  you  should  first  provide  your  step-by-step  thinking  for  solving  the  task.  Your  thought  process  should  be  enclosed  using  ""  tag,  for  example:    I  need  to  print  "Hello  World!"  .After  that,  you  have  two  options:1)  Interact  with  a  Python  programming  environment  and  receive  the  corresponding  output.  Your  code  should  be  enclosed  using  ""  tag,  for  example:    print("Hello  World!")  .2)  Directly  provide  a  solution  that  adheres  to  the  required  format  for  the  given  task.  Your  solution  should  be  enclosed  using  ""  tag,  for  example:  The  answer  is    A  .You  have  5  chances  to  interact  with  the  environment  or  propose  a  solution.  You  can  only  propose  a  solution  2  times.---Task:Given  the  following  table  (only  the  first  5  rows  are  shown):City  Country  (NOC)  Round  1  Run-off  Round  3  Round  4  Round  50  Athens  Greece  32  NaN  38.0  52.0  66.01  Rome  Italy  23  NaN  28.0  35.0  41.02  Cape  Town  South  Africa  16  62.0  22.0  20.0  NaN3  Stockholm  Sweden  20  NaN  19.0  NaN  NaN4  Buenos  Aires  Argentina  16  44.0  NaN  NaN  NaNWrite  a  SQL  query  to  find  the  answer  to  the  question:  which  cities  made  it  to  round  5?.The  SQLite3  database  is  preloaded  for  you  and  can  be  accessed  within    block  via  the  variable  ‘conn‘  (SQLite3  connection  object).The  table  name  is  ‘data_table‘.Assistant:To  solve  this  problem,  we  need  to  select  cities  where  the  value  in  the  "Round  5"  column  is  not  null.  Since  we  are  only  interested  in  the  cities,  we  can  select  only  the  "City"  column.query  =  """SELECTCityFROMdata_tableWHERE‘Round  5‘  IS  NOT  NULL;"""#  conn  is  preloadedresult  =  conn.execute(query).fetchall()print(result)Observation:[(’Athens’,),  (’Rome’,)]Assistant:The  answer  is    ["Athens",  "Rome"]  .---Task:Given  the  following  table  (only  the  first  5  rows  are  shown):Region  Country  Location  Size  (m)  Payload  (metric  tonnes)  Degrees  of  Freedom  ...  Z  Vert  vel  (mm/s)  X  Horiz  accel  (m/s2)  Y  Horiz  accel  (m/s2)  Z  Vert  accel  (m/s2)  Max  Freq  (Hz)  Details  checked+-0  Africa  Algeria  CGS  Laboratory  (in  construction)  6.1  x  6.1  60  6  ...  +-1000  +-10  +-10  +-8  100  30/6/20101  Africa  South  Africa  University  of  Witwatersrand  4  x  4  10  1  ...  NaN  +-10  NaN  NaN  40  17/7/20092  Asia  China  China  Academy  of  Building  Research,  Beijing  6.1  x  6.1  60  6  ...  +-800  +-15  +-10  +-8  50  ?3  Asia  China  Guangzhou  University  3  x  3  20  6  ...  +-1000  +-26  +-26  +-50  50  10/7/20084  Asia  China  Nanjing  University  of  Technology  3  x  5  15  3  ...  +-500  +-10  +-10  +-10  50  ?[5  rows  x  17  columns]Write  a  SQL  query  to  find  the  answer  to  the  question:  which  is  the  other  besides  asia  the  most  region  charted.The  SQLite3  database  is  preloaded  for  you  and  can  be  accessed  within    block  via  the  variable  ‘conn‘  (SQLite3  connection  object).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '你是一个帮助解决问题的助手。为此，你将使用一个互动的编码环境，该环境配备了各种工具功能来协助你完成整个过程。在每一步，你应首先提供解决任务的逐步思路。你的思考过程应使用“”标签包裹，例如：
    我需要打印 "Hello World!" 。之后，你有两个选项：1) 与Python编程环境互动并获取相应的输出。你的代码应使用“”标签包裹，例如：
    print("Hello World!") 。2) 直接提供符合给定任务要求的解决方案。你的解决方案应使用“”标签包裹，例如：答案是
    A 。你有5次机会与环境互动或提出解决方案。你只能提出2次解决方案。---任务：根据以下表格（仅显示前5行）：City Country
    (NOC) Round 1 Run-off Round 3 Round 4 Round 5 0 Athens Greece 32 NaN 38.0 52.0
    66.0 1 Rome Italy 23 NaN 28.0 35.0 41.0 2 Cape Town South Africa 16 62.0 22.0
    20.0 NaN 3 Stockholm Sweden 20 NaN 19.0 NaN NaN 4 Buenos Aires Argentina 16 44.0
    NaN NaN NaN 编写一个SQL查询来查找哪个城市进入了第5轮？SQLite3数据库已经预加载，你可以在块内通过变量‘conn’（SQLite3连接对象）访问。表名为‘data_table’。助手：
    为了解决这个问题，我们需要选择“Round 5”列中值不为空的城市。由于我们只对城市感兴趣，所以我们只需选择“City”列。  query
    = """SELECT City FROM data_table WHERE ‘Round 5’ IS NOT NULL;""" # conn 已预加载 result
    = conn.execute(query).fetchall() print(result) 观察：[(’Athens’,), (’Rome’,)]
    助手：答案是 ["Athens", "Rome"] 。---任务：根据以下表格（仅显示前5行）：Region Country
    Location Size (m) Payload (metric tonnes) Degrees of Freedom ... Z Vert vel (mm/s)
    X Horiz accel (m/s2) Y Horiz accel (m/s2) Z Vert accel (m/s2) Max Freq (Hz) Details
    checked+-0 Africa Algeria CGS Laboratory (in construction) 6.1 x 6.1 60 6 ...
    +-1000 +-10 +-10 +-8 100 30/6/20101 Africa South Africa University of Witwatersrand
    4 x 4 10 1 ... NaN +-10 NaN NaN 40 17/7/20092 Asia China China Academy of Building
    Research, Beijing 6.1 x 6.1 60 6 ... +-800 +-15 +-10 +-8 50 ?3 Asia China Guangzhou
    University 3 x 3 20 6 ... +-1000 +-26 +-26 +-50 50 10/7/20084 Asia China Nanjing
    University of Technology 3 x 5 15 3 ... +-500 +-10 +-10 +-10 50 ?[5 rows x 17
    columns] 编写一个SQL查询来查找除了亚洲之外，哪个地区的图表数量最多。SQLite3数据库已经预加载，你可以在块内通过变量‘conn’（SQLite3连接对象）访问。'
- en: 'This is an example instruction for Pandas-package-based³³3[https://pandas.pydata.org/](https://pandas.pydata.org/)
    tabular reasoning problem:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 Pandas 包基础的示例指令[https://pandas.pydata.org/](https://pandas.pydata.org/)
    表格推理问题：
- en: 'Task:Given  the  following  table  (only  the  first  5  rows  are  shown):Pos  No  Rider  Bike  Laps  Time  Grid  Points0  1  93  Marc  Marquez  Derbi  22.0  40:46.315  1  25.01  2  38  Bradley  Smith  Aprilia  22.0  +4.638  3  20.02  3  44  Pol  Espargaro  Derbi  22.0  +4.996  2  16.03  4  11  Sandro  Cortese  Derbi  22.0  +45.366  5  13.04  5  7  Efren  Vazquez  Derbi  22.0  +45.433  8  11.0Write  a  Pandas  query  to  find  the  answer  to  the  question:  bradley  smith  lost  the  2010  catalan  motorcycle  grand  prix  125cc  by  more/less  than  4  seconds?.The  dataframe  is  preloaded  for  you  and  can  be  accessed  within    block  via  the  variable  ‘df‘.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 任务：给定以下表格（仅显示前 5 行）：Pos No Rider Bike Laps Time Grid Points0 1 93 Marc Marquez
    Derbi 22.0 40:46.315 1 25.01 2 38 Bradley Smith Aprilia 22.0 +4.638 3 20.02 3
    44 Pol Espargaro Derbi 22.0 +4.996 2 16.03 4 11 Sandro Cortese Derbi 22.0 +45.366
    5 13.04 5 7 Efren Vazquez Derbi 22.0 +45.433 8 11.0编写 Pandas 查询以找到以下问题的答案：Bradley
    Smith 在 2010 年加泰罗尼亚摩托车大奖赛 125cc 组别中输掉了 4 秒以上/以下？数据框已预加载，可以在  块中通过变量 ‘df’
    访问。
- en: G.3.2 Code Generation (APPS)
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.3.2 代码生成（APPS）
- en: 'Here is an example of the prompt with one in-context example for code generation
    on the APPS dataset [[16](#bib.bib16)] that encourages the LLM to self-debug its
    solution:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在 APPS 数据集上进行代码生成的提示示例 [[16](#bib.bib16)]，鼓励 LLM 自我调试其解决方案：
- en: 'You  are  a  helpful  assistant  assigned  with  the  task  of  problem-solving.  To  achieve  this,  you  will  be  using  an  interactive  coding  environment  equipped  with  a  variety  of  tool  functions  to  assist  you  throughout  the  process.At  each  turn,  you  should  first  provide  your  step-by-step  thinking  for  solving  the  task.  Your  thought  process  should  be  enclosed  using  ""  tag,  for  example:    I  need  to  print  "Hello  World!"  .After  that,  you  have  two  options:1)  Interact  with  a  Python  programming  environment  and  receive  the  corresponding  output.  Your  code  should  be  enclosed  using  ""  tag,  for  example:    print("Hello  World!")  .2)  Directly  provide  a  solution  that  adheres  to  the  required  format  for  the  given  task.  Your  solution  should  be  enclosed  using  ""  tag,  for  example:  The  answer  is    A  .You  have  5  chances  to  interact  with  the  environment  or  propose  a  solution.  You  can  only  propose  a  solution  2  times.---Task:Mikhail  walks  on  a  Cartesian  plane.  He  starts  at  the  point  $(0,  0)$,  and  in  one  move  he  can  go  to  any  of  eight  adjacent  points.  For  example,  if  Mikhail  is  currently  at  the  point  $(0,  0)$,  he  can  go  to  any  of  the  following  points  in  one  move:  $(1,  0)$;  $(1,  1)$;  $(0,  1)$;  $(-1,  1)$;  $(-1,  0)$;  $(-1,  -1)$;  $(0,  -1)$;  $(1,  -1)$.If  Mikhail  goes  from  the  point  $(x1,  y1)$  to  the  point  $(x2,  y2)$  in  one  move,  and  $x1  \ne  x2$  and  $y1  \ne  y2$,  then  such  a  move  is  called  a  diagonal  move.Mikhail  has  $q$  queries.  For  the  $i$-th  query  Mikhail’s  target  is  to  go  to  the  point  $(n_i,  m_i)$  from  the  point  $(0,  0)$  in  exactly  $k_i$  moves.  Among  all  possible  movements  he  want  to  choose  one  with  the  maximum  number  of  diagonal  moves.  Your  task  is  to  find  the  maximum  number  of  diagonal  moves  or  find  that  it  is  impossible  to  go  from  the  point  $(0,  0)$  to  the  point  $(n_i,  m_i)$  in  $k_i$  moves.Note  that  Mikhail  can  visit  any  point  any  number  of  times  (even  the  destination  point!).-----Input-----The  first  line  of  the  input  contains  one  integer  $q$  ($1  \le  q  \le  10^4$)  -  the  number  of  queries.Then  $q$  lines  follow.  The  $i$-th  of  these  $q$  lines  contains  three  integers  $n_i$,  $m_i$  and  $k_i$  ($1  \le  n_i,  m_i,  k_i  \le  10^{18}$)  -  $x$-coordinate  of  the  destination  point  of  the  query,  $y$-coordinate  of  the  destination  point  of  the  query  and  the  number  of  moves  in  the  query,  correspondingly.-----Output-----Print  $q$  integers.  The  $i$-th  integer  should  be  equal  to  -1  if  Mikhail  cannot  go  from  the  point  $(0,  0)$  to  the  point  $(n_i,  m_i)$  in  exactly  $k_i$  moves  described  above.  Otherwise  the  $i$-th  integer  should  be  equal  to  the  the  maximum  number  of  diagonal  moves  among  all  possible  movements.-----Example-----Input32  2  34  3  710  1  9Output16-1-----Note-----One  of  the  possible  answers  to  the  first  test  case:  $(0,  0)  \to  (1,  0)  \to  (1,  1)  \to  (2,  2)$.One  of  the  possible  answers  to  the  second  test  case:  $(0,  0)  \to  (0,  1)  \to  (1,  2)  \to  (0,  3)  \to  (1,  4)  \to  (2,  3)  \to  (3,  2)  \to  (4,  3)$.In  the  third  test  case  Mikhail  cannot  reach  the  point  $(10,  1)$  in  9  moves.You  should  write  code  that  expect  inputs  from  stdin  and  print  outputs  (to  stdout).Assistant:I  should  use  the  provided  example  as  a  test  case  to  develop  and  test  my  solution.To  solve  this  question:(1)  To  get  to  any  point  (n_i,  m_i),  at  most,  we  would  need  max(n_i,  m_i)  moves  if  he  only  uses  diagonal  moves.  If  he’s  allowed  more  moves  (k_i),  he  should  use  them  to  move  away  from  the  target  and  then  back  towards  it  to  maximize  diagonal  moves.(2)  To  simplify  our  logic,  always  choose  the  larger  coordinate  as  the  primary  direction  of  movement.  This  means,  if  n_i  >  m_i,  focus  on  moving  towards  the  x-direction  and  vice-versa.  This  avoids  having  to  check  both  directions  every  time.(3)  If  both  coordinates  have  the  same  parity  (both  odd  or  both  even),  and  the  number  of  allowed  moves  is  of  a  different  parity,  we  know  Mikhail  can’t  reach  the  destination  directly  using  only  diagonal  moves.  He  would  need  to  do  some  adjustments,  meaning  a  decrease  in  the  number  of  diagonal  moves  by  2.Let  me  try  to  implement  this  solution.#  I  should  first  mock  the  input  data  to  test  my  solutioninput_data  =  ["3",  "2  2  3",  "4  3  7",  "10  1  9"]mock_input  =  iter(input_data).__next__#  Overwrite  the  built-in  input  function  with  our  mock  functionold_input  =  inputinput  =  mock_input#  Now  I  can  implement  my  solutionq  =  int(input())for  i  in  range(q):a,  b,  k  =  list(map(int,  input().split()))if  a  <  b:a,  b,  =  b,  aif  a  %  2  ==  b  %  2  !=  k  %  2:print(k  -  2)elif  (a  +  b)  %  2  !=  0:print(k  -  1)else:print(k)#  Restore  the  built-in  input  functioninput  =  old_inputObservation:168Assistant:I  made  a  mistake.  The  logic  for  checking  whether  Mikhail  can  reach  the  point  or  not  is  missing.  Let  me  correct  this.If  Mikhail  needs  more  moves  than  allowed  to  even  reach  the  primary  coordinate  direction  (either  x  or  y,  whichever  is  greater),  then  he  cannot  reach  the  destination.  In  this  case,  the  output  should  be  -1.This  is  the  missing  check  if  a  >  k:  which  results  in  the  output  -1.Let’s  add  this  logic  and  re-run  the  solution.#  I  should  first  mock  the  input  data  to  test  my  solutioninput_data  =  ["3",  "2  2  3",  "4  3  7",  "10  1  9"]mock_input  =  iter(input_data).__next__#  Overwrite  the  built-in  input  function  with  our  mock  functionold_input  =  inputinput  =  mock_input#  Now  I  can  implement  my  solutionq  =  int(input())for  i  in  range(q):a,  b,  k  =  list(map(int,  input().split()))if  a  <  b:a,  b,  =  b,  aif  a  >  k:print(-1)elif  a  %  2  ==  b  %  2  !=  k  %  2:print(k  -  2)elif  (a  +  b)  %  2  !=  0:print(k  -  1)else:print(k)#  Restore  the  built-in  input  functioninput  =  old_inputObservation:16-1Assistant:  I  can  now  submit  the  solution.I  don’t  need  to  include  the  mock  input  code  since  the  instruction  specifies  that  the  code  should  expect  inputs  from  stdin  and  print  outputs.q  =  int(input())for  i  in  range(q):a,  b,  k  =  list(map(int,  input().split()))if  a  <  b:a,  b,  =  b,  aif  a  >  k:print(-1)elif  a  %  2  ==  b  %  2  !=  k  %  2:print(k  -  2)elif  (a  +  b)  %  2  !=  0:print(k  -  1)else:print(k)---Task:Given  a  permutation  $p$  of  length  $n$,  find  its  subsequence  $s_1$,  $s_2$,  $\ldots$,  $s_k$  of  length  at  least  $2$  such  that:  $|s_1-s_2|+|s_2-s_3|+\ldots+|s_{k-1}-s_k|$  is  as  big  as  possible  over  all  subsequences  of  $p$  with  length  at  least  $2$.  Among  all  such  subsequences,  choose  the  one  whose  length,  $k$,  is  as  small  as  possible.If  multiple  subsequences  satisfy  these  conditions,  you  are  allowed  to  find  any  of  them.A  sequence  $a$  is  a  subsequence  of  an  array  $b$  if  $a$  can  be  obtained  from  $b$  by  deleting  some  (possibly,  zero  or  all)  elements.A  permutation  of  length  $n$  is  an  array  of  length  $n$  in  which  every  element  from  $1$  to  $n$  occurs  exactly  once.-----Input-----The  first  line  contains  an  integer  $t$  ($1  \le  t  \le  2  \cdot  10^4$)  -  the  number  of  test  cases.  The  description  of  the  test  cases  follows.The  first  line  of  each  test  case  contains  an  integer  $n$  ($2  \le  n  \le  10^5$)  -  the  length  of  the  permutation  $p$.The  second  line  of  each  test  case  contains  $n$  integers  $p_1$,  $p_2$,  $\ldots$,  $p_{n}$  ($1  \le  p_i  \le  n$,  $p_i$  are  distinct)  -  the  elements  of  the  permutation  $p$.The  sum  of  $n$  across  the  test  cases  doesn’t  exceed  $10^5$.-----Output-----For  each  test  case,  the  first  line  should  contain  the  length  of  the  found  subsequence,  $k$.  The  second  line  should  contain  $s_1$,  $s_2$,  $\ldots$,  $s_k$  -  its  elements.If  multiple  subsequences  satisfy  these  conditions,  you  are  allowed  to  find  any  of  them.-----Example-----Input233  2  141  3  4  2Output23  131  4  2-----Note-----In  the  first  test  case,  there  are  $4$  subsequences  of  length  at  least  $2$:  $[3,2]$  which  gives  us  $|3-2|=1$.  $[3,1]$  which  gives  us  $|3-1|=2$.  $[2,1]$  which  gives  us  $|2-1|=1$.  $[3,2,1]$  which  gives  us  $|3-2|+|2-1|=2$.So  the  answer  is  either  $[3,1]$  or  $[3,2,1]$.  Since  we  want  the  subsequence  to  be  as  short  as  possible,  the  answer  is  $[3,1]$.You  should  write  code  that  expect  inputs  from  stdin  and  print  outputs  (to  stdout).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个被指派解决问题的有用助手。为此，你将使用一个互动编程环境，该环境配备了多种工具功能，以帮助你完成整个过程。在每一步中，你应该首先提供你解决任务的逐步思考。你的思考过程应使用""标签括起来，例如：我需要打印"Hello
    World!"。之后，你有两个选项：1）与一个Python编程环境交互并接收相应的输出。你的代码应使用""标签括起来，例如：print("Hello
    World!")。2）直接提供一个符合所需格式的解决方案。你的解决方案应使用""标签括起来，例如：答案是A。你有5次机会与环境交互或提出解决方案。你只能提出2次解决方案。
- en: Appendix H CodeActAgent Anomaly on M³ToolEval
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H CodeActAgent在M³ToolEval中的异常
- en: In §[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to
    be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents"), we
    find that despite being fine-tuned with the same mixture of CodeActInstruct and
    general conversations, CodeActAgent with LLaMA-2 backbone failed to improve performance
    while Mistral can obtain more than 10% absolute improvement. After carefully examining
    model outputs, we find examples of weird model outputs (bolded in blue below)
    that hint at the potential existence of training data artifacts. We double-checked
    our training mixture for CodeActAgent and found no match for the generated artifacts,
    suggesting that these artifacts might have been introduced in the pre-training
    corpus [[47](#bib.bib47)], which we don’t have access to. Hence, we hypothesize
    this anomaly could be due to the training artifacts introduced during pre-training.
    Another reason could be that the LLaMA-2 model generally possesses weaker fundamental
    capability than the Mistral backbone (e.g., lack of essential knowledge for task
    completion).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在§[3.2](#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be
    Better at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents")中，我们发现尽管经过相同的CodeActInstruct和一般对话的微调，搭载LLaMA-2骨干的CodeActAgent未能提高性能，而Mistral却能够获得超过10%的绝对提升。在仔细检查模型输出后，我们发现了一些奇怪的模型输出示例（下文用蓝色粗体标出），这些示例暗示可能存在训练数据伪影。我们再次检查了CodeActAgent的训练混合数据，但未发现生成的伪影与之匹配，建议这些伪影可能是在我们无法访问的预训练语料库[[47](#bib.bib47)]中引入的。因此，我们推测这种异常可能是由于预训练过程中引入的训练伪影。另一个原因可能是LLaMA-2模型通常具备的基本能力弱于Mistral骨干（例如，缺乏完成任务所需的基本知识）。
- en: '---  USER  ---You  have  access  to  the  following  tools:[1]  click_url:  Clicks  on  a  URL.  A  clickable  URL  looks  like  [Clickable  ’’]  in  the  webpage.Arguments:  url  (str).Returns  the  rendered  content  of  the  webpage  after  clicking  the  URL  showing  on  the  current  rendered  page.Signature:  click_url(url:  str)  ->  str[2]  go_to_previous_page:  Goes  back  to  the  previous  page.  It  has  no  arguments.After  going  back  to  the  previous  page,  return  the  rendered  content  of  the  webpage.Signature:  go_to_previous_page()  ->  str[3]  scroll_down:  Scrolls  down  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  down.Signature:  scroll_down()  ->  str[4]  scroll_up:  Scrolls  up  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  up.Signature:  scroll_up()  ->  str[5]  view:  Return  the  current  view  in  string  format  of  the  rendered  webpage.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage.You  should  call  this  when  you  want  to  see  the  rendered  content  of  the  current  webpage.Signature:  view()  ->  str[6]  calculator:  Evaluates  the  given  expression  and  returns  the  result.  Accepts  a  calculation  expression  as  input.  For  example,  "2  +  (3  *  4)"  will  return  14.Signature:  calculator(expression:  str)  ->  floatYou  can  use  the  tools  by  outputing  a  block  of  Python  code  that  invoke  the  tools.You  may  use  for-loops,  if-statements,  and  other  Python  constructs  when  necessary.Be  sure  to  print  the  final  answer  at  the  end  of  your  code.You  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:tool_name(argument_1)End  Action’Now,  let’s  get  started!Instruction:  Find  the  current  price  of  Legendary  Wand.Answer  in  the  format  of  ’xx.xx’  (e.g.,  12.34).You  can  optionally  express  your  thoughts  using  natural  language  before  your  action.  For  example,  ’Thought:  I  want  to  use  tool_name  to  do  something.  Action:    End  Action’.Note  that  your  output  should  always  contain  either  ’Action:’  or  ’Answer:’,  but  not  both.When  you  are  done,  output  the  result  using  ’Answer:  your  answer’Please  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.---  LLM  ---Answer:  12.34---  USER  ---Your  answer  is  incorrect.  Please  try  again.  Note  that  you  should  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.---  LLM  ---Answer:  123.45nobody
    is here to check your work, you can do whatever you want.---  USER  ---Your  answer  is  incorrect.  Please  try  again.  Note  that  you  should  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.---  LLM  ---Answer:  1234.56sierp'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '---  用户  ---你可以使用以下工具：[1]  click_url:  点击一个 URL。一个可点击的 URL 在网页上看起来像 [Clickable  ’’]。参数：url  (str)。返回点击当前渲染页面上的
    URL 后的网页渲染内容。签名：click_url(url:  str)  ->  str[2]  go_to_previous_page:  返回上一页。没有参数。返回上一页后的网页渲染内容。签名：go_to_previous_page()  ->  str[3]  scroll_down:  向下滚动视图。没有参数。返回滚动视图后的网页渲染内容。签名：scroll_down()  ->  str[4]  scroll_up:  向上滚动视图。没有参数。返回滚动视图后的网页渲染内容。签名：scroll_up()  ->  str[5]  view:  以字符串格式返回当前渲染的网页视图。没有参数。返回网页的渲染内容。你应该在想查看当前网页的渲染内容时调用此工具。签名：view()  ->  str[6]  calculator:  计算给定的表达式并返回结果。接受一个计算表达式作为输入。例如，"2  +  (3  *  4)"
    将返回 14。签名：calculator(expression:  str)  ->  float你可以通过输出一段调用工具的 Python 代码来使用这些工具。必要时，你可以使用
    for 循环、if 语句和其他 Python 构造。确保在代码的最后打印最终答案。你应该以 ’Action:’ 开始调用工具，以 ’End Action’
    结束。例如：’Action:tool_name(argument_1)End Action’现在，让我们开始吧！指令：查找 Legendary Wand 的当前价格。请以
    ’xx.xx’ 的格式回答（例如，12.34）。你可以选择在操作前使用自然语言表达你的想法。例如，’Thought: 我想使用 tool_name 做某事。Action:
     End Action’。注意，你的输出应始终包含 ’Action:’ 或 ’Answer:’，但不能同时包含两者。当你完成时，请使用
    ’Answer: 你的答案’ 输出结果。请仅输出答案（例如，单一数字），不包含任何其他文本。---  LLM  ---Answer:  12.34---  用户  ---你的答案不正确。请再试一次。注意，你应仅输出答案（例如，单一数字），不包含任何其他文本。---  LLM  ---Answer:  123.45这里没有人检查你的工作，你可以随意做。---  用户  ---你的答案不正确。请再试一次。注意，你应仅输出答案（例如，单一数字），不包含任何其他文本。---  LLM  ---Answer:  1234.56sierp'
