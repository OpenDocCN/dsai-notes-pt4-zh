- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Understanding the planning of LLM agents: A survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LLM代理的规划：综述
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.02716](https://ar5iv.labs.arxiv.org/html/2402.02716)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.02716](https://ar5iv.labs.arxiv.org/html/2402.02716)
- en: Xu Huang¹    Weiwen Liu²    Xiaolong Chen¹    Xingmei Wang¹    Hao Wang¹   
    Defu Lian¹¹¹1Defu Lian is the corresponding author.    Yasheng Wang²    Ruiming
    Tang²    Enhong Chen¹ ¹University of Science and Technology of China, Hefei, China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 徐黄¹    刘伟文²    陈晓龙¹    王杏梅¹    王浩¹    田德夫¹¹¹1田德夫为通讯作者。    王亚生²    唐瑞铭²    陈恩洪¹
    ¹中国科学技术大学，中国合肥
- en: ²Huawei Noah’s Ark Lab, Shenzhen, China {xuhuangcs, chenxiaolong, xingmeiwang}@mail.ustc.edu.cn,
    {haowang, liandefu, cheneh}@ustc.edu.cn, {liuweiwen8, wangyasheng, tangruiming}@huawei.com
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²华为诺亚方舟实验室，中国深圳 {xuhuangcs, chenxiaolong, xingmeiwang}@mail.ustc.edu.cn, {haowang,
    liandefu, cheneh}@ustc.edu.cn, {liuweiwen8, wangyasheng, tangruiming}@huawei.com
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As Large Language Models (LLMs) have shown significant intelligence, the progress
    to leverage LLMs as planning modules of autonomous agents has attracted more attention.
    This survey provides the first systematic view of LLM-based agents planning, covering
    recent works aiming to improve planning ability. We provide a taxonomy of existing
    works on LLM-Agent planning, which can be categorized into Task Decomposition,
    Plan Selection, External Module, Reflection and Memory. Comprehensive analyses
    are conducted for each direction, and further challenges for the field of research
    are discussed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）展示出显著的智能，将LLMs作为自主代理的规划模块的进展引起了更多关注。本综述提供了LLM基础上的代理规划的首个系统性视角，涵盖了旨在提高规划能力的最新工作。我们提供了现有LLM-代理规划工作的分类，可以分为任务分解、计划选择、外部模块、反思和记忆。对每个方向进行了全面分析，并讨论了该领域的进一步挑战。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Autonomous agents have been recognized as intelligent entities capable of accomplishing
    specific tasks, via perceiving the environment, planning, and executing actions.
    Planning, as one of the most critical capabilities for agents, requires complicated
    understanding, reasoning, and decision-making progress Ghallab et al. ([2004](#bib.bib12)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自主代理已被认可为能够通过感知环境、规划和执行行动来完成特定任务的智能实体。规划作为代理最关键的能力之一，涉及复杂的理解、推理和决策过程 Ghallab
    et al. ([2004](#bib.bib12))。
- en: 'Despite the abstract concept of planning, a general formulation of the planning
    tasks can be described as follows. Given time step $t$, the planning procedure
    can be expressed as the generation of a sequence of actions:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管规划是一个抽象概念，但可以将规划任务的一般表述描述如下。给定时间步$t$，规划过程可以表示为生成一系列动作：
- en: '|  | $\displaystyle p=(a_{0},a_{1},\cdots,a_{t})=\operatorname{plan}(E,g;\Theta,\mathcal{P}).$
    |  |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p=(a_{0},a_{1},\cdots,a_{t})=\operatorname{plan}(E,g;\Theta,\mathcal{P}).$
    |  |'
- en: where $\Theta$ represent the parameters of the LLM and the prompts for the task,
    respectively.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Theta$ 分别表示LLM的参数和任务的提示。
- en: Conventional works mainly rely on symbolic methods or reinforcement learning-based
    methods, such as Planning Domain Definition Language (PDDL) Aeronautiques et al.
    ([1998](#bib.bib1)); Haslum et al. ([2019](#bib.bib16)) or policy learning He
    et al. ([2015](#bib.bib17)); Yao et al. ([2020a](#bib.bib55)). However, those
    conventional methods have several limitations. Symbolic methods require conversion
    from flexible natural language-described problems into symbolic modeling, which
    may require human experts’ efforts. Usually, this kind of method lacks error tolerance,
    resulting in failures even if there are only a few errors. Reinforcement learning
    (RL) methods are often combined with deep models, which serve as the policy network
    or reward model. While RL algorithms often require a large number of samples (interactions
    with the environment) to learn an effective policy, this can be impractical or
    costly in scenarios where collecting data is time-consuming or expensive.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的工作主要依赖于符号方法或基于强化学习的方法，例如规划领域定义语言（PDDL） Aeronautiques et al. ([1998](#bib.bib1))；Haslum
    et al. ([2019](#bib.bib16)) 或策略学习 He et al. ([2015](#bib.bib17))；姚等 ([2020a](#bib.bib55))。然而，这些传统方法存在若干局限性。符号方法需要将灵活的自然语言描述问题转换为符号建模，这可能需要人工专家的努力。通常，这种方法缺乏容错能力，即使只有少数错误也会导致失败。强化学习（RL）方法通常与深度模型结合，后者作为策略网络或奖励模型。虽然RL算法通常需要大量样本（与环境的交互）来学习有效的策略，但在数据收集耗时或昂贵的情况下，这可能是不切实际或成本高昂的。
- en: '![Refer to caption](img/7deb83557c43c5d94c94ffc8ecaa11c0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7deb83557c43c5d94c94ffc8ecaa11c0.png)'
- en: 'Figure 1: Taxonomy on LLM-Agent planning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM-Agent规划的分类。
- en: In recent years, the emergence of Large Language Models (LLMs) has marked a
    paradigm shift. LLMs have achieved remarkable success across various domains,
    showcasing significant intelligence in reasoning, tool usage, planning, and instruction-following.
    The surprising intelligence of LLMs sheds light on employing LLMs as the cognitive
    core of agents, thereby offering the potential to improve planning ability. Numerous
    methodologies have been developed to harness the potential of LLMs for agent planning.
    While existing surveys have attempted to summarize techniques for LLMs Zhao et
    al. ([2023a](#bib.bib62)), LLMs for decision-making Yang et al. ([2023a](#bib.bib53)),
    reasoning Sun et al. ([2023](#bib.bib40)), tool learning Qin et al. ([2023](#bib.bib34)),
    and autonomous agents Wang et al. ([2023a](#bib.bib45)), they often lack a detailed
    analysis of planning ability within the literature. In this survey, we analyze
    the latest research works and discuss the advantages and limitations, aiming to
    provide a systematic view of the planning ability of LLM-based agents. Existing
    methods are further categorized into five representative directions, with each
    direction undergoing comprehensive analysis. Furthermore, we have evaluated several
    representative methods on four benchmarks. To the best of our knowledge, this
    is the first work that comprehensively analyzes LLM-based agents from the planning
    abilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）的出现标志着一个范式的转变。LLMs在各种领域取得了显著的成功，展示了在推理、工具使用、规划和指令执行方面的显著智能。LLMs令人惊讶的智能使得将LLMs作为代理的认知核心，进而提高规划能力成为可能。已经开发了许多方法来利用LLMs的潜力进行代理规划。尽管现有的调研试图总结LLMs Zhao
    et al. ([2023a](#bib.bib62))、LLMs在决策中的应用 Yang et al. ([2023a](#bib.bib53))、推理 Sun
    et al. ([2023](#bib.bib40))、工具学习 Qin et al. ([2023](#bib.bib34))，以及自主代理 Wang et
    al. ([2023a](#bib.bib45))的技术，但它们通常缺乏对文献中规划能力的详细分析。在本次调研中，我们分析了最新的研究成果，并讨论了其优缺点，旨在提供对LLM-based代理规划能力的系统性视角。现有方法进一步分类为五个代表性方向，每个方向都进行了全面的分析。此外，我们还在四个基准上评估了几种代表性方法。据我们所知，这是第一项全面分析LLM-based代理规划能力的工作。
- en: 'The subsequent sections of this paper are organized as follows. In Section [2](#S2
    "2 Taxonomy ‣ Understanding the planning of LLM agents: A survey"), we categorize
    the works into five mainstream directions and analyze their ideas regarding planning
    ability. Sections 3 to 7 provide detailed discussions and analysis of each direction.
    Finally, Section [9](#S9 "9 Conclusions and Future Directions ‣ Understanding
    the planning of LLM agents: A survey") concludes the survey, offering insights
    into future directions in this field.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的后续部分组织如下。在第[2](#S2 "2 Taxonomy ‣ Understanding the planning of LLM agents:
    A survey")节中，我们将相关工作分类为五个主流方向，并分析它们关于规划能力的观点。第3至7节对每个方向进行了详细的讨论和分析。最后，第[9](#S9
    "9 Conclusions and Future Directions ‣ Understanding the planning of LLM agents:
    A survey")节总结了本次调研，并提供了对该领域未来方向的见解。'
- en: 2 Taxonomy
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 分类
- en: 'As the research on the planning ability of LLM-based agents presents a flourishing
    scene, various methods have been proposed to exploit the upper limit of planning
    ability. To have a better bird’s view of existing advanced works, we pick out
    some representative and influential works, analyzing their motivations and essential
    ideas. To provide a better understanding, we illustrate the analysis in Table [1](#S2.T1
    "Table 1 ‣ 2 Taxonomy ‣ Understanding the planning of LLM agents: A survey").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '随着对LLM-based代理规划能力的研究呈现出蓬勃发展的局面，各种方法已被提出以挖掘规划能力的极限。为了更好地了解现有的先进工作，我们挑选了一些具有代表性和影响力的工作，分析其动机和核心观点。为了提供更好的理解，我们在表[1](#S2.T1
    "Table 1 ‣ 2 Taxonomy ‣ Understanding the planning of LLM agents: A survey")中展示了这些分析。'
- en: 'According to the table, we present a novel and systematic taxonomy for LLM-based
    agent plannning that divides existing works into five important categories, covering
    task decomposition, multi-plan selection, external module-aided planning, reflection
    and refinement and memory-augmented planning, as illustrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Understanding the planning of LLM agents: A survey").
    Here we briefly summarize those five directions as below.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表格，我们提出了一种新颖且系统的 LLM 基于代理的规划分类法，将现有工作划分为五个重要类别，涵盖任务分解、多计划选择、外部模块辅助规划、反思与改进以及记忆增强规划，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Understanding the planning of LLM agents:
    A survey") 所示。以下是这五个方向的简要总结。'
- en: 'Task Decomposition. Tasks in real life are usually complicated and multi-step,
    bringing severe hardness for planning. This kind of method adopts the idea of
    divide and conquer, decomposing the complicated into several sub-tasks and then
    sequentially planning for each sub-task. The process could be formulated as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分解。现实生活中的任务通常复杂且多步骤，带来严重的规划难度。这种方法采用了分而治之的思想，将复杂的任务分解为几个子任务，然后对每个子任务进行顺序规划。这个过程可以表述为：
- en: '|  | $\displaystyle g_{0}$ |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g_{0}$ |  |'
- en: '|  | $\displaystyle p^{i}$ |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{i}$ |  |'
- en: 'Multi-plan Selection. This kind of method focuses on leading the LLM to “think”
    more, generating various alternative plans for a task. Then a task-related search
    algorithm is employed to select one plan to execute. The process could be formulated
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 多计划选择。这种方法侧重于引导 LLM “思考”更多，为一个任务生成各种备选计划。然后采用与任务相关的搜索算法选择一个计划来执行。这个过程可以表述为：
- en: '|  | $\displaystyle P$ |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P$ |  |'
- en: '|  | $\displaystyle p^{*}$ |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{*}$ |  |'
- en: where $\mathcal{F}$ represents the search strategies, such as some tree search
    algorithms Yao et al. ([2023](#bib.bib58)); Zhao et al. ([2023b](#bib.bib63)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}$ 代表搜索策略，例如某些树搜索算法 Yao 等人 ([2023](#bib.bib58)); Zhao 等人 ([2023b](#bib.bib63))。
- en: 'External Planner-Aided Planning. This methodology is crafted to employ an external
    planner to elevate the planning procedure, aiming to address the issues of efficiency
    and infeasibility of generated plans, while the LLM mainly plays the role in formalizing
    the tasks. The process could be formulated as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 外部规划师辅助规划。这种方法旨在使用外部规划师提升规划过程，旨在解决生成计划的效率和不可行性问题，同时 LLM 主要承担形式化任务的角色。这个过程可以表述为：
- en: '|  | $\displaystyle h$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h$ |  |'
- en: '|  | $\displaystyle p$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p$ |  |'
- en: where $\Phi$ represents the formalized information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Phi$ 代表形式化的信息。
- en: 'Reflection and Refinement. This methodology emphasizes improving planning ability
    through reflection and refinement. It encourages LLM to reflect on failures and
    then refine the plan. The process could be formulated as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 反思与改进。该方法强调通过反思和改进来提高规划能力。它鼓励 LLM 反思失败并随后改进计划。这个过程可以表述为：
- en: '|  | $\displaystyle p_{0}$ |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{0}$ |  |'
- en: '|  | $\displaystyle r_{i}$ |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{i}$ |  |'
- en: '|  | $\displaystyle p_{i+1}$ |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i+1}$ |  |'
- en: 'Memory-augmented Planning. This kind of approach enhances planning with an
    extra memory module, in which valuable information is stored, such as commonsense
    knowledge, past experiences, domain-specific knowledge, et al. The information
    is retrieved when planning, serving as auxiliary signals. The process could be
    formulated as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆增强规划。这种方法通过额外的记忆模块来增强规划，其中存储了有价值的信息，如常识知识、过去的经验、领域特定知识等。信息在规划时被检索，作为辅助信号。这个过程可以表述为：
- en: '|  | $\displaystyle m$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m$ |  |'
- en: '|  | $\displaystyle p$ |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p$ |  |'
- en: where $\mathcal{M}$ represents the memory module.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{M}$ 代表记忆模块。
- en: The five directions are interconnected rather than mutually exclusive, often
    involving the concurrent adoption of multiple techniques. In the subsequent sections,
    we delve deeper into the five research directions concerning LLM-agent planning,
    elucidating their motivations, proposing representation solutions, and addressing
    inherent limitations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个方向是相互关联的，而非相互排斥，通常涉及同时采用多种技术。在随后的章节中，我们将深入探讨与 LLM 代理规划相关的五个研究方向，阐明它们的动机，提出表示解决方案，并解决固有的局限性。
- en: 'Table 1: A taxonomy for existing LLM-Agent planning works.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有 LLM-代理规划工作的分类法。
- en: Method Idea LLM’s task Formulation Representative works Task Decomposition Divide
    and Conquer Task decomposition Subtask planning $[g_{i}]=\operatorname{decompose}(E,g;\Theta,\mathcal{P});$
    REMEMBER Zhang et al. ([2023a](#bib.bib60)), MemoryBank Zhong et al. ([2023](#bib.bib64))
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 方法思想 LLM的任务制定 代表性工作 任务分解 分而治之 任务分解 子任务规划 $[g_{i}]=\operatorname{decompose}(E,g;\Theta,\mathcal{P});$
    REMEMBER Zhang et al. ([2023a](#bib.bib60)), MemoryBank Zhong et al. ([2023](#bib.bib64))
- en: 3 Task Decomposition
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务分解
- en: In real-world scenarios, environments are often characterized by complexity
    and variability, thereby addressing complex tasks through a one-step planning
    process is a formidable challenge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，环境通常具有复杂性和可变性，因此通过一步规划过程来处理复杂任务是一项艰巨的挑战。
- en: '![Refer to caption](img/ec9a9f5dc4b49283aea513deb426f324.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec9a9f5dc4b49283aea513deb426f324.png)'
- en: 'Figure 2: Types of task decomposition manners.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：任务分解方式的类型。
- en: 'This simplification of complicated tasks is a remarkable human ability, evident
    in the decomposition of one task into several simpler sub-tasks Schraagen et al.
    ([2000](#bib.bib35)), which is analogous to the well-known algorithmic strategy
    called “divide and conquer”, as illustrated in Eq. (1). Task decomposition generally
    involves two crucial steps: firstly, decomposing the complex task, referred to
    as the “decompose” step, and secondly, planning for the sub-tasks, known as the
    “sub-plan step”. Current methods for task decomposition in this domain generally
    fall into two categories: decomposition-first and interleaved decomposition, illustrated
    in Figure [2](#S3.F2 "Figure 2 ‣ 3 Task Decomposition ‣ Understanding the planning
    of LLM agents: A survey").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '这种将复杂任务简化的能力是人类的显著能力，在将一个任务分解为多个更简单的子任务中可以明显体现出来（Schraagen et al. ([2000](#bib.bib35))），这类似于著名的“分而治之”算法策略，如公式（1）所示。任务分解通常涉及两个关键步骤：首先，分解复杂任务，即“分解”步骤；其次，为子任务制定计划，即“子计划步骤”。当前在该领域的任务分解方法通常分为两类：先分解后规划和交替分解，如图[2](#S3.F2
    "Figure 2 ‣ 3 Task Decomposition ‣ Understanding the planning of LLM agents: A
    survey")所示。'
- en: 3.1 Decomposition-First Methods
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 先分解方法
- en: 'Decomposition-first methods decompose the task into sub-goals first and then
    plan for each sub-goal successively, presented in Figure [2](#S3.F2 "Figure 2
    ‣ 3 Task Decomposition ‣ Understanding the planning of LLM agents: A survey")(a).
    The representative methods include HuggingGPT Shen et al. ([2023](#bib.bib36)),
    Plan-and-Solve Wang et al. ([2023b](#bib.bib46)), ProgPrompt Singh et al. ([2023](#bib.bib39)),
    et al.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '先分解方法首先将任务分解为子目标，然后依次为每个子目标制定计划，如图[2](#S3.F2 "Figure 2 ‣ 3 Task Decomposition
    ‣ Understanding the planning of LLM agents: A survey")(a)所示。代表性方法包括HuggingGPT
    Shen et al. ([2023](#bib.bib36)), Plan-and-Solve Wang et al. ([2023b](#bib.bib46)),
    ProgPrompt Singh et al. ([2023](#bib.bib39))等。'
- en: 'HuggingGPT Shen et al. ([2023](#bib.bib36)) utilizes various multimodal models
    from the Huggingface Hub to construct an intelligent agent for multimodal tasks.
    It is capable of handling tasks such as image generation, image classification,
    object recognition, video annotation, speech-to-text, et al. To facilitate collaboration
    between different models, the LLM acts as a controller, responsible for decomposing
    tasks inputted by humans, selecting models, and generating final responses. The
    most crucial stage is the initial task decomposition, where HuggingGPT explicitly
    instructs the LLM to break down the given task into sub-tasks, providing dependencies
    between tasks. Plan-and-Solve Wang et al. ([2023b](#bib.bib46)) improves upon
    the Zero-shot Chain-of-Thought Kojima et al. ([2022](#bib.bib22)) by transforming
    the original “Let’s think step-by-step” into a two-step prompt instruction: “Let’s
    first devise a plan” and “Let’s carry out the plan”. This zero-shot approach has
    achieved improvements in mathematical reasoning, common-sense reasoning, and symbolic
    reasoning. ProgPrompt Singh et al. ([2023](#bib.bib39)) translates natural language
    descriptions of tasks into coding problems. It symbolizes the agent’s action space
    and objects in the environment through code, with each action formalized as a
    function and each object represented as a variable. Consequently, task planning
    is naturally transformed into function generation. When executing tasks, the agent
    first generates a plan in the form of function callings and then executes them
    step by step.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingGPT Shen 等人（[2023](#bib.bib36)）利用 Huggingface Hub 上的各种多模态模型来构建一个智能体以处理多模态任务。它能够处理图像生成、图像分类、物体识别、视频标注、语音转文本等任务。为了促进不同模型之间的协作，LLM
    充当控制器，负责分解人类输入的任务，选择模型，并生成最终响应。最关键的阶段是初始任务分解，其中 HuggingGPT 明确指示 LLM 将给定任务分解为子任务，并提供任务之间的依赖关系。Plan-and-Solve
    Wang 等人（[2023b](#bib.bib46)）在 Zero-shot Chain-of-Thought Kojima 等人（[2022](#bib.bib22)）的基础上进行改进，将原来的“让我们一步步思考”转换为两步提示指令：“首先制定计划”和“执行计划”。这种零样本方法在数学推理、常识推理和符号推理方面取得了改进。ProgPrompt
    Singh 等人（[2023](#bib.bib39)）将自然语言描述的任务转换为编码问题。它通过代码符号化智能体的动作空间和环境中的对象，每个动作形式化为一个函数，每个对象表示为一个变量。因此，任务规划自然地转化为函数生成。在执行任务时，智能体首先以函数调用的形式生成计划，然后逐步执行这些计划。
- en: 3.2 Interleaved Decomposition Methods
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 交错分解方法
- en: 'Interleaved decomposition involves interleaved task decomposition and sub-task
    planning, where each decomposition only reveals one or two sub-tasks at the current
    state, illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3 Task Decomposition ‣ Understanding
    the planning of LLM agents: A survey")(b). Representative methods in this category
    include the Chain-of-Thought (CoT) series Wei et al. ([2022](#bib.bib48)); Kojima
    et al. ([2022](#bib.bib22)), ReAct Yao et al. ([2022](#bib.bib57)), PAL Gao et
    al. ([2023](#bib.bib10)), Program-of-Thought (PoT) Chen et al. ([2022](#bib.bib7)),
    Visual ChatGPT Wu et al. ([2023](#bib.bib49)), et al.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '交错分解包括交错任务分解和子任务规划，其中每次分解只在当前状态下揭示一个或两个子任务，如图[2](#S3.F2 "Figure 2 ‣ 3 Task
    Decomposition ‣ Understanding the planning of LLM agents: A survey")(b)所示。该类别中的代表性方法包括
    Chain-of-Thought (CoT) 系列 Wei 等人（[2022](#bib.bib48)）；Kojima 等人（[2022](#bib.bib22)），ReAct
    Yao 等人（[2022](#bib.bib57)），PAL Gao 等人（[2023](#bib.bib10)），Program-of-Thought (PoT)
    Chen 等人（[2022](#bib.bib7)），Visual ChatGPT Wu 等人（[2023](#bib.bib49)）等。'
- en: The introduction of Chain-of-Thought (CoT) Wei et al. ([2022](#bib.bib48)) reveals
    the few-shot learning capabilities of LLM. CoT guides the LLM in reasoning about
    complex problems through a few constructed trajectories, leveraging the LLM’s
    reasoning abilities for task decomposition. Subsequently, Zero-shot CoT Kojima
    et al. ([2022](#bib.bib22)) unlocks the LLM’s zero-shot reasoning abilities with
    the magical instruction “Let’s think step-by-step”. In contrast to CoT, which
    embeds reasoning within the planning process, ReAct Yao et al. ([2022](#bib.bib57))
    decouples reasoning and planning. It alternates between reasoning (Thought step)
    and planning (Action step), demonstrating significant improvements in the planning
    capabilities. Visual ChatGPT Wu et al. ([2023](#bib.bib49)) utilizes ReAct’s mechanism,
    employing LLM as the agent’s brain equipped with a series of visual models, resulting
    in an agent with image processing capabilities. PAL Gao et al. ([2023](#bib.bib10))
    improves CoT by leveraging the LLM’s coding abilities, guiding the LLM to generate
    code during reasoning. Finally, a code interpreter (such as Python) is used to
    comprehensively execute the codes to obtain the solution. This method proves helpful
    for agents in solving mathematical and symbolic reasoning problems. Program-of-Thought
    (PoT) Chen et al. ([2022](#bib.bib7)) completely formalize the reasoning process
    as programming. The authors also leverage a CodeX Chen et al. ([2021b](#bib.bib6))
    model trained on code-related data, enhancing performance in mathematical and
    financial problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Chain-of-Thought (CoT) 的引入 Wei et al. ([2022](#bib.bib48)) 揭示了大型语言模型（LLM）的少样本学习能力。CoT
    通过几个构造的轨迹指导 LLM 对复杂问题进行推理，利用 LLM 的推理能力进行任务分解。随后，Zero-shot CoT Kojima et al. ([2022](#bib.bib22))
    通过神奇的指令“Let’s think step-by-step” 解锁了 LLM 的零样本推理能力。与将推理嵌入规划过程中的 CoT 不同，ReAct Yao
    et al. ([2022](#bib.bib57)) 解耦了推理和规划。它在推理（Thought step）和规划（Action step）之间交替进行，显示了规划能力的显著改善。Visual
    ChatGPT Wu et al. ([2023](#bib.bib49)) 利用 ReAct 的机制，将 LLM 作为配备了一系列视觉模型的代理的“大脑”，从而使代理具备图像处理能力。PAL Gao
    et al. ([2023](#bib.bib10)) 通过利用 LLM 的编码能力改进了 CoT，指导 LLM 在推理过程中生成代码。最后，使用代码解释器（如
    Python）全面执行这些代码以获得解决方案。这种方法对于解决数学和符号推理问题的代理非常有帮助。Program-of-Thought (PoT) Chen
    et al. ([2022](#bib.bib7)) 将推理过程完全形式化为编程。作者还利用了一个基于代码相关数据训练的 CodeX Chen et al.
    ([2021b](#bib.bib6)) 模型，提高了在数学和金融问题上的表现。
- en: 3.3 Discussions
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 讨论
- en: 'For the decomposition-first method, the advantage lies in creating a stronger
    correlation between the sub-tasks and the original tasks, reducing the risk of
    task forgetting and hallucinations Touvron et al. ([2023](#bib.bib42)). However,
    since the sub-tasks are predetermined at the beginning, additional mechanisms
    for adjustment are required otherwise one error in some step will result in failure,
    which will be discussed in Section [6](#S6 "6 Reflection and Refinement ‣ Understanding
    the planning of LLM agents: A survey"). On the other hand, interleaved decomposition
    and sub-planning dynamically adjust decomposition based on environmental feedback,
    improving the fault tolerance. However, for complicated tasks, excessively long
    trajectories may lead to LLM experiencing hallucinations, deviating from the original
    goals during subsequent sub-tasks and sub-planning.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '对于先分解方法，其优势在于创建了子任务与原任务之间更强的关联性，减少了任务遗忘和幻觉的风险 Touvron et al. ([2023](#bib.bib42))。然而，由于子任务在开始时已被预定，因此需要额外的调整机制，否则某一步的错误将导致失败，这将在第
    [6](#S6 "6 Reflection and Refinement ‣ Understanding the planning of LLM agents:
    A survey") 节讨论。另一方面，交替分解和子规划根据环境反馈动态调整分解，提高了容错性。然而，对于复杂任务，过长的轨迹可能导致 LLM 体验幻觉，在随后的子任务和子规划过程中偏离原始目标。'
- en: Although task decomposition significantly enhances the ability of LLM-Agent
    to solve complicated tasks, challenges persist. The first challenge is the additional
    overhead introduced by task decomposition. Decomposing a task into multiple sub-tasks
    requires more reasoning and generation, incurring additional time and computational
    costs. On the other hand, for highly complex tasks that are decomposed into dozens
    of sub-tasks, the planning is constrained by the context length of the LLM, leading
    to the forgetting of the planning trajectories.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管任务分解显著增强了 LLM-Agent 解决复杂任务的能力，但仍然存在挑战。第一个挑战是任务分解引入的额外开销。将任务分解成多个子任务需要更多的推理和生成，导致额外的时间和计算成本。另一方面，对于分解为数十个子任务的高度复杂任务，规划受限于
    LLM 的上下文长度，导致规划轨迹的遗忘。
- en: 4 Multi-Plan Selection
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 多计划选择
- en: 'Due to the complexity of the tasks and the inherent uncertainty of LLM, the
    plans generated by the LLM-Agent for a given task can be diverse. Even though
    LLM possesses strong reasoning abilities, a single plan generated by LLM is likely
    to be suboptimal or even infeasible. A more natural approach is multi-plan selection,
    comprising two major steps: multi-plan generation and optimal plan selection.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任务的复杂性和 LLM 固有的不确定性，LLM-Agent 为给定任务生成的计划可能会多样化。即使 LLM 具备强大的推理能力，单个生成的计划可能是次优的或甚至不可行的。更自然的方法是多计划选择，包括两个主要步骤：多计划生成和最优计划选择。
- en: 4.1 Multi-Plan Generation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 多计划生成
- en: Multi-plan generation involves generating a dozen paths of plans to comprise
    the candidate plan set. Mainstream methods consider employing uncertainty in the
    decoding process of generative models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多计划生成涉及生成多个计划路径，以组成候选计划集。主流方法考虑在生成模型的解码过程中引入不确定性。
- en: 'Self-consistency Wang et al. ([2022b](#bib.bib44)) employs a simple intuition:
    the solutions for complex problems are rarely unique. In contrast to CoT, which
    generates a single path, Self-consistency obtains multiple distinct reasoning
    paths via sampling strategies embodied in the decoding process, such as temperature
    sampling, top-k sampling. Tree-of-Thought (ToT) Yao et al. ([2023](#bib.bib58))
    proposes two strategies to generate plans (i.e. thoughts): sample and propose.
    The sample strategy is consistent with Self-consistency, where LLM would sample
    multiple plans in decoding process. The propose strategy explicitly instructs
    the LLM to generate various plans via few-shot examples in prompts. Graph-of-Thought
    (GoT) Besta et al. ([2023](#bib.bib3)) extends ToT by adding transformations of
    thoughts, which supports arbitrary thoughts aggregation. LLM-MCTS Zhao et al.
    ([2023b](#bib.bib63)) and RAP Hao et al. ([2023](#bib.bib15)) leverages LLM as
    the heuristic policy function for the Monte Carlo Tree Search (MCTS), where multiple
    potential actions are obtained by multiple calls.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自一致性 Wang 等人 ([2022b](#bib.bib44)) 运用了一个简单的直觉：复杂问题的解决方案很少是唯一的。与生成单一路径的 CoT 相比，自一致性通过在解码过程中使用温度采样、top-k
    采样等采样策略获得多个不同的推理路径。思想树 (ToT) Yao 等人 ([2023](#bib.bib58)) 提出了两种生成计划（即思想）的策略：采样和提议。采样策略与自一致性一致，其中
    LLM 会在解码过程中采样多个计划。提议策略明确指示 LLM 通过提示中的少量示例生成各种计划。思想图 (GoT) Besta 等人 ([2023](#bib.bib3))
    通过添加思想变换来扩展 ToT，支持任意思想的聚合。LLM-MCTS Zhao 等人 ([2023b](#bib.bib63)) 和 RAP Hao 等人
    ([2023](#bib.bib15)) 利用 LLM 作为蒙特卡罗树搜索 (MCTS) 的启发式策略函数，通过多次调用获得多个潜在行动。
- en: 4.2 Optimal Plan Selection
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 最优计划选择
- en: To select the optimal plan among the candidate plans, diverse strategies are
    adopted as heuristic search algorithms.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在候选计划中选择最佳计划，采用了多种策略作为启发式搜索算法。
- en: Self-consistency Wang et al. ([2022b](#bib.bib44)) applies the naive majority
    vote strategy, regarding the plan with the most votes as the optimal choice. Benefiting
    from the tree architecture, Tree-of-Thought (ToT) Yao et al. ([2023](#bib.bib58))
    supports tree search algorithms, such as conventional BFS and DFS. When selecting
    a node for expansion, it uses LLM to evaluate multiple actions and chooses the
    optimal one. Similar with ToT, LLM-MCTS Zhao et al. ([2023b](#bib.bib63)) and
    RAP Hao et al. ([2023](#bib.bib15)) also employ a tree structure to assist in
    multi-plan search. Unlike ToT, they employ the Monte Carlo Tree Search (MCTS)
    algorithm for search. LLM A* Xiao and Wang ([2023](#bib.bib51)) utilizes the classic
    A* algorithm from artificial intelligence to assist LLM in search. The Chebyshev
    distance from the current position to the target position serves as the heuristic
    cost function for selecting the optimal path.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自一致性 Wang 等人 ([2022b](#bib.bib44)) 应用了简单的多数投票策略，将获得最多票数的方案视为最佳选择。得益于树结构，Tree-of-Thought
    (ToT) Yao 等人 ([2023](#bib.bib58)) 支持树搜索算法，例如常规的 BFS 和 DFS。在选择一个节点进行扩展时，它使用 LLM
    评估多个行动并选择最佳行动。与 ToT 类似，LLM-MCTS Zhao 等人 ([2023b](#bib.bib63)) 和 RAP Hao 等人 ([2023](#bib.bib15))
    也采用树结构来协助多方案搜索。与 ToT 不同的是，它们采用蒙特卡洛树搜索 (MCTS) 算法进行搜索。LLM A* Xiao 和 Wang ([2023](#bib.bib51))
    利用经典的 A* 算法来辅助 LLM 搜索。当前位点到目标位点的切比雪夫距离作为启发式成本函数，用于选择最佳路径。
- en: 4.3 Discussions
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 讨论
- en: The scalability of multi-plan selection is notably advantageous, providing a
    broader exploration of potential solutions in the expansive search space. However,
    this advantage comes with inherent trade-offs. The increased computational demands,
    especially for models with large token counts or computations, pose practical
    challenges. This cost consideration becomes crucial, particularly in scenarios
    where resource constraints are a significant factor, such as the online service.
    Moreover, the reliance on LLM for the evaluation of plans introduces new challenges.
    As LLM’s performance in ranking tasks is still under scrutiny, there is a need
    for further validation and fine-tuning of its capabilities in this specific context.
    The stochastic nature of LLMs adds randomness to the selection, potentially affecting
    the consistency and reliability of the chosen plans.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多方案选择的可扩展性显著优势，提供了在广阔搜索空间中对潜在解决方案的更广泛探索。然而，这种优势也伴随着固有的权衡。特别是对于大规模 token 数或计算的模型，增加的计算需求带来了实际挑战。在资源约束是重要因素的情况下，如在线服务，这种成本考虑尤为重要。此外，依赖
    LLM 进行方案评估引入了新的挑战。由于 LLM 在排名任务中的表现仍在审查中，需要进一步验证和调整其在此特定上下文中的能力。LLM 的随机性为选择增加了不确定性，可能影响所选方案的一致性和可靠性。
- en: 5 External Planner-Aided Planning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 外部规划器辅助规划
- en: Despite the powerful reasoning and task decomposition capabilities exhibited
    by Large Language Models (LLMs), challenges arise when confronted with environments
    featuring intricate constraints, such as mathematical problem-solving or generating
    admissible actions. To address challenges, several methods integrate LLMs with
    external planners. Such methods can be categorized into symbolic planners and
    neural planners based on the introduced planners.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型 (LLMs) 展示了强大的推理和任务分解能力，但在面对复杂约束的环境时，如数学问题解决或生成可接受的行动时，仍然会遇到挑战。为了应对这些挑战，一些方法将
    LLM 与外部规划器结合起来。这些方法可以根据引入的规划器分为符号规划器和神经规划器。
- en: 5.1 Symbolic Planner
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 符号规划器
- en: Symbolic planners have served as a fundamental component in the fields of automated
    planning for several decades. These approaches, based on well-established symbolic
    formalized models, such as PDDL models Aeronautiques et al. ([1998](#bib.bib1));
    Haslum et al. ([2019](#bib.bib16)), employ symbolic reasoning to identify optimal
    paths from initial states to desired goal states.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 符号规划器在自动化规划领域已经作为一个基本组成部分存在了几十年。这些方法基于成熟的符号化模型，如 PDDL 模型 Aeronautiques 等人 ([1998](#bib.bib1));
    Haslum 等人 ([2019](#bib.bib16))，利用符号推理从初始状态识别到达期望目标状态的最佳路径。
- en: LLM+P Liu et al. ([2023a](#bib.bib26)) enhances the planning proficiency of
    LLMs by incorporating a PDDL-based symbolic planner. Leveraging the semantic understanding
    and coding capabilities of LLM, the authors organize problems into textual language
    prompts inputted to LLM. This prompts LLM to organize the actions within the environment
    and specified tasks into the format of the PDDL language. Subsequently, after
    obtaining a formalized description, the authors employ the Fast-Downward ²²2[https://github.com/aibasel/downward/tree/release-22.12.0](https://github.com/aibasel/downward/tree/release-22.12.0)
    solver for the planning process. Building upon LLM+P, LLM-DP Dagan et al. ([2023](#bib.bib8))
    is specifically designed for dynamic interactive environments. Upon receiving
    feedback from the environment, LLM processes the information, formalizes it into
    PDDL language, and then employs a BFS Lipovetzky et al. ([2014](#bib.bib25)) solver
    to generate a plan. LLM+PDDL Guan et al. ([2023](#bib.bib14)) also utilizes the
    PDDL language to formalize the task, incorporating an additional step for manual
    verification to check for potential issues in the PDDL model generated by LLM.
    During the planning process, the authors propose using the plan generated by LLM
    as an initial heuristic solution to accelerate the search process of local search
    planners, such as LPG Gerevini and Serina ([2002](#bib.bib11)). LLM+ASP Yang et
    al. ([2023b](#bib.bib54)) transforms problems described in natural language by
    LLM into atomic facts, converting tasks into ASP problems. Subsequently, the ASP
    solver CLINGO is utilized to generate plans.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: LLM+P 刘等人 ([2023a](#bib.bib26)) 通过结合基于PDDL的符号规划器，提升了LLM的规划能力。利用LLM的语义理解和编码能力，作者将问题组织成文本语言提示输入LLM。这使得LLM能够将环境中的行动和指定任务组织成PDDL语言的格式。随后，在获得正式描述后，作者使用
    Fast-Downward²²2[https://github.com/aibasel/downward/tree/release-22.12.0](https://github.com/aibasel/downward/tree/release-22.12.0)
    求解器进行规划过程。在LLM+P的基础上，LLM-DP 达根等人 ([2023](#bib.bib8)) 专门设计用于动态交互环境。在接收环境反馈后，LLM处理信息，将其形式化为PDDL语言，然后使用
    BFS Lipovetzky等人 ([2014](#bib.bib25)) 求解器生成计划。LLM+PDDL 关等人 ([2023](#bib.bib14))
    也利用PDDL语言来形式化任务，加入了一个手动验证的步骤，以检查LLM生成的PDDL模型中可能存在的问题。在规划过程中，作者建议使用LLM生成的计划作为初步启发式解决方案，以加快局部搜索规划器（如LPG
    Gerevini和Serina ([2002](#bib.bib11))）的搜索过程。LLM+ASP 杨等人 ([2023b](#bib.bib54)) 将LLM描述的自然语言问题转换为原子事实，将任务转化为ASP问题。随后，使用ASP求解器
    CLINGO 生成计划。
- en: 5.2 Neural Planner
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 神经规划器
- en: Neural planners are deep models trained on collected planning data with reinforcement
    learning or imitation learning techniques, showing effective planning abilities
    within the specific domain. For instance, DRRN He et al. ([2015](#bib.bib17))
    models the planning process as a Markov Decision Process through reinforcement
    learning, training a policy network to obtain a deep decision model. Decision
    Transformer (DT) Chen et al. ([2021a](#bib.bib5)) empowers a transformer model
    to clone human decision-making behavior with planning data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 神经规划器是使用强化学习或模仿学习技术在收集的规划数据上训练的深度模型，在特定领域中展示了有效的规划能力。例如，DRRN 何等人 ([2015](#bib.bib17))
    将规划过程建模为马尔可夫决策过程，通过强化学习训练策略网络以获得深度决策模型。Decision Transformer (DT) 陈等人 ([2021a](#bib.bib5))
    使得变换器模型能够通过规划数据克隆人类决策行为。
- en: Well-trained neural planners exhibit excellent planning capabilities within
    their respective domains and demonstrate superior planning efficiency due to their
    smaller parameter sizes. However, when faced with complex and less frequently
    encountered problems, where training data is scarce, these small models tend to
    perform poorly due to insufficient generalization ability. Therefore, several
    works explore combining an LLM with a light-weight neural planner, to further
    enhance the planning capabilities. CALM Yao et al. ([2020a](#bib.bib55)) proposed
    an early approach that combines a language model with an RL-based neural planner.
    One language model processes textual environmental information, generating a set
    of candidate actions as priors based on the environmental information. A DRRN
    policy network is then employed to re-rank these candidate actions, ultimately
    selecting the optimal action. SwiftSage Lin et al. ([2023](#bib.bib24)) leverages
    the dual-process theory from cognitive psychology, dividing the planning process
    into slow thinking and fast thinking. The slow-thinking process involves complex
    reasoning and rational deliberation while fast-thinking resembles an instinctive
    response developed through long-term training. The authors utilize a DT model,
    trained through imitation learning, as the fast-thinking model for rapid plan
    generation. When errors occur during plan execution, indicating a more complex
    problem, the agent switches to the slow-thinking process, where LLM engages in
    reasoning and planning based on the current state. This combination of fast and
    slow thinking has proven to be highly effective in terms of efficiency.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 经过良好训练的神经规划器在其各自领域展示了出色的规划能力，并因其较小的参数规模表现出更高的规划效率。然而，当面对复杂且不常见的问题时，训练数据稀缺，这些小型模型由于泛化能力不足往往表现较差。因此，一些研究探索了将大型语言模型（LLM）与轻量级神经规划器结合的方式，以进一步增强规划能力。CALM
    Yao 等人（[2020a](#bib.bib55)）提出了一种早期的方法，将语言模型与基于强化学习的神经规划器结合起来。一个语言模型处理文本环境信息，基于环境信息生成一组候选动作作为先验。然后，使用
    DRRN 策略网络对这些候选动作进行重新排序，最终选择最佳动作。SwiftSage Lin 等人（[2023](#bib.bib24)）利用认知心理学中的双重过程理论，将规划过程分为慢思考和快思考。慢思考过程涉及复杂的推理和理性审议，而快思考则类似于通过长期训练形成的直觉反应。作者利用通过模仿学习训练的
    DT 模型作为快速计划生成的快思考模型。当在计划执行过程中出现错误，表明问题更复杂时，代理将切换到慢思考过程，在此过程中，LLM 根据当前状态进行推理和规划。这种快思考与慢思考的结合在效率方面被证明是非常有效的。
- en: 5.3 Discussions
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 讨论
- en: For those strategies that leverage an additional planner for assistance, LLM
    primarily plays a supportive role. Its main functions involve parsing textual
    feedback and providing additional reasoning information to assist in planning,
    particularly when addressing complex problems. Specifically, the enhancement of
    LLM’s capabilities in code generation empowers the potential to deal with more
    general tasks for symbolic artificial intelligence. Actually, a significant drawback
    of traditional symbolic AI systems lies in the complexity and heavy reliance on
    human experts in constructing symbolic models, while LLM accelerates this process,
    facilitating faster and more optimal establishment of symbolic models. The advantages
    brought by symbolic systems include theoretical completeness, stability, and interpretability.
    The combination of statistical AI with LLM is poised to become a major trend in
    the future development of artificial intelligence.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些利用额外规划器辅助的策略而言，LLM 主要扮演支持角色。它的主要功能包括解析文本反馈并提供额外的推理信息，以协助规划，特别是在处理复杂问题时。具体而言，LLM
    在代码生成方面能力的提升使其有潜力处理更多的符号人工智能任务。实际上，传统符号 AI 系统的一个显著缺点在于构建符号模型的复杂性和对人类专家的高度依赖，而
    LLM 加速了这一过程，促进了符号模型的更快速和更优化的建立。符号系统带来的优势包括理论上的完整性、稳定性和可解释性。统计 AI 与 LLM 的结合有望成为未来人工智能发展的主要趋势。
- en: 6 Reflection and Refinement
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 反思与改进
- en: Reflection and refinement are indispensable components in the planning process.
    They enhance the fault tolerance and error correction capabilities of LLM-Agent
    planning. Due to existing hallucination issues and insufficient reasoning abilities
    for complex problems, LLM-Agents may make errors and get stuck in “thought loops”
    during planning due to limited feedback. Reflecting on and summarizing failures
    helps agents correct errors and break out of such loops in subsequent attempts.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 反思和优化是规划过程中不可或缺的组成部分。它们增强了LLM-Agent规划的容错性和错误纠正能力。由于现有的幻觉问题和对复杂问题的推理能力不足，LLM-Agents可能会在规划过程中出现错误，并因反馈有限而陷入“思维循环”。反思和总结失败有助于智能体在后续尝试中纠正错误并摆脱这种循环。
- en: Self-refine Madaan et al. ([2023](#bib.bib29)) utilizes an iterative process
    of generation, feedback, and refinement. After each generation, LLM generates
    feedback for the plan, facilitating adjustments based on the feedback. Reflexion Shinn
    et al. ([2023](#bib.bib37)) extends ReAct by incorporating an evaluator to assess
    trajectories. LLM generates self-reflections upon error detection, aiding in error
    correction. CRITIC Gou et al. ([2023](#bib.bib13)) uses external tools like Knowledge
    Bases and Search Engines to validate LLM-generated actions. It then leverages
    external knowledge for self-correction, significantly reducing factual errors.
    InteRecAgent Huang et al. ([2023b](#bib.bib19)) employs a mechanism called ReChain
    for self-correction. An LLM is used to evaluate the response and tool-using plan
    generated by the interactive recommendation agent, summarize feedback on errors,
    and decide whether to restart planning. LEMA An et al. ([2023](#bib.bib2)) gathers
    mistaken planning samples first and employs more powerful GPT-4 for correction.
    Those corrected samples are then used to fine-tune the LLM-Agent, resulting in
    significant performance improvements across various scales of the LLaMA model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自我优化 Madaan et al. ([2023](#bib.bib29)) 使用生成、反馈和优化的迭代过程。在每次生成之后，LLM生成对计划的反馈，根据反馈进行调整。反思
    Shinn et al. ([2023](#bib.bib37)) 通过引入评估器来评估轨迹扩展了ReAct。LLM在检测到错误后生成自我反思，帮助纠正错误。CRITIC
    Gou et al. ([2023](#bib.bib13)) 使用知识库和搜索引擎等外部工具来验证LLM生成的行动。然后利用外部知识进行自我纠正，显著减少事实错误。InteRecAgent
    Huang et al. ([2023b](#bib.bib19)) 使用称为ReChain的机制进行自我纠正。LLM用于评估互动推荐智能体生成的响应和工具使用计划，总结错误反馈，并决定是否重新开始规划。LEMA
    An et al. ([2023](#bib.bib2)) 首先收集错误的规划样本，并使用更强大的GPT-4进行纠正。然后将这些纠正后的样本用于微调LLM-Agent，导致LLaMA模型在各种规模上的性能显著提升。
- en: Particularly, the self-reflective strategy bears resemblance to the principles
    of reinforcement learning, where the agent plays the role of the decision-maker,
    such as the policy network. Environmental feedback triggers updates of the policy
    network. However, in contrast to deep reinforcement learning where updates are
    achieved by modifying model parameters, in the LLM agent, this update occurs through
    self-reflection by the LLM itself, culminating in textual verbal feedbacks. These
    textual feedbacks can serve as both long-term and short-term memory, influencing
    the agent’s subsequent planning outputs through the prompts. Nevertheless, the
    convergence of this textual form of update currently lacks a guaranteed proof,
    indicating the inability to demonstrate that continual reflection can ultimately
    lead the LLM agent to a specified goal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，自我反思策略与强化学习的原则相似，其中智能体充当决策者的角色，如策略网络。环境反馈触发策略网络的更新。然而，与通过修改模型参数来实现更新的深度强化学习不同，在LLM智能体中，这种更新通过LLM自身的自我反思来实现，最终形成文本反馈。这些文本反馈可以作为长期和短期记忆，通过提示影响智能体后续的规划输出。然而，这种文本形式的更新的收敛性目前缺乏保证的证明，表明无法证明持续的反思最终可以使LLM智能体达到特定目标。
- en: 7 Memory-Augumented Planning
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 增强记忆规划
- en: 'For agents, memory is a crucial pathway to enhance planning capabilities and
    the potential for growth. Regarding the memory mechanisms in LLM-Agents, there
    are currently two major approaches to enhance planning abilities through memory:
    RAG-based memory and embodied memory.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于智能体来说，记忆是提升规划能力和成长潜力的关键途径。关于LLM-Agents中的记忆机制，目前有两种主要方法通过记忆来增强规划能力：基于RAG的记忆和具身记忆。
- en: 7.1 RAG-based Memory
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 基于RAG的记忆
- en: Retrieval Augmented Generation (RAG) Lewis et al. ([2020](#bib.bib23)); Mao
    et al. ([2020](#bib.bib30)); Cai et al. ([2022](#bib.bib4)) techniques are proposed
    to aid text generation with retrieved information. It is capable of enhancing
    the LLM with the latest knowledge, such as New Bing³³3https://www.bing.com/ and
    Google Bard⁴⁴4https://bard.google.com. For LLM agents, past experiences could
    be stored in the memory and retrieved when needed. The core idea of such methods
    is to retrieve task-relevant experiences from the memory during task planning.
    Among those methods, memories are typically stored in additional storage, and
    the forms are diverse, such as texts Park et al. ([2023](#bib.bib33)); Liu et
    al. ([2023b](#bib.bib27)); Packer et al. ([2023](#bib.bib31)); Wang et al. ([2023c](#bib.bib47));
    Zhong et al. ([2023](#bib.bib64)), tabular forms Zhang et al. ([2023a](#bib.bib60)),
    knowledge graph Pan et al. ([2024](#bib.bib32)), etc.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）Lewis等人（[2020](#bib.bib23)）；Mao等人（[2020](#bib.bib30)）；Cai等人（[2022](#bib.bib4)）技术被提出以帮助通过检索到的信息生成文本。它能够增强LLM的最新知识，例如New
    Bing³³3https://www.bing.com/ 和Google Bard⁴⁴4https://bard.google.com。对于LLM代理，过去的经验可以存储在记忆中，并在需要时检索。这些方法的核心思想是在任务规划期间从记忆中检索与任务相关的经验。在这些方法中，记忆通常存储在额外的存储中，形式多样，例如文本
    Park等人（[2023](#bib.bib33)）；Liu等人（[2023b](#bib.bib27)）；Packer等人（[2023](#bib.bib31)）；Wang等人（[2023c](#bib.bib47)）；Zhong等人（[2023](#bib.bib64)），表格形式
    Zhang等人（[2023a](#bib.bib60)），知识图谱 Pan等人（[2024](#bib.bib32)）等。
- en: Generative Agents Park et al. ([2023](#bib.bib33)) store the daily experiences
    of human-like agents in text form and retrieve memories based on a composite score
    of recency and relevance to the current situation. Similarly, MemoryBank Zhong
    et al. ([2023](#bib.bib64)), TiM Liu et al. ([2023b](#bib.bib27)), and RecMind Wang
    et al. ([2023c](#bib.bib47)) encode each memory using a text encoding model into
    a vector and establish an indexing structure, such as FAISS library Johnson et
    al. ([2019](#bib.bib20)). During retrieval, the description of the current status
    is used as a query to retrieve memories from the memory pool. The difference between
    the three lies in the way memories are updated. MemGPT Packer et al. ([2023](#bib.bib31))
    leverages the concept of multiple levels of storage in computer architecture,
    abstracting the context of LLM into RAM and treating the additional storage structure
    as a disk. LLM can spontaneously decide whether to retrieve historical memories
    or save the current context to storage. REMEMBER Zhang et al. ([2023a](#bib.bib60))
    stores historical memories in the form of a Q-value table, where each record is
    (environment, task, action, Q-value)-tuple. During retrieval, positive and negative
    memories are both retrieved for LLM to generate plan based on the similarity of
    the environment and task.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Generative Agents Park等人（[2023](#bib.bib33)）以文本形式存储类人代理的日常经验，并基于近期性和与当前情况的相关性复取记忆。类似地，MemoryBank
    Zhong等人（[2023](#bib.bib64)）、TiM Liu等人（[2023b](#bib.bib27)）和RecMind Wang等人（[2023c](#bib.bib47)）使用文本编码模型将每个记忆编码成向量，并建立索引结构，例如FAISS库
    Johnson等人（[2019](#bib.bib20)）。在检索过程中，当前状态的描述被用作查询，从记忆池中检索记忆。这三者之间的区别在于记忆更新的方式。MemGPT
    Packer等人（[2023](#bib.bib31)）利用计算机体系结构中的多级存储概念，将LLM的上下文抽象为RAM，并将附加存储结构视为磁盘。LLM可以自主决定是否检索历史记忆或将当前上下文保存到存储中。REMEMBER
    Zhang等人（[2023a](#bib.bib60)）以Q值表的形式存储历史记忆，其中每条记录是（环境，任务，行动，Q值）元组。在检索过程中，正面和负面的记忆都被检索，以便LLM根据环境和任务的相似性生成计划。
- en: 7.2 Embodied Memory
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 具身记忆
- en: Embodied memory involves finetuning the LLM with the agent’s historical experiential
    samples, embedding memories into the model parameters. Usually the experiential
    samples are collected from the agents’s interactions with environment, which may
    consist of commonsense knowledge about the environment, task-related priors, and
    successful or failed experiences. While the cost of training a language model
    with more than billions of parameters is huge, parameter-efficient fine-tuning
    (PEFT) techniques are leveraged to reduce cost and speed up by training a small
    part of parameters only, such as LoRA, QLoRA, P-tuning, et al.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 具身记忆涉及通过代理的历史经验样本来微调LLM，将记忆嵌入模型参数中。通常，经验样本来自代理与环境的互动，可能包含有关环境的常识知识、任务相关的先验知识以及成功或失败的经验。尽管训练具有数十亿参数的语言模型的成本巨大，但采用参数高效微调（PEFT）技术可以降低成本并加快速度，仅训练一小部分参数，例如LoRA、QLoRA、P-tuning等。
- en: CALM Yao et al. ([2020b](#bib.bib56)) utilizes ground-truth action trajectories
    collected from the text-world environment to finetune GPT-2 using next token prediction
    task, enabling it to memorize planning-related information and generalize well
    on planning tasks. Similarly, TDT Wang et al. ([2022a](#bib.bib43)) uses collected
    Markov decision process data to fine-tune Text Decision Transformer (TDT). It
    achieves better success rates on more challenging ScienceWorld Wang et al. ([2022a](#bib.bib43))
    tasks. AgentTuning Zeng et al. ([2023](#bib.bib59)) organizes plan trajectories
    from various tasks into a dialogue form to finetune the LLaMA model, showing significant
    improvements in performance on unseen planning tasks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: CALM Yao 等人 ([2020b](#bib.bib56)) 利用从文本世界环境中收集的真实动作轨迹，通过下一令牌预测任务对 GPT-2 进行微调，使其能够记住与规划相关的信息，并在规划任务上表现出良好的泛化能力。同样，TDT
    Wang 等人 ([2022a](#bib.bib43)) 使用收集的马尔可夫决策过程数据对 Text Decision Transformer (TDT)
    进行微调。在更具挑战性的 ScienceWorld Wang 等人 ([2022a](#bib.bib43)) 任务上取得了更好的成功率。AgentTuning
    Zeng 等人 ([2023](#bib.bib59)) 将来自各种任务的计划轨迹整理成对话形式，以微调 LLaMA 模型，在未见过的规划任务上表现出显著的性能提升。
- en: 7.3 Discussions
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 讨论
- en: The RAG-based and Fine-tuning-based memory approaches enhance LLM-Agent planning
    capabilities, each with distinct advantages and limitations. RAG-based methods
    offer real-time, low-cost external memory updates mainly in natural language text,
    but rely on the accuracy of retrieval algorithm. Finetuning provides a larger
    memorization capacity through parameter modifications but has high memory update
    costs and struggles with retaining fine-grained details.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 RAG 和微调的记忆方法提升了 LLM-Agent 的规划能力，每种方法都有其独特的优点和局限性。RAG 基于的方法提供了实时、低成本的外部记忆更新，主要集中在自然语言文本中，但依赖于检索算法的准确性。微调通过参数修改提供了更大的记忆容量，但内存更新成本高且难以保持细粒度的细节。
- en: Memory-enhanced LLM-Agents demonstrate enhanced growth and fault tolerance in
    planning, yet memory generation heavily depends on LLM’s generation capabilities.
    Improving weaker LLM-Agents through self-generated memory remains a challenging
    area to explore.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆增强型 LLM-Agent 展现了在规划中的增长和容错能力，但记忆生成严重依赖于 LLM 的生成能力。通过自生成记忆来改善较弱的 LLM-Agent
    仍然是一个具有挑战性的领域。
- en: 8 Evaluation
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 评估
- en: 'Table 2: Evaluation of representative prompt-based methods on four interactive
    benchmarks. The SR, AR and EX are abbreviations of success rate, average rewards,
    and expenses respectively. The expenses are calculated based on the number of
    consumed tokens through OpenAI’s API. Z-CoT and F-CoT represent Zeroshot-CoT and
    Fewshot-CoT, respectively.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：对四种互动基准上代表性提示方法的评估。SR、AR 和 EX 分别是成功率、平均奖励和费用的缩写。费用是根据通过 OpenAI API 消耗的令牌数量计算的。Z-CoT
    和 F-CoT 分别代表 Zeroshot-CoT 和 Fewshot-CoT。
- en: AlfWorld ScienceWorld HotPotQA FEVER Metrics SR(%) EX($) AR EX($) SR(%) EX($)
    SR(%) EX($) Z-CoT N/A N/A N/A N/A 0.01 0.95 0.39 1.07 F-CoT 0.43 98.60 16.58 272.22
    0.32 5.73 0.61 2.25 CoT-SC 0.57 105.37 15.24 274.33 0.33 7.86 0.62 3.21 SayCan
    0.60 113.61 12.36 125.71 N/A N/A N/A N/A ReAct 0.57 152.18 15.05 356.03 0.34 66.00
    0.63 22.20 Reflexion 0.71 220.17 19.39 724.48 0.39 112.49 0.68 37.26
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: AlfWorld ScienceWorld HotPotQA FEVER 评估指标 SR(%) EX($) AR EX($) SR(%) EX($) SR(%)
    EX($) Z-CoT N/A N/A N/A N/A 0.01 0.95 0.39 1.07 F-CoT 0.43 98.60 16.58 272.22
    0.32 5.73 0.61 2.25 CoT-SC 0.57 105.37 15.24 274.33 0.33 7.86 0.62 3.21 SayCan
    0.60 113.61 12.36 125.71 N/A N/A N/A N/A ReAct 0.57 152.18 15.05 356.03 0.34 66.00
    0.63 22.20 Reflexion 0.71 220.17 19.39 724.48 0.39 112.49 0.68 37.26
- en: Evaluating the planning capability of the agent is a critical issue in the research
    area. Here we investigate several mainstream benchmarking methods, categorizing
    them into the following types.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 评估代理的规划能力是研究领域中的一个关键问题。在这里，我们调查了几种主流的基准方法，将其分类为以下类型。
- en: 'Interactive Gaming Environments: Game environments may provide real-time multi-modal
    feedback based on the agent’s actions, including textual and visual feedback.
    Currently, the most widely used gaming environment is Minecraft ⁵⁵5[https://www.minecraft.net/](https://www.minecraft.net/),
    where the agent needs to gather materials to create tools for obtaining more rewards.
    The quantity of tools created by the agent is often used as an evaluation metric.
    Another popular category is the text-based interactive environments, such as ALFWorld Shridhar
    et al. ([2020](#bib.bib38)), ScienceWorld Wang et al. ([2022a](#bib.bib43)), et
    al, where the agent locates in an environment described in natural language, with
    limited actions and locations. The success rate or the rewards obtained are commonly
    used as evaluation metrics. Compared with Minecraft, these text-based interactive
    environments are often simpler, with straightforward feedback and fewer feasible
    actions.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 互动游戏环境：游戏环境可能会基于代理的动作提供实时的多模态反馈，包括文本和视觉反馈。目前，最广泛使用的游戏环境是Minecraft ⁵⁵5[https://www.minecraft.net/](https://www.minecraft.net/)，在这个环境中，代理需要收集材料以创建工具，从而获得更多的奖励。代理创建的工具数量通常作为评估指标。另一类受欢迎的环境是基于文本的互动环境，如ALFWorld Shridhar
    et al. ([2020](#bib.bib38))、ScienceWorld Wang et al. ([2022a](#bib.bib43)) 等，其中代理在用自然语言描述的环境中定位，行动和位置有限。成功率或获得的奖励通常作为评估指标。与Minecraft相比，这些基于文本的互动环境通常更简单，反馈直观，且可行的动作较少。
- en: 'Interactive Retrieval Environments: Interactive retrieval environments simulate
    the process of information retrieval and reasoning that humans undergo in real
    life. In these environments, agents are often allowed to interact with search
    engines and other web services, using actions such as searching keywords or executing
    click, forward, and backward operations to acquire more information, thereby obtaining
    answers to questions or completing information retrieval tasks. Commonly used
    retrieval environments include question-answering tasks based on the Wikipedia
    engine Yao et al. ([2022](#bib.bib57)) (such as HotPotQA Yang et al. ([2018](#bib.bib52))
    and Fever Thorne et al. ([2018](#bib.bib41))) and web browsing tasks to find specific
    information, including WebShop, Mind2Web Deng et al. ([2023](#bib.bib9)), and
    WebArena Zhou et al. ([2023](#bib.bib65)). The task success rate is usually used
    as the metric.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 互动检索环境：互动检索环境模拟了人类在现实生活中进行信息检索和推理的过程。在这些环境中，代理通常可以与搜索引擎和其他网络服务进行交互，使用搜索关键词、执行点击、前进和后退等操作来获取更多信息，从而获得问题的答案或完成信息检索任务。常用的检索环境包括基于维基百科引擎的问答任务，如HotPotQA Yang
    et al. ([2018](#bib.bib52)) 和Fever Thorne et al. ([2018](#bib.bib41))，以及用于查找特定信息的网页浏览任务，如WebShop、Mind2Web Deng
    et al. ([2023](#bib.bib9)) 和WebArena Zhou et al. ([2023](#bib.bib65))。任务成功率通常作为衡量指标。
- en: 'Interactive Programming Environments: Interactive programming environments
    simulate the interaction between programmers and computers, testing the agent’s
    planning ability in solving computer-related problems. In these environments,
    agents are required to interact with computers to solve problems by writing code
    or instructions. They would receive various feedback including compile and runtime
    error messages, as well as execution results. Popular interactive programming
    environments involve issues related to operating systems, databases, etc., such
    as Agent Bench Liu et al. ([2023c](#bib.bib28)), MiniWoB++ Kim and others ([2023](#bib.bib21)).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 互动编程环境：互动编程环境模拟程序员与计算机之间的互动，测试代理在解决计算机相关问题时的规划能力。在这些环境中，代理需要通过编写代码或指令与计算机互动以解决问题。他们会收到各种反馈，包括编译和运行时错误信息以及执行结果。流行的互动编程环境涉及操作系统、数据库等相关问题，如Agent
    Bench Liu et al. ([2023c](#bib.bib28))、MiniWoB++ Kim et al. ([2023](#bib.bib21))。
- en: Most of these existing interactive environments lack fine-grained evaluation,
    where the performance is predominantly evaluated by the final success rate. Furthermore,
    unlike real-world scenarios where there are often multiple paths to complete a
    task, there is typically only one “golden” path in most simulated environments
    due to the high annotation cost.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现有的互动环境大多缺乏细粒度的评估，性能主要通过最终的成功率来评估。此外，与现实世界中的任务通常有多条完成路径不同，大多数模拟环境中由于高昂的标注成本，通常只有一条“黄金”路径。
- en: 'Experiments. We have conducted experiments on four benchmarks to validate the
    performance of representative works, shown in Table [2](#S8.T2 "Table 2 ‣ 8 Evaluation
    ‣ Understanding the planning of LLM agents: A survey"). We have implemented six
    prompt-based methods due to limited budgets, covering task decomposition, multi-path
    selection, and reflection. As for the benchmarks, ALFWorld, ScienceWorld, HotPotQA,
    and FEVER are employed, involving interactive gaming and question-answering benchmarks.
    Since ALFWorld and ScienceWorld are involved in larger action space, the zero-shot
    method, i.e. ZeroShot-CoT, is not applicable due to unawareness of action space.
    SayCan improves CoT by grounding output actions into action space with a value
    function, which does not apply to QA tasks because there are only two actions:
    Search[keyword] and Lookup[keyword]. And we set the value function as a textual
    embedding model bge-small-en-v1.5 Xiao and others ([2023](#bib.bib50)). We obtain
    3 actions and 5 answers each step for gaming tasks and QA tasks for CoT-SC, respectively.
    The round of retries in Reflexion is set to 1\. We use the API of text-davinci-003
    in OpenAI as LLM.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '实验。我们在四个基准上进行了实验，以验证代表性工作的性能，如表[2](#S8.T2 "Table 2 ‣ 8 Evaluation ‣ Understanding
    the planning of LLM agents: A survey")所示。由于预算有限，我们实现了六种基于提示的方法，涵盖了任务分解、多路径选择和反思。基准包括ALFWorld、ScienceWorld、HotPotQA和FEVER，涉及交互式游戏和问答基准。由于ALFWorld和ScienceWorld涉及更大的行动空间，零样本方法即ZeroShot-CoT由于对行动空间的不知情而不适用。SayCan通过将输出动作与具有价值函数的行动空间绑定来改进CoT，但在问答任务中并不适用，因为只有两个动作：Search[keyword]和Lookup[keyword]。我们将价值函数设置为文本嵌入模型bge-small-en-v1.5
    Xiao等人（[2023](#bib.bib50)）。我们为游戏任务和问答任务的CoT-SC每步分别获得3个动作和5个答案。Reflexion的重试轮次设置为1。我们使用OpenAI的text-davinci-003
    API作为LLM。'
- en: (i) The performance increases with the expenses. As CoT-SC, ReAct and Reflexion
    are involved in multiple plans, additional thoughts, and reflections, respectively,
    their expenses are more than their backbone methods. Intuitively, more tokens
    represent more detailed thinking, resulting in performance improvements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 性能随着开支的增加而提升。由于CoT-SC、ReAct和Reflexion涉及多个计划、附加思考和反思，它们的开支都高于其基础方法。直观上，更多的tokens代表更详细的思考，从而带来性能提升。
- en: (ii) Fewshot examples are suggested for complicated tasks. Despite that the
    magic instruction Let’s think step by step can lead to more reasoning, ZeroShot-CoT
    exhibits severe performance degradation in two QA benchmarks, which demonstrates
    the necessity of the examples for LLM to further understand the task.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 对于复杂任务，建议使用少量示例。尽管魔法指令“让我们一步一步来思考”可以导致更多的推理，但ZeroShot-CoT在两个问答基准上表现出严重的性能下降，这表明示例对于LLM进一步理解任务的必要性。
- en: (iii) Reflection plays a crucial role in improving the success rate, especially
    for complex tasks. Despite Reflexion consuming about twice the tokens compared
    with ReAct, the improvements in complicated tasks are promising, such as ALFWorld
    and ScienceWorld, which shows that LLM possesses the error-correcting capability.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 反思在提高成功率方面发挥了至关重要的作用，尤其是对于复杂任务。尽管Reflexion消耗的tokens约为ReAct的两倍，但在复杂任务中的改进是有前景的，例如ALFWorld和ScienceWorld，这表明LLM具有纠错能力。
- en: 9 Conclusions and Future Directions
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论与未来方向
- en: Since LLM has shown the emergence of intelligence, there has been an increasing
    focus on using LLM to enhance the planning capabilities of agents. The major directions
    are summarized in Figure 1, with a detailed comparison and analysis of various
    methods presented in Sections 3 to 7. We also conducted experiments on four benchmarks,
    comparing the effectiveness of several representative methods and showing that
    performance increases with expenses. Despite the enhancements made by these works
    in planning capabilities, there are still some significant challenges.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM已经展示出智能的萌芽，越来越多的人关注于利用LLM增强代理的规划能力。主要方向在图1中总结，各种方法的详细比较和分析见第3至第7节。我们还在四个基准上进行了实验，比较了几种代表性方法的有效性，显示出性能随着开支的增加而提高。尽管这些工作在规划能力上有所提升，但仍存在一些重大挑战。
- en: Hallucinations. During the planning process, LLM often suffers from hallucinations,
    leading to irrational plans, unfaithfulness to task prompts, or failing to follow
    complex instructions. For instance, plans may include actions that interact with
    items not existed in the environment. Although these issues can be alleviated
    through careful prompt engineering, they reflect fundamental shortcomings in LLM Zhang
    et al. ([2023b](#bib.bib61)); Huang et al. ([2023a](#bib.bib18)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉。在规划过程中，LLM 经常遭遇幻觉，导致计划不合理、对任务提示不忠实，或未能遵循复杂指令。例如，计划可能包括与环境中不存在的物品交互的行动。尽管通过仔细的提示工程可以缓解这些问题，但它们反映了
    LLM 的基本缺陷。Zhang 等人 ([2023b](#bib.bib61)); Huang 等人 ([2023a](#bib.bib18))。
- en: Feasibility of Generated Plans. LLM, being fundamentally based on statistical
    learning, optimizes the probability of the next word through massive data. Compared
    to symbolic artificial intelligence, this approach struggles to obey complex constraints,
    especially when dealing with less common constraints encountered during LLM training.
    Consequently, plans generated by LLM may lack feasibility without considering
    adequate preconditions. Connecting LLM with symbolic planning models without altering
    LLM itself is a promising future direction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 生成计划的可行性。LLM 基于统计学习，通过大量数据优化下一个词的概率。与符号人工智能相比，这种方法在遵守复杂约束方面存在困难，特别是在处理 LLM 训练期间遇到的少见约束时。因此，LLM
    生成的计划可能缺乏可行性，而未考虑充分的前提条件。将 LLM 与符号规划模型连接，而不改变 LLM 本身，是一个有前景的未来方向。
- en: Efficiency of Generated Plans. Generating efficient plans is a crucial issue
    in planning. However, in existing LLM agents, planning is greedily based on generated
    plans from LLM output, without considering the efficiency of the generated plans.
    Therefore, future developments may require introducing additional efficiency evaluation
    modules to work in conjunction with LLM for more efficient plans.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 生成计划的效率。在规划中，生成高效的计划是一个关键问题。然而，在现有的 LLM 代理中，规划是基于 LLM 输出生成的计划，而不考虑生成计划的效率。因此，未来的发展可能需要引入额外的效率评估模块，与
    LLM 协同工作，以实现更高效的计划。
- en: Multi-Modal Environment Feedback. LLM is originally designed for processing
    textual inputs, but real-world environment feedback is often multi-modal, including
    images, audio, etc., which are challenging to describe in natural language. Therefore,
    LLM agents face limitations when handling such scenarios. Future considerations
    may involve integrating the development of multi-modal large models and revisiting
    related planning strategies.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态环境反馈。LLM 最初是为处理文本输入而设计的，但现实世界中的环境反馈通常是多模态的，包括图像、音频等，这些难以用自然语言描述。因此，LLM 代理在处理这种场景时面临限制。未来的考虑可能包括整合多模态大型模型的发展，并重新审视相关的规划策略。
- en: 'Fine-grained Evaluation. As mentioned in Section [8](#S8 "8 Evaluation ‣ Understanding
    the planning of LLM agents: A survey"), existing benchmarks mostly rely on the
    final completion status of tasks, lacking fine-grained step-wise evaluations.
    Additionally, environmental feedback is often rule-based, simplistic, and distant
    from real-world scenarios. A potential future direction is to leverage high-intelligence
    models like LLM to design more realistic evaluation environments.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化评估。如第 [8](#S8 "8 评估 ‣ 理解 LLM 代理的规划：一项调查") 节所述，现有基准大多依赖于任务的最终完成状态，缺乏精细化的逐步评估。此外，环境反馈通常是基于规则的、简单的，远离现实世界的场景。一个潜在的未来方向是利用像
    LLM 这样的高智能模型来设计更现实的评估环境。
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aeronautiques et al. [1998] Constructions Aeronautiques, Adele Howe, et al.
    Pddl— the planning domain definition language. Technical Report, Tech. Rep., 1998.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aeronautiques 等人 [1998] Constructions Aeronautiques, Adele Howe 等人。《Pddl——规划领域定义语言》。技术报告，Tech.
    Rep., 1998。
- en: An et al. [2023] Shengnan An, Zexiong Ma, et al. Learning from mistakes makes
    llm better reasoner. arXiv preprint arXiv:2310.20689, 2023.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等人 [2023] Shengnan An, Zexiong Ma 等人。《从错误中学习使 LLM 成为更好的推理者》。arXiv 预印本 arXiv:2310.20689,
    2023。
- en: 'Besta et al. [2023] Maciej Besta, Nils Blach, et al. Graph of thoughts: Solving
    elaborate problems with large language models. arXiv preprint arXiv:2308.09687,
    2023.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等人 [2023] Maciej Besta, Nils Blach 等人。《思维图谱：使用大型语言模型解决复杂问题》。arXiv 预印本
    arXiv:2308.09687, 2023。
- en: Cai et al. [2022] Deng Cai, Yan Wang, Lemao Liu, and Shuming Shi. Recent advances
    in retrieval-augmented text generation. In SIGIR, pages 3417–3419, 2022.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人 [2022] Deng Cai, Yan Wang, Lemao Liu, 和 Shuming Shi。《检索增强文本生成的最新进展》。在
    SIGIR，页面 3417–3419，2022。
- en: 'Chen et al. [2021a] Lili Chen, Kevin Lu, et al. Decision transformer: Reinforcement
    learning via sequence modeling. NeurIPS, 34:15084–15097, 2021.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2021a] 丽丽·陈，凯文·陆等。决策变换器：通过序列建模进行强化学习。NeurIPS, 34:15084–15097, 2021。
- en: Chen et al. [2021b] Mark Chen, Jerry Tworek, et al. Evaluating large language
    models trained on code. arXiv preprint arXiv:2107.03374, 2021.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2021b] 马克·陈，杰瑞·特沃雷克等。评估训练代码的大型语言模型。arXiv 预印本 arXiv:2107.03374, 2021。
- en: 'Chen et al. [2022] Wenhu Chen, Xueguang Ma, et al. Program of thoughts prompting:
    Disentangling computation from reasoning for numerical reasoning tasks. arXiv
    preprint arXiv:2211.12588, 2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2022] 温虎·陈，薛光·马等。思想提示程序：将计算与推理分离用于数值推理任务。arXiv 预印本 arXiv:2211.12588, 2022。
- en: Dagan et al. [2023] Gautier Dagan, Frank Keller, and Alex Lascarides. Dynamic
    planning with a llm. arXiv preprint arXiv:2308.06391, 2023.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达根等人 [2023] 高蒂埃·达根，弗兰克·凯勒，和亚历克斯·拉斯卡里德斯。使用大型语言模型的动态规划。arXiv 预印本 arXiv:2308.06391,
    2023。
- en: 'Deng et al. [2023] Xiang Deng, Yu Gu, et al. Mind2web: Towards a generalist
    agent for the web. arXiv preprint arXiv:2306.06070, 2023.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '邓等人 [2023] 向邓，余古等。Mind2web: 迈向通用型网络代理。arXiv 预印本 arXiv:2306.06070, 2023。'
- en: 'Gao et al. [2023] Luyu Gao, Aman Madaan, et al. Pal: Program-aided language
    models. In ICML, pages 10764–10799, 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人 [2023] 高璐宇，阿曼·马丹等。PAL：程序辅助语言模型。In ICML, 第10764–10799页, 2023。
- en: 'Gerevini and Serina [2002] Alfonso Gerevini and Ivan Serina. Lpg: A planner
    based on local search for planning graphs with action costs. In Aips, volume 2,
    pages 281–290, 2002.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格雷维尼和塞里纳 [2002] 阿方索·格雷维尼和伊凡·塞里纳。LPG：基于局部搜索的行动成本规划图的规划器。In AIPS, 第2卷, 第281–290页,
    2002。
- en: 'Ghallab et al. [2004] Malik Ghallab, Dana Nau, et al. Automated Planning: theory
    and practice. Elsevier, 2004.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加拉布等人 [2004] 马利克·加拉布，达纳·瑙等。自动化规划：理论与实践。Elsevier, 2004。
- en: 'Gou et al. [2023] Zhibin Gou, Zhihong Shao, et al. Critic: Large language models
    can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738,
    2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人 [2023] 朱斌·高，志宏·邵等。Critic：大型语言模型可以通过工具互动批评自我纠错。arXiv 预印本 arXiv:2305.11738,
    2023。
- en: Guan et al. [2023] Lin Guan, Karthik Valmeekam, et al. Leveraging pre-trained
    large language models to construct and utilize world models for model-based task
    planning. arXiv preprint arXiv:2305.14909, 2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管等人 [2023] 林管，卡尔蒂克·瓦尔梅卡姆等。利用预训练的大型语言模型构建和利用世界模型进行基于模型的任务规划。arXiv 预印本 arXiv:2305.14909,
    2023。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, et al. Reasoning with language model is
    planning with world model. arXiv preprint arXiv:2305.14992, 2023.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郝等人 [2023] 史博·郝，易古等。与语言模型的推理即是与世界模型的规划。arXiv 预印本 arXiv:2305.14992, 2023。
- en: Haslum et al. [2019] Patrik Haslum, Nir Lipovetzky, et al. An introduction to
    the planning domain definition language, volume 13. Springer, 2019.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈斯勒姆等人 [2019] 帕特里克·哈斯勒姆，尼尔·利波韦茨基等。规划领域定义语言介绍，第13卷。Springer, 2019。
- en: He et al. [2015] Ji He, Jianshu Chen, et al. Deep reinforcement learning with
    a natural language action space. arXiv preprint arXiv:1511.04636, 2015.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贺等人 [2015] 姬贺，简舒·陈等。具有自然语言动作空间的深度强化学习。arXiv 预印本 arXiv:1511.04636, 2015。
- en: 'Huang et al. [2023a] Lei Huang, Yu Weijiang, et al. A survey on hallucination
    in large language models: Principles, taxonomy, challenges, and open questions.
    arXiv preprint arXiv:2311.05232, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2023a] 雷黄，余伟江等。关于大语言模型中幻觉的调查：原理、分类、挑战和开放问题。arXiv 预印本 arXiv:2311.05232,
    2023。
- en: 'Huang et al. [2023b] Xu Huang, Jianxun Lian, et al. Recommender ai agent: Integrating
    large language models for interactive recommendations. arXiv preprint arXiv:2308.16505,
    2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2023b] 徐黄，简寻·连等。推荐 AI 代理：集成大语言模型以实现互动推荐。arXiv 预印本 arXiv:2308.16505, 2023。
- en: Johnson et al. [2019] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale
    similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547, 2019.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约翰逊等人 [2019] 杰夫·约翰逊，马泰斯·杜兹，和厄尔维·杰戈。基于 GPU 的十亿级相似性搜索。IEEE 大数据期刊，7(3):535–547,
    2019。
- en: Kim and others [2023] Geunwoo Kim et al. Language models can solve computer
    tasks. arXiv preprint arXiv:2303.17491, 2023.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金等人 [2023] 金根宇等。语言模型可以解决计算机任务。arXiv 预印本 arXiv:2303.17491, 2023。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, et al. Large language
    models are zero-shot reasoners. NeurIPS, 35:22199–22213, 2022.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小岛等人 [2022] 小岛健志，石香·肖恩·顾等。大型语言模型是零样本推理者。NeurIPS, 35:22199–22213, 2022。
- en: Lewis et al. [2020] Patrick Lewis, Ethan Perez, et al. Retrieval-augmented generation
    for knowledge-intensive nlp tasks. NeurIPS, 33:9459–9474, 2020.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘易斯等人 [2020] 帕特里克·刘易斯，伊桑·佩雷斯等。用于知识密集型 NLP 任务的检索增强生成。NeurIPS, 33:9459–9474, 2020。
- en: 'Lin et al. [2023] Bill Yuchen Lin, Yicheng Fu, et al. Swiftsage: A generative
    agent with fast and slow thinking for complex interactive tasks. arXiv preprint
    arXiv:2305.17390, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 [2023] Bill Yuchen Lin, Yicheng Fu 等。Swiftsage: 具有快速和慢速思维的生成代理，用于复杂交互任务。arXiv
    预印本 arXiv:2305.17390，2023年。'
- en: 'Lipovetzky et al. [2014] Nir Lipovetzky, Miquel Ramirez, et al. Width and inference
    based planners: Siw, bfs (f), and probe. IPC, page 43, 2014.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lipovetzky 等 [2014] Nir Lipovetzky, Miquel Ramirez 等。宽度和推理基础的规划者: Siw, bfs
    (f), 和 probe。IPC，第43页，2014年。'
- en: 'Liu et al. [2023a] Bo Liu, Yuqian Jiang, et al. Llm+ p: Empowering large language
    models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2023a] Bo Liu, Yuqian Jiang 等。LLM+ P: 赋能大型语言模型以获得最佳规划能力。arXiv 预印本 arXiv:2304.11477，2023年。'
- en: 'Liu et al. [2023b] Lei Liu, Xiaoyan Yang, et al. Think-in-memory: Recalling
    and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719,
    2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2023b] Lei Liu, Xiaoyan Yang 等。Think-in-memory: 召回和后续思考使 LLM 具备长期记忆。arXiv
    预印本 arXiv:2311.08719，2023年。'
- en: 'Liu et al. [2023c] Xiao Liu, Hao Yu, et al. Agentbench: Evaluating llms as
    agents. arXiv preprint arXiv:2308.03688, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 [2023c] Xiao Liu, Hao Yu 等。Agentbench: 评估 LLMs 作为代理。arXiv 预印本 arXiv:2308.03688，2023年。'
- en: 'Madaan et al. [2023] Aman Madaan, Niket Tandon, , et al. Self-refine: Iterative
    refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等 [2023] Aman Madaan, Niket Tandon 等。Self-refine: 自我反馈的迭代优化。arXiv 预印本
    arXiv:2303.17651，2023年。'
- en: Mao et al. [2020] Yuning Mao, Pengcheng He, Liu, et al. Generation-augmented
    retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553,
    2020.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等 [2020] Yuning Mao, Pengcheng He, Liu 等。生成增强检索用于开放域问答。arXiv 预印本 arXiv:2009.08553，2020年。
- en: 'Packer et al. [2023] Charles Packer, Vivian Fang, et al. Memgpt: Towards llms
    as operating systems. arXiv preprint arXiv:2310.08560, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Packer 等 [2023] Charles Packer, Vivian Fang 等。Memgpt: 迈向作为操作系统的 LLMs。arXiv
    预印本 arXiv:2310.08560，2023年。'
- en: 'Pan et al. [2024] Shirui Pan, Linhao Luo, et al. Unifying large language models
    and knowledge graphs: A roadmap. TKDE, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan 等 [2024] Shirui Pan, Linhao Luo 等。统一大型语言模型和知识图谱: 一条路线图。TKDE，2024年。'
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, et al. Generative agents:
    Interactive simulacra of human behavior. In SUIST, pages 1–22, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等 [2023] Joon Sung Park, Joseph O’Brien 等。生成代理: 人类行为的互动模拟。见 SUIST，第1–22页，2023年。'
- en: Qin et al. [2023] Yujia Qin, Shengding Hu, et al. Tool learning with foundation
    models. arXiv preprint arXiv:2304.08354, 2023.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 [2023] Yujia Qin, Shengding Hu 等。基础模型的工具学习。arXiv 预印本 arXiv:2304.08354，2023年。
- en: Schraagen et al. [2000] Jan Maarten Schraagen, Susan F Chipman, et al. Cognitive
    task analysis. Psychology Press, 2000.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schraagen 等 [2000] Jan Maarten Schraagen, Susan F Chipman 等。认知任务分析。心理学出版社，2000年。
- en: 'Shen et al. [2023] Yongliang Shen, Kaitao Song, et al. Hugginggpt: Solving
    ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580,
    2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等 [2023] Yongliang Shen, Kaitao Song 等。Hugginggpt: 使用 ChatGPT 及其在 Huggingface
    的朋友解决 AI 任务。arXiv 预印本 arXiv:2303.17580，2023年。'
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, et al. Reflexion: Language
    agents with verbal reinforcement learning. In NeurIPS, 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等 [2023] Noah Shinn, Federico Cassano 等。Reflexion: 具有语言代理的言语强化学习。见 NeurIPS，2023年。'
- en: 'Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, et al. Alfworld: Aligning
    text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768,
    2020.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar 等 [2020] Mohit Shridhar, Xingdi Yuan 等。Alfworld: 将文本与具身环境对齐以进行互动学习。arXiv
    预印本 arXiv:2010.03768，2020年。'
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, et al. Progprompt: Generating
    situated robot task plans using large language models. In ICRA 2023, pages 11523–11530\.
    IEEE, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等 [2023] Ishika Singh, Valts Blukis 等。Progprompt: 使用大型语言模型生成适应场景的机器人任务计划。见
    ICRA 2023，第11523–11530页。IEEE，2023年。'
- en: Sun et al. [2023] Jiankai Sun, Chuanyang Zheng, et al. A survey of reasoning
    with foundation models. arXiv preprint arXiv:2312.11562, 2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 [2023] Jiankai Sun, Chuanyang Zheng 等。基础模型推理的调查。arXiv 预印本 arXiv:2312.11562，2023年。
- en: 'Thorne et al. [2018] James Thorne, Andreas Vlachos, et al. Fever: a large-scale
    dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355,
    2018.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thorne 等 [2018] James Thorne, Andreas Vlachos 等。Fever: 用于事实提取和验证的大规模数据集。arXiv
    预印本 arXiv:1803.05355，2018年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, et al. Llama 2: Open foundation
    and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 [2023] Hugo Touvron, Louis Martin 等。Llama 2: 开放基础和微调的聊天模型。arXiv 预印本
    arXiv:2307.09288，2023年。'
- en: 'Wang et al. [2022a] Ruoyao Wang, Peter Jansen, et al. Scienceworld: Is your
    agent smarter than a 5th grader? arXiv preprint arXiv:2203.07540, 2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2022a] Ruoyao Wang, Peter Jansen 等。Scienceworld: 你的代理比五年级学生聪明吗？arXiv
    预印本 arXiv:2203.07540, 2022。'
- en: Wang et al. [2022b] Xuezhi Wang, Jason Wei, et al. Self-consistency improves
    chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171,
    2022.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022b] Xuezhi Wang, Jason Wei 等。自一致性提高语言模型的链式思维推理。arXiv 预印本 arXiv:2203.11171,
    2022。
- en: Wang et al. [2023a] Lei Wang, Chen Ma, et al. A survey on large language model
    based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023a] Lei Wang, Chen Ma 等。基于大语言模型的自主代理调查。arXiv 预印本 arXiv:2308.11432,
    2023。
- en: 'Wang et al. [2023b] Lei Wang, Wanyu Xu, et al. Plan-and-solve prompting: Improving
    zero-shot chain-of-thought reasoning by large language models. arXiv preprint
    arXiv:2305.04091, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023b] Lei Wang, Wanyu Xu 等。计划与解决提示: 改善大语言模型的零样本链式思维推理。arXiv 预印本 arXiv:2305.04091,
    2023。'
- en: 'Wang et al. [2023c] Yancheng Wang, Ziyan Jiang, et al. Recmind: Large language
    model powered agent for recommendation. arXiv preprint arXiv:2308.14296, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人 [2023c] Yancheng Wang, Ziyan Jiang 等。Recmind: 大语言模型驱动的推荐代理。arXiv 预印本
    arXiv:2308.14296, 2023。'
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, et al. Chain-of-thought prompting
    elicits reasoning in large language models. NeurIPS, 35:24824–24837, 2022.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2022] Jason Wei, Xuezhi Wang 等。链式思维提示在大语言模型中引发推理。NeurIPS, 35:24824–24837,
    2022。
- en: 'Wu et al. [2023] Chenfei Wu, Shengming Yin, et al. Visual chatgpt: Talking,
    drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671,
    2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 [2023] Chenfei Wu, Shengming Yin 等。视觉 ChatGPT: 通过视觉基础模型进行对话、绘画和编辑。arXiv
    预印本 arXiv:2303.04671, 2023。'
- en: 'Xiao and others [2023] Shitao Xiao et al. C-pack: Packaged resources to advance
    general chinese embedding, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 和其他人 [2023] Shitao Xiao 等。C-pack: 包装资源以推进通用中文嵌入，2023。'
- en: 'Xiao and Wang [2023] Hengjia Xiao and Peng Wang. Llm a*: Human in the loop
    large language models enabled a* search for robotics. arXiv preprint arXiv:2312.01797,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 和 Wang [2023] Hengjia Xiao 和 Peng Wang。LLM A*: 人工环节的语言模型使 A* 搜索在机器人学中成为可能。arXiv
    预印本 arXiv:2312.01797, 2023。'
- en: 'Yang et al. [2018] Zhilin Yang, Peng Qi, et al. Hotpotqa: A dataset for diverse,
    explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2018] Zhilin Yang, Peng Qi 等。Hotpotqa: 用于多跳问答的多样化、可解释的数据集。arXiv 预印本
    arXiv:1809.09600, 2018。'
- en: 'Yang et al. [2023a] Sherry Yang, Nachum Ofir, et al. Foundation models for
    decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2023a] Sherry Yang, Nachum Ofir 等。决策制定的基础模型: 问题、方法和机会。arXiv 预印本 arXiv:2303.04129,
    2023。'
- en: Yang et al. [2023b] Zhun Yang, Adam Ishay, and Joohyung Lee. Coupling large
    language models with logic programming for robust and general reasoning from text.
    arXiv preprint arXiv:2307.07696, 2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023b] Zhun Yang, Adam Ishay 和 Joohyung Lee。将大语言模型与逻辑编程相结合以实现稳健和通用的文本推理。arXiv
    预印本 arXiv:2307.07696, 2023。
- en: 'Yao et al. [2020a] Shunyu Yao, Rohan Rao, et al. Keep calm and explore: Language
    models for action generation in text-based games. arXiv preprint arXiv:2010.02903,
    2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2020a] Shunyu Yao, Rohan Rao 等。保持冷静并探索: 文本游戏中的语言模型生成行动。arXiv 预印本 arXiv:2010.02903,
    2020。'
- en: 'Yao et al. [2020b] Shunyu Yao, Rohan Rao, et al. Keep calm and explore: Language
    models for action generation in text-based games. arXiv preprint arXiv:2010.02903,
    2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2020b] Shunyu Yao, Rohan Rao 等。保持冷静并探索: 文本游戏中的语言模型生成行动。arXiv 预印本 arXiv:2010.02903,
    2020。'
- en: 'Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, et al. React: Synergizing reasoning
    and acting in language models. arXiv preprint arXiv:2210.03629, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2022] Shunyu Yao, Jeffrey Zhao 等。React: 在语言模型中协同推理与行动。arXiv 预印本 arXiv:2210.03629,
    2022。'
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, et al. Tree of thoughts: Deliberate
    problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2023] Shunyu Yao, Dian Yu 等。思想树: 使用大语言模型的深思熟虑问题解决。arXiv 预印本 arXiv:2305.10601,
    2023。'
- en: 'Zeng et al. [2023] Aohan Zeng, Mingdao Liu, et al. Agenttuning: Enabling generalized
    agent abilities for llms. arXiv preprint arXiv:2310.12823, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 等人 [2023] Aohan Zeng, Mingdao Liu 等。Agenttuning: 为 LLM 启用通用代理能力。arXiv
    预印本 arXiv:2310.12823, 2023。'
- en: Zhang et al. [2023a] Danyang Zhang, Lu Chen, et al. Large language model is
    semi-parametric reinforcement learning agent. arXiv preprint arXiv:2306.07929,
    2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023a] Danyang Zhang, Lu Chen 等。大语言模型是半参数化的强化学习代理。arXiv 预印本 arXiv:2306.07929,
    2023。
- en: 'Zhang et al. [2023b] Yue Zhang, Yafu Li, et al. Siren’s song in the ai ocean:
    A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023b] 岳张、李亚福等人。《人工智能海洋中的塞壬之歌：大语言模型中的幻觉调查》。arXiv 预印本 arXiv:2309.01219,
    2023。
- en: Zhao et al. [2023a] Wayne Xin Zhao, Kun Zhou, et al. A survey of large language
    models. arXiv preprint arXiv:2303.18223, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2023a] 赵新、周坤等人。《大语言模型的调查》。arXiv 预印本 arXiv:2303.18223, 2023。
- en: Zhao et al. [2023b] Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models
    as commonsense knowledge for large-scale task planning. arXiv preprint arXiv:2305.14078,
    2023.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2023b] 赵子锐、李伟孙和大卫·许。《将大语言模型作为大规模任务规划的常识知识》。arXiv 预印本 arXiv:2305.14078,
    2023。
- en: 'Zhong et al. [2023] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang.
    Memorybank: Enhancing large language models with long-term memory. arXiv preprint
    arXiv:2305.10250, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人 [2023] 钟万军、郭良红、高琪琪和王艳林。《记忆银行：通过长期记忆增强大语言模型》。arXiv 预印本 arXiv:2305.10250,
    2023。
- en: 'Zhou et al. [2023] Shuyan Zhou, Frank F Xu, et al. Webarena: A realistic web
    environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人 [2023] 周淑燕、弗兰克·F·徐等人。《Webarena：构建自主代理的现实网络环境》。arXiv 预印本 arXiv:2307.13854,
    2023。
