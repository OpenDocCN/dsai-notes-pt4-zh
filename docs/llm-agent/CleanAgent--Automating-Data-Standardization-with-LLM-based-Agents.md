<!--yml
category: 未分类
date: 2025-01-11 12:46:46
-->

# CleanAgent: Automating Data Standardization with LLM-based Agents

> 来源：[https://arxiv.org/html/2403.08291/](https://arxiv.org/html/2403.08291/)

Danrui Qi, Jiannan Wang Simon Fraser University
{dqi, jnwang}@sfu.ca

###### Abstract.

Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplifying the LLM’s code generation with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent , data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process. To demonstrate the practical utility of CleanAgent , it has been developed as a user-friendly web application, allowing VLDB attendees to interact with it using real-world datasets.

## 1\. Introduction

Data standardization, which is pivotal in the realm of data science, aims to transform heterogeneous data formats within a single column into an unified data format. This crucial data preprocessing step is essential for enabling effective data integration, data analysis, and decision-making.

Example 1. We illustrate the data standardization task in Figure [1](https://arxiv.org/html/2403.08291v2#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ CleanAgent: Automating Data Standardization with LLM-based Agents"). Given an input table $T$ which needs to be standardized, it is obvious that data in the “Admission Date” column and the “Address” column are with different formats. Data in the three cells of “Admission Date” column includes two different date formats. The goal of data standardization is to unify the format of data in each column in $T$ meeting data scientists’ requirements and to get the standardized table $T^{\prime}$. In Figure [1](https://arxiv.org/html/2403.08291v2#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ CleanAgent: Automating Data Standardization with LLM-based Agents"), the data scientists input their requirement to standardize “Admission Date” with the “MM/DD/YYYY HH:MM:SS” format. And in $T^{\prime}$, data in the three cells of “Admission Date” column follows only one date format, i.e. the “MM/DD/YYYY HH:MM:SS” format.

Traditionally, data scientists heavily rely on libraries such as Pandas (McKinney et al., [2024](https://arxiv.org/html/2403.08291v2#bib.bib3)) for data standardization tasks. Even though Pandas is a powerful tool, achieving data standardization often requires writing hundreds or thousands of lines of code. The standardization process for single column involves identifying the column type, applying intricate methods such as regular expressions to each cell within this column for validation, and converting each cell into desired formats. Moreover, a table may contain multiple columns, each possibly of a different type, requiring bespoke standardization code for each column type.

Example 2. Still considering the data standardization task in Figure [1](https://arxiv.org/html/2403.08291v2#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ CleanAgent: Automating Data Standardization with LLM-based Agents"). For standardizing “Admission Date” and “Address” , data scientists need to write the datetime standardization code for “Admission Date” and address standardization code for “Address” by using regex. The example standardization code for “Address” is shown as follows.

[⬇](data:text/plain;base64,ZGVmIHN0YW5kYXJkaXplX2FkZHJlc3MoYWRkcik6CiAgICAjIEV4dHJhY3Qgc3RyZWV0IG51bWJlciBhbmQgc3RyZWV0IG5hbWUKICAgIHN0cmVldCA9IHBkLlNlcmllcyhhZGRyKS5zdHIuZXh0cmFjdChyJyhcZCsgW14sXSspJykuc3F1ZWV6ZSgpCiAgICAjIEV4dHJhY3Qgc3RhdGUgbmFtZQogICAgc3RhdGUgPSAiTEEiCiAgICAjIEV4dHJhY3QgemlwY29kZQogICAgemlwY29kZSA9IHBkLlNlcmllcyhhZGRyKS5zdHIuZXh0cmFjdChyJyhcZHs1fSknKS5zcXVlZXplKCkKICAgICMgT3V0cHV0IHN0YW5kYXJkaXplZCBhZGRyZXNzCiAgICByZXR1cm4gZiJ7c3RyZWV0fSwge3N0YXRlfSwge3ppcGNvZGV9Ig==)1def  standardize_address(addr):2  #  Extract  street  number  and  street  name3  street  =  pd.Series(addr).str.extract(r’(\d+  [^,]+)’).squeeze()4  #  Extract  state  name5  state  =  "LA"6  #  Extract  zipcode7  zipcode  =  pd.Series(addr).str.extract(r’(\d{5})’).squeeze()8  #  Output  standardized  address9  return  f"{street},  {state},  {zipcode}"

If the input table $T$ has other column types such as email and IP addresses, data scientists also need to write standardization code specially for email type and IP address type, which is time-consuming.

Recently, the emergence of LLMs, especially ChatGPT, has shown potential in revolutionizing this process. By leveraging their natural language understanding and code generation ability, these models could significantly aid data scientists by autonomously generating standardization code in response to conversational prompts. However, this method still necessitates detailed prompt crafting and often involves multi-turn dialogues (Chen et al., [2023](https://arxiv.org/html/2403.08291v2#bib.bib2)) for standardizing column types in $T$ one by one, which limits efficiency and practicality.

Figure 1\. An example of automatic data standardization process with CleanAgent.

To overcome these limitations, our key idea is to introduce a Python library involving declarative and unified APIs specifically designed for standardizing different column types. This idea simplifies the LLM’s task to converting natural language (NL) instructions into succinct, declarative API calls. Such an approach simplifies the LLM’s code generation process for data standardization, requiring just a few lines of code.

This above idea, however, introduces two primary challenges. The first challenge (C1) is the design of the declarative and unified APIs for data standardization, ensuring it can effectively reduce the intricacies involved in standardizing specific column types (ideally one line of code per column type). The second challenge (C2) centers on optimizing the interaction between data scientists and LLMs. Our goal is to minimize human involvement, ideally allowing data scientists to input their standardization requirements in one instance, thereby enabling an autonomous and hands-off data standardization process.

To solve C1, we propose the type-specific Clean module in [Dataprep Library](https://github.com/sfu-db/dataprep), named [Dataprep.Clean](https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean). By observing the common steps of data standardization for specific column types, we design unified APIs clean_type(df, column_name, target_format), where the type represents the desired standardization type, such as date, address and phone, etc. These unified APIs offer enhanced expressiveness compared to raw Pandas code, reducing the complexity of standardizing specific column types. With Dataprep.Clean, data scientists can standardize one column type with only one line of code.

To solve C2, we propose the CleanAgent framework which automates data standardization with Dataprep.Clean and LLM-based Agents  (Xi et al., [2023](https://arxiv.org/html/2403.08291v2#bib.bib5); Wu et al., [2023](https://arxiv.org/html/2403.08291v2#bib.bib4)). Once users have entered their final goals, the LLM-based Agents can free their hands, autonomously generate thoughts and execute particular tasks. CleanAgent leverages both the capabilities of both Dataprep.Clean and LLM-based Agents. Data scientists only need to input the table that needs to be standardized and their requirements, CleanAgent will complete the data standardization process automatically with three steps: annotating the type of each column, generating concise Python code for standardization and executing the generated Python code.

Example 3. Continuing with Example 1\. Given an input table $T$ which needs to be standardized and the data scientists’ requirements, the CleanAgent first recognizes that the “Admission Date” column belongs to the date type, and the “Address” column belongs to the address type. According to the column-type annotation results, the CleanAgent generates and executes Python code for standardization by calling “clean_date” and “clean_address” functions, then returns the standardized table $T^{\prime}$.

CleanAgent is built as a web application. We allow the VLDB attendees to choose sample data and communicate with CleanAgent for standardization. We provide the demonstration video which can be found on [Youtube](https://youtu.be/fSYXVM6qeqM).

To summarize, we make the following contributions: (1) We propose Dataprep.Clean, an open-sourced library showing reducing the complexity of implementing standardization for specific column types with type-specific standardization functions. (2) We propose CleanAgent , which automates the data standardization process by combining both the advantages of Dataprep.Clean and LLM-based Agents. (3) We deploy CleanAgent as a web application with user-friendly interface and demonstrate its utility. We also open-sourced the implementation of CleanAgent on [Github](https://github.com/sfu-db/CleanAgent).

## 2\. Type-Specific Standardization API Design

In this section, we first describe the common steps of data standardization. Then we introduce the type-specific API design of Dataprep.Clean.

Common Steps of Data Standardization. Inspired by the steps human standardizing data cell, we identify three common steps of data standardization. We take the datetime column type as an example to illustrate these steps.

Assume a data scientist is dealing with an datetime column including two records ”Thu Sep 25 10:36:28 2003” and ”1996.07.10 AD at 15:08:56”. The data scientist wants to unify the chaotic column into a target format ”YYYY-MM-DD hh:mm:ss”.

(1) Split. At the beginning, the data scientist needs to split the datetime string into several single parts which include one kind of specific information. In our example, the data scientist can get several tokens {’Thu’,’Sep’,’25’,’10’,’36’,’28’,’2003’} from the first record by using space and colon as separators. Different type has its own splitting strategy which is not always splitting into tokens. For example, the data scientist will split the email string into the username part and the domain part.

(2) Validate. Standardization can only be performed on valid inputs. Thus the second step should be validation. For example, if the string ”little cat” is an instance of the datetime column, this string is invalid and the data scientist will transform it to a default value like NaN. Intuitively, a string is valid means that each part of this string after splitting is valid. Hence, validation of each split part is important. Usually, the data scientist will recognize and validate each part by their domain knowledge, some corpus or some rules. If every split part is valid, the string is also valid. For instance, the token ’Sep’ can be recognized as a valid representation of month, and ’2003’ can be recognized as a valid year.

(3) Transform. The last step of standardization is to transform each split part and combine them into target format. In our example, because the target format is ”YYYY-MM-DD hh:mm:ss”, the month Sep is transformed into number 09 and recombined with other parts to the target "2003-09-25 10:36:28".

Figure 2\. Basic Structure of LLM-based Agent.

The Design of Unified APIs. The goal of our API design is to enable data scientists to complete all the common steps of data standardization of one column with a single function call. The simplicity and consistency are considerdered as the pinciple of API design. The observation on the common steps of data standardization brings the type-specific API design idea. Thus, we design the API in the following form:

|  | clean_type(df, column_name, target_format) |  |

Figure 3\. The Workflow of CleanAgent.

where clean_type is the function name, type represents the type of the current column. The first argument df represents the input DataFrame, the second argument column_name is the column needs to be standardized and the third argument target_type is the target standardization format users specified. Our APIs design is flexible and extensible, which is convenient for users to add new standardization functions dealing with new data types. Currently, we have 142 standardization functions in Dataprep.Clean, representing 142 data types.

## 3\. CleanAgent Workflow

In this section, we first introduce the basic structure of the LLM-based agent. Then we describe the CleanAgent workflow constructed by four agents. The automatic data standardization process can be completed by the cooperation of the four agents in CleanAgent .

Basic Structure of LLM-based Agent. According to the previous surveys on LLM-based Agent (Xi et al., [2023](https://arxiv.org/html/2403.08291v2#bib.bib5)), we conclude the basic structure of LLM-based Agent shown in Figure [2](https://arxiv.org/html/2403.08291v2#S2.F2 "Figure 2 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents"). An LLM-based agent includes four main components: (1) an LLM used to generate replies for input information (2) a memory used to store historical conversation messages (3) a system message defining the role of the agent (4) a set of external tools which can be called by the LLM-based agent to complete specific tasks, such as web searching, code running, etc.

Detailed Workflow. The detailed workflow of CleanAgent is shown in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents"). The CleanAgent is composed of four agents, including a Chat Manager, a Column-type Annotator, a Python Programmer and a Code Executor. They can communicate with each other and automatically complete the data standardization process by cooperation. As mentioned in Figure [2](https://arxiv.org/html/2403.08291v2#S2.F2 "Figure 2 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents"), each agent has its own memory to store the historical conversational messages between it and other agents. It is important to highlight that the memory of the Chat Manager is uniquely comprehensive, encompassing the entire historical conversational messages from all agents within the CleanAgent system. This extensive memory enables every agent in the CleanAgent to generate responses that are informed by the complete historical messages.

Figure 4\. User interface of CleanAgent.

The input of CleanAgent includes a table $T$ that needs to be standardized. Data scientists can also input extra requirements such as “the format of the date type column should be MM/DD/YYYY”. By receiving the input table and data scientists’ extra requirements, CleanAgent stores these information into the Chat Manager’s memory and starts to complete the data standardization process. The Chat Manager delivers messages in its memory to the Column-type Annotator(① in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")). Then The Column-type Annotator receives the table information and leverages LLMs to annotate the type of each column in the input table. If the The Column-type Annotator cannot figure out the specific type of one column, the Column-type Annotator outputs “I do not know”. The annotation result is returned to the Chat Manager and stored in the Chat Manager’s memory (② in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")). Thirdly, The Python Programmer receives historical messages from the Chat Manager including the column-type annotation results (③ in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")), picks up the corresponding clean functions and generates Python code for the data standardization process. The generated Python code is also returned to the Chat Manager and stored in the Chat Manager’s memory (④ in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")). Finally, The Code Executor receives historical messages from the Chat Manager including the column-type annotation results and the generated Python code (⑤ in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")), then executes the generated Python code. If the generated code executes without errors, the standardized table $T^{\prime}$ is returned. If the generated code executes with errors, the error message is returned to the Chat Manager and stored in its memory (⑥ in Figure [3](https://arxiv.org/html/2403.08291v2#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")). Then CleanAgent will retry the whole workflow until it can complete the data standardization process successfully.

## 4\. Demonstration Scenarios

The demonstration setup includes a table need to be standardized and a laptop. The laptop must connect to the Internet for visitors can use CleanAgent smoothly with OpenAI’s GPT service. If the conference Internet fails, a mobile hotspot (established via cell phone) can also be used for running CleanAgent .

Figure [4](https://arxiv.org/html/2403.08291v2#S3.F4 "Figure 4 ‣ 3\. CleanAgent Workflow ‣ CleanAgent: Automating Data Standardization with LLM-based Agents") shows the user interface of CleanAgent . As area ① shows, users must first upload a CSV file that needs to be cleaned. Then CleanAgent receives and shows the basic information of the uploaded file (number of rows and number of columns). If users want CleanAgent to start the data standardization process, they should just click “Start Standardization” button.

After clicking the “Start Standardization” button, as area ② shows, the User_Proxy generates three detailed steps to complete the data standardization task. Firstly, the Column-type Annotator receives messages from the Chat Manager, annotates and outputs the type of each column, as area ③ shows. Then the Python Programmer picks up standardization functions from Dataprep.Clean with respect to the type of each column, and write proper Python code to standardize columns in the input table, as area ④ shows. Thirdly, the Code Executor executes the Python code generated by the Python Programmer and collects the execution messages, as area ⑤ shows. If the Code Executor gets the error message when executing generated Python code, the error message is sent to the Chat Manager and becomes part of the prompt of the next try. If the Code Executor gets the message of successful execution, CleanAgent will report that the data standardization is completed, as area ⑥ shows. Moreover, users can click the “Show Cleaned Table” button to check whether the standardized table matches users’ requirements. If yes, users can directly download the standardized table. Otherwise, users can input their extra requirements with natural language. CleanAgent will start a new data standardization process according to users’ input.

## 5\. Conclusion

In this demo paper, we proposed CleanAgent to automate the data standardization process with Dataprep.Clean and LLM-based Agents. We implemented CleanAgent as a web service to visualize the conversations among agents. Other tasks in data science life cycle such as data cleaning and data visualization can also be completed by LLM-based agents (Xue et al., [2023](https://arxiv.org/html/2403.08291v2#bib.bib6)).

## References

*   (1)
*   Chen et al. (2023) Sibei Chen, Hanbing Liu, Weiting Jin, Xiangyu Sun, Xiaoyao Feng, Ju Fan, Xiaoyong Du, and Nan Tang. 2023. ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions. *CoRR* abs/2304.03540 (2023). [https://doi.org/10.48550/ARXIV.2304.03540](https://doi.org/10.48550/ARXIV.2304.03540) arXiv:2304.03540
*   McKinney et al. (2024) Wes McKinney et al. 2024. pandas: powerful Python data analysis toolkit. [https://pandas.pydata.org/](https://pandas.pydata.org/) Accessed: 2024-01-25.
*   Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. *CoRR* abs/2308.08155 (2023). [https://doi.org/10.48550/ARXIV.2308.08155](https://doi.org/10.48550/ARXIV.2308.08155) arXiv:2308.08155
*   Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: A Survey. *CoRR* abs/2309.07864 (2023). [https://doi.org/10.48550/ARXIV.2309.07864](https://doi.org/10.48550/ARXIV.2309.07864) arXiv:2309.07864
*   Xue et al. (2023) Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, et al. 2023. Db-gpt: Empowering database interactions with private large language models. *arXiv preprint arXiv:2312.17449* (2023).