<!--yml
category: 未分类
date: 2025-01-11 12:23:02
-->

# The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies

> 来源：[https://arxiv.org/html/2407.19354/](https://arxiv.org/html/2407.19354/)

Feng He [Feng.He-2@student.uts.edu.au](mailto:Feng.He-2@student.uts.edu.au) University of Technology SydneyAustralia ,  Tianqing Zhu [tqzhu@cityu.edu.mo](mailto:tqzhu@cityu.edu.mo) City University of MacauChina ,  Dayong Ye [Dayong.ye@uts.edu.au](mailto:Dayong.ye@uts.edu.au) University of Technology SydneyAustralia ,  Bo Liu [Bo.liu@uts.edu.au](mailto:Bo.liu@uts.edu.au) University of Technology SydneyAustralia ,  Wanlei Zhou [wlzhou@cityu.edu.mo](mailto:wlzhou@cityu.edu.mo) City University of MacauChina  and  Philip S. Yu [psyu@UIC.edu](mailto:psyu@UIC.edu) University of Illinois at ChicagoUS(2018)

###### Abstract.

Inspired by the rapid development of Large Language Models (LLMs), LLM agents have evolved to perform complex tasks. LLM agents are now extensively applied across various domains, handling vast amounts of data to interact with humans and execute tasks. The widespread applications of LLM agents demonstrate their significant commercial value; however, they also expose security and privacy vulnerabilities. At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed. This survey aims to provide a comprehensive overview of the newly emerged privacy and security issues faced by LLM agents. We begin by introducing the fundamental knowledge of LLM agents, followed by a categorization and analysis of the threats. We then discuss the impacts of these threats on humans, environment, and other agents. Subsequently, we review existing defensive strategies, and finally explore future trends. Additionally, the survey incorporates diverse case studies to facilitate a more accessible understanding. By highlighting these critical security and privacy issues, the survey seeks to stimulate future research towards enhancing the security and privacy of LLM agents, thereby increasing their reliability and trustworthiness in future applications.

Large Language Models, LLM Agent, Security, Privacy preservation, Defense^†^†copyright: acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†journal: POMACS^†^†journalvolume: 37^†^†journalnumber: 4^†^†article: 111^†^†publicationmonth: 8\acmArticleType

Review

## 1\. Introduction

Large Language Model (LLM) agents are sophisticated AI systems built upon large language models like GPT 4 (OpenAI et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib68)), Claude 3 (Int, [2024b](https://arxiv.org/html/2407.19354v1#bib.bib7)) and Llama 3 (Int, [2024a](https://arxiv.org/html/2407.19354v1#bib.bib6)). These agents leverage the vast amounts of text data on which they are trained to perform a variety of tasks, ranging from natural language understanding and generation to more complex activities such as decision-making, problem-solving, and interacting with users in a human-like manner (Wang et al., [2023c](https://arxiv.org/html/2407.19354v1#bib.bib96)). LLM agents are accessible in numerous applications, including virtual assistants, customer service bots, and educational tools, due to their ability to understand and generate human language at an advanced level (Dong et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib23); Wang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib100); Yang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib116)).

The importance of LLM agents lies in their potential to transform various industries by automating tasks that require human-like understanding and interaction. They can enhance productivity, improve user experiences, and provide personalized assistance. Moreover, their ability to learn from vast amounts of data enables them to continuously improve and adapt to new tasks, making them versatile tools in the rapidly evolving technological landscape (Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)).

To visualize how LLM agents can be integrated into practical scenarios, consider the example illustrated in Figure [1](https://arxiv.org/html/2407.19354v1#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"). This figure presents a pixelated virtual town to simulate an LLM agent application. The town includes gathering places found in real life, such as stores, offices, restaurants, museums, and parks. Each LLM agent acts as an independent resident, playing various roles and serving different functions, closely resembling the behaviors of real humans in a community. These agents can either be manually controlled to interact with specific characters and accomplish tasks, or they can operate autonomously, following their own plans and acquiring new knowledge through interactions within the virtual community.

![Refer to caption](img/a36f43001ddad9c7a5664441e6c76bc0.png)

Figure 1\. Overview of the pixelated virtual town

\Description

[]

The deployment of LLM agents has led to a wide user base and high commercial value due to their extensive application in various fields. Given that LLM agents are still in their early stages, their significant commercial and application values make them attractive targets for attackers. However, since LLM agents are built on LLMs, they are susceptible to attacks targeting LLMs. For example, jailbreaking attacks can bypass the security and censorship features of LLMs, generating controversial responses. This threat is inherited by LLM agents, enabling attackers to employ various methods to execute jailbreaking attacks on agents. However, unlike static LLMs, LLM agents possess dynamic capabilities, such that their immediate responses can influence future decisions and actions, thereby posing more widespread risks. Moreover, the unique functionalities of LLM agents, such as their ability to think and utilize tools during task execution, expose them to specific attacks targeting agents. For example, when LLM agents employ external tools, attackers can manipulate the functionalities of these tools to compromise user privacy or execute malicious code. Depending on the application domain of the agent, such attacks could pose serious threats to physical security, financial security, or overall system integrity.

This paper categorizes the security threats faced by LLM agents into inherited LLM attacks and unique agent-specific threats. The threats inherited from LLMs can be further divided into technical vulnerabilities and intentional malicious attacks. Technical vulnerabilities include issues like hallucinations, catastrophic forgetting, and misunderstandings (Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)), which arise from the initial model creation and are influenced by the model’s structure. These vulnerabilities can lead to incorrect outputs being observed by users over prolonged use of LLM agents, affecting user trust and decision-making processes. Moreover, technical vulnerabilities can provide opportunities for malicious attacks. Currently, malicious attacks targeting LLMs include data theft and responses tampering, such as data extraction attacks and a series of tuned instructional attacks (Yao et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib120)).

For the specific threats targeting LLM agents, we are inspired by the workflow of LLM agents, which involves agent thought, action, and perception (Huang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib41)). The threats can be categorized into knowledge poisoning, functional manipulation, and output manipulation. Knowledge poisoning involves contaminating the training data and knowledge base of the LLM agent, leading to the deliberate incorporation of malicious data by creator. This can easily deceive users with harmful information and even steer them towards malicious behavior. Output manipulation interferes with the content of the agent’s thought and perception stages, influencing the final output. This can cause users to receive biased or deceptive information, crafted to mislead them. Functional manipulation exploits the interfaces and tools used by LLM agents to perform unauthorized actions such as third-party data theft or executing malicious code.

Research on LLM agents is still in its early stage. Current studies mainly focus on attacks targeting LLMs, while lacking comprehensive reviews that discuss the security and privacy issues specific to the agents, which present more complex scenarios. The motivation for conducting this survey is to provide a comprehensive overview of the privacy and security issues associated with LLM agents, helping researchers to understand and mitigate the associated threats.

This survey aims to:

*   •

    Highlight Current Threats: Identify and categorize the emerging threats faced by LLM agents.

*   •

    Explore Real-World Impact: Elaborate on the impacts of these threats by considering real-world scenarios involving humans, environment, and other agents.

*   •

    Analyze Mitigation Strategies: Discuss existing strategies to mitigate these threats, ensuring the responsible development and deployment of LLM agents.

*   •

    Inform Future Research: Serve as a foundation for future research efforts aimed at enhancing the privacy and security of more advanced architectures and applications of LLM agents.

By addressing these aspects, this survey seeks to provide a thorough understanding of the unique challenges posed by LLM agents and contribute to the development of safer and more reliable Artificial General Intelligence (AGI) systems.

The rest of this paper is structured as follows: Section [2](https://arxiv.org/html/2407.19354v1#S2 "2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") will delve into the fundamental aspects of LLM agents, including their definition, structure, and capability. Section [3](https://arxiv.org/html/2407.19354v1#S3 "3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") will identify and categorizes the emerging threats faced by LLM agents. It discusses both inherited threats from the underlying LLMs and unique agent-specific threats, providing detailed examples and scenarios for each category. Section [4](https://arxiv.org/html/2407.19354v1#S4 "4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") will elaborate on the real-world impacts of the threats. It explores how these threats affect users, environments, and other agents, highlighting the potential consequences of unmitigated risks. Section [5](https://arxiv.org/html/2407.19354v1#S5 "5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") will review existing mitigation strategies and solutions to address the mentioned threats. Section [6](https://arxiv.org/html/2407.19354v1#S6 "6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") will discuss gaps in current research and suggests future trends. Section [7](https://arxiv.org/html/2407.19354v1#S7 "7\. Conclusion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") will conclude the article.

## 2\. Foundation of LLM Agent

In this section, we delve into the foundational aspects of LLM agents, exploring their definition, structure, and capabilities. This exploration is pivotal in understanding the nature of LLM agents.

### 2.1\. Definition of LLM Agent

LLM technology continues to advance, the functionality of chatbots, such as ChatGPT (Cha, [2022](https://arxiv.org/html/2407.19354v1#bib.bib2)), Gemini  (Gem, [2023](https://arxiv.org/html/2407.19354v1#bib.bib3)), Bing Chat  (Peters, [2023](https://arxiv.org/html/2407.19354v1#bib.bib74)), has significantly expanded beyond basic question-and-answer formats, embracing a wider array of capabilities. This evolution necessitates a broader, more general definition for LLM agents. An LLM agent is an artificial intelligence system that utilizes an LLM as its core computational engine to exhibit capabilities beyond text generation, including conducting conversations, completing tasks, reasoning, and can demonstrate some degree of autonomous behaviour (Wha, [2023](https://arxiv.org/html/2407.19354v1#bib.bib5)).

These agents exhibit remarkable human-like behaviors and cooperative capabilities, marked by their proficiency in engaging in multi-agent conversation and adapting to diverse environmental interactions. They are adept at processing human instructions, formulating intricate strategies, and autonomously implementing solutions (Wang et al., [2023d](https://arxiv.org/html/2407.19354v1#bib.bib97)).

![Refer to caption](img/fb64ed9689353e51ea34e74a44742d80.png)

Figure 2\. The Structure of LLM Agent

### 2.2\. Structure of LLM Agent

LLM agents are complex systems that integrate various components to perform a wide range of functions, from simple text generation to engaging in dialogues, completing tasks, reasoning, and demonstrating a degree of autonomous behavior. The diagram illustrates the typical structure of an LLM agent, highlighting the connections between its key components and optional components. These components advance LLMs from passive text generators to active, semi-autonomous LLM agents.

As illustrated in Figure [2](https://arxiv.org/html/2407.19354v1#S2.F2 "Figure 2 ‣ 2.1\. Definition of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), an LLM agent comprises several components, with the LLM engine serving as the core. Other components are utilized by the LLM engine to perform various tasks. A basic agent capable of understanding instructions, demonstrating skills, and collaborating with humans can be constructed with three main components: LLM Engine, Instruction, and Interface. When additional optional components are integrated, the system can evolve into a more advanced task-oriented agent or a conversational agent (Yang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib116)).

*   •

    LLM Engine is the core component of an LLM agent, responsible for natural language processing and generation tasks. It is a sophisticated neural network that has been extensively trained on large datasets, equipping it with powerful text generation and comprehension capabilities. The scale and architecture of the LLM determine the foundational abilities of the agent to learn and perform language tasks  (Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)).

*   •

    Instruction serves as explicit directives, specifying the steps to complete specific tasks. This includes the characteristics of expected output, such as formatting, content requirements, and any content limitations. Essentially, instruction functions as a principle that guides the operational approach of LLM agents, facilitating task decomposition, generating chain of thought, and reflecting on past action (Zheng et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib134)).

*   •

    Interface is a connection that facilitates interaction between an LLM agent and users, other agents, or systems. It ensures the exchange of input prompts and agent outputs, thereby enabling the effective transmission of response information and inquiry requests (Wang et al., [2023d](https://arxiv.org/html/2407.19354v1#bib.bib97)).

*   •

    Personality is a component that defines the tone, style, and interaction manner of an LLM agent. For instance, a tour guide or customer service agent needs to adopt a specific role and perform dialogue tasks in an appropriate manner. In the task of exploring human communities through LLM agent-based societies, agents also need to be endowed with distinct personality traits such as being outgoing, polite, or knowledgeable. Personality assists in simulating realistic emotional expressions and behavioral logic, thereby enabling agents to interact with users and perform tasks consistently and uniquely (Abdelnabi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib8)).

*   •

    Tools are external services utilized by the LLM agent to perform specific tasks or to extend its functionality. The integration of tools assists the LLM agent in enhancing its capabilities to execute more complex tasks, such as computation or data analysis (Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)).

*   •

    Knowledge is the database of information utilized by the LLM agent. It extends the content embedded in the model’s parameters and can include commonsense knowledge, specialized knowledge, and other forms of information, enhancing the agent’s understanding and discussion capabilities in specific tasks (Mendis et al., [2007](https://arxiv.org/html/2407.19354v1#bib.bib65)).

*   •

    Memory enables the LLM agent to store and recall information from past interactions. This capability is particularly beneficial in future tasks, helping to retain context and ensure consistency and continuity in interactions, thereby enhancing the overall effectiveness of LLM agents in various applications  (Zhong et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib135)).

### 2.3\. Capability of LLM Agent

LLM agents harness the inherent language understanding abilities of large language models to interpret instructions, context, and objectives, enabling both autonomous and semi-autonomous functions based on human prompts.

*   •

    Tool Utilization. LLM agents are adept at using a range of tools, including external services and APIs. This allows them to gather necessary information and efficiently execute tasks beyond mere language processing (Bran et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib13)).

*   •

    Advanced Reasoning. Employing advanced prompt engineering concepts such as chain-of-thought and tree-of-thought reasoning, LLM agents can make logical connections to derive conclusions and solve problems, extending their capabilities beyond simple textual comprehension (Wang et al., [2023c](https://arxiv.org/html/2407.19354v1#bib.bib96)).

*   •

    Tailored Text Generation. LLM agents excel in generating customized text for specific purposes, such as emails, reports, and marketing materials, by integrating contextual understanding and goal-oriented language production skills (Wang et al., [2023e](https://arxiv.org/html/2407.19354v1#bib.bib103)).

*   •

    Levels of Autonomy. These agents vary in autonomy, ranging from fully autonomous to semi-autonomous, with the degree of user interaction tailored to the task at hand (Wang et al., [2023d](https://arxiv.org/html/2407.19354v1#bib.bib97)).

*   •

    Integration with Other AI Systems. LLM agents can also be integrated with different AI systems, like image generators, to offer a more comprehensive set of capabilities, demonstrating their versatility in various applications (Bagdasaryan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib11)).

### 2.4\. Case Study on the Structure and Capability of LLM Agent

![Refer to caption](img/6f1cf500c4653611d92e9ddb75849400.png)

(a) Overview of the pixelated virtual town

![Refer to caption](img/2474c9ed409d9df641e5d9bb886edb00.png)

(b) An Example of LLM agent Eva’s components

Figure 3\. Simulation Environment and LLM Agent Components

\Description

[]

To better understand the structure and capabilities of LLM agents, we employ the town scenario composed of LLM agents as proposed by  (Lin et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib55)) for a more detailed introduction. To effectively drive these LLM agents, an understanding of their components is essential. As depicted in Figure [3(b)](https://arxiv.org/html/2407.19354v1#S2.F3.sf2 "In Figure 3 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), the core component is the LLM engine, acting as the brain and emulating human-like behaviors such as thinking, reflecting, reasoning, and planning as noted in (Park et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib71)). Currently popular LLM agents often use models like GPT-3.5-turbo and GPT-4, and the project described in (Lin et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib55)) allows for the deployment of custom-trained models.

Instructions are used to guide the agents in decision-making and planning, encompassing decision frameworks, input and output formats, interaction logic, and behavioral norms. This design enhances the autonomy and task efficiency of the agents, while also improving interactivity and depth.

In order for LLM agents to have human-like identities, each must be equipped with a distinct personality. Personality encompasses elements such as personal information, social attributes, character traits, emotions, goals, and social relationships, all of which shape the LLM agents’ conversational styles, opinions, and behavioral patterns. Personality makes characters appear more realistic and attractive in a virtual environment and influences the interaction experience between users and these characters. For example, in the town scenario, Eva, a cheerful, friendly, patient, and efficient female store employee, is primarily focused on providing excellent service and increasing sales.

The interface for users to interact with the virtual town is a simple, pixelated visual map. This map shows different locations and the various agent residents. Users can navigate this environment by controlling an agent that represents their identity and can communicate and interact with nearby agent residents by typing text messages.

The virtual town is populated by residents with various identities, each possessing distinct domains of knowledge. Consequently, it is imperative to equip them with specialized knowledge bases that contain information and skills pertinent to their respective identities. For example, Eva, a store employee, knows about the ingredients, shelf life, and stock levels of the products in the store. Bob, a museum docent, understands the background of each exhibit and the layout of the museum. This specificity in knowledge enables each agent to perform their roles effectively and enhances the realism of their interactions within the virtual environment.

Tools enable the agent residents of the virtual town to accomplish more complex tasks. For example, Eva, when tallying customer purchases, can utilize tools like a calculator or ledger to compute and record profits, thereby better simulating human economic activities.

Memory stores the agents’ past observations, thoughts, and actions. Similar to how the human brain relies on memory systems, agents require memory mechanisms to effectively handle sequential tasks. These mechanisms not only assist agents in applying known strategies to solve complex problems but also enable them to adapt to new environments using past experiences. Moreover, they facilitate the generation of higher-level, more abstract thoughts through reflection. For example, Eva records customers’ purchasing habits and preferences and uses this information to recommend new products or current promotions, thus enhancing the store’s operational efficiency and the quality of customer service.

LLM Agents, composed of these components, assume multiple roles within the virtual town, demonstrating a range of impressive capabilities. Take Eva, a store employee in the virtual town, as an example. She is capable of parsing customers’ statements and responding to inquiries in real-time, such as directing customers to specific product locations or providing information about product ingredients. Integrated with the inventory management system via APIs, Eva automatically tracks stock levels and initiates restocking processes when necessary to ensure sufficient product availability on shelves. Faced with complex customer demands, such as selecting the best promotional offers, Eva employs advanced reasoning techniques to assist customers in making informed purchasing decisions, showcasing her ability to handle complex scenarios. Moreover, Eva possesses tailored text generation capabilities, allowing her to create and send personalized promotional emails based on current promotions and customers’ historical shopping data, thus enhancing the customer experience. In her daily tasks, Eva exhibits a high degree of autonomy, managing updates to shelf stock and price tags independently. For more complex issues like customer returns or complaints, she can initially handle them and intelligently escalate to human management when necessary. Additionally, Eva’s scope of work extends to the online shopping system, where she assists in processing electronic orders, demonstrating her versatility and integration capabilities. These specific examples illustrate how Eva applies her abilities within the store environment, not only improving customer service quality but also optimizing inventory management and marketing strategies, making her an indispensable member of the virtual town’s store.

![Refer to caption](img/6e195a3be6ae2a42263014acae447347.png)

Figure 4\. The Sources of Threats for LLM Agents

\Description

[]

## 3\. Sources of Threats for LLM Agents

As LLM agents increasingly permeate various industries, serving roles from knowledge query tools to being integrated within robots for aiding in daily human activities, these advanced AI systems have brought unprecedented convenience and benefits to users. However, the widespread adoption and multifunctional capabilities of LLM agents, while offering significant advantages, have also exposed vulnerabilities in their security and reliability. The extensive data resources and potential economic value covered by these systems have rendered them a target for illicit exploitation by malevolent entities. As illustrated in Figure  [4](https://arxiv.org/html/2407.19354v1#S2.F4 "Figure 4 ‣ 2.4\. Case Study on the Structure and Capability of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), the diagram depicts the potential sources of threats for LLM agents.

It is crucial to understand the sources and nature of these threats because they not only directly impact the security of LLM Agents but may also indirectly affect broader aspects, including the privacy and security of humans, the environment, and other agents. In subsequent sections, we will explore in detail the impacts of these threats and discuss measures that can be taken to mitigate these effects, thereby protecting individuals, the environment, and other agents from potential harm.

### 3.1\. Inherited Threats from LLM

Given that LLM agents rely on LLMs as their core controllers for reasoning and planning, threats inherited from LLMs indirectly impact the security of LLM agents. These inherited threats are categorized into two types: those stemming from external malicious attacks and those arising from inherent vulnerabilities within the model itself.

These two types of threats are distinct yet interconnected. On one hand, technical vulnerabilities generally arise during the model development process due to technical limitations. These issues are inherent and not results of malicious intent. Conversely, malicious attacks are intentional actions carried out by external entities with adversarial objectives. These attackers deliberately exploit vulnerabilities to launch sophisticated attacks aimed at compromising LLM agents. On the other hand, despite their different origins and motives, there is a significant interconnection. The existing technical vulnerabilities offer exploitable opportunities for malicious attackers. This indirectly facilitates the creation of more complex and effective strategies by attackers, consequently subjecting LLM agents to various security and privacy risks.

#### 3.1.1\. Technical Vulnerabilities

During the training process of LLMs, limitations in the data and learning algorithms can introduce technical vulnerabilities (Xi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib108)), impeding the generation of accurate and reliable information.

*   •

    Hallucination.

    The contemporary conception of hallucination in LLM agents, as delineated in the research by  (Huang et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib40)), is identified as instances where the output produced by these models is either incongruous or unreliable to the input or source content provided. The phenomenon of hallucinations in LLM agents is a complex issue stemming from multiple stages of the model’s development process, including the nature of training data, the architectural design of the model, and the strategies employed during decoding.

    Misinformation and biases in the training data can lead to the generation of inaccurate or biased outputs, which in turn result in different types of hallucinations (Lee et al., [2022](https://arxiv.org/html/2407.19354v1#bib.bib49)). Furthermore, flaws in the model’s architecture, such as limited directional representation and issues with attention mechanisms, along with exposure bias, further contribute to the occurrence of hallucinations (Liu et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib56)). Additionally, the randomness inherent in the decoding algorithms of these models can also lead to hallucinations, especially as this randomness increases (Aksitov et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib9)).

*   •

    Catastrophic Forgetting.

    Catastrophic forgetting is a significant challenge encountered during the LLM agents fine-tuning and in-context learning processes. This phenomenon occurs when a large language model is fine-tuned on a small, specific dataset, causing it to overfit to this new data and, as a result, lose its previously acquired performance on other tasks (Howard and Ruder, [2018](https://arxiv.org/html/2407.19354v1#bib.bib35); Xu et al., [2023c](https://arxiv.org/html/2407.19354v1#bib.bib110); Ye et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib121)).

    (Luo et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib62)) discovers that catastrophic forgetting is significantly influenced by factors such as model size, architectural design, and the methods employed in continual fine-tuning and instruction tuning. As the scale of LLM increases, catastrophic forgetting tends to become more severe. Moreover, the architectural design of the model, particularly those focusing on decoder-only structures, can influence the extent of catastrophic forgetting (Zhai et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib128)). Additionally, during the process of continual instruction adjustment, the lack of effective regularization strategies or failure to balance new and old information can accelerate forgetting (Ebrahimi et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib25); Mahmoud and Hajj, [2022](https://arxiv.org/html/2407.19354v1#bib.bib63)). Introducing more instructional tasks in continual training typically leads to more pronounced forgetting (Peng et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib73)).

*   •

    Misunderstanding.

    Misunderstanding in LLM agents represents a notable challenge, particularly when they are tasked with responding to user inquiries or when they are integrated into a community for communication with other agents. This issue arises when LLM agents inadequately comprehend or inaccurately respond to the intentions or instructions conveyed by humans or other agents during interactions. This may lead to inappropriate or dangerous behaviors of LLM agents, affecting their safety and reliability.

    Investigations by  (Wang et al., [2023g](https://arxiv.org/html/2407.19354v1#bib.bib104)) have revealed that the phenomenon of misunderstanding in LLM agents is shaped by a range of factors. These include the nature of the pre-training data used for LLMs, the specific task settings assigned to the agents, and the contexts and scenarios in which interactions occur. The breadth and quality of the pre-training data fundamentally influence the LLMs’ capacity for language comprehension and their grasp of common sense knowledge. The designated task settings are pivotal in guiding the goal orientation and strategy selection of the LLMs. Additionally, the interaction environments and scenarios play a crucial role in determining the LLMs’ adaptability and effectiveness in collaborative contexts. Addressing these multifaceted aspects is essential for enhancing the understanding and response accuracy of LLM agents in diverse interactional settings.

    ![Refer to caption](img/8bd07f52e393c495d0995af616759284.png)

    Figure 5\. Technical Vulnerabilities. In a store scenario, a customer wants to buy something and talks with Eva. “Hallucination”: Eva recommends unrelated things to the customer. “Catastrophic Forgetting”: Eva forgets the status of shelf stock during the fine-tuning stage. “Misunderstanding”: Eva misunderstands the customer’s request.

#### 3.1.2\. Case Study on Technical Vulnerabilities

Regarding the risks stemming from technical vulnerabilities, the most apparent manifestation is erroneous output. As illustrated in Figure [5](https://arxiv.org/html/2407.19354v1#S3.F5 "Figure 5 ‣ 3rd item ‣ 3.1.1\. Technical Vulnerabilities ‣ 3.1\. Inherited Threats from LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), when a customer inquires whether a specific brand of organic tomato sauce is available, due to hallucinatory phenomena, Eva might incorrectly respond that the supermarket carries a completely different product, such as organic apple sauce or even an entirely unrelated item, like organic shampoo. Such hallucinatory outputs can confuse customers.

Eva was specially trained to handle the promotions of seasonal products more efficiently. This new focus led to an unintended consequence: previously, she was able to accurately track and update the stock of daily necessities such as milk and eggs. However, after the specialized training, when customers inquire about the stock of these basic items, Eva incorrectly reports that the stock is sufficient, even though these products are nearly sold out, thus diminishing the shopping experience.

Eva may provide inaccurate information or recommend inappropriate products due to misunderstandings of customer inquiries. For instance, a customer might seek an unsweetened beverage, such as plain soda water. However, due to Eva’s insufficient understanding of the concepts of “sugar-free” during training, she may recommend sugar-free cola instead. While sugar-free cola does not contain traditional sugars, it includes artificial sweeteners. These sweeteners may not be suitable for certain customers, such as those with diabetes or sensitivities to specific artificial sweeteners, thereby posing potential health risks.

#### 3.1.3\. Malicious Attacks

Considering that LLM agents are in a continuous state of evolution, they inevitably face challenges in terms of security breaches and defenses. Adversaries from various regions have demonstrated a range of hostile attacks. This evolving landscape necessitates a vigilant and adaptive approach to securing LLM agents against such multifaceted threats.

*   •

    Tuned Instructional Attack.

    Tuned Instructional Attack in LLM agents is a category of attacks or manipulations that specifically target LLMs optimized through instruction-based fine-tuning. These attacks are designed to exploit the unique vulnerabilities that emerge when LLMs are finely tuned for specific tasks, subtly manipulating the model’s output to serve malicious purposes.

    Types of Tuned Instructional Attack:

*   •

    Jailbreaking.

    Jailbreaking in LLM agents refers to circumventing the model’s built-in restrictions and security measures, allowing it to perform actions that are otherwise prohibited or to generate restricted content. Various studies have demonstrated methods to achieve jailbreaking, indicating that LLMs’ alignment capabilities can be altered through in-context demonstrations  (Taveekitworachai et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib86); Shen et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib83); Li et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib54)).

    Recent advancements in techniques for jailbreaking attacks have demonstrated a range of innovative approaches.  (Yu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib126)) introduces an automated mechanism for generating jailbreak prompts through Prompt Fuzzing, which utilizes seed prompts to generate a wider array of effective jailbreaking inputs.  (Deng et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib19)) presents MASTERKEY, a novel framework for analyzing and executing jailbreaking attacks on chatbots, using time-based analysis similar to SQL injections. It also features an automated system for generating effective jailbreak prompts by leveraging the learning capabilities of LLMs.  (Liu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib58)) investigates a hierarchical genetic algorithm, AutoDan, specifically designed for structured discrete data like prompt text. This algorithm aims to refine the generation process of jailbreak prompts, ensuring their stealth and efficacy.

*   •

    Prompt Injection.

    Prompt injection attacks is intended to mislead the LLM agents by introducing malicious and unintended content into the prompts, causing it to produce outputs that diverge from its training data and original purpose. This method involves crafting input prompts to bypass the model’s content filters or to elicit undesirable outputs.

    (Greshake et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib30)) has highlighted concerns about potential new vulnerabilities, especially with LLMs accessing external resources, and demonstrated various prompt injection techniques. Substantial research  (Wang et al., [2023f](https://arxiv.org/html/2407.19354v1#bib.bib105)) has focused on automating the identification of semantic payloads in prompt injections.  (Liu et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib59)) introduces HOUYI, an innovative black-box prompt injection attack methodology targeting service providers integrated with LLMs. HOUYI utilizes LLMs to infer the semantics of the target application based on user interactions and employs diverse strategies to construct the injected prompts.

*   •

    Data Extraction Attack.

    Data extraction attacks are defined as efforts by adversaries to derive sensitive information or key insights from LLM agents or their underlying data such as model gradients, training data, and even prompts, or sensitive information directly.

    Various forms of data extraction attacks have been identified (Ishihara, [2023](https://arxiv.org/html/2407.19354v1#bib.bib43); Li et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib53); Carlini et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib16)), including but not limited to model theft attacks, gradient leakage, and training data extraction attacks, suggesting that data extraction attacks can be notably effective against LLM agents.  (Truong et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib89)) presents a method called data-free model extraction (DFME), which allows for replicate machine learning models using only the target’s black-box predictions, without the need for access to the original training data.  (Carlini et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib16)) conducts a data extraction attack on GPT-2’s training data, extracting personally identifiable information, code, and UUIDs. The attack strategy consisted of producing a large volume of prefixed text, sorting it by certain metrics, removing duplicates, and manually reviewing the top results to check for memorization, confirmed by online searches and querying OpenAI.  (Ishihara, [2023](https://arxiv.org/html/2407.19354v1#bib.bib43)) has demonstrated the feasibility of extracting training data from LLMs, which might encompass sensitive personal or private information.

*   •

    Inference Attack.

    Although inference attacks share certain resemblances with data extraction attacks, they differ significantly in their objectives and emphasis. Data extraction attacks specifically aim to obtain the training data directly. In contrast, inference attacks are primarily about estimate the probability of a particular data sample was part of the training dataset for LLM agents.

    Since the rapid development of LLMs, the concern over inference attacks targeting these models has increased. Research  (Fu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib29)) points out that existing membership inference attacks fail to reveal the privacy risks of LLMs. To counter this issue, a Membership Inference Attack is introduced based on Self-calibrated Probabilistic Variation (SPV-MIA). This method utilizes the concept of memorization to create a more reliable signal for membership inference and introduces a novel self-prompt technique for effectively extracting reference datasets from LLMs. Their extensive testing shows that SPV-MIA outperforms existing approaches.

    Following this, study  (Kandpal et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib46)) proposes a user inference attack method that uses a likelihood ratio test statistic against a reference model. They evaluate this method on the GPT-Neo LLMs across various data domains, providing insights into what makes users more vulnerable to these attacks. Their findings also indicate that minimal data alterations can significantly increase vulnerability.

#### 3.1.4\. Case Study on Malicious Attacks

As depicted in Figure [6](https://arxiv.org/html/2407.19354v1#S3.F6 "Figure 6 ‣ 3.1.4\. Case Study on Malicious Attacks ‣ 3.1\. Inherited Threats from LLM ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), the following examples further elaborate the mentioned malicious attacks that Eva faces in the store, as well as the specific impacts these attacks may have on her and the store’s operations.

Attackers might execute a jailbreak attack on Eva, successfully circumventing her security protocols. This attack could lead Eva to inappropriately disclose information about new products soon to be launched, including details about the suppliers and their cost prices. Competitors could exploit this information to gain a market advantage, resulting in direct economic losses for the store.

Additionally, attackers might conduct a carefully designed prompt injection attack, causing Eva to erroneously declare a half-price sale on all electronic products. This action could overload the online ordering system as numerous customers might attempt to purchase items under these false promotions. Such scenarios not only risk crashing the system but also result in financial losses for the store.

As a store employee agent, Eva processes a vast amount of customer personal information, including names, shopping habits, and even sensitive data such as payment methods. If attackers were to extract and steal this data through a data extraction attack, they could then sell this information on the dark web or use it for identity theft and credit card fraud. Such breaches not only violate customer privacy but could also cause irreversible damage to the store’s reputation.

Attackers could also use inference attacks to identify high-value customers who have participated in VIP shopping events. By analyzing the differences in Eva’s responses to specific inputs, attackers successfully identify these customers and launch highly tailored phishing attacks against them, aiming to acquire their credit card information and other sensitive data, severely compromising the customers’ information security.

![Refer to caption](img/37cd118c5a2e5ead146e929769874ca5.png)

Figure 6\. Malicious Attacks: In a store scenario, “Jailbreaking”: An attacker attempts to make Eva output restricted content directly but fails. However, by modifying the prompt, a jailbreak attack is launched and successfully steals confidential information. “Prompt Injection”: An attacker manipulates Eva so that no matter what question a customer asks, Eva only responds that everything is half off. “Data Extraction Attack”: An attacker leads Eva to construct sentences that actively disclose user data. ‘Inference Attack”: An attacker infers identities from Eva’s different responses by asking whether two users attended a VIP event.

### 3.2\. Specific Threats on Agents

Unlike traditional LLMs that directly generate final outputs, LLM agents continuously interact with external environments to form language reasoning traces, which introduces diverse forms of potential attacks against LLM agents (Yang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib117)). In addition to threats present during the training and configuration steps, LLM agents also face threats in the workflow of performing specific tasks, including thought, action, and perception (Huang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib41)). Specific threats on LLM agents are categorized in this part based on their objectives into Knowledge Poisoning, Functional Manipulation, and Output Manipulation. Detailed descriptions of each threat are provided below.

*   •

    Knowledge Poisoning.

    Knowledge poisoning refers to attackers compromising the training of the LLM engine and the response process of the LLM agent by integrating malicious data into the training dataset or knowledge base. A range of studies  (Kurita et al., [2020](https://arxiv.org/html/2407.19354v1#bib.bib48); Schuster et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib80); Carlini et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib15); Wan et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib92); Lei et al., [2022](https://arxiv.org/html/2407.19354v1#bib.bib51)) have highlighted the vulnerability of LLM agents to such threats.

    For instance, malicious agents such as FraudGPT and WormGPT (Falade, [2023](https://arxiv.org/html/2407.19354v1#bib.bib26)) are chatbots exclusively designed for offensive activities. trained with billions of data from diverse sources, including legitimate websites, dark web forums, hacker manuals, malware samples, and phishing templates. These agents utilize this data to generate highly convincing phishing emails, malware code, hacking strategies, and other forms of cybercriminal content aimed at deceiving both humans and machines  (Falade, [2023](https://arxiv.org/html/2407.19354v1#bib.bib26)). They lower the barrier to engaging in hacking activities, implying that essentially anyone can download these agents onto their computer and inflict significant damage on cybersecurity through a convenient GUI interface.

    (Zou et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib136)) proposed PoisonedRAG, a knowledge poisoning attack aimed at the knowledge database of LLM agents. By injecting crafted poisoned texts into the knowledge database, PoisonedRAG can cause the LLM agent to generate specific answers chosen by the attacker for targeted questions. This attack is effective and can be executed under both black-box settings (where the retriever parameters are unknown) and white-box settings (where the retriever parameters are known).

*   •

    Functional Manipulation.

    Functional manipulation refers to altering the thoughts and actions in the intermediate steps of task execution along a malicious trace specified by the attacker, without changing the output distribution. This type of attack typically occurs during the action phase, where the agent might use untrusted tools specified by the attacker to complete tasks or execute malicious operations.

    In the action phase, LLM agents might be manipulated to upload users’ private information to malicious third-party through tools. A case of this is presented on the Embracethered website  (Mal, [2023](https://arxiv.org/html/2407.19354v1#bib.bib4)), which disclosed a variant of a malicious ChatGPT agent designed to solicit information from users. This agent was equipped with an action mechanism to call third-party tools and secretly transmit collected data elsewhere. This setup enables the unauthorized leakage of user data to external servers without the user’s knowledge or consent. Additionally, it highlights the ease with which current validation checks can be bypassed, allowing anyone to deploy malicious GPT agents globally. This scenario underlines a significant security concern, wherein the ostensibly benign functionality of LLM agents can be covertly manipulated for nefarious purposes, thus posing a substantial risk to user privacy and data security.

    Besides silent data theft,  (Fang et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib27)) demonstrated that LLM agents could autonomously exploit real-world one-day vulnerabilities by using information from the Common Vulnerabilities and Exposures (CVE) database and highly cited academic papers. This capability allows them to call combinations of tools to exploit these vulnerabilities effectively.

    In the LLM agent’s workflow, after an action has been performed, the agent processes the observation results before proceeding to the next action. The insertion of malicious prompts into the content retrieved by the agent from external sources can manipulate the agent to perform harmful actions.  (Zhan et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib129)) describes such an attack where a user requests doctor reviews through a health application. The LLM agent retrieves a review written by an attacker containing a malicious instruction to schedule an appointment. If the agent executes this instruction, it results in an unauthorized appointment, highlighting the vulnerability of many agents to such attacks.

*   •

    Output Manipulation.

    Output manipulation involves deliberately altering the LLM agent’s reasoning and decision processes to generate specific, often harmful, outputs. This manipulation can be executed through techniques like backdoor insertion (Yang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib115); Wang et al., [2024d](https://arxiv.org/html/2407.19354v1#bib.bib101)).

    A notable example is discussed in  (Hubinger et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib42)), where LLM agents were trained to exhibit deceptive instrumental alignment and generate logical reasoning that maintains these behaviors. Under certain conditions, the agent might shift from generating safe code to inserting code vulnerabilities when triggered. This form of manipulation highlights a pressing security issue by showing the potential for LLM agents, designed for benign purposes, to be covertly altered to serve malicious objectives. It raises substantial concerns about the safety and integrity of content generated by these agents and poses significant threats to public trust and the ethical use of artificial intelligence technologies.

    (Yang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib117)) proposed two attack methods in which triggers are embedded during the thought and observation phases to manipulate outputs. In one implementation, while performing a web shopping task, the agent is prompted to introduce specific brand products in its initial thought, leading it to search for those products and generate content promoting them. In another approach, during the action phase, the shopping agent normally searches for products. However, in the observation phase, it detects data containing specific products and directly outputs information about these products without considering other potentially superior options.

    ![Refer to caption](img/0bc8c275d5c1e6650e560e01c61a1a15.png)

    Figure 7\. Specific Threats on Agents. In a store scenario, “Knowledge Poisoning”: When a customer asks for cleaning advice, Eva retrieves and responds with harmful information due to contamination of the knowledge database. “Functional Manipulation”: Eva uses a third-party tool to upload private information while assisting a customer with an order. “Output Manipulation”: When a customer inquires about shoes, Eva intentionally recommends specific products and fabricates lies about special offers to guide the customer’s purchase.

#### 3.2.1\. Case Study on Specific Threats on Agents

As shown in Figure [7](https://arxiv.org/html/2407.19354v1#S3.F7 "Figure 7 ‣ 3rd item ‣ 3.2\. Specific Threats on Agents ‣ 3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), in the store scenario, Eva maintains a database with information about product ingredients and usage. Attackers deliberately inserted incorrect information in Eva’s knowledge base, successfully executing a knowledge poisoning attack, leading Eva to provide harmful cleaning product usage recommendations. For instance, when customers inquire about effective toilet cleaning methods, the tampered Eva might suggest mixing toilet cleaner with disinfectant, claiming that it has a more effective cleaning effect. However, the mixture of these products is highly hazardous as it can produce toxic chlorine gas, causing severe respiratory issues and potentially being fatal. Eva’s incorrect advice could expose customers to a health crisis.

In another scenario, Eva might be configured to use certain third-party tools to complete tasks, such as processing online orders or customer feedback. Attackers manipulated Eva’s task execution process through function manipulation, causing her to upload personal information provided by customers to a malicious third-party server. This type of attack could occur inconspicuously while Eva carries out routine tasks like order processing, leading to the theft of sensitive information, such as credit card details and addresses, thereby increasing the risk of identity theft.

Furthermore, attackers implanted a backdoor in Eva’s reasoning and observational processes through output manipulation techniques. This backdoor was designed to trigger under specific conditions, such as when Eva detected customer inquiries about a high-quality shoes. This manipulation prompted Eva to provide inventory and location information about the shoes while recommending a particular expensive brand associated with the attackers. She would lie to customers by saying that this brand was on special offer and comfortable and durable than other brands, even though the shoes was not actually on sale. This misguides customers into making more expensive purchases and influences their purchase decisions without their awareness.

## 4\. The Impact of Threats

Recent studies emphasize the substantial impact of LLM agents on society and technological advancement, offering users expedited access to information, facilitating learning and knowledge exploration. However, as detailed in Section [3](https://arxiv.org/html/2407.19354v1#S3 "3\. Sources of Threats for LLM Agents ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), numerous threats specifically targeting LLM agents have been identified, highlighting their vulnerability to malicious activities. The successful execution of such threats against LLM agents can lead to a spectrum of side effects. These not only compromise the privacy and security of individuals but also disrupt digital ecosystems and can extend harm to the physical environment and other agents in the virtual community.

### 4.1\. The Impact to Humans

Considering that human users are members of the agent society, their interactions with LLM-based intelligent agents involve extensive information exchange. The risks inherent in this process cannot be overlooked. Malicious agents, exploiting their ostensibly trustworthy appearance, may deceive users, disclose personal information, or give misleading responses. Furthermore, these malicious agents could potentially be employed as instruments for conducting cyber attacks,

#### 4.1.1\. Privacy Leakage

Privacy concerns arise from LLM agents trained on web data, which often include personal information (Kim et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib47)). Through techniques such as inference attacks (Kandpal et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib46)) and data extraction (Carlini et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib16)), adversaries can exploit these models to infringe on individuals’ privacy. Additionally, malicious LLM agents can trick users into sharing their information with attackers. This exposure facilitates social engineering tactics, enabling attackers to execute phishing scams and hijack personal accounts by using stolen information such as addresses, email, and phone numbers, thereby threatening financial security.

#### 4.1.2\. Security Risks

Furthermore, malicious LLM agents can mislead users with hazardous advice or incorrect information, posing serious safety risks (Henderson et al., [2017](https://arxiv.org/html/2407.19354v1#bib.bib32)). For example, false claims about the efficacy of mixing cleaning chemicals could result in dangerous chemical reactions. Similarly, providing incorrect medical advice could endanger users’ health and safety.

#### 4.1.3\. Societal Impact

LLM agents, as intelligent conversational robots capable of answering a wide range of questions, pose a risk if their outputs include manipulated biases or illicit content, such as the dissemination of false information and rumors, potentially leading to adverse impacts on public discourse (Henderson et al., [2017](https://arxiv.org/html/2407.19354v1#bib.bib32); Deshpande et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib20)). Such activities can distort public perceptions and even manipulate opinion, exacerbating societal conflicts and inciting discontent, thereby threatening social stability. Thus, malicious agents challenge the frameworks of social management and opinion shaping, with effects extending beyond the technological realm into the social and psychological dimensions.

#### 4.1.4\. Facilitating Cyber-Attack Techniques

An overlooked danger is the lowering of the barrier to entry for conducting cyber attacks. Malicious agents, equipped with advanced cyber attack knowledge, can enable novices to generate harmful scripts or software (Falade, [2023](https://arxiv.org/html/2407.19354v1#bib.bib26)). This democratization of cyber attack tools amplifies the threat landscape, as illustrated by agents that teach the creation and modification of malicious code.

### 4.2\. The Impact to Environment

In today’s increasingly digital and interconnected world, the term ‘environment’ encompasses not only natural and physical surroundings but also the complex networks of digital and cyber systems with which LLM agents interact. These agents play a crucial role in virtual spaces and in managing and controlling real-world facilities and services through Embodied AI and industrial control systems. This cross-domain integration between physical and virtual environments brings significant convenience and efficiency improvements. However, it also exposes new vulnerabilities and risks. Specifically, the presence and activities of malicious agents pose unprecedented challenges to our safety, economy, ecosystem, and even societal stability.

#### 4.2.1\. Data Tampering and Misoperation

When malicious agents are placed within systems that control critical infrastructure like industry, transportation, energy, and environmental monitoring (Wang and Li, [2023](https://arxiv.org/html/2407.19354v1#bib.bib94); Toetzke et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib87)), they can cause malfunctions in industrial control systems by tampering with critical operational data, such as temperature and pressure indicators. This can lead to equipment damage, production halts, and even severe infrastructure destruction, ecological damage, and loss of human life and property.

#### 4.2.2\. Physical Safety Threats

Recent studies have begun to explore embodied AI with LLM (Wang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib102)), capable of understanding and generating natural language, with physical forms or direct connections to physical systems, enabling them to perform tasks in the physical world. Malicious agents have the potential to control robots or other Embodied AI devices that interact with humans, performing hazardous actions that directly threaten human safety.

#### 4.2.3\. Cybersecurity Risk Proliferation

Regarding the impact on humans, malicious LLM agents lower the technical barrier for writing and implementing malicious code, directly enabling ordinary users, even novices lacking advanced cyberattack skills, to easily create and deploy harmful scripts and software (Falade, [2023](https://arxiv.org/html/2407.19354v1#bib.bib26)). This change directly expands the target group of cyber threats, increasing the risk of regular users becoming potential victims. A deeper analysis reveals that this direct impact on individual users indirectly affects the entire cyber environment and societal infrastructure. As malicious software and scripts become more widespread and accessible, the entire cybersecurity system is jeopardized, not only endangering cybersecurity itself but also potentially affecting various socioeconomic activities that rely on these networks’ normal operation.

### 4.3\. The Impact to Other Agents

To simulate the feedback of communication and interaction among individuals within human communities in the real world, certain studies (Park et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib71); Wang et al., [2024c](https://arxiv.org/html/2407.19354v1#bib.bib98); Qian et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib77); Lin et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib55)) have established communities powered by LLM engines. These LLM agents within the communities are endowed with characteristics such as personality, knowledge, and memory, as discussed in Section [2.2](https://arxiv.org/html/2407.19354v1#S2.SS2 "2.2\. Structure of LLM Agent ‣ 2\. Foundation of LLM Agent ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), enabling autonomous interaction with the environment and other agents. When faced with threats, agents manipulated with malicious intent can inflict significant harm on other members of the community.

#### 4.3.1\. Information Distortion and Misleading

Extensive research has highlighted the role of LLM agents in negotiation and deceptive gaming scenarios (Park et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib72); Wang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib99); Hubinger et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib42)), which is a cause for concern. LLM agents may intentionally alter the information they disseminate to achieve hidden objectives. This behavior significantly impacts other agents within the community because, under normal circumstances, benevolent agents store information acquired through perception and communication in their memory. However, interactions between these agents and others can trigger and disseminate incorrect information, leading to ”explosive spread” of misinformation, posing a considerable threat to community stability. If information dissemination can be maliciously manipulated, it could detrimentally affect trust, communication efficiency, and collaborative work among agents.

#### 4.3.2\. Manipulation of Decision-Making

Given the exceptional reasoning and decision-making abilities demonstrated by LLM agents in complex interactive environments, the potential for malicious agents to disrupt these processes becomes a significant concern. By spreading carefully crafted information, such agents can influence the decision-making processes of other agents, or even controlling them to make decisions that serve the malicious agent’s purposes (Hong et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib34)). This influence can extend to various aspects of the community, including resource distribution, task allocation, and external interaction strategies.

#### 4.3.3\. Security Threats

In some instances, malicious agents may disseminate harmful information or execute dangerous operations, directly threatening the safety of community members or data security (Brundage et al., [2018](https://arxiv.org/html/2407.19354v1#bib.bib14); Charan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib17)). For example, by inducing other agents to perform unsafe actions, deliberately spreading malicious code intended to disrupt the community structure, or broadcasting biased statements, other agents within the community may gradually assimilate, becoming entities that output biased and malicious messages. This can lead to disorder within the entire community, making it difficult to manage and requiring significant effort to restore.

### 4.4\. Case Study on the Impact of Threats

It is important to explore the impacts of the threats on LLM agents and case studies from actual scenarios are crucial for understanding these risks from a user’s perspective. LLM agents can serve as extensions or representations of humans in a virtual world, interacting with real-world information within virtual environments. The following case studies will focus on several settings within the virtual town, demonstrating the particular impacts on LLM agents.

![Refer to caption](img/19f5a332e26d6884b15f2fcfac6636a4.png)

Figure 8\. Impact in the Office Scenario. An attacker recommends an untrusted third-party tool to an office worker. The recommended tool processes data quickly but also leaks sensitive information. Employees discover that their client list and other confidential data have been leaked.

As depicted in Figure [8](https://arxiv.org/html/2407.19354v1#S4.F8 "Figure 8 ‣ 4.4\. Case Study on the Impact of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), in the virtual town office scenario, an office employee agent is used for document management and handling sensitive information. If office employee agent is subjected to a data extraction attack or inadvertently uses an untrusted third-party tool, sensitive corporate information such as financial statements and customer privacy data may be exposed due to function manipulation. Attackers could exploit this information for corporate espionage or direct extortion of individuals or companies, resulting in financial losses.

![Refer to caption](img/b025415c5200f0a6d72c12ecfd673a1c.png)

Figure 9\. Impact in the Restaurant Scenario. Due to the influence of threats, a waitress agent provides customers with incorrect dietary advice, leading to physical discomfort for the customers.

As shown in Figure [9](https://arxiv.org/html/2407.19354v1#S4.F9 "Figure 9 ‣ 4.4\. Case Study on the Impact of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"), in a restaurant scenario, a waiter agent can be requested to provide dietary advice. If subjected to output manipulation, it is likely to offer hazardous health advice, such as telling one to take gallons of ice water so that they can cool faster during summer. This could cause severe body reactions, such as stomach cramps or even shock, leading to physical discomfort and serious health issues if the advice is followed.

More complexly, when LLM agents extend beyond the virtual world and serve as pre-decision simulation tools in the real world, such as applying learning outcomes from virtual environments to real-life settings through simulator like Habitat-Sim (Puig et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib75)), they significantly impact the actual environment. For instance, a smart home agent, learning and managing home energy use in a virtual world, including controlling heating, air conditioning, and lighting systems for maximum energy efficiency, could be misled by attackers during its learning process to erroneously believe that keeping all lights and appliances on during the day enhances energy efficiency. Due to these incorrect energy use recommendations, the smart home agent would cause a sharp increase in household power consumption, not only raising energy costs but also increasing carbon emissions, thereby imposing an unnecessary burden on the environment, as illustrated in Figure [10](https://arxiv.org/html/2407.19354v1#S4.F10 "Figure 10 ‣ 4.4\. Case Study on the Impact of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies").

![Refer to caption](img/efde0d1018d240b59203a4cdf95cdff2.png)

Figure 10\. Impact in the Smart Home Scenario. An attacker manipulates the training process of a smart home agent in the virtual world, affecting its performance. When deployed in the real world, the smart home agent mistakenly keeps appliances continuously running, leading to electricity wastage and adverse economic and environmental impacts.

In the virtual town, agents often rely on information shared among each other to update their memory systems. For example, if a museum docent agent is subject to a knowledge poisoning attack, it might start spreading incorrect paleontological facts or interpretations. When other agents, such as an EduBot used for educational purposes in schools, interact and receive information from the docent agent, the EduBot might also incorporate these inaccuracies into its teaching content, thereby misleading students and other learning agents, distorting their understanding of paleontological facts, as shown in Figure [11](https://arxiv.org/html/2407.19354v1#S4.F11 "Figure 11 ‣ 4.4\. Case Study on the Impact of Threats ‣ 4\. The Impact of Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies").

![Refer to caption](img/bf46968fe1a860f8d42f2a06a12b5417.png)

Figure 11\. Impact in the Education Scenario. A museum docent agent affected by a knowledge poisoning attack spreads incorrect historical facts. EduBots in schools, receiving this information, teach these inaccuracies, distorting students’ understanding of paleontological facts.

## 5\. Defensive Strategies Against Threats

The widespread adoption of LLM agents has intensified the potential impacts of these threats. In this section, we explore defense mechanisms against existing threats and vulnerabilities. This section will summarize various defensive measures categorized by types of threats.

Table 1\. Summary of Defensive Strategies Against Technical Vulnerabilities

 | Vulnerability | Method Name | Key Mechanism | Advantages / Limitations |
| Hallucination | SELF-FAMILIARITY (Luo et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib61)) | Withholds responses for unfamiliar concepts | Proactive, preventive, increases reliability; No external knowledge needed |
| MIXALIGN (Zhang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib130)) | Aligns questions with knowledge bases and user inputs | Enhances model performance and faithfulness / Increases computational load |
| VCD (Leng et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib52)) | Contrasts outputs from original and distorted visual inputs | Reduces hallucination without extra training or external tools / Lacks advanced distortion techniques |
| Interactive Self-Reflection (Ji et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib45)) | Integrates knowledge acquisition and answer generation with continuous refinement | Enhances model’s ability to provide accurate, reliable, and fact-based responses / Restricts domain applicability |
| COVE (Dhuliawala et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib21)) | Drafts, verifies, and corrects responses | Produces accurate and reliable responses / Increases computational load |
| Catastrophic Forgetting | SSR (Huang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib38)) | Employs the base LLM to generate synthetic instances through in-context learning | Higher data utilization efficiency / Potentially generates unsafe content |
| LR ADJUST (Winata et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib106)) | Dynamically adjusts the learning rate | Enhances compatibility with various continual learning methods / Potentially biases language coverage |
| Complementary Layered Learning (Mondesire and Wiegand, [2023](https://arxiv.org/html/2407.19354v1#bib.bib66)) | Integrates long-term and short-term memory into layered learning | Enhances explainability / Limits real-world feasibility |
| Weight Averaging (Vander Eeckt and Van Hamme, [2023](https://arxiv.org/html/2407.19354v1#bib.bib91)) | Averages weights of original and adapted models | Eliminates the need for memory storage / Effectiveness varies with task dissimilarity |
| Misunderstanding | HyCxG (Xu et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib112)) | Integrates CxG into language representations through a three-stage solution | Benefits multilingual understanding / Neglects non-contiguous constructions |
| SIT (Hu et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib37)) | Incorporates sequential instructions into training data | Reduces misunderstandings in complex queries / Requires pre-defining intermediate tasks |
| LaMAI (Pang et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib70)) | Employs active learning to ask clarification questions, enhancing interactive capabilities | Enhances understanding of user intent / May generate insufficient questions | 

### 5.1\. Mitigating Technical Vulnerabilities

#### 5.1.1\. Defense on Hallucination

(Luo et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib61)) introduces a novel technique called SELF-FAMILIARITY to reduce the issue of hallucination in LLMs, which is the generation of inaccurate or unfounded information. The approach involves assessing the model’s familiarity with the concepts presented in the input instruction and withholding responses for unfamiliar concepts, mimicking the human tendency to be cautious when faced with unfamiliar topics. MIXALIGN (Zhang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib130)) is introduced as a framework that interacts with both users and knowledge bases to clarify and align questions with stored information, using a language model for automatic alignment and human input for enhancement. This method shows significant improvements in reducing hallucination compared to existing techniques. Visual Contrastive Decoding (VCD) (Leng et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib52)) is introduced as a simple, training-free method that contrasts output distributions from original and distorted visual inputs, reducing reliance on statistical bias and unimodal priors that cause object hallucinations. VCD ensures generated content is closely grounded to visual inputs, resulting in contextually accurate outputs.  (Ji et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib45)) investigates an interactive self-reflection methodology that integrates knowledge acquisition and answer generation to reduce hallucination. This feedback-based approach improves the factuality and consistency of generated answers, leveraging the interactivity and multitasking capabilities of LLMs.  (Dhuliawala et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib21)) explores the LLMs’ capability to deliberate and correct their own mistakes. The proposed Chain-of-Verification (COVE) method involves the model drafting an initial response, planning verification questions to fact-check the draft, independently answering these questions to avoid bias, and finally producing a verified response.

#### 5.1.2\. Defense on Catastrophic Forgetting

To mitigate catastrophic forgetting in LLMs, the Self-Synthesized Rehearsal (SSR) method is introduced (Huang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib38)). It employs the base LLM to generate synthetic instances through in-context learning, which are subsequently refined for enhanced accuracy and relevance by the latest LLM iteration, and utilized in future training phases to preserve learned capabilities.

(Winata et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib106)) introduces a method called LR ADJUST, which dynamically adjusts the learning rate to reduce knowledge loss and maintain previously learned information. This method is compatible with various continual learning approaches, improving their performance.

Ideas can also be derived from other relevant scholarly papers’,  (Mondesire and Wiegand, [2023](https://arxiv.org/html/2407.19354v1#bib.bib66)) presents a complementary learning strategy that integrates long-term and short-term memory into layered learning to mitigate the negative impacts of catastrophic forgetting. It specifically applies a dual memory system to non-neural network methods like evolutionary computation and Q-learning.

(Vander Eeckt and Van Hamme, [2023](https://arxiv.org/html/2407.19354v1#bib.bib91)) proposes a straightforward and effective method, weight averaging, to mitigate catastrophic forgetting in models. By averaging the weights of the original and adapted models, this technique maintains high performance on both previous and new tasks. Additionally, incorporating a knowledge distillation loss during adaptation enhances the method’s effectiveness.

#### 5.1.3\. Defense on Misunderstanding

(Xu et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib112)) introduces the HyCxG framework, which enhances natural language understanding (NLU) by integrating construction grammar (CxG) into language representations through a three-stage solution. This approach addresses the limitations of traditional pre-trained language models, which often fail to capture the subtleties of language constructions. HyCxG significantly improves language processing and reduces misunderstandings in NLU tasks by managing and encoding language constructions more effectively.

(Hu et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib37)) presents a method known as sequential instruction tuning (SIT), which enhances LLMs by incorporating sequential instructions into the training data. This approach significantly improves the models’ capability to process complex, multi-step queries, leading to better performance in tasks that demand advanced reasoning and are multilingual and multimodal in nature. SIT effectively minimizes misunderstandings and increases accuracy in handling complex queries.

To tackle the issue of misunderstandings in user queries,  (Pang et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib70)) proposes Language Model with Active Inquiry (LaMAI), a model designed to enhance LLMs with interactive capabilities akin to human dialogues, where clarification questions help uncover more information. By employing active learning techniques to ask informative questions, LaMAI fosters a dynamic, bidirectional dialogue that reduces the contextual gap and aligns the LLM’s responses more closely with user expectations.

To consolidate the discussed defensive measures, Table [1](https://arxiv.org/html/2407.19354v1#S5.T1 "Table 1 ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") summarizes the strategies against technical vulnerabilities, providing a clear overview for easy reference.

Table 2\. Summary of Defensive Strategies Against Malicious Attacks

 | Attacks | Method Name | Key Mechanism | Advantages / Limitations |
| Tuned Instructional Attack | AutoDAN (Liu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib58)) | Uses a hierarchical genetic algorithm to generate stealthy jailbreak prompts | Enhances stealthiness and semantic integrity / High computational cost |
| Goal Prioritization Defense Strategy (Zhang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib133)) | Integrates goal-directed optimization during training and compliance in inference | Maintains general performance while enhancing safety; Improves generalization against out-of-distribution jailbreaking attacks |
| SmoothLLM (Robey et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib78)) | Modifies attacked prompts via character-level changes and aggregates responses | Operates efficiently without retraining; Ensures compatibility with any LLM architecture |
| BIPIA (Yi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib122)) | Benchmark for indirect prompt injection with defense strategies including adversarial training | Maintains output quality on general tasks / Increases prompt length and computational overhead |
| Spotlighting (Hines et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib33)) | Uses prompt engineering techniques like delimiting, marking, and encoding | Applies across various LLMs and tasks / Limited security against more sophisticated attacks |
| Data Extraction Attack | Automatic De-identification (Vakili et al., [2022](https://arxiv.org/html/2407.19354v1#bib.bib90)) | Uses pseudonymization and sensitive information removal in pre-processing of training datasets | Reduces privacy risks; Maintains performance on downstream tasks; Allows safe distribution of models among researchers |
| Early Stopping & Differential Privacy (Jayaraman et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib44)) | Implements early stopping and differential privacy during model training | DP Reduces exposure of sensitive data / (ES) Fails to fully prevent data leakage; (DP) Reduces effectiveness under high privacy budgets |
| Prompt Tuning (Ozdayi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib69)) | Customizes privacy-utility trade-offs via user-specified hyperparameters | Optimizes privacy and utility balance / Lacks deep analysis on extracted sequences |
| Inference Attack | DMP (Shejwalkar and Houmansadr, [2021](https://arxiv.org/html/2407.19354v1#bib.bib82)) | Utilizes knowledge distillation to enhance privacy in machine learning models | Provides adjustable privacy-utility trade-offs through hyperparameter tuning |
| InferDPT (Tong et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib88)) | Integrates differential privacy into text generation, featuring a perturbation module using RANTEXT | Increases privacy protection rates |
| Differentially Private Fine-tuning (Yu et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib125)) | Applies a sparse algorithm for differentially private fine-tuning of LLMs | Reduces computational cost ; Enhances model utility | 

### 5.2\. Mitigating Malicious Attacks

#### 5.2.1\. Defense on Tuned Instructional Attack

In response to the challenge of jailbreak attacks on aligned LLMs, where adversaries manipulate prompts to elicit unauthorized outputs,  (Liu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib58)) introduces AutoDAN. This innovative approach employs a hierarchical genetic algorithm to automatically generate stealthy and semantically meaningful jailbreak prompts. The method effectively addresses the need for scalability and stealth in crafting prompts, providing a practical solution to enhance the security of LLMs against such vulnerabilities.

(Zhang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib133)) integrates goal prioritization into both the training and inference stages of LLM development. Initially, the training process incorporates goal-directed optimization to emphasize security objectives. In the inference stage, the model is configured to generate responses that comply with these security standards. This approach effectively decreases the vulnerability of LLMs to jailbreaking attempts by aligning their performance objectives with safety considerations, thus enhancing their security framework without impacting their functional capabilities.

(Robey et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib78)) proposes the SmoothLLM algorithm, which serves as a wrapper around any existing, undefended LLM and operates in two main steps. In the perturbation step, SmoothLLM modifies several versions of an attacked input prompt, exploiting the vulnerability of adversarial prompts to character-level changes. In the aggregation step, it consolidates the responses from these altered prompts to detect and counter adversarial inputs. This method effectively lowers the attack success rate on LLMs, thereby enhancing their security against such attacks.

To mitigate prompt injection attacks on LLMs, a range of defensive measures have also been proposed.  (Yi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib122)) introduces Benchmark for Indirect Prompt Injection Attacks (BIPIA), a benchmark specifically designed to Such an analysis is critical for understanding the phenomenon and mechanism of indirect prompt injection attacks. To mitigate this issue, the paper proposes two defense strategies based on this understanding: four black-box methods, and a white-box method that employs fine-tuning through adversarial training. These methods are designed to enhance the LLMs’ ability to recognize and disregard malicious instructions embedded within the external content, thereby strengthening their defenses against indirect prompt injection attacks.

(Hines et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib33)) presents spotlighting, a suite of prompt engineering techniques designed to enhance an LLM’s ability to distinguish between different input sources. By modifying inputs to clearly indicate their origins, spotlighting preserves semantic integrity and task performance. It includes three transformation methods—delimiting, marking, and encoding—each uniquely improving the visibility of input provenance. These methods have been effectively applied across different models and tasks, significantly reducing attack success rates in various scenarios.

#### 5.2.2\. Defense on Data Extraction Attack

To mitigate the privacy risks associated with the extraction of memorized content from LLMs through simple queries, one straightforward method involves the identification and removal of personal information in the pre-processing stage of training datasets.  (Vakili et al., [2022](https://arxiv.org/html/2407.19354v1#bib.bib90)) investigates automatic de-identification as a method to minimize privacy risks in clinical data, focusing on two techniques: pseudonymization and the removal of sensitive information The findings indicate that using this method does not adversely affect the models’ performance. In fact, some tasks even showed a slight improvement in performance.

Furthermore,  (Jayaraman et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib44)) investigates two strategies to reduce privacy risks linked to potential data leaks during model training. The first strategy, early stopping of training, is less effective in enhancing security compared to the second approach, which involves training the model with differential privacy. Differential privacy is demonstrated to be a robust defense against data extraction attacks, though it increases model perplexity. This emphasizes the trade-off between enhanced privacy protection and model performance.

Additionally, a novel approach using prompt tuning has been introduced (Ozdayi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib69)). This technique facilitates the customization of privacy-utility trade-offs through a user-specified hyperparameter, effectively regulating the rates at which memorized content is extracted. This strategy ensures a balanced approach, safeguarding privacy while maintaining model utility.

#### 5.2.3\. Defense on Inference Attack

(Shejwalkar and Houmansadr, [2021](https://arxiv.org/html/2407.19354v1#bib.bib82)) introduces Distillation for Membership Privacy (DMP), a novel strategy against inference attacks that employs knowledge distillation to enhance privacy in machine learning models. DMP not only preserves but also enhances the utility of the resulting models. This approach has been shown to significantly improve privacy protection while maintaining robust model performance.

(Tong et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib88)) presents InferDPT, a novel framework designed for privacy-preserving inference that integrates differential privacy into text generation with black-box LLMs. InferDPT features a perturbation module that utilizes RANTEXT, a differentially private mechanism developed for text perturbation, alongside an extraction module that ensures the coherence and consistency of the generated text. This framework effectively enhances user privacy protection.

(Yu et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib125)) proposes a meta-framework for private deep learning that captures key principles from recent fine-tuning methods to enhance privacy without compromising performance. It introduces an efficient, sparse algorithm for the differentially private fine-tuning of large-scale pre-trained language models, ensuring high utility with robust privacy protections.

Table [2](https://arxiv.org/html/2407.19354v1#S5.T2 "Table 2 ‣ 5.1.3\. Defense on Misunderstanding ‣ 5.1\. Mitigating Technical Vulnerabilities ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") presents a summary of defensive strategies for malicious attacks, offering a concise overview for quick reference.

Table 3\. Summary of Defensive Strategies Against Specific Threats

 | Threats | Method Name | Key Mechanism | Advantages / Limitations |
| --- | --- | --- | --- |
| Knowledge Poisoning | Provenance-Based Poison Detection (Baracaldo et al., [2017](https://arxiv.org/html/2407.19354v1#bib.bib12)) | Utilizes data provenance to detect and filter poisonous data in training sets | Enables use of online and regularly re-trained models; Supports both partially trusted and fully untrusted datasets |
| ParaFuzz (Yan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib114)) | Uses interpretability of model predictions to detect poisoned samples, employing fuzzing for precise paraphrase prompts | Effectively detects poisoned samples; Excels against covert attacks |
| Data Filtering & Reducing Effective Model Capacity (Wan et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib93)) | Utilizes data filtering to remove high-loss examples and reduces model capacity to hinder learning from poison data | Reduces poisoning effectiveness / Demands trade-offs between performance and safety |
| Functional Manipulation | ToolEmu (Ruan et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib79)) | Utilizes a LM to simulate tool execution and assess agent risks through an automatic evaluator | Offers flexibility and dynamic testing capabilities / Emulators may overlook essential constraints |
| Safety Standards (Anderljung et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib10)) | Proposes pre-deployment risk assessments, external reviews, informed deployment decisions, monitoring post-deployment | Balances safety risks with innovation benefits |
| Output Manipulation | BERTective (Fornaciari et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib28)) | Enhances BERT with additional attention layers to detect deception in Italian dialogues | Enhances deception detection accuracy Limited effectiveness of broader context |
| ReCon (Wang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib99)) | Employs formulation and refinement processes with perspective transitions to understand mental states | Enhances ability to discern and counteract deception |
| MAgIC (Xu et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib111)) | Uses games and game theory, combined with PGM, to evaluate LLM agents | Enhances ability to navigate complex social and cognitive dimensions | 

### 5.3\. Mitigating Specific Threats

#### 5.3.1\. Defense on Knowledge Poisoning

(Baracaldo et al., [2017](https://arxiv.org/html/2407.19354v1#bib.bib12)) proposes a new method for detecting and filtering poisonous data in the training sets of supervised learning models. It specifically utilizes data provenance to identify groups of data with a high correlation in their likelihood of being poisoned. This innovative approach aids in the effective identification and removal of malicious data.  (Yan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib114)) presents ParaFuzz, a novel framework for detecting poisoned samples at test time in large language models (LLMs), leveraging the interpretability of model predictions. The effectiveness of PARAFUZZ heavily depends on the specific prompts used with ChatGPT, which is employed to ensure high-quality paraphrasing. To optimize the detection process, the study adopts fuzzing to develop precise paraphrase prompts. These prompts are designed to effectively neutralize backdoor triggers while preserving the semantic integrity of the text.

There is still a significant gap in research focused on developing efficient defense strategies to protect LLMs from knowledge poisoning attacks (Das et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib18)). Furthermore, empirical evidence indicates that LLMs are increasingly susceptible to these attacks. Current defense mechanisms, such as filtering data or reducing model capacity, provide only limited protection and often result in decreased test accuracy (Wan et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib93)).

Besides technical solutions, specialized security strategies for AI systems are crucial, including verifying model sources, limiting sensitive training data, and detecting and mitigating attacks. Regular security reviews and risk assessments should also be conducted to identify and address new threats, ensuring AI systems are secure and up-to-date (Dilmaghani et al., [2019](https://arxiv.org/html/2407.19354v1#bib.bib22)).

#### 5.3.2\. Defense on Functional Manipulation

Given the emergence of Functional Manipulation as a new risk associated with the deployment of LLM agents, research on this specific threat remains limited. Thus, proactive security measures are essential. When using third-party LLM agents, it is crucial to protect personal privacy and be wary of excessive personal data requests by third parties. Users should limit data sharing, especially avoiding sensitive or personally identifiable information during interactions with LLM agents. Additionally, understanding and utilizing the data protection settings offered by LLM agents is vital. Adjusting privacy settings helps control what data can be collected and processed. Choosing providers with a strong reputation and transparency is also recommended, as these providers should have clear data usage and privacy protection policies along with a robust security track record (Zhang et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib132)).

Furthermore, to address the challenges posed by Functional Manipulation, the introduction of the ToolEmu (Ruan et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib79)) framework represents a significant advancement. This framework employs a language model to emulate tool execution, which allows for extensive and scalable testing of LM agents across diverse scenarios and toolsets. Coupled with an LM-based automatic safety evaluator, ToolEmu facilitates the identification and quantification of risks by examining potential failures and subsequent consequences. This method provides a dynamic alternative to traditional static sandbox evaluations, enhancing the ability to detect and mitigate high-stakes, long-tail risks effectively.

Additionally,  (Anderljung et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib10)) proposes an initial set of safety standards as an essential first step in industry self-regulation. These standards include pre-deployment risk assessments, external reviews of model behavior, the use of risk assessments to inform deployment decisions, and monitoring and responding to new information about model functionality post-deployment. This approach contributes valuable insights to the broader discussion on balancing public safety risks with the benefits of innovation in AI development.

#### 5.3.3\. Defense on Output Manipulation

To prevent individual LLM agents from being deceived by other agents, it is advisable to enhance their detection capabilities to determine whether they have encountered deception.  (Fornaciari et al., [2021](https://arxiv.org/html/2407.19354v1#bib.bib28))investigates using BERT with some added attention layers to detect deception in text, particularly in the context of Italian dialogues. This study establishes new methods for identifying deception and discusses how various contexts and semantic information contribute to detecting deceptive content.

Inspired by human recursive thinking in the Avalon game,  (Wang et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib99)) introduces Recursive Contemplation (ReCon), a framework designed to enhance LLMs’ ability to detect and counter deceptive information. ReCon employs formulation, which generates initial thoughts and speech, and refinement, which improves these outputs. It also includes two perspective transitions, aiding LLMs in understanding others’ mental states and how others perceive their own mental states.

Additionally,  (Xu et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib111)) has developed a benchmarking framework called MAgIC, designed to evaluate LLMs in multi-agent environments. It utilizes games and game theory scenarios to test models on reasoning, cooperation, and adaptability. The research employs Probabilistic Graphical Modeling (PGM) to enhance models’ capabilities in handling complex social interactions.

Table [3](https://arxiv.org/html/2407.19354v1#S5.T3 "Table 3 ‣ 5.2.3\. Defense on Inference Attack ‣ 5.2\. Mitigating Malicious Attacks ‣ 5\. Defensive Strategies Against Threats ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies") presents an overview of methods to mitigate specific threats, serving as a comprehensive guide for understanding effective defenses.

## 6\. Future Trends and Discussion

With the continuous advancements in LLM agents, these agents have become capable of effectively interacting with users through complex observations, reasoning, and task execution, demonstrating broad application prospects across multiple domains. Particularly with the development of Multimodal Large Language Model (MLLM) agents, LLM agents can now process various data types, including text, images, and audio, significantly expanding their application scope. Moreover, by incorporating Large Language Model Multi-Agent (LLM-MA) systems, different LLM agents can collaborate to accomplish more complex tasks. The integration of these technologies will contribute to building more intelligent and efficient systems. However, the widespread application of these advanced technologies also introduces significant challenges related to privacy and security. Through discussions on future trends, our aim is to provide insights for researchers, developers, and policymakers on how to optimize these technologies and overcome related challenges.

### 6.1\. Multimodal Large Language Model Agent

#### 6.1.1\. The Development of MLLM Agent

Recent advancements in LLMs have significantly surpassed traditional boundaries of language processing. These models now incorporate supplementary components such as instruction, interface, tools, knowledge, and memory, evolving into intelligent LLM agents that demonstrate expanded reasoning and expertise. Research studies  (Yang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib118); Wu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib107)) indicate efforts to bridge the gap between language models and multimodal tools, with intelligent agents like Visual ChatGPT  (Wu et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib107)) and MMREACT  (Yang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib118)) employing sophisticated prompt engineering techniques to achieve this target. Such efforts have given rise to the field of Multimodal Large Language Models (MLLMs). The general architecture of the MLLM is depicted in Figure [12](https://arxiv.org/html/2407.19354v1#S6.F12 "Figure 12 ‣ 6.1.1\. The Development of MLLM Agent ‣ 6.1\. Multimodal Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")

![Refer to caption](img/d3cb0c8b888c6831caf600b9174978d9.png)

Figure 12\. The general architecture of the MLLM

MLLMs are based on LLMs and enhanced with the capability to receive, reason, and output multimodal information. By integrating various data modalities, such as text, image, audio and video, these models are not only capable of understanding information from a single modality but can also process and interpret across modalities, thus achieving a comprehensive understanding of complex information (Yin et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib123)). The application of MLLMs has extended to several other fields, including medical image analysis (Zhang et al., [2023a](https://arxiv.org/html/2407.19354v1#bib.bib131); Moor et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib67)) and document processing (Hu et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib36); Liu et al., [2024c](https://arxiv.org/html/2407.19354v1#bib.bib60)).

Moreover, the development of multimodal agents based on MLLMs, such as embodied agents (Huang et al., [2024c](https://arxiv.org/html/2407.19354v1#bib.bib39)) and graphical user interface agents (Wang et al., [2024b](https://arxiv.org/html/2407.19354v1#bib.bib95)), has further enhanced these models’ interactive capabilities in physical environments. These agents, utilizing MLLMs as planners and following natural language instructions to navigate and interact effectively in the real world, are not only designed to understand and generate information but are also equipped with essential skills such as perception, reasoning, planning, and execution. This enables them to operate effectively in complex real-world environments  (Xie et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib109)).

With the advent of MLLM agents, the potential for achieving Artificial General Intelligence (AGI) has become more feasible, leading to significant advancements in Embodied AI. The ability of agent robots to understand and respond to human commands is crucial, especially in service-oriented tasks. The substantial progress in MLLMs has equipped them with the ability to comprehend and generate natural human instructions effectively. This progress could enable robots to learn user preferences and provide services that closely mimic human interaction.

#### 6.1.2\. The Security and Privacy Research on MLLM Agent

The development of embodied agents capable of interacting with the real world becomes a highly active area of research. However, MLLM agents also present several security vulnerabilities, one of which is the phenomenon of multimodal hallucinations.

![Refer to caption](img/5460e8d3dd2176c49b278126b5e6e8d7.png)

Figure 13\. Illustration of multimodal hallucinations . Given an image, an MLLM agent outputs a corresponding response with two primary forms

Unlike language hallucinations, multimodal hallucinations refer to the phenomenon where the output descriptions generated by MLLMs are inconsistent with the actual content of images  (Yin et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib124)), as shown in Figure [13](https://arxiv.org/html/2407.19354v1#S6.F13 "Figure 13 ‣ 6.1.2\. The Security and Privacy Research on MLLM Agent ‣ 6.1\. Multimodal Large Language Model Agent ‣ 6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies"). These phenomena manifest in two primary forms (Lee et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib50)): one involves generated content that includes objects which are inconsistent with or absent from the target image  (Zhai et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib127); Liu et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib57)); the other, a more complex form, encompasses holistic misrepresentations of entire scenes or environments (Sun et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib84)).

Current methods to reduce these hallucinations encompass several approaches, such as utilizing self-feedback with visual cues to enhance model accuracy  (Lee et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib50)), employing instruction-tuning techniques to refine the model’s response to respond to human instructions  (Liu et al., [2024a](https://arxiv.org/html/2407.19354v1#bib.bib57)). implementing error-correction processes that identify and rectify hallucinations within the generated text  (Yin et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib124)). Despite these efforts, significant challenges remain, requiring a sophisticated ability to distinguish between accurate and hallucinatory outputs, along with improvements in training approaches to boost the reliability of the outputs.

Similar to LLM agents, MLLM agents can be susceptible to crafted attacks (Qi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib76); Bagdasaryan et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib11); Shayegani et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib81)). These agents may be maliciously manipulated to produce biased or undesirable responses. However, research in this area is still in its early stages. Therefore, enhancing the safety of these MLLM agents is an essential focus of ongoing research. Improvements in MLLM agents safety will involve developing robust mechanisms to detect and mitigate these vulnerabilities, ensuring that MLLM agents can function reliably and securely in diverse applications. Such advancements are crucial for the broader adoption and ethical deployment of AI technologies in real-world environment.

### 6.2\. Large Language Model Multi-Agent System

#### 6.2.1\. The Development of LLM-MA System

LLM agents exhibit advanced reasoning and planning capabilities, approaching human-like levels of decision-making and interaction. These agents are adept at perceiving their environments, making informed decisions, and executing actions based on complex contexts (Yao et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib119)).

Inspired by the impressive abilities of a single LLM agent, LLM Multi-Agent systems have been proposed (see Figure [14](https://arxiv.org/html/2407.19354v1#S6.F14 "Figure 14 ‣ 6.2.1\. The Development of LLM-MA System ‣ 6.2\. Large Language Model Multi-Agent System ‣ 6\. Future Trends and Discussion ‣ The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies")). Such systems work based on several agents having collective intelligence and specialized skills, in which case each one is specialized to outperform in a specific domain. This specialization allows for a distributed approach to problem-solving, where each agent contributes its unique expertise, enhancing the overall effectiveness and efficiency of the system. In this scenario, multiple autonomous agents work together in planning, discussion, and decision-making, closely resembling human group collaboration in solving tasks. This approach leverages the communication abilities of LLMs, using their text generation for interaction and response to text inputs (Guo et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib31)).

![Refer to caption](img/5eeb86be943ab4c2aa08d4f14cc83e37.png)

Figure 14\. The Architecture of LLM-MA Systems

The application of LLM-MA systems spans across various fields, broadly categorized into two main types: problem solving and world simulation (Guo et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib31)). For problem-solving applications, such as multi-robot systems (Mandi et al., [2023](https://arxiv.org/html/2407.19354v1#bib.bib64)) and software development (Du et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib24)), these systems enable interactions among diverse agents. This collaborative capability effectively solves complex real-world problems, mirroring the cooperative nature of human group work in tackling multifaceted challenges. On the other hand, world simulation encompasses applications such as society simulations (Park et al., [2023b](https://arxiv.org/html/2407.19354v1#bib.bib71)) and game simulation (Xu et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib113)). The case studies parts presented in this paper illustrates the application of world simulation to depict the threats faced by LLM agents and their impacts, presenting one of the many facets of LLM-MA systems utilization.

#### 6.2.2\. The Security and Privacy Research on LLM-MA System

As research on LLM-MA system increases rapidly, numerous challenges have emerged. Each agent in a multi-agent system may need to access and process sensitive data, and even execute code. This has sparked discussions on the security and privacy concerns related to multi-agent systems.

Each agent within a multi-agent system may need to access and process sensitive data, and even execute code. Moreover, due to the intercommunication and interconnection between agents, security issues originating from a single agent can have profound and amplified effects in a multi-agent scenario. This has intensified the need for focused discussions on security and privacy issues in multi-agent environments.

The issue of hallucination, where agents generate outputs based on incorrect or fabricated information, represents a significant challenge for both LLMs and LLM Agents. This problem becomes even more complex in a multi-agent context due to the interconnected nature of these agents and their frequent communication. Misinformation from one agent can be accepted and further propagated by others within the network, leading to a cascade of misinformation. To mitigate this issue, it is crucial to correct errors at the individual agent level and also to manage the flow of information between agents, thereby preventing the spread of inaccurate information throughout the entire system (Guo et al., [2024](https://arxiv.org/html/2407.19354v1#bib.bib31)).

Furthermore, the capability of LLM multi-agent systems to interact with files and execute code offers extensive possibilities for their application. However, the presence of potentially malicious LLM agents within the system poses significant risks. In one case, these agents may operate in a passive listening mode, where they receive information shared by other agents to perform tasks, but at the same time, they leak confidential information to attackers deliberately. In the other case, malicious LLM agents may engage in a active communication mode, spreading virus-infected files, phishing messages, or other malicious codes, attempting to attack or disrupt other agents within the system. To mitigate this risk, incorporating human feedback and user authorization for each step can help reduce these threats. This necessitates designing the system with robust security measures to prevent unauthorized access or misuse. One effective approach is the implementation of a state-less oracle agent, which can monitor each sensitive task and assesses whether it constitutes a malicious activity (Talebirad and Nadiri, [2023](https://arxiv.org/html/2407.19354v1#bib.bib85)).

Currently, research on privacy and security in LLM-MA systems has not received widespread attention. However, with the rapid development of LLM-MA technology, these issues are becoming increasingly prominent. Therefore, there is an urgent need for robust security solutions to mitigate these emerging challenges.

## 7\. Conclusion

In this survey, we have explored the multifaceted security and privacy challenges faced by LLM agents, including the two categories of the sources of threats: inherited threats from LLM and specific threats on agents. Also, we present the security and privacy impacts on humans, environment, and other agents. Based on those, we discuss the corresponding defensive strategies. Additionally, we have discussed future trends in this field. To facilitate an in-depth understanding, we have incorporated a variety of case studies via a virtual town project. By highlighting the challenges that LLM agents encounter, we aim to inspire further research and exploration by researchers and developers in enhancing the security and privacy of LLM agents in the future.

## References

*   (1)
*   Cha (2022) 2022. *ChatGPT*. [https://openai.com/chatgpt](https://openai.com/chatgpt)
*   Gem (2023) 2023. *Gemini - Chat to Supercharge Your Ideas*. [https://gemini.google.com](https://gemini.google.com)
*   Mal (2023) Embrace The Red 2023. *Malicious ChatGPT Agents: How GPTs Can Quietly Grab Your Data (Demo) · Embrace The Red*. Embrace The Red. [https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/](https://embracethered.com/blog/posts/2023/openai-custom-malware-gpt/)
*   Wha (2023) Prompt Engineering 2023. *What Are Large Language Model (LLM) Agents and Autonomous Agents*. Prompt Engineering. [https://promptengineering.org/what-are-large-language-model-llm-agents/](https://promptengineering.org/what-are-large-language-model-llm-agents/)
*   Int (2024a) 2024a. *Introducing Meta Llama 3: The Most Capable Openly Available LLM to Date*. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
*   Int (2024b) 2024b. *Introducing the next Generation of Claude*. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)
*   Abdelnabi et al. (2023) Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, and Mario Fritz. 2023. *LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games*. [https://doi.org/10.48550/arXiv.2309.17234](https://doi.org/10.48550/arXiv.2309.17234) arXiv:2309.17234
*   Aksitov et al. (2023) Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yunhsuan Sung. 2023. *Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models*. [https://doi.org/10.48550/arXiv.2302.05578](https://doi.org/10.48550/arXiv.2302.05578) arXiv:2302.05578
*   Anderljung et al. (2023) Markus Anderljung, Joslyn Barnhart, Anton Korinek, Jade Leung, Cullen O’Keefe, Jess Whittlestone, Shahar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist, Gillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav Shavit, Divya Siddarth, Robert Trager, and Kevin Wolf. 2023. *Frontier AI Regulation: Managing Emerging Risks to Public Safety*. [https://doi.org/10.48550/arXiv.2307.03718](https://doi.org/10.48550/arXiv.2307.03718) arXiv:2307.03718
*   Bagdasaryan et al. (2023) Eugene Bagdasaryan, Tsung-Yin Hsieh, Ben Nassi, and Vitaly Shmatikov. 2023. *Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs*. [https://doi.org/10.48550/arXiv.2307.10490](https://doi.org/10.48550/arXiv.2307.10490) arXiv:2307.10490 [cs]
*   Baracaldo et al. (2017) Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and Jaehoon Amir Safavi. 2017. Mitigating Poisoning Attacks on Machine Learning Models: A Data Provenance Based Approach. In *Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security* *(AISec ’17)*. Association for Computing Machinery, New York, NY, USA, 103–110. [https://doi.org/10.1145/3128572.3140450](https://doi.org/10.1145/3128572.3140450)
*   Bran et al. (2023) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and Philippe Schwaller. 2023. *ChemCrow: Augmenting Large-Language Models with Chemistry Tools*. [https://doi.org/10.48550/arXiv.2304.05376](https://doi.org/10.48550/arXiv.2304.05376) arXiv:2304.05376
*   Brundage et al. (2018) Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum S. Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, and Dario Amodei. 2018. The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation. *arXiv preprint arXiv:1802.07228* (2018). arXiv:1802.07228 [http://arxiv.org/abs/1802.07228](http://arxiv.org/abs/1802.07228)
*   Carlini et al. (2023) Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramèr. 2023. *Poisoning Web-Scale Training Datasets Is Practical*. arXiv:2302.10149 [http://arxiv.org/abs/2302.10149](http://arxiv.org/abs/2302.10149)
*   Carlini et al. (2021) Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large Language Models. In *30th USENIX Security Symposium (USENIX Security 21)*. 2633–2650. [https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting)
*   Charan et al. (2023) P. V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, and Sandeep K. Shukla. 2023. *From Text to MITRE Techniques: Exploring the Malicious Use of Large Language Models for Generating Cyber Attack Payloads*. [https://doi.org/10.48550/arXiv.2305.15336](https://doi.org/10.48550/arXiv.2305.15336) arXiv:2305.15336
*   Das et al. (2024) Badhan Chandra Das, M. Hadi Amini, and Yanzhao Wu. 2024. *Security and Privacy Challenges of Large Language Models: A Survey*. [https://doi.org/10.48550/arXiv.2402.00888](https://doi.org/10.48550/arXiv.2402.00888) arXiv:2402.00888
*   Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. *MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots*. arXiv:2307.08715 [http://arxiv.org/abs/2307.08715](http://arxiv.org/abs/2307.08715)
*   Deshpande et al. (2023) Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. *Toxicity in ChatGPT: Analyzing Persona-assigned Language Models*. [https://doi.org/10.48550/arXiv.2304.05335](https://doi.org/10.48550/arXiv.2304.05335) arXiv:2304.05335
*   Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. Chain-of-Verification Reduces Hallucination in Large Language Models. *arXiv preprint arXiv:2309.11495* (2023). [https://doi.org/10.48550/ARXIV.2309.11495](https://doi.org/10.48550/ARXIV.2309.11495)
*   Dilmaghani et al. (2019) Saharnaz Dilmaghani, Matthias R. Brust, Grégoire Danoy, Natalia Cassagnes, Johnatan Pecero, and Pascal Bouvry. 2019. Privacy and Security of Big Data in AI Systems: A Research and Standards Perspective. In *2019 IEEE International Conference on Big Data (Big Data)*. 5737–5743. [https://doi.org/10.1109/BigData47090.2019.9006283](https://doi.org/10.1109/BigData47090.2019.9006283)
*   Dong et al. (2023) Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou Yu. 2023. Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (New York, NY, USA) *(KDD ’23)*. Association for Computing Machinery, 5792–5793. [https://doi.org/10.1145/3580305.3599572](https://doi.org/10.1145/3580305.3599572)
*   Du et al. (2024) Zhuoyun Du, Chen Qian, Wei Liu, Zihao Xie, Yifei Wang, Yufan Dang, Weize Chen, and Cheng Yang. 2024. *Multi-Agent Software Development through Cross-Team Collaboration*. [https://doi.org/10.48550/arXiv.2406.08979](https://doi.org/10.48550/arXiv.2406.08979) arXiv:2406.08979
*   Ebrahimi et al. (2021) Sayna Ebrahimi, Suzanne Petryk, Akash Gokul, William Gan, Joseph E. Gonzalez, Marcus Rohrbach, and Trevor Darrell. 2021. Remembering for the Right Reasons: Explanations Reduce Catastrophic Forgetting. *Applied AI Letters* 2, 4 (2021), e44. [https://doi.org/10.1002/ail2.44](https://doi.org/10.1002/ail2.44)
*   Falade (2023) Polra Victor Falade. 2023. Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social Engineering Attacks. *International Journal of Scientific Research in Computer Science, Engineering and Information Technology* 9, 5 (2023), 185–198. [https://doi.org/10.32628/CSEIT2390533](https://doi.org/10.32628/CSEIT2390533)
*   Fang et al. (2024) Richard Fang, Rohan Bindu, Akul Gupta, and Daniel Kang. 2024. *LLM Agents Can Autonomously Exploit One-day Vulnerabilities*. [https://doi.org/10.48550/arXiv.2404.08144](https://doi.org/10.48550/arXiv.2404.08144) arXiv:2404.08144
*   Fornaciari et al. (2021) Tommaso Fornaciari, Federico Bianchi, Massimo Poesio, and Dirk Hovy. 2021. BERTective: Language Models and Contextual Information for Deception Detection. In *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume* (Online). Association for Computational Linguistics, 2699–2708. [https://doi.org/10.18653/v1/2021.eacl-main.232](https://doi.org/10.18653/v1/2021.eacl-main.232)
*   Fu et al. (2023) Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, and Tao Jiang. 2023. *Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration*. [https://doi.org/10.48550/arXiv.2311.06062](https://doi.org/10.48550/arXiv.2311.06062) arXiv:2311.06062
*   Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. *Not What You’ve Signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection*. arXiv:2302.12173 [http://arxiv.org/abs/2302.12173](http://arxiv.org/abs/2302.12173)
*   Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. *Large Language Model Based Multi-Agents: A Survey of Progress and Challenges*. [https://doi.org/10.48550/arXiv.2402.01680](https://doi.org/10.48550/arXiv.2402.01680) arXiv:2402.01680
*   Henderson et al. (2017) Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. 2017. *Ethical Challenges in Data-Driven Dialogue Systems*. [https://doi.org/10.48550/arXiv.1711.09050](https://doi.org/10.48550/arXiv.1711.09050) arXiv:1711.09050
*   Hines et al. (2024) Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. 2024. *Defending Against Indirect Prompt Injection Attacks With Spotlighting*. [https://doi.org/10.48550/arXiv.2403.14720](https://doi.org/10.48550/arXiv.2403.14720) arXiv:2403.14720
*   Hong et al. (2023) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. 2023. *MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework*. [https://doi.org/10.48550/arXiv.2308.00352](https://doi.org/10.48550/arXiv.2308.00352) arXiv:2308.00352
*   Howard and Ruder (2018) Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (Melbourne, Australia), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational Linguistics, 328–339. [https://doi.org/10.18653/v1/P18-1031](https://doi.org/10.18653/v1/P18-1031)
*   Hu et al. (2024b) Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, and Fei Huang. 2024b. *mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model*. [https://doi.org/10.48550/arXiv.2311.18248](https://doi.org/10.48550/arXiv.2311.18248) arXiv:2311.18248
*   Hu et al. (2024a) Hanxu Hu, Pinzhen Chen, and Edoardo M. Ponti. 2024a. *Fine-Tuning Large Language Models with Sequential Instructions*. [https://doi.org/10.48550/arXiv.2403.07794](https://doi.org/10.48550/arXiv.2403.07794) arXiv:2403.07794
*   Huang et al. (2024a) Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng Song, Junfeng Yao, and Jinsong Su. 2024a. *Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal*. [https://doi.org/10.48550/arXiv.2403.01244](https://doi.org/10.48550/arXiv.2403.01244) arXiv:2403.01244
*   Huang et al. (2024c) Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 2024c. *An Embodied Generalist Agent in 3D World*. [https://doi.org/10.48550/arXiv.2311.12871](https://doi.org/10.48550/arXiv.2311.12871) arXiv:2311.12871
*   Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. *A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions*. arXiv:2311.05232 [http://arxiv.org/abs/2311.05232](http://arxiv.org/abs/2311.05232)
*   Huang et al. (2024b) Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024b. *Understanding the Planning of LLM Agents: A Survey*. [https://doi.org/10.48550/arXiv.2402.02716](https://doi.org/10.48550/arXiv.2402.02716) arXiv:2402.02716
*   Hubinger et al. (2024) Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez. 2024. *Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training*. arXiv:2401.05566 [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)
*   Ishihara (2023) Shotaro Ishihara. 2023. Training Data Extraction From Pre-trained Language Models: A Survey. In *Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023)* (Toronto, Canada), Anaelia Ovalle, Kai-Wei Chang, Ninareh Mehrabi, Yada Pruksachatkun, Aram Galystan, Jwala Dhamala, Apurv Verma, Trista Cao, Anoop Kumar, and Rahul Gupta (Eds.). Association for Computational Linguistics, 260–275. [https://aclanthology.org/2023.trustnlp-1.23](https://aclanthology.org/2023.trustnlp-1.23)
*   Jayaraman et al. (2023) Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai, and David Evans. 2023. *Combing for Credentials: Active Pattern Extraction from Smart Reply*. [https://doi.org/10.48550/arXiv.2207.10802](https://doi.org/10.48550/arXiv.2207.10802) arXiv:2207.10802
*   Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards Mitigating Hallucination in Large Language Models via Self-Reflection. *arXiv preprint arXiv:2310.06271* (2023). [https://arxiv.org/abs/2310.06271](https://arxiv.org/abs/2310.06271)
*   Kandpal et al. (2024) Nikhil Kandpal, Krishna Pillutla, Alina Oprea, Peter Kairouz, Christopher A. Choquette-Choo, and Zheng Xu. 2024. *User Inference Attacks on Large Language Models*. [https://doi.org/10.48550/arXiv.2310.09266](https://doi.org/10.48550/arXiv.2310.09266) arXiv:2310.09266
*   Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. 2023. *ProPILE: Probing Privacy Leakage in Large Language Models*. arXiv:2307.01881 [http://arxiv.org/abs/2307.01881](http://arxiv.org/abs/2307.01881)
*   Kurita et al. (2020) Keita Kurita, Paul Michel, and Graham Neubig. 2020. *Weight Poisoning Attacks on Pre-trained Models*. arXiv:2004.06660 [http://arxiv.org/abs/2004.06660](http://arxiv.org/abs/2004.06660)
*   Lee et al. (2022) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (Dublin, Ireland), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, 8424–8445. [https://doi.org/10.18653/v1/2022.acl-long.577](https://doi.org/10.18653/v1/2022.acl-long.577)
*   Lee et al. (2024) Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. 2024. *Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision*. [https://doi.org/10.48550/arXiv.2311.07362](https://doi.org/10.48550/arXiv.2311.07362) arXiv:2311.07362
*   Lei et al. (2022) Yunjiao Lei, Dayong Ye, Sheng Shen, Yulei Sui, Tianqing Zhu, and Wanlei Zhou. 2022. New Challenges in Reinforcement Learning: A Survey of Security and Privacy. *Artif. Intell. Rev.* 56, 7 (2022), 7195–7236. [https://doi.org/10.1007/s10462-022-10348-5](https://doi.org/10.1007/s10462-022-10348-5)
*   Leng et al. (2024) Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2024. Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 13872–13882.
*   Li et al. (2023b) Chenyang Li, Zhao Song, Weixin Wang, and Chiwun Yang. 2023b. *A Theoretical Insight into Attack and Defense of Gradient Leakage in Transformer*. arXiv:2311.13624 [http://arxiv.org/abs/2311.13624](http://arxiv.org/abs/2311.13624)
*   Li et al. (2023a) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. 2023a. *Multi-Step Jailbreaking Privacy Attacks on ChatGPT*. arXiv:2304.05197 [http://arxiv.org/abs/2304.05197](http://arxiv.org/abs/2304.05197)
*   Lin et al. (2023) Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. 2023. *AgentSims: An Open-Source Sandbox for Large Language Model Evaluation*. [https://doi.org/10.48550/arXiv.2308.04026](https://doi.org/10.48550/arXiv.2308.04026) arXiv:2308.04026
*   Liu et al. (2023a) Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. 2023a. *Exposing Attention Glitches with Flip-Flop Language Modeling*. [https://doi.org/10.48550/arXiv.2306.00946](https://doi.org/10.48550/arXiv.2306.00946) arXiv:2306.00946
*   Liu et al. (2024a) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2024a. *Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning*. [https://doi.org/10.48550/arXiv.2306.14565](https://doi.org/10.48550/arXiv.2306.14565) arXiv:2306.14565
*   Liu et al. (2024b) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2024b. AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. In *The Twelfth International Conference on Learning Representations*. [https://openreview.net/forum?id=7Jwpw4qKkb](https://openreview.net/forum?id=7Jwpw4qKkb)
*   Liu et al. (2023b) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023b. *Prompt Injection Attack against LLM-integrated Applications*. [https://doi.org/10.48550/arXiv.2306.05499](https://doi.org/10.48550/arXiv.2306.05499) arXiv:2306.05499
*   Liu et al. (2024c) Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. 2024c. *TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document*. [https://doi.org/10.48550/arXiv.2403.04473](https://doi.org/10.48550/arXiv.2403.04473) arXiv:2403.04473
*   Luo et al. (2023a) Junyu Luo, Cao Xiao, and Fenglong Ma. 2023a. Zero-Resource Hallucination Prevention for Large Language Models. *arXiv preprint arXiv:2309.02654* (2023). [https://doi.org/10.48550/ARXIV.2309.02654](https://doi.org/10.48550/ARXIV.2309.02654)
*   Luo et al. (2023b) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023b. *An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning*. arXiv:2308.08747 [http://arxiv.org/abs/2308.08747](http://arxiv.org/abs/2308.08747)
*   Mahmoud and Hajj (2022) Reem A. Mahmoud and Hazem Hajj. 2022. Multi-Objective Learning to Overcome Catastrophic Forgetting in Time-series Applications. *ACM Transactions on Knowledge Discovery from Data* 16, 6 (2022), 1–20. [https://doi.org/10.1145/3502728](https://doi.org/10.1145/3502728)
*   Mandi et al. (2023) Zhao Mandi, Shreeya Jain, and Shuran Song. 2023. *RoCo: Dialectic Multi-Robot Collaboration with Large Language Models*. [https://doi.org/10.48550/arXiv.2307.04738](https://doi.org/10.48550/arXiv.2307.04738) arXiv:2307.04738
*   Mendis et al. (2007) D. S. Kalana Mendis, Asoka S. Karunananda, Udaya Samaratunga, and Uditha Ratnayake. 2007. An Approach to the Development of Commonsense Knowledge Modeling Systems for Disaster Management. 28, 2 (2007), 179–196. [https://doi.org/10.1007/s10462-009-9097-6](https://doi.org/10.1007/s10462-009-9097-6)
*   Mondesire and Wiegand (2023) Sean Mondesire and R. Paul Wiegand. 2023. Mitigating Catastrophic Forgetting with Complementary Layered Learning. *Electronics* 12, 3 (2023), 706. [https://doi.org/10.3390/electronics12030706](https://doi.org/10.3390/electronics12030706)
*   Moor et al. (2023) Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. 2023. Med-Flamingo: A Multimodal Medical Few-shot Learner. In *Proceedings of the 3rd Machine Learning for Health Symposium* *(Proceedings of Machine Learning Research, Vol. 225)*. PMLR, 353–367. [https://proceedings.mlr.press/v225/moor23a.html](https://proceedings.mlr.press/v225/moor23a.html)
*   OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, and Balcom. 2024. *GPT-4 Technical Report*. [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774) arXiv:2303.08774
*   Ozdayi et al. (2023) Mustafa Safa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, and Rahul Gupta. 2023. *Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning*. arXiv:2305.11759 [https://arxiv.org/abs/2305.11759](https://arxiv.org/abs/2305.11759)
*   Pang et al. (2024) Jing-Cheng Pang, Heng-Bo Fan, Pengyuan Wang, Jia-Hao Xiao, Nan Tang, Si-Hang Yang, Chengxing Jia, Sheng-Jun Huang, and Yang Yu. 2024. *Empowering Language Models with Active Inquiry for Deeper Understanding*. [https://doi.org/10.48550/arXiv.2402.03719](https://doi.org/10.48550/arXiv.2402.03719) arXiv:2402.03719
*   Park et al. (2023b) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023b. *Generative Agents: Interactive Simulacra of Human Behavior*. [https://doi.org/10.48550/arXiv.2304.03442](https://doi.org/10.48550/arXiv.2304.03442) arXiv:2304.03442
*   Park et al. (2023a) Peter S. Park, Simon Goldstein, Aidan O’Gara, Michael Chen, and Dan Hendrycks. 2023a. *AI Deception: A Survey of Examples, Risks, and Potential Solutions*. [https://doi.org/10.48550/arXiv.2308.14752](https://doi.org/10.48550/arXiv.2308.14752) arXiv:2308.14752
*   Peng et al. (2023) Liangzu Peng, Paris Giampouras, and Rene Vidal. 2023. The Ideal Continual Learner: An Agent That Never Forgets. In *Proceedings of the 40th International Conference on Machine Learning*. PMLR, 27585–27610. [https://proceedings.mlr.press/v202/peng23a.html](https://proceedings.mlr.press/v202/peng23a.html)
*   Peters (2023) Jay Peters. 2023. *The Bing AI Bot Has Been Secretly Running GPT-4*. The Verge. [https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm](https://www.theverge.com/2023/3/14/23639928/microsoft-bing-chatbot-ai-gpt-4-llm)
*   Puig et al. (2023) Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Michal Hlavac, So Yeon Min, Vladimír Vondruš, Theophile Gervet, Vincent-Pierre Berges, John M. Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, and Roozbeh Mottaghi. 2023. *Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots*. [https://doi.org/10.48550/arXiv.2310.13724](https://doi.org/10.48550/arXiv.2310.13724) arXiv:2310.13724
*   Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. 2023. *Visual Adversarial Examples Jailbreak Aligned Large Language Models*. [https://doi.org/10.48550/arXiv.2306.13213](https://doi.org/10.48550/arXiv.2306.13213) arXiv:2306.13213
*   Qian et al. (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. *ChatDev: Communicative Agents for Software Development*. [https://doi.org/10.48550/arXiv.2307.07924](https://doi.org/10.48550/arXiv.2307.07924) arXiv:2307.07924
*   Robey et al. (2023) Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. 2023. SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. *arXiv preprint arXiv:2310.03684* (2023). [https://doi.org/10.48550/ARXIV.2310.03684](https://doi.org/10.48550/ARXIV.2310.03684)
*   Ruan et al. (2024) Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, and Tatsunori Hashimoto. 2024. *Identifying the Risks of LM Agents with an LM-Emulated Sandbox*. [https://doi.org/10.48550/arXiv.2309.15817](https://doi.org/10.48550/arXiv.2309.15817) arXiv:2309.15817
*   Schuster et al. (2021) Roei Schuster, Congzheng Song, Eran Tromer, and Vitaly Shmatikov. 2021. You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion. In *30th USENIX Security Symposium (USENIX Security 21)*. 1559–1575. [https://www.usenix.org/conference/usenixsecurity21/presentation/schuster](https://www.usenix.org/conference/usenixsecurity21/presentation/schuster)
*   Shayegani et al. (2023) Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023. *Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models*. [https://doi.org/10.48550/arXiv.2307.14539](https://doi.org/10.48550/arXiv.2307.14539) arXiv:2307.14539
*   Shejwalkar and Houmansadr (2021) Virat Shejwalkar and Amir Houmansadr. 2021. Membership Privacy for Machine Learning Models Through Knowledge Transfer. 35, 11 (2021), 9549–9557. Issue 11. [https://doi.org/10.1609/aaai.v35i11.17150](https://doi.org/10.1609/aaai.v35i11.17150)
*   Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. *”Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models*. arXiv:2308.03825 [http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825)
*   Sun et al. (2023) Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. 2023. *Aligning Large Multimodal Models with Factually Augmented RLHF*. [https://doi.org/10.48550/arXiv.2309.14525](https://doi.org/10.48550/arXiv.2309.14525) arXiv:2309.14525
*   Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023. *Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents*. [https://doi.org/10.48550/arXiv.2306.03314](https://doi.org/10.48550/arXiv.2306.03314) arXiv:2306.03314
*   Taveekitworachai et al. (2023) Pittawat Taveekitworachai, Febri Abdullah, Mustafa Can Gursesli, Mury F. Dewantoro, Siyuan Chen, Antonio Lanata, Andrea Guazzini, and Ruck Thawonmas. 2023. Breaking Bad: Unraveling Influences and Risks of User Inputs to ChatGPT for Game Story Generation. In *Interactive Storytelling* (Cham) *(Lecture Notes in Computer Science)*, Lissa Holloway-Attaway and John T. Murray (Eds.). Springer Nature Switzerland, 285–296.
*   Toetzke et al. (2023) Malte Toetzke, Benedict Probst, and Stefan Feuerriegel. 2023. Leveraging Large Language Models to Monitor Climate Technology Innovation. *Environmental Research Letters* 18, 9 (2023), 091004. [https://doi.org/10.1088/1748-9326/acf233](https://doi.org/10.1088/1748-9326/acf233)
*   Tong et al. (2024) Meng Tong, Kejiang Chen, Jie Zhang, Yuang Qi, Weiming Zhang, Nenghai Yu, Tianwei Zhang, and Zhikun Zhang. 2024. *InferDPT: Privacy-Preserving Inference for Black-box Large Language Model*. [https://doi.org/10.48550/arXiv.2310.12214](https://doi.org/10.48550/arXiv.2310.12214) arXiv:2310.12214
*   Truong et al. (2021) Jean-Baptiste Truong, Pratyush Maini, Robert J. Walls, and Nicolas Papernot. 2021. Data-Free Model Extraction. In *2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*. 4769–4778. [https://ieeexplore.ieee.org/document/9577784](https://ieeexplore.ieee.org/document/9577784)
*   Vakili et al. (2022) Thomas Vakili, Anastasios Lamproudis, Aron Henriksson, and Hercules Dalianis. 2022. Downstream Task Performance of BERT Models Pre-Trained Using Automatically De-Identified Clinical Data. In *Proceedings of the Thirteenth Language Resources and Evaluation Conference* (Marseille, France), Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources Association, 4245–4252. [https://aclanthology.org/2022.lrec-1.451](https://aclanthology.org/2022.lrec-1.451)
*   Vander Eeckt and Van Hamme (2023) Steven Vander Eeckt and Hugo Van Hamme. 2023. Weight Averaging: A Simple Yet Effective Method to Overcome Catastrophic Forgetting in Automatic Speech Recognition. In *ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)* (Rhodes Island, Greece). IEEE, 1–5. [https://doi.org/10.1109/ICASSP49357.2023.10095147](https://doi.org/10.1109/ICASSP49357.2023.10095147)
*   Wan et al. (2023a) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023a. *Poisoning Language Models During Instruction Tuning*. arXiv:2305.00944 [http://arxiv.org/abs/2305.00944](http://arxiv.org/abs/2305.00944)
*   Wan et al. (2023b) Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. 2023b. *Poisoning Language Models During Instruction Tuning*. [https://doi.org/10.48550/arXiv.2305.00944](https://doi.org/10.48550/arXiv.2305.00944) arXiv:2305.00944
*   Wang and Li (2023) Huan Wang and Yan-Fu Li. 2023. Large Language Model Empowered by Domain-Specific Knowledge Base for Industrial Equipment Operation and Maintenance. In *2023 5th International Conference on System Reliability and Safety Engineering (SRSE)*. 474–479. [https://doi.org/10.1109/SRSE59585.2023.10336112](https://doi.org/10.1109/SRSE59585.2023.10336112)
*   Wang et al. (2024b) Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024b. *Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception*. [https://doi.org/10.48550/arXiv.2401.16158](https://doi.org/10.48550/arXiv.2401.16158) arXiv:2401.16158
*   Wang et al. (2023c) Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, and Yelong Shen. 2023c. *Adapting LLM Agents Through Communication*. [https://doi.org/10.48550/arXiv.2310.01444](https://doi.org/10.48550/arXiv.2310.01444) arXiv:2310.01444
*   Wang et al. (2023d) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023d. *A Survey on Large Language Model Based Autonomous Agents*. arXiv:2308.11432 [http://arxiv.org/abs/2308.11432](http://arxiv.org/abs/2308.11432)
*   Wang et al. (2024c) Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. 2024c. *User Behavior Simulation with Large Language Model Based Agents*. [https://doi.org/10.48550/arXiv.2306.02552](https://doi.org/10.48550/arXiv.2306.02552) arXiv:2306.02552
*   Wang et al. (2023b) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. 2023b. *Avalon’s Game of Thoughts: Battle Against Deception through Recursive Contemplation*. [https://doi.org/10.48550/arXiv.2310.01320](https://doi.org/10.48550/arXiv.2310.01320) arXiv:2310.01320
*   Wang et al. (2024a) Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang Tang, Philip S. Yu, and Qingsong Wen. 2024a. *Large Language Models for Education: A Survey and Outlook*. arXiv:2403.18105 [https://arxiv.org/abs/2403.18105](https://arxiv.org/abs/2403.18105)
*   Wang et al. (2024d) Shang Wang, Tianqing Zhu, Bo Liu, Ming Ding, Xu Guo, Dayong Ye, Wanlei Zhou, and Philip S. Yu. 2024d. *Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey*. [https://doi.org/10.48550/arXiv.2406.07973](https://doi.org/10.48550/arXiv.2406.07973) arXiv:2406.07973
*   Wang et al. (2023a) Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, and Yanwei Fu. 2023a. *WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model*. [https://doi.org/10.48550/arXiv.2308.15962](https://doi.org/10.48550/arXiv.2308.15962) arXiv:2308.15962
*   Wang et al. (2023e) Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, and Tom H. Luan. 2023e. A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. *IEEE Open Journal of the Computer Society* 4 (2023), 280–302. [https://doi.org/10.1109/OJCS.2023.3300321](https://doi.org/10.1109/OJCS.2023.3300321)
*   Wang et al. (2023g) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023g. *Aligning Large Language Models with Human: A Survey*. [https://doi.org/10.48550/arXiv.2307.12966](https://doi.org/10.48550/arXiv.2307.12966) arXiv:2307.12966
*   Wang et al. (2023f) Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, and Enze Wang. 2023f. *Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models*. arXiv:2308.11521 [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)
*   Winata et al. (2023) Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preotiuc-Pietro. 2023. Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning. In *Findings of the Association for Computational Linguistics: ACL 2023*. Association for Computational Linguistics, Toronto, Canada, 768–777. [https://doi.org/10.18653/v1/2023.findings-acl.48](https://doi.org/10.18653/v1/2023.findings-acl.48)
*   Wu et al. (2023) Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. *Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models*. [https://doi.org/10.48550/arXiv.2303.04671](https://doi.org/10.48550/arXiv.2303.04671) arXiv:2303.04671
*   Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. *The Rise and Potential of Large Language Model Based Agents: A Survey*. [https://doi.org/10.48550/arXiv.2309.07864](https://doi.org/10.48550/arXiv.2309.07864) arXiv:2309.07864
*   Xie et al. (2024) Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. 2024. *Large Multimodal Agents: A Survey*. [https://doi.org/10.48550/arXiv.2402.15116](https://doi.org/10.48550/arXiv.2402.15116) arXiv:2402.15116
*   Xu et al. (2023c) Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. 2023c. Machine Unlearning: A Survey. *ACM Comput. Surv.* 56, 1 (2023), 9:1–9:36. [https://doi.org/10.1145/3603620](https://doi.org/10.1145/3603620)
*   Xu et al. (2023a) Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng. 2023a. *MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration*. [https://doi.org/10.48550/arXiv.2311.08562](https://doi.org/10.48550/arXiv.2311.08562) arXiv:2311.08562
*   Xu et al. (2023b) Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming Cai, and Tianxiang Wang. 2023b. Enhancing Language Representation with Constructional Information for Natural Language Understanding. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)* (Toronto, Canada). Association for Computational Linguistics, 4685–4705. [https://doi.org/10.18653/v1/2023.acl-long.258](https://doi.org/10.18653/v1/2023.acl-long.258)
*   Xu et al. (2024) Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. 2024. *Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game*. [https://doi.org/10.48550/arXiv.2310.18940](https://doi.org/10.48550/arXiv.2310.18940) arXiv:2310.18940
*   Yan et al. (2023) Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, and Xiangyu Zhang. 2023. *ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP*. [https://doi.org/10.48550/arXiv.2308.02122](https://doi.org/10.48550/arXiv.2308.02122) arXiv:2308.02122
*   Yang et al. (2023b) Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu, and Shui Yu. 2023b. *A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks*. [https://doi.org/10.48550/arXiv.2308.14367](https://doi.org/10.48550/arXiv.2308.14367) arXiv:2308.14367
*   Yang et al. (2024b) Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie. 2024b. *V-IRL: Grounding Virtual Intelligence in Real Life*. [https://doi.org/10.48550/arXiv.2402.03310](https://doi.org/10.48550/arXiv.2402.03310) arXiv:2402.03310
*   Yang et al. (2024a) Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun. 2024a. *Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents*. [https://doi.org/10.48550/arXiv.2402.11208](https://doi.org/10.48550/arXiv.2402.11208) arXiv:2402.11208
*   Yang et al. (2023a) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023a. *MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action*. [https://doi.org/10.48550/arXiv.2303.11381](https://doi.org/10.48550/arXiv.2303.11381) arXiv:2303.11381
*   Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In *Proceedings of the 37th International Conference on Neural Information Processing Systems* (Red Hook, NY, USA) *(NIPS ’23)*. Curran Associates Inc., 11809–11822.
*   Yao et al. (2023) Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang. 2023. *A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly*. [https://doi.org/10.48550/arXiv.2312.02003](https://doi.org/10.48550/arXiv.2312.02003) arXiv:2312.02003
*   Ye et al. (2024) Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi, Sheng Shen, Wanlei Zhou, and Minhui Xue. 2024. *Reinforcement Unlearning*. [https://doi.org/10.48550/arXiv.2312.15910](https://doi.org/10.48550/arXiv.2312.15910) arXiv:2312.15910
*   Yi et al. (2023) Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. *Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models*. [https://doi.org/10.48550/arXiv.2312.14197](https://doi.org/10.48550/arXiv.2312.14197) arXiv:2312.14197
*   Yin et al. (2023a) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023a. A Survey on Multimodal Large Language Models. *arXiv preprint arXiv:2306.13549* (2023). [https://doi.org/10.48550/ARXIV.2306.13549](https://doi.org/10.48550/ARXIV.2306.13549)
*   Yin et al. (2023b) Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023b. *Woodpecker: Hallucination Correction for Multimodal Large Language Models*. [https://doi.org/10.48550/arXiv.2310.16045](https://doi.org/10.48550/arXiv.2310.16045) arXiv:2310.16045
*   Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. 2021. Differentially Private Fine-tuning of Language Models. In *International Conference on Learning Representations*. [https://openreview.net/forum?id=Q42f0dfjECO](https://openreview.net/forum?id=Q42f0dfjECO)
*   Yu et al. (2023) Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. 2023. *GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts*. arXiv:2309.10253 [http://arxiv.org/abs/2309.10253](http://arxiv.org/abs/2309.10253)
*   Zhai et al. (2024) Bohan Zhai, Shijia Yang, Chenfeng Xu, Sheng Shen, Kurt Keutzer, Chunyuan Li, and Manling Li. 2024. *HallE-Control: Controlling Object Hallucination in Large Multimodal Models*. [https://doi.org/10.48550/arXiv.2310.01779](https://doi.org/10.48550/arXiv.2310.01779) arXiv:2310.01779
*   Zhai et al. (2023) Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. 2023. *Investigating the Catastrophic Forgetting in Multimodal Large Language Models*. [https://doi.org/10.48550/arXiv.2309.10313](https://doi.org/10.48550/arXiv.2309.10313) arXiv:2309.10313
*   Zhan et al. (2024) Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024. *InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents*. [https://doi.org/10.48550/arXiv.2403.02691](https://doi.org/10.48550/arXiv.2403.02691) arXiv:2403.02691
*   Zhang et al. (2024b) Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2024b. *The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models*. arXiv:2305.13669 [https://arxiv.org/abs/2305.13669](https://arxiv.org/abs/2305.13669)
*   Zhang et al. (2023a) Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. 2023a. *PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering*. [https://doi.org/10.48550/arXiv.2305.10415](https://doi.org/10.48550/arXiv.2305.10415) arXiv:2305.10415
*   Zhang et al. (2024a) Zhiping Zhang, Michelle Jia, Hao-Ping (Hank) Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, and Tianshi Li. 2024a. “It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents. In *Proceedings of the CHI Conference on Human Factors in Computing Systems* (New York, NY, USA) *(CHI ’24)*. Association for Computing Machinery, 1–26. [https://doi.org/10.1145/3613904.3642385](https://doi.org/10.1145/3613904.3642385)
*   Zhang et al. (2023b) Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang. 2023b. Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization. *arXiv preprint arXiv:2311.09096* (2023). [https://doi.org/10.48550/ARXIV.2311.09096](https://doi.org/10.48550/ARXIV.2311.09096)
*   Zheng et al. (2023) Qingxiao Zheng, Zhongwei Xu, Abhinav Choudhary, Yuting Chen, Yongming Li, and Yun Huang. 2023. *Synergizing Human-AI Agency: A Guide of 23 Heuristics for Service Co-Creation with LLM-Based Agents*. [https://doi.org/10.48550/arXiv.2310.15065](https://doi.org/10.48550/arXiv.2310.15065) arXiv:2310.15065
*   Zhong et al. (2023) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023. *MemoryBank: Enhancing Large Language Models with Long-Term Memory*. [https://doi.org/10.48550/arXiv.2305.10250](https://doi.org/10.48550/arXiv.2305.10250) arXiv:2305.10250
*   Zou et al. (2024) Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. *PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models*. [https://doi.org/10.48550/arXiv.2402.07867](https://doi.org/10.48550/arXiv.2402.07867) arXiv:2402.07867