- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:45:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Devil’s Advocate: Anticipatory Reflection for LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 魔鬼的辩护者：LLM代理的预期反思
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16334](https://ar5iv.labs.arxiv.org/html/2405.16334)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16334](https://ar5iv.labs.arxiv.org/html/2405.16334)
- en: Haoyu Wang¹   Tao Li²  Zhiwei Deng²  Dan Roth¹   Yang Li² ¹UPenn   ²Google DeepMind
    {why16gzl, danroth}@seas.upenn.edu {tlinlp, zhiweideng, liyang}@google.com Work
    done during internship at Google DeepMind.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 王浩宇¹   李涛²  邓志伟²  罗丹¹   李杨² ¹宾夕法尼亚大学   ²谷歌DeepMind {why16gzl, danroth}@seas.upenn.edu
    {tlinlp, zhiweideng, liyang}@google.com 工作在谷歌DeepMind实习期间完成。
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this work, we introduce a novel approach that equips LLM agents with introspection,
    enhancing consistency and adaptability in solving complex tasks. Our approach
    prompts LLM agents to decompose a given task into manageable subtasks (i.e., to
    make a plan), and to continuously introspect upon the suitability and results
    of their actions. We implement a three-fold introspective intervention: 1) anticipatory
    reflection on potential failures and alternative remedy before action execution,
    2) post-action alignment with subtask objectives and backtracking with remedy
    to ensure utmost effort in plan execution, and 3) comprehensive review upon plan
    completion for future strategy refinement. By deploying and experimenting with
    this methodology—a zero-shot approach—within WebArena for practical tasks in web
    environments, our agent demonstrates superior performance with a success rate
    of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest
    that our introspection-driven approach not only enhances the agent’s ability to
    navigate unanticipated challenges through a robust mechanism of plan execution,
    but also improves efficiency by reducing the number of trials and plan revisions
    by 45% needed to achieve a task.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的方法，该方法为大型语言模型（LLM）代理提供了自省能力，增强了在解决复杂任务时的一致性和适应性。我们的方法促使LLM代理将给定任务分解为可管理的子任务（即制定计划），并持续自省其行动的适宜性和结果。我们实施了三重自省干预：1）在执行行动之前对潜在失败和替代补救措施的预测性反思，2）在行动后与子任务目标对齐，并通过补救措施进行回溯，以确保在计划执行中付出最大努力，以及3）在计划完成后进行全面回顾，以便于未来策略的优化。通过在WebArena中对这一零样本方法进行部署和实验，我们的代理在实际的网络环境任务中表现出优越的性能，其成功率比现有的零样本方法高出3.5%，达到了23.5%。实验结果表明，我们的自省驱动方法不仅通过稳健的计划执行机制提升了代理应对意外挑战的能力，还通过减少完成任务所需的试验和计划修订次数45%来提高了效率。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Two roads diverged in a yellow wood,
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 两条道路在黄色的林间分开，
- en: And sorry I could not travel both
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对不起，我不能同时走两条路
- en: $\cdots$
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $\cdots$
- en: Then took the other, as just as fair,
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后选择了另一条，同样公平，
- en: And having perhaps the better claim
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 也许更好的说法
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Robert Frost
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 罗伯特·弗罗斯特
- en: The enduring appeal of Frost’s emblematic poem, “The Road Not Taken,” resides
    not just in its poetic elegance, but also in the profound lesson it imparts about
    decision-making. As we stand at the crossroads of a choice, it is a daunting challenge
    to assess probable outcomes and choose a course that best aligns with our objectives.
    This task becomes even more formidable when Large Language Model (LLM) agents
    [[9](#bib.bib9), [29](#bib.bib29), [18](#bib.bib18)] have to navigate complex
    scenarios unfolding in real time, e.g., solving tasks in web environments [[11](#bib.bib11),
    [27](#bib.bib27), [2](#bib.bib2), [32](#bib.bib32)], conducting simulated science
    experiments [[23](#bib.bib23)], and solving embodied household tasks [[17](#bib.bib17)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 弗罗斯特标志性诗作《未选择的路》之所以长久吸引人，不仅在于其诗意的优雅，还在于它关于决策的深刻启示。当我们站在选择的十字路口时，评估可能的结果并选择与我们的目标最匹配的路径是一项令人畏惧的挑战。当大型语言模型（LLM）代理[[9](#bib.bib9),
    [29](#bib.bib29), [18](#bib.bib18)]必须在实时展开的复杂场景中进行导航时，例如在网络环境中解决任务[[11](#bib.bib11),
    [27](#bib.bib27), [2](#bib.bib2), [32](#bib.bib32)]，进行模拟科学实验[[23](#bib.bib23)]，以及解决具身的家庭任务[[17](#bib.bib17)]，这一任务变得更加艰巨。
- en: Indeed, LLM agent decision-making has witnessed enhancement by post-hoc reflection
    and correction [[16](#bib.bib16), [19](#bib.bib19)], coupled with adaptive planning
    [[20](#bib.bib20), [15](#bib.bib15)], where the agents learn from past successes
    and failures while concurrently mapping out flexible strategies. However, frequent
    shifts in plans, albeit a mere inconvenience for humans, can lead to disorientation
    for AI agents. This may produce confusion, a standstill, or even an infinite loop
    of failure, which substantiates the importance of *thoroughly executing a set
    plan with utmost effort before resorting to a plan revision*. Therefore, this
    paper puts forward a methodology aimed at achieving an optimal balance between
    consistency and adaptability. This critical equilibrium mirrors the resilience
    and agility that is anticipated of a capable system that is prepared for curveballs
    but unwavering in the execution of its plan.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，LLM代理的决策制定通过事后反思和修正[[16](#bib.bib16), [19](#bib.bib19)]以及适应性规划[[20](#bib.bib20),
    [15](#bib.bib15)]得到了提升，其中代理通过从过去的成功和失败中学习，同时制定灵活的策略。然而，尽管计划频繁调整对人类来说只是一个小麻烦，却可能导致AI代理的迷失方向。这可能导致混乱、停滞，甚至无限失败的循环，这证明了*在进行计划修订之前，彻底执行既定计划的极端重要性*。因此，本文提出了一种旨在在一致性和适应性之间实现最佳平衡的方法。这一关键平衡反映了一个预备迎接挑战但在计划执行上坚定不移的系统所期待的韧性和灵活性。
- en: 'In this paper, we introduce a novel approach that integrates introspection
    into the fabric of LLM agents. This approach enables agents to continuously reflect
    on their actions, thereby stimulating a learning process that dynamically optimizes
    exploration paths and enhances robust decision-making under uncertainty. Our introspective
    intervention focuses on three principal dimensions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种将自省融入LLM代理的方法。这种方法使代理能够持续反思其行动，从而刺激动态优化探索路径和增强在不确定性下的强大决策能力的学习过程。我们的自省干预关注三个主要方面：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Anticipatory reflection before action execution (similar to a devil’s advocate);
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行动作前的预期反思（类似于魔鬼代言人）；
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Post-action evaluation and backtracking with remedy when necessary, to ensure
    the outcome aligns with subtask objectives;
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动后的评估和必要时的回溯修正，以确保结果与子任务目标一致；
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: An extensive review upon plan completion to generate finer plans for subsequent
    trials.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在计划完成后进行广泛审查，以生成用于后续试验的更精细计划。
- en: 'We implement this introspective methodology within WebArena [[32](#bib.bib32)],
    a comprehensive web environment featuring 812 tasks in five scenarios: online
    shopping, e-commerce management, social discussion forums, maps, and software
    development platforms. Experimental results demonstrate that our approach, which
    is zero-shot, significantly outperforms existing zero-shot methods while improving
    efficiency, paving the way for a new paradigm of intelligent systems that are
    more consistent, adaptable, and effective at solving problems¹¹1Code to reproduce
    our results will be released..'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在WebArena [[32](#bib.bib32)]中实施了这种自省方法，WebArena是一个全面的网络环境，包含五个场景中的812项任务：在线购物、电子商务管理、社交讨论论坛、地图和软件开发平台。实验结果表明，我们的方法（零样本）显著优于现有的零样本方法，同时提高了效率，为智能系统的新范式铺平了道路，这些系统在解决问题方面更为一致、适应性强且有效¹¹1重现我们结果的代码将被发布。
- en: 2 Related Works
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: In this paper, we develop and expand upon several key themes within the realm
    of natural language processing, with a specific focus on the integration of action
    generation, planning, and reflection in the construction of LLM agents.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们在自然语言处理领域发展并扩展了若干关键主题，特别关注于在LLM代理的构建中整合行动生成、规划和反思。
- en: 'Action Generation: LLMs have been employed in tasks requiring decision-making
    or action generation and have proven useful as agent-controlling policies in embodied
    environments [[9](#bib.bib9), [8](#bib.bib8), [3](#bib.bib3), [21](#bib.bib21),
    [33](#bib.bib33)]. They have also demonstrated effectiveness in text-based environments
    [[11](#bib.bib11), [17](#bib.bib17), [12](#bib.bib12)], where techniques like
    ReAct [[29](#bib.bib29)] have shown notable benefits. Despite its success, ReAct’s
    limitation lies in its inability to adjust to changes in the environment. Several
    improvements [[13](#bib.bib13), [16](#bib.bib16)] have been proposed to counter
    these limitations, advocating for self-reflection to enhance decision-making and
    reasoning. However, these techniques primarily aim to improve single plans or
    trajectories without considering alternative actions, which could modify the plan
    in a wrong direction.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 行动生成：大型语言模型（LLMs）已被用于需要决策或行动生成的任务，并在具身环境中作为代理控制策略证明了其有效性[[9](#bib.bib9), [8](#bib.bib8),
    [3](#bib.bib3), [21](#bib.bib21), [33](#bib.bib33)]。它们在基于文本的环境中也展示了有效性[[11](#bib.bib11),
    [17](#bib.bib17), [12](#bib.bib12)]，其中ReAct [[29](#bib.bib29)]等技术表现出了显著的好处。尽管取得了成功，ReAct的局限性在于它无法适应环境的变化。为应对这些局限性，已经提出了若干改进[[13](#bib.bib13),
    [16](#bib.bib16)]，提倡通过自我反思来增强决策和推理。然而，这些技术主要旨在改进单一计划或轨迹，而未考虑替代行动，这可能会使计划朝着错误的方向发展。
- en: 'Position Bias Mitigation: While comparing answer choices is generally effective,
    large language models used for action generation are not without flaws. They can
    exhibit bias, especially towards the first (or sometimes second) answer they see,
    regardless of its quality. This is known as position bias [[30](#bib.bib30), [22](#bib.bib22)].
    Our method mitigates this bias by asking follow-up questions that challenge its
    own answer.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 位置偏差缓解：虽然比较答案选项通常是有效的，但用于行动生成的大型语言模型并非没有缺陷。它们可能表现出偏差，尤其是对它们首先（或有时第二个）看到的答案，无论其质量如何。这被称为位置偏差[[30](#bib.bib30),
    [22](#bib.bib22)]。我们的方法通过提出挑战自己答案的后续问题来缓解这种偏差。
- en: 'Planning: Extensive research has explored the potential of LLMs in task planning
    [[4](#bib.bib4), [15](#bib.bib15), [20](#bib.bib20), [25](#bib.bib25), [5](#bib.bib5),
    [6](#bib.bib6)]. The concept of decoupling planning and execution in formulating
    LLM agents has been validated through numerous paradigms such as ReWOO [[26](#bib.bib26)],
    ADaPT [[15](#bib.bib15)], Structured Self-Reflection [[10](#bib.bib10)], and DEFS
    [[24](#bib.bib24)]. Nonetheless, these methods exhibit a deficiency in establishing
    a resilient mechanism for plan execution, with agents frequently revisiting and
    revising their plans following each instance of adverse environmental feedback,
    often due to inaccurately executed actions. Our approach, conversely, emphasizes
    executing a previously defined plan with unwavering effort before considering
    any modifications. This guarantees a more stable and consistent problem-solving
    process. To implement this, the factor of tree search becomes crucial for exploring
    the best solutions. Past approaches, including ToT [[28](#bib.bib28)], RAP [[7](#bib.bib7)],
    LATS [[31](#bib.bib31)], AdaPlanner [[20](#bib.bib20)], and ToolChain* [[34](#bib.bib34)],
    have incorporated tree search techniques in identifying the optimal route to the
    desired solution. However, our approach distinguishes itself by engaging the LLM
    in preparing alternate solutions in anticipation of impending failures, ensuring
    more comprehensive consideration in action generation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 规划：大量研究探讨了LLMs在任务规划中的潜力[[4](#bib.bib4), [15](#bib.bib15), [20](#bib.bib20),
    [25](#bib.bib25), [5](#bib.bib5), [6](#bib.bib6)]。在制定LLM代理时，规划和执行的解耦概念通过多种范式得到了验证，如ReWOO
    [[26](#bib.bib26)]、ADaPT [[15](#bib.bib15)]、Structured Self-Reflection [[10](#bib.bib10)]
    和DEFS [[24](#bib.bib24)]。然而，这些方法在建立一个强健的计划执行机制方面存在不足，代理在每次遭遇不利环境反馈后，常常需要重新审视和修订其计划，通常由于执行不准确的操作。我们的做法则强调在考虑任何修改之前，先坚定地执行预定义的计划。这保证了一个更稳定、一致的问题解决过程。为实现这一点，树搜索的因素变得至关重要，以探索最佳解决方案。过去的做法，包括ToT
    [[28](#bib.bib28)]、RAP [[7](#bib.bib7)]、LATS [[31](#bib.bib31)]、AdaPlanner [[20](#bib.bib20)]
    和ToolChain* [[34](#bib.bib34)]，在识别通向理想解决方案的最佳路径时都采用了树搜索技术。然而，我们的方法通过让LLM准备备用解决方案，以应对即将到来的失败，从而确保在行动生成中的更全面的考虑。
- en: 'Reflection and Self-refinement: Reflection and refinement techniques have advanced
    significantly through works such as Reflexion [[16](#bib.bib16)], AdaPlanner [[20](#bib.bib20)],
    and AutoEval [[14](#bib.bib14)]. Our methodology further enhances this by incorporating
    an anticipatory reflection mechanism that operates before each action rather than
    performing post-action reflection. This approach simplifies exploration by expediting
    remedial action and reducing extensive backtracking and serial plan revisions,
    thereby improving efficiency in the overall task handling process.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 反思与自我完善：通过Reflexion [[16](#bib.bib16)]、AdaPlanner [[20](#bib.bib20)]和AutoEval
    [[14](#bib.bib14)]等工作，反思和完善技术已显著进步。我们的方法通过在每次行动前实施预测性反思机制进一步增强了这一点，而不是在行动后进行反思。这种方法通过加速补救行动和减少广泛的回溯及串行计划修订，简化了探索过程，从而提高了整体任务处理效率。
- en: 3 Method
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Given a task $\mathcal{T}$ interacts, our objective is to enable the agent
    to systematically and adaptively complete the task through introspective methods.
    We first present how we decompose the task and generate action regarding each
    state in the environment in [section 3.1](#S3.SS1 "3.1 Task Decomposition and
    Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")
    and [section 3.2](#S3.SS2 "3.2 State and Action Representation ‣ 3 Method ‣ Devil’s
    Advocate: Anticipatory Reflection for LLM Agents"). Then we introduce the introspection
    mechanism in [section 3.3](#S3.SS3 "3.3 Introspective Mechanisms ‣ 3 Method ‣
    Devil’s Advocate: Anticipatory Reflection for LLM Agents").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一个任务$\mathcal{T}$互动，我们的目标是通过内省方法使代理能够系统性和适应性地完成任务。我们首先在[3.1节](#S3.SS1 "3.1
    Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents")和[3.2节](#S3.SS2 "3.2 State and Action Representation ‣ 3 Method
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")中介绍如何分解任务并生成与环境中每个状态相关的行动。然后我们在[3.3节](#S3.SS3
    "3.3 Introspective Mechanisms ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents")中介绍内省机制。'
- en: 3.1 Task Decomposition and Planning
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务分解和规划
- en: 'The first step involves decomposing the task $\mathcal{T}$:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步涉及将任务$\mathcal{T}$分解：
- en: '|  | $\displaystyle\mathcal{P}\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}).$
    |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{P}\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}).$
    |  | (1) |'
- en: 'Here, the plan $\mathcal{P}$ is parsed into a sequence of ordered subtasks:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，计划$\mathcal{P}$被解析为一个有序子任务的序列：
- en: '|  | $\displaystyle\mathcal{P}=(\tau_{1},\tau_{2},\ldots,\tau_{N}),$ |  | (2)
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{P}=(\tau_{1},\tau_{2},\ldots,\tau_{N}),$ |  | (2)
    |'
- en: 'where $\tau_{i}$ is the number of subtasks. For instance, Fig. [1](#S3.F1 "Figure
    1 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory
    Reflection for LLM Agents") shows a plan with 5 subtasks for solving a task in
    WebArena. The distribution of WebArena tasks based on the number of subtasks within
    each task is illustrated in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Task Decomposition
    and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents").
    This also reflects the difficulty of the tasks in WebArena, where most tasks take
    4-8 steps to complete.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\tau_{i}$是子任务的数量。例如，图[1](#S3.F1 "Figure 1 ‣ 3.1 Task Decomposition and Planning
    ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")显示了一个包含5个子任务的计划，用于解决WebArena中的一个任务。图[2](#S3.F2
    "Figure 2 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate:
    Anticipatory Reflection for LLM Agents")展示了基于每个任务中子任务数量的WebArena任务分布。这也反映了WebArena中任务的难度，大多数任务需要4-8个步骤才能完成。'
- en: 'Plan for task: What is the color configuration of the picture frame I bought
    in Nov 2022: 1. Click on the ‘My Account’ link to access your account details.
    2. Click on the ‘Order History’ link to view your past orders. 3. Scroll down
    the page until you find the order from November 2022. 4. Click on the order details
    link for the order from November 2022. 5. Scroll down to the product details section
    to find the color configuration of the picture frame.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 任务计划：我在2022年11月购买的相框的颜色配置是什么：1. 点击“我的账户”链接以访问您的账户详情。2. 点击“订单历史”链接查看您的过去订单。3.
    向下滚动页面直到找到2022年11月的订单。4. 点击2022年11月订单的订单详情链接。5. 向下滚动到产品详情部分以找到相框的颜色配置。
- en: 'Figure 1: An example plan with 5 subtasks, generated by GPT-4.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个由GPT-4生成的包含5个子任务的示例计划。
- en: '![Refer to caption](img/ca8c33cce37d828cc62d7a7b22afdd25.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ca8c33cce37d828cc62d7a7b22afdd25.png)'
- en: 'Figure 2: Distribution of WebArena tasks based on the number of subtasks within
    each task.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于每个任务中子任务数量的WebArena任务分布。
- en: 3.2 State and Action Representation
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 状态和动作表示
- en: 'Let $S_{t}\in\mathcal{S}$:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让$S_{t}\in\mathcal{S}$：
- en: '|  | $\displaystyle a_{t}\sim G_{\text{action}}(\tau_{i},S_{t},\mathcal{H}_{t-1}),$
    |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{t}\sim G_{\text{action}}(\tau_{i},S_{t},\mathcal{H}_{t-1}),$
    |  | (3) |'
- en: 'where $G_{\text{action}}$:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{\text{action}}$：
- en: '|  | $\displaystyle\mathcal{H}_{t}=\{\hat{a}_{1},\hat{a}_{2},\ldots,\hat{a}_{t}\},$
    |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{H}_{t}=\{\hat{a}_{1},\hat{a}_{2},\ldots,\hat{a}_{t}\},$
    |  | (4) |'
- en: 'where $\hat{a}_{t}$ accepts as input the state before the action, the action
    itself, the state after the action:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{a}_{t}$ 接受的输入包括行动前的状态、行动本身以及行动后的状态：
- en: '|  | $\displaystyle\hat{a}_{t}\sim G_{\text{describe}}(S_{t},a_{t},S_{t+1}).$
    |  | (5) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{a}_{t}\sim G_{\text{describe}}(S_{t},a_{t},S_{t+1}).$
    |  | (5) |'
- en: 'When the state observation is too long to fit in the context window of an LLM,
    the state is first summarized by the LLM into a shorter description before being
    fed to $G_{\text{describe}}$:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当状态观察过长而无法适应 LLM 的上下文窗口时，状态会首先由 LLM 摘要成更短的描述，然后再传递给 $G_{\text{describe}}$：
- en: '|  | $\displaystyle\mathcal{C}_{\tau_{i}}$ |  | (6) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{\tau_{i}}$ |  | (6) |'
- en: '|  | $\displaystyle\mathcal{C}_{\mathcal{T}}$ |  | (7) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{\mathcal{T}}$ |  | (7) |'
- en: where $G_{\text{completed}}$ is finished.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{\text{completed}}$ 已完成。
- en: Algorithm 1 Introspective Agent
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 自省代理
- en: 'Input: task $\mathcal{T};$'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：任务 $\mathcal{T};$
- en: time $t=0;$
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 时间 $t=0;$
- en: 1:while $\neg G_{\text{completed}}(\mathcal{T},\cdot)$
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '1: while $\neg G_{\text{completed}}(\mathcal{T},\cdot)$'
- en: 3.3 Introspective Mechanisms
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 自省机制
- en: The sequential action generation above can potentially execute the plan and
    solve the task already. Nevertheless, without proper introspection and adaptation,
    the agent might be stuck at a certain unsolvable subtask or go into a loop of
    failure when unexpected problems emerge. Thus, we introduce three introspective
    mechanisms to enhance our LLM agent’s problem-solving ability below.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述顺序行动生成可能已经执行了计划并解决了任务。然而，如果没有适当的自省和调整，代理可能会卡在某个不可解决的子任务上或在遇到意外问题时进入失败循环。因此，我们引入三种自省机制来增强我们的
    LLM 代理的解决问题能力。
- en: 3.3.1 Anticipatory Reflection (Devil’s Advocate)
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 预期反思（魔鬼的辩护者）
- en: 'The first layer of introspection occurs before each action execution. The agent
    anticipates potential failures and comes up with $R$:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首层自省发生在每次行动执行之前。代理预期潜在的失败并提出 $R$：
- en: '"If your answer above is not correct, instead, the next action should be:"'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '"如果你上面的回答不正确，接下来应该是："'
- en: 'We use $G_{\text{remedy}}$ at first attempt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先尝试使用 $G_{\text{remedy}}$：
- en: '|  | $\displaystyle a_{t}^{r}$ |  | (8) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{t}^{r}$ |  | (8) |'
- en: 'If later found necessary, the agent can go back to state $S_{t}$ is the last
    one to be pushed to the stack so it can be popped and executed first (see line
    18 in Alg. [1](#alg1 "Algorithm 1 ‣ 3.2 State and Action Representation ‣ 3 Method
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"))..'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果后续发现必要，代理可以返回到状态 $S_{t}$，这是最后一个被推送到栈中的状态，以便它可以首先弹出并执行（见算法 [1](#alg1 "算法 1
    ‣ 3.2 状态和行动表示 ‣ 3 方法 ‣ 魔鬼的辩护者：LLM 代理的预期反思") 行 18）。
- en: 3.3.2 Post-action Evaluation and Backtracking
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 后行动评估与回溯
- en: 'The second introspective mechanism kicks in after the execution of each action.
    Here, the agent evaluates whether the action and the resulting state align with
    the subtask objective. This introspective function, denoted as $G_{\text{align}}$:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层自省机制在每次行动执行后启动。此时，代理评估行动和结果状态是否与子任务目标一致。这个自省函数表示为 $G_{\text{align}}$：
- en: '|  | $\displaystyle\theta_{t}$ |  | (9) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{t}$ |  | (9) |'
- en: Here $\theta_{t}\in(0,1)$. When backtracking, we can easily navigate back to
    the URL. However, the element information on the URL might differ from the state
    we first encountered upon arriving at that page. To address this, we prompt the
    LLM to map the recorded element in the action to the new element with which we
    want to interact, if necessary.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\theta_{t}\in(0,1)$。在回溯时，我们可以轻松导航回 URL。然而，URL 上的元素信息可能与我们最初到达该页面时遇到的状态不同。为了解决这个问题，我们提示
    LLM 将行动中记录的元素映射到我们想要交互的新元素上（如有必要）。
- en: '![Refer to caption](img/7527811ae7f3eb980610c19ad51afbf6.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7527811ae7f3eb980610c19ad51afbf6.png)'
- en: 'Figure 3: Screen observation at one step in solving the subtask: Click on the
    order details link for the order from November 2022. The agent might decide to
    click ($a_{t}$.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：解决子任务的一个步骤中的屏幕观察：点击 2022 年 11 月订单的订单详情链接。代理可能决定点击 ($a_{t}$)。
- en: 3.3.3 Plan Revision
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 计划修订
- en: 'The third introspective mechanism occurs upon plan failure, i.e., when the
    stack is empty and $\mathcal{C}_{\mathcal{T}}=0$. Now the agent performs a thorough
    review of the actions executed and the notes taken, and refines its future plan
    based on identified problems:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种自省机制发生在计划失败时，即当堆栈为空且 $\mathcal{C}_{\mathcal{T}}=0$ 时。此时智能体对已执行的操作和记录进行彻底回顾，并根据发现的问题完善未来的计划。
- en: '|  | $\displaystyle\mathcal{P}_{\text{new}}$ |  | (10) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{P}_{\text{new}}$ |  | (10) |'
- en: Here, $\mathcal{P}_{\text{new}}$ is the new plan after reflecting on the past
    failed trials. The agent then re-enters the plan execution phase and starts a
    new episode.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathcal{P}_{\text{new}}$ 是在反思过去失败的尝试后形成的新计划。智能体随后重新进入计划执行阶段并开始新的回合。
- en: 'Through these three layers of introspection, our agent is more capable of navigating
    the complexities of unforeseen circumstances and addressing tasks, bringing us
    a significant stride closer to achieving truly autonomous, adaptable, and intelligent
    systems. By structuring the problem in this manner, we have established a clear
    framework for enabling LLM agents to perform tasks autonomously and adaptively
    through introspection. Alg. [1](#alg1 "Algorithm 1 ‣ 3.2 State and Action Representation
    ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents") shows
    a pseudo code demonstration of our approach.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这三层的自省，我们的智能体更能应对不可预见的复杂情况并处理任务，使我们在实现真正自主、适应性强且智能的系统上迈出了重要一步。通过这种方式构建问题，我们为LLM智能体通过自省自主适应地执行任务建立了一个清晰的框架。算法
    [1](#alg1 "Algorithm 1 ‣ 3.2 State and Action Representation ‣ 3 Method ‣ Devil’s
    Advocate: Anticipatory Reflection for LLM Agents") 展示了我们方法的伪代码示例。'
- en: '![Refer to caption](img/71f8a7b1151e2277b23f400a791de26b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/71f8a7b1151e2277b23f400a791de26b.png)'
- en: 'Figure 4: Decision making process of our agent in solving the task: What is
    the color configuration of the picture frame that I bought in Sep 2022? Before
    execution of the predicted action, the agent asks a follow-up question to itself
    regarding its decision: what if the picture frame is not in order #179? what should
    be the alternative remedy? And after finding out that order #179 contains no picture
    frame at all, the agent backtracks to the previous state to view order #175 and
    continue.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：我们的智能体在解决任务时的决策过程：我在2022年9月购买的相框的颜色配置是什么？在执行预测的操作之前，智能体向自己提出一个后续问题：如果相框不在订单#179中会怎么样？应该采取什么替代措施？在发现订单#179根本没有相框之后，智能体回到之前的状态查看订单#175并继续。
- en: 4 Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we demonstrate how introspection enhances consistency and
    adaptability of LLM agents in solving complex tasks in web environments. We first
    introduce the experimental setup for evaluation ([section 4.1](#S4.SS1 "4.1 Experimental
    Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")),
    followed by evaluation results ([section 4.2](#S4.SS2 "4.2 Results ‣ 4 Experiments
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")). Detailed error
    analysis is provided in [section 4.3](#S4.SS3 "4.3 Error Analysis ‣ 4 Experiments
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"), which highlights
    the directions for future endeavor.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了自省如何增强LLM智能体在解决网页环境中复杂任务时的一致性和适应性。我们首先介绍了评估的实验设置（[第4.1节](#S4.SS1
    "4.1 Experimental Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents")），接着是评估结果（[第4.2节](#S4.SS2 "4.2 Results ‣ 4 Experiments ‣ Devil’s
    Advocate: Anticipatory Reflection for LLM Agents")）。详细的错误分析见[第4.3节](#S4.SS3 "4.3
    Error Analysis ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for
    LLM Agents")，其中突出了未来努力的方向。'
- en: 4.1 Experimental Setup
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Live environments: We evaluate our proposed method in the simulated web environments
    of WebArena [[32](#bib.bib32)], a dataset of human-annotated web browsing tasks
    designed to evaluate the ability of LLMs to perform complex, real-world actions
    on the internet. The 812 tasks in WebArena involve five websites: an online shopping
    website, a software development website, a social forum platform, a map, and an
    e-commerce management platform; and these tasks can be categorized into three
    classes: information seeking tasks, site navigation and content & config tasks,
    and unachievable tasks. Though WebArena provides visual observation (screenshots),
    in this work we use the text observation only. The observation at each step is
    the accessibility tree of the webpage, and the elements in the accessibility tree
    are all within the current viewport of a 1280$\times$720 screen. The action space
    of our LLM agent includes actions that interact with environment: click, type,
    scroll, goto, go_back, go_forward, and also a note_down action that takes down
    useful snippet/summary for answering information-seeking questions.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实时环境：我们在 WebArena [[32](#bib.bib32)] 的模拟网络环境中评估我们提出的方法，WebArena 是一个旨在评估 LLM
    执行复杂的现实世界互联网操作能力的人工标注网页浏览任务数据集。WebArena 中的812个任务涉及五个网站：一个在线购物网站，一个软件开发网站，一个社交论坛平台，一个地图，以及一个电子商务管理平台；这些任务可以分为三类：信息查找任务、站点导航和内容与配置任务，以及无法完成的任务。尽管
    WebArena 提供了视觉观察（截图），但在这项工作中我们仅使用文本观察。每一步的观察是网页的可访问树，且可访问树中的元素都位于1280$\times$720屏幕的当前视口内。我们
    LLM 代理的动作空间包括与环境交互的动作：点击、输入、滚动、前往、返回、向前移动，以及一个记录动作，用于记录有用的片段/总结以回答信息查找问题。
- en: 'Baselines: We employ gpt-4-0613⁴⁴4[https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    [[1](#bib.bib1)] with a context window of 8k tokens to build the agents and compare
    our method with three other agent construction strategies: planning and sequential
    decision making (Plan + Act w/o reflexion), similar to ReWOO [[26](#bib.bib26)];
    planning and sequential decision making with reflection (Plan + Act), similar
    to AdaPlanner [[20](#bib.bib20)]; and tree search based planning, similar to LATS
    [[31](#bib.bib31)], but with reflection. In all methods, we set the upper limit
    on the number of actions to 30, i.e., after the agent executes 30 actions for
    a given task, it has to stop. In all three methods, we adopt the same prompts
    for action generation $G_{\text{action}}$ to ensure a fair comparison⁵⁵5Detailed
    prompts are shown in Appendix.. In our experiments, we set the LLM temperature
    to 1.0 and max_tokens to 512, and keep all other parameters as default.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基线：我们使用 `gpt-4-0613`⁴⁴4[https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    [[1](#bib.bib1)]，其上下文窗口为8k标记，用于构建代理并将我们的方法与三种其他代理构建策略进行比较：规划和顺序决策（Plan + Act w/o
    reflexion），类似于 ReWOO [[26](#bib.bib26)]; 带有反思的规划和顺序决策（Plan + Act），类似于 AdaPlanner
    [[20](#bib.bib20)]; 以及基于树搜索的规划，类似于 LATS [[31](#bib.bib31)]，但带有反思。在所有方法中，我们将动作的上限设置为30，即代理执行30个动作后必须停止。在所有三种方法中，我们采用相同的动作生成提示
    $G_{\text{action}}$ 以确保公平比较⁵⁵5详细提示见附录。实验中，我们将 LLM 温度设置为 1.0，max_tokens 设置为 512，其余参数保持默认。
- en: 'Metrics: We follow the evaluation metric Success Rate in [[32](#bib.bib32)],
    and count the number of actions per trial and the number of plan revisions per
    task. To determine whether a task is successfully completed, the exact_match metric
    is used for some site navigation and information seeking tasks. However, this
    can sometimes be overly stringent. For instance, consider the URLs below that
    display the same content (under ‘electronics’, the category id of ‘headphones’
    is 60): both of them are correct answers but only one exact match with the gold
    answer (the first one) is considered correct⁶⁶6URL string matching is used to
    determine if a task is finished.. To address this issue, we manually review the
    evaluation process and correct such cases in our results.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：我们遵循 [[32](#bib.bib32)] 中的评估指标成功率，并统计每次试验的动作数量和每个任务的计划修订数量。为确定任务是否成功完成，对于一些站点导航和信息查找任务使用精确匹配指标。然而，这有时可能过于严格。例如，考虑以下显示相同内容的
    URL（在“电子产品”下，“耳机”的类别 ID 为 60）：这两个 URL 都是正确答案，但只有一个与金标准答案（第一个）完全匹配被视为正确⁶⁶6 使用 URL
    字符串匹配来确定任务是否完成。为解决此问题，我们手动审查评估过程并纠正结果中的此类情况。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[http://localhost:7770/electronics/headphones.html](http://localhost:7770/electronics/headphones.html)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://localhost:7770/electronics/headphones.html](http://localhost:7770/electronics/headphones.html)'
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[http://localhost:7770/electronics.html?cat=60](http://localhost:7770/electronics.html?cat=60)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://localhost:7770/electronics.html?cat=60](http://localhost:7770/electronics.html?cat=60)'
- en: '![Refer to caption](img/d728ed85de1214acf62000ce97f4d50d.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d728ed85de1214acf62000ce97f4d50d.png)'
- en: 'Figure 5: Results of different agent construction strategies on WebArena. AR
    is short for our method, anticipatory reflection; LATS represents our in-house
    implementation of the approach proposed by Zhou et al. [[31](#bib.bib31)]; Plan
    + Act is a method of decomposition of task and execution of each subtask, similar
    to ReWOO [[26](#bib.bib26)]. All three methods are equipped with plan revision
    (post-failure reflection).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同代理构建策略在 WebArena 上的结果。AR 是我们的方法，预期反思；LATS 代表我们内部实现的 Zhou 等人提出的方法[[31](#bib.bib31)]；Plan
    + Act 是任务分解和执行每个子任务的方法，类似于 ReWOO [[26](#bib.bib26)]。这三种方法都配备了计划修订（失败后的反思）。
- en: 4.2 Results
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: 'The experimental results, depicted in Fig. [5](#S4.F5 "Figure 5 ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"),
    demonstrate the efficacy of our introspection-driven approach in enhancing the
    consistency and adaptability of LLM agents in web environments. We compare the
    success rates of various agent construction strategies across multiple episodes.
    Our method, anticipatory reflection (AR), consistently outperforms the others,
    achieving a success rate of 23.5% after seven episodes, closely followed by LATS
    with 22.7%. In contrast, the Plan + Act method shows gradual improvement, reaching
    19.8%, but remains significantly lower than the tree-search-based AR and LATS
    methods. Taking a closer look at the last few rounds of LATS reveals marginal
    improvements due to the homogeneous generated actions through direct sampling.
    In comparison, AR benefits from the “devil’s advocate” approach, enabling more
    thorough planning and execution due to introspective follow-up questions. This
    trend underscores the importance of incorporating introspection mechanisms for
    both plan execution and revision, highlighting their critical role in achieving
    more consistent and efficient results.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果如图[5](#S4.F5 "图 5 ‣ 4.1 实验设置 ‣ 4 实验 ‣ 魔鬼代言人：预期反思用于 LLM 代理")所示，展示了我们以自省为驱动的方法在提高
    LLM 代理在网页环境中的一致性和适应性方面的有效性。我们比较了多次实验中不同代理构建策略的成功率。我们的方法，预期反思（AR），始终优于其他方法，在七次实验后成功率达到
    23.5%，紧随其后的是 LATS，成功率为 22.7%。相比之下，Plan + Act 方法逐渐改进，成功率达到 19.8%，但仍显著低于基于树搜索的 AR
    和 LATS 方法。仔细观察 LATS 的最后几轮显示，由于通过直接采样生成的动作同质，改进幅度很小。相比之下，AR 受益于“魔鬼代言人”方法，能够通过自省式的后续问题进行更彻底的规划和执行。这一趋势突显了在计划执行和修订中融入自省机制的重要性，强调了它们在实现更一致和高效结果中的关键作用。
- en: '|  | # of Actions | # of Plan Revisions |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动次数 | 计划修订次数 |'
- en: '| --- | --- | --- |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | First Trial | Last Trial |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 初次试验 | 最后试验 |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Plan+Act | 4.01 | 4.47 | 2.03 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Plan+Act | 4.01 | 4.47 | 2.03 |'
- en: '| LATS | 6.08 | 6.45 | 1.16 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LATS | 6.08 | 6.45 | 1.16 |'
- en: '| AR | 6.39 | 7.07 | 0.64 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AR | 6.39 | 7.07 | 0.64 |'
- en: 'Table 1: Statistics of the trajectory of different agents solving tasks on
    WebArena. We report the number of actions in the first and last trial, and also
    the number of plan revisions, i.e., trials.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同代理在 WebArena 上解决任务的轨迹统计。我们报告了第一次和最后一次试验中的行动次数，以及计划修订的次数，即试验次数。
- en: 'Further insights can be gleaned from [Table 1](#S4.T1 "In 4.2 Results ‣ 4 Experiments
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"), which compares the
    average number of actions in the first and last trials across different methods.
    Our AR method shows an increase in the average number of actions from 6.39 in
    the first trial to 7.07 in the last trial, indicating a robust learning and adaptation
    process. In comparison, the average number of actions in the first trial of the
    Plan+Act method is only 4.01, suggesting that it stops at an early stage without
    completing full plan execution. Thus, our method effectively leverages a greater
    number of actions to achieve better outcomes, thereby reducing the number of plan
    revisions by 45% and improving overall efficiency.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表 1](#S4.T1 "在 4.2 结果 ‣ 4 实验 ‣ 魔鬼代言人：LLM 代理的预期反思")中可以获得进一步的见解，该表比较了不同方法在第一次和最后一次试验中的平均行动次数。我们的
    AR 方法显示，从第一次试验的 6.39 增加到最后一次试验的 7.07，表明了一个强大的学习和适应过程。相比之下，Plan+Act 方法第一次试验的平均行动次数仅为
    4.01，表明它停留在早期阶段而未完成完整的计划执行。因此，我们的方法有效地利用了更多的行动来实现更好的结果，从而将计划修订的数量减少了 45%，并提高了整体效率。
- en: 4.3 Error Analysis
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 错误分析
- en: 'The subsequent sections shed light on an analysis of errors we observed from
    the agent’s behavior when executing tasks. Two key areas have been identified
    for detailed discussion: an agent’s occasional inability to fully learn from past
    failures, and inefficiencies in solving specific kinds of tasks due to a sequential
    planning scheme.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的部分阐明了我们在执行任务时从代理行为中观察到的错误分析。已确定两个关键领域进行详细讨论：代理偶尔无法从过去的失败中完全学习，以及由于顺序规划方案导致在解决特定任务时的低效率。
- en: 4.3.1 Agent only takes partial lesson from past failures
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 代理只从过去的失败中吸取部分教训
- en: 'One category of errors we notice is that the agent is not taking full lesson
    from past failure in generating a new plan. As illustrated in Fig. [6(a)](#S4.F6.sf1
    "Figure 6(a) ‣ Figure 6 ‣ 4.3.1 Agent only takes partial lesson from past failures
    ‣ 4.3 Error Analysis ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents"), the agent is at the final step of drafting a refund message
    for a Bluetooth speaker, after a series of steps taken to seek information for
    the order. From the screen, we know that the agent should consolidate all the
    gathered information and type one piece of text into the (only) box titled “What’s
    on your mind?”. However, as depicted in Fig. [6(b)](#S4.F6.sf2 "Figure 6(b) ‣
    Figure 6 ‣ 4.3.1 Agent only takes partial lesson from past failures ‣ 4.3 Error
    Analysis ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"),
    while some improvements were made by adding the date of purchase and a more detailed
    explanation in the revised plan, the agent still failed to optimize the input
    process, repeating the typing actions separately for fields that do not exist.
    This inefficiency in the agent’s behavior showcases the need for either an LLM
    with stronger reasoning ability or a better mechanism to solicit more comprehensive
    and accurate reflection.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到的一个错误类别是代理在生成新计划时没有从过去的失败中吸取完整的教训。如图[6(a)](#S4.F6.sf1 "图 6(a) ‣ 图 6 ‣ 4.3.1
    代理只从过去的失败中吸取部分教训 ‣ 4.3 错误分析 ‣ 4 实验 ‣ 魔鬼代言人：LLM 代理的预期反思")所示，代理在完成一系列查询订单的步骤后，正处于草拟蓝牙扬声器退款信息的最后一步。从屏幕上，我们知道代理应该整合所有收集到的信息，并在标题为“你在想什么？”的（唯一）框中输入一段文字。然而，如图[6(b)](#S4.F6.sf2
    "图 6(b) ‣ 图 6 ‣ 4.3.1 代理只从过去的失败中吸取部分教训 ‣ 4.3 错误分析 ‣ 4 实验 ‣ 魔鬼代言人：LLM 代理的预期反思")所示，尽管在修订计划中添加了购买日期和更详细的解释，代理仍未能优化输入过程，为不存在的字段单独重复输入。这种代理行为中的低效率展示了需要一个具有更强推理能力的LLM或一个更好的机制，以便进行更全面和准确的反思。
- en: '![Refer to caption](img/bc9f7b43b60762d498bfc6f1662c52b8.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bc9f7b43b60762d498bfc6f1662c52b8.png)'
- en: '(a) Screen observation at the last step to solve the task: Draft a refund message
    via their "contact us" form for the bluetooth speaker I bought Feb 2023\. It broke
    after three days of use. The shop requires the order id, the reason and the amount
    to refund in the message. Don’t submit yet.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在最后一步解决任务的屏幕观察：通过他们的“联系我们”表单草拟我在 2023 年 2 月购买的蓝牙扬声器的退款信息。使用三天后坏了。商店要求在信息中提供订单号、原因和退款金额。不要提交。
- en: 'Initial Plan: • … • Type the order id into the appropriate field in the contact
    form. • Type "The bluetooth speaker I bought broke after three days of use" into
    the ‘Reason’ field of the contact form. • Type the amount to refund into the appropriate
    field in the contact form. Revised Plan: • … • Type the order id ‘000000161’ into
    the appropriate field in the contact form. • Type "The bluetooth speaker I bought
    in Feb 2023 broke after three days of use. I would like to request a refund."
    into the ‘Reason’ field of the contact form. • Type the amount to refund ‘$56.35’
    into the appropriate field in the contact form.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 初始计划：• … • 将订单ID输入到联系表单中的适当字段。 • 在联系表单的“原因”字段中输入“我购买的蓝牙音响在使用三天后坏了”。 • 将退款金额输入到联系表单中的适当字段。
    修订计划：• … • 将订单ID ‘000000161’ 输入到联系表单中的适当字段。 • 在联系表单的“原因”字段中输入“我在2023年2月购买的蓝牙音响在使用三天后坏了。我想申请退款。”
    • 将退款金额‘$56.35’ 输入到联系表单中的适当字段。
- en: (b) Comparison between the last few steps of the initial plan and the revised
    plan for the same task.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 对同一任务的初始计划和修订计划的最后几步进行比较。
- en: 'Figure 6: Illustration of the first type of errors: the agent is not taking
    full lesson from past experience in plan revision. The agent does not capture
    the root cause of its failure, i.e., separately typing each piece of information
    into different fields that do not even exist.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：第一种错误的说明：代理在计划修订中没有充分吸取过去经验。代理没有捕捉到其失败的根本原因，即将每一条信息分别输入到根本不存在的不同字段中。
- en: 4.3.2 Sequential planning cannot solve all tasks
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 顺序规划无法解决所有任务
- en: 'In our analysis, we observed a recurrent error pertaining to the design of
    the agent’s planning process. Currently, the proposed methodology structures a
    plan as a sequence of tasks that are executed in a specific order. This approach,
    effective in a decent amount of use cases, seems to falter when faced with tasks
    necessitating more sophisticated logic. Specifically, tasks that mandate implementing
    a function encapsulating several actions, employing a loop construct, or those
    executed repetitively, tend to challenge the model’s current configuration. For
    example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们观察到一个与代理规划过程设计相关的重复错误。目前，提出的方法将计划结构化为按特定顺序执行的一系列任务。这种方法在相当多的用例中有效，但在面对需要更复杂逻辑的任务时似乎会失败。具体而言，需要实现封装多个操作的函数、使用循环结构，或重复执行的任务，往往会挑战模型的当前配置。例如：
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: List out reviewers, if exist, who mention about average print quality
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列出提到平均打印质量的审稿人（如果存在的话）
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Give me the SKU of the products that have 1-3 units left.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给我剩余1-3件的产品SKU。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Like all submissions created by CameronKelsey in subreddit earthporn.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像 CameronKelsey 在 subreddit earthporn 创建的所有提交一样。
- en: The ability to process these tasks effectively would necessitate the incorporation
    of additional cognitive constructs into the planning model—e.g., loops, repetitive
    actions, or encapsulation of a group of actions into callable functions. Though
    taking notes can help the agent eliminate wrong choices, these systemic extensions
    would add crucial capabilities to the web agent, significantly enhancing its navigation
    and problem-solving competence in realistic web environments. Moreover, while
    the current agent can succeed in the limited search space of simple tasks, it
    often struggles to review and introspect upon more extensive descriptive tasks
    requiring dynamic problem-solving. By addressing these limitations in future work,
    i.e., effectively converting textual description of a plan into robust execution
    of callable functions and loops, we believe that the reasoning capability of our
    agent can be substantially improved, leading to better outcomes in understanding
    and solving tasks that involve dynamic cognition in web environments.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些任务的能力将需要将额外的认知结构纳入规划模型，例如，循环、重复操作或将一组操作封装成可调用的函数。虽然记笔记可以帮助代理消除错误选择，但这些系统扩展将为网络代理增加关键功能，显著提升其在实际网络环境中的导航和解决问题的能力。此外，虽然当前代理能够在简单任务的有限搜索空间中取得成功，但在面对需要动态解决问题的更广泛描述性任务时常常感到困难。通过在未来的工作中解决这些局限性，即有效地将计划的文本描述转换为强健的可调用函数和循环的执行，我们相信我们的代理的推理能力可以得到显著提升，从而在理解和解决涉及动态认知的网络环境中的任务时取得更好的结果。
- en: 5 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduce a novel introspective methodology that significantly
    enhances the problem-solving capabilities of LLMs in complex environments, as
    demonstrated through comprehensive evaluations in the WebArena setting. Our approach
    strategically decomposes tasks into actionable subtasks and incorporates a three-tiered
    introspection process, which includes anticipatory reflection, robust post-action
    evaluation, and episode-level plan revision. This setup not only allows LLM agents
    to adapt their strategies in real time but also fosters long-term learning, reducing
    the need for frequent interventions as experience accumulates. The successful
    application of our introspective methodology in the WebArena suggests its potential
    transferability to other domains that demand dynamic decision-making such as autonomous
    driving, healthcare, and interactive customer services. By enabling LLM agents
    to proactively contemplate potential failures, evaluate actions post-execution,
    and continuously refine their strategy based on experiential insights, our approach
    equips AI systems with a human-like strategic thinking capability.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了一种新颖的自我反省方法，该方法显著增强了大型语言模型在复杂环境中的问题解决能力，通过在 WebArena 环境中的全面评估得到了验证。我们的方法将任务战略性地分解为可操作的子任务，并采用了三层自我反省过程，包括前瞻性反思、稳健的行动后评估以及阶段级计划修订。这一设置不仅允许大型语言模型代理实时调整其策略，还促进了长期学习，随着经验的积累减少了对频繁干预的需求。我们的方法在
    WebArena 中的成功应用表明其在需要动态决策的其他领域（如自动驾驶、医疗保健和互动客户服务）具有潜在的迁移性。通过使大型语言模型代理主动思考潜在的失败、在执行后评估行动并根据经验见解持续改进其策略，我们的方法赋予了人工智能系统类似于人类的战略思维能力。
- en: Broader Impact
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: Looking forward, the integration of multi-modal data inputs could further enhance
    the contextual understanding and decision-making accuracy of these agents. The
    principles and findings from our approach provide a robust foundation for future
    research in AI, particularly in aspects of autonomous decision-making, learning
    efficiency, and adaptability. As AI continues to integrate into diverse aspects
    of decision-making, embedding introspective capabilities will be essential to
    ensure these systems operate not only with precision but with an understanding
    akin to strategic human cognition.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，多模态数据输入的整合可能进一步提升这些代理的情境理解和决策准确性。我们的方法的原则和发现为未来的人工智能研究提供了坚实的基础，特别是在自主决策、学习效率和适应性方面。随着人工智能继续融入决策的各个方面，嵌入自我反省能力将对确保这些系统不仅精确运作，而且具备类似于战略人类认知的理解至关重要。
- en: Ethics Statement
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: As the capabilities of LLM agents enhance and their deployment in real-world
    applications increases, it is crucial to address potential ethical concerns, particularly
    regarding data privacy, bias, and transparency. Our work focuses on improving
    agent introspection to enhance task performance and decision-making explanations,
    aiming to develop more transparent and trustworthy AI systems. We emphasize the
    importance of human oversight to monitor and mitigate unforeseen consequences
    and encourage the responsible use of this technology for societal benefit. By
    promoting continuous evaluation and fair practices, we seek to minimize biases
    and ensure that the deployment of these agents does not exacerbate social inequalities.
    Furthermore, we are committed to optimizing computational resources to reduce
    the environmental impact, advocating for sustainable AI practices.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）代理的能力提升以及其在现实世界应用中的增加，解决潜在的伦理问题变得至关重要，特别是关于数据隐私、偏见和透明度。我们的工作重点在于改善代理的自我反省能力，以提升任务表现和决策解释，旨在开发更透明和值得信赖的人工智能系统。我们强调人类监督的重要性，以监控和减轻不可预见的后果，并鼓励负责任地使用这项技术以造福社会。通过推动持续评估和公平实践，我们力求最小化偏见，并确保这些代理的部署不会加剧社会不平等。此外，我们致力于优化计算资源以减少环境影响，倡导可持续的人工智能实践。
- en: Limitations
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Despite substantial progress made with our current design, limitations persist
    that inhibit optimal performance. Notably, the agent lacks a full learning mechanism
    to capitalize on past failures when generating a new plan, resulting in inefficient
    execution and recurring mistakes. Furthermore, while the sequential planning approach
    is effective for simpler tasks, it falls short for more sophisticated operations,
    such as those requiring encapsulated actions or loop constructs. Additionally,
    the agent struggles with tasks that expand beyond a simple search space, suggesting
    obstacles in handling dynamic problem-solving. Last but not least, our agent needs
    significant amounts of LLM generation (i.e., API calling), consequently requiring
    substantial time and computational resources, which dents its efficiency. Therefore,
    future work needs to concentrate on improving the agent’s ability to fully learn
    from prior shortcomings, adapt to handle complex tasks, enhance dynamic problem-solving
    capabilities, and optimize time and resource utilization with more efficient LLM
    calling.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们当前设计已经取得了相当大的进展，但仍存在一些限制，这些限制阻碍了最佳性能的发挥。特别是，代理缺乏完整的学习机制，无法在生成新计划时利用过去的失败，导致执行效率低下和重复错误。此外，虽然顺序规划方法对于简单任务有效，但对于更复杂的操作，如需要封装操作或循环结构的任务，则显得不足。此外，代理在处理超出简单搜索空间的任务时遇到困难，这表明其在处理动态问题解决方面存在障碍。最后但同样重要的是，我们的代理需要大量的
    LLM 生成（即 API 调用），因此需要大量的时间和计算资源，这影响了其效率。因此，未来的工作需要集中于提高代理从先前缺陷中全面学习的能力，适应处理复杂任务，增强动态问题解决能力，并通过更高效的
    LLM 调用优化时间和资源利用。
- en: References
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人 2023. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*。
- en: 'Deng et al. [2023] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2Web: Towards a Generalist Agent for
    the Web](http://arxiv.org/abs/2306.06070).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人 [2023] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, 和 Yu Su. 2023. [Mind2Web: Towards a Generalist Agent for
    the Web](http://arxiv.org/abs/2306.06070)。'
- en: 'Driess et al. [2023] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,
    Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
    Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal
    Language Model. In *arXiv preprint arXiv:2303.03378*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess 等人 [2023] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,
    Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
    Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor
    Mordatch, 和 Pete Florence. 2023. PaLM-E: An Embodied Multimodal Language Model。
    在 *arXiv 预印本 arXiv:2303.03378*。'
- en: 'Dror et al. [2023] Rotem Dror, Haoyu Wang, and Dan Roth. 2023. [Zero-Shot On-the-Fly
    Event Schema Induction](https://doi.org/10.18653/v1/2023.findings-eacl.53). In
    *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    705–725, Dubrovnik, Croatia. Association for Computational Linguistics.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dror 等人 [2023] Rotem Dror, Haoyu Wang, 和 Dan Roth. 2023. [Zero-Shot On-the-Fly
    Event Schema Induction](https://doi.org/10.18653/v1/2023.findings-eacl.53)。在 *计算语言学协会会议：EACL
    2023*，第 705–725 页，杜布罗夫尼克，克罗地亚。计算语言学协会。
- en: Guan et al. [2023] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging Pre-trained Large Language Models to Construct
    and Utilize World Models for Model-based Task Planning](http://arxiv.org/abs/2305.14909).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan 等人 [2023] Lin Guan, Karthik Valmeekam, Sarath Sreedharan, 和 Subbarao Kambhampati.
    2023. [Leveraging Pre-trained Large Language Models to Construct and Utilize World
    Models for Model-based Task Planning](http://arxiv.org/abs/2305.14909)。
- en: Gur et al. [2024] Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. [A Real-World WebAgent
    with Planning, Long Context Understanding, and Program Synthesis](https://openreview.net/forum?id=9JQtrumvg8).
    In *The Twelfth International Conference on Learning Representations*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur 等人 [2024] Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, 和 Aleksandra Faust. 2024. [A Real-World WebAgent with
    Planning, Long Context Understanding, and Program Synthesis](https://openreview.net/forum?id=9JQtrumvg8)。在
    *第十二届国际学习表征会议*。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with Language Model is Planning with World
    Model](https://doi.org/10.18653/v1/2023.emnlp-main.507). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郝等人 [2023] 石博郝、伊古、郝迪马、约书亚洪、郑王、黛西王和志廷胡。2023。 [语言模型的推理就是用世界模型进行规划](https://doi.org/10.18653/v1/2023.emnlp-main.507)。收录于
    *2023年自然语言处理领域实证方法会议论文集*，第8154–8173页，新加坡。计算语言学协会。
- en: 'Huang et al. [2022a] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
    Mordatch. 2022a. Language Models as Zero-Shot Planners: Extracting Actionable
    Knowledge for Embodied Agents. *arXiv preprint arXiv:2201.07207*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2022a] 温龙黄、彼得·阿贝尔、深ak·帕塔克和伊戈尔·莫达奇。2022a。语言模型作为零样本规划者：为具身代理提取可操作的知识。*arXiv
    预印本 arXiv:2201.07207*。
- en: 'Huang et al. [2022b] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre
    Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,
    and Brian Ichter. 2022b. Inner Monologue: Embodied Reasoning through Planning
    with Language Models. In *arXiv preprint arXiv:2207.05608*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2022b] 温龙黄、费晓、泰德·肖、哈里斯·陈、杰基·梁、皮特·弗洛伦斯、安迪·曾、乔纳森·汤普森、伊戈尔·莫达奇、耶夫根·切博塔尔、皮埃尔·塞尔曼、诺亚·布朗、托马斯·杰克逊、琳达·卢、谢尔盖·莱文、卡罗尔·豪斯曼和布赖恩·伊赫特。2022b。内在独白：通过规划与语言模型进行具身推理。收录于
    *arXiv 预印本 arXiv:2207.05608*。
- en: 'Li et al. [2023] Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. 2023.
    [A Zero-Shot Language Agent for Computer Control with Structured Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.753).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    11261–11274, Singapore. Association for Computational Linguistics.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023] 陶李、刚李、志伟邓、布莱恩·王和杨李。2023。 [一种用于计算机控制的零样本语言代理，具有结构化反思](https://doi.org/10.18653/v1/2023.findings-emnlp.753)。收录于
    *计算语言学协会会议成果：EMNLP 2023*，第11261–11274页，新加坡。计算语言学协会。
- en: Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided
    exploration. *arXiv preprint arXiv:1802.08802*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2018] 艾凡·哲然刘、凯尔文·顾、潘普翁·帕苏帕特、田林石和珀西·梁。2018。基于工作流引导探索的网页界面强化学习。*arXiv 预印本
    arXiv:1802.08802*。
- en: 'Liu et al. [2023] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs
    as Agents. *arXiv preprint arXiv: 2308.03688*.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等人 [2023] 肖刘、郝宇、韩晨张、伊凡徐、玄宇雷、汉宇赖、余古、航梁丁、凯文门、克娟杨、曙丹张、向邓、奥汉曾、郑晓杜、陈辉张、盛沈、田俊张、余苏、欢孙、敏丽黄、余小董和杰唐。2023。AgentBench:
    评估大语言模型作为代理的能力。*arXiv 预印本 arXiv: 2308.03688*。'
- en: 'Madaan et al. [2023] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. [Self-Refine: Iterative Refinement with Self-Feedback](http://arxiv.org/abs/2303.17651).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马丹等人 [2023] 阿曼·马丹、尼克特·坦顿、普拉卡尔·古普塔、斯凯勒·哈利南、吕宇高、萨拉·维格雷夫、乌里·阿隆、诺哈·兹里、施瑞迈·普拉布莫耶、易明·杨、肖恩·韦雷克、博迪萨特瓦·普拉萨德·马吉姆德、沙尚克·古普塔、阿米尔·雅兹丹巴赫和彼得·克拉克。2023。
    [自我改进：通过自我反馈进行迭代改进](http://arxiv.org/abs/2303.17651)。
- en: Pan et al. [2024] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, and Alane Suhr. 2024. [Autonomous Evaluation and Refinement of Digital
    Agents](http://arxiv.org/abs/2404.06474).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潘等人 [2024] 贾毅潘、易驰张、尼古拉斯·汤姆林、伊菲·周、谢尔盖·莱文和阿莱恩·苏尔。2024。 [数字代理的自主评估与改进](http://arxiv.org/abs/2404.06474)。
- en: 'Prasad et al. [2023] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter
    Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2023. ADaPT: As-Needed
    Decomposition and Planning with Language Models. *arXiv*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '普拉萨德等人 [2023] 阿尔奇基·普拉萨德、亚历山大·科勒、玛雷克·哈特曼、彼得·克拉克、阿希什·萨巴尔瓦尔、莫希特·班萨尔和图沙尔·霍特。2023。ADaPT:
    按需分解和与语言模型规划。*arXiv*。'
- en: 'Shinn et al. [2023] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language Agents with Verbal
    Reinforcement Learning](http://arxiv.org/abs/2303.11366).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '辛等人 [2023] 诺亚·辛、费德里科·卡萨诺、爱德华·伯曼、阿什文·戈比纳斯、卡尔蒂克·纳拉辛汉和顺宇·姚。2023。 [Reflexion: 带有语言强化学习的语言代理](http://arxiv.org/abs/2303.11366)。'
- en: 'Shridhar et al. [2021] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2021. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768).
    In *Proceedings of the International Conference on Learning Representations (ICLR)*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar等人 [2021] 莫希特·施里达、邢笛袁、马克-亚历山大·科特、约纳坦·比斯克、亚当·特里施勒和马修·豪斯克内赫特。2021。 [ALFWorld:
    对齐文本和具身环境以进行互动学习](https://arxiv.org/abs/2010.03768)。发表于*国际学习表征会议（ICLR）会议论文集*。'
- en: 'Song et al. [2023] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler,
    Wei-Lun Chao, and Yu Su. 2023. LLM-Planner: Few-Shot Grounded Planning for Embodied
    Agents with Large Language Models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '宋等人 [2023] 陈熙宋、佳满吴、克莱顿·华盛顿、布莱恩·M·萨德勒、魏伦·赵和余苏。2023。LLM-Planner: 少量样本基础的具身代理规划，利用大型语言模型。发表于*IEEE/CVF国际计算机视觉会议（ICCV）会议论文集*。'
- en: 'Song et al. [2024] Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. 2024. Trial and Error: Exploration-Based Trajectory Optimization
    for LLM Agents. *arXiv preprint arXiv:2403.02502*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宋等人 [2024] 一帆宋、大尹、向跃、杰黄、苏剑李和比尔·宇辰林。2024。试错法：基于探索的轨迹优化用于LLM代理。*arXiv 预印本 arXiv:2403.02502*。
- en: 'Sun et al. [2023] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. 2023. [AdaPlanner: Adaptive Planning from Feedback with Language Models](http://arxiv.org/abs/2305.16653).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '孙等人 [2023] 昊天孙、宇辰庄、凌凯孔、博戴和超张。2023。 [AdaPlanner: 从反馈中自适应规划与语言模型](http://arxiv.org/abs/2305.16653)。'
- en: 'Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An Open-Ended
    Embodied Agent with Large Language Models. *arXiv preprint arXiv: Arxiv-2305.16291*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2023a] 关志王、余琦谢、云凡姜、阿杰·曼德尔卡、肖超伟、朱雨科、范林熙和安尼玛·安南德库马尔。2023a。Voyager: 一种具有大型语言模型的开放式具身代理。*arXiv
    预印本 arXiv: Arxiv-2305.16291*。'
- en: Wang et al. [2023b] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai
    Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. [Large Language Models
    are not Fair Evaluators](http://arxiv.org/abs/2305.17926).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023b] 佩伊·王、雷李、梁陈、泽凡·蔡、大伟朱、丙怀林、云波曹、祁刘、天宇刘和志芳隋。2023b。 [大型语言模型不是公平的评估者](http://arxiv.org/abs/2305.17926)。
- en: 'Wang et al. [2022] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. 2022. [ScienceWorld: Is your Agent Smarter than a 5th Grader?](https://doi.org/10.18653/v1/2022.emnlp-main.775)
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 11279–11298, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人 [2022] 若尧王、彼得·詹森、马克-亚历山大·科特和普里特维拉吉·阿曼纳布罗卢。2022。 [ScienceWorld: 你的代理比一个五年级学生聪明吗？](https://doi.org/10.18653/v1/2022.emnlp-main.775)
    发表在*2022年自然语言处理实证方法会议论文集*，第11279-11298页，阿布扎比，阿联酋。计算语言学协会。'
- en: 'Wang et al. [2023c] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. 2023c. [Describe, Explain, Plan and Select: Interactive Planning
    with LLMs Enables Open-World Multi-Task Agents](https://openreview.net/forum?id=KtvPdGb31Z).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人 [2023c] 资浩王、邵飞蔡、关洲陈、安琪刘、小剑马和逸涛梁。2023c。 [描述、解释、计划和选择：与LLMs互动的开放世界多任务代理](https://openreview.net/forum?id=KtvPdGb31Z)。发表于*第37届神经信息处理系统会议*。
- en: Wu et al. [2023] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan.
    2023. Embodied Task Planning with Large Language Models. *arXiv preprint arXiv:2305.03716*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人 [2023] 甄瑜吴、紫薇王、秀伟徐、季文陆和海滨颜。2023。具身任务规划与大型语言模型。*arXiv 预印本 arXiv:2305.03716*。
- en: 'Xu et al. [2023] Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee,
    Yuchen Liu, and Dongkuan Xu. 2023. [ReWOO: Decoupling Reasoning from Observations
    for Efficient Augmented Language Models](http://arxiv.org/abs/2305.18323).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '许等人 [2023] 宾锋许、智远彭、博文雷、苏巴布拉塔·穆克吉、宇辰刘和东宽许。2023。 [ReWOO: 从观察中解耦推理以提高增强型语言模型的效率](http://arxiv.org/abs/2305.18323)。'
- en: 'Yao et al. [preprint] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    preprint. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language
    Agents. In *ArXiv*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等人 [preprint] 申宇姚、霍华德·陈、约翰·杨和卡尔蒂克·纳拉辛汉。预印本。WebShop: 朝着可扩展的现实世界网络交互与基础语言代理的方向发展。发表于*ArXiv*。'
- en: 'Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a. [Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models](https://openreview.net/forum?id=5Xc1ecxO1h).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023a] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas
    L. Griffiths, Yuan Cao, 和 Karthik R Narasimhan. 2023a. [思维树：利用大型语言模型进行深思熟虑的问题解决](https://openreview.net/forum?id=5Xc1ecxO1h)。在*第三十七届神经信息处理系统会议*。
- en: 'Yao et al. [2023b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. [2023b] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, 和 Yuan Cao. 2023b. ReAct: 在语言模型中协同推理和行动。在*国际学习表征会议（ICLR）*。'
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging LLM-as-a-Judge with MT-Bench
    and Chatbot Arena](https://openreview.net/forum?id=uccHPGDlao). In *Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E. Gonzalez, 和 Ion Stoica. 2023. [用MT-Bench和Chatbot Arena评估LLM作为评判者](https://openreview.net/forum?id=uccHPGDlao)。在*第三十七届神经信息处理系统会议数据集和基准测试跟踪*。
- en: Zhou et al. [2024a] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. 2024a. [Language Agent Tree Search Unifies Reasoning Acting
    and Planning in Language Models](https://openreview.net/forum?id=6LNTSrJjBe).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2024a] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    和 Yu-Xiong Wang. 2024a. [语言代理树搜索统一了语言模型中的推理、行动和规划](https://openreview.net/forum?id=6LNTSrJjBe)。
- en: 'Zhou et al. [2024b] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon,
    and Graham Neubig. 2024b. [WebArena: A Realistic Web Environment for Building
    Autonomous Agents](https://openreview.net/forum?id=oKn9c6ytLx). In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. [2024b] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon,
    和 Graham Neubig. 2024b. [WebArena: 建立自主代理的现实网络环境](https://openreview.net/forum?id=oKn9c6ytLx)。在*第十二届国际学习表征会议*。'
- en: 'Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. 2023. Ghost in the Minecraft: Generally Capable Agents for Open-World
    Environments via Large Language Models with Text-based Knowledge and Memory. *arXiv
    preprint arXiv:2305.17144*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    和 Jifeng Dai. 2023. Minecraft中的鬼影：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供一般能力的代理。*arXiv预印本
    arXiv:2305.17144*。
- en: 'Zhuang et al. [2024] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. 2024. [ToolChain*: Efficient
    Action Space Navigation in Large Language Models with A* Search](https://openreview.net/forum?id=B6pQxqUcT8).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhuang et al. [2024] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, 和 Chao Zhang. 2024. [ToolChain*: 使用A*搜索在大型语言模型中高效导航行动空间](https://openreview.net/forum?id=B6pQxqUcT8)。在*第十二届国际学习表征会议*。'
- en: Appendix
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Prompt for Plan Generation ($G_{\text{plan}}$)
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计划生成提示 ($G_{\text{plan}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
    You can click an element with the mouse, scroll up or down, go to a certain URL
    or go back to previous page, or type some text with the keyboard (e.g., click(),
    scroll(), goto(), go_back(), and type() functions in playwright). One step means
    one operation within any of the mentioned actions.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在模仿人类在网站上逐步执行任务。你可以用鼠标点击一个元素，向上或向下滚动，访问某个网址或返回上一页，或用键盘输入一些文本（例如，playwright中的click()、scroll()、goto()、go_back()和type()函数）。一步意味着在上述任何操作中进行的一次操作。
- en: 'You are within a sandbox and only have access to the following websites to
    work with:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你在一个沙盒中，只能访问以下网站进行操作：
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An online shopping website (OneStopShop): {webarena_root}:7770'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个在线购物网站（OneStopShop）：{webarena_root}:7770
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An e-commerce management website (Magento): {webarena_root}:7780/admin'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个电子商务管理网站（Magento）：{webarena_root}:7780/admin
- en: •
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Reddit website (Postmill): {webarena_root}:9999'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个Reddit网站（Postmill）：{webarena_root}:9999
- en: •
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A GitLab website: {webarena_root}:8023'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个GitLab网站：{webarena_root}:8023
- en: •
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A map website (OpenStreetMap): [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个地图网站（OpenStreetMap）：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)
- en: •
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Wikipedia website: [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个维基百科网站：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)
- en: 'Notes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：
- en: '1.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: If you want to use the search function, you don’t need to click on the search
    bar. You can directly use “type [element_id] [things_to_type]”, and generally
    afterwards, you don’t need to click the search button (by default, the command
    contains an ENTER at the end).
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想使用搜索功能，你不需要点击搜索栏。你可以直接使用“type [element_id] [things_to_type]”，通常之后你不需要点击搜索按钮（默认情况下，命令包含一个ENTER键）。
- en: '2.'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: You can assume that you have signed in to your account (we have set up the cookies,
    so login is not needed).
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以假设你已经登录到你的账户（我们已经设置好cookies，所以不需要登录）。
- en: 'The website that you will be working with is:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你将要处理的网站是：
- en: '{WEBSITE INTRO}'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '{WEBSITE INTRO}'
- en: 'Please follow these specific instructions to solve tasks:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请遵循这些具体指示来解决任务：
- en: '{INSTRUCTION}'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '{INSTRUCTION}'
- en: 'Here is a more detailed description of the starting screen:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是启动屏幕的更详细描述：
- en: '{STARTING SCREEN DESCRIPTION}'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '{STARTING SCREEN DESCRIPTION}'
- en: 'Now, based on the information above, what should be the steps to achieve the
    following goal (please give me a list of textual description of playwright actions,
    starting with ‘List’):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，基于上述信息，完成以下目标的步骤应该是什么（请给出以‘List’开头的剧本动作文本描述列表）：
- en: '{TASK}'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'For your reference, here are some experiences from previous failed trials (please
    consider the following information to generate a better plan):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 供参考，这里是一些以前失败尝试的经验（请考虑以下信息以生成更好的计划）：
- en: '{FAILED PLAN}'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '{FAILED PLAN}'
- en: 'Past experience:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的经验：
- en: '{HISTORY}'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: To be successful in generating a new plan, you need to provide a list (1, 2,
    3, …), in which each item is a natural language description of one playwright
    action that is necessary to complete the task (e.g., click on the ‘Account’ button;
    scroll down; use the search bar to search for iPhone 13). You should use the information
    from the past experiences to save unnecessary steps!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功生成新计划，你需要提供一个列表（1, 2, 3, …），其中每项是完成任务所需的一个剧本动作的自然语言描述（例如，点击‘账户’按钮；向下滚动；使用搜索栏搜索
    iPhone 13）。你应该利用过去的经验来节省不必要的步骤！
- en: Prompt for Action Generation ($G_{\text{action}}$)
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成动作提示（$G_{\text{action}}$）
- en: 'I am in a sandbox and only have access to the following websites (i.e., no
    access to external website like www.reddit.com):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我在一个沙箱环境中，只能访问以下网站（即，无法访问像 www.reddit.com 这样的外部网站）：
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An online shopping website (OneStopShop): {webarena_root}:7770'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个在线购物网站（OneStopShop）：{webarena_root}:7770
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An e-commerce management website (Magento): {webarena_root}:7780/admin'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个电商管理网站（Magento）：{webarena_root}:7780/admin
- en: •
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Reddit website (Postmill): {webarena_root}:9999'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个Reddit网站（Postmill）：{webarena_root}:9999
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A GitLab website: {webarena_root}:8023'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个GitLab网站：{webarena_root}:8023
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A map website (OpenStreetMap): [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个地图网站（OpenStreetMap）：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Wikipedia website: [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个维基百科网站：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)
- en: Now I’m trying to complete a task on a website.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我正在尝试在一个网站上完成一个任务。
- en: 'The task is:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是：
- en: '{TASK}'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'The plan to complete this task is:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此任务的计划是：
- en: '{PLAN}'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '{PLAN}'
- en: 'I have executed the following actions:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经执行了以下操作：
- en: '{HISTORY}'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: 'And now I’m at this step: {STEP}'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我在这个步骤：{STEP}
- en: 'Here is the screen I am looking at:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我当前看到的屏幕：
- en: '{OBS}'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS}'
- en: 'I have taken down the following notes:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我记录了以下笔记：
- en: '{NOTES}'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '{NOTES}'
- en: What should be the next action to complete this step in my plan (only give one
    action)?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的计划中完成此步骤的下一步操作是什么（仅给出一个操作）？
- en: 'Note:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the next action is to click, please indicate the element id in [] (format:
    click [element_id]).'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下一步操作是点击，请在 [] 中指明元素 ID（格式：click [element_id]）。
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the next action is to scroll, please indicate the direction in [] (format:
    scroll [up or down]).'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下一步操作是滚动，请在 [] 中指明方向（格式：scroll [up or down]）。
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If you need to navigate to a URL, please indicate the URL in [] (format: goto
    [url]).'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你需要导航到一个 URL，请在 [] 中指明 URL（格式：goto [url]）。
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If you need to go back to the previous page, please use go_back.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你需要返回上一页，请使用 go_back。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the next action is to type, please indicate both element id and the things
    to type in [] (format: type [element_id] [things to type]).'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下一步操作是输入，请在 [] 中指明元素 ID 和输入内容（格式：type [element_id] [things to type]）。
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If you want to note down something, use this format: note_down [things to note
    down].'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想记下某些内容，请使用以下格式：note_down [things to note down]。
- en: 'The next action is:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步操作是：
- en: Prompt for Objective Alignment ($G_{\text{align}}$)
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标对齐提示 ($G_{\text{align}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在模仿人们逐步在网站上执行任务。
- en: 'You are currently working on this step:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你目前正在进行以下步骤：
- en: '{STEP}.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '{STEP}。'
- en: 'The step above is one of the steps in the following plan:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤是以下计划中的其中一步：
- en: '{PLAN}.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '{PLAN}。'
- en: From Screen 1, you executed an action and then arrived at Screen 2.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 从屏幕 1，你执行了一个操作，然后到达了屏幕 2。
- en: 'The action you executed was:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 你执行的操作是：
- en: '{ACTION}.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '{ACTION}。'
- en: 'Screen 1:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕 1：
- en: '{OBS1}.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS1}。'
- en: 'Screen 2:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕 2：
- en: '{OBS2}.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS2}。'
- en: Now describe what this action is about in one sentence, starting with ‘The action
    is to’.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在用一句话描述这个操作的内容，以“这个操作是”开头。
- en: Does this action align with the goal of the following step (i.e., are we moving
    towards the right direction; Answer YES or NO)?
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作是否与以下步骤的目标对齐（即我们是否朝着正确的方向前进；回答 YES 或 NO）？
- en: '{STEP}'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '{STEP}'
- en: Prompt for Task / Subtask Completion Evaluation ($G_{\text{completed}}$)
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务/子任务完成评估提示 ($G_{\text{completed}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在模仿人们逐步在网站上执行任务。
- en: 'You are asked to solve the following task:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求解决以下任务：
- en: '{TASK}'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'You made the following plan to solve it:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你制定了以下计划来解决它：
- en: '{PLAN}'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '{PLAN}'
- en: 'To reach the current screen, you have previously executed the following actions:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要到达当前屏幕，你之前执行了以下操作：
- en: '{HISTORY}'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: 'You have taken down a few notes after each action as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你在每个操作后记下了一些笔记，如下：
- en: '{NOTES}'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '{NOTES}'
- en: 'And here is the accessibility tree of the current screen you are looking at:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你当前查看的屏幕的可访问性树：
- en: '{OBS}'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS}'
- en: Look at the screen, the task, and the actions you executed, and think thoroughly,
    is the task completed now?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 查看屏幕、任务和你执行的操作，仔细思考，现在任务是否完成？
- en: If the task is completed, answer YES.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务完成，回答 YES。
- en: If the task is not yet completed (meaning further actions are yet to be executed),
    answer NO.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务尚未完成（意味着仍需执行进一步操作），请回答 NO。
- en: Prompt for Answer Delivery ($G_{\text{answer}}$)
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 答案交付提示 ($G_{\text{answer}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在模仿人们逐步在网站上执行任务。
- en: 'You are asked to solve the following task:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求解决以下任务：
- en: '{TASK}'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'To reach the current screen, you have previously executed the following actions:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 要到达当前屏幕，你之前执行了以下操作：
- en: '{HISTORY}'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: 'You have taken down the following notes (to help you answer the question eventually)
    after each action:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你在每个操作后记下了以下笔记（以帮助你最终回答问题）：
- en: '{NOTES}'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '{NOTES}'
- en: 'And here is the accessibility tree of the current screen you are looking at:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你当前查看的屏幕的可访问性树：
- en: '{OBS}'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS}'
- en: 'Based on the above information, answer the question in the task (starting with
    ###Answer).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述信息，回答任务中的问题（以###Answer开头）。
- en: Prompt for Element Mapping ($G_{\text{map}}$)
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元素映射提示 ($G_{\text{map}}$)
- en: 'I want to interact with an element with element id: {element_id} in the following
    screen:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在下一屏幕中与元素 ID 为 {element_id} 的元素交互：
- en: '{OBS1}'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS1}'
- en: Now if I want to click on the same element in the following screen, what should
    be the element id now?
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我想点击下一屏幕中的相同元素，元素 ID 应该是什么？
- en: '{OBS2}'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS2}'
- en: 'New element id is:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 新的元素 ID 是：
