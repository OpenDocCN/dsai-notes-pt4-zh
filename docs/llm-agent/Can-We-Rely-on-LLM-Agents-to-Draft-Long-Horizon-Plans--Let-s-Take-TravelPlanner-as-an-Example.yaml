- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:39:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:39:02'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们可以依靠大语言模型（LLM）代理来制定长期计划吗？以 TravelPlanner 为例
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.06318](https://ar5iv.labs.arxiv.org/html/2408.06318)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.06318](https://ar5iv.labs.arxiv.org/html/2408.06318)
- en: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu    Dong Hoon Yi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu    Dong Hoon Yi
- en: LG Electronics, Toronto AI Lab, Toronto, Canada
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LG 电子, 多伦多 AI 实验室, 加拿大多伦多
- en: '{yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) have brought autonomous agents closer to artificial
    general intelligence (AGI) due to their promising generalization and emergent
    capabilities. There is, however, a lack of studies on how LLM-based agents behave,
    why they could potentially fail, and how to improve them, particularly in demanding
    real-world planning tasks. In this paper, as an effort to fill the gap, we present
    our study using a realistic benchmark, TravelPlanner Xie et al. ([2024](#bib.bib45)),
    where an agent must meet multiple constraints to generate accurate plans. We leverage
    this benchmark to address four key research questions: (1) are LLM agents robust
    enough to lengthy and noisy contexts when it comes to reasoning and planning?
    (2) can few-shot prompting adversely impact the performance of LLM agents in scenarios
    with long context? (3) can we rely on refinement to improve plans, and (4) can
    fine-tuning LLMs with both positive and negative feedback lead to further improvement?
    Our comprehensive experiments indicate that, firstly, LLMs often fail to attend
    to crucial parts of a long context, despite their ability to handle extensive
    reference information and few-shot examples; secondly, they still struggle with
    analyzing the long plans and cannot provide accurate feedback for refinement;
    thirdly, we propose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive
    and negative feedback, resulting in substantial gains over Supervised Fine-Tuning
    (SFT). Our findings offer in-depth insights to the community on various aspects
    related to real-world planning applications.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）由于其有希望的泛化能力和突现能力，使得自主代理更接近于人工通用智能（AGI）。然而，关于 LLM 基础的代理如何表现、为什么可能会失败以及如何改进它们，特别是在要求严格的现实世界规划任务中，仍然缺乏研究。本文旨在填补这一空白，我们使用了一个现实的基准测试
    TravelPlanner Xie 等人 ([2024](#bib.bib45))，其中代理必须满足多个约束以生成准确的计划。我们利用这一基准测试来解决四个关键研究问题：（1）在推理和规划方面，LLM
    代理是否足够健壮以处理冗长和嘈杂的上下文？（2）少量示例提示是否会对长上下文中的 LLM 代理性能产生不利影响？（3）我们是否可以依靠细化来改进计划，以及（4）使用正负反馈对
    LLM 进行微调是否能进一步改进？我们的全面实验表明，首先，尽管 LLM 能处理大量参考信息和少量示例，但它们常常未能关注长上下文中的关键部分；其次，它们在分析长期计划时仍然存在困难，无法提供准确的细化反馈；第三，我们提出了反馈感知微调（FAFT），该方法利用正负反馈，相比于监督微调（SFT）有了显著的提升。我们的发现为社区提供了有关现实世界规划应用的各个方面的深入见解。
- en: Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以依靠大语言模型（LLM）代理来制定长期计划吗？以 TravelPlanner 为例
- en: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu,  and Dong Hoon Yi LG Electronics,
    Toronto AI Lab, Toronto, Canada {yanan.chen, ali.pesaranghader, tanmana.sadh,
    donghoon9.yi}@lge.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Yanan Chen, Ali Pesaranghader, Tanmana Sadhu 和 Dong Hoon Yi LG 电子, 多伦多 AI 实验室,
    加拿大多伦多 {yanan.chen, ali.pesaranghader, tanmana.sadh, donghoon9.yi}@lge.com
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'LLMs have shown significant reasoning and planning results against various
    benchmarks such as WebArena Zhou et al. ([2023](#bib.bib58)), WebShop Yao et al.
    ([2022a](#bib.bib48)), AgentBench Liu et al. ([2023b](#bib.bib17)) and AgentGym
    Xi et al. ([2024b](#bib.bib44)) where they act as agents to finish a given task
    on behalf of humans. In this vein, the community considers two main directions
    for developing LLM-based agents: (1) prompting LLMs for reasoning, planning, and
    execution Qin et al. ([2023](#bib.bib25)); Wei et al. ([2022](#bib.bib38)); Yao
    et al. ([2024](#bib.bib49)); Wang et al. ([2022](#bib.bib37)), and (2) fine-tuning
    LLMs for a given task Chen et al. ([2023b](#bib.bib3)); Zeng et al. ([2023](#bib.bib51));
    Zhang et al. ([2024b](#bib.bib53)); Chen et al. ([2024](#bib.bib4)); Song et al.
    ([2024b](#bib.bib33)). Despite promising contributions in each direction, it is
    seen that LLMs still fall short in more complex scenarios. TravelPlanner Xie et al.
    ([2024](#bib.bib45)), as an example, is a benchmark where an agent should generate
    a plan which must meet multiple constraints with respect to input queries. The
    authors showed that GPT-4-Turbo OpenAI ([2023](#bib.bib21)) could only reach to
    Final Pass Rate of 4.4%. This indicates that LLM agents cannot handle long-horizon
    reasoning and planning. In this paper, we investigate these challenges further
    with four research questions using TravelPlanner as the benchmark, and we trust
    that our promising and negative findings will benefit the community.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 在 WebArena Zhou 等 ([2023](#bib.bib58))、WebShop Yao 等 ([2022a](#bib.bib48))、AgentBench
    Liu 等 ([2023b](#bib.bib17)) 和 AgentGym Xi 等 ([2024b](#bib.bib44)) 等各种基准测试中展示了显著的推理和规划成果，在这些基准测试中，LLM
    充当代理人，代表人类完成给定任务。在这方面，社区考虑了两种主要方向来开发基于 LLM 的代理人：（1）促使 LLM 进行推理、规划和执行 Qin 等 ([2023](#bib.bib25))；Wei
    等 ([2022](#bib.bib38))；Yao 等 ([2024](#bib.bib49))；Wang 等 ([2022](#bib.bib37))，和（2）对
    LLM 进行针对特定任务的微调 Chen 等 ([2023b](#bib.bib3))；Zeng 等 ([2023](#bib.bib51))；Zhang
    等 ([2024b](#bib.bib53))；Chen 等 ([2024](#bib.bib4))；Song 等 ([2024b](#bib.bib33))。尽管在每个方向上都有令人鼓舞的贡献，但
    LLM 在更复杂的场景中仍显不足。例如，TravelPlanner Xie 等 ([2024](#bib.bib45)) 是一个基准测试，其中代理人必须生成一个符合输入查询的多个约束的计划。作者展示了
    GPT-4-Turbo OpenAI ([2023](#bib.bib21)) 仅能达到 4.4% 的最终通过率。这表明 LLM 代理人无法处理长期推理和规划。在本文中，我们通过四个研究问题进一步探讨这些挑战，以
    TravelPlanner 作为基准，我们相信我们的有前景的和负面的发现将对社区有所裨益。
- en: Our extensive experiments indicate that (1) lengthy and noisy context can adversely
    impact planning ability of the LLM agent, (2) more shots do not necessarily guarantee
    performance improvement, (3) refinement may not be effective when LLMs are employed
    as feedback generators; however, it is more likely to work if the feedback generator
    is based on heuristic rules, and (4) feedback-aware fine-tuning (FAFT), our proposed
    approach, inspired by negative aware training (NAT) Wang et al. ([2024b](#bib.bib36)),
    can show remarkable improvement in planning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大量实验表明：（1）冗长且嘈杂的上下文会对 LLM 代理的规划能力产生不利影响，（2）更多的尝试不一定能保证性能提升，（3）当 LLM 被用作反馈生成器时，精化可能效果不佳；但如果反馈生成器基于启发式规则，它更有可能有效，（4）反馈感知微调（FAFT），我们提出的方法，受负面感知训练（NAT）
    Wang 等 ([2024b](#bib.bib36)) 的启发，可以在规划中表现出显著的改善。
- en: '![Refer to caption](img/b2774a67abf2928bd40a770d675d54d2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2774a67abf2928bd40a770d675d54d2.png)'
- en: 'Figure 1: Four LLM agents interact to generate a plan. (Fig. [A.1](#A1.F1 "Figure
    A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") is an example
    for the refinement module.)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：四个 LLM 代理交互生成一个计划。（图 [A.1](#A1.F1 "Figure A.1 ‣ A.2 Evaluation metrics ‣
    Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s
    Take TravelPlanner as an Example") 是精化模块的示例。）
- en: 2 Methodology
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'Our framework, built upon TravelPlanner, consists of five main components:
    Scrubber, Planner, Feedback Generator, Refiner, and the Evaluation module (as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). The scrubber
    provides clean reference information¹¹1This is a terminology that TravelPlanner
    uses to refer to necessary information for generating a plan. and few-shot examples
    to the Planner for generating a plan. Then, the Feedback Generator provides feedback
    to the Refiner for improving the plan if required. The interaction continues until
    the pre-defined settings are met. The Planner is the core of the framework which
    can be based on either (1) in-context learning (ICL), or (2) supervised fine-tuning
    (SFT), e.g., FAFT as proposed in Section [4](#S4 "4 Findings ‣ Can We Rely on
    LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")-RQ4\.
    Appx. [A.3](#A1.SS3 "A.3 Framework ‣ Appendix A Appendix ‣ Can We Rely on LLM
    Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") describes
    each agent in detail.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架建立在 TravelPlanner 之上，由五个主要组件组成：Scrubber、Planner、Feedback Generator、Refiner
    和 Evaluation 模块（如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM
    Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") 所示）。Scrubber
    为 Planner 提供干净的参考信息¹¹1这是 TravelPlanner 用于生成计划所需信息的术语。以及少量示例，以生成计划。然后，Feedback
    Generator 向 Refiner 提供反馈，以在需要时改进计划。交互继续进行，直到满足预定义设置。Planner 是框架的核心，可以基于（1）上下文学习（ICL）或（2）监督微调（SFT），例如第
    [4](#S4 "4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s
    Take TravelPlanner as an Example")-RQ4 节中提出的 FAFT。附录 [A.3](#A1.SS3 "A.3 Framework
    ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans?
    Let’s Take TravelPlanner as an Example") 详细描述了每个代理。
- en: 3 Experimental Settings
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: Basic Setting. Since the focus of our work is on agents’ capabilities in drafting
    plans, we only rely on the Sole Planning setting from TravelPlanner. That is,
    all comprehensive and necessary information, which are human annotations, is directly
    provided to the planner agent. We also consider the Direct²²2the query is input
    directly into the model along with instructions detailing the task and relevant
    information gathered. planning strategy for its simplicity because it performs
    at a similar level to other reasoning techniques such as ZS-CoT Wei et al. ([2022](#bib.bib38)),
    ReAct Yao et al. ([2022b](#bib.bib50)) and Reflexion Shinn et al. ([2024](#bib.bib30)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基本设置。由于我们工作的重点是代理在制定计划方面的能力，我们仅依赖于 TravelPlanner 的 Sole Planning 设置。也就是说，所有综合且必要的信息（即人工注释）直接提供给规划代理。我们还考虑了
    Direct²²2，因为它的规划策略简单易行，表现与其他推理技术（如 ZS-CoT Wei 等 ([2022](#bib.bib38))，ReAct Yao
    等 ([2022b](#bib.bib50)) 和 Reflexion Shinn 等 ([2024](#bib.bib30))）相当。
- en: Dataset. (See Appx. [A.1](#A1.SS1 "A.1 Dataset ‣ Appendix A Appendix ‣ Can We
    Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an
    Example")) We use the training set for both few-shot prompting and fine-tuning
    because it provides annotated plans. We evaluate the agent against both validation
    and test sets for RQ1 and RQ2 in Section [4](#S4 "4 Findings ‣ Can We Rely on
    LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example").
    As for RQ3, we consider only the validation set because we do not have access
    to the system feedback offline. Regarding RQ4, we use the training set for fine-tuning
    the (Open-LLM) planner agent.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。（参见附录 [A.1](#A1.SS1 "A.1 Dataset ‣ Appendix A Appendix ‣ Can We Rely on
    LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")）我们使用训练集进行少量提示和微调，因为它提供了注释计划。我们对代理进行验证和测试集的评估，以回答第
    [4](#S4 "4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s
    Take TravelPlanner as an Example")节中的 RQ1 和 RQ2。至于 RQ3，我们仅考虑验证集，因为我们无法离线访问系统反馈。关于
    RQ4，我们使用训练集对 (Open-LLM) 规划代理进行微调。
- en: Metrics. We utilize the original evaluation metrics from TravelPlanner, which
    evaluate performance based on the pass rates of multiple constraints. Additional
    details are available in Appx. [A.2](#A1.SS2 "A.2 Evaluation metrics ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example").
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。我们利用 TravelPlanner 的原始评估指标，这些指标基于多个约束的通过率来评估性能。更多细节请参见附录 [A.2](#A1.SS2 "A.2
    Evaluation metrics ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft
    Long-Horizon Plans? Let’s Take TravelPlanner as an Example")。
- en: '| GPT-3.5-Turbo as Planner |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo 作为 Planner |'
- en: '|  |  | Validation Set (#180) | Test Set (#1,000) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 验证集 (#180) | 测试集 (#1,000) |'
- en: '| Reference Scrubbed? | Num. Shots | Delivery Rate |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 参考是否已清理？ | 拍摄数量 | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate | Halluc. Rate | Delivery Rate |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 | 幻觉率 | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate | Halluc. Rate |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 | 幻觉率 |'
- en: '| (RQ1) | (RQ2) | Micro | Macro | Micro | Macro | Micro | Macro | Micro | Macro
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| (RQ1) | (RQ2) | 微观 | 宏观 | 微观 | 宏观 | 微观 | 宏观 | 微观 | 宏观 |'
- en: '| No | 0 | 100 | 60.2 | 4.4 | 11.0 | 2.8 | 0.0 | 57.4 | 100 | 60.8 | 3.5 |
    13.6 | 4.9 | 0.6 | 61.1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 0 | 100 | 60.2 | 4.4 | 11.0 | 2.8 | 0.0 | 57.4 | 100 | 60.8 | 3.5 | 13.6
    | 4.9 | 0.6 | 61.1 |'
- en: '| No | 1 | 100 | 65.4 | 11.0 | 17.5 | 5.1 | 1.0 | 52.3 | 100 | 64.0 | 10.1
    | 16.1 | 6.4 | 1.2 | 59.4 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 1 | 100 | 65.4 | 11.0 | 17.5 | 5.1 | 1.0 | 52.3 | 100 | 64.0 | 10.1 |
    16.1 | 6.4 | 1.2 | 59.4 |'
- en: '| Yes | 0 | 100 | 74.4 | 18.9 | 29.0 | 14.4 | 4.4 | 41.6 | 100 | 70.3 | 12.3
    | 25.0 | 10.7 | 2.7 | 49.8 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 0 | 100 | 74.4 | 18.9 | 29.0 | 14.4 | 4.4 | 41.6 | 100 | 70.3 | 12.3
    | 25.0 | 10.7 | 2.7 | 49.8 |'
- en: '| Yes | 1 | 100 | 80.6 | 24.4 | 40.2 | 17.8 | 7.2 | 35.5 | 100 | 78.0 | 18.6
    | 36.1 | 17.7 | 4.9 | 40.8 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 1 | 100 | 80.6 | 24.4 | 40.2 | 17.8 | 7.2 | 35.5 | 100 | 78.0 | 18.6
    | 36.1 | 17.7 | 4.9 | 40.8 |'
- en: '| Yes | 2 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | 38.8 | 100 | 80.9 | 22.4
    | 34.3 | 16.7 | 6.5 | 44.3 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 2 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | 38.8 | 100 | 80.9 | 22.4
    | 34.3 | 16.7 | 6.5 | 44.3 |'
- en: '| Yes | 4 | 100 | 81.5 | 29.4 | 35.5 | 12.2 | 5.8 | 46.6 | 100 | 80.3 | 21.1
    | 31.5 | 15.0 | 5.2 | 50.8 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 4 | 100 | 81.5 | 29.4 | 35.5 | 12.2 | 5.8 | 46.6 | 100 | 80.3 | 21.1
    | 31.5 | 15.0 | 5.2 | 50.8 |'
- en: '| Yes | 5 | 100 | 81.1 | 26.3 | 32.6 | 12.4 | 4.8 | 49.8 | 100 | 79.5 | 20.4
    | 30.4 | 13.2 | 5.6 | 53.1 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 5 | 100 | 81.1 | 26.3 | 32.6 | 12.4 | 4.8 | 49.8 | 100 | 79.5 | 20.4
    | 30.4 | 13.2 | 5.6 | 53.1 |'
- en: 'Table 1: Performance of GPT-3.5-Turbo as the Planner agent for different settings
    for RQ1 and RQ2'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：GPT-3.5-Turbo 作为规划代理在不同设置下对 RQ1 和 RQ2 的表现
- en: '|  |  | GPT-3.5-Turbo as Planner and GPT-4-Turbo as Refiner |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPT-3.5-Turbo 作为规划者与 GPT-4-Turbo 作为精炼者 |'
- en: '|  |  | vs. Validation Set (#180) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 与验证集对比（#180） |'
- en: '| Feedback Generator (RQ3) | Refinement Iteration | Delivery Rate |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 反馈生成器（RQ3） | 精炼迭代 | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate | Uplift Ratio ($\uparrow$) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 | 提升比率（$\uparrow$） |'
- en: '| Micro | Macro | Micro | Macro |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 微观 | 宏观 | 微观 | 宏观 |'
- en: '| None | 0 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | – | – | – |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 无 | 0 | 100 | 82.6 | 32.2 | 41.2 | 17.8 | 7.2 | – | – | – |'
- en: '| Oracle (Heuristic Rules) | 1 | 100 | 89.7 | 51.1 | 50.0 | 22.2 | 11.7 | 46.1
    | 52.8 | 1.1 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 预言者（启发式规则） | 1 | 100 | 89.7 | 51.1 | 50.0 | 22.2 | 11.7 | 46.1 | 52.8 | 1.1
    |'
- en: '| 2 | 100 | 89.0 | 54.4 | 50.5 | 18.3 | 12.8 | 8.9 | 76.7 | 14.4 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 89.0 | 54.4 | 50.5 | 18.3 | 12.8 | 8.9 | 76.7 | 14.4 |'
- en: '| 3 | 100 | 89.9 | 56.1 | 50.7 | 21.1 | 13.3 | 13.9 | 78.9 | 7.2 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 89.9 | 56.1 | 50.7 | 21.1 | 13.3 | 13.9 | 78.9 | 7.2 |'
- en: '| 4 | 100 | 89.1 | 59.4 | 49.8 | 21.7 | 13.9 | 5.0 | 86.7 | 8.3 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 89.1 | 59.4 | 49.8 | 21.7 | 13.9 | 5.0 | 86.7 | 8.3 |'
- en: '| Random | 1 | 100 | 82.3 | 31.1 | 47.1 | 21.1 | 7.2 | 21.7 | 53.3 | 25.0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 1 | 100 | 82.3 | 31.1 | 47.1 | 21.1 | 7.2 | 21.7 | 53.3 | 25.0 |'
- en: '| 2 | 100 | 82.3 | 32.2 | 46.2 | 20.0 | 8.3 | 18.3 | 63.9 | 17.8 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 82.3 | 32.2 | 46.2 | 20.0 | 8.3 | 18.3 | 63.9 | 17.8 |'
- en: '| 3 | 100 | 82.0 | 30.6 | 45.5 | 20.0 | 7.2 | 19.4 | 61.7 | 18.9 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 82.0 | 30.6 | 45.5 | 20.0 | 7.2 | 19.4 | 61.7 | 18.9 |'
- en: '| 4 | 100 | 82.6 | 30.6 | 44.8 | 18.3 | 7.2 | 18.9 | 62.8 | 18.3 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 82.6 | 30.6 | 44.8 | 18.3 | 7.2 | 18.9 | 62.8 | 18.3 |'
- en: '| GPT-3.5-Turbo (0125) | 1 | 100 | 82.0 | 24.4 | 41.4 | 21.7 | 8.9 | 22.2 |
    52.8 | 25.0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo (0125) | 1 | 100 | 82.0 | 24.4 | 41.4 | 21.7 | 8.9 | 22.2 |
    52.8 | 25.0 |'
- en: '| 2 | 100 | 82.9 | 28.9 | 40.9 | 18.3 | 8.9 | 24.4 | 55.0 | 20.6 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 82.9 | 28.9 | 40.9 | 18.3 | 8.9 | 24.4 | 55.0 | 20.6 |'
- en: '| 3 | 100 | 83.8 | 27.8 | 41.7 | 20.0 | 8.9 | 25.0 | 56.1 | 18.9 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 83.8 | 27.8 | 41.7 | 20.0 | 8.9 | 25.0 | 56.1 | 18.9 |'
- en: '| 4 | 100 | 82.4 | 26.7 | 40.7 | 18.9 | 7.8 | 19.4 | 57.8 | 22.8 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 82.4 | 26.7 | 40.7 | 18.9 | 7.8 | 19.4 | 57.8 | 22.8 |'
- en: '| GPT-4-Turbo (1106-preview) | 1 | 100 | 86.9 | 32.8 | 39.3 | 20.0 | 9.4 |
    34.4 | 40.6 | 25.0 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-Turbo (1106-preview) | 1 | 100 | 86.9 | 32.8 | 39.3 | 20.0 | 9.4 |
    34.4 | 40.6 | 25.0 |'
- en: '| 2 | 100 | 84.3 | 29.4 | 37.9 | 15.6 | 7.2 | 20.0 | 59.4 | 20.6 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 100 | 84.3 | 29.4 | 37.9 | 15.6 | 7.2 | 20.0 | 59.4 | 20.6 |'
- en: '| 3 | 100 | 84.6 | 30.0 | 40.5 | 18.3 | 7.2 | 18.3 | 67.2 | 14.4 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 100 | 84.6 | 30.0 | 40.5 | 18.3 | 7.2 | 18.3 | 67.2 | 14.4 |'
- en: '| 4 | 100 | 86.4 | 28.3 | 37.9 | 20.0 | 6.7 | 19.4 | 58.3 | 22.2 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 100 | 86.4 | 28.3 | 37.9 | 20.0 | 6.7 | 19.4 | 58.3 | 22.2 |'
- en: 'Table 2: Performance of different Feedback Generators. Uplift Ratio ($\uparrow$)
    show what percentage of plans has improved, not changed, and deteriorated, respectively.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同反馈生成器的性能。提升比例（$\uparrow$）显示了计划改进、未变化和恶化的百分比。
- en: 4 Findings
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 发现
- en: 'RQ1: Are LLM agents robust enough to noisy information for reasoning and planning?
    Table [1](#S3.T1 "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example") shows that
    GPT-3.5-Turbo has a better performance when it receives *shrunk* reference information.
    This indicates that GPT-3.5-Turbo still struggles to attend to the most important
    parts of a given context for reasoning, prone to excessive irrelevant (context)
    chunks. Therefore, it is worth considering an external intelligent context-cleaning
    agent.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1：LLM代理是否足够强健以应对嘈杂信息进行推理和规划？表[1](#S3.T1 "Table 1 ‣ 3 Experimental Settings
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")显示，当GPT-3.5-Turbo接收到*压缩*的参考信息时，其表现更佳。这表明GPT-3.5-Turbo在推理时仍然难以关注给定上下文中的最重要部分，容易受到过多无关（上下文）片段的影响。因此，考虑一个外部智能的上下文清理代理是值得的。
- en: 'RQ2: Can more shots help with the planning task, or does it worsen hallucination?
    It is commonly accepted that having more few-shots is helpful in ICL, but does
    it apply to TravelPlanner? As Table [1](#S3.T1 "Table 1 ‣ 3 Experimental Settings
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example") shows, the Final Pass rate reaches its highest value when there
    are $2$ shots, while having more shots may not improve if not hurt more. We presume
    that more shots in the context window may distract the LLM and lead to hallucination
    (e.g., using entities that do not exist in the given reference information). The
    results of the Hallucination Rate attest to this assumption. That is, giving more
    shots may potentially cause severer hallucination in tasks where the context of
    the reference information is complex tabular texts. Another finding is that at
    least one in-context example is beneficial Xie and Min ([2022](#bib.bib46)). Finally,
    we conclude that as we have more shots, the pass rate and hallucination rate results
    worsen.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2：更多的示例是否有助于规划任务，还是会加剧幻觉现象？通常认为，在*少量示例学习*（ICL）中，更多的示例是有帮助的，但这是否适用于TravelPlanner？如表[1](#S3.T1
    "Table 1 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example")所示，当示例数量为$2$时，最终通过率达到了最高值，而更多的示例可能不会提升，甚至可能会更糟。我们推测，背景窗口中更多的示例可能会分散LLM的注意力，并导致幻觉（例如，使用不存在于给定参考信息中的实体）。幻觉率的结果证实了这一假设。也就是说，在参考信息的背景复杂的表格文本任务中，提供更多示例可能会导致更严重的幻觉。另一项发现是，至少一个背景示例是有益的（Xie
    和 Min，[2022](#bib.bib46)）。最后，我们得出结论，当示例数量增加时，通过率和幻觉率的结果会恶化。
- en: Our RQ1 and RQ2 observations align with the existing theoretical and experimental
    works, e.g., Han et al. ([2023](#bib.bib10)); Levy et al. ([2024](#bib.bib13)),
    which identify the potential causes underlying current LLMs’ failure in length
    generalization, that when they encounter a much longer context, the attention
    scores are diluted, and thus the score distribution becomes flat leading to information
    loss. That is, the entropy of the attention score will explode with increasing
    context. In other words, LLMs become lost in how to focus on the right information,
    especially when pre-training is done on shorter text segments.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的RQ1和RQ2观察结果与现有的理论和实验工作一致，例如，Han等人（[2023](#bib.bib10)）；Levy等人（[2024](#bib.bib13)），这些研究识别了当前LLM在长度泛化中的潜在失败原因，即当它们遇到更长的上下文时，注意力分数会被稀释，从而使得分布变得平坦，导致信息丢失。也就是说，注意力分数的熵会随着上下文的增加而爆炸。换句话说，LLM在如何关注正确的信息方面会变得迷失，特别是当预训练是在较短文本片段上完成的情况下。
- en: 'RQ3: Can we rely on refinement to improve plans? To address this, we require
    feedback that highlights what went wrong, accompanied by explanations of the reasons
    behind the issues. For that, we examine the reliability of GPT-3.5-Turbo and GPT-4-Turbo
    as LLM-based feedback generators. In addition, as an ablative point of view, we
    also consider Random and Oracle feedback generators. The former refers to the
    setting where we fabricate the feedback using random content in a valid format,
    and the latter uses heuristic hard-coded rules³³3Rules from TravelPlanner: [https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation](https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation)
    to check whether the plan complies with the constraints. Furthermore, we do not
    consider any weaker language models because they have shown to be incapable of
    handling this kind of task in previous studies Madaan et al. ([2024](#bib.bib20)).
    We only focus on commonsense constraints in this part due to the frequent absence
    of hard constraints in the queries and the feedback⁴⁴4 Consistent with the original
    setting of TravelPlanner, i.e., plans that fail to satisfy all commonsense constraints
    will not proceed to receive feedback regarding hard constraints. This decision
    is rooted in the dependency of hard constraint computation on commonsense criteria..
    We present the results for RQ3 in Table [2](#S3.T2 "Table 2 ‣ 3 Experimental Settings
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example"). The feedback generator and the Refiner agent interact iteratively;
    at each iteration, the previously generated plans are reviewed by the feedback
    generator to draft feedback subjectively. Considering the feedback, if any refinement
    is needed, i.e., any constraint is not met, the Refiner agent is triggered to
    modify the plan. *This design simulates the production-level environment where
    no Oracle feedback generator is available to check whether a plan needs refinement.*
    We summarize our findings as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3：我们可以依赖优化来改进计划吗？为了解决这个问题，我们需要反馈突出指出错误，并附有问题原因的解释。为此，我们考察了GPT-3.5-Turbo和GPT-4-Turbo作为基于LLM的反馈生成器的可靠性。此外，从消融的角度出发，我们还考虑了随机和Oracle反馈生成器。前者指的是使用有效格式的随机内容来生成反馈，后者使用来自TravelPlanner的启发式硬编码规则³³3Rules：[https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation](https://github.com/OSU-NLP-Group/TravelPlanner/tree/main/evaluation)
    来检查计划是否符合约束。此外，我们没有考虑任何较弱的语言模型，因为在之前的研究中，它们被证明无法处理这种任务（Madaan et al. ([2024](#bib.bib20))）。由于查询和反馈中经常缺少硬约束，我们在这一部分仅关注常识性约束。这一决定基于硬约束计算对常识标准的依赖。我们在表[2](#S3.T2
    "Table 2 ‣ 3 Experimental Settings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example")中展示了RQ3的结果。反馈生成器和优化器代理进行迭代交互；在每次迭代中，之前生成的计划由反馈生成器主观地审查，以起草反馈。考虑到反馈，如果需要优化，即有任何约束未满足，优化器代理会被触发以修改计划。*这个设计模拟了生产级环境，其中没有Oracle反馈生成器来检查计划是否需要优化。*
    我们总结了我们的发现如下：
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Refinement can help improve the plans if the feedback is of high quality and
    precise – We see that the refinement helps with improving the plans if the feedback
    is accurate and well-organized as in the Oracle setting. The table shows, in the
    first iteration, $46.1\%$. From the second iteration, we do not see any significant
    improvement and the pass rates saturate.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果反馈质量高且精确，优化可以帮助改善计划——我们看到，如果反馈准确且组织良好，如在Oracle设置中，优化有助于改善计划。表格显示，在第一次迭代中为$46.1\%$。从第二次迭代开始，我们没有看到任何显著的改善，合格率趋于饱和。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM feedback generators are not reliable – The LLM-based feedback generators,
    equipped with meticulously designed prompts with two-shots, still struggle with
    writing unerring feedback. Specifically, for faulty plans, these feedback generators
    cannot identify where the violation is or write excessive (baseless) feedback.
    Additionally, for qualified plans, they may generate false negative feedback which
    triggers the refinement module and causes unnecessary modification potentially
    leading to an invalid plan. As a result, the overall performance becomes stagnant,
    i.e., the Flat Ratio dominates, and modifications in a negative direction (downgrade
    ratio) have counteracted the positive changes (uplift ratio).
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 反馈生成器不可靠 —— 基于 LLM 的反馈生成器，尽管配备了精心设计的两次尝试的提示，仍然难以写出无误的反馈。特别是，对于有缺陷的计划，这些反馈生成器无法识别违规的具体位置，或生成过多（无根据的）反馈。此外，对于合格的计划，它们可能会产生虚假的负面反馈，这会触发修正模块，导致不必要的修改，可能会导致计划无效。因此，整体性能变得停滞不前，即，平坦比率主导，而负面修改（降级比率）抵消了积极的变化（提升比率）。
- en: 'RQ4: Can we enhance the development of a superior planner by employing our
    feedback-aware fine-tuning (FAFT) technique, as opposed to relying on off-the-shelf
    proprietary LLMs? For plan generation, we can use the Oracle feedback for in-context
    learning (as shown in RQ3) or fine-tuning an (open-source) LLM. The focus of this
    experiment lies in the latter aspect, where we examine the performance of SFT
    and FAFT in building the Planner agent.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: RQ4：我们能否通过使用我们关注反馈的微调（FAFT）技术，来提升优质规划器的开发，而不是依赖现成的专有 LLM？对于计划生成，我们可以使用 Oracle
    反馈进行上下文学习（如 RQ3 所示）或微调（开源）LLM。此实验的重点在于后者，我们将考察 SFT 和 FAFT 在构建 Planner agent 中的表现。
- en: SFT vs. FAFT – In our proposed approach, i.e., FAFT, we extend beyond the considerations
    of query, reference information, and annotated plan as in SFT, by also incorporating
    feedback into the fine-tuning process (Appx. [A.4](#A1.SS4 "A.4 Supervised Fine-Tuning
    and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). To generate
    feedback, we initially use the queries from the training set and prompt the Planner
    agent to generate plans⁵⁵5Under the same setting as in RQ1 and RQ2.. Subsequently,
    we gather feedback by evaluating the generated plans using the Oracle (i.e., the
    system). We set `temperature` to $1.0$ original annotated plans from the training
    set together with their `all-success` feedback⁶⁶6It is noteworthy that more samples
    could be collected. (Appx. [A.6.2](#A1.SS6.SSS2 "A.6.2 Feedback Examples Generated
    by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")). During
    inference, in the prompt, the feedback will be set to `all-success`, aiming to
    encourage the model to generate a correct plan. Appx. [A.4.3](#A1.SS4.SSS3 "A.4.3
    Inference Example Template for FAFT ‣ A.4 Supervised Fine-Tuning and Feedback-Aware
    Fine-Tuning ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") provides more information.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 与 FAFT —— 在我们提出的方法，即 FAFT 中，我们不仅考虑查询、参考信息和注释计划（如 SFT），还将反馈纳入微调过程（附录 [A.4](#A1.SS4
    "A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning ‣ Appendix A Appendix
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example")）。为了生成反馈，我们首先使用训练集中的查询，并提示 Planner agent 生成计划⁵⁵5在与 RQ1 和 RQ2 相同的设置下。随后，我们通过使用
    Oracle（即系统）评估生成的计划来收集反馈。我们将 `temperature` 设置为 $1.0$，将训练集中原始注释计划与它们的 `all-success`
    反馈一起使用⁶⁶6值得注意的是，可以收集更多样本。（附录 [A.6.2](#A1.SS6.SSS2 "A.6.2 Feedback Examples Generated
    by LLMs ‣ A.6 Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents
    to Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example")）。在推理过程中，在提示中，反馈将设置为
    `all-success`，旨在鼓励模型生成正确的计划。附录 [A.4.3](#A1.SS4.SSS3 "A.4.3 Inference Example Template
    for FAFT ‣ A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example") 提供了更多信息。
- en: Table [3](#S4.T3 "Table 3 ‣ 4 Findings ‣ Can We Rely on LLM Agents to Draft
    Long-Horizon Plans? Let’s Take TravelPlanner as an Example") presents the impact
    of FAFT where a significant improvement is witnessed across all pass rates, compared
    to Vanilla Llama-3-8B and its SFT version. This observation aligns with the previous
    studies, e.g., Negative-Aware Training (NAT) Wang et al. ([2024b](#bib.bib36)),
    that the performance can be boosted by increasing the diversity of prompts. In
    FAFT, elaborative and rich feedback acts as thought chains to improve the agent’s
    planning. Further, our results validate the recent works Lee et al. ([2023](#bib.bib12));
    Wei et al. ([2023](#bib.bib39)) by suggesting that (1) injecting auxiliary information
    in conventional SFT data can markedly improve the performance, and (2) a CoT-style
    training set and detailed scratchpads can significantly improve learning by reducing
    sample complexity.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [3](#S4.T3 "Table 3 ‣ 4 Findings ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") 展示了 FAFT 的影响，相较于 Vanilla Llama-3-8B
    及其 SFT 版本，所有通过率都有显著提高。这一观察与之前的研究相符，例如 Negative-Aware Training (NAT) Wang et al.
    ([2024b](#bib.bib36))，表明通过增加提示的多样性可以提升性能。在 FAFT 中，详细和丰富的反馈充当思维链条来改进代理的规划。此外，我们的结果验证了近期的研究
    Lee et al. ([2023](#bib.bib12)); Wei et al. ([2023](#bib.bib39))，表明（1）在传统 SFT
    数据中注入辅助信息可以显著提升性能，（2）CoT 风格的训练集和详细的草稿可以通过减少样本复杂性显著提升学习效果。
- en: Our findings advocate that when annotation is scarce while interaction with
    the system is affordable, collecting samples with comprehensive and rich feedback
    (either positive or negative), can be worthwhile. This approach can be seen as
    a promising alternative to RL-based solutions, such as PPO Schulman et al. ([2017](#bib.bib28)),
    which has been criticized for instability.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明，当注释稀缺而与系统交互成本可接受时，收集具有全面和丰富反馈（无论是正面还是负面）的样本是值得的。这种方法可以被视为对基于RL的解决方案（如PPO
    Schulman et al. ([2017](#bib.bib28))）的一种有前途的替代方案，因为后者因不稳定性而受到批评。
- en: '| Llama-3-8B as Planner |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-8B 作为规划器 |'
- en: '| Planner (RQ4) | Delivery Rate |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 规划器 (RQ4) | 交付率 |'
- en: '&#124; Commonsense &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 常识 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard Constraint &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 硬性约束 &#124;'
- en: '&#124; Pass Rate &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过率 &#124;'
- en: '| Final Pass Rate |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 最终通过率 |'
- en: '| Micro | Macro | Micro | Macro |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 微观 | 宏观 | 微观 | 宏观 |'
- en: '| Vanilla | 94.4 | 49.5 | 1.1 | 7.9 | 0.0 | 0.0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Vanilla | 94.4 | 49.5 | 1.1 | 7.9 | 0.0 | 0.0 |'
- en: '| + SFT | 97.8 | 64.2 | 11.1 | 12.4 | 6.1 | 3.9 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| + SFT | 97.8 | 64.2 | 11.1 | 12.4 | 6.1 | 3.9 |'
- en: '| + FAFT | 98.9 | 81.7 | 28.9 | 36.9 | 15.0 | 8.3 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| + FAFT | 98.9 | 81.7 | 28.9 | 36.9 | 15.0 | 8.3 |'
- en: 'Table 3: Performance of Llama-3-8B +SFT and +FAFT.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3: Llama-3-8B +SFT 和 +FAFT 的性能。'
- en: 5 Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we studied the impacts of context, the number of shots, and the
    utilization of feedback on a complex long-horizon planning task known as TravelPlanner.
    Our findings aim to advance a broader spectrum of agentic frameworks and strategies
    within the research community.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们研究了上下文、样本数量和反馈利用对复杂的长期规划任务（TravelPlanner）的影响。我们的发现旨在推动研究社区中更广泛的代理框架和策略。
- en: For future work, we plan to explore methods that incorporate annotated shots
    in SFT and post-training. This approach can address the bottleneck where LLMs’
    knowledge and skills are predominantly acquired during pre-training, while alignment
    SFT teaches the model which sub-distribution of formats to use when interacting
    with users Zhou et al. ([2024a](#bib.bib57)). Finally, we will explore the interplay
    between RLHF and FAFT.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未来的工作，我们计划探索在 SFT 和训练后融入注释样本的方法。这种方法可以解决 LLM 的知识和技能主要在预训练期间获得，而对齐 SFT 教授模型在与用户互动时使用哪种子分布格式
    Zhou et al. ([2024a](#bib.bib57)) 的瓶颈。最后，我们将探索 RLHF 和 FAFT 之间的相互作用。
- en: Limitations
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Due to budget constraints, we were only able to use GPT-3.5-Turbo as the Planner
    agent for RQ1 and RQ2\. For RQ4, further investigations are needed to explore
    the relationship between the magnitude of gains and the size of the FAFT training
    set, as well as the impact of the ratio of positive to negative samples on the
    final performance. Additionally, enhancing the feedback expressions could further
    improve the performance of FAFT. It would also be interesting to investigate RLHF
    techniques, such as DPO Rafailov et al. ([2024](#bib.bib26)) and PRO Song et al.
    ([2024a](#bib.bib32)), to better utilize feedback.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预算限制，我们只能使用 GPT-3.5-Turbo 作为 RQ1 和 RQ2 的 Planner 代理。对于 RQ4，需要进一步调查增益的大小与 FAFT
    训练集的规模之间的关系，以及正负样本比例对最终性能的影响。此外，增强反馈表达也可以进一步提高 FAFT 的性能。调查 RLHF 技术，如 DPO Rafailov
    et al. ([2024](#bib.bib26)) 和 PRO Song et al. ([2024a](#bib.bib32))，以更好地利用反馈也将是有趣的。
- en: Ethics Statement
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: Our work is founded upon TravelPlanner, a benchmark designed for complex planning
    tasks. We adhere to the original work’s specifications, utilizing their data,
    evaluation scripts, and definitions of commonsense. Acknowledging the foundational
    concepts and designs of the original benchmark, we strictly adhere to TravelPlanner’s
    guidelines, ensuring the integrity of the evaluation process by prohibiting any
    form of cheating in the validation and test sets. This commitment upholds the
    fairness and reliability of this work.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作基于 TravelPlanner，这是一个用于复杂规划任务的基准测试。我们遵循原始工作的规范，使用他们的数据、评估脚本和常识定义。承认原始基准的基础概念和设计，我们严格遵守
    TravelPlanner 的指南，通过禁止在验证和测试集中使用任何形式的作弊，确保评估过程的完整性。这一承诺维护了工作的公平性和可靠性。
- en: As for environmental cost, we acknowledge that our work necessitated extensive
    experiments to derive robust conclusions. However, future endeavours can leverage
    these insights, potentially reducing the need for numerous large-scale comparisons.
    Models intended for production could undergo training once, utilizing the most
    promising settings identified through our research.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关于环境成本，我们承认我们的工作需要进行广泛的实验以得出可靠的结论。然而，未来的工作可以利用这些见解，可能减少对大量大规模比较的需求。计划用于生产的模型可以进行一次训练，利用我们研究中确定的最有前景的设置。
- en: References
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A
    survey on evaluation of large language models. *ACM Transactions on Intelligent
    Systems and Technology*, 15(3):1–45.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. 大语言模型评估综述。*ACM
    智能系统与技术通讯*, 15(3):1–45。
- en: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    Improving code generation by training with natural language feedback. *arXiv preprint
    arXiv:2303.16749*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023a) Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander
    Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. 2023a.
    通过自然语言反馈训练提高代码生成。*arXiv 预印本 arXiv:2303.16749*。
- en: 'Chen et al. (2023b) Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, and Shunyu Yao. 2023b. Fireact: Toward language agent fine-tuning.
    *arXiv preprint arXiv:2310.05915*.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2023b) Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik
    Narasimhan, and Shunyu Yao. 2023b. Fireact: 面向语言代理的微调。*arXiv 预印本 arXiv:2310.05915*。'
- en: 'Chen et al. (2024) Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning
    Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and
    methods of effective agent tuning for large language models. *arXiv preprint arXiv:2403.12881*.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024) Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning
    Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: 设计有效的大语言模型代理调整的数据和方法。*arXiv
    预印本 arXiv:2403.12881*。'
- en: 'Christianos et al. (2023) Filippos Christianos, Georgios Papoudakis, Matthieu
    Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran,
    Xidong Feng, Jiacheng Liu, et al. 2023. Pangu-agent: A fine-tunable generalist
    agent with structured reasoning. *arXiv preprint arXiv:2312.14878*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Christianos et al. (2023) Filippos Christianos, Georgios Papoudakis, Matthieu
    Zimmer, Thomas Coste, Zhihao Wu, Jingxuan Chen, Khyati Khandelwal, James Doran,
    Xidong Feng, Jiacheng Liu, et al. 2023. Pangu-agent: 一种可微调的通用代理，具有结构化推理。*arXiv
    预印本 arXiv:2312.14878*。'
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: Towards a generalist agent for
    the web](http://arxiv.org/abs/2306.06070).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: 朝着一个通用代理迈进](http://arxiv.org/abs/2306.06070)。'
- en: Fei et al. (2023) Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng,
    and Wei Han. 2023. Extending context window of large language models via semantic
    compression. *arXiv preprint arXiv:2312.09571*.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei et al. (2023) Weizhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng,
    and Wei Han. 2023. 通过语义压缩扩展大型语言模型的上下文窗口。*arXiv 预印本 arXiv:2312.09571*。
- en: gkamradt (2023) gkamradt. 2023. Llmtest needle in a haystack - pressure testing
    llms. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gkamradt (2023) gkamradt. 2023. Llmtest 针对稻草堆中的针 - 压力测试 llms。 [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)。
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model
    based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. 基于大型语言模型的多代理：进展和挑战的综述。*arXiv
    预印本 arXiv:2402.01680*。
- en: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large
    language models. *arXiv preprint arXiv:2308.16137*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han et al. (2023) Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and
    Sinong Wang. 2023. Lm-infinite: 大型语言模型的简单实时长度泛化。*arXiv 预印本 arXiv:2308.16137*。'
- en: Kim et al. (2024) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. Language
    models can solve computer tasks. *Advances in Neural Information Processing Systems*,
    36.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2024) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024. 语言模型可以解决计算机任务。*神经信息处理系统进展*，36。
- en: Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee,
    and Dimitris Papailiopoulos. 2023. Teaching arithmetic to small transformers.
    *arXiv preprint arXiv:2307.03381*.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee,
    and Dimitris Papailiopoulos. 2023. 教授小型变换器算术。*arXiv 预印本 arXiv:2307.03381*。
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task,
    more tokens: the impact of input length on the reasoning performance of large
    language models. *arXiv preprint arXiv:2402.14848*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. 相同任务，多 tokens：输入长度对大型语言模型推理性能的影响。*arXiv
    预印本 arXiv:2402.14848*。
- en: 'Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2024. Camel: Communicative agents for" mind" exploration of
    large language model society. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2024) Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. 2024. Camel: 用于大型语言模型社会的“思维”探索的交流代理。*神经信息处理系统进展*，36。'
- en: 'Li et al. (2023) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun,
    Xinglin Wang, Heda Wang, and Kan Li. 2023. [Turning dust into gold: Distilling
    complex reasoning capabilities from llms by leveraging negative data](https://api.semanticscholar.org/CorpusID:266375154).
    In *AAAI Conference on Artificial Intelligence*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Bin Sun,
    Xinglin Wang, Heda Wang, and Kan Li. 2023. [将尘土变为黄金：利用负数据从 llms 中提取复杂推理能力](https://api.semanticscholar.org/CorpusID:266375154)。在*AAAI
    人工智能会议*上。
- en: 'Liu et al. (2023a) Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming
    Qian. 2023a. Tcra-llm: Token compression retrieval augmented large language model
    for inference cost reduction. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, pages 9796–9810.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023a) Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming
    Qian. 2023a. Tcra-llm: 令牌压缩检索增强型大型语言模型以降低推理成本。在*计算语言学协会发现：EMNLP 2023*，页 9796–9810。'
- en: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: Evaluating llms
    as agents. *arXiv preprint arXiv: 2308.03688*.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023b) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agentbench: 评估 llms 作为代理。*arXiv
    预印本 arXiv: 2308.03688*。'
- en: 'Liu et al. (2024) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. 2024.
    Agentlite: A lightweight library for building and advancing task-oriented llm
    agent system. *arXiv preprint arXiv:2402.15538*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2024) Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin
    Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, 等. 2024. Agentlite:
    一个轻量级库，用于构建和推动任务导向的语言模型代理系统。*arXiv 预印本 arXiv:2402.15538*。'
- en: 'Ma et al. (2024) Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang,
    Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An
    analytical evaluation board of multi-turn llm agents. *arXiv preprint arXiv:2401.13178*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2024) Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang,
    Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, 和 Junxian He. 2024. Agentboard: 一个多轮语言模型代理的分析评估板。*arXiv
    预印本 arXiv:2401.13178*。'
- en: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    等. 2024. Self-refine: 自反馈的迭代优化。*神经信息处理系统进展*，36。'
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *arXiv preprint arXiv:2303.08774*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [GPT-4 技术报告](https://arxiv.org/abs/2303.08774)。*arXiv
    预印本 arXiv:2303.08774*。
- en: Pan et al. (2024) Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, and Alane Suhr. 2024. Autonomous evaluation and refinement of digital
    agents. *arXiv preprint arXiv:2404.06474*.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2024) Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, 和 Alane Suhr. 2024. 数字代理的自主评估和优化。*arXiv 预印本 arXiv:2404.06474*。
- en: 'Paul et al. (2023) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback
    on intermediate representations. *arXiv preprint arXiv:2304.01904*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paul et al. (2023) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, 和 Boi Faltings. 2023. Refiner: 中间表示的推理反馈。*arXiv
    预印本 arXiv:2304.01904*。'
- en: Qian et al. (2024) Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia
    Zhou, Xu Chen, and Zhicheng Dou. 2024. Are long-llms a necessity for long-context
    tasks? *arXiv preprint arXiv:2405.15318*.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. (2024) Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Yujia
    Zhou, Xu Chen, 和 Zhicheng Dou. 2024. 长期语言模型在长期上下文任务中是否必不可少？*arXiv 预印本 arXiv:2405.15318*。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, 等. 2023. Toolllm: 促进大型语言模型掌握
    16000+ 真实世界 API。*arXiv 预印本 arXiv:2307.16789*。'
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher
    D Manning, Stefano Ermon, 和 Chelsea Finn. 2024. 直接偏好优化：你的语言模型实际上是一个奖励模型。*神经信息处理系统进展*，36。
- en: 'Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
    2023. [Parallel context windows for large language models](https://doi.org/10.18653/v1/2023.acl-long.352).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 6383–6402, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ratner et al. (2023) Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal
    Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, 和 Yoav Shoham.
    2023. [大型语言模型的并行上下文窗口](https://doi.org/10.18653/v1/2023.acl-long.352)。在 *第61届计算语言学协会年会论文集（第1卷：长篇论文）*，第
    6383–6402 页，多伦多，加拿大。计算语言学协会。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, 和 Oleg Klimov. 2017. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*。
- en: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models
    can be easily distracted by irrelevant context. In *International Conference on
    Machine Learning*, pages 31210–31227\. PMLR.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David
    Dohan, Ed Chi, Nathanael Schärli, 和 Denny Zhou. 2023. 大型语言模型容易被无关上下文分散注意力。载于 *国际机器学习大会*，第
    31210–31227 页。PMLR。
- en: 'Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan,
    和 Shunyu Yao. 2024. Reflexion：带有语言强化学习的语言代理。*神经信息处理系统进展*，第 36 卷。
- en: 'Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and
    embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shridhar et al. (2020) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, 和 Matthew Hausknecht. 2020. Alfworld：为互动学习对齐文本和具身环境。*arXiv
    预印本 arXiv:2010.03768*。
- en: Song et al. (2024a) Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang,
    Yongbin Li, and Houfeng Wang. 2024a. Preference ranking optimization for human
    alignment. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, pages 18990–18998.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2024a) Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang,
    Yongbin Li, 和 Houfeng Wang. 2024a. 人类对齐的偏好排序优化。载于 *AAAI 人工智能会议论文集*，第 38 卷，第 18990–18998
    页。
- en: 'Song et al. (2024b) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. 2024b. Trial and error: Exploration-based trajectory optimization
    for llm agents. *arXiv preprint arXiv:2403.02502*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2024b) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, 和 Bill
    Yuchen Lin. 2024b. 试错法：基于探索的轨迹优化用于 LLM 代理。*arXiv 预印本 arXiv:2403.02502*。
- en: 'Talebirad and Nadiri (2023) Yashar Talebirad and Amirhossein Nadiri. 2023.
    Multi-agent collaboration: Harnessing the power of intelligent llm agents. *arXiv
    preprint arXiv:2306.03314*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talebirad 和 Nadiri (2023) Yashar Talebirad 和 Amirhossein Nadiri. 2023. 多代理协作：利用智能
    LLM 代理的力量。*arXiv 预印本 arXiv:2306.03314*。
- en: Wang et al. (2024a) Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James
    Zou. 2024a. Mixture-of-agents enhances large language model capabilities. *arXiv
    preprint arXiv:2406.04692*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024a) Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, 和 James
    Zou. 2024a. Mixture-of-agents 提升大语言模型能力。*arXiv 预印本 arXiv:2406.04692*。
- en: 'Wang et al. (2024b) Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, and Timothy
    Baldwin. 2024b. Learning from failure: Integrating negative examples when fine-tuning
    large language models as agents. *arXiv preprint arXiv:2402.11651*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024b) Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, 和 Timothy
    Baldwin. 2024b. 从失败中学习：在微调大型语言模型作为代理时整合负面示例。*arXiv 预印本 arXiv:2402.11651*。
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2022. 自我一致性改善语言模型中的思维链推理。*arXiv
    预印本 arXiv:2203.11171*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 2022. 思维链提示在大型语言模型中引发推理。*神经信息处理系统进展*，第
    35 卷：24824–24837。
- en: 'Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. 2023. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, 和 Lingming
    Zhang. 2023. Magicoder：源代码就是你所需的一切。*arXiv 预印本 arXiv:2312.02120*。
- en: 'Wu et al. (2023a) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun
    Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a. Autogen:
    Enabling next-gen llm applications via multi-agent conversation framework. *arXiv
    preprint arXiv:2308.08155*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023a) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. 2023a. Autogen：通过多代理对话框架实现下一代
    LLM 应用。*arXiv 预印本 arXiv:2308.08155*。
- en: Wu et al. (2024) Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and
    Yanghua Xiao. 2024. How easily do irrelevant inputs skew the responses of large
    language models? *arXiv preprint arXiv:2404.03302*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2024) Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, 和
    Yanghua Xiao. 2024. 无关输入如何轻易偏斜大型语言模型的回应？*arXiv 预印本 arXiv:2404.03302*。
- en: Wu et al. (2023b) Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu,
    Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b. An empirical
    study on challenging math problem solving with gpt-4. In *ArXiv preprint arXiv:2306.01337*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等人（2023b）吴怡然、贾飞然、张绍坤、李航宇、朱尔康、王越、李银达、彭理查、吴清云、王池。2023b。关于使用gpt-4解决挑战性数学问题的实证研究。发表于*ArXiv预印本arXiv:2306.01337*。
- en: Xi et al. (2024a) Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng,
    Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, et al. 2024a. Training
    large language models for reasoning through reverse curriculum reinforcement learning.
    *arXiv preprint arXiv:2402.05808*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奚等人（2024a）奚智恒、陈文翔、洪博洋、金森杰、郑睿、何伟、丁一文、刘世春、郭鑫、王军哲等。2024a。通过反向课程强化学习训练大型语言模型进行推理。*arXiv预印本arXiv:2402.05808*。
- en: 'Xi et al. (2024b) Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin
    Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao,
    Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang,
    Zuxuan Wu, and Yu-Gang Jiang. 2024b. [Agentgym: Evolving large language model-based
    agents across diverse environments](http://arxiv.org/abs/2406.04151).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奚等人（2024b）奚智恒、丁一文、陈文翔、洪博洋、郭洪林、王军哲、杨丁文、廖辰阳、郭鑫、何伟、高松阳、陈璐、郑睿、邹义诚、桂涛、张琦、邱希鹏、黄轩景、吴祖轩、姜玉刚。2024b。
    [Agentgym：在多样化环境中进化的基于大型语言模型的代理](http://arxiv.org/abs/2406.04151)。
- en: 'Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou,
    Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world
    planning with language agents. *arXiv preprint arXiv:2402.01622*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等人（2024）谢剑、张凯、陈江杰、朱婷辉、楼仁泽、田宇东、肖杨华、苏宇。2024。Travelplanner：基于语言代理的现实世界规划基准。*arXiv预印本arXiv:2402.01622*。
- en: Xie and Min (2022) Sang Michael Xie and Sewon Min. 2022. How does in-context
    learning work? a framework for understanding the differences from traditional
    supervised learning.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢和敏（2022）谢桑·迈克尔和敏修文。2022。上下文学习是如何工作的？理解与传统监督学习差异的框架。
- en: 'Yang et al. (2023) Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang,
    Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. *arXiv
    preprint arXiv:2312.13771*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人（2023）杨兆、刘佳轩、韩育成、陈欣、黄泽标、傅斌、余刚。2023。Appagent：作为智能手机用户的多模态代理。*arXiv预印本arXiv:2312.13771*。
- en: 'Yao et al. (2022a) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022a. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2022a）姚顺宇、陈浩、杨军、纳拉斯曼。2022a。Webshop：迈向具有扎根语言代理的可扩展现实世界网页互动。*神经信息处理系统进展*，35:20744–20757。
- en: 'Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2024）姚顺宇、于点、赵杰弗里、沙夫兰、格里菲斯、曹远、纳拉斯曼。2024。思维之树：利用大型语言模型进行深思熟虑的问题解决。*神经信息处理系统进展*，36。
- en: 'Yao et al. (2022b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2022b. React: Synergizing reasoning and acting
    in language models. *arXiv preprint arXiv:2210.03629*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2022b）姚顺宇、赵杰弗里、于点、杜楠、沙夫兰、纳拉斯曼、曹远。2022b。React：在语言模型中协同推理和行动。*arXiv预印本arXiv:2210.03629*。
- en: 'Zeng et al. (2023) Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao
    Dong, and Jie Tang. 2023. Agenttuning: Enabling generalized agent abilities for
    llms. *arXiv preprint arXiv:2310.12823*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曾等人（2023）曾敖寒、刘铭道、卢瑞、王博文、刘晓、董宇霄、唐洁。2023。Agenttuning：为LLMs启用通用代理能力。*arXiv预印本arXiv:2310.12823*。
- en: Zhang et al. (2024a) Cong Zhang, Deik Derrick Goh Xin, Dexun Li, Hao Zhang,
    and Yong Liu. 2024a. Meta-task planning for language agents. *arXiv preprint arXiv:2405.16510*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2024a）张聪、戚德克·戚欣、李德勋、张浩、刘永。2024a。语言代理的元任务规划。*arXiv预印本arXiv:2405.16510*。
- en: 'Zhang et al. (2024b) Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran
    Yao, Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, et al. 2024b.
    Agentohana: Design unified data and training pipeline for effective agent learning.
    *arXiv preprint arXiv:2402.15506*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2024b）张建国、兰天、穆提什·穆尔提、刘志伟、姚伟然、谭军涛、黄泰、杨良伟、冯一豪、刘祖鑫等。2024b。Agentohana：设计统一的数据和训练流程以有效地学习代理。*arXiv预印本arXiv:2402.15506*。
- en: Zhang et al. (2024c) Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi
    Wang, Ranjay Krishna, and Qingyun Wu. 2024c. Training language model agents without
    modifying language models. *ICML’24*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024c) Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi
    Wang, Ranjay Krishna, 和 Qingyun Wu. 2024c. 在不修改语言模型的情况下训练语言模型代理。*ICML’24*。
- en: 'Zhao et al. (2024) Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao
    Gui, Qi Zhang, and Xuanjing Huang. 2024. Longagent: Scaling language models to
    128k context through multi-agent collaboration. *arXiv preprint arXiv:2402.11550*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2024) Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao
    Gui, Qi Zhang, 和 Xuanjing Huang. 2024. Longagent: 通过多代理协作将语言模型扩展到 128k 上下文。*arXiv
    预印本 arXiv:2402.11550*。'
- en: Zheng et al. (2024) Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su.
    2024. Gpt-4v (ision) is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024) Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, 和 Yu Su.
    2024. Gpt-4v (ision) 是一个通用的网络代理，如果是基于实际的。*arXiv 预印本 arXiv:2401.01614*。
- en: 'Zhou et al. (2024a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,
    Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2024a. Lima:
    Less is more for alignment. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2024a) Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,
    Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, 等. 2024a. Lima:
    对齐的**少即是多**。*神经信息处理系统的进展*，36。'
- en: 'Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023.
    [Webarena: A realistic web environment for building autonomous agents](https://webarena.dev).
    *arXiv preprint arXiv:2307.13854*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2023) Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, 等. 2023.
    [Webarena: 构建自主代理的现实网络环境](https://webarena.dev)。*arXiv 预印本 arXiv:2307.13854*。'
- en: 'Zhou et al. (2024b) Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and
    Aviral Kumar. 2024b. Archer: Training language model agents via hierarchical multi-turn
    rl. *arXiv preprint arXiv:2402.19446*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2024b) Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, 和
    Aviral Kumar. 2024b. Archer: 通过层次化多回合强化学习训练语言模型代理。*arXiv 预印本 arXiv:2402.19446*。'
- en: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al.
    2023. Promptbench: Towards evaluating the robustness of large language models
    on adversarial prompts. *arXiv preprint arXiv:2306.04528*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, 等. 2023.
    Promptbench: 评估大型语言模型在对抗性提示下的鲁棒性。*arXiv 预印本 arXiv:2306.04528*。'
- en: Appendix A Appendix
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Dataset
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数据集
- en: 'The TravelPlanner dataset⁷⁷7TravelPlanner Dataset: [https://huggingface.co/datasets/osunlp/TravelPlanner](https://huggingface.co/datasets/osunlp/TravelPlanner)
    consists of three splits of training, validation, and test sets as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'TravelPlanner 数据集⁷⁷7TravelPlanner 数据集: [https://huggingface.co/datasets/osunlp/TravelPlanner](https://huggingface.co/datasets/osunlp/TravelPlanner)
    包含以下三个拆分的训练、验证和测试集：'
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Training Set consists of $45$ triplets of query, reference, and human annotated
    plan. The annotations are used as demonstrations for in-context learning or supervised
    fine-tuning in our paper. Please note that these annotated plans are merely a
    subset of many feasible plans. As expected, the Oracle (i.e., system) returns
    the feedback for the annotations where no issue is raised (Appx. [A.6.2](#A1.SS6.SSS2
    "A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")).
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练集由 $45$ 个查询、参考和人工注释计划的三元组组成。这些注释作为我们论文中上下文学习或监督微调的示例。请注意，这些注释计划仅是许多可行计划的一个子集。正如预期的那样，Oracle（即系统）对没有提出问题的注释返回反馈（附录
    [A.6.2](#A1.SS6.SSS2 "A.6.2 由LLMs生成的反馈示例 ‣ A.6 案例展示 ‣ 附录 A 附录 ‣ 我们可以依赖LLM代理来制定长期计划吗？以TravelPlanner为例")）。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Validation Set comes with $180$ pairs of query and reference, with no annotated
    plans.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证集包含 $180$ 对查询和参考，没有注释计划。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The Test Set holds $1,000$ queries together with their references, without any
    annotated plans.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试集包含 $1,000$ 个查询及其参考，没有任何注释计划。
- en: For a given query, agents are expected to formulate a (comprehensive) plan which
    includes transportation, restaurants, attractions, and accommodation for each
    day (Appx. [A.6.1](#A1.SS6.SSS1 "A.6.1 Query Example with its Travel Plan ‣ A.6
    Case Presentation ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example") shows an example).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的查询，代理需要制定一个（全面的）计划，包括每天的交通、餐馆、景点和住宿（附录 [A.6.1](#A1.SS6.SSS1 "A.6.1 查询示例及其旅行计划
    ‣ A.6 案例展示 ‣ 附录 A 附录 ‣ 我们能否依赖 LLM 代理来制定长期计划？以 TravelPlanner 为例") 显示了一个示例）。
- en: A.2 Evaluation metrics
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评估指标
- en: Following TravelPlanner, we use automatic evaluation metrics to assess whether
    a plan generated by the agent meets the (correct) format condition as well as
    all the constraints.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随 TravelPlanner，我们使用自动评估指标来评估代理生成的计划是否符合（正确的）格式条件以及所有约束。
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Delivery Rate measures whether the agent could successfully generate a plan
    within a limited number of steps. Falling into any dead loops or invalid plan
    formats leads to failure. In the sole-planning setting, any failure in drafting
    a plan negatively impacts the delivery rate.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交付率衡量代理是否能够在有限的步骤内成功生成一个计划。陷入任何死循环或无效的计划格式都会导致失败。在单独规划的设置中，任何制定计划的失败都会对交付率产生负面影响。
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Commonsense Constraint Pass Rate assesses whether the agent can incorporate
    commonsense while drafting plans without explicit instructions. For example, the
    agent has to pick valid entities (incl. restaurants, hotels, etc.) from the reference
    information and not hallucinate.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 常识约束通过率评估代理在制定计划时是否能够结合常识，而无需明确的指示。例如，代理需要从参考信息中选择有效的实体（包括餐馆、酒店等），而不是凭空想象。
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hard Constraint Pass Rate measures whether a plan meets all hard constraints
    mentioned in the query, e.g., budget limit, cuisine preference, or accommodation
    type.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 硬约束通过率衡量一个计划是否满足查询中提到的所有硬性约束，例如预算限制、餐饮偏好或住宿类型。
- en: N.B. For Commonsense and Hard Constraint Pass Rates, the evaluation is done
    in two ways, Micro and Macro, which evaluate the agent’s capability of following
    individual constraints vs. all the constraints holistically Xie et al. ([2024](#bib.bib45)).
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：对于常识和硬约束通过率，评估有两种方式：微观和宏观，分别评估代理遵循个别约束与全面约束的能力 Xie et al. ([2024](#bib.bib45))。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Final Pass Rate measures whether a plan satisfies all hard and commonsense constraints.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终通过率衡量一个计划是否满足所有硬性和常识约束。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hallucination Rate measures whether a plan contains entities that cannot be
    found in the reference information.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 幻觉率衡量一个计划是否包含在参考信息中找不到的实体。
- en: 'TravelPlanner’s Leaderboard⁸⁸8TravelPlanner Leaderboard: [https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard](https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard)
    let us evaluate the performance of agents against both validation and test sets
    online. This creates a stage for fair evaluation for all researchers. We use this
    leaderboard to calculate the figures for the validation and test sets for our
    experiments. We run each five times, with a different random seed, and report
    the average scores.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 'TravelPlanner 排行榜⁸⁸8TravelPlanner Leaderboard: [https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard](https://huggingface.co/spaces/osunlp/TravelPlannerLeaderboard)
    使我们能够在线评估代理在验证集和测试集上的表现。这为所有研究人员提供了一个公平评估的舞台。我们使用这个排行榜来计算实验的验证集和测试集的数据。我们运行每个实验五次，使用不同的随机种子，并报告平均分数。'
- en: '![Refer to caption](img/1d5f5a88017206862b1eb60709905cf6.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1d5f5a88017206862b1eb60709905cf6.png)'
- en: 'Figure A.1: Toy Example: The Planner initially generates a plan w.r.t. query
    and reference, then the feedback generator generates feedback considering the
    commonsense constraints. Then, the Refiner modifies the plan to meet the requirements
    for all constraints.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 A.1：示例：规划者最初生成一个与查询和参考信息相关的计划，然后反馈生成器根据常识约束生成反馈。接着，优化者修改计划以满足所有约束的要求。
- en: A.3 Framework
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 框架
- en: In Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Can We Rely on LLM Agents to
    Draft Long-Horizon Plans? Let’s Take TravelPlanner as an Example"), we show that
    the Planner agent generates a plan for a given query and (cleaned) reference information.
    In TravelPlanner’s Two-Staging setting, the reference information is collected
    by an upstream tool agent which gathers valid information related to transportation,
    dining, attractions, and accommodation from their corresponding source files.
    The original benchmark also particularly creates valid reference information for
    the Sole Planning setting where the focus is on the Planner agent. Hence, we evaluate
    our solution only in the Sole Planning setting since our focus is on planning.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[1](#S1.F1 "图1 ‣ 1 介绍 ‣ 我们可以依赖LLM代理来制定长远计划吗？以TravelPlanner为例")中，我们展示了规划代理为给定的查询和（已清理的）参考信息生成计划。在TravelPlanner的双阶段设置中，参考信息由上游工具代理收集，该代理从相关的源文件中收集有关交通、餐饮、景点和住宿的有效信息。原始基准还特别创建了适用于单一规划设置的有效参考信息，其中重点是规划代理。因此，我们仅在单一规划设置中评估我们的解决方案，因为我们关注的是规划。
- en: A.3.1 The Scrubber Agent
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.1 清理代理
- en: Since the reference information is massive and lengthy (i.e., $10,000$, after
    removing the hotels whose prices are above this limit, there are still other choices
    left for the Planner agent to reason and draft a plan to meet the budget and other
    constraints. The prompt for the Scrubber agent is found in Appx. [A.5.1](#A1.SS5.SSS1
    "A.5.1 The Scrubber’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example").
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于参考信息庞大且冗长（即，$10,000$，在去除价格超过该限制的酒店后，仍然有其他选择供规划代理进行推理并制定符合预算和其他约束的计划。清理代理的提示见附录[A.5.1](#A1.SS5.SSS1
    "A.5.1 清理者的提示模板 ‣ A.5 代理提示模板 ‣ 附录A 附录 ‣ 我们可以依赖LLM代理来制定长远计划吗？以TravelPlanner为例")。
- en: A.3.2 The Feedback Generator and Refiner
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.3.2 反馈生成器和精炼者
- en: Once the original plan has been drafted, refinement is conducted in an iterative
    manner. For this, we follow previous works where two agents are separately created
    with natural language communication capabilities.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦原始计划草案完成，精炼将以迭代的方式进行。为此，我们遵循以往的工作，其中两个代理被分别创建，具有自然语言通信能力。
- en: The Feedback Generator which is responsible for generating nuanced task-dependent
    feedback that addresses multiple constraints. We tailor a prompt, as shown in
    Appx. [A.5.2](#A1.SS5.SSS2 "A.5.2 Feedback Generator’s Prompt ‣ A.5 Prompt Templates
    for Agents ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon
    Plans? Let’s Take TravelPlanner as an Example"), to ask LLMs to write feedback
    with regard to commonsense constraints. In the instructions, we provide a list
    of constraints with their descriptions. Here two-shots are used to help with feedback
    generation. The shots are randomly selected from the training set.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈生成器负责生成细致的、依赖任务的反馈，以满足多重约束。我们根据附录中的示例[A.5.2](#A1.SS5.SSS2 "A.5.2 反馈生成器的提示
    ‣ A.5 代理提示模板 ‣ 附录A 附录 ‣ 我们可以依赖LLM代理来制定长远计划吗？以TravelPlanner为例")定制提示，以要求LLMs针对常识性约束编写反馈。在指令中，我们提供了约束及其描述的列表。这里使用了两次示例来帮助生成反馈。这些示例是从训练集中随机选择的。
- en: The Refiner Agent refines the generated plan based on the feedback received
    from the Feedback Generator towards a better version (see the prompt in Appx. [A.5.3](#A1.SS5.SSS3
    "A.5.3 The Refiner’s Prompt Template ‣ A.5 Prompt Templates for Agents ‣ Appendix
    A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take
    TravelPlanner as an Example")).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼代理根据从反馈生成器收到的反馈，进一步精炼生成的计划，以得到更好的版本（见附录中的提示[A.5.3](#A1.SS5.SSS3 "A.5.3 精炼者的提示模板
    ‣ A.5 代理提示模板 ‣ 附录A 附录 ‣ 我们可以依赖LLM代理来制定长远计划吗？以TravelPlanner为例")）。
- en: Fig. [A.1](#A1.F1 "Figure A.1 ‣ A.2 Evaluation metrics ‣ Appendix A Appendix
    ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let’s Take TravelPlanner
    as an Example") illustrates the entire refinement phase. The feedback points out
    that there is a repeated attraction for Days $1$, and the accommodation does not
    satisfy the minimum number of nights requirement. Then, the Refiner agent refines
    this draft plan into a new plan where the attraction for the first day is replaced
    to avoid repetition, and another hotel is chosen which allows a two-night stay.
    Finally, based on the system assessment, the refined plan meets all commonsense
    constraints.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [A.1](#A1.F1 "图 A.1 ‣ A.2 评估指标 ‣ 附录 A 附录 ‣ 我们可以依赖 LLM 代理来制定长期计划吗？以 TravelPlanner
    为例") 展示了整个细化阶段。反馈指出第$1$天的景点有重复，并且住宿不满足最低夜数要求。然后，Refiner 代理将这个草拟计划细化为一个新的计划，其中第一天的景点被替换以避免重复，选择了另一家允许两晚住宿的酒店。最后，根据系统评估，细化后的计划符合所有常识约束。
- en: A.4 Supervised Fine-Tuning and Feedback-Aware Fine-Tuning
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 监督微调和反馈感知微调
- en: A.4.1 Training Example Template for SFT
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.1 SFT 训练示例模板
- en: The TravelPlanner training set consists of $45$ samples with annotated plans.
    We use reference information, queries, and annotated plans for general SFT (which
    is a baseline).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TravelPlanner 训练集包含$45$个带有注释的计划样本。我们使用参考信息、查询和注释计划来进行一般的 SFT（这是基线）。
- en: 'reference  information  box:  {ref}query:  {query}draft  travel  plan:  {plan}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 参考信息框：{ref}查询：{query}草拟旅行计划：{plan}
- en: A.4.2 Training Example Template for FAFT
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.2 FAFT 训练示例模板
- en: reference  information  box:{ref}query:{query}feedback:{feedback}draft  travel  plan:{plan}
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 参考信息框：{ref}查询：{query}反馈：{feedback}草拟旅行计划：{plan}
- en: A.4.3 Inference Example Template for FAFT
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.3 FAFT 推理示例模板
- en: 'reference  information  box:{ref}query:{query}feedback:{feedback}is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  successdraft  travel  plan:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 参考信息框：{ref}查询：{query}反馈：{feedback}是否合理访问城市：成功是否有效餐厅：成功是否有效景点：成功是否有效住宿：成功是否有效交通：成功是否有效当前城市信息：成功是否有效沙箱信息：成功是否缺失：成功草拟旅行计划：
- en: A.4.4 Fine-tuning Setup
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.4.4 微调设置
- en: In RQ4, for the Planner agent, we fine-tune Llama3-8B for $3$.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RQ4 中，对于 Planner 代理，我们对 Llama3-8B 进行了$3$次微调。
- en: A.5 Prompt Templates for Agents
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 代理的提示模板
- en: A.5.1 The Scrubber’s Prompt Template
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.1 Scrubber 的提示模板
- en: Can  you  assist  in  creating  a  5-day  travel  itinerary  starting  in  Sacramento  and  covering  2  cities  in  Washington  state  from  March  22nd  to  March  26th,  2022?  The  journey  will  be  for  a  group  of  three  with  a  budget  of  $3,600.  We  require  accommodations  that  provide  entire  rooms  and  do  not  plan  to  travel  by  flight.  As  far  as  cuisines  are  concerned,  we’d  love  to  experience  American,  Mediterranean,  Italian,  and  French  during  our  trip.===>  [’American’,  ’Mediterranean’,  ’Italian’,  ’French’]Can  you  help  with  generating  a  7-day  travel  plan  for  a  party  of  5?  We’re  setting  off  from  Indianapolis  and  planning  to  explore  3  cities  in  Colorado  from  March  11th  to  March  17th,  2022.  We  have  a  budget  of  $15,100  for  this  trip.  We’ll  be  bringing  our  pets,  so  pet-friendly  accommodations  are  a  must.  We’re  also  hoping  to  find  places  that  offer  Mexican,  Italian,  Mediterranean,  and  Indian  cuisines.  Entire  rooms  for  accommodations  would  be  ideal.===>  [’Mexican’,  ’Italian’,  ’Mediterranean’,  ’Indian’]Can  you  assist  in  creating  a  travel  itinerary  for  a  group  of  4,  starting  in  Seattle  and  visiting  3  unique  cities  across  Texas?  This  trip  will  span  over  7  days  from  March  10th  through  March  16th,  2022.  We  have  a  budget  of  $11,000.  Regarding  our  accommodations,  we  would  like  to  rent  entire  rooms,  and  it’s  important  that  our  lodgings  allow  parties.  As  for  transportation,  we  do  not  plan  to  drive  ourselves  around.===>  []...{45  shots  from  trainset}...I  need  your  help  to  plan  a  5-day  vacation  for  a  group  of  4  people.  We’re  departing  from  Honolulu  and  planning  to  visit  2  cities  in  California  from  March  19th  to  March  23rd,  2022.  The  budget  for  our  trip  is  $11,200.  For  food  preferences,  we  enjoy  Mediterranean  and  Mexican  dishes.===>{inference  for  the  cuisine  preference}
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 您能帮助制定一个为期5天的旅行计划吗？行程从萨克拉门托开始，涵盖华盛顿州的两个城市，时间为2022年3月22日至3月26日。此次旅行将有三人同行，预算为3,600美元。我们需要提供整间房间的住宿，不打算乘飞机旅行。在美食方面，我们希望在旅途中体验**美式**、**地中海**、**意大利**和**法式**菜肴。===>
    [’美式’， ’地中海’， ’意大利’， ’法式’]您能帮助制定一个为期7天的旅行计划吗？我们从印第安纳波利斯出发，计划在2022年3月11日至3月17日之间探索科罗拉多州的三个城市。我们的预算为15,100美元。我们会带上宠物，因此必须找到宠物友好的住宿。我们还希望找到提供**墨西哥**、**意大利**、**地中海**和**印度**菜肴的地方。提供整间房间的住宿将是理想的选择。===>
    [’墨西哥’， ’意大利’， ’地中海’， ’印度’]您能帮助制定一个为期7天的旅行计划吗？我们将从西雅图出发，访问德克萨斯州的三个独特城市。此行程从2022年3月10日到3月16日，共7天。我们的预算为11,000美元。关于住宿，我们希望租用整间房间，并且住宿地需要允许聚会。至于交通，我们不打算自己开车。===>
    []...{45 shots from trainset}...我需要您的帮助来规划一个为期5天的假期，旅行团体为4人。我们将从檀香山出发，计划在2022年3月19日至3月23日之间访问加利福尼亚州的两个城市。我们的预算为11,200美元。至于餐饮偏好，我们喜欢**地中海**和**墨西哥**菜肴。===>{对餐饮偏好的推断}
- en: A.5.2 Feedback Generator’s Prompt
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.2 反馈生成器的提示
- en: 'Now  You  are  an  advanced  reasoning,  analyzing  and  advisory  agent  who  can  write  feedback  and  insights  for  a  given  draft  travel  plan,  based  on  the  given  query  and  reference  information  box.The  feedback  you  write  should  check  and  judge  if  the  given  draft  travel  plan  violates  one  or  several  following  constraints:*  is_reasonalbe_visiting_city:  {success  or  fail}.  This  refers  to  Reasonable  City  Route:  Changes  in  cities  during  the  trip  must  be  reasonable.*  is_valid_restaurants:  {success  or  fail}.  This  refers  to  Diverse  Restaurants:  Restaurant  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_attractions:  {success  or  fail}.  This  refers  to  Diverse  Attractions:  Attraction  choices  should  not  be  repeated  throughout  the  trip.*  is_valid_accommodation:  {success  or  fail}.  This  refers  to  Minimum  Nights  Stay:  The  number  of  consecutive  days  spent  in  a  specific  accommodation  during  the  trip  must  meet  the  corresponding  required  minimum  number  of  nights’  stay.*  is_valid_transportation:  {success  or  fail}.  This  refers  to  No  conflict  Transportation:  Transportation  choices  within  the  trip  must  be  reasonable.  For  example,  having  both  "self-driving"  and  "flight"  would  be  considered  a  conflict.*  is_valid_information_in_current_city:  {success  or  fail}.  This  refers  to  Within  Current  City:  All  scheduled  activities  for  the  day  must  be  located  within  that  day’s  city(s).*  is_valid_information_in_sandbox:  {success  or  fail}.  This  refers  to  Within  Sandbox:  All  information,  such  as  restaurants,  attractions,  accommodations  and  transportation,  in  the  plan,  must  be  within  the  closed  sandbox  (reference  information  box);  otherwise,  it  will  be  considered  a  hallucination.*  is_not_absent:  {success  or  fail}.  This  refers  to  Complete  Information:  No  key  information  should  be  left  out  of  the  plan,  such  as  the  lack  of  accommodation  during  travel.Here  are  some  examples  for  your  information  as  demonstrations:*****  Example  Starts  *****reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}-------------------------------------reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:{feedback}*****  Example  Ends  *****Now,  You  should  write  the  feedback  with  regard  to  the  aspect  of  constraints  shown  above.  Follow  the  formats  shown  in  the  examples  above.reference  information  box:{ref}query:{query}draft  travel  plan:{plan}feedback:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你是一个高级推理、分析和顾问代理，能够根据给定的查询和参考信息框，为给定的旅行计划草案撰写反馈和见解。你撰写的反馈应该检查并判断给定的旅行计划草案是否违反了以下一个或多个约束条件：*is_reasonable_visiting_city*：{成功或失败}。这指的是合理的城市路线：旅行中的城市变更必须是合理的。*is_valid_restaurants*：{成功或失败}。这指的是多样化的餐馆：餐馆选择在整个旅行中不应重复。*is_valid_attractions*：{成功或失败}。这指的是多样化的景点：景点选择在整个旅行中不应重复。*is_valid_accommodation*：{成功或失败}。这指的是最少住宿天数：在旅行中，在特定住宿地点的连续天数必须符合相应的最少住宿要求。*is_valid_transportation*：{成功或失败}。这指的是交通选择无冲突：旅行中的交通选择必须合理。例如，同时使用“自驾”和“航班”将被视为冲突。*is_valid_information_in_current_city*：{成功或失败}。这指的是在当前城市内：当天安排的所有活动必须位于当天的城市中。*is_valid_information_in_sandbox*：{成功或失败}。这指的是在沙盒内：计划中的所有信息，如餐馆、景点、住宿和交通，必须在封闭的沙盒（参考信息框）内；否则，将被视为幻觉。*is_not_absent*：{成功或失败}。这指的是完整信息：计划中不应遗漏任何关键信息，例如旅行中缺少住宿。以下是一些示例供您参考：*****
    示例开始 *****参考信息框：{ref}查询：{query}旅行计划草案：{plan}反馈：{feedback}-------------------------------------参考信息框：{ref}查询：{query}旅行计划草案：{plan}反馈：{feedback}*****
    示例结束 *****现在，您应该根据上述约束条件撰写反馈。请遵循上述示例中显示的格式。参考信息框：{ref}查询：{query}旅行计划草案：{plan}反馈：
- en: A.5.3 The Refiner’s Prompt Template
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.5.3 提炼者的提示模板
- en: 'You  are  a  proficient  planner.  Based  on  the  provided  information  and  query,  please  give  me  a  detailed  plan,  including  specifics  such  as  flight  numbers  (e.g.,  F0123456),  restaurant  names,  and  accommodation  names.  Note  that  all  the  information  in  your  plan  should  be  derived  from  the  provided  data.  You  must  adhere  to  the  format  given  in  the  example.  Additionally,  all  details  should  align  with  commonsense.  The  symbol  ’-’  indicates  that  information  is  unnecessary.  For  example,  in  the  provided  sample,  you  do  not  need  to  plan  after  returning  to  the  departure  city.  When  you  travel  to  two  cities  in  one  day,  you  should  note  it  in  the  ’Current  City’  section  as  in  the  example  (i.e.,  from  A  to  B).*****  Example  *****Query:  Could  you  create  a  travel  plan  for  7  people  from  Ithaca  to  Charlotte  spanning  3  days,  from  March  8th  to  March  14th,  2022,  with  a  budget  of  $30,200?Travel  Plan:Day  1:Current  City:  from  Ithaca  to  CharlotteTransportation:  Flight  Number:  F3633413,  from  Ithaca  to  Charlotte,  Departure  Time:  05:38,  Arrival  Time:  07:46Breakfast:  Nagaland’s  Kitchen,  CharlotteAttraction:  The  Charlotte  Museum  of  History,  CharlotteLunch:  Cafe  Maple  Street,  CharlotteDinner:  Bombay  Vada  Pav,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  2:Current  City:  CharlotteTransportation:  -Breakfast:  Olive  Tree  Cafe,  CharlotteAttraction:  The  Mint  Museum,  Charlotte;  Romare  Bearden  Park,  Charlotte.Lunch:  Birbal  Ji  Dhaba,  CharlotteDinner:  Pind  Balluchi,  CharlotteAccommodation:  Affordable  Spacious  Refurbished  Room  in  Bushwick!,  CharlotteDay  3:Current  City:  from  Charlotte  to  IthacaTransportation:  Flight  Number:  F3786167,  from  Charlotte  to  Ithaca,  Departure  Time:  21:42,  Arrival  Time:  23:26Breakfast:  Subway,  CharlotteAttraction:  Books  Monument,  Charlotte.Lunch:  Olive  Tree  Cafe,  CharlotteDinner:  Kylin  Skybar,  CharlotteAccommodation:  -*****  Example  Ends  *****Given  information:  {reference  information  box}Query:  {query}Travel  Plan:  {original  draft  travel  plan}Now  You  are  an  advanced  reasoning  and  self-corrective  agent  that  can  improve  based  on  self  refection  and  the  feedback.Based  on  given  query,  reference  information  box,  and  proposed  Travel  Plan,  above,  you  are  now  given  the  feedback  which  includes  the  reason  why  it  fails.Try  to  write  a  new  plan  in  which  the  errors  are  fixed.Keep  in  mind  that  you  only  make  changes  or  replace  the  item  which  causes  the  issue.If  it  appears  at  multiple  places,  correct  them  all  at  once.Try  to  avoid  making  unnecessary  changes  on  the  previous  proposed  plan.Always  make  sure  that  your  generation,  such  as  the  names  of  resuturants,  attractions,  accommodations,  transportations,  can  be  found  in  the  given  reference  information  box  above.For  attraction,  breakfast,  dinner  and  lunch,  do  not  give  repetition  within  each  day  and  among  the  days  in  the  plan,  i.e.  each  of  them  should  NOT  appear  more  than  once  in  the  whole  travel  plan.Feel  free  to  ignore  irrelevant  information  in  reference  information  box.*  If  the  feedback  is  about  repeated  restaurant,  for  example,  "The  restaurant  in  day  4  dinner  is  repeated.",  then  you  need  to  take  another  restuartant  from  reference  infobox,  which  is  different  from  the  previous  one  and  all  other  chosen  ones  in  the  plan;*  If  the  feedback  is  about  "The  breakfast/lunch/dinner/attraction/accommodation  in  day  X  is  invalid  in  the  sandbox",  for  example,  "The  lunch  in  day  3  is  invalid  in  the  sandbox.",  this  means  that  the  choice  cannot  be  found  in  reference  infobox.  Then  you  should  take  another  one  which  is  definitely  within  the  inference  infobox.*  If  the  feedback  is  about  "The  accommodation  X  do  not  obey  the  minumum  nights  rule",  this  means  that  the  total  days/nights  spent  in  the  accommodation  place  chosen  in  the  plan,  does  not  obey  the  minumum  nights  rule.For  example,  if  the  days  spent  in  that  accommodation  in  the  plan  are  2  days,  but  the  ’minumum  nights’  of  that  accommodation  is  greater  than  2,  then  the  plan  violates  the  rule.Therefore,  you  should  review  and  examine  the  number  of  ’minumum  nights’  of  each  accommodation  in  the  reference  information  box  and  make  sure  the  days  spent  in  that  accommodation  is  equal  or  greater  than  that  number.*  If  the  feedback  is  about  "No  accommodation/transportation/attaction/meal  in  day  X  is  not  allowed",  this  means  that  on  that  day,  you  should  arrange  the  corresponding  activity  rather  than  leave  it  blank(denoted  as  ’-’).*  If  the  feedback  is  about  "The  transportation  is  conflicting.",  this  means  that  you  cannot  select  neither  the  combination  of  Taxi  and  Self-driving  nor  the  combination  of  Flight  and  Self-driving,  at  the  same  time,  in  terms  of  transportation.Feedback:  {feedback}Write  a  new  plan:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一位熟练的规划师。根据提供的信息和查询，请给我一个详细的计划，包括具体信息，如航班号（例如，F0123456）、餐厅名称和住宿名称。请注意，你的计划中的所有信息都应来源于提供的数据。你必须遵守示例中给出的格式。此外，所有细节应符合常识。符号'-'表示信息不必要。例如，在提供的样本中，你不需要在返回出发城市后进行规划。当你在一天内前往两个城市时，应在“当前城市”部分注明，如示例中所示（即，从A到B）。*****
    示例 ***** 查询：你能为7个人制定一份从伊萨卡到夏洛特的旅行计划吗？计划跨度为3天，从2022年3月8日到3月14日，预算为30,200美元？旅行计划：
    第1天： 当前城市：从伊萨卡到夏洛特 交通：航班号：F3633413，从伊萨卡到夏洛特，出发时间：05:38，到达时间：07:46 早餐：Nagaland’s
    Kitchen，夏洛特 景点：The Charlotte Museum of History，夏洛特 午餐：Cafe Maple Street，夏洛特 晚餐：Bombay
    Vada Pav，夏洛特 住宿：Affordable Spacious Refurbished Room in Bushwick！，夏洛特 第2天： 当前城市：夏洛特
    交通：- 早餐：Olive Tree Cafe，夏洛特 景点：The Mint Museum，夏洛特；Romare Bearden Park，夏洛特。 午餐：Birbal
    Ji Dhaba，夏洛特 晚餐：Pind Balluchi，夏洛特 住宿：Affordable Spacious Refurbished Room in Bushwick！，夏洛特
    第3天： 当前城市：从夏洛特到伊萨卡 交通：航班号：F3786167，从夏洛特到伊萨卡，出发时间：21:42，到达时间：23:26 早餐：Subway，夏洛特
    景点：Books Monument，夏洛特。 午餐：Olive Tree Cafe，夏洛特 晚餐：Kylin Skybar，夏洛特 住宿：- ***** 示例结束
    ***** 给定信息：{参考信息框} 查询：{查询} 旅行计划：{原始草稿旅行计划} 现在你是一个高级推理和自我修正的代理，可以根据自我反思和反馈进行改进。根据给定的查询、参考信息框和上述提出的旅行计划，你现在收到包括失败原因的反馈。尝试编写一个修正错误的新计划。请记住，你只需更改或替换导致问题的项目。如果它在多个地方出现，请一次性修正所有这些地方。尽量避免对之前提出的计划进行不必要的更改。始终确保你的生成内容，如餐厅名称、景点、住宿、交通，在给定的参考信息框中可以找到。对于景点、早餐、晚餐和午餐，在每一天内及整个计划中不要重复出现，即它们在整个旅行计划中不应出现多于一次。可以忽略参考信息框中不相关的信息。*
    如果反馈涉及到重复的餐厅，例如“第4天晚餐的餐厅重复了。”，那么你需要从参考信息框中选择另一个与之前所有选择不同的餐厅；* 如果反馈是关于“第3天的午餐在沙盒中无效。”，这意味着该选择在参考信息框中找不到。然后你应该选择一个在参考信息框中可以找到的餐厅；*
    如果反馈涉及到“住宿X不遵守最小住宿夜数规则”，这意味着在计划中选择的住宿的总天数/夜数不符合最小住宿规则。例如，如果在计划中该住宿的天数是2天，但该住宿的‘最小住宿夜数’大于2，则该计划违反了规则。因此，你应检查参考信息框中每个住宿的‘最小住宿夜数’，并确保在该住宿的天数等于或大于该数字；*
    如果反馈涉及到“第X天没有住宿/交通/景点/餐点安排”，这意味着在那一天你应该安排相应的活动，而不是留空（用‘-’表示）；* 如果反馈涉及到“交通冲突”，这意味着你不能选择出租车和自驾车的组合，也不能选择航班和自驾车的组合。
- en: A.6 Case Presentation
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.6 案例展示
- en: A.6.1 Query Example with its Travel Plan
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.1 查询示例及其旅行计划
- en: 'QUERY:Can  you  create  a  travel  plan  for  a  group  of  4  departing  from  Seattle  2  and  heading  to  San  Francisco  for  3  days,  from  March  6  th  to  March  8th,2022?  Our  budget  is  $2,900.  We  are  bringing  pets,  so  accommodations  need  to  be  pet-friendly.  We  are  interested  in  trying  Mexican,  French,  American,  and  Mediterranean  cuisines  during  our  visit.  We  would  also  prefer  to  avoid  flying  for  transportation.TRAVEL  PLAN:Day  1:Current  City:  from  Seattle  to  San  FranciscoTransportation:  Self-Driving  from  Seattle  to  San  Francisco,  Duration:  12  hours  28  mins,  Cost:  $65Breakfast:  -Attraction:  -Lunch:  -Dinner:  Anupam  Eating  Point,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  2:Current  City:  San  FranciscoTransportation:  -Breakfast:  Coffee  &  Chai  Co.,  San  FranciscoAttraction:  Golden  Gate  Bridge,  San  Francisco;  Golden  Gate  Park,  San  FranciscoLunch:  Bonne  Bouche,  San  FranciscoDinner:  Empress,  San  FranciscoAccommodation:  Room  in  Down  town  Brooklyn  Parkslop,  San  FranciscoDay  3:Current  City:  from  San  Francisco  to  SeattleTransportation:  Self-Driving  from  San  Francisco  to  Seattle,  Duration  :12  hours  25  mins,  Cost:  $65Breakfast:  Gupta’s  Rasoi,  San  FranciscoAttraction:  PIER  39,  San  FranciscoLunch:  Shammi  Bhai  Lassi  Wala,  San  FranciscoDinner:  -Accommodation:  -'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '查询:你能为一个从西雅图出发、前往旧金山的4人小组制定一个为期3天的旅行计划吗？旅行时间为2022年3月6日至3月8日。我们的预算为$2,900。我们带着宠物，所以住宿需要允许宠物。我们希望在访问期间尝试墨西哥菜、法国菜、美国菜和地中海菜。我们也希望避免飞行作为交通方式。旅行计划:第1天:
    当前城市: 从西雅图到旧金山交通: 自驾从西雅图到旧金山，时长: 12小时28分钟，费用: $65早餐: -景点: -午餐: -晚餐: Anupam Eating
    Point，旧金山住宿: 旧金山Brooklyn Parkslop市区房间第2天: 当前城市: 旧金山交通: -早餐: Coffee & Chai Co.，旧金山景点:
    金门大桥，旧金山; 金门公园，旧金山午餐: Bonne Bouche，旧金山晚餐: Empress，旧金山住宿: 旧金山Brooklyn Parkslop市区房间第3天:
    当前城市: 从旧金山到西雅图交通: 自驾从旧金山到西雅图，时长: 12小时25分钟，费用: $65早餐: Gupta’s Rasoi，旧金山景点: PIER
    39，旧金山午餐: Shammi Bhai Lassi Wala，旧金山晚餐: -住宿: -'
- en: A.6.2 Feedback Examples Generated by LLMs
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.6.2 LLMs生成的反馈示例
- en: The feedback generated by LLMs is in the same format of the system feedback.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs生成的反馈与系统反馈格式相同。
- en: 'is_reasonalbe_visiting_city:  fail,  reason:The  trip  should  be  a  closed  circle.is_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  fail,  reason:The  accommodation  Harlem  cozy  nights,  Denver(Colorado)  do  not  obey  the  minumum  nights  rule.is_valid_transportation:  fail,  reason:The  transportation  is  conflicting.is_valid_information_in_current_city:  successis_valid_information_in_sandbox:  fail,  reason:The  accommodation  in  day  3  is  invalid  in  the  sandbox.is_not_absent:  success'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'is_reasonalbe_visiting_city:  失败, 原因: 旅行应为闭环.is_valid_restaurants:  成功is_valid_attractions:  成功is_valid_accommodation:  失败,
    原因: 住宿Harlem cozy nights, Denver(Colorado) 不遵守最少住宿夜数规则.is_valid_transportation:  失败,
    原因: 交通存在冲突.is_valid_information_in_current_city:  成功is_valid_information_in_sandbox:  失败,
    原因: 第3天的住宿在沙盒中无效.is_not_absent:  成功'
- en: 'Recall that, there are $45$ annotated plans in the training set. For each plan,
    without any exception, the generated feedback from the Oracle system is `all-success`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，训练集中有45个标注计划。每个计划生成的Oracle系统反馈都是`all-success`：
- en: 'is_reasonalbe_visiting_city:  successis_valid_restaurants:  successis_valid_attractions:  successis_valid_accommodation:  successis_valid_transportation:  successis_valid_information_in_current_city:  successis_valid_information_in_sandbox:  successis_not_absent:  success
    | Benchmark | Task |'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 'is_reasonalbe_visiting_city:  成功is_valid_restaurants:  成功is_valid_attractions:  成功is_valid_accommodation:  成功is_valid_transportation:  成功is_valid_information_in_current_city:  成功is_valid_information_in_sandbox:  成功is_not_absent:  成功
    | 基准 | 任务 |'
- en: '&#124; Feedback &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 反馈 &#124;'
- en: '&#124; Provided? &#124;'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供? &#124;'
- en: '|'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Trajectory &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '&#124; Released? &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 发布? &#124;'
- en: '| Baseline |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 基准 |'
- en: '&#124; Realistic &#124;'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 现实的 &#124;'
- en: '&#124; Interface? &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 接口? &#124;'
- en: '|'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WebShop Yao et al. ([2022a](#bib.bib48)) | Web | No | Expert | Rule, IL,
    RL, IL+RL | Yes |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| WebShop Yao et al. ([2022a](#bib.bib48)) | Web | 否 | 专家 | 规则, IL, RL, IL+RL
    | 是 |'
- en: '| WebArena Zhou et al. ([2023](#bib.bib58)) | Web | No | Expert, Agent | Direct,
    CoT | Yes |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| WebArena Zhou et al. ([2023](#bib.bib58)) | Web | 否 | 专家, 代理 | 直接, CoT |
    是 |'
- en: '| AgentBench Liu et al. ([2023b](#bib.bib17)) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| AgentBench Liu 等 ([2023b](#bib.bib17)) |'
- en: '&#124; Web, Code, &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网页、代码，&#124;'
- en: '&#124; Game, Embodiment &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 游戏，体现 &#124;'
- en: '| No | Not Found | CoT | Yes |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 未找到 | CoT | 是 |'
- en: '| TravelPlanner Xie et al. ([2024](#bib.bib45)) | Tool, Planning | Yes | Expert
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| TravelPlanner Xie 等 ([2024](#bib.bib45)) | 工具，规划 | 是 | 专家 |'
- en: '&#124; Direct, CoT, &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直接，CoT，&#124;'
- en: '&#124; ReAct, Reflexion &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ReAct, Reflexion &#124;'
- en: '| No |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 否 |'
- en: '| AgentBoard Ma et al. ([2024](#bib.bib19)) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| AgentBoard Ma 等 ([2024](#bib.bib19)) |'
- en: '&#124; Web, Game, Tool, &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网页、游戏、工具，&#124;'
- en: '&#124; Embodiment &#124;'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 体现 &#124;'
- en: '| Partially | Not Found | Direct | Partially |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | 未找到 | 直接 | 部分 |'
- en: '| AgentGym Xi et al. ([2024b](#bib.bib44)) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| AgentGym Xi 等 ([2024b](#bib.bib44)) |'
- en: '&#124; Web, Code, Game, &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 网页、代码、游戏，&#124;'
- en: '&#124; Tool, Embodiment &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具，体现 &#124;'
- en: '| Partially | Expert, Agent |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 部分 | 专家，代理 |'
- en: '&#124; BC (SFT), &#124;'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BC (SFT), &#124;'
- en: '&#124; ReAct, AGENTEVOL &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ReAct, AGENTEVOL &#124;'
- en: '| Yes |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 是 |'
- en: 'Table A.1: Popular Benchmarks for LLM Agents'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.1：LLM 代理的流行基准
- en: A.7 Related Works
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.7 相关工作
- en: A.7.1 Benchmarks for LLM-based Generalist Agents
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.1 LLM 基于的通用代理的基准
- en: It has been anticipated that generalist agents can handle diverse tasks and
    evolve across different (cyber) environments at the human level which is a long-term
    goal in the AGI community. LLMs can be used as experts, which mimic humans, that
    have a strong generalization capability that not only suits conventional NLP but
    also agentic tasks. Recently, plenty of benchmarks have been proposed to evaluate
    the agents across various tasks and environments comprehensively and fairly. We
    provide an overview of popular benchmarks in the community in Table [A.1](#A1.T1
    "Table A.1 ‣ A.6.2 Feedback Examples Generated by LLMs ‣ A.6 Case Presentation
    ‣ Appendix A Appendix ‣ Can We Rely on LLM Agents to Draft Long-Horizon Plans?
    Let’s Take TravelPlanner as an Example"). Some benchmarks such as ALFWorld Shridhar
    et al. ([2020](#bib.bib31)) and Mind2Web Deng et al. ([2023](#bib.bib6)), which
    are already included in larger benchmarks, are not listed in the table. Although
    the recent progress in multi-modal LLMs has spurred research into multi-modal
    LLM agents Yang et al. ([2023](#bib.bib47)); Zheng et al. ([2024](#bib.bib56)),
    we only list benchmarks that focus exclusively on text-based environments which
    assess LLM agents’ abilities via textual reasoning and taking actions in-depth.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 已经预期到通用代理可以在不同的（网络）环境中处理各种任务，并达到人类水平，这是 AGI 社区的长期目标。LLM 可以作为模拟人类的专家，具有强大的泛化能力，不仅适用于传统
    NLP，还适用于代理任务。最近，提出了大量的基准来全面、公正地评估代理在各种任务和环境中的表现。我们在表格 [A.1](#A1.T1 "表 A.1 ‣ A.6.2
    LLM 生成的反馈示例 ‣ A.6 案例展示 ‣ 附录 A 附录 ‣ 我们可以依赖 LLM 代理来制定长远计划吗？以 TravelPlanner 为例")
    中提供了社区中流行基准的概述。一些基准，如 ALFWorld Shridhar 等 ([2020](#bib.bib31)) 和 Mind2Web Deng
    等 ([2023](#bib.bib6))，虽然已经包含在更大的基准中，但未列在表中。尽管多模态 LLM 的最新进展刺激了对多模态 LLM 代理的研究 Yang
    等 ([2023](#bib.bib47)); Zheng 等 ([2024](#bib.bib56))，我们只列出了专注于基于文本的环境的基准，这些基准通过文本推理和深入行动评估
    LLM 代理的能力。
- en: The listed benchmarks support agents powered by both API-based proprietary and
    open-weight LLMs with convenient drop-in replacement interfaces. It is also free
    to add few-shots or use other prompting strategies to generate actions.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列出的基准支持由 API 基于的专有和开放权重 LLM 驱动的代理，并具有方便的即插即用替换接口。还可以自由添加少量样本或使用其他提示策略来生成动作。
- en: A.7.2 Long Contexts Challenge for LLMs
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.2 LLM 的长上下文挑战
- en: Besides the fact that more and more LLMs offer long-context capabilities Fei
    et al. ([2023](#bib.bib7)); Ratner et al. ([2023](#bib.bib27)); Liu et al. ([2023a](#bib.bib16));
    Zhao et al. ([2024](#bib.bib55)); Qian et al. ([2024](#bib.bib24)), recent studies
    question LLMs’ ability to find needles in a haystack because they face challenges
    in discriminating highly semantically related information, and can be easily distracted
    by irrelevant and misleading contents in long contexts Wu et al. ([2024](#bib.bib41));
    Zhu et al. ([2023](#bib.bib60)); Chang et al. ([2024](#bib.bib1)); Shi et al.
    ([2023](#bib.bib29)); gkamradt ([2023](#bib.bib8)). The TravelPlanner Xie et al.
    ([2024](#bib.bib45)) is a benchmark to provide insightful answers to this problem,
    wherein lengthy context information, noise, and relevant snippets are deeply intertwined.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 除了越来越多的 LLM 提供长上下文能力 Fei et al. ([2023](#bib.bib7)); Ratner et al. ([2023](#bib.bib27));
    Liu et al. ([2023a](#bib.bib16)); Zhao et al. ([2024](#bib.bib55)); Qian et al.
    ([2024](#bib.bib24))，最近的研究质疑 LLM 在大海捞针方面的能力，因为它们在区分高度语义相关的信息时面临挑战，并且容易受到长上下文中的无关和误导性内容的干扰
    Wu et al. ([2024](#bib.bib41)); Zhu et al. ([2023](#bib.bib60)); Chang et al.
    ([2024](#bib.bib1)); Shi et al. ([2023](#bib.bib29)); gkamradt ([2023](#bib.bib8))。TravelPlanner
    Xie et al. ([2024](#bib.bib45)) 是一个基准，用于深入解答这一问题，其中长上下文信息、噪声和相关片段深度交织在一起。
- en: A.7.3 Multi-Agent Collaboration
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.3 多智能体协作
- en: Recent studies have borrowed the multiple-agent methodology for collaboration
    on cyber tasks, gaming, coding, math reasoning, conversation responding, and question
    answering Guo et al. ([2024](#bib.bib9)); Wu et al. ([2023a](#bib.bib40), [b](#bib.bib42));
    Zhang et al. ([2024c](#bib.bib54)); Li et al. ([2024](#bib.bib14)); Liu et al.
    ([2024](#bib.bib18)); Talebirad and Nadiri ([2023](#bib.bib34)); Zhang et al.
    ([2024a](#bib.bib52)); Wang et al. ([2024a](#bib.bib35)). Under the hood, these
    works assign role-specific prompts to the LLM to build multiple agents for synergy
    and collaboration. The self-refinement works can be classified into this realm,
    where the advisor and refiner agents can troubleshoot and modify the response
    in a few rounds Madaan et al. ([2024](#bib.bib20)); Paul et al. ([2023](#bib.bib23));
    Kim et al. ([2024](#bib.bib11)); Pan et al. ([2024](#bib.bib22)); Chen et al.
    ([2023a](#bib.bib2)). However, few works study the reliability and robustness
    of multi-agent collaboration in more complex and practical tasks. Compared to
    the previous testbeds where generation errors are easily noticeable and unambiguous,
    it is questionable whether refinement can work on TravelPlanner, where the glitches
    are hard to find due to implicit commonsense constraints. Multi-agent collaboration
    also places higher demands on the capabilities of individual agents since a failure
    at any stage from any agent can lead to a collapse, such as a dead loop or deviation
    from the goal.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究借鉴了多智能体方法来协作处理网络任务、游戏、编码、数学推理、对话响应和问答任务 Guo et al. ([2024](#bib.bib9));
    Wu et al. ([2023a](#bib.bib40), [b](#bib.bib42)); Zhang et al. ([2024c](#bib.bib54));
    Li et al. ([2024](#bib.bib14)); Liu et al. ([2024](#bib.bib18)); Talebirad 和 Nadiri
    ([2023](#bib.bib34)); Zhang et al. ([2024a](#bib.bib52)); Wang et al. ([2024a](#bib.bib35))。在这些工作中，角色特定的提示被分配给大型语言模型（LLM），以构建多个智能体进行协作与协同。自我修正的工作可以归入这一领域，其中顾问和修正智能体可以在几轮中排除故障并修改响应
    Madaan et al. ([2024](#bib.bib20)); Paul et al. ([2023](#bib.bib23)); Kim et al.
    ([2024](#bib.bib11)); Pan et al. ([2024](#bib.bib22)); Chen et al. ([2023a](#bib.bib2))。然而，少有研究探讨多智能体协作在更复杂和实际任务中的可靠性和鲁棒性。与之前生成错误容易显现且明确的测试环境相比，是否能在
    TravelPlanner 上进行修正存在疑问，因为由于隐含的常识约束，缺陷很难发现。多智能体协作还对个体智能体的能力提出了更高的要求，因为任何智能体在任何阶段的失败都可能导致崩溃，如死循环或偏离目标。
- en: A.7.4 Reinforcement Learning via Feedback
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.7.4 通过反馈进行的强化学习
- en: On top of works that only use successful trajectories for behavioural cloning
    Zeng et al. ([2023](#bib.bib51)); Chen et al. ([2023b](#bib.bib3)); Zhang et al.
    ([2024b](#bib.bib53)); Chen et al. ([2024](#bib.bib4)), another line of work trains
    LLM-based agents based on environmental feedback, referred to as interactive learning
    methods Song et al. ([2024b](#bib.bib33)); Zhou et al. ([2024b](#bib.bib59));
    Christianos et al. ([2023](#bib.bib5)); Xi et al. ([2024a](#bib.bib43)). Specifically,
    they train the agents via reinforcement learning. However, poor transferability
    among scenarios, reward inconsistency, off-policy shift, step-level reward sparsity,
    and training stability and expenses are the main roots of performance bottlenecks.
    Possible alternative approaches such as Negative Aware Training (NAT) Wang et al.
    ([2024b](#bib.bib36)); Li et al. ([2023](#bib.bib15)) can be a more robust solution.
    Our FAFT approach is motivated by NAT, and it can be seamlessly migrated to other
    agentic tasks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅使用成功轨迹进行行为克隆的研究基础上，Zeng 等人 ([2023](#bib.bib51))；Chen 等人 ([2023b](#bib.bib3))；Zhang
    等人 ([2024b](#bib.bib53))；Chen 等人 ([2024](#bib.bib4))，另一类研究基于环境反馈训练基于 LLM 的智能体，被称为交互学习方法，Song
    等人 ([2024b](#bib.bib33))；Zhou 等人 ([2024b](#bib.bib59))；Christianos 等人 ([2023](#bib.bib5))；Xi
    等人 ([2024a](#bib.bib43))。具体来说，他们通过强化学习来训练智能体。然而，场景间的转移能力差、奖励不一致、策略外转移、步骤级奖励稀疏以及训练稳定性和开销是性能瓶颈的主要根源。可能的替代方法，如负面意识训练（NAT），Wang
    等人 ([2024b](#bib.bib36))；Li 等人 ([2023](#bib.bib15))，可能是一种更为稳健的解决方案。我们的 FAFT 方法受到
    NAT 的启发，并且可以无缝迁移到其他智能体任务。
