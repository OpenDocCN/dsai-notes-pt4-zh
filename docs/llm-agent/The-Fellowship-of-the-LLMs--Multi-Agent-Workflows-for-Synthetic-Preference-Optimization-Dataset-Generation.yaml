- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:38:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:38:49'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的联盟：用于合成偏好优化数据集生成的多智能体工作流程
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08688](https://ar5iv.labs.arxiv.org/html/2408.08688)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08688](https://ar5iv.labs.arxiv.org/html/2408.08688)
- en: Samee Arif¹, Sualeha Farid², Abdul Hameed Azeemi¹, Awais Athar³, Agha Ali Raza¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Samee Arif¹, Sualeha Farid², Abdul Hameed Azeemi¹, Awais Athar³, Agha Ali Raza¹
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This paper presents synthetic Preference Optimization (PO) datasets generated
    using multi-agent workflows and evaluates the effectiveness and potential of these
    workflows in the dataset generation process. PO dataset generation requires two
    modules: (1) response evaluation, and (2) response generation. In the response
    evaluation module, the responses from Large Language Models (LLMs) are evaluated
    and ranked - a task typically carried out by human annotators that we automate
    using LLMs. We assess the response evaluation module in a 2 step process. In step
    1, we assess LLMs as evaluators using three distinct prompting strategies. In
    step 2, we apply the winning prompting strategy to compare the performance of
    LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. In each step, we use inter-rater
    agreement using Cohen’s Kappa between human annotators and LLMs. For the response
    generation module, we compare different configurations for the LLM Feedback Loop
    using the identified LLM evaluator configuration. We use the win rate (the fraction
    of times a generation framework is selected as the best by an LLM evaluator) to
    determine the best multi-agent configuration for generation. After identifying
    the best configurations for both modules, we use models from the GPT, Gemma, and
    Llama families to generate our PO datasets using the above pipeline. We generate
    two types of PO datasets, one to improve the generation capabilities of individual
    LLM and the other to improve the multi-agent workflow. Our evaluation shows that
    GPT-4o-as-a-Judge is more consistent across datasets when the candidate responses
    do not include responses from the GPT family. Additionally, we find that the LLM
    Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves
    a notable 71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了使用多智能体工作流程生成的合成偏好优化（PO）数据集，并评估了这些工作流程在数据集生成过程中的有效性和潜力。PO 数据集生成需要两个模块：（1）响应评估，和（2）响应生成。在响应评估模块中，对来自大语言模型（LLMs）的响应进行评估和排名——这通常由人工注释员完成，我们通过
    LLMs 实现自动化。我们在两个步骤中评估响应评估模块。在步骤 1 中，我们使用三种不同的提示策略评估 LLMs 作为评估者。在步骤 2 中，我们应用获胜的提示策略来比较
    LLM-as-a-Judge、LLMs-as-a-Jury 和 LLM 辩论的表现。在每一步中，我们使用 Cohen’s Kappa 进行人类注释员和 LLMs
    之间的评估一致性。对于响应生成模块，我们使用识别的 LLM 评估配置来比较 LLM 反馈循环的不同配置。我们使用胜率（LLM 评估者选择生成框架的次数比例）来确定最佳的多智能体生成配置。在确定两个模块的最佳配置后，我们使用
    GPT、Gemma 和 Llama 系列的模型，通过上述管道生成我们的 PO 数据集。我们生成了两种类型的 PO 数据集，一种用于提升单个 LLM 的生成能力，另一种用于改进多智能体工作流程。我们的评估表明，当候选响应不包括来自
    GPT 系列的响应时，GPT-4o-as-a-Judge 在数据集之间的一致性更高。此外，我们发现，LLM 反馈循环中，Llama 作为生成器和 Gemma
    作为审阅者的组合在单智能体 Llama 和 Gemma 中分别取得了显著的 71.8% 和 73.8% 胜率。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) demonstrate a range of Natural Language Processing
    (NLP) capabilities, including text generation, question answering, and language
    understanding. However, LLMs can sometimes deviate from user instructions and
    exhibit unintended behaviors (Tamkin et al. [2021](#bib.bib16)). To mitigate this
    problem and align the LLM outputs more closely with human preferences, techniques
    like Reinforcement Learning from Human Feedback (RLHF) are used, which involves
    fine-tuning LLMs using the reward signal from human preferences (Christiano et al.
    [2017](#bib.bib2)). Improved methods like Direct Preference Optimization (DPO)
    (Rafailov et al. [2024](#bib.bib14)) eliminate the need for fitting the reward
    model and are more stable and performant. In DPO, the preference optimization
    dataset requires a pair of accepted and rejected responses for each prompt. The
    accepted response is one that better aligns with the desired human preferences.
    Other techniques like Kahneman-Tversky Optimization (KTO) (Ethayarajh et al. [2024](#bib.bib4))
    require each response to indicate whether it is good or bad (i.e., as a binary
    classification task) instead of pairwise preferences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）展示了一系列自然语言处理（NLP）能力，包括文本生成、问题回答和语言理解。然而，LLMs 有时会偏离用户指令并表现出意外行为（Tamkin
    等人 [2021](#bib.bib16)）。为了减轻这个问题并使 LLM 输出更接近人类偏好，使用了像从人类反馈中进行强化学习（RLHF）这样的技术，它涉及使用来自人类偏好的奖励信号对
    LLMs 进行微调（Christiano 等人 [2017](#bib.bib2)）。改进的方法如直接偏好优化（DPO）（Rafailov 等人 [2024](#bib.bib14)）消除了对奖励模型拟合的需求，并且更稳定、性能更好。在
    DPO 中，偏好优化数据集要求每个提示都有一对接受和拒绝的响应。接受的响应是那些更符合期望的人类偏好的响应。其他技术如卡尼曼-特沃斯基优化（KTO）（Ethayarajh
    等人 [2024](#bib.bib4)）要求每个响应指明其好坏（即作为二元分类任务），而不是成对的偏好。
- en: 'In the process of constructing the dataset of human preferences, the evaluation
    and ranking of the outputs generated by LLMs are typically done by human annotators,
    who assess these outputs based on various criteria such as instruction following,
    helpfulness, relevance, accuracy, depth, and creativity. The PO dataset generation
    process is divided into two modules: response evaluation and response generation.
    The response evaluation module involves assessing and ranking responses generated
    by LLMs, while the response generation module focuses on creating responses that
    align with the identified preferences. This manual process, while effective, is
    labor-intensive, time-consuming, inconsistent, and subject to human biases. In
    this work, we thus ask the question, “Can we use LLM agents to automate and improve
    response evaluation and generation for constructing preference optimization (PO)
    datasets?”.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建人类偏好数据集的过程中，LLMs 生成的输出通常由人工注释者进行评估和排名，这些注释者根据各种标准进行评估，如遵循指令、有效性、相关性、准确性、深度和创造力。PO
    数据集生成过程分为两个模块：响应评估和响应生成。响应评估模块涉及评估和排名 LLMs 生成的响应，而响应生成模块则专注于创建符合识别的偏好的响应。尽管这种手动过程有效，但它劳动密集、耗时、不一致，并且容易受到人为偏见的影响。因此，在本研究中，我们提出了一个问题：“我们能否使用
    LLM 代理来自动化和改进响应评估和生成，以构建偏好优化（PO）数据集？”
- en: For the response evaluation step, we leverage LLMs as evaluators and compare
    several configurations including LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate
    to pick the best evaluation strategy. Previously, single agents have been used
    to generate the responses for PO datasets. However, we use a multi-agent framework
    for response generation, which allows us to generate more refined and higher-quality
    responses. The multi-agent approach uses the collaboration between multiple LLMs,
    where one agent can provide suggestions for improvements, and the other can revise
    the response based on the feedback. This iterative process leads to a thorough
    refinement of the generated content, ensuring that the final output better aligns
    with human preferences and expectations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于响应评估步骤，我们利用 LLMs 作为评估者，并比较了包括 LLM-as-a-Judge、LLMs-as-a-Jury 和 LLM Debate 等多种配置，以选择最佳评估策略。以前，单个代理已被用于生成
    PO 数据集的响应。然而，我们使用了多代理框架来生成响应，这使我们能够生成更精炼和更高质量的响应。多代理方法利用多个 LLMs 之间的协作，其中一个代理可以提供改进建议，而另一个代理可以根据反馈修订响应。这种迭代过程有助于对生成内容进行彻底的改进，确保最终输出更好地符合人类的偏好和期望。
- en: 'In this paper, we present multiple DPO and KTO datasets. Our focus is on generating
    two separate categories of datasets: one aimed at improving the performance of
    individual LLMs and the other to enhance the effectiveness of multi-agent workflows.
    The primary aim of the first dataset is to enhance the performance and capabilities
    of individual LLMs by providing high-quality PO training data that better aligns
    with human judgment and expectations. The goal of the second category of the dataset
    is to improve the multi-agent frameworks of LLM Feedback Loop generation approach,
    enabling better feedback provision, response refinement, and decision-making.
    Our contributions can be summarized as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了多个DPO和KTO数据集。我们的重点是生成两类不同的数据集：一类旨在提升单个LLM的性能，另一类则旨在提高多代理工作流的效果。第一个数据集的主要目标是通过提供与人类判断和期望更好对齐的高质量PO训练数据，来提升单个LLM的性能和能力。第二类数据集的目标是改善LLM反馈循环生成方法的多代理框架，从而实现更好的反馈提供、响应优化和决策。我们的贡献可以总结如下：
- en: '1.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We generate synthetic PO datasets for single-agent improvement by combining
    the best configuration for the evaluation and generation module. We also generate
    PO datasets for multi-agent improvement.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过结合评估和生成模块的最佳配置，为单一代理改进生成合成PO数据集。我们还为多代理改进生成PO数据集。
- en: '2.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'We present a comprehensive evaluation of using LLMs as evaluators on the task
    of selecting the better response among the candidate responses. We specifically
    compare the performance of three distinct approaches: LLM-as-a-Judge, LLMs-as-a-Jury,
    and LLM Debate.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对使用LLM作为评估者选择候选响应中较佳响应的任务进行了全面评估。我们特别比较了三种不同方法的表现：LLM-as-a-Judge，LLMs-as-a-Jury，以及LLM
    Debate。
- en: '3.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We present an evaluation of the LLM Feedback Loop workflow for the response
    generation module, specifically testing different configurations using Llama-3.1-8
    and Gemma-2-9b models.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对响应生成模块的LLM反馈循环工作流进行了评估，特别测试了使用Llama-3.1-8和Gemma-2-9b模型的不同配置。
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Preference Optimization
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 偏好优化
- en: Preference Optimization has emerged as a pivotal technique for aligning model
    outputs with human preferences. Rafailov et al. [2024](#bib.bib14) introduce DPO,
    a method that simplifies solving the standard RLHF (Reinforcement Learning from
    Human Feedback) problem by converting it into a classification task, enabling
    the extraction of the optimal policy in a straightforward way. Hong, Lee, and
    Thorne [2024](#bib.bib7) introduce ORPO algorithm that combines the traditional
    supervised fine-tuning and preference alignment stages into a single process.
    The dataset for DPO and ORPO require annotated preference pairs, where each pair
    consists of two model outputs labeled according to which one better aligns with
    human preferences. Ethayarajh et al. [2024](#bib.bib4) introduce KTO, a cost-effective
    approach to align Large Language Models (LLMs) with human feedback, improving
    performance without the need for preference pairs. Argilla Distilabel (Álvaro
    Bartolomé Del Canto et al. [2024](#bib.bib23)) uses LLM to judge between the responses
    of two models to create synthetic PO datasets. The datasets are available on Hugging
    Face¹¹1https://huggingface.co/argilla. To our knowledge, no one has yet explored
    the use of Multi-Agent workflows for the generation of PO datasets. However, multi-agent
    frameworks have been utilized for other tasks which we discuss below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好优化已经成为将模型输出与人类偏好对齐的关键技术。Rafailov等人[2024](#bib.bib14)提出了DPO，一种通过将标准RLHF（来自人类反馈的强化学习）问题简化为分类任务的方法，从而以简单的方式提取最优策略。Hong、Lee和Thorne
    [2024](#bib.bib7)提出了ORPO算法，该算法将传统的监督微调和偏好对齐阶段合并为一个过程。DPO和ORPO的数据集需要带有注释的偏好对，这些偏好对包括两个模型输出，并根据哪个输出更符合人类偏好进行标记。Ethayarajh等人[2024](#bib.bib4)提出了KTO，这是一种经济高效的方法，用于将大型语言模型（LLMs）与人类反馈对齐，提高性能而不需要偏好对。Argilla
    Distilabel（Álvaro Bartolomé Del Canto等人[2024](#bib.bib23)）使用LLM在两个模型的响应之间进行判断，以创建合成的PO数据集。这些数据集可以在Hugging
    Face¹¹1https://huggingface.co/argilla上获取。根据我们的了解，目前尚未有人探索使用多代理工作流来生成PO数据集。然而，多代理框架已被用于其他任务，我们将在下文中讨论。
- en: 2.2 Agent Frameworks
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 代理框架
- en: 'Recently, there has been a growing interest in using LLM multi-agent frameworks
    for different tasks. Zheng et al. [2023a](#bib.bib21) presents an evaluation of
    LLM-as-a-Judge on the MT-Bench (Zheng et al. [2023b](#bib.bib22)) and Chatbot
    Arena (Li et al. [2024](#bib.bib8)). Their results reveal that strong LLM judges
    like GPT-4 can match both controlled and crowd-sourced human preferences well,
    achieving over 80% agreement, the same level of agreement between humans. Additionally,
    they evaluate several variants of Llama and Vicuna on the dataset. They study
    the limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement
    biases, as well as limited reasoning ability. Verga et al. [2024](#bib.bib17)
    explore the use of LLMs-as-a-Jury. Their approach, a Panel of LLM evaluators (PoLL),
    composed of a larger number of smaller models outperforms a single large judge.
    They also show that the PoLL approach exhibits less intra-model bias as compared
    to LLM-as-a-Judge. They use Command-R, GPT, Claude-3, and Mistral families for
    their study. Additionally, they compare two prompting strategies: (1) reference-based
    scoring where they provide the LLM with a reference answer, and (2) candidate
    answer and pair-wise scoring where they ask the LLM to pick the better response
    from the candidate responses. PoLL outperforms single-agents on KILT (Petroni
    et al. [2021](#bib.bib13)) and Chatbot Arena.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，使用 LLM 多智能体框架来处理不同任务的兴趣日益增加。郑等人 [2023a](#bib.bib21) 对 MT-Bench（郑等人 [2023b](#bib.bib22)）和
    Chatbot Arena（李等人 [2024](#bib.bib8)）中的 LLM 作为裁判进行了评估。他们的结果揭示了强大的 LLM 裁判如 GPT-4
    可以很好地匹配受控和众包的人类偏好，达到了超过 80% 的一致性，与人类之间的一致性相同。此外，他们还在数据集中评估了几种 Llama 和 Vicuna 的变体。他们研究了
    LLM 作为裁判的局限性，包括位置、冗长和自我增强偏见，以及有限的推理能力。Verga 等人 [2024](#bib.bib17) 探索了将 LLM 作为陪审团的使用。他们的方法，即由更多小模型组成的
    LLM 评估小组（PoLL），优于单一大型裁判。他们还表明，PoLL 方法比 LLM 作为裁判表现出更少的模型内偏见。他们在研究中使用了 Command-R、GPT、Claude-3
    和 Mistral 系列。此外，他们比较了两种提示策略：（1）基于参考的评分，即向 LLM 提供参考答案；（2）候选答案和配对评分，即要求 LLM 从候选回答中选择更好的回答。PoLL
    在 KILT（Petroni 等人 [2021](#bib.bib13)）和 Chatbot Arena 上优于单一智能体。
- en: 'Liang et al.[2024](#bib.bib10) introduce Multi-Agent Debate (MAD) to encourage
    divergent thinking in LLMs. They mitigate the Degeneration-of-Thought (DoT) problem,
    which is that once the LLM has established confidence in its solutions, it is
    unable to generate novel thoughts. In their approach, the affirmative LLM and
    the negative LLM debate on the answer while the LLM judge evaluates both arguments
    after each round of debate. They evaluate the approach on the Commonsense Machine
    Translation Dataset (Chinese to English) (He et al. [2020](#bib.bib6)) and their
    Counter-Intuitive Arithmetic Reasoning (CIAR) dataset. MAD was able to achieve
    a 37% accuracy on the CIAR dataset using GPT-3.5-Turbo which outperforms Chain-of-Thought,
    Self-Consistency, and Self-Reflection prompting. They also show that using the
    MAD approach decreases bias and increases response diversity. Du et al. [2023](#bib.bib3)
    evaluates a different variant of multi-agent debate where multiple models generate
    their own responses, and each model receives the opinions of the other models,
    then updates its response if necessary. This is done for multiple rounds. Du et al.
    [2023](#bib.bib3) evaluates the approach on the following tasks: Biography generation,
    MMLU, Chess move validity, Arithmetic, Grade school math, and Chess move optimality.
    Their approach using ChatGPT and Bard outperforms single-agent on all the tasks.
    To evaluate LLM responses Chan et al. [2023](#bib.bib1) presents another variant
    of multi-agent debate. Their architecture involves assigning agents different
    roles such as General Public, Critic, Psychologist, News Author, and Scientist.
    They used ChatGPT and GPT-4 for their evaluation on FairEval (Wang et al. [2023a](#bib.bib18))
    dataset and achieved a Cohen’s Kappa score of 0.40 using LLM Debate, 0.03 more
    than the single agent.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Liang 等人 [2024](#bib.bib10) 引入了多智能体辩论（MAD）以鼓励 LLM 中的发散思维。他们缓解了思想退化（DoT）问题，即一旦
    LLM 对其解决方案建立了信心，它将无法产生新颖的想法。在他们的方法中，肯定派 LLM 和反对派 LLM 对答案进行辩论，同时 LLM 法官在每轮辩论后评估双方论点。他们在
    Commonsense Machine Translation Dataset（中译英）（He 等人 [2020](#bib.bib6)）和他们的反直觉算术推理（CIAR）数据集上评估了这种方法。MAD
    在 CIAR 数据集上使用 GPT-3.5-Turbo 实现了 37% 的准确率，优于 Chain-of-Thought、Self-Consistency
    和 Self-Reflection 提示。他们还展示了使用 MAD 方法可以减少偏见并增加回应多样性。Du 等人 [2023](#bib.bib3) 评估了另一种多智能体辩论的变体，其中多个模型生成自己的回应，每个模型接收其他模型的意见，然后在必要时更新其回应。这一过程会重复多轮。Du
    等人 [2023](#bib.bib3) 在以下任务上评估了这种方法：传记生成、MMLU、棋步有效性、算术、小学数学和棋步优化。他们使用 ChatGPT 和
    Bard 的方法在所有任务上均优于单智能体。为了评估 LLM 的回应，Chan 等人 [2023](#bib.bib1) 提出了另一种多智能体辩论的变体。他们的架构包括为代理分配不同角色，如普通公众、批评家、心理学家、新闻作者和科学家。他们使用
    ChatGPT 和 GPT-4 对 FairEval（Wang 等人 [2023a](#bib.bib18)）数据集进行了评估，并使用 LLM 辩论获得了
    0.40 的 Cohen’s Kappa 分数，比单智能体高出 0.03。
- en: 3 Methodology
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Experimental Setup
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'In this study, we perform experiments on the three categories of models given
    in Table [1](#S3.T1 "Table 1 ‣ 3.1 Experimental Setup ‣ 3 Methodology ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). For the evaluation module, we evaluate single agents and multi-agent
    frameworks on four datasets, Alpaca Eval (Li et al. [2023](#bib.bib9)), FairEval
    (Wang et al. [2023a](#bib.bib18)), PandaLM-Eval (Wang et al. [2024](#bib.bib20),
    [2023b](#bib.bib19)) and MT-Bench (Zheng et al. [2023b](#bib.bib22)). For the
    generation module, we compare the multi-agent frameworks using win rate - the
    ratio of times a generation framework is selected as the best by an LLM evaluator
    when comparing outputs from all generation workflows. After the extensive evaluation
    of both modules, we used the picked strategies to generate synthetic PO datasets.
    We set the temperature to 0 in all our evaluations to ensure reproducibility.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们对表格[1](#S3.T1 "表格 1 ‣ 3.1 实验设置 ‣ 3 方法论 ‣ 大型语言模型的联合体：多智能体工作流用于合成偏好优化数据集生成")中给出的三类模型进行了实验。对于评估模块，我们在四个数据集上评估了单智能体和多智能体框架，分别是
    Alpaca Eval（Li 等人 [2023](#bib.bib9)）、FairEval（Wang 等人 [2023a](#bib.bib18)）、PandaLM-Eval（Wang
    等人 [2024](#bib.bib20)、[2023b](#bib.bib19)）和 MT-Bench（Zheng 等人 [2023b](#bib.bib22)）。对于生成模块，我们使用胜率来比较多智能体框架——即生成框架被
    LLM 评估者选为最佳的次数比例，比较所有生成工作流的输出。在对两个模块进行广泛评估后，我们使用选择的策略生成了合成 PO 数据集。为了确保可重复性，我们在所有评估中将温度设置为
    0。
- en: '| Category | Models |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 模型 |'
- en: '| --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Small-Scale LLM | Llama-3.1-8b |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 小规模 LLM | Llama-3.1-8b |'
- en: '| Gemma-2-9b |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b |'
- en: '| Mid-Scale LLM | Gemma-2-27b |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 中规模 LLM | Gemma-2-27b |'
- en: '| Llama-3.1-70b |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b |'
- en: '| Large-Scale LLM | GPT-4o-Mini (2024-07-18) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 大规模 LLM | GPT-4o-Mini (2024-07-18) |'
- en: '| GPT-4o (2024-05-13) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o (2024-05-13) |'
- en: 'Table 1: Categories of LLMs used in the study.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：研究中使用的 LLM 分类。
- en: 3.2 LLM-as-Evaluator
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM 作为评估者
- en: 'With the aim of automating the evaluation component of PO dataset generation,
    we assess the performance of LLMs in the role of evaluators using the Alpaca Eval,
    FairEval, PandaLM-Eval, and MT-Bench datasets. Our goal is to determine whether
    multi-agent workflows work better than a single agent for LLM evaluation. The
    system prompts for this task are modified version of the prompts used by Zheng
    et al. [2023a](#bib.bib21) and are given in Appendix [A](#A1 "Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在自动化 PO 数据集生成的评估组件，我们评估了 LLM 作为评审的表现，使用 Alpaca Eval、FairEval、PandaLM-Eval 和
    MT-Bench 数据集。我们的目标是确定多代理工作流是否优于单一代理进行 LLM 评估。此任务的系统提示是 Zheng 等人 [2023a](#bib.bib21)
    使用的提示的修改版，并在附录 [A](#A1 "附录 A 系统提示 ‣ LLM 的伙伴关系：合成偏好优化数据集生成的多代理工作流") 中提供。
- en: LLM-as-Judge.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 作为评审。
- en: 'We evaluate six different LLMs on the Alpaca Eval dataset, calculating Cohen’s
    Kappa with the human annotations. Our evaluation involved three distinct prompting
    strategies for the LLM-as-a-Judge:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Alpaca Eval 数据集上评估了六种不同的 LLM，通过与人工注释计算 Cohen’s Kappa。我们的评估涉及三种不同的提示策略用于
    LLM 作为评审：
- en: '1.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Direct Comparison: The Judge-LLM is provided with the user question and the
    responses generated by different LLMs. It is asked to pick the best response among
    the given options.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接比较：Judge-LLM 收到用户问题和不同 LLM 生成的回答。它被要求在给定的选项中挑选最佳回答。
- en: '2.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Independent Scoring: The Judge-LLM is given the user question and each response
    in separate conversations. It is asked to score each response independently.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 独立评分：Judge-LLM 在不同的对话中接收用户问题和每个回答。它被要求独立评分每个回答。
- en: '3.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Combined Scoring: The Judge-LLM is provided with the user question and all
    the responses in a single conversation thread. It is asked to assign a score to
    each response within the same conversation context. To observe if the scoring
    range influences the LLM’s scoring consistency and its alignment with human annotations,
    we test three different scoring totals: 5, 10, and 100.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 组合评分：Judge-LLM 接收到用户问题和所有回答在同一对话线程中。它被要求在相同对话上下文中为每个回答分配分数。为了观察评分范围是否影响 LLM
    的评分一致性及其与人工注释的一致性，我们测试了三种不同的评分总数：5、10 和 100。
- en: 'For each of these prompting strategy, we systematically analyze the performance
    of the LLMs by calculating Cohen’s Kappa, against the human annotations. The system
    prompts are given in Table [7](#A1.T7 "Table 7 ‣ Appendix A System Prompts ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation") in Appendix [A](#A1 "Appendix A System Prompts ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种提示策略，我们通过计算 Cohen’s Kappa 来系统地分析 LLM 的表现，与人工注释进行比较。系统提示见附录 [A](#A1 "附录 A
    系统提示 ‣ LLM 的伙伴关系：合成偏好优化数据集生成的多代理工作流") 的表 [7](#A1.T7 "表 7 ‣ 附录 A 系统提示 ‣ LLM 的伙伴关系：合成偏好优化数据集生成的多代理工作流")。
- en: LLMs-as-Jury.
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 作为陪审团。
- en: 'We extend the evaluation from the LLM-as-a-Judge approach by forming juries
    composed of multiple LLMs. We test all possible permutations of the jury configurations.
    We use three datasets: FairEval, PandaLM-Eval and MT-Bench datasets for a more
    comprehensive analysis. We systematically analyze the performance of each jury
    configuration, focusing on how the size and combination of the LLMs affect their
    judgment accuracy. The Combined Scoring system prompt in Table [7](#A1.T7 "Table
    7 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") in Appendix [A](#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") is used for all the
    jurors because it performed the best in our previous evaluation.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过组成由多个 LLM 组成的陪审团来扩展 LLM 作为评判者的方法。我们测试了所有可能的陪审团配置排列。我们使用了三个数据集：FairEval、PandaLM-Eval
    和 MT-Bench 数据集，以便进行更全面的分析。我们系统地分析了每种陪审团配置的性能，重点关注 LLM 的大小和组合如何影响其判断准确性。附录[A](#A1
    "附录 A 系统提示 ‣ LLM 的合作：多智能体工作流程用于合成偏好优化数据集生成")中的表格[7](#A1.T7 "表格 7 ‣ 附录 A 系统提示 ‣
    LLM 的合作：多智能体工作流程用于合成偏好优化数据集生成")中所示的组合评分系统提示用于所有陪审团成员，因为它在我们之前的评估中表现最佳。
- en: LLM Debate.
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 辩论。
- en: 'We also evaluate the LLM Debate framework following the implementation described
    by Chan et al. [2023](#bib.bib1). In this approach, we assign three distinct roles—Psychologist,
    General Public, and Critic—and the three agents debate the scores that should
    be assigned to candidate responses. After the debate, each agent gives its final
    score which is used to determine which candidate response they vote for. These
    votes are then used to pick the best response. This strategy is evaluated using
    the FairEval, PandaLM-Eval, and MT-Bench benchmarks. Figure [1](#S3.F1 "Figure
    1 ‣ LLM Debate. ‣ 3.2 LLM-as-Evaluator ‣ 3 Methodology ‣ The Fellowship of the
    LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    illustrates the debate workflow employed in our study. The system prompt, the
    user message structure and the prompts for the roles used are given in Table [8](#A1.T8
    "Table 8 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") and Table
    [9](#A1.T9 "Table 9 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    in Appendix [A](#A1 "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还根据 Chan 等人[2023](#bib.bib1)描述的实现方法评估了 LLM 辩论框架。在这种方法中，我们分配了三个不同的角色——心理学家、公众和评论员——这三个代理对候选响应应该获得的分数进行辩论。辩论后，每个代理给出最终分数，该分数用于确定他们投票的候选响应。这些投票随后用于挑选最佳响应。这个策略通过
    FairEval、PandaLM-Eval 和 MT-Bench 基准进行评估。图[1](#S3.F1 "图 1 ‣ LLM 辩论。 ‣ 3.2 LLM 作为评估者
    ‣ 3 方法论 ‣ LLM 的合作：多智能体工作流程用于合成偏好优化数据集生成")展示了我们研究中采用的辩论工作流程。系统提示、用户消息结构和角色提示的详细信息见附录[A](#A1
    "附录 A 系统提示 ‣ LLM 的合作：多智能体工作流程用于合成偏好优化数据集生成")中的表格[8](#A1.T8 "表格 8 ‣ 附录 A 系统提示 ‣
    LLM 的合作：多智能体工作流程用于合成偏好优化数据集生成")和表格[9](#A1.T9 "表格 9 ‣ 附录 A 系统提示 ‣ LLM 的合作：多智能体工作流程用于合成偏好优化数据集生成")。
- en: '![Refer to caption](img/1fb608703f54e839fc2b1f7843b65f59.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1fb608703f54e839fc2b1f7843b65f59.png)'
- en: 'Figure 1: LLM Debate for evaluation'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM 辩论评估
- en: 3.3 LLM-as-Generator
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LLM 作为生成器
- en: 'To evaluate the LLM Feedback Loop workflow for the generation module, we test
    different configurations using Llama-3.1-8b (Meta [2024](#bib.bib11)) and Gemma-2-9b
    (Google [2024](#bib.bib5)) models. In this framework, a generator LLM produces
    a response, which is then evaluated by a feedback LLM that provides improvement
    suggestions as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.3 LLM-as-Generator ‣ 3
    Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation"). The generator revises the response
    based on these suggestions, and the process repeats for multiple iterations. The
    system prompt for the generator and reviewer is given in Table [10](#A1.T10 "Table
    10 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") and [11](#A1.T11 "Table
    11 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") in Appendix [A](#A1
    "Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). We calculate the win
    rate against single-agent GPT-4o (OpenAI [2024](#bib.bib12)), Llama-3.1-8b and
    Gemma-2-9b baseline outputs on a subset of 500 prompts from the Argilla Capybara
    DPO dataset²²2https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized
    to identify the best configuration. We test the following configuration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 LLM 反馈循环工作流程在生成模块中的表现，我们使用 Llama-3.1-8b (Meta [2024](#bib.bib11)) 和 Gemma-2-9b
    (Google [2024](#bib.bib5)) 模型测试不同的配置。在这个框架中，一个生成器 LLM 生成响应，然后由反馈 LLM 进行评估，提供改进建议，如图
    [2](#S3.F2 "Figure 2 ‣ 3.3 LLM-as-Generator ‣ 3 Methodology ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") 所示。生成器根据这些建议修订响应，过程会重复多次。生成器和评审员的系统提示在附录 [A](#A1 "Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") 的表格 [10](#A1.T10 "Table 10 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") 和 [11](#A1.T11 "Table 11 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") 中给出。我们计算了相对于单代理 GPT-4o (OpenAI [2024](#bib.bib12))、Llama-3.1-8b
    和 Gemma-2-9b 基准输出的胜率，测试了来自 Argilla Capybara DPO 数据集的 500 个提示的子集²²2https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized，以确定最佳配置。我们测试了以下配置：'
- en: '1.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Same Model as Both Agents: Gemma-2-9b or Llama-3.1-8b as both the feedback
    and generation agent.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相同模型作为两个代理：Gemma-2-9b 或 Llama-3.1-8b 作为反馈和生成代理。
- en: '2.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Different Models for Each Agent: Gemma-2-9b as the feedback agent and Llama-3.1-8b
    as the generation agent, or vice versa.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个代理使用不同模型：Gemma-2-9b 作为反馈代理，Llama-3.1-8b 作为生成代理，或反之亦然。
- en: '3.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Both Models for Feedback, One for Generation: Gemma-2-9b or Llama-3.1-8b as
    the generation agent, with both models as feedback agents.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两种模型用于反馈，一种用于生成：Gemma-2-9b 或 Llama-3.1-8b 作为生成代理，两个模型作为反馈代理。
- en: '![Refer to caption](img/082528037b479c44def78f56fb661d1b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/082528037b479c44def78f56fb661d1b.png)'
- en: 'Figure 2: LLM Feedback Loop for response generation'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLM 反馈循环用于响应生成
- en: 3.4 Preference Optimization Dataset
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 偏好优化数据集
- en: 'We use Llama-3.1-8b and Gemma-2-9b in the generation module and GPT-4o in the
    evaluation module to generate multiple DPO datasets and KTO datasets for single-agent
    improvement and multi-agent improvement. The prompts used for single-agent improvement
    dataset generation are given in Table [7](#A1.T7 "Table 7 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation"), [10](#A1.T10 "Table 10 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") and [11](#A1.T11 "Table 11 ‣ Appendix A System
    Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") in Appendix [A](#A1 "Appendix A System Prompts
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"). The prompt used for multi-agent improvement dataset generation
    is given in Table [12](#A1.T12 "Table 12 ‣ Appendix A System Prompts ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") in Appendix [A](#A1 "Appendix A System Prompts ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). The evaluation code, all the evaluation outputs and the generated
    datasets are publicly available on GitHub³³3https://github.com/ulrs0/MA-PO.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在生成模块中使用了Llama-3.1-8b和Gemma-2-9b，在评估模块中使用了GPT-4o，以生成多个DPO数据集和KTO数据集用于单代理改进和多代理改进。用于单代理改进数据集生成的提示见附录[A](#A1
    "附录 A 系统提示 ‣ LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")中的表[7](#A1.T7 "表 7 ‣ 附录 A 系统提示 ‣ LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")、[10](#A1.T10
    "表 10 ‣ 附录 A 系统提示 ‣ LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")和[11](#A1.T11 "表 11 ‣ 附录 A
    系统提示 ‣ LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")。用于多代理改进数据集生成的提示见附录[A](#A1 "附录 A 系统提示 ‣
    LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")中的表[12](#A1.T12 "表 12 ‣ 附录 A 系统提示 ‣ LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")。评估代码、所有评估输出和生成的数据集均可在GitHub上公开获取³³3https://github.com/ulrs0/MA-PO。
- en: Single-Agent Improvement.
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单代理改进。
- en: We use the prompts from Argilla Capybara DPO dataset. The Feedback Loop framework
    generates $N$ is the number of iterations). LLM-as-Evaluator picks the best response
    from the candidates to create the DPO and KTO dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自Argilla Capybara DPO数据集的提示。反馈循环框架生成$N$是迭代次数）。LLM作为评估者从候选响应中挑选最佳响应以创建DPO和KTO数据集。
- en: Multi-Agent Improvement.
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多代理改进。
- en: 'We use the prompts and the human-generated responses from the No Robots dataset
    (Rajani et al. [2023](#bib.bib15)). The evaluator is given the human response
    (reference response) and LLM response and is asked to generate feedback based
    on the reference response. The generated feedback and the human response are used
    to create the PO dataset. The goal of this dataset is to improve both response
    generation and response evaluation. The structure of the dataset is given in Figure
    [3](#S3.F3 "Figure 3 ‣ Multi-Agent Improvement. ‣ 3.4 Preference Optimization
    Dataset ‣ 3 Methodology ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自No Robots数据集（Rajani等 [2023](#bib.bib15)）的提示和人工生成的回应。评估者获得人工回应（参考回应）和LLM回应，并被要求基于参考回应生成反馈。生成的反馈和人工回应用于创建PO数据集。该数据集的目标是改进回应生成和回应评估。数据集的结构见图[3](#S3.F3
    "图 3 ‣ 多代理改进 ‣ 3.4 偏好优化数据集 ‣ 3 方法论 ‣ LLM的联谊会：用于合成偏好优化数据集生成的多代理工作流程")。
- en: '![Refer to caption](img/e033ba4c6cae36739220d5850c207ecc.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e033ba4c6cae36739220d5850c207ecc.png)'
- en: 'Figure 3: Dataset structure for the mult-agent improvement PO dataset. The
    blue boxes represent the accepted responses and the yellow boxes represent the
    rejected responses.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：多代理改进PO数据集的数据结构。蓝色框表示接受的回应，黄色框表示拒绝的回应。
- en: 4 Results and Discussion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与讨论
- en: 4.1 LLM-as-Evaluator
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LLM作为评估者
- en: Prompting Strategies.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示策略。
- en: 'Table [2](#S4.T2 "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣
    4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation") shows the results of LLM-as-a-Judge
    approach on the three prompting strategies. The Independent Scoring prompt strategy
    consistently under-performs compared to the Direct Comparison and Combined Scoring
    approaches across all evaluated LLMs. This result is reflected in lower Cohen’s
    Kappa values in Table [2](#S4.T2 "Table 2 ‣ Prompting Strategies. ‣ 4.1 LLM-as-Evaluator
    ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). In evaluating responses
    in isolation the LLM has to re-calibrate its scoring mechanism for every new response.
    This can lead to inconsistencies, especially when multiple responses are closely
    matched in quality. Due to the low Kappa values observed, we opted not to conduct
    experiments with the scoring-out-of-5 and 100 scales for Independent Scoring.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 提示策略。 ‣ 4.1 LLM作为评估者 ‣ 4 结果与讨论 ‣ LLM联盟：用于合成偏好优化数据集生成的多代理工作流")
    显示了LLM作为评估者在三种提示策略上的结果。独立评分提示策略在所有评估的LLM中始终表现不如直接比较和组合评分方法。这一结果反映在表 [2](#S4.T2
    "表 2 ‣ 提示策略。 ‣ 4.1 LLM作为评估者 ‣ 4 结果与讨论 ‣ LLM联盟：用于合成偏好优化数据集生成的多代理工作流") 中较低的Cohen's
    Kappa值。在孤立评估响应时，LLM需要为每个新响应重新校准其评分机制。这可能导致不一致，特别是在多个响应质量接近时。由于观察到的Kappa值较低，我们选择不进行使用评分满分5和100的独立评分实验。
- en: The Direct Comparison Strategy performs better than the Independent Scoring
    approach across most LLMs, with a notable improvement for GPT-4o (0.372 vs. 0.249)
    and GPT-4o-mini (0.342 vs. 0.254). However, it generally falls short when compared
    to the Combined Scoring method, where GPT-4o achieves a score of 0.401 using the
    scoring-out-of-100 scale. The higher Cohen’s Kappa values indicate that the Direct
    Comparison and Combined Scoring strategy benefits from providing the LLM with
    a side-by-side evaluation of responses, allowing for more accurate and consistent
    judgments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 直接比较策略在大多数LLM中表现优于独立评分方法，特别是在GPT-4o（0.372 vs. 0.249）和GPT-4o-mini（0.342 vs. 0.254）上有显著改进。然而，与组合评分方法相比，直接比较策略通常效果较差，在组合评分方法中，GPT-4o使用评分满分100的标准取得了0.401的分数。较高的Cohen's
    Kappa值表明，直接比较和组合评分策略通过提供LLM并排评估响应的方式，从而使判断更准确、一致。
- en: 'The Combined Scoring strategy, as presented in Table [2](#S4.T2 "Table 2 ‣
    Prompting Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The
    Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation"), shows consistent performance using all the scoring scales.
    It outperforms both the other prompts. The scoring scales of 5, 10, and 100 show
    variability across different models, with certain scales performing better for
    some models than others. For example, GPT-4o performs the best in scoring-out-of-10
    scale with a Kappa score of 0.382 while Gemma-2-9b performs best under scoring-out-of-5
    scale. Given these results, we selected the scoring-out-of-10 scale as the most
    effective option for the Combined Scoring approach. We use this prompt for all
    our further evaluations.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 组合评分策略如表 [2](#S4.T2 "表 2 ‣ 提示策略。 ‣ 4.1 LLM作为评估者 ‣ 4 结果与讨论 ‣ LLM联盟：用于合成偏好优化数据集生成的多代理工作流")
    中所示，在所有评分尺度上表现一致。它优于其他提示。评分尺度5、10和100在不同模型中显示出变异性，某些尺度在一些模型上表现更好。例如，GPT-4o在评分满分10的尺度上表现最佳，Kappa值为0.382，而Gemma-2-9b在评分满分5的尺度上表现最佳。根据这些结果，我们选择了评分满分10的尺度作为组合评分方法的最有效选项。我们将使用这一提示进行所有进一步的评估。
- en: '|  | Comp. | Ind. | Combined |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 比较 | 独立 | 组合 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Judge |  | 10 | 5 | 10 | 100 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Judge |  | 10 | 5 | 10 | 100 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Gemma-2-9b | 0.226 | 0.170 | 0.243 | 0.254 | 0.233 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b | 0.226 | 0.170 | 0.243 | 0.254 | 0.233 |'
- en: '| Llama-3.1-8b | 0.265 | 0.181 | 0.255 | 0.240 | 0.242 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8b | 0.265 | 0.181 | 0.255 | 0.240 | 0.242 |'
- en: '| Gemma-2-27b | 0.233 | 0.173 | 0.284 | 0.266 | 0.252 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b | 0.233 | 0.173 | 0.284 | 0.266 | 0.252 |'
- en: '| Llama-3.1-70b | 0.305 | 0.214 | 0.337 | 0.333 | 0.339 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b | 0.305 | 0.214 | 0.337 | 0.333 | 0.339 |'
- en: '| GPT-4o-mini | 0.342 | 0.254 | 0.374 | 0.382 | 0.347 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 0.342 | 0.254 | 0.374 | 0.382 | 0.347 |'
- en: '| GPT-4o | 0.372 | 0.249 | 0.393 | 0.382 | 0.401 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.372 | 0.249 | 0.393 | 0.382 | 0.401 |'
- en: 'Table 2: Performance comparison of LLM-as-a-Judge on Alpaca-Eval using different
    prompting strategies. Direct Comparison (Comp.) vs. Independent Scoring (Ind.)
    vs. Combined Scoring (Combined). The bold values indicate the highest Cohen’s
    kappa values for a particular strategy.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用不同提示策略对Alpaca-Eval上LLM作为评审的性能比较。直接比较（Comp.）与独立评分（Ind.）与综合评分（Combined）。粗体值表示某一策略的最高Cohen’s
    kappa值。
- en: LLM-as-a-Judge.
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM作为评审。
- en: 'The LLM-as-Judge evaluations, as shown in Table [2](#S4.T2 "Table 2 ‣ Prompting
    Strategies. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"), indicate that GPT-4o outperforms all the models on PandaLM-Eval
    and MT-Bench achieving a Cohen’s Kappa score of 0.688 and 0.410 respectively.
    Additionally, GPT-4o consistently ranks in second position across all three datasets.
    This consistent top-tier performance underscores GPT’s effectiveness as a reliable
    judge in evaluating LLM responses. Gemma-2-27b outperforms all other models on
    the Fair-Eval dataset, achieving the highest score in this particular evaluation.
    However, it’s important to note that the Fair-Eval dataset is relatively small,
    consisting of only 80 samples. Furthermore, the Fair-Eval dataset primarily compares
    GPT-3.5-Turbo with Vicuna-13b, which might introduce a bias in favor of GPT models
    when GPT is the evaluator. Figure [4](#S4.F4 "Figure 4 ‣ LLM-as-a-Judge. ‣ 4.1
    LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") shows that
    GPT-4o selects GPT-3.5-Turbo as the better agent 50 times and Vicuna-13b 30 times.
    This indicates a potential bias in favor of GPT responses when GPT-4o is the evaluator.
    Additionally, we can observe in the figure that Llama models also display a similar
    bias towards GPT responses, whereas Gemma models do not exhibit this bias, suggesting
    that Gemma is more impartial in its evaluations.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S4.T2 "表2 ‣ 提示策略 ‣ 4.1 LLM作为评审 ‣ 4 结果与讨论 ‣ LLM的联盟：合成偏好优化数据集生成的多代理工作流程")所示，LLM作为评审的评估表明，GPT-4o在PandaLM-Eval和MT-Bench上优于所有模型，分别达到了0.688和0.410的Cohen’s
    Kappa分数。此外，GPT-4o在所有三个数据集中始终排名第二。这种持续的顶级表现突显了GPT作为评审在评估LLM回应中的有效性。Gemma-2-27b在Fair-Eval数据集上优于所有其他模型，获得了这一特定评估的最高分。然而，需要注意的是，Fair-Eval数据集相对较小，仅包含80个样本。此外，Fair-Eval数据集主要比较GPT-3.5-Turbo与Vicuna-13b，这可能在GPT作为评审时引入了对GPT模型的偏见。图[4](#S4.F4
    "图4 ‣ LLM作为评审 ‣ 4.1 LLM作为评审 ‣ 4 结果与讨论 ‣ LLM的联盟：合成偏好优化数据集生成的多代理工作流程")显示，GPT-4o选择GPT-3.5-Turbo作为更好的代理50次，选择Vicuna-13b
    30次。这表明在GPT-4o作为评审时可能存在对GPT回应的偏见。此外，我们可以在图中观察到Llama模型也显示出对GPT回应的类似偏见，而Gemma模型没有这种偏见，这表明Gemma在评估中更为公正。
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
- en: '| --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Judge |  |  |  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 评审 |  |  |  |'
- en: '| Gemma-2-9b | 0.279 | 0.595 | 0.354 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b | 0.279 | 0.595 | 0.354 |'
- en: '| Llama-3.1-8b | 0.206 | 0.523 | 0.339 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8b | 0.206 | 0.523 | 0.339 |'
- en: '| Gemma-2-27b | 0.389 | 0.586 | 0.354 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b | 0.389 | 0.586 | 0.354 |'
- en: '| Llama-3.1-70b | 0.257 | 0.597 | 0.387 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b | 0.257 | 0.597 | 0.387 |'
- en: '| GPT-4o-mini | 0.333 | 0.613 | 0.388 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 0.333 | 0.613 | 0.388 |'
- en: '| GPT-4o | 0.327 | 0.688 | 0.410 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.327 | 0.688 | 0.410 |'
- en: 'Table 3: Performance comparison of LLM-as-a-Judge on Alpaca-Eval using different
    prompting strategies. Direct Comparison vs. Independent Scoring (out of 10) vs.
    Combined Scoring (out of 5, 10 and 100).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用不同提示策略对Alpaca-Eval上LLM作为评审的性能比较。直接比较与独立评分（满分10）与综合评分（满分5、10和100）。
- en: '![Refer to caption](img/0803c477727bc09f4e7058ea4fbed36d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0803c477727bc09f4e7058ea4fbed36d.png)'
- en: 'Figure 4: Number of times GPT-3.5-Turbo and Vicuna-13b are picked by each LLM
    Judge.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：每个LLM评审选择GPT-3.5-Turbo和Vicuna-13b的次数。
- en: LLMs-as-a-Jury.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs作为评审。
- en: 'In evaluating of LLMs-as-a-Jury, we analyze the top three juries from each
    dataset as shown in Table [4](#S4.T4 "Table 4 ‣ LLMs-as-a-Jury. ‣ 4.1 LLM-as-Evaluator
    ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation"). Notably, the scores
    exhibit considerable variation across the different datasets. On the Fair-Eval
    and MT-Bench datasets, the jury approach outperformed the judge approach, indicating
    a potential advantage in using multiple models for evaluation. For instance, on
    Fair-Eval, the highest-performing jury achieves a Cohen’s Kappa of 0.428 while
    the judge achieves Kappa of 0.389, suggesting a relatively strong agreement with
    human judgments compared to individual judges. This configuration, however, shows
    a drop in performance on other datasets with a kappa of 0.604 on PandaLM-Eval
    and 0.395 on MT-Bench, underscoring the challenge of generalizing a single jury
    setup across varied datasets. However, the judge approach outperforms the jury
    on the PandaLM-Eval dataset, where the best judge attained a kappa of 0.688, surpassing
    the top jury’s kappa of 0.673\. The best jury on MT-Bench, with a kappa of 0.429,
    also demonstrates variability in its performance across datasets as well, with
    a kappa of 0.636 on PandaLM-Eval and only 0.273 on Fair-Eval.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '在LLMs作为评审的评估中，我们分析了每个数据集中的前三名评审，如表[4](#S4.T4 "Table 4 ‣ LLMs-as-a-Jury. ‣ 4.1
    LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation")所示。值得注意的是，这些分数在不同的数据集上表现出显著的变化。在Fair-Eval和MT-Bench数据集上，评审团方法优于评审员方法，表明使用多个模型进行评估可能具有潜在优势。例如，在Fair-Eval上，表现最佳的评审团获得了0.428的Cohen’s
    Kappa，而评审员获得了0.389的Kappa，表明与人类判断的相对一致性较强。然而，这种配置在其他数据集上表现下降，PandaLM-Eval上的Kappa为0.604，MT-Bench上的Kappa为0.395，突显了在不同数据集上泛化单一评审团设置的挑战。然而，评审员在PandaLM-Eval数据集上的表现优于评审团，其中最佳评审员获得了0.688的Kappa，超越了最佳评审团的0.673的Kappa。最佳评审团在MT-Bench上的Kappa为0.429，在数据集上的表现也显示出变异性，在PandaLM-Eval上Kappa为0.636，而在Fair-Eval上仅为0.273。'
- en: 'The jury approach, by incorporating diverse models, mitigates the biases that
    occur in LLM-as-a-Judge approach (as shown in Figure [4](#S4.F4 "Figure 4 ‣ LLM-as-a-Judge.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation"))
    when bench-marking on the Fair-Eval dataset. However while the jury approach can
    offer robustness through diversity, in evaluation task, it does not universally
    outperform single judges. The decision to employ a jury versus a judge should
    consider whether the candidate responses being evaluated include output from the
    judge itself, which can introduce bias in the results. Additionally, scalability
    should be taken into account, as the jury approach might require more computational
    resources. Another critical consideration is the variability in performance across
    different datasets, which poses a challenge for generalization.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '评审团方法通过整合多样化模型，减轻了LLM作为评审方法中出现的偏差（如图[4](#S4.F4 "Figure 4 ‣ LLM-as-a-Judge.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")所示）在Fair-Eval数据集上的基准测试。然而，尽管评审团方法通过多样性提供了稳健性，在评估任务中，它并不总是优于单一评审。选择使用评审团还是评审员应考虑被评估的候选响应是否包含来自评审员本身的输出，这可能会引入结果偏差。此外，还应考虑可扩展性，因为评审团方法可能需要更多的计算资源。另一个关键因素是不同数据集的表现差异，这对泛化提出了挑战。'
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
- en: '| Jury |  |  |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Jury |  |  |  |'
- en: '| Gemma-2-9b, Gemma-2-27b, Llama-3.1-8b, GPT-4o-mini | 0.428 | 0.604 | 0.395
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, Gemma-2-27b, Llama-3.1-8b, GPT-4o-mini | 0.428 | 0.604 | 0.395
    |'
- en: '| Gemma-2-9b, Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.415 | 0.639 | 0.418 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.415 | 0.639 | 0.418 |'
- en: '| Gemma-2-27b, Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.412 | 0.637 | 0.410 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b, Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.412 | 0.637 | 0.410 |'
- en: '| Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.396 | 0.673 | 0.400 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b, GPT-4o-mini, GPT-4o | 0.396 | 0.673 | 0.400 |'
- en: '| Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.365 | 0.663 | 0.410 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b, GPT-4o-mini, GPT-4o | 0.365 | 0.663 | 0.410 |'
- en: '| Gemma-2-9b, GPT-4o-mini, GPT-4o | 0.375 | 0.662 | 0.416 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, GPT-4o-mini, GPT-4o | 0.375 | 0.662 | 0.416 |'
- en: '| Llama-3.1-70b, GPT-4o | 0.273 | 0.636 | 0.429 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b, GPT-4o | 0.273 | 0.636 | 0.429 |'
- en: '| GPT-4o-mini, GPT-4o | 0.315 | 0.660 | 0.426 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini, GPT-4o | 0.315 | 0.660 | 0.426 |'
- en: '| Gemma-2-9b, GPT-4o | 0.290 | 0.609 | 0.422 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b, GPT-4o | 0.290 | 0.609 | 0.422 |'
- en: 'Table 4: Performance comparison of LLMs-as-a-Jury on the three datasets. For
    each dataset, we pick the top 3 juries. The bold score is for the best jury for
    the specific dataset and the underlined one is the second best.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在三个数据集上 LLMs-as-a-Jury 的性能比较。对于每个数据集，我们选择前三名评审。粗体分数为特定数据集中最佳的评审，带下划线的是第二名。
- en: LLM Debate.
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM Debate.
- en: 'The LLM Debate approach, as summarized in Table [5](#S4.T5 "Table 5 ‣ LLM Debate.
    ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation"),
    showcases varying degrees of effectiveness across three different datasets: Fair-Eval,
    PandaLM-Eval, and MT-Bench. GPT-4o performs the best across all datasets, with
    Cohen’s Kappa scores of 0.404, 0.654, and 0.402 respectively. LLM Debate outperforms
    LLM-as-a-Judge on Fair-Eval only and does not surpass the LLMs-as-a-Jury approach
    on any dataset. On Fair-Eval using the Debate framework increases the Kappa score
    of GPT-4o from 0.327 to 0.404 and of GPT-4o-mini from 0.333 to 0.360\. It shows
    that the debate approach decreases the bias of GPT-4o and GPT-4o-mini towards
    the responses of it’s family.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM Debate 方法，如表[5](#S4.T5 "Table 5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣
    4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows for
    Synthetic Preference Optimization Dataset Generation")中总结，展示了在三个不同数据集（Fair-Eval、PandaLM-Eval
    和 MT-Bench）上的不同效果。GPT-4o 在所有数据集中的表现最佳，Cohen’s Kappa 分数分别为 0.404、0.654 和 0.402。LLM
    Debate 仅在 Fair-Eval 上优于 LLM-as-a-Judge，并且在任何数据集上都没有超过 LLMs-as-a-Jury 方法。在 Fair-Eval
    上使用辩论框架使 GPT-4o 的 Kappa 分数从 0.327 提高到 0.404，GPT-4o-mini 从 0.333 提高到 0.360。这表明，辩论方法减少了
    GPT-4o 和 GPT-4o-mini 对其家族回应的偏见。'
- en: 'There is a significant variance in the performance of LLM Debate across the
    models and the datasets. For instance, as seen in Table [5](#S4.T5 "Table 5 ‣
    LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship
    of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") Gemma-2-27b in debate architecture outperforms Gemma-as-a-Judge on
    PandaLM-Eval and MT-Bench but on Fair-Eval judge performers better. Gemma-2-9b
    in debate architecture has a Kappa score of 0.323 on Fair-Eval, outperforming
    0.279 of Gemma-as-a-Judge. However on PandaLM-Eval and MT-Bench Gemma-2-9b in
    debate framework achieves a Kappa score of 0.520 and 0.326, repectively. Both
    scores lower as compared to Gemma-as-a-Judge scores of 0.595 and 0.354\. In case
    of Llama, Llama-3.1-8b in judge configuration outperforms itself in debate configuration.
    Llama-3.1-70b in debate framework only outperforms Llama-as-a-judge on Fair-Eval.
    Figure [5](#S4.F5 "Figure 5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator ‣ 4 Results and
    Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference
    Optimization Dataset Generation") shows a comparison of Cohen’s Kappa of LLM Debate
    and LLM-as-a-Judge across the three datasets and all the models.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在不同模型和数据集之间，LLM Debate 的表现存在显著差异。例如，在表[5](#S4.T5 "Table 5 ‣ LLM Debate. ‣ 4.1
    LLM-as-Evaluator ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation")中所示，Gemma-2-27b
    在辩论架构中在 PandaLM-Eval 和 MT-Bench 上的表现优于 Gemma-as-a-Judge，但在 Fair-Eval 上表现更佳。Gemma-2-9b
    在辩论架构中在 Fair-Eval 上的 Kappa 分数为 0.323，优于 Gemma-as-a-Judge 的 0.279。然而，在 PandaLM-Eval
    和 MT-Bench 上，Gemma-2-9b 在辩论框架中的 Kappa 分数分别为 0.520 和 0.326，均低于 Gemma-as-a-Judge
    的 0.595 和 0.354。就 Llama 而言，Llama-3.1-8b 在裁判配置下的表现优于其在辩论配置下的表现。Llama-3.1-70b 在辩论框架下仅在
    Fair-Eval 上优于 Llama-as-a-Judge。图[5](#S4.F5 "Figure 5 ‣ LLM Debate. ‣ 4.1 LLM-as-Evaluator
    ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") 显示了 LLM Debate 和 LLM-as-a-Judge
    在三个数据集和所有模型中的 Cohen’s Kappa 比较。'
- en: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | Fair-Eval | PandaLM-Eval | MT-Bench |'
- en: '| --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Debater |  |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 辩论者 |  |  |  |'
- en: '| Gemma-2-9b | 0.323 | 0.520 | 0.326 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-9b | 0.323 | 0.520 | 0.326 |'
- en: '| Llama-3.1-8b | 0.080 | 0.440 | 0.309 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-8b | 0.080 | 0.440 | 0.309 |'
- en: '| Gemma-2-27b | 0.336 | 0.605 | 0.363 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b | 0.336 | 0.605 | 0.363 |'
- en: '| Llama-3.1-70b | 0.292 | 0.547 | 0.381 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3.1-70b | 0.292 | 0.547 | 0.381 |'
- en: '| GPT-4o-mini | 0.360 | 0.625 | 0.376 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-mini | 0.360 | 0.625 | 0.376 |'
- en: '| GPT-4o | 0.404 | 0.654 | 0.402 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.404 | 0.654 | 0.402 |'
- en: 'Table 5: Performance comparison of LLM Debate on the three datasets.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在三个数据集上 LLM Debate 的性能比较。
- en: '![Refer to caption](img/1c1a734471fb93f29e4fae79555da3e1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c1a734471fb93f29e4fae79555da3e1.png)'
- en: 'Figure 5: Comparison of LLM Debate and LLM-as-a-Judge across the three datasets
    and different models.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在三个数据集和不同模型中，LLM Debate 和 LLM-as-a-Judge 的比较。
- en: Evaluation Framework for PO Dataset.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PO 数据集的评估框架。
- en: 'Based on the comparative evaluation scores across the three datasets and the
    advantages and disadvantages associated with each multi-agent framework, we have
    chosen to use the LLM-as-a-Judge approach with GPT-4o as our primary evaluator
    for generating the PO dataset. This decision is driven by multiple factors:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据三种数据集上的比较评估分数以及每种多代理框架的优缺点，我们选择了使用 LLM-as-a-Judge 方法，GPT-4o 作为主要评估者来生成 PO
    数据集。这一决策受到多个因素的驱动：
- en: '1.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: In our context, the task involves generating a PO dataset using Llama-3.1-8b
    and Gemma-2-9b. Therefore there will be no bias in the evaluation when using GPT-4o
    as the judge.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的背景下，任务涉及使用 Llama-3.1-8b 和 Gemma-2-9b 生成 PO 数据集。因此，在使用 GPT-4o 作为评审时，评估不会存在偏差。
- en: '2.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The performance of GPT-4o-as-a-Judge has been consistently high across various
    evaluations, indicating its reliability as a judge. While the LLMs-as-a-Jury and
    LLM Debate approaches have a high variance in Cohen’s Kappa score across different
    datasets.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**GPT-4o-as-a-Judge** 的表现一直在各种评估中保持高水平，显示了其作为评审者的可靠性。与此相比，LLMs-as-a-Jury 和
    LLM Debate 方法在不同数据集上的 Cohen’s Kappa 分数具有较高的变异性。'
- en: '3.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The computational resources required for managing the LLM Debate and LLM Jury
    frameworks are considerably higher than those needed for a single-judge setup.
    The LLM-as-a-Judge method is simpler to implement and scale.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管理 LLM Debate 和 LLM Jury 框架所需的计算资源明显高于单一评审设置所需的资源。LLM-as-a-Judge 方法更易于实施和扩展。
- en: 4.2 LLM-as-Generator
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LLM-as-Generator
- en: 'We compare the performance of Multi-Agent Feedback Loop with the baseline Single-Agents
    (GPT-4o, Llama-3.1-8b, Gemma-2-9b) using win rate as shown in Table [6](#S4.T6
    "Table 6 ‣ 4.2 LLM-as-Generator ‣ 4 Results and Discussion ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation"). We utilize GPT-4o-as-a-judge in this evaluation process. For the
    baseline we find the win rate of Gemma and Llama against GPT-4o and each other.
    Both smaller models have similar win rate of 38.6% and 39.2% against GPT, while
    Gemma has a win rate of 66.6% against Llama.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将多代理反馈循环的表现与基线单一代理（GPT-4o、Llama-3.1-8b、Gemma-2-9b）的表现进行比较，使用胜率进行展示，如表格 [6](#S4.T6
    "Table 6 ‣ 4.2 LLM-as-Generator ‣ 4 Results and Discussion ‣ The Fellowship of
    the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset
    Generation") 所示。我们在此评估过程中使用 GPT-4o 作为评审者。对于基线，我们发现 Gemma 和 Llama 对抗 GPT-4o 以及彼此之间的胜率。两个较小的模型对抗
    GPT 的胜率相似，为 38.6% 和 39.2%，而 Gemma 对抗 Llama 的胜率为 66.6%。'
- en: In the Multi-Agent setting, all variations outperform the single-agents against
    GPT-4o, with the highest win rate of 49.0% for Llama as a generator and Gemma
    as a reviewer. This configuration performs the best against Llama and Gemma too,
    with 71.8% and 73.8% win rate respectively. We observe that using Llama as the
    generator improves the performance as compared to using Gemma as the generator
    because this configuration leads to a better win rate against all three baselines.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在多代理设置中，所有变体相较于单一代理在 GPT-4o 的表现上都优于单一代理，其中 Llama 作为生成器和 Gemma 作为评审者的配置具有最高的胜率，为
    49.0%。该配置在 Llama 和 Gemma 的对比中表现最佳，胜率分别为 71.8% 和 73.8%。我们观察到，使用 Llama 作为生成器相比于使用
    Gemma 作为生成器能提高表现，因为这一配置在对抗所有三个基线时胜率更高。
- en: Llama’s strengths in generating responses may be enhanced by Gemma’s ability
    to fine-tune and correct the errors, leading to more polished outputs. The results
    underscore the importance of assigning appropriate roles based on the specific
    strengths of each model. Llama, when set as the generator, appears to leverage
    its capabilities more effectively than Gemma in this role. The use of diverse
    models in the feedback loop likely helps mitigate biases that any single model
    might introduce. This diversity ensures a broader range of perspectives while
    answer a question. In conclusion, the demonstrated efficacy of the Multi-Agent
    Feedback Loop, especially with Llama as the generator and Gemma as the reviewer,
    validates the concept of collaborative AI systems.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 在生成响应方面的优势可以通过 Gemma 的微调和纠错能力得到增强，从而产生更为精细的输出。结果强调了根据每个模型的具体优势分配适当角色的重要性。Llama
    作为生成器时，似乎比 Gemma 更有效地发挥了其能力。反馈循环中使用不同模型可能有助于减轻任何单一模型可能引入的偏差。这种多样性确保了在回答问题时能够获得更广泛的观点。总之，多代理反馈循环的有效性，特别是
    Llama 作为生成器和 Gemma 作为评审者的配置，验证了协作 AI 系统的概念。
- en: '|  |  | Win Rate (%) Against |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 胜率 (%) 对抗 |'
- en: '| Generator | Reviewer | GPT | Llama | Gemma |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 生成器 | 评审者 | GPT | Llama | Gemma |'
- en: '| Gemma | - | 38.6 | 66.6 | - |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Gemma | - | 38.6 | 66.6 | - |'
- en: '| Llama | - | 39.2 | - | 33.4 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Llama | - | 39.2 | - | 33.4 |'
- en: '| Gemma | Gemma | 41.4 | 64.8 | 52.6 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Gemma | Gemma | 41.4 | 64.8 | 52.6 |'
- en: '|  | Llama | 41.2 | 61.8 | 47.8 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama | 41.2 | 61.8 | 47.8 |'
- en: '|  | Gemma + Llama | 42.0 | 67.6 | 52.4 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | Gemma + Llama | 42.0 | 67.6 | 52.4 |'
- en: '| Llama | Gemma | 49.0 | 71.8 | 73.8 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Llama | Gemma | 49.0 | 71.8 | 73.8 |'
- en: '|  | Llama | 47.8 | 65.8 | 65.6 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama | 47.8 | 65.8 | 65.6 |'
- en: '|  | Gemma + Llama | 48.6 | 68.2 | 69.4 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | Gemma + Llama | 48.6 | 68.2 | 69.4 |'
- en: 'Table 6: Win Rate of Multi-Agent and Single-Agent against GPT-4o, Llama-3.1-8b
    and Gemma-2-9b'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：多智能体和单智能体对GPT-4o、Llama-3.1-8b和Gemma-2-9b的胜率
- en: 4.3 Preference Optimization Dataset
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 偏好优化数据集
- en: Single-Agent Improvement.
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单智能体改进。
- en: 'For the Single-Agent improvement dataset generation we use GPT-4o-as-a-Judge
    in the evaluation module. In the generation module, we use LLM Feedback Loop with
    Llama-3.1-8b as the generator and Gemma-2-9b as the reviewer. The framework is
    shown in Figure [6](#S4.F6 "Figure 6 ‣ Single-Agent Improvement. ‣ 4.3 Preference
    Optimization Dataset ‣ 4 Results and Discussion ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation").
    For the dataset generation, we use $N=3$ iterations. We pick the best response
    (judged by GPT-4o) as accepted and the other 2 responses as rejected and generate
    two datasets, one for DPO and one for KTO.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单智能体改进数据集的生成，我们在评估模块中使用GPT-4o-as-a-Judge。在生成模块中，我们使用LLM反馈循环，Llama-3.1-8b作为生成器，Gemma-2-9b作为审阅者。框架如图[6](#S4.F6
    "图6 ‣ 单智能体改进 ‣ 4.3 偏好优化数据集 ‣ 4 结果与讨论 ‣ LLM的联盟：用于合成偏好优化数据集生成的多智能体工作流程")所示。对于数据集生成，我们使用$N=3$次迭代。我们选取最佳回应（由GPT-4o判断）作为接受的回应，另外两个回应作为被拒绝的回应，并生成两个数据集，一个用于DPO，一个用于KTO。
- en: '![Refer to caption](img/94bcdaac3032df199db2173448a407f9.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94bcdaac3032df199db2173448a407f9.png)'
- en: 'Figure 6: Multi-agent framework for PO dataset generation.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：用于PO数据集生成的多智能体框架。
- en: Multi-Agent Improvement.
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多智能体改进。
- en: For the Multi-Agent improvement dataset generation we use Gemma-2-9b because
    the win rate for Gemma against Llama-3.1-8b is 66.6%. We give Gemma user prompt,
    human response and it’s own response and ask it to generate feedback based on
    the human reference. For the rejected responses we use the feedback without human
    reference. We generate two datasets, one for DPO and one for KTO.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多智能体改进数据集的生成，我们使用**Gemma-2-9b**，因为Gemma对抗Llama-3.1-8b的胜率为66.6%。我们给Gemma用户提示、人工回应和它自己的回应，并要求它基于人工参考生成反馈。对于被拒绝的回应，我们使用没有人工参考的反馈。我们生成了两个数据集，一个用于DPO，一个用于KTO。
- en: Finally, we combine the two categories of dataset to make our final PO dataset
    to improve LLM for single-agent setting and LLM for multi-agent setting.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将这两类数据集合并，形成最终的PO数据集，以改善单智能体设置下的LLM和多智能体设置下的LLM。
- en: 5 Conclusion
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This paper presents PO datasets generated using multi-agent frameworks, and
    evaluates these frameworks by highlighting the advantages, drawbacks, and challenges
    of each approach. In the response evaluation module, our comparative analysis
    of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate shows the suitability of each
    setup depending on the context of use. For the response generation module, we
    evaluate the LLM Feedback loop using Llama-3.1-8b and Gemma-2-9b in various configurations.
    LLM-as-a-Judge proved to be highly effective when candidate responses don’t have
    a response from the Judge LLM. Whereas LLMs-as-a-Jury and LLM Debate demonstrated
    robustness, particularly useful in reducing evaluator bias. However, Cohen’s Kappa
    for both of these approaches has a high variance making them less suitable for
    novel applications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了使用多智能体框架生成的PO数据集，并通过突出每种方法的优缺点和挑战来评估这些框架。在回应评估模块中，我们对LLM-as-a-Judge、LLMs-as-a-Jury和LLM
    Debate的比较分析显示了每种设置在使用上下文中的适用性。对于回应生成模块，我们在不同配置下评估了使用Llama-3.1-8b和Gemma-2-9b的LLM反馈循环。当候选回应没有来自Judge
    LLM的回应时，LLM-as-a-Judge被证明非常有效。而LLMs-as-a-Jury和LLM Debate展示了鲁棒性，特别是在减少评估者偏见方面非常有用。然而，这两种方法的Cohen’s
    Kappa有较高的方差，使它们在新应用中不太适用。
- en: Our experiments with LLM Feedback Loop using Llama-3.1-8b and Gemma-2-9b configurations
    show the potential of multi-agent frameworks in refined content generation. Configurations
    where Llama-3.1-8b served as the generator and Gemma-2-9b as the reviewer consistently
    delivered better results, demonstrating the benefits of leveraging complementary
    strengths of different models to refine output quality. These findings indicate
    the effectiveness of multi-agent frameworks for varied AI applications, showing
    promise for moving towards systems requiring minimal human intervention - however,
    this method is computationally expensive in comparison.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Llama-3.1-8b 和 Gemma-2-9b 配置的 LLM 反馈回路实验展示了多代理框架在精细内容生成中的潜力。Llama-3.1-8b
    作为生成器，Gemma-2-9b 作为评审者的配置持续提供更好的结果，展示了利用不同模型互补优势以提高输出质量的好处。这些发现表明，多代理框架在各种 AI
    应用中具有有效性，并显示出在向要求最小人工干预的系统发展方面的前景 - 然而，相比之下，该方法计算成本较高。
- en: We also generate multiple DPO and KPO datasets using LLM Feedback Loop with
    Llama-3.1-8b as the generator and Gemma-2-9b as the evaluator and GPT-4o-as-a-Judge.
    The aim of these datasets is to improve single-agent capabilities for better response
    generation and multi-agent capabilities including better communication and improved
    feedback.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用 LLM 反馈回路生成多个 DPO 和 KPO 数据集，其中 Llama-3.1-8b 作为生成器，Gemma-2-9b 作为评估者，GPT-4o
    作为裁判。这些数据集的目的是提高单一代理能力以生成更好的响应，以及提高多代理能力，包括更好的沟通和改进的反馈。
- en: In order to facilitate further research and ensure transparency, all code, LLM
    responses, and generated datasets have been made public.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进进一步研究并确保透明性，所有代码、LLM 回复和生成的数据集已公开。
- en: 6 Future Work
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来工作
- en: 'In terms of future work, there are three avenues of investigation: (1) Performance
    comparison of models fine-tuned on our PO dataset versus widely-used LLMs to investigate
    the impact of our generated datasets through a series of experiments. (2) Using
    larger models such as Llama-3.1-70b and Gemma-2-27b for dataset generation as
    this may provide more diverse and higher-quality training data, potentially leading
    to further advancements in model performance and generalizability. (3) Experimenting
    with the number of iterations used in the Feedback Loop framework and including
    other LLM families in the dataset generation process.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 就未来的工作而言，有三个研究方向：（1）对我们 PO 数据集上微调的模型与广泛使用的 LLM 进行性能比较，通过一系列实验调查我们生成的数据集的影响。（2）使用像
    Llama-3.1-70b 和 Gemma-2-27b 这样的更大模型进行数据集生成，因为这可能提供更为多样和高质量的训练数据，可能会导致模型性能和泛化能力的进一步提升。（3）实验反馈回路框架中使用的迭代次数，并将其他
    LLM 家族纳入数据集生成过程。
- en: References
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chan et al. (2023) Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang, S.;
    Fu, J.; and Liu, Z. 2023. ChatEval: Towards Better LLM-based Evaluators through
    Multi-Agent Debate. arXiv:2308.07201.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan 等（2023）Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang, S.; Fu,
    J.; 和 Liu, Z. 2023. ChatEval: 通过多代理辩论提升 LLM 评估器的质量。arXiv:2308.07201。'
- en: Christiano et al. (2017) Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.;
    Legg, S.; and Amodei, D. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等（2017）Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg,
    S.; 和 Amodei, D. 2017. 从人类偏好中进行深度强化学习。*神经信息处理系统的进展*，30。
- en: Du et al. (2023) Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch,
    I. 2023. Improving Factuality and Reasoning in Language Models through Multiagent
    Debate. arXiv:2305.14325.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2023）Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; 和 Mordatch, I. 2023.
    通过多代理辩论提升语言模型的事实性和推理能力。arXiv:2305.14325。
- en: 'Ethayarajh et al. (2024) Ethayarajh, K.; Xu, W.; Muennighoff, N.; Jurafsky,
    D.; and Kiela, D. 2024. KTO: Model Alignment as Prospect Theoretic Optimization.
    arXiv:2402.01306.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ethayarajh 等（2024）Ethayarajh, K.; Xu, W.; Muennighoff, N.; Jurafsky, D.; 和
    Kiela, D. 2024. KTO: 模型对齐作为前景理论优化。arXiv:2402.01306。'
- en: 'Google (2024) Google. 2024. Google Gemma 2. https://blog.google/technology/developers/google-gemma-2/.
    Accessed: 2024-08-16.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google（2024）Google. 2024. Google Gemma 2. https://blog.google/technology/developers/google-gemma-2/.
    访问日期：2024-08-16。
- en: 'He et al. (2020) He, J.; Wang, T.; Xiong, D.; and Liu, Q. 2020. The Box is
    in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation. In
    Cohn, T.; He, Y.; and Liu, Y., eds., *Findings of the Association for Computational
    Linguistics: EMNLP 2020*, 3662–3672\. Online: Association for Computational Linguistics.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2020）He, J.; Wang, T.; Xiong, D.; 和 Liu, Q. 2020. 箱子在笔中：评估神经机器翻译中的常识推理。见
    Cohn, T.; He, Y.; 和 Liu, Y., 编，《计算语言学协会会议记录：EMNLP 2020》，3662–3672。在线：计算语言学协会。
- en: 'Hong, Lee, and Thorne (2024) Hong, J.; Lee, N.; and Thorne, J. 2024. ORPO:
    Monolithic Preference Optimization without Reference Model. arXiv:2403.07691.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong, Lee 和 Thorne（2024）Hong, J.; Lee, N.; 和 Thorne, J. 2024. ORPO：无参考模型的整体偏好优化。arXiv:2403.07691。
- en: 'Li et al. (2024) Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu,
    B.; Gonzalez, J. E.; and Stoica, I. 2024. From Crowdsourced Data to High-Quality
    Benchmarks: Arena-Hard and BenchBuilder Pipeline. arXiv:2406.11939.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2024）Li, T.; Chiang, W.-L.; Frick, E.; Dunlap, L.; Wu, T.; Zhu, B.; Gonzalez,
    J. E.; 和 Stoica, I. 2024. 从众包数据到高质量基准：Arena-Hard 和 BenchBuilder 流水线。arXiv:2406.11939。
- en: 'Li et al. (2023) Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin,
    C.; Liang, P.; and Hashimoto, T. B. 2023. AlpacaEval: An Automatic Evaluator of
    Instruction-following Models. https://github.com/tatsu-lab/alpaca˙eval.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Li, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.; Guestrin,
    C.; Liang, P.; 和 Hashimoto, T. B. 2023. AlpacaEval：一个自动化的指令跟随模型评估器。https://github.com/tatsu-lab/alpaca˙eval。
- en: Liang et al. (2024) Liang, T.; He, Z.; Jiao, W.; Wang, X.; Wang, R.; Yang, Y.;
    Tu, Z.; and Shi, S. 2024. Encouraging Divergent Thinking in Large Language Models
    through Multi-Agent Debate. arXiv:2305.19118.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2024）Liang, T.; He, Z.; Jiao, W.; Wang, X.; Wang, R.; Yang, Y.; Tu,
    Z.; 和 Shi, S. 2024. 通过多代理辩论鼓励大型语言模型的发散思维。arXiv:2305.19118。
- en: 'Meta (2024) Meta. 2024. Meta LLaMA 3. https://ai.meta.com/blog/meta-llama-3/.
    Accessed: 2024-08-16.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta（2024）Meta. 2024. Meta LLaMA 3。https://ai.meta.com/blog/meta-llama-3/。访问日期：2024-08-16。
- en: OpenAI (2024) OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI. 2024. GPT-4 技术报告。arXiv:2303.08774。
- en: 'Petroni et al. (2021) Petroni, F.; Piktus, A.; Fan, A.; Lewis, P.; Yazdani,
    M.; Cao, N. D.; Thorne, J.; Jernite, Y.; Karpukhin, V.; Maillard, J.; Plachouras,
    V.; Rocktäschel, T.; and Riedel, S. 2021. KILT: a Benchmark for Knowledge Intensive
    Language Tasks. arXiv:2009.02252.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petroni 等（2021）Petroni, F.; Piktus, A.; Fan, A.; Lewis, P.; Yazdani, M.; Cao,
    N. D.; Thorne, J.; Jernite, Y.; Karpukhin, V.; Maillard, J.; Plachouras, V.; Rocktäschel,
    T.; 和 Riedel, S. 2021. KILT：一个知识密集型语言任务基准。arXiv:2009.02252。
- en: 'Rafailov et al. (2024) Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
    C. D.; and Finn, C. 2024. Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model. arXiv:2305.18290.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov 等（2024）Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
    C. D.; 和 Finn, C. 2024. 直接偏好优化：你的语言模型秘密地是一个奖励模型。arXiv:2305.18290。
- en: Rajani et al. (2023) Rajani, N.; Tunstall, L.; Beeching, E.; Lambert, N.; Rush,
    A. M.; and Wolf, T. 2023. No Robots. https://huggingface.co/datasets/HuggingFaceH4/no˙robots.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajani 等（2023）Rajani, N.; Tunstall, L.; Beeching, E.; Lambert, N.; Rush, A.
    M.; 和 Wolf, T. 2023. 无机器人。https://huggingface.co/datasets/HuggingFaceH4/no˙robots。
- en: Tamkin et al. (2021) Tamkin, A.; Brundage, M.; Clark, J.; and Ganguli, D. 2021.
    Understanding the capabilities, limitations, and societal impact of large language
    models. *arXiv preprint arXiv:2102.02503*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tamkin 等（2021）Tamkin, A.; Brundage, M.; Clark, J.; 和 Ganguli, D. 2021. 理解大型语言模型的能力、局限性和社会影响。*arXiv
    预印本 arXiv:2102.02503*。
- en: 'Verga et al. (2024) Verga, P.; Hofstatter, S.; Althammer, S.; Su, Y.; Piktus,
    A.; Arkhangorodsky, A.; Xu, M.; White, N.; and Lewis, P. 2024. Replacing Judges
    with Juries: Evaluating LLM Generations with a Panel of Diverse Models. arXiv:2404.18796.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verga 等（2024）Verga, P.; Hofstatter, S.; Althammer, S.; Su, Y.; Piktus, A.; Arkhangorodsky,
    A.; Xu, M.; White, N.; 和 Lewis, P. 2024. 用陪审团替代评委：用多样模型小组评估LLM生成。arXiv:2404.18796。
- en: Wang et al. (2023a) Wang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y.; Liu,
    Q.; Liu, T.; and Sui, Z. 2023a. Large Language Models are not Fair Evaluators.
    *ArXiv*, abs/2305.17926.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023a）Wang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y.; Liu, Q.;
    Liu, T.; 和 Sui, Z. 2023a. 大型语言模型不是公平的评估者。*ArXiv*, abs/2305.17926。
- en: 'Wang et al. (2023b) Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Heng, Q.; Wang, C.;
    Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang,
    Y. 2023b. PandaLM: Reproducible and Automated Language Model Assessment. https://github.com/WeOpenML/PandaLM.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023b）Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Heng, Q.; Wang, C.; Chen,
    H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; 和 Zhang, Y. 2023b.
    PandaLM：可复现的自动化语言模型评估。https://github.com/WeOpenML/PandaLM。
- en: 'Wang et al. (2024) Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.;
    Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; and Zhang, Y. 2024.
    PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024) Wang, Y.; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.; Chen, H.;
    Jiang, C.; Xie, R.; Wang, J.; Xie, X.; Ye, W.; Zhang, S.; 和 Zhang, Y. 2024. PandaLM:
    用于 LLM 指令调优优化的自动评估基准。'
- en: Zheng et al. (2023a) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.;
    and Stoica, I. 2023a. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.
    arXiv:2306.05685.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023a) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.;
    和 Stoica, I. 2023a. 使用 MT-Bench 和 Chatbot Arena 对 LLM 作为法官进行评估。arXiv:2306.05685。
- en: Zheng et al. (2023b) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.;
    and Stoica, I. 2023b. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.
    arXiv:2306.05685.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2023b) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.;
    和 Stoica, I. 2023b. 使用 MT-Bench 和 Chatbot Arena 对 LLM 作为法官进行评估。arXiv:2306.05685。
- en: 'Álvaro Bartolomé Del Canto et al. (2024) Álvaro Bartolomé Del Canto; Blázquez,
    G. M.; Lajarín, A. P.; and Suero, D. V. 2024. Distilabel: An AI Feedback (AIF)
    framework for building datasets with and for LLMs. https://github.com/argilla-io/distilabel.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Álvaro Bartolomé Del Canto et al. (2024) Álvaro Bartolomé Del Canto; Blázquez,
    G. M.; Lajarín, A. P.; 和 Suero, D. V. 2024. Distilabel: 用于构建与 LLM 相关的数据集的 AI 反馈
    (AIF) 框架。 https://github.com/argilla-io/distilabel。'
- en: Appendix A System Prompts
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 系统提示
- en: 'Table [7](#A1.T7 "Table 7 ‣ Appendix A System Prompts ‣ The Fellowship of the
    LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    contains the three categories of system prompts tested for LLM-as-a-Judge approach.
    The winning prompt with Combined Scoring was used for LLMs-as-a-Jury. These prompts
    are modified versions of those used by (Zheng et al. [2023a](#bib.bib21)). Table
    [8](#A1.T8 "Table 8 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs:
    Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    present the system prompt and user message structure for LLM Debate and [9](#A1.T9
    "Table 9 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") shows the
    prompt for each role in the debate. This is based on the system prompt and the
    input structure used by (Chan et al. [2023](#bib.bib1)). Table [10](#A1.T10 "Table
    10 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") shows the user message
    structure for the generator LLM and Table [11](#A1.T11 "Table 11 ‣ Appendix A
    System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation") shows the system prompt and user
    message for reviewer LLM in LLM Feedback Loop. Table [12](#A1.T12 "Table 12 ‣
    Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows
    for Synthetic Preference Optimization Dataset Generation") shows the system prompt
    and user message structure used for Multi-Agent improvement dataset generation.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [7](#A1.T7 "Table 7 ‣ Appendix A System Prompts ‣ The Fellowship of the
    LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation")
    包含了用于 LLM-as-a-Judge 方法测试的三类系统提示。获胜的 Combined Scoring 提示被用于 LLMs-as-a-Jury。这些提示是对
    (Zheng et al. [2023a](#bib.bib21)) 使用的提示的修改版本。表格 [8](#A1.T8 "Table 8 ‣ Appendix
    A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation") 展示了 LLM Debate 的系统提示和用户消息结构，而 [9](#A1.T9
    "Table 9 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") 展示了辩论中每个角色的提示。这是基于
    (Chan et al. [2023](#bib.bib1)) 使用的系统提示和输入结构。表格 [10](#A1.T10 "Table 10 ‣ Appendix
    A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic
    Preference Optimization Dataset Generation") 展示了生成器 LLM 的用户消息结构，而表格 [11](#A1.T11
    "Table 11 ‣ Appendix A System Prompts ‣ The Fellowship of the LLMs: Multi-Agent
    Workflows for Synthetic Preference Optimization Dataset Generation") 展示了 LLM Feedback
    Loop 中评审 LLM 的系统提示和用户消息。表格 [12](#A1.T12 "Table 12 ‣ Appendix A System Prompts
    ‣ The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization
    Dataset Generation") 展示了用于多智能体改进数据集生成的系统提示和用户消息结构。'
- en: 'Table 7: The three types of system prompts for LLM-as-a-Judge and LLMs-as-a-Jury.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 7: LLM-as-a-Judge 和 LLMs-as-a-Jury 的三种系统提示类型。'
- en: '| Prompt Type | Prompt |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Direct Comparison | Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question displayed
    below. You should choose the assistant that follows the user’s instructions and
    answers the user’s questions better. Your evaluation should consider factors such
    as the helpfulness, relevance, accuracy, depth, creativity, and level of detail
    of their responses. Begin your evaluation by comparing the two responses and provide
    a short explanation. Avoid any position biases and ensure that the order in which
    the responses were presented does not influence your decision. Do not allow the
    length of the responses to influence your evaluation. Answer options: A: If response
    by assistant A is better'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '| 直接比较 | 请作为公正的评审，评估两个 AI 助手对下面用户问题的回答质量。您应该选择遵循用户指示并更好地回答用户问题的助手。您的评估应考虑回答的有用性、相关性、准确性、深度、创造力和细节水平。在开始评估时，比较两个回答并提供简短解释。避免任何立场偏见，确保回答的顺序不影响您的决定。不要让回答的长度影响您的评估。答案选项：A：如果助理
    A 的回答更好'
- en: 'B: If response by assistant B is better'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: B：如果助理 B 的回答更好
- en: 'C: If it is a tie'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'C: 如果是平局'
- en: 'Use the following format to respond:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式回复：
- en: 'Evaluation Evidence:'
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估证据：
- en: '[Add your explanation here]'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的解释]'
- en: 'Answer:'
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 答案：
- en: A or B or C |
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: A 或 B 或 C |
- en: '| Independent Scoring | Please act as an impartial judge and evaluate the quality
    of the response provided by an AI assistants to the user question displayed below.
    Assign an overall score out of 10, where a higher score indicates better overall
    performance. Your evaluation should consider factors such as the helpfulness,
    relevance, accuracy, depth, creativity, and level of detail of their response.
    Begin your evaluation by comparing the two responses and provide a short explanation.
    Do not allow the length of the response to influence your evaluation.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '| 独立评分 | 请作为公正的评审，评估 AI 助手对下面用户问题的回答质量。给出整体得分，满分 10 分，分数越高表示整体表现越好。您的评估应考虑回答的有用性、相关性、准确性、深度、创造力和细节水平。在开始评估时，比较两个回答并提供简短解释。不要让回答的长度影响您的评估。'
- en: 'Use the following format to respond:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式回复：
- en: 'Evaluation Evidence:'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估证据：
- en: '[Add your explanation here]'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的解释]'
- en: 'Overall Score:'
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体得分：
- en: X/10 |
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: X/10 |
- en: '| Combined Scoring | Please act as an impartial judge and evaluate the quality
    of the responses provided by two AI assistants to the user question displayed
    below. You should choose the assistant that follows the user’s instructions and
    answers the user’s questions better. Each response receives an overall score out
    of 10, where a higher score indicates better overall performance. Your evaluation
    should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,
    and level of detail of their responses. Begin your evaluation by comparing the
    two responses and provide a short explanation. Avoid any position biases and ensure
    that the order in which the responses were presented does not influence your decision.
    Do not allow the length of the responses to influence your evaluation.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '| 综合评分 | 请作为公正的评审，评估两个 AI 助手对下面用户问题的回答质量。您应该选择遵循用户指示并更好地回答用户问题的助手。每个回答的整体得分为
    10 分，分数越高表示整体表现越好。您的评估应考虑回答的有用性、相关性、准确性、深度、创造力和细节水平。在开始评估时，比较两个回答并提供简短解释。避免任何立场偏见，确保回答的顺序不影响您的决定。不要让回答的长度影响您的评估。'
- en: 'Use the following format to respond:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式回复：
- en: 'Evaluation Evidence:'
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估证据：
- en: '[Add your explanation here]'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的解释]'
- en: 'Score Assistant A:'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分数助理 A：
- en: X/10
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: X/10
- en: 'Score Assistant B:'
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分数助理 B：
- en: Y/10 |
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Y/10 |
- en: 'Table 8: The system prompt and the user message structure for LLM Debate.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：LLM 辩论的系统提示和用户消息结构。
- en: '| Message Type | Prompt |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| System Prompt | We would like to request your feedback on the performance
    of two AI assistants in response to the user question. There are a few other referee
    assigned the same task, it’s your responsibility to discuss with them and think
    critically before you make your final judgement.Each response receives an overall
    score on a scale of 1 to 10, where a higher score indicates better overall performance.
    You should choose the assistant that follows the user’s instructions and answers
    the user’s question better. You don’t necessarily have to agree with others.Your
    evaluation should consider factors such as the helpfulness, relevance, accuracy,
    depth, creativity, and level of detail of their responses. Avoid any position
    biases and ensure that the order in which the responses were presented does not
    influence your decision. Do not allow the length of the responses to influence
    your evaluation. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 我们希望您对两位 AI 助理在回答用户问题时的表现提供反馈。还有其他裁判被分配了相同的任务，你有责任与他们讨论并在做出最终判断前进行批判性思考。每个响应根据
    1 到 10 的评分范围进行总体评分，其中较高的分数表示整体表现较好。你应该选择更好地遵循用户指示并回答用户问题的助理。你不必一定同意他人。你的评估应考虑到他们回应的有用性、相关性、准确性、深度、创造力和细节水平。避免任何立场偏见，并确保响应的呈现顺序不会影响你的决定。不要让响应的长度影响你的评估。
    |'
- en: '| User Message | <&#124;Start of User Question&#124;> {User Question} <&#124;End
    of User Question&#124;>'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息 | <&#124;用户问题开始&#124;> {用户问题} <&#124;用户问题结束&#124;>'
- en: <&#124;The Start of Assistant 1’s Answer&#124;> {Assistant 1}
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助理 1 答案开始&#124;> {助理 1}
- en: <&#124;The End of Assistant 1’s Answer&#124;>
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助理 1 答案开始&#124;>
- en: <&#124;The Start of Assistant 2’s Answer&#124;> {Assistant 2}
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助理 2 答案开始&#124;> {助理 2}
- en: '<&#124;The End of Assistant 2’s Answer&#124;> Here is your discussion history:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <&#124;助理 2 答案结束&#124;> 这是你的讨论历史：
- en: '{Chat History}'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '{聊天记录}'
- en: '{Role} |'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '{角色} |'
- en: 'Table 9: The prompt for each role used in LLM Debate.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 9: LLM 辩论中使用的每个角色的提示。'
- en: '| Role | Prompt |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 角色 | 提示 |'
- en: '| --- | --- |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| General Public | You are now General Public, one of the referees in this
    task. You are interested in the story and looking for updates on the investigation.
    Please think critically by yourself and note that it’s your responsibility to
    choose one of which is the better first.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '| 一般公众 | 你现在是**一般公众**，本任务的裁判之一。你对故事感兴趣并关注调查进展。请自行批判性思考，并注意你有责任选择哪个更好。'
- en: Now it’s your turn to speak General Public, please make your talk short and
    clear.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你发言了**一般公众**，请简明扼要。
- en: '**General Public**: |'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般公众**: |'
- en: '| Psychologist | You are now Psychologist, one of the referees in this task.
    You will study human behavior and mental processes in order to understand and
    explain human behavior. Please help other people to determine which response is
    the better one.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '| 心理学家 | 你现在是**心理学家**，本任务的裁判之一。你将研究人类行为和心理过程，以理解和解释人类行为。请帮助他人确定哪个回应更好。'
- en: Now it’s your turn to speak Psychologist, please make your talk short and clear.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你发言了**心理学家**，请简明扼要。
- en: '**Psychologist**: |'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**心理学家**: |'
- en: '| Critic | You are now Critic, one of the referees in this task. You will check
    fluent writing, clear sentences, and good wording in summary writing. Your job
    is to question others judgement to make sure their judgement is well-considered
    and offer an alternative solution if two responses are at the same level.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '| 批评者 | 你现在是**批评者**，本任务的裁判之一。你将检查总结写作中的流畅性、句子清晰度和用词准确性。你的工作是质疑他人的判断，以确保他们的判断经过深思熟虑，如果两个响应处于同一水平，则提供替代方案。'
- en: Now it’s your turn to speak Critic, please make your talk short and clear.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你发言了**批评者**，请简明扼要。
- en: '**Critic**:” |'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**批评者**:” |'
- en: 'Table 10: The user message structure for the generator in LLM Feedback.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 10: LLM 反馈中生成器的用户消息结构。'
- en: '| Message Type | Prompt |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| User Message (Single Feedback) | Update your response based on the feedback:
    [Start of Feedback]'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息（单一反馈） | 根据反馈更新你的回应: [反馈开始]'
- en: '{Feedback}'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '{反馈}'
- en: '[End of Feedback]'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[反馈结束]'
- en: Do not engage in formalities such as ’Thank you for your feedback’ or ’Here
    is an updated version…’ etc, just update the response. |
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 不要进行诸如“感谢您的反馈”或“这是更新版本……”等正式客套，只需更新响应。 |
- en: '| User Message (Double Feedback) | Update your response based on the feedback
    by the two assistant: [Start of Assistant 1’s Feedback]'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息（双重反馈） | 根据两位助手的反馈更新您的回应：[助手1反馈开始]'
- en: '{Assistant 1’s Feedback}'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手1反馈}'
- en: '[End of Assistant 1’s Feedback]'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手1反馈结束]'
- en: '[Start of Assistant 2’s Feedback]'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手2反馈开始]'
- en: '{Assistant 2’s Feedback}'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手2反馈}'
- en: '[End of Assistant 2’s Feedback]'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手2反馈结束]'
- en: Do not engage in formalities such as ’Thank you for your feedback’ or ’Here
    is an updated version…’ etc, just update the response. |
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不要涉及诸如“感谢您的反馈”或“这是更新版本……”等形式化的表达，只需更新回应。 |
- en: 'Table 11: The prompt and user message structure for the reviewer in LLM Feedback.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：LLM反馈中审阅者的提示和用户消息结构。
- en: '| Message Type | Prompt |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| System Prompt | Please give constructive feedback on how to improve the response
    provided by an AI assistant to the user question. Your evaluation should consider
    factors such as the instruction following (the response should align with the
    user instructions), helpfulness, relevance, accuracy, and creativity of the response.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '| 系统提示 | 请给出建设性的反馈，说明如何改进AI助手对用户问题的回应。您的评价应考虑诸如指令遵循（回应应符合用户指示）、帮助程度、相关性、准确性和回应的创造性等因素。'
- en: Assign an overall score out of 10, up to one decimal place, where a higher score
    indicates better overall performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个1到10的总体评分，保留一位小数，分数越高表示整体表现越好。
- en: 'Use the following format to respond:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下格式回应：
- en: 'Evaluation:'
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评价：
- en: '[Add your evaluation here]'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的评价]'
- en: 'Overall Score:'
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体评分：
- en: X/10
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: X/10
- en: 'Feedback:'
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反馈：
- en: '[Add your feedback here] |'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的反馈] |'
- en: '| User Message | [Start of User Question] {User Question}'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息 | [用户问题开始] {用户问题}'
- en: '[End of User Question]'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[用户问题结束]'
- en: '[Start of Assistant’s Response]'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手回应开始]'
- en: '{Assistant’s Response}'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手回应}'
- en: '[End of Assistant’s Response] |'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手回应结束] |'
- en: 'Table 12: The prompt and user message structure for the feedback generation
    for Multi-Agent improvement dataset.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：多代理改进数据集的反馈生成的提示和用户消息结构。
- en: '| Message Type | Prompt |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 消息类型 | 提示 |'
- en: '| --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| System Prompt | Please give constructive feedback on how to improve the response
    provided by an AI assistant to the user question. Your evaluation should consider
    factors such as the instruction following (the response should align with the
    user instructions), helpfulness, relevance, accuracy, and creativity of the response.
    Use the human answer as the reference answer, so that the evaluation and feedback
    is based on how to get closer to human answer.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '| 系统提示 | 请给出建设性的反馈，说明如何改进AI助手对用户问题的回应。您的评价应考虑诸如指令遵循（回应应符合用户指示）、帮助程度、相关性、准确性和回应的创造性等因素。使用人类回答作为参考答案，以便评价和反馈基于如何接近人类答案。'
- en: 'Assign an overall score out of 10, up to one decimal place, where a higher
    score indicates better overall performance. Use the following format to respond:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 给出一个1到10的总体评分，保留一位小数，分数越高表示整体表现越好。使用以下格式回应：
- en: 'Evaluation:'
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评价：
- en: '[Add your evaluation here]'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的评价]'
- en: 'Overall Score:'
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体评分：
- en: X/10
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: X/10
- en: 'Feedback:'
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反馈：
- en: '[Add your feedback here] |'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[在此添加您的反馈] |'
- en: '| User Message | [Start of User Question] {User Question}'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '| 用户消息 | [用户问题开始] {用户问题}'
- en: '[End of User Question]'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[用户问题结束]'
- en: '[Start of Human’s Response]'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[人类回应开始]'
- en: '{Human’s Response}'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '{人类回应}'
- en: '[End of Human’s Response]'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[用户回应结束]'
- en: '[Start of Assistant’s Response]'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手回应开始]'
- en: '{Assistant’s Response}'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '{助手回应}'
- en: '[End of Assistant’s Response]'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[助手回应结束]'
- en: Base your evaluation and feedback on human response. We want the LLM to mimic
    the human response. Do not mention the word ’human response’ in your evaluation
    and feedback. The user should not know that you have a reference answer. |
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人类回应进行评价和反馈。我们希望LLM模仿人类回应。不要在评价和反馈中提及“人类回应”一词。用户不应知道您有参考答案。 |
