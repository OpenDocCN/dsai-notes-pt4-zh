- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LAVE: 基于LLM的代理辅助和语言增强视频编辑'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10294](https://ar5iv.labs.arxiv.org/html/2402.10294)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10294](https://ar5iv.labs.arxiv.org/html/2402.10294)
- en: Bryan Wang University of TorontoTorontoONCanada [bryanw@dgp.toronto.edu](mailto:bryanw@dgp.toronto.edu)
    ,  Yuliang Li Reality Labs Research, MetaSunnyvaleCAUSA [yuliangli@meta.com](mailto:yuliangli@meta.com)
    ,  Zhaoyang Lv Reality Labs Research, MetaSunnyvaleCAUSA [zhaoyang@meta.com](mailto:zhaoyang@meta.com)
    ,  Haijun Xia University of California San Diego La JollaCAUSA [haijunxia@ucsd.edu](mailto:haijunxia@ucsd.edu)
    ,  Yan Xu Reality Labs Research, MetaRedmondWAUSA [yanx@meta.com](mailto:yanx@meta.com)
     and  Raj Sodhi Reality Labs Research, MetaRedmondWAUSA [rsodhi@meta.com](mailto:rsodhi@meta.com)(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 布莱恩·王 多伦多大学 多伦多 ON 加拿大 [bryanw@dgp.toronto.edu](mailto:bryanw@dgp.toronto.edu)
    ，余亮 Reality Labs Research, Meta 圣尼古拉 CA 美国 [yuliangli@meta.com](mailto:yuliangli@meta.com)
    ，赵阳 Reality Labs Research, Meta 圣尼古拉 CA 美国 [zhaoyang@meta.com](mailto:zhaoyang@meta.com)
    ，海军·夏 加州大学圣地亚哥分校 拉荷亚 CA 美国 [haijunxia@ucsd.edu](mailto:haijunxia@ucsd.edu) ，闫旭
    Reality Labs Research, Meta 雷德蒙德 WA 美国 [yanx@meta.com](mailto:yanx@meta.com) 和
    拉杰·索德希 Reality Labs Research, Meta 雷德蒙德 WA 美国 [rsodhi@meta.com](mailto:rsodhi@meta.com)（2024）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Video creation has become increasingly popular, yet the expertise and effort
    required for editing often pose barriers to beginners. In this paper, we explore
    the integration of large language models (LLMs) into the video editing workflow
    to reduce these barriers. Our design vision is embodied in LAVE, a novel system
    that provides LLM-powered agent assistance and language-augmented editing features.
    LAVE automatically generates language descriptions for the user’s footage, serving
    as the foundation for enabling the LLM to process videos and assist in editing
    tasks. When the user provides editing objectives, the agent plans and executes
    relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through
    either the agent or direct UI manipulation, providing flexibility and enabling
    manual refinement of agent actions. Our user study, which included eight participants
    ranging from novices to proficient editors, demonstrated LAVE’s effectiveness.
    The results also shed light on user perceptions of the proposed LLM-assisted editing
    paradigm and its impact on users’ creativity and sense of co-creation. Based on
    these findings, we propose design implications to inform the future development
    of agent-assisted content editing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 视频创作变得越来越受欢迎，但编辑所需的专业知识和努力通常对初学者构成障碍。本文探讨了将大型语言模型（LLMs）集成到视频编辑工作流中，以减少这些障碍。我们的设计愿景体现在LAVE中，这是一种新颖的系统，提供基于LLM的代理辅助和语言增强编辑功能。LAVE自动为用户的视频生成语言描述，作为LLM处理视频和协助编辑任务的基础。当用户提供编辑目标时，代理会计划并执行相关操作以实现这些目标。此外，LAVE允许用户通过代理或直接UI操作编辑视频，提供了灵活性，并允许对代理操作进行手动调整。我们的用户研究，包括八名从初学者到熟练编辑者的参与者，证明了LAVE的有效性。结果还揭示了用户对所提LLM辅助编辑范式的看法及其对用户创造力和共同创作感的影响。基于这些发现，我们提出了设计启示，以指导未来代理辅助内容编辑的发展。
- en: 'Video Editing, LLMs, Agents, Human-AI Co-Creation^†^†journalyear: 2024^†^†copyright:
    acmlicensed^†^†conference: 29th International Conference on Intelligent User Interfaces;
    March 18–21, 2024; Greenville, SC, USA^†^†booktitle: 29th International Conference
    on Intelligent User Interfaces (IUI ’24), March 18–21, 2024, Greenville, SC, USA^†^†doi:
    10.1145/3640543.3645143^†^†isbn: 979-8-4007-0508-3/24/03![Refer to caption](img/e0e1c07ab5e6211250ecc35ab33b52af.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '视频编辑, LLMs, 代理, 人工智能与人类共同创作^†^†期刊年份: 2024^†^†版权: acmlicensed^†^†会议: 第29届国际智能用户界面会议;
    2024年3月18-21日; 南卡罗来纳州格林维尔, 美国^†^†书名: 第29届国际智能用户界面会议（IUI ’24），2024年3月18-21日，南卡罗来纳州格林维尔,
    美国^†^†doi: 10.1145/3640543.3645143^†^†isbn: 979-8-4007-0508-3/24/03![参见说明](img/e0e1c07ab5e6211250ecc35ab33b52af.png)'
- en: Figure 1\. The LAVE system is a video editing tool that offers LLM-powered agent
    assistance and language-augmented features. A) LAVE’s video editing agent assists
    with several video editing tasks, with which users can converse to obtain agent
    assistance throughout the editing process. B) A language-augmented video gallery.
    Users can click on a desired video to select and add it to the editing timeline.
    Videos added to the timeline will be displayed in reduced opacity. C) LAVE automatically
    generates succinct titles for each video. D) Hovering over a video in the gallery
    displays a tooltip with the video summary, allowing users to understand the video
    content without playing it. E) An editing timeline where users can reorder and
    trim clips. These edits can be performed either with LLM assistance or manually.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. LAVE 系统是一个视频编辑工具，提供 LLM 驱动的代理辅助和语言增强功能。A) LAVE 的视频编辑代理协助完成多个视频编辑任务，用户可以通过对话获得代理的帮助。B)
    一个语言增强的视频库。用户可以点击所需的视频，将其选择并添加到编辑时间轴中。添加到时间轴的视频将以减少的不透明度显示。C) LAVE 自动生成每个视频的简洁标题。D)
    将鼠标悬停在库中的视频上，会显示一个工具提示，提供视频摘要，使用户在不播放视频的情况下了解内容。E) 一个编辑时间轴，用户可以在其中重新排序和剪辑片段。这些编辑可以通过
    LLM 辅助或手动完成。
- en: \Description
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: .
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 1\. Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Videos are a powerful medium for communication and storytelling. Their popularity
    has surged with the advent of social media and video-sharing platforms, inspiring
    many to produce and share their content. However, the complexity of video editing
    can pose significant barriers for beginners. For example, the initial ideation
    and planning phases, crucial in the early stages of the creative process, can
    be challenging for those unfamiliar with video concept development. Furthermore,
    editing operations often involve meticulous selection, trimming, and sequencing
    of clips to create a coherent narrative. This not only requires mastery of the
    often complex user interfaces of editing software but also significant manual
    effort and storytelling skills.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 视频是强大的沟通和讲故事的媒介。随着社交媒体和视频分享平台的兴起，它们的受欢迎程度激增，激励了许多人制作和分享他们的内容。然而，视频编辑的复杂性可能对初学者构成重大障碍。例如，创作过程的早期阶段中的初步构思和规划阶段，对于那些不熟悉视频概念开发的人来说可能是具有挑战性的。此外，编辑操作通常涉及对剪辑的精细选择、修剪和排序，以创建连贯的叙事。这不仅需要掌握往往复杂的编辑软件用户界面，还需要大量的手动努力和讲故事的技能。
- en: Recently, natural language has been used to address the challenges associated
    with video editing. Utilizing language as an interaction medium for video editing
    allows users to directly convey their intentions, bypassing the need to translate
    thoughts into manual operations. For instance, recent AI products (run, [2023](#bib.bib6))
    allow users to edit video leveraging the power of text-to-video models (Singer
    et al., [2022](#bib.bib64); Ho et al., [2022](#bib.bib26)); voice-based video
    navigation enables users to browse videos using voice commands instead of manual
    scrubbing (Chang et al., [2021](#bib.bib17), [2019](#bib.bib18)). In addition,
    language has been used to represent video content, thereby streamlining the manual
    editing process. A prominent example is text-based editing, which enables users
    to efficiently edit a narrative video by adjusting its time-aligned transcripts
    (Fried et al., [2019](#bib.bib24); Pavel et al., [2020](#bib.bib57); Huber et al.,
    [2019](#bib.bib29); Huh et al., [2023](#bib.bib30)). Despite these advancements,
    the majority of video editing tools still heavily rely on manual editing and often
    lack customized, in-context assistance. Consequently, users are left to grapple
    with the intricacies of video editing on their own.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，自然语言已被用于解决与视频编辑相关的挑战。利用语言作为视频编辑的交互媒介，允许用户直接传达意图，绕过将思维转化为手动操作的需要。例如，最近的 AI
    产品（run，[2023](#bib.bib6)）允许用户利用文本到视频模型（Singer et al., [2022](#bib.bib64)；Ho et
    al., [2022](#bib.bib26)）编辑视频；基于语音的视频导航使用户可以使用语音命令浏览视频，而无需手动拖动（Chang et al., [2021](#bib.bib17)，[2019](#bib.bib18)）。此外，语言还被用于表示视频内容，从而简化手动编辑过程。一个突出的例子是基于文本的编辑，允许用户通过调整时间对齐的转录本来高效编辑叙事视频（Fried
    et al., [2019](#bib.bib24)；Pavel et al., [2020](#bib.bib57)；Huber et al., [2019](#bib.bib29)；Huh
    et al., [2023](#bib.bib30)）。尽管取得了这些进展，大多数视频编辑工具仍然严重依赖手动编辑，且往往缺乏定制的上下文辅助。因此，用户不得不独自应对视频编辑的复杂性。
- en: How can we design a video editing tool that acts as a collaborator, constantly
    assisting users in the editing process? Such a tool could help users generate
    video editing ideas, browse and find relevant clips, and sequence them to craft
    a compelling narrative. Building upon previous work that integrates natural language
    with video editing, we propose to instrument video editing with LLM’s versatile
    linguistic capabilities, e.g., storytelling and reasoning, which have proven useful
    in assisting various creative tasks (Yuan et al., [2022](#bib.bib84); Chung et al.,
    [2022](#bib.bib21); Mirowski et al., [2023](#bib.bib52); Chakrabarty et al., [2023](#bib.bib16);
    Liu et al., [2022](#bib.bib44), [2023c](#bib.bib45); Wang et al., [2023b](#bib.bib77);
    Liu et al., [2023b](#bib.bib43)). In doing so, we probe into a future video editing
    paradigm that, through the power of natural language, reduces the barriers typically
    associated with manual video editing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何设计一种视频编辑工具，使其能作为一个协作伙伴，持续协助用户进行编辑过程？这样的工具可以帮助用户生成视频编辑创意，浏览并找到相关片段，并将其排序以构建引人入胜的叙事。基于以前将自然语言与视频编辑结合的工作，我们建议利用LLM的多功能语言能力（例如，讲故事和推理）来提升视频编辑，这些能力已被证明在协助各种创意任务中有效（Yuan
    et al., [2022](#bib.bib84); Chung et al., [2022](#bib.bib21); Mirowski et al.,
    [2023](#bib.bib52); Chakrabarty et al., [2023](#bib.bib16); Liu et al., [2022](#bib.bib44),
    [2023c](#bib.bib45); Wang et al., [2023b](#bib.bib77); Liu et al., [2023b](#bib.bib43)）。通过这种方式，我们深入探讨了一个未来的视频编辑范式，该范式通过自然语言的力量，降低了通常与手动视频编辑相关的障碍。
- en: 'We present LAVE, a video editing tool that offers language augmentation powered
    by LLMs. LAVE introduces an LLM-based plan-and-execute agent capable of interpreting
    users’ free-form language commands, planning, and executing relevant actions to
    achieve users’ editing objectives. These actions encompass conceptualization assistance,
    such as brainstorming ideas and summarizing a video corpus with an overview, as
    well as operational assistance, including semantic-based video retrieval, storyboarding
    (sequencing videos to form a narrative), and trimming clips. To enable these agent
    actions, LAVE automatically generates language descriptions of the video’s visuals
    using visual-language models (VLMs). These descriptions, which we refer to as
    visual narrations, allow LLMs to understand the video content and leverage their
    linguistic capabilities to assist users in editing tasks. LAVE offers two interaction
    modalities for video editing: agent assistance and direct manipulation. The dual
    modalities provide users with flexibility and allow them to refine agent actions
    as needed.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推出了LAVE，这是一款提供LLM语言增强的视频编辑工具。LAVE引入了一种基于LLM的计划与执行代理，能够解读用户的自由形式语言命令，规划并执行相关操作以实现用户的编辑目标。这些操作包括概念化辅助，如头脑风暴创意和总结视频语料库概述，以及操作性辅助，包括基于语义的视频检索、故事板（将视频排序形成叙事）和剪辑片段。为了实现这些代理操作，LAVE使用视觉语言模型（VLMs）自动生成视频视觉内容的语言描述。这些描述，我们称之为视觉叙述，使LLM能够理解视频内容，并利用其语言能力来协助用户进行编辑任务。LAVE提供了两种视频编辑交互模式：代理辅助和直接操作。这两种模式为用户提供了灵活性，并允许他们根据需要优化代理操作。
- en: 'We conducted a user study with eight participants, which included both novice
    and proficient video editors, to assess the effectiveness of LAVE in aiding video
    editing. The results demonstrated that participants could produce satisfactory
    AI-collaborative video outcomes using LAVE. Users expressed appreciation for the
    system’s functionalities, finding them easy to use and useful for producing creative
    video artifacts. Furthermore, our study uncovered insights into users’ perceptions
    of the proposed editing paradigm, their acceptance of agent assistance across
    different tasks, as well as the system’s influence on their creativity and sense
    of human-AI co-creation. Based on these findings, we proposed design implications
    to inform the development of future multimedia content editing tools that integrate
    LLMs and agents. In summary, this paper makes the following contributions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个包括八名参与者的用户研究，参与者中既有新手也有熟练的视频编辑人员，以评估LAVE在视频编辑中的有效性。结果表明，参与者能够使用LAVE产生令人满意的AI协作视频成果。用户对系统的功能表示赞赏，认为其易于使用且有助于产生创意视频作品。此外，我们的研究揭示了用户对所提编辑范式的看法、他们对代理辅助的接受度以及系统对他们创造力和人机共创感的影响。基于这些发现，我们提出了设计启示，以指导未来多媒体内容编辑工具的开发，这些工具将整合LLM和代理。总之，本文做出了以下贡献：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The conceptualization and implementation of the LAVE system, a language-augmented
    video editing tool that leverages LLM’s linguistic intelligence to facilitate
    an agent-assisted video editing experience.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LAVE 系统的概念化和实施，这是一个语言增强的视频编辑工具，利用大型语言模型（LLM）的语言智能来促进代理辅助的视频编辑体验。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The design of an LLM-based computational pipeline that enables LAVE’s video
    editing agent to plan and execute a range of editing functions to help achieve
    users’ editing objectives.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计了一个基于LLM的计算管道，使LAVE的视频编辑代理能够规划和执行一系列编辑功能，以帮助实现用户的编辑目标。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The user study results showcasing the advantages and challenges of integrating
    LLMs with video editing. The findings highlight user perceptions and emerging
    behaviors with the proposed editing paradigm, from which we propose design implications
    for future agent-assisted content editing.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户研究结果展示了将LLM与视频编辑集成的优势和挑战。这些发现突出了用户对提议的编辑范式的感知和新兴行为，从中我们提出了未来代理辅助内容编辑的设计启示。
- en: 2\. Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: LAVE builds upon existing work in language as a medium for video editing, LLM
    and agents, and human-AI co-creation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 基于现有的语言作为视频编辑媒介、LLM 和代理、以及人机协作的工作。
- en: 2.1\. Language as Medium for Video Editing
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 语言作为视频编辑的媒介
- en: Traditional video editing tools like Premier Pro (pre, [2023](#bib.bib2)) and
    Final Cut Pro (fin, [2023](#bib.bib4)) demand manual interaction with raw clips.
    While precise, it can be cumbersome due to UI complexity. Additionally, visual
    elements of raw footage such as thumbnails and audio waveforms might not always
    convey its semantics effectively. Language, on the other hand, offers an intuitive
    and efficient alternative to complex UI in video editing and has been investigated
    in video editing tool research (Xia et al., [2020](#bib.bib80); Xia, [2020](#bib.bib79);
    Fried et al., [2019](#bib.bib24); Pavel et al., [2020](#bib.bib57); Huber et al.,
    [2019](#bib.bib29); Huh et al., [2023](#bib.bib30); Truong et al., [2016](#bib.bib69)).
    One common approach treats language as a ”Command”, where users employ language
    to instruct tools for specific operations. This is evident in multimodal authoring
    tools that support speech commands (Laput et al., [2013](#bib.bib36)) and voice-based
    video navigation (Lin et al., [2023](#bib.bib40); Chang et al., [2019](#bib.bib18)).
    However, existing work primarily supports single-turn interactions and provides
    a limited range of commands. As a result, they do not accommodate diverse language
    and long-term conversations. In contrast, LAVE accepts free-form language, supporting
    natural interaction and allowing back-and-forth discussions with an agent throughout
    the video editing process. Another significant body of work treats language as
    ”Content”, where language becomes part of the content being edited. For instance,
    text-based editing for narrative videos (Fried et al., [2019](#bib.bib24); Pavel
    et al., [2020](#bib.bib57); Huber et al., [2019](#bib.bib29); Huh et al., [2023](#bib.bib30))
    and creating video montages by scripting (Wang et al., [2019b](#bib.bib76)). Nevertheless,
    these techniques rely on either the pre-existing language content in the videos,
    such as narration, or on language annotations provided by the user (Truong et al.,
    [2016](#bib.bib69); Wang et al., [2019b](#bib.bib76)). The former is often missing
    in everyday videos recorded by individuals, while the latter requires additional
    manual effort. In contrast, LAVE automatically generates language descriptions
    for each video and leverages LLM’s linguistic capabilities to automate and facilitate
    content editing. Recent work in generative AI, such as Make-A-Video (Singer et al.,
    [2022](#bib.bib64)) and Imagen Video (Ho et al., [2022](#bib.bib26)), have investigated
    synthesizing videos from textual prompts using diffusion techniques. Unlike these
    efforts, which aim to generate new footage, our objective is to facilitate the
    editing of existing videos. That said, we anticipate that video generation techniques
    will complement editing tools like LAVE, especially in use cases like creating
    B-rolls.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的视频编辑工具如 Premier Pro (pre, [2023](#bib.bib2)) 和 Final Cut Pro (fin, [2023](#bib.bib4))
    需要对原始剪辑进行手动操作。虽然这些工具精确，但由于用户界面的复杂性，操作起来可能比较繁琐。此外，原始素材的视觉元素，如缩略图和音频波形，可能无法有效传达其语义。另一方面，语言为视频编辑提供了一种直观且高效的替代复杂用户界面的方式，这在视频编辑工具研究中已有探讨（Xia
    等，[2020](#bib.bib80)；Xia，[2020](#bib.bib79)；Fried 等，[2019](#bib.bib24)；Pavel 等，[2020](#bib.bib57)；Huber
    等，[2019](#bib.bib29)；Huh 等，[2023](#bib.bib30)；Truong 等，[2016](#bib.bib69)）。一种常见的方法将语言视为“命令”，用户通过语言指令来操作工具进行特定操作。这在支持语音命令的多模态创作工具（Laput
    等，[2013](#bib.bib36)）和基于语音的视频导航（Lin 等，[2023](#bib.bib40)；Chang 等，[2019](#bib.bib18)）中有所体现。然而，现有工作主要支持单轮交互，并提供有限的命令范围。因此，它们不能适应多样化的语言和长期对话。相比之下，LAVE
    接受自由形式的语言，支持自然互动，并允许在整个视频编辑过程中与代理进行往返讨论。另一重要的研究方向将语言视为“内容”，其中语言成为编辑内容的一部分。例如，叙事视频的基于文本的编辑（Fried
    等，[2019](#bib.bib24)；Pavel 等，[2020](#bib.bib57)；Huber 等，[2019](#bib.bib29)；Huh
    等，[2023](#bib.bib30)）和通过脚本创建视频蒙太奇（Wang 等，[2019b](#bib.bib76)）。然而，这些技术依赖于视频中预先存在的语言内容，如旁白，或由用户提供的语言注释（Truong
    等，[2016](#bib.bib69)；Wang 等，[2019b](#bib.bib76)）。前者在个人录制的日常视频中通常缺失，而后者需要额外的手动工作。相比之下，LAVE
    自动生成每个视频的语言描述，并利用 LLM 的语言能力来自动化和促进内容编辑。最近的生成 AI 研究，如 Make-A-Video（Singer 等，[2022](#bib.bib64)）和
    Imagen Video（Ho 等，[2022](#bib.bib26)），已探讨了使用扩散技术从文本提示中合成视频。与这些旨在生成新素材的努力不同，我们的目标是促进现有视频的编辑。也就是说，我们预计视频生成技术将补充像
    LAVE 这样的编辑工具，特别是在创建 B-rolls 这类用例中。
- en: 2.2\. Large Language Models and Agents
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 大型语言模型与智能体
- en: LLMs, such as GPT-4 (OpenAI, [2023](#bib.bib54)) and LLaMA (Touvron et al.,
    [2023](#bib.bib68)), are trained on vast amounts of text data and possess immense
    model sizes. They have been shown to encode a wealth of human knowledge (Huang
    et al., [2022](#bib.bib28); Roberts et al., [2020](#bib.bib59); Li et al., [2021](#bib.bib37))
    and can perform sophisticated reasoning (Wei et al., [2023](#bib.bib78); Kojima
    et al., [2023](#bib.bib35); Nye et al., [2021](#bib.bib53)) and action planning
    (Huang et al., [2022](#bib.bib28)). Their linguistic and storytelling capabilities
    have been utilized in creative writing (Yuan et al., [2022](#bib.bib84); Chung
    et al., [2022](#bib.bib21); Mirowski et al., [2023](#bib.bib52); Chakrabarty et al.,
    [2023](#bib.bib16)) and a myriad of other creative applications (Liu et al., [2022](#bib.bib44),
    [2023c](#bib.bib45); Wang et al., [2023b](#bib.bib77); Liu et al., [2023b](#bib.bib43);
    Brade et al., [2023](#bib.bib12)). Moreover, LLMs can adapt to new tasks based
    on a given description without re-training, a method known as prompting. Owing
    to the efficiency and adaptability, there has been a surge in interest in prompting
    techniques (Zamfirescu-Pereira et al., [2023](#bib.bib85); Kim et al., [2023](#bib.bib34);
    Arawjo et al., [2023](#bib.bib10); Brown et al., [2020](#bib.bib14); Wang et al.,
    [2023a](#bib.bib73); Logan IV et al., [2021](#bib.bib46)). Notable ones include
    few-shot prompting (Brown et al., [2020](#bib.bib14)), where multiple input/output
    data examples are provided to enhance task performances, and chain-of-thought
    prompting (Wei et al., [2023](#bib.bib78)), which directs the LLM in generating
    a sequence of intermediate reasoning steps prior to the final output. Leveraging
    these techniques, recent studies have explored the development of agents autonomously
    interacting with various environments using LLMs (Wang et al., [2023c](#bib.bib75);
    Park et al., [2023](#bib.bib55); Shaw et al., [2023](#bib.bib60); Song et al.,
    [2023](#bib.bib65); Bran et al., [2023](#bib.bib13); Shinn et al., [2023](#bib.bib63);
    Yao et al., [2023](#bib.bib82); Li et al., [2023a](#bib.bib39)). For example,
    Wang et al. (Wang et al., [2023c](#bib.bib75)) introduced an agent that devises
    a plan dividing tasks into subtasks and executes them. Yao et al. (Yao et al.,
    [2023](#bib.bib82)) presented the ReAct framework, where LLMs generate interleaved
    reasoning sequences and task-specific actions. This paper builds upon prior work
    in this area and proposes an agent architecture designed for interactive video
    editing, which plans and executes relevant editing actions based on the user’s
    instructions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如GPT-4（OpenAI，[2023](#bib.bib54)）和LLaMA（Touvron等，[2023](#bib.bib68)），在大量文本数据上进行训练，拥有巨大的模型规模。它们已被证明能够编码丰富的人类知识（Huang等，[2022](#bib.bib28)；Roberts等，[2020](#bib.bib59)；Li等，[2021](#bib.bib37)），并能进行复杂的推理（Wei等，[2023](#bib.bib78)；Kojima等，[2023](#bib.bib35)；Nye等，[2021](#bib.bib53)）和行动计划（Huang等，[2022](#bib.bib28)）。它们的语言和讲故事能力已被应用于创意写作（Yuan等，[2022](#bib.bib84)；Chung等，[2022](#bib.bib21)；Mirowski等，[2023](#bib.bib52)；Chakrabarty等，[2023](#bib.bib16)）及众多其他创意应用（Liu等，[2022](#bib.bib44)，[2023c](#bib.bib45)；Wang等，[2023b](#bib.bib77)；Liu等，[2023b](#bib.bib43)；Brade等，[2023](#bib.bib12)）。此外，LLMs可以根据给定的描述适应新任务，而无需重新训练，这种方法称为提示。由于其效率和适应性，对提示技术的兴趣激增（Zamfirescu-Pereira等，[2023](#bib.bib85)；Kim等，[2023](#bib.bib34)；Arawjo等，[2023](#bib.bib10)；Brown等，[2020](#bib.bib14)；Wang等，[2023a](#bib.bib73)；Logan
    IV等，[2021](#bib.bib46)）。其中显著的包括少样本提示（Brown等，[2020](#bib.bib14)），在这种方法中提供多个输入/输出数据示例以提高任务表现，以及思维链提示（Wei等，[2023](#bib.bib78)），它引导LLM在最终输出之前生成一系列中间推理步骤。利用这些技术，近期研究探索了利用LLMs开发与各种环境自主互动的代理（Wang等，[2023c](#bib.bib75)；Park等，[2023](#bib.bib55)；Shaw等，[2023](#bib.bib60)；Song等，[2023](#bib.bib65)；Bran等，[2023](#bib.bib13)；Shinn等，[2023](#bib.bib63)；Yao等，[2023](#bib.bib82)；Li等，[2023a](#bib.bib39)）。例如，Wang等（Wang等，[2023c](#bib.bib75)）介绍了一个代理，它制定计划将任务分解为子任务并执行这些任务。Yao等（Yao等，[2023](#bib.bib82)）提出了ReAct框架，其中LLMs生成交错的推理序列和任务特定的行动。本文在这一领域的先前工作基础上提出了一种用于交互式视频编辑的代理架构，该架构根据用户的指示计划和执行相关的编辑动作。
- en: 2.3\. Human-AI Co-Creation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 人工智能与人类的共同创作
- en: As AI continues to advance in its capability to generate content and automate
    tasks, it is being increasingly incorporated into the creative processes across
    various domains (Mirowski et al., [2023](#bib.bib52); Yuan et al., [2022](#bib.bib84);
    Chung et al., [2022](#bib.bib21); Mirowski et al., [2023](#bib.bib52); Chakrabarty
    et al., [2023](#bib.bib16); Louie et al., [2022](#bib.bib49); Huang et al., [2020](#bib.bib27);
    Louie et al., [2020](#bib.bib48); Suh et al., [2022](#bib.bib66); Wang et al.,
    [2022](#bib.bib72)). This includes areas such as story writing (Chung et al.,
    [2022](#bib.bib21); Yuan et al., [2022](#bib.bib84); Mirowski et al., [2023](#bib.bib52);
    Chakrabarty et al., [2023](#bib.bib16)), music composition (Louie et al., [2022](#bib.bib49);
    Huang et al., [2020](#bib.bib27); Louie et al., [2020](#bib.bib48)), comic creation
    (Suh et al., [2022](#bib.bib66)), and game design (Zhu et al., [2018](#bib.bib87)).
    For instance, TaleBrush (Chung et al., [2022](#bib.bib21)) enables users to craft
    stories with the support of language models by sketching storylines metaphorically.
    Storybuddy (Zhang et al., [2022](#bib.bib86)) produces interactive storytelling
    experiences by generating story-related questions. Cococo (Louie et al., [2020](#bib.bib48))
    investigates the challenges and opportunities inherent in co-creating music with
    AI, especially for beginners. CodeToon (Suh et al., [2022](#bib.bib66)) automatically
    converts code into comics. However, while AI holds significant promise for enhancing
    a user’s creative abilities by managing certain aspects of the creative workflow,
    it also brings forward challenges and concerns such as user agency and trusts
    (Kang and Lou, [2022](#bib.bib31)), the authenticity of the creation (McCormack
    et al., [2019](#bib.bib51)), potential creative biases (Magni et al., [2023](#bib.bib50);
    Loughran, [2022](#bib.bib47)), and ownership and credit attribution (Eshraghian,
    [2020](#bib.bib22); Bisoyi, [2022](#bib.bib11)). Our work builds upon existing
    literature in human-AI co-creation (Wang et al., [2019a](#bib.bib74); Rezwana
    and Maher, [2022](#bib.bib58); Buschek et al., [2021](#bib.bib15); Park et al.,
    [2019](#bib.bib56); Khadpe et al., [2020](#bib.bib33); Amershi et al., [2019](#bib.bib9);
    Liu, [2021](#bib.bib41); Glikson and Woolley, [2020](#bib.bib25); Eshraghian,
    [2020](#bib.bib22); Kang and Lou, [2022](#bib.bib31)) and further contributes
    by developing a new AI system for video editing and studying its impact. Through
    the lens of LAVE, we examined the dynamics of user interactions with an LLM-based
    agent and explored the opportunities and challenges inherent in the proposed editing
    paradigm.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能在生成内容和自动化任务方面能力的不断提升，它越来越多地被纳入到各个领域的创作过程当中（Mirowski 等，[2023](#bib.bib52)；Yuan
    等，[2022](#bib.bib84)；Chung 等，[2022](#bib.bib21)；Mirowski 等，[2023](#bib.bib52)；Chakrabarty
    等，[2023](#bib.bib16)；Louie 等，[2022](#bib.bib49)；Huang 等，[2020](#bib.bib27)；Louie
    等，[2020](#bib.bib48)；Suh 等，[2022](#bib.bib66)；Wang 等，[2022](#bib.bib72)）。这包括故事写作（Chung
    等，[2022](#bib.bib21)；Yuan 等，[2022](#bib.bib84)；Mirowski 等，[2023](#bib.bib52)；Chakrabarty
    等，[2023](#bib.bib16)）、音乐创作（Louie 等，[2022](#bib.bib49)；Huang 等，[2020](#bib.bib27)；Louie
    等，[2020](#bib.bib48)）、漫画创作（Suh 等，[2022](#bib.bib66)）以及游戏设计（Zhu 等，[2018](#bib.bib87)）。例如，TaleBrush（Chung
    等，[2022](#bib.bib21)）通过隐喻地勾画故事情节，帮助用户借助语言模型创作故事。Storybuddy（Zhang 等，[2022](#bib.bib86)）通过生成与故事相关的问题，创造互动的讲故事体验。Cococo（Louie
    等，[2020](#bib.bib48)）研究了与人工智能共同创作音乐的挑战和机遇，特别是对于初学者而言。CodeToon（Suh 等，[2022](#bib.bib66)）自动将代码转换成漫画。然而，尽管人工智能在通过管理创作工作流的某些方面来提升用户的创作能力方面具有重要潜力，但它也带来了如用户代理权和信任（Kang
    和 Lou，[2022](#bib.bib31)）、创作真实性（McCormack 等，[2019](#bib.bib51)）、潜在的创作偏见（Magni
    等，[2023](#bib.bib50)；Loughran，[2022](#bib.bib47)）以及所有权和归属权（Eshraghian，[2020](#bib.bib22)；Bisoyi，[2022](#bib.bib11)）等挑战和关注点。我们的工作基于现有的人工智能与人类共同创作的文献（Wang
    等，[2019a](#bib.bib74)；Rezwana 和 Maher，[2022](#bib.bib58)；Buschek 等，[2021](#bib.bib15)；Park
    等，[2019](#bib.bib56)；Khadpe 等，[2020](#bib.bib33)；Amershi 等，[2019](#bib.bib9)；Liu，[2021](#bib.bib41)；Glikson
    和 Woolley，[2020](#bib.bib25)；Eshraghian，[2020](#bib.bib22)；Kang 和 Lou，[2022](#bib.bib31)），并通过开发一种新的视频编辑人工智能系统并研究其影响，进一步贡献了新的见解。通过LAVE的视角，我们考察了用户与基于LLM的代理的互动动态，并探索了提出的编辑范式中固有的机遇和挑战。
- en: 3\. Design Goals
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 设计目标
- en: This work aims to explore the potential of a collaborative experience between
    humans and LLM agents in video editing through the design, implementation, and
    evaluation of the LAVE system. To this end, we outlined two primary design goals
    that serve as the guiding principles for the system design.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的目标是通过LAVE系统的设计、实施和评估，探索人类与LLM代理之间的协作体验的潜力。为此，我们概述了两个主要设计目标，这些目标作为系统设计的指导原则。
- en: D1\. Harnessing Natural Language to Lower Editing Barriers. The central proposition
    of this work is to enhance manual video editing paradigms with the power of natural
    language and LLMs. We intended to design LAVE to lower barriers to editing for
    users by leveraging the linguistic intelligence of LLMs from the initial ideation
    to the editing operations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: D1\. 利用自然语言降低编辑障碍。本工作的核心提议是利用自然语言和LLMs的力量来增强手动视频编辑范式。我们旨在设计LAVE，以通过从初步构想到编辑操作过程中利用LLMs的语言智能，降低用户的编辑障碍。
- en: D2\. Preserving User Agency in the Editing Process. A common concern regarding
    AI-assisted content editing is the potential loss of user autonomy and control.
    To mitigate this concern, we designed LAVE to offer both AI-assisted and manual
    editing options. This allows users to refine or opt out of AI assistance as needed,
    thereby preserving user agency. It ensures that the final product reflects the
    user’s artistic vision and grants them decision-making authority.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: D2\. 在编辑过程中保留用户自主权。有关AI辅助内容编辑的一个常见担忧是用户自主权和控制权的潜在丧失。为了缓解这一担忧，我们设计了LAVE，提供AI辅助和手动编辑选项。这使得用户可以根据需要精细调整或选择退出AI帮助，从而保留用户自主权。它确保最终产品反映用户的艺术愿景，并赋予他们决策权。
- en: '![Refer to caption](img/47c940531e679038ee1da400493a744d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/47c940531e679038ee1da400493a744d.png)'
- en: 'Figure 2\. LAVE’s video editing timeline: Users can drag and drop video clips
    to rearrange their order. The order can also be changed through LAVE’s video editing
    agent’s storyboarding function. To trim a clip, users can double-click it, revealing
    a pop-up window for trimming as shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.2.2\.
    Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE:
    LLM-Powered Agent Assistance and Language Augmentation for Video Editing") .'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '图2\. LAVE的视频编辑时间轴：用户可以拖放视频剪辑以重新排列顺序。顺序也可以通过LAVE的视频编辑代理的故事板功能进行更改。要修剪剪辑，用户可以双击它，弹出修剪窗口，如图[4](#S4.F4
    "图4 ‣ 4.2.2\. 剪辑修剪 ‣ 4.2\. 视频编辑时间轴 ‣ 4\. LAVE用户界面 ‣ LAVE: LLM驱动的代理辅助与语言增强视频编辑")所示。'
- en: 4\. The LAVE User Interface
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. LAVE用户界面
- en: 'Guided by the design goals, we developed the LAVE system. LAVE’s UI comprises
    three primary components: 1) the Language Augmented Video Gallery, which displays
    video footage with automatically generated language descriptions; 2) the Video
    Editing Timeline, containing the master timeline for editing; and 3) the Video
    Editing Agent, enabling users to interact with a conversational agent and receive
    assistance. When users communicate with the agent, the message exchanges are displayed
    in the chat UI. The agent can also make changes to the video gallery and the editing
    timeline when relevant actions are taken. Additionally, users can interact directly
    with the gallery and timeline using a cursor, similar to traditional editing interfaces.
    In the subsequent sections, we describe the details of each component and highlight
    their connection to the design goals.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计目标的指导下，我们开发了LAVE系统。LAVE的UI包含三个主要组件：1) 语言增强视频库，显示带有自动生成语言描述的视频片段；2) 视频编辑时间轴，包含用于编辑的主时间轴；3)
    视频编辑代理，允许用户与对话代理互动并获得帮助。当用户与代理沟通时，消息交换会显示在聊天UI中。代理还可以在采取相关操作时对视频库和编辑时间轴进行更改。此外，用户还可以使用光标直接与库和时间轴进行交互，类似于传统编辑界面。在后续部分，我们将描述每个组件的详细信息，并强调它们与设计目标的关联。
- en: 4.1\. Language-Augmented Video Gallery
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 语言增强视频库
- en: 'LAVE features a language-augmented video gallery, as shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1\. Language-Augmented Video Gallery ‣ 4\. The LAVE User Interface
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing").
    Like traditional tools, it allows clip playback but uniquely offers visual narrations,
    i.e., auto-generated textual descriptions for each video, including semantic titles
    and summaries. The titles can assist in understanding and indexing clips without
    needing playback. The summaries provide an overview of each clip’s visual content,
    which could assist users in shaping the storylines for their editing projects.
    The title and duration are displayed under each video. Hovering over a video reveals
    a tooltip with the narrative summary. Users can select clips to add to the editing
    timeline using the ‘Add to Timeline’ button. If users wish to use all of their
    videos (e.g., all footage from a trip), they can simply use the ‘Select/Deselect
    All’ option to add them to the timeline. Moreover, LAVE enables users to search
    for videos using semantic language queries, with the retrieved videos presented
    in the gallery and sorted by relevance. This function must be performed through
    the editing agent, which we will discuss further in the corresponding sections.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 具有一个语言增强的视频库，如图 [3](#S4.F3 "图 3 ‣ 4.1\. 语言增强的视频库 ‣ 4\. LAVE 用户界面 ‣ LAVE:
    基于 LLM 的代理辅助和语言增强的视频编辑") 所示。与传统工具一样，它允许剪辑播放，但独特之处在于提供视觉叙述，即每个视频的自动生成文本描述，包括语义标题和摘要。标题可以帮助理解和索引剪辑，而无需播放。摘要提供了每个剪辑的视觉内容概述，这有助于用户为其编辑项目构建故事情节。标题和时长显示在每个视频下方。将鼠标悬停在视频上会显示带有叙述摘要的工具提示。用户可以使用“添加到时间轴”按钮选择剪辑以添加到编辑时间轴中。如果用户希望使用所有视频（例如，一次旅行的所有镜头），他们只需使用“全选/取消全选”选项将其添加到时间轴中。此外，LAVE
    允许用户通过语义语言查询来搜索视频，检索到的视频将呈现在图库中，并按相关性排序。此功能必须通过编辑代理进行，我们将在相应部分进一步讨论。'
- en: '![Refer to caption](img/f366cb3acab079847dc95e90d0df1422.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f366cb3acab079847dc95e90d0df1422.png)'
- en: Figure 3\. LAVE’s language-augmented video gallery features each video with
    a semantic title and its length (A). When users hover their cursor over a video,
    a detailed summary appears, allowing them to preview the video content without
    playing it (B). Users can select multiple videos to add to the timeline. Selected
    videos will be highlighted in light grey (C) and those already added will appear
    with faded opacity (D).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. LAVE 的语言增强视频库为每个视频提供了一个语义标题和长度 (A)。当用户将光标悬停在视频上时，会出现详细的摘要，允许他们在不播放视频的情况下预览内容
    (B)。用户可以选择多个视频添加到时间轴中。选定的视频将以浅灰色突出显示 (C)，而已添加的视频将以褪色的不透明度显示 (D)。
- en: 4.2\. Video Editing Timeline
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 视频编辑时间轴
- en: 'Once videos are selected from the video gallery and added to the editing timeline,
    they are displayed on the video editing timeline at the bottom of the interface
    (Figure [2](#S3.F2 "Figure 2 ‣ 3\. Design Goals ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing")). Each clip on the timeline is represented
    by a box that showcases three thumbnails: the start, midpoint, and end frames
    of the video to illustrate its content. In the LAVE system, each thumbnail frame
    represents one second worth of footage within the clip. As in the video gallery,
    the titles and descriptions of each clip are also provided. The editing timeline
    in LAVE features two key functions: clip sequencing and trimming. Each offers
    LLM-based and manual options, affording users flexibility and control over AI
    assistance (D2).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦从视频库中选择视频并添加到编辑时间轴中，它们会显示在界面底部的视频编辑时间轴上（图 [2](#S3.F2 "图 2 ‣ 3\. 设计目标 ‣ LAVE:
    基于 LLM 的代理辅助和语言增强的视频编辑")）。时间轴上的每个剪辑由一个框表示，框内展示了三个缩略图：视频的开始、中点和结束帧，以说明其内容。在 LAVE
    系统中，每个缩略图帧代表剪辑中一秒钟的镜头。与视频库中一样，每个剪辑的标题和描述也会提供。LAVE 的编辑时间轴具有两个关键功能：剪辑排序和裁剪。每个功能提供基于
    LLM 的选项和手动选项，为用户提供对 AI 辅助的灵活性和控制（D2）。'
- en: 4.2.1\. Clip Sequencing
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 剪辑排序
- en: 'Sequencing clips on a timeline is a common task in video editing, essential
    for creating a cohesive narrative. LAVE supports two sequencing methods: 1) LLM-based
    sequencing operates via the storyboarding function of LAVE’s video editing agent.
    This function orders clips based on a user-provided or LLM-generated storyline.
    We will further this feature in the agent sections. 2) Manual sequencing allows
    users to arrange clips through direct manipulation, enabling them to drag and
    drop each video box to set the order in which the clips will appear. If users
    want to remove videos from the timeline, they can select specific clips and click
    the ”Delete” button. There is also a ”Clear All” option for removing all videos
    from the timeline simultaneously. Additionally, users can reverse any edits using
    the ”Undo” button. To preview the combined output of the current clip sequence,
    users can click the ”Play” button, after which the system generates a preview
    video for review.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间轴上排序片段是视频编辑中的常见任务，对于创建连贯的叙述至关重要。LAVE 支持两种排序方法：1) 基于 LLM 的排序通过 LAVE 的视频编辑助手的故事板功能操作。此功能根据用户提供或
    LLM 生成的故事情节对片段进行排序。我们将在助手部分进一步介绍此功能。2) 手动排序允许用户通过直接操作来安排片段，用户可以拖放每个视频框来设置片段出现的顺序。如果用户想从时间轴中移除视频，可以选择特定片段并点击“删除”按钮。也可以使用“清除所有”选项一次性移除所有视频。此外，用户还可以使用“撤销”按钮来恢复任何编辑操作。要预览当前片段序列的合成输出，用户可以点击“播放”按钮，系统会生成预览视频供审阅。
- en: 4.2.2\. Clip Trimming
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 剪辑剪裁
- en: 'Trimming is essential in video editing to highlight key segments and remove
    redundant content. To trim, users double-click a clip in the timeline, opening
    a pop-up that displays one-second frames (Figure [4](#S4.F4 "Figure 4 ‣ 4.2.2\.
    Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE:
    LLM-Powered Agent Assistance and Language Augmentation for Video Editing")). Similar
    to Clip sequencing, LAVE supports both LLM-based and manual clip trimming:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '剪辑在视频编辑中至关重要，可以突出关键片段并去除冗余内容。要剪辑，用户双击时间轴中的片段，会弹出显示一秒钟画面的窗口（图 [4](#S4.F4 "Figure
    4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE User Interface
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")）。类似于片段排序，LAVE
    支持基于 LLM 和手动剪辑：'
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM-based Trimming: Below the frames, a text box is provided for users to input
    trimming commands to extract video segments based on their specifications. These
    commands can be free-form. For instance, they might refer to the video’s semantic
    content, such as ”keep only the segment focusing on the baseball game”, or specify
    precise trimming details like ”Give me the last 5 seconds.” Commands can also
    combine both elements, like ”get 3 seconds where the dog sits on the chair”. This
    functionality harnesses the LLM’s information extraction capability (Agrawal et al.,
    [2022](#bib.bib8)) to identify segments aligning with user descriptions. For transparency,
    the LLM also explains its rationale for the trimmings, detailing how they align
    with user instructions. Note that while the feature is also powered by LLM, it
    is not part of the LAVE editing agent’s operations, which primarily handle video
    operations at the project level. This trimming feature is specifically designed
    for individual clip adjustments.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 LLM 的剪裁：在画面下方提供了一个文本框，供用户输入剪裁命令，以根据他们的规格提取视频片段。这些命令可以是自由格式的。例如，它们可能会提到视频的语义内容，如“仅保留专注于棒球比赛的片段”，或者指定精确的剪裁细节，如“给我最后
    5 秒”。命令也可以结合这两者，例如“获取狗坐在椅子上的 3 秒”。此功能利用 LLM 的信息提取能力（Agrawal et al., [2022](#bib.bib8)）来识别与用户描述相符的片段。为了透明起见，LLM
    还解释了其剪裁的理由，详细说明了如何与用户指令对齐。请注意，虽然此功能也由 LLM 提供支持，但它不是 LAVE 编辑助手操作的一部分，后者主要处理项目级别的视频操作。此剪裁功能专为单个片段调整而设计。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Manual Trimming: Users can manually select frames to define the starting and
    ending points of a clip by clicking on the thumbnails. This feature also allows
    users to refine LLM-based trimming when it does not align with their intentions.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 手动剪裁：用户可以通过点击缩略图手动选择帧，以定义剪辑的起始和结束点。此功能还允许用户在 LLM 基于剪裁不符合其意图时进行调整。
- en: '![Refer to caption](img/4294439602eca8e9462533216da0a3eb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4294439602eca8e9462533216da0a3eb.png)'
- en: Figure 4\. LAVE’s clip-trimming window displays user guide (A) and video frames
    sampled every second from the clip (B). Users can manually set the start and end
    frames for trimming. Alternatively, they can use the LLM-powered trimming feature
    with commands like ”Give me 5 seconds focusing on the nearby cherry blossom tree.”
    (D). With this approach, the trim automatically adjusts and includes a rationale
    explaining the LLM’s choice (C). Frames not included in the trimmed clip are displayed
    in a dimmed color.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. LAVE的剪辑裁剪窗口显示用户指南（A）和从剪辑中每秒采样的视频帧（B）。用户可以手动设置裁剪的开始和结束帧。或者，他们可以使用LLM驱动的裁剪功能，输入诸如“给我5秒钟专注于附近的樱花树”这样的命令（D）。使用这种方法，裁剪会自动调整，并包含解释LLM选择的理由（C）。未包含在裁剪剪辑中的帧会以暗色显示。
- en: '![Refer to caption](img/1674ebca43fb076bfc272d5688f4aaed.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1674ebca43fb076bfc272d5688f4aaed.png)'
- en: 'Figure 5\. LAVE’s video editing agent operates in two states: Planning and
    Executing. In the Planning state (left), users provide editing commands (A). The
    agent then clarifies the goal (B) and proposes actionable steps to achieve the
    goal (C). Users have the option to revise the plan if they are not satisfied with
    the proposed steps. Upon user approval of the plan, the agent transitions to the
    Executing state (right). In this state, the user approves the agent’s actions
    sequentially. Following each action, the agent presents the results (Ds). If additional
    actions are outlined in the plan, the agent notifies the user of the next action
    (Es) and waits for their approval (Fs).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. LAVE的视频编辑助手在两种状态下运行：规划状态和执行状态。在规划状态（左侧），用户提供编辑命令（A）。然后，助手澄清目标（B），并提出实现目标的可操作步骤（C）。如果用户对提出的步骤不满意，可以选择修订计划。在用户批准计划后，助手过渡到执行状态（右侧）。在此状态下，用户依次批准助手的操作。每次操作后，助手展示结果（Ds）。如果计划中列出了额外的操作，助手会通知用户下一步操作（Es），并等待其批准（Fs）。
- en: 4.3\. Video Editing Agent
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 视频编辑助手
- en: LAVE’s video editing agent is a chat-based component that facilitates interactions
    between the user and an LLM-based agent. Unlike command-line tools, users can
    interact with the agent using free-form language. The agent offers video editing
    assistance leveraging the linguistic intelligence of LLMs and can provide tailored
    responses to guide and assist users throughout the editing process (D1). LAVE’s
    agent assistance is provided through agent actions, each involving the execution
    of an editing function supported by the system. In the following sections, we
    outline the interaction experience with the agent and describe the editing functions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE的视频编辑助手是一个基于聊天的组件，旨在促进用户与LLM（大语言模型）助手之间的互动。与命令行工具不同，用户可以使用自由形式的语言与助手互动。该助手利用LLM的语言智能提供视频编辑帮助，并能提供量身定制的响应，以指导和协助用户完成整个编辑过程（D1）。LAVE的助手帮助通过助手操作提供，每个操作都涉及执行系统支持的编辑功能。在接下来的部分中，我们将详细说明与助手的互动体验并描述编辑功能。
- en: 4.3.1\. Interacting with the Plan-and-Execute Agent
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 与计划和执行助手的互动
- en: 'To collaborate with the agent, users begin the process by typing their editing
    objectives. The agent interprets the user’s objectives and formulates an action
    plan to fulfill them (Karpas et al., [2022](#bib.bib32); Yao et al., [2023](#bib.bib82);
    Shinn et al., [2023](#bib.bib63); Shen et al., [2023](#bib.bib61)). The agent
    operates in two modes: Planning and Executing. By default, the agent starts in
    the Planning state (Figure [5](#S4.F5 "Figure 5 ‣ 4.2.2\. Clip Trimming ‣ 4.2\.
    Video Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing")-left). In this state,
    whenever a user inputs an editing goal, the agent evaluates it to determine what
    actions to perform to fulfill the user’s goal. The agent can execute multiple
    actions, particularly when a user’s objective is broad and involves diverse operations.
    For instance, if a user types, ”I want to make a video but I don’t have any ideas,”
    the agent may propose a plan that includes brainstorming ideas, finding relevant
    footage, and arranging clips to craft a narrative based on the brainstormed concepts.
    On the other hand, users can also issue a specific command so the action plan
    contains exactly one desired action. The proposed plan requires user approval
    before execution and the user can request adjustments or clarifications (D2).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '为了与代理程序协作，用户首先输入他们的编辑目标。代理程序会解释用户的目标，并制定一个行动计划来实现这些目标（Karpas et al., [2022](#bib.bib32);
    Yao et al., [2023](#bib.bib82); Shinn et al., [2023](#bib.bib63); Shen et al.,
    [2023](#bib.bib61)）。代理程序有两种模式：规划和执行。默认情况下，代理程序从规划状态开始（见图 [5](#S4.F5 "图 5 ‣ 4.2.2\.
    剪辑修剪 ‣ 4.2\. 视频编辑时间线 ‣ 4\. LAVE 用户界面 ‣ LAVE: 基于LLM的代理助手与视频编辑语言增强")-左）。在这种状态下，每当用户输入一个编辑目标时，代理程序会评估这个目标，以确定需要执行哪些操作来实现用户的目标。代理程序可以执行多个操作，特别是当用户的目标广泛且涉及多种操作时。例如，如果用户输入“我想做一个视频，但没有任何想法”，代理程序可能会提出一个计划，其中包括头脑风暴、寻找相关素材和安排剪辑，以根据头脑风暴的概念构建叙事。另一方面，用户也可以发出具体命令，使得行动计划中只包含一个期望的操作。提出的计划需要用户确认后才能执行，用户可以请求调整或澄清（D2）。'
- en: 'Execution begins after the user presses ”enter”—this user approval transitions
    the agent to the Executing state, wherein it begins executing the planned actions
    sequentially (Figure [5](#S4.F5 "Figure 5 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video
    Editing Timeline ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing")-right). After each action is carried
    out, the agent informs the user of the results and the next action, if available.
    The user can then either press ”enter” again to proceed with subsequent actions
    or engage with the agent to alter or cancel the remaining plan. The agent maintains
    a memory buffer for previous conversations, allowing it to access the recent context
    when proposing functions. For example, if the agent has previously brainstormed
    ideas with the user, it might suggest performing video retrieval based on the
    idea the user selected.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '执行在用户按下“enter”键后开始——这种用户确认将代理程序转换到执行状态，在该状态下，代理程序开始按顺序执行计划中的操作（见图 [5](#S4.F5
    "图 5 ‣ 4.2.2\. 剪辑修剪 ‣ 4.2\. 视频编辑时间线 ‣ 4\. LAVE 用户界面 ‣ LAVE: 基于LLM的代理助手与视频编辑语言增强")-右）。每执行一个操作后，代理程序会通知用户结果和下一步操作（如果有的话）。用户可以选择再次按下“enter”键以继续后续操作，或与代理程序互动以修改或取消剩余计划。代理程序保持一个内存缓冲区，用于存储先前的对话，从而在提出功能建议时可以访问最近的上下文。例如，如果代理程序之前与用户进行了头脑风暴，它可能会根据用户选择的想法建议执行视频检索。'
- en: 4.3.2\. Editing Functions
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 编辑功能
- en: 'LAVE’s agent supports four editing functions: Footage Overviewing and Idea
    Brainstorming provide conceptualization assistance based on LLM’s summarization
    and ideation abilities, respectively. The other two, Video Retrieval and Storyboarding,
    leverage LLM’s embedding and storytelling capabilities, respectively, to facilitate
    the manual editing process.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 的代理程序支持四种编辑功能：素材概览和创意头脑风暴分别提供基于LLM的总结和创意能力的概念化辅助。其他两种功能，即视频检索和故事板制作，分别利用LLM的嵌入和叙事能力，以促进手动编辑过程。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Footage Overviewing: The agent can generate an overview text that summarizes
    the videos the user provided in the gallery, categorizing them based on themes
    or topics. For instance, clips from a road trip to the Grand Canyon might be categorized
    under themes like ”Hiking and Outdoor Adventures” or ”Driving on Highways.” This
    feature is particularly helpful when users are not familiar with the footage,
    such as when editing videos from older collections or dealing with extensive video
    sets.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频概述：代理可以生成总结用户在画廊中提供的视频的概述文本，根据主题或话题对其进行分类。例如，前往大峡谷的公路旅行剪辑可能会被归类为“徒步旅行和户外冒险”或“高速公路驾驶”等主题。这一功能对于用户不熟悉素材时特别有用，比如在编辑旧集合中的视频或处理大量视频集时。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Idea Brainstorming: The agent can assist in brainstorming video editing ideas
    based on the gallery videos. This allows the agent to suggest various concepts,
    helping to ignite the users’ creative sparks. For example, the agent might suggest
    using several clips of the user’s pet to create a video on the topic, ”A Day in
    the Life of Pets—from Day to Night.” Additionally, users can provide the agent
    with optional creative guidance or constraints to guide the agent’s ideation process.'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思路头脑风暴：代理可以根据画廊视频协助头脑风暴视频编辑创意。这允许代理建议各种概念，帮助激发用户的创意火花。例如，代理可能建议使用用户宠物的多个剪辑来创建一个主题为“宠物的一天——从白天到黑夜”的视频。此外，用户还可以为代理提供可选的创意指导或限制，以指导代理的创意过程。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Video Retrieval: Searching for relevant footage is a fundamental yet often
    tedious aspect of video editing. Instead of the user manually searching the gallery,
    the agent can assist by retrieving videos based on language queries, such as ”Strolling
    around the Eiffel Tower.” After completing the retrieval, the agent will present
    the most relevant videos in the language-augmented video gallery, sorted by relevance.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频检索：寻找相关素材是视频编辑中一个基本但常常繁琐的环节。代理可以通过基于语言查询（例如“在埃菲尔铁塔周围散步”）来协助检索视频，而无需用户手动搜索画廊。完成检索后，代理将在语言增强的视频画廊中按相关性展示最相关的视频。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Storyboarding: Video editing often requires sequencing clips in the timeline
    to construct a specific narrative. The agent can assist users in ordering these
    clips based on a narrative or storyline provided by the users. The narrative can
    be as concise as ”from indoor to outdoor”, or more detailed, for example, ”starting
    with city landscapes, transitioning to food and drinks, then moving to the night
    social gathering.” If users do not provide a storyline, the agent will automatically
    generate one based on the videos already added to the timeline. Once the agent
    generates a storyboard, the videos in the timeline will be re-ordered accordingly.
    The agent will also provide a scene-by-scene description of the storyboard in
    the chatroom.'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 故事板制作：视频编辑通常需要在时间轴中排列剪辑，以构建特定的叙事。代理可以根据用户提供的叙事或故事情节协助用户排列这些剪辑。叙事可以简短如“从室内到室外”，也可以更详细，例如“从城市风景开始，过渡到食品和饮料，然后转到夜间社交聚会。”如果用户未提供故事情节，代理将根据已添加到时间轴中的视频自动生成一个。代理生成故事板后，时间轴中的视频将相应地重新排序。代理还将在聊天室中提供逐场景的故事板描述。
- en: 4.4\. Supported Workflows and Target Use Cases
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 支持的工作流程和目标使用案例
- en: Altogether, LAVE provides features that span a workflow from ideation and pre-planning
    to the actual editing operations. However, the system does not impose a strict
    workflow. Users have the flexibility to utilize a subset of features that align
    with their editing objectives. For instance, a user with a clear editing vision
    and a well-defined storyline might bypass the ideation phase and dive directly
    into editing. In addition, LAVE is currently designed for casual editing, such
    as creating videos for social media platforms. We leave the integration of LLM
    agents into professional editing, where utmost precision is crucial, as future
    work.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，LAVE 提供了一个涵盖从创意和前期规划到实际编辑操作的工作流程的功能。然而，系统并未强制要求严格的工作流程。用户可以灵活使用符合其编辑目标的部分功能。例如，具有明确编辑视野和精确故事情节的用户可能会跳过创意阶段，直接进入编辑。此外，LAVE
    目前设计用于休闲编辑，如为社交媒体平台创建视频。我们将 LLM 代理集成到专业编辑中（需要极高精度的领域）留待未来工作。
- en: '![Refer to caption](img/6ecc82600fb1bcdbc298faafb247b7e0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6ecc82600fb1bcdbc298faafb247b7e0.png)'
- en: 'Figure 6\. LAVE’s plan-and-execute agent design: Upon receiving an input containing
    the user’s editing command, a planning prompt is constructed. This prompt includes
    the planning instruction, past conversations, and the new user command. It is
    then sent to the LLM to produce an action plan, which reflects the user’s editing
    goal and outlines actions to assist the user in achieving this goal. Each action
    is accompanied by a context, which provides additional information relevant to
    the action, such as a language query for video retrieval. The user reviews and
    approves the actions one by one. After an action is approved, it is translated
    into actual Python function calls and executed. This process continues for all
    the actions in the plan, unless the user decides to provide new instructions to
    revise or cancel the plan.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. LAVE的计划-执行代理设计：在收到包含用户编辑命令的输入后，构建一个规划提示。该提示包括规划指令、过去的对话以及新的用户命令。然后将其发送到LLM，生成一个行动计划，该计划反映了用户的编辑目标，并概述了帮助用户实现该目标的行动。每个行动都附有一个上下文，提供与该行动相关的额外信息，例如视频检索的语言查询。用户逐一审查并批准这些行动。经过批准的行动被翻译成实际的Python函数调用并执行。这个过程会持续进行，直到计划中的所有行动都被处理完毕，除非用户决定提供新的指令来修订或取消计划。
- en: 5\. Backend System
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 后台系统
- en: 'We now describe the backend processing and system design that enable the interactive
    components outlined in Section [4](#S4 "4\. The LAVE User Interface ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing"). We start by describing
    the design of LAVE’s video editing agent and delve deeper into the implementation
    of the editing functions. We utilize OpenAI’s GPT-4 (OpenAI, [2023](#bib.bib54))
    for all LLM mentions in the subsequent sections unless stated otherwise.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在描述支持第[4](#S4 "4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing")节中概述的交互组件的后台处理和系统设计。我们首先描述LAVE的视频编辑代理的设计，并深入探讨编辑功能的实现。我们利用OpenAI的GPT-4（OpenAI，[2023](#bib.bib54)）处理随后的所有LLM提及，除非另有说明。'
- en: 5.1\. Agent Design
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 代理设计
- en: 'We built the LAVE agent by leveraging LLMs’ diverse language capabilities,
    including reasoning, planning, and storytelling. The LAVE Agent has two states:
    Planning and Executing. The plan-and-execute approach offers two primary benefits:
    1) It allows users to set high-level objectives encompassing multiple actions,
    removing the necessity to detail every individual action like traditional command
    line tools. 2) Before execution, the agent presents the plan to the user, providing
    a chance for revisions and ensuring that users maintain complete control (D2).
    We designed a backend pipeline to facilitate this plan-and-execute agent. As depicted
    in Figure [6](#S4.F6 "Figure 6 ‣ 4.4\. Supported Workflows and Target Use Cases
    ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing"), the pipeline begins by creating an action plan
    based on user input. This plan is then translated from textual descriptions into
    function calls, which subsequently execute the corresponding functions. We expand
    on the specifics of each step in the subsequent sections.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过利用大型语言模型（LLMs）多样的语言能力，包括推理、规划和讲故事，构建了LAVE代理。LAVE代理有两个状态：规划和执行。计划-执行方法提供了两个主要好处：1)
    它允许用户设定包含多个动作的高层次目标，从而免去了像传统命令行工具那样详细描述每个单独动作的必要性。2) 在执行之前，代理会向用户展示计划，提供修订的机会，并确保用户保持完全控制（D2）。我们设计了一个后台管道来支持这种计划-执行代理。正如图[6](#S4.F6
    "Figure 6 ‣ 4.4\. Supported Workflows and Target Use Cases ‣ 4\. The LAVE User
    Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video
    Editing")所示，该管道首先基于用户输入创建行动计划。然后，将该计划从文本描述翻译成函数调用，随后执行相应的函数。我们将在随后的章节中详细介绍每个步骤的具体内容。'
- en: 5.1.1\. Action Planning
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 行动计划
- en: The action planning of LAVE’s video editing agent employs a specialized LLM
    prompt format, which is informed by previous research on LLM prompting. We incorporated
    action/tool-use agent prompting techniques (Karpas et al., [2022](#bib.bib32);
    Yao et al., [2023](#bib.bib82); Shinn et al., [2023](#bib.bib63); Shen et al.,
    [2023](#bib.bib61)). In this context, the ”tools” or ”actions” are equal to the
    system’s editing functions. We also leveraged insights from the chain-of-thought
    prompting (Wei et al., [2023](#bib.bib78)), which uses LLMs’ reasoning capabilities
    to decompose complex tasks (user goals) into sub-tasks (editing functions). The
    prompt preamble of our system consists of three segments.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE的视频编辑代理的行动计划使用了一种专业的LLM提示格式，这种格式受到先前关于LLM提示的研究启发。我们结合了行动/工具使用代理提示技术（Karpas
    et al., [2022](#bib.bib32); Yao et al., [2023](#bib.bib82); Shinn et al., [2023](#bib.bib63);
    Shen et al., [2023](#bib.bib61)）。在这个背景下，“工具”或“行动”等同于系统的编辑功能。我们还利用了链式思维提示的见解（Wei
    et al., [2023](#bib.bib78)），它利用LLM的推理能力将复杂任务（用户目标）分解为子任务（编辑功能）。我们系统的提示引言由三个部分组成。
- en: (1)
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: 'Role Assignment: An opening paragraph directing the agent to act as a video
    editing assistant tasked with generating an action plan from user commands.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色分配：一个开篇段落，指示代理充当视频编辑助手，负责从用户命令生成行动计划。
- en: (2)
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: 'Action Descriptions: Following the role assignment, we describe a list of actions
    that the agent can perform. Each action corresponds to an editing function supported
    by LAVE. We detail the functionality and use cases of each, assisting the agent
    in selecting appropriate responses to meet the user’s commands.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动描述：在角色分配之后，我们描述代理可以执行的一系列行动。每个行动对应LAVE支持的一个编辑功能。我们详细说明每个功能及其使用案例，以帮助代理选择适当的响应以满足用户的命令。
- en: (3)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （3）
- en: 'Format Instruction: Lastly, we guide the agent to output the action plan in
    a consistent format: First, determine the user’s editing goal, followed by a stepwise
    plan enumerating suggested actions to achieve that goal. Each action includes
    the function name and its associated context, if applicable. For instance, ”Storyboarding
    (function name): Create a storyboard from day to night. (context)” We also instruct
    the model to prefix the user’s goal and action list with the capitalized words
    ”GOAL” and ”ACTIONS,” respectively, to facilitate text parsing for downstream
    processing.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 格式说明：最后，我们指导代理以一致的格式输出行动计划：首先，确定用户的编辑目标，然后列出逐步计划，列举实现该目标的建议行动。每个行动包括功能名称及其相关上下文（如适用）。例如，“故事板（功能名称）：从白天到夜晚创建一个故事板。（上下文）”。我们还指示模型用大写字母“GOAL”和“ACTIONS”分别为用户的目标和行动列表添加前缀，以便于后续处理的文本解析。
- en: After the preamble, we append the recent conversation history, along with the
    latest user input. This combination forms the complete prompt sent to the LLM
    for generating an action plan. The conversation history is useful when the user
    wants to refer to a previous message or a generated plan, e.g., if they want to
    change a plan that the agent proposed. The system retains up to 6000 tokens of
    message history. If this limit is exceeded, it will begin removing messages starting
    with the second oldest, while preserving the oldest message, i.e., the preamble.
    The 6000-token limit, set empirically, is approximately 2000 tokens fewer than
    the context window of the LLM used, ensuring space for text generation (25% of
    context limit). This setting can be adjusted to accommodate the lengths of different
    LLMs’ context windows. The tokens are byte pair encoding (BPE) (Shibata et al.,
    [1999](#bib.bib62)) tokens utilized by LLMs such as GPT-4.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在引言之后，我们附上最近的对话历史以及最新的用户输入。这些组合形成了发送给LLM的完整提示，以生成行动计划。对话历史在用户希望引用以前的消息或生成的计划时非常有用，例如，如果他们想要更改代理提出的计划。系统最多保留6000个令牌的消息历史。如果超出此限制，它将开始删除从第二条最旧的消息开始的消息，同时保留最旧的消息，即引言。6000个令牌的限制是根据经验设定的，比使用的LLM的上下文窗口少大约2000个令牌，确保文本生成的空间（上下文限制的25%）。此设置可以调整以适应不同LLM的上下文窗口长度。这些令牌是LLMs（如GPT-4）使用的字节对编码（BPE）令牌（Shibata
    et al., [1999](#bib.bib62)）。
- en: 5.1.2\. Translating Action Plan to Executable Functions
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2. 将行动计划翻译为可执行功能
- en: 'As discussed in Section [4.3.1](#S4.SS3.SSS1 "4.3.1\. Interacting with the
    Plan-and-Execute Agent ‣ 4.3\. Video Editing Agent ‣ 4\. The LAVE User Interface
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing"),
    upon formulating an action plan, it is presented to the user for approval. Rather
    than batch approval, each action is sequentially approved by the user. This method
    allows the user to execute one action, observe its results, and then decide whether
    to proceed with the subsequent action. To facilitate this process, LAVE parses
    each action description from the action plan and translates it into a corresponding
    backend function call. We utilize an OpenAI GPT-4 checkpoint, which has been fine-tuned
    for Function Calling (fun, [2023](#bib.bib5)), to accomplish this translation.
    To make use of the Function Calling feature, we provide detailed descriptions
    of each function’s capabilities. Once completed, the LLM can transform a textual
    prompt, specifically an action description in our case, into the corresponding
    editing function call with contextually extracted arguments. The results of the
    function execution are updated in the frontend UI and presented to the user.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '如[4.3.1](#S4.SS3.SSS1 "4.3.1\. 与计划执行代理的互动 ‣ 4.3\. 视频编辑代理 ‣ 4\. LAVE 用户界面 ‣
    LAVE: LLM 驱动的代理支持和语言增强视频编辑")节所讨论，一旦制定行动计划，它会呈现给用户进行批准。不是批量批准，而是每个行动都由用户逐一批准。这种方法允许用户执行一个行动，观察其结果，然后决定是否继续执行后续行动。为了简化这一过程，LAVE
    从行动计划中解析每个行动描述，并将其转换为相应的后台函数调用。我们使用了经过函数调用微调的 OpenAI GPT-4 检查点（fun, [2023](#bib.bib5)）来完成这一翻译。为了利用函数调用功能，我们提供了每个函数能力的详细描述。一旦完成，LLM
    就可以将文本提示，特别是我们案例中的行动描述，转换为相应的编辑函数调用，并提取上下文参数。函数执行的结果会更新到前端 UI，并呈现给用户。'
- en: Table 1\. Input, output, and the parts of the UI that receive updates for each
    LLM-powred editing function. Gallery Videos and Timeline Videos refer to the visual
    narration of the corresponding videos in text format. Optional Guidance indicates
    that the user can provide extra, optional input to guide the function.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 每个 LLM 驱动的编辑功能的输入、输出以及接收更新的 UI 部分。画廊视频和时间轴视频指的是相应视频的文本格式视觉叙述。可选指导表示用户可以提供额外的可选输入以指导功能。
- en: '| Function | Input | Output | UI Updates |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | 输入 | 输出 | UI 更新 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Video Retrieval | Text Query + Vector Store | Ranked Videos | Video Gallery
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 视频检索 | 文本查询 + 向量存储 | 排名视频 | 视频画廊 |'
- en: '| Footage Overviewing | Gallery Videos | Overview | Agent Chat |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 镜头概述 | 画廊视频 | 概述 | 代理聊天 |'
- en: '| Idea Brainstorming | Gallery Videos + Optional Guidance | Ideas | Agent Chat
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 创意头脑风暴 | 画廊视频 + 可选指导 | 创意 | 代理聊天 |'
- en: '| Storyboarding | Timeline Videos + Optional Guidance | Storyboard + Video
    Order | Agent Chat + Timeline |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 故事板制作 | 时间轴视频 + 可选指导 | 故事板 + 视频顺序 | 代理聊天 + 时间轴 |'
- en: '| Clip Trimming | Frame Captions + Trimming Command | Start/End Frame IDs +
    Rationale | Timeline |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 剪辑修整 | 帧字幕 + 修整命令 | 起始/结束帧 ID + 理由 | 时间轴 |'
- en: 5.2\. Implementation of LLM-Powered Editing Functions
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. LLM 驱动编辑功能的实现
- en: 'LAVE supports five LLM-powered functions to assist users in video editing tasks:
    1) Footage Overview, 2) Idea Brainstorming, 3) Video Retrieval, 4) Storyboarding,
    and 5) Clip Trimming. The first four of them are accessible through the agent
    (Figure [5](#S4.F5 "Figure 5 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline
    ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing")), while clip trimming is available via the window
    that appears when double-clicking clips on the editing timeline (Figure [4](#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")). Among them, language-based video retrieval is implemented
    with a vector store database, while the rest are achieved through LLM prompt engineering.
    All functions are built on top of the automatically generated language descriptions
    of raw footage, including the titles and summaries of each clip as illustrated
    in the video gallery (Figure [3](#S4.F3 "Figure 3 ‣ 4.1\. Language-Augmented Video
    Gallery ‣ 4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance and
    Language Augmentation for Video Editing")). We refer to these textual descriptions
    of videos as visual narrations as they describe the narratives in the visual aspects
    of the video. Table [1](#S5.T1 "Table 1 ‣ 5.1.2\. Translating Action Plan to Executable
    Functions ‣ 5.1\. Agent Design ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing") outlines the input, output,
    and UI updates for each function. Figure [10](#A0.F10 "Figure 10 ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing") in the appendix
    provides additional illustrations of each function’s mechanism. In the following
    sub-sections, we start by describing the pre-processing that generates visual
    narrations and then delve into the implementation of each function.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 支持五种由 LLM 驱动的功能来协助用户进行视频编辑任务：1）镜头概览，2）创意头脑风暴，3）视频检索，4）分镜头脚本制作，以及 5）剪辑修整。其中前四项功能可以通过代理访问（图
    [5](#S4.F5 "Figure 5 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣
    4\. The LAVE User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing")），而剪辑修整则可以通过在编辑时间线中双击剪辑时出现的窗口来访问（图 [4](#S4.F4
    "Figure 4 ‣ 4.2.2\. Clip Trimming ‣ 4.2\. Video Editing Timeline ‣ 4\. The LAVE
    User Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")）。其中，基于语言的视频检索是通过一个向量存储数据库实现的，而其他功能则是通过 LLM 提示工程实现的。所有功能都建立在对原始镜头的自动生成语言描述之上，包括每个剪辑的标题和摘要，如视频画廊中所示（图
    [3](#S4.F3 "Figure 3 ‣ 4.1\. Language-Augmented Video Gallery ‣ 4\. The LAVE User
    Interface ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video
    Editing")）。我们将这些视频的文本描述称为视觉叙述，因为它们描述了视频的视觉方面的叙事。表 [1](#S5.T1 "Table 1 ‣ 5.1.2\.
    Translating Action Plan to Executable Functions ‣ 5.1\. Agent Design ‣ 5\. Backend
    System ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video
    Editing") 概述了每个功能的输入、输出和 UI 更新。附录中的图 [10](#A0.F10 "Figure 10 ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing") 提供了每个功能机制的附加说明。在接下来的子章节中，我们将首先描述生成视觉叙述的预处理，然后深入探讨每个功能的实现。'
- en: '5.2.1\. Generating Visual Narration: Video Title and Summary'
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 生成视觉叙述：视频标题和摘要
- en: The process of generating visual narrations involves sampling video frames at
    a rate of one frame per second. Each frame is then captioned using LLaVA (Liu
    et al., [2023a](#bib.bib42)) v1.0, which is built upon Vicuna-V1-13B (Chiang et al.,
    [2023](#bib.bib20)), a fine-tuned checkpoint of LLaMA-V1-13B model (Touvron et al.,
    [2023](#bib.bib68)). After compiling the frame descriptions, we leverage GPT-4
    to generate titles and summaries. Furthermore, each video is assigned a unique
    numeric ID. This ID aids the LLM in referencing individual clips for functions
    such as storyboarding. Note that during the development phase, we chose LLaVA
    due to its language model’s ability to generate more comprehensive captions than
    other contemporary models commonly used for image captioning, such as BLIP-2 (Li
    et al., [2023b](#bib.bib38)). However, we were aware of the rapid evolution of
    VLMs, and that newer models might soon outperform LLaVA v.1.0\. We discuss the
    integration of these models in the future work section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生成视觉叙述的过程包括以每秒一个帧的速度对视频帧进行采样。每个帧然后使用LLaVA（Liu et al., [2023a](#bib.bib42)）v1.0进行标注，该模型基于Vicuna-V1-13B（Chiang
    et al., [2023](#bib.bib20)），这是LLaMA-V1-13B模型（Touvron et al., [2023](#bib.bib68)）的微调检查点。编译帧描述后，我们利用GPT-4生成标题和摘要。此外，每个视频被分配一个唯一的数字ID。这个ID帮助LLM引用个别片段用于诸如故事板制作等功能。请注意，在开发阶段，我们选择了LLaVA，因为它的语言模型能生成比其他常用于图像标注的现代模型（如BLIP-2（Li
    et al., [2023b](#bib.bib38)））更全面的标注。然而，我们意识到VLMs的快速发展，更新的模型可能很快会超越LLaVA v.1.0。我们在未来工作部分讨论了这些模型的整合。
- en: 5.2.2\. Video Retrieval based on Text Embedding
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 基于文本嵌入的视频检索
- en: LAVE’s video retrieval feature utilizes a vector store, constructed by embedding
    the visual narrations (titles and summaries) of each video using OpenAI’s text-embedding-ada-002.
    This process results in 1536-dimensional embeddings for each video. During retrieval,
    LAVE embeds the query, identified from the user’s command, with the same model
    and computes the cosine distances between the query and the stored video embeddings
    to rank the videos accordingly. Subsequently, LAVE updates the frontend UI video
    gallery with videos sorted based on the ranking. Although our design primarily
    focuses on a ranking-based approach for displaying the retrieved results, it can
    easily be modified to incorporate filtering methods, such as displaying only the
    top-k relevant videos.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE的视频检索功能利用了一个向量存储，通过使用OpenAI的text-embedding-ada-002对每个视频的视觉叙述（标题和摘要）进行嵌入，构建了这个存储。这一过程为每个视频生成了1536维的嵌入。在检索过程中，LAVE使用相同的模型对从用户命令中识别出的查询进行嵌入，并计算查询与存储视频嵌入之间的余弦距离，从而对视频进行排名。随后，LAVE更新前端UI视频画廊，按照排名对视频进行排序。虽然我们的设计主要侧重于基于排名的方法来显示检索结果，但可以轻松修改为包含过滤方法，例如仅显示前k个相关视频。
- en: 5.2.3\. Footage Overviewing
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 片段概述
- en: 'We prompt the LLM to categorize videos into common themes, providing a summary
    of topics within a user’s video collection. The prompt includes a function instruction
    ([A.1](#A1.SS1 "A.1\. Footage Overview ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing")), followed by the
    visual narrations of the gallery videos. This prompt is then sent to the LLM to
    generate the overview, which is subsequently presented in the chat UI for the
    user to review.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提示LLM将视频分类到常见主题中，并提供用户视频集合中的主题摘要。提示包括一个功能指令（[A.1](#A1.SS1 "A.1\. Footage
    Overview ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance and
    Language Augmentation for Video Editing")），接着是画廊视频的视觉叙述。然后将这个提示发送到LLM，以生成概述，随后在聊天UI中呈现供用户审阅。'
- en: 5.2.4\. Idea Brainstorming
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 思路头脑风暴
- en: 'We prompt the LLM to generate creative video editing ideas based on all the
    user’s videos. The prompt structure begins with a function instruction (see [A.2](#A1.SS2
    "A.2\. Idea Brainstorming ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing")). If provided, we include
    creative guidance from the user in the prompt to guide the brainstorming. The
    creative guidance is extracted as a string argument when LAVE maps action descriptions
    to function calls (Section [5.1.2](#S5.SS1.SSS2 "5.1.2\. Translating Action Plan
    to Executable Functions ‣ 5.1\. Agent Design ‣ 5\. Backend System ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing")). If the user does
    not provide any guidance, it defaults to ”general”. Following the creative direction,
    we append the visual narrations of all gallery videos and send the prompt to LLM
    for completion. Similar to the footage overview, the generated video ideas will
    be presented in the chat UI.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提示 LLM 基于所有用户的视频生成创意视频编辑想法。提示结构以功能指令开始（见 [A.2](#A1.SS2 "A.2\. Idea Brainstorming
    ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing")）。如果提供了创意指导，我们将在提示中包含这些指导以指导头脑风暴。创意指导在 LAVE 将动作描述映射到功能调用时被提取为字符串参数（见
    [5.1.2](#S5.SS1.SSS2 "5.1.2\. Translating Action Plan to Executable Functions
    ‣ 5.1\. Agent Design ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing")）。如果用户没有提供任何指导，则默认设置为“普通”。在创意方向的指导下，我们附加所有画廊视频的视觉叙述，并将提示发送给
    LLM 以完成。与素材概述类似，生成的视频创意将在聊天 UI 中展示。'
- en: 5.2.5\. Storyboarding
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. 分镜绘制
- en: 'LAVE’s storyboarding function arranges video clips in a sequence based on a
    user-provided narrative. Unlike the former functions, it affects only the videos
    in the timeline. Similar to Idea Brainstorming, the system checks for any creative
    guidance on the narrative provided by the user, for example, ”Start with my dog’s
    videos then transition to my cat’s.” If no guidance is given, the LLM is instructed
    to create a narrative based on timeline videos. The prompt begins with a function
    instruction ([A.3](#A1.SS3 "A.3\. Storyboarding ‣ Appendix A Prompt Preambles
    ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing")),
    followed by any user narrative guidance, and then the visual narrations of the
    timeline videos. The output is structured in JSON format, with the key "storyboard"
    mapping to texts detailing each scene, and "video_ids" mapping to a list of video
    IDs indicating the sequence. This format aids downstream processing in parsing
    the results. Once the execution is complete, the "storyboard" containing scene
    descriptions will be displayed in the chat UI, and the video order on the timeline
    will be updated according to "video_ids"'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 的分镜功能根据用户提供的叙述将视频剪辑按顺序排列。与之前的功能不同，它只影响时间线上的视频。类似于创意头脑风暴，系统会检查用户提供的叙述是否有创意指导，例如，“从我狗的视频开始，然后过渡到我猫的视频。”
    如果没有提供指导，则指示LLM根据时间线视频创建叙述。提示以功能指令开始（[A.3](#A1.SS3 "A.3\. Storyboarding ‣ Appendix
    A Prompt Preambles ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")），接着是任何用户叙述指导，然后是时间线视频的视觉叙述。输出结构为 JSON 格式，其中键 "storyboard"
    对应详细描述每个场景的文本，"video_ids" 对应表示顺序的视频 ID 列表。该格式有助于下游处理解析结果。执行完成后，包含场景描述的 "storyboard"
    将显示在聊天 UI 中，并且时间线上的视频顺序将根据 "video_ids" 更新。'
- en: 5.2.6\. Clip Trimming
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. 剪辑修剪
- en: 'LAVE leverages the reasoning and information parsing capabilities of LLMs for
    trimming video clips. This function analyzes frame captions to identify a video
    segment that matches a user’s trimming command. The function instruction is detailed
    in [A.4](#A1.SS4 "A.4\. Clip Trimming ‣ Appendix A Prompt Preambles ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing"). Following the
    instruction, the user’s trimming command and the frame-by-frame captions generated
    during preprocessing are appended. This compiled prompt is then sent to the LLM
    for completion. The outputs are also structured in JSON format: "segment": ["start",
    "end", "rationale"], indicating the start and end frame IDs, as well as the rationale
    for this prediction. Upon receiving the LLM’s response, LAVE updates the UI to
    display the suggested trim segment and its rationale, thereby aiding the user’s
    understanding of the LLM’s decision-making process. Currently, LAVE’s trimming
    precision is one second based on the frame sample rate used in preprocessing.
    This precision can be adjusted by varying the sampling rates.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'LAVE 利用 LLM 的推理和信息解析能力来修剪视频剪辑。此功能分析帧标题，以识别与用户修剪命令匹配的视频段。功能说明详见 [A.4](#A1.SS4
    "A.4\. 剪辑修剪 ‣ 附录 A 提示前言 ‣ LAVE: LLM 驱动的代理辅助和语言增强用于视频编辑")。根据说明，用户的修剪命令和预处理期间生成的逐帧标题被附加。这个编译的提示然后被发送到
    LLM 进行完成。输出也以 JSON 格式结构化："segment": ["start", "end", "rationale"]，表示开始和结束帧 ID
    以及此预测的理由。收到 LLM 的响应后，LAVE 更新 UI 以显示建议的修剪段及其理由，从而帮助用户理解 LLM 的决策过程。目前，LAVE 的修剪精度基于预处理时使用的帧采样率为一秒。通过改变采样率可以调整这种精度。'
- en: Table 2\. Background of the study participants, including their prior experience
    in video editing, the types of videos they have previously created, and their
    self-reported understanding of LLM’s capabilities and limitations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 研究参与者的背景，包括他们之前的视频编辑经验、他们之前创建的视频类型以及他们自我报告的对 LLM 能力和局限性的理解。
- en: '| Participants | Editing Experience | Types of Videos Created Before | Understand
    LLM’s Capability |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 参与者 | 编辑经验 | 之前创建的视频类型 | 理解 LLM 的能力 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| P1 | Proficient | Animated/Explainer | Slightly Disagree |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| P1 | 熟练 | 动画/解释 | 稍微不同意 |'
- en: '| P2 | Proficient | Project | Slightly Agree |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| P2 | 熟练 | 项目 | 稍微同意 |'
- en: '| P3 | Proficient | Promotional/Action | Slightly Agree |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| P3 | 熟练 | 宣传/行动 | 稍微同意 |'
- en: '| P4 | Beginner | Social Media | Slightly Agree |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| P4 | 初学者 | 社交媒体 | 稍微同意 |'
- en: '| P5 | Beginner | Project/Presentation | Slightly Agree |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| P5 | 初学者 | 项目/演示 | 稍微同意 |'
- en: '| P6 | Proficient | Social Media | Slightly Agree |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| P6 | 熟练 | 社交媒体 | 稍微同意 |'
- en: '| P7 | Beginner | Presentation | Slightly Disagree |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| P7 | 初学者 | 演示 | 稍微不同意 |'
- en: '| P8 | Beginner | (Outdated Experience) | Strongly Agree |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| P8 | 初学者 | （过时经验） | 强烈同意 |'
- en: 5.3\. System Implementation
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 系统实施
- en: We implemented the LAVE system as a full-stack web application. The frontend
    UI was developed using React.js, while the backend server uses Flask. For LLM
    inferences, we primarily use the latest GPT-4 model from OpenAI. However, for
    mapping action plans to functions, we employ the gpt-4-0613 checkpoint, specifically
    fine-tuned for function call usage. The maximum context window length for GPT-4
    was 8192 tokens during the time we built the system. With these limits, our agent
    could accommodate and process descriptions from approximately 40 videos in a single
    LLM call. We use LangChain (lan, [2023](#bib.bib7))’s wrapper of ChromaDB (chr,
    [2023](#bib.bib3)) to construct the vector store. Video pre-processing is performed
    on a Linux machine equipped with an Nvidia V100 GPU. Finally, we use ffmpeg to
    synthesize the outcome of users’ video edits.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 LAVE 系统实现为一个全栈 web 应用程序。前端 UI 使用 React.js 开发，而后端服务器使用 Flask。对于 LLM 推理，我们主要使用
    OpenAI 最新的 GPT-4 模型。然而，对于将行动计划映射到功能上，我们使用了 gpt-4-0613 检查点，该检查点特别针对函数调用进行了微调。在我们构建系统的过程中，GPT-4
    的最大上下文窗口长度为 8192 个标记。在这些限制下，我们的代理可以在一次 LLM 调用中容纳和处理来自大约 40 个视频的描述。我们使用 LangChain
    (lan, [2023](#bib.bib7)) 的 ChromaDB (chr, [2023](#bib.bib3)) 封装器来构建向量存储。视频预处理在配备
    Nvidia V100 GPU 的 Linux 机器上进行。最后，我们使用 ffmpeg 来合成用户视频编辑的结果。
- en: 6\. User Study
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 用户研究
- en: We conducted a user study to obtain user feedback on the usage of LAVE. Our
    study aimed to 1) gauge the effectiveness of LAVE’s language augmentation in assisting
    video editing tasks, and 2) understand user perceptions of an LLM-powered agent
    within the editing process, particularly its impact on their sense of agency and
    creativity. For the study, we enlisted participants to use LAVE for editing videos
    using their own footage, allowing us to test LAVE’s functionality and utility
    across a diverse range of content. In presenting the results, we relate the findings
    to the design goals of lowering editing barriers with natural language (D1) and
    maintaining user agency (D2), highlighting their fulfillment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了用户研究，以获取对LAVE使用情况的用户反馈。我们的研究旨在1）评估LAVE在协助视频编辑任务中的语言增强效果，以及2）了解用户对编辑过程中的LLM驱动代理的看法，特别是其对他们的自主性和创造力的影响。为了进行这项研究，我们邀请参与者使用自己的素材编辑视频，以测试LAVE在各种内容中的功能和实用性。在呈现结果时，我们将发现与降低自然语言编辑障碍（D1）和保持用户自主性（D2）的设计目标相关联，突出了它们的实现情况。
- en: 6.1\. Participants
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 参与者
- en: 'We are interested in understanding how users with diverse video editing experiences
    receive language-augmented video editing powered by LLM. To this end, we recruited
    participants with varying video editing experiences to gather feedback on their
    perceptions of LAVE. Table [2](#S5.T2 "Table 2 ‣ 5.2.6\. Clip Trimming ‣ 5.2\.
    Implementation of LLM-Powered Editing Functions ‣ 5\. Backend System ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing") presents the background
    information of each participant. We recruited eight participants from a tech company,
    of which three were female, with an average age of 27.6 (STD=3.16). Four of them
    (P4, P5, P7, P8) identified as beginners in video editing, possessing little to
    moderate experience. Among the beginners, P8 reported having the least experience,
    and the last time he edited videos was years ago. Conversely, the other four participants
    (P1-3, P6) view themselves as proficient, having extensive experience with video
    editing tools. Among the proficient participants: P1 is a designer but occasionally
    edits videos for work; P2, having minored in film studies, has been editing videos
    since high school; P3 runs a YouTube channel and also edits personal family videos;
    while P6, a PhD student, edits life-log videos for social media weekly. This diverse
    group allowed us to evaluate LAVE’s performance across various editing backgrounds.
    All participants have had some experience with LLMs. When asked whether they understood
    the capabilities and limitations of LLMs, participants’ responses ranged from
    ”Slightly Disagree” to ”Strongly Agree”.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对不同视频编辑经验的用户如何接受LLM增强的视频编辑感兴趣。为此，我们招募了具有不同视频编辑经验的参与者，以收集他们对LAVE的看法。表[2](#S5.T2
    "Table 2 ‣ 5.2.6\. Clip Trimming ‣ 5.2\. Implementation of LLM-Powered Editing
    Functions ‣ 5\. Backend System ‣ LAVE: LLM-Powered Agent Assistance and Language
    Augmentation for Video Editing")展示了每位参与者的背景信息。我们从一家科技公司招募了八名参与者，其中三名为女性，平均年龄为27.6岁（STD=3.16）。其中四名（P4、P5、P7、P8）自认为是视频编辑初学者，拥有少量到中等的经验。在这些初学者中，P8表示其经验最少，上一次编辑视频是在多年前。相反，其他四名参与者（P1-3、P6）认为自己是熟练者，具有丰富的视频编辑工具经验。在熟练参与者中：P1是一名设计师，但偶尔为了工作编辑视频；P2辅修了电影学，自高中起便开始编辑视频；P3经营一个YouTube频道，同时编辑个人家庭视频；P6是一名博士生，每周为社交媒体编辑生活记录视频。这一多样化的群体使我们能够评估LAVE在各种编辑背景下的表现。所有参与者都有一定的LLM经验。当被问及是否理解LLM的能力和局限性时，参与者的回应从“稍微不同意”到“强烈同意”不等。'
- en: 6.2\. Study Protocol
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 研究协议
- en: A day before the study, participants were asked to submit a set of videos for
    pre-processing. They were asked to provide at least 20 clips, each less than a
    minute long, to fully leverage the system’s features. The study duration ranged
    from 1 to 1.5 hours and was conducted in a quiet environment to minimize distractions.
    Upon arrival, participants were provided with an overview of the study and a detailed
    explanation of the LAVE system’s features, which took about 15 to 20 minutes.
    They then engaged with the LAVE system using their own footage, aiming to produce
    at least one video. Participants had the freedom to explore and produce multiple
    videos, yet they were required to adhere to a 20 to 30-minute time frame. After
    their session with the system, participants completed a questionnaire. We solicited
    feedback on the usefulness and ease of use of each LLM-powered function and the
    system as a whole. Questions were also posed regarding trust, agency, outcome
    responsibility, and participants’ perceptions of the roles played by the agent.
    Additionally, we adapted applicable questions from the Creative Support Index
    (Cherry and Latulipe, [2014](#bib.bib19)). Finally, users provided their preferences
    between agent assistance and manual operations for each editing function. All
    the questions in the questionnaire were based on a 7-point Likert Scale. Subsequently,
    we conducted semi-structured interviews lasting approximately 20 to 30 minutes.
    Throughout the study, participants were encouraged to share their thoughts and
    ask any questions following the think-aloud method (Van Someren et al., [1994](#bib.bib70)).
    We did not instruct participants to prioritize speed during the study, as it was
    not the objective. The aim was to observe how users leverage LAVE for video editing
    and gather feedback.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究前一天，参与者被要求提交一组视频进行预处理。他们被要求提供至少 20 个每个时长不到一分钟的剪辑，以充分利用系统的功能。研究持续时间为 1 到 1.5
    小时，并在安静环境中进行，以减少干扰。到达后，参与者会收到关于研究的概述和 LAVE 系统功能的详细解释，耗时约 15 到 20 分钟。然后，他们使用自己的镜头与
    LAVE 系统进行互动，目标是制作至少一个视频。参与者可以自由探索并制作多个视频，但需要遵守 20 到 30 分钟的时间框架。与系统互动后，参与者填写了问卷。我们征求了关于每个
    LLM 驱动功能和整体系统的有用性和易用性的反馈。同时，还提出了有关信任、代理、结果责任和参与者对代理角色的看法的问题。此外，我们从 Creative Support
    Index (Cherry 和 Latulipe, [2014](#bib.bib19)) 中调整了适用的问题。最后，用户提供了关于每个编辑功能中代理协助和手动操作的偏好。问卷中的所有问题均基于
    7 分 Likert 量表。随后，我们进行了约 20 到 30 分钟的半结构化访谈。在整个研究过程中，鼓励参与者分享他们的想法并提出任何问题，采用了边想边说的方法
    (Van Someren 等人, [1994](#bib.bib70))。我们没有指导参与者在研究期间优先考虑速度，因为这不是目标。研究的目的是观察用户如何利用
    LAVE 进行视频编辑并收集反馈。
- en: '![Refer to caption](img/78443416c34eabd05078f53ff66d0bce.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/78443416c34eabd05078f53ff66d0bce.png)'
- en: Figure 7\. Boxplots showing the ease of use and usefulness of each LLM-powered
    feature in LAVE, including video retrieval, footage overview, idea brainstorming,
    storyboarding, and clip trimming. We also solicited feedback on the video editing
    agent and the overall system. Ratings were based on a 7-point Likert Scale, with
    7 indicating ”extremely easy to use/useful” and 1 being the opposite. Participants
    generally found the capabilities of the agent and the full system easy to use.
    However, variances were observed in usefulness ratings.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 箱线图显示了 LAVE 中每个 LLM 驱动功能的易用性和有用性，包括视频检索、镜头概览、创意头脑风暴、故事板制作和剪辑修剪。我们还征求了关于视频编辑代理和整体系统的反馈。评分基于
    7 分 Likert 量表，其中 7 表示“非常易用/有用”，1 则相反。参与者通常发现代理和整个系统的功能易于使用。然而，在有用性评分上观察到了差异。
- en: 6.3\. Results and Findings
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 结果和发现
- en: We summarize the important results and observations obtained from the user study
    as follows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了从用户研究中获得的重要结果和观察。
- en: 6.3.1\. Editing Outcome and General Impressions
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. 编辑结果和总体印象
- en: All subjects were able to use LAVE to produce satisfactory video results within
    the study session with low frustration (Mean=2, STD=1.3). Seven participants rated
    their satisfaction with the final outcome at 6 out of 7, while participant P2
    gave a score of 5\. Participants found LAVE enjoyable to use (Mean=6.3, STD=0.5)
    and expressed a desire to use it regularly (Mean=5.8, STD=0.9). Notably, we observed
    encouraging results indicating that LAVE reduces editing barriers for inexperienced
    users (D1). For instance, P8, who had edited videos only once before, praised
    the efficiency of using LAVE, stating, ”I really see the value of the tool… in
    20 or 30 minutes, you have a really nice video.” This is reinforced by the fact
    that all beginner users in our study produced satisfactory outcomes in collaboration
    with LAVE during their first session. The findings underscore LAVE’s effectiveness
    in supporting the video editing process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所有受试者在研究会话中能够使用LAVE产生令人满意的视频结果，且挫败感较低（均值=2，标准差=1.3）。七名参与者将最终结果的满意度评分为7分中的6分，而参与者P2则给出了5分的评分。参与者发现使用LAVE令人愉快（均值=6.3，标准差=0.5），并表达了定期使用的愿望（均值=5.8，标准差=0.9）。值得注意的是，我们观察到令人鼓舞的结果，表明LAVE减少了对缺乏经验的用户的编辑障碍（D1）。例如，曾仅编辑过一次视频的P8赞扬了使用LAVE的效率，称“我真的看到了这个工具的价值……在20或30分钟内，你就有了一段非常好的视频。”这一点在我们研究中所有初学者用户与LAVE的首次会话中产生了令人满意的结果得到了证实。这些发现突显了LAVE在支持视频编辑过程中的有效性。
- en: 6.3.2\. Constrating LAVE with Existing Editing Tools
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 将LAVE与现有编辑工具对比
- en: Participants appreciated the novelty of LAVE’s editing paradigm. For instance,
    P3, who is familiar with video editing, commented, ”I think there’s nothing like
    that right now in the market, and I was able to edit a video real quick.” Similarly,
    P5 made intriguing comments about the role he perceived himself in when using
    LAVE, saying, ”The system makes me feel like a director, and it’s nice to edit
    videos with a conversational interface because it feels more natural.” He went
    on to express that he felt he ”operated at a higher level of thinking, which was
    kind of liberating.” (D1). We view this as a promising sign for future content
    editing, where the incorporation of agent-based editing features offers an effective
    alternative to manual operations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者赞赏LAVE编辑范式的新颖性。例如，熟悉视频编辑的P3评论道：“我认为市场上现在没有类似的东西，我能够非常迅速地编辑视频。”类似地，P5对使用LAVE时自己所感受到的角色发表了有趣的评论，称“系统让我感觉像是一个导演，而且用对话界面编辑视频很不错，因为感觉更自然。”他进一步表示，他感到自己“在更高的思维层次上操作，这种感觉很解放。”（D1）。我们将其视为对未来内容编辑的一个积极信号，代理编辑功能的融入为手动操作提供了有效的替代方案。
- en: 6.3.3\. Usability and Usefulness
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3\. 可用性和实用性
- en: 'Participants found the design of LAVE useful and easy to use, as echoed by
    the overall positive ratings LAVE received, which is illustrated in Figure [7](#S6.F7
    "Figure 7 ‣ 6.2\. Study Protocol ‣ 6\. User Study ‣ LAVE: LLM-Powered Agent Assistance
    and Language Augmentation for Video Editing"). However, there were divergent ratings
    regarding the usefulness of some features. We noticed that negative feedback typically
    stemmed from two main reasons. Firstly, participants who highly value originality,
    often proficient editors, tend to prefer maintaining autonomy when conceptualizing
    video ideas and forming their understanding of videos. As a result, they could
    be prone to reject conceptualization assistance from the agent. Secondly, due
    to the stochastic nature of LLMs, outputs of functions such as trimming and storyboarding
    may not always align with user expectations, leading some participants to rate
    their usefulness lower. To gain a deeper understanding of the capabilities and
    limitations of the proposed design, we collected additional user feedback on each
    LLM-powered editing function, which we discuss below.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '参与者发现LAVE的设计实用且易于使用，这从LAVE收到的整体正面评价中得到了体现，如图[7](#S6.F7 "Figure 7 ‣ 6.2\. Study
    Protocol ‣ 6\. User Study ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation
    for Video Editing")所示。然而，对于一些功能的实用性，评分存在分歧。我们注意到，负面反馈通常源于两个主要原因。首先，重视原创性的参与者，通常是熟练的编辑人员，倾向于在构思视频创意和形成对视频的理解时保持自主。因此，他们可能会倾向于拒绝代理的概念化帮助。其次，由于LLMs的随机性，修剪和分镜等功能的输出可能无法始终符合用户期望，导致一些参与者对其实用性评分较低。为了更深入了解所提设计的能力和局限性，我们收集了关于每个LLM驱动编辑功能的额外用户反馈，以下是讨论内容。'
- en: 'Video Retrieval: The feature is unanimously praised for its efficiency in finding
    relevant videos and received the highest ratings for usefulness. As P1 noted,
    ”Having the search function is super helpful. Creative people tend to be disorganized,
    so this can be extremely useful.” Overall, participants were generally surprised
    by how easily they could find videos using natural language without having to
    scroll through the corpus.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 视频检索：该功能因其在查找相关视频方面的高效性而受到一致赞扬，并获得了最高的实用性评分。正如 P1 所述，“拥有搜索功能非常有帮助。创造性的人往往会很混乱，所以这可以极其有用。”总体而言，参与者通常对使用自然语言轻松找到视频而无需滚动查看语料库感到惊讶。
- en: 'Footage Overview and Video Descriptions: In soliciting feedback, we combined
    the footage overview with the pre-generated video narrations (video titles and
    summaries), as they share the same objective of helping users quickly understand
    footage content. P6 found the topics and themes outlined in the footage overview
    useful in aiding him in categorizing the available videos. P2 found the semantic
    titles helpful and commented on the description’s accuracy, stating, ”There was
    a delight in seeing how it titled everything… Sometimes it was a little wrong,
    but often it was very right and impressive.” She also highlighted the usefulness
    of semantic titles over arbitrary IDs commonly assigned by capture devices.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 镜头概述和视频描述：在征求反馈时，我们将镜头概述与预生成的视频叙述（视频标题和摘要）结合在一起，因为它们具有相同的目标，即帮助用户快速了解镜头内容。P6
    发现镜头概述中概述的主题和内容对他分类可用视频很有帮助。P2 认为语义标题很有用，并评论了描述的准确性，表示：“看到它如何为所有内容命名是一种欣喜……有时会稍有错误，但通常非常正确和令人印象深刻。”她还强调了语义标题相对于捕捉设备常分配的任意
    ID 的实用性。
- en: 'Idea Brainstorming: Brainstorming was found beneficial in aiding the initial
    conceptualization for the majority of the participants. As quoted by P3, it can
    ”spark my creativity.” P8 noted its usefulness in providing starting concepts
    when he had no ideas. However, not all participants welcomed external input. P2,
    for example, resisted such suggestions and stated, ”The brainstorming didn’t totally
    work for me. Part of that’s because I wouldn’t even outsource that to a human
    assistant.” In addition, P7, having already formed an idea, found brainstorming
    unnecessary. Moreover, P6, while appreciating the ideas generated by the LLM,
    expressed concerns about potential bias, commenting, ”It’s giving you initial
    ideas, but it might also bias you into thinking about specific things.”'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 创意头脑风暴：头脑风暴被发现对大多数参与者在初步构思阶段有帮助。正如 P3 所引用的，它可以“激发我的创造力。”P8 指出，当他没有想法时，它在提供起始概念方面非常有用。然而，并不是所有参与者都欢迎外部输入。例如，P2
    对这些建议持抵触态度，并表示：“头脑风暴对我来说完全不起作用。部分原因是我甚至不愿将其外包给人类助手。”此外，P7 在已经形成想法的情况下，觉得头脑风暴是不必要的。此外，P6
    尽管欣赏 LLM 生成的想法，但对潜在的偏见表示担忧，评论道：“它给你初始想法，但也可能让你偏向于考虑特定的事物。”
- en: 'Storyboarding: The feature was generally viewed as beneficial for sequencing
    clips. P8 found it supportive, and P4 praised its utility, saying, ”I think it’s
    quite good. I actually had no idea how to sequence them together.” She also valued
    the narratives provided by the LLM, stating, ”It provided pretty good reasoning
    for why it sequenced the videos like this.” However, P2 found the reasoning behind
    some of the storyboards was less grounded, noting, ”When I asked it for the reason,
    it would give something grammatically correct but somewhat nonsensical artistically.”
    This highlights the challenges involved in using LLMs to support storytelling,
    as they may generate or fabricate implausible narratives.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 故事板：这一功能通常被视为对剪辑顺序有益。P8 发现它很有支持性，而 P4 赞扬了其实用性，表示：“我认为它相当不错。我实际上不知道如何将它们排序在一起。”她还重视
    LLM 提供的叙事，称：“它提供了相当好的理由，说明为什么这样排序视频。”然而，P2 发现一些故事板的推理基础较为薄弱，并指出：“当我问它原因时，它会给出一些语法正确但在艺术上有些荒谬的东西。”这突显了使用
    LLM 支持讲故事时所面临的挑战，因为它们可能生成或编造不可信的叙事。
- en: 'Clip Trimming: Being able to trim clips based on language commands has generated
    excitement among users. For instance, while using this feature, P5 remarked, ”It’s
    like telling people what to do, and then they do it; it’s kind of amazing.” He
    was editing a clip that panned from a view inside the car, focusing on the road
    ahead, to the side window showcasing a group of tall buildings. P5 requested the
    system to trim five seconds from that transition, and the resulting clip perfectly
    met his expectations. However, we have observed a non-negligible number of occasions
    when the LLM inaccurately trimmed videos. This often occurred when users input
    commands about elements not captured in the system-generated visual narrations,
    such as brightness or motion. The inaccuracy has led to diminished enthusiasm
    and lower usefulness ratings, indicating the need for future research.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 剪辑修剪：能够根据语言指令修剪剪辑引起了用户的兴奋。例如，在使用此功能时，P5 说：“这就像告诉人们该做什么，然后他们就会去做，这有点令人惊讶。”他正在编辑一个从车内视角，聚焦前方道路，到侧窗展示一组高楼的剪辑。P5
    请求系统将过渡部分修剪五秒钟，结果剪辑完美符合他的期望。然而，我们观察到 LLM 不准确修剪视频的情况并不少见。这通常发生在用户输入关于系统生成的视觉叙述中未捕捉到的元素的指令时，如亮度或运动。这种不准确性导致了热情降低和有用性评分下降，表明需要进一步的研究。
- en: 6.3.4\. Trust, Agency, and Outcome Responsibility
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4\. 信任、代理和结果责任
- en: 'Figure [8](#S6.F8 "Figure 8 ‣ 6.3.5\. Perceptions of the Role of the Editing
    Agent ‣ 6.3\. Results and Findings ‣ 6\. User Study ‣ LAVE: LLM-Powered Agent
    Assistance and Language Augmentation for Video Editing") showcases the user ratings
    for the questions related to trust, agency, and outcome responsibility. Participants
    found the automation of LAVE to be generally trustworthy and felt they had a high
    level of control when using the system, highlighting that they retained agency
    despite the AI’s automation (D2). When inquiring about responsibility for final
    outcomes—whether attributed to the AI, the user, or a combined effort—the prevailing
    sentiment rejected the notion that AI solely influenced the end product. Most
    agreed they were personally accountable or that it was a joint effort with the
    AI for the results, except P8, who felt he largely relied on the AI’s suggestions.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8](#S6.F8 "图 8 ‣ 6.3.5\. 对编辑代理角色的看法 ‣ 6.3\. 结果和发现 ‣ 6\. 用户研究 ‣ LAVE：LLM 驱动的代理辅助和语言增强视频编辑")
    展示了与信任、代理和结果责任相关的问题的用户评分。参与者认为 LAVE 的自动化一般是值得信赖的，并感到在使用系统时拥有较高的控制水平，强调了尽管 AI 自动化，他们仍然保留了代理权（D2）。在询问对最终结果的责任——是否归因于
    AI、用户或共同努力时——普遍的观点否定了 AI 单独影响最终产品的看法。大多数人同意他们个人负责或与 AI 共同努力取得结果，除 P8 外，他觉得他主要依赖于
    AI 的建议。
- en: 6.3.5\. Perceptions of the Role of the Editing Agent
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.5\. 对编辑代理角色的看法
- en: 'We further explored how users perceived the role of LAVE’s editing agent: whether
    as an assistant, partner, or leader. Half of the participants regarded the agent
    as an ”assistant” (P2, P3, P7, P8), while the other half perceived it as a ”partner”
    (P1, P4, P5, P6). Notably, none felt as though the AI agent took on a leadership
    role. Those in the assistant category generally viewed the agent as a responsive
    tool, following their directives. Conversely, the partner group likened the agent
    to an equal collaborator, sometimes even equating the experience to engaging with
    a human peer. P5 remarked, ”When using the tool, I had this partnership with the
    AI that is kind of similar to having a conversation with somebody, and we’re trying
    to brainstorm ideas.”. In addition, all participants appreciated the ability to
    have the final say in any editing decision using LAVE, emphasizing their ability
    to easily refine or dismiss the AI’s suggestions (D2).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步探讨了用户对 LAVE 编辑代理角色的看法：是否将其视为助手、伙伴或领导者。半数参与者将代理视为“助手”（P2、P3、P7、P8），而另一半则将其视为“伙伴”（P1、P4、P5、P6）。值得注意的是，没有人觉得
    AI 代理扮演了领导角色。那些在助手类别中的人通常将代理视为响应工具，按照他们的指令行事。相反，伙伴组将代理视为平等的合作者，有时甚至将这种体验等同于与人类同事的互动。P5
    说道：“使用这个工具时，我和 AI 有种类似于与某人交谈的伙伴关系，我们在尝试集思广益。”此外，所有参与者都欣赏在使用 LAVE 时能够对任何编辑决策拥有最终决定权，强调了他们能够轻松地完善或拒绝
    AI 的建议（D2）。
- en: '![Refer to caption](img/63f33dc03bce91c155ba1d63ac77985d.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/63f33dc03bce91c155ba1d63ac77985d.png)'
- en: Figure 8\. Boxplots showcasing user ratings on trust (first row), agency (second
    to fourth rows), and outcome responsibility (fifth to seventh rows). All scores
    use a 7-point Likert scale, where 7 means ”strongly agree” and 1 denotes ”strongly
    disagree.” Questions marked with (+) indicate that a higher score is preferable.,
    while (-) means the contrary.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 展示用户在信任（第一行）、代理性（第二到第四行）和结果责任（第五到第七行）方面的评分的箱线图。所有分数使用7分李克特量表，其中7表示“强烈同意”，1表示“强烈不同意”。标有（+）的问题表示更高的分数更可取，而（-）表示相反。
- en: 6.3.6\. Supporting Creativity and Sense of Co-Creation
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.6\. 支持创造力和共创感
- en: 'As depicted in Figure [9](#S6.F9 "Figure 9 ‣ 6.3.6\. Supporting Creativity
    and Sense of Co-Creation ‣ 6.3\. Results and Findings ‣ 6\. User Study ‣ LAVE:
    LLM-Powered Agent Assistance and Language Augmentation for Video Editing"), users
    generally were positive about the system’s impact on creativity. All users agreed
    to some extent that AI contributed to the creative process. Furthermore, 6 out
    of 8 participants believed that the system enhanced their creativity. As P8 mentioned,
    ”What’s really hindering me from doing video editing is that it’s a very creative
    job, and I feel I lack creativity. This tool addresses that precisely.” (D1).
    However, not all participants felt that the system boosted their creativity–P7
    was neutral, and P2 strongly disagreed with the statement. When inquiring about
    users’ sense of co-creation, the responses ranged from ”slightly disagree” to
    ”strongly agree”. Upon analysis, we found that participants who saw the LAVE agent
    more as a partner (Section [6.3.5](#S6.SS3.SSS5 "6.3.5\. Perceptions of the Role
    of the Editing Agent ‣ 6.3\. Results and Findings ‣ 6\. User Study ‣ LAVE: LLM-Powered
    Agent Assistance and Language Augmentation for Video Editing")) were more likely
    to feel they were co-creating with AI during the video editing process (Mean=6.5,
    STD=1). In contrast, those who regarded the LAVE agent merely as an assistant
    reported a lower sense of co-creation with AI (Mean=4.25, STD=1.9). Lastly, all
    users were positive that the final results were worth the efforts they exerted
    in the process, echoing the reported satisfaction with the outcome.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[9](#S6.F9 "图 9 ‣ 6.3.6\. 支持创造力和共创感 ‣ 6.3\. 结果与发现 ‣ 6\. 用户研究 ‣ LAVE: LLM驱动的代理支持和语言增强用于视频编辑")所示，用户通常对系统在创造力方面的影响持积极态度。所有用户在一定程度上都认为人工智能对创作过程有所贡献。此外，8名参与者中的6名认为该系统提升了他们的创造力。正如P8所提到的，“真正阻碍我进行视频编辑的是这是一个非常创造性的工作，而我觉得自己缺乏创造力。这个工具恰恰解决了这个问题。”（D1）。然而，并非所有参与者都感到系统提升了他们的创造力——P7持中立态度，P2对该声明强烈不同意。在询问用户的共创感时，回应的范围从“稍微不同意”到“强烈同意”。经过分析，我们发现，那些将LAVE代理视为伙伴的参与者（见[6.3.5](#S6.SS3.SSS5
    "6.3.5\. 对编辑代理角色的看法 ‣ 6.3\. 结果与发现 ‣ 6\. 用户研究 ‣ LAVE: LLM驱动的代理支持和语言增强用于视频编辑")）更有可能感到在视频编辑过程中与人工智能共同创作（均值=6.5，标准差=1）。相比之下，那些仅将LAVE代理视为助手的参与者则报告了较低的与人工智能的共创感（均值=4.25，标准差=1.9）。最后，所有用户都对最终结果感到满意，认为这些结果值得他们在过程中付出的努力，这与对结果的满意度一致。'
- en: '![Refer to caption](img/b43b34f4236c3c3082983eb093a7e598.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b43b34f4236c3c3082983eb093a7e598.png)'
- en: Figure 9\. Stacked bar chart of user-reported ratings on questions related to
    the sense of co-creation and those adopted from the creativity support index (Cherry
    and Latulipe, [2014](#bib.bib19)). The horizontal axis in the upper right represents
    the cumulative number of participants. All responses are rated on a 7-point Likert
    scale with 1 being ”strongly disagree” and 7 being ”strongly agree”. Overall,
    participants expressed positive feelings about their sense of co-creation and
    the creativity support provided by the LAVE. However, some questions did receive
    occasional negative feedback, indicating varied perceptions among users. We use
    a stacked bar graph to highlight the exact proportions of user ratings, particularly
    those that lean towards disagreement for additional discussion.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 用户报告的关于共创感的问卷评分和采用创造力支持指数（Cherry 和 Latulipe, [2014](#bib.bib19)）的堆叠条形图。右上角的横轴表示参与者的累计人数。所有回应都使用7分李克特量表进行评分，其中1表示“强烈不同意”，7表示“强烈同意”。总体而言，参与者对他们的共创感以及LAVE提供的创造力支持表现出积极的情感。然而，一些问题确实收到了偶尔的负面反馈，表明用户的感知存在差异。我们使用堆叠条形图来突出用户评分的具体比例，特别是那些倾向于不同意的评分，以便进一步讨论。
- en: 6.3.7\. User Preferences for Agent Support
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.7\. 用户对代理支持的偏好
- en: We observed a spectrum of assistance that users desired from the LAVE agent.
    For conceptualization-related tasks, we observed that users who emphasized their
    creative control showed a tendency to dislike input from the agent (P2, P3). In
    contrast, P8 expressed a strong desire to intake whatever ideas the agent can
    offer. For manual operation tasks, a similar trend exists where not all users
    welcome agent intervention. For example, P8 wants pure automation for storyboarding,
    while P2 and P7 prefer manually sequencing videos. When it comes to clip trimming,
    P3, P7, and P8 preferred manual adjustments, emphasizing that the LLM prediction
    did not fully match their intentions. Overall, the varying preferences across
    users and tasks indicate future agent-assisted content editing should provide
    adaptive support rather than a one-size-fits-all approach.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到用户对LAVE代理所需的辅助程度有所不同。对于概念化相关任务，我们观察到强调创意控制的用户倾向于不喜欢代理的输入（P2，P3）。相比之下，P8表达了强烈的愿望，愿意接收代理提供的任何想法。对于手动操作任务，类似的趋势存在，并非所有用户都欢迎代理干预。例如，P8希望故事板完全自动化，而P2和P7则更喜欢手动排序视频。在剪辑修整方面，P3，P7和P8更倾向于手动调整，强调LLM预测并未完全符合他们的意图。总体而言，不同用户和任务的不同偏好表明，未来的代理辅助内容编辑应提供适应性的支持，而非一刀切的方法。
- en: 6.3.8\. Users’ Mental Models Based on Prior Experience with LLMs
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.8\. 基于对LLM的先前经验的用户心理模型
- en: We observed that prior experience with LLMs could occasionally influence how
    users perceived and interacted with LAVE. We found that users with a deeper understanding
    of LLMs’ capabilities and limitations seemed to quickly develop a mental model
    of how the agent operates. Such users could adapt the way they use the system
    based on what they believe the LLM can process more effectively. For example,
    P5 attempted to reuse words from video titles, assuming that the LLM agent would
    better understand his commands. He also exhibited greater patience when the LLM
    made errors. Further studying how users develop mental models for LAVE and similar
    systems, both among those with and without prior experience with LLMs, is an intriguing
    subject for future research.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，以前对LLM的经验有时会影响用户对LAVE的感知和互动方式。我们发现，对LLM能力和局限性有更深刻理解的用户似乎能迅速形成对代理操作的心理模型。这些用户可以根据他们认为LLM能更有效处理的内容来调整使用系统的方式。例如，P5试图重复使用视频标题中的词语，假设LLM代理会更好地理解他的命令。他在LLM出现错误时也表现出更多的耐心。进一步研究用户如何为LAVE及类似系统建立心理模型，无论是对LLM有经验还是没有经验的用户，都是未来研究的一个有趣课题。
- en: 7\. Design Implications
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 设计含义
- en: Based on the study findings, we discuss design implications to inform the future
    design of LLM-assisted content editing systems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据研究结果，我们讨论了设计含义，以指导未来LLM辅助内容编辑系统的设计。
- en: '1\. Harnessing Language as a Medium for User Interaction and Content Representation
    to Enhance Multimedia Editing: Our study demonstrates the effectiveness of using
    natural language as a medium for both user interaction with the system and for
    representing multimedia content—in our case, representing videos with textual
    descriptions. The use of language as an interaction medium acts as a liberating
    factor, reducing manual effort and enhancing user understanding of the editing
    process. In representing content, language enables us to harness the capabilities
    of LLMs for versatile processing and editing assistance. A significant implication
    of our approach extends beyond mere video editing, suggesting that future systems
    could convert multimedia elements, such as speech, sound events, or even sensory
    inputs like motion, into textual descriptions. This conversion could allow the
    systems to leverage the strengths of natural language and LLMs to improve the
    editing process for a wide range of multimedia content.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 利用语言作为用户互动和内容表示的媒介来提升多媒体编辑：我们的研究展示了使用自然语言作为用户与系统互动的媒介以及表示多媒体内容（在我们的案例中，是用文本描述视频）的有效性。作为互动媒介的语言起到了一个解放因素，减少了手动操作的工作量，并增强了用户对编辑过程的理解。在内容表示方面，语言使我们能够利用LLM的能力进行多功能处理和编辑辅助。我们方法的一个重要含义不仅限于视频编辑，还建议未来的系统可以将多媒体元素，如语音、声音事件或甚至运动等感官输入，转换为文本描述。这种转换可以使系统利用自然语言和LLM的优势，改进广泛多媒体内容的编辑过程。
- en: '2\. Adapting Agent Assistance to User and Task Variability: Our research exemplifies
    how incorporating an LLM agent can improve content editing experiences. However,
    our study also uncovers that preferences for agent assistance can differ across
    user groups and the nature of the editing task at hand (Section [6.3.7](#S6.SS3.SSS7
    "6.3.7\. User Preferences for Agent Support ‣ 6.3\. Results and Findings ‣ 6\.
    User Study ‣ LAVE: LLM-Powered Agent Assistance and Language Augmentation for
    Video Editing")). For instance, users who value original ideas may abstain from
    using brainstorming assistance, while others can be receptive to any suggestions
    from agents. Moreover, the demand for agent support varies among tasks; particularly,
    repetitive or tedious tasks are more likely to be delegated to agents. Consequently,
    we recommend that future systems should provide adaptive agent support, automatically
    tailored to the preferences of the user and the nature of the task. These systems
    could also enable users to activate, deactivate, or customize each assistance
    as needed. Additionally, we suggest they offer flexibility between agent assistance
    and manual editing, allowing users to refine AI predictions and correct potential
    inaccuracies, as demonstrated by LAVE.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '2\. 适应用户和任务变化的代理支持：我们的研究示范了如何通过整合LLM代理来改善内容编辑体验。然而，我们的研究还发现，代理支持的偏好在不同用户群体和编辑任务的性质之间可能存在差异（第[6.3.7节](#S6.SS3.SSS7
    "6.3.7\. 用户对代理支持的偏好 ‣ 6.3\. 结果与发现 ‣ 6\. 用户研究 ‣ LAVE: 基于LLM的代理支持和视频编辑语言增强")）。例如，重视原创想法的用户可能会避免使用头脑风暴辅助，而其他用户则可能对代理的任何建议持开放态度。此外，任务的需求对代理支持的要求也有所不同；特别是重复或单调的任务更可能被委托给代理。因此，我们建议未来的系统应提供自适应代理支持，自动根据用户的偏好和任务的性质进行调整。这些系统还应允许用户根据需要激活、停用或自定义每项辅助功能。此外，我们建议提供代理辅助与手动编辑之间的灵活性，让用户能够细化AI预测并纠正潜在的错误，如LAVE所示。'
- en: '3\. Considering Users’ Prior Experience with LLM Agents in System Design: Our
    study suggests that a user’s prior experience with LLMs may influence the way
    they interact with an editing system featuring an LLM agent. Users with a deep
    understanding of LLMs will likely form a mental model of the agent’s functionalities
    more quickly. Furthermore, those who are adept at using prompting techniques might
    develop more efficient strategies for interacting with the agent. On the other
    hand, users who are not well-informed about the capabilities and constraints of
    LLMs may not utilize the system to its fullest potential. Therefore, it may be
    beneficial for future systems that incorporate LLM agents to integrate instructional
    support, such as visual cues that provide feedforward and feedback guidance, especially
    for users new to LLMs.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 在系统设计中考虑用户对LLM代理的先前经验：我们的研究表明，用户对LLM的先前经验可能会影响他们与包含LLM代理的编辑系统的互动方式。对LLM有深入理解的用户可能会更快地形成对代理功能的心理模型。此外，擅长使用提示技术的用户可能会发展出更有效的与代理互动的策略。另一方面，对LLM的能力和限制不了解的用户可能无法充分利用系统的潜力。因此，将来整合LLM代理的系统可能需要融入教学支持，例如提供前馈和反馈指导的视觉提示，特别是对LLM新用户而言。
- en: '4\. Mitigating Potential Biases in LLM-Assisted Creative Process: LAVE’s ability
    to engage users through conversational interactions was perceived by our study
    participants as both innovative and liberating for video editing, enhancing their
    ability to operate at a higher level of thinking. However, due to the seemingly
    human-like nature of LLMs’ natural language communication, there is a potential
    for user mistrust or biases. Some participants highlighted that reliance on LLM
    suggestions might inadvertently cause them to overlook certain videos they would
    have considered had they worked independently. Moreover, biases present in LLMs
    during their training phase (Felkner et al., [2023](#bib.bib23); Venkit et al.,
    [2023](#bib.bib71); Yu et al., [2023](#bib.bib83)) have the potential to subtly
    influence users’ creative endeavors. Therefore, it is important to carefully consider
    the potential for bias introduced by LLMs in the creative process and take steps
    to mitigate it.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 减少 LLM 辅助创作过程中的潜在偏见：我们的研究参与者认为 LAVE 通过对话互动让用户参与的能力在视频编辑方面既创新又解放，提升了他们在更高层次思考的能力。然而，由于
    LLM 的自然语言交流似乎类似于人类，因此存在用户不信任或偏见的潜在风险。一些参与者指出，依赖 LLM 的建议可能会使他们忽视某些视频，而这些视频如果独立工作可能会被考虑。此外，LLM
    在训练阶段存在的偏见（Felkner et al., [2023](#bib.bib23); Venkit et al., [2023](#bib.bib71);
    Yu et al., [2023](#bib.bib83)）可能会潜移默化地影响用户的创作工作。因此，必须仔细考虑 LLM 在创作过程中引入的偏见，并采取措施加以缓解。
- en: 8\. Limitations and Future Work
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 限制与未来工作
- en: LAVE represents an initial step into the emerging field of system design for
    LLM-assisted content editing. Due to the rapid evolution of LLM research, we acknowledge
    the transient nature of our current design and implementation. We believe the
    enduring value of this work lies not in its specific implementation, which may
    soon evolve, but in its role as the first investigation of the proposed editing
    paradigm. This sets the stage for the continuous evolution of the field. Below,
    we discuss limitations that warrant future investigations.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 代表了向 LLM 辅助内容编辑的新兴系统设计领域迈出的第一步。由于 LLM 研究的快速发展，我们承认当前设计和实施的暂时性。我们相信这项工作的持久价值不在于其具体实施，这可能很快会发展，而在于它作为首次调查所提出编辑范式的角色。这为该领域的持续演变奠定了基础。下面，我们讨论了需要未来研究的限制。
- en: 8.1\. Agent Design
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 代理设计
- en: Our agent design draws inspiration from recent work on tool-use agents (Karpas
    et al., [2022](#bib.bib32); Yao et al., [2023](#bib.bib82); Shinn et al., [2023](#bib.bib63);
    Shen et al., [2023](#bib.bib61)). We anticipate that more sophisticated designs
    will be proposed to support more robust and versatile interactions. For example,
    LAVE currently incorporates a single agent with several functions. These functions
    are executed linearly and do not facilitate back-and-forth discussion within each
    of them. A potential improvement would be to construct a multi-agent system where
    each function is represented as a separate agent with which the user can directly
    interact, for example, a storyboarding agent that engages with users to discuss
    and clarify desired narratives. In addition, LAVE’s agent requires sequential
    user approval for actions, a process that future work could vary or make more
    adaptive. Lastly, while LAVE’s agent presently supports only LLM-based editing
    functions, in practice, it can also incorporate non-LLM-based editing functions,
    such as those in traditional video editing tools, e.g., visual or sound effects.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理设计灵感来源于近期对工具使用代理的研究（Karpas et al., [2022](#bib.bib32); Yao et al., [2023](#bib.bib82);
    Shinn et al., [2023](#bib.bib63); Shen et al., [2023](#bib.bib61)）。我们预计会提出更复杂的设计来支持更强大和多功能的互动。例如，LAVE
    当前整合了一个具有多个功能的单一代理。这些功能是线性执行的，无法在各个功能之间进行来回讨论。一个潜在的改进是构建一个多代理系统，每个功能都表示为一个独立的代理，用户可以直接与之互动，例如，一个故事板代理，可以与用户互动以讨论和澄清所需的叙事。此外，LAVE
    的代理需要用户对每个动作进行顺序批准，未来的工作可以改变或使这一过程更具适应性。最后，尽管 LAVE 的代理目前仅支持基于 LLM 的编辑功能，但实际上，它还可以整合非
    LLM 基础的编辑功能，例如传统视频编辑工具中的视觉或声音效果。
- en: 8.2\. Editing Functions
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 编辑功能
- en: The editing functions provided by LAVE are not intended to be exhaustive, leaving
    room for potential improvements. For instance, they could be improved by explicitly
    distinguishing different aspects of videos, such as objects and activities, and
    exposing these aspects to the agent for more fine-grained editing control. Future
    work could also investigate end-user prompting, enabling users to modify or introduce
    new prompts and tailor their LLM-powered video editing assistance as desired.
    Lastly, future systems could develop evaluation components to provide an automated
    feedback loop in the editing process. An evaluation component could be created,
    for instance, using models capable of assessing visual aesthetics and examining
    the logic flow in videos. This feature could assess the quality of the editing
    function’s output before it is presented, or it could review the current editing
    draft to offer users detailed critiques and suggestions.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LAVE 提供的编辑功能并非详尽无遗，仍有改进的空间。例如，可以通过明确区分视频的不同方面，如对象和活动，并将这些方面暴露给代理，以实现更细致的编辑控制。未来的工作还可以探讨最终用户提示，使用户能够修改或引入新的提示，并根据需要定制他们的
    LLM 驱动的视频编辑辅助功能。最后，未来的系统可以开发评估组件，以在编辑过程中提供自动反馈回路。例如，可以创建一个评估组件，使用能够评估视觉美学和检查视频逻辑流的模型。该功能可以在编辑功能输出呈现之前评估其质量，或审查当前编辑草稿，向用户提供详细的批评和建议。
- en: 8.3\. Model Limitations
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 模型限制
- en: There are key limitations in using LLMs within LAVE that merit investigation.
    Firstly, LLMs such as the GPT-4 model (OpenAI, [2023](#bib.bib54)), initially
    limited to an 8192 token window (now increased to 128k), restrict the amount of
    video information that can be included in a single prompt. Additionally, LLMs
    tend to hallucinate, producing grammatically correct yet nonsensical responses,
    as observed in our user study. Addressing this issue to improve LLM factual accuracy
    is crucial (Tam et al., [2022](#bib.bib67)). While LLMs cannot currently effectively
    process video input, recent advancements in VLMs capable of handling image sequences
    (Yang et al., [2023](#bib.bib81)) suggest the potential for integrating future
    VLMs into LAVE. That said, a benefit of our current setup is that when users interact
    with the system, they may experience quicker processing of textual representation
    of visuals, as opposed to potentially slower processing of images or videos in
    real time.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LAVE 中使用大型语言模型（LLMs）存在一些关键限制，需要进一步研究。首先，像 GPT-4 模型（OpenAI，[2023](#bib.bib54)）这样的
    LLMs，最初限制在 8192 个标记的窗口（现在已增加到 128k），限制了在单个提示中可以包含的视频信息量。此外，LLMs 往往会产生幻觉，生成语法正确但无意义的回应，这在我们的用户研究中有所观察。解决这一问题以提高
    LLM 的事实准确性至关重要（Tam 等，[2022](#bib.bib67)）。虽然 LLMs 目前无法有效处理视频输入，但最近在处理图像序列的视觉语言模型（VLMs）方面的进展（Yang
    等，[2023](#bib.bib81)）表明，未来 VLMs 融入 LAVE 的潜力。也就是说，我们当前设置的一个好处是，当用户与系统互动时，他们可能会体验到文本表示视觉信息的处理速度更快，而不是实时处理图像或视频时可能较慢。
- en: 8.4\. User Evaluation
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4\. 用户评估
- en: Our user study evaluated LAVE with eight participants of varying experience,
    enabling us to understand user perceptions from diverse backgrounds. However,
    we acknowledge potential limitations in generalizability due to our sample size.
    Future studies could involve larger participant groups, diverse user backgrounds,
    or different video editing scenarios to further validate and expand upon our initial
    findings. Moreover, future work can conduct a quantitative evaluation of the agent’s
    performance and longitudinal studies to examine whether users’ behavior with would
    change as they gain more experience with the proposed editing paradigm.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用户研究评估了 LAVE 的八名具有不同经验的参与者，使我们能够了解来自不同背景的用户感知。然而，我们承认由于样本量的限制，可能存在一般化的局限性。未来的研究可以涉及更大规模的参与者群体、多样化的用户背景或不同的视频编辑场景，以进一步验证和扩展我们的初步发现。此外，未来的工作可以对代理的表现进行定量评估，并进行纵向研究，以检查用户行为是否会随着他们对提议编辑范式的更多经验而变化。
- en: 9\. Conclusion
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 结论
- en: We have introduced LAVE, a video editing tool that enables a novel agent-assisted
    video editing paradigm through LLM-powered assistance and language augmentation.
    We outlined the system’s unique design and implementation, along with its supported
    functions and language-augmented features. Our user study assessed the effectiveness
    of LAVE and garnered insights into users’ perceptions and reactions to an LLM
    agent assisting in video editing. Based on the study’s findings, we proposed design
    implications to inform future designs of systems alike. Our work sheds light on
    the future development of agent-assisted media content editing tools. We are optimistic
    about the direction and believe we have only begun to scratch the surface of what
    is possible.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 LAVE，这是一种通过 LLM 驱动的辅助和语言增强实现的新型视频编辑工具。我们概述了系统的独特设计和实施，以及其支持的功能和语言增强特性。我们的用户研究评估了
    LAVE 的有效性，并获得了用户对 LLM 代理在视频编辑中辅助作用的感知和反应的见解。基于研究结果，我们提出了设计启示，以指导未来类似系统的设计。我们的工作为代理辅助媒体内容编辑工具的未来发展提供了启示。我们对这一方向感到乐观，并相信我们仅仅开始触及可能性的表面。
- en: References
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: pre (2023) 2023. *Adobe Premiere Pro*. [https://www.adobe.com/products/premiere.html](https://www.adobe.com/products/premiere.html)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pre (2023) 2023. *Adobe Premiere Pro*。 [https://www.adobe.com/products/premiere.html](https://www.adobe.com/products/premiere.html)
- en: chr (2023) 2023. *ChromaDB*. [https://www.trychroma.com/](https://www.trychroma.com/)
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: chr (2023) 2023. *ChromaDB*。 [https://www.trychroma.com/](https://www.trychroma.com/)
- en: fin (2023) 2023. *Final Cut Pro*. [https://www.apple.com/final-cut-pro/](https://www.apple.com/final-cut-pro/)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fin (2023) 2023. *Final Cut Pro*。 [https://www.apple.com/final-cut-pro/](https://www.apple.com/final-cut-pro/)
- en: fun (2023) 2023. *Function calling and other API updates*. [https://openai.com/blog/function-calling-and-other-api-updates](https://openai.com/blog/function-calling-and-other-api-updates)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fun (2023) 2023. *函数调用及其他 API 更新*。 [https://openai.com/blog/function-calling-and-other-api-updates](https://openai.com/blog/function-calling-and-other-api-updates)
- en: run (2023) 2023. *Gen-2 Runway*. [https://runwayml.com/ai-magic-tools/gen-2/](https://runwayml.com/ai-magic-tools/gen-2/)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: run (2023) 2023. *Gen-2 Runway*。 [https://runwayml.com/ai-magic-tools/gen-2/](https://runwayml.com/ai-magic-tools/gen-2/)
- en: lan (2023) 2023. *Langchain*. [https://www.langchain.com/](https://www.langchain.com/)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lan (2023) 2023. *Langchain*。 [https://www.langchain.com/](https://www.langchain.com/)
- en: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    and David Sontag. 2022. Large language models are zero-shot clinical information
    extractors. *arXiv preprint arXiv:2205.12689* (2022).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    和 David Sontag. 2022. 大型语言模型是零-shot 临床信息提取器。*arXiv 预印本 arXiv:2205.12689* (2022)。
- en: Amershi et al. (2019) Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney,
    Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,
    et al. 2019. Guidelines for human-AI interaction. In *Proceedings of the 2019
    chi conference on human factors in computing systems*. 1–13.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amershi et al. (2019) Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney,
    Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen,
    等. 2019. 人工智能互动指南。见 *2019 年计算系统人因会议论文集*，1–13。
- en: 'Arawjo et al. (2023) Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin
    Wattenberg, and Elena Glassman. 2023. ChainForge: A Visual Toolkit for Prompt
    Engineering and LLM Hypothesis Testing. *arXiv preprint arXiv:2309.09128* (2023).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arawjo et al. (2023) Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin
    Wattenberg, 和 Elena Glassman. 2023. ChainForge: 用于提示工程和 LLM 假设测试的可视化工具包。*arXiv
    预印本 arXiv:2309.09128* (2023)。'
- en: 'Bisoyi (2022) Akanksha Bisoyi. 2022. Ownership, liability, patentability, and
    creativity issues in artificial intelligence. *Information Security Journal: A
    Global Perspective* 31, 4 (2022), 377–386.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisoyi (2022) Akanksha Bisoyi. 2022. 人工智能中的所有权、责任、专利性和创造性问题。*信息安全期刊：全球视角* 31,
    4 (2022), 377–386。
- en: 'Brade et al. (2023) Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore,
    and Tovi Grossman. 2023. Promptify: Text-to-Image Generation through Interactive
    Prompt Exploration with Large Language Models. arXiv:2304.09337 [cs.HC]'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brade et al. (2023) Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore,
    和 Tovi Grossman. 2023. Promptify: 通过大型语言模型的互动提示探索进行文本到图像生成。arXiv:2304.09337 [cs.HC]'
- en: 'Bran et al. (2023) Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller.
    2023. ChemCrow: Augmenting large-language models with chemistry tools. *arXiv
    preprint arXiv:2304.05376* (2023).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bran et al. (2023) Andres M Bran, Sam Cox, Andrew D White, 和 Philippe Schwaller.
    2023. ChemCrow: 用化学工具增强大型语言模型。*arXiv 预印本 arXiv:2304.05376* (2023)。'
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. 语言模型是少样本学习者。arXiv:2005.14165 [cs.CL]
- en: Buschek et al. (2021) Daniel Buschek, Lukas Mecke, Florian Lehmann, and Hai
    Dang. 2021. Nine potential pitfalls when designing human-ai co-creative systems.
    *arXiv preprint arXiv:2104.00358* (2021).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buschek et al. (2021) Daniel Buschek, Lukas Mecke, Florian Lehmann, 和 Hai Dang.
    2021. 设计人机协作创作系统时的九个潜在陷阱。*arXiv预印本 arXiv:2104.00358*（2021年）。
- en: 'Chakrabarty et al. (2023) Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman,
    and Smaranda Muresan. 2023. Creativity Support in the Age of Large Language Models:
    An Empirical Study Involving Emerging Writers. arXiv:2309.12570 [cs.HC]'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakrabarty et al. (2023) Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman,
    和 Smaranda Muresan. 2023. 大型语言模型时代的创意支持：涉及新兴作家的实证研究。arXiv:2309.12570 [cs.HC]
- en: 'Chang et al. (2021) Minsuk Chang, Mina Huh, and Juho Kim. 2021. RubySlippers:
    Supporting Content-Based Voice Navigation for How-to Videos. In *Proceedings of
    the 2021 CHI Conference on Human Factors in Computing Systems* (Yokohama, Japan)
    *(CHI ’21)*. Association for Computing Machinery, New York, NY, USA, Article 97,
    14 pages. [https://doi.org/10.1145/3411764.3445131](https://doi.org/10.1145/3411764.3445131)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang et al. (2021) Minsuk Chang, Mina Huh, 和 Juho Kim. 2021. RubySlippers:
    支持内容为基础的语音导航的操作视频。在 *2021年CHI人机交互系统会议论文集*（横滨，日本）*(CHI ’21)*. 计算机协会，纽约，NY，美国，文章97，14页。
    [https://doi.org/10.1145/3411764.3445131](https://doi.org/10.1145/3411764.3445131)'
- en: Chang et al. (2019) Minsuk Chang, Anh Truong, Oliver Wang, Maneesh Agrawala,
    and Juho Kim. 2019. How to Design Voice Based Navigation for How-To Videos. In
    *Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems*
    (Glasgow, Scotland Uk) *(CHI ’19)*. Association for Computing Machinery, New York,
    NY, USA, 1–11. [https://doi.org/10.1145/3290605.3300931](https://doi.org/10.1145/3290605.3300931)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. (2019) Minsuk Chang, Anh Truong, Oliver Wang, Maneesh Agrawala,
    和 Juho Kim. 2019. 如何为操作视频设计基于语音的导航。在 *2019年CHI人机交互系统会议论文集*（格拉斯哥，苏格兰，英国）*(CHI ’19)*.
    计算机协会，纽约，NY，美国，1–11。 [https://doi.org/10.1145/3290605.3300931](https://doi.org/10.1145/3290605.3300931)
- en: Cherry and Latulipe (2014) Erin Cherry and Celine Latulipe. 2014. Quantifying
    the Creativity Support of Digital Tools through the Creativity Support Index.
    *ACM Trans. Comput.-Hum. Interact.* 21, 4, Article 21 (jun 2014), 25 pages. [https://doi.org/10.1145/2617588](https://doi.org/10.1145/2617588)
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cherry and Latulipe (2014) Erin Cherry 和 Celine Latulipe. 2014. 通过创意支持指数量化数字工具的创意支持。*ACM
    Trans. Comput.-Hum. Interact.* 21, 4, 文章21（2014年6月），25页。 [https://doi.org/10.1145/2617588](https://doi.org/10.1145/2617588)
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing
    GPT-4 with 90%* ChatGPT Quality. [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing. 2023. Vicuna: 一款开源聊天机器人，以90%* ChatGPT质量打动GPT-4。 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)'
- en: 'Chung et al. (2022) John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran
    Lee, Eytan Adar, and Minsuk Chang. 2022. TaleBrush: Sketching Stories with Generative
    Pretrained Language Models. In *Proceedings of the 2022 CHI Conference on Human
    Factors in Computing Systems* (New Orleans, LA, USA) *(CHI ’22)*. Association
    for Computing Machinery, New York, NY, USA, Article 209, 19 pages. [https://doi.org/10.1145/3491102.3501819](https://doi.org/10.1145/3491102.3501819)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chung et al. (2022) John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran
    Lee, Eytan Adar, 和 Minsuk Chang. 2022. TaleBrush: 使用生成预训练语言模型绘制故事。在 *2022年CHI人机交互系统会议论文集*（新奥尔良，LA，美国）*(CHI
    ’22)*. 计算机协会，纽约，NY，美国，文章209，19页。 [https://doi.org/10.1145/3491102.3501819](https://doi.org/10.1145/3491102.3501819)'
- en: Eshraghian (2020) Jason K Eshraghian. 2020. Human ownership of artificial creativity.
    *Nature Machine Intelligence* 2, 3 (2020), 157–160.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eshraghian (2020) Jason K Eshraghian. 2020. 人类对人工创造力的拥有权. *Nature Machine Intelligence*
    2, 3 (2020), 157–160.
- en: 'Felkner et al. (2023) Virginia K Felkner, Ho-Chun Herbert Chang, Eugene Jang,
    and Jonathan May. 2023. WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+
    Bias in Large Language Models. *arXiv preprint arXiv:2306.15087* (2023).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Felkner et al. (2023) Virginia K Felkner, Ho-Chun Herbert Chang, Eugene Jang,
    和 Jonathan May. 2023. WinoQueer: 针对大规模语言模型的反 LGBTQ+ 偏见的社区循环基准. *arXiv 预印本 arXiv:2306.15087*
    (2023).'
- en: Fried et al. (2019) Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein,
    Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh
    Agrawala. 2019. Text-Based Editing of Talking-Head Video. *ACM Trans. Graph.*
    38, 4, Article 68 (jul 2019), 14 pages. [https://doi.org/10.1145/3306346.3323028](https://doi.org/10.1145/3306346.3323028)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fried et al. (2019) Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein,
    Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, 和 Maneesh
    Agrawala. 2019. 基于文本的谈话头视频编辑. *ACM Trans. Graph.* 38, 4, Article 68 (2019年7月),
    14 页. [https://doi.org/10.1145/3306346.3323028](https://doi.org/10.1145/3306346.3323028)
- en: 'Glikson and Woolley (2020) Ella Glikson and Anita Williams Woolley. 2020. Human
    trust in artificial intelligence: Review of empirical research. *Academy of Management
    Annals* 14, 2 (2020), 627–660.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Glikson and Woolley (2020) Ella Glikson 和 Anita Williams Woolley. 2020. 人类对人工智能的信任:
    实证研究综述. *Academy of Management Annals* 14, 2 (2020), 627–660.'
- en: 'Ho et al. (2022) Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi
    Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J.
    Fleet, and Tim Salimans. 2022. Imagen Video: High Definition Video Generation
    with Diffusion Models. arXiv:2210.02303 [cs.CV]'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ho et al. (2022) Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi
    Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David
    J. Fleet, 和 Tim Salimans. 2022. Imagen Video: 使用扩散模型生成高清晰度视频. arXiv:2210.02303
    [cs.CV]'
- en: 'Huang et al. (2020) Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex,
    Monica Dinculescu, and Carrie J Cai. 2020. AI song contest: Human-AI co-creation
    in songwriting. *arXiv preprint arXiv:2010.05388* (2020).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2020) Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex,
    Monica Dinculescu, 和 Carrie J Cai. 2020. AI 歌曲比赛: 人类与 AI 共同创作歌曲. *arXiv 预印本 arXiv:2010.05388*
    (2020).'
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for
    Embodied Agents. arXiv:2201.07207 [cs.LG]'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch.
    2022. 语言模型作为零样本规划器: 为具身体代理提取可操作的知识. arXiv:2201.07207 [cs.LG]'
- en: 'Huber et al. (2019) Bernd Huber, Hijung Valentina Shin, Bryan Russell, Oliver
    Wang, and Gautham J Mysore. 2019. B-script: Transcript-based b-roll video editing
    with recommendations. In *Proceedings of the 2019 CHI Conference on Human Factors
    in Computing Systems*. 1–11.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huber et al. (2019) Bernd Huber, Hijung Valentina Shin, Bryan Russell, Oliver
    Wang, 和 Gautham J Mysore. 2019. B-script: 基于转录本的 B-roll 视频编辑与推荐. 在 *2019 年 CHI
    人机交互系统会议论文集* 中. 1–11.'
- en: 'Huh et al. (2023) Mina Huh, Saelyne Yang, Yi-Hao Peng, Xiang ’Anthony’ Chen,
    Young-Ho Kim, and Amy Pavel. 2023. AVscript: Accessible Video Editing with Audio-Visual
    Scripts. In *Proceedings of the 2023 CHI Conference on Human Factors in Computing
    Systems* (Hamburg, Germany) *(CHI ’23)*. Association for Computing Machinery,
    New York, NY, USA, Article 796, 17 pages. [https://doi.org/10.1145/3544548.3581494](https://doi.org/10.1145/3544548.3581494)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huh et al. (2023) Mina Huh, Saelyne Yang, Yi-Hao Peng, Xiang ''Anthony'' Chen,
    Young-Ho Kim, 和 Amy Pavel. 2023. AVscript: 使用音视频脚本进行可访问的视频编辑. 在 *2023 年 CHI 人机交互系统会议论文集*（德国汉堡）*(CHI
    ’23)* 中. 计算机协会, 纽约, NY, USA, Article 796, 17 页. [https://doi.org/10.1145/3544548.3581494](https://doi.org/10.1145/3544548.3581494)'
- en: 'Kang and Lou (2022) Hyunjin Kang and Chen Lou. 2022. AI agency vs. human agency:
    understanding human–AI interactions on TikTok and their implications for user
    engagement. *Journal of Computer-Mediated Communication* 27, 5 (2022), zmac014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kang and Lou (2022) Hyunjin Kang 和 Chen Lou. 2022. AI 代理与人类代理: 理解 TikTok 上的人类–AI
    互动及其对用户参与的影响. *Journal of Computer-Mediated Communication* 27, 5 (2022), zmac014.'
- en: 'Karpas et al. (2022) Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz,
    Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown,
    Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon
    Shashua, and Moshe Tenenholtz. 2022. MRKL Systems: A modular, neuro-symbolic architecture
    that combines large language models, external knowledge sources and discrete reasoning.
    arXiv:2205.00445 [cs.CL]'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpas et al. (2022) Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz,
    Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown,
    Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon
    Shashua, 和 Moshe Tenenholtz. 2022. MRKL 系统：一种模块化的神经符号架构，结合了大型语言模型、外部知识源和离散推理。arXiv:2205.00445
    [cs.CL]
- en: Khadpe et al. (2020) Pranav Khadpe, Ranjay Krishna, Li Fei-Fei, Jeffrey T Hancock,
    and Michael S Bernstein. 2020. Conceptual metaphors impact perceptions of human-AI
    collaboration. *Proceedings of the ACM on Human-Computer Interaction* 4, CSCW2
    (2020), 1–26.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khadpe et al. (2020) Pranav Khadpe, Ranjay Krishna, Li Fei-Fei, Jeffrey T Hancock,
    和 Michael S Bernstein. 2020. 概念隐喻影响对人机协作的认知。*ACM 人机交互会议论文集* 4, CSCW2 (2020), 1–26.
- en: 'Kim et al. (2023) Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho
    Kim. 2023. EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined
    Criteria. *arXiv preprint arXiv:2309.13633* (2023).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, 和 Juho
    Kim. 2023. EvalLM：根据用户定义标准的交互式大型语言模型提示评估。*arXiv 预印本 arXiv:2309.13633* (2023).
- en: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners.
    arXiv:2205.11916 [cs.CL]
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2023) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, 和 Yusuke Iwasawa. 2023. 大型语言模型是零样本推理器。arXiv:2205.11916 [cs.CL]
- en: 'Laput et al. (2013) Gierad P. Laput, Mira Dontcheva, Gregg Wilensky, Walter
    Chang, Aseem Agarwala, Jason Linder, and Eytan Adar. 2013. PixelTone: A Multimodal
    Interface for Image Editing. In *Proceedings of the SIGCHI Conference on Human
    Factors in Computing Systems* (Paris, France) *(CHI ’13)*. Association for Computing
    Machinery, New York, NY, USA, 2185–2194. [https://doi.org/10.1145/2470654.2481301](https://doi.org/10.1145/2470654.2481301)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laput et al. (2013) Gierad P. Laput, Mira Dontcheva, Gregg Wilensky, Walter
    Chang, Aseem Agarwala, Jason Linder, 和 Eytan Adar. 2013. PixelTone：一种用于图像编辑的多模态界面。见于
    *SIGCHI 人机因素会议论文集*（法国巴黎）*(CHI ’13)*。计算机协会，美国纽约，2185–2194. [https://doi.org/10.1145/2470654.2481301](https://doi.org/10.1145/2470654.2481301)
- en: Li et al. (2021) Belinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021. Implicit
    Representations of Meaning in Neural Language Models. arXiv:2106.00737 [cs.CL]
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Belinda Z. Li, Maxwell Nye, 和 Jacob Andreas. 2021. 神经语言模型中的隐式意义表示。arXiv:2106.00737
    [cs.CL]
- en: 'Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b.
    BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and
    Large Language Models. arXiv:2301.12597 [cs.CV]'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Junnan Li, Dongxu Li, Silvio Savarese, 和 Steven Hoi. 2023b.
    BLIP-2：通过冻结图像编码器和大型语言模型引导语言-图像预训练。arXiv:2301.12597 [cs.CV]
- en: Li et al. (2023a) Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. 2023a.
    A Zero-Shot Language Agent for Computer Control with Structured Reflection. arXiv:2310.08740 [cs.CL]
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, 和 Yang Li. 2023a.
    一种用于计算机控制的零样本语言代理，具备结构化反射能力。arXiv:2310.08740 [cs.CL]
- en: Lin et al. (2023) Georgianna Lin, Jin Yi Li, Afsaneh Fazly, Vladimir Pavlovic,
    and Khai Truong. 2023. Identifying Multimodal Context Awareness Requirements for
    Supporting User Interaction with Procedural Videos. In *Proceedings of the 2023
    CHI Conference on Human Factors in Computing Systems*. 1–17.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) Georgianna Lin, Jin Yi Li, Afsaneh Fazly, Vladimir Pavlovic,
    和 Khai Truong. 2023. 确定支持用户与程序视频交互的多模态上下文感知需求。见于 *2023年计算机系统人机因素会议论文集*。1–17.
- en: Liu (2021) Bingjie Liu. 2021. In AI we trust? Effects of agency locus and transparency
    on uncertainty reduction in human–AI interaction. *Journal of Computer-Mediated
    Communication* 26, 6 (2021), 384–402.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu (2021) Bingjie Liu. 2021. 我们信任 AI 吗？代理归属和透明度对人机交互中不确定性减少的影响。*计算机媒介沟通杂志*
    26, 6 (2021), 384–402.
- en: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2023a. Visual Instruction Tuning. (2023).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 2023a.
    视觉指令调优。 (2023).
- en: 'Liu et al. (2023b) Vivian Liu, Tao Long, Nathan Raw, and Lydia Chilton. 2023b.
    Generative Disco: Text-to-Video Generation for Music Visualization. *arXiv preprint
    arXiv:2304.08551* (2023).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Vivian Liu, Tao Long, Nathan Raw, 和 Lydia Chilton. 2023b.
    生成性 Disco：音乐可视化的文本到视频生成。*arXiv 预印本 arXiv:2304.08551* (2023).
- en: 'Liu et al. (2022) Vivian Liu, Han Qiao, and Lydia Chilton. 2022. Opal: Multimodal
    image generation for news illustration. In *Proceedings of the 35th Annual ACM
    Symposium on User Interface Software and Technology*. 1–17.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2022) Vivian Liu, Han Qiao, 和 Lydia Chilton. 2022. Opal: 用于新闻插图的多模态图像生成。在
    *第35届ACM用户界面软件和技术年会论文集* 中，1–17。'
- en: 'Liu et al. (2023c) Vivian Liu, Jo Vermeulen, George Fitzmaurice, and Justin
    Matejka. 2023c. 3DALL-E: Integrating text-to-image AI in 3D design workflows.
    In *Proceedings of the 2023 ACM designing interactive systems conference*. 1955–1977.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023c) Vivian Liu, Jo Vermeulen, George Fitzmaurice, 和 Justin Matejka.
    2023c. 3DALL-E: 将文本到图像的人工智能整合到3D设计工作流程中。在 *2023年ACM设计互动系统会议论文集* 中，1955–1977。'
- en: 'Logan IV et al. (2021) Robert L Logan IV, Ivana Balažević, Eric Wallace, Fabio
    Petroni, Sameer Singh, and Sebastian Riedel. 2021. Cutting down on prompts and
    parameters: Simple few-shot learning with language models. *arXiv preprint arXiv:2106.13353*
    (2021).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logan IV et al. (2021) Robert L Logan IV, Ivana Balažević, Eric Wallace, Fabio
    Petroni, Sameer Singh, 和 Sebastian Riedel. 2021. 减少提示和参数：简单的少样本学习与语言模型。 *arXiv预印本
    arXiv:2106.13353*（2021）。
- en: Loughran (2022) Róisín Loughran. 2022. Bias and Creativity.. In *ICCC*. 354–358.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loughran (2022) Róisín Loughran. 2022. 偏见与创造力。在 *ICCC* 中，354–358。
- en: Louie et al. (2020) Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry,
    and Carrie J. Cai. 2020. Novice-AI Music Co-Creation via AI-Steering Tools for
    Deep Generative Models. In *Proceedings of the 2020 CHI Conference on Human Factors
    in Computing Systems* (Honolulu, HI, USA) *(CHI ’20)*. Association for Computing
    Machinery, New York, NY, USA, 1–13. [https://doi.org/10.1145/3313831.3376739](https://doi.org/10.1145/3313831.3376739)
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louie et al. (2020) Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry,
    和 Carrie J. Cai. 2020. 通过AI引导工具进行新手AI音乐共创。 在 *2020年CHI计算机系统人因会议论文集*（夏威夷州檀香山，美国）*(CHI
    ’20)* 中。计算机协会，美国纽约，1–13。 [https://doi.org/10.1145/3313831.3376739](https://doi.org/10.1145/3313831.3376739)
- en: 'Louie et al. (2022) Ryan Louie, Jesse Engel, and Cheng-Zhi Anna Huang. 2022.
    Expressive Communication: Evaluating Developments in Generative Models and Steering
    Interfaces for Music Creation. In *27th International Conference on Intelligent
    User Interfaces* (Helsinki, Finland) *(IUI ’22)*. Association for Computing Machinery,
    New York, NY, USA, 405–417. [https://doi.org/10.1145/3490099.3511159](https://doi.org/10.1145/3490099.3511159)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louie et al. (2022) Ryan Louie, Jesse Engel, 和 Cheng-Zhi Anna Huang. 2022. 表达性沟通：评估音乐创作中的生成模型和引导接口的发展。在
    *第27届国际智能用户界面会议*（芬兰赫尔辛基）*(IUI ’22)* 中。计算机协会，美国纽约，405–417。 [https://doi.org/10.1145/3490099.3511159](https://doi.org/10.1145/3490099.3511159)
- en: 'Magni et al. (2023) Federico Magni, Jiyoung Park, and Melody Manchi Chao. 2023.
    Humans as Creativity Gatekeepers: Are We Biased Against AI Creativity? *Journal
    of Business and Psychology* (2023), 1–14.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magni et al. (2023) Federico Magni, Jiyoung Park, 和 Melody Manchi Chao. 2023.
    人类作为创造力的守门人：我们对AI创造力是否存在偏见？ *商业与心理学杂志*（2023），1–14。
- en: McCormack et al. (2019) Jon McCormack, Toby Gifford, and Patrick Hutchings.
    2019. Autonomy, authenticity, authorship and intention in computer generated art.
    In *International conference on computational intelligence in music, sound, art
    and design (part of EvoStar)*. Springer, 35–50.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCormack et al. (2019) Jon McCormack, Toby Gifford, 和 Patrick Hutchings. 2019.
    计算机生成艺术中的自主性、真实性、作者性和意图。在 *国际计算智能音乐、声音、艺术和设计会议（EvoStar的一部分）* 中。Springer，35–50。
- en: 'Mirowski et al. (2023) Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and
    Richard Evans. 2023. Co-Writing Screenplays and Theatre Scripts with Language
    Models: Evaluation by Industry Professionals. In *Proceedings of the 2023 CHI
    Conference on Human Factors in Computing Systems* (Hamburg, Germany) *(CHI ’23)*.
    Association for Computing Machinery, New York, NY, USA, Article 355, 34 pages.
    [https://doi.org/10.1145/3544548.3581225](https://doi.org/10.1145/3544548.3581225)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirowski et al. (2023) Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, 和
    Richard Evans. 2023. 与语言模型共同编写剧本和戏剧：行业专业人士的评估。在 *2023年CHI计算机系统人因会议论文集*（德国汉堡）*(CHI
    ’23)* 中。计算机协会，美国纽约，文章355，34页。 [https://doi.org/10.1145/3544548.3581225](https://doi.org/10.1145/3544548.3581225)
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your Work: Scratchpads
    for Intermediate Computation with Language Models. arXiv:2112.00114 [cs.LG]'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, Charles Sutton, 和 Augustus Odena. 2021. 展示你的工作：中间计算的草稿本与语言模型。
    arXiv:2112.00114 [cs.LG]
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. *arXiv preprint arXiv:2304.03442* (2023).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park et al. (2023) Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith
    Ringel Morris, Percy Liang, 和 Michael S Bernstein. 2023. 生成代理: 人类行为的交互式模拟。*arXiv
    预印本 arXiv:2304.03442* (2023)。'
- en: Park et al. (2019) Sun Young Park, Pei-Yi Kuo, Andrea Barbarin, Elizabeth Kaziunas,
    Astrid Chow, Karandeep Singh, Lauren Wilcox, and Walter S Lasecki. 2019. Identifying
    challenges and opportunities in human-AI collaboration in healthcare. In *Conference
    Companion Publication of the 2019 on Computer Supported Cooperative Work and Social
    Computing*. 506–510.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2019) Sun Young Park, Pei-Yi Kuo, Andrea Barbarin, Elizabeth Kaziunas,
    Astrid Chow, Karandeep Singh, Lauren Wilcox, 和 Walter S Lasecki. 2019. 识别医疗保健中人类-AI
    合作的挑战与机遇。收录于 *2019年计算机支持的协作工作和社会计算会议的会议附录*。506–510。
- en: 'Pavel et al. (2020) Amy Pavel, Gabriel Reyes, and Jeffrey P. Bigham. 2020.
    Rescribe: Authoring and Automatically Editing Audio Descriptions. In *Proceedings
    of the 33rd Annual ACM Symposium on User Interface Software and Technology* (Virtual
    Event, USA) *(UIST ’20)*. Association for Computing Machinery, New York, NY, USA,
    747–759. [https://doi.org/10.1145/3379337.3415864](https://doi.org/10.1145/3379337.3415864)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pavel et al. (2020) Amy Pavel, Gabriel Reyes, 和 Jeffrey P. Bigham. 2020. Rescribe:
    创作和自动编辑音频描述。收录于 *第33届ACM用户界面软件和技术年会论文集*（虚拟会议，美国）*(UIST ’20)*。计算机协会，纽约，NY，美国，747–759。
    [https://doi.org/10.1145/3379337.3415864](https://doi.org/10.1145/3379337.3415864)'
- en: Rezwana and Maher (2022) Jeba Rezwana and Mary Lou Maher. 2022. Identifying
    ethical issues in ai partners in human-ai co-creation. *arXiv preprint arXiv:2204.07644*
    (2022).
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezwana and Maher (2022) Jeba Rezwana 和 Mary Lou Maher. 2022. 在人类-AI 共同创作中识别伦理问题。*arXiv
    预印本 arXiv:2204.07644* (2022)。
- en: Roberts et al. (2020) Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How
    Much Knowledge Can You Pack Into the Parameters of a Language Model?. In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
    Association for Computational Linguistics, Online, 5418–5426. [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts et al. (2020) Adam Roberts, Colin Raffel, 和 Noam Shazeer. 2020. 你能在语言模型的参数中容纳多少知识？收录于
    *2020年自然语言处理实证方法会议论文集 (EMNLP)*。计算语言学协会，在线，5418–5426。 [https://doi.org/10.18653/v1/2020.emnlp-main.437](https://doi.org/10.18653/v1/2020.emnlp-main.437)
- en: 'Shaw et al. (2023) Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
    Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova.
    2023. From Pixels to UI Actions: Learning to Follow Instructions via Graphical
    User Interfaces. *arXiv preprint arXiv:2306.00245* (2023).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaw et al. (2023) Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
    Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, 和 Kristina Toutanova.
    2023. 从像素到用户界面操作: 通过图形用户界面学习遵循指令。*arXiv 预印本 arXiv:2306.00245* (2023)。'
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its
    Friends in Hugging Face. arXiv:2303.17580 [cs.CL]'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, 和 Yueting Zhuang. 2023. HuggingGPT: 使用 ChatGPT 和 Hugging Face 的朋友解决 AI 任务。arXiv:2303.17580
    [cs.CL]'
- en: 'Shibata et al. (1999) Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki
    Takeda, Ayumi Shinohara, Takeshi Shinohara, and Setsuo Arikawa. 1999. Byte Pair
    encoding: A text compression scheme that accelerates pattern matching. (1999).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shibata et al. (1999) Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki
    Takeda, Ayumi Shinohara, Takeshi Shinohara, 和 Setsuo Arikawa. 1999. Byte Pair
    编码: 一种加速模式匹配的文本压缩方案。(1999)。'
- en: 'Shinn et al. (2023) Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion:
    an autonomous agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*
    (2023).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn et al. (2023) Noah Shinn, Beck Labash, 和 Ashwin Gopinath. 2023. Reflexion:
    一个具有动态记忆和自我反思的自主体。*arXiv 预印本 arXiv:2303.11366* (2023)。'
- en: 'Singer et al. (2022) Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
    Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal
    Gupta, and Yaniv Taigman. 2022. Make-A-Video: Text-to-Video Generation without
    Text-Video Data. arXiv:2209.14792 [cs.CV]'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singer et al. (2022) Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
    Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal
    Gupta, 和 Yaniv Taigman. 2022. Make-A-Video: 无需文本-视频数据的文本到视频生成。arXiv:2209.14792
    [cs.CV]'
- en: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 2998–3009.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2023）Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun
    Chao 和 Yu Su。2023。Llm-planner：利用大型语言模型进行少量实例的实体代理规划。见于 *IEEE/CVF 国际计算机视觉会议论文集*。2998–3009。
- en: 'Suh et al. (2022) Sangho Suh, Jian Zhao, and Edith Law. 2022. CodeToon: Story
    Ideation, Auto Comic Generation, and Structure Mapping for Code-Driven Storytelling.
    In *Proceedings of the 35th Annual ACM Symposium on User Interface Software and
    Technology* (Bend, OR, USA) *(UIST ’22)*. Association for Computing Machinery,
    New York, NY, USA, Article 13, 16 pages. [https://doi.org/10.1145/3526113.3545617](https://doi.org/10.1145/3526113.3545617)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suh 等（2022）Sangho Suh, Jian Zhao 和 Edith Law。2022。CodeToon：面向代码驱动叙事的故事构思、自动漫画生成和结构映射。见于
    *第 35 届 ACM 用户界面软件与技术年会论文集*（美国俄勒冈州本德）*(UIST ’22)*。计算机协会，纽约，NY，美国，文章 13，16 页。 [https://doi.org/10.1145/3526113.3545617](https://doi.org/10.1145/3526113.3545617)
- en: Tam et al. (2022) Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit
    Bansal, and Colin Raffel. 2022. Evaluating the Factual Consistency of Large Language
    Models Through Summarization. arXiv:2211.08412 [cs.CL]
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tam 等（2022）Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal
    和 Colin Raffel。2022。通过总结评估大型语言模型的事实一致性。arXiv:2211.08412 [cs.CL]
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.
    arXiv:2302.13971 [cs.CL]'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample。2023。LLaMA：开放且高效的基础语言模型。arXiv:2302.13971
    [cs.CL]
- en: 'Truong et al. (2016) Anh Truong, Floraine Berthouzoz, Wilmot Li, and Maneesh
    Agrawala. 2016. QuickCut: An Interactive Tool for Editing Narrated Video. In *Proceedings
    of the 29th Annual Symposium on User Interface Software and Technology* (Tokyo,
    Japan) *(UIST ’16)*. Association for Computing Machinery, New York, NY, USA, 497–507.
    [https://doi.org/10.1145/2984511.2984569](https://doi.org/10.1145/2984511.2984569)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Truong 等（2016）Anh Truong, Floraine Berthouzoz, Wilmot Li 和 Maneesh Agrawala。2016。QuickCut：一种用于编辑讲解视频的交互式工具。见于
    *第 29 届用户界面软件与技术年会论文集*（日本东京）*(UIST ’16)*。计算机协会，纽约，NY，美国，497–507。 [https://doi.org/10.1145/2984511.2984569](https://doi.org/10.1145/2984511.2984569)
- en: 'Van Someren et al. (1994) Maarten Van Someren, Yvonne F Barnard, and J Sandberg.
    1994. The think aloud method: a practical approach to modelling cognitive. *London:
    AcademicPress* 11 (1994), 29–41.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Someren 等（1994）Maarten Van Someren, Yvonne F Barnard 和 J Sandberg。1994。思维大声法：建模认知的实用方法。*伦敦：AcademicPress*
    11（1994），29–41。
- en: Venkit et al. (2023) Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar,
    Shomir Wilson, et al. 2023. Nationality Bias in Text Generation. *arXiv preprint
    arXiv:2302.02463* (2023).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Venkit 等（2023）Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar,
    Shomir Wilson 等。2023。文本生成中的国籍偏见。*arXiv 预印本 arXiv:2302.02463*（2023）。
- en: 'Wang et al. (2022) Bryan Wang, Zeyu Jin, and Gautham Mysore. 2022. Record Once,
    Post Everywhere: Automatic Shortening of Audio Stories for Social Media. In *Proceedings
    of the 35th Annual ACM Symposium on User Interface Software and Technology* (Bend,
    OR, USA) *(UIST ’22)*. Association for Computing Machinery, New York, NY, USA,
    Article 14, 11 pages. [https://doi.org/10.1145/3526113.3545680](https://doi.org/10.1145/3526113.3545680)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022）Bryan Wang, Zeyu Jin 和 Gautham Mysore。2022。一次录音，到处发布：社交媒体上自动缩短音频故事。见于
    *第 35 届 ACM 用户界面软件与技术年会论文集*（美国俄勒冈州本德）*(UIST ’22)*。计算机协会，纽约，NY，美国，文章 14，11 页。 [https://doi.org/10.1145/3526113.3545680](https://doi.org/10.1145/3526113.3545680)
- en: Wang et al. (2023a) Bryan Wang, Gang Li, and Yang Li. 2023a. Enabling Conversational
    Interaction with Mobile UI Using Large Language Models. In *Proceedings of the
    2023 CHI Conference on Human Factors in Computing Systems* (Hamburg, Germany)
    *(CHI ’23)*. Association for Computing Machinery, New York, NY, USA, Article 432,
    17 pages. [https://doi.org/10.1145/3544548.3580895](https://doi.org/10.1145/3544548.3580895)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023a）Bryan Wang, Gang Li 和 Yang Li。2023a。利用大型语言模型实现移动 UI 的对话互动。见于 *2023
    年 CHI 计算机系统人因会议论文集*（德国汉堡）*(CHI ’23)*。计算机协会，纽约，NY，美国，文章 432，17 页。 [https://doi.org/10.1145/3544548.3580895](https://doi.org/10.1145/3544548.3580895)
- en: 'Wang et al. (2019a) Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit
    Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander
    Gray. 2019a. Human-AI Collaboration in Data Science: Exploring Data Scientists’
    Perceptions of Automated AI. *Proc. ACM Hum.-Comput. Interact.* 3, CSCW, Article
    211 (nov 2019), 24 pages. [https://doi.org/10.1145/3359313](https://doi.org/10.1145/3359313)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019a) 王大夸、贾斯廷·D·韦斯、迈克尔·穆勒、帕里克什·拉姆、维尔纳·盖耶、凯西·杜根、伊拉·陶斯齐克、霍斯特·萨穆洛维茨和亚历山大·格雷。2019a。《数据科学中的人类-AI
    协作：探索数据科学家对自动化 AI 的看法》。*Proc. ACM Hum.-Comput. Interact.* 3, CSCW, 文章 211（2019年11月），24页。
    [https://doi.org/10.1145/3359313](https://doi.org/10.1145/3359313)
- en: 'Wang et al. (2023c) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023c. Plan-and-Solve Prompting: Improving Zero-Shot
    Chain-of-Thought Reasoning by Large Language Models. arXiv:2305.04091 [cs.CL]'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023c) 王磊、徐婉瑜、蓝怡慧、胡志强、兰云石、李凯伟和林易鹏。2023c。《计划和解决提示：通过大型语言模型改进零-shot
    思维链推理》。arXiv:2305.04091 [cs.CL]
- en: 'Wang et al. (2019b) Miao Wang, Guo-Wei Yang, Shi-Min Hu, Shing-Tung Yau, Ariel
    Shamir, et al. 2019b. Write-a-video: computational video montage from themed text.
    *ACM Trans. Graph.* 38, 6 (2019), 177–1.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019b) 王淼、杨国伟、胡世敏、丘成桐、阿里尔·沙米尔等。2019b。《写视频：从主题文本生成计算机视频剪辑》。*ACM
    Trans. Graph.* 38, 6（2019年），177–1。
- en: 'Wang et al. (2023b) Sitong Wang, Samia Menon, Tao Long, Keren Henderson, Dingzeyu
    Li, Kevin Crowston, Mark Hansen, Jeffrey V Nickerson, and Lydia B Chilton. 2023b.
    ReelFramer: Co-creating News Reels on Social Media with Generative AI. *arXiv
    preprint arXiv:2304.09653* (2023).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) 王思同、萨米亚·梅农、龙涛、亨德森·克伦、李丁泽、凯文·克劳斯顿、马克·汉森、杰弗里·V·尼克森和莉迪亚·B·奇尔顿。2023b。《ReelFramer：与生成式
    AI 在社交媒体上共同创作新闻片段》。*arXiv 预印本 arXiv:2304.09653*（2023年）。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) 韦杰森、王雪志、戴尔·舒尔曼斯、马尔滕·博斯马、布赖恩·伊赫特、费晓、埃德·奇、阮奇和丹尼·周。2023年。《链式思维提示引发大型语言模型中的推理》。arXiv:2201.11903
    [cs.CL]
- en: 'Xia (2020) Haijun Xia. 2020. Crosspower: Bridging Graphics and Linguistics.
    In *Proceedings of the 33rd Annual ACM Symposium on User Interface Software and
    Technology* (Virtual Event, USA) *(UIST ’20)*. Association for Computing Machinery,
    New York, NY, USA, 722–734. [https://doi.org/10.1145/3379337.3415845](https://doi.org/10.1145/3379337.3415845)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia (2020) 夏海军。2020年。《Crosspower：弥合图形与语言学》。在*第33届年度 ACM 用户界面软件与技术研讨会*（虚拟活动，美国）*(UIST
    ’20)*中。计算机协会，美国纽约，722–734。 [https://doi.org/10.1145/3379337.3415845](https://doi.org/10.1145/3379337.3415845)
- en: 'Xia et al. (2020) Haijun Xia, Jennifer Jacobs, and Maneesh Agrawala. 2020.
    Crosscast: adding visuals to audio travel podcasts. In *Proceedings of the 33rd
    annual ACM symposium on user interface software and technology*. 735–746.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia et al. (2020) 夏海军、詹妮弗·雅各布斯和曼尼什·阿格拉瓦拉。2020年。《Crosscast：将视觉添加到音频旅行播客中》。在*第33届年度
    ACM 用户界面软件与技术研讨会论文集*中。735–746。
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching
    Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations
    with gpt-4v (ision). *arXiv preprint arXiv:2309.17421* 9, 1 (2023).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) 杨正远、李林杰、林凯文、王建锋、林中清、刘子成和王丽娟。2023年。《lmms 的黎明：对 gpt-4v (ision)
    的初步探索》。*arXiv 预印本 arXiv:2309.17421* 9, 1（2023年）。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting
    in Language Models. arXiv:2210.03629 [cs.CL]'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023) 姚顺宇、赵杰弗里、于典、杜楠、伊扎克·沙夫兰、卡尔提克·纳拉辛汉和曹远。2023年。《ReAct：在语言模型中协同推理与行动》。arXiv:2210.03629
    [cs.CL]
- en: 'Yu et al. (2023) Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner,
    Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2023. Large Language Model as Attributed
    Training Data Generator: A Tale of Diversity and Bias. *arXiv preprint arXiv:2306.15895*
    (2023).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2023) 岳宇、庄宇晨、张杰宇、孟宇、亚历山大·拉特纳、兰杰·克里希纳、沈家明和张超。2023年。《作为属性训练数据生成器的大型语言模型：多样性与偏见的故事》。*arXiv
    预印本 arXiv:2306.15895*（2023年）。
- en: 'Yuan et al. (2022) Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito.
    2022. Wordcraft: Story Writing With Large Language Models. In *27th International
    Conference on Intelligent User Interfaces* (Helsinki, Finland) *(IUI ’22)*. Association
    for Computing Machinery, New York, NY, USA, 841–852. [https://doi.org/10.1145/3490099.3511105](https://doi.org/10.1145/3490099.3511105)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等 (2022) Ann Yuan, Andy Coenen, Emily Reif, 和 Daphne Ippolito. 2022. Wordcraft：使用大型语言模型进行故事创作。发表于
    *第 27 届国际智能用户界面会议* (赫尔辛基，芬兰) *(IUI ’22)*. 计算机协会，纽约，NY，USA, 841–852. [https://doi.org/10.1145/3490099.3511105](https://doi.org/10.1145/3490099.3511105)
- en: 'Zamfirescu-Pereira et al. (2023) JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern
    Hartmann, and Qian Yang. 2023. Why Johnny can’t prompt: how non-AI experts try
    (and fail) to design LLM prompts. In *Proceedings of the 2023 CHI Conference on
    Human Factors in Computing Systems*. 1–21.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zamfirescu-Pereira 等 (2023) JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann,
    和 Qian Yang. 2023. 为什么 Johnny 无法提示：非人工智能专家如何尝试 (并失败) 设计 LLM 提示。发表于 *2023 CHI 计算机系统人因会议论文集*.
    1–21.
- en: 'Zhang et al. (2022) Zheng Zhang, Ying Xu, Yanhao Wang, Bingsheng Yao, Daniel
    Ritchie, Tongshuang Wu, Mo Yu, Dakuo Wang, and Toby Jia-Jun Li. 2022. Storybuddy:
    A human-ai collaborative chatbot for parent-child interactive storytelling with
    flexible parental involvement. In *Proceedings of the 2022 CHI Conference on Human
    Factors in Computing Systems*. 1–21.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2022) Zheng Zhang, Ying Xu, Yanhao Wang, Bingsheng Yao, Daniel Ritchie,
    Tongshuang Wu, Mo Yu, Dakuo Wang, 和 Toby Jia-Jun Li. 2022. Storybuddy：一种用于亲子互动讲故事的人工智能协作聊天机器人，具有灵活的家长参与。发表于
    *2022 CHI 计算机系统人因会议论文集*. 1–21.
- en: 'Zhu et al. (2018) Jichen Zhu, Antonios Liapis, Sebastian Risi, Rafael Bidarra,
    and G. Michael Youngblood. 2018. Explainable AI for Designers: A Human-Centered
    Perspective on Mixed-Initiative Co-Creation. In *2018 IEEE Conference on Computational
    Intelligence and Games (CIG)*. 1–8. [https://doi.org/10.1109/CIG.2018.8490433](https://doi.org/10.1109/CIG.2018.8490433)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2018) Jichen Zhu, Antonios Liapis, Sebastian Risi, Rafael Bidarra, 和
    G. Michael Youngblood. 2018. 面向设计师的可解释 AI：一种以人为中心的混合倡议协作视角。发表于 *2018 IEEE 计算智能与游戏会议
    (CIG)*. 1–8. [https://doi.org/10.1109/CIG.2018.8490433](https://doi.org/10.1109/CIG.2018.8490433)
- en: '![Refer to caption](img/4b034cda423568c8114fc2d42a806294.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4b034cda423568c8114fc2d42a806294.png)'
- en: 'Figure 10\. Graph illustrations of the mechanisms of each LLM-based editing
    function supported by LAVE: (1) In Footage Overview, gallery videos (Gs) are categorized
    into several common themes or topics (Cs). (2) In Idea Brainstorming, gallery
    videos (Gs) are used to develop video creation ideas (Is), with the option of
    creative guidance (CG) being provided by the user. (3) In Video Retrieval, gallery
    videos (Gs) are ranked based on their relevance to the language query (LQ) extracted
    from the user’s command. Deeper colors in the ranked videos represent higher relevance.
    (4) In Storyboarding, timeline videos (Ts) are reordered to match narrative guidance
    (NG) or a storyline optionally provided by the users. If not provided, the model
    will be asked to generate one itself. (5) In Clip Trimming, captions of each frame
    in a clip (Fs) will be provided to the model along with the user’s trimming command
    (TC). The function will output the trimmed clip’s start and end frame IDs as well
    as its rationale for the predictions.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 显示 LAVE 支持的每种 LLM 基于编辑功能机制的图示： (1) 在影像概览中，画廊视频 (Gs) 被分类为几个共同的主题或话题 (Cs)。
    (2) 在创意头脑风暴中，画廊视频 (Gs) 用于开发视频创意 (Is)，用户可以选择提供创意指导 (CG)。 (3) 在视频检索中，画廊视频 (Gs) 根据与用户命令中提取的语言查询
    (LQ) 的相关性进行排序。排名靠前的视频颜色更深，表示相关性更高。 (4) 在分镜头剧本中，时间轴视频 (Ts) 被重新排序以匹配叙事指导 (NG) 或用户可选提供的故事情节。如果未提供，模型将被要求自行生成。
    (5) 在剪辑修剪中，每个剪辑帧 (Fs) 的字幕将与用户的修剪命令 (TC) 一起提供给模型。该功能将输出修剪剪辑的起始和结束帧 ID 以及其预测的理由。
- en: Appendix A Prompt Preambles
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 提示前言
- en: This section contains prompt preambles that instruct the LLM to perform specific
    editing functions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含指示 LLM 执行特定编辑功能的提示前言。
- en: A.1\. Footage Overview
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1\. 影像概览
- en: Summarize the common topics or themes within all the provided videos, or categorize
    the videos by topic. The overview should be short, informative, and comprehensive,
    covering all the videos. For each topic or theme, list the titles of the videos
    that belong to it below.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 总结所有提供的视频中的共同主题或主题，或按主题对视频进行分类。概述应简短、信息丰富且全面，涵盖所有视频。对于每个主题或主题，列出属于它的视频标题。
- en: A.2\. Idea Brainstorming
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. 创意头脑风暴
- en: Use all of the provided videos to brainstorm ideas for video editing. For each
    idea, specify which video should be used and why. Aim for broad integration of
    multiple clips; the more comprehensive the integration, the better. Users may
    provide creative guidance for brainstorming. If the guidance is general, feel
    free to brainstorm using the videos mentioned below; otherwise, adhere to that
    guidance.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有提供的视频来头脑风暴视频编辑的创意。对于每个创意，指定应使用哪个视频以及原因。目标是广泛整合多个片段；整合的范围越广泛，效果越好。用户可以提供创意指导。如果指导是一般性的，可以使用下面提到的视频进行头脑风暴；否则，请遵循该指导。
- en: A.3\. Storyboarding
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3\. 故事板制作
- en: 'Use all the provided videos to devise a storyboard for video editing. If the
    user provides creative guidance, follow it closely. Reference videos in the storyboard
    by their title and ID. The output should be a dictionary with keys: "storyboard"
    and "video_ids". The "storyboard" key maps to a string detailing each scene in
    the storyboard, in the format of "Scene X:  (ID=X), ". The "video_ids" key maps to a sequence of video IDs as referenced
    in the storyboard. Ensure all input videos are included in the output.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '使用所有提供的视频制定视频编辑的故事板。如果用户提供了创意指导，请严格遵循。故事板中的视频应通过标题和ID进行引用。输出应为一个字典，包含键："storyboard"
    和 "video_ids"。其中 "storyboard" 键映射到一个字符串，详细说明故事板中的每个场景，格式为 "场景 X:  (ID=X)，"。
    "video_ids" 键映射到故事板中引用的视频ID序列。确保所有输入视频都包含在输出中。'
- en: A.4\. Clip Trimming
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4\. 剪辑裁剪
- en: 'Given video frame captions with timestamps, where each description represents
    1 second of video, and the user’s trimming command, determine the new start and
    end timestamps for the trimmed clip. If a specific clip length constraint is mentioned,
    adhere to it. The expected output is a Python dictionary formatted as: Final Answer:
    {"segment": ["start", "end", "rationale"]}. Both "start" and "end" should be integers.
    If no segment matches the user’s command, "segment" should contain an empty list.
    Prioritize longer segments when multiple qualify.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '给定带有时间戳的视频帧说明，其中每个描述代表1秒的视频，以及用户的裁剪命令，确定裁剪片段的新开始和结束时间戳。如果提到了特定的片段长度限制，请遵守。期望的输出格式为一个Python字典，如下所示：最终答案：{"segment":
    ["start", "end", "rationale"]}。其中“start”和“end”应为整数。如果没有段落符合用户的命令，“segment”应包含一个空列表。当多个段落符合要求时，优先考虑较长的段落。'
