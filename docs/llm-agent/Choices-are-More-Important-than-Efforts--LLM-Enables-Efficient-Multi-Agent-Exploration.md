<!--yml
category: 未分类
date: 2025-01-11 12:11:16
-->

# Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration

> 来源：[https://arxiv.org/html/2410.02511/](https://arxiv.org/html/2410.02511/)

Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao,
Cheems Wang, Chang Liu, Xiangyang Ji Department of Automation, Tsinghua University, Beijing, China
{qy22, wangby22, jiangyh19, sjz18, myx21}@mails.tsinghua.edu.cn
cheemswang@mail.tsinghua.edu.cn, {liuchang2022, xyji}@tsinghua.edu.cn 

###### Abstract

With expansive state-action spaces, efficient multi-agent exploration remains a longstanding challenge in reinforcement learning. Although pursuing novelty, diversity, or uncertainty attracts increasing attention, redundant efforts brought by exploration without proper guidance choices poses a practical issue for the community. This paper introduces a systematic approach, termed LEMAE, choosing to channel informative task-relevant guidance from a knowledgeable Large Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically, we ground linguistic knowledge from LLM into symbolic key states, that are critical for task fulfillment, in a discriminative manner at low LLM inference costs. To unleash the power of key states, we design Subspace-based Hindsight Intrinsic Reward (SHIR) to guide agents toward key states by increasing reward density. Additionally, we build the Key State Memory Tree (KSMT) to track transitions between key states in a specific task for organized exploration. Benefiting from diminishing redundant explorations, LEMAE outperforms existing SOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large margin, achieving a 10x acceleration in certain scenarios.

## 1 Introduction

Exploration stands as a fundamental issue in reinforcement learning (RL) (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35); Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)). Researchers have developed several exploration strategies directed by novelty, diversity, or uncertainty (Linke et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib88); Burda et al., [2018b](https://arxiv.org/html/2410.02511v1#bib.bib20); Pathak et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib121)), mainly in single-agent reinforcement learning. However, these methods may induce task-irrelevant redundant exploration, especially in complex environments (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)). In the realm of Multi-Agent Reinforcement Learning (MARL), the need to mitigate exploration redundancy becomes even more urgent due to the challenges like exponential expansion of the state-action spaces. Widespread real-world applications, including MOBA games (Qu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib126)), social science (Jaques et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib55)), and multi-vehicle control (Xu et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib181)), further underscore the growing need for efficient multi-agent exploration.

This work identifies task-relevant guidance as an important consideration in enhancing exploration efficiency. Incorporating priors in exploration mechanism design, such as complex reward structures, typically requires expert knowledge and substantial human efforts (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Abbeel & Ng, [2004](https://arxiv.org/html/2410.02511v1#bib.bib1)). Hopefully, recent advances have witnessed the remarkable reasoning and planning capabilities of Large Language Models (Touvron et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib159); Achiam et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib2)), providing a plausible choice to facilitate efficient exploration through LLM’s effortless prior provision. However, it is non-trivial to effectively comprise linguistic LLM priors into symbolically represented RL tasks (Peng et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib123); Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23)), and the investigation of practical ways to avoid nuisances caused by such an expression discrepancy is of critical importance.

![Refer to caption](img/813150d86be815a26b07493fd28d088c.png)

Figure 1: (a) The map of the task Pass. Two agents are initially positioned in the left room, requiring cooperation to explore the rooms, uncover the hidden switches, and move to the right room. (b) The key states ($\kappa_{1}$ and $\kappa_{2}$) generated by LLM for the task Pass, where the superscripts $A,B$ of $\kappa_{i}$ denote two agents Alice and Bob. (c) Visitation Map (log scale) of SOTA baseline method CMAE. (d) Visitation Map (log scale) of our method LEMAE. Our method exhibits a significant reduction in redundant exploration. Furthermore, an organic division of labor among agents emerges.

In response to the above issue, we propose LEMAE, a novel framework to enable efficient multi-agent exploration with LLM. The framework primarily consists of two components: (i) key states localization with LLM and (ii) key state-guided exploration. The first component grounds linguistic knowledge from LLM into symbolic key states by automatically localizing key states that are essential for task fulfillment. Specifically, the discriminator function induced by LLM works to discriminate key states from rollout trajectories, avoiding the overburden of LLM inference costs. The second component harnesses the localized key states as meaningful guidance to achieve efficient exploration. In implementation, we encourage agents toward targeted key states by devising Subspace-based Hindsight Intrinsic Reward (SHIR) to increase reward density. For the purpose of organized exploration, Key States Memory Tree (KSMT) is further constructed to track key state transitions, mitigating exploration complexity and enhancing guidance in SHIR. As illustrated in Fig. [1](https://arxiv.org/html/2410.02511v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), our design empowers LEMAE with a significant performance advantage through notably reducing redundant exploration.

Our main contributions are summarized as follows:

1.  1.

    We build a bridge between LLM and RL to facilitate efficient multi-agent exploration by developing a systematic approach dubbed LEMAE.

2.  2.

    We devise a computationally efficient inference strategy channeling task-specific information from LLM to distinguish key states critical for task fulfillment as subgoals for targeted exploration.

3.  3.

    We introduce a Key State Memory Tree to organize exploration according to historic key state transitions and devise the Subspace-based Hindsight Intrinsic Reward, encouraging agents’ guidance.

We conduct extensive experiments on typical multi-agent exploration benchmarks. LEMAE (i) consistently outperforms the state-of-the-art (SOTA) baselines with 10x acceleration in certain scenarios, (ii) achieves performance comparable to the baseline trained with human-designed dense rewards in sparse reward scenarios, and (iii) exhibits potential to generalize to brand-new, non-symbolic tasks. These observations validate the effectiveness of our design in reducing redundant exploration and improving exploration efficiency, showing promise for real-world deployment in scenarios requiring efficient exploration.

## 2 Preliminary

The environments considered in this work are characterized as a decentralized partially observable Markov decision process (Dec-POMDP) (Oliehoek et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib115)) with $n$ agents, which can be defined as a tuple $G=\langle S,A,I,P,r,Z,O,n,\gamma\rangle$, where $s\in S$ is the global state, $A$ is the action space for each agent, and $\gamma\in[0,1)$ is the discount factor. At time step $t$, each agent $i\in I\equiv\{1,...,n\}$ has its local observations $o^{i}\in O$ drawn from the observation function $Z(s,i):S\times I\rightarrow O$ and chooses an action $a^{i}\in A$ by its policy $\pi^{i}(a^{i}|o^{i}):O\rightarrow\Delta([0,1]^{|A|})$, forming a joint action $\mathbf{a}\in\mathbf{A}\equiv A^{n}$. $T(s^{\prime}|s,\mathbf{a}):S\times\mathbf{A}\times S\rightarrow[0,1]$ is the environment’s state transition distribution. All agents share a common reward function $r(s,\mathbf{a}):S\times\mathbf{A}\rightarrow\mathbb{R}$. The agents’ joint policy $\bm{\pi}:=\prod_{i=1}^{n}\pi^{i}$ induces a joint *action-value function*: $Q^{\bm{\pi}}(s,\mathbf{a})=\mathbb{E}[R|s,\mathbf{a}]$, where $R=\sum^{\infty}_{t=0}\gamma^{t}r_{t}$ is the expected discounted return. The goal of MARL is to find the optimal joint policy $\bm{\pi}^{*}$ such that $Q^{\bm{\pi}^{*}}(s,\mathbf{a})\geq Q^{\bm{\pi}}(s,\mathbf{a})$, $\forall\bm{\pi}\ \text{and}\ (s,\mathbf{a})\in S\times\mathbf{A}$. Notably, we specifically focus on sparse reward tasks, i.e., $r_{t}=1$ only when $s_{t+1}=s_{success}$, otherwise $r_{t}=0$. We denote the symbol for the $i$-th key state by $\kappa_{i}$ together with its discriminator function $\mathcal{F}_{i}$.

## 3 Related Works

##### LLM in Decision Making.

Large Language Models have showcased impressive capabilities across various downstream tasks (Touvron et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib159); Radford et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib127); Brown et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib16)). Recent advances indicate a growing trend of using LLM in decision-making problems (Wang et al., [2023b](https://arxiv.org/html/2410.02511v1#bib.bib167)). A primary challenge within this domain is grounding LLM’s linguistic knowledge into specific low-level control tasks typically represented in symbolic form (Peng et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib123); Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23)), especially in RL. Creating linguistic twin tasks (Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23)) are intuitive but require substantial manual workloads. Some works employ LLMs as high-level planners, e.g., coding with APIs (Liang et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib85)), using human-annotated or LLM-summarized action template (Yao et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib186); Shinn et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib141); Lin et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib86); Zhu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib200); Wang et al., [2023a](https://arxiv.org/html/2410.02511v1#bib.bib165)). Despite significant progress, they rely on difficult-to-obtain low-level policies or APIs, limiting their real-world applicability. Recently, LLMs have been integrated with RL to directly enhance low-level decision making (Cao et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib22)). LLMs can act as environmental information processors, reducing learning complexity (Paischer et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib117); [2024](https://arxiv.org/html/2410.02511v1#bib.bib118); Kim et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib64); Wang et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib164)), but cannot directly facilitate efficient exploration. Some works utilize LLMs as goal selectors in goal-conditioned RL (Su & Zhang, [2023](https://arxiv.org/html/2410.02511v1#bib.bib148); Shukla2023lgts) or teacher policy (Zhou et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib199)) but require predefined skills or subgoals. Alternative methods like LLM-based reward or policy design (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98); Kwon et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib75); Song et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib146); Liu et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib93); Chen et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib24)) and fine-tuning (Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23); Shi et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib140)) are either limited to simple tasks with sufficient information or demand enormous data and resources. ELLM (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)) aims to enhance exploration using LLM but depends on predefined symbolic observation captioner and frequent LLM inferences. Its semantic similarity-based rewards may also struggle to generalize across diverse scenarios. In contrast, LEMAE integrates linguistic LLM priors into symbolic states with minimal task-specific information and LLM inference costs, achieved by localizing key states in rollout trajectories using LLM-generated discriminator functions.

##### Efficient Multi-Agent Exploration.

Exploration efficiency has long been a focal point in RL (Thrun, [1992](https://arxiv.org/html/2410.02511v1#bib.bib156); Cai et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib21); Seo et al., [2021](https://arxiv.org/html/2410.02511v1#bib.bib137); Mahajan et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib100); Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56); Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). Typical exploration methods focus on random exploration (Mnih et al., [2013](https://arxiv.org/html/2410.02511v1#bib.bib109); Rashid et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib128)) or heuristic indicators, such as diversity or novelty, to facilitate exhaustive exploration, particularly in single agent exploration (Linke et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib88); Burda et al., [2018b](https://arxiv.org/html/2410.02511v1#bib.bib20); Pathak et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib121); Burda et al., [2018a](https://arxiv.org/html/2410.02511v1#bib.bib19); Bellemare et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib13)). Despite their success, they may induce notable redundant exploration due to a lack of task-relevant guidance (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)). The exponential expansion of the state-action spaces renders exhaustive exploration impractical in multi-agent settings. Consequently, efficient multi-agent exploration (MAE) becomes increasingly imperative and necessary (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56); Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). MAE is also challenging due to the complex configurations, e.g., the entangled effect of multi-agent actions and intricate reward design (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Qu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib126); Xu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib180)). Given our emphasis on efficient exploration, we prioritize evaluation in multi-agent settings. Some MAE methods encourage influential behaviors during agent interactions (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Jaques et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib55); Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)). Nevertheless, they may lead to unintended coalitions or require additional priors (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)). Certain studies leverage subgoals to guide exploration (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)). However, due to challenges in integrating task-related information into subgoals, they either necessitate human expertise for subgoals design (Tang et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib155); Kulkarni et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib70)) or struggle to identify useful subgoals (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56); Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). Distinguished from the above, this work underscores the significance of task-relevant guidance in exploration and utilizes the key state priors extracted from LLM to enable efficient multi-agent exploration.

## 4 Method

![Refer to caption](img/825b9c53b801799370d6c95a343b6acd.png)

Figure 2: Overview of the training process. (a) Key States Localization with LLM: We devise a set of prompts to guide LLM in localizing key states based on task-specific information. Refinements of the response are achieved through iterative self-checks by LLM. Subsequently, discriminator functions are derived from the final response to discriminate key states within trajectories. (b) Key States-Guided Exploration: Using the achieved key states chain within the processed trajectory, we look up KSMT to get the most probable next key states. By sampling from them as the subgoal for the concluding sub-trajectory, we integrate intrinsic rewards into the overall trajectory using SHIR.

This section elaborates on the developed LEMAE. The concept of the key states is first induced as the task-relevant guidance in Sec. [4.1](https://arxiv.org/html/2410.02511v1#S4.SS1 "4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). Centering around the key states, we construct two components: (i) key states localization with LLM (Sec. [4.2](https://arxiv.org/html/2410.02511v1#S4.SS2 "4.2 Key States Localization with LLM ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")) and (ii) key state-guided exploration (Sec. [4.3](https://arxiv.org/html/2410.02511v1#S4.SS3 "4.3 Key State-Guided Exploration ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")). The former directs LLM to generate discriminator functions for localizing key states in rollout trajectories, while the latter guides and organizes exploration with the introduced Subspace-based Hindsight Intrinsic Reward and Key States Memory Tree. Please refer to Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2 "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") and Algorithm [1](https://arxiv.org/html/2410.02511v1#algorithm1 "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") for details. Also, we provide a [demonstration](https://sites.google.com/view/lemae)¹¹1[https://sites.google.com/view/lemae](https://sites.google.com/view/lemae) to clarity the LEMAE’s execution pipeline.

### 4.1 Devil is in the Key States

Previous methods suffer from redundant exploration efforts in pursuing task-agnostic novelty (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)), potentially reducing training efficiency. This motivates us to integrate task-relevant information as a better guidance choice for efficient exploration. Nevertheless, practical proposals are limited in the field. This work identifies the Key States as the novel task-relevant prior, which corresponds to intermediate states with explicit semantics and expressions pertaining to the task. Meanwhile, Proposition [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1 "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") explicitly reflects the efficacy of incorporating them.

###### Proposition 4.1.

Consider the one-dimensional asymmetric random walk problem, where an agent starts at $x=0$ and aims to reach $x=N\in\mathbb{N^{+}},N>1$. The initial policy is asymmetric and random with probabilities $p\in(0.5,1)$ and $1-p$ for right and left movements, respectively. Without prior knowledge, the expected first hitting time is $\mathbb{E}(T_{0\rightarrow N})=\frac{N}{2p-1}$. After introducing the task-relevant information that the agent must first reach key states $\kappa=1,...,N-1$ before reaching $x=N$, we can decrease the expected first hitting time by $\mathbb{E}(T_{0\rightarrow N})-\mathbb{E}(T^{prior}_{0\rightarrow N})=(N-1)*(% \frac{1}{2p-1}-\frac{2}{p}+1)>0$.

The proof is deferred to Appendix [C](https://arxiv.org/html/2410.02511v1#A3 "Appendix C Proof of Proposition 4.1 ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). The exploration policy substantially benefits from the involvement of key states, e.g., $\mathbb{E}(T_{0\rightarrow N})-\mathbb{E}(T^{prior}_{0\rightarrow N})\to\infty$ with $p\to 0.5$. Such a concept is also commonly seen in practical scenarios, such as in-game checkpoints (Demaine et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib31)) and landmarks in navigation (Becker et al., [1995](https://arxiv.org/html/2410.02511v1#bib.bib11)).

### 4.2 Key States Localization with LLM

To reduce manual workload, we employ LLM to localize key states. Although generating the aforementioned symbolic key states can be straightforward, LLM’s weakness in comprehending symbolic states or environment details necessitates additional information in certain tasks and can lead to errors and hallucinations that are difficult to detect. Here, we stress the importance of LLM’s discriminative ability to localize key states in rollout trajectories to better leverage LLM’s general knowledge. The rationale is that discrimination demands only a high-level task understanding and is more reliable and universal than naive generation, as discussed in detail in Appendix [B.1](https://arxiv.org/html/2410.02511v1#A2.SS1 "B.1 The Insights Behind Key States Discrimination ‣ Appendix B Further Discussions ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

To discriminate key states, we prompt LLM to generate $m$ discriminator functions $\{\mathcal{F}_{i}\}_{i=1}^{m}$, as depicted in Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2 "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). Each discriminator function $\mathcal{F}_{i}$ takes in the state $s_{t}$ at timestep $t$ and outputs a boolean value to tell whether the input state is the key state $\kappa_{i}$. Such an approach systematically annotates each state in trajectories as a key state instance or not. Notably, LEMAE injects task-relevant information into the symbolic states without predefined components such as observation captioners (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)) or environment codes (Xie et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib179)), which require manual fine-tuning, may be unavailable in many scenarios, or could introduce extra information. In addition, the discriminator functions’ reusability avoids frequent calls, and our method empirically requires fewer than three LLM inferences for a specific task. These advantages highlight the potential of LEMAE to expand the scope of application scenarios with fewer constraints and reduced costs.

We design prompts to alleviate the burden of labor-intensive prompt engineering across tasks. As illustrated in Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2 "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), each task’s prompt is structured by a standardized prompt template and task information. The prompt template, consistent across tasks, primarily contains several role instructions to guide LLM in role understandings (including promoting labor division among agents in MARL) and output constraints. For a new task with symbolic state space, the prompt template requires only essential details, i.e., the task description and the state form, which can be easily extracted from the task document without additional processing, making it less demanding than previous methods (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98); Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)). An extension to vision-based tasks is described in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2 "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

Considering that LLM sometimes generates inaccurate responses and non-executable codes, we develop a Self-Check mechanism to enable LLM’s autonomous evaluation and response improvement, which is inspired by recent approaches (Shinn et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib141); Dhuliawala et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib32)). The mechanism comprises two checking operations: LLM rethinking and code verification. The former prompts LLM with a set of queries for self-assessment, ensuring compliance with specified criteria. The latter verifies the executability of discriminator functions with actual state inputs, providing feedback until all functions are executable.

We use GPT-4-turbo from OpenAI API and prompt details are attached in Appendix [D](https://arxiv.org/html/2410.02511v1#A4 "Appendix D LLM Prompts and Responses ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

### 4.3 Key State-Guided Exploration

#### 4.3.1 Subspace-based Hindsight Intrinsic Reward

With the annotated key states, trajectories can naturally be segmented into sub-trajectories. Drawing inspiration from Andrychowicz et al. ([2017](https://arxiv.org/html/2410.02511v1#bib.bib8)), we integrate hindsight intrinsic rewards by conceptualizing the annotated key states as sub-trajectories’ subgoals, which is further discussed in Appendix [E.3](https://arxiv.org/html/2410.02511v1#A5.SS3 "E.3 Connection and Comparison with HER ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). Such integration guides the policy toward achieving these key states by increasing reward density, thus reducing manual reward design burdens. Moreover, the state vector index from the discriminator function constitutes the reward-related subspace of the state (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). Here, we write the Subspace-based Hindsight Intrinsic Reward (SHIR) function as:

|  | $r_{I}^{\kappa_{m}}(t)=\&#124;\Phi_{m}(s_{t})-\Phi_{m}(\kappa_{m})\&#124;-\&#124;\Phi_{m}(s_{t% +1})-\Phi_{m}(\kappa_{m})\&#124;,$ |  | (1) |

where $\|\cdot\|$ denotes a distance metric, e.g., Manhattan Distance; $\Phi_{m}(s)=(s_{e})_{e\in\upsilon_{m}}$ restricts the state space to elements $e\in\upsilon_{m}$, $s_{e}$ is the $e$-th element of the full-state $s$, and $\upsilon_{m}\subset\mathbb{N}^{+}$ refers to the subset of entire state space from the discriminator function $\mathcal{F}_{m}$.

Given that rewards generally rely on a limited subset of the entire state space (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92); Todorov et al., [2012](https://arxiv.org/html/2410.02511v1#bib.bib158)), adopting subspace-based rewards helps avoid the potential redundancy and bias associated with the design of intrinsic rewards in the entire state space. LEMAE is also applicable to scenarios where rewards depend on the global state space, as it imposes no strict constraints. Hence, the final reward function is further derived as:

|  | $r(t)=\alpha\cdot r_{E}(t)+\beta\cdot r_{I}^{\kappa_{m}}(t),$ |  | (2) |

where $r_{E}$ denotes the extrinsic reward with $\alpha,\beta\in\mathbb{R^{+}}$ non-negative scaling factors.

#### 4.3.2 Key States Memory Tree

To organize exploration with memory, we introduce the concept of Key States Memory Tree (KSMT). It tracks transitions between key states and further serves exploration and planning. Compared with the naive $\epsilon$-greedy method, gradually revealing the KSMT helps avoid redundant exploration throughout the state space, particularly beneficial in more complicated real-world scenarios. Notably, LEMAE is compatible with other memory structures, such as Directed Acyclic Graphs.

Construct KSMT: Initialized at the root node, KSMT dynamically expands by iteratively incorporating key state chains obtained from annotated trajectories, as outlined in Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2 "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). These steps repeat until either reaching the success state or fully depicting the transitions between key states.

Explore with KSMT: To discover new KSMT branches, we adopt an exploration strategy that balances high-randomness policy $\bm{\pi}_{\theta}^{\epsilon_{h}}$ for exploring under-explored nodes with low-randomness policy $\bm{\pi}_{\theta}^{\epsilon_{l}}$ to minimize interference with policy learning, as shown in Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2 "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). Upon reaching a leaf node, agents execute $\bm{\pi}_{\theta}^{\epsilon_{h}}$ to deepen KSMT. While reaching a non-leaf node $\xi_{i}$, the agents take $\bm{\pi}_{\theta}^{\epsilon_{h}}$ with probability $p_{i}$ to expand the breadth or $\bm{\pi}_{\theta}^{\epsilon_{l}}$ with probability $1-p_{i}$ for progression towards the next key state. The probability $p_{i}$ is calculated as $p_{i}=\frac{1}{d_{i}+1}$, with $d_{i}$ the degree of the node $\xi_{i}$ as an indicator of the degree of under-exploration. The exploration phase completes upon the discovery of the success state. We also prune branches that do not lead to success to circumvent task-irrelevant key states. In this way, KSMT enables exploration in a more meaningful state subspace.

Plan with KSMT: Since KSMT acts as a dynamic model within the key state space, we plan the subgoal for the final sub-trajectory based on it. As shown in Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2 "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")b, given the achieved key states chain, we identify the corresponding branch ($\kappa_{2}\rightarrow\kappa_{1}\rightarrow\text{children}$) in KSMT through a lookup operation. Since they have been validated by memory, the children represent the most likely next key states, from which we randomly sample the final subgoal. This process mainly handles cases where trajectories fail to reach a key state as the final subgoal. It enhances SHIR and improves the efficacy of exploring KSMT by encouraging agents to access existing key states.

## 5 Experiments

![Refer to caption](img/0f6ac5d4680bff46aad4e78cf48aaf6e.png)

Figure 3: Evaluating LEMAE against baseline methods on four MPE maps with sparse rewards, using test win rate as the evaluation metric. The acceleration rate refers to how much faster LEMAE finds the success state compared to CMAE.

We conduct experiments on commonly used multi-agent exploration benchmarks: (1) the Multiple-Particle Environment (Lowe et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib97); Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)) and (2) the StarCraft Multi-Agent Challenge (Samvelyan et al., [2019b](https://arxiv.org/html/2410.02511v1#bib.bib133)). Following previous studies (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98); Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92); Xu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib180)), we focus primarily on tasks with symbolic state spaces and use the sparse reward version for all tasks without specific instructions.

Baselines. We compare LEMAE with representative baselines: IPPO is a MARL algorithm which extends PPO (Schulman et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib136)); QMIX (Rashid et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib128)) is a widely adopted MARL baseline; EITI and EDTI (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)) employ the impact of interaction in coordinated agents’ behaviors; MAVEN (Mahajan et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib100)) combine value-based and policy-based approaches through hierarchical control; CMAE (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)) learns cooperative exploration by selecting shared goals from multiple projected state space; RODE (Wang et al., [2020d](https://arxiv.org/html/2410.02511v1#bib.bib172)) decomposes joint action spaces into role-based ones to enhance exploration; MASER (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)) generates subgoals automatically for multiple agents from the experience replay buffer; LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)) addresses the lazy agents problem by mathematical definition and causal analysis. ELLM (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)) employs LLM priors to guide vision-based exploration, using state captioners and semantic similarity-based rewards. LEMAE is implemented on IPPO in MPE and QMIX in SMAC, consistent with previous works (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169); Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)) to ensure fair comparisons.

We run each algorithm on five random seeds and report the mean performance with standard deviation. Further details can be referenced in Appendix [E](https://arxiv.org/html/2410.02511v1#A5 "Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

### 5.1 Multiple-Particle Environment (MPE)

In MPE, we evaluate LEMAE on Pass, Secret-Room, Push-Box, and Large-Pass, which are commonly used multi-agent exploration tasks in previous works (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169); Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)).

LLM can effectively discriminate key states. To start with, we examine the efficacy of LLM in discriminating key states. On the Pass task, as shown in Fig. [1](https://arxiv.org/html/2410.02511v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a, a room is divided by a wall, each half containing an invisible switch. Passage through the door is allowed only when an agent occupies a switch. Initially, in the left half-room, agents must cooperate to move to the right half-room. In Fig. [4(a)](https://arxiv.org/html/2410.02511v1#S5.F4.sf1 "In Figure 4 ‣ 5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), LLM exhibits a precise understanding of the task and generates meaningful discriminator functions, demonstrating the feasibility of our approach based on the current LLM.

![Refer to caption](img/d26a8f83c7b7f3b87a742cb3469729f3.png)

(a)   

![Refer to caption](img/c1cc1bd491d7dd5cf405f927e32c909d.png)

(b)   

![Refer to caption](img/e223b87513c11a928b1f849f679368f2.png)

(c)   

Figure 4: (a) Key states discrimination functions generated on task Pass. (b) The map of Secret-Room with key states: $\kappa_{1}$ represents occupying the left switch to open all doors, while $\kappa_{2}$, $\kappa_{3}$, and $\kappa_{4}$ represent exploring right rooms 1, 2, and 3, respectively. The directional arrows symbolize the transitional relationships within KSMT. (c) The key states number curve in Secret-Room shows that LEMAE can identify all key states and proficiently prune task-irrelevant ones.

LEMAE achieves superior performance. We investigate how LEMAE enhances exploration by comparing it with baselines, confirming the value of incorporating LLM priors. The training curves are depicted in Fig. [3](https://arxiv.org/html/2410.02511v1#S5.F3 "Figure 3 ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). The failure of commonly used baselines highlights the necessity and urgency for efficient exploration, while the superior performance of LEMAE underscores the effectiveness of augmenting RL with task-specific guidance from LLM. Specifically, the failure of EITI, EDTI, and LAIES may be attributed to the complexity of learning dynamics or the scarcity of external state changes in the tasks. While CMAE learns effective strategies for simple tasks, its redundant exploration hampers efficiency, rendering it inadequate for tasks with expansive exploration spaces, such as Large-Pass. Although it benefits from LLM priors, ELLM performs worse than LEMAE due to the weak guidance provided by semantic similarity-based rewards, not to mention its reliance on frequent LLM inference and a predefined state captioner. Furthermore, we compare LEMAE with traditional SOTA baseline CMAE using the metric of the number of exploration steps taken to find the success state. The results indicate a significant exploration acceleration rate, up to 10x, underscoring LEMAE’s efficiency. The superior performance of our method can be attributed to the mitigating of redundant exploration by incorporating task-relevant information.

LEMAE benefits from LLM priors through discrimination. We evaluate HER (Andrychowicz et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib8)), which also employs hindsight intrinsic rewards but selects goals randomly from memory. HER’s poor performance emphasizes the critical role of incorporating LLM priors for localizing key states in achieving efficient exploration. To further support our claim about the superiority of LLM discrimination over generation, we evaluate Eureka-si, a single-iteration variant of Eureka (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)), which uses LLM to generate reward functions directly. While Eureka-si performs comparably to LEMAE in simple tasks, it struggles in most complex tasks with partial observability, indicating that LLM-based discrimination may offer a more general and effective integration of LLM. Notably, these two methods are not specifically designed for efficient exploration. Please refer to Appendix [E.2](https://arxiv.org/html/2410.02511v1#A5.SS2 "E.2 Comparison with LLM Reward Design ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), [E.3](https://arxiv.org/html/2410.02511v1#A5.SS3 "E.3 Connection and Comparison with HER ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") for more details.

LEMAE reduces redundant exploration. We further compare the exploration behavior of LEMAE with that of CMAE on the Pass task. The visitation maps, displayed in log scale, are depicted in Fig. [1](https://arxiv.org/html/2410.02511v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). The illustration reveals that LEMAE markedly avoids redundant exploration: agents trained with CMAE tend to excessively explore the left room, while the agents’ visitation area in LEMAE is notably concentrated around the success path. Furthermore, an organic division of labor among agents emerges, affirming the efficacy of encouraging labor division in prompt design.

LEMAE circumvents task-irrelevant key states. Due to the incomplete information, LLM may discriminate task-irrelevant key states. For instance, in the Secret-Room task, three rooms are present on the right, but LLM is not informed about the real target room for fairness. In Fig. [4(b)](https://arxiv.org/html/2410.02511v1#S5.F4.sf2 "In Figure 4 ‣ 5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), LLM discriminates two task-irrelevant key states, denoted as $\kappa_{2}$ and $\kappa_{4}$, which represent an exploration of the two irrelevant rooms, respectively. Fig. [4(c)](https://arxiv.org/html/2410.02511v1#S5.F4.sf3 "In Figure 4 ‣ 5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") shows that the pruning mechanism after finding the success state in LEMAE makes it effective in circumventing task-irrelevant key states. A more detailed robustness analysis is provided in Sec. [5.5](https://arxiv.org/html/2410.02511v1#S5.SS5 "5.5 Sensitivity & Robustness Analysis ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

![Refer to caption](img/6465f1af5771ae7d7e4e532d65ceca45.png)

Figure 5: Evaluating LEMAE on six SMAC maps with sparse rewards, using test win rate as the evaluation metric. Notably, QMIX-DR is QMIX with dense rewards in the original SMAC.

### 5.2 StarCraft Multi-Agent Challenge (SMAC)

SMAC is a widely-used challenging benchmark in MARL. In contrast to dense or semi-sparse reward versions used before, we employ fully sparse-reward tasks to emphasize exploration, rewarding agents only upon complete enemy elimination. In addition, to validate LEMAE across diverse scenarios, we conduct experiments on six maps with varied difficulty and agent numbers.

In Fig. [5](https://arxiv.org/html/2410.02511v1#S5.F5 "Figure 5 ‣ 5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), LEMAE demonstrates superior performance over all baselines. Although baselines QMIX, MAVEN, CDS, and MASER excel in dense or semi-sparse reward settings, they struggle in fully sparse reward scenarios. CMAE shows partial efficacy in simpler tasks but fails in harder scenarios due to the lack of task-related information in curiosity-driven goal selection. LAIES is the only non-LLM baseline comparable to LEMAE. However, it requires handcrafted external state priors and still underperforms compared to LEMAE, especially on more challenging tasks. ELLM, benefiting from LLM priors, performs well on simpler tasks, but its effectiveness diminishes on harder ones, likely due to the instability and less reliable guidance of semantic similarity-based rewards. Notably, we add QMIX-DR, which augments QMIX with dense rewards in the original SMAC. Surprisingly, LEMAE demonstrates the potential to match or even surpass QMIX-DR, particularly in hard maps, shedding light on minimizing the manual workload in complex reward design in real-world scenarios. Given the complexity of the SMAC benchmark, the consistent superiority of LEMAE confirms its potential applicability in more complex real-world scenarios. We further evaluate LEMAE on SMACv2 (Ellis et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib39)), an enhanced version with more stochasticity, as detailed in Appendix [F.1](https://arxiv.org/html/2410.02511v1#A6.SS1 "F.1 More Complicated Benchmark: SMACv2 ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

![Refer to caption](img/dfeb4592d67fcf7574f3201d3d99dc11.png)

(a)   

![Refer to caption](img/6a22426428fa9a7984e678d499c0eb33.png)

(b)   

Figure 6: (a) Evaluations on baselines of well-known MARL algorithms, i.e., QPLEX and VMIX. Notably, both QPLEX and VMIX exhibit complete failure unless integrated with our approach. (b) Ablation studies are conducted on two exemplary tasks from MPE and SMAC to assess the significance of KSMT and SHIR within LEMAE.

### 5.3 Compatiblility with Various Algorithms

LEMAE incorporates task-relevant guidance in the form of intrinsic rewards and is agnostic to RL algorithms. Sec. [5.1](https://arxiv.org/html/2410.02511v1#S5.SS1 "5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") and [5.2](https://arxiv.org/html/2410.02511v1#S5.SS2 "5.2 StarCraft Multi-Agent Challenge (SMAC) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") have verified the compatibility through implementing on two distinct MARL algorithms: IPPO in MPE and QMIX in SMAC. To further substantiate this claim, we build our method on two widely-used MARL algorithms, namely QPLEX (Wang et al., [2020a](https://arxiv.org/html/2410.02511v1#bib.bib166)) and VMIX (Su et al., [2021](https://arxiv.org/html/2410.02511v1#bib.bib149)), adopting a value-based and actor-critic methodology respectively. As illustrated in Fig. [6(a)](https://arxiv.org/html/2410.02511v1#S5.F6.sf1 "In Figure 6 ‣ 5.2 StarCraft Multi-Agent Challenge (SMAC) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), algorithms combined with LEMAE consistently improve performance, underscoring the potential of LEMAE to integrate with alternative algorithms across diverse fields in the future. Additionally, LEMAE is a versatile approach for efficient exploration, not limited to MARL. To validate this assertion, we conduct further evaluations of LEMAE in a single-agent variant of MPE, as demonstrated in Appendix [F.4](https://arxiv.org/html/2410.02511v1#A6.SS4 "F.4 Experiments for Single-Agent setups ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

### 5.4 Ablation Studies

Role of SHIR and KSMT. We conduct an ablation study to assess the significance of KSMT and SHIR within LEMAE. We select two exemplary tasks from MPE and SMAC and report results in Fig. [6(b)](https://arxiv.org/html/2410.02511v1#S5.F6.sf2 "In Figure 6 ‣ 5.2 StarCraft Multi-Agent Challenge (SMAC) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). In SMAC, Base refers to QMIX, while in MPE, it denotes IPPO. Besides, SHIR represents subspace-based hindsight intrinsic reward, KSMTE signifies exploration with KSMT, KSMTP denotes planning with KSMT, and LEMAE encompasses Base+SHIR+KSMTE+KSMTP. As illustrated, the absence of SHIR or KSMT significantly deteriorates performance, revealing both components’ pivotal roles in achieving effective key state-guided exploration.

Role of Self-Check mechanism and LLMs. We conduct a comparative analysis between GPT-4-turbo and GPT-3.5-turbo regarding generating discriminator functions. Meanwhile, we investigate the performance of GPT-4-turbo without the Self-Check mechanism (GPT-4-turbo w/o). The Acceptance Rate ($r_{acc}$) denotes the proportion of seeds achieving over $80\%$ of the best performance after RL training, while the Execution Rate ($r_{exe}$) indicates the proportion of seeds for which all discriminator functions are executable. As depicted in Table [7](https://arxiv.org/html/2410.02511v1#S5.F7 "Figure 7 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), the results demonstrate that a powerful LLM with our Self-Check mechanism effectively ensures the high quality of key states, as evidenced by the code’s executability and the final performance. The scalability of LEMAE to LLM and our Self-Check mechanism promise that LEMAE can leverage more powerful LLMs in the future and be applied to more challenging real-world tasks safely and efficiently.

Table 1: Ablation studies on Self-Check mechanism and LLMs. We compare the performance of two LLMs (GPT-4-turbo and GPT-3.5-turbo), recording the Acceptance Rate ($r_{acc}$) and Execution Rate ($r_{exe}$) in ten runs of the generated discriminator functions. w/o denotes the absence of our Self-Check mechanism.

 | $r_{acc}$ ($r_{exe}$) | GPT-4-turbo | GPT-4-turbo w/o | GPT-3.5-turbo |
| Large-Pass | 1.0 (1.0) | 0.8 (1.0) | 0.7 (1.0) |
| 2m_vs_1z | 1.0 (1.0) | 0.7 (1.0) | 0.6 (1.0) |
| 5m_vs_6m | 1.0 (1.0) | 0.9 (1.0) | 1.0 (1.0) |
| MMM2 | 0.8 (1.0) | 0.6 (0.7) | 0.0 (1.0) | ![Refer to caption](img/bc63e1e1e6d9022b792b3a3cabd9fcc5.png)

Figure 7: Hyperparameter examination on reward scaling rate $\alpha$ and $\beta$. The x-axis represents the relative values with respect to the default parameters.

### 5.5 Sensitivity & Robustness Analysis

Sensitivity to Hyperparameters. We conduct experiments on the pivotal hyperparameters in LEMAE, i.e., reward scaling rates $\alpha$ and $\beta$. The x-axis represents the relative values with respect to the default ($\alpha=10$, $\beta=1$), encompassing evaluations for $\alpha\in\{1,5,10,50,100\}$ and $\beta\in\{0.1,0.5,1,5,10\}$. Fig. [7](https://arxiv.org/html/2410.02511v1#S5.F7 "Figure 7 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") illustrates that LEMAE is robust to these hyperparameters across a considerable range. Notably, excessive extrinsic reward scaling rate $\alpha$ or insufficient intrinsic reward scaling rate $\beta$ can cause performance degradation due to the abrupt alteration of the reward or the inadequate motivational impact. Additionally, we conduct an ablation study on mixed-randomness exploration in Appendix [F.3](https://arxiv.org/html/2410.02511v1#A6.SS3 "F.3 Ablation Studies on Mixed-Randomness Exploration ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

Robustness to Perturbations in Key States. We conduct experiments to evaluate the robustness of LEMAE to perturbations in key states. Specifically, Reduction simulates the absence of key states by randomly clearing a certain percentage of key states. Distraction simulates the misidentification of common states as key states by randomly adding a certain portion of

Table 2: Robustness analysis of LEMAE to perturbations in key states, whether randomly deleting key states (Reduction) or adding distracting states (Distraction).

 |  |  | Reduction | Distraction |
| Tasks | Default | 25% | 50% | 50% | 100% |
| 1c3s5z | 0.98$\pm$0.02 | 0.97$\pm$0.01 | 0.97$\pm$0.02 | 0.92$\pm$0.04 | 0.89$\pm$0.05 |
| 3s_vs_5z | 0.83$\pm$0.07 | 0.80$\pm$0.18 | 0.57$\pm$0.28 | 0.80$\pm$0.11 | 0.66$\pm$0.08 |
| MMM2 | 0.89$\pm$0.08 | 0.89$\pm$0.03 | 0.79$\pm$0.09 | 0.86$\pm$0.04 | 0.79$\pm$0.08 | 

distracting states (encouraging a random state dimension to 0). The observed performance decrease with increasing perturbations in Table [2](https://arxiv.org/html/2410.02511v1#S5.T2 "Table 2 ‣ 5.5 Sensitivity & Robustness Analysis ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") underscores the significance of key states’ quality. LEMAE exhibits notable robustness to perturbations, ensuring its reliability across diverse application scenarios, particularly in light of the limited capabilities of current LLMs.

### 5.6 Scalability & Generalization Analysis

![Refer to caption](img/3aad021b494c9f0131b57baf962b72a3.png)

Figure 8: (a) A brand new task, River, which LLM has never encountered before. (b) The training curves of LEMAE and baselines using the evaluation metric of test win rate.

To rule out the possibility that LEMAE’s success relies on LLM’s familiarity with the chosen tasks, we’ve handcrafted a brand new task, termed River, which LLM has never encountered before. The task is illustrated in Fig. [8](https://arxiv.org/html/2410.02511v1#S5.F8 "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a, where the objective is for Bob to help Alice, who is afraid of water, cross two rivers to reach the bottom-right corner. As shown in Fig. [8](https://arxiv.org/html/2410.02511v1#S5.F8 "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")b, LEMAE outperforms the baselines, and this confirms LLM’s generalization capabilities to empower LEMAE’s effectiveness in promoting efficient exploration in diverse new tasks. Please refer to Appendix [E.4.3](https://arxiv.org/html/2410.02511v1#A5.SS4.SSS3 "E.4.3 A Brand New Task: River ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") for details on the task.

Additionally, we extend LEMAE to a vision-based task, as described in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2 "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), demonstrating the scalability potential of LEMAE.

## 6 Conclusion

Summary of This Work: We present LEMAE, a novel framework that benefits multi-agent exploration with task-specific guidance from LLM. LEMAE executes the key states localization with LLM and enables the key state-guided exploration to improve sample efficiency. In this way, we can (i) build up connections between LLM and RL to ground linguistic knowledge into decision-making, (ii) reduce the manual workload in accessing knowledge and intensive inference calls from LLM, and (iii) significantly boost exploration efficiency through guided and organized exploration. Extensive experiments further examine the effectiveness of LEMAE in typical benchmarks.

Limitations & Future Investigations: In developing LEMAE, we made efforts to compensate for the pitfalls of concurrent LLMs, e.g., careful preparation for prompt engineering and task-related prior provision to avoid the nuisances in LLM usages. All of these can be circumvented with the progress of LLM’s capability enhancement. This work paves the way for LLM-empowered RL to achieve the potential in complicated decision-making scenarios.

## References

*   Abbeel & Ng (2004) Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In *Proceedings of the twenty-first international conference on Machine learning*, pp.  1, 2004.
*   Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
*   Ackermann et al. (2019) Johannes Ackermann, Volker Gabler, Takayuki Osa, and Masashi Sugiyama. Reducing overestimation bias in multi-agent domains using double centralized critics. *arXiv preprint arXiv:1910.01465*, 2019.
*   Agarwal et al. (2019) Akshat Agarwal, Sumit Kumar, and Katia Sycara. Learning transferable cooperative behavior in multi-agent teams. *arXiv preprint arXiv:1906.01202*, 2019.
*   Agarwal et al. (2020) Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In *International Conference on Machine Learning*, pp. 104–114\. PMLR, 2020.
*   Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. *arXiv preprint arXiv:2204.01691*, 2022.
*   An et al. (2021) Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. *Advances in Neural Information Processing Systems*, 34, 2021.
*   Andrychowicz et al. (2017) Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. *Advances in neural information processing systems*, 30, 2017.
*   Author (2021) N. N. Author. Suppressed for anonymity, 2021.
*   Baker et al. (2019) Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. *arXiv preprint arXiv:1909.07528*, 2019.
*   Becker et al. (1995) Craig Becker, Joaquin Salas, Kentaro Tokusei, and J-C Latombe. Reliable navigation using landmarks. In *Proceedings of 1995 IEEE International conference on Robotics and Automation*, volume 1, pp.  401–406\. IEEE, 1995.
*   Belghazi et al. (2018) Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In *International conference on machine learning*, pp. 531–540\. PMLR, 2018.
*   Bellemare et al. (2016) Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. *Advances in neural information processing systems*, 29, 2016.
*   Berner et al. (2019) Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. *arXiv preprint arXiv:1912.06680*, 2019.
*   Boss (1978) R Wayne Boss. The effects of leader absence on a confrontation team-building design. *The Journal of Applied Behavioral Science*, 14(4):469–478, 1978.
*   Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901, 2020.
*   Browne et al. (2012) Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. *IEEE Transactions on Computational Intelligence and AI in games*, 4(1):1–43, 2012.
*   Buckman et al. (2020) Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in fixed-dataset policy optimization. *arXiv preprint arXiv:2009.06799*, 2020.
*   Burda et al. (2018a) Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. *arXiv preprint arXiv:1808.04355*, 2018a.
*   Burda et al. (2018b) Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. *arXiv preprint arXiv:1810.12894*, 2018b.
*   Cai et al. (2020) Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy optimization. In *International Conference on Machine Learning*, pp. 1283–1294\. PMLR, 2020.
*   Cao et al. (2024) Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang, Junhua Zhao, and Yun Li. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. *arXiv preprint arXiv:2404.00282*, 2024.
*   Carta et al. (2023) Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. *arXiv preprint arXiv:2302.02662*, 2023.
*   Chen et al. (2024) Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, and Liangjun Zhang. Rlingua: Improving reinforcement learning sample efficiency in robotic manipulations with large language models. *IEEE Robotics and Automation Letters*, 2024.
*   Cheng et al. (2022) Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. *arXiv preprint arXiv:2202.02446*, 2022.
*   Cheng et al. (2020) Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence Carin. Club: A contrastive log-ratio upper bound of mutual information. In *International conference on machine learning*, pp. 1779–1788\. PMLR, 2020.
*   Claus & Boutilier (1998) Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. In *AAAI Conference on Artificial Intelligence (AAAI)*, 1998.
*   Colas et al. (2023) Cédric Colas, Laetitia Teodorescu, Pierre-Yves Oudeyer, Xingdi Yuan, and Marc-Alexandre Côté. Augmenting autotelic agents with large language models. *arXiv preprint arXiv:2305.12487*, 2023.
*   Das et al. (2019) Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication. In *International Conference on Machine Learning*, pp. 1538–1546\. PMLR, 2019.
*   de Witt et al. (2020) Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? *arXiv preprint arXiv:2011.09533*, 2020.
*   Demaine et al. (2016) Erik D Demaine, Giovanni Viglietta, and Aaron Williams. Super mario bros. is harder/easier than we thought. 2016.
*   Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. *arXiv preprint arXiv:2309.11495*, 2023.
*   Ding et al. (2020) Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually inferred communication for multi-agent cooperation. *arXiv preprint arXiv:2006.06455*, 2020.
*   Dong et al. (2017) Xiwang Dong, Qingdong Li, Qilun Zhao, and Zhang Ren. Time-varying group formation analysis and design for general linear multi-agent systems with directed topologies. *International Journal of Robust and Nonlinear Control*, 27(9):1640–1652, 2017.
*   Du et al. (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretraining in reinforcement learning with large language models. *arXiv preprint arXiv:2302.06692*, 2023.
*   Duda et al. (2000) R. O. Duda, P. E. Hart, and D. G. Stork. *Pattern Classification*. John Wiley and Sons, 2nd edition, 2000.
*   Durrett (2019) Rick Durrett. *Probability: theory and examples*, volume 49. Cambridge university press, 2019.
*   Ecoffet et al. (2021) Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then explore. *Nature*, 590(7847):580–586, 2021.
*   Ellis et al. (2024) Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multi-agent reinforcement learning. *Advances in Neural Information Processing Systems*, 36, 2024.
*   Fischer (2020) Ian Fischer. The conditional entropy bottleneck. *Entropy*, 22(9):999, 2020.
*   Foerster et al. (2016) Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In *Advances in neural information processing systems (NeurIPS)*, 2016.
*   Foerster et al. (2018) Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2018.
*   Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. *arXiv preprint arXiv:2004.07219*, 2020.
*   Fujimoto & Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. *Advances in neural information processing systems*, 34:20132–20145, 2021.
*   Fujimoto et al. (2018) Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In *International conference on machine learning*, pp. 1587–1596\. PMLR, 2018.
*   Fujimoto et al. (2019) Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In *International conference on machine learning*, pp. 2052–2062\. PMLR, 2019.
*   Gupta et al. (2017) Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative multi-agent control using deep reinforcement learning. In *International Conference on Autonomous Agents and Multiagent Systems (AAMAS)*, 2017.
*   Han et al. (2017) Tao Han, Zhi-Hong Guan, Ming Chi, Bin Hu, Tao Li, and Xian-He Zhang. Multi-formation control of nonlinear leader-following multi-agent systems. *ISA transactions*, 69:140–147, 2017.
*   Hooper et al. (2010) Paul L Hooper, Hillard S Kaplan, and James L Boone. A theory of leadership in human cooperative groups. *Journal of Theoretical Biology*, 265(4):633–646, 2010.
*   Hu et al. (2019) Junyan Hu, Parijat Bhowmick, and Alexander Lanzon. Distributed adaptive time-varying group formation tracking for multiagent systems with multiple leaders on directed graphs. *IEEE Transactions on Control of Network Systems*, 7(1):140–150, 2019.
*   Hu et al. (2020) Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal multi-agent rl via policy decoupling with transformers. In *International Conference on Learning Representations (ICLR)*, 2020.
*   Iqbal et al. (2021) Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin Böhmer, Shimon Whiteson, and Fei Sha. Randomized entity-wise factorization for multi-agent reinforcement learning. In *International Conference on Machine Learning (ICML)*, 2021.
*   Jaderberg et al. (2019) Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3d multiplayer games with population-based reinforcement learning. *Science*, 364(6443):859–865, 2019.
*   Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. *arXiv preprint arXiv:1906.08253*, 2019.
*   Jaques et al. (2019) Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep reinforcement learning. In *International Conference on Machine Learning (ICML)*, 2019.
*   Jeon et al. (2022) Jeewon Jeon, Woojun Kim, Whiyoung Jung, and Youngchul Sung. Maser: Multi-agent reinforcement learning with subgoals generated from experience replay buffer. In *International Conference on Machine Learning*, pp. 10041–10052\. PMLR, 2022.
*   Jiang & Lu (2018) Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation. *arXiv preprint arXiv:1805.07733*, 2018.
*   Jiang & Lu (2021) Jiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent reinforcement learning. *arXiv preprint arXiv:2108.01832*, 2021.
*   Jiang et al. (2018) Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning. *arXiv preprint arXiv:1810.09202*, 2018.
*   Jin et al. (2021) Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In *International Conference on Machine Learning*, pp. 5084–5096\. PMLR, 2021.
*   Jo et al. (2023) Yonghyeon Jo, Sunwoo Lee, Junghyuk Yum, and Seungyul Han. Fox: Formation-aware exploration in multi-agent reinforcement learning. *arXiv preprint arXiv:2308.11272*, 2023.
*   Kearns (1989) M. J. Kearns. *Computational Complexity of Machine Learning*. PhD thesis, Department of Computer Science, Harvard University, 1989.
*   Kidambi et al. (2020) Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. *arXiv preprint arXiv:2005.05951*, 2020.
*   Kim et al. (2024) Woo Kyung Kim, SeungHyun Kim, Honguk Woo, et al. Efficient policy adaptation with contrastive prompt ensemble for embodied agents. *Advances in Neural Information Processing Systems*, 36, 2024.
*   Kingma & Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. *arXiv preprint arXiv:1312.6114*, 2013.
*   Kocsis & Szepesvári (2006) Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In *European conference on machine learning*, pp.  282–293. Springer, 2006.
*   Kostrikov et al. (2021) Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisher divergence critic regularization. In *International Conference on Machine Learning*, pp. 5774–5783\. PMLR, 2021.
*   Kuba et al. (2021) Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. *arXiv preprint arXiv:2109.11251*, 2021.
*   Kulesza & Taskar (2012) Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. *arXiv preprint arXiv:1207.6083*, 2012.
*   Kulkarni et al. (2016) Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. *Advances in neural information processing systems*, 29, 2016.
*   Kumar et al. (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. *Advances in Neural Information Processing Systems*, 32, 2019.
*   Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. *Advances in Neural Information Processing Systems*, 33:1179–1191, 2020.
*   Kumar et al. (2021) Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine. Should i run offline reinforcement learning or behavioral cloning? In *Deep RL Workshop NeurIPS 2021*, 2021.
*   Kurach et al. (2019) Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac, Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski, Olivier Bousquet, et al. Google research football: A novel reinforcement learning environment. *arXiv preprint arXiv:1907.11180*, 2019.
*   Kwon et al. (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models. *arXiv preprint arXiv:2303.00001*, 2023.
*   Langley (2000) P. Langley. Crafting papers on machine learning. In Pat Langley (ed.), *Proceedings of the 17th International Conference on Machine Learning (ICML 2000)*, pp.  1207–1216, Stanford, CA, 2000\. Morgan Kaufmann.
*   Lawler & Limic (2010) Gregory F Lawler and Vlada Limic. *Random walk: a modern introduction*, volume 123. Cambridge University Press, 2010.
*   Lee et al. (2020) Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, and Sergio Guadarrama. Predictive information accelerates learning in rl. *arXiv preprint arXiv:2007.12401*, 2020.
*   Lee et al. (2022) Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble. In *Conference on Robot Learning*, pp.  1702–1712\. PMLR, 2022.
*   Leibo et al. (2021) Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement learning with melting pot. In *International Conference on Machine Learning (ICML)*, 2021.
*   Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. *arXiv preprint arXiv:2005.01643*, 2020.
*   Li et al. (2023) Belinda Z Li, William Chen, Pratyusha Sharma, and Jacob Andreas. Lampp: Language models as probabilistic priors for perception and action. *arXiv e-prints*, pp.  arXiv–2302, 2023.
*   Li et al. (2021) Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, and Chongjie Zhang. Celebrating diversity in shared multi-agent reinforcement learning. *Advances in Neural Information Processing Systems*, 34:3991–4002, 2021.
*   Li et al. (2022) Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-making. *Advances in Neural Information Processing Systems*, 35:31199–31212, 2022.
*   Liang et al. (2023) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In *2023 IEEE International Conference on Robotics and Automation (ICRA)*, pp.  9493–9500\. IEEE, 2023.
*   Lin et al. (2023) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. *arXiv preprint arXiv:2305.17390*, 2023.
*   Lin et al. (2021) Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, and Phillip Isola. Learning to ground multi-agent communication with autoencoders. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.
*   Linke et al. (2020) Cam Linke, Nadia M Ady, Martha White, Thomas Degris, and Adam White. Adapting behavior via intrinsic reward: A survey and empirical study. *Journal of artificial intelligence research*, 69:1287–1332, 2020.
*   Liu et al. (2021a) Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and Animashree Anandkumar. Coach-player multi-agent reinforcement learning for dynamic team composition. *arXiv preprint arXiv:2105.08692*, 2021a.
*   Liu et al. (2023) Boyin Liu, Zhiqiang Pu, Yi Pan, Jianqiang Yi, Yanyan Liang, and Du Zhang. Lazy agents: a new perspective on solving sparse reward problem in multi-agent reinforcement learning. In *International Conference on Machine Learning*, pp. 21937–21950\. PMLR, 2023.
*   Liu & Abbeel (2021) Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. *Advances in Neural Information Processing Systems*, 34:18459–18473, 2021.
*   Liu et al. (2021b) Iou-Jen Liu, Unnat Jain, Raymond A Yeh, and Alexander Schwing. Cooperative exploration for multi-agent deep reinforcement learning. In *International Conference on Machine Learning*, pp. 6826–6836\. PMLR, 2021b.
*   Liu et al. (2024) Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. Rl-gpt: Integrating reinforcement learning and code-as-policy. *arXiv preprint arXiv:2402.19299*, 2024.
*   Liu et al. (2020a) Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch reinforcement learning without great exploration. *arXiv preprint arXiv:2007.08202*, 2020a.
*   Liu et al. (2020b) Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, and Yang Gao. Multi-agent game abstraction via graph attention neural network. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2020b.
*   Long et al. (2020) Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, and Xiaolong Wang. Evolutionary population curriculum for scaling multi-agent reinforcement learning. *arXiv preprint arXiv:2003.10423*, 2020.
*   Lowe et al. (2017) Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In *Advances in neural information processing systems (NeurIPS)*, 2017.
*   Ma et al. (2023) Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. *arXiv preprint arXiv:2310.12931*, 2023.
*   Macchi (1977) Odile Macchi. The fermion process—a model of stochastic point process with repulsive points. In *Transactions of the Seventh Prague Conference on Information Theory, Statistical Decision Functions, Random Processes and of the 1974 European Meeting of Statisticians*, 1977.
*   Mahajan et al. (2019) Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational exploration. *Advances in neural information processing systems*, 32, 2019.
*   Mao et al. (2020a) Hangyu Mao, Wulong Liu, Jianye Hao, Jun Luo, Dong Li, Zhengchao Zhang, Jun Wang, and Zhen Xiao. Neighborhood cognition consistent multi-agent reinforcement learning. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pp.  7219–7226, 2020a.
*   Mao et al. (2020b) Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan Ni. Learning agent communication under limited bandwidth by message pruning. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pp.  5142–5149, 2020b.
*   Meng et al. (2021) Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained multi-agent decision transformer: One big sequence model conquers all starcraftii tasks. *arXiv preprint arXiv:2112.02845*, 2021.
*   Michalski et al. (1983) R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (eds.). *Machine Learning: An Artificial Intelligence Approach, Vol. I*. Tioga, Palo Alto, CA, 1983.
*   Michie et al. (1990) Donald Michie, Michael Bain, and J Hayes-Miches. Cognitive models from subcognitive skills. *IEE control engineering series*, 44:71–99, 1990.
*   Miller (2024) D. Lee Miller. Wordllama: Recycled token embeddings from large language models, 2024. URL [https://github.com/dleemiller/wordllama](https://github.com/dleemiller/wordllama).
*   Mirsky et al. (2020) Reuth Mirsky, William Macke, Andy Wang, Harel Yedidsion, and Peter Stone. A penny for your thoughts: The value of communication in ad hoc teamwork. In *International Joint Conferences on Artificial Intelligence (IJCAI)*, 2020.
*   Mitchell (1980) T. M. Mitchell. The need for biases in learning generalizations. Technical report, Computer Science Department, Rutgers University, New Brunswick, MA, 1980.
*   Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
*   Nachum et al. (2019) Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. *arXiv preprint arXiv:1912.02074*, 2019.
*   Nair et al. (2020a) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets. *arXiv preprint arXiv:2006.09359*, 2020a.
*   Nair et al. (2020b) Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. *arXiv preprint arXiv:2006.09359*, 2020b.
*   Newell & Rosenbloom (1981) A. Newell and P. S. Rosenbloom. Mechanisms of skill acquisition and the law of practice. In J. R. Anderson (ed.), *Cognitive Skills and Their Acquisition*, chapter 1, pp.  1–51\. Lawrence Erlbaum Associates, Inc., Hillsdale, NJ, 1981.
*   Niu et al. (2021) Yaru Niu, Rohan R Paleja, and Matthew C Gombolay. Multi-agent graph-attention communication and teaming. In *AAMAS*, pp.  964–973, 2021.
*   Oliehoek et al. (2016) Frans A Oliehoek, Christopher Amato, et al. *A concise introduction to decentralized POMDPs*. Springer, 2016.
*   Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*, 2018.
*   Paischer et al. (2022) Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, Sebastian Lehner, Hamid Eghbal-Zadeh, and Sepp Hochreiter. History compression via language models in reinforcement learning. In *International Conference on Machine Learning*, pp. 17156–17185\. PMLR, 2022.
*   Paischer et al. (2024) Fabian Paischer, Thomas Adler, Markus Hofmarcher, and Sepp Hochreiter. Semantic helm: A human-readable memory for reinforcement learning. *Advances in Neural Information Processing Systems*, 36, 2024.
*   Pan et al. (2021) Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon Whiteson. Regularized softmax deep multi-agent q-learning. *Advances in Neural Information Processing Systems*, 34:1365–1377, 2021.
*   Pan et al. (2022) Ling Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. Plan better amid conservatism: Offline multi-agent reinforcement learning with actor rectification. In *International Conference on Machine Learning*, pp. 17221–17237\. PMLR, 2022.
*   Pathak et al. (2017) Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In *International Conference on Machine Learning (ICML)*, 2017.
*   Peng et al. (2021) Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. *Advances in Neural Information Processing Systems*, 34:12208–12221, 2021.
*   Peng et al. (2023) Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang Tian, Ruizhi Chen, Zidong Du, Qi Guo, et al. Self-driven grounding: Large language model agents with automatical language-aligned skill learning. *arXiv preprint arXiv:2309.01352*, 2023.
*   Peng et al. (2019) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. *arXiv preprint arXiv:1910.00177*, 2019.
*   Poole et al. (2019) Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In *International Conference on Machine Learning*, pp. 5171–5180\. PMLR, 2019.
*   Qu et al. (2023) Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen, Zhenbin Ye, Lin Liu, Yang Jun Feng, Lin Lai, Hongyang Qin, et al. Hokoff: Real game dataset from honor of kings and its offline reinforcement learning benchmarks. In *Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2023.
*   Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. *OpenAI blog*, 1(8):9, 2019.
*   Rashid et al. (2018) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. *arXiv preprint arXiv:1803.11485*, 2018.
*   Rashid et al. (2020) Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonic value function factorisation. *arXiv e-prints*, pp.  arXiv–2006, 2020.
*   Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. *Advances in Neural Information Processing Systems*, 34, 2021.
*   Samuel (1959) A. L. Samuel. Some studies in machine learning using the game of checkers. *IBM Journal of Research and Development*, 3(3):211–229, 1959.
*   Samvelyan et al. (2019a) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge. *CoRR*, abs/1902.04043, 2019a.
*   Samvelyan et al. (2019b) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. *arXiv preprint arXiv:1902.04043*, 2019b.
*   Scarselli et al. (2008) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. *IEEE transactions on neural networks*, 20(1):61–80, 2008.
*   Schroeder de Witt et al. (2019) Christian Schroeder de Witt, Jakob Foerster, Gregory Farquhar, Philip Torr, Wendelin Boehmer, and Shimon Whiteson. Multi-agent common knowledge reinforcement learning. *Advances in Neural Information Processing Systems*, 32:9927–9939, 2019.
*   Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*, 2017.
*   Seo et al. (2021) Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient exploration. In *International Conference on Machine Learning*, pp. 9443–9454\. PMLR, 2021.
*   Shao et al. (2023) Jianzhun Shao, Hongchang Zhang, Yun Qu, Chang Liu, Shuncheng He, Yuhang Jiang, and Xiangyang Ji. Complementary attention for multi-agent reinforcement learning. In *International Conference on Machine Learning*, pp. 30776–30793\. PMLR, 2023.
*   Shao et al. (2024) Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang Ji. Counterfactual conservative q learning for offline multi-agent reinforcement learning. *Advances in Neural Information Processing Systems*, 36, 2024.
*   Shi et al. (2023) Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du, and Huazhe Xu. Unleashing the power of pre-trained language models for offline reinforcement learning. *arXiv preprint arXiv:2310.20587*, 2023.
*   Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. In *Thirty-seventh Conference on Neural Information Processing Systems*, 2023.
*   Shukla et al. (2023) Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez, Robert Wright, and Jivko Sinapov. Lgts: Dynamic task sampling using llm-generated sub-goals for reinforcement learning agents. *arXiv preprint arXiv:2310.09454*, 2023.
*   Singh et al. (2018) Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale in multiagent cooperative and competitive tasks. *arXiv preprint arXiv:1812.09755*, 2018.
*   Singh (1998) Munindar P Singh. Agent communication languages: Rethinking the principles. *Computer*, 31(12):40–47, 1998.
*   Son et al. (2019) Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In *International Conference on Machine Learning (ICML)*, 2019.
*   Song et al. (2023) Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan Shu, and Lei Ma. Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics. *arXiv preprint arXiv:2309.06687*, 2023.
*   Stone et al. (2010) Peter Stone, Gal A Kaminka, Sarit Kraus, and Jeffrey S Rosenschein. Ad hoc autonomous agent teams: Collaboration without pre-coordination. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2010.
*   Su & Zhang (2023) Jianhai Su and Qi Zhang. Subgoal proposition using a vision-language model. In *CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP)*, 2023.
*   Su et al. (2021) Jianyu Su, Stephen Adams, and Peter Beling. Value-decomposition multi-agent actor-critics. In *Proceedings of the AAAI conference on artificial intelligence*, volume 35, pp.  11352–11360, 2021.
*   Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropagation. In *Advances in neural information processing systems (NeurIPS)*, 2016.
*   Sunehag et al. (2018) Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In *International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)*, 2018.
*   Suttle et al. (2020) Wesley Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang, Tamer Başar, and Ji Liu. A multi-agent off-policy actor-critic algorithm for distributed reinforcement learning. *IFAC-PapersOnLine*, 53(2):1549–1554, 2020.
*   Sutton & Barto (2018) Richard S Sutton and Andrew G Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
*   Tan (1993) Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In *International Conference on Machine Learning (ICML)*, 1993.
*   Tang et al. (2018) Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang Zhang, Hangtian Jia, Chunxu Ren, Yan Zheng, Zhaopeng Meng, Changjie Fan, et al. Hierarchical deep multiagent reinforcement learning with temporal abstraction. *arXiv preprint arXiv:1809.09332*, 2018.
*   Thrun (1992) Sebastian B Thrun. *Efficient exploration in reinforcement learning*. Carnegie Mellon University, 1992.
*   Tian et al. (2020) Yuandong Tian, Qucheng Gong, and Tina Jiang. Joint policy search for multi-agent collaboration with imperfect information. *arXiv preprint arXiv:2008.06495*, 2020.
*   Todorov et al. (2012) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In *2012 IEEE/RSJ international conference on intelligent robots and systems*, pp.  5026–5033\. IEEE, 2012.
*   Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*, 2023.
*   Tseng et al. (2022) Wei-Cheng Tseng, Tsun-Hsuan Wang, Yen-Chen Lin, and Phillip Isola. Offline multi-agent reinforcement learning with knowledge distillation. In *Advances in Neural Information Processing Systems*, 2022.
*   Van der Maaten & Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. *Journal of machine learning research*, 9(11), 2008.
*   Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in neural information processing systems (NeurIPS)*, 2017.
*   Wan et al. (2021) Lipeng Wan, Zeyang Liu, Xingyu Chen, Han Wang, and Xuguang Lan. Greedy-based value representation for optimal coordination in multi-agent reinforcement learning. *arXiv preprint arXiv:2112.04454*, 2021.
*   Wang et al. (2024) Boyuan Wang, Yun Qu, Yuhang Jiang, Jianzhun Shao, Chang Liu, Wenming Yang, and Xiangyang Ji. Llm-empowered state representation for reinforcement learning. *arXiv preprint arXiv:2407.13237*, 2024.
*   Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023a.
*   Wang et al. (2020a) Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. *arXiv preprint arXiv:2008.01062*, 2020a.
*   Wang et al. (2023b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. *arXiv preprint arXiv:2308.11432*, 2023b.
*   Wang et al. (2020b) Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning efficient multi-agent communication: An information bottleneck approach. In *International Conference on Machine Learning*, pp. 9908–9918\. PMLR, 2020b.
*   Wang et al. (2019a) Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based multi-agent exploration. *arXiv preprint arXiv:1910.05512*, 2019a.
*   Wang et al. (2019b) Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie Zhang. Learning nearly decomposable value functions via communication minimization. *arXiv preprint arXiv:1910.05366*, 2019b.
*   Wang et al. (2020c) Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang. Roma: Multi-agent reinforcement learning with emergent roles. In *International Conference on Machine Learning (ICML)*, 2020c.
*   Wang et al. (2020d) Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang. Rode: Learning roles to decompose multi-agent tasks. *arXiv preprint arXiv:2010.01523*, 2020d.
*   Wang et al. (2020e) Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, and Yang Gao. From few to more: Large-scale dynamic multiagent curriculum learning. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2020e.
*   Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. *Advances in neural information processing systems*, 35:24824–24837, 2022.
*   Wu et al. (2022) Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. In *Advances in Neural Information Processing Systems*, 2022.
*   Wu et al. (2019) Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. *arXiv preprint arXiv:1911.11361*, 2019.
*   Wu et al. (2021) Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. *arXiv preprint arXiv:2105.08140*, 2021.
*   Xie et al. (2021) Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. *Advances in neural information processing systems*, 34, 2021.
*   Xie et al. (2023) Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward function generation for reinforcement learning. *arXiv preprint arXiv:2309.11489*, 2023.
*   Xu et al. (2023) Pei Xu, Junge Zhang, Qiyue Yin, Chao Yu, Yaodong Yang, and Kaiqi Huang. Subspace-aware exploration for sparse-reward multi-agent tasks. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37, pp.  11717–11725, 2023.
*   Xu et al. (2018) Zhao Xu, Yang Lyu, Quan Pan, Jinwen Hu, Chunhui Zhao, and Shuai Liu. Multi-vehicle flocking control with deep deterministic policy gradient method. In *International Conference on Control and Automation (ICCA)*, 2018.
*   Xu & Tewari (2020) Ziping Xu and Ambuj Tewari. Reinforcement learning in factored mdps: Oracle-efficient algorithms and tighter regret bounds for the non-episodic setting. *arXiv preprint arXiv:2002.02302*, 2020.
*   Yang et al. (2020a) Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. *arXiv preprint arXiv:2002.03939*, 2020a.
*   Yang et al. (2020b) Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang. Multi-agent determinantal q-learning. In *International Conference on Machine Learning (ICML)*, 2020b.
*   Yang et al. (2021) Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang, Gao Huang, Jun Yang, and Qianchuan Zhao. Believe what you see: Implicit constraint approach for offline multi-agent reinforcement learning. *Advances in Neural Information Processing Systems*, 34:10299–10312, 2021.
*   Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. *arXiv preprint arXiv:2210.03629*, 2022.
*   Yao et al. (2023) Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. *arXiv preprint arXiv:2308.02151*, 2023.
*   Ye et al. (2020) Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning. *arXiv preprint arXiv:2011.12692*, 2020.
*   Yu et al. (2021) Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. *arXiv preprint arXiv:2103.01955*, 2021.
*   Yu et al. (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. *arXiv preprint arXiv:2005.13239*, 2020.
*   Yu & Zhou (2008) Yang Yu and Zhi-Hua Zhou. A new approach to estimating the expected first hitting time of evolutionary algorithms. *Artificial Intelligence*, 172(15):1809–1832, 2008.
*   Yuan et al. (2022) Lei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, Zongzhang Zhang, Yang Yu, and Chongjie Zhang. Multi-agent incentive communication via decentralized teammate modeling. 2022.
*   Zanette et al. (2021) Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. *Advances in neural information processing systems*, 34, 2021.
*   Zhang & Lesser (2013) Chongjie Zhang and Victor R Lesser. Coordinating multi-agent reinforcement learning with limited communication. In *International Conference on Autonomous Agents and Multiagent Systems (AAMAS)*, 2013.
*   Zhang et al. (2023) Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, and Joseph J Lim. Bootstrap your own skills: Learning to solve new tasks with large language model guidance. *arXiv preprint arXiv:2310.10021*, 2023.
*   Zhang et al. (2018) Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-agent reinforcement learning with networked agents. In *International Conference on Machine Learning (ICML)*, 2018.
*   Zhao et al. (2023) Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models as commonsense knowledge for large-scale task planning. *arXiv preprint arXiv:2305.14078*, 2023.
*   Zhou et al. (2020) Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying Chung. Learning implicit credit assignment for multi-agent actor-critic. *arXiv preprint arXiv:2007.02529*, 2020.
*   Zhou et al. (2023) Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, and Bin Liu. Large language model as a policy teacher for training reinforcement learning agents. *arXiv preprint arXiv:2311.13373*, 2023.
*   Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*, 2023.

## Appendix A Algorithm

This section includes the pseudo algorithms. Algorithm [1](https://arxiv.org/html/2410.02511v1#algorithm1 "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") presents LEMAE’s main algorithm. LEMAE consists of four phases: generating discriminator functions with LLM, exploring with KSMT, calculating SHIR, and performing RL training. For on-policy RL, the buffer $D$ corresponds to a rollout buffer, while for off-policy RL, it is initialized as a replay buffer (Paischer et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib117)). Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2 "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") illustrates the process of exploring with KSMT. As our approach is agnostic to reinforcement learning algorithms, we leave out the details of standard RL training in the main paper.

Input: Large language model $\mathcal{M}$, prompt $\mathcal{P}$, rethinking prompt $\mathcal{P}^{re}$, non-negative scaling factors $\alpha,\beta$, randomness epsilon $\epsilon_{l},\epsilon_{h}$ ($\epsilon_{l}<\epsilon_{h}$), training frequency $\mathcal{N}$, max episodes $\mathcal{N}^{max}$, key states numbers $\mathcal{K}$.Output: Policy network $\bm{\pi}_{\theta}$.Randomly initialize the policy network parameter $\theta$.Initialize key states memory tree $\mathcal{T}\leftarrow[root]$, replay buffer $\mathcal{D}$, key states chain replay buffer $\mathcal{D}^{ks}$Initial discriminator functions $\{\mathcal{\hat{F}}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(\mathcal{P})$;// Self-Check MechanismLLM rethinking $\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(% \mathcal{P},\{\mathcal{\hat{F}}_{i}\}_{i=1}^{\mathcal{K}},\mathcal{P}^{re})$;while *there are non-executable discriminator functions in $\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}$* do       $\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(% \mathcal{P},\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}},error)$end whileFinal discriminator functions $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$;for *$episode=1$ to $\mathcal{N}^{max}$* do       // Explore with Key States Memory Tree (Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2 "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"))       $\kappa\_chain$, $\mathcal{T}$, $\tau$ $\leftarrow$ KSMT-Exp ($\bm{\pi}_{\theta}$, $\mathcal{T}$, $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$, $\epsilon_{l}$, $\epsilon_{h}$);       $\mathcal{D}\leftarrow\mathcal{D}\cup\{\tau\}$, $\mathcal{D}^{ks}\leftarrow\mathcal{D}^{ks}\cup\{\kappa\_chain\}$;       if *$episode$ mod $\mathcal{N}$ = 0* then             Sample a batch $B=\{\tau_{i}\}_{i=1}^{|B|}$ from $\mathcal{D}$ and the corresponding batch of key states chains $B^{ks}=\{\kappa\_chain_{i}\}_{i=1}^{|B|}$ from $\mathcal{D}^{ks}$;             for *$\tau=\{(s_{t},\bm{a}_{t},s_{t+1},r_{t})\}_{t=1}^{t_{max}},\kappa\_chain\in B,B% ^{ks}$* do                   $t^{start}\leftarrow 1$;                   for *$\{t^{end},\kappa_{m}\}\in\kappa\_chain$* do                         for *$t=t^{start}$ to $t^{end}$* do                               // Subspace-based Hindsight Intrinsic Reward (equation [1](https://arxiv.org/html/2410.02511v1#S4.E1 "In 4.3.1 Subspace-based Hindsight Intrinsic Reward ‣ 4.3 Key State-Guided Exploration ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"))                               Update $\tau$ with $r_{t}=\alpha\cdot r_{t}+\beta\cdot r_{I}^{\kappa_{m}}(s_{t},s_{t+1})$; $t^{start}\leftarrow t^{end}$;                         end for                   end for                  // Plan with Key States Memory Tree                   if *branch corresponding to $\kappa\_chain$ in $\mathcal{T}$* then                         Extract all children nodes $\mathcal{C}=\{\xi_{i}\}_{i=1}^{|\mathcal{C}|}$ and randomly sample a node $\xi_{i}\sim\mathcal{C}$;                   end if                  else                         Randomly sample a node $\xi_{i}$ from all nodes (not in $\kappa\_chain$) in $\mathcal{T}$;                   end if                  $\kappa_{plan}$ corresponds to the sampled node $\xi_{i}$;                   for *$t=t^{start}$ to $t^{max}$* do                         Update $\tau$ with $r_{t}=\alpha\cdot r_{t}+\beta\cdot r_{I}^{\kappa_{plan}}(s_{t},s_{t+1})$;                   end for                  Update $B$ with $\tau$;                   // RL Training (Algorithm Agnostic)                   Use an MARL algorithm to train $\theta$ with $B$;             end for       end ifend for

Algorithm 1 LEMAE

Input: Policy network $\bm{\pi}_{\theta}$, key states memory tree $\mathcal{T}$, discriminator functions $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$, randomness epsilon $\epsilon_{l},\epsilon_{h}$ ($\epsilon_{l}<\epsilon_{h}$).Output: $\kappa\_chain$, key states memory tree $\mathcal{T}$, trajectory $\tau$.Initialize $\kappa\_chain\leftarrow\left[\ \right],\tau\leftarrow\{\}$for *$t=1$ to $t_{\max}$* do       // key states localization in rollout trajectories       Discriminate $s_{t}$ with $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$;       if *($s_{t}$ is a key state $\kappa_{m}$) and ($\kappa_{m}$ not in $\kappa\_chain$)* then             $\kappa\_chain$.append($\{t,\kappa_{m}\}$);             if *branch corresponding to $\kappa\_chain$ not in $\mathcal{T}$* then                   // update KSMT                   add the branch into $\mathcal{T}$;             end if            else if *$\kappa_{m}$ correponds to a non-leaf node $\xi$* then                   // mixed-randomness exploration strategy                   $d\leftarrow$ the degree of the node $\xi$;                   $p=\frac{1}{d+1}$;                   $\epsilon=\begin{cases}\epsilon_{h}&\text{with probability }p\\ \epsilon_{l}&\text{with probability }1-p\end{cases}$;             end if            else                   $\epsilon=\epsilon_{h}$;             end if       end if      With probability $\epsilon$ select a random action $\bm{a}_{t}$;       Otherwise select $\bm{a}_{t}\sim\bm{\pi}_{\theta}(s_{t})$;       Obtain a tuple $(s_{t},\bm{a}_{t},s_{t+1},r_{t})$ by executing $\bm{a}_{t}$;       $\tau\leftarrow\tau\cup\{(s_{t},\bm{a}_{t},s_{t+1},r_{t})\}$end for

Algorithm 2 Explore with Key States Memory Tree (KSMT-Exp)

## Appendix B Further Discussions

### B.1 The Insights Behind Key States Discrimination

In our considered scenarios, we claim that discrimination is generally easier and more universal than key state generation by LLM, particularly in the context of high-dimensional states and partial observability. The reasons are as follows:

1.  1.

    Discrimination focuses on high-level task understanding and identifying key state characteristics, while generation requires detailed, low-level comprehension, assigning values to each element. This makes generation more challenging and error-prone, particularly in high-dimensional settings. Discrimination equivalently simplifies the output space to key state labels, thus alleviating issues like hallucinations.

2.  2.

    In implementations, errors in discriminator functions are easier to examine and correct through testing with real states. In contrast, errors in generated key states are harder to detect and are typically inferred from training performance.

3.  3.

    In cases of partial observability, generating key states directly is unreliable. For example, in the Pass task, the positions of hidden switches are unknown and must be inferred from the door’s status. LLM cannot generate key states accurately without knowledge of the specific agents’ positions required to activate a switch.

### B.2 Limitations

We build a bridge between LLM and RL to facilitate efficient exploration by leveraging task-related guidance provided by LLM. However, persistent constraints inherent to LLMs, such as their limited capacity to comprehend task-specific information and the inevitable hallucination, become bottlenecks in our approach, which induces the following limitations:

1.  1.

    We mitigate heavy prompt engineering through the use of a standardized prompt template but the necessity persists for manually providing task information for LLM. Thus, we assume the availability of semantic meanings for symbolic states. This assumption is feasible, as these manually designed states have inherent meanings documented in task specifications (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92); Samvelyan et al., [2019b](https://arxiv.org/html/2410.02511v1#bib.bib133)), and is no stronger than prior works requiring a state captioner (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)) or environment code (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)), which also involve manual fine-tuning or access to additional state information.

2.  2.

    Constrained by the limitations of LLM’s capabilities and the inherent issue of hallucination, it may face challenges in directly providing effective key state priors for more complex tasks. Besides, due to our use of LLMs, this work primarily focuses on tasks with symbolic states. Future research could extend its application to more complicated tasks, e.g., image-based tasks, by employing advanced multi-modal LLMs. We provide an initial attempt to extend LEMAE beyond symbolic tasks in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2 "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

Since the efficiency of the proposed LEMAE is essentially derived from versatile LLMs, we believe that the surge of foundation model exploration will flourish LLM-empowered RL.

### B.3 Future Works

The success of the proposed LEMAE highlights the necessity and efficacy of empowering RL with LLM. To enhance performance and extend applicability, we will explore two avenues for future research aimed at addressing the identified limitations. These avenues are outlined as follows:

1.  1.

    Streamlining the task information provision through multi-modal self-collection: Multi-modal LLMs are garnering increasing attention for their ability to comprehend situations through various modalities. Incorporating them with self-exploration and memory mechanisms shows promise in automating the collection and understanding of task information, thereby streamlining the implementation and enhancing the adaptability of LEMAE. We provide an initial attempt to extend LEMAE beyond symbolic tasks in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2 "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

2.  2.

    Unleashing the power of better LLM with an iterative feedback mechanism: Undoubtedly, given the rapid pace of LLM development, the emergence of more powerful LLMs is imminent. On one hand, we intend to harness the capabilities of these advanced LLMs. On the other hand, to fully unleash the potential of LLMs, we plan to devise an iterative feedback mechanism to feedback LLM in LEMAE during RL training to mitigate issues like hallucinations and errors in task understanding.

### B.4 Broader Impacts

Large Language Models have demonstrated considerable potential in showcasing impressive capabilities across various downstream tasks. However, research on empowering RL with LLMs is still nascent. As a pioneering endeavor to empower RL with LLM, we propose a general approach facilitating efficient exploration in RL with task-specific guidance from LLM.

1.  1.

    For the research community, the publication of this work will inspire further exploration into encouraging the integration of LLMs with RL to address the inherent challenges in RL, such as efficient exploration, limited sample efficiency, and unsatisfactory generalization. Additionally, our design promotes the application of discrimination and coding to ground linguistic knowledge from LLMs into symbolic tasks.

2.  2.

    LEMAE shows promise for real-world deployment in scenarios requiring efficient exploration, such as autonomous vehicle control and robot manipulation. Moreover, as LLM is growing by leaps and bounds, it is foreseeable that LEMAE can be applied to more challenging real-world tasks by taking advantage of more powerful LLM. Notably, to mitigate potential risks, it is imperative to conduct LLM generation and RL training under human supervision, thereby ensuring undesirable outcomes are averted.

## Appendix C Proof of Proposition [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1 "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")

###### Proof.

Random walk is a fundamental stochastic process, formed by successive summation of independent, identically distributed random variables (Lawler & Limic, [2010](https://arxiv.org/html/2410.02511v1#bib.bib77)). This work considers the one-dimensional asymmetric random walk problem, where an agent starts at $x=0$ and aims to reach $x=N\in\mathbb{N^{+}},N>1$. The expected first hitting time considered as the metric of performance, implying the average computational time complexity (Yu & Zhou, [2008](https://arxiv.org/html/2410.02511v1#bib.bib191)). Below is the proof of Proposition [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1 "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

Firstly, we can prove the expected first hitting time within the default setting through the application of martingale theory. According to the problem setting, we can define the movement at each time step as: $M_{0}=0$, $M_{1},M_{2}...$ are i.i.d. random variables with distribution $P(M_{i}=1)=p,P(M_{i}=-1)=1-p,p\in(0.5,1)$. Then the position of agent after n steps can be represented as:

|  | $S_{n}=\sum_{i=1}^{n}M_{i},S_{0}=0$ |  | (3) |

However, because of the asymmetry of random variables $M_{i}$, $\{S_{n},n\geq 0\}$ does not pertain to the martingale $w.r.t.$ $\{M_{n},n\geq 1\}$. It’s observed that $\mathbb{E}(M_{i})=2p-1,i\geq 1$. Then, we can define:

|  | $Y_{n}=\sum_{i=1}^{n}\left(M_{i}-(2p-1)\right),Y_{0}=0$ |  | (4) |

It’s easy to prove that

|  | $\mathbb{E}&#124;Y_{n}&#124;=\sum_{i=1}^{n}E&#124;M_{i}&#124;-n(2p-1)=2n-2np<\infty$ |  | (5) |

|  | $\mathbb{E}(Y_{n+1}&#124;M_{0},M_{1},...M_{n})=Y_{n}+\mathbb{E}(M_{n+1})-(2p-1)=Y_{n}$ |  | (6) |

So, according to the definition, $\{Y_{n},n\geq 0\}$ is a martingale $w.r.t.$ $\{M_{n},n\geq 1\}$

Let $T_{0\rightarrow N}=min\{n:S_{0}=0,S_{n}=N\}=min\{n:Y_{0}=0,Y_{n}=N-n*(2p-1)\}$. It’s clear that $T_{0\rightarrow N}$ is a stopping time $w.r.t.$ $\{M_{n},n\geq 1\}$.

It’s easy to prove that

|  | $\mathbb{E}\left(&#124;Y_{n+1}-Y_{n}&#124;&#124;M_{0},M_{1},...M_{n}\right)=\mathbb{E}\left(&#124;M% _{n+1}&#124;\right)-(2p-1)=2-2p<2$ |  | (7) |

We can assume that $\mathbb{E}(T_{0\rightarrow N})<\infty$. Then, according to the Optional Stopping Theorem (Durrett, [2019](https://arxiv.org/html/2410.02511v1#bib.bib37)), we can get

|  | $\mathbb{E}(Y_{T_{0\rightarrow N}})=N-\mathbb{E}(T_{0\rightarrow N})*(2p-1)=% \mathbb{E}(Y_{0})=0$ |  | (8) |

Then

|  | $\mathbb{E}(T_{0\rightarrow N})=\frac{N}{2p-1}$ |  | (9) |

The assumption $\mathbb{E}(T_{0\rightarrow N})<\infty$ is thereby validated. Consequently, the expected first hitting time within the default setting is $\mathbb{E}(T_{0\rightarrow N})=\frac{N}{2p-1}$, a conclusion also articulated in Theorem 4.8.9 of Durrett ([2019](https://arxiv.org/html/2410.02511v1#bib.bib37)).

We can introduce the task-relevant information that the agent must first reach key states: $\kappa=1,...,N-1$ before progressing to $x=N$. It is presupposed that every time the agent achieves at $x=\kappa$, the policy where $x<\kappa$ is updated to a deterministic rightward movement, i.e., $P(M_{x}=1)=1,x<\kappa$, thereby emulating the update process in Reinforcement Learning.

The expected first hitting time from $x=0$ to $x=1$ is $\mathbb{E}(T_{0\rightarrow 1})=\frac{1}{2p-1}$. After reaching $x=1$, the expected first hitting time from $x=1$ to $x=2$ can be calculated as:

|  | $\mathbb{E}(T^{prior}_{1\rightarrow 2})=p*\sum^{\infty}_{n=0}(2n+1)(1-p)=\frac{% 2}{p}-1$ |  | (10) |

Similarly, we can easily prove that

|  | $\mathbb{E}(T^{prior}_{1\rightarrow N})=(N-1)*\left(\frac{2}{p}-1\right)$ |  | (11) |

Consequently, the expected first hitting time after the integration of priors becomes $\mathbb{E}(T^{prior}_{0\rightarrow N})=\mathbb{E}(T_{0\rightarrow 1})+\mathbb{% E}(T^{prior}_{1\rightarrow N})=\frac{1}{2p-1}+(N-1)*\left(\frac{2}{p}-1\right)$.

The total advantage resulting from the integration of appropriate priors is expressed as $\mathbb{E}(T_{0\rightarrow N})-\mathbb{E}(T^{prior}_{0\rightarrow N})=(N-1)*(% \frac{1}{2p-1}-\frac{2}{p}+1)>0,p\in(0.5,1),N\in\mathbb{N}^{+},N>1$

∎

## Appendix D LLM Prompts and Responses

Here are the example prompt and response in our work. Please reference the code for further details. Notably, we adopt the chain-of-thought technique from Wei et al. ([2022](https://arxiv.org/html/2410.02511v1#bib.bib174)).

<svg class="ltx_picture" height="539.53" id="A4.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,539.53) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 521.33)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SMAC Prompt and Response Example</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="489.83" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM: (Task_Description) We are playing StarCraft II micro scenario, tring to control our agents to defeat all of the enemy units. (State_Form) In each step, the current state is represented as a 1-dimensional list: $[nf\_al]*n\_agents+[nf\_en]*n\_enemies+[last\_actions]$. $nf\_al$ denotes the unit state for each agent with attributes $[health\_rate,weapon\_cooldown\_rate,relative\_x\_to\_map\_center,\\ relative\_y\_to\_map\_center,shield\_rate$ (1 dimension if a_race is P else 0 dimension), $unit\_type\_bits$ (the dimension is defined in the map config)$]$. $nf\_en$ represents the unit state for each enemy with attributes $[health\_rate,relative\_x\_to\_map\_center,relative\_y\_to\_map\_center,\\ shield\_rate$ (1 dimension if b_race in map config is P else 0 dimension), $unit\_type\_bits$ (the dimension is defined in the map config)$]$. The $last\_actions$ component does not require consideration. (Role_Instruction)(Template) Your role is to give several critical key states in the task which we should try to reach and generate the corresponding discriminator function for each key state which can discriminate if the input state has reached the key state. Note: 1\. Don’t use the information you are not told. 2\. The code should be as generic as possible. 3\. The discriminator functions for different key states should be independent. 4\. Your answer should be complete and not omitted. Please think step by step and adhere to the following JSON format (just replace the () with your answer): {<svg class="ltx_picture" height="904.82" id="A4.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,904.82) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="877.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Thought: (Please fully understand the task. Think step by step and give your thought about the concrete state here), Key states: { init: all agents and enemies are alive, key_state_1: (description), key_state_2: (description), …, success: all enemies are dead and at least one agent is alive }, Discriminators: { key_state_1: (a python function with the form of def iskeystate1(state): … return 0 or 1), key_state_2: (a python function with the form of def iskeystate2(state): … return 0 or 1), … } Subspace_states: { key_state_1: (extract subspace state used in corresponding discriminator functions, such as [1] or [2,5]), key_state_2: (extract subspace state used in corresponding discriminator functions, such as [1] or [2,5]), … } } USER: Task is 2m_vs_1z. The map config is: { n_agents: 2, n_enemies: 1, limit: 150, a_race: T, b_race: P, unit_type_bits: 0, map_type: marines }. Give more than 2 key states and the corresponding discriminator functions. ASSISTANT: (LLM response) { Thought: Since a_race is T, there is no shield_rate in nf_al; b_race is P, so there is 1-dim shield_rate in nf_en; the dim of unit_type_bits is 0\. The dimension of nf_al = 4+0+0 = 4, the dimension of nf_en = 3+1+0 = 4\. The concrete state form is $[$agent_0_health_rate, agent_0_weapon_cooldown_rate, agent_0_relative_x_to_map_center, agent_0_relative_y_to_map_center, agent_1_health_rate, agent_1_weapon_cooldown_rate, agent_1_relative_x_to_map_center, agent_1_relative_y_to_map_center, enemy_0_health_rate, enemy_0_relative_x_to_map_center, enemy_0_relative_y_to_map_center, enemy_0_shield_rate, last_actions$]$. Key states should focus on the health and shield status of the enemy and the health of our agents., Key_states: { init:all agents and enemies are alive, key_state_1: enemy_0’s shield is significantly damaged, key_state_2: enemy_0 is significantly damaged, key_state_3: enemy_0 is about to die, success:all enemies are dead and at least one agent is alive }, Discriminators: { key_state_1: def iskeystate1(state): return 1 if state[11] <0.6 else 0, key_state_2: def iskeystate2(state): return 1 if state[8] <0.6 else 0, key_state_3: def iskeystate3(state): return 1 if state[8] <0.05 else 0, }<svg class="ltx_picture" height="257.25" id="A4.p4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,257.25) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="229.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Subspace_states: { key_state_1: [11], key_state_2: [8], key_state_3: [8], } } USER: (LLM Rethinking) Please carefully check your response to make sure it meets the task requirements and notes without including unnecessary details. Also, confirm that the discriminator functions do not use any undefined variables. ASSISTANT: (LLM response) …… (We have omitted the intermediate LLM outputs to maintain conciseness, retaining only the initial generation, as the differences between the initial and rechecked generations are minimal in the absence of errors.)<svg class="ltx_picture" height="561.67" id="A4.p5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,561.67) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 543.46)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">MPE Prompt and Response Example</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="511.97" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">USER: (Task_Description) Two agents are in a 30x30 room with the coordinate of the point in the upper left corner is (0,0) and the point in the lower right corner is (29,29), separated by a wall with a door. The door can not be directly open. There is a hidden switch in each room (The switch coordinates are unknown and differ from the coordinate of the door.). The door will be open only when one of the switch is occupied by an agent otherwise close. Both agents begin in the left half of the room(agent0.x<15 and agent1.x<15). The task is considered successfully completed as long as both agents are located in the right half-room(agent0.x>15 and agent1.x>15). (State_Form) In each step we can get current state. The state is a 1-dim list: (agent0.x, agent0.y, agent1.x, agent1.y,door.open). (Role_Instruction)(Template) Your role is to give several critical key states in the task which we should try to reach and generate the corresponding discriminator function for each key state which can discriminate if the input state has reached the key state. Note: 1\. Don’t use the information you are not told. 2\. The code should be as generic as possible. 3\. The discriminator functions for different key states should be independent. 4\. Try not to put too much distance between the key states. Please think step by step and adhere to the following JSON format (just replace the () with your answer): { Thought: (Please fully understand the task. Think step by step and give your thought about the concrete state here), Key states: { init: init state, key_state_1: (description), key_state_2: (description),<svg class="ltx_picture" height="841.17" id="A4.p6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,841.17) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="813.62" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">…, success: success state }, Discriminators: { key_state_1: (a python function with the form of def iskeystate1(state): … return 0 or 1), key_state_2: (a python function with the form of def iskeystate2(state): … return 0 or 1), … }, Subspace_states: { key_state_1: (extract subspace state used in corresponding discriminator functions, such as [1] or [2,5]), key_state_2: (extract subspace state used in corresponding discriminator functions, such as [1] or [2,5]), … } } ASSISTANT: (LLM response) { Thought: To complete the task, we need to find the hidden switch in one of the rooms to open the door, then move both agents through the door to the right half of the room. We should establish key states that represent the discovery of the switch, the opening of the door, and the successful transition of both agents to the right half of the room. We’ll assume that agent0 is responsible for finding the switch and agent1 is responsible for moving through the door once it’s open., Key_states: { init:Initial state where both agents are in the left half of the room., key_state_1:Agent0 has found the switch and is standing on it, opening the door, key_state_2:Agent1 has moved through the open door to the right half of the room while agent0 is still on the switch, key_state_3:Agent0 has joined agent1 in the right half of the room }, Discriminators: { key_state_1:def iskeystate1(state): return 1 if state[4] == 1 and state[0] >15 else 0, key_state_2:def iskeystate2(state): return 1 if state[4] == 1 and state[2] >15 and state[0] <15 else 0, key_state_3:def iskeystate3(state): return 1 if state[0] >15 and state[2] >15 else 0 },     Subspace_states: { key_state_1: [4,0], key_state_2: [0,2,4], key_state_3: [0,2], } } USER: (LLM Rethinking) Please carefully check your response to make sure it meets the task requirements and notes without including unnecessary details. Also, confirm that the discriminator functions do not use any undefined variables.<svg class="ltx_picture" height="93.98" id="A4.p7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,93.98) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="66.42" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">ASSISTANT: (LLM response) …… (We have omitted the intermediate LLM outputs to maintain conciseness, retaining only the initial generation, as the differences between the initial and rechecked generations are minimal in the absence of errors.)</foreignobject></g></g></svg>![Refer to caption](img/c0ee1a5daa49eb9c86b253b1132b7e65.png)

Figure 9: The number of LLM-generated key state discriminator functions.

Notably, the number of key states is primarily determined by LLM. For each task, we only prompt LLM to prevent it from generating too few functions according to the complexity of the environment. Specifically, as detailed in Appendix D, we instruct LLM to generate several critical key states for MPE and more than 2*n_enemies critical key states for SMAC.

As shown in Fig. [9](https://arxiv.org/html/2410.02511v1#A4.F9 "Figure 9 ‣ Appendix D LLM Prompts and Responses ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), we summarize the number of LLM-generated key state discriminator functions. It is notable that the number of discriminator functions increases with the difficulty of the task or the number of interactive objects in the environment, which aligns with intuition.

Additionally, we have omitted the intermediate LLM outputs to maintain conciseness in this section, retaining only the initial generation, as the differences between the initial and rechecked generations are minimal in the absence of errors.

## Appendix E Implementation Details

### E.1 LEMAE and Baselines

LEMAE: Our code is based on the widely-used code framework pymarl2 at [https://github.com/hijkzzz/pymarl2](https://github.com/hijkzzz/pymarl2). In this study, we have integrated our method with several base algorithms IPPO, QMIX, QPLEX, and VMIX. Throughout the integration process, we have refrained from modifying the algorithmic implementation and have maintained consistency in parameters, including batch size, learning rate, and loss coefficients, in alignment with the configurations of the base algorithms.

EITI and EDTI (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)): We compare our method with EITI and EDTI on MPE tasks proposed in Liu et al. ([2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). We use the experiment results reported in Liu et al. ([2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)), which found that these algorithms perform poor because a long rollout (512 steps × 32 processes) between model updates is used.

CMAE (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)): We compare our method with CMAE on MPE and SMAC tasks. On MPE tasks, the results of CMAE are reproduced using the publicly available code released by the authors at [https://github.com/IouJenLiu/CMAE](https://github.com/IouJenLiu/CMAE). As CMAE lacks an implementation for SMAC, we use the results reported in the original paper.

MAVEN (Mahajan et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib100)): We use the code at [https://github.com/starry-sky6688/MARL-Algorithms](https://github.com/starry-sky6688/MARL-Algorithms), which contains pytorch implementations of various MARL algorithms on SMAC, like the choice in LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)).

RODE (Wang et al., [2020d](https://arxiv.org/html/2410.02511v1#bib.bib172)) and MASER (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)): We utilized the publicly available code provided by the authors, accessible at [https://github.com/Jiwonjeon9603/MASER](https://github.com/Jiwonjeon9603/MASER) and [https://github.com/TonghanWang/RODE](https://github.com/TonghanWang/RODE), respectively. Default configurations were employed, and their suboptimal performance is also documented in LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)).

LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)): We employed the publicly accessible code provided by the authors, which can be accessed at [https://github.com/liuboyin/LAIES](https://github.com/liuboyin/LAIES). When conducting experiments on SMAC, we adhered to the default configurations and external states. Notably, the original LAIES paper evaluation did not include assessments on the MPE. Consequently, we integrated the MPE environment into the LAIES codebase, designating the external states to represent the door status or the position of the box.

ELLM (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)): Since the tasks in this work have clearly defined goals, we minimize LLM inference costs by following the ELLM methodology but adapting its goal generation to occur only once at the start of the training. Consistent with the hyperparameters in the official codebase [https://github.com/yuqingd/ellm](https://github.com/yuqingd/ellm), we set the similarity threshold to 0.99, rewarding only when the goal is achieved. We rely on LLM-generated functions to verify goal achievement, which we found to be more effective than directly using semantic similarity-based rewards.

For all algorithms, we ensure the same environmental settings, including observation space, environment reward function, and so on.

### E.2 Comparison with LLM Reward Design

We conduct additional experiments comparing LEMAE with a baseline called Eureka-si, which can be seen as a single-iteration variant of Eureka (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)), where LLM designs rewards directly. For fairness, we does not adopt evolutionary optimization in Eureka and use LLM to generate reward functions with the same role instructions as in Eureka, while maintaining designs like Self-Check as in LEMAE. As shown in Fig. [3](https://arxiv.org/html/2410.02511v1#S5.F3 "Figure 3 ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), Eureka-si is comparable to LEMAE in simple tasks like Push-Box but fails in challenging tasks with characteristics like partial observability, such as Pass, where hidden switches make it difficult to design effective reward functions. In contrast, LEMAE consistently demonstrates impressive performance. Notably, comparing LEMAE with Eureka directly would be unfair since Eureka’s evolutionary search requires multiple training iterations and candidates, leading to significantly more sampling and training than LEMAE. Overall, LEMAE’s advantage over RL algorithms lies in incorporating prior knowledge from the LLM, and its advantage over other LLM-based methods is due to our designs for better LLM incorporation, such as utilizing discrimination, SHIR, and KSMT.

### E.3 Connection and Comparison with HER

The proposed Key State-Guided Exploration is similar to Hindsight Experience Replay (HER) (Andrychowicz et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib8)) in form, where key states and subgoals are certain states from sampled trajectories. However, unlike HER, which samples goals from memory using random or heuristic strategies and often struggles with shaped rewards, our method incorporates LLM priors for more targeted goal selection (key states localization). Additionally, the proposed KSMT and SHIR facilitate organized exploration and enhanced reward guidance.

We conduct additional experiments to further confirm the advantages of our method. We evaluate HER with IPPO as the backbone in MPE. We use the future strategy for goal selection, as proposed in the HER paper, and employ a reward function based on the Manhattan Distance, which we find to be the best match. However, as depicted in Fig. [3](https://arxiv.org/html/2410.02511v1#S5.F3 "Figure 3 ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), HER does not perform well on both MPE tasks. This outcome suggests that the random sampling strategy for goals may not be sufficient, underscoring the importance of incorporating LLM priors for efficient exploration as we proposed.

### E.4 Tasks

#### E.4.1 Multiple-Particle Environment (MPE)

The Multiple-Particle Environment serves as a widely-adopted benchmark for multi-agent scenarios. In this work, we employ tasks specifically crafted for evaluating multi-agent exploration, proposed by Wang et al. ([2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)). The implementation utilized in this study is based on the work by Liu et al. ([2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). In this section, we provide details of the four sparse-reward tasks we adopted.

$\bullet$ Pass: In the Pass task, depicted in Fig. [10](https://arxiv.org/html/2410.02511v1#A5.F10 "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a, two agents are positioned in a room of 30 x 30 grid. The room is divided into two halves by a wall featuring a door. Each half-room contains an invisible switch, the details of which are not contained in the state or prompt for LLM. The door permits passage only when one of the switches is occupied by an agent. Initially situated within the left half-room, both agents must cooperate to transfer to the right half-room. The external reward function is denoted as $r_{E}=I(two\ agents\ are\ in\ the\ right\ room)$, where $I$ represents the indicator function.

$\bullet$ Secret-Room: Secret-Room is an extension task of Pass. As illustrated in Fig. [10](https://arxiv.org/html/2410.02511v1#A5.F10 "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")b, the configuration comprises one sizable room on the left and three smaller rooms on the right, interconnected by three doors. Within each room, there is an invisible switch; notably, the switch in the left room has the capability to control all three doors, whereas each right room’s switch exclusively controls its respective door. The grid size is 25 x 25\. Two agents are initialized within the left room and are required to collaborate in order to transition to the real target room, which is the right room 2\. The external reward function is denoted as $r_{E}=I(two\ agents\ are\ in\ the\ right\ room\ 2)$, where $I$ represents the indicator function.

$\bullet$ Push-Box: As depicted in Fig. [10](https://arxiv.org/html/2410.02511v1#A5.F10 "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")(c), two agents and a box are initially positioned within a 15 x 15 grid. To successfully move the box, both agents must simultaneously exert force in the same direction. The task is deemed accomplished when the box is successfully pushed to the wall. The external reward function is denoted as $r_{E}=I(the\ box\ is\ pushed\ to\ the\ wall)$, where $I$ represents the indicator function.

$\bullet$ Large-Pass: Large-Pass is a direct extension task of Pass by enlarging the grid dimensions to 50 x 50, which makes it more challenging. The external reward function aligns with that of the Pass task.

The details of these tasks, including observation space and action space, are listed in Table [3](https://arxiv.org/html/2410.02511v1#A5.T3 "Table 3 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").

Table 3: Details of MPE tasks

| MPE tasks | n_agents | observation space | state space | action space |
| Pass | 2 | 5 | 5 | 4 |
| Secret-Room | 2 | 5 | 5 | 4 |
| Push-Box | 2 | 6 | 6 | 4 |
| Large-Pass | 2 | 5 | 5 | 4 |

![Refer to caption](img/2eb34d14fbbe7e51ec82dd1469c43743.png)

Figure 10: MPE Tasks.

#### E.4.2 StarCraftII Multi-Agent Challenge (SMAC)

StarCraftII Multi-Agent Challenge (SMAC) (Samvelyan et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib132)) is a widely-used benchmark in the realm of cooperative multi-agent reinforcement learning research (Rashid et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib128); Shao et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib138); Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Shao et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib139)). Derived from the renowned real-time strategy game StarCraft II, SMAC concentrates specifically on decentralized micromanagement scenarios rather than the full game. Typically, the tasks within SMAC adopt a dense-reward framework, wherein agents receive dense rewards for damage received, attacking and eliminating enemies. To promote the need for exploration, we adopt fully sparse-reward versions of tasks in SMAC where agents are solely rewarded upon the successful elimination of all enemies. The external reward function is denoted as $r_{E}=I(all\ enemies\ are\ eliminated)$, where $I$ represents the indicator function. Notably, this sparse-reward setting differs from the sparse SMAC, which can be called semi-sparse SMAC, used in some previous studies (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56); Jo et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib61)), where agents are rewarded when one or all enemies die or when one ally dies. In addition, to validate the versatility of LEMAE across diverse scenarios, we conducted experiments on six maps with different difficulty and diverse agent numbers, as illustrated in Table [4](https://arxiv.org/html/2410.02511v1#A5.T4 "Table 4 ‣ E.4.2 StarCraftII Multi-Agent Challenge (SMAC) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") and Table [5](https://arxiv.org/html/2410.02511v1#A5.T5 "Table 5 ‣ E.4.2 StarCraftII Multi-Agent Challenge (SMAC) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"). We use the version of SC2.4.10. Please refer to the official document²²2[https://github.com/oxwhirl/smac/blob/master/docs/smac.md](https://github.com/oxwhirl/smac/blob/master/docs/smac.md) for more details.

Table 4: SMAC tasks.

 | Task | Ally Units | Enemy Units | Type | Difficulty |
| 1c3s5z | 1 Colossi, 3 Stalkers, 5 Zealots | 1 Colossi, 3 Stalkers, 5 Zealots | heterogeneous & symmetric | Easy |
| 2m_vs_1z | 2 Marines | 1 Zealot | micro-trick: alternating fire | Easy |
| 3s_vs_5z | 3 Stalkers | 5 Zealots | micro-trick: kiting | Hard |
| 5m_vs_6m | 5 Marines | 6 Marines | homogeneous & asymmetric | Hard |
| 3s5z_vs_3s6z | 3 Stalkers, 5 Zealots | 3 Stalkers, 6 Zealots | heterogeneous & asymmetric | Super-Hard |
| MMM2 | 1 Medivac, 2 Marauders, 7 Marines | 1 Medivac, 3 Marauders, 8 Marines | heterogeneous & asymmetric | Super-Hard | 

Table 5: Details of SMAC tasks

| SMAC tasks | n_agents | n_enemies | observation space | state space | action space |
| 2m_vs_1z | 2 | 1 | 16 | 26 | 7 |
| 1c3s5z | 9 | 9 | 162 | 270 | 15 |
| 3s_vs_5z | 3 | 5 | 48 | 68 | 11 |
| 5m_vs_6m | 5 | 6 | 55 | 98 | 12 |
| 3s5z_vs_3s6z | 8 | 9 | 136 | 230 | 15 |
| MMM2 | 10 | 12 | 176 | 322 | 18 |

#### E.4.3 A Brand New Task: River

To exclude the probability that LEMAE’s success relies on LLM’s familiarity with the chosen tasks, we’ve designed a brand new task, termed River, which LLM has never encountered before. The task is detailed as follows:

The River task is adapted from the Multiple-Particle Environment and its map is illustrated in Fig. [8](https://arxiv.org/html/2410.02511v1#S5.F8 "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a. Two agents, Alice and Bob, are placed in a 30 x 30 grid field intersected by two rivers running vertically and horizontally. A mountain in the bottom-left corner obstructs the passage. Alice and Bob start randomly in the top-left part of the field and need to move to the bottom-right part. However, Alice is afraid of water and cannot cross the river unless Bob stays in the river to act as a bridge for her.

The observation space is discrete with four dimensions, representing the positions of two agents, i.e., $o=[x_{1},y_{1},x_{2},y_{2}]$. The action space is also discrete, allowing movement in four directions. Agents receive a positive reward only when both agents reach the bottom-right corner of the field.

### E.5 Hyperparameters

In LEMAE, we introduce three important hyperparameters: extrinsic reward scaling rate $\alpha$, intrinsic reward scaling rate $\beta$, and high randomness epsilon $\epsilon_{h}$. Notably, the low randomness epsilon $\epsilon_{l}$ is the hyperparameter in the base algorithms, such as 0.05 for QMIX and 0.0 for IPPO.

For MPE, we adopt $\{\alpha=10,\beta=0.1,\epsilon_{h}=1\}$ on Pass, Secret-Room, and Large-Pass and use $\{\alpha=10,\beta=0.05,\epsilon_{h}=0.2\}$ on Push-Box.

For SMAC, we adopt $\{\alpha=50,\beta=1,\epsilon_{h}=0.5\}$ on MMM2 and 1c3s5z, $\{\alpha=10,\beta=1,\epsilon_{h}=0.5\}$ on 3s_vs_5z and 2m_vs_1z, $\{\alpha=1,\beta=1,\epsilon_{h}=0.5\}$ on 5m_vs_6m and 3s5z_vs_3s6z.

### E.6 Resources

We use a server with 8*NVIDIA RTX 3090 GPUs, and 2*AMD 7H12 CPUs to run all the experiments. Without specifying, each setting is repeated for 5 seeds. For one seed in SC2, the running time ranges from approximately 50 minutes to 12 hours. For MPE, the running time varies from around 3 to 7 hours. The input for each LLM (GPT-4-1106-preview) inference comprises approximately 600-4000 tokens (0.006-0.04 dollars), yielding an output of about 300-1600 tokens (0.009-0.048 dollars).

## Appendix F Additional Experimental Results

### F.1 More Complicated Benchmark: SMACv2

We have evaluated LEMAE on three typical tasks, protoss_5_vs_5, terran_5_vs_5, and zerg_5_vs_5, in SMACv2 (Ellis et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib39)) to demonstrate its effectiveness under stochastic dynamics settings. We utilized the typical hyperparameters for both LEMAE and LAIES as used in SMAC. As shown in Fig. [11](https://arxiv.org/html/2410.02511v1#A6.F11 "Figure 11 ‣ F.1 More Complicated Benchmark: SMACv2 ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), LEMAE achieves outstanding performance, confirming its applicability to such settings. This result further demonstrates LEMAE’s potential for real-world scenarios with complexity and stochasticity.

![Refer to caption](img/4ed524eb10c8f9104f8544376acd10c9.png)

Figure 11: Evaluating LEMAE against baseline methods across three maps in SMACv2, which features greater randomness, using the evaluation metric of test win rate. QMIX-DR is QMIX with dense rewards in the original SMACv2\. This result further demonstrates LEMAE’s potential for real-world scenarios with complexity and stochasticity.

### F.2 Extending LEMAE Beyond Symbolic Tasks

This work primarily focuses on tasks with symbolic state spaces, where states are represented as symbolic arrays describing the agent and environment. As discussed in Appendix [B.3](https://arxiv.org/html/2410.02511v1#A2.SS3 "B.3 Future Works ‣ Appendix B Further Discussions ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), to extend LEMAE from symbolic tasks to vision-based tasks, we can exchange the LLM for a multi-modal LM in LEMAE for key state localization. To confirm the applicability of LEMAE to vision-based tasks, we conduct a demonstrative experiment: We extend the task Pass to a vision-based task Visual-Pass, as illustrated in Fig. [12](https://arxiv.org/html/2410.02511v1#A6.F12 "Figure 12 ‣ F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a. We prompt a LLM to define key states with the same task description and role instruction as proposed in Sec. [4.2](https://arxiv.org/html/2410.02511v1#S4.SS2 "4.2 Key States Localization with LLM ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") and use the LLM-generated definition as the prompt for a Vision Language Model (GPT-4o). Then, it is prompted to discriminate key states in the randomly sampled states. GPT-4o achieves a 98% accuracy rate in discriminating key states among the 50 sampled image states. This confirms that with a proper extension of the LLM, LEMAE can eliminate dependence on state semantics and be applied to other tasks such as visual-input.

![Refer to caption](img/ec88b3c895c14cd73a87226e9d64e31f.png)

Figure 12: (a) An illustration of the vision-based task Visual-Pass. (b) The prompt for the Vision Language Model (VLM), which includes the task description, role instruction, image state and key states definition provided by another LLM. The VLM is tasked with determining whether the given image state corresponds to a key state.

### F.3 Ablation Studies on Mixed-Randomness Exploration

As demonstrated in Fig [13](https://arxiv.org/html/2410.02511v1#A6.F13 "Figure 13 ‣ F.3 Ablation Studies on Mixed-Randomness Exploration ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), we conduct an ablation study on mixed-randomness exploration within the 3s_vs_5z map. Results indicate that LEMAE exhibits insensitivity to the parameter $\epsilon_{h}$, provided that the level of randomness remains moderate, as opposed to being excessively extreme (0.1 or 0.9). Besides, the effectiveness of our design is highlighted through a comparison between LEMAE and its variants, namely, only leaf node and LEMAE w/o KSMTE.

![Refer to caption](img/88900913d926314e9d0f090f7a669026.png)

Figure 13: Ablation study on mixed-randomness exploration. The default hyperparameter used in LEMAE is $\epsilon_{h}=0.5$. Robustness evaluation included variations in $\epsilon_{h}=[0.1,0.3,0.7,0.9]$. The only leaf node denotes simply using $\epsilon_{l}$ for the non-leaf node and $\epsilon_{h}$ for the leaf node. The LEMAE w/o KSMTE denotes LEMAE without mixed-randomness exploration.

### F.4 Experiments for Single-Agent setups

Indeed, we propose LEMAE as a general approach for LLM-empowered efficient exploration in reinforcement learning, applicable to both single-agent and multi-agent settings. We underscore the evaluation of its performance in multi-agent settings due to its inherent complexity.

As the proposed method can seamlessly extend to single-agent scenarios, we introduce a single-agent variant of MPE and assess PPO (Schulman et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib136)) and PPO-based LEMAE across four tasks. We run each algorithm using three random seeds with 300k environment steps, using the evaluation metric of the test win rate. The following table shows that LEMAE can facilitate efficient exploration in single-agent scenarios.

Table 6: Final test win rate of LEMAE and PPO on single-agent variant of MPE tasks.

| Single MPE | PPO | LEMAE |
| Single Pass | 0.00$\pm$0.00 | 1.00$\pm$0.00 |
| Single Secret-Room | 0.00$\pm$0.00 | 0.98$\pm$0.01 |
| Single Large-Pass | 0.00$\pm$0.00 | 0.99$\pm$0.01 |
| Single Push-Box | 0.00$\pm$0.00 | 0.96$\pm$0.08 |

### F.5 Discussion on the KSMT

Using KSMT could pose a limitation due to potential memory costs in certain scenarios. However, this has not been a significant issue in our experiments, as the key states are relatively few, primarily focusing on the most critical ones, with a natural sequential relationship typically existing between them. Notably, LEMAE is also compatible with other memory structures, such as Directed Acyclic Graphs (DAGs), which could be an interesting direction for future exploration.

To demonstrate the effectiveness of LEMAE with other memory structures, in scenarios where task completion follows a linear pattern (e.g., $Init\rightarrow A\rightarrow B\rightarrow Success$), we employ a more efficient strategy by using a KSMT variant with a single branch representing the sequential order of key states. Specifically, we systematically assign a priority value to each key state, continuously updating it based on its occurrence order within the sequence of attained key states. The determination of the ranking of key states within the one-branch KSMT relies on this established priority.

As illustrated in Fig. [14](https://arxiv.org/html/2410.02511v1#A6.F14 "Figure 14 ‣ F.5 Discussion on the KSMT ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), an ablation study is conducted to compare the performance between raw KSMT and the one-branch KSMT variant across six maps in SMAC. The results demonstrate the increased necessity of employing the one-branch KSMT variant for tasks involving a larger number of agents and greater complexity, such as 5m_vs_6m, 3s5z_vs_3s6z, and MMM2. Consequently, we have adopted the one-branch KSMT approach for these specific SMAC tasks: 5m_vs_6m, 3s5z_vs_3s6z, and MMM2.

![Refer to caption](img/f8e86845c849b0d32b952cf860c5c98f.png)

Figure 14: Ablation study conducted to compare the performance between raw KSMT and the one-branch KSMT variant across six maps in SMAC.

#### F.5.1 Working with dense reward settings

We also evaluate LEMAE in tasks with dense rewards in SMAC, denoted as LEMAE-DR. As shown in Fig. [15](https://arxiv.org/html/2410.02511v1#A6.F15 "Figure 15 ‣ F.5.1 Working with dense reward settings ‣ F.5 Discussion on the KSMT ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), the results confirm that LEMAE-DR facilitates efficient exploration in both dense and sparse reward settings, highlighting the main contribution of our method. Additionally, LEMAE-DR achieves better convergence than LEMAE due to the guidance provided by dense rewards.

![Refer to caption](img/52adb1012fefd986f56618e9d2f565cc.png)

Figure 15: Evaluating LEMAE with dense rewards across three maps in SMAC using the evaluation metric of test win rate. LEMAE-DR is LEMAE with dense rewards in the original SMAC, which effectively ensures efficient exploration and achieves better convergence.

### F.6 Additional Results

Table 7: Final test win rate of LEMAE and comparable baseline (CMAE) on MPE tasks.

| MPE | LEMAE (Ours) | CMAE |
| Pass | 1.00$\pm$0.00 | 0.75$\pm$0.43 |
| Secret-Room | 1.00$\pm$0.00 | 0.80$\pm$0.40 |
| Push-Box | 1.00$\pm$0.00 | 0.80$\pm$0.40 |
| Large-Pass | 1.00$\pm$0.00 | 0.00$\pm$0.00 |

Table 8: Final test win rate of LEMAE and comparable baseline (LAIES) on SMAC tasks. QMIX-DR denotes training QMIX with dense reward.

 | SMAC | LEMAE (Ours) | LAIES | QMIX-DR |
| 1c3s5z | 0.98$\pm$0.02 | 0.89$\pm$0.09 | 0.99$\pm$0.01 |
| 2m_vs_1z | 1.00$\pm$0.01 | 0.73$\pm$0.24 | 1.00$\pm$0.01 |
| 3s_vs_5z | 0.83$\pm$0.07 | 0.10$\pm$0.12 | 0.66$\pm$0.16 |
| 5m_vs_6m | 0.74$\pm$0.08 | 0.68$\pm$0.10 | 0.78$\pm$0.08 |
| 3s5z_vs_3s6z | 0.73$\pm$0.14 | 0.45$\pm$0.35 | 0.73$\pm$0.07 |
| MMM2 | 0.89$\pm$0.08 | 0.62$\pm$0.25 | 0.90$\pm$0.05 | 

Table 9: Comparing LEMAE with SOTA baseline CMAE across four maps in MPE using the metric of the number of exploration steps (in thousand) taken to find the success state

 | MPE | LEMAE (Ours) | CMAE | Acceleration rate |
| Pass | 153.1$\pm$20.7 | 2114.8$\pm$157.4 | 13.8 |
| Secret-Room | 316.6$\pm$134.6 | 1448.5$\pm$467.2 | 4.6 |
| Push-Box | 159.0$\pm$42.5 | 972.3$\pm$887.3 | 6.1 |
| Large-Pass | 446.9$\pm$256 | >3000 | >6.7 | ![Refer to caption](img/f2f30baedd4755cfc0d34671e708e977.png)

Figure 16: The training curve while evaluating the efficacy of combining our method with various algorithms.

In this section, we provide some additional experimental results.

As demonstrated in Table [7](https://arxiv.org/html/2410.02511v1#A6.T7 "Table 7 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") and Table [8](https://arxiv.org/html/2410.02511v1#A6.T8 "Table 8 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), we augment the final test win rate of our proposed method, LEMAE, with comparable baseline algorithms in MPE and SMAC tasks. This augmentation serves to elucidate the superior performance of our method. It is pertinent to note that baseline algorithms, the performance of which has been demonstrated to be poor in the training curves, are omitted from the tables for conciseness.

As demonstrated in Table [9](https://arxiv.org/html/2410.02511v1#A6.T9 "Table 9 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), we compare LEMAE with SOTA baseline CMAE using the metric of the number of exploration steps taken to find the success state. The results indicate a significant exploration acceleration rate, up to 10x, underscoring LEMAE’s superior efficiency.

Moreover, as illustrated in Fig. [16](https://arxiv.org/html/2410.02511v1#A6.F16 "Figure 16 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"), we supplement the training curve while evaluating the efficacy of combining our method with various algorithms, i.e., QPLEX and VMIX.</foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg>