- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:50:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CleanAgent: Automating Data Standardization with LLM-based Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CleanAgent: 用基于 LLM 的代理自动化数据标准化'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08291](https://ar5iv.labs.arxiv.org/html/2403.08291)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.08291](https://ar5iv.labs.arxiv.org/html/2403.08291)
- en: Danrui Qi, Jiannan Wang Simon Fraser University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Danrui Qi, Jiannan Wang 西门菲莎大学
- en: '{dqi, jnwang}@sfu.ca'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '{dqi, jnwang}@sfu.ca'
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Data standardization is a crucial part in data science life cycle. While tools
    like Pandas offer robust functionalities, their complexity and the manual effort
    required for customizing code to diverse column types pose significant challenges.
    Although large language models (LLMs) like ChatGPT have shown promise in automating
    this process through natural language understanding and code generation, it still
    demands expert-level programming knowledge and continuous interaction for prompt
    refinement. To solve these challenges, our key idea is to propose a Python library
    with declarative, unified APIs for standardizing column types, simplifying the
    LLM’s code generation with concise API calls. We first propose Dataprep.Clean
    which is written as a component of the Dataprep Library, offers a significant
    reduction in complexity by enabling the standardization of specific column types
    with a single line of code. Then we introduce the CleanAgent framework integrating
    Dataprep.Clean and LLM-based agents to automate the data standardization process.
    With CleanAgent , data scientists need only provide their requirements once, allowing
    for a hands-free, automatic standardization process. To demonstrate the practical
    utility of CleanAgent , it has been developed as a user-friendly web application,
    allowing VLDB attendees to interact with it using real-world datasets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化是数据科学生命周期中的关键部分。虽然像 Pandas 这样的工具提供了强大的功能，但它们的复杂性以及为不同列类型定制代码所需的手动工作提出了重大挑战。尽管像
    ChatGPT 这样的语言模型在通过自然语言理解和代码生成来自动化这一过程方面显示出前景，但仍然需要专家级的编程知识和持续的互动以精细调整提示。为了应对这些挑战，我们的关键想法是提出一个具有声明性、统一
    API 的 Python 库，用于标准化列类型，通过简洁的 API 调用简化 LLM 的代码生成。我们首先提出 Dataprep.Clean，它作为 Dataprep
    Library 的一个组件编写，通过一行代码实现对特定列类型的标准化，显著减少了复杂性。接着我们介绍了 CleanAgent 框架，将 Dataprep.Clean
    和基于 LLM 的代理集成，以自动化数据标准化过程。通过 CleanAgent，数据科学家只需一次性提供需求，从而实现无须干预的自动标准化过程。为了展示 CleanAgent
    的实际效用，它被开发为一个用户友好的 Web 应用程序，使 VLDB 参会者能够使用真实世界的数据集与之互动。
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 介绍
- en: Data standardization, which is pivotal in the realm of data science, aims to
    transform heterogeneous data formats within a single column into an unified data
    format. This crucial data preprocessing step is essential for enabling effective
    data integration, data analysis, and decision-making.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化在数据科学领域中至关重要，旨在将单列中的异构数据格式转化为统一的数据格式。这个关键的数据预处理步骤对于有效的数据集成、数据分析和决策制定至关重要。
- en: 'Example 1. We illustrate the data standardization task in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ CleanAgent: Automating Data Standardization with
    LLM-based Agents"). Given an input table $T$, data in the three cells of “Admission
    Date” column follows only one date format, i.e. the “MM/DD/YYYY” format.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 1. 我们在图 [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents") 中展示了数据标准化任务。给定一个输入表 $T$，其中“Admission
    Date”列的三个单元格数据都遵循唯一的日期格式，即“MM/DD/YYYY”格式。'
- en: Traditionally, data scientists heavily rely on libraries such as Pandas (McKinney
    et al., [2024](#bib.bib2)) for data standardization tasks. Even though Pandas
    is a powerful tool, achieving data standardization often requires writing hundreds
    or thousands of lines of code. The standardization process for single column involves
    identifying the column type, applying intricate methods such as regular expressions
    to each cell within this column for validation, and converting each cell into
    desired formats. Moreover, a table may contain multiple columns, each possibly
    of a different type, requiring bespoke standardization code for each column type.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数据科学家们通常依赖于如 Pandas（McKinney 等，[2024](#bib.bib2)）等库来执行数据标准化任务。尽管 Pandas
    是一个强大的工具，但实现数据标准化通常需要编写数百行或数千行代码。单列的标准化过程包括识别列类型、对该列中的每个单元格应用如正则表达式等复杂方法进行验证，并将每个单元格转换为所需格式。此外，一个表可能包含多个列，每个列可能有不同的类型，因此需要针对每种列类型编写定制的标准化代码。
- en: Recently, the emergence of large language models (LLMs), especially ChatGPT,
    has shown potential in revolutionizing this process. By leveraging their natural
    language understanding and code generation ability, these models could significantly
    aid data scientists by autonomously generating standardization code in response
    to conversational prompts. However, this method still necessitates detailed prompt
    crafting and often involves multi-turn dialogues, which limits efficiency and
    practicality.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的出现，特别是 ChatGPT，显示出在革命性改造这一过程中的潜力。通过利用它们的自然语言理解和代码生成能力，这些模型可以通过响应对话提示，自动生成标准化代码，从而显著帮助数据科学家。然而，这种方法仍然需要详细的提示构造，并且通常涉及多轮对话，这限制了效率和实用性。
- en: '![Refer to caption](img/ba8ea4cfa416e2872d34615fce97faa8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ba8ea4cfa416e2872d34615fce97faa8.png)'
- en: Figure 1\. An example of automatic data standardization process with CleanAgent.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. CleanAgent 自动数据标准化过程的示例。
- en: To overcome these limitations, our key idea is to introduce a Python library
    involving declarative and unified APIs specifically designed for standardizing
    different column types. This idea simplifies the LLM’s task to converting natural
    language (NL) instructions into succinct, declarative API calls. Such an approach
    streamlines the LLM’s code generation process, requiring just a few lines of code,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，我们的关键想法是引入一个 Python 库，涉及专门设计用于标准化不同列类型的声明性和统一 API。这个想法将 LLM 的任务简化为将自然语言（NL）指令转换为简洁的声明性
    API 调用。这种方法简化了 LLM 的代码生成过程，仅需几行代码，
- en: This above idea, however, introduces two primary challenges. The first challenge
    (C1) is the design of the declarative and unified APIs for data standardization,
    ensuring it can effectively reduce the intricacies involved in standardizing specific
    column types (ideally one line of code per column type). The second challenge
    (C2) centers on optimizing the interaction between data scientists and LLMs. Our
    goal is to minimize human involvement, ideally allowing data scientists to input
    their standardization requirements in one instance, thereby enabling an autonomous
    and hands-off data standardization process.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上述想法引入了两个主要挑战。第一个挑战（C1）是设计用于数据标准化的声明性和统一 API，确保它可以有效地减少标准化特定列类型所涉及的复杂性（理想情况下，每种列类型一行代码）。第二个挑战（C2）集中在优化数据科学家与
    LLM 之间的互动。我们的目标是最小化人工干预，理想情况下允许数据科学家一次性输入他们的标准化要求，从而实现自主和无需干预的数据标准化过程。
- en: To solve C1, we propose the type-specific Clean module in Dataprep Library¹¹1[https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep),
    named Dataprep.Clean²²2[https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean](https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean).
    By observing the common steps of data standardization for specific column types,
    we design unified APIs clean_type(df, column_name, target_format), where the type
    represents the desired standardization type, such as date, address and phone,
    etc. These unified APIs offer enhanced expressiveness compared to raw Pandas code,
    reducing the complexity of standardizing specific column types. With Dataprep.Clean,
    data scientists can standardize one column type with only one line of code.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决 C1，我们提出了 Dataprep Library¹¹1[https://github.com/sfu-db/dataprep](https://github.com/sfu-db/dataprep)
    中的特定类型 Clean 模块，命名为 Dataprep.Clean²²2[https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean](https://github.com/sfu-db/dataprep/tree/develop/dataprep/clean)。通过观察特定列类型的数据标准化的常见步骤，我们设计了统一的
    API clean_type(df, column_name, target_format)，其中类型表示所需的标准化类型，如日期、地址和电话等。这些统一的
    API 提供了比原始 Pandas 代码更强的表达能力，减少了标准化特定列类型的复杂性。使用 Dataprep.Clean，数据科学家可以仅用一行代码标准化一种列类型。
- en: 'To solve C2, we propose the CleanAgent framework which automates data standardization
    with Dataprep.Clean and LLM-based Agents  (Xi et al., [2023](#bib.bib4); Wu et al.,
    [2023](#bib.bib3)). Once users have entered their final goals, the LLM-based Agents
    can free their hands, autonomously generate thoughts and execute particular tasks.
    CleanAgent leverages both the capabilities of both Dataprep.Clean and LLM-based
    Agents. Data scientists only need to input the table that needs to be standardized
    and their requirements, CleanAgent will complete the data standardization process
    automatically with three steps: annotating the type of each column, generating
    concise Python code for standardization and executing the generated Python code.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决C2问题，我们提出了CleanAgent框架，该框架通过Dataprep.Clean和基于LLM的Agent自动化数据标准化（Xi等，[2023](#bib.bib4)；Wu等，[2023](#bib.bib3)）。一旦用户输入了他们的最终目标，基于LLM的Agent可以解放他们的双手，自动生成思路并执行特定任务。CleanAgent充分利用了Dataprep.Clean和基于LLM的Agent的能力。数据科学家只需输入需要标准化的表格及其要求，CleanAgent将通过三个步骤自动完成数据标准化过程：标注每一列的类型，生成用于标准化的简洁Python代码并执行生成的Python代码。
- en: Example 2. Continuing with Example 1\. Given an input table $T$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 示例2。继续示例1。给定一个输入表$T$。
- en: CleanAgent is built as a web application. We allow the VLDB attendees to choose
    sample data and communicate with CleanAgent for standardization. We provide the
    demonstration video which can be found on Youtube³³3[https://youtu.be/fSYXVM6qeqM](https://youtu.be/fSYXVM6qeqM).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CleanAgent作为一个Web应用程序构建。我们允许VLDB与会者选择样本数据并与CleanAgent进行标准化的通信。我们提供了演示视频，您可以在YouTube³³3[https://youtu.be/fSYXVM6qeqM](https://youtu.be/fSYXVM6qeqM)找到。
- en: 'To summarize, we make the following contributions: (1) We propose Dataprep.Clean,
    an open-sourced library showing reducing the complexity of implementing standardization
    for specific column types with type-specific standardization functions. (2) We
    propose CleanAgent , which automates the data standardization process by combining
    both the advantages of Dataprep.Clean and LLM-based Agents. (3) We deploy CleanAgent as
    a web application with user-friendly interface and demonstrate its utility. We
    also open-sourced the implementation of CleanAgent on GitHub⁴⁴4[https://github.com/sfu-db/CleanAgent](https://github.com/sfu-db/CleanAgent).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们做出了以下贡献：（1）我们提出了Dataprep.Clean，一个开源库，展示了通过类型特定标准化函数减少特定列类型标准化复杂性的能力。（2）我们提出了CleanAgent，通过结合Dataprep.Clean和基于LLM的Agent的优势，自动化数据标准化过程。（3）我们将CleanAgent部署为具有用户友好界面的Web应用程序，并展示其实用性。我们还在GitHub⁴⁴4[https://github.com/sfu-db/CleanAgent](https://github.com/sfu-db/CleanAgent)上开源了CleanAgent的实现。
- en: 2\. Type-Specific Standardization API Design
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 类型特定标准化API设计
- en: In this section, we first describe the common steps of data standardization.
    Then we introduce the type-specific API design of Dataprep.Clean.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先描述数据标准化的常见步骤。然后，我们介绍Dataprep.Clean的类型特定API设计。
- en: Common Steps of Data Standardization. Inspired by the steps human standardizing
    data cell, we identify three common steps of data standardization. We take the
    datetime column type as an example to illustrate these steps.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标准化的常见步骤。受到人类标准化数据单元步骤的启发，我们确定了数据标准化的三个常见步骤。我们以日期时间列类型为例来说明这些步骤。
- en: Assume a data scientist is dealing with an datetime column including two records
    ”Thu Sep 25 10:36:28 2003” and ”1996.07.10 AD at 15:08:56”. The data scientist
    wants to unify the chaotic column into a target format ”YYYY-MM-DD hh:mm:ss”.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据科学家正在处理一个包含两个记录的日期时间列：“Thu Sep 25 10:36:28 2003”和“1996.07.10 AD at 15:08:56”。数据科学家希望将混乱的列统一为目标格式“YYYY-MM-DD
    hh:mm:ss”。
- en: (1) Split. At the beginning, the data scientist needs to split the datetime
    string into several single parts which include one kind of specific information.
    In our example, the data scientist can get several tokens {’Thu’,’Sep’,’25’,’10’,’36’,’28’,’2003’}
    from the first record by using space and colon as separators. Different type has
    its own splitting strategy which is not always splitting into tokens. For example,
    the data scientist will split the email string into the username part and the
    domain part.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 拆分。开始时，数据科学家需要将日期时间字符串拆分为几个包含一种特定信息的单独部分。在我们的例子中，数据科学家可以通过使用空格和冒号作为分隔符，从第一个记录中获取几个标记{’Thu’，’Sep’，’25’，’10’，’36’，’28’，’2003’}。不同类型有其自己的拆分策略，这种策略不总是将字符串拆分为标记。例如，数据科学家将把电子邮件字符串拆分为用户名部分和域名部分。
- en: (2) Validate. Standardization can only be performed on valid inputs. Thus the
    second step should be validation. For example, if the string ”little cat” is an
    instance of the datetime column, this string is invalid and the data scientist
    will transform it to a default value like NaN. Intuitively, a string is valid
    means that each part of this string after splitting is valid. Hence, validation
    of each split part is important. Usually, the data scientist will recognize and
    validate each part by their domain knowledge, some corpus or some rules. If every
    split part is valid, the string is also valid. For instance, the token ’Sep’ can
    be recognized as a valid representation of month, and ’2003’ can be recognized
    as a valid year.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 验证。标准化只能在有效输入上进行。因此，第二步应为验证。例如，如果字符串“little cat”是 datetime 列的实例，这个字符串就是无效的，数据科学家将把它转换为如
    NaN 的默认值。直观地说，一个字符串有效意味着该字符串分割后的每一部分都是有效的。因此，每个分割部分的验证是重要的。通常，数据科学家会通过他们的领域知识、一些语料库或一些规则来识别和验证每一部分。如果每个分割部分都有效，那么该字符串也是有效的。例如，标记‘Sep’可以被识别为有效的月份表示，而‘2003’可以被识别为有效的年份。
- en: (3) Transform. The last step of standardization is to transform each split part
    and combine them into target format. In our example, because the target format
    is ”YYYY-MM-DD hh:mm:ss”, the month Sep is transformed into number 09 and recombined
    with other parts to the target "2003-09-25 10:36:28".
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 转换。标准化的最后一步是将每个分割部分转换并组合成目标格式。在我们的例子中，由于目标格式是“YYYY-MM-DD hh:mm:ss”，月份 Sep
    被转换为数字 09，并与其他部分重新组合为目标“2003-09-25 10:36:28”。
- en: '![Refer to caption](img/7c834ff80a47ae5828bc64b62acb3573.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c834ff80a47ae5828bc64b62acb3573.png)'
- en: Figure 2\. Basic Structure of LLM-based Agent.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 基于 LLM 的代理的基本结构。
- en: 'The Design of Unified APIs. The goal of our API design is to enable data scientists
    to complete all the common steps of data standardization of one column with a
    single function call. The simplicity and consistency are considerdered as the
    pinciple of API design. The observation on the common steps of data standardization
    brings the type-specific API design idea. Thus, we design the API in the following
    form:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 统一 API 设计。我们 API 设计的目标是使数据科学家能够通过一次函数调用完成一列的所有常见数据标准化步骤。简洁性和一致性被视为 API 设计的原则。对数据标准化常见步骤的观察带来了特定类型的
    API 设计理念。因此，我们将 API 设计成以下形式：
- en: '|  | clean_type(df, column_name, target_format) |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | clean_type(df, column_name, target_format) |  |'
- en: '![Refer to caption](img/893b4838c0ed4f61acbb2cbe4ef4e601.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/893b4838c0ed4f61acbb2cbe4ef4e601.png)'
- en: Figure 3\. The Workflow of CleanAgent.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. CleanAgent 的工作流程。
- en: where clean_type is the function name, type represents the type of the current
    column. The first argument df represents the input DataFrame, the second argument
    column_name is the column needs to be standardized and the third argument target_type
    is the target standardization format users specified. Our APIs design is flexible
    and extensible, which is convenient for users to add new standardization functions
    dealing with new data types. Currently, we have 142 standardization functions
    in Dataprep.Clean, representing 142 data types.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 clean_type 是函数名，type 表示当前列的类型。第一个参数 df 表示输入的 DataFrame，第二个参数 column_name
    是需要标准化的列，第三个参数 target_type 是用户指定的目标标准化格式。我们的 API 设计灵活且可扩展，方便用户添加处理新数据类型的新标准化函数。目前，我们在
    Dataprep.Clean 中有 142 个标准化函数，代表 142 种数据类型。
- en: 3\. CleanAgent Workflow
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. CleanAgent 工作流程
- en: In this section, we first introduce the basic structure of the LLM-based agent.
    Then we describe the CleanAgent workflow constructed by four agents. The automatic
    data standardization process can be completed by the cooperation of the four agents
    in CleanAgent .
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍基于 LLM 的代理的基本结构。然后，我们描述由四个代理构成的 CleanAgent 工作流程。通过 CleanAgent 中四个代理的合作，可以完成自动数据标准化过程。
- en: 'Basic Structure of LLM-based Agent. According to the previous surveys on LLM-based
    Agent (Xi et al., [2023](#bib.bib4)), we conclude the basic structure of LLM-based
    Agent shown in Figure [2](#S2.F2 "Figure 2 ‣ 2\. Type-Specific Standardization
    API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents").
    An LLM-based agent includes four main components: (1) an LLM used to generate
    replies for input information (2) a memory used to store historical conversation
    messages (3) a system message defining the role of the agent (4) a set of external
    tools which can be called by the LLM-based agent to complete specific tasks, such
    as web searching, code running, etc.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '基于LLM的智能体的基本结构。根据之前对基于LLM的智能体的调查（Xi等，[2023](#bib.bib4)），我们总结了图[2](#S2.F2 "Figure
    2 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data
    Standardization with LLM-based Agents")中所示的基于LLM的智能体的基本结构。一个基于LLM的智能体包括四个主要组件：（1）用于生成输入信息回复的LLM（2）用于存储历史对话信息的内存（3）定义智能体角色的系统消息（4）一组可以由LLM智能体调用以完成特定任务的外部工具，如网页搜索、代码运行等。'
- en: 'Detailed Workflow. The detailed workflow of CleanAgent is shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents"). The CleanAgent is composed of four
    agents, including a Chat Manager, a Column-type Annotator, a Python Programmer
    and a Code Executor. They can communicate with each other and automatically complete
    the data standardization process by cooperation. As mentioned in Figure [2](#S2.F2
    "Figure 2 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents"), each agent has its own memory to
    store the historical conversational messages between it and other agents. It is
    important to highlight that the memory of the Chat Manager is uniquely comprehensive,
    encompassing the entire historical conversational messages from all agents within
    the CleanAgent system. This extensive memory enables every agent in the CleanAgent to
    generate responses that are informed by the complete historical messages.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '详细工作流程。CleanAgent的详细工作流程如图[3](#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization
    API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")所示。CleanAgent由四个智能体组成，包括聊天管理器、列类型注释器、Python程序员和代码执行器。它们可以相互通信，并通过合作自动完成数据标准化过程。如图[2](#S2.F2
    "Figure 2 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents")所述，每个智能体都有自己的内存，用于存储它与其他智能体之间的历史对话信息。需要强调的是，聊天管理器的内存是独一无二的，涵盖了CleanAgent系统中所有智能体的完整历史对话信息。这种广泛的内存使CleanAgent中的每个智能体能够生成基于完整历史信息的响应。'
- en: '![Refer to caption](img/11d4e810b45c064f19923ce532cd8ac3.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/11d4e810b45c064f19923ce532cd8ac3.png)'
- en: Figure 4\. User interface of CleanAgent.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. CleanAgent的用户界面。
- en: 'The input of CleanAgent includes a table $T$ is returned. If the generated
    code executes with errors, the error message is returned to the Chat Manager and
    stored in its memory (⑥ in Figure [3](#S2.F3 "Figure 3 ‣ 2\. Type-Specific Standardization
    API Design ‣ CleanAgent: Automating Data Standardization with LLM-based Agents")).
    Then CleanAgent will retry the whole workflow until it can complete the data standardization
    process successfully.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'CleanAgent的输入包括一个返回的表格$T$。如果生成的代码执行出错，错误消息将返回给聊天管理器，并存储在其内存中（图[3](#S2.F3 "Figure
    3 ‣ 2\. Type-Specific Standardization API Design ‣ CleanAgent: Automating Data
    Standardization with LLM-based Agents")中的⑥）。然后，CleanAgent将重试整个工作流程，直到成功完成数据标准化过程。'
- en: 4\. Demonstration Scenarios
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 演示场景
- en: The demonstration setup includes a table need to be standardized and a laptop.
    The laptop must connect to the Internet for visitors can use CleanAgent smoothly
    with OpenAI’s GPT service. If the conference Internet fails, a mobile hotspot
    (established via cell phone) can also be used for running CleanAgent .
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 演示设置包括一个需要标准化的表格和一台笔记本电脑。笔记本电脑必须连接到互联网，以便访客可以顺利使用CleanAgent的OpenAI GPT服务。如果会议期间互联网失败，可以使用通过手机建立的移动热点来运行CleanAgent。
- en: 'Figure [4](#S3.F4 "Figure 4 ‣ 3\. CleanAgent Workflow ‣ CleanAgent: Automating
    Data Standardization with LLM-based Agents") shows the user interface of CleanAgent .
    As area ① shows, users must first upload a CSV file that needs to be cleaned.
    Then CleanAgent receives and shows the basic information of the uploaded file
    (number of rows and number of columns). If users want CleanAgent to start the
    data standardization process, they should just click “Start Standardization” button.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [4](#S3.F4 "Figure 4 ‣ 3\. CleanAgent Workflow ‣ CleanAgent: Automating Data
    Standardization with LLM-based Agents") 显示了 CleanAgent 的用户界面。如区域 ① 所示，用户必须首先上传需要清理的
    CSV 文件。然后 CleanAgent 接收并显示上传文件的基本信息（行数和列数）。如果用户希望 CleanAgent 开始数据标准化过程，只需点击“开始标准化”按钮。'
- en: After clicking the “Start Standardization” button, as area ② shows, the User_Proxy
    generates three detailed steps to complete the data standardization task. Firstly,
    the Column-type Annotator receives messages from the Chat Manager, annotates and
    outputs the type of each column, as area ③ shows. Then the Python Programmer picks
    up standardization functions from Dataprep.Clean with respect to the type of each
    column, and write proper Python code to standardize columns in the input table,
    as area ④ shows. Thirdly, the Code Executor executes the Python code generated
    by the Python Programmer and collects the execution messages, as area ⑤ shows.
    If the Code Executor gets the error message when executing generated Python code,
    the error message is sent to the Chat Manager and becomes part of the prompt of
    the next try. If the Code Executor gets the message of successful execution, CleanAgent will
    report that the data standardization is completed, as area ⑥ shows. Moreover,
    users can click the “Show Cleaned Table” button to check whether the standardized
    table matches users’ requirements. If yes, users can directly download the standardized
    table. Otherwise, users can input their extra requirements with natural language.
    CleanAgent will start a new data standardization process according to users’ input.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“开始标准化”按钮后，如区域 ② 所示，User_Proxy 生成三个详细步骤来完成数据标准化任务。首先，Column-type Annotator
    从 Chat Manager 接收消息，注释并输出每列的类型，如区域 ③ 所示。然后，Python Programmer 根据每列的类型从 Dataprep.Clean
    中选择标准化函数，并编写适当的 Python 代码以标准化输入表中的列，如区域 ④ 所示。第三，Code Executor 执行由 Python Programmer
    生成的 Python 代码并收集执行消息，如区域 ⑤ 所示。如果 Code Executor 在执行生成的 Python 代码时收到错误消息，则该错误消息会被发送到
    Chat Manager，并成为下一次尝试的提示的一部分。如果 Code Executor 收到成功执行的消息，CleanAgent 将报告数据标准化已完成，如区域
    ⑥ 所示。此外，用户可以点击“显示清理后的表格”按钮，检查标准化后的表格是否符合用户的要求。如果符合，用户可以直接下载标准化后的表格。否则，用户可以用自然语言输入额外要求。CleanAgent
    将根据用户的输入启动新的数据标准化过程。
- en: 5\. Conclusion and Future Work
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论与未来工作
- en: In this demo paper, we proposed CleanAgent to automate the data standardization
    process with Dataprep.Clean and LLM-based Agents. We implemented CleanAgent as
    a web service to visualize the conversations among agents. Other tasks in data
    science life cycle such as data cleaning and data visualization can also be completed
    by LLM-based agents (Xue et al., [2023](#bib.bib5)). In the future, it is promising
    that the data science life cycle can be automatically planned and completed by
    LLM-based agents’ cooperation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇演示论文中，我们提出了 CleanAgent，以通过 Dataprep.Clean 和基于 LLM 的代理自动化数据标准化过程。我们将 CleanAgent
    实现为一个 Web 服务，以可视化代理之间的对话。数据科学生命周期中的其他任务，例如数据清理和数据可视化，也可以通过基于 LLM 的代理完成（Xue 等，[2023](#bib.bib5)）。未来，数据科学生命周期有望通过基于
    LLM 的代理的合作自动规划和完成。
- en: References
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'McKinney et al. (2024) Wes McKinney et al. 2024. pandas: powerful Python data
    analysis toolkit. [https://pandas.pydata.org/](https://pandas.pydata.org/) Accessed:
    2024-01-25.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKinney 等（2024）Wes McKinney 等人，2024。pandas：强大的 Python 数据分析工具包。 [https://pandas.pydata.org/](https://pandas.pydata.org/)
    访问日期：2024-01-25。
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling
    Next-Gen LLM Applications via Multi-Agent Conversation Framework. *CoRR* abs/2308.08155
    (2023). [https://doi.org/10.48550/ARXIV.2308.08155](https://doi.org/10.48550/ARXIV.2308.08155)
    arXiv:2308.08155'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2023）Qingyun Wu、Gagan Bansal、Jieyu Zhang、Yiran Wu、Shaokun Zhang、Erkang
    Zhu、Beibin Li、Li Jiang、Xiaoyun Zhang 和 Chi Wang。2023。AutoGen：通过多代理对话框架实现下一代 LLM
    应用。*CoRR* abs/2308.08155 (2023)。 [https://doi.org/10.48550/ARXIV.2308.08155](https://doi.org/10.48550/ARXIV.2308.08155)
    arXiv:2308.08155
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The
    Rise and Potential of Large Language Model Based Agents: A Survey. *CoRR* abs/2309.07864
    (2023). [https://doi.org/10.48550/ARXIV.2309.07864](https://doi.org/10.48550/ARXIV.2309.07864)
    arXiv:2309.07864'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan,
    Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou,
    Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang,
    Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. 基于大型语言模型的智能体的崛起与潜力：一项调查。*CoRR*
    abs/2309.07864 (2023). [https://doi.org/10.48550/ARXIV.2309.07864](https://doi.org/10.48550/ARXIV.2309.07864)
    arXiv:2309.07864
- en: 'Xue et al. (2023) Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting
    Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang
    Zhao, Fan Zhou, Danrui Qi, Hong Yi, Shaodong Liu, and Faqiang Chen. 2023. DB-GPT:
    Empowering Database Interactions with Private Large Language Models. *CoRR* abs/2312.17449
    (2023). [https://doi.org/10.48550/ARXIV.2312.17449](https://doi.org/10.48550/ARXIV.2312.17449)
    arXiv:2312.17449'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xue et al. (2023) Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting
    Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang
    Zhao, Fan Zhou, Danrui Qi, Hong Yi, Shaodong Liu, and Faqiang Chen. 2023. DB-GPT:
    利用私有大型语言模型增强数据库交互。*CoRR* abs/2312.17449 (2023). [https://doi.org/10.48550/ARXIV.2312.17449](https://doi.org/10.48550/ARXIV.2312.17449)
    arXiv:2312.17449'
