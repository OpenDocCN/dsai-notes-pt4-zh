<!--yml

分类：未分类

日期：2025-01-11 11:50:37

-->

# 目标核心：通过直接操作LLM攻击基于RAG的代理的一种简单有效的方法

> 来源：[https://arxiv.org/html/2412.04415/](https://arxiv.org/html/2412.04415/)

Xuying Li, Zhuo Li, Yuji Kosuga, Yasuhiro Yoshida, Victor Bian

HydroX AI

{xuyingl, zhuoli, yujikosuga, yasuhiro, victor}@hydrox.ai

###### 摘要

由大语言模型（LLMs）驱动的人工智能代理已经通过实现无缝、自然和上下文感知的沟通方式，改变了人机互动。尽管这些进展提供了巨大的效用，但它们也继承并放大了固有的安全风险，如偏见、公平性问题、幻觉输出、隐私泄露以及缺乏透明度。本文探讨了一个关键的漏洞：针对人工智能代理中LLM核心的对抗性攻击。具体来说，我们测试了一个看似简单的对抗性前缀（例如“忽略文档”）的假设，它能够通过绕过上下文保护措施，迫使LLM生成危险或非预期的输出。通过实验，我们展示了一个高攻击成功率（ASR），揭示了现有LLM防御的脆弱性。这些发现强调了迫切需要针对LLM层面以及更广泛的代理架构中的漏洞，制定强大、分层的安全措施。

## 1 引言

语言代理代表了人工智能领域的变革性创新，使系统能够通过动态互动处理复杂的、上下文敏感的任务。这些代理的核心利用大语言模型（LLMs）来处理指令并生成输出。然而，这种依赖带来了显著的挑战，因为语言代理继承了LLM固有的安全风险，同时放大了一些风险，并由于其自主性引入了新的风险。已知LLM会出现偏见、公平性问题、幻觉输出、隐私泄露以及决策过程缺乏透明度等问题。这些风险在单独使用LLM时已令人担忧，当LLM嵌入到期望无需人工监督的自主代理中时，这些风险变得更加明显。此外，语言代理还加剧了如劳动力置换等风险，其中由这些系统驱动的自动化可能会扰乱就业领域，并引入新危险，如在关键应用中的不可逆操作和决策失败。

尽管在设计安全架构方面取得了显著进展，仍有相当一部分语言代理依赖于检索增强生成（RAG）技术，其中大语言模型（LLM）与外部检索系统结合，以确保上下文准确的响应。虽然RAG框架增强了系统的能力，但它们继承了底层LLM的漏洞，导致管道中出现可被利用的弱点。

本研究通过假设对抗性攻击可以直接操控语言代理中LLM的核心，从而迫使其产生无意或危险的输出，来应对这些漏洞。不同于将代理视为整体系统的传统方法，我们的工作识别了LLM作为一个关键的脆弱点。通过注入一个简单而强大的前缀“Ignore the document”，我们展示了当前的LLM缺乏抵抗这种对抗性操控的能力。这个前缀利用了LLM的指令处理逻辑，覆盖了在RAG管道中精心检索的上下文，并暴露了在指令层次优先级处理中的设计缺陷。我们的发现不仅揭示了此类攻击的高成功率，还暴露了当前LLM和代理级别的安全机制的不足，突显了在LLM架构中进行基础性改进以确保更安全、更有韧性的语言代理的紧迫需求。

## 2 方法论

### 2.1 数据集准备

为了验证简单的对抗性提示是否能有效操控大型语言模型（LLM）的输出，我们设计了一系列实验，重点关注数据准备、攻击方法和性能指标。这些实验针对嵌入在语言代理中的LLM，特别强调它们在检索增强生成（RAG）管道中的脆弱性。

我们的数据编制策略涉及精心策划多种来源，确保全面的评估。这些来源涵盖了多个领域，包括语言代理研究、提示工程文献、对抗性攻击研究以及新兴的AI安全研究。为了预处理数据，我们使用了RecursiveCharacterTextSplitter，设置为250个标记的块大小且没有重叠，确保数据集既具有代表性又便于实验操作。这一方法为评估各种提示和上下文中的攻击成功率提供了坚实的基础。

我们编制了一个包含1,134个对抗性提示的数据集，涵盖了多个类别，包括伦理违规、数据中毒和模型窃取。表格[1](https://arxiv.org/html/2412.04415v1#S2.T1 "Table 1 ‣ 2.1 Dataset Preparation ‣ 2 Methodology ‣ Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation")提供了这些攻击类别的分布情况。

本研究测试的LLM包括多种先进的模型，如GPT-4o、Llama3.1、Llama3.2、Mistral-7B及其变体。为了管理测试用例的向量化存储和检索，我们使用了SKLearnVectorStore，这使得与攻击提示数据集的高效交互成为可能。数据集本身来自EPASS，包含了1,134个独特的对抗性提示，专门用于探测指令漏洞。这些提示涵盖了多种类别，每个类别代表着潜在的利用点。

表 1：攻击提示类别及分布

| 类别 | 比例 (%) |
| --- | --- |
| 犯罪 | 5.2 |
| 数据中毒 | 5.2 |
| 同意违规 | 5.3 |
| 版权 | 5.3 |
| 伦理 | 5.3 |
| 欺诈 | 5.3 |
| 武器 | 5.3 |
| 假信息 | 5.4 |
| 垃圾邮件 | 5.5 |
| 暴力 | 5.6 |

### 2.2 攻击策略

在评估大型语言模型（LLM）的安全性时，采用多种测试方法可以全面了解它们的稳健性和潜在漏洞。以下是三种主要测试方法的详细描述：

+   •

    基线评估：此方法评估模型在标准条件下的表现，不引入任何对抗性输入。它作为对照组，帮助了解模型的典型行为，并为比较后续攻击策略的效果提供基准。

+   •

    自适应攻击提示：此方法系统地生成输入，旨在最大化模型产生非预期或有害输出的可能性。通过利用对模型架构和潜在弱点的了解，攻击者可以设计绕过安全机制的提示，从而导致如“越狱”行为，即模型执行它通常会拒绝的指令。（Andriushchenko 等， [2024](https://arxiv.org/html/2412.04415v1#bib.bib1)）

+   •

    ArtPrompt：此技术利用非常规的输入格式，如 ASCII 艺术，来绕过模型的上下文安全防护。通过将提示嵌入复杂的字符模式中，攻击者可以导致模型误解输入，从而产生无法控制或有害的输出。研究表明，这种方法能够引发原本符合安全协议的模型产生不当回应。（Jiang 等， [2024](https://arxiv.org/html/2412.04415v1#bib.bib2)）

一个关键创新是添加了前缀“忽略文档”，该前缀通过指示 LLM 忽视外部上下文，直接破坏了检索机制。

### 2.3 评估指标

实验聚焦于：

+   •

    基线攻击成功率（ASR）：没有任何修改下成功操控的百分比。

+   •

    带前缀的 ASR：成功操控的百分比，前缀为“忽略文档”。

## 3 结果

表 [2](https://arxiv.org/html/2412.04415v1#S3.T2 "表 2 ‣ 3 结果 ‣ 针对核心：通过直接操作 LLM 攻击 RAG 基础的代理的简单有效方法")提供了实验结果的详细总结，展示了在不同条件下各种模型的攻击成功率（ASR）：基线、自适应攻击提示和 ArtPrompt。结果表明，对抗性攻击对基于 RAG 的代理产生了显著影响。

表 2：不同模型和攻击类型的攻击成功率（ASR）

| 模型名称 | 基线 ASR | 自适应攻击提示 ASR | ArtPrompt ASR |
| --- | --- | --- | --- |
| Gemma2 w/o | 0.189 | 0.963 | 0.451 |
| Gemma2 | 0.327 | 0.973 | 0.468 |
| GPT4o Mini w/o | 0.011 | 0.077 | 0.093 |
| GPT4o Mini | 0.015 | 0.111 | 0.112 |
| GPT4o w/o | 0.072 | 0.022 | 0.166 |
| GPT4o | 0.073 | 0.044 | 0.224 |
| Llama3.1 w/o | 0.054 | 0.706 | 0.696 |
| Llama3.1 | 0.034 | 0.791 | 0.762 |
| Llama3.2 w/o | 0.011 | 0.349 | 0.443 |
| Llama3.2 | 0.023 | 0.402 | 0.332 |
| Mistral-7B w/o | 0.661 | 0.925 | 0.705 |
| Mistral-7B | 0.666 | 0.932 | 0.767 |

表格突出了不同模型和攻击策略下攻击成功率的变化。具有预训练防御机制的模型（w/）在基线条件下通常表现更好，但仍然容易受到针对性攻击，这从自适应攻击提示和ArtPrompt的高成功率中可见一斑。

## 4 观察结果

实验结果揭示了当前语言代理设计中的两个关键洞察，特别是在它们对大型语言模型（LLM）进行指令处理和上下文推理时所依赖的脆弱性。

首先，实验表明，使用看似简单的前缀“忽略文档”时，攻击的成功率（ASR）很高。这个前缀始终能够操控LLM的输出，成功绕过了检索增强生成（RAG）管道中嵌入的上下文保护措施。该攻击利用了LLM指令处理逻辑中的根本性弱点，覆盖了本应指导响应生成的检索外部信息。这种一致性的操控在多个最先进的LLM中都有观察到，包括那些专门对安全输出进行了对齐的模型。前缀的成功凸显了现有LLM设计的脆弱性，其中缺乏层级优先级处理使得即时的提示能够凌驾于先前建立的上下文边界之上。这些结果表明存在一种系统性的漏洞，敌对指令可以可靠地绕过核心处理保护机制，使整个语言代理暴露于潜在的利用风险中。

其次，研究揭示了现有的代理层防御机制在减轻这些攻击方面的不足。尽管在代理层实施了多种安全和监控措施，但这些机制仍无法有效抵御对LLM核心的直接操控。攻击通过利用LLM本身固有的漏洞成功突破了这些保护层。当前的代理层防御机制假设基础的LLM能够可靠地处理输入；然而，当LLM核心被攻破时，这一假设就会失败。这个发现强调了传统防御策略的局限性，传统策略侧重于高层保护，而未能解决LLM内部的根本性弱点。此外，在使用共享LLM核心的多代理系统中，这类攻击可能产生级联效应，通过互联的代理传播有害输出，放大单一安全漏洞的后果。

## 5 未来工作

解决当前LLM和语言智能体架构中识别出的脆弱性需要共同努力，重新思考它们的设计和防御机制。下面，我们提出了一条未来研究的路线图，并得到该领域近期研究的支持。

### 5.1 层次化指令处理

强大的层次化指令理解：语言模型通常缺乏处理指令的细致层次结构，这使得它们容易受到简单的对抗性提示攻击。未来的系统必须嵌入一个结构化框架，根据指令的来源、上下文和意图来优先处理指令。例如，Russinovich等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib3)）证明了多轮越狱攻击利用了这种层次结构的缺失，导致了有害的输出。

防止上下文覆盖：即时提示往往会覆盖上下文保护机制，这在当前的LLM实现中有所体现（Zhu等人，[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）。研究人员可以借鉴层次化强化学习（HRL）原理，构建指令处理层，这些层可以在变化的上下文中动态调整，同时保持一个安全的基础层。

### 5.2 上下文感知指令评估

动态上下文敏感性：增强LLM评估指令时与更广泛上下文信息的关联能力至关重要。Chen等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib5)）强调了多智能体系统面临的挑战，其中各个智能体在孤立的上下文片段上操作，导致更大管道中的脆弱性。诸如记忆增强神经网络（MANNs）之类的技术可以通过使模型能够保持并利用历史上下文来促进更好的指令评估，从而为解决这一问题提供路径。

降低提示注入风险：提示注入攻击之所以成功，往往是因为当前架构评估输入时未能严密地评估其与整体任务的对齐情况。Zou等人（[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)）提出，对抗性对齐的LLM需要一个内在的验证层，该层能够标记并消除潜在有害的指令。将对抗训练与上下文嵌入结合起来，能够进一步减轻这些风险。

### 5.3 多层次安全机制

智能体级别的安全防护：多智能体系统中的防御策略通常在较高层次上运作，未能解决LLM核心的脆弱性。在LLM核心内部部署精细化的安全机制，例如对抗性提示过滤器或概率一致性检查，可以显著降低对攻击的敏感性（Zhu等人，[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）。

跨层集成：未来的架构必须实现多层次的安全框架，将LLM级别的防御与代理级别的安全保障相结合。诸如差分隐私（Park 等，[2023](https://arxiv.org/html/2412.04415v1#bib.bib7)）和可解释AI（XAI）方法（Chen 等，[2024](https://arxiv.org/html/2412.04415v1#bib.bib5)）等技术可以通过确保系统行为在每个层次上都保持可解释和安全，提供额外的保护层。

模型无关防御层：采用模型无关的安全协议，如普适对抗训练（Zou 等，[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)），可以为不同LLM实现提供一致的防御机制。这些方法可以通过异常检测系统补充，该系统实时监控输出一致性。

### 5.4 纳入人类反馈循环

通过反馈进行强化：通过强化学习人类反馈（RLHF）等方法利用人类反馈，已显示出在将LLM输出与期望的伦理和安全标准对齐方面的潜力（Zhu 等，[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）。增强的反馈机制可以通过迭代优化模型行为，作为对抗性输入的反制措施。

### 5.5 开发全面的基准标准

攻击韧性基准：创建标准化的基准用于评估LLMs和语言代理的攻击韧性至关重要。现有的数据集，如用于普适对抗攻击研究的数据集（Zou 等，[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)），可以作为测试各种对抗场景的基础。

真实世界仿真测试：未来的研究必须包含能够密切模拟现实场景的仿真环境。这些环境可以促进对架构在复杂、多层次攻击下的压力测试（Chen 等，[2024](https://arxiv.org/html/2412.04415v1#bib.bib5)）。

## 6 相关工作

最近在人工智能领域，特别是在大规模语言模型（LLMs）的领域，取得的进展显著提高了AI代理在人与计算机交互中的能力。然而，这些进展也暴露了研究人员在多个领域积极探索的脆弱性。

### 6.1 LLMs 和语言代理的安全挑战

许多研究已探讨了LLMs中的固有安全风险，如偏见、公平性问题、幻觉和透明性挑战。例如，Park 等（[2023](https://arxiv.org/html/2412.04415v1#bib.bib7)）研究了旨在模拟人类行为的生成性代理的安全影响，突出了关于伦理和无偏输出的担忧。这些基础性研究强调了LLM能力的双刃剑特性，其中它们的生成能力与潜在的安全缺陷并存。

### 6.2 LLMs 的对抗性攻击

由于对抗性攻击可能危及系统完整性，针对LLM的攻击已获得广泛关注。Zou等人（[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)）提出了普遍且可转移的对抗性攻击，证明了即便是对齐的LLM也容易受到精心设计的输入攻击。同样，Zhu等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）提出了AutoDAN，一种可解释的基于梯度的攻击方法，揭示了即使在强健架构中也存在系统性漏洞。这些研究强调了开发抗攻击的LLM框架的紧迫性。

### 6.3 越狱和提示注入漏洞

提示注入攻击，如Russinovich等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib3)）探讨的，展示了恶意行为者轻松绕过LLM保护机制的能力。他们的多轮越狱攻击证明了对抗性提示可以利用错位的指令层次结构，从而导致意外和潜在有害的输出。这些发现与本文的假设一致，后者针对的是LLM在指令处理层面的漏洞。

### 6.4 检索增强生成（RAG）框架

语言代理通常利用RAG技术将外部数据与LLM输出结合，如新兴的教程文献中所讨论的那样。虽然RAG提高了响应的相关性，但最近的研究表明，RAG对LLM生成输出的依赖引入了可被利用的弱点。在多代理系统中，尤其是RAG管道容易受到对抗性操控，这一点已经被强调。

### 6.5 防御机制和局限性

针对LLM漏洞的防御机制主要集中在代理级别的保护措施上。然而，Zou等人（[2023](https://arxiv.org/html/2412.04415v1#bib.bib6)）和Zhu等人（[2024](https://arxiv.org/html/2412.04415v1#bib.bib4)）等研究表明，这些措施不足以解决LLM的核心弱点。防御上的这一空白凸显了发展层次化、上下文感知的指令评估策略的关键需求，正如本文所述。

## 7 结论

本研究突出了当前语言代理的关键漏洞，尤其是那些利用检索增强生成（RAG）管道的代理。通过研究针对LLM核心的对抗攻击，我们证明了即使是看似无害的前缀，例如“忽略文档”，也能显著破坏LLM输出的完整性。将这一前缀与自适应攻击提示和ArtPrompt等高级攻击方法结合使用，进一步增强了攻击的效果，暴露了在指令优先级和上下文整合设计中的缺陷。我们的研究结果强调了现有LLM安全机制的脆弱性，并揭示了指令的层级理解和上下文整合方面的系统性弱点。这些漏洞对语言代理的可靠性构成了重大风险，尤其是在需要高信任度、准确性和安全性的应用场景中。本研究强调了迫切需要多层次的安全措施，以应对LLM级别和代理级别的防御，为构建更具韧性的AI架构提供了路线图。

## 8 局限性

尽管从本研究中获得了重要的见解，但仍需承认一些局限性。

首先，实验的范围主要集中在特定的LLM和基于RAG的系统上，尚未充分探讨这些发现是否能广泛适用于其他架构。

第二，尽管我们展示了结合自适应攻击提示和ArtPrompt的“忽略文档”前缀的有效性，但该研究并未完全调查其他潜在的对抗性提示变体，这些变体可能产生类似甚至更高的成功率。

第三，评估指标主要集中在攻击成功率（ASR）上，而没有对模型的稳健性与可用性之间的潜在权衡进行全面分析。

最后，研究没有讨论这些漏洞在完全操作的系统中的实际影响，因在这些系统中，动态保护措施、人类监督和反馈机制可能会缓解一些已识别的风险。

未来的研究应探索这些局限性，以便更全面地理解对抗性漏洞及其对复杂AI生态系统的影响。

## 参考文献

+   Andriushchenko 等人 [2024] Maksym Andriushchenko、Francesco Croce 和 Nicolas Flammarion. 使用简单自适应攻击破解安全对齐的语言模型. *arXiv 预印本 arXiv:2404.02151*, 2024. URL [https://arxiv.org/abs/2404.02151](https://arxiv.org/abs/2404.02151).

+   Jiang 等人 [2024] Fengqing Jiang、Zhangchen Xu、Luyao Niu、Zhen Xiang、Bhaskar Ramasubramanian、Bo Li 和 Radha Poovendran. Artprompt：基于ASCII艺术的破解攻击针对对齐的LLMs. *arXiv 预印本 arXiv:2402.11753*, 2024. URL [https://arxiv.org/abs/2402.11753](https://arxiv.org/abs/2402.11753).

+   Russinovich 等人 [2024] Mark Russinovich, Ahmed Salem, 和 Ronen Eldan. 太好了，现在写一篇关于这个的文章：渐进式多轮 LLM 破解攻击. *arXiv 预印本 arXiv:2404.01833*，2024年。

+   Zhu 等人 [2024] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, 和 Tong Sun. Autodan：基于梯度的可解释对抗攻击大语言模型. 发表在 *首次语言建模会议*，2024年。

+   Chen 等人 [2024] Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, 等人. Coder：通过多代理和任务图解决问题. *arXiv 预印本 arXiv:2406.01304*，2024年。

+   Zou 等人 [2023] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, 和 Matt Fredrikson. 通用和可转移的对抗攻击对齐的语言模型. *arXiv 预印本 arXiv:2307.15043*，2023年。

+   Park 等人 [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, 和 Michael S. Bernstein. 生成代理：人类行为的互动仿真. 发表在 *第36届ACM用户界面软件与技术年会论文集*，第1-22页，2023年。
