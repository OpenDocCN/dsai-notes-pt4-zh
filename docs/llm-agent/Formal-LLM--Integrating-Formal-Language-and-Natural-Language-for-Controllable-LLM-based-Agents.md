<!--yml
category: 未分类
date: 2025-01-11 12:56:40
-->

# Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

> 来源：[https://arxiv.org/html/2402.00798/](https://arxiv.org/html/2402.00798/)

Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang Department of Computer Science, Rutgers University, New Brunswick
{zelong.li, wenyue.hua, hw488, hz375, yongfeng.zhang}@rutgers.edu

###### Abstract

Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM’s content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users’ trust in LLM-based agents. In response, this paper proposes a novel “Formal-LLM” framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows agent developers to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The source code of this work is available at [https://github.com/agiresearch/Formal-LLM](https://github.com/agiresearch/Formal-LLM).

Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents

Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang Department of Computer Science, Rutgers University, New Brunswick {zelong.li, wenyue.hua, hw488, hz375, yongfeng.zhang}@rutgers.edu

## 1 Introduction

Numerous applications have emerged with the rapid development of Large Language Models (LLM). One notable application is the LLM-based agent, which is capable of automatically generating and executing multi-step plans to solve complex tasks. While the LLM-based agent exhibits creativity, there is a concern about the potential generation of unreasonable and invalid plans, undermining the effectiveness of agents. For example, generating a plan that attempts to process image data using a tool designed for text can lead to errors. Recent studies have pointed out the challenges of LLM-based agents in developing non-executable plans without sufficient human oversight (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Yuan et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib49)). Addressing these challenges is crucial for improving agent performance, increasing the generation of valid plans, and maintaining user trust. Various attempts have been made to control LLM text generation, such as incorporating hard constraints (Takase and Okazaki, [2019](https://arxiv.org/html/2402.00798v4#bib.bib39); Carlsson et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib4)), soft constraints (Gu et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib16); Lu et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib28)), or a combination of both (Chen et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib5)). However, the focus of controlling LLM-based agents emphasizes the validity of plans and tool use over purely text generation. Some studies (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Yuan et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib49)) use LLM as a parser to extract a chain of tools from the generated texts based on prompts, yet satisfactory ratios for valid plans remain elusive.

To tackle the problems of invalid plan generation, we propose a framework named “Formal-LLM”, which integrates the expressiveness of natural language and the precision of formal language, as shown by the toy example in Figure [1](https://arxiv.org/html/2402.00798v4#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). Specifically, to control the LLM-based agent’s plan generation, agent developers construct a context-free grammar (CFG) as the formal language to represent the constraints for the agent. The CFG is then automatically translated into a pushdown automaton (PDA). When LLM conducts planning, it is prompted to follow the state transition defined by the automaton. This is realized by limiting the LLM-based agent’s choices at each step to the valid actions defined by the PDA at its current state, which helps to guarantee that the constraint is satisfied in the final generated plan. We choose PDA in this work because some tasks need to be solved by tree-structured plans rather than chain-structured plans, and generating tree-structured plans requires a PDA.

![Refer to caption](img/3a7971b48e4c862acbc4af63033c5908.png)

Figure 1: The Formal-LLM workflow with a toy example. To control the LLM-based agent’s plan generation, agent developers construct a formal language (context-free grammar, CFG) to represent the natural language constraints. The formal language is then translated into a pushdown automaton (PDA). When LLM conducts planning, it needs to follow the state transition defined by the automaton, which helps to guarantee that the constraint is satisfied in the final generated plan. We choose PDA because some plans exhibit tree-structure (as shown by other examples in the paper), which requires a PDA to generate.

Furthermore, we introduce the backtracking mechanism in the Formal-LLM to increase the probability of finding valid plans for the agent, which enables the planning process to return to the previous step when a dead end is reached on the automaton. Besides, traditional LLM-based agent fine-tuning techniques such as Reinforcement Learning from Task Feedback (RLTF) (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) rely on the rewards from the agent’s plan execution to fine-tune the LLM parameters. However, since many invalid plans may be generated, many rewards are actually not informative for LLM-based agent fine-tuning. Our Formal-LLM approach guarantees that invalid plans are excluded during the agent’s plan generation. As a result, our approach helps to increase the amount of valid rewards for LLM fine-tuning, which improves the performance of fine-tuned LLM-based agents.

In our experiments, we implement the Formal-LLM framework across various LLMs, including both open-source and closed-source LLMs. We also test our framework on both benchmark tasks and real-life practical tasks. Specifically, the benchmark tasks involve the LLM-based agent utilizing different tools to solve complex problems through multiple steps. The real-life scenarios encompass daily routines, cooking instructions, and commercial risk management, each imposing specific domain-knowledge or common-sense constraints. Our findings demonstrate the Formal-LLM’s capability to generate reasonable plans. The framework substantially enhances the overall performance on the benchmark tasks by over 50% and can always generate executable plans. In real-life scenarios, we provide qualitative analyses into the improvements brought by the Formal-LLM framework, affirming its feasibility and effectiveness in rendering the agents more controllable.

## 2 Related Work

### 2.1 LLM-based AI Agent

An AI agent is an autonomous entity capable of making decisions and executing actions within a specific environment to effectively tackle diverse complex tasks using AI techniques (Ge et al., [2023b](https://arxiv.org/html/2402.00798v4#bib.bib13); Mei et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib29); Wang et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib43); Xi et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib46)). The emergence of Large Language Models (LLMs), exemplified by the GPT series (Radford et al., [2019](https://arxiv.org/html/2402.00798v4#bib.bib34); Brown et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib2); OpenAI, [2023](https://arxiv.org/html/2402.00798v4#bib.bib30)) and the LLaMA series (Touvron et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib40), [b](https://arxiv.org/html/2402.00798v4#bib.bib41)), has spurred the exploration of LLM-based agents (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Huang et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib22)). These agents utilize LLMs as their central cognitive component or controller, broadening their perceptual and action capabilities through approaches like multimodal perception and tool usage (Schick et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib38); Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Qin et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib33)). In contrast to pre-LLM era AI agents, LLM-based agents showcase creativity, manifesting in the ability to generate innovative ideas without additional learning (Franceschelli and Musolesi, [2023](https://arxiv.org/html/2402.00798v4#bib.bib11)), indicating a degree of self-directed exploration and decision-making (Xi et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib46)). Despite LLM-based agent’s application in diverse real-world scenarios such as software development (Li et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib24); Qian et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib31)), scientific research (Boiko et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib1)), and system management (Liu et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib27)), recent studies have highlighted issues of non-executable plans being generated without adequate human oversight (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Yuan et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib49)). If these plans lack executability, the utility of LLM-based agents in fields that require high validity is compromised, and their unreliability erodes user trust. To address this challenge, we propose integrating natural language and precise automaton during the LLM-based agent’s planning process.

### 2.2 Controllable LLM Generation

To the best of our knowledge, there are few works on controllable LLM-based agents, and discussions primarily revolve around controllable LLM text generation. Controlling LLM text generation typically falls into two categories: hard constraints and soft constraints (Qin et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib32)). Hard constraints involve constraints on the desired text length, designated keywords for the generated text (Takase and Okazaki, [2019](https://arxiv.org/html/2402.00798v4#bib.bib39); Carlsson et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib4)), and constraint vocabulary space in decoding time (Hemmer et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib17); Geng et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib14), [2023](https://arxiv.org/html/2402.00798v4#bib.bib15)), while soft constraints restrict the output based on specific semantics, such as sentiments or topics (Gu et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib16); Lu et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib28); Li et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib25)). A recent work (Chen et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib5)) attempts to integrate both constraints into a unified approach. However, text generation control has disadvantages and problems. First, text generation control could hardly be directly applied to closed-source LLMs due to their need to access the decoding probability (Geng et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib14)). Second, controllable LLM-based agents prioritize the validity of plan and tool use over purely text generation. Recent studies on LLM-based agents have noted limited effectiveness when employing text constraint generation on LLM-based agents and have explored using LLMs as a parser to extract planning information from generated texts (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Yuan et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib49)); but relying heavily on the parser LLM’s effectiveness may not yield a satisfactory plan valid rate. We propose an automaton-guided agent planning to ensure that the generated plans adhere 100% to the constraints.

## 3 Preliminary Knowledge

### 3.1 Context-free Grammar

Many LLM agents operate tools that take multiple inputs and produce one output. For example, the tool “Visual Question Answering” receives an image and a text as the input and outputs a text. The plan involving such tools forms a tree structure, and such tree-structured plans require Context-Free Grammar (CFG) to express the rules.

The CFG consists of four components: terminals (symbols unable to be replaced), nonterminals (symbols subject to replacement), start symbol (a unique nonterminal, usually denoted as $S$), and productions (rules governing symbol substitution). The CFG production format follows:

|  | $A\rightarrow\alpha$ |  | (1) |

where $A$ is a single nonterminal, and $\alpha$ is a string comprising any combination of terminals and nonterminals. $A$ can be replaced by $\alpha$ in any situation.

|  | $\begin{split}&S\rightarrow\varepsilon&#124;aSb\end{split}$ |  | (2) |

Eq.([2](https://arxiv.org/html/2402.00798v4#S3.E2 "Equation 2 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) is a CFG example with two productions separated by “$|$”, where $a,b$ are terminals, $S$ is the start symbol and the only nonterminal, and $\varepsilon$ stands for the empty string. From the start symbol, we can use these two productions to generate infinite strings, such as the empty string, $ab$, and $aabb$. Take the derivation of $aabb$ as an example. We first apply the second production twice ($S\rightarrow aSb\rightarrow aaSbb$) and then the first production once ($aaSbb\rightarrow aabb)$. The formal language, Context-Free Language (CFL), contains the set of strings of terminals, which can be derived from $S$ in a certain amount of CFG production steps.

![Refer to caption](img/239595e97874ab53d561e2a677877dd1.png)

Figure 2: A Pushdown Automaton (PDA) example.

![Refer to caption](img/9f6387fa799b89c3e9e12c966ffd0f8c.png)

Figure 3: An equivalent PDA to the CFG of the combination of Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) and Eq.([4](https://arxiv.org/html/2402.00798v4#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")). The lowercase letters $a$ to $f$ are $a_{i}$ to $f_{j}$ in Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")). For example, the transit $(b,B;\varepsilon)$ implies the PDA removes the stack top $B$ and remains in state $q_{1}$ if the PDA is at state $q_{1}$, the stack top is $B$, and the next string letter is $b_{1}/b_{2}/b_{3}$.

### 3.2 Pushdown Automaton

Every CFG can be transformed into an equivalent nondeterministic Pushdown Automaton (PDA) with an algorithm (Hopcroft et al., [2001](https://arxiv.org/html/2402.00798v4#bib.bib19)). A PDA is a machine that moves through a series of states based on a given string input, a transition function, and the top element of a stack. The automaton reads the letters in the input sequentially, and if there exists a path leading the machine to an accept state right after consuming the entire string, the input string is considered an accepting word. The set of all accepting words is referred to the CFL accepted by the automaton. Since the PDA process is more intuitive and automated than CFG for guiding the prompting of LLMs, our framework is designed on top of PDA.

Figure [2](https://arxiv.org/html/2402.00798v4#S3.F2 "Figure 2 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") shows how PDA works as an example. This automaton includes a stack alphabet ($\{S,Z\}$) with an initial stack symbol $Z$, a start state $q_{0}$, an accept state set $\{q_{2}\}$, and a transit function over states. The transit functions are in the form of triplets. For example, the edge $(a,Z;SZ)$ from $q_{0}$ to $q_{0}$ means that if the stack top is $Z$ and the next letter of the string is $a$, the PDA remains in $q_{0}$ state and the stack top changes from $Z$ to $SZ$ ($S$ is the stack top now).

This PDA is equivalent to the CFG in Eq.([2](https://arxiv.org/html/2402.00798v4#S3.E2 "Equation 2 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")), i.e., every accepting word of this PDA belongs to the CFL defined in Eq.([2](https://arxiv.org/html/2402.00798v4#S3.E2 "Equation 2 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")). We still use $aabb$ as an example to show the PDA mechanism. Initially, the PDA is at $q_{0}$ and the stack only contains $Z$. We first apply $(a,Z;SZ)$ when receiving the first letter $a$, and the PDA remains in $q_{0}$ with the stack as $SZ$. Then, when consuming the next letter $a$, we use $(a,S;SS)$ and $(\varepsilon,S;S)$, move to $q_{1}$ state, and change the stack to $SSZ$. Next, the PDA remains in $q_{1}$ and applies $(b,S;\varepsilon)$ function twice to consume the two $b$’s in the input string, and at the same time, the stack transforms from $SSZ$ to $Z$. Finally, the automaton use $(\varepsilon,Z;Z)$ to move to the accept state $q_{2}$. Since the machine finishes reading the input string, $aabb$ is an accepting word of this CFL. $aabbb$ is not an accepting word because, after reading $aabb$, the automaton is at state $q_{1}$ with the stack as $Z$, and there is no valid function to consume the final $b$ with $Z$ as the stack top.

## 4 The Formal-LLM Framework

### 4.1 Motivation and Challenge

Natural language is easily understandable by humans, but it may also lack precision in certain application scenarios. In contrast, formal language is defined in clear mathematical and machine readable forms, which could be hard to comprehend for humans, but it also possesses great precision. Thus, we aim to effectively integrate the benefits of both natural and formal languages through the pushdown automaton (PDA) for more controllable planning of LLM-based agents in scenarios requiring precise and valid plans.

Constructing our Formal-LLM framework presents non-trivial challenges. First, LLMs may struggle to directly comprehend and process formal language due to the limited CFG- or PDA-related corpus in pre-training data. Hence, natural language prompts are necessary to effectively describe the status of the automaton, bridging the gap between formal and natural language for LLM’s planning. Given that the automaton has the potential to generate an infinite number of plans, the second challenge arises because the automaton only ensures the validity and executability of the plans by adhering to defined constraints, but cannot guarantee the performance and optimality of the generated plan. Therefore, leveraging the solid natural language understanding ability of LLM and other methods, such as fine-tuning and backtracking mechanisms, becomes crucial for enhancing planning performance.

### 4.2 Formulating Constraint to Automaton

The overall flow of our framework is presented in Figure [1](https://arxiv.org/html/2402.00798v4#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). We begin by illustrating the process of converting constraints into formal language and automata for benchmark tasks, using the OpenAGI benchmark (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) as an example, which incorporates a set of domain expert models as tools and lists a series of intricate problems that cannot be addressed with a single tool. These tools are categorized into six primary groups according to the input and output modalities, as indicated in Table [3](https://arxiv.org/html/2402.00798v4#A1.T3 "Table 3 ‣ A.1 OpenAGI Benchmark Tasks and Tools ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") in the Appendix.

If the plans for the benchmark tasks generated by the LLM-based agent deviate from the expected data format, the execution performance degrades significantly or even results in errors. For instance, the Colorization tool operates on image data as input and output. Since image data is represented as a 3-D tensor, providing a 1-D tensor text string as input could lead to errors, due to incorrect data dimensions. Thus, the entire plan becomes invalid and cannot be executed. In such cases, natural language may struggle to articulate these constraints precisely and concisely, but formal language can express them straightforwardly.

The CFG in Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) outlines the constraints on data formats with Polish prefix notation: $T$ for text and $I$ for image, capital letters $A$ to $F$ for the six types of tools, and lowercase letters such as $a_{i}\in A$ or $f_{j}\in F$ for specific tools in the order of Table [3](https://arxiv.org/html/2402.00798v4#A1.T3 "Table 3 ‣ A.1 OpenAGI Benchmark Tasks and Tools ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents").

|  |  | $\displaystyle I\rightarrow AI&#124;CT\qquad$ |  | $\displaystyle T\rightarrow BI&#124;DT&#124;EIT&#124;FTT$ |  | (3) |
|  |  | $\displaystyle A\rightarrow a_{1}&#124;a_{2}&#124;a_{3}&#124;a_{4}\qquad$ |  | $\displaystyle B\rightarrow b_{1}&#124;b_{2}&#124;b_{3}$ |  |
|  |  | $\displaystyle C\rightarrow c_{1}\qquad$ |  | $\displaystyle D\rightarrow d_{1}&#124;d_{2}&#124;d_{3}&#124;d_{4}&#124;d_{5}$ |  |
|  |  | $\displaystyle E\rightarrow e_{1}\qquad$ |  | $\displaystyle F\rightarrow f_{1}$ |  |

where $I\rightarrow AI|CT$ means $I$ can be replaced by $AI$ or $CT$, because $A$ is an image-to-image tool and $C$ is a text-to-image tool. As a result, when $A$ is applied on an image ($AI$) or $C$ is applied on a text ($CT$), the result is still an image. Considering a specific task, such as “Given blurry gray-scale images, how to return the object names in English step by step?”, we know that the task’s inputs are images and the final outputs should be texts. To formalize this, we add two constraints to Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")):

|  |  | $\displaystyle S\rightarrow T\qquad$ |  | $\displaystyle I\rightarrow i$ |  | (4) |

i.e., the CFG starts with the symbol $T$, implying the final text format, and uses the lowercase letter $i$ to represent the input image. The combination of Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) and Eq.([4](https://arxiv.org/html/2402.00798v4#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) renders the data format constraints of the specific task. For instance, $e_{1}a_{1}ib_{1}i$ is an accepted word, representing a valid plan: “Utilize Image Classification ($b_{1}$) on the input image to obtain text and apply Colorization ($a_{1}$) on the input image to generate the output image. Then, employ Visual Question Answering ($e_{1}$) with the text and output image to derive the final text.” As mentioned in Section [3.2](https://arxiv.org/html/2402.00798v4#S3.SS2 "3.2 Pushdown Automaton ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"), the CFG can be equivalently converted into a PDA in Figure [3](https://arxiv.org/html/2402.00798v4#S3.F3 "Figure 3 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). These steps allow us to transform the constraints into automata for subsequent utilization.

In certain scenarios, creating an automaton can be more straightforward than formulating a formal language. Section [A.5.1](https://arxiv.org/html/2402.00798v4#A1.SS5.SSS1 "A.5.1 Daily Plan ‣ A.5 Prompt for Real-life Tasks ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") describes a real-life task for daily work planning. The constraints in this case revolve around time, allowing us to represent time as distinct states, as depicted in Figure [4](https://arxiv.org/html/2402.00798v4#S4.F4 "Figure 4 ‣ 4.3 Formal-LLM Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). The automaton initiates the design of an activity that concludes at 20:00, and a valid plan is generated when the automaton reaches 10:00 while being in a state where three meals have been consumed. In this case, we design a PDA without a CFG.

### 4.3 Formal-LLM Prompts and Planning from Automaton

In Section [4.2](https://arxiv.org/html/2402.00798v4#S4.SS2 "4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"), we convert the natural language planning constraints into a PDA, where any accepted word represents a valid and executable plan. The reason is that LLM could struggle to directly comprehend or process the CFG due to limited exposure to formal language during pre-training. Hence, we employ natural language prompts for LLM to comprehend the task and generate plans that can be easily read by humans; simultaneously, we use the PDA to guide the process of generating natural language plans.

The automaton initiates the plan generation process from the start state. In situations where multiple viable options are available for the automaton to proceed, a prompt is created to inquire the LLM, as shown in Figure [1](https://arxiv.org/html/2402.00798v4#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). This prompt encompasses a comprehensive {task description} to mitigate potential forgetting issues of the LLM (Hua et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib21)), details about the {current progress}, and {feasible choices} determined by the PDA. For example, when the PDA in Figure [3](https://arxiv.org/html/2402.00798v4#S3.F3 "Figure 3 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") is in state $q_{1}$ with the symbol $I$ at the top of the stack, three viable transits: $(\varepsilon,I;AI)$, $(\varepsilon,I;CT)$, and $(i,I;\varepsilon)$ are included in the {feasible choices} of the prompt. A prompt example is provided in Appendix [A.3](https://arxiv.org/html/2402.00798v4#A1.SS3 "A.3 Prompt for OpenAGI Benchmark Tasks ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents").

Another advantage of employing PDA for LLM-based agent planning is its ability to articulate plans that involve tools with many-input-single-output (e.g., Image-Text pair as input, Text as output), assuming the number of inputs of each tool is deterministic. To illustrate, we depict the derivation tree of the string $e_{1}a_{1}ib_{1}i$ in Figure [5](https://arxiv.org/html/2402.00798v4#S4.F5 "Figure 5 ‣ 4.3 Formal-LLM Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). Treating the tools $a_{1}$ and $b_{1}$ as functions, the string is functionally equivalent to $t=e_{1}\big{(}a_{1}(i),b_{1}(i)\big{)}$. When the PDA finishes the substitution of symbols $E$ and $I$, it can well know that the stack symbol $T$ is derived from $(\varepsilon,T;EIT)$ and matches the input of tool $e_{1}$, because they are pushed into the stack at the same time. Finally, we can identify the tools that will operate on this image/text data in the subsequent step and incorporate this information in the prompt, enabling the LLM to make informed decisions.

![Refer to caption](img/0bdc1f0742a7a9338c285f898ae92384.png)

Figure 4: The PDA includes 13 states, $\{10,11,...,19,20,Start,Fin\}$, with $Start$ as the start state and $\{Fin\}$ as the accept state set. The lowercase letter $a$ is for breakfast, $b$ is for lunch, $c$ is for supper, $d$ is for playing basketball, $e$ is for grocery shopping, $f$ is for house cleaning, $g$ is for homework, and $h$ is for laundry. The stack symbols $D,Z,E,N$ represent 3, 2, 1, and 0 meals remaining to plan, respectively. $\forall$ represents any stack symbol. The variable $i$ is enumerated from 20 to 11, $j$ is enumerated from 20 to 11 except 13, and $k$ is enumerated from 20 to 13.

![Refer to caption](img/4d4175851bb757661507f182e908cfc8.png)

Figure 5: The derivation tree for $e_{1}a_{1}ib_{1}i$ for the example benchmark task in Section [4.2](https://arxiv.org/html/2402.00798v4#S4.SS2 "4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents").

### 4.4 Reinforcement Learning from Task Feedback and Backtracking Mechanism

An accepting word of an automaton can be infinitely long. However, we anticipate that the plan proposed by the LLM-based agent can be executed within a finite number of steps. Consider the PDA in Figure [2](https://arxiv.org/html/2402.00798v4#S3.F2 "Figure 2 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"), any length of the string in the form of $a^{n}b^{n}$ where $n\geq 0$ is an accepting word. Similarly, in planning tasks, tools can be employed multiple times, such as applying the Text Summarization tool to the original text and infinitely repeating the same tool on the output. However, such planning behavior may be nonsensical and inefficient. Hence, for the tasks outlined in this paper, each tool is restricted to a single use for a given task, preventing infinite planning for both benchmark and real-life scenarios.

Upon imposing constraints on the tool usage, a new challenge emerges: the automaton may encounter a dead-end during plan generation. Consider the formal language in Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) and Eq.([4](https://arxiv.org/html/2402.00798v4#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")). Upon analyzing the language, we observe that the only means of transitioning from the symbol $T$ to $I$ (and subsequently to a terminal $i$) involve $B$-type (Image-in, Text-out) and $E$-type (Image-Text-pair-in, Text-out) tools. If these two tool types are exhausted, yet there are still $T$ symbols in the stack, the automaton will inevitably fail to reach the accept state $q_{2}$, as no methods remain to clear the symbol $T$. Finally, a dead-end situation may arise following the imposition of tool usage limits.

To address this challenge, we propose to document the automaton’s details at each step, encompassing the current state and stack, and the part of the word that has been generated. When the automaton confronts a dead-end, we initiate backtracking to the preceding steps until there is an unexplored branch. The backtracking involves retracing the automaton to the prior step, reinstating the automaton details based on the recorded stack, and eliminating the dead-end branch from the prior choice list. Thus, the automaton is capable and guaranteed to generate a valid plan if it exists.

The design above only guarantees the validity of the generated plans, but not the optimal plan. Thus, as an additional enhancement in plan quality, particularly when employing open-source LLMs, we integrate Reinforcement Learning from Task Feedback (RLTF) (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) following the application of our Formal-LLM framework. After the LLM-based agent generates a plan for a training task, the plan is executed on benchmark data to assess its performance. Then, the performance is used as a reward for RL to update the parameters of the LLM. Our framework ensures that invalid plans are always excluded during the LLM-based agent’s plan generation. Therefore, the number of valid rewards for LLM fine-tuning increases, and our approach can improve the performance of fine-tuned LLM-based agents.

## 5 Experiments

### 5.1 Backbone Large Language Model (LLM)

We test our Formal-LLM framework on these closed-source LLMs:

*   •

    GPT-3.5-turbo (Brown et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib2)) is a generative pre-trained transformer of OpenAI.

*   •

    Claude-2 (Claude-2, [2023](https://arxiv.org/html/2402.00798v4#bib.bib8)) is a transformer LLM of Anthropic.

*   •

    GPT-4 (OpenAI, [2023](https://arxiv.org/html/2402.00798v4#bib.bib30)) is a follow-up version of GPT-3.5.

and these open-source LLMs:

*   •

    Flan-T5-Large (Chung et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib7)) is a language model with 770 million parameters.

*   •

    Vicuna-7B (Chiang et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib6)) is a 7-billion size chatbot trained by fine-tuning the LLaMA model (Touvron et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib40)).

*   •

    LLaMA-2-13B (Touvron et al., [2023b](https://arxiv.org/html/2402.00798v4#bib.bib41)) is a successor to the 13-billion size LLaMA model.

### 5.2 Learning Schema of LLMs

We adopt the following LLM learning schema:

*   •

    Zero-shot Learning (Zero) directly inputs the prompt to the LLM.

*   •

    Few-shot Learning (Few) presents a set of high-quality examples in the prompt, each consisting of input and desired output on the target task.

*   •

    Reinforcement Learning from Task Feedback (RLTF) applies text constraint generation, executes the plan, and takes its performance as the reward to optimize the LLM by RL.

*   •

    Formal-LLM (F-LLM) is our proposed framework, using automaton to control planning.

*   •

    Formal-LLM plus RLTF (F-LLM+RLTF) applies our Formal-LLM framework on top of RLTF without text constraint generation. We use automaton to exclude invalid plans.

Specifically, we utilize Zero, Few, and F-LLM frameworks for closed-source LLMs, as these frameworks do not need to modify the LLM’s parameters. Regarding open-source LLMs, we compare RLTF, F-LLM and F-LLM+RLTF, given that RLTF surpasses Zero and Few, as exhaustively discussed in recent work (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)).

| Metrics / Task | GPT-3.5-turbo | Claude-2 | GPT-4 |
| Zero | Few | F-LLM (Ours) | Zero | Few | F-LLM (Ours) | Zero | Few | F-LLM (Ours) |
| % of Valid Plans | 29% | 71% | 100% | 29% | 47% | 100% | 53% | 76% | 100% |
| Task 1 (CLIP Score) | 0.0 | 0.0 | 0.3056 | 0.0 | 0.2543 | 0.3056 | 0.0 | 0.3055 | 0.3056 |
| Task 2 (BERT Score) | 0.1914 | 0.3820 | 0.6364 | 0.2111 | 0.5038 | 0.6275 | 0.2076 | 0.6307 | 0.5102 |
| Task 3 (ViT Score) | 0.2437 | 0.7497 | 0.6470 | 0.4082 | 0.5416 | 0.7137 | 0.5058 | 0.6480 | 0.7689 |
| Task X | 0.0 | 0.0 | 0.0658 | 0.0 | 0.0 | 0.2799 | 0.0 | 0.0 | 0.2876 |
| Average over tasks | 0.1443 | 0.3345 | 0.4846 | 0.1838 | 0.3773 | 0.5420 | 0.1992 | 0.4662 | 0.4914 |

Table 1: Benchmark task performances under different settings for three closed-source LLMs. Zero is for Zero-shot Learning, Few is for Few-shot Learning, and F-LLM is for Formal-LLM. The boldface numbers denote the highest score under each task type using the same LLM.

| Metrics / Tasks | Flan-T5-Large | Vicuna-7B | LLaMA-2-13B |
| RLTF | F-LLM | F-LLM+RLTF | RLTF | F-LLM | F-LLM+RLTF | RLTF | F-LLM | F-LLM+RLTF |
| % of Valid Plans | 24% | 100% | 100% | 29% | 100% | 100% | 47% | 100% | 100% |
| Task 1 (CLIP Score) | 0.0 | 0.3049 | 0.3049 | 0.0 | 0.3122 | 0.3139 | 0.0610 | 0.1601 | 0.3060 |
| Task 2 (BERT Score) | 0.3327 | 0.5164 | 0.5287 | 0.1475 | 0.4948 | 0.4673 | 0.1611 | 0.4220 | 0.5565 |
| Task 3 (ViT Score) | 0.6632 | 0.6264 | 0.7469 | 0.6958 | 0.5948 | 0.8618 | 0.7106 | 0.7043 | 0.6808 |
| Task X | 0.0 | 0.0728 | 0.4046 | 0.0 | 0.4127 | 0.4029 | 0.0 | 0.3846 | 0.4163 |
| Average over tasks | 0.3111 | 0.4451 | 0.5321 | 0.2009 | 0.4824 | 0.5162 | 0.3101 | 0.4498 | 0.5390 |

Table 2: Benchmark task performances under different settings for three open-source LLMs. RLTF is for Reinforcement Learning from Task Feedback, F-LLM is for Formal-LLM, and F-LLM+RLTF is for using the generated plans by F-LLM to calculate the reward for RLTF. The boldface numbers denote the highest score under each task type using the same LLM.

### 5.3 Experimentation Datasets

#### 5.3.1 Benchmark Datasets

We conduct experiments on two benchmark datasets, i.e., OpenAGI (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) and TravelPlanner (Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)). Due to space limit, we provide the results on OpenAGI in the following and provide the results on TravelPlanner in Appendix Section [A.4](https://arxiv.org/html/2402.00798v4#A1.SS4 "A.4 Formal-LLM on TravelPlanner Benchmark ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). The OpenAGI benchmark tasks are categorized based on their output type and ground-truth label type (Task 1, 2, and 3). Then, based on different task types, different metrics are employed to gauge the performance: CLIP Score (Hessel et al., [2021](https://arxiv.org/html/2402.00798v4#bib.bib18)), assessing the similarity between text and image, is utilized for Text-to-Image tasks; BERT Score (Zhang et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib52)), evaluating text generation with BERT, is applied when both data labels and the expected outputs are texts; and ViT Score (Wu et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib45)) gauges the similarity between the image label and image output. Additionally, we construct Task X, which is a subset of “Task 1 $\cup$ Task 2 $\cup$ Task 3” that requires a tree-structured plan due to the use of many-input-single-ouput tools, such as Question Answering. Task X is used to test the complex planning ability of our Formal-LLM framework, and is evaluated using the corresponding metric. The tools are detailed in Table [3](https://arxiv.org/html/2402.00798v4#A1.T3 "Table 3 ‣ A.1 OpenAGI Benchmark Tasks and Tools ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") and example tasks of each category are in Table [4](https://arxiv.org/html/2402.00798v4#A1.T4 "Table 4 ‣ A.3 Prompt for OpenAGI Benchmark Tasks ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") in the Appendix.

#### 5.3.2 Real-life Practical Tasks

We also experiment with real-life planning scenarios, encompassing daily plans, cooking recipes, and risk management, where validity and rationality are crucial. In these scenarios, the concept of tool is generalized to include various types of steps in a plan, such as events, actions, or activities, as they can take diverse forms supporting the execution of the plan. For example, a tool in the daily planning task may be an activity such as taking breakfast. We provide qualitative analyses between the Zero and F-LLM learning schema using GPT-4 backbone for practical tasks. We use GPT-4 for the experiment of practical tasks because other LLMs can hardly generate readable plans for these tasks, and we test under Zero and F-LLM due to the limited sample size and that these two learning frameworks do not require accessing to the LLM parameters.

### 5.4 Experimental Analysis

The experiment results on the benchmark tasks are shown in Table [1](https://arxiv.org/html/2402.00798v4#S5.T1 "Table 1 ‣ 5.2 Learning Schema of LLMs ‣ 5 Experiments ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") and Table [2](https://arxiv.org/html/2402.00798v4#S5.T2 "Table 2 ‣ 5.2 Learning Schema of LLMs ‣ 5 Experiments ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"), referring to the closed-source and open-source LLMs, respectively. Each row stands for a type of task, each column represents the learning schema of an LLM-based agent, and every three columns are the results of the same LLM. For closed-source LLMs, among the three learning schemata without revising the LLM parameter, almost all the best scores under each type of task belong to our F-LLM framework. For open-source LLMs, our F-LLM without fine-tuning is already better than RLTF (the best schema in the OpenAGI platform (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12))) in most cases, except for the ViT Score (Task 3), because the Type-3 tasks take the most significant portion of the RLTF’s fine-tuning data, and thus RLTF is adequately optimized on these tasks. The performance gain of our F-LLM framework comes from the 100% executable plans and supporting tree-structured planning for the difficult tasks in Task X. As a comparison, the best open-source LLM (LLaMA-2-13B with RLTF) can only generate 47% executable plans, and the best closed-source LLM (GPT-4 with few shot) can generate 76% executable plans. Additionally, none of the baselines can handle the Task X (Score = 0.0). Due to the increased amounts of valid rewards from the 100% executable plans with our framework, the F-LLM+RLTF approach enables boosted performance, as demonstrated by its better scores in Table [2](https://arxiv.org/html/2402.00798v4#S5.T2 "Table 2 ‣ 5.2 Learning Schema of LLMs ‣ 5 Experiments ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). To summarize, the benchmark experiment results show that our Formal-LLM framework is an effective method to integrate the benefits of both natural and formal languages for more controllable and valid planning of LLM-based agents.

### 5.5 Case Study

We show the complete results of real-life examples by applying our Formal-LLM to the GPT models in the Appendix. From the results, after applying our Formal-LLM, the generated plan from the GPT-based agent is more complete, reasonable and specific to the case. In the daily plan example, the agent fails to fit all the activities into the plan without the Formal-LLM, while it can achieve this goal by applying the Formal-LLM. Still take Figure [4](https://arxiv.org/html/2402.00798v4#S4.F4 "Figure 4 ‣ 4.3 Formal-LLM Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") as an example, our Formal-LLM can limit the planning to 10:00$\sim$20:00, while without Formal-LLM, the agent schedules activities after 20:00, even if we mention the constraint “Generate a plan for all the activities between 10:00 and 20:00” in natural language, which shows the advantages of formal language guided planning. For the cooking recipe example, without strict automaton constraint, GPT may generate steps such as “stir-fry the Chinese broccoli until it changes color” after “once the water boils, add the Chinese broccoli”. This is unreasonable because broccoli cannot be stir-fried in boiling water. For the risk management example, the generated plan without our framework is too general that it could be used for any other two companies. However, the plan after applying Formal-LLM is more specific and focuses on the potential antitrust risk between Microsoft and Blizzard. Thus, the Formal-LLM can use the automaton describing the constraints to generate higher-quality plans.

## 6 Conclusions and Future Work

In this study, we introduce the innovative Formal-LLM framework for LLM-based agents, combining LLM’s power of natural language comprehension with the precision of formal language. Our experiments, encompassing benchmark tasks and real-life practical scenarios, affirm the feasibility and effectiveness of employing automaton to control the agent’s generation of valid plans. More controllable LLM-based agents can augment the potential utilization of LLM in applications where high validity of planning is important.

Our work has several potential extensions. First, automating the translation of natural language into formal language for agents could further improve the framework.Additionally, this work focuses on LLM plan generation based on formal language. Another important problem to explore is LLM plan verification based on formal language.

## Acknowledgment

We thank Harun Taha Kepenek for providing helpful suggestions to improve this research paper.

## References

*   Boiko et al. (2023) Daniil A. Boiko, Robert MacKnight, and Gabe Gomes. 2023. [Emergent autonomous scientific research capabilities of large language models](https://arxiv.org/abs/2304.05332). *Preprint*, arXiv:2304.05332.
*   Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877–1901.
*   Carion et al. (2020) Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In *European conference on computer vision*, pages 213–229\. Springer.
*   Carlsson et al. (2022) Fredrik Carlsson, Joey Öhman, Fangyu Liu, Severine Verlinden, Joakim Nivre, and Magnus Sahlgren. 2022. Fine-grained controllable text generation using non-residual prompting. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 6837–6857.
*   Chen et al. (2024) Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, and Zhendong Mao. 2024. [Benchmarking large language models on controllable generation under diversified instructions](https://arxiv.org/abs/2401.00690). *Preprint*, arXiv:2401.00690.
*   Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
*   Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. [Scaling instruction-finetuned language models](https://arxiv.org/abs/2210.11416). *Preprint*, arXiv:2210.11416.
*   Claude-2 (2023) Claude-2\. 2023. Model card and evaluations for claude models.
*   Conde et al. (2022) Marcos V Conde, Ui-Jin Choi, Maxime Burchi, and Radu Timofte. 2022. Swin2sr: Swinv2 transformer for compressed image super-resolution and restoration. In *European Conference on Computer Vision*, pages 669–687.
*   Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. *ICLR*.
*   Franceschelli and Musolesi (2023) Giorgio Franceschelli and Mirco Musolesi. 2023. [On the creativity of large language models](https://arxiv.org/abs/2304.00008). *Preprint*, arXiv:2304.00008.
*   Ge et al. (2023a) Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, and Yongfeng Zhang. 2023a. OpenAGI: When LLM meets domain experts. In *Thirty-seventh Conference on Neural Information Processing Systems*.
*   Ge et al. (2023b) Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. 2023b. Llm as os, agents as apps: Envisioning aios, agents and the aios-agent ecosystem. *arXiv*.
*   Geng et al. (2024) Saibo Geng, Berkay Döner, Chris Wendler, Martin Josifoski, and Robert West. 2024. Sketch-guided constrained decoding for boosting blackbox large language models without logit access. *arXiv preprint arXiv:2401.09967*.
*   Geng et al. (2023) Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-constrained decoding for structured nlp tasks without finetuning. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 10932–10952.
*   Gu et al. (2022) Yuxuan Gu, Xiaocheng Feng, Sicheng Ma, Lingyuan Zhang, Heng Gong, and Bing Qin. 2022. [A distributional lens for multi-aspect controllable text generation](https://doi.org/10.18653/v1/2022.emnlp-main.67). In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 1023–1043, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
*   Hemmer et al. (2023) Arthur Hemmer, Mickael Coustaty, Nicola Bartolo, Jerome Brachat, and Jean-Marc Ogier. 2023. Lazy-k decoding: Constrained decoding for information extraction. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 6727–6736.
*   Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: a reference-free evaluation metric for image captioning.
*   Hopcroft et al. (2001) John E Hopcroft, Rajeev Motwani, and Jeffrey D Ullman. 2001. Introduction to automata theory, languages, and computation. *Acm Sigact News*, 32(1):60–65.
*   Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. [Lora: Low-rank adaptation of large language models](https://arxiv.org/abs/2106.09685). *Preprint*, arXiv:2106.09685.
*   Hua et al. (2023) Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. 2023. War and peace (waragent): Large language model-based multi-agent simulation of world wars. *arXiv:2311.17227*.
*   Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In *Proceedings of the 39th International Conference on Machine Learning*, volume 162 of *Proceedings of Machine Learning Research*, pages 9118–9147\. PMLR.
*   Lewis et al. (2019) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. [Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension](https://arxiv.org/abs/1910.13461). *Preprint*, arXiv:1910.13461.
*   Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for "mind" exploration of large language model society. In *Thirty-seventh Conference on Neural Information Processing Systems*.
*   Li et al. (2020) Lei Li, Li Chen, and Yongfeng Zhang. 2020. Towards controllable explanation generation for recommender systems via neural template. In *Companion proceedings of the web conference 2020*, pages 198–202.
*   Li et al. (2023b) Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. 2023b. Trocr: transformer-based optical character recognition with pre-trained models. In *Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence*, pages 13094–13102.
*   Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. [Agentbench: Evaluating llms as agents](https://arxiv.org/abs/2308.03688). *Preprint*, arXiv:2308.03688.
*   Lu et al. (2022) Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. *Advances in neural information processing systems*, 35:27591–27609.
*   Mei et al. (2024) Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. 2024. Aios: Llm agent operating system. *arXiv*.
*   OpenAI (2023) Josh et al OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774). *Preprint*, arXiv:2303.08774.
*   Qian et al. (2023) Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. [Communicative agents for software development](https://arxiv.org/abs/2307.07924). *Preprint*, arXiv:2307.07924.
*   Qin et al. (2022) Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. Cold decoding: Energy-based constrained text generation with langevin dynamics. *Advances in Neural Information Processing Systems*, 35:9538–9551.
*   Qin et al. (2023) Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023. [Tool learning with foundation models](https://arxiv.org/abs/2304.08354). *Preprint*, arXiv:2304.08354.
*   Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. *OpenAI blog*, 1(8):9.
*   Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. *The Journal of Machine Learning Research*, 21(1):5485–5551.
*   Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 10684–10695.
*   Sanh et al. (2020) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. [Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). *Preprint*, arXiv:1910.01108.
*   Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. [Toolformer: Language models can teach themselves to use tools](https://arxiv.org/abs/2302.04761). *Preprint*, arXiv:2302.04761.
*   Takase and Okazaki (2019) Sho Takase and Naoaki Okazaki. 2019. Positional encoding to control output sequence length. In *Proceedings of the 2019 Conference of the North*. Association for Computational Linguistics.
*   Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971). *Preprint*, arXiv:2302.13971.
*   Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. [Llama 2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288). *Preprint*, arXiv:2307.09288.
*   Wang et al. (2022) Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022. [Git: A generative image-to-text transformer for vision and language](https://arxiv.org/abs/2205.14100). *Preprint*, arXiv:2205.14100.
*   Wang et al. (2023) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023. [A survey on large language model based autonomous agents](https://arxiv.org/abs/2308.11432). *Preprint*, arXiv:2308.11432.
*   Williams (1992) Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. *Machine learning*, 8:229–256.
*   Wu et al. (2020) Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. 2020. [Visual transformers: Token-based image representation and processing for computer vision](https://arxiv.org/abs/2006.03677). *Preprint*, arXiv:2006.03677.
*   Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui. 2023. [The rise and potential of large language model based agents: A survey](https://arxiv.org/abs/2309.07864). *Preprint*, arXiv:2309.07864.
*   Xie et al. (2024) Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024. Travelplanner: A benchmark for real-world planning with language agents. *arXiv preprint arXiv:2402.01622*.
*   Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In *International Conference on Learning Representations (ICLR)*.
*   Yuan et al. (2024) Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. 2024. Easytool: Enhancing llm-based agents with concise tool instruction. *arXiv preprint arXiv:2401.06201*.
*   Zamir et al. (2022) Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. 2022. Restormer: Efficient transformer for high-resolution image restoration. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 5728–5739.
*   Zhang et al. (2017) Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A Efros. 2017. Real-time user-guided image colorization with learned deep priors. *ACM Transactions on Graphics (TOG)*, 36(4):1–11.
*   Zhang et al. (2020) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert.

¹¹footnotetext: [https://github.com/richzhang/colorization](https://github.com/richzhang/colorization)²²footnotetext: [https://huggingface.co/caidas/swin2SR-classical-sr-x2-64](https://huggingface.co/caidas/swin2SR-classical-sr-x2-64)³³footnotetext: [https://github.com/swz30/Restormer](https://github.com/swz30/Restormer)⁴⁴footnotetext: [https://huggingface.co/google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)⁵⁵footnotetext: [https://huggingface.co/facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)⁶⁶footnotetext: [https://huggingface.co/nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)⁷⁷footnotetext: [https://huggingface.co/CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)⁸⁸footnotetext: [https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)⁹⁹footnotetext: [https://huggingface.co/facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)^(10)^(10)footnotetext: [https://huggingface.co/t5-base](https://huggingface.co/t5-base)^(11)^(11)footnotetext: [https://huggingface.co/distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)^(12)^(12)footnotetext: [https://huggingface.co/gpt2](https://huggingface.co/gpt2)^(13)^(13)footnotetext: [https://huggingface.co/microsoft/git-base-textvqa](https://huggingface.co/microsoft/git-base-textvqa)^(14)^(14)footnotetext: [https://huggingface.co/distilbert-base-cased-distilled-squad](https://huggingface.co/distilbert-base-cased-distilled-squad)

## Appendix A Appendix

### A.1 OpenAGI Benchmark Tasks and Tools

The tool list integrated in the OpenAGI platform (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) is shown in Table [3](https://arxiv.org/html/2402.00798v4#A1.T3 "Table 3 ‣ A.1 OpenAGI Benchmark Tasks and Tools ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") and the different types of tasks as well as example tasks are shown in Table [4](https://arxiv.org/html/2402.00798v4#A1.T4 "Table 4 ‣ A.3 Prompt for OpenAGI Benchmark Tasks ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents").

| Input Modality | Output Modality | Tool Description | Expert Model |
| Image | Image ($A$) | Colorization ($a_{1}$) | Colorizer[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex1 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Zhang et al., [2017](https://arxiv.org/html/2402.00798v4#bib.bib51)) |
| Super-Resolution ($a_{2}$) | Swin2SR[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex2 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Conde et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib9)) |
| Image Denoising ($a_{3}$) | Restormer[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex3 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Zamir et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib50)) |
| Image Deblurring ($a_{4}$) | Restormer[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex3 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Zamir et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib50)) |
| Text ($B$) | Image Classification ($b_{1}$) | ViT[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex4 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Dosovitskiy et al., [2021](https://arxiv.org/html/2402.00798v4#bib.bib10)) |
| Object Detection ($b_{2}$) | DETR[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex5 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Carion et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib3)) |
| Image Captioning ($b_{3}$) | Vision Encoder Decoder[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex6 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Li et al., [2023b](https://arxiv.org/html/2402.00798v4#bib.bib26)) |
| Text | Image ($C$) | Text-to-Image Generation ($c_{1}$) | Stable Diffusion[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex7 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Rombach et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib36)) |
| Text ($D$) | Sentiment Analysis ($d_{1}$) | DistilBERT[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex8 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Sanh et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib37)) |
| Text Summarization ($d_{2}$) | BART[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex9 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Lewis et al., [2019](https://arxiv.org/html/2402.00798v4#bib.bib23)) |
| Machine Translation ($d_{3}$) | T5[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex10 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Raffel et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib35)) |
| Fill Mask ($d_{4}$) | DistilBERT[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex11 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Sanh et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib37)) |
| Text Generation ($d_{5}$) | GPT-2[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex12 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Radford et al., [2019](https://arxiv.org/html/2402.00798v4#bib.bib34)) |
| Image-Text Pair | Text ($E$) | Visual Question Answering ($e_{1}$) | GIT[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex13 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Wang et al., [2022](https://arxiv.org/html/2402.00798v4#bib.bib42)) |
| Text-Text Pair | Text ($F$) | Question Answering ($f_{1}$) | DistilBERT[Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents](https://arxiv.org/html/2402.00798v4#footnotex14 "Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") (Sanh et al., [2020](https://arxiv.org/html/2402.00798v4#bib.bib37)) |

Table 3: Tool list integrated in OpenAGI platform (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) for benchmark tasks. The tools are categorized into six primary groups according to the input and output modalities.

### A.2 Implementation Details

Our framework and all baselines are implemented by PyTorch, an open-source library. We follow the implementation setting of the OpenAGI platform (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) for baselines. For the F-LLM+RLTF framework, we use the REINFORCE (Williams, [1992](https://arxiv.org/html/2402.00798v4#bib.bib44)) as the core reinforcement learning algorithm of RLTF. We use the original checkpoint of each backbone LLM without supervised fine-tuning. We set the maximum number of updating epochs at 30 and use Adam as the optimizer with the learning rate at 0.001 for RLTF. Also, we apply Low-Rank Adaptation (LoRA) (Hu et al., [2021](https://arxiv.org/html/2402.00798v4#bib.bib20)) to the RLTF for efficient fine-tuning with the rank as 8 and the placement as $v_{proj}$ ($W_{v}$) and $q_{proj}$ ($W_{q}$). Our experimental results are the average of five runs.

### A.3 Prompt for OpenAGI Benchmark Tasks

An example of the CFG and the corresponding PDA are shown in Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) and Eq.([4](https://arxiv.org/html/2402.00798v4#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) as well as Figure [3](https://arxiv.org/html/2402.00798v4#S3.F3 "Figure 3 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). Different benchmark tasks share a common subset of constraints shown in Eq.([3](https://arxiv.org/html/2402.00798v4#S4.E3 "Equation 3 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) corresponding to the input-output constraints of the tools. Besides, each task has its own constraint on the input-output of the task, resulting in different constraints in Eq.([4](https://arxiv.org/html/2402.00798v4#S4.E4 "Equation 4 ‣ 4.2 Formulating Constraint to Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")), and thus the final PDA (Figure [3](https://arxiv.org/html/2402.00798v4#S3.F3 "Figure 3 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")) for different tasks are different. Also, we directly use the tool name (e.g., Colorization) instead of the category name (e.g., “Image-in, Image-out tools”) in the prompt. For example, consider the PDA in Figure [3](https://arxiv.org/html/2402.00798v4#S3.F3 "Figure 3 ‣ 3.1 Context-free Grammar ‣ 3 Preliminary Knowledge ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") when it is in state $q_{1}$ with the symbol $I$ at the top of the stack. There are three viable transits: $(\varepsilon,I;AI)$, $(\varepsilon,I;CT)$, and $(i,I;\varepsilon)$. In practice, as nonterminals $A$ and $C$ are promptly replaced by $terminals$ from $a_{1}\sim a_{4}$ and $c_{1}$. This is because the tool name provides more concrete information for the LLM to comprehend the tool’s functionality, enabling the LLM to make informed decisions without influencing the plan’s executability.

Zero-shot Prompt:

[⬇](data:text/plain;base64,IFByb2JsZW06IHt0YXNrX2Rlc2NyaXB0aW9ufS4KIFdoYXQgaXMgaXRzIHNvbHR1aW9uPyBVc2UgJ1NldHAnIHRvIG1hcmsu)Problem:  {task_description}.What  is  its  soltuion?  Use  ’Setp’  to  mark.

Few-shot Prompt:

[⬇](data:text/plain;base64,IHtmZXcgc2hvdCBleGFtcGxlcyBpbiB0aGUgZm9ybWF0IG9mOgogICAgewogICAgICAgIFByb2JsZW06IHt0YXNrX2Rlc2NyaXB0aW9ufS4KICAgICAgICBTb2x1dGlvbjoKICAgICAgICBTdGVwIDE6IC4uLgogICAgICAgIFN0ZXAgMjogLi4uCiAgICAgICAgLi4uCiAgICAgICAgU3RlcCBrOiAuLi4KICAgIH0KIH0KIFByb2JsZW06IHt0YXNrX2Rlc2NyaXB0aW9ufS4KIFNvbHV0aW9uOg==){few  shot  examples  in  the  format  of:{Problem:  {task_description}.Solution:Step  1:  ...Step  2:  ......Step  k:  ...}}Problem:  {task_description}.Solution:

RLTF Prompt (RLTF executes the solution and use the performance as reward to fine-tune the LLM):

[⬇](data:text/plain;base64,IFByb2JsZW06IHt0YXNrX2Rlc2NyaXB0aW9ufS4KIFNvbHV0aW9uOg==)Problem:  {task_description}.Solution:

Formal-LLM Prompt (zero-shot):

[⬇](data:text/plain;base64,WW91IHdpbGwgaGVscCBtZSBnZW5lcmF0ZSBhIHBsYW4gZm9yIHRoZSBwcm9ibGVtOiB7dGFza19kZXNjcmlwdGlvbn0gYnkgYW5zd2VyaW5nIGEgc2VyaWVzIG9mIG15IHF1ZXN0aW9ucy4KCntjdXJyZW50X3Byb2dyZXNzfQoKVG8gZ2V0IHRoZSB7ZGF0YV9tb2RhbGl0eX0sIHdlIGhhdmUgdGhlIGZvbGxvd2luZyBjaG9pY2VzOgoKe2Nob2ljZV9saXN0fQoKWW91ciBhbnN3ZXIgc2hvdWxkIGJlIG9ubHkgYW4gaW50ZWdlciwgcmVmZXJyaW5nIHRvIHRoZSBkZXNpcmVkIGNob2ljZS4=)You  will  help  me  generate  a  plan  for  the  problem:  {task_description}  by  answering  a  series  of  my  questions.{current_progress}To  get  the  {data_modality},  we  have  the  following  choices:{choice_list}Your  answer  should  be  only  an  integer,  referring  to  the  desired  choice.

Formal-LLM + RLTF (execute the solution of Formal-LLM and use the performance as reward to fine-tune the LLM):

[⬇](data:text/plain;base64,WW91IHdpbGwgaGVscCBtZSBnZW5lcmF0ZSBhIHBsYW4gZm9yIHRoZSBwcm9ibGVtOiB7dGFza19kZXNjcmlwdGlvbn0gYnkgYW5zd2VyaW5nIGEgc2VyaWVzIG9mIG15IHF1ZXN0aW9ucy4KCntjdXJyZW50X3Byb2dyZXNzfQoKVG8gZ2V0IHRoZSB7ZGF0YV9tb2RhbGl0eX0sIHdlIGhhdmUgdGhlIGZvbGxvd2luZyBjaG9pY2VzOgoKe2Nob2ljZV9saXN0fQoKWW91ciBhbnN3ZXIgc2hvdWxkIGJlIG9ubHkgYW4gaW50ZWdlciwgcmVmZXJyaW5nIHRvIHRoZSBkZXNpcmVkIGNob2ljZS4=)You  will  help  me  generate  a  plan  for  the  problem:  {task_description}  by  answering  a  series  of  my  questions.{current_progress}To  get  the  {data_modality},  we  have  the  following  choices:{choice_list}Your  answer  should  be  only  an  integer,  referring  to  the  desired  choice.

Formal-LLM Prompt Example:

[⬇](data:text/plain;base64,WW91IHdpbGwgaGVscCBtZSBnZW5lcmF0ZSBhIHBsYW4gZm9yIHRoZSBwcm9ibGVtOiAiR2l2ZW4gYSBncmF5c2NhbGUgaW1hZ2UsIGhvdyB0byByZXR1cm4gdGhlIHJlZ3VsYXIgaW1hZ2Ugc3RlcCBieSBzdGVwPyIgYnkgYW5zd2VyaW5nIGEgc2VyaWVzIG9mIG15IHF1ZXN0aW9ucy4KCkN1cnJlbnQgUHJvZ3Jlc3M6CgpTdGVwIG46IFVzZSBJbWFnZSBTdXBlciBSZXNvbHV0aW9uOwpTdGVwIChuLTEpOiA/CgpUbyBnZXQgdGhlIGlucHV0IGltYWdlIG9mICJJbWFnZSBTdXBlciBSZXNvbHV0aW9uIiwgd2UgaGF2ZSB0aGUgZm9sbG93aW5nIGNob2ljZXM6CgoxOiB0aGUgb3V0cHV0IG9mIENvbG9yaXphdGlvbiwKMjogdGhlIG91dHB1dCBvZiBJbWFnZSBEZW5vaXNpbmcsCjM6IHRoZSBvdXRwdXQgb2YgSW1hZ2UgRGVibHVycmluZywKNDogdGhlIG91dHB1dCBvZiBUZXh0IHRvIEltYWdlIEdlbmVyYXRpb24sCjU6IElucHV0IEltYWdlLgoKWW91ciBhbnN3ZXIgc2hvdWxkIGJlIG9ubHkgYW4gaW50ZWdlciwgcmVmZXJyaW5nIHRvIHRoZSBkZXNpcmVkIGNob2ljZS4=)You  will  help  me  generate  a  plan  for  the  problem:  "Given  a  grayscale  image,  how  to  return  the  regular  image  step  by  step?"  by  answering  a  series  of  my  questions.Current  Progress:Step  n:  Use  Image  Super  Resolution;Step  (n-1):  ?To  get  the  input  image  of  "Image  Super  Resolution",  we  have  the  following  choices:1:  the  output  of  Colorization,2:  the  output  of  Image  Denoising,3:  the  output  of  Image  Deblurring,4:  the  output  of  Text  to  Image  Generation,5:  Input  Image.Your  answer  should  be  only  an  integer,  referring  to  the  desired  choice.

| Task | Metrics | Output | Label | Evaluation | Task Example |
| Task 1 | CLIP Score | Image | Text | Text-to-Image similarity | Given clozed English text, |
| how to generate an image |
| step by step? |
| Task 2 | BERT Score | Text | Text | Text-to-Text similarity | Given noisy grayscale image, |
| how to return the caption in |
| German step by step? |
| Task 3 | ViT Score | Image | Image | Image-to-Image similarity | Given blurry grayscale image, |
| how to return the regular |
| image step by step? |
| Task X | Corresponding Score | / | / | / | Given low-resolution noisy |
| blurry grayscale image and |
| English query, how to |
| answer the question in |
| German step by step? |

Table 4: Benchmark task examples under each category. Task X is a subset of “Task 1 $\cup$ Task 2 $\cup$ Task 3”, which is a subset of tasks that require a tree-structured plan rather than chain-structured plan due to the use of many input-single-output tools. Task X is used to test the complex planning ability of our Formal-LLM framework.

### A.4 Formal-LLM on TravelPlanner Benchmark

We apply our Formal-LLM framework to the TravelPlanner (Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)) platform. TravelPlanner is a benchmark designed to assess language agents’ performance in tool use and complex planning under various constraints. We use GPT-4 as the backbone LLM because TravelPlanner’s experiments show that other LLMs cannot generate valid plans that meet all constraints. We use the workflow in Figure [6](https://arxiv.org/html/2402.00798v4#A1.F6 "Figure 6 ‣ A.4.2 Baseline ‣ A.4 Formal-LLM on TravelPlanner Benchmark ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents") as the automaton to control the agent planning in the Formal-LLM framework. The flowchart is essentially a Deterministic Finite Automaton (DFA), which is a special case of Pushdown Automaton (PDA). We provide an introduction to the experiment in the following.

#### A.4.1 Evaluation Metrics

*   •

    Delivery Rate: This metric assesses whether agents can successfully deliver a final plan.

*   •

    Commonsense Constraint Pass Rate: This metric evaluates whether a plan satisfies the pre-defined eight commonsense constraints.

*   •

    Hard Constraint Pass Rate: This metric measures whether a plan meets all explicit constraints in the given query.

*   •

    Final Pass Rate: This metric represents the proportion of the plans that meet all aforementioned constraints among all tested plans.

Besides, for the Commonsense Constraint Pass Rate and Hard Constraint Pass Rate, we report the Micro Pass Rate, calculating the ratio of satisfied constraints to the total number of constraints, and the Macro Pass Rate, calculating the ratio of plans that meet all commonsense and hard constraints among all tested plans.

#### A.4.2 Baseline

We utilize GPT-4 as the backbone LLM and adopt the ReAct (Yao et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib48)) framework as our baseline setting, as it demonstrated the highest performance in previous experiments (Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)).

![Refer to caption](img/cf1f516f642071f37736ab412f299edd.png)

Figure 6: The flowchart for TravelPlanner benchmark.

| Planning Strategy | Delivery Rate | Commonsense Pass Rate | Hard Constraint Pass Rate | Final Pass Rate |
|  |  | Micro | Macro | Micro | Macro |  |
| ReAct (Yao et al., [2023](https://arxiv.org/html/2402.00798v4#bib.bib48)) | 93.1 | 63.3 | 2.0 | 10.5 | 5.5 | 0.6 |
| Formal-LLM (Ours) | $100$ | $82.9$ | $16.7$ | $41.3$ | $31.7$ | $9.2$ |

Table 5: Experiment results on the testing dataset of TravelPlanner (Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)) benchmark when using GPT-4 as the backbone LLM. The best results are marked in bold.

#### A.4.3 Experiment Result

The experiment results on the TravelPlanner benchmark are shown in Table [5](https://arxiv.org/html/2402.00798v4#A1.T5 "Table 5 ‣ A.4.2 Baseline ‣ A.4 Formal-LLM on TravelPlanner Benchmark ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). Each row stands for a planning strategy and each column represents an evaluation metric. The results show that our Formal-LLM is better than the best baseline in every metric, validating that our Formal-LLM can generate more reasonable plans. Also, unlike the experiments on the OpenAGI platform, Formal-LLM does not generate 100% valid plans on the TravelPlanner benchmark, due to the task complexity and automaton quality. However, the generated plans meet the constraints described in the automaton in Figure [6](https://arxiv.org/html/2402.00798v4#A1.F6 "Figure 6 ‣ A.4.2 Baseline ‣ A.4 Formal-LLM on TravelPlanner Benchmark ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). We can improve the automaton quality for TravelPlanner and report even better performances in the future.

### A.5 Prompt for Real-life Tasks

#### A.5.1 Daily Plan

The automaton is displayed in Figure [4](https://arxiv.org/html/2402.00798v4#S4.F4 "Figure 4 ‣ 4.3 Formal-LLM Prompts and Planning from Automaton ‣ 4 The Formal-LLM Framework ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents").

GPT-4 Prompt:

[⬇](data:text/plain;base64,R2VuZXJhdGUgYSBwbGFuIGZvciBhY3Rpdml0aWVzIGJldHdlZW4gMTA6MDAgYW5kIDIwOjAwLgoKQnJlYWtmYXN0LCBsdW5jaCwgYW5kIHN1cHBlciBuZWVkIDEgaG91ciBlYWNoLgoKT3V0ZG9vciBhY3Rpdml0aWVzOiBiYXNrZXRiYWxsIHBsYXlpbmcgMTM6MDAgLSAxNTowMDsgZG8gZ3JvY2VyeSBzaG9wcGluZyBuZWVkcyAxIGhvdXIuCgpJbmRvb3IgYWN0aXZpdGllczogaG91c2UgY2xlYW5pbmcgbmVlZHMgMSBob3VyOyBob21ld29yayBuZWVkcyB0aHJlZSBob3VyczsgdHVybmluZyBvbiB0aGUgd2FzaGVyL2xhdW5kcnkgbWFjaGluZSBuZWVkcyAwIG1pbnV0ZXMgYnV0IG5lZWRzIHRvIHN0YXkgaG9tZSBmb3Igb25lIGhvdXIuCgpPdGhlciBjb25zdHJhaW46CkNhbm5vdCBwbGF5IGJhc2tldGJhbGwgd2l0aGluIGFuIGhvdXIgYWZ0ZXIgYSBtZWFsLg==)Generate  a  plan  for  activities  between  10:00  and  20:00.Breakfast,  lunch,  and  supper  need  1  hour  each.Outdoor  activities:  basketball  playing  13:00  -  15:00;  do  grocery  shopping  needs  1  hour.Indoor  activities:  house  cleaning  needs  1  hour;  homework  needs  three  hours;  turning  on  the  washer/laundry  machine  needs  0  minutes  but  needs  to  stay  home  for  one  hour.Other  constrain:Cannot  play  basketball  within  an  hour  after  a  meal.

Formal-LLM Prompt:

[⬇](data:text/plain;base64,R2VuZXJhdGUgYSBwbGFuIGZvciBhY3Rpdml0aWVzIGJldHdlZW4gMTA6MDAgYW5kIDIwOjAwLgoKQnJlYWtmYXN0LCBsdW5jaCwgYW5kIHN1cHBlciBuZWVkIDEgaG91ciBlYWNoLgoKT3V0ZG9vciBhY3Rpdml0aWVzOiBiYXNrZXRiYWxsIHBsYXlpbmcgMTM6MDAgLSAxNTowMDsgZG8gZ3JvY2VyeSBzaG9wcGluZyBuZWVkcyAxIGhvdXIuCgpJbmRvb3IgYWN0aXZpdGllczogaG91c2UgY2xlYW5pbmcgbmVlZHMgMSBob3VyOyBob21ld29yayBuZWVkcyB0aHJlZSBob3VyczsgdHVybmluZyBvbiB0aGUgd2FzaGVyL2xhdW5kcnkgbWFjaGluZSBuZWVkcyAwIG1pbnV0ZXMgYnV0IG5lZWRzIHRvIHN0YXkgaG9tZSBmb3Igb25lIGhvdXIuCgpPdGhlciBjb25zdHJhaW46CkNhbm5vdCBwbGF5IGJhc2tldGJhbGwgd2l0aGluIGFuIGhvdXIgYWZ0ZXIgYSBtZWFsLgoKTGV0J3Mgc3RhcnQgcGxhbm5pbmcgZnJvbSB0aGUgZW5kLgoKe2N1cnJlbnRfcHJvZ3Jlc3N9CgpEZWNpZGUgb24gdGhlIGFjdGl2aXR5IGVuZGluZyBhdCB7Y3VycmVudF9ob3VyfTowMC4KCkhlcmUgYXJlIHBvc3NpYmxlIG9wdGlvbnM6Cgp7Y2hvaWNlX2xpc3R9CgpZb3VyIHJlcGx5IHNob3VsZCBiZSBvbmx5IG9uZSBudW1iZXIsIHN1Y2ggYXMgMSwgcmVmZXJyaW5nIHRvIHRoZSBvcHRpb24u)Generate  a  plan  for  activities  between  10:00  and  20:00.Breakfast,  lunch,  and  supper  need  1  hour  each.Outdoor  activities:  basketball  playing  13:00  -  15:00;  do  grocery  shopping  needs  1  hour.Indoor  activities:  house  cleaning  needs  1  hour;  homework  needs  three  hours;  turning  on  the  washer/laundry  machine  needs  0  minutes  but  needs  to  stay  home  for  one  hour.Other  constrain:Cannot  play  basketball  within  an  hour  after  a  meal.Let’s  start  planning  from  the  end.{current_progress}Decide  on  the  activity  ending  at  {current_hour}:00.Here  are  possible  options:{choice_list}Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.

Formal-LLM Prompt Example:

[⬇](data:text/plain;base64,R2VuZXJhdGUgYSBwbGFuIGZvciBhY3Rpdml0aWVzIGJldHdlZW4gMTA6MDAgYW5kIDIwOjAwLgoKQnJlYWtmYXN0LCBsdW5jaCwgYW5kIHN1cHBlciBuZWVkIDEgaG91ciBlYWNoLgoKT3V0ZG9vciBhY3Rpdml0aWVzOiBiYXNrZXRiYWxsIHBsYXlpbmcgMTM6MDAgLSAxNTowMDsgZG8gZ3JvY2VyeSBzaG9wcGluZyBuZWVkcyAxIGhvdXIuCgpJbmRvb3IgYWN0aXZpdGllczogaG91c2UgY2xlYW5pbmcgbmVlZHMgMSBob3VyOyBob21ld29yayBuZWVkcyB0aHJlZSBob3VyczsgdHVybmluZyBvbiB0aGUgd2FzaGVyL2xhdW5kcnkgbWFjaGluZSBuZWVkcyAwIG1pbnV0ZXMgYnV0IG5lZWRzIHRvIHN0YXkgaG9tZSBmb3Igb25lIGhvdXIuCgpPdGhlciBjb25zdHJhaW46CkNhbm5vdCBwbGF5IGJhc2tldGJhbGwgd2l0aGluIGFuIGhvdXIgYWZ0ZXIgYSBtZWFsLgoKTGV0J3Mgc3RhcnQgcGxhbm5pbmcgZnJvbSB0aGUgZW5kLgoKQ3VycmVudCBQcm9ncmVzczoKCjE3OjAwIC0gMjA6MDAgRG9pbmcgaG9tZXdvcmsuCgpEZWNpZGUgb24gdGhlIGFjdGl2aXR5IGVuZGluZyBhdCAxNzowMC4KCkhlcmUgYXJlIHBvc3NpYmxlIG9wdGlvbnM6CgoxLiBFYXRpbmcgc3VwcGVyLgoyLiBHcm9jZXJ5IHNob3BwaW5nLgozLiBIb3VzZSBjbGVhbmluZy4KNC4gVHVybmluZyBvbiB0aGUgd2FzaGVyL2xhdW5kcnkgbWFjaGluZS4KNS4gRG9pbmcgbm90aGluZyBmb3Igb25lIGhvdXIuCgpZb3VyIHJlcGx5IHNob3VsZCBiZSBvbmx5IG9uZSBudW1iZXIsIHN1Y2ggYXMgMSwgcmVmZXJyaW5nIHRvIHRoZSBvcHRpb24u)Generate  a  plan  for  activities  between  10:00  and  20:00.Breakfast,  lunch,  and  supper  need  1  hour  each.Outdoor  activities:  basketball  playing  13:00  -  15:00;  do  grocery  shopping  needs  1  hour.Indoor  activities:  house  cleaning  needs  1  hour;  homework  needs  three  hours;  turning  on  the  washer/laundry  machine  needs  0  minutes  but  needs  to  stay  home  for  one  hour.Other  constrain:Cannot  play  basketball  within  an  hour  after  a  meal.Let’s  start  planning  from  the  end.Current  Progress:17:00  -  20:00  Doing  homework.Decide  on  the  activity  ending  at  17:00.Here  are  possible  options:1.  Eating  supper.2.  Grocery  shopping.3.  House  cleaning.4.  Turning  on  the  washer/laundry  machine.5.  Doing  nothing  for  one  hour.Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.

#### A.5.2 Cooking Recipe

The formal language for the cooking task is a CFG, shown as Eq.([5](https://arxiv.org/html/2402.00798v4#A1.E5 "Equation 5 ‣ A.5.2 Cooking Recipe ‣ A.5 Prompt for Real-life Tasks ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents")).

|  | $\begin{split}&S\rightarrow aAB\\ &B\rightarrow aCD&#124;aCE\\ &C\rightarrow aF\\ &D\rightarrow aAI\\ &E\rightarrow bH\\ &F\rightarrow bAG\\ &G\rightarrow cd\\ &H\rightarrow ef&#124;eI\\ &I\rightarrow cf\end{split}$ |  | (5) |

where nonterminals: $S$ is for the final beef broccoli; $A$ is for seasoning and can be replaced by an element of the power set of $\{$onion, ginger, garlic, oil, salt, light soy oil, cooking wine, white pepper, sugar, vinegar$\}$; $B$ is for beef and broccoli mixture; $C$ is for lightly cooked beef; $D$ is for lightly cooked broccoli; $E$ is for blanched and drained broccoli; $F$ is for marinaded beef; $G$ is for clean slices of beef; $H$ is for blanched broccoli; $I$ is for clean broccoli; and terminals: $a$ is for wok; $b$ is for bowl; $c$ is for water; $d$ is for raw beef slices; $e$ is for cooking pot; $f$ is for broccoli.

GPT-4 Prompt:

[⬇](data:text/plain;base64,R2VuZXJhdGUgYSBicm9jY29saSBiZWVmIGNvb2tpbmcgcGxhbi4KClRoZSBpbmdyZWRpZW50cyBpbmNsdWRlIHJhdyBiZWVmIHNsaWNlcywgY2FycGFjY2lvLCBicm9jY29saSwgb25pb25zLCBnaW5nZXIsIGdhcmxpYywgYW5kIHdhdGVyLgoKVGhlIHNlYXNvbmluZ3MgaW5jbHVkZSBjb29raW5nIG9pbCwgc2FsdCwgbGlnaHQgc295IHNhdWNlLCBjb29raW5nIHdpbmUsIHdoaXRlIHBlcHBlciwgc3VnYXIsIHZpbmVnYXIuCgpDb29raW5nIHV0ZW5zaWxzIGluY2x1ZGluZyB3b2tzIGFuZCBjb29raW5nIHBvdHMuCgpUYWJsZXdhcmUgaW5jbHVkaW5nIGNob3BzdGlja3MsIHNwb29ucywgd29rIHNwb29ucywgYW5kIHNldmVyYWwgYm93bHMu)Generate  a  broccoli  beef  cooking  plan.The  ingredients  include  raw  beef  slices,  carpaccio,  broccoli,  onions,  ginger,  garlic,  and  water.The  seasonings  include  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper,  sugar,  vinegar.Cooking  utensils  including  woks  and  cooking  pots.Tableware  including  chopsticks,  spoons,  wok  spoons,  and  several  bowls.

Formal-LLM Prompt:

[⬇](data:text/plain;base64,R2VuZXJhdGUgYSBicm9jY29saSBiZWVmIGNvb2tpbmcgcGxhbi4KClRoZSBpbmdyZWRpZW50cyBpbmNsdWRlIHJhdyBiZWVmIHNsaWNlcywgY2FycGFjY2lvLCBicm9jY29saSwgb25pb25zLCBnaW5nZXIsIGdhcmxpYywgYW5kIHdhdGVyLgoKVGhlIHNlYXNvbmluZ3MgaW5jbHVkZSBjb29raW5nIG9pbCwgc2FsdCwgbGlnaHQgc295IHNhdWNlLCBjb29raW5nIHdpbmUsIHdoaXRlIHBlcHBlciwgc3VnYXIsIHZpbmVnYXIuCgpDb29raW5nIHV0ZW5zaWxzIGluY2x1ZGluZyB3b2tzIGFuZCBjb29raW5nIHBvdHMuCgpUYWJsZXdhcmUgaW5jbHVkaW5nIGNob3BzdGlja3MsIHNwb29ucywgd29rIHNwb29ucywgYW5kIHNldmVyYWwgYm93bHMuCgp7Y3VycmVudF9wcm9ncmVzc30KCkRlY2lkZSBvbiB0aGUgcHJldmlvdXMgc3RlcCBiZWZvcmUgY3VycmVudCBwcm9ncmVzcy4KCkhlcmUgYXJlIHBvc3NpYmxlIG9wdGlvbnMgdG8gZ2V0IHt0YXJnZXRfaXRlbX0gZm9yIHRoZSBzdGVwOiB7cGFyZW50X3N0ZXB9LgoKe2Nob2ljZV9saXN0fQoKWW91ciByZXBseSBzaG91bGQgYmUgb25seSBvbmUgbnVtYmVyLCBzdWNoIGFzIDEsIHJlZmVycmluZyB0byB0aGUgb3B0aW9uLg==)Generate  a  broccoli  beef  cooking  plan.The  ingredients  include  raw  beef  slices,  carpaccio,  broccoli,  onions,  ginger,  garlic,  and  water.The  seasonings  include  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper,  sugar,  vinegar.Cooking  utensils  including  woks  and  cooking  pots.Tableware  including  chopsticks,  spoons,  wok  spoons,  and  several  bowls.{current_progress}Decide  on  the  previous  step  before  current  progress.Here  are  possible  options  to  get  {target_item}  for  the  step:  {parent_step}.{choice_list}Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.

Formal-LLM Prompt Example:

[⬇](data:text/plain;base64,R2VuZXJhdGUgYSBicm9jY29saSBiZWVmIGNvb2tpbmcgcGxhbi4KClRoZSBpbmdyZWRpZW50cyBpbmNsdWRlIHJhdyBiZWVmIHNsaWNlcywgY2FycGFjY2lvLCBicm9jY29saSwgb25pb25zLCBnaW5nZXIsIGdhcmxpYywgYW5kIHdhdGVyLgoKVGhlIHNlYXNvbmluZ3MgaW5jbHVkZSBjb29raW5nIG9pbCwgc2FsdCwgbGlnaHQgc295IHNhdWNlLCBjb29raW5nIHdpbmUsIHdoaXRlIHBlcHBlciwgc3VnYXIsIHZpbmVnYXIuCgpDb29raW5nIHV0ZW5zaWxzIGluY2x1ZGluZyB3b2tzIGFuZCBjb29raW5nIHBvdHMuCgpUYWJsZXdhcmUgaW5jbHVkaW5nIGNob3BzdGlja3MsIHNwb29ucywgd29rIHNwb29ucywgYW5kIHNldmVyYWwgYm93bHMuCgpDdXJyZW50IFByb2dyZXNzOgoKU3RlcCBuOiBUaGVuLCB3ZSBnZXQgdGhlIGNvb2tlZCBicm9jY29saSBiZWVmLgpTdGVwIG4tMTogU3Rpci1mcnkgdGhlIGJlZWYgYW5kIGJyb2Njb2xpIG1peHR1cmUgd2l0aCB0aGUgc2Vhc29uaW5nIGluIGEgd29rLgpTdGVwIG4tMjogUHJlcGFyZSB0aGUgc2Vhc29uaW5nOiBnaW5nZXIsIGdhcmxpYywgY29va2luZyBvaWwsIHNhbHQsIGxpZ2h0IHNveSBzYXVjZSwgY29va2luZyB3aW5lLCB3aGl0ZSBwZXBwZXIgZm9yIHRoZSBzdGVwOiAiU3Rpci1mcnkgdGhlIGJlZWYgYW5kIGJyb2Njb2xpIG1peHR1cmUgd2l0aCB0aGUgc2Vhc29uaW5nIGluIGEgd29rLiIKU3RlcCBuLTM6ID8KCkRlY2lkZSBvbiB0aGUgcHJldmlvdXMgc3RlcCBiZWZvcmUgY3VycmVudCBwcm9ncmVzcy4KCkhlcmUgYXJlIHBvc3NpYmxlIG9wdGlvbnMgdG8gZ2V0IHRoZSBtaXh0dXJlIG9mIGJlZWYgYW5kIGJyb2Njb2xpIGZvciB0aGUgc3RlcDogIlN0aXItZnJ5IHRoZSBiZWVmIGFuZCBicm9jY29saSBtaXh0dXJlIHdpdGggdGhlIHNlYXNvbmluZyBpbiBhIHdvay4iCgoxOiBDb21iaW5lIGxpZ2h0bHkgY29va2VkIGJlZWYgYW5kIGxpZ2h0bHkgY29va2VkIGJyb2Njb2xpIGluIGEgd29rLgoyOiBDb21iaW5lIGxpZ2h0bHkgY29va2VkIGJlZWYgYW5kIGJsYW5jaGVkIGFuZCBkcmFpbmVkIGJyb2Njb2xpIGluIGEgd29rLgoKWW91ciByZXBseSBzaG91bGQgYmUgb25seSBvbmUgbnVtYmVyLCBzdWNoIGFzIDEsIHJlZmVycmluZyB0byB0aGUgb3B0aW9uLg==)Generate  a  broccoli  beef  cooking  plan.The  ingredients  include  raw  beef  slices,  carpaccio,  broccoli,  onions,  ginger,  garlic,  and  water.The  seasonings  include  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper,  sugar,  vinegar.Cooking  utensils  including  woks  and  cooking  pots.Tableware  including  chopsticks,  spoons,  wok  spoons,  and  several  bowls.Current  Progress:Step  n:  Then,  we  get  the  cooked  broccoli  beef.Step  n-1:  Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok.Step  n-2:  Prepare  the  seasoning:  ginger,  garlic,  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper  for  the  step:  "Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok."Step  n-3:  ?Decide  on  the  previous  step  before  current  progress.Here  are  possible  options  to  get  the  mixture  of  beef  and  broccoli  for  the  step:  "Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok."1:  Combine  lightly  cooked  beef  and  lightly  cooked  broccoli  in  a  wok.2:  Combine  lightly  cooked  beef  and  blanched  and  drained  broccoli  in  a  wok.Your  reply  should  be  only  one  number,  such  as  1,  referring  to  the  option.

#### A.5.3 Risk Management

![Refer to caption](img/ebcc919173c0c67e8df1454088c32b62.png)

Figure 7: The flowchart for risk management task.

Domain experts draw a flowchart of risk management as Figure [7](https://arxiv.org/html/2402.00798v4#A1.F7 "Figure 7 ‣ A.5.3 Risk Management ‣ A.5 Prompt for Real-life Tasks ‣ Appendix A Appendix ‣ Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents"). The flowchart is essentially a Deterministic Finite Automaton (DFA), which is a special case of Pushdown Automaton (PDA). A question is generated to inquiry LLM when the automaton reaches a new state. The LLM can select a reasonable branch according to its knowledge of the specific seller, buyer, and company name. In the experiment, the seller is Shareholders of Blizzard Entertainment, the company is Blizzard Entertainment, and the buyer is Microsoft.

GPT-4 Prompt:

[⬇](data:text/plain;base64,VGFzazogWW91IGFyZSBhIHBsYW4gbWFrZXIgdG8gZGVzaWduIGEgcmlzayBtYW5hZ2VtZW50IHBsYW4gZm9yIGRlYWxzIHJlbGF0ZWQgdG8gY29tcGFuaWVzLgoKU2NlbmFyaW86IHtTZWxsZXIgQX0gd2FudHMgdG8gc2VsbCB0aGUge0NvbXBhbnkgQn0gd2hpY2gge0J1eWVyIEN9IHdhbnRzIHRvIGJ1eS4gRGVzaWduIGEgcmlzayBtYW5hZ2VtZW50IHBsYW4gZm9yIHtCdXllciBDfSBiZWZvcmUgbmVnb3RpYXRpbmcgYSBkZXRhaWxlZCBjb250cmFjdC4=)Task:  You  are  a  plan  maker  to  design  a  risk  management  plan  for  deals  related  to  companies.Scenario:  {Seller  A}  wants  to  sell  the  {Company  B}  which  {Buyer  C}  wants  to  buy.  Design  a  risk  management  plan  for  {Buyer  C}  before  negotiating  a  detailed  contract.

Formal-LLM Prompt:

[⬇](data:text/plain;base64,VGFzazogWW91IGFyZSBhIHBsYW4gbWFrZXIgdG8gZGVzaWduIGEgcmlzayBtYW5hZ2VtZW50IHBsYW4gZm9yIGRlYWxzIHJlbGF0ZWQgdG8gY29tcGFuaWVzLgoKU2NlbmFyaW86IHtTZWxsZXIgQX0gd2FudHMgdG8gc2VsbCB0aGUge0NvbXBhbnkgQn0gd2hpY2gge0J1eWVyIEN9IHdhbnRzIHRvIGJ1eS4gRGVzaWduIGEgcmlzayBtYW5hZ2VtZW50IHBsYW4gZm9yIHtCdXllciBDfSBiZWZvcmUgbmVnb3RpYXRpbmcgYSBkZXRhaWxlZCBjb250cmFjdC4KCntxdWVzdGlvbiBpbiB0aGUgZmxvd2NoYXJ0fQoKe2Nob2ljZV9saXN0fQoKWW91ciBhbnN3ZXIgc2hvdWxkIGJlIG9ubHkgb25lIG51bWJlciwgc3VjaCBhcyAxLCByZWZlcnJpbmcgdG8gdGhlIG9wdGlvbi4=)Task:  You  are  a  plan  maker  to  design  a  risk  management  plan  for  deals  related  to  companies.Scenario:  {Seller  A}  wants  to  sell  the  {Company  B}  which  {Buyer  C}  wants  to  buy.  Design  a  risk  management  plan  for  {Buyer  C}  before  negotiating  a  detailed  contract.{question  in  the  flowchart}{choice_list}Your  answer  should  be  only  one  number,  such  as  1,  referring  to  the  option.

Formal-LLM Prompt Example:

[⬇](data:text/plain;base64,VGFzazogWW91IGFyZSBhIHBsYW4gbWFrZXIgdG8gZGVzaWduIGEgcmlzayBtYW5hZ2VtZW50IHBsYW4gZm9yIGRlYWxzIHJlbGF0ZWQgdG8gY29tcGFuaWVzLgoKU2NlbmFyaW86IFNoYXJlaG9sZGVycyBvZiBCbGl6emFyZCBFbnRlcnRhaW5tZW50IHdhbnRzIHRvIHNlbGwgdGhlIEJsaXp6YXJkIEVudGVydGFpbm1lbnQgd2hpY2ggTWljcm9zb2Z0IHdhbnRzIHRvIGJ1eS4gRGVzaWduIGEgcmlzayBtYW5hZ2VtZW50IHBsYW4gZm9yIE1pY3Jvc29mdCBiZWZvcmUgbmVnb3RpYXRpbmcgYSBkZXRhaWxlZCBjb250cmFjdC4KClF1ZXN0aW9uOiBEb2VzIE1pY3Jvc29mdCBidXlpbmcgQmxpenphcmQgRW50ZXJ0YWlubWVudCB0cmlnZ2VyIG1hbmRhdG9yeSBhbnRpdHJ1c3QgZmlsaW5nPwoKMTogWWVzLgoyOiBOby4KMzogSW5zdWZmaWNpZW50IGluZm9ybWF0aW9uIHRvIG1ha2UgYSBqdWRnbWVudC4KCllvdXIgYW5zd2VyIHNob3VsZCBiZSBvbmx5IG9uZSBudW1iZXIsIHN1Y2ggYXMgMSwgcmVmZXJyaW5nIHRvIHRoZSBvcHRpb24u)Task:  You  are  a  plan  maker  to  design  a  risk  management  plan  for  deals  related  to  companies.Scenario:  Shareholders  of  Blizzard  Entertainment  wants  to  sell  the  Blizzard  Entertainment  which  Microsoft  wants  to  buy.  Design  a  risk  management  plan  for  Microsoft  before  negotiating  a  detailed  contract.Question:  Does  Microsoft  buying  Blizzard  Entertainment  trigger  mandatory  antitrust  filing?1:  Yes.2:  No.3:  Insufficient  information  to  make  a  judgment.Your  answer  should  be  only  one  number,  such  as  1,  referring  to  the  option.

### A.6 Complete Results of Real-life Tasks

Daily plan result from GPT-4:

[⬇](data:text/plain;base64,MTA6MDAgLSAxMTowMCA6IEJyZWFrZmFzdAoxMTowMCAtIDEyOjAwIDogSG91c2UgQ2xlYW5pbmcKMTI6MDAgLSAxMzowMCA6IEZyZWUgVGltZQoxMzowMCAtIDE1OjAwIDogQmFza2V0YmFsbCBQbGF5aW5nCjE1OjAwIC0gMTY6MDAgOiBGcmVlIFRpbWUKMTY6MDAgLSAxNzowMCA6IEx1bmNoCjE3OjAwIC0gMTg6MDAgOiBHcm9jZXJ5IFNob3BwaW5nCjE4OjAwIC0gMTk6MDAgOiBIb21ld29yawoxOTowMCAtIDIwOjAwIDogTGF1bmRyeSBhbmQgU3VwcGVyCjIwOjAwIC0gMjM6MDAgOiBDb250aW51ZSBIb21ld29yawoKSW4gdGhpcyBwbGFuLCBiYXNrZXRiYWxsIHBsYXlpbmcgaXMgc2NoZWR1bGVkIGZyb20gMTMuMDAgdG8gMTUuMDAgd2l0aCBhbiBob3VyIGFmdGVyIGJyZWFrZmFzdCB0byBhdm9pZCBwbGF5aW5nIHJpZ2h0IGFmdGVyIGVhdGluZyBhcyBwZXIgdGhlIGNvbnN0cmFpbnQuIEx1bmNoIGlzIHRoZW4gc2NoZWR1bGVkIGFmdGVyIGJhc2tldGJhbGwgYW5kIFNob3BwaW5nIGRvbmUgYWZ0ZXIgdGhhdC4gU2luY2UgbGF1bmRyeSBkb2Vzbid0IHRha2UgdGltZSwgaXQgaGFzIGJlZW4gYWRkZWQgd2l0aCBzdXBwZXIgYW5kIHJlc3Qgb2YgdGhlIHRpbWUgaXMgYWxsb2NhdGVkIHRvIGhvbWV3b3JrLg==)10:00  -  11:00  :  Breakfast11:00  -  12:00  :  House  Cleaning12:00  -  13:00  :  Free  Time13:00  -  15:00  :  Basketball  Playing15:00  -  16:00  :  Free  Time16:00  -  17:00  :  Lunch17:00  -  18:00  :  Grocery  Shopping18:00  -  19:00  :  Homework19:00  -  20:00  :  Laundry  and  Supper20:00  -  23:00  :  Continue  HomeworkIn  this  plan,  basketball  playing  is  scheduled  from  13.00  to  15.00  with  an  hour  after  breakfast  to  avoid  playing  right  after  eating  as  per  the  constraint.  Lunch  is  then  scheduled  after  basketball  and  Shopping  done  after  that.  Since  laundry  doesn’t  take  time,  it  has  been  added  with  supper  and  rest  of  the  time  is  allocated  to  homework.

Daily plan result after applying Formal-LLM:

[⬇](data:text/plain;base64,MTA6MDAgLSAxMDowMCBUdXJuaW5nIG9uIHRoZSB3YXNoZXIvbGF1bmRyeSBtYWNoaW5lLgoxMDowMCAtIDExOjAwIEhvdXNlIGNsZWFuaW5nLgoxMTowMCAtIDEyOjAwIEVhdGluZyBicmVha2Zhc3QuCjEyOjAwIC0gMTM6MDAgR3JvY2VyeSBzaG9wcGluZy4KMTM6MDAgLSAxNTowMCBQbGF5aW5nIGJhc2tldGJhbGwuCjE1OjAwIC0gMTY6MDAgRWF0aW5nIGx1bmNoLgoxNjowMCAtIDE5OjAwIERvaW5nIGhvbWV3b3JrLgoxOTowMCAtIDIwOjAwIEVhdGluZyBzdXBwZXIu)10:00  -  10:00  Turning  on  the  washer/laundry  machine.10:00  -  11:00  House  cleaning.11:00  -  12:00  Eating  breakfast.12:00  -  13:00  Grocery  shopping.13:00  -  15:00  Playing  basketball.15:00  -  16:00  Eating  lunch.16:00  -  19:00  Doing  homework.19:00  -  20:00  Eating  supper.

Cooking recipe result from GPT-4:

[⬇](data:text/plain;base64,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)Ingredients  Preparation1.  Sliced  Beef:  300  grams2.  Chinese  Broccoli  (Gai  Lan):  As  needed,  cut  into  segments3.  Scallions:  1  stalk,  cut  into  sections4.  Ginger:  As  needed,  sliced5.  Garlic:  As  needed,  minced6.  Water:  As  needed7.  Cooking  Oil:  As  needed8.  Salt:  As  needed9.  Light  Soy  Sauce:  2  tablespoons10.  Cooking  Wine:  1  tablespoon11.  White  Pepper  Powder:  As  needed12.  Sugar:  1  tablespoon13.  Vinegar:  1  tablespoonCooking  Utensils  and  Tableware  Preparation1.  Stir-fry  Pan:  12.  Cooking  Pot:  13.  Tableware:  Chopsticks,  Soup  Spoon,  Stir-fry  Pan  Spoon,  Several  BowlsCooking  StepsPreparatory  Steps1.  Marinate  the  beef  slices  with  salt  and  cooking  wine,  stirring  evenly,  and  let  it  marinate  for  10  minutes.2.  Heat  the  stir-fry  pan  and  add  an  appropriate  amount  of  cooking  oil.Sautéing  the  Beef1.  Once  the  oil  is  hot,  quickly  stir-fry  the  marinated  beef  slices  until  they  change  color.2.  Add  scallion  sections,  ginger  slices,  and  minced  garlic.  Continue  stirring  until  a  fragrant  aroma  is  released.Sautéing  the  Chinese  Broccoli1.  In  the  stir-fry  pan,  add  water.  Once  the  water  boils,  add  the  Chinese  broccoli.2.  Quickly  stir-fry  the  Chinese  broccoli  until  it  changes  color,  and  season  with  salt.Seasoning1.  Add  light  soy  sauce,  white  pepper  powder,  sugar,  and  vinegar.  Stir  well.2.  Continue  stirring  to  ensure  that  the  seasoning  is  evenly  coated  on  the  beef  and  Chinese  broccoli.Completion1.  Ensure  the  beef  is  cooked  through,  and  the  Chinese  broccoli  is  flavorful.  Remove  from  heat.Serving1.  Transfer  the  Chinese  Broccoli  Beef  to  bowls.2.  Serve  with  rice  or  noodles.Tips1.  Avoid  marinating  the  beef  slices  for  too  long  to  maintain  a  good  texture.2.  Control  the  sautéing  time  for  Chinese  broccoli  to  preserve  its  crisp  and  tender  texture.3.  Adjust  the  seasoning  according  to  personal  taste  preferences,  adding  or  reducing  the  amount  of  salt,  sugar,  and  vinegar.

Cooking recipe result after applying Formal-LLM:

[⬇](data:text/plain;base64,SGVyZSBpcyB0aGUgZmluYWwgcGxhbjoKClN0ZXAgMTogV2FzaCByYXcgYmVlZiBzbGljZXMgd2l0aCB3YXRlci4KU3RlcCAyOiBQcmVwYXJlIHRoZSBzZWFzb25pbmc6IGdpbmdlciwgZ2FybGljLCBjb29raW5nIG9pbCwgc2FsdCwgbGlnaHQgc295IHNhdWNlLCBjb29raW5nIHdpbmUsIHdoaXRlIHBlcHBlciBmb3IgdGhlIHN0ZXA6ICJNYXJpbmF0ZSBjbGVhbiBzbGljZXMgb2YgYmVlZiBpbiBhIGJvd2wgd2l0aCBzZWFzb25pbmciLgpTdGVwIDM6IE1hcmluYXRlIGNsZWFuIHNsaWNlcyBvZiBiZWVmIGluIGEgYm93bCB3aXRoIHNlYXNvbmluZy4KU3RlcCA0OiBTdGlyLWZyeSB0aGUgbWFyaW5hZGVkIGJlZWYgaW4gYSB3b2suClN0ZXAgNTogV2FzaCBicm9jY29saSB3aXRoIHdhdGVyLgpTdGVwIDY6IEJsYW5jaCBjbGVhbiBicm9jY29saSBpbiBhIGNvb2tpbmcgcG90LgpTdGVwIDc6IFVzZSBhIGJvd2wgdG8gZHJhaW4gdGhlIGJsYW5jaGVkIGJyb2Njb2xpLgpTdGVwIDg6IENvbWJpbmUgbGlnaHRseSBjb29rZWQgYmVlZiBhbmQgYmxhbmNoZWQgYW5kIGRyYWluZWQgYnJvY2NvbGkgaW4gYSB3b2suClN0ZXAgOTogUHJlcGFyZSB0aGUgc2Vhc29uaW5nOiBnaW5nZXIsIGdhcmxpYywgY29va2luZyBvaWwsIHNhbHQsIGxpZ2h0IHNveSBzYXVjZSwgY29va2luZyB3aW5lLCB3aGl0ZSBwZXBwZXIgZm9yIHRoZSBzdGVwOiAiU3Rpci1mcnkgdGhlIGJlZWYgYW5kIGJyb2Njb2xpIG1peHR1cmUgd2l0aCB0aGUgc2Vhc29uaW5nIGluIGEgd29rIi4KU3RlcCAxMDogU3Rpci1mcnkgdGhlIGJlZWYgYW5kIGJyb2Njb2xpIG1peHR1cmUgd2l0aCB0aGUgc2Vhc29uaW5nIGluIGEgd29rLgpTdGVwIDExOiBUaGVuLCB3ZSBnZXQgdGhlIGNvb2tlZCBicm9jY29saSBiZWVmLgoKQ2FyZWZ1bGx5IGNvb2tpbmcgYXQgZWFjaCBzdGVwIQ==)Here  is  the  final  plan:Step  1:  Wash  raw  beef  slices  with  water.Step  2:  Prepare  the  seasoning:  ginger,  garlic,  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper  for  the  step:  "Marinate  clean  slices  of  beef  in  a  bowl  with  seasoning".Step  3:  Marinate  clean  slices  of  beef  in  a  bowl  with  seasoning.Step  4:  Stir-fry  the  marinaded  beef  in  a  wok.Step  5:  Wash  broccoli  with  water.Step  6:  Blanch  clean  broccoli  in  a  cooking  pot.Step  7:  Use  a  bowl  to  drain  the  blanched  broccoli.Step  8:  Combine  lightly  cooked  beef  and  blanched  and  drained  broccoli  in  a  wok.Step  9:  Prepare  the  seasoning:  ginger,  garlic,  cooking  oil,  salt,  light  soy  sauce,  cooking  wine,  white  pepper  for  the  step:  "Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok".Step  10:  Stir-fry  the  beef  and  broccoli  mixture  with  the  seasoning  in  a  wok.Step  11:  Then,  we  get  the  cooked  broccoli  beef.Carefully  cooking  at  each  step!

Risk management result from GPT-4:

[⬇](data:text/plain;base64,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)Risk  Management  Plan:Objective:  To  ensure  a  beneficial  acquisition  of  Blizzard  Entertainment  by  Microsoft  while  managing  and  mitigating  potential  risks.I.  Risk  Identification1.  Financial  Risks:  A  complete  and  thorough  financial  audit  of  Blizzard  must  be  performed  to  determine  potential  financial  risks,  such  bright  as  debts,  pending  lawsuit  settlements,  past  or  ongoing  tax  issues,  declining  revenue,  etc.2.  Operational  Risks:  Understand  Blizzard’s  operational  weaknesses  and  strengths,  and  the  impact  of  the  acquisition  on  Microsoft’s  operations.3.  Legal  Risks:  Review  Blizzard’s  legal  compliance  status  in  all  jurisdictions  it  operates  in.  Especially,  risks  related  to  intellectual  property  rights,  privacy  and  data  security  laws,  and  labor  laws  must  be  evaluated.4.  Regulatory  Risks:  Determine  any  potential  regulatory  approvals  Microsoft  might  need  for  the  acquisition.II.  Risk  Assessment:1.  Quantify  each  identified  risk  wherever  possible,  in  terms  of  potential  financial  impact,  time  delay,  brand  damage  etc.2.  Determine  the  likelihood  of  each  risk  occurrence.3.  Prioritize  the  risks  based  on  their  potential  impact  and  the  likelihood  of  occurrence.III.  Risk  Mitigation  Planning:1.  Financial  Risk:  This  could  be  mitigated  through  leverage  and  insurance,  renegotiation  of  terms,  or  such  other  financial  instruments.2.  Operational  Risk:  To  mitigate  this  risk,  Microsoft  can  plan  to  gradually  integrate  Blizzard,  keeping  key  management  personnel,  or  finding  synergies  with  Microsoft’s  current  operations.3.  Legal  Risk:  Microsoft  should  get  warranties  and  indemnities  from  Blizzard’s  owners  to  cover  potential  post-acquisition  legal  disputes.4.  Regulatory  Risk:  Early  engagement  with  regulators  and  professional  advice  can  help  smooth  the  acquisition  process.IV.  Risk  Monitoring:A  system  to  continuously  monitor  and  manage  these  risks  should  be  set  up.  This  could  involve  regular  reporting,  monitoring  industry  developments,  legal  changes  etc.V.  Contingency  Planning:Establish  contingency  plans  for  high-impact  risks.  For  instance,  a  backup  plan  if  the  acquisition  fails,  or  if  post-acquisition  integration  does  not  go  as  planned.VI.  Communication:Continual  communication  with  stakeholders  to  obtain  feedback  and  to  convey  risk  management  efforts.  This  will  ensure  that  all  parties  are  aware  of  the  risks  and  the  measures  being  taken  to  mitigate  them.VII.  Insurance  Cover:Consider  adequate  insurance  coverage  that  includes  Directors’  and  Officers’  Liability  and  Professional  Indemnity  Insurance  for  all  potential  issues  that  might  arise  in  connection  with  the  proposed  transaction.The  above  steps  will  form  the  roadmap  for  Microsoft  while  negotiating  terms  for  the  acquisition  of  Blizzard  Entertainment.  Understanding  risks  before  they  come  up  ensures  that  Microsoft  isn’t  surprised  during  negotiations,  hence  effectively  mitigating  them.

Risk management result after applying Formal-LLM:

[⬇](data:text/plain;base64,SGVyZSBpcyB0aGUgZmluYWwgcGxhbiBmb3IgTWljcm9zb2Z0OgoKMTogV2UgbmVlZCBtb3JlIGluZm9ybWF0aW9uIHRvIGVuc3VyZSBTaGFyZWhvbGRlcnMgb2YgQmxpenphcmQgRW50ZXJ0YWlubWVudCBpcyBlbnRpdGxlZCB0byBzZWxsIEJsaXp6YXJkIEVudGVydGFpbm1lbnQuIEJ1dCB3ZSBhc3N1bWUgd2UgaGF2ZSBmb3VuZCBvdXQgZW5vdWdoIGluZm9ybWF0aW9uIHRvIHByb2NlZWQgdGhlIHJpc2sgYXNzZXNzbWVudCBwcm9jZXNzLgoyOiBUaGVuLCBiYXNlZCBvbiBjdXJyZW50IGluZm9ybWF0aW9uLCB3ZSBiZWxpZXZlIEJsaXp6YXJkIEVudGVydGFpbm1lbnQgaXMgaW4gZ29vZCBzdGFuZGluZyB3aXRob3V0IG1hdGVyaWFsIG91dHN0YW5kaW5nIGxpYWJpbGl0aWVzLgozOiBUaGVuLCBiYXNlZCBvbiBjdXJyZW50IGluZm9ybWF0aW9uLCB3ZSBiZWxpZXZlIG5laXRoZXIgQ0ZJVVMgZmlsaW5ncyBub3IgZm9yZWlnbiBpbnZlc3RtZW50IGZpbGluZ3MgaW4gQ2hpbmEgYXJlIG5lZWRlZC4KNDogVGhlbiwgd2UgbmVlZCB0byBzdWJtaXQgdGhlIHJlZ2lvbmFsIGFudGl0cnVzdCBmaWxpbmcgb2YgdGhlIFVuaXRlZCBTdGF0ZXMgYW5kL29yIENoaW5hLiBCdXQgd2UgYXNzdW1lIHRoZSBmaWxpbmdzIHdpbGwgYmUgYXBwcm92ZWQgdG8gcHJvY2VlZCB0aGUgcmlzayBhc3Nlc3NtZW50IHByb2Nlc3MuCjU6IFBhc3MgdGhlIHByZWxpbWluYXJ5IHJpc2sgYW5hbHlzaXMuIERldGFpbHMgdG8gYmUgZGlzY3Vzc2VkIGluIHRoZSB0cmFuc2FjdGlvbiBkb2N1bWVudHMuCgpOb3RlOiBUaGlzIGlzIGEgcmlzayBhc3Nlc3NtZW50IHByb2Nlc3MgcHJvdmlkZWQgYmFzZWQgb24gY3VycmVudCBpbmZvcm1hdGlvbi4gUGxlYXNlIGVuc3VyZSB0aGUgYWNjdXJhY3kgb2YgdGhlIHByb3ZpZGVkIGluZm9ybWF0aW9uIGFuZCBwb3NzaWJsZSBhZGRpdGlvbmFsIHN1cHBsZW1lbnRhbCBpbmZvcm1hdGlvbi4=)Here  is  the  final  plan  for  Microsoft:1:  We  need  more  information  to  ensure  Shareholders  of  Blizzard  Entertainment  is  entitled  to  sell  Blizzard  Entertainment.  But  we  assume  we  have  found  out  enough  information  to  proceed  the  risk  assessment  process.2:  Then,  based  on  current  information,  we  believe  Blizzard  Entertainment  is  in  good  standing  without  material  outstanding  liabilities.3:  Then,  based  on  current  information,  we  believe  neither  CFIUS  filings  nor  foreign  investment  filings  in  China  are  needed.4:  Then,  we  need  to  submit  the  regional  antitrust  filing  of  the  United  States  and/or  China.  But  we  assume  the  filings  will  be  approved  to  proceed  the  risk  assessment  process.5:  Pass  the  preliminary  risk  analysis.  Details  to  be  discussed  in  the  transaction  documents.Note:  This  is  a  risk  assessment  process  provided  based  on  current  information.  Please  ensure  the  accuracy  of  the  provided  information  and  possible  additional  supplemental  information.

## Appendix B Potential Risk

Our research focuses on making LLM-based agent planning more controllable. To the best of our knowledge, we do not notice the potential risk of our research.

## Appendix C Additional information of Datasets

In our research, we used two benchmark datasets, OpenAGI (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) and TravelPlanner (Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)). They are both under MIT license, which means these two datasets are publicly available.

OpenAGI (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12)) is used as the agent creation package to build agents for LLM-based agents. TravelPlanner (Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)) is a benchmark crafted for evaluating language agents in tool-use and complex planning within multiple constraints. Our work is to explore the controllable planning ability of LLM-based agents, which is consistent with their intended use.

These two benchmark datasets are synthesized by algorithm and thus, do not contain any individual personal information.

The language of these two benchmark datasets is English.

For the number of examples and details of train / test / dev splits, we use the same dataset and follow the the same data splitting method with these two papers (Ge et al., [2023a](https://arxiv.org/html/2402.00798v4#bib.bib12); Xie et al., [2024](https://arxiv.org/html/2402.00798v4#bib.bib47)).