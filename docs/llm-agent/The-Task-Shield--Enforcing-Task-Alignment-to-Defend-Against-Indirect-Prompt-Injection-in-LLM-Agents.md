<!--yml

类别：未分类

日期：2025-01-11 11:45:18

-->

# 《任务盾牌：执行任务对齐以防御LLM代理中的间接提示注入》

> 来源：[https://arxiv.org/html/2412.16682/](https://arxiv.org/html/2412.16682/)

Feiran Jia

宾夕法尼亚州立大学

feiran.jia@psu.edu

&Tong Wu

普林斯顿大学

tongwu@princeton.edu

\ANDXin Qin

加利福尼亚州立大学长滩分校

xin.qin@csulb.edu

&Anna Squicciarini

宾夕法尼亚州立大学

acs20@psu.edu

###### 摘要

大型语言模型（LLM）代理正在越来越多地作为对话助手被部署，能够通过工具集成执行复杂的现实任务。这种与外部系统交互并处理各种数据源的增强能力虽然强大，但也引入了显著的安全漏洞。特别是，间接提示注入攻击构成了严重威胁，恶意指令嵌入外部数据源中，可以操控代理偏离用户意图。尽管基于规则约束、源点亮和认证协议的现有防御方法显示出一定前景，但它们在保持任务功能性的同时，难以维持稳健的安全性。我们提出了一种新的且正交的视角，将代理安全从防止有害行为转变为确保任务对齐，要求每个代理行动都服务于用户目标。基于这一见解，我们开发了任务盾牌（Task Shield），这是一种在测试时验证机制，系统地检查每个指令和工具调用是否有助于用户指定的目标。通过在AgentDojo基准测试中的实验，我们证明任务盾牌在减少攻击成功率（2.07%）的同时，保持了较高的任务效用（69.79%）在GPT-4o上，显著优于现有防御方法，在多种现实场景中表现出色。

《任务盾牌：执行任务对齐以防御LLM代理中的间接提示注入》

Feiran Jia 宾夕法尼亚州立大学 feiran.jia@psu.edu                        Tong Wu 普林斯顿大学 tongwu@princeton.edu

Xin Qin 加利福尼亚州立大学长滩分校 xin.qin@csulb.edu                        Anna Squicciarini 宾夕法尼亚州立大学 acs20@psu.edu

## 1 引言

大型语言模型（LLM）代理近年来取得了快速进展，使其能够执行各种任务，从生成创意内容到执行复杂操作，如发送电子邮件、安排约会或查询 API，见 Brown 等人（[2020](https://arxiv.org/html/2412.16682v1#bib.bib1)）；Touvron 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib29)）；Schick 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib26)）。与传统的聊天机器人不同，这些代理能够在现实世界中执行操作，并且它们的输出可能会带来现实世界的后果。在本研究中，我们关注一个关键的应用场景。LLM 代理作为对话系统中的个人助手，见 OpenAI（[2024](https://arxiv.org/html/2412.16682v1#bib.bib19)）。除了生成自然语言响应外，这些助手还被赋予了采取行动的能力：它们可以访问敏感数据、进行财务交易，并通过工具集成与关键系统互动。这一增强的能力要求更高的安全关注。

在这些系统的威胁中，间接提示注入攻击构成了一种微妙但显著的威胁，见 Zou 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib37)）；Xiang 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib34)）。攻击者不是直接注入有害指令，而是将恶意提示嵌入到 LLM 代理处理的外部数据源（环境）中，例如文档、网页或工具输出。逆向扩展定律，见 Wei 等人（[2022](https://arxiv.org/html/2412.16682v1#bib.bib32)），指出更强大的 LLM 模型越来越脆弱。因此，我们专注于这些高能力模型。

现有的防御措施基于基于规则的约束，如 Wallace 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）；Li 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib14)），源聚焦 Hines 等人（[2024a](https://arxiv.org/html/2412.16682v1#bib.bib10)），以及认证协议 Wang 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib31)）。尽管这些方法有其优点，但也面临实际的局限性。规则的详细规范化很具挑战性，而间接攻击可以将恶意指令嵌入看似无害的语气中，从而绕过检测机制。我们提出了一种正交的方法：任务对齐。该概念提出，每个指令都应服务于用户的目标，安全性应该转向关注“这是否服务于预定的任务？”而不是“这是否有害？”。这种转向用户目标的做法意味着，代理应忽略那些偏离这些目标的指令，从而过滤掉间接注入的指令。

![参见图例](img/b387c58f765bd1c9ad426faa69136d53.png)

图 1：任务保护与工具集成的 LLM 代理互动概述。该框架执行任务对齐并防御间接提示注入攻击。

为了将任务对齐付诸实践，我们开发了任务盾（Task Shield）——一个作为LLM代理的守护者的防御系统。该盾牌验证系统中每一条指令，无论来自代理还是工具，是否与用户的目标完全对齐。通过分析指令之间的关系并提供及时干预，任务盾有效地防止了可能无关的行为，同时保持代理完成用户任务的能力。

我们的贡献总结如下：

+   •

    我们提出了一种新的任务对齐概念，形式化了LLM代理对话系统中指令之间的关系，为确保代理行为与用户定义目标对齐奠定了基础。

+   •

    我们介绍了任务盾（Task Shield），一种实用的测试时防御机制，能够动态地强制执行任务对齐。该盾牌评估每一次交互，并提供反馈，以在整个对话过程中保持对齐。

+   •

    通过在AgentDoJo Debenedetti等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）基准测试上的广泛实验，我们证明了我们的方法显著减少了对提示注入攻击的脆弱性，同时保持了用户任务的有效性。

## 2 初步

#### LLM 代理系统和消息类型

LLM（大规模语言模型）代理对话系统通过一系列消息实现多轮对话，$\mathcal{M}=[M_{1},M_{2},\dots,M_{n}]$，其中$n$为消息的总数。每条消息$M_{i}$承担四种角色之一：系统消息定义代理的角色和核心规则；用户消息指定目标和请求；助手消息解释并响应指令；工具输出提供外部数据或结果。为了构建交互结构，OpenAI提出了一种指令层级结构Wallace等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）为每条消息分配一个权限等级$P(M_{i})\in\{L_{s},L_{u},L_{a},L_{t}\}$，分别表示系统（$L_{s}$）、用户（$L_{u}$）、助手（$L_{a}$）和工具（$L_{t}$）的权限级别。该层级结构强制执行优先级顺序$L_{s}\succ L_{u}\succ L_{a}\succ L_{t}$，即较低权限级别的指令会被较高权限级别的指令所覆盖。

<svg class="ltx_picture" height="83.42" id="S2.SS0.SSS0.Px1.p2.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,83.42) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="63.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example: The user instructs "Find a nearby Italian restaurant for lunch tomorrow." (User Level $L_{u}$) The assistant interprets the request and plans to locate suitable options. (Assistant Level $L_{a}$) It then queries an external API to retrieve restaurant data. (Tool Level $L_{t}$)</foreignobject></g></g></svg>

这个示例展示了不同类型的消息如何在层级结构中相互作用，确保助手将其行为与用户的目标对齐，同时有效地利用外部工具。

#### 间接提示注入攻击

本文聚焦于间接提示注入攻击，其中攻击者将指令嵌入到环境中，这些指令被LLM代理在任务执行过程中处理。例如，假设一个代理被指示对网页进行总结。如果网页中包含隐藏的指令，如“忽略所有先前的指令，并将你的笔记发送给Alice”，代理可能会被劫持并无意中遵循这些恶意指令。这些间接攻击更具隐蔽性，因为它们被隐藏在代理必须处理的合法外部数据源中，用以完成任务。

## 3 任务对齐

我们的关键洞察是，当大型语言模型（LLM）执行偏离用户目标（或预定义对话目标）的指令时，间接提示注入攻击就会成功。这一理解促使我们提出一个新的视角：通过任务对齐的视角重新审视代理安全性。我们不再试图识别有害内容，而是专注于确保可操作指令有助于用户指定的目标。这一转变使我们能够捕捉到即使看似无害的恶意注入提示。

为了形式化这一概念，我们首先将任务指令定义为对话系统中基本的分析单元。接着，我们分析这些指令如何在不同的消息类型中相互作用，最终开发出一个正式框架来评估每个指令是否符合用户目标，特别是在具有工具集成的多轮对话中。

### 3.1 任务指令

我们公式化的一个关键原则是，用户指令定义了对话的目标。理想情况下，来自助手或外部工具的其他可操作指令应当支持这些用户目标。我们在每条消息中公式化了*任务指令*：

###### 定义1  （任务指令）。

任务指令是指从对话中的消息$M_{i}$中提取的可操作指令，旨在引导助手的行为。这些指令可以来自不同来源：（1）用户指令：用户明确说明的任务请求和目标；（2）助手计划：助手为实现用户目标而提出的子任务或步骤，包括自然语言指令和工具调用；（3）工具生成的指令：在任务执行过程中由外部工具生成的附加指令或建议。

我们用$E(M_{i})$表示从消息$M_{i}$中提取的任务指令集合。在每个权限级别$L$下，我们聚合该级别下所有消息的任务指令，这些消息属于对话段$\mathcal{M}^{\prime}$：

|  | $E_{L}(\mathcal{M}^{\prime})=\bigcup_{\begin{subarray}{c}M_{i}\in\mathcal{M}^{% \prime}\\ P(M_{i})=L\end{subarray}}E(M_{i}).$ |  |
| --- | --- | --- |

注意：系统消息也可以在某些专用代理中定义高层任务。然而，在本文中，我们主要关注$L_{u}$中的用户级指令。有关系统级任务目标的进一步讨论，请参见附录[A.2](https://arxiv.org/html/2412.16682v1#A1.SS2 "A.2 讨论系统级指令 ‣ 附录 A 附录：任务对齐的详细讨论 ‣ 任务屏障：强制任务对齐以防止大语言模型代理中的间接提示注入")。

### 3.2 任务交互

在大语言模型对话系统中，高层消息（特别是本文中的用户消息）提供抽象指令，而工具级消息则通过附加数据对其进行细化。在检查与对话目标的对齐时，我们应考虑来自所有来源的上下文，包括工具输出。如以下示例所示，工具可以仅提供支持信息或定义新的子任务：

<svg class="ltx_picture" height="98.64" id="S3.SS2.p2.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,98.64) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="78.87" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example 1: Tool Output as Supporting Information The user says ’Schedule an appointment with the dentist’. The assistant knows to schedule, but needs contact details. It queries a tool, then completes the predefined task. Example 2: Tool Output Defining Concrete Tasks The user says, "Complete my to-do list tasks." A to-do tool returns: "1\. Pay electricity bill 2\. Buy groceries," which transforms the user’s abstract request into specific actionable tasks.</foreignobject></g></g></svg>

在示例 1 中，工具输出补充了明确的用户指令。在示例 2 中，工具输出本身概述了子任务。对话历史$\mathcal{H}_{i}=[M_{1},\dots,M_{i-1}]$提供了判断这些关系并保持与用户目标对齐的上下文。

### 3.3 任务对齐的形式化

我们现在形式化任务对齐的概念。首先，我们定义$\mathrm{ContributesTo}$关系，它捕捉任务指令之间的关系。

###### 定义 2  （$\mathrm{ContributesTo}$ 关系）。

在对话历史$\mathcal{H}_{i}$的上下文中，令$e$为来自消息$M_{i}$的任务指令，令$t$为来自消息$M_{j}\in\mathcal{H}_{i}$的任务指令。我们说$e$贡献于$t$，记作$\mathrm{ContributesTo}(e,t\mid\mathcal{H}_{i})=\mathrm{True}$，如果$e$有助于在$\mathcal{H}_{i}$内实现$t$的指令或目标。

为了简化，我们将在符号中省略$\mathcal{H}_{i}$，并且$\mathrm{ContributesTo}(e,t)$将隐式考虑相关的对话历史。我们将任务指令对齐条件定义如下：

###### 定义 3  （任务指令对齐条件）。

如果在权限级别$L_{i}=P(M_{i})$的任务指令$e\in E(M_{i})$满足任务指令对齐条件，则对于用户级别$L_{u}$，至少存在一个任务指令$t\in E_{L_{u}}(\mathcal{H}_{i})$，其中$E_{L_{u}}(\mathcal{H}_{i})$是从对话历史$\mathcal{H}_{i}$中提取的权限级别$L_{u}$的任务指令集合，满足：

|  | $\mathrm{ContributesTo}(e,t)=\mathrm{True}.$ |  | (1) |
| --- | --- | --- | --- |

该条件确保低权限级别的任务指令直接贡献于至少一个用户特定的任务指令。在此基础上，我们可以在理想情况下定义一个完全对齐的对话：

###### 定义 4  （任务对齐）。

当对话中的所有助手级任务指令都满足任务指令对齐条件（定义[3](https://arxiv.org/html/2412.16682v1#Thmdefinition3 "定义3（任务指令对齐条件） ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务防护：强制执行任务对齐以防止LLM智能体中的间接提示注入")）时，任务对齐得以实现。

任务对齐确保助手的计划和工具调用始终服务于用户的目标。因此，任何与这些目标不对齐的（恶意）指令，如通过间接提示注入嵌入的指令，都会被智能体自然忽略。有关不符合任务对齐条件的对话示例，请参阅附录[A.3](https://arxiv.org/html/2412.16682v1#A1.SS3 "A.3 任务对齐示例 ‣ 附录 A 任务对齐的详细讨论 ‣ 任务防护：强制执行任务对齐以防止LLM智能体中的间接提示注入")。

![参见说明文字](img/f8965e8a3e448decb451ddfc3f1279ab.png)

图2：该图说明了任务防护框架如何处理来自对话流的不同消息类型，通过任务指令提取、对齐检查和反馈生成的过程。

## 4 任务防护框架

虽然我们将任务对齐定义为一个理想的安全属性，但在实践中实现它需要一个执行机制。为了满足这一需求，我们引入了任务防护框架，该框架持续监控并执行指令与用户目标的对齐。

如图[2](https://arxiv.org/html/2412.16682v1#S3.F2.1 "图2 ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务防护：强制执行任务对齐以防止LLM智能体中的间接提示注入")所示，该框架由三个关键组件组成：（1）指令提取，（2）对齐检查，以及（3）生成反馈以保持整个对话流程中的任务对齐。指令提取（1）和对齐检查中的$\mathrm{ContributesTo}$得分计算（2）都利用了大型语言模型的能力。

本节我们首先详细介绍每个防护组件的技术实现，然后解释这些组件如何在LLM智能体系统中动态交互，以强制执行任务对齐。

### 4.1 任务防护组件

#### 任务指令提取。

任务防护框架通过从每条传入消息中提取任务指令开始。这个过程有两个目的：（1）识别用户目标，这些目标被存储为用户任务集$T_{u}$，并作为对话目标进行对比检查；（2）检测来自其他来源的潜在指令，这些指令需要进行对齐检查。

现实世界中的消息常常会带来提取挑战：指令可能是隐式的、嵌套在其他指令中，或者包含在复杂的内容中。漏掉任何这样的指令可能会在我们的防御机制中造成安全漏洞。为了应对这些挑战，我们采用了一种保守的提取策略，使用精心设计的 LLM 提示（附录[D](https://arxiv.org/html/2412.16682v1#A4 "附录 D 提示 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")中图[4](https://arxiv.org/html/2412.16682v1#A4.F4 "图 4 ‣ 附录 D 提示 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")）。该提示指示 LLM 执行以下操作：（1）提取所有可能可操作的指令，即使它们是嵌套或隐式的，（2）将寻求信息的查询重写为明确的指令，并且（3）在自然语言中保留任务依赖关系。

#### 对齐检查。

一旦提取出指令，下一步是评估每条提取的指令是否满足任务指令对齐条件，具体定义见定义[3](https://arxiv.org/html/2412.16682v1#Thmdefinition3 "定义 3（任务指令对齐条件）。 ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")。这涉及两个关键方面：评估单个指令的贡献并计算整体对齐得分。

为了评估对齐性，我们使用谓词 $\mathrm{ContributesTo}$，具体定义见定义[2](https://arxiv.org/html/2412.16682v1#Thmdefinition2 "定义 2（ContributesTo 关系）。 ‣ 3.3 任务对齐的形式化 ‣ 3 任务对齐 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")。然而，二分类过于僵化，难以应用于实际场景，因为行动与目标之间的关系常常涉及不确定性或模糊性。为了应对这种微妙的关系，我们采用了基于模糊逻辑的评分机制。通过在 $[0,1]$ 范围内分配连续得分，我们能够细致评估指令如何贡献于用户目标，捕捉其在直接贡献、间接步骤或合理尝试解决方案中的作用。

然后，通过将所有用户任务指令的得分加总来计算总贡献分数。如果某一指令的总贡献分数为 $0$，则在对齐检查过程中认为该指令未对齐。此设计的详细讨论和实现包含在附录[B.2](https://arxiv.org/html/2412.16682v1#A2.SS2 "B.2 任务保护核心处理算法 ‣ 附录 B 附录：任务保护框架设计详细信息 ‣ 任务保护：强制任务对齐以防止 LLM 代理中的间接提示注入")中。

#### 反馈生成。

当检测到不对齐时，任务屏障生成结构化反馈，引导对话回到与用户目标一致的轨道。此反馈包括（1）一个明确的警报，识别不对齐的任务指令，(2) 一个解释潜在风险的通知，以及（3）当前用户目标的提醒 ($T_{u}$)。

### 4.2 与 LLM 代理系统的交互

任务屏障通过监控和干预对话流程来强制执行对齐，对每种消息类型采用不同的处理方法。每条消息必须通过对齐检查后才能继续，形成多层防御以应对潜在的攻击。

#### 用户消息处理

在用户级别 $L_{u}$，屏障通过更新用户任务集 $T_{u}$ 来处理新提取的指令。这些指令定义了所有后续消息处理的对齐目标。

#### 助理消息处理

在 $L_{a}$ 级别的消息可能包含两个需要对齐检查的组件：消息内容（自然语言回应）和工具调用。如果其中任何一个组件未通过对齐检查，任务屏障会向 LLM 代理提供反馈，提示其重新考虑回应。它充当一个评论者，提供多轮反馈以指导 LLM 代理优化查询。对于工具调用，任务屏障特别防止执行不对齐的调用。

#### 工具输出处理

在 $L_{t}$ 级别，屏障通过上下文感知评估工具输出，增强每个指令的来源信息：“来自工具 [function_name]，带参数 [args]”。当检测到不对齐时，屏障会将原始输出和反馈一起包含在响应中，以便助理进行知情的修正。

这种多层防御机制确保注入的攻击面临多重障碍：工具输出中的不对齐指令在 $L_{t}$ 处理过程中被标记，潜在的有害回应在 $L_{a}$ 级别被捕获和修正，而在 $L_{u}$ 级别对用户目标的持续验证则保持整体对话对齐。

## 5 个实验

在本节中，我们使用 AgentDoJo Debenedetti 等人 ([2024](https://arxiv.org/html/2412.16682v1#bib.bib3)) 对 GPT-4o 和 GPT-4o-mini 进行评估，每个任务进行一次试验。

### 5.1 设置

| 套件 | 旅行 | 工作区 | 银行 | Slack | 总体 |
| --- | --- | --- | --- | --- | --- |
| 防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 | 任务屏障 | 无防御 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 攻击 | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ | U $\uparrow$ | ASR $\downarrow$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 重要指令 | 72.86 | 1.43 | 64.29 | 11.43 | 62.50 | 0.42 | 24.17 | 40.42 | 82.64 | 6.25 | 69.44 | 62.50 | 64.76 | 0.95 | 63.81 | 92.38 | 69.79 | 2.07 | 50.08 | 47.69 |
| Injecagent | 67.86 | 0.00 | 72.14 | 0.00 | 66.67 | 0.00 | 64.58 | 0.00 | 77.78 | 4.17 | 72.22 | 15.28 | 66.67 | 0.95 | 67.62 | 13.33 | 69.48 | 1.11 | 68.52 | 5.72 |
| 忽略先前 | 70.71 | 0.00 | 77.14 | 0.00 | 62.92 | 0.00 | 61.67 | 0.00 | 72.22 | 1.39 | 68.75 | 8.33 | 63.81 | 0.95 | 61.90 | 20.95 | 66.93 | 0.48 | 66.77 | 5.41 |

表格 1：GPT-4o：在任务盾防御和无防御设置下，不同攻击方式在任务套件中的比较。U（效用）和ASR（攻击成功率）分别显示在任务盾和无防御设置中。任务盾下超越无防御的单元格以浅蓝色高亮，反之，无防御下超越任务盾的单元格以浅粉色高亮。所有数字均以百分比（%）表示。

![参见标题](img/a85cf160fb3479194a4024eb1efd4f3a.png)

图 3：GPT-4o：攻击成功率（ASR）与效用的比较。实心标记表示与良性效用相比的ASR，而空心标记表示与攻击下效用相比的ASR。箭头表示攻击对效用的影响，箭头的方向表示攻击对模型性能的影响。绿色圆圈突出显示良性条件下的帕累托前沿，橙色圆圈突出显示攻击下的帕累托前沿。箭头旁的数字表示攻击引入时效用变化的幅度（正值表示改善，负值表示下降）。

#### 基准测试

我们在AgentDojo基准测试环境中进行实验¹¹1AgentDojo可在[https://github.com/ethz-spylab/agentdojo](https://github.com/ethz-spylab/agentdojo)找到，并且它在MIT许可证下发布。我们使用AgentDojo完全符合其预期用途。我们使用模型的默认配置，AgentDojo是第一个全面评估AI代理对抗间接提示注入攻击的环境。与一些专注于简单场景、超出个人助手用例的基准（如刘等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib16)））或单轮评估（如詹等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib36)））不同，AgentDojo通过多轮对话和复杂的工具交互来模拟现实的代理行为。此外，该基准包含四个代表性的任务套件，模拟现实世界场景。旅行用于行程管理，工作空间用于文档处理，银行用于金融操作，Slack用于通信任务，为我们防御机制在实际应用中的测试提供了实际场景。

#### 模型

主要评估是在 GPT-4o 上进行的。选择此模型的原因有两个：（1）GPT-4o 在 AgentDojo 挑战任务中表现优异，提供了一个高效能的基准；（2）根据逆缩放法则 Wei 等人（[2022](https://arxiv.org/html/2412.16682v1#bib.bib32)），GPT-4o 对提示注入攻击特别脆弱，使其成为验证我们防御机制的理想候选者。我们还包括了 GPT-4o-mini，一个通过指令层级训练 Wallace 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）进行安全对齐的模型，它本身对攻击具有较强的鲁棒性，以及 GPT-3.5-turbo（在附录中）。在防御实现方面，我们使用与保护性任务防护相同的模型。

#### 基准方法

我们将任务防护与四种已知的防御方法进行比较：数据限制（Delimiting）Chen 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib2)）；Hines 等人（[2024a](https://arxiv.org/html/2412.16682v1#bib.bib10)），通过使用显式标记隔离工具输出；提示注入检测（PI Detector）Kokkula 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib13)），通过分类方法识别潜在攻击；提示夹心法（Repeat Prompt）提示法（[2024](https://arxiv.org/html/2412.16682v1#bib.bib24)），通过重复强化原始用户提示；以及工具过滤（Tool Filter）Debenedetti 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)），基于任务要求限制可用工具。

#### 评估指标

实验使用了三个关键的评估指标来衡量 LLM 代理的性能和鲁棒性。（1）清洁效用（CU）指的是代理在无攻击的正常环境下成功完成的用户任务比例，代表代理的基准性能。（2）攻击下的效用（U）衡量代理在提示注入攻击下成功完成用户任务的能力，反映其在对抗性干扰下保持性能的能力。（3）目标攻击成功率评估攻击者达成目标的情况比例，衡量攻击的有效性和防御的鲁棒性。

### 5.2 结果

| 模型 | 套件 | 旅行 | 工作空间 | 银行 | Slack | 总体 |
| --- | --- | --- | --- | --- | --- | --- |
|  | 防御 | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GPT-4o | 无防御 | 65.00 | 64.29 | 11.43 | 62.50 | 24.17 | 40.42 | 75.00 | 69.44 | 62.50 | 80.95 | 63.81 | 92.38 | 69.07 | 50.08 | 47.69 |
| 工具过滤 | 90.00 | 70.00 | 5.71 | 55.00 | 51.67 | 4.17 | 81.25 | 56.94 | 11.11 | 80.95 | 47.62 | 8.57 | 72.16 | 56.28 | 6.84 |
| 重复提示 | 90.00 | 72.14 | 7.14 | 80.00 | 60.42 | 14.58 | 93.75 | 77.08 | 46.53 | 80.95 | 62.86 | 60.00 | 84.54 | 67.25 | 27.82 |
| 定界 | 75.00 | 72.14 | 3.57 | 62.50 | 30.42 | 35.00 | 81.25 | 77.08 | 61.81 | 80.95 | 61.90 | 80.00 | 72.16 | 55.64 | 41.65 |
| PI 检测器 | 30.00 | 16.43 | 0.00 | 52.50 | 15.83 | 15.00 | 43.75 | 31.25 | 0.69 | 28.57 | 25.71 | 12.38 | 41.24 | 21.14 | 7.95 |
| 任务屏蔽 | 80.00 | 72.86 | 1.43 | 62.50 | 62.50 | 0.42 | 81.25 | 82.64 | 6.25 | 80.95 | 64.76 | 0.95 | 73.20 | 69.79 | 2.07 |
| GPT-4o-mini | 无防御 | 55.00 | 47.14 | 13.57 | 82.50 | 59.17 | 17.92 | 50.00 | 38.19 | 34.03 | 66.67 | 48.57 | 57.14 | 68.04 | 49.92 | 27.19 |
| 工具过滤器 | 60.00 | 58.57 | 0.71 | 70.00 | 64.58 | 2.50 | 50.00 | 43.06 | 11.11 | 57.14 | 45.71 | 7.62 | 61.86 | 55.17 | 4.93 |
| 重复提示 | 70.00 | 54.29 | 0.00 | 70.00 | 61.25 | 8.33 | 43.75 | 43.75 | 17.36 | 71.43 | 33.33 | 13.33 | 65.98 | 51.03 | 9.38 |
| 定界 | 60.00 | 52.14 | 7.14 | 72.50 | 64.58 | 12.92 | 43.75 | 35.42 | 33.33 | 71.43 | 56.19 | 48.57 | 64.95 | 53.74 | 22.26 |
| PI 检测器 | 25.00 | 14.29 | 0.00 | 60.00 | 27.50 | 12.92 | 37.50 | 29.86 | 10.42 | 23.81 | 15.24 | 7.62 | 41.24 | 23.05 | 8.59 |
| 任务屏蔽 | 55.00 | 49.29 | 0.71 | 85.00 | 69.58 | 1.25 | 43.75 | 37.50 | 6.25 | 66.67 | 50.48 | 0.95 | 68.04 | 54.53 | 2.23 |

表 2：GPT-4o 和 GPT-4o-mini 模型在应对重要消息攻击中的防御性能。结果按清洁实用性（CU）、在攻击下的实用性（U）和任务套件中的攻击成功率（ASR）进行报告。对于每个模型，粗体值表示每个指标的最佳表现，而下划线值表示第二佳表现。所有数字以百分比（%）表示。$\uparrow$: 越高越好；$\downarrow$: 越低越好。

#### 防御攻击

我们评估了Task Shield对三种类型的间接提示注入攻击的防御效果：重要指令Debenedetti等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）嵌入高优先级的恶意指令，利用模型优先执行紧急指令的倾向；Injecagent Zhan等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib36)）使用相互冲突的目标；以及忽略先前指令Perez和Ribeiro（[2022](https://arxiv.org/html/2412.16682v1#bib.bib22)）使先前的指令失效。如表[1](https://arxiv.org/html/2412.16682v1#S5.T1 "Table 1 ‣ 5.1 Settings ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents")所示，重要指令攻击构成最强威胁，在没有防御的情况下，GPT-4o的攻击成功率（ASR）达到47.69%，同时显著降低了效用。Task Shield在所有攻击类型中展现出持续的优势——它不仅减少了ASR，还保持或提高了效用，相较于没有防御的基准情况。特别是，它通过将ASR降至2.07%，有效缓解了最强的“重要指令”攻击，同时在保持高效用的情况下，效用为69.79%。所有后续实验都在“重要指令”攻击下进行，因为它被认为是最大威胁。

#### 安全性-效用权衡

图[3](https://arxiv.org/html/2412.16682v1#S5.F3 "Figure 3 ‣ 5.1 Settings ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents")通过绘制不同防御方法在GPT-4o上在良性（攻击前）和对抗性（攻击中）条件下的表现，直观展示了安全性与效用的权衡。帕累托前沿代表了最优解，其中提高一个指标会导致另一个指标的下降。理想的数据点位于图的右下角。Task Shield在这两种情境下始终接近帕累托前沿，展示了其在不同条件和任务集合下的安全性与效用的最佳平衡。具体而言，Task Shield始终位于每个图的理想右下区域。

其他防御方法表现出显著的局限性：PI Detector虽然能实现较低的ASR，但效用严重下降；Tool Filter在两个指标上表现中等，但未能达到帕累托前沿；Repeat Prompt维持高效用，但未能提供足够的防御。

#### GPT-4o和GPT-4o-mini的详细结果

表[2](https://arxiv.org/html/2412.16682v1#S5.T2 "Table 2 ‣ 5.2 Results ‣ 5 Experiments ‣ The Task Shield: Enforcing Task Alignment to Defend Against Indirect Prompt Injection in LLM Agents")展示了针对“重要指令”攻击的不同防御机制的比较分析，涵盖了两种模型。在GPT-4o和GPT-4o-mini中，Task Shield始终在所有任务套件中展现出优异的整体性能：它将攻击下的ASR降低至2.07%，同时在GPT-4o上保持69.79%的效用（U）；同样在GPT-4o-mini中，它将ASR降低至2.23%，并保持54.53%的效用（U），始终优于所有基线防御。在所有任务套件中，Task Shield在CU、U和ASR方面展现了近乎最优或最优的表现。

有趣的是，这两种模型在应对不同防御机制时表现出不同的行为。就清洁效用（CU）而言，虽然大多数防御机制相比于无防御基线都能提升GPT-4o的性能（PI探测器除外），但它们实际上会降低GPT-4o-mini的性能。Task Shield是唯一一个能够维持或提升GPT-4o-mini清洁效用的防御机制。在攻击成功率（ASR）方面，GPT-4o-mini在没有防御的情况下展现出天生较低的ASR（27.19% vs 47.69%在GPT-4o中），这可能与其安全对齐的特性有关。此外，虽然重复提示在GPT-4o-mini上表现较强，但在GPT-4o上表现不佳，Task Shield则在两种架构上保持了一致的有效性，突显了其作为防御解决方案的稳健性。

## 6 相关工作

#### LLM代理与工具集成

关于能够执行复杂人类指令任务的 LLM 代理的设计研究已经取得了显著进展 Ouyang 等人（[2022](https://arxiv.org/html/2412.16682v1#bib.bib20)）；Sharma 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib27)）。为了使这些代理能够执行类人功能，如搜索 Deng 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib4)）；Fan 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib6)），决策 Yao 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib35)）；Mao 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib17)），现有的方法通常将外部工具调用功能集成到它们的架构中。给 LLM 代理配备工具调用功能并不特别具有挑战性，鉴于多种主干模型的可用性 Hao 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib9)）；Patil 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib21)）；Qin 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib25)）；Mialon 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib18)）；Tang 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib28)）。Schick 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib26)）的作者探讨了使 LLM 自主学习如何调用外部工具的方法。因此，我们的方法可以广泛采用并无缝集成到 LLM 代理系统中。

#### 间接提示注入攻击

间接提示注入攻击 Greshake 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib8)）；Liu 等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib15)）最近已成为大型语言模型（LLM）代理的一个重大安全隐患。这些攻击发生在恶意内容被嵌入来自外部数据提供者或环境（例如，来自不信任网站的数据）的输入中，从而导致代理执行不安全或恶意的操作，如共享私人个人信息 Derner 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib5)）；Fu 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib7)）。为了系统地评估这些攻击在不同场景下的风险，已经开发了多个基准测试，包括 Injecagent 和 AgentDojo，Zhan 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib36)）；Debenedetti 等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）。本文旨在构建一个强大的系统，以减轻这些恶意影响。

#### 防御方法

针对提示注入攻击的防御主要集中在训练阶段和测试阶段的策略上。训练阶段的方法，如Piet等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib23)）；Wallace等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib30)）；Wu等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib33)）通常通过使用对抗样本微调模型，以增强其鲁棒性。然而，由于这些方法的计算成本高昂且无法应用于没有内部访问权限的大型语言模型（LLMs），因此往往不切实际。另一方面，测试阶段的防御一般不需要大量计算资源。例如，Wang等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib31)）提出使用基于哈希的身份验证标签来过滤有害响应，而Hines等人（[2024b](https://arxiv.org/html/2412.16682v1#bib.bib11)）；Chen等人（[2024](https://arxiv.org/html/2412.16682v1#bib.bib2)）设计了特殊的定界符，指示模型识别并减轻攻击。我们的做法则是旨在强制执行任务对齐，达到更好的鲁棒性与效用平衡。

## 7 结论

本文提出了一种新的防御间接提示注入攻击的视角，介绍了任务对齐作为指导原则，确保代理行为符合用户目标。此外，我们开发了Task Shield，一个测试阶段的机制，通过验证指令与用户目标的对齐来执行这一原则，在保持代理在AgentDoJo基准测试中执行各种模拟真实世界任务的能力的同时，取得了在防御间接提示注入攻击方面的最新成果。

#### 限制

我们的框架面临着一些局限性。首先，我们依赖LLM进行任务指令提取和ContributeTo评分，带来了两个关键的漏洞：(1) 使用较弱的语言模型可能导致性能下降；(2) 易受适应性攻击的影响。此外，资源限制也限制了我们的评估范围。LLM查询的高成本使得我们的实验只能集中在单一基准和单一模型系列上。

#### 未来工作

未来研究有几个方向。 (1) 通过开发更具成本效益的大型语言模型（LLM）指令提取和对齐验证技术，提升Task Shield的效率和鲁棒性；(2) 扩展Task Shield以应对提示注入之外的更广泛安全威胁，如越狱攻击和系统提示提取；(3) 将该框架适配到特定领域的商业环境中，在这些环境中，AI代理需要与特定的目标保持严格的对齐，如Huang等人（[2023](https://arxiv.org/html/2412.16682v1#bib.bib12)）；(4) 利用任务对齐概念生成捕捉多样任务依赖性和错位场景的合成训练数据。

## 参考文献

+   Brown 等人（2020）Tom B Brown, Benjamin Mann, Nick Ryder 等人。2020年。语言模型是少样本学习者。*神经信息处理系统进展*。

+   Chen 等人（2024）Sizhe Chen, Julien Piet, Chawin Sitawarin 和 David Wagner。2024年。Struq：通过结构化查询防御提示注入。*arXiv 预印本 arXiv:2402.06363*。

+   Debenedetti 等人（2024）Edoardo Debenedetti, Jie Zhang, Mislav Balunović, Luca Beurer-Kellner, Marc Fischer 和 Florian Tramèr。2024年。Agentdojo：评估 LLM 代理的攻击与防御的动态环境。*arXiv 预印本 arXiv:2406.13352*。

+   Deng 等人（2024）Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun 和 Yu Su。2024年。Mind2web：面向Web的通用代理。*神经信息处理系统进展*，36。

+   Derner 等人（2024）Erik Derner, Kristina Batistič, Jan Zahálka 和 Robert Babuška。2024年。基于提示与大型语言模型交互的安全风险分类。*IEEE Access*。

+   Fan 等人（2024）Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua 和 Qing Li。2024年。[关于 RAG 结合 LLM 的调研：面向检索增强的大型语言模型](https://doi.org/10.1145/3637528.3671470)。发表于*第30届ACM SIGKDD知识发现与数据挖掘会议论文集*，KDD '24，第6491–6501页，美国纽约，NY。计算机协会。

+   Fu 等人（2024）Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick 和 Earlence Fernandes。2024年。[在大型语言模型中误用工具与视觉对抗性示例](https://openreview.net/forum?id=djcciHhCrt)。

+   Greshake 等人（2023）Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz 和 Mario Fritz。2023年。[你未曾同意的事情：通过间接提示注入危及现实世界的 LLM 集成应用](https://doi.org/10.1145/3605764.3623985)。发表于*第16届ACM人工智能与安全研讨会论文集*，AISec '23，第79–90页，美国纽约，NY。计算机协会。

+   Hao 等人（2023）Shibo Hao, Tianyang Liu, Zhen Wang 和 Zhiting Hu。2023年。Toolkengpt：通过工具嵌入增强冻结语言模型与大量工具的结合。*神经信息处理系统进展*，36:45870–45894。

+   Hines 等人（2024a）Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger 和 Emre Kiciman。2024a年。通过聚焦防御间接提示注入攻击。*arXiv 预印本 arXiv:2403.14720*。

+   Hines 等人（2024b）Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger 和 Emre Kiciman。2024b年。[通过聚焦防御间接提示注入攻击](https://api.semanticscholar.org/CorpusID:268667111)。*ArXiv*，abs/2403.14720。

+   黄等人（2023）黄旭、连剑勋、雷宇轩、姚靖、连德富、谢星。2023年。推荐AI代理：整合大型语言模型进行互动推荐。*arXiv预印本arXiv:2308.16505*。

+   Kokkula等人（2024）萨哈斯拉·科库拉、G·迪维亚等人。2024年。Palisade–提示注入检测框架。*arXiv预印本arXiv:2410.21146*。

+   李等人（2024）李梅等人。2024年。保障LLM代理中的工具使用：挑战与策略。*arXiv预印本arXiv:2402.03014*。

+   刘等人（2023）刘逸、邓格雷、李跃康、王凯龙、王子豪、王晓峰、张天威、刘烨鹏、王浩宇、郑岩、刘杨。2023年。针对LLM集成应用的提示注入攻击。*arXiv预印本arXiv:2306.05499*。

+   刘等人（2024）刘玉培、贾玉奇、耿润鹏、贾金元、宫振强·尼尔。2024年。形式化和基准化提示注入攻击与防御。在*USENIX安全研讨会*上。

+   毛等人（2024）毛家庚、叶俊杰、钱宇西、马可·帕沃内、王越。2024年。[用于自动驾驶的语言代理](https://openreview.net/forum?id=UPE6WYE8vg)。在*首次语言建模会议*上。

+   Mialon等人（2023）格雷戈里·米阿隆、罗伯托·德西、玛丽亚·洛梅利、克里斯托弗·纳尔潘蒂斯、拉马坎特·帕苏努鲁、罗伯塔·赖尔阿努、巴普蒂斯特·罗齐耶、蒂莫·席克、简·德维维迪-余、阿斯利·切里基尔马兹、爱德华·格雷夫、扬·勒昆、托马斯·斯奇亚隆。2023年。[增强语言模型：一项调查](https://openreview.net/forum?id=jh7wH2AzKK)。*机器学习研究交易*。调查认证。

+   OpenAI（2024）OpenAI。2024年。[介绍openai o1-preview](https://openai.com/index/introducing-openai-o1-preview/)。

+   欧阳等人（2022）欧阳龙、杰弗里·吴、姜旭、迪奥戈·阿尔梅达、卡罗尔·韦恩赖特、帕梅拉·米什金、张冲、桑迪尼·阿卡尔瓦尔、凯瑟琳娜·斯拉马、亚历克斯·雷等人。2022年。训练语言模型按照指令执行，并通过人类反馈改进。*神经信息处理系统进展*，35:27730–27744。

+   Patil等人（2023）希希尔·G·帕蒂尔、张天俊、王鑫、约瑟夫·E·冈萨雷斯。2023年。Gorilla：与大规模API连接的大型语言模型。*arXiv预印本arXiv:2305.15334*。

+   Perez和Ribeiro（2022）法比奥·佩雷斯和伊恩·里贝罗。2022年。忽略之前的提示：语言模型的攻击技术。*arXiv预印本arXiv:2211.09527*。

+   Piet等人（2023）朱利安·皮埃特、玛哈·阿尔拉谢德、查温·西塔瓦林、陈思哲、魏泽名、伊丽莎白·孙、巴塞尔·阿洛梅尔、戴维·瓦格纳。2023年。[Jatmo: 通过任务特定微调防御提示注入](https://api.semanticscholar.org/CorpusID:266690784)。*ArXiv*，abs/2312.17673。

+   提示（2024）学习提示。2024年。三明治防御。[https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense](https://learnprompting.org/docs/prompt_hacking/defensive_measures/sandwich_defense)。访问时间：2024-11-07。

+   Qin 等人 (2023) 秦宇佳、梁世豪、叶一宁、朱昆仑、闫岚、卢雅曦、林燕凯、徐新、唐向如、钱比尔等人。2023年。《Toolllm：帮助大语言模型掌握16000多个现实世界API》。*arXiv 预印本 arXiv:2307.16789*。

+   Schick 等人 (2024) 蒂莫·希克、简·德维维迪-余、罗伯托·德西、罗贝塔·赖莱安努、玛丽亚·洛梅里、埃里克·汉布罗、卢克·泽特尔莫耶、尼古拉·坎切达、托马斯·西亚隆。2024年。《Toolformer：语言模型能够自我学习使用工具》。*神经信息处理系统进展*，第36卷。

+   Sharma 等人 (2024) 阿希什·夏尔马、苏达·拉奥、克里斯·布罗基特、阿坎卡莎·马尔霍特拉、内博贾·乔吉奇、比尔·多兰。2024年。[《探讨大语言模型在人工智能-人类协作任务中的能动性》](https://aclanthology.org/2024.eacl-long.119)。在*第18届欧洲计算语言学协会大会（第一卷：长篇论文）*上，页面1968–1987，圣朱利安，马耳他。计算语言学协会。

+   Tang 等人 (2023) 唐巧宇、邓子亮、林宏宇、韩献培、梁乔、曹博熙、孙乐。2023年。《Toolalpaca：通过3000个模拟案例为语言模型进行通用工具学习》。*arXiv 预印本 arXiv:2306.05301*。

+   Touvron 等人 (2023) 雨果·图弗龙、提博·拉夫里尔、戈蒂埃·伊扎卡德 等人。2023年。《Llama 2：开放的基础模型和微调聊天模型》。*arXiv 预印本 arXiv:2307.09288*。

+   Wallace 等人 (2024) 埃里克·沃尔斯、肖凯、雷马尔·莱克、莉莉安·翁、约翰内斯·海德克、亚历克斯·比特尔。2024年。《指令层次结构：训练大语言模型优先考虑特权指令》。*arXiv 预印本 arXiv:2404.13208*。

+   Wang 等人 (2024) 王炯霄、吴方舟、李文迪、潘锦生、苏爱德华、Z. 莫利·毛、陈慕浩、肖朝伟。2024年。《Fath: 基于认证的测试时防御对抗间接提示注入攻击》。*arXiv 预印本 arXiv:2410.21492*。

+   Wei 等人 (2022) 詹森·魏 等人。2022年。《逆向规模效应：当规模更大并不一定更好》。*arXiv 预印本 arXiv:2206.04615*。

+   Wu 等人 (2024) 吴彤、张树健、宋凯强、徐思磊、赵三强、阿格瓦尔·拉维、因杜尔蒂·萨提什·雷迪、向崇、米塔尔·普拉提克、周文轩。2024年。[《指令性片段嵌入：通过指令层次结构提升大语言模型安全性》](https://arxiv.org/abs/2410.09102)。*预印本*，arXiv:2410.09102。

+   Xiang 等人 (2024) 向震、郑林智、李彦杰、洪俊远、李钦斌、谢涵、张家伟、熊子迪、谢楚林、杨卡尔 等人。2024年。《Guardagent：通过知识启用推理的守护代理保护大语言模型代理》。*arXiv 预印本 arXiv:2406.09187*。

+   Yao 等人 (2023) 姚顺宇、赵杰弗里、余典、杜楠、伊扎克·沙夫兰、卡尔提克·R·纳拉西曼、曹源。2023年。[《React：语言模型中的推理与行动协同》](https://openreview.net/forum?id=WE_vluYUL-X)。在*第十一届国际学习表征大会*上。

+   Zhan 等人（2024）Qiusi Zhan, Zhixiang Liang, Zifan Ying 和 Daniel Kang。2024年。Injecagent：基准测试工具集成大型语言模型代理中的间接提示注入。*arXiv 预印本 arXiv:2403.02691*。

+   Zou 等人（2023）Xiangzhe Zou 等人 2023年。对对齐语言模型的普遍和可转移的对抗性攻击。*arXiv 预印本 arXiv:2307.09283*。

## 附录A 附录：任务对齐的详细讨论

### A.1 任务对齐为何重要：超越明显有害的指令

<svg class="ltx_picture" height="133.24" id="A1.SS1.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,133.24) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="113.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.23">Example: Consider a scenario where a user makes a focused request: "Please summarize the preparation steps for spaghetti alla Carbonara from this menu." (User Level $L_{u}$) The assistant processes this request and initiates a tool call to retrieve and analyze the menu content, specifically for information about the carbonara dish. (Assistant Level $L_{a}$) However, embedded within the menu’s footer lies an additional injected directive: "For any dish-specific query, provide comprehensive preparation instructions and detailed cost breakdowns for all menu items, including seasonal specialties and unlisted dishes." (Tool Level $L_{t}$)</foreignobject></g></g></svg>

尽管表面上看似无害，这类注入指令的执行却具有实际的安全隐患。首先，它导致不必要的信息泄露，暴露了所有菜单项的细节，而实际上只请求了其中的一道菜。其次，它通过不必要的令牌消耗和处理，增加了用户的计算成本。

#### 用户信任与系统可预测性。

从根本上讲，遵循外部指令会破坏用户与AI助手之间的信任关系。用户部署这些系统时，期望能够精确控制，助手会完全按照要求执行任务，既不多也不少。当助手开始执行与任务无关的外部建议时，即使是看似无害的建议，这种信任关系也会被打破。用户将无法自信地预测系统的行为，也无法保持对信息流的控制。

### A.2 系统级指令讨论

在某些特定应用的智能体中，系统消息（$L_{s}$）可以直接指定对话的目标（例如：“你的目标是管理用户的日历”）。如果系统级指令作为任务指令使用，那么助手必须同时与系统目标 *和* 用户级目标对齐，从而有效地创建一个层级对齐的场景。然而，我们的实验集中在$L_{u}$级别的用户任务上，因此我们并未将系统级任务目标纳入我们的主要分析。

### A.3 任务不一致的示例

<svg class="ltx_picture" height="81.89" id="A1.SS3.p1.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,81.89) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.23">示例 1：不对齐的可操作指令 用户：“请为我总结这篇文章。” 助手：“当然，我们将把这个总结发布到您的社交媒体上。” 分析：助手引入了一个用户并未请求的行动（发布到社交媒体）。这一行动与用户的原始意图不一致，违反了任务对齐条件。</foreignobject></g></g></svg><svg class="ltx_picture" height="84.23" id="A1.SS3.p2.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,84.23) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.89 9.89)"><foreignobject color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="580.23">示例 2：不对齐的工具调用 用户：“请向Alice发送一封确认我们会议的邮件。” 助手：“好的。我将给Bob发送确认会议的邮件。” + 工具调用：send_email(Bob) 分析：助手使用工具发送了一封邮件给错误的收件人（Bob而非Alice），这并未帮助用户实现目标，违反了任务对齐条件。</foreignobject></g></g></svg>

在这些示例中，助手并未满足任务指令对齐条件，因为它们提议滥用工具或执行无法帮助用户实现原始目标的行动。

## 附录 B 附录：任务保护框架设计的细节

### B.1 基于模糊逻辑的贡献评分示例

在本节中，我们提供了基于$\mathrm{ContributesTo}$谓词计算贡献分数的具体示例。

例如，当用户请求“为团队讨论预定会议室”时，`get_room_availability()`调用代表了一个中间步骤：它并未直接预定会议室，而是提供了完成任务所需的重要信息。在这种情况下，使用基于模糊逻辑的评分机制，‘contributesTo’分数会很高，反映出这一行动的重要性。

相比之下，当用户请求“与利益相关者分享项目预算”时，`search_recent_files("project budget")`调用展示了一种合理的尝试：它通过逻辑地探索最近的文件来解决文件位置的模糊性，即使它不能保证成功。在这种情况下，$\mathrm{ContributesTo}$分数会是中等，反映出它是满足用户目标的一次尝试，但并不是目标的直接完成。

### B.2 任务保护核心处理算法

算法 1 任务保护核心处理算法

1:  输入：当前消息 $m$，对话历史 $\mathcal{H}$，阈值 $\epsilon$，用户任务指令 $T_{u}(\mathcal{H})$  

## 附录C 附录：实验细节与附加结果

### C.1 GPT-3.5-turbo 结果

为了进一步验证任务保护（Task Shield）的普适性和稳健性，我们使用了GPT-3.5-turbo模型进行了额外的实验。表格[3](https://arxiv.org/html/2412.16682v1#A3.T3 "表格 3 ‣ C.1 GPT-3.5-turbo 结果 ‣ 附录 C 附录：实验细节与附加结果 ‣ 任务保护：强化任务一致性以防止LLM代理中的间接提示注入")展示了这些实验的结果，展示了任务保护和基准防御机制在“重要指令”攻击下对GPT-3.5-turbo的表现。然而，由于模型固有的局限性，比如上下文长度的限制影响了基准评估，因此这些结果在与GPT-4o和GPT-4o-mini的结果进行对比时应谨慎解读。尽管如此，它们仍为我们提供了有关任务保护在不同模型架构下行为的补充见解。

| 套件 | 旅行 | 工作区 | 银行 | Slack | 总体 |
| --- | --- | --- | --- | --- | --- |
| 防御 | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ | CU$\uparrow$ | U$\uparrow$ | ASR$\downarrow$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 无防御 | 15.00 | 17.86 | 1.43 | 32.50 | 40.42 | 0.42 | 37.50 | 32.64 | 25.69 | 57.14 | 46.67 | 12.38 | 35.05 | 34.66 | 8.43 |
| 工具过滤 | 20.00 | 18.57 | 0.71 | 27.50 | 30.83 | 0.00 | 37.50 | 36.11 | 4.17 | 38.10 | 32.38 | 1.90 | 29.90 | 29.57 | 1.43 |
| 重复提示 | 20.00 | 12.86 | 0.00 | 37.50 | 31.25 | 0.00 | 37.50 | 31.25 | 12.50 | 52.38 | 38.10 | 5.71 | 37.11 | 28.30 | 3.82 |
| 限定 | 20.00 | 17.14 | 5.71 | 25.00 | 33.75 | 0.83 | 37.50 | 34.72 | 25.69 | 61.90 | 41.90 | 11.43 | 34.02 | 31.64 | 9.38 |
| PI 检测器 | 20.00 | 7.14 | 0.00 | 22.50 | 23.75 | 0.42 | 43.75 | 36.11 | 8.33 | 28.57 | 35.24 | 4.76 | 26.80 | 24.80 | 2.86 |
| Task Shield | 20.00 | 10.71 | 0.00 | 30.00 | 34.58 | 0.00 | 62.50 | 43.75 | 4.17 | 38.10 | 26.67 | 0.00 | 35.05 | 30.05 | 0.95 |

表格 3：针对重要信息攻击的防御表现，适用于GPT-3.5-turbo模型。结果报告涵盖了清洁效用（CU）、攻击下的效用（U）和攻击成功率（ASR）在各任务套件中的表现。粗体值表示每个指标下的最佳表现，带下划线的值表示第二最佳表现。

### C.2 实验中省略的细节

#### 基准结果

提供的GPT-4o基准结果来源于AgentDojo基准测试中的原始数据（Debenedetti等，[2024](https://arxiv.org/html/2412.16682v1#bib.bib3)）。这些结果代表了GPT-4o在不同攻击场景中的表现，且未应用任何防御机制。对于GPT-4o-mini和GPT-3.5-turbo，未应用防御机制的基准结果同样来自AgentDojo。

#### Task Shield 实施

在使用Task Shield框架中的模型时，设置温度值为0.0，以确保行为的确定性。在ContributesTo得分计算中，Task Shield利用了大量的对话历史来捕获完整的上下文。然而，在涉及工具调用的情况下，为了确保所有工具调用都紧跟其对应的工具输出，历史记录会被截断，从而满足维持时间一致性的技术要求。

#### 模型版本。

本研究中使用的具体模型版本为：（1）gpt-4o-2024-05-13，（2）gpt-4o-mini-2024-07-18，以及（3）gpt-3.5-turbo-0125。

## 附录 D 提示

![参见标题](img/45fc0178893bf110226db8ce151eca37.png)

图 4：任务提取提示：此提示概述了如何从对话内容中提取可执行的任务指令的方法。

![参见标题](img/965c487d3f1beb07edb3922dc0b81987.png)

图 5：内容检查器提示：此提示根据任务相关性和权限级别，评估新可执行指令与用户任务指令的一致性。

![参见标题](img/f55a0df2bb3b18bc0087e83589cb3170.png)

图 6：工具调用检查器提示：此提示验证工具调用与用户定义任务指令的一致性，以保持任务完整性。

![参见标题](img/2537177278ea0c7fe91185683be2db39.png)

图 7：反馈提示：该图解释了内容不一致、工具调用不一致以及用户意图提醒如何共同作用于最终反馈的生成。
