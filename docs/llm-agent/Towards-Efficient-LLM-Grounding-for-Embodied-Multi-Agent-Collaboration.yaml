- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:46:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:46:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向高效LLM基础奠定的具身多智能体协作
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14314](https://ar5iv.labs.arxiv.org/html/2405.14314)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14314](https://ar5iv.labs.arxiv.org/html/2405.14314)
- en: Yang Zhang^(1,2) ,  Shixin Yang^(3∗),  Chenjia Bai²  ,  Fei Wu^(2,4),  Xiu Li¹,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yang Zhang^(1,2) ,  Shixin Yang^(3∗),  Chenjia Bai²  ,  Fei Wu^(2,4),  Xiu Li¹,
- en: Zhen Wang^(2,3),  Xuelong Li^(2,5)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhen Wang^(2,3),  Xuelong Li^(2,5)
- en: ¹Tsinghua University, ²Shanghai AI Laboratory, ³Northwestern Polytechnical University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹清华大学，²上海人工智能实验室，³西北工业大学
- en: ⁴Zhejiang University, ⁵Institute of Artificial Intelligence (TeleAI), China
    Telecom
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴浙江大学，⁵中国电信人工智能研究院（TeleAI）
- en: baichenjia@pjlab.org.cn, xuelong_li@ieee.org Equal Contribution
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: baichenjia@pjlab.org.cn, xuelong_li@ieee.org 等贡献
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Grounding the reasoning ability of large language models (LLMs) for embodied
    tasks is challenging due to the complexity of the physical world. Especially,
    LLM planning for multi-agent collaboration requires communication of agents or
    credit assignment as the feedback to re-adjust the proposed plans and achieve
    effective coordination. However, existing methods that overly rely on physical
    verification or self-reflection suffer from excessive and inefficient querying
    of LLMs. In this paper, we propose a novel framework for multi-agent collaboration
    that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement
    of plans. Specifically, we perform critic regression to learn a sequential advantage
    function from LLM-planned data, and then treat the LLM planner as an optimizer
    to generate actions that maximize the advantage function. It endows the LLM with
    the foresight to discern whether the action contributes to accomplishing the final
    task. We provide theoretical analysis by extending advantage-weighted regression
    in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI
    and a difficult variant of RoCoBench show that ReAd surpasses baselines in success
    rate, and also significantly decreases the interaction steps of agents and query
    rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results
    are given at [https://read-llm.github.io/](https://read-llm.github.io/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将大型语言模型（LLMs）在具身任务中的推理能力奠定基础，由于物理世界的复杂性，这是一项具有挑战性的任务。特别是，LLM对多智能体协作的规划需要智能体之间的沟通或作为反馈的信用分配，以重新调整提出的计划并实现有效的协调。然而，现有方法过度依赖物理验证或自我反思，导致对LLMs的查询过多且效率低下。本文提出了一种新颖的多智能体协作框架，引入了强化优势反馈（ReAd）以高效地自我完善计划。具体而言，我们执行批评回归以从LLM规划的数据中学习序列优势函数，然后将LLM规划者视为优化器，生成最大化优势函数的动作。它赋予LLM洞察力，以辨别动作是否有助于完成最终任务。我们通过将优势加权回归在强化学习中的扩展到多智能体系统来提供理论分析。在Overcooked-AI和RoCoBench的困难变体上的实验表明，ReAd在成功率上超越了基线，并且显著减少了智能体的交互步骤和LLMs的查询轮次，证明了其在奠定LLMs基础上的高效性。更多结果请参见
    [https://read-llm.github.io/](https://read-llm.github.io/)。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have exhibited remarkable capabilities across various
    domains, including long-text understanding, reasoning, and text generation [devlin2019bert](#bib.bib13)
    ; [radford2019language](#bib.bib47) ; [brown2020language](#bib.bib6) ; [raffel2023exploring](#bib.bib48)
    . Benefiting from large-scale text corpora mined from the web, LLMs can absorb
    and capture vast quantities of knowledge about the world for decision-making.
    Recent research has shown that LLMs can interactively make decisions through zero-shot
    or few-shot example prompting to solve embodied tasks [Firoozi2023FoundationMI](#bib.bib18)
    via chain-of-thought (CoT) [wei2023chainofthought](#bib.bib61) or tree-of-thought
    [yao2023tree](#bib.bib67) planning. However, LLMs perform planning only using
    their internal knowledge, which is often not grounded in the physical world due
    to the lack of task-specific knowledge of complex embodied agents. Such a problem
    can lead to fact hallucination and nonsensical instruction interpretation issues
    in reasoning [ahn2022i](#bib.bib2) . To prevent LLMs from outputting infeasible
    plans in embodied tasks, existing methods mostly design a closed-loop framework
    for the interaction process with feedback. Specifically, one line of research
    adopts *self-reflection* by performing self-evaluation by LLMs to improve the
    plan generation of LLM planner [shinn2023reflexion](#bib.bib51) ; [yao2023react](#bib.bib68)
    ; [hao2023reasoning](#bib.bib21) ; [liu2023RAFA](#bib.bib40) ; and the other works
    perform *physical verification* by using feedback of the external environment
    to dynamically replan depending on unexpected feedback [huang2022Monologue](#bib.bib26)
    ; [song2023llmplanner](#bib.bib53) . Nevertheless, these feedback is often sparse
    or designed heuristically, a more principled feedback mechanism for LLM-based
    embodied task planning is still lacking.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种领域展示了卓越的能力，包括长文本理解、推理和文本生成 [devlin2019bert](#bib.bib13) ; [radford2019language](#bib.bib47)
    ; [brown2020language](#bib.bib6) ; [raffel2023exploring](#bib.bib48) 。得益于从网络中挖掘的大规模文本语料库，LLMs可以吸收并捕捉大量关于世界的知识，用于决策。近期研究表明，LLMs可以通过零-shot或少-shot示例提示进行交互式决策，以解决具身任务
    [Firoozi2023FoundationMI](#bib.bib18) ，通过链式思维（CoT） [wei2023chainofthought](#bib.bib61)
    或思维树 [yao2023tree](#bib.bib67) 规划。然而，LLMs仅使用其内部知识进行规划，由于缺乏复杂具身体代理的任务特定知识，这种知识通常与物理世界脱节。这种问题可能导致事实幻觉和推理中的荒谬指令解释
    [ahn2022i](#bib.bib2) 。为了防止LLMs在具身任务中输出不可行的计划，现有的方法大多设计了一个闭环框架来进行交互过程中的反馈。具体来说，一类研究通过LLMs自我评估来采用*自我反思*，以改进LLM规划器的计划生成
    [shinn2023reflexion](#bib.bib51) ; [yao2023react](#bib.bib68) ; [hao2023reasoning](#bib.bib21)
    ; [liu2023RAFA](#bib.bib40) ; 另一类研究则通过使用外部环境的反馈来执行*物理验证*，根据意外反馈动态重新规划 [huang2022Monologue](#bib.bib26)
    ; [song2023llmplanner](#bib.bib53) 。然而，这些反馈通常是稀疏的或启发式设计的，LLM基于具身任务规划的更有原则的反馈机制仍然缺乏。
- en: '![Refer to caption](img/e82ad0480cf677ae876ebb8425e52e37.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e82ad0480cf677ae876ebb8425e52e37.png)'
- en: 'Figure 1: An illustration of the negotiation process of RoCo and our method.
    RoCo interacts with the environment for each plan and takes the environment’s
    feedback as prompts. In contrast, our method takes the advantage function (Adv.)
    evaluated by a critic as feedback, and revises the plan if the advantage value
    is lower than the threshold, which significantly reduces the interaction rounds
    to the environment.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：RoCo与我们方法的协商过程示意图。RoCo针对每个计划与环境进行交互，并将环境的反馈作为提示。相比之下，我们的方法利用由评论者评估的优势函数（Adv.）作为反馈，如果优势值低于阈值，则修订计划，这显著减少了与环境的交互轮次。
- en: Considering more challenging planning problems in multi-agent settings, an LLM-based
    agent needs to cooperate with other agents through communication and negotiation,
    which causes more difficulties in effective feedback. Specifically, it is hard
    for both self-reflection and physical verification to evaluate the effects of
    individual action in a team outcome of multi-agents. Consequently, the feedback
    mechanisms suffer from either excessive queries of LLMs or frequent interactions
    with the physical environment. For instance, RoCo [mandi2023roco](#bib.bib42)
    introduces physical verification as feedback to refine the LLM-generated actions
    in multi-agent cooperative settings, but faces the difficulty of poor efficiency.
    As we illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration"), RoCo requires excessive
    interaction to obtain physical feedback and queries to LLMs to get feasible joint-action
    plans, which can be heavily inefficient for embodied tasks. In contrast, various
    methods in Multi-Agent Reinforcement Learning (MARL) [MAsurvey](#bib.bib74) have
    developed value or advantage decomposition theories for credit assignment of multiple
    agents [QMIX](#bib.bib49) ; [kuba2022happo](#bib.bib29) , which provide effective
    mechanisms to evaluate the contribution of individual actions in accomplishing
    final tasks and can generate actions for monotonic policy improvement [Mirror](#bib.bib30)
    . Inspired by these principles, we ask "*How to enhance the reasoning ability
    of LLMs for embodied multi-agent collaboration* with theoretical supports of MARL?".
    Our objective is to build an efficient feedback and refinement algorithm with
    utilizing multi-agent advantage functions, for multi-agent planning assisted by
    LLMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体设置中考虑更具挑战性的规划问题时，基于LLM的智能体需要通过沟通和协商与其他智能体合作，这导致了有效反馈的更多困难。具体来说，自我反思和物理验证都难以评估单个动作对多智能体团队结果的影响。因此，反馈机制要么遭受LLM的过多查询，要么频繁与物理环境互动。例如，RoCo
    [mandi2023roco](#bib.bib42) 引入了物理验证作为反馈，以优化LLM生成的多智能体合作设置中的动作，但面临效率低下的困难。正如我们在图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration")中所示，RoCo需要过多的交互以获取物理反馈，并且需要查询LLM以获得可行的联合行动计划，这对于体现任务来说可能非常低效。相比之下，多智能体强化学习（MARL）中的各种方法
    [MAsurvey](#bib.bib74) 已经开发出价值或优势分解理论用于多个智能体的信用分配 [QMIX](#bib.bib49)；[kuba2022happo](#bib.bib29)，这些方法提供了有效的机制来评估单个动作在完成最终任务中的贡献，并可以生成单调政策改进的动作
    [Mirror](#bib.bib30)。受到这些原则的启发，我们提出了“*如何利用MARL的理论支持来增强LLM在体现多智能体合作中的推理能力*？”我们的目标是构建一个高效的反馈和优化算法，利用多智能体优势函数来辅助LLM进行多智能体规划。
- en: In this paper, we propose Reinforced Advantage (*ReAd*) as a closed-loop feedback
    for LLMs in multi-agent collaboration. We provide two optional LLM-generated plan
    refinement scheme, including Sequential Individual Plan Refinement with the local
    advantage (named *ReAd-S*) and Joint Plan Refinement with the joint advantage
    (named *ReAd-J*). Among them, (i) *ReAd-J* evaluates the advantage function of
    joint actions, which requires LLMs to generate the joint planning of all agents
    at once. In contrast, (ii) *ReAd-S* evaluates the local advantages of each agent’s
    action by following the principle of multi-agent advantage decomposition [kuba2022happo](#bib.bib29)
    in MARL, which allows LLMs to generate actions for each agent sequentially. Both
    advantage functions are estimated by a critic network that regresses LLM-planned
    data. Based on the advantage function, an LLM planner is used as an optimizer
    by prompting to generate actions that maximize the advantage value. Otherwise,
    the LLM planner is required to re-plan if the advantage value is small. We provide
    a theoretical motivation for such a process by extending advantage-weighted regression
    [peng2019advantageweighted](#bib.bib46) to multi-agent settings. In experiments,
    we extend RoCoBench [mandi2023roco](#bib.bib42) to a difficult variant, which
    we term *DV-RoCoBench*. The results on *DV-RoCoBench* and *Overcooked-AI* show
    that *ReAd* significantly decreases the interaction and query rounds, and also
    surpasses baselines in success rate, highlighting its effectiveness for grounding
    LLMs in embodied multi-agent collaboration tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了强化优势（*ReAd*）作为LLM在多智能体协作中的闭环反馈。我们提供了两种可选的LLM生成计划细化方案，包括带有局部优势的顺序个体计划细化（称为*ReAd-S*）和带有联合优势的联合计划细化（称为*ReAd-J*）。其中，(i)
    *ReAd-J*评估联合行动的优势函数，这要求LLM一次性生成所有智能体的联合计划。相比之下，(ii) *ReAd-S*通过遵循多智能体优势分解的原则[kuba2022happo](#bib.bib29)来评估每个智能体行动的局部优势，这允许LLM顺序生成每个智能体的行动。这两种优势函数均通过一个回归LLM规划数据的评论网络进行估计。基于优势函数，LLM规划器被用作优化器，通过提示生成最大化优势值的行动。否则，如果优势值较小，则要求LLM规划器重新规划。我们通过将优势加权回归[peng2019advantageweighted](#bib.bib46)扩展到多智能体设置，提供了这种过程的理论动机。在实验中，我们将RoCoBench[mandi2023roco](#bib.bib42)扩展到一个困难的变体，称为*DV-RoCoBench*。*DV-RoCoBench*和*Overcooked-AI*上的结果表明，*ReAd*显著减少了交互和查询轮次，并且在成功率上超越了基线，突显了其在具身多智能体协作任务中的有效性。
- en: 2 Preliminaries
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识
- en: We consider a Markov game, which is defined by a tuple $\langle\mathcal{N},\mathcal{S},\boldsymbol{\mathcal{A}},P,r,\gamma\rangle$.
    In the following, we consider the LLM planner as a special RL policy, which can
    be evaluated by a value function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个马尔可夫游戏，该游戏由一个元组$\langle\mathcal{N},\mathcal{S},\boldsymbol{\mathcal{A}},P,r,\gamma\rangle$定义。在下面的讨论中，我们将LLM规划器视为一种特殊的RL策略，可以通过价值函数进行评估。
- en: 3 Methodology
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: We first give definitions and learning algorithms for the two kinds of advantage
    functions in §[3.1](#S3.SS1 "3.1 Learning of Advantage Functions ‣ 3 Methodology
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"). Then,
    we provide theoretical motivation for grounding LLMs by extending advantage-weighted
    regression in multi-agent settings in §[3.2](#S3.SS2 "3.2 Theoretical Motivation
    for Grounding LLM ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied
    Multi-Agent Collaboration"). Finally, we describe how to derive Reinforced Advantage
    (*ReAd*) feedback from the theoretical motivation and use an LLM planner as an
    optimizer and refine the plan in §[3.3](#S3.SS3 "3.3 Prompting by Reinforced Advantage
    Feedback ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration").
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先给出两种优势函数的定义和学习算法，在§[3.1](#S3.SS1 "3.1 Learning of Advantage Functions ‣
    3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")中介绍。然后，我们在§[3.2](#S3.SS2
    "3.2 Theoretical Motivation for Grounding LLM ‣ 3 Methodology ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration")中通过扩展优势加权回归在多智能体环境中的应用，提供对LLM
    grounding的理论动机。最后，我们在§[3.3](#S3.SS3 "3.3 Prompting by Reinforced Advantage Feedback
    ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")中描述如何从理论动机中推导强化优势（*ReAd*）反馈，并使用LLM规划器作为优化器，细化计划。
- en: 3.1 Learning of Advantage Functions
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 优势函数的学习
- en: We first introduce the estimation of *joint* advantage function. Then the *local*
    advantage is obtained via advantage decomposition by following theories from MARL.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍*联合*优势函数的估计。然后，通过遵循MARL中的理论，通过优势分解获得*局部*优势。
- en: Joint Advantage Function. Based on joint value functions $Q_{\boldsymbol{\pi}}(s,\boldsymbol{a})$,
    we define the *joint* advantage function as
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 联合优势函数。基于联合值函数 $Q_{\boldsymbol{\pi}}(s,\boldsymbol{a})$，我们定义*联合*优势函数为
- en: '|  | $A_{\boldsymbol{\pi}}(s,\boldsymbol{a})\triangleq Q_{\boldsymbol{\pi}}(s,\boldsymbol{a})-V_{\boldsymbol{\pi}}(s),$
    |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{\boldsymbol{\pi}}(s,\boldsymbol{a})\triangleq Q_{\boldsymbol{\pi}}(s,\boldsymbol{a})-V_{\boldsymbol{\pi}}(s),$
    |  |'
- en: which evaluates the advantage value of joint actions $\boldsymbol{a}_{t}=(a_{t}^{1},a_{t}^{2},...,a_{t}^{n})$.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式评估联合动作 $\boldsymbol{a}_{t}=(a_{t}^{1},a_{t}^{2},...,a_{t}^{n})$ 的优势值。
- en: When taking WAIT action $a=w$, as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当采取 WAIT 动作 $a=w$ 时，定义为
- en: '|  | $1$2 |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Therefore, the *joint* advantage function can be derived by using only the $Q_{\boldsymbol{\pi}}$
    function, as
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*联合*优势函数可以仅通过使用 $Q_{\boldsymbol{\pi}}$ 函数来推导，公式为
- en: '|  | $A_{\boldsymbol{\pi}}(s,\boldsymbol{a})=Q_{\boldsymbol{\pi}}(s,\boldsymbol{a})-\frac{1}{\gamma}Q_{\boldsymbol{\pi}}(s,\boldsymbol{w}).$
    |  | (1) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{\boldsymbol{\pi}}(s,\boldsymbol{a})=Q_{\boldsymbol{\pi}}(s,\boldsymbol{a})-\frac{1}{\gamma}Q_{\boldsymbol{\pi}}(s,\boldsymbol{w}).$
    |  | (1) |'
- en: Local Advantage Function. In cooperative multi-agent settings, we can further
    consider the contribution to performance in different subsets of agents’ views.
    We adopt the standard definition in MARL to measure the local advantages.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 局部优势函数。在合作的多智能体设置中，我们可以进一步考虑不同子集的智能体视角对性能的贡献。我们采用 MARL 中的标准定义来衡量局部优势。
- en: Definition 1.
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.
- en: '[kuba2022happo](#bib.bib29) Let $i_{1:m}$ agent in the ordered subset. Correspondingly,
    the multi-agent local state-action value function is defined as'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[kuba2022happo](#bib.bib29) 设 $i_{1:m}$ 为有序子集中的智能体。相应地，多智能体局部状态-行动值函数定义为'
- en: '|  | $1$2 |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: and for disjoint sets $j_{1:k}$, the multi-agent local advantage function is
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不相交的集合 $j_{1:k}$，多智能体局部优势函数为
- en: '|  | $1$2 |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Monte Carlo Estimation. Both Eqs. ([1](#S3.E1 "In 3.1 Learning of Advantage
    Functions ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration")) and ([3](#S3.E3 "In Definition 1\. ‣ 3.1 Learning of Advantage
    Functions ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration")) can be estimated via the local value function $Q_{\boldsymbol{\pi}}^{i_{1:u}}(s,\boldsymbol{a}^{i_{1:u}})$,
    where the complement sets is sampled from the dataset. Then the value function
    is learned by a regression loss as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗估计。公式 ([1](#S3.E1 "在 3.1 优势函数学习 ‣ 3 方法论 ‣ 迈向高效的 LLM 基础搭建以支持多智能体协作")) 和 ([3](#S3.E3
    "在定义 1. ‣ 3.1 优势函数学习 ‣ 3 方法论 ‣ 迈向高效的 LLM 基础搭建以支持多智能体协作")) 可以通过局部值函数 $Q_{\boldsymbol{\pi}}^{i_{1:u}}(s,\boldsymbol{a}^{i_{1:u}})$
    进行估计，其中补充集合是从数据集中采样的。然后通过回归损失来学习值函数，如下所示
- en: '|  | $1$2 |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: We refer to Alg. [1](#alg1 "Algorithm 1 ‣ Appendix C Algorithmic Description
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration") in
    §[C](#A3 "Appendix C Algorithmic Description ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration") for the details. The setting of reward
    $r_{t}$ otherwise. The details of data collection are given in §[E.4](#A5.SS4
    "E.4 Dataset and Critic Network ‣ Appendix E Additional Experimental Results ‣
    Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们参考 §[C](#A3 "附录 C 算法描述 ‣ 迈向高效的 LLM 基础搭建以支持多智能体协作") 中的算法 [1](#alg1 "算法 1 ‣
    附录 C 算法描述 ‣ 迈向高效的 LLM 基础搭建以支持多智能体协作") 了解详细信息。奖励设置 $r_{t}$ 的其他情况。数据收集的详细信息见 §[E.4](#A5.SS4
    "E.4 数据集和评论网络 ‣ 附录 E 附加实验结果 ‣ 迈向高效的 LLM 基础搭建以支持多智能体协作")。
- en: Advantage Decomposition. Based on Eq. ([2](#S3.E2 "In Definition 1\. ‣ 3.1 Learning
    of Advantage Functions ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied
    Multi-Agent Collaboration")), we can express the state value function $V_{\boldsymbol{\pi}}(s)$,
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 优势分解。基于公式 ([2](#S3.E2 "在定义 1. ‣ 3.1 优势函数学习 ‣ 3 方法论 ‣ 迈向高效的 LLM 基础搭建以支持多智能体协作"))，我们可以表达状态值函数
    $V_{\boldsymbol{\pi}}(s)$，
- en: '|  | $V_{\boldsymbol{\pi}}(s)=\mathbb{E}_{a^{1:n}\sim\boldsymbol{\pi}^{1:n}}\left[Q_{\boldsymbol{\pi}}(s,\boldsymbol{a}^{1:n})\right].$
    |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{\boldsymbol{\pi}}(s)=\mathbb{E}_{a^{1:n}\sim\boldsymbol{\pi}^{1:n}}\left[Q_{\boldsymbol{\pi}}(s,\boldsymbol{a}^{1:n})\right].$
    |  |'
- en: Based on Definition [1](#Thmdefinition1 "Definition 1\. ‣ 3.1 Learning of Advantage
    Functions ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration"), we can introduce a pivotal lemma, which reveals that joint advantage
    function can be decomposed into the summation of local advantages of each agent.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于定义 [1](#Thmdefinition1 "Definition 1\. ‣ 3.1 Learning of Advantage Functions
    ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")，我们可以引入一个关键引理，揭示了联合优势函数可以分解为每个智能体的局部优势之和。
- en: Lemma 1.
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1。
- en: (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given
    a joint policy $\boldsymbol{\pi}$ of all agents, we have
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: （多智能体优势分解）。在任何合作的马尔可夫游戏中，给定所有智能体的联合策略 $\boldsymbol{\pi}$，我们有
- en: '|  | $A_{\boldsymbol{\pi}}(s,\boldsymbol{a})=\sum_{k=1}^{n}A_{\boldsymbol{\pi}}^{i_{k}}(s,\boldsymbol{a}^{i_{1:k-1}},a^{i_{k}}),$
    |  | (4) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{\boldsymbol{\pi}}(s,\boldsymbol{a})=\sum_{k=1}^{n}A_{\boldsymbol{\pi}}^{i_{k}}(s,\boldsymbol{a}^{i_{1:k-1}},a^{i_{k}}),$
    |  | (4) |'
- en: where $\boldsymbol{a}=(a^{1},a^{2},...,a^{n})$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{a}=(a^{1},a^{2},...,a^{n})$。
- en: The proof follows [kuba2022happo](#bib.bib29) and is given in §[A.1](#A1.SS1
    "A.1 Proof of Multi-Agent Advantage Decomposition ‣ Appendix A Theoretical Proof
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"). Lemma
    [1](#Thmlemma1 "Lemma 1\. ‣ 3.1 Learning of Advantage Functions ‣ 3 Methodology
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration") will
    be used for derivation in §[3.2](#S3.SS2 "3.2 Theoretical Motivation for Grounding
    LLM ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration").
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 证明参见 [kuba2022happo](#bib.bib29)，并在 §[A.1](#A1.SS1 "A.1 Proof of Multi-Agent
    Advantage Decomposition ‣ Appendix A Theoretical Proof ‣ Towards Efficient LLM
    Grounding for Embodied Multi-Agent Collaboration") 中给出。引理 [1](#Thmlemma1 "Lemma
    1\. ‣ 3.1 Learning of Advantage Functions ‣ 3 Methodology ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration") 将在 §[3.2](#S3.SS2 "3.2
    Theoretical Motivation for Grounding LLM ‣ 3 Methodology ‣ Towards Efficient LLM
    Grounding for Embodied Multi-Agent Collaboration") 中用于推导。
- en: 3.2 Theoretical Motivation for Grounding LLM
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLM 的理论动机
- en: In this section, we give a theoretical motivation that closely resembles advantage-weighted
    regression [peng2019advantageweighted](#bib.bib46) in single-agent RL, while we
    extend it for multi-agents via advantage decomposition in Lemma [1](#Thmlemma1
    "Lemma 1\. ‣ 3.1 Learning of Advantage Functions ‣ 3 Methodology ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration"). To achieve efficient LLM
    grounding, i.e., to obtain a superior policy to the LLM planner, one option is
    adopting LLM as a basic policy and searching for a stronger policy than it. Therefore,
    we derive our objective as an approximate optimization of a constrained policy
    search problem. Specifically, we denote the policy of LLM planners as $\boldsymbol{\mu}=\boldsymbol{\pi}_{\rm
    llm}(\boldsymbol{a}|s)$, as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们提供一个理论动机，它与单智能体 RL 中的优势加权回归 [peng2019advantageweighted](#bib.bib46)
    非常相似，同时我们通过引理 [1](#Thmlemma1 "Lemma 1\. ‣ 3.1 Learning of Advantage Functions
    ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    将其扩展到多智能体。为了实现高效的 LLM 对接，即获得优于 LLM 规划器的策略，一种选择是采用 LLM 作为基础策略，并搜索比它更强的策略。因此，我们将目标推导为约束策略搜索问题的近似优化。具体来说，我们将
    LLM 规划器的策略表示为 $\boldsymbol{\mu}=\boldsymbol{\pi}_{\rm llm}(\boldsymbol{a}|s)$，如
- en: '|  | $\begin{split}\eta(\boldsymbol{\pi})=\mathbb{E}_{s\sim\rho_{\boldsymbol{\pi}}(s),\boldsymbol{a}\sim\boldsymbol{\pi}(\boldsymbol{a}&#124;s)}\left[A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\right],\end{split}$
    |  | (5) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\eta(\boldsymbol{\pi})=\mathbb{E}_{s\sim\rho_{\boldsymbol{\pi}}(s),\boldsymbol{a}\sim\boldsymbol{\pi}(\boldsymbol{a}&#124;s)}\left[A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\right],\end{split}$
    |  | (5) |'
- en: where $\rho_{\boldsymbol{\pi}}(s)=\sum_{i=0}^{\infty}\gamma^{i}P(s_{i}=s)$,
    instructed by [pmlr-v37-schulman15ppo](#bib.bib50) , as
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho_{\boldsymbol{\pi}}(s)=\sum_{i=0}^{\infty}\gamma^{i}P(s_{i}=s)$，由 [pmlr-v37-schulman15ppo](#bib.bib50)
    指导，如
- en: '|  | $\hat{\eta}(\boldsymbol{\pi})=\mathbb{E}_{s\sim\rho_{\boldsymbol{\mu}}(s),\boldsymbol{a}\sim\boldsymbol{\pi}(\boldsymbol{a}&#124;s)}\left[A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\right].$
    |  | (6) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\eta}(\boldsymbol{\pi})=\mathbb{E}_{s\sim\rho_{\boldsymbol{\mu}}(s),\boldsymbol{a}\sim\boldsymbol{\pi}(\boldsymbol{a}&#124;s)}\left[A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\right].$
    |  | (6) |'
- en: By replacing the original objective with the surrogate objective, we can formulate
    the following constrained policy search problem as
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将原始目标替换为替代目标，我们可以将以下约束策略搜索问题表述为
- en: '|  | $\begin{split}\mathop{\arg\max}\limits_{\boldsymbol{\pi}}\int_{s}\rho_{\boldsymbol{\mu}}(s)\int_{\boldsymbol{a}}\boldsymbol{\pi}(\boldsymbol{a}&#124;s)A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\,d\boldsymbol{a}\,ds,\quad{\rm
    s.t.}\int_{s}\rho_{\boldsymbol{\mu}}(s){\rm D}_{KL}\left(\boldsymbol{\pi}(\cdot&#124;s)\&#124;\boldsymbol{\mu}(\cdot&#124;s)\right)\,ds\leq\epsilon.\end{split}$
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathop{\arg\max}\limits_{\boldsymbol{\pi}}\int_{s}\rho_{\boldsymbol{\mu}}(s)\int_{\boldsymbol{a}}\boldsymbol{\pi}(\boldsymbol{a}|s)A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\,d\boldsymbol{a}\,ds,\quad{\rm
    s.t.}\int_{s}\rho_{\boldsymbol{\mu}}(s){\rm D}_{KL}\left(\boldsymbol{\pi}(\cdot|s)\|\boldsymbol{\mu}(\cdot|s)\right)\,ds\leq\epsilon.\end{split}$
    |  |'
- en: The constraint asserts that when the new policy $\boldsymbol{\pi}$¹¹1We refer
    to [pmlr-v37-schulman15ppo](#bib.bib50) for a detailed derivation.. To get the
    solution to this constrained optimization, we form the Lagrangian of the primal
    problem presented above,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 约束条件表明，当新的策略$\boldsymbol{\pi}$时，我们参考[pmlr-v37-schulman15ppo](#bib.bib50)中的详细推导。为了获得这个约束优化问题的解，我们构造了上述原始问题的拉格朗日函数，
- en: '|  | $\begin{split}\mathcal{L}(\boldsymbol{\pi},\beta)=\,\int_{s}\rho_{\boldsymbol{\mu}}(s)\int_{\boldsymbol{a}}\boldsymbol{\pi}(\boldsymbol{a}&#124;s)A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\,d\boldsymbol{a}\,ds+\beta\left(\epsilon-\int_{s}\rho_{\boldsymbol{\mu}}(s){\rm
    D}_{\rm KL}\left(\boldsymbol{\pi}(\cdot&#124;s)\&#124;\boldsymbol{\mu}(\cdot&#124;s)\right)\,ds\right)\end{split}$
    |  | (7) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\mathcal{L}(\boldsymbol{\pi},\beta)=\,\int_{s}\rho_{\boldsymbol{\mu}}(s)\int_{\boldsymbol{a}}\boldsymbol{\pi}(\boldsymbol{a}|s)A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\,d\boldsymbol{a}\,ds+\beta\left(\epsilon-\int_{s}\rho_{\boldsymbol{\mu}}(s){\rm
    D}_{\rm KL}\left(\boldsymbol{\pi}(\cdot|s)\|\boldsymbol{\mu}(\cdot|s)\right)\,ds\right)\end{split}$
    |  | (7) |'
- en: where $$\beta> is a Lagrange multiplier.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$$\beta$$是拉格朗日乘子。
- en: Optimal Joint Policy. According to KKT conditions [kkt](#bib.bib31) , the optimal
    policy $\boldsymbol{\pi}^{*}$ for the constrained optimization problem in Eq. ([7](#S3.E7
    "In 3.2 Theoretical Motivation for Grounding LLM ‣ 3 Methodology ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration")) is expressed by
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最优联合策略。根据KKT条件[kkt](#bib.bib31)，约束优化问题（见Eq. ([7](#S3.E7 "在3.2节中，LLM基础的理论动机 ‣
    3 方法论 ‣ 迈向高效的LLM基础在具身多智能体协作中")）的最优策略$\boldsymbol{\pi}^{*}$由以下公式表示：
- en: '|  | $\boldsymbol{\pi}^{*}(\boldsymbol{a}&#124;s)=\frac{1}{Z(s)}\boldsymbol{\mu}(\boldsymbol{a}&#124;s)\exp\left(\frac{1}{\beta}A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\right),$
    |  | (8) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\pi}^{*}(\boldsymbol{a}|s)=\frac{1}{Z(s)}\boldsymbol{\mu}(\boldsymbol{a}|s)\exp\left(\frac{1}{\beta}A_{\boldsymbol{\mu}}(s,\boldsymbol{a})\right),$
    |  | (8) |'
- en: where $Z(s)$ is the partition function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z(s)$是配分函数。
- en: Optimal Individual Policy. Following advantage decomposition in Lemma [1](#Thmlemma1
    "Lemma 1\. ‣ 3.1 Learning of Advantage Functions ‣ 3 Methodology ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration"), we can decompose optimal
    joint policy $\boldsymbol{\pi}^{*}(\boldsymbol{a}|s)$, as
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最优个体策略。按照引理[1](#Thmlemma1 "引理1. ‣ 3.1 Advantage Functions的学习 ‣ 3 方法论 ‣ 迈向高效的LLM基础在具身多智能体协作中")中的优势分解，我们可以将最优联合策略$\boldsymbol{\pi}^{*}(\boldsymbol{a}|s)$分解为：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: where $Z^{i}(s)$ is the partition function. We refer to §[A.2](#A1.SS2 "A.2
    Derivation of Optimal Joint Policy and Optimal Individual Policy ‣ Appendix A
    Theoretical Proof ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    for a detailed derivation of Eqs. ([8](#S3.E8 "In 3.2 Theoretical Motivation for
    Grounding LLM ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration")) and ([9](#S3.E9 "In 3.2 Theoretical Motivation for Grounding
    LLM ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration")).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Z^{i}(s)$是配分函数。我们参考§[A.2](#A1.SS2 "A.2 最优联合策略和最优个体策略的推导 ‣ 附录A 理论证明 ‣ 迈向高效的LLM基础在具身多智能体协作中")中的详细推导，以获得Eqs. ([8](#S3.E8
    "在3.2节中，LLM基础的理论动机 ‣ 3 方法论 ‣ 迈向高效的LLM基础在具身多智能体协作中"))和([9](#S3.E9 "在3.2节中，LLM基础的理论动机
    ‣ 3 方法论 ‣ 迈向高效的LLM基础在具身多智能体协作中"))。
- en: By maximizing the expected policy improvement $\eta(\boldsymbol{\pi})=J(\boldsymbol{\pi})-J(\boldsymbol{\mu})$,
    as we discussed in §[3.1](#S3.SS1 "3.1 Learning of Advantage Functions ‣ 3 Methodology
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化期望策略改进$\eta(\boldsymbol{\pi})=J(\boldsymbol{\pi})-J(\boldsymbol{\mu})$，正如我们在§[3.1](#S3.SS1
    "3.1 Advantage Functions的学习 ‣ 3 方法论 ‣ 迈向高效的LLM基础在具身多智能体协作中")中讨论的那样。
- en: 3.3 Prompting by Reinforced Advantage Feedback
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 通过强化优势反馈进行提示
- en: Upon the basic policy $\boldsymbol{\mu}=\boldsymbol{\pi}_{\rm llm}$, there being
    convenient for grounding close-sourced LLMs. We provide theoretical proof for
    the monotonic improvement with the *binary filtering* in §[A.3](#A1.SS3 "A.3 Proof
    of Monotonic Improvement with Binary Filtering ‣ Appendix A Theoretical Proof
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本策略 $\boldsymbol{\mu}=\boldsymbol{\pi}_{\rm llm}$ 下，便于地面化闭源 LLM。我提供了在 §[A.3](#A1.SS3
    "A.3 使用二进制过滤的单调改进证明 ‣ 附录 A 理论证明 ‣ 朝着高效的 LLM 地面化实现多智能体协作") 中对 *二进制过滤* 的单调改进的理论证明。
- en: 'Inspired by the *binary filtering*, we develop a novel feedback mechanism,
    wherein the main idea is to convert the filter $\mathds{1}[A_{\boldsymbol{\mu}}^{i}(s,\boldsymbol{a}^{1:i-1},a^{i})>
    for refining the plans. Based on different types of advantages, we design two
    algorithms for plan refinement: *ReAd-S* and *ReAd-J*. The process of prompting
    and refinement is depicted in Figure [2](#S3.F2 $. The (refined) action is used
    to interact with the environment, and the LLM planner is processed in the next
    step.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 *二进制过滤* 的启发，我们开发了一种新颖的反馈机制，其主要思想是将过滤器 $\mathds{1}[A_{\boldsymbol{\mu}}^{i}(s,\boldsymbol{a}^{1:i-1},a^{i})>$
    用于细化计划。基于不同类型的优势，我们设计了两种计划细化算法：*ReAd-S* 和 *ReAd-J*。提示和细化的过程如图 [2](#S3.F2) 所示。所（精细化的）行动用于与环境交互，LLM
    规划器将在下一步骤处理中。
- en: Prompting and Refinement for *ReAd-S*. For each time step, we initialize an
    empty action-set $\boldsymbol{a}_{t}=\{\}$ to estimate the advantage score of
    a generated action, as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *ReAd-S* 的提示和细化。对于每个时间步骤，我们初始化一个空的动作集 $\boldsymbol{a}_{t}=\{\}$ 来估计生成动作的优势得分，如
- en: '|  | $1$2 |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Given a threshold $\epsilon\geq 0$.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个阈值 $\epsilon\geq 0$。
- en: Prompting and Refinement for *ReAd-J*. The planning process of the LLM planner
    for *ReAd-J* is similar to that of *ReAd-S*. The main difference is the LLM planner
    for *ReAd-J* is required to give a joint action $\boldsymbol{a}_{t}$ rather than
    considering the local advantages. The score function is
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对 *ReAd-J* 的提示和细化。*ReAd-J* 的 LLM 规划过程类似于 *ReAd-S*。主要区别在于 *ReAd-J* 的 LLM 规划器需要给出一个联合行动
    $\boldsymbol{a}_{t}$，而不是考虑局部优势。得分函数是
- en: '|  | $1$2 |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: based on Eq. ([8](#S3.E8 "In 3.2 Theoretical Motivation for Grounding LLM ‣
    3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")).
    The joint plan $\boldsymbol{a}_{t}$).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Eq. ([8](#S3.E8 "在 3.2 地面 LLM 的理论动机 ‣ 3 方法 ‣ 朝着高效的 LLM 地面化实现多智能体协作"))。联合计划
    $\boldsymbol{a}_{t}$)。
- en: 4 Related Works
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Task Planning with LLMs. LLMs [chowdhery2023palm](#bib.bib10) ; [openAI2023GPT](#bib.bib45)
    ; [touvron2023llama1](#bib.bib55) ; [touvron2023llama2](#bib.bib56) trained on
    a large-scale corpus exhibits notable reasoning abilities via in-context learning
    [dong2022survey](#bib.bib14) ; [abernethy2023mechanism](#bib.bib1) ; [akyurek2023what](#bib.bib3)
    . However, LLMs can also give infeasible plans for embodied agents due to the
    lack of real-world knowledge. A line of research modifies the open-loop planning
    framework to a closed-loop one via self-evaluation and reflection. For example,
    ReAct [yao2023react](#bib.bib68) , Reflexion [shinn2023reflexion](#bib.bib51)
    , and BeamSearch [xie2023beam](#bib.bib65) incorporate feedback from an LLM evaluator
    into prompts after the previous plan is finished. Other works integrate domain
    knowledge of embodied agents in feedback. For example, RoCo [mandi2023roco](#bib.bib42)
    and Inner Monologue [huang2022Monologue](#bib.bib26) design physical verification
    such as collision checking, object recognition, and scene description for feedback.
    DoReMi [guo2023DoReMi](#bib.bib20) leverages LLM to generate physical constraints,
    and ViLA [hu2023look](#bib.bib23) adopts Vision-Language-Model (VLM) as a constraint
    detector for verification. Another line of research develops advanced reasoning
    frameworks, including chain-of-thought [wei2023chainofthought](#bib.bib61) ; [mu2023embodiedgpt](#bib.bib43)
    and tree-of-thought [yao2023tree](#bib.bib67) . Works like [zhao2023large](#bib.bib75)
    ; [hao2023reasoning](#bib.bib21) consider LLMs as a world model [ano2023learning](#bib.bib37)
    and adopt tree search in planning [hu2023tree](#bib.bib22) . Other works adopt
    planning domain definition language (PDDL) for searching in long-horizon problems
    [silver2023PDDL](#bib.bib52) ; [liu2023llm+p](#bib.bib39) ; [zhou2023isr](#bib.bib76)
    . Our work lies in closed-loop frameworks but has a novel advantage function in
    feedback, which is different from self-reflection or physical feedback and does
    not rely on advanced searching algorithms.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 任务规划与LLMs。LLMs [chowdhery2023palm](#bib.bib10) ; [openAI2023GPT](#bib.bib45)
    ; [touvron2023llama1](#bib.bib55) ; [touvron2023llama2](#bib.bib56) 在大规模语料库上训练，表现出通过上下文学习的显著推理能力
    [dong2022survey](#bib.bib14) ; [abernethy2023mechanism](#bib.bib1) ; [akyurek2023what](#bib.bib3)
    。然而，由于缺乏现实世界知识，LLMs 也可能给出不切实际的计划。研究的一种方向是通过自我评估和反思将开放环路规划框架修改为闭环框架。例如，ReAct [yao2023react](#bib.bib68)
    、Reflexion [shinn2023reflexion](#bib.bib51) 和BeamSearch [xie2023beam](#bib.bib65)
    在之前的计划完成后，将LLM评估者的反馈纳入提示。其他工作则在反馈中集成了体现代理的领域知识。例如，RoCo [mandi2023roco](#bib.bib42)
    和Inner Monologue [huang2022Monologue](#bib.bib26) 为反馈设计了物理验证，如碰撞检查、物体识别和场景描述。DoReMi
    [guo2023DoReMi](#bib.bib20) 利用LLM生成物理约束，而ViLA [hu2023look](#bib.bib23) 采用视觉语言模型（VLM）作为验证约束检测器。另一条研究方向则发展了先进的推理框架，包括chain-of-thought
    [wei2023chainofthought](#bib.bib61) ; [mu2023embodiedgpt](#bib.bib43) 和tree-of-thought
    [yao2023tree](#bib.bib67) 。如 [zhao2023large](#bib.bib75) ; [hao2023reasoning](#bib.bib21)
    的工作将LLMs视为世界模型 [ano2023learning](#bib.bib37) 并在规划中采用树搜索 [hu2023tree](#bib.bib22)
    。其他工作则采用规划领域定义语言（PDDL）来搜索长期问题 [silver2023PDDL](#bib.bib52) ; [liu2023llm+p](#bib.bib39)
    ; [zhou2023isr](#bib.bib76) 。我们的工作属于闭环框架，但在反馈中具有一种新颖的优势函数，这与自我反思或物理反馈不同，并且不依赖于先进的搜索算法。
- en: Grounding LLM with RL. RL with Human Feedback (RLHF) has been used for aligning
    LLMs with human preference via parameter tuning [dai2023safe](#bib.bib12) ; [fernandes2023bridging](#bib.bib17)
    ; [song2023preference](#bib.bib54) . In contrast, our work focuses on grounding
    closed-source LLM with RL via few-shot prompting and closed-loop feedback [zeng2023socratic](#bib.bib70)
    ; [wu2023embodied](#bib.bib62) ; [huang2022zero](#bib.bib24) ; [lin2023grounded](#bib.bib36)
    . Previous works try to integrate RL into LLM planning under the framework tree
    search [browne2012survey](#bib.bib7) . For example, FAFA [liu2023RAFA](#bib.bib40)
    and TS-LLM [feng2023alphazero](#bib.bib16) learn an environment model and value
    function to plan subroutine in MCTS. REX [murthy2023rex](#bib.bib44) proposes
    to balance exploration and exploitation in LLM-based MCTS. Other works like SayCan
    [ahn2022i](#bib.bib2) and Text2Motion [Kevin2023Motion](#bib.bib38) adopt a model-free
    manner by learning value functions to connect LLM knowledge to physical environments.
    SwiftSage [lin2023swiftsage](#bib.bib35) performs imitation learning for rapid
    thinking and LLM for methodical training. Remember [zhang2023large](#bib.bib72)
    learns value functions for LLM to predict $Q$-values. Different from the Remember
    framework that retrieves similar states from a buffer, we evaluate the advantage
    function of planned actions via a neural network and follow advantage-weighted
    regression in prompting. We employ the advantage function in a multi-agent setting,
    while previous methods focus on single-agent planning. Compared to previous LLM-based
    multi-agent works that manually design communication, reflection, and reasoning
    modules [zhang2023proagent](#bib.bib71) ; [zhang2023building](#bib.bib73) ; [Shyam2023MA](#bib.bib28)
    ; [chen2023MA](#bib.bib9) , we propose a more principled way by using the sequential
    advantage function from multi-agent RL for cooperation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM与RL对齐。通过参数调整，RL与人类反馈（RLHF）已被用于将LLM与人类偏好对齐 [dai2023safe](#bib.bib12) ; [fernandes2023bridging](#bib.bib17)
    ; [song2023preference](#bib.bib54) 。相比之下，我们的工作重点是通过少量样本提示和闭环反馈来将封闭源LLM与RL对齐 [zeng2023socratic](#bib.bib70)
    ; [wu2023embodied](#bib.bib62) ; [huang2022zero](#bib.bib24) ; [lin2023grounded](#bib.bib36)
    。以往的工作尝试将RL融入LLM规划框架下的树搜索 [browne2012survey](#bib.bib7) 。例如，FAFA [liu2023RAFA](#bib.bib40)
    和 TS-LLM [feng2023alphazero](#bib.bib16) 学习环境模型和值函数以在MCTS中规划子例程。REX [murthy2023rex](#bib.bib44)
    提出了在基于LLM的MCTS中平衡探索与利用。其他工作如SayCan [ahn2022i](#bib.bib2) 和 Text2Motion [Kevin2023Motion](#bib.bib38)
    采用无模型的方法，通过学习值函数将LLM知识与物理环境连接起来。SwiftSage [lin2023swiftsage](#bib.bib35) 进行模仿学习以实现快速思考，而LLM则用于系统性训练。Remember
    [zhang2023large](#bib.bib72) 学习值函数以使LLM预测 $Q$-值。不同于从缓冲区检索相似状态的Remember框架，我们通过神经网络评估计划行动的优势函数，并在提示中采用优势加权回归。我们在多智能体环境中使用优势函数，而以往的方法则专注于单智能体规划。与以往手动设计通信、反思和推理模块的LLM多智能体工作
    [zhang2023proagent](#bib.bib71) ; [zhang2023building](#bib.bib73) ; [Shyam2023MA](#bib.bib28)
    ; [chen2023MA](#bib.bib9) 相比，我们提出了一种更为原理化的方法，即使用来自多智能体RL的顺序优势函数进行合作。
- en: 5 Experiments
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: We first introduce two multi-agent collaboration environment in §[5.1](#S5.SS1
    "5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Efficient LLM Grounding for
    Embodied Multi-Agent Collaboration"). Then we design a series of experiments to
    compare our approach with baselines in §[5.2](#S5.SS2 "5.2 Results ‣ 5 Experiments
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"). Finally,
    we conduct ablation studies and analyze the impact of modules in §[5.3](#S5.SS3
    "5.3 Ablation Studies ‣ 5 Experiments ‣ Towards Efficient LLM Grounding for Embodied
    Multi-Agent Collaboration").
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在§[5.1](#S5.SS1 "5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration")介绍两个多智能体协作环境。然后，我们在§[5.2](#S5.SS2
    "5.2 Results ‣ 5 Experiments ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration")设计了一系列实验，将我们的方法与基线进行比较。最后，我们在§[5.3](#S5.SS3 "5.3 Ablation Studies
    ‣ 5 Experiments ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")进行消融研究，并分析模块的影响。
- en: 5.1 Experimental Setup
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: DV-RoCoBench. We present *Difficult Variants of RoCoBench (DV-RoCoBench)* for
    embodied multi-robot collaboration, which is derived from RoCoBench [mandi2023roco](#bib.bib42)
    . RoCoBench consists of 6 multi-robot collaboration tasks in a tabletop manipulation
    environment, typically involving interactive objects that are semantically straightforward
    to comprehend and reason about for LLMs. The tasks encompass a range of collaboration
    scenarios that necessitate robots’ communication and coordination behaviors. Robots
    receive their observation and select one action from the high-level actions set,
    which includes diverse functionalities such as WAIT, moving, sweeping, grasping,
    and dropping, across multiple tasks. The execution of high-level actions is subsequently
    translated into low-level actions for manipulation. In contrast to RoCoBench,
    which primarily focuses on tasks with a fixed difficulty level, we select three
    tasks to enrich the complexity of the benchmark and create the new *DV-RoCoBench*,
    where each task is tailored to have 4-5 difficulty levels for experiments.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: DV-RoCoBench。我们提出了*RoCoBench的困难变体（DV-RoCoBench）*，用于体现多机器人协作，这一变体源自于RoCoBench
    [mandi2023roco](#bib.bib42)。RoCoBench 包含6个多机器人协作任务，设定在桌面操作环境中，通常涉及与LLMs语义上容易理解和推理的互动对象。这些任务涵盖了一系列需要机器人进行沟通和协调行为的协作场景。机器人接收观察信息，并从高层次动作集中选择一个动作，这些动作包括
    WAIT、移动、扫拭、抓取和放置等多种功能，适用于多个任务。随后，这些高层次动作被转换为操作的低层次动作。与主要集中在固定难度级别任务的RoCoBench不同，我们选择了三个任务来丰富基准的复杂性，并创建了新的*DV-RoCoBench*，每个任务都根据实验需要设定了4到5个难度级别。
- en: In the following, we give a brief description of tasks and settings. See §[D](#A4
    "Appendix D Environment Details ‣ Towards Efficient LLM Grounding for Embodied
    Multi-Agent Collaboration") for details.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下内容中，我们简要描述了任务和设置。详细信息请参见§[D](#A4 "附录 D 环境细节 ‣ 迈向高效的LLM基础构建用于体现多代理协作")。
- en: '-'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-'
- en: Sweep Floor. Two robot arms need to work together to sweep all the cubes on
    the table into the bin. The aim is to sweep away the cubes with given colors.
    We establish 5 difficulty levels based on the number of overall cubes and the
    target cubes. An LLM planner is more likely to produce fact hallucinations in
    more difficult settings.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扫地。两个机器人手臂需要协作，将桌上的所有立方体扫进垃圾桶。目标是扫除具有指定颜色的立方体。我们根据总体立方体数量和目标立方体数量设定了5个难度级别。在更困难的设置中，LLM规划者更容易产生事实幻觉。
- en: '-'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-'
- en: Make Sandwich. Two robot arms need to stack the ingredients to make a sandwich
    according to the recipe. Each arm is limited in operating range and cooperation
    between agents is required. We establish 4 difficulty levels depending on the
    length of the recipe.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 制作三明治。两个机器人手臂需要按照食谱叠放配料以制作三明治。每个手臂的操作范围有限，且需要代理之间的合作。我们根据食谱的长度设定了4个难度级别。
- en: '-'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-'
- en: Sort Cubes. Three robot arms within their operating ranges are required to coordinate
    and place cubes on the table to their target positions. We establish 5 different
    difficulty levels based on the distance between the cubes and their target locations.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 排序立方体。三个机器人手臂在其操作范围内需要协调，将立方体放置到桌上的目标位置。我们根据立方体与目标位置之间的距离，设定了5个不同的难度级别。
- en: 'Overcooked-AI. *Overcooked-AI* [micah2019overcooked_ai](#bib.bib8) is a fully
    cooperative multi-agent benchmark environment based on the wildly popular video
    game Overcooked. In this environment, agents need to deliver soups as fast as
    possible. Each soup requires placing up to 3 ingredients in a pot, waiting for
    the soup to cook, and having an agent pick up the soup and deliver it. The environment
    consists of 5 different kitchen scenarios, covering from low-level motion coordination
    challenges to high-level strategy coordination challenges. In our experiment,
    we chose two representative scenarios: Cramped Room and Forced Coordination,and
    set the number of ingredients for making soups as 2 and the timesteps for cooking
    as 2. To enable the computation of the success rate, we modify the task to cook
    and deliver a soup within a specified number of timesteps. Details of the environment
    are given in §[D.4](#A4.SS4 "D.4 Overcooked-AI ‣ Appendix D Environment Details
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"). For
    quantitative comparisons, we impose the maximum number of environment steps per
    episode to 15 in *DV-RoCoBench*, 20 in Cramped Room, and 25 in Forced Coordination.
    And the maximum rounds of re-planning per step is set to 15 for all tasks except
    for Sort Cubes where it is set to 10.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Overcooked-AI。*Overcooked-AI* [micah2019overcooked_ai](#bib.bib8) 是一个完全合作的多智能体基准环境，基于广受欢迎的视频游戏
    Overcooked。在这个环境中，智能体需要尽可能快地送达汤品。每种汤需要将最多 3 种配料放入锅中，等待汤煮熟，然后由智能体取走汤并送达。环境包括 5
    种不同的厨房场景，从低级的动作协调挑战到高级的策略协调挑战。在我们的实验中，我们选择了两个代表性场景：狭小房间和强制协调，并将制作汤的配料数量设置为 2，将烹饪的时间步数设置为
    2。为了计算成功率，我们将任务修改为在指定的时间步数内烹饪和送达一份汤。环境的详细信息见 §[D.4](#A4.SS4 "D.4 Overcooked-AI
    ‣ Appendix D Environment Details ‣ Towards Efficient LLM Grounding for Embodied
    Multi-Agent Collaboration")。对于定量比较，我们将*DV-RoCoBench*中每集的最大环境步骤数限制为 15，在狭小房间中为
    20，在强制协调中为 25。每步的最大重新规划回合数设置为 15，除了排序立方体任务，其回合数设置为 10。
- en: Baseline Methods. We use GPT-4-Turbo [openAI2023GPT](#bib.bib45) as the basic
    LLM policy for all experiments. On both benchmarks, we compare *ReAd-J* with three
    strong close-loop baselines – ReAct [yao2023react](#bib.bib68) , Reflexion [shinn2023reflexion](#bib.bib51)
    and MindAgent [gong2023mindagent](#bib.bib19) , and a planner named Central Plan
    which instructs the LLM to generate actions for all robots based on the history
    of all agents. These five methods output agents’ plans in a parallel manner. In
    *DV-RoCoBench*, we particularly add one more baseline RoCo [mandi2023roco](#bib.bib42)
    which achieves the state-of-the-art performance in RoCoBench [mandi2023roco](#bib.bib42)
    , for comparisons with *ReAd-S*. Both of them generate joint plans in a sequential
    manner. Due to the expensive cost of sequential planning with more environment
    steps in *Overcooked-AI*, we only evaluate the performance of methods that generate
    joint plans in a parallel manner. We provide a detailed comparison with baselines
    in Table [3](#A5.T3 "Table 3 ‣ E.1 Comparison of Baselines ‣ Appendix E Additional
    Experimental Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") of §[E.1](#A5.SS1 "E.1 Comparison of Baselines ‣ Appendix E Additional
    Experimental Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration").
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基线方法。我们使用 GPT-4-Turbo [openAI2023GPT](#bib.bib45) 作为所有实验的基本 LLM 策略。在两个基准测试中，我们将*ReAd-J*与三个强大的闭环基线方法进行比较——ReAct
    [yao2023react](#bib.bib68)，Reflexion [shinn2023reflexion](#bib.bib51) 和 MindAgent
    [gong2023mindagent](#bib.bib19)，以及一个名为 Central Plan 的规划器，该规划器指示 LLM 基于所有智能体的历史生成所有机器人的动作。这五种方法以并行方式输出智能体的计划。在*DV-RoCoBench*中，我们特别添加了一个额外的基线
    RoCo [mandi2023roco](#bib.bib42)，它在 RoCoBench [mandi2023roco](#bib.bib42) 中实现了最先进的性能，以与*ReAd-S*进行比较。它们都以顺序方式生成联合计划。由于在*Overcooked-AI*中顺序规划的环境步骤成本较高，我们仅评估以并行方式生成联合计划的方法的性能。我们在
    §[E.1](#A5.SS1 "E.1 Comparison of Baselines ‣ Appendix E Additional Experimental
    Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    的表格 [3](#A5.T3 "Table 3 ‣ E.1 Comparison of Baselines ‣ Appendix E Additional
    Experimental Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") 中提供了与基线方法的详细比较。
- en: 'Evaluation Metrics. We evaluate the performance of algorithms on three metrics
    that closely resemble that in RoCoBench: (i) SR: the success rate of completing
    tasks within the limited interaction rounds; (ii) ES: the number of interaction
    steps to the environment taken by the robots to complete the task; (iii) NQ: the
    number of queries to LLMs in completing the task, which measures the efficiency
    in enquiring LLMs to obtain a feasible plan. An algorithm is better if it has
    *higher SR, fewer ES, and fewer NQ*. Among these metrics, SR and ES directly reflect
    the effectiveness of a planner in completing tasks, while NQ can be somewhat trivial
    since a planner can have much fewer queries to LLM but has a low SR. In contrast,
    methods that require policy refinement often require more queries to lead to a
    high SR.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标。我们在三个指标上评估算法性能，这些指标与 RoCoBench 中的指标非常相似：(i) SR：在有限交互轮次内完成任务的成功率；(ii) ES：机器人完成任务所需的与环境交互的步骤数；(iii)
    NQ：完成任务时向 LLM 提出的查询次数，这衡量了询问 LLM 以获取可行计划的效率。如果一个算法具有*更高的 SR、更少的 ES 和更少的 NQ*，则其性能更好。在这些指标中，SR
    和 ES 直接反映了规划者完成任务的有效性，而 NQ 可能显得有些琐碎，因为一个规划者可能有更少的对 LLM 的查询但 SR 低。相反，需要政策调整的方法通常需要更多的查询才能获得高
    SR。
- en: '![Refer to caption](img/d30081f6855a360f93b93b0b40124ebd.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d30081f6855a360f93b93b0b40124ebd.png)'
- en: 'Figure 3: We report mean SR ($\boldsymbol{\uparrow}$) in 3 tasks with various
    difficulty levels averaged over 10 random seeds. The detailed score is given in
    Table [4](#A5.T4 "Table 4 ‣ E.2 Main Experiments ‣ Appendix E Additional Experimental
    Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    of §[E.2](#A5.SS2 "E.2 Main Experiments ‣ Appendix E Additional Experimental Results
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration").'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们报告了在 3 个不同难度级别的任务中平均 SR（$\boldsymbol{\uparrow}$），数据是基于 10 次随机种子的平均值。详细得分见
    §[E.2](#A5.SS2 "E.2 主要实验 ‣ 附录 E 额外实验结果 ‣ 迈向高效的 LLM 基础知识应用于多体协作") 的表格 [4](#A5.T4
    "表格 4 ‣ E.2 主要实验 ‣ 附录 E 额外实验结果 ‣ 迈向高效的 LLM 基础知识应用于多体协作")。
- en: '![Refer to caption](img/2c0dd88e1ebfea2c5231dcab8422fab2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2c0dd88e1ebfea2c5231dcab8422fab2.png)'
- en: 'Figure 4: We report mean SR ($\boldsymbol{\uparrow}$) in two scenarios of *Overcooked-AI*
    averaged over 10 random seeds. The detailed score is given in Table [5](#A5.T5
    "Table 5 ‣ E.2 Main Experiments ‣ Appendix E Additional Experimental Results ‣
    Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration") of §[E.2](#A5.SS2
    "E.2 Main Experiments ‣ Appendix E Additional Experimental Results ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：我们报告了在*Overcooked-AI*的两种场景中的平均 SR（$\boldsymbol{\uparrow}$），数据是基于 10 次随机种子的平均值。详细得分见
    §[E.2](#A5.SS2 "E.2 主要实验 ‣ 附录 E 额外实验结果 ‣ 迈向高效的 LLM 基础知识应用于多体协作") 的表格 [5](#A5.T5
    "表格 5 ‣ E.2 主要实验 ‣ 附录 E 额外实验结果 ‣ 迈向高效的 LLM 基础知识应用于多体协作")。
- en: 5.2 Results
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: '*ReAd-S* and *ReAd-J* outperform their corresponding strong baselines on all
    metrics and achieve more efficient LLM grounding. As shown in Figure [3](#S5.F3
    "Figure 3 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration"), with the increase of difficulty levels
    in *DV-RoCoBench*, the performance contrast in SR becomes pronounced gradually.
    In more difficult settings (e.g., level 4 or 5 in tasks), our approach obtains
    higher success rates while baseline methods fail to make progress. Meanwhile,
    *ReAd-S* and *ReAd-J* present lower ES and comparable or even lower NQ on most
    tasks in *DV-RoCoBench* when compared to their corresponding baselines. A lower
    ES suggests that prompting LLMs to generate actions maximizing the advantages
    can improve the optimality of the proposed plans because a higher advantage implies
    the generated action contributes more to accomplishing the task. Furthermore,
    as shown in Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Experimental Setup ‣ 5 Experiments
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration"), our
    methods achieve a significantly higher SR compared with the methods relying on
    *physical verification* as feedback in *Overcooked-AI*. Due to the heavy coordination
    challenges inherent to *Overcooked-AI*, LLM-based agents cannot advance toward
    task completion unless the LLM planner generates highly collaborative plans. By
    replacing the *physical verification* feedback with *advantage function*, we implicitly
    transfer the understanding and reasoning of the LLMs from semantic comprehension
    towards the current state of the environment to digesting the numerical relationship.
    As the scenario becomes more challenging for multi-agent collaboration, it is
    inevitable to involve more redundant information and disturbing components in
    the environment, which poses a challenge for the LLM planner to capture and reason
    about the essential part inside the state and physical feedback. In contrast,
    benefiting from *ReAd* feedback, the LLM planner only needs to concentrate on
    how to maximize the advantage score no matter how challenging the scenario is.
    Hence, our approach exhibits superior planning capabilities and better LLM grounding
    results for embodied tasks.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReAd-S* 和 *ReAd-J* 在所有指标上均优于其对应的强基准，并实现了更高效的 LLM 基础。正如图 [3](#S5.F3 "图 3 ‣
    5.1 实验设置 ‣ 5 实验 ‣ 实现高效的 LLM 基础") 所示，随着 *DV-RoCoBench* 难度等级的增加，SR 的性能差异逐渐显著。在更困难的设置（例如，任务中的第
    4 或第 5 级）中，我们的方法获得了更高的成功率，而基准方法未能取得进展。同时，与其对应的基准方法相比，*ReAd-S* 和 *ReAd-J* 在 *DV-RoCoBench*
    的大多数任务上表现出更低的 ES 和相当或更低的 NQ。较低的 ES 表明，提示 LLM 生成最大化优势的动作可以提高提出计划的最优性，因为更高的优势意味着生成的动作对完成任务的贡献更大。此外，如图
    [4](#S5.F4 "图 4 ‣ 5.1 实验设置 ‣ 5 实验 ‣ 实现高效的 LLM 基础") 所示，我们的方法相比于依赖 *物理验证* 作为反馈的
    *Overcooked-AI* 方法实现了显著更高的 SR。由于 *Overcooked-AI* 内在的高度协调挑战，LLM 基于的代理在 LLM 规划器生成高度协作的计划之前无法向任务完成迈进。通过将
    *物理验证* 反馈替换为 *优势函数*，我们隐式地将 LLM 的理解和推理从语义理解转移到当前环境状态的数字关系分析上。随着场景对于多代理协作的挑战加剧，环境中不可避免地会涉及更多的冗余信息和干扰成分，这对
    LLM 规划器捕捉和推理状态及物理反馈中的关键部分提出了挑战。相比之下，借助 *ReAd* 反馈，LLM 规划器只需专注于如何最大化优势评分，无论场景有多么具有挑战性。因此，我们的方法在规划能力和
    LLM 基础结果方面优于体现任务的结果。'
- en: With sudden disturbances towards the environments, the LLM-planner can re-adjust
    plans rapidly to accomplish the task via *ReAd* feedback. Since the critic takes
    both the current state and the proposed actions as input, it endows the LLM planner
    with not only the foresight to discern whether the action contributes to realizing
    the goal but also the ability to reschedule the planning quickly when encountering
    sudden disturbances to the advancement of the task. To evaluate
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 面对环境中的突然干扰，LLM 规划器可以通过 *ReAd* 反馈迅速调整计划以完成任务。由于评论员将当前状态和建议的动作作为输入，它赋予 LLM 规划器不仅能够预测动作是否有助于实现目标的前瞻性，还能在遇到突然干扰时迅速重新安排计划。为了评估
- en: 'Table 1: Evaluation results over 10 runs of *ReAd-S* and RoCo and its modified
    versions on disturbances at timestep $n$: no resetting.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：*ReAd-S* 和 RoCo 及其修改版本在时间步 $n$ 上的干扰的评估结果：无重置。
- en: '|  | Method | NQ | ES | SR |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | NQ | ES | SR |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | ReAd-S | 22.1±1.65 | 8.9±0.28 | 1.0±0.00 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | ReAd-S | 22.1±1.65 | 8.9±0.28 | 1.0±0.00 |'
- en: '| recipe3 | RoCo-L | 44.7±4.90 | 12.0±0.54 | 0.9±0.10 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| recipe3 | RoCo-L | 44.7±4.90 | 12.0±0.54 | 0.9±0.10 |'
- en: '| $(n=0)$ | RoCo-P | 33.7±3.16 | 11.5±0.95 | 0.8±0.13 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| $(n=0)$ | RoCo-P | 33.7±3.16 | 11.5±0.95 | 0.8±0.13 |'
- en: '|  | RoCo | 33.7±3.16 | 11.5±0.95 | 0.8±0.13 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | RoCo | 33.7±3.16 | 11.5±0.95 | 0.8±0.13 |'
- en: '|  | ReAd-S | 39.7±5.30 | 10.4±0.34 | 1.0±0.00 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | ReAd-S | 39.7±5.30 | 10.4±0.34 | 1.0±0.00 |'
- en: '| recipe3 | RoCo-L | 55.3±2.63 | 14.1±0.28 | 0.8±0.13 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| recipe3 | RoCo-L | 55.3±2.63 | 14.1±0.28 | 0.8±0.13 |'
- en: '| $(n=1)$ | RoCo-P | 33.6±2.03 | 12.5±0.73 | 0.9±0.10 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| $(n=1)$ | RoCo-P | 33.6±2.03 | 12.5±0.73 | 0.9±0.10 |'
- en: '|  | RoCo | 46.3±3.60 | 13.9±0.43 | 0.7±0.15 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | RoCo | 46.3±3.60 | 13.9±0.43 | 0.7±0.15 |'
- en: '|  | ReAd-S | 44.9±4.34 | 12.5±0.34 | 1.0±0.00 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | ReAd-S | 44.9±4.34 | 12.5±0.34 | 1.0±0.00 |'
- en: '| recipe3 | RoCo-L | 53.4±2.28 | 14.8±0.20 | 0.3±0.15 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| recipe3 | RoCo-L | 53.4±2.28 | 14.8±0.20 | 0.3±0.15 |'
- en: '| $(n=2)$ | RoCo-P | 35.2±0.98 | 14.3±0.26 | 0.8±0.13 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| $(n=2)$ | RoCo-P | 35.2±0.98 | 14.3±0.26 | 0.8±0.13 |'
- en: '|  | RoCo | 61.2±11.95 | 14.2±0.44 | 0.5±0.16 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | RoCo | 61.2±11.95 | 14.2±0.44 | 0.5±0.16 |'
- en: '|  | ReAd-S | 49.1±4.53 | 13.4±0.54 | 1.0±0.0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | ReAd-S | 49.1±4.53 | 13.4±0.54 | 1.0±0.0 |'
- en: '| recipe3 | RoCo-L | 75.9±6.91 | 15.0±0.00 | 0.0±0.00 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| recipe3 | RoCo-L | 75.9±6.91 | 15.0±0.00 | 0.0±0.00 |'
- en: '| $(n=3)$ | RoCo-P | 40.0±2.94 | 14.3±0.26 | 0.5±0.17 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| $(n=3)$ | RoCo-P | 40.0±2.94 | 14.3±0.26 | 0.5±0.17 |'
- en: '|  | RoCo | 74.8±10.79 | 15.0±0.00 | 0.0±0.00 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | RoCo | 74.8±10.79 | 15.0±0.00 | 0.0±0.00 |'
- en: the robustness of the LLM planner, we compare *ReAd-S* and RoCo in extra extended
    scenarios with unexpected disruptions. We select *‘recipe3’* (3rd difficulty level
    in Make Sandwich) that takes a minimum environment step of 8 to accomplish the
    task. When a disruption occurs at timestep $n\ (0\leq n |  | (29) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|  | $$k\ln(\epsilon)+1> |  | (29) |'
- en: With an arbitrary $\epsilon> that satisfies Eq. ([29](#A1.E29 $.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对于满足 Eq. ([29](#A1.E29 $ 的任意 $\epsilon>，
- en: Proof.
  id: totrans-287
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'According to BC on a filtered dataset with *Binary Filtering* <math id=$$,
    we have:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 根据在 *二元过滤* 上的 BC <math id=$$，我们有：
- en: '|  | $1$2 |  | (30) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (30) |'
- en: where $Z^{i}(s)$, the expected improvement from Eq. ([6](#S3.E6 "In 3.2 Theoretical
    Motivation for Grounding LLM ‣ 3 Methodology ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration")) can be rewritten as,
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Z^{i}(s)$，来自 Eq. ([6](#S3.E6 "在 3.2 理论动机 ‣ 3 方法论 ‣ 迈向高效的 LLM 嵌入式多智能体协作"))
    的预期改进可以被重写为，
- en: '|  | $\displaystyle\hat{\eta}(\boldsymbol{\pi})$ |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\eta}(\boldsymbol{\pi})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'Substituting Lemma [1](#Thmlemma1 "Lemma 1\. ‣ 3.1 Learning of Advantage Functions
    ‣ 3 Methodology ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    and Eq. ([30](#A1.E30 "In Proof. ‣ A.3 Proof of Monotonic Improvement with Binary
    Filtering ‣ Appendix A Theoretical Proof ‣ Towards Efficient LLM Grounding for
    Embodied Multi-Agent Collaboration")) into the above equation, we get:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 将引理 [1](#Thmlemma1 "引理 1. ‣ 3.1 优势函数学习 ‣ 3 方法论 ‣ 迈向高效的 LLM 嵌入式多智能体协作") 和 Eq.
    ([30](#A1.E30 "在证明. ‣ A.3 二元过滤的单调改进证明 ‣ 附录 A 理论证明 ‣ 迈向高效的 LLM 嵌入式多智能体协作")) 代入上述方程中，我们得到：
- en: '|  | $\displaystyle\hat{\eta}(\boldsymbol{\pi})$ |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\eta}(\boldsymbol{\pi})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  | (31) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (31) |'
- en: And we note that the expected improvement from Eq. ([6](#S3.E6 "In 3.2 Theoretical
    Motivation for Grounding LLM ‣ 3 Methodology ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration")) entails the following relationship,
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到来自 Eq. ([6](#S3.E6 "在 3.2 理论动机 ‣ 3 方法论 ‣ 迈向高效的 LLM 嵌入式多智能体协作")) 的预期改进涉及以下关系，
- en: '|  | $\displaystyle\hat{\eta}(\boldsymbol{\mu})=J(\boldsymbol{\mu})-J(\boldsymbol{\mu})$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\eta}(\boldsymbol{\mu})=J(\boldsymbol{\mu})-J(\boldsymbol{\mu})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=0$ |  | (32) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=0$ |  | (32) |'
- en: Comparing Eq. ([31](#A1.E31 "In Proof. ‣ A.3 Proof of Monotonic Improvement
    with Binary Filtering ‣ Appendix A Theoretical Proof ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration")) with Eq. ([32](#A1.E32 "In Proof. ‣
    A.3 Proof of Monotonic Improvement with Binary Filtering ‣ Appendix A Theoretical
    Proof ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")),
    it is obvious that those local advantages $A_{\boldsymbol{\mu}}^{i}(s,\boldsymbol{a}^{1:i-1},a^{i})$.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 比较 Eq. ([31](#A1.E31 "在证明. ‣ A.3 二元过滤的单调改进证明 ‣ 附录 A 理论证明 ‣ 迈向高效的 LLM 嵌入式多智能体协作"))
    与 Eq. ([32](#A1.E32 "在证明. ‣ A.3 二元过滤的单调改进证明 ‣ 附录 A 理论证明 ‣ 迈向高效的 LLM 嵌入式多智能体协作"))，可以明显看出那些局部优势
    $A_{\boldsymbol{\mu}}^{i}(s,\boldsymbol{a}^{1:i-1},a^{i})$。
- en: ∎
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Appendix B Additional Related Works
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外相关工作
- en: Other LLM-based Embodied Agent.
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他基于 LLM 的嵌入式智能体。
- en: Beyond task planning, LLMs also shoulder other roles for embodied agents. (i)
    Foundation Policy. Robot Transformer [[5](#bib.bib5), [4](#bib.bib4)], PaLM-E
    [[15](#bib.bib15)], Open-X [[11](#bib.bib11)], and RoboFlamingo [[33](#bib.bib33)]
    use pre-trained LLM or VLM as the foundation policies and fine-tune the parameters
    with embodied data from real-world tasks. The LLM tokens and action tokens of
    agents are unified in fine-tuning. (ii) Code Generator. Given high-level task
    descriptions, LLMs can generate executable code by calling the basic control primitives
    [[34](#bib.bib34), [57](#bib.bib57)] or low-level actions [[59](#bib.bib59)] of
    embodied agents. VoxPoser [[25](#bib.bib25)] leverages the code-writing capabilities
    of LLMs to compose 3D value maps via VLM and adopt model-predictive control (MPC)
    for planning. (iii) Reward Designer. Text2Reward [[64](#bib.bib64)], Language2Reward
    [[69](#bib.bib69)], and Eureka [[41](#bib.bib41)] leverage GPT-4 to produce interpretable
    reward codes, and allow iterative refinement with feedback. (iv) Data Generator.
    To enhance task-level generalization, GenSim [[58](#bib.bib58)] adopts LLMs to
    propose task curriculum and novel sub-tasks to solve complex tasks. RoboGen [[60](#bib.bib60)]
    proposes a closed-loop process to generate robot data, including proposing tasks,
    generating simulation environments, decomposing sub-tasks, and solving sub-tasks
    via RL or MPC.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 除了任务规划，LLMs 还肩负着其他角色。 (i) 基础策略。机器人变压器 [[5](#bib.bib5), [4](#bib.bib4)]、PaLM-E
    [[15](#bib.bib15)]、Open-X [[11](#bib.bib11)] 和 RoboFlamingo [[33](#bib.bib33)]
    使用预训练的LLM或VLM作为基础策略，并用来自真实世界任务的具身数据进行微调。LLM的token和代理的动作token在微调中被统一。(ii) 代码生成器。给定高级任务描述，LLMs可以通过调用基本控制原语
    [[34](#bib.bib34), [57](#bib.bib57)] 或具身代理的低级动作 [[59](#bib.bib59)] 生成可执行代码。VoxPoser
    [[25](#bib.bib25)] 利用LLMs的代码编写能力，通过VLM合成3D值图，并采用模型预测控制（MPC）进行规划。(iii) 奖励设计师。Text2Reward
    [[64](#bib.bib64)]、Language2Reward [[69](#bib.bib69)] 和 Eureka [[41](#bib.bib41)]
    利用 GPT-4 生成可解释的奖励代码，并允许通过反馈进行迭代优化。(iv) 数据生成器。为了增强任务级别的泛化能力，GenSim [[58](#bib.bib58)]
    采用LLMs提出任务课程和新颖子任务来解决复杂任务。RoboGen [[60](#bib.bib60)] 提出了一种闭环过程来生成机器人数据，包括提出任务、生成仿真环境、分解子任务以及通过RL或MPC解决子任务。
- en: Appendix C Algorithmic Description
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 算法描述
- en: In this section, we give the algorithm descriptions of critic regression via
    Monte Carlo estimation, as well as the process of *ReAd-S* and *ReAd-J* algorithms.
    We highlight the difference between *ReAd-S* and *ReAd-J* by different colors.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们给出了通过蒙特卡洛估计进行的评论回归算法描述，以及*ReAd-S*和*ReAd-J*算法的过程。我们通过不同颜色突出*ReAd-S*和*ReAd-J*之间的差异。
- en: Algorithm 1 Critic regression on $\mathcal{D}$
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 评论回归在 $\mathcal{D}$
- en: 'Require: data buffer $\mathcal{D}$'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 需求：数据缓冲区 $\mathcal{D}$
- en: '|  | $\mathcal{R}_{s,\boldsymbol{a}^{i_{1:u}}}=\sum_{\boldsymbol{a}^{-i_{1:u}}\in\mathcal{D}}\sum_{t=0}^{T}\gamma^{t}r_{t}$
    |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{R}_{s,\boldsymbol{a}^{i_{1:u}}}=\sum_{\boldsymbol{a}^{-i_{1:u}}\in\mathcal{D}}\sum_{t=0}^{T}\gamma^{t}r_{t}$
    |  |'
- en: update estimated critic $Q_{\theta}^{i_{1:u}}$ by using
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更新的评论 $Q_{\theta}^{i_{1:u}}$
- en: '|  | $1$2 |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: end for  end for
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 结束 for  结束 for
- en: 'Algorithm 2 *ReAd-S*: Reinforced Advantage Feedback with Sequential Individual
    Plan Refinement'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 *ReAd-S*：顺序个体计划优化的强化优势反馈
- en: 'Require: agent name $u^{1},...,u^{N}$ then        break     end if  end while'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 需求：代理名称 $u^{1},...,u^{N}$ 然后        退出     结束 if  结束 while
- en: 'Algorithm 3 *ReAd-J*: Reinforced Advantage Feedback with Joint Plan Refinement'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 *ReAd-J*：联合计划优化的强化优势反馈
- en: 'Require: agent name $u^{1},...,u^{N}$ then        break     end if  end while'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 需求：代理名称 $u^{1},...,u^{N}$ 然后        退出     结束 if  结束 while
- en: Appendix D Environment Details
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 环境细节
- en: 'We use Difficult Variants of RoCoBench (*DV-RoCoBench*) adapted from RoCoBench
    [[42](#bib.bib42)] and *Overcooked-AI* [[8](#bib.bib8)] in our experiments. *DV-RoCoBench*
    involves three tasks: Sweep Floor, Make Sandwich and Sort Cubes. And we choose
    two representative scenarios – Cramped Room and Forced Coordination from *Overcooked-AI*
    in our experiments. In this section, we present a comprehensive overview of the
    task specifications along with the difficulty modifications we have made in *DV-RoCoBench*
    and the scenario specifications in two scenarios of *Overcooked-AI*.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中使用了从 RoCoBench [[42](#bib.bib42)] 和 *Overcooked-AI* [[8](#bib.bib8)] 适配的
    RoCoBench 的困难变体 (*DV-RoCoBench*)。*DV-RoCoBench* 涉及三个任务：扫地、制作三明治和排序立方体。在实验中，我们选择了两个具有代表性的场景——*Overcooked-AI*
    中的狭小房间和强制协调。在本节中，我们提供了任务规格的全面概述，以及我们在*DV-RoCoBench* 中所做的难度修改和*Overcooked-AI* 中两个场景的场景规格。
- en: As for *DV-RoCoBench*, we directly inherit the action set and quantity of robots
    from RoCoBench, but design diverse task goals to introduce different difficulty
    levels. In original RoCoBench, the action set is not the same among different
    tasks.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *DV-RoCoBench*，我们直接继承了 RoCoBench 的动作集和机器人数量，但设计了不同的任务目标以引入不同的难度等级。在原始 RoCoBench
    中，不同任务之间的动作集并不相同。
- en: As for *Overcooked-AI*, different scenarios share the same action space but
    are initialized with different kitchen layouts.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 至于 *Overcooked-AI*，不同的场景共享相同的动作空间，但初始化时具有不同的厨房布局。
- en: D.1 Sweep Floor
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 清扫地面
- en: Task Description.
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务描述。
- en: In this task, the two robots are positioned on opposite sides of the table.
    Each robot arm equipped with a dustpan and broom must collaborate to efficiently
    sweep all cubes of the designated color into the dustpan. Subsequently, the robot
    that holds the dustpan is responsible for disposing of the collected cubes in
    the trash bin. In this environment, two distinct types of robots with different
    action sets are used.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，两个机器人被放置在桌子的对面。每个机器人臂配备了扫帚和垃圾铲，必须协作有效地将指定颜色的所有立方体扫入垃圾铲中。随后，持有垃圾铲的机器人负责将收集到的立方体丢弃到垃圾桶中。在这个环境中，使用了具有不同动作集的两种不同类型的机器人。
- en: '1.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'UR5E robot holding a dustpan (‘Alice’): can move to all cubes and can perform
    only three operations: MOVE, DUMP, and WAIT.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持有扫帚的 UR5E 机器人（‘Alice’）：可以移动到所有的立方体，并且只能执行三项操作：MOVE、DUMP 和 WAIT。
- en: '2.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Franka Panda holding a broom (‘Bob’): can move to all cubes and can perform
    only three operations: MOVE, SWEEP, and WAIT.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 持有扫帚的 Franka Panda（‘Bob’）：可以移动到所有的立方体，并且只能执行三项操作：MOVE、SWEEP 和 WAIT。
- en: '3.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Action sets: (i) MOVE [target]: target can only be a cube. (ii) DUMP: pour
    all cubes in the dustpan into the trash bin. (iii) SWEEP [target]: sweep the target
    cube into the dustpan. (iv) WAIT.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作集：（i）MOVE [target]：目标只能是一个立方体。（ii）DUMP：将扫帚中的所有立方体倒入垃圾桶。（iii）SWEEP [target]：将目标立方体扫入扫帚中。（iv）WAIT。
- en: Difficulty Settings.
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 难度设置。
- en: We shift the task goal from sweeping away all the cubes to sweeping away the
    cubes of a given color. We establish 5 distinct difficulty levels based on the
    number of cubes and the number of the target cubes. By increasing the difficulty
    level step by step, the quantity of all cubes and the cubes of a given color increase
    also gradually, as shown in Figure [5](#A4.F5 "Figure 5 ‣ Difficulty Settings.
    ‣ D.1 Sweep Floor ‣ Appendix D Environment Details ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration").
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将任务目标从清扫所有立方体转移到清扫指定颜色的立方体。我们基于立方体的数量和目标立方体的数量建立了 5 个不同的难度等级。通过逐步增加难度等级，所有立方体的数量以及指定颜色的立方体数量也逐渐增加，如图
    [5](#A4.F5 "图 5 ‣ 难度设置 ‣ D.1 清扫地面 ‣ 附录 D 环境细节 ‣ 实现有效的 LLM 基础设定用于具身多智能体协作") 所示。
- en: '![Refer to caption](img/ad62a29f673dd8901495cb9a28923ffe.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad62a29f673dd8901495cb9a28923ffe.png)'
- en: 'Figure 5: The initial states of the 5 difficulty levels in modified Sweep Floor.
    The yellow and green squares are the ones to be swept in this task. The first
    three tasks have a total of 7 squares, while the last two have 9\. We assess task
    difficulty based on the number of cubes to be swept and the total cube number.
    For example, the Y1_G1 in the figure represents 1 yellow cube and 1 green cube
    needs to be swept.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：修改后的清扫地面任务中 5 个难度等级的初始状态。黄色和绿色的方块是本任务中需要清扫的。前三个任务共有 7 个方块，而最后两个任务有 9 个。我们根据需要清扫的立方体数量和总立方体数量来评估任务难度。例如，图中的
    Y1_G1 代表需要清扫 1 个黄色立方体和 1 个绿色立方体。
- en: D.2 Make Sandwich
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 制作三明治
- en: Task Description.
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务描述。
- en: In this task, two robots are positioned on opposite sides of a table to assemble
    a sandwich based on a given recipe, requiring collaborative effort to collect
    and stack the ingredients in the specified order until all components have been
    properly arranged. This environment accommodates two distinct types of robots
    capable of executing all actions in the action set. Each robot has a restricted
    range to manipulate the cubes.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，两个机器人被放置在桌子的对面，根据给定的食谱来组装三明治，需要协作收集并按指定顺序堆叠配料，直到所有组件都正确排列。这个环境支持两种不同类型的机器人，这些机器人能够执行所有动作集中的操作。每个机器人在操作立方体时都有一个受限的范围。
- en: '1.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'UR5E robot (‘Chad’): can only retrieve the food on the right side.'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UR5E 机器人（‘Chad’）：只能取出右侧的食物。
- en: '2.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Humanoid robot (‘Dave’): can only retrieve the food on the left side.'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类人机器人（‘Dave’）：只能取出左侧的食物。
- en: '3.'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Action set: 1) PICK [object]: object must be a food. 2) PUT [object] on [target]:
    object must be a food and target could be a food, cutting_board, or table. 3)
    WAIT.'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作集合：1) PICK [object]：对象必须是食物。2) PUT [object] on [target]：对象必须是食物，目标可以是食物、切菜板或桌子。3)
    WAIT。
- en: Difficulty Settings.
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 难度设置。
- en: We establish 4 distinct difficulty levels dependent on the length of the recipe.
    A longer recipe requires more complex collaboration between humanoid and robot
    arm. The recipe lengths for these different settings are set to 3, 5, 7, and 9,
    respectively, as shown in Figure [6](#A4.F6 "Figure 6 ‣ Difficulty Settings. ‣
    D.2 Make Sandwich ‣ Appendix D Environment Details ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration").
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据配方长度建立了4个不同的难度级别。较长的配方需要类人和机器人手臂之间更复杂的协作。这些不同设置的配方长度分别为3、5、7和9，如图[6](#A4.F6
    "图6 ‣ 难度设置 ‣ D.2 制作三明治 ‣ 附录D 环境细节 ‣ 向有效的LLM对接多智能体协作")所示。
- en: '![Refer to caption](img/fbe00bbf702627c2fea9680e857b411a.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fbe00bbf702627c2fea9680e857b411a.png)'
- en: 'Figure 6: The initial states of the 4 difficulty levels in modified Make Sandwich.
    The initial three tasks shared the same food and layout, differing only in the
    length of the recipe. Conversely, the final task presented distinct food and layout,
    accompanied by a lengthier recipe. The recipe lengths for four tasks are set to
    3, 5, 7, and 9, respectively.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：修改后的“制作三明治”任务中4个难度级别的初始状态。前面三个任务共享相同的食物和布局，仅在配方长度上有所不同。相反，最后一个任务呈现了不同的食物和布局，并且配方较长。四个任务的配方长度分别设置为3、5、7和9。
- en: D.3 Sort Cubes
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 排序立方体
- en: Task Description.
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务描述。
- en: The task requires three robots positioned on opposite sides of a table to collaboratively
    place three target blocks in specific locations, utilizing their limited range
    of motion and assisting each other as needed. The current environment consists
    of three robots capable of executing all actions in the action set, albeit with
    limited mobility range.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务要求三台机器人在桌子的对面协作，将三个目标块放置在特定位置，利用它们有限的运动范围并根据需要互相协助。当前环境包括三台机器人，可以执行动作集合中的所有动作，但移动范围有限。
- en: '1.'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'UR5E with robotic gripper (‘Alice’): must put the blue square on panel2, can
    only reach: panel1, panel2, panel3.'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UR5E带机械夹具（‘Alice’）：必须将蓝色正方形放在panel2上，只能到达：panel1、panel2、panel3。
- en: '2.'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Franka Panda (‘Bob’): must put pink polygon on panel4, can only reach: panel3,
    panel4, panel5.'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Franka Panda（‘Bob’）：必须将粉色多边形放在panel4上，只能到达：panel3、panel4、panel5。
- en: '3.'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'UR5E with suction gripper (‘Chad’): must put yellow trapezoid on panel6, can
    only reach: panel5, panel6, panel7.'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UR5E带吸盘夹具（‘Chad’）：必须将黄色梯形放在panel6上，只能到达：panel5、panel6、panel7。
- en: '4.'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Action set: 1) PICK [object] PLACE [panelX]: the object must be a cube and
    panelX cannot be the target panel of another cube. 2) WAIT.'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作集合：1) PICK [object] PLACE [panelX]：对象必须是一个立方体，panelX不能是另一个立方体的目标面板。2) WAIT。
- en: Difficulty Settings.
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 难度设置。
- en: We establish 5 difficulty levels based on the distance of the three blocks towards
    their corresponding target location. Since each robot has limited range of motion,
    picking further cube to the target location requires more complex collaboration
    between three robot arms.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据三个块到其对应目标位置的距离设置了5个难度级别。由于每台机器人的运动范围有限，将更远的立方体移至目标位置需要三台机器人手臂之间更复杂的协作。
- en: '![Refer to caption](img/8d72d689eb72315bacc77a2ca20a9cb8.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d72d689eb72315bacc77a2ca20a9cb8.png)'
- en: 'Figure 7: The initial states of the 5 difficulty levels in modified Sort Cubes.
    In these tasks, we orchestrated the initial placement of each block, and gauged
    difficulty based on the cumulative distance between the three blocks and the target
    panel. The shape of the three cubes was modified to avoid the robot’s inability
    to pick up the objects due to their shape.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：修改后的“排序立方体”任务中5个难度级别的初始状态。在这些任务中，我们安排了每个块的初始放置，并根据三个块与目标面板之间的累计距离来衡量难度。为了避免由于形状问题导致机器人无法拾取物体，修改了三个立方体的形状。
- en: D.4 Overcooked-AI
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 烧烤-AI
- en: 'In *Overcooked-AI*, two agents are originally required to make as much soup
    as possible in limited timesteps with high coordination efficiency. Agents place
    a specified number of onions in a pot, leave them to cook for a specified number
    of timesteps, put the resulting soup in a dish, and serve it, giving all agents
    a reward. The capacity of all agents to pick up items is 1. Every agent can only
    carry 1 item such as the dish and the onion. In our experiment, to enable measuring
    with the success rate metric, we modify the task as cooking and delivering a soup
    to the service counter within a specified number of timesteps. The action set
    of this environment are as following:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Overcooked-AI* 中，两个代理最初需要在有限的时间步内尽可能高效地协作制作汤。代理将指定数量的洋葱放入锅中，留给它们煮一定的时间步，将煮好的汤放入盘子中，并上菜，给所有代理奖励。所有代理捡起物品的能力为1。每个代理只能携带1件物品，如盘子和洋葱。在我们的实验中，为了能够使用成功率指标进行测量，我们将任务修改为在指定的时间步内将汤煮好并送到服务台。该环境的动作集如下：
- en: '1.'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'north: agent moves one step north. If agent collides with another object, it
    will not move.'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'north: 代理向北移动一步。如果代理与另一个对象碰撞，它将不会移动。'
- en: '2.'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'south: agent moves one step south. Same as the previous term.'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'south: 代理向南移动一步。与前一个术语相同。'
- en: '3.'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'east: agent moves one step east. Same as the previous term.'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'east: 代理向东移动一步。与前一个术语相同。'
- en: '4.'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'west: agent moves one step west. Same as the previous term.'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'west: 代理向西移动一步。与前一个术语相同。'
- en: '5.'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'interact: agent interacts with a object, including picking up or putting down
    an item, turning on the cooking table, and putting the cooked soup in the dish.'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'interact: 代理与对象进行互动，包括捡起或放下物品，打开烹饪台，以及将煮好的汤放入盘子中。'
- en: '6.'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'stay: agent does nothing.'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'stay: 代理什么都不做。'
- en: 'The first four actions (north, south, east and west) cover the movement of
    the agent, and the interact action enables the interaction between the agent and
    other objects. We use Figure [8](#A4.F8 "Figure 8 ‣ D.4 Overcooked-AI ‣ Appendix
    D Environment Details ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") to explain the above rules:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个动作（north、south、east 和 west）涵盖了代理的移动，而 interact 动作使代理与其他对象进行互动。我们使用图 [8](#A4.F8
    "Figure 8 ‣ D.4 Overcooked-AI ‣ Appendix D Environment Details ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration")来解释上述规则：
- en: '![Refer to caption](img/31a0b259fc2d36e653b08a34d2486172.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/31a0b259fc2d36e653b08a34d2486172.png)'
- en: 'Figure 8: In 2nd frame, since both agents collide with the workbench, the agents
    merely change their current orientation. In 4th frame, since both agents have
    picked up an object in their hands, executing "interact" again will not pick up
    additional items. In 7th frame, agent1 places the onion on the cooking table.
    And in 8th frame, agent1 turns on the cooking table and starts cooking. In 10th
    and 11th frames, the soup is done and then put in a dish by agent0\. In the last
    frame, agent0 serves the cooked soup.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '图8: 在第二帧中，由于两个代理都与工作台碰撞，因此代理仅更改当前方向。在第四帧中，由于两个代理都手里拿着物品，再次执行“interact”不会捡起额外的物品。在第七帧中，代理1将洋葱放在烹饪台上。在第八帧中，代理1打开烹饪台并开始烹饪。在第十和第十一帧中，汤煮好了，然后由代理0放入盘子中。在最后一帧中，代理0上菜。'
- en: Cramped Room.
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 狭小的房间。
- en: Two agents collaborate in a relatively small kitchen, and thus two agents must
    be extremely careful to avoid collisions in order to complete the cooking task
    as quickly as possible. The scenario is shown in the Figure [8](#A4.F8 "Figure
    8 ‣ D.4 Overcooked-AI ‣ Appendix D Environment Details ‣ Towards Efficient LLM
    Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 两个代理在一个相对较小的厨房里协作，因此两个代理必须非常小心，以避免碰撞，从而尽可能快地完成烹饪任务。该场景如图 [8](#A4.F8 "Figure
    8 ‣ D.4 Overcooked-AI ‣ Appendix D Environment Details ‣ Towards Efficient LLM
    Grounding for Embodied Multi-Agent Collaboration")所示。
- en: Forced Coordination.
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强制协调。
- en: The working spaces of two agents are completely separated, where one agent only
    has access to the cooking table and the service counter and the other only has
    access to onions and dishes. The scenario is shown in the Figure [9](#A4.F9 "Figure
    9 ‣ Forced Coordination. ‣ D.4 Overcooked-AI ‣ Appendix D Environment Details
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 两个代理的工作区域完全分开，其中一个代理只能访问烹饪台和服务台，而另一个代理只能访问洋葱和盘子。该场景如图 [9](#A4.F9 "Figure 9 ‣
    Forced Coordination. ‣ D.4 Overcooked-AI ‣ Appendix D Environment Details ‣ Towards
    Efficient LLM Grounding for Embodied Multi-Agent Collaboration")所示。
- en: '![Refer to caption](img/5ff63b3531fb75e610275b4c5e599f66.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5ff63b3531fb75e610275b4c5e599f66.png)'
- en: 'Figure 9: In this task, agent0 must wait for agent1 to deliver the onion to
    the table before agent0 can place it on the cooking table, and after the soup
    is ready, agent0 must wait for agent1 to place the plate on the table before it
    can serve the soup and deliver it to the service table.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 在此任务中，agent0必须等待agent1将洋葱送到桌子上后，才能将其放到烹饪台上，并且在汤做好后，agent0必须等待agent1将盘子放到桌子上，然后才能上菜并将其送到服务台。'
- en: Appendix E Additional Experimental Results
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 额外实验结果
- en: In this section, we give the detailed experiment results of 3 tasks in *DV-RoCoBench*
    and 2 scenarios in *Overcooked-AI*. We also show the execution screenshots of
    our method and baselines in the representative environments.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细展示了在*DV-RoCoBench*中的3个任务和在*Overcooked-AI*中的2个场景的实验结果。我们还展示了我们方法和基线在代表性环境中的执行截图。
- en: E.1 Comparison of Baselines
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 基线比较
- en: 'Table 3: Overview of the key properties that distinguish four methods. (i)
    State Type: whether the environment state included in the prompt is global or
    not; (ii) Planning Scheme: whether LLM output plans sequentially or not; (iii)
    History Info: whether all the history before is reserved in the prompt or not.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 区分四种方法的关键属性概览。(i) 状态类型: 提示中包含的环境状态是否为全局的; (ii) 规划方案: LLM输出的计划是否顺序; (iii)
    历史信息: 提示中是否保留了所有历史信息。'
- en: '|  | State Type | Planning Scheme | History Info | Feedback Type |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | 状态类型 | 规划方案 | 历史信息 | 反馈类型 |'
- en: '| RoCo | partial | Sequential | all previous rounds | Physical Verification
    |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| RoCo | 部分 | 顺序 | 所有前几轮 | 实体验证 |'
- en: '| ReAd-S | partial | Sequential | last round | Advantage Score |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-S | 部分 | 顺序 | 最后一轮 | 优势分数 |'
- en: '| Central-Plan | global | Parallel | all previous rounds | Physical Verification
    |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Central-Plan | 全局 | 并行 | 所有前几轮 | 实体验证 |'
- en: '| ReAd-J | global | Parallel | last round | Advantage Score |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-J | 全局 | 并行 | 最后一轮 | 优势分数 |'
- en: '| ReAct | global | Parallel | all previous rounds | Physical Verification |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 全局 | 并行 | 所有前几轮 | 实体验证 |'
- en: '| Reflexion | global | Parallel | all previous rounds | Physical Verification
    |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion | 全局 | 并行 | 所有前几轮 | 实体验证 |'
- en: '| MindAgent | global | Parallel | all previous rounds | Physical Verification
    |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| MindAgent | 全局 | 并行 | 所有前几轮 | 实体验证 |'
- en: E.2 Main Experiments
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 主要实验
- en: The results of all experiments are shown in Table [4](#A5.T4 "Table 4 ‣ E.2
    Main Experiments ‣ Appendix E Additional Experimental Results ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration"), and Table [5](#A5.T5 "Table
    5 ‣ E.2 Main Experiments ‣ Appendix E Additional Experimental Results ‣ Towards
    Efficient LLM Grounding for Embodied Multi-Agent Collaboration"). SR, NQ and ES
    represent success rates, the average number of requests to LLMs, and rounds of
    environment interactions, respectively. We have provided a detailed introduction
    to these metrics in §[5.1](#S5.SS1 "5.1 Experimental Setup ‣ 5 Experiments ‣ Towards
    Efficient LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验结果见表 [4](#A5.T4 "表 4 ‣ E.2 主要实验 ‣ 附录 E 额外实验结果 ‣ 针对具身多智能体协作的高效LLM基础"), 和表 [5](#A5.T5
    "表 5 ‣ E.2 主要实验 ‣ 附录 E 额外实验结果 ‣ 针对具身多智能体协作的高效LLM基础")。SR、NQ 和 ES 分别表示成功率、LLM的请求平均次数和环境交互轮次。我们在§[5.1](#S5.SS1
    "5.1 实验设置 ‣ 5 实验 ‣ 针对具身多智能体协作的高效LLM基础")中详细介绍了这些指标。
- en: 'Table 4: The detailed results of the comparison in different tasks with various
    difficulty levels in *DV-RoCoBench*. The mean value and standard error are calculated
    over 10 random seeds.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 在*DV-RoCoBench*中，不同任务和各种难度级别的详细比较结果。均值和标准误差是基于10个随机种子计算的。'
- en: RoCo ReAct Central Plan Reflexion SR NQ ES SR NQ ES SR NQ ES SR NQ ES sweep
    Y1_G1 0.9±0.32 14.4±5.95 6.2±3.12 1.0±0.00 5.5±0.50 5.5±0.50 0.4±0.52 15.3±0.48
    11.2±4.92 1.0±0.00 5.0±0.00 5.0±0.00 Y1_G2 1.0±0.00 24.2±4.18 8.9±1.45 1.0±0.00
    8.2±0.25 8.2±0.25 1.0±0.00 7.8±1.99 7.8±1.99 1.0±0.00 7.0±0.00 7.0±0.00 Y2_G2
    1.0±0.00 29.1±5.40 10.6±1.35 1.0±0.00 10.0±0.00 10.0±0.00 0.8±0.42 12.7±1.77 12.7±1.77
    1.0±0.00 10.1±0.10 10.0±0.00 Y2_G3 0.7±0.48 36.7±6.63 13.5±1.27 0.6±0.16 14.4±0.67
    13.8±0.33 0.2±0.42 14.6±0.97 14.6±0.97 0.7±0.15 14.3±0.87 12.9±0.48 Y3_G3 0.6±0.52
    41.8±7.73 14.7±0.48 0.4±0.16 15.2±0.25 14.9±0.32 0.0±0.00 15.0±0.00 15.0±0.00
    0.3±0.15 15.1±0.23 14.9±0.10 sandwich recipe1 1.0±0.00 13.2±3.74 4.7±0.67 1.0±0.00
    4.0±0.00 4.0±0.00 1.0±0.00 6.2±0.63 4.0±0.00 1.0±0.00 5.0±0.00 4.0±0.00 recipe2
    0.9±0.32 28.9±11.25 9.1±2.42 1.0±0.00 6.0±0.00 6.0±0.00 1.0±0.00 8.2±0.42 6.0±0.00
    1.0±0.00 6.8±0.13 6.0±0.00 recipe3 0.8±0.42 33.7±10.00 11.5±2.99 0.7±0.15 12.9±2.61
    10.1±1.07 1.0±0.00 10.2±0.42 8.0±0.00 0.6±0.16 14.9±2.47 10.8±1.14 recipe4 0.5±0.53
    43.1±17.84 13.1±2.47 0.6±0.16 16.7±2.60 12.5±0.75 0.4±0.52 80.5±53.35 14.2±1.14
    0.5±0.17 17.7±2.39 13.1±0.67 sort sort1 1.0±0.00 3.3±0.95 1.1±0.32 1.0±0.00 1.2±0.13
    1.0±0.00 1.0±0.00 1.0±0.00 1.0±0.00 1.0±0.00 1.2±0.13 1.0±0.00 sort2 1.0±0.00
    13.5±4.67 3.4±0.52 0.6±0.16 14.8±4.56 7.8±1.96 1.0±0.00 16.9±9.13 2.6±0.52 1.0±0.00
    5.5±0.48 2.9±0.10 sort3 1.0±0.00 18.6±15.10 4.9±2.60 0.8±0.13 19.4±6.18 6.4±1.45
    1.0±0.00 8.3±4.32 2.3±0.95 1.0±0.00 6.6±0.50 4.7±0.33 sort4 1.0±0.00 24.8±9.37
    6.4±1.78 0.8±0.13 24.0±11.31 6.1±1.49 1.0±0.00 37.2±25.05 7.1±2.77 0.7±0.13 19.2±6.83
    7.1±1.45 sort5 1.0±0.00 38.5±9.96 7.4±2.95 0.7±0.15 17.3±3.00 8.4±1.59 0.6±0.52
    128.4±115.99 11.0±3.97 0.8±0.13 13.9±3.27 6.9±1.43 average 0.89±0.19 25.99±8.06
    8.25±1.74 0.80±0.09 12.11±2.29 8.19±0.69 0.74±0.17 25.88±15.32 8.39±1.36 0.83±0.06
    10.16±1.24 7.59±0.41 Mind ReAd-S ReAd-J SR NQ ES SR NQ ES SR NQ ES sweep Y1_G1
    1.0±0.00 5.0±0.00 5.0±0.00 1.0±0.00 10.4±0.52 5.0±0.00 1.0±0.00 5.9±0.99 5.0±0.00
    Y1_G2 1.0±0.00 7.1±0.10 7.1±0.10 1.0±0.00 14.4±0.84 7.0±0.00 1.0±0.00 7.6±0.70
    7.0±0.00 Y2_G2 1.0±0.00 9.9±0.18 9.8±0.13 1.0±0.00 19.9±3.28 9.4±0.70 1.0±0.00
    13.0±4.32 9.0±0.00 Y2_G3 0.7±0.15 13.4±0.48 13.4±0.48 0.9±0.32 26.8±5.20 12.2±1.32
    1.0±0.00 16.4±6.02 11.7±1.49 Y3_G3 0.2±0.13 15.1±0.10 15.0±0.00 0.8±0.42 31.4±3.50
    14.0±0.82 0.8±0.42 16.4±1.71 13.4±0.84 sandwich recipe1 1.0±0.00 5.1±0.10 4.0±0.00
    1.0±0.00 10.5±4.74 4.2±0.42 1.0±0.00 4.3±0.48 4.0±0.00 recipe2 1.0±0.00 6.6±0.16
    6.0±0.00 1.0±0.00 14.5±2.46 6.4±0.52 1.0±0.00 6.5±0.85 6.0±0.00 recipe3 0.7±0.16
    12.4±1.92 10.1±1.07 1.0±0.00 22.1±5.22 8.9±0.88 1.0±0.00 14.6±8.04 8.9±1.00 recipe4
    0.6±0.16 16.5±2.24 12.7±0.72 1.0±0.00 27.9±8.06 11.1±1.73 1.0±0.00 10.8±0.42 10.0±0.00
    sort sort1 1.0±0.00 1.2±0.13 1.0±0.00 1.0±0.00 3.4±0.52 1.0±0.00 1.0±0.00 1.1±0.32
    1.1±0.32 sort2 1.0±0.00 6.1±1.12 3.2±0.33 1.0±0.00 10.8±2.53 3.1±0.32 1.0±0.00
    7.3±2.91 3.3±0.48 sort3 0.8±0.13 11.1±3.70 6.2±1.54 1.0±0.00 17.5±2.80 3.9±0.57
    1.0±0.00 8.3±3.80 3.4±0.84 sort4 0.9±0.10 22.6±9.62 5.9±1.12 1.0±0.00 21.6±7.07
    3.7±0.67 1.0±0.00 18.8±6.29 4.3±0.95 sort5 0.8±0.13 18.0±4.12 7.8±1.35 1.0±0.00
    33.5±6.35 6.1±0.88 1.0±0.00 17.3±11.87 4.4±1.26 average 0.84±0.07 10.72±1.71 7.66±0.49
    0.98±0.05 18.91±3.79 6.86±0.63 0.99±0.03 10.59±3.48 6.54±0.51
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: RoCo ReAct 中央计划反射 SR NQ ES SR NQ ES SR NQ ES SR NQ ES 扫描 Y1_G1 0.9±0.32 14.4±5.95
    6.2±3.12 1.0±0.00 5.5±0.50 5.5±0.50 0.4±0.52 15.3±0.48 11.2±4.92 1.0±0.00 5.0±0.00
    5.0±0.00 Y1_G2 1.0±0.00 24.2±4.18 8.9±1.45 1.0±0.00 8.2±0.25 8.2±0.25 1.0±0.00
    7.8±1.99 7.8±1.99 1.0±0.00 7.0±0.00 7.0±0.00 Y2_G2 1.0±0.00 29.1±5.40 10.6±1.35
    1.0±0.00 10.0±0.00 10.0±0.00 0.8±0.42 12.7±1.77 12.7±1.77 1.0±0.00 10.1±0.10 10.0±0.00
    Y2_G3 0.7±0.48 36.7±6.63 13.5±1.27 0.6±0.16 14.4±0.67 13.8±0.33 0.2±0.42 14.6±0.97
    14.6±0.97 0.7±0.15 14.3±0.87 12.9±0.48 Y3_G3 0.6±0.52 41.8±7.73 14.7±0.48 0.4±0.16
    15.2±0.25 14.9±0.32 0.0±0.00 15.0±0.00 15.0±0.00 0.3±0.15 15.1±0.23 14.9±0.10
    三明治 配方1 1.0±0.00 13.2±3.74 4.7±0.67 1.0±0.00 4.0±0.00 4.0±0.00 1.0±0.00 6.2±0.63
    4.0±0.00 1.0±0.00 5.0±0.00 4.0±0.00 配方2 0.9±0.32 28.9±11.25 9.1±2.42 1.0±0.00
    6.0±0.00 6.0±0.00 1.0±0.00 8.2±0.42 6.0±0.00 1.0±0.00 6.8±0.13 6.0±0.00 配方3 0.8±0.42
    33.7±10.00 11.5±2.99 0.7±0.15 12.9±2.61 10.1±1.07 1.0±0.00 10.2±0.42 8.0±0.00
    0.6±0.16 14.9±2.47 10.8±1.14 配方4 0.5±0.53 43.1±17.84 13.1±2.47 0.6±0.16 16.7±2.60
    12.5±0.75 0.4±0.52 80.5±53.35 14.2±1.14 0.5±0.17 17.7±2.39 13.1±0.67 排序 排序1 1.0±0.00
    3.3±0.95 1.1±0.32 1.0±0.00 1.2±0.13 1.0±0.00 1.0±0.00 1.0±0.00 1.0±0.00 1.0±0.00
    1.2±0.13 1.0±0.00 排序2 1.0±0.00 13.5±4.67 3.4±0.52 0.6±0.16 14.8±4.56 7.8±1.96
    1.0±0.00 16.9±9.13 2.6±0.52 1.0±0.00 5.5±0.48 2.9±0.10 排序3 1.0±0.00 18.6±15.10
    4.9±2.60 0.8±0.13 19.4±6.18 6.4±1.45 1.0±0.00 8.3±4.32 2.3±0.95 1.0±0.00 6.6±0.50
    4.7±0.33 排序4 1.0±0.00 24.8±9.37 6.4±1.78 0.8±0.13 24.0±11.31 6.1±1.49 1.0±0.00
    37.2±25.05 7.1±2.77 0.7±0.13 19.2±6.83 7.1±1.45 排序5 1.0±0.00 38.5±9.96 7.4±2.95
    0.7±0.15 17.3±3.00 8.4±1.59 0.6±0.52 128.4±115.99 11.0±3.97 0.8±0.13 13.9±3.27
    6.9±1.43 平均 0.89±0.19 25.99±8.06 8.25±1.74 0.80±0.09 12.11±2.29 8.19±0.69 0.74±0.17
    25.88±15.32 8.39±1.36 0.83±0.06 10.16±1.24 7.59±0.41 心智 ReAd-S ReAd-J SR NQ ES
    SR NQ ES SR NQ ES 扫描 Y1_G1 1.0±0.00 5.0±0.00 5.0±0.00 1.0±0.00 10.4±0.52 5.0±0.00
    1.0±0.00 5.9±0.99 5.0±0.00 Y1_G2 1.0±0.00 7.1±0.10 7.1±0.10 1.0±0.00 14.4±0.84
    7.0±0.00 1.0±0.00 7.6±0.70 7.0±0.00 Y2_G2 1.0±0.00 9.9±0.18 9.8±0.13 1.0±0.00
    19.9±3.28 9.4±0.70 1.0±0.00 13.0±4.32 9.0±0.00 Y2_G3 0.7±0.15 13.4±0.48 13.4±0.48
    0.9±0.32 26.8±5.20 12.2±1.32 1.0±0.00 16.4±6.02 11.7±1.49 Y3_G3 0.2±0.13 15.1±0.10
    15.0±0.00 0.8±0.42 31.4±3.50 14.0±0.82 0.8±0.42 16.4±1.71 13.4±0.84 三明治 配方1 1.0±0.00
    5.1±0.10 4.0±0.00 1.0±0.00 10.5±4.74 4.2±0.42 1.0±0.00 4.3±0.48 4.0±0.00 配方2 1.0±0.00
    6.6±0.16 6.0±0.00 1.0±0.00 14.5±2.
- en: 'Table 5: The detailed results of the comparison in two scenarios in *Overcooked-AI*.
    The mean value and standard error are calculated over 10 random seeds.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：*Overcooked-AI* 中两种场景比较的详细结果。均值和标准误差是基于 10 个随机种子计算的。
- en: Cramped_room Forced_coordination average SR NQ ES SR NQ ES SR NQ ES ReAct 0.0±0.00
    20.1±0.10 20.0±0.00 0.0±0.00 26.9±0.75 25.0±0.00 0.00±0.00 23.50±0.43 22.50±0.00
    Reflexion 0.0±0.00 20.0±0.00 20.0±0.00 0.0±0.00 26.1±0.60 25.0±0.00 0.00±0.00
    23.05±0.30 22.50±0.00 MindAgent 0.0±0.00 20.8±0.47 20.0±0.00 0.0±0.00 26.9±0.80
    25.0±0.00 0.00±0.00 23.85±0.64 22.50±0.00 Central 0.0±0.00 20.0±0.00 20.0±0.00
    0.0±0.00 25.0±0.00 25.0±0.00 0.00±0.00 22.50±0.00 22.50±0.00 Read-J 0.4±0.16 23.9±1.49
    18.9±0.59 0.3±0.15 27.2±0.53 24.8±0.20 0.35±0.16 25.55±1.01 21.85±0.40
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Cramped_room Forced_coordination 平均 SR NQ ES SR NQ ES SR NQ ES ReAct 0.0±0.00
    20.1±0.10 20.0±0.00 0.0±0.00 26.9±0.75 25.0±0.00 0.00±0.00 23.50±0.43 22.50±0.00
    Reflexion 0.0±0.00 20.0±0.00 20.0±0.00 0.0±0.00 26.1±0.60 25.0±0.00 0.00±0.00
    23.05±0.30 22.50±0.00 MindAgent 0.0±0.00 20.8±0.47 20.0±0.00 0.0±0.00 26.9±0.80
    25.0±0.00 0.00±0.00 23.85±0.64 22.50±0.00 Central 0.0±0.00 20.0±0.00 20.0±0.00
    0.0±0.00 25.0±0.00 25.0±0.00 0.00±0.00 22.50±0.00 22.50±0.00 Read-J 0.4±0.16 23.9±1.49
    18.9±0.59 0.3±0.15 27.2±0.53 24.8±0.20 0.35±0.16 25.55±1.01 21.85±0.40
- en: E.3 Visualization of Robustness Evaluation
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 稳健性评估可视化
- en: We visualize the robustness comparison between *ReAd-S* and *RoCo* for accomplishing
    *Make Sandwich* recipe3 task when the environment resets at timestep $n=2$, as
    shown in Figure [10](#A5.F10 "Figure 10 ‣ E.3 Visualization of Robustness Evaluation
    ‣ Appendix E Additional Experimental Results ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration") and Figure [11](#A5.F11 "Figure 11 ‣
    E.3 Visualization of Robustness Evaluation ‣ Appendix E Additional Experimental
    Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可视化了 *ReAd-S* 和 *RoCo* 在环境在时间步 $n=2$ 重置时完成 *Make Sandwich* recipe3 任务的稳健性比较，如图
    [10](#A5.F10 "图 10 ‣ E.3 稳健性评估可视化 ‣ 附录 E 额外实验结果 ‣ 朝着高效的 LLM 基础设置迈进") 和图 [11](#A5.F11
    "图 11 ‣ E.3 稳健性评估可视化 ‣ 附录 E 额外实验结果 ‣ 朝着高效的 LLM 基础设置迈进") 所示。
- en: '![Refer to caption](img/047a4e6718503eaef005b763a33892a5.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/047a4e6718503eaef005b763a33892a5.png)'
- en: 'Figure 10: Screenshots of ReAd-S completing the recipe3 task in robustness
    test. After the environment is reset, our method will be affected by the historical
    dialogue information in a short period. After being prompted by the advantage
    function re-evaluated in the new state, our method can make a rapid re-plan based
    on the new state.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：ReAd-S 完成 recipe3 任务的稳健性测试截图。在环境重置后，我们的方法在短时间内会受到历史对话信息的影响。在新的状态下由重新评估的优势函数提示后，我们的方法能够根据新的状态快速重新规划。
- en: '![Refer to caption](img/d2ccff6a3dc7ae8028e84855b90ca1b0.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d2ccff6a3dc7ae8028e84855b90ca1b0.png)'
- en: 'Figure 11: Screenshots of RoCo completing the recipe3 task in robustness test.
    RoCo needs more steps to recover from the environmental disturbance. Since the
    reset information is not included in the history, RoCo will be misled by historical
    information and require multi-round physical feedback to adjust the plan.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：RoCo 完成 recipe3 任务的稳健性测试截图。RoCo 需要更多步骤才能从环境干扰中恢复。由于重置信息未包含在历史记录中，RoCo 会被历史信息误导，并需要多轮物理反馈来调整计划。
- en: E.4 Dataset and Critic Network
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4 数据集和评论网络
- en: Dataset Collection Details.
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集收集细节。
- en: The advantage function relies on the Monte-Carlo estimation of value function
    with access to an offline dataset collected by $\boldsymbol{\pi}_{\rm llm}$ policy,
    we intentionally reset the environment state to an unreachable state and initiate
    LLM-planning from that point.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数依赖于通过 $\boldsymbol{\pi}_{\rm llm}$ 策略收集的离线数据集对价值函数的蒙特卡罗估计，我们故意将环境状态重置为一个无法到达的状态，并从该点启动
    LLM 规划。
- en: Given that our theoretical analysis demonstrates that our method can achieve
    a superior policy compared to the behavior policy $\boldsymbol{\mu}$ can be utilized
    for dataset collection, potentially leading to further policy improvement during
    optimization. Subsequently, we conduct an ablation study utilizing a mixed dataset
    collected by an *expert policy* and an *LLM policy*. Our preliminary findings
    indicate that the inclusion of additional optimal data does not result in performance
    improvement. We hypothesize that two reasons contribute to these unexpected results.
    (i) The incorporation of data from a different policy introduces increased variance
    in Monte-Carlo estimation, thereby reducing the stability of the value functions.
    Consequently, the value function may produce high-variance outputs, potentially
    leading to misleading optimization of the LLM planner as prompts. (ii) The LLM
    planner equipped with enhanced augmentation techniques achieves improved data
    coverage of the resulting policy. In contrast, the optimal policy is more deterministic,
    leading to more limited state coverage, which poses challenges for value estimation
    of out-of-distribution (OOD) states and actions in LLM planning. This issue bears
    resemblance to the distribution shift problem encountered in offline RL [[32](#bib.bib32),
    [63](#bib.bib63)].
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们的理论分析表明，我们的方法可以实现比行为策略$\boldsymbol{\mu}$更优的策略，可用于数据集收集，从而可能在优化过程中进一步改进策略。随后，我们进行了一个消融研究，利用由*专家策略*和*LLM策略*收集的混合数据集。我们的初步发现表明，增加额外的最佳数据不会带来性能改进。我们假设两个原因导致了这些意外结果。（i）来自不同策略的数据的纳入引入了Monte-Carlo估计的方差增加，从而降低了价值函数的稳定性。因此，价值函数可能产生高方差的输出，可能导致LLM规划器在提示时的优化误导。（ii）配备增强技术的LLM规划器能够更好地覆盖数据，而最优策略则更具确定性，导致状态覆盖更有限，这对LLM规划中的分布外（OOD）状态和行动的价值估计构成挑战。这个问题类似于离线RL中遇到的分布转移问题[[32](#bib.bib32),
    [63](#bib.bib63)]。
- en: We describe the differences between *expert policy* and an *LLM policy* in detail
    here.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此详细描述*专家策略*和*LLM策略*之间的差异。
- en: •
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM policy: This policy is to leverage the reasoning power of LLM to solve
    specific tasks and use *physical verification* as feedback. It is recommended
    to use a variant of *ReAd-J* for data collection, which replaces *ReAd* feedback
    with *physical verification* and uses only the previous round of historical information
    in the prompts. At each time step $t$) until the task is accomplished.'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM策略：该策略利用LLM的推理能力来解决特定任务，并使用*物理验证*作为反馈。建议使用*ReAd-J*的变体进行数据收集，它将*ReAd*反馈替换为*物理验证*，并仅使用前一轮的历史信息作为提示。在每个时间步骤$t$)直到任务完成。
- en: •
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Expert policy: Here we implement this policy with human control. This requires
    a human player to analyze the task and infer the optimal action at each time step.
    The collected data format is the same as the method described above.'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 专家策略：这里我们用人工控制来实施这一策略。这需要人类玩家分析任务，并推断每个时间步骤的最佳行动。收集的数据格式与上述方法相同。
- en: 'Table 6: An ablation study of data ratio of optimal data and LLM planner data
    in the offline dataset. The mixing ratio is represented by $\mathbf{X}\%:\mathbf{Y}\%$
    denotes the percent of samples collected by the *optimal policy*.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：离线数据集中最佳数据和LLM规划器数据的数据比例的消融研究。混合比例表示为$\mathbf{X}\%:\mathbf{Y}\%$，表示由*最佳策略*收集的样本百分比。
- en: '|  | NQ | ES | SR |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | NQ | ES | SR |'
- en: '| ReAd-J(0%:100%) | 16.4±0.54 | 13.4±0.27 | 0.8±0.13 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-J(0%:100%) | 16.4±0.54 | 13.4±0.27 | 0.8±0.13 |'
- en: '| ReAd-J(50%:50%) | 15.8±1.12 | 13.9±0.35 | 0.6±0.16 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-J(50%:50%) | 15.8±1.12 | 13.9±0.35 | 0.6±0.16 |'
- en: '| ReAd-J(100%:0%) | 17.6±1.89 | 13.9±0.41 | 0.7±0.15 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-J(100%:0%) | 17.6±1.89 | 13.9±0.41 | 0.7±0.15 |'
- en: '| ReAd-S(0%:100%) | 31.4±1.11 | 14.0±0.26 | 0.8±0.13 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-S(0%:100%) | 31.4±1.11 | 14.0±0.26 | 0.8±0.13 |'
- en: '| ReAd-S(50%:50%) | 29.1±0.91 | 13.9±0.31 | 0.7±0.15 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-S(50%:50%) | 29.1±0.91 | 13.9±0.31 | 0.7±0.15 |'
- en: '| ReAd-S(100%:0%) | 34.2±2.18 | 14.3±0.30 | 0.5±0.17 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| ReAd-S(100%:0%) | 34.2±2.18 | 14.3±0.30 | 0.5±0.17 |'
- en: Critic Architecture.
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 批评家架构。
- en: The critic learns to estimate the value function of state-action pairs from
    the dataset. The state includes the environment state and the agent state, where
    the environment state contains variables of the simulator and the agent state
    is described by language. The action is also described by language. We adopt the
    pre-trained BERT Transformer model to extract language features of the agent state
    and actions. Then we concatenate the output feature with environment state features
    to some MLP layers to predict the $Q$-value. The structure of the critic network
    is given in Figure [12](#A5.F12 "Figure 12 ‣ Critic Architecture. ‣ E.4 Dataset
    and Critic Network ‣ Appendix E Additional Experimental Results ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration"), and the hyper-parameters
    are given in Table [7](#A5.T7 "Table 7 ‣ Critic Architecture. ‣ E.4 Dataset and
    Critic Network ‣ Appendix E Additional Experimental Results ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: Critic 学习从数据集中估计状态-动作对的价值函数。状态包括环境状态和代理状态，其中环境状态包含模拟器的变量，代理状态由语言描述。动作也由语言描述。我们采用预训练的
    BERT Transformer 模型来提取代理状态和动作的语言特征。然后，我们将输出特征与环境状态特征连接到一些 MLP 层中，以预测 $Q$ 值。Critic
    网络的结构如图 [12](#A5.F12 "Figure 12 ‣ Critic Architecture. ‣ E.4 Dataset and Critic
    Network ‣ Appendix E Additional Experimental Results ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration") 所示，超参数如表 [7](#A5.T7 "Table 7 ‣ Critic
    Architecture. ‣ E.4 Dataset and Critic Network ‣ Appendix E Additional Experimental
    Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    所示。
- en: '![Refer to caption](img/25d9691d33f025c5363377d3ef75a1b5.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/25d9691d33f025c5363377d3ef75a1b5.png)'
- en: 'Figure 12: In this figure, the parameters of BERT Transformer are fixed and
    will not be updated during the training of Critic.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：在此图中，BERT Transformer 的参数是固定的，并且在 Critic 训练过程中不会被更新。
- en: 'Table 7: The input dimensions for Critic of ReAd-J and ReAd-S are represented
    by JIS and SIS respectively, while HS represents the hidden layer input dimension,
    HN represents the number of hidden layers, LR is the learning rate, BS is batch
    size, TN represents the number of training iterations, SS is the dimension of
    environment state, and $n$ is the number of robots in the environment.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：ReAd-J 和 ReAd-S 的 Critic 输入维度分别由 JIS 和 SIS 表示，而 HS 表示隐藏层输入维度，HN 表示隐藏层的数量，LR
    是学习率，BS 是批量大小，TN 表示训练迭代次数，SS 是环境状态的维度，$n$ 是环境中的机器人数量。
- en: '|  | JIS | SIS | HS | HN | LR | BS | TN |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | JIS | SIS | HS | HN | LR | BS | TN |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| value | $768+$ |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| value | $768+$ |'
- en: Token Consumption.
  id: totrans-435
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Token 消耗。
- en: We report the details of token consumption on both benchmarks in Table [8](#A5.T8
    "Table 8 ‣ Critic Training. ‣ E.4 Dataset and Critic Network ‣ Appendix E Additional
    Experimental Results ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") and Table [9](#A5.T9 "Table 9 ‣ Critic Training. ‣ E.4 Dataset
    and Critic Network ‣ Appendix E Additional Experimental Results ‣ Towards Efficient
    LLM Grounding for Embodied Multi-Agent Collaboration") respectively. The total
    number of tokens consumed includes tokens consumed during pre-sampling data for
    training critic network. We utilize *LLM policy* to collect data for critic training
    in the experiment of *DV-RoCoBench*, while the data is collected by *expert policy*
    in the experiment of *Overcooked-AI*. Obviously, during the phase of planning,
    *ReAd-S* and *ReAd-J* consume less tokens than all other baselines. In terms of
    total consumed tokens, *ReAd-J* is comparable to the baselines which also generate
    joint plans in a parallel manner, and *ReAd-S* is significantly superior to RoCo.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [8](#A5.T8 "Table 8 ‣ Critic Training. ‣ E.4 Dataset and Critic Network
    ‣ Appendix E Additional Experimental Results ‣ Towards Efficient LLM Grounding
    for Embodied Multi-Agent Collaboration") 和表 [9](#A5.T9 "Table 9 ‣ Critic Training.
    ‣ E.4 Dataset and Critic Network ‣ Appendix E Additional Experimental Results
    ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration") 中报告了两个基准上的
    Token 消耗详细信息。总的 Token 消耗包括在预采样数据中为训练 Critic 网络而消耗的 Token。我们在 *DV-RoCoBench* 实验中使用
    *LLM policy* 来收集 Critic 训练数据，而在 *Overcooked-AI* 实验中则由 *expert policy* 收集数据。显然，在规划阶段，*ReAd-S*
    和 *ReAd-J* 消耗的 Token 少于所有其他基准。在总 Token 消耗方面，*ReAd-J* 与那些以并行方式生成联合计划的基准相当，而 *ReAd-S*
    显著优于 RoCo。
- en: Critic Training.
  id: totrans-437
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Critic 训练。
- en: The quantity of trajectories required for critic training depends on how challenging
    the task is. For 5 difficulty levels in *Sweep Floor*, critic training demands
    about 70, 120, 240, 600, and 1400 trajectories respectively. For 4 difficulty
    levels in *Make Sandwich*, about 60 trajectories are needed for critic training.
    For 5 difficulty levels in *Sort Cube*, critic training demands about 230, 240,
    300, 400 and 510 trajectories respectively. For *Cramped room* and *Forced coordination*,
    the number is about 128 and 2048 respectively. It is important to note that the
    volume of data utilized for critic training can be adjusted flexibly to align
    with the specific demands and challenges of the actual situation.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 批评训练所需的轨迹数量取决于任务的难度。在*Sweep Floor*的5个难度级别中，批评训练分别需要约70、120、240、600和1400条轨迹。在*Make
    Sandwich*的4个难度级别中，批评训练需要约60条轨迹。在*Sort Cube*的5个难度级别中，批评训练分别需要约230、240、300、400和510条轨迹。在*Cramped
    room*和*Forced coordination*中，分别需要约128和2048条轨迹。需要注意的是，批评训练使用的数据量可以灵活调整，以适应实际情况的具体需求和挑战。
- en: 'Table 8: Tokens consumed by all methods during the evaluation in *DV-RoCoBench*.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：在*DV-RoCoBench*评估期间所有方法消耗的令牌数量。
- en: '| Methods | ReAd-S | ReAd-J | RoCo | Central Plan | ReAct | Reflexion | MindAgent
    |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ReAd-S | ReAd-J | RoCo | Central Plan | ReAct | Reflexion | MindAgent
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Tokens for planning | 9M | 6M | 24M | 15M | 11M | 11M | 13M |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 规划令牌 | 9M | 6M | 24M | 15M | 11M | 11M | 13M |'
- en: '| Tokens for training $\hat{Q}$ | 7M | 7M | - | - | - | - | - |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: 训练令牌 $\hat{Q}$ | 7M | 7M | - | - | - | - | - |
- en: '| Total tokens | 16M | 13M | 24M | 15M | 11M | 11M | 13M |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 总令牌 | 16M | 13M | 24M | 15M | 11M | 11M | 13M |'
- en: 'Table 9: Tokens consumed by all methods during the evaluation in *Overcooked-AI*.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：在*Overcooked-AI*评估期间所有方法消耗的令牌数量。
- en: '| Methods | ReAd-J | Central Plan | ReAct | Reflexion | MindAgent |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ReAd-J | Central Plan | ReAct | Reflexion | MindAgent |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Tokens for planning | 1M | 2M | 4M | 3M | 4M |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: 规划令牌 | 1M | 2M | 4M | 3M | 4M |
- en: '| Tokens for training $\hat{Q}$ | - | - | - | - | - |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 训练令牌 $\hat{Q}$ | - | - | - | - | - |'
- en: '| Total tokens | 1M | 2M | 4M | 3M | 4M |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 总令牌 | 1M | 2M | 4M | 3M | 4M |'
- en: Appendix F Illustration of the Interaction Process
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 交互过程示意图
- en: we illustrate the distinctions between ReAd-S and RoCo by presenting a series
    of task execution screenshots. In Figure [13](#A6.F13 "Figure 13 ‣ Appendix F
    Illustration of the Interaction Process ‣ Towards Efficient LLM Grounding for
    Embodied Multi-Agent Collaboration") and Figure [14](#A6.F14 "Figure 14 ‣ Appendix
    F Illustration of the Interaction Process ‣ Towards Efficient LLM Grounding for
    Embodied Multi-Agent Collaboration"), we compare the screenshots of our method
    and RoCo algorithm in task *Sweep Floor* Y2_G2\. Our method can perform re-plan
    and correct the initial planning using advantage feedback, which results in a
    minimum number of environmental interactions. In contrast, RoCo which relies on
    physical feedback requires more negotiation and interactions with the environment.
    A similar comparison is shown in Figure [15](#A6.F15 "Figure 15 ‣ Appendix F Illustration
    of the Interaction Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") and Figure [16](#A6.F16 "Figure 16 ‣ Appendix F Illustration of
    the Interaction Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") for *Sort Cubes* sort4. A comparison between *ReAd-J* and Central
    Plan on *Forced Coordination* scenario is shown in Figure [17](#A6.F17 "Figure
    17 ‣ Appendix F Illustration of the Interaction Process ‣ Towards Efficient LLM
    Grounding for Embodied Multi-Agent Collaboration") and Figure [18](#A6.F18 "Figure
    18 ‣ Appendix F Illustration of the Interaction Process ‣ Towards Efficient LLM
    Grounding for Embodied Multi-Agent Collaboration").
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过展示一系列任务执行的截图来说明 ReAd-S 和 RoCo 之间的区别。在图 [13](#A6.F13 "Figure 13 ‣ Appendix
    F Illustration of the Interaction Process ‣ Towards Efficient LLM Grounding for
    Embodied Multi-Agent Collaboration") 和图 [14](#A6.F14 "Figure 14 ‣ Appendix F Illustration
    of the Interaction Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") 中，我们比较了我们的方法和 RoCo 算法在任务 *Sweep Floor* Y2_G2 中的截图。我们的方法可以通过优势反馈进行重新规划和修正初始规划，从而减少环境互动的次数。相比之下，依赖于物理反馈的
    RoCo 需要更多的环境协商和互动。在图 [15](#A6.F15 "Figure 15 ‣ Appendix F Illustration of the
    Interaction Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") 和图 [16](#A6.F16 "Figure 16 ‣ Appendix F Illustration of the Interaction
    Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    中展示了 *Sort Cubes* sort4 的类似比较。图 [17](#A6.F17 "Figure 17 ‣ Appendix F Illustration
    of the Interaction Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent
    Collaboration") 和图 [18](#A6.F18 "Figure 18 ‣ Appendix F Illustration of the Interaction
    Process ‣ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration")
    中展示了 *ReAd-J* 和 Central Plan 在 *Forced Coordination* 场景下的比较。
- en: '![Refer to caption](img/982ec166a4ddb1dad5d83e840475af8c.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/982ec166a4ddb1dad5d83e840475af8c.png)'
- en: 'Figure 13: Snapshots of the interaction process of *ReAd-J* in task *Sweep
    Floor* Y2_G2\. Our method obtains the minimum number of environmental interactions
    needed to complete the task.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：*ReAd-J* 在任务 *Sweep Floor* Y2_G2 中的互动过程快照。我们的方法获得了完成任务所需的最少环境互动次数。
- en: 'Figure 14: Snapshots of the interaction process of *RoCo* in task *Sweep Floor*
    Y2_G2\. The figure above shows that after planning and sweeping a cube into the
    dustpan, RoCo will dump it into the trash bin. However, after sweeping the last
    cube into the dustpan, instead of immediately planning to dump it to complete
    the task, LLM stubbornly believes that the task is done and plans to wait for
    the next two interactions.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：*RoCo* 在任务 *Sweep Floor* Y2_G2 中的互动过程快照。上图显示，在规划并将一个立方体扫入簸箕后，RoCo 会将其倒入垃圾桶。然而，在将最后一个立方体扫入簸箕后，LLM
    固执地认为任务已完成，并计划等待接下来的两个互动，而不是立即计划倒掉立方体以完成任务。
- en: '![Refer to caption](img/6753953a8244d96be35717b5a3f3c0b1.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6753953a8244d96be35717b5a3f3c0b1.png)'
- en: 'Figure 15: Snapshots of the interaction process of *ReAd-S* in task *Sort Cubes*
    sort4\. This task is challenging and requires the collaboration of three robots
    and takes a minimum of three steps to complete. Our approach efficiently accomplishes
    this task with minimal environment interactions.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：*ReAd-S* 在任务 *Sort Cubes* sort4 中的互动过程快照。该任务具有挑战性，需要三台机器人协作，至少需要三步才能完成。我们的方法在最小环境互动下高效完成了该任务。
- en: '![Refer to caption](img/a842b81ffa6c3b0a441d69d67b08d8e3.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a842b81ffa6c3b0a441d69d67b08d8e3.png)'
- en: 'Figure 16: Snapshots of the interaction process of *RoCo* in task *Sort Cubes*
    sort4\. Before the joint actions of all robots are executed, the planning result
    can only be improved through the dialogue of LLMs. In addition, environmental
    feedback can be generated only after the agent interacts with the simulator. In
    contrast, our advantage feedback provides timely feedback in the process of LLM
    planning for policy improvement before interaction.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：*排序立方体*任务中*RoCo*的互动过程快照。在所有机器人执行联合行动之前，规划结果只能通过LLM的对话来改进。此外，环境反馈只有在代理与模拟器互动后才会生成。相比之下，我们的优势反馈在LLM规划政策改进过程中提供及时反馈。
- en: '![Refer to caption](img/da9d2d7282557db92dee2f2b374633b7.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/da9d2d7282557db92dee2f2b374633b7.png)'
- en: 'Figure 17: Snapshots of the interaction process of *ReAd-J* in task *Forced
    Coordination*. This task is challenging and requires the collaboration of two
    agents and takes a minimum of 22 steps to complete. Most of the time, ReAd can
    improve the unreasonable planning result generated by LLM, so that *ReAd-J* can
    complete the task smoothly. However, due to the out-of-distribution (OOD), it
    is possible to evaluate the advantage value of some unreasonable planning to carry
    out environmental interaction'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：*强制协调*任务中*ReAd-J*的互动过程快照。该任务具有挑战性，需要两个代理的协作，并且需要至少22步才能完成。大多数时候，ReAd可以改进LLM生成的不合理规划结果，从而使*ReAd-J*能够顺利完成任务。然而，由于分布外（OOD），可能需要评估某些不合理规划的优势值，以进行环境互动。
- en: '![Refer to caption](img/7787642d81020475a144c977661be6af.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7787642d81020475a144c977661be6af.png)'
- en: 'Figure 18: Snapshots of the interaction process of *Central Plan* in task *Forced
    Coordination*. From the screenshot of the interaction process, it can be found
    that in the *Forced Coordination*, it is difficult for LLM to understand the state
    of the current environment, so hallucination occurs frequently, resulting in the
    failure of *Central Plan* to effectively complete the task.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：*强制协调*任务中*中央计划*的互动过程快照。从互动过程的截图可以发现，在*强制协调*任务中，LLM很难理解当前环境的状态，因此出现了频繁的幻觉，导致*中央计划*未能有效完成任务。
- en: Appendix G Examples of Reinforced Advantage Feedback
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 强化优势反馈示例
- en: Format of Advantage Feedback. The Advantage Feedback comprises of [Evaluation
    Score] and [Improvement Feedback]. If the advantage score of the action is higher
    than a threshold, the response will only contain [Evaluation Score]. Otherwise,
    we perform a re-plan to improve the previous action, and the response will include
    both [Evaluation Score] and [Improvement Feedback].
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 优势反馈格式。优势反馈包括[评估得分]和[改进反馈]。如果行动的优势得分高于某个阈值，响应将仅包含[评估得分]。否则，我们将进行重新规划以改进之前的行动，响应将包括[评估得分]和[改进反馈]。
- en: Below we will briefly show a few examples of the LLM’s planning corrected by
    advantage feedback, using ReAd-S to complete the Sweep Floor task.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将简要展示几个由优势反馈纠正的LLM规划示例，使用ReAd-S完成扫地任务。
- en: '[Alice Response]:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '[Alice Response]：'
- en: Bob, we have green_cube_1 and green_cube_2 in the dustpan. Let’s MOVE to the
    trash_bin so I can DUMP these cubes. After that, we can go for any remaining cubes.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: Bob，我们在簸箕中有green_cube_1和green_cube_2。我们去trash_bin，这样我可以将这些立方体丢弃。之后，我们可以去处理任何剩余的立方体。
- en: PROCEED
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Alice ACTION MOVE trash_bin.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: NAME Alice ACTION MOVE trash_bin。
- en: '[Advantage Feedback]:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '[优势反馈]：'
- en: '[Evaluation Score]:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '[评估得分]：'
- en: The advantage score of Alice’s action is -0.6652!
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: Alice的行动优势得分为-0.6652！
- en: The advantage score is low and the plan need to be changed!
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 优势得分较低，计划需要更改！
- en: '[Improve Feedback]'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '[改进反馈]'
- en: 'Alice’s previous response:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: Alice之前的响应：
- en: Bob, we have green_cube_1 and green_cube_2 in the dustpan. Let’s MOVE to the
    trash_bin so I can DUMP these cubes. After that, we can go for any remaining cubes.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Bob，我们在簸箕中有green_cube_1和green_cube_2。我们去trash_bin，这样我可以将这些立方体丢弃。之后，我们可以去处理任何剩余的立方体。
- en: PROCEED
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Alice ACTION MOVE trash_bin
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: NAME Alice ACTION MOVE trash_bin
- en: 'The advantage score is -0.665, it is too low to run. The answer may contain
    a misunderstanding of the state of the environment, you must change your action
    to raise the advantage score over 0! [Alice Response]:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 优势得分为-0.665，过低，无法继续执行。答案可能包含对环境状态的误解，你必须更改你的行动，以提高优势得分至0以上！[Alice Response]：
- en: Bob, I made a mistake in my previous plan. Since we already have green_cube_1
    and green_cube_2 in the dustpan, I should DUMP them into the trash_bin. I’ll do
    that while you WAIT.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: Bob，我在之前的计划中犯了一个错误。由于我们已经在扫帚上有绿色方块_1 和绿色方块_2，我应该把它们扔进垃圾桶。我会在你等待的时候做到这一点。
- en: PROCEED
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Alice ACTION DUMP
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 Alice 动作 DUMP
- en: '[Advantage Feedback]:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '[优势反馈]：'
- en: '[Evaluation Score]:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '[评估得分]：'
- en: The advantage score of Alice’s action is 17.376!
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: Alice 的动作优势得分是 17.376！
- en: The advantage score is high enough.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 优势得分足够高。
- en: Explain. In this example, the MOVE action can only be used for cubes due to
    the task setting, and the MOVE of the trash bin is illegal. However, the LLMs
    learned from the web may consider the MOVE action can also be applied to the trash
    bin, which does not ground well with the specific task, resulting in the fact
    of hallucination. Our method can correct this type of error with advantage feedback
    according to the reward since taking MOVE action to the trash bin will obtain
    a zero reward and lead to the same next state as the current state.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 解释。在这个例子中，MOVE 动作只能用于方块，由于任务设置，垃圾桶的 MOVE 是非法的。然而，来自网络的 LLM 可能认为 MOVE 动作也可以应用于垃圾桶，这与具体任务不符，导致了幻觉。我们的方法可以通过优势反馈根据奖励来纠正这种错误，因为将
    MOVE 动作应用于垃圾桶将获得零奖励，并导致与当前状态相同的下一个状态。
- en: '[Alice Response]:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '[Alice 回复]：'
- en: Bob, we have green_cube_1 and green_cube_2 in the dustpan. Let’s MOVE to the
    next cube we need to sweep. I suggest we go to pink_cube_1 since it’s the closest
    cube on the table. Once we’re in position, I will WAIT for you to SWEEP it.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: Bob，我们在扫帚上有绿色方块_1 和绿色方块_2。我们移动到下一个需要清扫的方块。我建议我们去粉色方块_1，因为它是桌子上最靠近的方块。一旦我们到位，我会等待你去清扫它。
- en: PROCEED
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Alice ACTION MOVE pink_cube_1
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 Alice 动作 MOVE pink_cube_1
- en: '[Advantage Feedback]:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '[优势反馈]：'
- en: '[Evaluation Score]:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '[评估得分]：'
- en: The advantage score of Alice’s action is -0.0927!
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: Alice 的动作优势得分是 -0.0927！
- en: The advantage score is low and the plan needs to be changed!
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 优势得分较低，计划需要更改！
- en: '[Improve Feedback]'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '[改进反馈]'
- en: 'Alice’s previous response:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: Alice 的之前回应：
- en: Bob, we have green_cube_1 and green_cube_2 in the dustpan. Let’s MOVE to the
    next cube we need to sweep. I suggest we go to pink_cube_1 since it’s the closest
    cube on the table. Once we’re in position, I will WAIT for you to SWEEP it.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: Bob，我们在扫帚上有绿色方块_1 和绿色方块_2。我们移动到下一个需要清扫的方块。我建议我们去粉色方块_1，因为它是桌子上最靠近的方块。一旦我们到位，我会等待你去清扫它。
- en: PROCEED
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Alice ACTION MOVE pink_cube_1 The advantage score is -0.0927, it is too
    low to run. The answer may contain a misunderstanding of the state of the environment,
    you must change your action to raise the advantage score over 0!
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 Alice 动作 MOVE pink_cube_1 优势得分是 -0.0927，这太低，无法执行。答案可能对环境状态有误解，你必须更改你的动作，将优势得分提高到
    0 以上！
- en: '[Alice Response]:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '[Alice 回复]：'
- en: Bob, I made a mistake in my previous suggestion. We should not move to pink_cube_1
    as it’s not one of our targets. Instead, let’s DUMP the cubes we have in the dustpan
    into the trash_bin. I will do that while you WAIT.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: Bob，我在之前的建议中犯了一个错误。我们不应该移动到粉色方块_1，因为它不是我们的目标。相反，让我们把扫帚上的方块扔进垃圾桶。我会在你等待的时候做到这一点。
- en: PROCEED
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Alice ACTION DUMP
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 Alice 动作 DUMP
- en: '[Advantage Feedback]:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[优势反馈]：'
- en: '[Evaluation Score]:'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '[评估得分]：'
- en: The advantage score of Alice’s action is 18.4184!
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: Alice 的动作优势得分是 18.4184！
- en: The advantage score is high enough.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 优势得分足够高。
- en: Explain. In this example, the target colors to sweep are yellow and green. Since
    the environment also includes many cubes with other colors, LLM may plan to sweep
    cubes with the wrong colors initially. In such cases, the advantage feedback is
    helpful to correct the plan generated by the LLM.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 解释。在这个例子中，目标颜色是黄色和绿色。由于环境中还包括许多其他颜色的方块，LLM 可能最初计划清扫错误颜色的方块。在这种情况下，优势反馈有助于纠正
    LLM 生成的计划。
- en: '[Bob Response]:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '[Bob 回复]'
- en: Alice, I will WAIT as you move into position for yellow_cube_3.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: Alice，我会等待你移动到黄色方块_3 的位置。
- en: PROCEED
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Bob ACTION WAIT
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 Bob 动作 WAIT
- en: '[Advantage Feedback]:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '[优势反馈]：'
- en: '[Evaluation Score]:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '[评估得分]：'
- en: The advantage score of Bob’s action is -3.2211!
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: Bob 的动作优势得分是 -3.2211！
- en: The score is low and the plan needs to be changed!
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 得分较低，计划需要更改！
- en: '[Improve Feedback] Bob’s previous response:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '[改进反馈] Bob 的之前回应：'
- en: Alice, I will WAIT as you move into position for yellow_cube_3.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: Alice，我会等待你移动到黄色方块_3 的位置。
- en: PROCEED
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Bob ACTION WAIT
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名 Bob 动作 WAIT
- en: 'The advantage score score is -3.221, it is too low to run, the answer may contain
    a misunderstanding of the state of the environment, you must change your action
    to raise the advantage score over -0.2! [Bob Response]:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 优势分数为 -3.221，分数过低，无法运行，答案可能包含对环境状态的误解，你必须更改你的动作将优势分数提高到 -0.2 以上！[Bob 响应]：
- en: Alice, since you are moving to yellow_cube_3, I will also MOVE to yellow_cube_3
    to be ready to SWEEP it once you are in position.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Alice，由于你将移动到 yellow_cube_3，我也会移动到 yellow_cube_3，以便在你到位后准备*清扫*。
- en: PROCEED
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 继续
- en: NAME Bob ACTION MOVE yellow_cube_3
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: NAME Bob ACTION MOVE yellow_cube_3
- en: '[Advantage Feedback]:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '[优势反馈]：'
- en: '[Evaluation Score]:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '[评价分数]：'
- en: The advantage score of Bob’s action is 0.6383!
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: Bob 的动作优势分数为 0.6383！
- en: The advantage score is high enough.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 优势分数足够高。
- en: Explain. In this example, the LLM plans for Bob to move to yellow_cube_3 with
    Alice, it can sweep directly into the dustpan in the next step. However, during
    the first LLM planning, the LLM plans the WAIT action for Bob. If Bob performs
    this action at this time, Alice can only wait for Bob to move to yellow_cube_3\.
    Thus, our method performs replanning based on the advantage feedback to reduces
    the interaction steps to the environment.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 解释。在这个例子中，LLM 计划 Bob 与 Alice 一起移动到 yellow_cube_3，这样在下一步就可以直接将灰尘扫进簸箕。然而，在第一次
    LLM 规划时，LLM 为 Bob 计划了 WAIT 动作。如果 Bob 在此时执行这个动作，Alice 只能等待 Bob 移动到 yellow_cube_3。因此，我们的方法基于优势反馈进行重新规划，以减少与环境的交互步骤。
