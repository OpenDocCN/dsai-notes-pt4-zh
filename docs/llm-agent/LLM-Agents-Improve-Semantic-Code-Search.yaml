- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:38:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:38:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM Agents Improve Semantic Code Search
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 代理提升语义代码搜索
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11058](https://ar5iv.labs.arxiv.org/html/2408.11058)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.11058](https://ar5iv.labs.arxiv.org/html/2408.11058)
- en: Sarthak Jain
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 萨尔塔克·贾恩
- en: Cisco
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 思科
- en: University of Illinois Urbana Champaign
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 伊利诺伊大学厄本那-香槟分校
- en: Urbana, Illinois
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 厄本那，伊利诺伊州
- en: sj84@illinois.edu, sarjain2@cisco.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: sj84@illinois.edu, sarjain2@cisco.com
- en: '&Aditya Dora'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&阿迪提亚·多拉'
- en: University of Illinois Urbana Champaign
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 伊利诺伊大学厄本那-香槟分校
- en: Urbana, Illinois
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 厄本那，伊利诺伊州
- en: adora2@illinois.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: adora2@illinois.edu
- en: '&Ka Seng Sam'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '&卡·成·萨姆'
- en: University of Illinois Urbana Champaign
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 伊利诺伊大学厄本那-香槟分校
- en: Urbana, Illinois
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 厄本那，伊利诺伊州
- en: samsam2@illinois.edu &Prabhat Singh
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: samsam2@illinois.edu &普拉巴特·辛格
- en: Cisco
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 思科
- en: San Jose, California
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 圣荷西，加利福尼亚
- en: prabhat7@cisco.com
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: prabhat7@cisco.com
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Code Search is a key task that many programmers often have to perform while
    developing solutions to problems. Current methodologies suffer from an inability
    to perform accurately on prompts that contain some ambiguity or ones that require
    additional context relative to a code-base. We introduce the approach of using
    Retrieval Augmented Generation (RAG) powered agents to inject information into
    user prompts allowing for better inputs into embedding models. By utilizing RAG,
    agents enhance user queries with relevant details from GitHub repositories, making
    them more informative and contextually aligned. Additionally, we introduce a multi-stream
    ensemble approach which when paired with agentic workflow can obtain improved
    retrieval accuracy, which we deploy on application called repo-rift.com. Experimental
    results on the CodeSearchNet dataset demonstrate that RepoRift significantly outperforms
    existing methods, achieving an 78.2% success rate at Success@10 and a 34.6% success
    rate at Success@1\. This research presents a substantial advancement in semantic
    code search, highlighting the potential of agentic LLMs and RAG to enhance code
    retrieval systems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代码搜索是许多程序员在开发解决方案时经常需要执行的关键任务。当前的方法在处理包含一些模糊性的提示或需要相对于代码库的额外上下文时存在准确性不足的问题。我们介绍了一种使用检索增强生成（RAG）驱动的代理来向用户提示中注入信息的方法，从而提供更好的嵌入模型输入。通过利用
    RAG，代理可以从 GitHub 仓库中增强用户查询，增加相关细节，使其更具信息性和上下文一致性。此外，我们引入了一种多流集成方法，与代理工作流配对时可以获得改进的检索准确性，我们将其部署在应用程序
    repo-rift.com 上。对 CodeSearchNet 数据集的实验结果表明，RepoRift 显著优于现有方法，在 Success@10 的成功率为78.2%，在
    Success@1 的成功率为34.6%。这项研究在语义代码搜索方面取得了重要进展，突显了代理 LLM 和 RAG 在提升代码检索系统方面的潜力。
- en: 1 Introduction
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'A key task that many programmers often perform is searching through codebases
    to find snippets that can solve specific problems. This practice coined as code
    search is essential for facilitating code reuse [[1](#bib.bib1)]. While traditional
    code search involves the usage of keyword matching, code search has evolved to
    learn and predict on the semantics behind queries and snippets allowing programmers
    to more accurately retrieve code that aligns with their intent. Recent advances
    in deep learning have been at the center of current methodologies. Through training
    large language models (LLM) on large corpora of text and code, LLMs have obtained
    strong natural language to code generation capabilities which has extended to
    better semantic code search. Notable research in this domain includes "Deep Code
    Search" [[2](#bib.bib2)], which utilizes recurrent neural networks, to learn sequential
    information behind code and their descriptions and consequently map them into
    a unified vector space. Building upon DeepCS, other architectures like Carl-CS
    which exploits co-attentive representation learning [[3](#bib.bib3)] and PSCS
    [[4](#bib.bib4)] which focuses on using code flow obtained from Abstract Syntax
    Trees also improved code search capabilities. Other significant work in this domain
    is "CodeBERT: A Pre-Trained Model for Programming and Natural Languages" [[5](#bib.bib5)],
    which leverages a bi-modal transformer to jointly model programming and natural
    languages, significantly enhancing the model’s ability to form accurate embeddings
    based on semantic content. Building on this approach, the paper "Text and Code
    Embeddings by Contrastive Pre-Training" by OpenAI [[6](#bib.bib6)] introduces
    a contrastive learning technique paired with unprecedented large training data
    to generate state of the art embeddings for both text and code, further enhancing
    the ability (even above CodeBERT and variations like GraphCodeBERT [[7](#bib.bib7)])
    to match natural language queries with relevant code snippets by distinguishing
    subtle differences in meaning across various contexts.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '许多程序员经常执行的一项关键任务是搜索代码库，以找到能够解决特定问题的代码片段。这种实践被称为代码搜索，对于促进代码重用至关重要[[1](#bib.bib1)]。传统的代码搜索通常依赖于关键词匹配，而代码搜索已经发展到能够学习和预测查询和代码片段背后的语义，从而使程序员能够更准确地检索与其意图一致的代码。最近，深度学习的进展处于当前方法论的核心。通过对大量文本和代码进行训练，**大型语言模型（LLM）**获得了强大的自然语言到代码生成能力，这也延伸到了更好的语义代码搜索。该领域的显著研究包括"Deep
    Code Search"[[2](#bib.bib2)]，它利用递归神经网络学习代码及其描述背后的序列信息，并将其映射到统一的向量空间。基于DeepCS的其他架构如Carl-CS，利用共同注意的表示学习[[3](#bib.bib3)]，以及PSCS[[4](#bib.bib4)]，专注于使用从抽象语法树获得的代码流，也改善了代码搜索能力。该领域的其他重要工作是"CodeBERT:
    A Pre-Trained Model for Programming and Natural Languages"[[5](#bib.bib5)]，它利用双模态变换器来共同建模编程语言和自然语言，显著增强了模型基于语义内容形成准确嵌入的能力。基于这种方法，OpenAI的论文"Text
    and Code Embeddings by Contrastive Pre-Training"[[6](#bib.bib6)]引入了一种对比学习技术，并配以前所未有的大规模训练数据，以生成最先进的文本和代码嵌入，进一步提高了匹配自然语言查询与相关代码片段的能力（甚至超越了CodeBERT及其变体如GraphCodeBERT[[7](#bib.bib7)]），通过区分各种上下文中的细微意义差异。'
- en: Despite these advancements, semantic code search still faces many challenges.
    Natural language queries provided by a user can be ambiguous or requiring more
    detail. One example of this is the Vocabulary Mismatch Problem where different
    individuals use varying keywords or terms to describe the same concept or functionality,
    or the same keywords to describe different functionalities. For instance, the
    term "model" can refer to a machine learning model, a database schema, or a software
    design pattern [[8](#bib.bib8)]. Even in the field of Artificial Intelligence,
    for example, the keyword "Positional Encoding" has a different context and purpose
    when referring to attention mechanisms in transformers [[9](#bib.bib9)] compared
    to its use in neural radiance fields [[10](#bib.bib10)]. This issue can lead to
    weak code search results or force a user to do extra work to provide additional
    details in their input prompt.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些进展，语义代码搜索仍面临许多挑战。用户提供的自然语言查询可能是模糊的或需要更多细节。例如，词汇不匹配问题就是其中之一，其中不同的人使用不同的关键字或术语来描述相同的概念或功能，或者使用相同的关键字来描述不同的功能。例如，术语“model”可以指代机器学习模型、数据库模式或软件设计模式[[8](#bib.bib8)]。即使在人工智能领域，例如，“Positional
    Encoding”这个关键词在提及transformer中的注意机制时具有不同的背景和目的[[9](#bib.bib9)]，与其在神经辐射场中的使用[[10](#bib.bib10)]相比。这一问题可能导致较弱的代码搜索结果，或迫使用户提供额外的细节以完善其输入提示。
- en: In this paper, we propose using agentic large language models (LLMs) to improve
    semantic code search. Agentic LLMs involve multiple specialized agents working
    collaboratively to handle different aspects of a task. Using agents lead to more
    powerful capabilities than single LLMS due to their augmented reasoning and ability
    to decision make [[11](#bib.bib11)]. In the context of semantic code search, these
    agents are designed to append useful information to the user prompt. By using
    Retrieval Augmented Generation (RAG), the system looks up information on the internet
    pertaining to a specific GitHub repository to understand its context. This allows
    agents to recursively call prompts, injecting relevant information into a user’s
    natural language query and effectively adding enough detail to eliminate the Vocabulary
    Mismatch Problem. Therefore, compared to previous research that focuses on improving
    mappings between natural language and code specifically, we focused on augmenting
    the user prompt via RAG powered agents. Through our results, we have shown how
    such augmentations trickle down and improve the performance of already created
    embedding based methods. We utilize OpenAI’s state-of-the-art text embeddings
    as they currently have the strongest performance in prominent code search evaluation
    sets like CodeSearchNet. [[6](#bib.bib6)]. Additionally, we translate the natural
    language output of agents into code to improve code search. The purpose of this
    is to bridge the semantic gap between human-readable natural language queries
    and code snippets in order to improve search accuracy and relevance in code search
    engines [[8](#bib.bib8)]. To maximize the accuracy of our code search results,
    we implement an ensemble approach. This approach involves conducting multiple
    comparisons to identify the most relevant code snippets.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们建议使用代理型大型语言模型（LLMs）来改进语义代码搜索。代理型LLMs涉及多个专业化的代理协同工作，以处理任务的不同方面。使用代理比单一的LLMs具有更强大的能力，因为它们增强了推理能力和决策能力[[11](#bib.bib11)]。在语义代码搜索的背景下，这些代理被设计为向用户提示添加有用的信息。通过使用检索增强生成（RAG），系统查找与特定GitHub库相关的信息以理解其背景。这使得代理能够递归调用提示，将相关信息注入到用户的自然语言查询中，并有效地添加足够的细节以消除词汇不匹配问题。因此，与专门关注改进自然语言与代码之间映射的先前研究相比，我们专注于通过RAG驱动的代理来增强用户提示。通过我们的结果，我们展示了这种增强如何渗透并改善已经创建的基于嵌入的方法的性能。我们利用OpenAI的最先进文本嵌入，因为它们在像CodeSearchNet这样的显著代码搜索评估集上表现最强[[6](#bib.bib6)]。此外，我们将代理的自然语言输出转换为代码以改进代码搜索。其目的是弥合人类可读的自然语言查询与代码片段之间的语义差距，以提高代码搜索引擎的搜索准确性和相关性[[8](#bib.bib8)]。为了最大化代码搜索结果的准确性，我们实施了一种集成方法。这种方法涉及进行多次比较，以识别最相关的代码片段。
- en: Additionally, we have built an online website, [RepoRift](https://repo-rift.com/),
    which implements these advanced code search techniques delineated in the paper.
    This platform allows for any user to enter in a github repository and ask their
    own natural language queries for code search. For more details, visit [www.repo-rift.com](https://repo-rift.com/).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们建立了一个在线网站，[RepoRift](https://repo-rift.com/)，实现了论文中详细描述的高级代码搜索技术。这个平台允许任何用户输入一个github仓库，并用自然语言查询代码搜索。更多详情，请访问
    [www.repo-rift.com](https://repo-rift.com/)。
- en: 'To summarize, our main contributions are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献包括：
- en: '1\. Information Injection via Agentic Large Language Models (LLMs) and Retrieval
    Augmented Generation (RAG): We use agents with RAG internet search capabilities
    to augment user prompts to break down technical terms, contain more specific information,
    and alleviate the Vocabulary Mismatch Problem. Moreover, we have shown how such
    a strategy leads to better inputs for embedding models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 信息注入通过代理大语言模型（LLMs）和检索增强生成（RAG）：我们使用具有RAG互联网搜索功能的代理来增强用户提示，以分解技术术语，包含更具体的信息，并缓解词汇不匹配问题。此外，我们展示了这种策略如何为嵌入模型提供更好的输入。
- en: '2\. Ensemble Architecture with Multi-Stream Comparisons: We utilize OpenAI’s
    state-of-the-art text embeddings to capture nuanced meanings, translating natural
    language queries into code and using an ensemble approach with multi-stream comparisons.
    This method enhances the accuracy and relevance of retrieved code snippets by
    examining multiple facets of the query and code context.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 集成架构与多流比较：我们利用OpenAI的最先进文本嵌入技术捕捉细微的含义，将自然语言查询转换为代码，并采用集成方法进行多流比较。这种方法通过检查查询和代码上下文的多个方面来提高检索代码片段的准确性和相关性。
- en: '3\. RepoRift Platform: We developed [www.repo-rift.com](https://repo-rift.com/),
    an online platform that implements these advanced techniques, providing developers
    with a practical tool for code searches. Powered by the architecture discussed
    in this paper, RepoRift offers a novel solution in 3 ways: (1.) It narrows down
    the context of a query to a single repository, (2.) It uses agentic interactions
    to hone accuracy and efficacy, and (3.) It returns easy-to-read results in a timely
    manner. Visit [www.repo-rift.com](https://repo-rift.com/) for more details. Currently
    only Python is supported.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 3. RepoRift平台：我们开发了 [www.repo-rift.com](https://repo-rift.com/)，这是一个实现这些高级技术的在线平台，为开发者提供了一个实用的代码搜索工具。RepoRift由本文讨论的架构提供支持，提供了三种新颖的解决方案：（1.）将查询的上下文缩小到一个仓库，（2.）使用代理互动来提高准确性和效率，（3.）及时返回易于阅读的结果。更多详情请访问
    [www.repo-rift.com](https://repo-rift.com/)。目前仅支持Python。
- en: 2 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 2.1 Information Injection via Agentic Large Language Models (LLMs) and Retrieval
    Augmented Generation (RAG)
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 通过代理大语言模型（LLMs）和检索增强生成（RAG）进行信息注入
- en: Given a natural language query $Q$ with additional details, thereby improving
    the match between the user input and the correct code snippet.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个自然语言查询$Q$及额外细节，从而提高用户输入与正确代码片段之间的匹配度。
- en: 'Our agent architecture, built using the CrewAI framework on top of OpenAI’s
    GPT-4 model, functions as a "Technical Research Writer." The agent augments the
    query based on the prompt:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理架构，使用CrewAI框架在OpenAI的GPT-4模型之上构建，充当“技术研究写作员”。该代理根据提示增强查询：
- en: '"Given an input text prompt: [$Q$]. If you can’t find how it is implemented
    in the repository, then provide information on how it is implemented generally.
    Ensure that you are not given more info than necessary and only give info on specifically
    the topics present in the input text prompt. Your paragraph will help localize
    the ideas in the input text prompt in a large repository so deviating from topic
    can lead to inaccuracies down the pipeline. You are on a timer be quick, so you
    must be called two times at most and look at one website at most each time called".'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '"给定一个输入文本提示：[$Q$]。如果你找不到它在仓库中的实现方式，那么提供一般的实现信息。确保你提供的信息不超过必要的内容，并且仅针对输入文本提示中的具体主题进行信息提供。你的段落将帮助在大型仓库中本地化输入文本提示中的想法，因此偏离主题可能导致后续的不准确。你有时间限制，所以你最多只能被调用两次，每次最多只能查看一个网站。"'
- en: This approach ensures the augmentation is relevant and focused on the topics
    present in the query.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法确保了增强内容与查询中的主题相关且集中。
- en: We employ a retrieval-augmented generation (RAG) technique, where information
    is first gathered from the internet. The retrieved information, determined to
    be relevant based on embedding cosine similarity, is then used to augment the
    query [[12](#bib.bib12)]. The output of the agent post-retrieval is the augmented
    prompt.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了一种增强检索生成（RAG）技术，其中首先从互联网收集信息。检索到的信息通过嵌入余弦相似度确定其相关性，然后用于增强查询 [[12](#bib.bib12)]。检索后代理的输出是增强的提示。
- en: '|  | $A=\text{Agent}(\text{Retrieval}(Q,D))$ |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $A=\text{Agent}(\text{Retrieval}(Q,D))$ |  | (1) |'
- en: '![Refer to caption](img/7d0e9f95263cb67a4862d8163dd65603.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7d0e9f95263cb67a4862d8163dd65603.png)'
- en: 'Figure 1: An example showing the idea of how a natural langauge query taken
    fron CodeSearchNet [[13](#bib.bib13)] is augmented by Agents allowing for better
    matching.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个示例展示了如何将从CodeSearchNet [[13](#bib.bib13)] 中获取的自然语言查询通过代理进行增强，从而实现更好的匹配。
- en: 2.2 Ensemble Architecture with Multi-Stream Comparisons
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多流比较的集成架构
- en: 'The purpose of the ensemble architecture is through many different comparisons,
    a more accurate final set of likely snippets can be formed. Additionally, during
    code generation, classes can be created with many different functions, and a multi-stream
    architecture that breaks down the generated code is needed. Once $A$ are added
    to the final target set:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 集成架构的目的是通过多次不同的比较，形成更准确的最终可能片段。此外，在代码生成期间，可以创建具有多种不同功能的类，需要一种多流架构来分解生成的代码。一旦$A$被添加到最终目标集中：
- en: '|  | $\text{Embedding}(A)=\mathbf{e}_{A}$ |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{嵌入}(A)=\mathbf{e}_{A}$ |  |'
- en: '|  | $\mathbf{e}_{Y}=\{\mathbf{e}_{Y_{1}},\mathbf{e}_{Y_{2}},\ldots,\mathbf{e}_{Y_{n}}\}$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{e}_{Y}=\{\mathbf{e}_{Y_{1}},\mathbf{e}_{Y_{2}},\ldots,\mathbf{e}_{Y_{n}}\}$
    |  |'
- en: '|  | $\text{Cosine Similarity}(\mathbf{e}_{Q},\mathbf{e}_{Y_{i}})=\frac{\mathbf{e}_{Q}\cdot\mathbf{e}_{Y_{i}}}{\&#124;\mathbf{e}_{Q}\&#124;\&#124;\mathbf{e}_{Y_{i}}\&#124;}$
    |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{余弦相似度}(\mathbf{e}_{Q},\mathbf{e}_{Y_{i}})=\frac{\mathbf{e}_{Q}\cdot\mathbf{e}_{Y_{i}}}{\|\mathbf{e}_{Q}\|\|\mathbf{e}_{Y_{i}}\|}$
    |  |'
- en: '|  | $\text{Top 3 in }Y=\{Y_{i_{1}},Y_{i_{2}},Y_{i_{3}}\}$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{前3名在 }Y=\{Y_{i_{1}},Y_{i_{2}},Y_{i_{3}}\}$ |  |'
- en: The second stream processes involves generation of code $A$ and then evaluate
    its quality.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个流处理涉及生成代码$A$并评估其质量。
- en: '|  | $\text{GPT-3.5-turbo}(A)\rightarrow C$ |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{GPT-3.5-turbo}(A)\rightarrow C$ |  |'
- en: $C$
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: $C$
- en: '|  | $\text{Embedding}(C)=\mathbf{e}_{C}$ |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{嵌入}(C)=\mathbf{e}_{C}$ |  |'
- en: '|  | $\mathbf{e}_{Y}=\{\mathbf{e}_{Y_{1}},\mathbf{e}_{Y_{2}},\ldots,\mathbf{e}_{Y_{m}}\}$
    |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{e}_{Y}=\{\mathbf{e}_{Y_{1}},\mathbf{e}_{Y_{2}},\ldots,\mathbf{e}_{Y_{m}}\}$
    |  |'
- en: '|  | $\mathbf{e}_{Z}=\{\mathbf{e}_{Z_{1}},\mathbf{e}_{Z_{2}},\ldots,\mathbf{e}_{Z_{k}}\}$
    |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{e}_{Z}=\{\mathbf{e}_{Z_{1}},\mathbf{e}_{Z_{2}},\ldots,\mathbf{e}_{Z_{k}}\}$
    |  |'
- en: '|  | $\text{Cosine Similarity}(\mathbf{e}_{C},\mathbf{e}_{Y_{i}})=\frac{\mathbf{e}_{C}\cdot\mathbf{e}_{Y_{i}}}{\&#124;\mathbf{e}_{C}\&#124;\&#124;\mathbf{e}_{Y_{i}}\&#124;}$
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{余弦相似度}(\mathbf{e}_{C},\mathbf{e}_{Y_{i}})=\frac{\mathbf{e}_{C}\cdot\mathbf{e}_{Y_{i}}}{\|\mathbf{e}_{C}\|\|\mathbf{e}_{Y_{i}}\|}$
    |  |'
- en: '|  | $\text{Cosine Similarity}(\mathbf{e}_{C},\mathbf{e}_{Z_{i}})=\frac{\mathbf{e}_{C}\cdot\mathbf{e}_{Z_{i}}}{\&#124;\mathbf{e}_{C}\&#124;\&#124;\mathbf{e}_{Z_{i}}\&#124;}$
    |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{余弦相似度}(\mathbf{e}_{C},\mathbf{e}_{Z_{i}})=\frac{\mathbf{e}_{C}\cdot\mathbf{e}_{Z_{i}}}{\|\mathbf{e}_{C}\|\|\mathbf{e}_{Z_{i}}\|}$
    |  |'
- en: 'The top three snippets in both $Y$ are compiled and added to the final target
    set:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: $Y$中的前三个片段被编译并添加到最终目标集中：
- en: '|  | $\text{Top 3 in }Y=\{Y_{i_{4}},Y_{i_{5}},Y_{i_{6}}\}$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{前3名在 }Y=\{Y_{i_{4}},Y_{i_{5}},Y_{i_{6}}\}$ |  |'
- en: '|  | $\text{Top 3 in }Z=\{Z_{i_{1}},Z_{i_{2}},Z_{i_{3}}\}$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{前3名在 }Z=\{Z_{i_{1}},Z_{i_{2}},Z_{i_{3}}\}$ |  |'
- en: The final stream involves a comparison of component functions. $C$, and the
    smallest cosine similarity distance for each component function is added to the
    final target set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最终流涉及对组件函数的比较。$C$，每个组件函数的最小余弦相似度距离被添加到最终目标集。
- en: '|  | $C=\{C_{1},C_{2},\ldots,C_{p}\}$ |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=\{C_{1},C_{2},\ldots,C_{p}\}$ |  |'
- en: '|  | $\mathbf{e}_{C_{i}}=\text{Embedding}(C_{i})$ |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{e}_{C_{i}}=\text{嵌入}(C_{i})$ |  |'
- en: '|  | $\text{Cosine Similarity}(\mathbf{e}_{C_{i}},\mathbf{e}_{Y_{j}})=\frac{\mathbf{e}_{C_{i}}\cdot\mathbf{e}_{Y_{j}}}{\&#124;\mathbf{e}_{C_{i}}\&#124;\&#124;\mathbf{e}_{Y_{j}}\&#124;}$
    |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{余弦相似度}(\mathbf{e}_{C_{i}},\mathbf{e}_{Y_{j}})=\frac{\mathbf{e}_{C_{i}}\cdot\mathbf{e}_{Y_{j}}}{\|\mathbf{e}_{C_{i}}\|\|\mathbf{e}_{Y_{j}}\|}$
    |  |'
- en: '|  | $\text{Smallest Cosine Similarity for }C_{i}=\min_{j}\text{Cosine Similarity}(\mathbf{e}_{C_{i}},\mathbf{e}_{Y_{j}})$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{最小余弦相似度为 }C_{i}=\min_{j}\text{余弦相似度}(\mathbf{e}_{C_{i}},\mathbf{e}_{Y_{j}})$
    |  |'
- en: 'The final target set is compiled by combining the top matches from all streams:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标集是通过将所有流中的最佳匹配项合并而编制的：
- en: '|  | $1$2 |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This multi-stream approach ensures a comprehensive and targeted selection of
    code snippets based on the initial input $A$. The creation of the final target
    set significantly reduces the number of potential code snippets from a large volume
    to approximately 5 to 15 snippets. To enhance the precision of similarity matching,
    we further process the final target set using GPT-4o to identify the most relevant
    snippets. It is important to note that GPT-4o has token limitations, making it
    impractical to input a large amount of snippet data directly. This constraint
    underscores the importance of using embeddings initially to generate a refined
    target set
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多流方法确保了基于初始输入$A$的代码片段的全面和有针对性的选择。最终目标集的创建将潜在的代码片段数量从大量减少到大约5到15个片段。为了提高相似性匹配的精度，我们进一步使用GPT-4o处理最终目标集，以识别最相关的片段。需要注意的是，GPT-4o有令牌限制，直接输入大量片段数据是不切实际的。这一限制凸显了最初使用嵌入生成精炼目标集的重要性。
- en: 3 Experimental Setup
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 3.0.1 Dataset
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.0.1 数据集
- en: Following [[4](#bib.bib4)], we leverage the CodeSearchNet dataset [[13](#bib.bib13)],
    which features numerous pairs of natural language queries and corresponding code
    snippets, all associated with specific GitHub repositories. To conduct our study,
    we manually processed each natural language query via repo-rift.com, randomly
    selecting 101 rows from the Python evaluation set of CodeSearchNet. To maintain
    fairness and ensure broad applicability, we included queries of varying lengths
    and only altered the natural language query if it detailed parameters or return
    types. Furthermore, some queries were left unmodified, even those containing parameter
    and return information, to uphold generalizability.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 参照 [[4](#bib.bib4)]，我们利用了CodeSearchNet数据集 [[13](#bib.bib13)]，该数据集包含大量自然语言查询及其对应的代码片段，所有这些都与特定的GitHub仓库相关。为了进行研究，我们通过repo-rift.com手动处理每个自然语言查询，从CodeSearchNet的Python评估集随机选择了101行。为了保持公平性和确保广泛适用性，我们包含了不同长度的查询，只有在查询中详细描述了参数或返回类型时才进行了修改。此外，一些查询未做修改，即使包含参数和返回信息，以保持普遍适用性。
- en: We excluded and replaced only those rows where the code snippet had been removed
    from the current main branch of the repository or when the repository size exceeded
    the upload capacity of our repo-rift.com application. The azure-sdk-python repo
    was the sole instance of the latter issue. We opted to exclude snippets not present
    in the main branch because our repo-rift.com application could not effectively
    upload files from previous branches, thus making replacement a more straightforward
    solution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只排除了和替换了那些代码片段已从当前主分支中移除的行，或者当仓库大小超过了我们repo-rift.com应用程序的上传容量时。azure-sdk-python仓库是后者问题的唯一实例。我们选择排除那些不在主分支中的片段，因为我们的repo-rift.com应用程序无法有效上传来自先前分支的文件，因此替换成为了更简单的解决方案。
- en: 3.0.2 Implementation Details
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.0.2 实施细节
- en: For the backend of the RepoRift application, we employ third-party packages
    and OpenAI APIs. The agent is constructed using the CrewAI framework. The website
    is built with the Vue JavaScript framework and SQL, and it is deployed on a standard
    AWS plan. To evaluate our software, we manually input 101 rows of data into our
    website and observe the results displayed as panels on the right.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RepoRift应用程序的后端，我们使用第三方包和OpenAI API。代理是使用CrewAI框架构建的。网站使用Vue JavaScript框架和SQL构建，并部署在标准AWS计划上。为了评估我们的软件，我们手动将101行数据输入到我们的网站中，并观察右侧显示的结果面板。
- en: 3.0.3 Evaluation Metrics
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.0.3 评估指标
- en: Following previous codesearch research from DeepCS [[2](#bib.bib2)], CARLCS
    [[3](#bib.bib3)] , and PSCS [[4](#bib.bib4)], we utilize the same Success@10 and
    Success@1 metrics to compare accuracy. While the aforementioned methods have been
    translated to evaluate the Java Dataset of CodeSearchNet, we test on the Python
    Dataset. Success@k is a metric that determines whether a detected code snippet
    from a system is in the top $k$ results. Therefore, to be positively labeled in
    the Success@1 metric, the result must be the highest rank.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 参考了DeepCS [[2](#bib.bib2)]、CARLCS [[3](#bib.bib3)] 和 PSCS [[4](#bib.bib4)] 的前期代码搜索研究，我们使用相同的Success@10和Success@1指标来比较准确性。虽然上述方法已被转化为评估CodeSearchNet的Java数据集，但我们在Python数据集上进行测试。Success@k是一个度量标准，用于确定系统检测到的代码片段是否在前$k$个结果中。因此，要在Success@1指标中被积极标记，结果必须是最高排名的。
- en: '|  | $\text{Success@k}=\begin{cases}1,&amp;\text{if the correct snippet is
    within the top }k\text{ results}\\ 0,&amp;\text{otherwise}\end{cases}$ |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Success@k}=\begin{cases}1,&amp;\text{如果正确片段在前 }k\text{ 个结果中}\\
    0,&amp;\text{否则}\end{cases}$ |  |'
- en: 'For Success@1, the metric is defined as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Success@1，该指标定义为：
- en: '|  | $\text{Success@1}=\begin{cases}1,&amp;\text{if the correct snippet is
    the top result}\\ 0,&amp;\text{otherwise}\end{cases}$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Success@1}=\begin{cases}1,&amp;\text{如果正确片段是排名第一的结果}\\ 0,&amp;\text{否则}\end{cases}$
    |  |'
- en: To determine the highest rank in our methodology, we look at all the streams
    in the multi-streamed process mentioned in the methods (Section [2.2](#S2.SS2
    "2.2 Ensemble Architecture with Multi-Stream Comparisons ‣ 2 Methodology ‣ LLM
    Agents Improve Semantic Code Search")), and take the snippet that was calculated
    with the highest cosine similarity. Additionally, since we reason that many methods
    can only be fully understood in the context of a class, we consider it a positive
    label if the class of the correct snippet is detected by RepoRift even if the
    individual function is not.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定我们方法中的最高排名，我们查看方法中提到的多流处理过程中的所有流（第 [2.2](#S2.SS2 "2.2 Ensemble Architecture
    with Multi-Stream Comparisons ‣ 2 Methodology ‣ LLM Agents Improve Semantic Code
    Search") 节），并选取计算出最高余弦相似度的片段。此外，由于我们推测许多方法只有在类的上下文中才能完全理解，如果 RepoRift 检测到正确片段的类，即使单个函数未被检测到，我们也视为正标签。
- en: 4 Results
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: We compare our methods to other baselines with the most similar evaluation setups.
    The evaluation for PSCS, CARLCS, and DeepCS have all been translated to CodeSearchNet,
    where they are given thousands of snippets and expected to find the correct snippet
    according to a provided natural language query. While these three methods have
    been built for Java, all we do differently is test on Python. And while the previous
    baselines, as per [[4](#bib.bib4)], conduct a search over all test snippets in
    CodeSearchNet, we conduct a search over all snippets in a GitHub repository as
    that is what our use case is specifically designed for. We directly take the success
    rates from [[4](#bib.bib4)] and compare them with the success rates calculated
    through our evaluation, making the judgment that the difference is trivial. Additionally
    we remove comments from all code snippets to ensure an obvious fair evaluation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与其他基线方法进行比较，这些基线方法具有最相似的评估设置。PSCS、CARLCS 和 DeepCS 的评估都被转换到 CodeSearchNet，其中它们会接收到数千个片段，并根据提供的自然语言查询找到正确的片段。尽管这三种方法是为
    Java 设计的，但我们所做的只是用 Python 进行测试。而之前的基线方法，如 [[4](#bib.bib4)] 所述，在 CodeSearchNet
    中对所有测试片段进行搜索，我们则对 GitHub 仓库中的所有片段进行搜索，因为这正是我们用例的设计目的。我们直接从 [[4](#bib.bib4)] 获取成功率，并将其与我们评估计算出的成功率进行比较，认为差异微不足道。此外，我们去除所有代码片段中的注释，以确保明显的公平评估。
- en: We chose methods that most closely mimic the real-world use of a tool across
    different GitHub repositories. This approach involves a constantly dynamic set
    and size of distractor codes that have tighter relationships to the correct snippet.
    The models chosen for comparison, such as DeepCS, CARLCS, and PSCS, are highly
    cited. While we couldn’t find a specific well-cited piece of research that used
    a dynamic set of distractor codes, we selected methods with large static distractor
    sets. The methods we compared do not inherently use a dynamic set of distractor
    codes. However, their distractor sets are substantial, with 19k snippets, providing
    a robust benchmark for evaluation. When ranking from 1 to 10, we make the sound
    conclusion that the distractor snippets from 10 to 999 would be significantly
    different each time a new natural language query is processed unlike the fixed
    distractor codes present in CodeBERT [[5](#bib.bib5)]. This variation closely
    simulates a dynamic distractor set, making our comparisons relevant and comprehensive.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了最接近工具在不同 GitHub 仓库中实际使用的方法。这种方法涉及到一个不断动态变化的干扰代码集合，这些干扰代码与正确片段的关系更加紧密。选择用于比较的模型，如
    DeepCS、CARLCS 和 PSCS，都是被广泛引用的。尽管我们没有找到使用动态干扰代码集合的具体广泛引用研究，但我们选择了干扰集合较大的方法。我们比较的方法本质上并未使用动态干扰代码集合。然而，它们的干扰集合规模庞大，拥有
    19k 个片段，为评估提供了可靠的基准。在从 1 到 10 排名时，我们得出合理的结论，即从 10 到 999 的干扰片段在每次处理新的自然语言查询时会显著不同，这与
    CodeBERT 中存在的固定干扰代码不同[[5](#bib.bib5)]。这种变化接近动态干扰集合，使我们的比较具有相关性和全面性。
- en: 'Table 1: Evaluation Results centered on a 95% confidence interval'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：以 95% 置信区间为中心的评估结果
- en: '| Model | Sucess@10 | Success@1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Sucess@10 | Success@1 |'
- en: '| --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| DeepCS (CodeSearchNet 19k Validation Set Java) | 40.3 | 14.6 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| DeepCS（CodeSearchNet 19k 验证集 Java） | 40.3 | 14.6 |'
- en: '| CARLCS (CodeSearchNet 19k Validation Set, Java) | 43.7 | 17.8 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| CARLCS（CodeSearchNet 19k 验证集，Java） | 43.7 | 17.8 |'
- en: '| PSCS (CodeSearchNet 19k Validation Set, Java) | 47.6 | 22.9 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| PSCS（CodeSearchNet 19k 验证集，Java） | 47.6 | 22.9 |'
- en: '| RepoRift (CodeSearchNet Entire Github Repo, Python) | 78.2 ±8.1 | 34.6 ±9.3
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| RepoRift（CodeSearchNet 整个 GitHub 仓库，Python） | 78.2 ±8.1 | 34.6 ±9.3 |'
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4 Results ‣ LLM Agents Improve Semantic Code Search")
    provides the evaluation results, comparing our method, RepoRift, against the baselines.
    The success rates are measured at two levels: Success@10 and Success@1, which
    indicate the percentage of correct snippets found within the top 10 and the top
    1 results, respectively. Despite not being optimized for Success@1 due to its
    ensemble approach, RepoRift significantly outperforms all other methods. Specifically,
    RepoRift achieves an 78.2% ±8.1 success rate at Success@10, which has a lower
    bound accuracy that is approximately 22.5% better than the highest-performing
    baseline (PSCS at 47.6%). For Success@1, RepoRift achieves a 34.6% ±9.3 success
    rate, which has a lower bound accuracy that is approximately 2.4% better than
    the highest-performing baseline (PSCS at 22.9%).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S4.T1 "表 1 ‣ 4 结果 ‣ LLM 代理提高语义代码搜索") 提供了评估结果，将我们的方法 RepoRift 与基准方法进行了比较。成功率在两个级别上进行测量：Success@10
    和 Success@1，分别表示在前 10 个和前 1 个结果中找到正确片段的百分比。尽管由于集成方法的原因未针对 Success@1 进行优化，但 RepoRift
    显著超越了所有其他方法。具体而言，RepoRift 在 Success@10 中达到了 78.2% ±8.1 的成功率，其下限准确率比表现最好的基准方法（PSCS
    47.6%）高出约 22.5%。对于 Success@1，RepoRift 达到了 34.6% ±9.3 的成功率，其下限准确率比表现最好的基准方法（PSCS
    22.9%）高出约 2.4%。
- en: RepoRift achieves high accuracy with minimal preprocessing of the evaluation
    set. It effectively handles queries in various forms, including those written
    in Russian, raw URLs, and vague conceptual information. This versatility showcases
    RepoRift’s capability to understand and process a wide range of input types without
    requiring extensive preprocessing. These results demonstrate that RepoRift not
    only outperforms other methods in both Success@10 and Success@1 metrics but also
    does so while maintaining a high level of flexibility and minimal preprocessing.
    The improvement in success rates highlights the effectiveness of our approach
    in searching and identifying relevant code snippets in a larger and more diverse
    dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: RepoRift 在最小化评估集预处理的情况下实现了高准确率。它有效处理各种形式的查询，包括俄语书写的查询、原始 URL 和模糊的概念信息。这种多样性展示了
    RepoRift 理解和处理各种输入类型的能力，而无需广泛的预处理。这些结果表明，RepoRift 不仅在 Success@10 和 Success@1 指标上超越了其他方法，而且在保持高灵活性和最小预处理的同时取得了成功率的提升。这些改进突显了我们的方法在更大和更多样化的数据集中搜索和识别相关代码片段的有效性。
- en: 5 Conclusion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we presented the use of information injection as a methodology
    to improve code search. The reasoning behind such a use case was to add vital
    details to alleviate the vagueness and ambiguity present in a user prompt for
    a code search application. By leveraging agentic LLM models and RAG, our system
    was able to perform internet searches relevant to a prompt and github repository,
    consequently addressing the Vocubulary Mismatch Problem and allowing for context-aware
    searches.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了信息注入作为一种改进代码搜索的方法。使用这种方法的原因是为了向用户提示中添加重要细节，以减轻代码搜索应用程序中存在的模糊性和歧义。通过利用代理型
    LLM 模型和 RAG，我们的系统能够对与提示和 GitHub 仓库相关的内容进行互联网搜索，从而解决了词汇不匹配问题，并实现了上下文感知搜索。
- en: We provide three main contributions. Firstly, we demonstrate how agentic LLMs
    in combination with RAG allow for further contextualization of queries, a methodology
    we coin as information injection. Secondly, by pairing this process with a multi-stream
    ensemble approach we achieve state-of-the-art accuracy for semantic code search.
    By translating the query to code and then utilizing many comparison to generate
    a final set, a larger variation of snippets are able to be captured. Finally for
    our third contribution, we deployed our advanced techniques onto a website called
    RepoRift (www.repo-rift.com). RepoRift allows users to perform semantic code searches
    within specific GitHub repositories. The platform’s practical utility and performance
    in real-world scenarios underscore the effectiveness of our approach.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了三项主要贡献。首先，我们展示了如何结合使用代理型LLM和RAG进一步上下文化查询，这一方法我们称之为信息注入。其次，通过将这一过程与多流集成方法配对，我们实现了语义代码搜索的最先进精度。通过将查询转换为代码，然后利用多种比较生成最终集合，可以捕获更多的代码片段变化。最后，在我们的第三项贡献中，我们将我们的先进技术部署到一个名为RepoRift的网站上（www.repo-rift.com）。RepoRift允许用户在特定的GitHub存储库中执行语义代码搜索。该平台在现实场景中的实际效用和性能突显了我们方法的有效性。
- en: Our experimental results, conducted on the CodeSearchNet dataset, show that
    RepoRift significantly outperforms existing methods such as DeepCS, CARLCS, and
    PSCS. Specifically, RepoRift achieved an 78.2% success rate at Success@10 and
    a 34.6% success rate at Success@1, demonstrating superior performance in both
    metrics. These results highlight the potential of our method to enhance the accuracy
    and relevance of semantic code searches. In conclusion, our research presents
    a significant advancement in the field of semantic code search. By integrating
    agentic LLMs and RAG, we have addressed critical challenges and improved the overall
    effectiveness of code retrieval systems.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在CodeSearchNet数据集上进行的实验结果表明，RepoRift显著优于现有方法，如DeepCS、CARLCS和PSCS。具体来说，RepoRift在Success@10上达到了78.2%的成功率，在Success@1上达到了34.6%的成功率，展示了在这两个指标上的优越表现。这些结果突显了我们方法提高语义代码搜索准确性和相关性的潜力。总之，我们的研究在语义代码搜索领域中呈现了显著的进展。通过整合代理型LLM和RAG，我们解决了关键挑战并提高了代码检索系统的整体效果。
- en: 5.1 Future Work
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 未来工作
- en: 'Further analyzing the full evaluation breakdown in section [6](#S6.SSx1 "Full
    Evaluation Breakdown ‣ 6 Appendix ‣ LLM Agents Improve Semantic Code Search"),
    we were able to discern several weaknesses in our approach that lays down a better
    idea for future work. While utilizing code generation before embeddings is helpful
    for code search [[2](#bib.bib2)], it struggles to account for snippets that are
    almost primarily constructed from other functions and classes within a codebase.
    Therefore, while sometimes through naming conventions in generated code embeddings
    can still retrieve the right snippet, the code search results for this case are
    significantly weaker. For instance, below are two example of snippets that RepoRift
    was unable to identify:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分析第[6](#S6.SSx1 "完整评估细分 ‣ 6 附录 ‣ LLM 代理改进语义代码搜索")节中的完整评估细分，我们能够辨别出我们方法的几个弱点，这为未来的工作奠定了更好的思路。尽管在嵌入之前利用代码生成有助于代码搜索[[2](#bib.bib2)]，但它难以处理几乎完全由代码库中的其他函数和类构建的代码片段。因此，尽管有时通过生成代码嵌入中的命名约定仍能检索到正确的代码片段，但在这种情况下的代码搜索结果明显较弱。例如，下面是RepoRift无法识别的两个代码片段示例：
- en: 'User Prompt: "convert a field’s content into some valid HTML"'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：“将字段内容转换为有效的HTML”
- en: 1#  Snippet  12def  make_html_items(  self,  items  ):3  lines  =  []4  for  item  in  items:5  if  item.lines:6  lines.append(  self.make_html_code(  item.lines  )  )7  else:8  lines.append(  self.make_html_para(  item.words  )  )9  return  string.join(  lines,  \’\\n\’␣)’
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 1#  代码片段  12def  make_html_items(  self,  items  ):3  lines  =  []4  for  item  in  items:5  if  item.lines:6  lines.append(  self.make_html_code(  item.lines  )  )7  else:8  lines.append(  self.make_html_para(  item.words  )  )9  return  string.join(  lines,  \’\\n\’␣)’
- en: 'User Prompt: "Parse module defined in *uri*"'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：“解析定义在*uri*中的模块”
- en: 1#  Snippet  22def  _parse_module(self,  uri):3  filename  =  self._uri2path(uri)4  if  filename  is  None:5  return  ([],[])6  f  =  open(filename,  ’rt’)7  functions,  classes  =  self._parse_lines(f)8  f.close()9  return  functions,  classes
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 1#  代码片段  22def  _parse_module(self,  uri):3  filename  =  self._uri2path(uri)4  if  filename  is  None:5  return  ([],[])6  f  =  open(filename,  ’rt’)7  functions,  classes  =  self._parse_lines(f)8  f.close()9  return  functions,  classes
- en: ASTs or any form of translating code to where these other functions and classes
    are replaced with their raw code serves as possible area of future work to better
    address this issue.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ASTs 或任何形式的将代码翻译到其他函数和类并用其原始代码替代的方式，作为未来可能的工作领域，以更好地解决此问题。
- en: Acknowledgments
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We acknowledge Prabhat Singh from Cisco for their valuable advice and support.
    This work was conducted independently and did not utilize any company resources
    or proprietary information.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢Cisco的Prabhat Singh提供的宝贵建议和支持。这项工作是独立进行的，并未使用任何公司资源或专有信息。
- en: References
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and John Grundy.
    Opportunities and challenges in code search tools. ACM Comput. Surv., 54(9), oct
    2021.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, 和 John Grundy。代码搜索工具中的机会与挑战。ACM计算机调查，54(9)，2021年10月。'
- en: '[2] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In Proceedings
    of the 40th International Conference on Software Engineering, pages 933–944, 2018.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Xiaodong Gu, Hongyu Zhang, 和 Sunghun Kim。深度代码搜索。载于第40届国际软件工程大会论文集，第933–944页，2018年。'
- en: '[3] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. Improving
    code search with co-attentive representation learning. In 2020 IEEE/ACM 28th International
    Conference on Program Comprehension (ICPC), pages 196–207, 2020.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, 和 Yan Lei。通过共同注意表示学习提高代码搜索。载于2020年IEEE/ACM第28届程序理解国际会议（ICPC），第196–207页，2020年。'
- en: '[4] Zhensu Sun, Yan Liu, Chen Yang, and Yu Qian. Pscs: A path-based neural
    model for semantic code search, 2020.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zhensu Sun, Yan Liu, Chen Yang, 和 Yu Qian。PSCS：一种基于路径的神经模型用于语义代码搜索，2020年。'
- en: '[5] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
    Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. Codebert: A pre-trained
    model for programming and natural languages. CoRR, abs/2002.08155, 2020.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
    Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, 和 Ming Zhou。Codebert：一种用于编程和自然语言的预训练模型。CoRR，abs/2002.08155，2020年。'
- en: '[6] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han,
    Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes
    Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen
    Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak
    Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code
    embeddings by contrastive pre-training. CoRR, abs/2201.10005, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han,
    Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes
    Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen
    Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak
    Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, 和 Lilian Weng。文本和代码嵌入通过对比预训练。CoRR，abs/2201.10005，2022年。'
- en: '[7] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
    Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng,
    Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou.
    Graphcodebert: Pre-training code representations with data flow, 2021.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long
    Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng,
    Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, 和 Ming Zhou。Graphcodebert：通过数据流预训练代码表示，2021年。'
- en: '[8] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In Proceedings
    of the 40th International Conference on Software Engineering, ICSE ’18, page 933–944,
    New York, NY, USA, 2018\. Association for Computing Machinery.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Xiaodong Gu, Hongyu Zhang, 和 Sunghun Kim。深度代码搜索。载于第40届国际软件工程大会论文集，ICSE
    ’18，第933–944页，美国纽约，2018年。计算机协会。'
- en: '[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need,
    2023.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin。注意力即你所需，2023年。'
- en: '[10] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron,
    Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields
    for view synthesis, 2020.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron,
    Ravi Ramamoorthi, 和 Ren Ng。Nerf：将场景表示为用于视图合成的神经辐射场，2020年。'
- en: '[11] Xinzhe Li. A survey on llm-based agents: Common workflows and reusable
    llm-profiled components, 2024.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Xinzhe Li。基于LLM的代理调查：常见工作流程和可重用的LLM配置组件，2024年。'
- en: '[12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel,
    Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive
    nlp tasks, 2021.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel,
    Sebastian Riedel 和 Douwe Kiela。用于知识密集型NLP任务的检索增强生成，2021年。'
- en: '[13] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
    Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search,
    2020.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis 和 Marc
    Brockschmidt。Codesearchnet挑战：评估语义代码搜索的现状，2020年。'
- en: Ethics Consideration
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: ChatGPT 4 was partly used for the rewording and rephrasing of ideas in this
    paper.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 4 部分用于重新措辞和重新表达本文中的思想。
- en: 6 Appendix
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 附录
- en: Full Evaluation Breakdown
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整评估细节
- en: We tested on 101 rows of CodeSearchNet. Table LABEL:tab:success-rates presents
    the detailed results for each data point examined. Any one of these rows can be
    re-tested by using repo-rift.com.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在101行CodeSearchNet上进行了测试。表 LABEL:tab:success-rates 展示了每个数据点的详细结果。任何一行都可以通过使用repo-rift.com重新测试。
- en: 'Table 2: Success rates of various text queries.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：各种文本查询的成功率。
- en: '|  |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Text Query | Success @ 10 | Success @ 1 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 文本查询 | 成功率 @ 10 | 成功率 @ 1 |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Converts an operating system path into a client path by replacing instances
    of os.path.sep with ’/’. Note: If the client path contains any instances of ’/’
    already, they will be replaced with ’-’. | ✓ | ✗ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 通过用’/’替换os.path.sep的实例将操作系统路径转换为客户端路径。注意：如果客户端路径中已包含任何’/’实例，它们将被替换为’-’。 |
    ✓ | ✗ |'
- en: '| Callback for an option that adds to the ‘actions‘ list. | ✓ | ✓ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 为添加到‘actions’列表的选项设置回调。 | ✓ | ✓ |'
- en: '| Sets up or removes a listener for children being changed on a specified object.
    | ✓ | ✗ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 为指定对象设置或移除子项更改的监听器。 | ✓ | ✗ |'
- en: '| Gets a dictionary of ref positions and the ref IDs of the refs for that game.
    | ✗ | ✗ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 获取一个字典，包含该游戏的引用位置和引用ID。 | ✗ | ✗ |'
- en: '| Return the status of all servers. | ✓ | ✗ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 返回所有服务器的状态。 | ✓ | ✗ |'
- en: '| Return a new “GroupBy“ object using this frame and the desired grouping columns.
    The returned groups are sorted by the natural group-by column sort. :param by:
    The columns to group on (either a single column name, or a list of column names,
    or a list of column indices). | ✓ | ✗ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 使用此框架和所需的分组列返回一个新的“GroupBy”对象。返回的组按自然分组列排序。:param by：要分组的列（可以是单个列名、列名列表或列索引列表）。
    | ✓ | ✗ |'
- en: '| Accepts data in zyx. !!! | ✓ | ✗ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 接受zyx格式的数据。！！！ | ✓ | ✗ |'
- en: '| Return an error dict for self.args and kwargs. | ✓ | ✗ |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 返回self.args和kwargs的错误字典。 | ✓ | ✗ |'
- en: '| Private helper method | ✓ | ✗ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 私有辅助方法 | ✓ | ✗ |'
- en: '| Adds the default_data to data and dumps it to a json. | ✓ | ✓ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 将default_data添加到数据中并将其导出为json。 | ✓ | ✓ |'
- en: '| Recursively flatten nested objects | ✓ | ✓ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 递归扁平化嵌套对象 | ✓ | ✓ |'
- en: '| Propagate "clk" clock and negative reset "rst_n" signal to all subcomponents
    | ✓ | ✓ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 将“clk”时钟和负重置“rst_n”信号传播到所有子组件 | ✓ | ✓ |'
- en: '| Read the default config file. :raises DefaultConfigValidationError: There
    was a validation error with the *default* file. | ✓ | ✓ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 读取默认配置文件。:raises DefaultConfigValidationError：*默认*文件存在验证错误。 | ✓ | ✓ |'
- en: '| Sets the service name and version the request should target Args: service
    (str): The name of the service as displayed in the services.json file version
    (str): The version of the service as displayed in the services.json file Returns:
    The request builder instance in order to chain calls | ✓ | ✗ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 设置请求应针对的服务名称和版本 参数：service (str)：在services.json文件中显示的服务名称 version (str)：在services.json文件中显示的服务版本
    返回：请求构建器实例，以便进行链式调用 | ✓ | ✗ |'
- en: '| Computes the standard deviation of a mixture distribution. This function
    works regardless of the component distribution, so long as each component’s mean
    and standard deviation can be provided. Args: mixture_weight_vector: A 2D tensor
    with shape [batch_size, num_components] mean_vector: A 2D tensor of mixture component
    means. Has shape ‘[batch_size, num_components]‘. stddev_vector: A 2D tensor of
    mixture component standard deviations. Has shape ‘[batch_size, num_components]‘.
    Returns: A 1D tensor of shape ‘[batch_size]‘ representing the standard deviation
    of the mixture distribution with given weights and component means and standard
    deviations. Raises: ValueError: If the shapes of the input tensors are not as
    expected." | ✓ | ✓ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 计算混合分布的标准差。此函数适用于任何成分分布，只要每个成分的均值和标准差能够提供即可。 参数：mixture_weight_vector：形状为[batch_size,
    num_components]的二维张量 mean_vector：混合成分均值的二维张量。形状为‘[batch_size, num_components]‘。
    stddev_vector：混合成分标准差的二维张量。形状为‘[batch_size, num_components]‘。 返回值：形状为‘[batch_size]‘的一维张量，表示具有给定权重和成分均值及标准差的混合分布的标准差。
    异常：ValueError：如果输入张量的形状与预期不符。 | ✓ | ✓ |'
- en: '| Invert all instructions. | ✗ | ✗ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 反转所有指令。 | ✗ | ✗ |'
- en: '| Encodes a byte string into trytes. | ✓ | ✗ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 将字节字符串编码为 trytes。 | ✓ | ✗ |'
- en: '| Convert a MySQL TIMESTAMP to a Timestamp object. | ✓ | ✓ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 将 MySQL TIMESTAMP 转换为 Timestamp 对象。 | ✓ | ✓ |'
- en: '| Removes all components from the canvas | ✗ | ✗ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 从画布中移除所有组件。 | ✗ | ✗ |'
- en: '| Handles the component being changed. | ✓ | ✗ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 处理成分被更改的情况。 | ✓ | ✗ |'
- en: '| http://stackoverflow.com/questions/29107800 | ✓ | ✗ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| http://stackoverflow.com/questions/29107800 | ✓ | ✗ |'
- en: '| Fetch the pages from the backend url for MediaWiki >=1.27 The method retrieves,
    from a MediaWiki url, the wiki pages. | ✗ | ✗ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 从后端 URL 中获取 MediaWiki >=1.27 的页面。该方法从 MediaWiki URL 中检索 wiki 页面。 | ✗ | ✗
    |'
- en: '| A change handler for the ’objects’ list of the Include. If the object is
    initialized objects which are removed will be unparented and objects which are
    added will be reparented. Old objects will be destroyed if the ’destroy_old’ flag
    is True. | ✓ | ✗ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 一个处理 ’objects’ 列表变化的处理程序。如果对象已初始化，移除的对象将被去除父级，添加的对象将被重新设置父级。如果 ’destroy_old’
    标志为 True，旧对象将被销毁。 | ✓ | ✗ |'
- en: '| Generate the time in seconds in which DHCPDISCOVER will be retransmitted.
    [:rfc:‘2131#section-3.1‘]:: might retransmit the DHCPREQUEST message four times,
    for a total delay of 60 seconds [:rfc:‘2131#section-4.1‘]:: For example, in a
    10Mb/sec Ethernet internetwork, the delay before the first retransmission SHOULD
    be 4 seconds randomized by the value of a uniform random number chosen from the
    range -1 to +1\. Clients with clocks that provide resolution granularity of less
    than one second may choose a non-integer randomization value. The delay before
    the next retransmission SHOULD be 8 seconds randomized by the value of a uniform
    number chosen from the range -1 to +1\. The retransmission delay SHOULD be doubled
    with subsequent retransmissions up to a maximum of 64 seconds. | ✓ | ✗ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 生成 DHCPDISCOVER 将被重传的秒数。[:rfc:‘2131#section-3.1‘]:: 可能会重传 DHCPREQUEST 消息四次，总延迟为
    60 秒 [:rfc:‘2131#section-4.1‘]:: 例如，在 10Mb/sec 以太网互联网络中，第一次重传前的延迟应为 4 秒，由范围 -1
    到 +1 中选择的均匀随机数值随机化。时钟分辨率小于一秒的客户端可以选择非整数随机化值。下一次重传前的延迟应为 8 秒，由范围 -1 到 +1 中选择的均匀随机数值随机化。重传延迟应在随后的重传中加倍，最多达到
    64 秒。 | ✓ | ✗ |'
- en: '| Get the items fetched by the jobs. | ✓ | ✗ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 获取作业提取的项目。 | ✓ | ✗ |'
- en: '| This is the actual zest.releaser entry point Relevant items in the context
    dict: name Name of the project being released tagdir Directory where the tag checkout
    is placed (*if* a tag checkout has been made) version Version we’re releasing
    workingdir Original working directory | ✓ | ✗ |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 这是实际的 zest.releaser 入口点。上下文字典中的相关项：name 项目名称 tagdir 标记检出目录（*如果*进行了标记检出） version
    我们正在发布的版本 workingdir 原始工作目录 | ✓ | ✗ |'
- en: '| Update the splitter and readline delims when greedy is changed | ✓ | ✓ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 当贪婪模式发生变化时，更新分隔符和读取行分隔符。 | ✓ | ✓ |'
- en: '| Given an array of datapoints, inserts them to the stream. This is different
    from insert(), because it requires an array of valid datapoints, whereas insert
    only requires the data portion of the datapoint, and fills out the rest:: s =
    cdb["mystream"] s.create("type": "number") s.insert_array(["d": 4, "t": time.time(),"d":
    5, "t": time.time()], restamp=False) The optional ‘restamp‘ parameter specifies
    whether or not the database should rewrite the timestamps of datapoints which
    have a timestamp that is less than one that already exists in the database. That
    is, if restamp is False, and a datapoint has a timestamp less than a datapoint
    that already exists in the database, then the insert will fail. If restamp is
    True, then all datapoints with timestamps below the datapoints already in the
    database will have their timestamps overwritten to the same timestamp as the most
    recent datapoint hat already exists in the database, and the insert will succeed
    | ✓ | ✓ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个数据点数组，将它们插入到流中。这与 insert() 不同，因为它要求一个有效的数据点数组，而 insert 只要求数据点的数据部分，并填写其余部分::
    s = cdb["mystream"] s.create("type": "number") s.insert_array(["d": 4, "t": time.time(),"d":
    5, "t": time.time()], restamp=False) 可选的‘restamp’参数指定数据库是否应该重写具有比数据库中已存在的时间戳更早的时间戳的数据点。也就是说，如果
    restamp 为 False，并且一个数据点的时间戳早于数据库中已存在的数据点的时间戳，则插入将失败。如果 restamp 为 True，则所有时间戳低于数据库中已存在的数据点的时间戳的数据点将被覆盖为与数据库中最接近的数据点相同的时间戳，插入将成功
    | ✓ | ✓ |'
- en: '| Gets the twitter feed for a given handle. :param handle: The twitter handle.
    :return: A list of entries in a user’s feed. :raises ApiError: When the api couldn’t
    connect. :raises CircuitBreakerError: When the circuit breaker is open. | ✓ |
    ✗ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 获取给定 handle 的 Twitter 提要。 :param handle: Twitter handle。 :return: 用户提要中的条目列表。
    :raises ApiError: 当 API 无法连接时。 :raises CircuitBreakerError: 当电路断路器打开时。 | ✓ | ✗
    |'
- en: '| Return list containing URIs with base URI. | ✓ | ✓ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 返回包含基本 URI 的 URI 列表。 | ✓ | ✓ |'
- en: '| Calls ‘fn‘ and computes the gradient of the result wrt ‘args_list‘ | ✗ |
    ✗ |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 调用‘fn’并计算结果相对于‘args_list’的梯度 | ✗ | ✗ |'
- en: '| Iterates over the actions and executes them in order. | ✗ | ✗ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 遍历动作并按顺序执行它们。 | ✗ | ✗ |'
- en: '| Returns the log found at the remote_log_location. Returns ” if no logs are
    found or there is an error. :param remote_log_location: the log’s location in
    remote storage :type remote_log_location: str (path) :param return_error: if True,
    returns a string error message if an error occurs. Otherwise returns ” when an
    error occurs. :type return_error: bool | ✓ | ✓ |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 返回在 remote_log_location 发现的日志。如果未找到日志或出现错误，则返回” :param remote_log_location:
    日志在远程存储中的位置 :type remote_log_location: str (路径) :param return_error: 如果为 True，发生错误时返回字符串错误消息。否则发生错误时返回”
    :type return_error: bool | ✓ | ✓ |'
- en: '| This will setup logging for stdout and stderr :param formatter: :param log_level:
    str of the overall logging level for setLevel :param log_stdout_level: str of
    the logging level of stdout :param str_format: str of the logging format :param
    date_format: str of the date format :param silence_modules: list of str of modules
    to exclude from logging :param log_filter: logging.filter instance to add to handler
    :return: None | ✓ | ✓ |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 这将设置 stdout 和 stderr 的日志记录 :param formatter: :param log_level: 日志记录的整体级别的字符串
    :param log_stdout_level: stdout 的日志记录级别的字符串 :param str_format: 日志记录格式的字符串 :param
    date_format: 日期格式的字符串 :param silence_modules: 要从日志记录中排除的模块的字符串列表 :param log_filter:
    要添加到处理程序的 logging.filter 实例 :return: 无 | ✓ | ✓ |'
- en: '| Logs in to Steam | ✓ | ✗ |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 登录到 Steam | ✓ | ✗ |'
- en: '| Iterate through the i_chunk and tmp_ner_path to generate a new Chunk with
    body.ner | ✗ | ✗ |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 遍历 i_chunk 和 tmp_ner_path 以生成一个新的 Chunk 与 body.ner | ✗ | ✗ |'
- en: '| Retrieves connection to Cloud Text to Speech | ✓ | ✗ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 检索连接到 Cloud Text to Speech | ✓ | ✗ |'
- en: '| Calculate image translations in parallel | ✓ | ✗ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 并行计算图像翻译 | ✓ | ✗ |'
- en: '| Estimate the ‘weighted Jaccard similarity‘ between the multi-sets represented
    by this weighted MinHash and the other. | ✓ | ✓ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 估算由该加权 MinHash 和另一个表示的多重集之间的‘加权 Jaccard 相似度’。 | ✓ | ✓ |'
- en: '| Write a ’.dot’ file. | ✓ | ✓ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 写一个’.dot’文件。 | ✓ | ✓ |'
- en: '| Build different type of Dingding message As most commonly used type, text
    message just need post message content rather than a dict like “’content’: ’message’
    | ✓ | ✗ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 构建不同类型的 Dingding 消息。作为最常用的类型，文本消息只需发布消息内容，而不是像“’content’: ’message’”这样的字典
    | ✓ | ✗ |'
- en: '| Return the list of all contained scope from global to local | ✓ | ✗ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 返回从全局到本地的所有包含的范围列表 | ✓ | ✗ |'
- en: '| Remove a process | ✓ | ✓ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 移除一个过程 | ✓ | ✓ |'
- en: '| Create a new code cell with input and output | ✓ | ✓ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个新的代码单元，包括输入和输出 | ✓ | ✓ |'
- en: '| r\’"[^"]+" | ✓ | ✗ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| r\’"[^"]+" | ✓ | ✗ |'
- en: '| Assign parameters to new parameters or values. | ✓ | ✗ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 将参数分配给新的参数或值。 | ✓ | ✗ |'
- en: '| Deletes the widget by the given name. Note that this feature is currently
    experimental as there seems to be a memory leak with this method. | ✓ | ✓ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 通过给定名称删除小部件。请注意，这个功能目前是实验性的，因为此方法似乎存在内存泄漏。 | ✓ | ✓ |'
- en: '| Tries to decode strings that look like dates into datetime objects. | ✓ |
    ✓ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 尝试将看起来像日期的字符串解码为日期时间对象。 | ✓ | ✓ |'
- en: '| Return a string representation of an object | ✗ | ✗ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 返回对象的字符串表示 | ✗ | ✗ |'
- en: '| [Russian text] | ✓ | ✗ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| [俄文文本] | ✓ | ✗ |'
- en: '| Get a selfLink for the manifest, for use by the client get_manifest function,
    along with the parents pull | ✓ | ✗ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 获取用于客户端get_manifest函数的manifest的selfLink，以及父级的pull | ✓ | ✗ |'
- en: '| Returns the year ID of the season in which this game took place. Useful for
    week 17 January games. | ✓ | ✗ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 返回进行此游戏的赛季的年份ID。对第17周的1月游戏有用。 | ✓ | ✗ |'
- en: '| Set the loop points within the sound. The sound must have been created with
    “loop=True“. The default parameters cause the loop points to be set to the entire
    sound duration. :note: There is currently no API for converting sample numbers
    to times. | ✓ | ✓ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 设置声音中的循环点。声音必须用“loop=True”创建。默认参数会将循环点设置为整个声音时长。:note: 目前没有将样本数转换为时间的API。
    | ✓ | ✓ |'
- en: '| Returns a completed game state object, setting an optional message to display
    after the game is over. | ✓ | ✗ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 返回一个完成的游戏状态对象，在游戏结束后设置一个可选消息进行显示。 | ✓ | ✗ |'
- en: '| Computes graph and static ‘sample_shape | ✗ | ✗ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 计算图形和静态的‘sampleshape | ✗ | ✗ |'
- en: '| Read header data from Gadget data file ’filename’ with Gadget file type ’gtype’.
    Returns offsets of positions and velocities. | ✓ | ✗ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 从Gadget数据文件’filename’中读取标题数据，Gadget文件类型为’gtype’。返回位置和速度的偏移量。 | ✓ | ✗ |'
- en: '| Get RtlNetlist context from signals | ✓ | ✓ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 从信号中获取RtlNetlist上下文 | ✓ | ✓ |'
- en: '| Return a schedule shifted forward by “time` | ✓ | ✗ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 返回“时间”前移的计划 | ✓ | ✗ |'
- en: '| Read the code and update all links. | ✓ | ✗ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 读取代码并更新所有链接。 | ✓ | ✗ |'
- en: '| Raise exception if clbit is not in this circuit or bad format. | ✗ | ✗ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 如果clbit不在该电路中或格式不正确，则引发异常。 | ✗ | ✗ |'
- en: '| Return archive name without extension | ✓ | ✓ |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 返回不带扩展名的归档名称 | ✓ | ✓ |'
- en: '| Serve custom HTML page | ✓ | ✗ |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 提供自定义HTML页面 | ✓ | ✗ |'
- en: '| Comparison for x coordinate | ✓ | ✗ |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| x坐标的比较 | ✓ | ✗ |'
- en: '| Close the socket to free system resources. After the socket is closed, further
    operations with socket will fail. Multiple calls to close will have no effect.
    | ✓ | ✓ |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 关闭套接字以释放系统资源。关闭套接字后，进一步的操作将失败。多次调用close将没有效果。 | ✓ | ✓ |'
- en: '| Find the path to the folder associated with a given profile. I.e. find $IPYTHONDIR/profile_whatever.
    | ✗ | ✗ |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 查找与给定配置文件相关联的文件夹路径。例如，查找$IPYTHONDIR/profile_whatever。 | ✗ | ✗ |'
- en: '| Opens a Python script for editing. | ✓ | ✓ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 打开Python脚本进行编辑。 | ✓ | ✓ |'
- en: '| Matches an outgoing HTTP request against the current mock matchers. This
    method acts like a delegator to ‘pook.MatcherEngine | ✗ | ✗ |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 将发出的HTTP请求与当前的模拟匹配器进行匹配。此方法类似于对‘pook.MatcherEngine的委托 | ✗ | ✗ |'
- en: '| Return a string summarizing the call stack. | ✓ | ✗ |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 返回总结调用堆栈的字符串。 | ✓ | ✗ |'
- en: '| Runs :attr:‘executable‘ with “input“ as stdin. :class:‘AssetHandlerError‘
    exception is raised, if execution is failed, otherwise stdout is returned. | ✓
    | ✓ |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 使用“input”作为标准输入运行:attr:‘executable‘。如果执行失败，将引发 :class:‘AssetHandlerError‘
    异常，否则返回stdout。 | ✓ | ✓ |'
- en: '| Determines if a given Auth header is from the Bot Framework Emulator | ✓
    | ✓ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 确定给定的Auth头是否来自Bot Framework Emulator | ✓ | ✓ |'
- en: '| Format level str | ✗ | ✗ |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 格式化级别str | ✗ | ✗ |'
- en: '| Deeply updates a dictionary. List values are concatenated. | ✓ | ✓ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 深度更新字典。列表值会被连接。 | ✓ | ✓ |'
- en: '| Obtain the reconstruction error for the input test_data. | ✓ | ✗ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 获取输入测试数据的重建误差。 | ✓ | ✗ |'
- en: '| Runs the model to generate multivariate normal distribution. | ✓ | ✓ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 运行模型以生成多变量正态分布。 | ✓ | ✓ |'
- en: '| A performant bulk insert for cx Oracle that uses prepared statements via
    ‘executemany()‘.For best performance, pass in ‘rows‘ as an iterator. | ✓ | ✗ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 对cx Oracle进行高效的批量插入，使用通过‘executemany()‘的预处理语句。为获得最佳性能，将‘rows‘作为迭代器传入。 | ✓
    | ✗ |'
- en: '| Console setup. | ✗ | ✗ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 控制台设置。 | ✗ | ✗ |'
- en: '| Pad dimensions of event tensors for mixture distributions. | ✓ | ✗ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 为混合分布填充事件张量的填充维度。 | ✓ | ✗ |'
- en: '| Try to parse a container type (dict, list, or tuple). | ✗ | ✗ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 尝试解析容器类型（字典、列表或元组）。 | ✗ | ✗ |'
- en: '| Convert number to string guaranteeing result is not in scientific notation.
    | ✓ | ✗ |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 将数字转换为字符串，保证结果不采用科学记数法。 | ✓ | ✗ |'
- en: '| Deserializes the Keras-serialized function. (De)serializing Python functions
    from/to bytecode is unsafe. Therefore we also use the function’s type as an anonymous
    function (’lambda’) or named function in the Python environment (’function’).
    | ✓ | ✓ |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 反序列化 Keras 序列化的函数。（反）序列化 Python 函数从/到字节码是不安全的。因此，我们还将函数的类型作为匿名函数（''lambda''）或
    Python 环境中的命名函数（''function''）。 | ✓ | ✓ |'
- en: '| Return a wrapper for an fd. | ✗ | ✗ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 返回一个 fd 的封装器。 | ✗ | ✗ |'
- en: '| Wrap the context data in a django.template.Context object. | ✓ | ✗ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 将上下文数据包装在一个 django.template.Context 对象中。 | ✓ | ✗ |'
- en: '| Return a Python AST Node for a ‘do‘ expression. | ✗ | ✗ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 返回一个 Python AST 节点，用于 ''do'' 表达式。 | ✗ | ✗ |'
- en: '| Takes a value from Postgres, and converts it to a value that’s safe for JSON/Google
    Cloud Storage/BigQuery. Dates are converted to UTC seconds. Decimals are converted
    to floats. Times are converted to seconds. | ✓ | ✓ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 从 Postgres 中获取一个值，并将其转换为适合 JSON/Google Cloud Storage/BigQuery 的值。日期转换为 UTC
    秒。小数转换为浮点数。时间转换为秒。 | ✓ | ✓ |'
- en: '| Hands-free plotting. | ✓ | ✗ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 免提绘图。 | ✓ | ✗ |'
- en: '| Gets a (single) value matching ‘partial_selector‘. If the partial_selector
    exactly matches a complete selector, the value associated with the complete selector
    is returned. | ✗ | ✗ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 获取一个与 ‘partial_selector‘ 匹配的（单个）值。如果 partial_selector 完全匹配一个完整选择器，则返回与完整选择器关联的值。
    | ✗ | ✗ |'
- en: '| Start a capture process but make sure to catch any errors during this process,
    log them but otherwise ignore them. | ✓ | ✓ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 启动捕获过程，但确保在此过程中捕获任何错误，记录它们，但其他情况忽略。 | ✓ | ✓ |'
- en: '| Return True if okay, raise Exception if not | ✗ | ✗ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 如果正常返回 True，否则抛出异常 | ✗ | ✗ |'
- en: '| Verify a message signature using the specified signing key | ✓ | ✓ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 使用指定的签名密钥验证消息签名 | ✓ | ✓ |'
- en: '| Parses the .nextflow.log file for signatures of pipeline status and sets
    the :attr:‘status_info‘ attribute. | ✓ | ✓ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 解析 .nextflow.log 文件以查找管道状态的签名，并设置 :attr:‘status_info‘ 属性。 | ✓ | ✓ |'
- en: '| Get a queue that allows direct access to the internal buffer. If the dataset
    to be read is chunked, the block size should be a multiple of the chunk size to
    maximise performance. In this case it is best to leave it to the default. When
    cyclicF̄alse, and block size does not divide the dataset evenly, the remainder
    elements will not be returned by the queue. When cyclicT̄rue, the remainder elements
    will be part of a block that wraps around the end and includes element from the
    beginning of the dataset. By default, blocks are returned in the order in which
    they become available. The ordered option will force blocks to be returned in
    on-disk order. | ✓ | ✗ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 获取一个允许直接访问内部缓冲区的队列。如果要读取的数据集是分块的，则块大小应为分块大小的倍数，以最大化性能。在这种情况下，最好将其保留为默认值。当
    cyclicF̄alse，并且块大小不能均匀地划分数据集时，队列将不会返回剩余元素。当 cyclicT̄rue 时，剩余元素将成为一个块的一部分，该块绕过末尾并包含来自数据集开头的元素。默认情况下，块按变得可用的顺序返回。ordered
    选项将强制按磁盘顺序返回块。 | ✓ | ✗ |'
- en: '| Parse module defined in *uri* | ✗ | ✗ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 解析模块中定义的 *uri* | ✗ | ✗ |'
- en: '| Count the objects of a repository. The method returns the total number of
    objects (packed and unpacked) available on the repository. | ✓ | ✓ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 计算存储库中的对象数量。该方法返回存储库中所有对象（打包和未打包）的总数。 | ✓ | ✓ |'
- en: '| Find the function that handles the retrieval of the code | ✓ | ✗ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 查找处理代码检索的函数 | ✓ | ✗ |'
- en: '| List all course roles available to an account, for the passed Canvas account
    ID, including course roles inherited from parent accounts. | ✓ | ✓ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 列出对一个帐户可用的所有课程角色，包括从父帐户继承的课程角色，针对传入的 Canvas 帐户 ID。 | ✓ | ✓ |'
- en: '| Example of printing the current upstream. | ✓ | ✓ |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 打印当前的上游示例。 | ✓ | ✓ |'
- en: '| Convert a field’s content into some valid HTML | ✗ | ✗ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 将字段内容转换为有效的 HTML | ✗ | ✗ |'
- en: '| Execute gerrit command against the archive | ✓ | ✗ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 执行 gerrit 命令以处理归档 | ✓ | ✗ |'
- en: '| Word : TERM &#124; LBRACKET TERM RBRACKET &#124; LBRACKET TERM RBRACKET literal_list
    | ✗ | ✗ |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Word : TERM &#124; LBRACKET TERM RBRACKET &#124; LBRACKET TERM RBRACKET literal_list
    | ✗ | ✗ |'
- en: '| Adds a number of zeros (digital silence) to the AudioSegment (returning a
    new one). | ✓ | ✗ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 向 AudioSegment 添加一系列零（数字静默）（返回一个新的）。 | ✓ | ✗ |'
- en: '| http://stackoverflow.com/questions/29107800. | ✓ | ✗ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| http://stackoverflow.com/questions/29107800。 | ✓ | ✗ |'
- en: Feedback of RepoRift Application
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RepoRift 应用反馈
- en: 'To further promote the application of research techniques, we deployed our
    research methods onto a website called repo-rift.com. To further learn about the
    application of code search in industry, we reached out and showed demos to 52
    researchers and software engineers working for organizations like Microsoft, Google,
    Snap, NASA, Intel, SpaceX, Caterpillar, Cisco, John Deere, Capital One, and so
    on along with research groups from some of the top universities in the world.
    Our major feedback from the demo expressed the desire for an application like
    repo-rift.com to allow easy access code search to anaybody. Such an outreach underscores
    the need for increased research in the code search space to create constantly
    improving solutions. Note: Due to limitations in deployment, some of the bigger
    repositories that can be run locally in our environment cannot be run on repo-rift.com
    (these issues are being addressed).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步推广研究技术的应用，我们将研究方法部署到一个名为repo-rift.com的网站上。为了进一步了解代码搜索在行业中的应用，我们联系并展示了演示给了52位在微软、谷歌、Snap、NASA、英特尔、SpaceX、卡特彼勒、思科、约翰·迪尔、Capital
    One等组织工作的研究人员和软件工程师，以及一些世界顶级大学的研究小组。我们从演示中获得的主要反馈是希望像repo-rift.com这样的应用能够让任何人轻松访问代码搜索。这种外展活动强调了在代码搜索领域增加研究的必要性，以创造不断改进的解决方案。注意：由于部署的限制，一些可以在本地环境中运行的大型仓库无法在repo-rift.com上运行（这些问题正在解决中）。
