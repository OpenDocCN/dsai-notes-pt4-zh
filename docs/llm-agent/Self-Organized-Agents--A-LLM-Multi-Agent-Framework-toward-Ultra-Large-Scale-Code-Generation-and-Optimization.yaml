- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:48:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale
    Code Generation and Optimization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自组织代理：一个LLM多代理框架旨在超大规模代码生成与优化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.02183](https://ar5iv.labs.arxiv.org/html/2404.02183)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.02183](https://ar5iv.labs.arxiv.org/html/2404.02183)
- en: Yoichi Ishibashi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 石桥洋一
- en: TsukushiAI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TsukushiAI
- en: ishibashi.tsukushiai@gmail.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ishibashi.tsukushiai@gmail.com
- en: '&Yoshimasa Nishimura'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&西村义正'
- en: TsukushiAI
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: TsukushiAI
- en: nishimura.tsukushiai@gmail.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: nishimura.tsukushiai@gmail.com
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advancements in automatic code generation using large language model
    (LLM) agent have brought us closer to the future of automated software development.
    However, existing single-agent approaches face limitations in generating and improving
    large-scale, complex codebases due to constraints in context length. To tackle
    this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel
    multi-agent framework that enables the scalable and efficient generation and optimization
    of large-scale code. In SoA, self-organized agents operate independently to generate
    and modify code components while seamlessly collaborating to construct the overall
    codebase. A key feature of our framework is the automatic multiplication of agents
    based on problem complexity, allowing for dynamic scalability. This enables the
    overall code volume to be increased indefinitely according to the number of agents,
    while the amount of code managed by each agent remains constant. We evaluate SoA
    on the HumanEval benchmark and demonstrate that, compared to a single-agent system,
    each agent in SoA handles significantly less code, yet the overall generated code
    is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline
    by 5% in terms of Pass@1 accuracy. ¹¹1Our code will be available at [https://github.com/tsukushiAI/self-organized-agent](https://github.com/tsukushiAI/self-organized-agent).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLM）代理的自动代码生成的最新进展让我们更接近自动化软件开发的未来。然而，现有的单代理方法在生成和优化大规模、复杂代码库时面临由于上下文长度限制的局限性。为了应对这一挑战，我们提出了自组织多代理框架（SoA），这是一种新颖的多代理框架，能够实现大规模代码的可扩展和高效生成与优化。在SoA中，自组织代理独立操作以生成和修改代码组件，同时无缝协作以构建整体代码库。我们框架的一个关键特性是根据问题复杂性自动增加代理的数量，实现动态可扩展性。这使得整体代码量可以根据代理数量无限增加，而每个代理管理的代码量保持不变。我们在HumanEval基准测试上评估了SoA，并证明与单代理系统相比，SoA中的每个代理处理的代码量显著减少，但整体生成的代码量大幅增加。此外，SoA在Pass@1准确性方面超越了强大的单代理基准5%。¹¹1我们的代码将会在[https://github.com/tsukushiAI/self-organized-agent](https://github.com/tsukushiAI/self-organized-agent)上发布。
- en: 'Self-Organized Agents: A LLM Multi-Agent Framework toward'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织代理：一个LLM多代理框架旨在
- en: Ultra Large-Scale Code Generation and Optimization
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 超大规模代码生成与优化
- en: Yoichi Ishibashi TsukushiAI ishibashi.tsukushiai@gmail.com                       
    Yoshimasa Nishimura TsukushiAI nishimura.tsukushiai@gmail.com
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 石桥洋一 TsukushiAI ishibashi.tsukushiai@gmail.com                        西村义正 TsukushiAI
    nishimura.tsukushiai@gmail.com
- en: §​ 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: §​ 1 引言
- en: In recent years, research on agents using Large Language Models (LLMs) Brown
    et al. ([2020](#bib.bib3)); OpenAI ([2023](#bib.bib16)); Touvron et al. ([2023](#bib.bib21)),
    such as ReAct Yao et al. ([2023b](#bib.bib24)), Reflexion Shinn et al. ([2023](#bib.bib20)),
    Toolformer Schick et al. ([2023](#bib.bib18)), and AutoGPT ²²2[https://github.com/Significant-Gravitas/](https://github.com/Significant-Gravitas/),
    has been expanding the possibilities of automating human tasks. These advancements
    have particularly contributed to the rapid development of automatic code generation
    techniques in the field of automated application and tool development Hong et al.
    ([2023](#bib.bib7)); Dong et al. ([2023](#bib.bib5)); Huang et al. ([2023](#bib.bib8)).
    Compared to non-agent-based methods Muennighoff et al. ([2023](#bib.bib14)); Li
    et al. ([2023b](#bib.bib11)), these research achievements have led to remarkable
    performance improvements in automatic code generation Zhong et al. ([2024](#bib.bib26));
    Zhou et al. ([2023](#bib.bib27)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，使用大型语言模型（LLMs）的代理研究已经扩展了自动化人类任务的可能性，如Brown等（[2020](#bib.bib3)）；OpenAI（[2023](#bib.bib16)）；Touvron等（[2023](#bib.bib21)），例如ReAct
    Yao等（[2023b](#bib.bib24)），Reflexion Shinn等（[2023](#bib.bib20)），Toolformer Schick等（[2023](#bib.bib18)），以及AutoGPT²²2
    [https://github.com/Significant-Gravitas/](https://github.com/Significant-Gravitas/)。这些进展尤其促进了自动应用和工具开发领域自动代码生成技术的快速发展
    Hong等（[2023](#bib.bib7)）；Dong等（[2023](#bib.bib5)）；Huang等（[2023](#bib.bib8)）。与非代理方法
    Muennighoff等（[2023](#bib.bib14)）；Li等（[2023b](#bib.bib11)）相比，这些研究成果在自动代码生成方面取得了显著的性能提升
    Zhong等（[2024](#bib.bib26)）；Zhou等（[2023](#bib.bib27)）。
- en: '![Refer to caption](img/a4b8e14d1d13838ae3d4102c6d7a1b6b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4b8e14d1d13838ae3d4102c6d7a1b6b.png)'
- en: 'Figure 1: Left (single agent): A single agent is solely responsible for the
    entire implementation. As the codebase grows larger, the load increases for code
    generation, modification, and memory management, making it difficult to manage
    and develop. The larger the entire codebase becomes, the more it puts pressure
    on the context length during self-debugging, limiting the amount of code that
    can be managed. Right (SoA): The implementation is distributed among multiple
    agents. The agents are independent; code generation, modification, and memory
    management are separated from other agents. Each agent manages only its own part,
    allowing it to focus on the implementation regardless of the complexity of the
    entire codebase. Furthermore, agents automatically multiply according to the complexity
    of the problem. This allows for the generation and modification of complex and
    large-scale code while maintaining a constant amount of code management/generation/modification
    per agent.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：左侧（单一代理）：一个单一代理负责整个实现。随着代码库的增长，代码生成、修改和内存管理的负担增加，使得管理和开发变得困难。整个代码库越大，自我调试时对上下文长度的压力就越大，限制了能够管理的代码量。右侧（SoA）：实现分布在多个代理之间。代理是独立的；代码生成、修改和内存管理与其他代理分开。每个代理只管理自己的部分，使其能够专注于实现，无论整个代码库的复杂性如何。此外，代理会根据问题的复杂性自动增加。这允许生成和修改复杂的大规模代码，同时保持每个代理的代码管理/生成/修改量恒定。
- en: Most recent research has focused on single-agent approaches for code generation.
    These single-agent code generation methods face limitations, especially in terms
    of scalability, when the implementation becomes complex and requires a large codebase.
    The main reason for this technical difficulty is that a single agent must manage
    the entire code generation process alone. For instance, implementing a machine
    learning algorithm involves several stages, such as data preprocessing, algorithm
    training, and result evaluation, which include many functions and classes. When
    these complex components are combined, the codebase inevitably becomes very large.
    However, there are limitations to the context length of LLMs, and as the number
    of input tokens increases, the inference performance decreases Levy et al. ([2024](#bib.bib9));
    Shaham et al. ([2023](#bib.bib19)); Li et al. ([2023a](#bib.bib10)). Consistently
    understanding and generating or modifying appropriate code for such an extensive
    codebase poses a significant challenge for a single agent in terms of comprehending
    and managing the context. Consequently, the single-agent approach struggles to
    efficiently generate and modify code as its complexity and size increase.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究大多集中在单智能体代码生成方法上。这些单智能体代码生成方法在实施变得复杂并且需要大规模代码库时面临限制。技术难点的主要原因是单个智能体必须独自管理整个代码生成过程。例如，实现一个机器学习算法涉及多个阶段，如数据预处理、算法训练和结果评估，这些阶段包含许多函数和类。当这些复杂组件组合在一起时，代码库不可避免地变得非常庞大。然而，LLMs
    的上下文长度有限，随着输入 token 数量的增加，推理性能下降 Levey 等人 ([2024](#bib.bib9))；Shaham 等人 ([2023](#bib.bib19))；Li
    等人 ([2023a](#bib.bib10))。在如此广泛的代码库中持续理解、生成或修改适当的代码对单个智能体而言是一个巨大的挑战，涉及到理解和管理上下文。因此，随着复杂性和规模的增加，单智能体方法在高效生成和修改代码方面面临困难。
- en: 'To tackle these challenges, we propose a self-organized multi agent framework
    that can automatically generate and modify large-scale code ([Figure 1](#S1.F1
    "Figure 1 ‣ §​ 1 Introduction ‣ Self-Organized Agents: A LLM Multi-Agent Framework
    toward Ultra Large-Scale Code Generation and Optimization")). *Self-organization* Ashby
    ([1947](#bib.bib2)) is a phenomenon in which living organisms or matter create
    an orderly, large structure as a result of their individual autonomous behaviors,
    despite lacking the ability to oversee the entire system. In our framework, self-organized
    agents, each responsible for different code parts or tasks, independently generate
    and modify code. With the self-organization of agents, a single agent no longer
    needs to comprehend the entire codebase, making it possible to scale up large-scale
    code simply by increasing the number of agents. Another feature of our framework
    is that agents automatically multiply according to the complexity of the problem,
    allowing the overall codebase to expand while keeping the amount of code handled
    by each agent constant. These features enable the dynamic and flexible generation
    and modification of large-scale code, which was impossible with the traditional
    single-agent approach.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对这些挑战，我们提出了一种自组织的多智能体框架，该框架能够自动生成和修改大规模代码（[图 1](#S1.F1 "Figure 1 ‣ §​ 1
    Introduction ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra
    Large-Scale Code Generation and Optimization")）。*自组织*的概念由 Ashby ([1947](#bib.bib2))
    提出，是指尽管缺乏对整个系统的监督能力，生物体或物质能够通过各自的自主行为创建有序的大型结构。在我们的框架中，自组织的智能体，每个智能体负责不同的代码部分或任务，独立生成和修改代码。通过智能体的自组织，单个智能体不再需要理解整个代码库，从而可以通过增加智能体的数量来扩展大规模代码。我们框架的另一个特点是，智能体会根据问题的复杂性自动增殖，使得整个代码库能够扩展，同时保持每个智能体处理的代码量不变。这些特点使得大规模代码的动态和灵活生成与修改成为可能，这是传统单智能体方法无法实现的。'
- en: 'In our experiments, we evaluated the performance of this framework using HumanEval Chen
    et al. ([2021](#bib.bib4)), a benchmark for code generation. The results show
    that our self-organized multi-agent framework outperformed Reflexion Shinn et al.
    ([2023](#bib.bib20)), an existing powerful code generation agent ([§​ 4.1](#S4.SS1
    "§​ 4.1 Main Results ‣ §​ 4 Experiments ‣ Self-Organized Agents: A LLM Multi-Agent
    Framework toward Ultra Large-Scale Code Generation and Optimization")), demonstrating
    the effectiveness of our approach in generating and modifying code. Furthermore,
    through a detailed analysis of the experimental results, we revealed how agents
    automatically multiply according to the complexity of the problem, effectively
    scaling up the overall code volume while keeping the code generation per agent
    constant ([§​ 4.2](#S4.SS2 "§​ 4.2 Analysis ‣ §​ 4 Experiments ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization")). These experimental results support the contribution of our framework,
    which overcomes the scalability issues faced by single-agent approaches and provides
    a solution capable of handling larger projects.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用HumanEval Chen et al.（[2021](#bib.bib4)），一个代码生成的基准，评估了该框架的性能。结果表明，我们的自组织多代理框架优于Reflexion
    Shinn et al.（[2023](#bib.bib20)），一个现有的强大代码生成代理（[§​ 4.1](#S4.SS1 "§​ 4.1 主要结果 ‣
    §​ 4 实验 ‣ 自组织代理：面向超大规模代码生成和优化的LLM多代理框架")），显示了我们方法在生成和修改代码方面的有效性。此外，通过对实验结果的详细分析，我们揭示了代理如何根据问题的复杂性自动增殖，有效扩大整体代码量，同时保持每个代理的代码生成量不变（[§​
    4.2](#S4.SS2 "§​ 4.2 分析 ‣ §​ 4 实验 ‣ 自组织代理：面向超大规模代码生成和优化的LLM多代理框架")）。这些实验结果支持了我们的框架的贡献，它克服了单代理方法面临的可扩展性问题，并提供了一个能够处理更大项目的解决方案。
- en: §​ 2 Code Generation Task
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: §​ 2 代码生成任务
- en: The code generation task involves generating Python functions from docstrings Chen
    et al. ([2021](#bib.bib4)). In this task, an agent is given a docstring that defines
    the types of the function’s inputs and expected outputs, as well as the specific
    requirements that the function should meet. The agent is then required to generate
    the code for a function that fulfills the specified functionality. The generated
    code is verified for accuracy using unit tests, and the quality of the code is
    evaluated based on its ability to pass the test cases. As with previous studies Shinn
    et al. ([2023](#bib.bib20)); Zhong et al. ([2024](#bib.bib26)); Zhou et al. ([2023](#bib.bib27)),
    we use the evaluation metric Pass@$1$ code samples pass all test cases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成任务涉及从docstrings生成Python函数 Chen et al.（[2021](#bib.bib4)）。在此任务中，代理会收到一个定义函数输入类型和期望输出的docstring，以及函数应满足的具体要求。然后，代理需要生成一个实现指定功能的函数代码。生成的代码通过单元测试进行准确性验证，并根据其通过测试用例的能力评估代码质量。与之前的研究
    Shinn et al.（[2023](#bib.bib20)）； Zhong et al.（[2024](#bib.bib26)）； Zhou et al.（[2023](#bib.bib27)）类似，我们使用评估指标Pass@$1$代码样本通过所有测试用例。
- en: §​ 3 Self-organized Agent Framework
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: §​ 3 自组织代理框架
- en: Our Self-organized Agents (SoA) framework enables efficient implementation of
    large-scale and complex code by having self-organized agents independently generate
    and modify small-scale and simple code. In this section, we introduce the important
    components of SoA, namely the agents and the layers responsible for more abstract
    processing than the agents, and finally introduce the code generation and modification
    protocols in the SoA framework.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自组织代理（SoA）框架通过让自组织代理独立生成和修改小规模和简单代码，从而实现大规模和复杂代码的高效实现。在本节中，我们介绍了SoA的重要组成部分，即代理和负责比代理更抽象处理的层，最后介绍了SoA框架中的代码生成和修改协议。
- en: §​ 3.1 Child Agent
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: §​ 3.1 子代理
- en: 'Child agents implement a given function based on its docstrings. As shown in
    [Figure 2](#S3.F2 "Figure 2 ‣ §​ 3.1 Child Agent ‣ §​ 3 Self-organized Agent Framework
    ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale
    Code Generation and Optimization"), this agent has a simple structure consisting
    of two elements: an LLM and memory. The LLM generates code from the given docstrings
    and modifies the code based on the results of unit tests. The memory stores the
    code generated by the agent itself and retrieves the latest code to be input to
    the LLM along with the unit test feedback during code modification. If an agent
    has these minimal specifications, it is possible to use an off-the-shelf agents
    (e.g., Reflexion) as a Child agent. We deliberately use a simple agent to verify
    the effectiveness of SoA in a simple setup.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '子代理根据其文档字符串实现给定函数。如[图 2](#S3.F2 "Figure 2 ‣ §​ 3.1 Child Agent ‣ §​ 3 Self-organized
    Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra
    Large-Scale Code Generation and Optimization")所示，该代理具有由两个元素组成的简单结构：一个 LLM 和内存。LLM
    根据给定的文档字符串生成代码，并根据单元测试的结果修改代码。内存存储由代理本身生成的代码，并在代码修改过程中检索最新代码以输入 LLM，同时接收单元测试反馈。如果一个代理具有这些最小规格，可以使用现成的代理（例如，Reflexion）作为子代理。我们故意使用简单代理来验证
    SoA 在简单设置中的有效性。'
- en: '![Refer to caption](img/084c52283bad4959cfc34fa9d62a7e41.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/084c52283bad4959cfc34fa9d62a7e41.png)'
- en: 'Figure 2: Overview of code generation. Child agents generate executable Python
    function from a given docstring. The Mother agent generates the skeleton of the
    function. The Mother spawns a new initialized agent (Child or Mother) and delegates
    unimplemented functions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：代码生成概述。子代理从给定的文档字符串生成可执行的 Python 函数。母代理生成函数的骨架。母代理生成一个新的初始化代理（子代理或母代理）并委派未实现的函数。
- en: Code Generation
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码生成
- en: 'The main role of Child agents is to generate functions that meet the specifications
    based on the given function’s docstrings. As shown in [Figure 2](#S3.F2 "Figure
    2 ‣ §​ 3.1 Child Agent ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization"), the agent follows the instructions to generate the rest of the
    function and complete it. The completed function implementation is stored in memory,
    and the unit tests for the function are also stored as they form the basis for
    future code modifications.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '子代理的主要作用是根据给定函数的文档字符串生成符合规范的函数。如[图 2](#S3.F2 "Figure 2 ‣ §​ 3.1 Child Agent
    ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent
    Framework toward Ultra Large-Scale Code Generation and Optimization")所示，代理按照指令生成剩余的函数并完成它。完成的函数实现被存储在内存中，函数的单元测试也被存储，因为它们构成了未来代码修改的基础。'
- en: 'Code Modification: Empowering Child Agents with Self-Organization and Adaptability'
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码修改：赋予子代理自我组织和适应性的能力
- en: 'One of the most remarkable aspects of agents in the SoA framework is their
    ability to autonomously improve their code based on the state of nearby agents
    . This process sets SoA apart from traditional agent approaches and showcases
    the power of self-organization in code modification. While existing agents like
    Reflexion Shinn et al. ([2023](#bib.bib20)) rely solely on the results of unit
    tests, Child agents in SoA go beyond this limitation by independently observing
    the state of their mother agent, such as differences in modifications and feedback.
    By gathering this invaluable information from their surrounding environment, Child
    agents can adapt their behavior and make more informed decisions about code modification,
    even without explicit instructions. The modifications and feedback generated by
    the Mother agent serve as an important source of information for the Child agents.
    Armed with these insights, Child agents can more effectively modify their own
    code, contributing to the overall improvement of the codebase in a way that is
    both efficient and adaptive. [Figure 3](#S3.F3 "Figure 3 ‣ §​ 3.2 Mother Agent
    ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent
    Framework toward Ultra Large-Scale Code Generation and Optimization") illustrates
    this process, which begins with the execution of unit tests and the retrieval
    of the latest implementation from memory. The Child agent then harnesses the power
    of the LLM to create a code modification proposal, seamlessly combining the information
    observed from the Mother agent with the test results and the latest implementation
    details. By storing the modified code in memory, Child agents create a feedback
    loop that continuously refines and improves the codebase over time. This iterative
    process, driven by the principles of self-organization and adaptability, enables
    SoA to tackle complex code modification tasks with efficiency and effectiveness.
    As Child agents work in harmony with their Mother agent, they contribute to the
    creation of a more optimized and large codebase.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SoA 框架中代理最显著的一个方面是其基于附近代理的状态自主改进代码的能力。这个过程使 SoA 与传统代理方法区别开来，并展示了代码修改中的自我组织能力。虽然像
    Reflexion Shinn 等（[2023](#bib.bib20)）的现有代理仅依赖于单元测试的结果，但 SoA 中的子体代理超越了这一限制，通过独立观察其母体代理的状态，例如修改和反馈的差异。通过从其周围环境中收集这些宝贵的信息，子体代理能够调整其行为并做出更明智的代码修改决策，即使没有明确的指示。母体代理生成的修改和反馈作为子体代理的重要信息来源。凭借这些洞察力，子体代理能够更有效地修改自己的代码，从而以高效和适应的方式促进代码库的整体改进。
    [图 3](#S3.F3 "图 3 ‣ §​ 3.2 母体代理 ‣ §​ 3 自组织代理框架 ‣ 自组织代理：一个面向超大规模代码生成和优化的 LLM 多代理框架")
    展示了这一过程，开始于单元测试的执行和从记忆中检索最新实现。子体代理随后利用 LLM 的能力创建代码修改提案，将从母体代理观察到的信息与测试结果和最新的实现细节无缝结合。通过将修改后的代码存储在记忆中，子体代理创建了一个反馈循环，持续地细化和改进代码库。这一迭代过程，由自我组织和适应性原则驱动，使得
    SoA 能够以高效和有效的方式处理复杂的代码修改任务。当子体代理与其母体代理和谐工作时，它们有助于创建一个更优化和大规模的代码库。
- en: §​ 3.2 Mother Agent
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: §​ 3.2 母体代理
- en: The Mother is an agent that generates new agents (Mother or Child). Similar
    to Child agents, the Mother agent independently implements the specific Python
    function based on its given docstrings. The Mother has memory, code generation
    capabilities, and self-debugging functions, as same as Child agents. The unique
    feature of the Mother agent is its ability to generate multiple Child agents according
    to the complexity of the problem and delegate parts of the implementation to these
    agents. This structure allows the Mother agent to focus on implementing abstract
    processes, while the Child agents generated by the Mother agent concentrate on
    implementing concrete processes. This division of labor enhances the overall efficiency
    and flexibility of the SoA framework.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 母体是一个生成新代理（母体或子体）的代理。类似于子体代理，母体代理根据其给定的文档字符串独立实现特定的 Python 函数。母体具有记忆、代码生成能力和自我调试功能，与子体代理相同。母体代理的独特功能在于其根据问题的复杂性生成多个子体代理，并将部分实现委托给这些代理。这种结构使得母体代理能够专注于实现抽象过程，而由母体代理生成的子体代理则专注于实现具体过程。这种分工提高了
    SoA 框架的整体效率和灵活性。
- en: '![Refer to caption](img/80e8aff588bcfc94d125f6ad72149257.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/80e8aff588bcfc94d125f6ad72149257.png)'
- en: 'Figure 3: Overview of code modification. Agents (Mother/Child) observe the
    state of Mother (feedback, old code, and updated code) and use this information
    to improve the functions for which they are responsible. The state of the upper
    agent is used to modify code by lower agents within the hierarchy. This state
    propagation promotes collaboration and information sharing throughout the hierarchy,
    enabling efficient code modification.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：代码修改概览。代理（母代理/子代理）观察母代理的状态（反馈、旧代码和更新的代码），并利用这些信息来改进它们负责的功能。上级代理的状态被用于修改层级内下级代理的代码。这种状态传播促进了整个层级的协作和信息共享，从而实现了高效的代码修改。
- en: Code Generation
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码生成
- en: 'We explain the code generation process by the Mother agent using the implementation
    example of the is_sum_of_odds_ten function shown in [Figure 2](#S3.F2 "Figure
    2 ‣ §​ 3.1 Child Agent ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization"). The starting point is the function’s docstrings and unit tests,
    which are memorized for reference in the later self-debugging phase. The first
    task of the Mother agent is to generate a skeleton of the implementation from
    the given docstrings, including subfunctions such as get_odd_numbers to extract
    odd numbers and sum_of_numbers to calculate their sum. The number and types of
    these subfunctions are automatically determined by the LLM based on the complexity
    of the problem.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过[图2](#S3.F2 "图2 ‣ §​ 3.1 子代理 ‣ §​ 3 自组织代理框架 ‣ 自组织代理：一个面向超大规模代码生成和优化的LLM多代理框架")中展示的is_sum_of_odds_ten函数的实现示例来解释母代理的代码生成过程。起点是函数的文档字符串和单元测试，它们会被记忆以便在后续自我调试阶段参考。母代理的第一项任务是从给定的文档字符串中生成一个实现框架，包括如get_odd_numbers提取奇数和sum_of_numbers计算它们的和等子函数。这些子函数的数量和类型由LLM根据问题的复杂性自动确定。
- en: It is important to note that these subfunctions are unimplemented, and the Mother
    agent does not directly implement them. Instead, it delegates the implementation
    of the subfunctions to other agents, allowing the Mother agent to focus on generating
    the skeleton and streamline its own code generation process. After the docstrings
    and unit tests for the subfunctions are generated, they are assigned to newly
    initialized agents for implementation. These agents proceed with the implementation
    of their respective functions without looking at the internals of the is_sum_of_odds_ten
    function implemented by the Mother agent. Since agents within the same Mother
    can work asynchronously, the overall code generation process is streamlined.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这些子函数尚未实现，母代理不会直接实现它们。相反，它将子函数的实现委托给其他代理，从而使母代理能够专注于生成框架并优化其自身的代码生成过程。在生成子函数的文档字符串和单元测试后，它们会被分配给新初始化的代理进行实现。这些代理在不查看母代理实现的is_sum_of_odds_ten函数内部的情况下进行各自函数的实现。由于同一母代理下的代理可以异步工作，因此整体代码生成过程得以简化。
- en: Code Modification
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码修改
- en: 'The Mother’s code modification is almost the same as the Child’s code modification
    ([Figure 3](#S3.F3 "Figure 3 ‣ §​ 3.2 Mother Agent ‣ §​ 3 Self-organized Agent
    Framework ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale
    Code Generation and Optimization")). It observes information from the upper Mother
    and uses it to modify the functions it is responsible for. The only difference
    is that the feedback it generates and the code before and after modification are
    used by lower-level agents (Child or Mother).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 母代理的代码修改与子代理的代码修改几乎相同（[图3](#S3.F3 "图3 ‣ §​ 3.2 母代理 ‣ §​ 3 自组织代理框架 ‣ 自组织代理：一个面向超大规模代码生成和优化的LLM多代理框架")）。它观察来自上级母代理的信息，并利用这些信息修改它负责的函数。唯一的不同之处在于，它生成的反馈以及修改前后的代码由下级代理（子代理或母代理）使用。
- en: §​ 3.3 Self-organized Agent Process
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: §​ 3.3 自组织代理过程
- en: 'The Self-organized Agent (SoA) framework is a distributed framework in which
    multiple agents (including Mother agents and Child agents) repeatedly generate
    and modify functions. The core of this framework lies in the principle of self-organization,
    where each agent functions independently without the need to directly observe
    the entire codebase. The hierarchical combination of Mother agents and Child agents
    forms an agent network that effectively constructs a single large-scale codebase.
    In this hierarchical structure, Mother agents decompose complex problems into
    more manageable smaller problems by dividing tasks and delegating them to the
    agents they have generated. Although each agent is independent, the agents as
    a whole can work efficiently towards the implementation of a single function.
    Despite the fact that the amount of code each agent generates, modifies, and manages
    is always small, the number of agents scales, allowing the amount of code generated
    to be increased indefinitely according to the difficulty of the problem. Detailed
    algorithms are presented in Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Pseudocode
    ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale
    Code Generation and Optimization") in the appendix.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织代理（SoA）框架是一个分布式框架，其中多个代理（包括母代理和子代理）反复生成和修改功能。这个框架的核心在于自组织原则，每个代理独立运行，无需直接观察整个代码库。母代理和子代理的层级组合形成了一个代理网络，有效地构建了一个单一的大规模代码库。在这个层级结构中，母代理通过划分任务并将其委派给其生成的代理，将复杂问题分解为更可管理的小问题。尽管每个代理是独立的，但代理整体可以有效地朝着实现单一功能的方向工作。尽管每个代理生成、修改和管理的代码量始终较小，但代理的数量可以扩展，使得生成的代码量可以根据问题的难度无限增加。详细的算法见附录中的算法[1](#alg1
    "算法 1 ‣ 附录 A 伪代码 ‣ 自组织代理：面向超大规模代码生成和优化的LLM多代理框架")。
- en: '![Refer to caption](img/7601a83d10590119720eefda9b656042.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7601a83d10590119720eefda9b656042.png)'
- en: 'Figure 4: Overview of the SoA framework. Mother agents and Child agents hierarchically
    construct a network and perform function generation and modification. Mother agents
    delegate tasks to other Mother agents or Child agents, and each agent independently
    executes tasks while effectively implementing a single function as a whole.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：SoA 框架概述。母代理和子代理以层级方式构建网络，进行功能生成和修改。母代理将任务委派给其他母代理或子代理，每个代理独立执行任务，同时整体有效地实现单一功能。
- en: Code Generation
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码生成
- en: 'The code generation process in the SoA framework begins with the function’s
    docstrings and unit tests. In the initial stage, there is only one initialized
    Mother agent, which is the root of the tree structure. Based on the input docstrings
    and unit tests, it generates docstrings and unit tests for subtasks and passes
    them to other agents it generates (see [§​ 3.2](#S3.SS2 "§​ 3.2 Mother Agent ‣
    §​ 3 Self-organized Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent
    Framework toward Ultra Large-Scale Code Generation and Optimization")). If the
    tree structure reaches a predetermined depth, the tasks are passed to Child agents;
    otherwise, they are passed to newly generated Mother agents. By repeatedly proliferating
    and increasing the number of agents until the last agent, it is possible to generate
    large-scale code while keeping the amount of code managed by individual agents
    constant.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SoA 框架中的代码生成过程以函数的 docstrings 和单元测试开始。在初始阶段，只有一个初始化的母代理，它是树结构的根。根据输入的 docstrings
    和单元测试，它生成子任务的 docstrings 和单元测试，并将其传递给它生成的其他代理（见[§ 3.2](#S3.SS2 "§ 3.2 母代理 ‣ §
    3 自组织代理框架 ‣ 自组织代理：面向超大规模代码生成和优化的LLM多代理框架")）。如果树结构达到预定的深度，任务将传递给子代理；否则，任务将传递给新生成的母代理。通过反复繁殖和增加代理数量，直到最后一个代理，可以在保持每个代理管理的代码量不变的情况下生成大规模代码。
- en: Code Modification
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码修改
- en: 'Once code generation is complete, the process transitions to the code modification
    phase. First, the implementations of all agents are combined to create the final
    implementation. This final implementation is evaluated using the unit tests provided
    to the root Mother, and feedback is generated from the results. Since there are
    no agents higher than this root Mother, information from higher-level agents as
    shown in [Figure 3](#S3.F3 "Figure 3 ‣ §​ 3.2 Mother Agent ‣ §​ 3 Self-organized
    Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra
    Large-Scale Code Generation and Optimization") is not used. The modification process
    starts based on this feedback and propagates information from the root Mother
    agent to the Child agents. Each agent updates its implementation based on the
    received feedback, generates new feedback, and transmits it to lower-level agents
    (see [§​ 3.2](#S3.SS2 "§​ 3.2 Mother Agent ‣ §​ 3 Self-organized Agent Framework
    ‣ Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale
    Code Generation and Optimization")). Finally, the Child agents update their own
    implementations, and the process terminates (see [§​ 3.1](#S3.SS1 "§​ 3.1 Child
    Agent ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent
    Framework toward Ultra Large-Scale Code Generation and Optimization")). This series
    of processes is repeated until a predetermined maximum number of iterations is
    reached.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦代码生成完成，流程就会过渡到代码修改阶段。首先，将所有代理的实现合并以创建最终实现。这个最终实现使用提供给根母代理的单元测试进行评估，并根据结果生成反馈。由于没有比这个根母代理更高级的代理，信息不会来自如[图3](#S3.F3
    "Figure 3 ‣ §​ 3.2 Mother Agent ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization")所示的更高级别代理。修改过程基于这个反馈开始，并将信息从根母代理传播到子代理。每个代理根据收到的反馈更新其实现，生成新的反馈，并将其传递给较低级别的代理（见[§​
    3.2](#S3.SS2 "§​ 3.2 Mother Agent ‣ §​ 3 Self-organized Agent Framework ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization")）。最后，子代理更新自己的实现，过程终止（见[§​ 3.1](#S3.SS1 "§​ 3.1 Child Agent ‣ §​
    3 Self-organized Agent Framework ‣ Self-Organized Agents: A LLM Multi-Agent Framework
    toward Ultra Large-Scale Code Generation and Optimization")）。这系列过程会重复，直到达到预定的最大迭代次数。'
- en: §​ 4 Experiments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: §​ 4 实验
- en: LLM Selection
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM选择
- en: We used GPT3.5-turbo³³3gpt3.5-turbo-1106 for code generation and feedback generation.⁴⁴4GPT-4
    was not selected due to the high experimental cost required.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`GPT3.5-turbo³³3gpt3.5-turbo-1106`进行代码生成和反馈生成。⁴⁴由于实验成本过高，未选择GPT-4。
- en: Baselines
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准线
- en: We compare SoA with several state-of-the-art code generation methods including
    AlphaCode Li et al. ([2022](#bib.bib12)), Incoder Fried et al. ([2023](#bib.bib6)),
    Codex Chen et al. ([2021](#bib.bib4)), CoT Wei et al. ([2022](#bib.bib22)), and
    Gemini Pro Anil et al. ([2023](#bib.bib1)). Additionally, we evaluate the performance
    of various GPT-3.5-based agents, such as ChatGPT, Self-Edit Zhang et al. ([2023](#bib.bib25)),
    and Reflexion Shinn et al. ([2023](#bib.bib20)). These baselines are chosen to
    represent a diverse range of approaches, including single-agent and multi-agent
    systems, as well as those with and without self-debugging capabilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SoA与几种最先进的代码生成方法进行比较，包括AlphaCode Li et al. ([2022](#bib.bib12))、Incoder Fried
    et al. ([2023](#bib.bib6))、Codex Chen et al. ([2021](#bib.bib4))、CoT Wei et al.
    ([2022](#bib.bib22))以及Gemini Pro Anil et al. ([2023](#bib.bib1))。此外，我们还评估了各种基于GPT-3.5的代理的性能，如ChatGPT、Self-Edit
    Zhang et al. ([2023](#bib.bib25))和Reflexion Shinn et al. ([2023](#bib.bib20))。这些基准线选择了多样化的方法，包括单代理和多代理系统，以及具有和不具有自我调试功能的方法。
- en: Agent Configuration
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理配置
- en: To evaluate the effectiveness of the SoA framework, we selected the Reflexion
    agent as a baseline. Reflexion iteratively modifies code based on the given docstrings
    and automatically generated unit tests until it reaches the maximum number of
    iterations or passes the unit tests. The main difference between Reflexion and
    SoA is that Reflexion is composed of a single agent, while SoA is composed of
    self-organized multiple agents. In the SoA configuration, we set the maximum number
    of iterations for the learning loop to 8 and the maximum tree depth to 2. Additionally,
    following Shinn et al. ([2023](#bib.bib20)), we provided a few-shot trajectory
    to the LLM.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 SoA 框架的有效性，我们选择了 Reflexion 代理作为基线。Reflexion 根据给定的文档字符串和自动生成的单元测试迭代修改代码，直到达到最大迭代次数或通过单元测试。Reflexion
    和 SoA 的主要区别在于 Reflexion 由单一代理组成，而 SoA 由自组织的多个代理组成。在 SoA 配置中，我们将学习循环的最大迭代次数设置为
    8，最大树深度设置为 2。此外，参考 Shinn 等 ([2023](#bib.bib20))，我们向 LLM 提供了几次示例轨迹。
- en: Data and Tasks
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据与任务
- en: To evaluate the performance of automatic code generation, we used the HumanEval
    Chen et al. ([2021](#bib.bib4)) benchmark. HumanEval is a set that includes diverse
    programming problems designed to measure the functional correctness of generated
    code. We used the Python language set for evaluation and followed the evaluation
    methodology of Reflexion Shinn et al. ([2023](#bib.bib20)). In this process, multiple
    test cases are created for each generated code, and $n$ test cases are randomly
    selected to construct a test suite. This test suite is used to verify whether
    the generated code functions correctly. We set 6 unit tests for Reflexion and
    1 unit test for SoA.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估自动代码生成的性能，我们使用了 HumanEval Chen 等 ([2021](#bib.bib4)) 基准。HumanEval 是一组包括多样化编程问题的测试集，旨在测量生成代码的功能正确性。我们使用
    Python 语言集进行评估，并遵循 Reflexion Shinn 等 ([2023](#bib.bib20)) 的评估方法。在此过程中，为每个生成的代码创建多个测试用例，并随机选择
    $n$ 个测试用例来构建测试套件。该测试套件用于验证生成的代码是否功能正确。我们为 Reflexion 设置了 6 个单元测试，为 SoA 设置了 1 个单元测试。
- en: '| Method | SD | SO | Pass@1 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SD | SO | Pass@1 |'
- en: '| AlphaCode   Li et al. ([2022](#bib.bib12)) | ✘ | ✘ | 17.1 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| AlphaCode   Li 等 ([2022](#bib.bib12)) | ✘ | ✘ | 17.1 |'
- en: '| Incoder   Fried et al. ([2023](#bib.bib6)) | ✘ | ✘ | 15.2 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Incoder   Fried 等 ([2023](#bib.bib6)) | ✘ | ✘ | 15.2 |'
- en: '| Codex   Chen et al. ([2021](#bib.bib4)) | ✘ | ✘ | 47.0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Codex   Chen 等 ([2021](#bib.bib4)) | ✘ | ✘ | 47.0 |'
- en: '| Gemini Pro   Anil et al. ([2023](#bib.bib1)) | ✘ | ✘ | 67.7 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Gemini Pro   Anil 等 ([2023](#bib.bib1)) | ✘ | ✘ | 67.7 |'
- en: '| GPT-3.5 | CoT Wei et al. ([2022](#bib.bib22)) | ✘ | ✘ | 44.6 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | CoT Wei 等 ([2022](#bib.bib22)) | ✘ | ✘ | 44.6 |'
- en: '| ChatGPT | ✘ | ✘ | 57.3 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | ✘ | ✘ | 57.3 |'
- en: '| Self-Edit Zhang et al. ([2023](#bib.bib25)) | ✔ | ✘ | 62.2 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Self-Edit 张等 ([2023](#bib.bib25)) | ✔ | ✘ | 62.2 |'
- en: '| Reflexion Shinn et al. ([2023](#bib.bib20)) | ✔ | ✘ | 66.5 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion Shinn 等 ([2023](#bib.bib20)) | ✔ | ✘ | 66.5 |'
- en: '| SoA (ours) | ✔ | ✔ | 71.4 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| SoA (我们的方法) | ✔ | ✔ | 71.4 |'
- en: 'Table 1: Results of SoA and baselines on HumanEval. The score of ChatGPT is
    taken from Dong et al. ([2023](#bib.bib5)). SD indicates whether the agent uses
    self-debugging with unit tests, while SO denotes whether the agent employs self-organized
    multi-agent collaboration.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：SoA 和基线在 HumanEval 上的结果。ChatGPT 的分数取自 Dong 等 ([2023](#bib.bib5))。SD 表示代理是否使用了带有单元测试的自我调试，而
    SO 表示代理是否采用了自组织的多代理协作。
- en: §​ 4.1 Main Results
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: §​ 4.1 主要结果
- en: '[Table 1](#S4.T1 "Table 1 ‣ Data and Tasks ‣ §​ 4 Experiments ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization") compares the Pass@1 accuracy of the proposed method and the baseline.
    Comparing SoA with Reflexion, a strong baseline, SoA outperforms Reflexion by
    5% in Pass@1\. Considering that each agent in SoA does not see the entire code,
    this is a surprising result. This result suggests that self-organized agents can
    generate code that functions well as a whole without needing to oversee the entire
    code, by independently implementing the functions assigned to them.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S4.T1 "表1 ‣ 数据与任务 ‣ §​ 4 实验 ‣ 自组织代理：一个面向超大规模代码生成和优化的 LLM 多代理框架") 比较了所提方法和基线的
    Pass@1 准确率。将 SoA 与强基线 Reflexion 进行比较，SoA 在 Pass@1 上超越 Reflexion 5%。考虑到 SoA 中的每个代理看不到整个代码，这一结果令人惊讶。这一结果表明，自组织代理可以生成整体功能良好的代码，而无需监督整个代码，通过独立实现分配给它们的功能。'
- en: §​ 4.2 Analysis
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: §​ 4.2 分析
- en: One of the most critical aspects of our study is the efficiency of the self-organized
    multi-agent approach in large-scale code generation. To showcase the superior
    performance of SoA, we conducted a comprehensive comparative analysis between
    Reflexion, a state-of-the-art single-agent system, and our proposed multi-agent
    system. Using the HumanEval benchmark, we meticulously examined the overall scale
    of the code generated by both systems and the amount of code each agent independently
    generated and memorized. To ensure a fair comparison, we removed comments and
    docstrings from the HumanEval results and focused on the number of characters
    and tokens of pure code.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的一个关键方面是自组织多代理方法在大规模代码生成中的效率。为了展示SoA的优越性能，我们对Reflexion（一种最先进的单一代理系统）和我们提出的多代理系统进行了全面的对比分析。通过使用HumanEval基准，我们细致地检查了两个系统生成的代码的总体规模，以及每个代理独立生成和记忆的代码量。为了确保公平比较，我们从HumanEval结果中去除了注释和文档字符串，专注于纯代码的字符数和标记数。
- en: '[Figure 5](#S4.F5 "Figure 5 ‣ §​ 4.2 Analysis ‣ §​ 4 Experiments ‣ Self-Organized
    Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and
    Optimization") presents a visualization of the average amount of code generated
    by SoA and Reflexion from the perspective of individual functions and all functions.
    In the context of HumanEval, which requires the implementation of a single function,
    SoA’s code amount is calculated by summing the code generated by each agent, while
    Reflexion’s code amount is based on a single function. The *code amount per function*
    in SoA refers to the code generated by each individual agent, whereas in Reflexion,
    it is equivalent to the code amount of a single function. The results unequivocally
    demonstrate SoA’s superiority over Reflexion in terms of the number of tokens
    per final code and the average number of characters per function. What is remarkable
    is that despite each agent in SoA handling significantly fewer tokens/characters
    compared to the single agent in Reflexion, the overall output generated by SoA
    is substantially greater. This finding underscores the exceptional scalability
    of SoA, indicating its ability to handle increasingly complex tasks by seamlessly
    adding more agents to the system. Our results suggest that by increasing the depth
    of the agent hierarchy and introducing more Mother agents, SoA can generate even
    larger-scale code by efficiently distributing the workload among multiple agents.
    As the tree structure becomes deeper, the system exhibits an infinite scaling
    potential, enabling the generation of increasingly complex and extensive codebases
    while ensuring that each agent handles a manageable portion of the code. Each
    agent can maintain a manageable amount of code while theoretically allowing for
    an indefinite increase in the overall code generation capacity.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5](#S4.F5 "图5 ‣ §​ 4.2 分析 ‣ §​ 4 实验 ‣ 自组织代理：一个面向超大规模代码生成与优化的LLM多代理框架")展示了从单个函数和所有函数的角度对SoA和Reflexion生成的代码平均量的可视化。在HumanEval的背景下，需要实现单个函数，SoA的代码量通过将每个代理生成的代码总和来计算，而Reflexion的代码量则基于单个函数。SoA中*每个函数的代码量*指的是每个独立代理生成的代码，而在Reflexion中，相当于单个函数的代码量。结果明确展示了SoA在每个最终代码的标记数量和每个函数的平均字符数方面优于Reflexion。值得注意的是，尽管SoA中的每个代理处理的标记/字符明显少于Reflexion中的单一代理，但SoA生成的总体输出显著更多。这一发现突显了SoA的卓越可扩展性，表明其通过无缝添加更多代理到系统中，可以处理越来越复杂的任务。我们的结果表明，通过增加代理层级的深度和引入更多的母代理，SoA可以通过在多个代理之间高效分配工作负载生成更大规模的代码。随着树状结构变得更深，系统展现出无限的扩展潜力，使其能够生成越来越复杂和广泛的代码库，同时确保每个代理处理可管理的代码量。每个代理可以保持可管理的代码量，同时理论上允许整体代码生成能力的无限增加。'
- en: This distributed approach empowers SoA to significantly scale up its ability
    to tackle large-scale and complex coding tasks with remarkable efficiency and
    high quality, far surpassing the limitations encountered by single-agent systems
    like Reflexion, where a sole agent is responsible for managing and generating
    the entire codebase.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分布式方法使得SoA能够显著提升其处理大规模和复杂编码任务的能力，效率和质量都显著高于单一代理系统（如Reflexion）所面临的局限性，在这些系统中，单一代理负责管理和生成整个代码库。
- en: '![Refer to caption](img/4175fd1c4c62fd9306f21e1a2f2b3f7b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4175fd1c4c62fd9306f21e1a2f2b3f7b.png)'
- en: 'Figure 5: Comparison of code generation amount between SoA (mulit-agent) and
    Reflexion (single agent).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：SoA（多代理）与Reflexion（单代理）在代码生成量上的比较
- en: §​ 5 Related Work
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: § 5 相关工作
- en: LLM Agents
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理
- en: Recent advancements in LLM agents, such as ReAct Yao et al. ([2023b](#bib.bib24)),
    Reflexion Shinn et al. ([2023](#bib.bib20)), Toolformer Schick et al. ([2023](#bib.bib18)),
    and Self-Refine Madaan et al. ([2023](#bib.bib13)), have primarily focused on
    single-agent approaches, where one agent is responsible for both generation and
    modification tasks. Among these, Reflexion Shinn et al. ([2023](#bib.bib20)) has
    gained significant attention in the field of code generation due to its outstanding
    performance. However, despite their strengths, these single-agent approaches face
    inherent limitations when it comes to generating and modifying large-scale codebases.
    To address these limitations and push the boundaries of what is possible with
    LLM agents, we propose SoA, a novel multi-agent framework that harnesses the power
    of self-organization and collaboration. While we intentionally adopted simple
    agents for SoA in this work, our framework is flexible enough to incorporate more
    sophisticated and powerful methods Zhong et al. ([2024](#bib.bib26)); Zhou et al.
    ([2023](#bib.bib27)) and other state-of-the-art LLMs ⁵⁵5[https://claude.ai/](https://claude.ai/),
    further enhancing its potential for large-scale code generation and modification.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在LLM代理方面的进展，例如ReAct Yao et al. ([2023b](#bib.bib24))、Reflexion Shinn et al.
    ([2023](#bib.bib20))、Toolformer Schick et al. ([2023](#bib.bib18))和Self-Refine
    Madaan et al. ([2023](#bib.bib13))，主要集中在单代理方法上，其中一个代理负责生成和修改任务。在这些方法中，Reflexion
    Shinn et al. ([2023](#bib.bib20))由于其出色的性能，在代码生成领域获得了显著关注。然而，尽管这些单代理方法有其优势，但在生成和修改大规模代码库时，它们面临固有的局限性。为了应对这些局限性并推动LLM代理的可能性，我们提出了SoA，一个新颖的多代理框架，利用自组织和协作的力量。虽然我们在这项工作中故意采用了简单代理来实现SoA，但我们的框架足够灵活，可以融入更复杂和更强大的方法
    Zhong et al. ([2024](#bib.bib26)); Zhou et al. ([2023](#bib.bib27))和其他前沿LLMs ⁵⁵5[https://claude.ai/](https://claude.ai/)，进一步提升其在大规模代码生成和修改方面的潜力。
- en: Multi-Agent Collaboration for Software Development
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 软件开发的多代理协作
- en: In recent years, several multi-agent-based approaches have emerged as promising
    solutions for software development, such as MetaGPT Hong et al. ([2023](#bib.bib7)),
    ChatDev Qian et al. ([2023](#bib.bib17)), Self-collaboration Dong et al. ([2023](#bib.bib5)),
    and AgentCoder Huang et al. ([2023](#bib.bib8)). These methods typically personify
    agents and assign them specific names or occupational roles, such as programmers,
    project managers, or QA engineers, to allocate tasks. While this approach has
    shown promise, our method takes a different and more flexible approach. Instead
    of assigning fixed occupational roles, we subdivide agent capabilities based on
    *code functionality*, allowing each agent to demonstrate its expertise without
    being constrained by predefined roles. This fine-grained task allocation enables
    more flexible problem-solving and adaptation to the complexity of the software
    development process. Moreover, by incorporating the concepts of self-organization
    and self-proliferation, our agents can dynamically scale up the overall code volume
    based on the difficulty of the problem at hand, providing a highly adaptable and
    efficient framework for large-scale code generation and modification.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，几种基于多代理的方法作为软件开发的有前景的解决方案出现，例如MetaGPT Hong et al. ([2023](#bib.bib7))、ChatDev
    Qian et al. ([2023](#bib.bib17))、Self-collaboration Dong et al. ([2023](#bib.bib5))和AgentCoder
    Huang et al. ([2023](#bib.bib8))。这些方法通常使代理具有人格化，并为其分配特定的名称或职业角色，如程序员、项目经理或QA工程师，以分配任务。虽然这种方法显示了潜力，但我们的方法采取了不同且更灵活的方式。我们不是分配固定的职业角色，而是基于*代码功能*细分代理能力，允许每个代理展示其专长，而不受预定义角色的限制。这种细粒度的任务分配使得问题解决更加灵活，并适应软件开发过程的复杂性。此外，通过融入自组织和自增殖的概念，我们的代理可以根据问题的难度动态扩展整体代码量，为大规模代码生成和修改提供一个高度适应性和高效的框架。
- en: Macro vs. Micro Perspectives
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 宏观与微观视角
- en: While both multi-agent-based methods Hong et al. ([2023](#bib.bib7)); Qian et al.
    ([2023](#bib.bib17)); Dong et al. ([2023](#bib.bib5)); Huang et al. ([2023](#bib.bib8))
    and our proposed SoA framework share the common goal of automating software development,
    they address different technical aspects of the process. Existing multi-agent
    methods primarily focus on optimizing the macro structure of software development,
    such as project management and task allocation. In contrast, our method takes
    a more micro-level perspective, focusing on the elemental technologies of code
    generation and modification. These approaches are not mutually exclusive but rather
    complementary, offering a more comprehensive solution to the challenges faced
    in automatic software development. By combining the strengths of both macro and
    micro-level approaches, we can create a powerful and holistic framework that efficiently
    handles the complexities of large-scale code generation and modification.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于多代理的方法  洪等人 ([2023](#bib.bib7)); 钱等人 ([2023](#bib.bib17)); 董等人 ([2023](#bib.bib5));
    黄等人 ([2023](#bib.bib8)) 和我们提出的 SoA 框架都共享自动化软件开发的共同目标，但它们解决了过程中的不同技术方面。现有的多代理方法主要关注软件开发的宏观结构优化，如项目管理和任务分配。相比之下，我们的方法采取了更微观的视角，专注于代码生成和修改的基本技术。这些方法并非相互排斥，而是互为补充，提供了对自动化软件开发挑战的更全面的解决方案。通过结合宏观和微观方法的优势，我们可以创建一个强大而全面的框架，有效处理大规模代码生成和修改的复杂性。
- en: Prompt Engineering
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示工程
- en: Tree-of-Thought (ToT) Yao et al. ([2023a](#bib.bib23)) and Skeleton of Thought
    (SoT) Ning et al. ([2023](#bib.bib15)) are prompt engineering techniques that
    utilize tree-like structures. ToT represents reasoning steps as nodes to explore
    correct reasoning paths, while SoT generates a skeleton of the answer and completes
    the contents in parallel to decrease generation latency. In contrast, SoA uses
    a tree structure with agents as nodes, focusing on their collaboration and self-organization
    to generate and modify code efficiently.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-of-Thought (ToT)  姚等人 ([2023a](#bib.bib23)) 和 Skeleton of Thought (SoT)
    宁等人 ([2023](#bib.bib15)) 是利用树状结构的提示工程技术。ToT 将推理步骤表示为节点，以探索正确的推理路径，而 SoT 生成答案的骨架并并行完成内容，以减少生成延迟。相比之下，SoA
    使用带有代理的树结构，专注于其协作和自组织以高效生成和修改代码。
- en: §​ 6 Conclusion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: § 6 结论
- en: In this work, we introduced Self-organized Agents (SoA), a novel multi-agent
    framework for efficient and scalable automatic code generation and optimization
    using large language models (LLMs). SoA addresses the limitations of single-agent
    approaches in handling large-scale, complex codebases by leveraging the power
    of self-organization and distributed code generation. In SoA, self-organized agents
    operate independently to generate and modify code components while seamlessly
    collaborating to construct the overall codebase. A key feature of our framework
    is the automatic multiplication of agents based on problem complexity, allowing
    for dynamic scalability and enabling the overall code volume to be increased indefinitely
    according to the number of agents, while the amount of code managed by each agent
    remains constant.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入了自组织代理（SoA），这是一个新颖的多代理框架，用于使用大型语言模型（LLMs）进行高效和可扩展的自动代码生成和优化。SoA 通过利用自组织和分布式代码生成的能力，解决了单一代理方法在处理大规模复杂代码库时的局限性。在
    SoA 中，自组织代理独立操作以生成和修改代码组件，同时无缝协作以构建整体代码库。我们框架的一个关键特性是根据问题复杂性自动倍增代理，从而实现动态可扩展性，并使整体代码量根据代理数量无限增加，而每个代理管理的代码量保持不变。
- en: We evaluated SoA on the HumanEval benchmark and demonstrated its superior performance
    compared to Reflexion, a state-of-the-art single-agent system, with SoA achieving
    a 5% improvement in terms of Pass@1 accuracy. Furthermore, our in-depth analysis
    revealed SoA’s remarkable scalability, as each agent in SoA handles significantly
    less code compared to the single-agent baseline, yet the overall generated code
    is substantially greater. These results highlight the effectiveness of SoA in
    generating and optimizing large-scale code efficiently and with high quality.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 HumanEval 基准上评估了 SoA，并展示了其相对于最先进的单代理系统 Reflexion 的优越性能，SoA 在 Pass@1 准确率方面提高了
    5%。此外，我们的深入分析揭示了 SoA 的显著可扩展性，因为 SoA 中的每个代理处理的代码显著少于单代理基线，但整体生成的代码却大大增加。这些结果突显了
    SoA 在高效且高质量生成和优化大规模代码方面的有效性。
- en: However, it is essential to acknowledge the limitations of the current implementation
    of SoA. The framework’s performance may be affected by the choice of LLM and the
    quality of the generated unit tests. Additionally, SoA has been evaluated on a
    limited set of programming tasks, and its effectiveness in handling more complex,
    real-world software development projects remains to be investigated. Furthermore,
    the communication and collaboration mechanisms among agents in SoA can be further
    optimized to improve efficiency and fault tolerance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须承认当前 SoA 实现的局限性。框架的性能可能会受到 LLM 选择和生成单元测试质量的影响。此外，SoA 已在有限的编程任务集上进行评估，其在处理更复杂的现实世界软件开发项目中的有效性仍需进一步研究。此外，SoA
    中代理之间的通信和协作机制还可以进一步优化，以提高效率和容错性。
- en: Despite these limitations, we believe that the SoA framework has significant
    potential for future research and development in the field of automatic software
    development.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些限制，我们相信 SoA 框架在自动软件开发领域具有显著的未来研究和开发潜力。
- en: References
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Anil et al. (2023) Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth,
    Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou,
    Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap,
    Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham,
    Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan
    Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub,
    Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay
    Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara
    von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub
    Sygnowski, and et al. 2023. [Gemini: A family of highly capable multimodal models](https://doi.org/10.48550/ARXIV.2312.11805).
    *CoRR*, abs/2312.11805.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil et al. (2023) Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth,
    Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou,
    Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap,
    Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham,
    Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan
    Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub,
    Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay
    Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara
    von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub
    Sygnowski, and et al. 2023. [Gemini：一组高能力的多模态模型](https://doi.org/10.48550/ARXIV.2312.11805)。*CoRR*，abs/2312.11805。
- en: Ashby (1947) W Ross Ashby. 1947. Principles of the self-organizing dynamic system.
    *The Journal of general psychology*, 37(2):125–128.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashby (1947) W Ross Ashby. 1947. 自组织动态系统的原则。*普通心理学杂志*，37(2)：125–128。
- en: 'Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).
    In *Advances in Neural Information Processing Systems 33: Annual Conference on
    Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual*.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [语言模型是少样本学习者](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)。在
    *神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6-12日，虚拟会议*。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
    *CoRR*, abs/2107.03374.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever 和 Wojciech
    Zaremba. 2021. [评估在代码上训练的大型语言模型](http://arxiv.org/abs/2107.03374)。*CoRR*，abs/2107.03374。
- en: Dong et al. (2023) Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023. [Self-collaboration
    code generation via chatgpt](https://doi.org/10.48550/ARXIV.2304.07590). *CoRR*,
    abs/2304.07590.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）Yihong Dong, Xue Jiang, Zhi Jin 和 Ge Li. 2023. [通过 chatgpt 自我协作代码生成](https://doi.org/10.48550/ARXIV.2304.07590)。*CoRR*，abs/2304.07590。
- en: 'Fried et al. (2023) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis.
    2023. [Incoder: A generative model for code infilling and synthesis](https://openreview.net/pdf?id=hQwb-lbM6EL).
    In *The Eleventh International Conference on Learning Representations, ICLR 2023,
    Kigali, Rwanda, May 1-5, 2023*. OpenReview.net.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fried 等（2023）Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace,
    Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer 和 Mike Lewis. 2023. [Incoder:
    一种用于代码填充和合成的生成模型](https://openreview.net/pdf?id=hQwb-lbM6EL)。在 *第十一届国际学习表征会议，ICLR
    2023，卢旺达基加利，2023年5月1日至5日*。OpenReview.net。'
- en: 'Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
    Ran, Lingfeng Xiao, and Chenglin Wu. 2023. [Metagpt: Meta programming for multi-agent
    collaborative framework](https://doi.org/10.48550/ARXIV.2308.00352). *CoRR*, abs/2308.00352.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等（2023）Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang,
    Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran,
    Lingfeng Xiao 和 Chenglin Wu. 2023. [Metagpt: 多智能体协作框架的元编程](https://doi.org/10.48550/ARXIV.2308.00352)。*CoRR*，abs/2308.00352。'
- en: 'Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck, and
    Heming Cui. 2023. [Agentcoder: Multi-agent-based code generation with iterative
    testing and optimisation](https://doi.org/10.48550/ARXIV.2312.13010). *CoRR*,
    abs/2312.13010.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等（2023）Dong Huang, Qingwen Bu, Jie M. Zhang, Michael Luck 和 Heming Cui.
    2023. [Agentcoder: 基于多智能体的代码生成及迭代测试与优化](https://doi.org/10.48550/ARXIV.2312.13010)。*CoRR*，abs/2312.13010。'
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. [Same task,
    more tokens: the impact of input length on the reasoning performance of large
    language models](https://doi.org/10.48550/ARXIV.2402.14848). *CoRR*, abs/2402.14848.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Levy 等（2024）Mosh Levy, Alon Jacoby 和 Yoav Goldberg. 2024. [相同任务，多 tokens: 输入长度对大型语言模型推理性能的影响](https://doi.org/10.48550/ARXIV.2402.14848)。*CoRR*，abs/2402.14848。'
- en: 'Li et al. (2023a) Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023a.
    [Loogle: Can long-context language models understand long contexts?](https://doi.org/10.48550/ARXIV.2311.04939)
    *CoRR*, abs/2311.04939.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2023a）Jiaqi Li, Mengmeng Wang, Zilong Zheng 和 Muhan Zhang. 2023a. [Loogle:
    长上下文语言模型能否理解长上下文？](https://doi.org/10.48550/ARXIV.2311.04939) *CoRR*，abs/2311.04939。'
- en: 'Li et al. (2023b) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier,
    Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin
    Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp
    Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy,
    Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,
    Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding,
    Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex
    Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,
    Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis,
    Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023b.
    [Starcoder: may the source be with you!](https://doi.org/10.48550/ARXIV.2305.06161)
    *CoRR*, abs/2305.06161.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2023b）Raymond Li、Loubna Ben Allal、Yangtian Zi、Niklas Muennighoff、Denis
    Kocetkov、Chenghao Mou、Marc Marone、Christopher Akiki、Jia Li、Jenny Chim、Qian Liu、Evgenii
    Zheltonozhskii、Terry Yue Zhuo、Thomas Wang、Olivier Dehaene、Mishig Davaadorj、Joel
    Lamy-Poirier、João Monteiro、Oleh Shliazhko、Nicolas Gontier、Nicholas Meade、Armel
    Zebaze、Ming-Ho Yee、Logesh Kumar Umapathi、Jian Zhu、Benjamin Lipkin、Muhtasham Oblokulov、Zhiruo
    Wang、Rudra Murthy V、Jason Stillerman、Siva Sankalp Patel、Dmitry Abulkhanov、Marco
    Zocca、Manan Dey、Zhihan Zhang、Nour Moustafa-Fahmy、Urvashi Bhattacharyya、Wenhao
    Yu、Swayam Singh、Sasha Luccioni、Paulo Villegas、Maxim Kunakov、Fedor Zhdanov、Manuel
    Romero、Tony Lee、Nadav Timor、Jennifer Ding、Claire Schlesinger、Hailey Schoelkopf、Jan
    Ebert、Tri Dao、Mayank Mishra、Alex Gu、Jennifer Robinson、Carolyn Jane Anderson、Brendan
    Dolan-Gavitt、Danish Contractor、Siva Reddy、Daniel Fried、Dzmitry Bahdanau、Yacine
    Jernite、Carlos Muñoz Ferrandis、Sean Hughes、Thomas Wolf、Arjun Guha、Leandro von
    Werra 和 Harm de Vries。2023b。[Starcoder: may the source be with you!](https://doi.org/10.48550/ARXIV.2305.06161)。*CoRR*，abs/2305.06161。'
- en: Li et al. (2022) Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal
    Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
    Koray Kavukcuoglu, and Oriol Vinyals. 2022. [Competition-level code generation
    with alphacode](https://doi.org/10.48550/ARXIV.2203.07814). *CoRR*, abs/2203.07814.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022）Yujia Li、David H. Choi、Junyoung Chung、Nate Kushman、Julian Schrittwieser、Rémi
    Leblond、Tom Eccles、James Keeling、Felix Gimeno、Agustin Dal Lago、Thomas Hubert、Peter
    Choy、Cyprien de Masson d’Autume、Igor Babuschkin、Xinyun Chen、Po-Sen Huang、Johannes
    Welbl、Sven Gowal、Alexey Cherepanov、James Molloy、Daniel J. Mankowitz、Esme Sutherland
    Robson、Pushmeet Kohli、Nando de Freitas、Koray Kavukcuoglu 和 Oriol Vinyals。2022年。[Competition-level
    code generation with alphacode](https://doi.org/10.48550/ARXIV.2203.07814)。*CoRR*，abs/2203.07814。
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck,
    Amir Yazdanbakhsh, and Peter Clark. 2023. [Self-refine: Iterative refinement with
    self-feedback](http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等人（2023）Aman Madaan、Niket Tandon、Prakhar Gupta、Skyler Hallinan、Luyu
    Gao、Sarah Wiegreffe、Uri Alon、Nouha Dziri、Shrimai Prabhumoye、Yiming Yang、Shashank
    Gupta、Bodhisattwa Prasad Majumder、Katherine Hermann、Sean Welleck、Amir Yazdanbakhsh
    和 Peter Clark。2023年。[Self-refine: Iterative refinement with self-feedback](http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html)。在*Advances
    in Neural Information Processing Systems 36: Annual Conference on Neural Information
    Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
    2023*。'
- en: 'Muennighoff et al. (2023) Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai
    Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra,
    and Shayne Longpre. 2023. [Octopack: Instruction tuning code large language models](https://doi.org/10.48550/ARXIV.2308.07124).
    *CoRR*, abs/2308.07124.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Muennighoff 等人（2023）Niklas Muennighoff、Qian Liu、Armel Zebaze、Qinkai Zheng、Binyuan
    Hui、Terry Yue Zhuo、Swayam Singh、Xiangru Tang、Leandro von Werra 和 Shayne Longpre。2023年。[Octopack:
    Instruction tuning code large language models](https://doi.org/10.48550/ARXIV.2308.07124)。*CoRR*，abs/2308.07124。'
- en: 'Ning et al. (2023) Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and
    Yu Wang. 2023. [Skeleton-of-thought: Large language models can do parallel decoding](https://doi.org/10.48550/ARXIV.2307.15337).
    *CoRR*, abs/2307.15337.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ning 等人（2023）Xuefei Ning、Zinan Lin、Zixuan Zhou、Huazhong Yang 和 Yu Wang。2023年。[Skeleton-of-thought:
    Large language models can do parallel decoding](https://doi.org/10.48550/ARXIV.2307.15337)。*CoRR*，abs/2307.15337。'
- en: OpenAI (2023) OpenAI. 2023. [GPT-4 technical report](https://doi.org/10.48550/arXiv.2303.08774).
    *CoRR*, abs/2303.08774.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 2023. [GPT-4 technical report](https://doi.org/10.48550/arXiv.2303.08774).
    *CoRR*，abs/2303.08774。
- en: Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. [Communicative agents for software
    development](https://doi.org/10.48550/ARXIV.2307.07924). *CoRR*, abs/2307.07924.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人（2023）Chen Qian、Xin Cong、Cheng Yang、Weize Chen、Yusheng Su、Juyuan Xu、Zhiyuan
    Liu 和 Maosong Sun. 2023. [Communicative agents for software development](https://doi.org/10.48550/ARXIV.2307.07924).
    *CoRR*，abs/2307.07924。
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2023. [Toolformer: Language models can teach themselves to use tools](http://papers.nips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人（2023）Timo Schick、Jane Dwivedi-Yu、Roberto Dessì、Roberta Raileanu、Maria
    Lomeli、Eric Hambro、Luke Zettlemoyer、Nicola Cancedda 和 Thomas Scialom. 2023. [Toolformer:
    Language models can teach themselves to use tools](http://papers.nips.cc/paper_files/paper/2023/hash/d842425e4bf79ba039352da0f658a906-Abstract-Conference.html).
    发表在 *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*。'
- en: 'Shaham et al. (2023) Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and
    Omer Levy. 2023. [Zeroscrolls: A zero-shot benchmark for long text understanding](https://aclanthology.org/2023.findings-emnlp.536).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore,
    December 6-10, 2023*, pages 7977–7989\. Association for Computational Linguistics.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shaham 等人（2023）Uri Shaham、Maor Ivgi、Avia Efrat、Jonathan Berant 和 Omer Levy.
    2023. [Zeroscrolls: A zero-shot benchmark for long text understanding](https://aclanthology.org/2023.findings-emnlp.536).
    发表在 *Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore,
    December 6-10, 2023*，第 7977–7989 页。计算语言学协会。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2023. [Reflexion: language agents with verbal reinforcement
    learning](http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人（2023）Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik Narasimhan
    和 Shunyu Yao. 2023. [Reflexion: language agents with verbal reinforcement learning](http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html).
    发表在 *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://doi.org/10.48550/arXiv.2307.09288).
    *CoRR*, abs/2307.09288.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton-Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurélien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom. 2023. [Llama 2: Open foundation and fine-tuned chat models](https://doi.org/10.48550/arXiv.2307.09288).
    *CoRR*，abs/2307.09288。'
- en: 'Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. [Chain-of-thought
    prompting elicits reasoning in large language models](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 35: Annual Conference on
    Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,
    November 28 - December 9, 2022*.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, 和 Denny Zhou. 2022. [链式思维提示在大型语言模型中引发推理](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).
    在 *神经信息处理系统进展 35：2022年神经信息处理系统年会，NeurIPS 2022，新奥尔良，美国，2022年11月28日 - 12月9日*.
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2023a. [Tree of thoughts: Deliberate problem
    solving with large language models](http://papers.nips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 36: Annual Conference on
    Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,
    December 10 - 16, 2023*.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 2023a. [思维树：使用大型语言模型进行深思熟虑的问题解决](http://papers.nips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html).
    在 *神经信息处理系统进展 36：2023年神经信息处理系统年会，NeurIPS 2023，新奥尔良，美国，2023年12月10 - 16日*.
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, and Yuan Cao. 2023b. [React: Synergizing reasoning and
    acting in language models](https://openreview.net/pdf?id=WE_vluYUL-X). In *The
    Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,
    Rwanda, May 1-5, 2023*. OpenReview.net.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, 和 Yuan Cao. 2023b. [React: 语言模型中推理与行动的协同作用](https://openreview.net/pdf?id=WE_vluYUL-X).
    在 *第十一届国际学习表征会议，ICLR 2023，基加利，卢旺达，2023年5月1-5日*. OpenReview.net.'
- en: 'Zhang et al. (2023) Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023.
    [Self-edit: Fault-aware code editor for code generation](https://doi.org/10.18653/V1/2023.ACL-LONG.45).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*,
    pages 769–787\. Association for Computational Linguistics.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2023) Kechi Zhang, Zhuo Li, Jia Li, Ge Li, 和 Zhi Jin. 2023. [Self-edit:
    面向代码生成的故障感知代码编辑器](https://doi.org/10.18653/V1/2023.ACL-LONG.45). 在 *第61届计算语言学协会年会会议录（第1卷：长篇论文），ACL
    2023，多伦多，加拿大，2023年7月9-14日*，第769–787页。计算语言学协会。'
- en: 'Zhong et al. (2024) Lily Zhong, Zilong Wang, and Jingbo Shang. 2024. [LDB:
    A large language model debugger via verifying runtime execution step-by-step](https://doi.org/10.48550/ARXIV.2402.16906).
    *CoRR*, abs/2402.16906.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. (2024) Lily Zhong, Zilong Wang, 和 Jingbo Shang. 2024. [LDB: 通过逐步验证运行时执行的大型语言模型调试器](https://doi.org/10.48550/ARXIV.2402.16906).
    *CoRR*, abs/2402.16906.'
- en: Zhou et al. (2023) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. 2023. [Language agent tree search unifies reasoning acting
    and planning in language models](https://doi.org/10.48550/ARXIV.2310.04406). *CoRR*,
    abs/2310.04406.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    和 Yu-Xiong Wang. 2023. [语言代理树搜索统一了语言模型中的推理、行动和规划](https://doi.org/10.48550/ARXIV.2310.04406).
    *CoRR*, abs/2310.04406.
- en: Appendix A Pseudocode
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 伪代码
- en: Algorithm 1 Generate Code with Self-organized Agent Framework
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 使用自组织代理框架生成代码
- en: 1:$docstrings$, None)50:end for51:52:return The final implementation combined
    from all agents
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 1:$docstrings$, None)50:end for51:52:return 从所有代理合并的最终实现
