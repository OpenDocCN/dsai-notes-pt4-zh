- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:45:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大语言模型哨兵：通过 LLM 代理提升对抗鲁棒性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20770](https://ar5iv.labs.arxiv.org/html/2405.20770)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20770](https://ar5iv.labs.arxiv.org/html/2405.20770)
- en: Guang Lin Tokyo University of Agriculture and Technology RIKEN Center for Advanced
    Intelligence Project (RIKEN AIP) Qibin Zhao Corresponding Author RIKEN Center
    for Advanced Intelligence Project (RIKEN AIP) Tokyo University of Agriculture
    and Technology
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 光广 林 东京农业大学 东京农业大学 RIKEN 高级智能项目中心（RIKEN AIP） 赵启斌 通讯作者 RIKEN 高级智能项目中心（RIKEN AIP）
    东京农业大学
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Over the past two years, the use of large language models (LLMs) has advanced
    rapidly. While these LLMs offer considerable convenience, they also raise security
    concerns, as LLMs are vulnerable to adversarial attacks by some well-designed
    textual perturbations. In this paper, we introduce a novel defense technique named
    Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial
    robustness of LLMs by purifying the adversarial textual examples before feeding
    them into the target LLM. Our method comprises two main components: a) Agent instruction,
    which can simulate a new agent for adversarial defense, altering minimal characters
    to maintain the original meaning of the sentence while defending against attacks;
    b) Defense guidance, which provides strategies for modifying clean or adversarial
    examples to ensure effective defense and accurate outputs from the target LLMs.
    Remarkably, the defense agent demonstrates robust defensive capabilities even
    without learning from adversarial examples. Additionally, we conduct an intriguing
    adversarial experiment where we develop two agents, one for defense and one for
    defense, and engage them in mutual confrontation. During the adversarial interactions,
    neither agent completely beat the other. Extensive experiments on both open-source
    and closed-source LLMs demonstrate that our method effectively defends against
    adversarial attacks, thereby enhancing adversarial robustness.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去两年里，大语言模型（LLMs）的使用迅速发展。虽然这些 LLMs 提供了相当大的便利，但它们也带来了安全隐患，因为 LLMs 容易受到一些精心设计的文本扰动的对抗攻击。本文介绍了一种新颖的防御技术，名为大型语言模型哨兵（LLAMOS），旨在通过在将对抗性文本示例输入目标
    LLM 之前对其进行净化，来增强 LLM 的对抗鲁棒性。我们的方法包括两个主要组成部分：a) 代理指令，可以模拟一个新的对抗防御代理，修改最小的字符以保持句子的原意，同时防御攻击；b)
    防御指导，提供修改干净或对抗示例的策略，以确保有效防御并从目标 LLM 获得准确的输出。值得注意的是，防御代理即使没有从对抗示例中学习也展示了强大的防御能力。此外，我们进行了一项有趣的对抗实验，开发了两个代理，一个用于防御，另一个也用于防御，并让它们相互对抗。在对抗交互过程中，没有一个代理完全击败另一个。对开源和闭源
    LLM 的大量实验表明，我们的方法有效地防御了对抗攻击，从而增强了对抗鲁棒性。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/e58bd46a7e07ffff5703c611e1aae8f8.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e58bd46a7e07ffff5703c611e1aae8f8.png)'
- en: 'Figure 1: Illustration of adversarial attacks and adversarial purification
    on large language models. (a) When the clean example $x$ can be predicted as the
    correct label.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：大语言模型上的对抗攻击和对抗净化的示意图。 (a) 当干净示例 $x$ 可以被预测为正确标签时。
- en: Large Language Models (LLMs) have garnered significant attention due to their
    impressive performance across a wide range of natural language tasks (Minaee et al.,
    [2024](#bib.bib34)). The pre-trained LLMs, such as Meta’s LLAMA (Touvron et al.,
    [2023a](#bib.bib53), [b](#bib.bib54)) and OpenAI’s ChatGPT (OpenAI, [2022](#bib.bib38);
    Achiam et al., [2023](#bib.bib1)), have become essential foundations for AI applications
    in various sectors such as healthcare, education, and visual tasks (Kasneci et al.,
    [2023](#bib.bib20); Thirunavukarasu et al., [2023](#bib.bib52); OpenAI, [2023](#bib.bib39);
    Köpf et al., [2024](#bib.bib21); Romera-Paredes et al., [2024](#bib.bib43)). Despite
    their widespread use and convenience, concerns about the security of these models
    are increasing. Specifically, LLMs have been shown to be vulnerable to adversarial
    textual examples (Wang et al., [2023a](#bib.bib57); Xu et al., [2024](#bib.bib64)),
    which involve subtle modifications to textual content that maintain the same meaning
    for humans but completely change the prediction results to LLMs, often with severe
    consequences.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其在广泛自然语言任务中的出色表现而获得了显著关注（Minaee et al., [2024](#bib.bib34)）。如 Meta
    的 LLAMA（Touvron et al., [2023a](#bib.bib53), [b](#bib.bib54)）和 OpenAI 的 ChatGPT（OpenAI,
    [2022](#bib.bib38); Achiam et al., [2023](#bib.bib1)）等预训练 LLM 已成为医疗、教育和视觉任务等各个领域
    AI 应用的基础（Kasneci et al., [2023](#bib.bib20); Thirunavukarasu et al., [2023](#bib.bib52);
    OpenAI, [2023](#bib.bib39); Köpf et al., [2024](#bib.bib21); Romera-Paredes et
    al., [2024](#bib.bib43)）。尽管它们广泛使用且便利，但对这些模型的安全性担忧正在增加。具体而言，LLM 已被证明对对抗性文本示例（Wang
    et al., [2023a](#bib.bib57); Xu et al., [2024](#bib.bib64)）具有脆弱性，这些示例涉及对文本内容的微妙修改，这些修改对人类而言意义相同，但会完全改变
    LLM 的预测结果，通常会带来严重后果。
- en: To achieve robust defense against adversarial attacks on Large Language Models
    (LLMs), a prevalent strategy is fine-tuning the LLMs with adversarial examples
    to enhance model alignment (Shen et al., [2023](#bib.bib44); Wang et al., [2023c](#bib.bib60)).
    LLM-based adversarial fine-tuning (AFT) can be implemented either through in-context
    learning (Dong et al., [2022](#bib.bib13); Xiang et al., [2024](#bib.bib63)) or
    by optimizing the parameters of pre-trained LLMs using adversarial examples (Dettmers
    et al., [2024](#bib.bib12); Li et al., [2024b](#bib.bib25)). However, LLM-based
    AFT methods necessitate additional computational resources and training time.
    Achieving robust and reliable LLMs typically requires significant costs (Hu et al.,
    [2022](#bib.bib19)), which is prohibitive for ordinary users. Additionally, due
    to the discrete nature of textual information, these adversarial examples can
    have substitutes in any token of the sentence, with each having a large candidate
    list (Li et al., [2023](#bib.bib23)). This leads to a combinatorial explosion,
    making the application of AFT methods challenging or resulting in poor generalization
    when trained on a limited dataset of adversarial examples. Consequently, developing
    an efficient and user-friendly robust LLM system remains a huge challenge and
    an urgent issue that continues to be addressed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现对大型语言模型（LLMs）对抗性攻击的强健防御，一种常见策略是使用对抗性示例对 LLM 进行微调，以增强模型的对齐性（Shen et al.,
    [2023](#bib.bib44); Wang et al., [2023c](#bib.bib60)）。基于 LLM 的对抗性微调（AFT）可以通过上下文学习（Dong
    et al., [2022](#bib.bib13); Xiang et al., [2024](#bib.bib63)）或通过使用对抗性示例优化预训练 LLM
    的参数（Dettmers et al., [2024](#bib.bib12); Li et al., [2024b](#bib.bib25)）来实现。然而，基于
    LLM 的 AFT 方法需要额外的计算资源和训练时间。实现强健且可靠的 LLM 通常需要显著的成本（Hu et al., [2022](#bib.bib19)），这对普通用户而言是不可承受的。此外，由于文本信息的离散性，这些对抗性示例可以在句子的任何标记中有替代项，每个标记都有一个大的候选列表（Li
    et al., [2023](#bib.bib23)）。这导致了组合爆炸，使得 AFT 方法的应用具有挑战性，或者当在有限的对抗性示例数据集上训练时，可能会导致较差的泛化。因此，开发高效且用户友好的强健
    LLM 系统仍然是一个巨大的挑战和紧迫问题。
- en: 'In this paper, focusing on adversarial textual attacks targeting LLM-based
    classification tasks, we propose a novel defense technique named Large LAnguage
    MOdel Sentinel (LLAMOS), which utilizes the LLM as a defense agent for adversarial
    purification, as illustrated in [Figure 1](#S1.F1 "In 1 Introduction ‣ Large Language
    Model Sentinel: Advancing Adversarial Robustness by LLM Agent"). To streamline
    our explanation, we condense certain details in Figure 1, with comprehensive instructions
    provided in [Section 3](#S3 "3 Methods ‣ Large Language Model Sentinel: Advancing
    Adversarial Robustness by LLM Agent"). Specifically, LLAMOS comprises two components:
    Agent instruction, which can simulate a new agent for adversarial defense, altering
    minimal characters to maintain the original meaning of the sentence, and Defense
    guidance, which provides strategies for modifying clean or adversarial examples
    to ensure effective defense and accurate outputs from the target LLMs. LLAMOS
    serves as a pre-processing method aiming to eliminate harmful information from
    potentially attacked textual inputs before feeding them into the target LLM for
    classification. In contrast to the AFT method, the LLM-based AP method functions
    as an additional module capable of defending against adversarial attacks without
    necessitating fine-tuning of the target LLM.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们针对 LLM 基于分类任务的对抗文本攻击，提出了一种新的防御技术，命名为 Large LAnguage MOdel Sentinel (LLAMOS)，该技术利用
    LLM 作为对抗净化的防御代理，如 [图 1](#S1.F1 "In 1 Introduction ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent") 所示。为了简化说明，我们在图 1 中浓缩了某些细节，全面的说明请参见
    [第 3 节](#S3 "3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent")。具体来说，LLAMOS 包括两个组件：代理指令，可以模拟一个新的对抗防御代理，改变最少的字符以保持句子的原意；以及防御指导，提供修改干净或对抗示例的策略，以确保有效防御和目标
    LLM 的准确输出。LLAMOS 作为一种预处理方法，旨在在将潜在攻击的文本输入送入目标 LLM 进行分类之前，去除有害信息。与 AFT 方法相比，基于 LLM
    的 AP 方法作为一个额外模块，能够防御对抗攻击，而无需对目标 LLM 进行微调。'
- en: We comprehensively evaluate the performance of our method on GLUE datasets,
    conducting experiments with both representative open-source and closed-source
    LLMs, LLAMA-2 (Touvron et al., [2023b](#bib.bib54)) and GPT-3.5 (OpenAI, [2022](#bib.bib38)).
    The experimental results demonstrate that the LLM-based AP method effectively
    defends against adversarial attacks. Specifically, our proposed method achieves
    a maximum reduction in the attack success rate (ASR) by up to 45.59% and 37.86%
    with LLAMA-2 and GPT-3.5, respectively. Additionally, we observe that the initial
    defense agent fails to achieve the expected results under some obvious attacks.
    Therefore, we employ the in-context learning (Dong et al., [2022](#bib.bib13))
    to further optimize the defense agent, significantly enhancing the defense capabilities
    almost without adding any additional costs. Finally, we conduct an intriguing
    online adversarial experiment, creating an adversarial system using two LLM-based
    agents (one for defense and one for attack) along with a target LLM for classification.
    During the adversarial interaction, the defense agent and attack agent continuously
    counter each other, resembling the adversarial training process in traditional
    image tasks (Goodfellow et al., [2015](#bib.bib17)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 GLUE 数据集上全面评估了我们方法的性能，进行了代表性的开源和闭源 LLMs 实验，包括 LLAMA-2 (Touvron et al., [2023b](#bib.bib54))
    和 GPT-3.5 (OpenAI, [2022](#bib.bib38))。实验结果表明，基于 LLM 的 AP 方法能够有效防御对抗攻击。具体来说，我们提出的方法在
    LLAMA-2 和 GPT-3.5 上分别实现了最大 45.59% 和 37.86% 的攻击成功率（ASR）减少。此外，我们观察到在某些明显的攻击下，初始防御代理未能达到预期效果。因此，我们采用了上下文学习
    (Dong et al., [2022](#bib.bib13)) 来进一步优化防御代理，显著增强了防御能力，几乎没有增加任何额外成本。最后，我们进行了一项有趣的在线对抗实验，创建了一个对抗系统，使用两个基于
    LLM 的代理（一个用于防御，一个用于攻击）以及一个目标 LLM 进行分类。在对抗互动过程中，防御代理和攻击代理不断相互对抗，类似于传统图像任务中的对抗训练过程
    (Goodfellow et al., [2015](#bib.bib17))。
- en: Our contributions are summarized as follows.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel defense technique named LLAMOS, which aims to purify the
    adversarial textual examples before feeding them into the target LLM. To the best
    of our knowledge, we are the first to employ an LLM agent to enhance the adversarial
    robustness of LLMs.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新的防御技术，命名为 LLAMOS，旨在对抗文本示例进行净化，然后再送入目标 LLM。根据我们的最佳知识，我们是首个使用 LLM 代理来增强
    LLM 对抗鲁棒性的方法。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing
    step. Notably, it operates without retraining of target LLM, rendering it efficient
    and user-friendly.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLAMOS中的防御代理是一个即插即用模块，作为预处理步骤。值得注意的是，它在不重新训练目标LLM的情况下运行，使其高效且用户友好。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive experiments to empirically demonstrate that the proposed
    method can effectively defend against adversarial attacks.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行了大量实验，以实证证明所提方法能够有效地防御对抗攻击。
- en: 2 Preliminary
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步工作
- en: This section briefly reviews the adversarial attacks and evaluations of adversarial
    robustness on LLMs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节简要回顾了对LLMs的对抗攻击以及对抗鲁棒性的评估。
- en: 2.1 Adversarial Attcks on LLMs
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 对LLMs的对抗攻击
- en: Given a target LLM $f_{t}$ with different system prompt (Xu et al., [2024](#bib.bib64)),
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个目标LLM $f_{t}$，具有不同的系统提示（Xu等人，[2024](#bib.bib64)），
- en: '|  | $x^{\prime}=f_{atk}(x)=x+\delta,\quad f_{t}(x^{\prime})=y^{\prime}\neq
    y,$ |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{\prime}=f_{atk}(x)=x+\delta,\quad f_{t}(x^{\prime})=y^{\prime}\neq
    y,$ |  |'
- en: 'where $\delta$ can be: “Your task is to generate a new sentence that keeps
    the same semantic meaning as the original one but be classified as a different
    label.” There are more details in [Appendix B](#A2 "Appendix B PromptAttack ‣
    Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\delta$ 可以是：“你的任务是生成一个新的句子，该句子的语义与原句相同，但被分类为不同的标签。”更多细节见[附录B](#A2 "附录B PromptAttack
    ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")。
- en: 2.2 Evaluations of LLMs Robustness
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM的鲁棒性评估
- en: To evaluate the effectiveness of the defense method, we follow the setting from
    Wang et al. ([2021](#bib.bib56)); Xu et al. ([2024](#bib.bib64)), using the attack
    success rate (ASR) and traditional robust accuracy (RA) on the adversarial examples
    as measures of the robustness of the defense method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估防御方法的有效性，我们遵循Wang等人（[2021](#bib.bib56)）和Xu等人（[2024](#bib.bib64)）的设置，使用攻击成功率（ASR）和传统的鲁棒性准确度（RA）作为防御方法鲁棒性的衡量标准。
- en: '|  | $1$2 |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\mathbbm{1}[\cdot]\in\{0,1\}$ is the number of examples. The lower the
    ASR, the higher the RA, indicating greater model robustness.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbbm{1}[\cdot]\in\{0,1\}$是示例的数量。ASR越低，RA越高，表示模型鲁棒性越强。
- en: 3 Methods
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'We propose a novel defense technique for large language model-based adversarial
    purification (LLAMOS), which purifies adversarial examples by the LLM-based defense
    agent before feeding examples into the target LLM. The overall pipeline of LLAMOS
    is outlined in [Section 3.1](#S3.SS1 "3.1 Overview of LLAMOS ‣ 3 Methods ‣ Large
    Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent"). Subsequently,
    we further augment the defense agent using in-context learning as discussed in
    [Section 3.2](#S3.SS2 "3.2 Enhencing the Defense Agent with In-Context Learning
    ‣ 3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). Finally, in [Section 3.3](#S3.SS3 "3.3 Adversarial System with
    Multiple LLMs ‣ 3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"), we present the design of the adversarial system, incorporating
    the defense agent, attack agent, and target LLM.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的大型语言模型基础的对抗净化技术（LLAMOS），通过LLM基础的防御代理在将示例输入目标LLM之前进行对抗净化。LLAMOS的整体流程在[第3.1节](#S3.SS1
    "3.1 LLAMOS概述 ‣ 3 方法 ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")中概述。随后，我们进一步通过[第3.2节](#S3.SS2
    "3.2 使用上下文学习增强防御代理 ‣ 3 方法 ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")讨论了利用上下文学习来增强防御代理。最后，在[第3.3节](#S3.SS3
    "3.3 多LLM对抗系统 ‣ 3 方法 ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")中，我们展示了对抗系统的设计，包括防御代理、攻击代理和目标LLM。
- en: 3.1 Overview of LLAMOS
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLAMOS概述
- en: 'To defend against adversarial textual attacks targeting LLM-based classification
    tasks, we propose Large LAnguage MOdel Sentinel (LLAMOS) that employs the LLM
    as a defense agent for adversarial purification. LLAMOS comprises two components:
    Agent instruction and Defense guidance. Next, we introduce the overall pipeline
    in sequential order.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防御针对LLM基础分类任务的对抗文本攻击，我们提出了大型语言模型哨兵（LLAMOS），该模型利用LLM作为对抗净化的防御代理。LLAMOS包含两个组件：代理说明和防御指导。接下来，我们按顺序介绍整体流程。
- en: In this paper, we utilize the existing LLMs denoted by target LLMs $f_{t}$ as
    described in the following.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们利用现有的LLMs，表示为目标LLMs $f_{t}$，如以下描述。
- en: Defense Agent Instruction
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防御代理说明
- en: 'To begin, let me provide a brief overview of the input text: [Input Description].
    The classification task for these sentences is [Task Description]. However, be
    aware that these sentences might be susceptible to adversarial attacks, which
    could lead to an incorrect label. Note that not all sentences will be affected
    by the attacks. Your task is to generate a new sentence that replaces the original
    one, which must satisfy the following conditions: [Defense Goal].'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我简要概述一下输入文本：[Input Description]。这些句子的分类任务是[Task Description]。然而，请注意，这些句子可能会受到对抗攻击的影响，这可能导致错误的标签。注意，并不是所有句子都会受到攻击的影响。你的任务是生成一个新的句子来替换原始句子，这个新句子必须满足以下条件：[Defense
    Goal]。
- en: Defense Guidance
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防御指导
- en: 'You can complete the task using the following guidance: [Defense Guidance].'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据以下指导完成任务：[Defense Guidance]。
- en: 'Input: [Input]. Now, let’s start the defense process and only output the generated
    sentence.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：[Input]。现在，让我们开始防御过程，并仅输出生成的句子。
- en: 'Input Description. The format of Input $x_{in}\in\{D_{i},i=1...6\}$ to correspond
    with the specific structure and content of each dataset, details in [Table 10](#A1.T10
    "In Appendix A GLUE Dataset ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"). For instance, the SST-2 dataset (Socher et al., [2013](#bib.bib47))
    typically consists of a single sentence per data point. On the other hand, the
    MNLI dataset (Williams et al., [2018](#bib.bib62)) is structured to include pairs
    of sentences labeled as premise and hypothesis.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输入描述。输入 $x_{in}\in\{D_{i},i=1...6\}$ 的格式要与每个数据集的具体结构和内容相对应，详细信息见[Table 10](#A1.T10
    "在附录A GLUE数据集 ‣ 大型语言模型哨兵：通过LLM代理推进对抗鲁棒性")。例如，SST-2数据集（Socher et al., [2013](#bib.bib47)）通常由每个数据点的单个句子组成。另一方面，MNLI数据集（Williams
    et al., [2018](#bib.bib62)）则包括被标记为前提和假设的句子对。
- en: 'Task Description. Similar to the input descriptions, the tasks associated with
    each dataset $D_{i}$ are distinct. As illustrated earlier, SST-2 focuses on determining
    the sentiment of a given sentence, making it a straightforward classification
    challenge. Conversely, MNLI presents a more complex task of natural language inference,
    where the relationship between a pair of sentences must be discerned and classified
    correctly. Detailed input and task descriptions are provided in [Table 9](#A1.T9
    "In Appendix A GLUE Dataset ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 任务描述。与输入描述类似，每个数据集 $D_{i}$ 相关的任务是不同的。如前所述，SST-2 关注于确定给定句子的情感，因此这是一个直接的分类挑战。相反，MNLI
    提出了一个更复杂的自然语言推理任务，需要辨别和正确分类一对句子之间的关系。详细的输入和任务描述见[Table 9](#A1.T9 "在附录A GLUE数据集
    ‣ 大型语言模型哨兵：通过LLM代理推进对抗鲁棒性")。
- en: 'Defense Goal. Based on traditional adversarial purification methods (Shi et al.,
    [2021](#bib.bib45); Srinivasan et al., [2021](#bib.bib48); Nie et al., [2022](#bib.bib37);
    Lin et al., [2024b](#bib.bib27)), we designed the defense goal for LLAMOS as follows:
    “1\. Keeping the semantic meaning of the new sentence the same as the original
    one; 2\. For natural examples, the new sentence should remain unchanged. For adversarial
    examples, modify the sentence so that it is classified as the correct label, effectively
    reversing the adversarial effect.”'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 防御目标。基于传统的对抗净化方法（Shi et al., [2021](#bib.bib45)；Srinivasan et al., [2021](#bib.bib48)；Nie
    et al., [2022](#bib.bib37)；Lin et al., [2024b](#bib.bib27)），我们为LLAMOS设计的防御目标如下：“1.
    保持新句子的语义与原句相同；2. 对于自然例子，新句子应保持不变。对于对抗例子，修改句子以使其被分类为正确的标签，从而有效地逆转对抗效果。”
- en: 'Defense Guidance. The defense guidance offers specific instructions to the
    defense agent on how to modify the input text to ensure effective defense and
    accurate outputs from the target LLM. In designing our guidance, we considered
    attacks at various levels (Xu et al., [2024](#bib.bib64)), including character,
    word, and sentence levels, which are presented in [Table 1](#S3.T1 "In 3.1 Overview
    of LLAMOS ‣ 3 Methods ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). These guidances are not rigidly fixed; they can be fine-tuned
    according to specific tasks.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 防御指导。防御指导提供了针对防御代理的具体指示，指导如何修改输入文本，以确保有效的防御和目标LLM的准确输出。在设计我们的指导时，我们考虑了各种级别的攻击（Xu
    et al., [2024](#bib.bib64)），包括字符级、词级和句子级，具体见[Table 1](#S3.T1 "在3.1 LLAMOS概述 ‣
    3方法 ‣ 大型语言模型哨兵：通过LLM代理推进对抗鲁棒性")。这些指导不是固定不变的；可以根据具体任务进行微调。
- en: '![Refer to caption](img/3596df9411ca53c281144e82c06ae68c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3596df9411ca53c281144e82c06ae68c.png)'
- en: 'Figure 2: Adversarial system with multiple LLMs.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 涉及多个LLMs的对抗系统。'
- en: 'Table 1: The defense guidances for defense agent.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 防御代理的防御指导。'
- en: '| Idx. | [Defense Guidance] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Idx. | [Defense Guidance] |'
- en: '| --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | Modify as few characters as possible. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 尽可能修改最少的字符。 |'
- en: '| 2 | Correct any clear spelling errors. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 纠正任何明显的拼写错误。 |'
- en: '| 3 | Eliminate redundant symbols. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 消除多余的符号。 |'
- en: '| 4 | If necessary, feel free to replace, delete, add words, or adjust the
    word order. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 如有必要，可以替换、删除、添加单词或调整单词顺序。 |'
- en: '| 5 | Improve structure for better readability. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 改善结构以提高可读性。 |'
- en: '| 6 | Ensure sentence is coherent and logical. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 确保句子连贯且合乎逻辑。 |'
- en: After the defense agent generates the new sentence, the purified example $\hat{x}=g_{stl}(x^{\prime})$
    for classification.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在防御代理生成新句子后，进行分类的净化示例为$\hat{x}=g_{stl}(x^{\prime})$。
- en: 3.2 Enhencing the Defense Agent with In-Context Learning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 通过上下文学习增强防御代理
- en: In the initial defense agent, the defense guidance relies on common sense, which
    may result in poor performance against some special attacks, even when the attacker
    adds obvious characters. To address this limitation, we introduce in-context learning
    (Dong et al., [2022](#bib.bib13)) to further optimize the defense agent. The prompts
    of in-context learning are described in the following.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始防御代理中，防御指导依赖于常识，这可能导致在面对某些特殊攻击时表现不佳，即使攻击者添加了明显的字符。为解决这一限制，我们引入了上下文学习（Dong
    et al., [2022](#bib.bib13)）以进一步优化防御代理。上下文学习的提示如下所述。
- en: In-Context Learning
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文学习
- en: The new sentence still contains a lot of harmful content caused by adversarial
    attacks, such as [Specific Guidance]. Please consider these contents and output
    a new sentence for me.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 新的句子仍然包含大量由于对抗性攻击而造成的有害内容，如 [Specific Guidance]。请考虑这些内容，为我输出一个新的句子。
- en: 'Input: [Input]. Now, let’s start the defense process and only output the generated
    sentence.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: [Input]。现在，让我们开始防御过程，并仅输出生成的句子。'
- en: The specific guidance is designed to assist the defense agent in better understanding
    an attack and generating a new sentence capable of effectively defending against
    the attack. These guidelines can be fine-tuned to address specific attacks and
    can be incorporated into the defense agent as needed. Through in-context learning,
    the defense agent can be continuously optimized, significantly enhancing its performance
    almost without adding any additional costs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 具体指导旨在帮助防御代理更好地理解攻击并生成能够有效防御攻击的新句子。这些指导方针可以根据具体攻击进行微调，并可根据需要纳入防御代理中。通过上下文学习，防御代理可以持续优化，显著提高其性能，几乎不增加任何额外成本。
- en: 3.3 Adversarial System with Multiple LLMs
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 涉及多个LLMs的对抗系统
- en: In this section, we devise an adversarial system involving multiple LLMs. Given
    that our method introduces a defense agent against attackers, a natural idea is
    to then create an attack agent to counter the defender. The attack agent is tasked
    with generating adversarial examples from purified examples to deceive the target
    LLM once more. To accomplish this, we design prompts for generating an attack
    agent $g_{atk}$, as described in the following.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们设计了一个涉及多个大型语言模型（LLMs）的对抗系统。由于我们的方法引入了一个防御代理来对抗攻击者，一个自然的想法是创建一个攻击代理来对抗防御者。攻击代理的任务是从净化示例中生成对抗性示例，以再次欺骗目标LLM。为实现这一点，我们设计了生成攻击代理$g_{atk}$的提示，如下所述。
- en: Attack Agent Instruction
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 攻击代理指导
- en: 'To begin, let me provide a brief overview of the input text: [Input Description].
    The classification task for these sentences is [Task Description]. Your task is
    to generate a new sentence that replaces the original one, which must satisfy
    the following conditions: [Attack Instruction].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，让我简要概述一下输入文本: [Input Description]。这些句子的分类任务是 [Task Description]。你的任务是生成一个新的句子来替换原句，必须满足以下条件:
    [Attack Instruction]。'
- en: Attack Guidance
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 攻击指导
- en: For example, the original sentence [Purified Example] is classified as [Correct
    Label]. You should generate a new sentence which is classified as [Incorrect Label].
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，原始句子 [Purified Example] 被分类为 [Correct Label]。你应该生成一个被分类为 [Incorrect Label]
    的新句子。
- en: 'Input: [Input]. Now, let’s start the attack process and only output the generated
    sentence.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: [Input]。现在，让我们开始攻击过程，并仅输出生成的句子。'
- en: The prompt structure of the attack agent and the defense agent is basically
    the same, although there are some differences in details. The input description
    of the attack agent includes the correct label $y$. The attack instruction is
    “1\. The new sentence should be classified as the opposite of the ‘correct label’.
    2\. Change at most two letters in the sentence.” Finally, we provide a specific
    example to help the attack agent better understand the attack task.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击代理和防御代理的提示结构基本相同，尽管在细节上有所不同。攻击代理的输入描述包括正确标签 $y$。攻击指令为：“1. 新句子应被分类为‘正确标签’的相反。2.
    句子中最多更改两个字母。”最后，我们提供一个具体示例，以帮助攻击代理更好地理解攻击任务。
- en: 'Then, we combine the defense agent and attack agent to form an adversarial
    system, as illustrated in [Figure 2](#S3.F2 "In 3.1 Overview of LLAMOS ‣ 3 Methods
    ‣ Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").
    In the adversarial system, the purified examples can be attacked again by the
    attack agent, and likewise, the adversarial examples can also be purified by the
    defense agent. They continuously counter each other, much like adversarial training
    (Goodfellow et al., [2015](#bib.bib17)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将防御代理和攻击代理结合形成对抗系统，如 [图 2](#S3.F2 "在 3.1 LLAMOS 概述 ‣ 3 方法 ‣ 大型语言模型哨兵：通过
    LLM 代理提升对抗鲁棒性") 所示。在对抗系统中，净化后的样本可以再次被攻击代理攻击，同样，对抗样本也可以被防御代理净化。它们不断相互对抗，类似于对抗训练（Goodfellow
    等，[2015](#bib.bib17)）。
- en: 4 Related Work
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Adversarial Attack. Deep neural networks (DNNs) are vulnerable to adversarial
    examples (Szegedy et al., [2014](#bib.bib50)), which are generated by adding small,
    human-imperceptible perturbations to natural examples, but completely change the
    prediction results to DNNs (Goodfellow et al., [2015](#bib.bib17); Lin et al.,
    [2024b](#bib.bib27)). With the rapidly increasing applications of LLMs (OpenAI,
    [2023](#bib.bib39); Köpf et al., [2024](#bib.bib21); Romera-Paredes et al., [2024](#bib.bib43)),
    security concerns have emerged as a critical area of research (Gehman et al.,
    [2020](#bib.bib14); Bender et al., [2021](#bib.bib3); Mckenna et al., [2023](#bib.bib33);
    Manakul et al., [2023](#bib.bib32); Liu et al., [2023b](#bib.bib29); Zhu et al.,
    [2023](#bib.bib69); Li et al., [2023](#bib.bib23); Qi et al., [2024](#bib.bib41);
    Yao et al., [2024b](#bib.bib68)), with researchers increasingly focusing on adversarial
    attacks targeting LLMs. In a similar setup to DNNs, for LLMs, attackers manipulate
    a small amount of text to change the output of the target LLM while maintaining
    the semantic information for humans (Wang et al., [2024](#bib.bib58); Xu et al.,
    [2024](#bib.bib64)). Presently, addressing the security issues surrounding LLMs
    is of paramount importance and requires urgent attention.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击。深度神经网络（DNNs）易受对抗样本的影响（Szegedy 等，[2014](#bib.bib50)），这些样本通过在自然样本中添加细微且人眼不可察觉的扰动生成，但却完全改变了
    DNNs 的预测结果（Goodfellow 等，[2015](#bib.bib17)；Lin 等，[2024b](#bib.bib27)）。随着大型语言模型（LLMs）的应用快速增长（OpenAI，[2023](#bib.bib39)；Köpf
    等，[2024](#bib.bib21)；Romera-Paredes 等，[2024](#bib.bib43)），安全问题已成为一个关键研究领域（Gehman
    等，[2020](#bib.bib14)；Bender 等，[2021](#bib.bib3)；Mckenna 等，[2023](#bib.bib33)；Manakul
    等，[2023](#bib.bib32)；Liu 等，[2023b](#bib.bib29)；Zhu 等，[2023](#bib.bib69)；Li 等，[2023](#bib.bib23)；Qi
    等，[2024](#bib.bib41)；Yao 等，[2024b](#bib.bib68)），研究人员越来越关注针对 LLMs 的对抗攻击。在与 DNNs
    相似的设置下，对于 LLMs，攻击者操控少量文本来改变目标 LLM 的输出，同时保持对人类的语义信息（Wang 等，[2024](#bib.bib58)；Xu
    等，[2024](#bib.bib64)）。目前，解决围绕 LLMs 的安全问题至关重要，需要紧急关注。
- en: Adversarial Defense. There are two main defense techniques on traditional DNNs,
    including adversarial training (AT) (Goodfellow et al., [2015](#bib.bib17)) and
    adversarial purification (AP) (Shi et al., [2021](#bib.bib45); Srinivasan et al.,
    [2021](#bib.bib48)). Unlike traditional DNNs, retraining LLMs is nearly impossible
    due to cost issues (Li et al., [2023](#bib.bib23)). Therefore, most methods enhance
    the robustness of LLMs through adversarial fine-tuning (AFT) (Xiang et al., [2024](#bib.bib63);
    Li et al., [2024b](#bib.bib25); Bianchi et al., [2023](#bib.bib5); Deng et al.,
    [2024](#bib.bib11); Qi et al., [2024](#bib.bib41)). While AFT can effectively
    defend against attacks, it remains susceptible to unseen attacks whose adversarial
    examples that the LLMs have not previously learned (Li et al., [2023](#bib.bib23)).
    Additionally, even with fine-tuning, training the LLMs will still consume a significant
    cost (Hu et al., [2022](#bib.bib19); Dettmers et al., [2024](#bib.bib12)). Adversarial
    purification (AP) aims to purify adversarial examples before feeding them into
    the target model, which has emerged as a promising defense method (Shi et al.,
    [2021](#bib.bib45); Srinivasan et al., [2021](#bib.bib48); Lin et al., [2024b](#bib.bib27)).
    Compared with the AT or AFT method, the AP method utilizes an additional model
    that can defend against unseen attacks without retraining the target model (Lin
    et al., [2024b](#bib.bib27); Li et al., [2023](#bib.bib23)). In some traditional
    computer vision and natural language processing tasks, researchers have started
    using LLMs as purifiers for adversarial purification (Singh and Subramanyam, [2024](#bib.bib46);
    Li et al., [2024a](#bib.bib24); Moraffah et al., [2024](#bib.bib35)), but the
    security issues of LLMs themselves have not been deeply considered. Therefore,
    we propose a novel LLM defense technique named LLAMOS to purify the adversarial
    textual examples before feeding them into the target LLM, aiming to improve the
    robustness of the entire system.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗防御。在传统的深度神经网络（DNNs）中，有两种主要的防御技术，包括对抗训练（AT）（Goodfellow 等人，[2015](#bib.bib17)）和对抗净化（AP）（Shi
    等人，[2021](#bib.bib45)；Srinivasan 等人，[2021](#bib.bib48)）。与传统的 DNNs 不同，由于成本问题，重新训练大型语言模型（LLMs）几乎是不可能的（Li
    等人，[2023](#bib.bib23)）。因此，大多数方法通过对抗微调（AFT）（Xiang 等人，[2024](#bib.bib63)；Li 等人，[2024b](#bib.bib25)；Bianchi
    等人，[2023](#bib.bib5)；Deng 等人，[2024](#bib.bib11)；Qi 等人，[2024](#bib.bib41)）来增强 LLMs
    的鲁棒性。虽然 AFT 可以有效地防御攻击，但它仍然容易受到 LLMs 之前未曾学到的对抗样本的攻击（Li 等人，[2023](#bib.bib23)）。此外，即使进行了微调，训练
    LLMs 仍会消耗大量成本（Hu 等人，[2022](#bib.bib19)；Dettmers 等人，[2024](#bib.bib12)）。对抗净化（AP）的目标是在将对抗样本输入目标模型之前对其进行净化，这已成为一种有前景的防御方法（Shi
    等人，[2021](#bib.bib45)；Srinivasan 等人，[2021](#bib.bib48)；Lin 等人，[2024b](#bib.bib27)）。与
    AT 或 AFT 方法相比，AP 方法利用额外的模型来防御未见过的攻击，而无需重新训练目标模型（Lin 等人，[2024b](#bib.bib27)；Li
    等人，[2023](#bib.bib23)）。在一些传统的计算机视觉和自然语言处理任务中，研究人员已经开始使用 LLMs 作为对抗净化的净化器（Singh
    和 Subramanyam，[2024](#bib.bib46)；Li 等人，[2024a](#bib.bib24)；Moraffah 等人，[2024](#bib.bib35)），但
    LLMs 自身的安全问题尚未被深入考虑。因此，我们提出了一种新颖的 LLM 防御技术，命名为 LLAMOS，旨在在将对抗文本样本输入目标 LLM 之前对其进行净化，旨在提高整个系统的鲁棒性。
- en: Large Language Model Agent. The LLM agent is a new research direction that has
    emerged in recent years (Ha et al., [2023](#bib.bib18); Mu et al., [2024](#bib.bib36);
    M. Bran et al., [2024](#bib.bib31)). This novel type of agent is capable of interacting
    with humans in natural language, leading to a significant increase in applications
    across fields such as chatbots, natural sciences, robotics, and workflows (Boiko
    et al., [2023](#bib.bib6); Yang et al., [2023](#bib.bib65); Lin et al., [2024a](#bib.bib26);
    Wang et al., [2023b](#bib.bib59); Liu et al., [2023a](#bib.bib28)). Furthermore,
    LLMs have demonstrated promising zero-shot/few-shot planning and reasoning capabilities
    across various configurations (Sumers et al., [2023](#bib.bib49)), covering specific
    environments and reasoning tasks (Yao et al., [2023](#bib.bib66); Gong et al.,
    [2023](#bib.bib16); Yao et al., [2024a](#bib.bib67)). In this paper, we introduce
    a new variant of the LLM agent designed specifically to purify adversarial textual
    examples generated by attacks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型代理。LLM 代理是近年来出现的一项新研究方向（Ha 等，[2023](#bib.bib18)；Mu 等，[2024](#bib.bib36)；M.
    Bran 等，[2024](#bib.bib31)）。这种新型代理能够用自然语言与人类互动，从而在聊天机器人、自然科学、机器人技术和工作流程等领域的应用显著增加（Boiko
    等，[2023](#bib.bib6)；Yang 等，[2023](#bib.bib65)；Lin 等，[2024a](#bib.bib26)；Wang 等，[2023b](#bib.bib59)；Liu
    等，[2023a](#bib.bib28)）。此外，LLMs 在各种配置中展示了有前景的零样本/少量样本规划和推理能力（Sumers 等，[2023](#bib.bib49)），涵盖了特定环境和推理任务（Yao
    等，[2023](#bib.bib66)；Gong 等，[2023](#bib.bib16)；Yao 等，[2024a](#bib.bib67)）。在本文中，我们介绍了一种新型的
    LLM 代理变体，专门设计用于净化由攻击生成的对抗文本示例。
- en: 5 Experiments
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: In this section, we conduct extensive experiments on GLUE datasets to evaluate
    the effectiveness of the proposed method (LLAMOS). Specifically, our method significantly
    reduces the attack success rate (ASR) by up to 37.86% with GPT-3.5 and 45.59%
    with LLAMA-2, respectively.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对 GLUE 数据集进行了广泛的实验，以评估所提出方法（LLAMOS）的有效性。具体而言，我们的方法在 GPT-3.5 上将攻击成功率（ASR）显著降低了最多
    37.86%，在 LLAMA-2 上降低了 45.59%。
- en: 5.1 Experimental Setup
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Datasets. The experiments are conducted on six tasks in GLUE datasets (Wang
    et al., [2018](#bib.bib55)), including SST-2, RTE, QQP, QNLI, MNLI-mm, MNLI-m
    (Socher et al., [2013](#bib.bib47); Dagan et al., [2005](#bib.bib10); [Bar-Haim
    et al.,](#bib.bib2) ; Giampiccolo et al., [2007](#bib.bib15); Bos and Markert,
    [2005](#bib.bib7); Bentivogli et al., [2009](#bib.bib4); Wang et al., [2017](#bib.bib61);
    Rajpurkar et al., [2016](#bib.bib42); Williams et al., [2018](#bib.bib62)). The
    detailed descriptions are provided in [Appendix A](#A1 "Appendix A GLUE Dataset
    ‣ Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。实验在 GLUE 数据集的六个任务上进行（Wang 等，[2018](#bib.bib55)），包括 SST-2、RTE、QQP、QNLI、MNLI-mm、MNLI-m（Socher
    等，[2013](#bib.bib47)；Dagan 等，[2005](#bib.bib10)；[Bar-Haim 等，](#bib.bib2)；Giampiccolo
    等，[2007](#bib.bib15)；Bos 和 Markert，[2005](#bib.bib7)；Bentivogli 等，[2009](#bib.bib4)；Wang
    等，[2017](#bib.bib61)；Rajpurkar 等，[2016](#bib.bib42)；Williams 等，[2018](#bib.bib62)）。详细描述见[附录
    A](#A1 "附录 A GLUE 数据集 ‣ 大语言模型哨兵：通过 LLM 代理推进对抗鲁棒性")。
- en: 'Adversarial Attacks. We evaluate our method against PromptAttack (Xu et al.,
    [2024](#bib.bib64)), which is a powerful attack that combines nine different types
    of attacks, as illustrated in [Table 11](#A2.T11 "In Appendix B PromptAttack ‣
    Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent").
    Furthermore, Xu et al. ([2024](#bib.bib64)) introduce the few-shot (FS) strategy
    (Logan IV et al., [2021](#bib.bib30)) and ensemble (EN) strategy (Croce and Hein,
    [2020](#bib.bib9)) to boost the attack power of PromptAttack, details in [Appendix B](#A2
    "Appendix B PromptAttack ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent")'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击。我们将我们的方法与 PromptAttack（Xu 等，[2024](#bib.bib64)）进行了比较，该攻击结合了九种不同类型的攻击，如[表
    11](#A2.T11 "在附录 B 中 PromptAttack ‣ 大语言模型哨兵：通过 LLM 代理推进对抗鲁棒性")所示。此外，Xu 等（[2024](#bib.bib64)）引入了少量样本（FS）策略（Logan
    IV 等，[2021](#bib.bib30)）和集成（EN）策略（Croce 和 Hein，[2020](#bib.bib9)）以增强 PromptAttack
    的攻击力，详细信息见[附录 B](#A2 "附录 B PromptAttack ‣ 大语言模型哨兵：通过 LLM 代理推进对抗鲁棒性")
- en: 'Evaluation Metrics. We evaluate the performance of defense methods using two
    metrics: attack success rate (ASR) and robust accuracy (RA). These metrics are
    derived from testing on adversarial examples, where a lower ASR or a higher RA
    indicates greater model robustness.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们使用两个指标来评估防御方法的性能：攻击成功率（ASR）和鲁棒准确率（RA）。这些指标来源于对抗样本的测试，其中较低的 ASR 或较高的 RA
    表示模型的鲁棒性更强。
- en: Training Details. The experiments in this paper are conducted using GPT-3.5
    (OpenAI, [2023](#bib.bib39)) with ‘GPT-3.5-Turbo-0613’ version and LLAMA-2 (Touvron
    et al., [2023b](#bib.bib54)) with ‘LLAMA-2-7b’ version. For GPT-3.5, we purchase
    OpenAI’s API service¹¹1https://openai.com/api/ and conduct testing experiments
    with the ‘openai’ package in Python. For LLAMA-2, we deploy it locally on NVIDIA
    RTX A6000 and utilize the available checkpoint published by MetaAI from HuggingFace²²2https://huggingface.co/meta-llama/.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练细节。本文的实验使用了GPT-3.5（OpenAI, [2023](#bib.bib39)）的‘GPT-3.5-Turbo-0613’版本和LLAMA-2（Touvron
    et al., [2023b](#bib.bib54)）的‘LLAMA-2-7b’版本。对于GPT-3.5，我们购买了OpenAI的API服务¹¹1https://openai.com/api/，并使用Python中的‘openai’包进行测试实验。对于LLAMA-2，我们在本地部署了NVIDIA
    RTX A6000，并利用MetaAI在HuggingFace²²2https://huggingface.co/meta-llama/上发布的可用检查点。
- en: 5.2 Results
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 结果
- en: 'Table 2: The attack success rate (ASR) defense against PromptAttack-EN and
    PromptAttack-FS-EN on the GLUE dataset with GPT-3.5\. The lower the ASR, the greater
    the model robustness.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：LLAMA-2在GLUE数据集上对PromptAttack-EN和PromptAttack-FS-EN的攻击成功率（ASR）防御。ASR越低，模型鲁棒性越强。
- en: '| Attacks | LLAMOS | SST-2 | RTE | QQP | QNLI | MNLI-mm | MNLI-m | Avg. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方式 | LLAMOS | SST-2 | RTE | QQP | QNLI | MNLI-mm | MNLI-m | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| PA-EN | $\times$ | 56.00 | 34.30 | 37.03 | 40.39 | 43.51 | 44.00 | 42.25
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| PA-EN | $\times$ | 56.00 | 34.30 | 37.03 | 40.39 | 43.51 | 44.00 | 42.25
    |'
- en: '| ✓ | 23.77 | 8.91 | 16.11 | 11.69 | 5.65 | 17.66 | 13.22 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 23.77 | 8.91 | 16.11 | 11.69 | 5.65 | 17.66 | 13.22 |'
- en: '| PA-FS-EN | $\times$ | 75.23 | 36.12 | 39.61 | 49.00 | 44.10 | 45.97 | 48.81
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| PA-FS-EN | $\times$ | 75.23 | 36.12 | 39.61 | 49.00 | 44.10 | 45.97 | 48.81
    |'
- en: '| ✓ | 48.94 | 9.58 | 16.49 | 14.33 | 7.73 | 19.05 | 19.42 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 48.94 | 9.58 | 16.49 | 14.33 | 7.73 | 19.05 | 19.42 |'
- en: 'Table 3: The attack success rate (ASR) defense against PA-EN and PA-FS-EN on
    the SST-2 dataset with LLAMA-2.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：LLAMA-2在SST-2数据集上对PA-EN和PA-FS-EN的攻击成功率（ASR）防御。
- en: '|      Defenses | PA-EN | PA-FS-EN |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|      防御措施 | PA-EN | PA-FS-EN |'
- en: '| --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|      Vanilla | 66.77 | 48.39 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|      基础模型 | 66.77 | 48.39 |'
- en: '|      LLAMOS | 21.18 | 37.82 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|      LLAMOS | 21.18 | 37.82 |'
- en: 'Table 4: Standard accuracy and average robust accuracy on LLAMA-2 defense against
    three types of PromptAttack: character (C), word (W), and sentence (S) attacks.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：LLAMA-2在三种类型的PromptAttack（字符（C）、单词（W）和句子（S）攻击）下的标准准确率和平均鲁棒性准确率。
- en: '| FS | Standard | Robust | C | W | S |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| FS | 标准 | 鲁棒性 | C | W | S |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| $\times$ | 92.18 | 47.93 | 83.59 | 85.03 | 84.62 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| $\times$ | 92.18 | 47.93 | 83.59 | 85.03 | 84.62 |'
- en: '| ✓ | 92.18 | 30.42 | 85.16 | 79.13 | 68.96 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 92.18 | 30.42 | 85.16 | 79.13 | 68.96 |'
- en: 'Evaluation of LLAMOS Performance on Attack Success Rate (ASR). We evaluate
    the ASR of the LLAMOS against PromptAttack-EN and PromptAttack-FS-EN on the GLUE
    datasets with GPT-3.5 (OpenAI, [2023](#bib.bib39)). As shown in [Table 2](#S5.T2
    "In 5.2 Results ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"), our method significantly reduces the ASR of both PromptAttack-EN
    and PromptAttack-FS-EN across all tasks. Specifically, our method achieves an
    average ASR reduction of 29.33% and 29.39%, respectively. These results demonstrate
    that LLAMOS is effective in defending against adversarial textual attacks. Additionally,
    we also evaluate the performance of LLAMOS on the SST-2 dataset with LLAMA-2 (Touvron
    et al., [2023b](#bib.bib54)), as shown in [Table 3](#S5.T3 "In 5.2 Results ‣ 5
    Experiments ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). The results are similar to the previous experiments. The ASR of
    PromptAttack-EN and PromptAttack-FS-EN is significantly reduced by 45.59% and
    10.57%, respectively.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLAMOS在攻击成功率（ASR）上的表现进行评估。我们在GLUE数据集上使用GPT-3.5（OpenAI, [2023](#bib.bib39)）评估LLAMOS对PromptAttack-EN和PromptAttack-FS-EN的ASR。如[表2](#S5.T2
    "在5.2 结果 ‣ 5 实验 ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")所示，我们的方法显著降低了PromptAttack-EN和PromptAttack-FS-EN在所有任务中的ASR。具体而言，我们的方法分别实现了29.33%和29.39%的平均ASR降低。这些结果表明LLAMOS在防御对抗性文本攻击方面是有效的。此外，我们还在SST-2数据集上使用LLAMA-2（Touvron
    et al., [2023b](#bib.bib54)）评估了LLAMOS的性能，如[表3](#S5.T3 "在5.2 结果 ‣ 5 实验 ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")所示。结果与之前的实验相似。PromptAttack-EN和PromptAttack-FS-EN的ASR分别显著降低了45.59%和10.57%。
- en: 'Evaluation of LLAMOS Performance on Robust Accuracy (RA). We evaluate the RA
    of LLAMOS on the SST-2 dataset with LLAMA-2 against three types of PromptAttack:
    character, word, and sentence attacks. In [Table 4](#S5.T4 "In 5.2 Results ‣ 5
    Experiments ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"), the accuracies in the first two columns represent the standard
    accuracy and robust accuracy without defense, while the last three columns represent
    the robust accuracy with LLAMOS. Under strong attacks, the classification accuracy
    of the target LLM decreased from 92.18% to 30.42%. LLAMOS can effectively defend
    against adversarial textual attacks, significantly improving the robust accuracy.
    Specifically, the lowest robust accuracy reaches 86.96%. Additionally, we conduct
    more comprehensive experiments across nine types of attacks and six tasks with
    GPT-3.5, as shown in [Table 5](#S5.T5 "In 5.2 Results ‣ 5 Experiments ‣ Large
    Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent"). LLAMOS
    can effectively defend against character-level attacks, achieving results on C1
    and C3 that closely match the standard accuracy.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 评估 LLAMOS 在鲁棒准确率（RA）上的表现。我们在 SST-2 数据集上评估了 LLAMOS 在 LLAMA-2 上的 RA，对三种类型的 PromptAttack
    进行了测试：字符攻击、单词攻击和句子攻击。在 [表 4](#S5.T4 "在 5.2 结果 ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理推动对抗性鲁棒性")
    中，前两列的准确率表示没有防御的标准准确率和鲁棒准确率，而后三列表示 LLAMOS 的鲁棒准确率。在强攻击下，目标 LLM 的分类准确率从 92.18% 降低到
    30.42%。LLAMOS 可以有效防御对抗性文本攻击，显著提高鲁棒准确率。具体而言，最低的鲁棒准确率达到 86.96%。此外，我们在九种攻击和六个任务中进行了更全面的实验，使用
    GPT-3.5，如 [表 5](#S5.T5 "在 5.2 结果 ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理推动对抗性鲁棒性") 所示。LLAMOS
    可以有效防御字符级别的攻击，在 C1 和 C3 上的表现与标准准确率相近。
- en: 'Table 5: Standard accuracy and robust accuracy defense against PromptAttack
    with GPT-3.5.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：GPT-3.5 对 PromptAttack 的标准准确率和鲁棒准确率防御。
- en: '| Attacks | FS | SST-2 | RTE | QQP | QNLI | MNLI-mm | MNLI-m | Avg. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | FS | SST-2 | RTE | QQP | QNLI | MNLI-mm | MNLI-m | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Standard | - | 97.66 | 80.47 | 75.78 | 66.41 | 66.41 | 71.87 | 76.43 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | - | 97.66 | 80.47 | 75.78 | 66.41 | 66.41 | 71.87 | 76.43 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Robust | $\times$ | 42.97 | 52.93 | 47.66 | 39.65 | 37.50 | 40.23 | 43.49
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Robust | $\times$ | 42.97 | 52.93 | 47.66 | 39.65 | 37.50 | 40.23 | 43.49
    |'
- en: '| ✓ | 24.22 | 51.37 | 45.70 | 33.79 | 37.11 | 38.87 | 38.51 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 24.22 | 51.37 | 45.70 | 33.79 | 37.11 | 38.87 | 38.51 |'
- en: '| C1 | $\times$ | 96.09 | 81.25 | 72.66 | 63.28 | 69.53 | 68.75 | 75.26 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| C1 | $\times$ | 96.09 | 81.25 | 72.66 | 63.28 | 69.53 | 68.75 | 75.26 |'
- en: '| ✓ | 96.88 | 81.25 | 66.41 | 64.84 | 68.75 | 66.41 | 74.09 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 96.88 | 81.25 | 66.41 | 64.84 | 68.75 | 66.41 | 74.09 |'
- en: '| C2 | $\times$ | 75.78 | 74.22 | 67.19 | 64.84 | 68.75 | 61.72 | 68.75 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| C2 | $\times$ | 75.78 | 74.22 | 67.19 | 64.84 | 68.75 | 61.72 | 68.75 |'
- en: '| ✓ | 83.59 | 77.34 | 63.28 | 60.16 | 67.97 | 58.59 | 68.49 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 83.59 | 77.34 | 63.28 | 60.16 | 67.97 | 58.59 | 68.49 |'
- en: '| C3 | $\times$ | 89.06 | 82.81 | 71.48 | 62.50 | 74.22 | 63.28 | 73.89 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| C3 | $\times$ | 89.06 | 82.81 | 71.48 | 62.50 | 74.22 | 63.28 | 73.89 |'
- en: '| ✓ | 66.41 | 81.25 | 73.44 | 64.84 | 70.31 | 69.53 | 70.96 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 66.41 | 81.25 | 73.44 | 64.84 | 70.31 | 69.53 | 70.96 |'
- en: '| W1 | $\times$ | 85.01 | 79.66 | 70.50 | 61.25 | 68.67 | 65.79 | 71.81 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| W1 | $\times$ | 85.01 | 79.66 | 70.50 | 61.25 | 68.67 | 65.79 | 71.81 |'
- en: '| ✓ | 80.18 | 77.93 | 69.92 | 59.19 | 64.63 | 63.40 | 69.21 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 80.18 | 77.93 | 69.92 | 59.19 | 64.63 | 63.40 | 69.21 |'
- en: '| W2 | $\times$ | 81.37 | 77.81 | 67.86 | 63.42 | 67.83 | 64.87 | 70.53 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| W2 | $\times$ | 81.37 | 77.81 | 67.86 | 63.42 | 67.83 | 64.87 | 70.53 |'
- en: '| ✓ | 80.04 | 75.77 | 66.15 | 61.18 | 64.28 | 63.31 | 68.46 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 80.04 | 75.77 | 66.15 | 61.18 | 64.28 | 63.31 | 68.46 |'
- en: '| W3 | $\times$ | 75.79 | 75.12 | 69.57 | 63.76 | 64.85 | 60.91 | 68.33 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| W3 | $\times$ | 75.79 | 75.12 | 69.57 | 63.76 | 64.85 | 60.91 | 68.33 |'
- en: '| ✓ | 64.48 | 75.00 | 68.17 | 61.69 | 63.00 | 60.51 | 65.47 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 64.48 | 75.00 | 68.17 | 61.69 | 63.00 | 60.51 | 65.47 |'
- en: '| S1 | $\times$ | 74.45 | 75.48 | 63.86 | 58.65 | 62.66 | 59.18 | 65.71 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| S1 | $\times$ | 74.45 | 75.48 | 63.86 | 58.65 | 62.66 | 59.18 | 65.71 |'
- en: '| ✓ | 71.18 | 72.76 | 64.01 | 56.89 | 61.79 | 58.18 | 64.14 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 71.18 | 72.76 | 64.01 | 56.89 | 61.79 | 58.18 | 64.14 |'
- en: '| S2 | $\times$ | 85.83 | 74.83 | 64.68 | 59.90 | 65.33 | 62.70 | 68.88 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| S2 | $\times$ | 85.83 | 74.83 | 64.68 | 59.90 | 65.33 | 62.70 | 68.88 |'
- en: '| ✓ | 58.77 | 76.53 | 64.52 | 59.01 | 63.39 | 61.08 | 63.88 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 58.77 | 76.53 | 64.52 | 59.01 | 63.39 | 61.08 | 63.88 |'
- en: '| S3 | $\times$ | 79.31 | 73.30 | 63.57 | 59.96 | 65.10 | 61.12 | 67.06 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| S3 | $\times$ | 79.31 | 73.30 | 63.57 | 59.96 | 65.10 | 61.12 | 67.06 |'
- en: '| ✓ | 49.86 | 75.06 | 64.59 | 58.31 | 63.25 | 62.61 | 62.28 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 49.86 | 75.06 | 64.59 | 58.31 | 63.25 | 62.61 | 62.28 |'
- en: 'Table 6: Robust accuracy against C3 attack with in-context learning.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：基于上下文学习的 C3 攻击的鲁棒准确率。
- en: '|       ICL | C3 | C3-FS |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|       ICL | C3 | C3-FS |'
- en: '| --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|       $\times$ | 89.06 | 66.41 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|       $\times$ | 89.06 | 66.41 |'
- en: '|       ✓ | 97.66 | 92.19 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|       ✓ | 97.66 | 92.19 |'
- en: 'Table 7: Robust accuracy defense against the attack agent under different iterations
    with GPT-3.5.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在不同迭代下，针对攻击代理的鲁棒性防御与 GPT-3.5。
- en: '| Iterations | Iter. 1 | Iter. 2 | Iter. 3 | Iter. 4 | Iter. 5 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 迭代次数 | 迭代 1 | 迭代 2 | 迭代 3 | 迭代 4 | 迭代 5 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Defense | 96.09 | 84.38 | 83.59 | 91.41 | 90.63 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 防御 | 96.09 | 84.38 | 83.59 | 91.41 | 90.63 |'
- en: '| Attack | 56.25 | 43.53 | 45.93 | 50.34 | 28.13 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 56.25 | 43.53 | 45.93 | 50.34 | 28.13 |'
- en: 'Evaluation of LLAMOS Performance with In-Context Learning (ICL). The C3-based
    attack (Xu et al., [2024](#bib.bib64)) is a very obvious attack that adds up to
    two extraneous characters to the end of the sentence, as shown in [Table 8](#S5.T8
    "In 5.3 Discussion ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing
    Adversarial Robustness by LLM Agent"). However, our method only achieves robust
    accuracies of 89.06% for the C3 attack and 66.41% for the C3-FS attack, respectively.
    To further improve the robustness, we introduce ICL to enhance the performance
    of the defense agent. As shown in [Table 6](#S5.T6 "In 5.2 Results ‣ 5 Experiments
    ‣ Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent"),
    the defense agent with ICL significantly improves the robust accuracy against
    the C3 attack, achieving a robust accuracies of 97.66% for the C3 attack and 92.19%
    for the C3-FS attack, respectively.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: LLAMOS 性能评估与上下文学习（ICL）。C3 基于攻击（Xu 等人，[2024](#bib.bib64)）是一种非常明显的攻击，它在句子的末尾添加了最多两个额外字符，如[表
    8](#S5.T8 "在 5.3 讨论 ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理提升对抗鲁棒性")所示。然而，我们的方法仅在 C3 攻击和 C3-FS
    攻击中分别实现了 89.06% 和 66.41% 的鲁棒准确率。为了进一步提高鲁棒性，我们引入了 ICL 以增强防御代理的性能。如[表 6](#S5.T6
    "在 5.2 结果 ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理提升对抗鲁棒性")所示，使用 ICL 的防御代理在 C3 攻击中显著提高了鲁棒准确率，分别达到了
    97.66% 的 C3 攻击和 92.19% 的 C3-FS 攻击鲁棒准确率。
- en: 'Analysis of Adversarial System. We conduct experiments with an adversarial
    system and evaluate the robust accuracy defense against adversarial examples generated
    by the attack agent over multiple iterations. As shown in [Table 7](#S5.T7 "In
    5.2 Results ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"), the defense agent initially achieves a robust accuracy
    of 96.09% in the first round of confrontation. However, after the purified examples
    are re-attacked by the attack agent, the robust accuracy decreases to 56.25%.
    The defense agent then purifies these adversarial examples again, leading to an
    increase in robust accuracy, but it will decrease once more by subsequent attacks.
    This continual fluctuation in robust accuracy is a common phenomenon in adversarial
    training (Goodfellow et al., [2015](#bib.bib17)). Upon reviewing the generated
    texts, we observe that after several rounds of confrontation, both the defense
    agent and attack agent may generate the same sentences as previous ones, resulting
    in a potential infinite loop, as shown in [Table 8](#S5.T8 "In 5.3 Discussion
    ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial Robustness
    by LLM Agent"). This is an interesting phenomenon that requires further investigation,
    particularly strategies to disrupt such loops.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗系统的分析。我们通过对抗系统进行实验，并评估对攻击代理生成的对抗样本的鲁棒性防御在多个迭代中的表现。如[表 7](#S5.T7 "在 5.2 结果
    ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理提升对抗鲁棒性")所示，防御代理在第一次对抗中最初实现了 96.09% 的鲁棒准确率。然而，在经过攻击代理重新攻击后，鲁棒准确率下降至
    56.25%。防御代理随后再次净化这些对抗样本，从而提高了鲁棒准确率，但在随后的攻击中又会下降。这种鲁棒准确率的持续波动是对抗训练中的一种常见现象（Goodfellow
    等人，[2015](#bib.bib17)）。在回顾生成的文本时，我们观察到在经过几轮对抗后，防御代理和攻击代理可能会生成与之前相同的句子，从而导致潜在的无限循环，如[表
    8](#S5.T8 "在 5.3 讨论 ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理提升对抗鲁棒性")所示。这是一个有趣的现象，需要进一步研究，特别是破坏这种循环的策略。
- en: 5.3 Discussion
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 讨论
- en: 'The Advantages of LLAMOS. As emphasized by the experimental results presented
    in [Section 5.2](#S5.SS2 "5.2 Results ‣ 5 Experiments ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent") and [Table 12](#A3.T12 "In Appendix
    C More Comparasion Results of Input Examples ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent"), LLAMOS significantly enhances
    performance across various tasks and attacks with LLAMA-2 and GPT-3.5\. Additionally,
    the defense agent in LLAMOS is a plug-and-play module, serving as a pre-processing
    step. Through in-context learning (ICL), the defense agent can be continuously
    optimized to defend against emerging attacks. This invisibly resolves a major
    challenge in adversarial robustness: Due to the significant differences between
    different attacks, the model trained on specific attacks often fails to generalize
    to other unseen attacks (Poursaeed et al., [2021](#bib.bib40); Laidlaw et al.,
    [2021](#bib.bib22); Tack et al., [2022](#bib.bib51)). The model necessitates to
    be continuously fine-tuning to adapt to emerging attacks. However, fine-tuning
    parameters requires substantial costs (Hu et al., [2022](#bib.bib19); Dettmers
    et al., [2024](#bib.bib12)), and the emergence of new attack techniques continually
    makes it impractical to train the model to defend against emerging attacks(Laidlaw
    et al., [2021](#bib.bib22); Lin et al., [2024b](#bib.bib27)). In contrast, our
    method can effectively enhance the robustness through ICL without adjusting the
    parameters of the LLMs, which is undoubtedly a significant advantage.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: LLAMOS 的优势。正如 [第 5.2 节](#S5.SS2 "5.2 结果 ‣ 5 实验 ‣ 大型语言模型哨兵：通过 LLM 代理推进对抗性鲁棒性")
    和 [表 12](#A3.T12 "附录 C 输入示例的更多比较结果 ‣ 大型语言模型哨兵：通过 LLM 代理推进对抗性鲁棒性") 中的实验结果所强调的，LLAMOS
    在 LLAMA-2 和 GPT-3.5 上显著提升了各种任务和攻击的性能。此外，LLAMOS 中的防御代理是一个即插即用模块，作为预处理步骤。通过上下文学习（ICL），防御代理可以不断优化以防御新兴攻击。这在隐形中解决了对抗性鲁棒性的一个主要挑战：由于不同攻击之间的显著差异，训练于特定攻击的模型往往无法推广到其他未见过的攻击（Poursaeed
    等，[2021](#bib.bib40)；Laidlaw 等，[2021](#bib.bib22)；Tack 等，[2022](#bib.bib51)）。模型需要不断微调以适应新兴攻击。然而，微调参数需要大量成本（Hu
    等，[2022](#bib.bib19)；Dettmers 等，[2024](#bib.bib12)），而新攻击技术的出现不断使得训练模型以防御新兴攻击变得不切实际（Laidlaw
    等，[2021](#bib.bib22)；Lin 等，[2024b](#bib.bib27)）。相比之下，我们的方法可以通过 ICL 有效增强鲁棒性，而无需调整
    LLM 的参数，这无疑是一个重要的优势。
- en: 'Table 8: Comparison between clean examples, adversarial examples, and purified
    examples. When incorrectly classified, the modified area is highlighted with a
    yellow background.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：干净示例、对抗示例和纯化示例之间的比较。当分类错误时，修改的区域用黄色背景突出显示。
- en: '| Method | [Input] | Prediction |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | [输入] | 预测 |'
- en: '| Target LLM | Clean (Original): a fast , funny , highly enjoyable movie .
    | Positive |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 目标 LLM | 干净（原始）：一部快速、有趣、非常令人愉快的电影。 | 正面 |'
- en: '| C3 Attack | Adversarial: a fast , funny , highly enjoyable movie . :( | Negative
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| C3 攻击 | 对抗性：一部快速、有趣、非常令人愉快的电影。 :( | 负面 |'
- en: '| LLAMOS | Purified: a fast , funny , highly enjoyable movie . :( | Negative
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| LLAMOS | 纯化：一部快速、有趣、非常令人愉快的电影。 :( | 负面 |'
- en: '| LLAMOS+ICL | Purified: a fast , funny , highly enjoyable movie .. | Positive
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| LLAMOS+ICL | 纯化：一部快速、有趣、非常令人愉快的电影。 | 正面 |'
- en: '| Iteration 1 | Purified: You don’t have to know music to appreciate the film’s
    easygoing blend of tragedy and romance. | Positive |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 迭代 1 | 纯化：你不必懂音乐也能欣赏电影那轻松的悲剧与浪漫融合。 | 正面 |'
- en: '| Adversarial: Youdo n’thave to know music to appreciate … | Negative |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性：你不必懂音乐也能欣赏… | 负面 |'
- en: '| Iteration 2 | Purified: You don’t have to know music to appreciate … | Positive
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 迭代 2 | 纯化：你不必懂音乐也能欣赏… | 正面 |'
- en: '| Adversarial: You don’t have to know music todepreciate… | Negative |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性：你不必懂音乐也能贬低… | 负面 |'
- en: '| Iteration 3 | Purified: You don’t have to know music to appreciate … | Positive
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 迭代 3 | 纯化：你不必懂音乐也能欣赏… | 正面 |'
- en: '| Adversarial: You don’t have to know music todepreciate… | Negative |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 对抗性：你不必懂音乐也能贬低… | 负面 |'
- en: '| … | … | … |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … |'
- en: 'The Challenges in LLM-based Defense. The defense agent is tasked with purifying
    adversarial examples, but it is difficult to distinguish between natural examples
    and adversarial examples in some cases. As shown in [Table 12](#A3.T12 "In Appendix
    C More Comparasion Results of Input Examples ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent").4, the attacker altered the original
    meaning by inserting ‘not’, rendering the adversarial example indistinguishable
    from a natural example, resulting in the defense agent failing to generate the
    correct sentence. Although we hope that the defense agent can observe the sentences
    like humans, it presents a huge challenge at present. Unlike the attacker or humans,
    the defense agent lacks access to the original label of the input sentence. Furthermore,
    although the defense agent can effectively defend against adversarial attacks,
    it cannot prevent subsequent attacks, as illustrated in [Table 7](#S5.T7 "In 5.2
    Results ‣ 5 Experiments ‣ Large Language Model Sentinel: Advancing Adversarial
    Robustness by LLM Agent"). For instance, the malicious LLMs can embed specific
    system prompts to influence the output, which is unbeknownst to users; they can
    add ‘:)’ to each input sentence for prediction rather than predicting the original
    sentence. In this case, the defense agent also fails to defend. This issue is
    also an important problem in traditional adversarial training (Goodfellow et al.,
    [2015](#bib.bib17)), and no method has completely resolved this issue. Nonetheless,
    as previously discussed, LLMs offer advantages not available to traditional DNNs,
    and we have naturally solved one challenge in adversarial robustness, which is
    that the model can adapt to new attacks. Hence, future advancements may resolve
    adversarial issues of attack and defense within LLM frameworks, representing a
    challenging but promising research direction.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于LLM的防御挑战**。防御代理的任务是净化对抗样本，但在某些情况下，很难区分自然样本和对抗样本。如[表12](#A3.T12 "在附录C中更多输入样本的比较结果
    ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")所示，攻击者通过插入‘not’改变了原始含义，使对抗样本无法与自然样本区分，导致防御代理未能生成正确的句子。尽管我们希望防御代理能够像人类一样观察句子，但这在目前仍然是一个巨大的挑战。与攻击者或人类不同，防御代理无法访问输入句子的原始标签。此外，尽管防御代理可以有效防御对抗攻击，但它无法防止随后的攻击，如[表7](#S5.T7
    "在5.2结果 ‣ 5个实验 ‣ 大型语言模型哨兵：通过LLM代理提升对抗鲁棒性")所示。例如，恶意LLMs可以嵌入特定的系统提示来影响输出，这对用户来说是未知的；它们可以在每个输入句子中添加‘:)’进行预测，而不是预测原始句子。在这种情况下，防御代理也无法进行防御。这一问题在传统的对抗训练（Goodfellow等人，[2015](#bib.bib17)）中也是一个重要问题，至今没有方法完全解决。然而，如前所述，LLMs提供了传统DNNs所没有的优势，我们自然地解决了对抗鲁棒性中的一个挑战，即模型可以适应新的攻击。因此，未来的进展可能会在LLM框架内解决攻击和防御的对抗问题，这代表了一个具有挑战性但充满希望的研究方向。'
- en: Limitations. It is well-known that training large language models (LLMs) requires
    significant resources and generates substantial carbon emissions, thereby burdening
    the planet. However, the long-term inference costs of LLMs far exceed the training
    costs. Chien et al. ([2023](#bib.bib8)) show that for ChatGPT-like services, inference
    dominates emissions due to its large user base, in one year producing 25 times
    the carbon emissions of training GPT-3. Our method introduces a defense agent
    for additional inference, which inevitably increases carbon emissions during the
    inference process, thereby exacerbating the negative impact on the climate, which
    is a limitation of our work. However, considering the nascent stage of LLM development,
    the trustworthiness issue is equally crucial. Therefore, we have dedicated significant
    effort to this area, but of course, we aspire to find future solutions that adequately
    address the environmental and climate challenges posed by AI.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**限制**。众所周知，训练大型语言模型（LLMs）需要大量资源并产生显著的碳排放，从而对地球造成负担。然而，LLMs的长期推理成本远高于训练成本。Chien等人（[2023](#bib.bib8)）显示，对于类似ChatGPT的服务，由于其庞大的用户基础，推理产生的碳排放是训练的25倍。我们的方法引入了一个额外推理的防御代理，这不可避免地增加了推理过程中的碳排放，从而加剧了对气候的负面影响，这是我们工作的一个局限。然而，考虑到LLM发展的初期阶段，可信度问题同样重要。因此，我们在这一领域投入了大量精力，但当然，我们希望找到未来能够充分解决AI所带来的环境和气候挑战的解决方案。'
- en: Impact Statements. This paper presents research aimed at enhancing the robustness
    of large language models (LLMs). With the rapidly increasing applications of LLMs,
    their security and trustworthiness have become a critical concern. Our work focuses
    on this significant issue and contributes positively to potential societal impacts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 影响声明。本论文提出了旨在增强大语言模型（LLMs）鲁棒性的研究。随着LLMs应用的快速增长，它们的安全性和可信度已成为一个关键问题。我们的工作聚焦于这一重要问题，并对潜在的社会影响做出积极贡献。
- en: 6 Conclusion
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we propose LLAMOS, a novel LLM-based defense technique designed
    to purify adversarial examples before feeding them into the target LLM. The defense
    agent within LLAMOS operates as a plug-and-play module that functions effectively
    as a pre-processing step without requiring retraining of the target LLM. We conduct
    extensive experiments across various tasks and attacks with LLAMA-2 and GPT-3.5\.
    The results demonstrate that LLAMOS can effectively defend against adversarial
    attacks. Furthermore, we discuss certain existing shortcomings and challenges,
    which we aim to address in future research.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了LLAMOS，一种新颖的基于LLM的防御技术，旨在在将对抗样本输入目标LLM之前对其进行净化。LLAMOS中的防御模块作为一个即插即用的模块进行操作，作为预处理步骤有效运行，无需重新训练目标LLM。我们在各种任务和攻击下进行了广泛的实验，使用了LLAMA-2和GPT-3.5。结果表明，LLAMOS可以有效抵御对抗攻击。此外，我们讨论了某些现有的不足和挑战，我们旨在未来的研究中加以解决。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人（2023）Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge Akkaya、Florencia
    Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal Anadkat等人。GPT-4技术报告。*arXiv预印本
    arXiv:2303.08774*，2023年。
- en: (2) Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo
    Magnini, and Idan Szpektor. The second pascal recognising textual entailment challenge.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Roy Bar-Haim、Ido Dagan、Bill Dolan、Lisa Ferro、Danilo Giampiccolo、Bernardo
    Magnini和Idan Szpektor。第二届Pascal文本蕴涵识别挑战。
- en: 'Bender et al. (2021) Emily M Bender, Timnit Gebru, Angelina McMillan-Major,
    and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language
    models be too big? In *Proceedings of the 2021 ACM conference on fairness, accountability,
    and transparency*, pages 610–623, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender等人（2021）Emily M Bender、Timnit Gebru、Angelina McMillan-Major和Shmargaret
    Shmitchell。关于随机鹦鹉的危险：语言模型是否会过于庞大？见于*2021年ACM公平性、责任性和透明度会议论文集*，第610–623页，2021年。
- en: Bentivogli et al. (2009) Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
    Giampiccolo. The fifth pascal recognizing textual entailment challenge. *TAC*,
    7(8):1, 2009.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bentivogli等人（2009）Luisa Bentivogli、Peter Clark、Ido Dagan和Danilo Giampiccolo。第五届Pascal文本蕴涵识别挑战。*TAC*，7(8):1，2009年。
- en: 'Bianchi et al. (2023) Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul
    Röttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. Safety-tuned llamas:
    Lessons from improving the safety of large language models that follow instructions.
    *arXiv preprint arXiv:2309.07875*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchi等人（2023）Federico Bianchi、Mirac Suzgun、Giuseppe Attanasio、Paul Röttger、Dan
    Jurafsky、Tatsunori Hashimoto和James Zou。安全调优的LLAMAS：改进遵循指令的大语言模型安全性的经验教训。*arXiv预印本
    arXiv:2309.07875*，2023年。
- en: Boiko et al. (2023) Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent
    autonomous scientific research capabilities of large language models. *arXiv preprint
    arXiv:2304.05332*, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boiko等人（2023）Daniil A Boiko、Robert MacKnight和Gabe Gomes。大语言模型的自主科学研究能力的突现。*arXiv预印本
    arXiv:2304.05332*，2023年。
- en: Bos and Markert (2005) Johan Bos and Katja Markert. Recognising textual entailment
    with logical inference. In *Proceedings of Human Language Technology Conference
    and Conference on Empirical Methods in Natural Language Processing*, pages 628–635,
    2005.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bos和Markert（2005）Johan Bos和Katja Markert。利用逻辑推理识别文本蕴涵。见于*人类语言技术会议和自然语言处理经验方法会议论文集*，第628–635页，2005年。
- en: Chien et al. (2023) Andrew A Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan
    Sharma, and Rajini Wijayawardana. Reducing the carbon impact of generative ai
    inference (today and in 2035). In *Proceedings of the 2nd Workshop on Sustainable
    Computer Systems*, pages 1–7, 2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chien等人（2023）Andrew A Chien、Liuzixuan Lin、Hai Nguyen、Varsha Rao、Tristan Sharma和Rajini
    Wijayawardana。减少生成AI推理的碳影响（今天和2035年）。见于*第二届可持续计算系统研讨会论文集*，第1–7页，2023年。
- en: Croce and Hein (2020) Francesco Croce and Matthias Hein. Reliable evaluation
    of adversarial robustness with an ensemble of diverse parameter-free attacks.
    In *International conference on machine learning*, pages 2206–2216\. PMLR, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce和Hein（2020） Francesco Croce 和 Matthias Hein. 通过多样的无参数攻击集来可靠地评估对抗鲁棒性。在*国际机器学习会议*，第2206–2216页，PMLR，2020年。
- en: Dagan et al. (2005) Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal
    recognising textual entailment challenge. In *Machine learning challenges workshop*,
    pages 177–190\. Springer, 2005.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dagan等（2005） Ido Dagan, Oren Glickman, 和 Bernardo Magnini. Pascal 识别文本蕴涵挑战。在*机器学习挑战工作坊*，第177–190页，Springer，2005年。
- en: Deng et al. (2024) Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing.
    Multilingual jailbreak challenges in large language models. In *The Twelfth International
    Conference on Learning Representations*, 2024.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng等（2024） Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, 和 Lidong Bing. 大型语言模型中的多语言越狱挑战。在*第十二届国际学习表征会议*，2024年。
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers等（2024） Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    Qlora: 高效微调量化LLMs。*神经信息处理系统进展*，36，2024年。'
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning.
    *arXiv preprint arXiv:2301.00234*, 2022.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等（2022） Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang,
    Xu Sun, Jingjing Xu, 和 Zhifang Sui. 关于上下文学习的调查。*arXiv 预印本 arXiv:2301.00234*，2022年。
- en: 'Gehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi,
    and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in
    language models. *Findings of the Association for Computational Linguistics: EMNLP
    2020*, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gehman等（2020） Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, 和
    Noah A Smith. Realtoxicityprompts: 评估语言模型中的神经毒性退化。*计算语言学协会发现: EMNLP 2020*，2020年。'
- en: Giampiccolo et al. (2007) Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
    William B Dolan. The third pascal recognizing textual entailment challenge. In
    *Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing*,
    pages 1–9, 2007.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giampiccolo等（2007） Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, 和 William
    B Dolan. 第三届pascal识别文本蕴涵挑战。在*ACL-PASCAL文本蕴涵与释义研讨会论文集*，第1–9页，2007年。
- en: 'Gong et al. (2023) Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante,
    Yusuke Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei, et al.
    Mindagent: Emergent gaming interaction. *arXiv preprint arXiv:2309.09971*, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gong等（2023） Ran Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane Durante, Yusuke
    Noda, Zilong Zheng, Song-Chun Zhu, Demetri Terzopoulos, Li Fei-Fei 等。Mindagent:
    新兴的游戏互动。*arXiv 预印本 arXiv:2309.09971*，2023年。'
- en: Goodfellow et al. (2015) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    Explaining and harnessing adversarial examples. *International Conference on Learning
    Representations*, 2015.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等（2015） Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy. 解释和利用对抗性示例。*国际学习表征会议*，2015年。
- en: 'Ha et al. (2023) Huy Ha, Pete Florence, and Shuran Song. Scaling up and distilling
    down: Language-guided robot skill acquisition. In *Conference on Robot Learning*,
    pages 3766–3777\. PMLR, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ha等（2023） Huy Ha, Pete Florence, 和 Shuran Song. 扩大和精炼: 语言引导的机器人技能获取。在*机器人学习会议*，第3766–3777页，PMLR，2023年。'
- en: 'Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. In *International Conference on Learning Representations*,
    2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu等（2022） Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. LoRA: 大型语言模型的低秩适应。在*国际学习表征会议*，2022年。'
- en: Kasneci et al. (2023) Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria
    Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,
    Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and challenges of
    large language models for education. *Learning and individual differences*, 103:102274,
    2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kasneci等（2023） Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert,
    Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke
    Hüllermeier 等。Chatgpt 好吗？关于大型语言模型在教育中的机遇与挑战。*学习与个体差异*，103:102274，2023年。
- en: Köpf et al. (2024) Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris
    Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver
    Stanley, Richárd Nagyfi, et al. Openassistant conversations-democratizing large
    language model alignment. *Advances in Neural Information Processing Systems*,
    36, 2024.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Köpf 等（2024）**Andreas Köpf**、**Yannic Kilcher**、**Dimitri von Rütte**、**Sotiris
    Anagnostidis**、**Zhi Rui Tam**、**Keith Stevens**、**Abdullah Barhoum**、**Duc Nguyen**、**Oliver
    Stanley**、**Richárd Nagyfi** 等。Openassistant 对话——民主化大语言模型对齐。*神经信息处理系统进展*，36，2024。
- en: 'Laidlaw et al. (2021) C Laidlaw, S Singla, and S Feizi. Perceptual adversarial
    robustness: Defense against unseen threat models. In *International Conference
    on Learning Representations (ICLR)*, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laidlaw 等（2021）**C Laidlaw**、**S Singla** 和 **S Feizi**。感知对抗性鲁棒性：防御未知威胁模型。在*国际学习表征会议（ICLR）*，2021。
- en: 'Li et al. (2023) Linyang Li, Demin Song, and Xipeng Qiu. Text adversarial purification
    as defense against adversarial attacks. In *Proceedings of the 61st Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages
    338–350, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）**Linyang Li**、**Demin Song** 和 **Xipeng Qiu**。文本对抗性净化作为对抗攻击的防御。在*第
    61 届计算语言学协会年会论文集（第 1 卷：长论文）*，第 338–350 页，2023。
- en: Li et al. (2024a) Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang
    Liu, and Min Lin. Purifying large language models by ensembling a small language
    model. *arXiv preprint arXiv:2402.14845*, 2024a.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2024a）**Tianlin Li**、**Qian Liu**、**Tianyu Pang**、**Chao Du**、**Qing Guo**、**Yang
    Liu** 和 **Min Lin**。通过集成小型语言模型净化大语言模型。*arXiv 预印本 arXiv:2402.14845*，2024a。
- en: 'Li et al. (2024b) Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing
    Liu, Wenhan Wang, Tianwei Zhang, and Yang Liu. Badedit: Backdooring large language
    models by model editing. In *The Twelfth International Conference on Learning
    Representations*, 2024b.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2024b）**Yanzhou Li**、**Tianlin Li**、**Kangjie Chen**、**Jian Zhang**、**Shangqing
    Liu**、**Wenhan Wang**、**Tianwei Zhang** 和 **Yang Liu**。Badedit：通过模型编辑对大语言模型进行后门攻击。在*第十二届国际学习表征会议*，2024b。
- en: 'Lin et al. (2024a) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman,
    Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang
    Ren. Swiftsage: A generative agent with fast and slow thinking for complex interactive
    tasks. *Advances in Neural Information Processing Systems*, 36, 2024a.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2024a）**Bill Yuchen Lin**、**Yicheng Fu**、**Karina Yang**、**Faeze Brahman**、**Shiyu
    Huang**、**Chandra Bhagavatula**、**Prithviraj Ammanabrolu**、**Yejin Choi** 和 **Xiang
    Ren**。Swiftsage：具有快速和慢速思维的生成智能体用于复杂的互动任务。*神经信息处理系统进展*，36，2024a。
- en: 'Lin et al. (2024b) Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, and
    Qibin Zhao. Adversarial training on purification (ATop): Advancing both robustness
    and generalization. In *The Twelfth International Conference on Learning Representations*,
    2024b.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2024b）**Guang Lin**、**Chao Li**、**Jianhai Zhang**、**Toshihisa Tanaka**
    和 **Qibin Zhao**。对抗训练的净化（ATop）：同时提升鲁棒性和泛化能力。在*第十二届国际学习表征会议*，2024b。
- en: 'Liu et al. (2023a) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating
    llms as agents. *arXiv preprint arXiv:2308.03688*, 2023a.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023a）**Xiao Liu**、**Hao Yu**、**Hanchen Zhang**、**Yifan Xu**、**Xuanyu
    Lei**、**Hanyu Lai**、**Yu Gu**、**Hangliang Ding**、**Kaiwen Men**、**Kejuan Yang**
    等。Agentbench：评估大语言模型作为智能体。*arXiv 预印本 arXiv:2308.03688*，2023a。
- en: 'Liu et al. (2023b) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt
    engineering: An empirical study. *arXiv preprint arXiv:2305.13860*, 2023b.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023b）**Yi Liu**、**Gelei Deng**、**Zhengzi Xu**、**Yuekang Li**、**Yaowen
    Zheng**、**Ying Zhang**、**Lida Zhao**、**Tianwei Zhang** 和 **Yang Liu**。通过提示工程破解
    ChatGPT：一项实证研究。*arXiv 预印本 arXiv:2305.13860*，2023b。
- en: 'Logan IV et al. (2021) Robert L Logan IV, Ivana Balažević, Eric Wallace, Fabio
    Petroni, Sameer Singh, and Sebastian Riedel. Cutting down on prompts and parameters:
    Simple few-shot learning with language models. *arXiv preprint arXiv:2106.13353*,
    2021.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logan IV 等（2021）**Robert L Logan IV**、**Ivana Balažević**、**Eric Wallace**、**Fabio
    Petroni**、**Sameer Singh** 和 **Sebastian Riedel**。减少提示和参数：通过语言模型实现简单的少样本学习。*arXiv
    预印本 arXiv:2106.13353*，2021。
- en: M. Bran et al. (2024) Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari,
    Andrew D White, and Philippe Schwaller. Augmenting large language models with
    chemistry tools. *Nature Machine Intelligence*, pages 1–11, 2024.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M. Bran 等（2024）**Andres M. Bran**、**Sam Cox**、**Oliver Schilter**、**Carlo Baldassari**、**Andrew
    D White** 和 **Philippe Schwaller**。利用化学工具增强大语言模型。*自然机器智能*，第 1–11 页，2024。
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark Gales. Selfcheckgpt:
    Zero-resource black-box hallucination detection for generative large language
    models. In *The 2023 Conference on Empirical Methods in Natural Language Processing*,
    2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manakul 等 (2023) Potsawee Manakul, Adian Liusie, 和 Mark Gales. Selfcheckgpt：针对生成性大型语言模型的零资源黑箱幻觉检测。发表于*2023年自然语言处理实证方法会议*，2023年。
- en: 'Mckenna et al. (2023) Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Hosseini,
    Mark Johnson, and Mark Steedman. Sources of hallucination by large language models
    on inference tasks. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pages 2758–2774, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mckenna 等 (2023) Nick Mckenna, Tianyi Li, Liang Cheng, Mohammad Hosseini, Mark
    Johnson, 和 Mark Steedman. 大型语言模型在推理任务中产生幻觉的来源。发表于*计算语言学协会年会论文集：EMNLP 2023*，第2758–2774页，2023年。
- en: 'Minaee et al. (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey.
    *arXiv preprint arXiv:2402.06196*, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等 (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain, 和 Jianfeng Gao. 大型语言模型：一项调查。*arXiv 预印本 arXiv:2402.06196*，2024年。
- en: 'Moraffah et al. (2024) Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee,
    and Huan Liu. Adversarial text purification: A large language model approach for
    defense. In *Pacific-Asia Conference on Knowledge Discovery and Data Mining*,
    pages 65–77\. Springer, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moraffah 等 (2024) Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee, 和 Huan
    Liu. 对抗性文本净化：一种大型语言模型防御方法。发表于*太平洋-亚洲知识发现与数据挖掘大会*，第65–77页。Springer，2024年。
- en: 'Mu et al. (2024) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,
    Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language
    pre-training via embodied chain of thought. *Advances in Neural Information Processing
    Systems*, 36, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等 (2024) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun
    Jin, Bin Wang, Jifeng Dai, Yu Qiao, 和 Ping Luo. Embodiedgpt：通过体现思维链的视觉-语言预训练。*神经信息处理系统进展*，36，2024年。
- en: Nie et al. (2022) Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat,
    and Animashree Anandkumar. Diffusion models for adversarial purification. In *International
    Conference on Machine Learning*, pages 16805–16827\. PMLR, 2022.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等 (2022) Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat,
    和 Animashree Anandkumar. 用于对抗性净化的扩散模型。发表于*国际机器学习大会*，第16805–16827页。PMLR，2022年。
- en: OpenAI (2022) OpenAI. Introducing chatgpt. 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 介绍 chatgpt。2022年。网址 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。
- en: OpenAI (2023) OpenAI. GPT-4V(ision) system card. 2023. URL [https://cdn.openai.com/papers/GPTV_System_Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. GPT-4V(ision) 系统卡。2023年。网址 [https://cdn.openai.com/papers/GPTV_System_Card.pdf](https://cdn.openai.com/papers/GPTV_System_Card.pdf)。
- en: Poursaeed et al. (2021) Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie,
    and Ser-Nam Lim. Robustness and generalization via generative adversarial training.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    pages 15711–15720, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poursaeed 等 (2021) Omid Poursaeed, Tianxing Jiang, Harry Yang, Serge Belongie,
    和 Ser-Nam Lim. 通过生成对抗训练提高鲁棒性和泛化能力。发表于*IEEE/CVF国际计算机视觉会议论文集*，第15711–15720页，2021年。
- en: Qi et al. (2024) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety,
    even when users do not intend to! In *The Twelfth International Conference on
    Learning Representations*, 2024.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等 (2024) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, 和 Peter Henderson. 精细调优对齐语言模型会妥协安全，即使用户没有意图！发表于*第十二届国际学习表征大会*，2024年。
- en: 'Rajpurkar et al. (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
    Percy Liang. Squad: 100,000+ questions for machine comprehension of text. *arXiv
    preprint arXiv:1606.05250*, 2016.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等 (2016) Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, 和 Percy
    Liang. Squad：100,000+ 个机器理解文本的问题。*arXiv 预印本 arXiv:1606.05250*，2016年。
- en: Romera-Paredes et al. (2024) Bernardino Romera-Paredes, Mohammadamin Barekatain,
    Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz,
    Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries
    from program search with large language models. *Nature*, 625(7995):468–475, 2024.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Romera-Paredes 等 (2024) Bernardino Romera-Paredes, Mohammadamin Barekatain,
    Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz,
    Jordan S Ellenberg, Pengming Wang, Omar Fawzi 等. 从程序搜索中发现的数学发现，利用大型语言模型。*自然*，625(7995)：468–475，2024年。
- en: 'Shen et al. (2023) Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong
    Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment:
    A survey. *arXiv preprint arXiv:2309.15025*, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2023) 田浩申、任任金、余飞黄、创刘、韦龙董、子山郭、欣伟吴、彦刘和德意熊。大型语言模型对齐：一项调查。*arXiv 预印本
    arXiv:2309.15025*，2023年。
- en: Shi et al. (2021) Changhao Shi, Chester Holtz, and Gal Mishne. Online adversarial
    purification based on self-supervision. *International Conference on Learning
    Representations*, 2021.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2021) 常浩·石、切斯特·霍尔茨和盖尔·米什内。基于自监督的在线对抗净化。*国际学习表征会议*，2021年。
- en: Singh and Subramanyam (2024) Himanshu Singh and AV Subramanyam. Language guided
    adversarial purification. In *ICASSP 2024-2024 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP)*, pages 7685–7689\. IEEE, 2024.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh and Subramanyam (2024) 希曼修·辛格和AV·苏布拉马尼亚姆。语言引导的对抗性净化。发表于*ICASSP 2024-2024
    IEEE国际声学、语音和信号处理会议 (ICASSP)*，页码 7685–7689。IEEE，2024年。
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 conference on empirical methods in natural language processing*, pages 1631–1642,
    2013.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2013) 理查德·索彻、亚历克斯·佩雷尔金、简·吴、贾森·庄、克里斯托弗·D·曼宁、安德鲁·Y·吴和克里斯托弗·波茨。情感树库上的递归深度模型用于语义组合性。发表于*2013年自然语言处理经验方法会议论文集*，页码
    1631–1642，2013年。
- en: Srinivasan et al. (2021) Vignesh Srinivasan, Csaba Rohrer, Arturo Marban, Klaus-Robert
    Müller, Wojciech Samek, and Shinichi Nakajima. Robustifying models against adversarial
    attacks by langevin dynamics. *Neural Networks*, 137:1–17, 2021.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivasan et al. (2021) 维格内什·斯里尼瓦桑、查巴·罗赫、阿图罗·马尔班、克劳斯-罗伯特·穆勒、沃伊切赫·萨梅克和真一·中岛。通过朗文动力学增强模型对抗攻击的鲁棒性。*神经网络*，137：1–17，2021年。
- en: Sumers et al. (2023) Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and
    Thomas L Griffiths. Cognitive architectures for language agents. *arXiv preprint
    arXiv:2309.02427*, 2023.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumers et al. (2023) 西奥多·R·苏默斯、顺宇姚、卡尔蒂克·纳拉西曼和托马斯·L·格里菲斯。语言代理的认知架构。*arXiv 预印本
    arXiv:2309.02427*，2023年。
- en: Szegedy et al. (2014) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of
    neural networks. *International Conference on Learning Representations*, 2014.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2014) 克里斯蒂安·泽格迪、沃伊切赫·扎伦巴、伊利亚·苏茨克弗、琼·布鲁纳、杜米特鲁·厄尔汉、伊恩·古德费洛和罗布·弗格斯。神经网络的有趣特性。*国际学习表征会议*，2014年。
- en: Tack et al. (2022) Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju
    Hwang, and Jinwoo Shin. Consistency regularization for adversarial robustness.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 8414–8422,
    2022.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tack et al. (2022) 奇勋·塔克、思贤·俞、钟宪·郑、敏善·金、成珠·黄和振宇·申。对抗鲁棒性的连续性正则化。发表于*AAAI人工智能会议论文集*，页码
    8414–8422，2022年。
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large
    language models in medicine. *Nature medicine*, 29(8):1930–1940, 2023.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirunavukarasu et al. (2023) 阿伦·詹姆斯·蒂鲁纳夫卡拉苏、达伦·舒·郑婷、卡比兰·艾兰戈万、劳拉·古铁雷斯、婷芳·谭和丹尼尔·舒·魏婷。医学中的大型语言模型。*自然医学*，29(8)：1930–1940，2023年。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023a) 雨果·图夫龙、蒂博·拉夫里尔、戈蒂埃·伊扎卡德、克塞维尔·马尔蒂内、玛丽-安·拉肖、蒂莫忒·拉克鲁瓦、巴普蒂斯特·罗兹耶、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔等。Llama：开放和高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023a年。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) 雨果·图夫龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马黑里、雅丝敏·巴巴伊、尼古拉·巴什利科夫、苏米亚·巴特拉、普拉吉瓦尔·巴尔加瓦、舒尔提·博萨尔等。Llama
    2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b年。
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) 亚历克斯·王、阿曼普里特·辛格、朱利安·迈克尔、费利克斯·希尔、奥默·列维和塞缪尔·R·博曼。Glue：一个用于自然语言理解的多任务基准和分析平台。*arXiv
    预印本 arXiv:1804.07461*，2018年。
- en: 'Wang et al. (2021) Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng,
    Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task
    benchmark for robustness evaluation of language models. In *Thirty-fifth Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track (Round
    2)*, 2021.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2021）Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng
    Gao, Ahmed Hassan Awadallah 和 Bo Li. Adversarial glue: 语言模型鲁棒性评估的多任务基准。在 *第三十五届神经信息处理系统会议数据集和基准轨道（第
    2 轮）*，2021年。'
- en: 'Wang et al. (2023a) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong
    Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al.
    Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In
    *Thirty-seventh Conference on Neural Information Processing Systems Datasets and
    Benchmarks Track*, 2023a.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023a）Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,
    Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer 等人. Decodingtrust:
    GPT 模型可信度的全面评估。在 *第三十七届神经信息处理系统会议数据集和基准轨道*，2023a年。'
- en: 'Wang et al. (2024) Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong
    Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al.
    Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2024）Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,
    Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer 等人. Decodingtrust:
    GPT 模型可信度的全面评估。*神经信息处理系统进展*，第 36 卷，2024年。'
- en: 'Wang et al. (2023b) Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo
    Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s
    game of thoughts: Battle against deception through recursive contemplation. *arXiv
    preprint arXiv:2310.01320*, 2023b.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b）Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen
    Yang, Andrew Zhao, Chaofei Wang, Shiji Song 和 Gao Huang. Avalon 的思维游戏：通过递归思考对抗欺骗。*arXiv
    预印本 arXiv:2310.01320*，2023b年。
- en: 'Wang et al. (2023c) Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan
    Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. Aligning large language
    models with human: A survey. *arXiv preprint arXiv:2307.12966*, 2023c.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023c）Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng,
    Wenyong Huang, Lifeng Shang, Xin Jiang 和 Qun Liu. 将大型语言模型与人类对齐：一项调查。*arXiv 预印本
    arXiv:2307.12966*，2023c年。
- en: Wang et al. (2017) Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective
    matching for natural language sentences. *arXiv preprint arXiv:1702.03814*, 2017.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2017）Zhiguo Wang, Wael Hamza 和 Radu Florian. 自然语言句子的双向多视角匹配。*arXiv 预印本
    arXiv:1702.03814*，2017年。
- en: Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel R Bowman. The
    multi-genre nli corpus. 2018.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams 等人（2018）Adina Williams, Nikita Nangia 和 Samuel R Bowman. 多类型 nli 语料库。2018年。
- en: 'Xiang et al. (2024) Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran, and Bo Li. Badchain: Backdoor chain-of-thought prompting for
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2024.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiang 等人（2024）Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian,
    Radha Poovendran 和 Bo Li. Badchain: 大型语言模型的后门链式思维提示。在 *第十二届国际学习表征会议*，2024年。'
- en: 'Xu et al. (2024) Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng
    Zhang, and Mohan Kankanhalli. An llm can fool itself: A prompt-based adversarial
    attack. In *The Twelfth International Conference on Learning Representations*,
    2024.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2024）Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang
    和 Mohan Kankanhalli. 大型语言模型可以欺骗自己：一种基于提示的对抗攻击。在 *第十二届国际学习表征会议*，2024年。
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action. *arXiv preprint arXiv:2303.11381*,
    2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人（2023）Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 和 Lijuan Wang. Mm-react: 促使 ChatGPT
    进行多模态推理和行动。*arXiv 预印本 arXiv:2303.11381*，2023年。'
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in
    language models. In *The Eleventh International Conference on Learning Representations*,
    2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人（2023）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    R Narasimhan 和 Yuan Cao. React: 在语言模型中协同推理与行动。在 *第十一届国际学习表征会议*，2023年。'
- en: 'Yao et al. (2024a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36, 2024a.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人（2024a）Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao 和 Karthik Narasimhan。思想树：利用大型语言模型进行深思熟虑的问题解决。*神经信息处理系统进展*，36，2024a。
- en: 'Yao et al. (2024b) Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun,
    and Yue Zhang. A survey on large language model (llm) security and privacy: The
    good, the bad, and the ugly. *High-Confidence Computing*, page 100211, 2024b.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人（2024b）Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun 和 Yue
    Zhang。大型语言模型（llm）安全与隐私调查：好、坏与丑。*高可信计算*，第 100211 页，2024b。
- en: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al.
    Promptbench: Towards evaluating the robustness of large language models on adversarial
    prompts. *arXiv preprint arXiv:2306.04528*, 2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023）Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong
    Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang 等人。Promptbench：评估大型语言模型对对抗性提示的鲁棒性。*arXiv
    预印本 arXiv:2306.04528*，2023。
- en: Appendix A GLUE Dataset
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A GLUE 数据集
- en: 'SST-2. The Stanford Sentiment Treebank (SST-2) (Socher et al., [2013](#bib.bib47))
    is a single-sentence classification task, includes sentences from movie reviews
    and their sentiment annotations by humans. This task involves determining the
    sentiment of a given sentence, categorized into two types: positive and negative.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: SST-2。斯坦福情感树库（SST-2）（Socher 等人，[2013](#bib.bib47)）是一个单句分类任务，包括来自电影评论的句子及其由人工标注的情感。该任务涉及确定给定句子的情感，分为两种类型：积极和消极。
- en: RTE. The Recognizing Textual Entailment (RTE) (Dagan et al., [2005](#bib.bib10);
    [Bar-Haim et al.,](#bib.bib2) ; Giampiccolo et al., [2007](#bib.bib15); Bos and
    Markert, [2005](#bib.bib7); Bentivogli et al., [2009](#bib.bib4)) is a binary
    classification task, where the goal is to determine whether a given sentence entails
    another sentence.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: RTE。识别文本蕴含（RTE）（Dagan 等人，[2005](#bib.bib10)；[Bar-Haim 等人](#bib.bib2)；Giampiccolo
    等人，[2007](#bib.bib15)；Bos 和 Markert，[2005](#bib.bib7)；Bentivogli 等人，[2009](#bib.bib4)）是一个二分类任务，目标是确定一个给定的句子是否蕴含另一个句子。
- en: QQP. The Quora Question Pairs (QQP) (Wang et al., [2017](#bib.bib61)) is a collection
    of question pairs from the community question-answering website Quora. The task
    is to determine whether a pair of questions are semantically equivalent.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: QQP。Quora 问题对（QQP）（Wang 等人，[2017](#bib.bib61)）是一个来自社区问答网站 Quora 的问题对集合。任务是确定一对问题是否语义等价。
- en: QNLI. The Question-answering NLI (QNLI) (Rajpurkar et al., [2016](#bib.bib42))
    is a natural language inference task, converted from another dataset, The Stanford
    Question Answering Dataset. The task is to determine whether a question and a
    sentence are entailed or not.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: QNLI。问答自然语言推断（QNLI）（Rajpurkar 等人，[2016](#bib.bib42)）是一个自然语言推断任务，转换自另一个数据集——斯坦福问答数据集。任务是确定一个问题和一个句子是否有蕴含关系。
- en: 'MNLI. The Multi-Genre Natural Language Inference (MNLI) Corpus (Williams et al.,
    [2018](#bib.bib62)) is a collection of sentence pairs with textual entailment
    annotations. Given a premise sentence and a hypothesis sentence, the task is to
    predict whether the premise entails the hypothesis, contradicts the hypothesis,
    or neither. Because MNLI comprises texts from many different domains and styles,
    it is divided into two versions: MNLI-m, where training and test datasets share
    the same sources, and MNLI-mm, where they differ.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: MNLI。多体裁自然语言推断（MNLI）语料库（Williams 等人，[2018](#bib.bib62)）是一组带有文本蕴含标注的句子对。给定一个前提句子和一个假设句子，任务是预测前提是否蕴含假设，是否与假设矛盾，或两者都不是。由于
    MNLI 包含来自许多不同领域和风格的文本，因此分为两个版本：MNLI-m，其中训练集和测试集共享相同的来源，以及 MNLI-mm，其中它们不同。
- en: 'Table 9: The task descriptions of the GLUE dataset.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：GLUE 数据集的任务描述。
- en: '| Datasets | [Task Description] |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | [任务描述] |'
- en: '| --- | --- |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SST-2 | Analyze the tone of this statement and respond with either ‘positive’
    or ‘negative’. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 分析该陈述的语气，并回应‘positive’或‘negative’。 |'
- en: '| RTE | Are the following two sentences entailment or not_entailment? Answer
    me with ‘entailment’ or ‘not_entailment’, just one word. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| RTE | 以下两个句子是否为蕴含关系？用‘entailment’或‘not_entailment’回答，仅一个词。 |'
- en: '| QQP | Are the following two questions equivalent or not? Answer me with ‘equivalent’
    or ‘not_equivalent’. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| QQP | 以下两个问题是否等价？用‘equivalent’或‘not_equivalent’回答。 |'
- en: '| QNLI | Given the question and context provided, determine if the answer can
    be inferred by choosing ‘entailment’ or ‘not_entailment’. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| QNLI | 给定问题和上下文，确定是否可以通过选择‘蕴涵’或‘非蕴涵’来推断答案。 |'
- en: '| MNLI-mm | Does the relationship between the given sentences represent entailment,
    neutral, or contradiction? Respond with ‘entailment’, ‘neutral’, or ‘contradiction’.
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| MNLI-mm | 给定的句子之间的关系是否表示蕴涵、中立还是矛盾？回复‘蕴涵’，‘中立’或‘矛盾’。 |'
- en: '| MNLI-m | Does the relationship between the given sentences represent entailment,
    neutral, or contradiction? Respond with ‘entailment’, ‘neutral’, or ‘contradiction’.
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| MNLI-m | 给定的句子之间的关系是否表示蕴涵、中立还是矛盾？回复‘蕴涵’，‘中立’或‘矛盾’。 |'
- en: 'Table 10: The label list and input description of the GLUE dataset.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: GLUE 数据集的标签列表和输入描述。'
- en: '| Datasets | [Label List] | [Input Description] |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | [标签列表] | [输入描述] |'
- en: '| --- | --- | --- |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SST-2 | [‘positive’, ‘negative’] | Each example contains one ‘sentence’.
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | [‘积极’，‘消极’] | 每个示例包含一个‘句子’。 |'
- en: '| RTE | [‘entailment’, ‘not_entailment’] | Each example contains ‘sentence1’
    and ‘sentence2’. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| RTE | [‘蕴涵’，‘非蕴涵’] | 每个示例包含‘句子1’和‘句子2’。 |'
- en: '| QQP | [‘equivalent’, ‘not_equivalent’] | Each example contains ‘question1’
    and ‘question2’. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| QQP | [‘等价’，‘不等价’] | 每个示例包含‘问题1’和‘问题2’。 |'
- en: '| QNLI | [‘entailment’, ‘not_entailment’] | Each example contains ‘question’
    and ‘sentence’. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| QNLI | [‘蕴涵’，‘非蕴涵’] | 每个示例包含‘问题’和‘句子’。 |'
- en: '| MNLI-mm | [‘entailment’, ‘neutral’, ‘contradiction’] | Each example contains
    ‘premise’ and ‘hypothesis’. |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| MNLI-mm | [‘蕴涵’，‘中立’，‘矛盾’] | 每个示例包含‘前提’和‘假设’。 |'
- en: '| MNLI-m | [‘entailment’, ‘neutral’, ‘contradiction’] | Each example contains
    ‘premise’ and ‘hypothesis’. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| MNLI-m | [‘蕴涵’，‘中立’，‘矛盾’] | 每个示例包含‘前提’和‘假设’。 |'
- en: Appendix B PromptAttack
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B PromptAttack
- en: 'PromptAttack (Xu et al., [2024](#bib.bib64)) modifies the clean examples at
    the character level, word level or sentence level. The specific guidance as shown
    in [Table 11](#A2.T11 "In Appendix B PromptAttack ‣ Large Language Model Sentinel:
    Advancing Adversarial Robustness by LLM Agent").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: PromptAttack (Xu 等人，[2024](#bib.bib64)) 在字符级、单词级或句子级别修改干净的示例。具体指导如 [表 11](#A2.T11
    "在附录 B PromptAttack ‣ 大型语言模型哨兵：通过 LLM 代理推进对抗鲁棒性") 所示。
- en: 'Table 11: Perturbation instructions at the character, word, and sentence levels,
    respectively. This table is referenced from Xu et al. ([2024](#bib.bib64)).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 字符、单词和句子级别的扰动指令。这张表引用自 Xu 等人 ([2024](#bib.bib64))。'
- en: '| Level | Abbre. | #perturbation_instruction |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Level | 缩写 | #扰动指令 |'
- en: '| Character | C1 | Choose at most two words in the sentence, and change them
    so that they have typos. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Character | C1 | 从句子中选择最多两个单词，将其更改为有拼写错误的形式。 |'
- en: '| C2 | Change at most two letters in the sentence. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| C2 | 最多更改句子中的两个字母。 |'
- en: '| C3 | Add at most two extraneous characters to the end of the sentence. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 在句子末尾最多添加两个多余的字符。 |'
- en: '| Word | W1 | Replace at most two words in the sentence with synonyms. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Word | W1 | 用同义词替换句子中的最多两个单词。 |'
- en: '| W2 | Choose at most two words in the sentence that do not contribute to the
    meaning of the sentence and delete them. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| W2 | 从句子中选择最多两个不影响句意的单词并删除它们。 |'
- en: '| W3 | Add at most two semantically neutral words to the sentence. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| W3 | 向句子中添加最多两个语义中立的单词。 |'
- en: '| Sentence | S1 | Add a randomly generated short meaningless handle after the
    sentence, such as @fasuv3. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Sentence | S1 | 在句子后添加一个随机生成的短无意义的标签，如 @fasuv3。 |'
- en: '| S2 | Paraphrase the sentence. |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 对句子进行释义。 |'
- en: '| S3 | Change the syntactic structure of the sentence. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| S3 | 更改句子的句法结构。 |'
- en: Few-shot (FS) strategy. Provide examples that match the task description of
    adversarial attacks to large language models (LLMs) to help the LLMs understand
    the task. By learning these examples, LLMs can generate adversarial examples with
    higher quality and greater attack strength.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 少量样本 (FS) 策略。提供符合对抗攻击任务描述的示例，以帮助 LLM 理解任务。通过学习这些示例，LLM 可以生成质量更高、攻击强度更大的对抗示例。
- en: Ensemble (EN) strategy. Utilize a collection of adversarial examples with various
    levels of perturbation, select the examples that are most likely to deceive the
    LLM as output, in order to increase the attack success rate.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 集成 (EN) 策略。利用具有不同扰动级别的对抗样本集合，选择最可能欺骗大型语言模型（LLM）的示例作为输出，以提高攻击成功率。
- en: Appendix C More Comparasion Results of Input Examples
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 输入示例的更多比较结果
- en: 'Table 12: Comparison between clean examples, adversarial examples and purified
    examples. When incorrectly classified, the modified area is highlighted with a
    yellow background'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：清晰示例、对抗性示例和纯化示例的比较。当分类错误时，修改的区域用黄色背景突出显示。
- en: '| [Input Examples] | Prediction |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| [输入示例] | 预测 |'
- en: '| --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|       1\. Clean Example: it ’s a charming and often affecting journey . |
    Positive |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|       1\. 清晰示例：这是一次迷人且常常令人感动的旅程。 | 积极 |'
- en: '|       Adversarial Example: it ’s a charming and oftenaffecctingjourney .
    | Negative |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|       对抗性示例：这是一次迷人且常常令人感动的旅程。 | 消极 |'
- en: '|       Purified Example: it’s a charming and often affecting journey. | Positive
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|       纯化示例：这是一次迷人且常常令人感动的旅程。 | 积极 |'
- en: '|       2\. Clean Example: … the movie is just a plain old monster | Negative
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|       2\. 清晰示例：… 这部电影就是一部普通的老怪兽。 | 消极 |'
- en: '|       Adversarial Example: … the movie is just a plain old monster:) | Positive
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|       对抗性示例：… 这部电影就是一部普通的老怪兽:) | 积极 |'
- en: '|       Purified Example: … the movie is just a plain old monster . | Negative
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|       纯化示例：… 这部电影就是一部普通的老怪兽。 | 消极 |'
- en: '|       3\. Clean Example: the year’s most intriguing explorations of alientation
    . | Positive |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|       3\. 清晰示例：今年最引人入胜的异化探索。 | 积极 |'
- en: '|       Adversarial Example: the year’s most intriguing explorations of alientation
    .:( | Negative |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|       对抗性示例：今年最引人入胜的异化探索。:( | 消极 |'
- en: '|       Purified Example: the year’s most intriguing explorations of alientation..
    | Positive |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|       纯化示例：今年最引人入胜的异化探索。 | 积极 |'
- en: '|       4\. Clean Example: it ’s a charming and often affecting journey . |
    Positive |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|       4\. 清晰示例：这是一次迷人且常常令人感动的旅程。 | 积极 |'
- en: '|       Adversarial Example: it ’snota charming and often affecting journey
    .@fasuv3 | Negative |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '|       对抗性示例：这不是一次迷人且常常令人感动的旅程。@fasuv3 | 消极 |'
- en: '|       Purified Example: it ’snota charming and often affecting journey .
    | Negative |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|       纯化示例：这不是一次迷人且常常令人感动的旅程。 | 消极 |'
- en: '|       5\. Clean Example: corny, schmaltzy and predictable, but still manages
    to be kind of heartwarming, nonetheless. | Positive |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|       5\. 清晰示例：陈腐、过于煽情且可预测，但依然有点让人感到温暖。 | 积极 |'
- en: '|       Adversarial Example: corny, schmaltzy and predictable, but still manages
    to be kind of heartwarming, nonetheless.@kjdjq2. | Negative |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '|       对抗性示例：陈腐、过于煽情且可预测，但依然有点让人感到温暖。@kjdjq2. | 消极 |'
- en: '|       Purified Example: corny, schmaltzy and predictable, but still manages
    to be kind of heartwarming, nonetheless.. | Positive |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|       纯化示例：陈腐、过于煽情且可预测，但依然有点让人感到温暖。 | 积极 |'
