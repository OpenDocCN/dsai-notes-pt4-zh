<!--yml
category: 未分类
date: 2025-01-11 12:18:07
-->

# Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions

> 来源：[https://arxiv.org/html/2408.15787/](https://arxiv.org/html/2408.15787/)

Huachuan Qiu [1234-5678-9012](https://orcid.org/1234-5678-9012 "ORCID identifier") ¹ Zhejiang University² School of Engineering, Westlake UniversityHangzhouZhejiangChina [qiuhuachuan@westlake.edu.cn](mailto:qiuhuachuan@westlake.edu.cn)  and  Zhenzhong Lan [1234-5678-9012](https://orcid.org/1234-5678-9012 "ORCID identifier") School of Engineering, Westlake UniversityHangzhouZhejiangChina [lanzhenzhong@westlake.edu.cn](mailto:lanzhenzhong@westlake.edu.cn)(2024)

###### Abstract.

Virtual counselors powered by large language models (LLMs) aim to create interactive support systems that effectively assist clients struggling with mental health challenges. To replicate counselor-client conversations, researchers have built an online mental health platform that allows professional counselors to provide clients with text-based counseling services for about an hour per session. Notwithstanding its effectiveness, challenges exist as human annotation is time-consuming, cost-intensive, privacy-protected, and not scalable. To address this issue and investigate the applicability of LLMs in psychological counseling conversation simulation, we propose a framework that employs two LLMs via role-playing for simulating counselor-client interactions. Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. We implement both the counselor and the client by zero-shot prompting the GPT-4 model. In order to assess the effectiveness of LLMs in simulating counselor-client interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the synthetic data from various perspectives. We begin by assessing the client’s performance through automatic evaluations. Next, we analyze and compare the disparities between dialogues generated by the LLM and those generated by professional counselors. Furthermore, we conduct extensive experiments to thoroughly examine the performance of our LLM-based counselor trained with synthetic interactive dialogues by benchmarking against state-of-the-art models for mental health.

Interactive Agents, LLM Simulation, Dialogue System, Role-playing, Synthetic Data, Dialogue Evaluation, Psychological Counseling^†^†copyright: rightsretained^†^†journalyear: 2024^†^†doi: XXXXXXX.XXXXXXX^†^†conference: WSDM; June 03–05, 2024; Woodstock, NY^†^†isbn: 978-1-4503-XXXX-X/24/06^†^†ccs: Computing methodologies Discourse, dialogue and pragmatics

## 1\. Introduction

Since the birth of ELIZA (Weizenbaum, [1966](https://arxiv.org/html/2408.15787v1#bib.bib47)), the community of conversational agents (CAs) (Li et al., [2023b](https://arxiv.org/html/2408.15787v1#bib.bib25)) has strived to create interactive and supportive AI that effectively assists clients struggling with mental health challenges. Recent advancements (Liu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib27); Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35), [b](https://arxiv.org/html/2408.15787v1#bib.bib37)) in dialogue systems for psychological counseling have been successful to some extent in achieving this goal by generating helpful and safe responses and engaging in back-and-forth interactions with clients to facilitate exploration, gain insight, take action, and ultimately heal themselves. Existing work has built an online mental health platform that allows professional counselors to provide clients with text-based counseling services (Li et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib23)). Furthermore, research efforts attempted to explore transforming long-text single-turn counseling dialogues or anonymized psychological counseling reports into multi-turn ones (Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35); Zhang et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib52); Chen et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib10)).

Challenge. Despite the effectiveness of previous efforts in this domain, several drawbacks exist. One major challenge (Liu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib27); Li et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib23)) in real-life counselor-client interactions is maintaining a large team of counselors and clients to generate a substantial number of conversations. Such a process can be time-consuming, cost-intensive, privacy-sensitive, and not scalable. Additionally, in many cases, the counselor cannot effectively help clients tackle their mental health issues that are outside of their background knowledge (Hill, [2020](https://arxiv.org/html/2408.15787v1#bib.bib17)). For example, a counselor who has never experienced a love affair before cannot explore love affair issues as effectively as a counselor who has. Another challenge (Zhang et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib52); Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35); Chen et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib10)) is that even though many research efforts utilize LLMs to transform long-text single-turn counseling dialogues or anonymized psychological counseling reports into multi-turn ones, this exciting approach ignores real-life interactions between the counselor and the client. Furthermore, the LLM prematurely knows all the client’s mental health problems from a god’s-eye view rather than exploring and understanding the client’s issues gradually, as in an actual counseling scenario.

Motivation. Interactive simulacra (Park et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib32); Dai et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib11); Grossmann et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib16); Abbasiantaeb et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib2)) is an important emerging research frontier for back-and-forth interaction development and evaluation. The motivation for developing a framework utilizing LLMs to simulate counselor-client interactions stems from the need to address significant challenges faced by existing research efforts. By leveraging LLMs, the proposed framework aims to create a scalable, efficient, and privacy-conscious solution that can simulate professional counseling sessions. The innovative approach of using two LLMs in a role-playing scenario—one as the client with a specific and real-life user profile and the other as an experienced counselor employing integrative therapy techniques—presents a promising method to generate synthetic data. This data can be used to benchmark and enhance the performance of AI-driven mental health support systems. The ultimate goal is to offer a viable alternative to traditional methods, ensuring broader access to practical mental health support while maintaining the quality and depth of human-generated interactions. Therefore, it is crucial to explore automatic approaches that can generate simulated dialogues, reducing the dependency on human participants and making the process more efficient and scalable.

Our Approach. In this work, we aim to explore LLMs’ effectiveness in simulating a psychological counseling session between an experienced counselor and a client seeking help for a mental disorder, problem, or chief complaint, wherein the client is predefined with a user profile in a conversational setting. To this aim, we replicate the counselor-client conversational simulation by replacing both human participants with interactive agents, enabling us to effectively evaluate and compare the performance of LLMs with human participants. This work leads us to our first research question, RQ1: How can we employ LLMs to generate such simulated conversations effectively and automatically? We answer this question by proposing a role-playing LLM-to-LLM interaction framework where an LLM-based client aims to seek help, and the LLM-based counselor helps the client explore their own values and beliefs, gain insight, and make positive changes in their lives. We implement both counselor and client by zero-shot prompting the GPT-4 model.

Using interactive agents in this setting leads us to the following two questions: RQ2: How can we evaluate the role of LLMs in a counselor-client simulation? Moreover, RQ3: How do LLM- and human-generated dialogues compare? To address these questions:

(1) We first conduct an extensive independent evaluation of the client, measuring its fidelity of role-playing and group diversity. To this aim, we conduct a comparison analysis of role-following, discovering that the user profile significantly influences the client’s generated utterances. Further, we find that simulated clients’ diversity is comparable to actual clients.

(2) We then evaluate the performance of the counselor. To this aim, we adopt the widely used Observer-rated Short version of the Working Alliance Inventory (WAI-O-S) (Form et al., [2000](https://arxiv.org/html/2408.15787v1#bib.bib14); Bayerl et al., [2022](https://arxiv.org/html/2408.15787v1#bib.bib6)) to assess the dialogue quality generated by counselor guidance.

(3) Finally, we conduct extensive experiments to thoroughly examine the performance of the dialogue system fine-tuned with our synthetic data by benchmarking against state-of-the-art models for mental health. We find that our dialogue system significantly outperforms existing state-of-the-art models, even including the model trained with real-life counseling dialogues.

Our Contributions. Importantly, LLM-generated dialogues are of high quality. In sum, our contributions can be presented as follows:

*   •

    We prompt LLMs to mimic counselor-client interaction in psychological counseling in a zero-shot prompting paradigm and propose an LLM-generated dataset called SimPsyDial.¹¹1Code, data, and models are available at [https://github.com/qiuhuachuan/interactive-agents](https://github.com/qiuhuachuan/interactive-agents)

*   •

    We propose and perform a comprehensive automatic evaluation framework for evaluating the effectiveness of LLM-based counselor-client simulation.

*   •

    We fine-tune two popular open-source large language models with 7B parameters. We compare our dialogue systems with existing state-of-the-art models for mental health and find out that our dialogue model achieves the best performance among them. Meanwhile, we open-source our dialogue models to enhance the community development in mental health.

## 2\. Method

### 2.1\. Problem Definition

Our experimental setup primarily focuses on simulating a psychological counseling dialogue, where an LLM-based counselor interacts with an LLM-based client seeking help to overcome mental health issues. First, we define the LLM-based client as $\Omega$ and the counselor as $\Psi$. Let $C$ represent an ongoing dialogue between the counselor simulator and the client simulator, with utterances $\{u_{1}^{\Omega},u_{1}^{\Psi},u_{2}^{\Omega},u_{2}^{\Psi},\ldots,u_{i}^{\Omega% },u_{i}^{\Psi}\}$, ending with an LLM-based counselor’s utterance $u_{i}^{\Psi}$ and initiated by the LLM-based client.

### 2.2\. Task Formulation

To evaluate the planning capability and counseling depth of LLM-based counselors, we propose to simulate the entire conversation between an LLM-based counselor and an LLM-based client. Here, we leverage long descriptions of client mental health issues collected from a publicly accessible online platform to simulate LLM-based clients using large language models. For each client’s description of a mental health issue, we provide it as a user profile to a large language model and ask it to simulate a client with a provided mental health issue talking to an LLM-based counselor in a simulator environment. The LLM-based client mimics an actual client who maintains the same conversational style, stating specific topics and concerns and discussing life events and emotions. Then, we generate a conversation between this LLM-based client and the LLM-based counselor. For convenience, we start the generation by picking the client to go first with the utterance of ”Hello.” We let them talk for up to 50 turns, which is larger than the average conversation turns during a formal counseling session, or until the LLM-based counselor outputs an end token that we predefined.

![Refer to caption](img/02573c74e9eb551cb4eb53ff0564a1d1.png)

Figure 1\. The overall architecture of our simulation framework. Left panel: construction of client pool. Middle panel: data collection with interactive simulation. Right panel: model training.

\Description

Overall architecture of our simulation framework. Left panel: construction of client pool. Middle panel: data collection with interactive simulation. Right panel: model training.

### 2.3\. Simulation Framework Overview

In order to have a better understanding of RQ1, we propose an LLM-based simulation framework. Figure [1](https://arxiv.org/html/2408.15787v1#S2.F1 "Figure 1 ‣ 2.2\. Task Formulation ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions") illustrates the overall architecture of our simulation method, showcasing the interactions between the two LLMs. Once we finish data collection, we can verify RQ2\. After that, we can train our dialogue systems, as shown in the right panel in Figure [1](https://arxiv.org/html/2408.15787v1#S2.F1 "Figure 1 ‣ 2.2\. Task Formulation ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"), to further analyze RQ3.

![Refer to caption](img/7af841739e1134bb2cf2412d68b996f7.png)

Figure 2\. The prompt for client simulation. For the Chinese version, please refer to our GitHub repository.

\Description

Prompt for client simulation.

![Refer to caption](img/f4c23523a54c0676b4efbaddf1b98c35.png)

Figure 3\. The prompt for counselor simulation. For the Chinese version, please refer to our GitHub repository.

\Description

Prompt for counselor simulation.

### 2.4\. Client Simulation

Generally, different clients often have different user profiles, which mainly reflect on their mental health issues. Therefore, the first step we need to do is to construct a pool of clients with different mental health issues. Here, the user profile $P^{\Omega}$ denotes the description of mental health issues, a detailed statement describing the client’s disorder, symptom, problem, and chief complaint. To this aim, we propose to extract the user’s long post from an open-sourced single-turn dialogue dataset, PsyQA (Sun et al., [2021](https://arxiv.org/html/2408.15787v1#bib.bib41)), which is collected from an online professional psychological platform²²2https://www.xinli001.com/qa, and only retain the user post exceeding 300 Chinese characters. The left panel in Figure [1](https://arxiv.org/html/2408.15787v1#S2.F1 "Figure 1 ‣ 2.2\. Task Formulation ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions") shows the construction of the client pool. This article presents the client simulation prompt in Figure [2](https://arxiv.org/html/2408.15787v1#S2.F2 "Figure 2 ‣ 2.3\. Simulation Framework Overview ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). To validate the research questions proposed in our paper, we set 1000 different user posts as the client pool. In addition, we establish another 100 different user posts as the help-out test set for assessing dialogue systems.

### 2.5\. Counselor Simulation

Research has shown that all mainstream types of psychotherapy are equally effective (Wampold, [2013](https://arxiv.org/html/2408.15787v1#bib.bib43)), and no differences have been found between individual and group treatments (Piper, [2008](https://arxiv.org/html/2408.15787v1#bib.bib34); Hill, [2020](https://arxiv.org/html/2408.15787v1#bib.bib17)). This conclusion is humorously called the ”dodo bird verdict,” meaning all therapies are winners. Therefore, we propose to build an experienced counselor based on the three-stage model with integrative therapy, which facilitates exploration, insight, and action. The theoretical foundation of counselor simulation in this paper is heavily influenced by integrating diverse therapeutic principles as conceptualized by Hill (Hill, [2020](https://arxiv.org/html/2408.15787v1#bib.bib17)). The theory of the three-stage model, including exploration, insight, and action, corresponds to client-centered therapy (Rogers, [1946](https://arxiv.org/html/2408.15787v1#bib.bib38)), psychodynamic therapy (Warren, [1998](https://arxiv.org/html/2408.15787v1#bib.bib46)), and cognitive behavioral therapy (Hofmann et al., [2012](https://arxiv.org/html/2408.15787v1#bib.bib18)), respectively. This integrative therapy framework, grounded in philosophical consistency, ensures that our simulations are robust, multifaceted, and responsive to the varied needs of clients. Hence, this three-stage model is the cornerstone of our counselor simulation framework. Each stage is aligned with a different therapeutic approach, providing a structured yet adaptable pathway for client engagement and progress. The present paper presents the prompt for counselor simulation in Figure [3](https://arxiv.org/html/2408.15787v1#S2.F3 "Figure 3 ‣ 2.3\. Simulation Framework Overview ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions").

Ending Interaction. To avoid infinite interactions between the LLM-based client and counselor and ensure the quality of the simulated dialogues, we propose a set of criteria for ending interaction, which is presented in the dashed box in Figure [1](https://arxiv.org/html/2408.15787v1#S2.F1 "Figure 1 ‣ 2.2\. Task Formulation ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). During each interaction, we check whether the LLM-based counselor’s response meets the ending interaction criteria in each turn.

Response Refinement. To ensure that a response generated by the LLM-based counselor is naturally and structurally sound, we employ a validation step called $\sigma^{\Psi}$. This component serves the purpose of verifying and validating the logic and naturalness of the generated response. We observe that while $u^{\Psi}_{i}$ is supposed to be concise and quickly understood in our setting, sometimes the LLM-based counselor generates a lengthy response in one go, unlike in a real-life setting. To address this issue, we consider a response if it adheres to the following criteria: (i) it should not exceed 200 Chinese characters in length, and (ii) it should not contain a newline character or enumerated items (e.g., 1, 2, 3). This simple yet effective validation helps to filter lengthy and superabundant responses.

### 2.6\. Experimental Setup

In our experiments, we adopt GPT-4³³3The model we use is gpt-4-1106-preview, with training data up to Apr 2023. as our base LLM for simulating client and counselor. In our preliminary experiments, we explore using other LLMs, such as GPT-3.5 (Brown et al., [2020](https://arxiv.org/html/2408.15787v1#bib.bib9)), GLM-4 (GLM et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib15)), DeepSeek-V2-Chat (DeepSeek-AI, [2024b](https://arxiv.org/html/2408.15787v1#bib.bib13)), and Qwen1.5-110B-Chat (Bai et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib4)), as client and counselor. However, we find that GPT-4 (OpenAI, [2024](https://arxiv.org/html/2408.15787v1#bib.bib30)) is the only LLM that can better mimic the client and counselor in a human-like way. Other models fail in this task by generating either lengthy utterances or short interaction turns, which fall far short of real-world settings. In our simulation framework, we set the patience parameter, $\sigma_{T}$, to a fixed value of 50, which means the interaction breaks after a maximum of 50 turns.

## 3\. Simulation Evaluation

SimPsyDial Dataset. We first introduce our dataset, SimPsyDial, for simulation evaluation using the simulation framework described in $\S$[2.3](https://arxiv.org/html/2408.15787v1#S2.SS3 "2.3\. Simulation Framework Overview ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). To collect SimPsyDial, we use GPT-4 to implement LLM-based counselor and client. SimPsyDial consists of 1000 dialogues with an average of 13 turns per conversation. This paper presents statistics of SimPsyDial in Table [1](https://arxiv.org/html/2408.15787v1#S3.T1 "Table 1 ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"), alongside those of the real counselor-client conversations, RealPsyDial. Next, we will answer RQ2 from the perspective of the client and counselor point by point.

Table 1\. Statistics of the collected dialogues by simulating counselor-client psychological counseling with the LLM-based counselor and client.

 |  | RealPsyDial | SimPsyDial |
| # Conversations | 550 | 1000 |
| Avg. Turns per Conversation | 40 | 13 |
| # Client Utterances | 22253 | 12948 |
| # Counselor Utterances | 22418 | 12948 |
| Avg. Len. of Client Utterances | 34.5 | 54.1 |
| Avg. Len. of Counselor Utterances | 26.1 | 70.8 | 

### 3.1\. Client Evaluation

Generally, any clients who seek help to overcome mental health issues are good clients, even if they do not show any resistance in the counseling session. The simulated clients are expected to behave consistently with their user profiles and emulate real clients in counseling sessions. We analyze the simulated clients’ behaviors from two aspects: vocabulary overlap rate and semantic consistency. Furthermore, we present the diversity between simulated clients and real clients. Next, we will elaborate on these aspects one by one.

Vocabulary Overlap Rate. Given a user profile $P^{\Omega}$, the LLM-based client will interact with the LLM-based counselor and thus produce a dialogue session. Therefore, considering such a generated counseling session, the following equation computes the vocabulary overlap rate between the client’s utterance and the corresponding user profile.

| (1) |  | $\frac{\left&#124;\mathrm{Set}(V(S^{\Omega}))\cap\mathrm{Set}(V(P^{\Omega}))\right&#124;}% {\left&#124;\mathrm{Set}(V(P^{\Omega}))\right&#124;}$ |  |

where $V(S^{\Omega})$ and $V(P^{\Omega})$ denote the vocabulary used by the client in the counseling session and the user profile, respectively. $S^{\Omega}=\{u^{\Omega}_{1},u^{\Omega}_{2},...,u^{\Omega}_{t}\}$ is the concatenation of the client’s utterances. $\mathrm{Set(\cdot)}$ is an operation that removes duplicate elements.

Results. The distribution of vocabulary overlap rates is presented in Figure [4](https://arxiv.org/html/2408.15787v1#S3.F4 "Figure 4 ‣ 3.1\. Client Evaluation ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions")a. We observe that the mapping group (mean = 0.406; std = 0.083) has a significantly larger vocabulary overlap rate (two-tailed t-test; $p$-value ¡ 0.001) than the random group (mean = 0.284; std = 0.060). These results suggest that the LLM-based client can better follow its user profile when conversing with the LLM-based counselor.

Semantic Consistency. To further evaluate the degree of client simulation, we propose to use semantic consistency. We utilize text embeddings for quantitative analysis. To obtain the text embedding of a given string, we use the BAAI/bge-m3 model⁴⁴4https://huggingface.co/BAAI/bge-m3, which accepts a maximum of 8192 tokens. Each string is encoded into a 1024-dimensional vector. For example, to compute the cosine similarity between two different strings, we can obtain

| (2) |  | $\mathrm{cos}(P^{\Omega},S^{\Omega})=\frac{e_{p}\cdot e_{s}}{\left\&#124;e_{p}\right% \&#124;\left\&#124;e_{s}\right\&#124;}$ |  |

where $e_{p}$ and $e_{s}$ denote the text embeddings of the user profile and the concatenation of the client’s utterances, respectively.

Results. The distribution of cosine similarity is presented in Figure [4](https://arxiv.org/html/2408.15787v1#S3.F4 "Figure 4 ‣ 3.1\. Client Evaluation ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions")b. We observe that the mapping group (mean = 0.791; std = 0.056) has a significantly larger semantic similarity (two-tailed t-test; $p$-value ¡ 0.001) compared to the random group (mean = 0.570; std = 0.059). These results further suggest that the LLM-based client significantly depends on its user profile when conversing with the LLM-based counselor.

![Refer to caption](img/a42df939bd094c828c8fb35c7100aa7c.png)

Figure 4\. Consistency of client simulation.

\Description

Consistency of client simulation.

![Refer to caption](img/89add377be1b45465503a457557b65d0.png)

Figure 5\. Topic distribution between RealPsyDial and SimPsyDial.

\Description

Topic distribution between RealPsyDial and SimPsyDial.

Diversity of Clients. The diversity of counseling sessions is often determined by the diversity of clients. Based on this, we adopt the setting established by the RealPsyDial dataset (Li et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib23)), which serves as a widely recognized dataset collected from real clients and professional counselors. We utilize the method proposed by Qiu et al. (Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35)) that prompts Qwen1.5-110B-Chat in a zero-shot prompting paradigm with a collection of 60 topics to generate topics related to clients’ chief complaints. To ensure the generation result is effective and consistent, we prompt Qwen1.5-110B-Chat to produce topics for each concatenation of the client’s utterances through three rounds and record the information entropy of topic distribution in each round.

Results. The topic distribution between RealPsyDial and SimPsyDial is presented in Figure [5](https://arxiv.org/html/2408.15787v1#S3.F5 "Figure 5 ‣ 3.1\. Client Evaluation ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). We observe that the information entropy of topics of clients’ chief complaints in SimPsyDial (mean = 4.526; std = 0.009) is slightly lower (two-tailed t-test; avg. $p$-value = 0.055 ¿ 0.05) than that in RealPsyDial (mean = 4.875; std = 0.020). Furthermore, we find that the topic distribution (topic and its corresponding frequency) between RealPsyDial and SimPsyDial is almost similar, demonstrating that our SimPsyDial is close to RealPsyDial with respective of the client side.

### 3.2\. Counselor Evaluation

Motivated by the prevalence and effectiveness of using an LLM as a judge and quality assessment of psychological counseling sessions with the Working Alliance Inventory (WAI), we propose to use LLMs as observers to evaluate the quality of counseling sessions. The prompt for WAI assessment is presented in Figure [6](https://arxiv.org/html/2408.15787v1#S3.F6 "Figure 6 ‣ 3.2\. Counselor Evaluation ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). For questionnaires and guidelines we use in our paper, please refer to previous research work (Form et al., [2000](https://arxiv.org/html/2408.15787v1#bib.bib14); Bayerl et al., [2022](https://arxiv.org/html/2408.15787v1#bib.bib6)). To ensure that the generation result is effective and consistent, we prompt Qwen1.5-110B-Chat to produce scores for each conversation through three rounds and calculate the average score of three rounds per questionnaire.

 <svg class="ltx_picture" height="218.72" id="S3.F6.1.1.1.1.1.1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,218.72) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="191.16" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">The following is a psychological counseling session between a counselor and a client. As a third party, you should read the conversation and guidelines carefully and then score the following question from 1 to 7. Start of the conversation
{conversation}
End of the conversation Questionnaire: {questionnaire}
Start of guidelines for the questionnaire
{guidelines}
End of guidelines for the questionnaire You should answer the questionnaire and provide a score that should be exactly a number from 1 to 7\. Your score is</foreignobject></g></g></svg> 

Figure 6\. Prompt for scoring the dialogue with the LLM as a judge.

\Description

Prompt for scoring the dialogue with the LLM as a judge.

![Refer to caption](img/9eb869a38ec31ba25930cf47f442452e.png)

Figure 7\. Comparisons of WAI-O-S scores between RealPsyDial and SimPsyDial.

\Description

Comparisons of WAI-O-S scores between RealPsyDial and SimPsyDial.

![Refer to caption](img/3144c77030ebb84642349be5b66516d1.png)

Figure 8\. Snippet examples of dialogue sessions between RealPsyDial and SimPsyDial.

\Description

Snippet examples of dialogue sessions between RealPsyDial and SimPsyDial.

Results. We present the comparisons of WAI-O-S scores between RealPsyDial and SimPsyDial, as shown in Figure [7](https://arxiv.org/html/2408.15787v1#S3.F7 "Figure 7 ‣ 3.2\. Counselor Evaluation ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). The goal score for SimPsyDial dataset (mean = 6.045; std = 0.265) is significantly higher ($p$-value ¡ 0.001) than that for RealPsyDial dataset (mean = 5.505; std = 0.744). The task score for SimPsyDial dataset (mean = 6.191; std = 0.417) is significantly higher ($p$-value ¡ 0.001) than that for RealPsyDial dataset (mean = 5.695; std = 0.690). The bond score for SimPsyDial dataset (mean = 5.953; std = 0.190) is higher ($p$-value ¡ 0.001) than that for RealPsyDial dataset (mean = 5.807; std = 0.507).

The simPsyDial dataset exhibits higher mean scores in goal, task, and bond categories than the RealPsyDial dataset, indicating superior and more consistent performance. The SimPsyDial dataset shows significantly lower standard deviations across all three categories, suggesting the data quality with reduced variability and more concentration. Furthermore, we present examples of dialogue session between RealPsyDial and SimPsyDial, as shown in Figure [8](https://arxiv.org/html/2408.15787v1#S3.F8 "Figure 8 ‣ 3.2\. Counselor Evaluation ‣ 3\. Simulation Evaluation ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions").

## 4\. Dialogue System

### 4.1\. Mathematical Formulation

To train a dialogue system for psychological counseling, we need to split each full dialogue $d\sim\mathcal{D}$ into several training sessions. Specifically, a sampled $t$-turn dialogue session can be represented as $d_{t}=\left\{u_{1}^{\Omega},u_{1}^{\Psi},u_{2}^{\Omega},u_{2}^{\Psi},\dots,u_{% t}^{\Omega},u_{t}^{\Psi}\right\}\sim\mathcal{D}$. Thus, we build a dialogue model that can predict the counselor’s utterance $u_{t}^{\Psi}$ based on the dialogue history $h_{t}=\left\{u_{1}^{\Omega},u_{1}^{\Psi},u_{2}^{\Omega},u_{2}^{\Psi},\dots,u_{% t}^{\Omega}\right\}$. Our objective is to use our synthetic conversation dataset $\mathcal{D}$ to fine-tune a large language model $\pi_{0}$ using supervised learning, i.e., maximum likelihood estimates (MLE):

| (3) |  | $J_{\mathrm{SFT}}(\theta)=\mathbb{E}_{(h_{t},u_{t}^{\Psi})\sim\mathcal{D}}\left% [\log\pi_{\theta}(u_{t}^{\Psi}\mid h_{t})\right]$ |  |

where $\pi_{\theta}$ is initialized from $\pi_{0}$.

### 4.2\. Experimental Setup

Comparison Models. MeChat (Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35)) is a model trained on the SmileChat dataset, which is generated by rewriting single-turn dialogues into multi-turn ones using ChatGPT. SoulChat (Chen et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib10)) is a model trained on the SoulChatCorpus (multi-turn) dataset, which is generated by rewriting the single-turn SoulChatCorpus into multi-turn dialogues using ChatGPT and GPT-4\. PsyChat (Qiu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib36)) is a model trained on RealPsyDial with Low-Rank Adaptation fine-tuning. CPsyCounX (Zhang et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib53)) is a model trained on CPsyCounD, a dataset generated from psychological counseling reports.

Backbone Model. To validate the dialogue quality of our collected dataset, we conduct fine-tuning experiments on two popular large language models, including Qwen2-7B-Instruct (Yang et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib50)) and deepseek-llm-7b-chat (DeepSeek-AI, [2024a](https://arxiv.org/html/2408.15787v1#bib.bib12)).

Format of Training Data. To meet the data format requirements for instruction-based fine-tuning, we split the dialogue into multiple training samples, each concluding with the counselor’s last utterance. Additionally, we incorporate the system prompt (as detailed in Figure [3](https://arxiv.org/html/2408.15787v1#S2.F3 "Figure 3 ‣ 2.3\. Simulation Framework Overview ‣ 2\. Method ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions")) as a prefix to the dialogue messages, adhering to OpenAI’s data format.

Full Fine-tuning. Considering that enough data and compute resources are available, full fine-tuning can achieve the best performance for the target task since every parameter can be optimized. Therefore, our paper uses full fine-tuning to train the dialogue system.

Hyperparameters. In this paper, all experiments are conducted on the NVIDIA A100 80G GPUs for model training. During model fine-tuning, we use 4 GPUs, set the training batch size to 1 per device, and set the step of gradient accumulation to 2, meaning that the gradient from every two steps would be accumulated and then used for parameter update. The learning rate is 1e-5\. We adopt the cosine-type learning rate scheduler to adjust the learning rate throughout the training process. The entire training will span across two epochs. To accelerate the training and balance model performance, we also enable using 16-bit half-precision floating point numbers. To avoid errors in the evaluation stage, we set the size of the validation set as 0.001\. This paper implements the fine-tuning based on LLaMA Factory (Zheng et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib58)), an efficient model-tuning framework.

![Refer to caption](img/7b69bd6316b803de54dddf8cc0766e41.png)

Figure 9\. Interaction with multiple counselors.

\Description

An illustration of interaction with multiple counselors.

### 4.3\. Interaction with Multiple Counselors

Automatic evaluation cannot faithfully reflect the quality of dialogue systems, especially in psychological counseling. Human evaluation is still more widely used and acknowledged. However, recruiting professional counselors to assess various virtual counselors is costly and not reproducible for the evaluation community. Therefore, to ensure high-quality evaluation, we consider human annotators and LLMs as judges (Zheng et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib56)) to evaluate model responses given a dialogue history. Nevertheless, effectively and efficiently identifying the optimal model among many remains a challenge. Motivated by a previous study (Zhang et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib54)), we designed an evaluation platform to interact with multiple virtual counselors simultaneously, as shown in Figure [9](https://arxiv.org/html/2408.15787v1#S4.F9 "Figure 9 ‣ 4.2\. Experimental Setup ‣ 4\. Dialogue System ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"). We randomly select two comparison models and one of our fine-tuned models as evaluation targets. We also present a set of evaluation standards for human and LLM annotators. For more details, please refer to our GitHub repository.

Evaluation Platform. Asking human annotators to act as clients is a challenging and costly task. To address this, we propose to directly adopt virtual clients powered by GPT-4\. For assessing each counseling session, a virtual client is randomly selected from the test set containing 100 different clients. This client will interact with three virtual counselors, with the only intervention being selecting the best response from among the candidates. A chatbot is considered to have superior performance if its responses are chosen over those of others more frequently. To ensure a fair comparison, we maintain the same dialogue history for all the chatbots at each turn. To facilitate this process, we record each turn’s message from the annotator and the selected response in the dialogue history. It is important to note that the chatbot’s name is not disclosed to annotators, and the order of the messages is shuffled before being displayed on the platform to prevent potential annotation bias.

Table 2\. Automatic evaluation with an LLM as a judge. We use DeepSeek-V2-Chat as our LLM judge.

 | Model | # Dialogues | # Total Selection | Avg. Score ($\uparrow$) | Elo ($\uparrow$) |
| --- | --- | --- | --- | --- |
| MeChat | 300 | 560 | 1.87 | 792.04 |
| SoulChat | 300 | 253 | 0.84 | 724.26 |
| PsyChat | 300 | 427 | 1.42 | 729.20 |
| CPsyCounX | 300 | 722 | 2.41 | 1187.81 |
|  

&#124; SimPsyBot &#124;
&#124; (backbone model: &#124;
&#124; deepseek-llm-7b-chat) &#124;

 | 600 | 5185 | 8.64 | 1895.27 |
| MeChat | 300 | 618 | 2.06 | 869.60 |
| SoulChat | 300 | 243 | 0.81 | 772.16 |
| PsyChat | 300 | 474 | 1.58 | 803.46 |
| CPsyCounX | 300 | 760 | 2.53 | 1288.04 |
|  

&#124; SimPsyBot &#124;
&#124; (backbone model: &#124;
&#124; Qwen2-7B-Instruct) &#124;

 | 600 | 4773 | 7.96 | 1758.05 | 

Table 3\. Human evaluation with 5 experts as judges.

 | Model | # Dialogues | # Total Selection | Avg. Score ($\uparrow$) | Elo ($\uparrow$) |
| --- | --- | --- | --- | --- |
| MeChat | 96 | 264 | 2.75 | 1352.80 |
| SoulChat | 95 | 200 | 2.11 | 1221.64 |
| PsyChat | 102 | 333 | 3.26 | 1355.53 |
| CPsyCounX | 107 | 167 | 1.56 | 1113.52 |
| SimPsyBot | 200 | 1570 | 7.85 | 1871.97 | 

For human evaluation, we recruit five professional counselors and require them to use our designed evaluation platform to assess which one is the best among the three generated responses given a shared dialogue history. By clicking the ”Start” button, the platform will randomly select a user profile to instantiate a virtual client from the help-out test set. For convenience, we start the conversation by picking the client to go first with the utterance of ”Hello.” Subsequently, three shuffled responses will be generated by three virtual counselors. The professional counselor is required to pick the best response. Once selected, the virtual client will continue the conversation, and another three responses will be displayed on the platform for the professional counselor to select. The professional counselor may use the ”Terminate” button to stop the current conversation, or the selected response meets the criteria for ending the interaction. Each professional counselor is required to evaluate 40 different dialogues, and each dialogue lasts more than five turns.

For automatic evaluation, the response selector is just replaced by the LLM as a judge. Therefore, the whole process will automatically proceed without any human intervention until the selected response meets the criteria for ending the interaction.

The average number of response selections per dialogue by professional counselors determines each chatbot’s score, which is determined as $score_{\mathrm{avg}}=\frac{\mathrm{\#\ Total\ Selection}}{\mathrm{\#\ Dialogue}}$. Furthermore, we also adopt the Elo rating to evaluate the model performance.

Advantages. The proposed evaluation has two main advantages:

*   •

    Centered conversation and comparative voting. Conversing with each chatbot independently will inevitably, therefore, have some bias. In addition, a conversation history often has one-to-many responses, so traditional automatic metrics are unsuitable for this situation. Therefore, to address these challenges, our proposed simultaneous chat with multiple chatbots not only accelerates the conversation model evaluation and reduces the cost of the annotators but also mitigates the evaluation bias and improves the fairness of the evaluation.

*   •

    Comprehensive assessment. Rather than considering conventional multi-dimensional metrics like empathy, informativeness, and helpfulness (e.g., because not every response needs to be empathetic), determining which response is more suitable for the dialogue history among many is a more comprehensive, efficient, and effective evaluation method.

Results. We present the results of automatic and human evaluation in Table [2](https://arxiv.org/html/2408.15787v1#S4.T2 "Table 2 ‣ 4.3\. Interaction with Multiple Counselors ‣ 4\. Dialogue System ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions") and [3](https://arxiv.org/html/2408.15787v1#S4.T3 "Table 3 ‣ 4.3\. Interaction with Multiple Counselors ‣ 4\. Dialogue System ‣ Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions"), respectively.

SimPsyBot vs. Other Models. SimPsyBot consistently outperforms other models in both automatic and human evaluations, demonstrating strong conversational abilities and high user satisfaction. In automatic evaluations, its average scores (8.64 and 7.96) and Elo ratings (1895.27 and 1758.05) significantly exceed those of other models. Similarly, in human evaluations, it leads with an impressive average score of 7.85 and an Elo rating of 1871.97\. These results indicate that SimPsyBot has a comprehensive advantage in dialogue tasks and is the best choice from both a system perspective and in terms of human feedback.

Automatic vs. Human Evaluations. Some models exhibit differences in performance between automatic and human evaluations. For example, PsyChat performs significantly better in human evaluations (3.26) than automatic evaluations (1.42 and 1.58). This result suggests that automatic scoring may not fully capture PsyChat’s capabilities, whereas it excels under human expert judgment. In contrast, SimPsyBot maintains a leading position in both types of evaluations, showcasing its consistently superior performance.

## 5\. RELATED WORK

### 5.1\. Interactive Simulacra

Large language models have already entered the interactive simulacra era (Park et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib32); Xie et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib48); Park et al., [2022](https://arxiv.org/html/2408.15787v1#bib.bib33); Bernard and Balog, [2024b](https://arxiv.org/html/2408.15787v1#bib.bib8); Lu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib28)). Interactive agents have been extensively studied within the fields of information retrieval (IR) and conversational agents (CA) (Tu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib42); Owoicho et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib31); Balog and Zhai, [2024](https://arxiv.org/html/2408.15787v1#bib.bib5)), enabling users to engage in multi-turn dialogues with agents to clarify and refine their queries and retrieve pertinent information. Due to the powerful potential of LLMs, such as generating coherent and contextually appropriate language similar to how humans communicate, interactive agents are ideal for human simulation in the natural language paradigm. Therefore, a myriad of research efforts of human simulation has been proposed to model human behaviors in various applications, including education (Hu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib19); Lee et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib22), [2024](https://arxiv.org/html/2408.15787v1#bib.bib21); Zhang et al., [2024c](https://arxiv.org/html/2408.15787v1#bib.bib55); Tu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib42)), recommender systems (Afzali et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib3); Bernard and Balog, [2024a](https://arxiv.org/html/2408.15787v1#bib.bib7); Huang et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib20)), social science (Xie et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib48); Dai et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib11)), medicine (Li et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib26); Schmidgall et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib39); Yan et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib49)), and psychological counseling (Li et al., [2023b](https://arxiv.org/html/2408.15787v1#bib.bib25); Wang et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib44), [a](https://arxiv.org/html/2408.15787v1#bib.bib45)). One such application in education, many researchers are mainly focusing on applying LLMs to assist teaching tasks, such as instructional design (Hu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib19); Zheng et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib57)), educational assessment (Li et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib24); Lee et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib21)), classroom simulation (Sonkar et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib40); Lee et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib22); Markel et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib29); Zhang et al., [2024c](https://arxiv.org/html/2408.15787v1#bib.bib55); Tu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib42); Yue et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib51)).

This paper also presents some studies about interactive simulacra in psychological counseling. Li et al. (Li et al., [2023b](https://arxiv.org/html/2408.15787v1#bib.bib25)) conducted a systematic review and found that AI-based conversational agents can promote mental health and well-being. To better assess the performance of LLM-based counselors, Wang et al. (Wang et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib44)) introduced ClientCAST, a client-centered approach for assessing the efficacy of LLM therapists through simulated client interactions. Further, Wang et al. (Wang et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib45)) introduced a patient simulation framework utilizing large language models for training mental health professionals in cognitive behavior therapy. To the best of our knowledge, our work is the first to utilize LLMs as annotator-free counselor-client simulators in psychological counseling, where the client is randomly selected from a real-life client pool.

### 5.2\. Conversational Agents for Mental Health

LLMs see extensive use across multiple application domains. LLMs have become powerful and intelligent agents across our daily lives, providing vast benefits from education (Hu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib19); Lee et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib22), [2024](https://arxiv.org/html/2408.15787v1#bib.bib21); Zhang et al., [2024c](https://arxiv.org/html/2408.15787v1#bib.bib55); Tu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib42)) to recommender systems (Afzali et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib3); Bernard and Balog, [2024a](https://arxiv.org/html/2408.15787v1#bib.bib7); Huang et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib20)), social science (Xie et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib48)) to medicine (Li et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib26); Schmidgall et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib39); Yan et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib49)), and mental health (Qiu et al., [2023b](https://arxiv.org/html/2408.15787v1#bib.bib37); Li et al., [2023b](https://arxiv.org/html/2408.15787v1#bib.bib25); Wang et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib44), [a](https://arxiv.org/html/2408.15787v1#bib.bib45)). In this paper, we mainly focus on conversational agents for mental health.

The utilization of LLMs in psychological counseling and mental health support is an emerging research area (Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35); Chen et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib10); Qiu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib36); Zhang et al., [2024a](https://arxiv.org/html/2408.15787v1#bib.bib52)). At the very beginning, Qiu et al. (Qiu et al., [2023b](https://arxiv.org/html/2408.15787v1#bib.bib37)) introduced a benchmark for assessing the safety of model responses in counseling conversations. Subsequently, many dialogue models, including English and Chinese, are developed to assist clients in healing themselves in mental health. Liu et al. (Liu et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib27)) developed ChatCounselor, which is trained with 260 in-depth interviews while focusing on English. Furthermore, a series of dialogue models focusing on Chinese is proposed. MeChat (Qiu et al., [2023a](https://arxiv.org/html/2408.15787v1#bib.bib35)) is a model trained on the SmileChat dataset, which is generated by rewriting single-turn dialogues into multi-turn ones using ChatGPT. SoulChat (Chen et al., [2023](https://arxiv.org/html/2408.15787v1#bib.bib10)) is a model trained on the SoulChatCorpus (multi-turn) dataset, which is generated by rewriting the single-turn SoulChatCorpus into multi-turn dialogues using ChatGPT and GPT-4\. PsyChat (Qiu et al., [2024](https://arxiv.org/html/2408.15787v1#bib.bib36)) is a model trained on RealPsyDial with Low-Rank Adaptation fine-tuning. CPsyCounX (Zhang et al., [2024b](https://arxiv.org/html/2408.15787v1#bib.bib53)) is a model trained on CPsyCounD, a dataset generated from psychological counseling reports. Based on these findings, our study will explore the potential of LLM-based counselor-client simulation to investigate privacy-free counseling dialogues and facilitate advancements in dialogue models in psychological counseling.

## 6\. Conclusion and Future Work

This paper introduces a framework using two LLMs in a role-playing setup to simulate counselor-client interactions. One LLM acts as a client with a real-life profile, while the other plays an experienced counselor using integrative therapy techniques. Both roles are implemented via zero-shot prompting with the GPT-4 model. We evaluate the effectiveness of these LLMs by comparing their simulated dialogues with those of professional counselors. Additionally, we conduct experiments to benchmark the LLM-based counselor against state-of-the-art mental health models. Our research highlights the potential of LLMs in enhancing psychological counseling simulations and client engagement.

In this paper, we identify three main directions for future work. First, we plan to incorporate resistance characteristics into client simulation and conduct a more comprehensive empirical study to analyze the influence of client resistance on generated dialogues. Second, we also plan to simulate a second counselor-client counseling session based on the first session and analyze how the client changes in the second session. Third, we aim to build a more human-like dialogue dataset. To this end, we propose to optimize the prompt and use retrieval-augmented generation (RAG) with real-life counseling sessions to construct verisimilar counselors and clients.

## 7\. Ethical Considerations

Important: Our research is reviewed and approved by the Westlake University Institutional Ethics Committee (20211013LZZ001). Our study explores the potential of LLMs to simulate counselors and clients in psychological counseling but does NOT recommend their use as a substitute for psychological treatment without professional supervision.

Considering the recent great attention on interactive simulacra and vast interest in utilizing them for various research directions, we are confident that exploring such a direction is meaningful because it unveils the potential of LLMs, while at the same time exhibiting their potential ethical considerations. Below, we will show some of these concerns that need to be considered and addressed in this research area:

*   •

    Inappropriate advice: LLMs are trained on vast amounts of data and thus have somewhat undesirable patterns regarding their training data. The synthetic data generated by interactive simulacra could, in turn, carry wrong advice or suggestions and further deepen wrong behaviors, including inappropriate or unprofessional advice. For example, reading books is good advice for most clients, but this suggestion may be inappropriate for a client who has a visual defect.

*   •

    Client simulation: Due to the training techniques involving instruction-following and reinforcement learning from human feedback, using an LLM to simulate clients potentially ignores the social impact on clients, such as family, job, and even suicide risk, which would lead to increased resistance in counseling for real clients.

*   •

    Counselor simulation: Using LLMs to simulate counselors has a limitation on counseling depth. In a real-life setting, the counselor often speaks less than the client, and the counselor could explore the client’s cockles of the heart, just like peeling onions.

*   •

    Environmental impact: The training and inference of LLMs involve significant computational resources, which leads to energy consumption and potentially has a negative environmental impact.

While simulating counselor-client interactions using LLMs has various advantages, we should pay more attention to carefully considering and addressing the potential ethical implications in future work.

## References

*   (1)
*   Abbasiantaeb et al. (2024) Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, and Mohammad Aliannejadi. 2024. Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions. In *Proceedings of the 17th ACM International Conference on Web Search and Data Mining* (Merida, Mexico) *(WSDM ’24)*. Association for Computing Machinery, New York, NY, USA, 8–17. [https://doi.org/10.1145/3616855.3635856](https://doi.org/10.1145/3616855.3635856)
*   Afzali et al. (2023) Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog, and Shuo Zhang. 2023. UserSimCRS: a user simulation toolkit for evaluating conversational recommender systems. In *Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining*. 1160–1163.
*   Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. arXiv:2309.16609 [cs.CL] [https://arxiv.org/abs/2309.16609](https://arxiv.org/abs/2309.16609)
*   Balog and Zhai (2024) Krisztian Balog and ChengXiang Zhai. 2024. Tutorial on User Simulation for Evaluating Information Access Systems on the Web. In *Companion Proceedings of the ACM on Web Conference 2024*. 1254–1257.
*   Bayerl et al. (2022) Sebastian P Bayerl, Gabriel Roccabruna, Shammur Absar Chowdhury, Tommaso Ciulli, Morena Danieli, Korbinian Riedhammer, and Giuseppe Riccardi. 2022. What can speech and language tell us about the working alliance in psychotherapy. *arXiv preprint arXiv:2206.08835* (2022).
*   Bernard and Balog (2024a) Nolwenn Bernard and Krisztian Balog. 2024a. Identifying Breakdowns in Conversational Recommender Systems using User Simulation. In *Proceedings of the 6th ACM Conference on Conversational User Interfaces*. 1–10.
*   Bernard and Balog (2024b) Nolwenn Bernard and Krisztian Balog. 2024b. Towards a Formal Characterization of User Simulation Objectives in Conversational Information Access. In *Proceedings of the 2024 ACM SIGIR International Conference on Theory of Information Retrieval*. 185–193.
*   Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
*   Chen et al. (2023) Yirong Chen, Xiaofen Xing, Jingkai Lin, Huimin Zheng, Zhenyu Wang, Qi Liu, and Xiangmin Xu. 2023. Soulchat: Improving llms’ empathy, listening, and comfort abilities through fine-tuning with multi-turn empathy conversations. In *Findings of the Association for Computational Linguistics: EMNLP 2023*. 1170–1183.
*   Dai et al. (2024) Gordon Dai, Weijia Zhang, Jinhan Li, Siqi Yang, Chidera Onochie lbe, Srihas Rao, Arthur Caetano, and Misha Sra. 2024. Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory. arXiv:2406.14373 [cs.AI] [https://arxiv.org/abs/2406.14373](https://arxiv.org/abs/2406.14373)
*   DeepSeek-AI (2024a) DeepSeek-AI. 2024a. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. arXiv:2401.02954 [cs.CL] [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954)
*   DeepSeek-AI (2024b) DeepSeek-AI. 2024b. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model. arXiv:2405.04434 [cs.CL] [https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434)
*   Form et al. (2000) Working Alliance Inventory-Observer Form, IV Revision, Andrew Darchuk, Victor Wang, David Weibel, Jennifer Fende, Timothy Anderson, and Adam Horvath. 2000. Department of Psychology Ohio University December 11, 2000. (2000).
*   GLM et al. (2024) Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools. arXiv:2406.12793
*   Grossmann et al. (2023) Igor Grossmann, Matthew Feinberg, Dawn C Parker, Nicholas A Christakis, Philip E Tetlock, and William A Cunningham. 2023. AI and the transformation of social science research. *Science* 380, 6650 (2023), 1108–1109.
*   Hill (2020) Clara E Hill. 2020. *Helping skills: Facilitating exploration, insight, and action*. American Psychological Association.
*   Hofmann et al. (2012) Stefan G Hofmann, Anu Asnaani, Imke JJ Vonk, Alice T Sawyer, and Angela Fang. 2012. The efficacy of cognitive behavioral therapy: A review of meta-analyses. *Cognitive therapy and research* 36 (2012), 427–440.
*   Hu et al. (2024) Bihao Hu, Longwei Zheng, Jiayi Zhu, Lishan Ding, Yilei Wang, and Xiaoqing Gu. 2024. Teaching Plan Generation and Evaluation With GPT-4: Unleashing the Potential of LLM in Instructional Design. *IEEE Transactions on Learning Technologies* 17 (2024), 1471–1485. [https://doi.org/10.1109/TLT.2024.3384765](https://doi.org/10.1109/TLT.2024.3384765)
*   Huang et al. (2024) Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei, Jiancheng Lv, and Tat-Seng Chua. 2024. Concept–An Evaluation Protocol on Conversation Recommender Systems with System-and User-centric Factors. *arXiv preprint arXiv:2404.03304* (2024).
*   Lee et al. (2024) Unggi Lee, Jiyeong Bae, Dohee Kim, Sookbun Lee, Jaekwon Park, Taekyung Ahn, Gunho Lee, Damji Stratton, and Hyeoncheol Kim. 2024. Language Model Can Do Knowledge Tracing: Simple but Effective Method to Integrate Language Model and Knowledge Tracing Task. arXiv:2406.02893 [cs.CL] [https://arxiv.org/abs/2406.02893](https://arxiv.org/abs/2406.02893)
*   Lee et al. (2023) Unggi Lee, Sanghyeok Lee, Junbo Koh, Yeil Jeong, Haewon Jung, Gyuri Byun, Jewoong Moon, Jieun Lim, and † HyeoncheolKim. 2023. Generative Agent for Teacher Training: Designing Educational Problem-Solving Simulations with Large Language Model-based Agents for Pre-Service Teachers. In *NeurIPS’23 Workshop on Generative AI for Education (GAIED)*. NeurIPS. [https://api.semanticscholar.org/CorpusID:266874743](https://api.semanticscholar.org/CorpusID:266874743)
*   Li et al. (2023a) Anqi Li, Lizhi Ma, Yaling Mei, Hongliang He, Shuai Zhang, Huachuan Qiu, and Zhenzhong Lan. 2023a. Understanding Client Reactions in Online Mental Health Counseling. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 10358–10376. [https://doi.org/10.18653/v1/2023.acl-long.577](https://doi.org/10.18653/v1/2023.acl-long.577)
*   Li et al. (2024b) Haoxuan Li, Jifan Yu, Yuanxin Ouyang, Zhuang Liu, Wenge Rong, Juanzi Li, and Zhang Xiong. 2024b. Explainable Few-shot Knowledge Tracing. arXiv:2405.14391 [cs.AI] [https://arxiv.org/abs/2405.14391](https://arxiv.org/abs/2405.14391)
*   Li et al. (2023b) Han Li, Renwen Zhang, Yi-Chieh Lee, Robert E Kraut, and David C Mohr. 2023b. Systematic review and meta-analysis of AI-based conversational agents for promoting mental health and well-being. *NPJ Digital Medicine* 6, 1 (2023), 236.
*   Li et al. (2024a) Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and Yang Liu. 2024a. Agent hospital: A simulacrum of hospital with evolvable medical agents. *arXiv preprint arXiv:2405.02957* (2024).
*   Liu et al. (2023) June M Liu, Donghao Li, He Cao, Tianhe Ren, Zeyi Liao, and Jiamin Wu. 2023. Chatcounselor: A large language models for mental health support. *arXiv preprint arXiv:2309.15461* (2023).
*   Lu et al. (2024) Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. arXiv:2408.06292 [cs.AI] [https://arxiv.org/abs/2408.06292](https://arxiv.org/abs/2408.06292)
*   Markel et al. (2023) Julia M Markel, Steven G Opferman, James A Landay, and Chris Piech. 2023. Gpteach: Interactive ta training with gpt-based students. In *Proceedings of the tenth acm conference on learning@ scale*. 226–236.
*   OpenAI (2024) OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)
*   Owoicho et al. (2023) Paul Owoicho, Ivan Sekulic, Mohammad Aliannejadi, Jeffrey Dalton, and Fabio Crestani. 2023. Exploiting simulated user feedback for conversational search: Ranking, rewriting, and beyond. In *Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval*. 632–642.
*   Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. arXiv:2304.03442 [cs.HC] [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442)
*   Park et al. (2022) Joon Sung Park, Lindsay Popowski, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2022. Social Simulacra: Creating Populated Prototypes for Social Computing Systems. arXiv:2208.04024 [cs.HC] [https://arxiv.org/abs/2208.04024](https://arxiv.org/abs/2208.04024)
*   Piper (2008) William E Piper. 2008. Underutilization of short-term group therapy: Enigmatic or understandable? *Psychotherapy Research* 18, 2 (2008), 127–138.
*   Qiu et al. (2023a) Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, and Zhenzhong Lan. 2023a. Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support. *arXiv preprint arXiv:2305.00450* (2023).
*   Qiu et al. (2024) Huachuan Qiu, Anqi Li, Lizhi Ma, and Zhenzhong Lan. 2024. PsyChat: A Client-Centric Dialogue System for Mental Health Support. In *2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)*. 2979–2984. [https://doi.org/10.1109/CSCWD61410.2024.10580641](https://doi.org/10.1109/CSCWD61410.2024.10580641)
*   Qiu et al. (2023b) Huachuan Qiu, Tong Zhao, Anqi Li, Shuai Zhang, Hongliang He, and Zhenzhong Lan. 2023b. A benchmark for understanding dialogue safety in mental health support. In *CCF International Conference on Natural Language Processing and Chinese Computing*. Springer, 1–13.
*   Rogers (1946) Carl R Rogers. 1946. Significant aspects of client-centered therapy. *American psychologist* 1, 10 (1946), 415–422.
*   Schmidgall et al. (2024) Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. 2024. AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments. *arXiv preprint arXiv:2405.07960* (2024).
*   Sonkar et al. (2023) Shashank Sonkar, Naiming Liu, Debshila Mallick, and Richard Baraniuk. 2023. CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 1941–1961. [https://doi.org/10.18653/v1/2023.findings-emnlp.130](https://doi.org/10.18653/v1/2023.findings-emnlp.130)
*   Sun et al. (2021) Hao Sun, Zhenru Lin, Chujie Zheng, Siyang Liu, and Minlie Huang. 2021. Psyqa: A chinese dataset for generating long counseling text for mental health support. *arXiv preprint arXiv:2106.01702* (2021).
*   Tu et al. (2023) Shangqing Tu, Zheyuan Zhang, Jifan Yu, Chunyang Li, Siyu Zhang, Zijun Yao, Lei Hou, and Juanzi Li. 2023. LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts. In *Proceedings of the 32nd ACM International Conference on Information and Knowledge Management*. 4843–4849.
*   Wampold (2013) Bruce E Wampold. 2013. *The great psychotherapy debate: Models, methods, and findings*. Routledge.
*   Wang et al. (2024b) Jiashuo Wang, Yang Xiao, Yanran Li, Changhe Song, Chunpu Xu, Chenhao Tan, and Wenjie Li. 2024b. Towards a Client-Centered Assessment of LLM Therapists by Client Simulation. *arXiv preprint arXiv:2406.12266* (2024).
*   Wang et al. (2024a) Ruiyi Wang, Stephanie Milani, Jamie C. Chiu, Jiayin Zhi, Shaun M. Eack, Travis Labrum, Samuel M. Murphy, Nev Jones, Kate Hardy, Hong Shen, Fei Fang, and Zhiyu Zoey Chen. 2024a. PATIENT-$\Psi$: Using Large Language Models to Simulate Patients for Training Mental Health Professionals. arXiv:2405.19660 [cs.CL] [https://arxiv.org/abs/2405.19660](https://arxiv.org/abs/2405.19660)
*   Warren (1998) C Seth Warren. 1998. Models of brief psychodynamic therapy: A comparative approach. *Psychology* (1998).
*   Weizenbaum (1966) Joseph Weizenbaum. 1966. ELIZA—a computer program for the study of natural language communication between man and machine. *Commun. ACM* 9, 1 (jan 1966), 36–45. [https://doi.org/10.1145/365153.365168](https://doi.org/10.1145/365153.365168)
*   Xie et al. (2024) Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi, Ziniu Hu, Philip Torr, Bernard Ghanem, and Guohao Li. 2024. Can Large Language Model Agents Simulate Human Trust Behaviors? arXiv:2402.04559 [cs.AI] [https://arxiv.org/abs/2402.04559](https://arxiv.org/abs/2402.04559)
*   Yan et al. (2024) Weixiang Yan, Haitian Liu, Tengxiao Wu, Qian Chen, Wen Wang, Haoyuan Chai, Jiayi Wang, Weishan Zhao, Yixin Zhang, Renjun Zhang, et al. 2024. ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World. *arXiv preprint arXiv:2406.13890* (2024).
*   Yang et al. (2024) An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv:2407.10671 [cs.CL] [https://arxiv.org/abs/2407.10671](https://arxiv.org/abs/2407.10671)
*   Yue et al. (2024) Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. 2024. MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education. arXiv:2404.06711 [cs.CL] [https://arxiv.org/abs/2404.06711](https://arxiv.org/abs/2404.06711)
*   Zhang et al. (2024a) Chenhao Zhang, Renhao Li, Minghuan Tan, Min Yang, Jingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye, Chengming Li, and Xiping Hu. 2024a. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. arXiv:2405.16433 [cs.CL] [https://arxiv.org/abs/2405.16433](https://arxiv.org/abs/2405.16433)
*   Zhang et al. (2024b) Chenhao Zhang, Renhao Li, Minghuan Tan, Min Yang, Jingwei Zhu, Di Yang, Jiahao Zhao, Guancheng Ye, Chengming Li, and Xiping Hu. 2024b. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. arXiv:2405.16433 [cs.CL] [https://arxiv.org/abs/2405.16433](https://arxiv.org/abs/2405.16433)
*   Zhang et al. (2023) Jing Zhang, Xiaokang Zhang, Daniel Zhang-Li, Jifan Yu, Zijun Yao, Zeyao Ma, Yiqi Xu, Haohua Wang, Xiaohan Zhang, Nianyi Lin, Sunrui Lu, Juanzi Li, and Jie Tang. 2023. GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (Long Beach, CA, USA) *(KDD ’23)*. Association for Computing Machinery, New York, NY, USA, 5564–5575. [https://doi.org/10.1145/3580305.3599832](https://doi.org/10.1145/3580305.3599832)
*   Zhang et al. (2024c) Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, and Juanzi Li. 2024c. Simulating Classroom Education with LLM-Empowered Agents. *arXiv preprint arXiv:2406.19226* (2024).
*   Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In *Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track*. [https://openreview.net/forum?id=uccHPGDlao](https://openreview.net/forum?id=uccHPGDlao)
*   Zheng et al. (2024a) Ying Zheng, Xueyi Li, Yaying Huang, Qianru Liang, Teng Guo, Mingliang Hou, Boyu Gao, Mi Tian, Zitao Liu, and Weiqi Luo. 2024a. Automatic Lesson Plan Generation via Large Language Models with Self-critique Prompting. In *International Conference on Artificial Intelligence in Education*. Springer, 163–178.
*   Zheng et al. (2024b) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024b. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. arXiv:2403.13372 [cs.CL] [https://arxiv.org/abs/2403.13372](https://arxiv.org/abs/2403.13372)