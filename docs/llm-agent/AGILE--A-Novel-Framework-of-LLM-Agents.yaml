- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AGILE: A Novel Framework of LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AGILE：一种新型的LLM代理框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14751](https://ar5iv.labs.arxiv.org/html/2405.14751)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14751](https://ar5iv.labs.arxiv.org/html/2405.14751)
- en: Peiyuan Feng^∗¹ Yichen He^∗¹  Guanhua Huang^∗^†²  Yuan Lin^∗¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Peiyuan Feng^∗¹ Yichen He^∗¹  Guanhua Huang^∗^†²  Yuan Lin^∗¹
- en: Hanchong Zhang^∗^†³ Yuchen Zhang^∗¹ Hang Li¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hanchong Zhang^∗^†³ Yuchen Zhang^∗¹ Hang Li¹
- en: ¹ByteDance Research  ²University of Science and Technology of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹字节跳动研究  ²中国科学技术大学
- en: ³Shanghai Jiao Tong University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海交通大学
- en: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{fpy,hyc,linyuan.0,zhangyuchen.zyc,lihang.lh}@bytedance.com,'
- en: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: guanhuahuang@mail.ustc.edu.cn, zhanghanchong@sjtu.edu.cn
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce a novel framework of LLM agents named AGILE (AGent that Interacts
    and Learns from Environments) designed to perform complex conversational tasks
    with users, leveraging LLMs, memory, tools, and interactions with experts. The
    agent’s abilities include not only conversation but also reflection, utilization
    of tools, and consultation with experts. We formulate the construction of such
    an LLM agent as a reinforcement learning problem, in which the LLM serves as the
    policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm.
    We focus on question answering and release a dataset for agents called ProductQA,
    comprising challenging questions in online shopping. Our extensive experiments
    on ProductQA and MedMCQA show that AGILE agents based on 13B and 7B LLMs trained
    with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability
    of memory, tools, consultation, reflection, and reinforcement learning in achieving
    the agent’s strong performance. Datasets and code are available at [https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一种名为AGILE（AGent that Interacts and Learns from Environments）的新型LLM代理框架，旨在执行复杂的用户对话任务，利用LLM、记忆、工具和与专家的互动。该代理不仅具备对话能力，还包括反思、工具使用和与专家咨询的能力。我们将这种LLM代理的构建形式化为强化学习问题，其中LLM充当策略模型。我们使用带标签的行动数据和PPO算法对LLM进行微调。我们专注于问答任务，并发布了一个名为ProductQA的代理数据集，包含在线购物中的挑战性问题。我们在ProductQA和MedMCQA上的广泛实验表明，基于13B和7B
    LLM的AGILE代理经过PPO训练可以超越GPT-4代理。我们的消融研究突显了记忆、工具、咨询、反思和强化学习在实现代理强大性能中的不可或缺性。数据集和代码可在[https://github.com/bytarnish/AGILE](https://github.com/bytarnish/AGILE)获得。
- en: '^($*$)^($*$)footnotetext: Equal contribution. Alphabet order.^($\dagger$)^($\dagger$)footnotetext:
    Work done during ByteDance Research internship.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ^($*$)^($*$)脚注：贡献相等。字母顺序。^($\dagger$)^($\dagger$)脚注：工作完成于ByteDance Research实习期间。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have exhibited remarkable capabilities such as
    instruction following, reasoning, and zero-shot learning [[2](#bib.bib2), [39](#bib.bib39),
    [15](#bib.bib15), [21](#bib.bib21)], which have greatly catalyzed the development
    of autonomous agents based on LLMs [[23](#bib.bib23), [25](#bib.bib25), [1](#bib.bib1)],
    also known as LLM agents. Recent works propose several essential components or
    workflows to enhance the abilities of LLM agents, such as planning [[39](#bib.bib39),
    [44](#bib.bib44), [33](#bib.bib33)], reflection [[16](#bib.bib16), [34](#bib.bib34)],
    tool-use [[24](#bib.bib24), [31](#bib.bib31), [41](#bib.bib41)] and life-long
    learning [[36](#bib.bib36)]. However, it remains unclear how to integrate all
    components into a unified framework and optimize them end-to-end.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已展示出显著的能力，如指令跟随、推理和零-shot学习[[2](#bib.bib2), [39](#bib.bib39), [15](#bib.bib15),
    [21](#bib.bib21)]，这些能力极大地促进了基于LLM的自主代理的发展[[23](#bib.bib23), [25](#bib.bib25),
    [1](#bib.bib1)]，也称为LLM代理。近期工作提出了若干关键组件或工作流程，以增强LLM代理的能力，如规划[[39](#bib.bib39),
    [44](#bib.bib44), [33](#bib.bib33)]、反思[[16](#bib.bib16), [34](#bib.bib34)]、工具使用[[24](#bib.bib24),
    [31](#bib.bib31), [41](#bib.bib41)]和终身学习[[36](#bib.bib36)]。然而，如何将所有组件整合到一个统一框架中并端到端优化仍不清楚。
- en: '![Refer to caption](img/d0ab0e0cb5ecfb005f3eb9e85e88f019.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d0ab0e0cb5ecfb005f3eb9e85e88f019.png)'
- en: 'Figure 1: (a) Architecture of our agent system, including LLM, memory, tools,
    and executor. (b) A running example of AGILE in a customer service QA environment.
    The tokens (actions) generated by the LLM are in orange color and the tokens appended
    by the executor are in blue color.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（a）我们代理系统的架构，包括LLM、记忆、工具和执行器。（b）AGILE在客户服务问答环境中的运行示例。LLM生成的令牌（动作）为橙色，执行器附加的令牌为蓝色。
- en: 'In this paper, we introduce a novel framework for LLM agents to unify various
    components and streamline their learning and operation processes. As shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of LLM
    Agents")(a), the architecture of the agent system, named AGILE, comprises four
    modules: LLM, memory, tools, and executor. Furthermore, the agent can interact
    with both users and experts. The LLM, functioning as the predictor of all actions,
    generates instructions and processes responses. The executor, working as the controller
    of all actions, interprets the LLM instructions to activate the corresponding
    modules and collects their responses for the LLM. For example, the executor can
    fetch a text from the memory and append it to the context of LLM, or extract an
    excerpt from the context and append it to the memory. The executor can also follow
    instructions of the LLM to utilize a search tool. In addition to skills such as
    reasoning, planning, and reflection, we propose a new ability called *seeking
    advice*, which means that the agent proactively consults human experts when it
    encounters a problem unsolvable. The agent can reflect on the expert feedback
    and memorize it for future use. Furthermore, we propose a training method based
    on reinforcement learning (RL), which simultaneously trains the policy of invoking
    different modules and the reasoning, planning, reflection, and seeking advice
    abilities of the LLM agent in an end-to-end fashion.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们介绍了一种新颖的LLM代理框架，旨在统一各种组件并简化其学习和操作过程。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Framework of LLM Agents")（a）所示，名为AGILE的代理系统架构包含四个模块：LLM、记忆、工具和执行器。此外，代理可以与用户和专家互动。LLM作为所有操作的预测器，生成指令并处理响应。执行器作为所有操作的控制器，解释LLM指令以激活相应的模块，并将其响应收集给LLM。例如，执行器可以从记忆中提取文本并将其附加到LLM的上下文中，或者从上下文中提取摘录并将其附加到记忆中。执行器还可以按照LLM的指令使用搜索工具。除了推理、规划和反思等技能外，我们提出了一种新的能力，称为*寻求建议*，即代理在遇到无法解决的问题时主动咨询人类专家。代理可以反思专家的反馈并记忆以备将来使用。此外，我们提出了一种基于强化学习（RL）的训练方法，该方法同时训练调用不同模块的策略和LLM代理的推理、规划、反思和寻求建议能力。'
- en: While the proposed agent framework is general, in this paper, we evaluate it
    in complex question answering (QA). It is a task an LLM agent has the potential
    of outperforming existing solutions such as the use of an LLM alone. However,
    existing QA benchmarks [[9](#bib.bib9), [42](#bib.bib42), [8](#bib.bib8), [22](#bib.bib22)]
    are designed for specific subsets of capabilities (e.g., reflection, memory retrieve,
    etc.) which cannot simultaneously investigate the ability to combine all modules
    and capabilities of the agent.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所提出的代理框架具有通用性，但在本文中，我们在复杂问题回答（QA）中对其进行了评估。这是一项LLM代理有可能超越现有解决方案的任务，比如单独使用LLM。然而，现有的QA基准[[9](#bib.bib9),
    [42](#bib.bib42), [8](#bib.bib8), [22](#bib.bib22)]是针对特定能力子集（例如，反思、记忆检索等）设计的，无法同时调查将所有模块和能力组合在一起的能力。
- en: To address this, we have developed a new benchmark called ProductQA. ProductQA
    comprises 88,229 question-answer pairs in customer service divided into 26 QA
    tasks, each corresponding to a distinct Amazon product category. This benchmark
    is based on real Amazon user queries and includes fact-based questions, reasoning
    questions, and product recommendation queries. It comprehensively evaluates agents’
    abilities to handle historical information and accumulated knowledge, leverage
    tools, interact with humans, perform self-evaluation, and conduct reflection.
    Additionally, the training and testing tasks are made disjoint to assess the agent’s
    ability to adapt to new product categories.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们开发了一个新的基准，称为ProductQA。ProductQA包含了88,229个客户服务中的问答对，分为26个QA任务，每个任务对应一个不同的亚马逊产品类别。这个基准基于真实的亚马逊用户查询，包括基于事实的问题、推理问题和产品推荐查询。它全面评估了代理处理历史信息和积累知识的能力、利用工具的能力、与人类互动的能力、自我评估的能力和反思的能力。此外，训练和测试任务被设为不重叠，以评估代理适应新产品类别的能力。
- en: 'We evaluate our agent framework on two tasks, ProductQA and MedMCQA [[22](#bib.bib22)].
    For ProductQA, we use a two-stage training method based on Vicuna-13b [[5](#bib.bib5)].
    In the first stage, imitation learning is employed to create agile-vic13b-sft.
    In the second stage, the policy gradient algorithm of PPO [[32](#bib.bib32)] produces
    agile-vic13b-ppo. Experimental results show that agile-vic13b-ppo improves the
    relative total performance score by 9.2% over GPT-4 and by 90.8% over GPT-3.5\.
    Ablation studies confirm that all modules in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Framework of LLM Agents") are indispensable. Specifically, removing
    tools or memory usage negatively impacts the agent’s performance, leading to a
    25.9% or 17.4% increase in seeking advice, respectively, or a 9.3% or 4.0% relative
    decrease in the total score, respectively. Disabling the seeking advice function
    results in a 10.7% decrease in accuracy. Finally, agile-vic13b-ppo achieves a
    2.3% relative increase in total score compared to agile-vic13b-sft, demonstrating
    the necessity of PPO training. On MedMCQA, we train an agile-mek7b-ppo agent,
    initialized from Meerkat-7b [[13](#bib.bib13)], following the same two-stage procedure.
    Our agent improves the base LLM’s accuracy from 53.4% to 85.2% by seeking advice
    on 31.6% instances. This accuracy surpasses the SOTA accuracy of 79.1% by GPT4-MedPrompt [[20](#bib.bib20)].
    When all agents are able to seek advice, our agent also outperforms the GPT-4
    agent in terms of the total score.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在两个任务 ProductQA 和 MedMCQA 上评估我们的代理框架[[22](#bib.bib22)]。对于 ProductQA，我们使用基于
    Vicuna-13b [[5](#bib.bib5)] 的两阶段训练方法。在第一阶段，使用模仿学习创建 agile-vic13b-sft。在第二阶段，PPO
    [[32](#bib.bib32)] 的策略梯度算法生成 agile-vic13b-ppo。实验结果表明，agile-vic13b-ppo 在相对总表现评分上比
    GPT-4 提高了 9.2%，比 GPT-3.5 提高了 90.8%。消融研究确认，图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ AGILE: A Novel Framework of LLM Agents") 中的所有模块都是不可或缺的。特别是，移除工具或内存使用会对代理的性能产生负面影响，分别导致寻求建议增加
    25.9% 或 17.4%，或者总分分别减少 9.3% 或 4.0%。禁用寻求建议功能会导致准确率下降 10.7%。最后，agile-vic13b-ppo
    相比 agile-vic13b-sft 实现了 2.3% 的相对总分提高，展示了 PPO 训练的必要性。在 MedMCQA 上，我们训练了一个 agile-mek7b-ppo
    代理，初始化自 Meerkat-7b [[13](#bib.bib13)]，遵循相同的两阶段程序。我们的代理通过在 31.6% 的情况下寻求建议，将基础 LLM
    的准确率从 53.4% 提高到 85.2%。这一准确率超越了 GPT4-MedPrompt [[20](#bib.bib20)] 的 79.1% 的 SOTA
    准确率。当所有代理能够寻求建议时，我们的代理在总评分上也优于 GPT-4 代理。'
- en: 'The main contributions of this paper are summarized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献总结如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We propose a novel framework of LLM agents, formulated in the context of reinforcement
    learning. It facilitates end-to-end learning of agents. Notably, this framework
    enables the agent to seek advice from human experts, providing two advantages:
    1) It ensures high-level accuracy when dealing with complex and challenging questions,
    and 2) it fosters learning from humans, thereby enhancing its abilities to adapt
    to new tasks.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新型的 LLM 代理框架，以强化学习的背景进行公式化。它支持代理的端到端学习。特别是，该框架使代理能够向人类专家寻求建议，提供了两个优势：1）确保在处理复杂和具有挑战性的问题时高水平的准确性，2）促进从人类学习，从而增强其适应新任务的能力。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a benchmark, ProductQA, to comprehensively evaluate the agent’s capabilities
    in complex question answering.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个基准测试 ProductQA，用于全面评估代理在复杂问答中的能力。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform experiments on ProductQA and MedMCQA to verify our framework and
    show that AGILE agents based on 13B and 7B LLMs trained with PPO can surpass GPT-4
    agents.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在 ProductQA 和 MedMCQA 上进行实验，以验证我们的框架，并展示基于 13B 和 7B LLM 的 AGILE 代理经过 PPO 训练后能够超越
    GPT-4 代理。
- en: 2 Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 RL formulation of agent
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代理的 RL 公式
- en: 'Our agent framework comprises four elements: LLM, memory, tools and executor,
    see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of
    LLM Agents")(a). The LLM possesses a *context*, defined as the sequence of tokens
    it utilizes to generate the next token. In RL terminology, the agent conducts
    a token-level Markov decision process (MDP). The action space $\mathcal{A}$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的代理框架包括四个元素：LLM、内存、工具和执行器，见图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE:
    A Novel Framework of LLM Agents")(a)。LLM 具有一个 *上下文*，定义为其用来生成下一个标记的标记序列。在 RL 术语中，代理执行的是标记级别的马尔可夫决策过程
    (MDP)。动作空间 $\mathcal{A}$。'
- en: 'Let us examine the state transition more closely. For each action, the executor’s
    first operation is to append the token to the context, preparing the LLM for generating
    the next token. Then, the executor checks a registered list of *functions*. Each
    function is designed to execute a set of operations, including memory I/O, tool
    usage, and interaction with the environment. If the action (i.e., the token) matches
    a function name, the executor will execute the associated function implementation,
    further mutating the agent state. For instance, if the token is [GetQuestion],
    the executor will prompt the user for a new question and append it to the context;
    if the token is [UpdateMemory], the executor will write a specific segment of
    the context into the memory; if the token is [ClearContext], the executor will
    reset the context to [BOS]. In summary, the LLM interacts with the memory and
    tools by predicting function names, relying on the executor to execute these functions.
    See Table [1](#S2.T1 "Table 1 ‣ 2.1 RL formulation of agent ‣ 2 Methods ‣ AGILE:
    A Novel Framework of LLM Agents") for a full list of functions defined for a QA
    agent and see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework
    of LLM Agents")(b) for a running example.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们更仔细地研究状态转换。对于每个操作，执行者的第一个操作是将令牌附加到上下文中，为 LLM 生成下一个令牌做准备。然后，执行者检查注册的*函数*列表。每个函数旨在执行一组操作，包括内存
    I/O、工具使用和与环境的交互。如果操作（即令牌）与函数名称匹配，执行者将执行相关的函数实现，进一步改变代理状态。例如，如果令牌是[GetQuestion]，执行者将提示用户提出新问题并将其附加到上下文中；如果令牌是[UpdateMemory]，执行者将把上下文中的特定段落写入内存；如果令牌是[ClearContext]，执行者将上下文重置为[BOS]。总之，LLM
    通过预测函数名称与内存和工具进行交互，依赖执行者来执行这些函数。有关 QA 代理定义的完整函数列表，请参见表[1](#S2.T1 "Table 1 ‣ 2.1
    RL formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Framework of LLM Agents")，有关运行示例，请参见图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ AGILE: A Novel Framework of LLM Agents")(b)。'
- en: 'Table 1: Functions for an exemplary customer service QA agent. Among them,
    [Reflection] and [PredictAnswer] are trivial functions, as the executor passes
    control immediately back to the LLM to start generating result tokens.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：示例客户服务 QA 代理的功能。其中，[Reflection] 和 [PredictAnswer] 是简单函数，因为执行者会立即将控制权交还给
    LLM，以开始生成结果令牌。
- en: '| Function name | Function implementation |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 功能名称 | 功能实现 |'
- en: '|  [GetQuestion] | Prompt the user for a question and append it to the context.
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  [GetQuestion] | 提示用户提问并将其附加到上下文中。 |'
- en: '|  [RetrieveMemory] | Retrieve relevant entries from the memory and append
    them to the context. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  [RetrieveMemory] | 从内存中检索相关条目并将其附加到上下文中。 |'
- en: '|  [SeekAdvice] | Ask human experts for advice and append it to the context.
    |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  [SeekAdvice] | 向人工专家寻求建议并将其附加到上下文中。 |'
- en: '|  [Reflection] | $\emptyset$ |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  [Reflection] | $\emptyset$ |'
- en: '|  [UpdateMemory] | Write a specific segment of the context into the memory.
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  [UpdateMemory] | 将上下文中的特定段落写入内存。 |'
- en: '|  [SearchProduct] | Extract a search query from the context, then invoke the
    search tool and append results to the context. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  [SearchProduct] | 从上下文中提取搜索查询，然后调用搜索工具并将结果附加到上下文中。 |'
- en: '|  [PredictAnswer] | $\emptyset$ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  [PredictAnswer] | $\emptyset$ |'
- en: '|  [SubmitAnswer] | Extract a predicted answer from the context and submit
    it to the user. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  [SubmitAnswer] | 从上下文中提取预测答案并提交给用户。 |'
- en: '|  [ClearContext] | Reset the context to a single token [BOS]. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  [ClearContext] | 将上下文重置为单个令牌 [BOS]。 |'
- en: 2.2 Policy learning
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 策略学习
- en: We frame the policy learning problem as a task of training a language model.
    Consider an agent trajectory $\tau=(s_{1},a_{1},...,s_{n},a_{n})$ because the
    executor can delete context tokens.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将策略学习问题框架设定为训练语言模型的任务。考虑一个代理轨迹 $\tau=(s_{1},a_{1},...,s_{n},a_{n})$，因为执行者可以删除上下文令牌。
- en: In Imitation Learning (IL), we generate trajectories by observing human experts
    or more proficient agents, then we derive the training sequences to fine-tune
    the LLM. It is important to point out that (1) the loss is calculated on the action
    tokens only, and (2) $c_{i}$ as the attention mask.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿学习（IL）中，我们通过观察人类专家或更熟练的代理生成轨迹，然后衍生训练序列以微调 LLM。需要指出的是（1）损失仅在动作令牌上计算，（2）$c_{i}$
    作为注意力掩码。
- en: 'In some situations, an agent may produce very long trajectories, potentially
    yielding training sequences that span millions of tokens and are impractical for
    training. We can leverage the structure of the trajectory to partition it into
    smaller segments. For instance, if the agent resets its LLM context at the beginning
    of every QA session, then we can partition by the session boundary. Nevertheless,
    these sessions are not entirely independent; actions taken in earlier sessions
    can influence memory, creating lasting effects on subsequent sessions. To tackle
    this challenge of long-range dependencies, we propose a training algorithm detailed
    in Appendix [A](#A1 "Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，代理可能会产生非常长的轨迹，这可能导致训练序列跨越数百万个标记，并且在训练中不切实际。我们可以利用轨迹的结构将其划分为更小的片段。例如，如果代理在每个QA会话开始时重置其LLM上下文，那么我们可以按会话边界进行划分。然而，这些会话并非完全独立；早期会话中的行为可能会影响记忆，从而对后续会话产生持久影响。为了应对这种长期依赖的问题，我们提出了一种训练算法，详细信息见附录[A](#A1
    "附录 A 会话级优化算法 ‣ AGILE：LLM代理的新框架")。
- en: 2.3 Interaction with human experts
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 与人类专家的互动
- en: Our agent framework enables the agent to proactively seek advice from human
    experts. For example, the agent can invoke a [SeekAdvice] function to request
    expert advice. This approach helps in two ways. Firstly, the agent can request
    the correct answer when its confidence is low, ensuring sufficient accuracy for
    the application. Secondly, the agent can use [Reflection] to distill general knowledge
    from the expert advice before storing it in memory. This accumulation of knowledge
    allows the agent to adapt to new tasks that it has not encountered during training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理框架使得代理能够主动寻求来自人类专家的建议。例如，代理可以调用一个[SeekAdvice]函数来请求专家建议。这种方法有两个好处。首先，当代理的信心较低时，它可以请求正确的答案，确保应用的准确性。其次，代理可以使用[Reflection]从专家建议中提取一般知识，然后将其存储在记忆中。这种知识积累使得代理能够适应训练过程中未遇到的新任务。
- en: Seeking advice involves complex decision-making. The agent must estimate its
    own confidence in the current session, predict the potential value of the advice
    for future sessions, and consider the cost of human resources. The optimal trade-off
    is difficult to annotate manually but aligns well with our RL framework. Specifically,
    the present risk, future value, and cost of action can all be represented as RL
    rewards, allowing this skill to be trained as part of the policy model on an end-to-end
    basis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求建议涉及复杂的决策制定。代理必须评估自己在当前会话中的信心，预测建议对未来会话的潜在价值，并考虑人力资源的成本。最佳的权衡难以手动标注，但与我们的RL框架非常契合。具体而言，目前的风险、未来的价值和行动成本都可以表示为RL奖励，从而使这一技能能够作为策略模型的一部分进行端到端的训练。
- en: 3 The ProductQA dataset
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 ProductQA数据集
- en: We believe that product question answering in a real online shopping environment
    offers a comprehensive challenge for evaluating LLM agents. First, it demands
    expert knowledge about millions of products, including their technical specifications,
    usage in particular scenarios, and compatibility with other products. Second,
    answering some questions requires the use of tools, such as a product search tool.
    Third, the continuous emergence of new products necessitates the adaptability
    of the agent. This has motivated the creation of the ProductQA dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，在真实的在线购物环境中进行产品问题解答是评估LLM代理的一个全面挑战。首先，这要求对数百万种产品具备专业知识，包括它们的技术规格、在特定场景中的使用情况以及与其他产品的兼容性。其次，回答某些问题需要使用工具，例如产品搜索工具。第三，新产品的不断出现要求代理具备适应能力。这促使我们创建了ProductQA数据集。
- en: 'The ProductQA dataset consists of 26 QA tasks, each representing a distinct
    group of products within a specific category. Each group encompasses 17-20 products.
    We collected 20 groups for training and 6 for testing, allowing for assessing
    the agent’s adaptability to new tasks. We collected an average of 3,393 question-answer
    pairs for each product group. The questions within the same group are correlated,
    as knowledge from one answer may aid in addressing other questions. The dataset
    statistics are presented in Table [9](#A3.T9 "Table 9 ‣ Appendix C Tables ‣ AGILE:
    A Novel Framework of LLM Agents").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ProductQA 数据集包含 26 个 QA 任务，每个任务代表一个特定类别中的不同产品组。每个组包含 17-20 个产品。我们为训练收集了 20 个组，为测试收集了
    6 个组，以评估代理对新任务的适应能力。我们为每个产品组收集了平均 3,393 对问答。相同组内的问题是相关的，因为一个答案中的知识可能有助于解答其他问题。数据集统计信息见表
    [9](#A3.T9 "表 9 ‣ 附录 C 表格 ‣ AGILE：一种新型 LLM 代理框架")。
- en: 'The dataset is annotated by 20 professional annotators, each with at least
    a college degree, employed by a commercial data annotation company. We pay the
    company at market rates for professional annotation. See annotation guidelines
    in Appendix [E.2](#A5.SS2 "E.2 Annotation guidelines ‣ Appendix E Development
    of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents"). In addition,
    we release the code for the data pre-processing before human annotation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集由 20 名专业注释员标注，他们每人至少拥有大学学位，并由商业数据注释公司雇佣。我们按照市场价格支付给公司专业注释费用。有关注释指南，请参见附录
    [E.2](#A5.SS2 "E.2 注释指南 ‣ 附录 E 产品QA 数据集的开发 ‣ AGILE：一种新型 LLM 代理框架")。此外，我们还发布了人类注释之前的数据预处理代码。
- en: 3.1 Product collection
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 产品收集
- en: 'We gather products from the Amazon Review Data [[18](#bib.bib18)], which includes
    product metadata as well as reviews. We initially filter the Amazon Review Data
    to retain only popular products with at least 100 reviews, then cluster them by
    category tags. From these clusters, we select 26 based on the size of the cluster,
    each defined as a *product group*. Subsequently, we sample products from each
    product group. See Appendix [E.1](#A5.SS1 "E.1 Product collection ‣ Appendix E
    Development of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents")
    for more details about product group and product selection.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 Amazon Review Data [[18](#bib.bib18)] 收集了产品数据，包括产品元数据和评论。我们首先筛选 Amazon Review
    Data，只保留至少有 100 条评论的热门产品，然后按类别标签对它们进行聚类。从这些聚类中，我们根据聚类的规模选择了 26 个，每个定义为*产品组*。随后，我们从每个产品组中抽样产品。有关产品组和产品选择的更多细节，请参见附录
    [E.1](#A5.SS1 "E.1 产品收集 ‣ 附录 E 产品QA 数据集的开发 ‣ AGILE：一种新型 LLM 代理框架")。
- en: 'After the products are collected, annotators compile an information table for
    each product group. An example of such a table is presented in Table [2](#S3.T2
    "Table 2 ‣ 3.1 Product collection ‣ 3 The ProductQA dataset ‣ AGILE: A Novel Framework
    of LLM Agents"). To enhance the efficiency of the annotation process, we employ
    GPT-4 to extract as many product features as possible from the reviews. These
    features, together with the product metadata, are provided to the annotators for
    table creation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 产品收集完成后，注释员为每个产品组编制信息表。表 [2](#S3.T2 "表 2 ‣ 3.1 产品收集 ‣ 3 ProductQA 数据集 ‣ AGILE：一种新型
    LLM 代理框架") 展示了这样一个表的示例。为了提高注释过程的效率，我们利用 GPT-4 从评论中提取尽可能多的产品特征。这些特征连同产品元数据一起提供给注释员，以便创建表格。
- en: 'Table 2: An example of an information table for the headphones group.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：耳机组信息表的示例。
- en: '| Product ID | Title | Price | Brand | Headphone | Cable Type | Audio | Audio
    | … |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 产品 ID | 标题 | 价格 | 品牌 | 头戴式 | 线缆类型 | 音频 | 音频 | … |'
- en: '| Type | Transmission | Output Mode |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 传输方式 | 输出模式 |'
- en: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | over-ear | bluetooth
    | kleer | stereo | … |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| B00WSLZFTK | Sennheiser RS 170 | $11.03 | Sennheiser | 耳罩式 | 蓝牙 | kleer |
    立体声 | … |'
- en: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | earbud | 3.5mm Jack | analog | bass
    boost | … |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| B003AIL2HE | JVC HAEB75B | $9.99 | JVC | 耳塞式 | 3.5mm 插孔 | 模拟 | 增强低音 | … |'
- en: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | earbud | bluetooth | bluetooth
    | stereo | … |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| B01C22IJV0 | Phaiser BHS-530 | $6.04 | Phaiser | 耳塞式 | 蓝牙 | 蓝牙 | 立体声 | …
    |'
- en: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | over-ear | 3.5mm Jack | analog |
    stereo | … |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| B0013OWPV4 | JVC HARX700 | $2.00 | JVC | 耳罩式 | 3.5mm 插孔 | 模拟 | 立体声 | … |'
- en: '| … | … | … | … | … | … | … | … | … |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| … | … | … | … | … | … | … | … | … |'
- en: 'Table 3: Examples of Fact-QA, Search-QA and Reasoning-QA in ProductQA.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：ProductQA 中 Fact-QA、Search-QA 和 Reasoning-QA 的示例。
- en: '| Type | Question | Long Answer | Short Answer |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 问题 | 长答案 | 短答案 |'
- en: '| Fact-QA | What is the size of the neodymium driver used in the JVC HA-EB75
    headphones? | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | 13.5 mm |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Fact-QA | JVC HA-EB75 耳机使用的钕驱动单元的尺寸是多少？ | JVC HA-EB75 耳机每只耳机中包含一个 13.5 毫米的钕驱动单元，这有助于提高音质。
    | 13.5 mm |'
- en: '| Search-QA | I’m an audiophile always on the move, so I need my music non-stop.
    Tell me, what’s the headphone with the longest playtime you have, either on-ear
    or in-ear? | I found a product that matches your criteria. ‘ABCShopUSA Wireless
    Earbuds True’ with asin: B00LJT2EPK | B00LJT2EPK |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Search-QA | 我是一个总是在移动的音响爱好者，所以我需要不间断的音乐。告诉我，你们有什么耳机的播放时间最长，无论是耳罩式还是入耳式？ |
    我找到了一款符合你标准的产品。‘ABCShopUSA 无线耳塞 True’，ASIN: B00LJT2EPK | B00LJT2EPK |'
- en: '| Reasoning-QA | Will these headphones deliver comparable sound quality to
    wired alternatives when I am editing music? | No, these headphones may not suit
    your needs for music editing since they are wireless and can introduce audio compression
    and slight latency. Such issues can impact the precise listening experience crucial
    for professional audio editing tasks. | no |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Reasoning-QA | 在编辑音乐时，这些耳机的音质是否能与有线耳机相媲美？ | 不，这些耳机可能不适合你的音乐编辑需求，因为它们是无线的，可能会引入音频压缩和轻微延迟。这些问题会影响对专业音频编辑任务至关重要的精确聆听体验。
    | no |'
- en: 3.2 QA collection
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 QA 收集
- en: 'We identify three predominant types of questions in online shopping contexts:
    1) Fact-QA: questions concerning specific product details; 2) Search-QA: searches
    for product recommendations tailored to user preferences; 3) Reasoning-QA: questions
    whose answers require domain-specific reasoning, such as the implications of a
    product feature. Accordingly, we annotate question-answer pairs for these types.
    Each question is annotated with both a detailed paragraph-long answer and a concise
    short answer. The long answer should resemble a response from human customer service,
    while the short answer consists of a few words. We train the model to predict
    both answer types. The accuracy of the long answers is evaluated using GPT-4 (see
    Appendix [F](#A6 "Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM
    Agents") for the prompt); the short answers are assessed by exact match and are
    used for defining rewards for RL training.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在在线购物环境中识别出三种主要类型的问题：1) Fact-QA：涉及特定产品细节的问题；2) Search-QA：寻找符合用户偏好的产品推荐；3)
    Reasoning-QA：需要领域特定推理的问题，例如产品特征的影响。因此，我们对这些类型的问题-答案对进行注释。每个问题都附有详细的段落长答案和简明的短答案。长答案应类似于人类客服的回答，而短答案则由几词组成。我们训练模型预测这两种答案类型。长答案的准确性使用
    GPT-4 进行评估（请参见附录 [F](#A6 "附录 F 提示模板 ‣ AGILE：一种新颖的 LLM 代理框架") 以获取提示）；短答案则通过精确匹配进行评估，并用于定义
    RL 训练的奖励。
- en: Fact-QA
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Fact-QA
- en: Fact-QAs are constructed from product reviews. For each product, we provide
    GPT-4 with a batch of 30 reviews, prompting it to generate 20 questions and their
    corresponding answers before moving on to the next batch. We encourage GPT-4 to
    create diverse questions. The results are then given to annotators to refine and
    finalize the question-answer pairs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Fact-QAs 是由产品评论构建的。对于每个产品，我们向 GPT-4 提供一批 30 条评论，提示它生成 20 个问题及其对应的答案，然后再处理下一批。我们鼓励
    GPT-4 创造多样化的问题。结果随后交给注释员进行精炼和最终确定问题-答案对。
- en: Search-QA
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Search-QA
- en: Starting with an information table for a given product group, we generate random
    SQL expressions using a set of predefined rules. These expressions are then translated
    into natural language questions by GPT-4\. The answers are obtained by executing
    the SQL queries. Subsequently, human annotators thoroughly revise the QA pairs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定产品组的信息表开始，我们使用一组预定义规则生成随机 SQL 表达式。这些表达式随后由 GPT-4 转换成自然语言问题。通过执行 SQL 查询获得答案。随后，人工注释员会彻底修订
    QA 对。
- en: Reasoning-QA
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Reasoning-QA
- en: 'As the first step, we collect professional knowledge for each product group.
    To enhance efficiency, we utilize GPT-4 to generate candidate knowledge entries
    based on the technical specifications from the information table. These entries
    are then curated and refined by human annotators. Here is an example of a knowledge
    entry: *Motherboards with the ATX form factor are ideally suited for high-performance
    computing tasks and gaming, due to their ample expansion slots for graphics cards
    and other peripherals that boost computing capabilities.* Finally, annotators
    develop question-answer pairs from these knowledge entries.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为每个产品组收集专业知识。为了提高效率，我们利用 GPT-4 根据信息表中的技术规格生成候选知识条目。这些条目随后由人工标注员进行筛选和精炼。以下是一个知识条目的示例：*ATX
    形状因子的主板由于其大量的扩展槽，能够支持显卡和其他提升计算能力的外设，非常适合高性能计算任务和游戏。* 最后，标注员根据这些知识条目开发问题-答案对。
- en: 4 Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental setting
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Dataset
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We evaluate our agent on two complex QA tasks: ProductQA and MedMCQA. MedMCQA [[22](#bib.bib22)]
    is a dataset for multiple-choice QA. It consists of questions from medical school
    entrance examinations, with 182,822 / 4,183 / 6,150 instances in the train/dev/test
    splits. We report results on the dev set without checkpoint selection.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个复杂的 QA 任务上评估我们的代理：ProductQA 和 MedMCQA。MedMCQA [[22](#bib.bib22)] 是一个多项选择
    QA 数据集。它包含来自医学学校入学考试的问题，训练/开发/测试分割中分别有 182,822 / 4,183 / 6,150 个实例。我们在开发集上报告结果，不进行检查点选择。
- en: Agent definition
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理定义
- en: 'Our agent can invoke functions defined in Table [1](#S2.T1 "Table 1 ‣ 2.1 RL
    formulation of agent ‣ 2 Methods ‣ AGILE: A Novel Framework of LLM Agents"). In
    a typical workflow, the agent prompts the user for a new question at the session
    start, then retrieves memory to get relevant information. The memory can be initialized
    as empty (ProdcutQA) or with domain knowledge (QA pairs from MedMCQA training
    dataset). The agent can optionally use tools (e.g. product search in ProductQA)
    to get more information, then decide whether to predict an answer directly or
    seek human advice. If the agent seeks advice, it obtains a human answer (ground-truth
    answer in our setting). The agent then uses a reflection round to extract general
    knowledge from the human answer, writing both the human answer and the reflected
    knowledge to its memory. Finally, the agent submits an answer to the user. In
    our setting, submitting a correct answer incurs a $+1$.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的代理可以调用表 [1](#S2.T1 "表 1 ‣ 2.1 代理的 RL 表述 ‣ 2 方法 ‣ AGILE: 一种新颖的 LLM 代理框架")
    中定义的功能。在典型的工作流程中，代理在会话开始时提示用户提出新问题，然后检索记忆以获取相关信息。记忆可以初始化为空（ProductQA）或带有领域知识（来自
    MedMCQA 训练数据集的 QA 对）。代理可以选择使用工具（例如 ProductQA 中的产品搜索）获取更多信息，然后决定是直接预测答案还是寻求人工建议。如果代理寻求建议，它会获取一个人工答案（在我们的设置中为真实答案）。然后，代理使用反思回合从人工答案中提取一般知识，将人工答案和反思知识写入其记忆中。最后，代理向用户提交答案。在我们的设置中，提交正确答案将获得
    $+1$。'
- en: Training
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training consists of two stages. First, we construct trajectories from
    the training data and employ imitation learning to train the agent. Then we apply
    Algorithm [1](#alg1 "Algorithm 1 ‣ Appendix A Session-level optimization algorithm
    ‣ AGILE: A Novel Framework of LLM Agents") for further optimization by reinforcement
    learning. See Appendix [B](#A2 "Appendix B Implementation details of AGILE ‣ AGILE:
    A Novel Framework of LLM Agents") for implementation details. The agent’s LLM
    is initialized from Vicuna-13b-1.5 for ProductQA and Meerkat-7b for MedMCQA. We
    fine-tune the model for 2 epochs with a learning rate of 1e-5 and a batch size
    of 64\. We implement PPO for 1 epoch with a learning rate of 1e-6 and a batch
    size of 64\. The training runs on NVIDIA-H800.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '训练分为两个阶段。首先，我们从训练数据中构建轨迹，并使用模仿学习来训练代理。然后，我们应用算法 [1](#alg1 "算法 1 ‣ 附录 A 会话级优化算法
    ‣ AGILE: 一种新颖的 LLM 代理框架")通过强化学习进行进一步优化。有关实现细节，请参见附录 [B](#A2 "附录 B AGILE 实现细节 ‣
    AGILE: 一种新颖的 LLM 代理框架")。代理的 LLM 从 Vicuna-13b-1.5 初始化用于 ProductQA，Meerkat-7b 初始化用于
    MedMCQA。我们对模型进行 2 个周期的微调，学习率为 1e-5，批量大小为 64。我们用 PPO 进行 1 个周期的训练，学习率为 1e-6，批量大小为
    64。训练运行在 NVIDIA-H800 上。'
- en: Evaluation and baselines
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估和基准
- en: 'We report three metrics for the agent: (a) Advice rate: the rate of seeking
    human advice; (b) Accuracy: the rate of predicting the correct answer; (c) Total
    score: the average reward across all sessions, taking the advice rate and the
    accuracy both into account.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了代理的三个指标：（a）建议率：寻求人工建议的频率；（b）准确率：预测正确答案的频率；（c）总分：所有会话的平均奖励，综合考虑建议率和准确率。
- en: 'We compare our agent against two types of baselines: 1) Prompting GPT-3.5 (gpt-3.5-turbo-0301)
    and GPT-4 (gpt-4-0613) [[21](#bib.bib21)] to directly answer the question, without
    working in an agent manner, noted as gpt3.5-prompt and gpt4-prompt. 2) Prompting
    GPT-3.5 and GPT-4 within the AGILE framework, noted as agile-gpt3.5-prompt and
    agile-gpt4-prompt. We carefully designed prompts for all baselines and they are
    shown in Appendix [F](#A6 "Appendix F Prompt templates ‣ AGILE: A Novel Framework
    of LLM Agents").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将我们的代理与两种类型的基线进行比较：1) 直接提示 GPT-3.5 (gpt-3.5-turbo-0301) 和 GPT-4 (gpt-4-0613)
    [[21](#bib.bib21)] 回答问题，而不是以代理的方式工作，标记为 gpt3.5-prompt 和 gpt4-prompt。2) 在 AGILE
    框架内提示 GPT-3.5 和 GPT-4，标记为 agile-gpt3.5-prompt 和 agile-gpt4-prompt。我们为所有基线精心设计了提示，详见附录
    [F](#A6 "附录 F 提示模板 ‣ AGILE: 一种新型 LLM 代理框架")。'
- en: 'Table 4: Results on ProductQA. Here, X-prompt represents directly prompting
    model X; agile-X-Y incorporates model X within the AGILE framework, while Y represents
    prompting or PPO training. We report results on short and long answers, respectively.
    The seeking advice cost is $c=0.3$. Results are average over six test tasks. See
    Table [10](#A3.T10 "Table 10 ‣ Appendix C Tables ‣ AGILE: A Novel Framework of
    LLM Agents") for individual product group performance.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表4：产品QA的结果。这里，X-prompt 代表直接提示模型 X；agile-X-Y 将模型 X 纳入 AGILE 框架，而 Y 代表提示或 PPO
    训练。我们分别报告了短答案和长答案的结果。寻求建议的成本为 $c=0.3$。结果是对六个测试任务的平均值。有关单独产品组的表现，请参见表 [10](#A3.T10
    "表 10 ‣ 附录 C 表 ‣ AGILE: 一种新型 LLM 代理框架")。'
- en: '| Method | Advice Rate  $\downarrow$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ |'
- en: '| Short | Long | Short | Long |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 短 | 长 | 短 | 长 |'
- en: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| gpt3.5-prompt | - | 0.202 | 0.322 | - | - |'
- en: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-prompt | - | 0.464 | 0.571 | - | - |'
- en: '| agile-vicuna-13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| agile-vicuna-13b-prompt | 0.174 | 0.174 | 0.294 | 0.122 | 0.242 |'
- en: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt3.5-prompt | 0.323 | 0.508 | 0.644 | 0.411 | 0.547 |'
- en: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.208 | 0.780 | 0.809 | 0.718 | 0.747 |'
- en: '| agile-vic7b-ppo (ours) | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic7b-ppo (我们的) | 0.179 | 0.818 | 0.800 | 0.764 | 0.746 |'
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo (我们的) | 0.233 | 0.854 | 0.854 | 0.784 | 0.784 |'
- en: 'Table 5: Ablation studies for disabling reflection, memory, seeking advice,
    tool use, or RL training. Here, non-adapt-advice means that seeking advice is
    invoked for the first $K$ equals to the number of [SeekAdvice] performed by agile-vic13b-ppo.
    See Table [11](#A3.T11 "Table 11 ‣ Appendix C Tables ‣ AGILE: A Novel Framework
    of LLM Agents") for ablation results on individual product groups.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '表5：禁用反射、记忆、寻求建议、工具使用或强化学习训练的消融研究。这里，non-adapt-advice 意味着在第一次调用寻求建议时，$K$ 等于
    agile-vic13b-ppo 执行的[SeekAdvice]次数。有关单独产品组的消融结果，请参见表 [11](#A3.T11 "表 11 ‣ 附录 C
    表 ‣ AGILE: 一种新型 LLM 代理框架")。'
- en: '| Method | Advice Rate $\downarrow$ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率 $\downarrow$ |'
- en: '| w/o Reflection | 0.270 | 0.852 | 0.771(-1.7%) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 无反射 | 0.270 | 0.852 | 0.771(-1.7%) |'
- en: '| w/o Memory | 0.407 | 0.876 | 0.754(-4.0%) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 无记忆 | 0.407 | 0.876 | 0.754(-4.0%) |'
- en: '| w/o Advice | 0.000 | 0.747 | 0.747(-5.0%) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 无建议 | 0.000 | 0.747 | 0.747(-5.0%) |'
- en: '| non-adapt-advice | 0.233 | 0.812 | 0.742(-5.7%) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| non-adapt-advice | 0.233 | 0.812 | 0.742(-5.7%) |'
- en: '| w/o Tool-Use | 0.492 | 0.864 | 0.717(-9.3%) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 无工具使用 | 0.492 | 0.864 | 0.717(-9.3%) |'
- en: '| w/o RL | 0.256 | 0.843 | 0.766(-2.3%) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 无强化学习 | 0.256 | 0.843 | 0.766(-2.3%) |'
- en: '| agile-vic13b-ppo (ours) | 0.233 | 0.854 | 0.784 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| agile-vic13b-ppo (我们的) | 0.233 | 0.854 | 0.784 |'
- en: 4.2 Results on ProductQA
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 产品QA的结果
- en: 'As Table [4](#S4.T4 "Table 4 ‣ Evaluation and baselines ‣ 4.1 Experimental
    setting ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") shows, our
    AGILE agent outperforms all baselines on ProductQA. Notably, the average total
    score of agile-vic13b-ppo across six test groups shows a relative improvement
    of 9.2% in short answers and 5.0% in long answers to agile-gpt4-prompt where the
    seeking advice cost is added into the prompt. Concretely, agile-vic13b-ppo uses
    a comparable number of seeking advice to achieve 7.4% higher accuracy in short
    answers than agile-gpt4-prompt, and as Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Results
    on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") shows,
    this accuracy improvement is consistent across the whole trajectory. Our agile-vic7b-ppo
    agent also outperforms agile-gpt4-prompt in average total scores. Note that the
    GPT-4 agent knows the seeking advice cost from its prompt (see Figure [6](#A6.F6
    "Figure 6 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE:
    A Novel Framework of LLM Agents")).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[4](#S4.T4 "表 4 ‣ 评估和基线 ‣ 4.1 实验设置 ‣ 4 实验 ‣ AGILE: 一种新颖的 LLM 代理框架")所示，我们的AGILE代理在ProductQA上的表现优于所有基线。值得注意的是，agile-vic13b-ppo在六个测试组中的平均总分相比于将寻求建议成本加入提示中的agile-gpt4-prompt，在短答案中有9.2%的相对提升，在长答案中有5.0%的提升。具体来说，agile-vic13b-ppo使用了类似数量的寻求建议，在短答案中比agile-gpt4-prompt提高了7.4%的准确率，如图[3](#S4.F3
    "图 3 ‣ 4.2 ProductQA 结果 ‣ 4 实验 ‣ AGILE: 一种新颖的 LLM 代理框架")所示，这种准确率提升在整个过程中是一致的。我们的agile-vic7b-ppo代理在平均总分上也优于agile-gpt4-prompt。请注意，GPT-4代理从其提示中知道寻求建议成本（参见图[6](#A6.F6
    "图 6 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: 一种新颖的 LLM 代理框架")）。'
- en: 'We investigate the impact of varying the seeking advice cost. As shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Results on ProductQA ‣ 4 Experiments ‣ AGILE: A Novel Framework
    of LLM Agents"), when the cost decreases, both the advice rate and the accuracy
    increase, indicating greater utilization of human assistance. Specifically, with
    a high cost of 0.5, the advice rate is close to 0, and at a low cost of 0.1, the
    accuracy is close to 1\. This result demonstrates that by adjusting the cost and
    through RL training, we can effectively manage the trade-off between accuracy
    and human cost. For instance, the agent can achieve 94.1% accuracy on the Motherboards
    task with a seeking advice cost of $c=0.1$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '我们调查了不同寻求建议成本的影响。如图[3](#S4.F3 "图 3 ‣ 4.2 ProductQA 结果 ‣ 4 实验 ‣ AGILE: 一种新颖的
    LLM 代理框架")所示，当成本降低时，建议率和准确率均增加，表明人力资源的利用度更高。具体来说，当成本为0.5时，建议率接近0，而在成本为0.1时，准确率接近1\.
    这一结果表明，通过调整成本和RL训练，我们可以有效管理准确率和人力成本之间的权衡。例如，代理在寻求建议成本为$c=0.1$时可以在主板任务上实现94.1%的准确率。'
- en: '![Refer to caption](img/11ebaea0130ede1f05fbfb7f80981544.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/11ebaea0130ede1f05fbfb7f80981544.png)'
- en: 'Figure 2: Accuracy and advice rate over the following 200 sessions on ProductQA
    ($c=0.3$).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 在ProductQA上，接下来的200次会话中的准确率和建议率（$c=0.3$）。'
- en: '![Refer to caption](img/c8876aba295eb5c94a265c610e02c322.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c8876aba295eb5c94a265c610e02c322.png)'
- en: 'Figure 3: Advice rate, accuracy along with seeking advice cost $c$ on ProductQA.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 在ProductQA上，建议率、准确率与寻求建议成本$c$的关系。'
- en: Ablation study
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 去除研究
- en: 'We present ablation studies in Table [5](#S4.T5 "Table 5 ‣ Evaluation and baselines
    ‣ 4.1 Experimental setting ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents")
    to assess the contributions of individual agent components and the effects of
    RL training. The table indicates that disabling the option to seek advice (w/o
    Advice) leads to a 10.7% drop in accuracy and a 5.0% relative reduction in total
    score. Forcing the agent to seek advice at the initial part of the trajectory
    (Non-adapt Advice) causes a 4.2% decrease in accuracy, underscoring the value
    of adaptive decision-making. Removing reflection and memory capabilities (w/o
    Memory and w/o Reflection) both increase the frequency of advice-seeking, as the
    agent struggles to accumulate or leverage valuable knowledge, consequently decreasing
    the total score. Furthermore, disabling tool use (w/o Tool-Use) causes a substantial
    25.9% increase in the advice-seeking rate because the agent’s capabilities are
    diminished, making it more reliant on external advice. Lastly, RL training improves
    the relative total score by 2.3%, lowers the advice-seeking rate, and boosts accuracy,
    demonstrating that RL training effectively optimizes the policy.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[5](#S4.T5 "Table 5 ‣ Evaluation and baselines ‣ 4.1 Experimental setting
    ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents")中展示了消融研究，以评估各个代理组件的贡献和RL训练的效果。表明禁用寻求建议选项（w/o
    Advice）会导致准确率下降10.7%和总分相对减少5.0%。在轨迹的初始阶段强制代理寻求建议（Non-adapt Advice）会导致准确率下降4.2%，突显了自适应决策的重要性。移除反射和记忆能力（w/o
    Memory和w/o Reflection）都会增加寻求建议的频率，因为代理难以积累或利用有价值的知识，从而降低总分。此外，禁用工具使用（w/o Tool-Use）会导致建议率显著增加25.9%，因为代理的能力被削弱，使其更加依赖外部建议。最后，RL训练将相对总分提高了2.3%，降低了建议率，并提高了准确率，表明RL训练有效地优化了策略。'
- en: 'In Appendix [D](#A4 "Appendix D Case study ‣ AGILE: A Novel Framework of LLM
    Agents"), we present detailed examples of agile-vic13b-ppo illustrating how memory,
    tools, seeking advice, and reflection enhance the agent workflow.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '在附录[D](#A4 "Appendix D Case study ‣ AGILE: A Novel Framework of LLM Agents")中，我们展示了详细的agile-vic13b-ppo示例，说明了记忆、工具、寻求建议和反射如何增强代理工作流程。'
- en: '![Refer to caption](img/318201b08ea44dac1475a1aafffcb6ad.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/318201b08ea44dac1475a1aafffcb6ad.png)'
- en: 'Figure 4: Advice rate over the following 200 sessions on ProductQA ($c=0.3$).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在ProductQA上的建议率（$c=0.3$）。
- en: Trend of advice rate
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 建议率的趋势
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ Ablation study ‣ 4.2 Results on ProductQA ‣ 4
    Experiments ‣ AGILE: A Novel Framework of LLM Agents") demonstrates a consistent
    decrease in the advice rate of agile-vic13b-ppo as more sessions are added to
    the trajectory. This decline can be attributed to the agent progressively accumulating
    knowledge and becoming more independent. Additionally, the figure illustrates
    that disabling RL training or reflection leads to a significant increase in the
    advice rate, underscoring the importance of RL training and reflection in reducing
    human costs.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S4.F4 "Figure 4 ‣ Ablation study ‣ 4.2 Results on ProductQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents")展示了随着更多会话被添加到轨迹中，agile-vic13b-ppo的建议率持续下降。这一下降可以归因于代理逐渐积累知识并变得更加独立。此外，图中还显示禁用RL训练或反射会导致建议率显著增加，强调了RL训练和反射在减少人力成本方面的重要性。'
- en: 4.3 Results on MedMCQA
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 MedMCQA上的结果
- en: 'Table 6: Results on the MedMCQA dev dataset. X-prompt represents directly prompting
    the model X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting, ablation studies or standard PPO training. The seeking
    advice cost is $c=0.4$.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在MedMCQA开发数据集上的结果。X-prompt代表直接提示模型X；agile-X-Y表示在AGILE框架内使用模型X，而Y代表提示、消融研究或标准PPO训练。寻求建议的成本为$c=0.4$。
- en: '| Method | Advice Rate $\downarrow$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 建议率$\downarrow$ |'
- en: '| Meerkat-7b-prompt | - | 0.534 | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Meerkat-7b-prompt | - | 0.534 | - |'
- en: '| gpt3.5-prompt[[19](#bib.bib19)] | - | 0.501 | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| gpt3.5-prompt[[19](#bib.bib19)] | - | 0.501 | - |'
- en: '| gpt4-prompt[[19](#bib.bib19)] | - | 0.695 | - |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-prompt[[19](#bib.bib19)] | - | 0.695 | - |'
- en: '| gpt4-Medprompt[[20](#bib.bib20)] | - | 0.791 | - |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| gpt4-Medprompt[[20](#bib.bib20)] | - | 0.791 | - |'
- en: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt3.5-prompt | 0.194 | 0.697 | 0.619 |'
- en: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| agile-gpt4-prompt | 0.421 | 0.884 | 0.721 |'
- en: '| agile-mek7b-w/o Reflection | 0.368 | 0.790 | 0.643 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o Reflection | 0.368 | 0.790 | 0.643 |'
- en: '| agile-mek7b-w/o Memory | 0.506 | 0.741 | 0.539 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o Memory | 0.506 | 0.741 | 0.539 |'
- en: '| agile-mek7b-w/o Advice | 0.000 | 0.620 | 0.620 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o Advice | 0.000 | 0.620 | 0.620 |'
- en: '| agile-mek7b-w/o RL | 0.322 | 0.837 | 0.708 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-w/o RL | 0.322 | 0.837 | 0.708 |'
- en: '| agile-mek7b-ppo (ours) | 0.316 | 0.852 | 0.726 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| agile-mek7b-ppo (我们的) | 0.316 | 0.852 | 0.726 |'
- en: 'Our agile-mek7b-ppo agent, based on the smaller Meerkat-7b [[13](#bib.bib13)]
    model, reaches an accuracy of 85.2% with an advice rate of 31.6%. As Table [6](#S4.T6
    "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of
    LLM Agents") shows, this represents a 31.8% accuracy increase over the base model
    Meerkat-7b-prompt and a 6.1% increase over the state-of-the-art gpt4-Medprompt
    [[20](#bib.bib20)]. Table [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents") also shows that the ability to seek
    advice alone contributes a 23.2% accuracy gain, meaning that each instance of
    seeking advice corrects an average of 0.73 prediction errors. This indicates that
    PPO training effectively helps the agent identify its mistakes. For a fair comparison,
    we also evaluate agile-gpt3.5-prompt and agile-gpt4-prompt, which incorporate
    GPT-3.5 and GPT-4 within our AGILE framework. These agents also leverage advice-seeking
    to enhance accuracy, but without RL training, their total scores are lower than
    agile-mek7b-ppo. Finally, through ablation studies, we confirmed the essential
    roles of memory, reflection, seeking advice, and RL training in achieving high
    performance. Removing these components leads to a significant drop in total scores,
    detailed in Table [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们基于较小的 Meerkat-7b [[13](#bib.bib13)] 模型的 agile-mek7b-ppo 代理，达到了 85.2% 的准确率，建议率为
    31.6%。如表 [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE:
    A Novel Framework of LLM Agents") 所示，这比基础模型 Meerkat-7b-prompt 提高了 31.8% 的准确率，比最先进的
    gpt4-Medprompt [[20](#bib.bib20)] 提高了 6.1%。表 [6](#S4.T6 "Table 6 ‣ 4.3 Results
    on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel Framework of LLM Agents") 还显示，单独寻求建议的能力贡献了
    23.2% 的准确率提升，这意味着每次寻求建议可以纠正平均 0.73 个预测错误。这表明 PPO 训练有效帮助代理识别其错误。为了公平比较，我们还评估了 agile-gpt3.5-prompt
    和 agile-gpt4-prompt，这些代理在我们的 AGILE 框架中集成了 GPT-3.5 和 GPT-4。这些代理也利用寻求建议来提高准确率，但由于没有
    RL 训练，它们的总分低于 agile-mek7b-ppo。最后，通过消融研究，我们确认了记忆、反射、寻求建议和 RL 训练在实现高性能中的关键作用。去除这些组件会导致总分显著下降，详细信息见表
    [6](#S4.T6 "Table 6 ‣ 4.3 Results on MedMCQA ‣ 4 Experiments ‣ AGILE: A Novel
    Framework of LLM Agents")。'
- en: 5 Related work
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 'Table 7: Related work on LLM agents. AGILE stands out as the pioneering work
    that trains the entire agent using reinforcement learning, incorporating proactive
    human advice-seeking.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：关于 LLM 代理的相关工作。AGILE 作为开创性工作，通过强化学习训练整个代理，并融入了主动寻求建议的机制，表现突出。
- en: '| LLM Agent | LLM | SFT | RL | Memory | Tools | Reflection |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| LLM 代理 | LLM | SFT | RL | 记忆 | 工具 | 反射 |'
- en: '&#124; Proactive &#124;'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 主动 &#124;'
- en: '&#124; Human-agent &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类-代理 &#124;'
- en: '&#124; Interaction &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动 &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| WebGPT [[17](#bib.bib17)] | GPT-3 | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| WebGPT [[17](#bib.bib17)] | GPT-3 | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ |'
- en: '| ReAct [[44](#bib.bib44)] | PaLM-540b | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [[44](#bib.bib44)] | PaLM-540b | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Reflexion [[34](#bib.bib34)] | GPT-3/3.5/4 | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [[34](#bib.bib34)] | GPT-3/3.5/4 | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| ChatDev [[25](#bib.bib25)] | ChatGPT-turbo-16k | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ChatDev [[25](#bib.bib25)] | ChatGPT-turbo-16k | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| RAP [[11](#bib.bib11)] | LLaMA-33b | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| RAP [[11](#bib.bib11)] | LLaMA-33b | ✗ | ✗ | ✓ | ✗ | ✗ | ✗ |'
- en: '| AutoAct [[27](#bib.bib27)] | LLaMA2-70b | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| AutoAct [[27](#bib.bib27)] | LLaMA2-70b | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| TPTU [[30](#bib.bib30)] | ChatGPT/InternLM | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| TPTU [[30](#bib.bib30)] | ChatGPT/InternLM | ✗ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| AGILE (Ours) | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| AGILE (我们的) | Vicuna-13b/Meerkat-7b | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: LLM agents
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 代理
- en: 'Large Language Models (LLMs) have demonstrated substantial capabilities in
    following instructions, reasoning, and planning. Numerous research works, as shown
    in Table [7](#S5.T7 "Table 7 ‣ 5 Related work ‣ AGILE: A Novel Framework of LLM
    Agents"), utilizing prompt engineering, have constructed remarkable LLM agents
    capable of autonomously resolving complex tasks across various environments [[23](#bib.bib23),
    [38](#bib.bib38), [1](#bib.bib1), [25](#bib.bib25), [3](#bib.bib3)]. Furthermore,
    extensive works identify key components in the design of LLM agents, including
    planning [[17](#bib.bib17), [33](#bib.bib33), [7](#bib.bib7), [27](#bib.bib27),
    [44](#bib.bib44), [30](#bib.bib30)], tool-use [[24](#bib.bib24), [41](#bib.bib41),
    [31](#bib.bib31)], and reflection [[34](#bib.bib34), [16](#bib.bib16)]. In this
    work, we enable the agent to utilize memory, tools and proactively learn from
    the environment. We then formulate the entire process within an RL framework so
    that all agent skills can be jointly optimized end-to-end.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型 (LLMs) 在跟随指令、推理和规划方面展现了显著的能力。许多研究工作，如表 [7](#S5.T7 "Table 7 ‣ 5 Related
    work ‣ AGILE: A Novel Framework of LLM Agents") 所示，利用提示工程，构建了能够自主解决各种环境中复杂任务的出色
    LLM 代理 [[23](#bib.bib23), [38](#bib.bib38), [1](#bib.bib1), [25](#bib.bib25),
    [3](#bib.bib3)]。此外，大量工作确定了 LLM 代理设计中的关键组件，包括规划 [[17](#bib.bib17), [33](#bib.bib33),
    [7](#bib.bib7), [27](#bib.bib27), [44](#bib.bib44), [30](#bib.bib30)]、工具使用 [[24](#bib.bib24),
    [41](#bib.bib41), [31](#bib.bib31)] 和反思 [[34](#bib.bib34), [16](#bib.bib16)]。在这项工作中，我们使代理能够利用记忆、工具并主动从环境中学习。然后，我们将整个过程形式化为
    RL 框架，以便所有代理技能可以端到端地共同优化。'
- en: Human-agent interaction
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人机交互
- en: Although LLMs face practical challenges, such as hallucination [[46](#bib.bib46)]
    and a lack of long-tail knowledge [[12](#bib.bib12)], consulting human experts
    can help mitigate these issues. Several studies have incorporated human experts
    into agent workflows. For instance, [[45](#bib.bib45)] establishes a static pipeline
    in which an agent passively accepts advice from a superior LLM. [[4](#bib.bib4),
    [26](#bib.bib26)] train models to proactively ask questions via behavior cloning.
    However, these methods ignore the fact that the decision to seek advice must be
    based on the LLM’s own knowledge and capabilities [[47](#bib.bib47), [14](#bib.bib14),
    [10](#bib.bib10)]. [[29](#bib.bib29)] use a calibrated version of an LLM’s token
    probabilities as a confidence measure, yet token probabilities tend to be overconfident [[40](#bib.bib40)],
    and existing calibration methods don’t generalize well to our agent setting when
    the LLM makes multiple decisions in sequence.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLM 面临实际挑战，例如幻觉 [[46](#bib.bib46)] 和缺乏长尾知识 [[12](#bib.bib12)]，但咨询人类专家可以帮助缓解这些问题。几项研究将人类专家纳入了代理工作流程。例如，[[45](#bib.bib45)]
    建立了一个静态管道，其中代理被动接受来自上级 LLM 的建议。[[4](#bib.bib4), [26](#bib.bib26)] 通过行为克隆训练模型主动提问。然而，这些方法忽略了寻求建议的决定必须基于
    LLM 自身的知识和能力 [[47](#bib.bib47), [14](#bib.bib14), [10](#bib.bib10)]。[[29](#bib.bib29)]
    使用了 LLM 的标记概率的校准版本作为置信度度量，但标记概率往往过于自信 [[40](#bib.bib40)]，而现有的校准方法在 LLM 连续做出多个决策时在我们的代理设置中效果不佳。
- en: LLM agent benchmarks
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 代理基准
- en: 'Several benchmarks have been designed to assess the capabilities of agents.
    For instance, the Webshop [[43](#bib.bib43)] and Mind2Web [[6](#bib.bib6)] datasets
    evaluate agents’ tool usage and planning abilities within a web environment. HotPotQA [[42](#bib.bib42)]
    and TriviaQA [[9](#bib.bib9)] focus on agents’ reasoning and tool usage for question
    answering. ALFWorld [[35](#bib.bib35)] examines planning and navigation skills,
    while ScienceWorld [[37](#bib.bib37)] provides an interactive text-based environment
    to evaluate agents’ scientific aptitude. As illustrated in Table [8](#S5.T8 "Table
    8 ‣ LLM agent benchmarks ‣ 5 Related work ‣ AGILE: A Novel Framework of LLM Agents"),
    despite these existing benchmarks, none comprehensively addresses all the core
    challenges of real-world agent applications, such as handling long-tail knowledge,
    human-agent interaction, long-term memory usage, tool usage, self-evaluation,
    and reflection. This motivated us to develop ProductQA.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '已设计多个基准来评估代理的能力。例如，Webshop [[43](#bib.bib43)] 和 Mind2Web [[6](#bib.bib6)] 数据集评估代理在网络环境中的工具使用和规划能力。HotPotQA
    [[42](#bib.bib42)] 和 TriviaQA [[9](#bib.bib9)] 关注代理在回答问题时的推理和工具使用。ALFWorld [[35](#bib.bib35)]
    检验规划和导航技能，而 ScienceWorld [[37](#bib.bib37)] 提供了一个互动的基于文本的环境来评估代理的科学能力。如表[8](#S5.T8
    "Table 8 ‣ LLM agent benchmarks ‣ 5 Related work ‣ AGILE: A Novel Framework of
    LLM Agents")所示，尽管已有这些基准，但没有一个全面解决现实世界代理应用的所有核心挑战，例如处理长尾知识、人机交互、长期记忆使用、工具使用、自我评估和反思。这促使我们开发了
    ProductQA。'
- en: 'Table 8: Benchmarks for evaluating LLM agents. ProductQA features long trajectories,
    tool use, long-term knowledge accumulation, and cross-task capabilities.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 用于评估 LLM 代理的基准。ProductQA 具有长轨迹、工具使用、长期知识积累和跨任务能力。'
- en: '| Datasets | Type | Fields | Size |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 | 领域 | 大小 |'
- en: '&#124; Long &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长期 &#124;'
- en: '&#124; Trajectory &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '|'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Tool &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具 &#124;'
- en: '&#124; Usage &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用 &#124;'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Long-term &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长期 &#124;'
- en: '&#124; Knowledge &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识 &#124;'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Cross &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 跨领域 &#124;'
- en: '&#124; Task &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务 &#124;'
- en: '|'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Webshop [[43](#bib.bib43)] | Simulator | Web | 12,087 | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Webshop [[43](#bib.bib43)] | 模拟器 | 网络 | 12,087 | ✗ | ✗ | ✗ | ✗ |'
- en: '| Mind2Web [[6](#bib.bib6)] | Simulator | Web | 2,350 | ✗ | ✗ | ✗ | ✓ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Mind2Web [[6](#bib.bib6)] | 模拟器 | 网络 | 2,350 | ✗ | ✗ | ✗ | ✓ |'
- en: '| ALFWorld [[35](#bib.bib35)] | Simulator | Navigation | 3,827 | ✗ | ✗ | ✗
    | ✓ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| ALFWorld [[35](#bib.bib35)] | 模拟器 | 导航 | 3,827 | ✗ | ✗ | ✗ | ✓ |'
- en: '| ScienceWorld [[37](#bib.bib37)] | Simulator | Science | 7,207 | ✗ | ✗ | ✗
    | ✗ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ScienceWorld [[37](#bib.bib37)] | 模拟器 | 科学 | 7,207 | ✗ | ✗ | ✗ | ✗ |'
- en: '| HotPotQA [[42](#bib.bib42)] | QA | Wikipedia | 112,779 | ✗ | ✓ | ✗ | ✗ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| HotPotQA [[42](#bib.bib42)] | QA | 维基百科 | 112,779 | ✗ | ✓ | ✗ | ✗ |'
- en: '| TriviaQA [[9](#bib.bib9)] | QA | Web | 95,956 | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| TriviaQA [[9](#bib.bib9)] | QA | 网络 | 95,956 | ✗ | ✓ | ✓ | ✗ |'
- en: '| ProductQA (ours) | QA | E-commerce | 88,229 | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ProductQA（我们） | QA | 电子商务 | 88,229 | ✓ | ✓ | ✓ | ✓ |'
- en: 6 Conclusion and future work
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this work, we introduce a novel framework of LLM agents, called AGILE. First,
    the whole system of AGILE is trained end-to-end by reinforcement learning. Second,
    AGILE has the ability of seeking advice from external human experts. In addition,
    we develop a challenging dataset of complex QA, ProductQA, for comprehensive evaluation
    of an agent’s capabilities. Extensive experiments demonstrate that within our
    framework, an agent based on a smaller model after RL training can outperform
    GPT-4.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入了一个新颖的 LLM 代理框架，称为 AGILE。首先，AGILE 的整个系统通过强化学习进行端到端训练。其次，AGILE 具备从外部人类专家那里寻求建议的能力。此外，我们开发了一个具有挑战性的复杂
    QA 数据集 ProductQA，用于全面评估代理的能力。广泛的实验表明，在我们的框架下，经过 RL 训练的基于较小模型的代理可以超越 GPT-4。
- en: 'AGILE is a general agent framework and we can certainly consider multiple extensions
    of it. An agent can be equipped with more tools, such as multimodal perception,
    manipulations in physical environments, logical reasoning, among others. We posit
    that AGILE’s activities can be categorized into two distinct types: utilizing
    its LLM alone, and integrating the LLM with other tools. These two approaches
    conceptually align with the human cognitive processes known as System 1 and System
    2\. Furthermore, AGILE’s memory serves as a repository for the accumulation of
    experiences and knowledge, which is crucial for self-improvement. Consequently,
    AGILE offers an architecture for an very powerful agent that has the potential
    to attain human-level intelligence.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: AGILE 是一个通用的代理框架，我们当然可以考虑它的多个扩展。一个代理可以配备更多工具，例如多模态感知、物理环境中的操作、逻辑推理等。我们认为 AGILE
    的活动可以分为两种不同的类型：单独使用其 LLM 和将 LLM 与其他工具整合。这两种方法在概念上与被称为系统1和系统2的人类认知过程相一致。此外，AGILE
    的记忆充当经验和知识积累的储存库，这对自我改进至关重要。因此，AGILE 提供了一种非常强大的代理架构，有潜力达到人类级别的智能。
- en: AGILE also includes interactions between the agent and external human experts.
    The framework can be extended to allow interactions with humans or machine agents
    in various roles such as students or teachers, and in different formats such as
    debates or coordination. Furthermore, AGILE can be employed in multi-agent systems.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: AGILE 还包括代理与外部人类专家之间的互动。该框架可以扩展以允许与各种角色如学生或教师的人工或机器代理进行互动，以及以不同格式如辩论或协调进行互动。此外，AGILE
    可以应用于多代理系统中。
- en: References
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry
    tools. arXiv preprint arXiv:2304.05376, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White,
    和 Philippe Schwaller. Chemcrow: 使用化学工具增强大语言模型。arXiv 预印本 arXiv:2304.05376，2023年。'
- en: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020年。'
- en: '[3] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, and Yemin Shi. Autoagents: A framework for automatic agent generation.
    arXiv preprint arXiv:2309.17288, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Börje F Karlsson,
    Jie Fu, 和 Yemin Shi. Autoagents: 自动代理生成框架。arXiv 预印本 arXiv:2309.17288，2023年。'
- en: '[4] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, and Jianyu Chen. Asking
    before action: Gather information in embodied decision making with language models.
    arXiv preprint arXiv:2305.15695, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Xiaoyu Chen, Shenao Zhang, Pushi Zhang, Li Zhao, 和 Jianyu Chen. 行动前询问：利用语言模型在具身决策中收集信息。arXiv
    预印本 arXiv:2305.15695，2023年。'
- en: '[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing. Vicuna: 一款以 90%* ChatGPT 质量令人印象深刻的开源聊天机器人，2023年3月。'
- en: '[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang,
    Huan Sun, 和 Yu Su. Mind2web: 面向网络的通用代理。神经信息处理系统进展，36，2024年。'
- en: '[7] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    and Zhiting Hu. Reasoning with language model is planning with world model. arXiv
    preprint arXiv:2305.14992, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
    和 Zhiting Hu. 使用语言模型进行推理即使用世界模型进行规划。arXiv 预印本 arXiv:2305.14992，2023年。'
- en: '[8] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
    Szolovits. What disease does this patient have? a large-scale open domain question
    answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, 和 Peter
    Szolovits. 这个病人得了什么病？一个来自医学考试的大规模开放领域问答数据集。应用科学，11(14):6421，2021年。'
- en: '[9] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa:
    A large scale distantly supervised challenge dataset for reading comprehension.
    arXiv preprint arXiv:1705.03551, 2017.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Mandar Joshi, Eunsol Choi, Daniel S Weld 和 Luke Zettlemoyer。Triviaqa：一个大规模远程监督的阅读理解挑战数据集。arXiv
    预印本 arXiv:1705.03551, 2017。'
- en: '[10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson,
    et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221,
    2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson
    等人。语言模型（大多数情况下）知道它们知道什么。arXiv 预印本 arXiv:2207.05221, 2022。'
- en: '[11] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented
    planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610,
    2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick 和 Yang You。Rap：通过上下文记忆的检索增强计划，适用于多模态
    LLM 代理。arXiv 预印本 arXiv:2402.03610, 2024。'
- en: '[12] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.
    Large language models struggle to learn long-tail knowledge. In International
    Conference on Machine Learning, pages 15696–15707\. PMLR, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace 和 Colin Raffel。大型语言模型难以学习长尾知识。在国际机器学习大会上，页码
    15696–15707。PMLR, 2023。'
- en: '[13] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, and Jaewoo Kang. Small language models
    learn enhanced reasoning skills from medical textbooks. arXiv preprint arXiv:2404.00376,
    2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee,
    Chanwoong Yoon, Jiwoong Sohn, Donghee Choi 和 Jaewoo Kang。小型语言模型从医学教科书中学习增强的推理技能。arXiv
    预印本 arXiv:2404.00376, 2024。'
- en: '[14] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty:
    Linguistic invariances for uncertainty estimation in natural language generation.
    In The Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Lorenz Kuhn, Yarin Gal 和 Sebastian Farquhar。语义不确定性：自然语言生成中的不确定性估计的语言不变性。在第十一届国际学习表示会议上,
    2022。'
- en: '[15] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar 等人。语言模型的整体评估。arXiv
    预印本 arXiv:2211.09110, 2022。'
- en: '[16] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine:
    Iterative refinement with self-feedback. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
    Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang 等人。Self-refine：自我反馈的迭代优化。神经信息处理系统进展,
    36, 2024。'
- en: '[17] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.
    Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint
    arXiv:2112.09332, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders 等人。Webgpt：利用浏览器辅助的问答系统和人工反馈。arXiv
    预印本 arXiv:2112.09332, 2021。'
- en: '[18] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations
    using distantly-labeled reviews and fine-grained aspects. In Proceedings of the
    2019 conference on empirical methods in natural language processing and the 9th
    international joint conference on natural language processing (EMNLP-IJCNLP),
    pages 188–197, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jianmo Ni, Jiacheng Li 和 Julian McAuley。利用远程标记的评论和细粒度方面来证明推荐的合理性。在 2019
    年自然语言处理经验方法会议及第九届国际联合自然语言处理会议 (EMNLP-IJCNLP) 论文集中，页码 188–197, 2019。'
- en: '[19] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric
    Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375,
    2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan 和 Eric
    Horvitz。GPT-4 在医学挑战问题上的能力。arXiv 预印本 arXiv:2303.13375, 2023。'
- en: '[20] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo
    Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer
    McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White,
    and Eric Horvitz. Can generalist foundation models outcompete special-purpose
    tuning? case study in medicine, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 哈尔沙·诺里、李银达、张胜、迪安·卡里甘、理查德·埃德加、尼科洛·富西、尼古拉斯·金、乔纳森·拉尔森、李元志、刘伟雄、罗仁千、斯科特·迈耶·麦金尼、罗伯特·奥萨祖瓦·内斯、霍夫峰·潘、秦涛、宇佐山尚人、克里斯·怀特、和埃里克·霍维茨。通用基础模型能否超越专用调优？医学领域的案例研究，2023年。'
- en: '[21] OpenAI. Gpt-4 technical report. ArXiv, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] OpenAI。GPT-4 技术报告。ArXiv，2023年。'
- en: '[22] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa:
    A large-scale multi-subject multi-choice dataset for medical domain question answering.
    In Conference on health, inference, and learning, pages 248–260\. PMLR, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ankit Pal、Logesh Kumar Umapathi、和 Malaikannan Sankarasubbu。Medmcqa: 医疗领域问答的大规模多学科多选择数据集。在健康、推理和学习会议上，页面
    248–260。PMLR，2022年。'
- en: '[23] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, pages 1–22, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 朴俊成、约瑟夫·奥布赖恩、凯瑞·君蔡、梅雷迪思·林戈尔·莫里斯、帕西·梁、和迈克尔·S·伯恩斯坦。生成代理: 人类行为的互动模拟。在第36届
    ACM 用户界面软件和技术年会论文集中，页面 1–22，2023年。'
- en: '[24] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla:
    Large language model connected with massive apis. arXiv preprint arXiv:2305.15334,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Shishir G Patil、张天俊、辛旺、和 Joseph E Gonzalez。Gorilla: 连接大量 API 的大型语言模型。arXiv
    预印本 arXiv:2305.15334，2023年。'
- en: '[25] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint
    arXiv:2307.07924, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 陈乾、辛聪、程阳、魏泽陈、于生苏、倪元徐、智远刘、和毛松孙。用于软件开发的沟通代理。arXiv 预印本 arXiv:2307.07924，2023年。'
- en: '[26] Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
    Yankai Lin, Zhong Zhang, Zhiyuan Liu, and Maosong Sun. Tell me more! towards implicit
    user intention understanding of language model driven agents. arXiv preprint arXiv:2402.09205,
    2024.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 程乾、何炳湘、庄中、姜登、秦宇佳、辛聪、林彦凯、张中、智远刘、和毛松孙。告诉我更多！朝着对语言模型驱动的代理的隐性用户意图理解迈进。arXiv
    预印本 arXiv:2402.09205，2024年。'
- en: '[27] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning
    from scratch via self-planning. arXiv preprint arXiv:2401.05268, 2024.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 乔硕飞、张宁玉、方润南、罗雨杰、周望春舒、姜雨晨、吕成飞、和陈华军。Autoact: 通过自我规划从零开始自动学习代理。arXiv 预印本
    arXiv:2401.05268，2024年。'
- en: '[28] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using
    siamese bert-networks, 2019.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 尼尔斯·赖默斯 和 伊琳娜·古雷维奇。Sentence-bert: 使用 Siamese BERT 网络的句子嵌入，2019年。'
- en: '[29] Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu,
    Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that
    ask for help: Uncertainty alignment for large language model planners. arXiv preprint
    arXiv:2307.01928, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Allen Z Ren、Anushri Dixit、Alexandra Bodrova、Sumeet Singh、Stephen Tu、Noah
    Brown、彭旭、Leila Takayama、费晓、杰克·瓦利等。会请求帮助的机器人: 大型语言模型规划者的不确定性对齐。arXiv 预印本 arXiv:2307.01928，2023年。'
- en: '[30] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, and Rui Zhao. Tptu: Task planning and
    tool usage of large language model-based ai agents. arXiv preprint arXiv:2308.03427,
    2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 冉晶清、陈毅宏、张彬、徐志伟、包天鹏、杜国庆、施伟、毛航宇、曾兴宇、和赵睿。Tptu: 基于大型语言模型的 AI 代理的任务规划和工具使用。arXiv
    预印本 arXiv:2308.03427，2023年。'
- en: '[31] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. Advances in Neural Information
    Processing Systems, 36, 2024.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 蒂莫·希克、简·德维维迪-余、罗伯托·德西、罗伯塔·雷莱努、玛利亚·洛梅利、埃里克·汉布罗、卢克·泽特尔摩耶、尼古拉·坎塞达、和托马斯·斯恰隆。Toolformer:
    语言模型可以自学使用工具。神经信息处理系统进展，36，2024年。'
- en: '[32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 约翰·舒尔曼、菲利普·沃尔斯基、普拉富拉·达里瓦尔、亚历克·拉德福德、和奥列格·克里莫夫。近端策略优化算法。arXiv 预印本 arXiv:1707.06347，2017年。'
- en: '[33] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
    Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.
    Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu 和 Yueting
    Zhuang。Hugginggpt：用 ChatGPT 和 Hugging Face 里的朋友解决 AI 任务。神经信息处理系统进展，36，2024年。'
- en: '[34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and
    Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances
    in Neural Information Processing Systems, 36, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan 和 Shunyu
    Yao。Reflexion：具有语言强化学习的语言代理。神经信息处理系统进展，36，2024年。'
- en: '[35] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler,
    and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
    interactive learning. In International Conference on Learning Representations,
    2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler
    和 Matthew Hausknecht。Alfworld：将文本与具身环境对齐以进行互动学习。国际学习表示会议，2020年。'
- en: '[36] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan 和 Anima Anandkumar。Voyager：一个开放式的具身代理与大型语言模型。arXiv 预印本 arXiv:2305.16291，2023年。'
- en: '[37] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu.
    ScienceWorld: Is your agent smarter than a 5th grader? In Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing, pages 11279–11298, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté 和 Prithviraj Ammanabrolu。ScienceWorld：你的代理比五年级学生更聪明吗？在
    Yoav Goldberg, Zornitsa Kozareva 和 Yue Zhang 主编的《2022年自然语言处理经验方法会议论文集》中，第11279–11298页，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。'
- en: '[38] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao
    Liang. Describe, explain, plan and select: Interactive planning with large language
    models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560,
    2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma 和 Yitao
    Liang。描述、解释、计划和选择：通过大型语言模型进行互动规划，使开放世界多任务代理成为可能。arXiv 预印本 arXiv:2302.01560，2023年。'
- en: '[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou 等。链式思考提示在大型语言模型中引发推理。神经信息处理系统进展，35:24824–24837，2022年。'
- en: '[40] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and
    Bryan Hooi. Can llms express their uncertainty? an empirical evaluation of confidence
    elicitation in llms. arXiv preprint arXiv:2306.13063, 2023.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He 和 Bryan
    Hooi。大型语言模型能否表达其不确定性？对大型语言模型中信心引发的实证评估。arXiv 预印本 arXiv:2306.13063，2023年。'
- en: '[41] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting
    chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381,
    2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 和 Lijuan Wang。MM-React：为多模态推理和行动提示
    ChatGPT。arXiv 预印本 arXiv:2303.11381，2023年。'
- en: '[42] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen,
    Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse,
    explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen,
    Ruslan Salakhutdinov 和 Christopher D Manning。Hotpotqa：一个用于多跳问题回答的数据集，具有多样性和可解释性。arXiv
    预印本 arXiv:1809.09600，2018年。'
- en: '[43] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards
    scalable real-world web interaction with grounded language agents. Advances in
    Neural Information Processing Systems, 35:20744–20757, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Shunyu Yao, Howard Chen, John Yang 和 Karthik Narasimhan。Webshop：朝着可扩展的真实世界网络互动迈进，结合了具身语言代理。神经信息处理系统进展，35:20744–20757，2022年。'
- en: '[44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv
    preprint arXiv:2210.03629, 2022.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan
    和 Yuan Cao。React：在语言模型中协同推理和行动。arXiv 预印本 arXiv:2210.03629，2022年。'
- en: '[45] Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    arXiv preprint arXiv:2305.17878, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Qiang Zhang, Jason Naradowsky, and Yusuke Miyao. Ask an expert: Leveraging
    language models to improve strategic reasoning in goal-oriented dialogue models.
    arXiv preprint arXiv:2305.17878, 2023.'
- en: '[46] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
    Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:
    a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
    Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean:
    a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219,
    2023.'
- en: '[47] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey
    area: Expressions of overconfidence and uncertainty in language models. arXiv
    e-prints, pages arXiv–2302, 2023.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey
    area: Expressions of overconfidence and uncertainty in language models. arXiv
    e-prints, pages arXiv–2302, 2023.'
- en: Appendix
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: \startcontents
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: \startcontents
- en: '[sections] \printcontents[sections]l1'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[sections] \printcontents[sections]l1'
- en: Appendix A Session-level optimization algorithm
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 会话级优化算法
- en: Assume that the entire trajectory $\tau$ is the memory before the session starts.
    In this section, we will explain how to transform a trajectory-level RL optimization
    algorithm into a session-level RL optimization algorithm.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 假设整个轨迹$\tau$是会话开始前的记忆。在本节中，我们将解释如何将轨迹级RL优化算法转换为会话级RL优化算法。
- en: 'Let $r(\tau)$. The optimization objective is to maximize the following expectation:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 令$r(\tau)$。优化目标是最大化以下期望：
- en: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta)=\mathbb{E}_{\tau\sim\pi_{\theta}}[r(\tau)].$ |  | (1) |'
- en: For an arbitrary session index $i$, respectively. Accordingly, we have
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意会话索引$i$，分别如此。因此，我们有
- en: '|  | $\displaystyle R(\theta)$ |  | (2) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(\theta)$ |  | (2) |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: 'Here, $\mathcal{S}_{i}$, indicating the expected total reward the agent expects
    to receive in the future. Averaging over all session indices, Eq. ([2](#A1.E2
    "In Appendix A Session-level optimization algorithm ‣ AGILE: A Novel Framework
    of LLM Agents")) gives:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '这里，$\mathcal{S}_{i}$，表示代理期望在未来获得的总奖励。对所有会话索引取平均，方程式 ([2](#A1.E2 "在附录A会话级优化算法
    ‣ AGILE: 一种新型的LLM代理框架"))给出：'
- en: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (3) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot\mid\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (3) |'
- en: 'In Eq. ([3](#A1.E3 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents")), the parameter $\theta$ only appears in the
    session-level expectation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '在方程式 ([3](#A1.E3 "在附录A会话级优化算法 ‣ AGILE: 一种新型的LLM代理框架"))中，参数$\theta$仅出现在会话级期望中：'
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (4) |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta\mid\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta}}\left[r(\tau_{1:i-1})+\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot\mid\mathcal{S}_{i})}\left[r(\tau_{i})+V_{\pi_{\theta_{k}}}\left(\mathcal{S}_{i+1}\right)\right]\right].$
    |  | (4) |'
- en: '$R(\theta|\theta_{k})$. If we employ an iterative optimization procedure:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: $R(\theta|\theta_{k})$。如果我们采用迭代优化程序：
- en: '1.'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Initialize $\theta_{0}$ from a reference policy (obtained through SFT).
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从参考策略（通过SFT获得）初始化$\theta_{0}$。
- en: '2.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For $k=0,1,2,\cdots$.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于$k=0,1,2,\cdots$。
- en: Then $\theta$ will converge to an (at least locally) optimal policy.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然后$\theta$将收敛到一个（至少是局部的）最优策略。
- en: Now we are ready to illustrate why the optimization of $R(\theta|\theta_{k})$
    can be solved at the session level. Notice that
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备说明为什么$R(\theta|\theta_{k})$的优化可以在会话级别解决。注意到
- en: '|  | $\displaystyle R(\theta&#124;\theta_{k})$ |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R(\theta\mid\theta_{k})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(\tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\tau_{1:i-1}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[r(\tau_{i})+V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})]\right]+\mathbb{E}_{\tau_{i}\sim\pi_{\theta_{k}}}[r(\tau)]$
    |  |'
- en: 'On the right-hand side, the first term involves two sampling steps. The first
    step samples $\tau_{1:i-1}\sim\pi_{\theta_{k}}$. As a result, if we define a *proxy
    reward*:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，第一个项涉及两个采样步骤。第一步采样 $\tau_{1:i-1}\sim\pi_{\theta_{k}}$。因此，如果我们定义一个*代理奖励*：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: Then, we have
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有
- en: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{constant}.$
    |  | (6) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\theta&#124;\theta_{k})=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}_{\mathcal{S}_{i}\sim\pi_{\theta_{k}}}\left[\mathbb{E}_{\tau_{i}\sim\pi_{\theta}(\cdot&#124;\mathcal{S}_{i})}[\tilde{r}_{k}(\tau_{i})]\right]+\text{常数}.$
    |  | (6) |'
- en: 'By Eq. ([6](#A1.E6 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents")), $R(\theta|\theta_{k})$.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '根据 Eq. ([6](#A1.E6 "In Appendix A Session-level optimization algorithm ‣ AGILE:
    A Novel Framework of LLM Agents"))，$R(\theta|\theta_{k})$。'
- en: Algorithm 1 Session-level optimization
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 会话级优化
- en: '1:Initialize $\theta_{0}$ by maximizing Eq. ([6](#A1.E6 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")).12:end for'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化 $\theta_{0}$ 通过最大化 Eq. ([6](#A1.E6 "In Appendix A Session-level optimization
    algorithm ‣ AGILE: A Novel Framework of LLM Agents"))。12:结束 for'
- en: 'Finally, we present the session-level optimization algorithm as Algorithm [1](#alg1
    "Algorithm 1 ‣ Appendix A Session-level optimization algorithm ‣ AGILE: A Novel
    Framework of LLM Agents"). In this algorithm, the state advantage function is
    the only component that concerns inter-session correlation. While the algorithm
    is iterative, we anticipate that in practice, the outer loop will require only
    a few iterations to converge.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们展示了会话级优化算法，见算法 [1](#alg1 "Algorithm 1 ‣ Appendix A Session-level optimization
    algorithm ‣ AGILE: A Novel Framework of LLM Agents")。在这个算法中，状态优势函数是唯一涉及会话间相关性的组件。虽然算法是迭代的，但我们预期在实际应用中，外部循环只需几次迭代即可收敛。'
- en: Appendix B Implementation details of AGILE
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B AGILE的实现细节
- en: B.1 ProductQA
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 ProductQA
- en: Implementation of [GetQuestion]
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[GetQuestion] 的实现'
- en: This function prompts the user for a new question and appends it to the LLM
    context. Every question is raised for a specific product, thus it has an associated
    product ID. Based on this ID, the function also appends the product information
    table’s schema and the product metadata to the context.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数提示用户提出一个新问题，并将其附加到LLM上下文中。每个问题都是针对特定产品提出的，因此它具有一个相关的产品ID。根据这个ID，函数还会将产品信息表的模式和产品元数据附加到上下文中。
- en: Implementation of [RetrieveMemory]
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[RetrieveMemory] 的实现'
- en: This function employs the provided question as a query to retrieve the most
    relevant historical QA pair and the most relevant knowledge entry from the agent’s
    memory. To safeguard sensitive data from sellers, the agent is restricted to accessing
    QA records exclusively for the queried product from historical interactions. However,
    it is permitted to retrieve general knowledge from the whole trajectory since
    this information is not seller-specific. We utilize an embedding-based retrieval
    method, specifically employing the all-MiniLM-L6-v2 model [[28](#bib.bib28)] as
    the embedding model.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数使用提供的问题作为查询，从代理的记忆中检索最相关的历史QA对和最相关的知识条目。为了保护卖家的敏感数据，代理被限制只能访问历史交互中查询产品的QA记录。然而，代理可以从整个轨迹中检索一般知识，因为这些信息不是特定于卖家的。我们使用基于嵌入的检索方法，具体使用全-MiniLM-L6-v2模型 [[28](#bib.bib28)]作为嵌入模型。
- en: Implementation of [SearchProdcut]
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[SearchProdcut] 的实现'
- en: This function utilizes the LLM to predict a SQL query based on the context,
    and then invoke a MySQL execution engine. It appends the result to the LLM context.
    If there is an execution error, then the error is appended to the context too.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数利用LLM根据上下文预测SQL查询，然后调用MySQL执行引擎。它将结果附加到LLM上下文中。如果发生执行错误，错误也会被附加到上下文中。
- en: Implementation of [SeekAdvice]
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[SeekAdvice] 的实现'
- en: This requests for human expert advice and append it to the LLM context. In our
    implementation, the human expert simply returns the ground truth long answer from
    the ProductQA dataset.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求人类专家提供建议并将其附加到LLM上下文中。在我们的实现中，人类专家仅从ProductQA数据集中返回真实的长答案。
- en: Implementation of [PredictAnswer]
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[PredictAnswer]的实现'
- en: This function passes control to the LLM to continue generating a long answer
    and a short answer.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将控制权传递给LLM，以继续生成长答案和短答案。
- en: Implementation of [Reflection]
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '[Reflection]的实现'
- en: This function passes control to the LLM to continue generating a reflection
    result.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将控制权传递给LLM，以继续生成反思结果。
- en: Training Data Generation
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: 'We generate training data on a session-by-session basis, where each session
    consists of a QA pair. A session begins with an initial memory, consisting of
    historical QA pairs and knowledge entries accumulated from previous sessions.
    Recall that the [RetrieveMemory] function retrieves only the most relevant QA
    pair and knowledge entry per session. Thus, in constructing training memories,
    it suffices to put the retrieved QA pair and the retrieved knowledge entry into
    the memory. We select them in the following stochastic way: the retrieved QA pair
    can be the most relevant QA pair from the training set, or a random QA pair, or
    omitted entirely; similarly for the retrieved knowledge entry.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按会话逐一生成训练数据，每个会话包括一个问答对。会话以初始记忆开始，记忆由历史问答对和从前面会话中积累的知识条目组成。请注意，[RetrieveMemory]函数每会话仅检索最相关的问答对和知识条目。因此，在构建训练记忆时，只需将检索到的问答对和知识条目放入记忆中即可。我们以以下随机方式选择它们：检索到的问答对可以是训练集中最相关的问答对，或随机问答对，或完全省略；知识条目也是类似。
- en: 'Based on the initial memory, we generate trajectories by following the agent
    workflow detailed in Section [4.1](#S4.SS1 "4.1 Experimental setting ‣ 4 Experiments
    ‣ AGILE: A Novel Framework of LLM Agents"). Each trajectory begins with [GetUserQuestion]
    and [RetrieveMemory]. For QAs classified as Search-QA, a [SearchProduct] function
    is appended, followed by the corresponding SQL query and its execution result.
    For other QA types, if an associated knowledge entry exists and is successfully
    retrieved, the trajectory will extend with a [PredictAnswer] call with the ground
    truth answer as its result. If the knowledge entry is not retrieved or is absent,
    we use GPT-4 to evaluate whether the question can be answered with the available
    context. If affirmative, a [PredictAnswer] with the ground truth answer is appended.
    Otherwise, the trajectory extends with a [SeekAdvice] call with the ground truth
    answer as the advice, and a [Reflection] call, where the reflection result is
    the knowledge entry if it exists, or "no information" if not. Then the reflection
    result is appended to the memory via [UpdateMemory]. Finally, the trajectory is
    concluded by [SubmitAnswer].'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '基于初始记忆，我们通过遵循第[4.1节](#S4.SS1 "4.1 实验设置 ‣ 4 实验 ‣ AGILE: 一种新型的LLM代理框架")中详细描述的代理工作流程生成轨迹。每条轨迹都以[GetUserQuestion]和[RetrieveMemory]开始。对于被分类为搜索型问答（Search-QA）的问答，添加一个[SearchProduct]函数，随后是相应的SQL查询及其执行结果。对于其他问答类型，如果存在关联的知识条目并成功检索，则轨迹将扩展为一个[PredictAnswer]调用，结果为真实答案。如果未检索到知识条目或条目不存在，我们使用GPT-4来评估是否可以利用现有上下文回答问题。如果可以，则添加一个[PredictAnswer]调用，结果为真实答案。否则，轨迹将扩展为一个[SeekAdvice]调用，真实答案作为建议，以及一个[Reflection]调用，其中反思结果如果存在则为知识条目，否则为“无信息”。然后通过[UpdateMemory]将反思结果附加到记忆中。最后，通过[SubmitAnswer]结束轨迹。'
- en: In this way, we constructed 55,772 session-level trajectories in total, from
    6 training tasks in ProductQA. This data is used for imitation learning. In PPO
    training, we reuse the initial memory data, while the session-level trajectories
    are generated by the model itself.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们总共构建了55,772个会话级轨迹，来自ProductQA中的6个训练任务。这些数据用于模仿学习。在PPO训练中，我们重复使用初始记忆数据，而会话级轨迹则由模型自身生成。
- en: B.2 MedMCQA
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 MedMCQA
- en: 'For MedMCQA, the memory is initialized with all QA pairs from the training
    set, simulating that the agent has processed the training set before reaching
    the test set. We also add a knowledge entry for each QA pair, obtained through
    GPT-4 reflection (see Figure [11](#A6.F11 "Figure 11 ‣ Prompt templates for MedMCQA
    ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents") for the
    prompt).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 MedMCQA，记忆初始化为训练集中的所有 QA 对，模拟代理在测试集之前已经处理过训练集。我们还为每个 QA 对添加了一个知识条目，通过 GPT-4
    反思获得（见图 [11](#A6.F11 "图11 ‣ MedMCQA 的提示模板 ‣ 附录F 提示模板 ‣ AGILE: 一种新型LLM代理框架") 以获取提示）。'
- en: Training data generation
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练数据生成
- en: We sample a subset of training data from MedMCQA to construct session-level
    trajectories. Each trajectory begins with [GetUserQuestion] and [RetrieveMemory].
    The [RetrieveMemory] function retrieves the five most relevant QA pairs and pieces
    of knowledge from the initial memory, using the same embedding similarity search
    method employed in ProductQA. Then, we prompt GPT-4 to predict an answer with
    chain-of-thought reasoning. If the GPT-4 answer is correct, we append a [PredictAnswer]
    call, the GPT-4 chain-of-thought, and the ground-truth answer to the trajectory.
    If the GPT-4 answer is wrong, which suggests that the question is hard, we append
    a [SeekAdvice] call with the ground-truth answer, followed by a [Reflection] call
    with the reflection result generated by GPT-4\. Then the reflection result is
    appended to the memory via [UpdateMemory]. Finally, the trajectory is concluded
    by [SubmitAnswer]. In this way, we obtain 23,015 session-level trajectories in
    total.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 MedMCQA 中抽取一个子集作为训练数据，以构建会话级轨迹。每条轨迹以 [GetUserQuestion] 和 [RetrieveMemory]
    开始。[RetrieveMemory] 功能从初始记忆中检索出五个最相关的 QA 对和知识片段，使用与 ProductQA 中相同的嵌入相似性搜索方法。然后，我们提示
    GPT-4 进行链式思考推理预测答案。如果 GPT-4 的回答正确，我们将 [PredictAnswer] 调用、GPT-4 的链式思考和真实答案添加到轨迹中。如果
    GPT-4 的回答错误，这表明问题较难，我们将 [SeekAdvice] 调用和真实答案添加到轨迹中，接着是 [Reflection] 调用及 GPT-4
    生成的反思结果。然后，通过 [UpdateMemory] 将反思结果添加到记忆中。最后，轨迹以 [SubmitAnswer] 结束。通过这种方式，我们总共获得了
    23,015 条会话级轨迹。
- en: B.3 Defining proxy reward for RL
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 为 RL 定义代理奖励
- en: 'In the question-answering tasks, sessions are not independent. Actions taken
    in earlier sessions can influence memory, creating lasting effects on subsequent
    sessions. As illustrated in Equation ([5](#A1.E5 "In Appendix A Session-level
    optimization algorithm ‣ AGILE: A Novel Framework of LLM Agents")), the term $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})$.
    Hence, we use the following heuristic definition,'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '在问答任务中，会话不是独立的。早期会话中的行动会影响记忆，从而对后续会话产生持久影响。如方程式 ([5](#A1.E5 "附录A会话级优化算法 ‣ AGILE:
    一种新型LLM代理框架"))中所示，术语 $A_{i}:=V_{\pi_{\theta_{k}}}(\mathcal{S}_{i+1})-V_{\pi_{\theta_{k}}}(\mathcal{S}_{i})$。因此，我们使用以下启发式定义，'
- en: '|  | 
    |  | (7) |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '|  | 
    |  | (7) |'
- en: where $q_{i}$ by default.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，$q_{i}$。
- en: Appendix C Tables
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C表格
- en: 'Table 9: Statistics of the ProductQA dataset. # Products indicates the number
    of products within each group. # Fact-QA, # Search-QA and # Reasoning-QA display
    the respective numbers of QA pairs categorized as Fact-QA, Search-QA, and Reasoning-QA.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '表9：ProductQA数据集的统计数据。# Products表示每个组中的产品数量。# Fact-QA、# Search-QA 和 # Reasoning-QA
    显示了分别分类为 Fact-QA、Search-QA 和 Reasoning-QA 的 QA 对的数量。'
- en: '| Groups | # Products | # Fact-QA | # Search-QA | # Reasoning-QA | Total |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 组 | # 产品 | # Fact-QA | # Search-QA | # Reasoning-QA | 总计 |'
- en: '| Train | Blades | 20 | 2,147 | 769 | 631 | 3,547 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 火车 | 刀片 | 20 | 2,147 | 769 | 631 | 3,547 |'
- en: '| Headlight Bulbs | 20 | 1,767 | 644 | 463 | 2,874 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 前照灯灯泡 | 20 | 1,767 | 644 | 463 | 2,874 |'
- en: '| Cell Phones | 20 | 1,636 | 761 | 374 | 2,771 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 手机 | 20 | 1,636 | 761 | 374 | 2,771 |'
- en: '| Portable Power Banks | 20 | 3,344 | 673 | 500 | 4,517 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 便携式充电宝 | 20 | 3,344 | 673 | 500 | 4,517 |'
- en: '| Dresses | 20 | 2,287 | 738 | 263 | 3,288 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 连衣裙 | 20 | 2,287 | 738 | 263 | 3,288 |'
- en: '| Everyday Bras | 20 | 1,942 | 684 | 336 | 2,962 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 日常文胸 | 20 | 1,942 | 684 | 336 | 2,962 |'
- en: '| Wrist Watches | 20 | 2,169 | 757 | 389 | 3,315 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 手表 | 20 | 2,169 | 757 | 389 | 3,315 |'
- en: '| Blu-ray Players | 20 | 1,630 | 688 | 572 | 2,890 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 蓝光播放器 | 20 | 1,630 | 688 | 572 | 2,890 |'
- en: '| Camera Lenses | 20 | 1,859 | 769 | 1,025 | 3,653 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 相机镜头 | 20 | 1,859 | 769 | 1,025 | 3,653 |'
- en: '| Headphones | 20 | 5,432 | 766 | 583 | 6,781 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 耳机 | 20 | 5,432 | 766 | 583 | 6,781 |'
- en: '| Mice | 20 | 5,653 | 490 | 294 | 6,437 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 鼠标 | 20 | 5,653 | 490 | 294 | 6,437 |'
- en: '| Point & Shoot Digital Cameras | 20 | 1,696 | 722 | 565 | 2,983 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 便携式数码相机 | 20 | 1,696 | 722 | 565 | 2,983 |'
- en: '| Coffee Machines | 20 | 4,184 | 681 | 638 | 5,503 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 咖啡机 | 20 | 4,184 | 681 | 638 | 5,503 |'
- en: '| Digital Scales | 20 | 2,724 | 391 | 682 | 3,797 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 数字秤 | 20 | 2,724 | 391 | 682 | 3,797 |'
- en: '| Space Heaters | 20 | 2,283 | 674 | 498 | 3,455 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 空气加热器 | 20 | 2,283 | 674 | 498 | 3,455 |'
- en: '| Printers | 20 | 1,431 | 760 | 489 | 2,680 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 打印机 | 20 | 1,431 | 760 | 489 | 2,680 |'
- en: '| Litter | 20 | 1,860 | 753 | 507 | 3,120 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾箱 | 20 | 1,860 | 753 | 507 | 3,120 |'
- en: '| Grips | 20 | 1,771 | 713 | 413 | 2,897 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 把手 | 20 | 1,771 | 713 | 413 | 2,897 |'
- en: '| Gun Holsters | 20 | 1,679 | 94 | 1,362 | 3,135 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 枪套 | 20 | 1,679 | 94 | 1,362 | 3,135 |'
- en: '| Handheld Flashlights | 20 | 2,009 | 768 | 482 | 3,259 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 手持手电筒 | 20 | 2,009 | 768 | 482 | 3,259 |'
- en: '| Total | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 400 | 49,503 | 13,295 | 11,066 | 73,864 |'
- en: '| Test | Leggings | 20 | 969 | 743 | 527 | 2,239 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 打底裤 | 20 | 969 | 743 | 527 | 2,239 |'
- en: '| Camera Cases | 20 | 975 | 706 | 898 | 2,579 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 相机保护套 | 20 | 975 | 706 | 898 | 2,579 |'
- en: '| Motherboards | 20 | 989 | 736 | 826 | 2,551 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 20 | 989 | 736 | 826 | 2,551 |'
- en: '| All Pans | 20 | 973 | 747 | 275 | 1,995 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 全部平底锅 | 20 | 973 | 747 | 275 | 1,995 |'
- en: '| Rollerball Pens | 20 | 967 | 760 | 603 | 2,330 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 20 | 967 | 760 | 603 | 2,330 |'
- en: '| Rifle Scopes | 17 | 979 | 714 | 978 | 2,671 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 17 | 979 | 714 | 978 | 2,671 |'
- en: '| Total | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 117 | 5,852 | 4,406 | 4,107 | 14,365 |'
- en: 'Table 10: Detail performance of our methods and other baselines on six test
    product groups of ProductQA. X-prompt represents directly prompting the model
    X; agile-X-Y represents incorporating the model X within the AGILE framework,
    while Y represents prompting or PPO training. The Short and Long stand for the
    results evaluated on short answers and long answers, respectively. The seeking
    advice cost is $c=0.3$. The best total scores are highlighted in bold.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：我们的方法与其他基线在 ProductQA 的六个测试产品组上的详细性能。X-prompt 代表直接提示模型 X；agile-X-Y 代表将模型
    X 纳入 AGILE 框架，而 Y 代表提示或 PPO 训练。Short 和 Long 分别代表在短答案和长答案上的结果。寻求建议的成本为 $c=0.3$。最佳总分用粗体突出显示。
- en: '| Group | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 组 | gpt3.5- | gpt4- | agile-vicuna- | agile-gpt3.5- | agile-gpt4- | agile-vic7b-
    | agile-vic13b- |'
- en: '| prompt | prompt | 13b-prompt | prompt | prompt | ppo(ours) | ppo(ours) |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| prompt | prompt | 13b-prompt | prompt | prompt | ppo(ours) | ppo(ours) |'
- en: '| Short | Long | Short | Long | Short | Long | Short | Long | Short | Long
    | Short | Long | Short | Long |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 | 短 | 长 |'
- en: '| Camera Cases | Advice Rate $\downarrow$ | - | - | - | - | 0.182 | 0.182 |
    0.313 | 0.313 | 0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 相机保护套 | 建议率 $\downarrow$ | - | - | - | - | 0.182 | 0.182 | 0.313 | 0.313
    | 0.175 | 0.175 | 0.199 | 0.199 | 0.263 | 0.263 |'
- en: '| Accuracy $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537
    | 0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.200 | 0.320 | 0.385 | 0.495 | 0.182 | 0.330 | 0.537 |
    0.644 | 0.775 | 0.791 | 0.818 | 0.776 | 0.860 | 0.841 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 |
    0.722 | 0.738 | 0.758 | 0.716 | 0.781 | 0.762 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.127 | 0.275 | 0.443 | 0.550 | 0.722 | 0.738
    | 0.758 | 0.716 | 0.781 | 0.762 |'
- en: '| Leggings | Advice Rate $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359
    | 0.359 | 0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 $\downarrow$ | - | - | - | - | 0.154 | 0.154 | 0.359 | 0.359 |
    0.200 | 0.200 | 0.201 | 0.201 | 0.251 | 0.251 |'
- en: '| Accuracy $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497
    | 0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.181 | 0.306 | 0.503 | 0.594 | 0.154 | 0.267 | 0.497 |
    0.646 | 0.766 | 0.790 | 0.837 | 0.834 | 0.876 | 0.885 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 |
    0.706 | 0.730 | 0.777 | 0.774 | 0.801 | 0.810 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.108 | 0.221 | 0.389 | 0.538 | 0.706 | 0.730
    | 0.777 | 0.774 | 0.801 | 0.810 |'
- en: '| All Pans | Advice Rate $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336
    | 0.336 | 0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 全部平底锅 | 建议率 $\downarrow$ | - | - | - | - | 0.167 | 0.167 | 0.336 | 0.336
    | 0.220 | 0.220 | 0.184 | 0.184 | 0.220 | 0.220 |'
- en: '| Accuracy $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506
    | 0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.201 | 0.297 | 0.470 | 0.538 | 0.167 | 0.272 | 0.506 |
    0.605 | 0.784 | 0.804 | 0.843 | 0.831 | 0.866 | 0.869 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 |
    0.718 | 0.738 | 0.788 | 0.776 | 0.800 | 0.803 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.117 | 0.222 | 0.405 | 0.504 | 0.718 | 0.738
    | 0.788 | 0.776 | 0.800 | 0.803 |'
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | - | - | - | - | 0.130 | 0.130
    | 0.333 | 0.333 | 0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 $\downarrow$ | - | - | - | - | 0.130 | 0.130 | 0.333 | 0.333 |
    0.231 | 0.231 | 0.162 | 0.162 | 0.212 | 0.212 |'
- en: '| Accuracy $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482
    | 0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.193 | 0.271 | 0.449 | 0.573 | 0.130 | 0.242 | 0.482 |
    0.627 | 0.767 | 0.808 | 0.776 | 0.769 | 0.816 | 0.824 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 |
    0.698 | 0.739 | 0.727 | 0.720 | 0.752 | 0.760 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.091 | 0.203 | 0.382 | 0.527 | 0.698 | 0.739
    | 0.727 | 0.720 | 0.752 | 0.760 |'
- en: '| Mother- boards | Advice Rate $\downarrow$ | - | - | - | - | 0.214 | 0.214
    | 0.303 | 0.303 | 0.225 | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 $\downarrow$ | - | - | - | - | 0.214 | 0.214 | 0.303 | 0.303 | 0.225
    | 0.225 | 0.162 | 0.162 | 0.235 | 0.235 |'
- en: '| Accuracy $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525
    | 0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.253 | 0.431 | 0.511 | 0.637 | 0.215 | 0.337 | 0.525 |
    0.686 | 0.815 | 0.855 | 0.835 | 0.831 | 0.877 | 0.882 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 |
    0.747 | 0.788 | 0.786 | 0.782 | 0.806 | 0.812 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.151 | 0.273 | 0.434 | 0.595 | 0.747 | 0.788
    | 0.786 | 0.782 | 0.806 | 0.812 |'
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | - | - | - | - | 0.197 | 0.197 |
    0.293 | 0.293 | 0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 $\downarrow$ | - | - | - | - | 0.197 | 0.197 | 0.293 | 0.293
    | 0.198 | 0.198 | 0.167 | 0.167 | 0.216 | 0.216 |'
- en: '| Accuracy $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502
    | 0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.187 | 0.306 | 0.463 | 0.587 | 0.197 | 0.313 | 0.502 |
    0.657 | 0.770 | 0.806 | 0.802 | 0.760 | 0.828 | 0.822 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 |
    0.711 | 0.747 | 0.752 | 0.710 | 0.763 | 0.757 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.138 | 0.254 | 0.414 | 0.569 | 0.711 | 0.747
    | 0.752 | 0.710 | 0.763 | 0.757 |'
- en: '| Average | Advice Rate $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323
    | 0.323 | 0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 $\downarrow$ | - | - | - | - | 0.174 | 0.174 | 0.323 | 0.323 |
    0.208 | 0.208 | 0.179 | 0.179 | 0.233 | 0.233 |'
- en: '| Accuracy $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508
    | 0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.202 | 0.322 | 0.464 | 0.571 | 0.174 | 0.294 | 0.508 |
    0.644 | 0.780 | 0.809 | 0.818 | 0.800 | 0.854 | 0.854 |'
- en: '| Total Score $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 |
    0.718 | 0.747 | 0.764 | 0.746 | 0.784 | 0.784 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | - | - | - | - | 0.122 | 0.242 | 0.411 | 0.547 | 0.718 | 0.747
    | 0.764 | 0.746 | 0.784 | 0.784 |'
- en: 'Table 11: Ablation study on ProductQA test tasks. w/o Reflection represents
    removing the reflection function. w/o Memory represents prohibiting memory component.
    w/o Advice represents removing the seeking advice function. Non-adapt advice represents
    seeking advice in the same number with agile-vic13b-ppo at the beginning of trajectory.
    w/o Tool-Use represents removing the search product function. w/o RL represents
    the agile-vic13b-sft. The best scores are highlighted in bold.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：关于 ProductQA 测试任务的消融研究。w/o Reflection 表示移除反射功能。w/o Memory 表示禁止记忆组件。w/o
    Advice 表示移除寻求建议功能。Non-adapt advice 表示在轨迹开始时与 agile-vic13b-ppo 寻求相同数量的建议。w/o Tool-Use
    表示移除搜索产品功能。w/o RL 表示 agile-vic13b-sft。最佳得分用粗体显示。
- en: '| Group | w/o | w/o | w/o | Non-adapt | w/o | w/o | agile-vic- |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 组 | w/o | w/o | w/o | 非适应 | w/o | w/o | agile-vic- |'
- en: '| Reflection | Memory | Advice | Advice | Tool-Use | RL | 13b-ppo |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 反射 | 记忆 | 建议 | 建议 | 工具使用 | 强化学习 | 13b-ppo |'
- en: '| Camera Cases | Advice Rate $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 |
    0.452 | 0.295 | 0.263 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 相机外壳 | 建议率 $\downarrow$ | 0.335 | 0.459 | 0.000 | 0.263 | 0.452 | 0.295 |
    0.263 |'
- en: '| Accuracy $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.851 | 0.869 | 0.735 | 0.827 | 0.870 | 0.849 | 0.860 |'
- en: '| Total Score $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.750(-4.1%) | 0.731(-6.8%) | 0.735(-6.3%) | 0.748(-4.4%)
    | 0.734(-6.4%) | 0.760(-2.8%) | 0.781 |'
- en: '| Leggings | Advice Rate $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529
    | 0.290 | 0.251 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 $\downarrow$ | 0.276 | 0.437 | 0.000 | 0.251 | 0.529 | 0.290 |
    0.251 |'
- en: '| Accuracy $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.874 | 0.902 | 0.762 | 0.828 | 0.880 | 0.867 | 0.876 |'
- en: '| Total Score $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.791(-1.3%) | 0.771(-3.9%) | 0.762(-5.1%) | 0.753(-6.4%)
    | 0.721(-11.1%) | 0.780(-2.7%) | 0.801 |'
- en: '| All Pans | Advice Rate $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550
    | 0.225 | 0.220 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 所有平底锅 | 建议率 $\downarrow$ | 0.263 | 0.413 | 0.000 | 0.220 | 0.550 | 0.225
    | 0.220 |'
- en: '| Accuracy $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.867 | 0.900 | 0.759 | 0.818 | 0.877 | 0.855 | 0.866 |'
- en: '| Total Score $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.788(-1.5%) | 0.776(-3.1%) | 0.759(-5.4%) | 0.752(-6.4%)
    | 0.712(-12.4%) | 0.788(-1.5%) | 0.800 |'
- en: '| Rollerball Pens | Advice Rate $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212
    | 0.501 | 0.220 | 0.212 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 $\downarrow$ | 0.237 | 0.378 | 0.000 | 0.212 | 0.501 | 0.220 |
    0.212 |'
- en: '| Accuracy $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816
    |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.818 | 0.843 | 0.727 | 0.785 | 0.868 | 0.812 | 0.816 |'
- en: '| Total Score $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.747(-0.7%) | 0.730(-3.0%) | 0.727(-3.4%) | 0.721(-4.3%)
    | 0.718(-4.7%) | 0.746(-0.8%) | 0.752 |'
- en: '| Mother- boards | Advice Rate $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235
    | 0.483 | 0.285 | 0.235 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 $\downarrow$ | 0.270 | 0.368 | 0.000 | 0.235 | 0.483 | 0.285 | 0.235
    |'
- en: '| Accuracy $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.878 | 0.886 | 0.766 | 0.829 | 0.873 | 0.871 | 0.877 |'
- en: '| Total Score $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.797(-1.1%) | 0.776(-3.9%) | 0.766(-5.2%) | 0.758(-6.3%)
    | 0.728(-10.7%) | 0.786(-2.5%) | 0.806 |'
- en: '| Rifle Scopes | Advice Rate $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 |
    0.440 | 0.221 | 0.216 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 $\downarrow$ | 0.237 | 0.385 | 0.000 | 0.216 | 0.440 | 0.221
    | 0.216 |'
- en: '| Accuracy $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.824 | 0.858 | 0.733 | 0.783 | 0.824 | 0.805 | 0.828 |'
- en: '| Total Score $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.753(-1.3%) | 0.742(-2.8%) | 0.733(-4.1%) | 0.718(-6.3%)
    | 0.692(-10.3%) | 0.739(-3.2%) | 0.763 |'
- en: '| Average | Advice Rate $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492
    | 0.256 | 0.233 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 $\downarrow$ | 0.270 | 0.407 | 0.000 | 0.233 | 0.492 | 0.256 |
    0.233 |'
- en: '| Accuracy $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ | 0.852 | 0.876 | 0.747 | 0.812 | 0.865 | 0.843 | 0.854 |'
- en: '| Total Score $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 总分 $\uparrow$ | 0.771(-1.7%) | 0.754(-4.0%) | 0.747(-5.0%) | 0.742(-5.7%)
    | 0.717(-9.3%) | 0.766(-2.3%) | 0.784 |'
- en: 'Table 12: Performance of the model (agile-vic13b-ppo) trained on different
    seeking advice cost settings.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 表12：在不同寻求建议成本设置下模型（agile-vic13b-ppo）的表现。
- en: '| Group | Seeking Advice Cost |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 寻求建议成本 |'
- en: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 0.5 | 0.4 | 0.3 | 0.2 | 0.1 |'
- en: '| Camera Cases | Advice Rate | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 相机保护套 | 建议率 | 0.108 | 0.189 | 0.263 | 0.339 | 0.458 |'
- en: '| Accuracy | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.806 | 0.829 | 0.860 | 0.885 | 0.929 |'
- en: '| Leggings | Advice Rate | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 打底裤 | 建议率 | 0.098 | 0.188 | 0.251 | 0.317 | 0.464 |'
- en: '| Accuracy | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.824 | 0.844 | 0.876 | 0.877 | 0.921 |'
- en: '| All Pans | Advice Rate | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 所有锅具 | 建议率 | 0.094 | 0.163 | 0.220 | 0.262 | 0.384 |'
- en: '| Accuracy | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.813 | 0.845 | 0.866 | 0.889 | 0.926 |'
- en: '| Rollerball Pens | Advice Rate | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 滚珠笔 | 建议率 | 0.100 | 0.163 | 0.212 | 0.264 | 0.406 |'
- en: '| Accuracy | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.780 | 0.799 | 0.816 | 0.829 | 0.891 |'
- en: '| Motherboards | Advice Rate | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 主板 | 建议率 | 0.103 | 0.162 | 0.235 | 0.307 | 0.443 |'
- en: '| Accuracy | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.825 | 0.839 | 0.877 | 0.901 | 0.941 |'
- en: '| Rifle Scopes | Advice Rate | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 步枪瞄准镜 | 建议率 | 0.087 | 0.144 | 0.216 | 0.257 | 0.385 |'
- en: '| Accuracy | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.780 | 0.797 | 0.828 | 0.845 | 0.897 |'
- en: '| Average | Advice Rate | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 建议率 | 0.098 | 0.168 | 0.233 | 0.291 | 0.423 |'
- en: '| Accuracy | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.805 | 0.825 | 0.854 | 0.871 | 0.918 |'
- en: Appendix D Case study
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 案例研究
- en: 'Case #1, illustrated in Table [13](#A4.T13 "Table 13 ‣ Appendix D Case study
    ‣ AGILE: A Novel Framework of LLM Agents"), provides a specific example demonstrating
    how agile-vic13b-ppo proactively seeks advice from a human expert for questions
    it cannot answer. Furthermore, it leverages reflection to extract general knowledge
    from the expert’s responses, which can then be applied in future QA sessions.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '案例 #1，见表 [13](#A4.T13 "表 13 ‣ 附录 D 案例研究 ‣ AGILE: LLM 代理的新框架")，提供了一个具体示例，展示了
    agile-vic13b-ppo 如何主动寻求无法回答的问题的人类专家的建议。此外，它还利用反思从专家的回答中提取一般知识，以便在未来的问答环节中应用。'
- en: 'Case #2, shown in Table [14](#A4.T14 "Table 14 ‣ Appendix D Case study ‣ AGILE:
    A Novel Framework of LLM Agents"), demonstrates how agile-vic13b-ppo utilizes
    tools to address product recommendation questions.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '案例 #2，见表 [14](#A4.T14 "表 14 ‣ 附录 D 案例研究 ‣ AGILE: LLM 代理的新框架")，展示了 agile-vic13b-ppo
    如何利用工具解决产品推荐问题。'
- en: 'Table 13: Case #1 from agile-vic13b-ppo on the test task, camera cases group,
    in ProductQA. ✓represents correct responses. Our agent proactively seeks advice
    from human experts at session 313 and summarizes a piece of general knowledge
    by reflection. In the following sessions, the agent leverages this knowledge to
    successfully answer three questions for different products at sessions 721, 1265,
    and 1364, respectively. It demonstrates the effectiveness of seeking advice and
    reflection.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：来自agile-vic13b-ppo测试任务的案例#1，在ProductQA中的相机包组。✓表示正确的回答。我们的代理在会话313中主动寻求人工专家的建议，并通过反思总结了一部分常识。在接下来的会话中，代理利用这些知识成功回答了会话721、1265和1364中关于不同产品的三个问题，展示了寻求建议和反思的有效性。
- en: '| Session | Context | Action | Reflection / Answer |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 上下文 | 行动 | 反思 / 答案 |'
- en: '| 313 | Answer users’ [Question] about product B00PV0IEDY based on the following
    three types of information: 1\. [Product Metadata] … ‘Carrying Options’: ‘backpack
    straps’ … 2\. [Memory] Existing knowledge in your memory about camera_cases. camera
    cases with no tripod mount are not suitable for scenarios where a stable camera
    | Seek Advice | Human Expert: Certainly, the backpack straps on the camera case
    are designed for comfort even when carrying heavier loads. These straps enable
    ergonomic weight distribution, making the case ideal for prolonged travel. ✓ |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 313 | 根据以下三种信息回答用户关于产品B00PV0IEDY的[问题]：1\. [产品元数据] … ‘携带选项’：‘背包肩带’ … 2\. [记忆]
    关于相机包的现有知识。没有三脚架安装点的相机包不适合在需要稳定相机的场景中使用 | 寻求建议 | 人工专家：当然，相机包上的背包肩带设计考虑了舒适性，即使在携带较重负荷时也是如此。这些肩带能够实现符合人体工程学的重量分配，使得该包非常适合长时间旅行。✓
    |'
- en: '| setup is crucial, such as long-exposure photography or macro work where camera
    shake can significantly impact image quality.  3.[History] … … Question: Is the
    camera case designed for prolonged travel and capable of withstanding heavy loads?
    | Reflection: Camera cases with backpack straps feature ergonomic designs that
    help distribute the weight of the camera equipment across the user’s back, reducing
    strain and increasing comfort during transport. |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 设置至关重要，例如在长时间曝光摄影或微距摄影中，相机抖动可能会显著影响图像质量。  3.[历史] … … 问题：这个相机包是否设计用于长时间旅行，并能够承受重负荷？
    | 反思：配有背包肩带的相机包具有符合人体工程学的设计，有助于将相机设备的重量分布在用户的背部，从而减少负担，增加运输过程中的舒适度。 |'
- en: '| 721 | Answer users’ [Question] about product B000WH86BQ … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Is the camera case suitable for extended journeys
    and potentially heavy loads? | Predict Answer | Answer: The case is designed to
    be adjustable and has padded straps which makes it suitable for long-term use
    with heavy equipment. ✓ |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 721 | 回答用户关于产品B000WH86BQ的[问题] … 1\. [产品元数据] … ‘携带选项’：‘背包肩带’ … 2\. [记忆] 关于相机包的现有知识。配有背包肩带的相机包具有符合人体工程学的设计，帮助将相机设备的重量分布在用户的背部
    …  3.[历史] … 问题：这个相机包适合长时间旅行和承受较重负荷吗？ | 预测答案 | 答案：该包设计为可调节，并配有填充肩带，非常适合长时间使用和承载重型设备。✓
    |'
- en: '| 1265 | Answer users’ [Question] about product B005J09OXU … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Will this camera case enhance the comfort of transporting
    my gear on treks? | Predict Answer | Answer: Certainly, a camera case with backpack
    straps is engineered to evenly distribute the weight across your shoulders, improving
    ergonomics and comfort. This is particularly beneficial for reducing fatigue and
    strain during prolonged activities like treks. ✓ |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 1265 | 回答用户关于产品B005J09OXU的[问题] … 1\. [产品元数据] … ‘携带选项’：‘背包肩带’ … 2\. [记忆] 关于相机包的现有知识。配有背包肩带的相机包具有符合人体工程学的设计，帮助将相机设备的重量分布在用户的背部
    …  3.[历史] … 问题：这个相机包是否能提高在远足时携带装备的舒适度？ | 预测答案 | 答案：当然，配有背包肩带的相机包设计用于均匀分配肩部的重量，提高人体工程学和舒适度。这对于减少长时间活动如远足中的疲劳和压力尤其有益。✓
    |'
- en: '| 1364 | Answer users’ [Question] about product B000YA33DC … 1\. [Product Metadata]
    … ‘Carrying Options’: ‘backpack straps’ … 2\. [Memory] Existing knowledge in your
    memory about camera_cases. Camera cases with backpack straps feature ergonomic
    designs that help distribute the weight of the camera equipment across the user’s
    back …  3.[History] … Question: Does this camera case facilitate easier transportation
    when I frequently travel? | Predict Answer | Answer: Certainly, the backpack straps
    on this camera case not only simplify the process of carrying but also evenly
    distribute the weight, making it an ideal option for those who are frequently
    on the move and prefer a more comfortable and balanced carrying solution. ✓ |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| 1364 | 根据以下三种信息回答用户关于产品 B000YA33DC 的[问题] … 1\. [产品元数据] … ‘携带选项’：‘背包带’ … 2\.
    [内存] 关于相机包的现有知识。带有背包带的相机包具有符合人体工程学的设计，有助于将相机设备的重量均匀分配到用户的背部 …  3.[历史] … 问题：这个相机包是否便于我频繁旅行时的运输？
    | 预测回答 | 答案：当然，这个相机包上的背包带不仅简化了携带过程，而且均匀分配重量，是那些经常移动并喜欢更舒适和平衡携带解决方案的理想选择。 ✓ |'
- en: 'Table 14: Case #2 from agile-vic13b-ppo on the ProductQA test set. ✓represents
    correct responses. By leveraging SQL searching tools, agent successfully find
    the correct product to recommend to users.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '表 14：来自 agile-vic13b-ppo 的案例 #2 在 ProductQA 测试集上。✓ 代表正确的回答。通过利用 SQL 搜索工具，代理成功找到了推荐给用户的正确产品。'
- en: '| Session | Context | Action | Output |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 上下文 | 操作 | 输出 |'
- en: '| 918 | Answer users’ [Question] about product B00FUHWE5M based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B00FUHWE5M from the leggings database. The schema of the leggings database is
    outlined below (in the format field[unit](value1, value2, …)). price[dollar] brand(Jescakoo,
    QZUnique, …) warmth(hot,moderate,cool) … 2\. [Memory] … 3\. [History] … … Question:
    I’ve heard good things about QZUnique. Spit out the first leggings you have from
    them. … | Search Product | SQL: SELECT product_id FROM “leggings” WHERE brand
    = “QZUnique” LIMIT 1;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '| 918 | 根据以下三种信息回答用户关于产品 B00FUHWE5M 的[问题]：1\. [产品元数据]提供来自打底裤数据库的产品 B00FUHWE5M
    的元数据。打底裤数据库的模式如下（格式为字段[单位](值1, 值2, …)）。价格[美元] 品牌(Jescakoo, QZUnique, …) 保暖度(热,
    适中, 凉爽) … 2\. [内存] … 3\. [历史] … … 问题：我听说 QZUnique 很不错。给我推荐第一款他们的打底裤。 … | 搜索产品
    | SQL: SELECT product_id FROM “leggings” WHERE brand = “QZUnique” LIMIT 1;'
- en: 'Answer: The product that meets your requirements is as follows. ‘QZUnique Women’s
    Classic Retro Printed Casual Pattern Ankle Length Elastic Tights Leggings’ with
    asin: B012IKGF62 ✓ |'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：符合您要求的产品如下：‘QZUnique 女士经典复古印花休闲图案脚踝长度弹力紧身裤’，asin: B012IKGF62 ✓ |'
- en: '| 1298 | Answer users’ [Question] about product B012AQGKXC based on the following
    three types of information: 1\. [Product Metadata] provides metadata about product
    B012AQGKXC from the motherboards database. The schema of the motherboards database
    is outlined below (in the format field[unit](value1, value2, …)). price[dollar]
    brand(ASRock, Gigabyte, …) memory_support[GB] … 2\. [Memory] … 3\. [History] …
    … Question: I’m looking for an ASRock motherboard with at least 32 GB of memory
    support. Don’t keep me waiting, hustle up and find it for me. … | Search Product
    | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND memory_support
    >= 32 LIMIT 1;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '| 1298 | 根据以下三种信息回答用户关于产品 B012AQGKXC 的[问题]：1\. [产品元数据]提供来自主板数据库的产品 B012AQGKXC
    的元数据。主板数据库的模式如下（格式为字段[单位](值1, 值2, …)）。价格[美元] 品牌(ASRock, Gigabyte, …) 内存支持[GB]
    … 2\. [内存] … 3\. [历史] … … 问题：我在寻找一款支持至少 32 GB 内存的 ASRock 主板。别让我等待，赶紧找给我。 … | 搜索产品
    | SQL: SELECT product_id FROM “motherboards” WHERE brand = “ASRock” AND memory_support
    >= 32 LIMIT 1;'
- en: 'Answer: The product that would suit your preferences is: ‘AS Rock LGA1155 DDR3
    SATA3 USB3.0 Quad CrossFireX and Quad SLI A E ATX Motherboard Z77 EXTREME4’ with
    asin: B007KTY4A6 ✓ |'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '答案：符合您要求的产品是：‘AS Rock LGA1155 DDR3 SATA3 USB3.0 Quad CrossFireX 和 Quad SLI
    A E ATX 主板 Z77 EXTREME4’，asin: B007KTY4A6 ✓ |'
- en: Appendix E Development of the ProductQA dataset
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 产品QA 数据集的开发
- en: E.1 Product collection
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 产品收集
- en: The product groups and the corresponding products are collected by the following
    steps.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 收集产品组和相应产品的步骤如下。
- en: '1.'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Filter the Amazon Review Data to retain only products with at least 100 reviews,
    then cluster them by category tags.
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过滤亚马逊评论数据，只保留至少有 100 条评论的产品，然后按类别标签进行聚类。
- en: '2.'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Sort the clusters by size, from largest to smallest. Manually review each cluster
    in order: we keep product clusters that involve diverse technical details and
    long-tail domain knowledge, such as electronics, from which we can potentially
    construct a diverse set of user questions. The manual review ends when we have
    collected 26 clusters. Each cluster is referred to as a *product group*.'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按大小对集群进行排序，从大到小。手动审查每个集群：我们保留涉及各种技术细节和长尾领域知识的产品集群，例如电子产品，从中我们可以潜在地构建多样化的用户问题。当我们收集到26个集群时，手动审查结束。每个集群被称为*产品组*。
- en: '3.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: For each product group, we remove the top 10% of products with the highest number
    of reviews. We exclude these most popular products from the datasets to prevent
    data leakage, as information about them is likely included in the pre-training
    set of LLMs. From the remaining items, we randomly select up to 20 products to
    form the final product set.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个产品组，我们会移除评论数量最多的前10%的产品。我们从数据集中排除这些最受欢迎的产品，以防数据泄露，因为关于这些产品的信息可能已包含在LLM的预训练集里。从剩余的项目中，我们随机选择最多20个产品来形成最终的产品集。
- en: E.2 Annotation guidelines
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 注释指南
- en: There are two annotation tasks, product table creation and QA collection. We
    provide the annotation guidelines in this Section.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个注释任务，产品表创建和QA收集。我们在本节中提供注释指南。
- en: 'Task 1: Product table creation'
  id: totrans-415
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务1：产品表创建
- en: 'For each product group, we provide a series of features and their corresponding
    values for each product in the group. This information is obtained by prompting
    GPT-4 to extract data from the reviews of each product. The task of annotators
    is to construct a product table containing only the metadata. Please follow these
    steps:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个产品组，我们提供一系列特征及其对应的值。此信息是通过提示GPT-4从每个产品的评论中提取的。注释员的任务是构建仅包含元数据的产品表。请按照以下步骤操作：
- en: '1.'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Select up to 15 common features relevant to the product group. These features
    must include product ID, product title, brand, and price. Choose additional features
    based on their commonality and necessity within the product group.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 选择与产品组相关的最多15个常见特征。这些特征必须包括产品ID、产品标题、品牌和价格。根据它们在产品组中的普遍性和必要性选择额外特征。
- en: '2.'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: For each product in the product group, verify the feature values for each selected
    feature.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于产品组中的每个产品，验证每个选择的特征的特征值。
- en: Finally, the product tables are reviewed and refined by the authors.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，作者会对产品表进行审查和完善。
- en: 'Task 2: QA collection'
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 任务2：QA收集
- en: 'Annotators are required to fill out a table as shown in Table [15](#A5.T15
    "Table 15 ‣ Task 2: QA collection ‣ E.2 Annotation guidelines ‣ Appendix E Development
    of the ProductQA dataset ‣ AGILE: A Novel Framework of LLM Agents"). Each row
    contains a triplet consisting of a *question*, a *long answer*, and a *short answer*,
    all generated by GPT-4\. Annotators should fill the following columns: *Is question
    reasonable*, *Is long answer correct*, *Refined long answer*, *Is short answer
    correct* and *Refined short answer*. Please follow these steps:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '注释员需要填写如表[15](#A5.T15 "Table 15 ‣ Task 2: QA collection ‣ E.2 Annotation guidelines
    ‣ Appendix E Development of the ProductQA dataset ‣ AGILE: A Novel Framework of
    LLM Agents")所示的表格。每行包含由GPT-4生成的*问题*、*长答案*和*短答案*的三元组。注释员应填写以下列：*问题是否合理*、*长答案是否正确*、*精炼的长答案*、*短答案是否正确*和*精炼的短答案*。请遵循这些步骤：'
- en: '1.'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Evaluate the question: Verify if the *question* resembles a typical query found
    in real-world product conversations in online shopping. Select ‘yes’ or ‘no’ in
    the *Is question reasonable* column. Any question containing harmful information
    is considered unreasonable and should be labeled as ‘no’. If ‘no’ is selected,
    the row will be dropped, and you do not need to proceed with the subsequent steps
    for that row.'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估问题：验证*问题*是否类似于现实世界在线购物对话中的典型查询。在*问题是否合理*列中选择“是”或“否”。任何包含有害信息的问题都被视为不合理，应该标记为“否”。如果选择了“否”，则该行将被删除，你无需继续处理该行的后续步骤。
- en: '2.'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Assess the long answer: Check if the *long answer* correctly responds to the
    *question*. Select ‘yes’, ‘no’ or ‘I do not know’ in the *Is long answer correct*
    column. Consider the following special cases:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估长答案：检查*长答案*是否正确回应了*问题*。在*长答案是否正确*列中选择“是”、“否”或“我不知道”。请考虑以下特殊情况：
- en: •
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer is ambiguous (e.g., ‘The product is designed to be waterproof,
    while some users do not think so.’), mark it as incorrect.
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案模糊（例如，“该产品设计为防水，但一些用户并不这样认为。”），则标记为不正确。
- en: •
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For numerical questions, an answer is considered correct if it fits the real-world
    scenario and the conclusion is clear. Specific values or ranges (e.g., 5cm, 5cm-10cm,
    several months) are acceptable if they correspond to the real-world scenario.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于数值问题，如果答案符合实际情况且结论清晰，则被视为正确。具体值或范围（例如，5cm、5cm-10cm、几个月）如果与实际情况相符也是可以接受的。
- en: •
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer contains a specific piece of knowledge, verify its accuracy.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案包含特定知识，请验证其准确性。
- en: •
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the long answer is incorrect or does not address the question, and you do
    not know the correct answer (even after checking the product information table,
    looking up the product URL, and searching online), select ‘I do not know’.
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果长答案不正确或没有回答问题，且你不知道正确答案（即使在查看产品信息表、查找产品网址和在线搜索后），请选择“我不知道”。
- en: •
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Any long answer containing harmful information should be labeled as ‘I do not
    know’.
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何包含有害信息的长答案应标记为“我不知道”。
- en: If you select ‘I do not know’, the row will be dropped, and you do not need
    to perform the subsequent steps for that row.
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你选择“我不知道”，该行将被丢弃，你无需对该行执行后续步骤。
- en: '3.'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Refine the long answer: If you select ‘no’ in step 2, provide a correct long
    answer in the *Refined long answer* column.'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精炼长答案：如果在第2步选择“否”，请在*精炼长答案*列中提供正确的长答案。
- en: '4.'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Assess the short answer: Determine whether the *short answer* is correct. A
    short answer must be ‘yes’, ‘no’, or an entity. Choose ‘yes’ or ‘no’ in the *Is
    short answer correct* column. Consider the following special cases:'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估简短答案：确定*简短答案*是否正确。简短答案必须是“是”、“否”或一个实体。在*简短答案是否正确*列中选择“是”或“否”。考虑以下特殊情况：
- en: •
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the question is a choice and the short answer is ‘yes’ or ‘no’, it is incorrect.
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果问题是选择题，且简短答案为“是”或“否”，则答案不正确。
- en: •
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the question pertains to degrees (e.g. ‘How durable … ?’) and the short answer
    is ‘yes’ or ‘no’, it is incorrect.
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果问题涉及程度（例如，“耐用性如何…？”）且简短答案为“是”或“否”，则答案不正确。
- en: •
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the short answer does not align with the long answer, it is incorrect.
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果简短答案与长答案不一致，则不正确。
- en: '5.'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Refine the short answer: If you select ‘no’ in step 4, provide a correct short
    answer in the *Refined short answer* column.'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精炼简短答案：如果在第4步选择“否”，请在*精炼简短答案*列中提供正确的简短答案。
- en: The authors will review the annotation in batches. Specifically, 5% of each
    batch will be checked. If the accuracy rate of the checked annotation is below
    98%, the entire batch will be relabeled.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将分批审核注释。具体来说，每批的5%将被检查。如果检查的注释准确率低于98%，整个批次将被重新标记。
- en: 'Table 15: An example of the ProductQA annotation table.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：ProductQA注释表的示例。
- en: '| Question | Is question | Long answer | Is long answer | Refined | Short answer
    | Is short answer | Refined |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 问题是否 | 长答案 | 长答案是否 | 精炼 | 简短答案 | 简短答案是否 | 精炼 |'
- en: '| reasonable | correct | long answer | correct | short answer |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 合理 | 正确 | 长答案 | 正确 | 简短答案 |'
- en: '| What is the size of the neodymium driver used in the JVC HA-EB75 headphones?
    | [To fill] | The JVC HA-EB75 headphones contain a 13.5 mm neodymium driver in
    each earpiece, which contributes to the enhanced sound quality. | [To fill] |
    [To fill] | 13.5 mm | [To fill] | [To fill] |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| JVC HA-EB75耳机使用的钕驱动单元的尺寸是多少？ | [待填写] | JVC HA-EB75耳机每个耳塞内含有一个13.5毫米的钕驱动单元，这有助于提升音质。
    | [待填写] | [待填写] | 13.5毫米 | [待填写] | [待填写] |'
- en: Appendix F Prompt templates
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 提示模板
- en: Prompt templates for ProductQA
  id: totrans-457
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 产品QA的提示模板
- en: 'Figure [5](#A6.F5 "Figure 5 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents") shows the prompt template
    for gpt3.5-prompt, gpt4-prompt. Figure [6](#A6.F6 "Figure 6 ‣ Prompt templates
    for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents")
    provides the prompt template for agile-vicuna-13b-prompt, agile-gpt3.5-prompt,
    and agile-gpt4-prompt. We leave the "{knowledge} and "{history}" empty when evaluate
    gpt3.5-prompt and gpt4-prompt. The prompt template for reflection is shown in
    Figure [7](#A6.F7 "Figure 7 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents").'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#A6.F5 "图 5 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: 一种新的 LLM 代理框架")展示了gpt3.5-prompt、gpt4-prompt的提示模板。图[6](#A6.F6
    "图 6 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: 一种新的 LLM 代理框架")提供了agile-vicuna-13b-prompt、agile-gpt3.5-prompt和agile-gpt4-prompt的提示模板。在评估gpt3.5-prompt和gpt4-prompt时，我们将"{knowledge}"和"{history}"留空。反射的提示模板见图[7](#A6.F7
    "图 7 ‣ MedMCQA 提示模板 ‣ 附录 F 提示模板 ‣ AGILE: 一种新的 LLM 代理框架")。'
- en: 'The prompt template for long answer evaluation is shown in Figure [8](#A6.F8
    "Figure 8 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE:
    A Novel Framework of LLM Agents").'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 长答案评估的提示模板见图[8](#A6.F8 "图8 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE：一种新型LLM代理框架")。
- en: Prompt templates for MedMCQA
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MedMCQA的提示模板
- en: 'Figure [9](#A6.F9 "Figure 9 ‣ Prompt templates for MedMCQA ‣ Appendix F Prompt
    templates ‣ AGILE: A Novel Framework of LLM Agents") provides the prompt template
    for Meerkat-7b-promp. Figure [10](#A6.F10 "Figure 10 ‣ Prompt templates for MedMCQA
    ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework of LLM Agents") illustrates
    the prompt template for agile-gpt3.5-prompt, agile-gpt4-prompt. We leave the "{related_question}
    and "{related_knowledge}" empty when evaluate gpt3.5-prompt and gpt4-prompt. The
    prompt template for reflection is shown in Figure [11](#A6.F11 "Figure 11 ‣ Prompt
    templates for MedMCQA ‣ Appendix F Prompt templates ‣ AGILE: A Novel Framework
    of LLM Agents").'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](#A6.F9 "图9 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE：一种新型LLM代理框架")提供了Meerkat-7b-prompt的提示模板。图[10](#A6.F10
    "图10 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE：一种新型LLM代理框架")展示了agile-gpt3.5-prompt和agile-gpt4-prompt的提示模板。在评估gpt3.5-prompt和gpt4-prompt时，我们将"{related_question}"和"{related_knowledge}"留空。图[11](#A6.F11
    "图11 ‣ MedMCQA的提示模板 ‣ 附录F 提示模板 ‣ AGILE：一种新型LLM代理框架")展示了反思的提示模板。
- en: '![Refer to caption](img/b8f2d9e157ef973cc7edef697f5a3458.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8f2d9e157ef973cc7edef697f5a3458.png)'
- en: 'Figure 5: The prompt for gpt3.5-prompt and gpt4-prompt on ProductQA.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：ProductQA中gpt3.5-prompt和gpt4-prompt的提示。
- en: '![Refer to caption](img/5dba15a3e003e34c49dd2fa514d1c440.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5dba15a3e003e34c49dd2fa514d1c440.png)'
- en: 'Figure 6: The prompt for agile-vicuna-13b-prompt, agile-gpt3.5-prompt, and
    agile-gpt4-prompt on ProductQA.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：ProductQA中agile-vicuna-13b-prompt、agile-gpt3.5-prompt和agile-gpt4-prompt的提示。
- en: '![Refer to caption](img/eb0af70333b2eedd94917368c390479e.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb0af70333b2eedd94917368c390479e.png)'
- en: 'Figure 7: The prompt for reflection on ProductQA.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：ProductQA中反思的提示。
- en: '![Refer to caption](img/e5645d4b1b9b6f42f853a91b29f896b4.png)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e5645d4b1b9b6f42f853a91b29f896b4.png)'
- en: 'Figure 8: The prompt for long answer evaluation on ProductQA.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：ProductQA中长答案评估的提示。
- en: '![Refer to caption](img/ec4d4bc5fb9d10d7589fe193311162a4.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec4d4bc5fb9d10d7589fe193311162a4.png)'
- en: 'Figure 9: The prompt for Meerkat-7b-prompt on MedMCQA.'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：MedMCQA中Meerkat-7b-prompt的提示。
- en: '![Refer to caption](img/40880a4f862b06e21276adbbce4f5867.png)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/40880a4f862b06e21276adbbce4f5867.png)'
- en: 'Figure 10: The prompt for agile-gpt3.5-prompt and agile-gpt4-prompt on MedMCQA.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：MedMCQA中agile-gpt3.5-prompt和agile-gpt4-prompt的提示。
- en: '![Refer to caption](img/3eb3b0c3a58ed7a1cd6e3928dae2383d.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3eb3b0c3a58ed7a1cd6e3928dae2383d.png)'
- en: 'Figure 11: The prompt for reflection on MedMCQA.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：MedMCQA中反思的提示。
