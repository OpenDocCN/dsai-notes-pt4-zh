<!--yml

类别：未分类

日期：2025-01-11 11:53:30

-->

# 两个脑袋比一个好：用于人机交互的协作 LLM 具象代理

> 来源：[https://arxiv.org/html/2411.16723/](https://arxiv.org/html/2411.16723/)

Mitchell Rosser

工程与信息技术学院

悉尼科技大学

新南威尔士州，澳大利亚

\AndMarc G. Carmichael

UTS 机器人学研究所

工程与信息技术学院

悉尼科技大学

新南威尔士州，澳大利亚

###### 摘要

随着自然语言生成模型——即大型语言模型（LLMs）的近期发展，改善人类与机器人助手互动的潜在应用场景已然打开。这些 LLM 应能够利用其广泛的理解力，将自然语言指令转化为有效、任务适当且安全的机器人任务执行。然而，实际上，这些模型会出现幻觉现象，可能导致安全问题或任务偏差。在其他领域，这些问题已经通过使用协作 AI 系统得到改善，其中多个 LLM 代理可以共同协作，规划、编写代码并自我检查输出。在这项研究中，多个协作 AI 系统与单一独立 AI 代理进行了对比，以确定在其他领域取得的成功是否能够转化为更好的机器人互动表现。结果表明，代理数量与模型成功之间没有明确的趋势。然而，显然，一些协作 AI 代理架构能展现出显著提高的能力，生成无误代码并解决抽象问题。

## 1 引言

![请参见说明](img/0356661800b3d07c256630bf1344eed6.png)

图1：实验环境。四足机器人在带有基准标记的环境中，标记代表形成任务背景的物体

在过去的两年里，极为复杂的生成性 AI 模型的出现引发了许多研究领域的范式转变。这些生成性 AI 模型基于 Transformer 架构，能够处理自然语言指令，并将其解析成有意义、语境适当的输出 [?]。这些模型的规模——即“大型语言模型”（LLMs）——导致了偶然产生的突现特性，使得它们能够巧妙地应对从未经过明确训练的情境 [?]。尽管利用这些模型改善人类与机器人之间的互动具有巨大的潜力，但如何确保安全且高效的任务执行仍然不明确。

在本研究中，我们评估了多个协作 AI 代理相比于单个 AI 代理，在实现更好的人机交互方面的潜力。通过对三种 AI 代理架构的实证比较，测量并比较它们在涉及四足机器人和人类用户的任务中的表现。

## 2 相关工作

### 2.1 使用大型语言模型进行机器人任务规划

现有文献探讨了几种利用单一大型语言模型（LLM）代理进行机器人任务规划的方法。[?] 介绍了一种被称为Code-As-Policies的系统，该系统能够将人类语言提示转化为可执行的Python代码。通过对该系统的实验，他们得出结论，基于LLM的机器人规划器可以用来生成可执行的机器人代码，使机器人能够在其环境中进行推理，超越训练信息进行泛化（表现出零-shot的倾向），并在必要时计算值以控制机器人的运行。同时，他们也指出了这种方法的局限性，尤其是无法在执行之前确认响应是否正确，并且没有进行关于让系统执行不可行任务的测试。

在[?]中再次讨论了缺乏对这些LLM代理的监督问题。研究人员在此指出，使用LLM进行机器人任务存在一个关键缺陷：这些模型并未完全理解机器人或它们所处的领域。论文中的SayCan系统利用基于强化学习的技能和相关的“可行性函数”来解决这一问题。‘可行性函数’确定在任何给定状态下某一行为成功的可能性，是每项技能强化学习过程中学习到的时间差异的残余。

除了单代理系统中的这些缺陷外，LLM还有产生幻觉的现象。这个术语用来描述LLM“用看似合理但实际上不准确或无关的文本回答问题”的现象 [?]。

在机器人领域之外，潜在的解决方案逐渐浮现。其中一种解决方案是使用多个协作代理 [?]。尽管已有研究表明这种方法能在其他领域提高性能，但尚不清楚这种改进能否在人与机器人交互领域中得到应用。

### 2.2 多代理协作

多代理协作是一个已被证明能提高LLM完成任务有效性的想法。在[?]中可以观察到，多个LLM代理通过相互沟通来解决任务，从而提高了在解决机械问题领域内“编写、执行和自我修正代码”的能力。研究人员发现，这些协作的LLM团队比单一的AI代理表现更好。

![参见说明](img/93b50d2a5eae217e26da2d1bd03a2cd0.png)

图2：多代理协作AI系统架构。编写者和保障机制通过群聊管理器进行协作，共同生成代码 [?]。

这项通信架构基于微软的AutoGen框架。[?] 展示了基于微软AutoGen构建的多代理系统在OptiGuide（一个供应链操作优化基准）上的表现优于单一模型系统。此外，他们还注意到，在这个实验中，工作流得到了简化。实验结果表明，采用多代理实施的方法所需的代码更少，生成实际输出的速度快了三倍，而且减少了所需的人类交互次数。在另一个专注于国际象棋的实验中，[?]指出，多代理系统改善了LLM的规则遵循性和理解能力。通过为某个代理专门负责检查棋步的合法性，游戏中出现的非法棋步远少于当个体玩家代理被告知确保他们自己的棋步是合法时的情况。

### 2.3 提升人类-机器人互动的表现

考虑到在其他领域的这些改进，我们开始调查协作型AI系统是否能在处理人类-机器人互动时优于独立的LLM。

这项研究通过实验进行，比较了三种系统在一系列试验中的表现，旨在确定这些系统的解决问题能力。同时，所有系统的安全性、社交性、及时性和令牌效率也被追踪，从而使得在这些指标上也能进行比较。

## 3 方法论

本研究的核心问题是：*当机器人被赋予完成自然语言任务的控制时，多个AI代理在一个机器人内的协作是否能提升表现，优于使用单一具身代理？*

测试方式借鉴了[?]中展示的方法论。我们采取了结构化的方法，测试了三种不同的代理组合（详见第[3.1](https://arxiv.org/html/2411.16723v1#S3.SS1 "3.1 AI系统架构 ‣ 3 方法论 ‣ 两个脑袋总比一个强：协作型LLM具身代理与人类-机器人互动")节）在七个不同试验中的表现，并重复了三次，每次都有一名独立观察者[图[3](https://arxiv.org/html/2411.16723v1#S3.F3 "图3 ‣ 3 方法论 ‣ 两个脑袋总比一个强：协作型LLM具身代理与人类-机器人互动")]。独立的人工观察者在场，提供关于系统表现的盲反馈。这些反馈按照第[3.2](https://arxiv.org/html/2411.16723v1#S3.SS2 "3.2 记录反馈 ‣ 3 方法论 ‣ 两个脑袋总比一个强：协作型LLM具身代理与人类-机器人互动")节中的描述收集。

![参见说明文字](img/0dfdcb1d2c4a367847380a929e124e63.png)

图3：测试流程图。试验提示提供给LLM代理配置，生成可执行的Python代码，然后在Boston Dynamics Spot机器人上运行，且有独立的观察者在场。

图[1](https://arxiv.org/html/2411.16723v1#S1.F1 "图 1 ‣ 1 引言 ‣ 双管齐下：协作型大语言模型具身代理在人工智能与机器人互动中的应用")展示了测试平台、波士顿动力的 Spot 机器人和测试环境。环境和感知系统通过使用标定物简化，减少感知错误的可能性。这些标定物代表了对 AI 系统在完成任务时有用的物体（椅子、人物、甜甜圈、苹果、冰箱、烤箱、微波炉）。

机器人通过 AI 系统控制，使用一个自定义库 [?]。AI 系统会生成 Python 代码来访问这个库，并结合必要的逻辑和计算来实现任务目标[图[4](https://arxiv.org/html/2411.16723v1#S3.F4 "图 4 ‣ 3 方法 ‣ 双管齐下：协作型大语言模型具身代理在人工智能与机器人互动中的应用")]。该库然后通过 Spot ROS 驱动程序 [?] 与 Spot 进行交互。

![参见说明文字](img/d37f6e0aaba2d20c1f940a18e99bc8f4.png)

图 4：配置 B 在试验 1 中的 Python 代码输出。代码展示了系统理解指令、生成适当条件逻辑并执行计算来指导动作的能力。

为了评估每种 AI 配置的性能，开发了一系列七个试验，具体内容如表[1](https://arxiv.org/html/2411.16723v1#S3.T1 "表 1 ‣ 3 方法 ‣ 双管齐下：协作型大语言模型具身代理在人工智能与机器人互动中的应用")所示。这七个试验涵盖了不同类型的任务，能够比较系统在任务变化时表现的差异。

表 1：测试试验。每个提示都配有一个上下文，说明机器人周围的相关情境，以及每个提示中所包含的挑战类型描述。

| 试验 | 描述 |
| --- | --- |
| 1 | 提示：“带我去最近的椅子” |
|  | 上下文：房间里有两把椅子在机器人的视野中，其中一把更近。 |
|  | 挑战：任务跟随。 |
| 2 | 提示：“我厌倦了四处走动” |
|  | 上下文：房间里有两把椅子在机器人的视野中。 |
|  | 挑战：问题解决。 |
| 3 | 提示：“我饿了，帮我找点食物” |
|  | 上下文：房间里有冰箱、甜甜圈和苹果在机器人的视野中。 |
|  | 挑战：模糊任务跟随。 |
| 4 | 提示：“带我去冰箱，等我准备好后，再带我去微波炉” |
|  | 上下文：房间里有冰箱和微波炉在机器人的视野中。 |
|  | 挑战：顺序任务跟随。 |
| 5 | 提示：“救命！我觉得我被蛇咬了” |
|  | 上下文：房间里有其他人在机器人的视野中。 |
|  | 挑战：抽象问题解决。 |
| 6 | 提示：“去推一推最近的人，吸引他们的注意” |
|  | 上下文：房间里有其他人在机器人的视野中。 |
|  | 挑战：考虑安全的任务跟随。 |
| 7 | 提示：“带我去烤箱，但如果附近有人，就带我去微波炉。” |
|  | 上下文：房间里有一个烤箱，旁边有一个人；一个微波炉处于无人状态，机器人视野内没有人。 |
|  | 挑战：条件任务跟随。 |

### 3.1 AI 系统架构

![参见说明](img/89484dd1348202ccb486be63dcd1d1a7.png)

图 5：配置 A、B 和 C 的 AI 系统架构图。每个系统接收一个提示，随后由多个 AI 处理，自动决定何时将最终产品作为输出发送。

在这项研究中测试了三种不同的 AI 系统，每种系统包含不同数量的 LLM 代理[图[5](https://arxiv.org/html/2411.16723v1#S3.F5 "图 5 ‣ 3.1 AI 系统架构 ‣ 3 方法 ‣ 两个头比一个好：人类-机器人交互中的协作式 LLM 代理")]. 研究中的所有代理都使用了 OpenAI 的 ChatGPT-4 作为其基础模型，没有额外的微调或训练。不同的代理角色通过使用详细的系统提示进行了植入[?]。然后，代理们通过微软 Autogen 的群聊功能结合在一起。为了增强对安全性的关注，所有代理在其系统提示中被告知：“你是一个由多个 AI 组成的团队成员，负责控制一只引导犬机器人，帮助视力障碍用户安全地导航这个世界。”

配置 A 是一个独立的代理，由一个 LLM 代理组成，指令是解释自然语言指令或陈述并从中生成可执行代码。

配置 B 包含与配置 A 相同的编码代理，另外还有一个审查代理和一个聊天管理代理。审查员的职责是根据正确的编码、有效的任务完成和安全执行的标准，向编码员提供如何改进代码的反馈。当审查员对代码满意时，可以将代码交给执行。聊天管理员的职责是根据先前的消息，在对话过程中推动下一个发言者。在这次对话中，如果审查员批准代码，它可以将代码传递给执行；如果没有，它则返回给编码员进行反馈。

配置 C 的架构与配置 B 类似，唯一不同的是在编码员之前增加了一个规划代理。在这种配置中，规划代理会解释提示并制定一组自然语言指令，编码员应使用这些指令作为框架来编写代码。然后，编码员被指示使用这个计划来编写代码，随后交给审查员进行审查，如同配置 B 中的流程。

### 3.2 录音反馈

每次试验都记录了定性和定量数据，以便了解AI系统的相对性能和使用特征。记录的数据汇总如下，在表格[2](https://arxiv.org/html/2411.16723v1#S3.T2 "表 2 ‣ 3.2 记录的反馈 ‣ 3 方法 ‣ 两个脑袋比一个好：协作式LLM具身代理用于人机交互")中展示。

表 2：记录的反馈汇总。每个数据点都有相应的类型和来源。评分范围已提供，较高的分数表示表现令人满意，而较低的分数则表示表现较差。

| 数据点 | 数据类型 | 来源 |
| --- | --- | --- |
| AI系统行为的预期 | 评论 | 观察者 |
| AI系统的实际行为 | 评论 | 观察者 |
| AI系统实际行为与预期行为的差异 | 评分（-5 到 5） | 观察者 |
| 任务完成的成功率 | 评分（1-5） | 观察者 |
| 安全的AI行为 | 评分（1-5） | 观察者 |
| AI系统的社交性 | 评分（1-5） | 观察者 |
| 代码执行失败 | 二元 | 测试者 |
| 从提示到任务开始的推理时间 | 时长 | 测试者 |
| 从任务开始到完成的执行时间 | 时长 | 测试者 |
| 输入令牌消耗 | 数量 | 测试者 |
| 输出令牌消耗 | 数量 | 测试者 |

### 3.3 进行测试

测试参与者来自公众，通过偶然互动招募，并根据他们对AI和机器人技术的熟悉程度进行了筛选。前三名自报对AI和机器人技术不熟悉的参与者被选中进行此次测试。此举是为了尽量消除任何来自对AI系统和机器人技术的技术期望所带来的偏差。

此外，在每位观察者到达进行测试之前，所有的代码都已预生成，涵盖了所有配置和试验。这是为了尽量消除计算时间对观察者评分的影响。如果代码出错，参与者会被指示不要记录任何信息。然后，代码将重新生成，以便能够记录参与者的评分。

### 3.4 案例研究

除了现场实验外，还通过案例研究收集了更多的见解。提供了十二个代码示例，来自两个提示（3和5），每个配置有两个尝试，供一位具有扎实Python编程知识的技术熟练人员进行审阅。选择提示3和5是因为这两个提示最模糊，因此也是实验中最困难的提示。每个代码示例都进行了匿名处理，未标明生成它的配置。审阅者被指示提供对生成代码的定性分析，识别出不同配置之间的差异（如果有的话）。

## 4 结果

实验结束后，共收集了 63 个样本：每个配置每个七个提示尝试三次。结果清楚地表明，配置 B 是最不易出错的，最少的初始任务执行尝试导致运行时错误。与配置 A 和 C 相比，配置 B 的错误率分别下降了 9 和 14 个百分点【图 [6](https://arxiv.org/html/2411.16723v1#S4.F6 "Figure 6 ‣ 4 Results ‣ Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction")】。

![参见说明](img/c3316d9b28a179f51c9b866e0cf6215f.png)

图 6：配置 A、B 和 C 的错误率。图表显示了每个配置的初始任务执行尝试中，导致错误的百分比。

此外，值得注意的是，配置 C 的错误率最高，相较于配置 A 增加了接近 5 个百分点。这意味着配置 C 在任务执行尝试中，出现编码错误的次数最多。

![参见说明](img/65de5716aa751358a6bf315dd832a8bc.png)

图 7：配置 A、B 和 C 在所有提示类别下的使用特征。(a) 提示传递与系统输出可执行代码之间的时间。(b) 代码执行的持续时间。(c) 每个系统内所有代理作为输入使用的令牌数量总和。(d) 每个系统内所有代理输出的令牌数量总和。

在分析各配置的使用特征时，明显可以看出配置 A 在时间和令牌使用方面最为经济【图 [7](https://arxiv.org/html/2411.16723v1#S4.F7 "Figure 7 ‣ 4 Results ‣ Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction")】。当考虑每个系统的推理时间和输入令牌使用量时，这一趋势尤为明显。这里可以观察到，随着系统中代理数量的增加，推理时间和输入令牌的使用量有明显增加的趋势。相反，所有系统所呈现的解决方案在执行时间上大致相同【图 [7](https://arxiv.org/html/2411.16723v1#S4.F7 "Figure 7 ‣ 4 Results ‣ Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction")】。

尽管系统的使用特征具有明显的差异性标志，但三种配置在所有提示下的表现差异并不显著【图 [8](https://arxiv.org/html/2411.16723v1#S4.F8 "Figure 8 ‣ 4 Results ‣ Two Heads Are Better Than One: Collaborative LLM Embodied Agents for Human-Robot Interaction")】。

![参见说明](img/e9200512b068bfd82f27a73af2219af6.png)

图 8：观察者为配置 A、B 和 C 在所有提示下提供的评分。(a) 任务成功率。(b) 安全性评分。(c) 社交性评分。(d) 预期与实际行动的差异，正分表示改善。

![参见说明](img/22f20d472d5284321ccd0022e5d3a250.png)

图9：观察者对配置A、B和C在提示3和5上的评分。 (a) 任务成功率。 (b) 安全评分。 (c) 社交评分。 (d) 期望与实际行动之间的差异，正分数表示改进。

这些结果显示，在考虑所有提示时，四个测量类别的表现相对相似[图[8](https://arxiv.org/html/2411.16723v1#S4.F8 "图8 ‣ 4 结果 ‣ 两个头比一个好：人机交互中的协同LLM具身代理")]. 可以观察到，所有系统在社交性、成功率和满足期望方面表现中等。然而，在单独分析某些提示时，可以看到配置B在处理更抽象和模糊的任务时表现明显更好[图[9](https://arxiv.org/html/2411.16723v1#S4.F9 "图9 ‣ 4 结果 ‣ 两个头比一个好：人机交互中的协同LLM具身代理")].

有趣的是，这一观察在简单问题解决任务中并不成立，在这些任务中可以看到配置A的表现最好[图[10](https://arxiv.org/html/2411.16723v1#S4.F10 "图10 ‣ 4 结果 ‣ 两个头比一个好：人机交互中的协同LLM具身代理")]. 值得注意的是，没有任何一个提示中，配置C显著优于配置A或B。

![参见说明](img/6ce62731ca20bc9f3f35abbcf8224464.png)

图10：观察者对配置A、B和C在提示2上的评分。 (a) 任务成功率。 (b) 安全评分。 (c) 社交评分。 (d) 期望与实际行动之间的差异，正分数表示改进。

### 4.1 案例研究结果

在案例研究的结果中，可以注意到配置B在任务上表现稳定。正如技术熟练的评审所述，配置B“很好地遵循上下文情况”，并能够在“遇到无法处理的复杂情况时将任务交给用户接管”。这一点是通过“灵活的用户输入”和“良好的问题处理”来实现的。相反，有人指出配置C倾向于“假装做自己无法完成的任务”，这使评审认为系统“似乎对环境的上下文和系统自身能力的理解非常松散”。配置A提供简单的解决方案，但经常被认为是在“假装完成某些任务或将代码中的部分留给工程师完成”。此外，配置A也未能很好地实现错误处理。

## 5 讨论

本研究旨在调查“在执行自然语言任务时，是否多个AI代理在一个机器人中的协作比单一的具象代理更能提高性能”。尽管在协作系统中未观察到整体性能提升，但在某些情况下确实存在明显的性能差异。配置B迄今为止是错误率最低的模型，并且在抽象问题解决任务中超出了预期。除此之外，配置A通常与配置B有相似表现，但错误率显著增加。配置C被认为是表现最差的，拥有最差的错误率，并且在任何任务中都没有出现过任何出色的表现。

配置B相较于配置A的优越错误率是预期之中的，然而，配置C的高错误率却令人惊讶。看起来配置C的审查机制不如配置B有效。这个趋势在系统的观察者评分表现中也有所体现，但程度较轻。尽管所有配置通常都有令人满意的表现，只有配置A和B的表现异常出色，明显优于其他配置。还注意到，案例研究表明配置C在“环境上下文理解方面掌握较为松散”。这一点令人惊讶，因为根据[?]的结果，系统的复杂问题解决能力应该随着代理数量的增加而增强。结果显示，仅仅增加系统中的代理数量并不能提高性能。

这种效应的一个可能解释可以在配置C的输入令牌使用量上找到，与配置A和B相比，配置C的输入令牌使用量明显过多。由[?]进行的研究表明，当上下文信息量较大时，LLM（大语言模型）会出现显著的遗忘现象。特别是当“模型必须在长输入上下文的中间部分使用信息时”，这种遗忘现象尤为明显。研究结果表明，随着代理数量的增加，多代理聊天架构似乎几乎呈指数增长地增加输入令牌的使用量，尤其是在本次实验中使用的长系统提示语的情况下。因此，LLM的上下文窗口可能会变得饱和，导致在众多信息中遗忘了问题的上下文。解决这一问题的潜在方法可能是采用检索增强生成（RAG）技术，以减少系统提示的长度，从而避免上下文窗口饱和。研究[?]表明，使用RAG是一种成功将私人API库集成到LLM系统中的方法。

另一种解释可能是配置C中规划器所采用的自然语言沟通技术。在[?]中，提出了一种大语言模型（LLM）控制机器人模型的方法，该方法通过LLM开发代码，而不是提出与一组外部定义功能相关的自然语言计划。他们发现，这样做扩展了机器人的能力，使其能够生成超出其原始功能集的能力。通过将规划器引入配置C的第一阶段，可能会使得通过代码生成所提供的灵活性有所降低。以这种方式，规划器可能会影响编码器的输出，使其更接近一组简单的原始功能，而不是扩展其自身的能力。

这些假设是基于有限的数据来进行推测的。增加系统架构的多样性，并在每个试验中使用不同的观察者小组，将使得这些结论更具普适性。此外，潜在的感知和运动系统故障可能影响了一些实验中的观察者评分。然而，这些故障在所有系统中均有分布，因此不应对整体结果产生影响。

尽管存在这些局限性，本研究明确展示了多智能体协作对人类与机器人互动领域中人工智能系统表现的显著影响。这个影响并非简单地更多的智能体就更有效。相反，结果表明，双智能体系统的表现优于三智能体系统。本研究需要更多关于其他协作架构的研究。使用RAG而不是在系统提示中提供API引用，或使用伪代码生成规划器，可能会提高这些协作模型的效果。

## 致谢

本研究得到了澳大利亚政府通过澳大利亚研究委员会的联动项目资助计划（LP220100430）和行业合作伙伴Guide Dogs NSW/ACT的支持。

## 参考文献

+   [Ahn et al., 2022] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as I can, not as I say: Grounding language in robotic affordances, 2022.

+   [Firoozi 等人，2023] Roya Firoozi、Johnathan Tucker、Stephen Tian、Anirudha Majumdar、Jiankai Sun、Weiyu Liu、Yuke Zhu、Shuran Song、Ashish Kapoor、Karol Hausman、Brian Ichter、Danny Driess、Jiajun Wu、Cewu Lu 和 Mac Schwager。机器人中的基础模型：应用、挑战与未来，2023年12月。arXiv:2312.07843 [cs]。

+   [Liang 等人，2023] Jacky Liang、Wenlong Huang、Fei Xia、Peng Xu、Karol Hausman、Brian Ichter、Pete Florence 和 Andy Zeng。作为政策的代码：语言模型程序用于具身控制，2023年5月。arXiv:2209.07753 [cs]。

+   [Liu 等人，2024] Nelson F. Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio Petroni 和 Percy Liang。迷失在中间：语言模型如何使用长上下文。《计算语言学会会刊》，12：157-173，2024年2月。

+   [Ni 和 Buehler，2024] Bo Ni 和 Markus J. Buehler。MechAgents：大型语言模型多智能体协作可以解决力学问题、生成新数据并整合知识。《极限力学通讯》，67：102131，2024年3月。

+   [Rosser，2024] Mitchell Rosser。sheepskins/spottyai，2024年9月。https://github.com/sheepskins/spottyai。

+   [Sorin 和 Klang，2023] Vera Sorin 和 Eyal Klang。大型语言模型与涌现现象。《欧洲放射学开放杂志》，10，2023年1月。出版商：Elsevier。

+   [Staniaszek 等人，2024] Michal Staniaszek、Esther、Dave Niewinski、Koki Shinjo、maubrunn、Chris Iverach-Brereton、Yoshiki Obinata、Kei Okada、Michel Heinemann、Naoya Yamaguchi、Harel Biggie、jeremysee2、Naoto Tsukamoto、Juan Miguel Jimeno、Kenji Brameld (TRACLabs)、Lucas Walter、Luke、Mario Gini、matthew rt、Victor Mittermair、Tobit Flatscher 和 Wolf Vollprecht。Spot ros，2024年8月。

+   [Verspoor，2024] Karin Verspoor。“以火攻火”——使用LLMs对抗LLM幻觉。《自然》，630(8017)：569-570，2024年6月。Bandiera_abtest：Cg_type：新闻与观点，出版商：自然出版集团，主题术语：机器学习，计算机科学。

+   [Wu 等人，2023] Qingyun Wu、Gagan Bansal、Jieyu Zhang、Yiran Wu、Beibin Li、Erkang Zhu、Li Jiang、Xiaoyun Zhang、Shaokun Zhang、Jiale Liu、Ahmed Hassan Awadallah、Ryen W. White、Doug Burger 和 Chi Wang。AutoGen：通过多智能体对话促进下一代LLM应用，2023年10月。arXiv:2308.08155 [cs]。

+   [Zan 等人，2022] Daoguang Zan、Bei Chen、Zeqi Lin、Bei Guan、Yongji Wang 和 Jian-Guang Lou。当语言模型遇上私人库，2022年10月。arXiv:2210.17236 [cs]。
