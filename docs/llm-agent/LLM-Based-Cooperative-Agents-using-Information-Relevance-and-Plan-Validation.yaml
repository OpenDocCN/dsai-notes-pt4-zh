- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-Based Cooperative Agents using Information Relevance and Plan Validation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的信息相关性和计划验证的合作代理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16751](https://ar5iv.labs.arxiv.org/html/2405.16751)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16751](https://ar5iv.labs.arxiv.org/html/2405.16751)
- en: SeungWon Seo
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: SeungWon Seo
- en: Department of Artificial Intelligence
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系
- en: Kyung Hee University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 庆熙大学
- en: Yongin, South Korea
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 韩国龙仁
- en: ssw03270@khu.ac.kr
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ssw03270@khu.ac.kr
- en: \AndJunhyeok Lee
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \AndJunhyeok Lee
- en: Department of Software Convergence
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 软件融合系
- en: Kyung Hee University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 庆熙大学
- en: Yongin, South Korea
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 韩国龙仁
- en: bluehyena123@khu.ac.kr
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: bluehyena123@khu.ac.kr
- en: \AndSeongRae Noh
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \AndSeongRae Noh
- en: Department of Artificial Intelligence
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系
- en: Kyung Hee University
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 庆熙大学
- en: Yongin, South Korea
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 韩国龙仁
- en: rhosunr99@khu.ac.kr
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: rhosunr99@khu.ac.kr
- en: \AndHyeongYeop Kang
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \AndHyeongYeop Kang
- en: Department of Software Convergence
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 软件融合系
- en: Kyung Hee University
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 庆熙大学
- en: Yongin, South Korea
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 韩国龙仁
- en: 'siamiz@khu.ac.kr Corresponding author: siamiz@khu.ac.kr'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: siamiz@khu.ac.kr 通讯作者：siamiz@khu.ac.kr
- en: Abstract
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We address the challenge of multi-agent cooperation, where agents achieve a
    common goal by interacting with a 3D scene and cooperating with decentralized
    agents under complex partial observations. This involves managing communication
    costs and optimizing interaction trajectories in dynamic environments. Our research
    focuses on three primary limitations of existing cooperative agent systems. Firstly,
    current systems demonstrate inefficiency in managing acquired information through
    observation, resulting in declining planning performance as the environment becomes
    more complex with additional objects or goals. Secondly, the neglect of false
    plans in partially observable settings leads to suboptimal cooperative performance,
    as agents struggle to adapt to environmental changes influenced by the unseen
    actions of other agents. Lastly, the failure to incorporate spatial data into
    decision-making processes restricts the agent’s ability to construct optimized
    trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced
    Cooperative Language Agent (REVECA), a novel cognitive architecture powered by
    GPT-3.5\. REVECA leverages relevance assessment, plan validation, and spatial
    information to enhance the efficiency and robustness of agent cooperation in dynamic
    and partially observable environments while minimizing continuous communication
    costs and effectively managing irrelevant dummy objects. Our extensive experiments
    demonstrate the superiority of REVECA over previous approaches, including those
    driven by GPT-4.0\. Additionally, a user study highlights REVECA’s potential for
    achieving trustworthy human-AI cooperation. We expect that REVECA will have significant
    applications in gaming, XR applications, educational tools, and humanoid robots,
    contributing to substantial economic, commercial, and academic advancements.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决了多代理合作的挑战，其中代理通过与3D场景互动并在复杂的部分观察下与去中心化的代理合作来实现共同目标。这涉及到管理通信成本和优化动态环境中的交互轨迹。我们的研究集中在现有合作代理系统的三个主要限制上。首先，当前系统在通过观察管理获取的信息方面表现低效，导致随着环境复杂性增加（如物体或目标增多），规划性能下降。其次，在部分可观察设置中忽视虚假计划导致合作性能不佳，因为代理在适应受其他代理未见动作影响的环境变化时存在困难。最后，决策过程中未能纳入空间数据限制了代理构建优化轨迹的能力。为克服这些限制，我们提出了RElevance
    and Validation-Enhanced Cooperative Language Agent（REVECA），这是一种由GPT-3.5驱动的新型认知架构。REVECA利用相关性评估、计划验证和空间信息来提升代理在动态和部分可观察环境中的合作效率和鲁棒性，同时最小化持续通信成本并有效管理无关虚拟对象。我们的广泛实验展示了REVECA相对于以往方法的优越性，包括那些由GPT-4.0驱动的方法。此外，一项用户研究突显了REVECA在实现可信的人机合作方面的潜力。我们期望REVECA在游戏、XR应用、教育工具和类人机器人领域具有重要应用，促进经济、商业和学术上的重大进步。
- en: '*K*eywords Multi-agent planning  $\cdot$ Large Language Models'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 多代理规划 $\cdot$ 大型语言模型'
- en: 1 Introduction
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Digital agents collaborating with humans are crucial in games, educational platforms,
    and virtual universes. Known as Non-Player Characters (NPCs), these agents enhance
    user immersion in commercial applications but often operate on pre-scripted behaviors,
    limiting adaptability and resulting in repetitive actions. In games like World
    of Warcraft and Diablo, NPCs who must be escorted by the player often fail to
    comprehend the situational context, rushing into danger and creating challenges
    for players. This lack of adaptive intelligence detracts from the gaming experience,
    highlighting the need for advanced AI-driven agents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类协作的数字代理在游戏、教育平台和虚拟宇宙中至关重要。这些被称为非玩家角色（NPCs）的代理在商业应用中增强了用户沉浸感，但通常依赖预先编写的行为脚本，限制了适应性，导致重复性动作。在《魔兽世界》和《暗黑破坏神》等游戏中，必须由玩家护送的NPC往往无法理解情境背景，急于进入危险区域，给玩家带来挑战。这种缺乏适应性智能的情况削弱了游戏体验，突显了对先进AI驱动代理的需求。
- en: Recently, OpenAI showcased an AGI Robot using large language models (LLMs) for
    recognition of natural language and household tasks. Inspired by this, we aim
    to create adaptive, intelligent cooperative agents. By facilitating natural language
    interactions, high-level planning, and scene-dedicated low-level controllers,
    these LLM-based agents will address cooperative problems more effectively than
    traditional methods relying on static, learnable models or reinforcement learning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，OpenAI展示了一款使用大型语言模型（LLMs）进行自然语言识别和家庭任务的AGI机器人。受到这一启发，我们旨在创建具有适应性和智能的协作代理。通过促进自然语言互动、高级规划和场景专用的低级控制器，这些基于LLM的代理将比传统的静态可学习模型或强化学习方法更有效地解决协作问题。
- en: This paper introduces REVECA, a RElevance and Validation-Enhanced Cooperative
    Language Agent, an LLM-based agent framework addressing decentralized control,
    costly communication, complex tasks, partially observable environments, and noisy
    settings. Similar to prior work [[1](#bib.bib1)], we focus on multi-objective
    household tasks using well-constructed virtual settings. Specifically, we focus
    on two decentralized agents cooperating on a multi-objective, long-horizon household
    task under complex partial observations. Additionally, continuous communication
    incurs time costs, and irrelevant dummy objects add noise, complicating the environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了REVECA，一种基于LLM的代理框架，专注于解决去中心化控制、昂贵通信、复杂任务、部分可观察环境和噪声设置的问题。与以往的工作[[1](#bib.bib1)]类似，我们专注于使用精心构建的虚拟环境进行多目标家庭任务。具体来说，我们关注两个去中心化的代理在复杂的部分观察下协作完成多目标、长期家庭任务。此外，持续的通信带来了时间成本，无关的虚假对象增加了噪声，复杂化了环境。
- en: The primary advancements of REVECA over previous works are threefold. Firstly,
    REVECA leverages LLMs to assess the relevance of newly acquired information before
    storage, prioritizing it and reducing reasoning complexity for planning. Previous
    research [[1](#bib.bib1), [2](#bib.bib2)] stored all information in memory, leading
    to performance declines due to increasing memory requirements, limited context
    length, and intensifying reasoning complexity. Some studies [[3](#bib.bib3), [4](#bib.bib4)]
    used queues to retain recent $K$ pieces of information, but this resulted in suboptimal
    planning due to the limited historical information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: REVECA在以往工作的基础上取得的主要进展有三方面。首先，REVECA利用大型语言模型（LLMs）在存储前评估新获取信息的相关性，优先处理这些信息，从而减少规划的推理复杂性。以往的研究[[1](#bib.bib1),
    [2](#bib.bib2)]将所有信息存储在记忆中，这导致由于内存需求增加、上下文长度有限以及推理复杂性加剧而性能下降。一些研究[[3](#bib.bib3),
    [4](#bib.bib4)]使用队列保留最近的$K$条信息，但这由于历史信息有限，导致规划效果不佳。
- en: Secondly, REVECA uses a novel modular cognitive architecture to mitigate false
    planning. This occurs in partially observable environments because changes induced
    by collaborators are not immediately reflected in the agent’s memory. Traditional
    approaches [[5](#bib.bib5)] update information via constant pre-task communication,
    which is costly. REVECA’s Validation Module checks plan validity by assessing
    information relevance and estimating collaborators’ recent plans, enabling communication
    only when necessary and improving efficiency and effectiveness.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，REVECA使用一种新颖的模块化认知架构来减轻虚假规划。这种情况发生在部分可观察的环境中，因为合作者引起的变化不会立即反映在代理的记忆中。传统方法[[5](#bib.bib5)]通过不断的任务前沟通来更新信息，这成本较高。REVECA的验证模块通过评估信息相关性和估计合作者的近期计划来检查计划的有效性，仅在必要时进行沟通，从而提高效率和有效性。
- en: Lastly, REVECA excels in planning by incorporating spatial information. Previous
    studies often neglected spatial information due to its integration challenge in
    language models. Although some studies [[1](#bib.bib1), [3](#bib.bib3)] have attempted
    to incorporate spatial information through image or text representations, they
    did not report performance improvements. We attempt to address this by considering
    the spatial distance between agents and objects in LLM reasoning processes. By
    employing a prompting technique, that effectively integrates numerical distances,
    REVECA enhances the cooperative performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，REVECA 通过融入空间信息在规划方面表现出色。由于语言模型中空间信息的集成挑战，之前的研究通常忽视了空间信息。虽然一些研究 [[1](#bib.bib1),
    [3](#bib.bib3)] 试图通过图像或文本表示来整合空间信息，但未报告性能提升。我们尝试通过在 LLM 推理过程中考虑代理与对象之间的空间距离来解决这个问题。通过采用有效集成数值距离的提示技术，REVECA
    增强了合作性能。
- en: 'We conducted comparative analysis, ablation studies, and user studies, using
    three multi-room simulation environments: Communicative Watch-And-Help (C-WAH),
    ThreeDWorld Multi-Agent Transport (TDW-MAT), and Noisy-C-WAH. Noisy-C-WAH, a variant
    of C-WAH with dummy obstacles, tested REVECA in noisy settings. Results show that
    REVECA achieves higher success rates, efficiency, and robustness than recent studies.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了比较分析、消融研究和用户研究，使用了三种多房间仿真环境：Communicative Watch-And-Help (C-WAH)、ThreeDWorld
    Multi-Agent Transport (TDW-MAT) 和 Noisy-C-WAH。Noisy-C-WAH 是 C-WAH 的一个变体，增加了虚拟障碍物，用于在嘈杂环境中测试
    REVECA。结果表明，REVECA 在成功率、效率和鲁棒性方面优于近期的研究。
- en: 'In summary, our main contributions are as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的主要贡献如下：
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Addressed cooperative challenges: decentralized control, costly communication,
    complex long-horizon tasks, partially observable environments, and noisy multi-object
    settings.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解决了合作挑战：去中心化控制、昂贵的通信、复杂的长远任务、部分可观察环境和嘈杂的多目标设置。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Developed an LLM-based relevance estimation method for optimal planning and
    robustness in noisy environments.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开发了一种基于 LLM 的相关性估计方法，以实现最佳规划和在嘈杂环境中的鲁棒性。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Introduced an agent framework that validates plans and incorporates spatial
    information for mitigating false planning.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引入了一个代理框架，该框架验证计划并结合空间信息以减少虚假规划。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conducted extensive evaluations that show REVECA’s superior performance over
    previous studies in C-WAH, TDW-MAT, and Noisy-C-WAH.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 进行了广泛的评估，结果显示 REVECA 在 C-WAH、TDW-MAT 和 Noisy-C-WAH 中的表现优于以往的研究。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Performed a user study demonstrating potential trustworthy human-AI cooperation.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 进行了用户研究，展示了潜在的可信赖人机合作。
- en: 2 Related Work
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Research on cooperative agents has a long history  [[6](#bib.bib6), [7](#bib.bib7)].
    The traditional cooperative agent has been studied across various directions,
    mainly leveraging reinforcement learning technique [[8](#bib.bib8), [9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25)]. These approaches have facilitated the development of agents
    that can autonomously learn to cooperate by maximizing cumulative rewards through
    trial and error in various simulated settings. Notable studies have explored aspects
    such as visual reinforcement learning for navigation and interaction [[8](#bib.bib8)],
    mapping state spaces to actions effectively [[9](#bib.bib9)], and enhancing the
    robustness of learning algorithms in multi-agent systems [[10](#bib.bib10)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 合作代理的研究有着悠久的历史 [[6](#bib.bib6), [7](#bib.bib7)]。传统的合作代理研究涉及多个方向，主要利用强化学习技术 [[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25)]。这些方法促进了可以通过试错在各种模拟环境中最大化累计奖励以自主学习合作的代理的发展。值得注意的研究探讨了视觉强化学习用于导航和交互
    [[8](#bib.bib8)]、将状态空间有效映射到动作 [[9](#bib.bib9)] 以及增强多代理系统中学习算法的鲁棒性 [[10](#bib.bib10)]。
- en: Some other researchers have aimed to develop platforms to test cooperative agents [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32), [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)]. These provide standardized environments to benchmark the performance
    and generalization capabilities of the agents.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他研究人员旨在开发测试合作代理的平台[[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]。这些平台提供了标准化的环境，用于基准测试代理的性能和泛化能力。
- en: However, a significant limitation of previous work is the lack of support for
    natural language-based communication between agents [[38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [11](#bib.bib11), [30](#bib.bib30), [34](#bib.bib34)]. The effective
    communication is crucial for enhancing collaboration, especially in complex, multi-agent
    environments. Natural language processing capabilities enable agents to exchange
    information in a more intuitive and human-like manner.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，以往工作的一个重大局限是缺乏对基于自然语言的代理间通信的支持[[38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40),
    [11](#bib.bib11), [30](#bib.bib30), [34](#bib.bib34)]。有效的沟通对于提高协作尤为重要，尤其是在复杂的多代理环境中。自然语言处理能力使得代理能够以更直观、更类似于人类的方式交换信息。
- en: LLMs have recently attracted significant interest in both academia and industry,
    also gaining attention in the field of agents [[41](#bib.bib41), [42](#bib.bib42)].
    The common-sense reasoning capabilities and natural language processing abilities
    of LLMs have contributed to enhancing the agent’s decision-making [[13](#bib.bib13),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48)] and planning [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [5](#bib.bib5), [54](#bib.bib54)] abilities.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 近期，LLM在学术界和工业界引起了广泛关注，并且在代理领域也获得了关注[[41](#bib.bib41), [42](#bib.bib42)]。LLM的常识推理能力和自然语言处理能力有助于提升代理的决策[[13](#bib.bib13),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48)]和规划[[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [5](#bib.bib5), [54](#bib.bib54)]能力。
- en: Recent studies on LLM-based embodied agents [[1](#bib.bib1), [3](#bib.bib3),
    [2](#bib.bib2), [4](#bib.bib4)] represent a significant step forward in this field.
    These agents utilize LLMs for understanding the environment, planning tasks, and
    communicating with human users and other agents. However, existing studies face
    suboptimal performance due to issues with LLMs, such as inadequate spatial data
    processing capabilities [[55](#bib.bib55)], performance degradation with large
    input data [[56](#bib.bib56)], and insufficient reasoning abilities with complex
    reasoning tasks [[57](#bib.bib57)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 近期关于基于LLM的具身代理的研究[[1](#bib.bib1), [3](#bib.bib3), [2](#bib.bib2), [4](#bib.bib4)]在该领域代表了一个重要的进步。这些代理利用LLM来理解环境、规划任务以及与人类用户和其他代理进行沟通。然而，现有研究因LLM存在的问题而表现不佳，如空间数据处理能力不足[[55](#bib.bib55)]、大输入数据时性能下降[[56](#bib.bib56)]以及处理复杂推理任务时推理能力不足[[57](#bib.bib57)]。
- en: This paper introduces REVECA, an LLM-based embodied agent framework designed
    to address the limitations of the previous works by leveraging information relevance,
    plan validation, and effective communication strategies.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了REVECA，一个基于LLM的具身代理框架，旨在通过利用信息相关性、计划验证和有效沟通策略来解决以往工作的局限性。
- en: 3 Problem Definition
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题定义
- en: The problem we focus on is an extension of the decentralized partially observable
    Markov decision process (DEC-POMDP) [[58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60),
    [10](#bib.bib10), [1](#bib.bib1), [4](#bib.bib4)], characterized by the tuple
    $(n,S,\{\Sigma_{i}\},\{A_{i}\},\{O_{i}\},T,G,h)$, based on partial observations
    that guide their decision-making processes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的问题是去中心化部分可观察马尔可夫决策过程（DEC-POMDP）的扩展[[58](#bib.bib58), [59](#bib.bib59),
    [60](#bib.bib60), [10](#bib.bib10), [1](#bib.bib1), [4](#bib.bib4)]，其特征为元组$(n,S,\{\Sigma_{i}\},\{A_{i}\},\{O_{i}\},T,G,h)$，基于部分观察来指导决策过程。
- en: This paper focuses on a scenario where two decentralized intelligent agents
    collaborate on a long-horizon rearrangement task [[61](#bib.bib61)] within an
    indoor multi-room environment, utilizing noise-free broadcast communication. While
    the focus is on two agents, the methods and experiments are generalizable to scenarios
    with more agents. The agents can perform navigation actions, interaction actions,
    and communication actions. The task includes several predicates $g_{i}$, which
    represents the sub-task of placing two cupcakes in the fridge.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本文关注于两个去中心化智能体在室内多房间环境中合作完成长期重新排列任务的场景[[61](#bib.bib61)]，利用无噪音广播通信。虽然重点在于两个智能体，但这些方法和实验可以推广到更多智能体的场景。智能体可以执行导航动作、交互动作和通信动作。任务包括几个谓词
    $g_{i}$，表示将两个杯形蛋糕放入冰箱的子任务。
- en: '![Refer to caption](img/1730da9428a8255f2d7e4381a0613f51.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1730da9428a8255f2d7e4381a0613f51.png)'
- en: 'Figure 1: The interaction flow between the six modules of REVECA and the environment.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：REVECA的六个模块与环境之间的交互流程。
- en: '![Refer to caption](img/3e961b0a4842f81abe977e7684c504fa.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3e961b0a4842f81abe977e7684c504fa.png)'
- en: 'Figure 2: An example scenario demonstrating REVECA’s comprehensive operational
    flow, highlighting the interaction between various modules, collaborators, and
    the environment to achieve a common goal.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个示例场景展示了REVECA的全面操作流程，突显了各模块、合作者和环境之间的互动，以实现共同目标。
- en: 4 REVECA framework
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 REVECA框架
- en: 'Our framework comprises six key modules: Communication Module, Observation
    Module, Memory Module, Planning Module, Validation Module, and Execution Module.
    Communication Module facilitates natural language-based information sharing. Observation
    Module gathers and categorizes information into four relevance levels while the
    agent explores the environment. Memory Module comprises six components: common
    goal, communication history, scene observation history, self-information history,
    collaborators-information history, and a low-level action skill book. It is responsible
    for storing and updating data. Planning Module employs memory data to generate
    efficient plans, considering information relevancy, predicted proximity of all
    agents, and collaborators’ information history. Validation Module checks for false
    plans by generating and confirming collaborators’ possible interaction scenarios
    between observation and planning times, then discards and reformulates plans if
    needed. Execution Module executes validated plans using the low-level action skill
    book. The modular design of REVECA is illustrated in [Figure 1](#S3.F1 "Figure
    1 ‣ 3 Problem Definition ‣ LLM-Based Cooperative Agents using Information Relevance
    and Plan Validation") and an example scenario presenting REVECA’s workflow is
    depicted in [Figure 2](#S3.F2 "Figure 2 ‣ 3 Problem Definition ‣ LLM-Based Cooperative
    Agents using Information Relevance and Plan Validation").'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的框架包括六个关键模块：通信模块、观察模块、记忆模块、规划模块、验证模块和执行模块。通信模块促进基于自然语言的信息共享。观察模块在智能体探索环境时，收集并将信息分类为四个相关性级别。记忆模块包括六个组件：共同目标、通信历史、场景观察历史、自身信息历史、合作者信息历史和一本低级动作技能书。它负责存储和更新数据。规划模块利用记忆数据生成高效的计划，考虑信息相关性、所有智能体的预测接近程度以及合作者的信息历史。验证模块通过生成并确认合作者在观察和规划时间之间可能的交互场景来检查虚假计划，如果需要，则丢弃并重新制定计划。执行模块使用低级动作技能书来执行经过验证的计划。REVECA的模块化设计如[图1](#S3.F1
    "Figure 1 ‣ 3 Problem Definition ‣ LLM-Based Cooperative Agents using Information
    Relevance and Plan Validation")所示，REVECA工作流程的示例场景见[图2](#S3.F2 "Figure 2 ‣ 3 Problem
    Definition ‣ LLM-Based Cooperative Agents using Information Relevance and Plan
    Validation")。
- en: '![Refer to caption](img/2ccf35f0f6ced03e8b49284bf2903ed9.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ccf35f0f6ced03e8b49284bf2903ed9.png)'
- en: 'Figure 3: Four cases when the Communication Module is invoked: (a) Simulation
    Initiation. (b) Validation Requests. (c) Response to Validation Requests. (d)
    Sub-goal Achievement.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：通信模块被调用的四种情况：(a) 模拟启动。(b) 验证请求。(c) 对验证请求的响应。(d) 子目标达成。
- en: 4.1 Communication for information sharing
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 信息共享的通信
- en: 'The Communication Module, facilitating natural language information sharing,
    is invoked in four cases. 1) Simulation Initiation: Agents exchange initial information
    about their locations and surrounding objects. 2) Validation Requests: Agents
    query about task history. 3) Response to Validation Requests: Agents provide task
    completion history. 4) Sub-goal Achievement: Agents announce sub-goal completion.
    The detailed illustrations for each case are presented in [Figure 3](#S4.F3 "Figure
    3 ‣ 4 REVECA framework ‣ LLM-Based Cooperative Agents using Information Relevance
    and Plan Validation").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通信模块，促进自然语言信息共享，在四种情况下被调用。1）模拟启动：代理交换有关其位置和周围物体的初始信息。2）验证请求：代理查询任务历史。3）响应验证请求：代理提供任务完成历史。4）子目标完成：代理宣布子目标完成。每种情况的详细说明见 [图3](#S4.F3
    "图3 ‣ 4 REVECA框架 ‣ 基于LLM的合作代理使用信息相关性和计划验证")。
- en: '![Refer to caption](img/ea97943cdc421851898e6ba58e9c3df4.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ea97943cdc421851898e6ba58e9c3df4.png)'
- en: 'Figure 4: An example of determining the relevance score for scene information.
    If the common goal is to put milk in the fridge, the fridge and milk have strong
    relevance. Since the milk could also be placed in containers, the drawer and cabinet
    are estimated to have a moderate relevance.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：确定场景信息相关性得分的示例。如果共同目标是将牛奶放入冰箱，那么冰箱和牛奶具有很强的相关性。由于牛奶也可以放入容器中，因此抽屉和柜子被估计为具有中等相关性。
- en: '4.2 Observation-time: Relevance Estimation and Storage'
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 观察时间：相关性估计和存储
- en: 'During observation time, agents acquire four types of information: scene, self-agent,
    communication, and collaborator-agent. Scene information includes object details
    like 3D position, ID, name, room, and interactions. Self-agent information includes
    the held object, current position, and completed plan history. These are obtained
    at every simulation step. Communication information, conveyed in natural language,
    is obtained when it occurs. Collaborator-agent information is acquired when communicating
    with or seeing a specific agent. It includes the same details as self-agent information.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察时间，代理获取四种类型的信息：场景、自我代理、通信和协作代理。场景信息包括物体细节，如3D位置、ID、名称、房间和交互。自我代理信息包括持有的物体、当前位置和完成的计划历史。这些信息在每个模拟步骤中获得。通信信息以自然语言传达，在发生时获取。协作代理信息在与特定代理沟通或看到时获取，包含与自我代理信息相同的细节。
- en: Scene and communication information are stored in scene observation history
    and communication history, respectively, with four relevance levels (strong, medium,
    low, none) assessed by LLMs. This prioritizes information, avoiding the need to
    re-reference all memory entries and reducing the reasoning complexity during the
    LLM-based planning process. An example of determining the relevance score for
    scene information is depicted in [Figure 4](#S4.F4 "Figure 4 ‣ 4.1 Communication
    for information sharing ‣ 4 REVECA framework ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 场景和通信信息分别存储在场景观察历史和通信历史中，由LLM评估四个相关性级别（强、中、低、无）。这将信息优先级排序，避免重新引用所有记忆条目，并减少LLM基于计划过程中的推理复杂性。确定场景信息相关性得分的示例见 [图4](#S4.F4
    "图4 ‣ 4.1 信息共享的通信 ‣ 4 REVECA框架 ‣ 基于LLM的合作代理使用信息相关性和计划验证")。
- en: Self-agent information, which is fully observable, is directly stored in self-information
    history. Conversely, collaborator-agent information, which is acquired discontinuously,
    is interpolated to address gaps before being stored in collaborators-information
    history.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自我代理信息是完全可观察的，直接存储在自我信息历史中。相反，协作代理信息是间歇性获取的，在存储到协作信息历史之前会进行插值以填补空白。
- en: '![Refer to caption](img/4c090236cce4d813fd609871ddeb139e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4c090236cce4d813fd609871ddeb139e.png)'
- en: 'Figure 5: The two-step planning procedure of REVECA. In the first step, a set
    of $K=3$ plans are created, each plan evaluated for proximity and relevance. The
    second step involves reasoning through these plans using the LLM with chain-of-thought
    prompting. The LLM evaluates the plans based on input data, including self-information
    and collaborator information, recommending the most efficient action.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：REVECA的两步规划过程。第一步，创建一组 $K=3$ 计划，每个计划都被评估其接近度和相关性。第二步，使用链式思维提示的LLM对这些计划进行推理。LLM根据输入数据，包括自我信息和协作信息，评估计划，并推荐最有效的行动。
- en: '4.3 Planning-time: Real-Time Adaptation'
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 规划时间：实时适应
- en: Existing research on agent-based simulations [[62](#bib.bib62), [63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65)] often constructs sequential long-term plans
    for specific goals, effective in individual tasks where the agent is the sole
    entity influencing the environment and in centralized settings where a manager
    oversees the environment. However, in decentralized cooperative tasks within partially
    observable environments, agents must dynamically adapt their plans to changes,
    as long-term plans devised at earlier stages can become impractical and lead to
    false plans.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现有关于基于代理的模拟的研究[[62](#bib.bib62), [63](#bib.bib63), [64](#bib.bib64), [65](#bib.bib65)]通常为特定目标构建顺序性的长期计划，这在代理是唯一影响环境的实体的个体任务和集中式设置中（其中一个经理监督环境）是有效的。然而，在部分可观察环境中的去中心化合作任务中，代理必须动态调整他们的计划，因为在早期阶段制定的长期计划可能会变得不切实际并导致错误的计划。
- en: 'To address this, we propose a Planning Module that devises the next optimal
    action at each decision point, both at the start of the simulation and following
    the execution of each action. It starts with creating $K$ plans based on information
    relevance and relative proximity of collaborators. LLMs then utilize zero-shot
    chain-of-thought prompting [[66](#bib.bib66)] to select the optimal plan, prioritizing
    tasks based on relevance, self-information, and collaborators’ information. This
    approach implicitly guides the planning process to prioritize tasks that are crucial
    for achieving the goal while ensuring the agent’s actions are more efficient than
    those of its collaborators. This two-step planning procedure is depicted in [Figure 5](#S4.F5
    "Figure 5 ‣ 4.2 Observation-time: Relevance Estimation and Storage ‣ 4 REVECA
    framework ‣ LLM-Based Cooperative Agents using Information Relevance and Plan
    Validation").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，我们提出了一个规划模块，该模块在每个决策点制定下一个最优行动，既包括模拟开始时，也包括每次行动执行后的决策。它首先基于信息相关性和协作者的相对接近度创建$K$个计划。然后，大型语言模型（LLMs）利用零-shot链式思维提示[[66](#bib.bib66)]来选择最优计划，优先考虑任务的相关性、自身信息和协作者的信息。这种方法隐式地指导规划过程，以优先考虑对实现目标至关重要的任务，同时确保代理的行动比协作者的行动更有效。这种两步规划程序如[图
    5](#S4.F5 "Figure 5 ‣ 4.2 Observation-time: Relevance Estimation and Storage ‣
    4 REVECA framework ‣ LLM-Based Cooperative Agents using Information Relevance
    and Plan Validation")所示。'
- en: '![Refer to caption](img/4ff98e714de7bda69265c08c105d3f80.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ff98e714de7bda69265c08c105d3f80.png)'
- en: 'Figure 6: Demonstrates how REVECA’s Validation Module enhances collaboration
    between Alice and Bob. The action history indicates which actions Alice knows
    about and which are unknown to her. Without validation, Alice’s plan fails due
    to missing information about Bob’s actions. When the Validation Module is used,
    Alice communicates with Bob, confirming the status of the milk and successfully
    completing the task by grabbing the cupcake.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：展示了REVECA的验证模块如何增强Alice和Bob之间的协作。行动历史显示了Alice知道哪些行动以及她不知道哪些行动。如果没有验证，Alice的计划会因为缺少关于Bob行动的信息而失败。当使用验证模块时，Alice与Bob沟通，确认了牛奶的状态，并通过拿到杯形蛋糕成功完成任务。
- en: '4.4 Validation-time: Scenario-based Validation'
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 验证时间：基于情景的验证
- en: Even a well-constructed plan may become invalid due to the environmental changes
    caused by collaborators during the interval between observation and planning.
    In partially observable environments, determining whether such changes have occurred
    is challenging for the agent.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一个构建良好的计划，也可能由于协作者在观察和规划之间的时间内导致环境变化而变得无效。在部分可观察环境中，代理很难确定是否发生了这种变化。
- en: A straightforward method to resolve this is to revisit the object’s location
    or query all collaborators about their interactions. However, this can result
    in inefficient path planning and incur significant communication costs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的一种简单方法是重新访问对象的位置或询问所有协作者他们的互动。然而，这可能导致低效的路径规划并产生较高的通信成本。
- en: 'To address this, REVECA includes scenario-based plan validation, estimating
    plan validity using stored information. The agent generates all possible scenarios
    for the object’s interactions with collaborators between the observation and planning
    times. The agent then uses collaborator information from its Memory Module to
    ask the LLM to determine the most likely scenario employing the zero-shot chain-of-thought
    prompting technique [[66](#bib.bib66)]. If the scenario where no collaborator
    interacted with the object is most likely, the agent assumes the plan is valid.
    If another scenario is likely, the agent communicates with the collaborator to
    confirm the interaction. If confirmed, the agent discards the original plan, creates
    a new one, and repeats the scenario-based validation. The scenarios with and without
    the Validation Module are shown in [Figure 6](#S4.F6 "Figure 6 ‣ 4.3 Planning-time:
    Real-Time Adaptation ‣ 4 REVECA framework ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation").'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，REVECA包括基于场景的计划验证，利用存储的信息估计计划的有效性。代理生成所有可能的场景，用于观察和计划之间的物体与协作者的交互。然后，代理使用其记忆模块中的协作者信息，请求LLM确定最可能的场景，采用零-shot连锁思维提示技术 [[66](#bib.bib66)]。如果最可能的场景是没有协作者与物体互动，代理假设计划有效。如果其他场景更可能，代理与协作者沟通以确认交互。如果确认，代理将原计划丢弃，创建一个新计划，并重复基于场景的验证。带有和不带有验证模块的场景见于[图6](#S4.F6
    "图6 ‣ 4.3 规划时间：实时适应 ‣ 4 REVECA框架 ‣ 基于LLM的合作代理使用信息相关性和计划验证")。
- en: 4.5 Executing Navigation and Contextual Actions
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 执行导航和情境动作
- en: After the plan is determined, the Execution Module retrieves sub-task information
    from the Memory Module to identify the target location. We use the A-star search
    algorithm for efficient pathfinding. Upon approaching the object, the agent retrieves
    a predefined animation from the low-level action skill book to execute the planned
    interaction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在计划确定后，执行模块从记忆模块中检索子任务信息以确定目标位置。我们使用A*搜索算法来进行高效的路径规划。当接近目标物体时，代理从低级动作技能库中检索预定义的动画以执行计划中的交互。
- en: '![Refer to caption](img/694e42402a5caeeb449c01004c48cd3c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/694e42402a5caeeb449c01004c48cd3c.png)'
- en: 'Figure 7: The layouts of the two virtual environments used in the REVECA experiments.
    (a) C-WAH. (b) TDW-MAT.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在REVECA实验中使用的两个虚拟环境的布局。(a) C-WAH。(b) TDW-MAT。
- en: 5 Experiment
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Experimental Settings
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'We conducted experiments using three types of indoor multi-room simulation
    environments: C-WAH, TDW-MAT [[1](#bib.bib1)], and Noisy-C-WAH. To ensure a fair
    comparison, we adopted the detailed parameters defined in the previous study’s
    settings [[1](#bib.bib1)].'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了三种类型的室内多房间模拟环境进行实验：C-WAH、TDW-MAT [[1](#bib.bib1)]和Noisy-C-WAH。为了确保公平比较，我们采用了之前研究中定义的详细参数设置 [[1](#bib.bib1)]。
- en: C-WAH [[1](#bib.bib1)], an extended version of the Watch-And-Help Challenge [[34](#bib.bib34)],
    is built on the realistic multi-agent simulation platform, VirtualHome [[67](#bib.bib67)].
    This environment includes five household tasks, such as setting the table and
    doing the dishes, forming the common goal. Our experiments consist of 10 episodes,
    each with five different household tasks across two test environments. In C-WAH,
    agents can acquire or provide information through communication with other agents
    while executing instructions. When an agent enters a specific room, it can observe
    all objects not inside a container like fridges or microwaves. To observe objects
    inside containers, the agent is required an additional action of opening containers.
    Agents are limited to using up to 500 characters per frame to mimic real-world
    communication costs. Horizon $h$ is set to 250 steps, and each task includes 3
    to 5 subgoals (or objects). Failing to achieve the common goal within the 250
    steps results in an episode failure. The layout is shown in [Figure 7](#S4.F7
    "Figure 7 ‣ 4.5 Executing Navigation and Contextual Actions ‣ 4 REVECA framework
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")(a).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: C-WAH[[1](#bib.bib1)]，观察与帮助挑战的扩展版本[[34](#bib.bib34)]，建立在现实的多智能体模拟平台VirtualHome[[67](#bib.bib67)]上。该环境包括五个家庭任务，如摆桌子和洗碗，构成了共同目标。我们的实验包括10集，每集有五个不同的家庭任务，分布在两个测试环境中。在C-WAH中，代理可以通过与其他代理的通信来获取或提供信息。在代理进入特定房间时，可以观察所有不在冰箱或微波炉等容器中的物体。要观察容器内的物体，代理需要额外的打开容器的动作。代理每帧限制使用500个字符，以模拟现实世界的通信成本。视野$h$设置为250步，每个任务包括3到5个子目标（或物体）。在250步内未能实现共同目标会导致集失败。布局如[图7](#S4.F7
    "Figure 7 ‣ 4.5 Executing Navigation and Contextual Actions ‣ 4 REVECA framework
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")(a)所示。
- en: 'TDW-MAT, an extended version of the ThreeDWorld Transport Challenge [[68](#bib.bib68)],
    is built on the TDW [[69](#bib.bib69)]. It features more natural object placements
    and a variety of objects and containers that assist in transporting items. The
    common goals involve transporting items in two categories: Food and Stuff. Each
    episode includes 10 target objects, and 2 to 5 containers are placed to facilitate
    moving multiple items simultaneously. Unlike C-WAH, agents in TDW-MAT cannot obtain
    complete room information without performing a 360-degree rotation in 15-degree
    increments. Communication is limited to 500 characters per frame, and $h$ is set
    to 3,000 steps. The layout is shown in [Figure 7](#S4.F7 "Figure 7 ‣ 4.5 Executing
    Navigation and Contextual Actions ‣ 4 REVECA framework ‣ LLM-Based Cooperative
    Agents using Information Relevance and Plan Validation")(b).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TDW-MAT，三维世界运输挑战的扩展版本[[68](#bib.bib68)]，建立在TDW[[69](#bib.bib69)]基础上。它具有更自然的物体放置方式和各种物体及容器，以协助运输物品。常见的目标包括运输两类物品：食品和杂物。每集包括10个目标物体，并放置2到5个容器，以便同时移动多个物品。与C-WAH不同，TDW-MAT中的代理在没有进行15度增量的360度旋转的情况下无法获取完整的房间信息。通信限制为每帧500个字符，$h$设置为3000步。布局如[图7](#S4.F7
    "Figure 7 ‣ 4.5 Executing Navigation and Contextual Actions ‣ 4 REVECA framework
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")(b)所示。
- en: Noisy-C-WAH is a customized version of C-WAH augmented with an additional 10
    or 20 dummy objects per episode to demonstrate the robustness of our framework
    in noisy environments. These dummy objects increase the complexity of the environment,
    challenging the agents’ observation, planning, and communication processes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Noisy-C-WAH是C-WAH的定制版本，每集增加了10或20个虚拟物体，以展示我们框架在噪声环境中的鲁棒性。这些虚拟物体增加了环境的复杂性，挑战了代理的观察、规划和通信过程。
- en: In C-WAH and Noisy-C-WAH, we evaluate the agent performance using Simulation
    Steps (SS), Travel Distance (TD), Communication Steps (CS), and Character Counts
    (CC) to measure the time cost to achieve the common goal, the average distance
    traveled, the time cost to perform communication, and the average number of characters
    used in the communication, respectively. In TDW-MAT, performance is evaluated
    based on the success rate of transporting items, including the overall success
    rate (TOTAL), and specific success rates for objects categorized as Food (FOOD)
    and Stuff (STUFF).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C-WAH 和 Noisy-C-WAH 中，我们使用模拟步骤（SS）、旅行距离（TD）、沟通步骤（CS）和字符数（CC）来评估代理的性能，以测量实现共同目标的时间成本、平均旅行距离、执行沟通的时间成本，以及沟通中使用的平均字符数。在
    TDW-MAT 中，性能基于运输物品的成功率进行评估，包括整体成功率（TOTAL），以及分类为食物（FOOD）和物品（STUFF）的物体的特定成功率。
- en: 5.2 REVECA and Baselines
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 REVECA 与基线
- en: 'In comparative experiments, our REVECA was evaluated against three baselines:
    the MCTS-based Hierarchical Planner (MHP), the Rule-based Hierarchical Planner
    (RHP), and the Cooperative Embodied Language Agent (CoELA). We compared REVECA
    with MHP and CoELA in C-WAH, with RHP and CoELA in TDW-MAT, and with CoELA in
    Noisy-C-WAH.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较实验中，我们将 REVECA 与三种基线进行评估：基于 MCTS 的分层规划器（MHP），基于规则的分层规划器（RHP），以及合作体语言代理（CoELA）。我们在
    C-WAH 中将 REVECA 与 MHP 和 CoELA 进行了比较，在 TDW-MAT 中与 RHP 和 CoELA 进行了比较，在 Noisy-C-WAH
    中与 CoELA 进行了比较。
- en: 'REVECA leverages GPT-3.5 through the OpenAI API. Although GPT-4 is the latest
    model, its high token cost makes extensive experimentation impractical. Thus,
    we employed GPT-3.5 with default parameters: a temperature of 0.7, top-p of 1,
    and a maximum token limit of 256.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: REVECA 通过 OpenAI API 利用 GPT-3.5。虽然 GPT-4 是最新的模型，但其高昂的代币成本使得广泛实验变得不切实际。因此，我们使用了默认参数的
    GPT-3.5：温度为 0.7，top-p 为 1，最大代币限制为 256。
- en: MHP, from the Watch-And-Help Challenge, is a Hierarchical Planner with a high-level
    planner utilizing MCTS and a low-level planner based on regression planning [[70](#bib.bib70)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: MHP，来自于 Watch-And-Help Challenge，是一个分层规划器，高层规划器利用 MCTS，低层规划器基于回归规划 [[70](#bib.bib70)]。
- en: RHP, from the ThreeDWorld Transport Challenge, is a Hierarchical Planner with
    a high-level planner using heuristics and a low-level A-start-based planner for
    navigation using a semantic map and Frontier Exploration strategy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: RHP，来自于 ThreeDWorld Transport Challenge，是一个分层规划器，其中高层规划器使用启发式方法，低层规划器基于 A* 算法进行导航，利用语义地图和
    Frontier Exploration 策略。
- en: CoELA leverages LLMs for planning and communication, enabling collaborative
    task-solving. For a fair comparison and cost efficiency, we used GPT-3.5-driven
    CoELA (CoELA-3.5) for the experiment and provided GPT-4.0-driven CoELA (CoELA-4.0)
    performance from the CoELA manuscript [[1](#bib.bib1)] for a comprehensive analysis
    of different LLM capacities.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: CoELA 利用 LLMs 进行规划和沟通，实现协作任务解决。为了公平比较和成本效益，我们在实验中使用了基于 GPT-3.5 的 CoELA（CoELA-3.5），并提供了基于
    GPT-4.0 的 CoELA（CoELA-4.0）的性能数据，来自 CoELA 手稿 [[1](#bib.bib1)]，以全面分析不同 LLM 的能力。
- en: 5.3 Comparative Analysis Results
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 比较分析结果
- en: 'Table 1: Comparative experimental results with C-WAH.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：与 C-WAH 的比较实验结果。
- en: '|  |  | SS $\downarrow$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SS $\downarrow$ |'
- en: '| REVECA |  | 48.90 |  | 40.34 |  | 6.90 |  | 79.65 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| REVECA |  | 48.90 |  | 40.34 |  | 6.90 |  | 79.65 |'
- en: '| CoELA-3.5 |  | 71.90 |  | 61.29 |  | 10.40 |  | 158.46 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CoELA-3.5 |  | 71.90 |  | 61.29 |  | 10.40 |  | 158.46 |'
- en: '| CoELA-4.0 |  | 57.00 |  | / |  | / |  | / |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CoELA-4.0 |  | 57.00 |  | / |  | / |  | / |'
- en: '| MHP |  | 69.40 |  | 58.96 |  | / |  | / |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MHP |  | 69.40 |  | 58.96 |  | / |  | / |'
- en: 'Table 2: Comparative experimental results with TDW-MAT.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：与 TDW-MAT 的比较实验结果。
- en: '|  | TOTAL (%) $\uparrow$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | TOTAL (%) $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| REVECA | 0.86 |  | 0.88 |  | 0.84 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| REVECA | 0.86 |  | 0.88 |  | 0.84 |'
- en: '| CoELA-3.5 | 0.64 |  | 0.69 |  | 0.59 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| CoELA-3.5 | 0.64 |  | 0.69 |  | 0.59 |'
- en: '| CoELA-4.0 | 0.71 |  | 0.82 |  | 0.61 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| CoELA-4.0 | 0.71 |  | 0.82 |  | 0.61 |'
- en: '| RHP | 0.79 |  | 0.83 |  | 0.76 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| RHP | 0.79 |  | 0.83 |  | 0.76 |'
- en: '![Refer to caption](img/8046913bef887f60eb30dd607e213c27.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8046913bef887f60eb30dd607e213c27.png)'
- en: 'Figure 8: Comparative experimental results with Noisy-C-WAH, incorporating
    varying numbers of dummy objects.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：与 Noisy-C-WAH 的比较实验结果，涵盖了不同数量的虚拟对象。
- en: '[Table 1](#S5.T1 "Table 1 ‣ 5.3 Comparative Analysis Results ‣ 5 Experiment
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")
    shows that REVECA outperforms other baselines in C-WAH. Specifically, GPT-3.5-driven
    REVECA uses fewer steps to complete tasks compared to CoELA-4.0, CoELA-3.5, and
    MHP, demonstrating superior effectiveness. The performance of CoELA significantly
    drops with GPT-3.5, even below MHP, highlighting CoELA’s reliance on GPT-4.0’s
    reasoning capability. REVECA also shows fewer average travel distances, communication
    steps, and characters used per communication.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1](#S5.T1 "表 1 ‣ 5.3 比较分析结果 ‣ 5 实验 ‣ 基于 LLM 的合作代理使用信息相关性和计划验证") 显示 REVECA
    在 C-WAH 中优于其他基线。具体而言，GPT-3.5 驱动的 REVECA 使用较少的步骤完成任务，相较于 CoELA-4.0、CoELA-3.5 和
    MHP，表现更为出色。CoELA 在 GPT-3.5 下的性能显著下降，甚至低于 MHP，突显出 CoELA 对 GPT-4.0 推理能力的依赖。REVECA
    还显示出较少的平均旅行距离、通信步骤和每次通信使用的字符数。'
- en: '[Table 2](#S5.T2 "Table 2 ‣ 5.3 Comparative Analysis Results ‣ 5 Experiment
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation")
    presents TDW-MAT results, where GPT-3.5-driven REVECA outperforms all baselines,
    including GPT-4.0-driven CoELA, across all metrics (TOTAL, FOOD, and STUFF).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#S5.T2 "表 2 ‣ 5.3 比较分析结果 ‣ 5 实验 ‣ 基于 LLM 的合作代理使用信息相关性和计划验证") 展示了 TDW-MAT
    结果，其中 GPT-3.5 驱动的 REVECA 在所有指标（TOTAL、FOOD 和 STUFF）上都优于所有基线，包括 GPT-4.0 驱动的 CoELA。'
- en: In Noisy-C-WAH, the experiment included 10 or 20 additional dummy objects. As
    shown in [Figure 8](#S5.F8 "Figure 8 ‣ 5.3 Comparative Analysis Results ‣ 5 Experiment
    ‣ LLM-Based Cooperative Agents using Information Relevance and Plan Validation"),
    REVECA outperforms CoELA-3.5 in all metrics. CoELA’s strategy of storing all acquired
    information in text form leads to a significant decline in reasoning performance.
    As dummy objects increase, the advantages of using relevance scores become more
    evident.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Noisy-C-WAH 中，实验包括了 10 或 20 个额外的虚拟对象。如 [图 8](#S5.F8 "图 8 ‣ 5.3 比较分析结果 ‣ 5
    实验 ‣ 基于 LLM 的合作代理使用信息相关性和计划验证") 所示，REVECA 在所有指标上均优于 CoELA-3.5。CoELA 存储所有获取的信息为文本形式的策略导致推理性能显著下降。随着虚拟对象的增加，使用相关性分数的优势变得更加明显。
- en: 5.4 Ablation Study Results
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融研究结果
- en: 'Table 3: Ablation study result with C-WAH. The highest performance score in
    each metric is highlighted in underlined bold, while scores that surpass those
    of REVECA are indicated in bold.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：C-WAH 的消融研究结果。每个指标中的最高性能分数用下划线粗体标出，而超过 REVECA 的分数用粗体标记。
- en: '|  | SS $\downarrow$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | SS $\downarrow$ |'
- en: '| REVECA | 48.90 | 40.34 | 6.90 | 79.65 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| REVECA | 48.90 | 40.34 | 6.90 | 79.65 |'
- en: '| w/o spatial info | 64.80 | 56.93 | 9.60 | 86.81 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 无空间信息 | 64.80 | 56.93 | 9.60 | 86.81 |'
- en: '| w/o relevance score | 43.80 | 35.75 | 8.20 | 79.64 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 无相关性分数 | 43.80 | 35.75 | 8.20 | 79.64 |'
- en: '| w/o Validation Module | 54.20 | 48.47 | 5.10 | 46.90 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 无验证模块 | 54.20 | 48.47 | 5.10 | 46.90 |'
- en: '| always ask before action | 46.90 | 38.05 | 9.10 | 84.00 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 始终在行动前询问 | 46.90 | 38.05 | 9.10 | 84.00 |'
- en: '| w/o collaborator’s info | 59.60 | 51.18 | 9.2 | 83.25 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 无协作者信息 | 59.60 | 51.18 | 9.2 | 83.25 |'
- en: '| K=2 | 55.40 | 46.32 | 7.50 | 66.47 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| K=2 | 55.40 | 46.32 | 7.50 | 66.47 |'
- en: '| K=4 | 53.30 | 45.00 | 9.30 | 86.72 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| K=4 | 53.30 | 45.00 | 9.30 | 86.72 |'
- en: '| R=3 | 53.00 | 46.02 | 8.10 | 77.41 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| R=3 | 53.00 | 46.02 | 8.10 | 77.41 |'
- en: '| R=5 | 54.90 | 46.75 | 8.70 | 88.47 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| R=5 | 54.90 | 46.75 | 8.70 | 88.47 |'
- en: 'Table 4: Ablation study result with Noisy-C-WAH augmented by 20 dummy objects.
    The highest performance score in each metric is highlighted in underlined bold,
    while scores that surpass those of REVECA are indicated in bold.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 Noisy-C-WAH 中增加了 20 个虚拟对象的消融研究结果。每个指标中的最高性能分数用下划线粗体标出，而超过 REVECA 的分数用粗体标记。
- en: '|  | SS $\downarrow$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | SS $\downarrow$ |'
- en: '| REVECA | 63.60 | 54.37 | 8.80 | 103.03 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| REVECA | 63.60 | 54.37 | 8.80 | 103.03 |'
- en: '| w/o spatial info | 89.00 | 77.86 | 13.10 | 126.00 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 无空间信息 | 89.00 | 77.86 | 13.10 | 126.00 |'
- en: '| w/o relevance score | 95.30 | 79.66 | 9.80 | 110.33 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 无相关性分数 | 95.30 | 79.66 | 9.80 | 110.33 |'
- en: '| w/o Validation Module | 75.80 | 68.53 | 5.00 | 46.84 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 无验证模块 | 75.80 | 68.53 | 5.00 | 46.84 |'
- en: '| always ask before action | 56.80 | 49.97 | 13.30 | 127.14 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 始终在行动前询问 | 56.80 | 49.97 | 13.30 | 127.14 |'
- en: '| w/o collaborator’s info | 80.40 | 70.96 | 9.70 | 110.42 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 无协作者信息 | 80.40 | 70.96 | 9.70 | 110.42 |'
- en: '| K=2 | 65.20 | 55.68 | 7.40 | 109.16 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| K=2 | 65.20 | 55.68 | 7.40 | 109.16 |'
- en: '| K=4 | 72.50 | 62.38 | 9.90 | 120.30 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| K=4 | 72.50 | 62.38 | 9.90 | 120.30 |'
- en: '| R=3 | 64.60 | 55.15 | 8.60 | 95.03 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| R=3 | 64.60 | 55.15 | 8.60 | 95.03 |'
- en: '| R=5 | 76.90 | 64.86 | 7.70 | 100.05 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| R=5 | 76.90 | 64.86 | 7.70 | 100.05 |'
- en: To demonstrate the significance of each component in our framework, we conducted
    an ablation study within C-WAH and Noisy-C-WAH environments, the latter augmented
    with 20 dummy objects. The results are presented in [Table 3](#S5.T3 "Table 3
    ‣ 5.4 Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation") and  [Table 4](#S5.T4 "Table 4 ‣ 5.4
    Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using Information
    Relevance and Plan Validation").
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们框架中每个组件的重要性，我们在 C-WAH 和 Noisy-C-WAH 环境中进行了消融研究，后者增加了 20 个虚拟对象。结果呈现在 [表
    3](#S5.T3 "Table 3 ‣ 5.4 Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative
    Agents using Information Relevance and Plan Validation") 和 [表 4](#S5.T4 "Table
    4 ‣ 5.4 Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using
    Information Relevance and Plan Validation") 中。
- en: '![Refer to caption](img/76a083036a4eac7024336f1fd2c9e7ab.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/76a083036a4eac7024336f1fd2c9e7ab.png)'
- en: 'Figure 9: Two example scenarios involving Alice, who needs to place a green
    plate and a red plate into the sink. This demonstrates the difference in path
    planning efficiency with and without using spatial information. In scenario (a),
    Alice, without spatial information, follows a suboptimal path to complete the
    task, moving back and forth unnecessarily. Scenario (b) showcases the improved
    efficiency when spatial information is used, enabling Alice to plan a more direct
    and logical route.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：两个示例场景，涉及 Alice，她需要将一个绿色盘子和一个红色盘子放入水槽。这展示了使用和不使用空间信息在路径规划效率上的差异。在场景（a）中，Alice
    没有空间信息，沿着一个次优路径完成任务，不必要地来回移动。场景（b）展示了使用空间信息时的效率提升，使 Alice 能够规划出更直接和合逻辑的路线。
- en: Initially, we examined the REVECA without spatial information and without relevance
    scores to evaluate their impact on performance. The absence of spatial information
    degraded performance across all metrics, highlighting its importance in effective
    planning. Omitting relevance scores improved SS and TD scores but worsened CS
    scores in C-WAH while degrading all scores in Noisy-C-WAH. This suggests that
    relevance scores are crucial in more complex environments like Noisy-C-WAH with
    over 20 objects. To aid readers understand, example scenarios with and without
    using spatial information are illustrated in [Figure 9](#S5.F9 "Figure 9 ‣ 5.4
    Ablation Study Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using Information
    Relevance and Plan Validation")
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我们在没有空间信息和相关性得分的情况下检查了 REVECA，以评估它们对性能的影响。缺少空间信息导致所有指标的性能下降，突出显示了其在有效规划中的重要性。忽略相关性得分改善了
    C-WAH 的 SS 和 TD 得分，但恶化了 CS 得分，同时在 Noisy-C-WAH 中导致所有得分下降。这表明相关性得分在如 Noisy-C-WAH
    这样更复杂的环境中（超过 20 个对象）至关重要。为了帮助读者理解，图 [图 9](#S5.F9 "Figure 9 ‣ 5.4 Ablation Study
    Results ‣ 5 Experiment ‣ LLM-Based Cooperative Agents using Information Relevance
    and Plan Validation") 展示了使用和不使用空间信息的示例场景。
- en: Next, we tested REVECA without the Validation Module and with a setting where
    communication occurred before every action. The absence of the Validation Module
    worsened SS and TD scores. Conversely, frequent communication before every action
    slightly improved SS and TD but significantly degraded CS and CC scores, indicating
    that the Validation Module enhances performance without excessive communication.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在没有验证模块的情况下测试了 REVECA，并设置了在每次行动之前进行通信的情境。缺少验证模块导致 SS 和 TD 得分恶化。相反，在每次行动之前频繁沟通略微改善了
    SS 和 TD，但显著降低了 CS 和 CC 得分，这表明验证模块在没有过度通信的情况下能提升性能。
- en: We also tested the REVECA without considering collaborator information, resulting
    in significantly degraded performance across all scores. This underscores the
    importance of collaborator information, even when some details are inferred.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还测试了没有考虑协作者信息的 REVECA，结果所有得分的性能显著下降。这突显了协作者信息的重要性，即使有些细节是推断出来的。
- en: Lastly, we varied the number of plans ($K$ = 4 yielded the best performance.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们改变了计划数量（$K$ = 4 取得了最佳性能）。
- en: '![Refer to caption](img/45383c69886076ef0adc1d760ead005f.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/45383c69886076ef0adc1d760ead005f.png)'
- en: 'Figure 10: User study results with C-WAH. This figure illustrates the mean
    scores and associated standard errors for responses to four research questions.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：使用 C-WAH 的用户研究结果。此图展示了对四个研究问题的平均得分及相关标准误差。
- en: 5.5 User Study Results
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 用户研究结果
- en: 'We conducted a user study to evaluate REVECA’s ability to collaborate seamlessly
    with humans to achieve common goals. Five participants (four men and one woman)
    with an average age of 23.4 years were recruited. The experiment was conducted
    in the C-WAH environment using four methods: REVECA, REVECA without communication
    (w/o comm), REVECA with always ask before action (always ask), and CoELA. Participants
    shared the same observation and action space as the agents and interacted with
    the environment by selecting actions from a predefined list.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个用户研究，以评估REVECA与人类无缝协作以实现共同目标的能力。招募了五名参与者（四名男性和一名女性），平均年龄为23.4岁。实验在C-WAH环境中进行，使用了四种方法：REVECA、无通信的REVECA（w/o
    comm）、始终在行动前询问的REVECA（always ask）和CoELA。参与者与代理共享相同的观察和行动空间，通过从预定义列表中选择行动来与环境互动。
- en: 'Participants completed 5 sub-goals with each method. After each, they answered
    a 7-point Likert scale (1: strongly disagree, 7: strongly agree) questionnaire
    on four research questions: 1) Did the agent respond appropriately to your intentions?
    (Appropriateness), 2) Was the interaction with the agent helpful in achieving
    the goal? (Usefulness), 3) Did the agent’s performance help achieve the goal quickly?
    (Efficiency), and 4) Did you feel a sense of trust with the agent? (Trust) The
    “w/o comm" method excluded Appropriateness and Usefulness questions, due to the
    lack of interaction. Participants were then interviewed about each method.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '参与者使用每种方法完成了5个子目标。在每次实验后，他们回答了一个7点Likert量表（1: 强烈不同意，7: 强烈同意）问卷，涉及四个研究问题：1）代理是否对你的意图作出了适当回应？（适当性），2）与代理的互动是否对实现目标有帮助？（有用性），3）代理的表现是否有助于快速实现目标？（效率），4）你是否感受到与代理的信任感？（信任）由于缺乏互动，“w/o
    comm”方法排除了适当性和有用性的问题。然后，参与者就每种方法进行了采访。'
- en: As shown in [Figure 10](#S5.F10 "Figure 10 ‣ 5.4 Ablation Study Results ‣ 5
    Experiment ‣ LLM-Based Cooperative Agents using Information Relevance and Plan
    Validation"), REVECA scored highest across all four questions, excelling in human
    collaboration. Participants noted that CoELA often produced messages focused on
    status reports and planning rather than addressing their questions, resulting
    in lower scores in Appropriateness and Usefulness. In the “always ask" condition,
    participants found the agent’s repetitive questions disruptive and demotivating.
    Regarding Trust, participants noted that the lack of communication in the “w/o
    comm" method made it difficult to understand the agent’s actions and situation,
    thereby hindering trust-based collaboration.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图10](#S5.F10 "图 10 ‣ 5.4 消融研究结果 ‣ 5 实验 ‣ 基于LLM的信息相关性和计划验证合作代理")所示，REVECA在所有四个问题中得分最高，在人类协作方面表现出色。参与者指出，CoELA通常产生集中于状态报告和计划的消息，而不是解答他们的问题，这导致在适当性和有用性方面的得分较低。在“始终询问”条件下，参与者发现代理的重复问题具有干扰性并且令人沮丧。关于信任，参与者注意到，“w/o
    comm”方法中的缺乏沟通使得理解代理的行动和情况变得困难，从而阻碍了基于信任的协作。
- en: 6 Conclusion
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduced REVECA, an LLM-driven cognitive architecture designed
    for multi-objective household tasks, enabling efficient cooperation between decentralized
    agents under complex partial observations. By leveraging relevance assessment,
    spatial information, and plan validation, REVECA enhances agent cooperation in
    dynamic and partially observable environments, while minimizing continuous communication
    costs and effectively managing irrelevant dummy objects.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了REVECA，一种基于LLM的认知架构，旨在处理多目标家庭任务，实现去中心化代理在复杂部分观察下的高效协作。通过利用相关性评估、空间信息和计划验证，REVECA提升了代理在动态和部分可观察环境中的协作能力，同时最小化了持续通信成本，并有效管理无关的虚拟对象。
- en: REVECA’s applications extend beyond household tasks to gaming, virtual universes,
    and educational environments. In gaming, REVECA can revolutionize NPC behavior,
    enabling adaptive and intelligent interactions for a richer player experience.
    In virtual universes, REVECA can enhance user-agent interactions, facilitating
    collaborative tasks and social interactions for more seamless experiences. In
    education, REVECA can manage complex tasks and provide real-time feedback, supporting
    personalized learning and creating interactive simulations and virtual tutors
    tailored to individual needs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: REVECA的应用超越了家庭任务，扩展到游戏、虚拟宇宙和教育环境。在游戏中，REVECA可以革新NPC行为，实现适应性和智能互动，提供更丰富的玩家体验。在虚拟宇宙中，REVECA可以增强用户与智能体的互动，促进协作任务和社会互动，带来更无缝的体验。在教育中，REVECA可以管理复杂任务并提供实时反馈，支持个性化学习，创建针对个人需求的互动模拟和虚拟辅导员。
- en: However, REVECA has limitations that warrant further exploration. Its effectiveness
    in highly dynamic and unpredictable outdoor settings remains to be validated.
    The framework has been tested primarily with two agents, and scaling to more agents
    may introduce new challenges in social interactions and coordination. Additionally,
    REVECA’s use of a low-level action skill book could be enhanced by integrating
    recent advancements in animation generation technologies, enabling more realistic
    and context-sensitive actions.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，REVECA存在局限性，需要进一步探索。其在高度动态和不可预测的户外环境中的有效性仍待验证。该框架主要在两个智能体的测试中进行，扩展到更多智能体可能会带来社会互动和协调的新挑战。此外，REVECA使用的低级动作技能书可以通过整合近期动画生成技术的进展来增强，实现更真实和情境敏感的动作。
- en: Addressing these limitations could make future REVECA even more robust and applicable
    across a broader range of multi-agent environments and tasks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些局限性可能使未来的REVECA更加稳健，并适用于更广泛的多智能体环境和任务。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B
    Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied agents modularly
    with large language models. arXiv preprint arXiv:2307.02485, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua
    B Tenenbaum, Tianmin Shu, 和 Chuang Gan. 使用大型语言模型模块化构建合作体代理. arXiv预印本 arXiv:2307.02485,
    2023.'
- en: '[2] Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, and Hongyuan
    Zha. Semantically aligned task decomposition in multi-agent reinforcement learning.
    ArXiv, abs/2305.10865, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, 和 Hongyuan
    Zha. 多智能体强化学习中的语义对齐任务分解. ArXiv, abs/2305.10865, 2023.'
- en: '[3] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yi Eve Sun,
    Chen Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang,
    F. Yin, Yitao Liang, and Yaodong Yang. Proagent: Building proactive cooperative
    agents with large language models. In AAAI Conference on Artificial Intelligence,
    2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yi Eve Sun,
    Chen Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang,
    F. Yin, Yitao Liang, 和 Yaodong Yang. Proagent: 使用大型语言模型构建主动合作代理. 在AAAI人工智能会议上,
    2023.'
- en: '[4] Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin
    Shu, Yilun Du, and Chuang Gan. Combo: Compositional world models for embodied
    multi-agent cooperation. ArXiv, abs/2404.10775, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Hongxin Zhang, Zeyuan Wang, Qiushi Lyu, Zheyuan Zhang, Sunli Chen, Tianmin
    Shu, Yilun Du, 和 Chuang Gan. Combo: 具有组合性的世界模型用于体现式多智能体合作. ArXiv, abs/2404.10775,
    2024.'
- en: '[5] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes,
    Michael Lewis, and Katia P. Sycara. Theory of mind for multi-agent collaboration
    via large language models. In Conference on Empirical Methods in Natural Language
    Processing, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes,
    Michael Lewis, 和 Katia P. Sycara. 通过大型语言模型实现多智能体协作的心智理论. 在自然语言处理的实证方法会议上, 2023.'
- en: '[6] Peter Stone and Manuela Veloso. Multiagent systems: A survey from a machine
    learning perspective. Autonomous Robots, 8:345–383, 2000.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Peter Stone 和 Manuela Veloso. 多智能体系统：来自机器学习视角的综述. Autonomous Robots, 8:345–383,
    2000.'
- en: '[7] Sven Gronauer and Klaus Diepold. Multi-agent deep reinforcement learning:
    a survey. Artificial Intelligence Review, 55(2):895–943, 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Sven Gronauer 和 Klaus Diepold. 多智能体深度强化学习：综述. Artificial Intelligence Review,
    55(2):895–943, 2022.'
- en: '[8] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta,
    Roozbeh Mottaghi, and Ali Farhadi. Visual semantic planning using deep successor
    representations. In Proceedings of the IEEE international conference on computer
    vision, pages 483–492, 2017.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta,
    Roozbeh Mottaghi, 和 Ali Farhadi. 使用深度继任者表示进行视觉语义规划. 发表在 IEEE 国际计算机视觉会议上，页码 483–492,
    2017。'
- en: '[9] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin,
    and Yoav Artzi. Mapping instructions to actions in 3d environments with visual
    goal prediction. arXiv preprint arXiv:1809.00786, 2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin,
    和 Yoav Artzi. 在3D环境中通过视觉目标预测将指令映射到动作. arXiv 预印本 arXiv:1809.00786, 2018。'
- en: '[10] Chris Amato, George Dimitri Konidaris, Leslie Pack Kaelbling, and Jonathan P.
    How. Modeling and planning with macro-actions in decentralized pomdps. The journal
    of artificial intelligence research, 64:817–859, 2019.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Chris Amato, George Dimitri Konidaris, Leslie Pack Kaelbling, 和 Jonathan
    P. How. 在去中心化的 POMDP 中建模和规划宏观动作. 人工智能研究杂志, 64:817–859, 2019。'
- en: '[11] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever,
    Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham
    Ruderman, et al. Human-level performance in 3d multiplayer games with population-based
    reinforcement learning. Science, 364(6443):859–865, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever,
    Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham
    Ruderman, 等. 在3D多人游戏中通过基于人群的强化学习实现人类水平表现. 科学, 364(6443):859–865, 2019。'
- en: '[12] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory
    Farquhar, Jakob Foerster, and Shimon Whiteson. Monotonic value function factorisation
    for deep multi-agent reinforcement learning. Journal of Machine Learning Research,
    21(178):1–51, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory
    Farquhar, Jakob Foerster, 和 Shimon Whiteson. 深度多智能体强化学习的单调值函数分解. 机器学习研究杂志, 21(178):1–51,
    2020。'
- en: '[13] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction
    and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Pratyusha Sharma, Antonio Torralba, 和 Jacob Andreas. 通过潜在语言进行技能引导和规划.
    arXiv 预印本 arXiv:2110.01517, 2021。'
- en: '[14] Hengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, and Jakob
    Foerster. Off-belief learning. In International Conference on Machine Learning,
    pages 4369–4379\. PMLR, 2021.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Hengyuan Hu, Adam Lerer, Brandon Cui, Luis Pineda, Noam Brown, 和 Jakob
    Foerster. Off-belief learning. 发表在国际机器学习会议上，页码 4369–4379。PMLR, 2021。'
- en: '[15] Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet: Universal
    multi-agent reinforcement learning via policy decoupling with transformers. arXiv
    preprint arXiv:2101.08001, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Siyi Hu, Fengda Zhu, Xiaojun Chang, 和 Xiaodan Liang. Updet: 通过变换器进行政策解耦的通用多智能体强化学习.
    arXiv 预印本 arXiv:2101.08001, 2021。'
- en: '[16] Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan
    Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained multi-agent
    decision transformer: One big sequence model tackles all smac tasks. arXiv preprint
    arXiv:2112.02845, 2021.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan
    Zhang, Ying Wen, Haifeng Zhang, Jun Wang, 和 Bo Xu. 离线预训练的多智能体决策变换器: 一个大型序列模型应对所有
    SMAC 任务. arXiv 预印本 arXiv:2112.02845, 2021。'
- en: '[17] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, and Richard Everett.
    Collaborating with humans without human data. Advances in Neural Information Processing
    Systems, 34:14502–14515, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] DJ Strouse, Kevin McKee, Matt Botvinick, Edward Hughes, 和 Richard Everett.
    在没有人类数据的情况下与人类合作. 神经信息处理系统进展, 34:14502–14515, 2021。'
- en: '[18] Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory
    diversity for zero-shot coordination. In International conference on machine learning,
    pages 7204–7213\. PMLR, 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Andrei Lupu, Brandon Cui, Hengyuan Hu, 和 Jakob Foerster. 零样本协调的轨迹多样性.
    发表在国际机器学习会议上，页码 7204–7213。PMLR, 2021。'
- en: '[19] Keane Lucas and Ross E Allen. Any-play: An intrinsic augmentation for
    zero-shot coordination. arXiv preprint arXiv:2201.12436, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Keane Lucas 和 Ross E Allen. Any-play: 用于零样本协调的内在增强. arXiv 预印本 arXiv:2201.12436,
    2022。'
- en: '[20] Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and
    Yaodong Yang. Multi-agent reinforcement learning is a sequence modeling problem.
    Advances in Neural Information Processing Systems, 35:16509–16521, 2022.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, 和
    Yaodong Yang. 多智能体强化学习是一个序列建模问题. 神经信息处理系统进展, 35:16509–16521, 2022。'
- en: '[21] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre
    Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent
    games. Advances in Neural Information Processing Systems, 35:24611–24624, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 余超，阿卡什·维鲁，尤金·维尼茨基，郭佳轩，王宇，亚历山大·贝延，吴艺。PPO在合作多智能体游戏中的惊人效果。神经信息处理系统进展，35:24611–24624，2022年。'
- en: '[22] Rui Zhao, Jinming Song, Yufeng Yuan, Haifeng Hu, Yang Gao, Yi Wu, Zhongqian
    Sun, and Wei Yang. Maximum entropy population-based training for zero-shot human-ai
    coordination. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 37, pages 6145–6153, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 赵睿，宋锦铭，袁玉峰，胡海峰，杨高，吴艺，孙中千，杨伟。基于最大熵的零-shot人类-人工智能协调训练。发表于AAAI人工智能会议，卷37，页码6145–6153，2023年。'
- en: '[23] Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing Wang, and
    Wei Pan. Cooperative open-ended learning framework for zero-shot coordination.
    In International Conference on Machine Learning, pages 20470–20484\. PMLR, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 杨力，邵璋，孙继晨，杜亚莉，文颖，王新兵，潘伟。零-shot协调的合作开放式学习框架。发表于国际机器学习会议，页码20470–20484。PMLR，2023年。'
- en: '[24] Yang Li, Shao Zhang, Jichen Sun, Wenhao Zhang, Yali Du, Ying Wen, Xinbing
    Wang, and Wei Pan. Tackling cooperative incompatibility for zero-shot human-ai
    coordination. arXiv preprint arXiv:2306.03034, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 杨力，邵璋，孙继晨，张文浩，杜亚莉，文颖，王新兵，潘伟。解决零-shot人类-人工智能协调的合作不兼容问题。arXiv预印本 arXiv:2306.03034，2023年。'
- en: '[25] Yifan Zhong, Jakub Grudzien Kuba, Xidong Feng, Siyi Hu, Jiaming Ji, and
    Yaodong Yang. Heterogeneous-agent reinforcement learning. Journal of Machine Learning
    Research, 25:1–67, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 钟一凡，雅库布·格鲁兹延·库巴，冯锡栋，胡思怡，季嘉铭，杨耀东。异质智能体强化学习。机器学习研究杂志，25:1–67，2024年。'
- en: '[26] Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor
    Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 瑞安·洛，吴艺，阿维夫·塔玛，尚让·哈布，OpenAI的皮特·阿贝尔，伊戈尔·莫达奇。用于混合合作-竞争环境的多智能体演员-评论家。神经信息处理系统进展，30，2017年。'
- en: '[27] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs,
    Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor:
    An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474,
    2017.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 埃里克·科尔夫，鲁兹贝赫·莫塔吉，温森·汉，伊莱·范德比尔特，卢卡·韦赫斯，阿尔瓦罗·赫拉斯提，马特·德伊特克，基安娜·艾赫萨尼，丹尼尔·戈登，朱雨可，等。Ai2-thor：用于视觉AI的互动3D环境。arXiv预印本
    arXiv:1712.05474，2017年。'
- en: '[28] Tianmin Shu and Yuandong Tian. M3̂rl: Mind-aware multi-agent management
    reinforcement learning. arXiv preprint arXiv:1810.00147, 2018.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 田敏舒，田远东。M3̂rl：心智感知的多智能体管理强化学习。arXiv预印本 arXiv:1810.00147，2018年。'
- en: '[29] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and
    Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 9068–9079,
    2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 费霞，阿米尔·R·扎米尔，何志阳，亚历山大·萨克斯，吉滕德拉·马利克，西尔维奥·萨瓦雷斯。Gibson env：用于具身智能体的真实世界感知。发表于IEEE计算机视觉与模式识别会议，页码9068–9079，2018年。'
- en: '[30] Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory
    Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob
    Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint
    arXiv:1902.04043, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 米卡耶尔·萨姆维利安，塔比什·拉希德，克里斯蒂安·施罗德·德·维特，格雷戈里·法尔夸尔，南塔斯·纳尔代利，蒂姆·GJ·鲁德纳，江志满，菲利普·HS·托尔，雅各布·福斯特，希蒙·怀特森。星际争霸多智能体挑战。arXiv预印本
    arXiv:1902.04043，2019年。'
- en: '[31] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans,
    Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat:
    A platform for embodied ai research. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 9339–9347, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 马诺利斯·萨瓦，阿比谢克·卡迪安，奥列克桑德·马克西梅茨，赵一力，埃里克·维曼斯，巴瓦娜·简，朱利安·斯特劳布，刘佳，弗拉德伦·科尔图恩，吉滕德拉·马利克，等。Habitat：一个用于具身AI研究的平台。发表于IEEE/CVF国际计算机视觉会议，页码9339–9347，2019年。'
- en: '[32] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han,
    Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting
    grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 10740–10749, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 莫希特·什里达尔，杰西·托马森，丹尼尔·戈登，约纳坦·比斯克，温森·汉，鲁兹贝赫·莫塔吉，卢克·泽特尔梅耶，迪特·福克斯。Alfred：用于解释日常任务的基础指令的基准测试。发表于IEEE/CVF计算机视觉与模式识别会议，页码10740–10749，2020年。'
- en: '[33] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu,
    Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based
    interactive environment. In Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, pages 11097–11107, 2020.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu,
    Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang 等人。Sapien: 一个基于部件的模拟交互环境。载于《IEEE/CVF
    计算机视觉与模式识别会议论文集》，第11097–11107页，2020年。'
- en: '[34] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B
    Tenenbaum, Sanja Fidler, and Antonio Torralba. Watch-and-help: A challenge for
    social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890,
    2020.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Xavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua
    B Tenenbaum, Sanja Fidler 和 Antonio Torralba。Watch-and-help：社会感知和人机协作的挑战。arXiv
    预印本 arXiv:2010.09890，2020年。'
- en: '[35] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange,
    Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek
    Hakkani-Tur. Teach: Task-driven embodied agents that chat. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 36, pages 2017–2025, 2022.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange,
    Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur 和 Dilek Hakkani-Tur。Teach：任务驱动的具身代理聊天系统。载于《AAAI
    人工智能大会论文集》，第36卷，第2017–2025页，2022年。'
- en: '[36] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava,
    Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai
    Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities
    and realistic simulation. In Conference on Robot Learning, pages 80–93\. PMLR,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava,
    Roberto Martín-Martín, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai
    Sun 等人。Behavior-1k：具有1000种日常活动和逼真模拟的具身AI基准。在《机器人学习会议》，第80–93页。PMLR，2023年。'
- en: '[37] Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang,
    Yilun Du, Joshua B Tenenbaum, and Chuang Gan. Hazard challenge: Embodied decision
    making in dynamically changing environments. arXiv preprint arXiv:2401.12975,
    2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang,
    Yilun Du, Joshua B Tenenbaum 和 Chuang Gan。Hazard challenge：在动态变化环境中的具身决策制定。arXiv
    预印本 arXiv:2401.12975，2024年。'
- en: '[38] Jiechuan Jiang and Zongqing Lu. Learning attentional communication for
    multi-agent cooperation. In Neural Information Processing Systems, 2018.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Jiechuan Jiang 和 Zongqing Lu。学习注意力通信以促进多代理合作。载于《神经信息处理系统会议》，2018年。'
- en: '[39] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh,
    Michael G. Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication.
    In International Conference on Machine Learning, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh,
    Michael G. Rabbat 和 Joelle Pineau。Tarmac：针对性多代理通信。载于《国际机器学习会议》，2018年。'
- en: '[40] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter
    Abbeel, and Anca Dragan. On the utility of learning about humans for human-ai
    coordination. Advances in neural information processing systems, 32, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter
    Abbeel 和 Anca Dragan。研究了解人类对人机协调的效用。《神经信息处理系统进展》，32，2019年。'
- en: '[41] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
    language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou 等人。基于大型语言模型的代理的崛起与潜力：一项调查。arXiv 预印本
    arXiv:2309.07864，2023年。'
- en: '[42] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language
    model based autonomous agents. Frontiers of Computer Science, 18(6):1–26, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin 等人。基于大型语言模型的自主代理调查。《计算机科学前沿》，18(6):1–26，2024年。'
- en: '[43] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan,
    Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al. Pre-trained language
    models for interactive decision-making. Advances in Neural Information Processing
    Systems, 35:31199–31212, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan,
    Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar 等人。预训练语言模型在交互决策中的应用。《神经信息处理系统进展》，35:31199–31212，2022年。'
- en: '[44] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale
    Schuurmans. Foundation models for decision making: Problems, methods, and opportunities.
    arXiv preprint arXiv:2303.04129, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel 和 Dale Schuurmans。决策制定的基础模型：问题、方法和机会。arXiv
    预印本 arXiv:2303.04129，2023年。'
- en: '[45] Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths.
    Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, 和 Thomas L Griffiths。语言代理的认知架构。arXiv
    预印本 arXiv:2309.02427，2023。'
- en: '[46] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, 和 Anima Anandkumar。Voyager：一个开放式的具体现实代理与大语言模型。arXiv 预印本 arXiv:2305.16291，2023。'
- en: '[47] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete
    Florence, Igor Mordatch, Sergey Levine, Karol Hausman, et al. Grounded decoding:
    Guiding text generation with grounded models for robot control. arXiv preprint
    arXiv:2303.00855, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Wenlong Huang, Fei Xia, Dhruv Shah, Danny Driess, Andy Zeng, Yao Lu, Pete
    Florence, Igor Mordatch, Sergey Levine, Karol Hausman 等人。基础解码：通过基础模型指导文本生成以进行机器人控制。arXiv
    预印本 arXiv:2303.00855，2023。'
- en: '[48] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert
    Jankowski, Yanghua Xiao, and Deqing Yang. Distilling script knowledge from large
    language models for constrained language planning. arXiv preprint arXiv:2305.05252,
    2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Robert
    Jankowski, Yanghua Xiao, 和 Deqing Yang。从大语言模型中提取脚本知识以进行受限语言规划。arXiv 预印本 arXiv:2305.05252，2023。'
- en: '[49] Wenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence,
    Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,
    Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian
    Ichter. Inner monologue: Embodied reasoning through planning with language models.
    In Conference on Robot Learning, 2022.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Wenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence,
    Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,
    Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, 和 Brian Ichter。内在独白：通过语言模型规划进行具体现实推理。在机器人学习大会，2022。'
- en: '[50] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language
    models as zero-shot planners: Extracting actionable knowledge for embodied agents.
    In International Conference on Machine Learning, pages 9118–9147\. PMLR, 2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch。语言模型作为零-shot
    规划者：为具体现实代理提取可操作知识。在国际机器学习大会，页码 9118–9147\. PMLR，2022。'
- en: '[51] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius,
    and Stefanie Tellex. Planning with large language models via corrective re-prompting.
    In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius,
    和 Stefanie Tellex。通过纠正重新提示进行大语言模型规划。在 NeurIPS 2022 决策制定基础模型研讨会，2022。'
- en: '[52] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi,
    Lior Horesh, Biplav Srivastava, Francesco Fabiano, and Andrea Loreggia. Plansformer:
    Generating symbolic plans using transformers. arXiv preprint arXiv:2212.08681,
    2022.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Vishal Pallagani, Bharath Muppasani, Keerthiram Murugesan, Francesca Rossi,
    Lior Horesh, Biplav Srivastava, Francesco Fabiano, 和 Andrea Loreggia。Plansformer：使用变换器生成符号计划。arXiv
    预印本 arXiv:2212.08681，2022。'
- en: '[53] Maitrey Gramopadhye and Daniel Szafir. Generating executable action plans
    with environmentally-aware language models. In 2023 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS), pages 3568–3575\. IEEE, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Maitrey Gramopadhye 和 Daniel Szafir。生成可执行的行动计划与环境感知语言模型。在 2023 IEEE/RSJ
    国际智能机器人与系统大会（IROS），页码 3568–3575\. IEEE，2023。'
- en: '[54] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and
    Yitao Liang. Describe, explain, plan and select: interactive planning with llms
    enables open-world multi-task agents. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, 和
    Yitao Liang。描述、解释、规划和选择：通过 LLM 进行交互式规划使得开放世界多任务代理成为可能。《神经信息处理系统进展》，36，2024。'
- en: '[55] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha
    Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general
    intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha
    Nori, Hamid Palangi, Marco Tulio Ribeiro, 和 Yi Zhang. 人工通用智能的火花：与 GPT-4 的早期实验。ArXiv，abs/2303.12712，2023。'
- en: '[56] Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the
    impact of input length on the reasoning performance of large language models.
    ArXiv, abs/2402.14848, 2024.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Mosh Levy, Alon Jacoby, 和 Yoav Goldberg。同样的任务，更多的令牌：输入长度对大语言模型推理性能的影响。ArXiv，abs/2402.14848，2024。'
- en: '[57] Tomer David Ullman. Large language models fail on trivial alterations
    to theory-of-mind tasks. ArXiv, abs/2302.08399, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Tomer David Ullman. 大型语言模型在理论心理学任务的微小改动上表现不佳。ArXiv, abs/2302.08399, 2023。'
- en: '[58] Daniel S. Bernstein, Shlomo Zilberstein, and Neil Immerman. The complexity
    of decentralized control of markov decision processes. In Conference on Uncertainty
    in Artificial Intelligence, 2000.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Daniel S. Bernstein, Shlomo Zilberstein 和 Neil Immerman. 马尔可夫决策过程的去中心化控制的复杂性。在《人工智能不确定性会议》，2000。'
- en: '[59] Claudia V. Goldman and Shlomo Zilberstein. Optimizing information exchange
    in cooperative multi-agent systems. In Adaptive Agents and Multi-Agent Systems,
    2003.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Claudia V. Goldman 和 Shlomo Zilberstein. 优化合作多智能体系统中的信息交换。在《自适应智能体与多智能体系统》，2003。'
- en: '[60] Matthijs T. J. Spaan, Geoffrey J. Gordon, and Nikos A. Vlassis. Decentralized
    planning under uncertainty for teams of communicating agents. In Adaptive Agents
    and Multi-Agent Systems, 2006.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Matthijs T. J. Spaan, Geoffrey J. Gordon 和 Nikos A. Vlassis. 面对不确定性的去中心化规划用于通信团队。在《自适应智能体与多智能体系统》，2006。'
- en: '[61] Dhruv Batra, Angel X. Chang, S. Chernova, Andrew J. Davison, Jia Deng,
    Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi,
    Manolis Savva, and Hao Su. Rearrangement: A challenge for embodied ai. ArXiv,
    abs/2011.01975, 2020.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Dhruv Batra, Angel X. Chang, S. Chernova, Andrew J. Davison, Jia Deng,
    Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi,
    Manolis Savva 和 Hao Su. Rearrangement: 具身 AI 的挑战。ArXiv, abs/2011.01975, 2020。'
- en: '[62] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee,
    and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought
    reasoning by large language models. In Annual Meeting of the Association for Computational
    Linguistics, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee
    和 Ee-Peng Lim. 计划与解决提示：通过大型语言模型改善零-shot 连锁思维推理。在《计算语言学协会年会》，2023。'
- en: '[63] Simeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu, and Mohit Iyyer. Pearl:
    Prompting large language models to plan and execute actions over long documents.
    ArXiv, abs/2305.14564, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Simeng Sun, Y. Liu, Shuo Wang, Chenguang Zhu 和 Mohit Iyyer. Pearl: 提示大型语言模型计划和执行长文档中的动作。ArXiv,
    abs/2305.14564, 2023。'
- en: '[64] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
    and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning.
    In Neural Information Processing Systems, 2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan
    和 Shunyu Yao. Reflexion: 带有语言强化学习的语言智能体。在《神经信息处理系统》，2023。'
- en: '[65] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish
    Sabharwal, Mohit Bansal, and Tushar Khot. Adapt: As-needed decomposition and planning
    with language models. ArXiv, abs/2311.05772, 2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish
    Sabharwal, Mohit Bansal 和 Tushar Khot. Adapt: 基于需求的分解与语言模型规划。ArXiv, abs/2311.05772,
    2023。'
- en: '[66] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
    Iwasawa. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916,
    2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo 和 Yusuke
    Iwasawa. 大型语言模型是零-shot 推理者。ArXiv, abs/2205.11916, 2022。'
- en: '[67] Xavier Puig, Kevin Kyunghwan Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities
    via programs. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 8494–8502, 2018.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Xavier Puig, Kevin Kyunghwan Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler 和 Antonio Torralba. Virtualhome: 通过程序模拟家庭活动。2018 IEEE/CVF 计算机视觉与模式识别会议，页码
    8494–8502，2018。'
- en: '[68] Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar,
    Dan Gutfreund, Daniel L. K. Yamins, James J. DiCarlo, Josh H. McDermott, Antonio
    Torralba, and Joshua B. Tenenbaum. The threedworld transport challenge: A visually
    guided task-and-motion planning benchmark towards physically realistic embodied
    ai. 2022 International Conference on Robotics and Automation (ICRA), pages 8847–8854,
    2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar,
    Dan Gutfreund, Daniel L. K. Yamins, James J. DiCarlo, Josh H. McDermott, Antonio
    Torralba 和 Joshua B. Tenenbaum. The threedworld transport challenge: 面向物理真实感的具身
    AI 的视觉引导任务和运动规划基准。2022 国际机器人与自动化会议（ICRA），页码 8847–8854，2021。'
- en: '[69] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer,
    Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano,
    Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis, Kevin T.
    Feigelis, Daniel Bear, Dan Gutfreund, David Cox, James J. DiCarlo, Josh H. McDermott,
    Joshua B. Tenenbaum, and Daniel L. K. Yamins. Threedworld: A platform for interactive
    multi-modal physical simulation. ArXiv, abs/2007.04954, 2020.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer,
    Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano,
    Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis, Kevin T.
    Feigelis, Daniel Bear, Dan Gutfreund, David Cox, James J. DiCarlo, Josh H. McDermott,
    Joshua B. Tenenbaum, 和 Daniel L. K. Yamins. Threedworld: 一个用于交互式多模态物理模拟的平台。ArXiv,
    abs/2007.04954, 2020.'
- en: '[70] Richard E. Korf. Planning as search: A quantitative approach. Artif. Intell.,
    33:65–88, 1987.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Richard E. Korf. 规划作为搜索：一种定量方法。Artif. Intell., 33:65–88, 1987.'
