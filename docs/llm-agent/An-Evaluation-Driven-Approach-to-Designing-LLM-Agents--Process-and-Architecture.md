<!--yml

类别：未分类

日期：2025-01-11 11:55:08

-->

# 基于评估驱动的方法设计LLM代理：过程与架构

> 来源：[https://arxiv.org/html/2411.13768/](https://arxiv.org/html/2411.13768/)

Boming Xia12, Qinghua Lu12, Liming Zhu12, Zhenchang Xing13, Dehai Zhao1, Hao Zhang1 1CSIRO’s Data61, Sydney, Australia 2University of New South Wales, Sydney, Australia 3Australian National University, Canberra, Australia

###### 摘要

大型语言模型（LLMs）的出现使得LLM代理的开发成为可能，这些代理能够自主地实现不完全明确的目标，并通过部署后的改进持续进化，有时甚至无需更新代码或模型。传统方法，如预定义的测试用例和代码/模型重新开发流程，无法解决LLM代理开发中的独特挑战，特别是在质量和风险控制方面。本文介绍了一种基于评估驱动的设计方法，灵感来自于测试驱动开发，旨在解决这些挑战。通过多声部文献综述（MLR），我们综合现有的LLM评估方法，并提出了一种新颖的过程模型和参考架构，专门为LLM代理设计。所提出的方法将在线和离线评估相结合，以支持适应性运行时调整和系统化的离线再开发，通过持续融合评估结果，包括来自人类和AI评估者的细粒度反馈，改善运行时流程、工件、系统架构和LLM。

###### 索引词：

大型语言模型（Large Language Model，LLM）、代理、评估、架构、人工智能安全、负责任的人工智能

## 引言

大型语言模型（LLMs）是具有数百亿个参数的大规模预训练语言模型，能够适应各种下游任务，如回答问题和总结信息[[1](https://arxiv.org/html/2411.13768v1#bib.bib1)]。LLM的最新进展使得基于LLM的代理的创建成为可能，这些代理通常被称为LLM代理，能够自主执行复杂任务（例如，AI科学家[[2](https://arxiv.org/html/2411.13768v1#bib.bib2)]）。与传统的依赖于详细指令来执行特定任务的AI/LLM系统不同，LLM代理通过感知上下文、推理、规划和执行工作流程，自动实现高层次的、不完全指定的目标，同时利用外部工具、知识库和其他代理来丰富其能力[[3](https://arxiv.org/html/2411.13768v1#bib.bib3)]。

尽管 LLM 代理具有提高生产力的潜力，但它们也引发了关于性能和安全的重大关注[[4](https://arxiv.org/html/2411.13768v1#bib.bib4)]。这些代理通常会表现出不可预测或不一致的行为，特别是在处理复杂或模糊任务时。此外，LLM 代理的自治性，比如决定使用哪些工具，可能导致结果偏离人类目标或违反治理标准。因此，严格的评估对于确保 LLM 代理始终与预期目标和治理要求保持一致至关重要，从而最小化不良后果。

尽管对 LLM 和 LLM 代理的测试与基准框架越来越感兴趣（例如，[[5](https://arxiv.org/html/2411.13768v1#bib.bib5)，[6](https://arxiv.org/html/2411.13768v1#bib.bib6)]），但在评估 LLM 代理的独特挑战方面仍存在显著的空白。具体来说，评估这些代理的有效性需要考虑以下几点：

+   •

    系统级评估。目前的评估框架主要集中在模型级评估，忽视了系统级评估。LLM 代理是复合型 AI 系统，不仅包括 LLM，还包括多个 LLM 之外的组件[[7](https://arxiv.org/html/2411.13768v1#bib.bib7)]，例如上下文引擎和防护措施[[8](https://arxiv.org/html/2411.13768v1#bib.bib8)]。虽然像 LangSmith¹¹1[https://www.langchain.com/langsmith](https://www.langchain.com/langsmith) 和 DeepEval²²2[https://docs.confident-ai.com/](https://docs.confident-ai.com/) 这样的工具和框架为系统级评估提供了部分支持，但通常过于关注特定的交互，例如提示-响应对或工具调用，而没有涉及完整的运行时范围。有效的系统级评估必须涵盖管道（包括提示、中间结果和最终结果）和工件（包括目标、记忆、推理、计划、工作流、工具、知识库、其他代理、LLM 和防护措施）[[9](https://arxiv.org/html/2411.13768v1#bib.bib9)，[8](https://arxiv.org/html/2411.13768v1#bib.bib8)]。

+   •

    评估驱动设计。虽然现有的工具和框架解决了部署前的离线评估和运行时监控方面的问题，但它们通常缺乏一个统一的方法，无法将持续评估与运行时适应和离线迭代开发结合起来。与传统软件不同，LLM 代理在高层次、不明确的目标下运行，并通过目标解释、推理、规划和执行的迭代周期进行动态适应。这种自适应行为，加上对人类/AI反馈和记忆的依赖，需要一种评估驱动设计方法，以系统地将不断发展的代理行为与质量、安全和运营目标对齐。

+   •

    评估结果的使用。

    在传统软件和独立 LLM 测试中，评估结果通常只识别出故障点，从而触发代码更新或 LLM 的再训练/微调。然而，对于 LLM 代理来说，评估结果具有更广泛的范围和影响。它们可以通过丰富的上下文（例如，纳入正面和负面示例）来指导优化提示，提升代理计划和工作流程，从而改进决策和执行，并更新测试和安全案例[[10](https://arxiv.org/html/2411.13768v1#bib.bib10)]，以反映不断变化的操作环境。

为了应对这些挑战，本文提出了一种受测试驱动开发启发的评估驱动方法，嵌入了贯穿整个 LLM 代理生命周期的持续评估，并考虑了全面的运行时管道和工件。我们探索了两个研究问题（RQ），每个问题都形成了一个关键贡献：

1.  1.

    RQ1：如何系统地评估 LLM 代理？为此，我们提出了一个结构化的过程模型，能够实现全面的、贯穿整个生命周期的评估，为研究人员和实践者在进行 LLM 代理的持续评估时提供具体且可操作的指导。

1.  2.

    RQ2：如何将持续评估集成到代理设计的架构中？为了解答这个问题，我们设计了一个参考架构，将评估作为核心设计元素，整合在线和离线评估，以指导代理的改进。

本文的其余部分组织结构如下。第二节讨论了背景与相关工作。第三节介绍了方法论细节。第四节展示了一个软件过程模型，指导 LLM 代理评估，第五节提出了一个面向评估驱动设计的 LLM 代理参考架构。第六节讨论了有效性的威胁，第七节总结了本文并概述了未来的工作。

## II 背景与相关工作

### II-A 评估 LLM 代理的重要性

评估在衡量准确性和功能正确性方面发挥着基础性作用，确保LLM代理在多样化、不断发展的环境中可靠地执行[[11](https://arxiv.org/html/2411.13768v1#bib.bib11)]。除了性能之外，对质量和风险属性（如公平性和安全性）的评估对于管理将LLM代理部署在复杂现实环境中的固有风险至关重要[[12](https://arxiv.org/html/2411.13768v1#bib.bib12), [13](https://arxiv.org/html/2411.13768v1#bib.bib13)]。此外，能力评估衡量代理是否适合执行复杂任务，识别既定功能和可能出现的无意行为，这些行为可能会带来安全隐患[[14](https://arxiv.org/html/2411.13768v1#bib.bib14)]。最后，持续评估支持不断改进和风险控制，使LLM代理能够从评估结果和操作数据中学习，适应新输入，并与不断变化的环境保持一致[[15](https://arxiv.org/html/2411.13768v1#bib.bib15)]。这些评估共同保障了LLM代理在动态应用中的安全性、适应性和现实世界适用性。

### II-B LLM代理评估中的架构与操作挑战

由于LLM代理具有复杂的架构和动态的操作需求，它们在评估时提出了独特的挑战。LLM代理主要由现成的LLM构建，这些代理将焦点从（再）训练和模型/代码更新转移到了系统级集成和持续的超出模型的改进。它们自主运行，必须处理不断变化的输入和外部数据源，这要求其具备适应实时条件的能力。复杂的架构组件会产生相互依赖性和潜在的故障点，增加了需要仔细评估的必要性。

LLM的非确定性行为，即相同的输入可能产生不同的输出，进一步加剧了一致性评估的复杂性，并要求灵活的评估方法[[16](https://arxiv.org/html/2411.13768v1#bib.bib16)]。例如，客户服务代理可能根据先前的交互，对相同的问题生成不同的回答，这使得统一评估变得具有挑战性。它们通过运行时反馈实现持续改进，这会引入突现行为，这些行为可能带来静态评估方法无法充分捕捉的安全风险[[17](https://arxiv.org/html/2411.13768v1#bib.bib17)]。这些挑战共同强调了需要一种评估驱动的设计方法，该方法将持续评估机制嵌入其中，以确保代理在整个生命周期内的安全、可靠和适应性操作。

### II-C 相关工作

#### II-C1 传统软件评估方法

传统的软件评估方法，包括测试驱动开发（Test-Driven Development）[[18](https://arxiv.org/html/2411.13768v1#bib.bib18)]和行为驱动开发（Behavior-Driven Development）[[19](https://arxiv.org/html/2411.13768v1#bib.bib19)]，强调在部署前进行早期测试，以确保功能的正确性。这些方法通常将评估限制在部署前，测试用例基于详细的、预定义的需求派生而来。尽管这些方法为确保性能和安全性提供了坚实的基础，但它们不太适合LLM代理的动态、不断发展的特性，LLM代理是基于高层目标而非明确需求操作的，并且会根据实时数据和用户互动动态调整行为。这样的传统方法并不天然地支持持续的、部署后的评估，也不容易基于评估结果支持持续改进。

#### II-C2 LLM代理评估方法

现有的评估框架和基准主要集中在模型层面[[20](https://arxiv.org/html/2411.13768v1#bib.bib20)]，专注于特定任务（例如，编码[[21](https://arxiv.org/html/2411.13768v1#bib.bib21)，[22](https://arxiv.org/html/2411.13768v1#bib.bib22)，[23](https://arxiv.org/html/2411.13768v1#bib.bib23)]和检索增强生成（RAG）[[24](https://arxiv.org/html/2411.13768v1#bib.bib24)，[25](https://arxiv.org/html/2411.13768v1#bib.bib25)])或某些领域（例如，医疗[[26](https://arxiv.org/html/2411.13768v1#bib.bib26)，[27](https://arxiv.org/html/2411.13768v1#bib.bib27)]，法律[[28](https://arxiv.org/html/2411.13768v1#bib.bib28)，[29](https://arxiv.org/html/2411.13768v1#bib.bib29)]，和金融[[30](https://arxiv.org/html/2411.13768v1#bib.bib30)，[31](https://arxiv.org/html/2411.13768v1#bib.bib31)])。尽管这些评估框架具有价值，但将其应用于评估能够跨多个任务和领域进行操作的整体、多组件LLM代理需要大量的集成工作[[32](https://arxiv.org/html/2411.13768v1#bib.bib32)，[33](https://arxiv.org/html/2411.13768v1#bib.bib33)]。

此外，许多现有基准依赖于固定的数据集和任务，带来了数据污染的风险[[34](https://arxiv.org/html/2411.13768v1#bib.bib34)]，使得在动态环境中评估代理变得困难。为了解决这一局限性，出现了适应变化的实时基准（例如，[[35](https://arxiv.org/html/2411.13768v1#bib.bib35)，[36](https://arxiv.org/html/2411.13768v1#bib.bib36)]）。尽管实时基准提供了一定的适应性，但它们主要通过周期性的数据更新进行，并未完全捕捉到运行时在线评估中所见的细微行为。

随着LLM代理的出现，系统级评估框架逐渐得到重视，这些框架评估的是整体代理行为。然而，许多此类框架侧重于端到端评估，评估LLM代理在各种任务和环境中的最终输出（即最终的成功/通过率）。虽然这些方法能提供代理整体表现的洞见，但它们往往忽视了中间决策步骤的细微差别以及各个代理组件的贡献，从而限制了它们在识别具体改进领域中的效用[[33](https://arxiv.org/html/2411.13768v1#bib.bib33)]。因此，出现了更为细化的框架（例如[[33](https://arxiv.org/html/2411.13768v1#bib.bib33), [37](https://arxiv.org/html/2411.13768v1#bib.bib37)]），以及像LangSmith这样的工具，它们支持基于步骤和轨迹的评估。通过强调中间阶段，这些框架揭示了操作工作流，提供了针对性改进和风险控制所需的重要数据。

另一种正在获得关注的方法是使用安全案例，这些案例提供了结构化的、基于证据的论据，用以证明在定义的条件下LLM代理的安全操作[[38](https://arxiv.org/html/2411.13768v1#bib.bib38), [10](https://arxiv.org/html/2411.13768v1#bib.bib10)]。它们作为确保符合安全标准和法规要求的正式文档。安全案例对系统开发者、集成商和监管者等利益相关者尤其有价值，他们依赖这些案例来评估风险、验证操作边界并保持问责。对于LLM代理而言，由于操作环境的动态性和不断变化的特点，需要不断更新安全案例，以反映新兴的风险，并与治理框架保持一致。

这些最新的进展标志着LLM代理评估的重大进展。然而，仍然缺乏一种系统性的方法，将评估结果整合起来，以指导持续的改进和风险控制——这是我们在本研究中所解决的问题。

## III 方法论

![参见标题](img/5c793b236ab2b310d79496a5e817b2d0.png)

图1：研究方法论

本研究采用了MLR（见图[1](https://arxiv.org/html/2411.13768v1#S3.F1 "图1 ‣ III 方法论 ‣ 基于评估驱动的LLM代理设计：过程与架构")）方法，综合了来自学术界和行业界关于LLM代理评估的见解，遵循了已建立的指南[[39](https://arxiv.org/html/2411.13768v1#bib.bib39), [40](https://arxiv.org/html/2411.13768v1#bib.bib40)]。选择MLR是因为它能够全面理解学术理论与行业实践，尤其在设计一个针对各种部署环境中，面向持续评估驱动的LLM代理设计的过程模型和参考架构时尤为重要。

### III-A MLR 规划

规划阶段始于识别需要采取多视角方法来弥合研究与行业视角之间的差距。制定了初步研究方案，包括旨在捕捉 LLM 代理评估全面范围的研究问题（RQ）和搜索词汇。通过试点研究测试了搜索词汇，评估了检索研究的相关性和数量。基于试点结果，方案进行了修订，最终确定了用于指导研究的 MLR 策略。

### III-B 多视角文献综述（MLR）

数据库和搜索策略：学术论文搜索在以下数据库中进行：Google Scholar、IEEE Xplore、ACM Digital Library、Science Direct 和 Springer。Google 搜索被用作灰色文献的来源。主要的搜索词汇为（“large language model” 或 “LLM” 或 “agent”）AND（“evaluate” 或 “benchmark” 或 “test”），并根据需要对词形变化（例如，复数、名词-动词转换和 -ing 形式）进行调整。搜索在 2024 年 6 月 5 日进行，目标是查找自 2022 年以来发布的文献，以反映 ChatGPT 发布后的最新进展。

筛选与选择：在去除重复项后，使用三步筛选过程对来源进行了筛选：标题、摘要和全文审查。两位作者独立审查每个阶段，以确保可靠性，并就筛选结果达成一致。筛选标准侧重于确定与 LLM 代理评估驱动设计相关的来源。符合条件的来源包括学术文章、会议论文、技术报告、白皮书和预印本（例如，arXiv），它们讨论了用于 LLM 代理评估的工具、框架或平台，前提是它们提供了明确的理论或实证贡献，并且可以提供英文版本。排除标准包括与 LLM 代理评估无关的来源（例如，LLM 微调）、缺乏实质性贡献或可信支持的来源，或提供文档不足的工具。

扩展搜索：为了扩大覆盖范围并确保没有遗漏相关研究，基于[[41](https://arxiv.org/html/2411.13768v1#bib.bib41)]对最终选择的文献进行了前向和后向雪球采样。此外，基于作者讨论，加入了初步搜索后发布的关键新文献。当没有从额外来源中出现新的显著主题时，认为已达到了饱和。

质量评估：为了确保严谨性，学术论文和灰色文献根据权威性、方法论和客观性进行评估，只有那些展示了可信和平衡贡献的来源才被纳入评估。工具则单独进行评估，考虑来源的声誉（例如，GitHub 星标的发布者声誉）、文档和维护活动。未能符合可信性和可靠性标准的来源被排除在外。

![参见说明](img/3a9a5d0e8778b60c928a09736d341ba2.png)

图 2：LLM 代理评估的过程模型

数据提取：数据提取系统地映射了与评估相关的元素，以解决研究问题（RQ）并为提出的过程模型和参考架构提供信息。它集中于评估实践，包括时间、范围以及评估结果在持续改进和风险控制中的应用。此外，还收集了架构方面的见解，涵盖了核心代理组件（例如，环境引擎、推理/规划）和评估基础设施（例如，存储、反馈回路），这些组件支持迭代优化。这种方法确保了提取的数据与过程模型和参考架构的设计要素之间有清晰的联系。

数据综合：采用主题分析方法识别反复出现的主题，特别关注评估活动和生命周期覆盖、风险控制以及代理的改进/适应性。我们从MLR中综合了见解，为过程模型和参考架构的结构提供了信息。MLR提供了一个以实证为基础的参考架构，具有行业跨领域的特性、经典性、促进性，并与[[42](https://arxiv.org/html/2411.13768v1#bib.bib42)]保持一致。为了增强可靠性，主题通过内部评审进行三角验证，并通过具有LLM代理设计和评估经验的作者反馈加以确认。这一迭代验证过程确保了综合的主题既严谨又具实际应用性。

## IV 过程模型用于LLM代理评估

为了回答RQ1，我们将MLR中的评估实践综合为一个过程模型（图[2](https://arxiv.org/html/2411.13768v1#S3.F2 "Figure 2 ‣ III-B Multivocal Literature Review (MLR) ‣ III Methodology ‣ An Evaluation-Driven Approach to Designing LLM Agents: Process and Architecture")）。该模型为在代理的开发、部署和运营阶段开展一致的生命周期评估提供了一种结构化方法。除了明确评估活动和范围外，该模型还推动了即时运行时改进（例如，响应实时用户反馈）和迭代优化（例如，通过代理架构调整），并通过利用离线和在线评估的见解支持风险控制。通过标准化这些实践，过程模型确保LLM代理在动态环境中保持安全和有效。

### IV-A 第一步：定义评估计划

过程模型的第一步是建立一个全面的评估计划，指导在LLM代理生命周期中的一致性和聚焦性评估。该计划整合了关键输入，包括用户目标、治理要求和初始代理架构，以确定明确的评估目标、范围和特定场景的评估。

输入：评估计划由三个主要输入提供信息：

+   •

    用户目标：代表用户需求和期望的高层目标，通常较为广泛或抽象（例如，“我需要一个能提供税务建议的代理”）。这些目标被转化为评估场景，概述了潜在的用户交互、相关背景和预期结果。这些场景随后将在步骤 2 中指导测试用例的创建。

+   •

    治理要求：与代理部署环境相关的法律、伦理和安全规定，例如《欧盟人工智能法案》。治理要求指导以合规性为重点的评估，如护栏的有效性，并影响测试用例和安全用例生成的标准。

+   •

    初始代理架构：代理的初步结构和行为蓝图，详细描述了关键组件、相互依赖关系和架构决策。该分析识别出可能影响评估重点的关键功能或依赖的组件，并突出初步的权衡和风险。

流程步骤：评估规划过程包括几个协调的活动，共同制定一个详细的、特定情境的评估计划。

1.  1.

    理解用户目标：作为一个并行分支，此步骤将用户目标转化为具体且可测试的评估场景。这些场景模拟潜在的用户交互、相关环境因素和预期结果，为评估代理行为和整体系统结果创建一个有针对性的基础。

1.  2.

    融入治理要求：与用户目标理解并行进行，此步骤将评估目标和场景与相关治理要求对齐 [[43](https://arxiv.org/html/2411.13768v1#bib.bib43)]。通过在规划阶段早期嵌入这些约束，确保从一开始就遵守规定并减少风险。

1.  3.

    生成评估计划：此最终步骤整合用户目标、治理要求和架构分析的洞察，生成一个结构化的评估计划，涵盖执行流程和生成的产物。该评估计划支持持续评估、后续改进和风险控制，确保代理与不断变化的目标保持一致。

输出：此步骤的主要输出是一个详细的评估计划，为系统化的、贯穿生命周期的评估提供基础。关键信息包括：

+   •

    评估目的和范围：阐明评估的总体目标（例如，准确性/正确性、质量/风险和能力[[20](https://arxiv.org/html/2411.13768v1#bib.bib20)]），并指定评估目标，如特定的中间管道（例如，增强检索生成[[44](https://arxiv.org/html/2411.13768v1#bib.bib44)]）和成果（例如，动态生成的计划和子目标[[45](https://arxiv.org/html/2411.13768v1#bib.bib45), [46](https://arxiv.org/html/2411.13768v1#bib.bib46)]）。这些目标根据架构洞察和用户需求进行优先级排序。此外，守卫有效性也可以通过管道和成果进行评估。

+   •

    评估目标和策略：概述了具体的评估活动和方法，重点是渐进式评估阶段。离线评估首先通过基准测试评估一般性能基线，随后进行更有针对性的测试，这些测试可以根据基准测试结果生成。在线评估则在这些基础上扩展，结合现实世界的反馈和动态条件。这一序列化策略确保评估的全面性，并实现了迭代验证和完善。

+   •

    评估标准和指标：定义了评估结果的定性和定量标准，如相关性、成功率、响应时间和风险阈值。此外，还强调了由人工或AI评估人员提供的解释性说明的价值，这些说明可以补充指标，澄清为什么某些结果被认为是可接受的或需要进一步改进[[47](https://arxiv.org/html/2411.13768v1#bib.bib47)]。

这一步骤还可能产生初步的安全案例[[38](https://arxiv.org/html/2411.13768v1#bib.bib38)]——基于评估目标、场景和架构分析的结构化论证——建立在定义条件下对代理操作安全性的初步证据。这些安全案例作为基准，在评估过程中逐步完善。

### IV-B 第2步：制定评估测试用例

在第1步中建立的评估计划基础上，本步骤通过将通用基准测试与特定场景的测试用例生成相结合，制定测试用例。这些测试用例评估LLM代理的管道和生成的成果，确保涵盖一般和特定场景，包括标准案例和边缘案例。

输入：测试用例的开发来源于多个渠道：

+   •

    评估计划：定义了第1步中的评估目标、场景和标准，指导测试用例的选择和创建。

+   •

    评估结果：来自先前评估的结果提供了历史数据（例如，来自生产环境的AgentOps日志），为新的回测测试用例提供依据[[48](https://arxiv.org/html/2411.13768v1#bib.bib48)]，使得能够对现实世界中曾遇到的输入进行评估。

+   •

    领域知识库：包括领域特定的资源，如行业文档、经过验证的用户论坛和认证的解决方案，提供真实且具有挑战性的示例，帮助构建测试用例。

处理步骤：此步骤通过四个协调活动来获取和细化测试数据：

1.  1.

    确定评估基准/框架：确定评估基准和框架（例如DeepEval和LangSmith），以支持代理评估。基准提供了通用的性能基线，有助于评估代理的基本能力，并进一步通过揭示性能差距来指导更具针对性、场景特定的测试用例。框架不仅限于基准测试，还提供了测试工具、测试用例生成、基于轨迹的评估和在线评估，支持全面的评估过程（步骤3）。步骤1中的治理要求和架构见解指导了选择过程，确保与评估目标和监管约束的一致性。选定的基准和框架是此活动的关键产出，供后续评估过程使用。

1.  2.

    收集领域知识：参考特定领域的知识库，创建测试用例，解决代理部署环境中特有的操作情境。这些来源通过反映代理可能遇到的领域特定挑战，增强了测试用例的真实性和相关性。

1.  3.

    与领域专家共同策划测试数据：领域专家帮助生成量身定制的测试用例，解决基准或知识库未完全覆盖的空白，增加了深度和特异性。例如，专家可能会将原始基准结果细化为可操作的场景，或为复杂的边缘条件创建针对性的测试用例。这确保了关键场景，特别是那些涉及复杂边缘情况的场景，得到全面的体现。

1.  4.

    使用LLM生成合成数据：在需要额外测试数据的情况下，可以使用LLM生成合成数据[[49](https://arxiv.org/html/2411.13768v1#bib.bib49), [50](https://arxiv.org/html/2411.13768v1#bib.bib50)]。这些合成数据通过模拟多样化和复杂的情境，填补了剩余的空白，扩大了评估的范围。尽管它不能替代现实世界的实例或专家策划的数据，但合成数据增加了广度，确保测试用例能够全面反映潜在的操作条件。

输出：

+   •

    测试用例：专门用于评估最终/中间流程和生成的产物，涵盖标准和边缘场景。测试用例包括参考性测试用例，设计时预定义了正确的结果（例如多项选择题），以及无参考测试用例，依赖人工或AI评估者来评估诸如相关性、连贯性和上下文适当性等标准。

+   •

    选定的基准和框架：已识别的基准和框架，支持后续评估。

### IV-C 步骤3：进行离线和在线评估

这一步实施了一种平衡的方法，逐步从受控的离线评估过渡到现实世界中的在线评估。离线评估验证了智能体在受控条件下是否符合基准标准，而在线评估则在不断变化的需求下提供现实世界环境中的持续性能和安全监控。

输入：评估过程的主要输入包括：

+   •

    确定的基准：在第2步中确定的基准提供了作为性能基准的通用评估，并且可以为更有针对性的测试用例生成提供指导。

+   •

    测试用例：在第2步中开发的这些测试用例构成了离线评估的基础，系统地评估智能体在受控数据集和特定场景下的表现。它们验证智能体在典型和高影响力的使用场景中提供正确、完整和上下文适当的输出的能力。测试用例还可能提供基准指标，或作为在线评估中异常检测的参考，补充实时反馈机制。

+   •

    评估框架：这些框架通过支持测试用例执行、工件分析和管道评估等任务来增强评估过程。它们使得在离线和在线环境中进行可扩展和全面的评估成为可能。

过程步骤：评估活动围绕以下内容展开：

1.  1.

    评估最终结果：这一步通过评估端到端结果来衡量智能体的整体表现，重点关注成功/通过率、准确性和用户满意度等标准。这里的目的是验证智能体的最终输出是否与预定目标和用户需求一致。然而，它通常缺乏诊断智能体决策过程中的具体问题或追溯失败根源所需的细节[[33](https://arxiv.org/html/2411.13768v1#bib.bib33), [51](https://arxiv.org/html/2411.13768v1#bib.bib51)]。单靠最终结果可能掩盖了中间的失误或推理错误，这些可能影响智能体的长期表现和安全性。

1.  2.

    评估中间管道和工件：此步骤专注于评估智能体的中间管道（例如，提示、 intermediate results）和执行工件（例如，计划[[45](https://arxiv.org/html/2411.13768v1#bib.bib45)]，检索的知识库信息[[44](https://arxiv.org/html/2411.13768v1#bib.bib44)]，以及工具输出[[52](https://arxiv.org/html/2411.13768v1#bib.bib52)]）。其目标是确保这些中间步骤在逻辑上的一致性、连贯性，并与智能体的目标对齐[[37](https://arxiv.org/html/2411.13768v1#bib.bib37)]。尽管这些细致的评估为诊断特定问题和指导改进提供了有价值的见解，但如果不将它们与智能体的整体表现联系起来，它们可能很难被有效地理解[[53](https://arxiv.org/html/2411.13768v1#bib.bib53)]。因此，需要一种平衡的方式将这些中间发现整合到对系统行为的全面理解中。

输出：此步骤生成来自离线和在线评估的全面评估结果，提供互补的见解。离线评估提供关键指标、错误分析以及受控测试场景中的通过/失败总结，建立了基线性能。在线评估则捕捉实际操作条件下的用户影响、适应性响应和行为模式等真实世界的度量。

解释性反馈对将评估结果转化为可操作的见解至关重要。一个结构化的反馈框架——识别细化的错误来源（例如，导致不满的具体问题）、反馈提供者（例如，最终用户、领域专家或AI）以及解释（例如，为什么一个输出是“可接受的但可以改进”）——确保反馈既有意义又系统化。这种方法有助于准确定位性能差距并有效地优先进行改进。这样的反馈补充了传统的度量标准，提供了细致的、符合上下文的评估，克服了死板或任意评分系统的局限性[[47](https://arxiv.org/html/2411.13768v1#bib.bib47)]。这些见解共同推动在线适应和迭代重开发，形成一个持续改进的循环，随着时间的推移，优化智能体的设计、性能和安全性。

### IV-D 步骤 4：分析与改进

此步骤将评估结果转化为可执行的改进措施，涉及在线（实时）和离线（重新开发）调整。在线改进涉及实时适应，以细化管道和工件，确保智能体能应对不断变化的上下文。重新开发则集中于对架构组件及大语言模型（LLM）本身的迭代性调整，以确保长期的性能和安全性。

输入：第3步的评估结果作为主要输入，包括离线（受控）和在线（现实世界）的见解。离线结果通常突出基准性能差距和系统性缺陷，而在线结果则揭示操作指标、用户反馈和新兴模式，指导即时和迭代改进。

过程步骤：此步骤包括运行时和重新开发时的改进活动：

1.  1.

    在运行时改进（在线）：通过利用在线评估结果，LLM代理动态调整运行时管道和工件，以提高响应能力并减少风险。例如，如果用户反馈或日志显示在多步任务中工具调用频繁失败，代理可以通过尝试替代工具或切换到缓存数据源来修正其计划。类似地，实时用户更正（如澄清模糊指令：“我指的是‘季度’而不是‘每月’”）会更新上下文记忆，确保随后的回应与更新后的输入一致。在目标驱动任务中，评估者的反馈可能触发目标的重新校准，优先考虑高影响的目标，以满足不断变化的用户需求。这些调整通常通过反馈循环和错误检测机制自动进行，使代理能够动态适应操作条件，而无需停机。

1.  2.

    在重新开发过程中改进（离线）：此阶段涉及根据通过离线评估识别的持续性问题和在线结果中的系统性见解进行大量部署后改进。主要活动包括：

    +   •

        精细化代理架构：通过增强推理和规划模块、保护机制等架构组件，解决设计层面的缺陷。例如，如果评估表明代理在处理敏感内容时偶尔会生成不恰当的回应，可以添加或改进保护机制，纳入更严格的内容过滤机制或特定领域的安全规则。此外，改进还可能涉及重新配置数据检索系统，以优先考虑更高质量的信息源，或优化组件间的工作流，提高效率和可靠性。

    +   •

        微调/重新训练/选择LLM：当评估结果表明LLM存在重大缺陷时，需要对模型进行改进。这可能涉及微调现有LLM以解决特定问题，或在可行的情况下重新训练，或者如果需要，选择替代的现成LLM。

输出：此步骤产生几个关键输出，整体提升LLM代理的设计和操作安全性：

+   •

    安全案例：更新后的安全案例结合了离线和在线评估的证据，确保代理在新的或变化的条件下保持安全。这些更新反映了新识别的风险和安全边界，确保在动态环境中符合安全阈值。

+   •

    精细化代理架构：系统组件的文档化修改反映了在重新开发过程中应用的迭代改进。这些改进使架构与已识别的性能差距和操作目标保持一致。

+   •

    更新的 LLM：输出可能包括经过微调、重新训练或新选择的 LLM，与性能和安全目标保持一致。

+   •

    精细化管道/工件：对运行时管道和工件进行调整，确保与运行时反馈和评估驱动的改进保持一致。

## V 评估驱动设计的 LLM 代理参考架构

为了回答 RQ2，我们提出了一个参考架构，将评估作为 LLM 代理设计中的核心元素（见图[3](https://arxiv.org/html/2411.13768v1#S5.F3 "图 3 ‣ V 评估驱动设计的 LLM 代理参考架构 ‣ 基于评估驱动的方法设计 LLM 代理：过程与架构")）。在此前关于 LLM 代理和 AI 系统设计的工作基础上[[7](https://arxiv.org/html/2411.13768v1#bib.bib7), [4](https://arxiv.org/html/2411.13768v1#bib.bib4)]，该架构将评估确立为三个相互关联层次中的基础性架构组成部分：供应链层、代理层和操作层。该架构确保持续且自适应的评估，反馈回路支持运行时适应和重新开发。操作层整合了评估组件和 AgentOps 基础设施，是该架构的核心。

![参见图注](img/517d82e164a4e42548f6848c7881f34d.png)

图 3：基于评估驱动设计的 LLM 代理参考架构

该架构遵循三个关键原则：

+   •

    生命周期集成：评估贯穿于部署前（设计和开发）和部署后（运行时和重新开发）阶段。这确保了评估是全面且持续的，并且能够在每个阶段提供见解，指导改进。

+   •

    有意义的反馈回路：来自离线和在线评估的见解会系统地反馈，用于告知轻量级的运行时调整和实质性的离线改进，从而实现有针对性的改进。

+   •

    持续学习与改进：该架构通过在模型和系统层面启用适应性调整，特别强调后者，促进了持续的改进。通过集成持续改进和模型外学习机制，确保代理能够有效适应不断变化的目标和操作环境，保持与性能和安全标准的一致性。

### V-A 供应链层

这一层建立了 LLM 代理的基础设计、功能和评估标准。它包括四个准备步骤，用虚线表示，表明它们是辅助性的，而非核心架构功能：

+   •

    计划与设计：理解用户目标，定义治理要求和高层架构，制定评估标准并识别评估场景。这一步为选择基准、创建测试用例以及使评估与合规性和性能标准对齐奠定基础。

+   •

    收集和处理数据：收集并预处理数据，以进行模型微调和模型/代理测试。在我们的背景下，重点是准备评估数据，因为我们优先使用现成的LLM。

+   •

    构建/选择与评估模型：使用已发布的系统卡（例如，OpenAI o1系统卡[[54](https://arxiv.org/html/2411.13768v1#bib.bib54)]）对专有模型进行评估，或通过API进行黑盒测试，以验证其是否适用于操作目标。开源模型可以根据需要进行微调并进行评估，以确保与特定任务和性能目标的一致性。

+   •

    构建与评估系统：将LLM与系统组件（例如保护措施）集成，并进行系统级评估，确保功能正确性、质量/风险和能力[[20](https://arxiv.org/html/2411.13768v1#bib.bib20)]。此类离线评估确认LLM代理在部署前的准备情况。

该层的主要输出，设计与开发文档库，整合了支持持续评估和改进的关键文档：

+   •

    测试用例：主要来源于以用户目标为驱动的场景的结构化评估，确保与特定任务和操作环境的一致性。测试用例是评估过程的支柱，提供针对管道和文档的有针对性的评估。来自离线基准测试的洞察不断完善测试用例，以应对特定场景的需求。测试用例会持续更新，以适应不断发展的目标和环境。

+   •

    安全案例：有证据支持的论证，定义了安全操作边界并识别关键风险场景。最初由用户和监管要求塑造，安全案例提供了基础的安全保障，并根据新出现的评估结果和运行时洞察不断进行迭代改进。

+   •

    离线评估结果：来自通用基准测试和特定场景测试用例评估的部署前结果，建立了基准性能指标并突出了已知的局限性。这些结果作为持续评估和后期部署阶段代理改进的重要参考。

### V-B 代理层

代理层关注LLM代理的适应能力，使其能够在动态的真实世界条件下有效运作。通过集成核心组件来理解上下文、决策和执行任务，该层支持与外部实体的无缝交互，并为基于评估的设计奠定基础。该层的主要组件包括：

外部环境：智能体与各种外部实体交互，这些实体塑造了智能体的操作上下文，提供必要的数据和情境见解，帮助智能体的响应和行动：

+   •

    用户：提供明确或隐含的反馈，指导智能体的行动，并为工作流程或文档的迭代改进提供依据。

+   •

    其他智能体：通过任务分配、数据共享或多智能体通信进行协作 [[55](https://arxiv.org/html/2411.13768v1#bib.bib55)]，实现协调的任务执行。

+   •

    工具：任务特定的API和处理模块，智能体使用这些工具来执行工作流程并完善其决策能力。

+   •

    知识库：静态或动态的信息来源（例如，数据库、本体论），通过提供情境性和领域特定的知识，增强推理过程。

智能体模块：该模块包括支持LLM智能体操作的关键功能组件：

+   •

    上下文引擎：该模块持续收集、处理和整合环境数据，将其与相关的记忆输入结合，以保持情境意识。此上下文使智能体能够生成准确且相关的响应，考虑到即时交互和适当时的过去用户输入。

+   •

    推理与规划模块：基于上下文引擎提供的情境化输入，该模块生成可操作的计划，指定执行任务的工作流程。此处生成的文档（例如，计划）是重要的评估目标。

+   •

    工作流执行：负责管理任务执行的组件，协调任务顺序、处理异常，并在必要时与外部环境（例如API、数据库）接口。工作流执行确保智能体遵循最佳的行动顺序，并具备检测和处理错误的机制。

+   •

    多层防护措施：防护措施在所有组件中执行操作边界，确保安全性和准确性/正确性。它们实时验证流程和文档，嵌入约束，确保符合负责任的AI原则和合规标准，同时防止不安全或次优的行动 [[8](https://arxiv.org/html/2411.13768v1#bib.bib8)]。

+   •

    记忆：记忆有助于信息的保持，使智能体能够随着时间的推移适应并与不断变化的用户需求保持一致。它存储历史数据、用户偏好和操作反馈，以改进流程、文档和连续性。记忆与上下文引擎协作，确保准确且具情境意识的响应。

### V-C 操作层

操作层是参考架构的核心，能够在运行时和重新开发过程中实现持续的评估和自适应改进。它由两个相互依赖的组件组成：评估和代理操作基础设施，共同构成了一个以评估为驱动的生命周期跨越的代理设计框架。评估驱动评估和反馈过程，而代理操作基础设施提供必要的可观察性和操作数据来支持这些过程。

#### V-C1 评估

这个组件是操作层的核心，协调跨生命周期的评估过程，以确保代理的性能和安全性。它整合了离线和在线评估的结果，形成一个持续反馈循环，推动运行时的适应和重新开发改进。

该组件的关键功能包括：

1.  1.

    综合评估过程：评估涵盖了离线和在线环境，关注代理性能的细节和系统层面。这些过程不仅评估最终结果（例如，整体成功率、用户满意度），还评估中间流程和工件（例如，计划、检索信息、工作流步骤），提供双重视角。这种全面的方法确保对代理行为的全面理解，从而实现精确的诊断和有针对性的改进。

1.  2.

    积极的反馈循环：评估结果被反馈到可执行的改进中。运行时评估为管道和工件提供即时适应，而离线评估则指导架构和组件的迭代重构。这些反馈循环确保评估不仅仅作为诊断工具，而是作为代理设计改进的积极驱动因素。

该组件的关键组成部分包括：

+   •

    评估者：人类评估者和自动化评估者共同参与评估过程。人类评估者（例如，领域专家或最终用户）为复杂的场景提供细致的评估，这些场景需要主观判断，如评估公平性、相关性或在高风险环境中的一致性。自动化评估者（基于大语言模型或代理的评估系统）则更快且成本效益高[[56](https://arxiv.org/html/2411.13768v1#bib.bib56)]，通过解决可扩展性和一致性问题，补充了人类评估。

+   •

    测试用例生成器：根据评估结果、操作日志和新兴模式，动态更新和完善测试用例。例如，运行时日志中发现的差异可能促使生成针对边缘情况或反复出现的错误的新测试用例，从而确保评估与不断变化的场景保持相关性。

+   •

    安全案例生成器：根据操作数据构建并迭代更新安全案例，确保它们与不断变化的风险和法规标准保持一致。通过动态集成新的评估洞察，为利益相关者——如开发人员、系统集成商和监管机构——提供最新的安全运营正当性依据。

+   •

    测试用例库：整合动态更新的测试用例，作为离线评估和实时监控的基础。通过对用例进行分类（例如标准场景、边缘案例和新兴模式），该库确保评估保持系统化并与实际情况相符。

+   •

    安全案例库：将安全案例作为正式的风险控制工具进行存储。这些案例会持续更新，以反映操作洞察，成为适应性风险管理的重要资源。

+   •

    评估结果库：作为汇总离线和在线评估结果的中央枢纽。该库不仅跟踪代理的性能和安全指标，还支持更广泛的架构决策。例如，存储的结果可以用于分析反复出现的故障模式或生成正面/负面示例，以便对运行时行为进行微调。

#### V-C2 AgentOps 基础设施与可观察性

AgentOps 基础设施支撑着持续的评估，通过捕获代理生命周期中的日志、指标、追踪和事件，实现动态性能诊断和事后分析[[9](https://arxiv.org/html/2411.13768v1#bib.bib9)]。通过提供实时洞察，该基础设施促进了即时的运行时调整，并生成结构化的工件以指导迭代改进。例如，来自高影响场景的运行时日志会转化为新的测试用例进行回测或安全案例更新，确保离线评估与现实世界的挑战保持一致。

可观察性还帮助检测新兴行为，并对反复出现的模式进行分类（例如正面/负面示例），丰富评估过程，完善流程和工件。与评估组件无缝集成，该基础设施确保操作数据直接为可操作的反馈环提供信息，从而提升代理在其生命周期中的质量和风险控制。

## VI 有效性威胁

方法论和设计并非没有局限性。以下是我们概述的潜在有效性威胁以及为减轻这些威胁所采取的步骤。

内部效度：数据选择和合成中的偏差可能会影响框架的准确性。为了解决这个问题，我们进行了严格的 MLR，并明确界定了纳入和排除标准。筛选过程辅以内部审查，确保一致性并减少主观偏差。此外，洞察通过从业人员讨论进行验证，为构建和评估 LLM 代理提供了实际视角。在合成过程中出现的分歧通过反复讨论解决，以保持一致性和严格性。

外部效度：尽管该方法在全面的 MLR 中扎根，合成了来自学术界和工业界的评估实践，但尚未通过大规模实证研究或跨领域应用进行验证。未来的工作将优先考虑真实世界案例研究和可扩展性评估，以评估其在不同操作环境中的适应性，并识别需要的领域特定改进。

构造效度：将多样化的评估实践汇聚成统一框架存在误表述或过度简化的风险。为减轻这种风险，我们明确记录了数据提取过程，确保透明性。

## VII 结论与未来工作

本文提出了一种面向评估的 LLM 代理设计方法，将持续评估贯穿整个生命周期，以指导运行时适应性和重开发。通过整合过程模型和参考架构，该方法结合了离线评估（建立受控基准）与在线评估（在动态条件下提供实时反馈）。这两种评估互为补充，提供了对运行时适应和迭代重开发的指导，确保代理始终与不断发展的性能目标和安全标准保持一致，同时主动管理风险。未来的工作将重点通过真实案例研究实证验证架构，并考察其在不同领域的可扩展性。

## 参考文献

+   [1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill *等人*, “关于基础模型的机遇与风险，” *arXiv 预印本 arXiv:2108.07258*, 2021。

+   [2] C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, 和 D. Ha, “AI 科学家：走向完全自动化的开放式科学发现，” *arXiv 预印本 arXiv:2408.06292*, 2024。

+   [3] L. Bass, Q. Lu, I. Weber, 和 L. Zhu, *工程化 AI 系统：架构与 DevOps 要素*。 Addison-Wesley, 2025。

+   [4] Q. Lu, L. Zhu, J. Whittle, X. Xu *等人*, *负责任的人工智能：创建可信赖的 AI 系统最佳实践*。 Addison-Wesley, 2023。

+   [5] J. Li 和 W. Lu, “多模态大语言模型基准测试调查，” *arXiv 预印本 arXiv:2408.08632*, 2024。

+   [6] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu, H. Ding, K. Men, K. Yang *等*，“Agentbench：评估 LLM 作为代理的表现，” 收录于 *第十二届国际学习表征会议*，2024年。

+   [7] Q. Lu, L. Zhu, X. Xu, Z. Xing, S. Harrer, 和 J. Whittle，“走向负责任的生成 AI：设计基于基础模型的代理的参考架构，” 收录于 *2024 IEEE 第21届国际软件架构会议（ICSA-C）*。 IEEE，2024年，页119–126。

+   [8] M. Shamsujjoha, Q. Lu, D. Zhao, 和 L. Zhu，“多层次运行时防护设计的分类：基于 AI 安全设计的瑞士奶酪模型，” 2024年。 [在线]. 可用：[https://arxiv.org/abs/2408.02205](https://arxiv.org/abs/2408.02205)

+   [9] L. Dong, Q. Lu, 和 L. Zhu，“AgentOps 分类：使基础模型代理可观察的能力，” 2024年。 [在线]. 可用：[https://arxiv.org/abs/2411.05285](https://arxiv.org/abs/2411.05285)

+   [10] AISI. (2024) AISI 的安全案例。[在线]. 可用：[https://www.aisi.gov.uk/work/safety-cases-at-aisi](https://www.aisi.gov.uk/work/safety-cases-at-aisi)

+   [11] OpenAI，“优化 LLM 准确性。” [在线]. 可用：[https://platform.openai.com/docs/guides/optimizing-llm-accuracy](https://platform.openai.com/docs/guides/optimizing-llm-accuracy)

+   [12] Y. Wang, T. Jiang, M. Liu, J. Chen, 和 Z. Zheng，“超越功能正确性：探索大型语言模型中的编码风格不一致性，” *arXiv 预印本 arXiv:2407.00456*，2024年。

+   [13] X. Tang, Q. Jin, K. Zhu, T. Yuan, Y. Zhang, W. Zhou, M. Qu, Y. Zhao, J. Tang, Z. Zhang *等*，“优先保护安全而非自主性：LLM 代理在科学中的风险，” *arXiv 预印本 arXiv:2402.04247*，2024年。

+   [14] M. Phuong, M. Aitchison, E. Catt, S. Cogan, A. Kaskasoli, V. Krakovna, D. Lindner, M. Rahtz, Y. Assael, S. Hodkinson *等*， “评估前沿模型的危险能力，” *arXiv 预印本 arXiv:2403.13793*，2024年。

+   [15] C.-K. Wu, Z. R. Tam, C.-Y. Lin, Y.-N. Chen, 和 H.-y. Lee，“Streambench：走向评估语言代理持续改进的基准，” *arXiv 预印本 arXiv:2406.08747*，2024年。

+   [16] S. Ouyang, J. M. Zhang, M. Harman, 和 M. Wang，“LLM 就像一盒巧克力：ChatGPT 在代码生成中的非确定性，” *arXiv 预印本 arXiv:2308.02828*，2023年。

+   [17] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler *等*， “大型语言模型的 emergent 能力，” *arXiv 预印本 arXiv:2206.07682*，2022年。

+   [18] K. Beck, *测试驱动开发：举例说明*。 Addison-Wesley Professional，2022年。

+   [19] Wikipedia 贡献者，“行为驱动开发 — 维基百科，自由百科全书，” [https://en.wikipedia.org/w/index.php?title=Behavior-driven_development&oldid=1250279482](https://en.wikipedia.org/w/index.php?title=Behavior-driven_development&oldid=1250279482)，2024年。

+   [20] B. 夏, Q. 陆, L. 朱, 和 Z. 邢, “推进人工智能安全的人工智能系统评估框架：术语、分类法、生命周期映射，” 见于 *第1届ACM国际人工智能驱动软件会议论文集*，2024年，第74–78页。

+   [21] M. 陈, J. Tworek, H. Jun, Q. 袁, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman *等人*, “评估基于代码训练的大型语言模型，” *arXiv 预印本 arXiv:2107.03374*, 2021年。

+   [22] T. Y. 卓, M. C. Vu, J. Chim, H. 胡, W. 于, R. Widyasari, I. N. B. Yusuf, H. 詹, J. 何, I. 保罗 *等人*, “Bigcodebench: 通过多样化的函数调用和复杂指令进行代码生成基准测试，” *arXiv 预印本 arXiv:2406.15877*, 2024年。

+   [23] X. 杜, M. 刘, K. 王, H. 王, J. 刘, Y. 陈, J. 冯, C. 沙, X. 彭, 和 Y. Lou, “Classeval: 一个手工制作的基准测试，用于评估大型语言模型在类级代码生成中的表现，” *arXiv 预印本 arXiv:2308.01861*, 2023年。

+   [24] J. 陈, H. 林, X. 韩, 和 L. 孙, “检验检索增强生成的大型语言模型,” 见于 *人工智能学会会议论文集*，第38卷，第16期，2024年，第17,754–17,762页。

+   [25] X. 杨, K. 孙, H. 辛, Y. 孙, N. Bhalla, X. 陈, S. Choudhary, R. D. Gui, Z. W. 江, Z. 江 *等人*, “Crag–综合RAG基准测试，” *arXiv 预印本 arXiv:2406.04744*, 2024年。

+   [26] J. 刘, P. 周, Y. 华, D. 崇, Z. 田, A. 刘, H. 王, C. 尤, Z. 郭, L. 朱 *等人*, “在cmexam——一个全面的中文医学考试数据集上对大型语言模型进行基准测试，” *神经信息处理系统进展*，第36卷，2024年。

+   [27] Y. 蔡, L. 王, Y. 王, G. de Melo, Y. 张, Y. 王, 和 L. 何, “Medbench: 一个用于评估医学大型语言模型的大规模中文基准测试，” 见于 *人工智能学会会议论文集*，第38卷，第16期，2024年，第17,709–17,717页。

+   [28] Z. 费, X. 沈, D. 朱, F. 周, Z. 韩, S. 张, K. 陈, Z. 沈, 和 J. 戈, “Lawbench: 基准测试大型语言模型的法律知识，” *arXiv 预印本 arXiv:2309.16289*, 2023年。

+   [29] N. 古哈, J. Nyarko, D. Ho, C. Ré, A. Chilton, A. Chohlas-Wood, A. Peters, B. Waldon, D. Rockmore, D. Zambrano *等人*, “Legalbench: 一个合作构建的基准测试，用于衡量大型语言模型的法律推理能力，” *神经信息处理系统进展*，第36卷，2024年。

+   [30] Q. 谢, W. 韩, X. 张, Y. 赖, M. 彭, A. Lopez-Lira, 和 J. 黄, “Pixiu: 一个面向金融领域的大型语言模型、指令数据和评估基准，” *arXiv 预印本 arXiv:2306.05443*, 2023年。

+   [31] P. 伊斯兰, A. Kannappan, D. Kiela, R. 钱, N. Scherrer, 和 B. Vidgen, “Financebench: 一个用于金融问答的新基准测试，” *arXiv 预印本 arXiv:2311.11944*, 2023年。

+   [32] X. 刘, H. 于, H. 张, Y. 许, X. 雷, H. 赖, Y. 顾, H. 丁, K. 门, K. 杨 *等人*, “Agentbench: 评估大型语言模型作为代理的能力，” *arXiv 预印本 arXiv:2308.03688*, 2023年。

+   [33] L. Gioacchini, G. Siracusano, D. Sanvito, K. Gashteovski, D. Friede, R. Bifulco 和 C. Lawrence, “Agentquest：一个模块化基准框架，用于衡量进展并改进大语言模型代理，” *arXiv 预印本 arXiv:2404.06411*，2024年。

+   [34] O. Sainz, J. A. Campos, I. García-Ferrero, J. Etxaniz, O. L. de Lacalle 和 E. Agirre, “自然语言处理评估困难：关于每个基准需要测量大语言模型数据污染的问题，” *arXiv 预印本 arXiv:2310.18018*，2023年。

+   [35] C. White, S. Dooley, M. Roberts, A. Pal, B. Feuer, S. Jain, R. Shwartz-Ziv, N. Jain, K. Saifullah, S. Naidu *等*，“Livebench：一个具有挑战性、无污染的大语言模型基准，” *arXiv 预印本 arXiv:2406.19314*，2024年。

+   [36] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen 和 I. Stoica, “Livecodebench：对代码的大语言模型进行全方位且无污染的评估，” *arXiv 预印本 arXiv:2403.07974*，2024年。

+   [37] W. Xiong, Y. Song, X. Zhao, W. Wu, X. Wang, K. Wang, C. Li, W. Peng 和 S. Li, “每一步都要小心！通过迭代步骤级过程优化进行大语言模型代理学习，” *arXiv 预印本 arXiv:2406.11176*，2024年。

+   [38] M. D. Buhl, G. Sett, L. Koessler, J. Schuett 和 M. Anderljung, “前沿人工智能的安全案例，” *arXiv 预印本 arXiv:2410.21572*，2024年。

+   [39] V. Garousi, M. Felderer 和 M. V. Mäntylä, “在软件工程中纳入灰色文献并进行多声音文学回顾的指导原则，” *信息与软件技术*，第106卷，第101–121页，2019年2月。[在线]。可用： [https://linkinghub.elsevier.com/retrieve/pii/S0950584918301939](https://linkinghub.elsevier.com/retrieve/pii/S0950584918301939)

+   [40] B. Kitchenham, O. P. Brereton, D. Budgen, M. Turner, J. Bailey 和 S. Linkman, “软件工程中的系统文献回顾——一项系统文献回顾，” *信息与软件技术*，第51卷，第1期，第7–15页，2009年。

+   [41] C. Wohlin, “系统文献研究中的滚雪球法指导原则及其在软件工程中的复制应用，” 见 *第18届国际软件工程评估与评测会议论文集*，2014年，第1–10页。

+   [42] M. Galster 和 P. Avgeriou, “基于经验的参考架构：一个提议，” 见 *联合 ACM SIGSOFT 会议——QoSA 与 ACM SIGSOFT 研讨会——ISARCS，软件架构质量与关键系统架构设计*，2011年，第153–158页。

+   [43] P. Guldimann, A. Spiridonov, R. Staab, N. Jovanović, M. Vero, V. Vechev, A. Gueorguieva, M. Balunović, N. Konstantinov, P. Bielik *等*，“Compl-ai 框架：欧盟人工智能法案的技术解读与大语言模型基准套件，” *arXiv 预印本 arXiv:2410.07959*，2024年。

+   [44] D. Ru, L. Qiu, X. Hu, T. Zhang, P. Shi, S. Chang, J. Cheng, C. Wang, S. Sun, H. Li *等*，“Ragchecker：一个用于诊断检索增强生成的细粒度框架，” *arXiv 预印本 arXiv:2408.08067*，2024年。

+   [45] K. Valmeekam, M. Marquez, A. Olmo, S. Sreedharan, 和 S. Kambhampati, “Planbench：评估大型语言模型在规划和推理变化中的扩展基准，” *神经信息处理系统进展*，第36卷，2024年。

+   [46] S. Qiao, R. Fang, Z. Qiu, X. Wang, N. Zhang, Y. Jiang, P. Xie, F. Huang, 和 H. Chen, “基准测试智能工作流生成，” *arXiv 预印本 arXiv:2410.07869*，2024年。

+   [47] H. Husain. (2024) 创建一个作为法官的LLM，推动商业成果。[在线]. 可用链接: [https://hamel.dev/blog/posts/llm-judge/#recap-of-critique-shadowing](https://hamel.dev/blog/posts/llm-judge/#recap-of-critique-shadowing)

+   [48] LangSmith, “评估 - LangSmith 文档，” 2024年。[在线]. 可用链接: [https://docs.smith.langchain.com/concepts/evaluation](https://docs.smith.langchain.com/concepts/evaluation)

+   [49] L. Long, R. Wang, R. Xiao, J. Zhao, X. Ding, G. Chen, 和 H. Wang, “关于LLM驱动的合成数据生成、整理与评估：一项调查，” *arXiv 预印本 arXiv:2406.15126*，2024年。

+   [50] D. S. at Microsoft Jane Huang, “评估大型语言模型(LLM)系统：指标、挑战与最佳实践，” 2024年。[在线]. 可用链接: [https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5)

+   [51] C. Ma, J. Zhang, Z. Zhu, C. Yang, Y. Yang, Y. Jin, Z. Lan, L. Kong, 和 J. He, “Agentboard：多轮LLM智能体的分析评估板，” *arXiv 预印本 arXiv:2401.13178*，2024年。

+   [52] C. Qu, S. Dai, X. Wei, H. Cai, S. Wang, D. Yin, J. Xu, 和 J.-R. Wen, “使用大型语言模型进行工具学习：一项调查，” *arXiv 预印本 arXiv:2405.17935*，2024年。

+   [53] J. Benton, M. Wagner, E. Christiansen, C. Anil, E. Perez, J. Srivastav, E. Durmus, D. Ganguli, S. Kravec, B. Shlegeris *等*， “前沿模型的破坏评估。”

+   [54] OpenAI, “OpenAI O1 系统卡，” 2024年。[在线]. 可用链接: [https://cdn.openai.com/o1-system-card-20240917.pdf](https://cdn.openai.com/o1-system-card-20240917.pdf)

+   [55] W. Chen, Y. Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan, Y. Qin, Y. Lu, R. Xie *等*， “Agentverse：促进多智能体协作并探索智能体的涌现行为，” *arXiv 预印本 arXiv:2308.10848*，第2卷，第4期，第5页，2023年。

+   [56] M. Zhuge, C. Zhao, D. Ashley, W. Wang, D. Khizbullin, Y. Xiong, Z. Liu, E. Chang, R. Krishnamoorthi, Y. Tian *等*， “作为法官的智能体：用智能体评估智能体，” *arXiv 预印本 arXiv:2410.10934*，2024年。
