- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:41:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LawLuo: A Chinese Law Firm Co-run by LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LawLuo：一个由LLM代理共同运营的中国律师事务所
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16252](https://ar5iv.labs.arxiv.org/html/2407.16252)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16252](https://ar5iv.labs.arxiv.org/html/2407.16252)
- en: Jingyun Sun¹, Chengxiao Dai², Zhongze Luo¹, Yangbo Chang³, Yang Li^(1,∗)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jingyun Sun¹, Chengxiao Dai², Zhongze Luo¹, Yangbo Chang³, Yang Li^(1,∗)
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) demonstrate substantial potential in delivering
    legal consultation services to users without a legal background, attributed to
    their superior text comprehension and generation capabilities. Nonetheless, existing
    Chinese legal LLMs limit interaction to a single model-user dialogue, unlike the
    collaborative consultations typical of law firms, where multiple staff members
    contribute to a single consultation. This limitation prevents an authentic consultation
    experience. Additionally, extant Chinese legal LLMs suffer from critical limitations:
    (1) insufficient control over the quality of instruction fine-tuning data; (2)
    increased model hallucination resulting from users’ ambiguous queries; and (3)
    a reduction in the model’s ability to follow instructions over multiple dialogue
    turns. In response to these challenges, we propose a novel legal dialogue framework
    that leverages the collaborative capabilities of multiple LLM agents, termed LawLuo.
    This framework encompasses four agents: a receptionist, a lawyer, a secretary,
    and a boss, each responsible for different functionalities, collaboratively providing
    a comprehensive legal consultation to users. Additionally, we constructed two
    high-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned ChatGLM-3-6b
    using these datasets. We propose a legal query clarification algorithm called
    ToLC. Experimental results demonstrate that LawLuo outperforms baseline LLMs,
    including GPT-4, across three dimensions: lawyer-like language style, the usefulness
    of legal advice, and the accuracy of legal knowledge. Our code and datasets are
    available at [https://github.com/NEFUJing/LawLuo](https://github.com/NEFUJing/LawLuo)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在为没有法律背景的用户提供法律咨询服务方面展示了巨大的潜力，这得益于其优越的文本理解和生成能力。然而，现有的中文法律LLM将互动限制在单一模型与用户对话中，这与律师事务所的协作咨询模式不同，后者通常由多名工作人员共同参与单次咨询。这种限制妨碍了真实的咨询体验。此外，现有的中文法律LLM存在几个关键问题：（1）对指令微调数据质量的控制不足；（2）用户模糊查询导致的模型幻觉增加；（3）模型在多个对话回合中的指令遵循能力下降。针对这些挑战，我们提出了一个新的法律对话框架，称为LawLuo。该框架包括四个代理：接待员、律师、秘书和老板，每个代理负责不同的功能，协作提供全面的法律咨询服务。此外，我们构建了两个高质量的法律对话数据集，KINLED和MURLED，并使用这些数据集对ChatGLM-3-6b进行了微调。我们提出了一种名为ToLC的法律查询澄清算法。实验结果表明，LawLuo在律师般的语言风格、法律建议的实用性和法律知识的准确性这三方面超越了基线LLM，包括GPT-4。我们的代码和数据集可在[https://github.com/NEFUJing/LawLuo](https://github.com/NEFUJing/LawLuo)找到。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Since the release of ChatGPT, the development of Chinese Large Language Models
    (LLMs) has advanced rapidly, resulting in the emergence of several influential
    base models, including ChatGLM (Du et al. [2022](#bib.bib2)), LLaMa (Touvron et al.
    [2023a](#bib.bib12)), and BaiChuan (Yang et al. [2023](#bib.bib24)). These models
    excel in fluent Chinese dialogue and comprehension of complex contexts and user
    intentions. Additionally, domain-specific Chinese LLMs such as Medical LLMs (Yang
    et al. [2024](#bib.bib25); Zhang et al. [2023a](#bib.bib27)), Legal LLMs (Zhou
    et al. [2024](#bib.bib31); Huang et al. [2023](#bib.bib5)), and Financial LLMs
    (Zhang and Yang [2023](#bib.bib29)) have emerged, showcasing exceptional domain-specific
    conversational abilities and meeting diverse user needs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自从ChatGPT发布以来，中国的大型语言模型（LLM）的发展迅速，出现了多个有影响力的基础模型，包括ChatGLM（Du等人 [2022](#bib.bib2)）、LLaMa（Touvron等人
    [2023a](#bib.bib12)）和BaiChuan（Yang等人 [2023](#bib.bib24)）。这些模型在流利的中文对话以及复杂背景和用户意图的理解方面表现优异。此外，领域特定的中文LLM如医疗LLM（Yang等人
    [2024](#bib.bib25)；Zhang等人 [2023a](#bib.bib27)）、法律LLM（Zhou等人 [2024](#bib.bib31)；Huang等人
    [2023](#bib.bib5)）和金融LLM（Zhang和Yang [2023](#bib.bib29)）也相继出现，展示了卓越的领域特定对话能力，满足了多样化的用户需求。
- en: 'Chinese Legal LLMs offer users without a legal background timely, accurate,
    and clear solutions to legal issues, effectively addressing the shortage of legal
    resources. Recently, notable Chinese Legal LLMs such as LawGPT (Zhou et al. [2024](#bib.bib31))
    and lawyer-llama (Huang et al. [2023](#bib.bib5)) have emerged. They leverage
    extensive Chinese legal dialogue datasets to fine-tune Chinese base large models,
    resulting in LLMs equipped with comprehensive Chinese legal knowledge and the
    capability to engage in legal consultation dialogues. Nevertheless, a standalone
    legal LLM fall short in simulate the Standard Operating Procedure (SOP) of real-world
    legal consultations, thus failing to deliver an authentic consulting experience
    to users. Additionally, existing LLMs encounter several issues: (1) Users consulting
    these models often lack a legal background, leading to vague and imprecise queries
    that exacerbate models’ tendency to produce hallucinations; (2) Despite utilizing
    large datasets for instruction fine-tuning, current legal LLMs do not sufficiently
    control for data quality; (3) Existing models employ single-turn legal dialogue
    data for fine-tuning, neglecting multi-turn dialogue data, which undermines models’
    ability to follow instructions after multi-turn conversation.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 中文法律LLM为没有法律背景的用户提供及时、准确、清晰的法律问题解决方案，有效解决了法律资源短缺的问题。最近，诸如LawGPT（Zhou et al.
    [2024](#bib.bib31)）和lawyer-llama（Huang et al. [2023](#bib.bib5)）等著名中文法律LLM相继出现。它们利用大量中文法律对话数据集来微调中文基础大模型，形成了具备全面中文法律知识并能够进行法律咨询对话的LLM。然而，单独的法律LLM在模拟现实法律咨询的标准操作程序（SOP）方面仍显不足，无法为用户提供真实的咨询体验。此外，现有LLM面临几个问题：（1）咨询这些模型的用户通常缺乏法律背景，导致模糊和不准确的查询，加剧了模型产生幻觉的倾向；（2）尽管利用大量数据集进行指令微调，但当前法律LLM未能充分控制数据质量；（3）现有模型使用单轮法律对话数据进行微调，忽视了多轮对话数据，削弱了模型在多轮对话后遵循指令的能力。
- en: '![Refer to caption](img/5907f0309c262a20fc1b514c0c3a4a8b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5907f0309c262a20fc1b514c0c3a4a8b.png)'
- en: 'Figure 1: To simulate a realistic legal consultation Standard Operating Procedure
    (SOP), we introduce the LawLuo framework, which encompasses four distinct agents:
    a receptionist, lawyers, the boss, and a secretary. Each of these agents is driven
    by language models: the receptionist by RoBERTa, the lawyers by our fine-tuned
    ChatGLM, the secretary by the base ChatGLM, and the boss by a reward model. These
    agents work in concert to provide a comprehensive and high-quality legal consultation
    experience.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：为了模拟现实法律咨询的标准操作程序（SOP），我们引入了LawLuo框架，该框架包含四个不同的角色：接待员、律师、老板和秘书。这些角色由语言模型驱动：接待员由RoBERTa驱动，律师由我们微调后的ChatGLM驱动，秘书由基础ChatGLM驱动，老板由奖励模型驱动。这些角色协同工作，提供全面而高质量的法律咨询体验。
- en: To address the aforementioned issues, we propose a novel legal dialogue framework,
    termed LawLuo, grounded in LLM multi-agents. Initially, we fine-tune the Chinese
    base LLM, ChatGLM-3-6b, leveraging our meticulously constructed high-quality legal
    dialogue dataset, KINLED, alongside the multi-turn legal consultation dataset,
    MURLED. Contrary to other models of employing extensive datasets, our findings
    indicate that fine-tuning with a smaller, high-quality legal dialogue dataset
    yields superior performance. Subsequently, we design multiple agents—receptionist,
    lawyer, secretary, and boss—based on the SOP of actual law firms. We design a
    collaborative framework wherein these agents synergistically handle a user’s legal
    consultation, thereby enhancing the user consultation experience. We further specialize
    the roles of lawyer agents via role enhancement (Wang et al. [2023b](#bib.bib17);
    Shao et al. [2023](#bib.bib9)), according to different consultation domains. For
    instance, a corporate lawyer is designated to engage with users seeking advice
    on corporate law, whereas a traffic accident lawyer is dedicated to consultations
    concerning traffic accidents. Moreover, to address users’ imprecise and ambiguous
    queries, we draw inspiration from the Tree of Clarification (ToC) (Kim et al.
    [2023](#bib.bib6)) and propose a novel Tree of Legal Clarification (ToLC) algorithm.
    ToLC employs a retrieve-generate-active choosing process to guide users in refining
    their legal queries.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们提出了一种新颖的法律对话框架，称为LawLuo，该框架基于LLM多代理。最初，我们利用精心构建的高质量法律对话数据集KINLED，以及多轮法律咨询数据集MURLED，对中文基础LLM
    ChatGLM-3-6b进行微调。与其他使用大规模数据集的模型不同，我们的研究结果表明，使用较小的高质量法律对话数据集进行微调能获得更好的性能。随后，我们根据实际律师事务所的标准操作程序设计了多个代理——接待员、律师、秘书和老板。我们设计了一个协作框架，使这些代理协同处理用户的法律咨询，从而提升用户的咨询体验。我们进一步通过角色增强（Wang
    et al. [2023b](#bib.bib17); Shao et al. [2023](#bib.bib9)）专业化律师代理的角色，根据不同的咨询领域。例如，企业律师专注于为寻求企业法建议的用户提供服务，而交通事故律师则专门处理有关交通事故的咨询。此外，为了解决用户的不准确和模糊查询，我们受到澄清树（ToC）（Kim
    et al. [2023](#bib.bib6)）的启发，提出了一种新颖的法律澄清树（ToLC）算法。ToLC采用检索-生成-主动选择过程，指导用户细化他们的法律查询。
- en: 'Experiment results demonstrate that LawLuo significantly outperforms the baselines
    across three key dimensions: lawyer-like language style, the utility of legal
    advice, and the accuracy of legal knowledge. Specifically, LawLuo exhibits a winning
    rate of 72% against ChatGLM-3-6b, the base LLM we utilized, which underscores
    the efficacy of our instruction fine-tuning. Remarkably, LawLuo achieves superior
    performance compared to other legal LLMs while utilizing fewer instruction fine-tuning
    data, emphasizing the critical role of data quality over quantity. Additionally,
    our experimental results indicate that LawLuo consistently generates high-quality
    responses even after multiple dialogue turns, highlighting the importance of using
    real multi-turn legal consultation data for instruction fine-tuning. Lastly, we
    confirmed the contribution of each component through a comprehensive ablation
    study.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，LawLuo在三个关键维度上显著优于基线：类似律师的语言风格、法律建议的实用性以及法律知识的准确性。具体而言，LawLuo的胜率为72%，相比于我们使用的基础LLM
    ChatGLM-3-6b，这突显了我们指令微调的有效性。值得注意的是，LawLuo在使用较少的指令微调数据的情况下，比其他法律LLM表现更好，这强调了数据质量而非数量的关键作用。此外，我们的实验结果表明，即使在多轮对话后，LawLuo也能持续生成高质量的响应，突显了使用真实多轮法律咨询数据进行指令微调的重要性。最后，我们通过全面的消融研究确认了每个组件的贡献。
- en: 'Overall, our primary contributions are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的主要贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a legal dialogue framework, LawLuo, based on the collaboration of
    multiple LLM agents.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个基于多个LLM代理合作的法律对话框架LawLuo。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We construct a high-quality dataset, KINLED, for legal LLM instruction fine-tuning.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们构建了一个高质量的数据集KINLED，用于法律LLM指令微调。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop a real-world multi-turn legal consultation dataset, MURLED.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个实际的多轮法律咨询数据集MURLED。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a algorithm for clarifying legal queries, ToLD.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种用于澄清法律查询的算法ToLD。
- en: 2\. Related work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 'Legal Question Answering. Legal Question Answering (LQA) is considered one
    of the most challenging tasks in legal artificial intelligence due to its flexible
    input-output requirements. In this task, users ask legal questions, and systems
    provide detailed answers that meet their expectations. Early LQA systems fall
    into three main categories: retrieval-based LQA systems (Yoshioka, Aoki, and Suzuki
    [2021](#bib.bib26)), knowledge-based LQA systems (Taniguchi and Kano [2017](#bib.bib10)),
    and machine reading comprehension-based LQA systems (Xiao et al. [2021](#bib.bib22)).
    Significant advancements in Chinese LQA have been driven by datasets like the
    Chinese Judicial Reading Comprehension dataset (CJRC) (Duan et al. [2019](#bib.bib3))
    and the Judicial Examination Chinese QA dataset (JEC-QA) (Zhong et al. [2020](#bib.bib30)).
    However, early methods struggle to personalize answers. Additionally, many legal
    consultations require multi-turn dialogues rather than a single question and answer
    exchange.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 法律问题回答。法律问题回答（LQA）被认为是法律人工智能领域中最具挑战性的任务之一，因为它具有灵活的输入输出要求。在这个任务中，用户提出法律问题，系统提供符合其期望的详细答案。早期的LQA系统主要分为三类：基于检索的LQA系统（Yoshioka,
    Aoki, 和 Suzuki [2021](#bib.bib26)）、基于知识的LQA系统（Taniguchi 和 Kano [2017](#bib.bib10)），以及基于机器阅读理解的LQA系统（Xiao
    等 [2021](#bib.bib22)）。中文LQA的显著进展得益于数据集如中国司法阅读理解数据集（CJRC）（Duan 等 [2019](#bib.bib3)）和司法考试中文QA数据集（JEC-QA）（Zhong
    等 [2020](#bib.bib30)）。然而，早期方法在个性化回答方面存在困难。此外，许多法律咨询需要多轮对话，而非单一的问答交换。
- en: In recent years, applying LLM to LQA has shown great promise. Compared to traditional
    LQA models, LLMs generate personalized responses based on user queries and support
    multi-turn dialogues for more precise legal inquiry resolution. LawGPT (Zhou et al.
    [2024](#bib.bib31)) continuing pre-trains and instruction fine-tunes Chinese-LLaMa-7B
    (Touvron et al. [2023a](#bib.bib12)), a Chinese base model, with publicly available
    legal documents and judicial examination data, thereby enhancing its understanding
    and execution of legal content. LexiLaw¹¹1[https://github.com/CSHaitao/LexiLaw](https://github.com/CSHaitao/LexiLaw)
    fine-tunes ChatGLM-6B with data from Huazhi.com²²2[https://www.66law.cn/](https://www.66law.cn/),
    judicial examination data, and Q&A data with legal references, using Freeze, Lora,
    and P-Tuning-V2 techniques to boost training efficiency. LawGPT_zh fine-tunes
    ChatGLM-6B using existing legal datasets and scenario-based Q&A data with statutory
    references, while lawyer-llama (Huang et al. [2023](#bib.bib5)) uses judicial
    examination data to fine-tune Chinese-LLaMA-13, enhancing its application of legal
    knowledge to dialogue scenarios. However, these models only use a single LLM for
    legal consultations, unlike real-world law firms where multiple staff collaborate
    on a user’s consultation. This limitation fails to provide users with the most
    realistic and expected LQA experience. Additionally, while current legal LLMs
    use numerous fine-tuning datasets, their quality varies significantly, contradicting
    recent findings that in large model instruction fine-tuning, “less is more” (i.e.,
    data quality is more important than quantity) (Wei et al. [2023b](#bib.bib19);
    Xia et al. [2024](#bib.bib21)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，将大型语言模型（LLM）应用于LQA显示出巨大的潜力。与传统的LQA模型相比，LLM根据用户查询生成个性化的响应，并支持多轮对话，以更精确地解决法律问题。LawGPT（Zhou
    等 [2024](#bib.bib31)）继续对中文基础模型Chinese-LLaMa-7B（Touvron 等 [2023a](#bib.bib12)）进行预训练和指令微调，使用公开的法律文档和司法考试数据，从而提升其对法律内容的理解和执行。LexiLaw¹¹1[https://github.com/CSHaitao/LexiLaw](https://github.com/CSHaitao/LexiLaw)对ChatGLM-6B进行微调，使用来自Huazhi.com²²2[https://www.66law.cn/](https://www.66law.cn/)的数据、司法考试数据和带有法律参考的问答数据，采用Freeze、Lora和P-Tuning-V2技术提高训练效率。LawGPT_zh利用现有法律数据集和带有法定参考的场景问答数据对ChatGLM-6B进行微调，而律师-llama（Huang
    等 [2023](#bib.bib5)）则使用司法考试数据对Chinese-LLaMA-13进行微调，提升其在对话场景中应用法律知识的能力。然而，这些模型仅使用单一的LLM进行法律咨询，不像现实世界的律师事务所那样，多名员工协作处理用户咨询。这一限制未能为用户提供最真实和最符合期望的LQA体验。此外，尽管当前的法律LLM使用了大量的微调数据集，但其质量差异显著，这与最近的研究发现相矛盾，即在大模型的指令微调中，“少即是多”（即数据质量比数量更重要）（Wei
    等 [2023b](#bib.bib19); Xia 等 [2024](#bib.bib21)）。
- en: Multi-Agent Collaboration. In LLM-based multi-agent systems, an agent is defined
    as an autonomous entity capable of perceiving, thinking, learning, making decisions,
    and interacting with other agents (Xi et al. [2023](#bib.bib20)). Research shows
    that breaking complex tasks into simpler subtasks and tackling these with agents
    that have diverse functions can significantly enhance the problem-solving capabilities
    of LLMs. (Wang et al. [2024](#bib.bib14)). Fundamentally, Retrieval-Augmented
    Generation (RAG) based on LLMs constitutes a straightforward multi-agent system
    wherein a retrieval agent is tasked with sourcing relevant knowledge from a vector
    database, while an LLM agent is responsible for generating responses based on
    the query and associated knowledge (Louis, van Dijck, and Spanakis [2024](#bib.bib7)).
    Furthermore, some works have developed more complex multi-agent collaborative
    systems to address intricate challenges. For instance, (Qian et al. [2023](#bib.bib8))
    designed a multi-agent collaborative workflow in which agents assuming roles such
    as CTO, programmer, designer, and tester work closely together to complete software
    development and document the development process. (Hemmer et al. [2022](#bib.bib4))
    have facilitated the construction of machine learning models through collaboration
    between multiple agents and humans. Additionally, LLM-based multi-agent systems
    can also be employed for realistic environment simulation. For example, (Wang
    et al. [2023a](#bib.bib15)) utilized multiple generative LLM agents in a sandbox
    environment to simulate consumer behavior in merchandise recommendation scenarios,
    (Wei et al. [2023a](#bib.bib18)) assigned various roles to agents for the collection
    and evaluation of multi-party dialogue data, and (Du et al. [2023](#bib.bib1))
    leveraged debates among agents with different personalities to enhance the factual
    accuracy of LLM reasoning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理协作。在基于 LLM 的多代理系统中，代理被定义为一个能够感知、思考、学习、做出决策并与其他代理互动的自主实体（Xi 等 [2023](#bib.bib20)）。研究表明，将复杂任务分解成更简单的子任务，并利用具有不同功能的代理来解决这些子任务，可以显著提高
    LLM 的问题解决能力（Wang 等 [2024](#bib.bib14)）。从根本上讲，基于 LLM 的检索增强生成（RAG）构成了一个简单的多代理系统，其中一个检索代理负责从向量数据库中获取相关知识，而一个
    LLM 代理则根据查询和相关知识生成响应（Louis、van Dijck 和 Spanakis [2024](#bib.bib7)）。此外，一些研究开发了更复杂的多代理协作系统来应对复杂挑战。例如，（Qian
    等 [2023](#bib.bib8)）设计了一个多代理协作工作流程，其中代理扮演 CTO、程序员、设计师和测试人员等角色，紧密合作完成软件开发并记录开发过程。（Hemmer
    等 [2022](#bib.bib4)）通过多个代理与人类之间的协作促进了机器学习模型的构建。此外，基于 LLM 的多代理系统还可以用于真实环境的模拟。例如，（Wang
    等 [2023a](#bib.bib15)）在沙盒环境中利用多个生成型 LLM 代理模拟商品推荐场景中的消费者行为，（Wei 等 [2023a](#bib.bib18)）为代理分配了不同角色以收集和评估多方对话数据，（Du
    等 [2023](#bib.bib1)）通过具有不同个性的代理之间的辩论来提高 LLM 推理的事实准确性。
- en: 3\. LawLuo Framework for LQA
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. LawLuo 框架用于法律问答
- en: In real-world scenarios, a consultation in law firms is conducted through the
    collaboration of multiple staff members, whereas existing legal LLMs generally
    engage with a user in isolation. To bridge this gap, we introduce a multi-agent
    collaborative legal dialogue framework, named LawLuo.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，律师事务所的咨询是通过多个工作人员的协作进行的，而现有的法律 LLM 通常与用户单独互动。为弥合这一差距，我们引入了一个名为 LawLuo
    的多代理协作法律对话框架。
- en: Firstly, we perform LoRA fine-tuning on ChatGLM-3-6b using knowledge-intensive
    dialogue data and real multi-turn dialogue data from a Chinese law firm, resulting
    in a multi-turn dialogue LLM with a legal background, denoted as $LLM_{Legal}:(s_{0},U_{1:T})\mapsto
    R_{1:T}$ represents the initial dialogue state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们对 ChatGLM-3-6b 进行 LoRA 微调，使用来自一家中国律师事务所的知识密集型对话数据和真实的多轮对话数据，得到一个具有法律背景的多轮对话
    LLM，记作 $LLM_{Legal}:(s_{0},U_{1:T})\mapsto R_{1:T}$ 表示初始对话状态。
- en: Next, we design various agents with different roles, including a receptionist
    agent responsible for allocating lawyers from different fields based on user queries,
    lawyer agents for conducting multi-turn LQA with users, a secretary agent for
    organizing the dialogue between users and lawyers into consultation reports, and
    a boss agent supervising other agents. Finally, we construct a collaborative framework
    to guide interactions among these agents, thereby providing users with a high-quality
    LQA experience.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设计了具有不同角色的各种代理，包括负责根据用户查询分配不同领域律师的接待员代理、与用户进行多轮 LQA 的律师代理、将用户和律师之间的对话整理成咨询报告的秘书代理，以及监督其他代理的老板代理。最后，我们构建了一个协作框架来指导这些代理之间的互动，从而为用户提供高质量的
    LQA 体验。
- en: 3.1 Instruction Fine-tuning
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 指令微调
- en: 'Existing LLMs in the legal domain, despite leveraging a substantial volume
    of dialogue data during the instruction fine-tuning process, frequently contend
    with quality deficiencies. Figure [2](#Sx3.F2 "Figure 2 ‣ 3.1 Instruction Fine-tuning
    ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")
    presents the lawyer consultation data³³3[https://github.com/liuhuanyong/CrimeKgAssitant](https://github.com/liuhuanyong/CrimeKgAssitant)
    utilized by LawGPT_zh and the Baidu Zhidao Q&A data⁴⁴4[https://pan.baidu.com/s/18Lwq16VBo6wBD_qLb3i33g](https://pan.baidu.com/s/18Lwq16VBo6wBD_qLb3i33g)
    employed by LexiLaw. It is apparent that the questions’ clarity and the responses’
    professionalism in these dialogues are suboptimal, impairing models’ instruction
    follow capabilities in LQA.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管法律领域现有的 LLM 在指令微调过程中利用了大量的对话数据，但常常面临质量不足的问题。图 [2](#Sx3.F2 "Figure 2 ‣ 3.1
    Instruction Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law
    Firm Co-run by LLM Agents") 展示了 LawGPT_zh 使用的律师咨询数据³³3[https://github.com/liuhuanyong/CrimeKgAssitant](https://github.com/liuhuanyong/CrimeKgAssitant)
    和 LexiLaw 使用的百度知道 Q&A 数据⁴⁴4[https://pan.baidu.com/s/18Lwq16VBo6wBD_qLb3i33g](https://pan.baidu.com/s/18Lwq16VBo6wBD_qLb3i33g)。显然，这些对话中问题的清晰度和回答的专业性都不尽如人意，影响了模型在
    LQA 中的指令跟随能力。'
- en: '![Refer to caption](img/225108e775af39240c4f69eff6619207.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/225108e775af39240c4f69eff6619207.png)'
- en: 'Figure 2: Examples of dialogue data used for instruction fine-tuning of existing
    Chinese legal LLMs. The upper section shows legal consultation data used by LawGPT_zh,
    while the lower section displays Q&A data from Baidu Zhidao used by LexiLaw. It
    is evident from these examples that the professionalism of both the questions
    and answers is relatively low.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于现有中文法律 LLM 指令微调的对话数据示例。上部分展示了 LawGPT_zh 使用的法律咨询数据，下部分显示了 LexiLaw 使用的百度知道
    Q&A 数据。从这些示例中可以明显看出，问题和回答的专业性相对较低。
- en: 'Recent studies indicate that utilizing fewer but higher-quality data can significantly
    enhance the instruction-following capabilities of LLMs (Wei et al. [2023b](#bib.bib19);
    Xia et al. [2024](#bib.bib21)). Consequently, we developed a smaller but higher-quality
    dataset, termed Knowledge-INtensive LEgal Dialogue (KINLED) dataset. Firstly,
    we screened existing legal dialogue datasets, discarding lower-quality dialogues
    and retaining those of higher-quality. Moreover, to enhance the model’s application
    of critical legal terminologies, we constructed legal term and explanation dialogue
    data. Additionally, to improve the model’s understanding of significant charges
    and legal provisions, we created legal judgment dialogue and judicial interpretation
    dialogue. The KINLED dataset construction employed the self-instruct strategy,
    with specific construction details provided in Appendix A. Table [1](#Sx3.T1 "Table
    1 ‣ 3.1 Instruction Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese
    Law Firm Co-run by LLM Agents") presents the statistical information of the KINLED
    dataset.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '最近的研究表明，利用较少但高质量的数据可以显著提升 LLM 的指令跟随能力（Wei 等 [2023b](#bib.bib19)；Xia 等 [2024](#bib.bib21)）。因此，我们开发了一个更小但高质量的数据集，称为知识密集型法律对话（KINLED）数据集。首先，我们筛选了现有的法律对话数据集，剔除了较低质量的对话，保留了高质量的对话。此外，为了增强模型对关键法律术语的应用，我们构建了法律术语及解释对话数据。为了提升模型对重要指控和法律条款的理解，我们还创建了法律判决对话和司法解释对话。KINLED
    数据集的构建采用了自我指令策略，具体构建细节见附录 A。表 [1](#Sx3.T1 "Table 1 ‣ 3.1 Instruction Fine-tuning
    ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")
    展示了 KINLED 数据集的统计信息。'
- en: '| Composition | No. Dialogue Entries |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 组成 | 对话条目数量 |'
- en: '| Legal Term Explanation Dialogue | 3,125 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 法律术语解释对话 | 3,125 |'
- en: '| Legal Judgment Dialogue | 533 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 法律判决对话 | 533 |'
- en: '| Judicial Interpretation Dialogue | 4,382 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 司法解释对话 | 4,382 |'
- en: '| Scenario Q&A Based on Legal Grounds | 3,026 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 基于法律依据的情景问答 | 3,026 |'
- en: '| Single-Turn Legal Dialogue | 995 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 单轮法律对话 | 995 |'
- en: '| Judicial Examination Dialogue | 985 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 司法考试对话 | 985 |'
- en: 'Table 1: Statistical information of the Knowledge-INtensive LEgal Dialogue
    (KINLED) dataset we constructed. The legal term and explanation dialogues, legal
    judgment dialogues, as well as judicial interpretation dialogues, were created
    by us, while the other dialogues were selected from existing works.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们构建的Knowledge-INtensive LEgal Dialogue（KINLED）数据集的统计信息。法律术语和解释对话、法律判断对话以及司法解释对话由我们创建，其他对话则选自现有的作品。
- en: 'The existing legal LLMs have all been fine-tuned using single-turn dialogue
    data, which hinders their ability to follow instructions in multi-turn dialogue
    scenarios. To address this limitation, we constructed a fine-tuning dataset based
    on 3,260 anonymized multi-turn legal consultations from a law firm, termed MUltiple
    Rounds LEgal Dialogue (MURLED). Figure [3](#Sx3.F3 "Figure 3 ‣ 3.1 Instruction
    Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run
    by LLM Agents") presents an example of a multi-turn dialogue from MURLED, demonstrating
    the clarity of the questions and the professionalism of the responses.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的法律LLM均使用单轮对话数据进行微调，这限制了它们在多轮对话场景中的指令跟随能力。为了解决这一限制，我们构建了一个基于3,260个匿名多轮法律咨询的数据集，称为MULtiple
    Rounds LEgal Dialogue（MURLED）。图 [3](#Sx3.F3 "图 3 ‣ 3.1 Instruction Fine-tuning
    ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")展示了MURLED中的一个多轮对话示例，展示了问题的清晰性和回应的专业性。'
- en: '![Refer to caption](img/dc4f2d21ccb412893b8ab7ebacdd4095.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dc4f2d21ccb412893b8ab7ebacdd4095.png)'
- en: 'Figure 3: An example dialogue from the MUltiple Rounds LEgal Dialogue (MURLED)
    dataset. As can be seen, the user’s inquiries are clear, and the lawyer’s responses
    are professional.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：来自MURLED数据集的一个对话示例。可以看出，用户的提问清晰，律师的回应专业。
- en: 'We use KINLED and MURLED to fine-tune ChatGLM-3-6b, a Chinese base LLM. To
    mitigate the risk of overfitting, we incorporated general conversational data
    from Alpaca-GPT4⁵⁵5[https://www.modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary),
    which comprises 52,000 generic Chinese dialogues, into the instruction fine-tuning
    process. To expedite the fine-tuning of the model and reduce reliance on computational
    resources, we employed the LoRA fine-tuning strategy, as illustrated in Equation
    [1](#Sx3.E1 "In 3.1 Instruction Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo:
    A Chinese Law Firm Co-run by LLM Agents"):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用KINLED和MURLED对中文基础LLM ChatGLM-3-6b进行微调。为了减少过拟合的风险，我们将来自Alpaca-GPT4⁵⁵5[https://www.modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary)的通用对话数据（包括52,000个通用中文对话）纳入指令微调过程中。为了加快模型的微调速度并减少对计算资源的依赖，我们采用了LoRA微调策略，如方程
    [1](#Sx3.E1 "在 3.1 Instruction Fine-tuning ‣ 3\. LawLuo Framework for LQA ‣ LawLuo:
    A Chinese Law Firm Co-run by LLM Agents")所示：'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\theta$ represents LoRA fine-tuning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\theta$代表LoRA微调。
- en: 3.2 Agent Definition
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 代理定义
- en: 'We define the following agents based on the Standard Operating Procedures (SOP)
    of law firms: 1) Receptionist, responsible for assigning lawyers to users based
    on the queries they present. 2) Lawyers, engage in multiple rounds dialogue to
    resolve users’ legal issues. 3) Secretary, responsible for compiling the dialogues
    between users and lawyers into consultation reports, and then submitting them
    to both the users and the boss. 4) Boss, charged with evaluating the lawyers and
    the secretary. Subsequently, we will provide a detailed exposition of these agents.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据法律事务所的标准操作程序（SOP）定义了以下代理：1）接待员，负责根据用户提出的查询分配律师。2）律师，进行多轮对话以解决用户的法律问题。3）秘书，负责将用户与律师之间的对话汇编成咨询报告，并提交给用户和老板。4）老板，负责评估律师和秘书。随后，我们将详细阐述这些代理。
- en: Receptionist
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 接待员
- en: 'Given the user’s initial question $u_{1}$, as shown in Equation [2](#Sx3.E2
    "In Receptionist ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo:
    A Chinese Law Firm Co-run by LLM Agents"):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '给定用户的初始问题$u_{1}$，如方程 [2](#Sx3.E2 "在 Receptionist ‣ 3.2 Agent Definition ‣ 3\.
    LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")所示：'
- en: '|  | $Rec:(u_{1},s_{0})\mapsto d\in\mathcal{D}$ |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $Rec:(u_{1},s_{0})\mapsto d\in\mathcal{D}$ |  | (2) |'
- en: 'where the set $\mathcal{D}$ represents our predefined consultation domains.
    We adhere to the classification of legal consultation domains as outlined on the
    HuLaw website⁶⁶6[https://www.66law.cn/](https://www.66law.cn/), encompassing a
    total of 16 domains, as depicted in Figure [4](#Sx3.F4 "Figure 4 ‣ Receptionist
    ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law
    Firm Co-run by LLM Agents"). Of these, 15 are common consultation fields, while
    other less common consultation areas are collectively categorized under “Others”.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中集合$\mathcal{D}$代表我们预定义的咨询领域。我们遵循了胡律师网站⁶⁶6[https://www.66law.cn/](https://www.66law.cn/)上所述的法律咨询领域分类，共包括16个领域，如图[4](#Sx3.F4
    "图4 ‣ 接待员 ‣ 3.2 代理人定义 ‣ 3\. LawLuo框架用于LQA ‣ LawLuo：由LLM代理人共同运营的中国律师事务所")所示。其中15个是常见咨询领域，而其他不太常见的咨询领域则统称为“其他”。
- en: '![Refer to caption](img/985979f56f59bd1abece45a457875e51.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/985979f56f59bd1abece45a457875e51.png)'
- en: 'Figure 4: Predefined legal consultation domains and statistical information'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：预定义法律咨询领域及统计信息
- en: 'We use a contrastive trained RoBERTa as the secretary agent. We first crawled
    35,060 legal questions with domain tags from HuaLaw web, with the number of questions
    in each domain illustrated in Figure [4](#Sx3.F4 "Figure 4 ‣ Receptionist ‣ 3.2
    Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run
    by LLM Agents"). Subsequently, we used the current question as the anchor, questions
    from the same domain as positive examples, and questions from different domains
    as negative examples. We optimized the semantic distance between samples using
    contrastive loss, as depicted in Equation [3](#Sx3.E3 "In Receptionist ‣ 3.2 Agent
    Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run
    by LLM Agents").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用经过对比训练的RoBERTa作为秘书代理。我们首先从华律网抓取了35,060个带有领域标签的法律问题，每个领域的问题数量如图[4](#Sx3.F4
    "图4 ‣ 接待员 ‣ 3.2 代理人定义 ‣ 3\. LawLuo框架用于LQA ‣ LawLuo：由LLM代理人共同运营的中国律师事务所")所示。随后，我们将当前问题作为锚点，将同一领域的问题作为正例，将不同领域的问题作为负例。我们通过对比损失优化样本之间的语义距离，如方程[3](#Sx3.E3
    "在接待员 ‣ 3.2 代理人定义 ‣ 3\. LawLuo框架用于LQA ‣ LawLuo：由LLM代理人共同运营的中国律师事务所")所示。
- en: '|  | $\mathcal{L}=\frac{1}{N}\sum_{n=1}^{N}\left[\left\&#124;\vec{a}_{i}-\vec{p}_{i}\right\&#124;^{2}+\max\left(0,\alpha-\left\&#124;\vec{a}_{i}-\vec{n}_{i}\right\&#124;\right)^{2}\right]$
    |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{N}\sum_{n=1}^{N}\left[\left\&#124;\vec{a}_{i}-\vec{p}_{i}\right\&#124;^{2}+\max\left(0,\alpha-\left\&#124;\vec{a}_{i}-\vec{n}_{i}\right\&#124;\right)^{2}\right]$
    |  | (3) |'
- en: where $\vec{a}_{i}$ denotes the Euclidean distance. We utilize Lawformer (Xiao
    et al. [2021](#bib.bib22)) as the embedding model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\vec{a}_{i}$表示欧几里得距离。我们使用Lawformer (Xiao et al. [2021](#bib.bib22))作为嵌入模型。
- en: Lawyers
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 律师
- en: 'We utilize the $LLM_{Legal}$, as shown in Equation [4](#Sx3.E4 "In Lawyers
    ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law
    Firm Co-run by LLM Agents")'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了$LLM_{Legal}$，如方程[4](#Sx3.E4 "在律师 ‣ 3.2 代理人定义 ‣ 3\. LawLuo框架用于LQA ‣ LawLuo：由LLM代理人共同运营的中国律师事务所")所示。
- en: '|  | $Lawyer_{d}\leftarrow RE_{d}(LLM_{Legal})$ |  | (4) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $Lawyer_{d}\leftarrow RE_{d}(LLM_{Legal})$ |  | (4) |'
- en: where $RE_{d}(\cdot)$ represents the prompt function used for role enhancement.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$RE_{d}(\cdot)$表示用于角色增强的提示函数。
- en: 'In addition, existing legal LLMs overlook a critical challenge in LQA: the
    legal questions posed by users are often rough and ambiguous. Directly inputting
    such questions into the LLM makes it difficult to obtain the desired answers.
    By contrast, in real-world practice, lawyers typically guide users to clarify
    their questions, resulting in clear and detailed inquiries. Therefore, we propose
    the Tree of Legal Clarifications (ToLC) algorithm, which builds on the Tree of
    Clarifications (ToC) proposed by (Kim et al. [2023](#bib.bib6)). The ToC uses
    top-K articles from Wikipedia to guide LLM in generating clarification questions.
    We adapted this by searching for top-K relevant cases from a legal case database
    to guide LLM in generating necessary legal clarifications. The ToC prunes unhelpful
    nodes through LLM’s self-verification, while we altered this process to involve
    active Yes/No answering by users. We believe users understand their own cases
    better than any language model or external knowledge. In other words, lawyers
    should guide users in clarifying legal facts, not fabricate the facts involved.
    Algorithm [1](#alg1 "Algorithm 1 ‣ Lawyers ‣ 3.2 Agent Definition ‣ 3\. LawLuo
    Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents") describes
    the implementation of ToLC.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，现有的法律LLMs忽略了LQA中的一个关键挑战：用户提出的法律问题往往粗糙且模糊。直接将这些问题输入LLM使得难以获得期望的答案。相比之下，在实际操作中，律师通常会引导用户澄清问题，从而形成清晰且详细的询问。因此，我们提出了法律澄清树（ToLC）算法，它基于（Kim
    et al. [2023](#bib.bib6)）提出的澄清树（ToC）。ToC使用维基百科中的前K篇文章来指导LLM生成澄清问题。我们通过从法律案例数据库中搜索前K个相关案例来改编这一点，以指导LLM生成必要的法律澄清。ToC通过LLM的自我验证来修剪无用的节点，而我们将这一过程改为用户主动回答是/否。我们相信用户比任何语言模型或外部知识更了解自己的案件。换句话说，律师应该引导用户澄清法律事实，而不是编造相关事实。算法
    [1](#alg1 "算法 1 ‣ 律师 ‣ 3.2 代理人定义 ‣ 3\. LawLuo LQA 框架 ‣ LawLuo：一个由LLM代理人共同运营的中国律师事务所")
    描述了ToLC的实现。
- en: Algorithm 1 Tree of Legal Clarifications (ToLC)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 法律澄清树（ToLC）
- en: 1:User’s query in the $t$ Lawyer agent generates a response
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 用户在 $t$ 律师代理人中生成的查询响应'
- en: Secretary
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 秘书
- en: 'The duty of the secretary agent is organizing the dialogue between the user
    and the lawyer into a consultation report and then submitting it to both the user
    and the boss, as shown in Equation [5](#Sx3.E5 "In Secretary ‣ 3.2 Agent Definition
    ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents").'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 秘书代理人的职责是将用户与律师之间的对话整理成咨询报告，然后提交给用户和首席，如方程 [5](#Sx3.E5 "在秘书 ‣ 3.2 代理人定义 ‣ 3\.
    LawLuo LQA 框架 ‣ LawLuo：一个由LLM代理人共同运营的中国律师事务所") 所示。
- en: '|  | $Sec:(s_{0},U_{1:T},R_{1:T})\mapsto r$ |  | (5) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $Sec:(s_{0},U_{1:T},R_{1:T})\mapsto r$ |  | (5) |'
- en: where $r$ denotes the consultation report generated by the secretary agent.
    We utilize ChatGLM-3-6b as the secretary agent and employ In-Context Learning
    (ICL) to guide the model in producing consultation reports that meet expectations.
    We have constructed four standard consultation report samples, as shown in Appendix
    B, to serve as demonstrations in ICL reasoning.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r$ 表示秘书代理人生成的咨询报告。我们使用ChatGLM-3-6b作为秘书代理人，并利用上下文学习（ICL）来指导模型生成符合期望的咨询报告。我们已经构建了四个标准咨询报告样本，如附录B所示，用于ICL推理中的演示。
- en: Boss
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 首席
- en: 'The boss agent is responsible for evaluating the lawyer and secretary agents.
    Essentially, the boss agent is a Reward Model (RM). We first train a binary evaluation
    RM, $RM:o\mapsto y$, categorized into “better” and “worse”. The training objective
    of the RM is to minimize the following loss function:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首席代理人负责评估律师和秘书代理人。实际上，首席代理人是一个奖励模型（RM）。我们首先训练一个二分类评估RM，$RM:o\mapsto y$，分类为“更好”和“更差”。RM的训练目标是最小化以下损失函数：
- en: '|  | $\displaystyle\mathcal{L}_{RM}=-\frac{1}{N}\sum_{i=1}^{N}$ |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{RM}=-\frac{1}{N}\sum_{i=1}^{N}$ |  | (6) |'
- en: '|  |  | $\displaystyle\left.+(1-y_{i})\cdot\log\left(1-\hat{y}_{i}(o_{i};\theta_{RM})\right)\right]$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left.+(1-y_{i})\cdot\log\left(1-\hat{y}_{i}(o_{i};\theta_{RM})\right)\right]$
    |  |'
- en: where $y_{i}$ as “better.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{i}$ 表示“更好”。
- en: We employed the PPO algorithm (Wang, He, and Tan [2020](#bib.bib16)) during
    the reinforcement learning phase, an efficient reinforcement learning method that
    utilizes RM’s evaluation outcomes to guide the updates of the LLM. This approach
    further aligns the LLM’s performance with the preferences of human experts (the
    boss agent).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在强化学习阶段采用了PPO算法（Wang, He, 和 Tan [2020](#bib.bib16)），这是一种高效的强化学习方法，利用RM的评估结果来指导LLM的更新。这种方法进一步使LLM的性能与人类专家（老板代理）的偏好保持一致。
- en: 'Based on the definitions of the agents, we constructed a collaborative pipeline
    designed to imitate the SOP of legal consultations in real law firms, as illustrated
    in Figure [5](#Sx3.F5 "Figure 5 ‣ Boss ‣ 3.2 Agent Definition ‣ 3\. LawLuo Framework
    for LQA ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents"). The pipeline begins
    with the user initiating a consultation. Subsequently, the receptionist agent
    refers the user to an appropriate lawyer agent based on the user’s inquiry. The
    lawyer agent then engages in a dialogue with the user, utilizing the ToLC algorithm
    to clarify any ambiguous inquiries. Once the consultation concludes, the secretary
    agent compiling the conversation records between the user and the lawyer agent
    into a consultation report and submitting it to the user and the boss. The boss
    agent evaluate the responses generated by the lawyer agent and the reports generated
    by the secretary agent to optimize the their subsequent outputs.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '根据代理定义，我们构建了一个协作流程，旨在模拟实际律师事务所的标准操作程序（SOP），如图[5](#Sx3.F5 "Figure 5 ‣ Boss ‣
    3.2 Agent Definition ‣ 3\. LawLuo Framework for LQA ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents")所示。该流程开始于用户发起咨询。随后，接待员代理根据用户的咨询将用户转介给合适的律师代理。律师代理与用户进行对话，利用ToLC算法澄清任何模糊的咨询。一旦咨询结束，秘书代理将用户与律师代理之间的对话记录编制成咨询报告，并提交给用户和老板。老板代理评估律师代理生成的响应和秘书代理生成的报告，以优化其后续输出。'
- en: '![Refer to caption](img/14219b06d7be4df07c2805b2845a33d7.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/14219b06d7be4df07c2805b2845a33d7.png)'
- en: 'Figure 5: Collaborative workflow of agents designed according to Standard Operating
    Procedures (SOP) of actual law firms'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：根据实际律师事务所的标准操作程序（SOP）设计的代理协作工作流程
- en: 4\. Experimental Setup
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验设置
- en: All our experiments were conducted on a 40G A100 GPU. The PyTorch 2.3.0 and
    the HuggingFace Transformers 4.40.0 were used. The learning rate for LoRA fine-tuning
    was set to 0.00005, with a training batch size of 2, over a total of 3 epochs,
    and model weights were saved every 1,000 steps. Additionally, the rank of LoRA
    was set to 16, the alpha parameter was set to 32, and the dropout rate was set
    to 0.05\. The case database used in the ToLC algorithm was sourced from the China
    Judgments Online website.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的实验均在40G A100 GPU上进行。使用了PyTorch 2.3.0和HuggingFace Transformers 4.40.0。LoRA微调的学习率设置为0.00005，训练批次大小为2，总共进行了3个epochs，模型权重每1,000步保存一次。此外，LoRA的秩设置为16，alpha参数设置为32，dropout率设置为0.05。ToLC算法中使用的案例数据库来源于中国裁判文书网。
- en: 5\. Results and Analysis
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结果与分析
- en: 5.1 Results on Single-turn Questions
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 单轮问题的结果
- en: 'We follow the current mainstream evaluation methods for large models to assess
    our LawLuo (Thirunavukarasu et al. [2023](#bib.bib11); Xiong et al. [2023](#bib.bib23);
    Zhang et al. [2023b](#bib.bib28)). We engaged both human experts and GPT-4o to
    evaluate the model’s performance based on the following criteria: lawyer-like
    language style, usefulness of legal advice, and accuracy of legal knowledge. We
    employed pairwise evaluation, where given a response from LawLuo and a baseline
    response, the better one is selected. Figure [6](#Sx5.F6 "Figure 6 ‣ 5.1 Results
    on Single-turn Questions ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents") shows the win rate of LawLuo compared to baselines. Overall,
    LawLuo significantly outperforms the baselines. Additionally, as seen in the figure,
    LawLuo achieved a 72% win rate against ChatGLM-3-6b, demonstrating the effectiveness
    of our instruction fine-tuning process, given that LawLuo was fine-tuned on ChatGLM-3-6b.
    Furthermore, the figure indicates that LawLuo outperforms the other two legal
    large models, LawGPT and LawyerLLaMa. Lastly, even when compared to GPT-4, LawLuo
    still exhibits a significant advantage.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '我们遵循当前主流的大型模型评估方法来评估我们的 LawLuo（Thirunavukarasu 等人 [2023](#bib.bib11)；Xiong
    等人 [2023](#bib.bib23)；Zhang 等人 [2023b](#bib.bib28)）。我们聘请了人类专家和 GPT-4o，根据以下标准评估模型的表现：类似律师的语言风格、法律建议的实用性和法律知识的准确性。我们采用了成对评估的方法，在
    LawLuo 的回应和基线回应中选择更好的一个。图 [6](#Sx5.F6 "图 6 ‣ 5.1 单轮问题的结果 ‣ 5\. 结果与分析 ‣ LawLuo:
    一个由 LLM 代理共同运营的中国律师事务所") 显示了 LawLuo 相对于基线的胜率。总体来看，LawLuo 显著优于基线。此外，如图所示，LawLuo
    对 ChatGLM-3-6b 的胜率为 72%，这证明了我们指令微调过程的有效性，考虑到 LawLuo 是在 ChatGLM-3-6b 上进行微调的。此外，图表还表明
    LawLuo 优于其他两个法律大型模型，LawGPT 和 LawyerLLaMa。最后，即使与 GPT-4 相比，LawLuo 依然展现出显著的优势。'
- en: '![Refer to caption](img/59f836fc9fbdb142bb14c4e5169358d7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/59f836fc9fbdb142bb14c4e5169358d7.png)'
- en: 'Figure 6: Win rate of LawLuo compared to the baselines'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LawLuo 相较于基线的胜率
- en: 5.2 Results on Multi-turn Dialogues
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 多轮对话的结果
- en: 'This section aims to analyze the advantages of LawLuo in multi-turn dialogue
    scenarios. We employed GPT-4o to rate the responses generated by the model in
    each turn of the dialogue, assessing the quality variation of the responses as
    the number of dialogue turns increased. We set a scoring range of 1-10, using
    the criteria of lawyer-like language style, the usefulness of legal advice, and
    the accuracy of legal knowledge as mentioned in Section 5.1\. The prompt driving
    GPT-4o’s scoring is provided in Appendix C. As shown in Figure [7](#Sx5.F7 "Figure
    7 ‣ 5.2 Results on Multi-turn Dialogues ‣ 5\. Results and Analysis ‣ LawLuo: A
    Chinese Law Firm Co-run by LLM Agents"), LawLuo’s responses maintain a high score
    as the number of dialogue turns increases. This result is primarily attributed
    to our use of multi-turn dialogue data, enhanced by ChatGPT and sourced from actual
    law firms, for instruction fine-tuning, as opposed to other legal LLMs that only
    use single-turn dialogue data.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '本节旨在分析 LawLuo 在多轮对话场景中的优势。我们使用 GPT-4o 对模型在每轮对话中生成的回应进行评分，评估回应质量随着对话轮次增加的变化。我们设定了
    1-10 的评分范围，使用第 5.1 节提到的律师风格的语言、法律建议的实用性和法律知识的准确性作为标准。驱动 GPT-4o 评分的提示在附录 C 中提供。如图
    [7](#Sx5.F7 "图 7 ‣ 5.2 多轮对话的结果 ‣ 5\. 结果与分析 ‣ LawLuo: 一个由 LLM 代理共同运营的中国律师事务所")
    所示，LawLuo 的回应在对话轮次增加时保持高分。这一结果主要归因于我们使用了由 ChatGPT 增强并来源于实际律师事务所的多轮对话数据进行指令微调，而不是像其他法律
    LLM 那样仅使用单轮对话数据。'
- en: '![Refer to caption](img/f154a9ec8a0c60f211039a68842e7c38.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f154a9ec8a0c60f211039a68842e7c38.png)'
- en: 'Figure 7: The variation in the quality of model-generated responses with increasing
    dialogue turns'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：模型生成的回应质量随对话轮次增加的变化
- en: 5.3 Ablation Study
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融研究
- en: 'This section aims to validate the contributions of each component within the
    framework. We continue to use GPT-4o as the evaluator to assess the win rate of
    LawLuo over ChatGPT after ablation, as illustrated in Figure [8](#Sx5.F8 "Figure
    8 ‣ 5.3 Ablation Study ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents"). From the figure, it is evident that the win rate of LawLuo
    over ChatGPT decreases by 3% after ablating the receptionist agent and role enhancement.
    This result validates our hypothesis that legal LLMs should be assigned different
    domain-specific roles to provide more targeted answers based on the user’s consultation
    field. Additionally, the figure shows that the boss agent also contributes to
    LawLuo’s performance, as it can optimize the responses generated by the lawyer.
    Finally, we observe a significant decline in model performance after removing
    the ToLD module. This indicates that clarifying users’ vague and ambiguous queries
    is crucial for generating high-quality responses in legal question-answering.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '本节旨在验证框架内每个组件的贡献。我们继续使用GPT-4o作为评估者，以评估LawLuo在去除部分功能后的胜率，如图[8](#Sx5.F8 "Figure
    8 ‣ 5.3 Ablation Study ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm
    Co-run by LLM Agents")所示。从图中可以看出，去除接待员代理和角色增强后，LawLuo相对于ChatGPT的胜率下降了3%。这一结果验证了我们的假设，即法律LLM应分配不同的领域特定角色，以根据用户咨询领域提供更有针对性的答案。此外，图中还显示，老板代理也对LawLuo的表现有所贡献，因为它可以优化律师生成的回应。最后，我们观察到在移除ToLD模块后模型性能显著下降。这表明澄清用户模糊和含糊的问题对生成高质量的法律问答至关重要。'
- en: '![Refer to caption](img/ad6e30cec051f8dff16b7d93deb8638d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad6e30cec051f8dff16b7d93deb8638d.png)'
- en: 'Figure 8: Results of Ablation Experiments'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：消融实验结果
- en: 5.4 Case Study of ToLD
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 ToLD案例研究
- en: 'This section elucidates the contributions of ToLD through a specific case study.
    We take the query “ 我要离婚，该怎么办？(I want a divorce, what should I do?)” as an example
    initiated by a user. The left side of Figure 9 displays the clarifying questions
    generated by the ToLD algorithm. The results after the user actively marked Yes/No
    are shown on the right side of Figure [9](#Sx5.F9 "Figure 9 ‣ 5.4 Case Study of
    ToLD ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '本节通过一个具体案例研究阐明了ToLD的贡献。我们以用户发起的查询“我要离婚，该怎么办？”为例。图9的左侧展示了ToLD算法生成的澄清问题。用户主动标记“是/否”后的结果显示在图[9](#Sx5.F9
    "Figure 9 ‣ 5.4 Case Study of ToLD ‣ 5\. Results and Analysis ‣ LawLuo: A Chinese
    Law Firm Co-run by LLM Agents")的右侧。'
- en: '![Refer to caption](img/119ec07bab5a722511fb38b061ccd08e.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/119ec07bab5a722511fb38b061ccd08e.png)'
- en: 'Figure 9: Subfigure (a) presents the clarification tree generated by the ToLD
    algorithm, while subfigure (b) depicts the outcome after the user has marked the
    nodes on the tree with Yes/No.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：子图（a）展示了ToLD算法生成的澄清树，子图（b）描绘了用户标记了树上节点的“是/否”后的结果。
- en: 'Additionally, the first row of Table [2](#A1.T2 "Table 2 ‣ D. Case Study of
    ToLC ‣ Appendix A Appendix ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents")
    in Appendix D presents the answer generated by LawLuo without ToLD, while the
    second row shows the answer generated by LawLuo with ToLD. It can be observed
    that ToLD assists legal LLMs in generating more accurate and personalized responses.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，附录D中表[2](#A1.T2 "Table 2 ‣ D. Case Study of ToLC ‣ Appendix A Appendix ‣
    LawLuo: A Chinese Law Firm Co-run by LLM Agents")的第一行展示了LawLuo在没有ToLD的情况下生成的回答，而第二行展示了LawLuo在有ToLD的情况下生成的回答。可以观察到，ToLD帮助法律LLM生成更准确和个性化的回答。'
- en: 6\. Discussion
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论
- en: Enlightenment
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启示
- en: This study has provided us with several intriguing insights. Firstly, we discovered
    that omitting continue pre-training and directly conducting instruction fine-tuning
    does not impact the effectiveness of the legal LLM. This indicates that the Chinese
    base model has already acquired a certain degree of legal knowledge during the
    pre-training phase, obviating the need for additional pre-training with legal
    corpora. In fact, the pre-training corpora used by many Chinese base models are
    extraordinarily large and likely encompass a substantial amount of legal text.
    We also found that the instruction fine-tuning phase does not require tens of
    thousands or even hundreds of thousands of dialogue data; approximately thousands
    high-quality legal dialogue data are sufficient to elicit the model’s legal knowledge
    application capabilities in dialogue scenarios. This paves a new path for the
    future fine-tuning of legal LLMs. Lastly, the effectiveness of role enhancement
    and multi-agent collaboration in our method validated our hypothesis that a legal
    consultation should not be a mere conversational process but a complex task involving
    multiple business processes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究为我们提供了几个有趣的见解。首先，我们发现省略继续预训练，直接进行指令微调不会影响法律LLM的效果。这表明中文基础模型在预训练阶段已经获得了一定的法律知识，因此无需使用法律语料库进行额外的预训练。实际上，许多中文基础模型使用的预训练语料库非常庞大，可能涵盖了大量的法律文本。我们还发现，指令微调阶段不需要数万甚至数十万条对话数据；大约几千条高质量法律对话数据就足以引发模型在对话场景中的法律知识应用能力。这为未来法律LLMs的微调开辟了一条新路。最后，我们方法中角色增强和多智能体协作的有效性验证了我们的假设，即法律咨询不应仅仅是一个对话过程，而是一个涉及多个业务流程的复杂任务。
- en: Limitation and Future Work
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局限性和未来工作
- en: Although we observed that the receptionist agent achieved a question classification
    accuracy rate exceeding 98% in our experiments, any misclassification leads to
    the lawyer agent continuing the interaction based on the initial error (a matrimonial
    lawyer could handle legal inquiries related to traffic accidents, albeit with
    reduced effectiveness). In the future, we plan to design a dynamic receptionist
    mechanism that reallocates the user to the appropriate lawyer based on conversation
    content whenever a domain mismatch is detected during the interaction with the
    legal LLM.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们观察到接待员智能体在实验中达到了超过98%的问题分类准确率，但任何错误分类都会导致律师智能体继续基于初始错误进行互动（例如婚姻律师可能处理与交通事故相关的法律咨询，但效果会有所下降）。未来，我们计划设计一个动态接待机制，根据对话内容在检测到领域不匹配时，将用户重新分配给合适的律师。
- en: Despite introducing the concept of multi-agent collaboration, not every agent
    is an LLM, resulting in limited thinking and decision-making capabilities. In
    the future, we intend to develop a collaborative framework in which all agents
    are LLMs, thereby enhancing the overall framework’s cognitive and decision-making
    abilities.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管引入了多智能体协作的概念，但并不是每个智能体都是LLM，这导致了思维和决策能力的局限性。未来，我们计划开发一个所有智能体都是LLM的协作框架，从而提升整体框架的认知和决策能力。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'We propose a multi-agent collaboration framework for legal dialogue, termed
    LawLuo. Experimental results demonstrate that LawLuo outperforms baseline LLMs
    in three dimensions: lawyer-like language style, the usefulness of legal advice,
    and the accuracy of legal knowledge. Moreover, it continues to produce high-quality
    answer even after multiple rounds of dialogue. We contribute two high-quality
    datasets for legal LLM instruction fine-tuning, KINLED and MURLED. Experimental
    results indicate that using these datasets can fine-tune more effective legal
    LLMs.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一个用于法律对话的多智能体协作框架，称为LawLuo。实验结果表明，LawLuo在三个维度上优于基线LLMs：类似律师的语言风格、法律建议的有用性以及法律知识的准确性。此外，它在多轮对话后仍能产生高质量的答案。我们贡献了两个用于法律LLM指令微调的高质量数据集：KINLED和MURLED。实验结果表明，使用这些数据集可以微调出更有效的法律LLMs。
- en: Acknowledgments
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by “the Fundamental Research Funds for the Central Universities,
    xxxx”
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了“中央高校基本科研业务费，xxxx”的支持。
- en: References
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Du et al. (2023) Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; and Mordatch,
    I. 2023. Improving factuality and reasoning in language models through multiagent
    debate. *arXiv preprint arXiv:2305.14325*.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人（2023）Du, Y.; Li, S.; Torralba, A.; Tenenbaum, J. B.; 和 Mordatch, I. 2023.
    通过多代理辩论提高语言模型的事实性和推理能力。*arXiv 预印本 arXiv:2305.14325*。
- en: 'Du et al. (2022) Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
    Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank
    Infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 320–335.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人（2022）Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; 和 Tang,
    J. 2022. GLM: 自回归空白填充的通用语言模型预训练。见于 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，320–335。'
- en: 'Duan et al. (2019) Duan, X.; Wang, B.; Wang, Z.; Ma, W.; Cui, Y.; Wu, D.; Wang,
    S.; Liu, T.; Huo, T.; Hu, Z.; et al. 2019. Cjrc: A reliable human-annotated benchmark
    dataset for chinese judicial reading comprehension. In *Chinese Computational
    Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October
    18–20, 2019, Proceedings 18*, 439–451\. Springer.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Duan 等人（2019）Duan, X.; Wang, B.; Wang, Z.; Ma, W.; Cui, Y.; Wu, D.; Wang, S.;
    Liu, T.; Huo, T.; Hu, Z.; 等. 2019. Cjrc: 一个可靠的中文司法阅读理解人工标注基准数据集。见于 *中国计算语言学：第18届中国国家会议，CCL
    2019，昆明，中国，2019年10月18–20日，论文集第18卷*，439–451。Springer。'
- en: 'Hemmer et al. (2022) Hemmer, P.; Schellhammer, S.; Vössing, M.; Jakubik, J.;
    and Satzger, G. 2022. Forming Effective Human-AI Teams: Building Machine Learning
    Models that Complement the Capabilities of Multiple Experts. In *Proceedings of
    the Thirty-First International Joint Conference on Artificial Intelligence*, 2478.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hemmer 等人（2022）Hemmer, P.; Schellhammer, S.; Vössing, M.; Jakubik, J.; 和 Satzger,
    G. 2022. 形成有效的人机团队：构建补充多个专家能力的机器学习模型。见于 *第31届国际人工智能联合会议论文集*，2478。
- en: Huang et al. (2023) Huang, Q.; Tao, M.; Zhang, C.; An, Z.; Jiang, C.; Chen,
    Z.; Wu, Z.; and Feng, Y. 2023. Lawyer llama technical report. *arXiv preprint
    arXiv:2305.15062*.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2023）Huang, Q.; Tao, M.; Zhang, C.; An, Z.; Jiang, C.; Chen, Z.; Wu,
    Z.; 和 Feng, Y. 2023. 律师 llama 技术报告。*arXiv 预印本 arXiv:2305.15062*。
- en: 'Kim et al. (2023) Kim, G.; Kim, S.; Jeon, B.; Park, J.; and Kang, J. 2023.
    Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented
    Large Language Models. In *Proceedings of the 2023 Conference on Empirical Methods
    in Natural Language Processing*, 996–1009.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人（2023）Kim, G.; Kim, S.; Jeon, B.; Park, J.; 和 Kang, J. 2023. 澄清树：使用检索增强的大型语言模型回答模糊问题。见于
    *2023年自然语言处理经验方法会议论文集*，996–1009。
- en: Louis, van Dijck, and Spanakis (2024) Louis, A.; van Dijck, G.; and Spanakis,
    G. 2024. Interpretable long-form legal question answering with retrieval-augmented
    large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, 22266–22275.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louis, van Dijck 和 Spanakis（2024）Louis, A.; van Dijck, G.; 和 Spanakis, G. 2024.
    使用检索增强的大型语言模型进行可解释的长篇法律问答。见于 *AAAI 人工智能会议论文集*，第38卷，22266–22275。
- en: Qian et al. (2023) Qian, C.; Cong, X.; Yang, C.; Chen, W.; Su, Y.; Xu, J.; Liu,
    Z.; and Sun, M. 2023. Communicative agents for software development. *arXiv preprint
    arXiv:2307.07924*.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人（2023）Qian, C.; Cong, X.; Yang, C.; Chen, W.; Su, Y.; Xu, J.; Liu, Z.;
    和 Sun, M. 2023. 软件开发的交互式代理。*arXiv 预印本 arXiv:2307.07924*。
- en: 'Shao et al. (2023) Shao, Y.; Li, L.; Dai, J.; and Qiu, X. 2023. Character-LLM:
    A Trainable Agent for Role-Playing. In *Proceedings of the 2023 Conference on
    Empirical Methods in Natural Language Processing*, 13153–13187.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao 等人（2023）Shao, Y.; Li, L.; Dai, J.; 和 Qiu, X. 2023. Character-LLM: 一种可训练的角色扮演代理。见于
    *2023年自然语言处理经验方法会议论文集*，13153–13187。'
- en: 'Taniguchi and Kano (2017) Taniguchi, R.; and Kano, Y. 2017. Legal yes/no question
    answering system using case-role analysis. In *New Frontiers in Artificial Intelligence:
    JSAI-isAI 2016 Workshops, LENLS, HAT-MASH, AI-Biz, JURISIN and SKL, Kanagawa,
    Japan, November 14-16, 2016, Revised Selected Papers*, 284–298\. Springer.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taniguchi 和 Kano（2017）Taniguchi, R.; 和 Kano, Y. 2017. 使用案例角色分析的法律是/否问答系统。见于
    *人工智能新前沿：JSAI-isAI 2016工作坊，LENLS，HAT-MASH，AI-Biz，JURISIN 和 SKL，日本神奈川，2016年11月14-16日，修订选择论文集*，284–298。Springer。
- en: 'Thirunavukarasu et al. (2023) Thirunavukarasu, A. J.; Ting, D. S. J.; Elangovan,
    K.; Gutierrez, L.; Tan, T. F.; and Ting, D. S. W. 2023. Large language models
    in medicine. *Nature medicine*, 29(8): 1930–1940.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thirunavukarasu 等人（2023）Thirunavukarasu, A. J.; Ting, D. S. J.; Elangovan,
    K.; Gutierrez, L.; Tan, T. F.; 和 Ting, D. S. W. 2023. 医学中的大型语言模型。*自然医学*，29(8):
    1930–1940。'
- en: 'Touvron et al. (2023a) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023a.
    Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023a）Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; 等. 2023a.
    Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b.
    Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023b）Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; 等. 2023b.
    Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: 'Wang et al. (2024) Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang,
    J.; Chen, Z.; Tang, J.; Chen, X.; Lin, Y.; et al. 2024. A survey on large language
    model based autonomous agents. *Frontiers of Computer Science*, 18(6): 186345.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2024）Wang, L.; Ma, C.; Feng, X.; Zhang, Z.; Yang, H.; Zhang, J.; Chen,
    Z.; Tang, J.; Chen, X.; Lin, Y.; 等. 2024. 基于大语言模型的自主智能体综述。*计算机科学前沿*，18(6): 186345。'
- en: 'Wang et al. (2023a) Wang, L.; Zhang, J.; Chen, X.; Lin, Y.; Song, R.; Zhao,
    W. X.; and Wen, J.-R. 2023a. Recagent: A novel simulation paradigm for recommender
    systems. *arXiv preprint arXiv:2306.02552*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023a）Wang, L.; Zhang, J.; Chen, X.; Lin, Y.; Song, R.; Zhao, W. X.;
    和 Wen, J.-R. 2023a. Recagent: 一个新颖的推荐系统模拟范式。*arXiv 预印本 arXiv:2306.02552*。'
- en: Wang, He, and Tan (2020) Wang, Y.; He, H.; and Tan, X. 2020. Truly proximal
    policy optimization. In *Uncertainty in artificial intelligence*, 113–122\. PMLR.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, He 和 Tan（2020）Wang, Y.; He, H.; 和 Tan, X. 2020. 真实的近端策略优化。见于 *人工智能中的不确定性*，113–122\.
    PMLR。
- en: 'Wang et al. (2023b) Wang, Z. M.; Peng, Z.; Que, H.; Liu, J.; Zhou, W.; Wu,
    Y.; Guo, H.; Gan, R.; Ni, Z.; Zhang, M.; et al. 2023b. Rolellm: Benchmarking,
    eliciting, and enhancing role-playing abilities of large language models. *arXiv
    preprint arXiv:2310.00746*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023b）Wang, Z. M.; Peng, Z.; Que, H.; Liu, J.; Zhou, W.; Wu, Y.; Guo,
    H.; Gan, R.; Ni, Z.; Zhang, M.; 等. 2023b. Rolellm: 基准测试、引导和提升大型语言模型的角色扮演能力。*arXiv
    预印本 arXiv:2310.00746*。'
- en: 'Wei et al. (2023a) Wei, J.; Shuster, K.; Szlam, A.; Weston, J.; Urbanek, J.;
    and Komeili, M. 2023a. Multi-party chat: Conversational agents in group settings
    with humans and models. *arXiv preprint arXiv:2304.13835*.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等（2023a）Wei, J.; Shuster, K.; Szlam, A.; Weston, J.; Urbanek, J.; 和 Komeili,
    M. 2023a. 多方聊天: 在群体环境中与人类和模型的对话代理。*arXiv 预印本 arXiv:2304.13835*。'
- en: 'Wei et al. (2023b) Wei, L.; Jiang, Z.; Huang, W.; and Sun, L. 2023b. Instructiongpt-4:
    A 200-instruction paradigm for fine-tuning minigpt-4. *arXiv preprint arXiv:2308.12067*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等（2023b）Wei, L.; Jiang, Z.; Huang, W.; 和 Sun, L. 2023b. Instructiongpt-4:
    一个用于微调 minigpt-4 的 200 指令范式。*arXiv 预印本 arXiv:2308.12067*。'
- en: 'Xi et al. (2023) Xi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y.; Hong, B.; Zhang,
    M.; Wang, J.; Jin, S.; Zhou, E.; et al. 2023. The rise and potential of large
    language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xi 等（2023）Xi, Z.; Chen, W.; Guo, X.; He, W.; Ding, Y.; Hong, B.; Zhang, M.;
    Wang, J.; Jin, S.; Zhou, E.; 等. 2023. 大型语言模型基础代理的崛起与潜力: 综述。*arXiv 预印本 arXiv:2309.07864*。'
- en: 'Xia et al. (2024) Xia, M.; Malladi, S.; Gururangan, S.; Arora, S.; and Chen,
    D. 2024. Less: Selecting influential data for targeted instruction tuning. *arXiv
    preprint arXiv:2402.04333*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等（2024）Xia, M.; Malladi, S.; Gururangan, S.; Arora, S.; 和 Chen, D. 2024.
    Less: 为有针对性的指令调优选择影响力数据。*arXiv 预印本 arXiv:2402.04333*。'
- en: 'Xiao et al. (2021) Xiao, C.; Hu, X.; Liu, Z.; Tu, C.; and Sun, M. 2021. Lawformer:
    A pre-trained language model for chinese legal long documents. *AI Open*, 2: 79–84.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等（2021）Xiao, C.; Hu, X.; Liu, Z.; Tu, C.; 和 Sun, M. 2021. Lawformer: 一个用于中文法律长文档的预训练语言模型。*AI
    Open*，2: 79–84。'
- en: 'Xiong et al. (2023) Xiong, H.; Wang, S.; Zhu, Y.; Zhao, Z.; Liu, Y.; Huang,
    L.; Wang, Q.; and Shen, D. 2023. Doctorglm: Fine-tuning your chinese doctor is
    not a herculean task. *arXiv preprint arXiv:2304.01097*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiong 等（2023）Xiong, H.; Wang, S.; Zhu, Y.; Zhao, Z.; Liu, Y.; Huang, L.; Wang,
    Q.; 和 Shen, D. 2023. Doctorglm: 微调你的中文医生不是一项艰巨的任务。*arXiv 预印本 arXiv:2304.01097*。'
- en: 'Yang et al. (2023) Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin,
    C.; Lv, C.; Pan, D.; Wang, D.; Yan, D.; et al. 2023. Baichuan 2: Open large-scale
    language models. *arXiv preprint arXiv:2309.10305*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等（2023）Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv,
    C.; Pan, D.; Wang, D.; Yan, D.; 等. 2023. Baichuan 2: 开放的大规模语言模型。*arXiv 预印本 arXiv:2309.10305*。'
- en: 'Yang et al. (2024) Yang, S.; Zhao, H.; Zhu, S.; Zhou, G.; Xu, H.; Jia, Y.;
    and Zan, H. 2024. Zhongjing: Enhancing the chinese medical capabilities of large
    language model through expert feedback and real-world multi-turn dialogue. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38, 19368–19376.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等 (2024) Yang, S.; Zhao, H.; Zhu, S.; Zhou, G.; Xu, H.; Jia, Y.; 和 Zan,
    H. 2024. Zhongjing: 通过专家反馈和实际多轮对话提升大型语言模型的中文医疗能力。收录于 *AAAI 人工智能会议论文集*，第38卷，19368–19376。'
- en: Yoshioka, Aoki, and Suzuki (2021) Yoshioka, M.; Aoki, Y.; and Suzuki, Y. 2021.
    Bert-based ensemble methods with data augmentation for legal textual entailment
    in coliee statute law task. In *Proceedings of the eighteenth international conference
    on artificial intelligence and law*, 278–284.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoshioka, Aoki, 和 Suzuki (2021) Yoshioka, M.; Aoki, Y.; 和 Suzuki, Y. 2021. 基于
    Bert 的集成方法和数据增强在 coliee 法律条文任务中的应用。收录于 *第十八届国际人工智能与法律会议论文集*，278–284。
- en: 'Zhang et al. (2023a) Zhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Chen,
    G.; Li, J.; Wu, X.; Zhiyi, Z.; Xiao, Q.; et al. 2023a. HuatuoGPT, Towards Taming
    Language Model to Be a Doctor. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, 10859–10885.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023a) Zhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Chen, G.;
    Li, J.; Wu, X.; Zhiyi, Z.; Xiao, Q.; 等. 2023a. HuatuoGPT, 朝着驯化语言模型成为医生的方向发展。收录于
    *计算语言学协会发现：EMNLP 2023*，10859–10885。
- en: 'Zhang et al. (2023b) Zhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Chen,
    G.; Li, J.; Wu, X.; Zhiyi, Z.; Xiao, Q.; et al. 2023b. HuatuoGPT, Towards Taming
    Language Model to Be a Doctor. In *Findings of the Association for Computational
    Linguistics: EMNLP 2023*, 10859–10885.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2023b) Zhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Chen, G.;
    Li, J.; Wu, X.; Zhiyi, Z.; Xiao, Q.; 等. 2023b. HuatuoGPT, 朝着驯化语言模型成为医生的方向发展。收录于
    *计算语言学协会发现：EMNLP 2023*，10859–10885。
- en: 'Zhang and Yang (2023) Zhang, X.; and Yang, Q. 2023. Xuanyuan 2.0: A large chinese
    financial chat model with hundreds of billions parameters. In *Proceedings of
    the 32nd ACM international conference on information and knowledge management*,
    4435–4439.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 和 Yang (2023) Zhang, X.; 和 Yang, Q. 2023. Xuanyuan 2.0: 一个拥有数百亿参数的大型中文金融聊天模型。收录于
    *第32届 ACM 国际信息与知识管理会议论文集*，4435–4439。'
- en: 'Zhong et al. (2020) Zhong, H.; Xiao, C.; Tu, C.; Zhang, T.; Liu, Z.; and Sun,
    M. 2020. JEC-QA: a legal-domain question answering dataset. In *Proceedings of
    the AAAI conference on artificial intelligence*, volume 34, 9701–9708.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等 (2020) Zhong, H.; Xiao, C.; Tu, C.; Zhang, T.; Liu, Z.; 和 Sun, M. 2020.
    JEC-QA: 一个法律领域的问答数据集。收录于 *AAAI 人工智能会议论文集*，第34卷，9701–9708。'
- en: 'Zhou et al. (2024) Zhou, Z.; Shi, J.-X.; Song, P.-X.; Yang, X.-W.; Jin, Y.-X.;
    Guo, L.-Z.; and Li, Y.-F. 2024. LawGPT: A Chinese Legal Knowledge-Enhanced Large
    Language Model. *arXiv preprint arXiv:2406.04614*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 (2024) Zhou, Z.; Shi, J.-X.; Song, P.-X.; Yang, X.-W.; Jin, Y.-X.; Guo,
    L.-Z.; 和 Li, Y.-F. 2024. LawGPT: 一个增强中文法律知识的大型语言模型。*arXiv 预印本 arXiv:2406.04614*。'
- en: Appendix A Appendix
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A. Construction of the KINLED Dataset
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A. KINLED 数据集的构建
- en: 'The KINLED dataset consists of six parts: legal term and explanation dialogues,
    legal judgment dialogues, judicial interpretation dialogues, scenario-based question-and-answer
    dialogues with legal grounds, single-round legal consultation dialogues and judicial
    examination dialogues.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: KINLED 数据集包含六个部分：法律术语及解释对话、法律判决对话、司法解释对话、基于场景的法律依据问答对话、单轮法律咨询对话和司法考试对话。
- en: legal Term and Explanation Dialogue
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法律术语及解释对话
- en: 'We first obtained 625 legal terms and their corresponding explanations from
    the Chinese Legal Terminology Compilation website, covering various legal fields
    such as civil law, criminal law, and jurisprudence. Subsequently, we called the
    official API of Moonshot AI, using the moonshot-v1-8k model to generate scenario
    dialogues related to these legal term explanations. To ensure the diversity of
    the dialogue data, we generated five different contextual question-and-answer
    pairs for each legal term and its corresponding explanation. The prompts we designed
    is as shown in Figure [10](#A1.F10 "Figure 10 ‣ legal Term and Explanation Dialogue
    ‣ A. Construction of the KINLED Dataset ‣ Appendix A Appendix ‣ LawLuo: A Chinese
    Law Firm Co-run by LLM Agents"):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先从中国法律术语编纂网站获得了625个法律术语及其相应解释，涵盖了民法、刑法和法理学等多个法律领域。随后，我们调用了 Moonshot AI 的官方
    API，使用 moonshot-v1-8k 模型生成与这些法律术语解释相关的场景对话。为了确保对话数据的多样性，我们为每个法律术语及其解释生成了五个不同的情境问答对。我们设计的提示如图
    [10](#A1.F10 "图 10 ‣ 法律术语及解释对话 ‣ A. KINLED 数据集的构建 ‣ 附录 A 附录 ‣ LawLuo: 一个由 LLM
    代理共同运营的中国律师事务所") 所示。'
- en: '![Refer to caption](img/7bf80e68374ec484a270844e30518d6f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7bf80e68374ec484a270844e30518d6f.png)'
- en: 'Figure 10: Prompt template to guide the moonshot-v1-8k model in generating
    legal term and explanation dialogues'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：用于指导moonshot-v1-8k模型生成法律术语和解释对话的提示模板
- en: Legal Judgment Dialogue
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 法律判断对话
- en: 'We obtained 200 representative legal judgments from the China Judgments Online
    database and performed data cleaning. These documents cover three different types
    of cases: civil, criminal, and administrative. Subsequently, we utilized the official
    API of Moonshot AI, employing the moonshot-v1-128k model to generate question-answer
    dialogues based on these 200 judgment documents. After human expert review, we
    produced 533 high-quality question-answer dialogues. To ensure diversity in the
    dialogues, multiple sets of question-answer pairs were generated for each judgment
    document. The prompts we designed is as shown in Figure [11](#A1.F11 "Figure 11
    ‣ Legal Judgment Dialogue ‣ A. Construction of the KINLED Dataset ‣ Appendix A
    Appendix ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents"):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从中国裁判文书网获得了200个代表性的法律判决，并进行了数据清洗。这些文件涵盖了三种不同类型的案件：民事、刑事和行政。随后，我们利用Moonshot
    AI的官方API，使用moonshot-v1-128k模型基于这200个判决文件生成问答对话。经过人工专家审核，我们生成了533个高质量问答对话。为了确保对话的多样性，为每个判决文件生成了多组问答对。我们设计的提示模板如图[11](#A1.F11
    "图 11 ‣ 法律判断对话 ‣ A. KINLED 数据集构建 ‣ 附录A 附录 ‣ LawLuo: 一个由LLM代理共同运营的中国律师事务所")所示：'
- en: '![Refer to caption](img/06c6c5f3a16806a7ef8fe5e797e85c9b.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/06c6c5f3a16806a7ef8fe5e797e85c9b.png)'
- en: 'Figure 11: Prompt template to guide the moonshot-v1-8k model in generating
    legal judgment dialogues'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：用于指导moonshot-v1-8k模型生成法律判断对话的提示模板
- en: Judicial Interpretation dialogue
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 司法解释对话
- en: We obtained 1,000 judicial interpretations from the official website of the
    Supreme People’s Court of China, covering various laws and categories. Utilizing
    the official API of Moonshot AI, we employed the moonshot-v1-128k large language
    model to generate and refine 4,382 high-quality judicial interpretation dialogues
    based on these interpretations, with human experts conducting the screening. To
    ensure the diversity and professionalism of the question-answer data, multiple
    sets of question-answer dialogues were constructed for each judicial interpretation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从中国最高人民法院官方网站获取了1,000条司法解释，涵盖了各种法律和类别。利用Moonshot AI的官方API，我们使用moonshot-v1-128k大语言模型基于这些解释生成和优化了4,382条高质量的司法解释对话，由人工专家进行筛选。为了确保问答数据的多样性和专业性，为每个司法解释构建了多组问答对话。
- en: Scenario-based Q&A Dialogue with Legal Grounds
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于场景的问答对话带有法律依据
- en: The scenario-based Q&A data with legal references was generated by (Zhou et al.
    [2024](#bib.bib31)) using ChatGPT, based on the most essential 9,000 legal provisions
    from the Chinese Legal Handbook. The original dataset comprised 92,000 dialogue
    samples. We invited human experts to manually screen these samples, resulting
    in a final collection of 3,026 high-quality scenario-based Q&A pairs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 基于场景的问答数据带有法律参考，由（周等人 [2024](#bib.bib31)）利用ChatGPT生成，基于《中国法律手册》中的最核心的9,000条法律条款。原始数据集包含92,000个对话样本。我们邀请了人工专家对这些样本进行人工筛选，最终收集了3,026对高质量的基于场景的问答对。
- en: Single-round Legal Consultation Dialogue
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单轮法律咨询对话
- en: The single single-round legal consultation dialogues are derived from real legal
    consultation questions and relevant legal provisions collected by (Huang et al.
    [2023](#bib.bib5)). These inputs were processed through GPT-4 to generate single-turn
    legal consultation dialogues, resulting in a total of 1,000 dialogue samples.
    We invited human experts to further refine these dialogues, ultimately yielding
    995 high-quality dialogue samples.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 单轮法律咨询对话来源于（黄等人 [2023](#bib.bib5)）收集的真实法律咨询问题和相关法律条款。这些输入经过GPT-4处理，生成了单轮法律咨询对话，共计1,000个对话样本。我们邀请了人工专家进一步优化这些对话，最终得到了995个高质量对话样本。
- en: Judicial Examination Dialogue
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 司法考试对话
- en: The judicial examination dialogue data was constructed by (Huang et al., 2023)
    based on the Chinese judicial examination dataset JEC-QA. They input the questions
    and answers from JEC-QA into ChatGPT, allowing the model to generate detailed
    explanations for each answer, ultimately resulting in 1,000 dialogue samples.
    We invited human experts to screen these dialogue data, and 985 high-quality dialogue
    samples were obtained.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 法律考试对话数据由（黄等，2023）基于中国司法考试数据集JEC-QA构建。他们将JEC-QA中的问题和答案输入ChatGPT，允许模型为每个答案生成详细解释，最终得到1,000个对话样本。我们邀请了人工专家对这些对话数据进行筛选，获得了985个高质量对话样本。
- en: B. Sample Consulting Report
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B. 咨询报告样本
- en: 'Figure [12](#A1.F12 "Figure 12 ‣ B. Sample Consulting Report ‣ Appendix A Appendix
    ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents") is a sample consultation report,
    serving as a demonstration for ICL. This consultation report comprises nine sections:
    report number, consultation date, client, subject of consultation, purpose of
    consultation, facts and background, legal analysis, legal advice, and risk warnings.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '图[12](#A1.F12 "图12 ‣ B. 咨询报告样本 ‣ 附录A 附录 ‣ LawLuo: 由LLM代理共同经营的中国律师事务所") 是一个咨询报告样本，作为ICL的示范。该咨询报告包含九个部分：报告编号、咨询日期、客户、咨询主题、咨询目的、事实与背景、法律分析、法律建议和风险警示。'
- en: '![Refer to caption](img/25a4859de7757fa92c8e1b0f553c83d5.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/25a4859de7757fa92c8e1b0f553c83d5.png)'
- en: 'Figure 12: A sample consulting report for in-context learning'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：一个关于情境学习的咨询报告样本
- en: C. Prompt Template for Guiding GPT-4o in Scoring
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C. 指导GPT-4o评分的提示模板
- en: 'Figure [13](#A1.F13 "Figure 13 ‣ C. Prompt Template for Guiding GPT-4o in Scoring
    ‣ Appendix A Appendix ‣ LawLuo: A Chinese Law Firm Co-run by LLM Agents") is a
    prompt template for guiding GPT-4o in scoring.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '图[13](#A1.F13 "图13 ‣ C. 指导GPT-4o评分的提示模板 ‣ 附录A 附录 ‣ LawLuo: 由LLM代理共同经营的中国律师事务所")
    是一个指导GPT-4o评分的提示模板。'
- en: '![Refer to caption](img/7f69efb8a364ebc49330a3240aee310d.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7f69efb8a364ebc49330a3240aee310d.png)'
- en: 'Figure 13: Prompt template for guiding GPT-4o in scoring'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：指导GPT-4o评分的提示模板
- en: D. Case Study of ToLC
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D. ToLC案例研究
- en: '| w/ ToLD | 首先，收集对方出轨的证据，这可能包括通信记录、照片或证人证言等。接着，对你们的共同财务状况进行彻底审计，明确债务的性质、金额和责任分配。尝试与对方协商离婚条件，包括财产分割和债务处理，如果协商失败，准备必要的法律文件，如离婚申请书和财产清单，并向法院提交离婚申请。如果对方不同意离婚，法院可能会介入调解，调解失败则进入正式的法庭程序。在法庭审理中，法官会根据法律规定和你们的具体情况，决定财产和债务的分割方式。一旦法院作出离婚判决，遵守判决内容，处理财产分割和债务偿还等后续事宜。在整个过程中，保持冷静和理智，避免情绪化决策，并确保在专业法律人士的帮助下保护自己的权益。
    (First, gather evidence of the partner’s infidelity, which may include communication
    records, photos, or witness testimonies. Then, conduct a thorough audit of your
    joint financial situation to clarify the nature, amount, and allocation of debts.
    Attempt to negotiate divorce terms with the partner, including property division
    and debt handling. If negotiations fail, prepare the necessary legal documents,
    such as the divorce petition and property inventory, and file for divorce in court.
    If the partner disagrees with the divorce, the court may intervene for mediation.
    If mediation fails, the case will proceed to a formal court hearing. During the
    trial, the judge will decide the division of property and debts based on legal
    provisions and your specific circumstances. Once the court issues a divorce decree,
    comply with the decree’s contents and handle subsequent matters such as property
    division and debt repayment. Throughout the process, remain calm and rational,
    avoid emotional decisions, and ensure the protection of your rights with the help
    of professional legal experts.) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| w/ ToLD | 首先，收集对方出轨的证据，这可能包括通信记录、照片或证人证言等。接着，对你们的共同财务状况进行彻底审计，明确债务的性质、金额和责任分配。尝试与对方协商离婚条件，包括财产分割和债务处理。如果协商失败，准备必要的法律文件，如离婚申请书和财产清单，并向法院提交离婚申请。如果对方不同意离婚，法院可能会介入调解，调解失败则进入正式的法庭程序。在法庭审理中，法官会根据法律规定和你们的具体情况，决定财产和债务的分割方式。一旦法院作出离婚判决，遵守判决内容，处理财产分割和债务偿还等后续事宜。在整个过程中，保持冷静和理智，避免情绪化决策，并确保在专业法律人士的帮助下保护自己的权益。'
- en: '| w/o ToLD | 离婚是一个重要的决定，涉及到情感、法律和财务等多个方面。以下是一些基本步骤，但请注意，具体情况可能因地区和个人情况而异，最好咨询专业的法律顾问：沟通与协商：首先尝试与配偶进行沟通，看是否能够就离婚达成共识，包括财产分割、子女抚养等问题。咨询律师：在正式提出离婚前，咨询专业的离婚律师，了解你所在地区的具体法律规定和程序。准备文件：根据律师的建议，准备必要的文件，如离婚协议书、财产清单等。提交申请：向法院或相关部门提交离婚申请，并按照要求提供相应的证据和文件。等待审理：法院将对离婚申请进行审理，可能包括调解程序。财产分割和子女抚养：根据法院的判决或双方的协议，进行财产分割和确定子女的抚养权。离婚证明：一旦离婚判决生效，你将获得离婚证明。后续调整：离婚后可能需要进行一些后续的调整，如更改个人文件、处理财务事务等。(Getting
    a divorce is an important decision involving multiple aspects such as emotions,
    legalities, and finances. The following are some basic steps, but please note
    that the specifics may vary depending on the region and individual circumstances.
    It is best to consult a professional legal advisor: 1\. Communication and negotiation:
    First, try to communicate with your spouse to see if a consensus on divorce can
    be reached, including issues like property division and child custody. 2\. Consult
    a lawyer: Before formally filing for divorce, consult a professional divorce lawyer
    to understand the specific legal regulations and procedures in your area. 3\.
    Prepare documents: According to the lawyer’s advice, prepare the necessary documents,
    such as a divorce agreement and a property list. 4\. Submit the application: Submit
    the divorce application to the court or relevant department, and provide the required
    evidence and documents as requested. 5\. Wait for the hearing: The court will
    review the divorce application, which may include mediation procedures. 6\. Property
    division and child custody: Based on the court’s judgment or the agreement between
    the parties, proceed with the property division and determine child custody. 7\.
    Divorce certificate: Once the divorce decree takes effect, you will receive a
    divorce certificate. 8\. Post-divorce adjustments: After the divorce, some follow-up
    adjustments may be needed, such as updating personal documents and handling financial
    matters.) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| w/o ToLD | 离婚是一个重要的决定，涉及到情感、法律和财务等多个方面。以下是一些基本步骤，但请注意，具体情况可能因地区和个人情况而异，最好咨询专业的法律顾问：沟通与协商：首先尝试与配偶进行沟通，看是否能够就离婚达成共识，包括财产分割、子女抚养等问题。咨询律师：在正式提出离婚前，咨询专业的离婚律师，了解你所在地区的具体法律规定和程序。准备文件：根据律师的建议，准备必要的文件，如离婚协议书、财产清单等。提交申请：向法院或相关部门提交离婚申请，并按照要求提供相应的证据和文件。等待审理：法院将对离婚申请进行审理，可能包括调解程序。财产分割和子女抚养：根据法院的判决或双方的协议，进行财产分割和确定子女的抚养权。离婚证明：一旦离婚判决生效，你将获得离婚证明。后续调整：离婚后可能需要进行一些后续的调整，如更改个人文件、处理财务事务等。'
- en: 'Table 2: The first row presents the answer generated by LawLuo with (w/) ToLD,
    while the second row presents the answer generated by LawLuo without (w/o) ToLD.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：第一行展示了 LawLuo 使用 (w/) ToLD 生成的回答，而第二行展示了 LawLuo 不使用 (w/o) ToLD 生成的回答。
