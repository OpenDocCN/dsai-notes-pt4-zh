<!--yml
category: 未分类
date: 2025-01-11 12:31:11
-->

# Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents

> 来源：[https://arxiv.org/html/2406.12806/](https://arxiv.org/html/2406.12806/)

Zehao Wang, Dong Jae Kim, Tse-Husn (Peter) Chen
Software PErformance, Analysis and Reliability (SPEAR) Lab
Concordia University, Montreal, Canada
w_zeha@encs.concordia.ca, k_dongja@encs.concordia.ca, peterc@encs.concordia.ca

###### Abstract.

Configuration settings are essential for tailoring software behavior to meet specific performance requirements. However, incorrect configurations are widespread, and identifying those that impact system performance is challenging due to the vast number and complexity of possible settings. In this work, we present PerfSense, a lightweight framework that leverages Large Language Models (LLMs) to efficiently identify performance-sensitive configurations with minimal overhead. PerfSense employs LLM agents to simulate interactions between developers and performance engineers using advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG). Our evaluation of seven open-source Java systems demonstrates that PerfSense achieves an average accuracy of 64.77% in classifying performance-sensitive configurations, outperforming both our LLM baseline (50.36%) and the previous state-of-the-art method (61.75%). Notably, our prompt chaining technique improves recall by 10% to 30% while maintaining similar precision levels. Additionally, a manual analysis of 362 misclassifications reveals common issues, including LLMs’ misunderstandings of requirements (26.8%). In summary, PerfSense significantly reduces manual effort in classifying performance-sensitive configurations and offers valuable insights for future LLM-based code analysis research.

Large language model, Configuration, Performance, Multi-Agent

## 1\. Introduction

Modern software systems feature numerous configuration options, enabling customization for diverse workloads and hardware platforms (Singh et al., [2016](https://arxiv.org/html/2406.12806v1#bib.bib44); Bao et al., [2018](https://arxiv.org/html/2406.12806v1#bib.bib3)). While these configurations provide flexibility, some configurations, known as performance-sensitive configurations, can impact system performance when their values change. Developers need to identify and understand the impact of such configurations to ensure they are set correctly, maintaining system performance and behavior. However, due to the large volume of configurations, pinpointing performance-sensitive configurations is time-consuming (Jin et al., [2012](https://arxiv.org/html/2406.12806v1#bib.bib21); Han and Yu, [2016a](https://arxiv.org/html/2406.12806v1#bib.bib17)) and incorrect settings are a common source of system misbehavior and performance degradation (Ganapathi et al., [2004](https://arxiv.org/html/2406.12806v1#bib.bib14); community, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib10)). Hence, automated approaches to quickly find performance-sensitive configurations that require special attention or further investigation are important to alleviate developers burden (Yonkovit, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib57); Tian et al., [2015](https://arxiv.org/html/2406.12806v1#bib.bib45)).

Performance experts have various tools at their disposal to assess performance-sensitive configurations. Alongside performance profiling tools (ej technologies, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib13); visualvm, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib50); Bornholt and Torlak, [2018](https://arxiv.org/html/2406.12806v1#bib.bib4)), they can identify inefficient code patterns (Chen et al., [2014](https://arxiv.org/html/2406.12806v1#bib.bib7); Liu et al., [2014](https://arxiv.org/html/2406.12806v1#bib.bib30); Nistor et al., [2015](https://arxiv.org/html/2406.12806v1#bib.bib35)), and utilize data-flow and dynamic analysis to find performance-sensitive configurations (Li et al., [2020](https://arxiv.org/html/2406.12806v1#bib.bib23); Lillack et al., [2014](https://arxiv.org/html/2406.12806v1#bib.bib27)). However, as highlighted by  Velez et al. ([2022a](https://arxiv.org/html/2406.12806v1#bib.bib48)), the adoption of these tools faces usability challenges for performance experts when analyzing the performance impact of configurations. These challenges arise from (1) a lack of comprehensive understanding of the codebase and its intricate interactions across multiple components, (2) difficulties in identifying the code affected by performance-sensitive configurations, and (3) the intricate cause-and-effect relationship between performance-sensitive configurations and the corresponding source code. Consequently, performance engineers may face challenges in accurately identifying the performance sensitivity of configurations. Effective collaboration between developers and performance engineers is crucial for overcoming these challenges and effectively identifying performance-sensitive configurations. Developers possess in-depth knowledge about the codebase and its functionality, while performance engineers specialize in analyzing performance-related issues. Leveraging their complementary expertise enables more thorough code analysis and more accurate classification of performance-sensitive configurations.

The rise of Large Language Models (LLMs) is revolutionizing programming and software engineering. Trained on vast code datasets, LLMs understand code deeply and excel in various code-related tasks. With tools like ChatGPT (OpenAI, [2023](https://arxiv.org/html/2406.12806v1#bib.bib36)) and LLaMA (Touvron et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib46)), researchers showcase LLMs’ potential in tasks like generating commit messages (Zhang et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib60)), resolving merge conflicts (Shen et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib43)), creating tests (Xie et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib53); Yuan et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib58); Schäfer et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib40)), renaming methods (AlOmar et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib2)), and aiding in log analytics (Ma et al., [2024b](https://arxiv.org/html/2406.12806v1#bib.bib32), [a](https://arxiv.org/html/2406.12806v1#bib.bib33)). Given the complexity of collaboration during software engineering tasks, using LLM agents stands out as a promising direction to replicate human workflows. Specifically, multi-agent systems have achieved significant progress in solving complex tasks by assigning agents to specific roles and emulating collaborative activities in software engineering practice (Hong et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib19); Dong et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib12); Qian et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib38)). For example, Dong et al. ([2023](https://arxiv.org/html/2406.12806v1#bib.bib12)) developed a self-collaboration framework, assigning LLM agents to work as distinct experts for sub-tasks in software development. Qian et al. ([2023](https://arxiv.org/html/2406.12806v1#bib.bib38)) proposed an end-to-end framework for software development through self-communication among the agents.

Inspired by multi-agent, we introduce PerfSense, a lightweight framework designed to effectively classify performance-sensitive configurations using Large Language Models (LLMs) as multi-agent systems. PerfSense leverages the collaborative capabilities of LLMs to mimic the interactions between developers and performance engineers, enabling a thorough analysis of the performance sensitivity of configurations. PerfSense employs two primary agents: DevAgent and PerfAgent. DevAgent focuses on retrieving relevant source code and documentation related to the configurations and conducting performance-aware code reviews. PerfAgent, on the other hand, utilizes the insights from DevAgent to classify configurations based on their performance sensitivity. This collaboration is facilitated through advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG), which enhance the agents’ understanding and analytical capabilities.

To address the challenge of navigating a large codebase with limited LLM context size, PerfSense iteratively breaks down complex tasks into manageable subtasks. Specifically, PerfAgent iteratively communicates with DevAgent to gather and analyze relevant source code associated with the configurations under scrutiny. Through a series of prompt chains, PerfAgent refines its understanding by requesting specific details, clarifications, and performance-related insights from DevAgent. This iterative communication ensures that PerfAgent accumulates a comprehensive knowledge base without exceeding the context size limitations, enabling accurate classification of performance-sensitive configurations.

Our evaluation of seven open-source systems demonstrates that PerfSense achieves 64.77% accuracy in classifying performance-sensitive configurations, outperforming state-of-the-art technique (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) and our LLM baseline with an average accuracy of 61.75% and 50.36%, respectively. Compared to prior technique (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) that requires tens or hundreds of hours to collect performance data manually, PerfSense is lightweight and requires minimal human effort.

In summary, we make the following contributions:

*   •

    Our evaluation of seven open-source systems demonstrates that PerfSense achieves an average accuracy of 64.77%, surpassing the state-of-the-art approaches with an average accuracy of 61.75%.

*   •

    We proposed a new LLM-based code analysis technique that employs two primary agents, DevAgent and PerfAgent, to navigate large codebases with limited LLM context sizes through advanced prompting techniques such as prompt chaining and retrieval-augmented generation (RAG).

*   •

    We analyzed the effect of different prompting components that we implemented in PerfSense. We found that our prompt chaining technique significantly improves the recall (10% to 30% improvement) while maintaining a similar level of precision.

*   •

    We conducted a manual study of the 362 misclassified configurations, identifying key reasons for misclassification, including LLM’s misunderstanding of requirements (26.8%) and incorrect interpretation of performance impact (10.0%).

*   •

    We provided a discussion on the implications of our findings and highlight future direction on LLM-based code analysis.

In conclusion, by leveraging multi-agent collaboration and advanced prompting techniques, PerfSense provides an efficient technique for classifying performance-sensitive configuration, one of the most important first steps in understanding system performance. PerfSense also presents a novel code navigation approach that may inspire future LLM-based research on code analysis.

Paper Organization. Section [2](https://arxiv.org/html/2406.12806v1#S2 "2\. Background ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") provides the background of the problem and technique. Section [3](https://arxiv.org/html/2406.12806v1#S3 "3\. Related Work ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") discusses related work. Section [4](https://arxiv.org/html/2406.12806v1#S4 "4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") presents the details of PerfSense. Section [5](https://arxiv.org/html/2406.12806v1#S5 "5\. Evaluation ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") shows the evaluation results. Section [6](https://arxiv.org/html/2406.12806v1#S6 "6\. Discussion ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") discusses the findings. Section [7](https://arxiv.org/html/2406.12806v1#S7 "7\. Threats to Validity ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") discusses the threats to validity. Section [8](https://arxiv.org/html/2406.12806v1#S8 "8\. Conclusion ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") concludes the paper.

## 2\. Background

In this section, we first discuss the definition and importance of performance-sensitive configuration. Then, we provide background on large language models (LLM) agents and retrieval-augmented generation (RAG).

### 2.1\. Performance-Sensitive Configurations

Software systems often contain various configuration parameters to provide flexibility in deployment and execution (Singh et al., [2016](https://arxiv.org/html/2406.12806v1#bib.bib44); Bao et al., [2018](https://arxiv.org/html/2406.12806v1#bib.bib3)). Some configurations, known as performance-sensitive configurations, affect performance when their values change. For example, an application’s name is generally not performance-sensitive, whereas memory allocation settings can significantly impact performance (Yin et al., [2011](https://arxiv.org/html/2406.12806v1#bib.bib56); Chen et al., [2016](https://arxiv.org/html/2406.12806v1#bib.bib6)). Identifying these configurations is crucial, as their usage directly impacts system efficiency and stability. However, developers may not always be aware of the performance implications of configuration changes, leading to common misconfigurations, impacting overall system performance (Yin et al., [2011](https://arxiv.org/html/2406.12806v1#bib.bib56); Xu et al., [2013](https://arxiv.org/html/2406.12806v1#bib.bib54); Chen et al., [2016](https://arxiv.org/html/2406.12806v1#bib.bib6); Han and Yu, [2016b](https://arxiv.org/html/2406.12806v1#bib.bib18); Velez et al., [2022b](https://arxiv.org/html/2406.12806v1#bib.bib49)).

Determining which configurations are performance-sensitive is challenging, given the high number of configurations and complex interactions among various system components (Zhang et al., [2015](https://arxiv.org/html/2406.12806v1#bib.bib59)), the absence of transparent documentation or feedback concerning the performance implications of each setting (Yin et al., [2011](https://arxiv.org/html/2406.12806v1#bib.bib56)), and the complexity and time-intensive nature of performance testing (Yonkovit, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib57)). Performance engineers need to conduct load tests to evaluate the performance sensitivity and impacts of various configurations. These tests involve altering the values of configuration parameters and assessing their impacts on system performance (Zhang et al., [2015](https://arxiv.org/html/2406.12806v1#bib.bib59); Singh et al., [2016](https://arxiv.org/html/2406.12806v1#bib.bib44); Wang et al., [2021](https://arxiv.org/html/2406.12806v1#bib.bib52); Vitui and Chen, [2021](https://arxiv.org/html/2406.12806v1#bib.bib51)). Therefore, an important step that can reduce the testing cost is to only conduct such tests on performance-sensitive configurations.

While developers implement code functionality with the best coding standards in mind, they may not always adhere to best-performance engineering practices. In collaborative efforts, developers and performance engineers work together to identify performance-sensitive configurations. Performance engineers leverage domain-specific knowledge to design and implement performance tests that uncover configuration sensitivities. However, performance engineers need the assistance of developers who have an in-depth understanding of the codebase to navigate across multiple source code components. Hence, to narrow down performance-sensitive configurations that impact overall system performance, there must be synergy in sharing knowledge between developers and performance engineers.

### 2.2\. LLM-based Multi-agent Framework

Large language models (LLMs) are pre-trained using vast datasets comprising a wide range of texts, such as documentation and source code. The core of LLM agents consists of large language models (LLMs) designed to understand questions and generate human-like responses. These agents refine their responses based on feedback (Madaan et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib34)), use memory mechanisms to learn from historical experiences (Li et al., [2024b](https://arxiv.org/html/2406.12806v1#bib.bib25)), retrieve informative knowledge to improve prompting and generate better responses (Zhao et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib61)), and collaborate with other LLM agents to solve complex tasks in a multi-agent process (Guo et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib16)). By using prompting, agents can assume specific roles (e.g., developer or tester) and provide domain-specific responses (Deshpande et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib11)). In particular, a multi-agent system has been shown to improve the capabilities of individual LLM agents by enabling collaboration among agents, each with specialized abilities (Hong et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib19); Chan et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib5)). Multiple LLM agents can share domain expertise and make collective decisions. Effective communication patterns are crucial for optimizing the overall performance of a multi-agent framework, allowing them to tackle complex projects using a divide-and-conquer approach (Chen et al., [2023b](https://arxiv.org/html/2406.12806v1#bib.bib8)). Finally, with modern frameworks like LangChain (langchain, [2023](https://arxiv.org/html/2406.12806v1#bib.bib22)), one key characteristic of LLM agents is their ability to interact with external tools to perform tasks similarly to humans. For example, an LLM agent acting as a test engineer can generate test cases, use test automation tools to collect code coverage, and answer further queries based on the gathered information.

In this paper, we propose PerfSense, which leverages LLM agents to emulate the collaboration between developers and performance engineers. PerfSense analyzes the source code and classifies whether a configuration is performance-sensitive. PerfSense is zero-shot and unsupervised. It requires minimal input from developers and achieves better results than the state-of-the-art technique on classifying performance-sensitive configuration (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)).

## 3\. Related Work

In this section, we discuss existing research and literature on three topics: 1) Performance Analysis of Configuration; 2) Using LLMs to Analyze Configuration; and 3) Multi-Agent-Based Code Analysis.

### 3.1\. Performance Analysis of Configuration

Some previous research aims to analyze the performance of configuration to help developers understand the performance issue during the software configuration tuning. ConfigCrusher (Velez et al., [2020](https://arxiv.org/html/2406.12806v1#bib.bib47)) relies on static taint analysis to reveal the relationship between an option and the affected code regions, dynamically analyze the influence of configuration options on the regions’ performance, and build the performance-influence model through white-box performance analysis. DiagConfig (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) leverages static taint analysis to identify the dependencies between performance-related operations and options. Through manual performance experiments and labeling on training systems, they build a random forest model to classify the performance-sensitive configurations. Different from the above work, in our work, we employ the LLM agent alongside static code analysis, specifically the call graph analysis, to study the performance of configurations. Given LLMs’ promising performance in understanding code, call graph analysis for LLMs can provide more information and incur lower overhead compared to taint analysis. More importantly, our approach is zero-shot and reduces minimal human effort, which can help developers efficiently identical potential performance-sensitive configurations for further analysis.

### 3.2\. Using LLMs to Analyze Configuration

Recently, large language models have shown promising performance on various software engineer tasks, such as code generation and summarization. Much research leveraged LLMs for tasks related to software configuration.  Lian et al. ([2024](https://arxiv.org/html/2406.12806v1#bib.bib26)) proposed an LLM-based framework, Ciri, using few-shot learning and prompt engineering to validate the correctness of configuration files from the file level and parameter level. From the evaluation of real-world misconfigurations, comprising 64 configurations, and synthesized misconfigurations involving 1,582 parameters, Ciri achieves F1 scores of 0.79 and 0.65 at the file level and parameter level, respectively. Liu et al. ([2024b](https://arxiv.org/html/2406.12806v1#bib.bib31)) introduced the LLM-CompDroid framework, which employs LLMs alongside the bug resolution tool to address configuration compatibility issues in Android applications. Their framework surpasses the state-of-the-art (SOTA) methods by at least 9.8% and 10.4% in the Correct and Correct@k metrics, respectively, respectively. Shan et al. ([2024](https://arxiv.org/html/2406.12806v1#bib.bib42)) came up with the framework, LogConfigLocalizer, which leverages Large Language Models and logs to localize root-cause configuration properties, achieving a high average accuracy of 99.91%. Different from these works, our work explores the potential of LLMs to analyze the performance sensitivity of configurations, which can assist developers in reducing performance testing costs.

### 3.3\. Multi-Agent Based Code Analysis

Agent-based code analysis emphasizes the importance of defining roles and facilitating communication among multiple LLM agents. Some approaches incorporate external tools as agents. For example, Huang et al. ([2023](https://arxiv.org/html/2406.12806v1#bib.bib20)) introduced a test executor agent that employs a Python interpreter to provide test logs for LLMs. Similarly, Zhong et al. ([2024](https://arxiv.org/html/2406.12806v1#bib.bib62)) presented a debugger agent that uses a static analysis tool to construct control flow graphs, aiding LLMs in locating bugs. Other studies (Hong et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib19); Qian et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib38); Dong et al., [2023](https://arxiv.org/html/2406.12806v1#bib.bib12); Lin et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib28)) assigned LLMs to emulate diverse human roles, such as analysts, engineers, testers, project managers, and chief technology officers (CTOs). These approaches use software process models (e.g., Waterfall) for inter-role communication, varying the prompts and roles to enhance code generation. Our technique leverages similar multi-agent systems to classify performance-sensitive configurations. By integrating prompt chaining and retrieval-augmented generation (RAG), PerfSense enhances the collaborative capabilities of LLM agents, leading to a lightweight technique that addresses the challenge of limited LLM context size when analyzing a large codebase.

![Refer to caption](img/06c9e24b222b4fdfffa9a3506748d243.png)

Figure 1. Overview of PerfSense

## 4\. Design of PerfSense

In this section, we introduce PerfSense, a lightweight framework designed for identifying performance-sensitive configurations. We begin by discussing various LLM agents and their communication and conclude with a detailed running example.

Figure [1](https://arxiv.org/html/2406.12806v1#S3.F1 "Figure 1 ‣ 3.3\. Multi-Agent Based Code Analysis ‣ 3\. Related Work ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") illustrates the overview of PerfSense. To analyze the performance sensitivity of configurations, PerfSense comprises two different agents: the developer (DevAgent) and the performance expert (PerfAgent). At a high level, given a potential performance-sensitive configuration, PerfAgent utilizes iterative self-refinement and retrieval-augmented prompting techniques in a zero-shot setting, with the assistance of DevAgent, to iteratively build a knowledge base of the codebase and classify whether the configuration is performance-sensitive. In the following section, we elaborate on the roles of PerfAgent and DevAgent, and their communication pattern for determining performance-sensitive configurations.

### 4.1\. Agent Roles and Definition

![Refer to caption](img/e2149954921a07c9a5982d9fae2bf970.png)

Figure 2. An example of performance-sensitive configuration.

#### 4.1.1\. Developer Agent: Retrieving Configuration-Related Code

The main role of a DevAgent is to retrieve source code and conduct performance-aware code review, upon PerfAgent’s request, and respond with the result so that PerfAgent has the necessary information to make the classification decision. Initially, PerfAgent receives the potential performance-sensitive configuration to analyze. However, multiple methods across various classes may have some dependencies with the configuration parameter, making it difficult for PerfAgent to assess the configuration’s performance sensitivity accurately. Providing additional summaries of the configuration-related code, such as related source code and documentation, can help improve PerfAgent’s output (Ye et al., [2020](https://arxiv.org/html/2406.12806v1#bib.bib55)). Hence, PerfAgent relies on DevAgent, which utilizes two tools: (1) traditional program analysis to extract source code that may be associated with the configuration through inter-procedural call graphs, and (2) document retrieval to extract official documentation associated with the configuration. In addition to retrieving the code, PerfAgent may also rely on DevAgent to provide feedback on the specific source code, since our intuition is that the developer should have a better understanding of the functionality of the code. Thus, DevAgent conducts performance-aware code reviews on the source code methods requested by PerfAgent, as indicated in Figure [3](https://arxiv.org/html/2406.12806v1#S4.F3 "Figure 3 ‣ 4.1.1\. Developer Agent: Retrieving Configuration-Related Code ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents"). Below, we further discuss how we extract code context, official documentation, and the prompt design for DevAgent’s performance-aware code review.

Extracting Configuration-Related Code. We define configuration-related code as the caller source code that invokes a method that directly accesses the configuration. For example, as indicated in Figure [2](https://arxiv.org/html/2406.12806v1#S4.F2 "Figure 2 ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents"), the configuration under analysis is key_cache_size_in_mb, and its related source code is initKeyCache, which is the caller source code that accesses the configuration. To extract configuration-related source code, we first utilize static code analysis to extract the inter-procedural call graph (Gousios, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib15)). We first identify the method that directly accesses the configuration, then we traverse the graph to retrieve all the methods that have either a direct or indirect caller-callee relationship.

Extracting Configuration Documentation. The description of a configuration on the document may provide additional information that can help classify configurations. For example, in the studied system Batik, the documentation for the configuration called Width provides additional information that it is the “Output Image Width”, which may help the agents with the analysis. Therefore, we extract the configuration descriptions, if they are available, from the official project website. The description is passed to both DevAgent and PerfAgent as part of the prompts when analyzing the configuration.

DevAgent’s Performance Aware Code Review. Figure [3](https://arxiv.org/html/2406.12806v1#S4.F3 "Figure 3 ‣ 4.1.1\. Developer Agent: Retrieving Configuration-Related Code ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") shows our prompt design for performance-aware code review that DevAgent carries out. Firstly, we give personification to the DevAgent, describing its role and goals, such as “You are a developer. Your job is to conduct performance-aware code review.”. Consequently, we provide context about the (1) source code and (2) configuration description to the DevAgent. Finally, we ask DevAgent to output the following requirements: (i) summarize the functionality of the code, (ii) how many times such source code may be triggered (estimation based on the provided textual information), and (iii) whether the code may have an impact on memory or execution time. It is important to note that performance-aware code review does not determine whether a configuration is performance-sensitive; this task falls to PerfAgent. However, DevAgent should be aware of common performance issues within the code they write, such as excessive memory usage and frequency of invocation, which may help PerfAgent with the analysis.

<svg class="ltx_picture ltx_centering" height="1494.14" id="S4.F3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1494.14) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 1469.01)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Prompt Template for Performance-Aware Code Review</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject color="#000000" height="1442.35" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Role: You are a developer. Your job is to conduct performance-aware code reviews on the given configuration-related code and official documentation for configuration to output the performance impact code that you wrote. Configuration-related code: [⬇](data:text/plain;base64,cHJpdmF0ZSBzdGF0aWMgQXV0b1NhdmluZ0NhY2hlIGluaXRLZXlDYWNoZSgpKXsKICAgIC4uLgogICAgbG9uZyBrZXlDYWNoZUluTWVtb3J5Q2FwYWNpdHkgPSBEYXRhYmFzZURlc2NyaXB0b3IuZ2V0S2V5Q2FjaGVTaXplSW5NQigpICogMTAyNCAqIDEwMjQ7CiAgICBrYyA9IENhZmZlaW5lQ2FjaGUuY3JlYXRlKGtleUNhY2hlSW5NZW1vcnlDYXBhY2l0eSk7CiAgICAuLi4KfQ==) 1private  static  AutoSavingCache  initKeyCache()){ 2  ... 3  long  keyCacheInMemoryCapacity  =  DatabaseDescriptor.getKeyCacheSizeInMB()  *  1024  *  1024; 4  kc  =  CaffeineCache.create(keyCacheInMemoryCapacity); 5  ... 6} Configuration description: Configuration Documentation after summarization. AutoSavingCache: “Specify the way Cassandra allocates and manages memtable memory.” Requirement: You must output three things below: 1\. Understand the functionality of the configuration in the code. 2\. Investigate triggering frequency of configuration-related operations. 3\. Check the potential impact of configuration on the system.</foreignobject></g></g></svg>

Figure 3. DevAgent’s Performance-Aware Code Review.

#### 4.1.2\. Performance Expert Agent: Analyzing the Performance Sensitivity of Configuration

Given DevAgent’s feedback on a specific configuration-related operation, PerfAgent utilizes this feedback to classify performance-sensitive configurations. However, PerfAgent may require further clarification on the retrieved code. For example, as indicated in Figure [4](https://arxiv.org/html/2406.12806v1#S4.F4 "Figure 4 ‣ 4.1.2\. Performance Expert Agent: Analyzing the Performance Sensitivity of Configuration ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents"), it may reference other methods (e.g., create) about which PerfAgent may lack performance knowledge. Hence, PerfAgent may request additional information about these operations. In particular, we use the prompt template in Figure [4](https://arxiv.org/html/2406.12806v1#S4.F4 "Figure 4 ‣ 4.1.2\. Performance Expert Agent: Analyzing the Performance Sensitivity of Configuration ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents"), which starts by personifying PerfAgent with the introduction, “You are a performance expert… Check whether the provided configuration-related code is sufficient for performance analysis.” In the prompt, PerfAgent receives the source code, as well as feedback from DevAgent as indicated in the template from Figure [3](https://arxiv.org/html/2406.12806v1#S4.F3 "Figure 3 ‣ 4.1.1\. Developer Agent: Retrieving Configuration-Related Code ‣ 4.1\. Agent Roles and Definition ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents"). Based on this context, PerfSense instructs PerfAgent to pinpoint unclear or ambiguous methods crucial for accurate performance analysis. Upon identifying the code that needs further analysis, PerfAgent requests DevAgent to retrieve and analyze it. By retrieving and clarifying the code when needed, PerfAgent can explore configuration-related code information, ensuring that all necessary code information is retrieved while minimizing the tokens and not exceeding the size limitation.

<svg class="ltx_picture ltx_centering" height="1458.24" id="S4.F4.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,1458.24) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 1433.12)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Prompt Template for Code Understanding</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject color="#000000" height="1406.45" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Role: You are a performance expert. Your job is to analyze the performance of the configuration. Check whether the provided configuration-related code is sufficient for performance analysis. Configuration-related code: [⬇](data:text/plain;base64,cHJpdmF0ZSBzdGF0aWMgQXV0b1NhdmluZ0NhY2hlIGluaXRLZXlDYWNoZSgpKXsKICAgIC4uLgogICAgbG9uZyBrZXlDYWNoZUluTWVtb3J5Q2FwYWNpdHkgPSBEYXRhYmFzZURlc2NyaXB0b3IuZ2V0S2V5Q2FjaGVTaXplSW5NQigpICogMTAyNCAqIDEwMjQ7CiAgICBrYyA9IENhZmZlaW5lQ2FjaGUuY3JlYXRlKGtleUNhY2hlSW5NZW1vcnlDYXBhY2l0eSk7CiAgICAuLi4KfQ==) 1private  static  AutoSavingCache  initKeyCache()){ 2  ... 3  long  keyCacheInMemoryCapacity  =  DatabaseDescriptor.getKeyCacheSizeInMB()  *  1024  *  1024; 4  kc  =  CaffeineCache.create(keyCacheInMemoryCapacity); 5  ... 6} Code Context: Responses received from DevAgent. Requirement: If you need further code context to help understand the code, return the name of method name.</foreignobject></g></g></svg>

Figure 4. PerfAgent’s Prompt for Code Understanding.

### 4.2\. Multi-Agent Communications

Based on our definition of DevAgent and PerfAgent, below we discuss how the agents collaborate together to classify performance-sensitive configurations.

#### 4.2.1\. Prompt Chaining to Iteratively Build Code Understanding

One effective technique for enhancing the reliability and performance of LLMs is to use a prompting paradigm called prompt chaining. Prompt chaining refers to breaking a complex task into simpler subtasks, prompting the LLM with each subtask sequentially, and using its responses as inputs for subsequent prompts (promptingguide, [[n.d.]](https://arxiv.org/html/2406.12806v1#bib.bib37)). In our performance chaining analysis, our goal is to retrieve all the necessary code for PerfAgent to assess the configuration’s sensitivity to performance. To achieve this, PerfAgent iteratively instructs DevAgent to retrieve source code methods sequentially. The DevAgent fetches a single method based on PerfAgent’s requests (include the source code, DevAgent’s description of the code, and DevAgent’s performance-aware code review result) until a termination condition is met, indicating that PerfAgent has gathered sufficient code and no longer requires assistance from DevAgent. PerfSense includes a memory mechanism that saves the DevAgent feedback at the end of each iteration of source code retrieval. This saved feedback can then be used as a code example in the next iteration of prompt chaining, allowing PerfAgent to clarify unclear contexts and request additional source code methods if needed.

#### 4.2.2\. Retrieval Augmented Generation for Performance Classifier

Based on the result of prompt chaining in prior steps, PerfAgent sequentially builds a memory of the knowledge base, which allows PerfAgent to classify performance-sensitive configurations more accurately. More precisely, we use the prompt template in Figure [5](https://arxiv.org/html/2406.12806v1#S4.F5 "Figure 5 ‣ 4.2.2\. Retrieval Augmented Generation for Performance Classifier ‣ 4.2\. Multi-Agent Communications ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents"). Like prior templates, our RAG starts by personifying PerfAgent with the introduction, “You are a performance expert. Given feedback from DevAgent, your job is to perform performance analysis of configurations.” We then provide the retrieved context from DevAgent: (1) configuration-related code, (2) performance-aware code reviews, and (3) other code contexts to resolve clarity issues related to the configuration. Finally, we require PerfAgent to classify whether or not the configuration is performance-sensitive.

<svg class="ltx_picture ltx_centering" height="244.04" id="S4.F5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,244.04) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 205)"><foreignobject color="#FFFFFF" height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Prompt Template for Retrieval Augmented Generation for Performance-Sensitive Configuration Classifier</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject color="#000000" height="178.34" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Role: You are a performance expert. Your job is to analyze the performance of the configuration. You can check the provided code if code is clear and enough for performance analysis of configuration. Background knowledge about performance: Background information about performance-sensitive configuration and performance operations. RAG information: Access the memory, which includes the following retrieved configuration information: 1\. Configuration-related code 2\. Code understanding 3\. Analysis result of unclear context Requirement: Classify the configuration as performance sensitive or insensitive.</foreignobject></g></g></svg>

Figure 5. Prompt Template 3: Retrieval Augmented Generation for Performance Classifier

### 4.3\. Implementation and Experiment Settings

Environment. We use GPT 3.5 (version gpt-3.5-turbo-0125) as our underlying LLM due to its popularity and wide usage. We leverage the OpenAI APIs and the LangGraph library (langchain, [2023](https://arxiv.org/html/2406.12806v1#bib.bib22)) to implement the LLM agents for recursive code analysis and performance configuration classification. Temperature is a parameter in LLMs that ranges from 0 to 1\. A low temperature makes the results more deterministic, and a higher value makes the results more diverse. To ensure the generated outputs are more stable across runs, We set the temperature to 0.3, which is a relatively low value but it still allows some diversity in the output. We also repeat our experiments five times and report the average. Note that although we use GPT 3.5 as the underlying LLM, our approach is general and can be replaced with other LLMs.

Benchmark Datasets. Table [1](https://arxiv.org/html/2406.12806v1#S4.T1 "Table 1 ‣ 4.3\. Implementation and Experiment Settings ‣ 4\. Design of PerfSense ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") presents the studied systems in our experiment. These seven systems are real-world open-source Java applications that cover various domains, ranging from databases to rendering engines. The systems have various configurations, some of which are related to performance. Previous work (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) conducted manual performance testing and provided the ground truth about the performance-sensitive configurations for these seven systems. We leverage the ground truth provided by Chen et al. (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) with some adjustments based on manually examining the source code and official documents. The replication package is available online (SensitiveTeeth, [2024](https://arxiv.org/html/2406.12806v1#bib.bib41)).

Table 1. An overview of the systems, versions, the number of configurations, and the number of performance-sensitive configurations that we studied.

 | System | Domain | Version | Config. | Perf. |
|  |  |  |  | Config. |
| Cassandra | NoSQL Database | 4.0.5 | 133 | 76 |
| DConverter | image Density Converter | bdf1535 | 23 | 5 |
| Prevayler | Database | 2.6 | 12 | 8 |
| BATIK | SVG rasterizer | 1.14 | 21 | 8 |
| Catena | Password hashing | 1281e4b | 12 | 6 |
| Sunflow | Rendering engine | 0.07.2 | 6 | 4 |
| H2 | Database | 2.1.210 | 20 | 11 | 

## 5\. Evaluation

In this section, we evaluate PerfSense by answering three research questions (RQs).

### RQ1: How effective is PerfSense in identifying performance-sensitive configurations?

In this RQ, we evaluate the classification result of PerfSense in identifying the performance-sensitive configuration. We compare PerfSense with two baselines: DiagConfig and ChatGPT. DiagConfig (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) utilized the taint static analysis on several systems to extract the performance-related operations related to configurations. Through manual performance tests by altering the configuration values and evaluating the variation of throughput, the performance-sensitive configurations would be identified and labeled. Utilizing the labeled configurations and taint static analysis of configuration, DiagConfig is trained using a random forest model to classify performance-sensitive configurations. ChatGPT directly calls ChatGPT APIs to classify if a configuration is performance-sensitive. We provide the system name, the configuration name, and the definition of a performance-sensitive configuration to ChatGPT (the same version as PerfSense) for classification. It is important to note that for PerfSense, the system name is not provided to reduce potential data leakage issues (Sallou et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib39)).

The classification result of PerfSense is assessed using three accuracy metrics: accuracy, precision, and recall. Specifically, we focus on the precision and recall of classifying the performance-sensitive configurations. True Positives (TP) happen when a performance-sensitive configuration is correctly classified. True Negatives (TN) happens when a non-performance-sensitive configuration is correctly classified as not performance-sensitive. False Positives (FP) happen when PerfSense incorrectly classifies a non-performance-sensitive configuration as performance-sensitive. False negatives (FN) happen when PerfSense misclassifies a performance-sensitive configuration as non-performance-sensitive. Given the TP, FP, and FN, we calculate precision as $\frac{TP}{TP+FP}$ and recall as $\frac{TP}{TP+FN}$. Finally, accuracy measures the overall correctness of the classification of performance-sensitivity of configurations and is calculated as $\frac{TP+TN}{TP+TN+FP+FN}$.

Because of the generative nature of LLMs, the output may vary in each execution. Hence, we repeat each experiment five times and report the average precision, recall, and accuracy.

Table 2. The accuracy, precision, and recall of PerfSense and the baselines in classifying performance-sensitive configurations. Note that DiagConfig’s results are unavailable for all the systems, except Cassandra, because DiagConfig trained a classifier using other systems and applied it on Cassandra.

 |  | PerfSense | ChatGPT | DiaConfig |
|  | Accuracy | Precision | Recall | Accuracy | precision | recall | Accuracy | precision | recall |
| Cassandra | 64.01% | 64.46% | 82.32% | 56.99% | 57.08% | 99.74% | 61.75% | 87.88% | 38.26% |
| DConverter | 66.09% | 39.06% | 100.00% | 26.96% | 20.79% | 84.00% | – | – | – |
| Prevayler | 75.00% | 75.51% | 95.50% | 66.70% | 66.70% | 100.00% | – | – | – |
| BATIK | 72.38% | 63.41% | 65.00% | 34.29% | 35.35% | 87.50% | – | – | – |
| Catena | 46.67% | 48.15% | 86.67% | 50.00% | 50.00% | 83.00% | – | – | – |
| Sunflow | 53.30% | 61.54% | 80.00% | 66.70% | 66.70% | 100.00% | – | – | – |
| H2 | 76% | 78.18% | 78.18% | 50.91% | 50.46% | 100.00% | – | – | – |
| Average | 64.77% | 61.47% | 83.95% | 50.36% | 49.58% | 93.46% | – | – | – | 

Results. PerfSense achieves a better accuracy (64.77% on average) compared to ChatGPT and DiagConfig (50.36% and 61.75%, respectively). Table [2](https://arxiv.org/html/2406.12806v1#S5.T2 "Table 2 ‣ RQ1: How effective is PerfSense in identifying performance-sensitive configurations? ‣ 5\. Evaluation ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") shows the classification result of PerfSense and the baselines. We find that the PerfSense provides a better balance of precision and recall, achieving better accuracy than the two baselines. ChatGPT achieves a higher recall (93.46%) than both PerfSense (83.95%) and DiagConfig (38.26%) but with a much lower precision (49.58% v.s. 61.47% and 87.88%). We find that the reason for a high recall and low precision is that ChatGPT misclassifies most configurations as performance-sensitive. In systems with less performance-sensitive configurations, ChatGPT achieves much worse results. For example, in DConverter, since 80% of the configurations are not performance-sensitive, ChatGPT achieves only a 27% accuracy rate. In contrast, the agents and prompting techniques implemented in PerfSense help improve the balance between precision and recall, resulting in much higher accuracy. We find that DiagConfig achieves a relatively high precision of 87.88% in Cassandra (it uses a classification model trained using data from all other systems, so the results are only available for Cassandra). However, DiagConfig has a very low recall (38.26% compared to PerfSense’s 82.32%) because DiagConfig misses many configurations where there were issues with obtaining the taint analysis result.

Note that, in theory, we can adjust PerfSense’s precision/recall by asking LLMs to estimate the performance impact (e.g., severe, medium, or low) and only classify the ones with a severe impact as performance sensitive to improve precision. In our pilot study, we achieve a much higher precision of 72.41% but a lower recall of 27.63%. PerfSense only classifies the configurations with severe performance impact as the performance-sensitive configurations. However, in this work, we aim to achieve higher recall and maintain good precision because the goal of PerfSense is to provide an initial list of configurations for performance engineers efficiently for further investigation.

Compared to DiagConfig, PerfSense requires less running time and no manual effort. DiagConfig requires a taint analysis to identify all the code that is reachable from the configuration, which may require tens of hours of computation time for large systems like Cassandra. Moreover, DiagConfig built a random forest classification model by manually collecting test results from other systems. This manual-intensive process may need to be repeated if we want to apply the model to systems in other domains or if they are developed by a different development practice. In contrast, PerfSense’s running time is less than 50 minutes for Cassandra (the largest studied system with over 130 configuration parameters) and can be easily extended to any system because of its zero-shot and unsupervised nature.

<svg class="ltx_picture" height="63.28" id="S5.SSx1.p7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,63.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject color="#000000" height="45.51" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Answers to RQ1. PerfSense provides a better balance of precision and recall, achieving the highest accuracy compared to the baselines. PerfSense is also lightweight and requires less than one hour to run for the largest studied system.</foreignobject></g></g></svg>

### RQ2: How do different components in PerfSense affect the classification result?

PerfSense contains various components, including the retrieval augmented generation (e.g., retrieving related code to help make classification decisions) and chain-of-thought (e.g., asking agents to generate a code summary and combine the generated summary with subsequent tasks). In this RQ, we aim to study the impact of each component. We remove each component separately, re-execute PerfSense, and re-evaluate the classification accuracy. In particular, we consider five combinations: 1) code: retrieve only the source code method that directly uses the configuration value; 2) code + analysis: expands code by enabling the DevAgent to iteratively traverse the code to analyze the methods that the agent believes is relevant (i.e., through prompt chaining); 3) dev: the DevAgent generates a summary and description of the retrieved code; 4) code + dev: expands code by asking the DevAgent to provide a summary and description of the retrieved code for subsequent prompts (i.e., chain-of-thought); and 5) code + dev + analysis: the full version of PerfSense.

Table 3. Classification results of PerfSense with different components. The best precision and recall values in each system are marked in bold. The numbers in the parentheses show the percentage difference compared to the full version of PerfSense (code+dev+analysis).

 | System | Approach | Precision | Recall |
| Cassandra | PerfSense${}_{\text{code+dev+analysis}}$ | 64.46% | 82.32% |
| PerfSense${}_{\text{code}}$ | 67.55% (+4.79%) | 73.42% (-10.81%) |
| PerfSense${}_{\text{code+analysis}}$ | 61.81% (-4.11%) | 77.11% (-6.32%) |
| PerfSense${}_{\text{dev}}$ | 62.32% (-3.31%) | 74.47% (-9.53%)% |
| PerfSense${}_{\text{code+dev}}$ | 64.12% (-0.53%) % | 73.09% (-11.23%) |
| Dconverter | PerfSense${}_{\text{code+dev+analysis}}$ | 39.06% | 100.00% |
| PerfSense${}_{\text{code}}$ | 30.49% (-21.94%) | 100.00% (-0.00%) |
| PerfSense${}_{\text{code+analysis}}$ | 29.76% (-23.81%) | 100.00% (-0.00%) |
| PerfSense${}_{\text{dev}}$ | 32.20% (-17.56%) | 100.00% (-0.00%) |
| PerfSense${}_{\text{code+dev}}$ | 36.76% (-5.89%) | 100.00% (-0.00%) |
| Prevayler | PerfSense${}_{\text{code+dev+analysis}}$ | 75.51% | 92.50% |
| PerfSense${}_{\text{code}}$ | 80.56% (+6.70%) | 72.50% (-21.62%) |
| PerfSense${}_{\text{code+analysis}}$ | 74.47% (-1.38%) | 87.50% (-5.41%) |
| PerfSense${}_{\text{dev}}$ | 70.83% (-6.20%) | 85.00% (-8.11%) |
| PerfSense${}_{\text{code+dev}}$ | 70.45% (-6.70%) | 79.49% (-14.06%) |
| BATIK | PerfSense${}_{\text{code+dev+analysis}}$ | 63.41% | 65.00% |
| PerfSense${}_{\text{code}}$ | 42.86% (-32.40%) | 45.00% (-30.76%) |
| PerfSense${}_{\text{code+analysis}}$ | 47.92% (-24.42%) | 57.50% (-11.53%) |
| PerfSense${}_{\text{dev}}$ | 59.09% (-6.86%) | 32.50% (-50.00%) |
| PerfSense${}_{\text{code+dev}}$ | 51.28 (-19.12%) | 50.00% (-23.07%) |
| Catena | PerfSense${}_{\text{code+dev+analysis}}$ | 48.15% | 86.67% |
| PerfSense${}_{\text{code}}$ | 51.43% (+6.81%) | 60.0% (-30.77%) |
| PerfSense${}_{\text{code+analysis}}$ | 48.89% (+1.53%) | 73.33% (-15.39%) |
| PerfSense${}_{\text{dev}}$ | 48.15% (-0.00%) | 86.67% (-0.00%) |
| PerfSense${}_{\text{code+dev}}$ | 50.00% (+3.84%) | 76.67% (-11.53%) |
| Sunflow | PerfSense${}_{\text{code+dev+analysis}}$ | 61.54% | 80.00 % |
| PerfSense${}_{\text{code}}$ | 83.00% (+34.87%) | 50.00% (-37.50%) |
| PerfSense${}_{\text{code+analysis}}$ | 65.22% (+6.00%) | 75.00% (-6.25%) |
| PerfSense${}_{\text{dev}}$ | 68.00% (+10.49%) | 100% (+25.00%) |
| PerfSense${}_{\text{code+dev}}$ | 65.38% (+6.23%) | 89.47% (+11.83%) |
| H2 | PerfSense${}_{\text{code+dev+analysis}}$ | 78.18% | 78.18% |
| PerfSense${}_{\text{code}}$ | 58.70% (-24.91%) | 50.00% (-36.04%) |
| PerfSense${}_{\text{code+analysis}}$ | 66.04% (-15.52%) | 63.64% (-18.59%) |
| PerfSense${}_{\text{dev}}$ | 66.07% (-15.48%) | 67.27% (-13.95%) |
| PerfSense${}_{\text{code+dev}}$ | 80.00% (+2.33%) | 72.73% (-6.97%) | 

Results. Integrating code results in the highest precision in 4/7 studied systems, with the sacrifice of recall. Further adding dev improves recall significantly across most systems, achieving a more balanced trade-off between precision and recall while maintaining reasonably high precision. Table [3](https://arxiv.org/html/2406.12806v1#S5.T3 "Table 3 ‣ RQ2: How do different components in PerfSense affect the classification result? ‣ 5\. Evaluation ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") shows the precision and recall of the combinations of the components in PerfSense. We find that a simple RAG approach by retrieving only the method that directly uses the configuration parameter (PerfSense ${}_{\text{code}}$) can achieve good precision across all studied systems but a much lower recall compared to the full version of PerfSense (12.00% to 36.04% lower). For instance, in the case of Cassandra, PerfSense ${}_{\text{code}}$ achieves a precision of 67.55%, which is 4.79% higher than the full version, but the recall drops significantly by 10.81% to 73.42%.

While code alone can achieve high precision, it often sacrifices recall significantly. On the other hand, code+dev tends to provide a better balance between precision and recall, with generally higher recall rates and more stable precision. This suggests that incorporating LLM-generated summaries and descriptions helps to enhance the overall performance of the tool by maintaining a more comprehensive approach. In comparison, integrating the LLM-generated summary/description of a given piece of code in the prompt (code v.s. code+dev) tends to provide a better balance between precision and recall, with generally higher recall rates and more stable precision. This suggests that incorporating LLM-generated summaries/descriptions helps to enhance the overall performance of PerfSense by maintaining a more comprehensive approach.

Adding analysis to code further improves recall significantly across most systems while maintaining similar precision. For example, in Prevayler, the precision of code+analysis (74.47%) is slightly lower than code (80.56%), but the recall increases from 72.56% to 87.50%. Similarly, in BATIK, while code+analysis achieves a precision of 47.92% compared to 42.86% for code, the recall improves significantly from 45.00% to 57.50%. This finding suggests that the additional context and understanding provided by the analysis help PerfSense identify more relevant methods, thereby improving recall and maintaining precision.

Integrating code+dev+analysis offers a holistic approach that leverages the strengths of individual components—code, developer insights, and analytical context—to achieve the best balanced performance. For instance, in the case of Cassandra, code+dev+analysis achieves a precision of 64.46% and a recall of 82.32%. The full version of PerfSense surpasses the recall of both code (73.42%) and code+dev (73.09%), while maintaining a competitive precision. Our findings show that each component has its own benefits to the result, and the integrated components enhance the overall effectiveness and reliability of PerfSense, providing a robust solution for identifying relevant methods and classifying performance-sensitive configurations within codebases.

<svg class="ltx_picture" height="77.2" id="S5.SSx2.p6.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,77.2) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject color="#000000" height="59.42" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Answers to RQ2. Adding dev improves recall with a balanced trade-off in precision, and incorporating analysis further enhances recall while maintaining competitive precision. The combined code+dev+analysis approach effectively leverages each component’s strengths for comprehensive method identification.</foreignobject></g></g></svg>

### RQ3: What are the reasons for PerfSense’s misclassification?

Despite PerfSense achieving the highest accuracy and maintaining a balance between precision and recall, there remain instances of misclassification. Understanding the underlying causes of these misclassifications is crucial for understanding the limitations of PerfSense and providing insights for future performance analysis utilizing LLMs. Hence, in this RQ, we conduct a detailed manual study on the reasons for misclassification.

We collected and examined 362 configurations incorrectly classified by PerfSense. To systematically analyze the reasons for misclassifications by PerfSense, we began by selecting a 20% random sample from our dataset of 362 misclassified configurations. This subset was thoroughly reviewed to identify and categorize the various reasons for misclassification. In particular, we studied the communication history among the agents, the source code, and all related documents that we could find. With these categories established, we then manually examined the remaining 80% of the dataset, applying the derived categories to each configuration to understand the distribution of the different reasons for misclassification. We did not find any new categories during the process.

Table 4. Reasons and prevalence for the misclassification of performance sensitivity by PerfSense.

 | Reason | Description | Percentage |
| No clear evidence of performance sensitivity | Through an examination of the related code and a careful review of available information (e.g., code and documentation), there is no clear evidence to indicate the performance sensitivity of the configuration. | 54.1% |
| Misunderstood requirements | LLMs misunderstand the requirements for classification of performance-sensitive. | 26.8% |
| Incorrect interpretation on the impact of performance-related operation | LLMs incorrectly interpreted the impact of performance-related operations. This misinterpretation led to the misclassification of the performance sensitivity of the configurations. | 10.0% |
| Influenced by performance-related keywords | When LLMs classifies performance-sensitive configurations, keywords like “memory” and “scalability” can lead to misclassifications. These keywords are inherently associated with performance-related aspects, which may cause performance-insensitive configurations to be incorrectly identified as performance-sensitive. | 8.0% |
| Hallucination | LLMs generate information that is not based on actual facts or truths. | 1.1% | 

Results Table [4](https://arxiv.org/html/2406.12806v1#S5.T4 "Table 4 ‣ RQ3: What are the reasons for PerfSense’s misclassification? ‣ 5\. Evaluation ‣ Identifying Performance-Sensitive Configurations in Software Systems through Code Analysis with LLM Agents") shows the reasons for the misclassification of performance-sensitivity by PerfSense and their percentage. In total, we uncovered five reasons that cause the misclassification.

Most misclassifications (54.1%) occur in configurations without clear evidence to support the performance sensitivity. For the configuration where the classification results by PerfSense differ from the ground truth (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)), although we conducted a thorough examination of the related code and a careful review of available information (e.g., documentation and source code), there is no substantive evidence supporting the performance sensitivity of this configuration. For example, in Cassandra, the configuration column_index_cache_size_in_kb is not performance-sensitive (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)). However, the LLM agents responded that this configuration can impact the amount of memory used for holding index entries in memory, which can cause performance variations. Setting a higher value may improve performance by reducing disk reads for index entries while setting a lower value may result in more disk reads and potentially slower performance. Based on reading the source code, we believe the explanation of PerfSense’s decision is valid, but there is no available evidence to support the performance sensitivity of the configuration. This misinterpretation can lead to inaccurate classification of performance sensitivity, highlighting the need for providing better requirements to LLMs for code understanding.

PerfSense may misunderstand requirements on performance sensitivities and classify other aspects as performance sensitive (26.8%). PerfSense may do some interpolation and infer that some non-performance-sensitive operations as performance sensitive. For example, the configuration _prevalenceDirectory in Prevayler specifies the directory used for reading and writing files. The configuration is related to file storage and is not performance-sensitive. However, PerfSense incorrectly assumes that the location of these read-write operations impacts system performance, whereas the configuration primarily pertains to system storage rather than performance.

10.0% of the misclassifications are due to incorrect interpretations of performance impact. In some instances, PerfSense inaccurately assesses the performance impact of specific configurations that do not inherently influence system efficiency. For example, the configuration BACKGROUND_COLOR in Batik, which sets the background color, is performance-sensitive. The reason is different color settings can have an impact on the performance of graph rendering. However, PerfSense incorrectly classifies the code related to the configuration of the color as performance-insensitive.

8.0% misclassification is related to having performance-related keywords for a performance non-sensitive configuration. LLMs are trained using natural language texts so they are sensitive to keywords in the prompts. If there are performance-related keywords in the prompt, PerfSense is more likely to classify a configuration as performance-sensitive. For example, the configuration gc_log_threshold_in_ms’ is not performance-sensitive in the Cassandra project based on the ground truth (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)). Enabling or disabling the configuration does not affect the system execution time. However, the keyword “gc” (garbage collection) is often considered to be performance-related in many situations, PerfSense incorrectly classified the configuration as performance-sensitive. However, the configuration is related to logging during gc, and setting a lower/higher threshold does not have a noticeable impact on the performance.

Hallucination is rare but can still cause misclassification (1.1%). Hallucination in LLMs can lead to incorrect or misleading results (Liu et al., [2024a](https://arxiv.org/html/2406.12806v1#bib.bib29); Li et al., [2024a](https://arxiv.org/html/2406.12806v1#bib.bib24)). For example, the configuration hinted_handoff_enabled in Cassandra is considered performance-sensitive (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)). This configuration is to allow Cassandra to “continue performing the same number of writes even when the cluster is operating at reduced capacity”. However, due to hallucination, PerfSense erroneously states that this configuration is related to the name of applications, causing misclassification of the configuration.

<svg class="ltx_picture" height="96.49" id="S5.SSx3.p9.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,96.49) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 12.82 8.89)"><foreignobject color="#000000" height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="574.35">Answers to RQ3. PerfSense’s misclassifications of performance-sensitive configurations are primarily due to a lack of clear evidence supporting performance sensitivity (54.1%) and misunderstanding of requirements leading to incorrect classifications (26.8%). Addressing these issues may require better requirements specification and enhanced understanding by LLMs to improve classification accuracy.</foreignobject></g></g></svg>

## 6\. Discussion

Better requirements for analyzing the performance sensitivity of configuration are needed. During the reason analysis of the misclassification of configurations, we find that misunderstanding the requirement on performance sensitivities affects the precision in identifying the performance-sensitive configurations by PerfSense. The specificity and clarity of prompts used to interact with these agents can influence their ability to accurately identify performance-sensitive configurations. Incorporating more explicit performance-related criteria into the prompts can help reduce misclassifications by aligning the LLM’s analysis more closely with the actual performance impacts of the configurations. This adjustment could guide the LLM agents to distinguish between configurations that truly affect performance and those that do not, despite potentially misleading indicators such as performance-related keywords.

PerfSense efficiently narrows down the scope of investigation performance sensitivity of configurations. One of the strengths of PerfSense is its ability to efficiently narrow down the list of configurations that need deeper investigation. Performance sensitivity in software configurations is a complex domain where manual identification processes are time-consuming and prone to errors. By automating the identification process, PerfSense allows performance engineers and developers to focus their efforts and expertise on a refined subset of configurations, enhancing productivity and optimizing resource allocation.

Prompt chaining and RAG technique enhance PerfSense understanding and analytical capabilities of the LLM on performance analysis. tool leverages advanced prompting techniques, such as prompt chaining and retrieval-augmented generation, to improve the interaction dynamics between the developer and performance expert agents. Prompt chaining breaks down complex tasks into simpler, sequential queries that build upon each other, which helps in constructing a comprehensive performance analysis for each performance-sensitive decision. RAG integrates the configuration-related information from external sources and reduces the context size, ensuring that the analysis from LLMs is both relevant and deeply informed for the performance assessment of configurations. The integration of various prompting techniques enhances PerfSense’s ability to accurately identify performance-sensitive configurations with balanced precision and recall, minimizing the risk of overlooking critical performance nuances in software behavior.

## 7\. Threats to Validity

### 7.1\. Internal Validity

Due to the generative nature of LLM, the responses may change across runs. To mitigate the threat, we try to execute the LLMs five times and report the average for our evaluation. We set the temperature value to 0.3, which makes the result more consistent but still allows some diversity. We find that the results are similar across runs, which means the outputs are stable. However, future studies are needed to understand the impact of temperature on the results. Since LLMs are trained using open-source systems, there is the possibility of data leakage problems. To minimize the impact, we excluded system-specific information (e.g., system and package names) when classifying configuration performance sensitivity to mitigate data(Sallou et al., [2024](https://arxiv.org/html/2406.12806v1#bib.bib39)).

### 7.2\. External Validity

We conducted the study on open-source Java systems. Although we tried to choose matured and popular systems that are also used in prior studies, the results may not apply to systems implemented in other programming languages. Future research is needed to examine the results of other types of systems.

### 7.3\. Construct Validity

Classifying the performance sensitivity of a configuration parameter is a challenging task due to varying workloads (Vitui and Chen, [2021](https://arxiv.org/html/2406.12806v1#bib.bib51)). Hence, in this paper, we rely on the prior benchmark (Chen et al., [2023a](https://arxiv.org/html/2406.12806v1#bib.bib9)) and validate the result by an in-depth analysis and all the documents that we could find. To encourage replication and validation of our study, we made the dataset publicly online (SensitiveTeeth, [2024](https://arxiv.org/html/2406.12806v1#bib.bib41)).

## 8\. Conclusion

Configuration parameters are crucial for customizing software behavior, and some configurations can have a performance impact on systems. However, misconfigurations are common and can lead to significant performance degradation, making it essential to identify performance-sensitive configurations. In this paper, we introduced PerfSense, a novel framework leveraging LLM-based multi-agent systems to identify performance-sensitive configurations in software systems. By combining static code analysis and retrieval-augmented prompting techniques, PerfSense can identify performance-sensitive configurations with minimal manual work. Our evaluation of seven open-source systems demonstrated that PerfSense achieves a higher accuracy of 64.77% compared to existing the state-of-the-art method (61.75%). Furthermore, our evaluation of studying the effect of different prompting components revealed that the implementation of prompt chaining in PerfSense substantially enhances recall, with improvements ranging from 10% to 30%. To understand the limitations of PerfSense, we conduct a manual analysis of 362 misclassification configurations to analyze and summarize the reasons for the misclassification of performance sensitivity by PerfSense. LLM’s misunderstanding of requirements (26.8%) is the key reason for misclassification. Additionally, we provide a detailed discussion to offer insights for future research to enhance the robustness and accuracy of LLM-based configuration performance analysis.

## References

*   (1)
*   AlOmar et al. (2024) Eman Abdullah AlOmar, Anushkrishna Venkatakrishnan, Mohamed Wiem Mkaouer, Christian D Newman, and Ali Ouni. 2024. How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations. *arXiv preprint arXiv:2402.06013* (2024).
*   Bao et al. (2018) Liang Bao, Xin Liu, Ziheng Xu, and Baoyin Fang. 2018. Autoconfig: Automatic configuration tuning for distributed message systems. In *Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering*. 29–40.
*   Bornholt and Torlak (2018) James Bornholt and Emina Torlak. 2018. Finding code that explodes under symbolic evaluation. *Proceedings of the ACM on Programming Languages* 2, OOPSLA (2018), 1–26.
*   Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based evaluators through multi-agent debate. *arXiv preprint arXiv:2308.07201* (2023).
*   Chen et al. (2016) Tse-Hsun Chen, Weiyi Shang, Ahmed E. Hassan, Mohamed Nasser, and Parminder Flora. 2016. CacheOptimizer: helping developers configure caching frameworks for hibernate-based database-centric web applications. In *Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering* (Seattle, WA, USA) *(FSE 2016)*. 666–677.
*   Chen et al. (2014) Tse-Hsun Chen, Weiyi Shang, Zhen Ming Jiang, Ahmed E Hassan, Mohamed Nasser, and Parminder Flora. 2014. Detecting performance anti-patterns for applications developed using object-relational mapping. In *Proceedings of the 36th international conference on software engineering*. 1001–1012.
*   Chen et al. (2023b) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. 2023b. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. *arXiv preprint arXiv:2308.10848* (2023).
*   Chen et al. (2023a) Zhiming Chen, Pengfei Chen, Peipei Wang, Guangba Yu, Zilong He, and Genting Mai. 2023a. DiagConfig: Configuration Diagnosis of Performance Violations in Configurable Software Systems. In *Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering* *(ESEC/FSE 2023)*. 566–578.
*   community ([n.d.]) LinkedIn community. [n.d.]. *What are the most common factors that slow down web application performance?* [https://www.linkedin.com/advice/0/what-most-common-factors-slow-down-web-application-lcsme](https://www.linkedin.com/advice/0/what-most-common-factors-slow-down-web-application-lcsme)
*   Deshpande et al. (2023) Ameet Deshpande, Tanmay Rajpurohit, Karthik Narasimhan, and Ashwin Kalyan. 2023. Anthropomorphization of AI: opportunities and risks. *arXiv preprint arXiv:2305.14784* (2023).
*   Dong et al. (2023) Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023. Self-collaboration code generation via chatgpt. *arXiv preprint arXiv:2304.07590* (2023).
*   ej technologies ([n.d.]) ej technologies. [n.d.]. *Jprofiler*. [https://www.ej-technologies.com/products/jprofiler/overview.html](https://www.ej-technologies.com/products/jprofiler/overview.html)
*   Ganapathi et al. (2004) Archana Ganapathi, Yi-Min Wang, Ni Lao, and Ji-Rong Wen. 2004. Why pcs are fragile and what we can do about it: A study of windows registry problems. In *International Conference on Dependable Systems and Networks, 2004*. IEEE, 561–566.
*   Gousios ([n.d.]) Georgios Gousios. [n.d.]. *java-callgraph: Java Call Graph Utilities*. [https://github.com/gousiosg/java-callgraph](https://github.com/gousiosg/java-callgraph)
*   Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680* (2024).
*   Han and Yu (2016a) Xue Han and Tingting Yu. 2016a. An empirical study on performance bugs for highly configurable software systems. In *Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement*. 1–10.
*   Han and Yu (2016b) Xue Han and Tingting Yu. 2016b. An Empirical Study on Performance Bugs for Highly Configurable Software Systems. In *Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement* (Ciudad Real, Spain) *(ESEM ’16)*. Article 23, 10 pages.
*   Hong et al. (2023) Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. *arXiv preprint arXiv:2308.00352* (2023).
*   Huang et al. (2023) Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023. AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation. *arXiv preprint arXiv:2312.13010* (2023).
*   Jin et al. (2012) Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. 2012. Understanding and detecting real-world performance bugs. *ACM SIGPLAN Notices* 47, 6 (2012), 77–88.
*   langchain (2023) langchain. 2023. LangGraph. [https://python.langchain.com/v0.1/docs/langgraph/](https://python.langchain.com/v0.1/docs/langgraph/).
*   Li et al. (2020) Chi Li, Shu Wang, Henry Hoffmann, and Shan Lu. 2020. Statically inferring performance properties of software configurations. In *Proceedings of the Fifteenth European Conference on Computer Systems*. 1–16.
*   Li et al. (2024a) Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024a. The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. arXiv:cs.CL/2401.03205
*   Li et al. (2024b) Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, et al. 2024b. Personal llm agents: Insights and survey about the capability, efficiency and security. *arXiv preprint arXiv:2401.05459* (2024).
*   Lian et al. (2024) Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth Thakkar, Minjia Zhang, and Tianyin Xu. 2024. Configuration Validation with Large Language Models. arXiv:cs.SE/2310.09690
*   Lillack et al. (2014) Max Lillack, Christian Kästner, and Eric Bodden. 2014. Tracking load-time configuration options. In *Proceedings of the 29th ACM/IEEE international conference on Automated software engineering*. 445–456.
*   Lin et al. (2024) Feng Lin, Dong Jae Kim, et al. 2024. When llm-based code generation meets the software development process. *arXiv preprint arXiv:2403.15852* (2024).
*   Liu et al. (2024a) Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li Zhang, Zhongqi Li, and Yuchi Ma. 2024a. Exploring and Evaluating Hallucinations in LLM-Powered Code Generation. arXiv:cs.SE/2404.00971
*   Liu et al. (2014) Yepang Liu, Chang Xu, and Shing-Chi Cheung. 2014. Characterizing and detecting performance bugs for smartphone applications. In *Proceedings of the 36th international conference on software engineering*. 1013–1024.
*   Liu et al. (2024b) Zhijie Liu, Yutian Tang, Meiyun Li, Xin Jin, Yunfei Long, Liang Feng Zhang, and Xiapu Luo. 2024b. LLM-CompDroid: Repairing Configuration Compatibility Bugs in Android Apps with Pre-trained Large Language Models. arXiv:cs.SE/2402.15078
*   Ma et al. (2024b) Lipeng Ma, Weidong Yang, Bo Xu, Sihang Jiang, Ben Fei, Jiaqing Liang, Mingjie Zhou, and Yanghua Xiao. 2024b. KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding. In *Proceedings of the 46th IEEE/ACM International Conference on Software Engineering*. 1–13.
*   Ma et al. (2024a) Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Chen, and Shaowei Wang. 2024a. LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing. In *2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)*. IEEE Computer Society, 883–883.
*   Madaan et al. (2024) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. *Advances in Neural Information Processing Systems* 36 (2024).
*   Nistor et al. (2015) Adrian Nistor, Po-Chun Chang, Cosmin Radoi, and Shan Lu. 2015. Caramel: Detecting and fixing performance problems that have non-intrusive fixes. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering. *IEEE, 902ś912* (2015).
*   OpenAI (2023) OpenAI. 2023. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/).
*   promptingguide ([n.d.]) promptingguide. [n.d.]. *Prompt Chaining*. [https://www.promptingguide.ai/techniques/prompt_chaining](https://www.promptingguide.ai/techniques/prompt_chaining)
*   Qian et al. (2023) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. *arXiv preprint arXiv:2307.07924* (2023).
*   Sallou et al. (2024) June Sallou, Thomas Durieux, and Annibale Panichella. 2024. Breaking the Silence: the Threats of Using LLMs in Software Engineering. In *Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results* (, Lisbon, Portugal,) *(ICSE-NIER’24)*. Association for Computing Machinery, New York, NY, USA, 102–106. [https://doi.org/10.1145/3639476.3639764](https://doi.org/10.1145/3639476.3639764)
*   Schäfer et al. (2023) Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An empirical evaluation of using large language models for automated unit test generation. *IEEE Transactions on Software Engineering* (2023).
*   SensitiveTeeth (2024) SensitiveTeeth 2024. Replication Package for SensitiveTeeth. [https://github.com/anonymous334455/SensitiveTeeth](https://github.com/anonymous334455/SensitiveTeeth).
*   Shan et al. (2024) Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li, and Zibin Zheng. 2024. Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize Configuration Errors via Logs. arXiv:cs.SE/2404.00640
*   Shen et al. (2023) Chaochao Shen, Wenhua Yang, Minxue Pan, and Yu Zhou. 2023. Git Merge Conflict Resolution Leveraging Strategy Classification and LLM. In *2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)*. IEEE, 228–239.
*   Singh et al. (2016) Ravjot Singh, Cor-Paul Bezemer, Weiyi Shang, and Ahmed E Hassan. 2016. Optimizing the performance-related configurations of object-relational mapping frameworks using a multi-objective genetic algorithm. In *Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering*. 309–320.
*   Tian et al. (2015) Xinhui Tian, Rui Han, Lei Wang, Gang Lu, and Jianfeng Zhan. 2015. Latency critical big data computing in finance. *The Journal of Finance and Data Science* 1, 1 (2015), 33–41.
*   Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971* (2023).
*   Velez et al. (2020) Miguel Velez, Pooyan Jamshidi, Florian Sattler, Norbert Siegmund, Sven Apel, and Christian Kästner. 2020. ConfigCrusher: towards white-box performance analysis for configurable systems. *Automated Software Engg.* 27, 3–4 (dec 2020), 265–300. [https://doi.org/10.1007/s10515-020-00273-8](https://doi.org/10.1007/s10515-020-00273-8)
*   Velez et al. (2022a) Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven Apel, and Christian Kästner. 2022a. On debugging the performance of configurable software systems: Developer needs and tailored tool support. In *Proceedings of the 44th International Conference on Software Engineering*. 1571–1583.
*   Velez et al. (2022b) Miguel Velez, Pooyan Jamshidi, Norbert Siegmund, Sven Apel, and Christian Kästner. 2022b. On debugging the performance of configurable software systems: developer needs and tailored tool support. In *Proceedings of the 44th International Conference on Software Engineering* (Pittsburgh, Pennsylvania) *(ICSE ’22)*. 1571–1583.
*   visualvm ([n.d.]) visualvm. [n.d.]. *VisualVM*. [https://visualvm.github.io/](https://visualvm.github.io/)
*   Vitui and Chen (2021) Arthur Vitui and Tse-Hsun Chen. 2021. MLASP: Machine learning assisted capacity planning: An industrial experience report. *Empirical Software Engineering* 26, 5 (2021), 87.
*   Wang et al. (2021) Zehao Wang, Haoxiang Zhang, Tse-Hsun Chen, and Shaowei Wang. 2021. Would you like a quick peek? providing logging support to monitor data processing in big data applications. In *Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering*. 516–526.
*   Xie et al. (2023) Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, and Jianwei Yin. 2023. ChatUniTest: a ChatGPT-based automated unit test generation tool. *arXiv preprint arXiv:2305.04764* (2023).
*   Xu et al. (2013) Tianyin Xu, Jiaqi Zhang, Peng Huang, Jing Zheng, Tianwei Sheng, Ding Yuan, Yuanyuan Zhou, and Shankar Pasupathy. 2013. Do not blame users for misconfigurations. In *Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles* (Farminton, Pennsylvania) *(SOSP ’13)*. New York, NY, USA, 244–259.
*   Ye et al. (2020) Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, and Shikun Zhang. 2020. Leveraging code generation to improve code retrieval and summarization via dual learning. In *Proceedings of The Web Conference 2020*. 2309–2319.
*   Yin et al. (2011) Zuoning Yin, Xiao Ma, Jing Zheng, Yuanyuan Zhou, Lakshmi N Bairavasundaram, and Shankar Pasupathy. 2011. An empirical study on configuration errors in commercial and open source systems. In *Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles*. 159–172.
*   Yonkovit ([n.d.]) Matt Yonkovit. [n.d.]. *Netherlands ’will pay the price’ for blocking Turkish visit – Erdoğan*. [https://www.percona.com/blog/cost-not-properly-managing-databases/](https://www.percona.com/blog/cost-not-properly-managing-databases/)
*   Yuan et al. (2023) Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin Wang, Yixuan Chen, and Xin Peng. 2023. No more manual tests? evaluating and improving ChatGPT for unit test generation. *arXiv preprint arXiv:2305.04207* (2023).
*   Zhang et al. (2015) Yi Zhang, Jianmei Guo, Eric Blais, and Krzysztof Czarnecki. 2015. Performance prediction of configurable software systems by fourier learning (t). In *2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)*. IEEE, 365–373.
*   Zhang et al. (2024) Yuxia Zhang, Zhiqing Qiu, Klaas-Jan Stol, Wenhui Zhu, Jiaxin Zhu, Yingchen Tian, and Hui Liu. 2024. Automatic Commit Message Generation: A Critical Review and Directions for Future Work. *IEEE Transactions on Software Engineering* (2024).
*   Zhao et al. (2023) Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai Jiao, Xuan Long Do, Chengwei Qin, Bosheng Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al. 2023. Retrieving multimodal information for augmented generation: A survey. *arXiv preprint arXiv:2303.10868* (2023).
*   Zhong et al. (2024) Li Zhong, Zilong Wang, and Jingbo Shang. 2024. LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step. *arXiv preprint arXiv:2402.16906* (2024).