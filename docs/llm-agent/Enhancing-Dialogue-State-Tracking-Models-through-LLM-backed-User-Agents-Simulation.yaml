- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:46:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:46:26'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过LLM支持的用户-代理模拟增强对话状态跟踪模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13037](https://ar5iv.labs.arxiv.org/html/2405.13037)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.13037](https://ar5iv.labs.arxiv.org/html/2405.13037)
- en: Cheng Niu NewsBreak Xingguang Wang NewsBreak Xuxin Cheng NewsBreak Juntong Song
    NewsBreak Tong Zhang University of Illinois Urbana-Champaign
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Cheng Niu NewsBreak Xingguang Wang NewsBreak Xuxin Cheng NewsBreak Juntong Song
    NewsBreak Tong Zhang University of Illinois Urbana-Champaign
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state
    in the conversations and plays a pivotal role in developing task-oriented dialogue
    systems. However, obtaining the annotated data for the DST task is usually a costly
    endeavor. In this paper, we focus on employing LLMs to generate dialogue data
    to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used
    to simulate the user and agent interaction, generating thousands of dialogues
    annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed
    on the generated data and the real data for the DST prediction. Experimental results
    on two public DST benchmarks show that with the generated dialogue data, our model
    performs better than the baseline trained solely on real data. In addition, our
    approach is also capable of adapting to the dynamic demands in real-world scenarios,
    generating dialogues in new domains swiftly. After replacing dialogue segments
    in any domain with the corresponding generated ones, the model achieves comparable
    performance to the model trained on real data.¹¹1All the source code, models,
    and generated dialogue data will be released after review for reproducibility.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态跟踪（DST）旨在监控对话中不断变化的状态，并在开发任务导向的对话系统中发挥关键作用。然而，获取DST任务的标注数据通常是一项昂贵的工作。本文集中于利用LLMs生成对话数据，以降低对话收集和标注成本。具体而言，使用GPT-4模拟用户和代理的交互，生成数千个带有DST标签的对话。然后，对生成的数据和真实数据进行LLaMA
    2的两阶段微调，以进行DST预测。在两个公共DST基准测试上的实验结果显示，使用生成的对话数据，我们的模型表现优于仅基于真实数据训练的基线。此外，我们的方法还能够适应现实场景中的动态需求，迅速生成新领域的对话。在任何领域用相应的生成对话片段替换对话段后，模型的性能与基于真实数据训练的模型相当。¹¹1所有源代码、模型和生成的对话数据将在审核后发布以供重现。
- en: Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通过LLM支持的用户-代理模拟增强对话状态跟踪模型
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Dialogue state tracking (DST) is a critical component of task-oriented dialogue
    systems, serving to track users’ goals and system actions in the conversation
    and facilitate precise information handling for communicating with external APIs Henderson
    et al. ([2014](#bib.bib14)); Mrkšić et al. ([2017](#bib.bib30)); Zhang et al.
    ([2023](#bib.bib53)); Hudeček and Dušek ([2023](#bib.bib16)). DST usually takes
    the form of key-value pairs, where the keys are denoted as slots which are defined
    in the system schema, outlining the specific information that the system aims
    to track or extract during the whole conversation Ren et al. ([2018](#bib.bib34)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态跟踪（DST）是任务导向对话系统的关键组成部分，旨在跟踪用户目标和系统在对话中的动作，并为与外部API的通信提供精确的信息处理Henderson
    et al. ([2014](#bib.bib14)); Mrkšić et al. ([2017](#bib.bib30)); Zhang et al.
    ([2023](#bib.bib53)); Hudeček and Dušek ([2023](#bib.bib16))。DST通常呈现为键值对的形式，其中键被表示为在系统模式中定义的槽，概述了系统在整个对话过程中旨在跟踪或提取的特定信息Ren
    et al. ([2018](#bib.bib34))。
- en: The design of DST models could be broadly categorized into two main types, classification-based
    DST models and generation-based DST models. Classification-based models select
    slot values from a set of candidates Ma et al. ([2019](#bib.bib29)); Ye et al.
    ([2021](#bib.bib51)), assuming that the dialogue ontology is pre-defined and hence
    lacking generalization capability Chen et al. ([2020](#bib.bib5)); Wang et al.
    ([2022](#bib.bib45)). Generation-based models directly generate the slot values
    to handle unseen domains and values Gao et al. ([2019](#bib.bib10), [2020](#bib.bib9));
    Lin et al. ([2020](#bib.bib27)); Peng et al. ([2021](#bib.bib33)). Recently, Feng
    et al. ([2023](#bib.bib8)) proposes a new DST framework LDST based on LLaMA Touvron
    et al. ([2023a](#bib.bib41)). By using an instruction tuning method, LDST achieves
    performance on par with ChatGPT OpenAI ([2023](#bib.bib31)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DST模型的设计可以大致分为两种主要类型：基于分类的DST模型和基于生成的DST模型。基于分类的模型从一组候选值中选择槽值Ma et al. ([2019](#bib.bib29));
    Ye et al. ([2021](#bib.bib51))，假设对话本体是预定义的，因此缺乏泛化能力Chen et al. ([2020](#bib.bib5));
    Wang et al. ([2022](#bib.bib45))。基于生成的模型直接生成槽值以处理未见领域和值Gao et al. ([2019](#bib.bib10),
    [2020](#bib.bib9)); Lin et al. ([2020](#bib.bib27)); Peng et al. ([2021](#bib.bib33))。最近，Feng
    et al. ([2023](#bib.bib8))提出了一个基于LLaMA的新的DST框架LDSTTouvron et al. ([2023a](#bib.bib41))。通过使用指令调优方法，LDST的性能达到了与ChatGPT
    OpenAI ([2023](#bib.bib31))相当的水平。
- en: Despite DST showing promising results, a significant challenge is that the annotation
    of dialogues entails significant costs. Furthermore, the dynamic nature of real-world
    demands highlights the urgent need to quickly generate utterances for new domains.
    Compared to other types of NLP data, collecting authentic dialogue data is particularly
    challenging. This difficulty is partly due to the dialogues frequently containing
    personal or sensitive information, which complicates data collection and sharing
    efforts. In response to these challenges, and inspired by the recent advancements
    of large language models (LLMs) Touvron et al. ([2023b](#bib.bib42)); Significant-gravitas
    ([2023](#bib.bib37)); Jablonka et al. ([2023](#bib.bib17)); Shen et al. ([2023](#bib.bib35)),
    we explore the use of these models for generating annotated DST data for data
    augmentation. By leveraging LLM’s cross-domain generation capability, we aim to
    create synthetic dialogues that can serve as replacements for manually annotated
    data, significantly reducing both financial cost and time constraints.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DST显示出令人鼓舞的结果，但一个重大挑战是对话的标注涉及显著的成本。此外，现实世界需求的动态性质突显了快速生成新领域话语的紧迫性。与其他类型的NLP数据相比，收集真实对话数据特别具有挑战性。这种困难部分是由于对话中经常包含个人或敏感信息，这使得数据收集和共享工作变得复杂。为应对这些挑战，并受最近大型语言模型（LLMs）Touvron
    et al. ([2023b](#bib.bib42)); Significant-gravitas ([2023](#bib.bib37)); Jablonka
    et al. ([2023](#bib.bib17)); Shen et al. ([2023](#bib.bib35))的最新进展的启发，我们探索利用这些模型生成标注的DST数据以进行数据扩充。通过利用LLM的跨领域生成能力，我们旨在创建可以替代手动标注数据的合成对话，从而显著降低财务成本和时间限制。
- en: In this paper, we propose a LLM-backed User-Agents Simulation (LUAS) algorithm
    to enhance DST. The process begins with the LLM generating a user profile that
    details the individual’s preferences for various tasks. Following this initial
    step, the LLM is prompted to simulate a conversation between the user and the
    agent. In these simulations, the user simulator makes requests and seeks recommendations
    or assistance, while the agent responds by understanding the user’s needs, providing
    suggestions, and taking appropriate actions. Through iterative conversations between
    the user and agent, complemented by a slot extractor also prompted by the LLM,
    we generate a substantial corpus of labeled, multi-turn dialogue data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于LLM的用户代理模拟（LUAS）算法来增强DST。该过程从LLM生成一个用户档案开始，详细描述了个人对各种任务的偏好。在此初步步骤之后，LLM被提示模拟用户与代理之间的对话。在这些模拟中，用户模拟器提出请求并寻求建议或帮助，而代理则通过理解用户的需求、提供建议和采取适当的行动来响应。通过用户与代理之间的迭代对话，再加上LLM提示的槽提取器，我们生成了大量标注的多轮对话数据。
- en: To verify the effectiveness of our approach and the quality of the generated
    data, experiments are conducted on two public DST datasets, MultiWOZ 2.2 (Zang
    et al., [2020](#bib.bib52)) and MultiWOZ 2.4 (Ye et al., [2022](#bib.bib50)).
    Following  Touvron et al. ([2023b](#bib.bib42)), LLaMa 2 is finetuned with real
    data as a strong baseline. By using both the generated and the real data, finetuning
    LLaMa 2 can further improve the performance. Besides, by replacing dialogue segments
    of any domain with the generated data, the newly trained model achieves comparable
    performance to the model trained on the real data, which shows the capability
    of our method to meet the dynamic requirements of real-world scenarios, generating
    dialogues in new domains and preserving the promising performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们方法的有效性和生成数据的质量，我们在两个公共DST数据集MultiWOZ 2.2（Zang 等，[2020](#bib.bib52)）和MultiWOZ
    2.4（Ye 等，[2022](#bib.bib50)）上进行了实验。遵循Touvron 等（[2023b](#bib.bib42)），用真实数据微调LLaMa
    2作为强基线。通过使用生成数据和真实数据，微调LLaMa 2可以进一步提高性能。此外，通过用生成数据替换任何领域的对话片段，新训练的模型在性能上可与真实数据训练的模型相媲美，这显示了我们方法在满足实际场景动态需求、生成新领域对话并保持有前景的表现方面的能力。
- en: 'In summary, the contributions of our work can be categorized into four aspects:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们工作的贡献可以分为四个方面：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a new framework that harnesses the power of GPT-4 to generate new
    labeled dialogue data, effectively reducing dialogue data collection and annotation
    costs.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个新框架，利用GPT-4的力量生成新的标注对话数据，从而有效降低对话数据的收集和标注成本。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experiment results on two datasets show the positive impact of the generated
    data on performance.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个数据集上的实验结果显示生成数据对性能的积极影响。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our method can swiftly generate data in new domains while maintaining promising
    performance.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法能够快速生成新领域的数据，同时保持有前景的表现。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We believe that our approach holds promise for extension to other dialogue-related
    tasks.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们相信我们的方法在扩展到其他对话相关任务方面具有潜力。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Dialogue State Tracking
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 对话状态跟踪
- en: Dialogue state tracking is an essential yet challenging task in task-oriented
    dialogue systems (Mrkšić et al., [2017](#bib.bib30)). Recent DST models (Lee et al.,
    [2021](#bib.bib24); Zhu et al., [2022](#bib.bib59); Yang et al., [2023b](#bib.bib49);
    Su et al., [2023](#bib.bib38); Lesci et al., [2023](#bib.bib25)), leveraging the
    different architectures and mechanisms, have convincingly demonstrated promising
    performance on several datasets (Budzianowski et al., [2018](#bib.bib4); Eric
    et al., [2020](#bib.bib7); Zang et al., [2020](#bib.bib52); Han et al., [2021](#bib.bib11);
    Ye et al., [2022](#bib.bib50)). To ease the burden of dialogue collection and
    annotation, Wu et al. ([2019](#bib.bib47)); Zhou et al. ([2023](#bib.bib58)) use
    few-shot learning to transfer to adapt existing models to the new domains. Drawn
    by the recent achievement of LLMs, Feng et al. ([2023](#bib.bib8)) leverages Low-Rank
    Adaptation (LoRA) Hu et al. ([2022](#bib.bib15)) to fine-tune the foundation model,
    achieving the promising performance in DST. In this paper, we utilize GPT-4 to
    simulate user-agent conversations, and the obtained dialogue data significantly
    enhances DST.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态跟踪是任务导向对话系统中的一个关键但具有挑战性的任务（Mrkšić 等，[2017](#bib.bib30)）。最近的DST模型（Lee 等，[2021](#bib.bib24)；Zhu
    等，[2022](#bib.bib59)；Yang 等，[2023b](#bib.bib49)；Su 等，[2023](#bib.bib38)；Lesci
    等，[2023](#bib.bib25)），通过利用不同的架构和机制，已在多个数据集上展现了有前景的表现（Budzianowski 等，[2018](#bib.bib4)；Eric
    等，[2020](#bib.bib7)；Zang 等，[2020](#bib.bib52)；Han 等，[2021](#bib.bib11)；Ye 等，[2022](#bib.bib50)）。为了减轻对话收集和标注的负担，Wu
    等（[2019](#bib.bib47)）；Zhou 等（[2023](#bib.bib58)）使用少量样本学习将现有模型适应到新领域。受到LLMs近期成就的吸引，Feng
    等（[2023](#bib.bib8)）利用低秩适应（LoRA）Hu 等（[2022](#bib.bib15)）对基础模型进行微调，在DST中取得了有前景的表现。在本文中，我们利用GPT-4模拟用户-代理对话，获得的对话数据显著提升了DST效果。
- en: 2.2 Data Augmentation by LLMs
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 通过LLMs的数据增强
- en: '| DST: [history], [user_utterance] $\rightarrow$ [service], [slot_key], [slot_val]
    |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| DST: [历史], [用户发言] $\rightarrow$ [服务], [槽位键], [槽位值] |'
- en: '| You are a local guide online, primarily handling the local services like
    finding the user’s place (such as attraction, hotel, train, restaurant, or hospital),
    calling taxis, contacting the police, or other convenient services. Your service
    is efficient and of high quality, earning widespread praise from the local community.
    Given the conversion history, your task is to help find what the user is looking
    for based on the whole conversion. Please output the current_service based on
    the user’s last utterance. And also please output all service information that
    needs to be paid attention to from the whole conversion. Here are the “conversion
    history”: {[history]} and the “user’s lastest utterance”: {[user_utterance]}.
    The output should be JSON-formatted like “current_service”: {[service]}, “slots”:
    {“[service]”: {“[slot_key]”: {[slot_val]}}}. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个在线本地向导，主要处理本地服务，如寻找用户的地点（如景点、酒店、火车、餐馆或医院）、叫出租车、联系警察或其他便利服务。你的服务高效且质量高，赢得了当地社区的广泛赞誉。根据对话历史，你的任务是根据整个对话帮助找出用户所寻求的内容。请根据用户的最后一句话输出current_service。还请输出整个对话中需要注意的所有服务信息。这里是“对话历史”：{[history]}
    和 “用户的最新话语”：{[user_utterance]}。输出应为JSON格式，如“current_service”：{[service]}，“slots”：{“[service]”：{“[slot_key]”：{[slot_val]}}}。|'
- en: '| Please give your decision: |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 请给出你的决定：|'
- en: 'Table 1: Proposed prompts to guide LLaMA 2 to generate JSON-formatted dialogue
    state predictions.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：建议的提示以引导 LLaMA 2 生成 JSON 格式的对话状态预测。
- en: Data augmentation has shown remarkable effectiveness in various domains, including
    computer vision Krizhevsky et al. ([2012](#bib.bib22)); Shorten and Khoshgoftaar
    ([2019](#bib.bib36)), text classification Zhang et al. ([2015](#bib.bib55)); Wei
    and Zou ([2019](#bib.bib46)), and speech recognition Ko et al. ([2015](#bib.bib21));
    Park et al. ([2019](#bib.bib32)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在各种领域显示出了显著的效果，包括计算机视觉 Krizhevsky 等 ([2012](#bib.bib22)); Shorten 和 Khoshgoftaar
    ([2019](#bib.bib36)), 文本分类 Zhang 等 ([2015](#bib.bib55)); Wei 和 Zou ([2019](#bib.bib46)),
    和语音识别 Ko 等 ([2015](#bib.bib21)); Park 等 ([2019](#bib.bib32))。
- en: In recent years, with the increasing prominence of LLMs, an increasing number
    of studies have begun to utilize LLMs for data augmentation. Kaddour and Liu ([2024](#bib.bib18))
    discovers that fine-tuning teacher LLMs to annotate unlabeled instances and generate
    new data points can notably boost the performance of downstream models. Yang et al.
    ([2023a](#bib.bib48)) generates truthful and customized dialogues to reduce hallucation.
    Ulmer et al. ([2024](#bib.bib43)) compares the effectiveness of various filtering
    strategies for the generated dialogue quality and introduces new methods to benchmark
    finetuned dialogue system. But their work does not discuss the DST task. Li et al.
    ([2022](#bib.bib26)) presented GPT-3 backed user-agent simulation system and showed
    positive results on DST task when the real data size is extremely small. Unlike
    Li et al. ([2022](#bib.bib26)), we abstract the common intentions of users and
    agents, crafting intent-specific prompts to ensure that the simulation adheres
    to task-oriented logic. This scheme enables the simulation to operate within a
    zero-shot setup, enhancing our approach’s adaptability to new domains. Moreover,
    by implementing a two-stage fine-tuning process, our approach demonstrates superior
    performance compared to strong baselines, even when trained with the full size
    of real data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着 LLMs 的日益突出，越来越多的研究开始利用 LLMs 进行数据增强。Kaddour 和 Liu ([2024](#bib.bib18))
    发现微调教师 LLMs 来标注未标记的实例并生成新的数据点可以显著提升下游模型的性能。Yang 等 ([2023a](#bib.bib48)) 生成真实且定制的对话以减少幻觉。Ulmer
    等 ([2024](#bib.bib43)) 比较了生成对话质量的各种过滤策略的有效性，并引入了新的方法来基准化微调对话系统。但他们的工作没有讨论 DST
    任务。Li 等 ([2022](#bib.bib26)) 提出了基于 GPT-3 的用户代理模拟系统，并在实际数据量极少时在 DST 任务上取得了积极结果。与
    Li 等 ([2022](#bib.bib26)) 不同，我们抽象了用户和代理的共同意图，设计了特定意图的提示，以确保模拟遵循任务导向的逻辑。这一方案使得模拟能够在零样本设置下运行，增强了我们方法对新领域的适应性。此外，通过实施两阶段微调过程，我们的方法在与强基准进行比较时表现出优越的性能，即使在使用完整的真实数据进行训练时也是如此。
- en: 3 Method
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: In this section, we will begin with the basic problem definition ($\S\ref{Problem
    Definition}$).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从基本问题定义开始 ($\S\ref{Problem Definition}$)。
- en: 3.1 Problem Definition
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: 'A task-oriented dialogue involves a multi-turn conversation between a user
    $U$, which is defined as a collection of (slot, value) pairs:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以任务为导向的对话涉及用户 $U$ 之间的多轮对话，这些对话被定义为一组（槽位，值）对：
- en: '|  | $y_{t}=\{(s^{i}_{t},v^{i}_{t})\;&#124;\;C_{t}\;,\forall s^{i}\in\mathcal{S}\}$
    |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}=\{(s^{i}_{t},v^{i}_{t})\;&#124;\;C_{t}\;,\forall s^{i}\in\mathcal{S}\}$
    |  |'
- en: where $\mathcal{S}$ denotes the set of the possible slots predefined in an ontology
    or schema. Following previous work Wang et al. ([2023](#bib.bib44)), the final
    slot is represented as the concatenation of the corresponding task domain and
    original slot, e.g., “”. The slots associated with each domain could
    be either categorical with a set of candidate values (e.g.  = “True”
    / “False”), or non-categorical, where the value is a span in the dialogue context
    (e.g.  = “Alexander”). Note that if no information is provided in
    the dialogue regarding a specific slot, the associated value for that slot is
    set to “NONE”.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{S}$ 表示本体或模式中预定义的可能槽的集合。遵循之前的工作 Wang 等人 ([2023](#bib.bib44))，最终槽表示为相应任务领域和原始槽的串联，例如，“”。与每个领域相关的槽可以是具有候选值的类别型（例如
     = “True” / “False”），也可以是非类别型，其中值是对话上下文中的一个跨度（例如  =
    “Alexander”）。注意，如果对话中没有提供关于特定槽的信息，则该槽的关联值设置为 “NONE”。
- en: 3.2 Using LLaMA 2 to Predict Dialogue State
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 使用 LLaMA 2 预测对话状态
- en: '![Refer to caption](img/ed1b4001bcc4c178cd73d4d53d304a55.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed1b4001bcc4c178cd73d4d53d304a55.png)'
- en: 'Figure 1: The simulation process of our approach. The blue boxes are intentions
    for the user and the agent, the ‘[RECOM]’, ‘[EOF]’, and ‘[EOD]’ are control identifiers.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们方法的仿真过程。蓝色框表示用户和代理的意图，‘[RECOM]’、‘[EOF]’ 和 ‘[EOD]’ 是控制标识符。
- en: We employ full-parameter fine-tuning on LLaMA 2 to predict dialogue states and
    employ pre-designed prompts to guide the LLaMA 2 model in generating predictions
    formatted in JSON. As demonstrated in Table [1](#S2.T1 "Table 1 ‣ 2.2 Data Augmentation
    by LLMs ‣ 2 Related Work ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation"), dialogue history and the user’s latest utterance are
    fed into LLaMA 2, which then conducts the prediction of the entire conversation’s
    intents and slot values. Specifically, predicted intents must fall within a predefined
    set, and predicted slots must align with the designated slots for the respective
    intents. We implement a schema to prevent the generation model from producing
    incoherent outputs and to enhance the overall quality and reliability of the outputs
    of LLaMA 2\. The optimization is conducted through the utilization of cross-entropy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 LLaMA 2 进行全参数微调以预测对话状态，并使用预设计的提示引导 LLaMA 2 模型生成 JSON 格式的预测。如表 [1](#S2.T1
    "Table 1 ‣ 2.2 Data Augmentation by LLMs ‣ 2 Related Work ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") 所示，对话历史和用户的最新发言被输入到
    LLaMA 2 中，然后预测整个对话的意图和槽值。具体来说，预测的意图必须在预定义的集合中，而预测的槽必须与各自意图的指定槽对齐。我们实现了一个方案，以防止生成模型产生不连贯的输出，并提高
    LLaMA 2 输出的整体质量和可靠性。优化通过使用交叉熵进行。
- en: 3.3 User-Agent Dialogue Simulation backed by GPT-4
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于 GPT-4 的用户-代理对话仿真
- en: As illustrated in Figure [1](#S3.F1 "Figure 1 ‣ 3.2 Using LLaMA 2 to Predict
    Dialogue State ‣ 3 Method ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation"), the dialogue simulation framework based on GPT-4 involves
    a multi-stage approach for producing labeled multi-turn dialogue data. In this
    arrangement, GPT-4 prompts two simulators, including the user simulator and the
    agent simulator, to engage in conversations aimed at completing specific dialogue
    tasks. Concurrently, GPT-4 also prompts a slot extractor to identify and extract
    all relevant slots throughout the entire conversation simulation process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [1](#S3.F1 "Figure 1 ‣ 3.2 Using LLaMA 2 to Predict Dialogue State ‣ 3 Method
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation")
    所示，基于 GPT-4 的对话仿真框架涉及一个多阶段的方法，用于生成标记的多轮对话数据。在这种安排中，GPT-4 促使两个模拟器，包括用户模拟器和代理模拟器，进行对话以完成特定的对话任务。同时，GPT-4
    还促使一个槽提取器在整个对话仿真过程中识别和提取所有相关槽。
- en: The details of the simulation generation process are outlined below, with all
    the prompts included in the appendix for reference.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仿真生成过程的详细信息如下所述，所有提示包含在附录中供参考。
- en: 3.3.1 Simulation Process Overview
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 仿真过程概述
- en: Before initiating the dialogue, GPT-4 is prompted to create a user profile that
    outlines the individual’s preferences across various tasks such as travel, accommodations,
    dining, and more. Each preference includes specific details like budget, travel
    distance, and other criteria. Following this setup, the user simulator begins
    interacting with the agent, presenting its requests and seeking recommendations
    or assistance with bookings and purchases. The agent, in turn, is prompted to
    delve into the user’s needs, conduct searches for pertinent information, offer
    suggestions, and execute necessary actions. After each interaction, the user simulator
    evaluates how well their needs have been met, deciding whether to continue the
    conversation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始对话之前，GPT-4 会被提示创建一个用户档案，概述个人在旅行、住宿、餐饮等各种任务中的偏好。每个偏好包括预算、旅行距离和其他标准等具体细节。在这个设置之后，用户模拟器开始与代理互动，提出请求并寻求建议或预订和购买的帮助。代理则会被提示深入了解用户的需求，进行相关信息的搜索，提供建议并执行必要的行动。在每次互动后，用户模拟器会评估其需求是否得到了满足，并决定是否继续对话。
- en: 3.3.2 User/Agent Intentions
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 用户/代理意图
- en: To effectively navigate the simulators through interactive tasks, we encounter
    the challenge of encoding complex dialogue logic within a single prompt. This
    task is demanding for both the user and the agent simulator. To simplify, we abstract
    the common intentions of users and agents, and craft prompts specifically for
    each unique intention of the user or agent. The detailed prompts of different
    intentions are shown in Appendix [A](#A1 "Appendix A Prompts for Simulation ‣
    Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation").
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地通过互动任务导航模拟器，我们面临着在单个提示中编码复杂对话逻辑的挑战。这项任务对用户和代理模拟器都是具有挑战性的。为简化起见，我们将用户和代理的常见意图抽象化，并针对每个用户或代理的独特意图制作提示。不同意图的详细提示见附录
    [A](#A1 "附录 A 模拟提示 ‣ 通过 LLM 支持的用户-代理模拟提升对话状态跟踪模型")。
- en: 'The user intentions are listed below:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 用户意图列在下方：
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inform Requirement, the user informs their requirement to the agent.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通知需求，用户向代理通知他们的需求。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Update Requirement, the user may update their requirements if the search result
    does not meet their criteria.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新需求，用户可以在搜索结果不符合其标准时更新其需求。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ask for Recommendation, the user asks for a recommendation given a few candidates
    meeting their criteria.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求推荐，用户在给定满足其标准的几个候选者的情况下请求推荐。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inquire Properties, the user asks for some properties (e.g. address, etc.) of
    the candidates.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 询问属性，用户询问候选人的一些属性（例如地址等）。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ask for Action, the user requires action after receiving the recommendation
    (e.g. making a reservation, etc.).
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求行动，用户在收到推荐后需要采取行动（例如进行预订等）。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: General Chat, other scenarios in the simulation, e.g. greeting or showing gratitude.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一般聊天，模拟中的其他场景，例如问候或表示感谢。
- en: 'The agent intentions are listed below:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 代理意图列在下方：
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inquire, ask the user’s need and preference or seek the user’s approval or confirmation.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 询问，询问用户的需求和偏好或寻求用户的批准或确认。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Report Search Results, based on the user’s preference, search the database and
    then make inquiries, recommendations, or reservations.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告搜索结果，根据用户的偏好，搜索数据库，然后进行查询、推荐或预订。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recommendation, when more than one candidate meets users’ search criteria, select
    the top candidate to recommend.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐，当多个候选者符合用户的搜索标准时，选择最优候选者进行推荐。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Answer, answer the user’s inquiry about a recommendation from the agent.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回答，回答用户关于代理推荐的询问。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Report Action Result, take action per the user’s request and report the outcome
    of the action.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告行动结果，根据用户的请求采取行动并报告行动结果。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: General Chat, other scenarios in the simulation, e.g. greetings or asking if
    there are any additional requirements to be addressed.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一般聊天，模拟中的其他场景，例如问候或询问是否有其他需求需要处理。
- en: Besides natural language outputs, the simulators are also prompted to generate
    the control identifiers in the responses, signaling the intention of the response.
    Given the input intention signaled by the control identifiers, the user or agent
    is prompted to select a proper intention and generate responses accordingly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自然语言输出外，模拟器还被提示在响应中生成控制标识符，以标示响应的意图。根据控制标识符传达的输入意图，用户或代理被提示选择适当的意图并相应生成响应。
- en: 3.3.3 Simulation Details
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 模拟细节
- en: As described in Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Simulation Process Overview
    ‣ 3.3 User-Agent Dialogue Simulation backed by GPT-4 ‣ 3 Method ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation"), the simulation
    begins by generating user profiles, which initializes the user requests. Following
    this, based on input intent from the preceding round, the simulation selects a
    user or agent response intent and then uses the corresponding prompt for dialogue
    generation. This selection process is governed by the predetermined logic listed
    below. The Generate Chat intent refers to the expressions of greetings and gratitude
    that are triggered only at the beginning or end of a conversation and are skipped
    in the subsequent list.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[3.3.1节](#S3.SS3.SSS1 "3.3.1 模拟过程概述 ‣ 3.3 基于GPT-4的用户代理对话模拟 ‣ 3 方法 ‣ 通过LLM支持的用户代理模拟增强对话状态跟踪模型")所述，模拟开始时生成用户档案，初始化用户请求。随后，根据前一轮的输入意图，模拟选择用户或代理的响应意图，并使用相应的提示生成对话。这个选择过程由下面列出的预定逻辑控制。生成对话意图指的是在对话开始或结束时触发的问候和感谢表达，并在后续列表中跳过。
- en: The conversation can be initiated by either the user or the agent. The following
    describes the detailed mechanism that triggers the user’s intent.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对话可以由用户或代理启动。以下描述了触发用户意图的详细机制。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Conversation Starts: triggers user’s Inform Requirement intent. Using randomization,
    the user simulator is instructed to choose a task of interest along with several
    related preferences and then generate a corresponding request.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话开始：触发用户的通知需求意图。通过随机化，用户模拟器被指示选择一个感兴趣的任务及其相关偏好，然后生成相应的请求。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inquire from the agent: triggers user’s Inform Requirement intent, corresponding
    to the scenario to answer the follow-up questions from the agent.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理询问：触发用户的通知需求意图，对应于回答代理的后续问题的场景。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Report Search Result from the agent: if the user’s preference has not been
    fully expressed, the user’s Inform Requirement intent will be triggered. If no
    candidate meets the search criteria, this will trigger the user’s Update Requirement
    intent. Otherwise, the presence of a single candidate will initiate the user’s
    Ask for Action intent, while the discovery of multiple candidates will prompt
    the user’s Ask for Recommendation intent.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告搜索结果：如果用户的偏好没有完全表达，将触发用户的通知需求意图。如果没有候选项符合搜索条件，将触发用户的更新需求意图。否则，如果存在单一候选项，将触发用户的请求操作意图，而发现多个候选项将触发用户的请求建议意图。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recommendation from the agent: the user will be prompted to select from (i)
    Inquire Propertied intent for more information or (ii) Ask for Action intent to
    proceed to make transactions.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理的建议：用户将被提示从(i) 询问属性意图以获取更多信息，或(ii) 请求操作意图以继续进行交易中进行选择。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Report Action Result from the agent: if all the tasks in the user profile have
    been completed, General Chat between the user and agent will be triggered, and
    then the conversation terminates; Otherwise, Inform Requirement intent is triggered
    for a new task.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告操作结果：如果用户档案中的所有任务已完成，将触发用户与代理之间的一般聊天，然后对话终止；否则，将触发通知需求意图以开始新任务。
- en: Below is the intent-triggering mechanism for the agent simulation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代理模拟的意图触发机制。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inform Requirement from the user: the agent is prompted to check if all the
    required slot values have been collected. If not, Inquire intent will be triggered
    to generate follow-up questions; Otherwise, the agent will search based on the
    user’s requirement, and then generate a response based on Report Search Result
    intent.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自用户的需求通知：代理被提示检查是否已经收集了所有必要的槽值。如果没有，将触发询问意图以生成后续问题；否则，代理将根据用户的需求进行搜索，然后根据报告搜索结果意图生成响应。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inquire Properties from the user: triggers agent’s Answer intent.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 询问属性：触发代理的回答意图。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ask for Recommendation from the user: the agent is prompted to select the top
    candidate and then generate the response based on Recommendation intent.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户请求建议：代理被提示选择最佳候选项，并根据推荐意图生成响应。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ask for Action from the user: the agent is prompted to make transactions and
    then generate a response based on Report Action Result intent.'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户请求操作：代理被提示进行交易，然后根据报告操作结果意图生成响应。
- en: 3.3.4 Slot Extraction
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 槽值提取
- en: It’s important to note that the agent simulator must verify that all necessary
    information has been gathered before initiating a search. To manage this, a slot
    tracking module is employed to keep track of both the required and filled slots.
    With the Inform Requirement prompt, the user simulator can simultaneously provide
    dialogue utterances and the corresponding filled slot values. However, there is
    a possibility that the conversation generated by GPT-4 might not align with the
    outcomes of slot filling. This discrepancy can lead to repeated or even endless
    query loops from the agent. To address this issue, a slot extraction model, backed
    by GPT-4, is utilized to ensure that the generated conversation matches the slot-filling
    results. If inconsistencies are found, the conversation must be regenerated to
    maintain coherence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，代理模拟器必须在启动搜索之前验证是否已收集到所有必要的信息。为此，采用了一个插槽跟踪模块来跟踪所需和已填充的插槽。在“信息需求”提示下，用户模拟器可以同时提供对话话语和相应的已填充插槽值。然而，GPT-4生成的对话可能与插槽填充的结果不一致。这种不一致可能导致代理出现重复或甚至无尽的查询循环。为了解决这个问题，使用了一个由GPT-4支持的插槽提取模型，以确保生成的对话与插槽填充结果匹配。如果发现不一致，必须重新生成对话以保持一致性。
- en: 3.3.5 Generation Diversity
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 生成多样性
- en: To obtain a high-quality DST model, it is essential to have dialogue data that
    encompasses a wide range of diversity. To ensure the data generated possesses
    this diversity, we manually created ten rewriting templates, which were then expanded
    into hundreds of templates by GPT-4\. These rewriting templates serve as a post-processing
    tool to enhance the diversity of the user and agent responses.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得高质量的DST模型，必须拥有涵盖广泛多样性的对话数据。为了确保生成的数据具有这种多样性，我们手动创建了十个重写模板，然后由GPT-4扩展为数百个模板。这些重写模板作为后处理工具，用于增强用户和代理响应的多样性。
- en: The details about the rewriting templates and rewritten outputs are shown in
    Appendix [B](#A2 "Appendix B Templates for Booking Responses ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation").
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 关于重写模板和重写输出的详细信息见附录 [B](#A2 "附录 B 预订响应模板 ‣ 通过LLM支持的用户代理模拟来增强对话状态跟踪模型")。
- en: 3.4 Two-stage Fine-tuning Strategy
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 两阶段微调策略
- en: Taking into account the discrepancy in distribution between GPT-4 generated
    and real dialogues, directly merging generated and real data could cause the resulting
    model to deviate from the true distribution. To address this issue, we have designed
    a two-stage fine-tuning approach. Initially, we fine-tuned the LLaMA 2 model using
    the generated dialogue data. Following this, we continue to fine-tune the model
    with real data. The first step enables the model to learn fundamental task-oriented
    dialogue patterns. The second step ensures that the model effectively bridges
    the gap between generated and real dialogues, aligning closely with the true distribution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到GPT-4生成的对话和真实对话之间的分布差异，直接合并生成数据和真实数据可能导致最终模型偏离真实分布。为了解决这个问题，我们设计了一种两阶段微调方法。最初，我们使用生成的对话数据对LLaMA
    2模型进行了微调。随后，我们继续用真实数据对模型进行微调。第一步使模型能够学习基本的任务导向对话模式。第二步确保模型有效地弥合生成对话和真实对话之间的差距，与真实分布紧密对齐。
- en: 4 Experiments
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets and Metrics
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集与评估指标
- en: We conduct all the experiments on MultiWOZ 2.2²²2[https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2](https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2)
    Zang et al. ([2020](#bib.bib52)) and MultiWOZ 2.4³³3[https://github.com/smartyfh/MultiWOZ2.4](https://github.com/smartyfh/MultiWOZ2.4) Ye
    et al. ([2022](#bib.bib50)). MultiWOZ (Budzianowski et al., [2018](#bib.bib4))
    has been extensively utilized for evaluating the performance of DST, including
    8,438, 1,000, and 1,000 samples for training, dev, and test sets with multi-turn
    dialogues, which are collected by a Wizard-of-Oz (WOZ) setup and encompass a diverse
    array of domains. MultiWOZ 2.2 dataset refines the annotations in dev and test
    sets of MultiWOZ 2.1 (Eric et al., [2020](#bib.bib7)). MultiWOZ 2.4 Ye et al.
    ([2022](#bib.bib50)) is the latest refined version correcting all incorrect labels
    in dev and test sets. Following Wu et al. ([2019](#bib.bib47)), we remove the
    domains of ‘hospital’ and ‘police’ from both MultiWOZ2.2 and MultiWOZ2.4 datasets
    because they only appear a few times in the training set and never occur in the
    dev and test set. By using the MultiWOZ schema, nearly 8000 new dialogues are
    generated. The detailed statistics of MultiWOZ 2.2 and MultiWOZ 2.4 datasets and
    the generated dialogue data are demonstrated in Table [2](#S4.T2 "Table 2 ‣ 4.1
    Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation").
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 MultiWOZ 2.2²²2[https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2](https://github.com/budzianowski/multiwoz/tree/master/data/MultiWOZ_2.2)
    Zang 等 ([2020](#bib.bib52)) 和 MultiWOZ 2.4³³3[https://github.com/smartyfh/MultiWOZ2.4](https://github.com/smartyfh/MultiWOZ2.4) Ye
    等 ([2022](#bib.bib50)) 上进行所有实验。MultiWOZ (Budzianowski 等，[2018](#bib.bib4)) 已广泛用于评估
    DST 的性能，包括用于训练、开发和测试集的 8,438、1,000 和 1,000 个多轮对话样本，这些样本由 Wizard-of-Oz (WOZ) 设置收集，涵盖了各种领域。MultiWOZ
    2.2 数据集细化了 MultiWOZ 2.1 (Eric 等，[2020](#bib.bib7)) 的开发和测试集注释。MultiWOZ 2.4 Ye 等
    ([2022](#bib.bib50)) 是最新的精细化版本，纠正了开发和测试集中的所有错误标签。根据 Wu 等 ([2019](#bib.bib47))，我们从
    MultiWOZ2.2 和 MultiWOZ2.4 数据集中移除了“医院”和“警察”领域，因为它们仅在训练集中出现过几次，并且从未出现在开发和测试集中。通过使用
    MultiWOZ 模式，生成了近 8000 个新对话。表 [2](#S4.T2 "表 2 ‣ 4.1 数据集和指标 ‣ 4 实验 ‣ 通过 LLM 支持的用户代理模拟增强对话状态跟踪模型")
    展示了 MultiWOZ 2.2 和 MultiWOZ 2.4 数据集及生成对话数据的详细统计信息。
- en: We adopt Joint Goal Accuracy (JGA) as the evaluation metric, which is the primary
    metric for DST. JGA is defined as the proportion of dialogue turns in which all
    the key-values are correctly predicted.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用**联合目标准确度**（JGA）作为评估指标，这是 DST 的主要指标。JGA 定义为所有关键值被正确预测的对话转弯的比例。
- en: '| Metric $\downarrow$ | 2.2 | 2.4 | Generated |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 指标 $\downarrow$ | 2.2 | 2.4 | 生成的 |'
- en: '| No. of domains | 8 | 7 | 5 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 领域数量 | 8 | 7 | 5 |'
- en: '| No. of dialogues | 8,438 | 8,438 | 7,556 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 对话数量 | 8,438 | 8,438 | 7,556 |'
- en: '| Total no. of turns | 113,556 | 113,556 | 102,602 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 总转弯数 | 113,556 | 113,556 | 102,602 |'
- en: '| Avg. turns per dialogue | 13.46 | 13.46 | 13.57 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 每对话平均转弯数 | 13.46 | 13.46 | 13.57 |'
- en: '| Avg. tokens per turn | 13.13 | 13.38 | 17.01 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 每转弯平均标记数 | 13.13 | 13.38 | 17.01 |'
- en: '| No. of slots | 61 | 37 | 17 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 插槽数量 | 61 | 37 | 17 |'
- en: '| Have schema description | Yes | Yes | - |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 是否有模式描述 | 是 | 是 | - |'
- en: '| Unseen domains in test set | No | No | - |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 测试集中未见领域 | 否 | 否 | - |'
- en: 'Table 2: Statistics of MultiWOZ (2.2 and 2.4) and the generated dataset used
    for training in our experiments.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：MultiWOZ (2.2 和 2.4) 和用于我们实验的生成数据集的统计信息。
- en: '| Models | MultiWOZ 2.2 | MultiWOZ 2.4 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MultiWOZ 2.2 | MultiWOZ 2.4 |'
- en: '| TRADE | 45.40 | 55.05 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| TRADE | 45.40 | 55.05 |'
- en: '| UniLM | 54.25 | - |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| UniLM | 54.25 | - |'
- en: '| DS-DST | 51.70 | - |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| DS-DST | 51.70 | - |'
- en: '| TripPy | 53.50 | 64.75 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| TripPy | 53.50 | 64.75 |'
- en: '| AG-DST | 57.26 | - |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| AG-DST | 57.26 | - |'
- en: '| SDP-DST | 57.60 | - |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SDP-DST | 57.60 | - |'
- en: '| D3ST${}_{\text{Base}}$ | 56.10 | 72.10 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| D3ST${}_{\text{Base}}$ | 56.10 | 72.10 |'
- en: '| D3ST${}_{\text{Large}}$ | 54.20 | 70.80 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| D3ST${}_{\text{Large}}$ | 54.20 | 70.80 |'
- en: '| D3ST${}_{\text{XXL}}$ | 58.70 | 75.90 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| D3ST${}_{\text{XXL}}$ | 58.70 | 75.90 |'
- en: '| SPACE-3 | 57.50 | - |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| SPACE-3 | 57.50 | - |'
- en: '| MSP-L | 57.70 | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| MSP-L | 57.70 | - |'
- en: '| RefPyDST | - | 65.20 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| RefPyDST | - | 65.20 |'
- en: '| Diable | 56.48 | 70.46 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Diable | 56.48 | 70.46 |'
- en: '| DDSA | - | 75.58 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| DDSA | - | 75.58 |'
- en: '| SPLAT | 56.60 | - |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| SPLAT | 56.60 | - |'
- en: '| MoNET | - | 76.02 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| MoNET | - | 76.02 |'
- en: '| SSNet | 62.10 | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| SSNet | 62.10 | - |'
- en: '| TOATOD${}_{\text{Small}}$ | 61.92 | - |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| TOATOD${}_{\text{Small}}$ | 61.92 | - |'
- en: '| TOATOD${}_{\text{Base}}$ | 63.79 | - |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| TOATOD${}_{\text{Base}}$ | 63.79 | - |'
- en: '| LUAS${}_{\text{R}}$ | 65.42 | 77.20 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| LUAS${}_{\text{R}}$ | 65.42 | 77.20 |'
- en: '| LUAS${}_{\text{R+G}}$ | 66.25 | 78.20 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LUAS${}_{\text{R+G}}$ | 66.25 | 78.20 |'
- en: 'Table 3: Joint Goal Accuracy for DST results on MultiWOZ 2.2 and MultiWOZ 2.4
    dataset. ‘-’ denotes that the results are not reported in the original paper.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：MultiWOZ 2.2 和 MultiWOZ 2.4 数据集的 DST 结果联合目标准确率。‘-’ 表示原文中未报告结果。
- en: '| Metric $\downarrow$ | Attraction | Hotel | Restaurant | Taxi | Train |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 指标 $\downarrow$ | 吸引力 | 酒店 | 餐馆 | 出租车 | 火车 |'
- en: '| Replaced Dialogues | 2538 | 3235 | 3666 | 1397 | 2840 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 替换对话 | 2538 | 3235 | 3666 | 1397 | 2840 |'
- en: '| Replaced Turns | 13348 | 30402 | 25768 | 6662 | 33364 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 替换轮次 | 13348 | 30402 | 25768 | 6662 | 33364 |'
- en: '| Avg. replaced turns per dialogue | 5.26 | 9.40 | 7.03 | 4.77 | 11.75 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 每个对话的平均替换轮次 | 5.26 | 9.40 | 7.03 | 4.77 | 11.75 |'
- en: '| Avg. tokens per replaced turn | 15.57 | 15.54 | 15.33 | 18.28 | 16.44 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 每个替换轮次的平均标记 | 15.57 | 15.54 | 15.33 | 18.28 | 16.44 |'
- en: '| Avg. slots per replaced user turn | 1.38 | 2.75 | 2.54 | 1.37 | 2.90 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 每个替换用户轮次的平均槽位 | 1.38 | 2.75 | 2.54 | 1.37 | 2.90 |'
- en: 'Table 4: Substituting details for 5 domains of MultiWOZ 2.2.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：MultiWOZ 2.2 的 5 个领域替换详细信息。
- en: '| Replaced Domain | Impact | JGA ($\Delta$  ) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 替换领域 | 影响 | JGA ($\Delta$) |'
- en: '| Base | 0% | 65.42 | 95.47% | 93.25% | 94.35% |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 0% | 65.42 | 95.47% | 93.25% | 94.35% |'
- en: '| Attraction | 28.1% | 64.99 ($-$0.18%) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 吸引力 | 28.1% | 64.99 ($-$0.18%) |'
- en: '| Hotel | 42.1% | 64.28 ($-$0.34%) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 酒店 | 42.1% | 64.28 ($-$0.34%) |'
- en: '| Restaurant | 41.2% | 64.61 ($-$0.01%) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 餐馆 | 41.2% | 64.61 ($-$0.01%) |'
- en: '| Taxi | 9.1% | 65.22 ($-$0.10%) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 出租车 | 9.1% | 65.22 ($-$0.10%) |'
- en: '| Train | 38.4% | 64.23 ($-$0.24%) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 38.4% | 64.23 ($-$0.24%) |'
- en: '| Averaged | 31.20% | 64.67 ($-$0.17%) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 31.20% | 64.67 ($-$0.17%) |'
- en: 'Table 5: JGA for substituting real data with generated data on MultiWOZ 2.2
    dataset.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在 MultiWOZ 2.2 数据集中用生成数据替换真实数据的 JGA。
- en: '| Dataset | Real Data Size | JGA${}_{\text{R}}$ ) | Slot |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 真实数据大小 | JGA${}_{\text{R}}$ | 槽位 |'
- en: '| $\text{Precision}_{\text{R+G}}$ ) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Precision}_{\text{R+G}}$ |'
- en: '| MultiWOZ 2.2 | 1000 | 58.77 | 63.06 ($+$1.08%) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| MultiWOZ 2.2 | 1000 | 58.77 | 63.06 ($+$1.08%) |'
- en: '| 2000 | 62.66 | 64.43 ($+$0.41%) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 62.66 | 64.43 ($+$0.41%) |'
- en: '| 4000 | 64.01 | 65.84 ($+$0.22%) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 4000 | 64.01 | 65.84 ($+$0.22%) |'
- en: '| All | 65.42 | 66.25 ($+$0.22%) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 65.42 | 66.25 ($+$0.22%) |'
- en: '| MultiWOZ 2.4 | 1000 | 64.60 | 69.69 ($+$0.83%) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| MultiWOZ 2.4 | 1000 | 64.60 | 69.69 ($+$0.83%) |'
- en: '| 2000 | 72.15 | 75.58 ($+$0.52%) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 2000 | 72.15 | 75.58 ($+$0.52%) |'
- en: '| 4000 | 75.81 | 77.29 ($+$0.21%) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 4000 | 75.81 | 77.29 ($+$0.21%) |'
- en: '| All | 77.20 | 78.20 ($+$0.14%) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 77.20 | 78.20 ($+$0.14%) |'
- en: 'Table 6: JGA and Slot Performance for fine-tuning with different sizes of real
    data from MultiWOZ 2.2 and 2.4.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：针对 MultiWOZ 2.2 和 2.4 的不同大小真实数据的 JGA 和槽位性能。
- en: 4.2 Implementation Details
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实现细节
- en: The GPT-4 version used for simulation is gpt-4-1106-preview. As for the fine-tuning
    stage, 8 Nvidia A100 (80G) GPUs are utilized for supervised full-parameter tuning
    with pytorch’s FSDP framework (Zhao et al., [2023](#bib.bib57)). The base model
    is 7B version⁴⁴4[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    of LLaMA 2\. For each fine-tuning stage, the learning rate is set to 2e-5 with
    the cosine scheduler Loshchilov and Hutter ([2016](#bib.bib28)), and the batch
    size is set to 8 on each GPU. We utilize Adam optimizer Kingma and Ba ([2015](#bib.bib20))
    with $\beta_{1}$ = 0.999, and the warm-up ratio is set to 3%. Both fine-tuning
    stages last around two hours. For inference, vLLM⁵⁵5[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm) Kwon
    et al. ([2023](#bib.bib23)) is used.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 用于仿真的 GPT-4 版本是 gpt-4-1106-preview。至于微调阶段，使用了 8 个 Nvidia A100 (80G) GPU，通过 pytorch
    的 FSDP 框架（Zhao et al., [2023](#bib.bib57)）进行有监督的全参数调整。基础模型是 LLaMA 2 的 7B 版本⁴⁴4[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)。每个微调阶段的学习率设置为
    2e-5，使用了 Loshchilov 和 Hutter ([2016](#bib.bib28)) 的余弦调度器，批量大小在每个 GPU 上设置为 8。我们使用了
    Adam 优化器 Kingma 和 Ba ([2015](#bib.bib20))，$\beta_{1}$ = 0.999，预热比例设置为 3%。两个微调阶段均持续约两个小时。推理时使用了
    vLLM⁵⁵5[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)
    Kwon et al. ([2023](#bib.bib23))。
- en: 4.3 Baselines
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基线
- en: To assess the efficacy of the generated dialogue data, fine-tune LLaMA 2 solely
    using real data, referring to it as LUAS${}_{\text{R}}$, which serves as a strong
    baseline. We also conduct comparisons between our model and other strong baselines,
    including TRADE (Wu et al., [2019](#bib.bib47)), UniLM Dong et al. ([2019](#bib.bib6)),
    DS-DST Zhang et al. ([2020](#bib.bib54)), TripPy Heck et al. ([2020](#bib.bib13)),
    AG-DST Tian et al. ([2021](#bib.bib40)), SDP-DST Lee et al. ([2021](#bib.bib24)),
    D3ST Zhao et al. ([2022](#bib.bib56)), SPACE-3 He et al. ([2022](#bib.bib12)),
    MSP-L Sun et al. ([2022](#bib.bib39)), RefPyDST King and Flanigan ([2023](#bib.bib19)),
    Diable Lesci et al. ([2023](#bib.bib25)), DDSA Yang et al. ([2023b](#bib.bib49)),
    SPLAT Bebensee and Lee ([2023](#bib.bib3)), MoNET Zhang et al. ([2023](#bib.bib53)),
    SSNet Atawulla et al. ([2023](#bib.bib1)), TOATOD Bang et al. ([2023](#bib.bib2)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估生成对话数据的效果，仅使用真实数据对LLaMA 2进行微调，称之为LUAS${}_{\text{R}}$，作为强有力的基线。我们还对我们的模型与其他强基线进行了比较，包括TRADE
    (Wu et al., [2019](#bib.bib47))、UniLM Dong et al. ([2019](#bib.bib6))、DS-DST Zhang
    et al. ([2020](#bib.bib54))、TripPy Heck et al. ([2020](#bib.bib13))、AG-DST Tian
    et al. ([2021](#bib.bib40))、SDP-DST Lee et al. ([2021](#bib.bib24))、D3ST Zhao
    et al. ([2022](#bib.bib56))、SPACE-3 He et al. ([2022](#bib.bib12))、MSP-L Sun et
    al. ([2022](#bib.bib39))、RefPyDST King and Flanigan ([2023](#bib.bib19))、Diable
    Lesci et al. ([2023](#bib.bib25))、DDSA Yang et al. ([2023b](#bib.bib49))、SPLAT
    Bebensee and Lee ([2023](#bib.bib3))、MoNET Zhang et al. ([2023](#bib.bib53))、SSNet
    Atawulla et al. ([2023](#bib.bib1))、TOATOD Bang et al. ([2023](#bib.bib2))。
- en: 4.4 Results for DST prediction
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 DST预测结果
- en: 'The whole results are shown in Table [3](#S4.T3 "Table 3 ‣ 4.1 Datasets and
    Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation"), it needs to be pointed out that our model is primarily
    compared with the generation-based models, because classification-based models
    can utilize external knowledge, leading to unfair comparisons. LUAS${}_{\text{R}}$
    is fine-tuned on both real and generated data. From these results, we have the
    following observations:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 所有结果如表[3](#S4.T3 "Table 3 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation")所示，需要指出的是，我们的模型主要与生成模型进行比较，因为基于分类的模型可以利用外部知识，导致比较不公平。LUAS${}_{\text{R}}$在真实数据和生成数据上进行微调。从这些结果中，我们得到以下观察：
- en: (1) On both MultiWOZ 2.2 and 2.4 datasets, the performance of LLaMA 2 fine-tuned
    on real data (LUAS${}_{\text{R}}$) surpasses previous DST baselines. This outcome
    underscores the exceptional effectiveness of LLaMA 2.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 在MultiWOZ 2.2和2.4数据集上，LLaMA 2在真实数据（LUAS${}_{\text{R}}$）上进行微调后的性能超越了之前的DST基准。这一结果凸显了LLaMA
    2的卓越效果。
- en: (2) Furthermore, the incorporation of additional generated data yields significant
    performance improvements, with enhancements of 0.83% on MultiWOZ 2.2 and 1% on
    MultiWOZ 2.4\. This improvement emphasizes the important role of generated data
    in boosting overall model performance. As is shown in the next section, the gain
    from the generated data can be even larger in case the real dialogue data is of
    a smaller size. For example, the enhancement can be as large as from 4.29% to
    5.09% if only 1,000 dialogue real data exists. Considering the challenge in dialogue
    data collection, this result highlights the pragmatic significance of integrating
    generated data for DST development across domains.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 此外，加入额外生成数据可以显著提升性能，在MultiWOZ 2.2上提高了0.83%，在MultiWOZ 2.4上提高了1%。这一改进强调了生成数据在提升整体模型性能中的重要作用。如下一节所示，当真实对话数据规模较小的时候，生成数据的增益可能更大。例如，如果只有1,000条真实对话数据，提升幅度可以达到从4.29%到5.09%。考虑到对话数据收集的挑战，这一结果突显了在跨领域DST开发中整合生成数据的实际意义。
- en: 4.5 Results of Substituting Real Data with Generated Data
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 使用生成数据替代真实数据的结果
- en: In order to further validate the quality and effectiveness of the generated
    dialogue data, we conduct a data replacement experiment for different domains
    on MultiWOZ 2.2\. In these experiments, all dialogue data segments related to
    a specific domain will be removed, and the newly generated data will be inserted
    at the removed location. After replacement, the new training set will consist
    of 1 domain with the generated data and 4 others with real data. The replacement
    details are shown in Table [4](#S4.T4 "Table 4 ‣ 4.1 Datasets and Metrics ‣ 4
    Experiments ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents
    Simulation").
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步验证生成对话数据的质量和有效性，我们在MultiWOZ 2.2上进行了不同领域的数据替换实验。在这些实验中，所有与特定领域相关的对话数据段将被移除，新的生成数据将插入到移除的位置。替换后，新训练集将由1个领域的生成数据和4个领域的真实数据组成。替换的详细信息见表[4](#S4.T4
    "Table 4 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State
    Tracking Models through LLM-backed User-Agents Simulation")。
- en: The model is also trained on LLaMA 2 7B, the results are shown in Table [5](#S4.T5
    "Table 5 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State
    Tracking Models through LLM-backed User-Agents Simulation"), and the ‘($\Delta$)’
    denotes the difference between the results of real data and real data with 1 domain
    replaced with generated data. Statistically, the generated data on average affects
    31.2% of the training data, the test JGA decrease is from -0.2 to -1.19 with an
    average of -0.75, and the slot precision is on par with before with the recall
    drops by -0.32% on average. Compared to the reduction in training data size, the
    decreases in JGA and slot performance are relatively minor, suggesting that using
    generated data can effectively adapt DST models to a new domain with decent accuracy.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型还在LLaMA 2 7B上进行了训练，结果展示在表[5](#S4.T5 "Table 5 ‣ 4.1 Datasets and Metrics ‣
    4 Experiments ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents
    Simulation")中，其中‘($\Delta$)’表示真实数据与用生成数据替换1个领域后的真实数据结果之间的差异。统计上，生成数据平均影响了31.2%的训练数据，测试JGA减少范围从-0.2到-1.19，平均为-0.75，槽位精度与之前持平，召回率平均下降了-0.32%。与训练数据大小的减少相比，JGA和槽位性能的下降相对较小，表明使用生成数据可以有效地将DST模型适应到新的领域，并保持较好的准确性。
- en: In practical applications, our method for automated dialogue generation offers
    a fast way to develop dialogue systems in new domains, resulting in considerable
    savings in both time and cost.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们的方法用于自动对话生成，提供了一种快速开发新领域对话系统的方法，从而在时间和成本上节省了大量开支。
- en: 4.6 Analysis
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 分析
- en: 4.6.1 The Effect of Adding Generated Data to Real Data of Various Sizes
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1 添加生成数据到各种大小的真实数据的效果
- en: '![Refer to caption](img/6e6607e2fda2e490fe03ec3bc9bbc26f.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e6607e2fda2e490fe03ec3bc9bbc26f.png)'
- en: 'Figure 2: The error distribution between $\text{LUAS}_{\text{R}}$ with different
    sizes of real data on MultiWOZ 2.2.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同大小的真实数据下$\text{LUAS}_{\text{R}}$的误差分布在MultiWOZ 2.2中。
- en: To better illustrate the impact of generated data, we conduct a series of experiments
    by combining generated data with various sizes of real data. The experiment results
    are demonstrated in Table [6](#S4.T6 "Table 6 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation")
    and the sizes of real data used are set to be 1000, 2000, 4000, and all. The $\text{JGA}_{\text{R}}$
    represent the DST and slots accuracy results obtained from training with the same
    real data along with additional generated data. The symbols used in Table [5](#S4.T5
    "Table 5 ‣ 4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State
    Tracking Models through LLM-backed User-Agents Simulation") are also used here.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明生成数据的影响，我们通过将生成数据与不同大小的真实数据结合，进行了一系列实验。实验结果展示在表[6](#S4.T6 "Table 6 ‣
    4.1 Datasets and Metrics ‣ 4 Experiments ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation")中，真实数据的大小设置为1000、2000、4000和全部。$\text{JGA}_{\text{R}}$
    代表了使用相同真实数据和额外生成数据进行训练的DST和槽位准确性结果。表[5](#S4.T5 "Table 5 ‣ 4.1 Datasets and Metrics
    ‣ 4 Experiments ‣ Enhancing Dialogue State Tracking Models through LLM-backed
    User-Agents Simulation")中使用的符号在这里也有使用。
- en: The findings indicate that incorporating generated data into the training process
    significantly enhances model performance, surpassing that achieved with solely
    real data, particularly in scenarios where real training data is scarce. Under
    such situation, the performance of a model trained with generated data can be
    comparable to the model trained with twice amount of real data. For example, when
    only using 1,000 real data, the JGA of the two datasets will increase by 4.29%
    and 5.09% if the generated data is used, which is comparable to the performance
    of using 2,000 real data. Such findings hold considerable practical relevance,
    as they underscore the capacity of generated data to substantially mitigate the
    limitations posed by insufficient original data in real-world contexts.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 研究结果表明，将生成数据纳入训练过程显著提升了模型性能，超越了仅使用真实数据所达到的效果，特别是在真实训练数据稀缺的情况下。在这种情况下，使用生成数据训练的模型性能可以与使用双倍真实数据的模型相媲美。例如，当仅使用
    1,000 条真实数据时，如果使用生成数据，两个数据集的 JGA 将分别提高 4.29% 和 5.09%，这与使用 2,000 条真实数据的性能相当。这些发现具有相当大的实际意义，因为它们强调了生成数据在缓解真实世界中原始数据不足所带来的限制方面的能力。
- en: 4.6.2 Error Distribution Analysis
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2 错误分布分析
- en: As illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4.6.1 The Effect of Adding Generated
    Data to Real Data of Various Sizes ‣ 4.6 Analysis ‣ 4 Experiments ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation"), to
    further highlight the superiority of our approach, we examine the error distribution
    of different sizes of real data on MultiWOZ 2.2 between $\text{LUAS}_{\text{R}}$.
    Using generated data leads to a reduction in errors across almost all domain categories
    compared to models fine-tuned solely on original data. This finding not only confirms
    the high quality of our generated data but also emphasizes the effectiveness of
    our approach in enhancing model performance in DST.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S4.F2 "图 2 ‣ 4.6.1 添加生成数据到不同大小的真实数据的效果 ‣ 4.6 分析 ‣ 4 实验 ‣ 通过 LLM 支持的用户-智能体模拟增强对话状态跟踪模型")
    所示，为进一步突出我们方法的优越性，我们检查了 MultiWOZ 2.2 上不同大小真实数据的错误分布，比较了 $\text{LUAS}_{\text{R}}$。使用生成数据相比仅在原始数据上进行微调的模型，几乎所有领域类别的错误都得到了减少。这一发现不仅确认了我们生成数据的高质量，还强调了我们方法在提升
    DST 模型性能方面的有效性。
- en: 5 Conclusion
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a novel approach utilizing GPT-4 to simulate conversations
    between users and agents and generate dialogues with dialogue state labels. We
    then conduct a two-stage fine-tuning of LLaMA 2 on both the generated and real
    data for DST prediction. Experimental results on two public DST benchmarks demonstrate
    that our model, augmented with generated data, outperforms the baseline trained
    solely on real data. Furthermore, detailed analysis confirms the adaptability
    of our approach, effectively meeting the dynamic requirements of transitioning
    to new domains in real-world scenarios. We believe that our method can be extended
    into a generalizable framework, offering benefits to a wide range of dialogue-related
    tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的方法，利用 GPT-4 模拟用户与智能体之间的对话，并生成带有对话状态标签的对话。然后，我们对生成的数据和真实数据进行 LLaMA
    2 的两阶段微调，以进行 DST 预测。对两个公开 DST 基准测试的实验结果表明，我们的模型在增强了生成数据的情况下，优于仅使用真实数据训练的基线模型。此外，详细分析确认了我们方法的适应性，能够有效满足在真实场景中向新领域过渡的动态要求。我们相信，我们的方法可以扩展为一种可推广的框架，为广泛的对话相关任务提供益处。
- en: Limitations
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: With the high-quality dialogue data produced by our algorithm, we have significantly
    enhanced the DST model. We are also confident that the data generated will serve
    as valuable resources for other dialogue-related tasks, such as dialogue generation.
    We plan to explore this aspect in future research.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 借助我们算法生成的高质量对话数据，我们显著提升了 DST 模型。我们还相信，这些生成的数据将成为其他对话相关任务（如对话生成）的宝贵资源。我们计划在未来的研究中探索这一方面。
- en: Ethics Statement
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: We conduct experiments using two publicly available datasets in addition to
    datasets created by GPT-4, with a specific focus on multi-domain task-oriented
    dialogue. Each dataset is subjected to thorough pre-processing for academic research
    purposes, which includes the removal of any personally identifiable information
    or offensive content. As a result, we are confident that our work presents no
    ethical risks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个公开可用的数据集以及由GPT-4创建的数据集进行实验，特别关注多领域任务导向对话。每个数据集都经过彻底的预处理以用于学术研究，包括去除任何个人身份信息或冒犯性内容。因此，我们相信我们的工作不存在伦理风险。
- en: References
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Atawulla et al. (2023) Abibulla Atawulla, Xi Zhou, Yating Yang, Bo Ma, and Fengyi
    Yang. 2023. A slot-shared span prediction-based neural network for multi-domain
    dialogue state tracking. In *ICASSP 2023-2023 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP)*, pages 1–5\. IEEE.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atawulla et al. (2023) Abibulla Atawulla、Xi Zhou、Yating Yang、Bo Ma 和 Fengyi
    Yang。2023年。基于插槽共享跨度预测的多领域对话状态追踪神经网络。在*ICASSP 2023-2023 IEEE国际声学、语音与信号处理会议（ICASSP）*，第1–5页。IEEE。
- en: 'Bang et al. (2023) Namo Bang, Jeehyun Lee, and Myoung-Wan Koo. 2023. [Task-optimized
    adapters for an end-to-end task-oriented dialogue system](https://doi.org/10.18653/v1/2023.findings-acl.464).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    7355–7369, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang et al. (2023) Namo Bang、Jeehyun Lee 和 Myoung-Wan Koo。2023年。[针对端到端任务导向对话系统的任务优化适配器](https://doi.org/10.18653/v1/2023.findings-acl.464)。在*计算语言学协会发现：ACL
    2023*，第7355–7369页，加拿大多伦多。计算语言学协会。
- en: 'Bebensee and Lee (2023) Björn Bebensee and Haejun Lee. 2023. [Span-selective
    linear attention transformers for effective and robust schema-guided dialogue
    state tracking](https://doi.org/10.18653/v1/2023.acl-long.6). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 78–91, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bebensee and Lee (2023) Björn Bebensee 和 Haejun Lee。2023年。[用于有效和鲁棒的模式引导对话状态追踪的跨度选择性线性注意力变换器](https://doi.org/10.18653/v1/2023.acl-long.6)。在*第61届年度计算语言学协会会议（第1卷：长篇论文集）*，第78–91页，加拿大多伦多。计算语言学协会。
- en: Budzianowski et al. (2018) Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng,
    Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ
    - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling.
    In *Proc. of EMNLP*.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Budzianowski et al. (2018) Paweł Budzianowski、Tsung-Hsien Wen、Bo-Hsiang Tseng、Iñigo
    Casanueva、Stefan Ultes、Osman Ramadan 和 Milica Gašić。2018年。MultiWOZ - 一个大规模多领域Wizard-of-Oz数据集，用于任务导向对话建模。在*EMNLP会议论文集*。
- en: Chen et al. (2020) Lu Chen, Boer Lv, Chi Wang, Su Zhu, Bowen Tan, and Kai Yu.
    2020. Schema-guided multi-domain dialogue state tracking with graph attention
    neural networks. In *Proc. of AAAI*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Lu Chen、Boer Lv、Chi Wang、Su Zhu、Bowen Tan 和 Kai Yu。2020年。基于图注意力神经网络的模式引导多领域对话状态追踪。在*AAAI会议论文集*。
- en: Dong et al. (2019) Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang,
    Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training
    for natural language understanding and generation. *Advances in neural information
    processing systems*, 32.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019) Li Dong、Nan Yang、Wenhui Wang、Furu Wei、Xiaodong Liu、Yu Wang、Jianfeng
    Gao、Ming Zhou 和 Hsiao-Wuen Hon。2019年。用于自然语言理解和生成的统一语言模型预训练。在*神经信息处理系统进展*，第32卷。
- en: 'Eric et al. (2020) Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit
    Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-Tur.
    2020. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections
    and state tracking baselines. In *Proc. of LREC*.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eric et al. (2020) Mihail Eric、Rahul Goel、Shachi Paul、Abhishek Sethi、Sanchit
    Agarwal、Shuyang Gao、Adarsh Kumar、Anuj Goyal、Peter Ku 和 Dilek Hakkani-Tur。2020年。MultiWOZ
    2.1：一个整合的多领域对话数据集，包含状态修正和状态追踪基线。在*LREC会议论文集*。
- en: Feng et al. (2023) Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-Ming
    Wu. 2023. Towards llm-driven dialogue state tracking. In *Proc. of EMNLP*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2023) Yujie Feng、Zexin Lu、Bo Liu、Liming Zhan 和 Xiao-Ming Wu。2023年。面向大型语言模型驱动的对话状态追踪。在*EMNLP会议论文集*。
- en: 'Gao et al. (2020) Shuyang Gao, Sanchit Agarwal, Di Jin, Tagyoung Chung, and
    Dilek Hakkani-Tur. 2020. From machine reading comprehension to dialogue state
    tracking: Bridging the gap. In *Proceedings of the 2nd Workshop on Natural Language
    Processing for Conversational AI*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2020) Shuyang Gao、Sanchit Agarwal、Di Jin、Tagyoung Chung 和 Dilek
    Hakkani-Tur。2020年。从机器阅读理解到对话状态追踪：弥合差距。在*第二届对话人工智能自然语言处理研讨会*。
- en: 'Gao et al. (2019) Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung,
    and Dilek Hakkani-Tur. 2019. Dialog state tracking: A neural reading comprehension
    approach. In *Proc. of SIGDIAL*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2019) Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, 和
    Dilek Hakkani-Tur. 2019. 对话状态跟踪：一种神经阅读理解方法。发表于*SIGDIAL会议论文集*。
- en: 'Han et al. (2021) Ting Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian, Chongxuan
    Huang, Dazhen Wan, Wei Peng, and Minlie Huang. 2021. Multiwoz 2.3: A multi-domain
    task-oriented dialogue dataset enhanced with annotation corrections and co-reference
    annotation. In *Proc. of NLPCC*.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 (2021) Ting Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian, Chongxuan Huang,
    Dazhen Wan, Wei Peng, 和 Minlie Huang. 2021. Multiwoz 2.3：一个经过注释修正和共指注释增强的多领域任务导向对话数据集。发表于*NLPCC会议论文集*。
- en: He et al. (2022) Wanwei He, Yinpei Dai, Min Yang, Jian Sun, Fei Huang, Luo Si,
    and Yongbin Li. 2022. Unified dialog model pre-training for task-oriented dialog
    understanding and generation. In *Proceedings of the 45th International ACM SIGIR
    Conference on Research and Development in Information Retrieval*, pages 187–200.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2022) Wanwei He, Yinpei Dai, Min Yang, Jian Sun, Fei Huang, Luo Si, 和
    Yongbin Li. 2022. 任务导向对话理解与生成的统一对话模型预训练。发表于*第45届国际ACM SIGIR信息检索研究与发展会议论文集*，第187–200页。
- en: 'Heck et al. (2020) Michael Heck, Carel van Niekerk, Nurul Lubis, Christian
    Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Gasic. 2020. TripPy: A triple
    copy strategy for value independent neural dialog state tracking. In *Proceedings
    of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heck 等 (2020) Michael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser,
    Hsien-Chin Lin, Marco Moresi, 和 Milica Gasic. 2020. TripPy：一种针对值独立神经对话状态跟踪的三重复制策略。发表于*第21届话语与对话特别兴趣组年会论文集*。
- en: Henderson et al. (2014) Matthew Henderson, Blaise Thomson, and Jason D. Williams.
    2014. The second dialog state tracking challenge. In *Proc. of SIGDIAL*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等 (2014) Matthew Henderson, Blaise Thomson, 和 Jason D. Williams. 2014.
    第二届对话状态跟踪挑战。发表于*SIGDIAL会议论文集*。
- en: 'Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation
    of large language models. In *Proc. of ICLR*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2022. Lora：大型语言模型的低秩适配。发表于*ICLR会议论文集*。
- en: Hudeček and Dušek (2023) Vojtěch Hudeček and Ondřej Dušek. 2023. Are large language
    models all you need for task-oriented dialogue? In *Proceedings of the 24th Annual
    Meeting of the Special Interest Group on Discourse and Dialogue*.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hudeček 和 Dušek (2023) Vojtěch Hudeček 和 Ondřej Dušek. 2023. 大型语言模型是否足以应对任务导向对话？发表于*第24届话语与对话特别兴趣组年会论文集*。
- en: Jablonka et al. (2023) Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero,
    and Berend Smit. 2023. Is gpt-3 all you need for low-data discovery in chemistry?
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jablonka 等 (2023) Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero,
    和 Berend Smit. 2023. gpt-3 是否是化学领域低数据发现的终极方法？
- en: Kaddour and Liu (2024) Jean Kaddour and Qi Liu. 2024. Synthetic data generation
    in low-resource settings via fine-tuning of large language models. *ArXiv preprint*.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaddour 和 Liu (2024) Jean Kaddour 和 Qi Liu. 2024. 通过对大型语言模型进行微调，在资源匮乏的环境中生成合成数据。*ArXiv预印本*。
- en: 'King and Flanigan (2023) Brendan King and Jeffrey Flanigan. 2023. Diverse retrieval-augmented
    in-context learning for dialogue state tracking. In *Findings of the Association
    for Computational Linguistics: ACL 2023*, pages 5570–5585.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: King 和 Flanigan (2023) Brendan King 和 Jeffrey Flanigan. 2023. 用于对话状态跟踪的多样化检索增强上下文学习。发表于*计算语言学协会：ACL
    2023的发现*，第5570–5585页。
- en: 'Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method
    for stochastic optimization. In *Proc. of ICLR*.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba (2015) Diederik P. Kingma 和 Jimmy Ba. 2015. Adam：一种随机优化方法。发表于*ICLR会议论文集*。
- en: Ko et al. (2015) Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur.
    2015. Audio augmentation for speech recognition. In *Proc. of Interspeech*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko 等 (2015) Tom Ko, Vijayaditya Peddinti, Daniel Povey, 和 Sanjeev Khudanpur.
    2015. 语音识别的音频增强。发表于*Interspeech会议论文集*。
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
    2012. Imagenet classification with deep convolutional neural networks. In *Proc.
    of NeurIPS*.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等 (2012) Alex Krizhevsky, Ilya Sutskever, 和 Geoffrey E. Hinton. 2012.
    使用深度卷积神经网络进行Imagenet分类。发表于*NeurIPS会议论文集*。
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
    memory management for large language model serving with pagedattention. In *Proceedings
    of the 29th Symposium on Operating Systems Principles*, pages 611–626.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, 和 Ion Stoica. 2023. 大型语言模型服务的高效内存管理与分页注意力。在
    *第29届操作系统原理研讨会论文集*，页611–626。
- en: Lee et al. (2021) Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021. Dialogue
    state tracking with a language model using schema-driven prompting. In *Proc.
    of EMNLP*.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2021) Chia-Hsuan Lee, Hao Cheng, 和 Mari Ostendorf. 2021. 使用基于模式的提示的语言模型进行对话状态跟踪。在
    *EMNLP 会议论文集*。
- en: 'Lesci et al. (2023) Pietro Lesci, Yoshinari Fujinuma, Momchil Hardalov, Chao
    Shang, Yassine Benajiba, and Lluis Marquez. 2023. Diable: Efficient dialogue state
    tracking as operations on tables. In *Proc. of ACL Findings*.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lesci et al. (2023) Pietro Lesci, Yoshinari Fujinuma, Momchil Hardalov, Chao
    Shang, Yassine Benajiba, 和 Lluis Marquez. 2023. Diable: 高效的对话状态跟踪作为表格上的操作。在 *ACL
    发现会议论文集*。'
- en: Li et al. (2022) Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and
    Yan Xifeng. 2022. Controllable dialogue simulation with in-context learning. In
    *Proc. of EMNLP*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, 和 Yan
    Xifeng. 2022. 可控对话模拟与上下文学习。在 *EMNLP 会议论文集*。
- en: 'Lin et al. (2020) Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale
    Fung. 2020. MinTL: Minimalist transfer learning for task-oriented dialogue systems.
    In *Proc. of EMNLP*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2020) Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, 和 Pascale
    Fung. 2020. MinTL: 面向任务的对话系统的极简转移学习。在 *EMNLP 会议论文集*。'
- en: 'Loshchilov and Hutter (2016) Ilya Loshchilov and Frank Hutter. 2016. Sgdr:
    Stochastic gradient descent with warm restarts. In *International Conference on
    Learning Representations*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Loshchilov and Hutter (2016) Ilya Loshchilov 和 Frank Hutter. 2016. Sgdr: 带有热启动的随机梯度下降。在
    *国际学习表示会议*。'
- en: Ma et al. (2019) Mingyu Derek Ma, Kevin Bowden, Jiaqi Wu, Wen Cui, and Marilyn
    Walker. 2019. Implicit discourse relation identification for open-domain dialogues.
    In *Proc. of ACL*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2019) Mingyu Derek Ma, Kevin Bowden, Jiaqi Wu, Wen Cui, 和 Marilyn
    Walker. 2019. 开放领域对话中的隐性话语关系识别。在 *ACL 会议论文集*。
- en: 'Mrkšić et al. (2017) Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien Wen, Blaise
    Thomson, and Steve Young. 2017. Neural belief tracker: Data-driven dialogue state
    tracking. In *Proc. of ACL*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mrkšić et al. (2017) Nikola Mrkšić, Diarmuid Ó Séaghdha, Tsung-Hsien Wen, Blaise
    Thomson, 和 Steve Young. 2017. 神经信念跟踪器: 数据驱动的对话状态跟踪。在 *ACL 会议论文集*。'
- en: OpenAI (2023) OpenAI. 2023. Chatgpt. [https://chat.openai.com](https://chat.openai.com).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. Chatgpt. [https://chat.openai.com](https://chat.openai.com)。
- en: 'Park et al. (2019) Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. Specaugment: A simple data augmentation
    method for automatic speech recognition. In *Proc. of INTERSPEECH*.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park et al. (2019) Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin D. Cubuk, 和 Quoc V. Le. 2019. Specaugment: 一种用于自动语音识别的简单数据增强方法。在
    *INTERSPEECH 会议论文集*。'
- en: 'Peng et al. (2021) Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh,
    Lars Liden, and Jianfeng Gao. 2021. Soloist: Building task bots at scale with
    transfer learning and machine teaching. *Transactions of the Association for Computational
    Linguistics*.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. (2021) Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh,
    Lars Liden, 和 Jianfeng Gao. 2021. Soloist: 通过转移学习和机器教学大规模构建任务机器人。*计算语言学会会刊*。'
- en: Ren et al. (2018) Liliang Ren, Kaige Xie, Lu Chen, and Kai Yu. 2018. Towards
    universal dialogue state tracking. In *Proc. of EMNLP*.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2018) Liliang Ren, Kaige Xie, Lu Chen, 和 Kai Yu. 2018. 迈向通用对话状态跟踪。在
    *EMNLP 会议论文集*。
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its
    friends in huggingface. In *Proc. of NeurIPS*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, 和 Yueting Zhuang. 2023. Hugginggpt: 利用 chatgpt 和其在 huggingface 的朋友解决 ai 任务。在
    *NeurIPS 会议论文集*。'
- en: Shorten and Khoshgoftaar (2019) Connor Shorten and Taghi M Khoshgoftaar. 2019.
    A survey on image data augmentation for deep learning. *Journal of Big Data*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shorten and Khoshgoftaar (2019) Connor Shorten 和 Taghi M Khoshgoftaar. 2019.
    关于深度学习的图像数据增强的综述。*大数据期刊*。
- en: 'Significant-gravitas (2023) Significant-gravitas. 2023. auto-gpt: An experimental
    open-source attempt to make gpt-4 fully autonomous. [https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Significant-gravitas (2023) Significant-gravitas. 2023. auto-gpt: 一个实验性的开源尝试，使
    gpt-4 完全自主。[https://github.com/Significant-Gravitas/AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)。'
- en: Su et al. (2023) Ruolin Su, Jingfeng Yang, Ting-Wei Wu, and Biing-Hwang Juang.
    2023. Choice fusion as knowledge for zero-shot dialogue state tracking. In *Proc.
    of ICASSP*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2023) Ruolin Su, Jingfeng Yang, Ting-Wei Wu, 和 Biing-Hwang Juang.
    2023. 选择融合作为零样本对话状态追踪的知识。收录于*ICASSP会议论文集*。
- en: Sun et al. (2022) Zhoujian Sun, Zhengxing Huang, and Nai Ding. 2022. [On tracking
    dialogue state by inheriting slot values in mentioned slot pools](https://doi.org/10.24963/ijcai.2022/607).
    In *Proceedings of the Thirty-First International Joint Conference on Artificial
    Intelligence, IJCAI-22*, pages 4375–4382\. International Joint Conferences on
    Artificial Intelligence Organization. Main Track.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) Zhoujian Sun, Zhengxing Huang, 和 Nai Ding. 2022. [通过继承提及槽池中的槽值来跟踪对话状态](https://doi.org/10.24963/ijcai.2022/607)。收录于*第三十一届国际人工智能联合会议论文集,
    IJCAI-22*，第4375–4382页。国际人工智能联合会议组织。主要轨道。
- en: Tian et al. (2021) Xin Tian, Liankai Huang, Yingzhan Lin, Siqi Bao, Huang He,
    Yunyi Yang, Hua Wu, Fan Wang, and Shuqi Sun. 2021. Amendable generation for dialogue
    state tracking. In *Proceedings of the 3rd Workshop on Natural Language Processing
    for Conversational AI*.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2021) Xin Tian, Liankai Huang, Yingzhan Lin, Siqi Bao, Huang He,
    Yunyi Yang, Hua Wu, Fan Wang, 和 Shuqi Sun. 2021. 对话状态追踪的可修改生成。收录于*第三届对话人工智能自然语言处理研讨会论文集*。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *ArXiv preprint*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. 2023a. Llama: 开放且高效的基础语言模型。*ArXiv 预印本*。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *ArXiv preprint*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023b. Llama 2: 开放基础和微调的对话模型。*ArXiv 预印本*。'
- en: Ulmer et al. (2024) Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun,
    Xibin Gao, and Yi Zhang. 2024. Bootstrapping llm-based task-oriented dialogue
    agents via self-talk. *ArXiv preprint*.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ulmer et al. (2024) Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun,
    Xibin Gao, 和 Yi Zhang. 2024. 通过自我对话启动基于LLM的任务导向对话代理。*ArXiv 预印本*。
- en: 'Wang et al. (2023) Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng
    Lin, Shi Wang, Dacheng Tao, and Li Guo. 2023. Divide, conquer, and combine: Mixture
    of semantic-independent experts for zero-shot dialogue state tracking. In *Proc.
    of ACL*.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng
    Lin, Shi Wang, Dacheng Tao, 和 Li Guo. 2023. 分而治之，合而为一: 语义独立专家混合用于零样本对话状态追踪。收录于*ACL会议论文集*。'
- en: 'Wang et al. (2022) Yifan Wang, Jing Zhao, Junwei Bao, Chaoqun Duan, Youzheng
    Wu, and Xiaodong He. 2022. LUNA: Learning slot-turn alignment for dialogue state
    tracking. In *Proc. of NAACL*.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2022) Yifan Wang, Jing Zhao, Junwei Bao, Chaoqun Duan, Youzheng
    Wu, 和 Xiaodong He. 2022. LUNA: 对话状态追踪中的槽-turn对齐学习。收录于*NAACL会议论文集*。'
- en: 'Wei and Zou (2019) Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation
    techniques for boosting performance on text classification tasks. In *Proc. of
    EMNLP*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei and Zou (2019) Jason Wei 和 Kai Zou. 2019. EDA: 提升文本分类任务性能的简单数据增强技术。收录于*EMNLP会议论文集*。'
- en: Wu et al. (2019) Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming
    Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state
    generator for task-oriented dialogue systems. In *Proc. of ACL*.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming
    Xiong, Richard Socher, 和 Pascale Fung. 2019. 任务导向对话系统的可转移多领域状态生成器。收录于*ACL会议论文集*。
- en: 'Yang et al. (2023a) Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili
    Wang, Shusen Wang, and Hai Zhao. 2023a. Refgpt: Dialogue generation of gpt, by
    gpt, and for gpt. In *Proc. of EMNLP Findings*.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2023a) Dongjie Yang, Ruifeng Yuan, Yuantao Fan, Yifei Yang, Zili
    Wang, Shusen Wang, 和 Hai Zhao. 2023a. Refgpt: GPT的对话生成，基于GPT，面向GPT。收录于*EMNLP发现会议论文集*。'
- en: Yang et al. (2023b) Longfei Yang, Jiyi Li, Sheng Li, and Takahiro Shinozaki.
    2023b. Multi-domain dialogue state tracking with disentangled domain-slot attention.
    In *Proc. of ACL Findings*.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023b) Longfei Yang, Jiyi Li, Sheng Li, 和 Takahiro Shinozaki. 2023b.
    基于解耦域-槽注意力的多领域对话状态追踪。收录于*ACL发现会议论文集*。
- en: 'Ye et al. (2022) Fanghua Ye, Jarana Manotumruksa, and Emine Yilmaz. 2022. MultiWOZ
    2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections
    to improve state tracking evaluation. In *Proceedings of the 23rd Annual Meeting
    of the Special Interest Group on Discourse and Dialogue*.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye 等人（2022）Fanghua Ye, Jarana Manotumruksa 和 Emine Yilmaz. 2022. MultiWOZ 2.4:
    一个多领域任务导向对话数据集，通过必要的注释修正提高状态跟踪评估。发表于*第23届对话与话语特别兴趣小组年会论文集*。'
- en: Ye et al. (2021) Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, Shenghui Li,
    and Emine Yilmaz. 2021. Slot self-attentive dialogue state tracking. In *Proc.
    of WWW*.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等人（2021）Fanghua Ye, Jarana Manotumruksa, Qiang Zhang, Shenghui Li 和 Emine
    Yilmaz. 2021. 槽自注意对话状态跟踪。发表于*WWW会议论文集*。
- en: 'Zang et al. (2020) Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav
    Gupta, Jianguo Zhang, and Jindong Chen. 2020. MultiWOZ 2.2 : A dialogue dataset
    with additional annotation corrections and state tracking baselines. In *Proceedings
    of the 2nd Workshop on Natural Language Processing for Conversational AI*.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zang 等人（2020）Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta,
    Jianguo Zhang 和 Jindong Chen. 2020. MultiWOZ 2.2: 一个带有附加注释修正和状态跟踪基准的对话数据集。发表于*第二届对话人工智能自然语言处理研讨会论文集*。'
- en: 'Zhang et al. (2023) Haoning Zhang, Junwei Bao, Haipeng Sun, Youzheng Wu, Wenye
    Li, Shuguang Cui, and Xiaodong He. 2023. [MoNET: Tackle state momentum via noise-enhanced
    training for dialogue state tracking](https://doi.org/10.18653/v1/2023.findings-acl.33).
    In *Findings of the Association for Computational Linguistics: ACL 2023*, pages
    520–534, Toronto, Canada. Association for Computational Linguistics.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2023）Haoning Zhang, Junwei Bao, Haipeng Sun, Youzheng Wu, Wenye Li,
    Shuguang Cui 和 Xiaodong He. 2023. [MoNET: 通过噪声增强训练处理状态动量以进行对话状态跟踪](https://doi.org/10.18653/v1/2023.findings-acl.33)。发表于*计算语言学协会会议论文集：ACL
    2023*，第520–534页，多伦多，加拿大。计算语言学协会。'
- en: Zhang et al. (2020) Jianguo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wang,
    Philip Yu, Richard Socher, and Caiming Xiong. 2020. Find or classify? dual strategy
    for slot-value predictions on multi-domain dialog state tracking. In *Proceedings
    of the Ninth Joint Conference on Lexical and Computational Semantics*.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2020）Jianguo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wang, Philip
    Yu, Richard Socher 和 Caiming Xiong. 2020. 查找还是分类？用于多领域对话状态跟踪的槽值预测双重策略。发表于*第九届词汇与计算语义联合会议论文集*。
- en: Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level
    convolutional networks for text classification. In *Proc. of NeurIPS*.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2015）Xiang Zhang, Junbo Jake Zhao 和 Yann LeCun. 2015. 用于文本分类的字符级卷积网络。发表于*NeurIPS会议论文集*。
- en: Zhao et al. (2022) Jeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu, Mingqiu Wang,
    Harrison Lee, Abhinav Rastogi, Izhak Shafran, and Yonghui Wu. 2022. Description-driven
    task-oriented dialog modeling. *ArXiv preprint*.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2022）Jeffrey Zhao, Raghav Gupta, Yuan Cao, Dian Yu, Mingqiu Wang, Harrison
    Lee, Abhinav Rastogi, Izhak Shafran 和 Yonghui Wu. 2022. 基于描述的任务导向对话建模。*ArXiv 预印本*。
- en: 'Zhao et al. (2023) Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin
    Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison,
    Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit
    Mathews, and Shen Li. 2023. Pytorch fsdp: Experiences on scaling fully sharded
    data parallel. *Proc. VLDB Endow.*'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人（2023）Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang,
    Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison,
    Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit
    Mathews 和 Shen Li. 2023. Pytorch fsdp: 扩展完全分片数据并行的经验。*Proc. VLDB Endow.*'
- en: 'Zhou et al. (2023) Han Zhou, Ignacio Iacobacci, and Pasquale Minervini. 2023.
    XQA-DST: Multi-domain and multi-lingual dialogue state tracking. In *Proc. of
    ACL Findings*.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人（2023）Han Zhou, Ignacio Iacobacci 和 Pasquale Minervini. 2023. XQA-DST:
    多领域和多语言对话状态跟踪。发表于*ACL发现论文集*。'
- en: Zhu et al. (2022) Qi Zhu, Bing Li, Fei Mi, Xiaoyan Zhu, and Minlie Huang. 2022.
    Continual prompt tuning for dialog state tracking. In *Proc. of ACL*.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2022）Qi Zhu, Bing Li, Fei Mi, Xiaoyan Zhu 和 Minlie Huang. 2022. 对话状态跟踪的持续提示调优。发表于*ACL会议论文集*。
- en: Appendix A Prompts for Simulation
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 模拟提示
- en: In this section, we illustrated a variety of typical prompts utilized by the
    user simulator and the agent simulator within the simulation. The symbol ‘$\backslash$n’
    of the prompts represents a line break.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了用户模拟器和代理模拟器在模拟过程中使用的各种典型提示。提示中的符号‘$\backslash$n’表示换行。
- en: Table [7](#A1.T7 "Table 7 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the user informs requirement to the agent. Table [8](#A1.T8 "Table
    8 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation") represents the prompt that the user
    updates the requirement to the agent. Table [9](#A1.T9 "Table 9 ‣ Appendix A Prompts
    for Simulation ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents
    Simulation") represents the prompt that the user asks for a recommendation with
    the control identifier ‘[RECOM]’ to request a recommendation from the agent. Table
    [10](#A1.T10 "Table 10 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the user inquires about some properties (e.g. address and postcode)
    for the candidates. Table [11](#A1.T11 "Table 11 ‣ Appendix A Prompts for Simulation
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation")
    represents the prompt that the user asks for a booking action from the agent.
    Table [12](#A1.T12 "Table 12 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the user’s general chat with the agent with the control identifier
    ‘[EOF]’ to inform the agent that all the needs are satisfied.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表[7](#A1.T7 "表 7 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示用户向代理传达需求的提示。 表[8](#A1.T8
    "表 8 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示用户更新需求给代理的提示。 表[9](#A1.T9 "表
    9 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示用户请求推荐的提示，控制标识符为‘[RECOM]’，用于向代理请求推荐。
    表[10](#A1.T10 "表 10 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示用户询问候选人一些属性（例如地址和邮政编码）的提示。
    表[11](#A1.T11 "表 11 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示用户要求代理进行预订操作的提示。
    表[12](#A1.T12 "表 12 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示用户与代理进行的一般聊天提示，控制标识符为‘[EOF]’，用于告知代理所有需求已满足。
- en: Table [13](#A1.T13 "Table 13 ‣ Appendix A Prompts for Simulation ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation") represents
    the prompt that is used by the slot extractor.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表[13](#A1.T13 "表 13 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟增强对话状态追踪模型")表示由槽提取器使用的提示。
- en: Table [14](#A1.T14 "Table 14 ‣ Appendix A Prompts for Simulation ‣ Enhancing
    Dialogue State Tracking Models through LLM-backed User-Agents Simulation") represents
    the prompt that the agent inquires the user for a specified requirement(e.g. restaurant-pricerange).
    Table [15](#A1.T15 "Table 15 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the agent responds with the properties that the user inquiries. Table
    [16](#A1.T16 "Table 16 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue
    State Tracking Models through LLM-backed User-Agents Simulation") represents the
    prompt that the agent reports search results to the user. Table [17](#A1.T17 "Table
    17 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue State Tracking Models
    through LLM-backed User-Agents Simulation") represents the prompt that the agent
    reports the action result (e.g. the reservation information, etc.) and control
    identifier ‘[BOOKED]’ to inform the successful of the reservation. Table [18](#A1.T18
    "Table 18 ‣ Appendix A Prompts for Simulation ‣ Enhancing Dialogue State Tracking
    Models through LLM-backed User-Agents Simulation") represents the prompt that
    the agent general chats with the user and outputs the control identifier ‘[EOD]’
    to end the whole simulation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [14](#A1.T14 "表 14 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟提升对话状态跟踪模型") 表示代理询问用户特定需求的提示（例如：餐厅价格范围）。表
    [15](#A1.T15 "表 15 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟提升对话状态跟踪模型") 表示代理响应用户询问的属性的提示。表
    [16](#A1.T16 "表 16 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟提升对话状态跟踪模型") 表示代理报告搜索结果给用户的提示。表
    [17](#A1.T17 "表 17 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟提升对话状态跟踪模型") 表示代理报告行动结果（例如预订信息等）以及控制标识符‘[BOOKED]’，以通知预订成功。表
    [18](#A1.T18 "表 18 ‣ 附录 A 模拟提示 ‣ 通过 LLM 支持的用户代理模拟提升对话状态跟踪模型") 表示代理与用户进行一般聊天，并输出控制标识符‘[EOD]’，以结束整个模拟。
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| DST: [历史记录], [需求] $\rightarrow$ [用户发言] |'
- en: '| Prompt: This is your first time in Cambridge and want to find a restaurant.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 提示: 这是你第一次来到剑桥，想找一家餐馆。$\backslash$n 只输出最新的发言，不要输出对话历史。 |'
- en: '| Output: Do you know of any good eateries in the north of Cambridge? |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 输出: 你知道剑桥北部有哪些好的餐馆吗？ |'
- en: 'Table 7: Prompt for the user to inform requirement.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 提示用户提供需求信息。'
- en: '| DST: [history], [old requirements], [new requirements] $\rightarrow$ [user_utterance]
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| DST: [历史记录], [旧需求], [新需求] $\rightarrow$ [用户发言] |'
- en: '| Prompt: You are the first time in Cambridge and want to find a hotel.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 提示: 你第一次来到剑桥，想找一家酒店。$\backslash$n 只输出最新的发言，不要输出对话历史。 |'
- en: '| Output: How about guesthouses? Do you have any 4-star options in the north
    of Cambridge for a group of 4, staying from Saturday for 4 nights? |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 输出: 旅馆怎么样？你有任何4星级的选择在剑桥北部，适合4人入住，从星期六开始住4晚吗？ |'
- en: 'Table 8: Prompt for the user to update requirement.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 提示用户更新需求。'
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| DST: [历史记录], [需求] $\rightarrow$ [用户发言] |'
- en: '| Prompt: This is your first time here and want to find a place to eat.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 提示: 这是你第一次来这里，想找个地方吃饭。$\backslash$n 只输出最新的发言，不要输出对话历史。 |'
- en: '| Output: Yes, please recommend one. [RECOM] |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 输出: 是的，请推荐一个。[RECOM] |'
- en: 'Table 9: Prompt for the user to ask a recommendation with control identifier
    ‘[RECOM]’.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 提示用户以控制标识符‘[RECOM]’请求推荐。'
- en: '| DST: [history], [requirements], [properties] $\rightarrow$ [user_utterance]
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| DST: [历史记录], [需求], [属性] $\rightarrow$ [用户发言] |'
- en: '| Prompt: This is your first time in here and want to find a place to eat.$\backslash$n
    Only output the newest utterance, don’t output the conversation history. |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 提示: 这是你第一次来这里，想找个地方吃饭。$\backslash$n 只输出最新的发言，不要输出对话历史。 |'
- en: '| Output: Could you provide the address and postcode for that place? |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 输出: 你能提供那个地方的地址和邮政编码吗？ |'
- en: 'Table 10: Prompt for the user to inquire the properties of the candidate.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 提示用户询问候选项的属性。'
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| DST: [历史记录], [需求] $\rightarrow$ [用户发言] |'
- en: '| Prompt: Now you are chatting with a local guide online.$\backslash$n Only
    output the newest utterance, don’t output the conversation history. |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 提示：现在你正在与本地导游在线聊天。$\backslash$n 仅输出最新的发言，不要输出对话历史。 |'
- en: '| Output: Yes, please proceed with the booking at Worth House. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 输出：是的，请继续在 Worth House 进行预订。 |'
- en: 'Table 11: Prompt for the user to ask for a booking action from the agent.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：用户要求代理进行预订操作的提示。
- en: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history], [requirements] $\rightarrow$ [user_utterance] |'
- en: '| Prompt: This is your first time in cambridge and want to find a hotel.$\backslash$n
    Please thanks for the help from the local guide and output the mark ‘[EOF]‘Only
    output the newest utterance, don’t output the conversation history. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 提示：这是你第一次来剑桥，想要找一家酒店。$\backslash$n 请感谢本地导游的帮助，并输出标记‘[EOF]’仅输出最新的发言，不要输出对话历史。
    |'
- en: '| Output: Grateful for your guidance.[EOF] |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 输出：感谢你的指导。[EOF] |'
- en: 'Table 12: Prompt for the user to chat with the agent with the control identifier
    ‘[EOF]’.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：用户与代理聊天时控制标识符为‘[EOF]’的提示。
- en: '| DST: [history], [slots schema] $\rightarrow$ [slots] |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history], [slots schema] $\rightarrow$ [slots] |'
- en: '| Prompt: You are a local agent, and now chatting with the user online for
    ‘restaurant.$\backslash$n - the ‘slot value‘ should be mentioned in the schema
    ‘possible_values‘ if the slot value is categorical or you need to extract its
    value exactly from the conversion history. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 提示：你是一个本地代理，现在正在与用户在线聊天关于‘餐馆’。$\backslash$n - 如果槽位值是分类的，或者你需要从对话历史中精确提取其值，那么‘slot
    value’ 应该在 schema ‘possible_values’ 中提到。 |'
- en: '| Output: {"slots": {"restaurant-area": "north"}, "service": "restaurant"}
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 输出：{"slots": {"restaurant-area": "north"}, "service": "restaurant"} |'
- en: 'Table 13: Prompt for the slot extractor of restaurant domain.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：餐馆领域的槽位提取提示。
- en: '| DST: [history], [inquire_requirements] $\rightarrow$ [agent_utterance], [inquire_requirements]
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history], [inquire_requirements] $\rightarrow$ [agent_utterance], [inquire_requirements]
    |'
- en: '| Prompt: You are a local agent for ‘restaurant‘, and are chatting with the
    user online.$\backslash$n Please answer in a JSON format, {"response": "", "inquire_requirements":
    []} |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 提示：你是一个本地代理，正在与用户在线聊天关于‘餐馆’。$\backslash$n 请以 JSON 格式回答，{"response": "", "inquire_requirements":
    []} |'
- en: '| Output: {"response": "Are you looking for something more upscale or casual?",
    "inquire_requirements": ["restaurant-pricerange"]} |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 输出：{"response": "你是在寻找更高档还是休闲的餐馆？", "inquire_requirements": ["restaurant-pricerange"]}
    |'
- en: 'Table 14: Prompt for the agent to inquire the user for a specified requirement.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：代理询问用户指定要求的提示。
- en: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
- en: '| Prompt: You are a local agent, and now are chatting with the user online
    for restaurant.$\backslash$n Please answer in a JSON format {"response": ""} |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 提示：你是一个本地代理，现在正在与用户在线聊天关于餐馆。$\backslash$n 请以 JSON 格式回答 {"response": ""} |'
- en: '| Output: Certainly! Golden Wok is located at 191 Histon Road Chesterton, with
    the postcode CB43HL. Would you like me to make a reservation for you? |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 输出：当然！Golden Wok 位于 191 Histon Road Chesterton，邮政编码 CB43HL。你希望我为你预订吗？ |'
- en: 'Table 15: Prompt for the agent to answer the user’s inquiry properties.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：代理回答用户询问属性的提示。
- en: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
- en: '| Prompt: You are a local agent, and now are chatting with the user online
    for restaurant.$\backslash$n Please answer in a JSON format {"response": ""} |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 提示：你是一个本地代理，现在正在与用户在线聊天关于餐馆。$\backslash$n 请以 JSON 格式回答 {"response": ""} |'
- en: '| Output: {"response": "I found a couple of places that fit your preferences.
    Would you like me to recommend one?"} |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 输出：{"response": "我找到了一些符合你偏好的地方。你希望我推荐一个吗？"} |'
- en: 'Table 16: Prompt for the agent to report search results.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16：代理报告搜索结果的提示。
- en: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history], [search condition], [search results] $\rightarrow$ [agent_utterance]
    |'
- en: '| Prompt: You are a local agent, and now are chatting with the user online
    for restaurant. $\backslash$n Please answer in a JSON format {"response": ""}
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 提示：你是一个本地代理，现在正在与用户在线聊天关于餐馆。$\backslash$n 请以 JSON 格式回答 {"response": ""} |'
- en: '| Output: {"response": "Your reservation at Golden Wok for eight people on
    Monday at 18:00 hours has been successfully made. Your reference code is 3FA7B6Z9\.
    [BOOKED]"} |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 输出：{"response": "你在 Golden Wok 的八人预订已成功完成，时间为周一 18:00。你的参考代码是 3FA7B6Z9。 [BOOKED]"}
    |'
- en: 'Table 17: Prompt for the agent to report reservation information of booking
    action with control identifier ‘[BOOKED]’.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '表 17: 提示代理报告预订操作的预订信息，控制标识符为‘[BOOKED]’。'
- en: '| DST: [history]$\rightarrow$ [agent_utterance] |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| DST: [history]$\rightarrow$ [agent_utterance] |'
- en: '| Prompt: You are a local agent for ‘restaurant‘, and are chatting with the
    user online.$\backslash$n Please answer in a JSON format, {"response": ""} |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 提示: 您是‘餐厅’的本地代理，正在与用户在线聊天。$\backslash$n 请以 JSON 格式回答，{"response": ""} |'
- en: '| Output: {"response": "You’re welcome! Enjoy your meal at Golden Wok. If you
    need anything else, just ask. [EOD]"} |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 输出: {"response": "不客气！祝您在 Golden Wok 用餐愉快。如果您还需要其他帮助，请随时告知。[EOD]"} |'
- en: 'Table 18: Prompt for the agent to conclude the dialogue with control identifier
    ‘[EOD]’.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '表 18: 提示代理以控制标识符‘[EOD]’结束对话。'
- en: Appendix B Templates for Booking Responses
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 预订响应模板
- en: We first crafted template responses like There are a lot of {type} attractions
    available. Would you like information about one of those? Perhaps, a {type} like
    {name}? and then expand them to hundreds with the GPT-4’s rewriting ability as
    shown in Table [19](#A2.T19 "Table 19 ‣ Appendix B Templates for Booking Responses
    ‣ Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation").
    In our simulation, the templated response will randomly substitute the recommendation
    responses of the agent to enhance the variety of interactions.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设计了像“有很多{type}的景点。您是否希望了解其中之一？例如，像{name}的{type}？”这样的模板响应，然后利用 GPT-4 的重写能力将其扩展到数百个，如表
    [19](#A2.T19 "表 19 ‣ 附录 B 预订响应模板 ‣ 通过 LLM 支持的用户代理模拟增强对话状态跟踪模型") 所示。在我们的模拟中，模板化响应将随机替代代理的推荐响应，以增强互动的多样性。
- en: '| DST: [template]$\rightarrow$ [rewrited templates] |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| DST: [template]$\rightarrow$ [rewrited templates] |'
- en: '| Prompt: $\backslash$n |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 提示: $\backslash$n |'
- en: '| Output: $\backslash$n ] |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 输出: $\backslash$n ] |'
- en: 'Table 19: Prompt for template rewriting.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '表 19: 模板重写提示。'
