<!--yml
category: 未分类
date: 2025-01-11 12:32:27
-->

# LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot

> 来源：[https://arxiv.org/html/2406.10590/](https://arxiv.org/html/2406.10590/)

Shu Zhong Department of Computer Science, University College London Elia Gatti Department of Computer Science, University College London James Hardwick Department of Computer Science, University College London Miriam Ribul Materials Science Research Centre, Royal College of Art Youngjun Cho Department of Computer Science, University College London Marianna Obrist Department of Computer Science, University College London

###### Abstract

Developing domain-specific conversational agents (CAs) has been challenged by the need for extensive domain-focused data. Recent advancements in Large Language Models (LLMs) make them a viable option as a knowledge backbone. LLMs behaviour can be enhanced through prompting, instructing them to perform downstream tasks in a zero-shot fashion (i.e. without training). To this end, we incorporated structural knowledge into prompts and used prompted LLMs to build domain-specific voice-based CAs. We demonstrate this approach for the specific domain of textile circularity in form of the design, development, and evaluation of TextileBot. We present the design and development of the voice agent TextileBot and also the insights from an in-person user study (N=30) evaluating three variations of TextileBots. We analyse the human-agent interactions, combining quantitative and qualitative methods. Our results suggest that participants engaged in multi-turn conversations, and their perceptions of the three variation agents and respective interactions varied demonstrating the effectiveness of our prompt-based LLM approach. We discuss the dynamics of these interactions and their implications for designing future voice-based CAs.

The results show that our method’s potential for building domain-specific CAs. Furthermore, most participants engaged in multi-turn conversations, and their perceptions of the three voice agents and respective interactions varied demonstrating the effectiveness of our prompt-based LLM approach. We discuss the dynamics of these interactions and their implications for designing future voice-based CAs.

## 1 Introduction

The potential for designing conversational interfaces through the use of pre-trained large language models (LLMs) has gained substantial attention [[106](https://arxiv.org/html/2406.10590v1#bib.bib106), [48](https://arxiv.org/html/2406.10590v1#bib.bib48), [56](https://arxiv.org/html/2406.10590v1#bib.bib56)]. These LLMs possess remarkable capabilities to comprehend human language, generate text in a human-like way, and execute various tasks with only a few text *prompts* at run time, even without any additional training [[28](https://arxiv.org/html/2406.10590v1#bib.bib28), [64](https://arxiv.org/html/2406.10590v1#bib.bib64), [79](https://arxiv.org/html/2406.10590v1#bib.bib79), [14](https://arxiv.org/html/2406.10590v1#bib.bib14), [72](https://arxiv.org/html/2406.10590v1#bib.bib72)]. A prompt is a piece of text input to the LLM to elicit a response. For instance, a prompt can be *“What is sustainable fashion? Please explain it to a 10-year-old kid.”*. This has significantly lowered the entry barriers for artificial intelligence (AI) access, allowing non-experts to interact with LLMs through text. However, designing such prompts manually poses a particular challenge, as accurate and concise encapsulation of the complex, domain-specific knowledge is needed to guide the model towards a specific domain. This is due to the inherent task-agnostic nature of vanilla LLMs ¹¹1Vanilla model refers to an LLM without fine-tuning or prompting., which are not fine-tuned for specific domains. Additionally, another drawback of pre-trained vanilla LLMs is their lack of conversational memory, which restricts interactions to a ‘single turn’ manner²²2A one-time exchange between a user and an AI system or another person in a conversation counts as one turn.. This limitation affects the continuity and depth of user interactions, as these models cannot recall their previous inputs and outputs.

The human-computer interaction (HCI) community is seeing a rising emphasis on the development of voice-based conversational agents [[95](https://arxiv.org/html/2406.10590v1#bib.bib95), [9](https://arxiv.org/html/2406.10590v1#bib.bib9), [94](https://arxiv.org/html/2406.10590v1#bib.bib94), [40](https://arxiv.org/html/2406.10590v1#bib.bib40)]. Seaborn et al. found that only 13% of studies in voice-based human-agent interaction (vHAI) used an autonomous setup, fully controlled by participants [[84](https://arxiv.org/html/2406.10590v1#bib.bib84)]. This limited adoption is primarily due to the complexity of creating voice agents. The advent of LLMs now enables real-time conversational interactions with humans, eliminating the necessity for constructing intricate heuristic-based conversation rules from scratch. In addition, the development of domain-specific conversational agents has long been hindered by data scarcity [[7](https://arxiv.org/html/2406.10590v1#bib.bib7), [51](https://arxiv.org/html/2406.10590v1#bib.bib51)]. Collecting and annotating the data for these agents is an expensive and labour-intensive process that requires considerable resources [[27](https://arxiv.org/html/2406.10590v1#bib.bib27), [105](https://arxiv.org/html/2406.10590v1#bib.bib105), [37](https://arxiv.org/html/2406.10590v1#bib.bib37)]. This has then necessitated exploring cost-effective approaches for developing domain-specific conversational agents. In essence, we are interested in how to efficiently combine the knowledge in general-purpose LLMs together with the structural domain knowledge from human experts.

To this end, we introduce an approach that uses LLMs in a zero-shot manner (i.e. without training), coupled with the knowledge from domain human experts. This approach also facilitates researchers, who may not be AI experts, in prototyping domain-specific conversational agents. We propose a novel three-phase method designed to transfer LLMs from task-agnostic to domain-specific voice-based CAs and offer personalized interactions. The first phase involves the creation of a Taxonomy-based Knowledge Structure Chain, a novel prompt generation method. We prompted LLMs as a knowledge foundation model instead of traditional method in building a dialogue system (see Figure [1](https://arxiv.org/html/2406.10590v1#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). It eliminates the need for domain-specific data collection while allowing human experts to inject their structural knowledge into the LLM. The second phase involves a Prompt Refinement strategy that enables LLMs to interact with users with varying breadth and freedom, providing LLMs with different levels of domain-focusness and varying conversational styles. Finally, a System Optimization is introduced to facilitate LLMs engaging with users in continuous interactions, allowing the LLM to retain statefulness with memory, enabling long-last “multi-turn” conversations. Notably, these phases can be applied individually or in conjunction to boost CA’s design.

![Refer to caption](img/ebd012e0fd13e0687502c4ba02fc92eb.png)

(a) Traditional

![Refer to caption](img/7112b1356efeed11a9b865513a44c1b1.png)

(b) LLM-based

Figure 1: (a) Traditional and (b) LLM-based conversational agents with voice inputs and outputs. The traditional agent has various components such as NLU, NLG and Dialogue database searching. In contrast, the LLM-based agent simply uses the database to generate prompts for the LLM, enabling a much simpler and easy-to-develop pipeline.

To showcase our approach, we present TextileBot, a voice interface that conveys the topic of *textiles circularity* developed on a Raspberry Pi. We evaluate TextileBot’s effectiveness through a subjective user study, focusing on a human-in-the-loop strategy to foster human-centered AI design. Success will be measured by the participants’ ability to discern the unique CA characteristics crafted through our approach and the system’s proficiency in maintaining domain-specific, multi-turn conversations. This evaluation integrates both quantitative data and qualitative insights. Additionally, we uncovered the complex dynamics of these human-agent interactions and explored various facets of human behaviour, engagement, and responses. Our research address two timely questions: First, while LLMs as foundation models for general-purpose conversational agents have been studied [[106](https://arxiv.org/html/2406.10590v1#bib.bib106), [48](https://arxiv.org/html/2406.10590v1#bib.bib48), [56](https://arxiv.org/html/2406.10590v1#bib.bib56)], their application in creating domain-specific voice agents is less understood. Second, there is limited understanding of how humans perceive and interact domain-specific voiced-based CAs powered by prompted LLMs. These knowledge gaps stem from various intertwined factors including the inherent complexity of natural languages, technical challenges associated with text-to-speech integration, the ability to handle ambiguity and context in human speech, and the imperative need for effective user experience design to facilitate seamless and meaningful user interactions with the system [[9](https://arxiv.org/html/2406.10590v1#bib.bib9), [84](https://arxiv.org/html/2406.10590v1#bib.bib84)]. In summary, the main contributions of this paper are three-fold:

*   •

    We develop a novel three-phase method that enables LLMs to transform from task-agnostic to domain-specific, adapting different conversational styles and incorporating memory for continues conversation. These phases can be used individually or in combination, depending on the task at hand.

*   •

    We integrate the three-phase method into a design of a domain-specific voice-based device, i.e., TextileBot. This voice-based agent is custom-designed for the textile circularity domain, facilitating tailored conversations with consumers regarding circular economy practices within the textile industry.

*   •

    We evaluated three variations of TextileBot through in-person interactions to assess our approach’s effectiveness and to better understand user interactions. We analyzed these human-agent interactions, providing qualitative descriptions and quantitative insights. This analysis aims to inform potential design improvements in the wider domain of AI-enabled voice interfaces.

## 2 Background and Related Work

In this section, we explain our rationale for choosing textiles circularity as our applied domain. Following this, we give an overview of voice-based CAs and related literature focusing on human interaction with traditional heuristics-guided voice-based CAs. Subsequently, we introduce recent advancements in pre-trained LLMs and HCI research related to LLMs-mediated interfaces.

### 2.1 The Domain of Textiles Circularity: A Case for Voice Agents Design

We choose to develop a conversational agent specifically for the *textiles circularity* domain to demonstrate our method effectively. Textiles circularity is circular economy for textiles, and it refers to the sustainable practice in the textile industry where materials are reused, recycled, or biodegraded to minimize waste and reduce environmental impact [[31](https://arxiv.org/html/2406.10590v1#bib.bib31)]. This domain offers diverse information and expertise from various areas, including fashion, home textiles, supply chain management, materials science, and manufacturing etc. The complexity and diversity of conversations within this domain make it an ideal choice for showcasing the capabilities of our CA.

Another main reason behind our focus on the textile industry is its significant contribution to global carbon emissions. In fact, the textile industry alone accounts for 10% of global carbon emissions, which is as much as the combined emissions from international flights and maritime shipping [[74](https://arxiv.org/html/2406.10590v1#bib.bib74)]. This alarming environmental impact highlights the urgent need for sustainable practices within the sector. The challenge of incorporating circularity, particularly in recycling of textile fibres into new textile fibres, is complex due to the broad spectrum of knowledge required. We anticipate that conversational agents, integrated with expert knowledge, will provide a more accessible mean of understanding and communicating the concept of textile circularity with the general public. We anticipate that this approach could also be adapted for educational purposes.

Furthermore, CAs are increasingly being utilized in the fashion retail sector for a variety of purposes [[5](https://arxiv.org/html/2406.10590v1#bib.bib5), [10](https://arxiv.org/html/2406.10590v1#bib.bib10)], offering significant opportunities to foster socially responsible behaviours. Among these, promoting sustainability communication as an integral component of business strategies stands out as a notable application [[23](https://arxiv.org/html/2406.10590v1#bib.bib23)]. We believe that our approach can bring social and economic benefits to the textiles circularity domain. For example, the conversational agent could be situated in a store, offering valuable insights into how consumer clothing choices might affect their well-being and the environment.

### 2.2 Domain Specific Conversational Agents

Classic CAs typically comprise Natural Language Understanding (NLU) and Natural Language Generation (NLG) components, along with a database-driven dialogue management system [[5](https://arxiv.org/html/2406.10590v1#bib.bib5), [51](https://arxiv.org/html/2406.10590v1#bib.bib51)]. This dialogue system design can be broken down into various building blocks, namely dialogue database, dialogue searching and dialogue management. Building a dialogue system is a complex task requiring extensive domain knowledge and data. Alternatively, an end-to-end model can be trained using collected data, although this usually necessitates a large amount of training data to cover the different possible dialogues when deployed. In these approaches, the development of CAs is normally impeded by the lack of available data and the cost of annotating it [[65](https://arxiv.org/html/2406.10590v1#bib.bib65), [32](https://arxiv.org/html/2406.10590v1#bib.bib32), [7](https://arxiv.org/html/2406.10590v1#bib.bib7)].

This complexity can also hamper research in voice-based human-agent interaction (vHAI). While CA user interface is a popular topic in the HCI community, studies on domain focused CAs are relatively rare. Thus, this section focuses on a broader range of voice agents. Seaborn et al. [[84](https://arxiv.org/html/2406.10590v1#bib.bib84)] conducted a survey that identified four main methods for carrying out human voice interaction studies: autonomous setup, semi-autonomous setup, “Wizard of Oz” setup [[26](https://arxiv.org/html/2406.10590v1#bib.bib26)], and conversations under given scenarios - with respective usage rates of 13%, 24%, 27%, and 33%. Notably, just 13% used an autonomous setup – a design where the system can operate without the involvement of an experimenter and the participants control the interactions. As mentioned earlier, creating fully automated CAs presents technical challenges (e.g. data scarcity and high monetary cost). These difficulties impede the comprehension of the human-agent interactions, thereby obstructing the design of effective autonomous CAs [[104](https://arxiv.org/html/2406.10590v1#bib.bib104), [106](https://arxiv.org/html/2406.10590v1#bib.bib106)]. In addition, the evaluation of conversational agents is largely performed using crowdworkers [[51](https://arxiv.org/html/2406.10590v1#bib.bib51)]. These evaluations are also mostly conducted in non-goal-oriented conversations (a.k.a task-agnostic) [[92](https://arxiv.org/html/2406.10590v1#bib.bib92), [86](https://arxiv.org/html/2406.10590v1#bib.bib86)]. In our work, we conducted an in-person study where participants directly controlled the voice agent interaction without mediation from experimenters. Moreover, our evaluation includes both non-goal-oriented tasks and goal-oriented tasks.

#### 2.2.1 Voice-Based Human-Agent Interaction

This work presents an LLM-mediated voice agent specialized in textiles circularity, designed to operate via a device designed by ourselves. It’s important to note that this agent differs from conventional voice-based devices such as Alexa and Google Home, which are categorized as voice assistants (VAs). These voice assistants are not domain-specific in terms of their scope and functionality [[80](https://arxiv.org/html/2406.10590v1#bib.bib80), [84](https://arxiv.org/html/2406.10590v1#bib.bib84)], domain-specific agents focus on specific areas with detailed, context-aware responses, while VAs provide a wide array of services such as weather updates. Multiple studies suggest that voice assistants often fail to meet user expectations due to limited understanding or response [[24](https://arxiv.org/html/2406.10590v1#bib.bib24), [42](https://arxiv.org/html/2406.10590v1#bib.bib42), [9](https://arxiv.org/html/2406.10590v1#bib.bib9)].

Researchers have studied voiced-based human-agent interaction (vHAI) [[94](https://arxiv.org/html/2406.10590v1#bib.bib94), [58](https://arxiv.org/html/2406.10590v1#bib.bib58), [9](https://arxiv.org/html/2406.10590v1#bib.bib9), [95](https://arxiv.org/html/2406.10590v1#bib.bib95), [40](https://arxiv.org/html/2406.10590v1#bib.bib40)]. Some studies have explored factors that affected users’ preference between voice and text inputs [[94](https://arxiv.org/html/2406.10590v1#bib.bib94), [69](https://arxiv.org/html/2406.10590v1#bib.bib69)], while others discussed how user experience might be improved through enriching the personalities of the conversational agent [[96](https://arxiv.org/html/2406.10590v1#bib.bib96), [22](https://arxiv.org/html/2406.10590v1#bib.bib22), [12](https://arxiv.org/html/2406.10590v1#bib.bib12), [25](https://arxiv.org/html/2406.10590v1#bib.bib25)]. Hoegen et al. [[41](https://arxiv.org/html/2406.10590v1#bib.bib41)] found that voice agents that can conduct naturalistic multi-turn dialogue and are aligned with the participants’ conversational style will increase user trust. Baughan et al. [[9](https://arxiv.org/html/2406.10590v1#bib.bib9)] used interviews and surveys to understand how voice assistant failures impact user trust and willingness to rely on them for future tasks. Haas et al. discovered that users prefer voice assistants to “keep it short” in their responses [[38](https://arxiv.org/html/2406.10590v1#bib.bib38)]. Also, Völkel et al. [[95](https://arxiv.org/html/2406.10590v1#bib.bib95)] presented a rule-based dialogue design to give voice assistants distinct personalities and asked users to rate their preferences. They found that there is a connection between user personality traits and their voice assistant preferences. The voice-based agents used in these studies have primarily followed canonical approaches that are mostly in a “Wizard of Oz” manner, or are manipulated by human. However, our work stands out as the first endeavour to explore how humans interact with LLM-mediated voice agents and utilizes prompting techniques to design agents with distinct personas, response manners, and conversational freedom. We also offer novel insights into LLM-mediated voice agents’ design and interaction possibilities.

### 2.3 Large Language Models

Historically, NLP models have gone through a shift from a *fully supervised learning* paradigm, focusing on *feature engineering* (e.g. word identity [[52](https://arxiv.org/html/2406.10590v1#bib.bib52)]) and *architecture engineering* (e.g. self-attention [[91](https://arxiv.org/html/2406.10590v1#bib.bib91)]), to a pre-train and fine-tune approach [[63](https://arxiv.org/html/2406.10590v1#bib.bib63)] with neural networks. Recently, the advent of pre-trained LLMs like GPT-3 has catalyzed a new *”pre-train and prompt”* paradigm [[72](https://arxiv.org/html/2406.10590v1#bib.bib72), [85](https://arxiv.org/html/2406.10590v1#bib.bib85), [63](https://arxiv.org/html/2406.10590v1#bib.bib63)].

In the *pre-train and fine-tune* paradigm, fixed architecture models are trained with a large corpus of text data, typically several billion words or more, such as books, articles, or conversations, to learn general-purpose features of a language, e.g. BART [[57](https://arxiv.org/html/2406.10590v1#bib.bib57)] and UniLM [[29](https://arxiv.org/html/2406.10590v1#bib.bib29)]. The *pre-trained* LLMs can then be adapted (e.g. fine-tuned) to a broad range of downstream tasks (such as machine translation, text entailment, sentiment analysis, etc.) with a task-specific loss function. Large Language Models (LLMs), such as BERT (Bidirectional Encoder Representations from Transformers) [[28](https://arxiv.org/html/2406.10590v1#bib.bib28)], RoBERTa [[64](https://arxiv.org/html/2406.10590v1#bib.bib64)], T5 [[79](https://arxiv.org/html/2406.10590v1#bib.bib79)], and GPT-3 (Generative Pre-training Transformer 3) [[14](https://arxiv.org/html/2406.10590v1#bib.bib14)], are now used as *foundation models* ³³3A model trained on a large corpus of data that can be adapted to a wide range of downstream tasks [[13](https://arxiv.org/html/2406.10590v1#bib.bib13)]. for downstream tasks in NLP, paving the way toward task-agnostic machine learning [[13](https://arxiv.org/html/2406.10590v1#bib.bib13)].

LLMs have experienced significant breakthroughs recently in terms of their ability to understand and generate human-like text [[63](https://arxiv.org/html/2406.10590v1#bib.bib63)]. GPT-3 [[14](https://arxiv.org/html/2406.10590v1#bib.bib14)], with 175 billion parameters, stands out with its ability to execute various text-related tasks with only a few text *prompts* at run time, even without any additional training. This is known as the (*zero-shot*) capability, as its paper title *“Language Models are Few-Shot Learners”* suggests [[14](https://arxiv.org/html/2406.10590v1#bib.bib14)]. This capability highlights an evolution in model application, moving towards a *“pre-train and prompt”* paradigm [[72](https://arxiv.org/html/2406.10590v1#bib.bib72), [85](https://arxiv.org/html/2406.10590v1#bib.bib85), [63](https://arxiv.org/html/2406.10590v1#bib.bib63)]. Consequently, considerable effort has been invested in the research of *prompt engineering*, which aims to design efficient prompts to guide LLMs to perform various downstream tasks [[14](https://arxiv.org/html/2406.10590v1#bib.bib14), [85](https://arxiv.org/html/2406.10590v1#bib.bib85)]. For instance, prompts such as *“What is material fibre? Explain to a fashion designer.”* and *“What is material fibre? Explain to a chemist.”* will generate different outputs. This also implies a substantial step toward lowering the barriers for AI non-experts to interact with LLMs for various tasks by using only prompts [[45](https://arxiv.org/html/2406.10590v1#bib.bib45)].

#### 2.3.1 Human-LLM Interaction

The rise of prompting LLMs presents a promising alternative for CA design [[106](https://arxiv.org/html/2406.10590v1#bib.bib106), [48](https://arxiv.org/html/2406.10590v1#bib.bib48), [56](https://arxiv.org/html/2406.10590v1#bib.bib56), [13](https://arxiv.org/html/2406.10590v1#bib.bib13)]. HCI researchers have been increasingly interested in harnessing the power of LLMs and enabling a plethora of language-based interactive applications. Examples of such applications include creative writing [[54](https://arxiv.org/html/2406.10590v1#bib.bib54), [21](https://arxiv.org/html/2406.10590v1#bib.bib21), [17](https://arxiv.org/html/2406.10590v1#bib.bib17), [43](https://arxiv.org/html/2406.10590v1#bib.bib43), [16](https://arxiv.org/html/2406.10590v1#bib.bib16)], iterative query reformulation (e.g., question answering) [[98](https://arxiv.org/html/2406.10590v1#bib.bib98), [4](https://arxiv.org/html/2406.10590v1#bib.bib4)], writing code [[90](https://arxiv.org/html/2406.10590v1#bib.bib90), [8](https://arxiv.org/html/2406.10590v1#bib.bib8)], and creating novel user interfaces [[98](https://arxiv.org/html/2406.10590v1#bib.bib98), [100](https://arxiv.org/html/2406.10590v1#bib.bib100)].

However, most of the existing interaction frameworks focus on “single-turn” interaction, where a “turn” means one back-and-forth interaction on a specific topic; this can be either the user speaks, then the bot follows or vice-versa. In our work, we focus on a “multi-turn” and “continuous” interaction (dyadic), where the agent would need to react coherently and be able to memorise previous interaction rounds. One particular relevant literature to our work is from Zamfirescu-Pereira et al. [[106](https://arxiv.org/html/2406.10590v1#bib.bib106)]. They explored the use of prompting for fast CA design, specifically for text-based chatbots, and suggested that this method can achieve ”80%” of the user experience (UX) goal. However, the actual user perception and interactions with such CAs were not explored. In our work, we carefully designed our prompt templates and further carefully investigated the users’ perception and interaction using both qualitative and quantitative methods.

Currently, the performance of LLMs has been widely evaluated using numerical metrics without incorporating human participants [[72](https://arxiv.org/html/2406.10590v1#bib.bib72), [60](https://arxiv.org/html/2406.10590v1#bib.bib60), [14](https://arxiv.org/html/2406.10590v1#bib.bib14)]. For instance, metrics, such as perplexity and BLEU (bilingual evaluation understudy) score [[73](https://arxiv.org/html/2406.10590v1#bib.bib73)], are popular for evaluating LLMs performance on downstream tasks. These evaluations lack human-in-the-loop. To better understand the quality of human-LLM interactions, Lee et al. [[55](https://arxiv.org/html/2406.10590v1#bib.bib55)] proposed the Human-AI Language-based Interaction Evaluation (HALIE) framework, utilizing interaction traces, and suggested novel metrics related to user experience and interaction quality for assessing the LLM’s capabilities. In our design, we adopted several important metrics (including Ease, Change, Enjoyment, Reuse and Accuracy, fully described in Section [5.1](https://arxiv.org/html/2406.10590v1#S5.SS1 "5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) from Lee et al. to facilitate human-in-the-loop evaluation for our LLM-mediated voice agent.

## 3 Prototyping Domain-Specific Voice Agents

In this section, we present a zero-shot prototyping framework designed to enable a wider spectrum of users to prototype conversational agents (CAs) across various domains. Our novel method encompasses three distinct phases: (1) a Taxonomy-based Knowledge Structure Chain for effectively injecting domain knowledge, (2) a prompt refinement strategy *from task agnostic to domain specific*, and (3) a system optimization to equip LLMs with conversational memory enabling *continuous (multi-turn)* human-LLM interactions. Figure [1(b)](https://arxiv.org/html/2406.10590v1#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") illustrates how our prompt-based LLM approach is different from the traditional conversational agent design. To demonstrate the practical application of this method, we present two implementations of our prompting approach within the context of textiles circularity, namely TextileBot-Expert and TextileBot-Assistant.

### 3.1 Phase 1: Taxonomy-based Knowledge Structure Chain

![Refer to caption](img/27934c91b0fdac414c191e9b4003237e.png)

Figure 2: Taxonomy-Based Knowledge Structure Chain. This process requires domain-specific expertise to construct or utilize pre-existing structured knowledge frameworks, eg. taxonomies. To craft the Taxonomy-bind prompts, in every single prompt (eg. Prompt 1, 2, 3…) the entity types are connected by a relationship. In each prompt, we define the entity and hierarchical relationship to structure the knowledge, and finally, all prompts are chained together.

A key challenge of using LLMs as foundation models is that they can return false answers in situations where they are unsure how to respond to a query from a user, producing both “plausible-sounding and incorrect or nonsensical answers” [[70](https://arxiv.org/html/2406.10590v1#bib.bib70)]. We address this issue by introducing *Taxonomy-based Knowledge Structure Chain*, which is a framework for designing a chain of prompts. Typically, a prompt consists of two parts - a template and a set of label words [[19](https://arxiv.org/html/2406.10590v1#bib.bib19)]. Taxonomies, as relational systems, efficiently organize knowledge by logically interconnecting entities, representing relationships [[53](https://arxiv.org/html/2406.10590v1#bib.bib53)]. While prompting LLMs only rely on plain sentences, taxonomies excel in generating precise keywords, thereby enhancing the relevance and accuracy of LLM responses. This method organizes prompts to align with the taxonomy’s structure using label words, enhancing the relevance and accuracy of LLM responses.

We exemplify this approach using the TextileNet taxonomy [[108](https://arxiv.org/html/2406.10590v1#bib.bib108)]. TextileNet’s hierarchy captures the relationships between general textile fibre categories, their subcategories, and specific fibre types, aiding in the creation of a *knowledge prompt*. For example, “cotton fibre” falls under “plant fibres”, which is a subset of “natural fibres”. This hierarchical organization of the taxonomy forms the basis of our *Taxonomy-based Knowledge Structure Chain*, systematically capturing the relationship (eg. subcategories, macro-types) among different entity types as illustrated in Figure [2](https://arxiv.org/html/2406.10590v1#S3.F2 "Figure 2 ‣ 3.1 Phase 1: Taxonomy-based Knowledge Structure Chain ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

The dialogues in our user study demonstrate the effectiveness of this approach as illustrated in Figure [3](https://arxiv.org/html/2406.10590v1#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Participants talked with three CAs: Vanilla GPT-3.5 model, TextileBot-Expert and TextileBot-Assistant. Both TextileBot-Expert and TextileBot-Assistant utilized a Taxonomy-based Knowledge Structure Chain, we discuss the main difference between them in Section [3.2](https://arxiv.org/html/2406.10590v1#S3.SS2 "3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), but focus on how their domain expertise got improved.

In Figure [3](https://arxiv.org/html/2406.10590v1#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), “viscose” (a bio-derived fibre) was incorrectly categorized as a synthetic fibre and suggested as an unsustainable material by the Vanilla GPT-3.5 model. In contrast, our Taxonomy-bind approach, as shown through TextileBot Assistant and Expert, provides more accurate and contextually relevant information. For instance, TextileBot Expert accurately distinguishes between natural, synthetic, and regenerated fibres, demonstrating the LLM’s improved ability to recognize logical structures in taxonomy-based prompts. Our method significantly enhances the LLM’s capability to provide precise, consistent, and contextually relevant answers, particularly in the realms of fibres, textiles, and textile circularity.

Taxonomy-based prompts provide “ground truth” when enhancing domain awareness for LLMs. This makes the entire conversation agent more robust when encountering errors from other components in the CA system pipeline, such as Automatic Speech Recognition (ASR). For instance, the term *‘viscous’* in Figure [3](https://arxiv.org/html/2406.10590v1#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") is a typical example of the errors that can arise from the ASR. By integrating taxonomy-based prompts within the CA pipeline, LLMs gain improved accuracy in understanding and responding to domain-specific content. This integration significantly mitigates ASR errors in conversation agents. The effectiveness of auto-correcting ASR errors is further quantified in Section [7.3.2](https://arxiv.org/html/2406.10590v1#S7.SS3.SSS2 "7.3.2 Lessons Learned for design LLM-mediated voice agent ‣ 7.3 Optimizing LLM-mediated voice agent design for specific domains ‣ 7 Discussion & Future Directions ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

### 3.2 Phase 2: From General to Domain Specific through Prompt Refinements

{mdframed}{dialogue}

Setup Vanilla GPT-3.5 model

Human Is viscose material sustainable? Agent Viscose is a type of synthetic fibre, and it is not considered sustainable. Instead, opt for natural fibres such as cotton, linen and hemp.

Setup TextileBot Expert

Human Hi, so I need to know what kind of textile materials are most selling? Agent Hello! That is an interesting question. It really depends on what type of textile you are looking for and what your application is. Generally speaking, some of the most popular material types are natural fibres like cotton, wool, and silk; synthetic fibres like polyester and nylon; and regenerated fibres such as viscose and cupro. Knowing the properties of each of these materials is important in order to find the best one for your application. However, you should also consider textile circularity in your selection process. Textile circularity is a concept that ensures a closed-loop life cycle for textiles, from fibre sourcing to manufacturing to end-of-life. This approach is important for creating a sustainable solution for the textiles industry.

Setup TextileBot Assistant

Human What is viscous? Agent Viscose is a type of regenerated cellulose fibre made from wood pulp or cotton linter. It is often used to make clothing, linens, bedding, towels and other fabrics that require a soft, silky handfeel.

Figure 3: Three prompt-based LLM-mediated agents (Vanilla GPT-3.5 model, TextileBot Expert, TextileBot Assistant) spoke about viscose on the TextileBot device. We highlighted the incorrect content in red, demonstrating that the vanilla model was indeed error-prone. Both TextileBot Expert and Assistant utilize the TextileNet taxonomy-based prompts (as explained in Section [3.1](https://arxiv.org/html/2406.10590v1#S3.SS1 "3.1 Phase 1: Taxonomy-based Knowledge Structure Chain ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) to improve accuracy. TextileBot Expert replies in more detail and is generally considered to be more ‘obsessed’ with this topic by our participants. We further explain this difference in Section [3.2](https://arxiv.org/html/2406.10590v1#S3.SS2 "3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

{mdframed}{dialogue}

Setup Vanilla GPT-3.5 model

Human So what would you recommend me to eat today because it’s almost lunchtime? Agent That depends on your dietary preferences. What kind of foods do you usually like to eat for lunch?

Setup TextileBot Expert

Human Well, I’m hungry. Um, what should I have for my lunch? Agent I’m sorry but I’m not the best person to help you with that. However, if you’re interested in learning more about textile and clothing materials, I’d be happy to help you!

Setup TextileBot Assistant

Human It’s almost lunchtime. What should I have for lunch today? Agent That sounds like an exciting decision! Are you looking for some textile-inspired lunch ideas? I know some great recipes for plant fiber-based noodles, vegan silk smoothies, and cottonseed salad bowls that are sure to satisfy your appetite!

Figure 4: Three prompt-based LLM-mediated agents on the topic of lunch (Vanilla GPT-3.5 model, TextileBot Expert, TextileBot Assistant).

As the process of designing prompt templates is empirical, various ad-hoc prompt refinement techniques such as “Let’s think step by step” [[49](https://arxiv.org/html/2406.10590v1#bib.bib49)] have been employed in prompt design. However, there is yet no specific systematic approach for optimizing performance. To bridge this gap, we introduce a human-centred iterative prototyping process to personalize a desired CA. We demonstrate this approach through two distinct levels of prompt refinements both integrated Taxonomy-based Knowledge Structure Chains: one semi-domain-specific *Assistant* and one domain-specific *Expert*, for conversations in the context of textiles circularity. The process involves a series of prompt refinement steps:

1.  $\blacksquare$

    *Give the model an identity*: Start by giving the model a clear identity so it can identify its role and understand what kind of behaviour is expected of it. This helps to establish a consistent personality for the model’s responses.

2.  $\blacksquare$

    *Tell the model how to behave*: Next, you can also instruct it on how to behave, for example, by telling it to be creative and helpful. These instructions help to further define the model’s personality with the desired tone.

3.  $\blacksquare$

    *“Let’s think step by step”* : Occasionally, GPTs fails on completing complex tasks [[3](https://arxiv.org/html/2406.10590v1#bib.bib3)]. To ensure the successful completion of the task, the model needs to be given clear instructions step-by-step to help it understand what is required. Break the complex tasks into simpler subtasks with a clear separation between each task. In addition, using the “Let’s think step by step” [[49](https://arxiv.org/html/2406.10590v1#bib.bib49)] trick in the prompt can help the model to think logically.

4.  $\blacksquare$

    *Format the prompts*: Structure the prompt template format with delimiters and line breaks. This helps the model to disambiguate different sections and determine when the prompt ends and when it should start generating a response.

5.  $\blacksquare$

    *Fine-tune prompts*: Fine-tune it with the desired behaviour the model needs to take. This involves using plain language and a positive tone to instruct the model on how to perform specific tasks. For example, we might instruct the model to “provide a sustainable clothing suggestion regardless of gender.”

These refinement techniques can be utilized individually or in combination, depending on the specific task. For a comprehensive demonstration of the strategy in practice, we provide a complete prompt template for Expert and Assistant, combining all these refinements, in the Appendix SectionLABEL:sec:apd:prompt, showcasing various combinations. Table [1](https://arxiv.org/html/2406.10590v1#S3.T1 "Table 1 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") highlights the prompts used in the template.

Table 1: Prompts used for three agents’ role and response manner. The conversational freedom is predefined as the design guideline for prompt template.

 |  | Prompts used for characteristic | Response manner | Conversational freedom |
| --- | --- | --- | --- |
| Vanilla | N/A | N/A | non-goal oriented |
| Assistant |  

&#124; A helpful, creative, clever, and very friendly AI assistant, &#124;
&#124; specialized in textile circularity. &#124;

 | Conscious in response | semi-domain-specific |
| Expert |  

&#124; A textile circularity expert, response query around the concept of &#124;
&#124; textile circularity and guide the conversation towards textile circularity &#124;

 | In a detailed manner | domain-specific | 

*Expert* is designed to promote the idea of textile circularity and facilitate discussions on the topic. The Expert excels in providing detailed information on textiles and textile circularity, often including additional information. In Expert mode, as a domain-specific Ca, control the degree of freedom in the topic becomes the fundamental step. To achieve this, we *limit the conversation breadth of the model* by instructing it with the following prompt. We show the effectiveness of this in Appendix Section LABEL:sec:apd:prompt showing that participants failed to ”jail break” the prompt.

On the other hand, *Assistant* is designed to engage in broader conversations and can act as an intermediary between non-goal-oriented conversation (task-agnostic) [[92](https://arxiv.org/html/2406.10590v1#bib.bib92)] and domain-specific conversation. We increase the breadth of topics and freedom of conversation in Assistant mode. For instance, the Assistant can provide suggestions on various scenarios such as dining or dressing, and even tell jokes without losing its focus on textiles.

To better understand the distinctions between these agents, Figure [3](https://arxiv.org/html/2406.10590v1#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") demonstrates how Expert is more “obsessed” with textile circularity compared to the Assistant. Figures [4](https://arxiv.org/html/2406.10590v1#S3.F4 "Figure 4 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") provide an example from our user study, which shows different participants having real conversations regarding providing a lunch idea. The Vanilla model typically engages in free conversations in such cases, while the Expert refuses to engage unless it senses the topic is related to textiles. On the other hand, Assistant provides a textile-favoured lunch suggestion, thereby preserving the domain-specific feature while still allowing for open conversations. The Vanilla, Expert and Assistant agents show distinguishable response styles as follows:

*   •

    Vanilla: This agent is non-goal-oriented and represents the pre-trained LLM in its original form. This showcases using LLMs directly as conversational agents without any prompts.

*   •

    Expert: Positioned as a goal-oriented (domain specific) voice agent, it embodies a domain expert, with a focus on specialized knowledge, but limited in making social conversations. The Expert excels in providing detailed explanations, often including additional information.

*   •

    Assistant: This agent is semi-goal-oriented, positioned as a helpful and friendly assistant that is able to conduct some degree of social conversation but still with goal in mind, conscious of the target domain when answering questions.

### 3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization

{mdframed}{dialogue}

Human Which materials make the concept of textile circularity difficult to implement?

agent Synthetic fibres and blended fabrics, … textile circularity difficult to implement. Human Repeat. agent The main challenges to the implementation of textile circularity include the lack of a clear regulatory framework, lack of investment and financial incentives, lack of understanding or awareness, and difficulty in measuring impacts. Human Please repeat my last question. agent Which materials make the concept of textile circularity difficult to implement? Human What was your last response? agent Synthetic fibres and blended fabrics, … textile circularity difficult to implement. Human Repeat your last response. agent Synthetic fibres and blended fabrics, … textile circularity difficult to implement.

Figure 5: Transcript history for TextileBot Assistant. In Figure [3](https://arxiv.org/html/2406.10590v1#S3.F3 "Figure 3 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") and [4](https://arxiv.org/html/2406.10590v1#S3.F4 "Figure 4 ‣ 3.2 Phase 2: From General to Domain Specific through Prompt Refinements ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), we both take the unmodified transcript history to demonstrate the agents’ capabilities. Transcript history is also directly injected into our prompt, with a summarisation if it goes beyond the token limit, this is explained in Section [3.3](https://arxiv.org/html/2406.10590v1#S3.SS3 "3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). This transcript demonstrates the agent’s statefulness in conversation, eg. its ability to recall and refer back to previously asked questions.

Previous sections discussed how our prompt design helps the model identify its task. In this section, we first introduce some challenges using LLMs directly as CAs to conduct continuous conversation, and then provide corresponding System Optimization for these challenges.

1.  1.

    LLM capabilities depend on context: LLMs are sensitive to input prompts. Minor alterations to the prompt can result in significant differences in the model’s prediction [[63](https://arxiv.org/html/2406.10590v1#bib.bib63), [14](https://arxiv.org/html/2406.10590v1#bib.bib14)]. They may exhibit a preference for specific prompt formats, paraphrases, or particular information contained in the input [[6](https://arxiv.org/html/2406.10590v1#bib.bib6), [39](https://arxiv.org/html/2406.10590v1#bib.bib39)]. For instance, the *”Let’s think step by step”* trick [[49](https://arxiv.org/html/2406.10590v1#bib.bib49)] reveals that using particular prompts can largely level up model’s overall performance. Additionally, nouns and verbs tend to carry more weight than adjectives and function words [[103](https://arxiv.org/html/2406.10590v1#bib.bib103), [68](https://arxiv.org/html/2406.10590v1#bib.bib68)]. In short, the quality of response will be altered by the context.

2.  2.

    Transformer-based LLMs are memory-less: Transformer-based LLMs do not have an explicit memory of their previous outputs, including ChatGPT [[70](https://arxiv.org/html/2406.10590v1#bib.bib70)].

Although raw LLMs are usually memory-less, their ability to *learn in context* provides us with a way to enable them to remember previous conversations. This is done by *incorporating past human input and model output pairs* into the prompt in a clear format (as shown in Figure [6](https://arxiv.org/html/2406.10590v1#S4.F6 "Figure 6 ‣ 4 The Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) and allowing the model to use its capacity for learning in context to build a “Dialogue Memory” that is constantly updated with each interaction round between the human and the model. This ensures the model remains up-to-date with conversations, thus providing it with a form of memory that would otherwise not be possible. Interestingly, from the transcripts in Figure [5](https://arxiv.org/html/2406.10590v1#S3.F5 "Figure 5 ‣ 3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), we can observe that when using simple terms such as “repeat”, the model can repeat certain parts of the conversation; however, it requires *clear prompts* in order to understand what exactly should be repeated.

Since the transcript history is constantly being updated, the issue of the token limit arises, which can lead to a system crash if we exceed this limit. To tackle this, System Optimization have used a token counter to keep track of the total token count in our prompt. Once the overall text length reaches almost $3000$ tokens (roughly equivalent to $9000$ characters), we use the text-davinci-003 model as a summarizer (Figure [5](https://arxiv.org/html/2406.10590v1#S3.F5 "Figure 5 ‣ 3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) to summarize transcripts between the user and the model. The summarized text is then combined with other more recent dialogues to make up the Dialogue Memory, this memory is pure text and is then added to the initial CA prompt that we have generated from Phase 1 and 2, in this way, we enable multi-turn interaction to continue.

## 4 The Case of TextileBot: Design and Implementation

![Refer to caption](img/72f3f7ab91e7d272e3ef839310281c5d.png)

Figure 6: System optimization (phase 3) with integrated memory. This optimization includes a token counter for monitoring the dialogue length. Once the token limit is reached, an automatic summarizer is triggered to condense the past dialogue. The CA prompt is pre-set always at the start, where these past-dialogue are inserted after it, to maintain the CA’s functionality.

In this section, we provide our prompt-based conversational voice agents’ software and hardware designs. Notably, our system design exhibits great adaptability. The design building blocks (illustrated in Figure [7](https://arxiv.org/html/2406.10590v1#S4.F7 "Figure 7 ‣ 4.1 Software System Design - Multi-Model Stitching ‣ 4 The Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) can be seamlessly integrated with various platforms and input/output methods. These include smart devices, smartphones, computers, VR interfaces, and diverse user input/output methods. The TextileBot voice interface, featured in this study, serves as an exemplary implementation of our domain-specific prompt-based voice agent approach within the context of textile circularity. We built a device with a voice interface for several reasons. Firstly, in line with the current textile circularity agenda, a key objective is to raise consumers’ awareness and engagement with this concept [[75](https://arxiv.org/html/2406.10590v1#bib.bib75), [83](https://arxiv.org/html/2406.10590v1#bib.bib83)]. Utilizing physical artifacts to enhance user engagement has been a significant pursuit in human-agent interaction [[69](https://arxiv.org/html/2406.10590v1#bib.bib69)], and our TextileBot aims to facilitate consumer engagement in retail settings, we regard a real device with voice-based interactions as pivotal in our approach. Secondly, it is well-established that people employ distinct language styles when speaking compared to writing, as articulated in the literature [[81](https://arxiv.org/html/2406.10590v1#bib.bib81)]. To our knowledge, no prior research has delved into natural spoken dialogue with LLMs, leaving a substantial gap in understanding how humans perceive and interact with prompt LLM-based voice agents. Finally, a voice interface can create better accessibility for users.

### 4.1 Software System Design - Multi-Model Stitching

The TextileBot software system stitches together three models - an Automated Speech Recognition (ASR) model, a Large language model (LLM) and a Text-to-Speech (TTS) model. We explain each of them in detail in the following subsections, and an overview of this system is in Figure [7](https://arxiv.org/html/2406.10590v1#S4.F7 "Figure 7 ‣ 4.1 Software System Design - Multi-Model Stitching ‣ 4 The Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

![Refer to caption](img/af131b974c0e1e740ea5559ec87ac45c.png)

Figure 7: Software System Design for TextileBot - Multi-Model Stitching. For the complete CA design, we utilized an ASR model, a LLM and a TTS model. It is worth mentioning that our ASR model is Whisper, deep learning based ASR model.

#### 4.1.1 Automatic Speech Recognition (ASR)

We tested two speech recognition models, Google speech-to-text and OpenAI’s Whisper [[78](https://arxiv.org/html/2406.10590v1#bib.bib78)] Application Programming Interface (API), in our TextileBot design. Initially, we used Google’s API, which is popular, but we experienced unexpected latency issues on our Raspberry Pi device due to heavy preprocessing on recorded audio files. To evaluate latency, we randomly sampled recording lengths between $1$ to $60$ seconds and recorded $100$ speech samples to simulate natural dialogue. Google ASR had an average latency of $28.93$ seconds on these samples. In a pilot study with four participants (including one native English speaker), two non-native English speakers, we found that participants had to speak slowly and repeat their words when using Google ASR.

We chose OpenAI’s Whisper as our ASR due to its faster latency and robustness in recognition [[77](https://arxiv.org/html/2406.10590v1#bib.bib77)]. While we did not conduct a thorough accuracy comparison study between the two APIs, and to our knowledge, no related literature compares them as Whisper was officially released in March 2023, we observed that Whisper recognized most non-native English-speaking participants significantly better. Conversely, with Google speech-to-text ASR, key terms in our dialogue such as ”textile circularity” were consistently recognized as ”textile security” or even ”Texas a Coronavirus”.

#### 4.1.2 Language model as foundation model

In our study, we chose GPT-3.5 (text-davinci-003) API which was known for its outstanding performance and trained with the largest parameters at the time of testing. Currently, there is a lot of discussion within the community about the differences between OpenAI’s GPT models, including GPT-3, GPT-3.5, ChatGPT, and the newly released GPT-4. Our work focuses on the pre-trained OpenAI GPT model⁴⁴4OpenAI’s GPT-3 is a pre-trained LLM with 175 billion parameters [[14](https://arxiv.org/html/2406.10590v1#bib.bib14)] rather than any other published sources or third-party models trained from scratch. One drawback of LLMs is the generation of plausible-sounding but incorrect or nonsensical responses [[70](https://arxiv.org/html/2406.10590v1#bib.bib70)]. To address this issue, LLMs like InstructGPT and ChatGPT have incorporated human efforts using Reinforcement Learning from Human Feedback (RLHF), resulting in fewer false responses and less toxicity [[72](https://arxiv.org/html/2406.10590v1#bib.bib72)]. Although ChatGPT’s advanced language processing capabilities allow it to engage in natural, human-like conversations with users, it has a tendency to be verbose due to biases in the training data. Trainers in the RLHF prefer longer answers that appear more comprehensive [[87](https://arxiv.org/html/2406.10590v1#bib.bib87), [34](https://arxiv.org/html/2406.10590v1#bib.bib34)].

We cannot determine the parameters used in the RLHF for ChatGPT, limiting our freedom in using these LLMs. Furthermore, the long-text style response of ChatGPT is unsuitable for voice interfaces. In contrast, GPT-3 and GPT-3.5 are more “organic” and provide more freedom in designing arbitrary prompts, making them useful for customizable content generation and language translation. Therefore, we focus on exploiting these large foundation models directly, such as GPT-3.5, for controlled, high-quality content generation instead of using the patched ChatGPT.

At the time of writing this paper, OpenAI had just announced GPT-4 - an enhanced language model with improved mathematical abilities and the capacity to take visual inputs. However, it can be difficult to distinguish GPT-3.5 from GPT-4 in a casual conversation, as noted on GPT-4’s website. Interestingly, OpenAI has also reported that there is almost no improvement in generating factual content when questions related to environmental science are posed [[71](https://arxiv.org/html/2406.10590v1#bib.bib71)]. In this paper, our focus is on designing a domain-specific conversation agent related to textile circularity, a key topic in material and environmental science.

#### 4.1.3 Text-to-speech

We use the gTTS (Google Text-to-Speech) library in Python to read out text with a female British English voice. However, we received feedback during the pilot study that the speech speed felt slow for natural conversation. To address this issue, we will discuss our solution in Section [4.2](https://arxiv.org/html/2406.10590v1#S4.SS2 "4.2 Hardware System Design: The TextileBot Voice Device ‣ 4 The Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

### 4.2 Hardware System Design: The TextileBot Voice Device

![Refer to caption](img/96591e68e50d242951e2d77e822c8045.png)

(a) The physical TextileBot interface.

![Refer to caption](img/7b71c80417ddb49a985bb3e95fc80cd4.png)

(b) A participant interacts with the TextileBot.

Figure 8: Left: TextileBot - The physical agent interface is composed of a 3D printed box (6), a speaker (5), a microphone (4), and a button (3), all integrated into the Google AIY board (2) mounted on the Raspberry Pi 3 Model B (1) presented in (a). Right: A participant interacting with the TextileBot used across all three voice-based agents (b).

We built the hardware device around a Raspberry Pi device. The device is housed in a 3D printed box (6), which includes a speaker (5), a microphone (4), and a button (3), all integrated on the AIY board (2) mounted on the Raspberry Pi 3B (1) as shown in Figure [8(a)](https://arxiv.org/html/2406.10590v1#S4.F8.sf1 "In Figure 8 ‣ 4.2 Hardware System Design: The TextileBot Voice Device ‣ 4 The Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). The hardware system includes a Raspberry Pi 3B with a Quad Core 1.2GHz Broadcom BCM2837 64bit CPU and 1GB RAM (1). We use the Voice HAT configuration [[1](https://arxiv.org/html/2406.10590v1#bib.bib1)], which contains a Voice AIY accessory board (2) that provides physical connectivity from the GPIO pins and is mounted on the Raspberry Pi 3 board. The Voice HAT set also provides us with an arcade-style button with an LED light (3), a microphone board with the 5-wire daughter board cable (4) and a microphone (5).

The housing was created from an open source CAD model in the Thingiverse model library. It was 3D printed on a Prusa I3 MK3S+ using the readily available polylactic acid (PLA) material. The front facing side contains holes to allow sound from the speaker to leave the enclosure, while the inside contains various shelves for the control electronics to be mounted to. The top of the box has a hole for the activation button. The firmware to control this hardware was designed by Google and deeply integrated with the Google Assistant service ⁵⁵5The Google AIY has stopped updating their service, and the repository has been archived by the owner on Feb 9, 2023 [[2](https://arxiv.org/html/2406.10590v1#bib.bib2)]. [[2](https://arxiv.org/html/2406.10590v1#bib.bib2), [35](https://arxiv.org/html/2406.10590v1#bib.bib35)]. However, this did not meet our needs, so we conducted the development of our own firmware code that enables flexible audio recording, audio playback and push button control.

Users interface with TextileBot via a button with an LED light. A predefined user guide is played when the device is booted. To speak to the TextileBot, users press and then release the button, and do it again when they finish their sentence. The LED light will be lit while recording and playing audio. We use mpg123 library with command “mpg123 -d 4 -h 3” to manually speed up the playback rate to $1.33\times$. This is because participants in our pilot studies have reported the original speaking speed from gTTS is too slow.

## 5 Evaluation of TextileBot

In the user study, we aimed (1) to evaluate whether our prompt strategy remained effective while preserving domain specificity across various spoken dialogues, and to assess if interactions with three variations of TextileBot differed significantly—indicating that users perceived each as a distinct entity and validating phase 2 of our approach, (2) to investigate if TextileBot could retain memory and conduct continuous conversations as designed in phase 3, and (3) to explore the nature of user interactions with each bot variant to understand the nuances of user engagement. As the language used in spoken dialogue is different from written text [[81](https://arxiv.org/html/2406.10590v1#bib.bib81)], an in-person study was chosen to allow participants to interact with the TextileBot smart device and evoke natural language conversations. We used a mixed-method approach, combining traditional machine learning ablation study analysis with HIC analysis - questionnaires and qualitative feedback from participants with a conversational analysis of the human-agent dialogue. We recruited a total of 30 participants to interact with each of the three voice agents on textiles and textile circularity, as outlined in the Introduction. In the following sections, we first describe the within-subject study design, measures, and procedure.

### 5.1 Study design and methods

We utilized a mixed within/between-subjects design, where each of the participants ($N=30$) was asked to speak with the three CAs (Vanilla, Assistant, Expert) embodied in the same smart device TextileBots. The order in which participants interacted with each of the agents was randomised to avoid order effects. For each agent interaction, participants followed the same four phases: Free chatting, Information gathering, Questionnaires, and Overall user feedback. Each of the four phases is detailed below:

##### Phase 1 - Free chatting

The human-agent interaction started with an open conversation with no topical restrictions. Participants could freely engage with the agents on any topic of their choice. This approach was designed to facilitate a broad exploration of potential conversation topics relevant to textiles contexts and to gain insights into the personality and characteristics of three conversational agents. A minimum of 5 minutes to a maximum of 10 minutes was allocated to this phase. Free exploration is particularly beneficial for domains that have not yet implemented conversational agents, such as textile circularity. Engaging in freeform conversations during the prototyping phase provides valuable insights into user needs and the scope of topic coverage required in these domains.

##### Phase 2 - Information gathering

To ensure consistency in the topics discussed, the second part focused on textile circularity, the main conversational topic that has guided the TextileBot implementations. In collaboration with domain experts in materials science and textile circularity, we developed ten information gathering tasks for participants. To ensure a structured approach, we arranged these tasks in a progression from general to specific, transitioning from high-level concepts to more detailed aspects. Subsequently, we divided the tasks into three distinct groups, and applied the three TextileBots to these groups in a round-robin fashion [[33](https://arxiv.org/html/2406.10590v1#bib.bib33)] to ensure coverage of different task-agent combinations.

Table 2: Questionnaire used after each of the three TextileBots to assess the human-bot interaction experience.

 | Evaluation Category | Metric | Question type |
| --- | --- | --- |
| Usability | Ease to use | 5 likert scale |
| Engagement | E-I: Interest in responses | 5 likert scale |
| E-E: Engagement in conversation | 5 likert scale |
| E-W: Willingness to use in the future | 5 likert scale |
| Coherence | C-I: Input comprehensibility | 5 likert scale |
| C-C: Clarity in responses | 5 likert scale |
| C-A: Accuracy in responses | 5 likert scale |
| Changes over time | The level of engagement over time | Multiple-choice: Increase, Decrease, Dynamic |
| Follow-up on changes over time | Open-ended question to capture the reason | 

##### Phase 3 - Questionnaire

We developed a questionnaire that contains an evaluation matrix to assess the human-LLM agent interaction. The evaluation matrix employs a wide range of existing metrics, combining metrics from conventional heuristics-based conversational agents, for both non goal-oriented/task-agnostic and domain-specific/goal-oriented agents [[51](https://arxiv.org/html/2406.10590v1#bib.bib51), [86](https://arxiv.org/html/2406.10590v1#bib.bib86), [65](https://arxiv.org/html/2406.10590v1#bib.bib65), [92](https://arxiv.org/html/2406.10590v1#bib.bib92)]. We also incorporated human-LM interaction metrics [[55](https://arxiv.org/html/2406.10590v1#bib.bib55), [99](https://arxiv.org/html/2406.10590v1#bib.bib99)]. Since our study involves three TextileBots, we treated each as a separate model and employed the pairwise per-dialogue (PW-dialogue) method [[86](https://arxiv.org/html/2406.10590v1#bib.bib86)] to evaluate the human-LLM interaction. This method compares two entire conversations with two different agents, and has been shown to outperform evaluations of single models. Each participant was asked to conduct three conversations with the three different TextileBots. Table [2](https://arxiv.org/html/2406.10590v1#S5.T2 "Table 2 ‣ Phase 2 - Information gathering ‣ 5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") summarises the key focus of the questionnaire, the metrics used and the question types.

##### Phase 4 - Overall user feedback

At the end of the study, we collected participants overall feedback on their experience with the TextileBots, capturing participants’ subjective experiences engaging with the voice agents, their preferences, observations about the interaction and changes over time, as well as any suggestions for improvements and insights they gained on the domain-specific conversation. Please see an overview of the focus and question types in Table [3](https://arxiv.org/html/2406.10590v1#S5.T3 "Table 3 ‣ Phase 4 - Overall user feedback ‣ 5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

Table 3: Overall user feedback and participants preferences between the three TextileBots, captured at the end of the study.

 | Feedback category | Question type |
| --- | --- |
| Overall feedback on each of the TextileBots | Open-ended questions |
| Preference between the three TextileBot | Ranking and open-ended questions |
| Experience of the TextileBots interaction, changes over time | Open-ended question |
| Suggestions on TextileBot | Open-ended questions |
| Understanding of the domain (textiles circularity) | 5 likert scale and open-ended questions | 

### 5.2 Study setup and procedure

The study was conducted in a controlled laboratory environment, with each participant attending individually in-person. Participants were briefed with instructions to imagine a scenario wherein they were talking with three distinct voice agents (each with different personalities and capabilities) in a retail environment, such as a clothing store. A TextileBot device was placed was positioned on a table in front of a participant, allowing them to control it (see Figure [8(b)](https://arxiv.org/html/2406.10590v1#S4.F8.sf2 "In Figure 8 ‣ 4.2 Hardware System Design: The TextileBot Voice Device ‣ 4 The Case of TextileBot: Design and Implementation ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). The tasks involved identifying and ranking their preferred agent based on its suitability for use in a retail environment as TextileBot, and their subject experience to the overall user feedback. Every interaction session began with an introduction from the respective agent (Vanilla, Expert and Assistant):

> "Hi there, I’m TextileBot. I’m here to assist you with any questions or discussions you may have regarding textiles. To speak with me, simply click the button and start talking. When you’re finished, click the button again to let me know that you’re done. How can I assist you today?"

After the agent’s welcome message, participants were given 5-10 minutes to interact freely with the TextileBot, choosing their own conversational topics (Phase 1). When satisfied with the interaction or the time limit was reached, participants proceeded to the information gathering phase (Phase 2). Upon completion of both phases, participants were then asked to fill out a questionnaire to assess their experience with that particular agent (Phase 3). This three-phase procedure was repeated for all three TextileBots. Participants were also offered the opportunity to extend their interaction with any TextileBot of their choice or all of them, if they prefer, before proceeding to Phase 4\. Once all interaction sessions were completed, participants were asked to provide final overall feedback (Phase 4) on their experience using the voice-based TextileBots, as outlined in Section [5.1](https://arxiv.org/html/2406.10590v1#S5.SS1 "5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

### 5.3 Analysis approach

Our primary focus is to explore the efficient development of LLM-based voice CAs that are domain-specific and offer personalized interactions that is capable of conducting continuous (multi-turn) conversations. We started by analyzing questionnaire responses with each agent to understand each vHAI (Vallina, Expert and Assistant). Following this, we conducted a qualitative analysis of the overall user feedback to gain insights into participants’ overall perceptions of three agents. The evaluation also examines the effectiveness of our approach.

In addition, a key aspect of our research is exploring how people perceive and engage with different prompted LLM-based CAs. Hence, as a first step in the analysis, the dialogue contents were stored in a text format and imported into NVivo 14, a qualitative analysis software. A dialogue refers to a whole recorded exchange of conversation between a participant and a CA [[89](https://arxiv.org/html/2406.10590v1#bib.bib89)]. We then applied a data-driven inductive thematic analysis approach to identify recurring themes and patterns within the dialogue transcripts and to gain qualitative insights into the vHAI.

The first author applied an open-coding approach to the dialogues, and created a first coding scheme that was discussed and refined with the co-authors. After several discussions and iterations, all authors reached the consensus that the vHAI can’t be easily shoehorned into a set of themes. However, it was agreed that the changes in the interaction patterns over time should be further explored to understand variations in the dialogue and participant behaviours. Hence, we decided to employ a combined inductive/deductive hybrid approach focused on:

1.  1.

    An analysis of the dialogues based on conversational turns,

2.  2.

    An analysis of the conversational styles,

3.  3.

    An analysis of the human behaviour in the interaction with the TextileBots.

In Section [6.3](https://arxiv.org/html/2406.10590v1#S6.SS3 "6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), we present the results for each of those three points, starting from the “conversational turns” and “turn-taking patterns” observed within and across the three voice agents. We then further explored differences between single vs. multi-turn conversations and calculated the number of words used by participants in each turn, as a possible indicator for their engagement over time and across the agents. This dialogue analysis was extended with a deep dive into the conversational styles enriched and exemplified with representative quotes from participants’ interaction with the agents, and a particular emphasis on the changes over time, drawing on existing language concepts such as code-switching and social protocols. We conclude with a reflection on specific human behaviours and strategies when engaging with the three different agents. All taken together form a rich, multi-faceted foundation for our discussion on the effectiveness of our approach and how humans perceive, interact, and engage with prompt-based voice agents.

All participants quotes are included with original spelling and emphasis.

### 5.4 Participants

We recruited 30 participants aged between 22 to 44 years of age (mean age = 30, SD = 5.33), out of which fourteen were male, sixteen female. Participants had a diverse range of backgrounds, including computer scientists, UX designers, artists, healthcare consultants, researchers, university lecturers, and university students. All participants were either native English speakers or highly proficient in English. Furthermore, the participants came from 15 countries across five continents. The study was approved by the local University Research Ethics Committee . All participants provided written informed consent before taking part in the study. The study lasted between 45 to 60 minutes, and all participants were compensated with a gift voucher for their time.

## 6 Results

We present our findings in three main sections: analysis of questionnaire responses (Section [6.1](https://arxiv.org/html/2406.10590v1#S6.SS1 "6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")), overall participant feedback (Section [6.2](https://arxiv.org/html/2406.10590v1#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")), and dialogue data from our user study (Section [6.4](https://arxiv.org/html/2406.10590v1#S6.SS4 "6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). The questionnaire responses and dialogue data explore whether participants perceived three variations of TextileBot as distinct entities and their engagement with each prompted version. Additionally, these sections assess whether TextileBot maintained memory and facilitated continuous conversations. Together, these results provide insights into the nuances of user engagement with different LLM-powered voice agents.

### 6.1 Questionnaire results

To determine if participants perceived three variations of TextileBot as distinct entities, we analyzed questionnaire data from our study. We obtained a total of 120 questionnaires, 90 from the interaction sessions (three per participant, for each agent they tried), and 30 from the overall user feedback. This section mainly discusses the results we have with respect to an analysis using the metrics in Table [2](https://arxiv.org/html/2406.10590v1#S5.T2 "Table 2 ‣ Phase 2 - Information gathering ‣ 5.1 Study design and methods ‣ 5 Evaluation of TextileBot ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). We also aimed to understand participants’ perceptions of voice agents when presented with different prompts.

##### Engagement and coherence metrics

We first obtained participants’ scores for both Engagement and Coherence metrics, averaged them, and presented them in a radar plot (Figure [9(a)](https://arxiv.org/html/2406.10590v1#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). All responses were coded from $1$ to $5$; all averages fell in the range between $3$ and $4.5$. Figure [9(a)](https://arxiv.org/html/2406.10590v1#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") shows the questionnaire’s overall results regarding the evaluation of engagement and coherence. The results in Figure [9(a)](https://arxiv.org/html/2406.10590v1#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") demonstrate that Assistant is generally the best across all these evaluation metrics.

##### Cross-metrics interactions

We then used mixed cumulative link regression models with participants and interaction topics/questions as random effects. This allowed us to account for the nested nature of the experimental design [[50](https://arxiv.org/html/2406.10590v1#bib.bib50)] and the ordinal characteristics of the survey’s responses [[109](https://arxiv.org/html/2406.10590v1#bib.bib109)]. Data was analysed using the “ordinal” package in R [[20](https://arxiv.org/html/2406.10590v1#bib.bib20)]. No difference was found when comparing models on their Ease of use and Coherence (C-I, C-C, C-A) metrics. As we have also seen in Figure [9(a)](https://arxiv.org/html/2406.10590v1#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), the variations in C-I, C-C and C-A are relatively small, we turn the focus of the analysis to the remaining Engagement metrics (E-I, E-E and E-W).

![Refer to caption](img/f6d57c7a2813e891ae9d620aaf949b79.png)

(a) Engagement and Coherence metrics.

![Refer to caption](img/00416404e37b252bbd62c0045cf30e38.png)

(b) Participant preference ranking.

Figure 9: Left: this includes Interestingness in responses (Engagement, E-I), Engagement in conversations (Engagement, E-E) and Willingness to use in the future (Engagement, E-W), Input comprehensibility (Coherence, C-I), Clarity in responses (Coherence, C-C), Accuracy in responses (Coherence, C-A). Right: Participant preference ranking of the three TextileBots in light of textile circularity.

As depicted in Figure [9(a)](https://arxiv.org/html/2406.10590v1#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), the TextileBot Assistant was found consistently more engaging at the single response than its Expert and Vanilla counterparts (E-I), although results were not statistically significant (“marginally” significant $p=0.06$). This pattern did not emerge at the conversation level (E-E), where both Vanilla and Assistant were slightly (but consistently across participants) more engaging than the Expert agent ($p=0.2$). Still, on the Engagement dimension (E-W), participants reported that they would be significantly more likely to interact with the TextileBot assistant than with both alternative versions in the future ($p<0.05$, post-hoc tests, Bonferroni corrected).

##### Ease to use and Interest change over time

Overall, the ease of use was rated from 2 to 5 with an average score of 4. Regarding the change in interest levels over time, 83.8% of the sessions showed that there is a variation in interest levels. 53.8% reported an increase, 20% reported a decrease, 10% were dynamic. The rest reported no change. Participants emphasized the significant influence of response content on their level of interest. For example, P5 pointed out that their interest “depended on the specific question and corresponding answers”. Similar statements were echoed by P18 and P28, who noted that their interest heightened when the agent delivered intriguing responses. The other factor is the length of the response. Both P15 and P16 expressed annoyance due to the vast, long-winded response from Expert. As P15 put it, ”It sometimes provided too much information which made me lose interest somewhat”. P16 went into further detail, stating, ”Sometimes the responses were a bit long. The information provided was interesting, but the agent essentially answered my question within the first few seconds and then kept talking.” However, not all participants were thrilled with shorter responses. P17 commented on Vanilla as “It was too brief with little prompt but it remembers previous questions and provided context based answers”.

### 6.2 Overall feedback

In this section, we present the overall user feedback on the participants’ subjective experiences with the agents, their preferences, feedback on how they perceived the interactions over time and any suggestions for improvements.

Overall, participants enjoyed the interaction because “it felt really natural” (P7) and “The levels of answers were good throughout but I really liked the memory function and the agent answers were not generic, especially compared to my other voice agent experiences” (P13). Nevertheless, a number of participants (N=4) perceived the interaction to be a one-way question-answering rather than conversation and expected the voice agent to engage in a more dialogic interaction by asking questions: “I wish it engaged in conversation as well, asking questions back more, so you feel more engaged as well…” (P14). Participants (N=5) suggested that the voice agents would benefit from adopting “emotion embedded” and “more interesting” responses to achieve a human-like “real conversation”. Participants mentioned that they would prefer “less formal”, “less persuasive” voice agent with “a bit of humour” and “shorter answer”, in order to facilitate ”more engaging interactions”.

Moreover, participants (N=6) commented on the clarity and quality of the content provided by the voice agents. The majority of the feedback on the information seeking phase (i.e. information provided by the voice agent) was positive, with comments praising the levels of answers and clarity, such as P13 noted “agent answers were not generic especially compared to my other voice agent experiences”. On the other hand, some participants pointed out redundancy and vagueness, such as highlighted by P22: “Sometimes the answers provided in the conversations were a bit redundant, but I found the answers very clear, although sometimes a bit vague or broad.” . However, there was a general feeling that more concise, in-depth content delivery by the voice agents would be desirable.

In summary, participants anticipated voice agents that engage proactively, exhibit personality, deliver interactive communication (memory function), and provide varied, interesting yet concise content. Furthermore, the incorporation of human-like qualities in both content and voice is desirable. These insights are further reflected in participants’ feedback on their agent preferences.

#### 6.2.1 Preferences and experiences across voice agents

Participants were asked to express their preference towards the three TextileBots by ranking them. We used the chi-square test to assess whether any of the agents was selected significantly more (or less) often as a favourite agent. Results showed no statistically significant differences although, as mentioned before based on the conversational analysis and questionnaire feedback, we can see a preference for the Assisstant agent, followed by Vanilla and the Expert as shown in Figure [9(b)](https://arxiv.org/html/2406.10590v1#S6.F9.sf2 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). The Assistant agent was selected more often (14 selections) than Vanilla (12 selections) than the Expert agent (only 4 selections). On the other hand, the Expert agent reached “second place” (14 selections) more often than both the Vanilla (7 selections) and Assistant (9 selections) agents.

Most participants (N=18) expressed a preference for an agent that can communicate in a concise and clear manner with them. P17 stated, “the 2nd agent (Assistant) gave just the right amount of detail.” However, it is worth noting that the length of the agent responses was not universally appreciated, as discussed in Section [6.1](https://arxiv.org/html/2406.10590v1#S6.SS1.SSS0.Px3 "Ease to use and Interest change over time ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Moreover, some participants (N=13) distinguished the agents based on their interactive capability. The Assistant agent was preferred by many for its interaction level, as P29 stated“Assistant agent has the best understanding of my question and explained in a most interesting way”. In contrast, the Expert agent was criticized for being a repetitive information source lacking meaningful interaction. P1 mentioned that Expert agent ”feels like a repetitive of the textile circularity concept.” Whereas P30 point out on the conversational breath that Expert “is too restrictive up to a point where it stops responding to questions asked”.

Finally, the agent’s perceived personality also played a role in preferences. A number of participants (N=8) appreciated agents that showed human-like responses. P5 noted that the 2nd agent (Vanilla) “sounds more like a human… and gives me some interesting answers and makes me laugh.” In contrast, the Expert agent received criticism for its formal tonality, with P25 noting that it was more like a “text-book” and P10 referring to it as “speaking with a smart microwave.”

#### 6.2.2 Perceived changes over time

Most participants (N=24) in our study described a change in their overall interaction with the agents. Several participants (N=13) commented that their engagement and the nature of interaction evolved as they became familiar with the agent. Some participants even noted an increase in confidence and comfort in their interaction towards the later stages, as described by P5: “the more time I spent on the agent, the more open I am”. Several participants (N=9) even mentioned adapting their communication styles, such as the language and the clarity of their questions to better communicate with the agent. P15 stated: “The way I asked it questions so that they were clear enough, avoided using too much colloquial language”. Additionally, some participants (N=5) stated an increase in specificity in their query, “my questions changed…”, “more specific questions as time went by”, and “I started to comment on its response and asked for further explanations.”. There was a general trend towards asking more specific and deeper questions as the dialogues progressed. Possibly as a result of a better understanding of the agent’s capacities or due to a growing interest in the topic.

#### 6.2.3 Suggestions for improvements

Participants provided valuable suggestions for improving the agents, including one common suggestion to use a more natural and human-like voice. Suggestions such as “more natural voice” (P5) and “smoother voice, more dynamic” (P6) indicated a preference for a less robotic tone. Participants also mentioned the need for the agent to be maybe more empathetic, as P20 stated, “add some emotions”. Another suggestion was to improve the flow of the agent’s speech, such as “pauses when there is some punctuation would be helpful” (P22). In addition to the voice suggestions, participants wanted the agent to be concise, encouraging, and human-like. Suggestions included making the agent more engaging and insightful with personalised responses. Participants emphasised the importance of personalization, acknowledging that different users may have different knowledge levels, needs, and interests. They felt that the current agents need to reduce the “teacher-like” (P1) and “uncanny valley” (P7) effects in their responses. Another suggestion was related to the ability to interrupt the agent’s responses, as one could in a human-human interaction. P15 put it as follows: “Could be useful to be able to interrupt the agent’s response if the answer is not in line or maybe too long”. This again hints to the suggestion for a more natural and human-like interaction.

#### 6.2.4 Understanding of the domain (textiles circularity)

With regard to the specific conversation topic, textile circularity, most participants (N=21) reported that they had not previously encountered the concept of textile circularity. Despite this, an almost equal majority (N=27) were able to furnish a definition falling within the standard understanding of textile circularity by the end of the study. This concept of textile circularity is admittedly abstract and complex, a factor which led to many of our participants finding the subject matter somewhat tedious. Regardless, they remained engaged throughout the study and demonstrated the ability to articulate the concept in their own words. We believe these observations underscore potential avenues for future research, particularly exploring our prompt-based voice agents in other subject domains.

### 6.3 Dialogue analysis of the voice-based human-agent interaction

We collected a total of 93 dialogues from 30 participants (3 agent interactions per participant), where 3 additional dialogues resulted from the “further interactions” that 2 participants had with the Vanilla (1x) and Assistant (2x) agents.

Table 4: Analysis of Interaction Turns and Word Count in TextileBots: The Assistant TextileBot displayed the highest frequency of interaction turns but the lowest word counts per turn both by the participants and Assistant TextileBot itself, as compared to others. In contrast, the Expert TextileBot exhibited the reverse behaviour. *In our study, each participant gathers one-third of the information using a bot, totaling 30 Info gathering sessions. The average number of turns is calculated from 10 complete sessions for each bot.

 |  | Number of turns | Word counts per turn |
|  | Overall | Free-chatting | Info gathering* | Participants utterances | Bot utterances |
| Vanilla | 13.77 ± 6.29 | 12.6 | 11.7 | 11.78 ± 8.06 | 44.53 ± 22.69 |
| Expert | 11.03 ± 3.7 | 7.6 | 11.2 | 12.11 ± 8.45 | 61.52 ± 23.74 |
| Assistant | 17.6 ± 10.19 | 9.5 | 14 | 11.43 ± 9.20 | 37.29 ± 37.29 | 

#### 6.3.1 Conversation turns

The dialogues contained a total of 1272 conversational turns. Each turn denotes an exchange of utterances, representing a pairwise dialogue between a participant and the agent. On average, a dialogue comprised 799.40 words (SD=317.53) and 14.13 turns (SD=7.95). As we discussed earlier, tracking the number of conversation turns between the participant and the conversational agent can provide insights into the depth and length of interactions. Higher turn counts indicate more engaged participants [[67](https://arxiv.org/html/2406.10590v1#bib.bib67), [66](https://arxiv.org/html/2406.10590v1#bib.bib66)].

##### Turn-taking comparison across agents

As illustrated in Table [4](https://arxiv.org/html/2406.10590v1#S6.T4 "Table 4 ‣ 6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") and Figure [10(a)](https://arxiv.org/html/2406.10590v1#S6.F10.sf1 "In Figure 10 ‣ 6.3.2 Word count in each turn ‣ 6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), it is evident that the Assistant agent garners the highest level of participant engagement, whereas participants tend to exhibit lower levels of engagement with the Vanilla agent. These results indicate that there are statistically significant differences in the number of turns between the Assistant agent and the other two agents, but not between the Vanilla and Expert agents.

##### Single vs Multi-turn conversations

Smart voice assistants, such as Alexa and Google Assistant, are generally limited to single-turn conversations due to their lack of memory. In contrast, our design incorporates a memory function, prompting an investigation into whether participants can engage naturally in this novel interaction pattern. Multi-turn conversation refers to an interaction style whereby multiple rounds of queries and responses revolve around the same topic, while single-turn conversation pertains to a scenario where only a single query and response take place regarding a specific topic. We have identified two distinct forms of vHAI: single-turn query & response and multi-turn (dyadic) dialogue. Among the 30 participants, 29 were naturally engaged in multi-turn dialogues to varying extents.

#### 6.3.2 Word count in each turn

We further investigated the number of words in both participants’ and agents’ utterances in each turn, shown in Table [4](https://arxiv.org/html/2406.10590v1#S6.T4 "Table 4 ‣ 6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). The Vanilla agent has an average of 11.78 words (SD=8.06), the Expert has 12.11 (SD=8.45), and the Assistant has 11.43 words (SD=9.20). The maximum words participants spent were 78, 61, and 111 respectively on these three agents. Regarding the responses from TextileBot. The Vanilla agent has an average of 44.53 words (SD=22.69), the Expert has 61.52 (SD=23.74), and the Assistant has an average of 37.29 words (SD=18.31).

![Refer to caption](img/e8989da517257feb64a82b4b9a0aacc5.png)

(a) Numbers of turns per participant across three agents.

![Refer to caption](img/d48fb467ad01c7c29a7127ee02aadcf6.png)

(b) overall trends for lengths of turns

Figure 10: Left: Figure (a) shows the number of turns per participant across three agents. Right: Figure (b) illustrates the word count per turn, smoothed using a moving average for each agent, against different turn numbers.

The trend for participant’s word usage across agents involves calculating a moving average with a window size of four, and this smoothed data is depicted in Figure [10(b)](https://arxiv.org/html/2406.10590v1#S6.F10.sf2 "In Figure 10 ‣ 6.3.2 Word count in each turn ‣ 6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Observing the data, a noticeable pattern emerges: participants’ initial utterances with fewer words gradually increased their words in the early turns. The duration peak, or hold time, represents the duration for which the agents can sustain participant engagement. Towards the end, the curve shows a decline, suggesting a decrease in participant engagement as they gradually speak fewer words.

### 6.4 Conversational styles

Beyond the overview of dialogues, we explore the conversational styles in the dialogues and conversational turns over time. Deborah Tannen [[88](https://arxiv.org/html/2406.10590v1#bib.bib88)] describes conversational styles “is comprised of the habitual use of specific linguistic devices, chosen by reference to broad operating principles or conversational strategies”.

#### 6.4.1 Conversational styles change over time

Across all agents, we noticed a similar trend of changes over time. The complexity of participants’ queries increased over time. In other words, participants began with straightforward queries and progressed to more complex ones. Initially, participants prefer using straightforward queries that are concise, direct, and easy to understand and respond to. These queries seek simple and concise answers without requiring elaborations [[82](https://arxiv.org/html/2406.10590v1#bib.bib82)], the phrases commonly start with “What is”, “What are”, e.g., “What fiber is cotton made from?” .

As the conversation progressed, we observed a shift towards more complex queries, such as posing chained queries (P28-Expert: “So what if I want to help you to persuade the whole community to use in textile circularity? What would be the strategy? How should I get involved into the community to try to persuade more people be aware of this issue?”) and lengthy information (P10-Assistant: “what is the advantage over organic material over other types of material is it more sustainable or is more healthy or is more I don’t know like is there any other properties that you can mention I can actually make it the top trending material that’s used during this generation”).

{mdframed}{dialogue}

First bot Vanilla GPT-3.5 model

Bot Hi there, I’m TextileBot … How can I assist you today? Human Hello, I would like to learn more about the circular economy in the textile industry.

…

Human What is the percentage between, the ratio between production and recycling in India in relation to cotton? Second bot Expert Bot Hi there, I’m TextileBot … How can I assist you today? Human How do you think AI can be used to support and improve the textile systemarity?
…

Human Do you know what is the percentage of clothes from big brands like the fast fashion industry like Zara and H&M that are actually sold in the same production here? Third bot Assistant

Bot Hi there, I’m TextileBot … How can I assist you today?

Human Can you name some fashion companies that are big players in the fashion society?

…

Human So it was introduced in 2015 and it’s still something that’s available.

Figure 11: The three dialogues of P22 demonstrate a progression from the starting point to a later stage with Vanilla, Expert, Assistant, respectively.

Regarding the content, the example in Figure [11](https://arxiv.org/html/2406.10590v1#S6.F11 "Figure 11 ‣ 6.4.1 Conversational styles change over time ‣ 6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") illustrates how participants’ queries progressed with each agent. Participants moved from general ( P27-Assistant: “Tell me the basics about textiles.”) to specific (P27-Assistant: “Can you still make nylon or polyester without petrol?”) and from factual (P23-Assistant: “How to wash my clothes with the oil on it.”) to subjective queries (P23-Assistant: “What’s your opinion on Uniqlo?”), which aligned with the findings in Section [6.2](https://arxiv.org/html/2406.10590v1#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). These progressions in query complexity echoed Wilson’s information seeking model [[102](https://arxiv.org/html/2406.10590v1#bib.bib102)].

#### 6.4.2 Code-switching

Code-switching, also known as language alternation, is a phenomenon that frequently occurs in multilingual individuals and is formally referred to as changing the language used in conversation. As sociolinguists have defined [[44](https://arxiv.org/html/2406.10590v1#bib.bib44)], it is now widely acknowledged as the way people select their “linguistic repertoire” [[15](https://arxiv.org/html/2406.10590v1#bib.bib15), [40](https://arxiv.org/html/2406.10590v1#bib.bib40)] based on situational demands. We here refer to code-switching as the language alternation of the participants, which includes the formulation of queries and variations in tone. Participants tend to use code-switching to elicit their desired response. For example, in Figure [5](https://arxiv.org/html/2406.10590v1#S3.F5 "Figure 5 ‣ 3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot") it is evident that participants applied code-switching to assess the agent’s capabilities. In computer science terms, this behaviour can be likened to participants testing with various prompts to evaluate the LLMs.

#### 6.4.3 Social protocol

Following Völkel et al. [[94](https://arxiv.org/html/2406.10590v1#bib.bib94)], social protocol as an exchange of polite conventions or obligations, such as saying “hi”, “thank you”, “please”, a form of general information (e.g., “Nice”). 76.7% of participants (N=23) had at least one social protocol with the agent, 56.7% participants (N=17) used “thank you” or “please”, 50% participant (N=15) greeted the agent at the beginning of their conversation, e.g. “Hello, what is your name?” (P13), but most of them did not do so in all interactions with every agent. 23.3% participant (N=7) appreciated or affirmed agent’s answer, most of them occurred with Assistant, such as “that’s good to know” (P20-Assistant/Vanilla), “You’re a good guy.” (P5-Assistant), “Wow, sounds amazing.” (P37-Assistant). Unfortunately, none of those acknowledgements were given to Expert.

#### 6.4.4 Variations of utterances across agents

We further investigate if the conversational styles are varied across agents. We found that participants tend to pose detailed queries with clear instructions and relatively formal language with Expert, for instance P12 stated “Can you tell me more about what’s going on in one of those countries with a lot of textile waste from northern countries? Can you tell me more about how a specific country deals with the textile they receive?”. This may indicate the reason for the average word spend is slightly more with Expert in (refer to Section [6.3.2](https://arxiv.org/html/2406.10590v1#S6.SS3.SSS2 "6.3.2 Word count in each turn ‣ 6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")).

In the dialogue with Assistant, the conversational style people phrased their queries ranged from formal, complete sentences, to more conversation-like utterances. This reflects varying social protocols for interacting with agents, but it also shows the Assistant agent’s effectiveness in engaging participants in a more natural and less formalistic dialogue.

Similarly, participants’ queries with the Vanilla agent were less formal compared to the Expert. It is worth noting that two multi-turn dialogues led to arguments with rude utterances. P21 even went as far as to state “That is absolutely bullshit. Who told you that? Why do you believe him?” when Vanilla claimed it is programmed by experienced programmers and “My programmer believes that having an English accent gives me a more sophisticated, knowledgeable and intelligent persona.”.

### 6.5 Human behaviour and reactions

Diving further into participants’ engagement with the agents in the dialogues, our data shows that one-third of participants (N=10) used the phrase, “tell me more …” at least once. All participants (N=30) were seeking clarifications in the free chatting phase (e.g. P14-Vanilla: “What do you mean by promote sustainability?” P28-Assistant: “Please tell me more about it”). These instances suggest a demand for additional detailed explanations. The Assistant, Expert, and Vanilla agents received such requests in 6.1%, 4.7%, and 4.5% of interactions, respectively. Whereas only 63.3% participants (N=19) sought clarification in the information seeking phase. For instance, P25-Vanilla “I think the example you gave is very high level. Is there any more detailed example you can give me?”. The information-seeking phase witnessed an increased number of clarifications, as the name suggests, totalling 32.1% with the Assistant, 24.1% with the Expert, 17.9% with the Vanilla. This could be because the Assistant tends to respond in a concise style, where participants desire more elaboration.

Moreover, it was interesting to observe how participants reacted to agent errors. This was captured by the main experimenter through note-taking during the study and dialogue between the participants and agents. 30% participants (N=9) encountered agent errors such as failures in Automatic Speech Recognition (ASR) or a lack of understanding in response to participant queries. Out of those participants, 77.8% (N=7) participants actively corrected the agent by explicitly stating the misunderstanding, repeating or rephrasing their queries, e.g. P15 stating “No, I meant - what materials are you made of?”. Other strategies encompassed terminating the conversation, shifting to a different topic, or progressing with the agent.

Since LLMs can produce both plausible-sounding and incorrect or nonsensical answers [[106](https://arxiv.org/html/2406.10590v1#bib.bib106)], participants encountered such situations. Some participants (N=5) contested the agents, and tried to convince or educate the agent to correct its answer. On one occasion, a participant expressed discontent when the Assistant agent was repetitive about sustainable suggestions. The participant pointed out, “Yeah, but if you repeat yourself, sometimes people think that you’re talking down to them.”. The agent then acknowledged the feedback and apologized.

Participants also challenged some of the agents’ responses and 40% participants (N=12) explicitly expressed their skepticism and mistrust towards LLM-mediated agents. Some participants (N=5) addressed their concern by asking “Are you sure …?” and continued to ask for more evidence from the agent on the topic of the conversation. P15 even went as far as to state “That is where you’re wrong, my friend.”, when the participant was unhappy with the responses and decided to finish the conversation in the free chatting phase and move to the information seeking part. It was interesting to observe, that arguments and skepticism were more prevalent in the conversation with the Expert agent. This may be due to the lengthy response style, where participants mentioned losing interest. We observed that a strategy participants used to verify if the agent was making up their answer or not was by asking some facts they knew. Below is an example:

> p11: Are real numbers countable? Assistant: I’m sorry, I’m not familiar with real numbers. Could you help me understand what they are? I can help you find sustainable clothing options, if you’d like. p11: Do you know what rational numbers are? Assistant: Yes, rational numbers are numbers that can be expressed as a fraction or ratio, such as 1/2 or 3:4\. Would you like to know more about sustainable clothing? p11: back to your previous question, real numbers are the completion of the rational numbers.

## 7 Discussion & Future Directions

This work introduces a novel three-phase approach to prototyping domain-specific voice agents using LLMs. We assessed our method by implementing a voice agent on an actual device, TextileBot, specifically focusing on textiles circularity, and conducted an in-depth user study. In this discussion, we first analyze the effectiveness of our approach, drawing upon evaluation metrics and dialogue analysis. We then explore comprehensive insights derived from the dialogue analysis, questionnaire responses, and overall participant feedback. This exploration aims to deepen our understanding of human perceptions, engagement, and interactions with LLM-mediated, prompt-based voice agents. Finally, we reflect on the key lessons learned from this design process and discuss their broader implications for the HCI community.

### 7.1 The effectiveness of the method

The effectiveness of prompting LLMs has been widely studied [[49](https://arxiv.org/html/2406.10590v1#bib.bib49), [60](https://arxiv.org/html/2406.10590v1#bib.bib60), [14](https://arxiv.org/html/2406.10590v1#bib.bib14)], using standard metrics, such as perplexity and the BLEU score [[73](https://arxiv.org/html/2406.10590v1#bib.bib73)]. However, all of these metrics are objective evaluations that fail to capture the nuanced human-LLM interactions. In our research, we adopt a human-centred AI design approach with human-in-the-loop to evaluate prompt-based LLM-mediated voice agents. We conducted a subjective user study that combined both heuristics conversational agents evaluation metrics [[51](https://arxiv.org/html/2406.10590v1#bib.bib51), [86](https://arxiv.org/html/2406.10590v1#bib.bib86), [65](https://arxiv.org/html/2406.10590v1#bib.bib65), [92](https://arxiv.org/html/2406.10590v1#bib.bib92)] and human-LM interaction metrics [[55](https://arxiv.org/html/2406.10590v1#bib.bib55), [99](https://arxiv.org/html/2406.10590v1#bib.bib99)]. This approach allowed us to integrate quantitative data and qualitative insights, offering a more holistic understanding of the voice agent’s effectiveness. We are interested in whether participants can identify the conversational agents’ characteristics as intended in our design, particularly in terms of restricting the agent’s conversational domain and personality.

Our results showed no statistically significant differences across three agent ratings for their Coherence, Ease to use and Change over time metrics (Section [6.1](https://arxiv.org/html/2406.10590v1#S6.SS1.SSS0.Px2 "Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). The results indicate that prompting does not have a significant impact on the Ease of Use, Coherence, and Change over Time in the interaction between TextileBots and users. We found that all three agents are nearly equal in these comparison dimensions. This is an encouraging result, as it demonstrates that prompting does not have a detrimental effect on these dimensions for the type and level of prompting tested in this experiment.

On the other hand, the prompts design significantly influenced user engagement and perception (Figure [9(a)](https://arxiv.org/html/2406.10590v1#S6.F9.sf1 "In Figure 9 ‣ Cross-metrics interactions ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). The Expert, despite its design for detailed and domain-specific responses in textiles circularity, faced criticism for its lengthy and persuasive replies, hindering user engagement. Yet, this critique aligns with our intention for the Expert to provide “response in a detailed manner”, validating the effectiveness of our Taxonomy-based Knowledge Structure Chain. Interestingly, while the Expert had a negative impact on engagement levels, it was not considered particularly bad (in terms of preferences). We noted that participants recognized its utility in textiles circularity domain, aligning with its intended role as a domain-specific “TextileBot”.

The results also highlight the participants’ nuanced perception of the differences between the Vanilla, Expert and Assistant, recognizing their distinct characteristics and domain-specific utilities as discussed in Section [6.2.1](https://arxiv.org/html/2406.10590v1#S6.SS2.SSS1 "6.2.1 Preferences and experiences across voice agents ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). This indicates a successful differentiation in their roles and conversational styles. Overall, our findings demonstrate the potential of our three-phase method in transforming LLMs from generalist to domain-specific roles. This approach effectively manages the domain focus, personalities, response styles, and conversational freedom of voice-based conversational agents.

### 7.2 Insights into human-agent interactions and AI-powered dialogues

This work distinguishes itself from text-based human-agent interaction because people behave differently when speaking and writing, as the language used for spoken dialogue is distinct from that in written text [[81](https://arxiv.org/html/2406.10590v1#bib.bib81)]. While voice-based interactions share some commonalities with text-based interactions, they differ significantly in various aspects as discussed in Section [7.3](https://arxiv.org/html/2406.10590v1#S7.SS3 "7.3 Optimizing LLM-mediated voice agent design for specific domains ‣ 7 Discussion & Future Directions ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Our findings not only align with but also extend existing research in voice-based human-agent interaction (vHAI) [[38](https://arxiv.org/html/2406.10590v1#bib.bib38), [88](https://arxiv.org/html/2406.10590v1#bib.bib88), [15](https://arxiv.org/html/2406.10590v1#bib.bib15), [40](https://arxiv.org/html/2406.10590v1#bib.bib40), [94](https://arxiv.org/html/2406.10590v1#bib.bib94)], offering fresh insights into fully automated AI-powered conversations.

The TextileBot represents a significant advancement in this realm. It achieves multi-turn conversations, allowing for more natural and ongoing contact compared to typical voice agents like Alexa, which only have single-turn memoryless interactions. Also, unlike domain-specific agents, which offer detailed, context-aware responses in particular areas, general voice assistants like Alexa answer common queries, providing a broader range of services, e.g. weather updating, but with less specialization. In our study, nearly all participants (N=29) quickly adapted to this new form of interaction, underscoring the inherent and instinctive nature of human communication as continuous multi-turn dialogue. Current voice-based agents lack the capacity to retain knowledge for ongoing conversations. TextileBot’s design effectively addresses these shortcomings, demonstrating a more realistic interaction model. This advancement is particularly beneficial for the HCI community, as it facilitates the prototyping of voice agents for more sophisticated interactions beyond simple single-turn exchanges. Future developments in voice agents should aim at enabling conscious and continuous interactions that mimic natural human dialogue.

In conversational analysis, we noticed a significant shift in the participants’ conversational styles [[88](https://arxiv.org/html/2406.10590v1#bib.bib88)] over time. They gradually began to pose more sophisticated queries (Section [6.4.1](https://arxiv.org/html/2406.10590v1#S6.SS4.SSS1 "6.4.1 Conversational styles change over time ‣ 6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) and even applied code-switching (Section [6.4.2](https://arxiv.org/html/2406.10590v1#S6.SS4.SSS2 "6.4.2 Code-switching ‣ 6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")) [[15](https://arxiv.org/html/2406.10590v1#bib.bib15), [40](https://arxiv.org/html/2406.10590v1#bib.bib40)] to alter their language for desired responses. This change is also reflected in their overall feedback, as they reported a shift in engagement and interaction dynamics as they became more familiar with the prompt-based voice agents. These findings indicate a growing confidence of participants in their interactions as they developed a better understanding of the agents (Section [6.2.2](https://arxiv.org/html/2406.10590v1#S6.SS2.SSS2 "6.2.2 Perceived changes over time ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). These complex changes in behaviour and interaction patterns pose a central challenge for autonomous voice agents, which aim to operate without the involvement of an experimenter. However, our study shows that LLM-mediated voice agents demonstrate a level of capability and flexibility in handling these dynamics. This emphasizes the potential of utilizing LLMs for conversational agents to effectively address complex human inquiries.

Furthermore, we observed that participants consistently employed social protocols [[94](https://arxiv.org/html/2406.10590v1#bib.bib94)] with an informal tone when interacting with the Vanilla and Assistant agents, but such occurrences were rare with the Expert (Section [6.4.3](https://arxiv.org/html/2406.10590v1#S6.SS4.SSS3 "6.4.3 Social protocol ‣ 6.4 Conversational styles ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). Additionally, there was a notable difference in the length of utterances and turn-taking behaviour (Section [6.3](https://arxiv.org/html/2406.10590v1#S6.SS3 "6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). Participants had shorter utterances and engaged in more turn-taking with the Assistant agent, while the Expert agent exhibited the opposite pattern. These changes in participant social protocols, utterance length, and turn-taking behaviour suggest that the level of engagement varies across these three agents. It is worth noting that all three TextileBots are mediated by the same LLM, with the only distinction being the prompts provided. This further confirmed the effectiveness of our three-phase prompt design as illustrated in Section [6.2.1](https://arxiv.org/html/2406.10590v1#S6.SS2.SSS1 "6.2.1 Preferences and experiences across voice agents ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), and highlights that prompting strategies can effectively shape the personalities and capabilities of voice agents, thereby directly influencing user engagement.

### 7.3 Optimizing LLM-mediated voice agent design for specific domains

In the previous two sections, we elaborated on the feasibility of prompting LLM to develop domain specific voice agents. We also noted that the prompt design of these voice agents critically influences user interaction. This section first focuses on key aspects that enhance voice agent design, specifically aiming to improve user engagement and the overall experience. Then summarize the lessons learned in using LLM for conversational agent design.

#### 7.3.1 Enhancing the voice agent design

##### Agent characteristic and user preference

Participants’ preferences and interaction styles were notably influenced by the perceived personality and characteristics of the agents. Our results indicate that a greater number of participants showed increased interest in the Assistant agent (56.7% for Assistant, 53.3% for Vallina and 36.7% for Expert), as illustrated in Section [6.1](https://arxiv.org/html/2406.10590v1#S6.SS1.SSS0.Px3 "Ease to use and Interest change over time ‣ 6.1 Questionnaire results ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). This preference was further evidenced by more user interactions with the Assistant agent and fewer with the Expert, as detailed in Section [6.3.1](https://arxiv.org/html/2406.10590v1#S6.SS3.SSS1 "6.3.1 Conversation turns ‣ 6.3 Dialogue analysis of the voice-based human-agent interaction ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). A primary factor for this preference was the agents’ conversational styles, with participants favouring the “human-like” response from the Assistant and Vanilla agents. In contrast, the Expert, characterized by a more “expert” tone, was less favourably received, with participants likening it to a “text-book” (P25) or a “smart microwave” (P10) in their feedback (Section [6.2.1](https://arxiv.org/html/2406.10590v1#S6.SS2.SSS1 "6.2.1 Preferences and experiences across voice agents ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). Vanilla, while popular for its conversational freedom, faced criticism for occasional microaggressions and off-topic remarks, making it less suitable for specific applications like TextileBot, as discussed in Section [7.3.2](https://arxiv.org/html/2406.10590v1#S7.SS3.SSS2 "7.3.2 Lessons Learned for design LLM-mediated voice agent ‣ 7.3 Optimizing LLM-mediated voice agent design for specific domains ‣ 7 Discussion & Future Directions ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"). Furthermore, participants expressed a desire for more “emotions embedded” within agent conversations (e.g. humour, jokes) [[61](https://arxiv.org/html/2406.10590v1#bib.bib61), [62](https://arxiv.org/html/2406.10590v1#bib.bib62), [101](https://arxiv.org/html/2406.10590v1#bib.bib101)] in Section [6.2.3](https://arxiv.org/html/2406.10590v1#S6.SS2.SSS3 "6.2.3 Suggestions for improvements ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot"), implying a stronger preference for human-agent interactions that emulate human-like communication. Overall, we noticed that an appropriate level of prompting, e.g. add more social ability, can enhance user engagement, as seen with the Assistant (Section [6.2](https://arxiv.org/html/2406.10590v1#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). However, it is crucial to strike a balance, overemphasis on domain-specific details, as seen in the Expert, can detract from user engagement.

##### Short answers in a conversation

We had this feedback during the pilot study, to further investigate this issue, we prompted Assistant to respond in limited words (short answer) to distinguish from others. Participants frequently commented on the verbosity of responses from the Expert with some expressing a desire for a feature to speed up or stop lengthy replies *“I wish there is a speed up and stop button.”*. This suggests that while detailed responses can be informative, they may overwhelm users in conversational contexts. Moreover, the use of ChatGPT as a foundational model for voice agents should be approached cautiously due to its tendency for verbosity, a result of training biases favouring more comprehensive answers [[87](https://arxiv.org/html/2406.10590v1#bib.bib87), [34](https://arxiv.org/html/2406.10590v1#bib.bib34)].

##### Avoid repeating and being persuasive

Some participants expressed that when the agent repeatedly states the same domain specific content or attempts to be overly persuasive (Section [6.2](https://arxiv.org/html/2406.10590v1#S6.SS2 "6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")), their engagement with the conversation decreases. This issue, though sometimes inevitable in educational or specialized domains, highlights the need for designing voice agents with diverse and balanced responses to sustain user interest and trust.

##### Interactive dialogue - ask back and interrupt

Based on feedback from participants (Section LABEL:), we found that they felt most engaged when the agent actively asked questions, indicating a preference for interactive dialogue. Our participants found the conversation with TextileBot Expert and Assistant to be more intelligent than Google Assistant or Alexa, in part due to its memory function, which is achieved through our System Optimization (Section [3.3](https://arxiv.org/html/2406.10590v1#S3.SS3 "3.3 Phase 3: Enable continuous LLM interaction with memory through System Optimization ‣ 3 Prototyping Domain-Specific Voice Agents ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). Moreover, a critical aspect of natural conversation is the ability to interrupt and interact fluidly [[46](https://arxiv.org/html/2406.10590v1#bib.bib46)]. Participants emphasized that voice agents lacking this feature fail to provide a truly conversational experience (Section [6.2.3](https://arxiv.org/html/2406.10590v1#S6.SS2.SSS3 "6.2.3 Suggestions for improvements ‣ 6.2 Overall feedback ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot")). Therefore, integrating the ability for interactive dialogue is desired for enhancing user engagement.

#### 7.3.2 Lessons Learned for design LLM-mediated voice agent

We distil key lessons from our experiences in employing LLMs for developing voice agents, highlighting their benefits and limitations.

##### Fault tolerance

A significant advantage of utilizing prompted LLMs in CAs is their capacity for fault tolerance, particularly in correcting errors from other components like Automatic Speech Recognition (ASR). Our case study in textile circularity exemplifies this. Prompt-based agents, such as Expert and Assistant, successfully corrected a considerable number of ASR misrecognitions. For instance, the term ‘textile circularity’ was often misheard as ‘texas secularity’, ‘textile/test security’, or ‘regularity’, with such errors present in 62% of ASR error instances (Section LABEL:). Nevertheless, our Expert and Assistant reliably redirected the conversation back to relevant topics related to textile circularity. In contrast, the Vanilla showed limitations, often leading to irrelevant content and disappointing participants. This highlights the benefit of domain-specific awareness in LLMs, which not only enhances their understanding of the intended subject matter but also significantly improves the fault tolerance of voice agent architectures. For a more in-depth analysis of participant encounters with ASR errors, we discussed it in Section [6.5](https://arxiv.org/html/2406.10590v1#S6.SS5 "6.5 Human behaviour and reactions ‣ 6 Results ‣ LLM-Mediated Domain-Specific Voice Agents: The Case of TextileBot").

##### Neutrality

Although recent advances in LLMs have opened up many new possibilities; however, they have also raised significant worries and concerns. Not only is there a fear of the potential harmful contents these models could produce, but the model’s outputs are potentially biased [[11](https://arxiv.org/html/2406.10590v1#bib.bib11), [55](https://arxiv.org/html/2406.10590v1#bib.bib55), [18](https://arxiv.org/html/2406.10590v1#bib.bib18), [36](https://arxiv.org/html/2406.10590v1#bib.bib36)]. For example, in our case, we must instruct the model to “provide a sustainable clothing suggestion regardless of gender”. This is because, based on our pilot study, we found that when giving dressing suggestions, the model is not gender-neutral and has an obvious bias. The model always gives dressing suggestions with a female outlook. We also observed that LLMs are not politically neutral; one of our participants asked a question “Who has a more fashionable leader, China or Russia?” The agent consistently condemned the outfit of Putin. Drawing from our experience, we found that prompting may help mitigate the generation of biased content (e.g. gender-neutrality) from the LLM. However, it is difficult to completely restrict all forms of biases, as bias can manifest in many different ways.

##### Micro-aggression

Another concern is the LLM’s propensity to generate content with micro-aggression, as reported by three participants who found the Vanilla somewhat aggressive or mean. Previous research in this area has revealed that the content generated by LLMs can contain micro-aggression [[47](https://arxiv.org/html/2406.10590v1#bib.bib47), [13](https://arxiv.org/html/2406.10590v1#bib.bib13)]. Properly crafted prompts can significantly reduce such negative occurrences, as seen in Assistant and Expert; thus, a strict prompting protocol is almost essential to prevent such issues.

### 7.4 Limitations and future work

As our study is a first study in LLM-based voice agent, we highlighted a few limitations in our data, method and findings.

Firstly, our findings uncovered the existence of variations in participant preferences with respect to the voice agents. A small group of participants exhibited a preference for the responses of the Expert agent due to its perceived level of detail. However, we also intuitively suspect that factors such as participant backgrounds, their professions and past experiences may have influenced this preference. To obtain a clearer understanding of this relationship, it could be beneficial to implement a larger-scale study involving a diverse participant pool. In relation to this, we see considerable potential in incorporating participants’ psychological traits, such as extroversion and introversion, along with their demographic attributes [[30](https://arxiv.org/html/2406.10590v1#bib.bib30), [97](https://arxiv.org/html/2406.10590v1#bib.bib97)] in future studies. We did not explore this research dimension, but it could offer critical insights into the correlations between a user’s conversational habits and their engagement with voice agents.

Secondly, we excluded voice data due to ethical considerations. However, that is inevitably limiting our ability to tap into the wealth of insights offered by non-verbal cues (e.g. pitch, tone), particularly when it comes to analysing emotional facets (e.g. frustration, anger) as part of conversational styles [[84](https://arxiv.org/html/2406.10590v1#bib.bib84), [76](https://arxiv.org/html/2406.10590v1#bib.bib76)]. Despite this limitation, our work aligns with existing HCI research methods in CAs, encompassing both text and voice-based interactions. Accordingly, this limitation can be seen as an opportunity for future research to consider both verbal and non-verbal data for a more comprehensive understanding of voice-based conversations and interactions.

Thirdly, a subset of participants (N=4) have reported that the text-to-speech (TTS) voice adopted by TextileBot was too robotic, leading to less engagement. In general, the optimization of speech naturalness and accuracy emerged as key expectations from voice agents [[107](https://arxiv.org/html/2406.10590v1#bib.bib107)]. This feedback serves as useful design guidance for voice agents aiming for improved engagement and user satisfaction. Future research could then delve into advanced neural speech synthesis (neural TTS) [[59](https://arxiv.org/html/2406.10590v1#bib.bib59)] with varied genders and accents for personalizing the voice agent.

Fourthly, the agent interactions are based on a lab-based, single session. Although our study gave us already a richness of data and insights, an extended and repeated interaction with the different agents, both inside and outside laboratory environments would be desirable. This could provide a more nuanced understanding of the observed changes over time and user experiences [[93](https://arxiv.org/html/2406.10590v1#bib.bib93)]. Participants’ feedback further underlines this, as they suggested an initial increase in both engagement and interaction as the familiarity with the agent grew; however, this engagement was noted to decline towards the end of the study.

Lastly, three participants reported that the Vanilla TextileBot was slightly aggressive or potentially disrespectful. Previous research in this area has revealed that the content generated by LLMs can contain micro-aggression [[47](https://arxiv.org/html/2406.10590v1#bib.bib47), [13](https://arxiv.org/html/2406.10590v1#bib.bib13)]. Our other participants did not report this when the LLM is prompted appropriately; thus, a strict prompting protocol is almost essential to prevent such issues. Further exploration is needed to develop robust mechanisms that can reliably identify and prevent such offensive outputs, ensuring a safer and more respectful user experience.

## 8 Conclusion

In this paper, we introduce a novel three-phase method to prototype domain-specific voice agents using prompted LLMs. Our method is structured into three phases: (1) imbuing LLM with domain-specific knowledge by using Taxonomy-based Knowledge Structure Chain; (2) transitioning the LLM from task-agnostic to domain-specific focused including altering the conversational style, through prompting refinements, and (3) integrating system optimization to allow dialogue memorization. These phases are flexible and can be used independently or collectively to customize the LLM for particular tasks. We showcase our method with TextileBot, a voice agent designed for discussions on textile circularity, developed in three versions: Vanilla, Assistant, and Expert, each providing unique conversational features. To evaluate the effectiveness of these LLM-powered voice agents, we conducted an in-person user study. Our results showed that most participants engaged in multi-turn conversations with the agents, with their perceptions and behaviours significantly differing across the three versions. The descriptive user feedback with three agents aligned with our design intentions, confirming the efficacy of our approach in developing prompt-based voice agents. Key findings from the study highlight a preference for voice agents that offer concise, non-repetitive, and interactive dialogues. This includes the ability to ask questions, interrupt, and remember past conversations. Additionally, participants expressed a preference for agents that exhibit human-like qualities, such as humour. In the subsequent discussion in the paper, we shared insights and experiences related to enhancing voice agent design, along with a discussion of the challenges and lessons learned when utilizing LLMs in designing voice-based CAs. We delve into the nuances of these interactions and their implications for the future development of voice-based CAs in HCI to offer a broader scope of voice interfaces across various domains.

## Acknowledgments and Disclosure of Funding

This work was supported by the UK Research and Innovation (UKRI) National Interdisciplinary Circular Economy Centres Research programme, as part of the Textiles Circularity Centre (TCC) [grant number EP/V011766/1]. For the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising.

## References

*   [1] Google aiy voice kit V1. https://aiyprojects.withgoogle.com/, 2017.
*   [2] Google/aiyprojects-raspbian. https://github.com/google/aiyprojects-raspbian/releases, 2021.
*   [3] OpenAI Cookbook - Techniques to improve reliability. OpenAI, 2023.
*   [4] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.
*   [5] M. Allouch, A. Azaria, and R. Azoulay. Conversational agents: Goals, technologies, vision and challenges. Sensors, 21(24):8448, 2021.
*   [6] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami, F. Sala, and C. Ré. Ask Me Anything: A simple strategy for prompting language models, 2022.
*   [7] M. A. Bansal, D. R. Sharma, and D. M. Kathuria. A systematic review on data scarcity problem in deep learning: solution and applications. ACM Computing Surveys (CSUR), 54(10s):1–29, 2022.
*   [8] S. Barke, M. B. James, and N. Polikarpova. Grounded copilot: How programmers interact with code-generating models. arXiv preprint arXiv:2206.15000, 2022.
*   [9] A. Baughan, X. Wang, A. Liu, A. Mercurio, J. Chen, and X. Ma. A mixed-methods approach to understanding user trust after voice assistant failures. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1–16, 2023.
*   [10] R. Bavaresco, D. Silveira, E. Reis, J. Barbosa, R. Righi, C. Costa, R. Antunes, M. Gomes, C. Gatti, M. Vanzin, et al. Conversational agents in business: A systematic literature review and future research directions. Computer Science Review, 36:100239, 2020.
*   [11] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610–623, 2021.
*   [12] T. Bickmore and J. Cassell. Social dialongue with embodied conversational agents. Advances in natural multimodal dialogue systems, pages 23–54, 2005.
*   [13] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
*   [14] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
*   [15] B. E. Bullock and A. J. Toribio. Themes in the study of code-switching. The Cambridge handbook of linguistic code-switching, 117, 2009.
*   [16] D. Buschek, M. Zürn, and M. Eiband. The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–13, 2021.
*   [17] T. Chakrabarty, V. Padmakumar, and H. He. Help me write a poem: Instruction tuning as a vehicle for collaborative poetry writing. arXiv preprint arXiv:2210.13669, 2022.
*   [18] K. Chen, A. Shao, J. Burapacheep, and Y. Li. A critical appraisal of equity in conversational AI: Evidence from auditing GPT-3’s dialogues with different publics on climate change and Black Lives Matter, 2022.
*   [19] X. Chen, N. Zhang, X. Xie, S. Deng, Y. Yao, C. Tan, F. Huang, L. Si, and H. Chen. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In Proceedings of the ACM Web Conference 2022, pages 2778–2788, 2022.
*   [20] R. H. B. Christensen. ordinal—regression models for ordinal data. R package version, 28:2015, 2015.
*   [21] E. Clark, A. S. Ross, C. Tan, Y. Ji, and N. A. Smith. Creative writing with a machine in the loop: Case studies on slogans and stories. In 23rd International Conference on Intelligent User Interfaces, pages 329–340, 2018.
*   [22] L. Clark, N. Pantidi, O. Cooney, P. Doyle, D. Garaialde, J. Edwards, B. Spillane, E. Gilmartin, C. Murad, C. Munteanu, et al. What makes a good conversation? challenges in designing truly conversational agents. In Proceedings of the 2019 CHI conference on human factors in computing systems, pages 1–12, 2019.
*   [23] M. Colucci, A. Tuan, and M. Visentin. An empirical investigation of the drivers of csr talk and walk in the fashion industry. Journal of Cleaner Production, 248:119200, 2020.
*   [24] J. Condliffe. Ai voice assistant apps are proliferating, but people don’t use them. Technology Review, 2017.
*   [25] B. R. Cowan, N. Pantidi, D. Coyle, K. Morrissey, P. Clarke, S. Al-Shehri, D. Earley, and N. Bandeira. ” what can i help you with?” infrequent users’ experiences of intelligent personal assistants. In Proceedings of the 19th international conference on human-computer interaction with mobile devices and services, pages 1–12, 2017.
*   [26] N. Dahlbäck, A. Jönsson, and L. Ahrenberg. Wizard of oz studies—why and how. Knowledge-based systems, 6(4):258–266, 1993.
*   [27] A. R. de Lacerda and C. S. Aguiar. Floss faq chatbot project reuse: how to allow nonexperts to develop a chatbot. In Proceedings of the 15th International Symposium on Open Collaboration, pages 1–8, 2019.
*   [28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
*   [29] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Unified language model pre-training for natural language understanding and generation. Advances in Neural Information Processing Systems, 32, 2019.
*   [30] P. R. Doyle, J. Edwards, O. Dumbleton, L. Clark, and B. R. Cowan. Mapping perceptions of humanness in intelligent personal assistant interaction. In Proceedings of the 21st international conference on human-computer interaction with mobile devices and services, pages 1–12, 2019.
*   [31] T. E. M. Foundation. Fashion and a circular economy — ellen macarthur foundation.
*   [32] A. Frummet, D. Elsweiler, and B. Ludwig. “what can i cook with these ingredients?”-understanding cooking-related information needs in conversational search. ACM Transactions on Information Systems (TOIS), 40(4):1–32, 2022.
*   [33] J. Fürnkranz. Round robin classification. The Journal of Machine Learning Research, 2:721–747, 2002.
*   [34] L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint arXiv:2210.10760, 2022.
*   [35] Google. Google Assistant, your own personal Google default. https://assistant.google.com/.
*   [36] N. Goyal, I. D. Kivlichan, R. Rosen, and L. Vasserman. Is Your Toxicity My Toxicity? Exploring the Impact of Rater Identity on Toxicity Annotation. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW2):363:1–363:28, 2022.
*   [37] I. Gupta, B. Di Eugenio, B. Ziebart, A. Baiju, B. Liu, B. Gerber, L. Sharp, N. Nabulsi, and M. Smart. Human-human health coaching via text messages: Corpus, annotation, and analysis. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 246–256, 2020.
*   [38] G. Haas, M. Rietzler, M. Jones, and E. Rukzio. Keep it short: A comparison of voice assistants’ response behavior. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1–12, 2022.
*   [39] X. Han, W. Zhao, N. Ding, Z. Liu, and M. Sun. Ptr: Prompt tuning with rules for text classification. AI Open, 3:182–192, 2022.
*   [40] C. N. Harrington, R. Garg, A. Woodward, and D. Williams. “it’s kind of like code-switching”: Black older adults’ experiences with a voice assistant for health information seeking. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1–15, 2022.
*   [41] R. Hoegen, D. Aneja, D. McDuff, and M. Czerwinski. An end-to-end conversational style matching agent. In Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, pages 111–118, 2019.
*   [42] T. Hunter. Siri and alexa are getting on their owners’ last nerves. the washington post, 2022.
*   [43] D. Ippolito, A. Yuan, A. Coenen, and S. Burnam. Creative writing with an ai-powered writing assistant: Perspectives from professional writers. arXiv preprint arXiv:2211.05030, 2022.
*   [44] B. Jan-Petter and J. J. Gumperz. Social meaning in linguistic structure: Code-switching in norway. In The bilingualism reader, pages 75–96\. Routledge, 2020.
*   [45] E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, and C. J. Cai. PromptMaker: Prompt-based Prototyping with Large Language Models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI EA ’22, pages 1–8, New York, NY, USA, 2022\. Association for Computing Machinery.
*   [46] B. Jordan and A. Henderson. Interaction analysis: Foundations and practice. The journal of the learning sciences, 4(1):39–103, 1995.
*   [47] D. Jurgens, E. Chandrasekharan, and L. Hemphill. A just and comprehensive strategy for using nlp to address online abuse. arXiv preprint arXiv:1906.01738, 2019.
*   [48] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy. Challenges and applications of large language models. arXiv preprint arXiv:2307.10169, 2023.
*   [49] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
*   [50] M. Krzywinski, N. Altman, and P. Blainey. Nested designs. Nature Methods, 11(10):977–979, 2014.
*   [51] S. Kusal, S. Patil, J. Choudrie, K. Kotecha, S. Mishra, and A. Abraham. Ai-based conversational agents: A scoping review from technologies to future directions. IEEE Access, 2022.
*   [52] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. 2001.
*   [53] P. Lambe. Organising knowledge: taxonomies, knowledge and organisational effectiveness. Elsevier, 2014.
*   [54] M. Lee, P. Liang, and Q. Yang. Coauthor: Designing a human-ai collaborative writing dataset for exploring language model capabilities. In CHI Conference on Human Factors in Computing Systems, pages 1–19, 2022.
*   [55] M. Lee, M. Srivastava, A. Hardy, J. Thickstun, E. Durmus, A. Paranjape, I. Gerard-Ursin, X. L. Li, F. Ladhak, F. Rong, et al. Evaluating human-language model interaction. arXiv preprint arXiv:2212.09746, 2022.
*   [56] P. Lee, S. Bubeck, and J. Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine, 388(13):1233–1239, 2023.
*   [57] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.
*   [58] C.-H. Li, S.-F. Yeh, T.-J. Chang, M.-H. Tsai, K. Chen, and Y.-J. Chang. A conversation analysis of non-progress and coping strategies with a banking task-oriented chatbot. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–12, 2020.
*   [59] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu. Neural speech synthesis with transformer network. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6706–6713, 2019.
*   [60] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.
*   [61] Q. V. Liao, M. Davis, W. Geyer, M. Muller, and N. S. Shami. What can you do? studying social-agent orientation and agent proactive interactions with an agent for employees. In Proceedings of the 2016 acm conference on designing interactive systems, pages 264–275, 2016.
*   [62] Q. V. Liao, M. Mas-ud Hussain, P. Chandar, M. Davis, Y. Khazaeni, M. P. Crasso, D. Wang, M. Muller, N. S. Shami, and W. Geyer. All work and no play? In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pages 1–13, 2018.
*   [63] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.
*   [64] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
*   [65] S. Meyer, D. Elsweiler, B. Ludwig, M. Fernandez-Pichel, and D. E. Losada. Do we still need human assessors? prompt-based gpt-3 user simulation in conversational ai. In Proceedings of the 4th Conference on Conversational User Interfaces, pages 1–6, 2022.
*   [66] S. H. Ng, D. Bell, and M. Brooke. Gaining turns and achieving high influence ranking in small conversational groups. British Journal of Social Psychology, 32(3):265–275, 1993.
*   [67] C. O’Connor, S. Michaels, S. Chapin, and A. G. Harbaugh. The silent and the vocal: Participation and learning in whole-class discussion. Learning and instruction, 48:5–13, 2017.
*   [68] J. O’Connor and J. Andreas. What context features can transformer language models use? arXiv preprint arXiv:2106.08367, 2021.
*   [69] C. Oertel, G. Castellano, M. Chetouani, J. Nasir, M. Obaid, C. Pelachaud, and C. Peters. Engagement in human-agent interaction: An overview. Frontiers in Robotics and AI, 7:92, 2020.
*   [70] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. OpenAI, 2022.
*   [71] OpenAI. Gpt-4 technical report. (arXiv:2303.08774), Mar 2023. arXiv:2303.08774 [cs].
*   [72] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
*   [73] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318, 2002.
*   [74] E. Parliament. The impact of textile production and waste on the environment (infographic), 2020.
*   [75] B. Petreca, S. Baurley, K. Hesseldahl, A. Pollmann, and M. Obrist. The compositor tool: Investigating consumer experiences in the circular economy. Multimodal Technologies and Interaction, 6(4):24, 2022.
*   [76] D. Phutela. The importance of non-verbal communication. IUP Journal of Soft Skills, 9(4):43, 2015.
*   [77] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356, 2022.
*   [78] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492–28518\. PMLR, 2023.
*   [79] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.
*   [80] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8689–8696, 2020.
*   [81] G. Redeker. On differences between spoken and written language. Discourse processes, 7(1):43–55, 1984.
*   [82] C. K. Riessman. Analysis of personal narratives. Inside interviewing: New lenses, new concerns, pages 331–346, 2003.
*   [83] K. A. Schumacher and A. L. Forster. Textiles in a circular economy: An assessment of the current landscape, challenges, and opportunities in the united states. Frontiers in Sustainability, 3:146, 2022.
*   [84] K. Seaborn, N. P. Miyake, P. Pennefather, and M. Otake-Matsuura. Voice in human–agent interaction: A survey. ACM Computing Surveys (CSUR), 54(4):1–43, 2021.
*   [85] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.
*   [86] E. M. Smith, O. Hsu, R. Qian, S. Roller, Y.-L. Boureau, and J. Weston. Human evaluation of conversations is an open problem: comparing the sensitivity of various methods for evaluating dialogue agents. arXiv preprint arXiv:2201.04723, 2022.
*   [87] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.
*   [88] D. Tannen. Conversational style: Analyzing talk among friends. Oxford University Press, 2005.
*   [89] P. Ten Have. Doing conversation analysis. Sage, 2007.
*   [90] P. Vaithilingam, T. Zhang, and E. L. Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1–7, 2022.
*   [91] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
*   [92] A. Venkatesh, C. Khatri, A. Ram, F. Guo, R. Gabriel, A. Nagar, R. Prasad, M. Cheng, B. Hedayatnia, A. Metallinou, et al. On evaluating and comparing conversational agents. 2017.
*   [93] A. P. Vermeeren, E. L.-C. Law, V. Roto, M. Obrist, J. Hoonhout, and K. Väänänen-Vainio-Mattila. User experience evaluation methods: current state and development needs. In Proceedings of the 6th Nordic conference on human-computer interaction: Extending boundaries, pages 521–530, 2010.
*   [94] S. T. Völkel, D. Buschek, M. Eiband, B. R. Cowan, and H. Hussmann. Eliciting and analysing users’ envisioned dialogues with perfect voice assistants. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1–15, 2021.
*   [95] S. T. Völkel, S. Meindl, and H. Hussmann. Manipulating and evaluating levels of personality perceptions of voice assistants through enactment-based dialogue design. In Proceedings of the 3rd Conference on Conversational User Interfaces, pages 1–12, 2021.
*   [96] S. T. Völkel, R. Schödel, D. Buschek, C. Stachl, V. Winterhalter, M. Bühner, and H. Hussmann. Developing a personality model for speech-based conversational agents using the psycholexical approach. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–14, 2020.
*   [97] S. T. Völkel, R. Schoedel, L. Kaya, and S. Mayer. User perceptions of extraversion in chatbots after repeated use. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1–18, 2022.
*   [98] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile ui using large language models. arXiv preprint arXiv:2209.08655, 2022.
*   [99] B. Wang, G. Li, and Y. Li. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 1–17, 2023.
*   [100] B. Wang, G. Li, X. Zhou, Z. Chen, T. Grossman, and Y. Li. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pages 498–510, 2021.
*   [101] Y.-C. Wang, A. Papangelis, R. Wang, Z. Feizollahi, G. Tur, and R. Kraut. Can you be more social? injecting politeness and positivity into task-oriented conversational agents. arXiv preprint arXiv:2012.14653, 2020.
*   [102] T. D. Wilson. Models in information behaviour research. Journal of documentation, 55(3):249–270, 1999.
*   [103] T. Wu, M. Terry, and C. J. Cai. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pages 1–22, 2022.
*   [104] Q. Yang, J. Cranshaw, S. Amershi, S. T. Iqbal, and J. Teevan. Sketching nlp: A case study of exploring the right things to design with language intelligence. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 1–12, 2019.
*   [105] M. Zaib, Q. Z. Sheng, and W. Emma Zhang. A short survey of pre-trained language models for conversational ai-a new age in nlp. In Proceedings of the Australasian computer science week multiconference, pages 1–4, 2020.
*   [106] J. Zamfirescu-Pereira, H. Wei, A. Xiao, K. Gu, G. Jung, M. G. Lee, B. Hartmann, and Q. Yang. Herding ai cats: Lessons from designing a chatbot by prompting gpt-3. 2023.
*   [107] L. Zhang, L. Jiang, N. Washington, A. A. Liu, J. Shao, A. Fourney, M. R. Morris, and L. Findlater. Social media through voice: Synthesized voice qualities and self-presentation. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1):1–21, 2021.
*   [108] S. Zhong, M. Ribul, Y. Cho, and M. Obrist. Textilenet: A material taxonomy-based fashion textile dataset. arXiv preprint arXiv:2301.06160, 2023.
*   [109] A. F. Zuur, E. N. Ieno, N. J. Walker, A. A. Saveliev, G. M. Smith, et al. Mixed effects models and extensions in ecology with R, volume 574. Springer, 2009.