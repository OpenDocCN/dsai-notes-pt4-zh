<!--yml
category: 未分类
date: 2025-01-11 12:42:41
-->

# WESE: Weak Exploration to Strong Exploitation for LLM Agents

> 来源：[https://arxiv.org/html/2404.07456/](https://arxiv.org/html/2404.07456/)

Anonymous Authors Anonymous Affiliation anonymous@example.com    Xu Huang${}^{1}$    Weiwen Liu${}^{2}$    Xiaolong Chen${}^{1}$    Xingmei Wang${}^{1}$    Defu Lian${}^{1}$¹¹1Defu Lian is the corresponding author.    Yasheng Wang${}^{2}$    Ruiming Tang${}^{2}$    Enhong Chen${}^{1}$ ${}^{1}$University of Science and Technology of China, Hefei, China
${}^{2}$Huawei Noah’s Ark Lab, Shenzhen, China xuhuangcs, chenxiaolong, xingmeiwang@mail.ustc.edu.cn, {liandefu, cheneh}@ustc.edu.cn,
{liuweiwen8,wangyasheng, tangruiming}@huawei.com

###### Abstract

Recently, large language models (LLMs) have demonstrated remarkable potential as an intelligent agent. However, existing researches mainly focus on enhancing the agent’s reasoning or decision-making abilities through well-designed prompt engineering or task-specific fine-tuning, ignoring the procedure of exploration and exploitation. When addressing complex tasks within open-world interactive environments, these methods exhibit limitations. Firstly, the lack of global information of environments leads to greedy decisions, resulting in sub-optimal solutions. On the other hand, irrelevant information acquired from the environment not only adversely introduces noise, but also incurs additional cost. This paper proposes a novel approach, Weak Exploration to Strong Exploitation (WESE), to enhance LLM agents in solving open-world interactive tasks. Concretely, WESE involves decoupling the exploration and exploitation process, employing a cost-effective weak agent to perform exploration tasks for global knowledge. A knowledge graph-based strategy is then introduced to store the acquired knowledge and extract task-relevant knowledge, enhancing the stronger agent in success rate and efficiency for the exploitation task. Our approach is flexible enough to incorporate diverse tasks, and obtains significant improvements in both success rates and efficiency across four interactive benchmarks.

## 1 Introduction

Large language models (LLMs) showcase a myriad of capabilities across diverse domains, encompassing human-computer conversation, instruction following, reasoning, and few-shot learning Zhao et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib30)). These comprehensive abilities form a robust foundation, positioning LLMs as intelligent agents in solving open-world tasks, such as household tasks and open-world question-answering tasks Wang et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib18)); Xi et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib24)). Recently, there have been numerous works to investigate the potential of LLM agents in enhancing their capabilities for open-world tasks.

Benefiting from the capabilities of LLMs in instruction-following and few-shot learning, most methods guide LLMs in decision-making tasks through human-crafted design, avoiding the costly fine-tuning of LLMs Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)); Wang et al. ([2022b](https://arxiv.org/html/2404.07456v1#bib.bib16)); Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)); Kojima et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib4)). Existing prompt-engineering approaches primarily consider two factors: how to incorporate task-relevant information in the prompt, and how to elicit the reasoning ability of LLMs through prompts. Task-relevant information encompasses task descriptions and contextual feedback, such as the question and pertinent task statements in question-answering tasks, along with textual materials retrieved by the agent from the web while problem-solving. To enhance the reasoning capabilities of LLM agents, methods like CoT Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)), ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) Reflexion Shinn et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9)), et al, inspire LLMs to engage in reasoning by constructing few-shot examples with explicit reasoning paths.

![Refer to caption](img/099da06558e40fdd623dadbb943f3aeb.png)

(a) ScienceWorld. Lack of global environmental information causes failure due to trapping in a loop or sub-optimal solution.

![Refer to caption](img/13a3944e82431c3ff9f16ae0b18c3c05.png)

(b) HotPotQA. The green sentence is helpful while others are task-irrelevant.

Figure 1: Examples for sub-optimal decisions and irrelevant information in feedbacks.

However, open-world tasks serve as a simulation of the real environment, wherein an agent explores and interacts continuously with the environment to acquire more information for solving complex tasks Côté et al. ([2019](https://arxiv.org/html/2404.07456v1#bib.bib2)); Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10)); Wang et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)). There are several characteristics of such tasks, making them more challenging. The ability of LLM agents is far from optimal due to the following challenges: 1) Complexity. Each task involves multi-step actions and each task can have multiple feasible solutions. 2) Uncertainty. The agent cannot obtain all the information from the initial task description, and additional information must be acquired through exploration. Regarding these challenges, solving these tasks necessitates multi-step exploration and exploitation by the agent. Exploration involves perceiving the environment and obtaining task-relevant information, while exploitation involves making action decisions based on existing knowledge. In existing prompt-based methods, exploration and exploitation issues are often overlooked, embedded within the reasoning process of the LLM Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)), leading to two major problems.

Firstly, the lack of global awareness of the environment at the outset solutions results in suboptimal decision-making by the LLM. As illustrated in Figure [1(a)](https://arxiv.org/html/2404.07456v1#S1.F1.sf1 "1(a) ‣ Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"), the goal is to find one aluminum object and test its conductivity. The agent is located outside initially. The best trajectory is marked with the white line, where the agent goes to the kitchen to take the aluminum fork first and then go to the workshop. When lack of global environmental information, the agent probably gets trapped in some room due to failure in finding an aluminum object (the red line) or chooses a more time-consuming way (the blue line). Secondly, the knowledge acquired by the LLM from environmental exploration tends to be excessive, including irrelevant information to the task. The presence of such information not only disrupts LLM decision-making but also incurs additional costs. Referred in Figure [1(b)](https://arxiv.org/html/2404.07456v1#S1.F1.sf2 "1(b) ‣ Figure 1 ‣ 1 Introduction ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"), the feedback from the environment usually consists of massive task-irrelevant information while only one helpful sentence, i.e. the green line in this example, resulting in extra token usage of LLM and a negative effect on making optimal decisions.

To address the above limitations, we propose a novel prompt-based strategy to enhance the LLM agent in this work, termed Weak Exploration to Strong Exploitation (WESE). To tackle the first limitation, we introduce an idea that decouples the exploration and exploitation. Specifically, we construct two distinct LLM agents for exploration and exploitation tasks, respectively. In the exploration task, the LLM agent’s goal is to interact with the environment, exploring potentially helpful environmental information for task resolution. In the exploitation task, the information obtained during exploration serves as a global environmental prior, aiding the LLM agent in reasoning and decision-making to generate decisions. Regarding the second limitation, we compress the environmental information acquired by the exploration agent, structuring it in the form of a knowledge graph. During exploitation, we adopt a one-hop knowledge retrieval approach, selecting one-hop neighbors of task-relevant entities from the graph as priors, thereby reducing interference from irrelevant information. Furthermore, to further minimize resource consumption, we observe that a cost-effective weaker LLM (such as a 7B model) is fully capable of the less challenging exploratory tasks. Therefore, we propose the strategy of weak exploration to strong exploitation—leveraging the knowledge explored by the weak LLM agent to enhance the performance of the strong LLM agent.

Our main contributions are summarized as follows:

*   •

    To the best of our knowledge, this is the first work to investigate the effect of decoupling exploration and exploitation for LLM agents in open-world tasks. We further propose WESE, leveraging a weaker agent to enhance the stronger agent in a cost-effective manner.

*   •

    To better leverage the environmental information obtained from exploration, we introduce a strategy to compress it into a knowledge graph. Then we devise a one-hop retrieval approach to filter out the irrelevant information.

*   •

    Experimental results over four open-world interactive benchmarks demonstrate the superiority of WESE, notably in achieving a remarkable balance between effectiveness, efficiency and cost.

## 2 Related Works

### 2.1 LLM agents

With the emergence of LLMs, their intelligence has sparked considerable potential in applying LLMs as the brains of agents. Existing LLM agent works primarily consider three key modules: planning, tool usage, and memory Wang et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib18)). Planning module aims to empower agent with the task-decomposition ability, encompassing works on task decomposition Wang et al. ([2023c](https://arxiv.org/html/2404.07456v1#bib.bib19)), feedback-driven adjustments Shinn et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9)), and multi-path reasoning Yao et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib28)); Besta et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib1)). Tool usage aims to strengthen the ability to use external tools Qin et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib7)). For instance, Visual ChatGPT Wu et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib23)) incorporates visual models as tools to augment the LLM’s visual capabilities. ToolLlama Qin et al. ([2023b](https://arxiv.org/html/2404.07456v1#bib.bib8)) fine-tunes Llama’s ability to leverage various APIs. The memory module focuses on storing feedback information perceived from the environment, assisting the agent with experience, and fostering the growth of the agent. In Generative Agents Park et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib6)), memories of simulated roles are stored as texts, utilizing RAG for relevant pieces. REMEMBER Zhang et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib29)) proposes a semi-parametric memory, i.e. the Q-value table, to record rewards as the value and action in a given environment and task as the key. MemoryBank Wang et al. ([2023d](https://arxiv.org/html/2404.07456v1#bib.bib20)) leverages the Ebbinghaus forgetting curve, incorporating update and forgetting mechanisms into the memory design.

In our proposed WESE, the knowledge graph is essentially a memory, updating information obtained through exploration into the graph.

### 2.2 LLM for open-world tasks

Open-world tasks represent the simulation of real-world environments. Within these tasks, agents engage in continuous interactions with the environment to gather pertinent information, subsequently making decisions and taking action to accomplish goals. Open-world tasks typically exhibit fewer constraints on the process, placing greater emphasis on the final rewards. Representative examples of open-world tasks include games like “Minecraft” Wang et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib17), [e](https://arxiv.org/html/2404.07456v1#bib.bib21)), where textual information and visual feedback are involved. Another category comprises text-based simulators based on the TextWorld Côté et al. ([2019](https://arxiv.org/html/2404.07456v1#bib.bib2)), such as AlfWorld Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10)), which involves household tasks, ScienceWorld Wang et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)), which involves simple scientific experiments, and question-answering tasks Yang et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib25)); Thorne et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib12)) where agents need to interact with the web to obtain supporting information, such as Wikipedia. In tackling such tasks, Chain-of-Thought(CoT) Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)) proposes adding few-shot examples in the prompt, guiding the LLM to solve the task step by step. ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) induces the reasoning capability of LLMs by introducing an extra thought step. Subsequent methods have built upon ReAct, with enhancements such as the Reflexion Shinn et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib9)) mechanism, allowing agents to learn from mistakes in subsequent attempts. Additionally, several methods leverage the coding capabilities of LLMs, transforming tasks into programming tasks and guiding LLMs to generate codes as plans, such as VOYAGER Wang et al. ([2023a](https://arxiv.org/html/2404.07456v1#bib.bib17)).

## 3 Methodologies

![Refer to caption](img/8d01c5c11b7ce33b88aa1886b8e48e62.png)

Figure 2: Framework of WESE. The left part represents the weak exploration and the right part represents the strong exploitation. We employ Llama-2-7B as the weak agent and text-davinci-003 as the strong agent in the implementation.

### 3.1 Decoupling Exploration and Exploitation

Open-world tasks differ from traditional reasoning and decision-making tasks. Traditional reasoning Huang and Chang ([2022](https://arxiv.org/html/2404.07456v1#bib.bib3)); Sun et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib11)) or decision-making Yang et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib26)) tasks typically present all relevant information at once, requiring the agent to deduce and make a plan based on the provided information, such as mathematical calculations or logical reasoning problems. Conversely, in open-world tasks, only the task description is initially specified. In this context, the agent must continually interact with the environment to obtain supporting information, comprising the exploration and exploitation steps.

Let $E$ and $T$ represent the environment and the task, $\Theta$ denote the LLM, and $P$ denote the prompt. The action space of the agent is defined as $\mathcal{A}=\mathcal{A}_{e}\cup\mathcal{A}_{t}$, where $\mathcal{A}_{e}$ and $\mathcal{A}_{t}$ represent the action set of exploration and exploitation, respectively. Exploration and exploitation are denoted by the functions $explore(\cdot)$ and $exploit(\cdot)$. The information given by the environment in the $i$-th step is denoted as $F_{i}$. Regarding existing methods such as ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) where exploration and exploitation steps are embedded in reasoning, the action taken at step $i$ is represented as follows:

|  | $\small\centering a_{i}=reason(E,T,s_{i-1};\Theta,P,K=\cup_{j<i}\{F_{j}\})\in% \mathcal{A}_{e}\cup\mathcal{A}_{t}.\@add@centering$ |  |

where $reasion(\cdot)$ denotes the mix of explore and exploit.

Input: Knowledge triplets set $K$.Output: Knowledge graph $G$.1  Entity set $E\leftarrow\{\}$;2 Relation set $R\leftarrow\{\}$;3 Adjacency matrix $M$;4  for *$x\in K$* do5        $h,r,t\leftarrow x$;6        $E\leftarrow E\cup\{h,t\}$; $R\leftarrow R\cup\{r\}$; $M[h][t]\leftarrow r$;7      8$G.E\leftarrow E$; $G.R\leftarrow R$; $G.M\leftarrow M$;

Algorithm 1 Graph construction algorithm.

Input: Knowledge graph $G$, Task $T$, LLM $\Theta$.Output: Triplets set $K$.1 Task-related entity set $E\leftarrow extract(G.E,T;\Theta)$;2  $K\leftarrow\{\}$;3  for *$e_{i}\in E$* do4        for *$e_{j}\in E\setminus\{e_{i}\}$* do5              $r\leftarrow G.M[e_{i}][e_{j}]$;6              if *$r\neq\text{empty}$* then7                    $K\leftarrow K\cup\left\{\big{(}e_{i},r,e_{j}\big{)}\right\}$;8                  9            10      

Algorithm 2 Triplet retrieval algorithm.

Within this paradigm, the knowledge $K$ utilized is solely the limited information about the environment obtained through partial observations. Particularly, greedy decisions are taken in the initial steps when the agent possesses limited awareness of the environment. For instance, in a task such as “cleaning some apples with soap” and the agent’s initial location is the hall. The actual locations of the apple and soap are in the drawer of the table in the hall and on the sink in the kitchen, respectively. The lack of environmental knowledge may lead the agent to be misled by the world knowledge of the LLM, going to the kitchen to find the apple. Consequently, substantial efforts traversing every corner of the kitchen are wasted, resulting in suboptimal plans and even failures due to trapping in the loop. Therefore, we investigate the strategy to decouple exploration and exploitation, formalized as follows:

|  | $\small a_{i}=\left\{\begin{aligned} &explore(E,T,s_{i-1};\Theta,P_{e})\in% \mathcal{A}_{e},\;i<N_{e};\\ &exploit(E,T,s_{i-1};\Theta,P_{t},K=\cup_{j\leq N}\{F_{j}\})\in\mathcal{A}_{t}% ,i\geq N_{e}.\end{aligned}\right.$ |  |

where $P_{e}$ and $P_{t}$ represent the prompts of the exploration and exploitation task, respectively. $N_{e}$ is the maximum number of steps of exploration, which could also be determined by the agent, such as terminating the exploration automatically when it thinks the obtained information is sufficient.

Different from the previous methods, our method places the whole exploration phase before exploitation explicitly, as opposed to the alternation of exploration and exploitation. In this manner, the agent has extensively explored the environment, acquiring global environmental prior knowledge denoted as $K=\cup_{j\leq N}\{F_{j}\}$. Exploitation with global knowledge benefits the effectiveness and efficiency of the solutions, which is empirically validated in our experiments.

However, two subsequent issues exist following the decoupling approach. Firstly, the information obtained from environmental feedback is huge due to the extensive exploration, including a lot of task-irrelevant information. Secondly, the extensive exploration contributes to increased resource consumption, such as token usage. Therefore, we demand an efficient mechanism for information transfer between exploration and exploitation and a cost-efficient exploration-exploitation strategy. We address the two issues in the subsequent parts of this section.

### 3.2 Knowledge Compression and Retrieval

Real-world textual information exhibits inherent sparsity, characterized by long sentences consisting of plenty of non-informative conjunctions and adjectives. Environmental feedback in open-world tasks manifests as such text, where the cumulative extensive exploration yield long and unstructured textual information, demonstrating serve sparsity. Considering the limited context window of the LLM and the expensive cost of token usage, it is necessary to compress the sparse information. Leveraging a knowledge graph (KG) to store information has proved advantageous in enhancing information density and leveraging domain-specific knowledge in existing works Pan et al. ([2024](https://arxiv.org/html/2404.07456v1#bib.bib5)).

Consequently, benefiting from the superiority of LLM in relation-extraction tasks Wadhwa et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib14)), we extract the knowledge from the received feedback to form an environmental knowledge graph. Specifically, the LLM extracts knowledge triplets from the environmental feedback after each exploration step, updating them into the knowledge graph. For example, as for the search result given by Wikipedia “Since 2005 Wendy Schaal has primarily worked in voice acting, most notably voicing Francine Smith in the animated comedy television series American Dad!”, knowledge triplets are extracted as $\langle$Wendy Schaal, voice for, Francine Smith$\rangle$ and $\langle$Francine Smith, character in, American Dad!$\rangle$. Notably, the environmental knowledge graph we obtained is task-relevant, serving as a memory like the Random Access Memory(RAM). Actually, a worldwide knowledge graph could be leveraged and continually in our method, serving as a general memory. We leave it for further work.

Input: Environment $E$, Task $T$, Initial state $s_{0}$, Weak LLM $\Theta_{w}$, Strong LLM $\Theta_{s}$, Exploration prompt $P_{e}$, Exploitation prompt $P_{t}$, Limit of steps $N_{e},N_{t}$.Output: Plan $p$.// Exploration with weak LLM agent.1  $K\leftarrow\{\}$; $i\leftarrow 0$; $s^{e}_{i}\leftarrow s_{0}$;2  for *$i<N_{e}$* do3        $a^{e}_{i}\leftarrow explore(E,T,s^{e}_{i};\Theta_{w},P_{e})$;4        $s^{e}_{i},F_{i}\leftarrow step(E,s^{e}_{i},a^{e}_{i})$;5        $K^{{}^{\prime}}\leftarrow extract(F_{i};\Theta_{w})$;6        $K\leftarrow K\cup K^{{}^{\prime}}$;7        $i\leftarrow i+1$;8      $G_{K}\leftarrow construct\_graph(K)$ ; // Alg [1](https://arxiv.org/html/2404.07456v1#alg1 "1 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents")// Exploitation with strong LLM agent.9  $i\leftarrow 0$; $s^{t}_{i}\leftarrow s_{0}$; $p\leftarrow[]$;$\tilde{K}\leftarrow retrieve\_triplets(G_{K},T;\Theta_{w})$ ; // Alg [2](https://arxiv.org/html/2404.07456v1#alg2 "2 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents")10  for *$i<N_{t}$ and $F_{i}\neq\text{Completed}$* do11        $a^{t}_{i}\leftarrow exploit(E,T,s^{t}_{i};\Theta_{s},P_{t},\tilde{K})$;12        $s^{t}_{i},F_{i}\leftarrow step(E,s^{t}_{i},a^{t}_{i})$;13        $i\leftarrow i+1$;14        $p\leftarrow p+[a^{t}_{i}]$;15      

Algorithm 3 WESE algorithm.

Nevertheless, it is imperative to acknowledge that not all information in the knowledge graph proves useful. The introduction of task-irrelevant information has the potential to lead the hallucination phenomena of LLM, such as the confusion of entity and relation. For example, giving the triplet $\langle$Bob, favorite fruit, apple$\rangle$ and the question is “What’s the favorite fruit of Bill?”, the LLM would confuse the relation and answer with apple. Benefiting from the graph structure, we adopt a one-hop retrieval method to extract task-related information easily, illustrated in Algorithm [2](https://arxiv.org/html/2404.07456v1#alg2 "2 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). Concretely, we initiate the process by extracting involved entities from the task description with LLM. Subsequently, we perform a one-hop retrieval on the graph to obtain the neighbors of these entities. The retrieved knowledge triplets are then injected into the prompt, serving as task-relevant knowledge during the exploitation phase, thereby assisting the LLM in task-solving.

### 3.3 Weak Exploration to Strong Exploitation

Table 1: Results on ALFWorld(134 tasks). SR and AS are abbreviations for success rate and average steps of successful tasks, respectively. SESE represents the variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents the relative improvements compared to base methods, i.e. Act and ReAct. The bold and underline represent the best and the second best for the same base method.

| Performance | Effectiveness | Efficiency | Cost |
| --- | --- | --- | --- |
| Method | SR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$ | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Act | 0.43 | 0.00 | 10.83 | 0.00 | 4,908,548 | 21,243 | 98.60 | 0.00 |
| Act-WESE | 0.63 | +46.51 | 7.54 | +30.38 | 3,746,290 | 19,562 | 75.32 | +23.61 |
| Act-SESE | 0.67 | +55.81 | 6.73 | +37.86 | 7,259,508 | 75,153 | 146.69 | -48.77 |
| ReAct | 0.57 | 0.00 | 16.64 | 0.00 | 7,565,676 | 43,250 | 152.18 | 0.00 |
| ReAct-WESE | 0.72 | +26.32 | 13.69 | +17.73 | 5,032,374 | 41,004 | 101.47 | +33.32 |
| ReAct-SESE | 0.75 | +31.58 | 12.41 | +25.42 | 8,996,182 | 97,286 | 181.87 | -19.51 |

Table 2: Results on ScienceWorld(296 tasks). TR, AR and AS are abbreviations for total reward, average reward and average steps to get positive reward, respectively. Other symbols are consistent with Table [1](https://arxiv.org/html/2404.07456v1#S3.T1 "Table 1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").

| Performance | Effectiveness | Efficiency | Cost |
| --- | --- | --- | --- |
| Method | TR$\uparrow$ | AR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$ | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Act | 4908 | 16.58 | 0.00 | 18.00 | 0.00 | 13,554,960 | 55,817 | 272.22 | 0.00 |
| Act-WESE | 5198 | 17.56 | 5.91 | 15.68 | +12.91 | 13,491,043 | 65,952 | 271.14 | +0.40 |
| Act-SESE | 5249 | 17.73 | 6.94 | 15.39 | +14.49 | 36,424,190 | 165,568 | 731.80 | -168.83 |
| ReAct | 4454 | 15.05 | 0.00 | 20.00 | 0.00 | 17,716,698 | 84,724 | 356.03 | 0.00 |
| ReAct-WESE | 5317 | 17.96 | 19.34 | 19.65 | +1.77 | 16,310,632 | 80,851 | 327.83 | +7.92 |
| ReAct-WESE | 5053 | 17.07 | 13.42 | 19.02 | +4.92 | 40,293,571 | 196,338 | 809.80 | -127.45 |

Acquiring more comprehensive global information about the environment demands a considerable resource cost in the exploration process. However, compared to exploitation, exploration exhibits lower complexity, requiring less reasoning and induction. Concretely, exploration operations exhibit low requirements for the logic and coherence of actions, emphasizing actions pertaining to environmental observation. For example, the exploration actions mainly consist of several simple actions on decision-making benchmarks, such as “go to [room]”, “look around”, et al, while exploitation involves a series of coherent operations like (go to sink/stove, put the bowl in/on the sink/stove, activate the sink/stove, wait, deactivate the sink/stove). Therefore, we propose to use a weaker agent for the exploration to mitigate resource consumption, namely the weak exploration. From the perspective of the LLM agent, a weaker agent represents substituting the underlying LLM for exploration with a weaker LLM, i.e. an LLM with fewer parameters, thereby reducing costs. In our experiments, we compare performance between strong exploration and weak exploration. Our findings reveal that a weaker exploration has a negligible impact on the final success rate, yet it significantly lowers costs.

The framework of WESE is illustrated in Figure [2](https://arxiv.org/html/2404.07456v1#S3.F2 "Figure 2 ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). There are three key components in the framework: a weak LLM agent, a strong LLM agent, and a KG-based memory. The whole process consists of the weak exploration (left) and the strong exploitation (right). Meanwhile, we offer an algorithmic pseudo-code in Algorithm [3](https://arxiv.org/html/2404.07456v1#alg3 "3 ‣ 3.2 Knowledge Compression and Retrieval ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). First, a weak LLM agent is employed to explore the interactive environment to obtain information in line 1 to 7\. Then those knowledge triplets are organized as a knowledge graph $G_{K}$ in line 8, as illustrated in Algorithm [1](https://arxiv.org/html/2404.07456v1#alg1 "1 ‣ 3.1 Decoupling Exploration and Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). Further, the involved entities are extracted from the task with a LLM and the relevant triplets are retrieved from the graph in line 10\. Retrieved knowledge is leveraged for exploitation in line 12, serving as the prior knowledge.

## 4 Experiments

We employ two categories of interactive open-world tasks as benchmarks: decision-making and question-answering, where each task requires multi-step interactions with the environment. We evaluate our methods from three perspectives: effectiveness, efficiency and cost, representing whether the agent can complete the tasks, how many steps the agent would take to finish the task, and the expenses for the agent to complete the task, respectively.

### 4.1 Decision Making Tasks

We begin with the open-world decision-making tasks, where environments are based on a text-based simulator. The tasks are about the household, where the agent needs to explore various rooms and take operations on several objects.

#### 4.1.1 ALFWorld

ALFWorld Shridhar et al. ([2020](https://arxiv.org/html/2404.07456v1#bib.bib10)) is a synthetic text-based simulated interactive environment. It comprises six types of tasks where agents need to interact with the environment to generate a series of actions to solve household tasks. For example, in the task “clean some knife and put it in countertop”, the ideal solution involves actions such as (go to countertop 2, take knife 1, go to sinkbasin 2, clean knife 1, put knife 1 on countertop 2). These tasks vary in difficulty, with challenging tasks encompassing over 50 locations and requiring more than 50-step actions, posing challenges for both the exploration and exploitation processes.

![Refer to caption](img/2b1dcd580f8f3ff52c2955cd985b24dd.png)

(a) Relative improvements for Act-based methods.

![Refer to caption](img/61d17d3ddc752c7b28ad5d89d43996ea.png)

(b) Relative improvements for ReAct-based methods.

Figure 3: Relative improvements in success rate over various types of tasks on ALFWorld. The left tasks are more complicated.

To validate the effectiveness of WESE, we adopt Act Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) and ReAct Yao et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib27)) as baselines. Act leverages the idea of CoT, providing LLMs with few-shot interactive examples. ReAct, building upon Act, introduces an extra “thought” step where LLMs can choose to explicitly output their thought about the current state or generate action. In WESE, we initially use a Weak LLM for exploration to acquire task-relevant knowledge. We then leverage the obtained knowledge to solve problems with two base methods. We employ Llama-2-7B Touvron et al. ([2023](https://arxiv.org/html/2404.07456v1#bib.bib13)) as the weak LLM and text-davinci-003 (with probably more than 175 billion parameters) developed by OpenAI ²²2[https://platform.openai.com/](https://platform.openai.com/) as the strong LLM. The limits of steps $N_{e},N_{t}$ are both set to 50\. Our evaluation focuses on success rates, average steps to complete tasks, and the cost of OpenAI API tokens as three key metrics. Additionally, we introduce a variant of WESE—Strong Exploration to Strong Exploitation (SESE), where the weak LLM in the exploration process is replaced with the strong LLM, to verify the effectiveness of the decoupling strategy and examine the impact of LLM strength on exploration quality.

#### 4.1.2 ScienceWorld

Similar to ALFWorld, ScienceWorld Wang et al. ([2022a](https://arxiv.org/html/2404.07456v1#bib.bib15)) is an interactive household environment as well. However, the tasks in ScienceWorld are more challenging, involving scientific experiments such as boiling and creating a new color by mixing primary colors. The environment is more complex, comprising ten distinct rooms, each with different furnishings, and not each pair of rooms is connected.

We conduct experiments on eight types of tasks within ScienceWorld, choosing about 30 instances for each task due to a limited budget. Unlike ALFWorld where the agent can get a reward of 1 only when the task is completed, the agent in ScienceWorld receives partial rewards upon completing crucial steps, with the total reward reaching 100\. Given the challenging nature of the tasks, achieving a full reward of 100 is rare. Therefore, we utilize the number of steps taken by the agent until it first obtains a positive reward as the metric for efficiency. Other settings are consistent with ALFWorld.

#### 4.1.3 Results

The results on ALFWorld and ScienceWorld are shown in Table [1](https://arxiv.org/html/2404.07456v1#S3.T1 "Table 1 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents") and Table [2](https://arxiv.org/html/2404.07456v1#S3.T2 "Table 2 ‣ 3.3 Weak Exploration to Strong Exploitation ‣ 3 Methodologies ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"), respectively. We conclude several findings based on the results. Consistent with results reported in ReAct, ReAct outperforms Act on two benchmarks, showing the superiority of the “thought” step. However, this additional step leads to a longer action sequence, resulting in an average relative 32.38% increase in average steps. Decoupling of exploration and exploitation demonstrates advantages in effectiveness and efficiency, resulting in SESE outperforming baselines significantly with average relative 26.94% and 20.67% improvements in terms of success rate (average reward) and average steps. However, the cost of SESE increases a lot due to the introduction of extensive strong exploration, showing an average relative 91.14% increase over baselines.

WESE shows a better balance between effectiveness, efficiency, and cost, which saves 53.83% of costs with only relative 1.43% and 6.89% degradations in effectiveness and efficiency compared with SESE. In WESE, the weak LLM agent undertakes the exploration process, resulting in cost savings for extensive exploration. Besides, benefiting from the related triplets extracted from the explored KG, the strong LLM agent only needs to focus on exploitation, further decreasing the number of steps, evidenced by the decreased completion tokens and average steps.

We further investigate the improvements of WESE on various types of tasks, shown in Figure [3](https://arxiv.org/html/2404.07456v1#S4.F3 "Figure 3 ‣ 4.1.1 ALFWorld ‣ 4.1 Decision Making Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"). Both WESE and SESE show improvements over almost all types of tasks, further indicating the effectiveness of the decoupling strategy. In addition, the improvements in “clean” and “heat” tasks are greater than other tasks. The reason lies in that the two tasks involved more complicated exploitation compared with “put”, where the agents need to find the object first and then clean or heat it instead of just moving it to another place. The result demonstrates extensive exploration benefits more for complex tasks.

### 4.2 Question Answering Tasks

We also validate our WESE on two open-world interactive question-answering benchmarks, i.e., HotPotQA and FEVER. Different from traditional question-answering tasks where supporting sentences are given, those tasks provide the question only and require the agent to search information on the web step by step to give the final answer.

Table 3: Results on HotPotQA(500 tasks). SR and AS are abbreviations for success rate and average steps of successful tasks, respectively. SESE represents the variant of WESE—Strong Exploration to Strong Exploitation. The Imp represents the relative improvements compared to base methods, i.e. Act and ReAct. The bold and underline represent the best and the second best for the same base method.

| Performance | Effectiveness | Efficiency | Cost |
| Method | SR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$ | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |
| CoT | 0.318 | N/A | 1.00 | N/A | 261,347 | 25,382 | 5.73 | N/A |
| Act | 0.296 | 0.00 | 3.53 | 0.00 | 2,390,041 | 14,236 | 48.09 | 0.00 |
| Act-WESE | 0.353 | +19.26 | 2.69 | +23.80 | 2,307,421 | 13,973 | 46.42 | +3.45 |
| Act-SESE | 0.361 | +21.96 | 2.58 | +26.91 | 7,522,826 | 27,1551 | 155.89 | -224.18 |
| ReAct | 0.342 | 0.00 | 3.17 | 0.00 | 3,234,876 | 65,306 | 66.00 | 0.00 |
| ReAct-WESE | 0.394 | +15.20 | 2.29 | +27.76 | 2,574,401 | 67,908 | 52.85 | +19.93 |
| ReAct-SESE | 0.416 | +21.64 | 2.11 | +33.44 | 7,338,590 | 323,401 | 153.24 | -132.17 |

Table 4: Results on FEVER(500 tasks). The meanings of abbreviations and symbols are consistent with Table [3](https://arxiv.org/html/2404.07456v1#S4.T3 "Table 3 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents").

| Performance | Effectiveness | Efficiency | Cost |
| Method | SR$\uparrow$ | Imp(%) | AS$\downarrow$ | Imp(%) | Prompt$\downarrow$ | Completion$\downarrow$ | Expense($)$\downarrow$ | Imp(%) |
| CoT | 0.61 | N/A | 1.00 | N/A | 100,387 | 11,942 | 2.25 | N/A |
| Act | 0.56 | 0.00 | 2.16 | 0.00 | 723,646 | 6,980 | 14.61 | 0.00 |
| Act-WESE | 0.62 | +10.71 | 1.58 | +26.66 | 723,867 | 5,937 | 14.60 | +0.11 |
| Act-SESE | 0.64 | +14.29 | 1.57 | +27.34 | 2,822,189 | 122,543 | 60.89 | -316.73 |
| ReAct | 0.63 | 0.00 | 2.18 | 0.00 | 1,074,080 | 36,040 | 22.20 | 0.00 |
| ReAct-WESE | 0.68 | +7.26 | 1.62 | +25.96 | 918,905 | 29,895 | 18.98 | +14.53 |
| ReAct-SESE | 0.70 | +10.09 | 1.59 | +27.18 | 3,104,924 | 162,363 | 65.35 | -194.32 |

#### 4.2.1 HotPotQA

HotPotQAYang et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib25)) is a question-answering dataset where each question is paired with supporting sentences from Wikipedia articles. In traditional QA tasks, the supporting sentences are given and the remained task is to reason. Referred in ReAct, we use the Wikipedia API with three types of actions to support interactive information retrieval: (1) search[entity], which searches the Wikipedia with the entity and returns the corresponding page if it exists, or suggests top-5 similar entities; (2) lookup[keyword], which looks up keyword in the page and returns the next sentence containing the keyword, simulating the Ctrl+F function in a web browser; (3) finish[answer], which answers the question with answer. Once the answer matches the ground truth, the environment would return reward 1\. We sample 500 tasks from the development set.

We employ the CoT Wei et al. ([2022](https://arxiv.org/html/2404.07456v1#bib.bib22)), Act and ReAct as baselines and empower Act and ReAct with WESE and SESE. Note that CoT is a one-step method that does not support interactive tasks, we inject the supporting sentences into the prompts and instruct the LLM to reason for the final answer without searching on the web. Also, WESE is not designed for such a purely reasoning method but for methods involving interactions with the environment. For Act and ReAct, we keep the settings consistent with the original paper. As there are probably lots of related triplets to the task-involved entities, we set the limit of retrieved triplets as 10 and the limits of steps $N_{e},N_{t}$ as 8\. The evaluation for effectiveness, efficiency and cost is consistent with the ALFWorld.

#### 4.2.2 FEVER

FEVER Thorne et al. ([2018](https://arxiv.org/html/2404.07456v1#bib.bib12)) is a fact verification dataset, consisting of instances where each instance comprises a claim and a justification(True or False or Not Clear). We employ the Wikipedia API to construct an interactive environment consistent with that in HotPotQA. Other settings are kept consistent with HotPotQA, such as the number of retrieved triplets and the maximum steps.

#### 4.2.3 Results

The results on HotPotQA and FEVER are shown in Table [3](https://arxiv.org/html/2404.07456v1#S4.T3 "Table 3 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents") and Table [4](https://arxiv.org/html/2404.07456v1#S4.T4 "Table 4 ‣ 4.2 Question Answering Tasks ‣ 4 Experiments ‣ WESE: Weak Exploration to Strong Exploitation for LLM Agents"), respectively. We can conclude several findings based on the results. Similar to decision-making tasks, ReAct outperforms Act significantly due to the additional “thought” step. Also, methods equipped with WESE or SESE outperform baselines in both success rate and the number of taken actions, resulting in average relative improvements of 19.5% and 28.0%, respectively. Especially, SESE methods surpass WESE slightly with average relative 3.5% and 3.6% improvements in terms of success rate and average steps, while increasing more than twice the expenses. This further demonstrates that the weak agent powered by Llama-2-7B is almost sufficient for the exploration task.

Different from decision-making tasks, question-answering tasks require fewer steps due to more information being returned with one search action. However, our WESE and SESE are still capable of reducing the number of steps, further showing the advantage of the explored knowledge. As for the cost, the tokens increased in SESE are far more than those in decision-making tasks, which can be attributed to the long-textual feedback from Wikipedia.

## 5 Conclusion

In this paper, we introduce WESE, a cost-effective method that enhances LLM agents in open-world interactive tasks. We decouple the exploration and exploitation, employing two agents for the distinct processes. To empower the communication between the two processes, we introduce a knowledge graph-based memory to compress and structure the information obtained in exploration, where task-relevant information is extracted from the graph by a one-hop retrieval method. We then propose to leverage a weaker agent for the exploration process, forming a cost-effective manner with negligible performance degradation. Experimental results demonstrate the superiority of WESE in effectiveness, efficiency, and cost.

## References

*   Besta et al. [2023] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.
*   Côté et al. [2019] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. In Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pages 41–75\. Springer, 2019.
*   Huang and Chang [2022] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.
*   Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.
*   Pan et al. [2024] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering, 2024.
*   Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1–22, 2023.
*   Qin et al. [2023a] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2023.
*   Qin et al. [2023b] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023.
*   Shinn et al. [2023] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.
*   Shridhar et al. [2020] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.
*   Sun et al. [2023] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. A survey of reasoning with foundation models. arXiv preprint arXiv:2312.11562, 2023.
*   Thorne et al. [2018] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355, 2018.
*   Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
*   Wadhwa et al. [2023] Somin Wadhwa, Silvio Amir, and Byron C Wallace. Revisiting relation extraction in the era of large language models. arXiv preprint arXiv:2305.05003, 2023.
*   Wang et al. [2022a] Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? arXiv preprint arXiv:2203.07540, 2022.
*   Wang et al. [2022b] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
*   Wang et al. [2023a] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.
*   Wang et al. [2023b] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023.
*   Wang et al. [2023c] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.
*   Wang et al. [2023d] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory. arXiv preprint arXiv:2306.07174, 2023.
*   Wang et al. [2023e] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.
*   Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.
*   Wu et al. [2023] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.
*   Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.
*   Yang et al. [2018] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.
*   Yang et al. [2023] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023.
*   Yao et al. [2022] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
*   Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.
*   Zhang et al. [2023] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language model is semi-parametric reinforcement learning agent. arXiv preprint arXiv:2306.07929, 2023.
*   Zhao et al. [2023] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.