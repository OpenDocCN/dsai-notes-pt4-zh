- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:47:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:47:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OmniDrive：用于3D感知、推理和规划的整体LLM代理框架
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.01533](https://ar5iv.labs.arxiv.org/html/2405.01533)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.01533](https://ar5iv.labs.arxiv.org/html/2405.01533)
- en: \floatsetup
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \floatsetup
- en: '[table]capposition=top \newfloatcommandcapbtabboxtable[][\FBwidth] (eccv) Package
    eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’, which is
    *not* recommended for camera-ready version'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格]capposition=top \newfloatcommandcapbtabboxtable[][\FBwidth] (eccv) 包含eccv
    包含警告：包‘hyperref’已使用‘pagebackref’选项加载，这对于最终稿*不*推荐'
- en: '¹¹institutetext: ¹ Beijing Inst of Tech, ² NVIDIA, ³ Huazhong Univ of Sci and
    Tech'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: ¹ 北京理工大学，² NVIDIA，³ 华中科技大学'
- en: '[https://github.com/NVlabs/OmniDrive](https://github.com/NVlabs/OmniDrive)Shihao
    Wang Work done during an internship at NVIDIA.11    Zhiding Yu Corresponding author:
    [zhidingy@nvidia.com](mailto:zhidingy@nvidia.com).22    Xiaohui Jiang 11    Shiyi
    Lan 22    Min Shi^∗ 33    Nadine Chang 22    Jan Kautz 22    Ying Li 11    Jose
    M. Alvarez 22'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/NVlabs/OmniDrive](https://github.com/NVlabs/OmniDrive)Shihao
    Wang 在NVIDIA实习期间完成的工作。11    Zhiding Yu 通讯作者：[zhidingy@nvidia.com](mailto:zhidingy@nvidia.com)。22
       Xiaohui Jiang 11    Shiyi Lan 22    Min Shi^∗ 33    Nadine Chang 22    Jan
    Kautz 22    Ying Li 11    Jose M. Alvarez 22'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The advances in multimodal large language models (MLLMs) have led to growing
    interests in LLM-based autonomous driving to leverage their strong reasoning capabilities.
    However, capitalizing on MLLMs’ strong reasoning capabilities for improved planning
    behavior is challenging since it requires full 3D situational awareness beyond
    2D reasoning. To address this challenge, our work proposes OmniDrive, a holistic
    framework for strong alignment between agent models and 3D driving tasks. Our
    framework starts with a novel 3D MLLM architecture that uses sparse queries to
    lift and compress visual representations into 3D before feeding them into an LLM.
    This query-based representation allows us to jointly encode dynamic objects and
    static map elements (e.g., traffic lanes), providing a condensed world model for
    perception-action alignment in 3D. We further propose a new benchmark with comprehensive
    visual question-answering (VQA) tasks, including scene description, traffic regulation,
    3D grounding, counterfactual reasoning, decision making and planning. Extensive
    studies show the excellent reasoning and planning capabilities of OmniDrive in
    complex 3D scenes.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大型语言模型（MLLMs）的进展引起了对基于LLM的自动驾驶的日益关注，以利用其强大的推理能力。然而，利用MLLMs的强大推理能力来改善规划行为具有挑战性，因为这需要超越2D推理的完整3D情境感知。为了解决这一挑战，我们的工作提出了OmniDrive，这是一个在代理模型和3D驾驶任务之间实现强对齐的整体框架。我们的框架从一个创新的3D
    MLLM架构开始，该架构使用稀疏查询将视觉表示提升和压缩为3D，然后输入到LLM中。这种基于查询的表示使我们能够联合编码动态对象和静态地图元素（例如交通车道），为3D中的感知-行动对齐提供一个精简的世界模型。我们进一步提出了一个新的基准，包含全面的视觉问答（VQA）任务，包括场景描述、交通规则、3D定位、反事实推理、决策制定和规划。广泛的研究表明，OmniDrive在复杂3D场景中具有出色的推理和规划能力。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Autonomous driving Large language model Planning![Refer to caption](img/166abc8bffeb1544812aa1a180270d6a.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶 大型语言模型 规划![参考说明](img/166abc8bffeb1544812aa1a180270d6a.png)
- en: 'Figure 1: We present OmniDrive, a novel framework for end-to-end autonomous
    driving with large language model (LLM) agents. Our main contributions involve
    novel solutions in both model (OmniDrive-Agent) and benchmark (OmniDrive-nuScenes).
    The former features a novel 3D vision-language model design, whereas the latter
    is constituted of comprehensive VQA tasks for reasoning and planning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们介绍了OmniDrive，这是一个用于端到端自动驾驶的创新框架，利用大型语言模型（LLM）代理。我们的主要贡献包括在模型（OmniDrive-Agent）和基准（OmniDrive-nuScenes）方面的创新解决方案。前者具有创新的3D视觉语言模型设计，而后者则包含全面的视觉问答（VQA）任务，用于推理和规划。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The recent rapid development of multimodal LLMs (MLLMs) [[1](#bib.bib1), [24](#bib.bib24),
    [31](#bib.bib31)] and their excellent reasoning capabilities have led to a stream
    of applications in end-to-end autonomous driving [[46](#bib.bib46), [58](#bib.bib58),
    [52](#bib.bib52), [39](#bib.bib39), [6](#bib.bib6)]. However, the challenge to
    extend the capabilities from 2D understanding to the intricacies of 3D space is
    a crucial hurdle to overcome to fully unlock its potential in real-world applications.
    Understanding and navigating through 3D space is indispensable for autonomous
    vehicles (AVs) because they directly impact an AV’s ability to make informed decisions,
    anticipate future states, and interact safely with their environment. Although
    previous works [[38](#bib.bib38), [47](#bib.bib47)] have demonstrated successful
    applications of LLM-agents in autonomous driving, a holistic and principled approach
    is still needed to fully extend the 2D understanding and reasoning capabilities
    of MLLMs into complex 3D scenes for understanding the 3D geometry and spatial
    relations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近多模态 LLMs（MLLMs）[[1](#bib.bib1), [24](#bib.bib24), [31](#bib.bib31)] 的快速发展及其卓越的推理能力，已经促使了一系列端到端自动驾驶的应用
    [[46](#bib.bib46), [58](#bib.bib58), [52](#bib.bib52), [39](#bib.bib39), [6](#bib.bib6)]。然而，从二维理解扩展到三维空间的复杂性，是完全释放其在现实世界应用中潜力的关键障碍。理解和导航三维空间对自动驾驶汽车（AVs）至关重要，因为这直接影响到
    AV 进行明智决策、预测未来状态和安全地与环境互动的能力。虽然之前的研究 [[38](#bib.bib38), [47](#bib.bib47)] 已经展示了
    LLM 代理在自动驾驶中的成功应用，但仍需要一种全面且原则性的 approach 来将 MLLMs 的二维理解和推理能力扩展到复杂的三维场景中，以理解三维几何和空间关系。
- en: Another open issue is the need to address multi-view high resolution video input.
    On one hand, many current popular 2D MLLM architectures, such as LLaVA-1.5 [[31](#bib.bib31),
    [30](#bib.bib30)], can only take $336\times 336$ image input due to the limited
    vision encoder resolution and LLM token sequence length. Increasing the limitations
    is not trivial, as it requires significantly more compute and memories. On the
    other hand, dealing with high resolution video input, oftentimes even multi-view,
    is a fundamental requirement for long-range AV perception and safe decision making.
    However, unlike many cloud-based services, real-world industrial autonomous driving
    applications are mostly on-device and compute-bound. As such, there is a need
    to design an efficient MLLM architectures with compressed 3D visual representations
    before feeding to the LLM.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个待解决的问题是如何处理多视角高分辨率视频输入。一方面，许多当前流行的二维 MLLM 架构，如 LLaVA-1.5 [[31](#bib.bib31),
    [30](#bib.bib30)]，由于视觉编码器分辨率和 LLM 令牌序列长度的限制，只能处理 $336\times 336$ 的图像输入。增加这些限制并非易事，因为这需要显著更多的计算和存储资源。另一方面，处理高分辨率的视频输入，甚至是多视角的，是长距离音视频感知和安全决策的基本要求。然而，与许多基于云的服务不同，现实世界中的工业自动驾驶应用大多是在设备上进行并受计算能力限制。因此，需要设计一种高效的
    MLLM 架构，以压缩的 3D 视觉表示形式输入到 LLM 中。
- en: 'Our answer to the above challenges is a novel Q-Former-styled [[24](#bib.bib24)]
    3D MLLM architecture as shown in  [Fig. 1](#S0.F1 "In OmniDrive: A Holistic LLM-Agent
    Framework for Autonomous Driving with 3D Perception, Reasoning and Planning").
    Unlike LLaVA which adopts a self-attention design, the cross-attention decoder
    in Q-Former makes it more scalable to higher resolution input by compressing the
    visual information into sparse queries. Interestingly, we notice that the Q-Former
    architecture shares considerable similarity with the family of perspective-view
    models, such as DETR3D [[53](#bib.bib53)], PETR(v2) [[32](#bib.bib32), [33](#bib.bib33)],
    StreamPETR [[50](#bib.bib50)] and Far3D [[21](#bib.bib21)]. Using sparse 3D queries,
    these models have demonstrated considerable advantages over dense bird’s-eye view
    (BEV) representation with leading performance [[50](#bib.bib50), [21](#bib.bib21)],
    long-range perception [[21](#bib.bib21)] and capability to jointly model map elements [[55](#bib.bib55)].
    The similarity in query-based decoder architecture enables us to align both worlds
    by appending 3D position encoding to the queries, lifting them to 3D, and attending
    the multi-view input, as shown in the left portion of Fig. [1](#S0.F1 "Figure
    1 ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning"). This process allows the MLLM to gain 3D spatial understanding
    with minimal efforts and changes, while leveraging the pre-trained knowledge from
    the abundant images in 2D.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对上述挑战的回答是一个新颖的Q-Former风格的[[24](#bib.bib24)] 3D MLLM架构，如[图1](#S0.F1 "在OmniDrive中：一个全面的LLM-Agent框架用于具有3D感知、推理和规划的自动驾驶")所示。与采用自注意力设计的LLaVA不同，Q-Former中的交叉注意力解码器通过将视觉信息压缩成稀疏查询，使其在更高分辨率输入下更具可扩展性。有趣的是，我们注意到Q-Former架构与透视视图模型家族，如DETR3D [[53](#bib.bib53)],
    PETR(v2) [[32](#bib.bib32), [33](#bib.bib33)], StreamPETR [[50](#bib.bib50)]和Far3D [[21](#bib.bib21)]有相当大的相似性。利用稀疏3D查询，这些模型在密集鸟瞰视图（BEV）表示方面展示了显著的优势，包括领先的性能 [[50](#bib.bib50),
    [21](#bib.bib21)], 长距离感知 [[21](#bib.bib21)]以及联合建模地图元素的能力 [[55](#bib.bib55)]。基于查询的解码器架构的相似性使我们能够通过将3D位置编码附加到查询中，从而对齐两个世界，将它们提升到3D，并关注多视图输入，如图[1](#S0.F1
    "图1 ‣ OmniDrive：一个全面的LLM-Agent框架用于具有3D感知、推理和规划的自动驾驶")的左侧部分所示。这个过程使得MLLM能够以最小的努力和变化获得3D空间理解，同时利用来自2D的丰富图像的预训练知识。
- en: 'Besides model architectures, recent drive LLM-agent works also feature the
    importance of benchmarks [[43](#bib.bib43), [38](#bib.bib38), [46](#bib.bib46),
    [39](#bib.bib39), [10](#bib.bib10), [47](#bib.bib47)]. Many of them are presented
    as question-answering (QA) datasets to train and benchmark the LLM-agent for either
    reasoning or planning. Despite the various QA setups, benchmarks that involve
    planning [[46](#bib.bib46), [10](#bib.bib10), [47](#bib.bib47)] still resort to
    an open-loop setting on real-world sessions (e.g., nuScenes) where expert trajectories
    are used. Recent studies [[60](#bib.bib60), [26](#bib.bib26)], however, reveal
    limitations of open-loop evaluation with implicit biases towards ego status, overly
    simple planning scenarios, and easy overfit to the expert trajectory. In light
    of the issue, the proposed benchmark OmniDrive-nuScenes features a counterfactual
    reasoning benchmark setting where simulated decision and trajectories are leveraged
    to reason potential consequences. Our benchmark also include other challenging
    tasks that require full 3D spatial understanding and long-horizon reasoning, as
    shown in Fig. [1](#S0.F1 "Figure 1 ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning") (right).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型架构，最近的驾驶LLM-agent工作还突出了基准的重要性[[43](#bib.bib43), [38](#bib.bib38), [46](#bib.bib46),
    [39](#bib.bib39), [10](#bib.bib10), [47](#bib.bib47)]。其中许多被呈现为问答（QA）数据集，以训练和基准测试LLM-agent的推理或规划能力。尽管有各种QA设置，但涉及规划的基准[[46](#bib.bib46),
    [10](#bib.bib10), [47](#bib.bib47)]仍然依赖于真实世界会话中的开环设置（例如，nuScenes），其中使用了专家轨迹。然而，最近的研究[[60](#bib.bib60),
    [26](#bib.bib26)]揭示了开环评估的局限性，存在对自我状态的隐性偏见、过于简单的规划场景，以及容易过拟合于专家轨迹的问题。鉴于此问题，提出的基准OmniDrive-nuScenes具有一个反事实推理基准设置，其中利用模拟决策和轨迹来推理潜在后果。我们的基准还包括其他需要全面3D空间理解和长期推理的挑战性任务，如图[1](#S0.F1
    "图1 ‣ OmniDrive：一个全面的LLM-Agent框架用于具有3D感知、推理和规划的自动驾驶")（右侧）所示。
- en: In summary, OmniDrive aims to provide a holistic framework for end-to-end autonomous
    driving with LLM-agents. We propose a principled model design excellent in 3D
    reasoning and planning, as well as a more challenging benchmark going beyond single
    expert trajectories.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，OmniDrive旨在提供一个全面的端到端自动驾驶框架与LLM代理。我们提出了一个在3D推理和规划方面出色的原则模型设计，以及一个超越单一专家轨迹的更具挑战性的基准。
- en: 2 OmniDrive-Agent
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 OmniDrive-Agent
- en: 'As a recap, we aim for a unified 3D MLLM design to: 1) leverage the 2D MLLM
    pre-training knowledge, and 2) addressing the high-resolution multi-view input
    in autonomous driving. We propose a Q-Former-styled architecture by compressing
    the visual features into a fixed number of queries before feeding to an LLM [[24](#bib.bib24)].
    Noticing the similarity between Q-Former and query-based 3D perception frameworks,
    we align our MLLM architecture with StreamPETR [[50](#bib.bib50)], where use queries
    to encode both dynamic objects and static map elements. These queries, together
    with additional carrier tokens, serve as a condensed world model to align perception
    with reasoning and planning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的目标是设计一个统一的3D MLLM：1）利用2D MLLM预训练知识，2）解决自动驾驶中的高分辨率多视角输入问题。我们提出了一种Q-Former风格的架构，通过将视觉特征压缩成固定数量的查询，然后输入到LLM中[[24](#bib.bib24)]。注意到Q-Former和基于查询的3D感知框架之间的相似性，我们将我们的MLLM架构与StreamPETR[[50](#bib.bib50)]对齐，在其中使用查询来编码动态对象和静态地图元素。这些查询与额外的载体标记一起，作为压缩的世界模型，将感知与推理和规划对齐。
- en: 2.1 Preliminaries
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基础知识
- en: 'The Q-Former based MLLMs are composed of a general visual encoder to extract
    single-view image features $F_{s}\in\mathbb{R}^{C\times H\times W}$, a projector
    (Q-Former) that serves as visual-language alignment module, and a large language
    model for text generation. The architecture of the projector is stacked transformer
    decoder layers. The projection process from image features to the textual embedding
    can be represented as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Q-Former的MLLMs由一个通用视觉编码器组成，用于提取单视角图像特征$F_{s}\in\mathbb{R}^{C\times H\times
    W}$，一个作为视觉-语言对齐模块的投影器（Q-Former），以及用于文本生成的大型语言模型。投影器的架构是堆叠的变换器解码器层。从图像特征到文本嵌入的投影过程可以表示为：
- en: '|  | $\displaystyle\tilde{Q}_{t}=f_{q}(Q_{t},{F}_{s})$ |  | (1) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}_{t}=f_{q}(Q_{t},{F}_{s})$ |  | (1) |'
- en: where $Q_{t}$ is the refined text embedding, which will be sent to the language
    model to generate the final text output.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Q_{t}$是优化后的文本嵌入，将被发送到语言模型以生成最终的文本输出。
- en: 'The query-based 3D perception models [[55](#bib.bib55), [21](#bib.bib21), [28](#bib.bib28),
    [29](#bib.bib29)] consist of a shared visual encoder to extract multi-view image
    features, and a detection head $f_{d}$, which can be formulated as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 基于查询的3D感知模型[[55](#bib.bib55), [21](#bib.bib21), [28](#bib.bib28), [29](#bib.bib29)]包括一个共享的视觉编码器，用于提取多视角图像特征，以及一个检测头$f_{d}$，可以表示为：
- en: '|  | $\displaystyle\tilde{Q}_{d}=f_{d}(Q_{d},F_{m}+P_{m})$ |  | (2) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}_{d}=f_{d}(Q_{d},F_{m}+P_{m})$ |  | (2) |'
- en: where $P_{m}$ is the initialized detection queries to gather the multi-view
    image features.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P_{m}$是初始化的检测查询，用于收集多视角图像特征。
- en: '![Refer to caption](img/ce963295473e165691a58737bd0ff551.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce963295473e165691a58737bd0ff551.png)'
- en: 'Figure 2: Overall pipeline of OmniDrive-Agent. The left diagram illustrates
    the overall framework of the model. We employ a 3D perception task to guide Q-Former’s
    learning. The right diagram depicts the specific structure of Q-Former3D, which
    is consist of six transformer decoder layers. The attention weights are initialized
    from 2D pre-pretrain. The input are multi-view image features. Additionally, 3D
    position encoding is added in the attention operation. Furthermore, we introduce
    temporal modeling through a memory bank.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：OmniDrive-Agent的整体流程图。左图展示了模型的整体框架。我们采用3D感知任务来指导Q-Former的学习。右图描述了Q-Former3D的具体结构，它由六个变换器解码器层组成。注意力权重初始化来自2D预训练。输入是多视角图像特征。此外，在注意力操作中添加了3D位置编码。我们还通过记忆库引入了时间建模。
- en: It can be observed that the Transformer decoder in Q-Former and the sparse query-based
    3D perception models, represented by StreamPETR [[50](#bib.bib50)], share highly
    similar architecture designs. To enhance the localization abilities of the MLLMs,
    we consider introducing the design of 3D position encoding and the supervision
    of the query-based perception models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以观察到，Q-Former中的变换器解码器与由StreamPETR[[50](#bib.bib50)]表示的稀疏查询基础的3D感知模型在架构设计上高度相似。为了增强MLLM的定位能力，我们考虑引入3D位置编码的设计和基于查询的感知模型的监督。
- en: 2.2 Overall Architecture
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 整体架构
- en: 'As show in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Preliminaries ‣ 2 OmniDrive-Agent
    ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning"), Omnidrive first uses a shared visual encoder to extract
    multi-view image features $F_{m}\in\mathbb{R}^{N\times C\times H\times W}$ are
    fed into the Q-Former3D. In Q-Former3D, we initialize the detection queries and
    carrier queries and perform self-attention to exchange their information, which
    can be summarized by the following formula:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S2.F2 "图 2 ‣ 2.1 初步 ‣ 2 OmniDrive-Agent ‣ OmniDrive: 一种用于具有 3D 感知、推理和规划的自主驾驶的整体
    LLM-Agent 框架") 所示，Omnidrive 首先使用共享视觉编码器来提取多视图图像特征 $F_{m}\in\mathbb{R}^{N\times
    C\times H\times W}$，然后将这些特征输入到 Q-Former3D 中。在 Q-Former3D 中，我们初始化检测查询和承载查询，并执行自注意力机制来交换它们的信息，可以用以下公式来总结：'
- en: '|  | $1$2 |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: '|  | $\displaystyle\tilde{Q}=\text{Multi-head Attention}(Q,K,V)$ |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}=\text{多头注意力}(Q,K,V)$ |  |'
- en: '$\textbf{[}\cdot\textbf{]}$ is the concatenation operation. For simplicity,
    we omit the position encoding. Then these queries collect information from multi-view
    images:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: $\textbf{[}\cdot\textbf{]}$ 是连接操作。为了简化起见，我们省略了位置编码。然后这些查询从多视图图像中收集信息：
- en: '|  | $\displaystyle(Q,K,V)=(\textbf{[}Q_{c},{Q}_{d}\textbf{]},P_{m}+F_{m},F_{m}),$
    |  | (4) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(Q,K,V)=(\textbf{[}Q_{c},{Q}_{d}\textbf{]},P_{m}+F_{m},F_{m}),$
    |  | (4) |'
- en: '|  | $\displaystyle\tilde{Q}=\text{Multi-head Attention}(Q,K,V)$ |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{Q}=\text{多头注意力}(Q,K,V)$ |  |'
- en: After that, the perception queries are used to predict the categories and coordinates
    of the foreground elements. The carrier queries are sent to a single layer MLP
    to align with the dimension of LLM tokens (e.g., 4096 dimensions in LLaMA [[48](#bib.bib48)])
    and further used for text generation following LLaVA [[31](#bib.bib31)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，感知查询用于预测前景元素的类别和坐标。承载查询被送入单层 MLP，以对齐 LLM 令牌的维度（例如，LLaMA 中的 4096 维 [[48](#bib.bib48)]），并进一步用于基于
    LLaVA [[31](#bib.bib31)] 的文本生成。
- en: In our model, the carrier queries play the role of the visual-language alignment.
    Additionally, this design enables carrier queries to leverage the geometric priors
    provided by the 3D position encoding, while also allowing them to leverage query-based
    representations acquired through the 3D perception tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，承载查询起到了视觉-语言对齐的作用。此外，这一设计使得承载查询能够利用由 3D 位置编码提供的几何先验，同时也允许它们利用通过 3D 感知任务获得的基于查询的表示。
- en: 2.3 Multi-task and Temporal Modeling
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 多任务和时间建模
- en: 'Our approach benefits from multi-task learning and temporal modeling [[33](#bib.bib33),
    [25](#bib.bib25)]. In multi-task learning, we can integrate task-specific Q-Former3D
    modules for each perception task, employing a uniform initialization strategy
    (please refer to [Sec. 2.4](#S2.SS4 "2.4 Training Strategy ‣ 2 OmniDrive-Agent
    ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning")). In different tasks, carrier queries can gather information
    of different traffic elements. In our implementation, we cover tasks such as center-line
    construction and 3D object detection. During the training and inference phases,
    both heads share the same 3D position encoding. Regarding temporal modeling, we
    store the perception queries with top-k classification scores into a memory bank
    and propagate them frame by frame [[28](#bib.bib28), [59](#bib.bib59)]. The propagated
    queries interact with the perception and carrier queries from the current frame
    through cross-attention, expanding the capabilities of our model to effectively
    process video input.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法得益于多任务学习和时间建模 [[33](#bib.bib33), [25](#bib.bib25)]。在多任务学习中，我们可以为每个感知任务集成特定任务的
    Q-Former3D 模块，采用统一的初始化策略（请参见 [第 2.4 节](#S2.SS4 "2.4 训练策略 ‣ 2 OmniDrive-Agent ‣
    OmniDrive: 一种用于具有 3D 感知、推理和规划的自主驾驶的整体 LLM-Agent 框架")）。在不同的任务中，承载查询可以收集不同交通元素的信息。在我们的实现中，我们涵盖了如中心线构建和
    3D 物体检测等任务。在训练和推理阶段，两者共享相同的 3D 位置编码。关于时间建模，我们将具有 top-k 分类得分的感知查询存储到一个记忆库中，并逐帧传播
    [[28](#bib.bib28), [59](#bib.bib59)]。传播的查询通过交叉注意力与当前帧的感知查询和承载查询进行交互，从而扩展了我们模型处理视频输入的能力。'
- en: 2.4 Training Strategy
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 训练策略
- en: 'The training of OmniDrive-agent comprises two stages: 2D-Pretraining and 3D-Finetuning.
    In the initial stage, we pretrain the MLLMs on 2D image tasks to initialize the
    Q-Former and carrier queries. Following this, the model is fine-tuned on 3D-related
    driving tasks (e.g., motion planning, 3D grounding, etc.). In both stages, we
    calculate the text generation loss without considering contrasting learning and
    matching loss for in BLIP-2 [[24](#bib.bib24)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: OmniDrive-agent的训练包括两个阶段：2D-预训练和3D-微调。在初始阶段，我们在2D图像任务上对MLLMs进行预训练，以初始化Q-Former和载体查询。随后，模型在3D相关的驾驶任务（例如，运动规划、3D定位等）上进行微调。在这两个阶段中，我们计算文本生成损失，但不考虑BLIP-2 [[24](#bib.bib24)]中的对比学习和匹配损失。
- en: 2D-Pretraining. The 2D-Pretraining stage aims to pretrain the carrier queries
    and the Q-Former, and achieve better alignment between image features and the
    large language model. When removing the detection queries, the OmniDrive model
    can be viewed as a standard vision language model capable of generating text conditioned
    on images. Therefore, we adopt the training recipe and data from LLaVA v1.5 [[30](#bib.bib30)]
    to pretrain OmniDrive on 2D images. The MLLMs is first trained on 558K image-text
    pairs, during which all parameters except the Q-Former are frozen. Subsequently,
    we fine-tune the MLLMs using the instruction tuning dataset from LLaVA v1.5\.
    During this fine-tuning step, only the image encoder is frozen, while all other
    parameters are trainable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2D-预训练。2D-预训练阶段旨在预训练载体查询和Q-Former，并实现图像特征与大型语言模型之间的更好对齐。当移除检测查询时，OmniDrive模型可以视为一种标准的视觉语言模型，能够生成以图像为条件的文本。因此，我们采用LLaVA
    v1.5 [[30](#bib.bib30)]的训练方案和数据来对OmniDrive进行2D图像预训练。MLLMs首先在558K图像-文本对上进行训练，在此过程中，除了Q-Former外的所有参数均被冻结。随后，我们使用LLaVA
    v1.5的指令调优数据集对MLLMs进行微调。在此微调步骤中，只有图像编码器被冻结，而所有其他参数均可训练。
- en: 3D-Finetuning. During the 3D finetuning phase, we aim to enhance the model’s
    3D localization capability while retaining its 2D semantic understanding as much
    as possible. We have augmented the original Q-Former with 3D position encoding
    and temporal modules. In this phase, we fine-tune the visual encoder and the large
    language model with Lora [[16](#bib.bib16)] using a small learning rate, and train
    Q-Former3D with a relatively larger learning rate.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3D-微调。在3D微调阶段，我们的目标是提高模型的3D定位能力，同时尽可能保留其2D语义理解。我们对原始Q-Former进行了3D位置编码和时间模块的增强。在这个阶段，我们使用较小的学习率通过Lora [[16](#bib.bib16)]对视觉编码器和大型语言模型进行微调，并使用相对较大的学习率训练Q-Former3D。
- en: 3 OmniDrive-nuScenes
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 OmniDrive-nuScenes
- en: To benchmark drive LLM-agents, we propose OmniDrive-nuScenes, a novel benchmark
    built on nuScenes [[4](#bib.bib4)] with high quality visual question-answering
    (QA) pairs covering perception, reasoning and planning in 3D domain.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估驱动LLM-agents，我们提出了OmniDrive-nuScenes，这是一个基于nuScenes [[4](#bib.bib4)]构建的新的基准，包含高质量的视觉问答（QA）对，涵盖3D领域中的感知、推理和规划。
- en: OmniDrive-nuScenes features a fully-automated procedural QA generation pipeline
    using GPT4\. Similar to LLaVA [[31](#bib.bib31)], the proposed pipeline takes
    the 3D perception ground truths as context information via prompt input. Traffic
    rules and planning simulations are further leveraged as additional inputs, thereby
    easing the challenges GPT-4V faces in comprehending 3D environments. Our benchmark
    asks long-horizon questions in the forms of attention, counterfactual reasoning,
    and open-loop planning. These questions challenge the true spatial understanding
    and planning capabilities in 3D space as they require planning simulations in
    the next few seconds to obtain the correct answers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: OmniDrive-nuScenes具有一个完全自动化的程序化QA生成流程，使用GPT-4\. 类似于LLaVA [[31](#bib.bib31)]，该流程通过提示输入将3D感知真实情况作为上下文信息。交通规则和规划模拟被进一步利用作为附加输入，从而减轻GPT-4V在理解3D环境时面临的挑战。我们的基准提出了以注意力、反事实推理和开放式规划形式的长期问题。这些问题挑战了在3D空间中的真实空间理解和规划能力，因为它们需要在接下来的几秒钟内进行规划模拟以获得正确答案。
- en: Besides using the above pipeline to curate the offline question-answering sessions
    for OmniDrive-nuScenes, we further propose a pipeline to online generate various
    types of grounding questions. This part can also be viewed as certain form of
    implicit data augmentation to enhance the 3D spatial understanding and reasoning
    capabilities of the models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用上述流程为OmniDrive-nuScenes策划离线问答会话外，我们还提出了一种在线生成各种类型定位问题的流程。这部分也可以视为一种隐性数据增强，以提升模型的3D空间理解和推理能力。
- en: 3.1 Offline Question-Answering.
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 离线问答。
- en: | ![[Uncaptioned image]](img/266c640a41841ea9cec09b2a8fdce411.png)  |  |
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: | ![[未标题的图片]](img/266c640a41841ea9cec09b2a8fdce411.png)  |  |
- en: '| Prompt type 1: Caption |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 1：标题 |  |'
- en: '| The images depict a daytime setting in a controlled-access area, likely a
    parking lot or a service entrance of a commercial or industrial facility. On the
    left, there’s a grassy area with trees and a building with blue accents. Moving
    towards the center, we see a security checkpoint with a raised barrier arm, indicating
    an entrance or exit point, and a security guard booth… |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 图像展示了一个白天的设置，可能是一个受控通行区域，如停车场或商业或工业设施的服务入口。左侧有一个草地区域，树木和一栋带蓝色装饰的建筑。向中心移动，我们看到一个设有升起的障碍杆的安全检查点，表示一个入口或出口点，以及一个安保亭……
    |  |'
- en: '| Prompt type 2: Lane-object association |  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 2：车道-物体关联 |  |'
- en: '| &#124;— your current straight lane [(-2.6, +0.5), (+1.2, +0.7), (+5.0, +0.9),
    (+8.8, +1.0)] |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| &#124;— 你当前的直行车道 [(-2.6, +0.5), (+1.2, +0.7), (+5.0, +0.9), (+8.8, +1.0)]
    |  |'
- en: '| &#124; &#124;— your own car |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| &#124; &#124;— 你自己的车 |  |'
- en: '| &#124; &#124;— movable_object.trafficcone in the front location: (+8.2, +2.4)…
    |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| &#124; &#124;— 前方的可移动交通锥：(+8.2, +2.4)… |  |'
- en: '| Prompt type 3: Simulated decision and trajectory |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 3：模拟决定和轨迹 |  |'
- en: '| Simulated decision: Moderate Speed, Left Turn |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 模拟决定：适中速度，左转 |  |'
- en: '| Simulated trajectory: [PT, (+4.85, -0.08), (+9.71, -0.22), …, (+27.42, -0.93)].
    Out of the drivable area |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 模拟轨迹：[PT, (+4.85, -0.08), (+9.71, -0.22), …, (+27.42, -0.93)]。超出可驾驶区域 |  |'
- en: '| Prompt type 4: Expert decision and trajectory |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 提示类型 4：专家决定和轨迹 |  |'
- en: '| Expert decision: Moving Slowly, Lane Keeping, Go Straight |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 专家决定：慢行，保持车道，直行 |  |'
- en: '| Expert trajectory: [PT, (+0.76, +0.02), (+1.45, +0.03), …, (+3.44, +0.12)].
    |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 专家轨迹：[PT, (+0.76, +0.02), (+1.45, +0.03), …, (+3.44, +0.12)]。 |  |'
- en: '| Objects near your path: vehicle.moving at (-6.0, +0.3), human.pedestrian.moving
    at (+4.4, -7.2). |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 路径附近的物体：车辆在 (-6.0, +0.3) 处移动，行人在人行道上移动，位置为 (+4.4, -7.2)。 |  |'
- en: '| Conversation type 1: Attention |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 对话类型 1：注意 |  |'
- en: '| Question: What traffic elements should I be aware of while driving in this
    area? |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 问题：在这个区域驾驶时，我需要注意哪些交通要素？ |  |'
- en: '| Answer: There are movable traffic cones at various points along your path:
    one directly in front at (+8.2, +2.4), and others in the surrounding lanes, such
    as those at (+4.7, -6.3)…. |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 答案：在你的路径上有可移动的交通锥：一个直接在前方位置 (+8.2, +2.4)，其他在周围车道，如 (+4.7, -6.3) 处…… |  |'
- en: '| Conversation type 2: Counterfactual reasoning |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 对话类型 2：反事实推理 |  |'
- en: '| Question: If I decide to accelerate and make a left turn, what could be the
    consequences? |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 问题：如果我决定加速并左转，可能会有什么后果？ |  |'
- en: '| Answer: It appears that you would be turning into the lane designated for
    allowing from right to left driving. This maneuver could potentially lead to a
    collision with oncoming traffic… |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 答案：看起来你将要转入右侧到左侧行驶的车道。这一操作可能会导致与迎面而来的交通发生碰撞…… |  |'
- en: '| Conversation type 3: Decision making and planning |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 对话类型 3：决策和规划 |  |'
- en: '| Question: What should be my next action given the current driving situation,
    and why? |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 问题：鉴于当前的驾驶情况，我应该采取什么行动？为什么？ |  |'
- en: '| Answer: The most suitable trajectory to follow would be similar to the one
    provided as safe, which is [PT, (+0.76, +0.02), (+1.45, +0.03),…,(+3.44, +0.12)].
    This trajectory ensures that you remain in your current straight lane [(-2.6,
    +0.5), (+1.2, +0.7), …], avoid any obstacles… |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 答案：最合适的轨迹类似于提供的安全轨迹，即 [PT, (+0.76, +0.02), (+1.45, +0.03), …, (+3.44, +0.12)]。这一轨迹确保你保持在当前的直行车道
    [(-2.6, +0.5), (+1.2, +0.7), …]，避免任何障碍物…… |  |'
- en: '| Other Conversations: |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 其他对话： |  |'
- en: '| … |  |'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '| … |  |'
- en: 'Table 1: An example to illustrate the instruction-following data generation
    for offline QAs. The top block shows contexts such as captions and boxes used
    to prompt GPT4, and the bottom block shows the four types of responses. Note that
    the visual image is not used to prompt GPT4 except for caption generation.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：一个示例说明离线 QA 的指令遵循数据生成。顶部块展示了用于提示 GPT4 的上下文信息，如描述和边框，而底部块展示了四种类型的响应。请注意，视觉图像不用于提示
    GPT4，除了生成描述。
- en: 'Tab. [1](#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes
    ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning") shows an example of the proposed offline data generation
    pipeline, where in-context information is used to generate the QA pairs on nuScenes.
    We begin with related details on how different types of the prompt information
    is obtained:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [1](#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes
    ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning") 显示了所提议的离线数据生成流程的示例，其中使用上下文信息生成 nuScenes 的 QA 对。我们首先介绍了如何获取不同类型提示信息的相关细节：'
- en: 'Caption. When both the image and lengthy scene information are fed into GPT-4V
    simultaneously, GPT-4V tends to overlook details in the image. So we first prompt
    GPT-4V to produce the scene description based on the multi-view input only. As
    shown in the top block of Tab. [1](#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering.
    ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous
    Driving with 3D Perception, Reasoning and Planning"), we stitch the three frontal
    views and three rear views into two separate images, and feed them into GPT-4V,
    as shown in the top of the Tab. [1](#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering.
    ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous
    Driving with 3D Perception, Reasoning and Planning"). We also prompt GPT-4V to
    include the following details: 1) mentions weather, time of day, scene type and
    other image contents; 2) understands the general direction of each view (e.g.,
    the first frontal view being front-left); 3) avoids mentioning the contents from
    each view independently and replaces with positions relative to the ego vehicle.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '描述。当图像和详细场景信息同时输入到 GPT-4V 时，GPT-4V 往往会忽视图像中的细节。因此，我们首先提示 GPT-4V 仅基于多视角输入生成场景描述。如图所示在表格
    [1](#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣
    OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning") 的顶部块中，我们将三张前视图和三张后视图拼接成两张单独的图像，并将它们输入到 GPT-4V 中，如表格 [1](#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning") 的顶部所示。我们还提示 GPT-4V 包含以下细节：1）提及天气、时间、场景类型和其他图像内容；2）理解每个视图的一般方向（例如，第一个前视图是前左）；3）避免独立提及每个视图的内容，并替换为相对于自车的位置。'
- en: Lane-object association. For GPT-4V, understanding the relative spatial relationships
    of traffic elements (such as objects, lane lines, etc.) in the 3D world is a highly
    challenging task. Directly inputting the 3D object coordinates and the curve representation
    of lane lines to GPT-4V does not enable effective reasoning. Therefore, we represent
    the relationships between objects and lane lines in the form of a file tree, and
    convert the information of objects into a natural language description based on
    their 3D bounding boxes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 道路物体关联。对于 GPT-4V，理解交通元素（如物体、车道线等）在 3D 世界中的相对空间关系是一项非常具有挑战性的任务。直接将 3D 物体坐标和车道线的曲线表示输入到
    GPT-4V 中并不能有效进行推理。因此，我们将物体与车道线之间的关系以文件树的形式表示，并根据它们的 3D 边界框将物体信息转换为自然语言描述。
- en: 'Simulated trajectories. Trajectories are sampled for counterfactual reasoning
    in two ways: 1) We select the initial lane based on three driving intentions:
    lane keeping, left lane change, and right lane change. Then we use the Depth-First
    Search (DFS) algorithm to link the lane centerline and get all possible vehicle
    trajectory paths. Then selecting different completion rates and speed targets
    for various lanes (acceleration, deceleration, and speed maintenance) to create
    simulated trajectories. 2) Generating driving trajectories based solely on lane
    centerlines makes it difficult to simulate scenarios that are ‘out of the drivable’.
    Therefore, we also performed clustering on the entire nuScenes dataset’s ego trajectories,
    selecting representative driving paths each time.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟轨迹。轨迹通过两种方式进行反事实推理：1）我们基于三种驾驶意图选择初始车道：保持车道、左侧变道和右侧变道。然后，我们使用深度优先搜索（DFS）算法链接车道中心线，获取所有可能的车辆轨迹路径。然后为各种车道选择不同的完成率和速度目标（加速、减速和速度保持）以创建模拟轨迹。2）仅基于车道中心线生成驾驶轨迹使得模拟“不可驾驶”场景变得困难。因此，我们还对整个nuScenes数据集的自车轨迹进行了聚类，每次选择具有代表性的驾驶路径。
- en: Expert trajectory. This is the log replay trajectory from nuScenes. The expert
    trajectories are classified into different types for high-level decision making.
    We also identify an object as “close”, if its minimum distance to the trajectory
    is smaller than 10 meters in the next 3 seconds. The close objects are then listed
    below the expert trajectory.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 专家轨迹。这是来自nuScenes的日志回放轨迹。专家轨迹被分类为不同类型，用于高层次决策。我们还将物体标识为“接近”，如果其与轨迹的最小距离在接下来的3秒内小于10米。接近的物体会在专家轨迹下方列出。
- en: 'In the bottom block of the Tab. [1](#S3.T1 "Table 1 ‣ 3.1 Offline Question-Answering.
    ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous
    Driving with 3D Perception, Reasoning and Planning"), we describe the different
    types of QA responses obtained by using the above context information:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格的底部块中，我们描述了通过使用上述上下文信息获得的不同类型的问答响应：
- en: 'Scene description. We directly take caption (prompt type 1 in Tab. [1](#S3.T1
    "Table 1 ‣ 3.1 Offline Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning")) as the answer of scene description.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 场景描述。我们直接采用标题（表格中的提示类型1）作为场景描述的答案。
- en: Attention. Given the simulated and expert trajectories, run simulation to identify
    close objects. At the same time, we also allowed GPT4 to use its own common sense
    to identify threatening traffic elements.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力。给定模拟轨迹和专家轨迹，运行模拟以识别接近的物体。同时，我们还允许GPT-4运用自身的常识来识别威胁性的交通元素。
- en: Counterfactual reasoning. Given the simulated trajectories, we simulate to check
    if the trajectories violate the traffic rules, such as run a red light, collision
    to other objects or the road boundary.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实推理。给定模拟轨迹，我们模拟检查这些轨迹是否违反交通规则，例如闯红灯、碰撞其他物体或路边界。
- en: Decision making and planning. We present the high-level decision making as well
    as the expert trajectory and use GPT-4V to reason why this trajectory is safe,
    given the previous prompt and response information as context.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 决策和规划。我们展示了高层次的决策制定以及专家轨迹，并使用GPT-4V推理为何该轨迹是安全的，前提是有之前的提示和响应信息作为上下文。
- en: General Conversation. We also prompt GPT-4 with generating multi-turn dialogues
    based on caption information and image content, involving the object countings,
    color, relative position, and OCR-type tasks. We found that this approach helps
    improve the model’s recognition of long-tail objects.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一般对话。我们还提示GPT-4基于标题信息和图像内容生成多轮对话，涉及物体计数、颜色、相对位置和OCR类型的任务。我们发现这种方法有助于提高模型对长尾物体的识别。
- en: 3.2 Online Question-Answering.
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 在线问答。
- en: '![Refer to caption](img/7cb52fb0ac5ca72b246c4006dc0585ee.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7cb52fb0ac5ca72b246c4006dc0585ee.png)'
- en: 'Figure 3: Online QA generation.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在线问答生成。
- en: 'To fully leverage the 3D perception labels in the autonomous driving dataset,
    we generate numerous grounding-like tasks during the training process in an online
    manner. Specifically, the following tasks (in Fig. [3](#S3.F3 "Figure 3 ‣ 3.2
    Online Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive: A Holistic LLM-Agent
    Framework for Autonomous Driving with 3D Perception, Reasoning and Planning"))
    are designed:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '为了充分利用自主驾驶数据集中的3D感知标签，我们在训练过程中以在线方式生成了大量类似基础任务的任务。具体而言，设计了以下任务（见图 [3](#S3.F3
    "Figure 3 ‣ 3.2 Online Question-Answering. ‣ 3 OmniDrive-nuScenes ‣ OmniDrive:
    A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning
    and Planning")）：'
- en: 2D-to-3D Grounding. Given a 2D bounding box on a specific camera, e.g., ,
    the model is required to provide the 3D properties of the corresponding object,
    including its 3D categories, location, size, orientation, and velocity.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 2D到3D基础。给定特定相机上的2D边界框，例如 ，模型需要提供相应物体的3D属性，包括其3D类别、位置、大小、方向和速度。
- en: 3D Distance. Based on the randomly generated 3D coordinate, identify the traffic
    elements near the corresponding location and provide the 3D properties of the
    traffic elements.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 3D距离。基于随机生成的3D坐标，识别相应位置附近的交通元素，并提供交通元素的3D属性。
- en: Lane-to-objects. Based on the randomly selected lane center-line, list the objects
    present on this lane and their 3D properties.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 车道与物体。基于随机选择的车道中心线，列出该车道上存在的物体及其3D属性。
- en: 3.3 Metrics
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 评估指标
- en: The proposed OmniDrive dataset involves captioning, open-loop planning and counterfactual
    reasoning tasks. Each of these tasks has distinct emphasis, and it’s challenging
    to evaluate them with a single metric. In this section, we will elaborate on how
    we assess the performance of models on our dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的OmniDrive数据集涉及标注、开环规划和反事实推理任务。这些任务各有侧重，用单一指标评估它们是具有挑战性的。在本节中，我们将详细说明如何评估模型在我们数据集上的表现。
- en: For caption-related tasks, such as scene description and the selection of attention
    objects, we utilize the commonly employed language-based metrics to evaluate the
    sentence similarity at the word level, including METEOR [[3](#bib.bib3)], ROUGE [[27](#bib.bib27)],
    and CIDEr [[49](#bib.bib49)]. Following BEV-Planner [[26](#bib.bib26)], Collision
    Rate and Intersection Rate with the road boundary are adopted to evaluate the
    performance of the Open-loop planning. To evaluate the performance of the counterfactual
    reasoning, we ask GPT-3.5 to extract keywords based on the predictions. The keywords
    include ‘safety,’ ‘collision,’, ‘running a red light,’ and ‘out of the drivable
    area.’ Then we compare extracted keywords with the ground truth to calculate the
    Precision and Recall for each category of the accident.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与标注相关的任务，如场景描述和注意对象选择，我们利用常用的基于语言的指标来评估词级别的句子相似度，包括METEOR [[3](#bib.bib3)]、ROUGE [[27](#bib.bib27)]和CIDEr [[49](#bib.bib49)]。根据BEV-Planner [[26](#bib.bib26)]，采用碰撞率和与道路边界的交叉率来评估开环规划的表现。为了评估反事实推理的表现，我们要求GPT-3.5根据预测提取关键词。这些关键词包括“安全”、“碰撞”、“闯红灯”和“超出可驾驶区域”。然后，我们将提取的关键词与真实情况进行比较，以计算每类事故的精确度和召回率。
- en: 4 Experiment
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Implementation Details
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实现细节
- en: Our model uses EVA-02-L [[14](#bib.bib14)] as the vision encoder. It applies
    masked image modeling to distill CLIP [[44](#bib.bib44)], which can extract language-aligned
    vision features.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型使用EVA-02-L [[14](#bib.bib14)] 作为视觉编码器。它应用掩蔽图像建模来提炼CLIP [[44](#bib.bib44)]，能够提取与语言对齐的视觉特征。
- en: During the 2D pre-training stage, the training data and strategies, including
    batchsize, learning rate, and optimizer are the same as LLaVA v1.5’s [[30](#bib.bib30)].
    In the finetuning stage, the model is trained by AdamW [[34](#bib.bib34)] optimizer
    with a batch size of 16\. The learning rate for the projector is 4e-4, while the
    visual encoder and the LLM’s learning rates are 5e-4\. The cosine annealing policy
    is employed for the training stability. The models in the ablation study are trained
    for 6 epochs unless specified otherwise. The number of object queries, lane queries
    and carrier queries is set to 900, 300 and 256 respectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2D 预训练阶段，训练数据和策略（包括批量大小、学习率和优化器）与 LLaVA v1.5 相同 [[30](#bib.bib30)]。在微调阶段，模型使用
    AdamW [[34](#bib.bib34)] 优化器进行训练，批量大小为 16。投影器的学习率为 4e-4，而视觉编码器和 LLM 的学习率为 5e-4。采用余弦退火策略以确保训练稳定性。消融研究中的模型训练
    6 个 epoch，除非另有说明。物体查询、车道查询和载体查询的数量分别设置为 900、300 和 256。
- en: We also explore alternative architectures. Q-Former2D is initialized with 2D
    pre-trained weights. It processes the image features individually in the projector
    and fuses them in the LLM. The Dense BEV approach uses LSS method [[41](#bib.bib41),
    [40](#bib.bib40)] to transform perspective features into a BEV feature map. We
    implement temporal modeling following SOLOFusion [[40](#bib.bib40)]. The BEV features
    will be consecutively fed into a MLP projector and a LLM.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了其他架构。Q-Former2D 以 2D 预训练权重进行初始化。它在投影器中单独处理图像特征，并在 LLM 中融合这些特征。Dense BEV
    方法使用 LSS 方法 [[41](#bib.bib41), [40](#bib.bib40)] 将透视特征转换为 BEV 特征图。我们按照 SOLOFusion
    [[40](#bib.bib40)] 实现了时间建模。BEV 特征将连续输入到 MLP 投影器和 LLM 中。
- en: '|  |  | Counterfactual | Open-loop |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 反事实 | 开环 |'
- en: '| Ablation | Exp. | Safe | Red Light | Collision | Drivable Area | Col(%) |
    Inter(%) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 消融 | 实验 | 安全 | 红灯 | 碰撞 | 可驾驶区域 | Col(%) | Inter(%) |'
- en: '|  |  | P | R | P | R | P | R | P | R | Avg. | Avg. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  | P | R | P | R | P | R | P | R | 平均 | 平均 |'
- en: '| Full Model | Q-Former3D | $70.7$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | Q-Former3D | $70.7$ |'
- en: '| Data | No Online | 69.4 | 39.4 | 36.2 | 65.6 | 29.7 | 69.4 | 48.0 | 57.8
    | 4.93 | 4.02 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 无在线 | 69.4 | 39.4 | 36.2 | 65.6 | 29.7 | 69.4 | 48.0 | 57.8 | 4.93 |
    4.02 |'
- en: '| Architecture | Q-Former2D | 71.4 | $39.3$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Q-Former2D | 71.4 | $39.3$ |'
- en: '| Dense BEV | $70.2$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Dense BEV | $70.2$ |'
- en: '| No Temporal | 67.8 | 48.4 | 47.0 | 62.6 | 31.2 | 63.8 | 46.5 | 55.3 | $6.07$
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 无时间 | 67.8 | 48.4 | 47.0 | 62.6 | 31.2 | 63.8 | 46.5 | 55.3 | $6.07$ |'
- en: '| Perception | No Lane | 67.7 | 57.3 | 58.1 | 59.6 | 31.0 | 56.7 | 47.9 | 56.8
    | $4.65$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 感知 | 无车道 | 67.7 | 57.3 | 58.1 | 59.6 | 31.0 | 56.7 | 47.9 | 56.8 | $4.65$
    |'
- en: '| Supervision | No Object & Lane | $69.0$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | 无物体与车道 | $69.0$ |'
- en: 'Table 2: Ablation study on planning related tasks. P and R represent Precision
    and Recall respectively. "No online" means removing the online traing data. "No
    temporal" means removing the temporal modules. Freeze means no gradients are applied
    to the backbone layers. "No Object" and "No Lane" indicate no corresponding 3D
    perception supervision. In this table, none of the models in the planner are using
    high-level command and ego status.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：与规划相关任务的消融研究。P 和 R 分别表示精度和召回率。“无在线”表示移除在线训练数据。“无时间”表示移除时间模块。冻结意味着不对主干层应用梯度。“无物体”和“无车道”表示没有相应的
    3D 感知监督。在该表中，规划器中的模型均未使用高层命令和自我状态。
- en: 4.2 Planning with Counterfactual Reasoning
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 反事实推理规划
- en: 'Based on our Omnidrive-nuScenes, we ablate various modifications in training
    recipes and model architectures. All analysis shown in [Tab. 2](#S4.T2 "In 4.1
    Implementation Details ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning") are conducted
    without using high-level commands and ego status [[20](#bib.bib20), [26](#bib.bib26)].
    Under this setting, it can be observed that there is a certain correlation between
    counterfactual metrics and open-loop planning.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们的 Omnidrive-nuScenes，我们在训练配方和模型架构中消融了各种修改。所有分析均在未使用高层命令和自我状态的情况下进行 [[20](#bib.bib20),
    [26](#bib.bib26)]。在这种设置下，可以观察到反事实指标与开环规划之间存在一定的关联。
- en: We found that Q-former2D performs better on 2D-related tasks, such as determining
    the status of traffic lights. However, Q-Former3D clearly has a greater advantage
    in 3D tasks such as collision detection (32.2% in Precision and 72.6% in Recall)
    and drivable area identification (48.5% in Precision and 58.6% in Recall). The
    models with center-line construction tasks (i.e., Full Model) outperforms the
    one without lane supervision in drivable area tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现Q-Former2D在2D相关任务上表现更好，例如确定交通信号灯的状态。然而，Q-Former3D在3D任务上明显具有更大的优势，例如碰撞检测（精度32.2%和召回率72.6%）和可驾驶区域识别（精度48.5%和召回率58.6%）。具有中心线构建任务的模型（即，完整模型）在可驾驶区域任务中的表现优于没有车道监督的模型。
- en: 4.3 Ablation Study & Analysis
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究与分析
- en: 'Counterfactual Reasoning and Captioning. In Tab. [3](#S4.T3 "Table 3 ‣ 4.3
    Ablation Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning"), the Full
    Model achieves the best performance in terms of the counterfactual reasoning,
    with average precision of 52.3% and average recall of 59.6% .'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实推理和标注。在表 [3](#S4.T3 "表 3 ‣ 4.3 消融研究与分析 ‣ 4 实验 ‣ OmniDrive：具有3D感知、推理和规划的自主驾驶全方位LLM-代理框架")中，完整模型在反事实推理方面取得了最佳表现，平均精度为52.3%和平均召回率为59.6%。
- en: More importantly, we show that our model benefits strongly from Q-Former3D and
    achieves comparable performance to that of Q-Former2D on caption tasks with $38.0\%$
    ROUGE score. Furthermore, our model can process multi-view cameras simultaneously,
    while Q-Former2D processes each view separately and necessitating an inefficiently
    high number of tokens (1500+) for input to LLM. We note that the dense BEV model
    yields the poorest results because it fails to leverage the benefits brought by
    2D pre-training. We also found that, with the same Q-Former equipped with pre-training,
    the performance on descriptive tasks is similar. Introducing additional 3D supervision
    and temporal information did not result in significant improvements.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，我们展示了我们的模型在使用Q-Former3D时受益明显，并在标注任务上达到了与Q-Former2D相当的表现，ROUGE得分为$38.0\%$。此外，我们的模型可以同时处理多视角相机，而Q-Former2D则需要分别处理每个视角，并且输入到LLM的令牌数量（1500+）效率低下。我们注意到，密集BEV模型的结果最差，因为它未能利用2D预训练带来的好处。我们还发现，使用相同的Q-Former进行预训练，描述性任务的表现相似。引入额外的3D监督和时间信息并未带来显著的改进。
- en: '| Abaltion | Exp. | Counterfactual | Caption |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 消融实验 | 实验 | 反事实 | 标注 |'
- en: '| AP (%) | AR (%) | METEOR$\uparrow$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| AP（%） | AR（%） | METEOR$\uparrow$ |'
- en: '| Full Model | Q-Former3D | 52.3 | 59.6 | 38.0 | 68.6 | 32.6 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | Q-Former3D | 52.3 | 59.6 | 38.0 | 68.6 | 32.6 |'
- en: '| Data | No Online | 45.8 | 58.1 | 38.2 | 69.0 | 32.7 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 无在线 | 45.8 | 58.1 | 38.2 | 69.0 | 32.7 |'
- en: '| Architecture | Q-Former2D | 51.5 | 55.0 | 38.3 | 67.1 | 32.5 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | Q-Former2D | 51.5 | 55.0 | 38.3 | 67.1 | 32.5 |'
- en: '| Dense BEV | 45.6 | 49.5 | 35.6 | 59.5 | 27.8 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 密集BEV | 45.6 | 49.5 | 35.6 | 59.5 | 27.8 |'
- en: '| No Temporal | 48.1 | 57.5 | 37.9 | 68.4 | 32.6 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 无时间信息 | 48.1 | 57.5 | 37.9 | 68.4 | 32.6 |'
- en: '| Perception | No Lane | 51.2 | 57.6 | 38.0 | 67.8 | 32.6 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 感知 | 无车道 | 51.2 | 57.6 | 38.0 | 67.8 | 32.6 |'
- en: '| Supervision | No Object & Lane | 48.9 | 57.3 | 38.2 | 67.8 | 32.6 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | 无对象和车道 | 48.9 | 57.3 | 38.2 | 67.8 | 32.6 |'
- en: 'Table 3: Analysis on our benchmark.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：我们基准测试的分析。
- en: 'Table 4: Results on NuScenes-QA [[43](#bib.bib43)]. L and C represent Lidar
    and Camera respectively. We highlight the SoTA methods in each modality.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：NuScenes-QA [[43](#bib.bib43)]的结果。L和C分别代表激光雷达和相机。我们突出显示了每种模态的最先进方法。
- en: '| Model | Modality | Acc.(%) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 模态 | 准确率（%） |'
- en: '| --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| BEVDet+BUTD [[43](#bib.bib43)] | C | 57.0 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| BEVDet+BUTD [[43](#bib.bib43)] | C | 57.0 |'
- en: '| BEVDet+MCAN [[43](#bib.bib43)] | C | 57.9 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| BEVDet+MCAN [[43](#bib.bib43)] | C | 57.9 |'
- en: '| CenterPoint+BUTD [[43](#bib.bib43)] | L | 58.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| CenterPoint+BUTD [[43](#bib.bib43)] | L | 58.1 |'
- en: '| CenterPoint+MCAN [[43](#bib.bib43)] | L | 59.5 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| CenterPoint+MCAN [[43](#bib.bib43)] | L | 59.5 |'
- en: '| OmniDrive | C | 59.2 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive | C | 59.2 |'
- en: 'Comparison on NuScenes-QA. We also present results on NuScenes-QA in Tab. [4](#S4.T4
    "Table 4 ‣ 4.3 Ablation Study & Analysis ‣ 4 Experiment ‣ OmniDrive: A Holistic
    LLM-Agent Framework for Autonomous Driving with 3D Perception, Reasoning and Planning").
    In NuScenes-QA, most answers in NuScenes-QA are single-word and related to perception
    only. In the same camera modality, our model surpasses BEVDet+MCAN by 1.3% in
    accuracy, demonstrating the importance of pre-training. Our model’s performance
    is comparable to the Lidar modality’s models.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 NuScenes-QA 的比较。我们还在表格 [4](#S4.T4 "表 4 ‣ 4.3 消融研究与分析 ‣ 4 实验 ‣ OmniDrive:
    一个用于自主驾驶的整体 LLM-Agent 框架，具有 3D 感知、推理和规划") 中展示了 NuScenes-QA 的结果。在 NuScenes-QA 中，大多数答案都是单词并且仅与感知相关。在相同的摄像头模态中，我们的模型在准确率上超越了
    BEVDet+MCAN 1.3%，这表明了预训练的重要性。我们的模型表现与激光雷达模态的模型相当。'
- en: '| Method | Ego Status | L2 (m) $\downarrow$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Ego 状态 | L2 (m) $\downarrow$ |'
- en: '| BEV | Planner | 1s | 2s | 3s | Avg. | 1s | 2s | 3s | Avg. | 1s | 2s | 3s
    | Avg. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| BEV | 规划器 | 1s | 2s | 3s | 平均 | 1s | 2s | 3s | 平均 | 1s | 2s | 3s | 平均 |'
- en: '| ST-P3 | - | - | 1.59${}^{\text{\textdagger}}$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ST-P3 | - | - | 1.59${}^{\text{\textdagger}}$ |'
- en: '| UniAD | - | - | 0.59 | 1.01 | 1.48 | 1.03 | 0.16 | 0.51 | 1.64 | 0.77 | 0.35
    | 1.46 | 3.99 | 1.93 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| UniAD | - | - | 0.59 | 1.01 | 1.48 | 1.03 | 0.16 | 0.51 | 1.64 | 0.77 | 0.35
    | 1.46 | 3.99 | 1.93 |'
- en: '| UniAD | ✓ | ✓ | 0.20 | 0.42 | 0.75 | 0.46 | 0.02 | 0.25 | 0.84 | 0.37 | 0.20
    | 1.33 | 3.24 | 1.59 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| UniAD | ✓ | ✓ | 0.20 | 0.42 | 0.75 | 0.46 | 0.02 | 0.25 | 0.84 | 0.37 | 0.20
    | 1.33 | 3.24 | 1.59 |'
- en: '| VAD-Base | - | - | 0.69 | 1.22 | 1.83 | 1.25 | 0.06 | 0.68 | 2.52 | 1.09
    | 1.02 | 3.44 | 7.00 | 3.82 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| VAD-Base | - | - | 0.69 | 1.22 | 1.83 | 1.25 | 0.06 | 0.68 | 2.52 | 1.09
    | 1.02 | 3.44 | 7.00 | 3.82 |'
- en: '| VAD-Base | ✓ | ✓ | 0.17 | 0.34 | 0.60 | 0.37 | 0.04 | 0.27 | 0.67 | 0.33
    | 0.21 | 2.13 | 5.06 | 2.47 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| VAD-Base | ✓ | ✓ | 0.17 | 0.34 | 0.60 | 0.37 | 0.04 | 0.27 | 0.67 | 0.33
    | 0.21 | 2.13 | 5.06 | 2.47 |'
- en: '| Ego-MLP | - | ✓ | 0.15 | 0.32 | 0.59 | 0.35 | 0.00 | 0.27 | 0.85 | 0.37 |
    0.27 | 2.52 | 6.60 | 2.93 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Ego-MLP | - | ✓ | 0.15 | 0.32 | 0.59 | 0.35 | 0.00 | 0.27 | 0.85 | 0.37 |
    0.27 | 2.52 | 6.60 | 2.93 |'
- en: '| BEV-Planner | - | - | 0.30 | 0.52 | 0.83 | 0.55 | 0.10 | 0.37 | 1.30 | 0.59
    | 0.78 | 3.79 | 8.22 | 4.26 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| BEV-Planner | - | - | 0.30 | 0.52 | 0.83 | 0.55 | 0.10 | 0.37 | 1.30 | 0.59
    | 0.78 | 3.79 | 8.22 | 4.26 |'
- en: '| BEV-Planner++ | ✓ | ✓ | 0.16 | 0.32 | 0.57 | 0.35 | 0.00 | 0.29 | 0.73 |
    0.34 | 0.35 | 2.62 | 6.51 | 3.16 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| BEV-Planner++ | ✓ | ✓ | 0.16 | 0.32 | 0.57 | 0.35 | 0.00 | 0.29 | 0.73 |
    0.34 | 0.35 | 2.62 | 6.51 | 3.16 |'
- en: '| OmniDrive$\ddagger$ | - | - | 1.15 | 1.96 | 2.84 | 1.98 | 0.80 | 3.12 | 7.46
    | 3.79 | 1.66 | 3.86 | 8.26 | 4.59 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive$\ddagger$ | - | - | 1.15 | 1.96 | 2.84 | 1.98 | 0.80 | 3.12 | 7.46
    | 3.79 | 1.66 | 3.86 | 8.26 | 4.59 |'
- en: '| OmniDrive | - | - | 0.40 | 0.80 | 1.32 | 0.84 | 0.04 | 0.46 | 2.32 | 0.94
    | 0.93 | 3.65 | 8.28 | 4.29 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive | - | - | 0.40 | 0.80 | 1.32 | 0.84 | 0.04 | 0.46 | 2.32 | 0.94
    | 0.93 | 3.65 | 8.28 | 4.29 |'
- en: '| OmniDrive++ | ✓ | ✓ | 0.14 | 0.29 | 0.55 | 0.33 | 0.00 | 0.13 | 0.78 | 0.30
    | 0.56 | 2.48 | 5.96 | 3.00 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| OmniDrive++ | ✓ | ✓ | 0.14 | 0.29 | 0.55 | 0.33 | 0.00 | 0.13 | 0.78 | 0.30
    | 0.56 | 2.48 | 5.96 | 3.00 |'
- en: 'Table 5: Comparison on the Open-loop planning. For a fair comparison, we referred
    to the reproduced results in BEV-Planner [[26](#bib.bib26)]. †: The official implementation
    of ST-P3 (ID-0) utilized partial erroneous ground truth. ${}\ddagger$: The high-level
    command is not used during the training and testing phases.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：开环规划的比较。为了公平比较，我们参考了 BEV-Planner 中重新制作的结果 [[26](#bib.bib26)]。†：ST-P3 (ID-0)
    的官方实现使用了部分错误的真实数据。${}\ddagger$: 在训练和测试阶段没有使用高层指令。'
- en: 4.4 Discussion on Open-loop Planning
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 关于开环规划的讨论
- en: 'We compare the proposed OmniDrive with previous state-of-the-art vision-based
    planners in Tab. [5](#S4.T5 "Table 5 ‣ 4.3 Ablation Study & Analysis ‣ 4 Experiment
    ‣ OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D Perception,
    Reasoning and Planning"). The MLLM based open-loop planning can also achieve comparable
    performance to SoTA methods. However, as mentioned in BEV-Planner [[26](#bib.bib26)],
    it is observed that encoding the ego status significantly improves the metrics
    across all methods. Additionally, we found that the high-level command also drastically
    reduces the collision rate and the intersection rate. Previous methods provided
    high-level commands based on the relative position of the ground-truth trajectory,
    which imposed significant constraints on the network’s regression, thereby reducing
    the likelihood of deviating too far from the ground truth trajectory. We believe
    this design is also unreasonable, so in other experiments, we eliminated this
    setting.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表[5](#S4.T5 "表 5 ‣ 4.3 消融研究与分析 ‣ 4 实验 ‣ OmniDrive：一个全面的LLM-Agent框架，用于具有3D感知、推理和规划的自动驾驶")中将所提出的OmniDrive与以前最先进的基于视觉的规划器进行了比较。基于MLLM的开环规划也可以实现与SoTA方法相媲美的性能。然而，正如BEV-Planner
    [[26](#bib.bib26)]中提到的，编码自我状态显著改善了所有方法的指标。此外，我们发现高层指令也显著减少了碰撞率和交叉率。以前的方法基于地面真实轨迹的相对位置提供高层指令，这对网络的回归施加了显著约束，从而减少了偏离地面真实轨迹的可能性。我们认为这种设计也不合理，因此在其他实验中，我们去除了这一设置。
- en: '| Scene Description & Counterfactual Reasoning |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 场景描述与反事实推理 |'
- en: '| --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  | ![Refer to caption](img/6f323f3584042d1a1748a11516d6c9a1.png)  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | ![参考说明](img/6f323f3584042d1a1748a11516d6c9a1.png) |'
- en: '| User | Can you summarize what the panoramic images show? |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 你能总结一下全景图展示了什么吗？ |'
- en: '| Ours | The panoramic images depict a rainy day in an urban setting with wet
    roads reflecting the overcast sky. On the left-front, we see a parking lot entrance
    and a traffic light showing green, indicating the flow of traffic is permitted.
    The front view reveals a multi-lane road with vehicles, including a white car
    turning right, and traffic lights at an intersection. The right-front shows a
    pedestrian crossing with a traffic light, and a person waiting to cross. The left-rear
    view captures a traffic light at red, a pedestrian crossing, and a black car waiting.
    The rear view shows a large building under construction, with a crane towering
    above and a few vehicles on the road. The right-rear view includes a traffic light
    at red, a pedestrian crossing, and a road leading to a roundabout, with a black
    car approaching the intersection. The overall scene is one of a bustling city
    despite the damp weather. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 全景图描绘了一个城市中的雨天，湿漉漉的道路反射着阴沉的天空。在左前方，我们看到一个停车场入口和一个显示绿色的交通灯，表明允许通行。前方视图显示了一条多车道的道路，包含一辆右转的白色汽车，以及一个交叉口的交通灯。右前方显示了一个行人过街道和一个等待过马路的人。左后方视图捕捉到一个红灯的交通灯、一个行人过街道以及一辆黑色汽车在等待。后方视图展示了一栋正在建设的大楼，一台塔吊高耸，几辆车在路上行驶。右后方视图包括一个红灯的交通灯、一个行人过街道，以及一条通往环形交叉路口的道路，一辆黑色汽车正在接近交叉口。整体场景是一个繁忙的城市，尽管天气潮湿。
    |'
- en: '| User | What would be the outcome if you decided to move slowly and make a
    right turn? |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 用户 | 如果你决定慢慢移动并右转，结果会是什么？ |'
- en: '| Ours | If I were to move slowly and make a right turn, following the trajectory
    [PT, (+2.44, -0.09), (+4.91, -0.36), (+7.41, -0.84), (+9.91, -1.53), (+12.37,
    -2.44), (+14.76, -3.55)], it would result in leaving the drivable area, which
    is a dangerous and incorrect action. The trajectory indicates a turn that is too
    sharp and not aligned with the road layout, leading to potential collisions or
    driving off the road. In this scenario, the correct behavior would be to maintain
    my lane and follow the road’s curvature, ensuring I stay within the bounds of
    the drivable area and adhere to traffic rules. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 我们 | 如果我慢慢移动并右转，沿着轨迹[PT, (+2.44, -0.09), (+4.91, -0.36), (+7.41, -0.84),
    (+9.91, -1.53), (+12.37, -2.44), (+14.76, -3.55)]，这将导致离开可驾驶区域，这是一个危险且不正确的动作。轨迹表明转弯过于急剧，并且与道路布局不一致，可能导致碰撞或驶离道路。在这种情况下，正确的行为是保持车道并遵循道路的弯曲，确保我留在可驾驶区域的范围内，并遵守交通规则。
    |'
- en: 'Table 6: Visualization results on scene description and counterfactual reasoning.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：场景描述和反事实推理的可视化结果。
- en: 4.5 Visualization
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 可视化
- en: 'A challenging scene is shown in Tab. [6](#S4.T6 "Table 6 ‣ 4.4 Discussion on
    Open-loop Planning ‣ 4 Experiment ‣ OmniDrive: A Holistic LLM-Agent Framework
    for Autonomous Driving with 3D Perception, Reasoning and Planning"). OmniDrive-Agent
    shows impressive results on scene description and counterfactual reasoning. The
    model has a basic understanding of relative positions and can make counterfactual
    reasoning correctly based on given trajectories and safety considerations.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[6](#S4.T6 "表 6 ‣ 4.4 关于开环规划的讨论 ‣ 4 实验 ‣ OmniDrive: 一个具备 3D 感知、推理和规划的全面
    LLM-Agent 框架的自主驾驶框架")中展示了一个具有挑战性的场景。OmniDrive-Agent在场景描述和反事实推理方面表现出色。该模型对相对位置有基本的理解，并能够根据给定的轨迹和安全考虑正确进行反事实推理。'
- en: 5 Related Works
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 5.1 End-to-End Autonomous Driving
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 端到端自主驾驶
- en: 'The objective of end-to-end autonomous driving is to create a fully differentiable
    system that spans from sensor input to control signals [[61](#bib.bib61), [57](#bib.bib57),
    [42](#bib.bib42)]. This system allows for the joint optimization of the entire
    system, thereby mitigating the accumulation of errors. The current technical road-map
    is primarily divided into two paths: open-loop autonomous driving and closed-loop
    autonomous driving.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端自主驾驶的目标是创建一个从传感器输入到控制信号的完全可微分系统[[61](#bib.bib61), [57](#bib.bib57), [42](#bib.bib42)]。该系统允许对整个系统进行联合优化，从而减轻误差的累积。当前的技术路线图主要分为两个路径：开环自主驾驶和闭环自主驾驶。
- en: In the open-loop autonomous driving, the training and evaluation processes are
    generally conducted on log-replayed real world datasets [[43](#bib.bib43)]. This
    approach overlooks the impact of interactions between the ego vehicle and other
    traffic participants, leading to cumulative errors. Pioneering work UniAD [[17](#bib.bib17)]
    and VAD [[20](#bib.bib20)] integrate modularized design of perception tasks such
    as object detection, tracking, and semantic segmentation into a unified planning
    framework. However, Ego-MLP [[60](#bib.bib60)] and BEV-Planner [[26](#bib.bib26)]
    highlight the limitations of open-loop end-to-end driving benchmarks. In these
    benchmarks, models may overfit the ego-status information to achieve unreasonably
    high performance. Researchers are addressing the challenges in open-loop evaluation
    by introducing closed-loop benchmarks. Recent works, e.g., MILE [[15](#bib.bib15)],
    ThinkTwice [[19](#bib.bib19)], VADv2 [[7](#bib.bib7)] leverage CARLA [[11](#bib.bib11)]
    as the simulator, which enables the creation of virtual environments with feedback
    from other agents. Researchers urgently need a reasonable way to evaluate end-to-end
    autonomous driving systems in the real world. MLLM models bridge the gap between
    data-driven decision-making and the user. This enables us to perform interpretable
    analysis and conduct counterfactual reasoning based on a specific trajectory,
    thereby enhancing the safety redundancy of the autonomous driving system.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在开环自主驾驶中，训练和评估过程通常在日志重放的真实世界数据集上进行[[43](#bib.bib43)]。这种方法忽视了自车与其他交通参与者之间互动的影响，导致累计误差。开创性的工作UniAD[[17](#bib.bib17)]和VAD[[20](#bib.bib20)]将对象检测、跟踪和语义分割等感知任务的模块化设计整合到一个统一的规划框架中。然而，Ego-MLP[[60](#bib.bib60)]和BEV-Planner[[26](#bib.bib26)]突出了开环端到端驾驶基准的局限性。在这些基准中，模型可能会过度拟合自车状态信息，以实现不合理的高性能。研究人员正在通过引入闭环基准来应对开环评估中的挑战。最近的工作，例如MILE[[15](#bib.bib15)]、ThinkTwice[[19](#bib.bib19)]、VADv2[[7](#bib.bib7)]利用CARLA[[11](#bib.bib11)]作为模拟器，这使得可以创建具有来自其他代理的反馈的虚拟环境。研究人员迫切需要一种合理的方法来评估现实世界中的端到端自主驾驶系统。MLLM模型弥合了数据驱动决策与用户之间的差距。这使我们能够进行可解释的分析并基于特定轨迹进行反事实推理，从而增强自主驾驶系统的安全冗余。
- en: 5.2 Multimodal Language Models (MLLMs)
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 多模态语言模型（MLLMs）
- en: 'Muiltimodal language models leverage LLMs and various modalities’ encoders
    to successfully bridge the gap between language and other modalities and perform
    well on multimodal tasks ranging from visual question answer, captioning, and
    open-world detection. Some MLLMs such as CLIP [[44](#bib.bib44)] and ALIGN [[18](#bib.bib18)]
    utilize contrastive learning to create a similar embedding space for both language
    and vision. More recently, others such as BLIP-2 [[24](#bib.bib24)] explicitly
    targets multimodal tasks and takes multimodal inputs. For these models, there
    are two common techniques in order to align language and other input modalities:
    self-attention and cross-attention. LLaVa [[31](#bib.bib31)], PaLM-E [[12](#bib.bib12)],
    PaLI [[8](#bib.bib8)], and RT2 [[62](#bib.bib62)] utilize self-attention for alignment
    by interleaving or concatenating image and text tokens in fixed sequence lengths.
    However, self-attention based MLLMs are unable to handle high resolution inputs
    and are unsuitable for autonomous driving with multi-camera high solution images.
    Conversely, Flamingo [[1](#bib.bib1)], Qwen-VL [[2](#bib.bib2)], BLIP-2 [[24](#bib.bib24)],
    utilize cross-attention and are able to extract a fixed number of visual tokens
    regardless of image resolution. Because of this, our model utilizes Qformer architecture
    from BLIP-2 to handle our high resolution images.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态语言模型利用 LLM 和各种模态编码器成功地弥合了语言与其他模态之间的差距，并在视觉问题回答、图像描述和开放世界检测等多模态任务中表现出色。一些多模态语言模型，如
    CLIP [[44](#bib.bib44)] 和 ALIGN [[18](#bib.bib18)]，利用对比学习为语言和视觉创建了类似的嵌入空间。最近，其他模型如
    BLIP-2 [[24](#bib.bib24)] 明确针对多模态任务并接受多模态输入。对于这些模型，有两种常见技术来对齐语言和其他输入模态：自注意力和交叉注意力。LLaVa [[31](#bib.bib31)]、PaLM-E [[12](#bib.bib12)]、PaLI [[8](#bib.bib8)]
    和 RT2 [[62](#bib.bib62)] 通过在固定序列长度中交错或连接图像和文本标记来利用自注意力进行对齐。然而，基于自注意力的多模态语言模型无法处理高分辨率输入，不适用于多摄像头高分辨率图像的自动驾驶。相反，Flamingo [[1](#bib.bib1)]、Qwen-VL [[2](#bib.bib2)]
    和 BLIP-2 [[24](#bib.bib24)] 利用交叉注意力，能够提取固定数量的视觉标记，无论图像分辨率如何。因此，我们的模型利用 BLIP-2
    的 Qformer 架构来处理高分辨率图像。
- en: 5.3 Drive LLM-Agents and Benchmarks
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 驱动 LLM-代理和基准测试
- en: Drive LLM-Agents. Given LLM/MLLMs’ high performance and ability to align modalities
    with language, there is a rush to incorporate MLLMs/LLMs with autonomous driving
    (AD). Most AD MLLMs methods attempt to create explainable autonomous driving with
    end-to-end learning. DriveGPT4 leverages LLMs to generate reasons for car actions
    while also predicting car’s next control signals [[58](#bib.bib58)]. Similarly,
    Drive Anywhere proposes a patch-aligned feature extraction for MLLMs that allow
    it to provide text query-able driving decisions [[51](#bib.bib51)]. Other works
    leverage MLLMs through graph-based VQA (DriveLM) [[46](#bib.bib46)] or chain-of-thought
    (CoT) design [[47](#bib.bib47), [54](#bib.bib54)]. They explicitly solve multiple
    driving tasks alongside typical MLLM tasks, such as generating scene description
    and analysis, prediction, and planning.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动 LLM-代理。鉴于 LLM/MLLM 的高性能和将模态与语言对齐的能力，急于将 MLLMs/LLMs 应用于自动驾驶（AD）。大多数 AD MLLMs
    方法尝试通过端到端学习创建可解释的自动驾驶。DriveGPT4 利用 LLM 生成车辆动作的原因，同时预测车辆的下一个控制信号 [[58](#bib.bib58)]。类似地，Drive
    Anywhere 提出了一个补丁对齐特征提取方法，允许 MLLMs 提供可文本查询的驾驶决策 [[51](#bib.bib51)]。其他工作通过基于图的 VQA（DriveLM） [[46](#bib.bib46)]
    或思维链（CoT）设计 [[47](#bib.bib47), [54](#bib.bib54)] 来利用 MLLMs。他们明确解决了多个驾驶任务以及典型的
    MLLM 任务，例如生成场景描述和分析、预测和规划。
- en: Benchmarks. To evaluate AD perception and planning, there are various datasets
    that capture perception, planning, steering, motion data (ONCE [[37](#bib.bib37)],
    NuPlan [[5](#bib.bib5)], nuScenes [[4](#bib.bib4)], CARLA [[11](#bib.bib11)],
    Waymo [[13](#bib.bib13)]). However, datasets with more comprehensive lanugage
    annotations are required to evaluate Drive LLM methods. Datasets focused on perception
    and tracking include reasoning, or descriptive like captions range from nuScenes-QA [[43](#bib.bib43)],
    NuPrompt,  [[56](#bib.bib56)]. HAD and Talk2Car both contain human like advice
    to best navigate the car [[22](#bib.bib22), [9](#bib.bib9)], while LaMPilot contains
    labels meant to evaluate transition from human commands to drive action [[35](#bib.bib35)].
    Beyond scene descriptions, DRAMA [[36](#bib.bib36)] and Rank2Tell [[45](#bib.bib45)]
    focus on risk object localization. Contrastly, BDD-X, Reason2Drive focus on car
    explainability by providing reasons behind ego car’s action and behavior [[23](#bib.bib23),
    [38](#bib.bib38), [39](#bib.bib39)]. LingoQA [[38](#bib.bib38)] has introduced
    counterfactual questions into the autonomous driving QA dataset. We believe that
    the interpretability and safety redundancy of autonomous driving in the open-loop
    setting can be further enhanced by applying counterfactual reasoning to 3D trajectory
    analysis.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试。为了评估自动驾驶感知和规划，有各种数据集捕捉感知、规划、转向、运动数据（ONCE [[37](#bib.bib37)]，NuPlan [[5](#bib.bib5)]，nuScenes [[4](#bib.bib4)]，CARLA [[11](#bib.bib11)]，Waymo [[13](#bib.bib13)]）。然而，需要具有更全面语言注释的数据集来评估Drive
    LLM方法。关注感知和跟踪的数据集包括推理或描述性如nuScenes-QA [[43](#bib.bib43)]，NuPrompt [[56](#bib.bib56)]。HAD和Talk2Car都包含类似于人类的建议，以最佳方式导航车辆 [[22](#bib.bib22)，[9](#bib.bib9)]，而LaMPilot包含旨在评估从人类命令到驾驶动作的标签 [[35](#bib.bib35)]。超越场景描述，DRAMA [[36](#bib.bib36)]和Rank2Tell [[45](#bib.bib45)]关注风险物体定位。相对而言，BDD-X、Reason2Drive关注通过提供自车行为背后的理由来实现汽车的可解释性 [[23](#bib.bib23)，[38](#bib.bib38)，[39](#bib.bib39)]。LingoQA [[38](#bib.bib38)]在自动驾驶QA数据集中引入了反事实问题。我们相信，通过将反事实推理应用于3D轨迹分析，可以进一步增强开放环设置中自动驾驶的可解释性和安全冗余。
- en: 6 Conclusion
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We address the challenges of end-to-end autonomous driving with LLM-agents by
    proposing OmniDrive-Agent and OmniDrive-nuScenes. OmniDrive-Agent adopts a novel
    Q-Former3D MLLM architecture that can efficiently handle high resolution multi-view
    videos. Our model design enables minimal adjustments to leverage 2D pre-trained
    knowledge while gaining important 3D spatial understanding. We additionally provide
    a novel benchmark for end-to-end autonomous driving which features counterfactual
    reasoning alongside 3D spatial awareness and reasoning tasks. OmniDrive-Agent
    demonstrates the efficacy by addressing high-resolution multi-view video input
    and illustrate excellent scene description and counterfactual reasoning. The model
    also yields compelling performance on open-loop 3D planning.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过提出OmniDrive-Agent和OmniDrive-nuScenes来解决端到端自动驾驶与LLM代理的挑战。OmniDrive-Agent采用了一种新型的Q-Former3D
    MLLM架构，能够高效处理高分辨率多视角视频。我们的模型设计使得在获得重要的3D空间理解的同时，仅需对2D预训练知识进行最小调整。此外，我们提供了一个新颖的端到端自动驾驶基准，结合了反事实推理、3D空间感知和推理任务。OmniDrive-Agent通过处理高分辨率多视角视频展示了其有效性，并展示了优秀的场景描述和反事实推理。该模型在开放环3D规划中也展现了引人注目的性能。
- en: Limitations. Our method has not been validated on even larger datasets e.g.,
    nuPlan [[5](#bib.bib5)]. The simulation of counterfactual outcomes, despite moving
    beyond single trajectories, does not yet consider reaction from other agents.
    This part can be further formed as a closed-loop setup, and we will leave it for
    future work.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性。我们的方法尚未在更大的数据集上验证，例如nuPlan [[5](#bib.bib5)]。尽管反事实结果的模拟已超越单一轨迹，但尚未考虑其他代理的反应。这部分可以进一步形成闭环设置，我们将留待未来工作。
- en: References
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
    K., Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language
    model for few-shot learning. In: NeurIPs (2022)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
    K., Mensch, A., Millican, K., Reynolds, M., 等：Flamingo：一种用于少量样本学习的视觉语言模型。发表于：NeurIPs
    (2022)'
- en: '[2] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou,
    C., Zhou, J.: Qwen-VL: A frontier large vision-language model with versatile abilities.
    arXiv:2308.12966 (2023)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou,
    C., Zhou, J.：Qwen-VL：一种具有多种能力的前沿大规模视觉语言模型。arXiv:2308.12966 (2023)'
- en: '[3] Banerjee, S., Lavie, A.: METEOR: An automatic metric for mt evaluation
    with improved correlation with human judgments. In: ACL workshop (2005)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Banerjee, S., Lavie, A.: METEOR: 一种自动化的机器翻译评估指标，与人工评估的相关性有所改进。发表于 ACL 研讨会
    (2005)'
- en: '[4] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan,
    A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes: A multimodal dataset for autonomous
    driving. In: CVPR (2020)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan,
    A., Pan, Y., Baldan, G., Beijbom, O.: nuScenes: 一个用于自动驾驶的多模态数据集。发表于 CVPR (2020)'
- en: '[5] Caesar, H., Kabzan, J., Tan, K.S., Fong, W.K., Wolff, E., Lang, A., Fletcher,
    L., Beijbom, O., Omari, S.: NuPlan: A closed-loop ml-based planning benchmark
    for autonomous vehicles. arXiv:2106.11810 (2021)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Caesar, H., Kabzan, J., Tan, K.S., Fong, W.K., Wolff, E., Lang, A., Fletcher,
    L., Beijbom, O., Omari, S.: NuPlan: 一种基于机器学习的封闭循环规划基准，用于自动驾驶车辆。arXiv:2106.11810
    (2021)'
- en: '[6] Chen, L., Sinavski, O., Hünermann, J., Karnsund, A., Willmott, A.J., Birch,
    D., Maund, D., Shotton, J.: Driving with LLMs: Fusing object-level vector modality
    for explainable autonomous driving. arXiv:2310.01957 (2023)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Chen, L., Sinavski, O., Hünermann, J., Karnsund, A., Willmott, A.J., Birch,
    D., Maund, D., Shotton, J.: 使用 LLMs 驾驶：融合对象级别的向量模态以实现可解释的自动驾驶。arXiv:2310.01957
    (2023)'
- en: '[7] Chen, S., Jiang, B., Gao, H., Liao, B., Xu, Q., Zhang, Q., Huang, C., Liu,
    W., Wang, X.: VADv2: End-to-end vectorized autonomous driving via probabilistic
    planning. arXiv:2402.13243 (2024)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Chen, S., Jiang, B., Gao, H., Liao, B., Xu, Q., Zhang, Q., Huang, C., Liu,
    W., Wang, X.: VADv2: 通过概率规划实现端到端的向量化自动驾驶。arXiv:2402.13243 (2024)'
- en: '[8] Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz,
    D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al.: PaLI: A jointly-scaled
    multilingual language-image model. arXiv:2209.06794 (2022)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz,
    D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., 等：PaLI: 一种联合缩放的多语言图像语言模型。arXiv:2209.06794
    (2022)'
- en: '[9] Deruyttere, T., Vandenhende, S., Grujicic, D., Van Gool, L., Moens, M.F.:
    Talk2Car: Taking control of your self-driving car. In: EMNLP-IJCNLP (2019)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Deruyttere, T., Vandenhende, S., Grujicic, D., Van Gool, L., Moens, M.F.:
    Talk2Car: 掌控你的自驾车。发表于 EMNLP-IJCNLP (2019)'
- en: '[10] Ding, X., Han, J., Xu, H., Liang, X., Zhang, W., Li, X.: Holistic autonomous
    driving understanding by bird’s-eye-view injected multi-modal large models. arXiv:2401.00988
    (2024)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ding, X., Han, J., Xu, H., Liang, X., Zhang, W., Li, X.: 全面自动驾驶理解通过鸟瞰图注入的多模态大模型。arXiv:2401.00988
    (2024)'
- en: '[11] Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: Carla:
    An open urban driving simulator. In: CoRL (2017)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: Carla:
    一个开放的城市驾驶模拟器。发表于 CoRL (2017)'
- en: '[12] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter,
    B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al.: Palm-e: An embodied multimodal
    language model. arXiv:2303.03378 (2023)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Ichter,
    B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., 等：Palm-e: 一种具身的多模态语言模型。arXiv:2303.03378
    (2023)'
- en: '[13] Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai,
    Y., Sapp, B., Qi, C.R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan,
    V., McCauley, A., Shlens, J., Anguelov, D.: Large scale interactive motion forecasting
    for autonomous driving: The waymo open motion dataset. In: ICCV (2021)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai,
    Y., Sapp, B., Qi, C.R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan,
    V., McCauley, A., Shlens, J., Anguelov, D.: 大规模互动运动预测用于自动驾驶：Waymo 开放运动数据集。发表于
    ICCV (2021)'
- en: '[14] Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., Cao, Y.: EVA-02: A visual
    representation for neon genesis. arXiv:2303.11331 (2023)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Fang, Y., Sun, Q., Wang, X., Huang, T., Wang, X., Cao, Y.: EVA-02: 一种用于新世纪的视觉表示。arXiv:2303.11331
    (2023)'
- en: '[15] Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall,
    A., Cipolla, R., Shotton, J.: Model-based imitation learning for urban driving.
    In: NeurIPS (2022)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall,
    A., Cipolla, R., Shotton, J.: 基于模型的模仿学习用于城市驾驶。发表于 NeurIPS (2022)'
- en: '[16] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.: LoRA: Low-rank adaptation of large language models. arXiv:2106.09685
    (2021)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.: LoRA: 大型语言模型的低秩适应。arXiv:2106.09685 (2021)'
- en: '[17] Hu, Y., Yang, J., Chen, L., Li, K., Sima, C., Zhu, X., Chai, S., Du, S.,
    Lin, T., Wang, W., et al.: Planning-oriented autonomous driving. In: CVPR (2023)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Hu, Y., Yang, J., Chen, L., Li, K., Sima, C., Zhu, X., Chai, S., Du, S.,
    Lin, T., Wang, W., 等：以规划为导向的自动驾驶。发表于 CVPR (2023)'
- en: '[18] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.,
    Sung, Y.H., Li, Z., Duerig, T.: Scaling up visual and vision-language representation
    learning with noisy text supervision. In: ICML (2021)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.,
    Sung, Y.H., Li, Z., Duerig, T.: 通过噪声文本监督扩大视觉和视觉语言表示学习。在：ICML (2021)'
- en: '[19] Jia, X., Wu, P., Chen, L., Xie, J., He, C., Yan, J., Li, H.: Think Twice
    before Driving: Towards scalable decoders for end-to-end autonomous driving. In:
    CVPR (2023)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Jia, X., Wu, P., Chen, L., Xie, J., He, C., Yan, J., Li, H.: 驾驶前三思：面向端到端自主驾驶的可扩展解码器。
    在：CVPR (2023)'
- en: '[20] Jiang, B., Chen, S., Xu, Q., Liao, B., Chen, J., Zhou, H., Zhang, Q.,
    Liu, W., Huang, C., Wang, X.: VAD: Vectorized scene representation for efficient
    autonomous driving. arXiv:2303.12077 (2023)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Jiang, B., Chen, S., Xu, Q., Liao, B., Chen, J., Zhou, H., Zhang, Q.,
    Liu, W., Huang, C., Wang, X.: VAD：用于高效自主驾驶的向量化场景表示。arXiv:2303.12077 (2023)'
- en: '[21] Jiang, X., Li, S., Liu, Y., Wang, S., Jia, F., Wang, T., Han, L., Zhang,
    X.: Far3d: Expanding the horizon for surround-view 3d object detection. arXiv:2308.09616
    (2023)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jiang, X., Li, S., Liu, Y., Wang, S., Jia, F., Wang, T., Han, L., Zhang,
    X.: Far3d：扩展环视 3D 物体检测的视野。arXiv:2308.09616 (2023)'
- en: '[22] Kim, J., Misu, T., Chen, Y.T., Tawari, A., Canny, J.: Grounding human-to-vehicle
    advice for self-driving vehicles. In: CVPR (2019)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Kim, J., Misu, T., Chen, Y.T., Tawari, A., Canny, J.: 为自动驾驶车辆奠定人类与车辆建议的基础。在：CVPR
    (2019)'
- en: '[23] Kim, J., Rohrbach, A., Darrell, T., Canny, J., Akata, Z.: Textual explanations
    for self-driving vehicles. ECCV (2018)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Kim, J., Rohrbach, A., Darrell, T., Canny, J., Akata, Z.: 自驾驶车辆的文本解释。ECCV
    (2018)'
- en: '[24] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models. In: ICML (2023)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2：通过冻结图像编码器和大型语言模型来引导语言-图像预训练。在：ICML
    (2023)'
- en: '[25] Li, Z., Deng, H., Li, T., Huang, Y., Sima, C., Geng, X., Gao, Y., Wang,
    W., Li, Y., Lu, L.: BEVFormer++ : Improving bevformer for 3d camera-only object
    detection: 1st place solution for waymo open dataset challenge 2022 (2023)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Li, Z., Deng, H., Li, T., Huang, Y., Sima, C., Geng, X., Gao, Y., Wang,
    W., Li, Y., Lu, L.: BEVFormer++：改进 BEVFormer 用于仅通过 3D 相机进行物体检测：Waymo 开放数据集挑战赛
    2022 年的第一名解决方案 (2023)'
- en: '[26] Li, Z., Yu, Z., Lan, S., Li, J., Kautz, J., Lu, T., Alvarez, J.M.: Is
    ego status all you need for open-loop end-to-end autonomous driving? arXiv:2312.03031
    (2023)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Li, Z., Yu, Z., Lan, S., Li, J., Kautz, J., Lu, T., Alvarez, J.M.: 自闭环端到端自主驾驶仅需自我状态吗？arXiv:2312.03031
    (2023)'
- en: '[27] Lin, C.Y.: ROUGE: A package for automatic evaluation of summaries. In:
    Text Summarization Branches Out. pp. 74–81 (2004)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Lin, C.Y.: ROUGE：一个用于自动评估摘要的工具包。在：Text Summarization Branches Out。第 74–81
    页 (2004)'
- en: '[28] Lin, X., Lin, T., Pei, Z., Huang, L., Su, Z.: Sparse4D: Multi-view 3d
    object detection with sparse spatial-temporal fusion. arXiv:2211.10581 (2022)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Lin, X., Lin, T., Pei, Z., Huang, L., Su, Z.: Sparse4D：具有稀疏时空融合的多视角 3D
    物体检测。arXiv:2211.10581 (2022)'
- en: '[29] Liu, H., Teng, Y., Lu, T., Wang, H., Wang, L.: SparseBEV: High-performance
    sparse 3d object detection from multi-camera videos. In: ICCV (2023)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Liu, H., Teng, Y., Lu, T., Wang, H., Wang, L.: SparseBEV：来自多摄像头视频的高性能稀疏
    3D 物体检测。在：ICCV (2023)'
- en: '[30] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction
    tuning. arXiv:2310.03744 (2023)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Liu, H., Li, C., Li, Y., Lee, Y.J.: 通过视觉指令调整改进的基准线。arXiv:2310.03744 (2023)'
- en: '[31] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. NeurIPS
    (2023)'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Liu, H., Li, C., Wu, Q., Lee, Y.J.: 视觉指令调整。NeurIPS (2023)'
- en: '[32] Liu, Y., Wang, T., Zhang, X., Sun, J.: PETR: Position embedding transformation
    for multi-view 3d object detection. arXiv:2203.05625 (2022)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Liu, Y., Wang, T., Zhang, X., Sun, J.: PETR：用于多视角 3D 物体检测的位置嵌入变换。arXiv:2203.05625
    (2022)'
- en: '[33] Liu, Y., Yan, J., Jia, F., Li, S., Gao, Q., Wang, T., Zhang, X., Sun,
    J.: PETRv2: A unified framework for 3d perception from multi-camera images. arXiv:2206.01256
    (2022)'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Liu, Y., Yan, J., Jia, F., Li, S., Gao, Q., Wang, T., Zhang, X., Sun,
    J.: PETRv2：一个统一的多摄像头图像 3D 感知框架。arXiv:2206.01256 (2022)'
- en: '[34] Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm
    restarts. arXiv:1608.03983 (2016)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Loshchilov, I., Hutter, F.: SGDR：带有温暖重启的随机梯度下降。arXiv:1608.03983 (2016)'
- en: '[35] Ma, Y., Cui, C., Cao, X., Ye, W., Liu, P., Lu, J., Abdelraouf, A., Gupta,
    R., Han, K., Bera, A., et al.: LaMPilot: An open benchmark dataset for autonomous
    driving with language model programs. arXiv:2312.04372 (2023)'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ma, Y., Cui, C., Cao, X., Ye, W., Liu, P., Lu, J., Abdelraouf, A., Gupta,
    R., Han, K., Bera, A., 等：LaMPilot：一个用于语言模型程序的自主驾驶开源基准数据集。arXiv:2312.04372 (2023)'
- en: '[36] Malla, S., Choi, C., Dwivedi, I., Choi, J.H., Li, J.: DRAMA: Joint risk
    localization and captioning in driving. In: WACV (2023)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Malla, S., Choi, C., Dwivedi, I., Choi, J.H., Li, J.: DRAMA：驾驶中的联合风险定位与字幕生成。在：WACV
    (2023)'
- en: '[37] Mao, J., Niu, M., Jiang, C., Liang, X., Li, Y., Ye, C., Zhang, W., Li,
    Z., Yu, J., Xu, C., et al.: One million scenes for autonomous driving: Once dataset
    (2021)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Mao, J., Niu, M., Jiang, C., Liang, X., Li, Y., Ye, C., Zhang, W., Li,
    Z., Yu, J., Xu, C., 等：百万场景自动驾驶：Once 数据集 (2021)'
- en: '[38] Marcu, A.M., Chen, L., Hünermann, J., Karnsund, A., Hanotte, B., Chidananda,
    P., Nair, S., Badrinarayanan, V., Kendall, A., Shotton, J., et al.: LingoQA: Video
    question answering for autonomous driving. arXiv:2312.14115 (2023)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Marcu, A.M., Chen, L., Hünermann, J., Karnsund, A., Hanotte, B., Chidananda,
    P., Nair, S., Badrinarayanan, V., Kendall, A., Shotton, J., 等：LingoQA：自动驾驶的视频问答。arXiv:2312.14115
    (2023)'
- en: '[39] Nie, M., Peng, R., Wang, C., Cai, X., Han, J., Xu, H., Zhang, L.: Reason2Drive:
    Towards interpretable and chain-based reasoning for autonomous driving. arXiv:2312.03661
    (2023)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Nie, M., Peng, R., Wang, C., Cai, X., Han, J., Xu, H., Zhang, L.：Reason2Drive：朝向可解释和链式推理的自动驾驶。arXiv:2312.03661
    (2023)'
- en: '[40] Park, J., Xu, C., Yang, S., Keutzer, K., Kitani, K., Tomizuka, M., Zhan,
    W.: Time will tell: New outlooks and a baseline for temporal multi-view 3d object
    detection. arXiv:2210.02443 (2022)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Park, J., Xu, C., Yang, S., Keutzer, K., Kitani, K., Tomizuka, M., Zhan,
    W.：时间将会告诉我们：时间多视角3D目标检测的新展望和基准。arXiv:2210.02443 (2022)'
- en: '[41] Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary
    camera rigs by implicitly unprojecting to 3d. In: ECCV (2020)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Philion, J., Fidler, S.：Lift, splat, shoot：通过隐式投影到3D编码来自任意相机设备的图像。会议：ECCV
    (2020)'
- en: '[42] Prakash, A., Chitta, K., Geiger, A.: Multi-modal fusion transformer for
    end-to-end autonomous driving. In: CVPR (2021)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Prakash, A., Chitta, K., Geiger, A.：用于端到端自动驾驶的多模态融合变换器。会议：CVPR (2021)'
- en: '[43] Qian, T., Chen, J., Zhuo, L., Jiao, Y., Jiang, Y.G.: NuScenes-QA: A multi-modal
    visual question answering benchmark for autonomous driving scenario. arXiv:2305.14836
    (2023)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Qian, T., Chen, J., Zhuo, L., Jiao, Y., Jiang, Y.G.：《NuScenes-QA：用于自动驾驶场景的多模态视觉问答基准》。arXiv:2305.14836
    (2023)'
- en: '[44] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
    visual models from natural language supervision. In: ICML (2021)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., 等：从自然语言监督中学习可转移的视觉模型。会议：ICML (2021)'
- en: '[45] Sachdeva, E., Agarwal, N., Chundi, S., Roelofs, S., Li, J., Kochenderfer,
    M., Choi, C., Dariush, B.: Rank2Tell: A multimodal driving dataset for joint importance
    ranking and reasoning. In: WACV (2024)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Sachdeva, E., Agarwal, N., Chundi, S., Roelofs, S., Li, J., Kochenderfer,
    M., Choi, C., Dariush, B.：Rank2Tell：一个用于联合重要性排序和推理的多模态驾驶数据集。会议：WACV (2024)'
- en: '[46] Sima, C., Renz, K., Chitta, K., Chen, L., Zhang, H., Xie, C., Luo, P.,
    Geiger, A., Li, H.: DriveLM: Driving with graph visual question answering. arXiv:2312.14150
    (2023)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Sima, C., Renz, K., Chitta, K., Chen, L., Zhang, H., Xie, C., Luo, P.,
    Geiger, A., Li, H.：DriveLM：通过图视觉问答进行驾驶。arXiv:2312.14150 (2023)'
- en: '[47] Tian, X., Gu, J., Li, B., Liu, Y., Hu, C., Wang, Y., Zhan, K., Jia, P.,
    Lang, X., Zhao, H.: DriveVLM: The convergence of autonomous driving and large
    vision-language models. arXiv:2402.12289 (2024)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Tian, X., Gu, J., Li, B., Liu, Y., Hu, C., Wang, Y., Zhan, K., Jia, P.,
    Lang, X., Zhao, H.：DriveVLM：自动驾驶与大型视觉-语言模型的融合。arXiv:2402.12289 (2024)'
- en: '[48] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open
    foundation and fine-tuned chat models. arXiv:2307.09288 (2023)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等：Llama 2：开放基础和微调聊天模型。arXiv:2307.09288
    (2023)'
- en: '[49] Vedantam, R., Lawrence Zitnick, C., Parikh, D.: CIDEr: Consensus-based
    image description evaluation. In: CVPR (2015)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Vedantam, R., Lawrence Zitnick, C., Parikh, D.：CIDEr：基于共识的图像描述评估。会议：CVPR
    (2015)'
- en: '[50] Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.: Exploring object-centric
    temporal modeling for efficient multi-view 3d object detection. arXiv:2303.11926
    (2023)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.：探索以对象为中心的时间建模以提高多视角3D目标检测的效率。arXiv:2303.11926
    (2023)'
- en: '[51] Wang, T.H., Maalouf, A., Xiao, W., Ban, Y., Amini, A., Rosman, G., Karaman,
    S., Rus, D.: Drive Anywhere: Generalizable end-to-end autonomous driving with
    multi-modal foundation models. arXiv:2310.17642 (2023)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Wang, T.H., Maalouf, A., Xiao, W., Ban, Y., Amini, A., Rosman, G., Karaman,
    S., Rus, D.：Drive Anywhere：使用多模态基础模型的可泛化端到端自动驾驶。arXiv:2310.17642 (2023)'
- en: '[52] Wang, W., Xie, J., Hu, C., Zou, H., Fan, J., Tong, W., Wen, Y., Wu, S.,
    Deng, H., Li, Z., et al.: DriveMLM: Aligning multi-modal large language models
    with behavioral planning states for autonomous driving. arXiv:2312.09245 (2023)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Wang, W., Xie, J., Hu, C., Zou, H., Fan, J., Tong, W., Wen, Y., Wu, S.,
    Deng, H., Li, Z., 等：DriveMLM：将多模态大型语言模型与行为规划状态对齐以进行自动驾驶。arXiv:2312.09245 (2023)'
- en: '[53] Wang, Y., Vitor Campagnolo, G., Zhang, T., Zhao, H., Solomon, J.: DETR3D:
    3d object detection from multi-view images via 3d-to-2d queries. In: CoRL (2022)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Wang, Y., Vitor Campagnolo, G., Zhang, T., Zhao, H., Solomon, J.: DETR3D：通过
    3d-to-2d 查询从多视角图像中进行 3d 目标检测。在：CoRL (2022)'
- en: '[54] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language
    models. NeurIPS (2022)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-thought 提示在大型语言模型中引发推理。NeurIPS (2022)'
- en: '[55] Wu, D., Chang, J., Jia, F., Liu, Y., Wang, T., Shen, J.: TopoMLP: A simple
    yet strong pipeline for driving topology reasoning. arXiv:2310.06753 (2023)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Wu, D., Chang, J., Jia, F., Liu, Y., Wang, T., Shen, J.: TopoMLP：一个简单但强大的驾驶拓扑推理管道。arXiv:2310.06753
    (2023)'
- en: '[56] Wu, D., Han, W., Wang, T., Liu, Y., Zhang, X., Shen, J.: Language prompt
    for autonomous driving. arXiv:2309.04379 (2023)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Wu, D., Han, W., Wang, T., Liu, Y., Zhang, X., Shen, J.: 自动驾驶的语言提示。arXiv:2309.04379
    (2023)'
- en: '[57] Wu, P., Jia, X., Chen, L., Yan, J., Li, H., Qiao, Y.: Trajectory-guided
    control prediction for end-to-end autonomous driving: A simple yet strong baseline.
    In: NeurIPS (2022)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Wu, P., Jia, X., Chen, L., Yan, J., Li, H., Qiao, Y.: 基于轨迹引导的端到端自动驾驶控制预测：一个简单但强大的基线。在：NeurIPS
    (2022)'
- en: '[58] Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.K., Li, Z., Zhao,
    H.: DriveGPT4: Interpretable end-to-end autonomous driving via large language
    model. arXiv:2310.01412 (2023)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.K., Li, Z., Zhao,
    H.: DriveGPT4：通过大型语言模型实现可解释的端到端自动驾驶。arXiv:2310.01412 (2023)'
- en: '[59] Yuan, T., Liu, Y., Wang, Y., Wang, Y., Zhao, H.: StreamMapNet: Streaming
    mapping network for vectorized online hd map construction. arXiv:2308.12570 (2023)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Yuan, T., Liu, Y., Wang, Y., Wang, Y., Zhao, H.: StreamMapNet：用于矢量化在线高清地图构建的流式映射网络。arXiv:2308.12570
    (2023)'
- en: '[60] Zhai, J.T., Feng, Z., Du, J., Mao, Y., Liu, J.J., Tan, Z., Zhang, Y.,
    Ye, X., Wang, J.: Rethinking the open-loop evaluation of end-to-end autonomous
    driving in nuscenes. arXiv:2305.10430 (2023)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Zhai, J.T., Feng, Z., Du, J., Mao, Y., Liu, J.J., Tan, Z., Zhang, Y.,
    Ye, X., Wang, J.: 重新思考在 nuscenes 中端到端自动驾驶的开环评估。arXiv:2305.10430 (2023)'
- en: '[61] Zhang, Z., Liniger, A., Dai, D., Yu, F., Van Gool, L.: End-to-end urban
    driving by imitating a reinforcement learning coach. In: ICCV (2021)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Zhang, Z., Liniger, A., Dai, D., Yu, F., Van Gool, L.: 通过模仿强化学习教练实现端到端城市驾驶。在：ICCV
    (2021)'
- en: '[62] Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart,
    P., Welker, S., Wahid, A., et al.: RT-2: Vision-language-action models transfer
    web knowledge to robotic control. In: CoRL (2023)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart,
    P., Welker, S., Wahid, A., et al.: RT-2：视觉-语言-行动模型将网络知识转移到机器人控制中。在：CoRL (2023)'
