- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型能否为事实核查生成可信的解释？通过多代理辩论实现可信的可解释事实核查
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.07401](https://ar5iv.labs.arxiv.org/html/2402.07401)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.07401](https://ar5iv.labs.arxiv.org/html/2402.07401)
- en: 'Kyungha Kim^($\heartsuit$)¹¹footnotemark: 1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kyungha Kim^($\heartsuit$)¹¹脚注标记：1
- en: Hou Pong Chan^($\spadesuit$)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hou Pong Chan^($\spadesuit$)
- en: ^($\heartsuit$)University of Illinois Urbana-Champaign
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\heartsuit$)伊利诺伊大学厄本那-香槟分校
- en: ^($\spadesuit$)Northwestern University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\spadesuit$)西北大学
- en: ^($\heartsuit$){kyungha2, slee677, khhuang3, hengji}@illinois.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\heartsuit$){kyungha2, slee677, khhuang3, hengji}@illinois.edu
- en: ^($\spadesuit$)manling.li@northwestern.edu   Equal contribution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\spadesuit$)manling.li@northwestern.edu   贡献相等。
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Fact-checking research has extensively explored verification but less so the
    generation of natural-language explanations, crucial for user trust. While Large
    Language Models (LLMs) excel in text generation, their capability for producing
    faithful explanations in fact-checking remains underexamined. Our study investigates
    LLMs’ ability to generate such explanations, finding that zero-shot prompts often
    result in unfaithfulness. To address these challenges, we propose the Multi-Agent
    Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse
    roles in an iterative refining process aimed at enhancing faithfulness in generated
    explanations. MADR ensures that the final explanation undergoes rigorous validation,
    significantly reducing the likelihood of unfaithful elements and aligning closely
    with the provided evidence. Experimental results demonstrate that MADR significantly
    improves the faithfulness of LLM-generated explanations to the evidence, advancing
    the credibility and trustworthiness of these explanations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 事实核查研究广泛探讨了验证过程，但对于自然语言解释的生成却涉及较少，而这对用户信任至关重要。虽然大型语言模型（LLMs）在文本生成方面表现出色，但它们在事实核查中生成可信解释的能力仍未被充分研究。我们的研究调查了LLMs生成此类解释的能力，发现零样本提示通常会导致解释不够可信。为了解决这些挑战，我们提出了多代理辩论精炼（MADR）框架，通过将多个LLMs作为具有不同角色的代理，进行迭代精炼过程，旨在提高生成解释的可信度。MADR确保最终解释经过严格验证，显著降低了不可信元素的可能性，并与提供的证据紧密对齐。实验结果表明，MADR显著提高了LLM生成解释的可信度，提升了这些解释的可靠性和信任度。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In the digital age, swiftly spreading misinformation necessitates not only the
    verification of claims but also the provision of clear explanations for these
    verifications. Such explanations are crucial for building trust within the audience,
    as lack of them often leads to distrust in fact-checking results Guo et al. ([2022](#bib.bib7)).
    Moreover, explanation generation becomes even more critical in multi-hop fact-checking,
    where complex reasoning across multiple evidence pieces is required to assess
    a claim’s veracity Reddy et al. ([2023](#bib.bib19)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字时代，迅速传播的虚假信息不仅需要对声明进行验证，还需要提供清晰的解释。这样的解释对于在受众中建立信任至关重要，因为缺乏解释往往会导致对事实核查结果的不信任
    Guo et al. ([2022](#bib.bib7))。此外，在多跳事实核查中，生成解释变得更加重要，因为这需要在多个证据片段之间进行复杂的推理，以评估声明的真实性
    Reddy et al. ([2023](#bib.bib19))。
- en: Despite the adeptness of Large Language Models (LLMs) in generating diverse
    texts, their capacity for crafting faithful¹¹1Faithfulness refers to the factual
    consistency between the explanation and the given evidence Huang et al. ([2023a](#bib.bib8)).
    explanations for fact-checking remains underexplored. Faithfulness is crucial;
    explanations that misrepresent evidence could exacerbate misinformation, posing
    a significant challenge. Thus, enhancing the faithfulness of generated explanations
    in fact-checking is an urgent, unresolved issue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在生成多样文本方面表现出色，但它们在为事实核查生成可信¹¹1可信度指的是解释与给定证据之间的事实一致性 Huang et al.
    ([2023a](#bib.bib8))的解释方面仍未得到充分探讨。可信度至关重要；错误描述证据的解释可能加剧虚假信息问题，带来重大挑战。因此，提高事实核查中生成解释的可信度是一个紧迫且未解决的问题。
- en: 'Our first research question asks: can LLMs generate faithful explanations for
    fact-checking in a zero-shot prompting setup? To facilitate analysis, we define
    a novel typology of common errors and unfaithfulness issues that arise in LLM-generated
    explanations. We conduct extensive experiments prompting ChatGPT OpenAI ([2023c](#bib.bib17))
    to explain fact checks from multiple sources. Our findings reveal that zero-shot
    prompting LLMs often fails to yield faithful explanations. 80% of the generated
    explanations include hallucinated details ([Section 5](#S5 "5 Results ‣ Can LLMs
    Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate")).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个研究问题是：LLM能否在零-shot提示设置下生成可信的解释用于事实检查？为了便于分析，我们定义了一种新型的常见错误和不可信性问题的类型学，这些问题在LLM生成的解释中出现。我们进行了大量实验，提示ChatGPT
    OpenAI（[2023c](#bib.bib17)）解释来自多个来源的事实检查。我们的发现揭示了零-shot提示的LLM通常未能生成可信的解释。生成的解释中有80%包含了虚构的细节（[第5节](#S5
    "5 结果 ‣ LLM能否为事实检查提供可信的解释？通过多智能体辩论实现可信的可解释事实检查")）。
- en: 'This leads to our second research question: how to address the unfaithfulness
    issues in LLM-generated explanations? We propose the Multi-Agent Debate Refinement
    (MADR) framework that uses multiple LLMs as agents to provide feedback for iterative
    refinement to produce faithful explanations ([Section 3](#S3 "3 Methodology ‣
    Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate")). The goal is to mitigate unfaithfulness
    and steer the LLM-generated texts towards true rationales. Experimental results
    show that MADR significantly improves faithfulness upon baselines, demonstrating
    the effectiveness of our approach.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了我们的第二个研究问题：如何解决LLM生成的解释中的不可信问题？我们提出了多智能体辩论优化（MADR）框架，该框架使用多个LLM作为智能体提供反馈，进行迭代优化以生成可信的解释（[第3节](#S3
    "3 方法 ‣ LLM能否为事实检查提供可信的解释？通过多智能体辩论实现可信的可解释事实检查")）。目标是减轻不可信性，并将LLM生成的文本引导到真实的推理上。实验结果表明，MADR在基线之上显著提高了可信度，展示了我们方法的有效性。
- en: '| Example |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 示例 |'
- en: '| Claim: Says Jeff Foxworthy wrote a list of examples explaining how "you might
    live in a nation that was founded by geniuses but is run by idiots." |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 主张：杰夫·福克斯沃西（Jeff Foxworthy）写了一份例子清单，解释了“你可能生活在一个由天才创立但由傻瓜管理的国家。” |'
- en: '| Evidence: … Foxworthy is famous for his "You might be a redneck if" jokes
    , but searching online, we couldn’t find any credible evidence that he penned
    this list that touches on abortion, Muslims and federal debt. Rather, we found
    users on web forums crediting someone named Fritz Edmunds with the list. Snopes,
    which fact-checked this claim back in 2013, also noted that "the original compiler
    of this appears to be Fritz Edmunds, who posted it to his Politically True blog
    back in Feb. 3, 2013 … |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 证据：… 福克斯沃西以其“你可能是红脖子”笑话而闻名，但在网上搜索时，我们找不到任何可信的证据证明他编写了这份涉及堕胎、穆斯林和联邦债务的清单。相反，我们发现网络论坛上的用户将这份清单归功于一个名叫弗里茨·艾德蒙兹的人。斯诺普斯（Snopes），在2013年对此声明进行了事实检查，也指出“这份清单的原始编制者似乎是弗里茨·艾德蒙兹，他在2013年2月3日将其发布到他的‘政治真实’博客上……
    |'
- en: '| Error Type & Explanation |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 错误类型与解释 |'
- en: '| Intrinsic Entity Error: The generated explanation misrepresents named entities,
    quantities, dates, or other surface realizations from the given source. E.g. Fritz
    Foxworthy was credited on a web forum with the list. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 内在实体错误：生成的解释错误地表示了源中的命名实体、数量、日期或其他表面实现。例如，弗里茨·福克斯沃西（Fritz Foxworthy）在网络论坛上被认可为该清单的作者。
    |'
- en: '| Extrinsic Entity Error: The generated explanation includes new entities that
    are not present in the given source. E.g. Elon Musk was credited on a web forum
    with the list. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 外在实体错误：生成的解释包含了源中不存在的新实体。例如，埃隆·马斯克（Elon Musk）在网络论坛上被认可为该清单的作者。 |'
- en: '| Intrinsic Event Error: The generated explanation misrepresents events mentioned
    in the source. E.g. They couldn’t find any credible evidence that Fritz Edmunds
    was credited on a web forum. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 内在事件错误：生成的解释误传了源中提到的事件。例如，他们找不到任何可信的证据证明弗里茨·艾德蒙兹（Fritz Edmunds）在某个网络论坛上获得了认可。
    |'
- en: '| Extrinsic Event Error: The generated explanation include new events that
    are not present in the given source. E.g. Foxworthy found that Fritz Edmunds made
    the “You might be a redneck if” jokes. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 外在事件错误：生成的解释包含了源中不存在的新事件。例如，福克斯沃西发现弗里茨·艾德蒙兹制作了“你可能是红脖子”的笑话。 |'
- en: '| Intrinsic Noun-Phrase Error: The explanation mistakenly represents the noun
    phrases in the given source like miscombining modifiers combined with one entity
    to another entity. E.g. They found the original user on web forums crediting someone
    named Fritz Edmunds. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 内在名词短语错误：解释错误地表示了给定来源中的名词短语，如将一个实体的修饰词误组合到另一个实体上。例如：他们在网络论坛上发现了原始用户，给某个名为弗里茨·爱德蒙兹的人记了功。
    |'
- en: '| Extrinsic Noun-Phrase Error: The explanation mistakenly represents new noun
    phrases that are not present in the given source like miscombining modifiers not
    presented in the source to entity. E.g. They found a mysterious user on web forums
    crediting someone named Fritz Edmunds. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 外在名词短语错误：解释错误地表示了在给定来源中不存在的新名词短语，如将未出现在来源中的修饰词误组合到实体上。例如：他们在网络论坛上发现了一个神秘用户，给某个名为弗里茨·爱德蒙兹的人记了功。
    |'
- en: '| Reasoning Coherence Error: There are logical flaws in the flow of reasoning
    within the generated explanation, leading to a lack of coherence or weak support
    for the claim. E.g. While they were searching online, they couldn’t find any credible
    evidence that he penned this list that touches on abortion. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 推理一致性错误：生成的解释中存在逻辑缺陷，导致推理流存在不一致性或对论点支持不足。例如：尽管他们在网上搜索，但找不到任何可信的证据表明他编写了涉及堕胎的这份名单。
    |'
- en: '| Overgeneralization Error: The generated explanation makes sweeping statements
    or draws conclusions that go beyond the evidence provided. E.g. Fritz Emunds is
    the one who spreaded the rumor and put the blame on Foxworthy. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 过度概括错误：生成的解释做出了笼统的陈述或得出了超出证据范围的结论。例如：弗里茨·爱德蒙兹是传播谣言并把责任推给福克斯沃西的人。 |'
- en: '| Irrelevant Evidence Error: The generated explanation includes evidence that
    is not directly related to the claim, leading to confusion and lack of support
    for the main argument. E.g. … Foxworthy is famous for his "You might be a redneck
    if" jokes. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 无关证据错误：生成的解释包含与主张无直接关系的证据，导致混淆和对主要论点支持不足。例如：……福克斯沃西因其“你可能是红脖子”的笑话而闻名。 |'
- en: 'Table 1: An illustration of error typology using using a sample data from PolitiHop
    Ostrowski et al. ([2021](#bib.bib18)). The errors in the sample summaries are
    in red color and italicized.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用PolitiHop Ostrowski等人（[2021](#bib.bib18)）的示例数据进行的错误分类示意图。样本摘要中的错误以红色显示并用斜体标出。
- en: 'Our contributions are summarized as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the first study of LLMs’ ability to produce faithful fact-checking
    explanations.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首次研究了LLM生成忠实的事实核查解释的能力。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present Multi-Agent Debate Refinement, an effective framework to produce
    faithful explanations based on iterative debating among LLMs.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了多主体辩论优化，这是一种有效的框架，通过LLM间的迭代辩论生成忠实的解释。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our correlation analysis reveals the most suitable LLM-based evaluation protocol
    for this task.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的相关性分析揭示了最适合此任务的基于LLM的评估协议。
- en: 2 Typology
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 分类法
- en: In our analysis of explanations generated by LLMs, we have introduced a novel
    typology encompassing a range of error categories, as shown in [Table 1](#S1.T1
    "In 1 Introduction ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). The classification
    of intrinsic and extrinsic errors within the domains of Entity-Related, Event-Related,
    and Noun-phrase Errors draws inspiration from relevant studies in other domains
    Goyal and Durrett ([2021](#bib.bib6)); Huang et al. ([2023c](#bib.bib10)). We
    have incorporated additional context-specific error types, enriching the overall
    typology.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对LLM生成的解释进行分析时，我们引入了一种新颖的分类法，涵盖了一系列错误类别，如[表1](#S1.T1 "在1 引言 ‣ LLM能否提供忠实的事实核查解释？通过多主体辩论实现忠实的可解释性事实核查")所示。实体相关、事件相关和名词短语错误领域中的内在和外在错误的分类受到了其他领域相关研究的启发，如Goyal和Durrett（[2021](#bib.bib6)）；Huang等（[2023c](#bib.bib10)）。我们还加入了额外的上下文特定错误类型，丰富了整体分类法。
- en: 3 Methodology
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: Zero-shot prompting LLMs often produce unfaithful explanations which contain
    multiple errors. In the early stage of our experiments, we incorporate an iterative
    refinement paradigm for improving their faithfulness. However, we found that self-refinement
    Madaan et al. ([2023](#bib.bib14)) alone was insufficient for faithfulness enhancement
    (see [Table 3](#S5.T3 "In 5 Results ‣ Can LLMs Produce Faithful Explanations For
    Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate")),
    as imprecise feedback tended to guide misguided refinements of the explanation.
    This underscores the pivotal role of precise feedback for efficient refinement
    by the LLM Wang et al. ([2023](#bib.bib22)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 提示的LLMs通常会产生不准确的解释，其中包含多个错误。在我们实验的早期阶段，我们引入了一个迭代改进范式来提高它们的准确性。然而，我们发现自我改进的Madaan等人（[2023](#bib.bib14)）单独使用不足以提高准确性（参见
    [表3](#S5.T3 "在 5 结果 ‣ LLMs 能否提供可信的解释以进行事实检查？通过多智能体辩论实现可信的可解释性事实检查")），因为不准确的反馈往往会引导对解释的错误改进。这凸显了精准反馈在LLM高效改进中的关键作用，Wang等人（[2023](#bib.bib22)）。
- en: '![Refer to caption](img/e6eab3a02ed452aa3b90354dc199ef0c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e6eab3a02ed452aa3b90354dc199ef0c.png)'
- en: 'Figure 1: An overview of MADR.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：MADR的概述。
- en: Thus, we propose Multi-Agent Debate Refinement (MADR), inspired by a debate-based
    methodology Du et al. ([2023](#bib.bib4)). While Du et al. ([2023](#bib.bib4))
    focus on refining explanations during a debate, MADR utilizes the debate for generating
    feedback to be employed in subsequent refinement stages. Our method offers several
    advantages. First, compared to directly refining explanations during a debate,
    MADR facilitates a dynamic and iterative feedback loop, enhancing the identification
    of errors. Secondly, it ensures more accurate feedback, reducing the likelihood
    of misguided refinements and ultimately enhancing overall faithfulness. Thirdly,
    this approach prompts bidirectional thinking within the LLM, enabling it to analyze
    explanations both with and without knowledge of predefined error types, fostering
    an explicit rationale.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们提出了多智能体辩论改进（MADR），灵感来源于辩论方法 Du 等人（[2023](#bib.bib4)）。虽然 Du 等人（[2023](#bib.bib4)）专注于在辩论中改进解释，但
    MADR 利用辩论生成的反馈用于后续的改进阶段。我们的方法提供了几个优点。首先，与直接在辩论中改进解释相比，MADR 促进了动态和迭代的反馈循环，提高了错误识别的效率。其次，它确保了更准确的反馈，减少了错误改进的可能性，并最终提高了整体准确性。第三，这种方法促使LLM进行双向思考，使其能够分析有和没有预定义错误类型的解释，从而培养明确的推理。
- en: 'The process of MADR is outlined in [Algorithm 1](#alg1 "In Appendix B MADR
    Details ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful
    Explainable Fact-Checking via Multi-Agent Debate") and depicted in [Figure 1](#S3.F1
    "In 3 Methodology ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). MADR employs
    multiple agents to identify errors and engage them in a debate until a consensus
    is reached on the debate. Four total roles are assigned to each agent: two serve
    as Debaters, one as a Judge, and one as a Refiner.An initial explanation $E$ identifies
    each Debater (lines 2-3). Distinctive instructions with varying goals are provided
    to the Debaters: Debater 1 identifies errors based on a predefined error typology
    (see [Section 2](#S2 "2 Typology ‣ Can LLMs Produce Faithful Explanations For
    Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate")),
    while Debater 2 focuses on potential errors that may affect the explanation’s
    faithfulness, without relying on the error typology (refer to [Table 8](#A3.T8
    "In Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate") and [Table 9](#A3.T9
    "In Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate") for prompt
    specifics). This setup ensures that MADR promotes the identification of errors
    that might be overlooked by either party.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: MADR的过程在[算法1](#alg1 "附录B MADR详情 ‣ LLM是否能产生可靠的解释？通过多智能体辩论实现可靠的可解释事实核查")中概述，并在[图1](#S3.F1
    "第3章方法 ‣ LLM是否能产生可靠的解释？通过多智能体辩论实现可靠的可解释事实核查")中展示。MADR使用多个代理来识别错误，并进行辩论直到达成共识。每个代理被分配四个角色：两个是辩手，一个是裁判，一个是修正者。初始解释$E$识别每个辩手（第2-3行）。为辩手提供了具有不同目标的特殊指令：辩手1根据预定义的错误分类（见[第2节](#S2
    "2 分类 ‣ LLM是否能产生可靠的解释？通过多智能体辩论实现可靠的可解释事实核查")）识别错误，而辩手2则关注可能影响解释可信度的潜在错误，而不依赖于错误分类（参见[表8](#A3.T8
    "附录C 提示 ‣ LLM是否能产生可靠的解释？通过多智能体辩论实现可靠的可解释事实核查")和[表9](#A3.T9 "附录C 提示 ‣ LLM是否能产生可靠的解释？通过多智能体辩论实现可靠的可解释事实核查")以获取提示细节）。这种设置确保MADR能够识别可能被任何一方忽略的错误。
- en: Next, in the $i$). They refine their feedback by adding any missed elements
    and removing errors (lines 9-10).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在$i$中。他们通过添加任何遗漏的元素和纠正错误（第9-10行）来完善反馈。
- en: To ensure the most accurate feedback, the two Debaters continue their discussion
    until they reach a mutual agreement on the feedback. During the $i$, the debate
    stops (lines 6-7). Finally, we concatenate the final feedback from both Debaters
    and feed it to the refiner to refine its explanation using the concatenated feedback
    (line 13). An example of the outputs from MADR is shown in [Table 11](#A3.T11
    "In Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). Additionally,
    to prevent endless debates, we set a fixed number of iterations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保最准确的反馈，两位辩手会继续讨论，直到他们就反馈达成一致。在$i$期间，辩论会停止（第6-7行）。最后，我们将两位辩手的最终反馈合并，并将其提供给修正者，以便使用合并的反馈来修正解释（第13行）。MADR的输出示例见[表11](#A3.T11
    "附录C 提示 ‣ LLM是否能产生可靠的解释？通过多智能体辩论实现可靠的可解释事实核查")。此外，为了防止无休止的辩论，我们设置了固定的迭代次数。
- en: 4 Experimental Settings
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: Dataset and Metric
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和指标
- en: Experiments are conducted on the PolitiHop multi-hop fact-checking dataset Ostrowski
    et al. ([2021](#bib.bib18)). PolitiHop consists of 445 test set instances, where
    each instance contains a claim and multiple pieces of evidence. The veracity of
    a claim can only be determined by reasoning across multiple pieces of evidence
    and the claim. For the evaluation metric, we use G-Eval Liu et al. ([2023](#bib.bib12))
    with GPT-4 Turbo OpenAI ([2023b](#bib.bib16)) to assess whether the generated
    explanation is consistent with the evidence. Following Huang et al. ([2023b](#bib.bib9)),
    we adopted 4 evaluation protocols based on G-Eval which vary in granularity, ranging
    from sentence-level to document-level assessments, and in the application of our
    error typology, validating the effectiveness of the error typology in assisting
    automatic evaluation. The prompt templates are shown in [Appendix C](#A3 "Appendix
    C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards
    Faithful Explainable Fact-Checking via Multi-Agent Debate").
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是在 PolitiHop 多跳事实核查数据集 Ostrowski 等人（[2021](#bib.bib18)）上进行的。PolitiHop 包含 445
    个测试集实例，每个实例包含一个声明和多条证据。声明的真实性只能通过对多条证据和声明进行推理来确定。对于评估指标，我们使用 G-Eval Liu 等人（[2023](#bib.bib12)）和
    GPT-4 Turbo OpenAI（[2023b](#bib.bib16)）来评估生成的解释是否与证据一致。遵循 Huang 等人（[2023b](#bib.bib9)），我们基于
    G-Eval 采用了 4 种评估协议，这些协议在粒度上从句子级别到文档级别进行评估，并在错误类型学的应用上进行验证，以检验错误类型学在辅助自动评估中的有效性。提示模板显示在
    [附录 C](#A3 "附录 C 提示 ‣ LLM 是否能为事实核查生成忠实的解释？通过多代理辩论实现忠实可解释的事实核查") 中。
- en: Baselines
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: We compare MADR with the following competitive methods. Zero-shot prompts an
    LLM to directly output an explanation given the input claim and evidence. CoT
    asks LLMs to generate the reasoning process before producing the final output.
    Self-Refine Madaan et al. ([2023](#bib.bib14)) generates an initial explanation
    and then iteratively refines the explanation with one agent. We conduct experiments
    by using GPT-3.5-Turbo OpenAI ([2023a](#bib.bib15)) to generate explanations across
    all experiments for fair comparisons. The prompts for these approaches are displayed
    in [Appendix C](#A3 "Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations
    For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent
    Debate"). The case study using Self-Refine is in Table [10](#A3.T10 "Table 10
    ‣ Appendix C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate").
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 MADR 与以下竞争方法进行比较。Zero-shot 直接让 LLM 根据输入声明和证据生成解释。CoT 让 LLM 在产生最终输出之前生成推理过程。Self-Refine
    Madaan 等人（[2023](#bib.bib14)）生成初步解释，然后通过一个代理迭代地细化解释。我们使用 GPT-3.5-Turbo OpenAI（[2023a](#bib.bib15)）进行所有实验，以确保公平比较。这些方法的提示显示在
    [附录 C](#A3 "附录 C 提示 ‣ LLM 是否能为事实核查生成忠实的解释？通过多代理辩论实现忠实可解释的事实核查") 中。使用 Self-Refine
    的案例研究见表 [10](#A3.T10 "表 10 ‣ 附录 C 提示 ‣ LLM 是否能为事实核查生成忠实的解释？通过多代理辩论实现忠实可解释的事实核查")。
- en: 5 Results
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个结果
- en: Granularity$\rightarrow\leavevmode\nobreak\ $ Zero-shot 4.87 4.84 4.70 4.92
    CoT 4.86 4.91 4.76 4.96 Self-Refine 4.70 4.86 4.89 4.81 MADR (ours) 4.82 4.99
    4.88 4.97
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 粒度$\rightarrow\leavevmode\nobreak\ $ Zero-shot 4.87 4.84 4.70 4.92 CoT 4.86
    4.91 4.76 4.96 Self-Refine 4.70 4.86 4.89 4.81 MADR（我们的方法） 4.82 4.99 4.88 4.97
- en: 'Table 2: Faithfulness evaluation on PolitiHop test set. Scores are computed
    using G-Eval with evaluation protocols of varying granularity and application
    of our error typology. The best score per column is bolded.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 PolitiHop 测试集上的忠实度评估。分数是使用 G-Eval 计算的，采用不同粒度的评估协议和我们错误类型学的应用。每列的最佳分数已加粗。
- en: 'Method Faithful Explanations (%) # Errors Zero-shot 20.0 25 CoT 5.0 42 Self-Refine
    20.0 32 MADR (ours) 30.0 17'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 忠实解释（%） 错误数量 Zero-shot 20.0 25 CoT 5.0 42 Self-Refine 20.0 32 MADR（我们的方法）
    30.0 17
- en: 'Table 3: Human evaluation results on 20 samples from PolitiHop. MADR produces
    the most faithful explanation compared to baselines.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：对 PolitiHop 中 20 个样本的人类评估结果。与基线方法相比，MADR 生成了最忠实的解释。
- en: The main results are summarized in [Table 2](#S5.T2 "In 5 Results ‣ Can LLMs
    Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"). MADR achieves the best faithfulness scores
    on two out of the four evaluation protocols, indicating its effectiveness in producing
    faithful explanations. To further validate the effectiveness of our method, we
    conduct human evaluations via Amazon Mechanical Turk, aiming to quantify the portion
    of faithful explanations and the total error count. Annotators were presented
    with our error typology and were tasked to identify the presence of each error
    type within individual sentences. The results of human evaluations are shown in
    [Table 3](#S5.T3 "In 5 Results ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"). We have the
    following observations. First, using simple prompting methods, such as zero-shot
    or CoT, LLMs often produce unfaithful explanations for fact-checking. This highlights
    the challenge of generating faithful explanations for LLMs in complex fact-checking
    scenarios, such as PolitiHop, which requires reasoning through multiple pieces
    of evidence. Second, despite the high faithfulness scores suggested by automatic
    evaluation (approaching the maximum score of 5) in [Table 2](#S5.T2 "In 5 Results
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"), human evaluators frequently deemed the
    LLM-generated explanations unfaithful, as per [Table 3](#S5.T3 "In 5 Results ‣
    Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"). This discrepancy suggests that even the
    most advanced LLM, GPT-4 Turbo, fails to reliably judge the faithfulness of generated
    explanations for fact-checking.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结果总结见[表2](#S5.T2 "在第5部分结果 ‣ LLMs能否提供真实的解释以进行事实检查？通过多智能体辩论实现真实可解释的事实检查")。MADR在四个评估协议中的两个上达到了最佳的真实性分数，表明其在生成真实解释方面的有效性。为了进一步验证我们方法的有效性，我们通过Amazon
    Mechanical Turk进行人工评估，旨在量化真实解释的比例和总错误数。评审员被提供了我们的错误分类，并被要求识别每种错误类型在单句中的存在。人工评估结果见[表3](#S5.T3
    "在第5部分结果 ‣ LLMs能否提供真实的解释以进行事实检查？通过多智能体辩论实现真实可解释的事实检查")。我们有以下观察结果。首先，使用简单的提示方法，如零样本或CoT，LLMs往往会生成不真实的解释进行事实检查。这突显了在复杂的事实检查场景中，如PolitiHop，生成真实解释的挑战，这需要通过多个证据进行推理。其次，尽管[表2](#S5.T2
    "在第5部分结果 ‣ LLMs能否提供真实的解释以进行事实检查？通过多智能体辩论实现真实可解释的事实检查")中的自动评估显示了较高的真实性分数（接近最大分数5），人工评估员经常认为LLM生成的解释不真实，参见[表3](#S5.T3
    "在第5部分结果 ‣ LLMs能否提供真实的解释以进行事实检查？通过多智能体辩论实现真实可解释的事实检查")。这种差异表明，即使是最先进的LLM，GPT-4
    Turbo，也无法可靠地判断生成的解释在事实检查中的真实性。
- en: To pinpoint the most effective LLM-based evaluation strategy for future research,
    we performed a correlation analysis, correlating human evaluations with automatic
    metrics using Kendall’s Tau (variant c). According to [Table 4](#S5.T4 "In 5 Results
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"), a granular evaluation aligns better with
    human judgments, and incorporating our error typology into automatic evaluations
    enhances the quality of LLM assessments. Details on human evaluation methodology
    are provided in [Appendix A](#A1 "Appendix A Human Evaluation Details ‣ Can LLMs
    Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出未来研究中最有效的LLM基于的评估策略，我们进行了相关性分析，将人工评估与使用Kendall’s Tau（变体c）的自动指标相关联。根据[表4](#S5.T4
    "在第5部分结果 ‣ LLMs能否提供真实的解释以进行事实检查？通过多智能体辩论实现真实可解释的事实检查")，细致的评估与人工判断更为一致，将我们的错误分类纳入自动评估可以提高LLM评估的质量。有关人工评估方法的详细信息见[附录A](#A1
    "附录A 人工评估细节 ‣ LLMs能否提供真实的解释以进行事实检查？通过多智能体辩论实现真实可解释的事实检查")。
- en: Furthermore, a case study showcased in [Table 11](#A3.T11 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate") highlights the superiority of MADR over
    self-refinement, by demonstrating that MADR allows Debaters to identify and correct
    errors missed during self-refinement, leading to more accurate explanations. In
    contrast, the Self-Refine approach, as shown in [Table 10](#A3.T10 "In Appendix
    C Prompts ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards
    Faithful Explainable Fact-Checking via Multi-Agent Debate"), fails to produce
    a faithful explanation, emphasizing the advantage of employing multiple perspectives
    with MADR for error identification and explanation validation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，案例研究展示在 [表 11](#A3.T11 "在附录 C 提示 ‣ LLMs 能否生成可信的事实核查解释？通过多代理辩论实现可信的可解释事实核查")
    中突显了 MADR 相比自我细化的优势，表明 MADR 允许辩论者识别并纠正自我细化过程中遗漏的错误，从而产生更准确的解释。相反，[表 10](#A3.T10
    "在附录 C 提示 ‣ LLMs 能否生成可信的事实核查解释？通过多代理辩论实现可信的可解释事实核查") 显示的自我细化方法未能生成可信的解释，强调了使用
    MADR 多角度进行错误识别和解释验证的优势。
- en: Evaluation Protocol Kendall’s Tau Document-level w/o Typology 0.008 Document-level
    w/ Typology 0.128 Sentence-level w/o Typology 0.105 Sentence-level w/ Typology
    0.150
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 评估协议 Kendall’s Tau 文档级别 无类型 0.008 文档级别 有类型 0.128 句子级别 无类型 0.105 句子级别 有类型 0.150
- en: 'Table 4: Correlation between evaluation protocols and human judgments on the
    PolitiHop dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：评估协议与 PolitiHop 数据集上人类判断的相关性。
- en: 6 Related Work
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Early approaches to producing explanations for fact-checks can largely be categorized
    into logic-based methods Gad-Elrab et al. ([2019](#bib.bib5)); Ahmadi et al. ([2019](#bib.bib1))
    and attention-based methods Shu et al. ([2019](#bib.bib21)); Lu and Li ([2020](#bib.bib13)).
    Recent work generates natural-language explanations using abstractive Kotonya
    and Toni ([2020](#bib.bib11)) or extractive Atanasova et al. ([2020](#bib.bib2))
    approaches. A very recent study benchmarks the ability of these models to generate
    explanations for fact-checks Russo et al. ([2023](#bib.bib20)). Our study complements
    this work by presenting the first empirical analysis on LLMs’ ability to generate
    fact-checking explanations and propose a method to enhance its faithfulness.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于生成事实核查解释的方法大致可以分为基于逻辑的方法，如 Gad-Elrab 等人 ([2019](#bib.bib5)) 和 Ahmadi 等人
    ([2019](#bib.bib1))，以及基于注意力的方法，如 Shu 等人 ([2019](#bib.bib21)) 和 Lu 与 Li ([2020](#bib.bib13))。最近的工作使用抽象的
    Kotonya 和 Toni ([2020](#bib.bib11)) 或提取的 Atanasova 等人 ([2020](#bib.bib2)) 方法生成自然语言解释。最近的一项研究基准测试了这些模型生成事实核查解释的能力，如
    Russo 等人 ([2023](#bib.bib20))。我们的研究通过提供对 LLMs 生成事实核查解释能力的首次实证分析，并提出了一种增强其可信度的方法，来补充这项工作。
- en: 7 Conclusion
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This paper empirically demonstrates that LLMs often produce unfaithful explanations
    for fact-checks. We introduce the Multi-Agent Debate Refinement (MADR) framework,
    which utilizes multiple LLM agents to iteratively debate and refine explanations,
    significantly enhancing their faithfulness as evidenced by both automatic and
    human evaluations. Our results underscore the efficacy of multi-agent debate in
    mitigating LLMs’ unfaithfulness. Additionally, we reveal that LLMs cannot reliably
    assess the faithfulness of the generated explanations and discover the most suitable
    evaluation protocols for LLM-based automatic evaluation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本文实证表明 LLMs 经常生成不可信的事实核查解释。我们引入了多代理辩论细化 (MADR) 框架，该框架利用多个 LLM 代理进行迭代辩论和细化解释，显著提高了它们的可信度，经过自动评估和人工评估都得到证实。我们的结果强调了多代理辩论在减轻
    LLMs 不可信度方面的有效性。此外，我们发现 LLMs 无法可靠地评估生成解释的可信度，并找出最合适的 LLM 基于自动评估的评估协议。
- en: 8 Ethical Considerations
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 伦理考量
- en: LLMs trained on internet data often show biases, but this focus mainly applies
    to data and models reflecting the culture of English-speaking communities. However,
    detailed reviews of model outputs for the PolitiHop dataset have found no signs
    of biases concerning gender, age, race, or other socioeconomic elements.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 训练于互联网数据上的 LLMs 通常表现出偏见，但这种关注主要适用于反映英语文化社区的数据和模型。然而，对 PolitiHop 数据集模型输出的详细审查发现，关于性别、年龄、种族或其他社会经济元素的偏见迹象并不存在。
- en: 9 Limitations
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 限制
- en: Our study did not thoroughly investigate the sensitivity of various systems
    to changes in input prompts. It is recognized that the effectiveness of numerous
    natural language processing tasks can significantly depend on how input prompts
    are designed. By not conducting a comprehensive analysis on prompt sensitivity,
    we acknowledge the possibility that different prompts might elicit a wide range
    of responses that we have not explored, potentially limiting the applicability
    of our findings. However, it is important to note that we did not engage in prompt
    tuning specifically to favor our proposed framework, ensuring that the comparisons
    between different techniques remain equitable. Given the scope of our research,
    the detailed exploration of prompt sensitivity is an area we have designated for
    future investigation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究没有彻底调查不同系统对输入提示变化的敏感性。我们认识到，许多自然语言处理任务的有效性可能会显著依赖于输入提示的设计方式。由于没有对提示敏感性进行全面分析，我们承认不同的提示可能会引发我们未曾探索的广泛反应，从而可能限制我们发现的适用性。然而，需要注意的是，我们并未专门对提示进行调整以支持我们提出的框架，确保了不同技术之间的比较保持公平。鉴于我们研究的范围，提示敏感性的详细探索是我们为未来研究指定的一个领域。
- en: References
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ahmadi et al. (2019) Naser Ahmadi, Joohyung Lee, Paolo Papotti, and Mohammed
    Saeed. 2019. Explainable fact checking with probabilistic answer set programming.
    *arXiv preprint arXiv:1906.09198*.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmadi等（2019）Naser Ahmadi, Joohyung Lee, Paolo Papotti, 和 Mohammed Saeed。2019。解释性事实检查与概率答集编程。*arXiv预印本
    arXiv:1906.09198*。
- en: Atanasova et al. (2020) Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma,
    and Isabelle Augenstein. 2020. [Generating fact checking explanations](https://doi.org/10.18653/v1/2020.acl-main.656).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 7352–7364, Online. Association for Computational Linguistics.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atanasova等（2020）Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, 和 Isabelle
    Augenstein。2020。[生成事实检查解释](https://doi.org/10.18653/v1/2020.acl-main.656)。在*第58届计算语言学协会年会论文集*，第7352–7364页，在线。计算语言学协会。
- en: Cohen (1960) Jacob Cohen. 1960. A coefficient of agreement for nominal scales.
    *Educational and psychological measurement*, 20(1):37–46.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen（1960）Jacob Cohen。1960。名义尺度的一致性系数。*教育与心理测量*，20(1)：37–46。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. 2023. Improving factuality and reasoning in language models
    through multiagent debate. *arXiv preprint arXiv:2305.14325*.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等（2023）Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, 和 Igor Mordatch。2023。通过多智能体辩论提高语言模型的事实性和推理能力。*arXiv预印本
    arXiv:2305.14325*。
- en: 'Gad-Elrab et al. (2019) Mohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani,
    and Gerhard Weikum. 2019. [Exfakt: A framework for explaining facts over knowledge
    graphs and text](https://doi.org/10.1145/3289600.3290996). In *Proceedings of
    the Twelfth ACM International Conference on Web Search and Data Mining*, WSDM
    ’19, page 87–95, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gad-Elrab等（2019）Mohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani, 和 Gerhard
    Weikum。2019。[Exfakt: 一个用于解释知识图谱和文本中事实的框架](https://doi.org/10.1145/3289600.3290996)。在*第十二届ACM国际网页搜索与数据挖掘会议论文集*，WSDM
    ’19，第87–95页，美国纽约。计算机协会。'
- en: Goyal and Durrett (2021) Tanya Goyal and Greg Durrett. 2021. [Annotating and
    modeling fine-grained factuality in summarization](http://arxiv.org/abs/2104.04302).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 和 Durrett（2021）Tanya Goyal 和 Greg Durrett。2021。[注释和建模摘要中的细粒度事实性](http://arxiv.org/abs/2104.04302)。
- en: Guo et al. (2022) Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos.
    2022. [A survey on automated fact-checking](http://arxiv.org/abs/2108.11896).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2022）Zhijiang Guo, Michael Schlichtkrull, 和 Andreas Vlachos。2022。[自动化事实检查的调查](http://arxiv.org/abs/2108.11896)。
- en: 'Huang et al. (2023a) Kung-Hsiang Huang, Hou Pong Chan, and Heng Ji. 2023a.
    [Zero-shot faithful factual error correction](https://doi.org/10.18653/v1/2023.acl-long.311).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 5660–5676, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2023a）Kung-Hsiang Huang, Hou Pong Chan, 和 Heng Ji。2023a。[零样本忠实事实错误纠正](https://doi.org/10.18653/v1/2023.acl-long.311)。在*第61届计算语言学协会年会（第一卷：长篇论文）*，第5660–5676页，加拿大多伦多。计算语言学协会。
- en: 'Huang et al. (2023b) Kung-Hsiang Huang, Philippe Laban, Alexander R Fabbri,
    Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2023b.
    Embrace divergence for richer insights: A multi-document summarization benchmark
    and a case study on summarizing diverse information from news articles. *arXiv
    preprint arXiv:2309.09369*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023b) 黄光翔、菲利普·拉班、亚历山大·R·法布里、普拉夫拉·库马尔·乔贝、沙菲克·乔提、肖铭、和陈胜吴。2023b。拥抱多样性以获取更丰富的见解：多文档摘要基准及其在总结新闻文章中多样化信息的案例研究。*arXiv
    预印本 arXiv:2309.09369*。
- en: Huang et al. (2023c) Kung-Hsiang Huang, Mingyang Zhou, Hou Pong Chan, Yi R Fung,
    Zhenhailong Wang, Lingyu Zhang, Shih-Fu Chang, and Heng Ji. 2023c. Do lvlms understand
    charts? analyzing and correcting factual errors in chart captioning. *arXiv preprint
    arXiv:2312.10160*.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023c) 黄光翔、明阳·周、侯鹏·陈、易瑞·冯、镇海龙·王、凌宇·张、施福·张和姜恒。2023c。大型语言模型是否理解图表？分析和纠正图表说明中的事实错误。*arXiv
    预印本 arXiv:2312.10160*。
- en: Kotonya and Toni (2020) Neema Kotonya and Francesca Toni. 2020. [Explainable
    automated fact-checking for public health claims](https://doi.org/10.18653/v1/2020.emnlp-main.623).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 7740–7754, Online. Association for Computational Linguistics.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotonya and Toni (2020) 尼玛·科通亚和弗朗西斯卡·托尼。2020。 [可解释的自动化事实检查公共健康声明](https://doi.org/10.18653/v1/2020.emnlp-main.623)。在
    *2020年自然语言处理实证方法会议论文集*，页码7740–7754，在线。计算语言学协会。
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. 2023. [G-eval: NLG evaluation using gpt-4 with better human
    alignment](https://doi.org/10.18653/v1/2023.emnlp-main.153). In *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    2511–2522, Singapore. Association for Computational Linguistics.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) 刘阳、丹·伊特、易聪·徐、硕航·王、若辰·徐和程光朱。2023。[G-eval: 使用GPT-4进行更好的人类对齐的NLG评估](https://doi.org/10.18653/v1/2023.emnlp-main.153)。在
    *2023年自然语言处理实证方法会议论文集*，页码2511–2522，新加坡。计算语言学协会。'
- en: 'Lu and Li (2020) Yi-Ju Lu and Cheng-Te Li. 2020. [GCAN: Graph-aware co-attention
    networks for explainable fake news detection on social media](https://doi.org/10.18653/v1/2020.acl-main.48).
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pages 505–514, Online. Association for Computational Linguistics.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lu and Li (2020) 盧依如和李承德。2020。 [GCAN: 图感知共注意力网络用于社交媒体上可解释的假新闻检测](https://doi.org/10.18653/v1/2020.acl-main.48)。在
    *第58届计算语言学协会年会论文集*，页码505–514，在线。计算语言学协会。'
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. [Self-refine: Iterative refinement with self-feedback](http://arxiv.org/abs/2303.17651).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan et al. (2023) 阿曼·马丹、尼凯特·坦登、普拉克哈尔·古普塔、斯凯勒·霍利南、吕雨·高、萨拉·维格里夫、乌里·阿隆、努哈·迪里、施瑞迈·普拉布摩耶、易铭·杨、肖恩·韦雷克、博迪萨特瓦·普拉萨德·马久姆德、沙尚克·古普塔、阿米尔·亚兹丹巴赫和彼得·克拉克。2023。[Self-refine:
    通过自我反馈进行迭代优化](http://arxiv.org/abs/2303.17651)。'
- en: OpenAI (2023a) OpenAI. 2023a. [Chatgpt](https://chat.openai.com/).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023a) OpenAI。2023a。 [Chatgpt](https://chat.openai.com/)。
- en: OpenAI (2023b) OpenAI. 2023b. [Gpt-4 turbo](https://help.openai.com/en/articles/8555510-gpt-4-turbo).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI。2023b。 [Gpt-4 turbo](https://help.openai.com/en/articles/8555510-gpt-4-turbo)。
- en: OpenAI (2023c) OpenAI. 2023c. [Gpt-4v(ision) system card](https://chat.openai.com/).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023c) OpenAI。2023c。 [Gpt-4v(ision) 系统卡片](https://chat.openai.com/)。
- en: Ostrowski et al. (2021) Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, and
    Isabelle Augenstein. 2021. Multi-hop fact checking of political claims. In *Proceedings
    of the Thirtieth International Joint Conference on Artificial Intelligence*.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ostrowski et al. (2021) 沃伊切赫·奥斯特罗夫斯基、阿尔纳夫·阿罗拉、佩帕·阿塔纳索瓦和伊莎贝尔·奥根斯坦。2021。政治声明的多跳事实检查。
    在 *第三十届国际联合人工智能会议论文集*。
- en: 'Reddy et al. (2023) Revanth Gangi Reddy, Yi R Fung, Qi Zeng, Manling Li, Ziqi
    Wang, Paul Sullivan, et al. 2023. Smartbook: Ai-assisted situation report generation.
    *arXiv preprint arXiv:2303.14337*.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reddy et al. (2023) 雷万特·甘吉·雷迪、易瑞·冯、齐·曾、满玲·李、子琪·王、保罗·沙利文等。2023。Smartbook: AI辅助的情况报告生成。*arXiv
    预印本 arXiv:2303.14337*。'
- en: Russo et al. (2023) Daniel Russo, Serra Sinem Tekiroğlu, and Marco Guerini.
    2023. [Benchmarking the generation of fact checking explanations](https://doi.org/10.1162/tacl_a_00601).
    *Transactions of the Association for Computational Linguistics*, 11:1250–1264.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russo et al. (2023) 丹尼尔·鲁索、塞拉·希嫩·泰基罗格鲁和马尔科·古埃里尼。2023。[生成事实检查解释的基准测试](https://doi.org/10.1162/tacl_a_00601)。*计算语言学协会交易*，11:1250–1264。
- en: 'Shu et al. (2019) Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu.
    2019. defend: Explainable fake news detection. In *Proceedings of the 25th ACM
    SIGKDD international conference on knowledge discovery & data mining*, pages 395–405.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shu et al. (2019) Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, 和 Huan Liu.
    2019. defend: 可解释的假新闻检测. 见于 *第25届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，页395–405。'
- en: 'Wang et al. (2023) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu
    Wei, and Heng Ji. 2023. Unleashing cognitive synergy in large language models:
    A task-solving agent through multi-persona selfcollaboration. *arXiv preprint
    arXiv:2307.05300*, 1(2):3.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu
    Wei, 和 Heng Ji. 2023. 释放大型语言模型中的认知协同：通过多角色自我协作的任务解决代理. *arXiv预印本 arXiv:2307.05300*,
    1(2):3.
- en: Appendix A Human Evaluation Details
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 人工评估细节
- en: '![Refer to caption](img/a30f5bed421c65361be8b41b71a10fe1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a30f5bed421c65361be8b41b71a10fe1.png)'
- en: 'Figure 2: The interface for our human evaluation.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们的人工评估界面。
- en: A.1 Evaluation Guidelines
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 评估指南
- en: 'In this task you will evaluate the faithulness of automatically generated fact-checking
    explanation using a label, claim, and source used to generate the explanation.
    To correctly solve this task, follow these steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在此任务中，你将使用生成解释所用的标签、主张和来源来评估自动生成的事实检查解释的真实性。要正确解决此任务，请遵循以下步骤：
- en: 'Carefully read and understand the topology of errors and examples given below.
    Carefully read the generated fact-checking explanation and the source. For each
    explanation, check it with the evidence and decide if any of the error exists
    in the explanation. Note: You will analyze each sentence, but you should consider
    the connection between other sentences as well. Warning: Annotations will be checked
    for quality against control labels, low quality work will be rejected.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细阅读并理解下面给出的错误拓扑结构和示例。仔细阅读生成的事实检查解释及其来源。对于每个解释，检查其证据，并决定解释中是否存在错误。注意：你将分析每一句话，但也应考虑其他句子之间的联系。警告：注释将根据控制标签检查质量，低质量的工作将被拒绝。
- en: 'Type of Errors:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 错误类型：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intrinsic Entity-Related Errors: Intrinsic entity-related errors occur when
    there is a mistake in representing named entities, quantities, dates, or other
    surface realizations from the given source within the generated explanation. Example:
    Incorrectly combining distinct entities from the given source.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内部实体相关错误：内部实体相关错误发生在生成的解释中对来源中的命名实体、数量、日期或其他表面表现形式的错误表现。例如：错误地将来源中的不同实体结合在一起。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extrinsic Entity-Related Errors: Extrinsic entity-related errors involve the
    introduction of new entities that are not present in the given source into the
    generated explanation. Example: Hallucinating new entities that do not exist in
    the source.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部实体相关错误：外部实体相关错误涉及在生成的解释中引入来源中没有的新实体。例如：虚构来源中不存在的新实体。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intrinsic Event-Related Errors: Intrinsic event-related errors pertain to mistakes
    in representing events mentioned in the generated explanation, leading to incorrect
    claims about events. Example: Making inaccurate claims about events mentioned
    in the explanation.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内部事件相关错误：内部事件相关错误涉及在生成的解释中对提及的事件的错误表现，导致对事件的错误陈述。例如：对解释中提及的事件做出不准确的陈述。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extrinsic Event-Related Errors: Extrinsic event-related errors occur when the
    generated explanation includes new events that are not present in the given source.
    Example: Introducing fabricated events that are not supported by the source.'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部事件相关错误：外部事件相关错误发生在生成的解释中包含了来源中没有的新事件。例如：引入来源中没有的虚构事件。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intrinsic Noun Phrase-Related Errors: Intrinsic noun phrase-related errors
    are mistakes related to noun phrases, excluding entity-specific errors. They may
    involve miscombining noun phrases with incorrect modifiers from the given source.
    Example: Incorrectly combining a noun phrase with the wrong modifier from the
    source.'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内部名词短语相关错误：内部名词短语相关错误是指与名词短语相关的错误，不包括特定实体错误。这些错误可能涉及将名词短语与来源中的不正确修饰词错误地组合在一起。例如：将名词短语与来源中的错误修饰词错误地结合。
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extrinsic Noun Phrase-Related Errors: Extrinsic noun phrase-related errors
    involve the introduction of new noun phrase modifiers that are not present in
    the given source into the generated explanation. Example: Hallucinating new noun
    phrase modifiers not supported by the source.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部名词短语相关错误：外部名词短语相关错误涉及在生成的解释中引入在给定来源中不存在的新名词短语修饰语。例如：幻觉产生了来源中未支持的新名词短语修饰语。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning Coherence Errors: Reasoning coherence errors occur when there are
    logical flaws in the flow of reasoning within the generated explanation, leading
    to a lack of coherence or weak support for the claim. Example: Presenting evidence
    that does not logically connect to the main claim, resulting in a disjointed explanation.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理一致性错误：推理一致性错误发生在生成的解释中的推理流程存在逻辑缺陷时，导致缺乏一致性或对声明的支持较弱。例如：提供的证据与主要声明逻辑上不相关，导致解释不连贯。
- en: •
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Overgeneralization Errors: Overgeneralization errors happen when the generated
    explanation makes sweeping statements or draws conclusions that go beyond the
    scope of the evidence provided.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 过度概括错误：过度概括错误发生在生成的解释中做出过于笼统的陈述或得出超出所提供证据范围的结论时。
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Irrelevant Evidence Errors: Irrelevant evidence errors occur when the generated
    explanation includes evidence that is not directly related to the claim, leading
    to confusion and lack of support for the main argument. Example: Including evidence
    that is tangential or unrelated to the claim being explained.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不相关证据错误：不相关证据错误发生在生成的解释中包含与声明不直接相关的证据时，导致混乱和对主要论点的支持不足。例如：包含与解释的声明无关或不相关的证据。
- en: A.2 Evaluation Interface
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评估界面
- en: We display our evaluation interface in [Figure 2](#A1.F2 "In Appendix A Human
    Evaluation Details ‣ Can LLMs Produce Faithful Explanations For Fact-checking?
    Towards Faithful Explainable Fact-Checking via Multi-Agent Debate").
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 2](#A1.F2 "在附录 A 人工评估细节 ‣ LLMs 是否能为事实检查提供可靠解释？通过多代理辩论进行可靠的可解释事实检查")中展示了我们的评估界面。
- en: A.3 Worker Qualification
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 工作者资格
- en: We established specific initial criteria for selecting highly efficient MTurk
    workers. These prerequisites include having a HIT approval rate of at least 99%,
    completing a minimum of 10,000 approved HITs, and being located in the United
    Kingdom, Canada, or the United States.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为选择高效的 MTurk 工作者设定了具体的初步标准。这些前提条件包括 HIT 批准率至少为99%，完成至少10,000个批准的 HIT，并且位于英国、加拿大或美国。
- en: Furthermore, beyond these initial requirements, qualified workers must pass
    two rounds of qualification tests aimed at identifying errors in generated explanations.
    To refine the qualification process, we manually annotated two HITs, each featuring
    one multi-hop fact-checking instance from PolitiHop and an explanation generated
    by one of the models. In each qualification phase, annotators review one of these
    annotated examples. Those whose annotations do not closely match ours are excluded
    from the selection process.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了这些初步要求外，合格的工人必须通过两轮资格测试，以识别生成解释中的错误。为了完善资格过程，我们手动标注了两个 HIT，每个 HIT 包含一个来自
    PolitiHop 的多跳事实检查实例和一个由模型生成的解释。在每个资格阶段，注释员审查其中一个标注的示例。那些注释与我们的标注不紧密匹配的，将被排除在选择过程之外。
- en: Ultimately, 4 annotators who successfully completed all two stages of the qualification
    tests were selected. Additionally, we carefully designed each HIT to ensure that
    annotators could earn an hourly rate of $15 to $20, provided they work continuously.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，成功完成所有两个阶段资格测试的4名注释员被选中。此外，我们精心设计了每个 HIT，以确保注释员能够在持续工作的情况下获得每小时15到20美元的报酬。
- en: A.4 Annotation Quality
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 注释质量
- en: We computed the agreement between each annotator with one of the authors of
    this paper. The agreement is 0.69 per Cohen’s Kappa Cohen ([1960](#bib.bib3)),
    indicating a moderate-to-high level of agreement.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了每个注释员与本文作者之一的协议度。根据 Cohen 的 Kappa Cohen（[1960](#bib.bib3)），协议度为 0.69，表示中等到高水平的一致性。
- en: Appendix B MADR Details
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B MADR 详细信息
- en: Algorithm 1 MADR
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 MADR
- en: '1:Input: Given claim $C$'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：给定声明 $C$'
- en: Appendix C Prompts
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示
- en: Evaluation prompts are shown in [Table 6](#A3.T6 "In Appendix C Prompts ‣ Can
    LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate") and [Table 5](#A3.T5 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"). The prompts for self-refinement and Multi-Agent
    Debate Refinement (MADR) are displayed in [table 7](#A3.T7 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate") and [table 11](#A3.T11 "In Appendix C Prompts
    ‣ Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable
    Fact-Checking via Multi-Agent Debate"), respectively.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 评价提示见于[表6](#A3.T6 "在附录C提示 ‣ LLMs能否提供忠实的事实检查解释？通过多代理辩论实现忠实的可解释性事实检查")和[表5](#A3.T5
    "在附录C提示 ‣ LLMs能否提供忠实的事实检查解释？通过多代理辩论实现忠实的可解释性事实检查")。自我修正和多代理辩论修正（MADR）的提示分别展示在[表7](#A3.T7
    "在附录C提示 ‣ LLMs能否提供忠实的事实检查解释？通过多代理辩论实现忠实的可解释性事实检查")和[表11](#A3.T11 "在附录C提示 ‣ LLMs能否提供忠实的事实检查解释？通过多代理辩论实现忠实的可解释性事实检查")。
- en: '| You will be given a fact-checking explanation along with the evidence used
    for fact-checking. Your task is to rate the explanation on one metric. Please
    make sure you read and understand these instructions carefully. Please keep this
    document open while reviewing, and refer to it as needed. Evaluation Criteria:
    Faithfulness (1-5) - the factual alignment between the fact-checking explanation
    and the evidence. The explanation should accurately reflect the evidence and its
    context, without misrepresenting or omitting crucial details. Annotators were
    instructed to penalize explanations that contain inaccuracies, misinterpretations,
    or fail to adequately represent the evidence provided. Below are the error typology
    that you need to utilize to determine faithfulness between the explanation and
    evidence: {error typology} Evaluation Steps: 1\. Read the fact-checking explanation
    and the evidence provided carefully. 2\. Compare the explanation to the evidence
    to identify how well it represents the facts, context, and conclusions drawn from
    the evidence. 3\. Assess how accurately and completely the explanation reflects
    the evidence without distortion or significant omission. Assign a faithfulness
    score from 1 to 5. Evidence Provided: {evidence} Fact-Checking Explanation: {explanation}
    Evaluation Form: Faithfulness: |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 你将获得一个事实检查解释和用于事实检查的证据。你的任务是根据一个指标对解释进行评分。请确保仔细阅读并理解这些说明。在审查过程中请保持此文档开启，并根据需要参考它。评价标准：忠实度（1-5）
    - 事实检查解释与证据之间的事实一致性。解释应准确反映证据及其背景，而不歪曲或遗漏重要细节。注释员被指示惩罚包含不准确、误解或未能充分代表所提供证据的解释。以下是用于确定解释与证据之间忠实度的错误类型：{error
    typology} 评价步骤：1\. 仔细阅读事实检查解释和提供的证据。2\. 将解释与证据进行比较，确定其如何准确表示事实、背景和从证据中得出的结论。3\.
    评估解释如何准确而完整地反映证据，避免扭曲或重大遗漏。根据忠实度从1到5进行评分。提供的证据：{evidence} 事实检查解释：{explanation}
    评价表格：忠实度：|'
- en: 'Table 5: Prompt templates for document-level automatic evaluation. The texts
    in grey are only presented in the prompts when error typology is applied.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：文档级自动评价的提示模板。灰色文本仅在应用错误类型时出现在提示中。
- en: '| You will be given a sentence from a fact-checking explanation along with
    the evidence used for fact-checking. Your task is to rate the explanation sentence
    on one metric. Please make sure you read and understand these instructions carefully.
    Please keep this document open while reviewing, and refer to it as needed. Evaluation
    Criteria: Faithfulness (1-5) - the factual alignment between the fact-checking
    explanation sentence and the evidence. The explanation should accurately reflect
    the evidence and its context, without misrepresenting or omitting crucial details.
    Annotators were instructed to penalize explanations that contain inaccuracies,
    misinterpretations, or fail to adequately represent the evidence provided. Below
    are the error typology that you need to utilize to determine faithfulness between
    the explanation and evidence: {error typology} Evaluation Steps: 1\. Read the
    fact-checking explanation sentence and the evidence provided carefully. 2\. Compare
    the explanation sentence to the evidence to identify how well it represents the
    facts, context, and conclusions drawn from the evidence using the error typology
    above. 3\. Assess how accurately and completely the explanation sentence reflects
    the evidence without distortion or significant omission. Assign a faithfulness
    score from 1 (unfaithful) to 5 (faithful). Evidence Provided: {evidence} Fact-Checking
    Explanation: {explanation} Evaluation Form: Faithfulness: |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 你将获得一个来自事实检查解释的句子，以及用于事实检查的证据。你的任务是根据一个指标对解释句子进行评分。请确保仔细阅读并理解这些说明。在审查时请保持此文档打开，并在需要时参考它。评估标准：忠实度（1-5）
    - 事实检查解释句子与证据之间的事实一致性。解释应准确反映证据及其背景，不得歪曲或遗漏关键细节。注释者被指示对包含不准确、误解或未能充分代表提供的证据的解释进行惩罚。以下是你需要利用的错误类型，以确定解释与证据之间的忠实度：{error
    typology} 评估步骤：1\. 仔细阅读事实检查解释句子和提供的证据。2\. 将解释句子与证据进行比较，以确定它在多大程度上代表了从证据中得出的事实、背景和结论，使用上述错误类型。3\.
    评估解释句子在没有扭曲或重大遗漏的情况下，准确完整地反映了证据。给出忠实度评分，范围从 1（不忠实）到 5（忠实）。提供的证据：{evidence} 事实检查解释：{explanation}
    评估表单：忠实度： |'
- en: 'Table 6: Prompt templates for sentence-level automatic evaluation. The texts
    in grey are only presented in the prompts when error typology is applied.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：句子级自动评估的提示模板。灰色文本仅在应用错误类型时出现在提示中。
- en: 'Prompt for Feedback Generation Human: Give me the error types that the generated
    explanation can contain. LLM: Below are the error typology that you need to utilize
    to determine faithfulness between the explanation and evidence: {error typology}
    Human: Provide the claim, its corresponding label (true, false, or half-true),
    and the supporting evidence. LLM: Generate the initial explanation. Human: Find
    all errors (Intrinsic Entity-Related error, Extrinsic Entity-Related error, Intrinsic
    Event-Related error, Extrinsic Event-Related error, Intrinsic Noun-Phrase-Related
    error, Extrinsic Noun-Phrase-Related error, Reasonability-Related error, Connected
    evidence related error) in the "generated explanation" and provide the feedback
    by following the steps; Error count: how many errors have been found (what types
    of error); Step 1) Recognize what type(s) of error has been found in the generated
    explanation; Step 2) Recognize which sentence(s) contain(s) the error(s); Step
    3) Recognize what causes the error; Step 4) Why is the error; Step 5) How the
    error should be corrected; If there are multiple errors, please write 5 steps
    for each error. Prompt for Refinement Human: (Provide the feedback of two agents.)
    Please revise the generated explanation for the label on fact-checking using the
    given feedback without any modification other than feedback. (Provide the example
    of the refinement as guidance.)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 生成反馈的提示 人类：告诉我生成的解释可能包含的错误类型。 LLM：以下是你需要利用的错误类型，以确定解释与证据之间的真实性：{错误类型} 人类：提供声明、其对应的标签（真实、虚假或半真实）以及支持证据。
    LLM：生成初步解释。 人类：在“生成的解释”中找出所有错误（内在实体相关错误、外在实体相关错误、内在事件相关错误、外在事件相关错误、内在名词短语相关错误、外在名词短语相关错误、合理性相关错误、连接证据相关错误），并按照以下步骤提供反馈；
    错误数量：发现了多少个错误（错误类型）； 步骤 1）识别生成的解释中发现了哪些类型的错误； 步骤 2）识别哪些句子包含了错误； 步骤 3）识别错误的原因；
    步骤 4）错误的原因是什么； 步骤 5）错误应该如何纠正； 如果有多个错误，请为每个错误编写 5 个步骤。 优化提示 人类：（提供两个代理的反馈。）请根据给定的反馈修订生成的解释以进行事实核查，除了反馈外不做任何修改。（提供优化示例作为指导。）
- en: 'Table 7: The prompt for the self-refinement approach.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：自我优化方法的提示。
- en: 'Prompt for Debater 1 in MADR Human: Give me the error types that the generated
    explanation can contain. LLM: Below are the error typology that you need to utilize
    to determine faithfulness between the explanation and evidence: {error typology}
    Human: Provide the claim, its corresponding label (true, false, or half-true),
    and the supporting evidence. LLM: Generate the initial explanation. Human: You
    are a professional analyzer who find potential errors, which might weaken faithfulness,
    in the generated explanation (not in the source) and categorize them according
    to predefined error types. Thoroughly comprehend the provided source and the task
    carefully. Your task: – Step 1: Find all potential errors, which might weaken
    faithfulness, in the generated explanation (not in the source) and provide exact
    senteces where the errors are found with quotation. – Step 2: Categorize them
    according to predefined error types above. – Step 3: Provide specific and actionable
    feedbacks with instruction how to fix them. Please provide only the feedback,
    not the revised explanation. Remember that explanation can contain multiple same
    errors. LLM: Generate the feedback. Your task: – Step 1: Take your whole previous
    feedback. – Step 2: Compare your previous feedback with feedback from another
    professional analyzer to check whether your previous feedback contains any wrong
    error or feedback. – Step 3: Find the errors or feedbacks that you think they
    are valid and should be added to your feedback from other’s feedbacks (errors
    must be found from the generated explanation not the feedback). – Step 4: Rewrite
    the feedback based from your previous feedback using the answers from the steps
    above. Do not add any extra words than feedback. Remember you should follow this
    rule: do not to copy feedback from other and provide what are errors, exact senteces
    where the errors are found with quotation, and feedbacks. These are feedbacks
    from another professional analyzer: {Feedback from Debater 2}'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: MADR 人类对辩手 1 的提示：告诉我生成的解释可能包含的错误类型。LLM：以下是你需要利用的错误类型，以确定解释和证据之间的忠实度：{错误类型} 人类：提供声明、其对应的标签（真、假或半真）以及支持证据。LLM：生成初步解释。人类：你是一个专业分析师，负责发现生成解释中（而非来源中）可能削弱忠实度的潜在错误，并根据预定义的错误类型对其进行分类。请彻底理解提供的来源和任务要求。你的任务：–
    步骤 1：找到生成解释中（而非来源中）所有可能削弱忠实度的潜在错误，并提供错误所在的确切句子及其引文。– 步骤 2：根据上述预定义的错误类型对其进行分类。–
    步骤 3：提供具体且可操作的反馈，并说明如何修正这些错误。请仅提供反馈，不要提供修订后的解释。请记住，解释中可能包含多个相同的错误。LLM：生成反馈。你的任务：–
    步骤 1：拿到你所有的之前反馈。– 步骤 2：将你之前的反馈与另一位专业分析师的反馈进行比较，以检查你之前的反馈是否包含任何错误的错误或反馈。– 步骤 3：从其他人的反馈中找到你认为有效且应该添加到你的反馈中的错误或反馈（错误必须从生成的解释中找到，而不是从反馈中找到）。–
    步骤 4：根据你之前的反馈，结合以上步骤的答案重写反馈。不要添加任何多余的词语。记住你应该遵循以下规则：不要复制其他人的反馈，并提供错误是什么、错误所在的确切句子及其引文，以及反馈。这些是另一位专业分析师的反馈：{辩手
    2 的反馈}
- en: 'Table 8: The prompt for Debater 1 in MADR.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：MADR 中辩手 1 的提示。
- en: 'Prompt for Debater 2 in MADR Human: Give me the error types that the generated
    explanation can contain. LLM: Below are the error typology that you need to utilize
    to determine faithfulness between the explanation and evidence: {error typology}
    Human: Provide the claim, its corresponding label (true, false, or half-true),
    and the supporting evidence. LLM: Generate the initial explanation. Human: You
    are a professional analyzer who find errors, classified by predefined error types,
    in the generated explanation (not in the source) and provide feedback for correcting
    them. Thoroughly comprehend the provided source and the task carefully. Your task:
    – Step 1: Find all errors categorized by predefined error types, which undermine
    the faithfulness of the generated explanation (not in the source) and provide
    exact senteces where the errors are found with quotation. – Step 2: Provide specific
    and actionable feedbacks with instruction how to fix them. Please provide only
    the feedback, not the revised explanation. Remember that explanation can contain
    multiple same errors. LLM: Generate the feedback. Your task: – Step 1: Take your
    whole previous feedback. – Step 2: Compare your previous feedback with feedback
    from another professional analyzer to check whether your previous feedback contains
    any wrong error or feedback. – Step 3: Find the errors or feedbacks that you think
    they are valid and should be added to your feedback from other’s feedbacks (errors
    must be found from the generated explanation not the feedback). – Step 4: Rewrite
    the feedback based from your previous feedback using the answers from the steps
    above. Do not add any extra words than feedback. Remember you should follow this
    rule: do not to copy feedback from other and provide what are errors, exact senteces
    where the errors are found with quotation, and feedbacks. These are feedbacks
    from another professional analyzer: {Feedback from Debater 1}'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 辩手2在MADR中的提示 人类：给出生成解释可能包含的错误类型。 LLM：以下是你需要利用的错误类型，用以确定解释与证据之间的可信度：{错误类型} 人类：提供声明、其对应的标签（真实、虚假或半真实），以及支持证据。
    LLM：生成初步解释。 人类：你是一个专业分析师，负责在生成的解释中（而不是在来源中）找到由预定义错误类型分类的错误，并提供纠正这些错误的反馈。仔细理解提供的来源和任务。你的任务是：
    – 步骤1：找到所有由预定义错误类型分类的错误，这些错误破坏了生成解释的可信度（而不是来源中的），并提供准确的错误所在句子及其引文。 – 步骤2：提供具体且可操作的反馈，说明如何修复这些错误。请仅提供反馈，不要提供修订后的解释。请记住，解释中可能包含多个相同的错误。
    LLM：生成反馈。 你的任务是： – 步骤1：获取你之前的所有反馈。 – 步骤2：将你之前的反馈与另一位专业分析师的反馈进行比较，以检查你之前的反馈是否包含任何错误或不正确的反馈。
    – 步骤3：从其他人的反馈中找到你认为有效且应该添加到你反馈中的错误或反馈（错误必须从生成的解释中找到，而不是反馈中）。 – 步骤4：基于你之前的反馈，使用上述步骤的答案重写反馈。不要添加任何额外的文字，只提供反馈。记住，你应该遵循这个规则：不要复制其他人的反馈，而是提供错误、准确的错误所在句子及其引文以及反馈。这些是另一位专业分析师的反馈：{辩手1的反馈}
- en: 'Table 9: The prompt for Debater 2 in MADR.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：MADR中辩手2的提示。
- en: 'Self-Refine Input Prompt Claim (C): Says Jeff Foxworthy wrote a list of examples
    explaining how "you might live in a nation that was founded by geniuses but is
    run by idiots." Label (L): False Source (S): If you can get arrested for hunting
    or fishing without a license, but not for entering and remaining in the country
    illegally 2014 you might live in a country that was founded by geniuses but is
    run by idiots. That’s one of many examples in a list posted to Facebook on Nov.
    19 and falsely attributed to comedian Jeff Foxworthy. The post was flagged as
    part of Facebook’s efforts to combat false news and misinformation on its News
    Feed. (Read more about our partnership with Facebook.) Foxworthy is famous for
    his Ÿou might be a redneck if 2026 jokes , but searching online we couldn’t find
    any credible evidence that he penned this list that touches on abortion, Muslims
    and federal debt. Rather, we found users on web forums crediting someone named
    Fritz Edmunds with the list. Snopes, which fact-checked this claim back in 2013,
    also noted that the original compiler of this appears to be Fritz Edmunds, who
    posted it to his 2018 Politically True blog back in Feb. 3, 2013 (albeit with
    a disclaimer noting that 2018some of the ideas were from an email that did not
    contain a copyright). The link to that post is now dead but the url includes the
    words channeling Jeff Foxworthy. Initial Explanation The claim is labeled as false.
    The claim suggests that comedian Jeff Foxworthy wrote a list of examples explaining
    how "you might live in a nation that was founded by geniuses but is run by idiots."
    However, this claim is inaccurate and misleading. There is no credible evidence
    to suggest that Jeff Foxworthy wrote the specific list mentioned in the claim.
    … Feedback Error count: 1 (Intrinsic Entity-Related error) Step 1) Recognize what
    type(s) of error has been found in the generated explanation: Extrinsic Noun-Phrase-Related
    error Step 2) Recognize which sentence(s) contain(s) the error(s): The sentence
    containing the error is: "The claim suggests that comedian Jeff Foxworthy wrote
    a list of examples explaining how "you might live in a nation that was founded
    by geniuses but is run by idiots."" Step 3) Recognize what causes the error: The
    error occurs when the explanation incorrectly attributes the creation of the list
    to Jeff Foxworthy. Step 4) Why is the error: The error is due to the fact that
    the list was not authored by Jeff Foxworthy, but rather falsely attributed to
    him. Step 5) How the error should be corrected: The sentence should be revised
    to accurately reflect that the list was not written by Jeff Foxworthy. For example:
    "The claim falsely attributes a list of examples to comedian Jeff Foxworthy, suggesting
    that he wrote about how ’you might live in a nation that was founded by geniuses
    but is run by idiots.’ However, there is no evidence to support this claim." Revised
    Explanation The claim is labeled as false. The claim falsely attributes a list
    of examples to comedian Jeff Foxworthy, suggesting that he wrote about how ’you
    might live in a nation that was founded by geniuses but is run by idiots.’ However,
    there is no evidence to support this claim. …'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自我修正输入提示声明（C）：称**杰夫·福克斯沃西**写了一份解释“你可能生活在一个由天才创立但由傻瓜管理的国家”的例子列表。标签（L）：虚假 来源（S）：如果你因为没有许可证而被逮捕猎鱼，但非法入境和滞留该国却不会被逮捕，你可能生活在一个由天才创立但由傻瓜管理的国家。这是2014年11月19日发布在Facebook上的一个例子列表中的众多例子之一，错误地归因于喜剧演员**杰夫·福克斯沃西**。该帖子被标记为Facebook努力打击虚假新闻和信息错误的一部分。（阅读更多关于我们与Facebook的合作。）**福克斯沃西**以“你可能是个乡巴佬如果”笑话而闻名，但我们在网上搜索未能找到任何可靠证据表明他编写了这份涉及堕胎、穆斯林和联邦债务的列表。相反，我们发现网络论坛上的用户将这份列表归功于一个名为**弗里茨·埃德蒙兹**的人。Snopes在2013年对这一声明进行了事实检查，并指出这份列表的原始编制者似乎是**弗里茨·埃德蒙兹**，他于2013年2月3日将其发布到他的2018年《政治真实》博客（尽管附有免责声明，说明2018年有些观点来自一封未包含版权的电子邮件）。该帖子的链接现在已失效，但网址包含“引导**杰夫·福克斯沃西**”的字样。初步解释
    该声明被标记为虚假。该声明错误地将一份例子列表归因于喜剧演员**杰夫·福克斯沃西**，暗示他写了关于“你可能生活在一个由天才创立但由傻瓜管理的国家”的内容。然而，这一声明不准确且具有误导性。没有可信证据表明**杰夫·福克斯沃西**写了声明中提到的具体列表。……反馈
    错误数量：1（固有实体相关错误） 第一步）识别生成解释中发现的错误类型：外在名词短语相关错误 第二步）识别哪个句子包含错误：包含错误的句子是：“该声明暗示喜剧演员**杰夫·福克斯沃西**编写了一份解释‘你可能生活在一个由天才创立但由傻瓜管理的国家’的例子列表。”
    第三步）识别错误的原因：错误发生在解释中错误地将列表的创作归因于**杰夫·福克斯沃西**。 第四步）为什么会有错误：错误的原因在于这份列表并非由**杰夫·福克斯沃西**编写，而是错误地归因于他。
    第五步）错误应如何纠正：句子应修订为准确反映该列表并非由**杰夫·福克斯沃西**编写。例如：“该声明错误地将一份例子列表归因于喜剧演员**杰夫·福克斯沃西**，暗示他写了关于‘你可能生活在一个由天才创立但由傻瓜管理的国家’的内容。然而，没有证据支持这一声明。”
    修订解释 该声明被标记为虚假。该声明错误地将一份例子列表归因于喜剧演员**杰夫·福克斯沃西**，暗示他写了关于“你可能生活在一个由天才创立但由傻瓜管理的国家”的内容。然而，没有证据支持这一声明。
- en: 'Table 10: An example output from Self-Refine. Intrinsic Entity-Related Error
    initially identified from the feedback is marked in blue. The suggested revisions
    from the feedback and the actual revisions made from the initial explanation are
    highlighted in yellow.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：Self-Refine 的一个示例输出。从反馈中初步识别出的内在实体相关错误以蓝色标记。反馈中建议的修订和从初始解释中实际进行的修订以黄色突出显示。
- en: 'Debate Input Prompt Claim (C): Says Jeff Foxworthy wrote a list of examples
    explaining how "you might live in a nation that was founded by geniuses but is
    run by idiots." Label (L): False Source (S): If you can get arrested for hunting
    or fishing without a license, but not for entering and remaining in the country
    illegally 2014 you might live in a country that was founded by geniuses but is
    run by idiots. That’s one of many examples in a list posted to Facebook on Nov.
    19 and falsely attributed to comedian Jeff Foxworthy. The post was flagged as
    part of Facebook’s efforts to combat false news and misinformation on its News
    Feed. (Read more about our partnership with Facebook.) Foxworthy is famous for
    his Ÿou might be a redneck if 2026 jokes , but searching online we couldn’t find
    any credible evidence that he penned this list that touches on abortion, Muslims
    and federal debt. Rather, we found users on web forums crediting someone named
    Fritz Edmunds with the list. Snopes, which fact-checked this claim back in 2013,
    also noted that the original compiler of this appears to be Fritz Edmunds, who
    posted it to his 2018 Politically True blog back in Feb. 3, 2013 (albeit with
    a disclaimer noting that 2018some of the ideas were from an email that did not
    contain a copyright). The link to that post is now dead but the url includes the
    words channeling Jeff Foxworthy. Initial Explanation The claim is labeled as false.
    The claim suggests that comedian Jeff Foxworthy wrote a list of examples explaining
    how "you might live in a nation that was founded by geniuses but is run by idiots."
    However, this claim is inaccurate and misleading. There is no credible evidence
    to suggest that Jeff Foxworthy wrote the specific list mentioned in the claim.
    … Feedback 1 $F_{0,1}$ The claim falsely suggests that Jeff Foxworthy wrote a
    list of examples explaining how ’you might live in a nation that was founded by
    geniuses but is run by idiots.’ However, as of now, there is no credible evidence
    to suggest that Jeff Foxworthy wrote the specific list mentioned in the claim.
    …'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 辩论输入提示声明（C）：称杰夫·福克斯沃西编写了一份示例列表，解释如何“你可能生活在一个由天才创立但由傻瓜管理的国家”。标签（L）：虚假 来源（S）：如果你因没有许可证而被逮捕打猎或钓鱼，但因为非法入境和逗留在该国却没有被逮捕，那么2014年你可能生活在一个由天才创立但由傻瓜管理的国家。这是11月19日发布在Facebook上的一系列例子之一，错误归于喜剧演员杰夫·福克斯沃西。这一帖子被标记为Facebook为了打击虚假新闻和错误信息而采取的措施的一部分。（阅读更多关于我们与Facebook的合作。）福克斯沃西因其“你可能是个乡巴佬，如果2026”的笑话而闻名，但在线搜索中我们未能找到任何可靠证据表明他编写了这个涉及堕胎、穆斯林和联邦债务的列表。相反，我们发现网络论坛用户将这个列表归功于一个叫弗里茨·埃德蒙兹的人。Snopes在2013年检查了这一声明，也注意到原始的编者似乎是弗里茨·埃德蒙兹，他在2013年2月3日将其发布到他的2018年政治真实博客上（尽管有免责声明说明2018年一些观点来自于不包含版权的电子邮件）。该帖子的链接现在已经失效，但网址中包含了“借用杰夫·福克斯沃西”的字样。初步解释该声明被标记为虚假。该声明暗示喜剧演员杰夫·福克斯沃西编写了一份示例列表，解释如何“你可能生活在一个由天才创立但由傻瓜管理的国家”。然而，这一声明是不准确和误导的。没有可靠的证据表明杰夫·福克斯沃西编写了声明中提到的具体列表。…
- en: 'Table 11: An example output from MADR. Extrinsic Event Error initially identified
    from Feedback 1 are marked in red, while Intrinsic Entity-Related Error initially
    identified from Feedback 2 are marked in blue. Both the suggested revisions from
    each feedback and the actual revisions made from the initial explanation are highlighted
    in yellow.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：MADR 的一个示例输出。从反馈1中初步识别出的外在事件错误以红色标记，而从反馈2中初步识别出的内在实体相关错误以蓝色标记。每个反馈中建议的修订和从初始解释中实际进行的修订都以黄色突出显示。
