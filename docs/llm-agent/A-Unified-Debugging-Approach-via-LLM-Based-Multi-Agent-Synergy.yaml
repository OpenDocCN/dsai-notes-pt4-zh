- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:47:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:47:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A Unified Debugging Approach via LLM-Based Multi-Agent Synergy
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的多代理协同的统一调试方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17153](https://ar5iv.labs.arxiv.org/html/2404.17153)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17153](https://ar5iv.labs.arxiv.org/html/2404.17153)
- en: 'Cheryl Lee1, Chunqiu Steven Xia2, Jen-tse Huang1, Zhouruixin Zhu3, Lingming
    Zhang2, and Michael R. Lyu1 1The Chinese University of Hong Kong. Email: cheryllee@link.cuhk.edu.hk,
    {jthuang, lyu}@cse.cuhk.edu.hk 2University of Illinois Urbana-Champaign. Email:
    {chunqiu2, lingming}@illinois.edu 3The Chinese University of Hong Kong, Shenzhen.
    Email: zhouruixingzhu@link.cuhk.edu.cn'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cheryl Lee1、Chunqiu Steven Xia2、Jen-tse Huang1、Zhouruixin Zhu3、Lingming Zhang2
    和 Michael R. Lyu1 1香港中文大学。电子邮件: cheryllee@link.cuhk.edu.hk, {jthuang, lyu}@cse.cuhk.edu.hk
    2伊利诺伊大学厄本那-香槟分校。电子邮件: {chunqiu2, lingming}@illinois.edu 3香港中文大学深圳。电子邮件: zhouruixingzhu@link.cuhk.edu.cn'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Tremendous efforts have been devoted to automating software debugging, a time-consuming
    process involving fault localization and repair generation. Recently, Large Language
    Models (LLMs) have shown great potential in automated debugging. However, we identified
    three challenges posed to traditional and LLM-based debugging tools: 1) the upstream
    imperfection of fault localization affects the downstream repair, 2) the deficiency
    in handling complex logic errors, and 3) the ignorance of program contexts. In
    this context, we propose the first automated, unified debugging framework, FixAgent,
    via LLM agent synergy. FixAgent can perform end-to-end localization, repair, and
    analysis of bugs. Our insight is that LLMs can benefit from general software engineering
    principles recognized by human developers in debugging, such as rubber duck debugging,
    enabling a better understanding of program functionality and logic bugs. Hence,
    we create three designs inspired by rubber ducking to address these challenges.
    They are LLM agent specialization and synergy, key variable tracking, and program
    context comprehension, which request LLMs to provide explicit explanations and
    force them to focus on crucial program logic information. Experiments on the widely
    used dataset QuixBugs show that FixAgent correctly fixes 79 out of 80 bugs, 9
    of which have never been fixed. It also plausibly patches 1.9X more defects than
    the best-performing repair tool on Codeflaws, even with no bug location information
    and fewer than 0.6% sampling times. On average, FixAgent increases about 20% plausible
    and correct fixes compared to its base model using different LLMs, showing the
    effectiveness of our designs. Moreover, the correctness rate of FixAgent reaches
    remarkably 97.26%, indicating that FixAgent can potentially overcome the overfitting
    issue of the existing approaches.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的努力已经投入到自动化软件调试中，这是一个耗时的过程，涉及故障定位和修复生成。最近，大型语言模型（LLMs）在自动化调试中展示了巨大的潜力。然而，我们发现传统和基于LLM的调试工具面临三个挑战：1）故障定位的上游不完善影响下游修复，2）处理复杂逻辑错误的能力不足，以及
    3）忽视程序上下文。在这种背景下，我们提出了第一个自动化的统一调试框架 FixAgent，通过LLM代理的协同工作。FixAgent 能够执行端到端的定位、修复和错误分析。我们的见解是，LLMs
    可以从人类开发者在调试中认可的通用软件工程原则中受益，例如橡皮鸭调试，这有助于更好地理解程序功能和逻辑错误。因此，我们创造了三种受橡皮鸭调试启发的设计来应对这些挑战。它们是
    LLM 代理专业化与协同、关键变量跟踪和程序上下文理解，这要求 LLM 提供明确的解释并强迫它们关注关键的程序逻辑信息。对广泛使用的数据集 QuixBugs
    的实验表明，FixAgent 正确修复了 80 个错误中的 79 个，其中 9 个是以前未曾修复过的。它还在 Codeflaws 上合理地修复了比表现最佳的修复工具多
    1.9 倍的缺陷，即使没有错误定位信息且采样次数少于 0.6%。与其基模型使用不同的 LLMs 相比，FixAgent 的合理和正确修复率提高了约 20%，显示了我们设计的有效性。此外，FixAgent
    的正确率达到了显著的 97.26%，这表明 FixAgent 有可能克服现有方法的过拟合问题。
- en: I Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: In an era where software systems are ubiquitous, permeating every facet of modern
    life, the inevitability of software bugs is a stark reality. These bugs, far from
    being mere nuisances, have the potential to cause catastrophic failures [[1](#bib.bib1)].
    Identifying and rectifying these bugs falls upon developers, who must be ensnared
    in the time-consuming and complex process of debugging [[2](#bib.bib2)]. It is
    reported that developers usually spend over 50% of their programming time on debugging,
    and the cost of debugging amounts to billions of dollars per year [[3](#bib.bib3)].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件系统无处不在、渗透到现代生活的每一个方面的时代，软件漏洞的不可避免性是一种严峻的现实。这些漏洞不仅仅是麻烦，它们有可能导致灾难性的失败 [[1](#bib.bib1)]。识别和修复这些漏洞的任务落在开发人员的肩上，他们必须被困在耗时且复杂的调试过程中 [[2](#bib.bib2)]。有报告指出，开发人员通常将超过50%的编程时间花费在调试上，调试的成本每年高达数十亿美元 [[3](#bib.bib3)]。
- en: 'The debugging demand calls for automated tools to relieve the manual burden.
    Automated debugging generally consists of two sequent stages: Fault Localization
    (FL) and Automated Program Repair (APR). FL aims to identify precise buggy statements
    and provide a ranked list of suspicious code lines, often the first step in debugging.
    Classic FL analyzes test outcomes to localize faults, using either statistical
    analysis [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] or mutation analysis [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)] to qualify the faulty likelihood of code statements.
    Its effectiveness relies highly on human-written test cases and is thus variable.
    On the other hand, APR attempts to generate correct patches to replace faulty
    code segments. Traditional APR explores the space of possible patches [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)] built by pre-defined fix patterns or synthesizes
    patches via symbolic execution [[13](#bib.bib13), [14](#bib.bib14)] based on human-written
    test cases. However, the search spaces may only contain very few correct patches [[15](#bib.bib15)],
    and such methods require extensive customization to re-implement fix patterns
    when transformed across different programming languages. Learning-based techniques
    have shown promise in both areas. Learning-based FL [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)] models program behavior from source code,
    execution features, and test outcomes to localize bugs. Learning-based APR [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)] often “translates” buggy code snippets into
    fixes via neural machine translation (NMT), despite its heavy reliance on high-quality
    bug-fix pairs for training or fine-tuning [[23](#bib.bib23)]. Large Language Models
    (LLMs) have been regarded as the most effective learning models for coding-related
    tasks, including debugging. A recent study [[24](#bib.bib24)] shows that directly
    applying LLMs can significantly outperform advanced APR techniques. Other LLM-based
    FL [[19](#bib.bib19)] and APR studies [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)] also obtain promising results.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 调试的需求呼唤自动化工具来减轻人工负担。自动化调试通常包括两个顺序的阶段：故障定位（FL）和自动程序修复（APR）。FL旨在识别精确的错误语句，并提供一个可疑代码行的排名列表，通常是调试的第一步。经典的FL通过分析测试结果来定位故障，使用统计分析 [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)] 或变异分析 [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]
    来评估代码语句的故障可能性。其效果高度依赖于人工编写的测试用例，因此具有一定的变动性。另一方面，APR尝试生成正确的补丁来替换有故障的代码段。传统的APR通过预定义的修复模式探索可能的补丁空间 [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)] 或通过符号执行 [[13](#bib.bib13), [14](#bib.bib14)]
    基于人工编写的测试用例来合成补丁。然而，搜索空间中可能仅包含很少的正确补丁 [[15](#bib.bib15)]，这些方法在跨编程语言时需要大量的定制来重新实现修复模式。基于学习的技术在这两个领域都显示出了希望。基于学习的FL [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)] 从源代码、执行特征和测试结果中建模程序行为以定位错误。基于学习的APR [[20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22)] 通常通过神经机器翻译（NMT）将错误的代码片段“翻译”成修复，尽管它对高质量的错误修复对的训练或微调有很高的依赖 [[23](#bib.bib23)]。大语言模型（LLMs）被认为是最有效的学习模型，适用于包括调试在内的编码相关任务。最近的一项研究 [[24](#bib.bib24)]
    显示，直接应用LLMs可以显著优于先进的APR技术。其他基于LLM的FL [[19](#bib.bib19)] 和APR研究 [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)] 也取得了令人鼓舞的成果。
- en: 'However, both traditional and LLM-based debugging tools still face three main
    challenges: 1) Imperfect fault localization Previous studies assume off-the-shelf
    FL tools perfectly identify bug locations, so APR should only patch the suspicious
    code statements. Yet, existing FL techniques show limited effectiveness in practice [[29](#bib.bib29),
    [30](#bib.bib30)], and the performance of APR could be largely biased by FL results [[31](#bib.bib31)].
    2) Struggling with complex logic bugs. Though LLMs have shown human-like logic
    understanding abilities [[32](#bib.bib32)], they still struggle to repair complex
    logic errors, as debugging is a multi-step reasoning process, which is challenging
    for models that rely on pattern recognition rather than genuine thinking. When
    the program structure is complex or poorly documented, LLMs may perform even worse
    than small models [[33](#bib.bib33)]. 3) Context ignorance. Debugging demands
    understanding both the purpose and the broader operational context of a program,
    including intended outcomes and potential side effects. Most LLMs are trained
    on file-level source code, lacking the ability to analyze dependencies. Plus,
    LLM-based APR considers the code and test outcomes only, ignoring the informative
    program contexts, such as variable scopes, function definitions, and external
    libraries, limiting the debugging capability of LLMs.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论是传统的还是基于LLM的调试工具仍然面临三个主要挑战：1) 不完美的故障定位。以往的研究假设现成的故障定位工具可以完美地识别错误位置，因此APR只应修补可疑的代码语句。然而，现有的故障定位技术在实践中效果有限[[29](#bib.bib29),
    [30](#bib.bib30)]，APR的性能可能受到故障定位结果的很大影响[[31](#bib.bib31)]。2) 处理复杂逻辑错误的困难。尽管LLM展示了类似人类的逻辑理解能力[[32](#bib.bib32)]，但它们仍然难以修复复杂的逻辑错误，因为调试是一个多步骤推理过程，这对依赖模式识别而非真正思考的模型来说是一个挑战。当程序结构复杂或文档不完整时，LLM的表现可能甚至不如小型模型[[33](#bib.bib33)]。3)
    上下文忽视。调试要求理解程序的目的和更广泛的操作背景，包括预期结果和潜在副作用。大多数LLM在文件级源代码上进行训练，缺乏分析依赖性的能力。此外，基于LLM的APR只考虑代码和测试结果，而忽视了信息丰富的程序上下文，如变量作用域、函数定义和外部库，限制了LLM的调试能力。
- en: 'Our insight: LLMs closely mimic developers when performing coding-related tasks,
    so they can benefit from general software engineering principles. We adopt the
    principle of rubber duck debugging (or rubber ducking), a debugging method where
    developers articulate their code in spoken natural languages, often line by line.
    Inspired by rubber ducking, we propose FixAgent, the first unified, automated
    debugging framework via LLM multi-agent synergy.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的见解：LLM在执行与编码相关的任务时与开发者非常相似，因此可以受益于通用的软件工程原则。我们采用了橡皮鸭调试（或橡皮鸭法则）这一原则，即开发者以自然语言口头表达他们的代码，通常逐行进行。受到橡皮鸭调试的启发，我们提出了FixAgent，这是第一个通过LLM多代理协同的统一自动化调试框架。
- en: 'Specifically, we create three main designs to address the above challenges:
    1) Specialized agent synergy. We first specialize two LLM agents to serve as a
    bug localizer and program repairer, respectively, to complete multi-stage debugging,
    followed by an extra LLM agent to analyze the bug-repair pair. Each agent explains
    its work to a “rubber duck” in detail. Their synergy delivers program repairs
    with explanations without any prior bug locations. In addition, the repairer may
    correct the mistakes made by the localizer, i.e.,, patching code statements beyond
    those identified by the localizer. If a plausible patch is generated and the changed
    code elements are different from the localization, FixAgent will adjust the localization
    results. Moreover, explaining to a rubber duck separately guides the agents in
    focusing on a specific task and better assists developers since over 85% of developers
    want to know the rationale behind automated debugging [[34](#bib.bib34)]. 2) Intermediate
    variable tracking. We prompt each agent to explicitly track key variables at critical
    points in the buggy program and discuss how such tracking guides their task completion.
    This strategy forces agents to analyze the code along the logic execution paths
    and provide more bug-oriented explanations. We do not require a line-by-line explanation
    as the original rubber ducking does because a buggy program (with its context)
    can be very long, which may degrade the performance of LLMs or even extend the
    window length that LLMs can handle  [[35](#bib.bib35)]. 3) Program context construction.
    We construct the program context with respect to its specifications and dependencies.
    The context is provided along with the buggy program to FixAgent. Program specifications
    can include a functionality description, input/output format or examples, variable
    scopes, etc. Afterward, we parse the dependencies between files inside a repository
    to align with real-world projects. These two parts consist of the context. We
    also encourage agents to analyze the buggy program against the given context to
    obtain more attention to the context.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们创建了三个主要设计来应对上述挑战：1) 专门化代理协同。我们首先将两个LLM代理分别专门化为错误定位器和程序修复器，以完成多阶段调试，然后再添加一个LLM代理来分析错误修复对。每个代理详细地向“橡皮鸭”解释其工作。它们的协同作用可以在没有任何先前错误位置的情况下提供程序修复和解释。此外，修复器可能会纠正定位器所犯的错误，即修补定位器未识别的代码语句。如果生成了一个合理的修补程序，并且更改的代码元素与定位不同，FixAgent将调整定位结果。此外，单独向橡皮鸭解释可以引导代理集中于特定任务，并更好地帮助开发人员，因为超过85%的开发人员希望了解自动调试的原理[[34](#bib.bib34)]。2)
    中间变量跟踪。我们提示每个代理在有缺陷的程序中的关键点显式跟踪关键变量，并讨论这种跟踪如何指导他们完成任务。这一策略迫使代理沿逻辑执行路径分析代码，并提供更多面向错误的解释。我们不要求像原始橡皮鸭方法那样逐行解释，因为有缺陷的程序（及其上下文）可能非常长，这可能会降低LLM的性能或甚至扩展LLM能够处理的窗口长度[[35](#bib.bib35)]。3)
    程序上下文构建。我们根据程序的规格和依赖关系构建程序上下文。上下文与有缺陷的程序一起提供给FixAgent。程序规格可以包括功能描述、输入/输出格式或示例、变量范围等。之后，我们解析仓库内文件之间的依赖关系，以对齐真实世界的项目。这两个部分构成了上下文。我们还鼓励代理将有缺陷的程序与给定的上下文进行分析，以获得更多的上下文关注。
- en: Experimental results demonstrate the superiority of FixAgent. We compare FixAgent
    against 16 baselines, including 10 state-of-the-art APR tools and 6 base LLMs.
    The comparison experiments are conducted on two widely used datasets, QuixBugs [[36](#bib.bib36)]
    and Codeflaws [[37](#bib.bib37)], written in three programming languages (C, Python,
    and Java). Overall, FixAgent patches 2780 bugs (passing all test cases) out of
    3982 defects on these two datasets, with an estimated correctness rate of 96.5%
    on average. It outperforms all baselines significantly, even without any prior
    bug locations, whereas previous APR tools use ground-truth bug location information.
    For QuixBugs, it correctly fixes 79 out of 80 real-world bugs, including 10 bugs
    that have not been fixed before. FixAgent fixes 24 more bugs than the best-performing
    APR baseline (AlphaRepair). Plus, FixAgent patches 586 defects than the best LLM
    competitor (GPT4) on Codeflaws. We also conduct extensive ablation studies on
    Codeflaws and a recently collected dataset, ConDefects (Python and Java), to mitigate
    the threat of data leakage in LLM training. FixAgent can fix 368 bugs out of 600
    sampled bugs in ConDefects, with 375 plausibly patched. Results show that FixAgent
    performs well using various LLMs, not limited to its original setting (GPT4),
    and makes remarkable improvements compared to its base model. The studies further
    demonstrate that each design contributes to FixAgent positively.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果展示了 FixAgent 的优越性。我们将 FixAgent 与 16 个基线方法进行比较，包括 10 个最先进的自动化程序修复工具和 6 个基础大语言模型。比较实验在两个广泛使用的数据集
    QuixBugs [[36](#bib.bib36)] 和 Codeflaws [[37](#bib.bib37)] 上进行，这些数据集用三种编程语言（C、Python
    和 Java）编写。总体而言，FixAgent 在这两个数据集上修复了 2780 个缺陷（通过所有测试用例），在平均 96.5% 的估计正确率下。它显著超越了所有基线方法，即使在没有任何先验错误位置的情况下，而之前的自动化程序修复工具使用了真实错误位置的信息。对于
    QuixBugs，它正确修复了 80 个真实世界的错误中的 79 个，包括 10 个之前未被修复的错误。FixAgent 比表现最佳的自动化程序修复基线（AlphaRepair）多修复了
    24 个错误。此外，FixAgent 在 Codeflaws 上比表现最佳的大语言模型竞争对手（GPT4）修复了 586 个缺陷。我们还在 Codeflaws
    和最近收集的数据集 ConDefects（Python 和 Java）上进行了大量的消融研究，以减少大语言模型训练中的数据泄露威胁。FixAgent 能在
    ConDefects 中修复 600 个样本错误中的 368 个，可能修复了 375 个。结果表明，FixAgent 在使用各种大语言模型时表现良好，而不仅限于其原始设置（GPT4），并且相较于其基础模型有显著改进。这些研究进一步表明，每个设计都对
    FixAgent 的表现有积极贡献。
- en: 'Our main contributions are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direction: We are the first to propose that LLMs can benefit from a software
    engineering principle, rubber ducking, to enhance debugging performance.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方向：我们首次提出大语言模型可以借鉴软件工程原则——橡皮鸭调试法，以提高调试性能。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Approach: We designed the first automated, unified debugging framework based
    on LLM multi-agent synergy, inspired by rubber ducking. Our method can effectively
    produce repairs and explanations without knowing bug locations. The implementation
    is released at [[38](#bib.bib38)].'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法：我们设计了第一个基于大语言模型多智能体协同的自动化统一调试框架，灵感来源于“橡皮鸭调试法”。我们的方法可以有效地产生修复和解释，而无需知道错误位置。该实现已发布在
    [[38](#bib.bib38)]。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evalution: We conduct extensive experiments to evaluate FixAgent. The results
    show that FixAgent can significantly outperform baselines on widely used datasets.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 评估：我们进行了大量实验来评估 FixAgent。结果显示，FixAgent 在广泛使用的数据集上明显优于基线方法。
- en: II Background
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A Terminology
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 术语
- en: Software debugging is the process of tracking and fixing issues, such as bugs
    and vulnerabilities, where revising the source code is essential. Code debugging
    involves Fault Localization (FL), Automated Program Repair (APR), and possible
    post-error review. FL attempts to precisely identify buggy elements within a faulty
    program,through static or dynamic analysis. It calculates the probability of each
    code element being buggy to automatically produce a ranked list of suspicious
    code elements. Using this ranked list, APR then automatically patches the identified
    buggy code segments, consisting of patch generation and patch verification. A
    plausible patch can pass all human-written test cases, and it is correct when
    manually verified by developers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 软件调试是跟踪和修复问题的过程，如错误和漏洞，其中修订源代码是必不可少的。代码调试涉及故障定位（FL）、自动化程序修复（APR）和可能的错误后评审。故障定位尝试通过静态或动态分析准确识别故障程序中的错误元素。它计算每个代码元素为错误的概率，以自动生成一个可疑代码元素的排名列表。使用该排名列表，APR
    随后自动修补识别出的错误代码段，包括补丁生成和补丁验证。一个合理的补丁可以通过所有人工编写的测试用例，并且在开发人员手动验证时是正确的。
- en: Unified debugging [[39](#bib.bib39)] is a pioneering work that aims to better
    combine FL and APR. It leverages repair information to improve FL, believing that
    if a patch passes originally failing cases, its patch locations should be highly
    correlated with the groundtruth bug locations. Unified debugging highlights the
    important connection between APR and FL, thereby applying to our work. Differently,
    we aim to provide fixed programs using an end-to-end solution, where the FL and
    APR have an interactive but not determinative relationship. Such an architecture
    allows patching code elements beyond those localized by FL, and the FL results
    can also be adjusted based on repairs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 统一调试 [[39](#bib.bib39)] 是一项开创性的工作，旨在更好地结合 FL 和 APR。它利用修复信息来改善 FL，认为如果一个修补程序通过了最初失败的案例，则其修补位置应与真实错误位置高度相关。统一调试强调
    APR 和 FL 之间的重要联系，从而适用于我们的工作。不同的是，我们旨在提供使用端到端解决方案的修复程序，其中 FL 和 APR 具有互动但非决定性的关系。这种架构允许修补超出
    FL 定位的代码元素，FL 结果也可以根据修复进行调整。
- en: Rubber duck debugging (aka. rubber ducking) is a debugging method that forces
    developers to explain their code by speaking out their expectations and the real
    implementation to find the gap. The original rubber ducking requires explaining
    the code line by line, but following the essential idea—breaking code into pieces
    and articulating them in natural languages—can be beneficial already.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 橡皮鸭调试（又称为橡皮鸭调试）是一种迫使开发者通过大声讲述他们的期望和实际实现来解释代码的调试方法，以发现差距。最初的橡皮鸭调试要求逐行解释代码，但遵循其核心理念——将代码分解并用自然语言表达——已经能带来好处。
- en: II-B Large Language Models
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 大型语言模型
- en: Large Language Models (LLMs) have attained significant advancements in natural
    language processing, including text generation, conversational engagement, and
    logical reasoning [[40](#bib.bib40)]. LLMs is trained to predict tokens auto-regressively
    within a given textual context. This paradigm facilitates the unsupervised training
    on massive corpora of text sourced from the internet, removing the need for labeled
    datasets. In light of the remarkable success of general-purpose LLMs, Code LLMs
    have been extensively studied as code generation is exactly like text generation.
    These models are trained on code corpus (perhaps containing related natural languages).
    For example, DeepSeek-Coder [[41](#bib.bib41)] is trained on 2 trillion tokens
    crawled from GitHub and StackExchange, where 87% are code and 10% are code-related
    text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言处理方面取得了显著进展，包括文本生成、对话参与和逻辑推理 [[40](#bib.bib40)]。LLMs 被训练来在给定的文本上下文中自回归地预测标记。这种范式促进了对来自互联网的大量文本语料库的无监督训练，消除了对标记数据集的需求。鉴于通用
    LLM 的显著成功，代码 LLM 已被广泛研究，因为代码生成与文本生成非常相似。这些模型在代码语料库（可能包含相关的自然语言）上进行训练。例如，DeepSeek-Coder
    [[41](#bib.bib41)] 在从 GitHub 和 StackExchange 爬取的 2 万亿标记上进行训练，其中 87% 是代码，10% 是与代码相关的文本。
- en: LLMs are typically used with a prompt–an instruction to an LLM, initializing
    the LLM to perform inference and generate text until the model encounters a predetermined
    stop word or surpasses its designated maximum word limit. Through the deliberate
    construction of prompts, i.e., prompt engineering, researchers have harnessed
    the capabilities of models for a myriad of tasks without necessitating retraining
    or fine-tuning. In this paper, we adopt GPT4 [[42](#bib.bib42)] for automated
    debugging via prompt engineering. GPT4 is a state-of-the-art LLM with advanced
    understanding and reasoning capabilities in both natural languages and code.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 通常与提示一起使用——对 LLM 的指令，初始化 LLM 进行推理并生成文本，直到模型遇到预定的停止词或超出指定的最大词数限制。通过精心构造提示，即提示工程，研究人员利用模型的能力执行各种任务，而无需重新训练或微调。在本文中，我们采用
    GPT4 [[42](#bib.bib42)] 通过提示工程进行自动调试。GPT4 是一个最先进的 LLM，具有在自然语言和代码中的高级理解和推理能力。
- en: III Motivation
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 动机
- en: 'This section motivates FixAgent, an LLM-based unified debugging framework empowered
    by rubber ducking. Our motivation centers on three challenges of existing debugging
    tools: imperfect fault localization, complex bug repair, and context ignorance,
    as introduced in $\S$[I](#S1 "I Introduction ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") Introduction.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 FixAgent，这是一个基于 LLM 的统一调试框架，灵感来自橡皮鸭调试。我们的动机集中在现有调试工具的三个挑战上：不完善的故障定位、复杂的
    bug 修复以及上下文忽视，这些在$\S$[I](#S1 "I Introduction ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") 引言中有所介绍。
- en: III-A Imperfect Fault Localization
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 不完善的故障定位
- en: Imperfect FL results include wrong and missing locations. Such imperfection
    can considerably affect the downstream repair.= Firstly, FL tools can incorrectly
    identify non-buggy code as the source of the error, making the repair approach
    generate patches that are not only unnecessary but could also introduce new errors.
    As reported in [[19](#bib.bib19)], even state-of-the-art FL approaches suffer
    from low accuracy. Prior techniques can only achieve less than 22.3%, and 46.3%
    real faulty statements are ranked as the top-1 and top-5 suspicious ones, respectively,
    on the real-world dataset Defects4J V1.2.0 [[43](#bib.bib43)]. The result is far
    from satisfactory, and the patch space established based on such FL results can
    be problematic. The overwhelming majority of APR tools are confined to replacing
    the identified statements with those produced by APR, potentially assuming perfect
    localization is easily available. Such an assumption is unrealistic and affects
    the debugging performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不完善的 FL 结果包括错误和遗漏的位置。这种不完善可以显著影响下游修复。首先，FL 工具可能错误地将非错误代码识别为错误源，从而使修复方法生成不仅不必要的修补程序，还可能引入新的错误。如 [[19](#bib.bib19)] 所报道，即使是最先进的
    FL 方法也存在低准确率。先前的技术只能达到不到 22.3%，而 46.3% 的实际错误语句在真实世界数据集 Defects4J V1.2.0 上分别排名为
    top-1 和 top-5 可疑语句 [[43](#bib.bib43)]。结果远未令人满意，而基于这些 FL 结果建立的修补空间可能存在问题。绝大多数 APR
    工具局限于用 APR 生成的语句替换识别出的语句，可能假设完美的定位很容易获得。这种假设是不切实际的，影响了调试性能。
- en: Moreover, FL tools may miss faulty statements. First, most FL and APR tools
    assume that each program only contains one bug existing in an existing code statement [[44](#bib.bib44)].
    This is because 1) identifying multi-line bugs and making edits at multiple and
    non-contiguous locations is especially challenging [[45](#bib.bib45)]; and 2)
    APR tools often rely on spectrum-based FL, which can only isolate single-line
    bugs [[46](#bib.bib46)]. Second, failing case-dependent FL cannot identify bugs
    caused by non-existing statements. Such methods count the execution times of a
    certain statement, so they have difficulties finding missing conditions. However,
    real-world bugs are complex and diverse, which may be multi-line or missing-line,
    calling for more sophisticated debugging to mitigate the threat of localization
    imperfection.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，FL 工具可能会遗漏错误语句。首先，大多数 FL 和 APR 工具假设每个程序仅包含一个存在于现有代码语句中的错误 [[44](#bib.bib44)]。这是因为
    1) 识别多行错误并在多个非连续位置进行编辑尤其具有挑战性 [[45](#bib.bib45)]; 2) APR 工具通常依赖于基于谱的 FL，这只能孤立单行错误 [[46](#bib.bib46)]。其次，失败案例依赖的
    FL 无法识别由不存在的语句引起的错误。这些方法计算某个语句的执行次数，因此在寻找缺失条件时存在困难。然而，现实世界中的错误复杂多样，可能是多行的或缺失的，这需要更复杂的调试来减轻定位不完善的威胁。
- en: III-B Complex Logic Bug Fixing
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 复杂逻辑错误修复
- en: The emergence of LLMs shed light on a potential solution to debugging. A single
    LLM can already complete debugging. It simply regards debugging as producing tokens
    of a repair by calculating the probabilities from left to right, conditioned on
    the buggy program. A recent study comprehensively evaluates the debugging capability
    of LLMs [[47](#bib.bib47)]. It prompts LLMs to fix a buggy program without any
    other prior knowledge. Results show that the most advanced model, GPT4, can achieve
    comparable performance with humans on LeetCode, an online programming platform,
    submissions. However, LLMs still face huge difficulties in fixing logic errors,
    and even runtime information of failing cases is unhelpful for such errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的出现为调试提供了潜在的解决方案。一种 LLM 已经能够完成调试。它仅仅将调试视为通过从左到右计算概率来生成修复的令牌，以 buggy 程序为条件。一项最新研究全面评估了
    LLM 的调试能力 [[47](#bib.bib47)]。它促使 LLM 修复有缺陷的程序而无需任何其他先验知识。结果显示，最先进的模型 GPT4 在 LeetCode
    上的表现可以与人类相媲美，该平台是一个在线编程平台。然而，LLM 在修复逻辑错误方面仍面临巨大困难，即使是失败案例的运行时信息也对这些错误无济于事。
- en: '![Refer to caption](img/1e75324c84cad63543470ad072a5b7b6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1e75324c84cad63543470ad072a5b7b6.png)'
- en: 'Figure 1: Complex bug fixing is still challenging for LLMs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：复杂的错误修复对 LLM 仍然具有挑战性。
- en: We also observe a similar phenomenon. Figure [1](#S3.F1 "Figure 1 ‣ III-B Complex
    Logic Bug Fixing ‣ III Motivation ‣ A Unified Debugging Approach via LLM-Based
    Multi-Agent Synergy") displays a complex program with a logic bug and the wrong
    repair of GPT4\. The wrong repair can pass most test cases but fail to calculate
    the number of tank-tap pairs given three houses and only one pipe, a corner case.
    The key to this problem is to construct a directional chain using nodes and edges
    to represent the water-related objects, a thought of abstracting complex real-world
    systems into code. Such insight requirement goes beyond language understanding
    and is challenging to LLMs. Thus, simply applying LLMs cannot meet the growing
    demand for complex software debugging, motivating us to create designs that better
    unleash their capability.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也观察到了类似的现象。图 [1](#S3.F1 "Figure 1 ‣ III-B Complex Logic Bug Fixing ‣ III Motivation
    ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy") 显示了一个复杂的程序，其中包含一个逻辑错误和
    GPT4 的错误修复。错误的修复可以通过大多数测试用例，但在给定三所房子和只有一根管道的情况下，无法计算坦克-水龙头对的数量，这是一个边界情况。这个问题的关键在于构建一个有向链，使用节点和边来表示与水相关的对象，这是一种将复杂的现实世界系统抽象为代码的思路。这种洞察力的要求超出了语言理解的范围，对
    LLM 来说具有挑战性。因此，单纯应用 LLMs 无法满足对复杂软件调试日益增长的需求，这激励我们创建更能发挥其能力的设计。
- en: III-C Context Ignorance
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 上下文忽视
- en: Successful debugging requires a deep understanding of not just the syntax but
    the semantic purpose of code. However, existing APR tools focus on source code
    and test outcomes only, ignoring the important program contexts such as the intended
    functionality, the data flow, and the expected behaviors. Figure [2](#S3.F2 "Figure
    2 ‣ III-C Context Ignorance ‣ III Motivation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") presents a repair generated by an advanced APR
    tool. This repair is also regarded as “ground-truth” provided by the dataset Codeflaws.
    Yet, it ignores the variable scope, so it failed when $m\geq 99$, even an omniscient
    APR tool has trouble producing a correct repair. This case highlights the importance
    of program contexts. Programming is a problem-solving task, so we should conduct
    context-aware program debugging for real-world software.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的调试不仅需要对语法有深刻理解，还需要了解代码的语义目的。然而，现有的 APR 工具仅关注源代码和测试结果，忽视了重要的程序上下文，例如预期的功能、数据流和预期的行为。图
    [2](#S3.F2 "Figure 2 ‣ III-C Context Ignorance ‣ III Motivation ‣ A Unified Debugging
    Approach via LLM-Based Multi-Agent Synergy") 展示了一个由高级 APR 工具生成的修复。这一修复也被视为数据集
    Codeflaws 提供的“真实结果”。然而，它忽视了变量作用域，因此在 $m\geq 99$ 时失败，即使是全知的 APR 工具也难以产生正确的修复。这一案例突显了程序上下文的重要性。编程是一个解决问题的任务，因此我们应当进行上下文感知的程序调试以应对现实世界的软件问题。
- en: '![Refer to caption](img/d3fa478bb5981b0ff8aaf7efb50fbcdc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d3fa478bb5981b0ff8aaf7efb50fbcdc.png)'
- en: 'Figure 2: The repair made by an APR tool (also regarded as correct in the dataset)
    ignores the variable scope requirement.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：APR 工具进行的修复（在数据集中也被认为是正确的）忽视了变量作用域的要求。
- en: 'Insight: LLMs have the potential
    for unified debugging but face challenges of bug localization imperfection, complex
    error fixing, and context ignorance. They motivate us to create designs to unleash
    the debugging capabilities of LLMs. We propose to adopt rubber ducking to boost
    LLMs just like developers.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 洞察：LLMs 具有统一调试的潜力，但面临着错误定位不完善、复杂错误修复和上下文忽视的挑战。这些问题激励我们创建设计以释放
    LLMs 的调试能力。我们建议采用橡皮鸭调试法来提升 LLMs，类似于开发人员的做法。
- en: IV Methodology
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 方法论
- en: This section introduces the key ideas behind FixAgent, our LLM-based unified
    debugging framework. Figure [3](#S4.F3 "Figure 3 ‣ IV Methodology ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy") displays the overview of
    FixAgent, consisting of three specialized agents serving as bug localizer, patch
    generator, and post-error reviewer, respectively ($\S$[IV-D2](#S4.SS4.SSS2 "IV-D2
    Feedback-supported re-sampling ‣ IV-D Secondary Designs ‣ IV Methodology ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy")).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 FixAgent 的关键理念，它是我们基于 LLM 的统一调试框架。图 [3](#S4.F3 "图 3 ‣ IV 方法论 ‣ 通过 LLM
    基于多智能体协作的统一调试方法") 展示了 FixAgent 的概述，由三个专门的智能体组成，分别是错误定位器、补丁生成器和错误后审查者 ($\S$[IV-D2](#S4.SS4.SSS2
    "IV-D2 反馈支持的重新采样 ‣ IV-D 次级设计 ‣ IV 方法论 ‣ 通过 LLM 基于多智能体协作的统一调试方法"))。
- en: '![Refer to caption](img/b63ef81ed04197e8ccc83a00717f0a5c.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b63ef81ed04197e8ccc83a00717f0a5c.png)'
- en: 'Figure 3: Overview of FixAgent.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：FixAgent 概述。
- en: IV-A Specialized Agent Synergy
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 专门化智能体协作
- en: 'We specialize three LLM agents, each responsible for a stage in debugging separately:
    fault localization (localizer), patch generation (repairer), and post-error analysis
    (revisitor). Localizer identifies faulty code statements. It can even point out
    missing statements and label them in the buggy program like “ // missing
    this line causes a bug”. Repairer aims to generate an executable and correct patch.
    Revisitor analyzes why the original code was buggy and the rationale behind the
    patch. The agents work sequentially. Each agent passes its response to the downstream
    agent, and responses from these agents make up the final response.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专门化了三个 LLM 智能体，每个智能体分别负责调试的一个阶段：故障定位（定位器）、补丁生成（修复器）和错误后分析（复审者）。定位器识别有问题的代码语句。它甚至可以指出缺失的语句，并在有问题的程序中标记为“
    // 缺少这一行导致了一个错误”。修复器的目标是生成一个可执行且正确的补丁。复审者分析原始代码为何有问题以及补丁背后的理由。这些智能体按顺序工作。每个智能体将其响应传递给下游智能体，这些智能体的响应组成了最终的回应。
- en: 'Each prompt is a triplet consisting of a role profile, program specifications,
    and instructions. First, each agent is promoted with a clear role profile for
    task-oriented role-playing. LLMs can act like an expected agent if given detailed
    role descriptions[[48](#bib.bib48)]. A role profile consists of an expert identification
    and an agent description. Expert identification determines the role of an agent,
    e.g., “You are an expert in identifying specific faulty code elements.” The agent
    description introduces the specific task objective, e.g., “Your task is to fix
    buggy code snippets.” The role profile is followed by program specifications,
    including the buggy program, failing test case information, program contexts,
    and the response from the previous agent(s), if applicable. For example, we prompt
    the revisitor with identified bug locations generated by the localizer and a repair
    produced by the repairer. Afterward, we underline the task objective and provide
    detailed step-by-step instructions. For example, the repairer has the objective
    of returning the patch with an explanation in the desired format. It should carry
    out a series of steps: context comprehending, program analysis (including variable
    tracking introduced in $\S$[IV-B](#S4.SS2 "IV-B Intermediate Variable Tracking
    ‣ IV Methodology ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"))
    against the failing cases, making minimal changes for a patch, and double-checking
    the identified buggy statements.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示都是一个三元组，包括角色简介、程序规范和指令。首先，每个智能体都被赋予一个明确的角色简介，以进行任务导向的角色扮演。如果提供详细的角色描述，LLMs
    可以表现得像预期中的智能体[[48](#bib.bib48)]。角色简介包括专家识别和智能体描述。专家识别确定了智能体的角色，例如，“你是识别特定故障代码元素的专家。”智能体描述介绍了具体的任务目标，例如，“你的任务是修复有问题的代码片段。”角色简介后是程序规范，包括有问题的程序、失败的测试用例信息、程序上下文以及来自前一个智能体的回应（如适用）。例如，我们用由定位器生成的识别的错误位置和修复器生成的修复来提示复审者。随后，我们强调任务目标并提供详细的逐步指令。例如，修复者的目标是以所需格式返回补丁和解释。它应执行一系列步骤：理解上下文、程序分析（包括在
    $\S$[IV-B](#S4.SS2 "IV-B 中间变量跟踪 ‣ IV 方法论 ‣ 通过 LLM 基于多智能体协作的统一调试方法") 对失败的案例进行变量跟踪）、对补丁进行最小修改，并再次检查识别的有问题的语句。
- en: The downstream agent relies on upstream results while influencing them in turn.
    If the repairer generates a plausible patch that changes code statements different
    from the identified ones, the localizer also adjusts its results based on such
    changes, though it can also present the original responses upon user demand. Similarly,
    the analyzer may contain a better patch to boost the repairer or wiser localization,
    enabling answer adjusting for the localizer. Besides, our pipeline enables FixAgent
    to handle multiple programs in parallel, like an assembly line. The final answer
    returned to the user comes from their synergy, consisting of the identified buggy
    statements, the patch, and the post-error analysis.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下游代理依赖上游结果，同时也会反过来影响它们。如果修复者生成了一个合理的补丁，该补丁更改了与识别出的代码语句不同的代码语句，那么本地化器也会根据这些更改调整其结果，尽管在用户要求时也可以呈现原始响应。类似地，分析器可能包含一个更好的补丁来提升修复者或更明智的本地化，从而使本地化器能够调整答案。此外，我们的管道使FixAgent能够并行处理多个程序，就像一个流水线。最终返回给用户的答案来源于它们的协同，包括识别出的错误语句、补丁和后续错误分析。
- en: IV-B Intermediate Variable Tracking
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 中间变量跟踪
- en: We prompt each agent to track critical intermediate variable values against
    failed test cases and compare them to expected outcomes. Such prompting is positioned
    in the instructions mentioned in the previous section. Each agent is requested
    to explicitly present such tracking in its response, accompanied by a comprehensive
    explanation of how it facilitates the derivation of its answer. This design is
    inspired by the rubber ducking, using explanations to enhance programming. Compared
    with requiring line-by-line explanations as the original rubber ducking does,
    our design prioritizes information with significant impact on the program’s behavior,
    such as the core logic executions and states, helping LLM to concentrate on the
    parts that are most likely to influence the outcome, making it easier to identify
    and solve errors. It also corresponds to chain-of-thought (CoT) [[32](#bib.bib32)],
    whose idea is that decomposing a complex question into pieces can improve the
    reasoning capability of LLMs, and even simply adding “think it step by step” can
    lead to significant improvement. Our tracking design allows LLMs to decompose
    a complex program with multiple logic modules into several intermediate states
    and conduct extra calculations during debugging.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提示每个代理跟踪针对失败测试用例的关键中间变量值，并将其与预期结果进行比较。这种提示在前面提到的指令中进行。每个代理被要求在其响应中明确展示这些跟踪，并附上详细解释，说明这些跟踪如何帮助推导其答案。这个设计受到橡皮鸭子的启发，通过解释来增强编程能力。与原橡皮鸭子需要逐行解释相比，我们的设计优先考虑对程序行为有重大影响的信息，如核心逻辑执行和状态，帮助LLM专注于最可能影响结果的部分，使错误更容易被识别和解决。它还对应于链式思维（CoT）[[32](#bib.bib32)]，其思想是将复杂问题分解成若干部分可以提高LLM的推理能力，即使只是简单地添加“逐步思考”也能带来显著改善。我们的跟踪设计允许LLM将一个具有多个逻辑模块的复杂程序分解为几个中间状态，并在调试过程中进行额外计算。
- en: Moreover, this design enhances the transparency of LLM decision-making. We can
    see how an agent reaches its answer, enabling potential human interactions. For
    example, in a dynamic programming problem, developers can ask the model to focus
    on the state transfer equation and edge conditions. Even if the agent cannot eventually
    generate a correct repair, the thinking path will still provide insights to developers.
    Such interpretation also helps win the trust of developers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种设计提高了LLM决策的透明度。我们可以看到代理如何得出答案，从而支持潜在人类交互。例如，在动态规划问题中，开发者可以要求模型关注状态转移方程和边界条件。即使代理最终无法生成正确的修复，思考路径仍会为开发者提供见解。这种解释也有助于赢得开发者的信任。
- en: IV-C Context Construction
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 上下文构建
- en: FixAgent mines two aspects of the program context, i.e., requirements and dependencies.
    First, for programs with detailed documentation, we adopt descriptions of the
    program functionality, input/output format, precision requirement, and other related
    information to clarify the expected behavior of the program. If the program implements
    a well-known algorithm without documentation, we request a general LLM (may not
    be the base model of FixAgent) to introduce the algorithm given its name (usually
    the function name in the program). The introduction serves as a requirement description.
    Second, we parse the dependencies of the buggy program and extract the code of
    these dependent files. The extracted code is put on to the top of the program.
    This operation, though simple, can work well by ensuring LLMs handles the dependent
    code first and then the buggy program, as most LLMs deal with tokens from left
    to right and are trained on file-level code. These two strategies can construct
    an informative context for the program.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: FixAgent 挖掘程序上下文的两个方面，即需求和依赖关系。首先，对于有详细文档的程序，我们采用程序功能、输入/输出格式、精度要求和其他相关信息的描述，以澄清程序的预期行为。如果程序实现了一个没有文档的著名算法，我们会请求一个通用
    LLM（可能不是 FixAgent 的基础模型）介绍该算法及其名称（通常是程序中的函数名称）。该介绍作为需求描述。其次，我们解析有缺陷程序的依赖关系，并提取这些依赖文件的代码。提取的代码被放置在程序的顶部。这个操作虽然简单，但可以通过确保
    LLM 先处理依赖代码然后处理有缺陷程序来很好地工作，因为大多数 LLM 从左到右处理令牌，并且在文件级代码上进行了训练。这两种策略可以为程序构建有用的上下文。
- en: IV-D Secondary Designs
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 次级设计
- en: IV-D1 Test input generation
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 测试输入生成
- en: We specialize an extra agent, crafter, for test case generation beyond the human-written
    test cases. It is launched when a plausible patch is generated to mitigate the
    overfitting issue, i.e., plausible repairs generated by APR tools pass given test
    cases but cannot generalize on others, especially for test-dependent APR tools [[49](#bib.bib49)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专门为测试用例生成引入了一个额外的代理，称为 crafter，超出人类编写的测试用例。当生成一个可信的修补程序时，它会启动，以减轻过拟合问题，即由
    APR 工具生成的可信修补程序通过给定的测试用例，但无法在其他情况下推广，尤其是对于测试依赖的 APR 工具[[49](#bib.bib49)]。
- en: We also follow the $\langle$n1 2 ... 10000” is generated in an abbreviated form
    to save tokens and cannot be processed by the program. Still, we can extend it
    to a valid input with its explanation “1000 employees, 10000 applications, ....”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还遵循 $\langle$n1 2 ... 10000”以简略形式生成以节省令牌，并且程序无法处理。然而，我们可以将其扩展为有效输入及其解释“1000
    名员工，10000 个应用程序，....”
- en: Note that we need external efforts to calculate the expected outputs of the
    generated inputs to obtain test cases. We do not use LLMs for such calculation
    because they are not good at arithmetic problem-solving [[40](#bib.bib40)] and
    are unsuitable to serve as a standard criterion. In practice, developers can calculate
    the expected outputs for the insightful generated inputs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要外部努力来计算生成输入的预期输出，以获得测试用例。我们不使用 LLM 进行此类计算，因为它们在算术问题解决方面表现不佳[[40](#bib.bib40)]，且不适合作为标准准则。在实际操作中，开发人员可以计算有见地生成输入的预期输出。
- en: IV-D2 Feedback-supported re-sampling
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 支持反馈的重新采样
- en: Errors are inevitable in debugging. We adopt a feedback-supported design to
    reduce wrong repairs. If FixAgent generates an implausible patch, we re-sample
    FixAgent to get another patch, prompted with feedback of failing test information
    in a conversational manner, inspired by [[28](#bib.bib28)]. Assume we allow for
    $m$ that passes the most (or all) test cases serves as the final solution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 调试中不可避免地会出现错误。我们采用支持反馈的设计来减少错误修复。如果 FixAgent 生成了一个不可信的修补程序，我们会重新采样 FixAgent
    以获取另一个修补程序，并通过对话方式提示失败的测试信息，受到[[28](#bib.bib28)]的启发。假设我们允许 $m$ 通过最多（或所有）测试用例作为最终解决方案。
- en: This design aligns FixAgent with previous APR tools, which usually sample thousands
    of candidate patches. Notably, our strategy is different from [[28](#bib.bib28)],
    which keeps one conversation going until reaching a plausible fix, so the prompt
    contains all repairs and their failing information. Instead, we only input one
    repair and its feedback for one sampling, avoiding the possibility of extending
    the token window of an LLM. Lastly, the best repair with bug-fix analysis will
    be returned to the user. Users can also request the specific response of any agent,
    including the corresponding answer and explanation.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设计使 FixAgent 与之前的 APR 工具对齐，这些工具通常会采样数千个候选修复。值得注意的是，我们的策略不同于 [[28](#bib.bib28)]，后者保持一个对话进行直到找到一个可能的修复，因此提示中包含所有修复及其失败信息。相反，我们只输入一个修复及其反馈进行一次采样，避免了扩展
    LLM 的令牌窗口的可能性。最后，最好的修复与缺陷分析将返回给用户。用户还可以请求任何代理的具体响应，包括相应的答案和解释。
- en: V Evaluation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 评估
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ1: How does FixAgent compare against state-of-the-art APR tools and base
    LLMs?'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ1: FixAgent 与最先进的 APR 工具和基础 LLMs 相比如何？'
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ2: How capable is FixAgent in different LLMs and how does FixAgent make improvement
    on its base model?'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ2: FixAgent 在不同的 LLMs 中有多大的能力，以及 FixAgent 如何在其基础模型上进行改进？'
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RQ3: How does each design contribute to FixAgent?'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RQ3: 每种设计如何为 FixAgent 做出贡献？'
- en: V-A Experiment Setup
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 实验设置
- en: We compare FixAgent with advanced APR approaches and general LLMs on public
    databases with pairs of bug-fix. The comparison refers to APR metrics, using the
    number of plausible patches and correct patches, as well as the correctness rate
    (the correct patch count over generated plausible patches) to gauge the effectiveness
    of FixAgent. This is because our work is the first unified debugging framework,
    and the eventual goal is to obtain a correct and executable repair.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在公共数据库中对 FixAgent 与先进的 APR 方法和通用 LLMs 进行了比较，比较内容包括 APR 指标，使用可能修复补丁和正确补丁的数量，以及正确率（正确补丁数量与生成的可能补丁数量之比）来衡量
    FixAgent 的效果。这是因为我们的工作是首个统一的调试框架，最终目标是获得一个正确且可执行的修复。
- en: V-A1 Datasets
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 数据集
- en: We use three datasets for evaluation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了三个数据集进行评估。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Codeflaws [[37](#bib.bib37)] consists of 3902 faulty programs (2952 one-line
    bugs) and their corresponding repairs written in C collected from Codeforces,
    an online programming platform. These bugs are classified into 40 classes, including
    control flow, data flow, function call, pointer, variable type, etc.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Codeflaws [[37](#bib.bib37)] 包含 3902 个有缺陷的程序（2952 个一行缺陷）及其在 Codeforces 上收集的对应修复，这些程序用
    C 语言编写。所有的缺陷被分为 40 类，包括控制流、数据流、函数调用、指针、变量类型等。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: QuixBugs [[36](#bib.bib36)] consists of 40 faulty programs in Java and Python.
    Each program implements a classic algorithm in one function with a one-line defect.
    The bugs involve incorrect operators, variables, and field dereferences, as well
    as missing conditions, arithmetic expressions, etc.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: QuixBugs [[36](#bib.bib36)] 包含 40 个有缺陷的 Java 和 Python 程序。每个程序在一个函数中实现了经典算法，并有一个一行的缺陷。这些缺陷涉及不正确的操作符、变量和字段解引用，以及缺失的条件、算术表达式等。
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ConDefects [[51](#bib.bib51)] consists of 1254 Java and 1625 Python faulty programs
    covering diverse task difficulties, sourced from AtCoder, an online programming
    competition platform. Each faulty program is paired with its repair.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ConDefects [[51](#bib.bib51)] 包含 1254 个 Java 和 1625 个 Python 有缺陷的程序，涵盖了多种任务难度，来源于在线编程竞赛平台
    AtCoder。每个有缺陷的程序都配有其修复。
- en: 'The Codeflaws and QuixBugs are widely used in previous studies, whereas ConDefects
    is recently collected, making it impossible for direct data leakage in the training
    data of existing popular LLMs. Since no studies have reported experimental results
    on this new dataset and repairing all of the faulty programs is especially expensive,
    we sample 300 faulty programs for each programming language and only use them
    to compare FixAgent against base LLMs in RQ2 ($\S$[V-C](#S5.SS3 "V-C RQ2: Using
    different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy")).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 'Codeflaws 和 QuixBugs 在以往研究中被广泛使用，而 ConDefects 刚刚收集到，因此不存在现有流行 LLMs 训练数据中的直接数据泄露问题。由于没有研究报告在这个新数据集上的实验结果，且修复所有有缺陷的程序特别昂贵，我们对每种编程语言采样了
    300 个有缺陷的程序，并仅用它们来比较 FixAgent 与基础 LLMs 在 RQ2 的表现（$\S$[V-C](#S5.SS3 "V-C RQ2: 使用不同的
    LLMs 与 FixAgent ‣ V 评估 ‣ 基于 LLM 的多代理协同统一调试方法")）。'
- en: V-A2 Baselines
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A2 基准
- en: We compare FixAgent against 16 baselines, including 10 APR tools and 6 base
    LLMs. APR methods include three advanced C tools (Angelix [[14](#bib.bib14)],
    Prophet [[52](#bib.bib52)], SPR [[53](#bib.bib53)], CVC4 [[54](#bib.bib54)]),
    two genetic programming-based techniques (Semfix [[13](#bib.bib13)], GenProg [[10](#bib.bib10),
    [11](#bib.bib11)]), two recent NMT-based approaches (CoCoNuT [[21](#bib.bib21)],
    CURE [[22](#bib.bib22)]), and an LLM-based method (AlphaRepair [[27](#bib.bib27)]).
    We adopt the results reported by their original papers and follow-up surveys [[49](#bib.bib49),
    [55](#bib.bib55), [24](#bib.bib24)]. LLM baselines include three general-purpose
    models (Gemini [[56](#bib.bib56)], ChatGPT [[57](#bib.bib57)], GPT4 [[42](#bib.bib42)])
    and three code LLMs (DeepSeek-Coder [[41](#bib.bib41)], CodeLlama [[58](#bib.bib58)],
    Codex [[59](#bib.bib59)]). We also use the reported results of Codex since OpenAI
    no longer supports it. Other LLM baselines are implemented via their official
    APIs, whose versions are Gemini 1.0-Pro, GPT-3.5-Turbo-0125 (about 175 billion
    parameters, i.e., 175B, for simplicity), GPT-4-0125-preview, DeepSeek-Coder-Base
    (33B), and Phind-CodeLlama-V2 (34B), respectively.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 FixAgent 与16个基准进行了比较，包括10种 APR 工具和6种基础 LLMs。APR 方法包括三种高级 C 工具（Angelix [[14](#bib.bib14)]，Prophet [[52](#bib.bib52)]，SPR [[53](#bib.bib53)]，CVC4 [[54](#bib.bib54)]），两种基于遗传编程的技术（Semfix [[13](#bib.bib13)]，GenProg [[10](#bib.bib10)，[11](#bib.bib11)]），两种近期的
    NMT 基础方法（CoCoNuT [[21](#bib.bib21)]，CURE [[22](#bib.bib22)]），以及一种基于 LLM 的方法（AlphaRepair [[27](#bib.bib27)]）。我们采用了原始论文和后续调查报告的结果 [[49](#bib.bib49)，[55](#bib.bib55)，[24](#bib.bib24)]。LLM
    基准包括三种通用模型（Gemini [[56](#bib.bib56)]，ChatGPT [[57](#bib.bib57)]，GPT4 [[42](#bib.bib42)]）和三种代码
    LLMs（DeepSeek-Coder [[41](#bib.bib41)]，CodeLlama [[58](#bib.bib58)]，Codex [[59](#bib.bib59)]）。由于
    OpenAI 不再支持 Codex，我们还使用了 Codex 的报告结果。其他 LLM 基准通过其官方 API 实现，版本分别为 Gemini 1.0-Pro，GPT-3.5-Turbo-0125（大约
    1750 亿参数，简写为 175B），GPT-4-0125-preview，DeepSeek-Coder-Base（33B）和 Phind-CodeLlama-V2（34B）。
- en: 'TABLE I: Comparison with APR tools and base LLMs. #Correct and #Plausible represent
    the number of bugs correctly and plausibly patched, respectively. #Patch/bug is
    the sampling number per bug. Cells filled in by $(x,y)$ sampled bugs. Other cells
    are obtained on the whole dataset.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：与 APR 工具和基础 LLMs 的比较。#正确和#合理分别表示正确和合理修补的错误数量。#补丁/错误是每个错误的采样数量。用 $(x,y)$
    填充的单元格表示采样的错误。其他单元格基于整个数据集。
- en: 'Fault Localization Tools #Patch/bug Codeflaws-C (3902 bugs) QuixBugs-Java (40
    bugs) QuixBugs-Python (40 bugs) #Correct #Plausible #Correct #Plausible #Correct
    #Plausible Standard Angelix [[14](#bib.bib14)] 1000 318 591 - - - - Prophet [[52](#bib.bib52)]
    1000 310 839 - - - - SPR [[53](#bib.bib53)] 1000 283 783 - - - - CVC4 [[54](#bib.bib54)]
    - (15, 665) (91, 665) - - - - Semfix [[13](#bib.bib13)] - (38, 665) (56, 655)
    - - - - Perfect GenProg [[10](#bib.bib10), [11](#bib.bib11)] 1000 255-369 1423
    1 4 - - RewardRepair [[60](#bib.bib60)] 200 - - 20 - - - CoCoNuT [[21](#bib.bib21)]
    20000 423 716 13 20 19 21 CURE [[22](#bib.bib22)] 5,000 - - 26 35 - - AlphaRepair [[27](#bib.bib27)]
    5000 - - 28 30 27 32 Codex [[59](#bib.bib59)] 600 - - 32 35 37 37 None^⋆ CodeLlama [[58](#bib.bib58)]
    3 91^† 1353 25 28 33 33 Gemini [[56](#bib.bib56)] 3 89^† 1186 29 32 29 35 DeepSeek-Coder [[41](#bib.bib41)]
    3 93^† 1741 30 34 25 38 ChatGPT [[57](#bib.bib57)] 3 94^† 1927 33 34 34 36 GPT4 [[42](#bib.bib42)]
    3 93^† 2115 35 36 39 39 FixAgent 3 93^† 2701 39 39 40 40'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '故障定位工具 #补丁/错误 代码缺陷-C (3902 个错误) QuixBugs-Java (40 个错误) QuixBugs-Python (40
    个错误) #正确 #合理 #正确 #合理 #正确 #合理 标准 Angelix [[14](#bib.bib14)] 1000 318 591 - - -
    - Prophet [[52](#bib.bib52)] 1000 310 839 - - - - SPR [[53](#bib.bib53)] 1000
    283 783 - - - - CVC4 [[54](#bib.bib54)] - (15, 665) (91, 665) - - - - Semfix [[13](#bib.bib13)]
    - (38, 665) (56, 655) - - - - Perfect GenProg [[10](#bib.bib10), [11](#bib.bib11)]
    1000 255-369 1423 1 4 - - RewardRepair [[60](#bib.bib60)] 200 - - 20 - - - CoCoNuT [[21](#bib.bib21)]
    20000 423 716 13 20 19 21 CURE [[22](#bib.bib22)] 5000 - - 26 35 - - AlphaRepair [[27](#bib.bib27)]
    5000 - - 28 30 27 32 Codex [[59](#bib.bib59)] 600 - - 32 35 37 37 None^⋆ CodeLlama [[58](#bib.bib58)]
    3 91^† 1353 25 28 33 33 Gemini [[56](#bib.bib56)] 3 89^† 1186 29 32 29 35 DeepSeek-Coder [[41](#bib.bib41)]
    3 93^† 1741 30 34 25 38 ChatGPT [[57](#bib.bib57)] 3 94^† 1927 33 34 34 36 GPT4 [[42](#bib.bib42)]
    3 93^† 2115 35 36 39 39 FixAgent 3 93^† 2701 39 39 40 40'
- en: $\star$
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\star$
- en: For LLMs understanding both code and natural languages, no fault localization
    information is provided for unified debugging.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于大语言模型（LLMs）理解代码和自然语言的问题，统一调试时没有提供故障定位信息。
- en: $\dagger$
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: $\dagger$
- en: We randomly select 100 plausible patches to check their correctness because
    of the huge number of plausible patches.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于合理补丁的数量庞大，我们随机选择了100个可能的补丁来检查它们的正确性。
- en: V-A3 Implementations
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A3 实现
- en: A single base LLM performs the whole debugging process without our designs.
    To ensure a fair comparison, we replace the variable tracking prompt with “Think
    it step by step,” a well-known prompt enhancing the reasoning ability of LLMs [[32](#bib.bib32)].
    We also retain the program specifications when prompting the base models. Plus,
    FixAgent is implemented by Python3.9 with 1500+ lines of code. Following prior
    work [[27](#bib.bib27)], the generation uses the top-p nucleus sampling [[61](#bib.bib61)]
    with $p=1.0$.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 单一基础LLM在没有我们设计的情况下完成整个调试过程。为了确保公平比较，我们用“逐步思考”替换了变量跟踪提示，这是一种著名的提示，可以增强LLM的推理能力[[32](#bib.bib32)]。我们在提示基础模型时也保留了程序规格。此外，FixAgent是用Python3.9实现的，包含1500多行代码。遵循之前的工作[[27](#bib.bib27)]，生成使用的是top-p核采样[[61](#bib.bib61)]，其中$p=1.0$。
- en: Previous APR usually generates hundreds to thousands of patches per bug, whereas
    we only sample at most three times per bug for LLM baselines due to the high cost
    of token generation. Comparing thousands of samples (APR studies) with only three
    samples (LLM baselines) seems unfair. Nevertheless, we follow the tradition of
    sampling only several times in code generation evaluation on LLMs [[62](#bib.bib62)],
    and the performance of LLMs is already incredible. For evaluation purposes, we
    manually check the plausible patches. A patch is correct only when semantically
    equivalent to its corresponding reference patch or official answer in the programming
    contest platform. We carefully check the correctness of each plausible patch of
    QuixBugs and 100 sampled plausible patches of Codeflaws in RQ1. We do not check
    all patches of Codeflaws because 1) LLMs and FixAgent generate too many (thousands
    of) plausible patches, so it is extremely time-consuming to check them one by
    one; and 2) without ground-truth fault locations, these LLMs rewrite programs
    and change code statements, for example, replacing multiple “if else” by “switch
    case”, making it very different to manually check the correctness. Thus, we follow [[21](#bib.bib21)]
    to check sampled patches and report the correctness rate instead of the correct
    patch counts.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的APR通常为每个bug生成数百到数千个修补程序，而由于生成token的高成本，我们对LLM基线每个bug最多只采样三次。将数千个样本（APR研究）与仅三个样本（LLM基线）进行比较似乎不公平。然而，我们遵循了在LLM的代码生成评估中仅采样几次的传统[[62](#bib.bib62)]，LLM的表现已经非常出色。为了评估目的，我们手动检查了可能的修补程序。只有当修补程序在语义上等同于其相应的参考修补程序或编程竞赛平台上的官方答案时，才认为其正确。我们仔细检查了RQ1中QuixBugs的每一个可能修补程序和Codeflaws中100个采样的可能修补程序。我们没有检查Codeflaws中的所有修补程序，因为1)
    LLM和FixAgent生成的可能修补程序过多（数千个），逐一检查非常耗时；2) 由于没有真实的故障位置，这些LLM重写程序并更改代码语句，例如，将多个“if
    else”替换为“switch case”，使得手动检查正确性非常困难。因此，我们遵循[[21](#bib.bib21)]检查采样的修补程序，并报告正确率，而不是正确修补程序的数量。
- en: 'V-B RQ1: Overall Effectiveness of FixAgent'
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'V-B RQ1: FixAgent的总体效果'
- en: This section presents the overall comparison between FixAgent and baselines
    on Codeflaws and QuixBugs among C, Java, and Python. Since different APR approaches
    adopted different FL tools, we separate them according to the FL types adopted
    by their original papers, where standard FL refers to a traditional spectrum-based
    FL. Perfect FL assumes a APR tool knows the ground-truth bug locations, though
    it is unrealistic. Our work aims at an end-to-end solution without knowing the
    bug locations, so we do not provide FL results to our method and the base LLM
    baselines (i.e., None for the first column).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了FixAgent与基线在C、Java和Python的Codeflaws和QuixBugs上的总体比较。由于不同的APR方法采用了不同的FL工具，我们根据原始论文采用的FL类型将它们分开，其中标准FL指的是传统的基于谱的FL。完美FL假设APR工具知道真实的bug位置，尽管这不现实。我们的工作旨在提供一个端到端的解决方案，而不需要知道bug的位置，因此我们不为我们的方法和基础LLM基线提供FL结果（即第一列为None）。
- en: 'The results are shown in Table [I](#S5.T1 "Table I ‣ V-A2 Baselines ‣ V-A Experiment
    Setup ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent
    Synergy"). Overall, FixAgent plausibly patches 2780 out of 3982 bugs on Codeflaws
    and QuixBugs, performing the best compared to all baselines. In particular, on
    the Codeflaws dataset, FixAgent significantly outperforms existing APR methods
    and LLMs. It produces 1.9X plausible patches as the best APR method, GenProg.
    Moreover, it improves the correctness rate by 57.42% compared to that of CoCoNuT,
    which correctly fixes the most bugs among APR methods. We can also roughly estimate
    that FixAgent correctly fixes about 2512 bugs based on the correctness rate. Moreover,
    FixAgent outperforms all compared approaches on QuixBugs. It correctly fixes 39
    bugs in QuixBugs-Java, 7 of which have never been fixed before. FixAgent correctly
    fixes all bugs in QuixBugs-Python, showing superiority over all baselines. Figure [4](#S5.F4
    "Figure 4 ‣ V-B RQ1: Overall Effectiveness of FixAgent ‣ V Evaluation ‣ A Unified
    Debugging Approach via LLM-Based Multi-Agent Synergy") displays an example bug
    of QuixBugs that is uniquely fixed by FixAgent. The bug wrongly returns an empty
    array, but it is desired as a nested empty array. It is difficult to fix by traditional
    and learning-based APR because it requires adding multi-line edits, which simple
    modifications cannot achieve, and goes beyond typical bug-fix templates. Since
    QuixBugs only contains very brief descriptions of the programs, the debugging
    method must abstract the desired returning format from other returning statements
    in the program or the failing information (if any), requiring a comprehensive
    overview and deep analysis of the program. In fact, even FixAgent generates the
    correct patch upon the original failing test that triggers returning an empty
    set nested with an array. Note that FixAgent fixes four extra bugs in QuixBugs
    compared with its base model GPT4\. We will discuss the reasons in RQ2 ($\S$[V-C](#S5.SS3
    "V-C RQ2: Using different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging
    Approach via LLM-Based Multi-Agent Synergy")).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '结果见表格 [I](#S5.T1 "表格 I ‣ V-A2 基准 ‣ V-A 实验设置 ‣ V 评估 ‣ 基于 LLM 的多智能体协作的统一调试方法")。总体而言，FixAgent
    在 Codeflaws 和 QuixBugs 上成功修复了 2780 个 bug，相比所有基准方法表现最佳。特别是在 Codeflaws 数据集上，FixAgent
    显著优于现有的 APR 方法和 LLM。它生成的合理修复比最佳 APR 方法 GenProg 多 1.9 倍。此外，与 CoCoNuT 相比，它将正确率提高了
    57.42%，后者在 APR 方法中修复了最多的 bug。我们还可以粗略估算，FixAgent 基于正确率大约修复了 2512 个 bug。此外，FixAgent
    在 QuixBugs 上超越了所有对比方法。它在 QuixBugs-Java 中正确修复了 39 个 bug，其中 7 个是以前从未修复过的。FixAgent
    正确修复了 QuixBugs-Python 中的所有 bug，显示出优于所有基准方法的优势。图 [4](#S5.F4 "图 4 ‣ V-B RQ1: FixAgent
    的整体效果 ‣ V 评估 ‣ 基于 LLM 的多智能体协作的统一调试方法") 展示了一个仅由 FixAgent 修复的 QuixBugs 示例 bug。该
    bug 错误地返回了一个空数组，但实际上期望返回的是一个嵌套的空数组。由于需要添加多行修改，这种 bug 传统和基于学习的 APR 方法都难以修复，简单的修改无法实现，并超出了典型的
    bug 修复模板。由于 QuixBugs 仅包含程序的非常简短的描述，调试方法必须从程序中的其他返回语句或失败信息（如果有的话）中抽象出期望的返回格式，需要对程序进行全面的概述和深入的分析。实际上，即使
    FixAgent 在原始失败测试上生成了正确的修复，该测试触发了返回一个嵌套数组的空集。注意，FixAgent 在 QuixBugs 中修复了四个额外的 bug，相比其基础模型
    GPT4。我们将在 RQ2 ($\S$[V-C](#S5.SS3 "V-C RQ2: 使用不同的 LLM 与 FixAgent ‣ V 评估 ‣ 基于 LLM
    的多智能体协作的统一调试方法")) 中讨论原因。'
- en: '![Refer to caption](img/9de81bfa73f0a9a86c6108b60b739730.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/9de81bfa73f0a9a86c6108b60b739730.png)'
- en: 'Figure 4: Example of bug fixed by FixAgent in QuixBugs.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：FixAgent 在 QuixBugs 中修复的一个示例 bug。
- en: Furthermore, we find that LLMs achieve much better performance than traditional
    and NMT-based APR tools, with respect to both plausible patch generation and correctness
    rate, similar to previous findings [[24](#bib.bib24)]. LLMs achieve such results
    even without knowing fault locations, while APR baselines assume perfect FL or
    use a FL tool to facilitate repair generation. Even the worst-performing LLM baseline,
    Gemini, achieves comparable results to the best traditional tool, GenProg. We
    attribute this to the capabilities of LLMs in understanding the semantics and
    contexts of code. LLMs can leverage the knowledge learned from previous source
    code to understand the program semantics and requirements deeply, enabling them
    to fix not only syntactic errors but also semantic errors that previous approaches
    might miss, especially when the bug does not correspond to predefined rules or
    templates that most APR baselines rely on. Besides, LLMs more fully embody superiority
    on Codeflaws. This is because Codeflaws challenges require a deeper understanding
    of real-world contexts and conditions under which programs are written, demanding
    more context understanding. Every program in Codeflaws is written to solve a certain
    question described in natural languages connected with the real world, while QuixBugs
    consists of classic algorithms with one-line bugs that are more easily repaired
    by systematic modifications.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现 LLM 在生成合理补丁和正确率方面的表现远超传统和基于 NMT 的 APR 工具，类似于之前的发现[[24](#bib.bib24)]。LLM
    即使在不知道故障位置的情况下也能取得这样的结果，而 APR 基准则假设完美的故障定位（FL）或使用 FL 工具来辅助修复生成。即使是表现最差的 LLM 基准
    Gemini，其结果也可与最好的传统工具 GenProg 相媲美。我们将此归因于 LLM 在理解代码语义和上下文方面的能力。LLM 能够利用从之前源代码中学到的知识深入理解程序语义和需求，使它们不仅能修复语法错误，还能修复以前方法可能忽视的语义错误，特别是当错误不符合大多数
    APR 基准依赖的预定义规则或模板时。此外，LLM 在 Codeflaws 上的优势更为明显。这是因为 Codeflaws 挑战需要对程序编写的真实世界背景和条件有更深刻的理解，要求更多的上下文理解。Codeflaws
    中的每个程序都旨在解决用自然语言描述的与真实世界相关的某个问题，而 QuixBugs 包含的是经典算法，具有单行错误，更容易通过系统修改进行修复。
- en: FixAgent performs the best among LLM-based methods. We attribute this to our
    designs. LLM competitors conduct the whole debugging (or patch generation of AlphaRepair)
    in one turn, while FixAgent divides this complex task into several steps and requires
    explicit explanations focusing on crucial information. We instantiate agents to
    conduct different stages of debugging separately. Each agent can focus on its
    own and produce better results, so their synergy delivers better debugging performance.
    In addition, we prompt the agents to pay attention to key variables. Intuitively,
    if the program’s output does not conform to the expected, the bug usually appears
    before its final printing statement (though there are printing bugs, they are
    easy to fix), manifested by the intermediate values of variables, especially those
    in key logic expressions. Our prompt forces the agents to conduct reasoning along
    the program’s logic execution path, making bugs easily revealed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: FixAgent 在基于 LLM 的方法中表现最佳。我们将这一成果归功于我们的设计。LLM 竞争者在一个回合中进行整个调试（或 AlphaRepair
    的补丁生成），而 FixAgent 将这一复杂任务分解为几个步骤，并要求明确解释关键的信息。我们实例化了不同的代理来分别进行调试的不同阶段。每个代理可以专注于自身并产生更好的结果，因此它们的协同作用带来了更好的调试性能。此外，我们提示代理关注关键变量。直观上，如果程序的输出不符合预期，通常在最终打印语句之前会出现错误（虽然有打印错误，但容易修复），这些错误表现为变量的中间值，特别是关键逻辑表达式中的值。我们的提示迫使代理沿着程序的逻辑执行路径进行推理，从而使错误更容易暴露。
- en: Note that the debugging results on QuixBugs among LLMs are very close, so we
    omit it in the following RQs. This is because 1) QuixBugs programs implement classic
    algorithms with single-line bugs that are relatively easy to fix, and 2) LLMs
    may have been trained on these algorithm implementations, so they deliver remarkable
    results due to data leakage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LLM 在 QuixBugs 上的调试结果非常接近，因此我们在以下的 RQ 中省略了它。这是因为 1) QuixBugs 程序实现了经典算法，且有单行错误，相对较容易修复，以及
    2) LLM 可能已经对这些算法实现进行了训练，因此由于数据泄漏，它们取得了显著的结果。
- en: 'V-C RQ2: Using different LLMs with FixAgent'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C RQ2：使用不同的 LLM 与 FixAgent
- en: 'We evaluate the capability of FixAgent using different base LLMs on ConDefects,
    collected after the LLM releases, making it impossible to compose the training
    data, thereby avoiding data leakage. Table [II](#S5.T2 "Table II ‣ V-C RQ2: Using
    different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via
    LLM-Based Multi-Agent Synergy") shows the results, where FixAgent-$[model]$ bugs.
    FixAgent can correctly fix 115–205 bugs and plausibly patch 124–207 bugs out of
    the sampled 300 Java bugs using different LLMs. It can also fix 89–163 and patch
    99–168 Python bugs. The results indicate that FixAgent can work effectively with
    various base LLMs, though influenced by their capabilities. FixAgent with a larger
    model should perform better in experience, but FixAgent-DeepSeek-Coder outperforms
    FixAgent-ChatGPT and FixAgent-Gemini, while Gemini and ChatGPT have much larger
    model sizes. We attribute this to the specialized training data and objective
    of DeepSeek-Coder. DeepSeek-Coder is a code LLM trained on a real-world code corpus,
    making it proficient at coding-related tasks, including debugging. Though DeepSeek-Coder
    and CodeLlama are code LLM with similar model sizes, FixAgent-DeepSeek-Coder outperforms
    FixAgent-CodeLlama considerably. Their inherent coding ability gap can explain
    this. As reported in [[41](#bib.bib41)], DeepSeek-Coder performs much better than
    CodeLlama on solving programming contests. The debugging performance of LLM is
    strongly related to its coding ability, considering that debugging can be regarded
    as generating a correct program prompted by a buggy one.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了 FixAgent 在 ConDefects 上使用不同基础 LLM 的能力，这些数据在 LLM 发布后收集，因此无法编排训练数据，从而避免了数据泄漏。表
    [II](#S5.T2 "表 II ‣ V-C RQ2: 使用不同 LLMs 与 FixAgent ‣ V 评估 ‣ 通过基于 LLM 的多智能体协同的统一调试方法")
    显示了结果，其中 FixAgent-$[model]$ 漏洞。FixAgent 可以使用不同 LLM 正确修复 115–205 个漏洞，并可能修复 124–207
    个抽样的 300 个 Java 漏洞。它还可以修复 89–163 和修补 99–168 个 Python 漏洞。结果表明，FixAgent 可以与各种基础
    LLM 有效配合，尽管受到其能力的影响。经验上，使用更大模型的 FixAgent 应该表现更好，但 FixAgent-DeepSeek-Coder 超越了
    FixAgent-ChatGPT 和 FixAgent-Gemini，而 Gemini 和 ChatGPT 的模型规模要大得多。我们将此归因于 DeepSeek-Coder
    的专业训练数据和目标。DeepSeek-Coder 是一个基于真实世界代码语料库训练的代码 LLM，使其在编码相关任务，包括调试方面非常熟练。尽管 DeepSeek-Coder
    和 CodeLlama 是具有相似模型规模的代码 LLM，FixAgent-DeepSeek-Coder 显著优于 FixAgent-CodeLlama。这种固有的编码能力差距可以解释这一点。如
    [[41](#bib.bib41)] 所报告，DeepSeek-Coder 在解决编程竞赛中的表现远优于 CodeLlama。考虑到调试可以被视为生成正确程序的过程，LLM
    的调试性能与其编码能力紧密相关。'
- en: 'TABLE II: Effectiveness of FixAgent using different LLMs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：FixAgent 使用不同 LLM 的有效性。
- en: Models ConDefects-Java ConDefects-Python 300 bugs 300 bugs FixAgent-CodeLlama
    115$/$168
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 ConDefects-Java ConDefects-Python 300 个漏洞 300 个漏洞 FixAgent-CodeLlama 115$/$168
- en: 'We also evaluate how FixAgent improves its base model on Codeflaws, as shown
    in Table [III](#S5.T3 "Table III ‣ V-C RQ2: Using different LLMs with FixAgent
    ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy"),
    where we sample 300 bugs in this evaluation for efficiency. Compared to its base
    models, FixAgent plausibly patches 17–35 more bugs and correctly fixes 12–33 more
    bugs. Among all compared pairs, FixAgent improves the most on GPT4, increasing
    correctly and plausibly patched bugs by 33 and 35, respectively. Overall, applying
    FixAgent can make a 20% improvement on the original LLM, on average. This enhancement
    highlights the effectiveness of our designs that significantly improve the debugging
    abilities of LLMs in a non-invasive manner without any re-training or fine-tuning,
    demonstrating our insight that LLMs can benefit from software principles, such
    as rubber ducking.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还评估了 FixAgent 如何在 Codeflaws 上提升其基础模型，如表 [III](#S5.T3 "表 III ‣ V-C RQ2: 使用不同
    LLMs 与 FixAgent ‣ V 评估 ‣ 通过基于 LLM 的多智能体协同的统一调试方法") 所示，在这次评估中，我们抽样了 300 个漏洞以提高效率。与其基础模型相比，FixAgent
    可能修复了 17–35 个更多的漏洞，并正确修复了 12–33 个更多的漏洞。在所有比较的对比中，FixAgent 在 GPT4 上的提升最大，分别增加了
    33 和 35 个正确和可能修复的漏洞。总体而言，应用 FixAgent 可以使原始 LLM 的性能平均提高 20%。这种提升突显了我们设计的有效性，它显著提高了
    LLM 的调试能力，且不需要任何重新训练或微调，展示了我们的见解：LLM 可以从软件原则中受益，例如橡皮鸭调试。'
- en: 'TABLE III: Improvement made by FixAgent compared to its base model under different
    LLMs on 300 samples in Codeflaws.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：FixAgent 在 Codeflaws 上的 300 个样本中，相对于其基础模型的改进。
- en: Models Codeflaws-C, sampled 300 bugs Base Model (CoT) Applying FixAgent CodeLlama
    105$/$262
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 Codeflaws-C，抽样 300 个漏洞 基础模型 (CoT) 应用 FixAgent CodeLlama 105$/$262
- en: 'V-D RQ3: Ablation Studies on FixAgent'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'V-D RQ3: FixAgent 的消融研究'
- en: 'This section studies the contribution of each design to FixAgent. Table [IV](#S5.T4
    "Table IV ‣ V-D RQ3: Ablation Studies on FixAgent ‣ V Evaluation ‣ A Unified Debugging
    Approach via LLM-Based Multi-Agent Synergy") shows the results. Each row represents
    removing one design, the decrease in the number of correct or plausible generated
    patches. FixAgent $w/o$ feedback repeatedly samples FixAgent three times independently
    without failing information.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '本节研究了每个设计对FixAgent的贡献。表[IV](#S5.T4 "Table IV ‣ V-D RQ3: Ablation Studies on
    FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent
    Synergy")展示了结果。每一行表示移除一个设计，导致正确或合理生成的补丁数量减少。FixAgent $w/o$ feedback 反复独立地对FixAgent进行三次采样，而不依赖失败信息。'
- en: 'TABLE IV: Ablation study results of FixAgent.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：FixAgent的消融研究结果。
- en: 'Models Codeflaws-C, 300 bugs #Correct #Plausible FixAgent $w/o$ feedback -5
    -4'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'Models Codeflaws-C, 300 bugs #Correct #Plausible FixAgent $w/o$ feedback -5
    -4'
- en: 'We find that context construction contributes the most to FixAgent. Removing
    contexts reduces the correctly fixed bugs by 122. This highlights the significance
    of contexts in debugging as they provide the potential to understand the underlying
    problem domain, infer the intended functionality, and apply appropriate repair.
    Such degradation also illustrates why LLMs significantly outperform APR tools
    even without perfect fault localization, as shown in RQ1 ($\S$[V-B](#S5.SS2 "V-B
    RQ1: Overall Effectiveness of FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach
    via LLM-Based Multi-Agent Synergy")).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现上下文构建对FixAgent的贡献最大。移除上下文会减少122个正确修复的错误。这突显了上下文在调试中的重要性，因为它们提供了理解潜在问题领域、推断预期功能和应用适当修复的潜力。这种退化还说明了为什么即使在没有完美故障定位的情况下，LLMs
    也显著优于APR工具，如RQ1（$\S$[V-B](#S5.SS2 "V-B RQ1: Overall Effectiveness of FixAgent
    ‣ V Evaluation ‣ A Unified Debugging Approach via LLM-Based Multi-Agent Synergy")）所示。'
- en: 'The second important design is variable tracking. FixAgent generates 37 and
    38 fewer plausible and correct patches, respectively, without tracking. The foundation
    of this advancement is the enhancement of error diagnosis and reduction of reasoning
    overload in LLMs. By verbally articulating the role and expected behavior of each
    variable, LLMs are more likely to pinpoint discrepancies between expected and
    actual outcomes, facilitating a targeted approach to repair, mirroring the effective
    rubber ducking strategy. Furthermore, the structured explanation reduces the model’s
    reasoning overload. Traditional CoT prompting, while effective in encouraging
    step-by-step reasoning, does not inherently prioritize information in a way that
    minimizes reasoning strain. Instead, our strategy streamlines the process of LLMs
    in analyzing the program logic. Using one agent to replace multi-agent synergy
    has a similar negative effect, which reduces 28 plausible and correct fixes. The
    division of labor mirrors a well-established principle in software: specialization,
    which forces each agent to focus on its own task and reduces cognitive load.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要设计是变量跟踪。FixAgent 在没有跟踪的情况下分别减少了37和38个合理和正确的补丁。这一进展的基础在于提高了错误诊断和减少了LLMs的推理负担。通过口头阐述每个变量的角色和预期行为，LLMs
    更容易找出预期与实际结果之间的差异，从而采取有针对性的修复方法，类似于有效的橡皮鸭策略。此外，结构化的解释减少了模型的推理负担。传统的CoT提示虽然有效地鼓励逐步推理，但并未固有地优先考虑信息以减轻推理负担。相反，我们的策略简化了LLMs分析程序逻辑的过程。用一个代理来替代多代理协同具有类似的负面效果，减少了28个合理和正确的修复。劳动分工类似于软件中的一个成熟原则：专业化，这迫使每个代理专注于自己的任务，并减少认知负担。
- en: The least useful design is feedback-supported re-sampling. Though the added
    feedback provides more information, it expands the dialogue window, and the model
    must allocate its finite attention span to this new information, potentially at
    the expense of other critical details. This trade-off may explain why the additional
    feedback does not significantly enhance the debugging capability of LLMs compared
    to independent sampling, as our results show.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最不有用的设计是支持反馈的重新采样。尽管增加的反馈提供了更多的信息，但它扩大了对话窗口，模型必须将其有限的注意力分配给这些新信息，可能会牺牲其他重要细节。这种权衡可能解释了为什么额外的反馈没有显著提升LLMs的调试能力，相较于独立采样，我们的结果显示。
- en: VI Discussion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 讨论
- en: VI-A Limitations
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 限制
- en: VI-A1 Ceiling Improvement
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A1 顶点改进
- en: Though FixAgent significantly improves the debugging performance of multiple
    LLMs, its effectiveness is intrinsically tied to the foundational capabilities
    of the base model. Indeed, our method can not fundamentally alter the intrinsic
    capabilities of LLMs; instead, it optimizes their existing skills within a proportional
    range. For example, FixAgent improves Gemini by plausibly and correctly fixing
    26 and 17 more bugs on Codeflaws, but its performance is still poorer than the
    simple application of GPT4. Thus, models with inherently limited debugging abilities
    will not experience a significant leap in performance using FixAgent despite improvements.
    The ceiling enhancement is ultimately bounded by the capabilities of the LLM itself.
    Hence, we adopt GPT4 as the default base model due to its advanced performance
    in a broad range of cognitive tasks [[63](#bib.bib63)], though FixAgent is compatible
    with any LLM.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 FixAgent 显著提高了多种 LLM 的调试性能，但其有效性本质上与基础模型的能力相关。的确，我们的方法无法从根本上改变 LLM 的固有能力；相反，它在一个比例范围内优化现有技能。例如，FixAgent
    通过合理且正确地修复 Codeflaws 上的 26 个和 17 个错误来改进 Gemini，但其性能仍然不如简单应用 GPT4。因此，固有调试能力有限的模型在使用
    FixAgent 时不会经历显著的性能跃升，尽管有改进。最终的提升上限由 LLM 自身的能力决定。因此，我们选择 GPT4 作为默认基础模型，因为它在广泛的认知任务中表现出色[[63](#bib.bib63)]，尽管
    FixAgent 与任何 LLM 兼容。
- en: VI-A2 External Test Output Calculation
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VI-A2 外部测试输出计算
- en: We introduce an extra agent for test input generation to mitigate the overfitting
    problem, but it cannot directly calculate the corresponding expected outputs or
    test oracles, where a test oracle is a set of assertions that should pass when
    the program behaves as expected. This is primarily due to the intrinsic probabilistic
    mechanisms of LLMs, making it challenging to accurately deliver precise answers
    to computational questions. Addressing such problems akin to mathematical reasoning
    represents one of the significant challenges faced by LLMs. Our approach cannot
    overcome their inherent limitations; therefore, additional information, such as
    correct code or manually computed answers, must be introduced to obtain the outputs
    for generated inputs and form complete test cases.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了额外的代理用于测试输入生成，以缓解过拟合问题，但它无法直接计算相应的预期输出或测试 oracle，其中测试 oracle 是一组在程序按预期行为时应通过的断言。这主要是由于
    LLM 的固有概率机制，使得准确提供计算问题的精确答案具有挑战性。解决类似于数学推理的问题是 LLM 面临的重大挑战之一。我们的方法无法克服其固有的局限性；因此，必须引入额外的信息，如正确的代码或手动计算的答案，以获得生成输入的输出并形成完整的测试用例。
- en: VI-B Threats to Validity
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 真实性威胁
- en: 'Internal First, we share the same major internal threat to validity with previous
    LLMs-based coding-related techniques where the training data of closed-sourced
    LLMs may overlap with our evaluation datasets. Since we do not have access to
    the training data, we mitigate this threat from three steps. 1) We choose a recently
    collected dataset (ConDefects) for evaluation proposed to mitigate the data leakage
    problem of LLMs as its collection is posterior to the model release. 2) The other
    two datasets we use are believed to be not part of the training data as discussed
    in [[24](#bib.bib24)] because 1) they have a low number of stars on GitHub, and
    2) they focus on classic algorithms (QuixBugs) and programming assignments (Codeflaws)
    that do not belong to larger real-world projects. 3) RQ2 [V-C](#S5.SS3 "V-C RQ2:
    Using different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging Approach
    via LLM-Based Multi-Agent Synergy") demonstrates that our framework significantly
    improves the debugging effectiveness compared with its base LLM, where the improvement
    is orthogonal to the data leakage issue. Second, we cannot directly determine
    the correct patches and rely on manual validation of plausible patches. For Codeflaws
    and ConDefects, we have to check a sampled subset of the patches because of their
    huge size. To mitigate this threat, we carefully validate the randomly sampled
    patches and all the patches for QuixBugs. Also, we generate more inputs using
    the crafter agent to test the plausible patches.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '内部威胁 首先，我们与之前基于LLM的编码相关技术面临相同的主要内部威胁，即闭源LLM的训练数据可能与我们的评估数据集重叠。由于我们无法访问训练数据，我们通过三个步骤来减轻这一威胁。1)
    我们选择了一个最近收集的数据集（ConDefects）进行评估，以解决LLMs的数据泄漏问题，因为其收集时间晚于模型发布。2) 我们使用的另外两个数据集被认为不属于训练数据，如[[24](#bib.bib24)]中讨论的，因为1)
    它们在GitHub上的星数较少，2) 它们专注于经典算法（QuixBugs）和编程作业（Codeflaws），不属于更大的现实世界项目。3) RQ2 [V-C](#S5.SS3
    "V-C RQ2: Using different LLMs with FixAgent ‣ V Evaluation ‣ A Unified Debugging
    Approach via LLM-Based Multi-Agent Synergy") 证明了我们的框架显著提高了调试效果，与其基础LLM相比，这种改进与数据泄漏问题是正交的。其次，我们不能直接确定正确的补丁，并依赖于对可能补丁的人工验证。对于Codeflaws和ConDefects，由于其庞大的规模，我们必须检查采样子集。为了减轻这一威胁，我们仔细验证了随机采样的补丁以及所有QuixBugs的补丁。同时，我们使用crafter
    agent生成更多输入以测试可能的补丁。'
- en: External Threat The main external threat lies in the evaluation dataset, where
    the superiority of FixAgent may not generalize to other datasets, especially for
    those less widely used programming languages that have less open-source code data
    for LLM training. To mitigate this, we compare our method against advanced baselines,
    including both general LLMs and APR approaches on five benchmarks in three datasets,
    covering three programming languages. Experiments demonstrate the effectiveness
    of FixAgent among all these benchmarks. We will also evaluate our approach to
    more datasets across more diverse programming languages in the future.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 外部威胁 主要的外部威胁在于评估数据集，其中FixAgent的优越性可能无法推广到其他数据集，特别是那些使用较少、开源代码数据较少的编程语言，这些数据对LLM训练至关重要。为了减轻这一威胁，我们将我们的方法与先进的基准进行比较，包括三个数据集中的一般LLMs和APR方法，涵盖三种编程语言。实验展示了FixAgent在所有这些基准中的有效性。我们还将来会在更多数据集和多样化编程语言中评估我们的方法。
- en: VII Related Work
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 相关工作
- en: VII-A Debugging
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 调试
- en: Spectrum-based and mutation-based are twopopular FL techniques. Spectrum-based
    tools [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)] calculate the suspiciousness
    scores of each line of code based on the correlation between the execution of
    a code line and the occurrence of test failures. However, it relies solely on
    test coverage data, specifically the binary distinction of whether a test case
    executes a code line. This reliance implicitly assumes that all test cases contribute
    equally to uncovering faults, which oversimplifies the complex nature of software
    errors. Mutation-based [[8](#bib.bib8), [7](#bib.bib7), [9](#bib.bib9)] mitigates
    the above limitation by implementing simple modifications on the buggy program
    based on pre-defined rules (mutation operators), such as altering a conditional
    statement from equality (==) to inequality (!=), thereby creating variants of
    the program that slightly differ from the original buggy one. However, its efficacy
    is contingent upon the ability to generate meaningful mutants for each code element
    under scrutiny. Learning-based FL tools learn program behaviors from rich data
    sources, including code coverage matrix [[16](#bib.bib16)], statement-level calls [[64](#bib.bib64)],
    structural intricacies [[17](#bib.bib17)], or their combination [[18](#bib.bib18)]
    via multiple types of neural networks. A recent advancement, LLMAO [[19](#bib.bib19)],
    proposes to parse Abstract Syntax Tree and utilizes LLMs to achieve test-free
    FL with high confidence.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基于谱的方法和基于变异的方法是两种流行的故障定位（FL）技术。基于谱的方法[[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]通过计算每行代码的可疑性分数，该分数基于代码行执行与测试失败发生之间的相关性。然而，这种方法仅依赖于测试覆盖数据，具体来说，即测试用例是否执行了某行代码的二元区分。这种依赖隐含地假设所有测试用例对发现故障的贡献是相等的，这简化了软件错误的复杂性。基于变异的方法[[8](#bib.bib8),
    [7](#bib.bib7), [9](#bib.bib9)]通过根据预定义规则（变异操作符）对有缺陷的程序进行简单修改，如将条件语句从等于（==）改为不等于（!=），从而创建与原有缺陷程序稍有不同的变体，从而缓解了上述限制。然而，它的有效性取决于为每个被审查的代码元素生成有意义的变异体的能力。基于学习的故障定位工具从丰富的数据源中学习程序行为，包括代码覆盖矩阵[[16](#bib.bib16)]、语句级调用[[64](#bib.bib64)]、结构复杂性[[17](#bib.bib17)]，或其组合[[18](#bib.bib18)]，通过多种类型的神经网络实现。最近的进展，LLMAO[[19](#bib.bib19)]，提出解析抽象语法树并利用大型语言模型（LLMs）实现高信心的无测试故障定位。
- en: APR techniques can be broadly categorized into search-based and generate-and-validate
    (G&V)-based. Search-based employs meta-heuristic search algorithms, such as genetic
    programming [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)], to find suitable
    solutions by exploring the space of possible patches. The patch space can be built
    by fix synthesis through syntactic code modifications and human-defined or automatically
    mined templates. Such approaches assume that the correct expression exists in
    the program or a pre-defined template set, limiting their wider applications [[65](#bib.bib65)].
    G&V APR [[14](#bib.bib14), [13](#bib.bib13)] aims to generate patches logically
    and semantically coherent with the intended functionality. Traditional G&V methods
    represent the repair process as an explicit specification inference [[66](#bib.bib66)].
    Recent studies regard it as a translation from faulty code to correct code using
    NMT, such as CoCoNuT [[21](#bib.bib21)], CURE [[22](#bib.bib22)], and RewardRepair [[60](#bib.bib60)].
    Despite their effectiveness, they rely on bug-fixing training datasets usually
    crawled from commits of open-source repositories, which may contain irrelevant
    changes. To filter such noise, NMT tools are usually trained on small commit data,
    limiting their ability to fix more diverse bugs. Researchers have explored directly
    applying LLMs in APR using an infilling paradigm [[27](#bib.bib27), [24](#bib.bib24)],
    showing promising results. However, they still struggle to fix complex bugs requiring
    a deep understanding of program contexts or logic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: APR 技术可以大致分为基于搜索和生成-验证（G&V）两类。基于搜索的方法采用元启发式搜索算法，如遗传编程 [[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12)]，通过探索可能的补丁空间来寻找合适的解决方案。补丁空间可以通过语法代码修改和人工定义或自动挖掘的模板来构建。这些方法假设程序中存在正确的表达式或预定义的模板集，这限制了它们的广泛应用 [[65](#bib.bib65)]。G&V
    APR [[14](#bib.bib14), [13](#bib.bib13)] 旨在生成逻辑上和语义上与预期功能一致的补丁。传统的 G&V 方法将修复过程表示为明确的规范推断 [[66](#bib.bib66)]。近期研究将其视为使用
    NMT 从错误代码到正确代码的翻译，例如 CoCoNuT [[21](#bib.bib21)]、CURE [[22](#bib.bib22)] 和 RewardRepair [[60](#bib.bib60)]。尽管这些方法有效，但它们依赖于通常从开源仓库的提交中抓取的
    bug 修复训练数据，这些数据可能包含无关的更改。为了过滤这些噪声，NMT 工具通常在小规模提交数据上训练，限制了其修复更多样化的 bug 的能力。研究人员已经探索了使用填充范式直接应用
    LLM 在 APR 中 [[27](#bib.bib27), [24](#bib.bib24)]，显示了有前景的结果。然而，它们仍然难以修复需要深刻理解程序上下文或逻辑的复杂
    bug。
- en: Unified debugging is a pioneer concept [[39](#bib.bib39)] that aims to unify
    FL and APR to boost both areas. A recent study [[67](#bib.bib67)] has shown that
    FL can benefit from 16 different APR systems. However, existing studies only consider
    the contribution to FL made by APR. We are the first to propose a unified framework
    enabling end-to-end debugging, where stages of the debugging have an interactional
    rather than determined relationship.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 统一调试是一个开创性概念 [[39](#bib.bib39)]，旨在统一 FL 和 APR 以提升这两个领域。最近的一项研究 [[67](#bib.bib67)]
    显示，FL 可以从 16 个不同的 APR 系统中受益。然而，现有研究仅考虑了 APR 对 FL 的贡献。我们是首个提出一个统一框架以实现端到端调试的研究，其中调试的各个阶段具有互动关系，而非确定关系。
- en: VII-B Large Language Models
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 大型语言模型
- en: LLMs have revolutionized the field of software development with their rapid
    advancement. LLMs are trained on vast amounts of text data, enabling them to understand
    and generate human-like text [[42](#bib.bib42), [41](#bib.bib41)]. Existing studies
    have shown that a well-crafted prompt can lead to accurate, insightful, and useful
    domain-specific text generation [[32](#bib.bib32), [68](#bib.bib68)]. In addition
    to general-purpose LLMs [[57](#bib.bib57), [42](#bib.bib42), [56](#bib.bib56)],
    code LLMs are mainly trained on source code data, such as Codex [[59](#bib.bib59)],
    DeepSeek-Coder [[41](#bib.bib41)], and CodeLlama [[58](#bib.bib58)]. These models
    are proposed to automate and streamline software engineering, enhance productivity,
    and reduce human errors. Many studies have investigated their capability on coding
    tasks, including code generation [[62](#bib.bib62)], APR [[24](#bib.bib24), [33](#bib.bib33),
    [47](#bib.bib47)], and test generation [[69](#bib.bib69)]. On top of the direct
    application of APR [[24](#bib.bib24)], recent studies have researched prompt engineering [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [28](#bib.bib28), [73](#bib.bib73), [27](#bib.bib27)]
    or the combination of other techniques [[25](#bib.bib25), [69](#bib.bib69), [19](#bib.bib19)]
    to better unleash the capability of LLMs. For example, Repilot [[25](#bib.bib25)]
    copilots the LLMs via a code completion engine to synthesize more valid patches.
    AlphaRepair [[27](#bib.bib27)] proposes the first cloze-style APR approach that
    directly prompts LLMs to predict the correct code given context information with
    faulty code masked. A recent code generator [[70](#bib.bib70)] asks the LLM to
    explain the generated code line-by-line as rubber ducking, achieving promising
    performance. InferFix [[73](#bib.bib73)] augmented the prompt by incorporating
    similar fixes identified in a historical database of bugs through a dense retrieval
    model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 的快速发展已经彻底改变了软件开发领域。LLMs 在大量文本数据上进行训练，使其能够理解和生成类似人类的文本 [[42](#bib.bib42),
    [41](#bib.bib41)]。现有研究表明，一个精心设计的提示可以导致准确、有洞察力且有用的领域特定文本生成 [[32](#bib.bib32), [68](#bib.bib68)]。除了通用
    LLMs [[57](#bib.bib57), [42](#bib.bib42), [56](#bib.bib56)]，代码 LLMs 主要在源代码数据上进行训练，例如
    Codex [[59](#bib.bib59)]、DeepSeek-Coder [[41](#bib.bib41)] 和 CodeLlama [[58](#bib.bib58)]。这些模型旨在自动化和简化软件工程，提高生产力并减少人为错误。许多研究调查了它们在编码任务上的能力，包括代码生成
    [[62](#bib.bib62)]、APR [[24](#bib.bib24), [33](#bib.bib33), [47](#bib.bib47)]
    和测试生成 [[69](#bib.bib69)]。除了 APR 的直接应用 [[24](#bib.bib24)]，最近的研究还探讨了提示工程 [[70](#bib.bib70),
    [71](#bib.bib71), [72](#bib.bib72), [28](#bib.bib28), [73](#bib.bib73), [27](#bib.bib27)]
    或其他技术的组合 [[25](#bib.bib25), [69](#bib.bib69), [19](#bib.bib19)]，以更好地发挥 LLMs 的能力。例如，Repilot
    [[25](#bib.bib25)] 通过代码补全引擎辅佐 LLMs 合成更有效的修复补丁。AlphaRepair [[27](#bib.bib27)] 提出了首个填空式
    APR 方法，直接提示 LLMs 在上下文信息中预测正确的代码，并将有缺陷的代码进行掩盖。最近的一个代码生成器 [[70](#bib.bib70)] 要求
    LLMs 逐行解释生成的代码，类似于橡皮鸭调试，取得了令人满意的效果。InferFix [[73](#bib.bib73)] 通过密集检索模型将类似的修复集成到提示中，增强了提示的效果。
- en: Our method, FixAgent, provides an end-to-end solution for automated debugging
    with high effectiveness. It performs a universal prompt design inspired by rubber
    ducking, which can significantly improve the debugging ability of LLMs in a non-intrusive
    style without any retraining/fine-tuning or historical bug-fix information. FixAgent
    shows that LLMs can benefit from general software engineering principles recognized
    by developers, potentially besides rubber ducking.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法，FixAgent，提供了一种高效的端到端自动化调试解决方案。它采用了受“橡皮鸭调试”启发的通用提示设计，这可以显著提升 LLMs 的调试能力，且无需任何重新训练/微调或历史修复信息，风格上也不具侵入性。FixAgent
    证明了 LLMs 可以从开发者认可的一般软件工程原则中获益，可能还包括橡皮鸭调试之外的其他原则。
- en: VIII Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: We propose FixAgent, the first unified debugging framework via LLM agent synergy.
    It conducts fault localization, patch generation, and post-error analysis in an
    end-to-end manner. Our insight is that LLMs can benefit from software engineering
    principles recognized by developers. Thus, we follow the principle of rubber duck
    debugging-explaining the code in detail to create novel designs that unleash the
    debugging capability of LLMs and mitigate previous challenges. The evaluation
    of two widely used datasets demonstrates the superiority of FixAgent over APR
    tools and LLM-based competitors, and extra experiments on recently collected data
    (avoiding data leakage) further show our generalization and effectiveness in debugging
    compared with base LLMs. Our code and generated patches are publicly available
    for further research and reproduction.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了FixAgent，这是第一个通过LLM代理协同的统一调试框架。它以端到端的方式进行故障定位、补丁生成和错误分析。我们的见解是LLMs可以从开发者认可的软件工程原则中受益。因此，我们遵循橡皮鸭调试原则——详细解释代码，创建新颖的设计，释放LLMs的调试能力，缓解以前的挑战。对两个广泛使用的数据集的评估证明了FixAgent在APR工具和基于LLM的竞争者中的优越性，并且对最近收集的数据（避免数据泄露）进行的额外实验进一步展示了我们在调试方面的泛化和有效性，与基础LLMs相比。我们的代码和生成的补丁公开可用，供进一步研究和重现使用。
- en: References
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. Yuan, Y. Luo, X. Zhuang, G. R. Rodrigues, X. Zhao, Y. Zhang, P. Jain,
    and M. Stumm, “Simple testing can prevent most critical failures: An analysis
    of production failures in distributed data-intensive systems,” in *11th USENIX
    Symposium on Operating Systems Design and Implementation, OSDI ’14, Broomfield,
    CO, USA, October 6-8, 2014*.   USENIX Association, 2014, pp. 249–265.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. Yuan, Y. Luo, X. Zhuang, G. R. Rodrigues, X. Zhao, Y. Zhang, P. Jain,
    和 M. Stumm，“简单测试可以防止大多数关键故障：对分布式数据密集型系统中生产故障的分析，”收录于 *第11届USENIX操作系统设计与实现研讨会，OSDI
    ’14，2014年10月6-8日，科罗拉多州布鲁姆菲尔德，美国*。 USENIX协会，2014年，第249–265页。'
- en: '[2] A. Alaboudi and T. D. LaToza, “What constitutes debugging? an exploratory
    study of debugging episodes,” *Empir. Softw. Eng.*, vol. 28, no. 5, p. 117, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Alaboudi 和 T. D. LaToza，“什么构成了调试？对调试过程的探索性研究，” *Empir. Softw. Eng.*，第28卷，第5期，第117页，2023年。'
- en: '[3] C. Boulder, “University of cambridge study: Failure to adopt reverse debugging
    costs global economy $41 billion annually.” University of Cambridge, London, Technical
    Report, 2013.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. Boulder，“剑桥大学研究：未采用逆向调试每年给全球经济造成410亿美元的损失。” 剑桥大学，伦敦，技术报告，2013年。'
- en: '[4] R. Abreu, P. Zoeteweij, and A. J. C. van Gemund, “Spectrum-based multiple
    fault localization,” in *ASE 2009, 24th IEEE/ACM International Conference on Automated
    Software Engineering, Auckland, New Zealand, November 16-20, 2009*.   IEEE Computer
    Society, 2009, pp. 88–99.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Abreu, P. Zoeteweij, 和 A. J. C. van Gemund，“基于谱的多重故障定位，”收录于 *ASE 2009，第24届IEEE/ACM国际自动化软件工程会议，新西兰奥克兰，2009年11月16-20日*。
    IEEE计算机学会，2009年，第88–99页。'
- en: '[5] ——, “An evaluation of similarity coefficients for software fault localization,”
    in *12th IEEE Pacific Rim International Symposium on Dependable Computing (PRDC
    2006), 18-20 December, 2006, University of California, Riverside, USA*.   IEEE
    Computer Society, 2006, pp. 39–46.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] ——，“对软件故障定位的相似系数评估，”收录于 *第12届IEEE太平洋国际可靠计算研讨会（PRDC 2006），2006年12月18-20日，加州大学，河滨，美国*。
    IEEE计算机学会，2006年，第39–46页。'
- en: '[6] L. Zhang, M. Kim, and S. Khurshid, “Localizing failure-inducing program
    edits based on spectrum information,” in *IEEE 27th International Conference on
    Software Maintenance, ICSM 2011, Williamsburg, VA, USA, September 25-30, 2011*.   IEEE
    Computer Society, 2011, pp. 23–32.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Zhang, M. Kim, 和 S. Khurshid，“基于谱信息的故障诱发程序编辑的定位，”收录于 *IEEE第27届国际软件维护会议，ICSM
    2011，2011年9月25-30日，弗吉尼亚州威廉斯堡，美国*。 IEEE计算机学会，2011年，第23–32页。'
- en: '[7] X. Li and L. Zhang, “Transforming programs and tests in tandem for fault
    localization,” *Proc. ACM Program. Lang.*, vol. 1, no. OOPSLA, pp. 92:1–92:30,
    2017.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] X. Li 和 L. Zhang，“将程序和测试同步转换以进行故障定位，” *Proc. ACM Program. Lang.*，第1卷，OOPSLA期，第92:1–92:30页，2017年。'
- en: '[8] S. Moon, Y. Kim, M. Kim, and S. Yoo, “Ask the mutants: Mutating faulty
    programs for fault localization,” in *Seventh IEEE International Conference on
    Software Testing, Verification and Validation, ICST 2014, March 31 2014-April
    4, 2014, Cleveland, Ohio, USA*.   IEEE Computer Society, 2014, pp. 153–162.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] S. Moon, Y. Kim, M. Kim, 和 S. Yoo，“询问突变体：突变有缺陷的程序以进行故障定位，”收录于 *第七届IEEE国际软件测试、验证与验证会议，ICST
    2014，2014年3月31日-4月4日，俄亥俄州克利夫兰，美国*。 IEEE计算机学会，2014年，第153–162页。'
- en: '[9] L. Zhang, L. Zhang, and S. Khurshid, “Injecting mechanical faults to localize
    developer faults for evolving software,” in *Proceedings of the 2013 ACM SIGPLAN
    International Conference on Object Oriented Programming Systems Languages & Applications,
    OOPSLA 2013, part of SPLASH 2013, Indianapolis, IN, USA, October 26-31, 2013*.   ACM,
    2013, pp. 765–784.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. Zhang, L. Zhang, 和 S. Khurshid, “注入机械故障以定位演变软件中的开发者故障”，发表于 *2013 ACM
    SIGPLAN国际面向对象编程系统语言与应用会议，OOPSLA 2013，SPLASH 2013的一部分，印第安纳波利斯，IN，美国，2013年10月26-31日*。ACM,
    2013, 第765–784页。'
- en: '[10] C. L. Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer, “A systematic study
    of automated program repair: Fixing 55 out of 105 bugs for $8 each,” in *34th
    International Conference on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich,
    Switzerland*.   IEEE Computer Society, 2012, pp. 3–13.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] C. L. Goues, M. Dewey-Vogt, S. Forrest, 和 W. Weimer, “自动程序修复的系统研究：修复105个错误中的55个，每个$8”，发表于
    *第34届国际软件工程大会，ICSE 2012，2012年6月2-9日，苏黎世，瑞士*。IEEE计算机学会, 2012, 第3–13页。'
- en: '[11] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest, “Automatically finding
    patches using genetic programming,” in *31st International Conference on Software
    Engineering, ICSE 2009, May 16-24, 2009, Vancouver, Canada, Proceedings*.   IEEE,
    2009, pp. 364–374.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] W. Weimer, T. Nguyen, C. L. Goues, 和 S. Forrest, “使用遗传编程自动寻找修补”，发表于 *第31届国际软件工程大会，ICSE
    2009，2009年5月16-24日，温哥华，加拿大，会议录*。IEEE, 2009, 第364–374页。'
- en: '[12] ——, “Automatically finding patches using genetic programming,” in *31st
    International Conference on Software Engineering, ICSE 2009, May 16-24, 2009,
    Vancouver, Canada, Proceedings*.   IEEE, 2009, pp. 364–374.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] ——, “使用遗传编程自动寻找修补”，发表于 *第31届国际软件工程大会，ICSE 2009，2009年5月16-24日，温哥华，加拿大，会议录*。IEEE,
    2009, 第364–374页。'
- en: '[13] H. D. T. Nguyen, D. Qi, A. Roychoudhury, and S. Chandra, “Semfix: program
    repair via semantic analysis,” in *35th International Conference on Software Engineering,
    ICSE ’13, San Francisco, CA, USA, May 18-26, 2013*.   IEEE Computer Society, 2013,
    pp. 772–781.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] H. D. T. Nguyen, D. Qi, A. Roychoudhury, 和 S. Chandra, “Semfix: 通过语义分析进行程序修复”，发表于
    *第35届国际软件工程大会，ICSE ’13，旧金山，CA，美国，2013年5月18-26日*。IEEE计算机学会, 2013, 第772–781页。'
- en: '[14] S. Mechtaev, J. Yi, and A. Roychoudhury, “Angelix: scalable multiline
    program patch synthesis via symbolic analysis,” in *Proceedings of the 38th International
    Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016*.   ACM,
    2016, pp. 691–701.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. Mechtaev, J. Yi, 和 A. Roychoudhury, “Angelix: 通过符号分析实现可扩展的多行程序修补合成”，发表于
    *第38届国际软件工程大会，ICSE 2016，奥斯汀，TX，美国，2016年5月14-22日*。ACM, 2016, 第691–701页。'
- en: '[15] F. Long and M. C. Rinard, “An analysis of the search spaces for generate
    and validate patch generation systems,” in *Proceedings of the 38th International
    Conference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016*.   ACM,
    2016, pp. 702–713.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] F. Long 和 M. C. Rinard, “生成和验证修补生成系统的搜索空间分析”，发表于 *第38届国际软件工程大会，ICSE 2016，奥斯汀，TX，美国，2016年5月14-22日*。ACM,
    2016, 第702–713页。'
- en: '[16] Y. Li, S. Wang, and T. N. Nguyen, “Fault localization with code coverage
    representation learning,” in *43rd IEEE/ACM International Conference on Software
    Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021*.   IEEE, 2021, pp. 661–673.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Li, S. Wang, 和 T. N. Nguyen, “基于代码覆盖表示学习的故障定位”，发表于 *第43届IEEE/ACM国际软件工程大会，ICSE
    2021，马德里，西班牙，2021年5月22-30日*。IEEE, 2021, 第661–673页。'
- en: '[17] Y. Lou, Q. Zhu, J. Dong, X. Li, Z. Sun, D. Hao, L. Zhang, and L. Zhang,
    “Boosting coverage-based fault localization via graph-based representation learning,”
    in *ESEC/FSE ’21: 29th ACM Joint European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering, Athens, Greece, August 23-28,
    2021*.   ACM, 2021, pp. 664–676.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Lou, Q. Zhu, J. Dong, X. Li, Z. Sun, D. Hao, L. Zhang, 和 L. Zhang,
    “通过基于图的表示学习提升基于覆盖的故障定位”，发表于 *ESEC/FSE ’21：第29届ACM联合欧洲软件工程大会及软件工程基础研讨会，雅典，希腊，2021年8月23-28日*。ACM,
    2021, 第664–676页。'
- en: '[18] X. Li, W. Li, Y. Zhang, and L. Zhang, “Deepfl: integrating multiple fault
    diagnosis dimensions for deep fault localization,” in *Proceedings of the 28th
    ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2019,
    Beijing, China, July 15-19, 2019*.   ACM, 2019, pp. 169–180.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] X. Li, W. Li, Y. Zhang, 和 L. Zhang, “Deepfl: 综合多维度故障诊断以实现深度故障定位”，发表于 *第28届ACM
    SIGSOFT国际软件测试与分析研讨会，ISSTA 2019，北京，中国，2019年7月15-19日*。ACM, 2019, 第169–180页。'
- en: '[19] A. Z. H. Yang, C. L. Goues, R. Martins, and V. J. Hellendoorn, “Large
    language models for test-free fault localization,” in *Proceedings of the 46th
    IEEE/ACM International Conference on Software Engineering, ICSE 2024, Lisbon,
    Portugal, April 14-20, 2024*.   ACM, 2024, pp. 17:1–17:12.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Z. H. Yang, C. L. Goues, R. Martins 和 V. J. Hellendoorn, “无测试故障定位的大型语言模型，”
    发表在 *第46届IEEE/ACM国际软件工程会议，ICSE 2024，里斯本，葡萄牙，2024年4月14-20日*。ACM, 2024, 页码 17:1–17:12。'
- en: '[20] X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu, and C. Hu, “Template-based
    neural program repair,” in *45th IEEE/ACM International Conference on Software
    Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023*.   IEEE, 2023,
    pp. 1456–1468.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] X. Meng, X. Wang, H. Zhang, H. Sun, X. Liu 和 C. Hu, “基于模板的神经程序修复，” 发表在
    *第45届IEEE/ACM国际软件工程会议，ICSE 2023，墨尔本，澳大利亚，2023年5月14-20日*。IEEE, 2023, 页码 1456–1468。'
- en: '[21] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei, and L. Tan, “Coconut:
    combining context-aware neural translation models using ensemble for program repair,”
    in *ISSTA ’20: 29th ACM SIGSOFT International Symposium on Software Testing and
    Analysis, Virtual Event, USA, July 18-22, 2020*.   ACM, 2020, pp. 101–114.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] T. Lutellier, H. V. Pham, L. Pang, Y. Li, M. Wei 和 L. Tan, “Coconut：使用集成的上下文感知神经翻译模型进行程序修复，”
    发表在 *ISSTA ’20：第29届ACM SIGSOFT国际软件测试与分析研讨会，虚拟活动，美国，2020年7月18-22日*。ACM, 2020, 页码
    101–114。'
- en: '[22] N. Jiang, T. Lutellier, and L. Tan, “CURE: code-aware neural machine translation
    for automatic program repair,” in *43rd IEEE/ACM International Conference on Software
    Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021*.   IEEE, 2021, pp. 1161–1173.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] N. Jiang, T. Lutellier 和 L. Tan, “CURE：用于自动程序修复的代码感知神经机器翻译，” 发表在 *第43届IEEE/ACM国际软件工程会议，ICSE
    2021，马德里，西班牙，2021年5月22-30日*。IEEE, 2021, 页码 1161–1173。'
- en: '[23] A. Silva, S. Fang, and M. Monperrus, “Repairllama: Efficient representations
    and fine-tuned adapters for program repair,” *CoRR*, vol. abs/2312.15698, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. Silva, S. Fang 和 M. Monperrus, “Repairllama：程序修复的高效表示和微调适配器，” *CoRR*,
    vol. abs/2312.15698, 2023。'
- en: '[24] C. S. Xia, Y. Wei, and L. Zhang, “Automated program repair in the era
    of large pre-trained language models,” in *45th IEEE/ACM International Conference
    on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023*.   IEEE,
    2023, pp. 1482–1494.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] C. S. Xia, Y. Wei 和 L. Zhang, “在大型预训练语言模型时代的自动化程序修复，” 发表在 *第45届IEEE/ACM国际软件工程会议，ICSE
    2023，墨尔本，澳大利亚，2023年5月14-20日*。IEEE, 2023, 页码 1482–1494。'
- en: '[25] Y. Wei, C. S. Xia, and L. Zhang, “Copiloting the copilots: Fusing large
    language models with completion engines for automated program repair,” in *Proceedings
    of the 31st ACM Joint European Software Engineering Conference and Symposium on
    the Foundations of Software Engineering, ESEC/FSE 2023, San Francisco, CA, USA,
    December 3-9, 2023*.   ACM, 2023, pp. 172–184.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Wei, C. S. Xia 和 L. Zhang, “共同驾驶助手：将大型语言模型与完成引擎融合用于自动化程序修复，” 发表在 *第31届ACM欧洲联合软件工程会议暨软件工程基础研讨会，ESEC/FSE
    2023，旧金山，加利福尼亚州，美国，2023年12月3-9日*。ACM, 2023, 页码 172–184。'
- en: '[26] H. Joshi, J. P. C. Sánchez, S. Gulwani, V. Le, G. Verbruggen, and I. Radicek,
    “Repair is nearly generation: Multilingual program repair with llms,” in *Thirty-Seventh
    AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference
    on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium
    on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC,
    USA, February 7-14, 2023*.   AAAI Press, 2023, pp. 5131–5140.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] H. Joshi, J. P. C. Sánchez, S. Gulwani, V. Le, G. Verbruggen 和 I. Radicek,
    “修复几乎是生成：使用 llms 的多语言程序修复，” 发表在 *第37届AAAI人工智能会议，AAAI 2023，第35届创新应用人工智能会议，IAAI
    2023，第13届人工智能教育进展研讨会，EAAI 2023，华盛顿特区，美国，2023年2月7-14日*。AAAI Press, 2023, 页码 5131–5140。'
- en: '[27] C. S. Xia and L. Zhang, “Less training, more repairing please: revisiting
    automated program repair via zero-shot learning,” in *Proceedings of the 30th
    ACM Joint European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18,
    2022*.   ACM, 2022, pp. 959–971.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] C. S. Xia 和 L. Zhang, “少训练，多修复：通过零-shot学习重新审视自动化程序修复，” 发表在 *第30届ACM欧洲联合软件工程会议暨软件工程基础研讨会，ESEC/FSE
    2022，新加坡，新加坡，2022年11月14-18日*。ACM, 2022, 页码 959–971。'
- en: '[28] ——, “Keep the conversation going: Fixing 162 out of 337 bugs for $0.42
    each using chatgpt,” *CoRR*, vol. abs/2304.00385, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] ——, “保持对话持续：使用 chatgpt 以每个 $0.42 修复 337 个漏洞中的 162 个，” *CoRR*, vol. abs/2304.00385,
    2023。'
- en: '[29] P. S. Kochhar, X. Xia, D. Lo, and S. Li, “Practitioners’ expectations
    on automated fault localization,” in *Proceedings of the 25th International Symposium
    on Software Testing and Analysis, ISSTA 2016, Saarbrücken, Germany, July 18-20,
    2016*.   ACM, 2016, pp. 165–176.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] P. S. Kochhar, X. Xia, D. Lo 和 S. Li，“从业者对自动故障定位的期望”，在 *第 25 届国际软件测试和分析研讨会，ISSTA
    2016，德国萨尔布吕肯，2016 年 7 月 18-20*。ACM, 2016, 页 165–176。'
- en: '[30] J. Sohn and S. Yoo, “Fluccs: using code and change metrics to improve
    fault localization,” in *Proceedings of the 26th ACM SIGSOFT International Symposium
    on Software Testing and Analysis, Santa Barbara, CA, USA, July 10 - 14, 2017*.   ACM,
    2017, pp. 273–283.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Sohn 和 S. Yoo，“Fluccs: 使用代码和变化度量改进故障定位”，在 *第 26 届 ACM SIGSOFT 国际软件测试和分析研讨会，美国加州圣巴巴拉，2017
    年 7 月 10 - 14*。ACM, 2017, 页 273–283。'
- en: '[31] K. Liu, A. Koyuncu, T. F. Bissyandé, D. Kim, J. Klein, and Y. L. Traon,
    “You cannot fix what you cannot find! an investigation of fault localization bias
    in benchmarking automated program repair systems,” in *12th IEEE Conference on
    Software Testing, Validation and Verification, ICST,Xi’an, China, April 22-27,
    2019*.   IEEE, 2019, pp. 102–113.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] K. Liu, A. Koyuncu, T. F. Bissyandé, D. Kim, J. Klein 和 Y. L. Traon，“你无法修复你找不到的问题！对自动程序修复系统基准测试中故障定位偏差的调查”，在
    *第 12 届 IEEE 软件测试、验证与验证会议，ICST，西安，中国，2019 年 4 月 22-27*。IEEE, 2019, 页 102–113。'
- en: '[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi,
    Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large
    language models,” in *NeurIPS*, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi,
    Q. V. Le 和 D. Zhou，“链式思维提示在大型语言模型中引发推理”，在 *NeurIPS*, 2022。'
- en: '[33] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li, and Y. Zhang, “An
    empirical study on fine-tuning large language models of code for automated program
    repair,” in *38th IEEE/ACM International Conference on Automated Software Engineering,
    ASE 2023, Luxembourg, September 11-15, 2023*.   IEEE, 2023, pp. 1162–1174.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] K. Huang, X. Meng, J. Zhang, Y. Liu, W. Wang, S. Li 和 Y. Zhang，“关于微调大型语言模型进行自动程序修复的实证研究”，在
    *第 38 届 IEEE/ACM 国际自动化软件工程会议，ASE 2023，卢森堡，2023 年 9 月 11-15*。IEEE, 2023, 页 1162–1174。'
- en: '[34] S. Kirbas, E. Windels, O. McBello, K. Kells, M. W. Pagano, R. Szalanski,
    V. Nowack, E. R. Winter, S. Counsell, D. Bowes, T. Hall, S. Haraldsson, and J. R.
    Woodward, “On the introduction of automatic program repair in bloomberg,” *IEEE
    Softw.*, vol. 38, no. 4, pp. 43–51, 2021.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Kirbas, E. Windels, O. McBello, K. Kells, M. W. Pagano, R. Szalanski,
    V. Nowack, E. R. Winter, S. Counsell, D. Bowes, T. Hall, S. Haraldsson 和 J. R.
    Woodward，“在 Bloomberg 引入自动程序修复”，*IEEE 软件*, vol. 38, no. 4, 页 43–51, 2021。'
- en: '[35] M. Kumar. (2023) Understanding tokens in chatgpt. [Online]. Available:
    [https://medium.com/@manav.kumar87/understanding-tokens-in-chatgpt-32845987858d](https://medium.com/@manav.kumar87/understanding-tokens-in-chatgpt-32845987858d)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] M. Kumar. (2023) 了解 ChatGPT 中的令牌。 [在线]. 可用: [https://medium.com/@manav.kumar87/understanding-tokens-in-chatgpt-32845987858d](https://medium.com/@manav.kumar87/understanding-tokens-in-chatgpt-32845987858d)'
- en: '[36] D. Lin, J. Koppel, A. Chen, and A. Solar-Lezama, “Quixbugs: a multi-lingual
    program repair benchmark set based on the quixey challenge,” in *Proceedings Companion
    of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages,
    and Applications: Software for Humanity, SPLASH 2017, Vancouver, BC, Canada, October
    23 - 27, 2017*.   ACM, 2017, pp. 55–56.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Lin, J. Koppel, A. Chen 和 A. Solar-Lezama，“Quixbugs: 基于 quixey 挑战的多语言程序修复基准集”，在
    *2017 年 ACM SIGPLAN 国际系统、编程语言和应用程序：人类软件会议的会议附录，SPLASH 2017，加拿大温哥华，2017 年 10 月
    23 - 27*。ACM, 2017, 页 55–56。'
- en: '[37] S. H. Tan, J. Yi, Yulis, S. Mechtaev, and A. Roychoudhury, “Codeflaws:
    a programming competition benchmark for evaluating automated program repair tools,”
    in *Proceedings of the 39th International Conference on Software Engineering,
    ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 - Companion Volume*.   IEEE
    Computer Society, 2017, pp. 180–182.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. H. Tan, J. Yi, Yulis, S. Mechtaev 和 A. Roychoudhury，“Codeflaws: 一个用于评估自动程序修复工具的编程竞赛基准”，在
    *第 39 届国际软件工程大会，ICSE 2017，阿根廷布宜诺斯艾利斯，2017 年 5 月 20-28 - 附录卷*。IEEE 计算机协会, 2017,
    页 180–182。'
- en: '[38] A. Author(s). (2024) Rudra. [Online]. Available: [https://github.com/afortunado-aceptado/Rudra](https://github.com/afortunado-aceptado/Rudra)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. Author(s). (2024) Rudra. [在线]. 可用: [https://github.com/afortunado-aceptado/Rudra](https://github.com/afortunado-aceptado/Rudra)'
- en: '[39] Y. Lou, A. Ghanbari, X. Li, L. Zhang, D. Hao, and L. Zhang, “Can automated
    program repair refine fault localization?” *CoRR*, vol. abs/1910.01270, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Lou, A. Ghanbari, X. Li, L. Zhang, D. Hao 和 L. Zhang，“自动程序修复能否优化故障定位？”
    *CoRR*, vol. abs/1910.01270, 2019。'
- en: '[40] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, “Evaluating the
    logical reasoning ability of chatgpt and GPT-4,” *CoRR*, vol. abs/2304.03439,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, 和 Y. Zhang, “评估 ChatGPT 和 GPT-4
    的逻辑推理能力”，*CoRR*，卷 abs/2304.03439，2023。'
- en: '[41] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu,
    Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder: When the large language
    model meets programming - the rise of code intelligence,” *CoRR*, vol. abs/2401.14196,
    2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y.
    Wu, Y. K. Li, F. Luo, Y. Xiong, 和 W. Liang, “Deepseek-coder: 当大型语言模型遇上编程 - 代码智能的崛起”，*CoRR*，卷
    abs/2401.14196，2024。'
- en: '[42] OpenAI, “Gpt-4a technical report,” *CoRR*, vol. abs/2303.08774, 2023\.
    [Online]. Available: [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] OpenAI, “Gpt-4a 技术报告”，*CoRR*，卷 abs/2303.08774，2023。 [在线]. 可用: [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)'
- en: '[43] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: a database of existing
    faults to enable controlled testing studies for java programs,” in *International
    Symposium on Software Testing and Analysis, ISSTA ’14, San Jose, CA, USA - July
    21 - 26, 2014*.   ACM, 2014, pp. 437–440.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] R. Just, D. Jalali, 和 M. D. Ernst, “Defects4j: 现有缺陷的数据库，用于控制测试研究的 Java
    程序”，在 *国际软件测试与分析研讨会，ISSTA ’14，圣荷西，加州，美国 - 2014年7月21日至26日*。ACM，2014，第437–440页。'
- en: '[44] H. Ye and M. Monperrus, “ITER: iterative neural repair for multi-location
    patches,” in *Proceedings of the 46th IEEE/ACM International Conference on Software
    Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024*.   ACM, 2024, pp.
    10:1–10:13.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] H. Ye 和 M. Monperrus, “ITER: 多位置补丁的迭代神经修复”，在 *第46届 IEEE/ACM 国际软件工程大会，ICSE
    2024，里斯本，葡萄牙，2024年4月14-20日*。ACM，2024，第10:1–10:13页。'
- en: '[45] C. Wong, P. Santiesteban, C. Kästner, and C. L. Goues, “Varfix: balancing
    edit expressiveness and search effectiveness in automated program repair,” in
    *ESEC/FSE ’21: 29th ACM Joint European Software Engineering Conference and Symposium
    on the Foundations of Software Engineering, Athens, Greece, August 23-28, 2021*.   ACM,
    2021, pp. 354–366.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C. Wong, P. Santiesteban, C. Kästner, 和 C. L. Goues, “Varfix: 在自动程序修复中平衡编辑表达能力和搜索有效性”，在
    *ESEC/FSE ’21: 第29届 ACM 欧洲软件工程会议暨软件工程基础研讨会，雅典，希腊，2021年8月23-28日*。ACM，2021，第354–366页。'
- en: '[46] R. Abreu, P. Zoeteweij, and A. J. van Gemund, “On the accuracy of spectrum-based
    fault localization,” in *Testing: Academic and Industrial Conference Practice
    and Research Techniques - MUTATION (TAICPART-MUTATION 2007)*, 2007, pp. 89–98.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Abreu, P. Zoeteweij, 和 A. J. van Gemund, “基于谱的故障定位的准确性”，在 *测试: 学术和工业会议实践与研究技术
    - 突变 (TAICPART-MUTATION 2007)*，2007，第89–98页。'
- en: '[47] R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Y. Pan, Y. Wu, Z. Liu, and M. Sun,
    “Debugbench: Evaluating debugging capability of large language models,” *CoRR*,
    vol. abs/2401.04621, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] R. Tian, Y. Ye, Y. Qin, X. Cong, Y. Lin, Y. Pan, Y. Wu, Z. Liu, 和 M. Sun,
    “Debugbench: 评估大型语言模型的调试能力”，*CoRR*，卷 abs/2401.04621，2024。'
- en: '[48] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, and Z. Mao, “Expertprompting:
    Instructing large language models to be distinguished experts,” *CoRR*, vol. abs/2305.14688,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] B. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, 和 Z. Mao, “Expertprompting:
    指导大型语言模型成为杰出专家”，*CoRR*，卷 abs/2305.14688，2023。'
- en: '[49] X. D. Le, F. Thung, D. Lo, and C. L. Goues, “Overfitting in semantics-based
    automated program repair,” in *Proceedings of the 40th International Conference
    on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018*.   ACM,
    2018, p. 163.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. D. Le, F. Thung, D. Lo, 和 C. L. Goues, “基于语义的自动程序修复中的过拟合”，在 *第40届国际软件工程大会，ICSE
    2018，哥德堡，瑞典，2018年5月27日至6月3日*。ACM，2018，第163页。'
- en: '[50] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
    R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
    M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M.
    Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
    A. Radford, I. Sutskever, 和 D. Amodei, “语言模型是少样本学习者”，在 *神经信息处理系统进展 33: 2020 年神经信息处理系统年会，NeurIPS
    2020，2020年12月6-12日，虚拟会议*，2020。'
- en: '[51] Y. Wu, Z. Li, J. M. Zhang, and Y. Liu, “Condefects: A new dataset to address
    the data leakage concern for llm-based fault localization and program repair,”
    *CoRR*, vol. abs/2310.16253, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Y. Wu, Z. Li, J. M. Zhang 和 Y. Liu，“Condefects: 一个新的数据集，用于解决基于 LLM 的故障定位和程序修复中的数据泄漏问题，”
    *CoRR*, vol. abs/2310.16253, 2023.'
- en: '[52] F. Long, “Automatic patch generation via learning from successful human
    patches,” Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge,
    USA, 2018.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] F. Long，“通过学习成功的人类补丁来自动生成补丁，” 博士论文，麻省理工学院，美国剑桥，2018。'
- en: '[53] F. Long and M. C. Rinard, “Staged program repair with condition synthesis,”
    in *Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering,
    ESEC/FSE 2015, Bergamo, Italy, August 30 - September 4, 2015*.   ACM, 2015, pp.
    166–178.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] F. Long 和 M. C. Rinard，“带条件合成的分阶段程序修复，” 见于 *2015年第10届软件工程基础联合会议论文集，ESEC/FSE
    2015，意大利贝尔加莫，2015年8月30日 - 9月4日*。 ACM, 2015, 页 166–178.'
- en: '[54] A. Reynolds, M. Deters, V. Kuncak, C. Tinelli, and C. Barrett, “Counterexample-guided
    quantifier instantiation for synthesis in smt,” in *Computer Aided Verification:
    27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015,
    Proceedings, Part II 27*.   Springer, 2015, pp. 198–216.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Reynolds, M. Deters, V. Kuncak, C. Tinelli 和 C. Barrett，“用于 SMT 合成的反例引导量词实例化，”
    见于 *计算机辅助验证：第27届国际会议，CAV 2015，美国加利福尼亚州旧金山，2015年7月18-24日，论文集，第27部分*。 Springer,
    2015, 页 198–216.'
- en: '[55] H. Ye, M. Martinez, T. Durieux, and M. Monperrus, “A comprehensive study
    of automatic program repair on the quixbugs benchmark,” *J. Syst. Softw.*, vol.
    171, p. 110825, 2021\. [Online]. Available: [https://doi.org/10.1016/j.jss.2020.110825](https://doi.org/10.1016/j.jss.2020.110825)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] H. Ye, M. Martinez, T. Durieux 和 M. Monperrus，“对 Quixbugs 基准测试上自动程序修复的全面研究，”
    *J. Syst. Softw.*, vol. 171, 页 110825, 2021。 [在线]. 可用链接: [https://doi.org/10.1016/j.jss.2020.110825](https://doi.org/10.1016/j.jss.2020.110825)'
- en: '[56] R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk,
    A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou,
    J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. P. Lillicrap, A. Lazaridou,
    O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Viola, M. Reynolds,
    Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira, K. Ayoub,
    M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I. Danihelka,
    B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi, L. Gonzalez,
    M. Khalman, J. Sygnowski, and et al., “Gemini: A family of highly capable multimodal
    models,” *CoRR*, vol. abs/2312.11805, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] R. Anil, S. Borgeaud, Y. Wu, J. Alayrac, J. Yu, R. Soricut, J. Schalkwyk,
    A. M. Dai, A. Hauth, K. Millican, D. Silver, S. Petrov, M. Johnson, I. Antonoglou,
    J. Schrittwieser, A. Glaese, J. Chen, E. Pitler, T. P. Lillicrap, A. Lazaridou,
    O. Firat, J. Molloy, M. Isard, P. R. Barham, T. Hennigan, B. Lee, F. Viola, M.
    Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford, E. Moreira,
    K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov, I.
    Danihelka, B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi,
    L. Gonzalez, M. Khalman, J. Sygnowski 等人，“Gemini: 一系列高能力的多模态模型，” *CoRR*, vol.
    abs/2312.11805, 2023.'
- en: '[57] OpenAI. (2023) Gpt-3.5 turbo. [Online]. Available: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] OpenAI. (2023) Gpt-3.5 turbo. [在线]. 可用链接: [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)'
- en: '[58] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,
    J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. Canton-Ferrer,
    A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin,
    N. Usunier, T. Scialom, and G. Synnaeve, “Code llama: Open foundation models for
    code,” *CoRR*, vol. abs/2308.12950, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y.
    Adi, J. Liu, T. Remez, J. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt,
    C. Canton-Ferrer, A. Grattafiori, W. Xiong, A. Défossez, J. Copet, F. Azhar, H.
    Touvron, L. Martin, N. Usunier, T. Scialom 和 G. Synnaeve，“Code llama: 开放的代码基础模型，”
    *CoRR*, vol. abs/2308.12950, 2023.'
- en: '[59] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” *CoRR*, vol.
    abs/2107.03374, 2021.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, and W. Zaremba, “评估在代码上训练的大型语言模型，” *CoRR*, vol. abs/2107.03374,
    2021.'
- en: '[60] H. Ye, M. Martinez, and M. Monperrus, “Neural program repair with execution-based
    backpropagation,” in *44th IEEE/ACM 44th International Conference on Software
    Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022*.   ACM, 2022, pp.
    1506–1518.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Ye, M. Martinez, and M. Monperrus, “基于执行的反向传播进行神经程序修复，” 见 *第44届IEEE/ACM国际软件工程大会，ICSE
    2022，宾夕法尼亚州匹兹堡，美国，2022年5月25-27日*。ACM, 2022, pp. 1506–1518.'
- en: '[61] A. Holtzman, J. Buys, M. Forbes, and Y. Choi, “The curious case of neural
    text degeneration,” *CoRR*, vol. abs/1904.09751, 2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Holtzman, J. Buys, M. Forbes, and Y. Choi, “神经文本退化的奇怪案例，” *CoRR*, vol.
    abs/1904.09751, 2019.'
- en: '[62] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov,
    H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power,
    L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert,
    F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
    J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr,
    J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,
    M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever,
    and W. Zaremba, “Evaluating large language models trained on code,” *CoRR*, vol.
    abs/2107.03374, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
    H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M.
    Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov,
    A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
    M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A.
    Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
    A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight,
    M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish,
    I. Sutskever, and W. Zaremba, “评估在代码上训练的大型语言模型，” *CoRR*, vol. abs/2107.03374,
    2021.'
- en: '[63] R. Mao, G. Chen, X. Zhang, F. Guerin, and E. Cambria, “Gpteval: A survey
    on assessments of chatgpt and GPT-4,” *CoRR*, vol. abs/2308.12488, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Mao, G. Chen, X. Zhang, F. Guerin, and E. Cambria, “Gpteval：对chatgpt和GPT-4评估的调查，”
    *CoRR*, vol. abs/2308.12488, 2023.'
- en: '[64] Y. Li, S. Wang, and T. N. Nguyen, “Fault localization to detect co-change
    fixing locations,” in *Proceedings of the 30th ACM Joint European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE
    2022, Singapore, Singapore, November 14-18, 2022*.   ACM, 2022, pp. 659–671.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Li, S. Wang, and T. N. Nguyen, “故障定位以检测共同更改修复位置，” 见 *第30届ACM联合欧洲软件工程会议暨软件工程基础研讨会，ESEC/FSE
    2022，新加坡，新加坡，2022年11月14-18日*。ACM, 2022, pp. 659–671.'
- en: '[65] K. Liu, S. Wang, A. Koyuncu, K. Kim, T. F. Bissyandé, D. Kim, P. Wu, J. Klein,
    X. Mao, and Y. L. Traon, “On the efficiency of test suite based program repair:
    A systematic assessment of 16 automated repair systems for java programs,” in
    *ICSE ’20: 42nd International Conference on Software Engineering, Seoul, South
    Korea, 27 June - 19 July, 2020*.   ACM, 2020, pp. 615–627.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] K. Liu, S. Wang, A. Koyuncu, K. Kim, T. F. Bissyandé, D. Kim, P. Wu, J.
    Klein, X. Mao, and Y. L. Traon, “基于测试集的程序修复效率：对16个自动修复系统的系统评估，” 见 *ICSE ’20: 第42届国际软件工程会议，韩国首尔，2020年6月27日
    - 7月19日*。ACM, 2020, pp. 615–627.'
- en: '[66] X. Gao, Y. Noller, and A. Roychoudhury, “Program repair,” *CoRR*, vol.
    abs/2211.12787, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. Gao, Y. Noller, and A. Roychoudhury, “程序修复，” *CoRR*, vol. abs/2211.12787,
    2022.'
- en: '[67] S. Benton, X. Li, Y. Lou, and L. Zhang, “On the effectiveness of unified
    debugging: An extensive study on 16 program repair systems,” in *35th IEEE/ACM
    International Conference on Automated Software Engineering, ASE 2020, Melbourne,
    Australia, September 21-25, 2020*.   IEEE, 2020, pp. 907–918.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] S. Benton, X. Li, Y. Lou, 和 L. Zhang，“统一调试的有效性：对16个程序修复系统的广泛研究，”在*第35届IEEE/ACM国际自动化软件工程会议（ASE
    2020），2020年9月21-25日，澳大利亚墨尔本*。IEEE，2020年，第907-918页。'
- en: '[68] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang,
    and H. Chen, “Reasoning with language model prompting: A survey,” in *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*.   Association for
    Computational Linguistics, 2023, pp. 5368–5393.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng, C. Tan, F. Huang,
    和 H. Chen，“基于语言模型提示的推理：一项调查，”在*第61届计算语言学协会年会（第1卷：长篇论文），ACL 2023，2023年7月9-14日，加拿大多伦多*。计算语言学协会，2023年，第5368-5393页。'
- en: '[69] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large language models
    are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models,”
    in *Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing
    and Analysis, ISSTA 2023, Seattle, WA, USA, July 17-21, 2023*.   ACM, 2023, pp.
    423–435.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Y. Deng, C. S. Xia, H. Peng, C. Yang, 和 L. Zhang，“大型语言模型是零-shot模糊测试工具：通过大型语言模型对深度学习库进行模糊测试，”在*第32届ACM
    SIGSOFT国际软件测试与分析研讨会（ISSTA 2023），2023年7月17-21日，华盛顿州西雅图*。ACM，2023年，第423-435页。'
- en: '[70] X. Chen, M. Lin, N. Schärli, and D. Zhou, “Teaching large language models
    to self-debug,” *CoRR*, vol. abs/2304.05128, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] X. Chen, M. Lin, N. Schärli, 和 D. Zhou，“教大型语言模型自我调试，”*CoRR*，第abs/2304.05128卷，2023年。'
- en: '[71] N. Jain, S. Vaidyanath, A. S. Iyer, N. Natarajan, S. Parthasarathy, S. K.
    Rajamani, and R. Sharma, “Jigsaw: Large language models meet program synthesis,”
    in *44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
    2022, Pittsburgh, PA, USA, May 25-27, 2022*.   ACM, 2022, pp. 1219–1231.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] N. Jain, S. Vaidyanath, A. S. Iyer, N. Natarajan, S. Parthasarathy, S.
    K. Rajamani, 和 R. Sharma，“Jigsaw：大型语言模型遇上程序合成，”在*第44届IEEE/ACM国际软件工程会议（ICSE 2022），2022年5月25-27日，宾夕法尼亚州匹兹堡*。ACM，2022年，第1219-1231页。'
- en: '[72] H. Wang, Z. Liu, S. Wang, G. Cui, N. Ding, Z. Liu, and G. Yu, “INTERVENOR:
    prompt the coding ability of large language models with the interactive chain
    of repairing,” *CoRR*, vol. abs/2311.09868, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] H. Wang, Z. Liu, S. Wang, G. Cui, N. Ding, Z. Liu, 和 G. Yu，“INTERVENOR：通过交互链修复提升大型语言模型的编码能力，”*CoRR*，第abs/2311.09868卷，2023年。'
- en: '[73] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and A. Svyatkovskiy,
    “Inferfix: End-to-end program repair with llms,” *CoRR*, vol. abs/2303.07263,
    2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, 和 A. Svyatkovskiy，“Inferfix：利用大型语言模型的端到端程序修复，”*CoRR*，第abs/2303.07263卷，2023年。'
